import{S as ka,i as wa,s as ba,e as n,k as c,w as I,t as s,M as Ea,c as i,d as a,m,a as l,x as F,h as o,b as v,F as e,g as p,y as L,L as Oa,q as S,o as W,B as X}from"../chunks/vendor-75f4f71b.js";import{I as $a,C as V}from"../chunks/CodeBlock-09e20d6b.js";function Ta(Wt){let j,B,u,_,Y,T,Xt,Z,Bt,yt,d,Jt,tt,Gt,Kt,et,Ut,Vt,at,Yt,Zt,st,te,ee,vt,x,kt,h,ae,ot,se,oe,nt,ne,ie,it,le,re,lt,pe,ce,wt,C,bt,z,me,rt,de,he,pt,ue,fe,Et,R,Ot,f,ge,ct,_e,je,mt,ze,qe,D,dt,ye,ve,$t,A,Tt,w,ke,ht,we,be,xt,P,Ct,b,Ee,ut,Oe,$e,Rt,Q,Dt,J,Te,At,g,H,xe,ft,Ce,Re,De,k,Ae,gt,Pe,Qe,_t,He,Me,Ne,M,Ie,jt,Fe,Le,Se,zt,We,Pt,E,Xe,N,qt,Be,Je,Qt,G,Ge,Ht;return T=new $a({}),x=new V({props:{code:`from optimum.onnxruntime import ORTConfig, ORTQuantizer

# The model we wish to quantize
model_ckpt = "distilbert-base-uncased-finetuned-sst-2-english"
# The type of quantization to apply
ort_config = ORTConfig(quantization_approach="dynamic")
quantizer = ORTQuantizer(ort_config)
# Quantize the model!
quantizer.fit(model_ckpt, output_dir=".", feature="sequence-classification")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> optimum.onnxruntime <span class="hljs-keyword">import</span> ORTConfig, ORTQuantizer

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># The model we wish to quantize</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model_ckpt = <span class="hljs-string">&quot;distilbert-base-uncased-finetuned-sst-2-english&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># The type of quantization to apply</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>ort_config = ORTConfig(quantization_approach=<span class="hljs-string">&quot;dynamic&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>quantizer = ORTQuantizer(ort_config)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Quantize the model!</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>quantizer.fit(model_ckpt, output_dir=<span class="hljs-string">&quot;.&quot;</span>, feature=<span class="hljs-string">&quot;sequence-classification&quot;</span>)`}}),C=new V({props:{code:`from datasets import Dataset
from transformers import AutoTokenizer
from optimum.onnxruntime import ORTModel

# Load quantized model
ort_model = ORTModel("model-quantized.onnx", quantizer.onnx_config)
# Create a dataset or load one from the Hub
ds = Dataset.from_dict({"sentence": ["I love burritos!"]})
# Tokenize the inputs & convert to PyTorch tensors
tokenizer = AutoTokenizer.from_pretrained(model_ckpt)

def preprocess_fn(ex):
    return tokenizer(ex["sentence"])

tokenized_ds = ds.map(preprocess_fn, remove_columns=ds.column_names)
tokenized_ds.set_format("torch")
# Create dataloader and run evaluation
dataloader = DataLoader(tokenized_ds)
ort_outputs = ort_model.evaluation_loop(dataloader)
# Extract logits!
ort_outputs.predictions`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> Dataset
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> optimum.onnxruntime <span class="hljs-keyword">import</span> ORTModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Load quantized model</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>ort_model = ORTModel(<span class="hljs-string">&quot;model-quantized.onnx&quot;</span>, quantizer.onnx_config)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Create a dataset or load one from the Hub</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>ds = Dataset.from_dict({<span class="hljs-string">&quot;sentence&quot;</span>: [<span class="hljs-string">&quot;I love burritos!&quot;</span>]})
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Tokenize the inputs &amp; convert to PyTorch tensors</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(model_ckpt)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">def</span> <span class="hljs-title function_">preprocess_fn</span>(<span class="hljs-params">ex</span>):
<span class="hljs-meta">... </span>    <span class="hljs-keyword">return</span> tokenizer(ex[<span class="hljs-string">&quot;sentence&quot;</span>])

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenized_ds = ds.<span class="hljs-built_in">map</span>(preprocess_fn, remove_columns=ds.column_names)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenized_ds.set_format(<span class="hljs-string">&quot;torch&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Create dataloader and run evaluation</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>dataloader = DataLoader(tokenized_ds)
<span class="hljs-meta">&gt;&gt;&gt; </span>ort_outputs = ort_model.evaluation_loop(dataloader)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Extract logits!</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>ort_outputs.predictions`}}),R=new V({props:{code:'ort_config = ORTConfig(quantization_approach="static")',highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>ort_config = ORTConfig(quantization_approach=<span class="hljs-string">&quot;static&quot;</span>)'}}),A=new V({props:{code:`from transformers import DataCollatorWithPadding

# We use a data collator to pad the examples in a batch
data_collator = DataCollatorWithPadding(tokenizer)
# For calibration we define the dataset and preprocessing function
quantizer = ORTQuantizer(
    ort_config,
    dataset_name="glue",
    dataset_config_name="sst2",
    preprocess_function=preprocess_fn,
    data_collator=data_collator,
)
# Quantize the same way we did for dynamic quantization!
quantizer.fit(model_ckpt, output_dir=".", feature="sequence-classification")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DataCollatorWithPadding

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># We use a data collator to pad the examples in a batch</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>data_collator = DataCollatorWithPadding(tokenizer)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># For calibration we define the dataset and preprocessing function</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>quantizer = ORTQuantizer(
<span class="hljs-meta">... </span>    ort_config,
<span class="hljs-meta">... </span>    dataset_name=<span class="hljs-string">&quot;glue&quot;</span>,
<span class="hljs-meta">... </span>    dataset_config_name=<span class="hljs-string">&quot;sst2&quot;</span>,
<span class="hljs-meta">... </span>    preprocess_function=preprocess_fn,
<span class="hljs-meta">... </span>    data_collator=data_collator,
<span class="hljs-meta">... </span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Quantize the same way we did for dynamic quantization!</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>quantizer.fit(model_ckpt, output_dir=<span class="hljs-string">&quot;.&quot;</span>, feature=<span class="hljs-string">&quot;sequence-classification&quot;</span>)`}}),P=new V({props:{code:`# opt_level=99 enables all graph optimisations
ort_config = ORTConfig(opt_level=99)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># opt_level=99 enables all graph optimisations</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>ort_config = ORTConfig(opt_level=<span class="hljs-number">99</span>)`}}),Q=new V({props:{code:`from optimum.onnxruntime import ORTOptimizer

optimizer = ORTOptimizer(ort_config)
optimizer.fit(model_ckpt, output_dir=".", feature="sequence-classification")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> optimum.onnxruntime <span class="hljs-keyword">import</span> ORTOptimizer

<span class="hljs-meta">&gt;&gt;&gt; </span>optimizer = ORTOptimizer(ort_config)
<span class="hljs-meta">&gt;&gt;&gt; </span>optimizer.fit(model_ckpt, output_dir=<span class="hljs-string">&quot;.&quot;</span>, feature=<span class="hljs-string">&quot;sequence-classification&quot;</span>)`}}),{c(){j=n("meta"),B=c(),u=n("h1"),_=n("a"),Y=n("span"),I(T.$$.fragment),Xt=c(),Z=n("span"),Bt=s("Quickstart"),yt=c(),d=n("p"),Jt=s("At its core, \u{1F917} Optimum uses "),tt=n("em"),Gt=s("configuration objects"),Kt=s(" to define parameters for optimization on different accelerators. These objects are then used to instantiate dedicated "),et=n("em"),Ut=s("optimizers"),Vt=s(", "),at=n("em"),Yt=s("quantizers"),Zt=s(", and "),st=n("em"),te=s("pruners"),ee=s(". For example, here\u2019s how you can apply dynamic quantization with ONNX Runtime:"),vt=c(),I(x.$$.fragment),kt=c(),h=n("p"),ae=s("In this example, we\u2019ve quantized a model from the Hugging Face Hub, but it could also be a path to a local model directory. The "),ot=n("code"),se=s("feature"),oe=s(" argument in the "),nt=n("code"),ne=s("fit()"),ie=s(" method corresponds to the type of task that we wish to quantize the model for. The result from applying the "),it=n("code"),le=s("fit()"),re=s(" method is a "),lt=n("code"),pe=s("model-quantized.onnx"),ce=s(" file that can be used to run inference. Here\u2019s an example of how to load an ONNX Runtime model and generate predictions with it:"),wt=c(),I(C.$$.fragment),bt=c(),z=n("p"),me=s("Similarly, you can apply static quantization by simply changing the "),rt=n("code"),de=s("quantization_approach"),he=s(" in the "),pt=n("code"),ue=s("ORTConfig"),fe=s(" object:"),Et=c(),I(R.$$.fragment),Ot=c(),f=n("p"),ge=s("Static quantization relies on feeding batches of data through the model to observe the activation patterns ahead of inference time. The ideal quantization scheme is then calculated and saved. To support this, \u{1F917} Optimum allows you to provide a "),ct=n("em"),_e=s("calibration dataset"),je=s(". The calibration dataset can be a simple "),mt=n("code"),ze=s("Dataset"),qe=s(" object from the \u{1F917} Datasets library, or any dataset that\u2019s hosted on the Hugging Face Hub. For this example, we\u2019ll pick the "),D=n("a"),dt=n("code"),ye=s("sst2"),ve=s(" dataset that the model was originally trained on:"),$t=c(),I(A.$$.fragment),Tt=c(),w=n("p"),ke=s("As a final example, let\u2019s take a look at applying "),ht=n("em"),we=s("graph optimizations"),be=s(" techniques such as operator fusion and constant folding. As before, we load a configuration object, but this time by setting the optimization level instead of the quantization approach:"),xt=c(),I(P.$$.fragment),Ct=c(),b=n("p"),Ee=s("Next, we load an "),ut=n("em"),Oe=s("optimizer"),$e=s(" to apply these optimisations to our model:"),Rt=c(),I(Q.$$.fragment),Dt=c(),J=n("p"),Te=s("And that\u2019s it - the model is now optimized and ready for inference! As you can see, the process is similar in each case:"),At=c(),g=n("ol"),H=n("li"),xe=s("Define the optimization / quantization strategies via an "),ft=n("code"),Ce=s("ORTConfig"),Re=s(" object"),De=c(),k=n("li"),Ae=s("Instantiate a "),gt=n("code"),Pe=s("ORTQuantizer"),Qe=s(" or "),_t=n("code"),He=s("ORTOptimizer"),Me=s(" class"),Ne=c(),M=n("li"),Ie=s("Apply the "),jt=n("code"),Fe=s("fit()"),Le=s(" method"),Se=c(),zt=n("li"),We=s("Run inference"),Pt=c(),E=n("p"),Xe=s("Check out the "),N=n("a"),qt=n("code"),Be=s("examples"),Je=s(" directory for more sophisticated usage."),Qt=c(),G=n("p"),Ge=s("Happy optimising \u{1F917}!"),this.h()},l(t){const r=Ea('[data-svelte="svelte-1phssyn"]',document.head);j=i(r,"META",{name:!0,content:!0}),r.forEach(a),B=m(t),u=i(t,"H1",{class:!0});var Mt=l(u);_=i(Mt,"A",{id:!0,class:!0,href:!0});var Ke=l(_);Y=i(Ke,"SPAN",{});var Ue=l(Y);F(T.$$.fragment,Ue),Ue.forEach(a),Ke.forEach(a),Xt=m(Mt),Z=i(Mt,"SPAN",{});var Ve=l(Z);Bt=o(Ve,"Quickstart"),Ve.forEach(a),Mt.forEach(a),yt=m(t),d=i(t,"P",{});var q=l(d);Jt=o(q,"At its core, \u{1F917} Optimum uses "),tt=i(q,"EM",{});var Ye=l(tt);Gt=o(Ye,"configuration objects"),Ye.forEach(a),Kt=o(q," to define parameters for optimization on different accelerators. These objects are then used to instantiate dedicated "),et=i(q,"EM",{});var Ze=l(et);Ut=o(Ze,"optimizers"),Ze.forEach(a),Vt=o(q,", "),at=i(q,"EM",{});var ta=l(at);Yt=o(ta,"quantizers"),ta.forEach(a),Zt=o(q,", and "),st=i(q,"EM",{});var ea=l(st);te=o(ea,"pruners"),ea.forEach(a),ee=o(q,". For example, here\u2019s how you can apply dynamic quantization with ONNX Runtime:"),q.forEach(a),vt=m(t),F(x.$$.fragment,t),kt=m(t),h=i(t,"P",{});var y=l(h);ae=o(y,"In this example, we\u2019ve quantized a model from the Hugging Face Hub, but it could also be a path to a local model directory. The "),ot=i(y,"CODE",{});var aa=l(ot);se=o(aa,"feature"),aa.forEach(a),oe=o(y," argument in the "),nt=i(y,"CODE",{});var sa=l(nt);ne=o(sa,"fit()"),sa.forEach(a),ie=o(y," method corresponds to the type of task that we wish to quantize the model for. The result from applying the "),it=i(y,"CODE",{});var oa=l(it);le=o(oa,"fit()"),oa.forEach(a),re=o(y," method is a "),lt=i(y,"CODE",{});var na=l(lt);pe=o(na,"model-quantized.onnx"),na.forEach(a),ce=o(y," file that can be used to run inference. Here\u2019s an example of how to load an ONNX Runtime model and generate predictions with it:"),y.forEach(a),wt=m(t),F(C.$$.fragment,t),bt=m(t),z=i(t,"P",{});var K=l(z);me=o(K,"Similarly, you can apply static quantization by simply changing the "),rt=i(K,"CODE",{});var ia=l(rt);de=o(ia,"quantization_approach"),ia.forEach(a),he=o(K," in the "),pt=i(K,"CODE",{});var la=l(pt);ue=o(la,"ORTConfig"),la.forEach(a),fe=o(K," object:"),K.forEach(a),Et=m(t),F(R.$$.fragment,t),Ot=m(t),f=i(t,"P",{});var O=l(f);ge=o(O,"Static quantization relies on feeding batches of data through the model to observe the activation patterns ahead of inference time. The ideal quantization scheme is then calculated and saved. To support this, \u{1F917} Optimum allows you to provide a "),ct=i(O,"EM",{});var ra=l(ct);_e=o(ra,"calibration dataset"),ra.forEach(a),je=o(O,". The calibration dataset can be a simple "),mt=i(O,"CODE",{});var pa=l(mt);ze=o(pa,"Dataset"),pa.forEach(a),qe=o(O," object from the \u{1F917} Datasets library, or any dataset that\u2019s hosted on the Hugging Face Hub. For this example, we\u2019ll pick the "),D=i(O,"A",{href:!0,rel:!0});var ca=l(D);dt=i(ca,"CODE",{});var ma=l(dt);ye=o(ma,"sst2"),ma.forEach(a),ca.forEach(a),ve=o(O," dataset that the model was originally trained on:"),O.forEach(a),$t=m(t),F(A.$$.fragment,t),Tt=m(t),w=i(t,"P",{});var Nt=l(w);ke=o(Nt,"As a final example, let\u2019s take a look at applying "),ht=i(Nt,"EM",{});var da=l(ht);we=o(da,"graph optimizations"),da.forEach(a),be=o(Nt," techniques such as operator fusion and constant folding. As before, we load a configuration object, but this time by setting the optimization level instead of the quantization approach:"),Nt.forEach(a),xt=m(t),F(P.$$.fragment,t),Ct=m(t),b=i(t,"P",{});var It=l(b);Ee=o(It,"Next, we load an "),ut=i(It,"EM",{});var ha=l(ut);Oe=o(ha,"optimizer"),ha.forEach(a),$e=o(It," to apply these optimisations to our model:"),It.forEach(a),Rt=m(t),F(Q.$$.fragment,t),Dt=m(t),J=i(t,"P",{});var ua=l(J);Te=o(ua,"And that\u2019s it - the model is now optimized and ready for inference! As you can see, the process is similar in each case:"),ua.forEach(a),At=m(t),g=i(t,"OL",{});var $=l(g);H=i($,"LI",{});var Ft=l(H);xe=o(Ft,"Define the optimization / quantization strategies via an "),ft=i(Ft,"CODE",{});var fa=l(ft);Ce=o(fa,"ORTConfig"),fa.forEach(a),Re=o(Ft," object"),Ft.forEach(a),De=m($),k=i($,"LI",{});var U=l(k);Ae=o(U,"Instantiate a "),gt=i(U,"CODE",{});var ga=l(gt);Pe=o(ga,"ORTQuantizer"),ga.forEach(a),Qe=o(U," or "),_t=i(U,"CODE",{});var _a=l(_t);He=o(_a,"ORTOptimizer"),_a.forEach(a),Me=o(U," class"),U.forEach(a),Ne=m($),M=i($,"LI",{});var Lt=l(M);Ie=o(Lt,"Apply the "),jt=i(Lt,"CODE",{});var ja=l(jt);Fe=o(ja,"fit()"),ja.forEach(a),Le=o(Lt," method"),Lt.forEach(a),Se=m($),zt=i($,"LI",{});var za=l(zt);We=o(za,"Run inference"),za.forEach(a),$.forEach(a),Pt=m(t),E=i(t,"P",{});var St=l(E);Xe=o(St,"Check out the "),N=i(St,"A",{href:!0,rel:!0});var qa=l(N);qt=i(qa,"CODE",{});var ya=l(qt);Be=o(ya,"examples"),ya.forEach(a),qa.forEach(a),Je=o(St," directory for more sophisticated usage."),St.forEach(a),Qt=m(t),G=i(t,"P",{});var va=l(G);Ge=o(va,"Happy optimising \u{1F917}!"),va.forEach(a),this.h()},h(){v(j,"name","hf:doc:metadata"),v(j,"content",JSON.stringify(xa)),v(_,"id","quickstart"),v(_,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),v(_,"href","#quickstart"),v(u,"class","relative group"),v(D,"href","https://huggingface.co/datasets/glue/viewer/sst2/test"),v(D,"rel","nofollow"),v(N,"href","https://github.com/huggingface/optimum/tree/main/examples"),v(N,"rel","nofollow")},m(t,r){e(document.head,j),p(t,B,r),p(t,u,r),e(u,_),e(_,Y),L(T,Y,null),e(u,Xt),e(u,Z),e(Z,Bt),p(t,yt,r),p(t,d,r),e(d,Jt),e(d,tt),e(tt,Gt),e(d,Kt),e(d,et),e(et,Ut),e(d,Vt),e(d,at),e(at,Yt),e(d,Zt),e(d,st),e(st,te),e(d,ee),p(t,vt,r),L(x,t,r),p(t,kt,r),p(t,h,r),e(h,ae),e(h,ot),e(ot,se),e(h,oe),e(h,nt),e(nt,ne),e(h,ie),e(h,it),e(it,le),e(h,re),e(h,lt),e(lt,pe),e(h,ce),p(t,wt,r),L(C,t,r),p(t,bt,r),p(t,z,r),e(z,me),e(z,rt),e(rt,de),e(z,he),e(z,pt),e(pt,ue),e(z,fe),p(t,Et,r),L(R,t,r),p(t,Ot,r),p(t,f,r),e(f,ge),e(f,ct),e(ct,_e),e(f,je),e(f,mt),e(mt,ze),e(f,qe),e(f,D),e(D,dt),e(dt,ye),e(f,ve),p(t,$t,r),L(A,t,r),p(t,Tt,r),p(t,w,r),e(w,ke),e(w,ht),e(ht,we),e(w,be),p(t,xt,r),L(P,t,r),p(t,Ct,r),p(t,b,r),e(b,Ee),e(b,ut),e(ut,Oe),e(b,$e),p(t,Rt,r),L(Q,t,r),p(t,Dt,r),p(t,J,r),e(J,Te),p(t,At,r),p(t,g,r),e(g,H),e(H,xe),e(H,ft),e(ft,Ce),e(H,Re),e(g,De),e(g,k),e(k,Ae),e(k,gt),e(gt,Pe),e(k,Qe),e(k,_t),e(_t,He),e(k,Me),e(g,Ne),e(g,M),e(M,Ie),e(M,jt),e(jt,Fe),e(M,Le),e(g,Se),e(g,zt),e(zt,We),p(t,Pt,r),p(t,E,r),e(E,Xe),e(E,N),e(N,qt),e(qt,Be),e(E,Je),p(t,Qt,r),p(t,G,r),e(G,Ge),Ht=!0},p:Oa,i(t){Ht||(S(T.$$.fragment,t),S(x.$$.fragment,t),S(C.$$.fragment,t),S(R.$$.fragment,t),S(A.$$.fragment,t),S(P.$$.fragment,t),S(Q.$$.fragment,t),Ht=!0)},o(t){W(T.$$.fragment,t),W(x.$$.fragment,t),W(C.$$.fragment,t),W(R.$$.fragment,t),W(A.$$.fragment,t),W(P.$$.fragment,t),W(Q.$$.fragment,t),Ht=!1},d(t){a(j),t&&a(B),t&&a(u),X(T),t&&a(yt),t&&a(d),t&&a(vt),X(x,t),t&&a(kt),t&&a(h),t&&a(wt),X(C,t),t&&a(bt),t&&a(z),t&&a(Et),X(R,t),t&&a(Ot),t&&a(f),t&&a($t),X(A,t),t&&a(Tt),t&&a(w),t&&a(xt),X(P,t),t&&a(Ct),t&&a(b),t&&a(Rt),X(Q,t),t&&a(Dt),t&&a(J),t&&a(At),t&&a(g),t&&a(Pt),t&&a(E),t&&a(Qt),t&&a(G)}}}const xa={local:"quickstart",title:"Quickstart"};function Ca(Wt,j,B){let{fw:u}=j;return Wt.$$set=_=>{"fw"in _&&B(0,u=_.fw)},[u]}class Aa extends ka{constructor(j){super();wa(this,j,Ca,Ta,ba,{fw:0})}}export{Aa as default,xa as metadata};
