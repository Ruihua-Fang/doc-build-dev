import{S as Ce,i as Ee,s as ke,e as o,k as l,w as M,t as V,M as Te,c as a,d as n,m,a as i,x as C,h as W,b as r,F as t,g as p,y as E,L as De,q as k,o as T,B as D}from"../../chunks/vendor-19e06bd2.js";import{D as ae}from"../../chunks/Docstring-395e5a9c.js";import{I as ve}from"../../chunks/IconCopyLink-3c713d38.js";function Le(ie){let u,L,s,d,O,y,re,A,ce,B,f,v,P,w,se,S,le,R,h,$,me,z,q,de,F,ue,J,g,I,U,Q,pe,H,fe,G,_,x,he,b,N,ge,j,_e,K;return y=new ve({}),w=new ve({}),$=new ae({props:{name:"class optimum.intel.IncQuantizer",anchor:"optimum.intel.IncQuantizer",parameters:[{name:"model",val:": typing.Union[transformers.modeling_utils.PreTrainedModel, torch.nn.modules.module.Module]"},{name:"config_path_or_obj",val:": typing.Union[str, optimum.intel.neural_compressor.config.IncQuantizationConfig]"},{name:"tokenizer",val:": typing.Optional[transformers.tokenization_utils_base.PreTrainedTokenizerBase] = None"},{name:"eval_func",val:": typing.Optional[typing.Callable] = None"},{name:"train_func",val:": typing.Optional[typing.Callable] = None"},{name:"calib_dataloader",val:": typing.Optional[torch.utils.data.dataloader.DataLoader] = None"}],source:"https://github.com/huggingface/optimum/blob/pr_100/src/optimum/intel/neural_compressor/quantization.py#L53"}}),q=new ae({props:{name:"from_config",anchor:"optimum.intel.IncQuantizer.from_config",parameters:[{name:"model_name_or_path",val:": str"},{name:"inc_config",val:": typing.Union[optimum.intel.neural_compressor.config.IncQuantizationConfig, str, NoneType] = None"},{name:"config_name",val:": str = None"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/optimum/blob/pr_100/src/optimum/intel/neural_compressor/quantization.py#L179",parametersDescription:[{anchor:"optimum.intel.IncQuantizer.from_config.model_name_or_path",description:`<strong>model_name_or_path</strong> (<code>str</code>) &#x2014;
Repository name in the Hugging Face Hub or path to a local directory hosting the model.`,name:"model_name_or_path"},{anchor:"optimum.intel.IncQuantizer.from_config.inc_config",description:`<strong>inc_config</strong> (<code>Union[IncQuantizationConfig, str]</code>, <em>optional</em>) &#x2014;
Configuration file containing all the information related to the model quantization.
Can be either:<ul>
<li>an instance of the class <code>IncQuantizationConfig</code>,</li>
<li>a string valid as input to <code>IncQuantizationConfig.from_pretrained</code>.</li>
</ul>`,name:"inc_config"},{anchor:"optimum.intel.IncQuantizer.from_config.config_name",description:`<strong>config_name</strong> (<code>str</code>, <em>optional</em>) &#x2014;
Name of the configuration file.`,name:"config_name"},{anchor:"optimum.intel.IncQuantizer.from_config.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded configuration should be cached if the standard cache should
not be used.`,name:"cache_dir"},{anchor:"optimum.intel.IncQuantizer.from_config.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force to (re-)download the configuration files and override the cached versions if
they exist.`,name:"force_download"},{anchor:"optimum.intel.IncQuantizer.from_config.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received file. Attempts to resume the download if such a file
exists.`,name:"resume_download"},{anchor:"optimum.intel.IncQuantizer.from_config.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"optimum.intel.IncQuantizer.from_config.calib_dataloader",description:`<strong>calib_dataloader</strong> (<code>DataLoader</code>, <em>optional</em>) &#x2014;
DataLoader for post-training quantization calibration.`,name:"calib_dataloader"},{anchor:"optimum.intel.IncQuantizer.from_config.eval_func",description:`<strong>eval_func</strong> (<code>Callable</code>, <em>optional</em>) &#x2014;
Evaluation function to evaluate the tuning objective.`,name:"eval_func"},{anchor:"optimum.intel.IncQuantizer.from_config.train_func",description:`<strong>train_func</strong> (<code>Callable</code>, <em>optional</em>) &#x2014;
Training function for quantization aware training approach.`,name:"train_func"}],returnDescription:`
<p>IncQuantizer object.</p>
`,returnType:`
<p>quantizer</p>
`}}),Q=new ve({}),x=new ae({props:{name:"class optimum.intel.neural_compressor.quantization.IncQuantizedModel",anchor:"optimum.intel.neural_compressor.quantization.IncQuantizedModel",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/optimum/blob/pr_100/src/optimum/intel/neural_compressor/quantization.py#L349"}}),N=new ae({props:{name:"from_pretrained",anchor:"optimum.intel.neural_compressor.quantization.IncQuantizedModel.from_pretrained",parameters:[{name:"model_name_or_path",val:": str"},{name:"inc_config",val:": typing.Union[optimum.intel.neural_compressor.config.IncOptimizedConfig, str] = None"},{name:"q_model_name",val:": typing.Optional[str] = None"},{name:"input_names",val:": typing.Optional[typing.List[str]] = None"},{name:"batch_size",val:": typing.Optional[int] = None"},{name:"sequence_length",val:": typing.Union[int, typing.List[int], typing.Tuple[int], NoneType] = None"},{name:"num_choices",val:": typing.Optional[int] = -1"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/optimum/blob/pr_100/src/optimum/intel/neural_compressor/quantization.py#L359",parametersDescription:[{anchor:"optimum.intel.neural_compressor.quantization.IncQuantizedModel.from_pretrained.model_name_or_path",description:`<strong>model_name_or_path</strong> (<code>str</code>) &#x2014;
Repository name in the Hugging Face Hub or path to a local directory hosting the model.`,name:"model_name_or_path"},{anchor:"optimum.intel.neural_compressor.quantization.IncQuantizedModel.from_pretrained.inc_config",description:`<strong>inc_config</strong> (<code>Union[IncOptimizedConfig, str]</code>, <em>optional</em>) &#x2014;
Configuration file containing all the information related to the model quantization.
Can be either:<ul>
<li>an instance of the class <code>IncOptimizedConfig</code>,</li>
<li>a string valid as input to <code>IncOptimizedConfig.from_pretrained</code>.</li>
</ul>`,name:"inc_config"},{anchor:"optimum.intel.neural_compressor.quantization.IncQuantizedModel.from_pretrained.q_model_name",description:`<strong>q_model_name</strong> (<code>str</code>, <em>optional</em>) &#x2014;
Name of the state dictionary located in model_name_or_path used to load the quantized model. If
state_dict is specified, the latter will not be used.`,name:"q_model_name"},{anchor:"optimum.intel.neural_compressor.quantization.IncQuantizedModel.from_pretrained.input_names",description:`<strong>input_names</strong> (<code>List[str]</code>, <em>optional</em>) &#x2014;
List of names of the inputs used when tracing the model. If unset, model.dummy_inputs().keys() are used
instead.`,name:"input_names"},{anchor:"optimum.intel.neural_compressor.quantization.IncQuantizedModel.from_pretrained.batch_size",description:`<strong>batch_size</strong> (<code>int</code>, <em>optional</em>) &#x2014;
Batch size of the traced model inputs.`,name:"batch_size"},{anchor:"optimum.intel.neural_compressor.quantization.IncQuantizedModel.from_pretrained.sequence_length",description:`<strong>sequence_length</strong> (<code>Union[int, List[int], Tuple[int]]</code>, <em>optional</em>) &#x2014;
Sequence length of the traced model inputs. For sequence-to-sequence models with different sequence
lengths between the encoder and the decoder inputs, this must be <code>[encoder_sequence_length, decoder_sequence_length]</code>.`,name:"sequence_length"},{anchor:"optimum.intel.neural_compressor.quantization.IncQuantizedModel.from_pretrained.num_choices",description:`<strong>num_choices</strong> (<code>int</code>, <em>optional</em>, defaults to -1) &#x2014;
The number of possible choices for a multiple choice task.`,name:"num_choices"},{anchor:"optimum.intel.neural_compressor.quantization.IncQuantizedModel.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded configuration should be cached if the standard cache should
not be used.`,name:"cache_dir"},{anchor:"optimum.intel.neural_compressor.quantization.IncQuantizedModel.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force to (re-)download the configuration files and override the cached versions if
they exist.`,name:"force_download"},{anchor:"optimum.intel.neural_compressor.quantization.IncQuantizedModel.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received file. Attempts to resume the download if such a file
exists.`,name:"resume_download"},{anchor:"optimum.intel.neural_compressor.quantization.IncQuantizedModel.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"optimum.intel.neural_compressor.quantization.IncQuantizedModel.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<code>Dict[str, torch.Tensor]</code>, <em>optional</em>) &#x2014;
State dictionary of the quantized model, if not specified q_model_name will be used to load the
state dictionary.`,name:"state_dict"}],returnDescription:`
<p>Quantized model.</p>
`,returnType:`
<p>q_model</p>
`}}),{c(){u=o("meta"),L=l(),s=o("h1"),d=o("a"),O=o("span"),M(y.$$.fragment),re=l(),A=o("span"),ce=V("Quantization"),B=l(),f=o("h2"),v=o("a"),P=o("span"),M(w.$$.fragment),se=l(),S=o("span"),le=V("IncQuantizer"),R=l(),h=o("div"),M($.$$.fragment),me=l(),z=o("div"),M(q.$$.fragment),de=l(),F=o("p"),ue=V(`Instantiate a IncQuantizer object from a configuration file which can either be hosted on huggingface.co or
from a local directory path.`),J=l(),g=o("h2"),I=o("a"),U=o("span"),M(Q.$$.fragment),pe=l(),H=o("span"),fe=V("IncQuantizedModel"),G=l(),_=o("div"),M(x.$$.fragment),he=l(),b=o("div"),M(N.$$.fragment),ge=l(),j=o("p"),_e=V("Instantiate a quantized pytorch model from a given Intel Neural Compressor (INC) configuration file."),this.h()},l(e){const c=Te('[data-svelte="svelte-1phssyn"]',document.head);u=a(c,"META",{name:!0,content:!0}),c.forEach(n),L=m(e),s=a(e,"H1",{class:!0});var X=i(s);d=a(X,"A",{id:!0,class:!0,href:!0});var ze=i(d);O=a(ze,"SPAN",{});var Ie=i(O);C(y.$$.fragment,Ie),Ie.forEach(n),ze.forEach(n),re=m(X),A=a(X,"SPAN",{});var be=i(A);ce=W(be,"Quantization"),be.forEach(n),X.forEach(n),B=m(e),f=a(e,"H2",{class:!0});var Y=i(f);v=a(Y,"A",{id:!0,class:!0,href:!0});var ye=i(v);P=a(ye,"SPAN",{});var we=i(P);C(w.$$.fragment,we),we.forEach(n),ye.forEach(n),se=m(Y),S=a(Y,"SPAN",{});var $e=i(S);le=W($e,"IncQuantizer"),$e.forEach(n),Y.forEach(n),R=m(e),h=a(e,"DIV",{class:!0});var Z=i(h);C($.$$.fragment,Z),me=m(Z),z=a(Z,"DIV",{class:!0});var ee=i(z);C(q.$$.fragment,ee),de=m(ee),F=a(ee,"P",{});var qe=i(F);ue=W(qe,`Instantiate a IncQuantizer object from a configuration file which can either be hosted on huggingface.co or
from a local directory path.`),qe.forEach(n),ee.forEach(n),Z.forEach(n),J=m(e),g=a(e,"H2",{class:!0});var ne=i(g);I=a(ne,"A",{id:!0,class:!0,href:!0});var Qe=i(I);U=a(Qe,"SPAN",{});var xe=i(U);C(Q.$$.fragment,xe),xe.forEach(n),Qe.forEach(n),pe=m(ne),H=a(ne,"SPAN",{});var Ne=i(H);fe=W(Ne,"IncQuantizedModel"),Ne.forEach(n),ne.forEach(n),G=m(e),_=a(e,"DIV",{class:!0});var te=i(_);C(x.$$.fragment,te),he=m(te),b=a(te,"DIV",{class:!0});var oe=i(b);C(N.$$.fragment,oe),ge=m(oe),j=a(oe,"P",{});var Me=i(j);_e=W(Me,"Instantiate a quantized pytorch model from a given Intel Neural Compressor (INC) configuration file."),Me.forEach(n),oe.forEach(n),te.forEach(n),this.h()},h(){r(u,"name","hf:doc:metadata"),r(u,"content",JSON.stringify(Oe)),r(d,"id","quantization"),r(d,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(d,"href","#quantization"),r(s,"class","relative group"),r(v,"id","optimum.intel.IncQuantizer"),r(v,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(v,"href","#optimum.intel.IncQuantizer"),r(f,"class","relative group"),r(z,"class","docstring"),r(h,"class","docstring"),r(I,"id","optimum.intel.neural_compressor.quantization.IncQuantizedModel"),r(I,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(I,"href","#optimum.intel.neural_compressor.quantization.IncQuantizedModel"),r(g,"class","relative group"),r(b,"class","docstring"),r(_,"class","docstring")},m(e,c){t(document.head,u),p(e,L,c),p(e,s,c),t(s,d),t(d,O),E(y,O,null),t(s,re),t(s,A),t(A,ce),p(e,B,c),p(e,f,c),t(f,v),t(v,P),E(w,P,null),t(f,se),t(f,S),t(S,le),p(e,R,c),p(e,h,c),E($,h,null),t(h,me),t(h,z),E(q,z,null),t(z,de),t(z,F),t(F,ue),p(e,J,c),p(e,g,c),t(g,I),t(I,U),E(Q,U,null),t(g,pe),t(g,H),t(H,fe),p(e,G,c),p(e,_,c),E(x,_,null),t(_,he),t(_,b),E(N,b,null),t(b,ge),t(b,j),t(j,_e),K=!0},p:De,i(e){K||(k(y.$$.fragment,e),k(w.$$.fragment,e),k($.$$.fragment,e),k(q.$$.fragment,e),k(Q.$$.fragment,e),k(x.$$.fragment,e),k(N.$$.fragment,e),K=!0)},o(e){T(y.$$.fragment,e),T(w.$$.fragment,e),T($.$$.fragment,e),T(q.$$.fragment,e),T(Q.$$.fragment,e),T(x.$$.fragment,e),T(N.$$.fragment,e),K=!1},d(e){n(u),e&&n(L),e&&n(s),D(y),e&&n(B),e&&n(f),D(w),e&&n(R),e&&n(h),D($),D(q),e&&n(J),e&&n(g),D(Q),e&&n(G),e&&n(_),D(x),D(N)}}}const Oe={local:"quantization",sections:[{local:"optimum.intel.IncQuantizer",title:"IncQuantizer"},{local:"optimum.intel.neural_compressor.quantization.IncQuantizedModel",title:"IncQuantizedModel"}],title:"Quantization"};function Ae(ie,u,L){let{fw:s}=u;return ie.$$set=d=>{"fw"in d&&L(0,s=d.fw)},[s]}class Ue extends Ce{constructor(u){super();Ee(this,u,Ae,Le,ke,{fw:0})}}export{Ue as default,Oe as metadata};
