import{S as _o,i as Eo,s as yo,e as a,k as f,w as et,t as n,M as $o,c as o,d as r,m as p,a as l,x as rt,h as s,b as i,F as t,g as m,y as at,L as bo,q as ot,o as lt,B as it}from"../chunks/vendor-75f4f71b.js";import{I as Be,C as da}from"../chunks/CodeBlock-09e20d6b.js";function To(Me){let v,nt,d,c,Gt,H,Xe,Wt,Fe,fe,st,Ge,pe,ft,We,he,_,O,Yt,R,Ye,jt,je,me,pt,Je,de,ht,Ke,ce,P,Q,q,Ve,Ze,L,tr,er,Jt,rr,ue,E,k,Kt,U,ar,Vt,or,ve,mt,lr,ge,g,ir,B,nr,sr,M,fr,pr,we,D,Zt,y,dt,hr,mr,ct,dr,cr,ut,ur,vr,u,$,vt,gr,wr,gt,_r,Er,wt,yr,$r,b,_t,br,Tr,Et,Ar,Ir,yt,Nr,Or,T,$t,Pr,kr,bt,Dr,xr,Tt,Cr,zr,A,At,Sr,Hr,It,Rr,Qr,Nt,qr,_e,I,x,te,X,Lr,ee,Ur,Ee,C,Br,re,Mr,Xr,ye,F,$e,Ot,Fr,be,z,ae,G,Pt,Gr,Wr,kt,Yr,jr,N,W,Dt,Y,Jr,Kr,xt,oe,Vr,Zr,j,Ct,J,ta,ea,zt,le,ra,aa,K,St,V,oa,la,Ht,ie,ia,Te,Rt,na,Ae,Z,Ie,w,sa,ne,fa,pa,se,ha,ma,Ne,tt,Oe;return H=new Be({}),R=new Be({}),U=new Be({}),X=new Be({}),F=new da({props:{code:"python -m pip install optimum",highlighted:"python -m pip install optimum"}}),Z=new da({props:{code:"python -m pip install git+https://github.com/huggingface/optimum.git",highlighted:"python -m pip install git+https://github.com/huggingface/optimum.git"}}),tt=new da({props:{code:"python -m pip install git+https://github.com/huggingface/optimum.git#egg=optimum[onnxruntime]",highlighted:'python -m pip install git+https://github.com/huggingface/optimum.git<span class="hljs-comment">#egg=optimum[onnxruntime]</span>'}}),{c(){v=a("meta"),nt=f(),d=a("h1"),c=a("a"),Gt=a("span"),et(H.$$.fragment),Xe=f(),Wt=a("span"),Fe=n("\u{1F917} Optimum"),fe=f(),st=a("p"),Ge=n("\u{1F917} Optimum is an extension of \u{1F917} Transformers, providing a set of performance optimization tools enabling maximum efficiency to train and run models on targeted hardware."),pe=f(),ft=a("p"),We=n(`The AI ecosystem evolves quickly and more and more specialized hardware along with their own optimizations are emerging every day.
As such, Optimum enables users to efficiently use any of these platforms with the same ease inherent to transformers.`),he=f(),_=a("h2"),O=a("a"),Yt=a("span"),et(R.$$.fragment),Ye=f(),jt=a("span"),je=n("Integration with Hardware Partners"),me=f(),pt=a("p"),Je=n("\u{1F917} Optimum aims at providing more diversity towards the kind of hardware users can target to train and finetune their models."),de=f(),ht=a("p"),Ke=n("To achieve this, we are collaborating with the following hardware manufacturers in order to provide the best transformers integration:"),ce=f(),P=a("ul"),Q=a("li"),q=a("a"),Ve=n("GraphCore IPUs"),Ze=n(" - IPUs are a completely new kind of massively parallel processor to accelerate machine intelligence. "),L=a("a"),tr=n("More information here"),er=f(),Jt=a("li"),rr=n("More to come soon! :star:"),ue=f(),E=a("h2"),k=a("a"),Kt=a("span"),et(U.$$.fragment),ar=f(),Vt=a("span"),or=n("Optimizing models towards inference"),ve=f(),mt=a("p"),lr=n(`Along with supporting dedicated AI hardware for training, Optimum also provides inference optimizations towards various frameworks and
platforms.`),ge=f(),g=a("p"),ir=n("We currently support "),B=a("a"),nr=n("ONNX runtime"),sr=n(" along with "),M=a("a"),fr=n("Intel Neural Compressor (INC)"),pr=n("."),we=f(),D=a("table"),Zt=a("thead"),y=a("tr"),dt=a("th"),hr=n("Features"),mr=f(),ct=a("th"),dr=n("ONNX Runtime"),cr=f(),ut=a("th"),ur=n("Intel Neural Compressor"),vr=f(),u=a("tbody"),$=a("tr"),vt=a("td"),gr=n("Post-training Dynamic Quantization"),wr=f(),gt=a("td"),_r=n("\u2705"),Er=f(),wt=a("td"),yr=n("\u2705"),$r=f(),b=a("tr"),_t=a("td"),br=n("Post-training Static Quantization"),Tr=f(),Et=a("td"),Ar=n("\u2705"),Ir=f(),yt=a("td"),Nr=n("\u2705"),Or=f(),T=a("tr"),$t=a("td"),Pr=n("Quantization Aware Training (QAT)"),kr=f(),bt=a("td"),Dr=n("Stay tuned! \u2B50"),xr=f(),Tt=a("td"),Cr=n("\u2705"),zr=f(),A=a("tr"),At=a("td"),Sr=n("Pruning"),Hr=f(),It=a("td"),Rr=n("N/A"),Qr=f(),Nt=a("td"),qr=n("\u2705"),_e=f(),I=a("h2"),x=a("a"),te=a("span"),et(X.$$.fragment),Lr=f(),ee=a("span"),Ur=n("Installation"),Ee=f(),C=a("p"),Br=n("\u{1F917} Optimum can be installed using "),re=a("code"),Mr=n("pip"),Xr=n(" as follows:"),ye=f(),et(F.$$.fragment),$e=f(),Ot=a("p"),Fr=n("If you\u2019d like to use the accelerator-specific features of \u{1F917} Optimum, you can install the required dependencies according to the table below:"),be=f(),z=a("table"),ae=a("thead"),G=a("tr"),Pt=a("th"),Gr=n("Accelerator"),Wr=f(),kt=a("th"),Yr=n("Installation"),jr=f(),N=a("tbody"),W=a("tr"),Dt=a("td"),Y=a("a"),Jr=n("ONNX runtime"),Kr=f(),xt=a("td"),oe=a("code"),Vr=n("python -m pip install optimum[onnxruntime]"),Zr=f(),j=a("tr"),Ct=a("td"),J=a("a"),ta=n("Intel Neural Compressor (INC)"),ea=f(),zt=a("td"),le=a("code"),ra=n("python -m pip install optimum[intel]"),aa=f(),K=a("tr"),St=a("td"),V=a("a"),oa=n("Graphcore IPU"),la=f(),Ht=a("td"),ie=a("code"),ia=n("python -m pip install optimum[graphcore]"),Te=f(),Rt=a("p"),na=n("If you\u2019d like to play with the examples or need the bleeding edge of the code and can\u2019t wait for a new release, you can install the base library from source as follows:"),Ae=f(),et(Z.$$.fragment),Ie=f(),w=a("p"),sa=n("For the acclerator-specific features, you can install them by appending "),ne=a("code"),fa=n("#egg=optimum[accelerator_type]"),pa=n(" to the "),se=a("code"),ha=n("pip"),ma=n(" command, e.g."),Ne=f(),et(tt.$$.fragment),this.h()},l(e){const h=$o('[data-svelte="svelte-1phssyn"]',document.head);v=o(h,"META",{name:!0,content:!0}),h.forEach(r),nt=p(e),d=o(e,"H1",{class:!0});var Pe=l(d);c=o(Pe,"A",{id:!0,class:!0,href:!0});var ca=l(c);Gt=o(ca,"SPAN",{});var ua=l(Gt);rt(H.$$.fragment,ua),ua.forEach(r),ca.forEach(r),Xe=p(Pe),Wt=o(Pe,"SPAN",{});var va=l(Wt);Fe=s(va,"\u{1F917} Optimum"),va.forEach(r),Pe.forEach(r),fe=p(e),st=o(e,"P",{});var ga=l(st);Ge=s(ga,"\u{1F917} Optimum is an extension of \u{1F917} Transformers, providing a set of performance optimization tools enabling maximum efficiency to train and run models on targeted hardware."),ga.forEach(r),pe=p(e),ft=o(e,"P",{});var wa=l(ft);We=s(wa,`The AI ecosystem evolves quickly and more and more specialized hardware along with their own optimizations are emerging every day.
As such, Optimum enables users to efficiently use any of these platforms with the same ease inherent to transformers.`),wa.forEach(r),he=p(e),_=o(e,"H2",{class:!0});var ke=l(_);O=o(ke,"A",{id:!0,class:!0,href:!0});var _a=l(O);Yt=o(_a,"SPAN",{});var Ea=l(Yt);rt(R.$$.fragment,Ea),Ea.forEach(r),_a.forEach(r),Ye=p(ke),jt=o(ke,"SPAN",{});var ya=l(jt);je=s(ya,"Integration with Hardware Partners"),ya.forEach(r),ke.forEach(r),me=p(e),pt=o(e,"P",{});var $a=l(pt);Je=s($a,"\u{1F917} Optimum aims at providing more diversity towards the kind of hardware users can target to train and finetune their models."),$a.forEach(r),de=p(e),ht=o(e,"P",{});var ba=l(ht);Ke=s(ba,"To achieve this, we are collaborating with the following hardware manufacturers in order to provide the best transformers integration:"),ba.forEach(r),ce=p(e),P=o(e,"UL",{});var De=l(P);Q=o(De,"LI",{});var xe=l(Q);q=o(xe,"A",{href:!0,rel:!0});var Ta=l(q);Ve=s(Ta,"GraphCore IPUs"),Ta.forEach(r),Ze=s(xe," - IPUs are a completely new kind of massively parallel processor to accelerate machine intelligence. "),L=o(xe,"A",{href:!0,rel:!0});var Aa=l(L);tr=s(Aa,"More information here"),Aa.forEach(r),xe.forEach(r),er=p(De),Jt=o(De,"LI",{});var Ia=l(Jt);rr=s(Ia,"More to come soon! :star:"),Ia.forEach(r),De.forEach(r),ue=p(e),E=o(e,"H2",{class:!0});var Ce=l(E);k=o(Ce,"A",{id:!0,class:!0,href:!0});var Na=l(k);Kt=o(Na,"SPAN",{});var Oa=l(Kt);rt(U.$$.fragment,Oa),Oa.forEach(r),Na.forEach(r),ar=p(Ce),Vt=o(Ce,"SPAN",{});var Pa=l(Vt);or=s(Pa,"Optimizing models towards inference"),Pa.forEach(r),Ce.forEach(r),ve=p(e),mt=o(e,"P",{});var ka=l(mt);lr=s(ka,`Along with supporting dedicated AI hardware for training, Optimum also provides inference optimizations towards various frameworks and
platforms.`),ka.forEach(r),ge=p(e),g=o(e,"P",{});var Qt=l(g);ir=s(Qt,"We currently support "),B=o(Qt,"A",{href:!0,rel:!0});var Da=l(B);nr=s(Da,"ONNX runtime"),Da.forEach(r),sr=s(Qt," along with "),M=o(Qt,"A",{href:!0,rel:!0});var xa=l(M);fr=s(xa,"Intel Neural Compressor (INC)"),xa.forEach(r),pr=s(Qt,"."),Qt.forEach(r),we=p(e),D=o(e,"TABLE",{});var ze=l(D);Zt=o(ze,"THEAD",{});var Ca=l(Zt);y=o(Ca,"TR",{});var qt=l(y);dt=o(qt,"TH",{align:!0});var za=l(dt);hr=s(za,"Features"),za.forEach(r),mr=p(qt),ct=o(qt,"TH",{align:!0});var Sa=l(ct);dr=s(Sa,"ONNX Runtime"),Sa.forEach(r),cr=p(qt),ut=o(qt,"TH",{align:!0});var Ha=l(ut);ur=s(Ha,"Intel Neural Compressor"),Ha.forEach(r),qt.forEach(r),Ca.forEach(r),vr=p(ze),u=o(ze,"TBODY",{});var S=l(u);$=o(S,"TR",{});var Lt=l($);vt=o(Lt,"TD",{align:!0});var Ra=l(vt);gr=s(Ra,"Post-training Dynamic Quantization"),Ra.forEach(r),wr=p(Lt),gt=o(Lt,"TD",{align:!0});var Qa=l(gt);_r=s(Qa,"\u2705"),Qa.forEach(r),Er=p(Lt),wt=o(Lt,"TD",{align:!0});var qa=l(wt);yr=s(qa,"\u2705"),qa.forEach(r),Lt.forEach(r),$r=p(S),b=o(S,"TR",{});var Ut=l(b);_t=o(Ut,"TD",{align:!0});var La=l(_t);br=s(La,"Post-training Static Quantization"),La.forEach(r),Tr=p(Ut),Et=o(Ut,"TD",{align:!0});var Ua=l(Et);Ar=s(Ua,"\u2705"),Ua.forEach(r),Ir=p(Ut),yt=o(Ut,"TD",{align:!0});var Ba=l(yt);Nr=s(Ba,"\u2705"),Ba.forEach(r),Ut.forEach(r),Or=p(S),T=o(S,"TR",{});var Bt=l(T);$t=o(Bt,"TD",{align:!0});var Ma=l($t);Pr=s(Ma,"Quantization Aware Training (QAT)"),Ma.forEach(r),kr=p(Bt),bt=o(Bt,"TD",{align:!0});var Xa=l(bt);Dr=s(Xa,"Stay tuned! \u2B50"),Xa.forEach(r),xr=p(Bt),Tt=o(Bt,"TD",{align:!0});var Fa=l(Tt);Cr=s(Fa,"\u2705"),Fa.forEach(r),Bt.forEach(r),zr=p(S),A=o(S,"TR",{});var Mt=l(A);At=o(Mt,"TD",{align:!0});var Ga=l(At);Sr=s(Ga,"Pruning"),Ga.forEach(r),Hr=p(Mt),It=o(Mt,"TD",{align:!0});var Wa=l(It);Rr=s(Wa,"N/A"),Wa.forEach(r),Qr=p(Mt),Nt=o(Mt,"TD",{align:!0});var Ya=l(Nt);qr=s(Ya,"\u2705"),Ya.forEach(r),Mt.forEach(r),S.forEach(r),ze.forEach(r),_e=p(e),I=o(e,"H2",{class:!0});var Se=l(I);x=o(Se,"A",{id:!0,class:!0,href:!0});var ja=l(x);te=o(ja,"SPAN",{});var Ja=l(te);rt(X.$$.fragment,Ja),Ja.forEach(r),ja.forEach(r),Lr=p(Se),ee=o(Se,"SPAN",{});var Ka=l(ee);Ur=s(Ka,"Installation"),Ka.forEach(r),Se.forEach(r),Ee=p(e),C=o(e,"P",{});var He=l(C);Br=s(He,"\u{1F917} Optimum can be installed using "),re=o(He,"CODE",{});var Va=l(re);Mr=s(Va,"pip"),Va.forEach(r),Xr=s(He," as follows:"),He.forEach(r),ye=p(e),rt(F.$$.fragment,e),$e=p(e),Ot=o(e,"P",{});var Za=l(Ot);Fr=s(Za,"If you\u2019d like to use the accelerator-specific features of \u{1F917} Optimum, you can install the required dependencies according to the table below:"),Za.forEach(r),be=p(e),z=o(e,"TABLE",{});var Re=l(z);ae=o(Re,"THEAD",{});var to=l(ae);G=o(to,"TR",{});var Qe=l(G);Pt=o(Qe,"TH",{align:!0});var eo=l(Pt);Gr=s(eo,"Accelerator"),eo.forEach(r),Wr=p(Qe),kt=o(Qe,"TH",{align:!0});var ro=l(kt);Yr=s(ro,"Installation"),ro.forEach(r),Qe.forEach(r),to.forEach(r),jr=p(Re),N=o(Re,"TBODY",{});var Xt=l(N);W=o(Xt,"TR",{});var qe=l(W);Dt=o(qe,"TD",{align:!0});var ao=l(Dt);Y=o(ao,"A",{href:!0,rel:!0});var oo=l(Y);Jr=s(oo,"ONNX runtime"),oo.forEach(r),ao.forEach(r),Kr=p(qe),xt=o(qe,"TD",{align:!0});var lo=l(xt);oe=o(lo,"CODE",{});var io=l(oe);Vr=s(io,"python -m pip install optimum[onnxruntime]"),io.forEach(r),lo.forEach(r),qe.forEach(r),Zr=p(Xt),j=o(Xt,"TR",{});var Le=l(j);Ct=o(Le,"TD",{align:!0});var no=l(Ct);J=o(no,"A",{href:!0,rel:!0});var so=l(J);ta=s(so,"Intel Neural Compressor (INC)"),so.forEach(r),no.forEach(r),ea=p(Le),zt=o(Le,"TD",{align:!0});var fo=l(zt);le=o(fo,"CODE",{});var po=l(le);ra=s(po,"python -m pip install optimum[intel]"),po.forEach(r),fo.forEach(r),Le.forEach(r),aa=p(Xt),K=o(Xt,"TR",{});var Ue=l(K);St=o(Ue,"TD",{align:!0});var ho=l(St);V=o(ho,"A",{href:!0,rel:!0});var mo=l(V);oa=s(mo,"Graphcore IPU"),mo.forEach(r),ho.forEach(r),la=p(Ue),Ht=o(Ue,"TD",{align:!0});var co=l(Ht);ie=o(co,"CODE",{});var uo=l(ie);ia=s(uo,"python -m pip install optimum[graphcore]"),uo.forEach(r),co.forEach(r),Ue.forEach(r),Xt.forEach(r),Re.forEach(r),Te=p(e),Rt=o(e,"P",{});var vo=l(Rt);na=s(vo,"If you\u2019d like to play with the examples or need the bleeding edge of the code and can\u2019t wait for a new release, you can install the base library from source as follows:"),vo.forEach(r),Ae=p(e),rt(Z.$$.fragment,e),Ie=p(e),w=o(e,"P",{});var Ft=l(w);sa=s(Ft,"For the acclerator-specific features, you can install them by appending "),ne=o(Ft,"CODE",{});var go=l(ne);fa=s(go,"#egg=optimum[accelerator_type]"),go.forEach(r),pa=s(Ft," to the "),se=o(Ft,"CODE",{});var wo=l(se);ha=s(wo,"pip"),wo.forEach(r),ma=s(Ft," command, e.g."),Ft.forEach(r),Ne=p(e),rt(tt.$$.fragment,e),this.h()},h(){i(v,"name","hf:doc:metadata"),i(v,"content",JSON.stringify(Ao)),i(c,"id","optimum"),i(c,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(c,"href","#optimum"),i(d,"class","relative group"),i(O,"id","integration-with-hardware-partners"),i(O,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(O,"href","#integration-with-hardware-partners"),i(_,"class","relative group"),i(q,"href","https://github.com/huggingface/optimum-graphcore"),i(q,"rel","nofollow"),i(L,"href","https://www.graphcore.ai/products/ipu"),i(L,"rel","nofollow"),i(k,"id","optimizing-models-towards-inference"),i(k,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(k,"href","#optimizing-models-towards-inference"),i(E,"class","relative group"),i(B,"href","https://github.com/microsoft/onnxruntime"),i(B,"rel","nofollow"),i(M,"href","https://github.com/intel/neural-compressor"),i(M,"rel","nofollow"),i(dt,"align","center"),i(ct,"align","center"),i(ut,"align","center"),i(vt,"align","center"),i(gt,"align","center"),i(wt,"align","center"),i(_t,"align","center"),i(Et,"align","center"),i(yt,"align","center"),i($t,"align","center"),i(bt,"align","center"),i(Tt,"align","center"),i(At,"align","center"),i(It,"align","center"),i(Nt,"align","center"),i(x,"id","installation"),i(x,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(x,"href","#installation"),i(I,"class","relative group"),i(Pt,"align","left"),i(kt,"align","left"),i(Y,"href","https://github.com/microsoft/onnxruntime"),i(Y,"rel","nofollow"),i(Dt,"align","left"),i(xt,"align","left"),i(J,"href","https://github.com/intel/neural-compressor"),i(J,"rel","nofollow"),i(Ct,"align","left"),i(zt,"align","left"),i(V,"href","https://www.graphcore.ai/products/ipu"),i(V,"rel","nofollow"),i(St,"align","left"),i(Ht,"align","left")},m(e,h){t(document.head,v),m(e,nt,h),m(e,d,h),t(d,c),t(c,Gt),at(H,Gt,null),t(d,Xe),t(d,Wt),t(Wt,Fe),m(e,fe,h),m(e,st,h),t(st,Ge),m(e,pe,h),m(e,ft,h),t(ft,We),m(e,he,h),m(e,_,h),t(_,O),t(O,Yt),at(R,Yt,null),t(_,Ye),t(_,jt),t(jt,je),m(e,me,h),m(e,pt,h),t(pt,Je),m(e,de,h),m(e,ht,h),t(ht,Ke),m(e,ce,h),m(e,P,h),t(P,Q),t(Q,q),t(q,Ve),t(Q,Ze),t(Q,L),t(L,tr),t(P,er),t(P,Jt),t(Jt,rr),m(e,ue,h),m(e,E,h),t(E,k),t(k,Kt),at(U,Kt,null),t(E,ar),t(E,Vt),t(Vt,or),m(e,ve,h),m(e,mt,h),t(mt,lr),m(e,ge,h),m(e,g,h),t(g,ir),t(g,B),t(B,nr),t(g,sr),t(g,M),t(M,fr),t(g,pr),m(e,we,h),m(e,D,h),t(D,Zt),t(Zt,y),t(y,dt),t(dt,hr),t(y,mr),t(y,ct),t(ct,dr),t(y,cr),t(y,ut),t(ut,ur),t(D,vr),t(D,u),t(u,$),t($,vt),t(vt,gr),t($,wr),t($,gt),t(gt,_r),t($,Er),t($,wt),t(wt,yr),t(u,$r),t(u,b),t(b,_t),t(_t,br),t(b,Tr),t(b,Et),t(Et,Ar),t(b,Ir),t(b,yt),t(yt,Nr),t(u,Or),t(u,T),t(T,$t),t($t,Pr),t(T,kr),t(T,bt),t(bt,Dr),t(T,xr),t(T,Tt),t(Tt,Cr),t(u,zr),t(u,A),t(A,At),t(At,Sr),t(A,Hr),t(A,It),t(It,Rr),t(A,Qr),t(A,Nt),t(Nt,qr),m(e,_e,h),m(e,I,h),t(I,x),t(x,te),at(X,te,null),t(I,Lr),t(I,ee),t(ee,Ur),m(e,Ee,h),m(e,C,h),t(C,Br),t(C,re),t(re,Mr),t(C,Xr),m(e,ye,h),at(F,e,h),m(e,$e,h),m(e,Ot,h),t(Ot,Fr),m(e,be,h),m(e,z,h),t(z,ae),t(ae,G),t(G,Pt),t(Pt,Gr),t(G,Wr),t(G,kt),t(kt,Yr),t(z,jr),t(z,N),t(N,W),t(W,Dt),t(Dt,Y),t(Y,Jr),t(W,Kr),t(W,xt),t(xt,oe),t(oe,Vr),t(N,Zr),t(N,j),t(j,Ct),t(Ct,J),t(J,ta),t(j,ea),t(j,zt),t(zt,le),t(le,ra),t(N,aa),t(N,K),t(K,St),t(St,V),t(V,oa),t(K,la),t(K,Ht),t(Ht,ie),t(ie,ia),m(e,Te,h),m(e,Rt,h),t(Rt,na),m(e,Ae,h),at(Z,e,h),m(e,Ie,h),m(e,w,h),t(w,sa),t(w,ne),t(ne,fa),t(w,pa),t(w,se),t(se,ha),t(w,ma),m(e,Ne,h),at(tt,e,h),Oe=!0},p:bo,i(e){Oe||(ot(H.$$.fragment,e),ot(R.$$.fragment,e),ot(U.$$.fragment,e),ot(X.$$.fragment,e),ot(F.$$.fragment,e),ot(Z.$$.fragment,e),ot(tt.$$.fragment,e),Oe=!0)},o(e){lt(H.$$.fragment,e),lt(R.$$.fragment,e),lt(U.$$.fragment,e),lt(X.$$.fragment,e),lt(F.$$.fragment,e),lt(Z.$$.fragment,e),lt(tt.$$.fragment,e),Oe=!1},d(e){r(v),e&&r(nt),e&&r(d),it(H),e&&r(fe),e&&r(st),e&&r(pe),e&&r(ft),e&&r(he),e&&r(_),it(R),e&&r(me),e&&r(pt),e&&r(de),e&&r(ht),e&&r(ce),e&&r(P),e&&r(ue),e&&r(E),it(U),e&&r(ve),e&&r(mt),e&&r(ge),e&&r(g),e&&r(we),e&&r(D),e&&r(_e),e&&r(I),it(X),e&&r(Ee),e&&r(C),e&&r(ye),it(F,e),e&&r($e),e&&r(Ot),e&&r(be),e&&r(z),e&&r(Te),e&&r(Rt),e&&r(Ae),it(Z,e),e&&r(Ie),e&&r(w),e&&r(Ne),it(tt,e)}}}const Ao={local:"optimum",sections:[{local:"integration-with-hardware-partners",title:"Integration with Hardware Partners"},{local:"optimizing-models-towards-inference",title:"Optimizing models towards inference"},{local:"installation",title:"Installation"}],title:"\u{1F917} Optimum"};function Io(Me,v,nt){let{fw:d}=v;return Me.$$set=c=>{"fw"in c&&nt(0,d=c.fw)},[d]}class Po extends _o{constructor(v){super();Eo(this,v,Io,To,yo,{fw:0})}}export{Po as default,Ao as metadata};
