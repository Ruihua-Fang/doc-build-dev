import{S as qo,i as Bo,s as Xo,e as a,k as f,w as nt,t as n,M as Fo,c as o,d as r,m as h,a as l,x as st,h as s,b as i,F as t,g as d,y as ft,L as Wo,q as ht,o as pt,B as dt,v as Yo}from"../chunks/vendor-4918fc3c.js";import{I as Ve}from"../chunks/IconCopyLink-21d338b1.js";import{C as Na}from"../chunks/CodeBlock-99419108.js";function jo(ka){let w,ve,_,O,Zt,R,Ze,te,tr,we,mt,er,_e,ct,rr,Ee,E,D,ee,U,ar,re,or,ye,ut,lr,be,gt,ir,$e,u,L,M,nr,sr,G,fr,hr,Q,q,pr,dr,B,mr,cr,ae,ur,Te,y,N,oe,X,gr,le,vr,Pe,vt,wr,Ae,g,_r,F,Er,yr,W,br,$r,Ie,k,ie,b,wt,Tr,Pr,_t,Ar,Ir,Et,Or,Dr,m,$,yt,Nr,kr,bt,xr,Hr,$t,zr,Cr,T,Tt,Sr,Rr,Pt,Ur,Lr,At,Mr,Gr,P,It,Qr,qr,Ot,Br,Xr,Dt,Fr,Wr,A,Nt,Yr,jr,kt,Jr,Kr,xt,Vr,Oe,I,x,ne,Y,Zr,se,ta,De,H,ea,fe,ra,aa,Ne,j,ke,Ht,oa,xe,z,he,J,zt,la,ia,Ct,na,sa,c,K,St,V,fa,ha,Rt,pe,pa,da,Z,Ut,tt,ma,ca,Lt,de,ua,ga,et,Mt,rt,va,wa,Gt,me,_a,Ea,at,Qt,ot,ya,ba,qt,ce,$a,He,Bt,Ta,ze,lt,Ce,v,Pa,ue,Aa,Ia,ge,Oa,Da,Se,it,Re;return R=new Ve({}),U=new Ve({}),X=new Ve({}),Y=new Ve({}),j=new Na({props:{code:"python -m pip install optimum",highlighted:"python -m pip install optimum"}}),lt=new Na({props:{code:"python -m pip install git+https://github.com/huggingface/optimum.git",highlighted:"python -m pip install git+https://github.com/huggingface/optimum.git"}}),it=new Na({props:{code:"python -m pip install git+https://github.com/huggingface/optimum.git#egg=optimum[onnxruntime]",highlighted:'python -m pip install git+https://github.com/huggingface/optimum.git<span class="hljs-comment">#egg=optimum[onnxruntime]</span>'}}),{c(){w=a("meta"),ve=f(),_=a("h1"),O=a("a"),Zt=a("span"),nt(R.$$.fragment),Ze=f(),te=a("span"),tr=n("\u{1F917} Optimum"),we=f(),mt=a("p"),er=n("\u{1F917} Optimum is an extension of \u{1F917} Transformers, providing a set of performance optimization tools enabling maximum efficiency to train and run models on targeted hardware."),_e=f(),ct=a("p"),rr=n(`The AI ecosystem evolves quickly and more and more specialized hardware along with their own optimizations are emerging every day.
As such, Optimum enables users to efficiently use any of these platforms with the same ease inherent to transformers.`),Ee=f(),E=a("h2"),D=a("a"),ee=a("span"),nt(U.$$.fragment),ar=f(),re=a("span"),or=n("Integration with Hardware Partners"),ye=f(),ut=a("p"),lr=n("\u{1F917} Optimum aims at providing more diversity towards the kind of hardware users can target to train and finetune their models."),be=f(),gt=a("p"),ir=n("To achieve this, we are collaborating with the following hardware manufacturers in order to provide the best transformers integration:"),$e=f(),u=a("ul"),L=a("li"),M=a("a"),nr=n("Graphcore IPUs"),sr=n(" - IPUs are a completely new kind of massively parallel processor to accelerate machine intelligence. "),G=a("a"),fr=n("More information here."),hr=f(),Q=a("li"),q=a("a"),pr=n("Habana Gaudi Processor (HPU)"),dr=n(" - HPUs are designed to maximize training throughput and efficiency. "),B=a("a"),mr=n("More information here."),cr=f(),ae=a("li"),ur=n("More to come soon! :star:"),Te=f(),y=a("h2"),N=a("a"),oe=a("span"),nt(X.$$.fragment),gr=f(),le=a("span"),vr=n("Optimizing models towards inference"),Pe=f(),vt=a("p"),wr=n(`Along with supporting dedicated AI hardware for training, Optimum also provides inference optimizations towards various frameworks and
platforms.`),Ae=f(),g=a("p"),_r=n("We currently support "),F=a("a"),Er=n("ONNX runtime"),yr=n(" along with "),W=a("a"),br=n("Intel Neural Compressor (INC)"),$r=n("."),Ie=f(),k=a("table"),ie=a("thead"),b=a("tr"),wt=a("th"),Tr=n("Features"),Pr=f(),_t=a("th"),Ar=n("ONNX Runtime"),Ir=f(),Et=a("th"),Or=n("Intel Neural Compressor"),Dr=f(),m=a("tbody"),$=a("tr"),yt=a("td"),Nr=n("Post-training Dynamic Quantization"),kr=f(),bt=a("td"),xr=n("\u2705"),Hr=f(),$t=a("td"),zr=n("\u2705"),Cr=f(),T=a("tr"),Tt=a("td"),Sr=n("Post-training Static Quantization"),Rr=f(),Pt=a("td"),Ur=n("\u2705"),Lr=f(),At=a("td"),Mr=n("\u2705"),Gr=f(),P=a("tr"),It=a("td"),Qr=n("Quantization Aware Training (QAT)"),qr=f(),Ot=a("td"),Br=n("Stay tuned! \u2B50"),Xr=f(),Dt=a("td"),Fr=n("\u2705"),Wr=f(),A=a("tr"),Nt=a("td"),Yr=n("Pruning"),jr=f(),kt=a("td"),Jr=n("N/A"),Kr=f(),xt=a("td"),Vr=n("\u2705"),Oe=f(),I=a("h2"),x=a("a"),ne=a("span"),nt(Y.$$.fragment),Zr=f(),se=a("span"),ta=n("Installation"),De=f(),H=a("p"),ea=n("\u{1F917} Optimum can be installed using "),fe=a("code"),ra=n("pip"),aa=n(" as follows:"),Ne=f(),nt(j.$$.fragment),ke=f(),Ht=a("p"),oa=n("If you\u2019d like to use the accelerator-specific features of \u{1F917} Optimum, you can install the required dependencies according to the table below:"),xe=f(),z=a("table"),he=a("thead"),J=a("tr"),zt=a("th"),la=n("Accelerator"),ia=f(),Ct=a("th"),na=n("Installation"),sa=f(),c=a("tbody"),K=a("tr"),St=a("td"),V=a("a"),fa=n("ONNX runtime"),ha=f(),Rt=a("td"),pe=a("code"),pa=n("python -m pip install optimum[onnxruntime]"),da=f(),Z=a("tr"),Ut=a("td"),tt=a("a"),ma=n("Intel Neural Compressor (INC)"),ca=f(),Lt=a("td"),de=a("code"),ua=n("python -m pip install optimum[intel]"),ga=f(),et=a("tr"),Mt=a("td"),rt=a("a"),va=n("Graphcore IPU"),wa=f(),Gt=a("td"),me=a("code"),_a=n("python -m pip install optimum[graphcore]"),Ea=f(),at=a("tr"),Qt=a("td"),ot=a("a"),ya=n("Habana Gaudi Processor (HPU)"),ba=f(),qt=a("td"),ce=a("code"),$a=n("python -m pip install optimum[habana]"),He=f(),Bt=a("p"),Ta=n("If you\u2019d like to play with the examples or need the bleeding edge of the code and can\u2019t wait for a new release, you can install the base library from source as follows:"),ze=f(),nt(lt.$$.fragment),Ce=f(),v=a("p"),Pa=n("For the acclerator-specific features, you can install them by appending "),ue=a("code"),Aa=n("#egg=optimum[accelerator_type]"),Ia=n(" to the "),ge=a("code"),Oa=n("pip"),Da=n(" command, e.g."),Se=f(),nt(it.$$.fragment),this.h()},l(e){const p=Fo('[data-svelte="svelte-1phssyn"]',document.head);w=o(p,"META",{name:!0,content:!0}),p.forEach(r),ve=h(e),_=o(e,"H1",{class:!0});var Ue=l(_);O=o(Ue,"A",{id:!0,class:!0,href:!0});var xa=l(O);Zt=o(xa,"SPAN",{});var Ha=l(Zt);st(R.$$.fragment,Ha),Ha.forEach(r),xa.forEach(r),Ze=h(Ue),te=o(Ue,"SPAN",{});var za=l(te);tr=s(za,"\u{1F917} Optimum"),za.forEach(r),Ue.forEach(r),we=h(e),mt=o(e,"P",{});var Ca=l(mt);er=s(Ca,"\u{1F917} Optimum is an extension of \u{1F917} Transformers, providing a set of performance optimization tools enabling maximum efficiency to train and run models on targeted hardware."),Ca.forEach(r),_e=h(e),ct=o(e,"P",{});var Sa=l(ct);rr=s(Sa,`The AI ecosystem evolves quickly and more and more specialized hardware along with their own optimizations are emerging every day.
As such, Optimum enables users to efficiently use any of these platforms with the same ease inherent to transformers.`),Sa.forEach(r),Ee=h(e),E=o(e,"H2",{class:!0});var Le=l(E);D=o(Le,"A",{id:!0,class:!0,href:!0});var Ra=l(D);ee=o(Ra,"SPAN",{});var Ua=l(ee);st(U.$$.fragment,Ua),Ua.forEach(r),Ra.forEach(r),ar=h(Le),re=o(Le,"SPAN",{});var La=l(re);or=s(La,"Integration with Hardware Partners"),La.forEach(r),Le.forEach(r),ye=h(e),ut=o(e,"P",{});var Ma=l(ut);lr=s(Ma,"\u{1F917} Optimum aims at providing more diversity towards the kind of hardware users can target to train and finetune their models."),Ma.forEach(r),be=h(e),gt=o(e,"P",{});var Ga=l(gt);ir=s(Ga,"To achieve this, we are collaborating with the following hardware manufacturers in order to provide the best transformers integration:"),Ga.forEach(r),$e=h(e),u=o(e,"UL",{});var Xt=l(u);L=o(Xt,"LI",{});var Me=l(L);M=o(Me,"A",{href:!0,rel:!0});var Qa=l(M);nr=s(Qa,"Graphcore IPUs"),Qa.forEach(r),sr=s(Me," - IPUs are a completely new kind of massively parallel processor to accelerate machine intelligence. "),G=o(Me,"A",{href:!0,rel:!0});var qa=l(G);fr=s(qa,"More information here."),qa.forEach(r),Me.forEach(r),hr=h(Xt),Q=o(Xt,"LI",{});var Ge=l(Q);q=o(Ge,"A",{href:!0,rel:!0});var Ba=l(q);pr=s(Ba,"Habana Gaudi Processor (HPU)"),Ba.forEach(r),dr=s(Ge," - HPUs are designed to maximize training throughput and efficiency. "),B=o(Ge,"A",{href:!0,rel:!0});var Xa=l(B);mr=s(Xa,"More information here."),Xa.forEach(r),Ge.forEach(r),cr=h(Xt),ae=o(Xt,"LI",{});var Fa=l(ae);ur=s(Fa,"More to come soon! :star:"),Fa.forEach(r),Xt.forEach(r),Te=h(e),y=o(e,"H2",{class:!0});var Qe=l(y);N=o(Qe,"A",{id:!0,class:!0,href:!0});var Wa=l(N);oe=o(Wa,"SPAN",{});var Ya=l(oe);st(X.$$.fragment,Ya),Ya.forEach(r),Wa.forEach(r),gr=h(Qe),le=o(Qe,"SPAN",{});var ja=l(le);vr=s(ja,"Optimizing models towards inference"),ja.forEach(r),Qe.forEach(r),Pe=h(e),vt=o(e,"P",{});var Ja=l(vt);wr=s(Ja,`Along with supporting dedicated AI hardware for training, Optimum also provides inference optimizations towards various frameworks and
platforms.`),Ja.forEach(r),Ae=h(e),g=o(e,"P",{});var Ft=l(g);_r=s(Ft,"We currently support "),F=o(Ft,"A",{href:!0,rel:!0});var Ka=l(F);Er=s(Ka,"ONNX runtime"),Ka.forEach(r),yr=s(Ft," along with "),W=o(Ft,"A",{href:!0,rel:!0});var Va=l(W);br=s(Va,"Intel Neural Compressor (INC)"),Va.forEach(r),$r=s(Ft,"."),Ft.forEach(r),Ie=h(e),k=o(e,"TABLE",{});var qe=l(k);ie=o(qe,"THEAD",{});var Za=l(ie);b=o(Za,"TR",{});var Wt=l(b);wt=o(Wt,"TH",{align:!0});var to=l(wt);Tr=s(to,"Features"),to.forEach(r),Pr=h(Wt),_t=o(Wt,"TH",{align:!0});var eo=l(_t);Ar=s(eo,"ONNX Runtime"),eo.forEach(r),Ir=h(Wt),Et=o(Wt,"TH",{align:!0});var ro=l(Et);Or=s(ro,"Intel Neural Compressor"),ro.forEach(r),Wt.forEach(r),Za.forEach(r),Dr=h(qe),m=o(qe,"TBODY",{});var C=l(m);$=o(C,"TR",{});var Yt=l($);yt=o(Yt,"TD",{align:!0});var ao=l(yt);Nr=s(ao,"Post-training Dynamic Quantization"),ao.forEach(r),kr=h(Yt),bt=o(Yt,"TD",{align:!0});var oo=l(bt);xr=s(oo,"\u2705"),oo.forEach(r),Hr=h(Yt),$t=o(Yt,"TD",{align:!0});var lo=l($t);zr=s(lo,"\u2705"),lo.forEach(r),Yt.forEach(r),Cr=h(C),T=o(C,"TR",{});var jt=l(T);Tt=o(jt,"TD",{align:!0});var io=l(Tt);Sr=s(io,"Post-training Static Quantization"),io.forEach(r),Rr=h(jt),Pt=o(jt,"TD",{align:!0});var no=l(Pt);Ur=s(no,"\u2705"),no.forEach(r),Lr=h(jt),At=o(jt,"TD",{align:!0});var so=l(At);Mr=s(so,"\u2705"),so.forEach(r),jt.forEach(r),Gr=h(C),P=o(C,"TR",{});var Jt=l(P);It=o(Jt,"TD",{align:!0});var fo=l(It);Qr=s(fo,"Quantization Aware Training (QAT)"),fo.forEach(r),qr=h(Jt),Ot=o(Jt,"TD",{align:!0});var ho=l(Ot);Br=s(ho,"Stay tuned! \u2B50"),ho.forEach(r),Xr=h(Jt),Dt=o(Jt,"TD",{align:!0});var po=l(Dt);Fr=s(po,"\u2705"),po.forEach(r),Jt.forEach(r),Wr=h(C),A=o(C,"TR",{});var Kt=l(A);Nt=o(Kt,"TD",{align:!0});var mo=l(Nt);Yr=s(mo,"Pruning"),mo.forEach(r),jr=h(Kt),kt=o(Kt,"TD",{align:!0});var co=l(kt);Jr=s(co,"N/A"),co.forEach(r),Kr=h(Kt),xt=o(Kt,"TD",{align:!0});var uo=l(xt);Vr=s(uo,"\u2705"),uo.forEach(r),Kt.forEach(r),C.forEach(r),qe.forEach(r),Oe=h(e),I=o(e,"H2",{class:!0});var Be=l(I);x=o(Be,"A",{id:!0,class:!0,href:!0});var go=l(x);ne=o(go,"SPAN",{});var vo=l(ne);st(Y.$$.fragment,vo),vo.forEach(r),go.forEach(r),Zr=h(Be),se=o(Be,"SPAN",{});var wo=l(se);ta=s(wo,"Installation"),wo.forEach(r),Be.forEach(r),De=h(e),H=o(e,"P",{});var Xe=l(H);ea=s(Xe,"\u{1F917} Optimum can be installed using "),fe=o(Xe,"CODE",{});var _o=l(fe);ra=s(_o,"pip"),_o.forEach(r),aa=s(Xe," as follows:"),Xe.forEach(r),Ne=h(e),st(j.$$.fragment,e),ke=h(e),Ht=o(e,"P",{});var Eo=l(Ht);oa=s(Eo,"If you\u2019d like to use the accelerator-specific features of \u{1F917} Optimum, you can install the required dependencies according to the table below:"),Eo.forEach(r),xe=h(e),z=o(e,"TABLE",{});var Fe=l(z);he=o(Fe,"THEAD",{});var yo=l(he);J=o(yo,"TR",{});var We=l(J);zt=o(We,"TH",{align:!0});var bo=l(zt);la=s(bo,"Accelerator"),bo.forEach(r),ia=h(We),Ct=o(We,"TH",{align:!0});var $o=l(Ct);na=s($o,"Installation"),$o.forEach(r),We.forEach(r),yo.forEach(r),sa=h(Fe),c=o(Fe,"TBODY",{});var S=l(c);K=o(S,"TR",{});var Ye=l(K);St=o(Ye,"TD",{align:!0});var To=l(St);V=o(To,"A",{href:!0,rel:!0});var Po=l(V);fa=s(Po,"ONNX runtime"),Po.forEach(r),To.forEach(r),ha=h(Ye),Rt=o(Ye,"TD",{align:!0});var Ao=l(Rt);pe=o(Ao,"CODE",{});var Io=l(pe);pa=s(Io,"python -m pip install optimum[onnxruntime]"),Io.forEach(r),Ao.forEach(r),Ye.forEach(r),da=h(S),Z=o(S,"TR",{});var je=l(Z);Ut=o(je,"TD",{align:!0});var Oo=l(Ut);tt=o(Oo,"A",{href:!0,rel:!0});var Do=l(tt);ma=s(Do,"Intel Neural Compressor (INC)"),Do.forEach(r),Oo.forEach(r),ca=h(je),Lt=o(je,"TD",{align:!0});var No=l(Lt);de=o(No,"CODE",{});var ko=l(de);ua=s(ko,"python -m pip install optimum[intel]"),ko.forEach(r),No.forEach(r),je.forEach(r),ga=h(S),et=o(S,"TR",{});var Je=l(et);Mt=o(Je,"TD",{align:!0});var xo=l(Mt);rt=o(xo,"A",{href:!0,rel:!0});var Ho=l(rt);va=s(Ho,"Graphcore IPU"),Ho.forEach(r),xo.forEach(r),wa=h(Je),Gt=o(Je,"TD",{align:!0});var zo=l(Gt);me=o(zo,"CODE",{});var Co=l(me);_a=s(Co,"python -m pip install optimum[graphcore]"),Co.forEach(r),zo.forEach(r),Je.forEach(r),Ea=h(S),at=o(S,"TR",{});var Ke=l(at);Qt=o(Ke,"TD",{align:!0});var So=l(Qt);ot=o(So,"A",{href:!0,rel:!0});var Ro=l(ot);ya=s(Ro,"Habana Gaudi Processor (HPU)"),Ro.forEach(r),So.forEach(r),ba=h(Ke),qt=o(Ke,"TD",{align:!0});var Uo=l(qt);ce=o(Uo,"CODE",{});var Lo=l(ce);$a=s(Lo,"python -m pip install optimum[habana]"),Lo.forEach(r),Uo.forEach(r),Ke.forEach(r),S.forEach(r),Fe.forEach(r),He=h(e),Bt=o(e,"P",{});var Mo=l(Bt);Ta=s(Mo,"If you\u2019d like to play with the examples or need the bleeding edge of the code and can\u2019t wait for a new release, you can install the base library from source as follows:"),Mo.forEach(r),ze=h(e),st(lt.$$.fragment,e),Ce=h(e),v=o(e,"P",{});var Vt=l(v);Pa=s(Vt,"For the acclerator-specific features, you can install them by appending "),ue=o(Vt,"CODE",{});var Go=l(ue);Aa=s(Go,"#egg=optimum[accelerator_type]"),Go.forEach(r),Ia=s(Vt," to the "),ge=o(Vt,"CODE",{});var Qo=l(ge);Oa=s(Qo,"pip"),Qo.forEach(r),Da=s(Vt," command, e.g."),Vt.forEach(r),Se=h(e),st(it.$$.fragment,e),this.h()},h(){i(w,"name","hf:doc:metadata"),i(w,"content",JSON.stringify(Jo)),i(O,"id","optimum"),i(O,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(O,"href","#optimum"),i(_,"class","relative group"),i(D,"id","integration-with-hardware-partners"),i(D,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(D,"href","#integration-with-hardware-partners"),i(E,"class","relative group"),i(M,"href","https://github.com/huggingface/optimum-graphcore"),i(M,"rel","nofollow"),i(G,"href","https://www.graphcore.ai/products/ipu"),i(G,"rel","nofollow"),i(q,"href","https://github.com/huggingface/optimum-habana"),i(q,"rel","nofollow"),i(B,"href","https://habana.ai/training/"),i(B,"rel","nofollow"),i(N,"id","optimizing-models-towards-inference"),i(N,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(N,"href","#optimizing-models-towards-inference"),i(y,"class","relative group"),i(F,"href","https://github.com/microsoft/onnxruntime"),i(F,"rel","nofollow"),i(W,"href","https://github.com/intel/neural-compressor"),i(W,"rel","nofollow"),i(wt,"align","center"),i(_t,"align","center"),i(Et,"align","center"),i(yt,"align","center"),i(bt,"align","center"),i($t,"align","center"),i(Tt,"align","center"),i(Pt,"align","center"),i(At,"align","center"),i(It,"align","center"),i(Ot,"align","center"),i(Dt,"align","center"),i(Nt,"align","center"),i(kt,"align","center"),i(xt,"align","center"),i(x,"id","installation"),i(x,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(x,"href","#installation"),i(I,"class","relative group"),i(zt,"align","left"),i(Ct,"align","left"),i(V,"href","https://github.com/microsoft/onnxruntime"),i(V,"rel","nofollow"),i(St,"align","left"),i(Rt,"align","left"),i(tt,"href","https://github.com/intel/neural-compressor"),i(tt,"rel","nofollow"),i(Ut,"align","left"),i(Lt,"align","left"),i(rt,"href","https://www.graphcore.ai/products/ipu"),i(rt,"rel","nofollow"),i(Mt,"align","left"),i(Gt,"align","left"),i(ot,"href","https://habana.ai/training/"),i(ot,"rel","nofollow"),i(Qt,"align","left"),i(qt,"align","left")},m(e,p){t(document.head,w),d(e,ve,p),d(e,_,p),t(_,O),t(O,Zt),ft(R,Zt,null),t(_,Ze),t(_,te),t(te,tr),d(e,we,p),d(e,mt,p),t(mt,er),d(e,_e,p),d(e,ct,p),t(ct,rr),d(e,Ee,p),d(e,E,p),t(E,D),t(D,ee),ft(U,ee,null),t(E,ar),t(E,re),t(re,or),d(e,ye,p),d(e,ut,p),t(ut,lr),d(e,be,p),d(e,gt,p),t(gt,ir),d(e,$e,p),d(e,u,p),t(u,L),t(L,M),t(M,nr),t(L,sr),t(L,G),t(G,fr),t(u,hr),t(u,Q),t(Q,q),t(q,pr),t(Q,dr),t(Q,B),t(B,mr),t(u,cr),t(u,ae),t(ae,ur),d(e,Te,p),d(e,y,p),t(y,N),t(N,oe),ft(X,oe,null),t(y,gr),t(y,le),t(le,vr),d(e,Pe,p),d(e,vt,p),t(vt,wr),d(e,Ae,p),d(e,g,p),t(g,_r),t(g,F),t(F,Er),t(g,yr),t(g,W),t(W,br),t(g,$r),d(e,Ie,p),d(e,k,p),t(k,ie),t(ie,b),t(b,wt),t(wt,Tr),t(b,Pr),t(b,_t),t(_t,Ar),t(b,Ir),t(b,Et),t(Et,Or),t(k,Dr),t(k,m),t(m,$),t($,yt),t(yt,Nr),t($,kr),t($,bt),t(bt,xr),t($,Hr),t($,$t),t($t,zr),t(m,Cr),t(m,T),t(T,Tt),t(Tt,Sr),t(T,Rr),t(T,Pt),t(Pt,Ur),t(T,Lr),t(T,At),t(At,Mr),t(m,Gr),t(m,P),t(P,It),t(It,Qr),t(P,qr),t(P,Ot),t(Ot,Br),t(P,Xr),t(P,Dt),t(Dt,Fr),t(m,Wr),t(m,A),t(A,Nt),t(Nt,Yr),t(A,jr),t(A,kt),t(kt,Jr),t(A,Kr),t(A,xt),t(xt,Vr),d(e,Oe,p),d(e,I,p),t(I,x),t(x,ne),ft(Y,ne,null),t(I,Zr),t(I,se),t(se,ta),d(e,De,p),d(e,H,p),t(H,ea),t(H,fe),t(fe,ra),t(H,aa),d(e,Ne,p),ft(j,e,p),d(e,ke,p),d(e,Ht,p),t(Ht,oa),d(e,xe,p),d(e,z,p),t(z,he),t(he,J),t(J,zt),t(zt,la),t(J,ia),t(J,Ct),t(Ct,na),t(z,sa),t(z,c),t(c,K),t(K,St),t(St,V),t(V,fa),t(K,ha),t(K,Rt),t(Rt,pe),t(pe,pa),t(c,da),t(c,Z),t(Z,Ut),t(Ut,tt),t(tt,ma),t(Z,ca),t(Z,Lt),t(Lt,de),t(de,ua),t(c,ga),t(c,et),t(et,Mt),t(Mt,rt),t(rt,va),t(et,wa),t(et,Gt),t(Gt,me),t(me,_a),t(c,Ea),t(c,at),t(at,Qt),t(Qt,ot),t(ot,ya),t(at,ba),t(at,qt),t(qt,ce),t(ce,$a),d(e,He,p),d(e,Bt,p),t(Bt,Ta),d(e,ze,p),ft(lt,e,p),d(e,Ce,p),d(e,v,p),t(v,Pa),t(v,ue),t(ue,Aa),t(v,Ia),t(v,ge),t(ge,Oa),t(v,Da),d(e,Se,p),ft(it,e,p),Re=!0},p:Wo,i(e){Re||(ht(R.$$.fragment,e),ht(U.$$.fragment,e),ht(X.$$.fragment,e),ht(Y.$$.fragment,e),ht(j.$$.fragment,e),ht(lt.$$.fragment,e),ht(it.$$.fragment,e),Re=!0)},o(e){pt(R.$$.fragment,e),pt(U.$$.fragment,e),pt(X.$$.fragment,e),pt(Y.$$.fragment,e),pt(j.$$.fragment,e),pt(lt.$$.fragment,e),pt(it.$$.fragment,e),Re=!1},d(e){r(w),e&&r(ve),e&&r(_),dt(R),e&&r(we),e&&r(mt),e&&r(_e),e&&r(ct),e&&r(Ee),e&&r(E),dt(U),e&&r(ye),e&&r(ut),e&&r(be),e&&r(gt),e&&r($e),e&&r(u),e&&r(Te),e&&r(y),dt(X),e&&r(Pe),e&&r(vt),e&&r(Ae),e&&r(g),e&&r(Ie),e&&r(k),e&&r(Oe),e&&r(I),dt(Y),e&&r(De),e&&r(H),e&&r(Ne),dt(j,e),e&&r(ke),e&&r(Ht),e&&r(xe),e&&r(z),e&&r(He),e&&r(Bt),e&&r(ze),dt(lt,e),e&&r(Ce),e&&r(v),e&&r(Se),dt(it,e)}}}const Jo={local:"optimum",sections:[{local:"integration-with-hardware-partners",title:"Integration with Hardware Partners"},{local:"optimizing-models-towards-inference",title:"Optimizing models towards inference"},{local:"installation",title:"Installation"}],title:"\u{1F917} Optimum"};function Ko(ka){return Yo(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class el extends qo{constructor(w){super();Bo(this,w,Ko,jo,Xo,{})}}export{el as default,Jo as metadata};
