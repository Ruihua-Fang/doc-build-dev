import{S as Ce,i as Ee,s as ke,e as o,k as s,w as N,t as j,M as Le,c as a,d as n,m as l,a as i,x as C,h as V,b as r,F as t,g as m,y as E,L as Pe,q as k,o as L,B as P,v as Te}from"../../chunks/vendor-19e06bd2.js";import{D as ae}from"../../chunks/Docstring-395e5a9c.js";import{I as _e}from"../../chunks/IconCopyLink-3c713d38.js";function De(ve){let d,W,u,_,T,y,ie,D,re,B,p,v,O,w,ce,A,se,R,h,$,le,z,q,me,S,de,J,f,I,U,Q,ue,F,pe,G,g,x,he,b,M,fe,H,ge,K;return y=new _e({}),w=new _e({}),$=new ae({props:{name:"class optimum.intel.IncQuantizer",anchor:"optimum.intel.IncQuantizer",parameters:[{name:"model",val:": typing.Union[transformers.modeling_utils.PreTrainedModel, torch.nn.modules.module.Module]"},{name:"config_path_or_obj",val:": typing.Union[str, optimum.intel.neural_compressor.configuration.IncQuantizationConfig]"},{name:"tokenizer",val:": typing.Optional[transformers.tokenization_utils_base.PreTrainedTokenizerBase] = None"},{name:"eval_func",val:": typing.Optional[typing.Callable] = None"},{name:"train_func",val:": typing.Optional[typing.Callable] = None"},{name:"calib_dataloader",val:": typing.Optional[torch.utils.data.dataloader.DataLoader] = None"}],source:"https://github.com/huggingface/optimum/blob/pr_120/src/optimum/intel/neural_compressor/quantization.py#L53"}}),q=new ae({props:{name:"from_config",anchor:"optimum.intel.IncQuantizer.from_config",parameters:[{name:"model_name_or_path",val:": str"},{name:"inc_config",val:": typing.Union[optimum.intel.neural_compressor.configuration.IncQuantizationConfig, str, NoneType] = None"},{name:"config_name",val:": str = None"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/optimum/blob/pr_120/src/optimum/intel/neural_compressor/quantization.py#L179",parametersDescription:[{anchor:"optimum.intel.IncQuantizer.from_config.model_name_or_path",description:`<strong>model_name_or_path</strong> (<code>str</code>) &#x2014;
Repository name in the Hugging Face Hub or path to a local directory hosting the model.`,name:"model_name_or_path"},{anchor:"optimum.intel.IncQuantizer.from_config.inc_config",description:`<strong>inc_config</strong> (<code>Union[IncQuantizationConfig, str]</code>, <em>optional</em>) &#x2014;
Configuration file containing all the information related to the model quantization.
Can be either:<ul>
<li>an instance of the class <code>IncQuantizationConfig</code>,</li>
<li>a string valid as input to <code>IncQuantizationConfig.from_pretrained</code>.</li>
</ul>`,name:"inc_config"},{anchor:"optimum.intel.IncQuantizer.from_config.config_name",description:`<strong>config_name</strong> (<code>str</code>, <em>optional</em>) &#x2014;
Name of the configuration file.`,name:"config_name"},{anchor:"optimum.intel.IncQuantizer.from_config.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded configuration should be cached if the standard cache should
not be used.`,name:"cache_dir"},{anchor:"optimum.intel.IncQuantizer.from_config.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force to (re-)download the configuration files and override the cached versions if
they exist.`,name:"force_download"},{anchor:"optimum.intel.IncQuantizer.from_config.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received file. Attempts to resume the download if such a file
exists.`,name:"resume_download"},{anchor:"optimum.intel.IncQuantizer.from_config.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"optimum.intel.IncQuantizer.from_config.calib_dataloader",description:`<strong>calib_dataloader</strong> (<code>DataLoader</code>, <em>optional</em>) &#x2014;
DataLoader for post-training quantization calibration.`,name:"calib_dataloader"},{anchor:"optimum.intel.IncQuantizer.from_config.eval_func",description:`<strong>eval_func</strong> (<code>Callable</code>, <em>optional</em>) &#x2014;
Evaluation function to evaluate the tuning objective.`,name:"eval_func"},{anchor:"optimum.intel.IncQuantizer.from_config.train_func",description:`<strong>train_func</strong> (<code>Callable</code>, <em>optional</em>) &#x2014;
Training function for quantization aware training approach.`,name:"train_func"}],returnDescription:`
<p>IncQuantizer object.</p>
`,returnType:`
<p>quantizer</p>
`}}),Q=new _e({}),x=new ae({props:{name:"class optimum.intel.neural_compressor.quantization.IncQuantizedModel",anchor:"optimum.intel.neural_compressor.quantization.IncQuantizedModel",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/optimum/blob/pr_120/src/optimum/intel/neural_compressor/quantization.py#L349"}}),M=new ae({props:{name:"from_pretrained",anchor:"optimum.intel.neural_compressor.quantization.IncQuantizedModel.from_pretrained",parameters:[{name:"model_name_or_path",val:": str"},{name:"inc_config",val:": typing.Union[optimum.intel.neural_compressor.configuration.IncOptimizedConfig, str] = None"},{name:"q_model_name",val:": typing.Optional[str] = None"},{name:"input_names",val:": typing.Optional[typing.List[str]] = None"},{name:"batch_size",val:": typing.Optional[int] = None"},{name:"sequence_length",val:": typing.Union[int, typing.List[int], typing.Tuple[int], NoneType] = None"},{name:"num_choices",val:": typing.Optional[int] = -1"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/optimum/blob/pr_120/src/optimum/intel/neural_compressor/quantization.py#L359",parametersDescription:[{anchor:"optimum.intel.neural_compressor.quantization.IncQuantizedModel.from_pretrained.model_name_or_path",description:`<strong>model_name_or_path</strong> (<code>str</code>) &#x2014;
Repository name in the Hugging Face Hub or path to a local directory hosting the model.`,name:"model_name_or_path"},{anchor:"optimum.intel.neural_compressor.quantization.IncQuantizedModel.from_pretrained.inc_config",description:`<strong>inc_config</strong> (<code>Union[IncOptimizedConfig, str]</code>, <em>optional</em>) &#x2014;
Configuration file containing all the information related to the model quantization.
Can be either:<ul>
<li>an instance of the class <code>IncOptimizedConfig</code>,</li>
<li>a string valid as input to <code>IncOptimizedConfig.from_pretrained</code>.</li>
</ul>`,name:"inc_config"},{anchor:"optimum.intel.neural_compressor.quantization.IncQuantizedModel.from_pretrained.q_model_name",description:`<strong>q_model_name</strong> (<code>str</code>, <em>optional</em>) &#x2014;
Name of the state dictionary located in model_name_or_path used to load the quantized model. If
state_dict is specified, the latter will not be used.`,name:"q_model_name"},{anchor:"optimum.intel.neural_compressor.quantization.IncQuantizedModel.from_pretrained.input_names",description:`<strong>input_names</strong> (<code>List[str]</code>, <em>optional</em>) &#x2014;
List of names of the inputs used when tracing the model. If unset, model.dummy_inputs().keys() are used
instead.`,name:"input_names"},{anchor:"optimum.intel.neural_compressor.quantization.IncQuantizedModel.from_pretrained.batch_size",description:`<strong>batch_size</strong> (<code>int</code>, <em>optional</em>) &#x2014;
Batch size of the traced model inputs.`,name:"batch_size"},{anchor:"optimum.intel.neural_compressor.quantization.IncQuantizedModel.from_pretrained.sequence_length",description:`<strong>sequence_length</strong> (<code>Union[int, List[int], Tuple[int]]</code>, <em>optional</em>) &#x2014;
Sequence length of the traced model inputs. For sequence-to-sequence models with different sequence
lengths between the encoder and the decoder inputs, this must be <code>[encoder_sequence_length, decoder_sequence_length]</code>.`,name:"sequence_length"},{anchor:"optimum.intel.neural_compressor.quantization.IncQuantizedModel.from_pretrained.num_choices",description:`<strong>num_choices</strong> (<code>int</code>, <em>optional</em>, defaults to -1) &#x2014;
The number of possible choices for a multiple choice task.`,name:"num_choices"},{anchor:"optimum.intel.neural_compressor.quantization.IncQuantizedModel.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded configuration should be cached if the standard cache should
not be used.`,name:"cache_dir"},{anchor:"optimum.intel.neural_compressor.quantization.IncQuantizedModel.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force to (re-)download the configuration files and override the cached versions if
they exist.`,name:"force_download"},{anchor:"optimum.intel.neural_compressor.quantization.IncQuantizedModel.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received file. Attempts to resume the download if such a file
exists.`,name:"resume_download"},{anchor:"optimum.intel.neural_compressor.quantization.IncQuantizedModel.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"optimum.intel.neural_compressor.quantization.IncQuantizedModel.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<code>Dict[str, torch.Tensor]</code>, <em>optional</em>) &#x2014;
State dictionary of the quantized model, if not specified q_model_name will be used to load the
state dictionary.`,name:"state_dict"}],returnDescription:`
<p>Quantized model.</p>
`,returnType:`
<p>q_model</p>
`}}),{c(){d=o("meta"),W=s(),u=o("h1"),_=o("a"),T=o("span"),N(y.$$.fragment),ie=s(),D=o("span"),re=j("Quantization"),B=s(),p=o("h2"),v=o("a"),O=o("span"),N(w.$$.fragment),ce=s(),A=o("span"),se=j("IncQuantizer"),R=s(),h=o("div"),N($.$$.fragment),le=s(),z=o("div"),N(q.$$.fragment),me=s(),S=o("p"),de=j(`Instantiate a IncQuantizer object from a configuration file which can either be hosted on huggingface.co or
from a local directory path.`),J=s(),f=o("h2"),I=o("a"),U=o("span"),N(Q.$$.fragment),ue=s(),F=o("span"),pe=j("IncQuantizedModel"),G=s(),g=o("div"),N(x.$$.fragment),he=s(),b=o("div"),N(M.$$.fragment),fe=s(),H=o("p"),ge=j("Instantiate a quantized pytorch model from a given Intel Neural Compressor (INC) configuration file."),this.h()},l(e){const c=Le('[data-svelte="svelte-1phssyn"]',document.head);d=a(c,"META",{name:!0,content:!0}),c.forEach(n),W=l(e),u=a(e,"H1",{class:!0});var X=i(u);_=a(X,"A",{id:!0,class:!0,href:!0});var ze=i(_);T=a(ze,"SPAN",{});var Ie=i(T);C(y.$$.fragment,Ie),Ie.forEach(n),ze.forEach(n),ie=l(X),D=a(X,"SPAN",{});var be=i(D);re=V(be,"Quantization"),be.forEach(n),X.forEach(n),B=l(e),p=a(e,"H2",{class:!0});var Y=i(p);v=a(Y,"A",{id:!0,class:!0,href:!0});var ye=i(v);O=a(ye,"SPAN",{});var we=i(O);C(w.$$.fragment,we),we.forEach(n),ye.forEach(n),ce=l(Y),A=a(Y,"SPAN",{});var $e=i(A);se=V($e,"IncQuantizer"),$e.forEach(n),Y.forEach(n),R=l(e),h=a(e,"DIV",{class:!0});var Z=i(h);C($.$$.fragment,Z),le=l(Z),z=a(Z,"DIV",{class:!0});var ee=i(z);C(q.$$.fragment,ee),me=l(ee),S=a(ee,"P",{});var qe=i(S);de=V(qe,`Instantiate a IncQuantizer object from a configuration file which can either be hosted on huggingface.co or
from a local directory path.`),qe.forEach(n),ee.forEach(n),Z.forEach(n),J=l(e),f=a(e,"H2",{class:!0});var ne=i(f);I=a(ne,"A",{id:!0,class:!0,href:!0});var Qe=i(I);U=a(Qe,"SPAN",{});var xe=i(U);C(Q.$$.fragment,xe),xe.forEach(n),Qe.forEach(n),ue=l(ne),F=a(ne,"SPAN",{});var Me=i(F);pe=V(Me,"IncQuantizedModel"),Me.forEach(n),ne.forEach(n),G=l(e),g=a(e,"DIV",{class:!0});var te=i(g);C(x.$$.fragment,te),he=l(te),b=a(te,"DIV",{class:!0});var oe=i(b);C(M.$$.fragment,oe),fe=l(oe),H=a(oe,"P",{});var Ne=i(H);ge=V(Ne,"Instantiate a quantized pytorch model from a given Intel Neural Compressor (INC) configuration file."),Ne.forEach(n),oe.forEach(n),te.forEach(n),this.h()},h(){r(d,"name","hf:doc:metadata"),r(d,"content",JSON.stringify(Oe)),r(_,"id","quantization"),r(_,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(_,"href","#quantization"),r(u,"class","relative group"),r(v,"id","optimum.intel.IncQuantizer"),r(v,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(v,"href","#optimum.intel.IncQuantizer"),r(p,"class","relative group"),r(z,"class","docstring"),r(h,"class","docstring"),r(I,"id","optimum.intel.neural_compressor.quantization.IncQuantizedModel"),r(I,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(I,"href","#optimum.intel.neural_compressor.quantization.IncQuantizedModel"),r(f,"class","relative group"),r(b,"class","docstring"),r(g,"class","docstring")},m(e,c){t(document.head,d),m(e,W,c),m(e,u,c),t(u,_),t(_,T),E(y,T,null),t(u,ie),t(u,D),t(D,re),m(e,B,c),m(e,p,c),t(p,v),t(v,O),E(w,O,null),t(p,ce),t(p,A),t(A,se),m(e,R,c),m(e,h,c),E($,h,null),t(h,le),t(h,z),E(q,z,null),t(z,me),t(z,S),t(S,de),m(e,J,c),m(e,f,c),t(f,I),t(I,U),E(Q,U,null),t(f,ue),t(f,F),t(F,pe),m(e,G,c),m(e,g,c),E(x,g,null),t(g,he),t(g,b),E(M,b,null),t(b,fe),t(b,H),t(H,ge),K=!0},p:Pe,i(e){K||(k(y.$$.fragment,e),k(w.$$.fragment,e),k($.$$.fragment,e),k(q.$$.fragment,e),k(Q.$$.fragment,e),k(x.$$.fragment,e),k(M.$$.fragment,e),K=!0)},o(e){L(y.$$.fragment,e),L(w.$$.fragment,e),L($.$$.fragment,e),L(q.$$.fragment,e),L(Q.$$.fragment,e),L(x.$$.fragment,e),L(M.$$.fragment,e),K=!1},d(e){n(d),e&&n(W),e&&n(u),P(y),e&&n(B),e&&n(p),P(w),e&&n(R),e&&n(h),P($),P(q),e&&n(J),e&&n(f),P(Q),e&&n(G),e&&n(g),P(x),P(M)}}}const Oe={local:"quantization",sections:[{local:"optimum.intel.IncQuantizer",title:"IncQuantizer"},{local:"optimum.intel.neural_compressor.quantization.IncQuantizedModel",title:"IncQuantizedModel"}],title:"Quantization"};function Ae(ve){return Te(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class He extends Ce{constructor(d){super();Ee(this,d,Ae,De,ke,{})}}export{He as default,Oe as metadata};
