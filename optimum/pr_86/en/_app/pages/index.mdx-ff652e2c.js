import{S as wo,i as yo,s as Eo,e as a,k as h,w as te,t as n,M as $o,c as o,d as r,m as f,a as l,x as re,h as s,b as i,F as e,g as m,y as ae,L as bo,q as oe,o as le,B as ie}from"../chunks/vendor-19e06bd2.js";import{I as Bt}from"../chunks/IconCopyLink-3c713d38.js";import{C as ca}from"../chunks/CodeBlock-9dd1fdfb.js";function ko(Mt){let v,ne,c,d,Ge,H,Xt,We,Ft,ht,se,Gt,ft,he,Wt,pt,w,N,Ye,R,Yt,je,jt,mt,fe,Jt,ct,pe,Kt,dt,O,Q,q,Vt,Zt,L,er,tr,Je,rr,ut,y,P,Ke,U,ar,Ve,or,vt,me,lr,gt,g,ir,B,nr,sr,M,hr,fr,_t,D,Ze,E,ce,pr,mr,de,cr,dr,ue,ur,vr,u,$,ve,gr,_r,ge,wr,yr,_e,Er,$r,b,we,br,kr,ye,Tr,Ar,Ee,Ir,Nr,k,$e,Or,Pr,be,Dr,xr,ke,Cr,zr,T,Te,Sr,Hr,Ae,Rr,Qr,Ie,qr,wt,A,x,et,X,Lr,tt,Ur,yt,C,Br,rt,Mr,Xr,Et,F,$t,Ne,Fr,bt,z,at,G,Oe,Gr,Wr,Pe,Yr,jr,I,W,De,Y,Jr,Kr,xe,ot,Vr,Zr,j,Ce,J,ea,ta,ze,lt,ra,aa,K,Se,V,oa,la,He,it,ia,kt,Re,na,Tt,Z,At,_,sa,nt,ha,fa,st,pa,ma,It,ee,Nt;return H=new Bt({}),R=new Bt({}),U=new Bt({}),X=new Bt({}),F=new ca({props:{code:"python -m pip install optimum",highlighted:"python -m pip install optimum"}}),Z=new ca({props:{code:"python -m pip install git+https://github.com/huggingface/optimum.git",highlighted:"python -m pip install git+https://github.com/huggingface/optimum.git"}}),ee=new ca({props:{code:"python -m pip install git+https://github.com/huggingface/optimum.git#egg=optimum[onnxruntime]",highlighted:'python -m pip install git+https://github.com/huggingface/optimum.git<span class="hljs-comment">#egg=optimum[onnxruntime]</span>'}}),{c(){v=a("meta"),ne=h(),c=a("h1"),d=a("a"),Ge=a("span"),te(H.$$.fragment),Xt=h(),We=a("span"),Ft=n("\u{1F917} Optimum"),ht=h(),se=a("p"),Gt=n("\u{1F917} Optimum is an extension of \u{1F917} Transformers, providing a set of performance optimization tools enabling maximum efficiency to train and run models on targeted hardware."),ft=h(),he=a("p"),Wt=n(`The AI ecosystem evolves quickly and more and more specialized hardware along with their own optimizations are emerging every day.
As such, Optimum enables users to efficiently use any of these platforms with the same ease inherent to transformers.`),pt=h(),w=a("h2"),N=a("a"),Ye=a("span"),te(R.$$.fragment),Yt=h(),je=a("span"),jt=n("Integration with Hardware Partners"),mt=h(),fe=a("p"),Jt=n("\u{1F917} Optimum aims at providing more diversity towards the kind of hardware users can target to train and finetune their models."),ct=h(),pe=a("p"),Kt=n("To achieve this, we are collaborating with the following hardware manufacturers in order to provide the best transformers integration:"),dt=h(),O=a("ul"),Q=a("li"),q=a("a"),Vt=n("GraphCore IPUs"),Zt=n(" - IPUs are a completely new kind of massively parallel processor to accelerate machine intelligence. "),L=a("a"),er=n("More information here"),tr=h(),Je=a("li"),rr=n("More to come soon! :star:"),ut=h(),y=a("h2"),P=a("a"),Ke=a("span"),te(U.$$.fragment),ar=h(),Ve=a("span"),or=n("Optimizing models towards inference"),vt=h(),me=a("p"),lr=n(`Along with supporting dedicated AI hardware for training, Optimum also provides inference optimizations towards various frameworks and
platforms.`),gt=h(),g=a("p"),ir=n("We currently support "),B=a("a"),nr=n("ONNX runtime"),sr=n(" along with "),M=a("a"),hr=n("Intel Neural Compressor (INC)"),fr=n("."),_t=h(),D=a("table"),Ze=a("thead"),E=a("tr"),ce=a("th"),pr=n("Features"),mr=h(),de=a("th"),cr=n("ONNX Runtime"),dr=h(),ue=a("th"),ur=n("Intel Neural Compressor"),vr=h(),u=a("tbody"),$=a("tr"),ve=a("td"),gr=n("Post-training Dynamic Quantization"),_r=h(),ge=a("td"),wr=n(":heavy_check_mark:"),yr=h(),_e=a("td"),Er=n(":heavy_check_mark:"),$r=h(),b=a("tr"),we=a("td"),br=n("Post-training Static Quantization"),kr=h(),ye=a("td"),Tr=n(":heavy_check_mark:"),Ar=h(),Ee=a("td"),Ir=n(":heavy_check_mark:"),Nr=h(),k=a("tr"),$e=a("td"),Or=n("Quantization Aware Training (QAT)"),Pr=h(),be=a("td"),Dr=n("Stay tuned! :star:"),xr=h(),ke=a("td"),Cr=n(":heavy_check_mark:"),zr=h(),T=a("tr"),Te=a("td"),Sr=n("Pruning"),Hr=h(),Ae=a("td"),Rr=n("N/A"),Qr=h(),Ie=a("td"),qr=n(":heavy_check_mark:"),wt=h(),A=a("h2"),x=a("a"),et=a("span"),te(X.$$.fragment),Lr=h(),tt=a("span"),Ur=n("Installation"),yt=h(),C=a("p"),Br=n("\u{1F917} Optimum can be installed using "),rt=a("code"),Mr=n("pip"),Xr=n(" as follows:"),Et=h(),te(F.$$.fragment),$t=h(),Ne=a("p"),Fr=n("If you\u2019d like to use the accelerator-specific features of \u{1F917} Optimum, you can install the required dependencies according to the table below:"),bt=h(),z=a("table"),at=a("thead"),G=a("tr"),Oe=a("th"),Gr=n("Accelerator"),Wr=h(),Pe=a("th"),Yr=n("Installation"),jr=h(),I=a("tbody"),W=a("tr"),De=a("td"),Y=a("a"),Jr=n("ONNX runtime"),Kr=h(),xe=a("td"),ot=a("code"),Vr=n("python -m pip install optimum[onnxruntime]"),Zr=h(),j=a("tr"),Ce=a("td"),J=a("a"),ea=n("Intel Neural Compressor (INC)"),ta=h(),ze=a("td"),lt=a("code"),ra=n("python -m pip install optimum[intel]"),aa=h(),K=a("tr"),Se=a("td"),V=a("a"),oa=n("Graphcore IPU"),la=h(),He=a("td"),it=a("code"),ia=n("python -m pip install optimum[graphcore]"),kt=h(),Re=a("p"),na=n("If you\u2019d like to play with the examples or need the bleeding edge of the code and can\u2019t wait for a new release, you can install the base library from source as follows:"),Tt=h(),te(Z.$$.fragment),At=h(),_=a("p"),sa=n("For the acclerator-specific features, you can install them by appending "),nt=a("code"),ha=n("#egg=optimum[accelerator_type]"),fa=n(" to the "),st=a("code"),pa=n("pip"),ma=n(" command, e.g."),It=h(),te(ee.$$.fragment),this.h()},l(t){const p=$o('[data-svelte="svelte-1phssyn"]',document.head);v=o(p,"META",{name:!0,content:!0}),p.forEach(r),ne=f(t),c=o(t,"H1",{class:!0});var Ot=l(c);d=o(Ot,"A",{id:!0,class:!0,href:!0});var da=l(d);Ge=o(da,"SPAN",{});var ua=l(Ge);re(H.$$.fragment,ua),ua.forEach(r),da.forEach(r),Xt=f(Ot),We=o(Ot,"SPAN",{});var va=l(We);Ft=s(va,"\u{1F917} Optimum"),va.forEach(r),Ot.forEach(r),ht=f(t),se=o(t,"P",{});var ga=l(se);Gt=s(ga,"\u{1F917} Optimum is an extension of \u{1F917} Transformers, providing a set of performance optimization tools enabling maximum efficiency to train and run models on targeted hardware."),ga.forEach(r),ft=f(t),he=o(t,"P",{});var _a=l(he);Wt=s(_a,`The AI ecosystem evolves quickly and more and more specialized hardware along with their own optimizations are emerging every day.
As such, Optimum enables users to efficiently use any of these platforms with the same ease inherent to transformers.`),_a.forEach(r),pt=f(t),w=o(t,"H2",{class:!0});var Pt=l(w);N=o(Pt,"A",{id:!0,class:!0,href:!0});var wa=l(N);Ye=o(wa,"SPAN",{});var ya=l(Ye);re(R.$$.fragment,ya),ya.forEach(r),wa.forEach(r),Yt=f(Pt),je=o(Pt,"SPAN",{});var Ea=l(je);jt=s(Ea,"Integration with Hardware Partners"),Ea.forEach(r),Pt.forEach(r),mt=f(t),fe=o(t,"P",{});var $a=l(fe);Jt=s($a,"\u{1F917} Optimum aims at providing more diversity towards the kind of hardware users can target to train and finetune their models."),$a.forEach(r),ct=f(t),pe=o(t,"P",{});var ba=l(pe);Kt=s(ba,"To achieve this, we are collaborating with the following hardware manufacturers in order to provide the best transformers integration:"),ba.forEach(r),dt=f(t),O=o(t,"UL",{});var Dt=l(O);Q=o(Dt,"LI",{});var xt=l(Q);q=o(xt,"A",{href:!0,rel:!0});var ka=l(q);Vt=s(ka,"GraphCore IPUs"),ka.forEach(r),Zt=s(xt," - IPUs are a completely new kind of massively parallel processor to accelerate machine intelligence. "),L=o(xt,"A",{href:!0,rel:!0});var Ta=l(L);er=s(Ta,"More information here"),Ta.forEach(r),xt.forEach(r),tr=f(Dt),Je=o(Dt,"LI",{});var Aa=l(Je);rr=s(Aa,"More to come soon! :star:"),Aa.forEach(r),Dt.forEach(r),ut=f(t),y=o(t,"H2",{class:!0});var Ct=l(y);P=o(Ct,"A",{id:!0,class:!0,href:!0});var Ia=l(P);Ke=o(Ia,"SPAN",{});var Na=l(Ke);re(U.$$.fragment,Na),Na.forEach(r),Ia.forEach(r),ar=f(Ct),Ve=o(Ct,"SPAN",{});var Oa=l(Ve);or=s(Oa,"Optimizing models towards inference"),Oa.forEach(r),Ct.forEach(r),vt=f(t),me=o(t,"P",{});var Pa=l(me);lr=s(Pa,`Along with supporting dedicated AI hardware for training, Optimum also provides inference optimizations towards various frameworks and
platforms.`),Pa.forEach(r),gt=f(t),g=o(t,"P",{});var Qe=l(g);ir=s(Qe,"We currently support "),B=o(Qe,"A",{href:!0,rel:!0});var Da=l(B);nr=s(Da,"ONNX runtime"),Da.forEach(r),sr=s(Qe," along with "),M=o(Qe,"A",{href:!0,rel:!0});var xa=l(M);hr=s(xa,"Intel Neural Compressor (INC)"),xa.forEach(r),fr=s(Qe,"."),Qe.forEach(r),_t=f(t),D=o(t,"TABLE",{});var zt=l(D);Ze=o(zt,"THEAD",{});var Ca=l(Ze);E=o(Ca,"TR",{});var qe=l(E);ce=o(qe,"TH",{align:!0});var za=l(ce);pr=s(za,"Features"),za.forEach(r),mr=f(qe),de=o(qe,"TH",{align:!0});var Sa=l(de);cr=s(Sa,"ONNX Runtime"),Sa.forEach(r),dr=f(qe),ue=o(qe,"TH",{align:!0});var Ha=l(ue);ur=s(Ha,"Intel Neural Compressor"),Ha.forEach(r),qe.forEach(r),Ca.forEach(r),vr=f(zt),u=o(zt,"TBODY",{});var S=l(u);$=o(S,"TR",{});var Le=l($);ve=o(Le,"TD",{align:!0});var Ra=l(ve);gr=s(Ra,"Post-training Dynamic Quantization"),Ra.forEach(r),_r=f(Le),ge=o(Le,"TD",{align:!0});var Qa=l(ge);wr=s(Qa,":heavy_check_mark:"),Qa.forEach(r),yr=f(Le),_e=o(Le,"TD",{align:!0});var qa=l(_e);Er=s(qa,":heavy_check_mark:"),qa.forEach(r),Le.forEach(r),$r=f(S),b=o(S,"TR",{});var Ue=l(b);we=o(Ue,"TD",{align:!0});var La=l(we);br=s(La,"Post-training Static Quantization"),La.forEach(r),kr=f(Ue),ye=o(Ue,"TD",{align:!0});var Ua=l(ye);Tr=s(Ua,":heavy_check_mark:"),Ua.forEach(r),Ar=f(Ue),Ee=o(Ue,"TD",{align:!0});var Ba=l(Ee);Ir=s(Ba,":heavy_check_mark:"),Ba.forEach(r),Ue.forEach(r),Nr=f(S),k=o(S,"TR",{});var Be=l(k);$e=o(Be,"TD",{align:!0});var Ma=l($e);Or=s(Ma,"Quantization Aware Training (QAT)"),Ma.forEach(r),Pr=f(Be),be=o(Be,"TD",{align:!0});var Xa=l(be);Dr=s(Xa,"Stay tuned! :star:"),Xa.forEach(r),xr=f(Be),ke=o(Be,"TD",{align:!0});var Fa=l(ke);Cr=s(Fa,":heavy_check_mark:"),Fa.forEach(r),Be.forEach(r),zr=f(S),T=o(S,"TR",{});var Me=l(T);Te=o(Me,"TD",{align:!0});var Ga=l(Te);Sr=s(Ga,"Pruning"),Ga.forEach(r),Hr=f(Me),Ae=o(Me,"TD",{align:!0});var Wa=l(Ae);Rr=s(Wa,"N/A"),Wa.forEach(r),Qr=f(Me),Ie=o(Me,"TD",{align:!0});var Ya=l(Ie);qr=s(Ya,":heavy_check_mark:"),Ya.forEach(r),Me.forEach(r),S.forEach(r),zt.forEach(r),wt=f(t),A=o(t,"H2",{class:!0});var St=l(A);x=o(St,"A",{id:!0,class:!0,href:!0});var ja=l(x);et=o(ja,"SPAN",{});var Ja=l(et);re(X.$$.fragment,Ja),Ja.forEach(r),ja.forEach(r),Lr=f(St),tt=o(St,"SPAN",{});var Ka=l(tt);Ur=s(Ka,"Installation"),Ka.forEach(r),St.forEach(r),yt=f(t),C=o(t,"P",{});var Ht=l(C);Br=s(Ht,"\u{1F917} Optimum can be installed using "),rt=o(Ht,"CODE",{});var Va=l(rt);Mr=s(Va,"pip"),Va.forEach(r),Xr=s(Ht," as follows:"),Ht.forEach(r),Et=f(t),re(F.$$.fragment,t),$t=f(t),Ne=o(t,"P",{});var Za=l(Ne);Fr=s(Za,"If you\u2019d like to use the accelerator-specific features of \u{1F917} Optimum, you can install the required dependencies according to the table below:"),Za.forEach(r),bt=f(t),z=o(t,"TABLE",{});var Rt=l(z);at=o(Rt,"THEAD",{});var eo=l(at);G=o(eo,"TR",{});var Qt=l(G);Oe=o(Qt,"TH",{align:!0});var to=l(Oe);Gr=s(to,"Accelerator"),to.forEach(r),Wr=f(Qt),Pe=o(Qt,"TH",{align:!0});var ro=l(Pe);Yr=s(ro,"Installation"),ro.forEach(r),Qt.forEach(r),eo.forEach(r),jr=f(Rt),I=o(Rt,"TBODY",{});var Xe=l(I);W=o(Xe,"TR",{});var qt=l(W);De=o(qt,"TD",{align:!0});var ao=l(De);Y=o(ao,"A",{href:!0,rel:!0});var oo=l(Y);Jr=s(oo,"ONNX runtime"),oo.forEach(r),ao.forEach(r),Kr=f(qt),xe=o(qt,"TD",{align:!0});var lo=l(xe);ot=o(lo,"CODE",{});var io=l(ot);Vr=s(io,"python -m pip install optimum[onnxruntime]"),io.forEach(r),lo.forEach(r),qt.forEach(r),Zr=f(Xe),j=o(Xe,"TR",{});var Lt=l(j);Ce=o(Lt,"TD",{align:!0});var no=l(Ce);J=o(no,"A",{href:!0,rel:!0});var so=l(J);ea=s(so,"Intel Neural Compressor (INC)"),so.forEach(r),no.forEach(r),ta=f(Lt),ze=o(Lt,"TD",{align:!0});var ho=l(ze);lt=o(ho,"CODE",{});var fo=l(lt);ra=s(fo,"python -m pip install optimum[intel]"),fo.forEach(r),ho.forEach(r),Lt.forEach(r),aa=f(Xe),K=o(Xe,"TR",{});var Ut=l(K);Se=o(Ut,"TD",{align:!0});var po=l(Se);V=o(po,"A",{href:!0,rel:!0});var mo=l(V);oa=s(mo,"Graphcore IPU"),mo.forEach(r),po.forEach(r),la=f(Ut),He=o(Ut,"TD",{align:!0});var co=l(He);it=o(co,"CODE",{});var uo=l(it);ia=s(uo,"python -m pip install optimum[graphcore]"),uo.forEach(r),co.forEach(r),Ut.forEach(r),Xe.forEach(r),Rt.forEach(r),kt=f(t),Re=o(t,"P",{});var vo=l(Re);na=s(vo,"If you\u2019d like to play with the examples or need the bleeding edge of the code and can\u2019t wait for a new release, you can install the base library from source as follows:"),vo.forEach(r),Tt=f(t),re(Z.$$.fragment,t),At=f(t),_=o(t,"P",{});var Fe=l(_);sa=s(Fe,"For the acclerator-specific features, you can install them by appending "),nt=o(Fe,"CODE",{});var go=l(nt);ha=s(go,"#egg=optimum[accelerator_type]"),go.forEach(r),fa=s(Fe," to the "),st=o(Fe,"CODE",{});var _o=l(st);pa=s(_o,"pip"),_o.forEach(r),ma=s(Fe," command, e.g."),Fe.forEach(r),It=f(t),re(ee.$$.fragment,t),this.h()},h(){i(v,"name","hf:doc:metadata"),i(v,"content",JSON.stringify(To)),i(d,"id","optimum"),i(d,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(d,"href","#optimum"),i(c,"class","relative group"),i(N,"id","integration-with-hardware-partners"),i(N,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(N,"href","#integration-with-hardware-partners"),i(w,"class","relative group"),i(q,"href","https://github.com/huggingface/optimum-graphcore"),i(q,"rel","nofollow"),i(L,"href","https://www.graphcore.ai/products/ipu"),i(L,"rel","nofollow"),i(P,"id","optimizing-models-towards-inference"),i(P,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(P,"href","#optimizing-models-towards-inference"),i(y,"class","relative group"),i(B,"href","https://github.com/microsoft/onnxruntime"),i(B,"rel","nofollow"),i(M,"href","https://github.com/intel/neural-compressor"),i(M,"rel","nofollow"),i(ce,"align","center"),i(de,"align","center"),i(ue,"align","center"),i(ve,"align","center"),i(ge,"align","center"),i(_e,"align","center"),i(we,"align","center"),i(ye,"align","center"),i(Ee,"align","center"),i($e,"align","center"),i(be,"align","center"),i(ke,"align","center"),i(Te,"align","center"),i(Ae,"align","center"),i(Ie,"align","center"),i(x,"id","installation"),i(x,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(x,"href","#installation"),i(A,"class","relative group"),i(Oe,"align","left"),i(Pe,"align","left"),i(Y,"href","https://github.com/microsoft/onnxruntime"),i(Y,"rel","nofollow"),i(De,"align","left"),i(xe,"align","left"),i(J,"href","https://github.com/intel/neural-compressor"),i(J,"rel","nofollow"),i(Ce,"align","left"),i(ze,"align","left"),i(V,"href","https://www.graphcore.ai/products/ipu"),i(V,"rel","nofollow"),i(Se,"align","left"),i(He,"align","left")},m(t,p){e(document.head,v),m(t,ne,p),m(t,c,p),e(c,d),e(d,Ge),ae(H,Ge,null),e(c,Xt),e(c,We),e(We,Ft),m(t,ht,p),m(t,se,p),e(se,Gt),m(t,ft,p),m(t,he,p),e(he,Wt),m(t,pt,p),m(t,w,p),e(w,N),e(N,Ye),ae(R,Ye,null),e(w,Yt),e(w,je),e(je,jt),m(t,mt,p),m(t,fe,p),e(fe,Jt),m(t,ct,p),m(t,pe,p),e(pe,Kt),m(t,dt,p),m(t,O,p),e(O,Q),e(Q,q),e(q,Vt),e(Q,Zt),e(Q,L),e(L,er),e(O,tr),e(O,Je),e(Je,rr),m(t,ut,p),m(t,y,p),e(y,P),e(P,Ke),ae(U,Ke,null),e(y,ar),e(y,Ve),e(Ve,or),m(t,vt,p),m(t,me,p),e(me,lr),m(t,gt,p),m(t,g,p),e(g,ir),e(g,B),e(B,nr),e(g,sr),e(g,M),e(M,hr),e(g,fr),m(t,_t,p),m(t,D,p),e(D,Ze),e(Ze,E),e(E,ce),e(ce,pr),e(E,mr),e(E,de),e(de,cr),e(E,dr),e(E,ue),e(ue,ur),e(D,vr),e(D,u),e(u,$),e($,ve),e(ve,gr),e($,_r),e($,ge),e(ge,wr),e($,yr),e($,_e),e(_e,Er),e(u,$r),e(u,b),e(b,we),e(we,br),e(b,kr),e(b,ye),e(ye,Tr),e(b,Ar),e(b,Ee),e(Ee,Ir),e(u,Nr),e(u,k),e(k,$e),e($e,Or),e(k,Pr),e(k,be),e(be,Dr),e(k,xr),e(k,ke),e(ke,Cr),e(u,zr),e(u,T),e(T,Te),e(Te,Sr),e(T,Hr),e(T,Ae),e(Ae,Rr),e(T,Qr),e(T,Ie),e(Ie,qr),m(t,wt,p),m(t,A,p),e(A,x),e(x,et),ae(X,et,null),e(A,Lr),e(A,tt),e(tt,Ur),m(t,yt,p),m(t,C,p),e(C,Br),e(C,rt),e(rt,Mr),e(C,Xr),m(t,Et,p),ae(F,t,p),m(t,$t,p),m(t,Ne,p),e(Ne,Fr),m(t,bt,p),m(t,z,p),e(z,at),e(at,G),e(G,Oe),e(Oe,Gr),e(G,Wr),e(G,Pe),e(Pe,Yr),e(z,jr),e(z,I),e(I,W),e(W,De),e(De,Y),e(Y,Jr),e(W,Kr),e(W,xe),e(xe,ot),e(ot,Vr),e(I,Zr),e(I,j),e(j,Ce),e(Ce,J),e(J,ea),e(j,ta),e(j,ze),e(ze,lt),e(lt,ra),e(I,aa),e(I,K),e(K,Se),e(Se,V),e(V,oa),e(K,la),e(K,He),e(He,it),e(it,ia),m(t,kt,p),m(t,Re,p),e(Re,na),m(t,Tt,p),ae(Z,t,p),m(t,At,p),m(t,_,p),e(_,sa),e(_,nt),e(nt,ha),e(_,fa),e(_,st),e(st,pa),e(_,ma),m(t,It,p),ae(ee,t,p),Nt=!0},p:bo,i(t){Nt||(oe(H.$$.fragment,t),oe(R.$$.fragment,t),oe(U.$$.fragment,t),oe(X.$$.fragment,t),oe(F.$$.fragment,t),oe(Z.$$.fragment,t),oe(ee.$$.fragment,t),Nt=!0)},o(t){le(H.$$.fragment,t),le(R.$$.fragment,t),le(U.$$.fragment,t),le(X.$$.fragment,t),le(F.$$.fragment,t),le(Z.$$.fragment,t),le(ee.$$.fragment,t),Nt=!1},d(t){r(v),t&&r(ne),t&&r(c),ie(H),t&&r(ht),t&&r(se),t&&r(ft),t&&r(he),t&&r(pt),t&&r(w),ie(R),t&&r(mt),t&&r(fe),t&&r(ct),t&&r(pe),t&&r(dt),t&&r(O),t&&r(ut),t&&r(y),ie(U),t&&r(vt),t&&r(me),t&&r(gt),t&&r(g),t&&r(_t),t&&r(D),t&&r(wt),t&&r(A),ie(X),t&&r(yt),t&&r(C),t&&r(Et),ie(F,t),t&&r($t),t&&r(Ne),t&&r(bt),t&&r(z),t&&r(kt),t&&r(Re),t&&r(Tt),ie(Z,t),t&&r(At),t&&r(_),t&&r(It),ie(ee,t)}}}const To={local:"optimum",sections:[{local:"integration-with-hardware-partners",title:"Integration with Hardware Partners"},{local:"optimizing-models-towards-inference",title:"Optimizing models towards inference"},{local:"installation",title:"Installation"}],title:"\u{1F917} Optimum"};function Ao(Mt,v,ne){let{fw:c}=v;return Mt.$$set=d=>{"fw"in d&&ne(0,c=d.fw)},[c]}class Po extends wo{constructor(v){super();yo(this,v,Ao,ko,Eo,{fw:0})}}export{Po as default,To as metadata};
