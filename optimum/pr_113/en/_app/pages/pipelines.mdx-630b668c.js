import{S as Qa,i as Ia,s as Sa,e as n,k as m,w as $,t as s,M as Ha,c as r,d as o,m as u,a as i,x as O,h as a,b as d,F as t,g as p,y as T,q as b,o as x,B as j,v as La}from"../chunks/vendor-4918fc3c.js";import{T as Wa}from"../chunks/Tip-3d800dd6.js";import{I as ye}from"../chunks/IconCopyLink-21d338b1.js";import{C as Ee}from"../chunks/CodeBlock-99419108.js";function Ua(vt){let c,W,k,z,D,y,Z,R;return{c(){c=n("p"),W=s("You can also use the "),k=n("code"),z=s("transformers.pipeline"),D=s(" and provide your "),y=n("code"),Z=s("OptimumModel"),R=s(".")},l(N){c=r(N,"P",{});var E=i(c);W=a(E,"You can also use the "),k=r(E,"CODE",{});var w=i(k);z=a(w,"transformers.pipeline"),w.forEach(o),D=a(E," and provide your "),y=r(E,"CODE",{});var ve=i(y);Z=a(ve,"OptimumModel"),ve.forEach(o),R=a(E,"."),E.forEach(o)},m(N,E){p(N,c,E),t(c,W),t(c,k),t(k,z),t(c,D),t(c,y),t(y,Z),t(c,R)},d(N){N&&o(c)}}}function Ba(vt){let c,W,k,z,D,y,Z,R,N,E,w,ve,$e,Jt,Kt,ee,Vt,Xt,Oe,Zt,eo,kt,U,wt,ke,to,qt,we,Te,oo,yt,q,be,xe,so,ao,je,ze,no,ro,Ae,Me,io,lo,Fe,Ce,po,mo,Pe,De,uo,Et,Q,B,Re,te,co,Ne,fo,$t,A,ho,Qe,go,_o,Ie,vo,ko,Se,wo,qo,Ot,qe,oe,yo,He,Eo,$o,Tt,se,bt,ae,ne,Oo,Le,To,bo,xt,re,jt,I,Y,We,ie,xo,Ue,jo,zt,h,zo,Be,Ao,Mo,le,Fo,Co,Ye,Po,Do,Ge,Ro,No,Je,Qo,Io,At,pe,Mt,S,G,Ke,me,So,Ve,Ho,Ft,g,Lo,Xe,Wo,Uo,ue,Bo,Yo,Ze,Go,Jo,et,Ko,Vo,tt,Xo,Zo,Ct,de,Pt,H,J,ot,ce,es,st,ts,Dt,f,os,at,ss,as,nt,ns,rs,rt,is,ls,it,ps,ms,lt,us,ds,pt,cs,fs,mt,hs,gs,Rt,fe,Nt,L,K,ut,he,_s,dt,vs,Qt,_,ks,ct,ws,qs,ft,ys,Es,ht,$s,Os,gt,Ts,bs,_t,xs,js,It,ge,St;return y=new ye({}),U=new Wa({props:{$$slots:{default:[Ua]},$$scope:{ctx:vt}}}),te=new ye({}),se=new Ee({props:{code:`from optimum import optimum_pipeline

classifier = optimum_pipeline(task="text-classification", accelerator="onnx")
`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> optimum <span class="hljs-keyword">import</span> optimum_pipeline

<span class="hljs-meta">&gt;&gt;&gt; </span>classifier = optimum_pipeline(task=<span class="hljs-string">&quot;text-classification&quot;</span>, accelerator=<span class="hljs-string">&quot;onnx&quot;</span>)
`}}),re=new Ee({props:{code:'classifier("I like you. I love you.")',highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>classifier(<span class="hljs-string">&quot;I like you. I love you.&quot;</span>)'}}),ie=new ye({}),pe=new Ee({props:{code:`from transformers import AutoTokenizer
from optimum.onnxruntime import ORTModelForQuestionAnswering
from optimum import optimum_pipeline

tokenizer = AutoTokenizer.from_pretrained("deepset/roberta-base-squad2")
model = ORTModelForQuestionAnswering.from_transformers("deepset/roberta-base-squad2")

onnx_qa = optimum_pipeline("question-answering", model=model, tokenizer=tokenizer)
question = "Whats my name?"
context = "My Name is Philipp and I live in Nuremberg."

pred = onnx_qa(question=question, context=context)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> optimum.onnxruntime <span class="hljs-keyword">import</span> ORTModelForQuestionAnswering
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> optimum <span class="hljs-keyword">import</span> optimum_pipeline

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;deepset/roberta-base-squad2&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = ORTModelForQuestionAnswering.from_transformers(<span class="hljs-string">&quot;deepset/roberta-base-squad2&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>onnx_qa = optimum_pipeline(<span class="hljs-string">&quot;question-answering&quot;</span>, model=model, tokenizer=tokenizer)
<span class="hljs-meta">&gt;&gt;&gt; </span>question = <span class="hljs-string">&quot;Whats my name?&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>context = <span class="hljs-string">&quot;My Name is Philipp and I live in Nuremberg.&quot;</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>pred = onnx_qa(question=question, context=context)`}}),me=new ye({}),de=new Ee({props:{code:`from transformers import AutoTokenizer
from optimum.onnxruntime import ORTModelForQuestionAnswering
from optimum import optimum_pipeline

tokenizer = AutoTokenizer.from_pretrained("optimum/roberta-base-squad2")
model = ORTModelForQuestionAnswering.from_pretrained("optimum/roberta-base-squad2")

onnx_qa = optimum_pipeline("question-answering", model=model, tokenizer=tokenizer)
question = "Whats my name?"
context = "My Name is Philipp and I live in Nuremberg."

pred = onnx_qa(question=question, context=context)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> optimum.onnxruntime <span class="hljs-keyword">import</span> ORTModelForQuestionAnswering
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> optimum <span class="hljs-keyword">import</span> optimum_pipeline

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;optimum/roberta-base-squad2&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = ORTModelForQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;optimum/roberta-base-squad2&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>onnx_qa = optimum_pipeline(<span class="hljs-string">&quot;question-answering&quot;</span>, model=model, tokenizer=tokenizer)
<span class="hljs-meta">&gt;&gt;&gt; </span>question = <span class="hljs-string">&quot;Whats my name?&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>context = <span class="hljs-string">&quot;My Name is Philipp and I live in Nuremberg.&quot;</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>pred = onnx_qa(question=question, context=context)`}}),ce=new ye({}),fe=new Ee({props:{code:`from transformers import AutoTokenizer
from optimum.onnxruntime import ORTModelForQuestionAnswering
from optimum import optimum_pipeline

tokenizer = AutoTokenizer.from_pretrained("optimum/roberta-base-squad2")
model = ORTModelForQuestionAnswering.from_pretrained("optimum/roberta-base-squad2")

model.optimize()
model.quantize()

onnx_qa = optimum_pipeline("question-answering",
question = "Whats my name?"
context = "My Name is Philipp and I live in Nuremberg."

pred = onnx_qa(question=question, context=context)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> optimum.onnxruntime <span class="hljs-keyword">import</span> ORTModelForQuestionAnswering
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> optimum <span class="hljs-keyword">import</span> optimum_pipeline

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;optimum/roberta-base-squad2&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = ORTModelForQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;optimum/roberta-base-squad2&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>model.optimize()
<span class="hljs-meta">&gt;&gt;&gt; </span>model.quantize()

<span class="hljs-meta">&gt;&gt;&gt; </span>onnx_qa = optimum_pipeline(<span class="hljs-string">&quot;question-answering&quot;</span>,
                               model=model,
                               tokenizer=tokenizer, 
                               do_optimization=<span class="hljs-literal">True</span>, 
                               do_quantization=<span class="hljs-literal">True</span>
                               )
<span class="hljs-meta">&gt;&gt;&gt; </span>question = <span class="hljs-string">&quot;Whats my name?&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>context = <span class="hljs-string">&quot;My Name is Philipp and I live in Nuremberg.&quot;</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>pred = onnx_qa(question=question, context=context)`}}),he=new ye({}),ge=new Ee({props:{code:`from transformers import AutoTokenizer, pipeline
-from transformers import AutoModelForQuestionAnswering
+from optimum.onnxruntime import ORTModelForQuestionAnswering

-model = AutoModelForQuestionAnswering.from_pretrained("deepset/roberta-base-squad2")
+model = ORTModelForQuestionAnswering.from_transformers("deepset/roberta-base-squad2")
tokenizer = AutoTokenizer.from_pretrained("deepset/roberta-base-squad2")

onnx_qa = pipeline("question-answering",model=model,tokenizer=tokenizer)

question = "Whats my name?"
context = "My Name is Philipp and I live in Nuremberg."
pred = onnx_qa(question, context)`,highlighted:`from transformers import AutoTokenizer, pipeline
<span class="hljs-deletion">-from transformers import AutoModelForQuestionAnswering</span>
<span class="hljs-addition">+from optimum.onnxruntime import ORTModelForQuestionAnswering</span>

<span class="hljs-deletion">-model = AutoModelForQuestionAnswering.from_pretrained(&quot;deepset/roberta-base-squad2&quot;)</span>
<span class="hljs-addition">+model = ORTModelForQuestionAnswering.from_transformers(&quot;deepset/roberta-base-squad2&quot;)</span>
tokenizer = AutoTokenizer.from_pretrained(&quot;deepset/roberta-base-squad2&quot;)

onnx_qa = pipeline(&quot;question-answering&quot;,model=model,tokenizer=tokenizer)

question = &quot;Whats my name?&quot;
context = &quot;My Name is Philipp and I live in Nuremberg.&quot;
pred = onnx_qa(question, context)`}}),{c(){c=n("meta"),W=m(),k=n("h1"),z=n("a"),D=n("span"),$(y.$$.fragment),Z=m(),R=n("span"),N=s("Optimum pipelines for inference"),E=m(),w=n("p"),ve=s("The "),$e=n("code"),Jt=s("optimum_pipeline"),Kt=s(" makes it simple to use models from the "),ee=n("a"),Vt=s("Model Hub"),Xt=s(` for accelerated inference on a variety of tasks such as text classification.
Even if you don\u2019t have experience with a specific modality or understand the code powering the models, you can still use them with the `),Oe=n("code"),Zt=s("optimum_pipeline"),eo=s("! This tutorial will teach you to:"),kt=m(),$(U.$$.fragment),wt=m(),ke=n("p"),to=s("Currenlty supported tasks are:"),qt=m(),we=n("p"),Te=n("strong"),oo=s("Onnxruntime"),yt=m(),q=n("ul"),be=n("li"),xe=n("code"),so=s("feature-extraction"),ao=m(),je=n("li"),ze=n("code"),no=s("text-classification"),ro=m(),Ae=n("li"),Me=n("code"),io=s("token-classification"),lo=m(),Fe=n("li"),Ce=n("code"),po=s("question-answering"),mo=m(),Pe=n("li"),De=n("code"),uo=s("zero-shot-classification"),Et=m(),Q=n("h2"),B=n("a"),Re=n("span"),$(te.$$.fragment),co=m(),Ne=n("span"),fo=s("Optimum pipeline usage"),$t=m(),A=n("p"),ho=s("While each task has an associated "),Qe=n("code"),go=s("optimum_pipeline"),_o=s(", it is simpler to use the general "),Ie=n("code"),vo=s("pipeline"),ko=s(` abstraction which contains all the specific task pipelines.
The `),Se=n("code"),wo=s("optimum_pipeline"),qo=s(" automatically loads a default model and tokenizer capable of inference for your task."),Ot=m(),qe=n("ol"),oe=n("li"),yo=s("Start by creating a "),He=n("code"),Eo=s("optimum_pipeline"),$o=s(" and specify an inference task:"),Tt=m(),$(se.$$.fragment),bt=m(),ae=n("ol"),ne=n("li"),Oo=s("Pass your input text to the "),Le=n("code"),To=s("optimum_pipeline"),bo=s(":"),xt=m(),$(re.$$.fragment),jt=m(),I=n("h3"),Y=n("a"),We=n("span"),$(ie.$$.fragment),xo=m(),Ue=n("span"),jo=s("Use a vanilla transformers model and convert"),zt=m(),h=n("p"),zo=s("The "),Be=n("code"),Ao=s("optimum_pipeline"),Mo=s(" accepts supported model from the "),le=n("a"),Fo=s("Model Hub"),Co=s(`.
There are tags on the Model Hub that allow you to filter for a model you\u2019d like to use for your task.
Once you\u2019ve picked an appropriate model, load it with the `),Ye=n("code"),Po=s("from_transformers()"),Do=s(" method for corresponding "),Ge=n("code"),Ro=s("ORTModelFor*"),No=s("\nand [`AutoTokenizer\u2019] class. For example, load the "),Je=n("code"),Qo=s("ORTModelForQuestionAnswering"),Io=s(" class for a question answering modeling task:"),At=m(),$(pe.$$.fragment),Mt=m(),S=n("h3"),G=n("a"),Ke=n("span"),$(me.$$.fragment),So=m(),Ve=n("span"),Ho=s("Use optimum model"),Ft=m(),g=n("p"),Lo=s("The "),Xe=n("code"),Wo=s("optimum_pipeline"),Uo=s(" is tightly integrated with "),ue=n("a"),Bo=s("Model Hub"),Yo=s(` and can load optimized models directly, e.g. Onnxruntime.
There are tags on the Model Hub that allow you to filter for a model you\u2019d like to use for your task.
Once you\u2019ve picked an appropriate model, load it with the `),Ze=n("code"),Go=s(".from_pretrained"),Jo=s(" method for corresponding "),et=n("code"),Ko=s("ORTModelFor*"),Vo=s("\nand [`AutoTokenizer\u2019] class. For example, load the "),tt=n("code"),Xo=s("ORTModelForQuestionAnswering"),Zo=s(" class for a question answering modeling task:"),Ct=m(),$(de.$$.fragment),Pt=m(),H=n("h3"),J=n("a"),ot=n("span"),$(ce.$$.fragment),es=m(),st=n("span"),ts=s("Optimizing and Quantizing in Pipelines"),Dt=m(),f=n("p"),os=s("The "),at=n("code"),ss=s("optimum_pipeline"),as=s(` can not only run inference it also provides arguments to quantize and optimize your model on the fly.
Once you\u2019ve picked an appropriate model, load it with the `),nt=n("code"),ns=s(".from_transformers"),rs=s(" or "),rt=n("code"),is=s(".from_pretrained"),ls=s(" method for corresponding "),it=n("code"),ps=s("ORTModelFor*"),ms=s("\nand [`AutoTokenizer\u2019] class. For example, load the "),lt=n("code"),us=s("ORTModelForQuestionAnswering"),ds=s(` class for a question answering modeling task and provide
the `),pt=n("code"),cs=s("do_optimization=True"),fs=s(" and/or "),mt=n("code"),hs=s("do_quantization=True"),gs=s(" arguments:"),Rt=m(),$(fe.$$.fragment),Nt=m(),L=n("h2"),K=n("a"),ut=n("span"),$(he.$$.fragment),_s=m(),dt=n("span"),vs=s("Transformers pipeline usage"),Qt=m(),_=n("p"),ks=s("The "),ct=n("code"),ws=s("optimum_pipeline"),qs=s(" is just a light wrapper around the "),ft=n("code"),ys=s("transformers.pipeline"),Es=s(` to enable checks for supported tasks and additional features
, like quantization and optimization. This being said you can use the `),ht=n("code"),$s=s("transformers.pipeline"),Os=s(" and just replace your "),gt=n("code"),Ts=s("AutoFor*"),bs=s(` with the optimum
`),_t=n("code"),xs=s("ORTModelFor*"),js=s(" class."),It=m(),$(ge.$$.fragment),this.h()},l(e){const l=Ha('[data-svelte="svelte-1phssyn"]',document.head);c=r(l,"META",{name:!0,content:!0}),l.forEach(o),W=u(e),k=r(e,"H1",{class:!0});var _e=i(k);z=r(_e,"A",{id:!0,class:!0,href:!0});var zs=i(z);D=r(zs,"SPAN",{});var As=i(D);O(y.$$.fragment,As),As.forEach(o),zs.forEach(o),Z=u(_e),R=r(_e,"SPAN",{});var Ms=i(R);N=a(Ms,"Optimum pipelines for inference"),Ms.forEach(o),_e.forEach(o),E=u(e),w=r(e,"P",{});var V=i(w);ve=a(V,"The "),$e=r(V,"CODE",{});var Fs=i($e);Jt=a(Fs,"optimum_pipeline"),Fs.forEach(o),Kt=a(V," makes it simple to use models from the "),ee=r(V,"A",{href:!0,rel:!0});var Cs=i(ee);Vt=a(Cs,"Model Hub"),Cs.forEach(o),Xt=a(V,` for accelerated inference on a variety of tasks such as text classification.
Even if you don\u2019t have experience with a specific modality or understand the code powering the models, you can still use them with the `),Oe=r(V,"CODE",{});var Ps=i(Oe);Zt=a(Ps,"optimum_pipeline"),Ps.forEach(o),eo=a(V,"! This tutorial will teach you to:"),V.forEach(o),kt=u(e),O(U.$$.fragment,e),wt=u(e),ke=r(e,"P",{});var Ds=i(ke);to=a(Ds,"Currenlty supported tasks are:"),Ds.forEach(o),qt=u(e),we=r(e,"P",{});var Rs=i(we);Te=r(Rs,"STRONG",{});var Ns=i(Te);oo=a(Ns,"Onnxruntime"),Ns.forEach(o),Rs.forEach(o),yt=u(e),q=r(e,"UL",{});var P=i(q);be=r(P,"LI",{});var Qs=i(be);xe=r(Qs,"CODE",{});var Is=i(xe);so=a(Is,"feature-extraction"),Is.forEach(o),Qs.forEach(o),ao=u(P),je=r(P,"LI",{});var Ss=i(je);ze=r(Ss,"CODE",{});var Hs=i(ze);no=a(Hs,"text-classification"),Hs.forEach(o),Ss.forEach(o),ro=u(P),Ae=r(P,"LI",{});var Ls=i(Ae);Me=r(Ls,"CODE",{});var Ws=i(Me);io=a(Ws,"token-classification"),Ws.forEach(o),Ls.forEach(o),lo=u(P),Fe=r(P,"LI",{});var Us=i(Fe);Ce=r(Us,"CODE",{});var Bs=i(Ce);po=a(Bs,"question-answering"),Bs.forEach(o),Us.forEach(o),mo=u(P),Pe=r(P,"LI",{});var Ys=i(Pe);De=r(Ys,"CODE",{});var Gs=i(De);uo=a(Gs,"zero-shot-classification"),Gs.forEach(o),Ys.forEach(o),P.forEach(o),Et=u(e),Q=r(e,"H2",{class:!0});var Ht=i(Q);B=r(Ht,"A",{id:!0,class:!0,href:!0});var Js=i(B);Re=r(Js,"SPAN",{});var Ks=i(Re);O(te.$$.fragment,Ks),Ks.forEach(o),Js.forEach(o),co=u(Ht),Ne=r(Ht,"SPAN",{});var Vs=i(Ne);fo=a(Vs,"Optimum pipeline usage"),Vs.forEach(o),Ht.forEach(o),$t=u(e),A=r(e,"P",{});var X=i(A);ho=a(X,"While each task has an associated "),Qe=r(X,"CODE",{});var Xs=i(Qe);go=a(Xs,"optimum_pipeline"),Xs.forEach(o),_o=a(X,", it is simpler to use the general "),Ie=r(X,"CODE",{});var Zs=i(Ie);vo=a(Zs,"pipeline"),Zs.forEach(o),ko=a(X,` abstraction which contains all the specific task pipelines.
The `),Se=r(X,"CODE",{});var ea=i(Se);wo=a(ea,"optimum_pipeline"),ea.forEach(o),qo=a(X," automatically loads a default model and tokenizer capable of inference for your task."),X.forEach(o),Ot=u(e),qe=r(e,"OL",{});var ta=i(qe);oe=r(ta,"LI",{});var Lt=i(oe);yo=a(Lt,"Start by creating a "),He=r(Lt,"CODE",{});var oa=i(He);Eo=a(oa,"optimum_pipeline"),oa.forEach(o),$o=a(Lt," and specify an inference task:"),Lt.forEach(o),ta.forEach(o),Tt=u(e),O(se.$$.fragment,e),bt=u(e),ae=r(e,"OL",{start:!0});var sa=i(ae);ne=r(sa,"LI",{});var Wt=i(ne);Oo=a(Wt,"Pass your input text to the "),Le=r(Wt,"CODE",{});var aa=i(Le);To=a(aa,"optimum_pipeline"),aa.forEach(o),bo=a(Wt,":"),Wt.forEach(o),sa.forEach(o),xt=u(e),O(re.$$.fragment,e),jt=u(e),I=r(e,"H3",{class:!0});var Ut=i(I);Y=r(Ut,"A",{id:!0,class:!0,href:!0});var na=i(Y);We=r(na,"SPAN",{});var ra=i(We);O(ie.$$.fragment,ra),ra.forEach(o),na.forEach(o),xo=u(Ut),Ue=r(Ut,"SPAN",{});var ia=i(Ue);jo=a(ia,"Use a vanilla transformers model and convert"),ia.forEach(o),Ut.forEach(o),zt=u(e),h=r(e,"P",{});var M=i(h);zo=a(M,"The "),Be=r(M,"CODE",{});var la=i(Be);Ao=a(la,"optimum_pipeline"),la.forEach(o),Mo=a(M," accepts supported model from the "),le=r(M,"A",{href:!0,rel:!0});var pa=i(le);Fo=a(pa,"Model Hub"),pa.forEach(o),Co=a(M,`.
There are tags on the Model Hub that allow you to filter for a model you\u2019d like to use for your task.
Once you\u2019ve picked an appropriate model, load it with the `),Ye=r(M,"CODE",{});var ma=i(Ye);Po=a(ma,"from_transformers()"),ma.forEach(o),Do=a(M," method for corresponding "),Ge=r(M,"CODE",{});var ua=i(Ge);Ro=a(ua,"ORTModelFor*"),ua.forEach(o),No=a(M,"\nand [`AutoTokenizer\u2019] class. For example, load the "),Je=r(M,"CODE",{});var da=i(Je);Qo=a(da,"ORTModelForQuestionAnswering"),da.forEach(o),Io=a(M," class for a question answering modeling task:"),M.forEach(o),At=u(e),O(pe.$$.fragment,e),Mt=u(e),S=r(e,"H3",{class:!0});var Bt=i(S);G=r(Bt,"A",{id:!0,class:!0,href:!0});var ca=i(G);Ke=r(ca,"SPAN",{});var fa=i(Ke);O(me.$$.fragment,fa),fa.forEach(o),ca.forEach(o),So=u(Bt),Ve=r(Bt,"SPAN",{});var ha=i(Ve);Ho=a(ha,"Use optimum model"),ha.forEach(o),Bt.forEach(o),Ft=u(e),g=r(e,"P",{});var F=i(g);Lo=a(F,"The "),Xe=r(F,"CODE",{});var ga=i(Xe);Wo=a(ga,"optimum_pipeline"),ga.forEach(o),Uo=a(F," is tightly integrated with "),ue=r(F,"A",{href:!0,rel:!0});var _a=i(ue);Bo=a(_a,"Model Hub"),_a.forEach(o),Yo=a(F,` and can load optimized models directly, e.g. Onnxruntime.
There are tags on the Model Hub that allow you to filter for a model you\u2019d like to use for your task.
Once you\u2019ve picked an appropriate model, load it with the `),Ze=r(F,"CODE",{});var va=i(Ze);Go=a(va,".from_pretrained"),va.forEach(o),Jo=a(F," method for corresponding "),et=r(F,"CODE",{});var ka=i(et);Ko=a(ka,"ORTModelFor*"),ka.forEach(o),Vo=a(F,"\nand [`AutoTokenizer\u2019] class. For example, load the "),tt=r(F,"CODE",{});var wa=i(tt);Xo=a(wa,"ORTModelForQuestionAnswering"),wa.forEach(o),Zo=a(F," class for a question answering modeling task:"),F.forEach(o),Ct=u(e),O(de.$$.fragment,e),Pt=u(e),H=r(e,"H3",{class:!0});var Yt=i(H);J=r(Yt,"A",{id:!0,class:!0,href:!0});var qa=i(J);ot=r(qa,"SPAN",{});var ya=i(ot);O(ce.$$.fragment,ya),ya.forEach(o),qa.forEach(o),es=u(Yt),st=r(Yt,"SPAN",{});var Ea=i(st);ts=a(Ea,"Optimizing and Quantizing in Pipelines"),Ea.forEach(o),Yt.forEach(o),Dt=u(e),f=r(e,"P",{});var v=i(f);os=a(v,"The "),at=r(v,"CODE",{});var $a=i(at);ss=a($a,"optimum_pipeline"),$a.forEach(o),as=a(v,` can not only run inference it also provides arguments to quantize and optimize your model on the fly.
Once you\u2019ve picked an appropriate model, load it with the `),nt=r(v,"CODE",{});var Oa=i(nt);ns=a(Oa,".from_transformers"),Oa.forEach(o),rs=a(v," or "),rt=r(v,"CODE",{});var Ta=i(rt);is=a(Ta,".from_pretrained"),Ta.forEach(o),ls=a(v," method for corresponding "),it=r(v,"CODE",{});var ba=i(it);ps=a(ba,"ORTModelFor*"),ba.forEach(o),ms=a(v,"\nand [`AutoTokenizer\u2019] class. For example, load the "),lt=r(v,"CODE",{});var xa=i(lt);us=a(xa,"ORTModelForQuestionAnswering"),xa.forEach(o),ds=a(v,` class for a question answering modeling task and provide
the `),pt=r(v,"CODE",{});var ja=i(pt);cs=a(ja,"do_optimization=True"),ja.forEach(o),fs=a(v," and/or "),mt=r(v,"CODE",{});var za=i(mt);hs=a(za,"do_quantization=True"),za.forEach(o),gs=a(v," arguments:"),v.forEach(o),Rt=u(e),O(fe.$$.fragment,e),Nt=u(e),L=r(e,"H2",{class:!0});var Gt=i(L);K=r(Gt,"A",{id:!0,class:!0,href:!0});var Aa=i(K);ut=r(Aa,"SPAN",{});var Ma=i(ut);O(he.$$.fragment,Ma),Ma.forEach(o),Aa.forEach(o),_s=u(Gt),dt=r(Gt,"SPAN",{});var Fa=i(dt);vs=a(Fa,"Transformers pipeline usage"),Fa.forEach(o),Gt.forEach(o),Qt=u(e),_=r(e,"P",{});var C=i(_);ks=a(C,"The "),ct=r(C,"CODE",{});var Ca=i(ct);ws=a(Ca,"optimum_pipeline"),Ca.forEach(o),qs=a(C," is just a light wrapper around the "),ft=r(C,"CODE",{});var Pa=i(ft);ys=a(Pa,"transformers.pipeline"),Pa.forEach(o),Es=a(C,` to enable checks for supported tasks and additional features
, like quantization and optimization. This being said you can use the `),ht=r(C,"CODE",{});var Da=i(ht);$s=a(Da,"transformers.pipeline"),Da.forEach(o),Os=a(C," and just replace your "),gt=r(C,"CODE",{});var Ra=i(gt);Ts=a(Ra,"AutoFor*"),Ra.forEach(o),bs=a(C,` with the optimum
`),_t=r(C,"CODE",{});var Na=i(_t);xs=a(Na,"ORTModelFor*"),Na.forEach(o),js=a(C," class."),C.forEach(o),It=u(e),O(ge.$$.fragment,e),this.h()},h(){d(c,"name","hf:doc:metadata"),d(c,"content",JSON.stringify(Ya)),d(z,"id","optimum-pipelines-for-inference"),d(z,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(z,"href","#optimum-pipelines-for-inference"),d(k,"class","relative group"),d(ee,"href","https://huggingface.co/models"),d(ee,"rel","nofollow"),d(B,"id","optimum-pipeline-usage"),d(B,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(B,"href","#optimum-pipeline-usage"),d(Q,"class","relative group"),d(ae,"start","2"),d(Y,"id","use-a-vanilla-transformers-model-and-convert"),d(Y,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Y,"href","#use-a-vanilla-transformers-model-and-convert"),d(I,"class","relative group"),d(le,"href","https://huggingface.co/models"),d(le,"rel","nofollow"),d(G,"id","use-optimum-model"),d(G,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(G,"href","#use-optimum-model"),d(S,"class","relative group"),d(ue,"href","https://huggingface.co/models"),d(ue,"rel","nofollow"),d(J,"id","optimizing-and-quantizing-in-pipelines"),d(J,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(J,"href","#optimizing-and-quantizing-in-pipelines"),d(H,"class","relative group"),d(K,"id","transformers-pipeline-usage"),d(K,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(K,"href","#transformers-pipeline-usage"),d(L,"class","relative group")},m(e,l){t(document.head,c),p(e,W,l),p(e,k,l),t(k,z),t(z,D),T(y,D,null),t(k,Z),t(k,R),t(R,N),p(e,E,l),p(e,w,l),t(w,ve),t(w,$e),t($e,Jt),t(w,Kt),t(w,ee),t(ee,Vt),t(w,Xt),t(w,Oe),t(Oe,Zt),t(w,eo),p(e,kt,l),T(U,e,l),p(e,wt,l),p(e,ke,l),t(ke,to),p(e,qt,l),p(e,we,l),t(we,Te),t(Te,oo),p(e,yt,l),p(e,q,l),t(q,be),t(be,xe),t(xe,so),t(q,ao),t(q,je),t(je,ze),t(ze,no),t(q,ro),t(q,Ae),t(Ae,Me),t(Me,io),t(q,lo),t(q,Fe),t(Fe,Ce),t(Ce,po),t(q,mo),t(q,Pe),t(Pe,De),t(De,uo),p(e,Et,l),p(e,Q,l),t(Q,B),t(B,Re),T(te,Re,null),t(Q,co),t(Q,Ne),t(Ne,fo),p(e,$t,l),p(e,A,l),t(A,ho),t(A,Qe),t(Qe,go),t(A,_o),t(A,Ie),t(Ie,vo),t(A,ko),t(A,Se),t(Se,wo),t(A,qo),p(e,Ot,l),p(e,qe,l),t(qe,oe),t(oe,yo),t(oe,He),t(He,Eo),t(oe,$o),p(e,Tt,l),T(se,e,l),p(e,bt,l),p(e,ae,l),t(ae,ne),t(ne,Oo),t(ne,Le),t(Le,To),t(ne,bo),p(e,xt,l),T(re,e,l),p(e,jt,l),p(e,I,l),t(I,Y),t(Y,We),T(ie,We,null),t(I,xo),t(I,Ue),t(Ue,jo),p(e,zt,l),p(e,h,l),t(h,zo),t(h,Be),t(Be,Ao),t(h,Mo),t(h,le),t(le,Fo),t(h,Co),t(h,Ye),t(Ye,Po),t(h,Do),t(h,Ge),t(Ge,Ro),t(h,No),t(h,Je),t(Je,Qo),t(h,Io),p(e,At,l),T(pe,e,l),p(e,Mt,l),p(e,S,l),t(S,G),t(G,Ke),T(me,Ke,null),t(S,So),t(S,Ve),t(Ve,Ho),p(e,Ft,l),p(e,g,l),t(g,Lo),t(g,Xe),t(Xe,Wo),t(g,Uo),t(g,ue),t(ue,Bo),t(g,Yo),t(g,Ze),t(Ze,Go),t(g,Jo),t(g,et),t(et,Ko),t(g,Vo),t(g,tt),t(tt,Xo),t(g,Zo),p(e,Ct,l),T(de,e,l),p(e,Pt,l),p(e,H,l),t(H,J),t(J,ot),T(ce,ot,null),t(H,es),t(H,st),t(st,ts),p(e,Dt,l),p(e,f,l),t(f,os),t(f,at),t(at,ss),t(f,as),t(f,nt),t(nt,ns),t(f,rs),t(f,rt),t(rt,is),t(f,ls),t(f,it),t(it,ps),t(f,ms),t(f,lt),t(lt,us),t(f,ds),t(f,pt),t(pt,cs),t(f,fs),t(f,mt),t(mt,hs),t(f,gs),p(e,Rt,l),T(fe,e,l),p(e,Nt,l),p(e,L,l),t(L,K),t(K,ut),T(he,ut,null),t(L,_s),t(L,dt),t(dt,vs),p(e,Qt,l),p(e,_,l),t(_,ks),t(_,ct),t(ct,ws),t(_,qs),t(_,ft),t(ft,ys),t(_,Es),t(_,ht),t(ht,$s),t(_,Os),t(_,gt),t(gt,Ts),t(_,bs),t(_,_t),t(_t,xs),t(_,js),p(e,It,l),T(ge,e,l),St=!0},p(e,[l]){const _e={};l&2&&(_e.$$scope={dirty:l,ctx:e}),U.$set(_e)},i(e){St||(b(y.$$.fragment,e),b(U.$$.fragment,e),b(te.$$.fragment,e),b(se.$$.fragment,e),b(re.$$.fragment,e),b(ie.$$.fragment,e),b(pe.$$.fragment,e),b(me.$$.fragment,e),b(de.$$.fragment,e),b(ce.$$.fragment,e),b(fe.$$.fragment,e),b(he.$$.fragment,e),b(ge.$$.fragment,e),St=!0)},o(e){x(y.$$.fragment,e),x(U.$$.fragment,e),x(te.$$.fragment,e),x(se.$$.fragment,e),x(re.$$.fragment,e),x(ie.$$.fragment,e),x(pe.$$.fragment,e),x(me.$$.fragment,e),x(de.$$.fragment,e),x(ce.$$.fragment,e),x(fe.$$.fragment,e),x(he.$$.fragment,e),x(ge.$$.fragment,e),St=!1},d(e){o(c),e&&o(W),e&&o(k),j(y),e&&o(E),e&&o(w),e&&o(kt),j(U,e),e&&o(wt),e&&o(ke),e&&o(qt),e&&o(we),e&&o(yt),e&&o(q),e&&o(Et),e&&o(Q),j(te),e&&o($t),e&&o(A),e&&o(Ot),e&&o(qe),e&&o(Tt),j(se,e),e&&o(bt),e&&o(ae),e&&o(xt),j(re,e),e&&o(jt),e&&o(I),j(ie),e&&o(zt),e&&o(h),e&&o(At),j(pe,e),e&&o(Mt),e&&o(S),j(me),e&&o(Ft),e&&o(g),e&&o(Ct),j(de,e),e&&o(Pt),e&&o(H),j(ce),e&&o(Dt),e&&o(f),e&&o(Rt),j(fe,e),e&&o(Nt),e&&o(L),j(he),e&&o(Qt),e&&o(_),e&&o(It),j(ge,e)}}}const Ya={local:"optimum-pipelines-for-inference",sections:[{local:"optimum-pipeline-usage",sections:[{local:"use-a-vanilla-transformers-model-and-convert",title:"Use a vanilla transformers model and convert"},{local:"use-optimum-model",title:"Use optimum model"},{local:"optimizing-and-quantizing-in-pipelines",title:"Optimizing and Quantizing in Pipelines"}],title:"Optimum pipeline usage"},{local:"transformers-pipeline-usage",title:"Transformers pipeline usage"}],title:"Optimum pipelines for inference"};function Ga(vt){return La(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Za extends Qa{constructor(c){super();Ia(this,c,Ga,Ba,Sa,{})}}export{Za as default,Ya as metadata};
