import{S as Pa,i as Da,s as Ra,e as n,k as m,w as $,t as s,M as Na,c as r,d as o,m as u,a as i,x as T,h as a,b as d,F as t,g as p,y as O,q as b,o as x,B as j,v as Qa}from"../chunks/vendor-4918fc3c.js";import{T as Ia}from"../chunks/Tip-3d800dd6.js";import{I as ye}from"../chunks/IconCopyLink-21d338b1.js";import{C as Ee}from"../chunks/CodeBlock-99419108.js";function Sa(_t){let c,W,v,z,D,y,Z,R;return{c(){c=n("p"),W=s("You can also use the "),v=n("code"),z=s("transformers.pipeline"),D=s(" and provide your "),y=n("code"),Z=s("OptimumModel"),R=s(".")},l(N){c=r(N,"P",{});var E=i(c);W=a(E,"You can also use the "),v=r(E,"CODE",{});var k=i(v);z=a(k,"transformers.pipeline"),k.forEach(o),D=a(E," and provide your "),y=r(E,"CODE",{});var ve=i(y);Z=a(ve,"OptimumModel"),ve.forEach(o),R=a(E,"."),E.forEach(o)},m(N,E){p(N,c,E),t(c,W),t(c,v),t(v,z),t(c,D),t(c,y),t(y,Z),t(c,R)},d(N){N&&o(c)}}}function Ha(_t){let c,W,v,z,D,y,Z,R,N,E,k,ve,$e,Gt,Jt,ee,Kt,Vt,Te,Xt,Zt,vt,U,kt,ke,eo,wt,we,Oe,to,qt,w,be,xe,oo,so,je,ze,ao,no,Ae,Me,ro,io,Fe,Ce,lo,po,Pe,De,mo,yt,Q,B,Re,te,uo,Ne,co,Et,A,fo,Qe,ho,go,Ie,_o,vo,Se,ko,wo,$t,qe,oe,qo,He,yo,Eo,Tt,se,Ot,ae,ne,$o,Le,To,Oo,bt,re,xt,I,Y,We,ie,bo,Ue,xo,jt,h,jo,Be,zo,Ao,le,Mo,Fo,Ye,Co,Po,Ge,Do,Ro,Je,No,Qo,zt,pe,At,S,G,Ke,me,Io,Ve,So,Mt,g,Ho,Xe,Lo,Wo,ue,Uo,Bo,Ze,Yo,Go,et,Jo,Ko,tt,Vo,Xo,Ft,de,Ct,H,J,ot,ce,Zo,st,es,Pt,f,ts,at,os,ss,nt,as,ns,rt,rs,is,it,ls,ps,lt,ms,us,pt,ds,cs,Dt,fe,Rt,L,K,mt,he,fs,ut,hs,Nt,_,gs,dt,_s,vs,ct,ks,ws,ft,qs,ys,ht,Es,$s,gt,Ts,Os,Qt,ge,It;return y=new ye({}),U=new Ia({props:{$$slots:{default:[Sa]},$$scope:{ctx:_t}}}),te=new ye({}),se=new Ee({props:{code:`from optimum import optimum_pipeline

classifier = optimum_pipeline(task="text-classification", accelerator="onnx")
`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> optimum <span class="hljs-keyword">import</span> optimum_pipeline

<span class="hljs-meta">&gt;&gt;&gt; </span>classifier = optimum_pipeline(task=<span class="hljs-string">&quot;text-classification&quot;</span>, accelerator=<span class="hljs-string">&quot;onnx&quot;</span>)
`}}),re=new Ee({props:{code:'classifier("I like you. I love you.")',highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>classifier(<span class="hljs-string">&quot;I like you. I love you.&quot;</span>)'}}),ie=new ye({}),pe=new Ee({props:{code:`from transformers import AutoTokenizer
from optimum.onnxruntime import ORTModelForQuestionAnswering
from optimum import optimum_pipeline

tokenizer = AutoTokenizer.from_pretrained("deepset/roberta-base-squad2")
model = ORTModelForQuestionAnswering.from_pretrained("deepset/roberta-base-squad2",from_transformers=True)

onnx_qa = optimum_pipeline("question-answering", model=model, tokenizer=tokenizer)
question = "Whats my name?"
context = "My Name is Philipp and I live in Nuremberg."

pred = onnx_qa(question=question, context=context)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> optimum.onnxruntime <span class="hljs-keyword">import</span> ORTModelForQuestionAnswering
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> optimum <span class="hljs-keyword">import</span> optimum_pipeline

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;deepset/roberta-base-squad2&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = ORTModelForQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;deepset/roberta-base-squad2&quot;</span>,from_transformers=<span class="hljs-literal">True</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>onnx_qa = optimum_pipeline(<span class="hljs-string">&quot;question-answering&quot;</span>, model=model, tokenizer=tokenizer)
<span class="hljs-meta">&gt;&gt;&gt; </span>question = <span class="hljs-string">&quot;Whats my name?&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>context = <span class="hljs-string">&quot;My Name is Philipp and I live in Nuremberg.&quot;</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>pred = onnx_qa(question=question, context=context)`}}),me=new ye({}),de=new Ee({props:{code:`from transformers import AutoTokenizer
from optimum.onnxruntime import ORTModelForQuestionAnswering
from optimum import optimum_pipeline

tokenizer = AutoTokenizer.from_pretrained("optimum/roberta-base-squad2")
model = ORTModelForQuestionAnswering.from_pretrained("optimum/roberta-base-squad2")

onnx_qa = optimum_pipeline("question-answering", model=model, tokenizer=tokenizer)
question = "Whats my name?"
context = "My Name is Philipp and I live in Nuremberg."

pred = onnx_qa(question=question, context=context)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> optimum.onnxruntime <span class="hljs-keyword">import</span> ORTModelForQuestionAnswering
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> optimum <span class="hljs-keyword">import</span> optimum_pipeline

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;optimum/roberta-base-squad2&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = ORTModelForQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;optimum/roberta-base-squad2&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>onnx_qa = optimum_pipeline(<span class="hljs-string">&quot;question-answering&quot;</span>, model=model, tokenizer=tokenizer)
<span class="hljs-meta">&gt;&gt;&gt; </span>question = <span class="hljs-string">&quot;Whats my name?&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>context = <span class="hljs-string">&quot;My Name is Philipp and I live in Nuremberg.&quot;</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>pred = onnx_qa(question=question, context=context)`}}),ce=new ye({}),fe=new Ee({props:{code:`from transformers import AutoTokenizer
from optimum.onnxruntime import ORTModelForQuestionAnswering
from optimum import optimum_pipeline

tokenizer = AutoTokenizer.from_pretrained("optimum/roberta-base-squad2")
model = ORTModelForQuestionAnswering.from_pretrained("optimum/roberta-base-squad2")

model.optimize()
model.quantize()

onnx_qa = optimum_pipeline("question-answering",
question = "Whats my name?"
context = "My Name is Philipp and I live in Nuremberg."

pred = onnx_qa(question=question, context=context)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> optimum.onnxruntime <span class="hljs-keyword">import</span> ORTModelForQuestionAnswering
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> optimum <span class="hljs-keyword">import</span> optimum_pipeline

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;optimum/roberta-base-squad2&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = ORTModelForQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;optimum/roberta-base-squad2&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>model.optimize()
<span class="hljs-meta">&gt;&gt;&gt; </span>model.quantize()

<span class="hljs-meta">&gt;&gt;&gt; </span>onnx_qa = optimum_pipeline(<span class="hljs-string">&quot;question-answering&quot;</span>,
                               model=model,
                               tokenizer=tokenizer, 
                               do_optimization=<span class="hljs-literal">True</span>, 
                               do_quantization=<span class="hljs-literal">True</span>
                               )
<span class="hljs-meta">&gt;&gt;&gt; </span>question = <span class="hljs-string">&quot;Whats my name?&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>context = <span class="hljs-string">&quot;My Name is Philipp and I live in Nuremberg.&quot;</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>pred = onnx_qa(question=question, context=context)`}}),he=new ye({}),ge=new Ee({props:{code:`from transformers import AutoTokenizer, pipeline
-from transformers import AutoModelForQuestionAnswering
+from optimum.onnxruntime import ORTModelForQuestionAnswering

-model = AutoModelForQuestionAnswering.from_pretrained("deepset/roberta-base-squad2")
+model = ORTModelForQuestionAnswering.from_transformers("deepset/roberta-base-squad2")
tokenizer = AutoTokenizer.from_pretrained("deepset/roberta-base-squad2")

onnx_qa = pipeline("question-answering",model=model,tokenizer=tokenizer)

question = "Whats my name?"
context = "My Name is Philipp and I live in Nuremberg."
pred = onnx_qa(question, context)`,highlighted:`from transformers import AutoTokenizer, pipeline
<span class="hljs-deletion">-from transformers import AutoModelForQuestionAnswering</span>
<span class="hljs-addition">+from optimum.onnxruntime import ORTModelForQuestionAnswering</span>

<span class="hljs-deletion">-model = AutoModelForQuestionAnswering.from_pretrained(&quot;deepset/roberta-base-squad2&quot;)</span>
<span class="hljs-addition">+model = ORTModelForQuestionAnswering.from_transformers(&quot;deepset/roberta-base-squad2&quot;)</span>
tokenizer = AutoTokenizer.from_pretrained(&quot;deepset/roberta-base-squad2&quot;)

onnx_qa = pipeline(&quot;question-answering&quot;,model=model,tokenizer=tokenizer)

question = &quot;Whats my name?&quot;
context = &quot;My Name is Philipp and I live in Nuremberg.&quot;
pred = onnx_qa(question, context)`}}),{c(){c=n("meta"),W=m(),v=n("h1"),z=n("a"),D=n("span"),$(y.$$.fragment),Z=m(),R=n("span"),N=s("Optimum pipelines for inference"),E=m(),k=n("p"),ve=s("The "),$e=n("code"),Gt=s("optimum_pipeline"),Jt=s(" makes it simple to use models from the "),ee=n("a"),Kt=s("Model Hub"),Vt=s(` for accelerated inference on a variety of tasks such as text classification.
Even if you don\u2019t have experience with a specific modality or understand the code powering the models, you can still use them with the `),Te=n("code"),Xt=s("optimum_pipeline"),Zt=s("! This tutorial will teach you to:"),vt=m(),$(U.$$.fragment),kt=m(),ke=n("p"),eo=s("Currenlty supported tasks are:"),wt=m(),we=n("p"),Oe=n("strong"),to=s("Onnxruntime"),qt=m(),w=n("ul"),be=n("li"),xe=n("code"),oo=s("feature-extraction"),so=m(),je=n("li"),ze=n("code"),ao=s("text-classification"),no=m(),Ae=n("li"),Me=n("code"),ro=s("token-classification"),io=m(),Fe=n("li"),Ce=n("code"),lo=s("question-answering"),po=m(),Pe=n("li"),De=n("code"),mo=s("zero-shot-classification"),yt=m(),Q=n("h2"),B=n("a"),Re=n("span"),$(te.$$.fragment),uo=m(),Ne=n("span"),co=s("Optimum pipeline usage"),Et=m(),A=n("p"),fo=s("While each task has an associated "),Qe=n("code"),ho=s("optimum_pipeline"),go=s(", it is simpler to use the general "),Ie=n("code"),_o=s("pipeline"),vo=s(` abstraction which contains all the specific task pipelines.
The `),Se=n("code"),ko=s("optimum_pipeline"),wo=s(" automatically loads a default model and tokenizer capable of inference for your task."),$t=m(),qe=n("ol"),oe=n("li"),qo=s("Start by creating a "),He=n("code"),yo=s("optimum_pipeline"),Eo=s(" and specify an inference task:"),Tt=m(),$(se.$$.fragment),Ot=m(),ae=n("ol"),ne=n("li"),$o=s("Pass your input text to the "),Le=n("code"),To=s("optimum_pipeline"),Oo=s(":"),bt=m(),$(re.$$.fragment),xt=m(),I=n("h3"),Y=n("a"),We=n("span"),$(ie.$$.fragment),bo=m(),Ue=n("span"),xo=s("Use a vanilla transformers model and convert"),jt=m(),h=n("p"),jo=s("The "),Be=n("code"),zo=s("optimum_pipeline"),Ao=s(" accepts supported model from the "),le=n("a"),Mo=s("Model Hub"),Fo=s(`.
There are tags on the Model Hub that allow you to filter for a model you\u2019d like to use for your task.
Once you\u2019ve picked an appropriate model, load it with the `),Ye=n("code"),Co=s('from_pretrained("{model_id}",from_transformers=True)'),Po=s(" method for corresponding "),Ge=n("code"),Do=s("ORTModelFor*"),Ro=s("\nand [`AutoTokenizer\u2019] class. For example, load the "),Je=n("code"),No=s("ORTModelForQuestionAnswering"),Qo=s(" class for a question answering modeling task:"),zt=m(),$(pe.$$.fragment),At=m(),S=n("h3"),G=n("a"),Ke=n("span"),$(me.$$.fragment),Io=m(),Ve=n("span"),So=s("Use optimum model"),Mt=m(),g=n("p"),Ho=s("The "),Xe=n("code"),Lo=s("optimum_pipeline"),Wo=s(" is tightly integrated with "),ue=n("a"),Uo=s("Model Hub"),Bo=s(` and can load optimized models directly, e.g. Onnxruntime.
There are tags on the Model Hub that allow you to filter for a model you\u2019d like to use for your task.
Once you\u2019ve picked an appropriate model, load it with the `),Ze=n("code"),Yo=s(".from_pretrained"),Go=s(" method for corresponding "),et=n("code"),Jo=s("ORTModelFor*"),Ko=s("\nand [`AutoTokenizer\u2019] class. For example, load the "),tt=n("code"),Vo=s("ORTModelForQuestionAnswering"),Xo=s(" class for a question answering modeling task:"),Ft=m(),$(de.$$.fragment),Ct=m(),H=n("h3"),J=n("a"),ot=n("span"),$(ce.$$.fragment),Zo=m(),st=n("span"),es=s("Optimizing and Quantizing in Pipelines"),Pt=m(),f=n("p"),ts=s("The "),at=n("code"),os=s("optimum_pipeline"),ss=s(` can not only run inference it also provides arguments to quantize and optimize your model on the fly.
Once you\u2019ve picked an appropriate model, load it with the `),nt=n("code"),as=s(".from_pretrained"),ns=s(" method for corresponding "),rt=n("code"),rs=s("ORTModelFor*"),is=s("\nand [`AutoTokenizer\u2019] class. For example, load the "),it=n("code"),ls=s("ORTModelForQuestionAnswering"),ps=s(` class for a question answering modeling task and provide
the `),lt=n("code"),ms=s("do_optimization=True"),us=s(" and/or "),pt=n("code"),ds=s("do_quantization=True"),cs=s(" arguments:"),Dt=m(),$(fe.$$.fragment),Rt=m(),L=n("h2"),K=n("a"),mt=n("span"),$(he.$$.fragment),fs=m(),ut=n("span"),hs=s("Transformers pipeline usage"),Nt=m(),_=n("p"),gs=s("The "),dt=n("code"),_s=s("optimum_pipeline"),vs=s(" is just a light wrapper around the "),ct=n("code"),ks=s("transformers.pipeline"),ws=s(` to enable checks for supported tasks and additional features
, like quantization and optimization. This being said you can use the `),ft=n("code"),qs=s("transformers.pipeline"),ys=s(" and just replace your "),ht=n("code"),Es=s("AutoFor*"),$s=s(` with the optimum
`),gt=n("code"),Ts=s("ORTModelFor*"),Os=s(" class."),Qt=m(),$(ge.$$.fragment),this.h()},l(e){const l=Na('[data-svelte="svelte-1phssyn"]',document.head);c=r(l,"META",{name:!0,content:!0}),l.forEach(o),W=u(e),v=r(e,"H1",{class:!0});var _e=i(v);z=r(_e,"A",{id:!0,class:!0,href:!0});var bs=i(z);D=r(bs,"SPAN",{});var xs=i(D);T(y.$$.fragment,xs),xs.forEach(o),bs.forEach(o),Z=u(_e),R=r(_e,"SPAN",{});var js=i(R);N=a(js,"Optimum pipelines for inference"),js.forEach(o),_e.forEach(o),E=u(e),k=r(e,"P",{});var V=i(k);ve=a(V,"The "),$e=r(V,"CODE",{});var zs=i($e);Gt=a(zs,"optimum_pipeline"),zs.forEach(o),Jt=a(V," makes it simple to use models from the "),ee=r(V,"A",{href:!0,rel:!0});var As=i(ee);Kt=a(As,"Model Hub"),As.forEach(o),Vt=a(V,` for accelerated inference on a variety of tasks such as text classification.
Even if you don\u2019t have experience with a specific modality or understand the code powering the models, you can still use them with the `),Te=r(V,"CODE",{});var Ms=i(Te);Xt=a(Ms,"optimum_pipeline"),Ms.forEach(o),Zt=a(V,"! This tutorial will teach you to:"),V.forEach(o),vt=u(e),T(U.$$.fragment,e),kt=u(e),ke=r(e,"P",{});var Fs=i(ke);eo=a(Fs,"Currenlty supported tasks are:"),Fs.forEach(o),wt=u(e),we=r(e,"P",{});var Cs=i(we);Oe=r(Cs,"STRONG",{});var Ps=i(Oe);to=a(Ps,"Onnxruntime"),Ps.forEach(o),Cs.forEach(o),qt=u(e),w=r(e,"UL",{});var P=i(w);be=r(P,"LI",{});var Ds=i(be);xe=r(Ds,"CODE",{});var Rs=i(xe);oo=a(Rs,"feature-extraction"),Rs.forEach(o),Ds.forEach(o),so=u(P),je=r(P,"LI",{});var Ns=i(je);ze=r(Ns,"CODE",{});var Qs=i(ze);ao=a(Qs,"text-classification"),Qs.forEach(o),Ns.forEach(o),no=u(P),Ae=r(P,"LI",{});var Is=i(Ae);Me=r(Is,"CODE",{});var Ss=i(Me);ro=a(Ss,"token-classification"),Ss.forEach(o),Is.forEach(o),io=u(P),Fe=r(P,"LI",{});var Hs=i(Fe);Ce=r(Hs,"CODE",{});var Ls=i(Ce);lo=a(Ls,"question-answering"),Ls.forEach(o),Hs.forEach(o),po=u(P),Pe=r(P,"LI",{});var Ws=i(Pe);De=r(Ws,"CODE",{});var Us=i(De);mo=a(Us,"zero-shot-classification"),Us.forEach(o),Ws.forEach(o),P.forEach(o),yt=u(e),Q=r(e,"H2",{class:!0});var St=i(Q);B=r(St,"A",{id:!0,class:!0,href:!0});var Bs=i(B);Re=r(Bs,"SPAN",{});var Ys=i(Re);T(te.$$.fragment,Ys),Ys.forEach(o),Bs.forEach(o),uo=u(St),Ne=r(St,"SPAN",{});var Gs=i(Ne);co=a(Gs,"Optimum pipeline usage"),Gs.forEach(o),St.forEach(o),Et=u(e),A=r(e,"P",{});var X=i(A);fo=a(X,"While each task has an associated "),Qe=r(X,"CODE",{});var Js=i(Qe);ho=a(Js,"optimum_pipeline"),Js.forEach(o),go=a(X,", it is simpler to use the general "),Ie=r(X,"CODE",{});var Ks=i(Ie);_o=a(Ks,"pipeline"),Ks.forEach(o),vo=a(X,` abstraction which contains all the specific task pipelines.
The `),Se=r(X,"CODE",{});var Vs=i(Se);ko=a(Vs,"optimum_pipeline"),Vs.forEach(o),wo=a(X," automatically loads a default model and tokenizer capable of inference for your task."),X.forEach(o),$t=u(e),qe=r(e,"OL",{});var Xs=i(qe);oe=r(Xs,"LI",{});var Ht=i(oe);qo=a(Ht,"Start by creating a "),He=r(Ht,"CODE",{});var Zs=i(He);yo=a(Zs,"optimum_pipeline"),Zs.forEach(o),Eo=a(Ht," and specify an inference task:"),Ht.forEach(o),Xs.forEach(o),Tt=u(e),T(se.$$.fragment,e),Ot=u(e),ae=r(e,"OL",{start:!0});var ea=i(ae);ne=r(ea,"LI",{});var Lt=i(ne);$o=a(Lt,"Pass your input text to the "),Le=r(Lt,"CODE",{});var ta=i(Le);To=a(ta,"optimum_pipeline"),ta.forEach(o),Oo=a(Lt,":"),Lt.forEach(o),ea.forEach(o),bt=u(e),T(re.$$.fragment,e),xt=u(e),I=r(e,"H3",{class:!0});var Wt=i(I);Y=r(Wt,"A",{id:!0,class:!0,href:!0});var oa=i(Y);We=r(oa,"SPAN",{});var sa=i(We);T(ie.$$.fragment,sa),sa.forEach(o),oa.forEach(o),bo=u(Wt),Ue=r(Wt,"SPAN",{});var aa=i(Ue);xo=a(aa,"Use a vanilla transformers model and convert"),aa.forEach(o),Wt.forEach(o),jt=u(e),h=r(e,"P",{});var M=i(h);jo=a(M,"The "),Be=r(M,"CODE",{});var na=i(Be);zo=a(na,"optimum_pipeline"),na.forEach(o),Ao=a(M," accepts supported model from the "),le=r(M,"A",{href:!0,rel:!0});var ra=i(le);Mo=a(ra,"Model Hub"),ra.forEach(o),Fo=a(M,`.
There are tags on the Model Hub that allow you to filter for a model you\u2019d like to use for your task.
Once you\u2019ve picked an appropriate model, load it with the `),Ye=r(M,"CODE",{});var ia=i(Ye);Co=a(ia,'from_pretrained("{model_id}",from_transformers=True)'),ia.forEach(o),Po=a(M," method for corresponding "),Ge=r(M,"CODE",{});var la=i(Ge);Do=a(la,"ORTModelFor*"),la.forEach(o),Ro=a(M,"\nand [`AutoTokenizer\u2019] class. For example, load the "),Je=r(M,"CODE",{});var pa=i(Je);No=a(pa,"ORTModelForQuestionAnswering"),pa.forEach(o),Qo=a(M," class for a question answering modeling task:"),M.forEach(o),zt=u(e),T(pe.$$.fragment,e),At=u(e),S=r(e,"H3",{class:!0});var Ut=i(S);G=r(Ut,"A",{id:!0,class:!0,href:!0});var ma=i(G);Ke=r(ma,"SPAN",{});var ua=i(Ke);T(me.$$.fragment,ua),ua.forEach(o),ma.forEach(o),Io=u(Ut),Ve=r(Ut,"SPAN",{});var da=i(Ve);So=a(da,"Use optimum model"),da.forEach(o),Ut.forEach(o),Mt=u(e),g=r(e,"P",{});var F=i(g);Ho=a(F,"The "),Xe=r(F,"CODE",{});var ca=i(Xe);Lo=a(ca,"optimum_pipeline"),ca.forEach(o),Wo=a(F," is tightly integrated with "),ue=r(F,"A",{href:!0,rel:!0});var fa=i(ue);Uo=a(fa,"Model Hub"),fa.forEach(o),Bo=a(F,` and can load optimized models directly, e.g. Onnxruntime.
There are tags on the Model Hub that allow you to filter for a model you\u2019d like to use for your task.
Once you\u2019ve picked an appropriate model, load it with the `),Ze=r(F,"CODE",{});var ha=i(Ze);Yo=a(ha,".from_pretrained"),ha.forEach(o),Go=a(F," method for corresponding "),et=r(F,"CODE",{});var ga=i(et);Jo=a(ga,"ORTModelFor*"),ga.forEach(o),Ko=a(F,"\nand [`AutoTokenizer\u2019] class. For example, load the "),tt=r(F,"CODE",{});var _a=i(tt);Vo=a(_a,"ORTModelForQuestionAnswering"),_a.forEach(o),Xo=a(F," class for a question answering modeling task:"),F.forEach(o),Ft=u(e),T(de.$$.fragment,e),Ct=u(e),H=r(e,"H3",{class:!0});var Bt=i(H);J=r(Bt,"A",{id:!0,class:!0,href:!0});var va=i(J);ot=r(va,"SPAN",{});var ka=i(ot);T(ce.$$.fragment,ka),ka.forEach(o),va.forEach(o),Zo=u(Bt),st=r(Bt,"SPAN",{});var wa=i(st);es=a(wa,"Optimizing and Quantizing in Pipelines"),wa.forEach(o),Bt.forEach(o),Pt=u(e),f=r(e,"P",{});var q=i(f);ts=a(q,"The "),at=r(q,"CODE",{});var qa=i(at);os=a(qa,"optimum_pipeline"),qa.forEach(o),ss=a(q,` can not only run inference it also provides arguments to quantize and optimize your model on the fly.
Once you\u2019ve picked an appropriate model, load it with the `),nt=r(q,"CODE",{});var ya=i(nt);as=a(ya,".from_pretrained"),ya.forEach(o),ns=a(q," method for corresponding "),rt=r(q,"CODE",{});var Ea=i(rt);rs=a(Ea,"ORTModelFor*"),Ea.forEach(o),is=a(q,"\nand [`AutoTokenizer\u2019] class. For example, load the "),it=r(q,"CODE",{});var $a=i(it);ls=a($a,"ORTModelForQuestionAnswering"),$a.forEach(o),ps=a(q,` class for a question answering modeling task and provide
the `),lt=r(q,"CODE",{});var Ta=i(lt);ms=a(Ta,"do_optimization=True"),Ta.forEach(o),us=a(q," and/or "),pt=r(q,"CODE",{});var Oa=i(pt);ds=a(Oa,"do_quantization=True"),Oa.forEach(o),cs=a(q," arguments:"),q.forEach(o),Dt=u(e),T(fe.$$.fragment,e),Rt=u(e),L=r(e,"H2",{class:!0});var Yt=i(L);K=r(Yt,"A",{id:!0,class:!0,href:!0});var ba=i(K);mt=r(ba,"SPAN",{});var xa=i(mt);T(he.$$.fragment,xa),xa.forEach(o),ba.forEach(o),fs=u(Yt),ut=r(Yt,"SPAN",{});var ja=i(ut);hs=a(ja,"Transformers pipeline usage"),ja.forEach(o),Yt.forEach(o),Nt=u(e),_=r(e,"P",{});var C=i(_);gs=a(C,"The "),dt=r(C,"CODE",{});var za=i(dt);_s=a(za,"optimum_pipeline"),za.forEach(o),vs=a(C," is just a light wrapper around the "),ct=r(C,"CODE",{});var Aa=i(ct);ks=a(Aa,"transformers.pipeline"),Aa.forEach(o),ws=a(C,` to enable checks for supported tasks and additional features
, like quantization and optimization. This being said you can use the `),ft=r(C,"CODE",{});var Ma=i(ft);qs=a(Ma,"transformers.pipeline"),Ma.forEach(o),ys=a(C," and just replace your "),ht=r(C,"CODE",{});var Fa=i(ht);Es=a(Fa,"AutoFor*"),Fa.forEach(o),$s=a(C,` with the optimum
`),gt=r(C,"CODE",{});var Ca=i(gt);Ts=a(Ca,"ORTModelFor*"),Ca.forEach(o),Os=a(C," class."),C.forEach(o),Qt=u(e),T(ge.$$.fragment,e),this.h()},h(){d(c,"name","hf:doc:metadata"),d(c,"content",JSON.stringify(La)),d(z,"id","optimum-pipelines-for-inference"),d(z,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(z,"href","#optimum-pipelines-for-inference"),d(v,"class","relative group"),d(ee,"href","https://huggingface.co/models"),d(ee,"rel","nofollow"),d(B,"id","optimum-pipeline-usage"),d(B,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(B,"href","#optimum-pipeline-usage"),d(Q,"class","relative group"),d(ae,"start","2"),d(Y,"id","use-a-vanilla-transformers-model-and-convert"),d(Y,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Y,"href","#use-a-vanilla-transformers-model-and-convert"),d(I,"class","relative group"),d(le,"href","https://huggingface.co/models"),d(le,"rel","nofollow"),d(G,"id","use-optimum-model"),d(G,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(G,"href","#use-optimum-model"),d(S,"class","relative group"),d(ue,"href","https://huggingface.co/models"),d(ue,"rel","nofollow"),d(J,"id","optimizing-and-quantizing-in-pipelines"),d(J,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(J,"href","#optimizing-and-quantizing-in-pipelines"),d(H,"class","relative group"),d(K,"id","transformers-pipeline-usage"),d(K,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(K,"href","#transformers-pipeline-usage"),d(L,"class","relative group")},m(e,l){t(document.head,c),p(e,W,l),p(e,v,l),t(v,z),t(z,D),O(y,D,null),t(v,Z),t(v,R),t(R,N),p(e,E,l),p(e,k,l),t(k,ve),t(k,$e),t($e,Gt),t(k,Jt),t(k,ee),t(ee,Kt),t(k,Vt),t(k,Te),t(Te,Xt),t(k,Zt),p(e,vt,l),O(U,e,l),p(e,kt,l),p(e,ke,l),t(ke,eo),p(e,wt,l),p(e,we,l),t(we,Oe),t(Oe,to),p(e,qt,l),p(e,w,l),t(w,be),t(be,xe),t(xe,oo),t(w,so),t(w,je),t(je,ze),t(ze,ao),t(w,no),t(w,Ae),t(Ae,Me),t(Me,ro),t(w,io),t(w,Fe),t(Fe,Ce),t(Ce,lo),t(w,po),t(w,Pe),t(Pe,De),t(De,mo),p(e,yt,l),p(e,Q,l),t(Q,B),t(B,Re),O(te,Re,null),t(Q,uo),t(Q,Ne),t(Ne,co),p(e,Et,l),p(e,A,l),t(A,fo),t(A,Qe),t(Qe,ho),t(A,go),t(A,Ie),t(Ie,_o),t(A,vo),t(A,Se),t(Se,ko),t(A,wo),p(e,$t,l),p(e,qe,l),t(qe,oe),t(oe,qo),t(oe,He),t(He,yo),t(oe,Eo),p(e,Tt,l),O(se,e,l),p(e,Ot,l),p(e,ae,l),t(ae,ne),t(ne,$o),t(ne,Le),t(Le,To),t(ne,Oo),p(e,bt,l),O(re,e,l),p(e,xt,l),p(e,I,l),t(I,Y),t(Y,We),O(ie,We,null),t(I,bo),t(I,Ue),t(Ue,xo),p(e,jt,l),p(e,h,l),t(h,jo),t(h,Be),t(Be,zo),t(h,Ao),t(h,le),t(le,Mo),t(h,Fo),t(h,Ye),t(Ye,Co),t(h,Po),t(h,Ge),t(Ge,Do),t(h,Ro),t(h,Je),t(Je,No),t(h,Qo),p(e,zt,l),O(pe,e,l),p(e,At,l),p(e,S,l),t(S,G),t(G,Ke),O(me,Ke,null),t(S,Io),t(S,Ve),t(Ve,So),p(e,Mt,l),p(e,g,l),t(g,Ho),t(g,Xe),t(Xe,Lo),t(g,Wo),t(g,ue),t(ue,Uo),t(g,Bo),t(g,Ze),t(Ze,Yo),t(g,Go),t(g,et),t(et,Jo),t(g,Ko),t(g,tt),t(tt,Vo),t(g,Xo),p(e,Ft,l),O(de,e,l),p(e,Ct,l),p(e,H,l),t(H,J),t(J,ot),O(ce,ot,null),t(H,Zo),t(H,st),t(st,es),p(e,Pt,l),p(e,f,l),t(f,ts),t(f,at),t(at,os),t(f,ss),t(f,nt),t(nt,as),t(f,ns),t(f,rt),t(rt,rs),t(f,is),t(f,it),t(it,ls),t(f,ps),t(f,lt),t(lt,ms),t(f,us),t(f,pt),t(pt,ds),t(f,cs),p(e,Dt,l),O(fe,e,l),p(e,Rt,l),p(e,L,l),t(L,K),t(K,mt),O(he,mt,null),t(L,fs),t(L,ut),t(ut,hs),p(e,Nt,l),p(e,_,l),t(_,gs),t(_,dt),t(dt,_s),t(_,vs),t(_,ct),t(ct,ks),t(_,ws),t(_,ft),t(ft,qs),t(_,ys),t(_,ht),t(ht,Es),t(_,$s),t(_,gt),t(gt,Ts),t(_,Os),p(e,Qt,l),O(ge,e,l),It=!0},p(e,[l]){const _e={};l&2&&(_e.$$scope={dirty:l,ctx:e}),U.$set(_e)},i(e){It||(b(y.$$.fragment,e),b(U.$$.fragment,e),b(te.$$.fragment,e),b(se.$$.fragment,e),b(re.$$.fragment,e),b(ie.$$.fragment,e),b(pe.$$.fragment,e),b(me.$$.fragment,e),b(de.$$.fragment,e),b(ce.$$.fragment,e),b(fe.$$.fragment,e),b(he.$$.fragment,e),b(ge.$$.fragment,e),It=!0)},o(e){x(y.$$.fragment,e),x(U.$$.fragment,e),x(te.$$.fragment,e),x(se.$$.fragment,e),x(re.$$.fragment,e),x(ie.$$.fragment,e),x(pe.$$.fragment,e),x(me.$$.fragment,e),x(de.$$.fragment,e),x(ce.$$.fragment,e),x(fe.$$.fragment,e),x(he.$$.fragment,e),x(ge.$$.fragment,e),It=!1},d(e){o(c),e&&o(W),e&&o(v),j(y),e&&o(E),e&&o(k),e&&o(vt),j(U,e),e&&o(kt),e&&o(ke),e&&o(wt),e&&o(we),e&&o(qt),e&&o(w),e&&o(yt),e&&o(Q),j(te),e&&o(Et),e&&o(A),e&&o($t),e&&o(qe),e&&o(Tt),j(se,e),e&&o(Ot),e&&o(ae),e&&o(bt),j(re,e),e&&o(xt),e&&o(I),j(ie),e&&o(jt),e&&o(h),e&&o(zt),j(pe,e),e&&o(At),e&&o(S),j(me),e&&o(Mt),e&&o(g),e&&o(Ft),j(de,e),e&&o(Ct),e&&o(H),j(ce),e&&o(Pt),e&&o(f),e&&o(Dt),j(fe,e),e&&o(Rt),e&&o(L),j(he),e&&o(Nt),e&&o(_),e&&o(Qt),j(ge,e)}}}const La={local:"optimum-pipelines-for-inference",sections:[{local:"optimum-pipeline-usage",sections:[{local:"use-a-vanilla-transformers-model-and-convert",title:"Use a vanilla transformers model and convert"},{local:"use-optimum-model",title:"Use optimum model"},{local:"optimizing-and-quantizing-in-pipelines",title:"Optimizing and Quantizing in Pipelines"}],title:"Optimum pipeline usage"},{local:"transformers-pipeline-usage",title:"Transformers pipeline usage"}],title:"Optimum pipelines for inference"};function Wa(_t){return Qa(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Ja extends Pa{constructor(c){super();Da(this,c,Wa,Ha,Ra,{})}}export{Ja as default,La as metadata};
