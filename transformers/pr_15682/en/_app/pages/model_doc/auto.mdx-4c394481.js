import{S as lut,i as iut,s as dut,e as a,k as l,w as f,t as o,M as cut,c as n,d as t,m as i,a as s,x as m,h as r,b as c,F as e,g as b,y as g,q as h,o as p,B as _}from"../../chunks/vendor-4833417e.js";import{T as Zyr}from"../../chunks/Tip-fffd6df1.js";import{D as E}from"../../chunks/Docstring-44c5af16.js";import{C as w}from"../../chunks/CodeBlock-90ffda97.js";import{I as z}from"../../chunks/IconCopyLink-4b81c553.js";import"../../chunks/CopyButton-04a16537.js";function fut(yi){let J,Ae,ie,me,to,ce,ue,Do,wi,Ef,sa,Ai,Li,nM,yf,ye,io,Bi,Pn,sM,$n,In,lM,ki,jn,iM,xi,wf,$a;return{c(){J=a("p"),Ae=o("If your "),ie=a("code"),me=o("NewModelConfig"),to=o(" is a subclass of "),ce=a("code"),ue=o("PretrainedConfig"),Do=o(`, make sure its
`),wi=a("code"),Ef=o("model_type"),sa=o(" attribute is set to the same key you use when registering the config (here "),Ai=a("code"),Li=o('"new-model"'),nM=o(")."),yf=l(),ye=a("p"),io=o("Likewise, if your "),Bi=a("code"),Pn=o("NewModel"),sM=o(" is a subclass of "),$n=a("a"),In=o("PreTrainedModel"),lM=o(`, make sure its
`),ki=a("code"),jn=o("config_class"),iM=o(` attribute is set to the same class you use when registering the model (here
`),xi=a("code"),wf=o("NewModelConfig"),$a=o(")."),this.h()},l(co){J=n(co,"P",{});var ge=s(J);Ae=r(ge,"If your "),ie=n(ge,"CODE",{});var zL=s(ie);me=r(zL,"NewModelConfig"),zL.forEach(t),to=r(ge," is a subclass of "),ce=n(ge,"CODE",{});var Ri=s(ce);ue=r(Ri,"PretrainedConfig"),Ri.forEach(t),Do=r(ge,`, make sure its
`),wi=n(ge,"CODE",{});var VL=s(wi);Ef=r(VL,"model_type"),VL.forEach(t),sa=r(ge," attribute is set to the same key you use when registering the config (here "),Ai=n(ge,"CODE",{});var WL=s(Ai);Li=r(WL,'"new-model"'),WL.forEach(t),nM=r(ge,")."),ge.forEach(t),yf=i(co),ye=n(co,"P",{});var qo=s(ye);io=r(qo,"Likewise, if your "),Bi=n(qo,"CODE",{});var Ia=s(Bi);Pn=r(Ia,"NewModel"),Ia.forEach(t),sM=r(qo," is a subclass of "),$n=n(qo,"A",{href:!0});var QL=s($n);In=r(QL,"PreTrainedModel"),QL.forEach(t),lM=r(qo,`, make sure its
`),ki=n(qo,"CODE",{});var Af=s(ki);jn=r(Af,"config_class"),Af.forEach(t),iM=r(qo,` attribute is set to the same class you use when registering the model (here
`),xi=n(qo,"CODE",{});var HL=s(xi);wf=r(HL,"NewModelConfig"),HL.forEach(t),$a=r(qo,")."),qo.forEach(t),this.h()},h(){c($n,"href","/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel")},m(co,ge){b(co,J,ge),e(J,Ae),e(J,ie),e(ie,me),e(J,to),e(J,ce),e(ce,ue),e(J,Do),e(J,wi),e(wi,Ef),e(J,sa),e(J,Ai),e(Ai,Li),e(J,nM),b(co,yf,ge),b(co,ye,ge),e(ye,io),e(ye,Bi),e(Bi,Pn),e(ye,sM),e(ye,$n),e($n,In),e(ye,lM),e(ye,ki),e(ki,jn),e(ye,iM),e(ye,xi),e(xi,wf),e(ye,$a)},d(co){co&&t(J),co&&t(yf),co&&t(ye)}}}function mut(yi){let J,Ae,ie,me,to;return{c(){J=a("p"),Ae=o("Passing "),ie=a("code"),me=o("use_auth_token=True"),to=o(" is required when you want to use a private model.")},l(ce){J=n(ce,"P",{});var ue=s(J);Ae=r(ue,"Passing "),ie=n(ue,"CODE",{});var Do=s(ie);me=r(Do,"use_auth_token=True"),Do.forEach(t),to=r(ue," is required when you want to use a private model."),ue.forEach(t)},m(ce,ue){b(ce,J,ue),e(J,Ae),e(J,ie),e(ie,me),e(J,to)},d(ce){ce&&t(J)}}}function gut(yi){let J,Ae,ie,me,to;return{c(){J=a("p"),Ae=o("Passing "),ie=a("code"),me=o("use_auth_token=True"),to=o(" is required when you want to use a private model.")},l(ce){J=n(ce,"P",{});var ue=s(J);Ae=r(ue,"Passing "),ie=n(ue,"CODE",{});var Do=s(ie);me=r(Do,"use_auth_token=True"),Do.forEach(t),to=r(ue," is required when you want to use a private model."),ue.forEach(t)},m(ce,ue){b(ce,J,ue),e(J,Ae),e(J,ie),e(ie,me),e(J,to)},d(ce){ce&&t(J)}}}function hut(yi){let J,Ae,ie,me,to,ce,ue,Do,wi,Ef,sa,Ai,Li,nM,yf,ye,io,Bi,Pn,sM,$n,In,lM,ki,jn,iM,xi,wf,$a,co,ge,zL,Ri,VL,WL,qo,Ia,QL,Af,HL,Bxe,v7e,Si,Lf,zV,dM,kxe,VV,xxe,T7e,Nn,Rxe,WV,Sxe,Pxe,QV,$xe,Ixe,F7e,cM,C7e,UL,jxe,M7e,Bf,E7e,Pi,kf,HV,fM,Nxe,UV,Dxe,y7e,Go,mM,qxe,gM,Gxe,JL,Oxe,Xxe,zxe,hM,Vxe,JV,Wxe,Qxe,Hxe,fo,pM,Uxe,YV,Jxe,Yxe,$i,Kxe,KV,Zxe,eRe,ZV,oRe,rRe,tRe,v,xf,eW,aRe,nRe,YL,sRe,lRe,iRe,Rf,oW,dRe,cRe,KL,fRe,mRe,gRe,Sf,rW,hRe,pRe,ZL,_Re,uRe,bRe,Pf,tW,vRe,TRe,e7,FRe,CRe,MRe,$f,aW,ERe,yRe,o7,wRe,ARe,LRe,If,nW,BRe,kRe,r7,xRe,RRe,SRe,jf,sW,PRe,$Re,t7,IRe,jRe,NRe,Nf,lW,DRe,qRe,a7,GRe,ORe,XRe,Df,iW,zRe,VRe,n7,WRe,QRe,HRe,qf,dW,URe,JRe,s7,YRe,KRe,ZRe,Gf,cW,eSe,oSe,l7,rSe,tSe,aSe,Of,fW,nSe,sSe,i7,lSe,iSe,dSe,Xf,mW,cSe,fSe,d7,mSe,gSe,hSe,zf,gW,pSe,_Se,c7,uSe,bSe,vSe,Vf,hW,TSe,FSe,f7,CSe,MSe,ESe,Wf,pW,ySe,wSe,m7,ASe,LSe,BSe,Qf,_W,kSe,xSe,g7,RSe,SSe,PSe,Hf,uW,$Se,ISe,h7,jSe,NSe,DSe,Uf,bW,qSe,GSe,p7,OSe,XSe,zSe,Jf,vW,VSe,WSe,_7,QSe,HSe,USe,Yf,TW,JSe,YSe,u7,KSe,ZSe,ePe,Kf,FW,oPe,rPe,b7,tPe,aPe,nPe,Zf,CW,sPe,lPe,v7,iPe,dPe,cPe,em,MW,fPe,mPe,T7,gPe,hPe,pPe,om,EW,_Pe,uPe,F7,bPe,vPe,TPe,rm,yW,FPe,CPe,C7,MPe,EPe,yPe,tm,wW,wPe,APe,M7,LPe,BPe,kPe,am,AW,xPe,RPe,E7,SPe,PPe,$Pe,nm,LW,IPe,jPe,y7,NPe,DPe,qPe,sm,BW,GPe,OPe,w7,XPe,zPe,VPe,lm,kW,WPe,QPe,A7,HPe,UPe,JPe,im,xW,YPe,KPe,L7,ZPe,e$e,o$e,dm,RW,r$e,t$e,B7,a$e,n$e,s$e,cm,SW,l$e,i$e,k7,d$e,c$e,f$e,fm,PW,m$e,g$e,x7,h$e,p$e,_$e,mm,$W,u$e,b$e,R7,v$e,T$e,F$e,gm,IW,C$e,M$e,S7,E$e,y$e,w$e,hm,jW,A$e,L$e,P7,B$e,k$e,x$e,pm,NW,R$e,S$e,$7,P$e,$$e,I$e,_m,DW,j$e,N$e,I7,D$e,q$e,G$e,um,qW,O$e,X$e,j7,z$e,V$e,W$e,bm,GW,Q$e,H$e,N7,U$e,J$e,Y$e,vm,OW,K$e,Z$e,D7,eIe,oIe,rIe,Tm,XW,tIe,aIe,q7,nIe,sIe,lIe,Fm,zW,iIe,dIe,G7,cIe,fIe,mIe,Cm,VW,gIe,hIe,O7,pIe,_Ie,uIe,Mm,WW,bIe,vIe,X7,TIe,FIe,CIe,Em,QW,MIe,EIe,z7,yIe,wIe,AIe,ym,HW,LIe,BIe,V7,kIe,xIe,RIe,wm,UW,SIe,PIe,W7,$Ie,IIe,jIe,Am,JW,NIe,DIe,Q7,qIe,GIe,OIe,Lm,YW,XIe,zIe,H7,VIe,WIe,QIe,Bm,KW,HIe,UIe,U7,JIe,YIe,KIe,km,ZW,ZIe,eje,J7,oje,rje,tje,xm,eQ,aje,nje,Y7,sje,lje,ije,Rm,oQ,dje,cje,K7,fje,mje,gje,Sm,rQ,hje,pje,Z7,_je,uje,bje,Pm,tQ,vje,Tje,e9,Fje,Cje,Mje,$m,aQ,Eje,yje,o9,wje,Aje,Lje,Im,nQ,Bje,kje,r9,xje,Rje,Sje,jm,sQ,Pje,$je,t9,Ije,jje,Nje,Nm,lQ,Dje,qje,a9,Gje,Oje,Xje,Dm,iQ,zje,Vje,n9,Wje,Qje,Hje,qm,dQ,Uje,Jje,s9,Yje,Kje,Zje,Gm,cQ,eNe,oNe,l9,rNe,tNe,aNe,Om,fQ,nNe,sNe,i9,lNe,iNe,dNe,Xm,mQ,cNe,fNe,d9,mNe,gNe,hNe,zm,gQ,pNe,_Ne,c9,uNe,bNe,vNe,Vm,hQ,TNe,FNe,f9,CNe,MNe,ENe,Wm,pQ,yNe,wNe,m9,ANe,LNe,BNe,Qm,_Q,kNe,xNe,g9,RNe,SNe,PNe,Hm,uQ,$Ne,INe,h9,jNe,NNe,DNe,Um,bQ,qNe,GNe,p9,ONe,XNe,zNe,Jm,vQ,VNe,WNe,_9,QNe,HNe,UNe,Ym,TQ,JNe,YNe,u9,KNe,ZNe,eDe,Km,FQ,oDe,rDe,b9,tDe,aDe,nDe,Zm,CQ,sDe,lDe,v9,iDe,dDe,cDe,eg,MQ,fDe,mDe,T9,gDe,hDe,pDe,og,EQ,_De,uDe,F9,bDe,vDe,TDe,rg,yQ,FDe,CDe,C9,MDe,EDe,yDe,tg,wQ,wDe,ADe,M9,LDe,BDe,kDe,ag,AQ,xDe,RDe,E9,SDe,PDe,$De,ng,LQ,IDe,jDe,y9,NDe,DDe,qDe,sg,BQ,GDe,ODe,w9,XDe,zDe,VDe,lg,kQ,WDe,QDe,A9,HDe,UDe,JDe,ig,xQ,YDe,KDe,L9,ZDe,eqe,oqe,dg,RQ,rqe,tqe,B9,aqe,nqe,sqe,cg,SQ,lqe,iqe,k9,dqe,cqe,fqe,fg,PQ,mqe,gqe,x9,hqe,pqe,_qe,mg,$Q,uqe,bqe,R9,vqe,Tqe,Fqe,gg,IQ,Cqe,Mqe,S9,Eqe,yqe,wqe,hg,jQ,Aqe,Lqe,P9,Bqe,kqe,xqe,NQ,Rqe,Sqe,_M,Pqe,pg,uM,$qe,DQ,Iqe,w7e,Ii,_g,qQ,bM,jqe,GQ,Nqe,A7e,Oo,vM,Dqe,TM,qqe,$9,Gqe,Oqe,Xqe,FM,zqe,OQ,Vqe,Wqe,Qqe,mo,CM,Hqe,XQ,Uqe,Jqe,ja,Yqe,zQ,Kqe,Zqe,VQ,eGe,oGe,WQ,rGe,tGe,aGe,M,Dn,QQ,nGe,sGe,I9,lGe,iGe,j9,dGe,cGe,fGe,qn,HQ,mGe,gGe,N9,hGe,pGe,D9,_Ge,uGe,bGe,Gn,UQ,vGe,TGe,q9,FGe,CGe,G9,MGe,EGe,yGe,ug,JQ,wGe,AGe,O9,LGe,BGe,kGe,On,YQ,xGe,RGe,X9,SGe,PGe,z9,$Ge,IGe,jGe,bg,KQ,NGe,DGe,V9,qGe,GGe,OGe,vg,ZQ,XGe,zGe,W9,VGe,WGe,QGe,Tg,eH,HGe,UGe,Q9,JGe,YGe,KGe,Xn,oH,ZGe,eOe,H9,oOe,rOe,U9,tOe,aOe,nOe,zn,rH,sOe,lOe,J9,iOe,dOe,Y9,cOe,fOe,mOe,Vn,tH,gOe,hOe,K9,pOe,_Oe,Z9,uOe,bOe,vOe,Fg,aH,TOe,FOe,eB,COe,MOe,EOe,Cg,nH,yOe,wOe,oB,AOe,LOe,BOe,Wn,sH,kOe,xOe,rB,ROe,SOe,tB,POe,$Oe,IOe,Mg,lH,jOe,NOe,aB,DOe,qOe,GOe,Qn,iH,OOe,XOe,nB,zOe,VOe,sB,WOe,QOe,HOe,Hn,dH,UOe,JOe,lB,YOe,KOe,iB,ZOe,eXe,oXe,Un,cH,rXe,tXe,dB,aXe,nXe,fH,sXe,lXe,iXe,Eg,mH,dXe,cXe,cB,fXe,mXe,gXe,Jn,gH,hXe,pXe,fB,_Xe,uXe,mB,bXe,vXe,TXe,yg,hH,FXe,CXe,gB,MXe,EXe,yXe,Yn,pH,wXe,AXe,hB,LXe,BXe,pB,kXe,xXe,RXe,Kn,_H,SXe,PXe,_B,$Xe,IXe,uB,jXe,NXe,DXe,Zn,uH,qXe,GXe,bB,OXe,XXe,vB,zXe,VXe,WXe,wg,bH,QXe,HXe,TB,UXe,JXe,YXe,es,vH,KXe,ZXe,FB,eze,oze,CB,rze,tze,aze,Ag,TH,nze,sze,MB,lze,ize,dze,os,FH,cze,fze,EB,mze,gze,yB,hze,pze,_ze,rs,CH,uze,bze,wB,vze,Tze,AB,Fze,Cze,Mze,ts,MH,Eze,yze,LB,wze,Aze,BB,Lze,Bze,kze,as,EH,xze,Rze,kB,Sze,Pze,xB,$ze,Ize,jze,Lg,yH,Nze,Dze,RB,qze,Gze,Oze,ns,wH,Xze,zze,SB,Vze,Wze,PB,Qze,Hze,Uze,ss,AH,Jze,Yze,$B,Kze,Zze,IB,eVe,oVe,rVe,ls,LH,tVe,aVe,jB,nVe,sVe,NB,lVe,iVe,dVe,is,BH,cVe,fVe,DB,mVe,gVe,qB,hVe,pVe,_Ve,ds,kH,uVe,bVe,GB,vVe,TVe,OB,FVe,CVe,MVe,cs,xH,EVe,yVe,XB,wVe,AVe,zB,LVe,BVe,kVe,Bg,RH,xVe,RVe,VB,SVe,PVe,$Ve,fs,SH,IVe,jVe,WB,NVe,DVe,QB,qVe,GVe,OVe,kg,PH,XVe,zVe,HB,VVe,WVe,QVe,xg,$H,HVe,UVe,UB,JVe,YVe,KVe,ms,IH,ZVe,eWe,JB,oWe,rWe,YB,tWe,aWe,nWe,gs,jH,sWe,lWe,KB,iWe,dWe,ZB,cWe,fWe,mWe,Rg,NH,gWe,hWe,ek,pWe,_We,uWe,hs,DH,bWe,vWe,ok,TWe,FWe,rk,CWe,MWe,EWe,ps,qH,yWe,wWe,tk,AWe,LWe,ak,BWe,kWe,xWe,_s,GH,RWe,SWe,nk,PWe,$We,sk,IWe,jWe,NWe,us,OH,DWe,qWe,lk,GWe,OWe,ik,XWe,zWe,VWe,bs,XH,WWe,QWe,dk,HWe,UWe,ck,JWe,YWe,KWe,Sg,zH,ZWe,eQe,fk,oQe,rQe,tQe,Pg,VH,aQe,nQe,mk,sQe,lQe,iQe,$g,WH,dQe,cQe,gk,fQe,mQe,gQe,Ig,QH,hQe,pQe,hk,_Qe,uQe,bQe,vs,HH,vQe,TQe,pk,FQe,CQe,_k,MQe,EQe,yQe,jg,UH,wQe,AQe,uk,LQe,BQe,kQe,Ts,JH,xQe,RQe,bk,SQe,PQe,vk,$Qe,IQe,jQe,Fs,YH,NQe,DQe,Tk,qQe,GQe,Fk,OQe,XQe,zQe,Cs,KH,VQe,WQe,Ck,QQe,HQe,Mk,UQe,JQe,YQe,Ms,ZH,KQe,ZQe,Ek,eHe,oHe,yk,rHe,tHe,aHe,Es,eU,nHe,sHe,wk,lHe,iHe,Ak,dHe,cHe,fHe,Ng,oU,mHe,gHe,Lk,hHe,pHe,_He,Dg,rU,uHe,bHe,Bk,vHe,THe,FHe,ys,tU,CHe,MHe,kk,EHe,yHe,xk,wHe,AHe,LHe,ws,aU,BHe,kHe,Rk,xHe,RHe,Sk,SHe,PHe,$He,As,nU,IHe,jHe,Pk,NHe,DHe,$k,qHe,GHe,OHe,qg,sU,XHe,zHe,Ik,VHe,WHe,QHe,Gg,lU,HHe,UHe,jk,JHe,YHe,KHe,Og,iU,ZHe,eUe,Nk,oUe,rUe,tUe,Xg,dU,aUe,nUe,Dk,sUe,lUe,iUe,Ls,cU,dUe,cUe,qk,fUe,mUe,Gk,gUe,hUe,pUe,zg,fU,_Ue,uUe,Ok,bUe,vUe,TUe,Vg,mU,FUe,CUe,Xk,MUe,EUe,yUe,Bs,gU,wUe,AUe,zk,LUe,BUe,Vk,kUe,xUe,RUe,ks,hU,SUe,PUe,Wk,$Ue,IUe,Qk,jUe,NUe,DUe,pU,qUe,GUe,MM,OUe,Wg,EM,XUe,_U,zUe,L7e,ji,Qg,uU,yM,VUe,bU,WUe,B7e,Xo,wM,QUe,AM,HUe,Hk,UUe,JUe,YUe,LM,KUe,vU,ZUe,eJe,oJe,Le,BM,rJe,TU,tJe,aJe,Na,nJe,FU,sJe,lJe,CU,iJe,dJe,MU,cJe,fJe,mJe,se,Hg,EU,gJe,hJe,Uk,pJe,_Je,uJe,Ug,yU,bJe,vJe,Jk,TJe,FJe,CJe,Jg,wU,MJe,EJe,Yk,yJe,wJe,AJe,Yg,AU,LJe,BJe,Kk,kJe,xJe,RJe,Kg,LU,SJe,PJe,Zk,$Je,IJe,jJe,Zg,BU,NJe,DJe,ex,qJe,GJe,OJe,eh,kU,XJe,zJe,ox,VJe,WJe,QJe,oh,xU,HJe,UJe,rx,JJe,YJe,KJe,rh,RU,ZJe,eYe,tx,oYe,rYe,tYe,th,SU,aYe,nYe,ax,sYe,lYe,iYe,ah,PU,dYe,cYe,nx,fYe,mYe,gYe,nh,$U,hYe,pYe,sx,_Ye,uYe,bYe,sh,IU,vYe,TYe,lx,FYe,CYe,MYe,lh,jU,EYe,yYe,ix,wYe,AYe,LYe,ih,NU,BYe,kYe,dx,xYe,RYe,SYe,dh,PYe,DU,$Ye,IYe,kM,jYe,ch,xM,NYe,qU,DYe,k7e,Ni,fh,GU,RM,qYe,OU,GYe,x7e,zo,SM,OYe,PM,XYe,cx,zYe,VYe,WYe,$M,QYe,XU,HYe,UYe,JYe,Be,IM,YYe,zU,KYe,ZYe,Di,eKe,VU,oKe,rKe,WU,tKe,aKe,nKe,we,mh,QU,sKe,lKe,fx,iKe,dKe,cKe,gh,HU,fKe,mKe,mx,gKe,hKe,pKe,hh,UU,_Ke,uKe,gx,bKe,vKe,TKe,ph,JU,FKe,CKe,hx,MKe,EKe,yKe,_h,YU,wKe,AKe,px,LKe,BKe,kKe,uh,KU,xKe,RKe,_x,SKe,PKe,$Ke,bh,ZU,IKe,jKe,ux,NKe,DKe,qKe,vh,eJ,GKe,OKe,bx,XKe,zKe,VKe,Th,WKe,oJ,QKe,HKe,jM,UKe,Fh,NM,JKe,rJ,YKe,R7e,qi,Ch,tJ,DM,KKe,aJ,ZKe,S7e,Vo,qM,eZe,Gi,oZe,nJ,rZe,tZe,sJ,aZe,nZe,sZe,GM,lZe,lJ,iZe,dZe,cZe,Nr,OM,fZe,iJ,mZe,gZe,Oi,hZe,dJ,pZe,_Ze,cJ,uZe,bZe,vZe,fJ,TZe,FZe,XM,CZe,ke,zM,MZe,mJ,EZe,yZe,Da,wZe,gJ,AZe,LZe,hJ,BZe,kZe,pJ,xZe,RZe,SZe,F,Mh,_J,PZe,$Ze,vx,IZe,jZe,NZe,Eh,uJ,DZe,qZe,Tx,GZe,OZe,XZe,yh,bJ,zZe,VZe,Fx,WZe,QZe,HZe,wh,vJ,UZe,JZe,Cx,YZe,KZe,ZZe,Ah,TJ,eeo,oeo,Mx,reo,teo,aeo,Lh,FJ,neo,seo,Ex,leo,ieo,deo,Bh,CJ,ceo,feo,yx,meo,geo,heo,kh,MJ,peo,_eo,wx,ueo,beo,veo,xh,EJ,Teo,Feo,Ax,Ceo,Meo,Eeo,Rh,yJ,yeo,weo,Lx,Aeo,Leo,Beo,Sh,wJ,keo,xeo,Bx,Reo,Seo,Peo,Ph,AJ,$eo,Ieo,kx,jeo,Neo,Deo,$h,LJ,qeo,Geo,xx,Oeo,Xeo,zeo,Ih,BJ,Veo,Weo,Rx,Qeo,Heo,Ueo,jh,kJ,Jeo,Yeo,Sx,Keo,Zeo,eoo,Nh,xJ,ooo,roo,Px,too,aoo,noo,Dh,RJ,soo,loo,$x,ioo,doo,coo,qh,SJ,foo,moo,Ix,goo,hoo,poo,Gh,PJ,_oo,uoo,jx,boo,voo,Too,Oh,$J,Foo,Coo,Nx,Moo,Eoo,yoo,Xh,IJ,woo,Aoo,Dx,Loo,Boo,koo,zh,jJ,xoo,Roo,qx,Soo,Poo,$oo,Vh,NJ,Ioo,joo,Gx,Noo,Doo,qoo,Wh,DJ,Goo,Ooo,Ox,Xoo,zoo,Voo,Qh,qJ,Woo,Qoo,Xx,Hoo,Uoo,Joo,xs,GJ,Yoo,Koo,zx,Zoo,ero,Vx,oro,rro,tro,Hh,OJ,aro,nro,Wx,sro,lro,iro,Uh,XJ,dro,cro,Qx,fro,mro,gro,Jh,zJ,hro,pro,Hx,_ro,uro,bro,Yh,VJ,vro,Tro,Ux,Fro,Cro,Mro,Kh,WJ,Ero,yro,Jx,wro,Aro,Lro,Zh,QJ,Bro,kro,Yx,xro,Rro,Sro,ep,HJ,Pro,$ro,Kx,Iro,jro,Nro,op,UJ,Dro,qro,Zx,Gro,Oro,Xro,rp,JJ,zro,Vro,eR,Wro,Qro,Hro,tp,YJ,Uro,Jro,oR,Yro,Kro,Zro,ap,KJ,eto,oto,rR,rto,tto,ato,np,ZJ,nto,sto,tR,lto,ito,dto,sp,eY,cto,fto,aR,mto,gto,hto,lp,oY,pto,_to,nR,uto,bto,vto,ip,rY,Tto,Fto,sR,Cto,Mto,Eto,dp,tY,yto,wto,lR,Ato,Lto,Bto,cp,aY,kto,xto,iR,Rto,Sto,Pto,fp,nY,$to,Ito,dR,jto,Nto,Dto,mp,sY,qto,Gto,cR,Oto,Xto,zto,gp,lY,Vto,Wto,fR,Qto,Hto,Uto,hp,iY,Jto,Yto,mR,Kto,Zto,eao,pp,dY,oao,rao,gR,tao,aao,nao,_p,cY,sao,lao,hR,iao,dao,cao,up,fY,fao,mao,pR,gao,hao,pao,bp,mY,_ao,uao,_R,bao,vao,Tao,vp,gY,Fao,Cao,uR,Mao,Eao,yao,Tp,hY,wao,Aao,bR,Lao,Bao,kao,Fp,pY,xao,Rao,vR,Sao,Pao,$ao,Cp,_Y,Iao,jao,TR,Nao,Dao,qao,Mp,uY,Gao,Oao,FR,Xao,zao,Vao,Ep,bY,Wao,Qao,CR,Hao,Uao,Jao,yp,vY,Yao,Kao,MR,Zao,eno,ono,wp,TY,rno,tno,ER,ano,nno,sno,Ap,FY,lno,ino,yR,dno,cno,fno,Lp,CY,mno,gno,wR,hno,pno,_no,Bp,MY,uno,bno,AR,vno,Tno,Fno,kp,EY,Cno,Mno,LR,Eno,yno,wno,xp,yY,Ano,Lno,BR,Bno,kno,xno,Rp,wY,Rno,Sno,kR,Pno,$no,Ino,Sp,AY,jno,Nno,xR,Dno,qno,Gno,Pp,LY,Ono,Xno,RR,zno,Vno,Wno,$p,BY,Qno,Hno,SR,Uno,Jno,Yno,Ip,kY,Kno,Zno,PR,eso,oso,rso,jp,xY,tso,aso,$R,nso,sso,lso,Np,RY,iso,dso,IR,cso,fso,mso,Dp,SY,gso,hso,jR,pso,_so,uso,qp,PY,bso,vso,NR,Tso,Fso,Cso,Gp,$Y,Mso,Eso,DR,yso,wso,Aso,Op,IY,Lso,Bso,qR,kso,xso,Rso,Xp,jY,Sso,Pso,GR,$so,Iso,jso,zp,NY,Nso,Dso,OR,qso,Gso,Oso,Vp,DY,Xso,zso,XR,Vso,Wso,Qso,Wp,qY,Hso,Uso,zR,Jso,Yso,Kso,Qp,GY,Zso,elo,VR,olo,rlo,tlo,Hp,OY,alo,nlo,WR,slo,llo,ilo,Up,XY,dlo,clo,QR,flo,mlo,glo,Jp,zY,hlo,plo,HR,_lo,ulo,blo,Yp,VY,vlo,Tlo,UR,Flo,Clo,Mlo,Kp,WY,Elo,ylo,JR,wlo,Alo,Llo,Zp,Blo,QY,klo,xlo,HY,Rlo,Slo,UY,Plo,$lo,VM,P7e,Xi,e_,JY,WM,Ilo,YY,jlo,$7e,Wo,QM,Nlo,zi,Dlo,KY,qlo,Glo,ZY,Olo,Xlo,zlo,HM,Vlo,eK,Wlo,Qlo,Hlo,Dr,UM,Ulo,oK,Jlo,Ylo,Vi,Klo,rK,Zlo,eio,tK,oio,rio,tio,aK,aio,nio,JM,sio,xe,YM,lio,nK,iio,dio,qa,cio,sK,fio,mio,lK,gio,hio,iK,pio,_io,uio,x,o_,dK,bio,vio,YR,Tio,Fio,Cio,r_,cK,Mio,Eio,KR,yio,wio,Aio,t_,fK,Lio,Bio,ZR,kio,xio,Rio,a_,mK,Sio,Pio,eS,$io,Iio,jio,n_,gK,Nio,Dio,oS,qio,Gio,Oio,s_,hK,Xio,zio,rS,Vio,Wio,Qio,l_,pK,Hio,Uio,tS,Jio,Yio,Kio,i_,_K,Zio,edo,aS,odo,rdo,tdo,d_,uK,ado,ndo,nS,sdo,ldo,ido,c_,bK,ddo,cdo,sS,fdo,mdo,gdo,f_,vK,hdo,pdo,lS,_do,udo,bdo,m_,TK,vdo,Tdo,iS,Fdo,Cdo,Mdo,g_,FK,Edo,ydo,dS,wdo,Ado,Ldo,h_,CK,Bdo,kdo,cS,xdo,Rdo,Sdo,p_,MK,Pdo,$do,fS,Ido,jdo,Ndo,__,EK,Ddo,qdo,mS,Gdo,Odo,Xdo,u_,yK,zdo,Vdo,gS,Wdo,Qdo,Hdo,b_,wK,Udo,Jdo,hS,Ydo,Kdo,Zdo,v_,AK,eco,oco,pS,rco,tco,aco,T_,LK,nco,sco,_S,lco,ico,dco,F_,BK,cco,fco,uS,mco,gco,hco,C_,kK,pco,_co,bS,uco,bco,vco,M_,xK,Tco,Fco,vS,Cco,Mco,Eco,E_,RK,yco,wco,TS,Aco,Lco,Bco,y_,SK,kco,xco,FS,Rco,Sco,Pco,w_,PK,$co,Ico,CS,jco,Nco,Dco,A_,$K,qco,Gco,MS,Oco,Xco,zco,L_,IK,Vco,Wco,ES,Qco,Hco,Uco,B_,jK,Jco,Yco,yS,Kco,Zco,efo,k_,NK,ofo,rfo,wS,tfo,afo,nfo,x_,DK,sfo,lfo,AS,ifo,dfo,cfo,R_,qK,ffo,mfo,LS,gfo,hfo,pfo,S_,GK,_fo,ufo,BS,bfo,vfo,Tfo,P_,OK,Ffo,Cfo,kS,Mfo,Efo,yfo,$_,XK,wfo,Afo,xS,Lfo,Bfo,kfo,I_,zK,xfo,Rfo,RS,Sfo,Pfo,$fo,j_,VK,Ifo,jfo,SS,Nfo,Dfo,qfo,N_,WK,Gfo,Ofo,PS,Xfo,zfo,Vfo,D_,Wfo,QK,Qfo,Hfo,HK,Ufo,Jfo,UK,Yfo,Kfo,KM,I7e,Wi,q_,JK,ZM,Zfo,YK,emo,j7e,Qo,eE,omo,Qi,rmo,KK,tmo,amo,ZK,nmo,smo,lmo,oE,imo,eZ,dmo,cmo,fmo,qr,rE,mmo,oZ,gmo,hmo,Hi,pmo,rZ,_mo,umo,tZ,bmo,vmo,Tmo,aZ,Fmo,Cmo,tE,Mmo,Re,aE,Emo,nZ,ymo,wmo,Ga,Amo,sZ,Lmo,Bmo,lZ,kmo,xmo,iZ,Rmo,Smo,Pmo,$,G_,dZ,$mo,Imo,$S,jmo,Nmo,Dmo,O_,cZ,qmo,Gmo,IS,Omo,Xmo,zmo,X_,fZ,Vmo,Wmo,jS,Qmo,Hmo,Umo,z_,mZ,Jmo,Ymo,NS,Kmo,Zmo,ego,V_,gZ,ogo,rgo,DS,tgo,ago,ngo,W_,hZ,sgo,lgo,qS,igo,dgo,cgo,Q_,pZ,fgo,mgo,GS,ggo,hgo,pgo,H_,_Z,_go,ugo,OS,bgo,vgo,Tgo,U_,uZ,Fgo,Cgo,XS,Mgo,Ego,ygo,J_,bZ,wgo,Ago,zS,Lgo,Bgo,kgo,Y_,vZ,xgo,Rgo,VS,Sgo,Pgo,$go,K_,TZ,Igo,jgo,WS,Ngo,Dgo,qgo,Z_,FZ,Ggo,Ogo,QS,Xgo,zgo,Vgo,eu,CZ,Wgo,Qgo,HS,Hgo,Ugo,Jgo,ou,MZ,Ygo,Kgo,US,Zgo,eho,oho,ru,EZ,rho,tho,JS,aho,nho,sho,tu,yZ,lho,iho,YS,dho,cho,fho,au,wZ,mho,gho,KS,hho,pho,_ho,nu,AZ,uho,bho,ZS,vho,Tho,Fho,su,LZ,Cho,Mho,eP,Eho,yho,who,lu,BZ,Aho,Lho,oP,Bho,kho,xho,iu,kZ,Rho,Sho,rP,Pho,$ho,Iho,du,xZ,jho,Nho,tP,Dho,qho,Gho,cu,RZ,Oho,Xho,aP,zho,Vho,Who,fu,SZ,Qho,Hho,nP,Uho,Jho,Yho,mu,PZ,Kho,Zho,sP,epo,opo,rpo,gu,$Z,tpo,apo,lP,npo,spo,lpo,hu,IZ,ipo,dpo,iP,cpo,fpo,mpo,pu,jZ,gpo,hpo,dP,ppo,_po,upo,_u,NZ,bpo,vpo,cP,Tpo,Fpo,Cpo,uu,DZ,Mpo,Epo,fP,ypo,wpo,Apo,bu,qZ,Lpo,Bpo,mP,kpo,xpo,Rpo,vu,GZ,Spo,Ppo,gP,$po,Ipo,jpo,Tu,OZ,Npo,Dpo,hP,qpo,Gpo,Opo,Fu,Xpo,XZ,zpo,Vpo,zZ,Wpo,Qpo,VZ,Hpo,Upo,nE,N7e,Ui,Cu,WZ,sE,Jpo,QZ,Ypo,D7e,Ho,lE,Kpo,Ji,Zpo,HZ,e_o,o_o,UZ,r_o,t_o,a_o,iE,n_o,JZ,s_o,l_o,i_o,Gr,dE,d_o,YZ,c_o,f_o,Yi,m_o,KZ,g_o,h_o,ZZ,p_o,__o,u_o,eee,b_o,v_o,cE,T_o,Se,fE,F_o,oee,C_o,M_o,Oa,E_o,ree,y_o,w_o,tee,A_o,L_o,aee,B_o,k_o,x_o,I,Mu,nee,R_o,S_o,pP,P_o,$_o,I_o,Eu,see,j_o,N_o,_P,D_o,q_o,G_o,yu,lee,O_o,X_o,uP,z_o,V_o,W_o,wu,iee,Q_o,H_o,bP,U_o,J_o,Y_o,Au,dee,K_o,Z_o,vP,euo,ouo,ruo,Lu,cee,tuo,auo,TP,nuo,suo,luo,Bu,fee,iuo,duo,FP,cuo,fuo,muo,ku,mee,guo,huo,CP,puo,_uo,uuo,xu,gee,buo,vuo,MP,Tuo,Fuo,Cuo,Ru,hee,Muo,Euo,EP,yuo,wuo,Auo,Su,pee,Luo,Buo,yP,kuo,xuo,Ruo,Pu,_ee,Suo,Puo,wP,$uo,Iuo,juo,$u,uee,Nuo,Duo,AP,quo,Guo,Ouo,Iu,bee,Xuo,zuo,LP,Vuo,Wuo,Quo,ju,vee,Huo,Uuo,BP,Juo,Yuo,Kuo,Nu,Tee,Zuo,e2o,kP,o2o,r2o,t2o,Du,Fee,a2o,n2o,xP,s2o,l2o,i2o,qu,Cee,d2o,c2o,RP,f2o,m2o,g2o,Gu,Mee,h2o,p2o,SP,_2o,u2o,b2o,Ou,Eee,v2o,T2o,PP,F2o,C2o,M2o,Xu,yee,E2o,y2o,$P,w2o,A2o,L2o,zu,wee,B2o,k2o,IP,x2o,R2o,S2o,Vu,Aee,P2o,$2o,jP,I2o,j2o,N2o,Wu,Lee,D2o,q2o,NP,G2o,O2o,X2o,Qu,Bee,z2o,V2o,DP,W2o,Q2o,H2o,Hu,kee,U2o,J2o,qP,Y2o,K2o,Z2o,Uu,xee,e1o,o1o,GP,r1o,t1o,a1o,Ju,Ree,n1o,s1o,OP,l1o,i1o,d1o,Yu,See,c1o,f1o,XP,m1o,g1o,h1o,Ku,Pee,p1o,_1o,$ee,u1o,b1o,v1o,Zu,Iee,T1o,F1o,zP,C1o,M1o,E1o,e2,jee,y1o,w1o,VP,A1o,L1o,B1o,o2,Nee,k1o,x1o,WP,R1o,S1o,P1o,r2,Dee,$1o,I1o,QP,j1o,N1o,D1o,t2,q1o,qee,G1o,O1o,Gee,X1o,z1o,Oee,V1o,W1o,mE,q7e,Ki,a2,Xee,gE,Q1o,zee,H1o,G7e,Uo,hE,U1o,Zi,J1o,Vee,Y1o,K1o,Wee,Z1o,ebo,obo,pE,rbo,Qee,tbo,abo,nbo,Or,_E,sbo,Hee,lbo,ibo,ed,dbo,Uee,cbo,fbo,Jee,mbo,gbo,hbo,Yee,pbo,_bo,uE,ubo,Pe,bE,bbo,Kee,vbo,Tbo,Xa,Fbo,Zee,Cbo,Mbo,eoe,Ebo,ybo,ooe,wbo,Abo,Lbo,ae,n2,roe,Bbo,kbo,HP,xbo,Rbo,Sbo,s2,toe,Pbo,$bo,UP,Ibo,jbo,Nbo,l2,aoe,Dbo,qbo,JP,Gbo,Obo,Xbo,i2,noe,zbo,Vbo,YP,Wbo,Qbo,Hbo,d2,soe,Ubo,Jbo,KP,Ybo,Kbo,Zbo,c2,loe,e5o,o5o,ZP,r5o,t5o,a5o,f2,ioe,n5o,s5o,e$,l5o,i5o,d5o,m2,doe,c5o,f5o,o$,m5o,g5o,h5o,g2,coe,p5o,_5o,r$,u5o,b5o,v5o,h2,foe,T5o,F5o,t$,C5o,M5o,E5o,p2,moe,y5o,w5o,a$,A5o,L5o,B5o,_2,goe,k5o,x5o,n$,R5o,S5o,P5o,u2,hoe,$5o,I5o,s$,j5o,N5o,D5o,b2,poe,q5o,G5o,l$,O5o,X5o,z5o,v2,_oe,V5o,W5o,i$,Q5o,H5o,U5o,T2,uoe,J5o,Y5o,d$,K5o,Z5o,evo,F2,ovo,boe,rvo,tvo,voe,avo,nvo,Toe,svo,lvo,vE,O7e,od,C2,Foe,TE,ivo,Coe,dvo,X7e,Jo,FE,cvo,rd,fvo,Moe,mvo,gvo,Eoe,hvo,pvo,_vo,CE,uvo,yoe,bvo,vvo,Tvo,Xr,ME,Fvo,woe,Cvo,Mvo,td,Evo,Aoe,yvo,wvo,Loe,Avo,Lvo,Bvo,Boe,kvo,xvo,EE,Rvo,$e,yE,Svo,koe,Pvo,$vo,za,Ivo,xoe,jvo,Nvo,Roe,Dvo,qvo,Soe,Gvo,Ovo,Xvo,A,M2,Poe,zvo,Vvo,c$,Wvo,Qvo,Hvo,E2,$oe,Uvo,Jvo,f$,Yvo,Kvo,Zvo,y2,Ioe,e6o,o6o,m$,r6o,t6o,a6o,w2,joe,n6o,s6o,g$,l6o,i6o,d6o,A2,Noe,c6o,f6o,h$,m6o,g6o,h6o,L2,Doe,p6o,_6o,p$,u6o,b6o,v6o,B2,qoe,T6o,F6o,_$,C6o,M6o,E6o,k2,Goe,y6o,w6o,u$,A6o,L6o,B6o,x2,Ooe,k6o,x6o,b$,R6o,S6o,P6o,R2,Xoe,$6o,I6o,v$,j6o,N6o,D6o,S2,zoe,q6o,G6o,T$,O6o,X6o,z6o,P2,Voe,V6o,W6o,F$,Q6o,H6o,U6o,$2,Woe,J6o,Y6o,C$,K6o,Z6o,eTo,I2,Qoe,oTo,rTo,M$,tTo,aTo,nTo,j2,Hoe,sTo,lTo,E$,iTo,dTo,cTo,N2,Uoe,fTo,mTo,y$,gTo,hTo,pTo,D2,Joe,_To,uTo,w$,bTo,vTo,TTo,q2,Yoe,FTo,CTo,A$,MTo,ETo,yTo,G2,Koe,wTo,ATo,L$,LTo,BTo,kTo,O2,Zoe,xTo,RTo,B$,STo,PTo,$To,X2,ere,ITo,jTo,k$,NTo,DTo,qTo,z2,ore,GTo,OTo,x$,XTo,zTo,VTo,V2,rre,WTo,QTo,R$,HTo,UTo,JTo,W2,tre,YTo,KTo,S$,ZTo,e8o,o8o,Q2,are,r8o,t8o,P$,a8o,n8o,s8o,H2,nre,l8o,i8o,$$,d8o,c8o,f8o,U2,sre,m8o,g8o,I$,h8o,p8o,_8o,J2,lre,u8o,b8o,j$,v8o,T8o,F8o,Y2,ire,C8o,M8o,N$,E8o,y8o,w8o,K2,dre,A8o,L8o,D$,B8o,k8o,x8o,Z2,cre,R8o,S8o,q$,P8o,$8o,I8o,e1,fre,j8o,N8o,G$,D8o,q8o,G8o,o1,mre,O8o,X8o,O$,z8o,V8o,W8o,r1,gre,Q8o,H8o,X$,U8o,J8o,Y8o,t1,hre,K8o,Z8o,z$,eFo,oFo,rFo,a1,pre,tFo,aFo,V$,nFo,sFo,lFo,n1,_re,iFo,dFo,W$,cFo,fFo,mFo,s1,ure,gFo,hFo,Q$,pFo,_Fo,uFo,l1,bre,bFo,vFo,H$,TFo,FFo,CFo,i1,vre,MFo,EFo,U$,yFo,wFo,AFo,d1,Tre,LFo,BFo,J$,kFo,xFo,RFo,c1,Fre,SFo,PFo,Y$,$Fo,IFo,jFo,f1,Cre,NFo,DFo,K$,qFo,GFo,OFo,m1,Mre,XFo,zFo,Z$,VFo,WFo,QFo,g1,Ere,HFo,UFo,eI,JFo,YFo,KFo,h1,ZFo,yre,eCo,oCo,wre,rCo,tCo,Are,aCo,nCo,wE,z7e,ad,p1,Lre,AE,sCo,Bre,lCo,V7e,Yo,LE,iCo,nd,dCo,kre,cCo,fCo,xre,mCo,gCo,hCo,BE,pCo,Rre,_Co,uCo,bCo,zr,kE,vCo,Sre,TCo,FCo,sd,CCo,Pre,MCo,ECo,$re,yCo,wCo,ACo,Ire,LCo,BCo,xE,kCo,Ie,RE,xCo,jre,RCo,SCo,Va,PCo,Nre,$Co,ICo,Dre,jCo,NCo,qre,DCo,qCo,GCo,G,_1,Gre,OCo,XCo,oI,zCo,VCo,WCo,u1,Ore,QCo,HCo,rI,UCo,JCo,YCo,b1,Xre,KCo,ZCo,tI,e4o,o4o,r4o,v1,zre,t4o,a4o,aI,n4o,s4o,l4o,T1,Vre,i4o,d4o,nI,c4o,f4o,m4o,F1,Wre,g4o,h4o,sI,p4o,_4o,u4o,C1,Qre,b4o,v4o,lI,T4o,F4o,C4o,M1,Hre,M4o,E4o,iI,y4o,w4o,A4o,E1,Ure,L4o,B4o,dI,k4o,x4o,R4o,y1,Jre,S4o,P4o,cI,$4o,I4o,j4o,w1,Yre,N4o,D4o,fI,q4o,G4o,O4o,A1,Kre,X4o,z4o,mI,V4o,W4o,Q4o,L1,Zre,H4o,U4o,gI,J4o,Y4o,K4o,B1,ete,Z4o,eMo,hI,oMo,rMo,tMo,k1,ote,aMo,nMo,pI,sMo,lMo,iMo,x1,rte,dMo,cMo,_I,fMo,mMo,gMo,R1,tte,hMo,pMo,uI,_Mo,uMo,bMo,S1,ate,vMo,TMo,bI,FMo,CMo,MMo,P1,nte,EMo,yMo,vI,wMo,AMo,LMo,$1,ste,BMo,kMo,TI,xMo,RMo,SMo,I1,lte,PMo,$Mo,FI,IMo,jMo,NMo,j1,ite,DMo,qMo,CI,GMo,OMo,XMo,N1,dte,zMo,VMo,MI,WMo,QMo,HMo,D1,cte,UMo,JMo,EI,YMo,KMo,ZMo,q1,fte,eEo,oEo,yI,rEo,tEo,aEo,G1,mte,nEo,sEo,wI,lEo,iEo,dEo,O1,gte,cEo,fEo,AI,mEo,gEo,hEo,X1,pEo,hte,_Eo,uEo,pte,bEo,vEo,_te,TEo,FEo,SE,W7e,ld,z1,ute,PE,CEo,bte,MEo,Q7e,Ko,$E,EEo,id,yEo,vte,wEo,AEo,Tte,LEo,BEo,kEo,IE,xEo,Fte,REo,SEo,PEo,Vr,jE,$Eo,Cte,IEo,jEo,dd,NEo,Mte,DEo,qEo,Ete,GEo,OEo,XEo,yte,zEo,VEo,NE,WEo,je,DE,QEo,wte,HEo,UEo,Wa,JEo,Ate,YEo,KEo,Lte,ZEo,e3o,Bte,o3o,r3o,t3o,na,V1,kte,a3o,n3o,LI,s3o,l3o,i3o,W1,xte,d3o,c3o,BI,f3o,m3o,g3o,Q1,Rte,h3o,p3o,kI,_3o,u3o,b3o,H1,Ste,v3o,T3o,xI,F3o,C3o,M3o,U1,Pte,E3o,y3o,RI,w3o,A3o,L3o,J1,B3o,$te,k3o,x3o,Ite,R3o,S3o,jte,P3o,$3o,qE,H7e,cd,Y1,Nte,GE,I3o,Dte,j3o,U7e,Zo,OE,N3o,fd,D3o,qte,q3o,G3o,Gte,O3o,X3o,z3o,XE,V3o,Ote,W3o,Q3o,H3o,Wr,zE,U3o,Xte,J3o,Y3o,md,K3o,zte,Z3o,eyo,Vte,oyo,ryo,tyo,Wte,ayo,nyo,VE,syo,Ne,WE,lyo,Qte,iyo,dyo,Qa,cyo,Hte,fyo,myo,Ute,gyo,hyo,Jte,pyo,_yo,uyo,D,K1,Yte,byo,vyo,SI,Tyo,Fyo,Cyo,Z1,Kte,Myo,Eyo,PI,yyo,wyo,Ayo,eb,Zte,Lyo,Byo,$I,kyo,xyo,Ryo,ob,eae,Syo,Pyo,II,$yo,Iyo,jyo,rb,oae,Nyo,Dyo,jI,qyo,Gyo,Oyo,tb,rae,Xyo,zyo,NI,Vyo,Wyo,Qyo,ab,tae,Hyo,Uyo,DI,Jyo,Yyo,Kyo,nb,aae,Zyo,ewo,qI,owo,rwo,two,sb,nae,awo,nwo,GI,swo,lwo,iwo,lb,sae,dwo,cwo,OI,fwo,mwo,gwo,ib,lae,hwo,pwo,XI,_wo,uwo,bwo,db,iae,vwo,Two,zI,Fwo,Cwo,Mwo,cb,dae,Ewo,ywo,VI,wwo,Awo,Lwo,fb,cae,Bwo,kwo,WI,xwo,Rwo,Swo,mb,fae,Pwo,$wo,QI,Iwo,jwo,Nwo,gb,mae,Dwo,qwo,HI,Gwo,Owo,Xwo,hb,gae,zwo,Vwo,UI,Wwo,Qwo,Hwo,pb,hae,Uwo,Jwo,JI,Ywo,Kwo,Zwo,_b,pae,eAo,oAo,YI,rAo,tAo,aAo,ub,_ae,nAo,sAo,KI,lAo,iAo,dAo,bb,uae,cAo,fAo,ZI,mAo,gAo,hAo,vb,bae,pAo,_Ao,ej,uAo,bAo,vAo,Tb,vae,TAo,FAo,oj,CAo,MAo,EAo,Fb,Tae,yAo,wAo,rj,AAo,LAo,BAo,Cb,Fae,kAo,xAo,tj,RAo,SAo,PAo,Mb,Cae,$Ao,IAo,aj,jAo,NAo,DAo,Eb,Mae,qAo,GAo,nj,OAo,XAo,zAo,yb,Eae,VAo,WAo,sj,QAo,HAo,UAo,wb,yae,JAo,YAo,lj,KAo,ZAo,e0o,Ab,wae,o0o,r0o,ij,t0o,a0o,n0o,Lb,Aae,s0o,l0o,dj,i0o,d0o,c0o,Bb,Lae,f0o,m0o,cj,g0o,h0o,p0o,kb,_0o,Bae,u0o,b0o,kae,v0o,T0o,xae,F0o,C0o,QE,J7e,gd,xb,Rae,HE,M0o,Sae,E0o,Y7e,er,UE,y0o,hd,w0o,Pae,A0o,L0o,$ae,B0o,k0o,x0o,JE,R0o,Iae,S0o,P0o,$0o,Qr,YE,I0o,jae,j0o,N0o,pd,D0o,Nae,q0o,G0o,Dae,O0o,X0o,z0o,qae,V0o,W0o,KE,Q0o,De,ZE,H0o,Gae,U0o,J0o,Ha,Y0o,Oae,K0o,Z0o,Xae,eLo,oLo,zae,rLo,tLo,aLo,R,Rb,Vae,nLo,sLo,fj,lLo,iLo,dLo,Sb,Wae,cLo,fLo,mj,mLo,gLo,hLo,Pb,Qae,pLo,_Lo,gj,uLo,bLo,vLo,$b,Hae,TLo,FLo,hj,CLo,MLo,ELo,Ib,Uae,yLo,wLo,pj,ALo,LLo,BLo,jb,Jae,kLo,xLo,_j,RLo,SLo,PLo,Nb,Yae,$Lo,ILo,uj,jLo,NLo,DLo,Db,Kae,qLo,GLo,bj,OLo,XLo,zLo,qb,Zae,VLo,WLo,vj,QLo,HLo,ULo,Gb,ene,JLo,YLo,Tj,KLo,ZLo,e7o,Ob,one,o7o,r7o,Fj,t7o,a7o,n7o,Xb,rne,s7o,l7o,Cj,i7o,d7o,c7o,zb,tne,f7o,m7o,Mj,g7o,h7o,p7o,Vb,ane,_7o,u7o,Ej,b7o,v7o,T7o,Wb,nne,F7o,C7o,yj,M7o,E7o,y7o,Qb,sne,w7o,A7o,wj,L7o,B7o,k7o,Hb,lne,x7o,R7o,Aj,S7o,P7o,$7o,Ub,ine,I7o,j7o,Lj,N7o,D7o,q7o,Jb,dne,G7o,O7o,Bj,X7o,z7o,V7o,Yb,cne,W7o,Q7o,kj,H7o,U7o,J7o,Kb,fne,Y7o,K7o,xj,Z7o,e9o,o9o,Zb,mne,r9o,t9o,Rj,a9o,n9o,s9o,e5,gne,l9o,i9o,Sj,d9o,c9o,f9o,o5,hne,m9o,g9o,Pj,h9o,p9o,_9o,r5,pne,u9o,b9o,$j,v9o,T9o,F9o,t5,_ne,C9o,M9o,Ij,E9o,y9o,w9o,a5,une,A9o,L9o,jj,B9o,k9o,x9o,n5,bne,R9o,S9o,Nj,P9o,$9o,I9o,s5,vne,j9o,N9o,Dj,D9o,q9o,G9o,l5,Tne,O9o,X9o,qj,z9o,V9o,W9o,i5,Fne,Q9o,H9o,Gj,U9o,J9o,Y9o,d5,Cne,K9o,Z9o,Oj,eBo,oBo,rBo,c5,Mne,tBo,aBo,Xj,nBo,sBo,lBo,f5,Ene,iBo,dBo,zj,cBo,fBo,mBo,m5,yne,gBo,hBo,Vj,pBo,_Bo,uBo,g5,wne,bBo,vBo,Wj,TBo,FBo,CBo,h5,Ane,MBo,EBo,Qj,yBo,wBo,ABo,p5,Lne,LBo,BBo,Hj,kBo,xBo,RBo,_5,SBo,Bne,PBo,$Bo,kne,IBo,jBo,xne,NBo,DBo,e3,K7e,_d,u5,Rne,o3,qBo,Sne,GBo,Z7e,or,r3,OBo,ud,XBo,Pne,zBo,VBo,$ne,WBo,QBo,HBo,t3,UBo,Ine,JBo,YBo,KBo,Hr,a3,ZBo,jne,eko,oko,bd,rko,Nne,tko,ako,Dne,nko,sko,lko,qne,iko,dko,n3,cko,qe,s3,fko,Gne,mko,gko,Ua,hko,One,pko,_ko,Xne,uko,bko,zne,vko,Tko,Fko,Vne,b5,Wne,Cko,Mko,Uj,Eko,yko,wko,v5,Ako,Qne,Lko,Bko,Hne,kko,xko,Une,Rko,Sko,l3,e9e,vd,T5,Jne,i3,Pko,Yne,$ko,o9e,rr,d3,Iko,Td,jko,Kne,Nko,Dko,Zne,qko,Gko,Oko,c3,Xko,ese,zko,Vko,Wko,Ur,f3,Qko,ose,Hko,Uko,Fd,Jko,rse,Yko,Kko,tse,Zko,exo,oxo,ase,rxo,txo,m3,axo,Ge,g3,nxo,nse,sxo,lxo,Ja,ixo,sse,dxo,cxo,lse,fxo,mxo,ise,gxo,hxo,pxo,be,F5,dse,_xo,uxo,Jj,bxo,vxo,Txo,C5,cse,Fxo,Cxo,Yj,Mxo,Exo,yxo,Rs,fse,wxo,Axo,Kj,Lxo,Bxo,Zj,kxo,xxo,Rxo,M5,mse,Sxo,Pxo,eN,$xo,Ixo,jxo,la,gse,Nxo,Dxo,oN,qxo,Gxo,rN,Oxo,Xxo,tN,zxo,Vxo,Wxo,E5,hse,Qxo,Hxo,aN,Uxo,Jxo,Yxo,y5,pse,Kxo,Zxo,nN,eRo,oRo,rRo,w5,_se,tRo,aRo,sN,nRo,sRo,lRo,A5,use,iRo,dRo,lN,cRo,fRo,mRo,L5,gRo,bse,hRo,pRo,vse,_Ro,uRo,Tse,bRo,vRo,h3,r9e,Cd,B5,Fse,p3,TRo,Cse,FRo,t9e,tr,_3,CRo,Md,MRo,Mse,ERo,yRo,Ese,wRo,ARo,LRo,u3,BRo,yse,kRo,xRo,RRo,Jr,b3,SRo,wse,PRo,$Ro,Ed,IRo,Ase,jRo,NRo,Lse,DRo,qRo,GRo,Bse,ORo,XRo,v3,zRo,Oe,T3,VRo,kse,WRo,QRo,Ya,HRo,xse,URo,JRo,Rse,YRo,KRo,Sse,ZRo,eSo,oSo,Pse,k5,$se,rSo,tSo,iN,aSo,nSo,sSo,x5,lSo,Ise,iSo,dSo,jse,cSo,fSo,Nse,mSo,gSo,F3,a9e,yd,R5,Dse,C3,hSo,qse,pSo,n9e,ar,M3,_So,wd,uSo,Gse,bSo,vSo,Ose,TSo,FSo,CSo,E3,MSo,Xse,ESo,ySo,wSo,Yr,y3,ASo,zse,LSo,BSo,Ad,kSo,Vse,xSo,RSo,Wse,SSo,PSo,$So,Qse,ISo,jSo,w3,NSo,Xe,A3,DSo,Hse,qSo,GSo,Ka,OSo,Use,XSo,zSo,Jse,VSo,WSo,Yse,QSo,HSo,USo,ao,S5,Kse,JSo,YSo,dN,KSo,ZSo,ePo,P5,Zse,oPo,rPo,cN,tPo,aPo,nPo,$5,ele,sPo,lPo,fN,iPo,dPo,cPo,I5,ole,fPo,mPo,mN,gPo,hPo,pPo,j5,rle,_Po,uPo,gN,bPo,vPo,TPo,N5,tle,FPo,CPo,hN,MPo,EPo,yPo,D5,ale,wPo,APo,pN,LPo,BPo,kPo,q5,xPo,nle,RPo,SPo,sle,PPo,$Po,lle,IPo,jPo,L3,s9e,Ld,G5,ile,B3,NPo,dle,DPo,l9e,nr,k3,qPo,Bd,GPo,cle,OPo,XPo,fle,zPo,VPo,WPo,x3,QPo,mle,HPo,UPo,JPo,Kr,R3,YPo,gle,KPo,ZPo,kd,e$o,hle,o$o,r$o,ple,t$o,a$o,n$o,_le,s$o,l$o,S3,i$o,ze,P3,d$o,ule,c$o,f$o,Za,m$o,ble,g$o,h$o,vle,p$o,_$o,Tle,u$o,b$o,v$o,xd,O5,Fle,T$o,F$o,_N,C$o,M$o,E$o,X5,Cle,y$o,w$o,uN,A$o,L$o,B$o,z5,Mle,k$o,x$o,bN,R$o,S$o,P$o,V5,$$o,Ele,I$o,j$o,yle,N$o,D$o,wle,q$o,G$o,$3,i9e,Rd,W5,Ale,I3,O$o,Lle,X$o,d9e,sr,j3,z$o,Sd,V$o,Ble,W$o,Q$o,kle,H$o,U$o,J$o,N3,Y$o,xle,K$o,Z$o,eIo,Zr,D3,oIo,Rle,rIo,tIo,Pd,aIo,Sle,nIo,sIo,Ple,lIo,iIo,dIo,$le,cIo,fIo,q3,mIo,Ve,G3,gIo,Ile,hIo,pIo,en,_Io,jle,uIo,bIo,Nle,vIo,TIo,Dle,FIo,CIo,MIo,no,Q5,qle,EIo,yIo,vN,wIo,AIo,LIo,H5,Gle,BIo,kIo,TN,xIo,RIo,SIo,U5,Ole,PIo,$Io,FN,IIo,jIo,NIo,J5,Xle,DIo,qIo,CN,GIo,OIo,XIo,Y5,zle,zIo,VIo,MN,WIo,QIo,HIo,K5,Vle,UIo,JIo,EN,YIo,KIo,ZIo,Z5,Wle,ejo,ojo,yN,rjo,tjo,ajo,ev,njo,Qle,sjo,ljo,Hle,ijo,djo,Ule,cjo,fjo,O3,c9e,$d,ov,Jle,X3,mjo,Yle,gjo,f9e,lr,z3,hjo,Id,pjo,Kle,_jo,ujo,Zle,bjo,vjo,Tjo,V3,Fjo,eie,Cjo,Mjo,Ejo,et,W3,yjo,oie,wjo,Ajo,jd,Ljo,rie,Bjo,kjo,tie,xjo,Rjo,Sjo,aie,Pjo,$jo,Q3,Ijo,We,H3,jjo,nie,Njo,Djo,on,qjo,sie,Gjo,Ojo,lie,Xjo,zjo,iie,Vjo,Wjo,Qjo,U3,rv,die,Hjo,Ujo,wN,Jjo,Yjo,Kjo,tv,cie,Zjo,eNo,AN,oNo,rNo,tNo,av,aNo,fie,nNo,sNo,mie,lNo,iNo,gie,dNo,cNo,J3,m9e,Nd,nv,hie,Y3,fNo,pie,mNo,g9e,ir,K3,gNo,Dd,hNo,_ie,pNo,_No,uie,uNo,bNo,vNo,Z3,TNo,bie,FNo,CNo,MNo,ot,ey,ENo,vie,yNo,wNo,qd,ANo,Tie,LNo,BNo,Fie,kNo,xNo,RNo,Cie,SNo,PNo,oy,$No,Qe,ry,INo,Mie,jNo,NNo,rn,DNo,Eie,qNo,GNo,yie,ONo,XNo,wie,zNo,VNo,WNo,Gd,sv,Aie,QNo,HNo,LN,UNo,JNo,YNo,lv,Lie,KNo,ZNo,BN,eDo,oDo,rDo,iv,Bie,tDo,aDo,kN,nDo,sDo,lDo,dv,iDo,kie,dDo,cDo,xie,fDo,mDo,Rie,gDo,hDo,ty,h9e,Od,cv,Sie,ay,pDo,Pie,_Do,p9e,dr,ny,uDo,Xd,bDo,$ie,vDo,TDo,Iie,FDo,CDo,MDo,sy,EDo,jie,yDo,wDo,ADo,rt,ly,LDo,Nie,BDo,kDo,zd,xDo,Die,RDo,SDo,qie,PDo,$Do,IDo,Gie,jDo,NDo,iy,DDo,He,dy,qDo,Oie,GDo,ODo,tn,XDo,Xie,zDo,VDo,zie,WDo,QDo,Vie,HDo,UDo,JDo,Vd,fv,Wie,YDo,KDo,xN,ZDo,eqo,oqo,mv,Qie,rqo,tqo,RN,aqo,nqo,sqo,gv,Hie,lqo,iqo,SN,dqo,cqo,fqo,hv,mqo,Uie,gqo,hqo,Jie,pqo,_qo,Yie,uqo,bqo,cy,_9e,Wd,pv,Kie,fy,vqo,Zie,Tqo,u9e,cr,my,Fqo,Qd,Cqo,ede,Mqo,Eqo,ode,yqo,wqo,Aqo,gy,Lqo,rde,Bqo,kqo,xqo,tt,hy,Rqo,tde,Sqo,Pqo,Hd,$qo,ade,Iqo,jqo,nde,Nqo,Dqo,qqo,sde,Gqo,Oqo,py,Xqo,Ue,_y,zqo,lde,Vqo,Wqo,an,Qqo,ide,Hqo,Uqo,dde,Jqo,Yqo,cde,Kqo,Zqo,eGo,fde,_v,mde,oGo,rGo,PN,tGo,aGo,nGo,uv,sGo,gde,lGo,iGo,hde,dGo,cGo,pde,fGo,mGo,uy,b9e,Ud,bv,_de,by,gGo,ude,hGo,v9e,fr,vy,pGo,Jd,_Go,bde,uGo,bGo,vde,vGo,TGo,FGo,Ty,CGo,Tde,MGo,EGo,yGo,at,Fy,wGo,Fde,AGo,LGo,Yd,BGo,Cde,kGo,xGo,Mde,RGo,SGo,PGo,Ede,$Go,IGo,Cy,jGo,Je,My,NGo,yde,DGo,qGo,nn,GGo,wde,OGo,XGo,Ade,zGo,VGo,Lde,WGo,QGo,HGo,Bde,vv,kde,UGo,JGo,$N,YGo,KGo,ZGo,Tv,eOo,xde,oOo,rOo,Rde,tOo,aOo,Sde,nOo,sOo,Ey,T9e,Kd,Fv,Pde,yy,lOo,$de,iOo,F9e,mr,wy,dOo,Zd,cOo,Ide,fOo,mOo,jde,gOo,hOo,pOo,Ay,_Oo,Nde,uOo,bOo,vOo,nt,Ly,TOo,Dde,FOo,COo,ec,MOo,qde,EOo,yOo,Gde,wOo,AOo,LOo,Ode,BOo,kOo,By,xOo,Ye,ky,ROo,Xde,SOo,POo,sn,$Oo,zde,IOo,jOo,Vde,NOo,DOo,Wde,qOo,GOo,OOo,xy,Cv,Qde,XOo,zOo,IN,VOo,WOo,QOo,Mv,Hde,HOo,UOo,jN,JOo,YOo,KOo,Ev,ZOo,Ude,eXo,oXo,Jde,rXo,tXo,Yde,aXo,nXo,Ry,C9e,oc,yv,Kde,Sy,sXo,Zde,lXo,M9e,gr,Py,iXo,rc,dXo,ece,cXo,fXo,oce,mXo,gXo,hXo,$y,pXo,rce,_Xo,uXo,bXo,st,Iy,vXo,tce,TXo,FXo,tc,CXo,ace,MXo,EXo,nce,yXo,wXo,AXo,sce,LXo,BXo,jy,kXo,go,Ny,xXo,lce,RXo,SXo,ln,PXo,ice,$Xo,IXo,dce,jXo,NXo,cce,DXo,qXo,GXo,B,wv,fce,OXo,XXo,NN,zXo,VXo,WXo,Av,mce,QXo,HXo,DN,UXo,JXo,YXo,Lv,gce,KXo,ZXo,qN,ezo,ozo,rzo,Bv,hce,tzo,azo,GN,nzo,szo,lzo,kv,pce,izo,dzo,ON,czo,fzo,mzo,xv,_ce,gzo,hzo,XN,pzo,_zo,uzo,Rv,uce,bzo,vzo,zN,Tzo,Fzo,Czo,Sv,bce,Mzo,Ezo,VN,yzo,wzo,Azo,Pv,vce,Lzo,Bzo,WN,kzo,xzo,Rzo,$v,Tce,Szo,Pzo,QN,$zo,Izo,jzo,Iv,Fce,Nzo,Dzo,HN,qzo,Gzo,Ozo,jv,Cce,Xzo,zzo,UN,Vzo,Wzo,Qzo,Nv,Mce,Hzo,Uzo,JN,Jzo,Yzo,Kzo,Dv,Ece,Zzo,eVo,YN,oVo,rVo,tVo,qv,yce,aVo,nVo,KN,sVo,lVo,iVo,Gv,wce,dVo,cVo,ZN,fVo,mVo,gVo,Ss,Ace,hVo,pVo,eD,_Vo,uVo,oD,bVo,vVo,TVo,Ov,Lce,FVo,CVo,rD,MVo,EVo,yVo,Xv,Bce,wVo,AVo,tD,LVo,BVo,kVo,zv,kce,xVo,RVo,aD,SVo,PVo,$Vo,Vv,xce,IVo,jVo,nD,NVo,DVo,qVo,Wv,Rce,GVo,OVo,sD,XVo,zVo,VVo,Qv,Sce,WVo,QVo,lD,HVo,UVo,JVo,Hv,Pce,YVo,KVo,iD,ZVo,eWo,oWo,Uv,$ce,rWo,tWo,dD,aWo,nWo,sWo,Jv,Ice,lWo,iWo,cD,dWo,cWo,fWo,Yv,jce,mWo,gWo,fD,hWo,pWo,_Wo,Kv,Nce,uWo,bWo,mD,vWo,TWo,FWo,Zv,Dce,CWo,MWo,gD,EWo,yWo,wWo,e6,qce,AWo,LWo,hD,BWo,kWo,xWo,o6,Gce,RWo,SWo,pD,PWo,$Wo,IWo,r6,Oce,jWo,NWo,_D,DWo,qWo,GWo,t6,Xce,OWo,XWo,uD,zWo,VWo,WWo,a6,zce,QWo,HWo,bD,UWo,JWo,YWo,n6,Vce,KWo,ZWo,vD,eQo,oQo,rQo,s6,Wce,tQo,aQo,TD,nQo,sQo,lQo,l6,Qce,iQo,dQo,FD,cQo,fQo,mQo,i6,Hce,gQo,hQo,CD,pQo,_Qo,uQo,d6,Uce,bQo,vQo,MD,TQo,FQo,CQo,c6,Jce,MQo,EQo,ED,yQo,wQo,AQo,f6,Yce,LQo,BQo,yD,kQo,xQo,RQo,m6,Kce,SQo,PQo,wD,$Qo,IQo,jQo,Zce,NQo,DQo,Dy,E9e,ac,g6,efe,qy,qQo,ofe,GQo,y9e,hr,Gy,OQo,nc,XQo,rfe,zQo,VQo,tfe,WQo,QQo,HQo,Oy,UQo,afe,JQo,YQo,KQo,lt,Xy,ZQo,nfe,eHo,oHo,sc,rHo,sfe,tHo,aHo,lfe,nHo,sHo,lHo,ife,iHo,dHo,zy,cHo,ho,Vy,fHo,dfe,mHo,gHo,dn,hHo,cfe,pHo,_Ho,ffe,uHo,bHo,mfe,vHo,THo,FHo,H,h6,gfe,CHo,MHo,AD,EHo,yHo,wHo,p6,hfe,AHo,LHo,LD,BHo,kHo,xHo,_6,pfe,RHo,SHo,BD,PHo,$Ho,IHo,u6,_fe,jHo,NHo,kD,DHo,qHo,GHo,b6,ufe,OHo,XHo,xD,zHo,VHo,WHo,v6,bfe,QHo,HHo,RD,UHo,JHo,YHo,T6,vfe,KHo,ZHo,SD,eUo,oUo,rUo,F6,Tfe,tUo,aUo,PD,nUo,sUo,lUo,C6,Ffe,iUo,dUo,$D,cUo,fUo,mUo,M6,Cfe,gUo,hUo,ID,pUo,_Uo,uUo,E6,Mfe,bUo,vUo,jD,TUo,FUo,CUo,y6,Efe,MUo,EUo,ND,yUo,wUo,AUo,w6,yfe,LUo,BUo,DD,kUo,xUo,RUo,A6,wfe,SUo,PUo,qD,$Uo,IUo,jUo,L6,Afe,NUo,DUo,GD,qUo,GUo,OUo,B6,Lfe,XUo,zUo,OD,VUo,WUo,QUo,k6,Bfe,HUo,UUo,XD,JUo,YUo,KUo,x6,kfe,ZUo,eJo,zD,oJo,rJo,tJo,R6,xfe,aJo,nJo,VD,sJo,lJo,iJo,S6,Rfe,dJo,cJo,WD,fJo,mJo,gJo,P6,Sfe,hJo,pJo,QD,_Jo,uJo,bJo,$6,Pfe,vJo,TJo,HD,FJo,CJo,MJo,$fe,EJo,yJo,Wy,w9e,lc,I6,Ife,Qy,wJo,jfe,AJo,A9e,pr,Hy,LJo,ic,BJo,Nfe,kJo,xJo,Dfe,RJo,SJo,PJo,Uy,$Jo,qfe,IJo,jJo,NJo,it,Jy,DJo,Gfe,qJo,GJo,dc,OJo,Ofe,XJo,zJo,Xfe,VJo,WJo,QJo,zfe,HJo,UJo,Yy,JJo,po,Ky,YJo,Vfe,KJo,ZJo,cn,eYo,Wfe,oYo,rYo,Qfe,tYo,aYo,Hfe,nYo,sYo,lYo,he,j6,Ufe,iYo,dYo,UD,cYo,fYo,mYo,N6,Jfe,gYo,hYo,JD,pYo,_Yo,uYo,D6,Yfe,bYo,vYo,YD,TYo,FYo,CYo,q6,Kfe,MYo,EYo,KD,yYo,wYo,AYo,G6,Zfe,LYo,BYo,ZD,kYo,xYo,RYo,O6,eme,SYo,PYo,eq,$Yo,IYo,jYo,X6,ome,NYo,DYo,oq,qYo,GYo,OYo,z6,rme,XYo,zYo,rq,VYo,WYo,QYo,V6,tme,HYo,UYo,tq,JYo,YYo,KYo,W6,ame,ZYo,eKo,aq,oKo,rKo,tKo,nme,aKo,nKo,Zy,L9e,cc,Q6,sme,ew,sKo,lme,lKo,B9e,_r,ow,iKo,fc,dKo,ime,cKo,fKo,dme,mKo,gKo,hKo,rw,pKo,cme,_Ko,uKo,bKo,dt,tw,vKo,fme,TKo,FKo,mc,CKo,mme,MKo,EKo,gme,yKo,wKo,AKo,hme,LKo,BKo,aw,kKo,_o,nw,xKo,pme,RKo,SKo,fn,PKo,_me,$Ko,IKo,ume,jKo,NKo,bme,DKo,qKo,GKo,sw,H6,vme,OKo,XKo,nq,zKo,VKo,WKo,U6,Tme,QKo,HKo,sq,UKo,JKo,YKo,Fme,KKo,ZKo,lw,k9e,gc,J6,Cme,iw,eZo,Mme,oZo,x9e,ur,dw,rZo,hc,tZo,Eme,aZo,nZo,yme,sZo,lZo,iZo,cw,dZo,wme,cZo,fZo,mZo,ct,fw,gZo,Ame,hZo,pZo,pc,_Zo,Lme,uZo,bZo,Bme,vZo,TZo,FZo,kme,CZo,MZo,mw,EZo,uo,gw,yZo,xme,wZo,AZo,mn,LZo,Rme,BZo,kZo,Sme,xZo,RZo,Pme,SZo,PZo,$Zo,Y,Y6,$me,IZo,jZo,lq,NZo,DZo,qZo,K6,Ime,GZo,OZo,iq,XZo,zZo,VZo,Z6,jme,WZo,QZo,dq,HZo,UZo,JZo,eT,Nme,YZo,KZo,cq,ZZo,eer,oer,oT,Dme,rer,ter,fq,aer,ner,ser,rT,qme,ler,ier,mq,der,cer,fer,tT,Gme,mer,ger,gq,her,per,_er,aT,Ome,uer,ber,hq,ver,Ter,Fer,nT,Xme,Cer,Mer,pq,Eer,yer,wer,sT,zme,Aer,Ler,_q,Ber,ker,xer,lT,Vme,Rer,Ser,uq,Per,$er,Ier,iT,Wme,jer,Ner,bq,Der,qer,Ger,dT,Qme,Oer,Xer,vq,zer,Ver,Wer,cT,Hme,Qer,Her,Tq,Uer,Jer,Yer,fT,Ume,Ker,Zer,Fq,eor,oor,ror,mT,Jme,tor,aor,Cq,nor,sor,lor,gT,Yme,ior,dor,Mq,cor,mor,gor,hT,Kme,hor,por,Eq,_or,uor,bor,pT,Zme,vor,Tor,yq,For,Cor,Mor,_T,ege,Eor,yor,wq,wor,Aor,Lor,oge,Bor,kor,hw,R9e,_c,uT,rge,pw,xor,tge,Ror,S9e,br,_w,Sor,uc,Por,age,$or,Ior,nge,jor,Nor,Dor,uw,qor,sge,Gor,Oor,Xor,ft,bw,zor,lge,Vor,Wor,bc,Qor,ige,Hor,Uor,dge,Jor,Yor,Kor,cge,Zor,err,vw,orr,bo,Tw,rrr,fge,trr,arr,gn,nrr,mge,srr,lrr,gge,irr,drr,hge,crr,frr,mrr,pe,bT,pge,grr,hrr,Aq,prr,_rr,urr,vT,_ge,brr,vrr,Lq,Trr,Frr,Crr,TT,uge,Mrr,Err,Bq,yrr,wrr,Arr,FT,bge,Lrr,Brr,kq,krr,xrr,Rrr,CT,vge,Srr,Prr,xq,$rr,Irr,jrr,MT,Tge,Nrr,Drr,Rq,qrr,Grr,Orr,ET,Fge,Xrr,zrr,Sq,Vrr,Wrr,Qrr,yT,Cge,Hrr,Urr,Pq,Jrr,Yrr,Krr,wT,Mge,Zrr,etr,$q,otr,rtr,ttr,AT,Ege,atr,ntr,Iq,str,ltr,itr,yge,dtr,ctr,Fw,P9e,vc,LT,wge,Cw,ftr,Age,mtr,$9e,vr,Mw,gtr,Tc,htr,Lge,ptr,_tr,Bge,utr,btr,vtr,Ew,Ttr,kge,Ftr,Ctr,Mtr,mt,yw,Etr,xge,ytr,wtr,Fc,Atr,Rge,Ltr,Btr,Sge,ktr,xtr,Rtr,Pge,Str,Ptr,ww,$tr,vo,Aw,Itr,$ge,jtr,Ntr,hn,Dtr,Ige,qtr,Gtr,jge,Otr,Xtr,Nge,ztr,Vtr,Wtr,X,BT,Dge,Qtr,Htr,jq,Utr,Jtr,Ytr,kT,qge,Ktr,Ztr,Nq,ear,oar,rar,xT,Gge,tar,aar,Dq,nar,sar,lar,RT,Oge,iar,dar,qq,car,far,mar,ST,Xge,gar,har,Gq,par,_ar,uar,PT,zge,bar,Tar,Oq,Far,Car,Mar,$T,Vge,Ear,yar,Xq,war,Aar,Lar,IT,Wge,Bar,kar,zq,xar,Rar,Sar,jT,Qge,Par,$ar,Vq,Iar,jar,Nar,NT,Hge,Dar,qar,Wq,Gar,Oar,Xar,DT,Uge,zar,Var,Qq,War,Qar,Har,qT,Jge,Uar,Jar,Hq,Yar,Kar,Zar,GT,Yge,enr,onr,Uq,rnr,tnr,anr,OT,Kge,nnr,snr,Jq,lnr,inr,dnr,XT,Zge,cnr,fnr,Yq,mnr,gnr,hnr,zT,ehe,pnr,_nr,Kq,unr,bnr,vnr,VT,ohe,Tnr,Fnr,Zq,Cnr,Mnr,Enr,WT,rhe,ynr,wnr,eG,Anr,Lnr,Bnr,QT,the,knr,xnr,oG,Rnr,Snr,Pnr,HT,ahe,$nr,Inr,rG,jnr,Nnr,Dnr,UT,nhe,qnr,Gnr,tG,Onr,Xnr,znr,JT,she,Vnr,Wnr,aG,Qnr,Hnr,Unr,YT,lhe,Jnr,Ynr,nG,Knr,Znr,esr,KT,ihe,osr,rsr,sG,tsr,asr,nsr,ZT,dhe,ssr,lsr,lG,isr,dsr,csr,che,fsr,msr,Lw,I9e,Cc,e8,fhe,Bw,gsr,mhe,hsr,j9e,Tr,kw,psr,Mc,_sr,ghe,usr,bsr,hhe,vsr,Tsr,Fsr,xw,Csr,phe,Msr,Esr,ysr,gt,Rw,wsr,_he,Asr,Lsr,Ec,Bsr,uhe,ksr,xsr,bhe,Rsr,Ssr,Psr,vhe,$sr,Isr,Sw,jsr,To,Pw,Nsr,The,Dsr,qsr,pn,Gsr,Fhe,Osr,Xsr,Che,zsr,Vsr,Mhe,Wsr,Qsr,Hsr,te,o8,Ehe,Usr,Jsr,iG,Ysr,Ksr,Zsr,r8,yhe,elr,olr,dG,rlr,tlr,alr,t8,whe,nlr,slr,cG,llr,ilr,dlr,a8,Ahe,clr,flr,fG,mlr,glr,hlr,n8,Lhe,plr,_lr,mG,ulr,blr,vlr,s8,Bhe,Tlr,Flr,gG,Clr,Mlr,Elr,l8,khe,ylr,wlr,hG,Alr,Llr,Blr,i8,xhe,klr,xlr,pG,Rlr,Slr,Plr,d8,Rhe,$lr,Ilr,_G,jlr,Nlr,Dlr,c8,She,qlr,Glr,uG,Olr,Xlr,zlr,f8,Phe,Vlr,Wlr,bG,Qlr,Hlr,Ulr,m8,$he,Jlr,Ylr,vG,Klr,Zlr,eir,g8,Ihe,oir,rir,TG,tir,air,nir,h8,jhe,sir,lir,FG,iir,dir,cir,p8,Nhe,fir,mir,CG,gir,hir,pir,_8,Dhe,_ir,uir,MG,bir,vir,Tir,u8,qhe,Fir,Cir,EG,Mir,Eir,yir,Ghe,wir,Air,$w,N9e,yc,b8,Ohe,Iw,Lir,Xhe,Bir,D9e,Fr,jw,kir,wc,xir,zhe,Rir,Sir,Vhe,Pir,$ir,Iir,Nw,jir,Whe,Nir,Dir,qir,ht,Dw,Gir,Qhe,Oir,Xir,Ac,zir,Hhe,Vir,Wir,Uhe,Qir,Hir,Uir,Jhe,Jir,Yir,qw,Kir,Fo,Gw,Zir,Yhe,edr,odr,_n,rdr,Khe,tdr,adr,Zhe,ndr,sdr,epe,ldr,idr,ddr,ope,v8,rpe,cdr,fdr,yG,mdr,gdr,hdr,tpe,pdr,_dr,Ow,q9e,Lc,T8,ape,Xw,udr,npe,bdr,G9e,Cr,zw,vdr,Bc,Tdr,spe,Fdr,Cdr,lpe,Mdr,Edr,ydr,Vw,wdr,ipe,Adr,Ldr,Bdr,pt,Ww,kdr,dpe,xdr,Rdr,kc,Sdr,cpe,Pdr,$dr,fpe,Idr,jdr,Ndr,mpe,Ddr,qdr,Qw,Gdr,Co,Hw,Odr,gpe,Xdr,zdr,un,Vdr,hpe,Wdr,Qdr,ppe,Hdr,Udr,_pe,Jdr,Ydr,Kdr,K,F8,upe,Zdr,ecr,wG,ocr,rcr,tcr,C8,bpe,acr,ncr,AG,scr,lcr,icr,M8,vpe,dcr,ccr,LG,fcr,mcr,gcr,E8,Tpe,hcr,pcr,BG,_cr,ucr,bcr,y8,Fpe,vcr,Tcr,kG,Fcr,Ccr,Mcr,w8,Cpe,Ecr,ycr,xG,wcr,Acr,Lcr,A8,Mpe,Bcr,kcr,RG,xcr,Rcr,Scr,L8,Epe,Pcr,$cr,SG,Icr,jcr,Ncr,B8,ype,Dcr,qcr,PG,Gcr,Ocr,Xcr,k8,wpe,zcr,Vcr,$G,Wcr,Qcr,Hcr,x8,Ape,Ucr,Jcr,IG,Ycr,Kcr,Zcr,R8,Lpe,efr,ofr,jG,rfr,tfr,afr,S8,Bpe,nfr,sfr,NG,lfr,ifr,dfr,P8,kpe,cfr,ffr,DG,mfr,gfr,hfr,$8,xpe,pfr,_fr,qG,ufr,bfr,vfr,I8,Rpe,Tfr,Ffr,GG,Cfr,Mfr,Efr,j8,Spe,yfr,wfr,OG,Afr,Lfr,Bfr,N8,Ppe,kfr,xfr,XG,Rfr,Sfr,Pfr,D8,$pe,$fr,Ifr,zG,jfr,Nfr,Dfr,q8,Ipe,qfr,Gfr,VG,Ofr,Xfr,zfr,jpe,Vfr,Wfr,Uw,O9e,xc,G8,Npe,Jw,Qfr,Dpe,Hfr,X9e,Mr,Yw,Ufr,Rc,Jfr,qpe,Yfr,Kfr,Gpe,Zfr,emr,omr,Kw,rmr,Ope,tmr,amr,nmr,_t,Zw,smr,Xpe,lmr,imr,Sc,dmr,zpe,cmr,fmr,Vpe,mmr,gmr,hmr,Wpe,pmr,_mr,eA,umr,Mo,oA,bmr,Qpe,vmr,Tmr,bn,Fmr,Hpe,Cmr,Mmr,Upe,Emr,ymr,Jpe,wmr,Amr,Lmr,Z,O8,Ype,Bmr,kmr,WG,xmr,Rmr,Smr,X8,Kpe,Pmr,$mr,QG,Imr,jmr,Nmr,z8,Zpe,Dmr,qmr,HG,Gmr,Omr,Xmr,V8,e_e,zmr,Vmr,UG,Wmr,Qmr,Hmr,W8,o_e,Umr,Jmr,JG,Ymr,Kmr,Zmr,Q8,r_e,egr,ogr,YG,rgr,tgr,agr,H8,t_e,ngr,sgr,KG,lgr,igr,dgr,U8,a_e,cgr,fgr,ZG,mgr,ggr,hgr,J8,n_e,pgr,_gr,eO,ugr,bgr,vgr,Y8,s_e,Tgr,Fgr,oO,Cgr,Mgr,Egr,K8,l_e,ygr,wgr,rO,Agr,Lgr,Bgr,Z8,i_e,kgr,xgr,tO,Rgr,Sgr,Pgr,eF,d_e,$gr,Igr,aO,jgr,Ngr,Dgr,oF,c_e,qgr,Ggr,nO,Ogr,Xgr,zgr,rF,f_e,Vgr,Wgr,sO,Qgr,Hgr,Ugr,tF,m_e,Jgr,Ygr,lO,Kgr,Zgr,ehr,aF,g_e,ohr,rhr,iO,thr,ahr,nhr,nF,h_e,shr,lhr,dO,ihr,dhr,chr,sF,p_e,fhr,mhr,cO,ghr,hhr,phr,__e,_hr,uhr,rA,z9e,Pc,lF,u_e,tA,bhr,b_e,vhr,V9e,Er,aA,Thr,$c,Fhr,v_e,Chr,Mhr,T_e,Ehr,yhr,whr,nA,Ahr,F_e,Lhr,Bhr,khr,ut,sA,xhr,C_e,Rhr,Shr,Ic,Phr,M_e,$hr,Ihr,E_e,jhr,Nhr,Dhr,y_e,qhr,Ghr,lA,Ohr,Eo,iA,Xhr,w_e,zhr,Vhr,vn,Whr,A_e,Qhr,Hhr,L_e,Uhr,Jhr,B_e,Yhr,Khr,Zhr,k_e,iF,x_e,epr,opr,fO,rpr,tpr,apr,R_e,npr,spr,dA,W9e,jc,dF,S_e,cA,lpr,P_e,ipr,Q9e,yr,fA,dpr,Nc,cpr,$_e,fpr,mpr,I_e,gpr,hpr,ppr,mA,_pr,j_e,upr,bpr,vpr,bt,gA,Tpr,N_e,Fpr,Cpr,Dc,Mpr,D_e,Epr,ypr,q_e,wpr,Apr,Lpr,G_e,Bpr,kpr,hA,xpr,yo,pA,Rpr,O_e,Spr,Ppr,Tn,$pr,X_e,Ipr,jpr,z_e,Npr,Dpr,V_e,qpr,Gpr,Opr,W_e,cF,Q_e,Xpr,zpr,mO,Vpr,Wpr,Qpr,H_e,Hpr,Upr,_A,H9e,qc,fF,U_e,uA,Jpr,J_e,Ypr,U9e,wr,bA,Kpr,Gc,Zpr,Y_e,e_r,o_r,K_e,r_r,t_r,a_r,vA,n_r,Z_e,s_r,l_r,i_r,vt,TA,d_r,eue,c_r,f_r,Oc,m_r,oue,g_r,h_r,rue,p_r,__r,u_r,tue,b_r,v_r,FA,T_r,wo,CA,F_r,aue,C_r,M_r,Fn,E_r,nue,y_r,w_r,sue,A_r,L_r,lue,B_r,k_r,x_r,V,mF,iue,R_r,S_r,gO,P_r,$_r,I_r,gF,due,j_r,N_r,hO,D_r,q_r,G_r,hF,cue,O_r,X_r,pO,z_r,V_r,W_r,pF,fue,Q_r,H_r,_O,U_r,J_r,Y_r,_F,mue,K_r,Z_r,uO,eur,our,rur,uF,gue,tur,aur,bO,nur,sur,lur,bF,hue,iur,dur,vO,cur,fur,mur,vF,pue,gur,hur,TO,pur,_ur,uur,TF,_ue,bur,vur,FO,Tur,Fur,Cur,FF,uue,Mur,Eur,CO,yur,wur,Aur,CF,bue,Lur,Bur,MO,kur,xur,Rur,MF,vue,Sur,Pur,EO,$ur,Iur,jur,EF,Tue,Nur,Dur,yO,qur,Gur,Our,yF,Fue,Xur,zur,wO,Vur,Wur,Qur,wF,Cue,Hur,Uur,AO,Jur,Yur,Kur,AF,Mue,Zur,e2r,LO,o2r,r2r,t2r,LF,Eue,a2r,n2r,BO,s2r,l2r,i2r,BF,yue,d2r,c2r,kO,f2r,m2r,g2r,kF,wue,h2r,p2r,xO,_2r,u2r,b2r,xF,Aue,v2r,T2r,RO,F2r,C2r,M2r,RF,Lue,E2r,y2r,SO,w2r,A2r,L2r,SF,Bue,B2r,k2r,PO,x2r,R2r,S2r,PF,kue,P2r,$2r,$O,I2r,j2r,N2r,$F,xue,D2r,q2r,IO,G2r,O2r,X2r,Rue,z2r,V2r,MA,J9e,Xc,IF,Sue,EA,W2r,Pue,Q2r,Y9e,Ar,yA,H2r,zc,U2r,$ue,J2r,Y2r,Iue,K2r,Z2r,e1r,wA,o1r,jue,r1r,t1r,a1r,Tt,AA,n1r,Nue,s1r,l1r,Vc,i1r,Due,d1r,c1r,que,f1r,m1r,g1r,Gue,h1r,p1r,LA,_1r,Ao,BA,u1r,Oue,b1r,v1r,Cn,T1r,Xue,F1r,C1r,zue,M1r,E1r,Vue,y1r,w1r,A1r,Mn,jF,Wue,L1r,B1r,jO,k1r,x1r,R1r,NF,Que,S1r,P1r,NO,$1r,I1r,j1r,DF,Hue,N1r,D1r,DO,q1r,G1r,O1r,qF,Uue,X1r,z1r,qO,V1r,W1r,Q1r,Jue,H1r,U1r,kA,K9e,Wc,GF,Yue,xA,J1r,Kue,Y1r,Z9e,Lr,RA,K1r,Qc,Z1r,Zue,ebr,obr,e2e,rbr,tbr,abr,SA,nbr,o2e,sbr,lbr,ibr,Ft,PA,dbr,r2e,cbr,fbr,Hc,mbr,t2e,gbr,hbr,a2e,pbr,_br,ubr,n2e,bbr,vbr,$A,Tbr,Lo,IA,Fbr,s2e,Cbr,Mbr,En,Ebr,l2e,ybr,wbr,i2e,Abr,Lbr,d2e,Bbr,kbr,xbr,fe,OF,c2e,Rbr,Sbr,GO,Pbr,$br,Ibr,XF,f2e,jbr,Nbr,OO,Dbr,qbr,Gbr,zF,m2e,Obr,Xbr,XO,zbr,Vbr,Wbr,VF,g2e,Qbr,Hbr,zO,Ubr,Jbr,Ybr,WF,h2e,Kbr,Zbr,VO,e5r,o5r,r5r,QF,p2e,t5r,a5r,WO,n5r,s5r,l5r,HF,_2e,i5r,d5r,QO,c5r,f5r,m5r,UF,u2e,g5r,h5r,HO,p5r,_5r,u5r,JF,b2e,b5r,v5r,UO,T5r,F5r,C5r,YF,v2e,M5r,E5r,JO,y5r,w5r,A5r,KF,T2e,L5r,B5r,YO,k5r,x5r,R5r,F2e,S5r,P5r,jA,eBe,Uc,ZF,C2e,NA,$5r,M2e,I5r,oBe,Br,DA,j5r,Jc,N5r,E2e,D5r,q5r,y2e,G5r,O5r,X5r,qA,z5r,w2e,V5r,W5r,Q5r,Ct,GA,H5r,A2e,U5r,J5r,Yc,Y5r,L2e,K5r,Z5r,B2e,evr,ovr,rvr,k2e,tvr,avr,OA,nvr,Bo,XA,svr,x2e,lvr,ivr,yn,dvr,R2e,cvr,fvr,S2e,mvr,gvr,P2e,hvr,pvr,_vr,ve,eC,$2e,uvr,bvr,KO,vvr,Tvr,Fvr,oC,I2e,Cvr,Mvr,ZO,Evr,yvr,wvr,rC,j2e,Avr,Lvr,eX,Bvr,kvr,xvr,tC,N2e,Rvr,Svr,oX,Pvr,$vr,Ivr,aC,D2e,jvr,Nvr,rX,Dvr,qvr,Gvr,nC,q2e,Ovr,Xvr,tX,zvr,Vvr,Wvr,sC,G2e,Qvr,Hvr,aX,Uvr,Jvr,Yvr,lC,O2e,Kvr,Zvr,nX,e6r,o6r,r6r,iC,X2e,t6r,a6r,sX,n6r,s6r,l6r,z2e,i6r,d6r,zA,rBe,Kc,dC,V2e,VA,c6r,W2e,f6r,tBe,kr,WA,m6r,Zc,g6r,Q2e,h6r,p6r,H2e,_6r,u6r,b6r,QA,v6r,U2e,T6r,F6r,C6r,Mt,HA,M6r,J2e,E6r,y6r,ef,w6r,Y2e,A6r,L6r,K2e,B6r,k6r,x6r,Z2e,R6r,S6r,UA,P6r,ko,JA,$6r,e1e,I6r,j6r,wn,N6r,o1e,D6r,q6r,r1e,G6r,O6r,t1e,X6r,z6r,V6r,Te,cC,a1e,W6r,Q6r,lX,H6r,U6r,J6r,fC,n1e,Y6r,K6r,iX,Z6r,eTr,oTr,mC,s1e,rTr,tTr,dX,aTr,nTr,sTr,gC,l1e,lTr,iTr,cX,dTr,cTr,fTr,hC,i1e,mTr,gTr,fX,hTr,pTr,_Tr,pC,d1e,uTr,bTr,mX,vTr,TTr,FTr,_C,c1e,CTr,MTr,gX,ETr,yTr,wTr,uC,f1e,ATr,LTr,hX,BTr,kTr,xTr,bC,m1e,RTr,STr,pX,PTr,$Tr,ITr,g1e,jTr,NTr,YA,aBe,of,vC,h1e,KA,DTr,p1e,qTr,nBe,xr,ZA,GTr,rf,OTr,_1e,XTr,zTr,u1e,VTr,WTr,QTr,e0,HTr,b1e,UTr,JTr,YTr,Et,o0,KTr,v1e,ZTr,e8r,tf,o8r,T1e,r8r,t8r,F1e,a8r,n8r,s8r,C1e,l8r,i8r,r0,d8r,xo,t0,c8r,M1e,f8r,m8r,An,g8r,E1e,h8r,p8r,y1e,_8r,u8r,w1e,b8r,v8r,T8r,Fe,TC,A1e,F8r,C8r,_X,M8r,E8r,y8r,FC,L1e,w8r,A8r,uX,L8r,B8r,k8r,CC,B1e,x8r,R8r,bX,S8r,P8r,$8r,MC,k1e,I8r,j8r,vX,N8r,D8r,q8r,EC,x1e,G8r,O8r,TX,X8r,z8r,V8r,yC,R1e,W8r,Q8r,FX,H8r,U8r,J8r,wC,S1e,Y8r,K8r,CX,Z8r,eFr,oFr,AC,P1e,rFr,tFr,MX,aFr,nFr,sFr,LC,$1e,lFr,iFr,EX,dFr,cFr,fFr,I1e,mFr,gFr,a0,sBe,af,BC,j1e,n0,hFr,N1e,pFr,lBe,Rr,s0,_Fr,nf,uFr,D1e,bFr,vFr,q1e,TFr,FFr,CFr,l0,MFr,G1e,EFr,yFr,wFr,yt,i0,AFr,O1e,LFr,BFr,sf,kFr,X1e,xFr,RFr,z1e,SFr,PFr,$Fr,V1e,IFr,jFr,d0,NFr,Ro,c0,DFr,W1e,qFr,GFr,Ln,OFr,Q1e,XFr,zFr,H1e,VFr,WFr,U1e,QFr,HFr,UFr,Ce,kC,J1e,JFr,YFr,yX,KFr,ZFr,eCr,xC,Y1e,oCr,rCr,wX,tCr,aCr,nCr,RC,K1e,sCr,lCr,AX,iCr,dCr,cCr,SC,Z1e,fCr,mCr,LX,gCr,hCr,pCr,PC,ebe,_Cr,uCr,BX,bCr,vCr,TCr,$C,obe,FCr,CCr,kX,MCr,ECr,yCr,IC,rbe,wCr,ACr,xX,LCr,BCr,kCr,jC,tbe,xCr,RCr,RX,SCr,PCr,$Cr,NC,abe,ICr,jCr,SX,NCr,DCr,qCr,nbe,GCr,OCr,f0,iBe,lf,DC,sbe,m0,XCr,lbe,zCr,dBe,Sr,g0,VCr,df,WCr,ibe,QCr,HCr,dbe,UCr,JCr,YCr,h0,KCr,cbe,ZCr,e4r,o4r,wt,p0,r4r,fbe,t4r,a4r,cf,n4r,mbe,s4r,l4r,gbe,i4r,d4r,c4r,hbe,f4r,m4r,_0,g4r,So,u0,h4r,pbe,p4r,_4r,Bn,u4r,_be,b4r,v4r,ube,T4r,F4r,bbe,C4r,M4r,E4r,so,qC,vbe,y4r,w4r,PX,A4r,L4r,B4r,GC,Tbe,k4r,x4r,$X,R4r,S4r,P4r,OC,Fbe,$4r,I4r,IX,j4r,N4r,D4r,XC,Cbe,q4r,G4r,jX,O4r,X4r,z4r,zC,Mbe,V4r,W4r,NX,Q4r,H4r,U4r,VC,Ebe,J4r,Y4r,DX,K4r,Z4r,eMr,WC,ybe,oMr,rMr,qX,tMr,aMr,nMr,wbe,sMr,lMr,b0,cBe,ff,QC,Abe,v0,iMr,Lbe,dMr,fBe,Pr,T0,cMr,mf,fMr,Bbe,mMr,gMr,kbe,hMr,pMr,_Mr,F0,uMr,xbe,bMr,vMr,TMr,At,C0,FMr,Rbe,CMr,MMr,gf,EMr,Sbe,yMr,wMr,Pbe,AMr,LMr,BMr,$be,kMr,xMr,M0,RMr,Po,E0,SMr,Ibe,PMr,$Mr,kn,IMr,jbe,jMr,NMr,Nbe,DMr,qMr,Dbe,GMr,OMr,XMr,lo,HC,qbe,zMr,VMr,GX,WMr,QMr,HMr,UC,Gbe,UMr,JMr,OX,YMr,KMr,ZMr,JC,Obe,eEr,oEr,XX,rEr,tEr,aEr,YC,Xbe,nEr,sEr,zX,lEr,iEr,dEr,KC,zbe,cEr,fEr,VX,mEr,gEr,hEr,ZC,Vbe,pEr,_Er,WX,uEr,bEr,vEr,e4,Wbe,TEr,FEr,QX,CEr,MEr,EEr,Qbe,yEr,wEr,y0,mBe,hf,o4,Hbe,w0,AEr,Ube,LEr,gBe,$r,A0,BEr,pf,kEr,Jbe,xEr,REr,Ybe,SEr,PEr,$Er,L0,IEr,Kbe,jEr,NEr,DEr,Lt,B0,qEr,Zbe,GEr,OEr,_f,XEr,e5e,zEr,VEr,o5e,WEr,QEr,HEr,r5e,UEr,JEr,k0,YEr,$o,x0,KEr,t5e,ZEr,e3r,xn,o3r,a5e,r3r,t3r,n5e,a3r,n3r,s5e,s3r,l3r,i3r,l5e,r4,i5e,d3r,c3r,HX,f3r,m3r,g3r,d5e,h3r,p3r,R0,hBe,uf,t4,c5e,S0,_3r,f5e,u3r,pBe,Ir,P0,b3r,bf,v3r,m5e,T3r,F3r,g5e,C3r,M3r,E3r,$0,y3r,h5e,w3r,A3r,L3r,Bt,I0,B3r,p5e,k3r,x3r,vf,R3r,_5e,S3r,P3r,u5e,$3r,I3r,j3r,b5e,N3r,D3r,j0,q3r,Io,N0,G3r,v5e,O3r,X3r,Rn,z3r,T5e,V3r,W3r,F5e,Q3r,H3r,C5e,U3r,J3r,Y3r,D0,a4,M5e,K3r,Z3r,UX,eyr,oyr,ryr,n4,E5e,tyr,ayr,JX,nyr,syr,lyr,y5e,iyr,dyr,q0,_Be,Tf,s4,w5e,G0,cyr,A5e,fyr,uBe,jr,O0,myr,Ff,gyr,L5e,hyr,pyr,B5e,_yr,uyr,byr,X0,vyr,k5e,Tyr,Fyr,Cyr,kt,z0,Myr,x5e,Eyr,yyr,Cf,wyr,R5e,Ayr,Lyr,S5e,Byr,kyr,xyr,P5e,Ryr,Syr,V0,Pyr,jo,W0,$yr,$5e,Iyr,jyr,Sn,Nyr,I5e,Dyr,qyr,j5e,Gyr,Oyr,N5e,Xyr,zyr,Vyr,D5e,l4,q5e,Wyr,Qyr,YX,Hyr,Uyr,Jyr,G5e,Yyr,Kyr,Q0,bBe;return ce=new z({}),$a=new w({props:{code:'model = AutoModel.from_pretrained("bert-base-cased"),',highlighted:'model = AutoModel.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)'}}),dM=new z({}),cM=new w({props:{code:`from transformers import AutoConfig, AutoModel

AutoConfig.register("new-model", NewModelConfig)
AutoModel.register(NewModelConfig, NewModel),`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModel

AutoConfig.register(<span class="hljs-string">&quot;new-model&quot;</span>, NewModelConfig)
AutoModel.register(NewModelConfig, NewModel)`}}),Bf=new Zyr({props:{warning:"&lcub;true}",$$slots:{default:[fut]},$$scope:{ctx:yi}}}),fM=new z({}),mM=new E({props:{name:"class transformers.AutoConfig",anchor:"transformers.AutoConfig",parameters:[],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/configuration_auto.py#L518"}}),pM=new E({props:{name:"from_pretrained",anchor:"transformers.AutoConfig.from_pretrained",parameters:[{name:"pretrained_model_name_or_path",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/configuration_auto.py#L541",parametersDescription:[{anchor:"transformers.AutoConfig.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model configuration hosted inside a model repo on
huggingface.co. Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or
namespaced under a user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing a configuration file saved using the
<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig.save_pretrained">save_pretrained()</a> method, or the <a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> method,
e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a saved configuration JSON <em>file</em>, e.g.,
<code>./my_model_directory/configuration.json</code>.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.AutoConfig.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.AutoConfig.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download the model weights and configuration files and override the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.AutoConfig.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.AutoConfig.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.AutoConfig.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.AutoConfig.from_pretrained.return_unused_kwargs",description:`<strong>return_unused_kwargs</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
If <code>False</code>, then this function returns just the final configuration object.</p>
<p>If <code>True</code>, then this functions returns a <code>Tuple(config, unused_kwargs)</code> where <em>unused_kwargs</em> is a
dictionary consisting of the key/value pairs whose keys are not configuration attributes: i.e., the
part of <code>kwargs</code> which has not been used to update <code>config</code> and is otherwise ignored.`,name:"return_unused_kwargs"},{anchor:"transformers.AutoConfig.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.AutoConfig.from_pretrained.kwargs(additional",description:`<strong>kwargs(additional</strong> keyword arguments, <em>optional</em>) &#x2014;
The values in kwargs of any keys which are configuration attributes will be used to override the loaded
values. Behavior concerning key/value pairs whose keys are <em>not</em> configuration attributes is controlled
by the <code>return_unused_kwargs</code> keyword parameter.`,name:"kwargs(additional"}]}}),_M=new w({props:{code:`from transformers import AutoConfig

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-uncased")

# Download configuration from huggingface.co (user-uploaded) and cache.
config = AutoConfig.from_pretrained("dbmdz/bert-base-german-cased")

# If configuration file is in a directory (e.g., was saved using *save_pretrained('./test/saved_model/')*).
config = AutoConfig.from_pretrained("./test/bert_saved_model/")

# Load a specific configuration file.
config = AutoConfig.from_pretrained("./test/bert_saved_model/my_configuration.json")

# Change some config attributes when loading a pretrained config.
config = AutoConfig.from_pretrained("bert-base-uncased", output_attentions=True, foo=False)
config.output_attentions

config, unused_kwargs = AutoConfig.from_pretrained(
    "bert-base-uncased", output_attentions=True, foo=False, return_unused_kwargs=True
)
config.output_attentions

config.unused_kwargs,`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-uncased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co (user-uploaded) and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;dbmdz/bert-base-german-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># If configuration file is in a directory (e.g., was saved using *save_pretrained(&#x27;./test/saved_model/&#x27;)*).</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./test/bert_saved_model/&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Load a specific configuration file.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./test/bert_saved_model/my_configuration.json&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Change some config attributes when loading a pretrained config.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-uncased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>, foo=<span class="hljs-literal">False</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>config, unused_kwargs = AutoConfig.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;bert-base-uncased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>, foo=<span class="hljs-literal">False</span>, return_unused_kwargs=<span class="hljs-literal">True</span>
<span class="hljs-meta">... </span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>config.unused_kwargs
{<span class="hljs-string">&#x27;foo&#x27;</span>: <span class="hljs-literal">False</span>}`}}),uM=new E({props:{name:"register",anchor:"transformers.AutoConfig.register",parameters:[{name:"model_type",val:""},{name:"config",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/configuration_auto.py#L663",parametersDescription:[{anchor:"transformers.AutoConfig.register.model_type",description:"<strong>model_type</strong> (<code>str</code>) &#x2014; The model type like &#x201C;bert&#x201D; or &#x201C;gpt&#x201D;.",name:"model_type"},{anchor:"transformers.AutoConfig.register.config",description:'<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014; The config to register.',name:"config"}]}}),bM=new z({}),vM=new E({props:{name:"class transformers.AutoTokenizer",anchor:"transformers.AutoTokenizer",parameters:[],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/tokenization_auto.py#L351"}}),CM=new E({props:{name:"from_pretrained",anchor:"transformers.AutoTokenizer.from_pretrained",parameters:[{name:"pretrained_model_name_or_path",val:""},{name:"*inputs",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/tokenization_auto.py#L365",parametersDescription:[{anchor:"transformers.AutoTokenizer.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a predefined tokenizer hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing vocabulary files required by the tokenizer, for instance saved
using the <a href="/docs/transformers/pr_15682/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.save_pretrained">save_pretrained()</a> method, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a single saved vocabulary file if and only if the tokenizer only requires a
single vocabulary file (like Bert or XLNet), e.g.: <code>./my_model_directory/vocab.txt</code>. (Not
applicable to all derived classes)</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.AutoTokenizer.from_pretrained.inputs",description:`<strong>inputs</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the Tokenizer <code>__init__()</code> method.`,name:"inputs"},{anchor:"transformers.AutoTokenizer.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
The configuration object used to dertermine the tokenizer class to instantiate.`,name:"config"},{anchor:"transformers.AutoTokenizer.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.AutoTokenizer.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download the model weights and configuration files and override the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.AutoTokenizer.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.AutoTokenizer.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.AutoTokenizer.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.AutoTokenizer.from_pretrained.subfolder",description:`<strong>subfolder</strong> (<code>str</code>, <em>optional</em>) &#x2014;
In case the relevant files are located inside a subfolder of the model repo on huggingface.co (e.g. for
facebook/rag-token-base), specify it here.`,name:"subfolder"},{anchor:"transformers.AutoTokenizer.from_pretrained.use_fast",description:`<strong>use_fast</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to try to load the fast version of the tokenizer.`,name:"use_fast"},{anchor:"transformers.AutoTokenizer.from_pretrained.tokenizer_type",description:`<strong>tokenizer_type</strong> (<code>str</code>, <em>optional</em>) &#x2014;
Tokenizer type to be loaded.`,name:"tokenizer_type"},{anchor:"transformers.AutoTokenizer.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.AutoTokenizer.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Will be passed to the Tokenizer <code>__init__()</code> method. Can be used to set special tokens like
<code>bos_token</code>, <code>eos_token</code>, <code>unk_token</code>, <code>sep_token</code>, <code>pad_token</code>, <code>cls_token</code>, <code>mask_token</code>,
<code>additional_special_tokens</code>. See parameters in the <code>__init__()</code> for more details.`,name:"kwargs"}]}}),MM=new w({props:{code:`from transformers import AutoTokenizer

# Download vocabulary from huggingface.co and cache.
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

# Download vocabulary from huggingface.co (user-uploaded) and cache.
tokenizer = AutoTokenizer.from_pretrained("dbmdz/bert-base-german-cased")

# If vocabulary files are in a directory (e.g. tokenizer was saved using *save_pretrained('./test/saved_model/')*)
tokenizer = AutoTokenizer.from_pretrained("./test/bert_saved_model/"),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download vocabulary from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;bert-base-uncased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download vocabulary from huggingface.co (user-uploaded) and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;dbmdz/bert-base-german-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># If vocabulary files are in a directory (e.g. tokenizer was saved using *save_pretrained(&#x27;./test/saved_model/&#x27;)*)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;./test/bert_saved_model/&quot;</span>)`}}),EM=new E({props:{name:"register",anchor:"transformers.AutoTokenizer.register",parameters:[{name:"config_class",val:""},{name:"slow_tokenizer_class",val:" = None"},{name:"fast_tokenizer_class",val:" = None"}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/tokenization_auto.py#L561",parametersDescription:[{anchor:"transformers.AutoTokenizer.register.config_class",description:`<strong>config_class</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The configuration corresponding to the model to register.`,name:"config_class"},{anchor:"transformers.AutoTokenizer.register.slow_tokenizer_class",description:`<strong>slow_tokenizer_class</strong> (<code>PretrainedTokenizer</code>, <em>optional</em>) &#x2014;
The slow tokenizer to register.`,name:"slow_tokenizer_class"},{anchor:"transformers.AutoTokenizer.register.slow_tokenizer_class",description:`<strong>slow_tokenizer_class</strong> (<code>PretrainedTokenizerFast</code>, <em>optional</em>) &#x2014;
The fast tokenizer to register.`,name:"slow_tokenizer_class"}]}}),yM=new z({}),wM=new E({props:{name:"class transformers.AutoFeatureExtractor",anchor:"transformers.AutoFeatureExtractor",parameters:[],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/feature_extraction_auto.py#L169"}}),BM=new E({props:{name:"from_pretrained",anchor:"transformers.AutoFeatureExtractor.from_pretrained",parameters:[{name:"pretrained_model_name_or_path",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/feature_extraction_auto.py#L183",parametersDescription:[{anchor:"transformers.AutoFeatureExtractor.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
This can be either:</p>
<ul>
<li>a string, the <em>model id</em> of a pretrained feature_extractor hosted inside a model repo on
huggingface.co. Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or
namespaced under a user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>a path to a <em>directory</em> containing a feature extractor file saved using the
<a href="/docs/transformers/pr_15682/en/main_classes/feature_extractor#transformers.FeatureExtractionMixin.save_pretrained">save_pretrained()</a> method, e.g.,
<code>./my_model_directory/</code>.</li>
<li>a path or url to a saved feature extractor JSON <em>file</em>, e.g.,
<code>./my_model_directory/preprocessor_config.json</code>.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.AutoFeatureExtractor.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model feature extractor should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.AutoFeatureExtractor.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force to (re-)download the feature extractor files and override the cached versions
if they exist.`,name:"force_download"},{anchor:"transformers.AutoFeatureExtractor.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received file. Attempts to resume the download if such a file
exists.`,name:"resume_download"},{anchor:"transformers.AutoFeatureExtractor.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}.</code> The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.AutoFeatureExtractor.from_pretrained.use_auth_token",description:`<strong>use_auth_token</strong> (<code>str</code> or <em>bool</em>, <em>optional</em>) &#x2014;
The token to use as HTTP bearer authorization for remote files. If <code>True</code>, will use the token generated
when running <code>transformers-cli login</code> (stored in <code>~/.huggingface</code>).`,name:"use_auth_token"},{anchor:"transformers.AutoFeatureExtractor.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.AutoFeatureExtractor.from_pretrained.return_unused_kwargs",description:`<strong>return_unused_kwargs</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
If <code>False</code>, then this function returns just the final feature extractor object. If <code>True</code>, then this
functions returns a <code>Tuple(feature_extractor, unused_kwargs)</code> where <em>unused_kwargs</em> is a dictionary
consisting of the key/value pairs whose keys are not feature extractor attributes: i.e., the part of
<code>kwargs</code> which has not been used to update <code>feature_extractor</code> and is otherwise ignored.`,name:"return_unused_kwargs"},{anchor:"transformers.AutoFeatureExtractor.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.AutoFeatureExtractor.from_pretrained.kwargs",description:`<strong>kwargs</strong> (<code>Dict[str, Any]</code>, <em>optional</em>) &#x2014;
The values in kwargs of any keys which are feature extractor attributes will be used to override the
loaded values. Behavior concerning key/value pairs whose keys are <em>not</em> feature extractor attributes is
controlled by the <code>return_unused_kwargs</code> keyword parameter.`,name:"kwargs"}]}}),dh=new Zyr({props:{$$slots:{default:[mut]},$$scope:{ctx:yi}}}),kM=new w({props:{code:`from transformers import AutoFeatureExtractor

# Download feature extractor from huggingface.co and cache.
feature_extractor = AutoFeatureExtractor.from_pretrained("facebook/wav2vec2-base-960h")

# If feature extractor files are in a directory (e.g. feature extractor was saved using *save_pretrained('./test/saved_model/')*)
feature_extractor = AutoFeatureExtractor.from_pretrained("./test/saved_model/"),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoFeatureExtractor

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download feature extractor from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>feature_extractor = AutoFeatureExtractor.from_pretrained(<span class="hljs-string">&quot;facebook/wav2vec2-base-960h&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># If feature extractor files are in a directory (e.g. feature extractor was saved using *save_pretrained(&#x27;./test/saved_model/&#x27;)*)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>feature_extractor = AutoFeatureExtractor.from_pretrained(<span class="hljs-string">&quot;./test/saved_model/&quot;</span>)`}}),xM=new E({props:{name:"register",anchor:"transformers.AutoFeatureExtractor.register",parameters:[{name:"config_class",val:""},{name:"feature_extractor_class",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/feature_extraction_auto.py#L310",parametersDescription:[{anchor:"transformers.AutoFeatureExtractor.register.config_class",description:`<strong>config_class</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The configuration corresponding to the model to register.`,name:"config_class"},{anchor:"transformers.AutoFeatureExtractor.register.feature_extractor_class",description:"<strong>feature_extractor_class</strong> (<code>FeatureExtractorMixin</code>) &#x2014; The feature extractor to register.",name:"feature_extractor_class"}]}}),RM=new z({}),SM=new E({props:{name:"class transformers.AutoProcessor",anchor:"transformers.AutoProcessor",parameters:[],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/processing_auto.py#L71"}}),IM=new E({props:{name:"from_pretrained",anchor:"transformers.AutoProcessor.from_pretrained",parameters:[{name:"pretrained_model_name_or_path",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/processing_auto.py#L85",parametersDescription:[{anchor:"transformers.AutoProcessor.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
This can be either:</p>
<ul>
<li>a string, the <em>model id</em> of a pretrained feature_extractor hosted inside a model repo on
huggingface.co. Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or
namespaced under a user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>a path to a <em>directory</em> containing a processor files saved using the <code>save_pretrained()</code> method,
e.g., <code>./my_model_directory/</code>.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.AutoProcessor.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model feature extractor should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.AutoProcessor.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force to (re-)download the feature extractor files and override the cached versions
if they exist.`,name:"force_download"},{anchor:"transformers.AutoProcessor.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received file. Attempts to resume the download if such a file
exists.`,name:"resume_download"},{anchor:"transformers.AutoProcessor.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}.</code> The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.AutoProcessor.from_pretrained.use_auth_token",description:`<strong>use_auth_token</strong> (<code>str</code> or <em>bool</em>, <em>optional</em>) &#x2014;
The token to use as HTTP bearer authorization for remote files. If <code>True</code>, will use the token generated
when running <code>transformers-cli login</code> (stored in <code>~/.huggingface</code>).`,name:"use_auth_token"},{anchor:"transformers.AutoProcessor.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.AutoProcessor.from_pretrained.return_unused_kwargs",description:`<strong>return_unused_kwargs</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
If <code>False</code>, then this function returns just the final feature extractor object. If <code>True</code>, then this
functions returns a <code>Tuple(feature_extractor, unused_kwargs)</code> where <em>unused_kwargs</em> is a dictionary
consisting of the key/value pairs whose keys are not feature extractor attributes: i.e., the part of
<code>kwargs</code> which has not been used to update <code>feature_extractor</code> and is otherwise ignored.`,name:"return_unused_kwargs"},{anchor:"transformers.AutoProcessor.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.AutoProcessor.from_pretrained.kwargs",description:`<strong>kwargs</strong> (<code>Dict[str, Any]</code>, <em>optional</em>) &#x2014;
The values in kwargs of any keys which are feature extractor attributes will be used to override the
loaded values. Behavior concerning key/value pairs whose keys are <em>not</em> feature extractor attributes is
controlled by the <code>return_unused_kwargs</code> keyword parameter.`,name:"kwargs"}]}}),Th=new Zyr({props:{$$slots:{default:[gut]},$$scope:{ctx:yi}}}),jM=new w({props:{code:`from transformers import AutoProcessor

# Download processor from huggingface.co and cache.
processor = AutoProcessor.from_pretrained("facebook/wav2vec2-base-960h")

# If processor files are in a directory (e.g. processor was saved using *save_pretrained('./test/saved_model/')*)
processor = AutoProcessor.from_pretrained("./test/saved_model/"),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoProcessor

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download processor from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>processor = AutoProcessor.from_pretrained(<span class="hljs-string">&quot;facebook/wav2vec2-base-960h&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># If processor files are in a directory (e.g. processor was saved using *save_pretrained(&#x27;./test/saved_model/&#x27;)*)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>processor = AutoProcessor.from_pretrained(<span class="hljs-string">&quot;./test/saved_model/&quot;</span>)`}}),NM=new E({props:{name:"register",anchor:"transformers.AutoProcessor.register",parameters:[{name:"config_class",val:""},{name:"processor_class",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/processing_auto.py#L238",parametersDescription:[{anchor:"transformers.AutoProcessor.register.config_class",description:`<strong>config_class</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The configuration corresponding to the model to register.`,name:"config_class"},{anchor:"transformers.AutoProcessor.register.processor_class",description:"<strong>processor_class</strong> (<code>FeatureExtractorMixin</code>) &#x2014; The processor to register.",name:"processor_class"}]}}),DM=new z({}),qM=new E({props:{name:"class transformers.AutoModel",anchor:"transformers.AutoModel",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/modeling_auto.py#L673"}}),OM=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15682/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/albert#transformers.AlbertModel">AlbertModel</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/bart#transformers.BartModel">BartModel</a> (BART model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/beit#transformers.BeitConfig">BeitConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/beit#transformers.BeitModel">BeitModel</a> (BEiT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/bert#transformers.BertModel">BertModel</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/bert-generation#transformers.BertGenerationConfig">BertGenerationConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/bert-generation#transformers.BertGenerationEncoder">BertGenerationEncoder</a> (Bert Generation model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/big_bird#transformers.BigBirdModel">BigBirdModel</a> (BigBird model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusConfig">BigBirdPegasusConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusModel">BigBirdPegasusModel</a> (BigBirdPegasus model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/blenderbot#transformers.BlenderbotConfig">BlenderbotConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/blenderbot#transformers.BlenderbotModel">BlenderbotModel</a> (Blenderbot model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/blenderbot-small#transformers.BlenderbotSmallConfig">BlenderbotSmallConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/blenderbot-small#transformers.BlenderbotSmallModel">BlenderbotSmallModel</a> (BlenderbotSmall model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/clip#transformers.CLIPConfig">CLIPConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/clip#transformers.CLIPModel">CLIPModel</a> (CLIP model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/ctrl#transformers.CTRLConfig">CTRLConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/ctrl#transformers.CTRLModel">CTRLModel</a> (CTRL model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/camembert#transformers.CamembertModel">CamembertModel</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/canine#transformers.CanineConfig">CanineConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/canine#transformers.CanineModel">CanineModel</a> (Canine model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/convbert#transformers.ConvBertModel">ConvBertModel</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/convnext#transformers.ConvNextConfig">ConvNextConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/convnext#transformers.ConvNextModel">ConvNextModel</a> (ConvNext model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/dpr#transformers.DPRConfig">DPRConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/dpr#transformers.DPRQuestionEncoder">DPRQuestionEncoder</a> (DPR model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/deberta#transformers.DebertaModel">DebertaModel</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/deberta-v2#transformers.DebertaV2Model">DebertaV2Model</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/deit#transformers.DeiTConfig">DeiTConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/deit#transformers.DeiTModel">DeiTModel</a> (DeiT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/detr#transformers.DetrConfig">DetrConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/detr#transformers.DetrModel">DetrModel</a> (DETR model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/distilbert#transformers.DistilBertModel">DistilBertModel</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/electra#transformers.ElectraModel">ElectraModel</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/fnet#transformers.FNetConfig">FNetConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/fnet#transformers.FNetModel">FNetModel</a> (FNet model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/fsmt#transformers.FSMTConfig">FSMTConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/fsmt#transformers.FSMTModel">FSMTModel</a> (FairSeq Machine-Translation model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/flaubert#transformers.FlaubertModel">FlaubertModel</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/funnel#transformers.FunnelModel">FunnelModel</a> or <a href="/docs/transformers/pr_15682/en/model_doc/funnel#transformers.FunnelBaseModel">FunnelBaseModel</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/gpt2#transformers.GPT2Model">GPT2Model</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/gptj#transformers.GPTJConfig">GPTJConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/gptj#transformers.GPTJModel">GPTJModel</a> (GPT-J model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/gpt_neo#transformers.GPTNeoConfig">GPTNeoConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/gpt_neo#transformers.GPTNeoModel">GPTNeoModel</a> (GPT Neo model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/hubert#transformers.HubertConfig">HubertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/hubert#transformers.HubertModel">HubertModel</a> (Hubert model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/ibert#transformers.IBertConfig">IBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/ibert#transformers.IBertModel">IBertModel</a> (I-BERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/imagegpt#transformers.ImageGPTConfig">ImageGPTConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/imagegpt#transformers.ImageGPTModel">ImageGPTModel</a> (ImageGPT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/led#transformers.LEDConfig">LEDConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/led#transformers.LEDModel">LEDModel</a> (LED model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/layoutlm#transformers.LayoutLMModel">LayoutLMModel</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/layoutlmv2#transformers.LayoutLMv2Config">LayoutLMv2Config</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/layoutlmv2#transformers.LayoutLMv2Model">LayoutLMv2Model</a> (LayoutLMv2 model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/longformer#transformers.LongformerModel">LongformerModel</a> (Longformer model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/luke#transformers.LukeConfig">LukeConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/luke#transformers.LukeModel">LukeModel</a> (LUKE model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/lxmert#transformers.LxmertConfig">LxmertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/lxmert#transformers.LxmertModel">LxmertModel</a> (LXMERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/m2m_100#transformers.M2M100Config">M2M100Config</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/m2m_100#transformers.M2M100Model">M2M100Model</a> (M2M100 model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/mbart#transformers.MBartModel">MBartModel</a> (mBART model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/mpnet#transformers.MPNetModel">MPNetModel</a> (MPNet model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/mt5#transformers.MT5Config">MT5Config</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/mt5#transformers.MT5Model">MT5Model</a> (mT5 model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/marian#transformers.MarianConfig">MarianConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/marian#transformers.MarianModel">MarianModel</a> (Marian model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/maskformer#transformers.MaskFormerConfig">MaskFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/maskformer#transformers.MaskFormerModel">MaskFormerModel</a> (MaskFormer model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/megatron-bert#transformers.MegatronBertConfig">MegatronBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/megatron-bert#transformers.MegatronBertModel">MegatronBertModel</a> (MegatronBert model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/mobilebert#transformers.MobileBertModel">MobileBertModel</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/nystromformer#transformers.NystromformerConfig">NystromformerConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/nystromformer#transformers.NystromformerModel">NystromformerModel</a> (Nystromformer model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig">OpenAIGPTConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/openai-gpt#transformers.OpenAIGPTModel">OpenAIGPTModel</a> (OpenAI GPT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/plbart#transformers.PLBartConfig">PLBartConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/plbart#transformers.PLBartModel">PLBartModel</a> (PLBart model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/pegasus#transformers.PegasusConfig">PegasusConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/pegasus#transformers.PegasusModel">PegasusModel</a> (Pegasus model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/perceiver#transformers.PerceiverConfig">PerceiverConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/perceiver#transformers.PerceiverModel">PerceiverModel</a> (Perceiver model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/poolformer#transformers.PoolFormerConfig">PoolFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/poolformer#transformers.PoolFormerModel">PoolFormerModel</a> (PoolFormer model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/prophetnet#transformers.ProphetNetConfig">ProphetNetConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/prophetnet#transformers.ProphetNetModel">ProphetNetModel</a> (ProphetNet model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/qdqbert#transformers.QDQBertConfig">QDQBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/qdqbert#transformers.QDQBertModel">QDQBertModel</a> (QDQBert model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/reformer#transformers.ReformerConfig">ReformerConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/reformer#transformers.ReformerModel">ReformerModel</a> (Reformer model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/rembert#transformers.RemBertModel">RemBertModel</a> (RemBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/retribert#transformers.RetriBertConfig">RetriBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/retribert#transformers.RetriBertModel">RetriBertModel</a> (RetriBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/roformer#transformers.RoFormerModel">RoFormerModel</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/roberta#transformers.RobertaModel">RobertaModel</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/sew#transformers.SEWConfig">SEWConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/sew#transformers.SEWModel">SEWModel</a> (SEW model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/sew-d#transformers.SEWDConfig">SEWDConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/sew-d#transformers.SEWDModel">SEWDModel</a> (SEW-D model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/segformer#transformers.SegformerConfig">SegformerConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/segformer#transformers.SegformerModel">SegformerModel</a> (SegFormer model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/speech_to_text#transformers.Speech2TextConfig">Speech2TextConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/speech_to_text#transformers.Speech2TextModel">Speech2TextModel</a> (Speech2Text model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/splinter#transformers.SplinterConfig">SplinterConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/splinter#transformers.SplinterModel">SplinterModel</a> (Splinter model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/squeezebert#transformers.SqueezeBertConfig">SqueezeBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/squeezebert#transformers.SqueezeBertModel">SqueezeBertModel</a> (SqueezeBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/swin#transformers.SwinConfig">SwinConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/swin#transformers.SwinModel">SwinModel</a> (Swin model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/t5#transformers.T5Config">T5Config</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/t5#transformers.T5Model">T5Model</a> (T5 model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/tapas#transformers.TapasModel">TapasModel</a> (TAPAS model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/transfo-xl#transformers.TransfoXLConfig">TransfoXLConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/transfo-xl#transformers.TransfoXLModel">TransfoXLModel</a> (Transformer-XL model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/unispeech#transformers.UniSpeechConfig">UniSpeechConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/unispeech#transformers.UniSpeechModel">UniSpeechModel</a> (UniSpeech model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/unispeech-sat#transformers.UniSpeechSatConfig">UniSpeechSatConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/unispeech-sat#transformers.UniSpeechSatModel">UniSpeechSatModel</a> (UniSpeechSat model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/vit#transformers.ViTConfig">ViTConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/vit#transformers.ViTModel">ViTModel</a> (ViT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/vit_mae#transformers.ViTMAEConfig">ViTMAEConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/vit_mae#transformers.ViTMAEModel">ViTMAEModel</a> (ViTMAE model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/vilt#transformers.ViltConfig">ViltConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/vilt#transformers.ViltModel">ViltModel</a> (ViLT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/vision-text-dual-encoder#transformers.VisionTextDualEncoderConfig">VisionTextDualEncoderConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/vision-text-dual-encoder#transformers.VisionTextDualEncoderModel">VisionTextDualEncoderModel</a> (VisionTextDualEncoder model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/visual_bert#transformers.VisualBertConfig">VisualBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/visual_bert#transformers.VisualBertModel">VisualBertModel</a> (VisualBert model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/wav2vec2#transformers.Wav2Vec2Model">Wav2Vec2Model</a> (Wav2Vec2 model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/wavlm#transformers.WavLMConfig">WavLMConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/wavlm#transformers.WavLMModel">WavLMModel</a> (WavLM model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/xglm#transformers.XGLMConfig">XGLMConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/xglm#transformers.XGLMModel">XGLMModel</a> (XGLM model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/xlm#transformers.XLMModel">XLMModel</a> (XLM model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetConfig">XLMProphetNetConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetModel">XLMProphetNetModel</a> (XLMProphetNet model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/xlm-roberta#transformers.XLMRobertaModel">XLMRobertaModel</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLConfig">XLMRobertaXLConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLModel">XLMRobertaXLModel</a> (XLM-RoBERTa-XL model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/xlnet#transformers.XLNetModel">XLNetModel</a> (XLNet model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/yoso#transformers.YosoConfig">YosoConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/yoso#transformers.YosoModel">YosoModel</a> (YOSO model)</li>
</ul>`,name:"config"}]}}),XM=new w({props:{code:`from transformers import AutoConfig, AutoModel

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModel.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModel.from_config(config)`}}),zM=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),VM=new w({props:{code:`from transformers import AutoConfig, AutoModel

# Download model and configuration from huggingface.co and cache.
model = AutoModel.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModel.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModel.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModel.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModel.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModel.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),WM=new z({}),QM=new E({props:{name:"class transformers.AutoModelForPreTraining",anchor:"transformers.AutoModelForPreTraining",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/modeling_auto.py#L680"}}),UM=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15682/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/albert#transformers.AlbertForPreTraining">AlbertForPreTraining</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/bart#transformers.BartForConditionalGeneration">BartForConditionalGeneration</a> (BART model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/bert#transformers.BertForPreTraining">BertForPreTraining</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/big_bird#transformers.BigBirdForPreTraining">BigBirdForPreTraining</a> (BigBird model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/ctrl#transformers.CTRLConfig">CTRLConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/ctrl#transformers.CTRLLMHeadModel">CTRLLMHeadModel</a> (CTRL model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/camembert#transformers.CamembertForMaskedLM">CamembertForMaskedLM</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/deberta#transformers.DebertaForMaskedLM">DebertaForMaskedLM</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/deberta-v2#transformers.DebertaV2ForMaskedLM">DebertaV2ForMaskedLM</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/distilbert#transformers.DistilBertForMaskedLM">DistilBertForMaskedLM</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/electra#transformers.ElectraForPreTraining">ElectraForPreTraining</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/fnet#transformers.FNetConfig">FNetConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/fnet#transformers.FNetForPreTraining">FNetForPreTraining</a> (FNet model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/fsmt#transformers.FSMTConfig">FSMTConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/fsmt#transformers.FSMTForConditionalGeneration">FSMTForConditionalGeneration</a> (FairSeq Machine-Translation model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/flaubert#transformers.FlaubertWithLMHeadModel">FlaubertWithLMHeadModel</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/funnel#transformers.FunnelForPreTraining">FunnelForPreTraining</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/gpt2#transformers.GPT2LMHeadModel">GPT2LMHeadModel</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/ibert#transformers.IBertConfig">IBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/ibert#transformers.IBertForMaskedLM">IBertForMaskedLM</a> (I-BERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/layoutlm#transformers.LayoutLMForMaskedLM">LayoutLMForMaskedLM</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/longformer#transformers.LongformerForMaskedLM">LongformerForMaskedLM</a> (Longformer model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/lxmert#transformers.LxmertConfig">LxmertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/lxmert#transformers.LxmertForPreTraining">LxmertForPreTraining</a> (LXMERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/mpnet#transformers.MPNetForMaskedLM">MPNetForMaskedLM</a> (MPNet model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/megatron-bert#transformers.MegatronBertConfig">MegatronBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/megatron-bert#transformers.MegatronBertForPreTraining">MegatronBertForPreTraining</a> (MegatronBert model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/mobilebert#transformers.MobileBertForPreTraining">MobileBertForPreTraining</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig">OpenAIGPTConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/openai-gpt#transformers.OpenAIGPTLMHeadModel">OpenAIGPTLMHeadModel</a> (OpenAI GPT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/retribert#transformers.RetriBertConfig">RetriBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/retribert#transformers.RetriBertModel">RetriBertModel</a> (RetriBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/roberta#transformers.RobertaForMaskedLM">RobertaForMaskedLM</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/squeezebert#transformers.SqueezeBertConfig">SqueezeBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/squeezebert#transformers.SqueezeBertForMaskedLM">SqueezeBertForMaskedLM</a> (SqueezeBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/t5#transformers.T5Config">T5Config</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/t5#transformers.T5ForConditionalGeneration">T5ForConditionalGeneration</a> (T5 model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/tapas#transformers.TapasForMaskedLM">TapasForMaskedLM</a> (TAPAS model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/transfo-xl#transformers.TransfoXLConfig">TransfoXLConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/transfo-xl#transformers.TransfoXLLMHeadModel">TransfoXLLMHeadModel</a> (Transformer-XL model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/unispeech#transformers.UniSpeechConfig">UniSpeechConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/unispeech#transformers.UniSpeechForPreTraining">UniSpeechForPreTraining</a> (UniSpeech model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/unispeech-sat#transformers.UniSpeechSatConfig">UniSpeechSatConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/unispeech-sat#transformers.UniSpeechSatForPreTraining">UniSpeechSatForPreTraining</a> (UniSpeechSat model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/vit_mae#transformers.ViTMAEConfig">ViTMAEConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/vit_mae#transformers.ViTMAEForPreTraining">ViTMAEForPreTraining</a> (ViTMAE model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/visual_bert#transformers.VisualBertConfig">VisualBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/visual_bert#transformers.VisualBertForPreTraining">VisualBertForPreTraining</a> (VisualBert model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/wav2vec2#transformers.Wav2Vec2ForPreTraining">Wav2Vec2ForPreTraining</a> (Wav2Vec2 model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/xlm#transformers.XLMWithLMHeadModel">XLMWithLMHeadModel</a> (XLM model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/xlm-roberta#transformers.XLMRobertaForMaskedLM">XLMRobertaForMaskedLM</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLConfig">XLMRobertaXLConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForMaskedLM">XLMRobertaXLForMaskedLM</a> (XLM-RoBERTa-XL model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/xlnet#transformers.XLNetLMHeadModel">XLNetLMHeadModel</a> (XLNet model)</li>
</ul>`,name:"config"}]}}),JM=new w({props:{code:`from transformers import AutoConfig, AutoModelForPreTraining

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForPreTraining.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForPreTraining

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForPreTraining.from_config(config)`}}),YM=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),KM=new w({props:{code:`from transformers import AutoConfig, AutoModelForPreTraining

# Download model and configuration from huggingface.co and cache.
model = AutoModelForPreTraining.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForPreTraining.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForPreTraining.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForPreTraining

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForPreTraining.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForPreTraining.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForPreTraining.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),ZM=new z({}),eE=new E({props:{name:"class transformers.AutoModelForCausalLM",anchor:"transformers.AutoModelForCausalLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/modeling_auto.py#L695"}}),rE=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15682/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/bart#transformers.BartForCausalLM">BartForCausalLM</a> (BART model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/bert#transformers.BertLMHeadModel">BertLMHeadModel</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/bert-generation#transformers.BertGenerationConfig">BertGenerationConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/bert-generation#transformers.BertGenerationDecoder">BertGenerationDecoder</a> (Bert Generation model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/big_bird#transformers.BigBirdForCausalLM">BigBirdForCausalLM</a> (BigBird model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusConfig">BigBirdPegasusConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusForCausalLM">BigBirdPegasusForCausalLM</a> (BigBirdPegasus model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/blenderbot#transformers.BlenderbotConfig">BlenderbotConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/blenderbot#transformers.BlenderbotForCausalLM">BlenderbotForCausalLM</a> (Blenderbot model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/blenderbot-small#transformers.BlenderbotSmallConfig">BlenderbotSmallConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/blenderbot-small#transformers.BlenderbotSmallForCausalLM">BlenderbotSmallForCausalLM</a> (BlenderbotSmall model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/ctrl#transformers.CTRLConfig">CTRLConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/ctrl#transformers.CTRLLMHeadModel">CTRLLMHeadModel</a> (CTRL model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/camembert#transformers.CamembertForCausalLM">CamembertForCausalLM</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/electra#transformers.ElectraForCausalLM">ElectraForCausalLM</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/gpt2#transformers.GPT2LMHeadModel">GPT2LMHeadModel</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/gptj#transformers.GPTJConfig">GPTJConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/gptj#transformers.GPTJForCausalLM">GPTJForCausalLM</a> (GPT-J model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/gpt_neo#transformers.GPTNeoConfig">GPTNeoConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/gpt_neo#transformers.GPTNeoForCausalLM">GPTNeoForCausalLM</a> (GPT Neo model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/mbart#transformers.MBartForCausalLM">MBartForCausalLM</a> (mBART model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/marian#transformers.MarianConfig">MarianConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/marian#transformers.MarianForCausalLM">MarianForCausalLM</a> (Marian model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/megatron-bert#transformers.MegatronBertConfig">MegatronBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/megatron-bert#transformers.MegatronBertForCausalLM">MegatronBertForCausalLM</a> (MegatronBert model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig">OpenAIGPTConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/openai-gpt#transformers.OpenAIGPTLMHeadModel">OpenAIGPTLMHeadModel</a> (OpenAI GPT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/plbart#transformers.PLBartConfig">PLBartConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/plbart#transformers.PLBartForCausalLM">PLBartForCausalLM</a> (PLBart model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/pegasus#transformers.PegasusConfig">PegasusConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/pegasus#transformers.PegasusForCausalLM">PegasusForCausalLM</a> (Pegasus model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/prophetnet#transformers.ProphetNetConfig">ProphetNetConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/prophetnet#transformers.ProphetNetForCausalLM">ProphetNetForCausalLM</a> (ProphetNet model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/qdqbert#transformers.QDQBertConfig">QDQBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/qdqbert#transformers.QDQBertLMHeadModel">QDQBertLMHeadModel</a> (QDQBert model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/reformer#transformers.ReformerConfig">ReformerConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/reformer#transformers.ReformerModelWithLMHead">ReformerModelWithLMHead</a> (Reformer model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/rembert#transformers.RemBertForCausalLM">RemBertForCausalLM</a> (RemBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/roformer#transformers.RoFormerForCausalLM">RoFormerForCausalLM</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/roberta#transformers.RobertaForCausalLM">RobertaForCausalLM</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/speech_to_text_2#transformers.Speech2Text2Config">Speech2Text2Config</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/speech_to_text_2#transformers.Speech2Text2ForCausalLM">Speech2Text2ForCausalLM</a> (Speech2Text2 model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/trocr#transformers.TrOCRConfig">TrOCRConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/trocr#transformers.TrOCRForCausalLM">TrOCRForCausalLM</a> (TrOCR model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/transfo-xl#transformers.TransfoXLConfig">TransfoXLConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/transfo-xl#transformers.TransfoXLLMHeadModel">TransfoXLLMHeadModel</a> (Transformer-XL model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/xglm#transformers.XGLMConfig">XGLMConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/xglm#transformers.XGLMForCausalLM">XGLMForCausalLM</a> (XGLM model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/xlm#transformers.XLMWithLMHeadModel">XLMWithLMHeadModel</a> (XLM model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetConfig">XLMProphetNetConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetForCausalLM">XLMProphetNetForCausalLM</a> (XLMProphetNet model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/xlm-roberta#transformers.XLMRobertaForCausalLM">XLMRobertaForCausalLM</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLConfig">XLMRobertaXLConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForCausalLM">XLMRobertaXLForCausalLM</a> (XLM-RoBERTa-XL model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/xlnet#transformers.XLNetLMHeadModel">XLNetLMHeadModel</a> (XLNet model)</li>
</ul>`,name:"config"}]}}),tE=new w({props:{code:`from transformers import AutoConfig, AutoModelForCausalLM

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForCausalLM.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForCausalLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCausalLM.from_config(config)`}}),aE=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),nE=new w({props:{code:`from transformers import AutoConfig, AutoModelForCausalLM

# Download model and configuration from huggingface.co and cache.
model = AutoModelForCausalLM.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForCausalLM.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForCausalLM.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForCausalLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCausalLM.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),sE=new z({}),lE=new E({props:{name:"class transformers.AutoModelForMaskedLM",anchor:"transformers.AutoModelForMaskedLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/modeling_auto.py#L702"}}),dE=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15682/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/albert#transformers.AlbertForMaskedLM">AlbertForMaskedLM</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/bart#transformers.BartForConditionalGeneration">BartForConditionalGeneration</a> (BART model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/bert#transformers.BertForMaskedLM">BertForMaskedLM</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/big_bird#transformers.BigBirdForMaskedLM">BigBirdForMaskedLM</a> (BigBird model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/camembert#transformers.CamembertForMaskedLM">CamembertForMaskedLM</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/convbert#transformers.ConvBertForMaskedLM">ConvBertForMaskedLM</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/deberta#transformers.DebertaForMaskedLM">DebertaForMaskedLM</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/deberta-v2#transformers.DebertaV2ForMaskedLM">DebertaV2ForMaskedLM</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/distilbert#transformers.DistilBertForMaskedLM">DistilBertForMaskedLM</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/electra#transformers.ElectraForMaskedLM">ElectraForMaskedLM</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/fnet#transformers.FNetConfig">FNetConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/fnet#transformers.FNetForMaskedLM">FNetForMaskedLM</a> (FNet model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/flaubert#transformers.FlaubertWithLMHeadModel">FlaubertWithLMHeadModel</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/funnel#transformers.FunnelForMaskedLM">FunnelForMaskedLM</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/ibert#transformers.IBertConfig">IBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/ibert#transformers.IBertForMaskedLM">IBertForMaskedLM</a> (I-BERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/layoutlm#transformers.LayoutLMForMaskedLM">LayoutLMForMaskedLM</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/longformer#transformers.LongformerForMaskedLM">LongformerForMaskedLM</a> (Longformer model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/mbart#transformers.MBartForConditionalGeneration">MBartForConditionalGeneration</a> (mBART model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/mpnet#transformers.MPNetForMaskedLM">MPNetForMaskedLM</a> (MPNet model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/megatron-bert#transformers.MegatronBertConfig">MegatronBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/megatron-bert#transformers.MegatronBertForMaskedLM">MegatronBertForMaskedLM</a> (MegatronBert model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/mobilebert#transformers.MobileBertForMaskedLM">MobileBertForMaskedLM</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/nystromformer#transformers.NystromformerConfig">NystromformerConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/nystromformer#transformers.NystromformerForMaskedLM">NystromformerForMaskedLM</a> (Nystromformer model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/perceiver#transformers.PerceiverConfig">PerceiverConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/perceiver#transformers.PerceiverForMaskedLM">PerceiverForMaskedLM</a> (Perceiver model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/qdqbert#transformers.QDQBertConfig">QDQBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/qdqbert#transformers.QDQBertForMaskedLM">QDQBertForMaskedLM</a> (QDQBert model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/reformer#transformers.ReformerConfig">ReformerConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/reformer#transformers.ReformerForMaskedLM">ReformerForMaskedLM</a> (Reformer model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/rembert#transformers.RemBertForMaskedLM">RemBertForMaskedLM</a> (RemBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/roformer#transformers.RoFormerForMaskedLM">RoFormerForMaskedLM</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/roberta#transformers.RobertaForMaskedLM">RobertaForMaskedLM</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/squeezebert#transformers.SqueezeBertConfig">SqueezeBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/squeezebert#transformers.SqueezeBertForMaskedLM">SqueezeBertForMaskedLM</a> (SqueezeBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/tapas#transformers.TapasForMaskedLM">TapasForMaskedLM</a> (TAPAS model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <code>Wav2Vec2ForMaskedLM</code>(Wav2Vec2 model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/xlm#transformers.XLMWithLMHeadModel">XLMWithLMHeadModel</a> (XLM model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/xlm-roberta#transformers.XLMRobertaForMaskedLM">XLMRobertaForMaskedLM</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLConfig">XLMRobertaXLConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForMaskedLM">XLMRobertaXLForMaskedLM</a> (XLM-RoBERTa-XL model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/yoso#transformers.YosoConfig">YosoConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/yoso#transformers.YosoForMaskedLM">YosoForMaskedLM</a> (YOSO model)</li>
</ul>`,name:"config"}]}}),cE=new w({props:{code:`from transformers import AutoConfig, AutoModelForMaskedLM

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForMaskedLM.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForMaskedLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMaskedLM.from_config(config)`}}),fE=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),mE=new w({props:{code:`from transformers import AutoConfig, AutoModelForMaskedLM

# Download model and configuration from huggingface.co and cache.
model = AutoModelForMaskedLM.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForMaskedLM.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForMaskedLM.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForMaskedLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMaskedLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMaskedLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMaskedLM.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),gE=new z({}),hE=new E({props:{name:"class transformers.AutoModelForSeq2SeqLM",anchor:"transformers.AutoModelForSeq2SeqLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/modeling_auto.py#L709"}}),_E=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15682/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/bart#transformers.BartForConditionalGeneration">BartForConditionalGeneration</a> (BART model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusConfig">BigBirdPegasusConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusForConditionalGeneration">BigBirdPegasusForConditionalGeneration</a> (BigBirdPegasus model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/blenderbot#transformers.BlenderbotConfig">BlenderbotConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/blenderbot#transformers.BlenderbotForConditionalGeneration">BlenderbotForConditionalGeneration</a> (Blenderbot model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/blenderbot-small#transformers.BlenderbotSmallConfig">BlenderbotSmallConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/blenderbot-small#transformers.BlenderbotSmallForConditionalGeneration">BlenderbotSmallForConditionalGeneration</a> (BlenderbotSmall model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/encoder-decoder#transformers.EncoderDecoderConfig">EncoderDecoderConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/encoder-decoder#transformers.EncoderDecoderModel">EncoderDecoderModel</a> (Encoder decoder model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/fsmt#transformers.FSMTConfig">FSMTConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/fsmt#transformers.FSMTForConditionalGeneration">FSMTForConditionalGeneration</a> (FairSeq Machine-Translation model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/led#transformers.LEDConfig">LEDConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/led#transformers.LEDForConditionalGeneration">LEDForConditionalGeneration</a> (LED model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/m2m_100#transformers.M2M100Config">M2M100Config</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/m2m_100#transformers.M2M100ForConditionalGeneration">M2M100ForConditionalGeneration</a> (M2M100 model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/mbart#transformers.MBartForConditionalGeneration">MBartForConditionalGeneration</a> (mBART model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/mt5#transformers.MT5Config">MT5Config</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/mt5#transformers.MT5ForConditionalGeneration">MT5ForConditionalGeneration</a> (mT5 model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/marian#transformers.MarianConfig">MarianConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/marian#transformers.MarianMTModel">MarianMTModel</a> (Marian model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/plbart#transformers.PLBartConfig">PLBartConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/plbart#transformers.PLBartForConditionalGeneration">PLBartForConditionalGeneration</a> (PLBart model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/pegasus#transformers.PegasusConfig">PegasusConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/pegasus#transformers.PegasusForConditionalGeneration">PegasusForConditionalGeneration</a> (Pegasus model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/prophetnet#transformers.ProphetNetConfig">ProphetNetConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/prophetnet#transformers.ProphetNetForConditionalGeneration">ProphetNetForConditionalGeneration</a> (ProphetNet model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/t5#transformers.T5Config">T5Config</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/t5#transformers.T5ForConditionalGeneration">T5ForConditionalGeneration</a> (T5 model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetConfig">XLMProphetNetConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetForConditionalGeneration">XLMProphetNetForConditionalGeneration</a> (XLMProphetNet model)</li>
</ul>`,name:"config"}]}}),uE=new w({props:{code:`from transformers import AutoConfig, AutoModelForSeq2SeqLM

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("t5-base")
model = AutoModelForSeq2SeqLM.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForSeq2SeqLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;t5-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSeq2SeqLM.from_config(config)`}}),bE=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),vE=new w({props:{code:`from transformers import AutoConfig, AutoModelForSeq2SeqLM

# Download model and configuration from huggingface.co and cache.
model = AutoModelForSeq2SeqLM.from_pretrained("t5-base")

# Update configuration during loading
model = AutoModelForSeq2SeqLM.from_pretrained("t5-base", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/t5_tf_model_config.json")
model = AutoModelForSeq2SeqLM.from_pretrained(
    "./tf_model/t5_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForSeq2SeqLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSeq2SeqLM.from_pretrained(<span class="hljs-string">&quot;t5-base&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSeq2SeqLM.from_pretrained(<span class="hljs-string">&quot;t5-base&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/t5_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSeq2SeqLM.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/t5_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),TE=new z({}),FE=new E({props:{name:"class transformers.AutoModelForSequenceClassification",anchor:"transformers.AutoModelForSequenceClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/modeling_auto.py#L718"}}),ME=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15682/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/albert#transformers.AlbertForSequenceClassification">AlbertForSequenceClassification</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/bart#transformers.BartForSequenceClassification">BartForSequenceClassification</a> (BART model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/bert#transformers.BertForSequenceClassification">BertForSequenceClassification</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/big_bird#transformers.BigBirdForSequenceClassification">BigBirdForSequenceClassification</a> (BigBird model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusConfig">BigBirdPegasusConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusForSequenceClassification">BigBirdPegasusForSequenceClassification</a> (BigBirdPegasus model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/ctrl#transformers.CTRLConfig">CTRLConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/ctrl#transformers.CTRLForSequenceClassification">CTRLForSequenceClassification</a> (CTRL model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/camembert#transformers.CamembertForSequenceClassification">CamembertForSequenceClassification</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/canine#transformers.CanineConfig">CanineConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/canine#transformers.CanineForSequenceClassification">CanineForSequenceClassification</a> (Canine model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/convbert#transformers.ConvBertForSequenceClassification">ConvBertForSequenceClassification</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/deberta#transformers.DebertaForSequenceClassification">DebertaForSequenceClassification</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/deberta-v2#transformers.DebertaV2ForSequenceClassification">DebertaV2ForSequenceClassification</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/distilbert#transformers.DistilBertForSequenceClassification">DistilBertForSequenceClassification</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/electra#transformers.ElectraForSequenceClassification">ElectraForSequenceClassification</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/fnet#transformers.FNetConfig">FNetConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/fnet#transformers.FNetForSequenceClassification">FNetForSequenceClassification</a> (FNet model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/flaubert#transformers.FlaubertForSequenceClassification">FlaubertForSequenceClassification</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/funnel#transformers.FunnelForSequenceClassification">FunnelForSequenceClassification</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/gpt2#transformers.GPT2ForSequenceClassification">GPT2ForSequenceClassification</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/gptj#transformers.GPTJConfig">GPTJConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/gptj#transformers.GPTJForSequenceClassification">GPTJForSequenceClassification</a> (GPT-J model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/gpt_neo#transformers.GPTNeoConfig">GPTNeoConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/gpt_neo#transformers.GPTNeoForSequenceClassification">GPTNeoForSequenceClassification</a> (GPT Neo model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/ibert#transformers.IBertConfig">IBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/ibert#transformers.IBertForSequenceClassification">IBertForSequenceClassification</a> (I-BERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/led#transformers.LEDConfig">LEDConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/led#transformers.LEDForSequenceClassification">LEDForSequenceClassification</a> (LED model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/layoutlm#transformers.LayoutLMForSequenceClassification">LayoutLMForSequenceClassification</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/layoutlmv2#transformers.LayoutLMv2Config">LayoutLMv2Config</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/layoutlmv2#transformers.LayoutLMv2ForSequenceClassification">LayoutLMv2ForSequenceClassification</a> (LayoutLMv2 model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/longformer#transformers.LongformerForSequenceClassification">LongformerForSequenceClassification</a> (Longformer model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/mbart#transformers.MBartForSequenceClassification">MBartForSequenceClassification</a> (mBART model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/mpnet#transformers.MPNetForSequenceClassification">MPNetForSequenceClassification</a> (MPNet model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/megatron-bert#transformers.MegatronBertConfig">MegatronBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/megatron-bert#transformers.MegatronBertForSequenceClassification">MegatronBertForSequenceClassification</a> (MegatronBert model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/mobilebert#transformers.MobileBertForSequenceClassification">MobileBertForSequenceClassification</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/nystromformer#transformers.NystromformerConfig">NystromformerConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/nystromformer#transformers.NystromformerForSequenceClassification">NystromformerForSequenceClassification</a> (Nystromformer model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig">OpenAIGPTConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/openai-gpt#transformers.OpenAIGPTForSequenceClassification">OpenAIGPTForSequenceClassification</a> (OpenAI GPT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/plbart#transformers.PLBartConfig">PLBartConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/plbart#transformers.PLBartForSequenceClassification">PLBartForSequenceClassification</a> (PLBart model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/perceiver#transformers.PerceiverConfig">PerceiverConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/perceiver#transformers.PerceiverForSequenceClassification">PerceiverForSequenceClassification</a> (Perceiver model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/qdqbert#transformers.QDQBertConfig">QDQBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/qdqbert#transformers.QDQBertForSequenceClassification">QDQBertForSequenceClassification</a> (QDQBert model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/reformer#transformers.ReformerConfig">ReformerConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/reformer#transformers.ReformerForSequenceClassification">ReformerForSequenceClassification</a> (Reformer model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/rembert#transformers.RemBertForSequenceClassification">RemBertForSequenceClassification</a> (RemBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/roformer#transformers.RoFormerForSequenceClassification">RoFormerForSequenceClassification</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/roberta#transformers.RobertaForSequenceClassification">RobertaForSequenceClassification</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/squeezebert#transformers.SqueezeBertConfig">SqueezeBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/squeezebert#transformers.SqueezeBertForSequenceClassification">SqueezeBertForSequenceClassification</a> (SqueezeBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/tapas#transformers.TapasForSequenceClassification">TapasForSequenceClassification</a> (TAPAS model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/transfo-xl#transformers.TransfoXLConfig">TransfoXLConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/transfo-xl#transformers.TransfoXLForSequenceClassification">TransfoXLForSequenceClassification</a> (Transformer-XL model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/xlm#transformers.XLMForSequenceClassification">XLMForSequenceClassification</a> (XLM model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/xlm-roberta#transformers.XLMRobertaForSequenceClassification">XLMRobertaForSequenceClassification</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLConfig">XLMRobertaXLConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForSequenceClassification">XLMRobertaXLForSequenceClassification</a> (XLM-RoBERTa-XL model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/xlnet#transformers.XLNetForSequenceClassification">XLNetForSequenceClassification</a> (XLNet model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/yoso#transformers.YosoConfig">YosoConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/yoso#transformers.YosoForSequenceClassification">YosoForSequenceClassification</a> (YOSO model)</li>
</ul>`,name:"config"}]}}),EE=new w({props:{code:`from transformers import AutoConfig, AutoModelForSequenceClassification

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForSequenceClassification.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSequenceClassification.from_config(config)`}}),yE=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),wE=new w({props:{code:`from transformers import AutoConfig, AutoModelForSequenceClassification

# Download model and configuration from huggingface.co and cache.
model = AutoModelForSequenceClassification.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForSequenceClassification.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForSequenceClassification.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSequenceClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),AE=new z({}),LE=new E({props:{name:"class transformers.AutoModelForMultipleChoice",anchor:"transformers.AutoModelForMultipleChoice",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/modeling_auto.py#L752"}}),kE=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15682/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/albert#transformers.AlbertForMultipleChoice">AlbertForMultipleChoice</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/bert#transformers.BertForMultipleChoice">BertForMultipleChoice</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/big_bird#transformers.BigBirdForMultipleChoice">BigBirdForMultipleChoice</a> (BigBird model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/camembert#transformers.CamembertForMultipleChoice">CamembertForMultipleChoice</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/canine#transformers.CanineConfig">CanineConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/canine#transformers.CanineForMultipleChoice">CanineForMultipleChoice</a> (Canine model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/convbert#transformers.ConvBertForMultipleChoice">ConvBertForMultipleChoice</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/distilbert#transformers.DistilBertForMultipleChoice">DistilBertForMultipleChoice</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/electra#transformers.ElectraForMultipleChoice">ElectraForMultipleChoice</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/fnet#transformers.FNetConfig">FNetConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/fnet#transformers.FNetForMultipleChoice">FNetForMultipleChoice</a> (FNet model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/flaubert#transformers.FlaubertForMultipleChoice">FlaubertForMultipleChoice</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/funnel#transformers.FunnelForMultipleChoice">FunnelForMultipleChoice</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/ibert#transformers.IBertConfig">IBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/ibert#transformers.IBertForMultipleChoice">IBertForMultipleChoice</a> (I-BERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/longformer#transformers.LongformerForMultipleChoice">LongformerForMultipleChoice</a> (Longformer model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/mpnet#transformers.MPNetForMultipleChoice">MPNetForMultipleChoice</a> (MPNet model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/megatron-bert#transformers.MegatronBertConfig">MegatronBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/megatron-bert#transformers.MegatronBertForMultipleChoice">MegatronBertForMultipleChoice</a> (MegatronBert model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/mobilebert#transformers.MobileBertForMultipleChoice">MobileBertForMultipleChoice</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/nystromformer#transformers.NystromformerConfig">NystromformerConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/nystromformer#transformers.NystromformerForMultipleChoice">NystromformerForMultipleChoice</a> (Nystromformer model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/qdqbert#transformers.QDQBertConfig">QDQBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/qdqbert#transformers.QDQBertForMultipleChoice">QDQBertForMultipleChoice</a> (QDQBert model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/rembert#transformers.RemBertForMultipleChoice">RemBertForMultipleChoice</a> (RemBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/roformer#transformers.RoFormerForMultipleChoice">RoFormerForMultipleChoice</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/roberta#transformers.RobertaForMultipleChoice">RobertaForMultipleChoice</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/squeezebert#transformers.SqueezeBertConfig">SqueezeBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/squeezebert#transformers.SqueezeBertForMultipleChoice">SqueezeBertForMultipleChoice</a> (SqueezeBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/xlm#transformers.XLMForMultipleChoice">XLMForMultipleChoice</a> (XLM model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/xlm-roberta#transformers.XLMRobertaForMultipleChoice">XLMRobertaForMultipleChoice</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLConfig">XLMRobertaXLConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForMultipleChoice">XLMRobertaXLForMultipleChoice</a> (XLM-RoBERTa-XL model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/xlnet#transformers.XLNetForMultipleChoice">XLNetForMultipleChoice</a> (XLNet model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/yoso#transformers.YosoConfig">YosoConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/yoso#transformers.YosoForMultipleChoice">YosoForMultipleChoice</a> (YOSO model)</li>
</ul>`,name:"config"}]}}),xE=new w({props:{code:`from transformers import AutoConfig, AutoModelForMultipleChoice

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForMultipleChoice.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForMultipleChoice

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMultipleChoice.from_config(config)`}}),RE=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),SE=new w({props:{code:`from transformers import AutoConfig, AutoModelForMultipleChoice

# Download model and configuration from huggingface.co and cache.
model = AutoModelForMultipleChoice.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForMultipleChoice.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForMultipleChoice.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForMultipleChoice

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMultipleChoice.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMultipleChoice.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMultipleChoice.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),PE=new z({}),$E=new E({props:{name:"class transformers.AutoModelForNextSentencePrediction",anchor:"transformers.AutoModelForNextSentencePrediction",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/modeling_auto.py#L759"}}),jE=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15682/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/bert#transformers.BertForNextSentencePrediction">BertForNextSentencePrediction</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/fnet#transformers.FNetConfig">FNetConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/fnet#transformers.FNetForNextSentencePrediction">FNetForNextSentencePrediction</a> (FNet model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/megatron-bert#transformers.MegatronBertConfig">MegatronBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/megatron-bert#transformers.MegatronBertForNextSentencePrediction">MegatronBertForNextSentencePrediction</a> (MegatronBert model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/mobilebert#transformers.MobileBertForNextSentencePrediction">MobileBertForNextSentencePrediction</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/qdqbert#transformers.QDQBertConfig">QDQBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/qdqbert#transformers.QDQBertForNextSentencePrediction">QDQBertForNextSentencePrediction</a> (QDQBert model)</li>
</ul>`,name:"config"}]}}),NE=new w({props:{code:`from transformers import AutoConfig, AutoModelForNextSentencePrediction

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForNextSentencePrediction.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForNextSentencePrediction

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForNextSentencePrediction.from_config(config)`}}),DE=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),qE=new w({props:{code:`from transformers import AutoConfig, AutoModelForNextSentencePrediction

# Download model and configuration from huggingface.co and cache.
model = AutoModelForNextSentencePrediction.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForNextSentencePrediction.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForNextSentencePrediction.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForNextSentencePrediction

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForNextSentencePrediction.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForNextSentencePrediction.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForNextSentencePrediction.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),GE=new z({}),OE=new E({props:{name:"class transformers.AutoModelForTokenClassification",anchor:"transformers.AutoModelForTokenClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/modeling_auto.py#L745"}}),zE=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15682/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/albert#transformers.AlbertForTokenClassification">AlbertForTokenClassification</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/bert#transformers.BertForTokenClassification">BertForTokenClassification</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/big_bird#transformers.BigBirdForTokenClassification">BigBirdForTokenClassification</a> (BigBird model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/camembert#transformers.CamembertForTokenClassification">CamembertForTokenClassification</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/canine#transformers.CanineConfig">CanineConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/canine#transformers.CanineForTokenClassification">CanineForTokenClassification</a> (Canine model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/convbert#transformers.ConvBertForTokenClassification">ConvBertForTokenClassification</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/deberta#transformers.DebertaForTokenClassification">DebertaForTokenClassification</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/deberta-v2#transformers.DebertaV2ForTokenClassification">DebertaV2ForTokenClassification</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/distilbert#transformers.DistilBertForTokenClassification">DistilBertForTokenClassification</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/electra#transformers.ElectraForTokenClassification">ElectraForTokenClassification</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/fnet#transformers.FNetConfig">FNetConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/fnet#transformers.FNetForTokenClassification">FNetForTokenClassification</a> (FNet model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/flaubert#transformers.FlaubertForTokenClassification">FlaubertForTokenClassification</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/funnel#transformers.FunnelForTokenClassification">FunnelForTokenClassification</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/gpt2#transformers.GPT2ForTokenClassification">GPT2ForTokenClassification</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/ibert#transformers.IBertConfig">IBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/ibert#transformers.IBertForTokenClassification">IBertForTokenClassification</a> (I-BERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/layoutlm#transformers.LayoutLMForTokenClassification">LayoutLMForTokenClassification</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/layoutlmv2#transformers.LayoutLMv2Config">LayoutLMv2Config</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/layoutlmv2#transformers.LayoutLMv2ForTokenClassification">LayoutLMv2ForTokenClassification</a> (LayoutLMv2 model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/longformer#transformers.LongformerForTokenClassification">LongformerForTokenClassification</a> (Longformer model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/mpnet#transformers.MPNetForTokenClassification">MPNetForTokenClassification</a> (MPNet model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/megatron-bert#transformers.MegatronBertConfig">MegatronBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/megatron-bert#transformers.MegatronBertForTokenClassification">MegatronBertForTokenClassification</a> (MegatronBert model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/mobilebert#transformers.MobileBertForTokenClassification">MobileBertForTokenClassification</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/nystromformer#transformers.NystromformerConfig">NystromformerConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/nystromformer#transformers.NystromformerForTokenClassification">NystromformerForTokenClassification</a> (Nystromformer model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/qdqbert#transformers.QDQBertConfig">QDQBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/qdqbert#transformers.QDQBertForTokenClassification">QDQBertForTokenClassification</a> (QDQBert model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/rembert#transformers.RemBertForTokenClassification">RemBertForTokenClassification</a> (RemBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/roformer#transformers.RoFormerForTokenClassification">RoFormerForTokenClassification</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/roberta#transformers.RobertaForTokenClassification">RobertaForTokenClassification</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/squeezebert#transformers.SqueezeBertConfig">SqueezeBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/squeezebert#transformers.SqueezeBertForTokenClassification">SqueezeBertForTokenClassification</a> (SqueezeBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/xlm#transformers.XLMForTokenClassification">XLMForTokenClassification</a> (XLM model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/xlm-roberta#transformers.XLMRobertaForTokenClassification">XLMRobertaForTokenClassification</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLConfig">XLMRobertaXLConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForTokenClassification">XLMRobertaXLForTokenClassification</a> (XLM-RoBERTa-XL model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/xlnet#transformers.XLNetForTokenClassification">XLNetForTokenClassification</a> (XLNet model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/yoso#transformers.YosoConfig">YosoConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/yoso#transformers.YosoForTokenClassification">YosoForTokenClassification</a> (YOSO model)</li>
</ul>`,name:"config"}]}}),VE=new w({props:{code:`from transformers import AutoConfig, AutoModelForTokenClassification

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForTokenClassification.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForTokenClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForTokenClassification.from_config(config)`}}),WE=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),QE=new w({props:{code:`from transformers import AutoConfig, AutoModelForTokenClassification

# Download model and configuration from huggingface.co and cache.
model = AutoModelForTokenClassification.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForTokenClassification.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForTokenClassification.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForTokenClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForTokenClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForTokenClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForTokenClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),HE=new z({}),UE=new E({props:{name:"class transformers.AutoModelForQuestionAnswering",anchor:"transformers.AutoModelForQuestionAnswering",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/modeling_auto.py#L727"}}),YE=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15682/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/albert#transformers.AlbertForQuestionAnswering">AlbertForQuestionAnswering</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/bart#transformers.BartForQuestionAnswering">BartForQuestionAnswering</a> (BART model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/bert#transformers.BertForQuestionAnswering">BertForQuestionAnswering</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/big_bird#transformers.BigBirdForQuestionAnswering">BigBirdForQuestionAnswering</a> (BigBird model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusConfig">BigBirdPegasusConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusForQuestionAnswering">BigBirdPegasusForQuestionAnswering</a> (BigBirdPegasus model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/camembert#transformers.CamembertForQuestionAnswering">CamembertForQuestionAnswering</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/canine#transformers.CanineConfig">CanineConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/canine#transformers.CanineForQuestionAnswering">CanineForQuestionAnswering</a> (Canine model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/convbert#transformers.ConvBertForQuestionAnswering">ConvBertForQuestionAnswering</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/deberta#transformers.DebertaForQuestionAnswering">DebertaForQuestionAnswering</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/deberta-v2#transformers.DebertaV2ForQuestionAnswering">DebertaV2ForQuestionAnswering</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/distilbert#transformers.DistilBertForQuestionAnswering">DistilBertForQuestionAnswering</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/electra#transformers.ElectraForQuestionAnswering">ElectraForQuestionAnswering</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/fnet#transformers.FNetConfig">FNetConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/fnet#transformers.FNetForQuestionAnswering">FNetForQuestionAnswering</a> (FNet model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/flaubert#transformers.FlaubertForQuestionAnsweringSimple">FlaubertForQuestionAnsweringSimple</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/funnel#transformers.FunnelForQuestionAnswering">FunnelForQuestionAnswering</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/gptj#transformers.GPTJConfig">GPTJConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/gptj#transformers.GPTJForQuestionAnswering">GPTJForQuestionAnswering</a> (GPT-J model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/ibert#transformers.IBertConfig">IBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/ibert#transformers.IBertForQuestionAnswering">IBertForQuestionAnswering</a> (I-BERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/led#transformers.LEDConfig">LEDConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/led#transformers.LEDForQuestionAnswering">LEDForQuestionAnswering</a> (LED model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/layoutlmv2#transformers.LayoutLMv2Config">LayoutLMv2Config</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/layoutlmv2#transformers.LayoutLMv2ForQuestionAnswering">LayoutLMv2ForQuestionAnswering</a> (LayoutLMv2 model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/longformer#transformers.LongformerForQuestionAnswering">LongformerForQuestionAnswering</a> (Longformer model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/lxmert#transformers.LxmertConfig">LxmertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/lxmert#transformers.LxmertForQuestionAnswering">LxmertForQuestionAnswering</a> (LXMERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/mbart#transformers.MBartForQuestionAnswering">MBartForQuestionAnswering</a> (mBART model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/mpnet#transformers.MPNetForQuestionAnswering">MPNetForQuestionAnswering</a> (MPNet model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/megatron-bert#transformers.MegatronBertConfig">MegatronBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/megatron-bert#transformers.MegatronBertForQuestionAnswering">MegatronBertForQuestionAnswering</a> (MegatronBert model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/mobilebert#transformers.MobileBertForQuestionAnswering">MobileBertForQuestionAnswering</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/nystromformer#transformers.NystromformerConfig">NystromformerConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/nystromformer#transformers.NystromformerForQuestionAnswering">NystromformerForQuestionAnswering</a> (Nystromformer model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/qdqbert#transformers.QDQBertConfig">QDQBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/qdqbert#transformers.QDQBertForQuestionAnswering">QDQBertForQuestionAnswering</a> (QDQBert model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/reformer#transformers.ReformerConfig">ReformerConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/reformer#transformers.ReformerForQuestionAnswering">ReformerForQuestionAnswering</a> (Reformer model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/rembert#transformers.RemBertForQuestionAnswering">RemBertForQuestionAnswering</a> (RemBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/roformer#transformers.RoFormerForQuestionAnswering">RoFormerForQuestionAnswering</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/roberta#transformers.RobertaForQuestionAnswering">RobertaForQuestionAnswering</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/splinter#transformers.SplinterConfig">SplinterConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/splinter#transformers.SplinterForQuestionAnswering">SplinterForQuestionAnswering</a> (Splinter model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/squeezebert#transformers.SqueezeBertConfig">SqueezeBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/squeezebert#transformers.SqueezeBertForQuestionAnswering">SqueezeBertForQuestionAnswering</a> (SqueezeBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/xlm#transformers.XLMForQuestionAnsweringSimple">XLMForQuestionAnsweringSimple</a> (XLM model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/xlm-roberta#transformers.XLMRobertaForQuestionAnswering">XLMRobertaForQuestionAnswering</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLConfig">XLMRobertaXLConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForQuestionAnswering">XLMRobertaXLForQuestionAnswering</a> (XLM-RoBERTa-XL model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/xlnet#transformers.XLNetForQuestionAnsweringSimple">XLNetForQuestionAnsweringSimple</a> (XLNet model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/yoso#transformers.YosoConfig">YosoConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/yoso#transformers.YosoForQuestionAnswering">YosoForQuestionAnswering</a> (YOSO model)</li>
</ul>`,name:"config"}]}}),KE=new w({props:{code:`from transformers import AutoConfig, AutoModelForQuestionAnswering

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForQuestionAnswering.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForQuestionAnswering.from_config(config)`}}),ZE=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),e3=new w({props:{code:`from transformers import AutoConfig, AutoModelForQuestionAnswering

# Download model and configuration from huggingface.co and cache.
model = AutoModelForQuestionAnswering.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForQuestionAnswering.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForQuestionAnswering.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForQuestionAnswering.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),o3=new z({}),r3=new E({props:{name:"class transformers.AutoModelForTableQuestionAnswering",anchor:"transformers.AutoModelForTableQuestionAnswering",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/modeling_auto.py#L734"}}),a3=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15682/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/tapas#transformers.TapasForQuestionAnswering">TapasForQuestionAnswering</a> (TAPAS model)</li>
</ul>`,name:"config"}]}}),n3=new w({props:{code:`from transformers import AutoConfig, AutoModelForTableQuestionAnswering

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("google/tapas-base-finetuned-wtq")
model = AutoModelForTableQuestionAnswering.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForTableQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;google/tapas-base-finetuned-wtq&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForTableQuestionAnswering.from_config(config)`}}),s3=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),l3=new w({props:{code:`from transformers import AutoConfig, AutoModelForTableQuestionAnswering

# Download model and configuration from huggingface.co and cache.
model = AutoModelForTableQuestionAnswering.from_pretrained("google/tapas-base-finetuned-wtq")

# Update configuration during loading
model = AutoModelForTableQuestionAnswering.from_pretrained("google/tapas-base-finetuned-wtq", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/tapas_tf_model_config.json")
model = AutoModelForTableQuestionAnswering.from_pretrained(
    "./tf_model/tapas_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForTableQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForTableQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;google/tapas-base-finetuned-wtq&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForTableQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;google/tapas-base-finetuned-wtq&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/tapas_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForTableQuestionAnswering.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/tapas_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),i3=new z({}),d3=new E({props:{name:"class transformers.AutoModelForImageClassification",anchor:"transformers.AutoModelForImageClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/modeling_auto.py#L768"}}),f3=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15682/en/model_doc/beit#transformers.BeitConfig">BeitConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/beit#transformers.BeitForImageClassification">BeitForImageClassification</a> (BEiT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/convnext#transformers.ConvNextConfig">ConvNextConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/convnext#transformers.ConvNextForImageClassification">ConvNextForImageClassification</a> (ConvNext model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/deit#transformers.DeiTConfig">DeiTConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/deit#transformers.DeiTForImageClassification">DeiTForImageClassification</a> or <a href="/docs/transformers/pr_15682/en/model_doc/deit#transformers.DeiTForImageClassificationWithTeacher">DeiTForImageClassificationWithTeacher</a> (DeiT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/imagegpt#transformers.ImageGPTConfig">ImageGPTConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/imagegpt#transformers.ImageGPTForImageClassification">ImageGPTForImageClassification</a> (ImageGPT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/perceiver#transformers.PerceiverConfig">PerceiverConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/perceiver#transformers.PerceiverForImageClassificationLearned">PerceiverForImageClassificationLearned</a> or <a href="/docs/transformers/pr_15682/en/model_doc/perceiver#transformers.PerceiverForImageClassificationFourier">PerceiverForImageClassificationFourier</a> or <a href="/docs/transformers/pr_15682/en/model_doc/perceiver#transformers.PerceiverForImageClassificationConvProcessing">PerceiverForImageClassificationConvProcessing</a> (Perceiver model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/poolformer#transformers.PoolFormerConfig">PoolFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/poolformer#transformers.PoolFormerForImageClassification">PoolFormerForImageClassification</a> (PoolFormer model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/segformer#transformers.SegformerConfig">SegformerConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/segformer#transformers.SegformerForImageClassification">SegformerForImageClassification</a> (SegFormer model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/swin#transformers.SwinConfig">SwinConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/swin#transformers.SwinForImageClassification">SwinForImageClassification</a> (Swin model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/vit#transformers.ViTConfig">ViTConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/vit#transformers.ViTForImageClassification">ViTForImageClassification</a> (ViT model)</li>
</ul>`,name:"config"}]}}),m3=new w({props:{code:`from transformers import AutoConfig, AutoModelForImageClassification

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForImageClassification.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForImageClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForImageClassification.from_config(config)`}}),g3=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),h3=new w({props:{code:`from transformers import AutoConfig, AutoModelForImageClassification

# Download model and configuration from huggingface.co and cache.
model = AutoModelForImageClassification.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForImageClassification.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForImageClassification.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForImageClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForImageClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForImageClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForImageClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),p3=new z({}),_3=new E({props:{name:"class transformers.AutoModelForVision2Seq",anchor:"transformers.AutoModelForVision2Seq",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/modeling_auto.py#L798"}}),b3=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15682/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderConfig">VisionEncoderDecoderConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderModel">VisionEncoderDecoderModel</a> (Vision Encoder decoder model)</li>
</ul>`,name:"config"}]}}),v3=new w({props:{code:`from transformers import AutoConfig, AutoModelForVision2Seq

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForVision2Seq.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForVision2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForVision2Seq.from_config(config)`}}),T3=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),F3=new w({props:{code:`from transformers import AutoConfig, AutoModelForVision2Seq

# Download model and configuration from huggingface.co and cache.
model = AutoModelForVision2Seq.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForVision2Seq.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForVision2Seq.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForVision2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForVision2Seq.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForVision2Seq.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForVision2Seq.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),C3=new z({}),M3=new E({props:{name:"class transformers.AutoModelForAudioClassification",anchor:"transformers.AutoModelForAudioClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/modeling_auto.py#L805"}}),y3=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15682/en/model_doc/hubert#transformers.HubertConfig">HubertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/hubert#transformers.HubertForSequenceClassification">HubertForSequenceClassification</a> (Hubert model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/sew#transformers.SEWConfig">SEWConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/sew#transformers.SEWForSequenceClassification">SEWForSequenceClassification</a> (SEW model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/sew-d#transformers.SEWDConfig">SEWDConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/sew-d#transformers.SEWDForSequenceClassification">SEWDForSequenceClassification</a> (SEW-D model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/unispeech#transformers.UniSpeechConfig">UniSpeechConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/unispeech#transformers.UniSpeechForSequenceClassification">UniSpeechForSequenceClassification</a> (UniSpeech model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/unispeech-sat#transformers.UniSpeechSatConfig">UniSpeechSatConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/unispeech-sat#transformers.UniSpeechSatForSequenceClassification">UniSpeechSatForSequenceClassification</a> (UniSpeechSat model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/wav2vec2#transformers.Wav2Vec2ForSequenceClassification">Wav2Vec2ForSequenceClassification</a> (Wav2Vec2 model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/wavlm#transformers.WavLMConfig">WavLMConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/wavlm#transformers.WavLMForSequenceClassification">WavLMForSequenceClassification</a> (WavLM model)</li>
</ul>`,name:"config"}]}}),w3=new w({props:{code:`from transformers import AutoConfig, AutoModelForAudioClassification

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForAudioClassification.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForAudioClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioClassification.from_config(config)`}}),A3=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),L3=new w({props:{code:`from transformers import AutoConfig, AutoModelForAudioClassification

# Download model and configuration from huggingface.co and cache.
model = AutoModelForAudioClassification.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForAudioClassification.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForAudioClassification.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForAudioClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),B3=new z({}),k3=new E({props:{name:"class transformers.AutoModelForAudioFrameClassification",anchor:"transformers.AutoModelForAudioFrameClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/modeling_auto.py#L828"}}),R3=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15682/en/model_doc/unispeech-sat#transformers.UniSpeechSatConfig">UniSpeechSatConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/unispeech-sat#transformers.UniSpeechSatForAudioFrameClassification">UniSpeechSatForAudioFrameClassification</a> (UniSpeechSat model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/wav2vec2#transformers.Wav2Vec2ForAudioFrameClassification">Wav2Vec2ForAudioFrameClassification</a> (Wav2Vec2 model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/wavlm#transformers.WavLMConfig">WavLMConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/wavlm#transformers.WavLMForAudioFrameClassification">WavLMForAudioFrameClassification</a> (WavLM model)</li>
</ul>`,name:"config"}]}}),S3=new w({props:{code:`from transformers import AutoConfig, AutoModelForAudioFrameClassification

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForAudioFrameClassification.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForAudioFrameClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioFrameClassification.from_config(config)`}}),P3=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),$3=new w({props:{code:`from transformers import AutoConfig, AutoModelForAudioFrameClassification

# Download model and configuration from huggingface.co and cache.
model = AutoModelForAudioFrameClassification.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForAudioFrameClassification.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForAudioFrameClassification.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForAudioFrameClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioFrameClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioFrameClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioFrameClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),I3=new z({}),j3=new E({props:{name:"class transformers.AutoModelForCTC",anchor:"transformers.AutoModelForCTC",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/modeling_auto.py#L812"}}),D3=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15682/en/model_doc/hubert#transformers.HubertConfig">HubertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/hubert#transformers.HubertForCTC">HubertForCTC</a> (Hubert model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/sew#transformers.SEWConfig">SEWConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/sew#transformers.SEWForCTC">SEWForCTC</a> (SEW model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/sew-d#transformers.SEWDConfig">SEWDConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/sew-d#transformers.SEWDForCTC">SEWDForCTC</a> (SEW-D model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/unispeech#transformers.UniSpeechConfig">UniSpeechConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/unispeech#transformers.UniSpeechForCTC">UniSpeechForCTC</a> (UniSpeech model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/unispeech-sat#transformers.UniSpeechSatConfig">UniSpeechSatConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/unispeech-sat#transformers.UniSpeechSatForCTC">UniSpeechSatForCTC</a> (UniSpeechSat model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/wav2vec2#transformers.Wav2Vec2ForCTC">Wav2Vec2ForCTC</a> (Wav2Vec2 model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/wavlm#transformers.WavLMConfig">WavLMConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/wavlm#transformers.WavLMForCTC">WavLMForCTC</a> (WavLM model)</li>
</ul>`,name:"config"}]}}),q3=new w({props:{code:`from transformers import AutoConfig, AutoModelForCTC

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForCTC.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForCTC

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCTC.from_config(config)`}}),G3=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),O3=new w({props:{code:`from transformers import AutoConfig, AutoModelForCTC

# Download model and configuration from huggingface.co and cache.
model = AutoModelForCTC.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForCTC.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForCTC.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForCTC

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCTC.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCTC.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCTC.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),X3=new z({}),z3=new E({props:{name:"class transformers.AutoModelForSpeechSeq2Seq",anchor:"transformers.AutoModelForSpeechSeq2Seq",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/modeling_auto.py#L819"}}),W3=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15682/en/model_doc/speech_to_text#transformers.Speech2TextConfig">Speech2TextConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/speech_to_text#transformers.Speech2TextForConditionalGeneration">Speech2TextForConditionalGeneration</a> (Speech2Text model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/speech-encoder-decoder#transformers.SpeechEncoderDecoderConfig">SpeechEncoderDecoderConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/speech-encoder-decoder#transformers.SpeechEncoderDecoderModel">SpeechEncoderDecoderModel</a> (Speech Encoder decoder model)</li>
</ul>`,name:"config"}]}}),Q3=new w({props:{code:`from transformers import AutoConfig, AutoModelForSpeechSeq2Seq

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForSpeechSeq2Seq.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForSpeechSeq2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSpeechSeq2Seq.from_config(config)`}}),H3=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),J3=new w({props:{code:`from transformers import AutoConfig, AutoModelForSpeechSeq2Seq

# Download model and configuration from huggingface.co and cache.
model = AutoModelForSpeechSeq2Seq.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForSpeechSeq2Seq.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForSpeechSeq2Seq.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForSpeechSeq2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSpeechSeq2Seq.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSpeechSeq2Seq.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSpeechSeq2Seq.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),Y3=new z({}),K3=new E({props:{name:"class transformers.AutoModelForAudioXVector",anchor:"transformers.AutoModelForAudioXVector",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/modeling_auto.py#L837"}}),ey=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15682/en/model_doc/unispeech-sat#transformers.UniSpeechSatConfig">UniSpeechSatConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/unispeech-sat#transformers.UniSpeechSatForXVector">UniSpeechSatForXVector</a> (UniSpeechSat model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/wav2vec2#transformers.Wav2Vec2ForXVector">Wav2Vec2ForXVector</a> (Wav2Vec2 model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/wavlm#transformers.WavLMConfig">WavLMConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/wavlm#transformers.WavLMForXVector">WavLMForXVector</a> (WavLM model)</li>
</ul>`,name:"config"}]}}),oy=new w({props:{code:`from transformers import AutoConfig, AutoModelForAudioXVector

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForAudioXVector.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForAudioXVector

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioXVector.from_config(config)`}}),ry=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),ty=new w({props:{code:`from transformers import AutoConfig, AutoModelForAudioXVector

# Download model and configuration from huggingface.co and cache.
model = AutoModelForAudioXVector.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForAudioXVector.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForAudioXVector.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForAudioXVector

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioXVector.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioXVector.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioXVector.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),ay=new z({}),ny=new E({props:{name:"class transformers.AutoModelForMaskedImageModeling",anchor:"transformers.AutoModelForMaskedImageModeling",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/modeling_auto.py#L844"}}),ly=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15682/en/model_doc/deit#transformers.DeiTConfig">DeiTConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/deit#transformers.DeiTForMaskedImageModeling">DeiTForMaskedImageModeling</a> (DeiT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/swin#transformers.SwinConfig">SwinConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/swin#transformers.SwinForMaskedImageModeling">SwinForMaskedImageModeling</a> (Swin model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/vit#transformers.ViTConfig">ViTConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/vit#transformers.ViTForMaskedImageModeling">ViTForMaskedImageModeling</a> (ViT model)</li>
</ul>`,name:"config"}]}}),iy=new w({props:{code:`from transformers import AutoConfig, AutoModelForMaskedImageModeling

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForMaskedImageModeling.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForMaskedImageModeling

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMaskedImageModeling.from_config(config)`}}),dy=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),cy=new w({props:{code:`from transformers import AutoConfig, AutoModelForMaskedImageModeling

# Download model and configuration from huggingface.co and cache.
model = AutoModelForMaskedImageModeling.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForMaskedImageModeling.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForMaskedImageModeling.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForMaskedImageModeling

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMaskedImageModeling.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMaskedImageModeling.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMaskedImageModeling.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),fy=new z({}),my=new E({props:{name:"class transformers.AutoModelForObjectDetection",anchor:"transformers.AutoModelForObjectDetection",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/modeling_auto.py#L791"}}),hy=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15682/en/model_doc/detr#transformers.DetrConfig">DetrConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/detr#transformers.DetrForObjectDetection">DetrForObjectDetection</a> (DETR model)</li>
</ul>`,name:"config"}]}}),py=new w({props:{code:`from transformers import AutoConfig, AutoModelForObjectDetection

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForObjectDetection.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForObjectDetection

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForObjectDetection.from_config(config)`}}),_y=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),uy=new w({props:{code:`from transformers import AutoConfig, AutoModelForObjectDetection

# Download model and configuration from huggingface.co and cache.
model = AutoModelForObjectDetection.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForObjectDetection.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForObjectDetection.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForObjectDetection

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForObjectDetection.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForObjectDetection.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForObjectDetection.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),by=new z({}),vy=new E({props:{name:"class transformers.AutoModelForImageSegmentation",anchor:"transformers.AutoModelForImageSegmentation",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/modeling_auto.py#L775"}}),Fy=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15682/en/model_doc/detr#transformers.DetrConfig">DetrConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/detr#transformers.DetrForSegmentation">DetrForSegmentation</a> (DETR model)</li>
</ul>`,name:"config"}]}}),Cy=new w({props:{code:`from transformers import AutoConfig, AutoModelForImageSegmentation

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForImageSegmentation.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForImageSegmentation

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForImageSegmentation.from_config(config)`}}),My=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),Ey=new w({props:{code:`from transformers import AutoConfig, AutoModelForImageSegmentation

# Download model and configuration from huggingface.co and cache.
model = AutoModelForImageSegmentation.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForImageSegmentation.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForImageSegmentation.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForImageSegmentation

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForImageSegmentation.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForImageSegmentation.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForImageSegmentation.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),yy=new z({}),wy=new E({props:{name:"class transformers.AutoModelForSemanticSegmentation",anchor:"transformers.AutoModelForSemanticSegmentation",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/modeling_auto.py#L782"}}),Ly=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15682/en/model_doc/beit#transformers.BeitConfig">BeitConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/beit#transformers.BeitForSemanticSegmentation">BeitForSemanticSegmentation</a> (BEiT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/segformer#transformers.SegformerConfig">SegformerConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/segformer#transformers.SegformerForSemanticSegmentation">SegformerForSemanticSegmentation</a> (SegFormer model)</li>
</ul>`,name:"config"}]}}),By=new w({props:{code:`from transformers import AutoConfig, AutoModelForSemanticSegmentation

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForSemanticSegmentation.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForSemanticSegmentation

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSemanticSegmentation.from_config(config)`}}),ky=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),Ry=new w({props:{code:`from transformers import AutoConfig, AutoModelForSemanticSegmentation

# Download model and configuration from huggingface.co and cache.
model = AutoModelForSemanticSegmentation.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForSemanticSegmentation.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForSemanticSegmentation.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForSemanticSegmentation

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSemanticSegmentation.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSemanticSegmentation.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSemanticSegmentation.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),Sy=new z({}),Py=new E({props:{name:"class transformers.TFAutoModel",anchor:"transformers.TFAutoModel",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/modeling_tf_auto.py#L373"}}),Iy=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15682/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/albert#transformers.TFAlbertModel">TFAlbertModel</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/bart#transformers.TFBartModel">TFBartModel</a> (BART model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/bert#transformers.TFBertModel">TFBertModel</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/blenderbot#transformers.BlenderbotConfig">BlenderbotConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/blenderbot#transformers.TFBlenderbotModel">TFBlenderbotModel</a> (Blenderbot model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/blenderbot-small#transformers.BlenderbotSmallConfig">BlenderbotSmallConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/blenderbot-small#transformers.TFBlenderbotSmallModel">TFBlenderbotSmallModel</a> (BlenderbotSmall model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/clip#transformers.CLIPConfig">CLIPConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/clip#transformers.TFCLIPModel">TFCLIPModel</a> (CLIP model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/ctrl#transformers.CTRLConfig">CTRLConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/ctrl#transformers.TFCTRLModel">TFCTRLModel</a> (CTRL model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/camembert#transformers.TFCamembertModel">TFCamembertModel</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/convbert#transformers.TFConvBertModel">TFConvBertModel</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/convnext#transformers.ConvNextConfig">ConvNextConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/convnext#transformers.TFConvNextModel">TFConvNextModel</a> (ConvNext model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/dpr#transformers.DPRConfig">DPRConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/dpr#transformers.TFDPRQuestionEncoder">TFDPRQuestionEncoder</a> (DPR model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/deberta#transformers.TFDebertaModel">TFDebertaModel</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/deberta-v2#transformers.TFDebertaV2Model">TFDebertaV2Model</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/distilbert#transformers.TFDistilBertModel">TFDistilBertModel</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/electra#transformers.TFElectraModel">TFElectraModel</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/flaubert#transformers.TFFlaubertModel">TFFlaubertModel</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/funnel#transformers.TFFunnelModel">TFFunnelModel</a> or <a href="/docs/transformers/pr_15682/en/model_doc/funnel#transformers.TFFunnelBaseModel">TFFunnelBaseModel</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/gpt2#transformers.TFGPT2Model">TFGPT2Model</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/hubert#transformers.HubertConfig">HubertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/hubert#transformers.TFHubertModel">TFHubertModel</a> (Hubert model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/led#transformers.LEDConfig">LEDConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/led#transformers.TFLEDModel">TFLEDModel</a> (LED model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/layoutlm#transformers.TFLayoutLMModel">TFLayoutLMModel</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/longformer#transformers.TFLongformerModel">TFLongformerModel</a> (Longformer model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/lxmert#transformers.LxmertConfig">LxmertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/lxmert#transformers.TFLxmertModel">TFLxmertModel</a> (LXMERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/mbart#transformers.TFMBartModel">TFMBartModel</a> (mBART model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/mpnet#transformers.TFMPNetModel">TFMPNetModel</a> (MPNet model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/mt5#transformers.MT5Config">MT5Config</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/mt5#transformers.TFMT5Model">TFMT5Model</a> (mT5 model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/marian#transformers.MarianConfig">MarianConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/marian#transformers.TFMarianModel">TFMarianModel</a> (Marian model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/mobilebert#transformers.TFMobileBertModel">TFMobileBertModel</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig">OpenAIGPTConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/openai-gpt#transformers.TFOpenAIGPTModel">TFOpenAIGPTModel</a> (OpenAI GPT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/pegasus#transformers.PegasusConfig">PegasusConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/pegasus#transformers.TFPegasusModel">TFPegasusModel</a> (Pegasus model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/rembert#transformers.TFRemBertModel">TFRemBertModel</a> (RemBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/roformer#transformers.TFRoFormerModel">TFRoFormerModel</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/roberta#transformers.TFRobertaModel">TFRobertaModel</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/speech_to_text#transformers.Speech2TextConfig">Speech2TextConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/speech_to_text#transformers.TFSpeech2TextModel">TFSpeech2TextModel</a> (Speech2Text model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/t5#transformers.T5Config">T5Config</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/t5#transformers.TFT5Model">TFT5Model</a> (T5 model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/tapas#transformers.TFTapasModel">TFTapasModel</a> (TAPAS model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/transfo-xl#transformers.TransfoXLConfig">TransfoXLConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/transfo-xl#transformers.TFTransfoXLModel">TFTransfoXLModel</a> (Transformer-XL model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/vit#transformers.ViTConfig">ViTConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/vit#transformers.TFViTModel">TFViTModel</a> (ViT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/wav2vec2#transformers.TFWav2Vec2Model">TFWav2Vec2Model</a> (Wav2Vec2 model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/xlm#transformers.TFXLMModel">TFXLMModel</a> (XLM model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/xlm-roberta#transformers.TFXLMRobertaModel">TFXLMRobertaModel</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/xlnet#transformers.TFXLNetModel">TFXLNetModel</a> (XLNet model)</li>
</ul>`,name:"config"}]}}),jy=new w({props:{code:`from transformers import AutoConfig, TFAutoModel

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = TFAutoModel.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModel.from_config(config)`}}),Ny=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),Dy=new w({props:{code:`from transformers import AutoConfig, TFAutoModel

# Download model and configuration from huggingface.co and cache.
model = TFAutoModel.from_pretrained("bert-base-cased")

# Update configuration during loading
model = TFAutoModel.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = TFAutoModel.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModel.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModel.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModel.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),qy=new z({}),Gy=new E({props:{name:"class transformers.TFAutoModelForPreTraining",anchor:"transformers.TFAutoModelForPreTraining",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/modeling_tf_auto.py#L380"}}),Xy=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15682/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/albert#transformers.TFAlbertForPreTraining">TFAlbertForPreTraining</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/bart#transformers.TFBartForConditionalGeneration">TFBartForConditionalGeneration</a> (BART model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/bert#transformers.TFBertForPreTraining">TFBertForPreTraining</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/ctrl#transformers.CTRLConfig">CTRLConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/ctrl#transformers.TFCTRLLMHeadModel">TFCTRLLMHeadModel</a> (CTRL model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/camembert#transformers.TFCamembertForMaskedLM">TFCamembertForMaskedLM</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/distilbert#transformers.TFDistilBertForMaskedLM">TFDistilBertForMaskedLM</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/electra#transformers.TFElectraForPreTraining">TFElectraForPreTraining</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/flaubert#transformers.TFFlaubertWithLMHeadModel">TFFlaubertWithLMHeadModel</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/funnel#transformers.TFFunnelForPreTraining">TFFunnelForPreTraining</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/gpt2#transformers.TFGPT2LMHeadModel">TFGPT2LMHeadModel</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/layoutlm#transformers.TFLayoutLMForMaskedLM">TFLayoutLMForMaskedLM</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/lxmert#transformers.LxmertConfig">LxmertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/lxmert#transformers.TFLxmertForPreTraining">TFLxmertForPreTraining</a> (LXMERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/mpnet#transformers.TFMPNetForMaskedLM">TFMPNetForMaskedLM</a> (MPNet model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/mobilebert#transformers.TFMobileBertForPreTraining">TFMobileBertForPreTraining</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig">OpenAIGPTConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/openai-gpt#transformers.TFOpenAIGPTLMHeadModel">TFOpenAIGPTLMHeadModel</a> (OpenAI GPT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/roberta#transformers.TFRobertaForMaskedLM">TFRobertaForMaskedLM</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/t5#transformers.T5Config">T5Config</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/t5#transformers.TFT5ForConditionalGeneration">TFT5ForConditionalGeneration</a> (T5 model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/tapas#transformers.TFTapasForMaskedLM">TFTapasForMaskedLM</a> (TAPAS model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/transfo-xl#transformers.TransfoXLConfig">TransfoXLConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/transfo-xl#transformers.TFTransfoXLLMHeadModel">TFTransfoXLLMHeadModel</a> (Transformer-XL model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/xlm#transformers.TFXLMWithLMHeadModel">TFXLMWithLMHeadModel</a> (XLM model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForMaskedLM">TFXLMRobertaForMaskedLM</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/xlnet#transformers.TFXLNetLMHeadModel">TFXLNetLMHeadModel</a> (XLNet model)</li>
</ul>`,name:"config"}]}}),zy=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForPreTraining

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = TFAutoModelForPreTraining.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForPreTraining

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForPreTraining.from_config(config)`}}),Vy=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),Wy=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForPreTraining

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForPreTraining.from_pretrained("bert-base-cased")

# Update configuration during loading
model = TFAutoModelForPreTraining.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = TFAutoModelForPreTraining.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForPreTraining

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForPreTraining.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForPreTraining.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForPreTraining.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),Qy=new z({}),Hy=new E({props:{name:"class transformers.TFAutoModelForCausalLM",anchor:"transformers.TFAutoModelForCausalLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/modeling_tf_auto.py#L395"}}),Jy=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15682/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/bert#transformers.TFBertLMHeadModel">TFBertLMHeadModel</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/ctrl#transformers.CTRLConfig">CTRLConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/ctrl#transformers.TFCTRLLMHeadModel">TFCTRLLMHeadModel</a> (CTRL model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/gpt2#transformers.TFGPT2LMHeadModel">TFGPT2LMHeadModel</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig">OpenAIGPTConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/openai-gpt#transformers.TFOpenAIGPTLMHeadModel">TFOpenAIGPTLMHeadModel</a> (OpenAI GPT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/rembert#transformers.TFRemBertForCausalLM">TFRemBertForCausalLM</a> (RemBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/roformer#transformers.TFRoFormerForCausalLM">TFRoFormerForCausalLM</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/roberta#transformers.TFRobertaForCausalLM">TFRobertaForCausalLM</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/transfo-xl#transformers.TransfoXLConfig">TransfoXLConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/transfo-xl#transformers.TFTransfoXLLMHeadModel">TFTransfoXLLMHeadModel</a> (Transformer-XL model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/xlm#transformers.TFXLMWithLMHeadModel">TFXLMWithLMHeadModel</a> (XLM model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/xlnet#transformers.TFXLNetLMHeadModel">TFXLNetLMHeadModel</a> (XLNet model)</li>
</ul>`,name:"config"}]}}),Yy=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForCausalLM

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = TFAutoModelForCausalLM.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForCausalLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForCausalLM.from_config(config)`}}),Ky=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),Zy=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForCausalLM

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForCausalLM.from_pretrained("bert-base-cased")

# Update configuration during loading
model = TFAutoModelForCausalLM.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = TFAutoModelForCausalLM.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForCausalLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForCausalLM.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),ew=new z({}),ow=new E({props:{name:"class transformers.TFAutoModelForImageClassification",anchor:"transformers.TFAutoModelForImageClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/modeling_tf_auto.py#L402"}}),tw=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15682/en/model_doc/convnext#transformers.ConvNextConfig">ConvNextConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/convnext#transformers.TFConvNextForImageClassification">TFConvNextForImageClassification</a> (ConvNext model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/vit#transformers.ViTConfig">ViTConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/vit#transformers.TFViTForImageClassification">TFViTForImageClassification</a> (ViT model)</li>
</ul>`,name:"config"}]}}),aw=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForImageClassification

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = TFAutoModelForImageClassification.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForImageClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForImageClassification.from_config(config)`}}),nw=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),lw=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForImageClassification

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForImageClassification.from_pretrained("bert-base-cased")

# Update configuration during loading
model = TFAutoModelForImageClassification.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = TFAutoModelForImageClassification.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForImageClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForImageClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForImageClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForImageClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),iw=new z({}),dw=new E({props:{name:"class transformers.TFAutoModelForMaskedLM",anchor:"transformers.TFAutoModelForMaskedLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/modeling_tf_auto.py#L416"}}),fw=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15682/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/albert#transformers.TFAlbertForMaskedLM">TFAlbertForMaskedLM</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/bert#transformers.TFBertForMaskedLM">TFBertForMaskedLM</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/camembert#transformers.TFCamembertForMaskedLM">TFCamembertForMaskedLM</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/convbert#transformers.TFConvBertForMaskedLM">TFConvBertForMaskedLM</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/deberta#transformers.TFDebertaForMaskedLM">TFDebertaForMaskedLM</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/deberta-v2#transformers.TFDebertaV2ForMaskedLM">TFDebertaV2ForMaskedLM</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/distilbert#transformers.TFDistilBertForMaskedLM">TFDistilBertForMaskedLM</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/electra#transformers.TFElectraForMaskedLM">TFElectraForMaskedLM</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/flaubert#transformers.TFFlaubertWithLMHeadModel">TFFlaubertWithLMHeadModel</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/funnel#transformers.TFFunnelForMaskedLM">TFFunnelForMaskedLM</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/layoutlm#transformers.TFLayoutLMForMaskedLM">TFLayoutLMForMaskedLM</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/longformer#transformers.TFLongformerForMaskedLM">TFLongformerForMaskedLM</a> (Longformer model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/mpnet#transformers.TFMPNetForMaskedLM">TFMPNetForMaskedLM</a> (MPNet model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/mobilebert#transformers.TFMobileBertForMaskedLM">TFMobileBertForMaskedLM</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/rembert#transformers.TFRemBertForMaskedLM">TFRemBertForMaskedLM</a> (RemBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/roformer#transformers.TFRoFormerForMaskedLM">TFRoFormerForMaskedLM</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/roberta#transformers.TFRobertaForMaskedLM">TFRobertaForMaskedLM</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/tapas#transformers.TFTapasForMaskedLM">TFTapasForMaskedLM</a> (TAPAS model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/xlm#transformers.TFXLMWithLMHeadModel">TFXLMWithLMHeadModel</a> (XLM model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForMaskedLM">TFXLMRobertaForMaskedLM</a> (XLM-RoBERTa model)</li>
</ul>`,name:"config"}]}}),mw=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForMaskedLM

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = TFAutoModelForMaskedLM.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForMaskedLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForMaskedLM.from_config(config)`}}),gw=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),hw=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForMaskedLM

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForMaskedLM.from_pretrained("bert-base-cased")

# Update configuration during loading
model = TFAutoModelForMaskedLM.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = TFAutoModelForMaskedLM.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForMaskedLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForMaskedLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForMaskedLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForMaskedLM.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),pw=new z({}),_w=new E({props:{name:"class transformers.TFAutoModelForSeq2SeqLM",anchor:"transformers.TFAutoModelForSeq2SeqLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/modeling_tf_auto.py#L423"}}),bw=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15682/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/bart#transformers.TFBartForConditionalGeneration">TFBartForConditionalGeneration</a> (BART model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/blenderbot#transformers.BlenderbotConfig">BlenderbotConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/blenderbot#transformers.TFBlenderbotForConditionalGeneration">TFBlenderbotForConditionalGeneration</a> (Blenderbot model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/blenderbot-small#transformers.BlenderbotSmallConfig">BlenderbotSmallConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/blenderbot-small#transformers.TFBlenderbotSmallForConditionalGeneration">TFBlenderbotSmallForConditionalGeneration</a> (BlenderbotSmall model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/encoder-decoder#transformers.EncoderDecoderConfig">EncoderDecoderConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/encoder-decoder#transformers.TFEncoderDecoderModel">TFEncoderDecoderModel</a> (Encoder decoder model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/led#transformers.LEDConfig">LEDConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/led#transformers.TFLEDForConditionalGeneration">TFLEDForConditionalGeneration</a> (LED model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/mbart#transformers.TFMBartForConditionalGeneration">TFMBartForConditionalGeneration</a> (mBART model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/mt5#transformers.MT5Config">MT5Config</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/mt5#transformers.TFMT5ForConditionalGeneration">TFMT5ForConditionalGeneration</a> (mT5 model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/marian#transformers.MarianConfig">MarianConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/marian#transformers.TFMarianMTModel">TFMarianMTModel</a> (Marian model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/pegasus#transformers.PegasusConfig">PegasusConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/pegasus#transformers.TFPegasusForConditionalGeneration">TFPegasusForConditionalGeneration</a> (Pegasus model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/t5#transformers.T5Config">T5Config</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/t5#transformers.TFT5ForConditionalGeneration">TFT5ForConditionalGeneration</a> (T5 model)</li>
</ul>`,name:"config"}]}}),vw=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForSeq2SeqLM

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("t5-base")
model = TFAutoModelForSeq2SeqLM.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForSeq2SeqLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;t5-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSeq2SeqLM.from_config(config)`}}),Tw=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),Fw=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForSeq2SeqLM

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForSeq2SeqLM.from_pretrained("t5-base")

# Update configuration during loading
model = TFAutoModelForSeq2SeqLM.from_pretrained("t5-base", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/t5_pt_model_config.json")
model = TFAutoModelForSeq2SeqLM.from_pretrained(
    "./pt_model/t5_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForSeq2SeqLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSeq2SeqLM.from_pretrained(<span class="hljs-string">&quot;t5-base&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSeq2SeqLM.from_pretrained(<span class="hljs-string">&quot;t5-base&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/t5_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSeq2SeqLM.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/t5_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),Cw=new z({}),Mw=new E({props:{name:"class transformers.TFAutoModelForSequenceClassification",anchor:"transformers.TFAutoModelForSequenceClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/modeling_tf_auto.py#L432"}}),yw=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15682/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/albert#transformers.TFAlbertForSequenceClassification">TFAlbertForSequenceClassification</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/bert#transformers.TFBertForSequenceClassification">TFBertForSequenceClassification</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/ctrl#transformers.CTRLConfig">CTRLConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/ctrl#transformers.TFCTRLForSequenceClassification">TFCTRLForSequenceClassification</a> (CTRL model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/camembert#transformers.TFCamembertForSequenceClassification">TFCamembertForSequenceClassification</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/convbert#transformers.TFConvBertForSequenceClassification">TFConvBertForSequenceClassification</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/deberta#transformers.TFDebertaForSequenceClassification">TFDebertaForSequenceClassification</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/deberta-v2#transformers.TFDebertaV2ForSequenceClassification">TFDebertaV2ForSequenceClassification</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/distilbert#transformers.TFDistilBertForSequenceClassification">TFDistilBertForSequenceClassification</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/electra#transformers.TFElectraForSequenceClassification">TFElectraForSequenceClassification</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/flaubert#transformers.TFFlaubertForSequenceClassification">TFFlaubertForSequenceClassification</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/funnel#transformers.TFFunnelForSequenceClassification">TFFunnelForSequenceClassification</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/gpt2#transformers.TFGPT2ForSequenceClassification">TFGPT2ForSequenceClassification</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/layoutlm#transformers.TFLayoutLMForSequenceClassification">TFLayoutLMForSequenceClassification</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/longformer#transformers.TFLongformerForSequenceClassification">TFLongformerForSequenceClassification</a> (Longformer model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/mpnet#transformers.TFMPNetForSequenceClassification">TFMPNetForSequenceClassification</a> (MPNet model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/mobilebert#transformers.TFMobileBertForSequenceClassification">TFMobileBertForSequenceClassification</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig">OpenAIGPTConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/openai-gpt#transformers.TFOpenAIGPTForSequenceClassification">TFOpenAIGPTForSequenceClassification</a> (OpenAI GPT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/rembert#transformers.TFRemBertForSequenceClassification">TFRemBertForSequenceClassification</a> (RemBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/roformer#transformers.TFRoFormerForSequenceClassification">TFRoFormerForSequenceClassification</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/roberta#transformers.TFRobertaForSequenceClassification">TFRobertaForSequenceClassification</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/tapas#transformers.TFTapasForSequenceClassification">TFTapasForSequenceClassification</a> (TAPAS model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/transfo-xl#transformers.TransfoXLConfig">TransfoXLConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/transfo-xl#transformers.TFTransfoXLForSequenceClassification">TFTransfoXLForSequenceClassification</a> (Transformer-XL model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/xlm#transformers.TFXLMForSequenceClassification">TFXLMForSequenceClassification</a> (XLM model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForSequenceClassification">TFXLMRobertaForSequenceClassification</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/xlnet#transformers.TFXLNetForSequenceClassification">TFXLNetForSequenceClassification</a> (XLNet model)</li>
</ul>`,name:"config"}]}}),ww=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForSequenceClassification

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = TFAutoModelForSequenceClassification.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSequenceClassification.from_config(config)`}}),Aw=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),Lw=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForSequenceClassification

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForSequenceClassification.from_pretrained("bert-base-cased")

# Update configuration during loading
model = TFAutoModelForSequenceClassification.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = TFAutoModelForSequenceClassification.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSequenceClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),Bw=new z({}),kw=new E({props:{name:"class transformers.TFAutoModelForMultipleChoice",anchor:"transformers.TFAutoModelForMultipleChoice",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/modeling_tf_auto.py#L468"}}),Rw=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15682/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/albert#transformers.TFAlbertForMultipleChoice">TFAlbertForMultipleChoice</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/bert#transformers.TFBertForMultipleChoice">TFBertForMultipleChoice</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/camembert#transformers.TFCamembertForMultipleChoice">TFCamembertForMultipleChoice</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/convbert#transformers.TFConvBertForMultipleChoice">TFConvBertForMultipleChoice</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/distilbert#transformers.TFDistilBertForMultipleChoice">TFDistilBertForMultipleChoice</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/electra#transformers.TFElectraForMultipleChoice">TFElectraForMultipleChoice</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/flaubert#transformers.TFFlaubertForMultipleChoice">TFFlaubertForMultipleChoice</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/funnel#transformers.TFFunnelForMultipleChoice">TFFunnelForMultipleChoice</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/longformer#transformers.TFLongformerForMultipleChoice">TFLongformerForMultipleChoice</a> (Longformer model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/mpnet#transformers.TFMPNetForMultipleChoice">TFMPNetForMultipleChoice</a> (MPNet model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/mobilebert#transformers.TFMobileBertForMultipleChoice">TFMobileBertForMultipleChoice</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/rembert#transformers.TFRemBertForMultipleChoice">TFRemBertForMultipleChoice</a> (RemBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/roformer#transformers.TFRoFormerForMultipleChoice">TFRoFormerForMultipleChoice</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/roberta#transformers.TFRobertaForMultipleChoice">TFRobertaForMultipleChoice</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/xlm#transformers.TFXLMForMultipleChoice">TFXLMForMultipleChoice</a> (XLM model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForMultipleChoice">TFXLMRobertaForMultipleChoice</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/xlnet#transformers.TFXLNetForMultipleChoice">TFXLNetForMultipleChoice</a> (XLNet model)</li>
</ul>`,name:"config"}]}}),Sw=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForMultipleChoice

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = TFAutoModelForMultipleChoice.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForMultipleChoice

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForMultipleChoice.from_config(config)`}}),Pw=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),$w=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForMultipleChoice

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForMultipleChoice.from_pretrained("bert-base-cased")

# Update configuration during loading
model = TFAutoModelForMultipleChoice.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = TFAutoModelForMultipleChoice.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForMultipleChoice

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForMultipleChoice.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForMultipleChoice.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForMultipleChoice.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),Iw=new z({}),jw=new E({props:{name:"class transformers.TFAutoModelForTableQuestionAnswering",anchor:"transformers.TFAutoModelForTableQuestionAnswering",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/modeling_tf_auto.py#L448"}}),Dw=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15682/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/tapas#transformers.TFTapasForQuestionAnswering">TFTapasForQuestionAnswering</a> (TAPAS model)</li>
</ul>`,name:"config"}]}}),qw=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForTableQuestionAnswering

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("google/tapas-base-finetuned-wtq")
model = TFAutoModelForTableQuestionAnswering.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForTableQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;google/tapas-base-finetuned-wtq&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForTableQuestionAnswering.from_config(config)`}}),Gw=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),Ow=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForTableQuestionAnswering

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForTableQuestionAnswering.from_pretrained("google/tapas-base-finetuned-wtq")

# Update configuration during loading
model = TFAutoModelForTableQuestionAnswering.from_pretrained("google/tapas-base-finetuned-wtq", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/tapas_pt_model_config.json")
model = TFAutoModelForTableQuestionAnswering.from_pretrained(
    "./pt_model/tapas_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForTableQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForTableQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;google/tapas-base-finetuned-wtq&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForTableQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;google/tapas-base-finetuned-wtq&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/tapas_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForTableQuestionAnswering.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/tapas_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),Xw=new z({}),zw=new E({props:{name:"class transformers.TFAutoModelForTokenClassification",anchor:"transformers.TFAutoModelForTokenClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/modeling_tf_auto.py#L459"}}),Ww=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15682/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/albert#transformers.TFAlbertForTokenClassification">TFAlbertForTokenClassification</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/bert#transformers.TFBertForTokenClassification">TFBertForTokenClassification</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/camembert#transformers.TFCamembertForTokenClassification">TFCamembertForTokenClassification</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/convbert#transformers.TFConvBertForTokenClassification">TFConvBertForTokenClassification</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/deberta#transformers.TFDebertaForTokenClassification">TFDebertaForTokenClassification</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/deberta-v2#transformers.TFDebertaV2ForTokenClassification">TFDebertaV2ForTokenClassification</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/distilbert#transformers.TFDistilBertForTokenClassification">TFDistilBertForTokenClassification</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/electra#transformers.TFElectraForTokenClassification">TFElectraForTokenClassification</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/flaubert#transformers.TFFlaubertForTokenClassification">TFFlaubertForTokenClassification</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/funnel#transformers.TFFunnelForTokenClassification">TFFunnelForTokenClassification</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/layoutlm#transformers.TFLayoutLMForTokenClassification">TFLayoutLMForTokenClassification</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/longformer#transformers.TFLongformerForTokenClassification">TFLongformerForTokenClassification</a> (Longformer model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/mpnet#transformers.TFMPNetForTokenClassification">TFMPNetForTokenClassification</a> (MPNet model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/mobilebert#transformers.TFMobileBertForTokenClassification">TFMobileBertForTokenClassification</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/rembert#transformers.TFRemBertForTokenClassification">TFRemBertForTokenClassification</a> (RemBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/roformer#transformers.TFRoFormerForTokenClassification">TFRoFormerForTokenClassification</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/roberta#transformers.TFRobertaForTokenClassification">TFRobertaForTokenClassification</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/xlm#transformers.TFXLMForTokenClassification">TFXLMForTokenClassification</a> (XLM model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForTokenClassification">TFXLMRobertaForTokenClassification</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/xlnet#transformers.TFXLNetForTokenClassification">TFXLNetForTokenClassification</a> (XLNet model)</li>
</ul>`,name:"config"}]}}),Qw=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForTokenClassification

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = TFAutoModelForTokenClassification.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForTokenClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForTokenClassification.from_config(config)`}}),Hw=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),Uw=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForTokenClassification

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForTokenClassification.from_pretrained("bert-base-cased")

# Update configuration during loading
model = TFAutoModelForTokenClassification.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = TFAutoModelForTokenClassification.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForTokenClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForTokenClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForTokenClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForTokenClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),Jw=new z({}),Yw=new E({props:{name:"class transformers.TFAutoModelForQuestionAnswering",anchor:"transformers.TFAutoModelForQuestionAnswering",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/modeling_tf_auto.py#L441"}}),Zw=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15682/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/albert#transformers.TFAlbertForQuestionAnswering">TFAlbertForQuestionAnswering</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/bert#transformers.TFBertForQuestionAnswering">TFBertForQuestionAnswering</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/camembert#transformers.TFCamembertForQuestionAnswering">TFCamembertForQuestionAnswering</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/convbert#transformers.TFConvBertForQuestionAnswering">TFConvBertForQuestionAnswering</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/deberta#transformers.TFDebertaForQuestionAnswering">TFDebertaForQuestionAnswering</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/deberta-v2#transformers.TFDebertaV2ForQuestionAnswering">TFDebertaV2ForQuestionAnswering</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/distilbert#transformers.TFDistilBertForQuestionAnswering">TFDistilBertForQuestionAnswering</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/electra#transformers.TFElectraForQuestionAnswering">TFElectraForQuestionAnswering</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/flaubert#transformers.TFFlaubertForQuestionAnsweringSimple">TFFlaubertForQuestionAnsweringSimple</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/funnel#transformers.TFFunnelForQuestionAnswering">TFFunnelForQuestionAnswering</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/longformer#transformers.TFLongformerForQuestionAnswering">TFLongformerForQuestionAnswering</a> (Longformer model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/mpnet#transformers.TFMPNetForQuestionAnswering">TFMPNetForQuestionAnswering</a> (MPNet model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/mobilebert#transformers.TFMobileBertForQuestionAnswering">TFMobileBertForQuestionAnswering</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/rembert#transformers.TFRemBertForQuestionAnswering">TFRemBertForQuestionAnswering</a> (RemBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/roformer#transformers.TFRoFormerForQuestionAnswering">TFRoFormerForQuestionAnswering</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/roberta#transformers.TFRobertaForQuestionAnswering">TFRobertaForQuestionAnswering</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/xlm#transformers.TFXLMForQuestionAnsweringSimple">TFXLMForQuestionAnsweringSimple</a> (XLM model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForQuestionAnswering">TFXLMRobertaForQuestionAnswering</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/xlnet#transformers.TFXLNetForQuestionAnsweringSimple">TFXLNetForQuestionAnsweringSimple</a> (XLNet model)</li>
</ul>`,name:"config"}]}}),eA=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForQuestionAnswering

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = TFAutoModelForQuestionAnswering.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForQuestionAnswering.from_config(config)`}}),oA=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),rA=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForQuestionAnswering

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForQuestionAnswering.from_pretrained("bert-base-cased")

# Update configuration during loading
model = TFAutoModelForQuestionAnswering.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = TFAutoModelForQuestionAnswering.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForQuestionAnswering.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),tA=new z({}),aA=new E({props:{name:"class transformers.TFAutoModelForVision2Seq",anchor:"transformers.TFAutoModelForVision2Seq",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/modeling_tf_auto.py#L409"}}),sA=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15682/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderConfig">VisionEncoderDecoderConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/vision-encoder-decoder#transformers.TFVisionEncoderDecoderModel">TFVisionEncoderDecoderModel</a> (Vision Encoder decoder model)</li>
</ul>`,name:"config"}]}}),lA=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForVision2Seq

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = TFAutoModelForVision2Seq.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForVision2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForVision2Seq.from_config(config)`}}),iA=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),dA=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForVision2Seq

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForVision2Seq.from_pretrained("bert-base-cased")

# Update configuration during loading
model = TFAutoModelForVision2Seq.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = TFAutoModelForVision2Seq.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForVision2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForVision2Seq.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForVision2Seq.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForVision2Seq.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),cA=new z({}),fA=new E({props:{name:"class transformers.TFAutoModelForSpeechSeq2Seq",anchor:"transformers.TFAutoModelForSpeechSeq2Seq",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/modeling_tf_auto.py#L484"}}),gA=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15682/en/model_doc/speech_to_text#transformers.Speech2TextConfig">Speech2TextConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/speech_to_text#transformers.TFSpeech2TextForConditionalGeneration">TFSpeech2TextForConditionalGeneration</a> (Speech2Text model)</li>
</ul>`,name:"config"}]}}),hA=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForSpeechSeq2Seq

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = TFAutoModelForSpeechSeq2Seq.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForSpeechSeq2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSpeechSeq2Seq.from_config(config)`}}),pA=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),_A=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForSpeechSeq2Seq

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForSpeechSeq2Seq.from_pretrained("bert-base-cased")

# Update configuration during loading
model = TFAutoModelForSpeechSeq2Seq.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = TFAutoModelForSpeechSeq2Seq.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForSpeechSeq2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSpeechSeq2Seq.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSpeechSeq2Seq.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSpeechSeq2Seq.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),uA=new z({}),bA=new E({props:{name:"class transformers.FlaxAutoModel",anchor:"transformers.FlaxAutoModel",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/modeling_flax_auto.py#L220"}}),TA=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15682/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/albert#transformers.FlaxAlbertModel">FlaxAlbertModel</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/bart#transformers.FlaxBartModel">FlaxBartModel</a> (BART model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/beit#transformers.BeitConfig">BeitConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/beit#transformers.FlaxBeitModel">FlaxBeitModel</a> (BEiT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/bert#transformers.FlaxBertModel">FlaxBertModel</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/big_bird#transformers.FlaxBigBirdModel">FlaxBigBirdModel</a> (BigBird model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/blenderbot#transformers.BlenderbotConfig">BlenderbotConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/blenderbot#transformers.FlaxBlenderbotModel">FlaxBlenderbotModel</a> (Blenderbot model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/blenderbot-small#transformers.BlenderbotSmallConfig">BlenderbotSmallConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/blenderbot-small#transformers.FlaxBlenderbotSmallModel">FlaxBlenderbotSmallModel</a> (BlenderbotSmall model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/clip#transformers.CLIPConfig">CLIPConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/clip#transformers.FlaxCLIPModel">FlaxCLIPModel</a> (CLIP model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/distilbert#transformers.FlaxDistilBertModel">FlaxDistilBertModel</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/electra#transformers.FlaxElectraModel">FlaxElectraModel</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/gpt2#transformers.FlaxGPT2Model">FlaxGPT2Model</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/gptj#transformers.GPTJConfig">GPTJConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/gptj#transformers.FlaxGPTJModel">FlaxGPTJModel</a> (GPT-J model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/gpt_neo#transformers.GPTNeoConfig">GPTNeoConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/gpt_neo#transformers.FlaxGPTNeoModel">FlaxGPTNeoModel</a> (GPT Neo model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/mbart#transformers.FlaxMBartModel">FlaxMBartModel</a> (mBART model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/mt5#transformers.MT5Config">MT5Config</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/mt5#transformers.FlaxMT5Model">FlaxMT5Model</a> (mT5 model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/marian#transformers.MarianConfig">MarianConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/marian#transformers.FlaxMarianModel">FlaxMarianModel</a> (Marian model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/pegasus#transformers.PegasusConfig">PegasusConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/pegasus#transformers.FlaxPegasusModel">FlaxPegasusModel</a> (Pegasus model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/roformer#transformers.FlaxRoFormerModel">FlaxRoFormerModel</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/roberta#transformers.FlaxRobertaModel">FlaxRobertaModel</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/t5#transformers.T5Config">T5Config</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/t5#transformers.FlaxT5Model">FlaxT5Model</a> (T5 model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/vit#transformers.ViTConfig">ViTConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/vit#transformers.FlaxViTModel">FlaxViTModel</a> (ViT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/vision-text-dual-encoder#transformers.VisionTextDualEncoderConfig">VisionTextDualEncoderConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/vision-text-dual-encoder#transformers.FlaxVisionTextDualEncoderModel">FlaxVisionTextDualEncoderModel</a> (VisionTextDualEncoder model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/wav2vec2#transformers.FlaxWav2Vec2Model">FlaxWav2Vec2Model</a> (Wav2Vec2 model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/xglm#transformers.XGLMConfig">XGLMConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/xglm#transformers.FlaxXGLMModel">FlaxXGLMModel</a> (XGLM model)</li>
</ul>`,name:"config"}]}}),FA=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModel

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = FlaxAutoModel.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModel.from_config(config)`}}),CA=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),MA=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModel

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModel.from_pretrained("bert-base-cased")

# Update configuration during loading
model = FlaxAutoModel.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = FlaxAutoModel.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModel.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModel.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModel.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),EA=new z({}),yA=new E({props:{name:"class transformers.FlaxAutoModelForCausalLM",anchor:"transformers.FlaxAutoModelForCausalLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/modeling_flax_auto.py#L234"}}),AA=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15682/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/gpt2#transformers.FlaxGPT2LMHeadModel">FlaxGPT2LMHeadModel</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/gptj#transformers.GPTJConfig">GPTJConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/gptj#transformers.FlaxGPTJForCausalLM">FlaxGPTJForCausalLM</a> (GPT-J model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/gpt_neo#transformers.GPTNeoConfig">GPTNeoConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/gpt_neo#transformers.FlaxGPTNeoForCausalLM">FlaxGPTNeoForCausalLM</a> (GPT Neo model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/xglm#transformers.XGLMConfig">XGLMConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/xglm#transformers.FlaxXGLMForCausalLM">FlaxXGLMForCausalLM</a> (XGLM model)</li>
</ul>`,name:"config"}]}}),LA=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForCausalLM

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = FlaxAutoModelForCausalLM.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForCausalLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForCausalLM.from_config(config)`}}),BA=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),kA=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForCausalLM

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForCausalLM.from_pretrained("bert-base-cased")

# Update configuration during loading
model = FlaxAutoModelForCausalLM.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = FlaxAutoModelForCausalLM.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForCausalLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForCausalLM.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),xA=new z({}),RA=new E({props:{name:"class transformers.FlaxAutoModelForPreTraining",anchor:"transformers.FlaxAutoModelForPreTraining",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/modeling_flax_auto.py#L227"}}),PA=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15682/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/albert#transformers.FlaxAlbertForPreTraining">FlaxAlbertForPreTraining</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/bart#transformers.FlaxBartForConditionalGeneration">FlaxBartForConditionalGeneration</a> (BART model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/bert#transformers.FlaxBertForPreTraining">FlaxBertForPreTraining</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/big_bird#transformers.FlaxBigBirdForPreTraining">FlaxBigBirdForPreTraining</a> (BigBird model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/electra#transformers.FlaxElectraForPreTraining">FlaxElectraForPreTraining</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/mbart#transformers.FlaxMBartForConditionalGeneration">FlaxMBartForConditionalGeneration</a> (mBART model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/mt5#transformers.MT5Config">MT5Config</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/mt5#transformers.FlaxMT5ForConditionalGeneration">FlaxMT5ForConditionalGeneration</a> (mT5 model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/roformer#transformers.FlaxRoFormerForMaskedLM">FlaxRoFormerForMaskedLM</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/roberta#transformers.FlaxRobertaForMaskedLM">FlaxRobertaForMaskedLM</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/t5#transformers.T5Config">T5Config</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/t5#transformers.FlaxT5ForConditionalGeneration">FlaxT5ForConditionalGeneration</a> (T5 model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/wav2vec2#transformers.FlaxWav2Vec2ForPreTraining">FlaxWav2Vec2ForPreTraining</a> (Wav2Vec2 model)</li>
</ul>`,name:"config"}]}}),$A=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForPreTraining

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = FlaxAutoModelForPreTraining.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForPreTraining

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForPreTraining.from_config(config)`}}),IA=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),jA=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForPreTraining

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForPreTraining.from_pretrained("bert-base-cased")

# Update configuration during loading
model = FlaxAutoModelForPreTraining.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = FlaxAutoModelForPreTraining.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForPreTraining

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForPreTraining.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForPreTraining.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForPreTraining.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),NA=new z({}),DA=new E({props:{name:"class transformers.FlaxAutoModelForMaskedLM",anchor:"transformers.FlaxAutoModelForMaskedLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/modeling_flax_auto.py#L241"}}),GA=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15682/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/albert#transformers.FlaxAlbertForMaskedLM">FlaxAlbertForMaskedLM</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/bart#transformers.FlaxBartForConditionalGeneration">FlaxBartForConditionalGeneration</a> (BART model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/bert#transformers.FlaxBertForMaskedLM">FlaxBertForMaskedLM</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/big_bird#transformers.FlaxBigBirdForMaskedLM">FlaxBigBirdForMaskedLM</a> (BigBird model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/distilbert#transformers.FlaxDistilBertForMaskedLM">FlaxDistilBertForMaskedLM</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/electra#transformers.FlaxElectraForMaskedLM">FlaxElectraForMaskedLM</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/mbart#transformers.FlaxMBartForConditionalGeneration">FlaxMBartForConditionalGeneration</a> (mBART model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/roformer#transformers.FlaxRoFormerForMaskedLM">FlaxRoFormerForMaskedLM</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/roberta#transformers.FlaxRobertaForMaskedLM">FlaxRobertaForMaskedLM</a> (RoBERTa model)</li>
</ul>`,name:"config"}]}}),OA=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForMaskedLM

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = FlaxAutoModelForMaskedLM.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForMaskedLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForMaskedLM.from_config(config)`}}),XA=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),zA=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForMaskedLM

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForMaskedLM.from_pretrained("bert-base-cased")

# Update configuration during loading
model = FlaxAutoModelForMaskedLM.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = FlaxAutoModelForMaskedLM.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForMaskedLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForMaskedLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForMaskedLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForMaskedLM.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),VA=new z({}),WA=new E({props:{name:"class transformers.FlaxAutoModelForSeq2SeqLM",anchor:"transformers.FlaxAutoModelForSeq2SeqLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/modeling_flax_auto.py#L248"}}),HA=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15682/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/bart#transformers.FlaxBartForConditionalGeneration">FlaxBartForConditionalGeneration</a> (BART model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/blenderbot#transformers.BlenderbotConfig">BlenderbotConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/blenderbot#transformers.FlaxBlenderbotForConditionalGeneration">FlaxBlenderbotForConditionalGeneration</a> (Blenderbot model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/blenderbot-small#transformers.BlenderbotSmallConfig">BlenderbotSmallConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/blenderbot-small#transformers.FlaxBlenderbotSmallForConditionalGeneration">FlaxBlenderbotSmallForConditionalGeneration</a> (BlenderbotSmall model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/encoder-decoder#transformers.EncoderDecoderConfig">EncoderDecoderConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/encoder-decoder#transformers.FlaxEncoderDecoderModel">FlaxEncoderDecoderModel</a> (Encoder decoder model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/mbart#transformers.FlaxMBartForConditionalGeneration">FlaxMBartForConditionalGeneration</a> (mBART model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/mt5#transformers.MT5Config">MT5Config</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/mt5#transformers.FlaxMT5ForConditionalGeneration">FlaxMT5ForConditionalGeneration</a> (mT5 model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/marian#transformers.MarianConfig">MarianConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/marian#transformers.FlaxMarianMTModel">FlaxMarianMTModel</a> (Marian model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/pegasus#transformers.PegasusConfig">PegasusConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/pegasus#transformers.FlaxPegasusForConditionalGeneration">FlaxPegasusForConditionalGeneration</a> (Pegasus model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/t5#transformers.T5Config">T5Config</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/t5#transformers.FlaxT5ForConditionalGeneration">FlaxT5ForConditionalGeneration</a> (T5 model)</li>
</ul>`,name:"config"}]}}),UA=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForSeq2SeqLM

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("t5-base")
model = FlaxAutoModelForSeq2SeqLM.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForSeq2SeqLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;t5-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForSeq2SeqLM.from_config(config)`}}),JA=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),YA=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForSeq2SeqLM

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForSeq2SeqLM.from_pretrained("t5-base")

# Update configuration during loading
model = FlaxAutoModelForSeq2SeqLM.from_pretrained("t5-base", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/t5_pt_model_config.json")
model = FlaxAutoModelForSeq2SeqLM.from_pretrained(
    "./pt_model/t5_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForSeq2SeqLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForSeq2SeqLM.from_pretrained(<span class="hljs-string">&quot;t5-base&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForSeq2SeqLM.from_pretrained(<span class="hljs-string">&quot;t5-base&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/t5_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForSeq2SeqLM.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/t5_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),KA=new z({}),ZA=new E({props:{name:"class transformers.FlaxAutoModelForSequenceClassification",anchor:"transformers.FlaxAutoModelForSequenceClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/modeling_flax_auto.py#L257"}}),o0=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15682/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/albert#transformers.FlaxAlbertForSequenceClassification">FlaxAlbertForSequenceClassification</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/bart#transformers.FlaxBartForSequenceClassification">FlaxBartForSequenceClassification</a> (BART model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/bert#transformers.FlaxBertForSequenceClassification">FlaxBertForSequenceClassification</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/big_bird#transformers.FlaxBigBirdForSequenceClassification">FlaxBigBirdForSequenceClassification</a> (BigBird model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/distilbert#transformers.FlaxDistilBertForSequenceClassification">FlaxDistilBertForSequenceClassification</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/electra#transformers.FlaxElectraForSequenceClassification">FlaxElectraForSequenceClassification</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/mbart#transformers.FlaxMBartForSequenceClassification">FlaxMBartForSequenceClassification</a> (mBART model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/roformer#transformers.FlaxRoFormerForSequenceClassification">FlaxRoFormerForSequenceClassification</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/roberta#transformers.FlaxRobertaForSequenceClassification">FlaxRobertaForSequenceClassification</a> (RoBERTa model)</li>
</ul>`,name:"config"}]}}),r0=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForSequenceClassification

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = FlaxAutoModelForSequenceClassification.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForSequenceClassification.from_config(config)`}}),t0=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),a0=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForSequenceClassification

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForSequenceClassification.from_pretrained("bert-base-cased")

# Update configuration during loading
model = FlaxAutoModelForSequenceClassification.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = FlaxAutoModelForSequenceClassification.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForSequenceClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),n0=new z({}),s0=new E({props:{name:"class transformers.FlaxAutoModelForQuestionAnswering",anchor:"transformers.FlaxAutoModelForQuestionAnswering",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/modeling_flax_auto.py#L266"}}),i0=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15682/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/albert#transformers.FlaxAlbertForQuestionAnswering">FlaxAlbertForQuestionAnswering</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/bart#transformers.FlaxBartForQuestionAnswering">FlaxBartForQuestionAnswering</a> (BART model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/bert#transformers.FlaxBertForQuestionAnswering">FlaxBertForQuestionAnswering</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/big_bird#transformers.FlaxBigBirdForQuestionAnswering">FlaxBigBirdForQuestionAnswering</a> (BigBird model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/distilbert#transformers.FlaxDistilBertForQuestionAnswering">FlaxDistilBertForQuestionAnswering</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/electra#transformers.FlaxElectraForQuestionAnswering">FlaxElectraForQuestionAnswering</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/mbart#transformers.FlaxMBartForQuestionAnswering">FlaxMBartForQuestionAnswering</a> (mBART model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/roformer#transformers.FlaxRoFormerForQuestionAnswering">FlaxRoFormerForQuestionAnswering</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/roberta#transformers.FlaxRobertaForQuestionAnswering">FlaxRobertaForQuestionAnswering</a> (RoBERTa model)</li>
</ul>`,name:"config"}]}}),d0=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForQuestionAnswering

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = FlaxAutoModelForQuestionAnswering.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForQuestionAnswering.from_config(config)`}}),c0=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),f0=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForQuestionAnswering

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForQuestionAnswering.from_pretrained("bert-base-cased")

# Update configuration during loading
model = FlaxAutoModelForQuestionAnswering.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = FlaxAutoModelForQuestionAnswering.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForQuestionAnswering.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),m0=new z({}),g0=new E({props:{name:"class transformers.FlaxAutoModelForTokenClassification",anchor:"transformers.FlaxAutoModelForTokenClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/modeling_flax_auto.py#L273"}}),p0=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15682/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/albert#transformers.FlaxAlbertForTokenClassification">FlaxAlbertForTokenClassification</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/bert#transformers.FlaxBertForTokenClassification">FlaxBertForTokenClassification</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/big_bird#transformers.FlaxBigBirdForTokenClassification">FlaxBigBirdForTokenClassification</a> (BigBird model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/distilbert#transformers.FlaxDistilBertForTokenClassification">FlaxDistilBertForTokenClassification</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/electra#transformers.FlaxElectraForTokenClassification">FlaxElectraForTokenClassification</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/roformer#transformers.FlaxRoFormerForTokenClassification">FlaxRoFormerForTokenClassification</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/roberta#transformers.FlaxRobertaForTokenClassification">FlaxRobertaForTokenClassification</a> (RoBERTa model)</li>
</ul>`,name:"config"}]}}),_0=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForTokenClassification

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = FlaxAutoModelForTokenClassification.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForTokenClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForTokenClassification.from_config(config)`}}),u0=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),b0=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForTokenClassification

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForTokenClassification.from_pretrained("bert-base-cased")

# Update configuration during loading
model = FlaxAutoModelForTokenClassification.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = FlaxAutoModelForTokenClassification.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForTokenClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForTokenClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForTokenClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForTokenClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),v0=new z({}),T0=new E({props:{name:"class transformers.FlaxAutoModelForMultipleChoice",anchor:"transformers.FlaxAutoModelForMultipleChoice",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/modeling_flax_auto.py#L282"}}),C0=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15682/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/albert#transformers.FlaxAlbertForMultipleChoice">FlaxAlbertForMultipleChoice</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/bert#transformers.FlaxBertForMultipleChoice">FlaxBertForMultipleChoice</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/big_bird#transformers.FlaxBigBirdForMultipleChoice">FlaxBigBirdForMultipleChoice</a> (BigBird model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/distilbert#transformers.FlaxDistilBertForMultipleChoice">FlaxDistilBertForMultipleChoice</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/electra#transformers.FlaxElectraForMultipleChoice">FlaxElectraForMultipleChoice</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/roformer#transformers.FlaxRoFormerForMultipleChoice">FlaxRoFormerForMultipleChoice</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/roberta#transformers.FlaxRobertaForMultipleChoice">FlaxRobertaForMultipleChoice</a> (RoBERTa model)</li>
</ul>`,name:"config"}]}}),M0=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForMultipleChoice

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = FlaxAutoModelForMultipleChoice.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForMultipleChoice

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForMultipleChoice.from_config(config)`}}),E0=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),y0=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForMultipleChoice

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForMultipleChoice.from_pretrained("bert-base-cased")

# Update configuration during loading
model = FlaxAutoModelForMultipleChoice.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = FlaxAutoModelForMultipleChoice.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForMultipleChoice

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForMultipleChoice.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForMultipleChoice.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForMultipleChoice.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),w0=new z({}),A0=new E({props:{name:"class transformers.FlaxAutoModelForNextSentencePrediction",anchor:"transformers.FlaxAutoModelForNextSentencePrediction",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/modeling_flax_auto.py#L289"}}),B0=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15682/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/bert#transformers.FlaxBertForNextSentencePrediction">FlaxBertForNextSentencePrediction</a> (BERT model)</li>
</ul>`,name:"config"}]}}),k0=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForNextSentencePrediction

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = FlaxAutoModelForNextSentencePrediction.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForNextSentencePrediction

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForNextSentencePrediction.from_config(config)`}}),x0=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),R0=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForNextSentencePrediction

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForNextSentencePrediction.from_pretrained("bert-base-cased")

# Update configuration during loading
model = FlaxAutoModelForNextSentencePrediction.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = FlaxAutoModelForNextSentencePrediction.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForNextSentencePrediction

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForNextSentencePrediction.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForNextSentencePrediction.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForNextSentencePrediction.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),S0=new z({}),P0=new E({props:{name:"class transformers.FlaxAutoModelForImageClassification",anchor:"transformers.FlaxAutoModelForImageClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/modeling_flax_auto.py#L298"}}),I0=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15682/en/model_doc/beit#transformers.BeitConfig">BeitConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/beit#transformers.FlaxBeitForImageClassification">FlaxBeitForImageClassification</a> (BEiT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/vit#transformers.ViTConfig">ViTConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/vit#transformers.FlaxViTForImageClassification">FlaxViTForImageClassification</a> (ViT model)</li>
</ul>`,name:"config"}]}}),j0=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForImageClassification

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = FlaxAutoModelForImageClassification.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForImageClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForImageClassification.from_config(config)`}}),N0=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),q0=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForImageClassification

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForImageClassification.from_pretrained("bert-base-cased")

# Update configuration during loading
model = FlaxAutoModelForImageClassification.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = FlaxAutoModelForImageClassification.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForImageClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForImageClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForImageClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForImageClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),G0=new z({}),O0=new E({props:{name:"class transformers.FlaxAutoModelForVision2Seq",anchor:"transformers.FlaxAutoModelForVision2Seq",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/modeling_flax_auto.py#L307"}}),z0=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15682/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderConfig">VisionEncoderDecoderConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/vision-encoder-decoder#transformers.FlaxVisionEncoderDecoderModel">FlaxVisionEncoderDecoderModel</a> (Vision Encoder decoder model)</li>
</ul>`,name:"config"}]}}),V0=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForVision2Seq

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = FlaxAutoModelForVision2Seq.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForVision2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForVision2Seq.from_config(config)`}}),W0=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),Q0=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForVision2Seq

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForVision2Seq.from_pretrained("bert-base-cased")

# Update configuration during loading
model = FlaxAutoModelForVision2Seq.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = FlaxAutoModelForVision2Seq.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForVision2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForVision2Seq.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForVision2Seq.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForVision2Seq.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),{c(){J=a("meta"),Ae=l(),ie=a("h1"),me=a("a"),to=a("span"),f(ce.$$.fragment),ue=l(),Do=a("span"),wi=o("Auto Classes"),Ef=l(),sa=a("p"),Ai=o(`In many cases, the architecture you want to use can be guessed from the name or the path of the pretrained model you
are supplying to the `),Li=a("code"),nM=o("from_pretrained()"),yf=o(` method. AutoClasses are here to do this job for you so that you
automatically retrieve the relevant model given the name/path to the pretrained weights/config/vocabulary.`),ye=l(),io=a("p"),Bi=o("Instantiating one of "),Pn=a("a"),sM=o("AutoConfig"),$n=o(", "),In=a("a"),lM=o("AutoModel"),ki=o(`, and
`),jn=a("a"),iM=o("AutoTokenizer"),xi=o(" will directly create a class of the relevant architecture. For instance"),wf=l(),f($a.$$.fragment),co=l(),ge=a("p"),zL=o("will create a model that is an instance of "),Ri=a("a"),VL=o("BertModel"),WL=o("."),qo=l(),Ia=a("p"),QL=o("There is one class of "),Af=a("code"),HL=o("AutoModel"),Bxe=o(" for each task, and for each backend (PyTorch, TensorFlow, or Flax)."),v7e=l(),Si=a("h2"),Lf=a("a"),zV=a("span"),f(dM.$$.fragment),kxe=l(),VV=a("span"),xxe=o("Extending the Auto Classes"),T7e=l(),Nn=a("p"),Rxe=o(`Each of the auto classes has a method to be extended with your custom classes. For instance, if you have defined a
custom class of model `),WV=a("code"),Sxe=o("NewModel"),Pxe=o(", make sure you have a "),QV=a("code"),$xe=o("NewModelConfig"),Ixe=o(` then you can add those to the auto
classes like this:`),F7e=l(),f(cM.$$.fragment),C7e=l(),UL=a("p"),jxe=o("You will then be able to use the auto classes like you would usually do!"),M7e=l(),f(Bf.$$.fragment),E7e=l(),Pi=a("h2"),kf=a("a"),HV=a("span"),f(fM.$$.fragment),Nxe=l(),UV=a("span"),Dxe=o("AutoConfig"),y7e=l(),Go=a("div"),f(mM.$$.fragment),qxe=l(),gM=a("p"),Gxe=o(`This is a generic configuration class that will be instantiated as one of the configuration classes of the library
when created with the `),JL=a("a"),Oxe=o("from_pretrained()"),Xxe=o(" class method."),zxe=l(),hM=a("p"),Vxe=o("This class cannot be instantiated directly using "),JV=a("code"),Wxe=o("__init__()"),Qxe=o(" (throws an error)."),Hxe=l(),fo=a("div"),f(pM.$$.fragment),Uxe=l(),YV=a("p"),Jxe=o("Instantiate one of the configuration classes of the library from a pretrained model configuration."),Yxe=l(),$i=a("p"),Kxe=o("The configuration class to instantiate is selected based on the "),KV=a("code"),Zxe=o("model_type"),eRe=o(` property of the config object that
is loaded, or when it\u2019s missing, by falling back to using pattern matching on `),ZV=a("code"),oRe=o("pretrained_model_name_or_path"),rRe=o(":"),tRe=l(),v=a("ul"),xf=a("li"),eW=a("strong"),aRe=o("albert"),nRe=o(" \u2014 "),YL=a("a"),sRe=o("AlbertConfig"),lRe=o(" (ALBERT model)"),iRe=l(),Rf=a("li"),oW=a("strong"),dRe=o("bart"),cRe=o(" \u2014 "),KL=a("a"),fRe=o("BartConfig"),mRe=o(" (BART model)"),gRe=l(),Sf=a("li"),rW=a("strong"),hRe=o("beit"),pRe=o(" \u2014 "),ZL=a("a"),_Re=o("BeitConfig"),uRe=o(" (BEiT model)"),bRe=l(),Pf=a("li"),tW=a("strong"),vRe=o("bert"),TRe=o(" \u2014 "),e7=a("a"),FRe=o("BertConfig"),CRe=o(" (BERT model)"),MRe=l(),$f=a("li"),aW=a("strong"),ERe=o("bert-generation"),yRe=o(" \u2014 "),o7=a("a"),wRe=o("BertGenerationConfig"),ARe=o(" (Bert Generation model)"),LRe=l(),If=a("li"),nW=a("strong"),BRe=o("big_bird"),kRe=o(" \u2014 "),r7=a("a"),xRe=o("BigBirdConfig"),RRe=o(" (BigBird model)"),SRe=l(),jf=a("li"),sW=a("strong"),PRe=o("bigbird_pegasus"),$Re=o(" \u2014 "),t7=a("a"),IRe=o("BigBirdPegasusConfig"),jRe=o(" (BigBirdPegasus model)"),NRe=l(),Nf=a("li"),lW=a("strong"),DRe=o("blenderbot"),qRe=o(" \u2014 "),a7=a("a"),GRe=o("BlenderbotConfig"),ORe=o(" (Blenderbot model)"),XRe=l(),Df=a("li"),iW=a("strong"),zRe=o("blenderbot-small"),VRe=o(" \u2014 "),n7=a("a"),WRe=o("BlenderbotSmallConfig"),QRe=o(" (BlenderbotSmall model)"),HRe=l(),qf=a("li"),dW=a("strong"),URe=o("camembert"),JRe=o(" \u2014 "),s7=a("a"),YRe=o("CamembertConfig"),KRe=o(" (CamemBERT model)"),ZRe=l(),Gf=a("li"),cW=a("strong"),eSe=o("canine"),oSe=o(" \u2014 "),l7=a("a"),rSe=o("CanineConfig"),tSe=o(" (Canine model)"),aSe=l(),Of=a("li"),fW=a("strong"),nSe=o("clip"),sSe=o(" \u2014 "),i7=a("a"),lSe=o("CLIPConfig"),iSe=o(" (CLIP model)"),dSe=l(),Xf=a("li"),mW=a("strong"),cSe=o("convbert"),fSe=o(" \u2014 "),d7=a("a"),mSe=o("ConvBertConfig"),gSe=o(" (ConvBERT model)"),hSe=l(),zf=a("li"),gW=a("strong"),pSe=o("convnext"),_Se=o(" \u2014 "),c7=a("a"),uSe=o("ConvNextConfig"),bSe=o(" (ConvNext model)"),vSe=l(),Vf=a("li"),hW=a("strong"),TSe=o("ctrl"),FSe=o(" \u2014 "),f7=a("a"),CSe=o("CTRLConfig"),MSe=o(" (CTRL model)"),ESe=l(),Wf=a("li"),pW=a("strong"),ySe=o("deberta"),wSe=o(" \u2014 "),m7=a("a"),ASe=o("DebertaConfig"),LSe=o(" (DeBERTa model)"),BSe=l(),Qf=a("li"),_W=a("strong"),kSe=o("deberta-v2"),xSe=o(" \u2014 "),g7=a("a"),RSe=o("DebertaV2Config"),SSe=o(" (DeBERTa-v2 model)"),PSe=l(),Hf=a("li"),uW=a("strong"),$Se=o("deit"),ISe=o(" \u2014 "),h7=a("a"),jSe=o("DeiTConfig"),NSe=o(" (DeiT model)"),DSe=l(),Uf=a("li"),bW=a("strong"),qSe=o("detr"),GSe=o(" \u2014 "),p7=a("a"),OSe=o("DetrConfig"),XSe=o(" (DETR model)"),zSe=l(),Jf=a("li"),vW=a("strong"),VSe=o("distilbert"),WSe=o(" \u2014 "),_7=a("a"),QSe=o("DistilBertConfig"),HSe=o(" (DistilBERT model)"),USe=l(),Yf=a("li"),TW=a("strong"),JSe=o("dpr"),YSe=o(" \u2014 "),u7=a("a"),KSe=o("DPRConfig"),ZSe=o(" (DPR model)"),ePe=l(),Kf=a("li"),FW=a("strong"),oPe=o("electra"),rPe=o(" \u2014 "),b7=a("a"),tPe=o("ElectraConfig"),aPe=o(" (ELECTRA model)"),nPe=l(),Zf=a("li"),CW=a("strong"),sPe=o("encoder-decoder"),lPe=o(" \u2014 "),v7=a("a"),iPe=o("EncoderDecoderConfig"),dPe=o(" (Encoder decoder model)"),cPe=l(),em=a("li"),MW=a("strong"),fPe=o("flaubert"),mPe=o(" \u2014 "),T7=a("a"),gPe=o("FlaubertConfig"),hPe=o(" (FlauBERT model)"),pPe=l(),om=a("li"),EW=a("strong"),_Pe=o("fnet"),uPe=o(" \u2014 "),F7=a("a"),bPe=o("FNetConfig"),vPe=o(" (FNet model)"),TPe=l(),rm=a("li"),yW=a("strong"),FPe=o("fsmt"),CPe=o(" \u2014 "),C7=a("a"),MPe=o("FSMTConfig"),EPe=o(" (FairSeq Machine-Translation model)"),yPe=l(),tm=a("li"),wW=a("strong"),wPe=o("funnel"),APe=o(" \u2014 "),M7=a("a"),LPe=o("FunnelConfig"),BPe=o(" (Funnel Transformer model)"),kPe=l(),am=a("li"),AW=a("strong"),xPe=o("gpt2"),RPe=o(" \u2014 "),E7=a("a"),SPe=o("GPT2Config"),PPe=o(" (OpenAI GPT-2 model)"),$Pe=l(),nm=a("li"),LW=a("strong"),IPe=o("gpt_neo"),jPe=o(" \u2014 "),y7=a("a"),NPe=o("GPTNeoConfig"),DPe=o(" (GPT Neo model)"),qPe=l(),sm=a("li"),BW=a("strong"),GPe=o("gptj"),OPe=o(" \u2014 "),w7=a("a"),XPe=o("GPTJConfig"),zPe=o(" (GPT-J model)"),VPe=l(),lm=a("li"),kW=a("strong"),WPe=o("hubert"),QPe=o(" \u2014 "),A7=a("a"),HPe=o("HubertConfig"),UPe=o(" (Hubert model)"),JPe=l(),im=a("li"),xW=a("strong"),YPe=o("ibert"),KPe=o(" \u2014 "),L7=a("a"),ZPe=o("IBertConfig"),e$e=o(" (I-BERT model)"),o$e=l(),dm=a("li"),RW=a("strong"),r$e=o("imagegpt"),t$e=o(" \u2014 "),B7=a("a"),a$e=o("ImageGPTConfig"),n$e=o(" (ImageGPT model)"),s$e=l(),cm=a("li"),SW=a("strong"),l$e=o("layoutlm"),i$e=o(" \u2014 "),k7=a("a"),d$e=o("LayoutLMConfig"),c$e=o(" (LayoutLM model)"),f$e=l(),fm=a("li"),PW=a("strong"),m$e=o("layoutlmv2"),g$e=o(" \u2014 "),x7=a("a"),h$e=o("LayoutLMv2Config"),p$e=o(" (LayoutLMv2 model)"),_$e=l(),mm=a("li"),$W=a("strong"),u$e=o("led"),b$e=o(" \u2014 "),R7=a("a"),v$e=o("LEDConfig"),T$e=o(" (LED model)"),F$e=l(),gm=a("li"),IW=a("strong"),C$e=o("longformer"),M$e=o(" \u2014 "),S7=a("a"),E$e=o("LongformerConfig"),y$e=o(" (Longformer model)"),w$e=l(),hm=a("li"),jW=a("strong"),A$e=o("luke"),L$e=o(" \u2014 "),P7=a("a"),B$e=o("LukeConfig"),k$e=o(" (LUKE model)"),x$e=l(),pm=a("li"),NW=a("strong"),R$e=o("lxmert"),S$e=o(" \u2014 "),$7=a("a"),P$e=o("LxmertConfig"),$$e=o(" (LXMERT model)"),I$e=l(),_m=a("li"),DW=a("strong"),j$e=o("m2m_100"),N$e=o(" \u2014 "),I7=a("a"),D$e=o("M2M100Config"),q$e=o(" (M2M100 model)"),G$e=l(),um=a("li"),qW=a("strong"),O$e=o("marian"),X$e=o(" \u2014 "),j7=a("a"),z$e=o("MarianConfig"),V$e=o(" (Marian model)"),W$e=l(),bm=a("li"),GW=a("strong"),Q$e=o("maskformer"),H$e=o(" \u2014 "),N7=a("a"),U$e=o("MaskFormerConfig"),J$e=o(" (MaskFormer model)"),Y$e=l(),vm=a("li"),OW=a("strong"),K$e=o("mbart"),Z$e=o(" \u2014 "),D7=a("a"),eIe=o("MBartConfig"),oIe=o(" (mBART model)"),rIe=l(),Tm=a("li"),XW=a("strong"),tIe=o("megatron-bert"),aIe=o(" \u2014 "),q7=a("a"),nIe=o("MegatronBertConfig"),sIe=o(" (MegatronBert model)"),lIe=l(),Fm=a("li"),zW=a("strong"),iIe=o("mobilebert"),dIe=o(" \u2014 "),G7=a("a"),cIe=o("MobileBertConfig"),fIe=o(" (MobileBERT model)"),mIe=l(),Cm=a("li"),VW=a("strong"),gIe=o("mpnet"),hIe=o(" \u2014 "),O7=a("a"),pIe=o("MPNetConfig"),_Ie=o(" (MPNet model)"),uIe=l(),Mm=a("li"),WW=a("strong"),bIe=o("mt5"),vIe=o(" \u2014 "),X7=a("a"),TIe=o("MT5Config"),FIe=o(" (mT5 model)"),CIe=l(),Em=a("li"),QW=a("strong"),MIe=o("nystromformer"),EIe=o(" \u2014 "),z7=a("a"),yIe=o("NystromformerConfig"),wIe=o(" (Nystromformer model)"),AIe=l(),ym=a("li"),HW=a("strong"),LIe=o("openai-gpt"),BIe=o(" \u2014 "),V7=a("a"),kIe=o("OpenAIGPTConfig"),xIe=o(" (OpenAI GPT model)"),RIe=l(),wm=a("li"),UW=a("strong"),SIe=o("pegasus"),PIe=o(" \u2014 "),W7=a("a"),$Ie=o("PegasusConfig"),IIe=o(" (Pegasus model)"),jIe=l(),Am=a("li"),JW=a("strong"),NIe=o("perceiver"),DIe=o(" \u2014 "),Q7=a("a"),qIe=o("PerceiverConfig"),GIe=o(" (Perceiver model)"),OIe=l(),Lm=a("li"),YW=a("strong"),XIe=o("plbart"),zIe=o(" \u2014 "),H7=a("a"),VIe=o("PLBartConfig"),WIe=o(" (PLBart model)"),QIe=l(),Bm=a("li"),KW=a("strong"),HIe=o("poolformer"),UIe=o(" \u2014 "),U7=a("a"),JIe=o("PoolFormerConfig"),YIe=o(" (PoolFormer model)"),KIe=l(),km=a("li"),ZW=a("strong"),ZIe=o("prophetnet"),eje=o(" \u2014 "),J7=a("a"),oje=o("ProphetNetConfig"),rje=o(" (ProphetNet model)"),tje=l(),xm=a("li"),eQ=a("strong"),aje=o("qdqbert"),nje=o(" \u2014 "),Y7=a("a"),sje=o("QDQBertConfig"),lje=o(" (QDQBert model)"),ije=l(),Rm=a("li"),oQ=a("strong"),dje=o("rag"),cje=o(" \u2014 "),K7=a("a"),fje=o("RagConfig"),mje=o(" (RAG model)"),gje=l(),Sm=a("li"),rQ=a("strong"),hje=o("realm"),pje=o(" \u2014 "),Z7=a("a"),_je=o("RealmConfig"),uje=o(" (Realm model)"),bje=l(),Pm=a("li"),tQ=a("strong"),vje=o("reformer"),Tje=o(" \u2014 "),e9=a("a"),Fje=o("ReformerConfig"),Cje=o(" (Reformer model)"),Mje=l(),$m=a("li"),aQ=a("strong"),Eje=o("rembert"),yje=o(" \u2014 "),o9=a("a"),wje=o("RemBertConfig"),Aje=o(" (RemBERT model)"),Lje=l(),Im=a("li"),nQ=a("strong"),Bje=o("retribert"),kje=o(" \u2014 "),r9=a("a"),xje=o("RetriBertConfig"),Rje=o(" (RetriBERT model)"),Sje=l(),jm=a("li"),sQ=a("strong"),Pje=o("roberta"),$je=o(" \u2014 "),t9=a("a"),Ije=o("RobertaConfig"),jje=o(" (RoBERTa model)"),Nje=l(),Nm=a("li"),lQ=a("strong"),Dje=o("roformer"),qje=o(" \u2014 "),a9=a("a"),Gje=o("RoFormerConfig"),Oje=o(" (RoFormer model)"),Xje=l(),Dm=a("li"),iQ=a("strong"),zje=o("segformer"),Vje=o(" \u2014 "),n9=a("a"),Wje=o("SegformerConfig"),Qje=o(" (SegFormer model)"),Hje=l(),qm=a("li"),dQ=a("strong"),Uje=o("sew"),Jje=o(" \u2014 "),s9=a("a"),Yje=o("SEWConfig"),Kje=o(" (SEW model)"),Zje=l(),Gm=a("li"),cQ=a("strong"),eNe=o("sew-d"),oNe=o(" \u2014 "),l9=a("a"),rNe=o("SEWDConfig"),tNe=o(" (SEW-D model)"),aNe=l(),Om=a("li"),fQ=a("strong"),nNe=o("speech-encoder-decoder"),sNe=o(" \u2014 "),i9=a("a"),lNe=o("SpeechEncoderDecoderConfig"),iNe=o(" (Speech Encoder decoder model)"),dNe=l(),Xm=a("li"),mQ=a("strong"),cNe=o("speech_to_text"),fNe=o(" \u2014 "),d9=a("a"),mNe=o("Speech2TextConfig"),gNe=o(" (Speech2Text model)"),hNe=l(),zm=a("li"),gQ=a("strong"),pNe=o("speech_to_text_2"),_Ne=o(" \u2014 "),c9=a("a"),uNe=o("Speech2Text2Config"),bNe=o(" (Speech2Text2 model)"),vNe=l(),Vm=a("li"),hQ=a("strong"),TNe=o("splinter"),FNe=o(" \u2014 "),f9=a("a"),CNe=o("SplinterConfig"),MNe=o(" (Splinter model)"),ENe=l(),Wm=a("li"),pQ=a("strong"),yNe=o("squeezebert"),wNe=o(" \u2014 "),m9=a("a"),ANe=o("SqueezeBertConfig"),LNe=o(" (SqueezeBERT model)"),BNe=l(),Qm=a("li"),_Q=a("strong"),kNe=o("swin"),xNe=o(" \u2014 "),g9=a("a"),RNe=o("SwinConfig"),SNe=o(" (Swin model)"),PNe=l(),Hm=a("li"),uQ=a("strong"),$Ne=o("t5"),INe=o(" \u2014 "),h9=a("a"),jNe=o("T5Config"),NNe=o(" (T5 model)"),DNe=l(),Um=a("li"),bQ=a("strong"),qNe=o("tapas"),GNe=o(" \u2014 "),p9=a("a"),ONe=o("TapasConfig"),XNe=o(" (TAPAS model)"),zNe=l(),Jm=a("li"),vQ=a("strong"),VNe=o("transfo-xl"),WNe=o(" \u2014 "),_9=a("a"),QNe=o("TransfoXLConfig"),HNe=o(" (Transformer-XL model)"),UNe=l(),Ym=a("li"),TQ=a("strong"),JNe=o("trocr"),YNe=o(" \u2014 "),u9=a("a"),KNe=o("TrOCRConfig"),ZNe=o(" (TrOCR model)"),eDe=l(),Km=a("li"),FQ=a("strong"),oDe=o("unispeech"),rDe=o(" \u2014 "),b9=a("a"),tDe=o("UniSpeechConfig"),aDe=o(" (UniSpeech model)"),nDe=l(),Zm=a("li"),CQ=a("strong"),sDe=o("unispeech-sat"),lDe=o(" \u2014 "),v9=a("a"),iDe=o("UniSpeechSatConfig"),dDe=o(" (UniSpeechSat model)"),cDe=l(),eg=a("li"),MQ=a("strong"),fDe=o("vilt"),mDe=o(" \u2014 "),T9=a("a"),gDe=o("ViltConfig"),hDe=o(" (ViLT model)"),pDe=l(),og=a("li"),EQ=a("strong"),_De=o("vision-encoder-decoder"),uDe=o(" \u2014 "),F9=a("a"),bDe=o("VisionEncoderDecoderConfig"),vDe=o(" (Vision Encoder decoder model)"),TDe=l(),rg=a("li"),yQ=a("strong"),FDe=o("vision-text-dual-encoder"),CDe=o(" \u2014 "),C9=a("a"),MDe=o("VisionTextDualEncoderConfig"),EDe=o(" (VisionTextDualEncoder model)"),yDe=l(),tg=a("li"),wQ=a("strong"),wDe=o("visual_bert"),ADe=o(" \u2014 "),M9=a("a"),LDe=o("VisualBertConfig"),BDe=o(" (VisualBert model)"),kDe=l(),ag=a("li"),AQ=a("strong"),xDe=o("vit"),RDe=o(" \u2014 "),E9=a("a"),SDe=o("ViTConfig"),PDe=o(" (ViT model)"),$De=l(),ng=a("li"),LQ=a("strong"),IDe=o("vit_mae"),jDe=o(" \u2014 "),y9=a("a"),NDe=o("ViTMAEConfig"),DDe=o(" (ViTMAE model)"),qDe=l(),sg=a("li"),BQ=a("strong"),GDe=o("wav2vec2"),ODe=o(" \u2014 "),w9=a("a"),XDe=o("Wav2Vec2Config"),zDe=o(" (Wav2Vec2 model)"),VDe=l(),lg=a("li"),kQ=a("strong"),WDe=o("wavlm"),QDe=o(" \u2014 "),A9=a("a"),HDe=o("WavLMConfig"),UDe=o(" (WavLM model)"),JDe=l(),ig=a("li"),xQ=a("strong"),YDe=o("xglm"),KDe=o(" \u2014 "),L9=a("a"),ZDe=o("XGLMConfig"),eqe=o(" (XGLM model)"),oqe=l(),dg=a("li"),RQ=a("strong"),rqe=o("xlm"),tqe=o(" \u2014 "),B9=a("a"),aqe=o("XLMConfig"),nqe=o(" (XLM model)"),sqe=l(),cg=a("li"),SQ=a("strong"),lqe=o("xlm-prophetnet"),iqe=o(" \u2014 "),k9=a("a"),dqe=o("XLMProphetNetConfig"),cqe=o(" (XLMProphetNet model)"),fqe=l(),fg=a("li"),PQ=a("strong"),mqe=o("xlm-roberta"),gqe=o(" \u2014 "),x9=a("a"),hqe=o("XLMRobertaConfig"),pqe=o(" (XLM-RoBERTa model)"),_qe=l(),mg=a("li"),$Q=a("strong"),uqe=o("xlm-roberta-xl"),bqe=o(" \u2014 "),R9=a("a"),vqe=o("XLMRobertaXLConfig"),Tqe=o(" (XLM-RoBERTa-XL model)"),Fqe=l(),gg=a("li"),IQ=a("strong"),Cqe=o("xlnet"),Mqe=o(" \u2014 "),S9=a("a"),Eqe=o("XLNetConfig"),yqe=o(" (XLNet model)"),wqe=l(),hg=a("li"),jQ=a("strong"),Aqe=o("yoso"),Lqe=o(" \u2014 "),P9=a("a"),Bqe=o("YosoConfig"),kqe=o(" (YOSO model)"),xqe=l(),NQ=a("p"),Rqe=o("Examples:"),Sqe=l(),f(_M.$$.fragment),Pqe=l(),pg=a("div"),f(uM.$$.fragment),$qe=l(),DQ=a("p"),Iqe=o("Register a new configuration for this class."),w7e=l(),Ii=a("h2"),_g=a("a"),qQ=a("span"),f(bM.$$.fragment),jqe=l(),GQ=a("span"),Nqe=o("AutoTokenizer"),A7e=l(),Oo=a("div"),f(vM.$$.fragment),Dqe=l(),TM=a("p"),qqe=o(`This is a generic tokenizer class that will be instantiated as one of the tokenizer classes of the library when
created with the `),$9=a("a"),Gqe=o("AutoTokenizer.from_pretrained()"),Oqe=o(" class method."),Xqe=l(),FM=a("p"),zqe=o("This class cannot be instantiated directly using "),OQ=a("code"),Vqe=o("__init__()"),Wqe=o(" (throws an error)."),Qqe=l(),mo=a("div"),f(CM.$$.fragment),Hqe=l(),XQ=a("p"),Uqe=o("Instantiate one of the tokenizer classes of the library from a pretrained model vocabulary."),Jqe=l(),ja=a("p"),Yqe=o("The tokenizer class to instantiate is selected based on the "),zQ=a("code"),Kqe=o("model_type"),Zqe=o(` property of the config object (either
passed as an argument or loaded from `),VQ=a("code"),eGe=o("pretrained_model_name_or_path"),oGe=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),WQ=a("code"),rGe=o("pretrained_model_name_or_path"),tGe=o(":"),aGe=l(),M=a("ul"),Dn=a("li"),QQ=a("strong"),nGe=o("albert"),sGe=o(" \u2014 "),I9=a("a"),lGe=o("AlbertTokenizer"),iGe=o(" or "),j9=a("a"),dGe=o("AlbertTokenizerFast"),cGe=o(" (ALBERT model)"),fGe=l(),qn=a("li"),HQ=a("strong"),mGe=o("bart"),gGe=o(" \u2014 "),N9=a("a"),hGe=o("BartTokenizer"),pGe=o(" or "),D9=a("a"),_Ge=o("BartTokenizerFast"),uGe=o(" (BART model)"),bGe=l(),Gn=a("li"),UQ=a("strong"),vGe=o("barthez"),TGe=o(" \u2014 "),q9=a("a"),FGe=o("BarthezTokenizer"),CGe=o(" or "),G9=a("a"),MGe=o("BarthezTokenizerFast"),EGe=o(" (BARThez model)"),yGe=l(),ug=a("li"),JQ=a("strong"),wGe=o("bartpho"),AGe=o(" \u2014 "),O9=a("a"),LGe=o("BartphoTokenizer"),BGe=o(" (BARTpho model)"),kGe=l(),On=a("li"),YQ=a("strong"),xGe=o("bert"),RGe=o(" \u2014 "),X9=a("a"),SGe=o("BertTokenizer"),PGe=o(" or "),z9=a("a"),$Ge=o("BertTokenizerFast"),IGe=o(" (BERT model)"),jGe=l(),bg=a("li"),KQ=a("strong"),NGe=o("bert-generation"),DGe=o(" \u2014 "),V9=a("a"),qGe=o("BertGenerationTokenizer"),GGe=o(" (Bert Generation model)"),OGe=l(),vg=a("li"),ZQ=a("strong"),XGe=o("bert-japanese"),zGe=o(" \u2014 "),W9=a("a"),VGe=o("BertJapaneseTokenizer"),WGe=o(" (BertJapanese model)"),QGe=l(),Tg=a("li"),eH=a("strong"),HGe=o("bertweet"),UGe=o(" \u2014 "),Q9=a("a"),JGe=o("BertweetTokenizer"),YGe=o(" (Bertweet model)"),KGe=l(),Xn=a("li"),oH=a("strong"),ZGe=o("big_bird"),eOe=o(" \u2014 "),H9=a("a"),oOe=o("BigBirdTokenizer"),rOe=o(" or "),U9=a("a"),tOe=o("BigBirdTokenizerFast"),aOe=o(" (BigBird model)"),nOe=l(),zn=a("li"),rH=a("strong"),sOe=o("bigbird_pegasus"),lOe=o(" \u2014 "),J9=a("a"),iOe=o("PegasusTokenizer"),dOe=o(" or "),Y9=a("a"),cOe=o("PegasusTokenizerFast"),fOe=o(" (BigBirdPegasus model)"),mOe=l(),Vn=a("li"),tH=a("strong"),gOe=o("blenderbot"),hOe=o(" \u2014 "),K9=a("a"),pOe=o("BlenderbotTokenizer"),_Oe=o(" or "),Z9=a("a"),uOe=o("BlenderbotTokenizerFast"),bOe=o(" (Blenderbot model)"),vOe=l(),Fg=a("li"),aH=a("strong"),TOe=o("blenderbot-small"),FOe=o(" \u2014 "),eB=a("a"),COe=o("BlenderbotSmallTokenizer"),MOe=o(" (BlenderbotSmall model)"),EOe=l(),Cg=a("li"),nH=a("strong"),yOe=o("byt5"),wOe=o(" \u2014 "),oB=a("a"),AOe=o("ByT5Tokenizer"),LOe=o(" (ByT5 model)"),BOe=l(),Wn=a("li"),sH=a("strong"),kOe=o("camembert"),xOe=o(" \u2014 "),rB=a("a"),ROe=o("CamembertTokenizer"),SOe=o(" or "),tB=a("a"),POe=o("CamembertTokenizerFast"),$Oe=o(" (CamemBERT model)"),IOe=l(),Mg=a("li"),lH=a("strong"),jOe=o("canine"),NOe=o(" \u2014 "),aB=a("a"),DOe=o("CanineTokenizer"),qOe=o(" (Canine model)"),GOe=l(),Qn=a("li"),iH=a("strong"),OOe=o("clip"),XOe=o(" \u2014 "),nB=a("a"),zOe=o("CLIPTokenizer"),VOe=o(" or "),sB=a("a"),WOe=o("CLIPTokenizerFast"),QOe=o(" (CLIP model)"),HOe=l(),Hn=a("li"),dH=a("strong"),UOe=o("convbert"),JOe=o(" \u2014 "),lB=a("a"),YOe=o("ConvBertTokenizer"),KOe=o(" or "),iB=a("a"),ZOe=o("ConvBertTokenizerFast"),eXe=o(" (ConvBERT model)"),oXe=l(),Un=a("li"),cH=a("strong"),rXe=o("cpm"),tXe=o(" \u2014 "),dB=a("a"),aXe=o("CpmTokenizer"),nXe=o(" or "),fH=a("code"),sXe=o("CpmTokenizerFast"),lXe=o(" (CPM model)"),iXe=l(),Eg=a("li"),mH=a("strong"),dXe=o("ctrl"),cXe=o(" \u2014 "),cB=a("a"),fXe=o("CTRLTokenizer"),mXe=o(" (CTRL model)"),gXe=l(),Jn=a("li"),gH=a("strong"),hXe=o("deberta"),pXe=o(" \u2014 "),fB=a("a"),_Xe=o("DebertaTokenizer"),uXe=o(" or "),mB=a("a"),bXe=o("DebertaTokenizerFast"),vXe=o(" (DeBERTa model)"),TXe=l(),yg=a("li"),hH=a("strong"),FXe=o("deberta-v2"),CXe=o(" \u2014 "),gB=a("a"),MXe=o("DebertaV2Tokenizer"),EXe=o(" (DeBERTa-v2 model)"),yXe=l(),Yn=a("li"),pH=a("strong"),wXe=o("distilbert"),AXe=o(" \u2014 "),hB=a("a"),LXe=o("DistilBertTokenizer"),BXe=o(" or "),pB=a("a"),kXe=o("DistilBertTokenizerFast"),xXe=o(" (DistilBERT model)"),RXe=l(),Kn=a("li"),_H=a("strong"),SXe=o("dpr"),PXe=o(" \u2014 "),_B=a("a"),$Xe=o("DPRQuestionEncoderTokenizer"),IXe=o(" or "),uB=a("a"),jXe=o("DPRQuestionEncoderTokenizerFast"),NXe=o(" (DPR model)"),DXe=l(),Zn=a("li"),uH=a("strong"),qXe=o("electra"),GXe=o(" \u2014 "),bB=a("a"),OXe=o("ElectraTokenizer"),XXe=o(" or "),vB=a("a"),zXe=o("ElectraTokenizerFast"),VXe=o(" (ELECTRA model)"),WXe=l(),wg=a("li"),bH=a("strong"),QXe=o("flaubert"),HXe=o(" \u2014 "),TB=a("a"),UXe=o("FlaubertTokenizer"),JXe=o(" (FlauBERT model)"),YXe=l(),es=a("li"),vH=a("strong"),KXe=o("fnet"),ZXe=o(" \u2014 "),FB=a("a"),eze=o("FNetTokenizer"),oze=o(" or "),CB=a("a"),rze=o("FNetTokenizerFast"),tze=o(" (FNet model)"),aze=l(),Ag=a("li"),TH=a("strong"),nze=o("fsmt"),sze=o(" \u2014 "),MB=a("a"),lze=o("FSMTTokenizer"),ize=o(" (FairSeq Machine-Translation model)"),dze=l(),os=a("li"),FH=a("strong"),cze=o("funnel"),fze=o(" \u2014 "),EB=a("a"),mze=o("FunnelTokenizer"),gze=o(" or "),yB=a("a"),hze=o("FunnelTokenizerFast"),pze=o(" (Funnel Transformer model)"),_ze=l(),rs=a("li"),CH=a("strong"),uze=o("gpt2"),bze=o(" \u2014 "),wB=a("a"),vze=o("GPT2Tokenizer"),Tze=o(" or "),AB=a("a"),Fze=o("GPT2TokenizerFast"),Cze=o(" (OpenAI GPT-2 model)"),Mze=l(),ts=a("li"),MH=a("strong"),Eze=o("gpt_neo"),yze=o(" \u2014 "),LB=a("a"),wze=o("GPT2Tokenizer"),Aze=o(" or "),BB=a("a"),Lze=o("GPT2TokenizerFast"),Bze=o(" (GPT Neo model)"),kze=l(),as=a("li"),EH=a("strong"),xze=o("herbert"),Rze=o(" \u2014 "),kB=a("a"),Sze=o("HerbertTokenizer"),Pze=o(" or "),xB=a("a"),$ze=o("HerbertTokenizerFast"),Ize=o(" (HerBERT model)"),jze=l(),Lg=a("li"),yH=a("strong"),Nze=o("hubert"),Dze=o(" \u2014 "),RB=a("a"),qze=o("Wav2Vec2CTCTokenizer"),Gze=o(" (Hubert model)"),Oze=l(),ns=a("li"),wH=a("strong"),Xze=o("ibert"),zze=o(" \u2014 "),SB=a("a"),Vze=o("RobertaTokenizer"),Wze=o(" or "),PB=a("a"),Qze=o("RobertaTokenizerFast"),Hze=o(" (I-BERT model)"),Uze=l(),ss=a("li"),AH=a("strong"),Jze=o("layoutlm"),Yze=o(" \u2014 "),$B=a("a"),Kze=o("LayoutLMTokenizer"),Zze=o(" or "),IB=a("a"),eVe=o("LayoutLMTokenizerFast"),oVe=o(" (LayoutLM model)"),rVe=l(),ls=a("li"),LH=a("strong"),tVe=o("layoutlmv2"),aVe=o(" \u2014 "),jB=a("a"),nVe=o("LayoutLMv2Tokenizer"),sVe=o(" or "),NB=a("a"),lVe=o("LayoutLMv2TokenizerFast"),iVe=o(" (LayoutLMv2 model)"),dVe=l(),is=a("li"),BH=a("strong"),cVe=o("layoutxlm"),fVe=o(" \u2014 "),DB=a("a"),mVe=o("LayoutXLMTokenizer"),gVe=o(" or "),qB=a("a"),hVe=o("LayoutXLMTokenizerFast"),pVe=o(" (LayoutXLM model)"),_Ve=l(),ds=a("li"),kH=a("strong"),uVe=o("led"),bVe=o(" \u2014 "),GB=a("a"),vVe=o("LEDTokenizer"),TVe=o(" or "),OB=a("a"),FVe=o("LEDTokenizerFast"),CVe=o(" (LED model)"),MVe=l(),cs=a("li"),xH=a("strong"),EVe=o("longformer"),yVe=o(" \u2014 "),XB=a("a"),wVe=o("LongformerTokenizer"),AVe=o(" or "),zB=a("a"),LVe=o("LongformerTokenizerFast"),BVe=o(" (Longformer model)"),kVe=l(),Bg=a("li"),RH=a("strong"),xVe=o("luke"),RVe=o(" \u2014 "),VB=a("a"),SVe=o("LukeTokenizer"),PVe=o(" (LUKE model)"),$Ve=l(),fs=a("li"),SH=a("strong"),IVe=o("lxmert"),jVe=o(" \u2014 "),WB=a("a"),NVe=o("LxmertTokenizer"),DVe=o(" or "),QB=a("a"),qVe=o("LxmertTokenizerFast"),GVe=o(" (LXMERT model)"),OVe=l(),kg=a("li"),PH=a("strong"),XVe=o("m2m_100"),zVe=o(" \u2014 "),HB=a("a"),VVe=o("M2M100Tokenizer"),WVe=o(" (M2M100 model)"),QVe=l(),xg=a("li"),$H=a("strong"),HVe=o("marian"),UVe=o(" \u2014 "),UB=a("a"),JVe=o("MarianTokenizer"),YVe=o(" (Marian model)"),KVe=l(),ms=a("li"),IH=a("strong"),ZVe=o("mbart"),eWe=o(" \u2014 "),JB=a("a"),oWe=o("MBartTokenizer"),rWe=o(" or "),YB=a("a"),tWe=o("MBartTokenizerFast"),aWe=o(" (mBART model)"),nWe=l(),gs=a("li"),jH=a("strong"),sWe=o("mbart50"),lWe=o(" \u2014 "),KB=a("a"),iWe=o("MBart50Tokenizer"),dWe=o(" or "),ZB=a("a"),cWe=o("MBart50TokenizerFast"),fWe=o(" (mBART-50 model)"),mWe=l(),Rg=a("li"),NH=a("strong"),gWe=o("mluke"),hWe=o(" \u2014 "),ek=a("a"),pWe=o("MLukeTokenizer"),_We=o(" (mLUKE model)"),uWe=l(),hs=a("li"),DH=a("strong"),bWe=o("mobilebert"),vWe=o(" \u2014 "),ok=a("a"),TWe=o("MobileBertTokenizer"),FWe=o(" or "),rk=a("a"),CWe=o("MobileBertTokenizerFast"),MWe=o(" (MobileBERT model)"),EWe=l(),ps=a("li"),qH=a("strong"),yWe=o("mpnet"),wWe=o(" \u2014 "),tk=a("a"),AWe=o("MPNetTokenizer"),LWe=o(" or "),ak=a("a"),BWe=o("MPNetTokenizerFast"),kWe=o(" (MPNet model)"),xWe=l(),_s=a("li"),GH=a("strong"),RWe=o("mt5"),SWe=o(" \u2014 "),nk=a("a"),PWe=o("MT5Tokenizer"),$We=o(" or "),sk=a("a"),IWe=o("MT5TokenizerFast"),jWe=o(" (mT5 model)"),NWe=l(),us=a("li"),OH=a("strong"),DWe=o("openai-gpt"),qWe=o(" \u2014 "),lk=a("a"),GWe=o("OpenAIGPTTokenizer"),OWe=o(" or "),ik=a("a"),XWe=o("OpenAIGPTTokenizerFast"),zWe=o(" (OpenAI GPT model)"),VWe=l(),bs=a("li"),XH=a("strong"),WWe=o("pegasus"),QWe=o(" \u2014 "),dk=a("a"),HWe=o("PegasusTokenizer"),UWe=o(" or "),ck=a("a"),JWe=o("PegasusTokenizerFast"),YWe=o(" (Pegasus model)"),KWe=l(),Sg=a("li"),zH=a("strong"),ZWe=o("perceiver"),eQe=o(" \u2014 "),fk=a("a"),oQe=o("PerceiverTokenizer"),rQe=o(" (Perceiver model)"),tQe=l(),Pg=a("li"),VH=a("strong"),aQe=o("phobert"),nQe=o(" \u2014 "),mk=a("a"),sQe=o("PhobertTokenizer"),lQe=o(" (PhoBERT model)"),iQe=l(),$g=a("li"),WH=a("strong"),dQe=o("plbart"),cQe=o(" \u2014 "),gk=a("a"),fQe=o("PLBartTokenizer"),mQe=o(" (PLBart model)"),gQe=l(),Ig=a("li"),QH=a("strong"),hQe=o("prophetnet"),pQe=o(" \u2014 "),hk=a("a"),_Qe=o("ProphetNetTokenizer"),uQe=o(" (ProphetNet model)"),bQe=l(),vs=a("li"),HH=a("strong"),vQe=o("qdqbert"),TQe=o(" \u2014 "),pk=a("a"),FQe=o("BertTokenizer"),CQe=o(" or "),_k=a("a"),MQe=o("BertTokenizerFast"),EQe=o(" (QDQBert model)"),yQe=l(),jg=a("li"),UH=a("strong"),wQe=o("rag"),AQe=o(" \u2014 "),uk=a("a"),LQe=o("RagTokenizer"),BQe=o(" (RAG model)"),kQe=l(),Ts=a("li"),JH=a("strong"),xQe=o("reformer"),RQe=o(" \u2014 "),bk=a("a"),SQe=o("ReformerTokenizer"),PQe=o(" or "),vk=a("a"),$Qe=o("ReformerTokenizerFast"),IQe=o(" (Reformer model)"),jQe=l(),Fs=a("li"),YH=a("strong"),NQe=o("rembert"),DQe=o(" \u2014 "),Tk=a("a"),qQe=o("RemBertTokenizer"),GQe=o(" or "),Fk=a("a"),OQe=o("RemBertTokenizerFast"),XQe=o(" (RemBERT model)"),zQe=l(),Cs=a("li"),KH=a("strong"),VQe=o("retribert"),WQe=o(" \u2014 "),Ck=a("a"),QQe=o("RetriBertTokenizer"),HQe=o(" or "),Mk=a("a"),UQe=o("RetriBertTokenizerFast"),JQe=o(" (RetriBERT model)"),YQe=l(),Ms=a("li"),ZH=a("strong"),KQe=o("roberta"),ZQe=o(" \u2014 "),Ek=a("a"),eHe=o("RobertaTokenizer"),oHe=o(" or "),yk=a("a"),rHe=o("RobertaTokenizerFast"),tHe=o(" (RoBERTa model)"),aHe=l(),Es=a("li"),eU=a("strong"),nHe=o("roformer"),sHe=o(" \u2014 "),wk=a("a"),lHe=o("RoFormerTokenizer"),iHe=o(" or "),Ak=a("a"),dHe=o("RoFormerTokenizerFast"),cHe=o(" (RoFormer model)"),fHe=l(),Ng=a("li"),oU=a("strong"),mHe=o("speech_to_text"),gHe=o(" \u2014 "),Lk=a("a"),hHe=o("Speech2TextTokenizer"),pHe=o(" (Speech2Text model)"),_He=l(),Dg=a("li"),rU=a("strong"),uHe=o("speech_to_text_2"),bHe=o(" \u2014 "),Bk=a("a"),vHe=o("Speech2Text2Tokenizer"),THe=o(" (Speech2Text2 model)"),FHe=l(),ys=a("li"),tU=a("strong"),CHe=o("splinter"),MHe=o(" \u2014 "),kk=a("a"),EHe=o("SplinterTokenizer"),yHe=o(" or "),xk=a("a"),wHe=o("SplinterTokenizerFast"),AHe=o(" (Splinter model)"),LHe=l(),ws=a("li"),aU=a("strong"),BHe=o("squeezebert"),kHe=o(" \u2014 "),Rk=a("a"),xHe=o("SqueezeBertTokenizer"),RHe=o(" or "),Sk=a("a"),SHe=o("SqueezeBertTokenizerFast"),PHe=o(" (SqueezeBERT model)"),$He=l(),As=a("li"),nU=a("strong"),IHe=o("t5"),jHe=o(" \u2014 "),Pk=a("a"),NHe=o("T5Tokenizer"),DHe=o(" or "),$k=a("a"),qHe=o("T5TokenizerFast"),GHe=o(" (T5 model)"),OHe=l(),qg=a("li"),sU=a("strong"),XHe=o("tapas"),zHe=o(" \u2014 "),Ik=a("a"),VHe=o("TapasTokenizer"),WHe=o(" (TAPAS model)"),QHe=l(),Gg=a("li"),lU=a("strong"),HHe=o("transfo-xl"),UHe=o(" \u2014 "),jk=a("a"),JHe=o("TransfoXLTokenizer"),YHe=o(" (Transformer-XL model)"),KHe=l(),Og=a("li"),iU=a("strong"),ZHe=o("wav2vec2"),eUe=o(" \u2014 "),Nk=a("a"),oUe=o("Wav2Vec2CTCTokenizer"),rUe=o(" (Wav2Vec2 model)"),tUe=l(),Xg=a("li"),dU=a("strong"),aUe=o("wav2vec2_phoneme"),nUe=o(" \u2014 "),Dk=a("a"),sUe=o("Wav2Vec2PhonemeCTCTokenizer"),lUe=o(" (Wav2Vec2Phoneme model)"),iUe=l(),Ls=a("li"),cU=a("strong"),dUe=o("xglm"),cUe=o(" \u2014 "),qk=a("a"),fUe=o("XGLMTokenizer"),mUe=o(" or "),Gk=a("a"),gUe=o("XGLMTokenizerFast"),hUe=o(" (XGLM model)"),pUe=l(),zg=a("li"),fU=a("strong"),_Ue=o("xlm"),uUe=o(" \u2014 "),Ok=a("a"),bUe=o("XLMTokenizer"),vUe=o(" (XLM model)"),TUe=l(),Vg=a("li"),mU=a("strong"),FUe=o("xlm-prophetnet"),CUe=o(" \u2014 "),Xk=a("a"),MUe=o("XLMProphetNetTokenizer"),EUe=o(" (XLMProphetNet model)"),yUe=l(),Bs=a("li"),gU=a("strong"),wUe=o("xlm-roberta"),AUe=o(" \u2014 "),zk=a("a"),LUe=o("XLMRobertaTokenizer"),BUe=o(" or "),Vk=a("a"),kUe=o("XLMRobertaTokenizerFast"),xUe=o(" (XLM-RoBERTa model)"),RUe=l(),ks=a("li"),hU=a("strong"),SUe=o("xlnet"),PUe=o(" \u2014 "),Wk=a("a"),$Ue=o("XLNetTokenizer"),IUe=o(" or "),Qk=a("a"),jUe=o("XLNetTokenizerFast"),NUe=o(" (XLNet model)"),DUe=l(),pU=a("p"),qUe=o("Examples:"),GUe=l(),f(MM.$$.fragment),OUe=l(),Wg=a("div"),f(EM.$$.fragment),XUe=l(),_U=a("p"),zUe=o("Register a new tokenizer in this mapping."),L7e=l(),ji=a("h2"),Qg=a("a"),uU=a("span"),f(yM.$$.fragment),VUe=l(),bU=a("span"),WUe=o("AutoFeatureExtractor"),B7e=l(),Xo=a("div"),f(wM.$$.fragment),QUe=l(),AM=a("p"),HUe=o(`This is a generic feature extractor class that will be instantiated as one of the feature extractor classes of the
library when created with the `),Hk=a("a"),UUe=o("AutoFeatureExtractor.from_pretrained()"),JUe=o(" class method."),YUe=l(),LM=a("p"),KUe=o("This class cannot be instantiated directly using "),vU=a("code"),ZUe=o("__init__()"),eJe=o(" (throws an error)."),oJe=l(),Le=a("div"),f(BM.$$.fragment),rJe=l(),TU=a("p"),tJe=o("Instantiate one of the feature extractor classes of the library from a pretrained model vocabulary."),aJe=l(),Na=a("p"),nJe=o("The feature extractor class to instantiate is selected based on the "),FU=a("code"),sJe=o("model_type"),lJe=o(` property of the config object
(either passed as an argument or loaded from `),CU=a("code"),iJe=o("pretrained_model_name_or_path"),dJe=o(` if possible), or when it\u2019s
missing, by falling back to using pattern matching on `),MU=a("code"),cJe=o("pretrained_model_name_or_path"),fJe=o(":"),mJe=l(),se=a("ul"),Hg=a("li"),EU=a("strong"),gJe=o("beit"),hJe=o(" \u2014 "),Uk=a("a"),pJe=o("BeitFeatureExtractor"),_Je=o(" (BEiT model)"),uJe=l(),Ug=a("li"),yU=a("strong"),bJe=o("clip"),vJe=o(" \u2014 "),Jk=a("a"),TJe=o("CLIPFeatureExtractor"),FJe=o(" (CLIP model)"),CJe=l(),Jg=a("li"),wU=a("strong"),MJe=o("convnext"),EJe=o(" \u2014 "),Yk=a("a"),yJe=o("ConvNextFeatureExtractor"),wJe=o(" (ConvNext model)"),AJe=l(),Yg=a("li"),AU=a("strong"),LJe=o("deit"),BJe=o(" \u2014 "),Kk=a("a"),kJe=o("DeiTFeatureExtractor"),xJe=o(" (DeiT model)"),RJe=l(),Kg=a("li"),LU=a("strong"),SJe=o("detr"),PJe=o(" \u2014 "),Zk=a("a"),$Je=o("DetrFeatureExtractor"),IJe=o(" (DETR model)"),jJe=l(),Zg=a("li"),BU=a("strong"),NJe=o("hubert"),DJe=o(" \u2014 "),ex=a("a"),qJe=o("Wav2Vec2FeatureExtractor"),GJe=o(" (Hubert model)"),OJe=l(),eh=a("li"),kU=a("strong"),XJe=o("layoutlmv2"),zJe=o(" \u2014 "),ox=a("a"),VJe=o("LayoutLMv2FeatureExtractor"),WJe=o(" (LayoutLMv2 model)"),QJe=l(),oh=a("li"),xU=a("strong"),HJe=o("perceiver"),UJe=o(" \u2014 "),rx=a("a"),JJe=o("PerceiverFeatureExtractor"),YJe=o(" (Perceiver model)"),KJe=l(),rh=a("li"),RU=a("strong"),ZJe=o("poolformer"),eYe=o(" \u2014 "),tx=a("a"),oYe=o("PoolFormerFeatureExtractor"),rYe=o(" (PoolFormer model)"),tYe=l(),th=a("li"),SU=a("strong"),aYe=o("segformer"),nYe=o(" \u2014 "),ax=a("a"),sYe=o("SegformerFeatureExtractor"),lYe=o(" (SegFormer model)"),iYe=l(),ah=a("li"),PU=a("strong"),dYe=o("speech_to_text"),cYe=o(" \u2014 "),nx=a("a"),fYe=o("Speech2TextFeatureExtractor"),mYe=o(" (Speech2Text model)"),gYe=l(),nh=a("li"),$U=a("strong"),hYe=o("swin"),pYe=o(" \u2014 "),sx=a("a"),_Ye=o("ViTFeatureExtractor"),uYe=o(" (Swin model)"),bYe=l(),sh=a("li"),IU=a("strong"),vYe=o("vit"),TYe=o(" \u2014 "),lx=a("a"),FYe=o("ViTFeatureExtractor"),CYe=o(" (ViT model)"),MYe=l(),lh=a("li"),jU=a("strong"),EYe=o("vit_mae"),yYe=o(" \u2014 "),ix=a("a"),wYe=o("ViTFeatureExtractor"),AYe=o(" (ViTMAE model)"),LYe=l(),ih=a("li"),NU=a("strong"),BYe=o("wav2vec2"),kYe=o(" \u2014 "),dx=a("a"),xYe=o("Wav2Vec2FeatureExtractor"),RYe=o(" (Wav2Vec2 model)"),SYe=l(),f(dh.$$.fragment),PYe=l(),DU=a("p"),$Ye=o("Examples:"),IYe=l(),f(kM.$$.fragment),jYe=l(),ch=a("div"),f(xM.$$.fragment),NYe=l(),qU=a("p"),DYe=o("Register a new feature extractor for this class."),k7e=l(),Ni=a("h2"),fh=a("a"),GU=a("span"),f(RM.$$.fragment),qYe=l(),OU=a("span"),GYe=o("AutoProcessor"),x7e=l(),zo=a("div"),f(SM.$$.fragment),OYe=l(),PM=a("p"),XYe=o(`This is a generic processor class that will be instantiated as one of the processor classes of the library when
created with the `),cx=a("a"),zYe=o("AutoProcessor.from_pretrained()"),VYe=o(" class method."),WYe=l(),$M=a("p"),QYe=o("This class cannot be instantiated directly using "),XU=a("code"),HYe=o("__init__()"),UYe=o(" (throws an error)."),JYe=l(),Be=a("div"),f(IM.$$.fragment),YYe=l(),zU=a("p"),KYe=o("Instantiate one of the processor classes of the library from a pretrained model vocabulary."),ZYe=l(),Di=a("p"),eKe=o("The processor class to instantiate is selected based on the "),VU=a("code"),oKe=o("model_type"),rKe=o(` property of the config object (either
passed as an argument or loaded from `),WU=a("code"),tKe=o("pretrained_model_name_or_path"),aKe=o(" if possible):"),nKe=l(),we=a("ul"),mh=a("li"),QU=a("strong"),sKe=o("clip"),lKe=o(" \u2014 "),fx=a("a"),iKe=o("CLIPProcessor"),dKe=o(" (CLIP model)"),cKe=l(),gh=a("li"),HU=a("strong"),fKe=o("layoutlmv2"),mKe=o(" \u2014 "),mx=a("a"),gKe=o("LayoutLMv2Processor"),hKe=o(" (LayoutLMv2 model)"),pKe=l(),hh=a("li"),UU=a("strong"),_Ke=o("layoutxlm"),uKe=o(" \u2014 "),gx=a("a"),bKe=o("LayoutXLMProcessor"),vKe=o(" (LayoutXLM model)"),TKe=l(),ph=a("li"),JU=a("strong"),FKe=o("speech_to_text"),CKe=o(" \u2014 "),hx=a("a"),MKe=o("Speech2TextProcessor"),EKe=o(" (Speech2Text model)"),yKe=l(),_h=a("li"),YU=a("strong"),wKe=o("speech_to_text_2"),AKe=o(" \u2014 "),px=a("a"),LKe=o("Speech2Text2Processor"),BKe=o(" (Speech2Text2 model)"),kKe=l(),uh=a("li"),KU=a("strong"),xKe=o("trocr"),RKe=o(" \u2014 "),_x=a("a"),SKe=o("TrOCRProcessor"),PKe=o(" (TrOCR model)"),$Ke=l(),bh=a("li"),ZU=a("strong"),IKe=o("vision-text-dual-encoder"),jKe=o(" \u2014 "),ux=a("a"),NKe=o("VisionTextDualEncoderProcessor"),DKe=o(" (VisionTextDualEncoder model)"),qKe=l(),vh=a("li"),eJ=a("strong"),GKe=o("wav2vec2"),OKe=o(" \u2014 "),bx=a("a"),XKe=o("Wav2Vec2Processor"),zKe=o(" (Wav2Vec2 model)"),VKe=l(),f(Th.$$.fragment),WKe=l(),oJ=a("p"),QKe=o("Examples:"),HKe=l(),f(jM.$$.fragment),UKe=l(),Fh=a("div"),f(NM.$$.fragment),JKe=l(),rJ=a("p"),YKe=o("Register a new processor for this class."),R7e=l(),qi=a("h2"),Ch=a("a"),tJ=a("span"),f(DM.$$.fragment),KKe=l(),aJ=a("span"),ZKe=o("AutoModel"),S7e=l(),Vo=a("div"),f(qM.$$.fragment),eZe=l(),Gi=a("p"),oZe=o(`This is a generic model class that will be instantiated as one of the base model classes of the library when created
with the `),nJ=a("code"),rZe=o("from_pretrained()"),tZe=o("class method or the "),sJ=a("code"),aZe=o("from_config()"),nZe=o(`class
method.`),sZe=l(),GM=a("p"),lZe=o("This class cannot be instantiated directly using "),lJ=a("code"),iZe=o("__init__()"),dZe=o(" (throws an error)."),cZe=l(),Nr=a("div"),f(OM.$$.fragment),fZe=l(),iJ=a("p"),mZe=o("Instantiates one of the base model classes of the library from a configuration."),gZe=l(),Oi=a("p"),hZe=o(`Note:
Loading a model from its configuration file does `),dJ=a("strong"),pZe=o("not"),_Ze=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),cJ=a("code"),uZe=o("from_pretrained()"),bZe=o("to load the model weights."),vZe=l(),fJ=a("p"),TZe=o("Examples:"),FZe=l(),f(XM.$$.fragment),CZe=l(),ke=a("div"),f(zM.$$.fragment),MZe=l(),mJ=a("p"),EZe=o("Instantiate one of the base model classes of the library from a pretrained model."),yZe=l(),Da=a("p"),wZe=o("The model class to instantiate is selected based on the "),gJ=a("code"),AZe=o("model_type"),LZe=o(` property of the config object (either
passed as an argument or loaded from `),hJ=a("code"),BZe=o("pretrained_model_name_or_path"),kZe=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),pJ=a("code"),xZe=o("pretrained_model_name_or_path"),RZe=o(":"),SZe=l(),F=a("ul"),Mh=a("li"),_J=a("strong"),PZe=o("albert"),$Ze=o(" \u2014 "),vx=a("a"),IZe=o("AlbertModel"),jZe=o(" (ALBERT model)"),NZe=l(),Eh=a("li"),uJ=a("strong"),DZe=o("bart"),qZe=o(" \u2014 "),Tx=a("a"),GZe=o("BartModel"),OZe=o(" (BART model)"),XZe=l(),yh=a("li"),bJ=a("strong"),zZe=o("beit"),VZe=o(" \u2014 "),Fx=a("a"),WZe=o("BeitModel"),QZe=o(" (BEiT model)"),HZe=l(),wh=a("li"),vJ=a("strong"),UZe=o("bert"),JZe=o(" \u2014 "),Cx=a("a"),YZe=o("BertModel"),KZe=o(" (BERT model)"),ZZe=l(),Ah=a("li"),TJ=a("strong"),eeo=o("bert-generation"),oeo=o(" \u2014 "),Mx=a("a"),reo=o("BertGenerationEncoder"),teo=o(" (Bert Generation model)"),aeo=l(),Lh=a("li"),FJ=a("strong"),neo=o("big_bird"),seo=o(" \u2014 "),Ex=a("a"),leo=o("BigBirdModel"),ieo=o(" (BigBird model)"),deo=l(),Bh=a("li"),CJ=a("strong"),ceo=o("bigbird_pegasus"),feo=o(" \u2014 "),yx=a("a"),meo=o("BigBirdPegasusModel"),geo=o(" (BigBirdPegasus model)"),heo=l(),kh=a("li"),MJ=a("strong"),peo=o("blenderbot"),_eo=o(" \u2014 "),wx=a("a"),ueo=o("BlenderbotModel"),beo=o(" (Blenderbot model)"),veo=l(),xh=a("li"),EJ=a("strong"),Teo=o("blenderbot-small"),Feo=o(" \u2014 "),Ax=a("a"),Ceo=o("BlenderbotSmallModel"),Meo=o(" (BlenderbotSmall model)"),Eeo=l(),Rh=a("li"),yJ=a("strong"),yeo=o("camembert"),weo=o(" \u2014 "),Lx=a("a"),Aeo=o("CamembertModel"),Leo=o(" (CamemBERT model)"),Beo=l(),Sh=a("li"),wJ=a("strong"),keo=o("canine"),xeo=o(" \u2014 "),Bx=a("a"),Reo=o("CanineModel"),Seo=o(" (Canine model)"),Peo=l(),Ph=a("li"),AJ=a("strong"),$eo=o("clip"),Ieo=o(" \u2014 "),kx=a("a"),jeo=o("CLIPModel"),Neo=o(" (CLIP model)"),Deo=l(),$h=a("li"),LJ=a("strong"),qeo=o("convbert"),Geo=o(" \u2014 "),xx=a("a"),Oeo=o("ConvBertModel"),Xeo=o(" (ConvBERT model)"),zeo=l(),Ih=a("li"),BJ=a("strong"),Veo=o("convnext"),Weo=o(" \u2014 "),Rx=a("a"),Qeo=o("ConvNextModel"),Heo=o(" (ConvNext model)"),Ueo=l(),jh=a("li"),kJ=a("strong"),Jeo=o("ctrl"),Yeo=o(" \u2014 "),Sx=a("a"),Keo=o("CTRLModel"),Zeo=o(" (CTRL model)"),eoo=l(),Nh=a("li"),xJ=a("strong"),ooo=o("deberta"),roo=o(" \u2014 "),Px=a("a"),too=o("DebertaModel"),aoo=o(" (DeBERTa model)"),noo=l(),Dh=a("li"),RJ=a("strong"),soo=o("deberta-v2"),loo=o(" \u2014 "),$x=a("a"),ioo=o("DebertaV2Model"),doo=o(" (DeBERTa-v2 model)"),coo=l(),qh=a("li"),SJ=a("strong"),foo=o("deit"),moo=o(" \u2014 "),Ix=a("a"),goo=o("DeiTModel"),hoo=o(" (DeiT model)"),poo=l(),Gh=a("li"),PJ=a("strong"),_oo=o("detr"),uoo=o(" \u2014 "),jx=a("a"),boo=o("DetrModel"),voo=o(" (DETR model)"),Too=l(),Oh=a("li"),$J=a("strong"),Foo=o("distilbert"),Coo=o(" \u2014 "),Nx=a("a"),Moo=o("DistilBertModel"),Eoo=o(" (DistilBERT model)"),yoo=l(),Xh=a("li"),IJ=a("strong"),woo=o("dpr"),Aoo=o(" \u2014 "),Dx=a("a"),Loo=o("DPRQuestionEncoder"),Boo=o(" (DPR model)"),koo=l(),zh=a("li"),jJ=a("strong"),xoo=o("electra"),Roo=o(" \u2014 "),qx=a("a"),Soo=o("ElectraModel"),Poo=o(" (ELECTRA model)"),$oo=l(),Vh=a("li"),NJ=a("strong"),Ioo=o("flaubert"),joo=o(" \u2014 "),Gx=a("a"),Noo=o("FlaubertModel"),Doo=o(" (FlauBERT model)"),qoo=l(),Wh=a("li"),DJ=a("strong"),Goo=o("fnet"),Ooo=o(" \u2014 "),Ox=a("a"),Xoo=o("FNetModel"),zoo=o(" (FNet model)"),Voo=l(),Qh=a("li"),qJ=a("strong"),Woo=o("fsmt"),Qoo=o(" \u2014 "),Xx=a("a"),Hoo=o("FSMTModel"),Uoo=o(" (FairSeq Machine-Translation model)"),Joo=l(),xs=a("li"),GJ=a("strong"),Yoo=o("funnel"),Koo=o(" \u2014 "),zx=a("a"),Zoo=o("FunnelModel"),ero=o(" or "),Vx=a("a"),oro=o("FunnelBaseModel"),rro=o(" (Funnel Transformer model)"),tro=l(),Hh=a("li"),OJ=a("strong"),aro=o("gpt2"),nro=o(" \u2014 "),Wx=a("a"),sro=o("GPT2Model"),lro=o(" (OpenAI GPT-2 model)"),iro=l(),Uh=a("li"),XJ=a("strong"),dro=o("gpt_neo"),cro=o(" \u2014 "),Qx=a("a"),fro=o("GPTNeoModel"),mro=o(" (GPT Neo model)"),gro=l(),Jh=a("li"),zJ=a("strong"),hro=o("gptj"),pro=o(" \u2014 "),Hx=a("a"),_ro=o("GPTJModel"),uro=o(" (GPT-J model)"),bro=l(),Yh=a("li"),VJ=a("strong"),vro=o("hubert"),Tro=o(" \u2014 "),Ux=a("a"),Fro=o("HubertModel"),Cro=o(" (Hubert model)"),Mro=l(),Kh=a("li"),WJ=a("strong"),Ero=o("ibert"),yro=o(" \u2014 "),Jx=a("a"),wro=o("IBertModel"),Aro=o(" (I-BERT model)"),Lro=l(),Zh=a("li"),QJ=a("strong"),Bro=o("imagegpt"),kro=o(" \u2014 "),Yx=a("a"),xro=o("ImageGPTModel"),Rro=o(" (ImageGPT model)"),Sro=l(),ep=a("li"),HJ=a("strong"),Pro=o("layoutlm"),$ro=o(" \u2014 "),Kx=a("a"),Iro=o("LayoutLMModel"),jro=o(" (LayoutLM model)"),Nro=l(),op=a("li"),UJ=a("strong"),Dro=o("layoutlmv2"),qro=o(" \u2014 "),Zx=a("a"),Gro=o("LayoutLMv2Model"),Oro=o(" (LayoutLMv2 model)"),Xro=l(),rp=a("li"),JJ=a("strong"),zro=o("led"),Vro=o(" \u2014 "),eR=a("a"),Wro=o("LEDModel"),Qro=o(" (LED model)"),Hro=l(),tp=a("li"),YJ=a("strong"),Uro=o("longformer"),Jro=o(" \u2014 "),oR=a("a"),Yro=o("LongformerModel"),Kro=o(" (Longformer model)"),Zro=l(),ap=a("li"),KJ=a("strong"),eto=o("luke"),oto=o(" \u2014 "),rR=a("a"),rto=o("LukeModel"),tto=o(" (LUKE model)"),ato=l(),np=a("li"),ZJ=a("strong"),nto=o("lxmert"),sto=o(" \u2014 "),tR=a("a"),lto=o("LxmertModel"),ito=o(" (LXMERT model)"),dto=l(),sp=a("li"),eY=a("strong"),cto=o("m2m_100"),fto=o(" \u2014 "),aR=a("a"),mto=o("M2M100Model"),gto=o(" (M2M100 model)"),hto=l(),lp=a("li"),oY=a("strong"),pto=o("marian"),_to=o(" \u2014 "),nR=a("a"),uto=o("MarianModel"),bto=o(" (Marian model)"),vto=l(),ip=a("li"),rY=a("strong"),Tto=o("maskformer"),Fto=o(" \u2014 "),sR=a("a"),Cto=o("MaskFormerModel"),Mto=o(" (MaskFormer model)"),Eto=l(),dp=a("li"),tY=a("strong"),yto=o("mbart"),wto=o(" \u2014 "),lR=a("a"),Ato=o("MBartModel"),Lto=o(" (mBART model)"),Bto=l(),cp=a("li"),aY=a("strong"),kto=o("megatron-bert"),xto=o(" \u2014 "),iR=a("a"),Rto=o("MegatronBertModel"),Sto=o(" (MegatronBert model)"),Pto=l(),fp=a("li"),nY=a("strong"),$to=o("mobilebert"),Ito=o(" \u2014 "),dR=a("a"),jto=o("MobileBertModel"),Nto=o(" (MobileBERT model)"),Dto=l(),mp=a("li"),sY=a("strong"),qto=o("mpnet"),Gto=o(" \u2014 "),cR=a("a"),Oto=o("MPNetModel"),Xto=o(" (MPNet model)"),zto=l(),gp=a("li"),lY=a("strong"),Vto=o("mt5"),Wto=o(" \u2014 "),fR=a("a"),Qto=o("MT5Model"),Hto=o(" (mT5 model)"),Uto=l(),hp=a("li"),iY=a("strong"),Jto=o("nystromformer"),Yto=o(" \u2014 "),mR=a("a"),Kto=o("NystromformerModel"),Zto=o(" (Nystromformer model)"),eao=l(),pp=a("li"),dY=a("strong"),oao=o("openai-gpt"),rao=o(" \u2014 "),gR=a("a"),tao=o("OpenAIGPTModel"),aao=o(" (OpenAI GPT model)"),nao=l(),_p=a("li"),cY=a("strong"),sao=o("pegasus"),lao=o(" \u2014 "),hR=a("a"),iao=o("PegasusModel"),dao=o(" (Pegasus model)"),cao=l(),up=a("li"),fY=a("strong"),fao=o("perceiver"),mao=o(" \u2014 "),pR=a("a"),gao=o("PerceiverModel"),hao=o(" (Perceiver model)"),pao=l(),bp=a("li"),mY=a("strong"),_ao=o("plbart"),uao=o(" \u2014 "),_R=a("a"),bao=o("PLBartModel"),vao=o(" (PLBart model)"),Tao=l(),vp=a("li"),gY=a("strong"),Fao=o("poolformer"),Cao=o(" \u2014 "),uR=a("a"),Mao=o("PoolFormerModel"),Eao=o(" (PoolFormer model)"),yao=l(),Tp=a("li"),hY=a("strong"),wao=o("prophetnet"),Aao=o(" \u2014 "),bR=a("a"),Lao=o("ProphetNetModel"),Bao=o(" (ProphetNet model)"),kao=l(),Fp=a("li"),pY=a("strong"),xao=o("qdqbert"),Rao=o(" \u2014 "),vR=a("a"),Sao=o("QDQBertModel"),Pao=o(" (QDQBert model)"),$ao=l(),Cp=a("li"),_Y=a("strong"),Iao=o("reformer"),jao=o(" \u2014 "),TR=a("a"),Nao=o("ReformerModel"),Dao=o(" (Reformer model)"),qao=l(),Mp=a("li"),uY=a("strong"),Gao=o("rembert"),Oao=o(" \u2014 "),FR=a("a"),Xao=o("RemBertModel"),zao=o(" (RemBERT model)"),Vao=l(),Ep=a("li"),bY=a("strong"),Wao=o("retribert"),Qao=o(" \u2014 "),CR=a("a"),Hao=o("RetriBertModel"),Uao=o(" (RetriBERT model)"),Jao=l(),yp=a("li"),vY=a("strong"),Yao=o("roberta"),Kao=o(" \u2014 "),MR=a("a"),Zao=o("RobertaModel"),eno=o(" (RoBERTa model)"),ono=l(),wp=a("li"),TY=a("strong"),rno=o("roformer"),tno=o(" \u2014 "),ER=a("a"),ano=o("RoFormerModel"),nno=o(" (RoFormer model)"),sno=l(),Ap=a("li"),FY=a("strong"),lno=o("segformer"),ino=o(" \u2014 "),yR=a("a"),dno=o("SegformerModel"),cno=o(" (SegFormer model)"),fno=l(),Lp=a("li"),CY=a("strong"),mno=o("sew"),gno=o(" \u2014 "),wR=a("a"),hno=o("SEWModel"),pno=o(" (SEW model)"),_no=l(),Bp=a("li"),MY=a("strong"),uno=o("sew-d"),bno=o(" \u2014 "),AR=a("a"),vno=o("SEWDModel"),Tno=o(" (SEW-D model)"),Fno=l(),kp=a("li"),EY=a("strong"),Cno=o("speech_to_text"),Mno=o(" \u2014 "),LR=a("a"),Eno=o("Speech2TextModel"),yno=o(" (Speech2Text model)"),wno=l(),xp=a("li"),yY=a("strong"),Ano=o("splinter"),Lno=o(" \u2014 "),BR=a("a"),Bno=o("SplinterModel"),kno=o(" (Splinter model)"),xno=l(),Rp=a("li"),wY=a("strong"),Rno=o("squeezebert"),Sno=o(" \u2014 "),kR=a("a"),Pno=o("SqueezeBertModel"),$no=o(" (SqueezeBERT model)"),Ino=l(),Sp=a("li"),AY=a("strong"),jno=o("swin"),Nno=o(" \u2014 "),xR=a("a"),Dno=o("SwinModel"),qno=o(" (Swin model)"),Gno=l(),Pp=a("li"),LY=a("strong"),Ono=o("t5"),Xno=o(" \u2014 "),RR=a("a"),zno=o("T5Model"),Vno=o(" (T5 model)"),Wno=l(),$p=a("li"),BY=a("strong"),Qno=o("tapas"),Hno=o(" \u2014 "),SR=a("a"),Uno=o("TapasModel"),Jno=o(" (TAPAS model)"),Yno=l(),Ip=a("li"),kY=a("strong"),Kno=o("transfo-xl"),Zno=o(" \u2014 "),PR=a("a"),eso=o("TransfoXLModel"),oso=o(" (Transformer-XL model)"),rso=l(),jp=a("li"),xY=a("strong"),tso=o("unispeech"),aso=o(" \u2014 "),$R=a("a"),nso=o("UniSpeechModel"),sso=o(" (UniSpeech model)"),lso=l(),Np=a("li"),RY=a("strong"),iso=o("unispeech-sat"),dso=o(" \u2014 "),IR=a("a"),cso=o("UniSpeechSatModel"),fso=o(" (UniSpeechSat model)"),mso=l(),Dp=a("li"),SY=a("strong"),gso=o("vilt"),hso=o(" \u2014 "),jR=a("a"),pso=o("ViltModel"),_so=o(" (ViLT model)"),uso=l(),qp=a("li"),PY=a("strong"),bso=o("vision-text-dual-encoder"),vso=o(" \u2014 "),NR=a("a"),Tso=o("VisionTextDualEncoderModel"),Fso=o(" (VisionTextDualEncoder model)"),Cso=l(),Gp=a("li"),$Y=a("strong"),Mso=o("visual_bert"),Eso=o(" \u2014 "),DR=a("a"),yso=o("VisualBertModel"),wso=o(" (VisualBert model)"),Aso=l(),Op=a("li"),IY=a("strong"),Lso=o("vit"),Bso=o(" \u2014 "),qR=a("a"),kso=o("ViTModel"),xso=o(" (ViT model)"),Rso=l(),Xp=a("li"),jY=a("strong"),Sso=o("vit_mae"),Pso=o(" \u2014 "),GR=a("a"),$so=o("ViTMAEModel"),Iso=o(" (ViTMAE model)"),jso=l(),zp=a("li"),NY=a("strong"),Nso=o("wav2vec2"),Dso=o(" \u2014 "),OR=a("a"),qso=o("Wav2Vec2Model"),Gso=o(" (Wav2Vec2 model)"),Oso=l(),Vp=a("li"),DY=a("strong"),Xso=o("wavlm"),zso=o(" \u2014 "),XR=a("a"),Vso=o("WavLMModel"),Wso=o(" (WavLM model)"),Qso=l(),Wp=a("li"),qY=a("strong"),Hso=o("xglm"),Uso=o(" \u2014 "),zR=a("a"),Jso=o("XGLMModel"),Yso=o(" (XGLM model)"),Kso=l(),Qp=a("li"),GY=a("strong"),Zso=o("xlm"),elo=o(" \u2014 "),VR=a("a"),olo=o("XLMModel"),rlo=o(" (XLM model)"),tlo=l(),Hp=a("li"),OY=a("strong"),alo=o("xlm-prophetnet"),nlo=o(" \u2014 "),WR=a("a"),slo=o("XLMProphetNetModel"),llo=o(" (XLMProphetNet model)"),ilo=l(),Up=a("li"),XY=a("strong"),dlo=o("xlm-roberta"),clo=o(" \u2014 "),QR=a("a"),flo=o("XLMRobertaModel"),mlo=o(" (XLM-RoBERTa model)"),glo=l(),Jp=a("li"),zY=a("strong"),hlo=o("xlm-roberta-xl"),plo=o(" \u2014 "),HR=a("a"),_lo=o("XLMRobertaXLModel"),ulo=o(" (XLM-RoBERTa-XL model)"),blo=l(),Yp=a("li"),VY=a("strong"),vlo=o("xlnet"),Tlo=o(" \u2014 "),UR=a("a"),Flo=o("XLNetModel"),Clo=o(" (XLNet model)"),Mlo=l(),Kp=a("li"),WY=a("strong"),Elo=o("yoso"),ylo=o(" \u2014 "),JR=a("a"),wlo=o("YosoModel"),Alo=o(" (YOSO model)"),Llo=l(),Zp=a("p"),Blo=o("The model is set in evaluation mode by default using "),QY=a("code"),klo=o("model.eval()"),xlo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),HY=a("code"),Rlo=o("model.train()"),Slo=l(),UY=a("p"),Plo=o("Examples:"),$lo=l(),f(VM.$$.fragment),P7e=l(),Xi=a("h2"),e_=a("a"),JY=a("span"),f(WM.$$.fragment),Ilo=l(),YY=a("span"),jlo=o("AutoModelForPreTraining"),$7e=l(),Wo=a("div"),f(QM.$$.fragment),Nlo=l(),zi=a("p"),Dlo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a pretraining head) when created
with the `),KY=a("code"),qlo=o("from_pretrained()"),Glo=o("class method or the "),ZY=a("code"),Olo=o("from_config()"),Xlo=o(`class
method.`),zlo=l(),HM=a("p"),Vlo=o("This class cannot be instantiated directly using "),eK=a("code"),Wlo=o("__init__()"),Qlo=o(" (throws an error)."),Hlo=l(),Dr=a("div"),f(UM.$$.fragment),Ulo=l(),oK=a("p"),Jlo=o("Instantiates one of the model classes of the library (with a pretraining head) from a configuration."),Ylo=l(),Vi=a("p"),Klo=o(`Note:
Loading a model from its configuration file does `),rK=a("strong"),Zlo=o("not"),eio=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),tK=a("code"),oio=o("from_pretrained()"),rio=o("to load the model weights."),tio=l(),aK=a("p"),aio=o("Examples:"),nio=l(),f(JM.$$.fragment),sio=l(),xe=a("div"),f(YM.$$.fragment),lio=l(),nK=a("p"),iio=o("Instantiate one of the model classes of the library (with a pretraining head) from a pretrained model."),dio=l(),qa=a("p"),cio=o("The model class to instantiate is selected based on the "),sK=a("code"),fio=o("model_type"),mio=o(` property of the config object (either
passed as an argument or loaded from `),lK=a("code"),gio=o("pretrained_model_name_or_path"),hio=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),iK=a("code"),pio=o("pretrained_model_name_or_path"),_io=o(":"),uio=l(),x=a("ul"),o_=a("li"),dK=a("strong"),bio=o("albert"),vio=o(" \u2014 "),YR=a("a"),Tio=o("AlbertForPreTraining"),Fio=o(" (ALBERT model)"),Cio=l(),r_=a("li"),cK=a("strong"),Mio=o("bart"),Eio=o(" \u2014 "),KR=a("a"),yio=o("BartForConditionalGeneration"),wio=o(" (BART model)"),Aio=l(),t_=a("li"),fK=a("strong"),Lio=o("bert"),Bio=o(" \u2014 "),ZR=a("a"),kio=o("BertForPreTraining"),xio=o(" (BERT model)"),Rio=l(),a_=a("li"),mK=a("strong"),Sio=o("big_bird"),Pio=o(" \u2014 "),eS=a("a"),$io=o("BigBirdForPreTraining"),Iio=o(" (BigBird model)"),jio=l(),n_=a("li"),gK=a("strong"),Nio=o("camembert"),Dio=o(" \u2014 "),oS=a("a"),qio=o("CamembertForMaskedLM"),Gio=o(" (CamemBERT model)"),Oio=l(),s_=a("li"),hK=a("strong"),Xio=o("ctrl"),zio=o(" \u2014 "),rS=a("a"),Vio=o("CTRLLMHeadModel"),Wio=o(" (CTRL model)"),Qio=l(),l_=a("li"),pK=a("strong"),Hio=o("deberta"),Uio=o(" \u2014 "),tS=a("a"),Jio=o("DebertaForMaskedLM"),Yio=o(" (DeBERTa model)"),Kio=l(),i_=a("li"),_K=a("strong"),Zio=o("deberta-v2"),edo=o(" \u2014 "),aS=a("a"),odo=o("DebertaV2ForMaskedLM"),rdo=o(" (DeBERTa-v2 model)"),tdo=l(),d_=a("li"),uK=a("strong"),ado=o("distilbert"),ndo=o(" \u2014 "),nS=a("a"),sdo=o("DistilBertForMaskedLM"),ldo=o(" (DistilBERT model)"),ido=l(),c_=a("li"),bK=a("strong"),ddo=o("electra"),cdo=o(" \u2014 "),sS=a("a"),fdo=o("ElectraForPreTraining"),mdo=o(" (ELECTRA model)"),gdo=l(),f_=a("li"),vK=a("strong"),hdo=o("flaubert"),pdo=o(" \u2014 "),lS=a("a"),_do=o("FlaubertWithLMHeadModel"),udo=o(" (FlauBERT model)"),bdo=l(),m_=a("li"),TK=a("strong"),vdo=o("fnet"),Tdo=o(" \u2014 "),iS=a("a"),Fdo=o("FNetForPreTraining"),Cdo=o(" (FNet model)"),Mdo=l(),g_=a("li"),FK=a("strong"),Edo=o("fsmt"),ydo=o(" \u2014 "),dS=a("a"),wdo=o("FSMTForConditionalGeneration"),Ado=o(" (FairSeq Machine-Translation model)"),Ldo=l(),h_=a("li"),CK=a("strong"),Bdo=o("funnel"),kdo=o(" \u2014 "),cS=a("a"),xdo=o("FunnelForPreTraining"),Rdo=o(" (Funnel Transformer model)"),Sdo=l(),p_=a("li"),MK=a("strong"),Pdo=o("gpt2"),$do=o(" \u2014 "),fS=a("a"),Ido=o("GPT2LMHeadModel"),jdo=o(" (OpenAI GPT-2 model)"),Ndo=l(),__=a("li"),EK=a("strong"),Ddo=o("ibert"),qdo=o(" \u2014 "),mS=a("a"),Gdo=o("IBertForMaskedLM"),Odo=o(" (I-BERT model)"),Xdo=l(),u_=a("li"),yK=a("strong"),zdo=o("layoutlm"),Vdo=o(" \u2014 "),gS=a("a"),Wdo=o("LayoutLMForMaskedLM"),Qdo=o(" (LayoutLM model)"),Hdo=l(),b_=a("li"),wK=a("strong"),Udo=o("longformer"),Jdo=o(" \u2014 "),hS=a("a"),Ydo=o("LongformerForMaskedLM"),Kdo=o(" (Longformer model)"),Zdo=l(),v_=a("li"),AK=a("strong"),eco=o("lxmert"),oco=o(" \u2014 "),pS=a("a"),rco=o("LxmertForPreTraining"),tco=o(" (LXMERT model)"),aco=l(),T_=a("li"),LK=a("strong"),nco=o("megatron-bert"),sco=o(" \u2014 "),_S=a("a"),lco=o("MegatronBertForPreTraining"),ico=o(" (MegatronBert model)"),dco=l(),F_=a("li"),BK=a("strong"),cco=o("mobilebert"),fco=o(" \u2014 "),uS=a("a"),mco=o("MobileBertForPreTraining"),gco=o(" (MobileBERT model)"),hco=l(),C_=a("li"),kK=a("strong"),pco=o("mpnet"),_co=o(" \u2014 "),bS=a("a"),uco=o("MPNetForMaskedLM"),bco=o(" (MPNet model)"),vco=l(),M_=a("li"),xK=a("strong"),Tco=o("openai-gpt"),Fco=o(" \u2014 "),vS=a("a"),Cco=o("OpenAIGPTLMHeadModel"),Mco=o(" (OpenAI GPT model)"),Eco=l(),E_=a("li"),RK=a("strong"),yco=o("retribert"),wco=o(" \u2014 "),TS=a("a"),Aco=o("RetriBertModel"),Lco=o(" (RetriBERT model)"),Bco=l(),y_=a("li"),SK=a("strong"),kco=o("roberta"),xco=o(" \u2014 "),FS=a("a"),Rco=o("RobertaForMaskedLM"),Sco=o(" (RoBERTa model)"),Pco=l(),w_=a("li"),PK=a("strong"),$co=o("squeezebert"),Ico=o(" \u2014 "),CS=a("a"),jco=o("SqueezeBertForMaskedLM"),Nco=o(" (SqueezeBERT model)"),Dco=l(),A_=a("li"),$K=a("strong"),qco=o("t5"),Gco=o(" \u2014 "),MS=a("a"),Oco=o("T5ForConditionalGeneration"),Xco=o(" (T5 model)"),zco=l(),L_=a("li"),IK=a("strong"),Vco=o("tapas"),Wco=o(" \u2014 "),ES=a("a"),Qco=o("TapasForMaskedLM"),Hco=o(" (TAPAS model)"),Uco=l(),B_=a("li"),jK=a("strong"),Jco=o("transfo-xl"),Yco=o(" \u2014 "),yS=a("a"),Kco=o("TransfoXLLMHeadModel"),Zco=o(" (Transformer-XL model)"),efo=l(),k_=a("li"),NK=a("strong"),ofo=o("unispeech"),rfo=o(" \u2014 "),wS=a("a"),tfo=o("UniSpeechForPreTraining"),afo=o(" (UniSpeech model)"),nfo=l(),x_=a("li"),DK=a("strong"),sfo=o("unispeech-sat"),lfo=o(" \u2014 "),AS=a("a"),ifo=o("UniSpeechSatForPreTraining"),dfo=o(" (UniSpeechSat model)"),cfo=l(),R_=a("li"),qK=a("strong"),ffo=o("visual_bert"),mfo=o(" \u2014 "),LS=a("a"),gfo=o("VisualBertForPreTraining"),hfo=o(" (VisualBert model)"),pfo=l(),S_=a("li"),GK=a("strong"),_fo=o("vit_mae"),ufo=o(" \u2014 "),BS=a("a"),bfo=o("ViTMAEForPreTraining"),vfo=o(" (ViTMAE model)"),Tfo=l(),P_=a("li"),OK=a("strong"),Ffo=o("wav2vec2"),Cfo=o(" \u2014 "),kS=a("a"),Mfo=o("Wav2Vec2ForPreTraining"),Efo=o(" (Wav2Vec2 model)"),yfo=l(),$_=a("li"),XK=a("strong"),wfo=o("xlm"),Afo=o(" \u2014 "),xS=a("a"),Lfo=o("XLMWithLMHeadModel"),Bfo=o(" (XLM model)"),kfo=l(),I_=a("li"),zK=a("strong"),xfo=o("xlm-roberta"),Rfo=o(" \u2014 "),RS=a("a"),Sfo=o("XLMRobertaForMaskedLM"),Pfo=o(" (XLM-RoBERTa model)"),$fo=l(),j_=a("li"),VK=a("strong"),Ifo=o("xlm-roberta-xl"),jfo=o(" \u2014 "),SS=a("a"),Nfo=o("XLMRobertaXLForMaskedLM"),Dfo=o(" (XLM-RoBERTa-XL model)"),qfo=l(),N_=a("li"),WK=a("strong"),Gfo=o("xlnet"),Ofo=o(" \u2014 "),PS=a("a"),Xfo=o("XLNetLMHeadModel"),zfo=o(" (XLNet model)"),Vfo=l(),D_=a("p"),Wfo=o("The model is set in evaluation mode by default using "),QK=a("code"),Qfo=o("model.eval()"),Hfo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),HK=a("code"),Ufo=o("model.train()"),Jfo=l(),UK=a("p"),Yfo=o("Examples:"),Kfo=l(),f(KM.$$.fragment),I7e=l(),Wi=a("h2"),q_=a("a"),JK=a("span"),f(ZM.$$.fragment),Zfo=l(),YK=a("span"),emo=o("AutoModelForCausalLM"),j7e=l(),Qo=a("div"),f(eE.$$.fragment),omo=l(),Qi=a("p"),rmo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a causal language modeling head) when created
with the `),KK=a("code"),tmo=o("from_pretrained()"),amo=o("class method or the "),ZK=a("code"),nmo=o("from_config()"),smo=o(`class
method.`),lmo=l(),oE=a("p"),imo=o("This class cannot be instantiated directly using "),eZ=a("code"),dmo=o("__init__()"),cmo=o(" (throws an error)."),fmo=l(),qr=a("div"),f(rE.$$.fragment),mmo=l(),oZ=a("p"),gmo=o("Instantiates one of the model classes of the library (with a causal language modeling head) from a configuration."),hmo=l(),Hi=a("p"),pmo=o(`Note:
Loading a model from its configuration file does `),rZ=a("strong"),_mo=o("not"),umo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),tZ=a("code"),bmo=o("from_pretrained()"),vmo=o("to load the model weights."),Tmo=l(),aZ=a("p"),Fmo=o("Examples:"),Cmo=l(),f(tE.$$.fragment),Mmo=l(),Re=a("div"),f(aE.$$.fragment),Emo=l(),nZ=a("p"),ymo=o("Instantiate one of the model classes of the library (with a causal language modeling head) from a pretrained model."),wmo=l(),Ga=a("p"),Amo=o("The model class to instantiate is selected based on the "),sZ=a("code"),Lmo=o("model_type"),Bmo=o(` property of the config object (either
passed as an argument or loaded from `),lZ=a("code"),kmo=o("pretrained_model_name_or_path"),xmo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),iZ=a("code"),Rmo=o("pretrained_model_name_or_path"),Smo=o(":"),Pmo=l(),$=a("ul"),G_=a("li"),dZ=a("strong"),$mo=o("bart"),Imo=o(" \u2014 "),$S=a("a"),jmo=o("BartForCausalLM"),Nmo=o(" (BART model)"),Dmo=l(),O_=a("li"),cZ=a("strong"),qmo=o("bert"),Gmo=o(" \u2014 "),IS=a("a"),Omo=o("BertLMHeadModel"),Xmo=o(" (BERT model)"),zmo=l(),X_=a("li"),fZ=a("strong"),Vmo=o("bert-generation"),Wmo=o(" \u2014 "),jS=a("a"),Qmo=o("BertGenerationDecoder"),Hmo=o(" (Bert Generation model)"),Umo=l(),z_=a("li"),mZ=a("strong"),Jmo=o("big_bird"),Ymo=o(" \u2014 "),NS=a("a"),Kmo=o("BigBirdForCausalLM"),Zmo=o(" (BigBird model)"),ego=l(),V_=a("li"),gZ=a("strong"),ogo=o("bigbird_pegasus"),rgo=o(" \u2014 "),DS=a("a"),tgo=o("BigBirdPegasusForCausalLM"),ago=o(" (BigBirdPegasus model)"),ngo=l(),W_=a("li"),hZ=a("strong"),sgo=o("blenderbot"),lgo=o(" \u2014 "),qS=a("a"),igo=o("BlenderbotForCausalLM"),dgo=o(" (Blenderbot model)"),cgo=l(),Q_=a("li"),pZ=a("strong"),fgo=o("blenderbot-small"),mgo=o(" \u2014 "),GS=a("a"),ggo=o("BlenderbotSmallForCausalLM"),hgo=o(" (BlenderbotSmall model)"),pgo=l(),H_=a("li"),_Z=a("strong"),_go=o("camembert"),ugo=o(" \u2014 "),OS=a("a"),bgo=o("CamembertForCausalLM"),vgo=o(" (CamemBERT model)"),Tgo=l(),U_=a("li"),uZ=a("strong"),Fgo=o("ctrl"),Cgo=o(" \u2014 "),XS=a("a"),Mgo=o("CTRLLMHeadModel"),Ego=o(" (CTRL model)"),ygo=l(),J_=a("li"),bZ=a("strong"),wgo=o("electra"),Ago=o(" \u2014 "),zS=a("a"),Lgo=o("ElectraForCausalLM"),Bgo=o(" (ELECTRA model)"),kgo=l(),Y_=a("li"),vZ=a("strong"),xgo=o("gpt2"),Rgo=o(" \u2014 "),VS=a("a"),Sgo=o("GPT2LMHeadModel"),Pgo=o(" (OpenAI GPT-2 model)"),$go=l(),K_=a("li"),TZ=a("strong"),Igo=o("gpt_neo"),jgo=o(" \u2014 "),WS=a("a"),Ngo=o("GPTNeoForCausalLM"),Dgo=o(" (GPT Neo model)"),qgo=l(),Z_=a("li"),FZ=a("strong"),Ggo=o("gptj"),Ogo=o(" \u2014 "),QS=a("a"),Xgo=o("GPTJForCausalLM"),zgo=o(" (GPT-J model)"),Vgo=l(),eu=a("li"),CZ=a("strong"),Wgo=o("marian"),Qgo=o(" \u2014 "),HS=a("a"),Hgo=o("MarianForCausalLM"),Ugo=o(" (Marian model)"),Jgo=l(),ou=a("li"),MZ=a("strong"),Ygo=o("mbart"),Kgo=o(" \u2014 "),US=a("a"),Zgo=o("MBartForCausalLM"),eho=o(" (mBART model)"),oho=l(),ru=a("li"),EZ=a("strong"),rho=o("megatron-bert"),tho=o(" \u2014 "),JS=a("a"),aho=o("MegatronBertForCausalLM"),nho=o(" (MegatronBert model)"),sho=l(),tu=a("li"),yZ=a("strong"),lho=o("openai-gpt"),iho=o(" \u2014 "),YS=a("a"),dho=o("OpenAIGPTLMHeadModel"),cho=o(" (OpenAI GPT model)"),fho=l(),au=a("li"),wZ=a("strong"),mho=o("pegasus"),gho=o(" \u2014 "),KS=a("a"),hho=o("PegasusForCausalLM"),pho=o(" (Pegasus model)"),_ho=l(),nu=a("li"),AZ=a("strong"),uho=o("plbart"),bho=o(" \u2014 "),ZS=a("a"),vho=o("PLBartForCausalLM"),Tho=o(" (PLBart model)"),Fho=l(),su=a("li"),LZ=a("strong"),Cho=o("prophetnet"),Mho=o(" \u2014 "),eP=a("a"),Eho=o("ProphetNetForCausalLM"),yho=o(" (ProphetNet model)"),who=l(),lu=a("li"),BZ=a("strong"),Aho=o("qdqbert"),Lho=o(" \u2014 "),oP=a("a"),Bho=o("QDQBertLMHeadModel"),kho=o(" (QDQBert model)"),xho=l(),iu=a("li"),kZ=a("strong"),Rho=o("reformer"),Sho=o(" \u2014 "),rP=a("a"),Pho=o("ReformerModelWithLMHead"),$ho=o(" (Reformer model)"),Iho=l(),du=a("li"),xZ=a("strong"),jho=o("rembert"),Nho=o(" \u2014 "),tP=a("a"),Dho=o("RemBertForCausalLM"),qho=o(" (RemBERT model)"),Gho=l(),cu=a("li"),RZ=a("strong"),Oho=o("roberta"),Xho=o(" \u2014 "),aP=a("a"),zho=o("RobertaForCausalLM"),Vho=o(" (RoBERTa model)"),Who=l(),fu=a("li"),SZ=a("strong"),Qho=o("roformer"),Hho=o(" \u2014 "),nP=a("a"),Uho=o("RoFormerForCausalLM"),Jho=o(" (RoFormer model)"),Yho=l(),mu=a("li"),PZ=a("strong"),Kho=o("speech_to_text_2"),Zho=o(" \u2014 "),sP=a("a"),epo=o("Speech2Text2ForCausalLM"),opo=o(" (Speech2Text2 model)"),rpo=l(),gu=a("li"),$Z=a("strong"),tpo=o("transfo-xl"),apo=o(" \u2014 "),lP=a("a"),npo=o("TransfoXLLMHeadModel"),spo=o(" (Transformer-XL model)"),lpo=l(),hu=a("li"),IZ=a("strong"),ipo=o("trocr"),dpo=o(" \u2014 "),iP=a("a"),cpo=o("TrOCRForCausalLM"),fpo=o(" (TrOCR model)"),mpo=l(),pu=a("li"),jZ=a("strong"),gpo=o("xglm"),hpo=o(" \u2014 "),dP=a("a"),ppo=o("XGLMForCausalLM"),_po=o(" (XGLM model)"),upo=l(),_u=a("li"),NZ=a("strong"),bpo=o("xlm"),vpo=o(" \u2014 "),cP=a("a"),Tpo=o("XLMWithLMHeadModel"),Fpo=o(" (XLM model)"),Cpo=l(),uu=a("li"),DZ=a("strong"),Mpo=o("xlm-prophetnet"),Epo=o(" \u2014 "),fP=a("a"),ypo=o("XLMProphetNetForCausalLM"),wpo=o(" (XLMProphetNet model)"),Apo=l(),bu=a("li"),qZ=a("strong"),Lpo=o("xlm-roberta"),Bpo=o(" \u2014 "),mP=a("a"),kpo=o("XLMRobertaForCausalLM"),xpo=o(" (XLM-RoBERTa model)"),Rpo=l(),vu=a("li"),GZ=a("strong"),Spo=o("xlm-roberta-xl"),Ppo=o(" \u2014 "),gP=a("a"),$po=o("XLMRobertaXLForCausalLM"),Ipo=o(" (XLM-RoBERTa-XL model)"),jpo=l(),Tu=a("li"),OZ=a("strong"),Npo=o("xlnet"),Dpo=o(" \u2014 "),hP=a("a"),qpo=o("XLNetLMHeadModel"),Gpo=o(" (XLNet model)"),Opo=l(),Fu=a("p"),Xpo=o("The model is set in evaluation mode by default using "),XZ=a("code"),zpo=o("model.eval()"),Vpo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),zZ=a("code"),Wpo=o("model.train()"),Qpo=l(),VZ=a("p"),Hpo=o("Examples:"),Upo=l(),f(nE.$$.fragment),N7e=l(),Ui=a("h2"),Cu=a("a"),WZ=a("span"),f(sE.$$.fragment),Jpo=l(),QZ=a("span"),Ypo=o("AutoModelForMaskedLM"),D7e=l(),Ho=a("div"),f(lE.$$.fragment),Kpo=l(),Ji=a("p"),Zpo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a masked language modeling head) when created
with the `),HZ=a("code"),e_o=o("from_pretrained()"),o_o=o("class method or the "),UZ=a("code"),r_o=o("from_config()"),t_o=o(`class
method.`),a_o=l(),iE=a("p"),n_o=o("This class cannot be instantiated directly using "),JZ=a("code"),s_o=o("__init__()"),l_o=o(" (throws an error)."),i_o=l(),Gr=a("div"),f(dE.$$.fragment),d_o=l(),YZ=a("p"),c_o=o("Instantiates one of the model classes of the library (with a masked language modeling head) from a configuration."),f_o=l(),Yi=a("p"),m_o=o(`Note:
Loading a model from its configuration file does `),KZ=a("strong"),g_o=o("not"),h_o=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),ZZ=a("code"),p_o=o("from_pretrained()"),__o=o("to load the model weights."),u_o=l(),eee=a("p"),b_o=o("Examples:"),v_o=l(),f(cE.$$.fragment),T_o=l(),Se=a("div"),f(fE.$$.fragment),F_o=l(),oee=a("p"),C_o=o("Instantiate one of the model classes of the library (with a masked language modeling head) from a pretrained model."),M_o=l(),Oa=a("p"),E_o=o("The model class to instantiate is selected based on the "),ree=a("code"),y_o=o("model_type"),w_o=o(` property of the config object (either
passed as an argument or loaded from `),tee=a("code"),A_o=o("pretrained_model_name_or_path"),L_o=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),aee=a("code"),B_o=o("pretrained_model_name_or_path"),k_o=o(":"),x_o=l(),I=a("ul"),Mu=a("li"),nee=a("strong"),R_o=o("albert"),S_o=o(" \u2014 "),pP=a("a"),P_o=o("AlbertForMaskedLM"),$_o=o(" (ALBERT model)"),I_o=l(),Eu=a("li"),see=a("strong"),j_o=o("bart"),N_o=o(" \u2014 "),_P=a("a"),D_o=o("BartForConditionalGeneration"),q_o=o(" (BART model)"),G_o=l(),yu=a("li"),lee=a("strong"),O_o=o("bert"),X_o=o(" \u2014 "),uP=a("a"),z_o=o("BertForMaskedLM"),V_o=o(" (BERT model)"),W_o=l(),wu=a("li"),iee=a("strong"),Q_o=o("big_bird"),H_o=o(" \u2014 "),bP=a("a"),U_o=o("BigBirdForMaskedLM"),J_o=o(" (BigBird model)"),Y_o=l(),Au=a("li"),dee=a("strong"),K_o=o("camembert"),Z_o=o(" \u2014 "),vP=a("a"),euo=o("CamembertForMaskedLM"),ouo=o(" (CamemBERT model)"),ruo=l(),Lu=a("li"),cee=a("strong"),tuo=o("convbert"),auo=o(" \u2014 "),TP=a("a"),nuo=o("ConvBertForMaskedLM"),suo=o(" (ConvBERT model)"),luo=l(),Bu=a("li"),fee=a("strong"),iuo=o("deberta"),duo=o(" \u2014 "),FP=a("a"),cuo=o("DebertaForMaskedLM"),fuo=o(" (DeBERTa model)"),muo=l(),ku=a("li"),mee=a("strong"),guo=o("deberta-v2"),huo=o(" \u2014 "),CP=a("a"),puo=o("DebertaV2ForMaskedLM"),_uo=o(" (DeBERTa-v2 model)"),uuo=l(),xu=a("li"),gee=a("strong"),buo=o("distilbert"),vuo=o(" \u2014 "),MP=a("a"),Tuo=o("DistilBertForMaskedLM"),Fuo=o(" (DistilBERT model)"),Cuo=l(),Ru=a("li"),hee=a("strong"),Muo=o("electra"),Euo=o(" \u2014 "),EP=a("a"),yuo=o("ElectraForMaskedLM"),wuo=o(" (ELECTRA model)"),Auo=l(),Su=a("li"),pee=a("strong"),Luo=o("flaubert"),Buo=o(" \u2014 "),yP=a("a"),kuo=o("FlaubertWithLMHeadModel"),xuo=o(" (FlauBERT model)"),Ruo=l(),Pu=a("li"),_ee=a("strong"),Suo=o("fnet"),Puo=o(" \u2014 "),wP=a("a"),$uo=o("FNetForMaskedLM"),Iuo=o(" (FNet model)"),juo=l(),$u=a("li"),uee=a("strong"),Nuo=o("funnel"),Duo=o(" \u2014 "),AP=a("a"),quo=o("FunnelForMaskedLM"),Guo=o(" (Funnel Transformer model)"),Ouo=l(),Iu=a("li"),bee=a("strong"),Xuo=o("ibert"),zuo=o(" \u2014 "),LP=a("a"),Vuo=o("IBertForMaskedLM"),Wuo=o(" (I-BERT model)"),Quo=l(),ju=a("li"),vee=a("strong"),Huo=o("layoutlm"),Uuo=o(" \u2014 "),BP=a("a"),Juo=o("LayoutLMForMaskedLM"),Yuo=o(" (LayoutLM model)"),Kuo=l(),Nu=a("li"),Tee=a("strong"),Zuo=o("longformer"),e2o=o(" \u2014 "),kP=a("a"),o2o=o("LongformerForMaskedLM"),r2o=o(" (Longformer model)"),t2o=l(),Du=a("li"),Fee=a("strong"),a2o=o("mbart"),n2o=o(" \u2014 "),xP=a("a"),s2o=o("MBartForConditionalGeneration"),l2o=o(" (mBART model)"),i2o=l(),qu=a("li"),Cee=a("strong"),d2o=o("megatron-bert"),c2o=o(" \u2014 "),RP=a("a"),f2o=o("MegatronBertForMaskedLM"),m2o=o(" (MegatronBert model)"),g2o=l(),Gu=a("li"),Mee=a("strong"),h2o=o("mobilebert"),p2o=o(" \u2014 "),SP=a("a"),_2o=o("MobileBertForMaskedLM"),u2o=o(" (MobileBERT model)"),b2o=l(),Ou=a("li"),Eee=a("strong"),v2o=o("mpnet"),T2o=o(" \u2014 "),PP=a("a"),F2o=o("MPNetForMaskedLM"),C2o=o(" (MPNet model)"),M2o=l(),Xu=a("li"),yee=a("strong"),E2o=o("nystromformer"),y2o=o(" \u2014 "),$P=a("a"),w2o=o("NystromformerForMaskedLM"),A2o=o(" (Nystromformer model)"),L2o=l(),zu=a("li"),wee=a("strong"),B2o=o("perceiver"),k2o=o(" \u2014 "),IP=a("a"),x2o=o("PerceiverForMaskedLM"),R2o=o(" (Perceiver model)"),S2o=l(),Vu=a("li"),Aee=a("strong"),P2o=o("qdqbert"),$2o=o(" \u2014 "),jP=a("a"),I2o=o("QDQBertForMaskedLM"),j2o=o(" (QDQBert model)"),N2o=l(),Wu=a("li"),Lee=a("strong"),D2o=o("reformer"),q2o=o(" \u2014 "),NP=a("a"),G2o=o("ReformerForMaskedLM"),O2o=o(" (Reformer model)"),X2o=l(),Qu=a("li"),Bee=a("strong"),z2o=o("rembert"),V2o=o(" \u2014 "),DP=a("a"),W2o=o("RemBertForMaskedLM"),Q2o=o(" (RemBERT model)"),H2o=l(),Hu=a("li"),kee=a("strong"),U2o=o("roberta"),J2o=o(" \u2014 "),qP=a("a"),Y2o=o("RobertaForMaskedLM"),K2o=o(" (RoBERTa model)"),Z2o=l(),Uu=a("li"),xee=a("strong"),e1o=o("roformer"),o1o=o(" \u2014 "),GP=a("a"),r1o=o("RoFormerForMaskedLM"),t1o=o(" (RoFormer model)"),a1o=l(),Ju=a("li"),Ree=a("strong"),n1o=o("squeezebert"),s1o=o(" \u2014 "),OP=a("a"),l1o=o("SqueezeBertForMaskedLM"),i1o=o(" (SqueezeBERT model)"),d1o=l(),Yu=a("li"),See=a("strong"),c1o=o("tapas"),f1o=o(" \u2014 "),XP=a("a"),m1o=o("TapasForMaskedLM"),g1o=o(" (TAPAS model)"),h1o=l(),Ku=a("li"),Pee=a("strong"),p1o=o("wav2vec2"),_1o=o(" \u2014 "),$ee=a("code"),u1o=o("Wav2Vec2ForMaskedLM"),b1o=o("(Wav2Vec2 model)"),v1o=l(),Zu=a("li"),Iee=a("strong"),T1o=o("xlm"),F1o=o(" \u2014 "),zP=a("a"),C1o=o("XLMWithLMHeadModel"),M1o=o(" (XLM model)"),E1o=l(),e2=a("li"),jee=a("strong"),y1o=o("xlm-roberta"),w1o=o(" \u2014 "),VP=a("a"),A1o=o("XLMRobertaForMaskedLM"),L1o=o(" (XLM-RoBERTa model)"),B1o=l(),o2=a("li"),Nee=a("strong"),k1o=o("xlm-roberta-xl"),x1o=o(" \u2014 "),WP=a("a"),R1o=o("XLMRobertaXLForMaskedLM"),S1o=o(" (XLM-RoBERTa-XL model)"),P1o=l(),r2=a("li"),Dee=a("strong"),$1o=o("yoso"),I1o=o(" \u2014 "),QP=a("a"),j1o=o("YosoForMaskedLM"),N1o=o(" (YOSO model)"),D1o=l(),t2=a("p"),q1o=o("The model is set in evaluation mode by default using "),qee=a("code"),G1o=o("model.eval()"),O1o=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Gee=a("code"),X1o=o("model.train()"),z1o=l(),Oee=a("p"),V1o=o("Examples:"),W1o=l(),f(mE.$$.fragment),q7e=l(),Ki=a("h2"),a2=a("a"),Xee=a("span"),f(gE.$$.fragment),Q1o=l(),zee=a("span"),H1o=o("AutoModelForSeq2SeqLM"),G7e=l(),Uo=a("div"),f(hE.$$.fragment),U1o=l(),Zi=a("p"),J1o=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence language modeling head) when created
with the `),Vee=a("code"),Y1o=o("from_pretrained()"),K1o=o("class method or the "),Wee=a("code"),Z1o=o("from_config()"),ebo=o(`class
method.`),obo=l(),pE=a("p"),rbo=o("This class cannot be instantiated directly using "),Qee=a("code"),tbo=o("__init__()"),abo=o(" (throws an error)."),nbo=l(),Or=a("div"),f(_E.$$.fragment),sbo=l(),Hee=a("p"),lbo=o("Instantiates one of the model classes of the library (with a sequence-to-sequence language modeling head) from a configuration."),ibo=l(),ed=a("p"),dbo=o(`Note:
Loading a model from its configuration file does `),Uee=a("strong"),cbo=o("not"),fbo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Jee=a("code"),mbo=o("from_pretrained()"),gbo=o("to load the model weights."),hbo=l(),Yee=a("p"),pbo=o("Examples:"),_bo=l(),f(uE.$$.fragment),ubo=l(),Pe=a("div"),f(bE.$$.fragment),bbo=l(),Kee=a("p"),vbo=o("Instantiate one of the model classes of the library (with a sequence-to-sequence language modeling head) from a pretrained model."),Tbo=l(),Xa=a("p"),Fbo=o("The model class to instantiate is selected based on the "),Zee=a("code"),Cbo=o("model_type"),Mbo=o(` property of the config object (either
passed as an argument or loaded from `),eoe=a("code"),Ebo=o("pretrained_model_name_or_path"),ybo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),ooe=a("code"),wbo=o("pretrained_model_name_or_path"),Abo=o(":"),Lbo=l(),ae=a("ul"),n2=a("li"),roe=a("strong"),Bbo=o("bart"),kbo=o(" \u2014 "),HP=a("a"),xbo=o("BartForConditionalGeneration"),Rbo=o(" (BART model)"),Sbo=l(),s2=a("li"),toe=a("strong"),Pbo=o("bigbird_pegasus"),$bo=o(" \u2014 "),UP=a("a"),Ibo=o("BigBirdPegasusForConditionalGeneration"),jbo=o(" (BigBirdPegasus model)"),Nbo=l(),l2=a("li"),aoe=a("strong"),Dbo=o("blenderbot"),qbo=o(" \u2014 "),JP=a("a"),Gbo=o("BlenderbotForConditionalGeneration"),Obo=o(" (Blenderbot model)"),Xbo=l(),i2=a("li"),noe=a("strong"),zbo=o("blenderbot-small"),Vbo=o(" \u2014 "),YP=a("a"),Wbo=o("BlenderbotSmallForConditionalGeneration"),Qbo=o(" (BlenderbotSmall model)"),Hbo=l(),d2=a("li"),soe=a("strong"),Ubo=o("encoder-decoder"),Jbo=o(" \u2014 "),KP=a("a"),Ybo=o("EncoderDecoderModel"),Kbo=o(" (Encoder decoder model)"),Zbo=l(),c2=a("li"),loe=a("strong"),e5o=o("fsmt"),o5o=o(" \u2014 "),ZP=a("a"),r5o=o("FSMTForConditionalGeneration"),t5o=o(" (FairSeq Machine-Translation model)"),a5o=l(),f2=a("li"),ioe=a("strong"),n5o=o("led"),s5o=o(" \u2014 "),e$=a("a"),l5o=o("LEDForConditionalGeneration"),i5o=o(" (LED model)"),d5o=l(),m2=a("li"),doe=a("strong"),c5o=o("m2m_100"),f5o=o(" \u2014 "),o$=a("a"),m5o=o("M2M100ForConditionalGeneration"),g5o=o(" (M2M100 model)"),h5o=l(),g2=a("li"),coe=a("strong"),p5o=o("marian"),_5o=o(" \u2014 "),r$=a("a"),u5o=o("MarianMTModel"),b5o=o(" (Marian model)"),v5o=l(),h2=a("li"),foe=a("strong"),T5o=o("mbart"),F5o=o(" \u2014 "),t$=a("a"),C5o=o("MBartForConditionalGeneration"),M5o=o(" (mBART model)"),E5o=l(),p2=a("li"),moe=a("strong"),y5o=o("mt5"),w5o=o(" \u2014 "),a$=a("a"),A5o=o("MT5ForConditionalGeneration"),L5o=o(" (mT5 model)"),B5o=l(),_2=a("li"),goe=a("strong"),k5o=o("pegasus"),x5o=o(" \u2014 "),n$=a("a"),R5o=o("PegasusForConditionalGeneration"),S5o=o(" (Pegasus model)"),P5o=l(),u2=a("li"),hoe=a("strong"),$5o=o("plbart"),I5o=o(" \u2014 "),s$=a("a"),j5o=o("PLBartForConditionalGeneration"),N5o=o(" (PLBart model)"),D5o=l(),b2=a("li"),poe=a("strong"),q5o=o("prophetnet"),G5o=o(" \u2014 "),l$=a("a"),O5o=o("ProphetNetForConditionalGeneration"),X5o=o(" (ProphetNet model)"),z5o=l(),v2=a("li"),_oe=a("strong"),V5o=o("t5"),W5o=o(" \u2014 "),i$=a("a"),Q5o=o("T5ForConditionalGeneration"),H5o=o(" (T5 model)"),U5o=l(),T2=a("li"),uoe=a("strong"),J5o=o("xlm-prophetnet"),Y5o=o(" \u2014 "),d$=a("a"),K5o=o("XLMProphetNetForConditionalGeneration"),Z5o=o(" (XLMProphetNet model)"),evo=l(),F2=a("p"),ovo=o("The model is set in evaluation mode by default using "),boe=a("code"),rvo=o("model.eval()"),tvo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),voe=a("code"),avo=o("model.train()"),nvo=l(),Toe=a("p"),svo=o("Examples:"),lvo=l(),f(vE.$$.fragment),O7e=l(),od=a("h2"),C2=a("a"),Foe=a("span"),f(TE.$$.fragment),ivo=l(),Coe=a("span"),dvo=o("AutoModelForSequenceClassification"),X7e=l(),Jo=a("div"),f(FE.$$.fragment),cvo=l(),rd=a("p"),fvo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence classification head) when created
with the `),Moe=a("code"),mvo=o("from_pretrained()"),gvo=o("class method or the "),Eoe=a("code"),hvo=o("from_config()"),pvo=o(`class
method.`),_vo=l(),CE=a("p"),uvo=o("This class cannot be instantiated directly using "),yoe=a("code"),bvo=o("__init__()"),vvo=o(" (throws an error)."),Tvo=l(),Xr=a("div"),f(ME.$$.fragment),Fvo=l(),woe=a("p"),Cvo=o("Instantiates one of the model classes of the library (with a sequence classification head) from a configuration."),Mvo=l(),td=a("p"),Evo=o(`Note:
Loading a model from its configuration file does `),Aoe=a("strong"),yvo=o("not"),wvo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Loe=a("code"),Avo=o("from_pretrained()"),Lvo=o("to load the model weights."),Bvo=l(),Boe=a("p"),kvo=o("Examples:"),xvo=l(),f(EE.$$.fragment),Rvo=l(),$e=a("div"),f(yE.$$.fragment),Svo=l(),koe=a("p"),Pvo=o("Instantiate one of the model classes of the library (with a sequence classification head) from a pretrained model."),$vo=l(),za=a("p"),Ivo=o("The model class to instantiate is selected based on the "),xoe=a("code"),jvo=o("model_type"),Nvo=o(` property of the config object (either
passed as an argument or loaded from `),Roe=a("code"),Dvo=o("pretrained_model_name_or_path"),qvo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Soe=a("code"),Gvo=o("pretrained_model_name_or_path"),Ovo=o(":"),Xvo=l(),A=a("ul"),M2=a("li"),Poe=a("strong"),zvo=o("albert"),Vvo=o(" \u2014 "),c$=a("a"),Wvo=o("AlbertForSequenceClassification"),Qvo=o(" (ALBERT model)"),Hvo=l(),E2=a("li"),$oe=a("strong"),Uvo=o("bart"),Jvo=o(" \u2014 "),f$=a("a"),Yvo=o("BartForSequenceClassification"),Kvo=o(" (BART model)"),Zvo=l(),y2=a("li"),Ioe=a("strong"),e6o=o("bert"),o6o=o(" \u2014 "),m$=a("a"),r6o=o("BertForSequenceClassification"),t6o=o(" (BERT model)"),a6o=l(),w2=a("li"),joe=a("strong"),n6o=o("big_bird"),s6o=o(" \u2014 "),g$=a("a"),l6o=o("BigBirdForSequenceClassification"),i6o=o(" (BigBird model)"),d6o=l(),A2=a("li"),Noe=a("strong"),c6o=o("bigbird_pegasus"),f6o=o(" \u2014 "),h$=a("a"),m6o=o("BigBirdPegasusForSequenceClassification"),g6o=o(" (BigBirdPegasus model)"),h6o=l(),L2=a("li"),Doe=a("strong"),p6o=o("camembert"),_6o=o(" \u2014 "),p$=a("a"),u6o=o("CamembertForSequenceClassification"),b6o=o(" (CamemBERT model)"),v6o=l(),B2=a("li"),qoe=a("strong"),T6o=o("canine"),F6o=o(" \u2014 "),_$=a("a"),C6o=o("CanineForSequenceClassification"),M6o=o(" (Canine model)"),E6o=l(),k2=a("li"),Goe=a("strong"),y6o=o("convbert"),w6o=o(" \u2014 "),u$=a("a"),A6o=o("ConvBertForSequenceClassification"),L6o=o(" (ConvBERT model)"),B6o=l(),x2=a("li"),Ooe=a("strong"),k6o=o("ctrl"),x6o=o(" \u2014 "),b$=a("a"),R6o=o("CTRLForSequenceClassification"),S6o=o(" (CTRL model)"),P6o=l(),R2=a("li"),Xoe=a("strong"),$6o=o("deberta"),I6o=o(" \u2014 "),v$=a("a"),j6o=o("DebertaForSequenceClassification"),N6o=o(" (DeBERTa model)"),D6o=l(),S2=a("li"),zoe=a("strong"),q6o=o("deberta-v2"),G6o=o(" \u2014 "),T$=a("a"),O6o=o("DebertaV2ForSequenceClassification"),X6o=o(" (DeBERTa-v2 model)"),z6o=l(),P2=a("li"),Voe=a("strong"),V6o=o("distilbert"),W6o=o(" \u2014 "),F$=a("a"),Q6o=o("DistilBertForSequenceClassification"),H6o=o(" (DistilBERT model)"),U6o=l(),$2=a("li"),Woe=a("strong"),J6o=o("electra"),Y6o=o(" \u2014 "),C$=a("a"),K6o=o("ElectraForSequenceClassification"),Z6o=o(" (ELECTRA model)"),eTo=l(),I2=a("li"),Qoe=a("strong"),oTo=o("flaubert"),rTo=o(" \u2014 "),M$=a("a"),tTo=o("FlaubertForSequenceClassification"),aTo=o(" (FlauBERT model)"),nTo=l(),j2=a("li"),Hoe=a("strong"),sTo=o("fnet"),lTo=o(" \u2014 "),E$=a("a"),iTo=o("FNetForSequenceClassification"),dTo=o(" (FNet model)"),cTo=l(),N2=a("li"),Uoe=a("strong"),fTo=o("funnel"),mTo=o(" \u2014 "),y$=a("a"),gTo=o("FunnelForSequenceClassification"),hTo=o(" (Funnel Transformer model)"),pTo=l(),D2=a("li"),Joe=a("strong"),_To=o("gpt2"),uTo=o(" \u2014 "),w$=a("a"),bTo=o("GPT2ForSequenceClassification"),vTo=o(" (OpenAI GPT-2 model)"),TTo=l(),q2=a("li"),Yoe=a("strong"),FTo=o("gpt_neo"),CTo=o(" \u2014 "),A$=a("a"),MTo=o("GPTNeoForSequenceClassification"),ETo=o(" (GPT Neo model)"),yTo=l(),G2=a("li"),Koe=a("strong"),wTo=o("gptj"),ATo=o(" \u2014 "),L$=a("a"),LTo=o("GPTJForSequenceClassification"),BTo=o(" (GPT-J model)"),kTo=l(),O2=a("li"),Zoe=a("strong"),xTo=o("ibert"),RTo=o(" \u2014 "),B$=a("a"),STo=o("IBertForSequenceClassification"),PTo=o(" (I-BERT model)"),$To=l(),X2=a("li"),ere=a("strong"),ITo=o("layoutlm"),jTo=o(" \u2014 "),k$=a("a"),NTo=o("LayoutLMForSequenceClassification"),DTo=o(" (LayoutLM model)"),qTo=l(),z2=a("li"),ore=a("strong"),GTo=o("layoutlmv2"),OTo=o(" \u2014 "),x$=a("a"),XTo=o("LayoutLMv2ForSequenceClassification"),zTo=o(" (LayoutLMv2 model)"),VTo=l(),V2=a("li"),rre=a("strong"),WTo=o("led"),QTo=o(" \u2014 "),R$=a("a"),HTo=o("LEDForSequenceClassification"),UTo=o(" (LED model)"),JTo=l(),W2=a("li"),tre=a("strong"),YTo=o("longformer"),KTo=o(" \u2014 "),S$=a("a"),ZTo=o("LongformerForSequenceClassification"),e8o=o(" (Longformer model)"),o8o=l(),Q2=a("li"),are=a("strong"),r8o=o("mbart"),t8o=o(" \u2014 "),P$=a("a"),a8o=o("MBartForSequenceClassification"),n8o=o(" (mBART model)"),s8o=l(),H2=a("li"),nre=a("strong"),l8o=o("megatron-bert"),i8o=o(" \u2014 "),$$=a("a"),d8o=o("MegatronBertForSequenceClassification"),c8o=o(" (MegatronBert model)"),f8o=l(),U2=a("li"),sre=a("strong"),m8o=o("mobilebert"),g8o=o(" \u2014 "),I$=a("a"),h8o=o("MobileBertForSequenceClassification"),p8o=o(" (MobileBERT model)"),_8o=l(),J2=a("li"),lre=a("strong"),u8o=o("mpnet"),b8o=o(" \u2014 "),j$=a("a"),v8o=o("MPNetForSequenceClassification"),T8o=o(" (MPNet model)"),F8o=l(),Y2=a("li"),ire=a("strong"),C8o=o("nystromformer"),M8o=o(" \u2014 "),N$=a("a"),E8o=o("NystromformerForSequenceClassification"),y8o=o(" (Nystromformer model)"),w8o=l(),K2=a("li"),dre=a("strong"),A8o=o("openai-gpt"),L8o=o(" \u2014 "),D$=a("a"),B8o=o("OpenAIGPTForSequenceClassification"),k8o=o(" (OpenAI GPT model)"),x8o=l(),Z2=a("li"),cre=a("strong"),R8o=o("perceiver"),S8o=o(" \u2014 "),q$=a("a"),P8o=o("PerceiverForSequenceClassification"),$8o=o(" (Perceiver model)"),I8o=l(),e1=a("li"),fre=a("strong"),j8o=o("plbart"),N8o=o(" \u2014 "),G$=a("a"),D8o=o("PLBartForSequenceClassification"),q8o=o(" (PLBart model)"),G8o=l(),o1=a("li"),mre=a("strong"),O8o=o("qdqbert"),X8o=o(" \u2014 "),O$=a("a"),z8o=o("QDQBertForSequenceClassification"),V8o=o(" (QDQBert model)"),W8o=l(),r1=a("li"),gre=a("strong"),Q8o=o("reformer"),H8o=o(" \u2014 "),X$=a("a"),U8o=o("ReformerForSequenceClassification"),J8o=o(" (Reformer model)"),Y8o=l(),t1=a("li"),hre=a("strong"),K8o=o("rembert"),Z8o=o(" \u2014 "),z$=a("a"),eFo=o("RemBertForSequenceClassification"),oFo=o(" (RemBERT model)"),rFo=l(),a1=a("li"),pre=a("strong"),tFo=o("roberta"),aFo=o(" \u2014 "),V$=a("a"),nFo=o("RobertaForSequenceClassification"),sFo=o(" (RoBERTa model)"),lFo=l(),n1=a("li"),_re=a("strong"),iFo=o("roformer"),dFo=o(" \u2014 "),W$=a("a"),cFo=o("RoFormerForSequenceClassification"),fFo=o(" (RoFormer model)"),mFo=l(),s1=a("li"),ure=a("strong"),gFo=o("squeezebert"),hFo=o(" \u2014 "),Q$=a("a"),pFo=o("SqueezeBertForSequenceClassification"),_Fo=o(" (SqueezeBERT model)"),uFo=l(),l1=a("li"),bre=a("strong"),bFo=o("tapas"),vFo=o(" \u2014 "),H$=a("a"),TFo=o("TapasForSequenceClassification"),FFo=o(" (TAPAS model)"),CFo=l(),i1=a("li"),vre=a("strong"),MFo=o("transfo-xl"),EFo=o(" \u2014 "),U$=a("a"),yFo=o("TransfoXLForSequenceClassification"),wFo=o(" (Transformer-XL model)"),AFo=l(),d1=a("li"),Tre=a("strong"),LFo=o("xlm"),BFo=o(" \u2014 "),J$=a("a"),kFo=o("XLMForSequenceClassification"),xFo=o(" (XLM model)"),RFo=l(),c1=a("li"),Fre=a("strong"),SFo=o("xlm-roberta"),PFo=o(" \u2014 "),Y$=a("a"),$Fo=o("XLMRobertaForSequenceClassification"),IFo=o(" (XLM-RoBERTa model)"),jFo=l(),f1=a("li"),Cre=a("strong"),NFo=o("xlm-roberta-xl"),DFo=o(" \u2014 "),K$=a("a"),qFo=o("XLMRobertaXLForSequenceClassification"),GFo=o(" (XLM-RoBERTa-XL model)"),OFo=l(),m1=a("li"),Mre=a("strong"),XFo=o("xlnet"),zFo=o(" \u2014 "),Z$=a("a"),VFo=o("XLNetForSequenceClassification"),WFo=o(" (XLNet model)"),QFo=l(),g1=a("li"),Ere=a("strong"),HFo=o("yoso"),UFo=o(" \u2014 "),eI=a("a"),JFo=o("YosoForSequenceClassification"),YFo=o(" (YOSO model)"),KFo=l(),h1=a("p"),ZFo=o("The model is set in evaluation mode by default using "),yre=a("code"),eCo=o("model.eval()"),oCo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),wre=a("code"),rCo=o("model.train()"),tCo=l(),Are=a("p"),aCo=o("Examples:"),nCo=l(),f(wE.$$.fragment),z7e=l(),ad=a("h2"),p1=a("a"),Lre=a("span"),f(AE.$$.fragment),sCo=l(),Bre=a("span"),lCo=o("AutoModelForMultipleChoice"),V7e=l(),Yo=a("div"),f(LE.$$.fragment),iCo=l(),nd=a("p"),dCo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a multiple choice head) when created
with the `),kre=a("code"),cCo=o("from_pretrained()"),fCo=o("class method or the "),xre=a("code"),mCo=o("from_config()"),gCo=o(`class
method.`),hCo=l(),BE=a("p"),pCo=o("This class cannot be instantiated directly using "),Rre=a("code"),_Co=o("__init__()"),uCo=o(" (throws an error)."),bCo=l(),zr=a("div"),f(kE.$$.fragment),vCo=l(),Sre=a("p"),TCo=o("Instantiates one of the model classes of the library (with a multiple choice head) from a configuration."),FCo=l(),sd=a("p"),CCo=o(`Note:
Loading a model from its configuration file does `),Pre=a("strong"),MCo=o("not"),ECo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),$re=a("code"),yCo=o("from_pretrained()"),wCo=o("to load the model weights."),ACo=l(),Ire=a("p"),LCo=o("Examples:"),BCo=l(),f(xE.$$.fragment),kCo=l(),Ie=a("div"),f(RE.$$.fragment),xCo=l(),jre=a("p"),RCo=o("Instantiate one of the model classes of the library (with a multiple choice head) from a pretrained model."),SCo=l(),Va=a("p"),PCo=o("The model class to instantiate is selected based on the "),Nre=a("code"),$Co=o("model_type"),ICo=o(` property of the config object (either
passed as an argument or loaded from `),Dre=a("code"),jCo=o("pretrained_model_name_or_path"),NCo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),qre=a("code"),DCo=o("pretrained_model_name_or_path"),qCo=o(":"),GCo=l(),G=a("ul"),_1=a("li"),Gre=a("strong"),OCo=o("albert"),XCo=o(" \u2014 "),oI=a("a"),zCo=o("AlbertForMultipleChoice"),VCo=o(" (ALBERT model)"),WCo=l(),u1=a("li"),Ore=a("strong"),QCo=o("bert"),HCo=o(" \u2014 "),rI=a("a"),UCo=o("BertForMultipleChoice"),JCo=o(" (BERT model)"),YCo=l(),b1=a("li"),Xre=a("strong"),KCo=o("big_bird"),ZCo=o(" \u2014 "),tI=a("a"),e4o=o("BigBirdForMultipleChoice"),o4o=o(" (BigBird model)"),r4o=l(),v1=a("li"),zre=a("strong"),t4o=o("camembert"),a4o=o(" \u2014 "),aI=a("a"),n4o=o("CamembertForMultipleChoice"),s4o=o(" (CamemBERT model)"),l4o=l(),T1=a("li"),Vre=a("strong"),i4o=o("canine"),d4o=o(" \u2014 "),nI=a("a"),c4o=o("CanineForMultipleChoice"),f4o=o(" (Canine model)"),m4o=l(),F1=a("li"),Wre=a("strong"),g4o=o("convbert"),h4o=o(" \u2014 "),sI=a("a"),p4o=o("ConvBertForMultipleChoice"),_4o=o(" (ConvBERT model)"),u4o=l(),C1=a("li"),Qre=a("strong"),b4o=o("distilbert"),v4o=o(" \u2014 "),lI=a("a"),T4o=o("DistilBertForMultipleChoice"),F4o=o(" (DistilBERT model)"),C4o=l(),M1=a("li"),Hre=a("strong"),M4o=o("electra"),E4o=o(" \u2014 "),iI=a("a"),y4o=o("ElectraForMultipleChoice"),w4o=o(" (ELECTRA model)"),A4o=l(),E1=a("li"),Ure=a("strong"),L4o=o("flaubert"),B4o=o(" \u2014 "),dI=a("a"),k4o=o("FlaubertForMultipleChoice"),x4o=o(" (FlauBERT model)"),R4o=l(),y1=a("li"),Jre=a("strong"),S4o=o("fnet"),P4o=o(" \u2014 "),cI=a("a"),$4o=o("FNetForMultipleChoice"),I4o=o(" (FNet model)"),j4o=l(),w1=a("li"),Yre=a("strong"),N4o=o("funnel"),D4o=o(" \u2014 "),fI=a("a"),q4o=o("FunnelForMultipleChoice"),G4o=o(" (Funnel Transformer model)"),O4o=l(),A1=a("li"),Kre=a("strong"),X4o=o("ibert"),z4o=o(" \u2014 "),mI=a("a"),V4o=o("IBertForMultipleChoice"),W4o=o(" (I-BERT model)"),Q4o=l(),L1=a("li"),Zre=a("strong"),H4o=o("longformer"),U4o=o(" \u2014 "),gI=a("a"),J4o=o("LongformerForMultipleChoice"),Y4o=o(" (Longformer model)"),K4o=l(),B1=a("li"),ete=a("strong"),Z4o=o("megatron-bert"),eMo=o(" \u2014 "),hI=a("a"),oMo=o("MegatronBertForMultipleChoice"),rMo=o(" (MegatronBert model)"),tMo=l(),k1=a("li"),ote=a("strong"),aMo=o("mobilebert"),nMo=o(" \u2014 "),pI=a("a"),sMo=o("MobileBertForMultipleChoice"),lMo=o(" (MobileBERT model)"),iMo=l(),x1=a("li"),rte=a("strong"),dMo=o("mpnet"),cMo=o(" \u2014 "),_I=a("a"),fMo=o("MPNetForMultipleChoice"),mMo=o(" (MPNet model)"),gMo=l(),R1=a("li"),tte=a("strong"),hMo=o("nystromformer"),pMo=o(" \u2014 "),uI=a("a"),_Mo=o("NystromformerForMultipleChoice"),uMo=o(" (Nystromformer model)"),bMo=l(),S1=a("li"),ate=a("strong"),vMo=o("qdqbert"),TMo=o(" \u2014 "),bI=a("a"),FMo=o("QDQBertForMultipleChoice"),CMo=o(" (QDQBert model)"),MMo=l(),P1=a("li"),nte=a("strong"),EMo=o("rembert"),yMo=o(" \u2014 "),vI=a("a"),wMo=o("RemBertForMultipleChoice"),AMo=o(" (RemBERT model)"),LMo=l(),$1=a("li"),ste=a("strong"),BMo=o("roberta"),kMo=o(" \u2014 "),TI=a("a"),xMo=o("RobertaForMultipleChoice"),RMo=o(" (RoBERTa model)"),SMo=l(),I1=a("li"),lte=a("strong"),PMo=o("roformer"),$Mo=o(" \u2014 "),FI=a("a"),IMo=o("RoFormerForMultipleChoice"),jMo=o(" (RoFormer model)"),NMo=l(),j1=a("li"),ite=a("strong"),DMo=o("squeezebert"),qMo=o(" \u2014 "),CI=a("a"),GMo=o("SqueezeBertForMultipleChoice"),OMo=o(" (SqueezeBERT model)"),XMo=l(),N1=a("li"),dte=a("strong"),zMo=o("xlm"),VMo=o(" \u2014 "),MI=a("a"),WMo=o("XLMForMultipleChoice"),QMo=o(" (XLM model)"),HMo=l(),D1=a("li"),cte=a("strong"),UMo=o("xlm-roberta"),JMo=o(" \u2014 "),EI=a("a"),YMo=o("XLMRobertaForMultipleChoice"),KMo=o(" (XLM-RoBERTa model)"),ZMo=l(),q1=a("li"),fte=a("strong"),eEo=o("xlm-roberta-xl"),oEo=o(" \u2014 "),yI=a("a"),rEo=o("XLMRobertaXLForMultipleChoice"),tEo=o(" (XLM-RoBERTa-XL model)"),aEo=l(),G1=a("li"),mte=a("strong"),nEo=o("xlnet"),sEo=o(" \u2014 "),wI=a("a"),lEo=o("XLNetForMultipleChoice"),iEo=o(" (XLNet model)"),dEo=l(),O1=a("li"),gte=a("strong"),cEo=o("yoso"),fEo=o(" \u2014 "),AI=a("a"),mEo=o("YosoForMultipleChoice"),gEo=o(" (YOSO model)"),hEo=l(),X1=a("p"),pEo=o("The model is set in evaluation mode by default using "),hte=a("code"),_Eo=o("model.eval()"),uEo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),pte=a("code"),bEo=o("model.train()"),vEo=l(),_te=a("p"),TEo=o("Examples:"),FEo=l(),f(SE.$$.fragment),W7e=l(),ld=a("h2"),z1=a("a"),ute=a("span"),f(PE.$$.fragment),CEo=l(),bte=a("span"),MEo=o("AutoModelForNextSentencePrediction"),Q7e=l(),Ko=a("div"),f($E.$$.fragment),EEo=l(),id=a("p"),yEo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a next sentence prediction head) when created
with the `),vte=a("code"),wEo=o("from_pretrained()"),AEo=o("class method or the "),Tte=a("code"),LEo=o("from_config()"),BEo=o(`class
method.`),kEo=l(),IE=a("p"),xEo=o("This class cannot be instantiated directly using "),Fte=a("code"),REo=o("__init__()"),SEo=o(" (throws an error)."),PEo=l(),Vr=a("div"),f(jE.$$.fragment),$Eo=l(),Cte=a("p"),IEo=o("Instantiates one of the model classes of the library (with a next sentence prediction head) from a configuration."),jEo=l(),dd=a("p"),NEo=o(`Note:
Loading a model from its configuration file does `),Mte=a("strong"),DEo=o("not"),qEo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Ete=a("code"),GEo=o("from_pretrained()"),OEo=o("to load the model weights."),XEo=l(),yte=a("p"),zEo=o("Examples:"),VEo=l(),f(NE.$$.fragment),WEo=l(),je=a("div"),f(DE.$$.fragment),QEo=l(),wte=a("p"),HEo=o("Instantiate one of the model classes of the library (with a next sentence prediction head) from a pretrained model."),UEo=l(),Wa=a("p"),JEo=o("The model class to instantiate is selected based on the "),Ate=a("code"),YEo=o("model_type"),KEo=o(` property of the config object (either
passed as an argument or loaded from `),Lte=a("code"),ZEo=o("pretrained_model_name_or_path"),e3o=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Bte=a("code"),o3o=o("pretrained_model_name_or_path"),r3o=o(":"),t3o=l(),na=a("ul"),V1=a("li"),kte=a("strong"),a3o=o("bert"),n3o=o(" \u2014 "),LI=a("a"),s3o=o("BertForNextSentencePrediction"),l3o=o(" (BERT model)"),i3o=l(),W1=a("li"),xte=a("strong"),d3o=o("fnet"),c3o=o(" \u2014 "),BI=a("a"),f3o=o("FNetForNextSentencePrediction"),m3o=o(" (FNet model)"),g3o=l(),Q1=a("li"),Rte=a("strong"),h3o=o("megatron-bert"),p3o=o(" \u2014 "),kI=a("a"),_3o=o("MegatronBertForNextSentencePrediction"),u3o=o(" (MegatronBert model)"),b3o=l(),H1=a("li"),Ste=a("strong"),v3o=o("mobilebert"),T3o=o(" \u2014 "),xI=a("a"),F3o=o("MobileBertForNextSentencePrediction"),C3o=o(" (MobileBERT model)"),M3o=l(),U1=a("li"),Pte=a("strong"),E3o=o("qdqbert"),y3o=o(" \u2014 "),RI=a("a"),w3o=o("QDQBertForNextSentencePrediction"),A3o=o(" (QDQBert model)"),L3o=l(),J1=a("p"),B3o=o("The model is set in evaluation mode by default using "),$te=a("code"),k3o=o("model.eval()"),x3o=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Ite=a("code"),R3o=o("model.train()"),S3o=l(),jte=a("p"),P3o=o("Examples:"),$3o=l(),f(qE.$$.fragment),H7e=l(),cd=a("h2"),Y1=a("a"),Nte=a("span"),f(GE.$$.fragment),I3o=l(),Dte=a("span"),j3o=o("AutoModelForTokenClassification"),U7e=l(),Zo=a("div"),f(OE.$$.fragment),N3o=l(),fd=a("p"),D3o=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a token classification head) when created
with the `),qte=a("code"),q3o=o("from_pretrained()"),G3o=o("class method or the "),Gte=a("code"),O3o=o("from_config()"),X3o=o(`class
method.`),z3o=l(),XE=a("p"),V3o=o("This class cannot be instantiated directly using "),Ote=a("code"),W3o=o("__init__()"),Q3o=o(" (throws an error)."),H3o=l(),Wr=a("div"),f(zE.$$.fragment),U3o=l(),Xte=a("p"),J3o=o("Instantiates one of the model classes of the library (with a token classification head) from a configuration."),Y3o=l(),md=a("p"),K3o=o(`Note:
Loading a model from its configuration file does `),zte=a("strong"),Z3o=o("not"),eyo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Vte=a("code"),oyo=o("from_pretrained()"),ryo=o("to load the model weights."),tyo=l(),Wte=a("p"),ayo=o("Examples:"),nyo=l(),f(VE.$$.fragment),syo=l(),Ne=a("div"),f(WE.$$.fragment),lyo=l(),Qte=a("p"),iyo=o("Instantiate one of the model classes of the library (with a token classification head) from a pretrained model."),dyo=l(),Qa=a("p"),cyo=o("The model class to instantiate is selected based on the "),Hte=a("code"),fyo=o("model_type"),myo=o(` property of the config object (either
passed as an argument or loaded from `),Ute=a("code"),gyo=o("pretrained_model_name_or_path"),hyo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Jte=a("code"),pyo=o("pretrained_model_name_or_path"),_yo=o(":"),uyo=l(),D=a("ul"),K1=a("li"),Yte=a("strong"),byo=o("albert"),vyo=o(" \u2014 "),SI=a("a"),Tyo=o("AlbertForTokenClassification"),Fyo=o(" (ALBERT model)"),Cyo=l(),Z1=a("li"),Kte=a("strong"),Myo=o("bert"),Eyo=o(" \u2014 "),PI=a("a"),yyo=o("BertForTokenClassification"),wyo=o(" (BERT model)"),Ayo=l(),eb=a("li"),Zte=a("strong"),Lyo=o("big_bird"),Byo=o(" \u2014 "),$I=a("a"),kyo=o("BigBirdForTokenClassification"),xyo=o(" (BigBird model)"),Ryo=l(),ob=a("li"),eae=a("strong"),Syo=o("camembert"),Pyo=o(" \u2014 "),II=a("a"),$yo=o("CamembertForTokenClassification"),Iyo=o(" (CamemBERT model)"),jyo=l(),rb=a("li"),oae=a("strong"),Nyo=o("canine"),Dyo=o(" \u2014 "),jI=a("a"),qyo=o("CanineForTokenClassification"),Gyo=o(" (Canine model)"),Oyo=l(),tb=a("li"),rae=a("strong"),Xyo=o("convbert"),zyo=o(" \u2014 "),NI=a("a"),Vyo=o("ConvBertForTokenClassification"),Wyo=o(" (ConvBERT model)"),Qyo=l(),ab=a("li"),tae=a("strong"),Hyo=o("deberta"),Uyo=o(" \u2014 "),DI=a("a"),Jyo=o("DebertaForTokenClassification"),Yyo=o(" (DeBERTa model)"),Kyo=l(),nb=a("li"),aae=a("strong"),Zyo=o("deberta-v2"),ewo=o(" \u2014 "),qI=a("a"),owo=o("DebertaV2ForTokenClassification"),rwo=o(" (DeBERTa-v2 model)"),two=l(),sb=a("li"),nae=a("strong"),awo=o("distilbert"),nwo=o(" \u2014 "),GI=a("a"),swo=o("DistilBertForTokenClassification"),lwo=o(" (DistilBERT model)"),iwo=l(),lb=a("li"),sae=a("strong"),dwo=o("electra"),cwo=o(" \u2014 "),OI=a("a"),fwo=o("ElectraForTokenClassification"),mwo=o(" (ELECTRA model)"),gwo=l(),ib=a("li"),lae=a("strong"),hwo=o("flaubert"),pwo=o(" \u2014 "),XI=a("a"),_wo=o("FlaubertForTokenClassification"),uwo=o(" (FlauBERT model)"),bwo=l(),db=a("li"),iae=a("strong"),vwo=o("fnet"),Two=o(" \u2014 "),zI=a("a"),Fwo=o("FNetForTokenClassification"),Cwo=o(" (FNet model)"),Mwo=l(),cb=a("li"),dae=a("strong"),Ewo=o("funnel"),ywo=o(" \u2014 "),VI=a("a"),wwo=o("FunnelForTokenClassification"),Awo=o(" (Funnel Transformer model)"),Lwo=l(),fb=a("li"),cae=a("strong"),Bwo=o("gpt2"),kwo=o(" \u2014 "),WI=a("a"),xwo=o("GPT2ForTokenClassification"),Rwo=o(" (OpenAI GPT-2 model)"),Swo=l(),mb=a("li"),fae=a("strong"),Pwo=o("ibert"),$wo=o(" \u2014 "),QI=a("a"),Iwo=o("IBertForTokenClassification"),jwo=o(" (I-BERT model)"),Nwo=l(),gb=a("li"),mae=a("strong"),Dwo=o("layoutlm"),qwo=o(" \u2014 "),HI=a("a"),Gwo=o("LayoutLMForTokenClassification"),Owo=o(" (LayoutLM model)"),Xwo=l(),hb=a("li"),gae=a("strong"),zwo=o("layoutlmv2"),Vwo=o(" \u2014 "),UI=a("a"),Wwo=o("LayoutLMv2ForTokenClassification"),Qwo=o(" (LayoutLMv2 model)"),Hwo=l(),pb=a("li"),hae=a("strong"),Uwo=o("longformer"),Jwo=o(" \u2014 "),JI=a("a"),Ywo=o("LongformerForTokenClassification"),Kwo=o(" (Longformer model)"),Zwo=l(),_b=a("li"),pae=a("strong"),eAo=o("megatron-bert"),oAo=o(" \u2014 "),YI=a("a"),rAo=o("MegatronBertForTokenClassification"),tAo=o(" (MegatronBert model)"),aAo=l(),ub=a("li"),_ae=a("strong"),nAo=o("mobilebert"),sAo=o(" \u2014 "),KI=a("a"),lAo=o("MobileBertForTokenClassification"),iAo=o(" (MobileBERT model)"),dAo=l(),bb=a("li"),uae=a("strong"),cAo=o("mpnet"),fAo=o(" \u2014 "),ZI=a("a"),mAo=o("MPNetForTokenClassification"),gAo=o(" (MPNet model)"),hAo=l(),vb=a("li"),bae=a("strong"),pAo=o("nystromformer"),_Ao=o(" \u2014 "),ej=a("a"),uAo=o("NystromformerForTokenClassification"),bAo=o(" (Nystromformer model)"),vAo=l(),Tb=a("li"),vae=a("strong"),TAo=o("qdqbert"),FAo=o(" \u2014 "),oj=a("a"),CAo=o("QDQBertForTokenClassification"),MAo=o(" (QDQBert model)"),EAo=l(),Fb=a("li"),Tae=a("strong"),yAo=o("rembert"),wAo=o(" \u2014 "),rj=a("a"),AAo=o("RemBertForTokenClassification"),LAo=o(" (RemBERT model)"),BAo=l(),Cb=a("li"),Fae=a("strong"),kAo=o("roberta"),xAo=o(" \u2014 "),tj=a("a"),RAo=o("RobertaForTokenClassification"),SAo=o(" (RoBERTa model)"),PAo=l(),Mb=a("li"),Cae=a("strong"),$Ao=o("roformer"),IAo=o(" \u2014 "),aj=a("a"),jAo=o("RoFormerForTokenClassification"),NAo=o(" (RoFormer model)"),DAo=l(),Eb=a("li"),Mae=a("strong"),qAo=o("squeezebert"),GAo=o(" \u2014 "),nj=a("a"),OAo=o("SqueezeBertForTokenClassification"),XAo=o(" (SqueezeBERT model)"),zAo=l(),yb=a("li"),Eae=a("strong"),VAo=o("xlm"),WAo=o(" \u2014 "),sj=a("a"),QAo=o("XLMForTokenClassification"),HAo=o(" (XLM model)"),UAo=l(),wb=a("li"),yae=a("strong"),JAo=o("xlm-roberta"),YAo=o(" \u2014 "),lj=a("a"),KAo=o("XLMRobertaForTokenClassification"),ZAo=o(" (XLM-RoBERTa model)"),e0o=l(),Ab=a("li"),wae=a("strong"),o0o=o("xlm-roberta-xl"),r0o=o(" \u2014 "),ij=a("a"),t0o=o("XLMRobertaXLForTokenClassification"),a0o=o(" (XLM-RoBERTa-XL model)"),n0o=l(),Lb=a("li"),Aae=a("strong"),s0o=o("xlnet"),l0o=o(" \u2014 "),dj=a("a"),i0o=o("XLNetForTokenClassification"),d0o=o(" (XLNet model)"),c0o=l(),Bb=a("li"),Lae=a("strong"),f0o=o("yoso"),m0o=o(" \u2014 "),cj=a("a"),g0o=o("YosoForTokenClassification"),h0o=o(" (YOSO model)"),p0o=l(),kb=a("p"),_0o=o("The model is set in evaluation mode by default using "),Bae=a("code"),u0o=o("model.eval()"),b0o=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),kae=a("code"),v0o=o("model.train()"),T0o=l(),xae=a("p"),F0o=o("Examples:"),C0o=l(),f(QE.$$.fragment),J7e=l(),gd=a("h2"),xb=a("a"),Rae=a("span"),f(HE.$$.fragment),M0o=l(),Sae=a("span"),E0o=o("AutoModelForQuestionAnswering"),Y7e=l(),er=a("div"),f(UE.$$.fragment),y0o=l(),hd=a("p"),w0o=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a question answering head) when created
with the `),Pae=a("code"),A0o=o("from_pretrained()"),L0o=o("class method or the "),$ae=a("code"),B0o=o("from_config()"),k0o=o(`class
method.`),x0o=l(),JE=a("p"),R0o=o("This class cannot be instantiated directly using "),Iae=a("code"),S0o=o("__init__()"),P0o=o(" (throws an error)."),$0o=l(),Qr=a("div"),f(YE.$$.fragment),I0o=l(),jae=a("p"),j0o=o("Instantiates one of the model classes of the library (with a question answering head) from a configuration."),N0o=l(),pd=a("p"),D0o=o(`Note:
Loading a model from its configuration file does `),Nae=a("strong"),q0o=o("not"),G0o=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Dae=a("code"),O0o=o("from_pretrained()"),X0o=o("to load the model weights."),z0o=l(),qae=a("p"),V0o=o("Examples:"),W0o=l(),f(KE.$$.fragment),Q0o=l(),De=a("div"),f(ZE.$$.fragment),H0o=l(),Gae=a("p"),U0o=o("Instantiate one of the model classes of the library (with a question answering head) from a pretrained model."),J0o=l(),Ha=a("p"),Y0o=o("The model class to instantiate is selected based on the "),Oae=a("code"),K0o=o("model_type"),Z0o=o(` property of the config object (either
passed as an argument or loaded from `),Xae=a("code"),eLo=o("pretrained_model_name_or_path"),oLo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),zae=a("code"),rLo=o("pretrained_model_name_or_path"),tLo=o(":"),aLo=l(),R=a("ul"),Rb=a("li"),Vae=a("strong"),nLo=o("albert"),sLo=o(" \u2014 "),fj=a("a"),lLo=o("AlbertForQuestionAnswering"),iLo=o(" (ALBERT model)"),dLo=l(),Sb=a("li"),Wae=a("strong"),cLo=o("bart"),fLo=o(" \u2014 "),mj=a("a"),mLo=o("BartForQuestionAnswering"),gLo=o(" (BART model)"),hLo=l(),Pb=a("li"),Qae=a("strong"),pLo=o("bert"),_Lo=o(" \u2014 "),gj=a("a"),uLo=o("BertForQuestionAnswering"),bLo=o(" (BERT model)"),vLo=l(),$b=a("li"),Hae=a("strong"),TLo=o("big_bird"),FLo=o(" \u2014 "),hj=a("a"),CLo=o("BigBirdForQuestionAnswering"),MLo=o(" (BigBird model)"),ELo=l(),Ib=a("li"),Uae=a("strong"),yLo=o("bigbird_pegasus"),wLo=o(" \u2014 "),pj=a("a"),ALo=o("BigBirdPegasusForQuestionAnswering"),LLo=o(" (BigBirdPegasus model)"),BLo=l(),jb=a("li"),Jae=a("strong"),kLo=o("camembert"),xLo=o(" \u2014 "),_j=a("a"),RLo=o("CamembertForQuestionAnswering"),SLo=o(" (CamemBERT model)"),PLo=l(),Nb=a("li"),Yae=a("strong"),$Lo=o("canine"),ILo=o(" \u2014 "),uj=a("a"),jLo=o("CanineForQuestionAnswering"),NLo=o(" (Canine model)"),DLo=l(),Db=a("li"),Kae=a("strong"),qLo=o("convbert"),GLo=o(" \u2014 "),bj=a("a"),OLo=o("ConvBertForQuestionAnswering"),XLo=o(" (ConvBERT model)"),zLo=l(),qb=a("li"),Zae=a("strong"),VLo=o("deberta"),WLo=o(" \u2014 "),vj=a("a"),QLo=o("DebertaForQuestionAnswering"),HLo=o(" (DeBERTa model)"),ULo=l(),Gb=a("li"),ene=a("strong"),JLo=o("deberta-v2"),YLo=o(" \u2014 "),Tj=a("a"),KLo=o("DebertaV2ForQuestionAnswering"),ZLo=o(" (DeBERTa-v2 model)"),e7o=l(),Ob=a("li"),one=a("strong"),o7o=o("distilbert"),r7o=o(" \u2014 "),Fj=a("a"),t7o=o("DistilBertForQuestionAnswering"),a7o=o(" (DistilBERT model)"),n7o=l(),Xb=a("li"),rne=a("strong"),s7o=o("electra"),l7o=o(" \u2014 "),Cj=a("a"),i7o=o("ElectraForQuestionAnswering"),d7o=o(" (ELECTRA model)"),c7o=l(),zb=a("li"),tne=a("strong"),f7o=o("flaubert"),m7o=o(" \u2014 "),Mj=a("a"),g7o=o("FlaubertForQuestionAnsweringSimple"),h7o=o(" (FlauBERT model)"),p7o=l(),Vb=a("li"),ane=a("strong"),_7o=o("fnet"),u7o=o(" \u2014 "),Ej=a("a"),b7o=o("FNetForQuestionAnswering"),v7o=o(" (FNet model)"),T7o=l(),Wb=a("li"),nne=a("strong"),F7o=o("funnel"),C7o=o(" \u2014 "),yj=a("a"),M7o=o("FunnelForQuestionAnswering"),E7o=o(" (Funnel Transformer model)"),y7o=l(),Qb=a("li"),sne=a("strong"),w7o=o("gptj"),A7o=o(" \u2014 "),wj=a("a"),L7o=o("GPTJForQuestionAnswering"),B7o=o(" (GPT-J model)"),k7o=l(),Hb=a("li"),lne=a("strong"),x7o=o("ibert"),R7o=o(" \u2014 "),Aj=a("a"),S7o=o("IBertForQuestionAnswering"),P7o=o(" (I-BERT model)"),$7o=l(),Ub=a("li"),ine=a("strong"),I7o=o("layoutlmv2"),j7o=o(" \u2014 "),Lj=a("a"),N7o=o("LayoutLMv2ForQuestionAnswering"),D7o=o(" (LayoutLMv2 model)"),q7o=l(),Jb=a("li"),dne=a("strong"),G7o=o("led"),O7o=o(" \u2014 "),Bj=a("a"),X7o=o("LEDForQuestionAnswering"),z7o=o(" (LED model)"),V7o=l(),Yb=a("li"),cne=a("strong"),W7o=o("longformer"),Q7o=o(" \u2014 "),kj=a("a"),H7o=o("LongformerForQuestionAnswering"),U7o=o(" (Longformer model)"),J7o=l(),Kb=a("li"),fne=a("strong"),Y7o=o("lxmert"),K7o=o(" \u2014 "),xj=a("a"),Z7o=o("LxmertForQuestionAnswering"),e9o=o(" (LXMERT model)"),o9o=l(),Zb=a("li"),mne=a("strong"),r9o=o("mbart"),t9o=o(" \u2014 "),Rj=a("a"),a9o=o("MBartForQuestionAnswering"),n9o=o(" (mBART model)"),s9o=l(),e5=a("li"),gne=a("strong"),l9o=o("megatron-bert"),i9o=o(" \u2014 "),Sj=a("a"),d9o=o("MegatronBertForQuestionAnswering"),c9o=o(" (MegatronBert model)"),f9o=l(),o5=a("li"),hne=a("strong"),m9o=o("mobilebert"),g9o=o(" \u2014 "),Pj=a("a"),h9o=o("MobileBertForQuestionAnswering"),p9o=o(" (MobileBERT model)"),_9o=l(),r5=a("li"),pne=a("strong"),u9o=o("mpnet"),b9o=o(" \u2014 "),$j=a("a"),v9o=o("MPNetForQuestionAnswering"),T9o=o(" (MPNet model)"),F9o=l(),t5=a("li"),_ne=a("strong"),C9o=o("nystromformer"),M9o=o(" \u2014 "),Ij=a("a"),E9o=o("NystromformerForQuestionAnswering"),y9o=o(" (Nystromformer model)"),w9o=l(),a5=a("li"),une=a("strong"),A9o=o("qdqbert"),L9o=o(" \u2014 "),jj=a("a"),B9o=o("QDQBertForQuestionAnswering"),k9o=o(" (QDQBert model)"),x9o=l(),n5=a("li"),bne=a("strong"),R9o=o("reformer"),S9o=o(" \u2014 "),Nj=a("a"),P9o=o("ReformerForQuestionAnswering"),$9o=o(" (Reformer model)"),I9o=l(),s5=a("li"),vne=a("strong"),j9o=o("rembert"),N9o=o(" \u2014 "),Dj=a("a"),D9o=o("RemBertForQuestionAnswering"),q9o=o(" (RemBERT model)"),G9o=l(),l5=a("li"),Tne=a("strong"),O9o=o("roberta"),X9o=o(" \u2014 "),qj=a("a"),z9o=o("RobertaForQuestionAnswering"),V9o=o(" (RoBERTa model)"),W9o=l(),i5=a("li"),Fne=a("strong"),Q9o=o("roformer"),H9o=o(" \u2014 "),Gj=a("a"),U9o=o("RoFormerForQuestionAnswering"),J9o=o(" (RoFormer model)"),Y9o=l(),d5=a("li"),Cne=a("strong"),K9o=o("splinter"),Z9o=o(" \u2014 "),Oj=a("a"),eBo=o("SplinterForQuestionAnswering"),oBo=o(" (Splinter model)"),rBo=l(),c5=a("li"),Mne=a("strong"),tBo=o("squeezebert"),aBo=o(" \u2014 "),Xj=a("a"),nBo=o("SqueezeBertForQuestionAnswering"),sBo=o(" (SqueezeBERT model)"),lBo=l(),f5=a("li"),Ene=a("strong"),iBo=o("xlm"),dBo=o(" \u2014 "),zj=a("a"),cBo=o("XLMForQuestionAnsweringSimple"),fBo=o(" (XLM model)"),mBo=l(),m5=a("li"),yne=a("strong"),gBo=o("xlm-roberta"),hBo=o(" \u2014 "),Vj=a("a"),pBo=o("XLMRobertaForQuestionAnswering"),_Bo=o(" (XLM-RoBERTa model)"),uBo=l(),g5=a("li"),wne=a("strong"),bBo=o("xlm-roberta-xl"),vBo=o(" \u2014 "),Wj=a("a"),TBo=o("XLMRobertaXLForQuestionAnswering"),FBo=o(" (XLM-RoBERTa-XL model)"),CBo=l(),h5=a("li"),Ane=a("strong"),MBo=o("xlnet"),EBo=o(" \u2014 "),Qj=a("a"),yBo=o("XLNetForQuestionAnsweringSimple"),wBo=o(" (XLNet model)"),ABo=l(),p5=a("li"),Lne=a("strong"),LBo=o("yoso"),BBo=o(" \u2014 "),Hj=a("a"),kBo=o("YosoForQuestionAnswering"),xBo=o(" (YOSO model)"),RBo=l(),_5=a("p"),SBo=o("The model is set in evaluation mode by default using "),Bne=a("code"),PBo=o("model.eval()"),$Bo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),kne=a("code"),IBo=o("model.train()"),jBo=l(),xne=a("p"),NBo=o("Examples:"),DBo=l(),f(e3.$$.fragment),K7e=l(),_d=a("h2"),u5=a("a"),Rne=a("span"),f(o3.$$.fragment),qBo=l(),Sne=a("span"),GBo=o("AutoModelForTableQuestionAnswering"),Z7e=l(),or=a("div"),f(r3.$$.fragment),OBo=l(),ud=a("p"),XBo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a table question answering head) when created
with the `),Pne=a("code"),zBo=o("from_pretrained()"),VBo=o("class method or the "),$ne=a("code"),WBo=o("from_config()"),QBo=o(`class
method.`),HBo=l(),t3=a("p"),UBo=o("This class cannot be instantiated directly using "),Ine=a("code"),JBo=o("__init__()"),YBo=o(" (throws an error)."),KBo=l(),Hr=a("div"),f(a3.$$.fragment),ZBo=l(),jne=a("p"),eko=o("Instantiates one of the model classes of the library (with a table question answering head) from a configuration."),oko=l(),bd=a("p"),rko=o(`Note:
Loading a model from its configuration file does `),Nne=a("strong"),tko=o("not"),ako=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Dne=a("code"),nko=o("from_pretrained()"),sko=o("to load the model weights."),lko=l(),qne=a("p"),iko=o("Examples:"),dko=l(),f(n3.$$.fragment),cko=l(),qe=a("div"),f(s3.$$.fragment),fko=l(),Gne=a("p"),mko=o("Instantiate one of the model classes of the library (with a table question answering head) from a pretrained model."),gko=l(),Ua=a("p"),hko=o("The model class to instantiate is selected based on the "),One=a("code"),pko=o("model_type"),_ko=o(` property of the config object (either
passed as an argument or loaded from `),Xne=a("code"),uko=o("pretrained_model_name_or_path"),bko=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),zne=a("code"),vko=o("pretrained_model_name_or_path"),Tko=o(":"),Fko=l(),Vne=a("ul"),b5=a("li"),Wne=a("strong"),Cko=o("tapas"),Mko=o(" \u2014 "),Uj=a("a"),Eko=o("TapasForQuestionAnswering"),yko=o(" (TAPAS model)"),wko=l(),v5=a("p"),Ako=o("The model is set in evaluation mode by default using "),Qne=a("code"),Lko=o("model.eval()"),Bko=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Hne=a("code"),kko=o("model.train()"),xko=l(),Une=a("p"),Rko=o("Examples:"),Sko=l(),f(l3.$$.fragment),e9e=l(),vd=a("h2"),T5=a("a"),Jne=a("span"),f(i3.$$.fragment),Pko=l(),Yne=a("span"),$ko=o("AutoModelForImageClassification"),o9e=l(),rr=a("div"),f(d3.$$.fragment),Iko=l(),Td=a("p"),jko=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a image classification head) when created
with the `),Kne=a("code"),Nko=o("from_pretrained()"),Dko=o("class method or the "),Zne=a("code"),qko=o("from_config()"),Gko=o(`class
method.`),Oko=l(),c3=a("p"),Xko=o("This class cannot be instantiated directly using "),ese=a("code"),zko=o("__init__()"),Vko=o(" (throws an error)."),Wko=l(),Ur=a("div"),f(f3.$$.fragment),Qko=l(),ose=a("p"),Hko=o("Instantiates one of the model classes of the library (with a image classification head) from a configuration."),Uko=l(),Fd=a("p"),Jko=o(`Note:
Loading a model from its configuration file does `),rse=a("strong"),Yko=o("not"),Kko=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),tse=a("code"),Zko=o("from_pretrained()"),exo=o("to load the model weights."),oxo=l(),ase=a("p"),rxo=o("Examples:"),txo=l(),f(m3.$$.fragment),axo=l(),Ge=a("div"),f(g3.$$.fragment),nxo=l(),nse=a("p"),sxo=o("Instantiate one of the model classes of the library (with a image classification head) from a pretrained model."),lxo=l(),Ja=a("p"),ixo=o("The model class to instantiate is selected based on the "),sse=a("code"),dxo=o("model_type"),cxo=o(` property of the config object (either
passed as an argument or loaded from `),lse=a("code"),fxo=o("pretrained_model_name_or_path"),mxo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),ise=a("code"),gxo=o("pretrained_model_name_or_path"),hxo=o(":"),pxo=l(),be=a("ul"),F5=a("li"),dse=a("strong"),_xo=o("beit"),uxo=o(" \u2014 "),Jj=a("a"),bxo=o("BeitForImageClassification"),vxo=o(" (BEiT model)"),Txo=l(),C5=a("li"),cse=a("strong"),Fxo=o("convnext"),Cxo=o(" \u2014 "),Yj=a("a"),Mxo=o("ConvNextForImageClassification"),Exo=o(" (ConvNext model)"),yxo=l(),Rs=a("li"),fse=a("strong"),wxo=o("deit"),Axo=o(" \u2014 "),Kj=a("a"),Lxo=o("DeiTForImageClassification"),Bxo=o(" or "),Zj=a("a"),kxo=o("DeiTForImageClassificationWithTeacher"),xxo=o(" (DeiT model)"),Rxo=l(),M5=a("li"),mse=a("strong"),Sxo=o("imagegpt"),Pxo=o(" \u2014 "),eN=a("a"),$xo=o("ImageGPTForImageClassification"),Ixo=o(" (ImageGPT model)"),jxo=l(),la=a("li"),gse=a("strong"),Nxo=o("perceiver"),Dxo=o(" \u2014 "),oN=a("a"),qxo=o("PerceiverForImageClassificationLearned"),Gxo=o(" or "),rN=a("a"),Oxo=o("PerceiverForImageClassificationFourier"),Xxo=o(" or "),tN=a("a"),zxo=o("PerceiverForImageClassificationConvProcessing"),Vxo=o(" (Perceiver model)"),Wxo=l(),E5=a("li"),hse=a("strong"),Qxo=o("poolformer"),Hxo=o(" \u2014 "),aN=a("a"),Uxo=o("PoolFormerForImageClassification"),Jxo=o(" (PoolFormer model)"),Yxo=l(),y5=a("li"),pse=a("strong"),Kxo=o("segformer"),Zxo=o(" \u2014 "),nN=a("a"),eRo=o("SegformerForImageClassification"),oRo=o(" (SegFormer model)"),rRo=l(),w5=a("li"),_se=a("strong"),tRo=o("swin"),aRo=o(" \u2014 "),sN=a("a"),nRo=o("SwinForImageClassification"),sRo=o(" (Swin model)"),lRo=l(),A5=a("li"),use=a("strong"),iRo=o("vit"),dRo=o(" \u2014 "),lN=a("a"),cRo=o("ViTForImageClassification"),fRo=o(" (ViT model)"),mRo=l(),L5=a("p"),gRo=o("The model is set in evaluation mode by default using "),bse=a("code"),hRo=o("model.eval()"),pRo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),vse=a("code"),_Ro=o("model.train()"),uRo=l(),Tse=a("p"),bRo=o("Examples:"),vRo=l(),f(h3.$$.fragment),r9e=l(),Cd=a("h2"),B5=a("a"),Fse=a("span"),f(p3.$$.fragment),TRo=l(),Cse=a("span"),FRo=o("AutoModelForVision2Seq"),t9e=l(),tr=a("div"),f(_3.$$.fragment),CRo=l(),Md=a("p"),MRo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a vision-to-text modeling head) when created
with the `),Mse=a("code"),ERo=o("from_pretrained()"),yRo=o("class method or the "),Ese=a("code"),wRo=o("from_config()"),ARo=o(`class
method.`),LRo=l(),u3=a("p"),BRo=o("This class cannot be instantiated directly using "),yse=a("code"),kRo=o("__init__()"),xRo=o(" (throws an error)."),RRo=l(),Jr=a("div"),f(b3.$$.fragment),SRo=l(),wse=a("p"),PRo=o("Instantiates one of the model classes of the library (with a vision-to-text modeling head) from a configuration."),$Ro=l(),Ed=a("p"),IRo=o(`Note:
Loading a model from its configuration file does `),Ase=a("strong"),jRo=o("not"),NRo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Lse=a("code"),DRo=o("from_pretrained()"),qRo=o("to load the model weights."),GRo=l(),Bse=a("p"),ORo=o("Examples:"),XRo=l(),f(v3.$$.fragment),zRo=l(),Oe=a("div"),f(T3.$$.fragment),VRo=l(),kse=a("p"),WRo=o("Instantiate one of the model classes of the library (with a vision-to-text modeling head) from a pretrained model."),QRo=l(),Ya=a("p"),HRo=o("The model class to instantiate is selected based on the "),xse=a("code"),URo=o("model_type"),JRo=o(` property of the config object (either
passed as an argument or loaded from `),Rse=a("code"),YRo=o("pretrained_model_name_or_path"),KRo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Sse=a("code"),ZRo=o("pretrained_model_name_or_path"),eSo=o(":"),oSo=l(),Pse=a("ul"),k5=a("li"),$se=a("strong"),rSo=o("vision-encoder-decoder"),tSo=o(" \u2014 "),iN=a("a"),aSo=o("VisionEncoderDecoderModel"),nSo=o(" (Vision Encoder decoder model)"),sSo=l(),x5=a("p"),lSo=o("The model is set in evaluation mode by default using "),Ise=a("code"),iSo=o("model.eval()"),dSo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),jse=a("code"),cSo=o("model.train()"),fSo=l(),Nse=a("p"),mSo=o("Examples:"),gSo=l(),f(F3.$$.fragment),a9e=l(),yd=a("h2"),R5=a("a"),Dse=a("span"),f(C3.$$.fragment),hSo=l(),qse=a("span"),pSo=o("AutoModelForAudioClassification"),n9e=l(),ar=a("div"),f(M3.$$.fragment),_So=l(),wd=a("p"),uSo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a audio classification head) when created
with the `),Gse=a("code"),bSo=o("from_pretrained()"),vSo=o("class method or the "),Ose=a("code"),TSo=o("from_config()"),FSo=o(`class
method.`),CSo=l(),E3=a("p"),MSo=o("This class cannot be instantiated directly using "),Xse=a("code"),ESo=o("__init__()"),ySo=o(" (throws an error)."),wSo=l(),Yr=a("div"),f(y3.$$.fragment),ASo=l(),zse=a("p"),LSo=o("Instantiates one of the model classes of the library (with a audio classification head) from a configuration."),BSo=l(),Ad=a("p"),kSo=o(`Note:
Loading a model from its configuration file does `),Vse=a("strong"),xSo=o("not"),RSo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Wse=a("code"),SSo=o("from_pretrained()"),PSo=o("to load the model weights."),$So=l(),Qse=a("p"),ISo=o("Examples:"),jSo=l(),f(w3.$$.fragment),NSo=l(),Xe=a("div"),f(A3.$$.fragment),DSo=l(),Hse=a("p"),qSo=o("Instantiate one of the model classes of the library (with a audio classification head) from a pretrained model."),GSo=l(),Ka=a("p"),OSo=o("The model class to instantiate is selected based on the "),Use=a("code"),XSo=o("model_type"),zSo=o(` property of the config object (either
passed as an argument or loaded from `),Jse=a("code"),VSo=o("pretrained_model_name_or_path"),WSo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Yse=a("code"),QSo=o("pretrained_model_name_or_path"),HSo=o(":"),USo=l(),ao=a("ul"),S5=a("li"),Kse=a("strong"),JSo=o("hubert"),YSo=o(" \u2014 "),dN=a("a"),KSo=o("HubertForSequenceClassification"),ZSo=o(" (Hubert model)"),ePo=l(),P5=a("li"),Zse=a("strong"),oPo=o("sew"),rPo=o(" \u2014 "),cN=a("a"),tPo=o("SEWForSequenceClassification"),aPo=o(" (SEW model)"),nPo=l(),$5=a("li"),ele=a("strong"),sPo=o("sew-d"),lPo=o(" \u2014 "),fN=a("a"),iPo=o("SEWDForSequenceClassification"),dPo=o(" (SEW-D model)"),cPo=l(),I5=a("li"),ole=a("strong"),fPo=o("unispeech"),mPo=o(" \u2014 "),mN=a("a"),gPo=o("UniSpeechForSequenceClassification"),hPo=o(" (UniSpeech model)"),pPo=l(),j5=a("li"),rle=a("strong"),_Po=o("unispeech-sat"),uPo=o(" \u2014 "),gN=a("a"),bPo=o("UniSpeechSatForSequenceClassification"),vPo=o(" (UniSpeechSat model)"),TPo=l(),N5=a("li"),tle=a("strong"),FPo=o("wav2vec2"),CPo=o(" \u2014 "),hN=a("a"),MPo=o("Wav2Vec2ForSequenceClassification"),EPo=o(" (Wav2Vec2 model)"),yPo=l(),D5=a("li"),ale=a("strong"),wPo=o("wavlm"),APo=o(" \u2014 "),pN=a("a"),LPo=o("WavLMForSequenceClassification"),BPo=o(" (WavLM model)"),kPo=l(),q5=a("p"),xPo=o("The model is set in evaluation mode by default using "),nle=a("code"),RPo=o("model.eval()"),SPo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),sle=a("code"),PPo=o("model.train()"),$Po=l(),lle=a("p"),IPo=o("Examples:"),jPo=l(),f(L3.$$.fragment),s9e=l(),Ld=a("h2"),G5=a("a"),ile=a("span"),f(B3.$$.fragment),NPo=l(),dle=a("span"),DPo=o("AutoModelForAudioFrameClassification"),l9e=l(),nr=a("div"),f(k3.$$.fragment),qPo=l(),Bd=a("p"),GPo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a audio frame (token) classification head) when created
with the `),cle=a("code"),OPo=o("from_pretrained()"),XPo=o("class method or the "),fle=a("code"),zPo=o("from_config()"),VPo=o(`class
method.`),WPo=l(),x3=a("p"),QPo=o("This class cannot be instantiated directly using "),mle=a("code"),HPo=o("__init__()"),UPo=o(" (throws an error)."),JPo=l(),Kr=a("div"),f(R3.$$.fragment),YPo=l(),gle=a("p"),KPo=o("Instantiates one of the model classes of the library (with a audio frame (token) classification head) from a configuration."),ZPo=l(),kd=a("p"),e$o=o(`Note:
Loading a model from its configuration file does `),hle=a("strong"),o$o=o("not"),r$o=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),ple=a("code"),t$o=o("from_pretrained()"),a$o=o("to load the model weights."),n$o=l(),_le=a("p"),s$o=o("Examples:"),l$o=l(),f(S3.$$.fragment),i$o=l(),ze=a("div"),f(P3.$$.fragment),d$o=l(),ule=a("p"),c$o=o("Instantiate one of the model classes of the library (with a audio frame (token) classification head) from a pretrained model."),f$o=l(),Za=a("p"),m$o=o("The model class to instantiate is selected based on the "),ble=a("code"),g$o=o("model_type"),h$o=o(` property of the config object (either
passed as an argument or loaded from `),vle=a("code"),p$o=o("pretrained_model_name_or_path"),_$o=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Tle=a("code"),u$o=o("pretrained_model_name_or_path"),b$o=o(":"),v$o=l(),xd=a("ul"),O5=a("li"),Fle=a("strong"),T$o=o("unispeech-sat"),F$o=o(" \u2014 "),_N=a("a"),C$o=o("UniSpeechSatForAudioFrameClassification"),M$o=o(" (UniSpeechSat model)"),E$o=l(),X5=a("li"),Cle=a("strong"),y$o=o("wav2vec2"),w$o=o(" \u2014 "),uN=a("a"),A$o=o("Wav2Vec2ForAudioFrameClassification"),L$o=o(" (Wav2Vec2 model)"),B$o=l(),z5=a("li"),Mle=a("strong"),k$o=o("wavlm"),x$o=o(" \u2014 "),bN=a("a"),R$o=o("WavLMForAudioFrameClassification"),S$o=o(" (WavLM model)"),P$o=l(),V5=a("p"),$$o=o("The model is set in evaluation mode by default using "),Ele=a("code"),I$o=o("model.eval()"),j$o=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),yle=a("code"),N$o=o("model.train()"),D$o=l(),wle=a("p"),q$o=o("Examples:"),G$o=l(),f($3.$$.fragment),i9e=l(),Rd=a("h2"),W5=a("a"),Ale=a("span"),f(I3.$$.fragment),O$o=l(),Lle=a("span"),X$o=o("AutoModelForCTC"),d9e=l(),sr=a("div"),f(j3.$$.fragment),z$o=l(),Sd=a("p"),V$o=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a connectionist temporal classification head) when created
with the `),Ble=a("code"),W$o=o("from_pretrained()"),Q$o=o("class method or the "),kle=a("code"),H$o=o("from_config()"),U$o=o(`class
method.`),J$o=l(),N3=a("p"),Y$o=o("This class cannot be instantiated directly using "),xle=a("code"),K$o=o("__init__()"),Z$o=o(" (throws an error)."),eIo=l(),Zr=a("div"),f(D3.$$.fragment),oIo=l(),Rle=a("p"),rIo=o("Instantiates one of the model classes of the library (with a connectionist temporal classification head) from a configuration."),tIo=l(),Pd=a("p"),aIo=o(`Note:
Loading a model from its configuration file does `),Sle=a("strong"),nIo=o("not"),sIo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Ple=a("code"),lIo=o("from_pretrained()"),iIo=o("to load the model weights."),dIo=l(),$le=a("p"),cIo=o("Examples:"),fIo=l(),f(q3.$$.fragment),mIo=l(),Ve=a("div"),f(G3.$$.fragment),gIo=l(),Ile=a("p"),hIo=o("Instantiate one of the model classes of the library (with a connectionist temporal classification head) from a pretrained model."),pIo=l(),en=a("p"),_Io=o("The model class to instantiate is selected based on the "),jle=a("code"),uIo=o("model_type"),bIo=o(` property of the config object (either
passed as an argument or loaded from `),Nle=a("code"),vIo=o("pretrained_model_name_or_path"),TIo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Dle=a("code"),FIo=o("pretrained_model_name_or_path"),CIo=o(":"),MIo=l(),no=a("ul"),Q5=a("li"),qle=a("strong"),EIo=o("hubert"),yIo=o(" \u2014 "),vN=a("a"),wIo=o("HubertForCTC"),AIo=o(" (Hubert model)"),LIo=l(),H5=a("li"),Gle=a("strong"),BIo=o("sew"),kIo=o(" \u2014 "),TN=a("a"),xIo=o("SEWForCTC"),RIo=o(" (SEW model)"),SIo=l(),U5=a("li"),Ole=a("strong"),PIo=o("sew-d"),$Io=o(" \u2014 "),FN=a("a"),IIo=o("SEWDForCTC"),jIo=o(" (SEW-D model)"),NIo=l(),J5=a("li"),Xle=a("strong"),DIo=o("unispeech"),qIo=o(" \u2014 "),CN=a("a"),GIo=o("UniSpeechForCTC"),OIo=o(" (UniSpeech model)"),XIo=l(),Y5=a("li"),zle=a("strong"),zIo=o("unispeech-sat"),VIo=o(" \u2014 "),MN=a("a"),WIo=o("UniSpeechSatForCTC"),QIo=o(" (UniSpeechSat model)"),HIo=l(),K5=a("li"),Vle=a("strong"),UIo=o("wav2vec2"),JIo=o(" \u2014 "),EN=a("a"),YIo=o("Wav2Vec2ForCTC"),KIo=o(" (Wav2Vec2 model)"),ZIo=l(),Z5=a("li"),Wle=a("strong"),ejo=o("wavlm"),ojo=o(" \u2014 "),yN=a("a"),rjo=o("WavLMForCTC"),tjo=o(" (WavLM model)"),ajo=l(),ev=a("p"),njo=o("The model is set in evaluation mode by default using "),Qle=a("code"),sjo=o("model.eval()"),ljo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Hle=a("code"),ijo=o("model.train()"),djo=l(),Ule=a("p"),cjo=o("Examples:"),fjo=l(),f(O3.$$.fragment),c9e=l(),$d=a("h2"),ov=a("a"),Jle=a("span"),f(X3.$$.fragment),mjo=l(),Yle=a("span"),gjo=o("AutoModelForSpeechSeq2Seq"),f9e=l(),lr=a("div"),f(z3.$$.fragment),hjo=l(),Id=a("p"),pjo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) when created
with the `),Kle=a("code"),_jo=o("from_pretrained()"),ujo=o("class method or the "),Zle=a("code"),bjo=o("from_config()"),vjo=o(`class
method.`),Tjo=l(),V3=a("p"),Fjo=o("This class cannot be instantiated directly using "),eie=a("code"),Cjo=o("__init__()"),Mjo=o(" (throws an error)."),Ejo=l(),et=a("div"),f(W3.$$.fragment),yjo=l(),oie=a("p"),wjo=o("Instantiates one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) from a configuration."),Ajo=l(),jd=a("p"),Ljo=o(`Note:
Loading a model from its configuration file does `),rie=a("strong"),Bjo=o("not"),kjo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),tie=a("code"),xjo=o("from_pretrained()"),Rjo=o("to load the model weights."),Sjo=l(),aie=a("p"),Pjo=o("Examples:"),$jo=l(),f(Q3.$$.fragment),Ijo=l(),We=a("div"),f(H3.$$.fragment),jjo=l(),nie=a("p"),Njo=o("Instantiate one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) from a pretrained model."),Djo=l(),on=a("p"),qjo=o("The model class to instantiate is selected based on the "),sie=a("code"),Gjo=o("model_type"),Ojo=o(` property of the config object (either
passed as an argument or loaded from `),lie=a("code"),Xjo=o("pretrained_model_name_or_path"),zjo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),iie=a("code"),Vjo=o("pretrained_model_name_or_path"),Wjo=o(":"),Qjo=l(),U3=a("ul"),rv=a("li"),die=a("strong"),Hjo=o("speech-encoder-decoder"),Ujo=o(" \u2014 "),wN=a("a"),Jjo=o("SpeechEncoderDecoderModel"),Yjo=o(" (Speech Encoder decoder model)"),Kjo=l(),tv=a("li"),cie=a("strong"),Zjo=o("speech_to_text"),eNo=o(" \u2014 "),AN=a("a"),oNo=o("Speech2TextForConditionalGeneration"),rNo=o(" (Speech2Text model)"),tNo=l(),av=a("p"),aNo=o("The model is set in evaluation mode by default using "),fie=a("code"),nNo=o("model.eval()"),sNo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),mie=a("code"),lNo=o("model.train()"),iNo=l(),gie=a("p"),dNo=o("Examples:"),cNo=l(),f(J3.$$.fragment),m9e=l(),Nd=a("h2"),nv=a("a"),hie=a("span"),f(Y3.$$.fragment),fNo=l(),pie=a("span"),mNo=o("AutoModelForAudioXVector"),g9e=l(),ir=a("div"),f(K3.$$.fragment),gNo=l(),Dd=a("p"),hNo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a audio retrieval via x-vector head) when created
with the `),_ie=a("code"),pNo=o("from_pretrained()"),_No=o("class method or the "),uie=a("code"),uNo=o("from_config()"),bNo=o(`class
method.`),vNo=l(),Z3=a("p"),TNo=o("This class cannot be instantiated directly using "),bie=a("code"),FNo=o("__init__()"),CNo=o(" (throws an error)."),MNo=l(),ot=a("div"),f(ey.$$.fragment),ENo=l(),vie=a("p"),yNo=o("Instantiates one of the model classes of the library (with a audio retrieval via x-vector head) from a configuration."),wNo=l(),qd=a("p"),ANo=o(`Note:
Loading a model from its configuration file does `),Tie=a("strong"),LNo=o("not"),BNo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Fie=a("code"),kNo=o("from_pretrained()"),xNo=o("to load the model weights."),RNo=l(),Cie=a("p"),SNo=o("Examples:"),PNo=l(),f(oy.$$.fragment),$No=l(),Qe=a("div"),f(ry.$$.fragment),INo=l(),Mie=a("p"),jNo=o("Instantiate one of the model classes of the library (with a audio retrieval via x-vector head) from a pretrained model."),NNo=l(),rn=a("p"),DNo=o("The model class to instantiate is selected based on the "),Eie=a("code"),qNo=o("model_type"),GNo=o(` property of the config object (either
passed as an argument or loaded from `),yie=a("code"),ONo=o("pretrained_model_name_or_path"),XNo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),wie=a("code"),zNo=o("pretrained_model_name_or_path"),VNo=o(":"),WNo=l(),Gd=a("ul"),sv=a("li"),Aie=a("strong"),QNo=o("unispeech-sat"),HNo=o(" \u2014 "),LN=a("a"),UNo=o("UniSpeechSatForXVector"),JNo=o(" (UniSpeechSat model)"),YNo=l(),lv=a("li"),Lie=a("strong"),KNo=o("wav2vec2"),ZNo=o(" \u2014 "),BN=a("a"),eDo=o("Wav2Vec2ForXVector"),oDo=o(" (Wav2Vec2 model)"),rDo=l(),iv=a("li"),Bie=a("strong"),tDo=o("wavlm"),aDo=o(" \u2014 "),kN=a("a"),nDo=o("WavLMForXVector"),sDo=o(" (WavLM model)"),lDo=l(),dv=a("p"),iDo=o("The model is set in evaluation mode by default using "),kie=a("code"),dDo=o("model.eval()"),cDo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),xie=a("code"),fDo=o("model.train()"),mDo=l(),Rie=a("p"),gDo=o("Examples:"),hDo=l(),f(ty.$$.fragment),h9e=l(),Od=a("h2"),cv=a("a"),Sie=a("span"),f(ay.$$.fragment),pDo=l(),Pie=a("span"),_Do=o("AutoModelForMaskedImageModeling"),p9e=l(),dr=a("div"),f(ny.$$.fragment),uDo=l(),Xd=a("p"),bDo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a masked image modeling head) when created
with the `),$ie=a("code"),vDo=o("from_pretrained()"),TDo=o("class method or the "),Iie=a("code"),FDo=o("from_config()"),CDo=o(`class
method.`),MDo=l(),sy=a("p"),EDo=o("This class cannot be instantiated directly using "),jie=a("code"),yDo=o("__init__()"),wDo=o(" (throws an error)."),ADo=l(),rt=a("div"),f(ly.$$.fragment),LDo=l(),Nie=a("p"),BDo=o("Instantiates one of the model classes of the library (with a masked image modeling head) from a configuration."),kDo=l(),zd=a("p"),xDo=o(`Note:
Loading a model from its configuration file does `),Die=a("strong"),RDo=o("not"),SDo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),qie=a("code"),PDo=o("from_pretrained()"),$Do=o("to load the model weights."),IDo=l(),Gie=a("p"),jDo=o("Examples:"),NDo=l(),f(iy.$$.fragment),DDo=l(),He=a("div"),f(dy.$$.fragment),qDo=l(),Oie=a("p"),GDo=o("Instantiate one of the model classes of the library (with a masked image modeling head) from a pretrained model."),ODo=l(),tn=a("p"),XDo=o("The model class to instantiate is selected based on the "),Xie=a("code"),zDo=o("model_type"),VDo=o(` property of the config object (either
passed as an argument or loaded from `),zie=a("code"),WDo=o("pretrained_model_name_or_path"),QDo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Vie=a("code"),HDo=o("pretrained_model_name_or_path"),UDo=o(":"),JDo=l(),Vd=a("ul"),fv=a("li"),Wie=a("strong"),YDo=o("deit"),KDo=o(" \u2014 "),xN=a("a"),ZDo=o("DeiTForMaskedImageModeling"),eqo=o(" (DeiT model)"),oqo=l(),mv=a("li"),Qie=a("strong"),rqo=o("swin"),tqo=o(" \u2014 "),RN=a("a"),aqo=o("SwinForMaskedImageModeling"),nqo=o(" (Swin model)"),sqo=l(),gv=a("li"),Hie=a("strong"),lqo=o("vit"),iqo=o(" \u2014 "),SN=a("a"),dqo=o("ViTForMaskedImageModeling"),cqo=o(" (ViT model)"),fqo=l(),hv=a("p"),mqo=o("The model is set in evaluation mode by default using "),Uie=a("code"),gqo=o("model.eval()"),hqo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Jie=a("code"),pqo=o("model.train()"),_qo=l(),Yie=a("p"),uqo=o("Examples:"),bqo=l(),f(cy.$$.fragment),_9e=l(),Wd=a("h2"),pv=a("a"),Kie=a("span"),f(fy.$$.fragment),vqo=l(),Zie=a("span"),Tqo=o("AutoModelForObjectDetection"),u9e=l(),cr=a("div"),f(my.$$.fragment),Fqo=l(),Qd=a("p"),Cqo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a object detection head) when created
with the `),ede=a("code"),Mqo=o("from_pretrained()"),Eqo=o("class method or the "),ode=a("code"),yqo=o("from_config()"),wqo=o(`class
method.`),Aqo=l(),gy=a("p"),Lqo=o("This class cannot be instantiated directly using "),rde=a("code"),Bqo=o("__init__()"),kqo=o(" (throws an error)."),xqo=l(),tt=a("div"),f(hy.$$.fragment),Rqo=l(),tde=a("p"),Sqo=o("Instantiates one of the model classes of the library (with a object detection head) from a configuration."),Pqo=l(),Hd=a("p"),$qo=o(`Note:
Loading a model from its configuration file does `),ade=a("strong"),Iqo=o("not"),jqo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),nde=a("code"),Nqo=o("from_pretrained()"),Dqo=o("to load the model weights."),qqo=l(),sde=a("p"),Gqo=o("Examples:"),Oqo=l(),f(py.$$.fragment),Xqo=l(),Ue=a("div"),f(_y.$$.fragment),zqo=l(),lde=a("p"),Vqo=o("Instantiate one of the model classes of the library (with a object detection head) from a pretrained model."),Wqo=l(),an=a("p"),Qqo=o("The model class to instantiate is selected based on the "),ide=a("code"),Hqo=o("model_type"),Uqo=o(` property of the config object (either
passed as an argument or loaded from `),dde=a("code"),Jqo=o("pretrained_model_name_or_path"),Yqo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),cde=a("code"),Kqo=o("pretrained_model_name_or_path"),Zqo=o(":"),eGo=l(),fde=a("ul"),_v=a("li"),mde=a("strong"),oGo=o("detr"),rGo=o(" \u2014 "),PN=a("a"),tGo=o("DetrForObjectDetection"),aGo=o(" (DETR model)"),nGo=l(),uv=a("p"),sGo=o("The model is set in evaluation mode by default using "),gde=a("code"),lGo=o("model.eval()"),iGo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),hde=a("code"),dGo=o("model.train()"),cGo=l(),pde=a("p"),fGo=o("Examples:"),mGo=l(),f(uy.$$.fragment),b9e=l(),Ud=a("h2"),bv=a("a"),_de=a("span"),f(by.$$.fragment),gGo=l(),ude=a("span"),hGo=o("AutoModelForImageSegmentation"),v9e=l(),fr=a("div"),f(vy.$$.fragment),pGo=l(),Jd=a("p"),_Go=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a image segmentation head) when created
with the `),bde=a("code"),uGo=o("from_pretrained()"),bGo=o("class method or the "),vde=a("code"),vGo=o("from_config()"),TGo=o(`class
method.`),FGo=l(),Ty=a("p"),CGo=o("This class cannot be instantiated directly using "),Tde=a("code"),MGo=o("__init__()"),EGo=o(" (throws an error)."),yGo=l(),at=a("div"),f(Fy.$$.fragment),wGo=l(),Fde=a("p"),AGo=o("Instantiates one of the model classes of the library (with a image segmentation head) from a configuration."),LGo=l(),Yd=a("p"),BGo=o(`Note:
Loading a model from its configuration file does `),Cde=a("strong"),kGo=o("not"),xGo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Mde=a("code"),RGo=o("from_pretrained()"),SGo=o("to load the model weights."),PGo=l(),Ede=a("p"),$Go=o("Examples:"),IGo=l(),f(Cy.$$.fragment),jGo=l(),Je=a("div"),f(My.$$.fragment),NGo=l(),yde=a("p"),DGo=o("Instantiate one of the model classes of the library (with a image segmentation head) from a pretrained model."),qGo=l(),nn=a("p"),GGo=o("The model class to instantiate is selected based on the "),wde=a("code"),OGo=o("model_type"),XGo=o(` property of the config object (either
passed as an argument or loaded from `),Ade=a("code"),zGo=o("pretrained_model_name_or_path"),VGo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Lde=a("code"),WGo=o("pretrained_model_name_or_path"),QGo=o(":"),HGo=l(),Bde=a("ul"),vv=a("li"),kde=a("strong"),UGo=o("detr"),JGo=o(" \u2014 "),$N=a("a"),YGo=o("DetrForSegmentation"),KGo=o(" (DETR model)"),ZGo=l(),Tv=a("p"),eOo=o("The model is set in evaluation mode by default using "),xde=a("code"),oOo=o("model.eval()"),rOo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Rde=a("code"),tOo=o("model.train()"),aOo=l(),Sde=a("p"),nOo=o("Examples:"),sOo=l(),f(Ey.$$.fragment),T9e=l(),Kd=a("h2"),Fv=a("a"),Pde=a("span"),f(yy.$$.fragment),lOo=l(),$de=a("span"),iOo=o("AutoModelForSemanticSegmentation"),F9e=l(),mr=a("div"),f(wy.$$.fragment),dOo=l(),Zd=a("p"),cOo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a semantic segmentation head) when created
with the `),Ide=a("code"),fOo=o("from_pretrained()"),mOo=o("class method or the "),jde=a("code"),gOo=o("from_config()"),hOo=o(`class
method.`),pOo=l(),Ay=a("p"),_Oo=o("This class cannot be instantiated directly using "),Nde=a("code"),uOo=o("__init__()"),bOo=o(" (throws an error)."),vOo=l(),nt=a("div"),f(Ly.$$.fragment),TOo=l(),Dde=a("p"),FOo=o("Instantiates one of the model classes of the library (with a semantic segmentation head) from a configuration."),COo=l(),ec=a("p"),MOo=o(`Note:
Loading a model from its configuration file does `),qde=a("strong"),EOo=o("not"),yOo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Gde=a("code"),wOo=o("from_pretrained()"),AOo=o("to load the model weights."),LOo=l(),Ode=a("p"),BOo=o("Examples:"),kOo=l(),f(By.$$.fragment),xOo=l(),Ye=a("div"),f(ky.$$.fragment),ROo=l(),Xde=a("p"),SOo=o("Instantiate one of the model classes of the library (with a semantic segmentation head) from a pretrained model."),POo=l(),sn=a("p"),$Oo=o("The model class to instantiate is selected based on the "),zde=a("code"),IOo=o("model_type"),jOo=o(` property of the config object (either
passed as an argument or loaded from `),Vde=a("code"),NOo=o("pretrained_model_name_or_path"),DOo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Wde=a("code"),qOo=o("pretrained_model_name_or_path"),GOo=o(":"),OOo=l(),xy=a("ul"),Cv=a("li"),Qde=a("strong"),XOo=o("beit"),zOo=o(" \u2014 "),IN=a("a"),VOo=o("BeitForSemanticSegmentation"),WOo=o(" (BEiT model)"),QOo=l(),Mv=a("li"),Hde=a("strong"),HOo=o("segformer"),UOo=o(" \u2014 "),jN=a("a"),JOo=o("SegformerForSemanticSegmentation"),YOo=o(" (SegFormer model)"),KOo=l(),Ev=a("p"),ZOo=o("The model is set in evaluation mode by default using "),Ude=a("code"),eXo=o("model.eval()"),oXo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Jde=a("code"),rXo=o("model.train()"),tXo=l(),Yde=a("p"),aXo=o("Examples:"),nXo=l(),f(Ry.$$.fragment),C9e=l(),oc=a("h2"),yv=a("a"),Kde=a("span"),f(Sy.$$.fragment),sXo=l(),Zde=a("span"),lXo=o("TFAutoModel"),M9e=l(),gr=a("div"),f(Py.$$.fragment),iXo=l(),rc=a("p"),dXo=o(`This is a generic model class that will be instantiated as one of the base model classes of the library when created
with the `),ece=a("code"),cXo=o("from_pretrained()"),fXo=o("class method or the "),oce=a("code"),mXo=o("from_config()"),gXo=o(`class
method.`),hXo=l(),$y=a("p"),pXo=o("This class cannot be instantiated directly using "),rce=a("code"),_Xo=o("__init__()"),uXo=o(" (throws an error)."),bXo=l(),st=a("div"),f(Iy.$$.fragment),vXo=l(),tce=a("p"),TXo=o("Instantiates one of the base model classes of the library from a configuration."),FXo=l(),tc=a("p"),CXo=o(`Note:
Loading a model from its configuration file does `),ace=a("strong"),MXo=o("not"),EXo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),nce=a("code"),yXo=o("from_pretrained()"),wXo=o("to load the model weights."),AXo=l(),sce=a("p"),LXo=o("Examples:"),BXo=l(),f(jy.$$.fragment),kXo=l(),go=a("div"),f(Ny.$$.fragment),xXo=l(),lce=a("p"),RXo=o("Instantiate one of the base model classes of the library from a pretrained model."),SXo=l(),ln=a("p"),PXo=o("The model class to instantiate is selected based on the "),ice=a("code"),$Xo=o("model_type"),IXo=o(` property of the config object (either
passed as an argument or loaded from `),dce=a("code"),jXo=o("pretrained_model_name_or_path"),NXo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),cce=a("code"),DXo=o("pretrained_model_name_or_path"),qXo=o(":"),GXo=l(),B=a("ul"),wv=a("li"),fce=a("strong"),OXo=o("albert"),XXo=o(" \u2014 "),NN=a("a"),zXo=o("TFAlbertModel"),VXo=o(" (ALBERT model)"),WXo=l(),Av=a("li"),mce=a("strong"),QXo=o("bart"),HXo=o(" \u2014 "),DN=a("a"),UXo=o("TFBartModel"),JXo=o(" (BART model)"),YXo=l(),Lv=a("li"),gce=a("strong"),KXo=o("bert"),ZXo=o(" \u2014 "),qN=a("a"),ezo=o("TFBertModel"),ozo=o(" (BERT model)"),rzo=l(),Bv=a("li"),hce=a("strong"),tzo=o("blenderbot"),azo=o(" \u2014 "),GN=a("a"),nzo=o("TFBlenderbotModel"),szo=o(" (Blenderbot model)"),lzo=l(),kv=a("li"),pce=a("strong"),izo=o("blenderbot-small"),dzo=o(" \u2014 "),ON=a("a"),czo=o("TFBlenderbotSmallModel"),fzo=o(" (BlenderbotSmall model)"),mzo=l(),xv=a("li"),_ce=a("strong"),gzo=o("camembert"),hzo=o(" \u2014 "),XN=a("a"),pzo=o("TFCamembertModel"),_zo=o(" (CamemBERT model)"),uzo=l(),Rv=a("li"),uce=a("strong"),bzo=o("clip"),vzo=o(" \u2014 "),zN=a("a"),Tzo=o("TFCLIPModel"),Fzo=o(" (CLIP model)"),Czo=l(),Sv=a("li"),bce=a("strong"),Mzo=o("convbert"),Ezo=o(" \u2014 "),VN=a("a"),yzo=o("TFConvBertModel"),wzo=o(" (ConvBERT model)"),Azo=l(),Pv=a("li"),vce=a("strong"),Lzo=o("convnext"),Bzo=o(" \u2014 "),WN=a("a"),kzo=o("TFConvNextModel"),xzo=o(" (ConvNext model)"),Rzo=l(),$v=a("li"),Tce=a("strong"),Szo=o("ctrl"),Pzo=o(" \u2014 "),QN=a("a"),$zo=o("TFCTRLModel"),Izo=o(" (CTRL model)"),jzo=l(),Iv=a("li"),Fce=a("strong"),Nzo=o("deberta"),Dzo=o(" \u2014 "),HN=a("a"),qzo=o("TFDebertaModel"),Gzo=o(" (DeBERTa model)"),Ozo=l(),jv=a("li"),Cce=a("strong"),Xzo=o("deberta-v2"),zzo=o(" \u2014 "),UN=a("a"),Vzo=o("TFDebertaV2Model"),Wzo=o(" (DeBERTa-v2 model)"),Qzo=l(),Nv=a("li"),Mce=a("strong"),Hzo=o("distilbert"),Uzo=o(" \u2014 "),JN=a("a"),Jzo=o("TFDistilBertModel"),Yzo=o(" (DistilBERT model)"),Kzo=l(),Dv=a("li"),Ece=a("strong"),Zzo=o("dpr"),eVo=o(" \u2014 "),YN=a("a"),oVo=o("TFDPRQuestionEncoder"),rVo=o(" (DPR model)"),tVo=l(),qv=a("li"),yce=a("strong"),aVo=o("electra"),nVo=o(" \u2014 "),KN=a("a"),sVo=o("TFElectraModel"),lVo=o(" (ELECTRA model)"),iVo=l(),Gv=a("li"),wce=a("strong"),dVo=o("flaubert"),cVo=o(" \u2014 "),ZN=a("a"),fVo=o("TFFlaubertModel"),mVo=o(" (FlauBERT model)"),gVo=l(),Ss=a("li"),Ace=a("strong"),hVo=o("funnel"),pVo=o(" \u2014 "),eD=a("a"),_Vo=o("TFFunnelModel"),uVo=o(" or "),oD=a("a"),bVo=o("TFFunnelBaseModel"),vVo=o(" (Funnel Transformer model)"),TVo=l(),Ov=a("li"),Lce=a("strong"),FVo=o("gpt2"),CVo=o(" \u2014 "),rD=a("a"),MVo=o("TFGPT2Model"),EVo=o(" (OpenAI GPT-2 model)"),yVo=l(),Xv=a("li"),Bce=a("strong"),wVo=o("hubert"),AVo=o(" \u2014 "),tD=a("a"),LVo=o("TFHubertModel"),BVo=o(" (Hubert model)"),kVo=l(),zv=a("li"),kce=a("strong"),xVo=o("layoutlm"),RVo=o(" \u2014 "),aD=a("a"),SVo=o("TFLayoutLMModel"),PVo=o(" (LayoutLM model)"),$Vo=l(),Vv=a("li"),xce=a("strong"),IVo=o("led"),jVo=o(" \u2014 "),nD=a("a"),NVo=o("TFLEDModel"),DVo=o(" (LED model)"),qVo=l(),Wv=a("li"),Rce=a("strong"),GVo=o("longformer"),OVo=o(" \u2014 "),sD=a("a"),XVo=o("TFLongformerModel"),zVo=o(" (Longformer model)"),VVo=l(),Qv=a("li"),Sce=a("strong"),WVo=o("lxmert"),QVo=o(" \u2014 "),lD=a("a"),HVo=o("TFLxmertModel"),UVo=o(" (LXMERT model)"),JVo=l(),Hv=a("li"),Pce=a("strong"),YVo=o("marian"),KVo=o(" \u2014 "),iD=a("a"),ZVo=o("TFMarianModel"),eWo=o(" (Marian model)"),oWo=l(),Uv=a("li"),$ce=a("strong"),rWo=o("mbart"),tWo=o(" \u2014 "),dD=a("a"),aWo=o("TFMBartModel"),nWo=o(" (mBART model)"),sWo=l(),Jv=a("li"),Ice=a("strong"),lWo=o("mobilebert"),iWo=o(" \u2014 "),cD=a("a"),dWo=o("TFMobileBertModel"),cWo=o(" (MobileBERT model)"),fWo=l(),Yv=a("li"),jce=a("strong"),mWo=o("mpnet"),gWo=o(" \u2014 "),fD=a("a"),hWo=o("TFMPNetModel"),pWo=o(" (MPNet model)"),_Wo=l(),Kv=a("li"),Nce=a("strong"),uWo=o("mt5"),bWo=o(" \u2014 "),mD=a("a"),vWo=o("TFMT5Model"),TWo=o(" (mT5 model)"),FWo=l(),Zv=a("li"),Dce=a("strong"),CWo=o("openai-gpt"),MWo=o(" \u2014 "),gD=a("a"),EWo=o("TFOpenAIGPTModel"),yWo=o(" (OpenAI GPT model)"),wWo=l(),e6=a("li"),qce=a("strong"),AWo=o("pegasus"),LWo=o(" \u2014 "),hD=a("a"),BWo=o("TFPegasusModel"),kWo=o(" (Pegasus model)"),xWo=l(),o6=a("li"),Gce=a("strong"),RWo=o("rembert"),SWo=o(" \u2014 "),pD=a("a"),PWo=o("TFRemBertModel"),$Wo=o(" (RemBERT model)"),IWo=l(),r6=a("li"),Oce=a("strong"),jWo=o("roberta"),NWo=o(" \u2014 "),_D=a("a"),DWo=o("TFRobertaModel"),qWo=o(" (RoBERTa model)"),GWo=l(),t6=a("li"),Xce=a("strong"),OWo=o("roformer"),XWo=o(" \u2014 "),uD=a("a"),zWo=o("TFRoFormerModel"),VWo=o(" (RoFormer model)"),WWo=l(),a6=a("li"),zce=a("strong"),QWo=o("speech_to_text"),HWo=o(" \u2014 "),bD=a("a"),UWo=o("TFSpeech2TextModel"),JWo=o(" (Speech2Text model)"),YWo=l(),n6=a("li"),Vce=a("strong"),KWo=o("t5"),ZWo=o(" \u2014 "),vD=a("a"),eQo=o("TFT5Model"),oQo=o(" (T5 model)"),rQo=l(),s6=a("li"),Wce=a("strong"),tQo=o("tapas"),aQo=o(" \u2014 "),TD=a("a"),nQo=o("TFTapasModel"),sQo=o(" (TAPAS model)"),lQo=l(),l6=a("li"),Qce=a("strong"),iQo=o("transfo-xl"),dQo=o(" \u2014 "),FD=a("a"),cQo=o("TFTransfoXLModel"),fQo=o(" (Transformer-XL model)"),mQo=l(),i6=a("li"),Hce=a("strong"),gQo=o("vit"),hQo=o(" \u2014 "),CD=a("a"),pQo=o("TFViTModel"),_Qo=o(" (ViT model)"),uQo=l(),d6=a("li"),Uce=a("strong"),bQo=o("wav2vec2"),vQo=o(" \u2014 "),MD=a("a"),TQo=o("TFWav2Vec2Model"),FQo=o(" (Wav2Vec2 model)"),CQo=l(),c6=a("li"),Jce=a("strong"),MQo=o("xlm"),EQo=o(" \u2014 "),ED=a("a"),yQo=o("TFXLMModel"),wQo=o(" (XLM model)"),AQo=l(),f6=a("li"),Yce=a("strong"),LQo=o("xlm-roberta"),BQo=o(" \u2014 "),yD=a("a"),kQo=o("TFXLMRobertaModel"),xQo=o(" (XLM-RoBERTa model)"),RQo=l(),m6=a("li"),Kce=a("strong"),SQo=o("xlnet"),PQo=o(" \u2014 "),wD=a("a"),$Qo=o("TFXLNetModel"),IQo=o(" (XLNet model)"),jQo=l(),Zce=a("p"),NQo=o("Examples:"),DQo=l(),f(Dy.$$.fragment),E9e=l(),ac=a("h2"),g6=a("a"),efe=a("span"),f(qy.$$.fragment),qQo=l(),ofe=a("span"),GQo=o("TFAutoModelForPreTraining"),y9e=l(),hr=a("div"),f(Gy.$$.fragment),OQo=l(),nc=a("p"),XQo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a pretraining head) when created
with the `),rfe=a("code"),zQo=o("from_pretrained()"),VQo=o("class method or the "),tfe=a("code"),WQo=o("from_config()"),QQo=o(`class
method.`),HQo=l(),Oy=a("p"),UQo=o("This class cannot be instantiated directly using "),afe=a("code"),JQo=o("__init__()"),YQo=o(" (throws an error)."),KQo=l(),lt=a("div"),f(Xy.$$.fragment),ZQo=l(),nfe=a("p"),eHo=o("Instantiates one of the model classes of the library (with a pretraining head) from a configuration."),oHo=l(),sc=a("p"),rHo=o(`Note:
Loading a model from its configuration file does `),sfe=a("strong"),tHo=o("not"),aHo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),lfe=a("code"),nHo=o("from_pretrained()"),sHo=o("to load the model weights."),lHo=l(),ife=a("p"),iHo=o("Examples:"),dHo=l(),f(zy.$$.fragment),cHo=l(),ho=a("div"),f(Vy.$$.fragment),fHo=l(),dfe=a("p"),mHo=o("Instantiate one of the model classes of the library (with a pretraining head) from a pretrained model."),gHo=l(),dn=a("p"),hHo=o("The model class to instantiate is selected based on the "),cfe=a("code"),pHo=o("model_type"),_Ho=o(` property of the config object (either
passed as an argument or loaded from `),ffe=a("code"),uHo=o("pretrained_model_name_or_path"),bHo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),mfe=a("code"),vHo=o("pretrained_model_name_or_path"),THo=o(":"),FHo=l(),H=a("ul"),h6=a("li"),gfe=a("strong"),CHo=o("albert"),MHo=o(" \u2014 "),AD=a("a"),EHo=o("TFAlbertForPreTraining"),yHo=o(" (ALBERT model)"),wHo=l(),p6=a("li"),hfe=a("strong"),AHo=o("bart"),LHo=o(" \u2014 "),LD=a("a"),BHo=o("TFBartForConditionalGeneration"),kHo=o(" (BART model)"),xHo=l(),_6=a("li"),pfe=a("strong"),RHo=o("bert"),SHo=o(" \u2014 "),BD=a("a"),PHo=o("TFBertForPreTraining"),$Ho=o(" (BERT model)"),IHo=l(),u6=a("li"),_fe=a("strong"),jHo=o("camembert"),NHo=o(" \u2014 "),kD=a("a"),DHo=o("TFCamembertForMaskedLM"),qHo=o(" (CamemBERT model)"),GHo=l(),b6=a("li"),ufe=a("strong"),OHo=o("ctrl"),XHo=o(" \u2014 "),xD=a("a"),zHo=o("TFCTRLLMHeadModel"),VHo=o(" (CTRL model)"),WHo=l(),v6=a("li"),bfe=a("strong"),QHo=o("distilbert"),HHo=o(" \u2014 "),RD=a("a"),UHo=o("TFDistilBertForMaskedLM"),JHo=o(" (DistilBERT model)"),YHo=l(),T6=a("li"),vfe=a("strong"),KHo=o("electra"),ZHo=o(" \u2014 "),SD=a("a"),eUo=o("TFElectraForPreTraining"),oUo=o(" (ELECTRA model)"),rUo=l(),F6=a("li"),Tfe=a("strong"),tUo=o("flaubert"),aUo=o(" \u2014 "),PD=a("a"),nUo=o("TFFlaubertWithLMHeadModel"),sUo=o(" (FlauBERT model)"),lUo=l(),C6=a("li"),Ffe=a("strong"),iUo=o("funnel"),dUo=o(" \u2014 "),$D=a("a"),cUo=o("TFFunnelForPreTraining"),fUo=o(" (Funnel Transformer model)"),mUo=l(),M6=a("li"),Cfe=a("strong"),gUo=o("gpt2"),hUo=o(" \u2014 "),ID=a("a"),pUo=o("TFGPT2LMHeadModel"),_Uo=o(" (OpenAI GPT-2 model)"),uUo=l(),E6=a("li"),Mfe=a("strong"),bUo=o("layoutlm"),vUo=o(" \u2014 "),jD=a("a"),TUo=o("TFLayoutLMForMaskedLM"),FUo=o(" (LayoutLM model)"),CUo=l(),y6=a("li"),Efe=a("strong"),MUo=o("lxmert"),EUo=o(" \u2014 "),ND=a("a"),yUo=o("TFLxmertForPreTraining"),wUo=o(" (LXMERT model)"),AUo=l(),w6=a("li"),yfe=a("strong"),LUo=o("mobilebert"),BUo=o(" \u2014 "),DD=a("a"),kUo=o("TFMobileBertForPreTraining"),xUo=o(" (MobileBERT model)"),RUo=l(),A6=a("li"),wfe=a("strong"),SUo=o("mpnet"),PUo=o(" \u2014 "),qD=a("a"),$Uo=o("TFMPNetForMaskedLM"),IUo=o(" (MPNet model)"),jUo=l(),L6=a("li"),Afe=a("strong"),NUo=o("openai-gpt"),DUo=o(" \u2014 "),GD=a("a"),qUo=o("TFOpenAIGPTLMHeadModel"),GUo=o(" (OpenAI GPT model)"),OUo=l(),B6=a("li"),Lfe=a("strong"),XUo=o("roberta"),zUo=o(" \u2014 "),OD=a("a"),VUo=o("TFRobertaForMaskedLM"),WUo=o(" (RoBERTa model)"),QUo=l(),k6=a("li"),Bfe=a("strong"),HUo=o("t5"),UUo=o(" \u2014 "),XD=a("a"),JUo=o("TFT5ForConditionalGeneration"),YUo=o(" (T5 model)"),KUo=l(),x6=a("li"),kfe=a("strong"),ZUo=o("tapas"),eJo=o(" \u2014 "),zD=a("a"),oJo=o("TFTapasForMaskedLM"),rJo=o(" (TAPAS model)"),tJo=l(),R6=a("li"),xfe=a("strong"),aJo=o("transfo-xl"),nJo=o(" \u2014 "),VD=a("a"),sJo=o("TFTransfoXLLMHeadModel"),lJo=o(" (Transformer-XL model)"),iJo=l(),S6=a("li"),Rfe=a("strong"),dJo=o("xlm"),cJo=o(" \u2014 "),WD=a("a"),fJo=o("TFXLMWithLMHeadModel"),mJo=o(" (XLM model)"),gJo=l(),P6=a("li"),Sfe=a("strong"),hJo=o("xlm-roberta"),pJo=o(" \u2014 "),QD=a("a"),_Jo=o("TFXLMRobertaForMaskedLM"),uJo=o(" (XLM-RoBERTa model)"),bJo=l(),$6=a("li"),Pfe=a("strong"),vJo=o("xlnet"),TJo=o(" \u2014 "),HD=a("a"),FJo=o("TFXLNetLMHeadModel"),CJo=o(" (XLNet model)"),MJo=l(),$fe=a("p"),EJo=o("Examples:"),yJo=l(),f(Wy.$$.fragment),w9e=l(),lc=a("h2"),I6=a("a"),Ife=a("span"),f(Qy.$$.fragment),wJo=l(),jfe=a("span"),AJo=o("TFAutoModelForCausalLM"),A9e=l(),pr=a("div"),f(Hy.$$.fragment),LJo=l(),ic=a("p"),BJo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a causal language modeling head) when created
with the `),Nfe=a("code"),kJo=o("from_pretrained()"),xJo=o("class method or the "),Dfe=a("code"),RJo=o("from_config()"),SJo=o(`class
method.`),PJo=l(),Uy=a("p"),$Jo=o("This class cannot be instantiated directly using "),qfe=a("code"),IJo=o("__init__()"),jJo=o(" (throws an error)."),NJo=l(),it=a("div"),f(Jy.$$.fragment),DJo=l(),Gfe=a("p"),qJo=o("Instantiates one of the model classes of the library (with a causal language modeling head) from a configuration."),GJo=l(),dc=a("p"),OJo=o(`Note:
Loading a model from its configuration file does `),Ofe=a("strong"),XJo=o("not"),zJo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Xfe=a("code"),VJo=o("from_pretrained()"),WJo=o("to load the model weights."),QJo=l(),zfe=a("p"),HJo=o("Examples:"),UJo=l(),f(Yy.$$.fragment),JJo=l(),po=a("div"),f(Ky.$$.fragment),YJo=l(),Vfe=a("p"),KJo=o("Instantiate one of the model classes of the library (with a causal language modeling head) from a pretrained model."),ZJo=l(),cn=a("p"),eYo=o("The model class to instantiate is selected based on the "),Wfe=a("code"),oYo=o("model_type"),rYo=o(` property of the config object (either
passed as an argument or loaded from `),Qfe=a("code"),tYo=o("pretrained_model_name_or_path"),aYo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Hfe=a("code"),nYo=o("pretrained_model_name_or_path"),sYo=o(":"),lYo=l(),he=a("ul"),j6=a("li"),Ufe=a("strong"),iYo=o("bert"),dYo=o(" \u2014 "),UD=a("a"),cYo=o("TFBertLMHeadModel"),fYo=o(" (BERT model)"),mYo=l(),N6=a("li"),Jfe=a("strong"),gYo=o("ctrl"),hYo=o(" \u2014 "),JD=a("a"),pYo=o("TFCTRLLMHeadModel"),_Yo=o(" (CTRL model)"),uYo=l(),D6=a("li"),Yfe=a("strong"),bYo=o("gpt2"),vYo=o(" \u2014 "),YD=a("a"),TYo=o("TFGPT2LMHeadModel"),FYo=o(" (OpenAI GPT-2 model)"),CYo=l(),q6=a("li"),Kfe=a("strong"),MYo=o("openai-gpt"),EYo=o(" \u2014 "),KD=a("a"),yYo=o("TFOpenAIGPTLMHeadModel"),wYo=o(" (OpenAI GPT model)"),AYo=l(),G6=a("li"),Zfe=a("strong"),LYo=o("rembert"),BYo=o(" \u2014 "),ZD=a("a"),kYo=o("TFRemBertForCausalLM"),xYo=o(" (RemBERT model)"),RYo=l(),O6=a("li"),eme=a("strong"),SYo=o("roberta"),PYo=o(" \u2014 "),eq=a("a"),$Yo=o("TFRobertaForCausalLM"),IYo=o(" (RoBERTa model)"),jYo=l(),X6=a("li"),ome=a("strong"),NYo=o("roformer"),DYo=o(" \u2014 "),oq=a("a"),qYo=o("TFRoFormerForCausalLM"),GYo=o(" (RoFormer model)"),OYo=l(),z6=a("li"),rme=a("strong"),XYo=o("transfo-xl"),zYo=o(" \u2014 "),rq=a("a"),VYo=o("TFTransfoXLLMHeadModel"),WYo=o(" (Transformer-XL model)"),QYo=l(),V6=a("li"),tme=a("strong"),HYo=o("xlm"),UYo=o(" \u2014 "),tq=a("a"),JYo=o("TFXLMWithLMHeadModel"),YYo=o(" (XLM model)"),KYo=l(),W6=a("li"),ame=a("strong"),ZYo=o("xlnet"),eKo=o(" \u2014 "),aq=a("a"),oKo=o("TFXLNetLMHeadModel"),rKo=o(" (XLNet model)"),tKo=l(),nme=a("p"),aKo=o("Examples:"),nKo=l(),f(Zy.$$.fragment),L9e=l(),cc=a("h2"),Q6=a("a"),sme=a("span"),f(ew.$$.fragment),sKo=l(),lme=a("span"),lKo=o("TFAutoModelForImageClassification"),B9e=l(),_r=a("div"),f(ow.$$.fragment),iKo=l(),fc=a("p"),dKo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a image classification head) when created
with the `),ime=a("code"),cKo=o("from_pretrained()"),fKo=o("class method or the "),dme=a("code"),mKo=o("from_config()"),gKo=o(`class
method.`),hKo=l(),rw=a("p"),pKo=o("This class cannot be instantiated directly using "),cme=a("code"),_Ko=o("__init__()"),uKo=o(" (throws an error)."),bKo=l(),dt=a("div"),f(tw.$$.fragment),vKo=l(),fme=a("p"),TKo=o("Instantiates one of the model classes of the library (with a image classification head) from a configuration."),FKo=l(),mc=a("p"),CKo=o(`Note:
Loading a model from its configuration file does `),mme=a("strong"),MKo=o("not"),EKo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),gme=a("code"),yKo=o("from_pretrained()"),wKo=o("to load the model weights."),AKo=l(),hme=a("p"),LKo=o("Examples:"),BKo=l(),f(aw.$$.fragment),kKo=l(),_o=a("div"),f(nw.$$.fragment),xKo=l(),pme=a("p"),RKo=o("Instantiate one of the model classes of the library (with a image classification head) from a pretrained model."),SKo=l(),fn=a("p"),PKo=o("The model class to instantiate is selected based on the "),_me=a("code"),$Ko=o("model_type"),IKo=o(` property of the config object (either
passed as an argument or loaded from `),ume=a("code"),jKo=o("pretrained_model_name_or_path"),NKo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),bme=a("code"),DKo=o("pretrained_model_name_or_path"),qKo=o(":"),GKo=l(),sw=a("ul"),H6=a("li"),vme=a("strong"),OKo=o("convnext"),XKo=o(" \u2014 "),nq=a("a"),zKo=o("TFConvNextForImageClassification"),VKo=o(" (ConvNext model)"),WKo=l(),U6=a("li"),Tme=a("strong"),QKo=o("vit"),HKo=o(" \u2014 "),sq=a("a"),UKo=o("TFViTForImageClassification"),JKo=o(" (ViT model)"),YKo=l(),Fme=a("p"),KKo=o("Examples:"),ZKo=l(),f(lw.$$.fragment),k9e=l(),gc=a("h2"),J6=a("a"),Cme=a("span"),f(iw.$$.fragment),eZo=l(),Mme=a("span"),oZo=o("TFAutoModelForMaskedLM"),x9e=l(),ur=a("div"),f(dw.$$.fragment),rZo=l(),hc=a("p"),tZo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a masked language modeling head) when created
with the `),Eme=a("code"),aZo=o("from_pretrained()"),nZo=o("class method or the "),yme=a("code"),sZo=o("from_config()"),lZo=o(`class
method.`),iZo=l(),cw=a("p"),dZo=o("This class cannot be instantiated directly using "),wme=a("code"),cZo=o("__init__()"),fZo=o(" (throws an error)."),mZo=l(),ct=a("div"),f(fw.$$.fragment),gZo=l(),Ame=a("p"),hZo=o("Instantiates one of the model classes of the library (with a masked language modeling head) from a configuration."),pZo=l(),pc=a("p"),_Zo=o(`Note:
Loading a model from its configuration file does `),Lme=a("strong"),uZo=o("not"),bZo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Bme=a("code"),vZo=o("from_pretrained()"),TZo=o("to load the model weights."),FZo=l(),kme=a("p"),CZo=o("Examples:"),MZo=l(),f(mw.$$.fragment),EZo=l(),uo=a("div"),f(gw.$$.fragment),yZo=l(),xme=a("p"),wZo=o("Instantiate one of the model classes of the library (with a masked language modeling head) from a pretrained model."),AZo=l(),mn=a("p"),LZo=o("The model class to instantiate is selected based on the "),Rme=a("code"),BZo=o("model_type"),kZo=o(` property of the config object (either
passed as an argument or loaded from `),Sme=a("code"),xZo=o("pretrained_model_name_or_path"),RZo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Pme=a("code"),SZo=o("pretrained_model_name_or_path"),PZo=o(":"),$Zo=l(),Y=a("ul"),Y6=a("li"),$me=a("strong"),IZo=o("albert"),jZo=o(" \u2014 "),lq=a("a"),NZo=o("TFAlbertForMaskedLM"),DZo=o(" (ALBERT model)"),qZo=l(),K6=a("li"),Ime=a("strong"),GZo=o("bert"),OZo=o(" \u2014 "),iq=a("a"),XZo=o("TFBertForMaskedLM"),zZo=o(" (BERT model)"),VZo=l(),Z6=a("li"),jme=a("strong"),WZo=o("camembert"),QZo=o(" \u2014 "),dq=a("a"),HZo=o("TFCamembertForMaskedLM"),UZo=o(" (CamemBERT model)"),JZo=l(),eT=a("li"),Nme=a("strong"),YZo=o("convbert"),KZo=o(" \u2014 "),cq=a("a"),ZZo=o("TFConvBertForMaskedLM"),eer=o(" (ConvBERT model)"),oer=l(),oT=a("li"),Dme=a("strong"),rer=o("deberta"),ter=o(" \u2014 "),fq=a("a"),aer=o("TFDebertaForMaskedLM"),ner=o(" (DeBERTa model)"),ser=l(),rT=a("li"),qme=a("strong"),ler=o("deberta-v2"),ier=o(" \u2014 "),mq=a("a"),der=o("TFDebertaV2ForMaskedLM"),cer=o(" (DeBERTa-v2 model)"),fer=l(),tT=a("li"),Gme=a("strong"),mer=o("distilbert"),ger=o(" \u2014 "),gq=a("a"),her=o("TFDistilBertForMaskedLM"),per=o(" (DistilBERT model)"),_er=l(),aT=a("li"),Ome=a("strong"),uer=o("electra"),ber=o(" \u2014 "),hq=a("a"),ver=o("TFElectraForMaskedLM"),Ter=o(" (ELECTRA model)"),Fer=l(),nT=a("li"),Xme=a("strong"),Cer=o("flaubert"),Mer=o(" \u2014 "),pq=a("a"),Eer=o("TFFlaubertWithLMHeadModel"),yer=o(" (FlauBERT model)"),wer=l(),sT=a("li"),zme=a("strong"),Aer=o("funnel"),Ler=o(" \u2014 "),_q=a("a"),Ber=o("TFFunnelForMaskedLM"),ker=o(" (Funnel Transformer model)"),xer=l(),lT=a("li"),Vme=a("strong"),Rer=o("layoutlm"),Ser=o(" \u2014 "),uq=a("a"),Per=o("TFLayoutLMForMaskedLM"),$er=o(" (LayoutLM model)"),Ier=l(),iT=a("li"),Wme=a("strong"),jer=o("longformer"),Ner=o(" \u2014 "),bq=a("a"),Der=o("TFLongformerForMaskedLM"),qer=o(" (Longformer model)"),Ger=l(),dT=a("li"),Qme=a("strong"),Oer=o("mobilebert"),Xer=o(" \u2014 "),vq=a("a"),zer=o("TFMobileBertForMaskedLM"),Ver=o(" (MobileBERT model)"),Wer=l(),cT=a("li"),Hme=a("strong"),Qer=o("mpnet"),Her=o(" \u2014 "),Tq=a("a"),Uer=o("TFMPNetForMaskedLM"),Jer=o(" (MPNet model)"),Yer=l(),fT=a("li"),Ume=a("strong"),Ker=o("rembert"),Zer=o(" \u2014 "),Fq=a("a"),eor=o("TFRemBertForMaskedLM"),oor=o(" (RemBERT model)"),ror=l(),mT=a("li"),Jme=a("strong"),tor=o("roberta"),aor=o(" \u2014 "),Cq=a("a"),nor=o("TFRobertaForMaskedLM"),sor=o(" (RoBERTa model)"),lor=l(),gT=a("li"),Yme=a("strong"),ior=o("roformer"),dor=o(" \u2014 "),Mq=a("a"),cor=o("TFRoFormerForMaskedLM"),mor=o(" (RoFormer model)"),gor=l(),hT=a("li"),Kme=a("strong"),hor=o("tapas"),por=o(" \u2014 "),Eq=a("a"),_or=o("TFTapasForMaskedLM"),uor=o(" (TAPAS model)"),bor=l(),pT=a("li"),Zme=a("strong"),vor=o("xlm"),Tor=o(" \u2014 "),yq=a("a"),For=o("TFXLMWithLMHeadModel"),Cor=o(" (XLM model)"),Mor=l(),_T=a("li"),ege=a("strong"),Eor=o("xlm-roberta"),yor=o(" \u2014 "),wq=a("a"),wor=o("TFXLMRobertaForMaskedLM"),Aor=o(" (XLM-RoBERTa model)"),Lor=l(),oge=a("p"),Bor=o("Examples:"),kor=l(),f(hw.$$.fragment),R9e=l(),_c=a("h2"),uT=a("a"),rge=a("span"),f(pw.$$.fragment),xor=l(),tge=a("span"),Ror=o("TFAutoModelForSeq2SeqLM"),S9e=l(),br=a("div"),f(_w.$$.fragment),Sor=l(),uc=a("p"),Por=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence language modeling head) when created
with the `),age=a("code"),$or=o("from_pretrained()"),Ior=o("class method or the "),nge=a("code"),jor=o("from_config()"),Nor=o(`class
method.`),Dor=l(),uw=a("p"),qor=o("This class cannot be instantiated directly using "),sge=a("code"),Gor=o("__init__()"),Oor=o(" (throws an error)."),Xor=l(),ft=a("div"),f(bw.$$.fragment),zor=l(),lge=a("p"),Vor=o("Instantiates one of the model classes of the library (with a sequence-to-sequence language modeling head) from a configuration."),Wor=l(),bc=a("p"),Qor=o(`Note:
Loading a model from its configuration file does `),ige=a("strong"),Hor=o("not"),Uor=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),dge=a("code"),Jor=o("from_pretrained()"),Yor=o("to load the model weights."),Kor=l(),cge=a("p"),Zor=o("Examples:"),err=l(),f(vw.$$.fragment),orr=l(),bo=a("div"),f(Tw.$$.fragment),rrr=l(),fge=a("p"),trr=o("Instantiate one of the model classes of the library (with a sequence-to-sequence language modeling head) from a pretrained model."),arr=l(),gn=a("p"),nrr=o("The model class to instantiate is selected based on the "),mge=a("code"),srr=o("model_type"),lrr=o(` property of the config object (either
passed as an argument or loaded from `),gge=a("code"),irr=o("pretrained_model_name_or_path"),drr=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),hge=a("code"),crr=o("pretrained_model_name_or_path"),frr=o(":"),mrr=l(),pe=a("ul"),bT=a("li"),pge=a("strong"),grr=o("bart"),hrr=o(" \u2014 "),Aq=a("a"),prr=o("TFBartForConditionalGeneration"),_rr=o(" (BART model)"),urr=l(),vT=a("li"),_ge=a("strong"),brr=o("blenderbot"),vrr=o(" \u2014 "),Lq=a("a"),Trr=o("TFBlenderbotForConditionalGeneration"),Frr=o(" (Blenderbot model)"),Crr=l(),TT=a("li"),uge=a("strong"),Mrr=o("blenderbot-small"),Err=o(" \u2014 "),Bq=a("a"),yrr=o("TFBlenderbotSmallForConditionalGeneration"),wrr=o(" (BlenderbotSmall model)"),Arr=l(),FT=a("li"),bge=a("strong"),Lrr=o("encoder-decoder"),Brr=o(" \u2014 "),kq=a("a"),krr=o("TFEncoderDecoderModel"),xrr=o(" (Encoder decoder model)"),Rrr=l(),CT=a("li"),vge=a("strong"),Srr=o("led"),Prr=o(" \u2014 "),xq=a("a"),$rr=o("TFLEDForConditionalGeneration"),Irr=o(" (LED model)"),jrr=l(),MT=a("li"),Tge=a("strong"),Nrr=o("marian"),Drr=o(" \u2014 "),Rq=a("a"),qrr=o("TFMarianMTModel"),Grr=o(" (Marian model)"),Orr=l(),ET=a("li"),Fge=a("strong"),Xrr=o("mbart"),zrr=o(" \u2014 "),Sq=a("a"),Vrr=o("TFMBartForConditionalGeneration"),Wrr=o(" (mBART model)"),Qrr=l(),yT=a("li"),Cge=a("strong"),Hrr=o("mt5"),Urr=o(" \u2014 "),Pq=a("a"),Jrr=o("TFMT5ForConditionalGeneration"),Yrr=o(" (mT5 model)"),Krr=l(),wT=a("li"),Mge=a("strong"),Zrr=o("pegasus"),etr=o(" \u2014 "),$q=a("a"),otr=o("TFPegasusForConditionalGeneration"),rtr=o(" (Pegasus model)"),ttr=l(),AT=a("li"),Ege=a("strong"),atr=o("t5"),ntr=o(" \u2014 "),Iq=a("a"),str=o("TFT5ForConditionalGeneration"),ltr=o(" (T5 model)"),itr=l(),yge=a("p"),dtr=o("Examples:"),ctr=l(),f(Fw.$$.fragment),P9e=l(),vc=a("h2"),LT=a("a"),wge=a("span"),f(Cw.$$.fragment),ftr=l(),Age=a("span"),mtr=o("TFAutoModelForSequenceClassification"),$9e=l(),vr=a("div"),f(Mw.$$.fragment),gtr=l(),Tc=a("p"),htr=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence classification head) when created
with the `),Lge=a("code"),ptr=o("from_pretrained()"),_tr=o("class method or the "),Bge=a("code"),utr=o("from_config()"),btr=o(`class
method.`),vtr=l(),Ew=a("p"),Ttr=o("This class cannot be instantiated directly using "),kge=a("code"),Ftr=o("__init__()"),Ctr=o(" (throws an error)."),Mtr=l(),mt=a("div"),f(yw.$$.fragment),Etr=l(),xge=a("p"),ytr=o("Instantiates one of the model classes of the library (with a sequence classification head) from a configuration."),wtr=l(),Fc=a("p"),Atr=o(`Note:
Loading a model from its configuration file does `),Rge=a("strong"),Ltr=o("not"),Btr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Sge=a("code"),ktr=o("from_pretrained()"),xtr=o("to load the model weights."),Rtr=l(),Pge=a("p"),Str=o("Examples:"),Ptr=l(),f(ww.$$.fragment),$tr=l(),vo=a("div"),f(Aw.$$.fragment),Itr=l(),$ge=a("p"),jtr=o("Instantiate one of the model classes of the library (with a sequence classification head) from a pretrained model."),Ntr=l(),hn=a("p"),Dtr=o("The model class to instantiate is selected based on the "),Ige=a("code"),qtr=o("model_type"),Gtr=o(` property of the config object (either
passed as an argument or loaded from `),jge=a("code"),Otr=o("pretrained_model_name_or_path"),Xtr=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Nge=a("code"),ztr=o("pretrained_model_name_or_path"),Vtr=o(":"),Wtr=l(),X=a("ul"),BT=a("li"),Dge=a("strong"),Qtr=o("albert"),Htr=o(" \u2014 "),jq=a("a"),Utr=o("TFAlbertForSequenceClassification"),Jtr=o(" (ALBERT model)"),Ytr=l(),kT=a("li"),qge=a("strong"),Ktr=o("bert"),Ztr=o(" \u2014 "),Nq=a("a"),ear=o("TFBertForSequenceClassification"),oar=o(" (BERT model)"),rar=l(),xT=a("li"),Gge=a("strong"),tar=o("camembert"),aar=o(" \u2014 "),Dq=a("a"),nar=o("TFCamembertForSequenceClassification"),sar=o(" (CamemBERT model)"),lar=l(),RT=a("li"),Oge=a("strong"),iar=o("convbert"),dar=o(" \u2014 "),qq=a("a"),car=o("TFConvBertForSequenceClassification"),far=o(" (ConvBERT model)"),mar=l(),ST=a("li"),Xge=a("strong"),gar=o("ctrl"),har=o(" \u2014 "),Gq=a("a"),par=o("TFCTRLForSequenceClassification"),_ar=o(" (CTRL model)"),uar=l(),PT=a("li"),zge=a("strong"),bar=o("deberta"),Tar=o(" \u2014 "),Oq=a("a"),Far=o("TFDebertaForSequenceClassification"),Car=o(" (DeBERTa model)"),Mar=l(),$T=a("li"),Vge=a("strong"),Ear=o("deberta-v2"),yar=o(" \u2014 "),Xq=a("a"),war=o("TFDebertaV2ForSequenceClassification"),Aar=o(" (DeBERTa-v2 model)"),Lar=l(),IT=a("li"),Wge=a("strong"),Bar=o("distilbert"),kar=o(" \u2014 "),zq=a("a"),xar=o("TFDistilBertForSequenceClassification"),Rar=o(" (DistilBERT model)"),Sar=l(),jT=a("li"),Qge=a("strong"),Par=o("electra"),$ar=o(" \u2014 "),Vq=a("a"),Iar=o("TFElectraForSequenceClassification"),jar=o(" (ELECTRA model)"),Nar=l(),NT=a("li"),Hge=a("strong"),Dar=o("flaubert"),qar=o(" \u2014 "),Wq=a("a"),Gar=o("TFFlaubertForSequenceClassification"),Oar=o(" (FlauBERT model)"),Xar=l(),DT=a("li"),Uge=a("strong"),zar=o("funnel"),Var=o(" \u2014 "),Qq=a("a"),War=o("TFFunnelForSequenceClassification"),Qar=o(" (Funnel Transformer model)"),Har=l(),qT=a("li"),Jge=a("strong"),Uar=o("gpt2"),Jar=o(" \u2014 "),Hq=a("a"),Yar=o("TFGPT2ForSequenceClassification"),Kar=o(" (OpenAI GPT-2 model)"),Zar=l(),GT=a("li"),Yge=a("strong"),enr=o("layoutlm"),onr=o(" \u2014 "),Uq=a("a"),rnr=o("TFLayoutLMForSequenceClassification"),tnr=o(" (LayoutLM model)"),anr=l(),OT=a("li"),Kge=a("strong"),nnr=o("longformer"),snr=o(" \u2014 "),Jq=a("a"),lnr=o("TFLongformerForSequenceClassification"),inr=o(" (Longformer model)"),dnr=l(),XT=a("li"),Zge=a("strong"),cnr=o("mobilebert"),fnr=o(" \u2014 "),Yq=a("a"),mnr=o("TFMobileBertForSequenceClassification"),gnr=o(" (MobileBERT model)"),hnr=l(),zT=a("li"),ehe=a("strong"),pnr=o("mpnet"),_nr=o(" \u2014 "),Kq=a("a"),unr=o("TFMPNetForSequenceClassification"),bnr=o(" (MPNet model)"),vnr=l(),VT=a("li"),ohe=a("strong"),Tnr=o("openai-gpt"),Fnr=o(" \u2014 "),Zq=a("a"),Cnr=o("TFOpenAIGPTForSequenceClassification"),Mnr=o(" (OpenAI GPT model)"),Enr=l(),WT=a("li"),rhe=a("strong"),ynr=o("rembert"),wnr=o(" \u2014 "),eG=a("a"),Anr=o("TFRemBertForSequenceClassification"),Lnr=o(" (RemBERT model)"),Bnr=l(),QT=a("li"),the=a("strong"),knr=o("roberta"),xnr=o(" \u2014 "),oG=a("a"),Rnr=o("TFRobertaForSequenceClassification"),Snr=o(" (RoBERTa model)"),Pnr=l(),HT=a("li"),ahe=a("strong"),$nr=o("roformer"),Inr=o(" \u2014 "),rG=a("a"),jnr=o("TFRoFormerForSequenceClassification"),Nnr=o(" (RoFormer model)"),Dnr=l(),UT=a("li"),nhe=a("strong"),qnr=o("tapas"),Gnr=o(" \u2014 "),tG=a("a"),Onr=o("TFTapasForSequenceClassification"),Xnr=o(" (TAPAS model)"),znr=l(),JT=a("li"),she=a("strong"),Vnr=o("transfo-xl"),Wnr=o(" \u2014 "),aG=a("a"),Qnr=o("TFTransfoXLForSequenceClassification"),Hnr=o(" (Transformer-XL model)"),Unr=l(),YT=a("li"),lhe=a("strong"),Jnr=o("xlm"),Ynr=o(" \u2014 "),nG=a("a"),Knr=o("TFXLMForSequenceClassification"),Znr=o(" (XLM model)"),esr=l(),KT=a("li"),ihe=a("strong"),osr=o("xlm-roberta"),rsr=o(" \u2014 "),sG=a("a"),tsr=o("TFXLMRobertaForSequenceClassification"),asr=o(" (XLM-RoBERTa model)"),nsr=l(),ZT=a("li"),dhe=a("strong"),ssr=o("xlnet"),lsr=o(" \u2014 "),lG=a("a"),isr=o("TFXLNetForSequenceClassification"),dsr=o(" (XLNet model)"),csr=l(),che=a("p"),fsr=o("Examples:"),msr=l(),f(Lw.$$.fragment),I9e=l(),Cc=a("h2"),e8=a("a"),fhe=a("span"),f(Bw.$$.fragment),gsr=l(),mhe=a("span"),hsr=o("TFAutoModelForMultipleChoice"),j9e=l(),Tr=a("div"),f(kw.$$.fragment),psr=l(),Mc=a("p"),_sr=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a multiple choice head) when created
with the `),ghe=a("code"),usr=o("from_pretrained()"),bsr=o("class method or the "),hhe=a("code"),vsr=o("from_config()"),Tsr=o(`class
method.`),Fsr=l(),xw=a("p"),Csr=o("This class cannot be instantiated directly using "),phe=a("code"),Msr=o("__init__()"),Esr=o(" (throws an error)."),ysr=l(),gt=a("div"),f(Rw.$$.fragment),wsr=l(),_he=a("p"),Asr=o("Instantiates one of the model classes of the library (with a multiple choice head) from a configuration."),Lsr=l(),Ec=a("p"),Bsr=o(`Note:
Loading a model from its configuration file does `),uhe=a("strong"),ksr=o("not"),xsr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),bhe=a("code"),Rsr=o("from_pretrained()"),Ssr=o("to load the model weights."),Psr=l(),vhe=a("p"),$sr=o("Examples:"),Isr=l(),f(Sw.$$.fragment),jsr=l(),To=a("div"),f(Pw.$$.fragment),Nsr=l(),The=a("p"),Dsr=o("Instantiate one of the model classes of the library (with a multiple choice head) from a pretrained model."),qsr=l(),pn=a("p"),Gsr=o("The model class to instantiate is selected based on the "),Fhe=a("code"),Osr=o("model_type"),Xsr=o(` property of the config object (either
passed as an argument or loaded from `),Che=a("code"),zsr=o("pretrained_model_name_or_path"),Vsr=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Mhe=a("code"),Wsr=o("pretrained_model_name_or_path"),Qsr=o(":"),Hsr=l(),te=a("ul"),o8=a("li"),Ehe=a("strong"),Usr=o("albert"),Jsr=o(" \u2014 "),iG=a("a"),Ysr=o("TFAlbertForMultipleChoice"),Ksr=o(" (ALBERT model)"),Zsr=l(),r8=a("li"),yhe=a("strong"),elr=o("bert"),olr=o(" \u2014 "),dG=a("a"),rlr=o("TFBertForMultipleChoice"),tlr=o(" (BERT model)"),alr=l(),t8=a("li"),whe=a("strong"),nlr=o("camembert"),slr=o(" \u2014 "),cG=a("a"),llr=o("TFCamembertForMultipleChoice"),ilr=o(" (CamemBERT model)"),dlr=l(),a8=a("li"),Ahe=a("strong"),clr=o("convbert"),flr=o(" \u2014 "),fG=a("a"),mlr=o("TFConvBertForMultipleChoice"),glr=o(" (ConvBERT model)"),hlr=l(),n8=a("li"),Lhe=a("strong"),plr=o("distilbert"),_lr=o(" \u2014 "),mG=a("a"),ulr=o("TFDistilBertForMultipleChoice"),blr=o(" (DistilBERT model)"),vlr=l(),s8=a("li"),Bhe=a("strong"),Tlr=o("electra"),Flr=o(" \u2014 "),gG=a("a"),Clr=o("TFElectraForMultipleChoice"),Mlr=o(" (ELECTRA model)"),Elr=l(),l8=a("li"),khe=a("strong"),ylr=o("flaubert"),wlr=o(" \u2014 "),hG=a("a"),Alr=o("TFFlaubertForMultipleChoice"),Llr=o(" (FlauBERT model)"),Blr=l(),i8=a("li"),xhe=a("strong"),klr=o("funnel"),xlr=o(" \u2014 "),pG=a("a"),Rlr=o("TFFunnelForMultipleChoice"),Slr=o(" (Funnel Transformer model)"),Plr=l(),d8=a("li"),Rhe=a("strong"),$lr=o("longformer"),Ilr=o(" \u2014 "),_G=a("a"),jlr=o("TFLongformerForMultipleChoice"),Nlr=o(" (Longformer model)"),Dlr=l(),c8=a("li"),She=a("strong"),qlr=o("mobilebert"),Glr=o(" \u2014 "),uG=a("a"),Olr=o("TFMobileBertForMultipleChoice"),Xlr=o(" (MobileBERT model)"),zlr=l(),f8=a("li"),Phe=a("strong"),Vlr=o("mpnet"),Wlr=o(" \u2014 "),bG=a("a"),Qlr=o("TFMPNetForMultipleChoice"),Hlr=o(" (MPNet model)"),Ulr=l(),m8=a("li"),$he=a("strong"),Jlr=o("rembert"),Ylr=o(" \u2014 "),vG=a("a"),Klr=o("TFRemBertForMultipleChoice"),Zlr=o(" (RemBERT model)"),eir=l(),g8=a("li"),Ihe=a("strong"),oir=o("roberta"),rir=o(" \u2014 "),TG=a("a"),tir=o("TFRobertaForMultipleChoice"),air=o(" (RoBERTa model)"),nir=l(),h8=a("li"),jhe=a("strong"),sir=o("roformer"),lir=o(" \u2014 "),FG=a("a"),iir=o("TFRoFormerForMultipleChoice"),dir=o(" (RoFormer model)"),cir=l(),p8=a("li"),Nhe=a("strong"),fir=o("xlm"),mir=o(" \u2014 "),CG=a("a"),gir=o("TFXLMForMultipleChoice"),hir=o(" (XLM model)"),pir=l(),_8=a("li"),Dhe=a("strong"),_ir=o("xlm-roberta"),uir=o(" \u2014 "),MG=a("a"),bir=o("TFXLMRobertaForMultipleChoice"),vir=o(" (XLM-RoBERTa model)"),Tir=l(),u8=a("li"),qhe=a("strong"),Fir=o("xlnet"),Cir=o(" \u2014 "),EG=a("a"),Mir=o("TFXLNetForMultipleChoice"),Eir=o(" (XLNet model)"),yir=l(),Ghe=a("p"),wir=o("Examples:"),Air=l(),f($w.$$.fragment),N9e=l(),yc=a("h2"),b8=a("a"),Ohe=a("span"),f(Iw.$$.fragment),Lir=l(),Xhe=a("span"),Bir=o("TFAutoModelForTableQuestionAnswering"),D9e=l(),Fr=a("div"),f(jw.$$.fragment),kir=l(),wc=a("p"),xir=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a table question answering head) when created
with the `),zhe=a("code"),Rir=o("from_pretrained()"),Sir=o("class method or the "),Vhe=a("code"),Pir=o("from_config()"),$ir=o(`class
method.`),Iir=l(),Nw=a("p"),jir=o("This class cannot be instantiated directly using "),Whe=a("code"),Nir=o("__init__()"),Dir=o(" (throws an error)."),qir=l(),ht=a("div"),f(Dw.$$.fragment),Gir=l(),Qhe=a("p"),Oir=o("Instantiates one of the model classes of the library (with a table question answering head) from a configuration."),Xir=l(),Ac=a("p"),zir=o(`Note:
Loading a model from its configuration file does `),Hhe=a("strong"),Vir=o("not"),Wir=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Uhe=a("code"),Qir=o("from_pretrained()"),Hir=o("to load the model weights."),Uir=l(),Jhe=a("p"),Jir=o("Examples:"),Yir=l(),f(qw.$$.fragment),Kir=l(),Fo=a("div"),f(Gw.$$.fragment),Zir=l(),Yhe=a("p"),edr=o("Instantiate one of the model classes of the library (with a table question answering head) from a pretrained model."),odr=l(),_n=a("p"),rdr=o("The model class to instantiate is selected based on the "),Khe=a("code"),tdr=o("model_type"),adr=o(` property of the config object (either
passed as an argument or loaded from `),Zhe=a("code"),ndr=o("pretrained_model_name_or_path"),sdr=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),epe=a("code"),ldr=o("pretrained_model_name_or_path"),idr=o(":"),ddr=l(),ope=a("ul"),v8=a("li"),rpe=a("strong"),cdr=o("tapas"),fdr=o(" \u2014 "),yG=a("a"),mdr=o("TFTapasForQuestionAnswering"),gdr=o(" (TAPAS model)"),hdr=l(),tpe=a("p"),pdr=o("Examples:"),_dr=l(),f(Ow.$$.fragment),q9e=l(),Lc=a("h2"),T8=a("a"),ape=a("span"),f(Xw.$$.fragment),udr=l(),npe=a("span"),bdr=o("TFAutoModelForTokenClassification"),G9e=l(),Cr=a("div"),f(zw.$$.fragment),vdr=l(),Bc=a("p"),Tdr=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a token classification head) when created
with the `),spe=a("code"),Fdr=o("from_pretrained()"),Cdr=o("class method or the "),lpe=a("code"),Mdr=o("from_config()"),Edr=o(`class
method.`),ydr=l(),Vw=a("p"),wdr=o("This class cannot be instantiated directly using "),ipe=a("code"),Adr=o("__init__()"),Ldr=o(" (throws an error)."),Bdr=l(),pt=a("div"),f(Ww.$$.fragment),kdr=l(),dpe=a("p"),xdr=o("Instantiates one of the model classes of the library (with a token classification head) from a configuration."),Rdr=l(),kc=a("p"),Sdr=o(`Note:
Loading a model from its configuration file does `),cpe=a("strong"),Pdr=o("not"),$dr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),fpe=a("code"),Idr=o("from_pretrained()"),jdr=o("to load the model weights."),Ndr=l(),mpe=a("p"),Ddr=o("Examples:"),qdr=l(),f(Qw.$$.fragment),Gdr=l(),Co=a("div"),f(Hw.$$.fragment),Odr=l(),gpe=a("p"),Xdr=o("Instantiate one of the model classes of the library (with a token classification head) from a pretrained model."),zdr=l(),un=a("p"),Vdr=o("The model class to instantiate is selected based on the "),hpe=a("code"),Wdr=o("model_type"),Qdr=o(` property of the config object (either
passed as an argument or loaded from `),ppe=a("code"),Hdr=o("pretrained_model_name_or_path"),Udr=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),_pe=a("code"),Jdr=o("pretrained_model_name_or_path"),Ydr=o(":"),Kdr=l(),K=a("ul"),F8=a("li"),upe=a("strong"),Zdr=o("albert"),ecr=o(" \u2014 "),wG=a("a"),ocr=o("TFAlbertForTokenClassification"),rcr=o(" (ALBERT model)"),tcr=l(),C8=a("li"),bpe=a("strong"),acr=o("bert"),ncr=o(" \u2014 "),AG=a("a"),scr=o("TFBertForTokenClassification"),lcr=o(" (BERT model)"),icr=l(),M8=a("li"),vpe=a("strong"),dcr=o("camembert"),ccr=o(" \u2014 "),LG=a("a"),fcr=o("TFCamembertForTokenClassification"),mcr=o(" (CamemBERT model)"),gcr=l(),E8=a("li"),Tpe=a("strong"),hcr=o("convbert"),pcr=o(" \u2014 "),BG=a("a"),_cr=o("TFConvBertForTokenClassification"),ucr=o(" (ConvBERT model)"),bcr=l(),y8=a("li"),Fpe=a("strong"),vcr=o("deberta"),Tcr=o(" \u2014 "),kG=a("a"),Fcr=o("TFDebertaForTokenClassification"),Ccr=o(" (DeBERTa model)"),Mcr=l(),w8=a("li"),Cpe=a("strong"),Ecr=o("deberta-v2"),ycr=o(" \u2014 "),xG=a("a"),wcr=o("TFDebertaV2ForTokenClassification"),Acr=o(" (DeBERTa-v2 model)"),Lcr=l(),A8=a("li"),Mpe=a("strong"),Bcr=o("distilbert"),kcr=o(" \u2014 "),RG=a("a"),xcr=o("TFDistilBertForTokenClassification"),Rcr=o(" (DistilBERT model)"),Scr=l(),L8=a("li"),Epe=a("strong"),Pcr=o("electra"),$cr=o(" \u2014 "),SG=a("a"),Icr=o("TFElectraForTokenClassification"),jcr=o(" (ELECTRA model)"),Ncr=l(),B8=a("li"),ype=a("strong"),Dcr=o("flaubert"),qcr=o(" \u2014 "),PG=a("a"),Gcr=o("TFFlaubertForTokenClassification"),Ocr=o(" (FlauBERT model)"),Xcr=l(),k8=a("li"),wpe=a("strong"),zcr=o("funnel"),Vcr=o(" \u2014 "),$G=a("a"),Wcr=o("TFFunnelForTokenClassification"),Qcr=o(" (Funnel Transformer model)"),Hcr=l(),x8=a("li"),Ape=a("strong"),Ucr=o("layoutlm"),Jcr=o(" \u2014 "),IG=a("a"),Ycr=o("TFLayoutLMForTokenClassification"),Kcr=o(" (LayoutLM model)"),Zcr=l(),R8=a("li"),Lpe=a("strong"),efr=o("longformer"),ofr=o(" \u2014 "),jG=a("a"),rfr=o("TFLongformerForTokenClassification"),tfr=o(" (Longformer model)"),afr=l(),S8=a("li"),Bpe=a("strong"),nfr=o("mobilebert"),sfr=o(" \u2014 "),NG=a("a"),lfr=o("TFMobileBertForTokenClassification"),ifr=o(" (MobileBERT model)"),dfr=l(),P8=a("li"),kpe=a("strong"),cfr=o("mpnet"),ffr=o(" \u2014 "),DG=a("a"),mfr=o("TFMPNetForTokenClassification"),gfr=o(" (MPNet model)"),hfr=l(),$8=a("li"),xpe=a("strong"),pfr=o("rembert"),_fr=o(" \u2014 "),qG=a("a"),ufr=o("TFRemBertForTokenClassification"),bfr=o(" (RemBERT model)"),vfr=l(),I8=a("li"),Rpe=a("strong"),Tfr=o("roberta"),Ffr=o(" \u2014 "),GG=a("a"),Cfr=o("TFRobertaForTokenClassification"),Mfr=o(" (RoBERTa model)"),Efr=l(),j8=a("li"),Spe=a("strong"),yfr=o("roformer"),wfr=o(" \u2014 "),OG=a("a"),Afr=o("TFRoFormerForTokenClassification"),Lfr=o(" (RoFormer model)"),Bfr=l(),N8=a("li"),Ppe=a("strong"),kfr=o("xlm"),xfr=o(" \u2014 "),XG=a("a"),Rfr=o("TFXLMForTokenClassification"),Sfr=o(" (XLM model)"),Pfr=l(),D8=a("li"),$pe=a("strong"),$fr=o("xlm-roberta"),Ifr=o(" \u2014 "),zG=a("a"),jfr=o("TFXLMRobertaForTokenClassification"),Nfr=o(" (XLM-RoBERTa model)"),Dfr=l(),q8=a("li"),Ipe=a("strong"),qfr=o("xlnet"),Gfr=o(" \u2014 "),VG=a("a"),Ofr=o("TFXLNetForTokenClassification"),Xfr=o(" (XLNet model)"),zfr=l(),jpe=a("p"),Vfr=o("Examples:"),Wfr=l(),f(Uw.$$.fragment),O9e=l(),xc=a("h2"),G8=a("a"),Npe=a("span"),f(Jw.$$.fragment),Qfr=l(),Dpe=a("span"),Hfr=o("TFAutoModelForQuestionAnswering"),X9e=l(),Mr=a("div"),f(Yw.$$.fragment),Ufr=l(),Rc=a("p"),Jfr=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a question answering head) when created
with the `),qpe=a("code"),Yfr=o("from_pretrained()"),Kfr=o("class method or the "),Gpe=a("code"),Zfr=o("from_config()"),emr=o(`class
method.`),omr=l(),Kw=a("p"),rmr=o("This class cannot be instantiated directly using "),Ope=a("code"),tmr=o("__init__()"),amr=o(" (throws an error)."),nmr=l(),_t=a("div"),f(Zw.$$.fragment),smr=l(),Xpe=a("p"),lmr=o("Instantiates one of the model classes of the library (with a question answering head) from a configuration."),imr=l(),Sc=a("p"),dmr=o(`Note:
Loading a model from its configuration file does `),zpe=a("strong"),cmr=o("not"),fmr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Vpe=a("code"),mmr=o("from_pretrained()"),gmr=o("to load the model weights."),hmr=l(),Wpe=a("p"),pmr=o("Examples:"),_mr=l(),f(eA.$$.fragment),umr=l(),Mo=a("div"),f(oA.$$.fragment),bmr=l(),Qpe=a("p"),vmr=o("Instantiate one of the model classes of the library (with a question answering head) from a pretrained model."),Tmr=l(),bn=a("p"),Fmr=o("The model class to instantiate is selected based on the "),Hpe=a("code"),Cmr=o("model_type"),Mmr=o(` property of the config object (either
passed as an argument or loaded from `),Upe=a("code"),Emr=o("pretrained_model_name_or_path"),ymr=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Jpe=a("code"),wmr=o("pretrained_model_name_or_path"),Amr=o(":"),Lmr=l(),Z=a("ul"),O8=a("li"),Ype=a("strong"),Bmr=o("albert"),kmr=o(" \u2014 "),WG=a("a"),xmr=o("TFAlbertForQuestionAnswering"),Rmr=o(" (ALBERT model)"),Smr=l(),X8=a("li"),Kpe=a("strong"),Pmr=o("bert"),$mr=o(" \u2014 "),QG=a("a"),Imr=o("TFBertForQuestionAnswering"),jmr=o(" (BERT model)"),Nmr=l(),z8=a("li"),Zpe=a("strong"),Dmr=o("camembert"),qmr=o(" \u2014 "),HG=a("a"),Gmr=o("TFCamembertForQuestionAnswering"),Omr=o(" (CamemBERT model)"),Xmr=l(),V8=a("li"),e_e=a("strong"),zmr=o("convbert"),Vmr=o(" \u2014 "),UG=a("a"),Wmr=o("TFConvBertForQuestionAnswering"),Qmr=o(" (ConvBERT model)"),Hmr=l(),W8=a("li"),o_e=a("strong"),Umr=o("deberta"),Jmr=o(" \u2014 "),JG=a("a"),Ymr=o("TFDebertaForQuestionAnswering"),Kmr=o(" (DeBERTa model)"),Zmr=l(),Q8=a("li"),r_e=a("strong"),egr=o("deberta-v2"),ogr=o(" \u2014 "),YG=a("a"),rgr=o("TFDebertaV2ForQuestionAnswering"),tgr=o(" (DeBERTa-v2 model)"),agr=l(),H8=a("li"),t_e=a("strong"),ngr=o("distilbert"),sgr=o(" \u2014 "),KG=a("a"),lgr=o("TFDistilBertForQuestionAnswering"),igr=o(" (DistilBERT model)"),dgr=l(),U8=a("li"),a_e=a("strong"),cgr=o("electra"),fgr=o(" \u2014 "),ZG=a("a"),mgr=o("TFElectraForQuestionAnswering"),ggr=o(" (ELECTRA model)"),hgr=l(),J8=a("li"),n_e=a("strong"),pgr=o("flaubert"),_gr=o(" \u2014 "),eO=a("a"),ugr=o("TFFlaubertForQuestionAnsweringSimple"),bgr=o(" (FlauBERT model)"),vgr=l(),Y8=a("li"),s_e=a("strong"),Tgr=o("funnel"),Fgr=o(" \u2014 "),oO=a("a"),Cgr=o("TFFunnelForQuestionAnswering"),Mgr=o(" (Funnel Transformer model)"),Egr=l(),K8=a("li"),l_e=a("strong"),ygr=o("longformer"),wgr=o(" \u2014 "),rO=a("a"),Agr=o("TFLongformerForQuestionAnswering"),Lgr=o(" (Longformer model)"),Bgr=l(),Z8=a("li"),i_e=a("strong"),kgr=o("mobilebert"),xgr=o(" \u2014 "),tO=a("a"),Rgr=o("TFMobileBertForQuestionAnswering"),Sgr=o(" (MobileBERT model)"),Pgr=l(),eF=a("li"),d_e=a("strong"),$gr=o("mpnet"),Igr=o(" \u2014 "),aO=a("a"),jgr=o("TFMPNetForQuestionAnswering"),Ngr=o(" (MPNet model)"),Dgr=l(),oF=a("li"),c_e=a("strong"),qgr=o("rembert"),Ggr=o(" \u2014 "),nO=a("a"),Ogr=o("TFRemBertForQuestionAnswering"),Xgr=o(" (RemBERT model)"),zgr=l(),rF=a("li"),f_e=a("strong"),Vgr=o("roberta"),Wgr=o(" \u2014 "),sO=a("a"),Qgr=o("TFRobertaForQuestionAnswering"),Hgr=o(" (RoBERTa model)"),Ugr=l(),tF=a("li"),m_e=a("strong"),Jgr=o("roformer"),Ygr=o(" \u2014 "),lO=a("a"),Kgr=o("TFRoFormerForQuestionAnswering"),Zgr=o(" (RoFormer model)"),ehr=l(),aF=a("li"),g_e=a("strong"),ohr=o("xlm"),rhr=o(" \u2014 "),iO=a("a"),thr=o("TFXLMForQuestionAnsweringSimple"),ahr=o(" (XLM model)"),nhr=l(),nF=a("li"),h_e=a("strong"),shr=o("xlm-roberta"),lhr=o(" \u2014 "),dO=a("a"),ihr=o("TFXLMRobertaForQuestionAnswering"),dhr=o(" (XLM-RoBERTa model)"),chr=l(),sF=a("li"),p_e=a("strong"),fhr=o("xlnet"),mhr=o(" \u2014 "),cO=a("a"),ghr=o("TFXLNetForQuestionAnsweringSimple"),hhr=o(" (XLNet model)"),phr=l(),__e=a("p"),_hr=o("Examples:"),uhr=l(),f(rA.$$.fragment),z9e=l(),Pc=a("h2"),lF=a("a"),u_e=a("span"),f(tA.$$.fragment),bhr=l(),b_e=a("span"),vhr=o("TFAutoModelForVision2Seq"),V9e=l(),Er=a("div"),f(aA.$$.fragment),Thr=l(),$c=a("p"),Fhr=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a vision-to-text modeling head) when created
with the `),v_e=a("code"),Chr=o("from_pretrained()"),Mhr=o("class method or the "),T_e=a("code"),Ehr=o("from_config()"),yhr=o(`class
method.`),whr=l(),nA=a("p"),Ahr=o("This class cannot be instantiated directly using "),F_e=a("code"),Lhr=o("__init__()"),Bhr=o(" (throws an error)."),khr=l(),ut=a("div"),f(sA.$$.fragment),xhr=l(),C_e=a("p"),Rhr=o("Instantiates one of the model classes of the library (with a vision-to-text modeling head) from a configuration."),Shr=l(),Ic=a("p"),Phr=o(`Note:
Loading a model from its configuration file does `),M_e=a("strong"),$hr=o("not"),Ihr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),E_e=a("code"),jhr=o("from_pretrained()"),Nhr=o("to load the model weights."),Dhr=l(),y_e=a("p"),qhr=o("Examples:"),Ghr=l(),f(lA.$$.fragment),Ohr=l(),Eo=a("div"),f(iA.$$.fragment),Xhr=l(),w_e=a("p"),zhr=o("Instantiate one of the model classes of the library (with a vision-to-text modeling head) from a pretrained model."),Vhr=l(),vn=a("p"),Whr=o("The model class to instantiate is selected based on the "),A_e=a("code"),Qhr=o("model_type"),Hhr=o(` property of the config object (either
passed as an argument or loaded from `),L_e=a("code"),Uhr=o("pretrained_model_name_or_path"),Jhr=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),B_e=a("code"),Yhr=o("pretrained_model_name_or_path"),Khr=o(":"),Zhr=l(),k_e=a("ul"),iF=a("li"),x_e=a("strong"),epr=o("vision-encoder-decoder"),opr=o(" \u2014 "),fO=a("a"),rpr=o("TFVisionEncoderDecoderModel"),tpr=o(" (Vision Encoder decoder model)"),apr=l(),R_e=a("p"),npr=o("Examples:"),spr=l(),f(dA.$$.fragment),W9e=l(),jc=a("h2"),dF=a("a"),S_e=a("span"),f(cA.$$.fragment),lpr=l(),P_e=a("span"),ipr=o("TFAutoModelForSpeechSeq2Seq"),Q9e=l(),yr=a("div"),f(fA.$$.fragment),dpr=l(),Nc=a("p"),cpr=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) when created
with the `),$_e=a("code"),fpr=o("from_pretrained()"),mpr=o("class method or the "),I_e=a("code"),gpr=o("from_config()"),hpr=o(`class
method.`),ppr=l(),mA=a("p"),_pr=o("This class cannot be instantiated directly using "),j_e=a("code"),upr=o("__init__()"),bpr=o(" (throws an error)."),vpr=l(),bt=a("div"),f(gA.$$.fragment),Tpr=l(),N_e=a("p"),Fpr=o("Instantiates one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) from a configuration."),Cpr=l(),Dc=a("p"),Mpr=o(`Note:
Loading a model from its configuration file does `),D_e=a("strong"),Epr=o("not"),ypr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),q_e=a("code"),wpr=o("from_pretrained()"),Apr=o("to load the model weights."),Lpr=l(),G_e=a("p"),Bpr=o("Examples:"),kpr=l(),f(hA.$$.fragment),xpr=l(),yo=a("div"),f(pA.$$.fragment),Rpr=l(),O_e=a("p"),Spr=o("Instantiate one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) from a pretrained model."),Ppr=l(),Tn=a("p"),$pr=o("The model class to instantiate is selected based on the "),X_e=a("code"),Ipr=o("model_type"),jpr=o(` property of the config object (either
passed as an argument or loaded from `),z_e=a("code"),Npr=o("pretrained_model_name_or_path"),Dpr=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),V_e=a("code"),qpr=o("pretrained_model_name_or_path"),Gpr=o(":"),Opr=l(),W_e=a("ul"),cF=a("li"),Q_e=a("strong"),Xpr=o("speech_to_text"),zpr=o(" \u2014 "),mO=a("a"),Vpr=o("TFSpeech2TextForConditionalGeneration"),Wpr=o(" (Speech2Text model)"),Qpr=l(),H_e=a("p"),Hpr=o("Examples:"),Upr=l(),f(_A.$$.fragment),H9e=l(),qc=a("h2"),fF=a("a"),U_e=a("span"),f(uA.$$.fragment),Jpr=l(),J_e=a("span"),Ypr=o("FlaxAutoModel"),U9e=l(),wr=a("div"),f(bA.$$.fragment),Kpr=l(),Gc=a("p"),Zpr=o(`This is a generic model class that will be instantiated as one of the base model classes of the library when created
with the `),Y_e=a("code"),e_r=o("from_pretrained()"),o_r=o("class method or the "),K_e=a("code"),r_r=o("from_config()"),t_r=o(`class
method.`),a_r=l(),vA=a("p"),n_r=o("This class cannot be instantiated directly using "),Z_e=a("code"),s_r=o("__init__()"),l_r=o(" (throws an error)."),i_r=l(),vt=a("div"),f(TA.$$.fragment),d_r=l(),eue=a("p"),c_r=o("Instantiates one of the base model classes of the library from a configuration."),f_r=l(),Oc=a("p"),m_r=o(`Note:
Loading a model from its configuration file does `),oue=a("strong"),g_r=o("not"),h_r=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),rue=a("code"),p_r=o("from_pretrained()"),__r=o("to load the model weights."),u_r=l(),tue=a("p"),b_r=o("Examples:"),v_r=l(),f(FA.$$.fragment),T_r=l(),wo=a("div"),f(CA.$$.fragment),F_r=l(),aue=a("p"),C_r=o("Instantiate one of the base model classes of the library from a pretrained model."),M_r=l(),Fn=a("p"),E_r=o("The model class to instantiate is selected based on the "),nue=a("code"),y_r=o("model_type"),w_r=o(` property of the config object (either
passed as an argument or loaded from `),sue=a("code"),A_r=o("pretrained_model_name_or_path"),L_r=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),lue=a("code"),B_r=o("pretrained_model_name_or_path"),k_r=o(":"),x_r=l(),V=a("ul"),mF=a("li"),iue=a("strong"),R_r=o("albert"),S_r=o(" \u2014 "),gO=a("a"),P_r=o("FlaxAlbertModel"),$_r=o(" (ALBERT model)"),I_r=l(),gF=a("li"),due=a("strong"),j_r=o("bart"),N_r=o(" \u2014 "),hO=a("a"),D_r=o("FlaxBartModel"),q_r=o(" (BART model)"),G_r=l(),hF=a("li"),cue=a("strong"),O_r=o("beit"),X_r=o(" \u2014 "),pO=a("a"),z_r=o("FlaxBeitModel"),V_r=o(" (BEiT model)"),W_r=l(),pF=a("li"),fue=a("strong"),Q_r=o("bert"),H_r=o(" \u2014 "),_O=a("a"),U_r=o("FlaxBertModel"),J_r=o(" (BERT model)"),Y_r=l(),_F=a("li"),mue=a("strong"),K_r=o("big_bird"),Z_r=o(" \u2014 "),uO=a("a"),eur=o("FlaxBigBirdModel"),our=o(" (BigBird model)"),rur=l(),uF=a("li"),gue=a("strong"),tur=o("blenderbot"),aur=o(" \u2014 "),bO=a("a"),nur=o("FlaxBlenderbotModel"),sur=o(" (Blenderbot model)"),lur=l(),bF=a("li"),hue=a("strong"),iur=o("blenderbot-small"),dur=o(" \u2014 "),vO=a("a"),cur=o("FlaxBlenderbotSmallModel"),fur=o(" (BlenderbotSmall model)"),mur=l(),vF=a("li"),pue=a("strong"),gur=o("clip"),hur=o(" \u2014 "),TO=a("a"),pur=o("FlaxCLIPModel"),_ur=o(" (CLIP model)"),uur=l(),TF=a("li"),_ue=a("strong"),bur=o("distilbert"),vur=o(" \u2014 "),FO=a("a"),Tur=o("FlaxDistilBertModel"),Fur=o(" (DistilBERT model)"),Cur=l(),FF=a("li"),uue=a("strong"),Mur=o("electra"),Eur=o(" \u2014 "),CO=a("a"),yur=o("FlaxElectraModel"),wur=o(" (ELECTRA model)"),Aur=l(),CF=a("li"),bue=a("strong"),Lur=o("gpt2"),Bur=o(" \u2014 "),MO=a("a"),kur=o("FlaxGPT2Model"),xur=o(" (OpenAI GPT-2 model)"),Rur=l(),MF=a("li"),vue=a("strong"),Sur=o("gpt_neo"),Pur=o(" \u2014 "),EO=a("a"),$ur=o("FlaxGPTNeoModel"),Iur=o(" (GPT Neo model)"),jur=l(),EF=a("li"),Tue=a("strong"),Nur=o("gptj"),Dur=o(" \u2014 "),yO=a("a"),qur=o("FlaxGPTJModel"),Gur=o(" (GPT-J model)"),Our=l(),yF=a("li"),Fue=a("strong"),Xur=o("marian"),zur=o(" \u2014 "),wO=a("a"),Vur=o("FlaxMarianModel"),Wur=o(" (Marian model)"),Qur=l(),wF=a("li"),Cue=a("strong"),Hur=o("mbart"),Uur=o(" \u2014 "),AO=a("a"),Jur=o("FlaxMBartModel"),Yur=o(" (mBART model)"),Kur=l(),AF=a("li"),Mue=a("strong"),Zur=o("mt5"),e2r=o(" \u2014 "),LO=a("a"),o2r=o("FlaxMT5Model"),r2r=o(" (mT5 model)"),t2r=l(),LF=a("li"),Eue=a("strong"),a2r=o("pegasus"),n2r=o(" \u2014 "),BO=a("a"),s2r=o("FlaxPegasusModel"),l2r=o(" (Pegasus model)"),i2r=l(),BF=a("li"),yue=a("strong"),d2r=o("roberta"),c2r=o(" \u2014 "),kO=a("a"),f2r=o("FlaxRobertaModel"),m2r=o(" (RoBERTa model)"),g2r=l(),kF=a("li"),wue=a("strong"),h2r=o("roformer"),p2r=o(" \u2014 "),xO=a("a"),_2r=o("FlaxRoFormerModel"),u2r=o(" (RoFormer model)"),b2r=l(),xF=a("li"),Aue=a("strong"),v2r=o("t5"),T2r=o(" \u2014 "),RO=a("a"),F2r=o("FlaxT5Model"),C2r=o(" (T5 model)"),M2r=l(),RF=a("li"),Lue=a("strong"),E2r=o("vision-text-dual-encoder"),y2r=o(" \u2014 "),SO=a("a"),w2r=o("FlaxVisionTextDualEncoderModel"),A2r=o(" (VisionTextDualEncoder model)"),L2r=l(),SF=a("li"),Bue=a("strong"),B2r=o("vit"),k2r=o(" \u2014 "),PO=a("a"),x2r=o("FlaxViTModel"),R2r=o(" (ViT model)"),S2r=l(),PF=a("li"),kue=a("strong"),P2r=o("wav2vec2"),$2r=o(" \u2014 "),$O=a("a"),I2r=o("FlaxWav2Vec2Model"),j2r=o(" (Wav2Vec2 model)"),N2r=l(),$F=a("li"),xue=a("strong"),D2r=o("xglm"),q2r=o(" \u2014 "),IO=a("a"),G2r=o("FlaxXGLMModel"),O2r=o(" (XGLM model)"),X2r=l(),Rue=a("p"),z2r=o("Examples:"),V2r=l(),f(MA.$$.fragment),J9e=l(),Xc=a("h2"),IF=a("a"),Sue=a("span"),f(EA.$$.fragment),W2r=l(),Pue=a("span"),Q2r=o("FlaxAutoModelForCausalLM"),Y9e=l(),Ar=a("div"),f(yA.$$.fragment),H2r=l(),zc=a("p"),U2r=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a causal language modeling head) when created
with the `),$ue=a("code"),J2r=o("from_pretrained()"),Y2r=o("class method or the "),Iue=a("code"),K2r=o("from_config()"),Z2r=o(`class
method.`),e1r=l(),wA=a("p"),o1r=o("This class cannot be instantiated directly using "),jue=a("code"),r1r=o("__init__()"),t1r=o(" (throws an error)."),a1r=l(),Tt=a("div"),f(AA.$$.fragment),n1r=l(),Nue=a("p"),s1r=o("Instantiates one of the model classes of the library (with a causal language modeling head) from a configuration."),l1r=l(),Vc=a("p"),i1r=o(`Note:
Loading a model from its configuration file does `),Due=a("strong"),d1r=o("not"),c1r=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),que=a("code"),f1r=o("from_pretrained()"),m1r=o("to load the model weights."),g1r=l(),Gue=a("p"),h1r=o("Examples:"),p1r=l(),f(LA.$$.fragment),_1r=l(),Ao=a("div"),f(BA.$$.fragment),u1r=l(),Oue=a("p"),b1r=o("Instantiate one of the model classes of the library (with a causal language modeling head) from a pretrained model."),v1r=l(),Cn=a("p"),T1r=o("The model class to instantiate is selected based on the "),Xue=a("code"),F1r=o("model_type"),C1r=o(` property of the config object (either
passed as an argument or loaded from `),zue=a("code"),M1r=o("pretrained_model_name_or_path"),E1r=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Vue=a("code"),y1r=o("pretrained_model_name_or_path"),w1r=o(":"),A1r=l(),Mn=a("ul"),jF=a("li"),Wue=a("strong"),L1r=o("gpt2"),B1r=o(" \u2014 "),jO=a("a"),k1r=o("FlaxGPT2LMHeadModel"),x1r=o(" (OpenAI GPT-2 model)"),R1r=l(),NF=a("li"),Que=a("strong"),S1r=o("gpt_neo"),P1r=o(" \u2014 "),NO=a("a"),$1r=o("FlaxGPTNeoForCausalLM"),I1r=o(" (GPT Neo model)"),j1r=l(),DF=a("li"),Hue=a("strong"),N1r=o("gptj"),D1r=o(" \u2014 "),DO=a("a"),q1r=o("FlaxGPTJForCausalLM"),G1r=o(" (GPT-J model)"),O1r=l(),qF=a("li"),Uue=a("strong"),X1r=o("xglm"),z1r=o(" \u2014 "),qO=a("a"),V1r=o("FlaxXGLMForCausalLM"),W1r=o(" (XGLM model)"),Q1r=l(),Jue=a("p"),H1r=o("Examples:"),U1r=l(),f(kA.$$.fragment),K9e=l(),Wc=a("h2"),GF=a("a"),Yue=a("span"),f(xA.$$.fragment),J1r=l(),Kue=a("span"),Y1r=o("FlaxAutoModelForPreTraining"),Z9e=l(),Lr=a("div"),f(RA.$$.fragment),K1r=l(),Qc=a("p"),Z1r=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a pretraining head) when created
with the `),Zue=a("code"),ebr=o("from_pretrained()"),obr=o("class method or the "),e2e=a("code"),rbr=o("from_config()"),tbr=o(`class
method.`),abr=l(),SA=a("p"),nbr=o("This class cannot be instantiated directly using "),o2e=a("code"),sbr=o("__init__()"),lbr=o(" (throws an error)."),ibr=l(),Ft=a("div"),f(PA.$$.fragment),dbr=l(),r2e=a("p"),cbr=o("Instantiates one of the model classes of the library (with a pretraining head) from a configuration."),fbr=l(),Hc=a("p"),mbr=o(`Note:
Loading a model from its configuration file does `),t2e=a("strong"),gbr=o("not"),hbr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),a2e=a("code"),pbr=o("from_pretrained()"),_br=o("to load the model weights."),ubr=l(),n2e=a("p"),bbr=o("Examples:"),vbr=l(),f($A.$$.fragment),Tbr=l(),Lo=a("div"),f(IA.$$.fragment),Fbr=l(),s2e=a("p"),Cbr=o("Instantiate one of the model classes of the library (with a pretraining head) from a pretrained model."),Mbr=l(),En=a("p"),Ebr=o("The model class to instantiate is selected based on the "),l2e=a("code"),ybr=o("model_type"),wbr=o(` property of the config object (either
passed as an argument or loaded from `),i2e=a("code"),Abr=o("pretrained_model_name_or_path"),Lbr=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),d2e=a("code"),Bbr=o("pretrained_model_name_or_path"),kbr=o(":"),xbr=l(),fe=a("ul"),OF=a("li"),c2e=a("strong"),Rbr=o("albert"),Sbr=o(" \u2014 "),GO=a("a"),Pbr=o("FlaxAlbertForPreTraining"),$br=o(" (ALBERT model)"),Ibr=l(),XF=a("li"),f2e=a("strong"),jbr=o("bart"),Nbr=o(" \u2014 "),OO=a("a"),Dbr=o("FlaxBartForConditionalGeneration"),qbr=o(" (BART model)"),Gbr=l(),zF=a("li"),m2e=a("strong"),Obr=o("bert"),Xbr=o(" \u2014 "),XO=a("a"),zbr=o("FlaxBertForPreTraining"),Vbr=o(" (BERT model)"),Wbr=l(),VF=a("li"),g2e=a("strong"),Qbr=o("big_bird"),Hbr=o(" \u2014 "),zO=a("a"),Ubr=o("FlaxBigBirdForPreTraining"),Jbr=o(" (BigBird model)"),Ybr=l(),WF=a("li"),h2e=a("strong"),Kbr=o("electra"),Zbr=o(" \u2014 "),VO=a("a"),e5r=o("FlaxElectraForPreTraining"),o5r=o(" (ELECTRA model)"),r5r=l(),QF=a("li"),p2e=a("strong"),t5r=o("mbart"),a5r=o(" \u2014 "),WO=a("a"),n5r=o("FlaxMBartForConditionalGeneration"),s5r=o(" (mBART model)"),l5r=l(),HF=a("li"),_2e=a("strong"),i5r=o("mt5"),d5r=o(" \u2014 "),QO=a("a"),c5r=o("FlaxMT5ForConditionalGeneration"),f5r=o(" (mT5 model)"),m5r=l(),UF=a("li"),u2e=a("strong"),g5r=o("roberta"),h5r=o(" \u2014 "),HO=a("a"),p5r=o("FlaxRobertaForMaskedLM"),_5r=o(" (RoBERTa model)"),u5r=l(),JF=a("li"),b2e=a("strong"),b5r=o("roformer"),v5r=o(" \u2014 "),UO=a("a"),T5r=o("FlaxRoFormerForMaskedLM"),F5r=o(" (RoFormer model)"),C5r=l(),YF=a("li"),v2e=a("strong"),M5r=o("t5"),E5r=o(" \u2014 "),JO=a("a"),y5r=o("FlaxT5ForConditionalGeneration"),w5r=o(" (T5 model)"),A5r=l(),KF=a("li"),T2e=a("strong"),L5r=o("wav2vec2"),B5r=o(" \u2014 "),YO=a("a"),k5r=o("FlaxWav2Vec2ForPreTraining"),x5r=o(" (Wav2Vec2 model)"),R5r=l(),F2e=a("p"),S5r=o("Examples:"),P5r=l(),f(jA.$$.fragment),eBe=l(),Uc=a("h2"),ZF=a("a"),C2e=a("span"),f(NA.$$.fragment),$5r=l(),M2e=a("span"),I5r=o("FlaxAutoModelForMaskedLM"),oBe=l(),Br=a("div"),f(DA.$$.fragment),j5r=l(),Jc=a("p"),N5r=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a masked language modeling head) when created
with the `),E2e=a("code"),D5r=o("from_pretrained()"),q5r=o("class method or the "),y2e=a("code"),G5r=o("from_config()"),O5r=o(`class
method.`),X5r=l(),qA=a("p"),z5r=o("This class cannot be instantiated directly using "),w2e=a("code"),V5r=o("__init__()"),W5r=o(" (throws an error)."),Q5r=l(),Ct=a("div"),f(GA.$$.fragment),H5r=l(),A2e=a("p"),U5r=o("Instantiates one of the model classes of the library (with a masked language modeling head) from a configuration."),J5r=l(),Yc=a("p"),Y5r=o(`Note:
Loading a model from its configuration file does `),L2e=a("strong"),K5r=o("not"),Z5r=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),B2e=a("code"),evr=o("from_pretrained()"),ovr=o("to load the model weights."),rvr=l(),k2e=a("p"),tvr=o("Examples:"),avr=l(),f(OA.$$.fragment),nvr=l(),Bo=a("div"),f(XA.$$.fragment),svr=l(),x2e=a("p"),lvr=o("Instantiate one of the model classes of the library (with a masked language modeling head) from a pretrained model."),ivr=l(),yn=a("p"),dvr=o("The model class to instantiate is selected based on the "),R2e=a("code"),cvr=o("model_type"),fvr=o(` property of the config object (either
passed as an argument or loaded from `),S2e=a("code"),mvr=o("pretrained_model_name_or_path"),gvr=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),P2e=a("code"),hvr=o("pretrained_model_name_or_path"),pvr=o(":"),_vr=l(),ve=a("ul"),eC=a("li"),$2e=a("strong"),uvr=o("albert"),bvr=o(" \u2014 "),KO=a("a"),vvr=o("FlaxAlbertForMaskedLM"),Tvr=o(" (ALBERT model)"),Fvr=l(),oC=a("li"),I2e=a("strong"),Cvr=o("bart"),Mvr=o(" \u2014 "),ZO=a("a"),Evr=o("FlaxBartForConditionalGeneration"),yvr=o(" (BART model)"),wvr=l(),rC=a("li"),j2e=a("strong"),Avr=o("bert"),Lvr=o(" \u2014 "),eX=a("a"),Bvr=o("FlaxBertForMaskedLM"),kvr=o(" (BERT model)"),xvr=l(),tC=a("li"),N2e=a("strong"),Rvr=o("big_bird"),Svr=o(" \u2014 "),oX=a("a"),Pvr=o("FlaxBigBirdForMaskedLM"),$vr=o(" (BigBird model)"),Ivr=l(),aC=a("li"),D2e=a("strong"),jvr=o("distilbert"),Nvr=o(" \u2014 "),rX=a("a"),Dvr=o("FlaxDistilBertForMaskedLM"),qvr=o(" (DistilBERT model)"),Gvr=l(),nC=a("li"),q2e=a("strong"),Ovr=o("electra"),Xvr=o(" \u2014 "),tX=a("a"),zvr=o("FlaxElectraForMaskedLM"),Vvr=o(" (ELECTRA model)"),Wvr=l(),sC=a("li"),G2e=a("strong"),Qvr=o("mbart"),Hvr=o(" \u2014 "),aX=a("a"),Uvr=o("FlaxMBartForConditionalGeneration"),Jvr=o(" (mBART model)"),Yvr=l(),lC=a("li"),O2e=a("strong"),Kvr=o("roberta"),Zvr=o(" \u2014 "),nX=a("a"),e6r=o("FlaxRobertaForMaskedLM"),o6r=o(" (RoBERTa model)"),r6r=l(),iC=a("li"),X2e=a("strong"),t6r=o("roformer"),a6r=o(" \u2014 "),sX=a("a"),n6r=o("FlaxRoFormerForMaskedLM"),s6r=o(" (RoFormer model)"),l6r=l(),z2e=a("p"),i6r=o("Examples:"),d6r=l(),f(zA.$$.fragment),rBe=l(),Kc=a("h2"),dC=a("a"),V2e=a("span"),f(VA.$$.fragment),c6r=l(),W2e=a("span"),f6r=o("FlaxAutoModelForSeq2SeqLM"),tBe=l(),kr=a("div"),f(WA.$$.fragment),m6r=l(),Zc=a("p"),g6r=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence language modeling head) when created
with the `),Q2e=a("code"),h6r=o("from_pretrained()"),p6r=o("class method or the "),H2e=a("code"),_6r=o("from_config()"),u6r=o(`class
method.`),b6r=l(),QA=a("p"),v6r=o("This class cannot be instantiated directly using "),U2e=a("code"),T6r=o("__init__()"),F6r=o(" (throws an error)."),C6r=l(),Mt=a("div"),f(HA.$$.fragment),M6r=l(),J2e=a("p"),E6r=o("Instantiates one of the model classes of the library (with a sequence-to-sequence language modeling head) from a configuration."),y6r=l(),ef=a("p"),w6r=o(`Note:
Loading a model from its configuration file does `),Y2e=a("strong"),A6r=o("not"),L6r=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),K2e=a("code"),B6r=o("from_pretrained()"),k6r=o("to load the model weights."),x6r=l(),Z2e=a("p"),R6r=o("Examples:"),S6r=l(),f(UA.$$.fragment),P6r=l(),ko=a("div"),f(JA.$$.fragment),$6r=l(),e1e=a("p"),I6r=o("Instantiate one of the model classes of the library (with a sequence-to-sequence language modeling head) from a pretrained model."),j6r=l(),wn=a("p"),N6r=o("The model class to instantiate is selected based on the "),o1e=a("code"),D6r=o("model_type"),q6r=o(` property of the config object (either
passed as an argument or loaded from `),r1e=a("code"),G6r=o("pretrained_model_name_or_path"),O6r=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),t1e=a("code"),X6r=o("pretrained_model_name_or_path"),z6r=o(":"),V6r=l(),Te=a("ul"),cC=a("li"),a1e=a("strong"),W6r=o("bart"),Q6r=o(" \u2014 "),lX=a("a"),H6r=o("FlaxBartForConditionalGeneration"),U6r=o(" (BART model)"),J6r=l(),fC=a("li"),n1e=a("strong"),Y6r=o("blenderbot"),K6r=o(" \u2014 "),iX=a("a"),Z6r=o("FlaxBlenderbotForConditionalGeneration"),eTr=o(" (Blenderbot model)"),oTr=l(),mC=a("li"),s1e=a("strong"),rTr=o("blenderbot-small"),tTr=o(" \u2014 "),dX=a("a"),aTr=o("FlaxBlenderbotSmallForConditionalGeneration"),nTr=o(" (BlenderbotSmall model)"),sTr=l(),gC=a("li"),l1e=a("strong"),lTr=o("encoder-decoder"),iTr=o(" \u2014 "),cX=a("a"),dTr=o("FlaxEncoderDecoderModel"),cTr=o(" (Encoder decoder model)"),fTr=l(),hC=a("li"),i1e=a("strong"),mTr=o("marian"),gTr=o(" \u2014 "),fX=a("a"),hTr=o("FlaxMarianMTModel"),pTr=o(" (Marian model)"),_Tr=l(),pC=a("li"),d1e=a("strong"),uTr=o("mbart"),bTr=o(" \u2014 "),mX=a("a"),vTr=o("FlaxMBartForConditionalGeneration"),TTr=o(" (mBART model)"),FTr=l(),_C=a("li"),c1e=a("strong"),CTr=o("mt5"),MTr=o(" \u2014 "),gX=a("a"),ETr=o("FlaxMT5ForConditionalGeneration"),yTr=o(" (mT5 model)"),wTr=l(),uC=a("li"),f1e=a("strong"),ATr=o("pegasus"),LTr=o(" \u2014 "),hX=a("a"),BTr=o("FlaxPegasusForConditionalGeneration"),kTr=o(" (Pegasus model)"),xTr=l(),bC=a("li"),m1e=a("strong"),RTr=o("t5"),STr=o(" \u2014 "),pX=a("a"),PTr=o("FlaxT5ForConditionalGeneration"),$Tr=o(" (T5 model)"),ITr=l(),g1e=a("p"),jTr=o("Examples:"),NTr=l(),f(YA.$$.fragment),aBe=l(),of=a("h2"),vC=a("a"),h1e=a("span"),f(KA.$$.fragment),DTr=l(),p1e=a("span"),qTr=o("FlaxAutoModelForSequenceClassification"),nBe=l(),xr=a("div"),f(ZA.$$.fragment),GTr=l(),rf=a("p"),OTr=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence classification head) when created
with the `),_1e=a("code"),XTr=o("from_pretrained()"),zTr=o("class method or the "),u1e=a("code"),VTr=o("from_config()"),WTr=o(`class
method.`),QTr=l(),e0=a("p"),HTr=o("This class cannot be instantiated directly using "),b1e=a("code"),UTr=o("__init__()"),JTr=o(" (throws an error)."),YTr=l(),Et=a("div"),f(o0.$$.fragment),KTr=l(),v1e=a("p"),ZTr=o("Instantiates one of the model classes of the library (with a sequence classification head) from a configuration."),e8r=l(),tf=a("p"),o8r=o(`Note:
Loading a model from its configuration file does `),T1e=a("strong"),r8r=o("not"),t8r=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),F1e=a("code"),a8r=o("from_pretrained()"),n8r=o("to load the model weights."),s8r=l(),C1e=a("p"),l8r=o("Examples:"),i8r=l(),f(r0.$$.fragment),d8r=l(),xo=a("div"),f(t0.$$.fragment),c8r=l(),M1e=a("p"),f8r=o("Instantiate one of the model classes of the library (with a sequence classification head) from a pretrained model."),m8r=l(),An=a("p"),g8r=o("The model class to instantiate is selected based on the "),E1e=a("code"),h8r=o("model_type"),p8r=o(` property of the config object (either
passed as an argument or loaded from `),y1e=a("code"),_8r=o("pretrained_model_name_or_path"),u8r=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),w1e=a("code"),b8r=o("pretrained_model_name_or_path"),v8r=o(":"),T8r=l(),Fe=a("ul"),TC=a("li"),A1e=a("strong"),F8r=o("albert"),C8r=o(" \u2014 "),_X=a("a"),M8r=o("FlaxAlbertForSequenceClassification"),E8r=o(" (ALBERT model)"),y8r=l(),FC=a("li"),L1e=a("strong"),w8r=o("bart"),A8r=o(" \u2014 "),uX=a("a"),L8r=o("FlaxBartForSequenceClassification"),B8r=o(" (BART model)"),k8r=l(),CC=a("li"),B1e=a("strong"),x8r=o("bert"),R8r=o(" \u2014 "),bX=a("a"),S8r=o("FlaxBertForSequenceClassification"),P8r=o(" (BERT model)"),$8r=l(),MC=a("li"),k1e=a("strong"),I8r=o("big_bird"),j8r=o(" \u2014 "),vX=a("a"),N8r=o("FlaxBigBirdForSequenceClassification"),D8r=o(" (BigBird model)"),q8r=l(),EC=a("li"),x1e=a("strong"),G8r=o("distilbert"),O8r=o(" \u2014 "),TX=a("a"),X8r=o("FlaxDistilBertForSequenceClassification"),z8r=o(" (DistilBERT model)"),V8r=l(),yC=a("li"),R1e=a("strong"),W8r=o("electra"),Q8r=o(" \u2014 "),FX=a("a"),H8r=o("FlaxElectraForSequenceClassification"),U8r=o(" (ELECTRA model)"),J8r=l(),wC=a("li"),S1e=a("strong"),Y8r=o("mbart"),K8r=o(" \u2014 "),CX=a("a"),Z8r=o("FlaxMBartForSequenceClassification"),eFr=o(" (mBART model)"),oFr=l(),AC=a("li"),P1e=a("strong"),rFr=o("roberta"),tFr=o(" \u2014 "),MX=a("a"),aFr=o("FlaxRobertaForSequenceClassification"),nFr=o(" (RoBERTa model)"),sFr=l(),LC=a("li"),$1e=a("strong"),lFr=o("roformer"),iFr=o(" \u2014 "),EX=a("a"),dFr=o("FlaxRoFormerForSequenceClassification"),cFr=o(" (RoFormer model)"),fFr=l(),I1e=a("p"),mFr=o("Examples:"),gFr=l(),f(a0.$$.fragment),sBe=l(),af=a("h2"),BC=a("a"),j1e=a("span"),f(n0.$$.fragment),hFr=l(),N1e=a("span"),pFr=o("FlaxAutoModelForQuestionAnswering"),lBe=l(),Rr=a("div"),f(s0.$$.fragment),_Fr=l(),nf=a("p"),uFr=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a question answering head) when created
with the `),D1e=a("code"),bFr=o("from_pretrained()"),vFr=o("class method or the "),q1e=a("code"),TFr=o("from_config()"),FFr=o(`class
method.`),CFr=l(),l0=a("p"),MFr=o("This class cannot be instantiated directly using "),G1e=a("code"),EFr=o("__init__()"),yFr=o(" (throws an error)."),wFr=l(),yt=a("div"),f(i0.$$.fragment),AFr=l(),O1e=a("p"),LFr=o("Instantiates one of the model classes of the library (with a question answering head) from a configuration."),BFr=l(),sf=a("p"),kFr=o(`Note:
Loading a model from its configuration file does `),X1e=a("strong"),xFr=o("not"),RFr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),z1e=a("code"),SFr=o("from_pretrained()"),PFr=o("to load the model weights."),$Fr=l(),V1e=a("p"),IFr=o("Examples:"),jFr=l(),f(d0.$$.fragment),NFr=l(),Ro=a("div"),f(c0.$$.fragment),DFr=l(),W1e=a("p"),qFr=o("Instantiate one of the model classes of the library (with a question answering head) from a pretrained model."),GFr=l(),Ln=a("p"),OFr=o("The model class to instantiate is selected based on the "),Q1e=a("code"),XFr=o("model_type"),zFr=o(` property of the config object (either
passed as an argument or loaded from `),H1e=a("code"),VFr=o("pretrained_model_name_or_path"),WFr=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),U1e=a("code"),QFr=o("pretrained_model_name_or_path"),HFr=o(":"),UFr=l(),Ce=a("ul"),kC=a("li"),J1e=a("strong"),JFr=o("albert"),YFr=o(" \u2014 "),yX=a("a"),KFr=o("FlaxAlbertForQuestionAnswering"),ZFr=o(" (ALBERT model)"),eCr=l(),xC=a("li"),Y1e=a("strong"),oCr=o("bart"),rCr=o(" \u2014 "),wX=a("a"),tCr=o("FlaxBartForQuestionAnswering"),aCr=o(" (BART model)"),nCr=l(),RC=a("li"),K1e=a("strong"),sCr=o("bert"),lCr=o(" \u2014 "),AX=a("a"),iCr=o("FlaxBertForQuestionAnswering"),dCr=o(" (BERT model)"),cCr=l(),SC=a("li"),Z1e=a("strong"),fCr=o("big_bird"),mCr=o(" \u2014 "),LX=a("a"),gCr=o("FlaxBigBirdForQuestionAnswering"),hCr=o(" (BigBird model)"),pCr=l(),PC=a("li"),ebe=a("strong"),_Cr=o("distilbert"),uCr=o(" \u2014 "),BX=a("a"),bCr=o("FlaxDistilBertForQuestionAnswering"),vCr=o(" (DistilBERT model)"),TCr=l(),$C=a("li"),obe=a("strong"),FCr=o("electra"),CCr=o(" \u2014 "),kX=a("a"),MCr=o("FlaxElectraForQuestionAnswering"),ECr=o(" (ELECTRA model)"),yCr=l(),IC=a("li"),rbe=a("strong"),wCr=o("mbart"),ACr=o(" \u2014 "),xX=a("a"),LCr=o("FlaxMBartForQuestionAnswering"),BCr=o(" (mBART model)"),kCr=l(),jC=a("li"),tbe=a("strong"),xCr=o("roberta"),RCr=o(" \u2014 "),RX=a("a"),SCr=o("FlaxRobertaForQuestionAnswering"),PCr=o(" (RoBERTa model)"),$Cr=l(),NC=a("li"),abe=a("strong"),ICr=o("roformer"),jCr=o(" \u2014 "),SX=a("a"),NCr=o("FlaxRoFormerForQuestionAnswering"),DCr=o(" (RoFormer model)"),qCr=l(),nbe=a("p"),GCr=o("Examples:"),OCr=l(),f(f0.$$.fragment),iBe=l(),lf=a("h2"),DC=a("a"),sbe=a("span"),f(m0.$$.fragment),XCr=l(),lbe=a("span"),zCr=o("FlaxAutoModelForTokenClassification"),dBe=l(),Sr=a("div"),f(g0.$$.fragment),VCr=l(),df=a("p"),WCr=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a token classification head) when created
with the `),ibe=a("code"),QCr=o("from_pretrained()"),HCr=o("class method or the "),dbe=a("code"),UCr=o("from_config()"),JCr=o(`class
method.`),YCr=l(),h0=a("p"),KCr=o("This class cannot be instantiated directly using "),cbe=a("code"),ZCr=o("__init__()"),e4r=o(" (throws an error)."),o4r=l(),wt=a("div"),f(p0.$$.fragment),r4r=l(),fbe=a("p"),t4r=o("Instantiates one of the model classes of the library (with a token classification head) from a configuration."),a4r=l(),cf=a("p"),n4r=o(`Note:
Loading a model from its configuration file does `),mbe=a("strong"),s4r=o("not"),l4r=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),gbe=a("code"),i4r=o("from_pretrained()"),d4r=o("to load the model weights."),c4r=l(),hbe=a("p"),f4r=o("Examples:"),m4r=l(),f(_0.$$.fragment),g4r=l(),So=a("div"),f(u0.$$.fragment),h4r=l(),pbe=a("p"),p4r=o("Instantiate one of the model classes of the library (with a token classification head) from a pretrained model."),_4r=l(),Bn=a("p"),u4r=o("The model class to instantiate is selected based on the "),_be=a("code"),b4r=o("model_type"),v4r=o(` property of the config object (either
passed as an argument or loaded from `),ube=a("code"),T4r=o("pretrained_model_name_or_path"),F4r=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),bbe=a("code"),C4r=o("pretrained_model_name_or_path"),M4r=o(":"),E4r=l(),so=a("ul"),qC=a("li"),vbe=a("strong"),y4r=o("albert"),w4r=o(" \u2014 "),PX=a("a"),A4r=o("FlaxAlbertForTokenClassification"),L4r=o(" (ALBERT model)"),B4r=l(),GC=a("li"),Tbe=a("strong"),k4r=o("bert"),x4r=o(" \u2014 "),$X=a("a"),R4r=o("FlaxBertForTokenClassification"),S4r=o(" (BERT model)"),P4r=l(),OC=a("li"),Fbe=a("strong"),$4r=o("big_bird"),I4r=o(" \u2014 "),IX=a("a"),j4r=o("FlaxBigBirdForTokenClassification"),N4r=o(" (BigBird model)"),D4r=l(),XC=a("li"),Cbe=a("strong"),q4r=o("distilbert"),G4r=o(" \u2014 "),jX=a("a"),O4r=o("FlaxDistilBertForTokenClassification"),X4r=o(" (DistilBERT model)"),z4r=l(),zC=a("li"),Mbe=a("strong"),V4r=o("electra"),W4r=o(" \u2014 "),NX=a("a"),Q4r=o("FlaxElectraForTokenClassification"),H4r=o(" (ELECTRA model)"),U4r=l(),VC=a("li"),Ebe=a("strong"),J4r=o("roberta"),Y4r=o(" \u2014 "),DX=a("a"),K4r=o("FlaxRobertaForTokenClassification"),Z4r=o(" (RoBERTa model)"),eMr=l(),WC=a("li"),ybe=a("strong"),oMr=o("roformer"),rMr=o(" \u2014 "),qX=a("a"),tMr=o("FlaxRoFormerForTokenClassification"),aMr=o(" (RoFormer model)"),nMr=l(),wbe=a("p"),sMr=o("Examples:"),lMr=l(),f(b0.$$.fragment),cBe=l(),ff=a("h2"),QC=a("a"),Abe=a("span"),f(v0.$$.fragment),iMr=l(),Lbe=a("span"),dMr=o("FlaxAutoModelForMultipleChoice"),fBe=l(),Pr=a("div"),f(T0.$$.fragment),cMr=l(),mf=a("p"),fMr=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a multiple choice head) when created
with the `),Bbe=a("code"),mMr=o("from_pretrained()"),gMr=o("class method or the "),kbe=a("code"),hMr=o("from_config()"),pMr=o(`class
method.`),_Mr=l(),F0=a("p"),uMr=o("This class cannot be instantiated directly using "),xbe=a("code"),bMr=o("__init__()"),vMr=o(" (throws an error)."),TMr=l(),At=a("div"),f(C0.$$.fragment),FMr=l(),Rbe=a("p"),CMr=o("Instantiates one of the model classes of the library (with a multiple choice head) from a configuration."),MMr=l(),gf=a("p"),EMr=o(`Note:
Loading a model from its configuration file does `),Sbe=a("strong"),yMr=o("not"),wMr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Pbe=a("code"),AMr=o("from_pretrained()"),LMr=o("to load the model weights."),BMr=l(),$be=a("p"),kMr=o("Examples:"),xMr=l(),f(M0.$$.fragment),RMr=l(),Po=a("div"),f(E0.$$.fragment),SMr=l(),Ibe=a("p"),PMr=o("Instantiate one of the model classes of the library (with a multiple choice head) from a pretrained model."),$Mr=l(),kn=a("p"),IMr=o("The model class to instantiate is selected based on the "),jbe=a("code"),jMr=o("model_type"),NMr=o(` property of the config object (either
passed as an argument or loaded from `),Nbe=a("code"),DMr=o("pretrained_model_name_or_path"),qMr=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Dbe=a("code"),GMr=o("pretrained_model_name_or_path"),OMr=o(":"),XMr=l(),lo=a("ul"),HC=a("li"),qbe=a("strong"),zMr=o("albert"),VMr=o(" \u2014 "),GX=a("a"),WMr=o("FlaxAlbertForMultipleChoice"),QMr=o(" (ALBERT model)"),HMr=l(),UC=a("li"),Gbe=a("strong"),UMr=o("bert"),JMr=o(" \u2014 "),OX=a("a"),YMr=o("FlaxBertForMultipleChoice"),KMr=o(" (BERT model)"),ZMr=l(),JC=a("li"),Obe=a("strong"),eEr=o("big_bird"),oEr=o(" \u2014 "),XX=a("a"),rEr=o("FlaxBigBirdForMultipleChoice"),tEr=o(" (BigBird model)"),aEr=l(),YC=a("li"),Xbe=a("strong"),nEr=o("distilbert"),sEr=o(" \u2014 "),zX=a("a"),lEr=o("FlaxDistilBertForMultipleChoice"),iEr=o(" (DistilBERT model)"),dEr=l(),KC=a("li"),zbe=a("strong"),cEr=o("electra"),fEr=o(" \u2014 "),VX=a("a"),mEr=o("FlaxElectraForMultipleChoice"),gEr=o(" (ELECTRA model)"),hEr=l(),ZC=a("li"),Vbe=a("strong"),pEr=o("roberta"),_Er=o(" \u2014 "),WX=a("a"),uEr=o("FlaxRobertaForMultipleChoice"),bEr=o(" (RoBERTa model)"),vEr=l(),e4=a("li"),Wbe=a("strong"),TEr=o("roformer"),FEr=o(" \u2014 "),QX=a("a"),CEr=o("FlaxRoFormerForMultipleChoice"),MEr=o(" (RoFormer model)"),EEr=l(),Qbe=a("p"),yEr=o("Examples:"),wEr=l(),f(y0.$$.fragment),mBe=l(),hf=a("h2"),o4=a("a"),Hbe=a("span"),f(w0.$$.fragment),AEr=l(),Ube=a("span"),LEr=o("FlaxAutoModelForNextSentencePrediction"),gBe=l(),$r=a("div"),f(A0.$$.fragment),BEr=l(),pf=a("p"),kEr=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a next sentence prediction head) when created
with the `),Jbe=a("code"),xEr=o("from_pretrained()"),REr=o("class method or the "),Ybe=a("code"),SEr=o("from_config()"),PEr=o(`class
method.`),$Er=l(),L0=a("p"),IEr=o("This class cannot be instantiated directly using "),Kbe=a("code"),jEr=o("__init__()"),NEr=o(" (throws an error)."),DEr=l(),Lt=a("div"),f(B0.$$.fragment),qEr=l(),Zbe=a("p"),GEr=o("Instantiates one of the model classes of the library (with a next sentence prediction head) from a configuration."),OEr=l(),_f=a("p"),XEr=o(`Note:
Loading a model from its configuration file does `),e5e=a("strong"),zEr=o("not"),VEr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),o5e=a("code"),WEr=o("from_pretrained()"),QEr=o("to load the model weights."),HEr=l(),r5e=a("p"),UEr=o("Examples:"),JEr=l(),f(k0.$$.fragment),YEr=l(),$o=a("div"),f(x0.$$.fragment),KEr=l(),t5e=a("p"),ZEr=o("Instantiate one of the model classes of the library (with a next sentence prediction head) from a pretrained model."),e3r=l(),xn=a("p"),o3r=o("The model class to instantiate is selected based on the "),a5e=a("code"),r3r=o("model_type"),t3r=o(` property of the config object (either
passed as an argument or loaded from `),n5e=a("code"),a3r=o("pretrained_model_name_or_path"),n3r=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),s5e=a("code"),s3r=o("pretrained_model_name_or_path"),l3r=o(":"),i3r=l(),l5e=a("ul"),r4=a("li"),i5e=a("strong"),d3r=o("bert"),c3r=o(" \u2014 "),HX=a("a"),f3r=o("FlaxBertForNextSentencePrediction"),m3r=o(" (BERT model)"),g3r=l(),d5e=a("p"),h3r=o("Examples:"),p3r=l(),f(R0.$$.fragment),hBe=l(),uf=a("h2"),t4=a("a"),c5e=a("span"),f(S0.$$.fragment),_3r=l(),f5e=a("span"),u3r=o("FlaxAutoModelForImageClassification"),pBe=l(),Ir=a("div"),f(P0.$$.fragment),b3r=l(),bf=a("p"),v3r=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a image classification head) when created
with the `),m5e=a("code"),T3r=o("from_pretrained()"),F3r=o("class method or the "),g5e=a("code"),C3r=o("from_config()"),M3r=o(`class
method.`),E3r=l(),$0=a("p"),y3r=o("This class cannot be instantiated directly using "),h5e=a("code"),w3r=o("__init__()"),A3r=o(" (throws an error)."),L3r=l(),Bt=a("div"),f(I0.$$.fragment),B3r=l(),p5e=a("p"),k3r=o("Instantiates one of the model classes of the library (with a image classification head) from a configuration."),x3r=l(),vf=a("p"),R3r=o(`Note:
Loading a model from its configuration file does `),_5e=a("strong"),S3r=o("not"),P3r=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),u5e=a("code"),$3r=o("from_pretrained()"),I3r=o("to load the model weights."),j3r=l(),b5e=a("p"),N3r=o("Examples:"),D3r=l(),f(j0.$$.fragment),q3r=l(),Io=a("div"),f(N0.$$.fragment),G3r=l(),v5e=a("p"),O3r=o("Instantiate one of the model classes of the library (with a image classification head) from a pretrained model."),X3r=l(),Rn=a("p"),z3r=o("The model class to instantiate is selected based on the "),T5e=a("code"),V3r=o("model_type"),W3r=o(` property of the config object (either
passed as an argument or loaded from `),F5e=a("code"),Q3r=o("pretrained_model_name_or_path"),H3r=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),C5e=a("code"),U3r=o("pretrained_model_name_or_path"),J3r=o(":"),Y3r=l(),D0=a("ul"),a4=a("li"),M5e=a("strong"),K3r=o("beit"),Z3r=o(" \u2014 "),UX=a("a"),eyr=o("FlaxBeitForImageClassification"),oyr=o(" (BEiT model)"),ryr=l(),n4=a("li"),E5e=a("strong"),tyr=o("vit"),ayr=o(" \u2014 "),JX=a("a"),nyr=o("FlaxViTForImageClassification"),syr=o(" (ViT model)"),lyr=l(),y5e=a("p"),iyr=o("Examples:"),dyr=l(),f(q0.$$.fragment),_Be=l(),Tf=a("h2"),s4=a("a"),w5e=a("span"),f(G0.$$.fragment),cyr=l(),A5e=a("span"),fyr=o("FlaxAutoModelForVision2Seq"),uBe=l(),jr=a("div"),f(O0.$$.fragment),myr=l(),Ff=a("p"),gyr=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a vision-to-text modeling head) when created
with the `),L5e=a("code"),hyr=o("from_pretrained()"),pyr=o("class method or the "),B5e=a("code"),_yr=o("from_config()"),uyr=o(`class
method.`),byr=l(),X0=a("p"),vyr=o("This class cannot be instantiated directly using "),k5e=a("code"),Tyr=o("__init__()"),Fyr=o(" (throws an error)."),Cyr=l(),kt=a("div"),f(z0.$$.fragment),Myr=l(),x5e=a("p"),Eyr=o("Instantiates one of the model classes of the library (with a vision-to-text modeling head) from a configuration."),yyr=l(),Cf=a("p"),wyr=o(`Note:
Loading a model from its configuration file does `),R5e=a("strong"),Ayr=o("not"),Lyr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),S5e=a("code"),Byr=o("from_pretrained()"),kyr=o("to load the model weights."),xyr=l(),P5e=a("p"),Ryr=o("Examples:"),Syr=l(),f(V0.$$.fragment),Pyr=l(),jo=a("div"),f(W0.$$.fragment),$yr=l(),$5e=a("p"),Iyr=o("Instantiate one of the model classes of the library (with a vision-to-text modeling head) from a pretrained model."),jyr=l(),Sn=a("p"),Nyr=o("The model class to instantiate is selected based on the "),I5e=a("code"),Dyr=o("model_type"),qyr=o(` property of the config object (either
passed as an argument or loaded from `),j5e=a("code"),Gyr=o("pretrained_model_name_or_path"),Oyr=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),N5e=a("code"),Xyr=o("pretrained_model_name_or_path"),zyr=o(":"),Vyr=l(),D5e=a("ul"),l4=a("li"),q5e=a("strong"),Wyr=o("vision-encoder-decoder"),Qyr=o(" \u2014 "),YX=a("a"),Hyr=o("FlaxVisionEncoderDecoderModel"),Uyr=o(" (Vision Encoder decoder model)"),Jyr=l(),G5e=a("p"),Yyr=o("Examples:"),Kyr=l(),f(Q0.$$.fragment),this.h()},l(d){const u=cut('[data-svelte="svelte-1phssyn"]',document.head);J=n(u,"META",{name:!0,content:!0}),u.forEach(t),Ae=i(d),ie=n(d,"H1",{class:!0});var H0=s(ie);me=n(H0,"A",{id:!0,class:!0,href:!0});var O5e=s(me);to=n(O5e,"SPAN",{});var X5e=s(to);m(ce.$$.fragment,X5e),X5e.forEach(t),O5e.forEach(t),ue=i(H0),Do=n(H0,"SPAN",{});var ewr=s(Do);wi=r(ewr,"Auto Classes"),ewr.forEach(t),H0.forEach(t),Ef=i(d),sa=n(d,"P",{});var vBe=s(sa);Ai=r(vBe,`In many cases, the architecture you want to use can be guessed from the name or the path of the pretrained model you
are supplying to the `),Li=n(vBe,"CODE",{});var owr=s(Li);nM=r(owr,"from_pretrained()"),owr.forEach(t),yf=r(vBe,` method. AutoClasses are here to do this job for you so that you
automatically retrieve the relevant model given the name/path to the pretrained weights/config/vocabulary.`),vBe.forEach(t),ye=i(d),io=n(d,"P",{});var i4=s(io);Bi=r(i4,"Instantiating one of "),Pn=n(i4,"A",{href:!0});var rwr=s(Pn);sM=r(rwr,"AutoConfig"),rwr.forEach(t),$n=r(i4,", "),In=n(i4,"A",{href:!0});var twr=s(In);lM=r(twr,"AutoModel"),twr.forEach(t),ki=r(i4,`, and
`),jn=n(i4,"A",{href:!0});var awr=s(jn);iM=r(awr,"AutoTokenizer"),awr.forEach(t),xi=r(i4," will directly create a class of the relevant architecture. For instance"),i4.forEach(t),wf=i(d),m($a.$$.fragment,d),co=i(d),ge=n(d,"P",{});var TBe=s(ge);zL=r(TBe,"will create a model that is an instance of "),Ri=n(TBe,"A",{href:!0});var nwr=s(Ri);VL=r(nwr,"BertModel"),nwr.forEach(t),WL=r(TBe,"."),TBe.forEach(t),qo=i(d),Ia=n(d,"P",{});var FBe=s(Ia);QL=r(FBe,"There is one class of "),Af=n(FBe,"CODE",{});var swr=s(Af);HL=r(swr,"AutoModel"),swr.forEach(t),Bxe=r(FBe," for each task, and for each backend (PyTorch, TensorFlow, or Flax)."),FBe.forEach(t),v7e=i(d),Si=n(d,"H2",{class:!0});var CBe=s(Si);Lf=n(CBe,"A",{id:!0,class:!0,href:!0});var lwr=s(Lf);zV=n(lwr,"SPAN",{});var iwr=s(zV);m(dM.$$.fragment,iwr),iwr.forEach(t),lwr.forEach(t),kxe=i(CBe),VV=n(CBe,"SPAN",{});var dwr=s(VV);xxe=r(dwr,"Extending the Auto Classes"),dwr.forEach(t),CBe.forEach(t),T7e=i(d),Nn=n(d,"P",{});var KX=s(Nn);Rxe=r(KX,`Each of the auto classes has a method to be extended with your custom classes. For instance, if you have defined a
custom class of model `),WV=n(KX,"CODE",{});var cwr=s(WV);Sxe=r(cwr,"NewModel"),cwr.forEach(t),Pxe=r(KX,", make sure you have a "),QV=n(KX,"CODE",{});var fwr=s(QV);$xe=r(fwr,"NewModelConfig"),fwr.forEach(t),Ixe=r(KX,` then you can add those to the auto
classes like this:`),KX.forEach(t),F7e=i(d),m(cM.$$.fragment,d),C7e=i(d),UL=n(d,"P",{});var mwr=s(UL);jxe=r(mwr,"You will then be able to use the auto classes like you would usually do!"),mwr.forEach(t),M7e=i(d),m(Bf.$$.fragment,d),E7e=i(d),Pi=n(d,"H2",{class:!0});var MBe=s(Pi);kf=n(MBe,"A",{id:!0,class:!0,href:!0});var gwr=s(kf);HV=n(gwr,"SPAN",{});var hwr=s(HV);m(fM.$$.fragment,hwr),hwr.forEach(t),gwr.forEach(t),Nxe=i(MBe),UV=n(MBe,"SPAN",{});var pwr=s(UV);Dxe=r(pwr,"AutoConfig"),pwr.forEach(t),MBe.forEach(t),y7e=i(d),Go=n(d,"DIV",{class:!0});var Ps=s(Go);m(mM.$$.fragment,Ps),qxe=i(Ps),gM=n(Ps,"P",{});var EBe=s(gM);Gxe=r(EBe,`This is a generic configuration class that will be instantiated as one of the configuration classes of the library
when created with the `),JL=n(EBe,"A",{href:!0});var _wr=s(JL);Oxe=r(_wr,"from_pretrained()"),_wr.forEach(t),Xxe=r(EBe," class method."),EBe.forEach(t),zxe=i(Ps),hM=n(Ps,"P",{});var yBe=s(hM);Vxe=r(yBe,"This class cannot be instantiated directly using "),JV=n(yBe,"CODE",{});var uwr=s(JV);Wxe=r(uwr,"__init__()"),uwr.forEach(t),Qxe=r(yBe," (throws an error)."),yBe.forEach(t),Hxe=i(Ps),fo=n(Ps,"DIV",{class:!0});var ia=s(fo);m(pM.$$.fragment,ia),Uxe=i(ia),YV=n(ia,"P",{});var bwr=s(YV);Jxe=r(bwr,"Instantiate one of the configuration classes of the library from a pretrained model configuration."),bwr.forEach(t),Yxe=i(ia),$i=n(ia,"P",{});var ZX=s($i);Kxe=r(ZX,"The configuration class to instantiate is selected based on the "),KV=n(ZX,"CODE",{});var vwr=s(KV);Zxe=r(vwr,"model_type"),vwr.forEach(t),eRe=r(ZX,` property of the config object that
is loaded, or when it\u2019s missing, by falling back to using pattern matching on `),ZV=n(ZX,"CODE",{});var Twr=s(ZV);oRe=r(Twr,"pretrained_model_name_or_path"),Twr.forEach(t),rRe=r(ZX,":"),ZX.forEach(t),tRe=i(ia),v=n(ia,"UL",{});var T=s(v);xf=n(T,"LI",{});var z5e=s(xf);eW=n(z5e,"STRONG",{});var Fwr=s(eW);aRe=r(Fwr,"albert"),Fwr.forEach(t),nRe=r(z5e," \u2014 "),YL=n(z5e,"A",{href:!0});var Cwr=s(YL);sRe=r(Cwr,"AlbertConfig"),Cwr.forEach(t),lRe=r(z5e," (ALBERT model)"),z5e.forEach(t),iRe=i(T),Rf=n(T,"LI",{});var V5e=s(Rf);oW=n(V5e,"STRONG",{});var Mwr=s(oW);dRe=r(Mwr,"bart"),Mwr.forEach(t),cRe=r(V5e," \u2014 "),KL=n(V5e,"A",{href:!0});var Ewr=s(KL);fRe=r(Ewr,"BartConfig"),Ewr.forEach(t),mRe=r(V5e," (BART model)"),V5e.forEach(t),gRe=i(T),Sf=n(T,"LI",{});var W5e=s(Sf);rW=n(W5e,"STRONG",{});var ywr=s(rW);hRe=r(ywr,"beit"),ywr.forEach(t),pRe=r(W5e," \u2014 "),ZL=n(W5e,"A",{href:!0});var wwr=s(ZL);_Re=r(wwr,"BeitConfig"),wwr.forEach(t),uRe=r(W5e," (BEiT model)"),W5e.forEach(t),bRe=i(T),Pf=n(T,"LI",{});var Q5e=s(Pf);tW=n(Q5e,"STRONG",{});var Awr=s(tW);vRe=r(Awr,"bert"),Awr.forEach(t),TRe=r(Q5e," \u2014 "),e7=n(Q5e,"A",{href:!0});var Lwr=s(e7);FRe=r(Lwr,"BertConfig"),Lwr.forEach(t),CRe=r(Q5e," (BERT model)"),Q5e.forEach(t),MRe=i(T),$f=n(T,"LI",{});var H5e=s($f);aW=n(H5e,"STRONG",{});var Bwr=s(aW);ERe=r(Bwr,"bert-generation"),Bwr.forEach(t),yRe=r(H5e," \u2014 "),o7=n(H5e,"A",{href:!0});var kwr=s(o7);wRe=r(kwr,"BertGenerationConfig"),kwr.forEach(t),ARe=r(H5e," (Bert Generation model)"),H5e.forEach(t),LRe=i(T),If=n(T,"LI",{});var U5e=s(If);nW=n(U5e,"STRONG",{});var xwr=s(nW);BRe=r(xwr,"big_bird"),xwr.forEach(t),kRe=r(U5e," \u2014 "),r7=n(U5e,"A",{href:!0});var Rwr=s(r7);xRe=r(Rwr,"BigBirdConfig"),Rwr.forEach(t),RRe=r(U5e," (BigBird model)"),U5e.forEach(t),SRe=i(T),jf=n(T,"LI",{});var J5e=s(jf);sW=n(J5e,"STRONG",{});var Swr=s(sW);PRe=r(Swr,"bigbird_pegasus"),Swr.forEach(t),$Re=r(J5e," \u2014 "),t7=n(J5e,"A",{href:!0});var Pwr=s(t7);IRe=r(Pwr,"BigBirdPegasusConfig"),Pwr.forEach(t),jRe=r(J5e," (BigBirdPegasus model)"),J5e.forEach(t),NRe=i(T),Nf=n(T,"LI",{});var Y5e=s(Nf);lW=n(Y5e,"STRONG",{});var $wr=s(lW);DRe=r($wr,"blenderbot"),$wr.forEach(t),qRe=r(Y5e," \u2014 "),a7=n(Y5e,"A",{href:!0});var Iwr=s(a7);GRe=r(Iwr,"BlenderbotConfig"),Iwr.forEach(t),ORe=r(Y5e," (Blenderbot model)"),Y5e.forEach(t),XRe=i(T),Df=n(T,"LI",{});var K5e=s(Df);iW=n(K5e,"STRONG",{});var jwr=s(iW);zRe=r(jwr,"blenderbot-small"),jwr.forEach(t),VRe=r(K5e," \u2014 "),n7=n(K5e,"A",{href:!0});var Nwr=s(n7);WRe=r(Nwr,"BlenderbotSmallConfig"),Nwr.forEach(t),QRe=r(K5e," (BlenderbotSmall model)"),K5e.forEach(t),HRe=i(T),qf=n(T,"LI",{});var Z5e=s(qf);dW=n(Z5e,"STRONG",{});var Dwr=s(dW);URe=r(Dwr,"camembert"),Dwr.forEach(t),JRe=r(Z5e," \u2014 "),s7=n(Z5e,"A",{href:!0});var qwr=s(s7);YRe=r(qwr,"CamembertConfig"),qwr.forEach(t),KRe=r(Z5e," (CamemBERT model)"),Z5e.forEach(t),ZRe=i(T),Gf=n(T,"LI",{});var eve=s(Gf);cW=n(eve,"STRONG",{});var Gwr=s(cW);eSe=r(Gwr,"canine"),Gwr.forEach(t),oSe=r(eve," \u2014 "),l7=n(eve,"A",{href:!0});var Owr=s(l7);rSe=r(Owr,"CanineConfig"),Owr.forEach(t),tSe=r(eve," (Canine model)"),eve.forEach(t),aSe=i(T),Of=n(T,"LI",{});var ove=s(Of);fW=n(ove,"STRONG",{});var Xwr=s(fW);nSe=r(Xwr,"clip"),Xwr.forEach(t),sSe=r(ove," \u2014 "),i7=n(ove,"A",{href:!0});var zwr=s(i7);lSe=r(zwr,"CLIPConfig"),zwr.forEach(t),iSe=r(ove," (CLIP model)"),ove.forEach(t),dSe=i(T),Xf=n(T,"LI",{});var rve=s(Xf);mW=n(rve,"STRONG",{});var Vwr=s(mW);cSe=r(Vwr,"convbert"),Vwr.forEach(t),fSe=r(rve," \u2014 "),d7=n(rve,"A",{href:!0});var Wwr=s(d7);mSe=r(Wwr,"ConvBertConfig"),Wwr.forEach(t),gSe=r(rve," (ConvBERT model)"),rve.forEach(t),hSe=i(T),zf=n(T,"LI",{});var tve=s(zf);gW=n(tve,"STRONG",{});var Qwr=s(gW);pSe=r(Qwr,"convnext"),Qwr.forEach(t),_Se=r(tve," \u2014 "),c7=n(tve,"A",{href:!0});var Hwr=s(c7);uSe=r(Hwr,"ConvNextConfig"),Hwr.forEach(t),bSe=r(tve," (ConvNext model)"),tve.forEach(t),vSe=i(T),Vf=n(T,"LI",{});var ave=s(Vf);hW=n(ave,"STRONG",{});var Uwr=s(hW);TSe=r(Uwr,"ctrl"),Uwr.forEach(t),FSe=r(ave," \u2014 "),f7=n(ave,"A",{href:!0});var Jwr=s(f7);CSe=r(Jwr,"CTRLConfig"),Jwr.forEach(t),MSe=r(ave," (CTRL model)"),ave.forEach(t),ESe=i(T),Wf=n(T,"LI",{});var nve=s(Wf);pW=n(nve,"STRONG",{});var Ywr=s(pW);ySe=r(Ywr,"deberta"),Ywr.forEach(t),wSe=r(nve," \u2014 "),m7=n(nve,"A",{href:!0});var Kwr=s(m7);ASe=r(Kwr,"DebertaConfig"),Kwr.forEach(t),LSe=r(nve," (DeBERTa model)"),nve.forEach(t),BSe=i(T),Qf=n(T,"LI",{});var sve=s(Qf);_W=n(sve,"STRONG",{});var Zwr=s(_W);kSe=r(Zwr,"deberta-v2"),Zwr.forEach(t),xSe=r(sve," \u2014 "),g7=n(sve,"A",{href:!0});var eAr=s(g7);RSe=r(eAr,"DebertaV2Config"),eAr.forEach(t),SSe=r(sve," (DeBERTa-v2 model)"),sve.forEach(t),PSe=i(T),Hf=n(T,"LI",{});var lve=s(Hf);uW=n(lve,"STRONG",{});var oAr=s(uW);$Se=r(oAr,"deit"),oAr.forEach(t),ISe=r(lve," \u2014 "),h7=n(lve,"A",{href:!0});var rAr=s(h7);jSe=r(rAr,"DeiTConfig"),rAr.forEach(t),NSe=r(lve," (DeiT model)"),lve.forEach(t),DSe=i(T),Uf=n(T,"LI",{});var ive=s(Uf);bW=n(ive,"STRONG",{});var tAr=s(bW);qSe=r(tAr,"detr"),tAr.forEach(t),GSe=r(ive," \u2014 "),p7=n(ive,"A",{href:!0});var aAr=s(p7);OSe=r(aAr,"DetrConfig"),aAr.forEach(t),XSe=r(ive," (DETR model)"),ive.forEach(t),zSe=i(T),Jf=n(T,"LI",{});var dve=s(Jf);vW=n(dve,"STRONG",{});var nAr=s(vW);VSe=r(nAr,"distilbert"),nAr.forEach(t),WSe=r(dve," \u2014 "),_7=n(dve,"A",{href:!0});var sAr=s(_7);QSe=r(sAr,"DistilBertConfig"),sAr.forEach(t),HSe=r(dve," (DistilBERT model)"),dve.forEach(t),USe=i(T),Yf=n(T,"LI",{});var cve=s(Yf);TW=n(cve,"STRONG",{});var lAr=s(TW);JSe=r(lAr,"dpr"),lAr.forEach(t),YSe=r(cve," \u2014 "),u7=n(cve,"A",{href:!0});var iAr=s(u7);KSe=r(iAr,"DPRConfig"),iAr.forEach(t),ZSe=r(cve," (DPR model)"),cve.forEach(t),ePe=i(T),Kf=n(T,"LI",{});var fve=s(Kf);FW=n(fve,"STRONG",{});var dAr=s(FW);oPe=r(dAr,"electra"),dAr.forEach(t),rPe=r(fve," \u2014 "),b7=n(fve,"A",{href:!0});var cAr=s(b7);tPe=r(cAr,"ElectraConfig"),cAr.forEach(t),aPe=r(fve," (ELECTRA model)"),fve.forEach(t),nPe=i(T),Zf=n(T,"LI",{});var mve=s(Zf);CW=n(mve,"STRONG",{});var fAr=s(CW);sPe=r(fAr,"encoder-decoder"),fAr.forEach(t),lPe=r(mve," \u2014 "),v7=n(mve,"A",{href:!0});var mAr=s(v7);iPe=r(mAr,"EncoderDecoderConfig"),mAr.forEach(t),dPe=r(mve," (Encoder decoder model)"),mve.forEach(t),cPe=i(T),em=n(T,"LI",{});var gve=s(em);MW=n(gve,"STRONG",{});var gAr=s(MW);fPe=r(gAr,"flaubert"),gAr.forEach(t),mPe=r(gve," \u2014 "),T7=n(gve,"A",{href:!0});var hAr=s(T7);gPe=r(hAr,"FlaubertConfig"),hAr.forEach(t),hPe=r(gve," (FlauBERT model)"),gve.forEach(t),pPe=i(T),om=n(T,"LI",{});var hve=s(om);EW=n(hve,"STRONG",{});var pAr=s(EW);_Pe=r(pAr,"fnet"),pAr.forEach(t),uPe=r(hve," \u2014 "),F7=n(hve,"A",{href:!0});var _Ar=s(F7);bPe=r(_Ar,"FNetConfig"),_Ar.forEach(t),vPe=r(hve," (FNet model)"),hve.forEach(t),TPe=i(T),rm=n(T,"LI",{});var pve=s(rm);yW=n(pve,"STRONG",{});var uAr=s(yW);FPe=r(uAr,"fsmt"),uAr.forEach(t),CPe=r(pve," \u2014 "),C7=n(pve,"A",{href:!0});var bAr=s(C7);MPe=r(bAr,"FSMTConfig"),bAr.forEach(t),EPe=r(pve," (FairSeq Machine-Translation model)"),pve.forEach(t),yPe=i(T),tm=n(T,"LI",{});var _ve=s(tm);wW=n(_ve,"STRONG",{});var vAr=s(wW);wPe=r(vAr,"funnel"),vAr.forEach(t),APe=r(_ve," \u2014 "),M7=n(_ve,"A",{href:!0});var TAr=s(M7);LPe=r(TAr,"FunnelConfig"),TAr.forEach(t),BPe=r(_ve," (Funnel Transformer model)"),_ve.forEach(t),kPe=i(T),am=n(T,"LI",{});var uve=s(am);AW=n(uve,"STRONG",{});var FAr=s(AW);xPe=r(FAr,"gpt2"),FAr.forEach(t),RPe=r(uve," \u2014 "),E7=n(uve,"A",{href:!0});var CAr=s(E7);SPe=r(CAr,"GPT2Config"),CAr.forEach(t),PPe=r(uve," (OpenAI GPT-2 model)"),uve.forEach(t),$Pe=i(T),nm=n(T,"LI",{});var bve=s(nm);LW=n(bve,"STRONG",{});var MAr=s(LW);IPe=r(MAr,"gpt_neo"),MAr.forEach(t),jPe=r(bve," \u2014 "),y7=n(bve,"A",{href:!0});var EAr=s(y7);NPe=r(EAr,"GPTNeoConfig"),EAr.forEach(t),DPe=r(bve," (GPT Neo model)"),bve.forEach(t),qPe=i(T),sm=n(T,"LI",{});var vve=s(sm);BW=n(vve,"STRONG",{});var yAr=s(BW);GPe=r(yAr,"gptj"),yAr.forEach(t),OPe=r(vve," \u2014 "),w7=n(vve,"A",{href:!0});var wAr=s(w7);XPe=r(wAr,"GPTJConfig"),wAr.forEach(t),zPe=r(vve," (GPT-J model)"),vve.forEach(t),VPe=i(T),lm=n(T,"LI",{});var Tve=s(lm);kW=n(Tve,"STRONG",{});var AAr=s(kW);WPe=r(AAr,"hubert"),AAr.forEach(t),QPe=r(Tve," \u2014 "),A7=n(Tve,"A",{href:!0});var LAr=s(A7);HPe=r(LAr,"HubertConfig"),LAr.forEach(t),UPe=r(Tve," (Hubert model)"),Tve.forEach(t),JPe=i(T),im=n(T,"LI",{});var Fve=s(im);xW=n(Fve,"STRONG",{});var BAr=s(xW);YPe=r(BAr,"ibert"),BAr.forEach(t),KPe=r(Fve," \u2014 "),L7=n(Fve,"A",{href:!0});var kAr=s(L7);ZPe=r(kAr,"IBertConfig"),kAr.forEach(t),e$e=r(Fve," (I-BERT model)"),Fve.forEach(t),o$e=i(T),dm=n(T,"LI",{});var Cve=s(dm);RW=n(Cve,"STRONG",{});var xAr=s(RW);r$e=r(xAr,"imagegpt"),xAr.forEach(t),t$e=r(Cve," \u2014 "),B7=n(Cve,"A",{href:!0});var RAr=s(B7);a$e=r(RAr,"ImageGPTConfig"),RAr.forEach(t),n$e=r(Cve," (ImageGPT model)"),Cve.forEach(t),s$e=i(T),cm=n(T,"LI",{});var Mve=s(cm);SW=n(Mve,"STRONG",{});var SAr=s(SW);l$e=r(SAr,"layoutlm"),SAr.forEach(t),i$e=r(Mve," \u2014 "),k7=n(Mve,"A",{href:!0});var PAr=s(k7);d$e=r(PAr,"LayoutLMConfig"),PAr.forEach(t),c$e=r(Mve," (LayoutLM model)"),Mve.forEach(t),f$e=i(T),fm=n(T,"LI",{});var Eve=s(fm);PW=n(Eve,"STRONG",{});var $Ar=s(PW);m$e=r($Ar,"layoutlmv2"),$Ar.forEach(t),g$e=r(Eve," \u2014 "),x7=n(Eve,"A",{href:!0});var IAr=s(x7);h$e=r(IAr,"LayoutLMv2Config"),IAr.forEach(t),p$e=r(Eve," (LayoutLMv2 model)"),Eve.forEach(t),_$e=i(T),mm=n(T,"LI",{});var yve=s(mm);$W=n(yve,"STRONG",{});var jAr=s($W);u$e=r(jAr,"led"),jAr.forEach(t),b$e=r(yve," \u2014 "),R7=n(yve,"A",{href:!0});var NAr=s(R7);v$e=r(NAr,"LEDConfig"),NAr.forEach(t),T$e=r(yve," (LED model)"),yve.forEach(t),F$e=i(T),gm=n(T,"LI",{});var wve=s(gm);IW=n(wve,"STRONG",{});var DAr=s(IW);C$e=r(DAr,"longformer"),DAr.forEach(t),M$e=r(wve," \u2014 "),S7=n(wve,"A",{href:!0});var qAr=s(S7);E$e=r(qAr,"LongformerConfig"),qAr.forEach(t),y$e=r(wve," (Longformer model)"),wve.forEach(t),w$e=i(T),hm=n(T,"LI",{});var Ave=s(hm);jW=n(Ave,"STRONG",{});var GAr=s(jW);A$e=r(GAr,"luke"),GAr.forEach(t),L$e=r(Ave," \u2014 "),P7=n(Ave,"A",{href:!0});var OAr=s(P7);B$e=r(OAr,"LukeConfig"),OAr.forEach(t),k$e=r(Ave," (LUKE model)"),Ave.forEach(t),x$e=i(T),pm=n(T,"LI",{});var Lve=s(pm);NW=n(Lve,"STRONG",{});var XAr=s(NW);R$e=r(XAr,"lxmert"),XAr.forEach(t),S$e=r(Lve," \u2014 "),$7=n(Lve,"A",{href:!0});var zAr=s($7);P$e=r(zAr,"LxmertConfig"),zAr.forEach(t),$$e=r(Lve," (LXMERT model)"),Lve.forEach(t),I$e=i(T),_m=n(T,"LI",{});var Bve=s(_m);DW=n(Bve,"STRONG",{});var VAr=s(DW);j$e=r(VAr,"m2m_100"),VAr.forEach(t),N$e=r(Bve," \u2014 "),I7=n(Bve,"A",{href:!0});var WAr=s(I7);D$e=r(WAr,"M2M100Config"),WAr.forEach(t),q$e=r(Bve," (M2M100 model)"),Bve.forEach(t),G$e=i(T),um=n(T,"LI",{});var kve=s(um);qW=n(kve,"STRONG",{});var QAr=s(qW);O$e=r(QAr,"marian"),QAr.forEach(t),X$e=r(kve," \u2014 "),j7=n(kve,"A",{href:!0});var HAr=s(j7);z$e=r(HAr,"MarianConfig"),HAr.forEach(t),V$e=r(kve," (Marian model)"),kve.forEach(t),W$e=i(T),bm=n(T,"LI",{});var xve=s(bm);GW=n(xve,"STRONG",{});var UAr=s(GW);Q$e=r(UAr,"maskformer"),UAr.forEach(t),H$e=r(xve," \u2014 "),N7=n(xve,"A",{href:!0});var JAr=s(N7);U$e=r(JAr,"MaskFormerConfig"),JAr.forEach(t),J$e=r(xve," (MaskFormer model)"),xve.forEach(t),Y$e=i(T),vm=n(T,"LI",{});var Rve=s(vm);OW=n(Rve,"STRONG",{});var YAr=s(OW);K$e=r(YAr,"mbart"),YAr.forEach(t),Z$e=r(Rve," \u2014 "),D7=n(Rve,"A",{href:!0});var KAr=s(D7);eIe=r(KAr,"MBartConfig"),KAr.forEach(t),oIe=r(Rve," (mBART model)"),Rve.forEach(t),rIe=i(T),Tm=n(T,"LI",{});var Sve=s(Tm);XW=n(Sve,"STRONG",{});var ZAr=s(XW);tIe=r(ZAr,"megatron-bert"),ZAr.forEach(t),aIe=r(Sve," \u2014 "),q7=n(Sve,"A",{href:!0});var e0r=s(q7);nIe=r(e0r,"MegatronBertConfig"),e0r.forEach(t),sIe=r(Sve," (MegatronBert model)"),Sve.forEach(t),lIe=i(T),Fm=n(T,"LI",{});var Pve=s(Fm);zW=n(Pve,"STRONG",{});var o0r=s(zW);iIe=r(o0r,"mobilebert"),o0r.forEach(t),dIe=r(Pve," \u2014 "),G7=n(Pve,"A",{href:!0});var r0r=s(G7);cIe=r(r0r,"MobileBertConfig"),r0r.forEach(t),fIe=r(Pve," (MobileBERT model)"),Pve.forEach(t),mIe=i(T),Cm=n(T,"LI",{});var $ve=s(Cm);VW=n($ve,"STRONG",{});var t0r=s(VW);gIe=r(t0r,"mpnet"),t0r.forEach(t),hIe=r($ve," \u2014 "),O7=n($ve,"A",{href:!0});var a0r=s(O7);pIe=r(a0r,"MPNetConfig"),a0r.forEach(t),_Ie=r($ve," (MPNet model)"),$ve.forEach(t),uIe=i(T),Mm=n(T,"LI",{});var Ive=s(Mm);WW=n(Ive,"STRONG",{});var n0r=s(WW);bIe=r(n0r,"mt5"),n0r.forEach(t),vIe=r(Ive," \u2014 "),X7=n(Ive,"A",{href:!0});var s0r=s(X7);TIe=r(s0r,"MT5Config"),s0r.forEach(t),FIe=r(Ive," (mT5 model)"),Ive.forEach(t),CIe=i(T),Em=n(T,"LI",{});var jve=s(Em);QW=n(jve,"STRONG",{});var l0r=s(QW);MIe=r(l0r,"nystromformer"),l0r.forEach(t),EIe=r(jve," \u2014 "),z7=n(jve,"A",{href:!0});var i0r=s(z7);yIe=r(i0r,"NystromformerConfig"),i0r.forEach(t),wIe=r(jve," (Nystromformer model)"),jve.forEach(t),AIe=i(T),ym=n(T,"LI",{});var Nve=s(ym);HW=n(Nve,"STRONG",{});var d0r=s(HW);LIe=r(d0r,"openai-gpt"),d0r.forEach(t),BIe=r(Nve," \u2014 "),V7=n(Nve,"A",{href:!0});var c0r=s(V7);kIe=r(c0r,"OpenAIGPTConfig"),c0r.forEach(t),xIe=r(Nve," (OpenAI GPT model)"),Nve.forEach(t),RIe=i(T),wm=n(T,"LI",{});var Dve=s(wm);UW=n(Dve,"STRONG",{});var f0r=s(UW);SIe=r(f0r,"pegasus"),f0r.forEach(t),PIe=r(Dve," \u2014 "),W7=n(Dve,"A",{href:!0});var m0r=s(W7);$Ie=r(m0r,"PegasusConfig"),m0r.forEach(t),IIe=r(Dve," (Pegasus model)"),Dve.forEach(t),jIe=i(T),Am=n(T,"LI",{});var qve=s(Am);JW=n(qve,"STRONG",{});var g0r=s(JW);NIe=r(g0r,"perceiver"),g0r.forEach(t),DIe=r(qve," \u2014 "),Q7=n(qve,"A",{href:!0});var h0r=s(Q7);qIe=r(h0r,"PerceiverConfig"),h0r.forEach(t),GIe=r(qve," (Perceiver model)"),qve.forEach(t),OIe=i(T),Lm=n(T,"LI",{});var Gve=s(Lm);YW=n(Gve,"STRONG",{});var p0r=s(YW);XIe=r(p0r,"plbart"),p0r.forEach(t),zIe=r(Gve," \u2014 "),H7=n(Gve,"A",{href:!0});var _0r=s(H7);VIe=r(_0r,"PLBartConfig"),_0r.forEach(t),WIe=r(Gve," (PLBart model)"),Gve.forEach(t),QIe=i(T),Bm=n(T,"LI",{});var Ove=s(Bm);KW=n(Ove,"STRONG",{});var u0r=s(KW);HIe=r(u0r,"poolformer"),u0r.forEach(t),UIe=r(Ove," \u2014 "),U7=n(Ove,"A",{href:!0});var b0r=s(U7);JIe=r(b0r,"PoolFormerConfig"),b0r.forEach(t),YIe=r(Ove," (PoolFormer model)"),Ove.forEach(t),KIe=i(T),km=n(T,"LI",{});var Xve=s(km);ZW=n(Xve,"STRONG",{});var v0r=s(ZW);ZIe=r(v0r,"prophetnet"),v0r.forEach(t),eje=r(Xve," \u2014 "),J7=n(Xve,"A",{href:!0});var T0r=s(J7);oje=r(T0r,"ProphetNetConfig"),T0r.forEach(t),rje=r(Xve," (ProphetNet model)"),Xve.forEach(t),tje=i(T),xm=n(T,"LI",{});var zve=s(xm);eQ=n(zve,"STRONG",{});var F0r=s(eQ);aje=r(F0r,"qdqbert"),F0r.forEach(t),nje=r(zve," \u2014 "),Y7=n(zve,"A",{href:!0});var C0r=s(Y7);sje=r(C0r,"QDQBertConfig"),C0r.forEach(t),lje=r(zve," (QDQBert model)"),zve.forEach(t),ije=i(T),Rm=n(T,"LI",{});var Vve=s(Rm);oQ=n(Vve,"STRONG",{});var M0r=s(oQ);dje=r(M0r,"rag"),M0r.forEach(t),cje=r(Vve," \u2014 "),K7=n(Vve,"A",{href:!0});var E0r=s(K7);fje=r(E0r,"RagConfig"),E0r.forEach(t),mje=r(Vve," (RAG model)"),Vve.forEach(t),gje=i(T),Sm=n(T,"LI",{});var Wve=s(Sm);rQ=n(Wve,"STRONG",{});var y0r=s(rQ);hje=r(y0r,"realm"),y0r.forEach(t),pje=r(Wve," \u2014 "),Z7=n(Wve,"A",{href:!0});var w0r=s(Z7);_je=r(w0r,"RealmConfig"),w0r.forEach(t),uje=r(Wve," (Realm model)"),Wve.forEach(t),bje=i(T),Pm=n(T,"LI",{});var Qve=s(Pm);tQ=n(Qve,"STRONG",{});var A0r=s(tQ);vje=r(A0r,"reformer"),A0r.forEach(t),Tje=r(Qve," \u2014 "),e9=n(Qve,"A",{href:!0});var L0r=s(e9);Fje=r(L0r,"ReformerConfig"),L0r.forEach(t),Cje=r(Qve," (Reformer model)"),Qve.forEach(t),Mje=i(T),$m=n(T,"LI",{});var Hve=s($m);aQ=n(Hve,"STRONG",{});var B0r=s(aQ);Eje=r(B0r,"rembert"),B0r.forEach(t),yje=r(Hve," \u2014 "),o9=n(Hve,"A",{href:!0});var k0r=s(o9);wje=r(k0r,"RemBertConfig"),k0r.forEach(t),Aje=r(Hve," (RemBERT model)"),Hve.forEach(t),Lje=i(T),Im=n(T,"LI",{});var Uve=s(Im);nQ=n(Uve,"STRONG",{});var x0r=s(nQ);Bje=r(x0r,"retribert"),x0r.forEach(t),kje=r(Uve," \u2014 "),r9=n(Uve,"A",{href:!0});var R0r=s(r9);xje=r(R0r,"RetriBertConfig"),R0r.forEach(t),Rje=r(Uve," (RetriBERT model)"),Uve.forEach(t),Sje=i(T),jm=n(T,"LI",{});var Jve=s(jm);sQ=n(Jve,"STRONG",{});var S0r=s(sQ);Pje=r(S0r,"roberta"),S0r.forEach(t),$je=r(Jve," \u2014 "),t9=n(Jve,"A",{href:!0});var P0r=s(t9);Ije=r(P0r,"RobertaConfig"),P0r.forEach(t),jje=r(Jve," (RoBERTa model)"),Jve.forEach(t),Nje=i(T),Nm=n(T,"LI",{});var Yve=s(Nm);lQ=n(Yve,"STRONG",{});var $0r=s(lQ);Dje=r($0r,"roformer"),$0r.forEach(t),qje=r(Yve," \u2014 "),a9=n(Yve,"A",{href:!0});var I0r=s(a9);Gje=r(I0r,"RoFormerConfig"),I0r.forEach(t),Oje=r(Yve," (RoFormer model)"),Yve.forEach(t),Xje=i(T),Dm=n(T,"LI",{});var Kve=s(Dm);iQ=n(Kve,"STRONG",{});var j0r=s(iQ);zje=r(j0r,"segformer"),j0r.forEach(t),Vje=r(Kve," \u2014 "),n9=n(Kve,"A",{href:!0});var N0r=s(n9);Wje=r(N0r,"SegformerConfig"),N0r.forEach(t),Qje=r(Kve," (SegFormer model)"),Kve.forEach(t),Hje=i(T),qm=n(T,"LI",{});var Zve=s(qm);dQ=n(Zve,"STRONG",{});var D0r=s(dQ);Uje=r(D0r,"sew"),D0r.forEach(t),Jje=r(Zve," \u2014 "),s9=n(Zve,"A",{href:!0});var q0r=s(s9);Yje=r(q0r,"SEWConfig"),q0r.forEach(t),Kje=r(Zve," (SEW model)"),Zve.forEach(t),Zje=i(T),Gm=n(T,"LI",{});var e6e=s(Gm);cQ=n(e6e,"STRONG",{});var G0r=s(cQ);eNe=r(G0r,"sew-d"),G0r.forEach(t),oNe=r(e6e," \u2014 "),l9=n(e6e,"A",{href:!0});var O0r=s(l9);rNe=r(O0r,"SEWDConfig"),O0r.forEach(t),tNe=r(e6e," (SEW-D model)"),e6e.forEach(t),aNe=i(T),Om=n(T,"LI",{});var o6e=s(Om);fQ=n(o6e,"STRONG",{});var X0r=s(fQ);nNe=r(X0r,"speech-encoder-decoder"),X0r.forEach(t),sNe=r(o6e," \u2014 "),i9=n(o6e,"A",{href:!0});var z0r=s(i9);lNe=r(z0r,"SpeechEncoderDecoderConfig"),z0r.forEach(t),iNe=r(o6e," (Speech Encoder decoder model)"),o6e.forEach(t),dNe=i(T),Xm=n(T,"LI",{});var r6e=s(Xm);mQ=n(r6e,"STRONG",{});var V0r=s(mQ);cNe=r(V0r,"speech_to_text"),V0r.forEach(t),fNe=r(r6e," \u2014 "),d9=n(r6e,"A",{href:!0});var W0r=s(d9);mNe=r(W0r,"Speech2TextConfig"),W0r.forEach(t),gNe=r(r6e," (Speech2Text model)"),r6e.forEach(t),hNe=i(T),zm=n(T,"LI",{});var t6e=s(zm);gQ=n(t6e,"STRONG",{});var Q0r=s(gQ);pNe=r(Q0r,"speech_to_text_2"),Q0r.forEach(t),_Ne=r(t6e," \u2014 "),c9=n(t6e,"A",{href:!0});var H0r=s(c9);uNe=r(H0r,"Speech2Text2Config"),H0r.forEach(t),bNe=r(t6e," (Speech2Text2 model)"),t6e.forEach(t),vNe=i(T),Vm=n(T,"LI",{});var a6e=s(Vm);hQ=n(a6e,"STRONG",{});var U0r=s(hQ);TNe=r(U0r,"splinter"),U0r.forEach(t),FNe=r(a6e," \u2014 "),f9=n(a6e,"A",{href:!0});var J0r=s(f9);CNe=r(J0r,"SplinterConfig"),J0r.forEach(t),MNe=r(a6e," (Splinter model)"),a6e.forEach(t),ENe=i(T),Wm=n(T,"LI",{});var n6e=s(Wm);pQ=n(n6e,"STRONG",{});var Y0r=s(pQ);yNe=r(Y0r,"squeezebert"),Y0r.forEach(t),wNe=r(n6e," \u2014 "),m9=n(n6e,"A",{href:!0});var K0r=s(m9);ANe=r(K0r,"SqueezeBertConfig"),K0r.forEach(t),LNe=r(n6e," (SqueezeBERT model)"),n6e.forEach(t),BNe=i(T),Qm=n(T,"LI",{});var s6e=s(Qm);_Q=n(s6e,"STRONG",{});var Z0r=s(_Q);kNe=r(Z0r,"swin"),Z0r.forEach(t),xNe=r(s6e," \u2014 "),g9=n(s6e,"A",{href:!0});var eLr=s(g9);RNe=r(eLr,"SwinConfig"),eLr.forEach(t),SNe=r(s6e," (Swin model)"),s6e.forEach(t),PNe=i(T),Hm=n(T,"LI",{});var l6e=s(Hm);uQ=n(l6e,"STRONG",{});var oLr=s(uQ);$Ne=r(oLr,"t5"),oLr.forEach(t),INe=r(l6e," \u2014 "),h9=n(l6e,"A",{href:!0});var rLr=s(h9);jNe=r(rLr,"T5Config"),rLr.forEach(t),NNe=r(l6e," (T5 model)"),l6e.forEach(t),DNe=i(T),Um=n(T,"LI",{});var i6e=s(Um);bQ=n(i6e,"STRONG",{});var tLr=s(bQ);qNe=r(tLr,"tapas"),tLr.forEach(t),GNe=r(i6e," \u2014 "),p9=n(i6e,"A",{href:!0});var aLr=s(p9);ONe=r(aLr,"TapasConfig"),aLr.forEach(t),XNe=r(i6e," (TAPAS model)"),i6e.forEach(t),zNe=i(T),Jm=n(T,"LI",{});var d6e=s(Jm);vQ=n(d6e,"STRONG",{});var nLr=s(vQ);VNe=r(nLr,"transfo-xl"),nLr.forEach(t),WNe=r(d6e," \u2014 "),_9=n(d6e,"A",{href:!0});var sLr=s(_9);QNe=r(sLr,"TransfoXLConfig"),sLr.forEach(t),HNe=r(d6e," (Transformer-XL model)"),d6e.forEach(t),UNe=i(T),Ym=n(T,"LI",{});var c6e=s(Ym);TQ=n(c6e,"STRONG",{});var lLr=s(TQ);JNe=r(lLr,"trocr"),lLr.forEach(t),YNe=r(c6e," \u2014 "),u9=n(c6e,"A",{href:!0});var iLr=s(u9);KNe=r(iLr,"TrOCRConfig"),iLr.forEach(t),ZNe=r(c6e," (TrOCR model)"),c6e.forEach(t),eDe=i(T),Km=n(T,"LI",{});var f6e=s(Km);FQ=n(f6e,"STRONG",{});var dLr=s(FQ);oDe=r(dLr,"unispeech"),dLr.forEach(t),rDe=r(f6e," \u2014 "),b9=n(f6e,"A",{href:!0});var cLr=s(b9);tDe=r(cLr,"UniSpeechConfig"),cLr.forEach(t),aDe=r(f6e," (UniSpeech model)"),f6e.forEach(t),nDe=i(T),Zm=n(T,"LI",{});var m6e=s(Zm);CQ=n(m6e,"STRONG",{});var fLr=s(CQ);sDe=r(fLr,"unispeech-sat"),fLr.forEach(t),lDe=r(m6e," \u2014 "),v9=n(m6e,"A",{href:!0});var mLr=s(v9);iDe=r(mLr,"UniSpeechSatConfig"),mLr.forEach(t),dDe=r(m6e," (UniSpeechSat model)"),m6e.forEach(t),cDe=i(T),eg=n(T,"LI",{});var g6e=s(eg);MQ=n(g6e,"STRONG",{});var gLr=s(MQ);fDe=r(gLr,"vilt"),gLr.forEach(t),mDe=r(g6e," \u2014 "),T9=n(g6e,"A",{href:!0});var hLr=s(T9);gDe=r(hLr,"ViltConfig"),hLr.forEach(t),hDe=r(g6e," (ViLT model)"),g6e.forEach(t),pDe=i(T),og=n(T,"LI",{});var h6e=s(og);EQ=n(h6e,"STRONG",{});var pLr=s(EQ);_De=r(pLr,"vision-encoder-decoder"),pLr.forEach(t),uDe=r(h6e," \u2014 "),F9=n(h6e,"A",{href:!0});var _Lr=s(F9);bDe=r(_Lr,"VisionEncoderDecoderConfig"),_Lr.forEach(t),vDe=r(h6e," (Vision Encoder decoder model)"),h6e.forEach(t),TDe=i(T),rg=n(T,"LI",{});var p6e=s(rg);yQ=n(p6e,"STRONG",{});var uLr=s(yQ);FDe=r(uLr,"vision-text-dual-encoder"),uLr.forEach(t),CDe=r(p6e," \u2014 "),C9=n(p6e,"A",{href:!0});var bLr=s(C9);MDe=r(bLr,"VisionTextDualEncoderConfig"),bLr.forEach(t),EDe=r(p6e," (VisionTextDualEncoder model)"),p6e.forEach(t),yDe=i(T),tg=n(T,"LI",{});var _6e=s(tg);wQ=n(_6e,"STRONG",{});var vLr=s(wQ);wDe=r(vLr,"visual_bert"),vLr.forEach(t),ADe=r(_6e," \u2014 "),M9=n(_6e,"A",{href:!0});var TLr=s(M9);LDe=r(TLr,"VisualBertConfig"),TLr.forEach(t),BDe=r(_6e," (VisualBert model)"),_6e.forEach(t),kDe=i(T),ag=n(T,"LI",{});var u6e=s(ag);AQ=n(u6e,"STRONG",{});var FLr=s(AQ);xDe=r(FLr,"vit"),FLr.forEach(t),RDe=r(u6e," \u2014 "),E9=n(u6e,"A",{href:!0});var CLr=s(E9);SDe=r(CLr,"ViTConfig"),CLr.forEach(t),PDe=r(u6e," (ViT model)"),u6e.forEach(t),$De=i(T),ng=n(T,"LI",{});var b6e=s(ng);LQ=n(b6e,"STRONG",{});var MLr=s(LQ);IDe=r(MLr,"vit_mae"),MLr.forEach(t),jDe=r(b6e," \u2014 "),y9=n(b6e,"A",{href:!0});var ELr=s(y9);NDe=r(ELr,"ViTMAEConfig"),ELr.forEach(t),DDe=r(b6e," (ViTMAE model)"),b6e.forEach(t),qDe=i(T),sg=n(T,"LI",{});var v6e=s(sg);BQ=n(v6e,"STRONG",{});var yLr=s(BQ);GDe=r(yLr,"wav2vec2"),yLr.forEach(t),ODe=r(v6e," \u2014 "),w9=n(v6e,"A",{href:!0});var wLr=s(w9);XDe=r(wLr,"Wav2Vec2Config"),wLr.forEach(t),zDe=r(v6e," (Wav2Vec2 model)"),v6e.forEach(t),VDe=i(T),lg=n(T,"LI",{});var T6e=s(lg);kQ=n(T6e,"STRONG",{});var ALr=s(kQ);WDe=r(ALr,"wavlm"),ALr.forEach(t),QDe=r(T6e," \u2014 "),A9=n(T6e,"A",{href:!0});var LLr=s(A9);HDe=r(LLr,"WavLMConfig"),LLr.forEach(t),UDe=r(T6e," (WavLM model)"),T6e.forEach(t),JDe=i(T),ig=n(T,"LI",{});var F6e=s(ig);xQ=n(F6e,"STRONG",{});var BLr=s(xQ);YDe=r(BLr,"xglm"),BLr.forEach(t),KDe=r(F6e," \u2014 "),L9=n(F6e,"A",{href:!0});var kLr=s(L9);ZDe=r(kLr,"XGLMConfig"),kLr.forEach(t),eqe=r(F6e," (XGLM model)"),F6e.forEach(t),oqe=i(T),dg=n(T,"LI",{});var C6e=s(dg);RQ=n(C6e,"STRONG",{});var xLr=s(RQ);rqe=r(xLr,"xlm"),xLr.forEach(t),tqe=r(C6e," \u2014 "),B9=n(C6e,"A",{href:!0});var RLr=s(B9);aqe=r(RLr,"XLMConfig"),RLr.forEach(t),nqe=r(C6e," (XLM model)"),C6e.forEach(t),sqe=i(T),cg=n(T,"LI",{});var M6e=s(cg);SQ=n(M6e,"STRONG",{});var SLr=s(SQ);lqe=r(SLr,"xlm-prophetnet"),SLr.forEach(t),iqe=r(M6e," \u2014 "),k9=n(M6e,"A",{href:!0});var PLr=s(k9);dqe=r(PLr,"XLMProphetNetConfig"),PLr.forEach(t),cqe=r(M6e," (XLMProphetNet model)"),M6e.forEach(t),fqe=i(T),fg=n(T,"LI",{});var E6e=s(fg);PQ=n(E6e,"STRONG",{});var $Lr=s(PQ);mqe=r($Lr,"xlm-roberta"),$Lr.forEach(t),gqe=r(E6e," \u2014 "),x9=n(E6e,"A",{href:!0});var ILr=s(x9);hqe=r(ILr,"XLMRobertaConfig"),ILr.forEach(t),pqe=r(E6e," (XLM-RoBERTa model)"),E6e.forEach(t),_qe=i(T),mg=n(T,"LI",{});var y6e=s(mg);$Q=n(y6e,"STRONG",{});var jLr=s($Q);uqe=r(jLr,"xlm-roberta-xl"),jLr.forEach(t),bqe=r(y6e," \u2014 "),R9=n(y6e,"A",{href:!0});var NLr=s(R9);vqe=r(NLr,"XLMRobertaXLConfig"),NLr.forEach(t),Tqe=r(y6e," (XLM-RoBERTa-XL model)"),y6e.forEach(t),Fqe=i(T),gg=n(T,"LI",{});var w6e=s(gg);IQ=n(w6e,"STRONG",{});var DLr=s(IQ);Cqe=r(DLr,"xlnet"),DLr.forEach(t),Mqe=r(w6e," \u2014 "),S9=n(w6e,"A",{href:!0});var qLr=s(S9);Eqe=r(qLr,"XLNetConfig"),qLr.forEach(t),yqe=r(w6e," (XLNet model)"),w6e.forEach(t),wqe=i(T),hg=n(T,"LI",{});var A6e=s(hg);jQ=n(A6e,"STRONG",{});var GLr=s(jQ);Aqe=r(GLr,"yoso"),GLr.forEach(t),Lqe=r(A6e," \u2014 "),P9=n(A6e,"A",{href:!0});var OLr=s(P9);Bqe=r(OLr,"YosoConfig"),OLr.forEach(t),kqe=r(A6e," (YOSO model)"),A6e.forEach(t),T.forEach(t),xqe=i(ia),NQ=n(ia,"P",{});var XLr=s(NQ);Rqe=r(XLr,"Examples:"),XLr.forEach(t),Sqe=i(ia),m(_M.$$.fragment,ia),ia.forEach(t),Pqe=i(Ps),pg=n(Ps,"DIV",{class:!0});var wBe=s(pg);m(uM.$$.fragment,wBe),$qe=i(wBe),DQ=n(wBe,"P",{});var zLr=s(DQ);Iqe=r(zLr,"Register a new configuration for this class."),zLr.forEach(t),wBe.forEach(t),Ps.forEach(t),w7e=i(d),Ii=n(d,"H2",{class:!0});var ABe=s(Ii);_g=n(ABe,"A",{id:!0,class:!0,href:!0});var VLr=s(_g);qQ=n(VLr,"SPAN",{});var WLr=s(qQ);m(bM.$$.fragment,WLr),WLr.forEach(t),VLr.forEach(t),jqe=i(ABe),GQ=n(ABe,"SPAN",{});var QLr=s(GQ);Nqe=r(QLr,"AutoTokenizer"),QLr.forEach(t),ABe.forEach(t),A7e=i(d),Oo=n(d,"DIV",{class:!0});var $s=s(Oo);m(vM.$$.fragment,$s),Dqe=i($s),TM=n($s,"P",{});var LBe=s(TM);qqe=r(LBe,`This is a generic tokenizer class that will be instantiated as one of the tokenizer classes of the library when
created with the `),$9=n(LBe,"A",{href:!0});var HLr=s($9);Gqe=r(HLr,"AutoTokenizer.from_pretrained()"),HLr.forEach(t),Oqe=r(LBe," class method."),LBe.forEach(t),Xqe=i($s),FM=n($s,"P",{});var BBe=s(FM);zqe=r(BBe,"This class cannot be instantiated directly using "),OQ=n(BBe,"CODE",{});var ULr=s(OQ);Vqe=r(ULr,"__init__()"),ULr.forEach(t),Wqe=r(BBe," (throws an error)."),BBe.forEach(t),Qqe=i($s),mo=n($s,"DIV",{class:!0});var da=s(mo);m(CM.$$.fragment,da),Hqe=i(da),XQ=n(da,"P",{});var JLr=s(XQ);Uqe=r(JLr,"Instantiate one of the tokenizer classes of the library from a pretrained model vocabulary."),JLr.forEach(t),Jqe=i(da),ja=n(da,"P",{});var d4=s(ja);Yqe=r(d4,"The tokenizer class to instantiate is selected based on the "),zQ=n(d4,"CODE",{});var YLr=s(zQ);Kqe=r(YLr,"model_type"),YLr.forEach(t),Zqe=r(d4,` property of the config object (either
passed as an argument or loaded from `),VQ=n(d4,"CODE",{});var KLr=s(VQ);eGe=r(KLr,"pretrained_model_name_or_path"),KLr.forEach(t),oGe=r(d4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),WQ=n(d4,"CODE",{});var ZLr=s(WQ);rGe=r(ZLr,"pretrained_model_name_or_path"),ZLr.forEach(t),tGe=r(d4,":"),d4.forEach(t),aGe=i(da),M=n(da,"UL",{});var y=s(M);Dn=n(y,"LI",{});var U0=s(Dn);QQ=n(U0,"STRONG",{});var e7r=s(QQ);nGe=r(e7r,"albert"),e7r.forEach(t),sGe=r(U0," \u2014 "),I9=n(U0,"A",{href:!0});var o7r=s(I9);lGe=r(o7r,"AlbertTokenizer"),o7r.forEach(t),iGe=r(U0," or "),j9=n(U0,"A",{href:!0});var r7r=s(j9);dGe=r(r7r,"AlbertTokenizerFast"),r7r.forEach(t),cGe=r(U0," (ALBERT model)"),U0.forEach(t),fGe=i(y),qn=n(y,"LI",{});var J0=s(qn);HQ=n(J0,"STRONG",{});var t7r=s(HQ);mGe=r(t7r,"bart"),t7r.forEach(t),gGe=r(J0," \u2014 "),N9=n(J0,"A",{href:!0});var a7r=s(N9);hGe=r(a7r,"BartTokenizer"),a7r.forEach(t),pGe=r(J0," or "),D9=n(J0,"A",{href:!0});var n7r=s(D9);_Ge=r(n7r,"BartTokenizerFast"),n7r.forEach(t),uGe=r(J0," (BART model)"),J0.forEach(t),bGe=i(y),Gn=n(y,"LI",{});var Y0=s(Gn);UQ=n(Y0,"STRONG",{});var s7r=s(UQ);vGe=r(s7r,"barthez"),s7r.forEach(t),TGe=r(Y0," \u2014 "),q9=n(Y0,"A",{href:!0});var l7r=s(q9);FGe=r(l7r,"BarthezTokenizer"),l7r.forEach(t),CGe=r(Y0," or "),G9=n(Y0,"A",{href:!0});var i7r=s(G9);MGe=r(i7r,"BarthezTokenizerFast"),i7r.forEach(t),EGe=r(Y0," (BARThez model)"),Y0.forEach(t),yGe=i(y),ug=n(y,"LI",{});var L6e=s(ug);JQ=n(L6e,"STRONG",{});var d7r=s(JQ);wGe=r(d7r,"bartpho"),d7r.forEach(t),AGe=r(L6e," \u2014 "),O9=n(L6e,"A",{href:!0});var c7r=s(O9);LGe=r(c7r,"BartphoTokenizer"),c7r.forEach(t),BGe=r(L6e," (BARTpho model)"),L6e.forEach(t),kGe=i(y),On=n(y,"LI",{});var K0=s(On);YQ=n(K0,"STRONG",{});var f7r=s(YQ);xGe=r(f7r,"bert"),f7r.forEach(t),RGe=r(K0," \u2014 "),X9=n(K0,"A",{href:!0});var m7r=s(X9);SGe=r(m7r,"BertTokenizer"),m7r.forEach(t),PGe=r(K0," or "),z9=n(K0,"A",{href:!0});var g7r=s(z9);$Ge=r(g7r,"BertTokenizerFast"),g7r.forEach(t),IGe=r(K0," (BERT model)"),K0.forEach(t),jGe=i(y),bg=n(y,"LI",{});var B6e=s(bg);KQ=n(B6e,"STRONG",{});var h7r=s(KQ);NGe=r(h7r,"bert-generation"),h7r.forEach(t),DGe=r(B6e," \u2014 "),V9=n(B6e,"A",{href:!0});var p7r=s(V9);qGe=r(p7r,"BertGenerationTokenizer"),p7r.forEach(t),GGe=r(B6e," (Bert Generation model)"),B6e.forEach(t),OGe=i(y),vg=n(y,"LI",{});var k6e=s(vg);ZQ=n(k6e,"STRONG",{});var _7r=s(ZQ);XGe=r(_7r,"bert-japanese"),_7r.forEach(t),zGe=r(k6e," \u2014 "),W9=n(k6e,"A",{href:!0});var u7r=s(W9);VGe=r(u7r,"BertJapaneseTokenizer"),u7r.forEach(t),WGe=r(k6e," (BertJapanese model)"),k6e.forEach(t),QGe=i(y),Tg=n(y,"LI",{});var x6e=s(Tg);eH=n(x6e,"STRONG",{});var b7r=s(eH);HGe=r(b7r,"bertweet"),b7r.forEach(t),UGe=r(x6e," \u2014 "),Q9=n(x6e,"A",{href:!0});var v7r=s(Q9);JGe=r(v7r,"BertweetTokenizer"),v7r.forEach(t),YGe=r(x6e," (Bertweet model)"),x6e.forEach(t),KGe=i(y),Xn=n(y,"LI",{});var Z0=s(Xn);oH=n(Z0,"STRONG",{});var T7r=s(oH);ZGe=r(T7r,"big_bird"),T7r.forEach(t),eOe=r(Z0," \u2014 "),H9=n(Z0,"A",{href:!0});var F7r=s(H9);oOe=r(F7r,"BigBirdTokenizer"),F7r.forEach(t),rOe=r(Z0," or "),U9=n(Z0,"A",{href:!0});var C7r=s(U9);tOe=r(C7r,"BigBirdTokenizerFast"),C7r.forEach(t),aOe=r(Z0," (BigBird model)"),Z0.forEach(t),nOe=i(y),zn=n(y,"LI",{});var eL=s(zn);rH=n(eL,"STRONG",{});var M7r=s(rH);sOe=r(M7r,"bigbird_pegasus"),M7r.forEach(t),lOe=r(eL," \u2014 "),J9=n(eL,"A",{href:!0});var E7r=s(J9);iOe=r(E7r,"PegasusTokenizer"),E7r.forEach(t),dOe=r(eL," or "),Y9=n(eL,"A",{href:!0});var y7r=s(Y9);cOe=r(y7r,"PegasusTokenizerFast"),y7r.forEach(t),fOe=r(eL," (BigBirdPegasus model)"),eL.forEach(t),mOe=i(y),Vn=n(y,"LI",{});var oL=s(Vn);tH=n(oL,"STRONG",{});var w7r=s(tH);gOe=r(w7r,"blenderbot"),w7r.forEach(t),hOe=r(oL," \u2014 "),K9=n(oL,"A",{href:!0});var A7r=s(K9);pOe=r(A7r,"BlenderbotTokenizer"),A7r.forEach(t),_Oe=r(oL," or "),Z9=n(oL,"A",{href:!0});var L7r=s(Z9);uOe=r(L7r,"BlenderbotTokenizerFast"),L7r.forEach(t),bOe=r(oL," (Blenderbot model)"),oL.forEach(t),vOe=i(y),Fg=n(y,"LI",{});var R6e=s(Fg);aH=n(R6e,"STRONG",{});var B7r=s(aH);TOe=r(B7r,"blenderbot-small"),B7r.forEach(t),FOe=r(R6e," \u2014 "),eB=n(R6e,"A",{href:!0});var k7r=s(eB);COe=r(k7r,"BlenderbotSmallTokenizer"),k7r.forEach(t),MOe=r(R6e," (BlenderbotSmall model)"),R6e.forEach(t),EOe=i(y),Cg=n(y,"LI",{});var S6e=s(Cg);nH=n(S6e,"STRONG",{});var x7r=s(nH);yOe=r(x7r,"byt5"),x7r.forEach(t),wOe=r(S6e," \u2014 "),oB=n(S6e,"A",{href:!0});var R7r=s(oB);AOe=r(R7r,"ByT5Tokenizer"),R7r.forEach(t),LOe=r(S6e," (ByT5 model)"),S6e.forEach(t),BOe=i(y),Wn=n(y,"LI",{});var rL=s(Wn);sH=n(rL,"STRONG",{});var S7r=s(sH);kOe=r(S7r,"camembert"),S7r.forEach(t),xOe=r(rL," \u2014 "),rB=n(rL,"A",{href:!0});var P7r=s(rB);ROe=r(P7r,"CamembertTokenizer"),P7r.forEach(t),SOe=r(rL," or "),tB=n(rL,"A",{href:!0});var $7r=s(tB);POe=r($7r,"CamembertTokenizerFast"),$7r.forEach(t),$Oe=r(rL," (CamemBERT model)"),rL.forEach(t),IOe=i(y),Mg=n(y,"LI",{});var P6e=s(Mg);lH=n(P6e,"STRONG",{});var I7r=s(lH);jOe=r(I7r,"canine"),I7r.forEach(t),NOe=r(P6e," \u2014 "),aB=n(P6e,"A",{href:!0});var j7r=s(aB);DOe=r(j7r,"CanineTokenizer"),j7r.forEach(t),qOe=r(P6e," (Canine model)"),P6e.forEach(t),GOe=i(y),Qn=n(y,"LI",{});var tL=s(Qn);iH=n(tL,"STRONG",{});var N7r=s(iH);OOe=r(N7r,"clip"),N7r.forEach(t),XOe=r(tL," \u2014 "),nB=n(tL,"A",{href:!0});var D7r=s(nB);zOe=r(D7r,"CLIPTokenizer"),D7r.forEach(t),VOe=r(tL," or "),sB=n(tL,"A",{href:!0});var q7r=s(sB);WOe=r(q7r,"CLIPTokenizerFast"),q7r.forEach(t),QOe=r(tL," (CLIP model)"),tL.forEach(t),HOe=i(y),Hn=n(y,"LI",{});var aL=s(Hn);dH=n(aL,"STRONG",{});var G7r=s(dH);UOe=r(G7r,"convbert"),G7r.forEach(t),JOe=r(aL," \u2014 "),lB=n(aL,"A",{href:!0});var O7r=s(lB);YOe=r(O7r,"ConvBertTokenizer"),O7r.forEach(t),KOe=r(aL," or "),iB=n(aL,"A",{href:!0});var X7r=s(iB);ZOe=r(X7r,"ConvBertTokenizerFast"),X7r.forEach(t),eXe=r(aL," (ConvBERT model)"),aL.forEach(t),oXe=i(y),Un=n(y,"LI",{});var nL=s(Un);cH=n(nL,"STRONG",{});var z7r=s(cH);rXe=r(z7r,"cpm"),z7r.forEach(t),tXe=r(nL," \u2014 "),dB=n(nL,"A",{href:!0});var V7r=s(dB);aXe=r(V7r,"CpmTokenizer"),V7r.forEach(t),nXe=r(nL," or "),fH=n(nL,"CODE",{});var W7r=s(fH);sXe=r(W7r,"CpmTokenizerFast"),W7r.forEach(t),lXe=r(nL," (CPM model)"),nL.forEach(t),iXe=i(y),Eg=n(y,"LI",{});var $6e=s(Eg);mH=n($6e,"STRONG",{});var Q7r=s(mH);dXe=r(Q7r,"ctrl"),Q7r.forEach(t),cXe=r($6e," \u2014 "),cB=n($6e,"A",{href:!0});var H7r=s(cB);fXe=r(H7r,"CTRLTokenizer"),H7r.forEach(t),mXe=r($6e," (CTRL model)"),$6e.forEach(t),gXe=i(y),Jn=n(y,"LI",{});var sL=s(Jn);gH=n(sL,"STRONG",{});var U7r=s(gH);hXe=r(U7r,"deberta"),U7r.forEach(t),pXe=r(sL," \u2014 "),fB=n(sL,"A",{href:!0});var J7r=s(fB);_Xe=r(J7r,"DebertaTokenizer"),J7r.forEach(t),uXe=r(sL," or "),mB=n(sL,"A",{href:!0});var Y7r=s(mB);bXe=r(Y7r,"DebertaTokenizerFast"),Y7r.forEach(t),vXe=r(sL," (DeBERTa model)"),sL.forEach(t),TXe=i(y),yg=n(y,"LI",{});var I6e=s(yg);hH=n(I6e,"STRONG",{});var K7r=s(hH);FXe=r(K7r,"deberta-v2"),K7r.forEach(t),CXe=r(I6e," \u2014 "),gB=n(I6e,"A",{href:!0});var Z7r=s(gB);MXe=r(Z7r,"DebertaV2Tokenizer"),Z7r.forEach(t),EXe=r(I6e," (DeBERTa-v2 model)"),I6e.forEach(t),yXe=i(y),Yn=n(y,"LI",{});var lL=s(Yn);pH=n(lL,"STRONG",{});var e9r=s(pH);wXe=r(e9r,"distilbert"),e9r.forEach(t),AXe=r(lL," \u2014 "),hB=n(lL,"A",{href:!0});var o9r=s(hB);LXe=r(o9r,"DistilBertTokenizer"),o9r.forEach(t),BXe=r(lL," or "),pB=n(lL,"A",{href:!0});var r9r=s(pB);kXe=r(r9r,"DistilBertTokenizerFast"),r9r.forEach(t),xXe=r(lL," (DistilBERT model)"),lL.forEach(t),RXe=i(y),Kn=n(y,"LI",{});var iL=s(Kn);_H=n(iL,"STRONG",{});var t9r=s(_H);SXe=r(t9r,"dpr"),t9r.forEach(t),PXe=r(iL," \u2014 "),_B=n(iL,"A",{href:!0});var a9r=s(_B);$Xe=r(a9r,"DPRQuestionEncoderTokenizer"),a9r.forEach(t),IXe=r(iL," or "),uB=n(iL,"A",{href:!0});var n9r=s(uB);jXe=r(n9r,"DPRQuestionEncoderTokenizerFast"),n9r.forEach(t),NXe=r(iL," (DPR model)"),iL.forEach(t),DXe=i(y),Zn=n(y,"LI",{});var dL=s(Zn);uH=n(dL,"STRONG",{});var s9r=s(uH);qXe=r(s9r,"electra"),s9r.forEach(t),GXe=r(dL," \u2014 "),bB=n(dL,"A",{href:!0});var l9r=s(bB);OXe=r(l9r,"ElectraTokenizer"),l9r.forEach(t),XXe=r(dL," or "),vB=n(dL,"A",{href:!0});var i9r=s(vB);zXe=r(i9r,"ElectraTokenizerFast"),i9r.forEach(t),VXe=r(dL," (ELECTRA model)"),dL.forEach(t),WXe=i(y),wg=n(y,"LI",{});var j6e=s(wg);bH=n(j6e,"STRONG",{});var d9r=s(bH);QXe=r(d9r,"flaubert"),d9r.forEach(t),HXe=r(j6e," \u2014 "),TB=n(j6e,"A",{href:!0});var c9r=s(TB);UXe=r(c9r,"FlaubertTokenizer"),c9r.forEach(t),JXe=r(j6e," (FlauBERT model)"),j6e.forEach(t),YXe=i(y),es=n(y,"LI",{});var cL=s(es);vH=n(cL,"STRONG",{});var f9r=s(vH);KXe=r(f9r,"fnet"),f9r.forEach(t),ZXe=r(cL," \u2014 "),FB=n(cL,"A",{href:!0});var m9r=s(FB);eze=r(m9r,"FNetTokenizer"),m9r.forEach(t),oze=r(cL," or "),CB=n(cL,"A",{href:!0});var g9r=s(CB);rze=r(g9r,"FNetTokenizerFast"),g9r.forEach(t),tze=r(cL," (FNet model)"),cL.forEach(t),aze=i(y),Ag=n(y,"LI",{});var N6e=s(Ag);TH=n(N6e,"STRONG",{});var h9r=s(TH);nze=r(h9r,"fsmt"),h9r.forEach(t),sze=r(N6e," \u2014 "),MB=n(N6e,"A",{href:!0});var p9r=s(MB);lze=r(p9r,"FSMTTokenizer"),p9r.forEach(t),ize=r(N6e," (FairSeq Machine-Translation model)"),N6e.forEach(t),dze=i(y),os=n(y,"LI",{});var fL=s(os);FH=n(fL,"STRONG",{});var _9r=s(FH);cze=r(_9r,"funnel"),_9r.forEach(t),fze=r(fL," \u2014 "),EB=n(fL,"A",{href:!0});var u9r=s(EB);mze=r(u9r,"FunnelTokenizer"),u9r.forEach(t),gze=r(fL," or "),yB=n(fL,"A",{href:!0});var b9r=s(yB);hze=r(b9r,"FunnelTokenizerFast"),b9r.forEach(t),pze=r(fL," (Funnel Transformer model)"),fL.forEach(t),_ze=i(y),rs=n(y,"LI",{});var mL=s(rs);CH=n(mL,"STRONG",{});var v9r=s(CH);uze=r(v9r,"gpt2"),v9r.forEach(t),bze=r(mL," \u2014 "),wB=n(mL,"A",{href:!0});var T9r=s(wB);vze=r(T9r,"GPT2Tokenizer"),T9r.forEach(t),Tze=r(mL," or "),AB=n(mL,"A",{href:!0});var F9r=s(AB);Fze=r(F9r,"GPT2TokenizerFast"),F9r.forEach(t),Cze=r(mL," (OpenAI GPT-2 model)"),mL.forEach(t),Mze=i(y),ts=n(y,"LI",{});var gL=s(ts);MH=n(gL,"STRONG",{});var C9r=s(MH);Eze=r(C9r,"gpt_neo"),C9r.forEach(t),yze=r(gL," \u2014 "),LB=n(gL,"A",{href:!0});var M9r=s(LB);wze=r(M9r,"GPT2Tokenizer"),M9r.forEach(t),Aze=r(gL," or "),BB=n(gL,"A",{href:!0});var E9r=s(BB);Lze=r(E9r,"GPT2TokenizerFast"),E9r.forEach(t),Bze=r(gL," (GPT Neo model)"),gL.forEach(t),kze=i(y),as=n(y,"LI",{});var hL=s(as);EH=n(hL,"STRONG",{});var y9r=s(EH);xze=r(y9r,"herbert"),y9r.forEach(t),Rze=r(hL," \u2014 "),kB=n(hL,"A",{href:!0});var w9r=s(kB);Sze=r(w9r,"HerbertTokenizer"),w9r.forEach(t),Pze=r(hL," or "),xB=n(hL,"A",{href:!0});var A9r=s(xB);$ze=r(A9r,"HerbertTokenizerFast"),A9r.forEach(t),Ize=r(hL," (HerBERT model)"),hL.forEach(t),jze=i(y),Lg=n(y,"LI",{});var D6e=s(Lg);yH=n(D6e,"STRONG",{});var L9r=s(yH);Nze=r(L9r,"hubert"),L9r.forEach(t),Dze=r(D6e," \u2014 "),RB=n(D6e,"A",{href:!0});var B9r=s(RB);qze=r(B9r,"Wav2Vec2CTCTokenizer"),B9r.forEach(t),Gze=r(D6e," (Hubert model)"),D6e.forEach(t),Oze=i(y),ns=n(y,"LI",{});var pL=s(ns);wH=n(pL,"STRONG",{});var k9r=s(wH);Xze=r(k9r,"ibert"),k9r.forEach(t),zze=r(pL," \u2014 "),SB=n(pL,"A",{href:!0});var x9r=s(SB);Vze=r(x9r,"RobertaTokenizer"),x9r.forEach(t),Wze=r(pL," or "),PB=n(pL,"A",{href:!0});var R9r=s(PB);Qze=r(R9r,"RobertaTokenizerFast"),R9r.forEach(t),Hze=r(pL," (I-BERT model)"),pL.forEach(t),Uze=i(y),ss=n(y,"LI",{});var _L=s(ss);AH=n(_L,"STRONG",{});var S9r=s(AH);Jze=r(S9r,"layoutlm"),S9r.forEach(t),Yze=r(_L," \u2014 "),$B=n(_L,"A",{href:!0});var P9r=s($B);Kze=r(P9r,"LayoutLMTokenizer"),P9r.forEach(t),Zze=r(_L," or "),IB=n(_L,"A",{href:!0});var $9r=s(IB);eVe=r($9r,"LayoutLMTokenizerFast"),$9r.forEach(t),oVe=r(_L," (LayoutLM model)"),_L.forEach(t),rVe=i(y),ls=n(y,"LI",{});var uL=s(ls);LH=n(uL,"STRONG",{});var I9r=s(LH);tVe=r(I9r,"layoutlmv2"),I9r.forEach(t),aVe=r(uL," \u2014 "),jB=n(uL,"A",{href:!0});var j9r=s(jB);nVe=r(j9r,"LayoutLMv2Tokenizer"),j9r.forEach(t),sVe=r(uL," or "),NB=n(uL,"A",{href:!0});var N9r=s(NB);lVe=r(N9r,"LayoutLMv2TokenizerFast"),N9r.forEach(t),iVe=r(uL," (LayoutLMv2 model)"),uL.forEach(t),dVe=i(y),is=n(y,"LI",{});var bL=s(is);BH=n(bL,"STRONG",{});var D9r=s(BH);cVe=r(D9r,"layoutxlm"),D9r.forEach(t),fVe=r(bL," \u2014 "),DB=n(bL,"A",{href:!0});var q9r=s(DB);mVe=r(q9r,"LayoutXLMTokenizer"),q9r.forEach(t),gVe=r(bL," or "),qB=n(bL,"A",{href:!0});var G9r=s(qB);hVe=r(G9r,"LayoutXLMTokenizerFast"),G9r.forEach(t),pVe=r(bL," (LayoutXLM model)"),bL.forEach(t),_Ve=i(y),ds=n(y,"LI",{});var vL=s(ds);kH=n(vL,"STRONG",{});var O9r=s(kH);uVe=r(O9r,"led"),O9r.forEach(t),bVe=r(vL," \u2014 "),GB=n(vL,"A",{href:!0});var X9r=s(GB);vVe=r(X9r,"LEDTokenizer"),X9r.forEach(t),TVe=r(vL," or "),OB=n(vL,"A",{href:!0});var z9r=s(OB);FVe=r(z9r,"LEDTokenizerFast"),z9r.forEach(t),CVe=r(vL," (LED model)"),vL.forEach(t),MVe=i(y),cs=n(y,"LI",{});var TL=s(cs);xH=n(TL,"STRONG",{});var V9r=s(xH);EVe=r(V9r,"longformer"),V9r.forEach(t),yVe=r(TL," \u2014 "),XB=n(TL,"A",{href:!0});var W9r=s(XB);wVe=r(W9r,"LongformerTokenizer"),W9r.forEach(t),AVe=r(TL," or "),zB=n(TL,"A",{href:!0});var Q9r=s(zB);LVe=r(Q9r,"LongformerTokenizerFast"),Q9r.forEach(t),BVe=r(TL," (Longformer model)"),TL.forEach(t),kVe=i(y),Bg=n(y,"LI",{});var q6e=s(Bg);RH=n(q6e,"STRONG",{});var H9r=s(RH);xVe=r(H9r,"luke"),H9r.forEach(t),RVe=r(q6e," \u2014 "),VB=n(q6e,"A",{href:!0});var U9r=s(VB);SVe=r(U9r,"LukeTokenizer"),U9r.forEach(t),PVe=r(q6e," (LUKE model)"),q6e.forEach(t),$Ve=i(y),fs=n(y,"LI",{});var FL=s(fs);SH=n(FL,"STRONG",{});var J9r=s(SH);IVe=r(J9r,"lxmert"),J9r.forEach(t),jVe=r(FL," \u2014 "),WB=n(FL,"A",{href:!0});var Y9r=s(WB);NVe=r(Y9r,"LxmertTokenizer"),Y9r.forEach(t),DVe=r(FL," or "),QB=n(FL,"A",{href:!0});var K9r=s(QB);qVe=r(K9r,"LxmertTokenizerFast"),K9r.forEach(t),GVe=r(FL," (LXMERT model)"),FL.forEach(t),OVe=i(y),kg=n(y,"LI",{});var G6e=s(kg);PH=n(G6e,"STRONG",{});var Z9r=s(PH);XVe=r(Z9r,"m2m_100"),Z9r.forEach(t),zVe=r(G6e," \u2014 "),HB=n(G6e,"A",{href:!0});var eBr=s(HB);VVe=r(eBr,"M2M100Tokenizer"),eBr.forEach(t),WVe=r(G6e," (M2M100 model)"),G6e.forEach(t),QVe=i(y),xg=n(y,"LI",{});var O6e=s(xg);$H=n(O6e,"STRONG",{});var oBr=s($H);HVe=r(oBr,"marian"),oBr.forEach(t),UVe=r(O6e," \u2014 "),UB=n(O6e,"A",{href:!0});var rBr=s(UB);JVe=r(rBr,"MarianTokenizer"),rBr.forEach(t),YVe=r(O6e," (Marian model)"),O6e.forEach(t),KVe=i(y),ms=n(y,"LI",{});var CL=s(ms);IH=n(CL,"STRONG",{});var tBr=s(IH);ZVe=r(tBr,"mbart"),tBr.forEach(t),eWe=r(CL," \u2014 "),JB=n(CL,"A",{href:!0});var aBr=s(JB);oWe=r(aBr,"MBartTokenizer"),aBr.forEach(t),rWe=r(CL," or "),YB=n(CL,"A",{href:!0});var nBr=s(YB);tWe=r(nBr,"MBartTokenizerFast"),nBr.forEach(t),aWe=r(CL," (mBART model)"),CL.forEach(t),nWe=i(y),gs=n(y,"LI",{});var ML=s(gs);jH=n(ML,"STRONG",{});var sBr=s(jH);sWe=r(sBr,"mbart50"),sBr.forEach(t),lWe=r(ML," \u2014 "),KB=n(ML,"A",{href:!0});var lBr=s(KB);iWe=r(lBr,"MBart50Tokenizer"),lBr.forEach(t),dWe=r(ML," or "),ZB=n(ML,"A",{href:!0});var iBr=s(ZB);cWe=r(iBr,"MBart50TokenizerFast"),iBr.forEach(t),fWe=r(ML," (mBART-50 model)"),ML.forEach(t),mWe=i(y),Rg=n(y,"LI",{});var X6e=s(Rg);NH=n(X6e,"STRONG",{});var dBr=s(NH);gWe=r(dBr,"mluke"),dBr.forEach(t),hWe=r(X6e," \u2014 "),ek=n(X6e,"A",{href:!0});var cBr=s(ek);pWe=r(cBr,"MLukeTokenizer"),cBr.forEach(t),_We=r(X6e," (mLUKE model)"),X6e.forEach(t),uWe=i(y),hs=n(y,"LI",{});var EL=s(hs);DH=n(EL,"STRONG",{});var fBr=s(DH);bWe=r(fBr,"mobilebert"),fBr.forEach(t),vWe=r(EL," \u2014 "),ok=n(EL,"A",{href:!0});var mBr=s(ok);TWe=r(mBr,"MobileBertTokenizer"),mBr.forEach(t),FWe=r(EL," or "),rk=n(EL,"A",{href:!0});var gBr=s(rk);CWe=r(gBr,"MobileBertTokenizerFast"),gBr.forEach(t),MWe=r(EL," (MobileBERT model)"),EL.forEach(t),EWe=i(y),ps=n(y,"LI",{});var yL=s(ps);qH=n(yL,"STRONG",{});var hBr=s(qH);yWe=r(hBr,"mpnet"),hBr.forEach(t),wWe=r(yL," \u2014 "),tk=n(yL,"A",{href:!0});var pBr=s(tk);AWe=r(pBr,"MPNetTokenizer"),pBr.forEach(t),LWe=r(yL," or "),ak=n(yL,"A",{href:!0});var _Br=s(ak);BWe=r(_Br,"MPNetTokenizerFast"),_Br.forEach(t),kWe=r(yL," (MPNet model)"),yL.forEach(t),xWe=i(y),_s=n(y,"LI",{});var wL=s(_s);GH=n(wL,"STRONG",{});var uBr=s(GH);RWe=r(uBr,"mt5"),uBr.forEach(t),SWe=r(wL," \u2014 "),nk=n(wL,"A",{href:!0});var bBr=s(nk);PWe=r(bBr,"MT5Tokenizer"),bBr.forEach(t),$We=r(wL," or "),sk=n(wL,"A",{href:!0});var vBr=s(sk);IWe=r(vBr,"MT5TokenizerFast"),vBr.forEach(t),jWe=r(wL," (mT5 model)"),wL.forEach(t),NWe=i(y),us=n(y,"LI",{});var AL=s(us);OH=n(AL,"STRONG",{});var TBr=s(OH);DWe=r(TBr,"openai-gpt"),TBr.forEach(t),qWe=r(AL," \u2014 "),lk=n(AL,"A",{href:!0});var FBr=s(lk);GWe=r(FBr,"OpenAIGPTTokenizer"),FBr.forEach(t),OWe=r(AL," or "),ik=n(AL,"A",{href:!0});var CBr=s(ik);XWe=r(CBr,"OpenAIGPTTokenizerFast"),CBr.forEach(t),zWe=r(AL," (OpenAI GPT model)"),AL.forEach(t),VWe=i(y),bs=n(y,"LI",{});var LL=s(bs);XH=n(LL,"STRONG",{});var MBr=s(XH);WWe=r(MBr,"pegasus"),MBr.forEach(t),QWe=r(LL," \u2014 "),dk=n(LL,"A",{href:!0});var EBr=s(dk);HWe=r(EBr,"PegasusTokenizer"),EBr.forEach(t),UWe=r(LL," or "),ck=n(LL,"A",{href:!0});var yBr=s(ck);JWe=r(yBr,"PegasusTokenizerFast"),yBr.forEach(t),YWe=r(LL," (Pegasus model)"),LL.forEach(t),KWe=i(y),Sg=n(y,"LI",{});var z6e=s(Sg);zH=n(z6e,"STRONG",{});var wBr=s(zH);ZWe=r(wBr,"perceiver"),wBr.forEach(t),eQe=r(z6e," \u2014 "),fk=n(z6e,"A",{href:!0});var ABr=s(fk);oQe=r(ABr,"PerceiverTokenizer"),ABr.forEach(t),rQe=r(z6e," (Perceiver model)"),z6e.forEach(t),tQe=i(y),Pg=n(y,"LI",{});var V6e=s(Pg);VH=n(V6e,"STRONG",{});var LBr=s(VH);aQe=r(LBr,"phobert"),LBr.forEach(t),nQe=r(V6e," \u2014 "),mk=n(V6e,"A",{href:!0});var BBr=s(mk);sQe=r(BBr,"PhobertTokenizer"),BBr.forEach(t),lQe=r(V6e," (PhoBERT model)"),V6e.forEach(t),iQe=i(y),$g=n(y,"LI",{});var W6e=s($g);WH=n(W6e,"STRONG",{});var kBr=s(WH);dQe=r(kBr,"plbart"),kBr.forEach(t),cQe=r(W6e," \u2014 "),gk=n(W6e,"A",{href:!0});var xBr=s(gk);fQe=r(xBr,"PLBartTokenizer"),xBr.forEach(t),mQe=r(W6e," (PLBart model)"),W6e.forEach(t),gQe=i(y),Ig=n(y,"LI",{});var Q6e=s(Ig);QH=n(Q6e,"STRONG",{});var RBr=s(QH);hQe=r(RBr,"prophetnet"),RBr.forEach(t),pQe=r(Q6e," \u2014 "),hk=n(Q6e,"A",{href:!0});var SBr=s(hk);_Qe=r(SBr,"ProphetNetTokenizer"),SBr.forEach(t),uQe=r(Q6e," (ProphetNet model)"),Q6e.forEach(t),bQe=i(y),vs=n(y,"LI",{});var BL=s(vs);HH=n(BL,"STRONG",{});var PBr=s(HH);vQe=r(PBr,"qdqbert"),PBr.forEach(t),TQe=r(BL," \u2014 "),pk=n(BL,"A",{href:!0});var $Br=s(pk);FQe=r($Br,"BertTokenizer"),$Br.forEach(t),CQe=r(BL," or "),_k=n(BL,"A",{href:!0});var IBr=s(_k);MQe=r(IBr,"BertTokenizerFast"),IBr.forEach(t),EQe=r(BL," (QDQBert model)"),BL.forEach(t),yQe=i(y),jg=n(y,"LI",{});var H6e=s(jg);UH=n(H6e,"STRONG",{});var jBr=s(UH);wQe=r(jBr,"rag"),jBr.forEach(t),AQe=r(H6e," \u2014 "),uk=n(H6e,"A",{href:!0});var NBr=s(uk);LQe=r(NBr,"RagTokenizer"),NBr.forEach(t),BQe=r(H6e," (RAG model)"),H6e.forEach(t),kQe=i(y),Ts=n(y,"LI",{});var kL=s(Ts);JH=n(kL,"STRONG",{});var DBr=s(JH);xQe=r(DBr,"reformer"),DBr.forEach(t),RQe=r(kL," \u2014 "),bk=n(kL,"A",{href:!0});var qBr=s(bk);SQe=r(qBr,"ReformerTokenizer"),qBr.forEach(t),PQe=r(kL," or "),vk=n(kL,"A",{href:!0});var GBr=s(vk);$Qe=r(GBr,"ReformerTokenizerFast"),GBr.forEach(t),IQe=r(kL," (Reformer model)"),kL.forEach(t),jQe=i(y),Fs=n(y,"LI",{});var xL=s(Fs);YH=n(xL,"STRONG",{});var OBr=s(YH);NQe=r(OBr,"rembert"),OBr.forEach(t),DQe=r(xL," \u2014 "),Tk=n(xL,"A",{href:!0});var XBr=s(Tk);qQe=r(XBr,"RemBertTokenizer"),XBr.forEach(t),GQe=r(xL," or "),Fk=n(xL,"A",{href:!0});var zBr=s(Fk);OQe=r(zBr,"RemBertTokenizerFast"),zBr.forEach(t),XQe=r(xL," (RemBERT model)"),xL.forEach(t),zQe=i(y),Cs=n(y,"LI",{});var RL=s(Cs);KH=n(RL,"STRONG",{});var VBr=s(KH);VQe=r(VBr,"retribert"),VBr.forEach(t),WQe=r(RL," \u2014 "),Ck=n(RL,"A",{href:!0});var WBr=s(Ck);QQe=r(WBr,"RetriBertTokenizer"),WBr.forEach(t),HQe=r(RL," or "),Mk=n(RL,"A",{href:!0});var QBr=s(Mk);UQe=r(QBr,"RetriBertTokenizerFast"),QBr.forEach(t),JQe=r(RL," (RetriBERT model)"),RL.forEach(t),YQe=i(y),Ms=n(y,"LI",{});var SL=s(Ms);ZH=n(SL,"STRONG",{});var HBr=s(ZH);KQe=r(HBr,"roberta"),HBr.forEach(t),ZQe=r(SL," \u2014 "),Ek=n(SL,"A",{href:!0});var UBr=s(Ek);eHe=r(UBr,"RobertaTokenizer"),UBr.forEach(t),oHe=r(SL," or "),yk=n(SL,"A",{href:!0});var JBr=s(yk);rHe=r(JBr,"RobertaTokenizerFast"),JBr.forEach(t),tHe=r(SL," (RoBERTa model)"),SL.forEach(t),aHe=i(y),Es=n(y,"LI",{});var PL=s(Es);eU=n(PL,"STRONG",{});var YBr=s(eU);nHe=r(YBr,"roformer"),YBr.forEach(t),sHe=r(PL," \u2014 "),wk=n(PL,"A",{href:!0});var KBr=s(wk);lHe=r(KBr,"RoFormerTokenizer"),KBr.forEach(t),iHe=r(PL," or "),Ak=n(PL,"A",{href:!0});var ZBr=s(Ak);dHe=r(ZBr,"RoFormerTokenizerFast"),ZBr.forEach(t),cHe=r(PL," (RoFormer model)"),PL.forEach(t),fHe=i(y),Ng=n(y,"LI",{});var U6e=s(Ng);oU=n(U6e,"STRONG",{});var ekr=s(oU);mHe=r(ekr,"speech_to_text"),ekr.forEach(t),gHe=r(U6e," \u2014 "),Lk=n(U6e,"A",{href:!0});var okr=s(Lk);hHe=r(okr,"Speech2TextTokenizer"),okr.forEach(t),pHe=r(U6e," (Speech2Text model)"),U6e.forEach(t),_He=i(y),Dg=n(y,"LI",{});var J6e=s(Dg);rU=n(J6e,"STRONG",{});var rkr=s(rU);uHe=r(rkr,"speech_to_text_2"),rkr.forEach(t),bHe=r(J6e," \u2014 "),Bk=n(J6e,"A",{href:!0});var tkr=s(Bk);vHe=r(tkr,"Speech2Text2Tokenizer"),tkr.forEach(t),THe=r(J6e," (Speech2Text2 model)"),J6e.forEach(t),FHe=i(y),ys=n(y,"LI",{});var $L=s(ys);tU=n($L,"STRONG",{});var akr=s(tU);CHe=r(akr,"splinter"),akr.forEach(t),MHe=r($L," \u2014 "),kk=n($L,"A",{href:!0});var nkr=s(kk);EHe=r(nkr,"SplinterTokenizer"),nkr.forEach(t),yHe=r($L," or "),xk=n($L,"A",{href:!0});var skr=s(xk);wHe=r(skr,"SplinterTokenizerFast"),skr.forEach(t),AHe=r($L," (Splinter model)"),$L.forEach(t),LHe=i(y),ws=n(y,"LI",{});var IL=s(ws);aU=n(IL,"STRONG",{});var lkr=s(aU);BHe=r(lkr,"squeezebert"),lkr.forEach(t),kHe=r(IL," \u2014 "),Rk=n(IL,"A",{href:!0});var ikr=s(Rk);xHe=r(ikr,"SqueezeBertTokenizer"),ikr.forEach(t),RHe=r(IL," or "),Sk=n(IL,"A",{href:!0});var dkr=s(Sk);SHe=r(dkr,"SqueezeBertTokenizerFast"),dkr.forEach(t),PHe=r(IL," (SqueezeBERT model)"),IL.forEach(t),$He=i(y),As=n(y,"LI",{});var jL=s(As);nU=n(jL,"STRONG",{});var ckr=s(nU);IHe=r(ckr,"t5"),ckr.forEach(t),jHe=r(jL," \u2014 "),Pk=n(jL,"A",{href:!0});var fkr=s(Pk);NHe=r(fkr,"T5Tokenizer"),fkr.forEach(t),DHe=r(jL," or "),$k=n(jL,"A",{href:!0});var mkr=s($k);qHe=r(mkr,"T5TokenizerFast"),mkr.forEach(t),GHe=r(jL," (T5 model)"),jL.forEach(t),OHe=i(y),qg=n(y,"LI",{});var Y6e=s(qg);sU=n(Y6e,"STRONG",{});var gkr=s(sU);XHe=r(gkr,"tapas"),gkr.forEach(t),zHe=r(Y6e," \u2014 "),Ik=n(Y6e,"A",{href:!0});var hkr=s(Ik);VHe=r(hkr,"TapasTokenizer"),hkr.forEach(t),WHe=r(Y6e," (TAPAS model)"),Y6e.forEach(t),QHe=i(y),Gg=n(y,"LI",{});var K6e=s(Gg);lU=n(K6e,"STRONG",{});var pkr=s(lU);HHe=r(pkr,"transfo-xl"),pkr.forEach(t),UHe=r(K6e," \u2014 "),jk=n(K6e,"A",{href:!0});var _kr=s(jk);JHe=r(_kr,"TransfoXLTokenizer"),_kr.forEach(t),YHe=r(K6e," (Transformer-XL model)"),K6e.forEach(t),KHe=i(y),Og=n(y,"LI",{});var Z6e=s(Og);iU=n(Z6e,"STRONG",{});var ukr=s(iU);ZHe=r(ukr,"wav2vec2"),ukr.forEach(t),eUe=r(Z6e," \u2014 "),Nk=n(Z6e,"A",{href:!0});var bkr=s(Nk);oUe=r(bkr,"Wav2Vec2CTCTokenizer"),bkr.forEach(t),rUe=r(Z6e," (Wav2Vec2 model)"),Z6e.forEach(t),tUe=i(y),Xg=n(y,"LI",{});var eTe=s(Xg);dU=n(eTe,"STRONG",{});var vkr=s(dU);aUe=r(vkr,"wav2vec2_phoneme"),vkr.forEach(t),nUe=r(eTe," \u2014 "),Dk=n(eTe,"A",{href:!0});var Tkr=s(Dk);sUe=r(Tkr,"Wav2Vec2PhonemeCTCTokenizer"),Tkr.forEach(t),lUe=r(eTe," (Wav2Vec2Phoneme model)"),eTe.forEach(t),iUe=i(y),Ls=n(y,"LI",{});var NL=s(Ls);cU=n(NL,"STRONG",{});var Fkr=s(cU);dUe=r(Fkr,"xglm"),Fkr.forEach(t),cUe=r(NL," \u2014 "),qk=n(NL,"A",{href:!0});var Ckr=s(qk);fUe=r(Ckr,"XGLMTokenizer"),Ckr.forEach(t),mUe=r(NL," or "),Gk=n(NL,"A",{href:!0});var Mkr=s(Gk);gUe=r(Mkr,"XGLMTokenizerFast"),Mkr.forEach(t),hUe=r(NL," (XGLM model)"),NL.forEach(t),pUe=i(y),zg=n(y,"LI",{});var oTe=s(zg);fU=n(oTe,"STRONG",{});var Ekr=s(fU);_Ue=r(Ekr,"xlm"),Ekr.forEach(t),uUe=r(oTe," \u2014 "),Ok=n(oTe,"A",{href:!0});var ykr=s(Ok);bUe=r(ykr,"XLMTokenizer"),ykr.forEach(t),vUe=r(oTe," (XLM model)"),oTe.forEach(t),TUe=i(y),Vg=n(y,"LI",{});var rTe=s(Vg);mU=n(rTe,"STRONG",{});var wkr=s(mU);FUe=r(wkr,"xlm-prophetnet"),wkr.forEach(t),CUe=r(rTe," \u2014 "),Xk=n(rTe,"A",{href:!0});var Akr=s(Xk);MUe=r(Akr,"XLMProphetNetTokenizer"),Akr.forEach(t),EUe=r(rTe," (XLMProphetNet model)"),rTe.forEach(t),yUe=i(y),Bs=n(y,"LI",{});var DL=s(Bs);gU=n(DL,"STRONG",{});var Lkr=s(gU);wUe=r(Lkr,"xlm-roberta"),Lkr.forEach(t),AUe=r(DL," \u2014 "),zk=n(DL,"A",{href:!0});var Bkr=s(zk);LUe=r(Bkr,"XLMRobertaTokenizer"),Bkr.forEach(t),BUe=r(DL," or "),Vk=n(DL,"A",{href:!0});var kkr=s(Vk);kUe=r(kkr,"XLMRobertaTokenizerFast"),kkr.forEach(t),xUe=r(DL," (XLM-RoBERTa model)"),DL.forEach(t),RUe=i(y),ks=n(y,"LI",{});var qL=s(ks);hU=n(qL,"STRONG",{});var xkr=s(hU);SUe=r(xkr,"xlnet"),xkr.forEach(t),PUe=r(qL," \u2014 "),Wk=n(qL,"A",{href:!0});var Rkr=s(Wk);$Ue=r(Rkr,"XLNetTokenizer"),Rkr.forEach(t),IUe=r(qL," or "),Qk=n(qL,"A",{href:!0});var Skr=s(Qk);jUe=r(Skr,"XLNetTokenizerFast"),Skr.forEach(t),NUe=r(qL," (XLNet model)"),qL.forEach(t),y.forEach(t),DUe=i(da),pU=n(da,"P",{});var Pkr=s(pU);qUe=r(Pkr,"Examples:"),Pkr.forEach(t),GUe=i(da),m(MM.$$.fragment,da),da.forEach(t),OUe=i($s),Wg=n($s,"DIV",{class:!0});var kBe=s(Wg);m(EM.$$.fragment,kBe),XUe=i(kBe),_U=n(kBe,"P",{});var $kr=s(_U);zUe=r($kr,"Register a new tokenizer in this mapping."),$kr.forEach(t),kBe.forEach(t),$s.forEach(t),L7e=i(d),ji=n(d,"H2",{class:!0});var xBe=s(ji);Qg=n(xBe,"A",{id:!0,class:!0,href:!0});var Ikr=s(Qg);uU=n(Ikr,"SPAN",{});var jkr=s(uU);m(yM.$$.fragment,jkr),jkr.forEach(t),Ikr.forEach(t),VUe=i(xBe),bU=n(xBe,"SPAN",{});var Nkr=s(bU);WUe=r(Nkr,"AutoFeatureExtractor"),Nkr.forEach(t),xBe.forEach(t),B7e=i(d),Xo=n(d,"DIV",{class:!0});var Is=s(Xo);m(wM.$$.fragment,Is),QUe=i(Is),AM=n(Is,"P",{});var RBe=s(AM);HUe=r(RBe,`This is a generic feature extractor class that will be instantiated as one of the feature extractor classes of the
library when created with the `),Hk=n(RBe,"A",{href:!0});var Dkr=s(Hk);UUe=r(Dkr,"AutoFeatureExtractor.from_pretrained()"),Dkr.forEach(t),JUe=r(RBe," class method."),RBe.forEach(t),YUe=i(Is),LM=n(Is,"P",{});var SBe=s(LM);KUe=r(SBe,"This class cannot be instantiated directly using "),vU=n(SBe,"CODE",{});var qkr=s(vU);ZUe=r(qkr,"__init__()"),qkr.forEach(t),eJe=r(SBe," (throws an error)."),SBe.forEach(t),oJe=i(Is),Le=n(Is,"DIV",{class:!0});var xt=s(Le);m(BM.$$.fragment,xt),rJe=i(xt),TU=n(xt,"P",{});var Gkr=s(TU);tJe=r(Gkr,"Instantiate one of the feature extractor classes of the library from a pretrained model vocabulary."),Gkr.forEach(t),aJe=i(xt),Na=n(xt,"P",{});var c4=s(Na);nJe=r(c4,"The feature extractor class to instantiate is selected based on the "),FU=n(c4,"CODE",{});var Okr=s(FU);sJe=r(Okr,"model_type"),Okr.forEach(t),lJe=r(c4,` property of the config object
(either passed as an argument or loaded from `),CU=n(c4,"CODE",{});var Xkr=s(CU);iJe=r(Xkr,"pretrained_model_name_or_path"),Xkr.forEach(t),dJe=r(c4,` if possible), or when it\u2019s
missing, by falling back to using pattern matching on `),MU=n(c4,"CODE",{});var zkr=s(MU);cJe=r(zkr,"pretrained_model_name_or_path"),zkr.forEach(t),fJe=r(c4,":"),c4.forEach(t),mJe=i(xt),se=n(xt,"UL",{});var de=s(se);Hg=n(de,"LI",{});var tTe=s(Hg);EU=n(tTe,"STRONG",{});var Vkr=s(EU);gJe=r(Vkr,"beit"),Vkr.forEach(t),hJe=r(tTe," \u2014 "),Uk=n(tTe,"A",{href:!0});var Wkr=s(Uk);pJe=r(Wkr,"BeitFeatureExtractor"),Wkr.forEach(t),_Je=r(tTe," (BEiT model)"),tTe.forEach(t),uJe=i(de),Ug=n(de,"LI",{});var aTe=s(Ug);yU=n(aTe,"STRONG",{});var Qkr=s(yU);bJe=r(Qkr,"clip"),Qkr.forEach(t),vJe=r(aTe," \u2014 "),Jk=n(aTe,"A",{href:!0});var Hkr=s(Jk);TJe=r(Hkr,"CLIPFeatureExtractor"),Hkr.forEach(t),FJe=r(aTe," (CLIP model)"),aTe.forEach(t),CJe=i(de),Jg=n(de,"LI",{});var nTe=s(Jg);wU=n(nTe,"STRONG",{});var Ukr=s(wU);MJe=r(Ukr,"convnext"),Ukr.forEach(t),EJe=r(nTe," \u2014 "),Yk=n(nTe,"A",{href:!0});var Jkr=s(Yk);yJe=r(Jkr,"ConvNextFeatureExtractor"),Jkr.forEach(t),wJe=r(nTe," (ConvNext model)"),nTe.forEach(t),AJe=i(de),Yg=n(de,"LI",{});var sTe=s(Yg);AU=n(sTe,"STRONG",{});var Ykr=s(AU);LJe=r(Ykr,"deit"),Ykr.forEach(t),BJe=r(sTe," \u2014 "),Kk=n(sTe,"A",{href:!0});var Kkr=s(Kk);kJe=r(Kkr,"DeiTFeatureExtractor"),Kkr.forEach(t),xJe=r(sTe," (DeiT model)"),sTe.forEach(t),RJe=i(de),Kg=n(de,"LI",{});var lTe=s(Kg);LU=n(lTe,"STRONG",{});var Zkr=s(LU);SJe=r(Zkr,"detr"),Zkr.forEach(t),PJe=r(lTe," \u2014 "),Zk=n(lTe,"A",{href:!0});var exr=s(Zk);$Je=r(exr,"DetrFeatureExtractor"),exr.forEach(t),IJe=r(lTe," (DETR model)"),lTe.forEach(t),jJe=i(de),Zg=n(de,"LI",{});var iTe=s(Zg);BU=n(iTe,"STRONG",{});var oxr=s(BU);NJe=r(oxr,"hubert"),oxr.forEach(t),DJe=r(iTe," \u2014 "),ex=n(iTe,"A",{href:!0});var rxr=s(ex);qJe=r(rxr,"Wav2Vec2FeatureExtractor"),rxr.forEach(t),GJe=r(iTe," (Hubert model)"),iTe.forEach(t),OJe=i(de),eh=n(de,"LI",{});var dTe=s(eh);kU=n(dTe,"STRONG",{});var txr=s(kU);XJe=r(txr,"layoutlmv2"),txr.forEach(t),zJe=r(dTe," \u2014 "),ox=n(dTe,"A",{href:!0});var axr=s(ox);VJe=r(axr,"LayoutLMv2FeatureExtractor"),axr.forEach(t),WJe=r(dTe," (LayoutLMv2 model)"),dTe.forEach(t),QJe=i(de),oh=n(de,"LI",{});var cTe=s(oh);xU=n(cTe,"STRONG",{});var nxr=s(xU);HJe=r(nxr,"perceiver"),nxr.forEach(t),UJe=r(cTe," \u2014 "),rx=n(cTe,"A",{href:!0});var sxr=s(rx);JJe=r(sxr,"PerceiverFeatureExtractor"),sxr.forEach(t),YJe=r(cTe," (Perceiver model)"),cTe.forEach(t),KJe=i(de),rh=n(de,"LI",{});var fTe=s(rh);RU=n(fTe,"STRONG",{});var lxr=s(RU);ZJe=r(lxr,"poolformer"),lxr.forEach(t),eYe=r(fTe," \u2014 "),tx=n(fTe,"A",{href:!0});var ixr=s(tx);oYe=r(ixr,"PoolFormerFeatureExtractor"),ixr.forEach(t),rYe=r(fTe," (PoolFormer model)"),fTe.forEach(t),tYe=i(de),th=n(de,"LI",{});var mTe=s(th);SU=n(mTe,"STRONG",{});var dxr=s(SU);aYe=r(dxr,"segformer"),dxr.forEach(t),nYe=r(mTe," \u2014 "),ax=n(mTe,"A",{href:!0});var cxr=s(ax);sYe=r(cxr,"SegformerFeatureExtractor"),cxr.forEach(t),lYe=r(mTe," (SegFormer model)"),mTe.forEach(t),iYe=i(de),ah=n(de,"LI",{});var gTe=s(ah);PU=n(gTe,"STRONG",{});var fxr=s(PU);dYe=r(fxr,"speech_to_text"),fxr.forEach(t),cYe=r(gTe," \u2014 "),nx=n(gTe,"A",{href:!0});var mxr=s(nx);fYe=r(mxr,"Speech2TextFeatureExtractor"),mxr.forEach(t),mYe=r(gTe," (Speech2Text model)"),gTe.forEach(t),gYe=i(de),nh=n(de,"LI",{});var hTe=s(nh);$U=n(hTe,"STRONG",{});var gxr=s($U);hYe=r(gxr,"swin"),gxr.forEach(t),pYe=r(hTe," \u2014 "),sx=n(hTe,"A",{href:!0});var hxr=s(sx);_Ye=r(hxr,"ViTFeatureExtractor"),hxr.forEach(t),uYe=r(hTe," (Swin model)"),hTe.forEach(t),bYe=i(de),sh=n(de,"LI",{});var pTe=s(sh);IU=n(pTe,"STRONG",{});var pxr=s(IU);vYe=r(pxr,"vit"),pxr.forEach(t),TYe=r(pTe," \u2014 "),lx=n(pTe,"A",{href:!0});var _xr=s(lx);FYe=r(_xr,"ViTFeatureExtractor"),_xr.forEach(t),CYe=r(pTe," (ViT model)"),pTe.forEach(t),MYe=i(de),lh=n(de,"LI",{});var _Te=s(lh);jU=n(_Te,"STRONG",{});var uxr=s(jU);EYe=r(uxr,"vit_mae"),uxr.forEach(t),yYe=r(_Te," \u2014 "),ix=n(_Te,"A",{href:!0});var bxr=s(ix);wYe=r(bxr,"ViTFeatureExtractor"),bxr.forEach(t),AYe=r(_Te," (ViTMAE model)"),_Te.forEach(t),LYe=i(de),ih=n(de,"LI",{});var uTe=s(ih);NU=n(uTe,"STRONG",{});var vxr=s(NU);BYe=r(vxr,"wav2vec2"),vxr.forEach(t),kYe=r(uTe," \u2014 "),dx=n(uTe,"A",{href:!0});var Txr=s(dx);xYe=r(Txr,"Wav2Vec2FeatureExtractor"),Txr.forEach(t),RYe=r(uTe," (Wav2Vec2 model)"),uTe.forEach(t),de.forEach(t),SYe=i(xt),m(dh.$$.fragment,xt),PYe=i(xt),DU=n(xt,"P",{});var Fxr=s(DU);$Ye=r(Fxr,"Examples:"),Fxr.forEach(t),IYe=i(xt),m(kM.$$.fragment,xt),xt.forEach(t),jYe=i(Is),ch=n(Is,"DIV",{class:!0});var PBe=s(ch);m(xM.$$.fragment,PBe),NYe=i(PBe),qU=n(PBe,"P",{});var Cxr=s(qU);DYe=r(Cxr,"Register a new feature extractor for this class."),Cxr.forEach(t),PBe.forEach(t),Is.forEach(t),k7e=i(d),Ni=n(d,"H2",{class:!0});var $Be=s(Ni);fh=n($Be,"A",{id:!0,class:!0,href:!0});var Mxr=s(fh);GU=n(Mxr,"SPAN",{});var Exr=s(GU);m(RM.$$.fragment,Exr),Exr.forEach(t),Mxr.forEach(t),qYe=i($Be),OU=n($Be,"SPAN",{});var yxr=s(OU);GYe=r(yxr,"AutoProcessor"),yxr.forEach(t),$Be.forEach(t),x7e=i(d),zo=n(d,"DIV",{class:!0});var js=s(zo);m(SM.$$.fragment,js),OYe=i(js),PM=n(js,"P",{});var IBe=s(PM);XYe=r(IBe,`This is a generic processor class that will be instantiated as one of the processor classes of the library when
created with the `),cx=n(IBe,"A",{href:!0});var wxr=s(cx);zYe=r(wxr,"AutoProcessor.from_pretrained()"),wxr.forEach(t),VYe=r(IBe," class method."),IBe.forEach(t),WYe=i(js),$M=n(js,"P",{});var jBe=s($M);QYe=r(jBe,"This class cannot be instantiated directly using "),XU=n(jBe,"CODE",{});var Axr=s(XU);HYe=r(Axr,"__init__()"),Axr.forEach(t),UYe=r(jBe," (throws an error)."),jBe.forEach(t),JYe=i(js),Be=n(js,"DIV",{class:!0});var Rt=s(Be);m(IM.$$.fragment,Rt),YYe=i(Rt),zU=n(Rt,"P",{});var Lxr=s(zU);KYe=r(Lxr,"Instantiate one of the processor classes of the library from a pretrained model vocabulary."),Lxr.forEach(t),ZYe=i(Rt),Di=n(Rt,"P",{});var ez=s(Di);eKe=r(ez,"The processor class to instantiate is selected based on the "),VU=n(ez,"CODE",{});var Bxr=s(VU);oKe=r(Bxr,"model_type"),Bxr.forEach(t),rKe=r(ez,` property of the config object (either
passed as an argument or loaded from `),WU=n(ez,"CODE",{});var kxr=s(WU);tKe=r(kxr,"pretrained_model_name_or_path"),kxr.forEach(t),aKe=r(ez," if possible):"),ez.forEach(t),nKe=i(Rt),we=n(Rt,"UL",{});var No=s(we);mh=n(No,"LI",{});var bTe=s(mh);QU=n(bTe,"STRONG",{});var xxr=s(QU);sKe=r(xxr,"clip"),xxr.forEach(t),lKe=r(bTe," \u2014 "),fx=n(bTe,"A",{href:!0});var Rxr=s(fx);iKe=r(Rxr,"CLIPProcessor"),Rxr.forEach(t),dKe=r(bTe," (CLIP model)"),bTe.forEach(t),cKe=i(No),gh=n(No,"LI",{});var vTe=s(gh);HU=n(vTe,"STRONG",{});var Sxr=s(HU);fKe=r(Sxr,"layoutlmv2"),Sxr.forEach(t),mKe=r(vTe," \u2014 "),mx=n(vTe,"A",{href:!0});var Pxr=s(mx);gKe=r(Pxr,"LayoutLMv2Processor"),Pxr.forEach(t),hKe=r(vTe," (LayoutLMv2 model)"),vTe.forEach(t),pKe=i(No),hh=n(No,"LI",{});var TTe=s(hh);UU=n(TTe,"STRONG",{});var $xr=s(UU);_Ke=r($xr,"layoutxlm"),$xr.forEach(t),uKe=r(TTe," \u2014 "),gx=n(TTe,"A",{href:!0});var Ixr=s(gx);bKe=r(Ixr,"LayoutXLMProcessor"),Ixr.forEach(t),vKe=r(TTe," (LayoutXLM model)"),TTe.forEach(t),TKe=i(No),ph=n(No,"LI",{});var FTe=s(ph);JU=n(FTe,"STRONG",{});var jxr=s(JU);FKe=r(jxr,"speech_to_text"),jxr.forEach(t),CKe=r(FTe," \u2014 "),hx=n(FTe,"A",{href:!0});var Nxr=s(hx);MKe=r(Nxr,"Speech2TextProcessor"),Nxr.forEach(t),EKe=r(FTe," (Speech2Text model)"),FTe.forEach(t),yKe=i(No),_h=n(No,"LI",{});var CTe=s(_h);YU=n(CTe,"STRONG",{});var Dxr=s(YU);wKe=r(Dxr,"speech_to_text_2"),Dxr.forEach(t),AKe=r(CTe," \u2014 "),px=n(CTe,"A",{href:!0});var qxr=s(px);LKe=r(qxr,"Speech2Text2Processor"),qxr.forEach(t),BKe=r(CTe," (Speech2Text2 model)"),CTe.forEach(t),kKe=i(No),uh=n(No,"LI",{});var MTe=s(uh);KU=n(MTe,"STRONG",{});var Gxr=s(KU);xKe=r(Gxr,"trocr"),Gxr.forEach(t),RKe=r(MTe," \u2014 "),_x=n(MTe,"A",{href:!0});var Oxr=s(_x);SKe=r(Oxr,"TrOCRProcessor"),Oxr.forEach(t),PKe=r(MTe," (TrOCR model)"),MTe.forEach(t),$Ke=i(No),bh=n(No,"LI",{});var ETe=s(bh);ZU=n(ETe,"STRONG",{});var Xxr=s(ZU);IKe=r(Xxr,"vision-text-dual-encoder"),Xxr.forEach(t),jKe=r(ETe," \u2014 "),ux=n(ETe,"A",{href:!0});var zxr=s(ux);NKe=r(zxr,"VisionTextDualEncoderProcessor"),zxr.forEach(t),DKe=r(ETe," (VisionTextDualEncoder model)"),ETe.forEach(t),qKe=i(No),vh=n(No,"LI",{});var yTe=s(vh);eJ=n(yTe,"STRONG",{});var Vxr=s(eJ);GKe=r(Vxr,"wav2vec2"),Vxr.forEach(t),OKe=r(yTe," \u2014 "),bx=n(yTe,"A",{href:!0});var Wxr=s(bx);XKe=r(Wxr,"Wav2Vec2Processor"),Wxr.forEach(t),zKe=r(yTe," (Wav2Vec2 model)"),yTe.forEach(t),No.forEach(t),VKe=i(Rt),m(Th.$$.fragment,Rt),WKe=i(Rt),oJ=n(Rt,"P",{});var Qxr=s(oJ);QKe=r(Qxr,"Examples:"),Qxr.forEach(t),HKe=i(Rt),m(jM.$$.fragment,Rt),Rt.forEach(t),UKe=i(js),Fh=n(js,"DIV",{class:!0});var NBe=s(Fh);m(NM.$$.fragment,NBe),JKe=i(NBe),rJ=n(NBe,"P",{});var Hxr=s(rJ);YKe=r(Hxr,"Register a new processor for this class."),Hxr.forEach(t),NBe.forEach(t),js.forEach(t),R7e=i(d),qi=n(d,"H2",{class:!0});var DBe=s(qi);Ch=n(DBe,"A",{id:!0,class:!0,href:!0});var Uxr=s(Ch);tJ=n(Uxr,"SPAN",{});var Jxr=s(tJ);m(DM.$$.fragment,Jxr),Jxr.forEach(t),Uxr.forEach(t),KKe=i(DBe),aJ=n(DBe,"SPAN",{});var Yxr=s(aJ);ZKe=r(Yxr,"AutoModel"),Yxr.forEach(t),DBe.forEach(t),S7e=i(d),Vo=n(d,"DIV",{class:!0});var Ns=s(Vo);m(qM.$$.fragment,Ns),eZe=i(Ns),Gi=n(Ns,"P",{});var oz=s(Gi);oZe=r(oz,`This is a generic model class that will be instantiated as one of the base model classes of the library when created
with the `),nJ=n(oz,"CODE",{});var Kxr=s(nJ);rZe=r(Kxr,"from_pretrained()"),Kxr.forEach(t),tZe=r(oz,"class method or the "),sJ=n(oz,"CODE",{});var Zxr=s(sJ);aZe=r(Zxr,"from_config()"),Zxr.forEach(t),nZe=r(oz,`class
method.`),oz.forEach(t),sZe=i(Ns),GM=n(Ns,"P",{});var qBe=s(GM);lZe=r(qBe,"This class cannot be instantiated directly using "),lJ=n(qBe,"CODE",{});var eRr=s(lJ);iZe=r(eRr,"__init__()"),eRr.forEach(t),dZe=r(qBe," (throws an error)."),qBe.forEach(t),cZe=i(Ns),Nr=n(Ns,"DIV",{class:!0});var Ds=s(Nr);m(OM.$$.fragment,Ds),fZe=i(Ds),iJ=n(Ds,"P",{});var oRr=s(iJ);mZe=r(oRr,"Instantiates one of the base model classes of the library from a configuration."),oRr.forEach(t),gZe=i(Ds),Oi=n(Ds,"P",{});var rz=s(Oi);hZe=r(rz,`Note:
Loading a model from its configuration file does `),dJ=n(rz,"STRONG",{});var rRr=s(dJ);pZe=r(rRr,"not"),rRr.forEach(t),_Ze=r(rz,` load the model weights. It only affects the
model\u2019s configuration. Use `),cJ=n(rz,"CODE",{});var tRr=s(cJ);uZe=r(tRr,"from_pretrained()"),tRr.forEach(t),bZe=r(rz,"to load the model weights."),rz.forEach(t),vZe=i(Ds),fJ=n(Ds,"P",{});var aRr=s(fJ);TZe=r(aRr,"Examples:"),aRr.forEach(t),FZe=i(Ds),m(XM.$$.fragment,Ds),Ds.forEach(t),CZe=i(Ns),ke=n(Ns,"DIV",{class:!0});var St=s(ke);m(zM.$$.fragment,St),MZe=i(St),mJ=n(St,"P",{});var nRr=s(mJ);EZe=r(nRr,"Instantiate one of the base model classes of the library from a pretrained model."),nRr.forEach(t),yZe=i(St),Da=n(St,"P",{});var f4=s(Da);wZe=r(f4,"The model class to instantiate is selected based on the "),gJ=n(f4,"CODE",{});var sRr=s(gJ);AZe=r(sRr,"model_type"),sRr.forEach(t),LZe=r(f4,` property of the config object (either
passed as an argument or loaded from `),hJ=n(f4,"CODE",{});var lRr=s(hJ);BZe=r(lRr,"pretrained_model_name_or_path"),lRr.forEach(t),kZe=r(f4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),pJ=n(f4,"CODE",{});var iRr=s(pJ);xZe=r(iRr,"pretrained_model_name_or_path"),iRr.forEach(t),RZe=r(f4,":"),f4.forEach(t),SZe=i(St),F=n(St,"UL",{});var C=s(F);Mh=n(C,"LI",{});var wTe=s(Mh);_J=n(wTe,"STRONG",{});var dRr=s(_J);PZe=r(dRr,"albert"),dRr.forEach(t),$Ze=r(wTe," \u2014 "),vx=n(wTe,"A",{href:!0});var cRr=s(vx);IZe=r(cRr,"AlbertModel"),cRr.forEach(t),jZe=r(wTe," (ALBERT model)"),wTe.forEach(t),NZe=i(C),Eh=n(C,"LI",{});var ATe=s(Eh);uJ=n(ATe,"STRONG",{});var fRr=s(uJ);DZe=r(fRr,"bart"),fRr.forEach(t),qZe=r(ATe," \u2014 "),Tx=n(ATe,"A",{href:!0});var mRr=s(Tx);GZe=r(mRr,"BartModel"),mRr.forEach(t),OZe=r(ATe," (BART model)"),ATe.forEach(t),XZe=i(C),yh=n(C,"LI",{});var LTe=s(yh);bJ=n(LTe,"STRONG",{});var gRr=s(bJ);zZe=r(gRr,"beit"),gRr.forEach(t),VZe=r(LTe," \u2014 "),Fx=n(LTe,"A",{href:!0});var hRr=s(Fx);WZe=r(hRr,"BeitModel"),hRr.forEach(t),QZe=r(LTe," (BEiT model)"),LTe.forEach(t),HZe=i(C),wh=n(C,"LI",{});var BTe=s(wh);vJ=n(BTe,"STRONG",{});var pRr=s(vJ);UZe=r(pRr,"bert"),pRr.forEach(t),JZe=r(BTe," \u2014 "),Cx=n(BTe,"A",{href:!0});var _Rr=s(Cx);YZe=r(_Rr,"BertModel"),_Rr.forEach(t),KZe=r(BTe," (BERT model)"),BTe.forEach(t),ZZe=i(C),Ah=n(C,"LI",{});var kTe=s(Ah);TJ=n(kTe,"STRONG",{});var uRr=s(TJ);eeo=r(uRr,"bert-generation"),uRr.forEach(t),oeo=r(kTe," \u2014 "),Mx=n(kTe,"A",{href:!0});var bRr=s(Mx);reo=r(bRr,"BertGenerationEncoder"),bRr.forEach(t),teo=r(kTe," (Bert Generation model)"),kTe.forEach(t),aeo=i(C),Lh=n(C,"LI",{});var xTe=s(Lh);FJ=n(xTe,"STRONG",{});var vRr=s(FJ);neo=r(vRr,"big_bird"),vRr.forEach(t),seo=r(xTe," \u2014 "),Ex=n(xTe,"A",{href:!0});var TRr=s(Ex);leo=r(TRr,"BigBirdModel"),TRr.forEach(t),ieo=r(xTe," (BigBird model)"),xTe.forEach(t),deo=i(C),Bh=n(C,"LI",{});var RTe=s(Bh);CJ=n(RTe,"STRONG",{});var FRr=s(CJ);ceo=r(FRr,"bigbird_pegasus"),FRr.forEach(t),feo=r(RTe," \u2014 "),yx=n(RTe,"A",{href:!0});var CRr=s(yx);meo=r(CRr,"BigBirdPegasusModel"),CRr.forEach(t),geo=r(RTe," (BigBirdPegasus model)"),RTe.forEach(t),heo=i(C),kh=n(C,"LI",{});var STe=s(kh);MJ=n(STe,"STRONG",{});var MRr=s(MJ);peo=r(MRr,"blenderbot"),MRr.forEach(t),_eo=r(STe," \u2014 "),wx=n(STe,"A",{href:!0});var ERr=s(wx);ueo=r(ERr,"BlenderbotModel"),ERr.forEach(t),beo=r(STe," (Blenderbot model)"),STe.forEach(t),veo=i(C),xh=n(C,"LI",{});var PTe=s(xh);EJ=n(PTe,"STRONG",{});var yRr=s(EJ);Teo=r(yRr,"blenderbot-small"),yRr.forEach(t),Feo=r(PTe," \u2014 "),Ax=n(PTe,"A",{href:!0});var wRr=s(Ax);Ceo=r(wRr,"BlenderbotSmallModel"),wRr.forEach(t),Meo=r(PTe," (BlenderbotSmall model)"),PTe.forEach(t),Eeo=i(C),Rh=n(C,"LI",{});var $Te=s(Rh);yJ=n($Te,"STRONG",{});var ARr=s(yJ);yeo=r(ARr,"camembert"),ARr.forEach(t),weo=r($Te," \u2014 "),Lx=n($Te,"A",{href:!0});var LRr=s(Lx);Aeo=r(LRr,"CamembertModel"),LRr.forEach(t),Leo=r($Te," (CamemBERT model)"),$Te.forEach(t),Beo=i(C),Sh=n(C,"LI",{});var ITe=s(Sh);wJ=n(ITe,"STRONG",{});var BRr=s(wJ);keo=r(BRr,"canine"),BRr.forEach(t),xeo=r(ITe," \u2014 "),Bx=n(ITe,"A",{href:!0});var kRr=s(Bx);Reo=r(kRr,"CanineModel"),kRr.forEach(t),Seo=r(ITe," (Canine model)"),ITe.forEach(t),Peo=i(C),Ph=n(C,"LI",{});var jTe=s(Ph);AJ=n(jTe,"STRONG",{});var xRr=s(AJ);$eo=r(xRr,"clip"),xRr.forEach(t),Ieo=r(jTe," \u2014 "),kx=n(jTe,"A",{href:!0});var RRr=s(kx);jeo=r(RRr,"CLIPModel"),RRr.forEach(t),Neo=r(jTe," (CLIP model)"),jTe.forEach(t),Deo=i(C),$h=n(C,"LI",{});var NTe=s($h);LJ=n(NTe,"STRONG",{});var SRr=s(LJ);qeo=r(SRr,"convbert"),SRr.forEach(t),Geo=r(NTe," \u2014 "),xx=n(NTe,"A",{href:!0});var PRr=s(xx);Oeo=r(PRr,"ConvBertModel"),PRr.forEach(t),Xeo=r(NTe," (ConvBERT model)"),NTe.forEach(t),zeo=i(C),Ih=n(C,"LI",{});var DTe=s(Ih);BJ=n(DTe,"STRONG",{});var $Rr=s(BJ);Veo=r($Rr,"convnext"),$Rr.forEach(t),Weo=r(DTe," \u2014 "),Rx=n(DTe,"A",{href:!0});var IRr=s(Rx);Qeo=r(IRr,"ConvNextModel"),IRr.forEach(t),Heo=r(DTe," (ConvNext model)"),DTe.forEach(t),Ueo=i(C),jh=n(C,"LI",{});var qTe=s(jh);kJ=n(qTe,"STRONG",{});var jRr=s(kJ);Jeo=r(jRr,"ctrl"),jRr.forEach(t),Yeo=r(qTe," \u2014 "),Sx=n(qTe,"A",{href:!0});var NRr=s(Sx);Keo=r(NRr,"CTRLModel"),NRr.forEach(t),Zeo=r(qTe," (CTRL model)"),qTe.forEach(t),eoo=i(C),Nh=n(C,"LI",{});var GTe=s(Nh);xJ=n(GTe,"STRONG",{});var DRr=s(xJ);ooo=r(DRr,"deberta"),DRr.forEach(t),roo=r(GTe," \u2014 "),Px=n(GTe,"A",{href:!0});var qRr=s(Px);too=r(qRr,"DebertaModel"),qRr.forEach(t),aoo=r(GTe," (DeBERTa model)"),GTe.forEach(t),noo=i(C),Dh=n(C,"LI",{});var OTe=s(Dh);RJ=n(OTe,"STRONG",{});var GRr=s(RJ);soo=r(GRr,"deberta-v2"),GRr.forEach(t),loo=r(OTe," \u2014 "),$x=n(OTe,"A",{href:!0});var ORr=s($x);ioo=r(ORr,"DebertaV2Model"),ORr.forEach(t),doo=r(OTe," (DeBERTa-v2 model)"),OTe.forEach(t),coo=i(C),qh=n(C,"LI",{});var XTe=s(qh);SJ=n(XTe,"STRONG",{});var XRr=s(SJ);foo=r(XRr,"deit"),XRr.forEach(t),moo=r(XTe," \u2014 "),Ix=n(XTe,"A",{href:!0});var zRr=s(Ix);goo=r(zRr,"DeiTModel"),zRr.forEach(t),hoo=r(XTe," (DeiT model)"),XTe.forEach(t),poo=i(C),Gh=n(C,"LI",{});var zTe=s(Gh);PJ=n(zTe,"STRONG",{});var VRr=s(PJ);_oo=r(VRr,"detr"),VRr.forEach(t),uoo=r(zTe," \u2014 "),jx=n(zTe,"A",{href:!0});var WRr=s(jx);boo=r(WRr,"DetrModel"),WRr.forEach(t),voo=r(zTe," (DETR model)"),zTe.forEach(t),Too=i(C),Oh=n(C,"LI",{});var VTe=s(Oh);$J=n(VTe,"STRONG",{});var QRr=s($J);Foo=r(QRr,"distilbert"),QRr.forEach(t),Coo=r(VTe," \u2014 "),Nx=n(VTe,"A",{href:!0});var HRr=s(Nx);Moo=r(HRr,"DistilBertModel"),HRr.forEach(t),Eoo=r(VTe," (DistilBERT model)"),VTe.forEach(t),yoo=i(C),Xh=n(C,"LI",{});var WTe=s(Xh);IJ=n(WTe,"STRONG",{});var URr=s(IJ);woo=r(URr,"dpr"),URr.forEach(t),Aoo=r(WTe," \u2014 "),Dx=n(WTe,"A",{href:!0});var JRr=s(Dx);Loo=r(JRr,"DPRQuestionEncoder"),JRr.forEach(t),Boo=r(WTe," (DPR model)"),WTe.forEach(t),koo=i(C),zh=n(C,"LI",{});var QTe=s(zh);jJ=n(QTe,"STRONG",{});var YRr=s(jJ);xoo=r(YRr,"electra"),YRr.forEach(t),Roo=r(QTe," \u2014 "),qx=n(QTe,"A",{href:!0});var KRr=s(qx);Soo=r(KRr,"ElectraModel"),KRr.forEach(t),Poo=r(QTe," (ELECTRA model)"),QTe.forEach(t),$oo=i(C),Vh=n(C,"LI",{});var HTe=s(Vh);NJ=n(HTe,"STRONG",{});var ZRr=s(NJ);Ioo=r(ZRr,"flaubert"),ZRr.forEach(t),joo=r(HTe," \u2014 "),Gx=n(HTe,"A",{href:!0});var eSr=s(Gx);Noo=r(eSr,"FlaubertModel"),eSr.forEach(t),Doo=r(HTe," (FlauBERT model)"),HTe.forEach(t),qoo=i(C),Wh=n(C,"LI",{});var UTe=s(Wh);DJ=n(UTe,"STRONG",{});var oSr=s(DJ);Goo=r(oSr,"fnet"),oSr.forEach(t),Ooo=r(UTe," \u2014 "),Ox=n(UTe,"A",{href:!0});var rSr=s(Ox);Xoo=r(rSr,"FNetModel"),rSr.forEach(t),zoo=r(UTe," (FNet model)"),UTe.forEach(t),Voo=i(C),Qh=n(C,"LI",{});var JTe=s(Qh);qJ=n(JTe,"STRONG",{});var tSr=s(qJ);Woo=r(tSr,"fsmt"),tSr.forEach(t),Qoo=r(JTe," \u2014 "),Xx=n(JTe,"A",{href:!0});var aSr=s(Xx);Hoo=r(aSr,"FSMTModel"),aSr.forEach(t),Uoo=r(JTe," (FairSeq Machine-Translation model)"),JTe.forEach(t),Joo=i(C),xs=n(C,"LI",{});var GL=s(xs);GJ=n(GL,"STRONG",{});var nSr=s(GJ);Yoo=r(nSr,"funnel"),nSr.forEach(t),Koo=r(GL," \u2014 "),zx=n(GL,"A",{href:!0});var sSr=s(zx);Zoo=r(sSr,"FunnelModel"),sSr.forEach(t),ero=r(GL," or "),Vx=n(GL,"A",{href:!0});var lSr=s(Vx);oro=r(lSr,"FunnelBaseModel"),lSr.forEach(t),rro=r(GL," (Funnel Transformer model)"),GL.forEach(t),tro=i(C),Hh=n(C,"LI",{});var YTe=s(Hh);OJ=n(YTe,"STRONG",{});var iSr=s(OJ);aro=r(iSr,"gpt2"),iSr.forEach(t),nro=r(YTe," \u2014 "),Wx=n(YTe,"A",{href:!0});var dSr=s(Wx);sro=r(dSr,"GPT2Model"),dSr.forEach(t),lro=r(YTe," (OpenAI GPT-2 model)"),YTe.forEach(t),iro=i(C),Uh=n(C,"LI",{});var KTe=s(Uh);XJ=n(KTe,"STRONG",{});var cSr=s(XJ);dro=r(cSr,"gpt_neo"),cSr.forEach(t),cro=r(KTe," \u2014 "),Qx=n(KTe,"A",{href:!0});var fSr=s(Qx);fro=r(fSr,"GPTNeoModel"),fSr.forEach(t),mro=r(KTe," (GPT Neo model)"),KTe.forEach(t),gro=i(C),Jh=n(C,"LI",{});var ZTe=s(Jh);zJ=n(ZTe,"STRONG",{});var mSr=s(zJ);hro=r(mSr,"gptj"),mSr.forEach(t),pro=r(ZTe," \u2014 "),Hx=n(ZTe,"A",{href:!0});var gSr=s(Hx);_ro=r(gSr,"GPTJModel"),gSr.forEach(t),uro=r(ZTe," (GPT-J model)"),ZTe.forEach(t),bro=i(C),Yh=n(C,"LI",{});var e8e=s(Yh);VJ=n(e8e,"STRONG",{});var hSr=s(VJ);vro=r(hSr,"hubert"),hSr.forEach(t),Tro=r(e8e," \u2014 "),Ux=n(e8e,"A",{href:!0});var pSr=s(Ux);Fro=r(pSr,"HubertModel"),pSr.forEach(t),Cro=r(e8e," (Hubert model)"),e8e.forEach(t),Mro=i(C),Kh=n(C,"LI",{});var o8e=s(Kh);WJ=n(o8e,"STRONG",{});var _Sr=s(WJ);Ero=r(_Sr,"ibert"),_Sr.forEach(t),yro=r(o8e," \u2014 "),Jx=n(o8e,"A",{href:!0});var uSr=s(Jx);wro=r(uSr,"IBertModel"),uSr.forEach(t),Aro=r(o8e," (I-BERT model)"),o8e.forEach(t),Lro=i(C),Zh=n(C,"LI",{});var r8e=s(Zh);QJ=n(r8e,"STRONG",{});var bSr=s(QJ);Bro=r(bSr,"imagegpt"),bSr.forEach(t),kro=r(r8e," \u2014 "),Yx=n(r8e,"A",{href:!0});var vSr=s(Yx);xro=r(vSr,"ImageGPTModel"),vSr.forEach(t),Rro=r(r8e," (ImageGPT model)"),r8e.forEach(t),Sro=i(C),ep=n(C,"LI",{});var t8e=s(ep);HJ=n(t8e,"STRONG",{});var TSr=s(HJ);Pro=r(TSr,"layoutlm"),TSr.forEach(t),$ro=r(t8e," \u2014 "),Kx=n(t8e,"A",{href:!0});var FSr=s(Kx);Iro=r(FSr,"LayoutLMModel"),FSr.forEach(t),jro=r(t8e," (LayoutLM model)"),t8e.forEach(t),Nro=i(C),op=n(C,"LI",{});var a8e=s(op);UJ=n(a8e,"STRONG",{});var CSr=s(UJ);Dro=r(CSr,"layoutlmv2"),CSr.forEach(t),qro=r(a8e," \u2014 "),Zx=n(a8e,"A",{href:!0});var MSr=s(Zx);Gro=r(MSr,"LayoutLMv2Model"),MSr.forEach(t),Oro=r(a8e," (LayoutLMv2 model)"),a8e.forEach(t),Xro=i(C),rp=n(C,"LI",{});var n8e=s(rp);JJ=n(n8e,"STRONG",{});var ESr=s(JJ);zro=r(ESr,"led"),ESr.forEach(t),Vro=r(n8e," \u2014 "),eR=n(n8e,"A",{href:!0});var ySr=s(eR);Wro=r(ySr,"LEDModel"),ySr.forEach(t),Qro=r(n8e," (LED model)"),n8e.forEach(t),Hro=i(C),tp=n(C,"LI",{});var s8e=s(tp);YJ=n(s8e,"STRONG",{});var wSr=s(YJ);Uro=r(wSr,"longformer"),wSr.forEach(t),Jro=r(s8e," \u2014 "),oR=n(s8e,"A",{href:!0});var ASr=s(oR);Yro=r(ASr,"LongformerModel"),ASr.forEach(t),Kro=r(s8e," (Longformer model)"),s8e.forEach(t),Zro=i(C),ap=n(C,"LI",{});var l8e=s(ap);KJ=n(l8e,"STRONG",{});var LSr=s(KJ);eto=r(LSr,"luke"),LSr.forEach(t),oto=r(l8e," \u2014 "),rR=n(l8e,"A",{href:!0});var BSr=s(rR);rto=r(BSr,"LukeModel"),BSr.forEach(t),tto=r(l8e," (LUKE model)"),l8e.forEach(t),ato=i(C),np=n(C,"LI",{});var i8e=s(np);ZJ=n(i8e,"STRONG",{});var kSr=s(ZJ);nto=r(kSr,"lxmert"),kSr.forEach(t),sto=r(i8e," \u2014 "),tR=n(i8e,"A",{href:!0});var xSr=s(tR);lto=r(xSr,"LxmertModel"),xSr.forEach(t),ito=r(i8e," (LXMERT model)"),i8e.forEach(t),dto=i(C),sp=n(C,"LI",{});var d8e=s(sp);eY=n(d8e,"STRONG",{});var RSr=s(eY);cto=r(RSr,"m2m_100"),RSr.forEach(t),fto=r(d8e," \u2014 "),aR=n(d8e,"A",{href:!0});var SSr=s(aR);mto=r(SSr,"M2M100Model"),SSr.forEach(t),gto=r(d8e," (M2M100 model)"),d8e.forEach(t),hto=i(C),lp=n(C,"LI",{});var c8e=s(lp);oY=n(c8e,"STRONG",{});var PSr=s(oY);pto=r(PSr,"marian"),PSr.forEach(t),_to=r(c8e," \u2014 "),nR=n(c8e,"A",{href:!0});var $Sr=s(nR);uto=r($Sr,"MarianModel"),$Sr.forEach(t),bto=r(c8e," (Marian model)"),c8e.forEach(t),vto=i(C),ip=n(C,"LI",{});var f8e=s(ip);rY=n(f8e,"STRONG",{});var ISr=s(rY);Tto=r(ISr,"maskformer"),ISr.forEach(t),Fto=r(f8e," \u2014 "),sR=n(f8e,"A",{href:!0});var jSr=s(sR);Cto=r(jSr,"MaskFormerModel"),jSr.forEach(t),Mto=r(f8e," (MaskFormer model)"),f8e.forEach(t),Eto=i(C),dp=n(C,"LI",{});var m8e=s(dp);tY=n(m8e,"STRONG",{});var NSr=s(tY);yto=r(NSr,"mbart"),NSr.forEach(t),wto=r(m8e," \u2014 "),lR=n(m8e,"A",{href:!0});var DSr=s(lR);Ato=r(DSr,"MBartModel"),DSr.forEach(t),Lto=r(m8e," (mBART model)"),m8e.forEach(t),Bto=i(C),cp=n(C,"LI",{});var g8e=s(cp);aY=n(g8e,"STRONG",{});var qSr=s(aY);kto=r(qSr,"megatron-bert"),qSr.forEach(t),xto=r(g8e," \u2014 "),iR=n(g8e,"A",{href:!0});var GSr=s(iR);Rto=r(GSr,"MegatronBertModel"),GSr.forEach(t),Sto=r(g8e," (MegatronBert model)"),g8e.forEach(t),Pto=i(C),fp=n(C,"LI",{});var h8e=s(fp);nY=n(h8e,"STRONG",{});var OSr=s(nY);$to=r(OSr,"mobilebert"),OSr.forEach(t),Ito=r(h8e," \u2014 "),dR=n(h8e,"A",{href:!0});var XSr=s(dR);jto=r(XSr,"MobileBertModel"),XSr.forEach(t),Nto=r(h8e," (MobileBERT model)"),h8e.forEach(t),Dto=i(C),mp=n(C,"LI",{});var p8e=s(mp);sY=n(p8e,"STRONG",{});var zSr=s(sY);qto=r(zSr,"mpnet"),zSr.forEach(t),Gto=r(p8e," \u2014 "),cR=n(p8e,"A",{href:!0});var VSr=s(cR);Oto=r(VSr,"MPNetModel"),VSr.forEach(t),Xto=r(p8e," (MPNet model)"),p8e.forEach(t),zto=i(C),gp=n(C,"LI",{});var _8e=s(gp);lY=n(_8e,"STRONG",{});var WSr=s(lY);Vto=r(WSr,"mt5"),WSr.forEach(t),Wto=r(_8e," \u2014 "),fR=n(_8e,"A",{href:!0});var QSr=s(fR);Qto=r(QSr,"MT5Model"),QSr.forEach(t),Hto=r(_8e," (mT5 model)"),_8e.forEach(t),Uto=i(C),hp=n(C,"LI",{});var u8e=s(hp);iY=n(u8e,"STRONG",{});var HSr=s(iY);Jto=r(HSr,"nystromformer"),HSr.forEach(t),Yto=r(u8e," \u2014 "),mR=n(u8e,"A",{href:!0});var USr=s(mR);Kto=r(USr,"NystromformerModel"),USr.forEach(t),Zto=r(u8e," (Nystromformer model)"),u8e.forEach(t),eao=i(C),pp=n(C,"LI",{});var b8e=s(pp);dY=n(b8e,"STRONG",{});var JSr=s(dY);oao=r(JSr,"openai-gpt"),JSr.forEach(t),rao=r(b8e," \u2014 "),gR=n(b8e,"A",{href:!0});var YSr=s(gR);tao=r(YSr,"OpenAIGPTModel"),YSr.forEach(t),aao=r(b8e," (OpenAI GPT model)"),b8e.forEach(t),nao=i(C),_p=n(C,"LI",{});var v8e=s(_p);cY=n(v8e,"STRONG",{});var KSr=s(cY);sao=r(KSr,"pegasus"),KSr.forEach(t),lao=r(v8e," \u2014 "),hR=n(v8e,"A",{href:!0});var ZSr=s(hR);iao=r(ZSr,"PegasusModel"),ZSr.forEach(t),dao=r(v8e," (Pegasus model)"),v8e.forEach(t),cao=i(C),up=n(C,"LI",{});var T8e=s(up);fY=n(T8e,"STRONG",{});var ePr=s(fY);fao=r(ePr,"perceiver"),ePr.forEach(t),mao=r(T8e," \u2014 "),pR=n(T8e,"A",{href:!0});var oPr=s(pR);gao=r(oPr,"PerceiverModel"),oPr.forEach(t),hao=r(T8e," (Perceiver model)"),T8e.forEach(t),pao=i(C),bp=n(C,"LI",{});var F8e=s(bp);mY=n(F8e,"STRONG",{});var rPr=s(mY);_ao=r(rPr,"plbart"),rPr.forEach(t),uao=r(F8e," \u2014 "),_R=n(F8e,"A",{href:!0});var tPr=s(_R);bao=r(tPr,"PLBartModel"),tPr.forEach(t),vao=r(F8e," (PLBart model)"),F8e.forEach(t),Tao=i(C),vp=n(C,"LI",{});var C8e=s(vp);gY=n(C8e,"STRONG",{});var aPr=s(gY);Fao=r(aPr,"poolformer"),aPr.forEach(t),Cao=r(C8e," \u2014 "),uR=n(C8e,"A",{href:!0});var nPr=s(uR);Mao=r(nPr,"PoolFormerModel"),nPr.forEach(t),Eao=r(C8e," (PoolFormer model)"),C8e.forEach(t),yao=i(C),Tp=n(C,"LI",{});var M8e=s(Tp);hY=n(M8e,"STRONG",{});var sPr=s(hY);wao=r(sPr,"prophetnet"),sPr.forEach(t),Aao=r(M8e," \u2014 "),bR=n(M8e,"A",{href:!0});var lPr=s(bR);Lao=r(lPr,"ProphetNetModel"),lPr.forEach(t),Bao=r(M8e," (ProphetNet model)"),M8e.forEach(t),kao=i(C),Fp=n(C,"LI",{});var E8e=s(Fp);pY=n(E8e,"STRONG",{});var iPr=s(pY);xao=r(iPr,"qdqbert"),iPr.forEach(t),Rao=r(E8e," \u2014 "),vR=n(E8e,"A",{href:!0});var dPr=s(vR);Sao=r(dPr,"QDQBertModel"),dPr.forEach(t),Pao=r(E8e," (QDQBert model)"),E8e.forEach(t),$ao=i(C),Cp=n(C,"LI",{});var y8e=s(Cp);_Y=n(y8e,"STRONG",{});var cPr=s(_Y);Iao=r(cPr,"reformer"),cPr.forEach(t),jao=r(y8e," \u2014 "),TR=n(y8e,"A",{href:!0});var fPr=s(TR);Nao=r(fPr,"ReformerModel"),fPr.forEach(t),Dao=r(y8e," (Reformer model)"),y8e.forEach(t),qao=i(C),Mp=n(C,"LI",{});var w8e=s(Mp);uY=n(w8e,"STRONG",{});var mPr=s(uY);Gao=r(mPr,"rembert"),mPr.forEach(t),Oao=r(w8e," \u2014 "),FR=n(w8e,"A",{href:!0});var gPr=s(FR);Xao=r(gPr,"RemBertModel"),gPr.forEach(t),zao=r(w8e," (RemBERT model)"),w8e.forEach(t),Vao=i(C),Ep=n(C,"LI",{});var A8e=s(Ep);bY=n(A8e,"STRONG",{});var hPr=s(bY);Wao=r(hPr,"retribert"),hPr.forEach(t),Qao=r(A8e," \u2014 "),CR=n(A8e,"A",{href:!0});var pPr=s(CR);Hao=r(pPr,"RetriBertModel"),pPr.forEach(t),Uao=r(A8e," (RetriBERT model)"),A8e.forEach(t),Jao=i(C),yp=n(C,"LI",{});var L8e=s(yp);vY=n(L8e,"STRONG",{});var _Pr=s(vY);Yao=r(_Pr,"roberta"),_Pr.forEach(t),Kao=r(L8e," \u2014 "),MR=n(L8e,"A",{href:!0});var uPr=s(MR);Zao=r(uPr,"RobertaModel"),uPr.forEach(t),eno=r(L8e," (RoBERTa model)"),L8e.forEach(t),ono=i(C),wp=n(C,"LI",{});var B8e=s(wp);TY=n(B8e,"STRONG",{});var bPr=s(TY);rno=r(bPr,"roformer"),bPr.forEach(t),tno=r(B8e," \u2014 "),ER=n(B8e,"A",{href:!0});var vPr=s(ER);ano=r(vPr,"RoFormerModel"),vPr.forEach(t),nno=r(B8e," (RoFormer model)"),B8e.forEach(t),sno=i(C),Ap=n(C,"LI",{});var k8e=s(Ap);FY=n(k8e,"STRONG",{});var TPr=s(FY);lno=r(TPr,"segformer"),TPr.forEach(t),ino=r(k8e," \u2014 "),yR=n(k8e,"A",{href:!0});var FPr=s(yR);dno=r(FPr,"SegformerModel"),FPr.forEach(t),cno=r(k8e," (SegFormer model)"),k8e.forEach(t),fno=i(C),Lp=n(C,"LI",{});var x8e=s(Lp);CY=n(x8e,"STRONG",{});var CPr=s(CY);mno=r(CPr,"sew"),CPr.forEach(t),gno=r(x8e," \u2014 "),wR=n(x8e,"A",{href:!0});var MPr=s(wR);hno=r(MPr,"SEWModel"),MPr.forEach(t),pno=r(x8e," (SEW model)"),x8e.forEach(t),_no=i(C),Bp=n(C,"LI",{});var R8e=s(Bp);MY=n(R8e,"STRONG",{});var EPr=s(MY);uno=r(EPr,"sew-d"),EPr.forEach(t),bno=r(R8e," \u2014 "),AR=n(R8e,"A",{href:!0});var yPr=s(AR);vno=r(yPr,"SEWDModel"),yPr.forEach(t),Tno=r(R8e," (SEW-D model)"),R8e.forEach(t),Fno=i(C),kp=n(C,"LI",{});var S8e=s(kp);EY=n(S8e,"STRONG",{});var wPr=s(EY);Cno=r(wPr,"speech_to_text"),wPr.forEach(t),Mno=r(S8e," \u2014 "),LR=n(S8e,"A",{href:!0});var APr=s(LR);Eno=r(APr,"Speech2TextModel"),APr.forEach(t),yno=r(S8e," (Speech2Text model)"),S8e.forEach(t),wno=i(C),xp=n(C,"LI",{});var P8e=s(xp);yY=n(P8e,"STRONG",{});var LPr=s(yY);Ano=r(LPr,"splinter"),LPr.forEach(t),Lno=r(P8e," \u2014 "),BR=n(P8e,"A",{href:!0});var BPr=s(BR);Bno=r(BPr,"SplinterModel"),BPr.forEach(t),kno=r(P8e," (Splinter model)"),P8e.forEach(t),xno=i(C),Rp=n(C,"LI",{});var $8e=s(Rp);wY=n($8e,"STRONG",{});var kPr=s(wY);Rno=r(kPr,"squeezebert"),kPr.forEach(t),Sno=r($8e," \u2014 "),kR=n($8e,"A",{href:!0});var xPr=s(kR);Pno=r(xPr,"SqueezeBertModel"),xPr.forEach(t),$no=r($8e," (SqueezeBERT model)"),$8e.forEach(t),Ino=i(C),Sp=n(C,"LI",{});var I8e=s(Sp);AY=n(I8e,"STRONG",{});var RPr=s(AY);jno=r(RPr,"swin"),RPr.forEach(t),Nno=r(I8e," \u2014 "),xR=n(I8e,"A",{href:!0});var SPr=s(xR);Dno=r(SPr,"SwinModel"),SPr.forEach(t),qno=r(I8e," (Swin model)"),I8e.forEach(t),Gno=i(C),Pp=n(C,"LI",{});var j8e=s(Pp);LY=n(j8e,"STRONG",{});var PPr=s(LY);Ono=r(PPr,"t5"),PPr.forEach(t),Xno=r(j8e," \u2014 "),RR=n(j8e,"A",{href:!0});var $Pr=s(RR);zno=r($Pr,"T5Model"),$Pr.forEach(t),Vno=r(j8e," (T5 model)"),j8e.forEach(t),Wno=i(C),$p=n(C,"LI",{});var N8e=s($p);BY=n(N8e,"STRONG",{});var IPr=s(BY);Qno=r(IPr,"tapas"),IPr.forEach(t),Hno=r(N8e," \u2014 "),SR=n(N8e,"A",{href:!0});var jPr=s(SR);Uno=r(jPr,"TapasModel"),jPr.forEach(t),Jno=r(N8e," (TAPAS model)"),N8e.forEach(t),Yno=i(C),Ip=n(C,"LI",{});var D8e=s(Ip);kY=n(D8e,"STRONG",{});var NPr=s(kY);Kno=r(NPr,"transfo-xl"),NPr.forEach(t),Zno=r(D8e," \u2014 "),PR=n(D8e,"A",{href:!0});var DPr=s(PR);eso=r(DPr,"TransfoXLModel"),DPr.forEach(t),oso=r(D8e," (Transformer-XL model)"),D8e.forEach(t),rso=i(C),jp=n(C,"LI",{});var q8e=s(jp);xY=n(q8e,"STRONG",{});var qPr=s(xY);tso=r(qPr,"unispeech"),qPr.forEach(t),aso=r(q8e," \u2014 "),$R=n(q8e,"A",{href:!0});var GPr=s($R);nso=r(GPr,"UniSpeechModel"),GPr.forEach(t),sso=r(q8e," (UniSpeech model)"),q8e.forEach(t),lso=i(C),Np=n(C,"LI",{});var G8e=s(Np);RY=n(G8e,"STRONG",{});var OPr=s(RY);iso=r(OPr,"unispeech-sat"),OPr.forEach(t),dso=r(G8e," \u2014 "),IR=n(G8e,"A",{href:!0});var XPr=s(IR);cso=r(XPr,"UniSpeechSatModel"),XPr.forEach(t),fso=r(G8e," (UniSpeechSat model)"),G8e.forEach(t),mso=i(C),Dp=n(C,"LI",{});var O8e=s(Dp);SY=n(O8e,"STRONG",{});var zPr=s(SY);gso=r(zPr,"vilt"),zPr.forEach(t),hso=r(O8e," \u2014 "),jR=n(O8e,"A",{href:!0});var VPr=s(jR);pso=r(VPr,"ViltModel"),VPr.forEach(t),_so=r(O8e," (ViLT model)"),O8e.forEach(t),uso=i(C),qp=n(C,"LI",{});var X8e=s(qp);PY=n(X8e,"STRONG",{});var WPr=s(PY);bso=r(WPr,"vision-text-dual-encoder"),WPr.forEach(t),vso=r(X8e," \u2014 "),NR=n(X8e,"A",{href:!0});var QPr=s(NR);Tso=r(QPr,"VisionTextDualEncoderModel"),QPr.forEach(t),Fso=r(X8e," (VisionTextDualEncoder model)"),X8e.forEach(t),Cso=i(C),Gp=n(C,"LI",{});var z8e=s(Gp);$Y=n(z8e,"STRONG",{});var HPr=s($Y);Mso=r(HPr,"visual_bert"),HPr.forEach(t),Eso=r(z8e," \u2014 "),DR=n(z8e,"A",{href:!0});var UPr=s(DR);yso=r(UPr,"VisualBertModel"),UPr.forEach(t),wso=r(z8e," (VisualBert model)"),z8e.forEach(t),Aso=i(C),Op=n(C,"LI",{});var V8e=s(Op);IY=n(V8e,"STRONG",{});var JPr=s(IY);Lso=r(JPr,"vit"),JPr.forEach(t),Bso=r(V8e," \u2014 "),qR=n(V8e,"A",{href:!0});var YPr=s(qR);kso=r(YPr,"ViTModel"),YPr.forEach(t),xso=r(V8e," (ViT model)"),V8e.forEach(t),Rso=i(C),Xp=n(C,"LI",{});var W8e=s(Xp);jY=n(W8e,"STRONG",{});var KPr=s(jY);Sso=r(KPr,"vit_mae"),KPr.forEach(t),Pso=r(W8e," \u2014 "),GR=n(W8e,"A",{href:!0});var ZPr=s(GR);$so=r(ZPr,"ViTMAEModel"),ZPr.forEach(t),Iso=r(W8e," (ViTMAE model)"),W8e.forEach(t),jso=i(C),zp=n(C,"LI",{});var Q8e=s(zp);NY=n(Q8e,"STRONG",{});var e$r=s(NY);Nso=r(e$r,"wav2vec2"),e$r.forEach(t),Dso=r(Q8e," \u2014 "),OR=n(Q8e,"A",{href:!0});var o$r=s(OR);qso=r(o$r,"Wav2Vec2Model"),o$r.forEach(t),Gso=r(Q8e," (Wav2Vec2 model)"),Q8e.forEach(t),Oso=i(C),Vp=n(C,"LI",{});var H8e=s(Vp);DY=n(H8e,"STRONG",{});var r$r=s(DY);Xso=r(r$r,"wavlm"),r$r.forEach(t),zso=r(H8e," \u2014 "),XR=n(H8e,"A",{href:!0});var t$r=s(XR);Vso=r(t$r,"WavLMModel"),t$r.forEach(t),Wso=r(H8e," (WavLM model)"),H8e.forEach(t),Qso=i(C),Wp=n(C,"LI",{});var U8e=s(Wp);qY=n(U8e,"STRONG",{});var a$r=s(qY);Hso=r(a$r,"xglm"),a$r.forEach(t),Uso=r(U8e," \u2014 "),zR=n(U8e,"A",{href:!0});var n$r=s(zR);Jso=r(n$r,"XGLMModel"),n$r.forEach(t),Yso=r(U8e," (XGLM model)"),U8e.forEach(t),Kso=i(C),Qp=n(C,"LI",{});var J8e=s(Qp);GY=n(J8e,"STRONG",{});var s$r=s(GY);Zso=r(s$r,"xlm"),s$r.forEach(t),elo=r(J8e," \u2014 "),VR=n(J8e,"A",{href:!0});var l$r=s(VR);olo=r(l$r,"XLMModel"),l$r.forEach(t),rlo=r(J8e," (XLM model)"),J8e.forEach(t),tlo=i(C),Hp=n(C,"LI",{});var Y8e=s(Hp);OY=n(Y8e,"STRONG",{});var i$r=s(OY);alo=r(i$r,"xlm-prophetnet"),i$r.forEach(t),nlo=r(Y8e," \u2014 "),WR=n(Y8e,"A",{href:!0});var d$r=s(WR);slo=r(d$r,"XLMProphetNetModel"),d$r.forEach(t),llo=r(Y8e," (XLMProphetNet model)"),Y8e.forEach(t),ilo=i(C),Up=n(C,"LI",{});var K8e=s(Up);XY=n(K8e,"STRONG",{});var c$r=s(XY);dlo=r(c$r,"xlm-roberta"),c$r.forEach(t),clo=r(K8e," \u2014 "),QR=n(K8e,"A",{href:!0});var f$r=s(QR);flo=r(f$r,"XLMRobertaModel"),f$r.forEach(t),mlo=r(K8e," (XLM-RoBERTa model)"),K8e.forEach(t),glo=i(C),Jp=n(C,"LI",{});var Z8e=s(Jp);zY=n(Z8e,"STRONG",{});var m$r=s(zY);hlo=r(m$r,"xlm-roberta-xl"),m$r.forEach(t),plo=r(Z8e," \u2014 "),HR=n(Z8e,"A",{href:!0});var g$r=s(HR);_lo=r(g$r,"XLMRobertaXLModel"),g$r.forEach(t),ulo=r(Z8e," (XLM-RoBERTa-XL model)"),Z8e.forEach(t),blo=i(C),Yp=n(C,"LI",{});var eFe=s(Yp);VY=n(eFe,"STRONG",{});var h$r=s(VY);vlo=r(h$r,"xlnet"),h$r.forEach(t),Tlo=r(eFe," \u2014 "),UR=n(eFe,"A",{href:!0});var p$r=s(UR);Flo=r(p$r,"XLNetModel"),p$r.forEach(t),Clo=r(eFe," (XLNet model)"),eFe.forEach(t),Mlo=i(C),Kp=n(C,"LI",{});var oFe=s(Kp);WY=n(oFe,"STRONG",{});var _$r=s(WY);Elo=r(_$r,"yoso"),_$r.forEach(t),ylo=r(oFe," \u2014 "),JR=n(oFe,"A",{href:!0});var u$r=s(JR);wlo=r(u$r,"YosoModel"),u$r.forEach(t),Alo=r(oFe," (YOSO model)"),oFe.forEach(t),C.forEach(t),Llo=i(St),Zp=n(St,"P",{});var rFe=s(Zp);Blo=r(rFe,"The model is set in evaluation mode by default using "),QY=n(rFe,"CODE",{});var b$r=s(QY);klo=r(b$r,"model.eval()"),b$r.forEach(t),xlo=r(rFe,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),HY=n(rFe,"CODE",{});var v$r=s(HY);Rlo=r(v$r,"model.train()"),v$r.forEach(t),rFe.forEach(t),Slo=i(St),UY=n(St,"P",{});var T$r=s(UY);Plo=r(T$r,"Examples:"),T$r.forEach(t),$lo=i(St),m(VM.$$.fragment,St),St.forEach(t),Ns.forEach(t),P7e=i(d),Xi=n(d,"H2",{class:!0});var GBe=s(Xi);e_=n(GBe,"A",{id:!0,class:!0,href:!0});var F$r=s(e_);JY=n(F$r,"SPAN",{});var C$r=s(JY);m(WM.$$.fragment,C$r),C$r.forEach(t),F$r.forEach(t),Ilo=i(GBe),YY=n(GBe,"SPAN",{});var M$r=s(YY);jlo=r(M$r,"AutoModelForPreTraining"),M$r.forEach(t),GBe.forEach(t),$7e=i(d),Wo=n(d,"DIV",{class:!0});var qs=s(Wo);m(QM.$$.fragment,qs),Nlo=i(qs),zi=n(qs,"P",{});var tz=s(zi);Dlo=r(tz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a pretraining head) when created
with the `),KY=n(tz,"CODE",{});var E$r=s(KY);qlo=r(E$r,"from_pretrained()"),E$r.forEach(t),Glo=r(tz,"class method or the "),ZY=n(tz,"CODE",{});var y$r=s(ZY);Olo=r(y$r,"from_config()"),y$r.forEach(t),Xlo=r(tz,`class
method.`),tz.forEach(t),zlo=i(qs),HM=n(qs,"P",{});var OBe=s(HM);Vlo=r(OBe,"This class cannot be instantiated directly using "),eK=n(OBe,"CODE",{});var w$r=s(eK);Wlo=r(w$r,"__init__()"),w$r.forEach(t),Qlo=r(OBe," (throws an error)."),OBe.forEach(t),Hlo=i(qs),Dr=n(qs,"DIV",{class:!0});var Gs=s(Dr);m(UM.$$.fragment,Gs),Ulo=i(Gs),oK=n(Gs,"P",{});var A$r=s(oK);Jlo=r(A$r,"Instantiates one of the model classes of the library (with a pretraining head) from a configuration."),A$r.forEach(t),Ylo=i(Gs),Vi=n(Gs,"P",{});var az=s(Vi);Klo=r(az,`Note:
Loading a model from its configuration file does `),rK=n(az,"STRONG",{});var L$r=s(rK);Zlo=r(L$r,"not"),L$r.forEach(t),eio=r(az,` load the model weights. It only affects the
model\u2019s configuration. Use `),tK=n(az,"CODE",{});var B$r=s(tK);oio=r(B$r,"from_pretrained()"),B$r.forEach(t),rio=r(az,"to load the model weights."),az.forEach(t),tio=i(Gs),aK=n(Gs,"P",{});var k$r=s(aK);aio=r(k$r,"Examples:"),k$r.forEach(t),nio=i(Gs),m(JM.$$.fragment,Gs),Gs.forEach(t),sio=i(qs),xe=n(qs,"DIV",{class:!0});var Pt=s(xe);m(YM.$$.fragment,Pt),lio=i(Pt),nK=n(Pt,"P",{});var x$r=s(nK);iio=r(x$r,"Instantiate one of the model classes of the library (with a pretraining head) from a pretrained model."),x$r.forEach(t),dio=i(Pt),qa=n(Pt,"P",{});var m4=s(qa);cio=r(m4,"The model class to instantiate is selected based on the "),sK=n(m4,"CODE",{});var R$r=s(sK);fio=r(R$r,"model_type"),R$r.forEach(t),mio=r(m4,` property of the config object (either
passed as an argument or loaded from `),lK=n(m4,"CODE",{});var S$r=s(lK);gio=r(S$r,"pretrained_model_name_or_path"),S$r.forEach(t),hio=r(m4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),iK=n(m4,"CODE",{});var P$r=s(iK);pio=r(P$r,"pretrained_model_name_or_path"),P$r.forEach(t),_io=r(m4,":"),m4.forEach(t),uio=i(Pt),x=n(Pt,"UL",{});var S=s(x);o_=n(S,"LI",{});var tFe=s(o_);dK=n(tFe,"STRONG",{});var $$r=s(dK);bio=r($$r,"albert"),$$r.forEach(t),vio=r(tFe," \u2014 "),YR=n(tFe,"A",{href:!0});var I$r=s(YR);Tio=r(I$r,"AlbertForPreTraining"),I$r.forEach(t),Fio=r(tFe," (ALBERT model)"),tFe.forEach(t),Cio=i(S),r_=n(S,"LI",{});var aFe=s(r_);cK=n(aFe,"STRONG",{});var j$r=s(cK);Mio=r(j$r,"bart"),j$r.forEach(t),Eio=r(aFe," \u2014 "),KR=n(aFe,"A",{href:!0});var N$r=s(KR);yio=r(N$r,"BartForConditionalGeneration"),N$r.forEach(t),wio=r(aFe," (BART model)"),aFe.forEach(t),Aio=i(S),t_=n(S,"LI",{});var nFe=s(t_);fK=n(nFe,"STRONG",{});var D$r=s(fK);Lio=r(D$r,"bert"),D$r.forEach(t),Bio=r(nFe," \u2014 "),ZR=n(nFe,"A",{href:!0});var q$r=s(ZR);kio=r(q$r,"BertForPreTraining"),q$r.forEach(t),xio=r(nFe," (BERT model)"),nFe.forEach(t),Rio=i(S),a_=n(S,"LI",{});var sFe=s(a_);mK=n(sFe,"STRONG",{});var G$r=s(mK);Sio=r(G$r,"big_bird"),G$r.forEach(t),Pio=r(sFe," \u2014 "),eS=n(sFe,"A",{href:!0});var O$r=s(eS);$io=r(O$r,"BigBirdForPreTraining"),O$r.forEach(t),Iio=r(sFe," (BigBird model)"),sFe.forEach(t),jio=i(S),n_=n(S,"LI",{});var lFe=s(n_);gK=n(lFe,"STRONG",{});var X$r=s(gK);Nio=r(X$r,"camembert"),X$r.forEach(t),Dio=r(lFe," \u2014 "),oS=n(lFe,"A",{href:!0});var z$r=s(oS);qio=r(z$r,"CamembertForMaskedLM"),z$r.forEach(t),Gio=r(lFe," (CamemBERT model)"),lFe.forEach(t),Oio=i(S),s_=n(S,"LI",{});var iFe=s(s_);hK=n(iFe,"STRONG",{});var V$r=s(hK);Xio=r(V$r,"ctrl"),V$r.forEach(t),zio=r(iFe," \u2014 "),rS=n(iFe,"A",{href:!0});var W$r=s(rS);Vio=r(W$r,"CTRLLMHeadModel"),W$r.forEach(t),Wio=r(iFe," (CTRL model)"),iFe.forEach(t),Qio=i(S),l_=n(S,"LI",{});var dFe=s(l_);pK=n(dFe,"STRONG",{});var Q$r=s(pK);Hio=r(Q$r,"deberta"),Q$r.forEach(t),Uio=r(dFe," \u2014 "),tS=n(dFe,"A",{href:!0});var H$r=s(tS);Jio=r(H$r,"DebertaForMaskedLM"),H$r.forEach(t),Yio=r(dFe," (DeBERTa model)"),dFe.forEach(t),Kio=i(S),i_=n(S,"LI",{});var cFe=s(i_);_K=n(cFe,"STRONG",{});var U$r=s(_K);Zio=r(U$r,"deberta-v2"),U$r.forEach(t),edo=r(cFe," \u2014 "),aS=n(cFe,"A",{href:!0});var J$r=s(aS);odo=r(J$r,"DebertaV2ForMaskedLM"),J$r.forEach(t),rdo=r(cFe," (DeBERTa-v2 model)"),cFe.forEach(t),tdo=i(S),d_=n(S,"LI",{});var fFe=s(d_);uK=n(fFe,"STRONG",{});var Y$r=s(uK);ado=r(Y$r,"distilbert"),Y$r.forEach(t),ndo=r(fFe," \u2014 "),nS=n(fFe,"A",{href:!0});var K$r=s(nS);sdo=r(K$r,"DistilBertForMaskedLM"),K$r.forEach(t),ldo=r(fFe," (DistilBERT model)"),fFe.forEach(t),ido=i(S),c_=n(S,"LI",{});var mFe=s(c_);bK=n(mFe,"STRONG",{});var Z$r=s(bK);ddo=r(Z$r,"electra"),Z$r.forEach(t),cdo=r(mFe," \u2014 "),sS=n(mFe,"A",{href:!0});var eIr=s(sS);fdo=r(eIr,"ElectraForPreTraining"),eIr.forEach(t),mdo=r(mFe," (ELECTRA model)"),mFe.forEach(t),gdo=i(S),f_=n(S,"LI",{});var gFe=s(f_);vK=n(gFe,"STRONG",{});var oIr=s(vK);hdo=r(oIr,"flaubert"),oIr.forEach(t),pdo=r(gFe," \u2014 "),lS=n(gFe,"A",{href:!0});var rIr=s(lS);_do=r(rIr,"FlaubertWithLMHeadModel"),rIr.forEach(t),udo=r(gFe," (FlauBERT model)"),gFe.forEach(t),bdo=i(S),m_=n(S,"LI",{});var hFe=s(m_);TK=n(hFe,"STRONG",{});var tIr=s(TK);vdo=r(tIr,"fnet"),tIr.forEach(t),Tdo=r(hFe," \u2014 "),iS=n(hFe,"A",{href:!0});var aIr=s(iS);Fdo=r(aIr,"FNetForPreTraining"),aIr.forEach(t),Cdo=r(hFe," (FNet model)"),hFe.forEach(t),Mdo=i(S),g_=n(S,"LI",{});var pFe=s(g_);FK=n(pFe,"STRONG",{});var nIr=s(FK);Edo=r(nIr,"fsmt"),nIr.forEach(t),ydo=r(pFe," \u2014 "),dS=n(pFe,"A",{href:!0});var sIr=s(dS);wdo=r(sIr,"FSMTForConditionalGeneration"),sIr.forEach(t),Ado=r(pFe," (FairSeq Machine-Translation model)"),pFe.forEach(t),Ldo=i(S),h_=n(S,"LI",{});var _Fe=s(h_);CK=n(_Fe,"STRONG",{});var lIr=s(CK);Bdo=r(lIr,"funnel"),lIr.forEach(t),kdo=r(_Fe," \u2014 "),cS=n(_Fe,"A",{href:!0});var iIr=s(cS);xdo=r(iIr,"FunnelForPreTraining"),iIr.forEach(t),Rdo=r(_Fe," (Funnel Transformer model)"),_Fe.forEach(t),Sdo=i(S),p_=n(S,"LI",{});var uFe=s(p_);MK=n(uFe,"STRONG",{});var dIr=s(MK);Pdo=r(dIr,"gpt2"),dIr.forEach(t),$do=r(uFe," \u2014 "),fS=n(uFe,"A",{href:!0});var cIr=s(fS);Ido=r(cIr,"GPT2LMHeadModel"),cIr.forEach(t),jdo=r(uFe," (OpenAI GPT-2 model)"),uFe.forEach(t),Ndo=i(S),__=n(S,"LI",{});var bFe=s(__);EK=n(bFe,"STRONG",{});var fIr=s(EK);Ddo=r(fIr,"ibert"),fIr.forEach(t),qdo=r(bFe," \u2014 "),mS=n(bFe,"A",{href:!0});var mIr=s(mS);Gdo=r(mIr,"IBertForMaskedLM"),mIr.forEach(t),Odo=r(bFe," (I-BERT model)"),bFe.forEach(t),Xdo=i(S),u_=n(S,"LI",{});var vFe=s(u_);yK=n(vFe,"STRONG",{});var gIr=s(yK);zdo=r(gIr,"layoutlm"),gIr.forEach(t),Vdo=r(vFe," \u2014 "),gS=n(vFe,"A",{href:!0});var hIr=s(gS);Wdo=r(hIr,"LayoutLMForMaskedLM"),hIr.forEach(t),Qdo=r(vFe," (LayoutLM model)"),vFe.forEach(t),Hdo=i(S),b_=n(S,"LI",{});var TFe=s(b_);wK=n(TFe,"STRONG",{});var pIr=s(wK);Udo=r(pIr,"longformer"),pIr.forEach(t),Jdo=r(TFe," \u2014 "),hS=n(TFe,"A",{href:!0});var _Ir=s(hS);Ydo=r(_Ir,"LongformerForMaskedLM"),_Ir.forEach(t),Kdo=r(TFe," (Longformer model)"),TFe.forEach(t),Zdo=i(S),v_=n(S,"LI",{});var FFe=s(v_);AK=n(FFe,"STRONG",{});var uIr=s(AK);eco=r(uIr,"lxmert"),uIr.forEach(t),oco=r(FFe," \u2014 "),pS=n(FFe,"A",{href:!0});var bIr=s(pS);rco=r(bIr,"LxmertForPreTraining"),bIr.forEach(t),tco=r(FFe," (LXMERT model)"),FFe.forEach(t),aco=i(S),T_=n(S,"LI",{});var CFe=s(T_);LK=n(CFe,"STRONG",{});var vIr=s(LK);nco=r(vIr,"megatron-bert"),vIr.forEach(t),sco=r(CFe," \u2014 "),_S=n(CFe,"A",{href:!0});var TIr=s(_S);lco=r(TIr,"MegatronBertForPreTraining"),TIr.forEach(t),ico=r(CFe," (MegatronBert model)"),CFe.forEach(t),dco=i(S),F_=n(S,"LI",{});var MFe=s(F_);BK=n(MFe,"STRONG",{});var FIr=s(BK);cco=r(FIr,"mobilebert"),FIr.forEach(t),fco=r(MFe," \u2014 "),uS=n(MFe,"A",{href:!0});var CIr=s(uS);mco=r(CIr,"MobileBertForPreTraining"),CIr.forEach(t),gco=r(MFe," (MobileBERT model)"),MFe.forEach(t),hco=i(S),C_=n(S,"LI",{});var EFe=s(C_);kK=n(EFe,"STRONG",{});var MIr=s(kK);pco=r(MIr,"mpnet"),MIr.forEach(t),_co=r(EFe," \u2014 "),bS=n(EFe,"A",{href:!0});var EIr=s(bS);uco=r(EIr,"MPNetForMaskedLM"),EIr.forEach(t),bco=r(EFe," (MPNet model)"),EFe.forEach(t),vco=i(S),M_=n(S,"LI",{});var yFe=s(M_);xK=n(yFe,"STRONG",{});var yIr=s(xK);Tco=r(yIr,"openai-gpt"),yIr.forEach(t),Fco=r(yFe," \u2014 "),vS=n(yFe,"A",{href:!0});var wIr=s(vS);Cco=r(wIr,"OpenAIGPTLMHeadModel"),wIr.forEach(t),Mco=r(yFe," (OpenAI GPT model)"),yFe.forEach(t),Eco=i(S),E_=n(S,"LI",{});var wFe=s(E_);RK=n(wFe,"STRONG",{});var AIr=s(RK);yco=r(AIr,"retribert"),AIr.forEach(t),wco=r(wFe," \u2014 "),TS=n(wFe,"A",{href:!0});var LIr=s(TS);Aco=r(LIr,"RetriBertModel"),LIr.forEach(t),Lco=r(wFe," (RetriBERT model)"),wFe.forEach(t),Bco=i(S),y_=n(S,"LI",{});var AFe=s(y_);SK=n(AFe,"STRONG",{});var BIr=s(SK);kco=r(BIr,"roberta"),BIr.forEach(t),xco=r(AFe," \u2014 "),FS=n(AFe,"A",{href:!0});var kIr=s(FS);Rco=r(kIr,"RobertaForMaskedLM"),kIr.forEach(t),Sco=r(AFe," (RoBERTa model)"),AFe.forEach(t),Pco=i(S),w_=n(S,"LI",{});var LFe=s(w_);PK=n(LFe,"STRONG",{});var xIr=s(PK);$co=r(xIr,"squeezebert"),xIr.forEach(t),Ico=r(LFe," \u2014 "),CS=n(LFe,"A",{href:!0});var RIr=s(CS);jco=r(RIr,"SqueezeBertForMaskedLM"),RIr.forEach(t),Nco=r(LFe," (SqueezeBERT model)"),LFe.forEach(t),Dco=i(S),A_=n(S,"LI",{});var BFe=s(A_);$K=n(BFe,"STRONG",{});var SIr=s($K);qco=r(SIr,"t5"),SIr.forEach(t),Gco=r(BFe," \u2014 "),MS=n(BFe,"A",{href:!0});var PIr=s(MS);Oco=r(PIr,"T5ForConditionalGeneration"),PIr.forEach(t),Xco=r(BFe," (T5 model)"),BFe.forEach(t),zco=i(S),L_=n(S,"LI",{});var kFe=s(L_);IK=n(kFe,"STRONG",{});var $Ir=s(IK);Vco=r($Ir,"tapas"),$Ir.forEach(t),Wco=r(kFe," \u2014 "),ES=n(kFe,"A",{href:!0});var IIr=s(ES);Qco=r(IIr,"TapasForMaskedLM"),IIr.forEach(t),Hco=r(kFe," (TAPAS model)"),kFe.forEach(t),Uco=i(S),B_=n(S,"LI",{});var xFe=s(B_);jK=n(xFe,"STRONG",{});var jIr=s(jK);Jco=r(jIr,"transfo-xl"),jIr.forEach(t),Yco=r(xFe," \u2014 "),yS=n(xFe,"A",{href:!0});var NIr=s(yS);Kco=r(NIr,"TransfoXLLMHeadModel"),NIr.forEach(t),Zco=r(xFe," (Transformer-XL model)"),xFe.forEach(t),efo=i(S),k_=n(S,"LI",{});var RFe=s(k_);NK=n(RFe,"STRONG",{});var DIr=s(NK);ofo=r(DIr,"unispeech"),DIr.forEach(t),rfo=r(RFe," \u2014 "),wS=n(RFe,"A",{href:!0});var qIr=s(wS);tfo=r(qIr,"UniSpeechForPreTraining"),qIr.forEach(t),afo=r(RFe," (UniSpeech model)"),RFe.forEach(t),nfo=i(S),x_=n(S,"LI",{});var SFe=s(x_);DK=n(SFe,"STRONG",{});var GIr=s(DK);sfo=r(GIr,"unispeech-sat"),GIr.forEach(t),lfo=r(SFe," \u2014 "),AS=n(SFe,"A",{href:!0});var OIr=s(AS);ifo=r(OIr,"UniSpeechSatForPreTraining"),OIr.forEach(t),dfo=r(SFe," (UniSpeechSat model)"),SFe.forEach(t),cfo=i(S),R_=n(S,"LI",{});var PFe=s(R_);qK=n(PFe,"STRONG",{});var XIr=s(qK);ffo=r(XIr,"visual_bert"),XIr.forEach(t),mfo=r(PFe," \u2014 "),LS=n(PFe,"A",{href:!0});var zIr=s(LS);gfo=r(zIr,"VisualBertForPreTraining"),zIr.forEach(t),hfo=r(PFe," (VisualBert model)"),PFe.forEach(t),pfo=i(S),S_=n(S,"LI",{});var $Fe=s(S_);GK=n($Fe,"STRONG",{});var VIr=s(GK);_fo=r(VIr,"vit_mae"),VIr.forEach(t),ufo=r($Fe," \u2014 "),BS=n($Fe,"A",{href:!0});var WIr=s(BS);bfo=r(WIr,"ViTMAEForPreTraining"),WIr.forEach(t),vfo=r($Fe," (ViTMAE model)"),$Fe.forEach(t),Tfo=i(S),P_=n(S,"LI",{});var IFe=s(P_);OK=n(IFe,"STRONG",{});var QIr=s(OK);Ffo=r(QIr,"wav2vec2"),QIr.forEach(t),Cfo=r(IFe," \u2014 "),kS=n(IFe,"A",{href:!0});var HIr=s(kS);Mfo=r(HIr,"Wav2Vec2ForPreTraining"),HIr.forEach(t),Efo=r(IFe," (Wav2Vec2 model)"),IFe.forEach(t),yfo=i(S),$_=n(S,"LI",{});var jFe=s($_);XK=n(jFe,"STRONG",{});var UIr=s(XK);wfo=r(UIr,"xlm"),UIr.forEach(t),Afo=r(jFe," \u2014 "),xS=n(jFe,"A",{href:!0});var JIr=s(xS);Lfo=r(JIr,"XLMWithLMHeadModel"),JIr.forEach(t),Bfo=r(jFe," (XLM model)"),jFe.forEach(t),kfo=i(S),I_=n(S,"LI",{});var NFe=s(I_);zK=n(NFe,"STRONG",{});var YIr=s(zK);xfo=r(YIr,"xlm-roberta"),YIr.forEach(t),Rfo=r(NFe," \u2014 "),RS=n(NFe,"A",{href:!0});var KIr=s(RS);Sfo=r(KIr,"XLMRobertaForMaskedLM"),KIr.forEach(t),Pfo=r(NFe," (XLM-RoBERTa model)"),NFe.forEach(t),$fo=i(S),j_=n(S,"LI",{});var DFe=s(j_);VK=n(DFe,"STRONG",{});var ZIr=s(VK);Ifo=r(ZIr,"xlm-roberta-xl"),ZIr.forEach(t),jfo=r(DFe," \u2014 "),SS=n(DFe,"A",{href:!0});var ejr=s(SS);Nfo=r(ejr,"XLMRobertaXLForMaskedLM"),ejr.forEach(t),Dfo=r(DFe," (XLM-RoBERTa-XL model)"),DFe.forEach(t),qfo=i(S),N_=n(S,"LI",{});var qFe=s(N_);WK=n(qFe,"STRONG",{});var ojr=s(WK);Gfo=r(ojr,"xlnet"),ojr.forEach(t),Ofo=r(qFe," \u2014 "),PS=n(qFe,"A",{href:!0});var rjr=s(PS);Xfo=r(rjr,"XLNetLMHeadModel"),rjr.forEach(t),zfo=r(qFe," (XLNet model)"),qFe.forEach(t),S.forEach(t),Vfo=i(Pt),D_=n(Pt,"P",{});var GFe=s(D_);Wfo=r(GFe,"The model is set in evaluation mode by default using "),QK=n(GFe,"CODE",{});var tjr=s(QK);Qfo=r(tjr,"model.eval()"),tjr.forEach(t),Hfo=r(GFe,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),HK=n(GFe,"CODE",{});var ajr=s(HK);Ufo=r(ajr,"model.train()"),ajr.forEach(t),GFe.forEach(t),Jfo=i(Pt),UK=n(Pt,"P",{});var njr=s(UK);Yfo=r(njr,"Examples:"),njr.forEach(t),Kfo=i(Pt),m(KM.$$.fragment,Pt),Pt.forEach(t),qs.forEach(t),I7e=i(d),Wi=n(d,"H2",{class:!0});var XBe=s(Wi);q_=n(XBe,"A",{id:!0,class:!0,href:!0});var sjr=s(q_);JK=n(sjr,"SPAN",{});var ljr=s(JK);m(ZM.$$.fragment,ljr),ljr.forEach(t),sjr.forEach(t),Zfo=i(XBe),YK=n(XBe,"SPAN",{});var ijr=s(YK);emo=r(ijr,"AutoModelForCausalLM"),ijr.forEach(t),XBe.forEach(t),j7e=i(d),Qo=n(d,"DIV",{class:!0});var Os=s(Qo);m(eE.$$.fragment,Os),omo=i(Os),Qi=n(Os,"P",{});var nz=s(Qi);rmo=r(nz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a causal language modeling head) when created
with the `),KK=n(nz,"CODE",{});var djr=s(KK);tmo=r(djr,"from_pretrained()"),djr.forEach(t),amo=r(nz,"class method or the "),ZK=n(nz,"CODE",{});var cjr=s(ZK);nmo=r(cjr,"from_config()"),cjr.forEach(t),smo=r(nz,`class
method.`),nz.forEach(t),lmo=i(Os),oE=n(Os,"P",{});var zBe=s(oE);imo=r(zBe,"This class cannot be instantiated directly using "),eZ=n(zBe,"CODE",{});var fjr=s(eZ);dmo=r(fjr,"__init__()"),fjr.forEach(t),cmo=r(zBe," (throws an error)."),zBe.forEach(t),fmo=i(Os),qr=n(Os,"DIV",{class:!0});var Xs=s(qr);m(rE.$$.fragment,Xs),mmo=i(Xs),oZ=n(Xs,"P",{});var mjr=s(oZ);gmo=r(mjr,"Instantiates one of the model classes of the library (with a causal language modeling head) from a configuration."),mjr.forEach(t),hmo=i(Xs),Hi=n(Xs,"P",{});var sz=s(Hi);pmo=r(sz,`Note:
Loading a model from its configuration file does `),rZ=n(sz,"STRONG",{});var gjr=s(rZ);_mo=r(gjr,"not"),gjr.forEach(t),umo=r(sz,` load the model weights. It only affects the
model\u2019s configuration. Use `),tZ=n(sz,"CODE",{});var hjr=s(tZ);bmo=r(hjr,"from_pretrained()"),hjr.forEach(t),vmo=r(sz,"to load the model weights."),sz.forEach(t),Tmo=i(Xs),aZ=n(Xs,"P",{});var pjr=s(aZ);Fmo=r(pjr,"Examples:"),pjr.forEach(t),Cmo=i(Xs),m(tE.$$.fragment,Xs),Xs.forEach(t),Mmo=i(Os),Re=n(Os,"DIV",{class:!0});var $t=s(Re);m(aE.$$.fragment,$t),Emo=i($t),nZ=n($t,"P",{});var _jr=s(nZ);ymo=r(_jr,"Instantiate one of the model classes of the library (with a causal language modeling head) from a pretrained model."),_jr.forEach(t),wmo=i($t),Ga=n($t,"P",{});var g4=s(Ga);Amo=r(g4,"The model class to instantiate is selected based on the "),sZ=n(g4,"CODE",{});var ujr=s(sZ);Lmo=r(ujr,"model_type"),ujr.forEach(t),Bmo=r(g4,` property of the config object (either
passed as an argument or loaded from `),lZ=n(g4,"CODE",{});var bjr=s(lZ);kmo=r(bjr,"pretrained_model_name_or_path"),bjr.forEach(t),xmo=r(g4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),iZ=n(g4,"CODE",{});var vjr=s(iZ);Rmo=r(vjr,"pretrained_model_name_or_path"),vjr.forEach(t),Smo=r(g4,":"),g4.forEach(t),Pmo=i($t),$=n($t,"UL",{});var j=s($);G_=n(j,"LI",{});var OFe=s(G_);dZ=n(OFe,"STRONG",{});var Tjr=s(dZ);$mo=r(Tjr,"bart"),Tjr.forEach(t),Imo=r(OFe," \u2014 "),$S=n(OFe,"A",{href:!0});var Fjr=s($S);jmo=r(Fjr,"BartForCausalLM"),Fjr.forEach(t),Nmo=r(OFe," (BART model)"),OFe.forEach(t),Dmo=i(j),O_=n(j,"LI",{});var XFe=s(O_);cZ=n(XFe,"STRONG",{});var Cjr=s(cZ);qmo=r(Cjr,"bert"),Cjr.forEach(t),Gmo=r(XFe," \u2014 "),IS=n(XFe,"A",{href:!0});var Mjr=s(IS);Omo=r(Mjr,"BertLMHeadModel"),Mjr.forEach(t),Xmo=r(XFe," (BERT model)"),XFe.forEach(t),zmo=i(j),X_=n(j,"LI",{});var zFe=s(X_);fZ=n(zFe,"STRONG",{});var Ejr=s(fZ);Vmo=r(Ejr,"bert-generation"),Ejr.forEach(t),Wmo=r(zFe," \u2014 "),jS=n(zFe,"A",{href:!0});var yjr=s(jS);Qmo=r(yjr,"BertGenerationDecoder"),yjr.forEach(t),Hmo=r(zFe," (Bert Generation model)"),zFe.forEach(t),Umo=i(j),z_=n(j,"LI",{});var VFe=s(z_);mZ=n(VFe,"STRONG",{});var wjr=s(mZ);Jmo=r(wjr,"big_bird"),wjr.forEach(t),Ymo=r(VFe," \u2014 "),NS=n(VFe,"A",{href:!0});var Ajr=s(NS);Kmo=r(Ajr,"BigBirdForCausalLM"),Ajr.forEach(t),Zmo=r(VFe," (BigBird model)"),VFe.forEach(t),ego=i(j),V_=n(j,"LI",{});var WFe=s(V_);gZ=n(WFe,"STRONG",{});var Ljr=s(gZ);ogo=r(Ljr,"bigbird_pegasus"),Ljr.forEach(t),rgo=r(WFe," \u2014 "),DS=n(WFe,"A",{href:!0});var Bjr=s(DS);tgo=r(Bjr,"BigBirdPegasusForCausalLM"),Bjr.forEach(t),ago=r(WFe," (BigBirdPegasus model)"),WFe.forEach(t),ngo=i(j),W_=n(j,"LI",{});var QFe=s(W_);hZ=n(QFe,"STRONG",{});var kjr=s(hZ);sgo=r(kjr,"blenderbot"),kjr.forEach(t),lgo=r(QFe," \u2014 "),qS=n(QFe,"A",{href:!0});var xjr=s(qS);igo=r(xjr,"BlenderbotForCausalLM"),xjr.forEach(t),dgo=r(QFe," (Blenderbot model)"),QFe.forEach(t),cgo=i(j),Q_=n(j,"LI",{});var HFe=s(Q_);pZ=n(HFe,"STRONG",{});var Rjr=s(pZ);fgo=r(Rjr,"blenderbot-small"),Rjr.forEach(t),mgo=r(HFe," \u2014 "),GS=n(HFe,"A",{href:!0});var Sjr=s(GS);ggo=r(Sjr,"BlenderbotSmallForCausalLM"),Sjr.forEach(t),hgo=r(HFe," (BlenderbotSmall model)"),HFe.forEach(t),pgo=i(j),H_=n(j,"LI",{});var UFe=s(H_);_Z=n(UFe,"STRONG",{});var Pjr=s(_Z);_go=r(Pjr,"camembert"),Pjr.forEach(t),ugo=r(UFe," \u2014 "),OS=n(UFe,"A",{href:!0});var $jr=s(OS);bgo=r($jr,"CamembertForCausalLM"),$jr.forEach(t),vgo=r(UFe," (CamemBERT model)"),UFe.forEach(t),Tgo=i(j),U_=n(j,"LI",{});var JFe=s(U_);uZ=n(JFe,"STRONG",{});var Ijr=s(uZ);Fgo=r(Ijr,"ctrl"),Ijr.forEach(t),Cgo=r(JFe," \u2014 "),XS=n(JFe,"A",{href:!0});var jjr=s(XS);Mgo=r(jjr,"CTRLLMHeadModel"),jjr.forEach(t),Ego=r(JFe," (CTRL model)"),JFe.forEach(t),ygo=i(j),J_=n(j,"LI",{});var YFe=s(J_);bZ=n(YFe,"STRONG",{});var Njr=s(bZ);wgo=r(Njr,"electra"),Njr.forEach(t),Ago=r(YFe," \u2014 "),zS=n(YFe,"A",{href:!0});var Djr=s(zS);Lgo=r(Djr,"ElectraForCausalLM"),Djr.forEach(t),Bgo=r(YFe," (ELECTRA model)"),YFe.forEach(t),kgo=i(j),Y_=n(j,"LI",{});var KFe=s(Y_);vZ=n(KFe,"STRONG",{});var qjr=s(vZ);xgo=r(qjr,"gpt2"),qjr.forEach(t),Rgo=r(KFe," \u2014 "),VS=n(KFe,"A",{href:!0});var Gjr=s(VS);Sgo=r(Gjr,"GPT2LMHeadModel"),Gjr.forEach(t),Pgo=r(KFe," (OpenAI GPT-2 model)"),KFe.forEach(t),$go=i(j),K_=n(j,"LI",{});var ZFe=s(K_);TZ=n(ZFe,"STRONG",{});var Ojr=s(TZ);Igo=r(Ojr,"gpt_neo"),Ojr.forEach(t),jgo=r(ZFe," \u2014 "),WS=n(ZFe,"A",{href:!0});var Xjr=s(WS);Ngo=r(Xjr,"GPTNeoForCausalLM"),Xjr.forEach(t),Dgo=r(ZFe," (GPT Neo model)"),ZFe.forEach(t),qgo=i(j),Z_=n(j,"LI",{});var eCe=s(Z_);FZ=n(eCe,"STRONG",{});var zjr=s(FZ);Ggo=r(zjr,"gptj"),zjr.forEach(t),Ogo=r(eCe," \u2014 "),QS=n(eCe,"A",{href:!0});var Vjr=s(QS);Xgo=r(Vjr,"GPTJForCausalLM"),Vjr.forEach(t),zgo=r(eCe," (GPT-J model)"),eCe.forEach(t),Vgo=i(j),eu=n(j,"LI",{});var oCe=s(eu);CZ=n(oCe,"STRONG",{});var Wjr=s(CZ);Wgo=r(Wjr,"marian"),Wjr.forEach(t),Qgo=r(oCe," \u2014 "),HS=n(oCe,"A",{href:!0});var Qjr=s(HS);Hgo=r(Qjr,"MarianForCausalLM"),Qjr.forEach(t),Ugo=r(oCe," (Marian model)"),oCe.forEach(t),Jgo=i(j),ou=n(j,"LI",{});var rCe=s(ou);MZ=n(rCe,"STRONG",{});var Hjr=s(MZ);Ygo=r(Hjr,"mbart"),Hjr.forEach(t),Kgo=r(rCe," \u2014 "),US=n(rCe,"A",{href:!0});var Ujr=s(US);Zgo=r(Ujr,"MBartForCausalLM"),Ujr.forEach(t),eho=r(rCe," (mBART model)"),rCe.forEach(t),oho=i(j),ru=n(j,"LI",{});var tCe=s(ru);EZ=n(tCe,"STRONG",{});var Jjr=s(EZ);rho=r(Jjr,"megatron-bert"),Jjr.forEach(t),tho=r(tCe," \u2014 "),JS=n(tCe,"A",{href:!0});var Yjr=s(JS);aho=r(Yjr,"MegatronBertForCausalLM"),Yjr.forEach(t),nho=r(tCe," (MegatronBert model)"),tCe.forEach(t),sho=i(j),tu=n(j,"LI",{});var aCe=s(tu);yZ=n(aCe,"STRONG",{});var Kjr=s(yZ);lho=r(Kjr,"openai-gpt"),Kjr.forEach(t),iho=r(aCe," \u2014 "),YS=n(aCe,"A",{href:!0});var Zjr=s(YS);dho=r(Zjr,"OpenAIGPTLMHeadModel"),Zjr.forEach(t),cho=r(aCe," (OpenAI GPT model)"),aCe.forEach(t),fho=i(j),au=n(j,"LI",{});var nCe=s(au);wZ=n(nCe,"STRONG",{});var eNr=s(wZ);mho=r(eNr,"pegasus"),eNr.forEach(t),gho=r(nCe," \u2014 "),KS=n(nCe,"A",{href:!0});var oNr=s(KS);hho=r(oNr,"PegasusForCausalLM"),oNr.forEach(t),pho=r(nCe," (Pegasus model)"),nCe.forEach(t),_ho=i(j),nu=n(j,"LI",{});var sCe=s(nu);AZ=n(sCe,"STRONG",{});var rNr=s(AZ);uho=r(rNr,"plbart"),rNr.forEach(t),bho=r(sCe," \u2014 "),ZS=n(sCe,"A",{href:!0});var tNr=s(ZS);vho=r(tNr,"PLBartForCausalLM"),tNr.forEach(t),Tho=r(sCe," (PLBart model)"),sCe.forEach(t),Fho=i(j),su=n(j,"LI",{});var lCe=s(su);LZ=n(lCe,"STRONG",{});var aNr=s(LZ);Cho=r(aNr,"prophetnet"),aNr.forEach(t),Mho=r(lCe," \u2014 "),eP=n(lCe,"A",{href:!0});var nNr=s(eP);Eho=r(nNr,"ProphetNetForCausalLM"),nNr.forEach(t),yho=r(lCe," (ProphetNet model)"),lCe.forEach(t),who=i(j),lu=n(j,"LI",{});var iCe=s(lu);BZ=n(iCe,"STRONG",{});var sNr=s(BZ);Aho=r(sNr,"qdqbert"),sNr.forEach(t),Lho=r(iCe," \u2014 "),oP=n(iCe,"A",{href:!0});var lNr=s(oP);Bho=r(lNr,"QDQBertLMHeadModel"),lNr.forEach(t),kho=r(iCe," (QDQBert model)"),iCe.forEach(t),xho=i(j),iu=n(j,"LI",{});var dCe=s(iu);kZ=n(dCe,"STRONG",{});var iNr=s(kZ);Rho=r(iNr,"reformer"),iNr.forEach(t),Sho=r(dCe," \u2014 "),rP=n(dCe,"A",{href:!0});var dNr=s(rP);Pho=r(dNr,"ReformerModelWithLMHead"),dNr.forEach(t),$ho=r(dCe," (Reformer model)"),dCe.forEach(t),Iho=i(j),du=n(j,"LI",{});var cCe=s(du);xZ=n(cCe,"STRONG",{});var cNr=s(xZ);jho=r(cNr,"rembert"),cNr.forEach(t),Nho=r(cCe," \u2014 "),tP=n(cCe,"A",{href:!0});var fNr=s(tP);Dho=r(fNr,"RemBertForCausalLM"),fNr.forEach(t),qho=r(cCe," (RemBERT model)"),cCe.forEach(t),Gho=i(j),cu=n(j,"LI",{});var fCe=s(cu);RZ=n(fCe,"STRONG",{});var mNr=s(RZ);Oho=r(mNr,"roberta"),mNr.forEach(t),Xho=r(fCe," \u2014 "),aP=n(fCe,"A",{href:!0});var gNr=s(aP);zho=r(gNr,"RobertaForCausalLM"),gNr.forEach(t),Vho=r(fCe," (RoBERTa model)"),fCe.forEach(t),Who=i(j),fu=n(j,"LI",{});var mCe=s(fu);SZ=n(mCe,"STRONG",{});var hNr=s(SZ);Qho=r(hNr,"roformer"),hNr.forEach(t),Hho=r(mCe," \u2014 "),nP=n(mCe,"A",{href:!0});var pNr=s(nP);Uho=r(pNr,"RoFormerForCausalLM"),pNr.forEach(t),Jho=r(mCe," (RoFormer model)"),mCe.forEach(t),Yho=i(j),mu=n(j,"LI",{});var gCe=s(mu);PZ=n(gCe,"STRONG",{});var _Nr=s(PZ);Kho=r(_Nr,"speech_to_text_2"),_Nr.forEach(t),Zho=r(gCe," \u2014 "),sP=n(gCe,"A",{href:!0});var uNr=s(sP);epo=r(uNr,"Speech2Text2ForCausalLM"),uNr.forEach(t),opo=r(gCe," (Speech2Text2 model)"),gCe.forEach(t),rpo=i(j),gu=n(j,"LI",{});var hCe=s(gu);$Z=n(hCe,"STRONG",{});var bNr=s($Z);tpo=r(bNr,"transfo-xl"),bNr.forEach(t),apo=r(hCe," \u2014 "),lP=n(hCe,"A",{href:!0});var vNr=s(lP);npo=r(vNr,"TransfoXLLMHeadModel"),vNr.forEach(t),spo=r(hCe," (Transformer-XL model)"),hCe.forEach(t),lpo=i(j),hu=n(j,"LI",{});var pCe=s(hu);IZ=n(pCe,"STRONG",{});var TNr=s(IZ);ipo=r(TNr,"trocr"),TNr.forEach(t),dpo=r(pCe," \u2014 "),iP=n(pCe,"A",{href:!0});var FNr=s(iP);cpo=r(FNr,"TrOCRForCausalLM"),FNr.forEach(t),fpo=r(pCe," (TrOCR model)"),pCe.forEach(t),mpo=i(j),pu=n(j,"LI",{});var _Ce=s(pu);jZ=n(_Ce,"STRONG",{});var CNr=s(jZ);gpo=r(CNr,"xglm"),CNr.forEach(t),hpo=r(_Ce," \u2014 "),dP=n(_Ce,"A",{href:!0});var MNr=s(dP);ppo=r(MNr,"XGLMForCausalLM"),MNr.forEach(t),_po=r(_Ce," (XGLM model)"),_Ce.forEach(t),upo=i(j),_u=n(j,"LI",{});var uCe=s(_u);NZ=n(uCe,"STRONG",{});var ENr=s(NZ);bpo=r(ENr,"xlm"),ENr.forEach(t),vpo=r(uCe," \u2014 "),cP=n(uCe,"A",{href:!0});var yNr=s(cP);Tpo=r(yNr,"XLMWithLMHeadModel"),yNr.forEach(t),Fpo=r(uCe," (XLM model)"),uCe.forEach(t),Cpo=i(j),uu=n(j,"LI",{});var bCe=s(uu);DZ=n(bCe,"STRONG",{});var wNr=s(DZ);Mpo=r(wNr,"xlm-prophetnet"),wNr.forEach(t),Epo=r(bCe," \u2014 "),fP=n(bCe,"A",{href:!0});var ANr=s(fP);ypo=r(ANr,"XLMProphetNetForCausalLM"),ANr.forEach(t),wpo=r(bCe," (XLMProphetNet model)"),bCe.forEach(t),Apo=i(j),bu=n(j,"LI",{});var vCe=s(bu);qZ=n(vCe,"STRONG",{});var LNr=s(qZ);Lpo=r(LNr,"xlm-roberta"),LNr.forEach(t),Bpo=r(vCe," \u2014 "),mP=n(vCe,"A",{href:!0});var BNr=s(mP);kpo=r(BNr,"XLMRobertaForCausalLM"),BNr.forEach(t),xpo=r(vCe," (XLM-RoBERTa model)"),vCe.forEach(t),Rpo=i(j),vu=n(j,"LI",{});var TCe=s(vu);GZ=n(TCe,"STRONG",{});var kNr=s(GZ);Spo=r(kNr,"xlm-roberta-xl"),kNr.forEach(t),Ppo=r(TCe," \u2014 "),gP=n(TCe,"A",{href:!0});var xNr=s(gP);$po=r(xNr,"XLMRobertaXLForCausalLM"),xNr.forEach(t),Ipo=r(TCe," (XLM-RoBERTa-XL model)"),TCe.forEach(t),jpo=i(j),Tu=n(j,"LI",{});var FCe=s(Tu);OZ=n(FCe,"STRONG",{});var RNr=s(OZ);Npo=r(RNr,"xlnet"),RNr.forEach(t),Dpo=r(FCe," \u2014 "),hP=n(FCe,"A",{href:!0});var SNr=s(hP);qpo=r(SNr,"XLNetLMHeadModel"),SNr.forEach(t),Gpo=r(FCe," (XLNet model)"),FCe.forEach(t),j.forEach(t),Opo=i($t),Fu=n($t,"P",{});var CCe=s(Fu);Xpo=r(CCe,"The model is set in evaluation mode by default using "),XZ=n(CCe,"CODE",{});var PNr=s(XZ);zpo=r(PNr,"model.eval()"),PNr.forEach(t),Vpo=r(CCe,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),zZ=n(CCe,"CODE",{});var $Nr=s(zZ);Wpo=r($Nr,"model.train()"),$Nr.forEach(t),CCe.forEach(t),Qpo=i($t),VZ=n($t,"P",{});var INr=s(VZ);Hpo=r(INr,"Examples:"),INr.forEach(t),Upo=i($t),m(nE.$$.fragment,$t),$t.forEach(t),Os.forEach(t),N7e=i(d),Ui=n(d,"H2",{class:!0});var VBe=s(Ui);Cu=n(VBe,"A",{id:!0,class:!0,href:!0});var jNr=s(Cu);WZ=n(jNr,"SPAN",{});var NNr=s(WZ);m(sE.$$.fragment,NNr),NNr.forEach(t),jNr.forEach(t),Jpo=i(VBe),QZ=n(VBe,"SPAN",{});var DNr=s(QZ);Ypo=r(DNr,"AutoModelForMaskedLM"),DNr.forEach(t),VBe.forEach(t),D7e=i(d),Ho=n(d,"DIV",{class:!0});var zs=s(Ho);m(lE.$$.fragment,zs),Kpo=i(zs),Ji=n(zs,"P",{});var lz=s(Ji);Zpo=r(lz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a masked language modeling head) when created
with the `),HZ=n(lz,"CODE",{});var qNr=s(HZ);e_o=r(qNr,"from_pretrained()"),qNr.forEach(t),o_o=r(lz,"class method or the "),UZ=n(lz,"CODE",{});var GNr=s(UZ);r_o=r(GNr,"from_config()"),GNr.forEach(t),t_o=r(lz,`class
method.`),lz.forEach(t),a_o=i(zs),iE=n(zs,"P",{});var WBe=s(iE);n_o=r(WBe,"This class cannot be instantiated directly using "),JZ=n(WBe,"CODE",{});var ONr=s(JZ);s_o=r(ONr,"__init__()"),ONr.forEach(t),l_o=r(WBe," (throws an error)."),WBe.forEach(t),i_o=i(zs),Gr=n(zs,"DIV",{class:!0});var Vs=s(Gr);m(dE.$$.fragment,Vs),d_o=i(Vs),YZ=n(Vs,"P",{});var XNr=s(YZ);c_o=r(XNr,"Instantiates one of the model classes of the library (with a masked language modeling head) from a configuration."),XNr.forEach(t),f_o=i(Vs),Yi=n(Vs,"P",{});var iz=s(Yi);m_o=r(iz,`Note:
Loading a model from its configuration file does `),KZ=n(iz,"STRONG",{});var zNr=s(KZ);g_o=r(zNr,"not"),zNr.forEach(t),h_o=r(iz,` load the model weights. It only affects the
model\u2019s configuration. Use `),ZZ=n(iz,"CODE",{});var VNr=s(ZZ);p_o=r(VNr,"from_pretrained()"),VNr.forEach(t),__o=r(iz,"to load the model weights."),iz.forEach(t),u_o=i(Vs),eee=n(Vs,"P",{});var WNr=s(eee);b_o=r(WNr,"Examples:"),WNr.forEach(t),v_o=i(Vs),m(cE.$$.fragment,Vs),Vs.forEach(t),T_o=i(zs),Se=n(zs,"DIV",{class:!0});var It=s(Se);m(fE.$$.fragment,It),F_o=i(It),oee=n(It,"P",{});var QNr=s(oee);C_o=r(QNr,"Instantiate one of the model classes of the library (with a masked language modeling head) from a pretrained model."),QNr.forEach(t),M_o=i(It),Oa=n(It,"P",{});var h4=s(Oa);E_o=r(h4,"The model class to instantiate is selected based on the "),ree=n(h4,"CODE",{});var HNr=s(ree);y_o=r(HNr,"model_type"),HNr.forEach(t),w_o=r(h4,` property of the config object (either
passed as an argument or loaded from `),tee=n(h4,"CODE",{});var UNr=s(tee);A_o=r(UNr,"pretrained_model_name_or_path"),UNr.forEach(t),L_o=r(h4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),aee=n(h4,"CODE",{});var JNr=s(aee);B_o=r(JNr,"pretrained_model_name_or_path"),JNr.forEach(t),k_o=r(h4,":"),h4.forEach(t),x_o=i(It),I=n(It,"UL",{});var N=s(I);Mu=n(N,"LI",{});var MCe=s(Mu);nee=n(MCe,"STRONG",{});var YNr=s(nee);R_o=r(YNr,"albert"),YNr.forEach(t),S_o=r(MCe," \u2014 "),pP=n(MCe,"A",{href:!0});var KNr=s(pP);P_o=r(KNr,"AlbertForMaskedLM"),KNr.forEach(t),$_o=r(MCe," (ALBERT model)"),MCe.forEach(t),I_o=i(N),Eu=n(N,"LI",{});var ECe=s(Eu);see=n(ECe,"STRONG",{});var ZNr=s(see);j_o=r(ZNr,"bart"),ZNr.forEach(t),N_o=r(ECe," \u2014 "),_P=n(ECe,"A",{href:!0});var eDr=s(_P);D_o=r(eDr,"BartForConditionalGeneration"),eDr.forEach(t),q_o=r(ECe," (BART model)"),ECe.forEach(t),G_o=i(N),yu=n(N,"LI",{});var yCe=s(yu);lee=n(yCe,"STRONG",{});var oDr=s(lee);O_o=r(oDr,"bert"),oDr.forEach(t),X_o=r(yCe," \u2014 "),uP=n(yCe,"A",{href:!0});var rDr=s(uP);z_o=r(rDr,"BertForMaskedLM"),rDr.forEach(t),V_o=r(yCe," (BERT model)"),yCe.forEach(t),W_o=i(N),wu=n(N,"LI",{});var wCe=s(wu);iee=n(wCe,"STRONG",{});var tDr=s(iee);Q_o=r(tDr,"big_bird"),tDr.forEach(t),H_o=r(wCe," \u2014 "),bP=n(wCe,"A",{href:!0});var aDr=s(bP);U_o=r(aDr,"BigBirdForMaskedLM"),aDr.forEach(t),J_o=r(wCe," (BigBird model)"),wCe.forEach(t),Y_o=i(N),Au=n(N,"LI",{});var ACe=s(Au);dee=n(ACe,"STRONG",{});var nDr=s(dee);K_o=r(nDr,"camembert"),nDr.forEach(t),Z_o=r(ACe," \u2014 "),vP=n(ACe,"A",{href:!0});var sDr=s(vP);euo=r(sDr,"CamembertForMaskedLM"),sDr.forEach(t),ouo=r(ACe," (CamemBERT model)"),ACe.forEach(t),ruo=i(N),Lu=n(N,"LI",{});var LCe=s(Lu);cee=n(LCe,"STRONG",{});var lDr=s(cee);tuo=r(lDr,"convbert"),lDr.forEach(t),auo=r(LCe," \u2014 "),TP=n(LCe,"A",{href:!0});var iDr=s(TP);nuo=r(iDr,"ConvBertForMaskedLM"),iDr.forEach(t),suo=r(LCe," (ConvBERT model)"),LCe.forEach(t),luo=i(N),Bu=n(N,"LI",{});var BCe=s(Bu);fee=n(BCe,"STRONG",{});var dDr=s(fee);iuo=r(dDr,"deberta"),dDr.forEach(t),duo=r(BCe," \u2014 "),FP=n(BCe,"A",{href:!0});var cDr=s(FP);cuo=r(cDr,"DebertaForMaskedLM"),cDr.forEach(t),fuo=r(BCe," (DeBERTa model)"),BCe.forEach(t),muo=i(N),ku=n(N,"LI",{});var kCe=s(ku);mee=n(kCe,"STRONG",{});var fDr=s(mee);guo=r(fDr,"deberta-v2"),fDr.forEach(t),huo=r(kCe," \u2014 "),CP=n(kCe,"A",{href:!0});var mDr=s(CP);puo=r(mDr,"DebertaV2ForMaskedLM"),mDr.forEach(t),_uo=r(kCe," (DeBERTa-v2 model)"),kCe.forEach(t),uuo=i(N),xu=n(N,"LI",{});var xCe=s(xu);gee=n(xCe,"STRONG",{});var gDr=s(gee);buo=r(gDr,"distilbert"),gDr.forEach(t),vuo=r(xCe," \u2014 "),MP=n(xCe,"A",{href:!0});var hDr=s(MP);Tuo=r(hDr,"DistilBertForMaskedLM"),hDr.forEach(t),Fuo=r(xCe," (DistilBERT model)"),xCe.forEach(t),Cuo=i(N),Ru=n(N,"LI",{});var RCe=s(Ru);hee=n(RCe,"STRONG",{});var pDr=s(hee);Muo=r(pDr,"electra"),pDr.forEach(t),Euo=r(RCe," \u2014 "),EP=n(RCe,"A",{href:!0});var _Dr=s(EP);yuo=r(_Dr,"ElectraForMaskedLM"),_Dr.forEach(t),wuo=r(RCe," (ELECTRA model)"),RCe.forEach(t),Auo=i(N),Su=n(N,"LI",{});var SCe=s(Su);pee=n(SCe,"STRONG",{});var uDr=s(pee);Luo=r(uDr,"flaubert"),uDr.forEach(t),Buo=r(SCe," \u2014 "),yP=n(SCe,"A",{href:!0});var bDr=s(yP);kuo=r(bDr,"FlaubertWithLMHeadModel"),bDr.forEach(t),xuo=r(SCe," (FlauBERT model)"),SCe.forEach(t),Ruo=i(N),Pu=n(N,"LI",{});var PCe=s(Pu);_ee=n(PCe,"STRONG",{});var vDr=s(_ee);Suo=r(vDr,"fnet"),vDr.forEach(t),Puo=r(PCe," \u2014 "),wP=n(PCe,"A",{href:!0});var TDr=s(wP);$uo=r(TDr,"FNetForMaskedLM"),TDr.forEach(t),Iuo=r(PCe," (FNet model)"),PCe.forEach(t),juo=i(N),$u=n(N,"LI",{});var $Ce=s($u);uee=n($Ce,"STRONG",{});var FDr=s(uee);Nuo=r(FDr,"funnel"),FDr.forEach(t),Duo=r($Ce," \u2014 "),AP=n($Ce,"A",{href:!0});var CDr=s(AP);quo=r(CDr,"FunnelForMaskedLM"),CDr.forEach(t),Guo=r($Ce," (Funnel Transformer model)"),$Ce.forEach(t),Ouo=i(N),Iu=n(N,"LI",{});var ICe=s(Iu);bee=n(ICe,"STRONG",{});var MDr=s(bee);Xuo=r(MDr,"ibert"),MDr.forEach(t),zuo=r(ICe," \u2014 "),LP=n(ICe,"A",{href:!0});var EDr=s(LP);Vuo=r(EDr,"IBertForMaskedLM"),EDr.forEach(t),Wuo=r(ICe," (I-BERT model)"),ICe.forEach(t),Quo=i(N),ju=n(N,"LI",{});var jCe=s(ju);vee=n(jCe,"STRONG",{});var yDr=s(vee);Huo=r(yDr,"layoutlm"),yDr.forEach(t),Uuo=r(jCe," \u2014 "),BP=n(jCe,"A",{href:!0});var wDr=s(BP);Juo=r(wDr,"LayoutLMForMaskedLM"),wDr.forEach(t),Yuo=r(jCe," (LayoutLM model)"),jCe.forEach(t),Kuo=i(N),Nu=n(N,"LI",{});var NCe=s(Nu);Tee=n(NCe,"STRONG",{});var ADr=s(Tee);Zuo=r(ADr,"longformer"),ADr.forEach(t),e2o=r(NCe," \u2014 "),kP=n(NCe,"A",{href:!0});var LDr=s(kP);o2o=r(LDr,"LongformerForMaskedLM"),LDr.forEach(t),r2o=r(NCe," (Longformer model)"),NCe.forEach(t),t2o=i(N),Du=n(N,"LI",{});var DCe=s(Du);Fee=n(DCe,"STRONG",{});var BDr=s(Fee);a2o=r(BDr,"mbart"),BDr.forEach(t),n2o=r(DCe," \u2014 "),xP=n(DCe,"A",{href:!0});var kDr=s(xP);s2o=r(kDr,"MBartForConditionalGeneration"),kDr.forEach(t),l2o=r(DCe," (mBART model)"),DCe.forEach(t),i2o=i(N),qu=n(N,"LI",{});var qCe=s(qu);Cee=n(qCe,"STRONG",{});var xDr=s(Cee);d2o=r(xDr,"megatron-bert"),xDr.forEach(t),c2o=r(qCe," \u2014 "),RP=n(qCe,"A",{href:!0});var RDr=s(RP);f2o=r(RDr,"MegatronBertForMaskedLM"),RDr.forEach(t),m2o=r(qCe," (MegatronBert model)"),qCe.forEach(t),g2o=i(N),Gu=n(N,"LI",{});var GCe=s(Gu);Mee=n(GCe,"STRONG",{});var SDr=s(Mee);h2o=r(SDr,"mobilebert"),SDr.forEach(t),p2o=r(GCe," \u2014 "),SP=n(GCe,"A",{href:!0});var PDr=s(SP);_2o=r(PDr,"MobileBertForMaskedLM"),PDr.forEach(t),u2o=r(GCe," (MobileBERT model)"),GCe.forEach(t),b2o=i(N),Ou=n(N,"LI",{});var OCe=s(Ou);Eee=n(OCe,"STRONG",{});var $Dr=s(Eee);v2o=r($Dr,"mpnet"),$Dr.forEach(t),T2o=r(OCe," \u2014 "),PP=n(OCe,"A",{href:!0});var IDr=s(PP);F2o=r(IDr,"MPNetForMaskedLM"),IDr.forEach(t),C2o=r(OCe," (MPNet model)"),OCe.forEach(t),M2o=i(N),Xu=n(N,"LI",{});var XCe=s(Xu);yee=n(XCe,"STRONG",{});var jDr=s(yee);E2o=r(jDr,"nystromformer"),jDr.forEach(t),y2o=r(XCe," \u2014 "),$P=n(XCe,"A",{href:!0});var NDr=s($P);w2o=r(NDr,"NystromformerForMaskedLM"),NDr.forEach(t),A2o=r(XCe," (Nystromformer model)"),XCe.forEach(t),L2o=i(N),zu=n(N,"LI",{});var zCe=s(zu);wee=n(zCe,"STRONG",{});var DDr=s(wee);B2o=r(DDr,"perceiver"),DDr.forEach(t),k2o=r(zCe," \u2014 "),IP=n(zCe,"A",{href:!0});var qDr=s(IP);x2o=r(qDr,"PerceiverForMaskedLM"),qDr.forEach(t),R2o=r(zCe," (Perceiver model)"),zCe.forEach(t),S2o=i(N),Vu=n(N,"LI",{});var VCe=s(Vu);Aee=n(VCe,"STRONG",{});var GDr=s(Aee);P2o=r(GDr,"qdqbert"),GDr.forEach(t),$2o=r(VCe," \u2014 "),jP=n(VCe,"A",{href:!0});var ODr=s(jP);I2o=r(ODr,"QDQBertForMaskedLM"),ODr.forEach(t),j2o=r(VCe," (QDQBert model)"),VCe.forEach(t),N2o=i(N),Wu=n(N,"LI",{});var WCe=s(Wu);Lee=n(WCe,"STRONG",{});var XDr=s(Lee);D2o=r(XDr,"reformer"),XDr.forEach(t),q2o=r(WCe," \u2014 "),NP=n(WCe,"A",{href:!0});var zDr=s(NP);G2o=r(zDr,"ReformerForMaskedLM"),zDr.forEach(t),O2o=r(WCe," (Reformer model)"),WCe.forEach(t),X2o=i(N),Qu=n(N,"LI",{});var QCe=s(Qu);Bee=n(QCe,"STRONG",{});var VDr=s(Bee);z2o=r(VDr,"rembert"),VDr.forEach(t),V2o=r(QCe," \u2014 "),DP=n(QCe,"A",{href:!0});var WDr=s(DP);W2o=r(WDr,"RemBertForMaskedLM"),WDr.forEach(t),Q2o=r(QCe," (RemBERT model)"),QCe.forEach(t),H2o=i(N),Hu=n(N,"LI",{});var HCe=s(Hu);kee=n(HCe,"STRONG",{});var QDr=s(kee);U2o=r(QDr,"roberta"),QDr.forEach(t),J2o=r(HCe," \u2014 "),qP=n(HCe,"A",{href:!0});var HDr=s(qP);Y2o=r(HDr,"RobertaForMaskedLM"),HDr.forEach(t),K2o=r(HCe," (RoBERTa model)"),HCe.forEach(t),Z2o=i(N),Uu=n(N,"LI",{});var UCe=s(Uu);xee=n(UCe,"STRONG",{});var UDr=s(xee);e1o=r(UDr,"roformer"),UDr.forEach(t),o1o=r(UCe," \u2014 "),GP=n(UCe,"A",{href:!0});var JDr=s(GP);r1o=r(JDr,"RoFormerForMaskedLM"),JDr.forEach(t),t1o=r(UCe," (RoFormer model)"),UCe.forEach(t),a1o=i(N),Ju=n(N,"LI",{});var JCe=s(Ju);Ree=n(JCe,"STRONG",{});var YDr=s(Ree);n1o=r(YDr,"squeezebert"),YDr.forEach(t),s1o=r(JCe," \u2014 "),OP=n(JCe,"A",{href:!0});var KDr=s(OP);l1o=r(KDr,"SqueezeBertForMaskedLM"),KDr.forEach(t),i1o=r(JCe," (SqueezeBERT model)"),JCe.forEach(t),d1o=i(N),Yu=n(N,"LI",{});var YCe=s(Yu);See=n(YCe,"STRONG",{});var ZDr=s(See);c1o=r(ZDr,"tapas"),ZDr.forEach(t),f1o=r(YCe," \u2014 "),XP=n(YCe,"A",{href:!0});var eqr=s(XP);m1o=r(eqr,"TapasForMaskedLM"),eqr.forEach(t),g1o=r(YCe," (TAPAS model)"),YCe.forEach(t),h1o=i(N),Ku=n(N,"LI",{});var KCe=s(Ku);Pee=n(KCe,"STRONG",{});var oqr=s(Pee);p1o=r(oqr,"wav2vec2"),oqr.forEach(t),_1o=r(KCe," \u2014 "),$ee=n(KCe,"CODE",{});var rqr=s($ee);u1o=r(rqr,"Wav2Vec2ForMaskedLM"),rqr.forEach(t),b1o=r(KCe,"(Wav2Vec2 model)"),KCe.forEach(t),v1o=i(N),Zu=n(N,"LI",{});var ZCe=s(Zu);Iee=n(ZCe,"STRONG",{});var tqr=s(Iee);T1o=r(tqr,"xlm"),tqr.forEach(t),F1o=r(ZCe," \u2014 "),zP=n(ZCe,"A",{href:!0});var aqr=s(zP);C1o=r(aqr,"XLMWithLMHeadModel"),aqr.forEach(t),M1o=r(ZCe," (XLM model)"),ZCe.forEach(t),E1o=i(N),e2=n(N,"LI",{});var e4e=s(e2);jee=n(e4e,"STRONG",{});var nqr=s(jee);y1o=r(nqr,"xlm-roberta"),nqr.forEach(t),w1o=r(e4e," \u2014 "),VP=n(e4e,"A",{href:!0});var sqr=s(VP);A1o=r(sqr,"XLMRobertaForMaskedLM"),sqr.forEach(t),L1o=r(e4e," (XLM-RoBERTa model)"),e4e.forEach(t),B1o=i(N),o2=n(N,"LI",{});var o4e=s(o2);Nee=n(o4e,"STRONG",{});var lqr=s(Nee);k1o=r(lqr,"xlm-roberta-xl"),lqr.forEach(t),x1o=r(o4e," \u2014 "),WP=n(o4e,"A",{href:!0});var iqr=s(WP);R1o=r(iqr,"XLMRobertaXLForMaskedLM"),iqr.forEach(t),S1o=r(o4e," (XLM-RoBERTa-XL model)"),o4e.forEach(t),P1o=i(N),r2=n(N,"LI",{});var r4e=s(r2);Dee=n(r4e,"STRONG",{});var dqr=s(Dee);$1o=r(dqr,"yoso"),dqr.forEach(t),I1o=r(r4e," \u2014 "),QP=n(r4e,"A",{href:!0});var cqr=s(QP);j1o=r(cqr,"YosoForMaskedLM"),cqr.forEach(t),N1o=r(r4e," (YOSO model)"),r4e.forEach(t),N.forEach(t),D1o=i(It),t2=n(It,"P",{});var t4e=s(t2);q1o=r(t4e,"The model is set in evaluation mode by default using "),qee=n(t4e,"CODE",{});var fqr=s(qee);G1o=r(fqr,"model.eval()"),fqr.forEach(t),O1o=r(t4e,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Gee=n(t4e,"CODE",{});var mqr=s(Gee);X1o=r(mqr,"model.train()"),mqr.forEach(t),t4e.forEach(t),z1o=i(It),Oee=n(It,"P",{});var gqr=s(Oee);V1o=r(gqr,"Examples:"),gqr.forEach(t),W1o=i(It),m(mE.$$.fragment,It),It.forEach(t),zs.forEach(t),q7e=i(d),Ki=n(d,"H2",{class:!0});var QBe=s(Ki);a2=n(QBe,"A",{id:!0,class:!0,href:!0});var hqr=s(a2);Xee=n(hqr,"SPAN",{});var pqr=s(Xee);m(gE.$$.fragment,pqr),pqr.forEach(t),hqr.forEach(t),Q1o=i(QBe),zee=n(QBe,"SPAN",{});var _qr=s(zee);H1o=r(_qr,"AutoModelForSeq2SeqLM"),_qr.forEach(t),QBe.forEach(t),G7e=i(d),Uo=n(d,"DIV",{class:!0});var Ws=s(Uo);m(hE.$$.fragment,Ws),U1o=i(Ws),Zi=n(Ws,"P",{});var dz=s(Zi);J1o=r(dz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence language modeling head) when created
with the `),Vee=n(dz,"CODE",{});var uqr=s(Vee);Y1o=r(uqr,"from_pretrained()"),uqr.forEach(t),K1o=r(dz,"class method or the "),Wee=n(dz,"CODE",{});var bqr=s(Wee);Z1o=r(bqr,"from_config()"),bqr.forEach(t),ebo=r(dz,`class
method.`),dz.forEach(t),obo=i(Ws),pE=n(Ws,"P",{});var HBe=s(pE);rbo=r(HBe,"This class cannot be instantiated directly using "),Qee=n(HBe,"CODE",{});var vqr=s(Qee);tbo=r(vqr,"__init__()"),vqr.forEach(t),abo=r(HBe," (throws an error)."),HBe.forEach(t),nbo=i(Ws),Or=n(Ws,"DIV",{class:!0});var Qs=s(Or);m(_E.$$.fragment,Qs),sbo=i(Qs),Hee=n(Qs,"P",{});var Tqr=s(Hee);lbo=r(Tqr,"Instantiates one of the model classes of the library (with a sequence-to-sequence language modeling head) from a configuration."),Tqr.forEach(t),ibo=i(Qs),ed=n(Qs,"P",{});var cz=s(ed);dbo=r(cz,`Note:
Loading a model from its configuration file does `),Uee=n(cz,"STRONG",{});var Fqr=s(Uee);cbo=r(Fqr,"not"),Fqr.forEach(t),fbo=r(cz,` load the model weights. It only affects the
model\u2019s configuration. Use `),Jee=n(cz,"CODE",{});var Cqr=s(Jee);mbo=r(Cqr,"from_pretrained()"),Cqr.forEach(t),gbo=r(cz,"to load the model weights."),cz.forEach(t),hbo=i(Qs),Yee=n(Qs,"P",{});var Mqr=s(Yee);pbo=r(Mqr,"Examples:"),Mqr.forEach(t),_bo=i(Qs),m(uE.$$.fragment,Qs),Qs.forEach(t),ubo=i(Ws),Pe=n(Ws,"DIV",{class:!0});var jt=s(Pe);m(bE.$$.fragment,jt),bbo=i(jt),Kee=n(jt,"P",{});var Eqr=s(Kee);vbo=r(Eqr,"Instantiate one of the model classes of the library (with a sequence-to-sequence language modeling head) from a pretrained model."),Eqr.forEach(t),Tbo=i(jt),Xa=n(jt,"P",{});var p4=s(Xa);Fbo=r(p4,"The model class to instantiate is selected based on the "),Zee=n(p4,"CODE",{});var yqr=s(Zee);Cbo=r(yqr,"model_type"),yqr.forEach(t),Mbo=r(p4,` property of the config object (either
passed as an argument or loaded from `),eoe=n(p4,"CODE",{});var wqr=s(eoe);Ebo=r(wqr,"pretrained_model_name_or_path"),wqr.forEach(t),ybo=r(p4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),ooe=n(p4,"CODE",{});var Aqr=s(ooe);wbo=r(Aqr,"pretrained_model_name_or_path"),Aqr.forEach(t),Abo=r(p4,":"),p4.forEach(t),Lbo=i(jt),ae=n(jt,"UL",{});var le=s(ae);n2=n(le,"LI",{});var a4e=s(n2);roe=n(a4e,"STRONG",{});var Lqr=s(roe);Bbo=r(Lqr,"bart"),Lqr.forEach(t),kbo=r(a4e," \u2014 "),HP=n(a4e,"A",{href:!0});var Bqr=s(HP);xbo=r(Bqr,"BartForConditionalGeneration"),Bqr.forEach(t),Rbo=r(a4e," (BART model)"),a4e.forEach(t),Sbo=i(le),s2=n(le,"LI",{});var n4e=s(s2);toe=n(n4e,"STRONG",{});var kqr=s(toe);Pbo=r(kqr,"bigbird_pegasus"),kqr.forEach(t),$bo=r(n4e," \u2014 "),UP=n(n4e,"A",{href:!0});var xqr=s(UP);Ibo=r(xqr,"BigBirdPegasusForConditionalGeneration"),xqr.forEach(t),jbo=r(n4e," (BigBirdPegasus model)"),n4e.forEach(t),Nbo=i(le),l2=n(le,"LI",{});var s4e=s(l2);aoe=n(s4e,"STRONG",{});var Rqr=s(aoe);Dbo=r(Rqr,"blenderbot"),Rqr.forEach(t),qbo=r(s4e," \u2014 "),JP=n(s4e,"A",{href:!0});var Sqr=s(JP);Gbo=r(Sqr,"BlenderbotForConditionalGeneration"),Sqr.forEach(t),Obo=r(s4e," (Blenderbot model)"),s4e.forEach(t),Xbo=i(le),i2=n(le,"LI",{});var l4e=s(i2);noe=n(l4e,"STRONG",{});var Pqr=s(noe);zbo=r(Pqr,"blenderbot-small"),Pqr.forEach(t),Vbo=r(l4e," \u2014 "),YP=n(l4e,"A",{href:!0});var $qr=s(YP);Wbo=r($qr,"BlenderbotSmallForConditionalGeneration"),$qr.forEach(t),Qbo=r(l4e," (BlenderbotSmall model)"),l4e.forEach(t),Hbo=i(le),d2=n(le,"LI",{});var i4e=s(d2);soe=n(i4e,"STRONG",{});var Iqr=s(soe);Ubo=r(Iqr,"encoder-decoder"),Iqr.forEach(t),Jbo=r(i4e," \u2014 "),KP=n(i4e,"A",{href:!0});var jqr=s(KP);Ybo=r(jqr,"EncoderDecoderModel"),jqr.forEach(t),Kbo=r(i4e," (Encoder decoder model)"),i4e.forEach(t),Zbo=i(le),c2=n(le,"LI",{});var d4e=s(c2);loe=n(d4e,"STRONG",{});var Nqr=s(loe);e5o=r(Nqr,"fsmt"),Nqr.forEach(t),o5o=r(d4e," \u2014 "),ZP=n(d4e,"A",{href:!0});var Dqr=s(ZP);r5o=r(Dqr,"FSMTForConditionalGeneration"),Dqr.forEach(t),t5o=r(d4e," (FairSeq Machine-Translation model)"),d4e.forEach(t),a5o=i(le),f2=n(le,"LI",{});var c4e=s(f2);ioe=n(c4e,"STRONG",{});var qqr=s(ioe);n5o=r(qqr,"led"),qqr.forEach(t),s5o=r(c4e," \u2014 "),e$=n(c4e,"A",{href:!0});var Gqr=s(e$);l5o=r(Gqr,"LEDForConditionalGeneration"),Gqr.forEach(t),i5o=r(c4e," (LED model)"),c4e.forEach(t),d5o=i(le),m2=n(le,"LI",{});var f4e=s(m2);doe=n(f4e,"STRONG",{});var Oqr=s(doe);c5o=r(Oqr,"m2m_100"),Oqr.forEach(t),f5o=r(f4e," \u2014 "),o$=n(f4e,"A",{href:!0});var Xqr=s(o$);m5o=r(Xqr,"M2M100ForConditionalGeneration"),Xqr.forEach(t),g5o=r(f4e," (M2M100 model)"),f4e.forEach(t),h5o=i(le),g2=n(le,"LI",{});var m4e=s(g2);coe=n(m4e,"STRONG",{});var zqr=s(coe);p5o=r(zqr,"marian"),zqr.forEach(t),_5o=r(m4e," \u2014 "),r$=n(m4e,"A",{href:!0});var Vqr=s(r$);u5o=r(Vqr,"MarianMTModel"),Vqr.forEach(t),b5o=r(m4e," (Marian model)"),m4e.forEach(t),v5o=i(le),h2=n(le,"LI",{});var g4e=s(h2);foe=n(g4e,"STRONG",{});var Wqr=s(foe);T5o=r(Wqr,"mbart"),Wqr.forEach(t),F5o=r(g4e," \u2014 "),t$=n(g4e,"A",{href:!0});var Qqr=s(t$);C5o=r(Qqr,"MBartForConditionalGeneration"),Qqr.forEach(t),M5o=r(g4e," (mBART model)"),g4e.forEach(t),E5o=i(le),p2=n(le,"LI",{});var h4e=s(p2);moe=n(h4e,"STRONG",{});var Hqr=s(moe);y5o=r(Hqr,"mt5"),Hqr.forEach(t),w5o=r(h4e," \u2014 "),a$=n(h4e,"A",{href:!0});var Uqr=s(a$);A5o=r(Uqr,"MT5ForConditionalGeneration"),Uqr.forEach(t),L5o=r(h4e," (mT5 model)"),h4e.forEach(t),B5o=i(le),_2=n(le,"LI",{});var p4e=s(_2);goe=n(p4e,"STRONG",{});var Jqr=s(goe);k5o=r(Jqr,"pegasus"),Jqr.forEach(t),x5o=r(p4e," \u2014 "),n$=n(p4e,"A",{href:!0});var Yqr=s(n$);R5o=r(Yqr,"PegasusForConditionalGeneration"),Yqr.forEach(t),S5o=r(p4e," (Pegasus model)"),p4e.forEach(t),P5o=i(le),u2=n(le,"LI",{});var _4e=s(u2);hoe=n(_4e,"STRONG",{});var Kqr=s(hoe);$5o=r(Kqr,"plbart"),Kqr.forEach(t),I5o=r(_4e," \u2014 "),s$=n(_4e,"A",{href:!0});var Zqr=s(s$);j5o=r(Zqr,"PLBartForConditionalGeneration"),Zqr.forEach(t),N5o=r(_4e," (PLBart model)"),_4e.forEach(t),D5o=i(le),b2=n(le,"LI",{});var u4e=s(b2);poe=n(u4e,"STRONG",{});var eGr=s(poe);q5o=r(eGr,"prophetnet"),eGr.forEach(t),G5o=r(u4e," \u2014 "),l$=n(u4e,"A",{href:!0});var oGr=s(l$);O5o=r(oGr,"ProphetNetForConditionalGeneration"),oGr.forEach(t),X5o=r(u4e," (ProphetNet model)"),u4e.forEach(t),z5o=i(le),v2=n(le,"LI",{});var b4e=s(v2);_oe=n(b4e,"STRONG",{});var rGr=s(_oe);V5o=r(rGr,"t5"),rGr.forEach(t),W5o=r(b4e," \u2014 "),i$=n(b4e,"A",{href:!0});var tGr=s(i$);Q5o=r(tGr,"T5ForConditionalGeneration"),tGr.forEach(t),H5o=r(b4e," (T5 model)"),b4e.forEach(t),U5o=i(le),T2=n(le,"LI",{});var v4e=s(T2);uoe=n(v4e,"STRONG",{});var aGr=s(uoe);J5o=r(aGr,"xlm-prophetnet"),aGr.forEach(t),Y5o=r(v4e," \u2014 "),d$=n(v4e,"A",{href:!0});var nGr=s(d$);K5o=r(nGr,"XLMProphetNetForConditionalGeneration"),nGr.forEach(t),Z5o=r(v4e," (XLMProphetNet model)"),v4e.forEach(t),le.forEach(t),evo=i(jt),F2=n(jt,"P",{});var T4e=s(F2);ovo=r(T4e,"The model is set in evaluation mode by default using "),boe=n(T4e,"CODE",{});var sGr=s(boe);rvo=r(sGr,"model.eval()"),sGr.forEach(t),tvo=r(T4e,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),voe=n(T4e,"CODE",{});var lGr=s(voe);avo=r(lGr,"model.train()"),lGr.forEach(t),T4e.forEach(t),nvo=i(jt),Toe=n(jt,"P",{});var iGr=s(Toe);svo=r(iGr,"Examples:"),iGr.forEach(t),lvo=i(jt),m(vE.$$.fragment,jt),jt.forEach(t),Ws.forEach(t),O7e=i(d),od=n(d,"H2",{class:!0});var UBe=s(od);C2=n(UBe,"A",{id:!0,class:!0,href:!0});var dGr=s(C2);Foe=n(dGr,"SPAN",{});var cGr=s(Foe);m(TE.$$.fragment,cGr),cGr.forEach(t),dGr.forEach(t),ivo=i(UBe),Coe=n(UBe,"SPAN",{});var fGr=s(Coe);dvo=r(fGr,"AutoModelForSequenceClassification"),fGr.forEach(t),UBe.forEach(t),X7e=i(d),Jo=n(d,"DIV",{class:!0});var Hs=s(Jo);m(FE.$$.fragment,Hs),cvo=i(Hs),rd=n(Hs,"P",{});var fz=s(rd);fvo=r(fz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence classification head) when created
with the `),Moe=n(fz,"CODE",{});var mGr=s(Moe);mvo=r(mGr,"from_pretrained()"),mGr.forEach(t),gvo=r(fz,"class method or the "),Eoe=n(fz,"CODE",{});var gGr=s(Eoe);hvo=r(gGr,"from_config()"),gGr.forEach(t),pvo=r(fz,`class
method.`),fz.forEach(t),_vo=i(Hs),CE=n(Hs,"P",{});var JBe=s(CE);uvo=r(JBe,"This class cannot be instantiated directly using "),yoe=n(JBe,"CODE",{});var hGr=s(yoe);bvo=r(hGr,"__init__()"),hGr.forEach(t),vvo=r(JBe," (throws an error)."),JBe.forEach(t),Tvo=i(Hs),Xr=n(Hs,"DIV",{class:!0});var Us=s(Xr);m(ME.$$.fragment,Us),Fvo=i(Us),woe=n(Us,"P",{});var pGr=s(woe);Cvo=r(pGr,"Instantiates one of the model classes of the library (with a sequence classification head) from a configuration."),pGr.forEach(t),Mvo=i(Us),td=n(Us,"P",{});var mz=s(td);Evo=r(mz,`Note:
Loading a model from its configuration file does `),Aoe=n(mz,"STRONG",{});var _Gr=s(Aoe);yvo=r(_Gr,"not"),_Gr.forEach(t),wvo=r(mz,` load the model weights. It only affects the
model\u2019s configuration. Use `),Loe=n(mz,"CODE",{});var uGr=s(Loe);Avo=r(uGr,"from_pretrained()"),uGr.forEach(t),Lvo=r(mz,"to load the model weights."),mz.forEach(t),Bvo=i(Us),Boe=n(Us,"P",{});var bGr=s(Boe);kvo=r(bGr,"Examples:"),bGr.forEach(t),xvo=i(Us),m(EE.$$.fragment,Us),Us.forEach(t),Rvo=i(Hs),$e=n(Hs,"DIV",{class:!0});var Nt=s($e);m(yE.$$.fragment,Nt),Svo=i(Nt),koe=n(Nt,"P",{});var vGr=s(koe);Pvo=r(vGr,"Instantiate one of the model classes of the library (with a sequence classification head) from a pretrained model."),vGr.forEach(t),$vo=i(Nt),za=n(Nt,"P",{});var _4=s(za);Ivo=r(_4,"The model class to instantiate is selected based on the "),xoe=n(_4,"CODE",{});var TGr=s(xoe);jvo=r(TGr,"model_type"),TGr.forEach(t),Nvo=r(_4,` property of the config object (either
passed as an argument or loaded from `),Roe=n(_4,"CODE",{});var FGr=s(Roe);Dvo=r(FGr,"pretrained_model_name_or_path"),FGr.forEach(t),qvo=r(_4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Soe=n(_4,"CODE",{});var CGr=s(Soe);Gvo=r(CGr,"pretrained_model_name_or_path"),CGr.forEach(t),Ovo=r(_4,":"),_4.forEach(t),Xvo=i(Nt),A=n(Nt,"UL",{});var L=s(A);M2=n(L,"LI",{});var F4e=s(M2);Poe=n(F4e,"STRONG",{});var MGr=s(Poe);zvo=r(MGr,"albert"),MGr.forEach(t),Vvo=r(F4e," \u2014 "),c$=n(F4e,"A",{href:!0});var EGr=s(c$);Wvo=r(EGr,"AlbertForSequenceClassification"),EGr.forEach(t),Qvo=r(F4e," (ALBERT model)"),F4e.forEach(t),Hvo=i(L),E2=n(L,"LI",{});var C4e=s(E2);$oe=n(C4e,"STRONG",{});var yGr=s($oe);Uvo=r(yGr,"bart"),yGr.forEach(t),Jvo=r(C4e," \u2014 "),f$=n(C4e,"A",{href:!0});var wGr=s(f$);Yvo=r(wGr,"BartForSequenceClassification"),wGr.forEach(t),Kvo=r(C4e," (BART model)"),C4e.forEach(t),Zvo=i(L),y2=n(L,"LI",{});var M4e=s(y2);Ioe=n(M4e,"STRONG",{});var AGr=s(Ioe);e6o=r(AGr,"bert"),AGr.forEach(t),o6o=r(M4e," \u2014 "),m$=n(M4e,"A",{href:!0});var LGr=s(m$);r6o=r(LGr,"BertForSequenceClassification"),LGr.forEach(t),t6o=r(M4e," (BERT model)"),M4e.forEach(t),a6o=i(L),w2=n(L,"LI",{});var E4e=s(w2);joe=n(E4e,"STRONG",{});var BGr=s(joe);n6o=r(BGr,"big_bird"),BGr.forEach(t),s6o=r(E4e," \u2014 "),g$=n(E4e,"A",{href:!0});var kGr=s(g$);l6o=r(kGr,"BigBirdForSequenceClassification"),kGr.forEach(t),i6o=r(E4e," (BigBird model)"),E4e.forEach(t),d6o=i(L),A2=n(L,"LI",{});var y4e=s(A2);Noe=n(y4e,"STRONG",{});var xGr=s(Noe);c6o=r(xGr,"bigbird_pegasus"),xGr.forEach(t),f6o=r(y4e," \u2014 "),h$=n(y4e,"A",{href:!0});var RGr=s(h$);m6o=r(RGr,"BigBirdPegasusForSequenceClassification"),RGr.forEach(t),g6o=r(y4e," (BigBirdPegasus model)"),y4e.forEach(t),h6o=i(L),L2=n(L,"LI",{});var w4e=s(L2);Doe=n(w4e,"STRONG",{});var SGr=s(Doe);p6o=r(SGr,"camembert"),SGr.forEach(t),_6o=r(w4e," \u2014 "),p$=n(w4e,"A",{href:!0});var PGr=s(p$);u6o=r(PGr,"CamembertForSequenceClassification"),PGr.forEach(t),b6o=r(w4e," (CamemBERT model)"),w4e.forEach(t),v6o=i(L),B2=n(L,"LI",{});var A4e=s(B2);qoe=n(A4e,"STRONG",{});var $Gr=s(qoe);T6o=r($Gr,"canine"),$Gr.forEach(t),F6o=r(A4e," \u2014 "),_$=n(A4e,"A",{href:!0});var IGr=s(_$);C6o=r(IGr,"CanineForSequenceClassification"),IGr.forEach(t),M6o=r(A4e," (Canine model)"),A4e.forEach(t),E6o=i(L),k2=n(L,"LI",{});var L4e=s(k2);Goe=n(L4e,"STRONG",{});var jGr=s(Goe);y6o=r(jGr,"convbert"),jGr.forEach(t),w6o=r(L4e," \u2014 "),u$=n(L4e,"A",{href:!0});var NGr=s(u$);A6o=r(NGr,"ConvBertForSequenceClassification"),NGr.forEach(t),L6o=r(L4e," (ConvBERT model)"),L4e.forEach(t),B6o=i(L),x2=n(L,"LI",{});var B4e=s(x2);Ooe=n(B4e,"STRONG",{});var DGr=s(Ooe);k6o=r(DGr,"ctrl"),DGr.forEach(t),x6o=r(B4e," \u2014 "),b$=n(B4e,"A",{href:!0});var qGr=s(b$);R6o=r(qGr,"CTRLForSequenceClassification"),qGr.forEach(t),S6o=r(B4e," (CTRL model)"),B4e.forEach(t),P6o=i(L),R2=n(L,"LI",{});var k4e=s(R2);Xoe=n(k4e,"STRONG",{});var GGr=s(Xoe);$6o=r(GGr,"deberta"),GGr.forEach(t),I6o=r(k4e," \u2014 "),v$=n(k4e,"A",{href:!0});var OGr=s(v$);j6o=r(OGr,"DebertaForSequenceClassification"),OGr.forEach(t),N6o=r(k4e," (DeBERTa model)"),k4e.forEach(t),D6o=i(L),S2=n(L,"LI",{});var x4e=s(S2);zoe=n(x4e,"STRONG",{});var XGr=s(zoe);q6o=r(XGr,"deberta-v2"),XGr.forEach(t),G6o=r(x4e," \u2014 "),T$=n(x4e,"A",{href:!0});var zGr=s(T$);O6o=r(zGr,"DebertaV2ForSequenceClassification"),zGr.forEach(t),X6o=r(x4e," (DeBERTa-v2 model)"),x4e.forEach(t),z6o=i(L),P2=n(L,"LI",{});var R4e=s(P2);Voe=n(R4e,"STRONG",{});var VGr=s(Voe);V6o=r(VGr,"distilbert"),VGr.forEach(t),W6o=r(R4e," \u2014 "),F$=n(R4e,"A",{href:!0});var WGr=s(F$);Q6o=r(WGr,"DistilBertForSequenceClassification"),WGr.forEach(t),H6o=r(R4e," (DistilBERT model)"),R4e.forEach(t),U6o=i(L),$2=n(L,"LI",{});var S4e=s($2);Woe=n(S4e,"STRONG",{});var QGr=s(Woe);J6o=r(QGr,"electra"),QGr.forEach(t),Y6o=r(S4e," \u2014 "),C$=n(S4e,"A",{href:!0});var HGr=s(C$);K6o=r(HGr,"ElectraForSequenceClassification"),HGr.forEach(t),Z6o=r(S4e," (ELECTRA model)"),S4e.forEach(t),eTo=i(L),I2=n(L,"LI",{});var P4e=s(I2);Qoe=n(P4e,"STRONG",{});var UGr=s(Qoe);oTo=r(UGr,"flaubert"),UGr.forEach(t),rTo=r(P4e," \u2014 "),M$=n(P4e,"A",{href:!0});var JGr=s(M$);tTo=r(JGr,"FlaubertForSequenceClassification"),JGr.forEach(t),aTo=r(P4e," (FlauBERT model)"),P4e.forEach(t),nTo=i(L),j2=n(L,"LI",{});var $4e=s(j2);Hoe=n($4e,"STRONG",{});var YGr=s(Hoe);sTo=r(YGr,"fnet"),YGr.forEach(t),lTo=r($4e," \u2014 "),E$=n($4e,"A",{href:!0});var KGr=s(E$);iTo=r(KGr,"FNetForSequenceClassification"),KGr.forEach(t),dTo=r($4e," (FNet model)"),$4e.forEach(t),cTo=i(L),N2=n(L,"LI",{});var I4e=s(N2);Uoe=n(I4e,"STRONG",{});var ZGr=s(Uoe);fTo=r(ZGr,"funnel"),ZGr.forEach(t),mTo=r(I4e," \u2014 "),y$=n(I4e,"A",{href:!0});var eOr=s(y$);gTo=r(eOr,"FunnelForSequenceClassification"),eOr.forEach(t),hTo=r(I4e," (Funnel Transformer model)"),I4e.forEach(t),pTo=i(L),D2=n(L,"LI",{});var j4e=s(D2);Joe=n(j4e,"STRONG",{});var oOr=s(Joe);_To=r(oOr,"gpt2"),oOr.forEach(t),uTo=r(j4e," \u2014 "),w$=n(j4e,"A",{href:!0});var rOr=s(w$);bTo=r(rOr,"GPT2ForSequenceClassification"),rOr.forEach(t),vTo=r(j4e," (OpenAI GPT-2 model)"),j4e.forEach(t),TTo=i(L),q2=n(L,"LI",{});var N4e=s(q2);Yoe=n(N4e,"STRONG",{});var tOr=s(Yoe);FTo=r(tOr,"gpt_neo"),tOr.forEach(t),CTo=r(N4e," \u2014 "),A$=n(N4e,"A",{href:!0});var aOr=s(A$);MTo=r(aOr,"GPTNeoForSequenceClassification"),aOr.forEach(t),ETo=r(N4e," (GPT Neo model)"),N4e.forEach(t),yTo=i(L),G2=n(L,"LI",{});var D4e=s(G2);Koe=n(D4e,"STRONG",{});var nOr=s(Koe);wTo=r(nOr,"gptj"),nOr.forEach(t),ATo=r(D4e," \u2014 "),L$=n(D4e,"A",{href:!0});var sOr=s(L$);LTo=r(sOr,"GPTJForSequenceClassification"),sOr.forEach(t),BTo=r(D4e," (GPT-J model)"),D4e.forEach(t),kTo=i(L),O2=n(L,"LI",{});var q4e=s(O2);Zoe=n(q4e,"STRONG",{});var lOr=s(Zoe);xTo=r(lOr,"ibert"),lOr.forEach(t),RTo=r(q4e," \u2014 "),B$=n(q4e,"A",{href:!0});var iOr=s(B$);STo=r(iOr,"IBertForSequenceClassification"),iOr.forEach(t),PTo=r(q4e," (I-BERT model)"),q4e.forEach(t),$To=i(L),X2=n(L,"LI",{});var G4e=s(X2);ere=n(G4e,"STRONG",{});var dOr=s(ere);ITo=r(dOr,"layoutlm"),dOr.forEach(t),jTo=r(G4e," \u2014 "),k$=n(G4e,"A",{href:!0});var cOr=s(k$);NTo=r(cOr,"LayoutLMForSequenceClassification"),cOr.forEach(t),DTo=r(G4e," (LayoutLM model)"),G4e.forEach(t),qTo=i(L),z2=n(L,"LI",{});var O4e=s(z2);ore=n(O4e,"STRONG",{});var fOr=s(ore);GTo=r(fOr,"layoutlmv2"),fOr.forEach(t),OTo=r(O4e," \u2014 "),x$=n(O4e,"A",{href:!0});var mOr=s(x$);XTo=r(mOr,"LayoutLMv2ForSequenceClassification"),mOr.forEach(t),zTo=r(O4e," (LayoutLMv2 model)"),O4e.forEach(t),VTo=i(L),V2=n(L,"LI",{});var X4e=s(V2);rre=n(X4e,"STRONG",{});var gOr=s(rre);WTo=r(gOr,"led"),gOr.forEach(t),QTo=r(X4e," \u2014 "),R$=n(X4e,"A",{href:!0});var hOr=s(R$);HTo=r(hOr,"LEDForSequenceClassification"),hOr.forEach(t),UTo=r(X4e," (LED model)"),X4e.forEach(t),JTo=i(L),W2=n(L,"LI",{});var z4e=s(W2);tre=n(z4e,"STRONG",{});var pOr=s(tre);YTo=r(pOr,"longformer"),pOr.forEach(t),KTo=r(z4e," \u2014 "),S$=n(z4e,"A",{href:!0});var _Or=s(S$);ZTo=r(_Or,"LongformerForSequenceClassification"),_Or.forEach(t),e8o=r(z4e," (Longformer model)"),z4e.forEach(t),o8o=i(L),Q2=n(L,"LI",{});var V4e=s(Q2);are=n(V4e,"STRONG",{});var uOr=s(are);r8o=r(uOr,"mbart"),uOr.forEach(t),t8o=r(V4e," \u2014 "),P$=n(V4e,"A",{href:!0});var bOr=s(P$);a8o=r(bOr,"MBartForSequenceClassification"),bOr.forEach(t),n8o=r(V4e," (mBART model)"),V4e.forEach(t),s8o=i(L),H2=n(L,"LI",{});var W4e=s(H2);nre=n(W4e,"STRONG",{});var vOr=s(nre);l8o=r(vOr,"megatron-bert"),vOr.forEach(t),i8o=r(W4e," \u2014 "),$$=n(W4e,"A",{href:!0});var TOr=s($$);d8o=r(TOr,"MegatronBertForSequenceClassification"),TOr.forEach(t),c8o=r(W4e," (MegatronBert model)"),W4e.forEach(t),f8o=i(L),U2=n(L,"LI",{});var Q4e=s(U2);sre=n(Q4e,"STRONG",{});var FOr=s(sre);m8o=r(FOr,"mobilebert"),FOr.forEach(t),g8o=r(Q4e," \u2014 "),I$=n(Q4e,"A",{href:!0});var COr=s(I$);h8o=r(COr,"MobileBertForSequenceClassification"),COr.forEach(t),p8o=r(Q4e," (MobileBERT model)"),Q4e.forEach(t),_8o=i(L),J2=n(L,"LI",{});var H4e=s(J2);lre=n(H4e,"STRONG",{});var MOr=s(lre);u8o=r(MOr,"mpnet"),MOr.forEach(t),b8o=r(H4e," \u2014 "),j$=n(H4e,"A",{href:!0});var EOr=s(j$);v8o=r(EOr,"MPNetForSequenceClassification"),EOr.forEach(t),T8o=r(H4e," (MPNet model)"),H4e.forEach(t),F8o=i(L),Y2=n(L,"LI",{});var U4e=s(Y2);ire=n(U4e,"STRONG",{});var yOr=s(ire);C8o=r(yOr,"nystromformer"),yOr.forEach(t),M8o=r(U4e," \u2014 "),N$=n(U4e,"A",{href:!0});var wOr=s(N$);E8o=r(wOr,"NystromformerForSequenceClassification"),wOr.forEach(t),y8o=r(U4e," (Nystromformer model)"),U4e.forEach(t),w8o=i(L),K2=n(L,"LI",{});var J4e=s(K2);dre=n(J4e,"STRONG",{});var AOr=s(dre);A8o=r(AOr,"openai-gpt"),AOr.forEach(t),L8o=r(J4e," \u2014 "),D$=n(J4e,"A",{href:!0});var LOr=s(D$);B8o=r(LOr,"OpenAIGPTForSequenceClassification"),LOr.forEach(t),k8o=r(J4e," (OpenAI GPT model)"),J4e.forEach(t),x8o=i(L),Z2=n(L,"LI",{});var Y4e=s(Z2);cre=n(Y4e,"STRONG",{});var BOr=s(cre);R8o=r(BOr,"perceiver"),BOr.forEach(t),S8o=r(Y4e," \u2014 "),q$=n(Y4e,"A",{href:!0});var kOr=s(q$);P8o=r(kOr,"PerceiverForSequenceClassification"),kOr.forEach(t),$8o=r(Y4e," (Perceiver model)"),Y4e.forEach(t),I8o=i(L),e1=n(L,"LI",{});var K4e=s(e1);fre=n(K4e,"STRONG",{});var xOr=s(fre);j8o=r(xOr,"plbart"),xOr.forEach(t),N8o=r(K4e," \u2014 "),G$=n(K4e,"A",{href:!0});var ROr=s(G$);D8o=r(ROr,"PLBartForSequenceClassification"),ROr.forEach(t),q8o=r(K4e," (PLBart model)"),K4e.forEach(t),G8o=i(L),o1=n(L,"LI",{});var Z4e=s(o1);mre=n(Z4e,"STRONG",{});var SOr=s(mre);O8o=r(SOr,"qdqbert"),SOr.forEach(t),X8o=r(Z4e," \u2014 "),O$=n(Z4e,"A",{href:!0});var POr=s(O$);z8o=r(POr,"QDQBertForSequenceClassification"),POr.forEach(t),V8o=r(Z4e," (QDQBert model)"),Z4e.forEach(t),W8o=i(L),r1=n(L,"LI",{});var eMe=s(r1);gre=n(eMe,"STRONG",{});var $Or=s(gre);Q8o=r($Or,"reformer"),$Or.forEach(t),H8o=r(eMe," \u2014 "),X$=n(eMe,"A",{href:!0});var IOr=s(X$);U8o=r(IOr,"ReformerForSequenceClassification"),IOr.forEach(t),J8o=r(eMe," (Reformer model)"),eMe.forEach(t),Y8o=i(L),t1=n(L,"LI",{});var oMe=s(t1);hre=n(oMe,"STRONG",{});var jOr=s(hre);K8o=r(jOr,"rembert"),jOr.forEach(t),Z8o=r(oMe," \u2014 "),z$=n(oMe,"A",{href:!0});var NOr=s(z$);eFo=r(NOr,"RemBertForSequenceClassification"),NOr.forEach(t),oFo=r(oMe," (RemBERT model)"),oMe.forEach(t),rFo=i(L),a1=n(L,"LI",{});var rMe=s(a1);pre=n(rMe,"STRONG",{});var DOr=s(pre);tFo=r(DOr,"roberta"),DOr.forEach(t),aFo=r(rMe," \u2014 "),V$=n(rMe,"A",{href:!0});var qOr=s(V$);nFo=r(qOr,"RobertaForSequenceClassification"),qOr.forEach(t),sFo=r(rMe," (RoBERTa model)"),rMe.forEach(t),lFo=i(L),n1=n(L,"LI",{});var tMe=s(n1);_re=n(tMe,"STRONG",{});var GOr=s(_re);iFo=r(GOr,"roformer"),GOr.forEach(t),dFo=r(tMe," \u2014 "),W$=n(tMe,"A",{href:!0});var OOr=s(W$);cFo=r(OOr,"RoFormerForSequenceClassification"),OOr.forEach(t),fFo=r(tMe," (RoFormer model)"),tMe.forEach(t),mFo=i(L),s1=n(L,"LI",{});var aMe=s(s1);ure=n(aMe,"STRONG",{});var XOr=s(ure);gFo=r(XOr,"squeezebert"),XOr.forEach(t),hFo=r(aMe," \u2014 "),Q$=n(aMe,"A",{href:!0});var zOr=s(Q$);pFo=r(zOr,"SqueezeBertForSequenceClassification"),zOr.forEach(t),_Fo=r(aMe," (SqueezeBERT model)"),aMe.forEach(t),uFo=i(L),l1=n(L,"LI",{});var nMe=s(l1);bre=n(nMe,"STRONG",{});var VOr=s(bre);bFo=r(VOr,"tapas"),VOr.forEach(t),vFo=r(nMe," \u2014 "),H$=n(nMe,"A",{href:!0});var WOr=s(H$);TFo=r(WOr,"TapasForSequenceClassification"),WOr.forEach(t),FFo=r(nMe," (TAPAS model)"),nMe.forEach(t),CFo=i(L),i1=n(L,"LI",{});var sMe=s(i1);vre=n(sMe,"STRONG",{});var QOr=s(vre);MFo=r(QOr,"transfo-xl"),QOr.forEach(t),EFo=r(sMe," \u2014 "),U$=n(sMe,"A",{href:!0});var HOr=s(U$);yFo=r(HOr,"TransfoXLForSequenceClassification"),HOr.forEach(t),wFo=r(sMe," (Transformer-XL model)"),sMe.forEach(t),AFo=i(L),d1=n(L,"LI",{});var lMe=s(d1);Tre=n(lMe,"STRONG",{});var UOr=s(Tre);LFo=r(UOr,"xlm"),UOr.forEach(t),BFo=r(lMe," \u2014 "),J$=n(lMe,"A",{href:!0});var JOr=s(J$);kFo=r(JOr,"XLMForSequenceClassification"),JOr.forEach(t),xFo=r(lMe," (XLM model)"),lMe.forEach(t),RFo=i(L),c1=n(L,"LI",{});var iMe=s(c1);Fre=n(iMe,"STRONG",{});var YOr=s(Fre);SFo=r(YOr,"xlm-roberta"),YOr.forEach(t),PFo=r(iMe," \u2014 "),Y$=n(iMe,"A",{href:!0});var KOr=s(Y$);$Fo=r(KOr,"XLMRobertaForSequenceClassification"),KOr.forEach(t),IFo=r(iMe," (XLM-RoBERTa model)"),iMe.forEach(t),jFo=i(L),f1=n(L,"LI",{});var dMe=s(f1);Cre=n(dMe,"STRONG",{});var ZOr=s(Cre);NFo=r(ZOr,"xlm-roberta-xl"),ZOr.forEach(t),DFo=r(dMe," \u2014 "),K$=n(dMe,"A",{href:!0});var eXr=s(K$);qFo=r(eXr,"XLMRobertaXLForSequenceClassification"),eXr.forEach(t),GFo=r(dMe," (XLM-RoBERTa-XL model)"),dMe.forEach(t),OFo=i(L),m1=n(L,"LI",{});var cMe=s(m1);Mre=n(cMe,"STRONG",{});var oXr=s(Mre);XFo=r(oXr,"xlnet"),oXr.forEach(t),zFo=r(cMe," \u2014 "),Z$=n(cMe,"A",{href:!0});var rXr=s(Z$);VFo=r(rXr,"XLNetForSequenceClassification"),rXr.forEach(t),WFo=r(cMe," (XLNet model)"),cMe.forEach(t),QFo=i(L),g1=n(L,"LI",{});var fMe=s(g1);Ere=n(fMe,"STRONG",{});var tXr=s(Ere);HFo=r(tXr,"yoso"),tXr.forEach(t),UFo=r(fMe," \u2014 "),eI=n(fMe,"A",{href:!0});var aXr=s(eI);JFo=r(aXr,"YosoForSequenceClassification"),aXr.forEach(t),YFo=r(fMe," (YOSO model)"),fMe.forEach(t),L.forEach(t),KFo=i(Nt),h1=n(Nt,"P",{});var mMe=s(h1);ZFo=r(mMe,"The model is set in evaluation mode by default using "),yre=n(mMe,"CODE",{});var nXr=s(yre);eCo=r(nXr,"model.eval()"),nXr.forEach(t),oCo=r(mMe,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),wre=n(mMe,"CODE",{});var sXr=s(wre);rCo=r(sXr,"model.train()"),sXr.forEach(t),mMe.forEach(t),tCo=i(Nt),Are=n(Nt,"P",{});var lXr=s(Are);aCo=r(lXr,"Examples:"),lXr.forEach(t),nCo=i(Nt),m(wE.$$.fragment,Nt),Nt.forEach(t),Hs.forEach(t),z7e=i(d),ad=n(d,"H2",{class:!0});var YBe=s(ad);p1=n(YBe,"A",{id:!0,class:!0,href:!0});var iXr=s(p1);Lre=n(iXr,"SPAN",{});var dXr=s(Lre);m(AE.$$.fragment,dXr),dXr.forEach(t),iXr.forEach(t),sCo=i(YBe),Bre=n(YBe,"SPAN",{});var cXr=s(Bre);lCo=r(cXr,"AutoModelForMultipleChoice"),cXr.forEach(t),YBe.forEach(t),V7e=i(d),Yo=n(d,"DIV",{class:!0});var Js=s(Yo);m(LE.$$.fragment,Js),iCo=i(Js),nd=n(Js,"P",{});var gz=s(nd);dCo=r(gz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a multiple choice head) when created
with the `),kre=n(gz,"CODE",{});var fXr=s(kre);cCo=r(fXr,"from_pretrained()"),fXr.forEach(t),fCo=r(gz,"class method or the "),xre=n(gz,"CODE",{});var mXr=s(xre);mCo=r(mXr,"from_config()"),mXr.forEach(t),gCo=r(gz,`class
method.`),gz.forEach(t),hCo=i(Js),BE=n(Js,"P",{});var KBe=s(BE);pCo=r(KBe,"This class cannot be instantiated directly using "),Rre=n(KBe,"CODE",{});var gXr=s(Rre);_Co=r(gXr,"__init__()"),gXr.forEach(t),uCo=r(KBe," (throws an error)."),KBe.forEach(t),bCo=i(Js),zr=n(Js,"DIV",{class:!0});var Ys=s(zr);m(kE.$$.fragment,Ys),vCo=i(Ys),Sre=n(Ys,"P",{});var hXr=s(Sre);TCo=r(hXr,"Instantiates one of the model classes of the library (with a multiple choice head) from a configuration."),hXr.forEach(t),FCo=i(Ys),sd=n(Ys,"P",{});var hz=s(sd);CCo=r(hz,`Note:
Loading a model from its configuration file does `),Pre=n(hz,"STRONG",{});var pXr=s(Pre);MCo=r(pXr,"not"),pXr.forEach(t),ECo=r(hz,` load the model weights. It only affects the
model\u2019s configuration. Use `),$re=n(hz,"CODE",{});var _Xr=s($re);yCo=r(_Xr,"from_pretrained()"),_Xr.forEach(t),wCo=r(hz,"to load the model weights."),hz.forEach(t),ACo=i(Ys),Ire=n(Ys,"P",{});var uXr=s(Ire);LCo=r(uXr,"Examples:"),uXr.forEach(t),BCo=i(Ys),m(xE.$$.fragment,Ys),Ys.forEach(t),kCo=i(Js),Ie=n(Js,"DIV",{class:!0});var Dt=s(Ie);m(RE.$$.fragment,Dt),xCo=i(Dt),jre=n(Dt,"P",{});var bXr=s(jre);RCo=r(bXr,"Instantiate one of the model classes of the library (with a multiple choice head) from a pretrained model."),bXr.forEach(t),SCo=i(Dt),Va=n(Dt,"P",{});var u4=s(Va);PCo=r(u4,"The model class to instantiate is selected based on the "),Nre=n(u4,"CODE",{});var vXr=s(Nre);$Co=r(vXr,"model_type"),vXr.forEach(t),ICo=r(u4,` property of the config object (either
passed as an argument or loaded from `),Dre=n(u4,"CODE",{});var TXr=s(Dre);jCo=r(TXr,"pretrained_model_name_or_path"),TXr.forEach(t),NCo=r(u4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),qre=n(u4,"CODE",{});var FXr=s(qre);DCo=r(FXr,"pretrained_model_name_or_path"),FXr.forEach(t),qCo=r(u4,":"),u4.forEach(t),GCo=i(Dt),G=n(Dt,"UL",{});var O=s(G);_1=n(O,"LI",{});var gMe=s(_1);Gre=n(gMe,"STRONG",{});var CXr=s(Gre);OCo=r(CXr,"albert"),CXr.forEach(t),XCo=r(gMe," \u2014 "),oI=n(gMe,"A",{href:!0});var MXr=s(oI);zCo=r(MXr,"AlbertForMultipleChoice"),MXr.forEach(t),VCo=r(gMe," (ALBERT model)"),gMe.forEach(t),WCo=i(O),u1=n(O,"LI",{});var hMe=s(u1);Ore=n(hMe,"STRONG",{});var EXr=s(Ore);QCo=r(EXr,"bert"),EXr.forEach(t),HCo=r(hMe," \u2014 "),rI=n(hMe,"A",{href:!0});var yXr=s(rI);UCo=r(yXr,"BertForMultipleChoice"),yXr.forEach(t),JCo=r(hMe," (BERT model)"),hMe.forEach(t),YCo=i(O),b1=n(O,"LI",{});var pMe=s(b1);Xre=n(pMe,"STRONG",{});var wXr=s(Xre);KCo=r(wXr,"big_bird"),wXr.forEach(t),ZCo=r(pMe," \u2014 "),tI=n(pMe,"A",{href:!0});var AXr=s(tI);e4o=r(AXr,"BigBirdForMultipleChoice"),AXr.forEach(t),o4o=r(pMe," (BigBird model)"),pMe.forEach(t),r4o=i(O),v1=n(O,"LI",{});var _Me=s(v1);zre=n(_Me,"STRONG",{});var LXr=s(zre);t4o=r(LXr,"camembert"),LXr.forEach(t),a4o=r(_Me," \u2014 "),aI=n(_Me,"A",{href:!0});var BXr=s(aI);n4o=r(BXr,"CamembertForMultipleChoice"),BXr.forEach(t),s4o=r(_Me," (CamemBERT model)"),_Me.forEach(t),l4o=i(O),T1=n(O,"LI",{});var uMe=s(T1);Vre=n(uMe,"STRONG",{});var kXr=s(Vre);i4o=r(kXr,"canine"),kXr.forEach(t),d4o=r(uMe," \u2014 "),nI=n(uMe,"A",{href:!0});var xXr=s(nI);c4o=r(xXr,"CanineForMultipleChoice"),xXr.forEach(t),f4o=r(uMe," (Canine model)"),uMe.forEach(t),m4o=i(O),F1=n(O,"LI",{});var bMe=s(F1);Wre=n(bMe,"STRONG",{});var RXr=s(Wre);g4o=r(RXr,"convbert"),RXr.forEach(t),h4o=r(bMe," \u2014 "),sI=n(bMe,"A",{href:!0});var SXr=s(sI);p4o=r(SXr,"ConvBertForMultipleChoice"),SXr.forEach(t),_4o=r(bMe," (ConvBERT model)"),bMe.forEach(t),u4o=i(O),C1=n(O,"LI",{});var vMe=s(C1);Qre=n(vMe,"STRONG",{});var PXr=s(Qre);b4o=r(PXr,"distilbert"),PXr.forEach(t),v4o=r(vMe," \u2014 "),lI=n(vMe,"A",{href:!0});var $Xr=s(lI);T4o=r($Xr,"DistilBertForMultipleChoice"),$Xr.forEach(t),F4o=r(vMe," (DistilBERT model)"),vMe.forEach(t),C4o=i(O),M1=n(O,"LI",{});var TMe=s(M1);Hre=n(TMe,"STRONG",{});var IXr=s(Hre);M4o=r(IXr,"electra"),IXr.forEach(t),E4o=r(TMe," \u2014 "),iI=n(TMe,"A",{href:!0});var jXr=s(iI);y4o=r(jXr,"ElectraForMultipleChoice"),jXr.forEach(t),w4o=r(TMe," (ELECTRA model)"),TMe.forEach(t),A4o=i(O),E1=n(O,"LI",{});var FMe=s(E1);Ure=n(FMe,"STRONG",{});var NXr=s(Ure);L4o=r(NXr,"flaubert"),NXr.forEach(t),B4o=r(FMe," \u2014 "),dI=n(FMe,"A",{href:!0});var DXr=s(dI);k4o=r(DXr,"FlaubertForMultipleChoice"),DXr.forEach(t),x4o=r(FMe," (FlauBERT model)"),FMe.forEach(t),R4o=i(O),y1=n(O,"LI",{});var CMe=s(y1);Jre=n(CMe,"STRONG",{});var qXr=s(Jre);S4o=r(qXr,"fnet"),qXr.forEach(t),P4o=r(CMe," \u2014 "),cI=n(CMe,"A",{href:!0});var GXr=s(cI);$4o=r(GXr,"FNetForMultipleChoice"),GXr.forEach(t),I4o=r(CMe," (FNet model)"),CMe.forEach(t),j4o=i(O),w1=n(O,"LI",{});var MMe=s(w1);Yre=n(MMe,"STRONG",{});var OXr=s(Yre);N4o=r(OXr,"funnel"),OXr.forEach(t),D4o=r(MMe," \u2014 "),fI=n(MMe,"A",{href:!0});var XXr=s(fI);q4o=r(XXr,"FunnelForMultipleChoice"),XXr.forEach(t),G4o=r(MMe," (Funnel Transformer model)"),MMe.forEach(t),O4o=i(O),A1=n(O,"LI",{});var EMe=s(A1);Kre=n(EMe,"STRONG",{});var zXr=s(Kre);X4o=r(zXr,"ibert"),zXr.forEach(t),z4o=r(EMe," \u2014 "),mI=n(EMe,"A",{href:!0});var VXr=s(mI);V4o=r(VXr,"IBertForMultipleChoice"),VXr.forEach(t),W4o=r(EMe," (I-BERT model)"),EMe.forEach(t),Q4o=i(O),L1=n(O,"LI",{});var yMe=s(L1);Zre=n(yMe,"STRONG",{});var WXr=s(Zre);H4o=r(WXr,"longformer"),WXr.forEach(t),U4o=r(yMe," \u2014 "),gI=n(yMe,"A",{href:!0});var QXr=s(gI);J4o=r(QXr,"LongformerForMultipleChoice"),QXr.forEach(t),Y4o=r(yMe," (Longformer model)"),yMe.forEach(t),K4o=i(O),B1=n(O,"LI",{});var wMe=s(B1);ete=n(wMe,"STRONG",{});var HXr=s(ete);Z4o=r(HXr,"megatron-bert"),HXr.forEach(t),eMo=r(wMe," \u2014 "),hI=n(wMe,"A",{href:!0});var UXr=s(hI);oMo=r(UXr,"MegatronBertForMultipleChoice"),UXr.forEach(t),rMo=r(wMe," (MegatronBert model)"),wMe.forEach(t),tMo=i(O),k1=n(O,"LI",{});var AMe=s(k1);ote=n(AMe,"STRONG",{});var JXr=s(ote);aMo=r(JXr,"mobilebert"),JXr.forEach(t),nMo=r(AMe," \u2014 "),pI=n(AMe,"A",{href:!0});var YXr=s(pI);sMo=r(YXr,"MobileBertForMultipleChoice"),YXr.forEach(t),lMo=r(AMe," (MobileBERT model)"),AMe.forEach(t),iMo=i(O),x1=n(O,"LI",{});var LMe=s(x1);rte=n(LMe,"STRONG",{});var KXr=s(rte);dMo=r(KXr,"mpnet"),KXr.forEach(t),cMo=r(LMe," \u2014 "),_I=n(LMe,"A",{href:!0});var ZXr=s(_I);fMo=r(ZXr,"MPNetForMultipleChoice"),ZXr.forEach(t),mMo=r(LMe," (MPNet model)"),LMe.forEach(t),gMo=i(O),R1=n(O,"LI",{});var BMe=s(R1);tte=n(BMe,"STRONG",{});var ezr=s(tte);hMo=r(ezr,"nystromformer"),ezr.forEach(t),pMo=r(BMe," \u2014 "),uI=n(BMe,"A",{href:!0});var ozr=s(uI);_Mo=r(ozr,"NystromformerForMultipleChoice"),ozr.forEach(t),uMo=r(BMe," (Nystromformer model)"),BMe.forEach(t),bMo=i(O),S1=n(O,"LI",{});var kMe=s(S1);ate=n(kMe,"STRONG",{});var rzr=s(ate);vMo=r(rzr,"qdqbert"),rzr.forEach(t),TMo=r(kMe," \u2014 "),bI=n(kMe,"A",{href:!0});var tzr=s(bI);FMo=r(tzr,"QDQBertForMultipleChoice"),tzr.forEach(t),CMo=r(kMe," (QDQBert model)"),kMe.forEach(t),MMo=i(O),P1=n(O,"LI",{});var xMe=s(P1);nte=n(xMe,"STRONG",{});var azr=s(nte);EMo=r(azr,"rembert"),azr.forEach(t),yMo=r(xMe," \u2014 "),vI=n(xMe,"A",{href:!0});var nzr=s(vI);wMo=r(nzr,"RemBertForMultipleChoice"),nzr.forEach(t),AMo=r(xMe," (RemBERT model)"),xMe.forEach(t),LMo=i(O),$1=n(O,"LI",{});var RMe=s($1);ste=n(RMe,"STRONG",{});var szr=s(ste);BMo=r(szr,"roberta"),szr.forEach(t),kMo=r(RMe," \u2014 "),TI=n(RMe,"A",{href:!0});var lzr=s(TI);xMo=r(lzr,"RobertaForMultipleChoice"),lzr.forEach(t),RMo=r(RMe," (RoBERTa model)"),RMe.forEach(t),SMo=i(O),I1=n(O,"LI",{});var SMe=s(I1);lte=n(SMe,"STRONG",{});var izr=s(lte);PMo=r(izr,"roformer"),izr.forEach(t),$Mo=r(SMe," \u2014 "),FI=n(SMe,"A",{href:!0});var dzr=s(FI);IMo=r(dzr,"RoFormerForMultipleChoice"),dzr.forEach(t),jMo=r(SMe," (RoFormer model)"),SMe.forEach(t),NMo=i(O),j1=n(O,"LI",{});var PMe=s(j1);ite=n(PMe,"STRONG",{});var czr=s(ite);DMo=r(czr,"squeezebert"),czr.forEach(t),qMo=r(PMe," \u2014 "),CI=n(PMe,"A",{href:!0});var fzr=s(CI);GMo=r(fzr,"SqueezeBertForMultipleChoice"),fzr.forEach(t),OMo=r(PMe," (SqueezeBERT model)"),PMe.forEach(t),XMo=i(O),N1=n(O,"LI",{});var $Me=s(N1);dte=n($Me,"STRONG",{});var mzr=s(dte);zMo=r(mzr,"xlm"),mzr.forEach(t),VMo=r($Me," \u2014 "),MI=n($Me,"A",{href:!0});var gzr=s(MI);WMo=r(gzr,"XLMForMultipleChoice"),gzr.forEach(t),QMo=r($Me," (XLM model)"),$Me.forEach(t),HMo=i(O),D1=n(O,"LI",{});var IMe=s(D1);cte=n(IMe,"STRONG",{});var hzr=s(cte);UMo=r(hzr,"xlm-roberta"),hzr.forEach(t),JMo=r(IMe," \u2014 "),EI=n(IMe,"A",{href:!0});var pzr=s(EI);YMo=r(pzr,"XLMRobertaForMultipleChoice"),pzr.forEach(t),KMo=r(IMe," (XLM-RoBERTa model)"),IMe.forEach(t),ZMo=i(O),q1=n(O,"LI",{});var jMe=s(q1);fte=n(jMe,"STRONG",{});var _zr=s(fte);eEo=r(_zr,"xlm-roberta-xl"),_zr.forEach(t),oEo=r(jMe," \u2014 "),yI=n(jMe,"A",{href:!0});var uzr=s(yI);rEo=r(uzr,"XLMRobertaXLForMultipleChoice"),uzr.forEach(t),tEo=r(jMe," (XLM-RoBERTa-XL model)"),jMe.forEach(t),aEo=i(O),G1=n(O,"LI",{});var NMe=s(G1);mte=n(NMe,"STRONG",{});var bzr=s(mte);nEo=r(bzr,"xlnet"),bzr.forEach(t),sEo=r(NMe," \u2014 "),wI=n(NMe,"A",{href:!0});var vzr=s(wI);lEo=r(vzr,"XLNetForMultipleChoice"),vzr.forEach(t),iEo=r(NMe," (XLNet model)"),NMe.forEach(t),dEo=i(O),O1=n(O,"LI",{});var DMe=s(O1);gte=n(DMe,"STRONG",{});var Tzr=s(gte);cEo=r(Tzr,"yoso"),Tzr.forEach(t),fEo=r(DMe," \u2014 "),AI=n(DMe,"A",{href:!0});var Fzr=s(AI);mEo=r(Fzr,"YosoForMultipleChoice"),Fzr.forEach(t),gEo=r(DMe," (YOSO model)"),DMe.forEach(t),O.forEach(t),hEo=i(Dt),X1=n(Dt,"P",{});var qMe=s(X1);pEo=r(qMe,"The model is set in evaluation mode by default using "),hte=n(qMe,"CODE",{});var Czr=s(hte);_Eo=r(Czr,"model.eval()"),Czr.forEach(t),uEo=r(qMe,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),pte=n(qMe,"CODE",{});var Mzr=s(pte);bEo=r(Mzr,"model.train()"),Mzr.forEach(t),qMe.forEach(t),vEo=i(Dt),_te=n(Dt,"P",{});var Ezr=s(_te);TEo=r(Ezr,"Examples:"),Ezr.forEach(t),FEo=i(Dt),m(SE.$$.fragment,Dt),Dt.forEach(t),Js.forEach(t),W7e=i(d),ld=n(d,"H2",{class:!0});var ZBe=s(ld);z1=n(ZBe,"A",{id:!0,class:!0,href:!0});var yzr=s(z1);ute=n(yzr,"SPAN",{});var wzr=s(ute);m(PE.$$.fragment,wzr),wzr.forEach(t),yzr.forEach(t),CEo=i(ZBe),bte=n(ZBe,"SPAN",{});var Azr=s(bte);MEo=r(Azr,"AutoModelForNextSentencePrediction"),Azr.forEach(t),ZBe.forEach(t),Q7e=i(d),Ko=n(d,"DIV",{class:!0});var Ks=s(Ko);m($E.$$.fragment,Ks),EEo=i(Ks),id=n(Ks,"P",{});var pz=s(id);yEo=r(pz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a next sentence prediction head) when created
with the `),vte=n(pz,"CODE",{});var Lzr=s(vte);wEo=r(Lzr,"from_pretrained()"),Lzr.forEach(t),AEo=r(pz,"class method or the "),Tte=n(pz,"CODE",{});var Bzr=s(Tte);LEo=r(Bzr,"from_config()"),Bzr.forEach(t),BEo=r(pz,`class
method.`),pz.forEach(t),kEo=i(Ks),IE=n(Ks,"P",{});var eke=s(IE);xEo=r(eke,"This class cannot be instantiated directly using "),Fte=n(eke,"CODE",{});var kzr=s(Fte);REo=r(kzr,"__init__()"),kzr.forEach(t),SEo=r(eke," (throws an error)."),eke.forEach(t),PEo=i(Ks),Vr=n(Ks,"DIV",{class:!0});var Zs=s(Vr);m(jE.$$.fragment,Zs),$Eo=i(Zs),Cte=n(Zs,"P",{});var xzr=s(Cte);IEo=r(xzr,"Instantiates one of the model classes of the library (with a next sentence prediction head) from a configuration."),xzr.forEach(t),jEo=i(Zs),dd=n(Zs,"P",{});var _z=s(dd);NEo=r(_z,`Note:
Loading a model from its configuration file does `),Mte=n(_z,"STRONG",{});var Rzr=s(Mte);DEo=r(Rzr,"not"),Rzr.forEach(t),qEo=r(_z,` load the model weights. It only affects the
model\u2019s configuration. Use `),Ete=n(_z,"CODE",{});var Szr=s(Ete);GEo=r(Szr,"from_pretrained()"),Szr.forEach(t),OEo=r(_z,"to load the model weights."),_z.forEach(t),XEo=i(Zs),yte=n(Zs,"P",{});var Pzr=s(yte);zEo=r(Pzr,"Examples:"),Pzr.forEach(t),VEo=i(Zs),m(NE.$$.fragment,Zs),Zs.forEach(t),WEo=i(Ks),je=n(Ks,"DIV",{class:!0});var qt=s(je);m(DE.$$.fragment,qt),QEo=i(qt),wte=n(qt,"P",{});var $zr=s(wte);HEo=r($zr,"Instantiate one of the model classes of the library (with a next sentence prediction head) from a pretrained model."),$zr.forEach(t),UEo=i(qt),Wa=n(qt,"P",{});var b4=s(Wa);JEo=r(b4,"The model class to instantiate is selected based on the "),Ate=n(b4,"CODE",{});var Izr=s(Ate);YEo=r(Izr,"model_type"),Izr.forEach(t),KEo=r(b4,` property of the config object (either
passed as an argument or loaded from `),Lte=n(b4,"CODE",{});var jzr=s(Lte);ZEo=r(jzr,"pretrained_model_name_or_path"),jzr.forEach(t),e3o=r(b4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Bte=n(b4,"CODE",{});var Nzr=s(Bte);o3o=r(Nzr,"pretrained_model_name_or_path"),Nzr.forEach(t),r3o=r(b4,":"),b4.forEach(t),t3o=i(qt),na=n(qt,"UL",{});var el=s(na);V1=n(el,"LI",{});var GMe=s(V1);kte=n(GMe,"STRONG",{});var Dzr=s(kte);a3o=r(Dzr,"bert"),Dzr.forEach(t),n3o=r(GMe," \u2014 "),LI=n(GMe,"A",{href:!0});var qzr=s(LI);s3o=r(qzr,"BertForNextSentencePrediction"),qzr.forEach(t),l3o=r(GMe," (BERT model)"),GMe.forEach(t),i3o=i(el),W1=n(el,"LI",{});var OMe=s(W1);xte=n(OMe,"STRONG",{});var Gzr=s(xte);d3o=r(Gzr,"fnet"),Gzr.forEach(t),c3o=r(OMe," \u2014 "),BI=n(OMe,"A",{href:!0});var Ozr=s(BI);f3o=r(Ozr,"FNetForNextSentencePrediction"),Ozr.forEach(t),m3o=r(OMe," (FNet model)"),OMe.forEach(t),g3o=i(el),Q1=n(el,"LI",{});var XMe=s(Q1);Rte=n(XMe,"STRONG",{});var Xzr=s(Rte);h3o=r(Xzr,"megatron-bert"),Xzr.forEach(t),p3o=r(XMe," \u2014 "),kI=n(XMe,"A",{href:!0});var zzr=s(kI);_3o=r(zzr,"MegatronBertForNextSentencePrediction"),zzr.forEach(t),u3o=r(XMe," (MegatronBert model)"),XMe.forEach(t),b3o=i(el),H1=n(el,"LI",{});var zMe=s(H1);Ste=n(zMe,"STRONG",{});var Vzr=s(Ste);v3o=r(Vzr,"mobilebert"),Vzr.forEach(t),T3o=r(zMe," \u2014 "),xI=n(zMe,"A",{href:!0});var Wzr=s(xI);F3o=r(Wzr,"MobileBertForNextSentencePrediction"),Wzr.forEach(t),C3o=r(zMe," (MobileBERT model)"),zMe.forEach(t),M3o=i(el),U1=n(el,"LI",{});var VMe=s(U1);Pte=n(VMe,"STRONG",{});var Qzr=s(Pte);E3o=r(Qzr,"qdqbert"),Qzr.forEach(t),y3o=r(VMe," \u2014 "),RI=n(VMe,"A",{href:!0});var Hzr=s(RI);w3o=r(Hzr,"QDQBertForNextSentencePrediction"),Hzr.forEach(t),A3o=r(VMe," (QDQBert model)"),VMe.forEach(t),el.forEach(t),L3o=i(qt),J1=n(qt,"P",{});var WMe=s(J1);B3o=r(WMe,"The model is set in evaluation mode by default using "),$te=n(WMe,"CODE",{});var Uzr=s($te);k3o=r(Uzr,"model.eval()"),Uzr.forEach(t),x3o=r(WMe,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Ite=n(WMe,"CODE",{});var Jzr=s(Ite);R3o=r(Jzr,"model.train()"),Jzr.forEach(t),WMe.forEach(t),S3o=i(qt),jte=n(qt,"P",{});var Yzr=s(jte);P3o=r(Yzr,"Examples:"),Yzr.forEach(t),$3o=i(qt),m(qE.$$.fragment,qt),qt.forEach(t),Ks.forEach(t),H7e=i(d),cd=n(d,"H2",{class:!0});var oke=s(cd);Y1=n(oke,"A",{id:!0,class:!0,href:!0});var Kzr=s(Y1);Nte=n(Kzr,"SPAN",{});var Zzr=s(Nte);m(GE.$$.fragment,Zzr),Zzr.forEach(t),Kzr.forEach(t),I3o=i(oke),Dte=n(oke,"SPAN",{});var eVr=s(Dte);j3o=r(eVr,"AutoModelForTokenClassification"),eVr.forEach(t),oke.forEach(t),U7e=i(d),Zo=n(d,"DIV",{class:!0});var ol=s(Zo);m(OE.$$.fragment,ol),N3o=i(ol),fd=n(ol,"P",{});var uz=s(fd);D3o=r(uz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a token classification head) when created
with the `),qte=n(uz,"CODE",{});var oVr=s(qte);q3o=r(oVr,"from_pretrained()"),oVr.forEach(t),G3o=r(uz,"class method or the "),Gte=n(uz,"CODE",{});var rVr=s(Gte);O3o=r(rVr,"from_config()"),rVr.forEach(t),X3o=r(uz,`class
method.`),uz.forEach(t),z3o=i(ol),XE=n(ol,"P",{});var rke=s(XE);V3o=r(rke,"This class cannot be instantiated directly using "),Ote=n(rke,"CODE",{});var tVr=s(Ote);W3o=r(tVr,"__init__()"),tVr.forEach(t),Q3o=r(rke," (throws an error)."),rke.forEach(t),H3o=i(ol),Wr=n(ol,"DIV",{class:!0});var rl=s(Wr);m(zE.$$.fragment,rl),U3o=i(rl),Xte=n(rl,"P",{});var aVr=s(Xte);J3o=r(aVr,"Instantiates one of the model classes of the library (with a token classification head) from a configuration."),aVr.forEach(t),Y3o=i(rl),md=n(rl,"P",{});var bz=s(md);K3o=r(bz,`Note:
Loading a model from its configuration file does `),zte=n(bz,"STRONG",{});var nVr=s(zte);Z3o=r(nVr,"not"),nVr.forEach(t),eyo=r(bz,` load the model weights. It only affects the
model\u2019s configuration. Use `),Vte=n(bz,"CODE",{});var sVr=s(Vte);oyo=r(sVr,"from_pretrained()"),sVr.forEach(t),ryo=r(bz,"to load the model weights."),bz.forEach(t),tyo=i(rl),Wte=n(rl,"P",{});var lVr=s(Wte);ayo=r(lVr,"Examples:"),lVr.forEach(t),nyo=i(rl),m(VE.$$.fragment,rl),rl.forEach(t),syo=i(ol),Ne=n(ol,"DIV",{class:!0});var Gt=s(Ne);m(WE.$$.fragment,Gt),lyo=i(Gt),Qte=n(Gt,"P",{});var iVr=s(Qte);iyo=r(iVr,"Instantiate one of the model classes of the library (with a token classification head) from a pretrained model."),iVr.forEach(t),dyo=i(Gt),Qa=n(Gt,"P",{});var v4=s(Qa);cyo=r(v4,"The model class to instantiate is selected based on the "),Hte=n(v4,"CODE",{});var dVr=s(Hte);fyo=r(dVr,"model_type"),dVr.forEach(t),myo=r(v4,` property of the config object (either
passed as an argument or loaded from `),Ute=n(v4,"CODE",{});var cVr=s(Ute);gyo=r(cVr,"pretrained_model_name_or_path"),cVr.forEach(t),hyo=r(v4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Jte=n(v4,"CODE",{});var fVr=s(Jte);pyo=r(fVr,"pretrained_model_name_or_path"),fVr.forEach(t),_yo=r(v4,":"),v4.forEach(t),uyo=i(Gt),D=n(Gt,"UL",{});var q=s(D);K1=n(q,"LI",{});var QMe=s(K1);Yte=n(QMe,"STRONG",{});var mVr=s(Yte);byo=r(mVr,"albert"),mVr.forEach(t),vyo=r(QMe," \u2014 "),SI=n(QMe,"A",{href:!0});var gVr=s(SI);Tyo=r(gVr,"AlbertForTokenClassification"),gVr.forEach(t),Fyo=r(QMe," (ALBERT model)"),QMe.forEach(t),Cyo=i(q),Z1=n(q,"LI",{});var HMe=s(Z1);Kte=n(HMe,"STRONG",{});var hVr=s(Kte);Myo=r(hVr,"bert"),hVr.forEach(t),Eyo=r(HMe," \u2014 "),PI=n(HMe,"A",{href:!0});var pVr=s(PI);yyo=r(pVr,"BertForTokenClassification"),pVr.forEach(t),wyo=r(HMe," (BERT model)"),HMe.forEach(t),Ayo=i(q),eb=n(q,"LI",{});var UMe=s(eb);Zte=n(UMe,"STRONG",{});var _Vr=s(Zte);Lyo=r(_Vr,"big_bird"),_Vr.forEach(t),Byo=r(UMe," \u2014 "),$I=n(UMe,"A",{href:!0});var uVr=s($I);kyo=r(uVr,"BigBirdForTokenClassification"),uVr.forEach(t),xyo=r(UMe," (BigBird model)"),UMe.forEach(t),Ryo=i(q),ob=n(q,"LI",{});var JMe=s(ob);eae=n(JMe,"STRONG",{});var bVr=s(eae);Syo=r(bVr,"camembert"),bVr.forEach(t),Pyo=r(JMe," \u2014 "),II=n(JMe,"A",{href:!0});var vVr=s(II);$yo=r(vVr,"CamembertForTokenClassification"),vVr.forEach(t),Iyo=r(JMe," (CamemBERT model)"),JMe.forEach(t),jyo=i(q),rb=n(q,"LI",{});var YMe=s(rb);oae=n(YMe,"STRONG",{});var TVr=s(oae);Nyo=r(TVr,"canine"),TVr.forEach(t),Dyo=r(YMe," \u2014 "),jI=n(YMe,"A",{href:!0});var FVr=s(jI);qyo=r(FVr,"CanineForTokenClassification"),FVr.forEach(t),Gyo=r(YMe," (Canine model)"),YMe.forEach(t),Oyo=i(q),tb=n(q,"LI",{});var KMe=s(tb);rae=n(KMe,"STRONG",{});var CVr=s(rae);Xyo=r(CVr,"convbert"),CVr.forEach(t),zyo=r(KMe," \u2014 "),NI=n(KMe,"A",{href:!0});var MVr=s(NI);Vyo=r(MVr,"ConvBertForTokenClassification"),MVr.forEach(t),Wyo=r(KMe," (ConvBERT model)"),KMe.forEach(t),Qyo=i(q),ab=n(q,"LI",{});var ZMe=s(ab);tae=n(ZMe,"STRONG",{});var EVr=s(tae);Hyo=r(EVr,"deberta"),EVr.forEach(t),Uyo=r(ZMe," \u2014 "),DI=n(ZMe,"A",{href:!0});var yVr=s(DI);Jyo=r(yVr,"DebertaForTokenClassification"),yVr.forEach(t),Yyo=r(ZMe," (DeBERTa model)"),ZMe.forEach(t),Kyo=i(q),nb=n(q,"LI",{});var eEe=s(nb);aae=n(eEe,"STRONG",{});var wVr=s(aae);Zyo=r(wVr,"deberta-v2"),wVr.forEach(t),ewo=r(eEe," \u2014 "),qI=n(eEe,"A",{href:!0});var AVr=s(qI);owo=r(AVr,"DebertaV2ForTokenClassification"),AVr.forEach(t),rwo=r(eEe," (DeBERTa-v2 model)"),eEe.forEach(t),two=i(q),sb=n(q,"LI",{});var oEe=s(sb);nae=n(oEe,"STRONG",{});var LVr=s(nae);awo=r(LVr,"distilbert"),LVr.forEach(t),nwo=r(oEe," \u2014 "),GI=n(oEe,"A",{href:!0});var BVr=s(GI);swo=r(BVr,"DistilBertForTokenClassification"),BVr.forEach(t),lwo=r(oEe," (DistilBERT model)"),oEe.forEach(t),iwo=i(q),lb=n(q,"LI",{});var rEe=s(lb);sae=n(rEe,"STRONG",{});var kVr=s(sae);dwo=r(kVr,"electra"),kVr.forEach(t),cwo=r(rEe," \u2014 "),OI=n(rEe,"A",{href:!0});var xVr=s(OI);fwo=r(xVr,"ElectraForTokenClassification"),xVr.forEach(t),mwo=r(rEe," (ELECTRA model)"),rEe.forEach(t),gwo=i(q),ib=n(q,"LI",{});var tEe=s(ib);lae=n(tEe,"STRONG",{});var RVr=s(lae);hwo=r(RVr,"flaubert"),RVr.forEach(t),pwo=r(tEe," \u2014 "),XI=n(tEe,"A",{href:!0});var SVr=s(XI);_wo=r(SVr,"FlaubertForTokenClassification"),SVr.forEach(t),uwo=r(tEe," (FlauBERT model)"),tEe.forEach(t),bwo=i(q),db=n(q,"LI",{});var aEe=s(db);iae=n(aEe,"STRONG",{});var PVr=s(iae);vwo=r(PVr,"fnet"),PVr.forEach(t),Two=r(aEe," \u2014 "),zI=n(aEe,"A",{href:!0});var $Vr=s(zI);Fwo=r($Vr,"FNetForTokenClassification"),$Vr.forEach(t),Cwo=r(aEe," (FNet model)"),aEe.forEach(t),Mwo=i(q),cb=n(q,"LI",{});var nEe=s(cb);dae=n(nEe,"STRONG",{});var IVr=s(dae);Ewo=r(IVr,"funnel"),IVr.forEach(t),ywo=r(nEe," \u2014 "),VI=n(nEe,"A",{href:!0});var jVr=s(VI);wwo=r(jVr,"FunnelForTokenClassification"),jVr.forEach(t),Awo=r(nEe," (Funnel Transformer model)"),nEe.forEach(t),Lwo=i(q),fb=n(q,"LI",{});var sEe=s(fb);cae=n(sEe,"STRONG",{});var NVr=s(cae);Bwo=r(NVr,"gpt2"),NVr.forEach(t),kwo=r(sEe," \u2014 "),WI=n(sEe,"A",{href:!0});var DVr=s(WI);xwo=r(DVr,"GPT2ForTokenClassification"),DVr.forEach(t),Rwo=r(sEe," (OpenAI GPT-2 model)"),sEe.forEach(t),Swo=i(q),mb=n(q,"LI",{});var lEe=s(mb);fae=n(lEe,"STRONG",{});var qVr=s(fae);Pwo=r(qVr,"ibert"),qVr.forEach(t),$wo=r(lEe," \u2014 "),QI=n(lEe,"A",{href:!0});var GVr=s(QI);Iwo=r(GVr,"IBertForTokenClassification"),GVr.forEach(t),jwo=r(lEe," (I-BERT model)"),lEe.forEach(t),Nwo=i(q),gb=n(q,"LI",{});var iEe=s(gb);mae=n(iEe,"STRONG",{});var OVr=s(mae);Dwo=r(OVr,"layoutlm"),OVr.forEach(t),qwo=r(iEe," \u2014 "),HI=n(iEe,"A",{href:!0});var XVr=s(HI);Gwo=r(XVr,"LayoutLMForTokenClassification"),XVr.forEach(t),Owo=r(iEe," (LayoutLM model)"),iEe.forEach(t),Xwo=i(q),hb=n(q,"LI",{});var dEe=s(hb);gae=n(dEe,"STRONG",{});var zVr=s(gae);zwo=r(zVr,"layoutlmv2"),zVr.forEach(t),Vwo=r(dEe," \u2014 "),UI=n(dEe,"A",{href:!0});var VVr=s(UI);Wwo=r(VVr,"LayoutLMv2ForTokenClassification"),VVr.forEach(t),Qwo=r(dEe," (LayoutLMv2 model)"),dEe.forEach(t),Hwo=i(q),pb=n(q,"LI",{});var cEe=s(pb);hae=n(cEe,"STRONG",{});var WVr=s(hae);Uwo=r(WVr,"longformer"),WVr.forEach(t),Jwo=r(cEe," \u2014 "),JI=n(cEe,"A",{href:!0});var QVr=s(JI);Ywo=r(QVr,"LongformerForTokenClassification"),QVr.forEach(t),Kwo=r(cEe," (Longformer model)"),cEe.forEach(t),Zwo=i(q),_b=n(q,"LI",{});var fEe=s(_b);pae=n(fEe,"STRONG",{});var HVr=s(pae);eAo=r(HVr,"megatron-bert"),HVr.forEach(t),oAo=r(fEe," \u2014 "),YI=n(fEe,"A",{href:!0});var UVr=s(YI);rAo=r(UVr,"MegatronBertForTokenClassification"),UVr.forEach(t),tAo=r(fEe," (MegatronBert model)"),fEe.forEach(t),aAo=i(q),ub=n(q,"LI",{});var mEe=s(ub);_ae=n(mEe,"STRONG",{});var JVr=s(_ae);nAo=r(JVr,"mobilebert"),JVr.forEach(t),sAo=r(mEe," \u2014 "),KI=n(mEe,"A",{href:!0});var YVr=s(KI);lAo=r(YVr,"MobileBertForTokenClassification"),YVr.forEach(t),iAo=r(mEe," (MobileBERT model)"),mEe.forEach(t),dAo=i(q),bb=n(q,"LI",{});var gEe=s(bb);uae=n(gEe,"STRONG",{});var KVr=s(uae);cAo=r(KVr,"mpnet"),KVr.forEach(t),fAo=r(gEe," \u2014 "),ZI=n(gEe,"A",{href:!0});var ZVr=s(ZI);mAo=r(ZVr,"MPNetForTokenClassification"),ZVr.forEach(t),gAo=r(gEe," (MPNet model)"),gEe.forEach(t),hAo=i(q),vb=n(q,"LI",{});var hEe=s(vb);bae=n(hEe,"STRONG",{});var eWr=s(bae);pAo=r(eWr,"nystromformer"),eWr.forEach(t),_Ao=r(hEe," \u2014 "),ej=n(hEe,"A",{href:!0});var oWr=s(ej);uAo=r(oWr,"NystromformerForTokenClassification"),oWr.forEach(t),bAo=r(hEe," (Nystromformer model)"),hEe.forEach(t),vAo=i(q),Tb=n(q,"LI",{});var pEe=s(Tb);vae=n(pEe,"STRONG",{});var rWr=s(vae);TAo=r(rWr,"qdqbert"),rWr.forEach(t),FAo=r(pEe," \u2014 "),oj=n(pEe,"A",{href:!0});var tWr=s(oj);CAo=r(tWr,"QDQBertForTokenClassification"),tWr.forEach(t),MAo=r(pEe," (QDQBert model)"),pEe.forEach(t),EAo=i(q),Fb=n(q,"LI",{});var _Ee=s(Fb);Tae=n(_Ee,"STRONG",{});var aWr=s(Tae);yAo=r(aWr,"rembert"),aWr.forEach(t),wAo=r(_Ee," \u2014 "),rj=n(_Ee,"A",{href:!0});var nWr=s(rj);AAo=r(nWr,"RemBertForTokenClassification"),nWr.forEach(t),LAo=r(_Ee," (RemBERT model)"),_Ee.forEach(t),BAo=i(q),Cb=n(q,"LI",{});var uEe=s(Cb);Fae=n(uEe,"STRONG",{});var sWr=s(Fae);kAo=r(sWr,"roberta"),sWr.forEach(t),xAo=r(uEe," \u2014 "),tj=n(uEe,"A",{href:!0});var lWr=s(tj);RAo=r(lWr,"RobertaForTokenClassification"),lWr.forEach(t),SAo=r(uEe," (RoBERTa model)"),uEe.forEach(t),PAo=i(q),Mb=n(q,"LI",{});var bEe=s(Mb);Cae=n(bEe,"STRONG",{});var iWr=s(Cae);$Ao=r(iWr,"roformer"),iWr.forEach(t),IAo=r(bEe," \u2014 "),aj=n(bEe,"A",{href:!0});var dWr=s(aj);jAo=r(dWr,"RoFormerForTokenClassification"),dWr.forEach(t),NAo=r(bEe," (RoFormer model)"),bEe.forEach(t),DAo=i(q),Eb=n(q,"LI",{});var vEe=s(Eb);Mae=n(vEe,"STRONG",{});var cWr=s(Mae);qAo=r(cWr,"squeezebert"),cWr.forEach(t),GAo=r(vEe," \u2014 "),nj=n(vEe,"A",{href:!0});var fWr=s(nj);OAo=r(fWr,"SqueezeBertForTokenClassification"),fWr.forEach(t),XAo=r(vEe," (SqueezeBERT model)"),vEe.forEach(t),zAo=i(q),yb=n(q,"LI",{});var TEe=s(yb);Eae=n(TEe,"STRONG",{});var mWr=s(Eae);VAo=r(mWr,"xlm"),mWr.forEach(t),WAo=r(TEe," \u2014 "),sj=n(TEe,"A",{href:!0});var gWr=s(sj);QAo=r(gWr,"XLMForTokenClassification"),gWr.forEach(t),HAo=r(TEe," (XLM model)"),TEe.forEach(t),UAo=i(q),wb=n(q,"LI",{});var FEe=s(wb);yae=n(FEe,"STRONG",{});var hWr=s(yae);JAo=r(hWr,"xlm-roberta"),hWr.forEach(t),YAo=r(FEe," \u2014 "),lj=n(FEe,"A",{href:!0});var pWr=s(lj);KAo=r(pWr,"XLMRobertaForTokenClassification"),pWr.forEach(t),ZAo=r(FEe," (XLM-RoBERTa model)"),FEe.forEach(t),e0o=i(q),Ab=n(q,"LI",{});var CEe=s(Ab);wae=n(CEe,"STRONG",{});var _Wr=s(wae);o0o=r(_Wr,"xlm-roberta-xl"),_Wr.forEach(t),r0o=r(CEe," \u2014 "),ij=n(CEe,"A",{href:!0});var uWr=s(ij);t0o=r(uWr,"XLMRobertaXLForTokenClassification"),uWr.forEach(t),a0o=r(CEe," (XLM-RoBERTa-XL model)"),CEe.forEach(t),n0o=i(q),Lb=n(q,"LI",{});var MEe=s(Lb);Aae=n(MEe,"STRONG",{});var bWr=s(Aae);s0o=r(bWr,"xlnet"),bWr.forEach(t),l0o=r(MEe," \u2014 "),dj=n(MEe,"A",{href:!0});var vWr=s(dj);i0o=r(vWr,"XLNetForTokenClassification"),vWr.forEach(t),d0o=r(MEe," (XLNet model)"),MEe.forEach(t),c0o=i(q),Bb=n(q,"LI",{});var EEe=s(Bb);Lae=n(EEe,"STRONG",{});var TWr=s(Lae);f0o=r(TWr,"yoso"),TWr.forEach(t),m0o=r(EEe," \u2014 "),cj=n(EEe,"A",{href:!0});var FWr=s(cj);g0o=r(FWr,"YosoForTokenClassification"),FWr.forEach(t),h0o=r(EEe," (YOSO model)"),EEe.forEach(t),q.forEach(t),p0o=i(Gt),kb=n(Gt,"P",{});var yEe=s(kb);_0o=r(yEe,"The model is set in evaluation mode by default using "),Bae=n(yEe,"CODE",{});var CWr=s(Bae);u0o=r(CWr,"model.eval()"),CWr.forEach(t),b0o=r(yEe,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),kae=n(yEe,"CODE",{});var MWr=s(kae);v0o=r(MWr,"model.train()"),MWr.forEach(t),yEe.forEach(t),T0o=i(Gt),xae=n(Gt,"P",{});var EWr=s(xae);F0o=r(EWr,"Examples:"),EWr.forEach(t),C0o=i(Gt),m(QE.$$.fragment,Gt),Gt.forEach(t),ol.forEach(t),J7e=i(d),gd=n(d,"H2",{class:!0});var tke=s(gd);xb=n(tke,"A",{id:!0,class:!0,href:!0});var yWr=s(xb);Rae=n(yWr,"SPAN",{});var wWr=s(Rae);m(HE.$$.fragment,wWr),wWr.forEach(t),yWr.forEach(t),M0o=i(tke),Sae=n(tke,"SPAN",{});var AWr=s(Sae);E0o=r(AWr,"AutoModelForQuestionAnswering"),AWr.forEach(t),tke.forEach(t),Y7e=i(d),er=n(d,"DIV",{class:!0});var tl=s(er);m(UE.$$.fragment,tl),y0o=i(tl),hd=n(tl,"P",{});var vz=s(hd);w0o=r(vz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a question answering head) when created
with the `),Pae=n(vz,"CODE",{});var LWr=s(Pae);A0o=r(LWr,"from_pretrained()"),LWr.forEach(t),L0o=r(vz,"class method or the "),$ae=n(vz,"CODE",{});var BWr=s($ae);B0o=r(BWr,"from_config()"),BWr.forEach(t),k0o=r(vz,`class
method.`),vz.forEach(t),x0o=i(tl),JE=n(tl,"P",{});var ake=s(JE);R0o=r(ake,"This class cannot be instantiated directly using "),Iae=n(ake,"CODE",{});var kWr=s(Iae);S0o=r(kWr,"__init__()"),kWr.forEach(t),P0o=r(ake," (throws an error)."),ake.forEach(t),$0o=i(tl),Qr=n(tl,"DIV",{class:!0});var al=s(Qr);m(YE.$$.fragment,al),I0o=i(al),jae=n(al,"P",{});var xWr=s(jae);j0o=r(xWr,"Instantiates one of the model classes of the library (with a question answering head) from a configuration."),xWr.forEach(t),N0o=i(al),pd=n(al,"P",{});var Tz=s(pd);D0o=r(Tz,`Note:
Loading a model from its configuration file does `),Nae=n(Tz,"STRONG",{});var RWr=s(Nae);q0o=r(RWr,"not"),RWr.forEach(t),G0o=r(Tz,` load the model weights. It only affects the
model\u2019s configuration. Use `),Dae=n(Tz,"CODE",{});var SWr=s(Dae);O0o=r(SWr,"from_pretrained()"),SWr.forEach(t),X0o=r(Tz,"to load the model weights."),Tz.forEach(t),z0o=i(al),qae=n(al,"P",{});var PWr=s(qae);V0o=r(PWr,"Examples:"),PWr.forEach(t),W0o=i(al),m(KE.$$.fragment,al),al.forEach(t),Q0o=i(tl),De=n(tl,"DIV",{class:!0});var Ot=s(De);m(ZE.$$.fragment,Ot),H0o=i(Ot),Gae=n(Ot,"P",{});var $Wr=s(Gae);U0o=r($Wr,"Instantiate one of the model classes of the library (with a question answering head) from a pretrained model."),$Wr.forEach(t),J0o=i(Ot),Ha=n(Ot,"P",{});var T4=s(Ha);Y0o=r(T4,"The model class to instantiate is selected based on the "),Oae=n(T4,"CODE",{});var IWr=s(Oae);K0o=r(IWr,"model_type"),IWr.forEach(t),Z0o=r(T4,` property of the config object (either
passed as an argument or loaded from `),Xae=n(T4,"CODE",{});var jWr=s(Xae);eLo=r(jWr,"pretrained_model_name_or_path"),jWr.forEach(t),oLo=r(T4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),zae=n(T4,"CODE",{});var NWr=s(zae);rLo=r(NWr,"pretrained_model_name_or_path"),NWr.forEach(t),tLo=r(T4,":"),T4.forEach(t),aLo=i(Ot),R=n(Ot,"UL",{});var P=s(R);Rb=n(P,"LI",{});var wEe=s(Rb);Vae=n(wEe,"STRONG",{});var DWr=s(Vae);nLo=r(DWr,"albert"),DWr.forEach(t),sLo=r(wEe," \u2014 "),fj=n(wEe,"A",{href:!0});var qWr=s(fj);lLo=r(qWr,"AlbertForQuestionAnswering"),qWr.forEach(t),iLo=r(wEe," (ALBERT model)"),wEe.forEach(t),dLo=i(P),Sb=n(P,"LI",{});var AEe=s(Sb);Wae=n(AEe,"STRONG",{});var GWr=s(Wae);cLo=r(GWr,"bart"),GWr.forEach(t),fLo=r(AEe," \u2014 "),mj=n(AEe,"A",{href:!0});var OWr=s(mj);mLo=r(OWr,"BartForQuestionAnswering"),OWr.forEach(t),gLo=r(AEe," (BART model)"),AEe.forEach(t),hLo=i(P),Pb=n(P,"LI",{});var LEe=s(Pb);Qae=n(LEe,"STRONG",{});var XWr=s(Qae);pLo=r(XWr,"bert"),XWr.forEach(t),_Lo=r(LEe," \u2014 "),gj=n(LEe,"A",{href:!0});var zWr=s(gj);uLo=r(zWr,"BertForQuestionAnswering"),zWr.forEach(t),bLo=r(LEe," (BERT model)"),LEe.forEach(t),vLo=i(P),$b=n(P,"LI",{});var BEe=s($b);Hae=n(BEe,"STRONG",{});var VWr=s(Hae);TLo=r(VWr,"big_bird"),VWr.forEach(t),FLo=r(BEe," \u2014 "),hj=n(BEe,"A",{href:!0});var WWr=s(hj);CLo=r(WWr,"BigBirdForQuestionAnswering"),WWr.forEach(t),MLo=r(BEe," (BigBird model)"),BEe.forEach(t),ELo=i(P),Ib=n(P,"LI",{});var kEe=s(Ib);Uae=n(kEe,"STRONG",{});var QWr=s(Uae);yLo=r(QWr,"bigbird_pegasus"),QWr.forEach(t),wLo=r(kEe," \u2014 "),pj=n(kEe,"A",{href:!0});var HWr=s(pj);ALo=r(HWr,"BigBirdPegasusForQuestionAnswering"),HWr.forEach(t),LLo=r(kEe," (BigBirdPegasus model)"),kEe.forEach(t),BLo=i(P),jb=n(P,"LI",{});var xEe=s(jb);Jae=n(xEe,"STRONG",{});var UWr=s(Jae);kLo=r(UWr,"camembert"),UWr.forEach(t),xLo=r(xEe," \u2014 "),_j=n(xEe,"A",{href:!0});var JWr=s(_j);RLo=r(JWr,"CamembertForQuestionAnswering"),JWr.forEach(t),SLo=r(xEe," (CamemBERT model)"),xEe.forEach(t),PLo=i(P),Nb=n(P,"LI",{});var REe=s(Nb);Yae=n(REe,"STRONG",{});var YWr=s(Yae);$Lo=r(YWr,"canine"),YWr.forEach(t),ILo=r(REe," \u2014 "),uj=n(REe,"A",{href:!0});var KWr=s(uj);jLo=r(KWr,"CanineForQuestionAnswering"),KWr.forEach(t),NLo=r(REe," (Canine model)"),REe.forEach(t),DLo=i(P),Db=n(P,"LI",{});var SEe=s(Db);Kae=n(SEe,"STRONG",{});var ZWr=s(Kae);qLo=r(ZWr,"convbert"),ZWr.forEach(t),GLo=r(SEe," \u2014 "),bj=n(SEe,"A",{href:!0});var eQr=s(bj);OLo=r(eQr,"ConvBertForQuestionAnswering"),eQr.forEach(t),XLo=r(SEe," (ConvBERT model)"),SEe.forEach(t),zLo=i(P),qb=n(P,"LI",{});var PEe=s(qb);Zae=n(PEe,"STRONG",{});var oQr=s(Zae);VLo=r(oQr,"deberta"),oQr.forEach(t),WLo=r(PEe," \u2014 "),vj=n(PEe,"A",{href:!0});var rQr=s(vj);QLo=r(rQr,"DebertaForQuestionAnswering"),rQr.forEach(t),HLo=r(PEe," (DeBERTa model)"),PEe.forEach(t),ULo=i(P),Gb=n(P,"LI",{});var $Ee=s(Gb);ene=n($Ee,"STRONG",{});var tQr=s(ene);JLo=r(tQr,"deberta-v2"),tQr.forEach(t),YLo=r($Ee," \u2014 "),Tj=n($Ee,"A",{href:!0});var aQr=s(Tj);KLo=r(aQr,"DebertaV2ForQuestionAnswering"),aQr.forEach(t),ZLo=r($Ee," (DeBERTa-v2 model)"),$Ee.forEach(t),e7o=i(P),Ob=n(P,"LI",{});var IEe=s(Ob);one=n(IEe,"STRONG",{});var nQr=s(one);o7o=r(nQr,"distilbert"),nQr.forEach(t),r7o=r(IEe," \u2014 "),Fj=n(IEe,"A",{href:!0});var sQr=s(Fj);t7o=r(sQr,"DistilBertForQuestionAnswering"),sQr.forEach(t),a7o=r(IEe," (DistilBERT model)"),IEe.forEach(t),n7o=i(P),Xb=n(P,"LI",{});var jEe=s(Xb);rne=n(jEe,"STRONG",{});var lQr=s(rne);s7o=r(lQr,"electra"),lQr.forEach(t),l7o=r(jEe," \u2014 "),Cj=n(jEe,"A",{href:!0});var iQr=s(Cj);i7o=r(iQr,"ElectraForQuestionAnswering"),iQr.forEach(t),d7o=r(jEe," (ELECTRA model)"),jEe.forEach(t),c7o=i(P),zb=n(P,"LI",{});var NEe=s(zb);tne=n(NEe,"STRONG",{});var dQr=s(tne);f7o=r(dQr,"flaubert"),dQr.forEach(t),m7o=r(NEe," \u2014 "),Mj=n(NEe,"A",{href:!0});var cQr=s(Mj);g7o=r(cQr,"FlaubertForQuestionAnsweringSimple"),cQr.forEach(t),h7o=r(NEe," (FlauBERT model)"),NEe.forEach(t),p7o=i(P),Vb=n(P,"LI",{});var DEe=s(Vb);ane=n(DEe,"STRONG",{});var fQr=s(ane);_7o=r(fQr,"fnet"),fQr.forEach(t),u7o=r(DEe," \u2014 "),Ej=n(DEe,"A",{href:!0});var mQr=s(Ej);b7o=r(mQr,"FNetForQuestionAnswering"),mQr.forEach(t),v7o=r(DEe," (FNet model)"),DEe.forEach(t),T7o=i(P),Wb=n(P,"LI",{});var qEe=s(Wb);nne=n(qEe,"STRONG",{});var gQr=s(nne);F7o=r(gQr,"funnel"),gQr.forEach(t),C7o=r(qEe," \u2014 "),yj=n(qEe,"A",{href:!0});var hQr=s(yj);M7o=r(hQr,"FunnelForQuestionAnswering"),hQr.forEach(t),E7o=r(qEe," (Funnel Transformer model)"),qEe.forEach(t),y7o=i(P),Qb=n(P,"LI",{});var GEe=s(Qb);sne=n(GEe,"STRONG",{});var pQr=s(sne);w7o=r(pQr,"gptj"),pQr.forEach(t),A7o=r(GEe," \u2014 "),wj=n(GEe,"A",{href:!0});var _Qr=s(wj);L7o=r(_Qr,"GPTJForQuestionAnswering"),_Qr.forEach(t),B7o=r(GEe," (GPT-J model)"),GEe.forEach(t),k7o=i(P),Hb=n(P,"LI",{});var OEe=s(Hb);lne=n(OEe,"STRONG",{});var uQr=s(lne);x7o=r(uQr,"ibert"),uQr.forEach(t),R7o=r(OEe," \u2014 "),Aj=n(OEe,"A",{href:!0});var bQr=s(Aj);S7o=r(bQr,"IBertForQuestionAnswering"),bQr.forEach(t),P7o=r(OEe," (I-BERT model)"),OEe.forEach(t),$7o=i(P),Ub=n(P,"LI",{});var XEe=s(Ub);ine=n(XEe,"STRONG",{});var vQr=s(ine);I7o=r(vQr,"layoutlmv2"),vQr.forEach(t),j7o=r(XEe," \u2014 "),Lj=n(XEe,"A",{href:!0});var TQr=s(Lj);N7o=r(TQr,"LayoutLMv2ForQuestionAnswering"),TQr.forEach(t),D7o=r(XEe," (LayoutLMv2 model)"),XEe.forEach(t),q7o=i(P),Jb=n(P,"LI",{});var zEe=s(Jb);dne=n(zEe,"STRONG",{});var FQr=s(dne);G7o=r(FQr,"led"),FQr.forEach(t),O7o=r(zEe," \u2014 "),Bj=n(zEe,"A",{href:!0});var CQr=s(Bj);X7o=r(CQr,"LEDForQuestionAnswering"),CQr.forEach(t),z7o=r(zEe," (LED model)"),zEe.forEach(t),V7o=i(P),Yb=n(P,"LI",{});var VEe=s(Yb);cne=n(VEe,"STRONG",{});var MQr=s(cne);W7o=r(MQr,"longformer"),MQr.forEach(t),Q7o=r(VEe," \u2014 "),kj=n(VEe,"A",{href:!0});var EQr=s(kj);H7o=r(EQr,"LongformerForQuestionAnswering"),EQr.forEach(t),U7o=r(VEe," (Longformer model)"),VEe.forEach(t),J7o=i(P),Kb=n(P,"LI",{});var WEe=s(Kb);fne=n(WEe,"STRONG",{});var yQr=s(fne);Y7o=r(yQr,"lxmert"),yQr.forEach(t),K7o=r(WEe," \u2014 "),xj=n(WEe,"A",{href:!0});var wQr=s(xj);Z7o=r(wQr,"LxmertForQuestionAnswering"),wQr.forEach(t),e9o=r(WEe," (LXMERT model)"),WEe.forEach(t),o9o=i(P),Zb=n(P,"LI",{});var QEe=s(Zb);mne=n(QEe,"STRONG",{});var AQr=s(mne);r9o=r(AQr,"mbart"),AQr.forEach(t),t9o=r(QEe," \u2014 "),Rj=n(QEe,"A",{href:!0});var LQr=s(Rj);a9o=r(LQr,"MBartForQuestionAnswering"),LQr.forEach(t),n9o=r(QEe," (mBART model)"),QEe.forEach(t),s9o=i(P),e5=n(P,"LI",{});var HEe=s(e5);gne=n(HEe,"STRONG",{});var BQr=s(gne);l9o=r(BQr,"megatron-bert"),BQr.forEach(t),i9o=r(HEe," \u2014 "),Sj=n(HEe,"A",{href:!0});var kQr=s(Sj);d9o=r(kQr,"MegatronBertForQuestionAnswering"),kQr.forEach(t),c9o=r(HEe," (MegatronBert model)"),HEe.forEach(t),f9o=i(P),o5=n(P,"LI",{});var UEe=s(o5);hne=n(UEe,"STRONG",{});var xQr=s(hne);m9o=r(xQr,"mobilebert"),xQr.forEach(t),g9o=r(UEe," \u2014 "),Pj=n(UEe,"A",{href:!0});var RQr=s(Pj);h9o=r(RQr,"MobileBertForQuestionAnswering"),RQr.forEach(t),p9o=r(UEe," (MobileBERT model)"),UEe.forEach(t),_9o=i(P),r5=n(P,"LI",{});var JEe=s(r5);pne=n(JEe,"STRONG",{});var SQr=s(pne);u9o=r(SQr,"mpnet"),SQr.forEach(t),b9o=r(JEe," \u2014 "),$j=n(JEe,"A",{href:!0});var PQr=s($j);v9o=r(PQr,"MPNetForQuestionAnswering"),PQr.forEach(t),T9o=r(JEe," (MPNet model)"),JEe.forEach(t),F9o=i(P),t5=n(P,"LI",{});var YEe=s(t5);_ne=n(YEe,"STRONG",{});var $Qr=s(_ne);C9o=r($Qr,"nystromformer"),$Qr.forEach(t),M9o=r(YEe," \u2014 "),Ij=n(YEe,"A",{href:!0});var IQr=s(Ij);E9o=r(IQr,"NystromformerForQuestionAnswering"),IQr.forEach(t),y9o=r(YEe," (Nystromformer model)"),YEe.forEach(t),w9o=i(P),a5=n(P,"LI",{});var KEe=s(a5);une=n(KEe,"STRONG",{});var jQr=s(une);A9o=r(jQr,"qdqbert"),jQr.forEach(t),L9o=r(KEe," \u2014 "),jj=n(KEe,"A",{href:!0});var NQr=s(jj);B9o=r(NQr,"QDQBertForQuestionAnswering"),NQr.forEach(t),k9o=r(KEe," (QDQBert model)"),KEe.forEach(t),x9o=i(P),n5=n(P,"LI",{});var ZEe=s(n5);bne=n(ZEe,"STRONG",{});var DQr=s(bne);R9o=r(DQr,"reformer"),DQr.forEach(t),S9o=r(ZEe," \u2014 "),Nj=n(ZEe,"A",{href:!0});var qQr=s(Nj);P9o=r(qQr,"ReformerForQuestionAnswering"),qQr.forEach(t),$9o=r(ZEe," (Reformer model)"),ZEe.forEach(t),I9o=i(P),s5=n(P,"LI",{});var e3e=s(s5);vne=n(e3e,"STRONG",{});var GQr=s(vne);j9o=r(GQr,"rembert"),GQr.forEach(t),N9o=r(e3e," \u2014 "),Dj=n(e3e,"A",{href:!0});var OQr=s(Dj);D9o=r(OQr,"RemBertForQuestionAnswering"),OQr.forEach(t),q9o=r(e3e," (RemBERT model)"),e3e.forEach(t),G9o=i(P),l5=n(P,"LI",{});var o3e=s(l5);Tne=n(o3e,"STRONG",{});var XQr=s(Tne);O9o=r(XQr,"roberta"),XQr.forEach(t),X9o=r(o3e," \u2014 "),qj=n(o3e,"A",{href:!0});var zQr=s(qj);z9o=r(zQr,"RobertaForQuestionAnswering"),zQr.forEach(t),V9o=r(o3e," (RoBERTa model)"),o3e.forEach(t),W9o=i(P),i5=n(P,"LI",{});var r3e=s(i5);Fne=n(r3e,"STRONG",{});var VQr=s(Fne);Q9o=r(VQr,"roformer"),VQr.forEach(t),H9o=r(r3e," \u2014 "),Gj=n(r3e,"A",{href:!0});var WQr=s(Gj);U9o=r(WQr,"RoFormerForQuestionAnswering"),WQr.forEach(t),J9o=r(r3e," (RoFormer model)"),r3e.forEach(t),Y9o=i(P),d5=n(P,"LI",{});var t3e=s(d5);Cne=n(t3e,"STRONG",{});var QQr=s(Cne);K9o=r(QQr,"splinter"),QQr.forEach(t),Z9o=r(t3e," \u2014 "),Oj=n(t3e,"A",{href:!0});var HQr=s(Oj);eBo=r(HQr,"SplinterForQuestionAnswering"),HQr.forEach(t),oBo=r(t3e," (Splinter model)"),t3e.forEach(t),rBo=i(P),c5=n(P,"LI",{});var a3e=s(c5);Mne=n(a3e,"STRONG",{});var UQr=s(Mne);tBo=r(UQr,"squeezebert"),UQr.forEach(t),aBo=r(a3e," \u2014 "),Xj=n(a3e,"A",{href:!0});var JQr=s(Xj);nBo=r(JQr,"SqueezeBertForQuestionAnswering"),JQr.forEach(t),sBo=r(a3e," (SqueezeBERT model)"),a3e.forEach(t),lBo=i(P),f5=n(P,"LI",{});var n3e=s(f5);Ene=n(n3e,"STRONG",{});var YQr=s(Ene);iBo=r(YQr,"xlm"),YQr.forEach(t),dBo=r(n3e," \u2014 "),zj=n(n3e,"A",{href:!0});var KQr=s(zj);cBo=r(KQr,"XLMForQuestionAnsweringSimple"),KQr.forEach(t),fBo=r(n3e," (XLM model)"),n3e.forEach(t),mBo=i(P),m5=n(P,"LI",{});var s3e=s(m5);yne=n(s3e,"STRONG",{});var ZQr=s(yne);gBo=r(ZQr,"xlm-roberta"),ZQr.forEach(t),hBo=r(s3e," \u2014 "),Vj=n(s3e,"A",{href:!0});var eHr=s(Vj);pBo=r(eHr,"XLMRobertaForQuestionAnswering"),eHr.forEach(t),_Bo=r(s3e," (XLM-RoBERTa model)"),s3e.forEach(t),uBo=i(P),g5=n(P,"LI",{});var l3e=s(g5);wne=n(l3e,"STRONG",{});var oHr=s(wne);bBo=r(oHr,"xlm-roberta-xl"),oHr.forEach(t),vBo=r(l3e," \u2014 "),Wj=n(l3e,"A",{href:!0});var rHr=s(Wj);TBo=r(rHr,"XLMRobertaXLForQuestionAnswering"),rHr.forEach(t),FBo=r(l3e," (XLM-RoBERTa-XL model)"),l3e.forEach(t),CBo=i(P),h5=n(P,"LI",{});var i3e=s(h5);Ane=n(i3e,"STRONG",{});var tHr=s(Ane);MBo=r(tHr,"xlnet"),tHr.forEach(t),EBo=r(i3e," \u2014 "),Qj=n(i3e,"A",{href:!0});var aHr=s(Qj);yBo=r(aHr,"XLNetForQuestionAnsweringSimple"),aHr.forEach(t),wBo=r(i3e," (XLNet model)"),i3e.forEach(t),ABo=i(P),p5=n(P,"LI",{});var d3e=s(p5);Lne=n(d3e,"STRONG",{});var nHr=s(Lne);LBo=r(nHr,"yoso"),nHr.forEach(t),BBo=r(d3e," \u2014 "),Hj=n(d3e,"A",{href:!0});var sHr=s(Hj);kBo=r(sHr,"YosoForQuestionAnswering"),sHr.forEach(t),xBo=r(d3e," (YOSO model)"),d3e.forEach(t),P.forEach(t),RBo=i(Ot),_5=n(Ot,"P",{});var c3e=s(_5);SBo=r(c3e,"The model is set in evaluation mode by default using "),Bne=n(c3e,"CODE",{});var lHr=s(Bne);PBo=r(lHr,"model.eval()"),lHr.forEach(t),$Bo=r(c3e,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),kne=n(c3e,"CODE",{});var iHr=s(kne);IBo=r(iHr,"model.train()"),iHr.forEach(t),c3e.forEach(t),jBo=i(Ot),xne=n(Ot,"P",{});var dHr=s(xne);NBo=r(dHr,"Examples:"),dHr.forEach(t),DBo=i(Ot),m(e3.$$.fragment,Ot),Ot.forEach(t),tl.forEach(t),K7e=i(d),_d=n(d,"H2",{class:!0});var nke=s(_d);u5=n(nke,"A",{id:!0,class:!0,href:!0});var cHr=s(u5);Rne=n(cHr,"SPAN",{});var fHr=s(Rne);m(o3.$$.fragment,fHr),fHr.forEach(t),cHr.forEach(t),qBo=i(nke),Sne=n(nke,"SPAN",{});var mHr=s(Sne);GBo=r(mHr,"AutoModelForTableQuestionAnswering"),mHr.forEach(t),nke.forEach(t),Z7e=i(d),or=n(d,"DIV",{class:!0});var nl=s(or);m(r3.$$.fragment,nl),OBo=i(nl),ud=n(nl,"P",{});var Fz=s(ud);XBo=r(Fz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a table question answering head) when created
with the `),Pne=n(Fz,"CODE",{});var gHr=s(Pne);zBo=r(gHr,"from_pretrained()"),gHr.forEach(t),VBo=r(Fz,"class method or the "),$ne=n(Fz,"CODE",{});var hHr=s($ne);WBo=r(hHr,"from_config()"),hHr.forEach(t),QBo=r(Fz,`class
method.`),Fz.forEach(t),HBo=i(nl),t3=n(nl,"P",{});var ske=s(t3);UBo=r(ske,"This class cannot be instantiated directly using "),Ine=n(ske,"CODE",{});var pHr=s(Ine);JBo=r(pHr,"__init__()"),pHr.forEach(t),YBo=r(ske," (throws an error)."),ske.forEach(t),KBo=i(nl),Hr=n(nl,"DIV",{class:!0});var sl=s(Hr);m(a3.$$.fragment,sl),ZBo=i(sl),jne=n(sl,"P",{});var _Hr=s(jne);eko=r(_Hr,"Instantiates one of the model classes of the library (with a table question answering head) from a configuration."),_Hr.forEach(t),oko=i(sl),bd=n(sl,"P",{});var Cz=s(bd);rko=r(Cz,`Note:
Loading a model from its configuration file does `),Nne=n(Cz,"STRONG",{});var uHr=s(Nne);tko=r(uHr,"not"),uHr.forEach(t),ako=r(Cz,` load the model weights. It only affects the
model\u2019s configuration. Use `),Dne=n(Cz,"CODE",{});var bHr=s(Dne);nko=r(bHr,"from_pretrained()"),bHr.forEach(t),sko=r(Cz,"to load the model weights."),Cz.forEach(t),lko=i(sl),qne=n(sl,"P",{});var vHr=s(qne);iko=r(vHr,"Examples:"),vHr.forEach(t),dko=i(sl),m(n3.$$.fragment,sl),sl.forEach(t),cko=i(nl),qe=n(nl,"DIV",{class:!0});var Xt=s(qe);m(s3.$$.fragment,Xt),fko=i(Xt),Gne=n(Xt,"P",{});var THr=s(Gne);mko=r(THr,"Instantiate one of the model classes of the library (with a table question answering head) from a pretrained model."),THr.forEach(t),gko=i(Xt),Ua=n(Xt,"P",{});var F4=s(Ua);hko=r(F4,"The model class to instantiate is selected based on the "),One=n(F4,"CODE",{});var FHr=s(One);pko=r(FHr,"model_type"),FHr.forEach(t),_ko=r(F4,` property of the config object (either
passed as an argument or loaded from `),Xne=n(F4,"CODE",{});var CHr=s(Xne);uko=r(CHr,"pretrained_model_name_or_path"),CHr.forEach(t),bko=r(F4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),zne=n(F4,"CODE",{});var MHr=s(zne);vko=r(MHr,"pretrained_model_name_or_path"),MHr.forEach(t),Tko=r(F4,":"),F4.forEach(t),Fko=i(Xt),Vne=n(Xt,"UL",{});var EHr=s(Vne);b5=n(EHr,"LI",{});var f3e=s(b5);Wne=n(f3e,"STRONG",{});var yHr=s(Wne);Cko=r(yHr,"tapas"),yHr.forEach(t),Mko=r(f3e," \u2014 "),Uj=n(f3e,"A",{href:!0});var wHr=s(Uj);Eko=r(wHr,"TapasForQuestionAnswering"),wHr.forEach(t),yko=r(f3e," (TAPAS model)"),f3e.forEach(t),EHr.forEach(t),wko=i(Xt),v5=n(Xt,"P",{});var m3e=s(v5);Ako=r(m3e,"The model is set in evaluation mode by default using "),Qne=n(m3e,"CODE",{});var AHr=s(Qne);Lko=r(AHr,"model.eval()"),AHr.forEach(t),Bko=r(m3e,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Hne=n(m3e,"CODE",{});var LHr=s(Hne);kko=r(LHr,"model.train()"),LHr.forEach(t),m3e.forEach(t),xko=i(Xt),Une=n(Xt,"P",{});var BHr=s(Une);Rko=r(BHr,"Examples:"),BHr.forEach(t),Sko=i(Xt),m(l3.$$.fragment,Xt),Xt.forEach(t),nl.forEach(t),e9e=i(d),vd=n(d,"H2",{class:!0});var lke=s(vd);T5=n(lke,"A",{id:!0,class:!0,href:!0});var kHr=s(T5);Jne=n(kHr,"SPAN",{});var xHr=s(Jne);m(i3.$$.fragment,xHr),xHr.forEach(t),kHr.forEach(t),Pko=i(lke),Yne=n(lke,"SPAN",{});var RHr=s(Yne);$ko=r(RHr,"AutoModelForImageClassification"),RHr.forEach(t),lke.forEach(t),o9e=i(d),rr=n(d,"DIV",{class:!0});var ll=s(rr);m(d3.$$.fragment,ll),Iko=i(ll),Td=n(ll,"P",{});var Mz=s(Td);jko=r(Mz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a image classification head) when created
with the `),Kne=n(Mz,"CODE",{});var SHr=s(Kne);Nko=r(SHr,"from_pretrained()"),SHr.forEach(t),Dko=r(Mz,"class method or the "),Zne=n(Mz,"CODE",{});var PHr=s(Zne);qko=r(PHr,"from_config()"),PHr.forEach(t),Gko=r(Mz,`class
method.`),Mz.forEach(t),Oko=i(ll),c3=n(ll,"P",{});var ike=s(c3);Xko=r(ike,"This class cannot be instantiated directly using "),ese=n(ike,"CODE",{});var $Hr=s(ese);zko=r($Hr,"__init__()"),$Hr.forEach(t),Vko=r(ike," (throws an error)."),ike.forEach(t),Wko=i(ll),Ur=n(ll,"DIV",{class:!0});var il=s(Ur);m(f3.$$.fragment,il),Qko=i(il),ose=n(il,"P",{});var IHr=s(ose);Hko=r(IHr,"Instantiates one of the model classes of the library (with a image classification head) from a configuration."),IHr.forEach(t),Uko=i(il),Fd=n(il,"P",{});var Ez=s(Fd);Jko=r(Ez,`Note:
Loading a model from its configuration file does `),rse=n(Ez,"STRONG",{});var jHr=s(rse);Yko=r(jHr,"not"),jHr.forEach(t),Kko=r(Ez,` load the model weights. It only affects the
model\u2019s configuration. Use `),tse=n(Ez,"CODE",{});var NHr=s(tse);Zko=r(NHr,"from_pretrained()"),NHr.forEach(t),exo=r(Ez,"to load the model weights."),Ez.forEach(t),oxo=i(il),ase=n(il,"P",{});var DHr=s(ase);rxo=r(DHr,"Examples:"),DHr.forEach(t),txo=i(il),m(m3.$$.fragment,il),il.forEach(t),axo=i(ll),Ge=n(ll,"DIV",{class:!0});var zt=s(Ge);m(g3.$$.fragment,zt),nxo=i(zt),nse=n(zt,"P",{});var qHr=s(nse);sxo=r(qHr,"Instantiate one of the model classes of the library (with a image classification head) from a pretrained model."),qHr.forEach(t),lxo=i(zt),Ja=n(zt,"P",{});var C4=s(Ja);ixo=r(C4,"The model class to instantiate is selected based on the "),sse=n(C4,"CODE",{});var GHr=s(sse);dxo=r(GHr,"model_type"),GHr.forEach(t),cxo=r(C4,` property of the config object (either
passed as an argument or loaded from `),lse=n(C4,"CODE",{});var OHr=s(lse);fxo=r(OHr,"pretrained_model_name_or_path"),OHr.forEach(t),mxo=r(C4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),ise=n(C4,"CODE",{});var XHr=s(ise);gxo=r(XHr,"pretrained_model_name_or_path"),XHr.forEach(t),hxo=r(C4,":"),C4.forEach(t),pxo=i(zt),be=n(zt,"UL",{});var Ke=s(be);F5=n(Ke,"LI",{});var g3e=s(F5);dse=n(g3e,"STRONG",{});var zHr=s(dse);_xo=r(zHr,"beit"),zHr.forEach(t),uxo=r(g3e," \u2014 "),Jj=n(g3e,"A",{href:!0});var VHr=s(Jj);bxo=r(VHr,"BeitForImageClassification"),VHr.forEach(t),vxo=r(g3e," (BEiT model)"),g3e.forEach(t),Txo=i(Ke),C5=n(Ke,"LI",{});var h3e=s(C5);cse=n(h3e,"STRONG",{});var WHr=s(cse);Fxo=r(WHr,"convnext"),WHr.forEach(t),Cxo=r(h3e," \u2014 "),Yj=n(h3e,"A",{href:!0});var QHr=s(Yj);Mxo=r(QHr,"ConvNextForImageClassification"),QHr.forEach(t),Exo=r(h3e," (ConvNext model)"),h3e.forEach(t),yxo=i(Ke),Rs=n(Ke,"LI",{});var OL=s(Rs);fse=n(OL,"STRONG",{});var HHr=s(fse);wxo=r(HHr,"deit"),HHr.forEach(t),Axo=r(OL," \u2014 "),Kj=n(OL,"A",{href:!0});var UHr=s(Kj);Lxo=r(UHr,"DeiTForImageClassification"),UHr.forEach(t),Bxo=r(OL," or "),Zj=n(OL,"A",{href:!0});var JHr=s(Zj);kxo=r(JHr,"DeiTForImageClassificationWithTeacher"),JHr.forEach(t),xxo=r(OL," (DeiT model)"),OL.forEach(t),Rxo=i(Ke),M5=n(Ke,"LI",{});var p3e=s(M5);mse=n(p3e,"STRONG",{});var YHr=s(mse);Sxo=r(YHr,"imagegpt"),YHr.forEach(t),Pxo=r(p3e," \u2014 "),eN=n(p3e,"A",{href:!0});var KHr=s(eN);$xo=r(KHr,"ImageGPTForImageClassification"),KHr.forEach(t),Ixo=r(p3e," (ImageGPT model)"),p3e.forEach(t),jxo=i(Ke),la=n(Ke,"LI",{});var Mf=s(la);gse=n(Mf,"STRONG",{});var ZHr=s(gse);Nxo=r(ZHr,"perceiver"),ZHr.forEach(t),Dxo=r(Mf," \u2014 "),oN=n(Mf,"A",{href:!0});var eUr=s(oN);qxo=r(eUr,"PerceiverForImageClassificationLearned"),eUr.forEach(t),Gxo=r(Mf," or "),rN=n(Mf,"A",{href:!0});var oUr=s(rN);Oxo=r(oUr,"PerceiverForImageClassificationFourier"),oUr.forEach(t),Xxo=r(Mf," or "),tN=n(Mf,"A",{href:!0});var rUr=s(tN);zxo=r(rUr,"PerceiverForImageClassificationConvProcessing"),rUr.forEach(t),Vxo=r(Mf," (Perceiver model)"),Mf.forEach(t),Wxo=i(Ke),E5=n(Ke,"LI",{});var _3e=s(E5);hse=n(_3e,"STRONG",{});var tUr=s(hse);Qxo=r(tUr,"poolformer"),tUr.forEach(t),Hxo=r(_3e," \u2014 "),aN=n(_3e,"A",{href:!0});var aUr=s(aN);Uxo=r(aUr,"PoolFormerForImageClassification"),aUr.forEach(t),Jxo=r(_3e," (PoolFormer model)"),_3e.forEach(t),Yxo=i(Ke),y5=n(Ke,"LI",{});var u3e=s(y5);pse=n(u3e,"STRONG",{});var nUr=s(pse);Kxo=r(nUr,"segformer"),nUr.forEach(t),Zxo=r(u3e," \u2014 "),nN=n(u3e,"A",{href:!0});var sUr=s(nN);eRo=r(sUr,"SegformerForImageClassification"),sUr.forEach(t),oRo=r(u3e," (SegFormer model)"),u3e.forEach(t),rRo=i(Ke),w5=n(Ke,"LI",{});var b3e=s(w5);_se=n(b3e,"STRONG",{});var lUr=s(_se);tRo=r(lUr,"swin"),lUr.forEach(t),aRo=r(b3e," \u2014 "),sN=n(b3e,"A",{href:!0});var iUr=s(sN);nRo=r(iUr,"SwinForImageClassification"),iUr.forEach(t),sRo=r(b3e," (Swin model)"),b3e.forEach(t),lRo=i(Ke),A5=n(Ke,"LI",{});var v3e=s(A5);use=n(v3e,"STRONG",{});var dUr=s(use);iRo=r(dUr,"vit"),dUr.forEach(t),dRo=r(v3e," \u2014 "),lN=n(v3e,"A",{href:!0});var cUr=s(lN);cRo=r(cUr,"ViTForImageClassification"),cUr.forEach(t),fRo=r(v3e," (ViT model)"),v3e.forEach(t),Ke.forEach(t),mRo=i(zt),L5=n(zt,"P",{});var T3e=s(L5);gRo=r(T3e,"The model is set in evaluation mode by default using "),bse=n(T3e,"CODE",{});var fUr=s(bse);hRo=r(fUr,"model.eval()"),fUr.forEach(t),pRo=r(T3e,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),vse=n(T3e,"CODE",{});var mUr=s(vse);_Ro=r(mUr,"model.train()"),mUr.forEach(t),T3e.forEach(t),uRo=i(zt),Tse=n(zt,"P",{});var gUr=s(Tse);bRo=r(gUr,"Examples:"),gUr.forEach(t),vRo=i(zt),m(h3.$$.fragment,zt),zt.forEach(t),ll.forEach(t),r9e=i(d),Cd=n(d,"H2",{class:!0});var dke=s(Cd);B5=n(dke,"A",{id:!0,class:!0,href:!0});var hUr=s(B5);Fse=n(hUr,"SPAN",{});var pUr=s(Fse);m(p3.$$.fragment,pUr),pUr.forEach(t),hUr.forEach(t),TRo=i(dke),Cse=n(dke,"SPAN",{});var _Ur=s(Cse);FRo=r(_Ur,"AutoModelForVision2Seq"),_Ur.forEach(t),dke.forEach(t),t9e=i(d),tr=n(d,"DIV",{class:!0});var dl=s(tr);m(_3.$$.fragment,dl),CRo=i(dl),Md=n(dl,"P",{});var yz=s(Md);MRo=r(yz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a vision-to-text modeling head) when created
with the `),Mse=n(yz,"CODE",{});var uUr=s(Mse);ERo=r(uUr,"from_pretrained()"),uUr.forEach(t),yRo=r(yz,"class method or the "),Ese=n(yz,"CODE",{});var bUr=s(Ese);wRo=r(bUr,"from_config()"),bUr.forEach(t),ARo=r(yz,`class
method.`),yz.forEach(t),LRo=i(dl),u3=n(dl,"P",{});var cke=s(u3);BRo=r(cke,"This class cannot be instantiated directly using "),yse=n(cke,"CODE",{});var vUr=s(yse);kRo=r(vUr,"__init__()"),vUr.forEach(t),xRo=r(cke," (throws an error)."),cke.forEach(t),RRo=i(dl),Jr=n(dl,"DIV",{class:!0});var cl=s(Jr);m(b3.$$.fragment,cl),SRo=i(cl),wse=n(cl,"P",{});var TUr=s(wse);PRo=r(TUr,"Instantiates one of the model classes of the library (with a vision-to-text modeling head) from a configuration."),TUr.forEach(t),$Ro=i(cl),Ed=n(cl,"P",{});var wz=s(Ed);IRo=r(wz,`Note:
Loading a model from its configuration file does `),Ase=n(wz,"STRONG",{});var FUr=s(Ase);jRo=r(FUr,"not"),FUr.forEach(t),NRo=r(wz,` load the model weights. It only affects the
model\u2019s configuration. Use `),Lse=n(wz,"CODE",{});var CUr=s(Lse);DRo=r(CUr,"from_pretrained()"),CUr.forEach(t),qRo=r(wz,"to load the model weights."),wz.forEach(t),GRo=i(cl),Bse=n(cl,"P",{});var MUr=s(Bse);ORo=r(MUr,"Examples:"),MUr.forEach(t),XRo=i(cl),m(v3.$$.fragment,cl),cl.forEach(t),zRo=i(dl),Oe=n(dl,"DIV",{class:!0});var Vt=s(Oe);m(T3.$$.fragment,Vt),VRo=i(Vt),kse=n(Vt,"P",{});var EUr=s(kse);WRo=r(EUr,"Instantiate one of the model classes of the library (with a vision-to-text modeling head) from a pretrained model."),EUr.forEach(t),QRo=i(Vt),Ya=n(Vt,"P",{});var M4=s(Ya);HRo=r(M4,"The model class to instantiate is selected based on the "),xse=n(M4,"CODE",{});var yUr=s(xse);URo=r(yUr,"model_type"),yUr.forEach(t),JRo=r(M4,` property of the config object (either
passed as an argument or loaded from `),Rse=n(M4,"CODE",{});var wUr=s(Rse);YRo=r(wUr,"pretrained_model_name_or_path"),wUr.forEach(t),KRo=r(M4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Sse=n(M4,"CODE",{});var AUr=s(Sse);ZRo=r(AUr,"pretrained_model_name_or_path"),AUr.forEach(t),eSo=r(M4,":"),M4.forEach(t),oSo=i(Vt),Pse=n(Vt,"UL",{});var LUr=s(Pse);k5=n(LUr,"LI",{});var F3e=s(k5);$se=n(F3e,"STRONG",{});var BUr=s($se);rSo=r(BUr,"vision-encoder-decoder"),BUr.forEach(t),tSo=r(F3e," \u2014 "),iN=n(F3e,"A",{href:!0});var kUr=s(iN);aSo=r(kUr,"VisionEncoderDecoderModel"),kUr.forEach(t),nSo=r(F3e," (Vision Encoder decoder model)"),F3e.forEach(t),LUr.forEach(t),sSo=i(Vt),x5=n(Vt,"P",{});var C3e=s(x5);lSo=r(C3e,"The model is set in evaluation mode by default using "),Ise=n(C3e,"CODE",{});var xUr=s(Ise);iSo=r(xUr,"model.eval()"),xUr.forEach(t),dSo=r(C3e,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),jse=n(C3e,"CODE",{});var RUr=s(jse);cSo=r(RUr,"model.train()"),RUr.forEach(t),C3e.forEach(t),fSo=i(Vt),Nse=n(Vt,"P",{});var SUr=s(Nse);mSo=r(SUr,"Examples:"),SUr.forEach(t),gSo=i(Vt),m(F3.$$.fragment,Vt),Vt.forEach(t),dl.forEach(t),a9e=i(d),yd=n(d,"H2",{class:!0});var fke=s(yd);R5=n(fke,"A",{id:!0,class:!0,href:!0});var PUr=s(R5);Dse=n(PUr,"SPAN",{});var $Ur=s(Dse);m(C3.$$.fragment,$Ur),$Ur.forEach(t),PUr.forEach(t),hSo=i(fke),qse=n(fke,"SPAN",{});var IUr=s(qse);pSo=r(IUr,"AutoModelForAudioClassification"),IUr.forEach(t),fke.forEach(t),n9e=i(d),ar=n(d,"DIV",{class:!0});var fl=s(ar);m(M3.$$.fragment,fl),_So=i(fl),wd=n(fl,"P",{});var Az=s(wd);uSo=r(Az,`This is a generic model class that will be instantiated as one of the model classes of the library (with a audio classification head) when created
with the `),Gse=n(Az,"CODE",{});var jUr=s(Gse);bSo=r(jUr,"from_pretrained()"),jUr.forEach(t),vSo=r(Az,"class method or the "),Ose=n(Az,"CODE",{});var NUr=s(Ose);TSo=r(NUr,"from_config()"),NUr.forEach(t),FSo=r(Az,`class
method.`),Az.forEach(t),CSo=i(fl),E3=n(fl,"P",{});var mke=s(E3);MSo=r(mke,"This class cannot be instantiated directly using "),Xse=n(mke,"CODE",{});var DUr=s(Xse);ESo=r(DUr,"__init__()"),DUr.forEach(t),ySo=r(mke," (throws an error)."),mke.forEach(t),wSo=i(fl),Yr=n(fl,"DIV",{class:!0});var ml=s(Yr);m(y3.$$.fragment,ml),ASo=i(ml),zse=n(ml,"P",{});var qUr=s(zse);LSo=r(qUr,"Instantiates one of the model classes of the library (with a audio classification head) from a configuration."),qUr.forEach(t),BSo=i(ml),Ad=n(ml,"P",{});var Lz=s(Ad);kSo=r(Lz,`Note:
Loading a model from its configuration file does `),Vse=n(Lz,"STRONG",{});var GUr=s(Vse);xSo=r(GUr,"not"),GUr.forEach(t),RSo=r(Lz,` load the model weights. It only affects the
model\u2019s configuration. Use `),Wse=n(Lz,"CODE",{});var OUr=s(Wse);SSo=r(OUr,"from_pretrained()"),OUr.forEach(t),PSo=r(Lz,"to load the model weights."),Lz.forEach(t),$So=i(ml),Qse=n(ml,"P",{});var XUr=s(Qse);ISo=r(XUr,"Examples:"),XUr.forEach(t),jSo=i(ml),m(w3.$$.fragment,ml),ml.forEach(t),NSo=i(fl),Xe=n(fl,"DIV",{class:!0});var Wt=s(Xe);m(A3.$$.fragment,Wt),DSo=i(Wt),Hse=n(Wt,"P",{});var zUr=s(Hse);qSo=r(zUr,"Instantiate one of the model classes of the library (with a audio classification head) from a pretrained model."),zUr.forEach(t),GSo=i(Wt),Ka=n(Wt,"P",{});var E4=s(Ka);OSo=r(E4,"The model class to instantiate is selected based on the "),Use=n(E4,"CODE",{});var VUr=s(Use);XSo=r(VUr,"model_type"),VUr.forEach(t),zSo=r(E4,` property of the config object (either
passed as an argument or loaded from `),Jse=n(E4,"CODE",{});var WUr=s(Jse);VSo=r(WUr,"pretrained_model_name_or_path"),WUr.forEach(t),WSo=r(E4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Yse=n(E4,"CODE",{});var QUr=s(Yse);QSo=r(QUr,"pretrained_model_name_or_path"),QUr.forEach(t),HSo=r(E4,":"),E4.forEach(t),USo=i(Wt),ao=n(Wt,"UL",{});var Qt=s(ao);S5=n(Qt,"LI",{});var M3e=s(S5);Kse=n(M3e,"STRONG",{});var HUr=s(Kse);JSo=r(HUr,"hubert"),HUr.forEach(t),YSo=r(M3e," \u2014 "),dN=n(M3e,"A",{href:!0});var UUr=s(dN);KSo=r(UUr,"HubertForSequenceClassification"),UUr.forEach(t),ZSo=r(M3e," (Hubert model)"),M3e.forEach(t),ePo=i(Qt),P5=n(Qt,"LI",{});var E3e=s(P5);Zse=n(E3e,"STRONG",{});var JUr=s(Zse);oPo=r(JUr,"sew"),JUr.forEach(t),rPo=r(E3e," \u2014 "),cN=n(E3e,"A",{href:!0});var YUr=s(cN);tPo=r(YUr,"SEWForSequenceClassification"),YUr.forEach(t),aPo=r(E3e," (SEW model)"),E3e.forEach(t),nPo=i(Qt),$5=n(Qt,"LI",{});var y3e=s($5);ele=n(y3e,"STRONG",{});var KUr=s(ele);sPo=r(KUr,"sew-d"),KUr.forEach(t),lPo=r(y3e," \u2014 "),fN=n(y3e,"A",{href:!0});var ZUr=s(fN);iPo=r(ZUr,"SEWDForSequenceClassification"),ZUr.forEach(t),dPo=r(y3e," (SEW-D model)"),y3e.forEach(t),cPo=i(Qt),I5=n(Qt,"LI",{});var w3e=s(I5);ole=n(w3e,"STRONG",{});var eJr=s(ole);fPo=r(eJr,"unispeech"),eJr.forEach(t),mPo=r(w3e," \u2014 "),mN=n(w3e,"A",{href:!0});var oJr=s(mN);gPo=r(oJr,"UniSpeechForSequenceClassification"),oJr.forEach(t),hPo=r(w3e," (UniSpeech model)"),w3e.forEach(t),pPo=i(Qt),j5=n(Qt,"LI",{});var A3e=s(j5);rle=n(A3e,"STRONG",{});var rJr=s(rle);_Po=r(rJr,"unispeech-sat"),rJr.forEach(t),uPo=r(A3e," \u2014 "),gN=n(A3e,"A",{href:!0});var tJr=s(gN);bPo=r(tJr,"UniSpeechSatForSequenceClassification"),tJr.forEach(t),vPo=r(A3e," (UniSpeechSat model)"),A3e.forEach(t),TPo=i(Qt),N5=n(Qt,"LI",{});var L3e=s(N5);tle=n(L3e,"STRONG",{});var aJr=s(tle);FPo=r(aJr,"wav2vec2"),aJr.forEach(t),CPo=r(L3e," \u2014 "),hN=n(L3e,"A",{href:!0});var nJr=s(hN);MPo=r(nJr,"Wav2Vec2ForSequenceClassification"),nJr.forEach(t),EPo=r(L3e," (Wav2Vec2 model)"),L3e.forEach(t),yPo=i(Qt),D5=n(Qt,"LI",{});var B3e=s(D5);ale=n(B3e,"STRONG",{});var sJr=s(ale);wPo=r(sJr,"wavlm"),sJr.forEach(t),APo=r(B3e," \u2014 "),pN=n(B3e,"A",{href:!0});var lJr=s(pN);LPo=r(lJr,"WavLMForSequenceClassification"),lJr.forEach(t),BPo=r(B3e," (WavLM model)"),B3e.forEach(t),Qt.forEach(t),kPo=i(Wt),q5=n(Wt,"P",{});var k3e=s(q5);xPo=r(k3e,"The model is set in evaluation mode by default using "),nle=n(k3e,"CODE",{});var iJr=s(nle);RPo=r(iJr,"model.eval()"),iJr.forEach(t),SPo=r(k3e,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),sle=n(k3e,"CODE",{});var dJr=s(sle);PPo=r(dJr,"model.train()"),dJr.forEach(t),k3e.forEach(t),$Po=i(Wt),lle=n(Wt,"P",{});var cJr=s(lle);IPo=r(cJr,"Examples:"),cJr.forEach(t),jPo=i(Wt),m(L3.$$.fragment,Wt),Wt.forEach(t),fl.forEach(t),s9e=i(d),Ld=n(d,"H2",{class:!0});var gke=s(Ld);G5=n(gke,"A",{id:!0,class:!0,href:!0});var fJr=s(G5);ile=n(fJr,"SPAN",{});var mJr=s(ile);m(B3.$$.fragment,mJr),mJr.forEach(t),fJr.forEach(t),NPo=i(gke),dle=n(gke,"SPAN",{});var gJr=s(dle);DPo=r(gJr,"AutoModelForAudioFrameClassification"),gJr.forEach(t),gke.forEach(t),l9e=i(d),nr=n(d,"DIV",{class:!0});var gl=s(nr);m(k3.$$.fragment,gl),qPo=i(gl),Bd=n(gl,"P",{});var Bz=s(Bd);GPo=r(Bz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a audio frame (token) classification head) when created
with the `),cle=n(Bz,"CODE",{});var hJr=s(cle);OPo=r(hJr,"from_pretrained()"),hJr.forEach(t),XPo=r(Bz,"class method or the "),fle=n(Bz,"CODE",{});var pJr=s(fle);zPo=r(pJr,"from_config()"),pJr.forEach(t),VPo=r(Bz,`class
method.`),Bz.forEach(t),WPo=i(gl),x3=n(gl,"P",{});var hke=s(x3);QPo=r(hke,"This class cannot be instantiated directly using "),mle=n(hke,"CODE",{});var _Jr=s(mle);HPo=r(_Jr,"__init__()"),_Jr.forEach(t),UPo=r(hke," (throws an error)."),hke.forEach(t),JPo=i(gl),Kr=n(gl,"DIV",{class:!0});var hl=s(Kr);m(R3.$$.fragment,hl),YPo=i(hl),gle=n(hl,"P",{});var uJr=s(gle);KPo=r(uJr,"Instantiates one of the model classes of the library (with a audio frame (token) classification head) from a configuration."),uJr.forEach(t),ZPo=i(hl),kd=n(hl,"P",{});var kz=s(kd);e$o=r(kz,`Note:
Loading a model from its configuration file does `),hle=n(kz,"STRONG",{});var bJr=s(hle);o$o=r(bJr,"not"),bJr.forEach(t),r$o=r(kz,` load the model weights. It only affects the
model\u2019s configuration. Use `),ple=n(kz,"CODE",{});var vJr=s(ple);t$o=r(vJr,"from_pretrained()"),vJr.forEach(t),a$o=r(kz,"to load the model weights."),kz.forEach(t),n$o=i(hl),_le=n(hl,"P",{});var TJr=s(_le);s$o=r(TJr,"Examples:"),TJr.forEach(t),l$o=i(hl),m(S3.$$.fragment,hl),hl.forEach(t),i$o=i(gl),ze=n(gl,"DIV",{class:!0});var Ht=s(ze);m(P3.$$.fragment,Ht),d$o=i(Ht),ule=n(Ht,"P",{});var FJr=s(ule);c$o=r(FJr,"Instantiate one of the model classes of the library (with a audio frame (token) classification head) from a pretrained model."),FJr.forEach(t),f$o=i(Ht),Za=n(Ht,"P",{});var y4=s(Za);m$o=r(y4,"The model class to instantiate is selected based on the "),ble=n(y4,"CODE",{});var CJr=s(ble);g$o=r(CJr,"model_type"),CJr.forEach(t),h$o=r(y4,` property of the config object (either
passed as an argument or loaded from `),vle=n(y4,"CODE",{});var MJr=s(vle);p$o=r(MJr,"pretrained_model_name_or_path"),MJr.forEach(t),_$o=r(y4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Tle=n(y4,"CODE",{});var EJr=s(Tle);u$o=r(EJr,"pretrained_model_name_or_path"),EJr.forEach(t),b$o=r(y4,":"),y4.forEach(t),v$o=i(Ht),xd=n(Ht,"UL",{});var xz=s(xd);O5=n(xz,"LI",{});var x3e=s(O5);Fle=n(x3e,"STRONG",{});var yJr=s(Fle);T$o=r(yJr,"unispeech-sat"),yJr.forEach(t),F$o=r(x3e," \u2014 "),_N=n(x3e,"A",{href:!0});var wJr=s(_N);C$o=r(wJr,"UniSpeechSatForAudioFrameClassification"),wJr.forEach(t),M$o=r(x3e," (UniSpeechSat model)"),x3e.forEach(t),E$o=i(xz),X5=n(xz,"LI",{});var R3e=s(X5);Cle=n(R3e,"STRONG",{});var AJr=s(Cle);y$o=r(AJr,"wav2vec2"),AJr.forEach(t),w$o=r(R3e," \u2014 "),uN=n(R3e,"A",{href:!0});var LJr=s(uN);A$o=r(LJr,"Wav2Vec2ForAudioFrameClassification"),LJr.forEach(t),L$o=r(R3e," (Wav2Vec2 model)"),R3e.forEach(t),B$o=i(xz),z5=n(xz,"LI",{});var S3e=s(z5);Mle=n(S3e,"STRONG",{});var BJr=s(Mle);k$o=r(BJr,"wavlm"),BJr.forEach(t),x$o=r(S3e," \u2014 "),bN=n(S3e,"A",{href:!0});var kJr=s(bN);R$o=r(kJr,"WavLMForAudioFrameClassification"),kJr.forEach(t),S$o=r(S3e," (WavLM model)"),S3e.forEach(t),xz.forEach(t),P$o=i(Ht),V5=n(Ht,"P",{});var P3e=s(V5);$$o=r(P3e,"The model is set in evaluation mode by default using "),Ele=n(P3e,"CODE",{});var xJr=s(Ele);I$o=r(xJr,"model.eval()"),xJr.forEach(t),j$o=r(P3e,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),yle=n(P3e,"CODE",{});var RJr=s(yle);N$o=r(RJr,"model.train()"),RJr.forEach(t),P3e.forEach(t),D$o=i(Ht),wle=n(Ht,"P",{});var SJr=s(wle);q$o=r(SJr,"Examples:"),SJr.forEach(t),G$o=i(Ht),m($3.$$.fragment,Ht),Ht.forEach(t),gl.forEach(t),i9e=i(d),Rd=n(d,"H2",{class:!0});var pke=s(Rd);W5=n(pke,"A",{id:!0,class:!0,href:!0});var PJr=s(W5);Ale=n(PJr,"SPAN",{});var $Jr=s(Ale);m(I3.$$.fragment,$Jr),$Jr.forEach(t),PJr.forEach(t),O$o=i(pke),Lle=n(pke,"SPAN",{});var IJr=s(Lle);X$o=r(IJr,"AutoModelForCTC"),IJr.forEach(t),pke.forEach(t),d9e=i(d),sr=n(d,"DIV",{class:!0});var pl=s(sr);m(j3.$$.fragment,pl),z$o=i(pl),Sd=n(pl,"P",{});var Rz=s(Sd);V$o=r(Rz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a connectionist temporal classification head) when created
with the `),Ble=n(Rz,"CODE",{});var jJr=s(Ble);W$o=r(jJr,"from_pretrained()"),jJr.forEach(t),Q$o=r(Rz,"class method or the "),kle=n(Rz,"CODE",{});var NJr=s(kle);H$o=r(NJr,"from_config()"),NJr.forEach(t),U$o=r(Rz,`class
method.`),Rz.forEach(t),J$o=i(pl),N3=n(pl,"P",{});var _ke=s(N3);Y$o=r(_ke,"This class cannot be instantiated directly using "),xle=n(_ke,"CODE",{});var DJr=s(xle);K$o=r(DJr,"__init__()"),DJr.forEach(t),Z$o=r(_ke," (throws an error)."),_ke.forEach(t),eIo=i(pl),Zr=n(pl,"DIV",{class:!0});var _l=s(Zr);m(D3.$$.fragment,_l),oIo=i(_l),Rle=n(_l,"P",{});var qJr=s(Rle);rIo=r(qJr,"Instantiates one of the model classes of the library (with a connectionist temporal classification head) from a configuration."),qJr.forEach(t),tIo=i(_l),Pd=n(_l,"P",{});var Sz=s(Pd);aIo=r(Sz,`Note:
Loading a model from its configuration file does `),Sle=n(Sz,"STRONG",{});var GJr=s(Sle);nIo=r(GJr,"not"),GJr.forEach(t),sIo=r(Sz,` load the model weights. It only affects the
model\u2019s configuration. Use `),Ple=n(Sz,"CODE",{});var OJr=s(Ple);lIo=r(OJr,"from_pretrained()"),OJr.forEach(t),iIo=r(Sz,"to load the model weights."),Sz.forEach(t),dIo=i(_l),$le=n(_l,"P",{});var XJr=s($le);cIo=r(XJr,"Examples:"),XJr.forEach(t),fIo=i(_l),m(q3.$$.fragment,_l),_l.forEach(t),mIo=i(pl),Ve=n(pl,"DIV",{class:!0});var Ut=s(Ve);m(G3.$$.fragment,Ut),gIo=i(Ut),Ile=n(Ut,"P",{});var zJr=s(Ile);hIo=r(zJr,"Instantiate one of the model classes of the library (with a connectionist temporal classification head) from a pretrained model."),zJr.forEach(t),pIo=i(Ut),en=n(Ut,"P",{});var w4=s(en);_Io=r(w4,"The model class to instantiate is selected based on the "),jle=n(w4,"CODE",{});var VJr=s(jle);uIo=r(VJr,"model_type"),VJr.forEach(t),bIo=r(w4,` property of the config object (either
passed as an argument or loaded from `),Nle=n(w4,"CODE",{});var WJr=s(Nle);vIo=r(WJr,"pretrained_model_name_or_path"),WJr.forEach(t),TIo=r(w4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Dle=n(w4,"CODE",{});var QJr=s(Dle);FIo=r(QJr,"pretrained_model_name_or_path"),QJr.forEach(t),CIo=r(w4,":"),w4.forEach(t),MIo=i(Ut),no=n(Ut,"UL",{});var Jt=s(no);Q5=n(Jt,"LI",{});var $3e=s(Q5);qle=n($3e,"STRONG",{});var HJr=s(qle);EIo=r(HJr,"hubert"),HJr.forEach(t),yIo=r($3e," \u2014 "),vN=n($3e,"A",{href:!0});var UJr=s(vN);wIo=r(UJr,"HubertForCTC"),UJr.forEach(t),AIo=r($3e," (Hubert model)"),$3e.forEach(t),LIo=i(Jt),H5=n(Jt,"LI",{});var I3e=s(H5);Gle=n(I3e,"STRONG",{});var JJr=s(Gle);BIo=r(JJr,"sew"),JJr.forEach(t),kIo=r(I3e," \u2014 "),TN=n(I3e,"A",{href:!0});var YJr=s(TN);xIo=r(YJr,"SEWForCTC"),YJr.forEach(t),RIo=r(I3e," (SEW model)"),I3e.forEach(t),SIo=i(Jt),U5=n(Jt,"LI",{});var j3e=s(U5);Ole=n(j3e,"STRONG",{});var KJr=s(Ole);PIo=r(KJr,"sew-d"),KJr.forEach(t),$Io=r(j3e," \u2014 "),FN=n(j3e,"A",{href:!0});var ZJr=s(FN);IIo=r(ZJr,"SEWDForCTC"),ZJr.forEach(t),jIo=r(j3e," (SEW-D model)"),j3e.forEach(t),NIo=i(Jt),J5=n(Jt,"LI",{});var N3e=s(J5);Xle=n(N3e,"STRONG",{});var eYr=s(Xle);DIo=r(eYr,"unispeech"),eYr.forEach(t),qIo=r(N3e," \u2014 "),CN=n(N3e,"A",{href:!0});var oYr=s(CN);GIo=r(oYr,"UniSpeechForCTC"),oYr.forEach(t),OIo=r(N3e," (UniSpeech model)"),N3e.forEach(t),XIo=i(Jt),Y5=n(Jt,"LI",{});var D3e=s(Y5);zle=n(D3e,"STRONG",{});var rYr=s(zle);zIo=r(rYr,"unispeech-sat"),rYr.forEach(t),VIo=r(D3e," \u2014 "),MN=n(D3e,"A",{href:!0});var tYr=s(MN);WIo=r(tYr,"UniSpeechSatForCTC"),tYr.forEach(t),QIo=r(D3e," (UniSpeechSat model)"),D3e.forEach(t),HIo=i(Jt),K5=n(Jt,"LI",{});var q3e=s(K5);Vle=n(q3e,"STRONG",{});var aYr=s(Vle);UIo=r(aYr,"wav2vec2"),aYr.forEach(t),JIo=r(q3e," \u2014 "),EN=n(q3e,"A",{href:!0});var nYr=s(EN);YIo=r(nYr,"Wav2Vec2ForCTC"),nYr.forEach(t),KIo=r(q3e," (Wav2Vec2 model)"),q3e.forEach(t),ZIo=i(Jt),Z5=n(Jt,"LI",{});var G3e=s(Z5);Wle=n(G3e,"STRONG",{});var sYr=s(Wle);ejo=r(sYr,"wavlm"),sYr.forEach(t),ojo=r(G3e," \u2014 "),yN=n(G3e,"A",{href:!0});var lYr=s(yN);rjo=r(lYr,"WavLMForCTC"),lYr.forEach(t),tjo=r(G3e," (WavLM model)"),G3e.forEach(t),Jt.forEach(t),ajo=i(Ut),ev=n(Ut,"P",{});var O3e=s(ev);njo=r(O3e,"The model is set in evaluation mode by default using "),Qle=n(O3e,"CODE",{});var iYr=s(Qle);sjo=r(iYr,"model.eval()"),iYr.forEach(t),ljo=r(O3e,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Hle=n(O3e,"CODE",{});var dYr=s(Hle);ijo=r(dYr,"model.train()"),dYr.forEach(t),O3e.forEach(t),djo=i(Ut),Ule=n(Ut,"P",{});var cYr=s(Ule);cjo=r(cYr,"Examples:"),cYr.forEach(t),fjo=i(Ut),m(O3.$$.fragment,Ut),Ut.forEach(t),pl.forEach(t),c9e=i(d),$d=n(d,"H2",{class:!0});var uke=s($d);ov=n(uke,"A",{id:!0,class:!0,href:!0});var fYr=s(ov);Jle=n(fYr,"SPAN",{});var mYr=s(Jle);m(X3.$$.fragment,mYr),mYr.forEach(t),fYr.forEach(t),mjo=i(uke),Yle=n(uke,"SPAN",{});var gYr=s(Yle);gjo=r(gYr,"AutoModelForSpeechSeq2Seq"),gYr.forEach(t),uke.forEach(t),f9e=i(d),lr=n(d,"DIV",{class:!0});var ul=s(lr);m(z3.$$.fragment,ul),hjo=i(ul),Id=n(ul,"P",{});var Pz=s(Id);pjo=r(Pz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) when created
with the `),Kle=n(Pz,"CODE",{});var hYr=s(Kle);_jo=r(hYr,"from_pretrained()"),hYr.forEach(t),ujo=r(Pz,"class method or the "),Zle=n(Pz,"CODE",{});var pYr=s(Zle);bjo=r(pYr,"from_config()"),pYr.forEach(t),vjo=r(Pz,`class
method.`),Pz.forEach(t),Tjo=i(ul),V3=n(ul,"P",{});var bke=s(V3);Fjo=r(bke,"This class cannot be instantiated directly using "),eie=n(bke,"CODE",{});var _Yr=s(eie);Cjo=r(_Yr,"__init__()"),_Yr.forEach(t),Mjo=r(bke," (throws an error)."),bke.forEach(t),Ejo=i(ul),et=n(ul,"DIV",{class:!0});var bl=s(et);m(W3.$$.fragment,bl),yjo=i(bl),oie=n(bl,"P",{});var uYr=s(oie);wjo=r(uYr,"Instantiates one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) from a configuration."),uYr.forEach(t),Ajo=i(bl),jd=n(bl,"P",{});var $z=s(jd);Ljo=r($z,`Note:
Loading a model from its configuration file does `),rie=n($z,"STRONG",{});var bYr=s(rie);Bjo=r(bYr,"not"),bYr.forEach(t),kjo=r($z,` load the model weights. It only affects the
model\u2019s configuration. Use `),tie=n($z,"CODE",{});var vYr=s(tie);xjo=r(vYr,"from_pretrained()"),vYr.forEach(t),Rjo=r($z,"to load the model weights."),$z.forEach(t),Sjo=i(bl),aie=n(bl,"P",{});var TYr=s(aie);Pjo=r(TYr,"Examples:"),TYr.forEach(t),$jo=i(bl),m(Q3.$$.fragment,bl),bl.forEach(t),Ijo=i(ul),We=n(ul,"DIV",{class:!0});var Yt=s(We);m(H3.$$.fragment,Yt),jjo=i(Yt),nie=n(Yt,"P",{});var FYr=s(nie);Njo=r(FYr,"Instantiate one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) from a pretrained model."),FYr.forEach(t),Djo=i(Yt),on=n(Yt,"P",{});var A4=s(on);qjo=r(A4,"The model class to instantiate is selected based on the "),sie=n(A4,"CODE",{});var CYr=s(sie);Gjo=r(CYr,"model_type"),CYr.forEach(t),Ojo=r(A4,` property of the config object (either
passed as an argument or loaded from `),lie=n(A4,"CODE",{});var MYr=s(lie);Xjo=r(MYr,"pretrained_model_name_or_path"),MYr.forEach(t),zjo=r(A4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),iie=n(A4,"CODE",{});var EYr=s(iie);Vjo=r(EYr,"pretrained_model_name_or_path"),EYr.forEach(t),Wjo=r(A4,":"),A4.forEach(t),Qjo=i(Yt),U3=n(Yt,"UL",{});var vke=s(U3);rv=n(vke,"LI",{});var X3e=s(rv);die=n(X3e,"STRONG",{});var yYr=s(die);Hjo=r(yYr,"speech-encoder-decoder"),yYr.forEach(t),Ujo=r(X3e," \u2014 "),wN=n(X3e,"A",{href:!0});var wYr=s(wN);Jjo=r(wYr,"SpeechEncoderDecoderModel"),wYr.forEach(t),Yjo=r(X3e," (Speech Encoder decoder model)"),X3e.forEach(t),Kjo=i(vke),tv=n(vke,"LI",{});var z3e=s(tv);cie=n(z3e,"STRONG",{});var AYr=s(cie);Zjo=r(AYr,"speech_to_text"),AYr.forEach(t),eNo=r(z3e," \u2014 "),AN=n(z3e,"A",{href:!0});var LYr=s(AN);oNo=r(LYr,"Speech2TextForConditionalGeneration"),LYr.forEach(t),rNo=r(z3e," (Speech2Text model)"),z3e.forEach(t),vke.forEach(t),tNo=i(Yt),av=n(Yt,"P",{});var V3e=s(av);aNo=r(V3e,"The model is set in evaluation mode by default using "),fie=n(V3e,"CODE",{});var BYr=s(fie);nNo=r(BYr,"model.eval()"),BYr.forEach(t),sNo=r(V3e,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),mie=n(V3e,"CODE",{});var kYr=s(mie);lNo=r(kYr,"model.train()"),kYr.forEach(t),V3e.forEach(t),iNo=i(Yt),gie=n(Yt,"P",{});var xYr=s(gie);dNo=r(xYr,"Examples:"),xYr.forEach(t),cNo=i(Yt),m(J3.$$.fragment,Yt),Yt.forEach(t),ul.forEach(t),m9e=i(d),Nd=n(d,"H2",{class:!0});var Tke=s(Nd);nv=n(Tke,"A",{id:!0,class:!0,href:!0});var RYr=s(nv);hie=n(RYr,"SPAN",{});var SYr=s(hie);m(Y3.$$.fragment,SYr),SYr.forEach(t),RYr.forEach(t),fNo=i(Tke),pie=n(Tke,"SPAN",{});var PYr=s(pie);mNo=r(PYr,"AutoModelForAudioXVector"),PYr.forEach(t),Tke.forEach(t),g9e=i(d),ir=n(d,"DIV",{class:!0});var vl=s(ir);m(K3.$$.fragment,vl),gNo=i(vl),Dd=n(vl,"P",{});var Iz=s(Dd);hNo=r(Iz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a audio retrieval via x-vector head) when created
with the `),_ie=n(Iz,"CODE",{});var $Yr=s(_ie);pNo=r($Yr,"from_pretrained()"),$Yr.forEach(t),_No=r(Iz,"class method or the "),uie=n(Iz,"CODE",{});var IYr=s(uie);uNo=r(IYr,"from_config()"),IYr.forEach(t),bNo=r(Iz,`class
method.`),Iz.forEach(t),vNo=i(vl),Z3=n(vl,"P",{});var Fke=s(Z3);TNo=r(Fke,"This class cannot be instantiated directly using "),bie=n(Fke,"CODE",{});var jYr=s(bie);FNo=r(jYr,"__init__()"),jYr.forEach(t),CNo=r(Fke," (throws an error)."),Fke.forEach(t),MNo=i(vl),ot=n(vl,"DIV",{class:!0});var Tl=s(ot);m(ey.$$.fragment,Tl),ENo=i(Tl),vie=n(Tl,"P",{});var NYr=s(vie);yNo=r(NYr,"Instantiates one of the model classes of the library (with a audio retrieval via x-vector head) from a configuration."),NYr.forEach(t),wNo=i(Tl),qd=n(Tl,"P",{});var jz=s(qd);ANo=r(jz,`Note:
Loading a model from its configuration file does `),Tie=n(jz,"STRONG",{});var DYr=s(Tie);LNo=r(DYr,"not"),DYr.forEach(t),BNo=r(jz,` load the model weights. It only affects the
model\u2019s configuration. Use `),Fie=n(jz,"CODE",{});var qYr=s(Fie);kNo=r(qYr,"from_pretrained()"),qYr.forEach(t),xNo=r(jz,"to load the model weights."),jz.forEach(t),RNo=i(Tl),Cie=n(Tl,"P",{});var GYr=s(Cie);SNo=r(GYr,"Examples:"),GYr.forEach(t),PNo=i(Tl),m(oy.$$.fragment,Tl),Tl.forEach(t),$No=i(vl),Qe=n(vl,"DIV",{class:!0});var Kt=s(Qe);m(ry.$$.fragment,Kt),INo=i(Kt),Mie=n(Kt,"P",{});var OYr=s(Mie);jNo=r(OYr,"Instantiate one of the model classes of the library (with a audio retrieval via x-vector head) from a pretrained model."),OYr.forEach(t),NNo=i(Kt),rn=n(Kt,"P",{});var L4=s(rn);DNo=r(L4,"The model class to instantiate is selected based on the "),Eie=n(L4,"CODE",{});var XYr=s(Eie);qNo=r(XYr,"model_type"),XYr.forEach(t),GNo=r(L4,` property of the config object (either
passed as an argument or loaded from `),yie=n(L4,"CODE",{});var zYr=s(yie);ONo=r(zYr,"pretrained_model_name_or_path"),zYr.forEach(t),XNo=r(L4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),wie=n(L4,"CODE",{});var VYr=s(wie);zNo=r(VYr,"pretrained_model_name_or_path"),VYr.forEach(t),VNo=r(L4,":"),L4.forEach(t),WNo=i(Kt),Gd=n(Kt,"UL",{});var Nz=s(Gd);sv=n(Nz,"LI",{});var W3e=s(sv);Aie=n(W3e,"STRONG",{});var WYr=s(Aie);QNo=r(WYr,"unispeech-sat"),WYr.forEach(t),HNo=r(W3e," \u2014 "),LN=n(W3e,"A",{href:!0});var QYr=s(LN);UNo=r(QYr,"UniSpeechSatForXVector"),QYr.forEach(t),JNo=r(W3e," (UniSpeechSat model)"),W3e.forEach(t),YNo=i(Nz),lv=n(Nz,"LI",{});var Q3e=s(lv);Lie=n(Q3e,"STRONG",{});var HYr=s(Lie);KNo=r(HYr,"wav2vec2"),HYr.forEach(t),ZNo=r(Q3e," \u2014 "),BN=n(Q3e,"A",{href:!0});var UYr=s(BN);eDo=r(UYr,"Wav2Vec2ForXVector"),UYr.forEach(t),oDo=r(Q3e," (Wav2Vec2 model)"),Q3e.forEach(t),rDo=i(Nz),iv=n(Nz,"LI",{});var H3e=s(iv);Bie=n(H3e,"STRONG",{});var JYr=s(Bie);tDo=r(JYr,"wavlm"),JYr.forEach(t),aDo=r(H3e," \u2014 "),kN=n(H3e,"A",{href:!0});var YYr=s(kN);nDo=r(YYr,"WavLMForXVector"),YYr.forEach(t),sDo=r(H3e," (WavLM model)"),H3e.forEach(t),Nz.forEach(t),lDo=i(Kt),dv=n(Kt,"P",{});var U3e=s(dv);iDo=r(U3e,"The model is set in evaluation mode by default using "),kie=n(U3e,"CODE",{});var KYr=s(kie);dDo=r(KYr,"model.eval()"),KYr.forEach(t),cDo=r(U3e,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),xie=n(U3e,"CODE",{});var ZYr=s(xie);fDo=r(ZYr,"model.train()"),ZYr.forEach(t),U3e.forEach(t),mDo=i(Kt),Rie=n(Kt,"P",{});var eKr=s(Rie);gDo=r(eKr,"Examples:"),eKr.forEach(t),hDo=i(Kt),m(ty.$$.fragment,Kt),Kt.forEach(t),vl.forEach(t),h9e=i(d),Od=n(d,"H2",{class:!0});var Cke=s(Od);cv=n(Cke,"A",{id:!0,class:!0,href:!0});var oKr=s(cv);Sie=n(oKr,"SPAN",{});var rKr=s(Sie);m(ay.$$.fragment,rKr),rKr.forEach(t),oKr.forEach(t),pDo=i(Cke),Pie=n(Cke,"SPAN",{});var tKr=s(Pie);_Do=r(tKr,"AutoModelForMaskedImageModeling"),tKr.forEach(t),Cke.forEach(t),p9e=i(d),dr=n(d,"DIV",{class:!0});var Fl=s(dr);m(ny.$$.fragment,Fl),uDo=i(Fl),Xd=n(Fl,"P",{});var Dz=s(Xd);bDo=r(Dz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a masked image modeling head) when created
with the `),$ie=n(Dz,"CODE",{});var aKr=s($ie);vDo=r(aKr,"from_pretrained()"),aKr.forEach(t),TDo=r(Dz,"class method or the "),Iie=n(Dz,"CODE",{});var nKr=s(Iie);FDo=r(nKr,"from_config()"),nKr.forEach(t),CDo=r(Dz,`class
method.`),Dz.forEach(t),MDo=i(Fl),sy=n(Fl,"P",{});var Mke=s(sy);EDo=r(Mke,"This class cannot be instantiated directly using "),jie=n(Mke,"CODE",{});var sKr=s(jie);yDo=r(sKr,"__init__()"),sKr.forEach(t),wDo=r(Mke," (throws an error)."),Mke.forEach(t),ADo=i(Fl),rt=n(Fl,"DIV",{class:!0});var Cl=s(rt);m(ly.$$.fragment,Cl),LDo=i(Cl),Nie=n(Cl,"P",{});var lKr=s(Nie);BDo=r(lKr,"Instantiates one of the model classes of the library (with a masked image modeling head) from a configuration."),lKr.forEach(t),kDo=i(Cl),zd=n(Cl,"P",{});var qz=s(zd);xDo=r(qz,`Note:
Loading a model from its configuration file does `),Die=n(qz,"STRONG",{});var iKr=s(Die);RDo=r(iKr,"not"),iKr.forEach(t),SDo=r(qz,` load the model weights. It only affects the
model\u2019s configuration. Use `),qie=n(qz,"CODE",{});var dKr=s(qie);PDo=r(dKr,"from_pretrained()"),dKr.forEach(t),$Do=r(qz,"to load the model weights."),qz.forEach(t),IDo=i(Cl),Gie=n(Cl,"P",{});var cKr=s(Gie);jDo=r(cKr,"Examples:"),cKr.forEach(t),NDo=i(Cl),m(iy.$$.fragment,Cl),Cl.forEach(t),DDo=i(Fl),He=n(Fl,"DIV",{class:!0});var Zt=s(He);m(dy.$$.fragment,Zt),qDo=i(Zt),Oie=n(Zt,"P",{});var fKr=s(Oie);GDo=r(fKr,"Instantiate one of the model classes of the library (with a masked image modeling head) from a pretrained model."),fKr.forEach(t),ODo=i(Zt),tn=n(Zt,"P",{});var B4=s(tn);XDo=r(B4,"The model class to instantiate is selected based on the "),Xie=n(B4,"CODE",{});var mKr=s(Xie);zDo=r(mKr,"model_type"),mKr.forEach(t),VDo=r(B4,` property of the config object (either
passed as an argument or loaded from `),zie=n(B4,"CODE",{});var gKr=s(zie);WDo=r(gKr,"pretrained_model_name_or_path"),gKr.forEach(t),QDo=r(B4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Vie=n(B4,"CODE",{});var hKr=s(Vie);HDo=r(hKr,"pretrained_model_name_or_path"),hKr.forEach(t),UDo=r(B4,":"),B4.forEach(t),JDo=i(Zt),Vd=n(Zt,"UL",{});var Gz=s(Vd);fv=n(Gz,"LI",{});var J3e=s(fv);Wie=n(J3e,"STRONG",{});var pKr=s(Wie);YDo=r(pKr,"deit"),pKr.forEach(t),KDo=r(J3e," \u2014 "),xN=n(J3e,"A",{href:!0});var _Kr=s(xN);ZDo=r(_Kr,"DeiTForMaskedImageModeling"),_Kr.forEach(t),eqo=r(J3e," (DeiT model)"),J3e.forEach(t),oqo=i(Gz),mv=n(Gz,"LI",{});var Y3e=s(mv);Qie=n(Y3e,"STRONG",{});var uKr=s(Qie);rqo=r(uKr,"swin"),uKr.forEach(t),tqo=r(Y3e," \u2014 "),RN=n(Y3e,"A",{href:!0});var bKr=s(RN);aqo=r(bKr,"SwinForMaskedImageModeling"),bKr.forEach(t),nqo=r(Y3e," (Swin model)"),Y3e.forEach(t),sqo=i(Gz),gv=n(Gz,"LI",{});var K3e=s(gv);Hie=n(K3e,"STRONG",{});var vKr=s(Hie);lqo=r(vKr,"vit"),vKr.forEach(t),iqo=r(K3e," \u2014 "),SN=n(K3e,"A",{href:!0});var TKr=s(SN);dqo=r(TKr,"ViTForMaskedImageModeling"),TKr.forEach(t),cqo=r(K3e," (ViT model)"),K3e.forEach(t),Gz.forEach(t),fqo=i(Zt),hv=n(Zt,"P",{});var Z3e=s(hv);mqo=r(Z3e,"The model is set in evaluation mode by default using "),Uie=n(Z3e,"CODE",{});var FKr=s(Uie);gqo=r(FKr,"model.eval()"),FKr.forEach(t),hqo=r(Z3e,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Jie=n(Z3e,"CODE",{});var CKr=s(Jie);pqo=r(CKr,"model.train()"),CKr.forEach(t),Z3e.forEach(t),_qo=i(Zt),Yie=n(Zt,"P",{});var MKr=s(Yie);uqo=r(MKr,"Examples:"),MKr.forEach(t),bqo=i(Zt),m(cy.$$.fragment,Zt),Zt.forEach(t),Fl.forEach(t),_9e=i(d),Wd=n(d,"H2",{class:!0});var Eke=s(Wd);pv=n(Eke,"A",{id:!0,class:!0,href:!0});var EKr=s(pv);Kie=n(EKr,"SPAN",{});var yKr=s(Kie);m(fy.$$.fragment,yKr),yKr.forEach(t),EKr.forEach(t),vqo=i(Eke),Zie=n(Eke,"SPAN",{});var wKr=s(Zie);Tqo=r(wKr,"AutoModelForObjectDetection"),wKr.forEach(t),Eke.forEach(t),u9e=i(d),cr=n(d,"DIV",{class:!0});var Ml=s(cr);m(my.$$.fragment,Ml),Fqo=i(Ml),Qd=n(Ml,"P",{});var Oz=s(Qd);Cqo=r(Oz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a object detection head) when created
with the `),ede=n(Oz,"CODE",{});var AKr=s(ede);Mqo=r(AKr,"from_pretrained()"),AKr.forEach(t),Eqo=r(Oz,"class method or the "),ode=n(Oz,"CODE",{});var LKr=s(ode);yqo=r(LKr,"from_config()"),LKr.forEach(t),wqo=r(Oz,`class
method.`),Oz.forEach(t),Aqo=i(Ml),gy=n(Ml,"P",{});var yke=s(gy);Lqo=r(yke,"This class cannot be instantiated directly using "),rde=n(yke,"CODE",{});var BKr=s(rde);Bqo=r(BKr,"__init__()"),BKr.forEach(t),kqo=r(yke," (throws an error)."),yke.forEach(t),xqo=i(Ml),tt=n(Ml,"DIV",{class:!0});var El=s(tt);m(hy.$$.fragment,El),Rqo=i(El),tde=n(El,"P",{});var kKr=s(tde);Sqo=r(kKr,"Instantiates one of the model classes of the library (with a object detection head) from a configuration."),kKr.forEach(t),Pqo=i(El),Hd=n(El,"P",{});var Xz=s(Hd);$qo=r(Xz,`Note:
Loading a model from its configuration file does `),ade=n(Xz,"STRONG",{});var xKr=s(ade);Iqo=r(xKr,"not"),xKr.forEach(t),jqo=r(Xz,` load the model weights. It only affects the
model\u2019s configuration. Use `),nde=n(Xz,"CODE",{});var RKr=s(nde);Nqo=r(RKr,"from_pretrained()"),RKr.forEach(t),Dqo=r(Xz,"to load the model weights."),Xz.forEach(t),qqo=i(El),sde=n(El,"P",{});var SKr=s(sde);Gqo=r(SKr,"Examples:"),SKr.forEach(t),Oqo=i(El),m(py.$$.fragment,El),El.forEach(t),Xqo=i(Ml),Ue=n(Ml,"DIV",{class:!0});var ea=s(Ue);m(_y.$$.fragment,ea),zqo=i(ea),lde=n(ea,"P",{});var PKr=s(lde);Vqo=r(PKr,"Instantiate one of the model classes of the library (with a object detection head) from a pretrained model."),PKr.forEach(t),Wqo=i(ea),an=n(ea,"P",{});var k4=s(an);Qqo=r(k4,"The model class to instantiate is selected based on the "),ide=n(k4,"CODE",{});var $Kr=s(ide);Hqo=r($Kr,"model_type"),$Kr.forEach(t),Uqo=r(k4,` property of the config object (either
passed as an argument or loaded from `),dde=n(k4,"CODE",{});var IKr=s(dde);Jqo=r(IKr,"pretrained_model_name_or_path"),IKr.forEach(t),Yqo=r(k4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),cde=n(k4,"CODE",{});var jKr=s(cde);Kqo=r(jKr,"pretrained_model_name_or_path"),jKr.forEach(t),Zqo=r(k4,":"),k4.forEach(t),eGo=i(ea),fde=n(ea,"UL",{});var NKr=s(fde);_v=n(NKr,"LI",{});var eye=s(_v);mde=n(eye,"STRONG",{});var DKr=s(mde);oGo=r(DKr,"detr"),DKr.forEach(t),rGo=r(eye," \u2014 "),PN=n(eye,"A",{href:!0});var qKr=s(PN);tGo=r(qKr,"DetrForObjectDetection"),qKr.forEach(t),aGo=r(eye," (DETR model)"),eye.forEach(t),NKr.forEach(t),nGo=i(ea),uv=n(ea,"P",{});var oye=s(uv);sGo=r(oye,"The model is set in evaluation mode by default using "),gde=n(oye,"CODE",{});var GKr=s(gde);lGo=r(GKr,"model.eval()"),GKr.forEach(t),iGo=r(oye,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),hde=n(oye,"CODE",{});var OKr=s(hde);dGo=r(OKr,"model.train()"),OKr.forEach(t),oye.forEach(t),cGo=i(ea),pde=n(ea,"P",{});var XKr=s(pde);fGo=r(XKr,"Examples:"),XKr.forEach(t),mGo=i(ea),m(uy.$$.fragment,ea),ea.forEach(t),Ml.forEach(t),b9e=i(d),Ud=n(d,"H2",{class:!0});var wke=s(Ud);bv=n(wke,"A",{id:!0,class:!0,href:!0});var zKr=s(bv);_de=n(zKr,"SPAN",{});var VKr=s(_de);m(by.$$.fragment,VKr),VKr.forEach(t),zKr.forEach(t),gGo=i(wke),ude=n(wke,"SPAN",{});var WKr=s(ude);hGo=r(WKr,"AutoModelForImageSegmentation"),WKr.forEach(t),wke.forEach(t),v9e=i(d),fr=n(d,"DIV",{class:!0});var yl=s(fr);m(vy.$$.fragment,yl),pGo=i(yl),Jd=n(yl,"P",{});var zz=s(Jd);_Go=r(zz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a image segmentation head) when created
with the `),bde=n(zz,"CODE",{});var QKr=s(bde);uGo=r(QKr,"from_pretrained()"),QKr.forEach(t),bGo=r(zz,"class method or the "),vde=n(zz,"CODE",{});var HKr=s(vde);vGo=r(HKr,"from_config()"),HKr.forEach(t),TGo=r(zz,`class
method.`),zz.forEach(t),FGo=i(yl),Ty=n(yl,"P",{});var Ake=s(Ty);CGo=r(Ake,"This class cannot be instantiated directly using "),Tde=n(Ake,"CODE",{});var UKr=s(Tde);MGo=r(UKr,"__init__()"),UKr.forEach(t),EGo=r(Ake," (throws an error)."),Ake.forEach(t),yGo=i(yl),at=n(yl,"DIV",{class:!0});var wl=s(at);m(Fy.$$.fragment,wl),wGo=i(wl),Fde=n(wl,"P",{});var JKr=s(Fde);AGo=r(JKr,"Instantiates one of the model classes of the library (with a image segmentation head) from a configuration."),JKr.forEach(t),LGo=i(wl),Yd=n(wl,"P",{});var Vz=s(Yd);BGo=r(Vz,`Note:
Loading a model from its configuration file does `),Cde=n(Vz,"STRONG",{});var YKr=s(Cde);kGo=r(YKr,"not"),YKr.forEach(t),xGo=r(Vz,` load the model weights. It only affects the
model\u2019s configuration. Use `),Mde=n(Vz,"CODE",{});var KKr=s(Mde);RGo=r(KKr,"from_pretrained()"),KKr.forEach(t),SGo=r(Vz,"to load the model weights."),Vz.forEach(t),PGo=i(wl),Ede=n(wl,"P",{});var ZKr=s(Ede);$Go=r(ZKr,"Examples:"),ZKr.forEach(t),IGo=i(wl),m(Cy.$$.fragment,wl),wl.forEach(t),jGo=i(yl),Je=n(yl,"DIV",{class:!0});var oa=s(Je);m(My.$$.fragment,oa),NGo=i(oa),yde=n(oa,"P",{});var eZr=s(yde);DGo=r(eZr,"Instantiate one of the model classes of the library (with a image segmentation head) from a pretrained model."),eZr.forEach(t),qGo=i(oa),nn=n(oa,"P",{});var x4=s(nn);GGo=r(x4,"The model class to instantiate is selected based on the "),wde=n(x4,"CODE",{});var oZr=s(wde);OGo=r(oZr,"model_type"),oZr.forEach(t),XGo=r(x4,` property of the config object (either
passed as an argument or loaded from `),Ade=n(x4,"CODE",{});var rZr=s(Ade);zGo=r(rZr,"pretrained_model_name_or_path"),rZr.forEach(t),VGo=r(x4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Lde=n(x4,"CODE",{});var tZr=s(Lde);WGo=r(tZr,"pretrained_model_name_or_path"),tZr.forEach(t),QGo=r(x4,":"),x4.forEach(t),HGo=i(oa),Bde=n(oa,"UL",{});var aZr=s(Bde);vv=n(aZr,"LI",{});var rye=s(vv);kde=n(rye,"STRONG",{});var nZr=s(kde);UGo=r(nZr,"detr"),nZr.forEach(t),JGo=r(rye," \u2014 "),$N=n(rye,"A",{href:!0});var sZr=s($N);YGo=r(sZr,"DetrForSegmentation"),sZr.forEach(t),KGo=r(rye," (DETR model)"),rye.forEach(t),aZr.forEach(t),ZGo=i(oa),Tv=n(oa,"P",{});var tye=s(Tv);eOo=r(tye,"The model is set in evaluation mode by default using "),xde=n(tye,"CODE",{});var lZr=s(xde);oOo=r(lZr,"model.eval()"),lZr.forEach(t),rOo=r(tye,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Rde=n(tye,"CODE",{});var iZr=s(Rde);tOo=r(iZr,"model.train()"),iZr.forEach(t),tye.forEach(t),aOo=i(oa),Sde=n(oa,"P",{});var dZr=s(Sde);nOo=r(dZr,"Examples:"),dZr.forEach(t),sOo=i(oa),m(Ey.$$.fragment,oa),oa.forEach(t),yl.forEach(t),T9e=i(d),Kd=n(d,"H2",{class:!0});var Lke=s(Kd);Fv=n(Lke,"A",{id:!0,class:!0,href:!0});var cZr=s(Fv);Pde=n(cZr,"SPAN",{});var fZr=s(Pde);m(yy.$$.fragment,fZr),fZr.forEach(t),cZr.forEach(t),lOo=i(Lke),$de=n(Lke,"SPAN",{});var mZr=s($de);iOo=r(mZr,"AutoModelForSemanticSegmentation"),mZr.forEach(t),Lke.forEach(t),F9e=i(d),mr=n(d,"DIV",{class:!0});var Al=s(mr);m(wy.$$.fragment,Al),dOo=i(Al),Zd=n(Al,"P",{});var Wz=s(Zd);cOo=r(Wz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a semantic segmentation head) when created
with the `),Ide=n(Wz,"CODE",{});var gZr=s(Ide);fOo=r(gZr,"from_pretrained()"),gZr.forEach(t),mOo=r(Wz,"class method or the "),jde=n(Wz,"CODE",{});var hZr=s(jde);gOo=r(hZr,"from_config()"),hZr.forEach(t),hOo=r(Wz,`class
method.`),Wz.forEach(t),pOo=i(Al),Ay=n(Al,"P",{});var Bke=s(Ay);_Oo=r(Bke,"This class cannot be instantiated directly using "),Nde=n(Bke,"CODE",{});var pZr=s(Nde);uOo=r(pZr,"__init__()"),pZr.forEach(t),bOo=r(Bke," (throws an error)."),Bke.forEach(t),vOo=i(Al),nt=n(Al,"DIV",{class:!0});var Ll=s(nt);m(Ly.$$.fragment,Ll),TOo=i(Ll),Dde=n(Ll,"P",{});var _Zr=s(Dde);FOo=r(_Zr,"Instantiates one of the model classes of the library (with a semantic segmentation head) from a configuration."),_Zr.forEach(t),COo=i(Ll),ec=n(Ll,"P",{});var Qz=s(ec);MOo=r(Qz,`Note:
Loading a model from its configuration file does `),qde=n(Qz,"STRONG",{});var uZr=s(qde);EOo=r(uZr,"not"),uZr.forEach(t),yOo=r(Qz,` load the model weights. It only affects the
model\u2019s configuration. Use `),Gde=n(Qz,"CODE",{});var bZr=s(Gde);wOo=r(bZr,"from_pretrained()"),bZr.forEach(t),AOo=r(Qz,"to load the model weights."),Qz.forEach(t),LOo=i(Ll),Ode=n(Ll,"P",{});var vZr=s(Ode);BOo=r(vZr,"Examples:"),vZr.forEach(t),kOo=i(Ll),m(By.$$.fragment,Ll),Ll.forEach(t),xOo=i(Al),Ye=n(Al,"DIV",{class:!0});var ra=s(Ye);m(ky.$$.fragment,ra),ROo=i(ra),Xde=n(ra,"P",{});var TZr=s(Xde);SOo=r(TZr,"Instantiate one of the model classes of the library (with a semantic segmentation head) from a pretrained model."),TZr.forEach(t),POo=i(ra),sn=n(ra,"P",{});var R4=s(sn);$Oo=r(R4,"The model class to instantiate is selected based on the "),zde=n(R4,"CODE",{});var FZr=s(zde);IOo=r(FZr,"model_type"),FZr.forEach(t),jOo=r(R4,` property of the config object (either
passed as an argument or loaded from `),Vde=n(R4,"CODE",{});var CZr=s(Vde);NOo=r(CZr,"pretrained_model_name_or_path"),CZr.forEach(t),DOo=r(R4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Wde=n(R4,"CODE",{});var MZr=s(Wde);qOo=r(MZr,"pretrained_model_name_or_path"),MZr.forEach(t),GOo=r(R4,":"),R4.forEach(t),OOo=i(ra),xy=n(ra,"UL",{});var kke=s(xy);Cv=n(kke,"LI",{});var aye=s(Cv);Qde=n(aye,"STRONG",{});var EZr=s(Qde);XOo=r(EZr,"beit"),EZr.forEach(t),zOo=r(aye," \u2014 "),IN=n(aye,"A",{href:!0});var yZr=s(IN);VOo=r(yZr,"BeitForSemanticSegmentation"),yZr.forEach(t),WOo=r(aye," (BEiT model)"),aye.forEach(t),QOo=i(kke),Mv=n(kke,"LI",{});var nye=s(Mv);Hde=n(nye,"STRONG",{});var wZr=s(Hde);HOo=r(wZr,"segformer"),wZr.forEach(t),UOo=r(nye," \u2014 "),jN=n(nye,"A",{href:!0});var AZr=s(jN);JOo=r(AZr,"SegformerForSemanticSegmentation"),AZr.forEach(t),YOo=r(nye," (SegFormer model)"),nye.forEach(t),kke.forEach(t),KOo=i(ra),Ev=n(ra,"P",{});var sye=s(Ev);ZOo=r(sye,"The model is set in evaluation mode by default using "),Ude=n(sye,"CODE",{});var LZr=s(Ude);eXo=r(LZr,"model.eval()"),LZr.forEach(t),oXo=r(sye,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Jde=n(sye,"CODE",{});var BZr=s(Jde);rXo=r(BZr,"model.train()"),BZr.forEach(t),sye.forEach(t),tXo=i(ra),Yde=n(ra,"P",{});var kZr=s(Yde);aXo=r(kZr,"Examples:"),kZr.forEach(t),nXo=i(ra),m(Ry.$$.fragment,ra),ra.forEach(t),Al.forEach(t),C9e=i(d),oc=n(d,"H2",{class:!0});var xke=s(oc);yv=n(xke,"A",{id:!0,class:!0,href:!0});var xZr=s(yv);Kde=n(xZr,"SPAN",{});var RZr=s(Kde);m(Sy.$$.fragment,RZr),RZr.forEach(t),xZr.forEach(t),sXo=i(xke),Zde=n(xke,"SPAN",{});var SZr=s(Zde);lXo=r(SZr,"TFAutoModel"),SZr.forEach(t),xke.forEach(t),M9e=i(d),gr=n(d,"DIV",{class:!0});var Bl=s(gr);m(Py.$$.fragment,Bl),iXo=i(Bl),rc=n(Bl,"P",{});var Hz=s(rc);dXo=r(Hz,`This is a generic model class that will be instantiated as one of the base model classes of the library when created
with the `),ece=n(Hz,"CODE",{});var PZr=s(ece);cXo=r(PZr,"from_pretrained()"),PZr.forEach(t),fXo=r(Hz,"class method or the "),oce=n(Hz,"CODE",{});var $Zr=s(oce);mXo=r($Zr,"from_config()"),$Zr.forEach(t),gXo=r(Hz,`class
method.`),Hz.forEach(t),hXo=i(Bl),$y=n(Bl,"P",{});var Rke=s($y);pXo=r(Rke,"This class cannot be instantiated directly using "),rce=n(Rke,"CODE",{});var IZr=s(rce);_Xo=r(IZr,"__init__()"),IZr.forEach(t),uXo=r(Rke," (throws an error)."),Rke.forEach(t),bXo=i(Bl),st=n(Bl,"DIV",{class:!0});var kl=s(st);m(Iy.$$.fragment,kl),vXo=i(kl),tce=n(kl,"P",{});var jZr=s(tce);TXo=r(jZr,"Instantiates one of the base model classes of the library from a configuration."),jZr.forEach(t),FXo=i(kl),tc=n(kl,"P",{});var Uz=s(tc);CXo=r(Uz,`Note:
Loading a model from its configuration file does `),ace=n(Uz,"STRONG",{});var NZr=s(ace);MXo=r(NZr,"not"),NZr.forEach(t),EXo=r(Uz,` load the model weights. It only affects the
model\u2019s configuration. Use `),nce=n(Uz,"CODE",{});var DZr=s(nce);yXo=r(DZr,"from_pretrained()"),DZr.forEach(t),wXo=r(Uz,"to load the model weights."),Uz.forEach(t),AXo=i(kl),sce=n(kl,"P",{});var qZr=s(sce);LXo=r(qZr,"Examples:"),qZr.forEach(t),BXo=i(kl),m(jy.$$.fragment,kl),kl.forEach(t),kXo=i(Bl),go=n(Bl,"DIV",{class:!0});var ca=s(go);m(Ny.$$.fragment,ca),xXo=i(ca),lce=n(ca,"P",{});var GZr=s(lce);RXo=r(GZr,"Instantiate one of the base model classes of the library from a pretrained model."),GZr.forEach(t),SXo=i(ca),ln=n(ca,"P",{});var S4=s(ln);PXo=r(S4,"The model class to instantiate is selected based on the "),ice=n(S4,"CODE",{});var OZr=s(ice);$Xo=r(OZr,"model_type"),OZr.forEach(t),IXo=r(S4,` property of the config object (either
passed as an argument or loaded from `),dce=n(S4,"CODE",{});var XZr=s(dce);jXo=r(XZr,"pretrained_model_name_or_path"),XZr.forEach(t),NXo=r(S4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),cce=n(S4,"CODE",{});var zZr=s(cce);DXo=r(zZr,"pretrained_model_name_or_path"),zZr.forEach(t),qXo=r(S4,":"),S4.forEach(t),GXo=i(ca),B=n(ca,"UL",{});var k=s(B);wv=n(k,"LI",{});var lye=s(wv);fce=n(lye,"STRONG",{});var VZr=s(fce);OXo=r(VZr,"albert"),VZr.forEach(t),XXo=r(lye," \u2014 "),NN=n(lye,"A",{href:!0});var WZr=s(NN);zXo=r(WZr,"TFAlbertModel"),WZr.forEach(t),VXo=r(lye," (ALBERT model)"),lye.forEach(t),WXo=i(k),Av=n(k,"LI",{});var iye=s(Av);mce=n(iye,"STRONG",{});var QZr=s(mce);QXo=r(QZr,"bart"),QZr.forEach(t),HXo=r(iye," \u2014 "),DN=n(iye,"A",{href:!0});var HZr=s(DN);UXo=r(HZr,"TFBartModel"),HZr.forEach(t),JXo=r(iye," (BART model)"),iye.forEach(t),YXo=i(k),Lv=n(k,"LI",{});var dye=s(Lv);gce=n(dye,"STRONG",{});var UZr=s(gce);KXo=r(UZr,"bert"),UZr.forEach(t),ZXo=r(dye," \u2014 "),qN=n(dye,"A",{href:!0});var JZr=s(qN);ezo=r(JZr,"TFBertModel"),JZr.forEach(t),ozo=r(dye," (BERT model)"),dye.forEach(t),rzo=i(k),Bv=n(k,"LI",{});var cye=s(Bv);hce=n(cye,"STRONG",{});var YZr=s(hce);tzo=r(YZr,"blenderbot"),YZr.forEach(t),azo=r(cye," \u2014 "),GN=n(cye,"A",{href:!0});var KZr=s(GN);nzo=r(KZr,"TFBlenderbotModel"),KZr.forEach(t),szo=r(cye," (Blenderbot model)"),cye.forEach(t),lzo=i(k),kv=n(k,"LI",{});var fye=s(kv);pce=n(fye,"STRONG",{});var ZZr=s(pce);izo=r(ZZr,"blenderbot-small"),ZZr.forEach(t),dzo=r(fye," \u2014 "),ON=n(fye,"A",{href:!0});var eet=s(ON);czo=r(eet,"TFBlenderbotSmallModel"),eet.forEach(t),fzo=r(fye," (BlenderbotSmall model)"),fye.forEach(t),mzo=i(k),xv=n(k,"LI",{});var mye=s(xv);_ce=n(mye,"STRONG",{});var oet=s(_ce);gzo=r(oet,"camembert"),oet.forEach(t),hzo=r(mye," \u2014 "),XN=n(mye,"A",{href:!0});var ret=s(XN);pzo=r(ret,"TFCamembertModel"),ret.forEach(t),_zo=r(mye," (CamemBERT model)"),mye.forEach(t),uzo=i(k),Rv=n(k,"LI",{});var gye=s(Rv);uce=n(gye,"STRONG",{});var tet=s(uce);bzo=r(tet,"clip"),tet.forEach(t),vzo=r(gye," \u2014 "),zN=n(gye,"A",{href:!0});var aet=s(zN);Tzo=r(aet,"TFCLIPModel"),aet.forEach(t),Fzo=r(gye," (CLIP model)"),gye.forEach(t),Czo=i(k),Sv=n(k,"LI",{});var hye=s(Sv);bce=n(hye,"STRONG",{});var net=s(bce);Mzo=r(net,"convbert"),net.forEach(t),Ezo=r(hye," \u2014 "),VN=n(hye,"A",{href:!0});var set=s(VN);yzo=r(set,"TFConvBertModel"),set.forEach(t),wzo=r(hye," (ConvBERT model)"),hye.forEach(t),Azo=i(k),Pv=n(k,"LI",{});var pye=s(Pv);vce=n(pye,"STRONG",{});var iet=s(vce);Lzo=r(iet,"convnext"),iet.forEach(t),Bzo=r(pye," \u2014 "),WN=n(pye,"A",{href:!0});var det=s(WN);kzo=r(det,"TFConvNextModel"),det.forEach(t),xzo=r(pye," (ConvNext model)"),pye.forEach(t),Rzo=i(k),$v=n(k,"LI",{});var _ye=s($v);Tce=n(_ye,"STRONG",{});var cet=s(Tce);Szo=r(cet,"ctrl"),cet.forEach(t),Pzo=r(_ye," \u2014 "),QN=n(_ye,"A",{href:!0});var fet=s(QN);$zo=r(fet,"TFCTRLModel"),fet.forEach(t),Izo=r(_ye," (CTRL model)"),_ye.forEach(t),jzo=i(k),Iv=n(k,"LI",{});var uye=s(Iv);Fce=n(uye,"STRONG",{});var met=s(Fce);Nzo=r(met,"deberta"),met.forEach(t),Dzo=r(uye," \u2014 "),HN=n(uye,"A",{href:!0});var get=s(HN);qzo=r(get,"TFDebertaModel"),get.forEach(t),Gzo=r(uye," (DeBERTa model)"),uye.forEach(t),Ozo=i(k),jv=n(k,"LI",{});var bye=s(jv);Cce=n(bye,"STRONG",{});var het=s(Cce);Xzo=r(het,"deberta-v2"),het.forEach(t),zzo=r(bye," \u2014 "),UN=n(bye,"A",{href:!0});var pet=s(UN);Vzo=r(pet,"TFDebertaV2Model"),pet.forEach(t),Wzo=r(bye," (DeBERTa-v2 model)"),bye.forEach(t),Qzo=i(k),Nv=n(k,"LI",{});var vye=s(Nv);Mce=n(vye,"STRONG",{});var _et=s(Mce);Hzo=r(_et,"distilbert"),_et.forEach(t),Uzo=r(vye," \u2014 "),JN=n(vye,"A",{href:!0});var uet=s(JN);Jzo=r(uet,"TFDistilBertModel"),uet.forEach(t),Yzo=r(vye," (DistilBERT model)"),vye.forEach(t),Kzo=i(k),Dv=n(k,"LI",{});var Tye=s(Dv);Ece=n(Tye,"STRONG",{});var bet=s(Ece);Zzo=r(bet,"dpr"),bet.forEach(t),eVo=r(Tye," \u2014 "),YN=n(Tye,"A",{href:!0});var vet=s(YN);oVo=r(vet,"TFDPRQuestionEncoder"),vet.forEach(t),rVo=r(Tye," (DPR model)"),Tye.forEach(t),tVo=i(k),qv=n(k,"LI",{});var Fye=s(qv);yce=n(Fye,"STRONG",{});var Tet=s(yce);aVo=r(Tet,"electra"),Tet.forEach(t),nVo=r(Fye," \u2014 "),KN=n(Fye,"A",{href:!0});var Fet=s(KN);sVo=r(Fet,"TFElectraModel"),Fet.forEach(t),lVo=r(Fye," (ELECTRA model)"),Fye.forEach(t),iVo=i(k),Gv=n(k,"LI",{});var Cye=s(Gv);wce=n(Cye,"STRONG",{});var Cet=s(wce);dVo=r(Cet,"flaubert"),Cet.forEach(t),cVo=r(Cye," \u2014 "),ZN=n(Cye,"A",{href:!0});var Met=s(ZN);fVo=r(Met,"TFFlaubertModel"),Met.forEach(t),mVo=r(Cye," (FlauBERT model)"),Cye.forEach(t),gVo=i(k),Ss=n(k,"LI",{});var XL=s(Ss);Ace=n(XL,"STRONG",{});var Eet=s(Ace);hVo=r(Eet,"funnel"),Eet.forEach(t),pVo=r(XL," \u2014 "),eD=n(XL,"A",{href:!0});var yet=s(eD);_Vo=r(yet,"TFFunnelModel"),yet.forEach(t),uVo=r(XL," or "),oD=n(XL,"A",{href:!0});var wet=s(oD);bVo=r(wet,"TFFunnelBaseModel"),wet.forEach(t),vVo=r(XL," (Funnel Transformer model)"),XL.forEach(t),TVo=i(k),Ov=n(k,"LI",{});var Mye=s(Ov);Lce=n(Mye,"STRONG",{});var Aet=s(Lce);FVo=r(Aet,"gpt2"),Aet.forEach(t),CVo=r(Mye," \u2014 "),rD=n(Mye,"A",{href:!0});var Let=s(rD);MVo=r(Let,"TFGPT2Model"),Let.forEach(t),EVo=r(Mye," (OpenAI GPT-2 model)"),Mye.forEach(t),yVo=i(k),Xv=n(k,"LI",{});var Eye=s(Xv);Bce=n(Eye,"STRONG",{});var Bet=s(Bce);wVo=r(Bet,"hubert"),Bet.forEach(t),AVo=r(Eye," \u2014 "),tD=n(Eye,"A",{href:!0});var ket=s(tD);LVo=r(ket,"TFHubertModel"),ket.forEach(t),BVo=r(Eye," (Hubert model)"),Eye.forEach(t),kVo=i(k),zv=n(k,"LI",{});var yye=s(zv);kce=n(yye,"STRONG",{});var xet=s(kce);xVo=r(xet,"layoutlm"),xet.forEach(t),RVo=r(yye," \u2014 "),aD=n(yye,"A",{href:!0});var Ret=s(aD);SVo=r(Ret,"TFLayoutLMModel"),Ret.forEach(t),PVo=r(yye," (LayoutLM model)"),yye.forEach(t),$Vo=i(k),Vv=n(k,"LI",{});var wye=s(Vv);xce=n(wye,"STRONG",{});var Set=s(xce);IVo=r(Set,"led"),Set.forEach(t),jVo=r(wye," \u2014 "),nD=n(wye,"A",{href:!0});var Pet=s(nD);NVo=r(Pet,"TFLEDModel"),Pet.forEach(t),DVo=r(wye," (LED model)"),wye.forEach(t),qVo=i(k),Wv=n(k,"LI",{});var Aye=s(Wv);Rce=n(Aye,"STRONG",{});var $et=s(Rce);GVo=r($et,"longformer"),$et.forEach(t),OVo=r(Aye," \u2014 "),sD=n(Aye,"A",{href:!0});var Iet=s(sD);XVo=r(Iet,"TFLongformerModel"),Iet.forEach(t),zVo=r(Aye," (Longformer model)"),Aye.forEach(t),VVo=i(k),Qv=n(k,"LI",{});var Lye=s(Qv);Sce=n(Lye,"STRONG",{});var jet=s(Sce);WVo=r(jet,"lxmert"),jet.forEach(t),QVo=r(Lye," \u2014 "),lD=n(Lye,"A",{href:!0});var Net=s(lD);HVo=r(Net,"TFLxmertModel"),Net.forEach(t),UVo=r(Lye," (LXMERT model)"),Lye.forEach(t),JVo=i(k),Hv=n(k,"LI",{});var Bye=s(Hv);Pce=n(Bye,"STRONG",{});var Det=s(Pce);YVo=r(Det,"marian"),Det.forEach(t),KVo=r(Bye," \u2014 "),iD=n(Bye,"A",{href:!0});var qet=s(iD);ZVo=r(qet,"TFMarianModel"),qet.forEach(t),eWo=r(Bye," (Marian model)"),Bye.forEach(t),oWo=i(k),Uv=n(k,"LI",{});var kye=s(Uv);$ce=n(kye,"STRONG",{});var Get=s($ce);rWo=r(Get,"mbart"),Get.forEach(t),tWo=r(kye," \u2014 "),dD=n(kye,"A",{href:!0});var Oet=s(dD);aWo=r(Oet,"TFMBartModel"),Oet.forEach(t),nWo=r(kye," (mBART model)"),kye.forEach(t),sWo=i(k),Jv=n(k,"LI",{});var xye=s(Jv);Ice=n(xye,"STRONG",{});var Xet=s(Ice);lWo=r(Xet,"mobilebert"),Xet.forEach(t),iWo=r(xye," \u2014 "),cD=n(xye,"A",{href:!0});var zet=s(cD);dWo=r(zet,"TFMobileBertModel"),zet.forEach(t),cWo=r(xye," (MobileBERT model)"),xye.forEach(t),fWo=i(k),Yv=n(k,"LI",{});var Rye=s(Yv);jce=n(Rye,"STRONG",{});var Vet=s(jce);mWo=r(Vet,"mpnet"),Vet.forEach(t),gWo=r(Rye," \u2014 "),fD=n(Rye,"A",{href:!0});var Wet=s(fD);hWo=r(Wet,"TFMPNetModel"),Wet.forEach(t),pWo=r(Rye," (MPNet model)"),Rye.forEach(t),_Wo=i(k),Kv=n(k,"LI",{});var Sye=s(Kv);Nce=n(Sye,"STRONG",{});var Qet=s(Nce);uWo=r(Qet,"mt5"),Qet.forEach(t),bWo=r(Sye," \u2014 "),mD=n(Sye,"A",{href:!0});var Het=s(mD);vWo=r(Het,"TFMT5Model"),Het.forEach(t),TWo=r(Sye," (mT5 model)"),Sye.forEach(t),FWo=i(k),Zv=n(k,"LI",{});var Pye=s(Zv);Dce=n(Pye,"STRONG",{});var Uet=s(Dce);CWo=r(Uet,"openai-gpt"),Uet.forEach(t),MWo=r(Pye," \u2014 "),gD=n(Pye,"A",{href:!0});var Jet=s(gD);EWo=r(Jet,"TFOpenAIGPTModel"),Jet.forEach(t),yWo=r(Pye," (OpenAI GPT model)"),Pye.forEach(t),wWo=i(k),e6=n(k,"LI",{});var $ye=s(e6);qce=n($ye,"STRONG",{});var Yet=s(qce);AWo=r(Yet,"pegasus"),Yet.forEach(t),LWo=r($ye," \u2014 "),hD=n($ye,"A",{href:!0});var Ket=s(hD);BWo=r(Ket,"TFPegasusModel"),Ket.forEach(t),kWo=r($ye," (Pegasus model)"),$ye.forEach(t),xWo=i(k),o6=n(k,"LI",{});var Iye=s(o6);Gce=n(Iye,"STRONG",{});var Zet=s(Gce);RWo=r(Zet,"rembert"),Zet.forEach(t),SWo=r(Iye," \u2014 "),pD=n(Iye,"A",{href:!0});var eot=s(pD);PWo=r(eot,"TFRemBertModel"),eot.forEach(t),$Wo=r(Iye," (RemBERT model)"),Iye.forEach(t),IWo=i(k),r6=n(k,"LI",{});var jye=s(r6);Oce=n(jye,"STRONG",{});var oot=s(Oce);jWo=r(oot,"roberta"),oot.forEach(t),NWo=r(jye," \u2014 "),_D=n(jye,"A",{href:!0});var rot=s(_D);DWo=r(rot,"TFRobertaModel"),rot.forEach(t),qWo=r(jye," (RoBERTa model)"),jye.forEach(t),GWo=i(k),t6=n(k,"LI",{});var Nye=s(t6);Xce=n(Nye,"STRONG",{});var tot=s(Xce);OWo=r(tot,"roformer"),tot.forEach(t),XWo=r(Nye," \u2014 "),uD=n(Nye,"A",{href:!0});var aot=s(uD);zWo=r(aot,"TFRoFormerModel"),aot.forEach(t),VWo=r(Nye," (RoFormer model)"),Nye.forEach(t),WWo=i(k),a6=n(k,"LI",{});var Dye=s(a6);zce=n(Dye,"STRONG",{});var not=s(zce);QWo=r(not,"speech_to_text"),not.forEach(t),HWo=r(Dye," \u2014 "),bD=n(Dye,"A",{href:!0});var sot=s(bD);UWo=r(sot,"TFSpeech2TextModel"),sot.forEach(t),JWo=r(Dye," (Speech2Text model)"),Dye.forEach(t),YWo=i(k),n6=n(k,"LI",{});var qye=s(n6);Vce=n(qye,"STRONG",{});var lot=s(Vce);KWo=r(lot,"t5"),lot.forEach(t),ZWo=r(qye," \u2014 "),vD=n(qye,"A",{href:!0});var iot=s(vD);eQo=r(iot,"TFT5Model"),iot.forEach(t),oQo=r(qye," (T5 model)"),qye.forEach(t),rQo=i(k),s6=n(k,"LI",{});var Gye=s(s6);Wce=n(Gye,"STRONG",{});var dot=s(Wce);tQo=r(dot,"tapas"),dot.forEach(t),aQo=r(Gye," \u2014 "),TD=n(Gye,"A",{href:!0});var cot=s(TD);nQo=r(cot,"TFTapasModel"),cot.forEach(t),sQo=r(Gye," (TAPAS model)"),Gye.forEach(t),lQo=i(k),l6=n(k,"LI",{});var Oye=s(l6);Qce=n(Oye,"STRONG",{});var fot=s(Qce);iQo=r(fot,"transfo-xl"),fot.forEach(t),dQo=r(Oye," \u2014 "),FD=n(Oye,"A",{href:!0});var mot=s(FD);cQo=r(mot,"TFTransfoXLModel"),mot.forEach(t),fQo=r(Oye," (Transformer-XL model)"),Oye.forEach(t),mQo=i(k),i6=n(k,"LI",{});var Xye=s(i6);Hce=n(Xye,"STRONG",{});var got=s(Hce);gQo=r(got,"vit"),got.forEach(t),hQo=r(Xye," \u2014 "),CD=n(Xye,"A",{href:!0});var hot=s(CD);pQo=r(hot,"TFViTModel"),hot.forEach(t),_Qo=r(Xye," (ViT model)"),Xye.forEach(t),uQo=i(k),d6=n(k,"LI",{});var zye=s(d6);Uce=n(zye,"STRONG",{});var pot=s(Uce);bQo=r(pot,"wav2vec2"),pot.forEach(t),vQo=r(zye," \u2014 "),MD=n(zye,"A",{href:!0});var _ot=s(MD);TQo=r(_ot,"TFWav2Vec2Model"),_ot.forEach(t),FQo=r(zye," (Wav2Vec2 model)"),zye.forEach(t),CQo=i(k),c6=n(k,"LI",{});var Vye=s(c6);Jce=n(Vye,"STRONG",{});var uot=s(Jce);MQo=r(uot,"xlm"),uot.forEach(t),EQo=r(Vye," \u2014 "),ED=n(Vye,"A",{href:!0});var bot=s(ED);yQo=r(bot,"TFXLMModel"),bot.forEach(t),wQo=r(Vye," (XLM model)"),Vye.forEach(t),AQo=i(k),f6=n(k,"LI",{});var Wye=s(f6);Yce=n(Wye,"STRONG",{});var vot=s(Yce);LQo=r(vot,"xlm-roberta"),vot.forEach(t),BQo=r(Wye," \u2014 "),yD=n(Wye,"A",{href:!0});var Tot=s(yD);kQo=r(Tot,"TFXLMRobertaModel"),Tot.forEach(t),xQo=r(Wye," (XLM-RoBERTa model)"),Wye.forEach(t),RQo=i(k),m6=n(k,"LI",{});var Qye=s(m6);Kce=n(Qye,"STRONG",{});var Fot=s(Kce);SQo=r(Fot,"xlnet"),Fot.forEach(t),PQo=r(Qye," \u2014 "),wD=n(Qye,"A",{href:!0});var Cot=s(wD);$Qo=r(Cot,"TFXLNetModel"),Cot.forEach(t),IQo=r(Qye," (XLNet model)"),Qye.forEach(t),k.forEach(t),jQo=i(ca),Zce=n(ca,"P",{});var Mot=s(Zce);NQo=r(Mot,"Examples:"),Mot.forEach(t),DQo=i(ca),m(Dy.$$.fragment,ca),ca.forEach(t),Bl.forEach(t),E9e=i(d),ac=n(d,"H2",{class:!0});var Ske=s(ac);g6=n(Ske,"A",{id:!0,class:!0,href:!0});var Eot=s(g6);efe=n(Eot,"SPAN",{});var yot=s(efe);m(qy.$$.fragment,yot),yot.forEach(t),Eot.forEach(t),qQo=i(Ske),ofe=n(Ske,"SPAN",{});var wot=s(ofe);GQo=r(wot,"TFAutoModelForPreTraining"),wot.forEach(t),Ske.forEach(t),y9e=i(d),hr=n(d,"DIV",{class:!0});var xl=s(hr);m(Gy.$$.fragment,xl),OQo=i(xl),nc=n(xl,"P",{});var Jz=s(nc);XQo=r(Jz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a pretraining head) when created
with the `),rfe=n(Jz,"CODE",{});var Aot=s(rfe);zQo=r(Aot,"from_pretrained()"),Aot.forEach(t),VQo=r(Jz,"class method or the "),tfe=n(Jz,"CODE",{});var Lot=s(tfe);WQo=r(Lot,"from_config()"),Lot.forEach(t),QQo=r(Jz,`class
method.`),Jz.forEach(t),HQo=i(xl),Oy=n(xl,"P",{});var Pke=s(Oy);UQo=r(Pke,"This class cannot be instantiated directly using "),afe=n(Pke,"CODE",{});var Bot=s(afe);JQo=r(Bot,"__init__()"),Bot.forEach(t),YQo=r(Pke," (throws an error)."),Pke.forEach(t),KQo=i(xl),lt=n(xl,"DIV",{class:!0});var Rl=s(lt);m(Xy.$$.fragment,Rl),ZQo=i(Rl),nfe=n(Rl,"P",{});var kot=s(nfe);eHo=r(kot,"Instantiates one of the model classes of the library (with a pretraining head) from a configuration."),kot.forEach(t),oHo=i(Rl),sc=n(Rl,"P",{});var Yz=s(sc);rHo=r(Yz,`Note:
Loading a model from its configuration file does `),sfe=n(Yz,"STRONG",{});var xot=s(sfe);tHo=r(xot,"not"),xot.forEach(t),aHo=r(Yz,` load the model weights. It only affects the
model\u2019s configuration. Use `),lfe=n(Yz,"CODE",{});var Rot=s(lfe);nHo=r(Rot,"from_pretrained()"),Rot.forEach(t),sHo=r(Yz,"to load the model weights."),Yz.forEach(t),lHo=i(Rl),ife=n(Rl,"P",{});var Sot=s(ife);iHo=r(Sot,"Examples:"),Sot.forEach(t),dHo=i(Rl),m(zy.$$.fragment,Rl),Rl.forEach(t),cHo=i(xl),ho=n(xl,"DIV",{class:!0});var fa=s(ho);m(Vy.$$.fragment,fa),fHo=i(fa),dfe=n(fa,"P",{});var Pot=s(dfe);mHo=r(Pot,"Instantiate one of the model classes of the library (with a pretraining head) from a pretrained model."),Pot.forEach(t),gHo=i(fa),dn=n(fa,"P",{});var P4=s(dn);hHo=r(P4,"The model class to instantiate is selected based on the "),cfe=n(P4,"CODE",{});var $ot=s(cfe);pHo=r($ot,"model_type"),$ot.forEach(t),_Ho=r(P4,` property of the config object (either
passed as an argument or loaded from `),ffe=n(P4,"CODE",{});var Iot=s(ffe);uHo=r(Iot,"pretrained_model_name_or_path"),Iot.forEach(t),bHo=r(P4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),mfe=n(P4,"CODE",{});var jot=s(mfe);vHo=r(jot,"pretrained_model_name_or_path"),jot.forEach(t),THo=r(P4,":"),P4.forEach(t),FHo=i(fa),H=n(fa,"UL",{});var U=s(H);h6=n(U,"LI",{});var Hye=s(h6);gfe=n(Hye,"STRONG",{});var Not=s(gfe);CHo=r(Not,"albert"),Not.forEach(t),MHo=r(Hye," \u2014 "),AD=n(Hye,"A",{href:!0});var Dot=s(AD);EHo=r(Dot,"TFAlbertForPreTraining"),Dot.forEach(t),yHo=r(Hye," (ALBERT model)"),Hye.forEach(t),wHo=i(U),p6=n(U,"LI",{});var Uye=s(p6);hfe=n(Uye,"STRONG",{});var qot=s(hfe);AHo=r(qot,"bart"),qot.forEach(t),LHo=r(Uye," \u2014 "),LD=n(Uye,"A",{href:!0});var Got=s(LD);BHo=r(Got,"TFBartForConditionalGeneration"),Got.forEach(t),kHo=r(Uye," (BART model)"),Uye.forEach(t),xHo=i(U),_6=n(U,"LI",{});var Jye=s(_6);pfe=n(Jye,"STRONG",{});var Oot=s(pfe);RHo=r(Oot,"bert"),Oot.forEach(t),SHo=r(Jye," \u2014 "),BD=n(Jye,"A",{href:!0});var Xot=s(BD);PHo=r(Xot,"TFBertForPreTraining"),Xot.forEach(t),$Ho=r(Jye," (BERT model)"),Jye.forEach(t),IHo=i(U),u6=n(U,"LI",{});var Yye=s(u6);_fe=n(Yye,"STRONG",{});var zot=s(_fe);jHo=r(zot,"camembert"),zot.forEach(t),NHo=r(Yye," \u2014 "),kD=n(Yye,"A",{href:!0});var Vot=s(kD);DHo=r(Vot,"TFCamembertForMaskedLM"),Vot.forEach(t),qHo=r(Yye," (CamemBERT model)"),Yye.forEach(t),GHo=i(U),b6=n(U,"LI",{});var Kye=s(b6);ufe=n(Kye,"STRONG",{});var Wot=s(ufe);OHo=r(Wot,"ctrl"),Wot.forEach(t),XHo=r(Kye," \u2014 "),xD=n(Kye,"A",{href:!0});var Qot=s(xD);zHo=r(Qot,"TFCTRLLMHeadModel"),Qot.forEach(t),VHo=r(Kye," (CTRL model)"),Kye.forEach(t),WHo=i(U),v6=n(U,"LI",{});var Zye=s(v6);bfe=n(Zye,"STRONG",{});var Hot=s(bfe);QHo=r(Hot,"distilbert"),Hot.forEach(t),HHo=r(Zye," \u2014 "),RD=n(Zye,"A",{href:!0});var Uot=s(RD);UHo=r(Uot,"TFDistilBertForMaskedLM"),Uot.forEach(t),JHo=r(Zye," (DistilBERT model)"),Zye.forEach(t),YHo=i(U),T6=n(U,"LI",{});var ewe=s(T6);vfe=n(ewe,"STRONG",{});var Jot=s(vfe);KHo=r(Jot,"electra"),Jot.forEach(t),ZHo=r(ewe," \u2014 "),SD=n(ewe,"A",{href:!0});var Yot=s(SD);eUo=r(Yot,"TFElectraForPreTraining"),Yot.forEach(t),oUo=r(ewe," (ELECTRA model)"),ewe.forEach(t),rUo=i(U),F6=n(U,"LI",{});var owe=s(F6);Tfe=n(owe,"STRONG",{});var Kot=s(Tfe);tUo=r(Kot,"flaubert"),Kot.forEach(t),aUo=r(owe," \u2014 "),PD=n(owe,"A",{href:!0});var Zot=s(PD);nUo=r(Zot,"TFFlaubertWithLMHeadModel"),Zot.forEach(t),sUo=r(owe," (FlauBERT model)"),owe.forEach(t),lUo=i(U),C6=n(U,"LI",{});var rwe=s(C6);Ffe=n(rwe,"STRONG",{});var ert=s(Ffe);iUo=r(ert,"funnel"),ert.forEach(t),dUo=r(rwe," \u2014 "),$D=n(rwe,"A",{href:!0});var ort=s($D);cUo=r(ort,"TFFunnelForPreTraining"),ort.forEach(t),fUo=r(rwe," (Funnel Transformer model)"),rwe.forEach(t),mUo=i(U),M6=n(U,"LI",{});var twe=s(M6);Cfe=n(twe,"STRONG",{});var rrt=s(Cfe);gUo=r(rrt,"gpt2"),rrt.forEach(t),hUo=r(twe," \u2014 "),ID=n(twe,"A",{href:!0});var trt=s(ID);pUo=r(trt,"TFGPT2LMHeadModel"),trt.forEach(t),_Uo=r(twe," (OpenAI GPT-2 model)"),twe.forEach(t),uUo=i(U),E6=n(U,"LI",{});var awe=s(E6);Mfe=n(awe,"STRONG",{});var art=s(Mfe);bUo=r(art,"layoutlm"),art.forEach(t),vUo=r(awe," \u2014 "),jD=n(awe,"A",{href:!0});var nrt=s(jD);TUo=r(nrt,"TFLayoutLMForMaskedLM"),nrt.forEach(t),FUo=r(awe," (LayoutLM model)"),awe.forEach(t),CUo=i(U),y6=n(U,"LI",{});var nwe=s(y6);Efe=n(nwe,"STRONG",{});var srt=s(Efe);MUo=r(srt,"lxmert"),srt.forEach(t),EUo=r(nwe," \u2014 "),ND=n(nwe,"A",{href:!0});var lrt=s(ND);yUo=r(lrt,"TFLxmertForPreTraining"),lrt.forEach(t),wUo=r(nwe," (LXMERT model)"),nwe.forEach(t),AUo=i(U),w6=n(U,"LI",{});var swe=s(w6);yfe=n(swe,"STRONG",{});var irt=s(yfe);LUo=r(irt,"mobilebert"),irt.forEach(t),BUo=r(swe," \u2014 "),DD=n(swe,"A",{href:!0});var drt=s(DD);kUo=r(drt,"TFMobileBertForPreTraining"),drt.forEach(t),xUo=r(swe," (MobileBERT model)"),swe.forEach(t),RUo=i(U),A6=n(U,"LI",{});var lwe=s(A6);wfe=n(lwe,"STRONG",{});var crt=s(wfe);SUo=r(crt,"mpnet"),crt.forEach(t),PUo=r(lwe," \u2014 "),qD=n(lwe,"A",{href:!0});var frt=s(qD);$Uo=r(frt,"TFMPNetForMaskedLM"),frt.forEach(t),IUo=r(lwe," (MPNet model)"),lwe.forEach(t),jUo=i(U),L6=n(U,"LI",{});var iwe=s(L6);Afe=n(iwe,"STRONG",{});var mrt=s(Afe);NUo=r(mrt,"openai-gpt"),mrt.forEach(t),DUo=r(iwe," \u2014 "),GD=n(iwe,"A",{href:!0});var grt=s(GD);qUo=r(grt,"TFOpenAIGPTLMHeadModel"),grt.forEach(t),GUo=r(iwe," (OpenAI GPT model)"),iwe.forEach(t),OUo=i(U),B6=n(U,"LI",{});var dwe=s(B6);Lfe=n(dwe,"STRONG",{});var hrt=s(Lfe);XUo=r(hrt,"roberta"),hrt.forEach(t),zUo=r(dwe," \u2014 "),OD=n(dwe,"A",{href:!0});var prt=s(OD);VUo=r(prt,"TFRobertaForMaskedLM"),prt.forEach(t),WUo=r(dwe," (RoBERTa model)"),dwe.forEach(t),QUo=i(U),k6=n(U,"LI",{});var cwe=s(k6);Bfe=n(cwe,"STRONG",{});var _rt=s(Bfe);HUo=r(_rt,"t5"),_rt.forEach(t),UUo=r(cwe," \u2014 "),XD=n(cwe,"A",{href:!0});var urt=s(XD);JUo=r(urt,"TFT5ForConditionalGeneration"),urt.forEach(t),YUo=r(cwe," (T5 model)"),cwe.forEach(t),KUo=i(U),x6=n(U,"LI",{});var fwe=s(x6);kfe=n(fwe,"STRONG",{});var brt=s(kfe);ZUo=r(brt,"tapas"),brt.forEach(t),eJo=r(fwe," \u2014 "),zD=n(fwe,"A",{href:!0});var vrt=s(zD);oJo=r(vrt,"TFTapasForMaskedLM"),vrt.forEach(t),rJo=r(fwe," (TAPAS model)"),fwe.forEach(t),tJo=i(U),R6=n(U,"LI",{});var mwe=s(R6);xfe=n(mwe,"STRONG",{});var Trt=s(xfe);aJo=r(Trt,"transfo-xl"),Trt.forEach(t),nJo=r(mwe," \u2014 "),VD=n(mwe,"A",{href:!0});var Frt=s(VD);sJo=r(Frt,"TFTransfoXLLMHeadModel"),Frt.forEach(t),lJo=r(mwe," (Transformer-XL model)"),mwe.forEach(t),iJo=i(U),S6=n(U,"LI",{});var gwe=s(S6);Rfe=n(gwe,"STRONG",{});var Crt=s(Rfe);dJo=r(Crt,"xlm"),Crt.forEach(t),cJo=r(gwe," \u2014 "),WD=n(gwe,"A",{href:!0});var Mrt=s(WD);fJo=r(Mrt,"TFXLMWithLMHeadModel"),Mrt.forEach(t),mJo=r(gwe," (XLM model)"),gwe.forEach(t),gJo=i(U),P6=n(U,"LI",{});var hwe=s(P6);Sfe=n(hwe,"STRONG",{});var Ert=s(Sfe);hJo=r(Ert,"xlm-roberta"),Ert.forEach(t),pJo=r(hwe," \u2014 "),QD=n(hwe,"A",{href:!0});var yrt=s(QD);_Jo=r(yrt,"TFXLMRobertaForMaskedLM"),yrt.forEach(t),uJo=r(hwe," (XLM-RoBERTa model)"),hwe.forEach(t),bJo=i(U),$6=n(U,"LI",{});var pwe=s($6);Pfe=n(pwe,"STRONG",{});var wrt=s(Pfe);vJo=r(wrt,"xlnet"),wrt.forEach(t),TJo=r(pwe," \u2014 "),HD=n(pwe,"A",{href:!0});var Art=s(HD);FJo=r(Art,"TFXLNetLMHeadModel"),Art.forEach(t),CJo=r(pwe," (XLNet model)"),pwe.forEach(t),U.forEach(t),MJo=i(fa),$fe=n(fa,"P",{});var Lrt=s($fe);EJo=r(Lrt,"Examples:"),Lrt.forEach(t),yJo=i(fa),m(Wy.$$.fragment,fa),fa.forEach(t),xl.forEach(t),w9e=i(d),lc=n(d,"H2",{class:!0});var $ke=s(lc);I6=n($ke,"A",{id:!0,class:!0,href:!0});var Brt=s(I6);Ife=n(Brt,"SPAN",{});var krt=s(Ife);m(Qy.$$.fragment,krt),krt.forEach(t),Brt.forEach(t),wJo=i($ke),jfe=n($ke,"SPAN",{});var xrt=s(jfe);AJo=r(xrt,"TFAutoModelForCausalLM"),xrt.forEach(t),$ke.forEach(t),A9e=i(d),pr=n(d,"DIV",{class:!0});var Sl=s(pr);m(Hy.$$.fragment,Sl),LJo=i(Sl),ic=n(Sl,"P",{});var Kz=s(ic);BJo=r(Kz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a causal language modeling head) when created
with the `),Nfe=n(Kz,"CODE",{});var Rrt=s(Nfe);kJo=r(Rrt,"from_pretrained()"),Rrt.forEach(t),xJo=r(Kz,"class method or the "),Dfe=n(Kz,"CODE",{});var Srt=s(Dfe);RJo=r(Srt,"from_config()"),Srt.forEach(t),SJo=r(Kz,`class
method.`),Kz.forEach(t),PJo=i(Sl),Uy=n(Sl,"P",{});var Ike=s(Uy);$Jo=r(Ike,"This class cannot be instantiated directly using "),qfe=n(Ike,"CODE",{});var Prt=s(qfe);IJo=r(Prt,"__init__()"),Prt.forEach(t),jJo=r(Ike," (throws an error)."),Ike.forEach(t),NJo=i(Sl),it=n(Sl,"DIV",{class:!0});var Pl=s(it);m(Jy.$$.fragment,Pl),DJo=i(Pl),Gfe=n(Pl,"P",{});var $rt=s(Gfe);qJo=r($rt,"Instantiates one of the model classes of the library (with a causal language modeling head) from a configuration."),$rt.forEach(t),GJo=i(Pl),dc=n(Pl,"P",{});var Zz=s(dc);OJo=r(Zz,`Note:
Loading a model from its configuration file does `),Ofe=n(Zz,"STRONG",{});var Irt=s(Ofe);XJo=r(Irt,"not"),Irt.forEach(t),zJo=r(Zz,` load the model weights. It only affects the
model\u2019s configuration. Use `),Xfe=n(Zz,"CODE",{});var jrt=s(Xfe);VJo=r(jrt,"from_pretrained()"),jrt.forEach(t),WJo=r(Zz,"to load the model weights."),Zz.forEach(t),QJo=i(Pl),zfe=n(Pl,"P",{});var Nrt=s(zfe);HJo=r(Nrt,"Examples:"),Nrt.forEach(t),UJo=i(Pl),m(Yy.$$.fragment,Pl),Pl.forEach(t),JJo=i(Sl),po=n(Sl,"DIV",{class:!0});var ma=s(po);m(Ky.$$.fragment,ma),YJo=i(ma),Vfe=n(ma,"P",{});var Drt=s(Vfe);KJo=r(Drt,"Instantiate one of the model classes of the library (with a causal language modeling head) from a pretrained model."),Drt.forEach(t),ZJo=i(ma),cn=n(ma,"P",{});var $4=s(cn);eYo=r($4,"The model class to instantiate is selected based on the "),Wfe=n($4,"CODE",{});var qrt=s(Wfe);oYo=r(qrt,"model_type"),qrt.forEach(t),rYo=r($4,` property of the config object (either
passed as an argument or loaded from `),Qfe=n($4,"CODE",{});var Grt=s(Qfe);tYo=r(Grt,"pretrained_model_name_or_path"),Grt.forEach(t),aYo=r($4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Hfe=n($4,"CODE",{});var Ort=s(Hfe);nYo=r(Ort,"pretrained_model_name_or_path"),Ort.forEach(t),sYo=r($4,":"),$4.forEach(t),lYo=i(ma),he=n(ma,"UL",{});var Me=s(he);j6=n(Me,"LI",{});var _we=s(j6);Ufe=n(_we,"STRONG",{});var Xrt=s(Ufe);iYo=r(Xrt,"bert"),Xrt.forEach(t),dYo=r(_we," \u2014 "),UD=n(_we,"A",{href:!0});var zrt=s(UD);cYo=r(zrt,"TFBertLMHeadModel"),zrt.forEach(t),fYo=r(_we," (BERT model)"),_we.forEach(t),mYo=i(Me),N6=n(Me,"LI",{});var uwe=s(N6);Jfe=n(uwe,"STRONG",{});var Vrt=s(Jfe);gYo=r(Vrt,"ctrl"),Vrt.forEach(t),hYo=r(uwe," \u2014 "),JD=n(uwe,"A",{href:!0});var Wrt=s(JD);pYo=r(Wrt,"TFCTRLLMHeadModel"),Wrt.forEach(t),_Yo=r(uwe," (CTRL model)"),uwe.forEach(t),uYo=i(Me),D6=n(Me,"LI",{});var bwe=s(D6);Yfe=n(bwe,"STRONG",{});var Qrt=s(Yfe);bYo=r(Qrt,"gpt2"),Qrt.forEach(t),vYo=r(bwe," \u2014 "),YD=n(bwe,"A",{href:!0});var Hrt=s(YD);TYo=r(Hrt,"TFGPT2LMHeadModel"),Hrt.forEach(t),FYo=r(bwe," (OpenAI GPT-2 model)"),bwe.forEach(t),CYo=i(Me),q6=n(Me,"LI",{});var vwe=s(q6);Kfe=n(vwe,"STRONG",{});var Urt=s(Kfe);MYo=r(Urt,"openai-gpt"),Urt.forEach(t),EYo=r(vwe," \u2014 "),KD=n(vwe,"A",{href:!0});var Jrt=s(KD);yYo=r(Jrt,"TFOpenAIGPTLMHeadModel"),Jrt.forEach(t),wYo=r(vwe," (OpenAI GPT model)"),vwe.forEach(t),AYo=i(Me),G6=n(Me,"LI",{});var Twe=s(G6);Zfe=n(Twe,"STRONG",{});var Yrt=s(Zfe);LYo=r(Yrt,"rembert"),Yrt.forEach(t),BYo=r(Twe," \u2014 "),ZD=n(Twe,"A",{href:!0});var Krt=s(ZD);kYo=r(Krt,"TFRemBertForCausalLM"),Krt.forEach(t),xYo=r(Twe," (RemBERT model)"),Twe.forEach(t),RYo=i(Me),O6=n(Me,"LI",{});var Fwe=s(O6);eme=n(Fwe,"STRONG",{});var Zrt=s(eme);SYo=r(Zrt,"roberta"),Zrt.forEach(t),PYo=r(Fwe," \u2014 "),eq=n(Fwe,"A",{href:!0});var ett=s(eq);$Yo=r(ett,"TFRobertaForCausalLM"),ett.forEach(t),IYo=r(Fwe," (RoBERTa model)"),Fwe.forEach(t),jYo=i(Me),X6=n(Me,"LI",{});var Cwe=s(X6);ome=n(Cwe,"STRONG",{});var ott=s(ome);NYo=r(ott,"roformer"),ott.forEach(t),DYo=r(Cwe," \u2014 "),oq=n(Cwe,"A",{href:!0});var rtt=s(oq);qYo=r(rtt,"TFRoFormerForCausalLM"),rtt.forEach(t),GYo=r(Cwe," (RoFormer model)"),Cwe.forEach(t),OYo=i(Me),z6=n(Me,"LI",{});var Mwe=s(z6);rme=n(Mwe,"STRONG",{});var ttt=s(rme);XYo=r(ttt,"transfo-xl"),ttt.forEach(t),zYo=r(Mwe," \u2014 "),rq=n(Mwe,"A",{href:!0});var att=s(rq);VYo=r(att,"TFTransfoXLLMHeadModel"),att.forEach(t),WYo=r(Mwe," (Transformer-XL model)"),Mwe.forEach(t),QYo=i(Me),V6=n(Me,"LI",{});var Ewe=s(V6);tme=n(Ewe,"STRONG",{});var ntt=s(tme);HYo=r(ntt,"xlm"),ntt.forEach(t),UYo=r(Ewe," \u2014 "),tq=n(Ewe,"A",{href:!0});var stt=s(tq);JYo=r(stt,"TFXLMWithLMHeadModel"),stt.forEach(t),YYo=r(Ewe," (XLM model)"),Ewe.forEach(t),KYo=i(Me),W6=n(Me,"LI",{});var ywe=s(W6);ame=n(ywe,"STRONG",{});var ltt=s(ame);ZYo=r(ltt,"xlnet"),ltt.forEach(t),eKo=r(ywe," \u2014 "),aq=n(ywe,"A",{href:!0});var itt=s(aq);oKo=r(itt,"TFXLNetLMHeadModel"),itt.forEach(t),rKo=r(ywe," (XLNet model)"),ywe.forEach(t),Me.forEach(t),tKo=i(ma),nme=n(ma,"P",{});var dtt=s(nme);aKo=r(dtt,"Examples:"),dtt.forEach(t),nKo=i(ma),m(Zy.$$.fragment,ma),ma.forEach(t),Sl.forEach(t),L9e=i(d),cc=n(d,"H2",{class:!0});var jke=s(cc);Q6=n(jke,"A",{id:!0,class:!0,href:!0});var ctt=s(Q6);sme=n(ctt,"SPAN",{});var ftt=s(sme);m(ew.$$.fragment,ftt),ftt.forEach(t),ctt.forEach(t),sKo=i(jke),lme=n(jke,"SPAN",{});var mtt=s(lme);lKo=r(mtt,"TFAutoModelForImageClassification"),mtt.forEach(t),jke.forEach(t),B9e=i(d),_r=n(d,"DIV",{class:!0});var $l=s(_r);m(ow.$$.fragment,$l),iKo=i($l),fc=n($l,"P",{});var eV=s(fc);dKo=r(eV,`This is a generic model class that will be instantiated as one of the model classes of the library (with a image classification head) when created
with the `),ime=n(eV,"CODE",{});var gtt=s(ime);cKo=r(gtt,"from_pretrained()"),gtt.forEach(t),fKo=r(eV,"class method or the "),dme=n(eV,"CODE",{});var htt=s(dme);mKo=r(htt,"from_config()"),htt.forEach(t),gKo=r(eV,`class
method.`),eV.forEach(t),hKo=i($l),rw=n($l,"P",{});var Nke=s(rw);pKo=r(Nke,"This class cannot be instantiated directly using "),cme=n(Nke,"CODE",{});var ptt=s(cme);_Ko=r(ptt,"__init__()"),ptt.forEach(t),uKo=r(Nke," (throws an error)."),Nke.forEach(t),bKo=i($l),dt=n($l,"DIV",{class:!0});var Il=s(dt);m(tw.$$.fragment,Il),vKo=i(Il),fme=n(Il,"P",{});var _tt=s(fme);TKo=r(_tt,"Instantiates one of the model classes of the library (with a image classification head) from a configuration."),_tt.forEach(t),FKo=i(Il),mc=n(Il,"P",{});var oV=s(mc);CKo=r(oV,`Note:
Loading a model from its configuration file does `),mme=n(oV,"STRONG",{});var utt=s(mme);MKo=r(utt,"not"),utt.forEach(t),EKo=r(oV,` load the model weights. It only affects the
model\u2019s configuration. Use `),gme=n(oV,"CODE",{});var btt=s(gme);yKo=r(btt,"from_pretrained()"),btt.forEach(t),wKo=r(oV,"to load the model weights."),oV.forEach(t),AKo=i(Il),hme=n(Il,"P",{});var vtt=s(hme);LKo=r(vtt,"Examples:"),vtt.forEach(t),BKo=i(Il),m(aw.$$.fragment,Il),Il.forEach(t),kKo=i($l),_o=n($l,"DIV",{class:!0});var ga=s(_o);m(nw.$$.fragment,ga),xKo=i(ga),pme=n(ga,"P",{});var Ttt=s(pme);RKo=r(Ttt,"Instantiate one of the model classes of the library (with a image classification head) from a pretrained model."),Ttt.forEach(t),SKo=i(ga),fn=n(ga,"P",{});var I4=s(fn);PKo=r(I4,"The model class to instantiate is selected based on the "),_me=n(I4,"CODE",{});var Ftt=s(_me);$Ko=r(Ftt,"model_type"),Ftt.forEach(t),IKo=r(I4,` property of the config object (either
passed as an argument or loaded from `),ume=n(I4,"CODE",{});var Ctt=s(ume);jKo=r(Ctt,"pretrained_model_name_or_path"),Ctt.forEach(t),NKo=r(I4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),bme=n(I4,"CODE",{});var Mtt=s(bme);DKo=r(Mtt,"pretrained_model_name_or_path"),Mtt.forEach(t),qKo=r(I4,":"),I4.forEach(t),GKo=i(ga),sw=n(ga,"UL",{});var Dke=s(sw);H6=n(Dke,"LI",{});var wwe=s(H6);vme=n(wwe,"STRONG",{});var Ett=s(vme);OKo=r(Ett,"convnext"),Ett.forEach(t),XKo=r(wwe," \u2014 "),nq=n(wwe,"A",{href:!0});var ytt=s(nq);zKo=r(ytt,"TFConvNextForImageClassification"),ytt.forEach(t),VKo=r(wwe," (ConvNext model)"),wwe.forEach(t),WKo=i(Dke),U6=n(Dke,"LI",{});var Awe=s(U6);Tme=n(Awe,"STRONG",{});var wtt=s(Tme);QKo=r(wtt,"vit"),wtt.forEach(t),HKo=r(Awe," \u2014 "),sq=n(Awe,"A",{href:!0});var Att=s(sq);UKo=r(Att,"TFViTForImageClassification"),Att.forEach(t),JKo=r(Awe," (ViT model)"),Awe.forEach(t),Dke.forEach(t),YKo=i(ga),Fme=n(ga,"P",{});var Ltt=s(Fme);KKo=r(Ltt,"Examples:"),Ltt.forEach(t),ZKo=i(ga),m(lw.$$.fragment,ga),ga.forEach(t),$l.forEach(t),k9e=i(d),gc=n(d,"H2",{class:!0});var qke=s(gc);J6=n(qke,"A",{id:!0,class:!0,href:!0});var Btt=s(J6);Cme=n(Btt,"SPAN",{});var ktt=s(Cme);m(iw.$$.fragment,ktt),ktt.forEach(t),Btt.forEach(t),eZo=i(qke),Mme=n(qke,"SPAN",{});var xtt=s(Mme);oZo=r(xtt,"TFAutoModelForMaskedLM"),xtt.forEach(t),qke.forEach(t),x9e=i(d),ur=n(d,"DIV",{class:!0});var jl=s(ur);m(dw.$$.fragment,jl),rZo=i(jl),hc=n(jl,"P",{});var rV=s(hc);tZo=r(rV,`This is a generic model class that will be instantiated as one of the model classes of the library (with a masked language modeling head) when created
with the `),Eme=n(rV,"CODE",{});var Rtt=s(Eme);aZo=r(Rtt,"from_pretrained()"),Rtt.forEach(t),nZo=r(rV,"class method or the "),yme=n(rV,"CODE",{});var Stt=s(yme);sZo=r(Stt,"from_config()"),Stt.forEach(t),lZo=r(rV,`class
method.`),rV.forEach(t),iZo=i(jl),cw=n(jl,"P",{});var Gke=s(cw);dZo=r(Gke,"This class cannot be instantiated directly using "),wme=n(Gke,"CODE",{});var Ptt=s(wme);cZo=r(Ptt,"__init__()"),Ptt.forEach(t),fZo=r(Gke," (throws an error)."),Gke.forEach(t),mZo=i(jl),ct=n(jl,"DIV",{class:!0});var Nl=s(ct);m(fw.$$.fragment,Nl),gZo=i(Nl),Ame=n(Nl,"P",{});var $tt=s(Ame);hZo=r($tt,"Instantiates one of the model classes of the library (with a masked language modeling head) from a configuration."),$tt.forEach(t),pZo=i(Nl),pc=n(Nl,"P",{});var tV=s(pc);_Zo=r(tV,`Note:
Loading a model from its configuration file does `),Lme=n(tV,"STRONG",{});var Itt=s(Lme);uZo=r(Itt,"not"),Itt.forEach(t),bZo=r(tV,` load the model weights. It only affects the
model\u2019s configuration. Use `),Bme=n(tV,"CODE",{});var jtt=s(Bme);vZo=r(jtt,"from_pretrained()"),jtt.forEach(t),TZo=r(tV,"to load the model weights."),tV.forEach(t),FZo=i(Nl),kme=n(Nl,"P",{});var Ntt=s(kme);CZo=r(Ntt,"Examples:"),Ntt.forEach(t),MZo=i(Nl),m(mw.$$.fragment,Nl),Nl.forEach(t),EZo=i(jl),uo=n(jl,"DIV",{class:!0});var ha=s(uo);m(gw.$$.fragment,ha),yZo=i(ha),xme=n(ha,"P",{});var Dtt=s(xme);wZo=r(Dtt,"Instantiate one of the model classes of the library (with a masked language modeling head) from a pretrained model."),Dtt.forEach(t),AZo=i(ha),mn=n(ha,"P",{});var j4=s(mn);LZo=r(j4,"The model class to instantiate is selected based on the "),Rme=n(j4,"CODE",{});var qtt=s(Rme);BZo=r(qtt,"model_type"),qtt.forEach(t),kZo=r(j4,` property of the config object (either
passed as an argument or loaded from `),Sme=n(j4,"CODE",{});var Gtt=s(Sme);xZo=r(Gtt,"pretrained_model_name_or_path"),Gtt.forEach(t),RZo=r(j4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Pme=n(j4,"CODE",{});var Ott=s(Pme);SZo=r(Ott,"pretrained_model_name_or_path"),Ott.forEach(t),PZo=r(j4,":"),j4.forEach(t),$Zo=i(ha),Y=n(ha,"UL",{});var ee=s(Y);Y6=n(ee,"LI",{});var Lwe=s(Y6);$me=n(Lwe,"STRONG",{});var Xtt=s($me);IZo=r(Xtt,"albert"),Xtt.forEach(t),jZo=r(Lwe," \u2014 "),lq=n(Lwe,"A",{href:!0});var ztt=s(lq);NZo=r(ztt,"TFAlbertForMaskedLM"),ztt.forEach(t),DZo=r(Lwe," (ALBERT model)"),Lwe.forEach(t),qZo=i(ee),K6=n(ee,"LI",{});var Bwe=s(K6);Ime=n(Bwe,"STRONG",{});var Vtt=s(Ime);GZo=r(Vtt,"bert"),Vtt.forEach(t),OZo=r(Bwe," \u2014 "),iq=n(Bwe,"A",{href:!0});var Wtt=s(iq);XZo=r(Wtt,"TFBertForMaskedLM"),Wtt.forEach(t),zZo=r(Bwe," (BERT model)"),Bwe.forEach(t),VZo=i(ee),Z6=n(ee,"LI",{});var kwe=s(Z6);jme=n(kwe,"STRONG",{});var Qtt=s(jme);WZo=r(Qtt,"camembert"),Qtt.forEach(t),QZo=r(kwe," \u2014 "),dq=n(kwe,"A",{href:!0});var Htt=s(dq);HZo=r(Htt,"TFCamembertForMaskedLM"),Htt.forEach(t),UZo=r(kwe," (CamemBERT model)"),kwe.forEach(t),JZo=i(ee),eT=n(ee,"LI",{});var xwe=s(eT);Nme=n(xwe,"STRONG",{});var Utt=s(Nme);YZo=r(Utt,"convbert"),Utt.forEach(t),KZo=r(xwe," \u2014 "),cq=n(xwe,"A",{href:!0});var Jtt=s(cq);ZZo=r(Jtt,"TFConvBertForMaskedLM"),Jtt.forEach(t),eer=r(xwe," (ConvBERT model)"),xwe.forEach(t),oer=i(ee),oT=n(ee,"LI",{});var Rwe=s(oT);Dme=n(Rwe,"STRONG",{});var Ytt=s(Dme);rer=r(Ytt,"deberta"),Ytt.forEach(t),ter=r(Rwe," \u2014 "),fq=n(Rwe,"A",{href:!0});var Ktt=s(fq);aer=r(Ktt,"TFDebertaForMaskedLM"),Ktt.forEach(t),ner=r(Rwe," (DeBERTa model)"),Rwe.forEach(t),ser=i(ee),rT=n(ee,"LI",{});var Swe=s(rT);qme=n(Swe,"STRONG",{});var Ztt=s(qme);ler=r(Ztt,"deberta-v2"),Ztt.forEach(t),ier=r(Swe," \u2014 "),mq=n(Swe,"A",{href:!0});var eat=s(mq);der=r(eat,"TFDebertaV2ForMaskedLM"),eat.forEach(t),cer=r(Swe," (DeBERTa-v2 model)"),Swe.forEach(t),fer=i(ee),tT=n(ee,"LI",{});var Pwe=s(tT);Gme=n(Pwe,"STRONG",{});var oat=s(Gme);mer=r(oat,"distilbert"),oat.forEach(t),ger=r(Pwe," \u2014 "),gq=n(Pwe,"A",{href:!0});var rat=s(gq);her=r(rat,"TFDistilBertForMaskedLM"),rat.forEach(t),per=r(Pwe," (DistilBERT model)"),Pwe.forEach(t),_er=i(ee),aT=n(ee,"LI",{});var $we=s(aT);Ome=n($we,"STRONG",{});var tat=s(Ome);uer=r(tat,"electra"),tat.forEach(t),ber=r($we," \u2014 "),hq=n($we,"A",{href:!0});var aat=s(hq);ver=r(aat,"TFElectraForMaskedLM"),aat.forEach(t),Ter=r($we," (ELECTRA model)"),$we.forEach(t),Fer=i(ee),nT=n(ee,"LI",{});var Iwe=s(nT);Xme=n(Iwe,"STRONG",{});var nat=s(Xme);Cer=r(nat,"flaubert"),nat.forEach(t),Mer=r(Iwe," \u2014 "),pq=n(Iwe,"A",{href:!0});var sat=s(pq);Eer=r(sat,"TFFlaubertWithLMHeadModel"),sat.forEach(t),yer=r(Iwe," (FlauBERT model)"),Iwe.forEach(t),wer=i(ee),sT=n(ee,"LI",{});var jwe=s(sT);zme=n(jwe,"STRONG",{});var lat=s(zme);Aer=r(lat,"funnel"),lat.forEach(t),Ler=r(jwe," \u2014 "),_q=n(jwe,"A",{href:!0});var iat=s(_q);Ber=r(iat,"TFFunnelForMaskedLM"),iat.forEach(t),ker=r(jwe," (Funnel Transformer model)"),jwe.forEach(t),xer=i(ee),lT=n(ee,"LI",{});var Nwe=s(lT);Vme=n(Nwe,"STRONG",{});var dat=s(Vme);Rer=r(dat,"layoutlm"),dat.forEach(t),Ser=r(Nwe," \u2014 "),uq=n(Nwe,"A",{href:!0});var cat=s(uq);Per=r(cat,"TFLayoutLMForMaskedLM"),cat.forEach(t),$er=r(Nwe," (LayoutLM model)"),Nwe.forEach(t),Ier=i(ee),iT=n(ee,"LI",{});var Dwe=s(iT);Wme=n(Dwe,"STRONG",{});var fat=s(Wme);jer=r(fat,"longformer"),fat.forEach(t),Ner=r(Dwe," \u2014 "),bq=n(Dwe,"A",{href:!0});var mat=s(bq);Der=r(mat,"TFLongformerForMaskedLM"),mat.forEach(t),qer=r(Dwe," (Longformer model)"),Dwe.forEach(t),Ger=i(ee),dT=n(ee,"LI",{});var qwe=s(dT);Qme=n(qwe,"STRONG",{});var gat=s(Qme);Oer=r(gat,"mobilebert"),gat.forEach(t),Xer=r(qwe," \u2014 "),vq=n(qwe,"A",{href:!0});var hat=s(vq);zer=r(hat,"TFMobileBertForMaskedLM"),hat.forEach(t),Ver=r(qwe," (MobileBERT model)"),qwe.forEach(t),Wer=i(ee),cT=n(ee,"LI",{});var Gwe=s(cT);Hme=n(Gwe,"STRONG",{});var pat=s(Hme);Qer=r(pat,"mpnet"),pat.forEach(t),Her=r(Gwe," \u2014 "),Tq=n(Gwe,"A",{href:!0});var _at=s(Tq);Uer=r(_at,"TFMPNetForMaskedLM"),_at.forEach(t),Jer=r(Gwe," (MPNet model)"),Gwe.forEach(t),Yer=i(ee),fT=n(ee,"LI",{});var Owe=s(fT);Ume=n(Owe,"STRONG",{});var uat=s(Ume);Ker=r(uat,"rembert"),uat.forEach(t),Zer=r(Owe," \u2014 "),Fq=n(Owe,"A",{href:!0});var bat=s(Fq);eor=r(bat,"TFRemBertForMaskedLM"),bat.forEach(t),oor=r(Owe," (RemBERT model)"),Owe.forEach(t),ror=i(ee),mT=n(ee,"LI",{});var Xwe=s(mT);Jme=n(Xwe,"STRONG",{});var vat=s(Jme);tor=r(vat,"roberta"),vat.forEach(t),aor=r(Xwe," \u2014 "),Cq=n(Xwe,"A",{href:!0});var Tat=s(Cq);nor=r(Tat,"TFRobertaForMaskedLM"),Tat.forEach(t),sor=r(Xwe," (RoBERTa model)"),Xwe.forEach(t),lor=i(ee),gT=n(ee,"LI",{});var zwe=s(gT);Yme=n(zwe,"STRONG",{});var Fat=s(Yme);ior=r(Fat,"roformer"),Fat.forEach(t),dor=r(zwe," \u2014 "),Mq=n(zwe,"A",{href:!0});var Cat=s(Mq);cor=r(Cat,"TFRoFormerForMaskedLM"),Cat.forEach(t),mor=r(zwe," (RoFormer model)"),zwe.forEach(t),gor=i(ee),hT=n(ee,"LI",{});var Vwe=s(hT);Kme=n(Vwe,"STRONG",{});var Mat=s(Kme);hor=r(Mat,"tapas"),Mat.forEach(t),por=r(Vwe," \u2014 "),Eq=n(Vwe,"A",{href:!0});var Eat=s(Eq);_or=r(Eat,"TFTapasForMaskedLM"),Eat.forEach(t),uor=r(Vwe," (TAPAS model)"),Vwe.forEach(t),bor=i(ee),pT=n(ee,"LI",{});var Wwe=s(pT);Zme=n(Wwe,"STRONG",{});var yat=s(Zme);vor=r(yat,"xlm"),yat.forEach(t),Tor=r(Wwe," \u2014 "),yq=n(Wwe,"A",{href:!0});var wat=s(yq);For=r(wat,"TFXLMWithLMHeadModel"),wat.forEach(t),Cor=r(Wwe," (XLM model)"),Wwe.forEach(t),Mor=i(ee),_T=n(ee,"LI",{});var Qwe=s(_T);ege=n(Qwe,"STRONG",{});var Aat=s(ege);Eor=r(Aat,"xlm-roberta"),Aat.forEach(t),yor=r(Qwe," \u2014 "),wq=n(Qwe,"A",{href:!0});var Lat=s(wq);wor=r(Lat,"TFXLMRobertaForMaskedLM"),Lat.forEach(t),Aor=r(Qwe," (XLM-RoBERTa model)"),Qwe.forEach(t),ee.forEach(t),Lor=i(ha),oge=n(ha,"P",{});var Bat=s(oge);Bor=r(Bat,"Examples:"),Bat.forEach(t),kor=i(ha),m(hw.$$.fragment,ha),ha.forEach(t),jl.forEach(t),R9e=i(d),_c=n(d,"H2",{class:!0});var Oke=s(_c);uT=n(Oke,"A",{id:!0,class:!0,href:!0});var kat=s(uT);rge=n(kat,"SPAN",{});var xat=s(rge);m(pw.$$.fragment,xat),xat.forEach(t),kat.forEach(t),xor=i(Oke),tge=n(Oke,"SPAN",{});var Rat=s(tge);Ror=r(Rat,"TFAutoModelForSeq2SeqLM"),Rat.forEach(t),Oke.forEach(t),S9e=i(d),br=n(d,"DIV",{class:!0});var Dl=s(br);m(_w.$$.fragment,Dl),Sor=i(Dl),uc=n(Dl,"P",{});var aV=s(uc);Por=r(aV,`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence language modeling head) when created
with the `),age=n(aV,"CODE",{});var Sat=s(age);$or=r(Sat,"from_pretrained()"),Sat.forEach(t),Ior=r(aV,"class method or the "),nge=n(aV,"CODE",{});var Pat=s(nge);jor=r(Pat,"from_config()"),Pat.forEach(t),Nor=r(aV,`class
method.`),aV.forEach(t),Dor=i(Dl),uw=n(Dl,"P",{});var Xke=s(uw);qor=r(Xke,"This class cannot be instantiated directly using "),sge=n(Xke,"CODE",{});var $at=s(sge);Gor=r($at,"__init__()"),$at.forEach(t),Oor=r(Xke," (throws an error)."),Xke.forEach(t),Xor=i(Dl),ft=n(Dl,"DIV",{class:!0});var ql=s(ft);m(bw.$$.fragment,ql),zor=i(ql),lge=n(ql,"P",{});var Iat=s(lge);Vor=r(Iat,"Instantiates one of the model classes of the library (with a sequence-to-sequence language modeling head) from a configuration."),Iat.forEach(t),Wor=i(ql),bc=n(ql,"P",{});var nV=s(bc);Qor=r(nV,`Note:
Loading a model from its configuration file does `),ige=n(nV,"STRONG",{});var jat=s(ige);Hor=r(jat,"not"),jat.forEach(t),Uor=r(nV,` load the model weights. It only affects the
model\u2019s configuration. Use `),dge=n(nV,"CODE",{});var Nat=s(dge);Jor=r(Nat,"from_pretrained()"),Nat.forEach(t),Yor=r(nV,"to load the model weights."),nV.forEach(t),Kor=i(ql),cge=n(ql,"P",{});var Dat=s(cge);Zor=r(Dat,"Examples:"),Dat.forEach(t),err=i(ql),m(vw.$$.fragment,ql),ql.forEach(t),orr=i(Dl),bo=n(Dl,"DIV",{class:!0});var pa=s(bo);m(Tw.$$.fragment,pa),rrr=i(pa),fge=n(pa,"P",{});var qat=s(fge);trr=r(qat,"Instantiate one of the model classes of the library (with a sequence-to-sequence language modeling head) from a pretrained model."),qat.forEach(t),arr=i(pa),gn=n(pa,"P",{});var N4=s(gn);nrr=r(N4,"The model class to instantiate is selected based on the "),mge=n(N4,"CODE",{});var Gat=s(mge);srr=r(Gat,"model_type"),Gat.forEach(t),lrr=r(N4,` property of the config object (either
passed as an argument or loaded from `),gge=n(N4,"CODE",{});var Oat=s(gge);irr=r(Oat,"pretrained_model_name_or_path"),Oat.forEach(t),drr=r(N4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),hge=n(N4,"CODE",{});var Xat=s(hge);crr=r(Xat,"pretrained_model_name_or_path"),Xat.forEach(t),frr=r(N4,":"),N4.forEach(t),mrr=i(pa),pe=n(pa,"UL",{});var Ee=s(pe);bT=n(Ee,"LI",{});var Hwe=s(bT);pge=n(Hwe,"STRONG",{});var zat=s(pge);grr=r(zat,"bart"),zat.forEach(t),hrr=r(Hwe," \u2014 "),Aq=n(Hwe,"A",{href:!0});var Vat=s(Aq);prr=r(Vat,"TFBartForConditionalGeneration"),Vat.forEach(t),_rr=r(Hwe," (BART model)"),Hwe.forEach(t),urr=i(Ee),vT=n(Ee,"LI",{});var Uwe=s(vT);_ge=n(Uwe,"STRONG",{});var Wat=s(_ge);brr=r(Wat,"blenderbot"),Wat.forEach(t),vrr=r(Uwe," \u2014 "),Lq=n(Uwe,"A",{href:!0});var Qat=s(Lq);Trr=r(Qat,"TFBlenderbotForConditionalGeneration"),Qat.forEach(t),Frr=r(Uwe," (Blenderbot model)"),Uwe.forEach(t),Crr=i(Ee),TT=n(Ee,"LI",{});var Jwe=s(TT);uge=n(Jwe,"STRONG",{});var Hat=s(uge);Mrr=r(Hat,"blenderbot-small"),Hat.forEach(t),Err=r(Jwe," \u2014 "),Bq=n(Jwe,"A",{href:!0});var Uat=s(Bq);yrr=r(Uat,"TFBlenderbotSmallForConditionalGeneration"),Uat.forEach(t),wrr=r(Jwe," (BlenderbotSmall model)"),Jwe.forEach(t),Arr=i(Ee),FT=n(Ee,"LI",{});var Ywe=s(FT);bge=n(Ywe,"STRONG",{});var Jat=s(bge);Lrr=r(Jat,"encoder-decoder"),Jat.forEach(t),Brr=r(Ywe," \u2014 "),kq=n(Ywe,"A",{href:!0});var Yat=s(kq);krr=r(Yat,"TFEncoderDecoderModel"),Yat.forEach(t),xrr=r(Ywe," (Encoder decoder model)"),Ywe.forEach(t),Rrr=i(Ee),CT=n(Ee,"LI",{});var Kwe=s(CT);vge=n(Kwe,"STRONG",{});var Kat=s(vge);Srr=r(Kat,"led"),Kat.forEach(t),Prr=r(Kwe," \u2014 "),xq=n(Kwe,"A",{href:!0});var Zat=s(xq);$rr=r(Zat,"TFLEDForConditionalGeneration"),Zat.forEach(t),Irr=r(Kwe," (LED model)"),Kwe.forEach(t),jrr=i(Ee),MT=n(Ee,"LI",{});var Zwe=s(MT);Tge=n(Zwe,"STRONG",{});var ent=s(Tge);Nrr=r(ent,"marian"),ent.forEach(t),Drr=r(Zwe," \u2014 "),Rq=n(Zwe,"A",{href:!0});var ont=s(Rq);qrr=r(ont,"TFMarianMTModel"),ont.forEach(t),Grr=r(Zwe," (Marian model)"),Zwe.forEach(t),Orr=i(Ee),ET=n(Ee,"LI",{});var eAe=s(ET);Fge=n(eAe,"STRONG",{});var rnt=s(Fge);Xrr=r(rnt,"mbart"),rnt.forEach(t),zrr=r(eAe," \u2014 "),Sq=n(eAe,"A",{href:!0});var tnt=s(Sq);Vrr=r(tnt,"TFMBartForConditionalGeneration"),tnt.forEach(t),Wrr=r(eAe," (mBART model)"),eAe.forEach(t),Qrr=i(Ee),yT=n(Ee,"LI",{});var oAe=s(yT);Cge=n(oAe,"STRONG",{});var ant=s(Cge);Hrr=r(ant,"mt5"),ant.forEach(t),Urr=r(oAe," \u2014 "),Pq=n(oAe,"A",{href:!0});var nnt=s(Pq);Jrr=r(nnt,"TFMT5ForConditionalGeneration"),nnt.forEach(t),Yrr=r(oAe," (mT5 model)"),oAe.forEach(t),Krr=i(Ee),wT=n(Ee,"LI",{});var rAe=s(wT);Mge=n(rAe,"STRONG",{});var snt=s(Mge);Zrr=r(snt,"pegasus"),snt.forEach(t),etr=r(rAe," \u2014 "),$q=n(rAe,"A",{href:!0});var lnt=s($q);otr=r(lnt,"TFPegasusForConditionalGeneration"),lnt.forEach(t),rtr=r(rAe," (Pegasus model)"),rAe.forEach(t),ttr=i(Ee),AT=n(Ee,"LI",{});var tAe=s(AT);Ege=n(tAe,"STRONG",{});var int=s(Ege);atr=r(int,"t5"),int.forEach(t),ntr=r(tAe," \u2014 "),Iq=n(tAe,"A",{href:!0});var dnt=s(Iq);str=r(dnt,"TFT5ForConditionalGeneration"),dnt.forEach(t),ltr=r(tAe," (T5 model)"),tAe.forEach(t),Ee.forEach(t),itr=i(pa),yge=n(pa,"P",{});var cnt=s(yge);dtr=r(cnt,"Examples:"),cnt.forEach(t),ctr=i(pa),m(Fw.$$.fragment,pa),pa.forEach(t),Dl.forEach(t),P9e=i(d),vc=n(d,"H2",{class:!0});var zke=s(vc);LT=n(zke,"A",{id:!0,class:!0,href:!0});var fnt=s(LT);wge=n(fnt,"SPAN",{});var mnt=s(wge);m(Cw.$$.fragment,mnt),mnt.forEach(t),fnt.forEach(t),ftr=i(zke),Age=n(zke,"SPAN",{});var gnt=s(Age);mtr=r(gnt,"TFAutoModelForSequenceClassification"),gnt.forEach(t),zke.forEach(t),$9e=i(d),vr=n(d,"DIV",{class:!0});var Gl=s(vr);m(Mw.$$.fragment,Gl),gtr=i(Gl),Tc=n(Gl,"P",{});var sV=s(Tc);htr=r(sV,`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence classification head) when created
with the `),Lge=n(sV,"CODE",{});var hnt=s(Lge);ptr=r(hnt,"from_pretrained()"),hnt.forEach(t),_tr=r(sV,"class method or the "),Bge=n(sV,"CODE",{});var pnt=s(Bge);utr=r(pnt,"from_config()"),pnt.forEach(t),btr=r(sV,`class
method.`),sV.forEach(t),vtr=i(Gl),Ew=n(Gl,"P",{});var Vke=s(Ew);Ttr=r(Vke,"This class cannot be instantiated directly using "),kge=n(Vke,"CODE",{});var _nt=s(kge);Ftr=r(_nt,"__init__()"),_nt.forEach(t),Ctr=r(Vke," (throws an error)."),Vke.forEach(t),Mtr=i(Gl),mt=n(Gl,"DIV",{class:!0});var Ol=s(mt);m(yw.$$.fragment,Ol),Etr=i(Ol),xge=n(Ol,"P",{});var unt=s(xge);ytr=r(unt,"Instantiates one of the model classes of the library (with a sequence classification head) from a configuration."),unt.forEach(t),wtr=i(Ol),Fc=n(Ol,"P",{});var lV=s(Fc);Atr=r(lV,`Note:
Loading a model from its configuration file does `),Rge=n(lV,"STRONG",{});var bnt=s(Rge);Ltr=r(bnt,"not"),bnt.forEach(t),Btr=r(lV,` load the model weights. It only affects the
model\u2019s configuration. Use `),Sge=n(lV,"CODE",{});var vnt=s(Sge);ktr=r(vnt,"from_pretrained()"),vnt.forEach(t),xtr=r(lV,"to load the model weights."),lV.forEach(t),Rtr=i(Ol),Pge=n(Ol,"P",{});var Tnt=s(Pge);Str=r(Tnt,"Examples:"),Tnt.forEach(t),Ptr=i(Ol),m(ww.$$.fragment,Ol),Ol.forEach(t),$tr=i(Gl),vo=n(Gl,"DIV",{class:!0});var _a=s(vo);m(Aw.$$.fragment,_a),Itr=i(_a),$ge=n(_a,"P",{});var Fnt=s($ge);jtr=r(Fnt,"Instantiate one of the model classes of the library (with a sequence classification head) from a pretrained model."),Fnt.forEach(t),Ntr=i(_a),hn=n(_a,"P",{});var D4=s(hn);Dtr=r(D4,"The model class to instantiate is selected based on the "),Ige=n(D4,"CODE",{});var Cnt=s(Ige);qtr=r(Cnt,"model_type"),Cnt.forEach(t),Gtr=r(D4,` property of the config object (either
passed as an argument or loaded from `),jge=n(D4,"CODE",{});var Mnt=s(jge);Otr=r(Mnt,"pretrained_model_name_or_path"),Mnt.forEach(t),Xtr=r(D4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Nge=n(D4,"CODE",{});var Ent=s(Nge);ztr=r(Ent,"pretrained_model_name_or_path"),Ent.forEach(t),Vtr=r(D4,":"),D4.forEach(t),Wtr=i(_a),X=n(_a,"UL",{});var W=s(X);BT=n(W,"LI",{});var aAe=s(BT);Dge=n(aAe,"STRONG",{});var ynt=s(Dge);Qtr=r(ynt,"albert"),ynt.forEach(t),Htr=r(aAe," \u2014 "),jq=n(aAe,"A",{href:!0});var wnt=s(jq);Utr=r(wnt,"TFAlbertForSequenceClassification"),wnt.forEach(t),Jtr=r(aAe," (ALBERT model)"),aAe.forEach(t),Ytr=i(W),kT=n(W,"LI",{});var nAe=s(kT);qge=n(nAe,"STRONG",{});var Ant=s(qge);Ktr=r(Ant,"bert"),Ant.forEach(t),Ztr=r(nAe," \u2014 "),Nq=n(nAe,"A",{href:!0});var Lnt=s(Nq);ear=r(Lnt,"TFBertForSequenceClassification"),Lnt.forEach(t),oar=r(nAe," (BERT model)"),nAe.forEach(t),rar=i(W),xT=n(W,"LI",{});var sAe=s(xT);Gge=n(sAe,"STRONG",{});var Bnt=s(Gge);tar=r(Bnt,"camembert"),Bnt.forEach(t),aar=r(sAe," \u2014 "),Dq=n(sAe,"A",{href:!0});var knt=s(Dq);nar=r(knt,"TFCamembertForSequenceClassification"),knt.forEach(t),sar=r(sAe," (CamemBERT model)"),sAe.forEach(t),lar=i(W),RT=n(W,"LI",{});var lAe=s(RT);Oge=n(lAe,"STRONG",{});var xnt=s(Oge);iar=r(xnt,"convbert"),xnt.forEach(t),dar=r(lAe," \u2014 "),qq=n(lAe,"A",{href:!0});var Rnt=s(qq);car=r(Rnt,"TFConvBertForSequenceClassification"),Rnt.forEach(t),far=r(lAe," (ConvBERT model)"),lAe.forEach(t),mar=i(W),ST=n(W,"LI",{});var iAe=s(ST);Xge=n(iAe,"STRONG",{});var Snt=s(Xge);gar=r(Snt,"ctrl"),Snt.forEach(t),har=r(iAe," \u2014 "),Gq=n(iAe,"A",{href:!0});var Pnt=s(Gq);par=r(Pnt,"TFCTRLForSequenceClassification"),Pnt.forEach(t),_ar=r(iAe," (CTRL model)"),iAe.forEach(t),uar=i(W),PT=n(W,"LI",{});var dAe=s(PT);zge=n(dAe,"STRONG",{});var $nt=s(zge);bar=r($nt,"deberta"),$nt.forEach(t),Tar=r(dAe," \u2014 "),Oq=n(dAe,"A",{href:!0});var Int=s(Oq);Far=r(Int,"TFDebertaForSequenceClassification"),Int.forEach(t),Car=r(dAe," (DeBERTa model)"),dAe.forEach(t),Mar=i(W),$T=n(W,"LI",{});var cAe=s($T);Vge=n(cAe,"STRONG",{});var jnt=s(Vge);Ear=r(jnt,"deberta-v2"),jnt.forEach(t),yar=r(cAe," \u2014 "),Xq=n(cAe,"A",{href:!0});var Nnt=s(Xq);war=r(Nnt,"TFDebertaV2ForSequenceClassification"),Nnt.forEach(t),Aar=r(cAe," (DeBERTa-v2 model)"),cAe.forEach(t),Lar=i(W),IT=n(W,"LI",{});var fAe=s(IT);Wge=n(fAe,"STRONG",{});var Dnt=s(Wge);Bar=r(Dnt,"distilbert"),Dnt.forEach(t),kar=r(fAe," \u2014 "),zq=n(fAe,"A",{href:!0});var qnt=s(zq);xar=r(qnt,"TFDistilBertForSequenceClassification"),qnt.forEach(t),Rar=r(fAe," (DistilBERT model)"),fAe.forEach(t),Sar=i(W),jT=n(W,"LI",{});var mAe=s(jT);Qge=n(mAe,"STRONG",{});var Gnt=s(Qge);Par=r(Gnt,"electra"),Gnt.forEach(t),$ar=r(mAe," \u2014 "),Vq=n(mAe,"A",{href:!0});var Ont=s(Vq);Iar=r(Ont,"TFElectraForSequenceClassification"),Ont.forEach(t),jar=r(mAe," (ELECTRA model)"),mAe.forEach(t),Nar=i(W),NT=n(W,"LI",{});var gAe=s(NT);Hge=n(gAe,"STRONG",{});var Xnt=s(Hge);Dar=r(Xnt,"flaubert"),Xnt.forEach(t),qar=r(gAe," \u2014 "),Wq=n(gAe,"A",{href:!0});var znt=s(Wq);Gar=r(znt,"TFFlaubertForSequenceClassification"),znt.forEach(t),Oar=r(gAe," (FlauBERT model)"),gAe.forEach(t),Xar=i(W),DT=n(W,"LI",{});var hAe=s(DT);Uge=n(hAe,"STRONG",{});var Vnt=s(Uge);zar=r(Vnt,"funnel"),Vnt.forEach(t),Var=r(hAe," \u2014 "),Qq=n(hAe,"A",{href:!0});var Wnt=s(Qq);War=r(Wnt,"TFFunnelForSequenceClassification"),Wnt.forEach(t),Qar=r(hAe," (Funnel Transformer model)"),hAe.forEach(t),Har=i(W),qT=n(W,"LI",{});var pAe=s(qT);Jge=n(pAe,"STRONG",{});var Qnt=s(Jge);Uar=r(Qnt,"gpt2"),Qnt.forEach(t),Jar=r(pAe," \u2014 "),Hq=n(pAe,"A",{href:!0});var Hnt=s(Hq);Yar=r(Hnt,"TFGPT2ForSequenceClassification"),Hnt.forEach(t),Kar=r(pAe," (OpenAI GPT-2 model)"),pAe.forEach(t),Zar=i(W),GT=n(W,"LI",{});var _Ae=s(GT);Yge=n(_Ae,"STRONG",{});var Unt=s(Yge);enr=r(Unt,"layoutlm"),Unt.forEach(t),onr=r(_Ae," \u2014 "),Uq=n(_Ae,"A",{href:!0});var Jnt=s(Uq);rnr=r(Jnt,"TFLayoutLMForSequenceClassification"),Jnt.forEach(t),tnr=r(_Ae," (LayoutLM model)"),_Ae.forEach(t),anr=i(W),OT=n(W,"LI",{});var uAe=s(OT);Kge=n(uAe,"STRONG",{});var Ynt=s(Kge);nnr=r(Ynt,"longformer"),Ynt.forEach(t),snr=r(uAe," \u2014 "),Jq=n(uAe,"A",{href:!0});var Knt=s(Jq);lnr=r(Knt,"TFLongformerForSequenceClassification"),Knt.forEach(t),inr=r(uAe," (Longformer model)"),uAe.forEach(t),dnr=i(W),XT=n(W,"LI",{});var bAe=s(XT);Zge=n(bAe,"STRONG",{});var Znt=s(Zge);cnr=r(Znt,"mobilebert"),Znt.forEach(t),fnr=r(bAe," \u2014 "),Yq=n(bAe,"A",{href:!0});var est=s(Yq);mnr=r(est,"TFMobileBertForSequenceClassification"),est.forEach(t),gnr=r(bAe," (MobileBERT model)"),bAe.forEach(t),hnr=i(W),zT=n(W,"LI",{});var vAe=s(zT);ehe=n(vAe,"STRONG",{});var ost=s(ehe);pnr=r(ost,"mpnet"),ost.forEach(t),_nr=r(vAe," \u2014 "),Kq=n(vAe,"A",{href:!0});var rst=s(Kq);unr=r(rst,"TFMPNetForSequenceClassification"),rst.forEach(t),bnr=r(vAe," (MPNet model)"),vAe.forEach(t),vnr=i(W),VT=n(W,"LI",{});var TAe=s(VT);ohe=n(TAe,"STRONG",{});var tst=s(ohe);Tnr=r(tst,"openai-gpt"),tst.forEach(t),Fnr=r(TAe," \u2014 "),Zq=n(TAe,"A",{href:!0});var ast=s(Zq);Cnr=r(ast,"TFOpenAIGPTForSequenceClassification"),ast.forEach(t),Mnr=r(TAe," (OpenAI GPT model)"),TAe.forEach(t),Enr=i(W),WT=n(W,"LI",{});var FAe=s(WT);rhe=n(FAe,"STRONG",{});var nst=s(rhe);ynr=r(nst,"rembert"),nst.forEach(t),wnr=r(FAe," \u2014 "),eG=n(FAe,"A",{href:!0});var sst=s(eG);Anr=r(sst,"TFRemBertForSequenceClassification"),sst.forEach(t),Lnr=r(FAe," (RemBERT model)"),FAe.forEach(t),Bnr=i(W),QT=n(W,"LI",{});var CAe=s(QT);the=n(CAe,"STRONG",{});var lst=s(the);knr=r(lst,"roberta"),lst.forEach(t),xnr=r(CAe," \u2014 "),oG=n(CAe,"A",{href:!0});var ist=s(oG);Rnr=r(ist,"TFRobertaForSequenceClassification"),ist.forEach(t),Snr=r(CAe," (RoBERTa model)"),CAe.forEach(t),Pnr=i(W),HT=n(W,"LI",{});var MAe=s(HT);ahe=n(MAe,"STRONG",{});var dst=s(ahe);$nr=r(dst,"roformer"),dst.forEach(t),Inr=r(MAe," \u2014 "),rG=n(MAe,"A",{href:!0});var cst=s(rG);jnr=r(cst,"TFRoFormerForSequenceClassification"),cst.forEach(t),Nnr=r(MAe," (RoFormer model)"),MAe.forEach(t),Dnr=i(W),UT=n(W,"LI",{});var EAe=s(UT);nhe=n(EAe,"STRONG",{});var fst=s(nhe);qnr=r(fst,"tapas"),fst.forEach(t),Gnr=r(EAe," \u2014 "),tG=n(EAe,"A",{href:!0});var mst=s(tG);Onr=r(mst,"TFTapasForSequenceClassification"),mst.forEach(t),Xnr=r(EAe," (TAPAS model)"),EAe.forEach(t),znr=i(W),JT=n(W,"LI",{});var yAe=s(JT);she=n(yAe,"STRONG",{});var gst=s(she);Vnr=r(gst,"transfo-xl"),gst.forEach(t),Wnr=r(yAe," \u2014 "),aG=n(yAe,"A",{href:!0});var hst=s(aG);Qnr=r(hst,"TFTransfoXLForSequenceClassification"),hst.forEach(t),Hnr=r(yAe," (Transformer-XL model)"),yAe.forEach(t),Unr=i(W),YT=n(W,"LI",{});var wAe=s(YT);lhe=n(wAe,"STRONG",{});var pst=s(lhe);Jnr=r(pst,"xlm"),pst.forEach(t),Ynr=r(wAe," \u2014 "),nG=n(wAe,"A",{href:!0});var _st=s(nG);Knr=r(_st,"TFXLMForSequenceClassification"),_st.forEach(t),Znr=r(wAe," (XLM model)"),wAe.forEach(t),esr=i(W),KT=n(W,"LI",{});var AAe=s(KT);ihe=n(AAe,"STRONG",{});var ust=s(ihe);osr=r(ust,"xlm-roberta"),ust.forEach(t),rsr=r(AAe," \u2014 "),sG=n(AAe,"A",{href:!0});var bst=s(sG);tsr=r(bst,"TFXLMRobertaForSequenceClassification"),bst.forEach(t),asr=r(AAe," (XLM-RoBERTa model)"),AAe.forEach(t),nsr=i(W),ZT=n(W,"LI",{});var LAe=s(ZT);dhe=n(LAe,"STRONG",{});var vst=s(dhe);ssr=r(vst,"xlnet"),vst.forEach(t),lsr=r(LAe," \u2014 "),lG=n(LAe,"A",{href:!0});var Tst=s(lG);isr=r(Tst,"TFXLNetForSequenceClassification"),Tst.forEach(t),dsr=r(LAe," (XLNet model)"),LAe.forEach(t),W.forEach(t),csr=i(_a),che=n(_a,"P",{});var Fst=s(che);fsr=r(Fst,"Examples:"),Fst.forEach(t),msr=i(_a),m(Lw.$$.fragment,_a),_a.forEach(t),Gl.forEach(t),I9e=i(d),Cc=n(d,"H2",{class:!0});var Wke=s(Cc);e8=n(Wke,"A",{id:!0,class:!0,href:!0});var Cst=s(e8);fhe=n(Cst,"SPAN",{});var Mst=s(fhe);m(Bw.$$.fragment,Mst),Mst.forEach(t),Cst.forEach(t),gsr=i(Wke),mhe=n(Wke,"SPAN",{});var Est=s(mhe);hsr=r(Est,"TFAutoModelForMultipleChoice"),Est.forEach(t),Wke.forEach(t),j9e=i(d),Tr=n(d,"DIV",{class:!0});var Xl=s(Tr);m(kw.$$.fragment,Xl),psr=i(Xl),Mc=n(Xl,"P",{});var iV=s(Mc);_sr=r(iV,`This is a generic model class that will be instantiated as one of the model classes of the library (with a multiple choice head) when created
with the `),ghe=n(iV,"CODE",{});var yst=s(ghe);usr=r(yst,"from_pretrained()"),yst.forEach(t),bsr=r(iV,"class method or the "),hhe=n(iV,"CODE",{});var wst=s(hhe);vsr=r(wst,"from_config()"),wst.forEach(t),Tsr=r(iV,`class
method.`),iV.forEach(t),Fsr=i(Xl),xw=n(Xl,"P",{});var Qke=s(xw);Csr=r(Qke,"This class cannot be instantiated directly using "),phe=n(Qke,"CODE",{});var Ast=s(phe);Msr=r(Ast,"__init__()"),Ast.forEach(t),Esr=r(Qke," (throws an error)."),Qke.forEach(t),ysr=i(Xl),gt=n(Xl,"DIV",{class:!0});var zl=s(gt);m(Rw.$$.fragment,zl),wsr=i(zl),_he=n(zl,"P",{});var Lst=s(_he);Asr=r(Lst,"Instantiates one of the model classes of the library (with a multiple choice head) from a configuration."),Lst.forEach(t),Lsr=i(zl),Ec=n(zl,"P",{});var dV=s(Ec);Bsr=r(dV,`Note:
Loading a model from its configuration file does `),uhe=n(dV,"STRONG",{});var Bst=s(uhe);ksr=r(Bst,"not"),Bst.forEach(t),xsr=r(dV,` load the model weights. It only affects the
model\u2019s configuration. Use `),bhe=n(dV,"CODE",{});var kst=s(bhe);Rsr=r(kst,"from_pretrained()"),kst.forEach(t),Ssr=r(dV,"to load the model weights."),dV.forEach(t),Psr=i(zl),vhe=n(zl,"P",{});var xst=s(vhe);$sr=r(xst,"Examples:"),xst.forEach(t),Isr=i(zl),m(Sw.$$.fragment,zl),zl.forEach(t),jsr=i(Xl),To=n(Xl,"DIV",{class:!0});var ua=s(To);m(Pw.$$.fragment,ua),Nsr=i(ua),The=n(ua,"P",{});var Rst=s(The);Dsr=r(Rst,"Instantiate one of the model classes of the library (with a multiple choice head) from a pretrained model."),Rst.forEach(t),qsr=i(ua),pn=n(ua,"P",{});var q4=s(pn);Gsr=r(q4,"The model class to instantiate is selected based on the "),Fhe=n(q4,"CODE",{});var Sst=s(Fhe);Osr=r(Sst,"model_type"),Sst.forEach(t),Xsr=r(q4,` property of the config object (either
passed as an argument or loaded from `),Che=n(q4,"CODE",{});var Pst=s(Che);zsr=r(Pst,"pretrained_model_name_or_path"),Pst.forEach(t),Vsr=r(q4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Mhe=n(q4,"CODE",{});var $st=s(Mhe);Wsr=r($st,"pretrained_model_name_or_path"),$st.forEach(t),Qsr=r(q4,":"),q4.forEach(t),Hsr=i(ua),te=n(ua,"UL",{});var ne=s(te);o8=n(ne,"LI",{});var BAe=s(o8);Ehe=n(BAe,"STRONG",{});var Ist=s(Ehe);Usr=r(Ist,"albert"),Ist.forEach(t),Jsr=r(BAe," \u2014 "),iG=n(BAe,"A",{href:!0});var jst=s(iG);Ysr=r(jst,"TFAlbertForMultipleChoice"),jst.forEach(t),Ksr=r(BAe," (ALBERT model)"),BAe.forEach(t),Zsr=i(ne),r8=n(ne,"LI",{});var kAe=s(r8);yhe=n(kAe,"STRONG",{});var Nst=s(yhe);elr=r(Nst,"bert"),Nst.forEach(t),olr=r(kAe," \u2014 "),dG=n(kAe,"A",{href:!0});var Dst=s(dG);rlr=r(Dst,"TFBertForMultipleChoice"),Dst.forEach(t),tlr=r(kAe," (BERT model)"),kAe.forEach(t),alr=i(ne),t8=n(ne,"LI",{});var xAe=s(t8);whe=n(xAe,"STRONG",{});var qst=s(whe);nlr=r(qst,"camembert"),qst.forEach(t),slr=r(xAe," \u2014 "),cG=n(xAe,"A",{href:!0});var Gst=s(cG);llr=r(Gst,"TFCamembertForMultipleChoice"),Gst.forEach(t),ilr=r(xAe," (CamemBERT model)"),xAe.forEach(t),dlr=i(ne),a8=n(ne,"LI",{});var RAe=s(a8);Ahe=n(RAe,"STRONG",{});var Ost=s(Ahe);clr=r(Ost,"convbert"),Ost.forEach(t),flr=r(RAe," \u2014 "),fG=n(RAe,"A",{href:!0});var Xst=s(fG);mlr=r(Xst,"TFConvBertForMultipleChoice"),Xst.forEach(t),glr=r(RAe," (ConvBERT model)"),RAe.forEach(t),hlr=i(ne),n8=n(ne,"LI",{});var SAe=s(n8);Lhe=n(SAe,"STRONG",{});var zst=s(Lhe);plr=r(zst,"distilbert"),zst.forEach(t),_lr=r(SAe," \u2014 "),mG=n(SAe,"A",{href:!0});var Vst=s(mG);ulr=r(Vst,"TFDistilBertForMultipleChoice"),Vst.forEach(t),blr=r(SAe," (DistilBERT model)"),SAe.forEach(t),vlr=i(ne),s8=n(ne,"LI",{});var PAe=s(s8);Bhe=n(PAe,"STRONG",{});var Wst=s(Bhe);Tlr=r(Wst,"electra"),Wst.forEach(t),Flr=r(PAe," \u2014 "),gG=n(PAe,"A",{href:!0});var Qst=s(gG);Clr=r(Qst,"TFElectraForMultipleChoice"),Qst.forEach(t),Mlr=r(PAe," (ELECTRA model)"),PAe.forEach(t),Elr=i(ne),l8=n(ne,"LI",{});var $Ae=s(l8);khe=n($Ae,"STRONG",{});var Hst=s(khe);ylr=r(Hst,"flaubert"),Hst.forEach(t),wlr=r($Ae," \u2014 "),hG=n($Ae,"A",{href:!0});var Ust=s(hG);Alr=r(Ust,"TFFlaubertForMultipleChoice"),Ust.forEach(t),Llr=r($Ae," (FlauBERT model)"),$Ae.forEach(t),Blr=i(ne),i8=n(ne,"LI",{});var IAe=s(i8);xhe=n(IAe,"STRONG",{});var Jst=s(xhe);klr=r(Jst,"funnel"),Jst.forEach(t),xlr=r(IAe," \u2014 "),pG=n(IAe,"A",{href:!0});var Yst=s(pG);Rlr=r(Yst,"TFFunnelForMultipleChoice"),Yst.forEach(t),Slr=r(IAe," (Funnel Transformer model)"),IAe.forEach(t),Plr=i(ne),d8=n(ne,"LI",{});var jAe=s(d8);Rhe=n(jAe,"STRONG",{});var Kst=s(Rhe);$lr=r(Kst,"longformer"),Kst.forEach(t),Ilr=r(jAe," \u2014 "),_G=n(jAe,"A",{href:!0});var Zst=s(_G);jlr=r(Zst,"TFLongformerForMultipleChoice"),Zst.forEach(t),Nlr=r(jAe," (Longformer model)"),jAe.forEach(t),Dlr=i(ne),c8=n(ne,"LI",{});var NAe=s(c8);She=n(NAe,"STRONG",{});var elt=s(She);qlr=r(elt,"mobilebert"),elt.forEach(t),Glr=r(NAe," \u2014 "),uG=n(NAe,"A",{href:!0});var olt=s(uG);Olr=r(olt,"TFMobileBertForMultipleChoice"),olt.forEach(t),Xlr=r(NAe," (MobileBERT model)"),NAe.forEach(t),zlr=i(ne),f8=n(ne,"LI",{});var DAe=s(f8);Phe=n(DAe,"STRONG",{});var rlt=s(Phe);Vlr=r(rlt,"mpnet"),rlt.forEach(t),Wlr=r(DAe," \u2014 "),bG=n(DAe,"A",{href:!0});var tlt=s(bG);Qlr=r(tlt,"TFMPNetForMultipleChoice"),tlt.forEach(t),Hlr=r(DAe," (MPNet model)"),DAe.forEach(t),Ulr=i(ne),m8=n(ne,"LI",{});var qAe=s(m8);$he=n(qAe,"STRONG",{});var alt=s($he);Jlr=r(alt,"rembert"),alt.forEach(t),Ylr=r(qAe," \u2014 "),vG=n(qAe,"A",{href:!0});var nlt=s(vG);Klr=r(nlt,"TFRemBertForMultipleChoice"),nlt.forEach(t),Zlr=r(qAe," (RemBERT model)"),qAe.forEach(t),eir=i(ne),g8=n(ne,"LI",{});var GAe=s(g8);Ihe=n(GAe,"STRONG",{});var slt=s(Ihe);oir=r(slt,"roberta"),slt.forEach(t),rir=r(GAe," \u2014 "),TG=n(GAe,"A",{href:!0});var llt=s(TG);tir=r(llt,"TFRobertaForMultipleChoice"),llt.forEach(t),air=r(GAe," (RoBERTa model)"),GAe.forEach(t),nir=i(ne),h8=n(ne,"LI",{});var OAe=s(h8);jhe=n(OAe,"STRONG",{});var ilt=s(jhe);sir=r(ilt,"roformer"),ilt.forEach(t),lir=r(OAe," \u2014 "),FG=n(OAe,"A",{href:!0});var dlt=s(FG);iir=r(dlt,"TFRoFormerForMultipleChoice"),dlt.forEach(t),dir=r(OAe," (RoFormer model)"),OAe.forEach(t),cir=i(ne),p8=n(ne,"LI",{});var XAe=s(p8);Nhe=n(XAe,"STRONG",{});var clt=s(Nhe);fir=r(clt,"xlm"),clt.forEach(t),mir=r(XAe," \u2014 "),CG=n(XAe,"A",{href:!0});var flt=s(CG);gir=r(flt,"TFXLMForMultipleChoice"),flt.forEach(t),hir=r(XAe," (XLM model)"),XAe.forEach(t),pir=i(ne),_8=n(ne,"LI",{});var zAe=s(_8);Dhe=n(zAe,"STRONG",{});var mlt=s(Dhe);_ir=r(mlt,"xlm-roberta"),mlt.forEach(t),uir=r(zAe," \u2014 "),MG=n(zAe,"A",{href:!0});var glt=s(MG);bir=r(glt,"TFXLMRobertaForMultipleChoice"),glt.forEach(t),vir=r(zAe," (XLM-RoBERTa model)"),zAe.forEach(t),Tir=i(ne),u8=n(ne,"LI",{});var VAe=s(u8);qhe=n(VAe,"STRONG",{});var hlt=s(qhe);Fir=r(hlt,"xlnet"),hlt.forEach(t),Cir=r(VAe," \u2014 "),EG=n(VAe,"A",{href:!0});var plt=s(EG);Mir=r(plt,"TFXLNetForMultipleChoice"),plt.forEach(t),Eir=r(VAe," (XLNet model)"),VAe.forEach(t),ne.forEach(t),yir=i(ua),Ghe=n(ua,"P",{});var _lt=s(Ghe);wir=r(_lt,"Examples:"),_lt.forEach(t),Air=i(ua),m($w.$$.fragment,ua),ua.forEach(t),Xl.forEach(t),N9e=i(d),yc=n(d,"H2",{class:!0});var Hke=s(yc);b8=n(Hke,"A",{id:!0,class:!0,href:!0});var ult=s(b8);Ohe=n(ult,"SPAN",{});var blt=s(Ohe);m(Iw.$$.fragment,blt),blt.forEach(t),ult.forEach(t),Lir=i(Hke),Xhe=n(Hke,"SPAN",{});var vlt=s(Xhe);Bir=r(vlt,"TFAutoModelForTableQuestionAnswering"),vlt.forEach(t),Hke.forEach(t),D9e=i(d),Fr=n(d,"DIV",{class:!0});var Vl=s(Fr);m(jw.$$.fragment,Vl),kir=i(Vl),wc=n(Vl,"P",{});var cV=s(wc);xir=r(cV,`This is a generic model class that will be instantiated as one of the model classes of the library (with a table question answering head) when created
with the `),zhe=n(cV,"CODE",{});var Tlt=s(zhe);Rir=r(Tlt,"from_pretrained()"),Tlt.forEach(t),Sir=r(cV,"class method or the "),Vhe=n(cV,"CODE",{});var Flt=s(Vhe);Pir=r(Flt,"from_config()"),Flt.forEach(t),$ir=r(cV,`class
method.`),cV.forEach(t),Iir=i(Vl),Nw=n(Vl,"P",{});var Uke=s(Nw);jir=r(Uke,"This class cannot be instantiated directly using "),Whe=n(Uke,"CODE",{});var Clt=s(Whe);Nir=r(Clt,"__init__()"),Clt.forEach(t),Dir=r(Uke," (throws an error)."),Uke.forEach(t),qir=i(Vl),ht=n(Vl,"DIV",{class:!0});var Wl=s(ht);m(Dw.$$.fragment,Wl),Gir=i(Wl),Qhe=n(Wl,"P",{});var Mlt=s(Qhe);Oir=r(Mlt,"Instantiates one of the model classes of the library (with a table question answering head) from a configuration."),Mlt.forEach(t),Xir=i(Wl),Ac=n(Wl,"P",{});var fV=s(Ac);zir=r(fV,`Note:
Loading a model from its configuration file does `),Hhe=n(fV,"STRONG",{});var Elt=s(Hhe);Vir=r(Elt,"not"),Elt.forEach(t),Wir=r(fV,` load the model weights. It only affects the
model\u2019s configuration. Use `),Uhe=n(fV,"CODE",{});var ylt=s(Uhe);Qir=r(ylt,"from_pretrained()"),ylt.forEach(t),Hir=r(fV,"to load the model weights."),fV.forEach(t),Uir=i(Wl),Jhe=n(Wl,"P",{});var wlt=s(Jhe);Jir=r(wlt,"Examples:"),wlt.forEach(t),Yir=i(Wl),m(qw.$$.fragment,Wl),Wl.forEach(t),Kir=i(Vl),Fo=n(Vl,"DIV",{class:!0});var ba=s(Fo);m(Gw.$$.fragment,ba),Zir=i(ba),Yhe=n(ba,"P",{});var Alt=s(Yhe);edr=r(Alt,"Instantiate one of the model classes of the library (with a table question answering head) from a pretrained model."),Alt.forEach(t),odr=i(ba),_n=n(ba,"P",{});var G4=s(_n);rdr=r(G4,"The model class to instantiate is selected based on the "),Khe=n(G4,"CODE",{});var Llt=s(Khe);tdr=r(Llt,"model_type"),Llt.forEach(t),adr=r(G4,` property of the config object (either
passed as an argument or loaded from `),Zhe=n(G4,"CODE",{});var Blt=s(Zhe);ndr=r(Blt,"pretrained_model_name_or_path"),Blt.forEach(t),sdr=r(G4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),epe=n(G4,"CODE",{});var klt=s(epe);ldr=r(klt,"pretrained_model_name_or_path"),klt.forEach(t),idr=r(G4,":"),G4.forEach(t),ddr=i(ba),ope=n(ba,"UL",{});var xlt=s(ope);v8=n(xlt,"LI",{});var WAe=s(v8);rpe=n(WAe,"STRONG",{});var Rlt=s(rpe);cdr=r(Rlt,"tapas"),Rlt.forEach(t),fdr=r(WAe," \u2014 "),yG=n(WAe,"A",{href:!0});var Slt=s(yG);mdr=r(Slt,"TFTapasForQuestionAnswering"),Slt.forEach(t),gdr=r(WAe," (TAPAS model)"),WAe.forEach(t),xlt.forEach(t),hdr=i(ba),tpe=n(ba,"P",{});var Plt=s(tpe);pdr=r(Plt,"Examples:"),Plt.forEach(t),_dr=i(ba),m(Ow.$$.fragment,ba),ba.forEach(t),Vl.forEach(t),q9e=i(d),Lc=n(d,"H2",{class:!0});var Jke=s(Lc);T8=n(Jke,"A",{id:!0,class:!0,href:!0});var $lt=s(T8);ape=n($lt,"SPAN",{});var Ilt=s(ape);m(Xw.$$.fragment,Ilt),Ilt.forEach(t),$lt.forEach(t),udr=i(Jke),npe=n(Jke,"SPAN",{});var jlt=s(npe);bdr=r(jlt,"TFAutoModelForTokenClassification"),jlt.forEach(t),Jke.forEach(t),G9e=i(d),Cr=n(d,"DIV",{class:!0});var Ql=s(Cr);m(zw.$$.fragment,Ql),vdr=i(Ql),Bc=n(Ql,"P",{});var mV=s(Bc);Tdr=r(mV,`This is a generic model class that will be instantiated as one of the model classes of the library (with a token classification head) when created
with the `),spe=n(mV,"CODE",{});var Nlt=s(spe);Fdr=r(Nlt,"from_pretrained()"),Nlt.forEach(t),Cdr=r(mV,"class method or the "),lpe=n(mV,"CODE",{});var Dlt=s(lpe);Mdr=r(Dlt,"from_config()"),Dlt.forEach(t),Edr=r(mV,`class
method.`),mV.forEach(t),ydr=i(Ql),Vw=n(Ql,"P",{});var Yke=s(Vw);wdr=r(Yke,"This class cannot be instantiated directly using "),ipe=n(Yke,"CODE",{});var qlt=s(ipe);Adr=r(qlt,"__init__()"),qlt.forEach(t),Ldr=r(Yke," (throws an error)."),Yke.forEach(t),Bdr=i(Ql),pt=n(Ql,"DIV",{class:!0});var Hl=s(pt);m(Ww.$$.fragment,Hl),kdr=i(Hl),dpe=n(Hl,"P",{});var Glt=s(dpe);xdr=r(Glt,"Instantiates one of the model classes of the library (with a token classification head) from a configuration."),Glt.forEach(t),Rdr=i(Hl),kc=n(Hl,"P",{});var gV=s(kc);Sdr=r(gV,`Note:
Loading a model from its configuration file does `),cpe=n(gV,"STRONG",{});var Olt=s(cpe);Pdr=r(Olt,"not"),Olt.forEach(t),$dr=r(gV,` load the model weights. It only affects the
model\u2019s configuration. Use `),fpe=n(gV,"CODE",{});var Xlt=s(fpe);Idr=r(Xlt,"from_pretrained()"),Xlt.forEach(t),jdr=r(gV,"to load the model weights."),gV.forEach(t),Ndr=i(Hl),mpe=n(Hl,"P",{});var zlt=s(mpe);Ddr=r(zlt,"Examples:"),zlt.forEach(t),qdr=i(Hl),m(Qw.$$.fragment,Hl),Hl.forEach(t),Gdr=i(Ql),Co=n(Ql,"DIV",{class:!0});var va=s(Co);m(Hw.$$.fragment,va),Odr=i(va),gpe=n(va,"P",{});var Vlt=s(gpe);Xdr=r(Vlt,"Instantiate one of the model classes of the library (with a token classification head) from a pretrained model."),Vlt.forEach(t),zdr=i(va),un=n(va,"P",{});var O4=s(un);Vdr=r(O4,"The model class to instantiate is selected based on the "),hpe=n(O4,"CODE",{});var Wlt=s(hpe);Wdr=r(Wlt,"model_type"),Wlt.forEach(t),Qdr=r(O4,` property of the config object (either
passed as an argument or loaded from `),ppe=n(O4,"CODE",{});var Qlt=s(ppe);Hdr=r(Qlt,"pretrained_model_name_or_path"),Qlt.forEach(t),Udr=r(O4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),_pe=n(O4,"CODE",{});var Hlt=s(_pe);Jdr=r(Hlt,"pretrained_model_name_or_path"),Hlt.forEach(t),Ydr=r(O4,":"),O4.forEach(t),Kdr=i(va),K=n(va,"UL",{});var oe=s(K);F8=n(oe,"LI",{});var QAe=s(F8);upe=n(QAe,"STRONG",{});var Ult=s(upe);Zdr=r(Ult,"albert"),Ult.forEach(t),ecr=r(QAe," \u2014 "),wG=n(QAe,"A",{href:!0});var Jlt=s(wG);ocr=r(Jlt,"TFAlbertForTokenClassification"),Jlt.forEach(t),rcr=r(QAe," (ALBERT model)"),QAe.forEach(t),tcr=i(oe),C8=n(oe,"LI",{});var HAe=s(C8);bpe=n(HAe,"STRONG",{});var Ylt=s(bpe);acr=r(Ylt,"bert"),Ylt.forEach(t),ncr=r(HAe," \u2014 "),AG=n(HAe,"A",{href:!0});var Klt=s(AG);scr=r(Klt,"TFBertForTokenClassification"),Klt.forEach(t),lcr=r(HAe," (BERT model)"),HAe.forEach(t),icr=i(oe),M8=n(oe,"LI",{});var UAe=s(M8);vpe=n(UAe,"STRONG",{});var Zlt=s(vpe);dcr=r(Zlt,"camembert"),Zlt.forEach(t),ccr=r(UAe," \u2014 "),LG=n(UAe,"A",{href:!0});var eit=s(LG);fcr=r(eit,"TFCamembertForTokenClassification"),eit.forEach(t),mcr=r(UAe," (CamemBERT model)"),UAe.forEach(t),gcr=i(oe),E8=n(oe,"LI",{});var JAe=s(E8);Tpe=n(JAe,"STRONG",{});var oit=s(Tpe);hcr=r(oit,"convbert"),oit.forEach(t),pcr=r(JAe," \u2014 "),BG=n(JAe,"A",{href:!0});var rit=s(BG);_cr=r(rit,"TFConvBertForTokenClassification"),rit.forEach(t),ucr=r(JAe," (ConvBERT model)"),JAe.forEach(t),bcr=i(oe),y8=n(oe,"LI",{});var YAe=s(y8);Fpe=n(YAe,"STRONG",{});var tit=s(Fpe);vcr=r(tit,"deberta"),tit.forEach(t),Tcr=r(YAe," \u2014 "),kG=n(YAe,"A",{href:!0});var ait=s(kG);Fcr=r(ait,"TFDebertaForTokenClassification"),ait.forEach(t),Ccr=r(YAe," (DeBERTa model)"),YAe.forEach(t),Mcr=i(oe),w8=n(oe,"LI",{});var KAe=s(w8);Cpe=n(KAe,"STRONG",{});var nit=s(Cpe);Ecr=r(nit,"deberta-v2"),nit.forEach(t),ycr=r(KAe," \u2014 "),xG=n(KAe,"A",{href:!0});var sit=s(xG);wcr=r(sit,"TFDebertaV2ForTokenClassification"),sit.forEach(t),Acr=r(KAe," (DeBERTa-v2 model)"),KAe.forEach(t),Lcr=i(oe),A8=n(oe,"LI",{});var ZAe=s(A8);Mpe=n(ZAe,"STRONG",{});var lit=s(Mpe);Bcr=r(lit,"distilbert"),lit.forEach(t),kcr=r(ZAe," \u2014 "),RG=n(ZAe,"A",{href:!0});var iit=s(RG);xcr=r(iit,"TFDistilBertForTokenClassification"),iit.forEach(t),Rcr=r(ZAe," (DistilBERT model)"),ZAe.forEach(t),Scr=i(oe),L8=n(oe,"LI",{});var e0e=s(L8);Epe=n(e0e,"STRONG",{});var dit=s(Epe);Pcr=r(dit,"electra"),dit.forEach(t),$cr=r(e0e," \u2014 "),SG=n(e0e,"A",{href:!0});var cit=s(SG);Icr=r(cit,"TFElectraForTokenClassification"),cit.forEach(t),jcr=r(e0e," (ELECTRA model)"),e0e.forEach(t),Ncr=i(oe),B8=n(oe,"LI",{});var o0e=s(B8);ype=n(o0e,"STRONG",{});var fit=s(ype);Dcr=r(fit,"flaubert"),fit.forEach(t),qcr=r(o0e," \u2014 "),PG=n(o0e,"A",{href:!0});var mit=s(PG);Gcr=r(mit,"TFFlaubertForTokenClassification"),mit.forEach(t),Ocr=r(o0e," (FlauBERT model)"),o0e.forEach(t),Xcr=i(oe),k8=n(oe,"LI",{});var r0e=s(k8);wpe=n(r0e,"STRONG",{});var git=s(wpe);zcr=r(git,"funnel"),git.forEach(t),Vcr=r(r0e," \u2014 "),$G=n(r0e,"A",{href:!0});var hit=s($G);Wcr=r(hit,"TFFunnelForTokenClassification"),hit.forEach(t),Qcr=r(r0e," (Funnel Transformer model)"),r0e.forEach(t),Hcr=i(oe),x8=n(oe,"LI",{});var t0e=s(x8);Ape=n(t0e,"STRONG",{});var pit=s(Ape);Ucr=r(pit,"layoutlm"),pit.forEach(t),Jcr=r(t0e," \u2014 "),IG=n(t0e,"A",{href:!0});var _it=s(IG);Ycr=r(_it,"TFLayoutLMForTokenClassification"),_it.forEach(t),Kcr=r(t0e," (LayoutLM model)"),t0e.forEach(t),Zcr=i(oe),R8=n(oe,"LI",{});var a0e=s(R8);Lpe=n(a0e,"STRONG",{});var uit=s(Lpe);efr=r(uit,"longformer"),uit.forEach(t),ofr=r(a0e," \u2014 "),jG=n(a0e,"A",{href:!0});var bit=s(jG);rfr=r(bit,"TFLongformerForTokenClassification"),bit.forEach(t),tfr=r(a0e," (Longformer model)"),a0e.forEach(t),afr=i(oe),S8=n(oe,"LI",{});var n0e=s(S8);Bpe=n(n0e,"STRONG",{});var vit=s(Bpe);nfr=r(vit,"mobilebert"),vit.forEach(t),sfr=r(n0e," \u2014 "),NG=n(n0e,"A",{href:!0});var Tit=s(NG);lfr=r(Tit,"TFMobileBertForTokenClassification"),Tit.forEach(t),ifr=r(n0e," (MobileBERT model)"),n0e.forEach(t),dfr=i(oe),P8=n(oe,"LI",{});var s0e=s(P8);kpe=n(s0e,"STRONG",{});var Fit=s(kpe);cfr=r(Fit,"mpnet"),Fit.forEach(t),ffr=r(s0e," \u2014 "),DG=n(s0e,"A",{href:!0});var Cit=s(DG);mfr=r(Cit,"TFMPNetForTokenClassification"),Cit.forEach(t),gfr=r(s0e," (MPNet model)"),s0e.forEach(t),hfr=i(oe),$8=n(oe,"LI",{});var l0e=s($8);xpe=n(l0e,"STRONG",{});var Mit=s(xpe);pfr=r(Mit,"rembert"),Mit.forEach(t),_fr=r(l0e," \u2014 "),qG=n(l0e,"A",{href:!0});var Eit=s(qG);ufr=r(Eit,"TFRemBertForTokenClassification"),Eit.forEach(t),bfr=r(l0e," (RemBERT model)"),l0e.forEach(t),vfr=i(oe),I8=n(oe,"LI",{});var i0e=s(I8);Rpe=n(i0e,"STRONG",{});var yit=s(Rpe);Tfr=r(yit,"roberta"),yit.forEach(t),Ffr=r(i0e," \u2014 "),GG=n(i0e,"A",{href:!0});var wit=s(GG);Cfr=r(wit,"TFRobertaForTokenClassification"),wit.forEach(t),Mfr=r(i0e," (RoBERTa model)"),i0e.forEach(t),Efr=i(oe),j8=n(oe,"LI",{});var d0e=s(j8);Spe=n(d0e,"STRONG",{});var Ait=s(Spe);yfr=r(Ait,"roformer"),Ait.forEach(t),wfr=r(d0e," \u2014 "),OG=n(d0e,"A",{href:!0});var Lit=s(OG);Afr=r(Lit,"TFRoFormerForTokenClassification"),Lit.forEach(t),Lfr=r(d0e," (RoFormer model)"),d0e.forEach(t),Bfr=i(oe),N8=n(oe,"LI",{});var c0e=s(N8);Ppe=n(c0e,"STRONG",{});var Bit=s(Ppe);kfr=r(Bit,"xlm"),Bit.forEach(t),xfr=r(c0e," \u2014 "),XG=n(c0e,"A",{href:!0});var kit=s(XG);Rfr=r(kit,"TFXLMForTokenClassification"),kit.forEach(t),Sfr=r(c0e," (XLM model)"),c0e.forEach(t),Pfr=i(oe),D8=n(oe,"LI",{});var f0e=s(D8);$pe=n(f0e,"STRONG",{});var xit=s($pe);$fr=r(xit,"xlm-roberta"),xit.forEach(t),Ifr=r(f0e," \u2014 "),zG=n(f0e,"A",{href:!0});var Rit=s(zG);jfr=r(Rit,"TFXLMRobertaForTokenClassification"),Rit.forEach(t),Nfr=r(f0e," (XLM-RoBERTa model)"),f0e.forEach(t),Dfr=i(oe),q8=n(oe,"LI",{});var m0e=s(q8);Ipe=n(m0e,"STRONG",{});var Sit=s(Ipe);qfr=r(Sit,"xlnet"),Sit.forEach(t),Gfr=r(m0e," \u2014 "),VG=n(m0e,"A",{href:!0});var Pit=s(VG);Ofr=r(Pit,"TFXLNetForTokenClassification"),Pit.forEach(t),Xfr=r(m0e," (XLNet model)"),m0e.forEach(t),oe.forEach(t),zfr=i(va),jpe=n(va,"P",{});var $it=s(jpe);Vfr=r($it,"Examples:"),$it.forEach(t),Wfr=i(va),m(Uw.$$.fragment,va),va.forEach(t),Ql.forEach(t),O9e=i(d),xc=n(d,"H2",{class:!0});var Kke=s(xc);G8=n(Kke,"A",{id:!0,class:!0,href:!0});var Iit=s(G8);Npe=n(Iit,"SPAN",{});var jit=s(Npe);m(Jw.$$.fragment,jit),jit.forEach(t),Iit.forEach(t),Qfr=i(Kke),Dpe=n(Kke,"SPAN",{});var Nit=s(Dpe);Hfr=r(Nit,"TFAutoModelForQuestionAnswering"),Nit.forEach(t),Kke.forEach(t),X9e=i(d),Mr=n(d,"DIV",{class:!0});var Ul=s(Mr);m(Yw.$$.fragment,Ul),Ufr=i(Ul),Rc=n(Ul,"P",{});var hV=s(Rc);Jfr=r(hV,`This is a generic model class that will be instantiated as one of the model classes of the library (with a question answering head) when created
with the `),qpe=n(hV,"CODE",{});var Dit=s(qpe);Yfr=r(Dit,"from_pretrained()"),Dit.forEach(t),Kfr=r(hV,"class method or the "),Gpe=n(hV,"CODE",{});var qit=s(Gpe);Zfr=r(qit,"from_config()"),qit.forEach(t),emr=r(hV,`class
method.`),hV.forEach(t),omr=i(Ul),Kw=n(Ul,"P",{});var Zke=s(Kw);rmr=r(Zke,"This class cannot be instantiated directly using "),Ope=n(Zke,"CODE",{});var Git=s(Ope);tmr=r(Git,"__init__()"),Git.forEach(t),amr=r(Zke," (throws an error)."),Zke.forEach(t),nmr=i(Ul),_t=n(Ul,"DIV",{class:!0});var Jl=s(_t);m(Zw.$$.fragment,Jl),smr=i(Jl),Xpe=n(Jl,"P",{});var Oit=s(Xpe);lmr=r(Oit,"Instantiates one of the model classes of the library (with a question answering head) from a configuration."),Oit.forEach(t),imr=i(Jl),Sc=n(Jl,"P",{});var pV=s(Sc);dmr=r(pV,`Note:
Loading a model from its configuration file does `),zpe=n(pV,"STRONG",{});var Xit=s(zpe);cmr=r(Xit,"not"),Xit.forEach(t),fmr=r(pV,` load the model weights. It only affects the
model\u2019s configuration. Use `),Vpe=n(pV,"CODE",{});var zit=s(Vpe);mmr=r(zit,"from_pretrained()"),zit.forEach(t),gmr=r(pV,"to load the model weights."),pV.forEach(t),hmr=i(Jl),Wpe=n(Jl,"P",{});var Vit=s(Wpe);pmr=r(Vit,"Examples:"),Vit.forEach(t),_mr=i(Jl),m(eA.$$.fragment,Jl),Jl.forEach(t),umr=i(Ul),Mo=n(Ul,"DIV",{class:!0});var Ta=s(Mo);m(oA.$$.fragment,Ta),bmr=i(Ta),Qpe=n(Ta,"P",{});var Wit=s(Qpe);vmr=r(Wit,"Instantiate one of the model classes of the library (with a question answering head) from a pretrained model."),Wit.forEach(t),Tmr=i(Ta),bn=n(Ta,"P",{});var X4=s(bn);Fmr=r(X4,"The model class to instantiate is selected based on the "),Hpe=n(X4,"CODE",{});var Qit=s(Hpe);Cmr=r(Qit,"model_type"),Qit.forEach(t),Mmr=r(X4,` property of the config object (either
passed as an argument or loaded from `),Upe=n(X4,"CODE",{});var Hit=s(Upe);Emr=r(Hit,"pretrained_model_name_or_path"),Hit.forEach(t),ymr=r(X4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Jpe=n(X4,"CODE",{});var Uit=s(Jpe);wmr=r(Uit,"pretrained_model_name_or_path"),Uit.forEach(t),Amr=r(X4,":"),X4.forEach(t),Lmr=i(Ta),Z=n(Ta,"UL",{});var re=s(Z);O8=n(re,"LI",{});var g0e=s(O8);Ype=n(g0e,"STRONG",{});var Jit=s(Ype);Bmr=r(Jit,"albert"),Jit.forEach(t),kmr=r(g0e," \u2014 "),WG=n(g0e,"A",{href:!0});var Yit=s(WG);xmr=r(Yit,"TFAlbertForQuestionAnswering"),Yit.forEach(t),Rmr=r(g0e," (ALBERT model)"),g0e.forEach(t),Smr=i(re),X8=n(re,"LI",{});var h0e=s(X8);Kpe=n(h0e,"STRONG",{});var Kit=s(Kpe);Pmr=r(Kit,"bert"),Kit.forEach(t),$mr=r(h0e," \u2014 "),QG=n(h0e,"A",{href:!0});var Zit=s(QG);Imr=r(Zit,"TFBertForQuestionAnswering"),Zit.forEach(t),jmr=r(h0e," (BERT model)"),h0e.forEach(t),Nmr=i(re),z8=n(re,"LI",{});var p0e=s(z8);Zpe=n(p0e,"STRONG",{});var edt=s(Zpe);Dmr=r(edt,"camembert"),edt.forEach(t),qmr=r(p0e," \u2014 "),HG=n(p0e,"A",{href:!0});var odt=s(HG);Gmr=r(odt,"TFCamembertForQuestionAnswering"),odt.forEach(t),Omr=r(p0e," (CamemBERT model)"),p0e.forEach(t),Xmr=i(re),V8=n(re,"LI",{});var _0e=s(V8);e_e=n(_0e,"STRONG",{});var rdt=s(e_e);zmr=r(rdt,"convbert"),rdt.forEach(t),Vmr=r(_0e," \u2014 "),UG=n(_0e,"A",{href:!0});var tdt=s(UG);Wmr=r(tdt,"TFConvBertForQuestionAnswering"),tdt.forEach(t),Qmr=r(_0e," (ConvBERT model)"),_0e.forEach(t),Hmr=i(re),W8=n(re,"LI",{});var u0e=s(W8);o_e=n(u0e,"STRONG",{});var adt=s(o_e);Umr=r(adt,"deberta"),adt.forEach(t),Jmr=r(u0e," \u2014 "),JG=n(u0e,"A",{href:!0});var ndt=s(JG);Ymr=r(ndt,"TFDebertaForQuestionAnswering"),ndt.forEach(t),Kmr=r(u0e," (DeBERTa model)"),u0e.forEach(t),Zmr=i(re),Q8=n(re,"LI",{});var b0e=s(Q8);r_e=n(b0e,"STRONG",{});var sdt=s(r_e);egr=r(sdt,"deberta-v2"),sdt.forEach(t),ogr=r(b0e," \u2014 "),YG=n(b0e,"A",{href:!0});var ldt=s(YG);rgr=r(ldt,"TFDebertaV2ForQuestionAnswering"),ldt.forEach(t),tgr=r(b0e," (DeBERTa-v2 model)"),b0e.forEach(t),agr=i(re),H8=n(re,"LI",{});var v0e=s(H8);t_e=n(v0e,"STRONG",{});var idt=s(t_e);ngr=r(idt,"distilbert"),idt.forEach(t),sgr=r(v0e," \u2014 "),KG=n(v0e,"A",{href:!0});var ddt=s(KG);lgr=r(ddt,"TFDistilBertForQuestionAnswering"),ddt.forEach(t),igr=r(v0e," (DistilBERT model)"),v0e.forEach(t),dgr=i(re),U8=n(re,"LI",{});var T0e=s(U8);a_e=n(T0e,"STRONG",{});var cdt=s(a_e);cgr=r(cdt,"electra"),cdt.forEach(t),fgr=r(T0e," \u2014 "),ZG=n(T0e,"A",{href:!0});var fdt=s(ZG);mgr=r(fdt,"TFElectraForQuestionAnswering"),fdt.forEach(t),ggr=r(T0e," (ELECTRA model)"),T0e.forEach(t),hgr=i(re),J8=n(re,"LI",{});var F0e=s(J8);n_e=n(F0e,"STRONG",{});var mdt=s(n_e);pgr=r(mdt,"flaubert"),mdt.forEach(t),_gr=r(F0e," \u2014 "),eO=n(F0e,"A",{href:!0});var gdt=s(eO);ugr=r(gdt,"TFFlaubertForQuestionAnsweringSimple"),gdt.forEach(t),bgr=r(F0e," (FlauBERT model)"),F0e.forEach(t),vgr=i(re),Y8=n(re,"LI",{});var C0e=s(Y8);s_e=n(C0e,"STRONG",{});var hdt=s(s_e);Tgr=r(hdt,"funnel"),hdt.forEach(t),Fgr=r(C0e," \u2014 "),oO=n(C0e,"A",{href:!0});var pdt=s(oO);Cgr=r(pdt,"TFFunnelForQuestionAnswering"),pdt.forEach(t),Mgr=r(C0e," (Funnel Transformer model)"),C0e.forEach(t),Egr=i(re),K8=n(re,"LI",{});var M0e=s(K8);l_e=n(M0e,"STRONG",{});var _dt=s(l_e);ygr=r(_dt,"longformer"),_dt.forEach(t),wgr=r(M0e," \u2014 "),rO=n(M0e,"A",{href:!0});var udt=s(rO);Agr=r(udt,"TFLongformerForQuestionAnswering"),udt.forEach(t),Lgr=r(M0e," (Longformer model)"),M0e.forEach(t),Bgr=i(re),Z8=n(re,"LI",{});var E0e=s(Z8);i_e=n(E0e,"STRONG",{});var bdt=s(i_e);kgr=r(bdt,"mobilebert"),bdt.forEach(t),xgr=r(E0e," \u2014 "),tO=n(E0e,"A",{href:!0});var vdt=s(tO);Rgr=r(vdt,"TFMobileBertForQuestionAnswering"),vdt.forEach(t),Sgr=r(E0e," (MobileBERT model)"),E0e.forEach(t),Pgr=i(re),eF=n(re,"LI",{});var y0e=s(eF);d_e=n(y0e,"STRONG",{});var Tdt=s(d_e);$gr=r(Tdt,"mpnet"),Tdt.forEach(t),Igr=r(y0e," \u2014 "),aO=n(y0e,"A",{href:!0});var Fdt=s(aO);jgr=r(Fdt,"TFMPNetForQuestionAnswering"),Fdt.forEach(t),Ngr=r(y0e," (MPNet model)"),y0e.forEach(t),Dgr=i(re),oF=n(re,"LI",{});var w0e=s(oF);c_e=n(w0e,"STRONG",{});var Cdt=s(c_e);qgr=r(Cdt,"rembert"),Cdt.forEach(t),Ggr=r(w0e," \u2014 "),nO=n(w0e,"A",{href:!0});var Mdt=s(nO);Ogr=r(Mdt,"TFRemBertForQuestionAnswering"),Mdt.forEach(t),Xgr=r(w0e," (RemBERT model)"),w0e.forEach(t),zgr=i(re),rF=n(re,"LI",{});var A0e=s(rF);f_e=n(A0e,"STRONG",{});var Edt=s(f_e);Vgr=r(Edt,"roberta"),Edt.forEach(t),Wgr=r(A0e," \u2014 "),sO=n(A0e,"A",{href:!0});var ydt=s(sO);Qgr=r(ydt,"TFRobertaForQuestionAnswering"),ydt.forEach(t),Hgr=r(A0e," (RoBERTa model)"),A0e.forEach(t),Ugr=i(re),tF=n(re,"LI",{});var L0e=s(tF);m_e=n(L0e,"STRONG",{});var wdt=s(m_e);Jgr=r(wdt,"roformer"),wdt.forEach(t),Ygr=r(L0e," \u2014 "),lO=n(L0e,"A",{href:!0});var Adt=s(lO);Kgr=r(Adt,"TFRoFormerForQuestionAnswering"),Adt.forEach(t),Zgr=r(L0e," (RoFormer model)"),L0e.forEach(t),ehr=i(re),aF=n(re,"LI",{});var B0e=s(aF);g_e=n(B0e,"STRONG",{});var Ldt=s(g_e);ohr=r(Ldt,"xlm"),Ldt.forEach(t),rhr=r(B0e," \u2014 "),iO=n(B0e,"A",{href:!0});var Bdt=s(iO);thr=r(Bdt,"TFXLMForQuestionAnsweringSimple"),Bdt.forEach(t),ahr=r(B0e," (XLM model)"),B0e.forEach(t),nhr=i(re),nF=n(re,"LI",{});var k0e=s(nF);h_e=n(k0e,"STRONG",{});var kdt=s(h_e);shr=r(kdt,"xlm-roberta"),kdt.forEach(t),lhr=r(k0e," \u2014 "),dO=n(k0e,"A",{href:!0});var xdt=s(dO);ihr=r(xdt,"TFXLMRobertaForQuestionAnswering"),xdt.forEach(t),dhr=r(k0e," (XLM-RoBERTa model)"),k0e.forEach(t),chr=i(re),sF=n(re,"LI",{});var x0e=s(sF);p_e=n(x0e,"STRONG",{});var Rdt=s(p_e);fhr=r(Rdt,"xlnet"),Rdt.forEach(t),mhr=r(x0e," \u2014 "),cO=n(x0e,"A",{href:!0});var Sdt=s(cO);ghr=r(Sdt,"TFXLNetForQuestionAnsweringSimple"),Sdt.forEach(t),hhr=r(x0e," (XLNet model)"),x0e.forEach(t),re.forEach(t),phr=i(Ta),__e=n(Ta,"P",{});var Pdt=s(__e);_hr=r(Pdt,"Examples:"),Pdt.forEach(t),uhr=i(Ta),m(rA.$$.fragment,Ta),Ta.forEach(t),Ul.forEach(t),z9e=i(d),Pc=n(d,"H2",{class:!0});var exe=s(Pc);lF=n(exe,"A",{id:!0,class:!0,href:!0});var $dt=s(lF);u_e=n($dt,"SPAN",{});var Idt=s(u_e);m(tA.$$.fragment,Idt),Idt.forEach(t),$dt.forEach(t),bhr=i(exe),b_e=n(exe,"SPAN",{});var jdt=s(b_e);vhr=r(jdt,"TFAutoModelForVision2Seq"),jdt.forEach(t),exe.forEach(t),V9e=i(d),Er=n(d,"DIV",{class:!0});var Yl=s(Er);m(aA.$$.fragment,Yl),Thr=i(Yl),$c=n(Yl,"P",{});var _V=s($c);Fhr=r(_V,`This is a generic model class that will be instantiated as one of the model classes of the library (with a vision-to-text modeling head) when created
with the `),v_e=n(_V,"CODE",{});var Ndt=s(v_e);Chr=r(Ndt,"from_pretrained()"),Ndt.forEach(t),Mhr=r(_V,"class method or the "),T_e=n(_V,"CODE",{});var Ddt=s(T_e);Ehr=r(Ddt,"from_config()"),Ddt.forEach(t),yhr=r(_V,`class
method.`),_V.forEach(t),whr=i(Yl),nA=n(Yl,"P",{});var oxe=s(nA);Ahr=r(oxe,"This class cannot be instantiated directly using "),F_e=n(oxe,"CODE",{});var qdt=s(F_e);Lhr=r(qdt,"__init__()"),qdt.forEach(t),Bhr=r(oxe," (throws an error)."),oxe.forEach(t),khr=i(Yl),ut=n(Yl,"DIV",{class:!0});var Kl=s(ut);m(sA.$$.fragment,Kl),xhr=i(Kl),C_e=n(Kl,"P",{});var Gdt=s(C_e);Rhr=r(Gdt,"Instantiates one of the model classes of the library (with a vision-to-text modeling head) from a configuration."),Gdt.forEach(t),Shr=i(Kl),Ic=n(Kl,"P",{});var uV=s(Ic);Phr=r(uV,`Note:
Loading a model from its configuration file does `),M_e=n(uV,"STRONG",{});var Odt=s(M_e);$hr=r(Odt,"not"),Odt.forEach(t),Ihr=r(uV,` load the model weights. It only affects the
model\u2019s configuration. Use `),E_e=n(uV,"CODE",{});var Xdt=s(E_e);jhr=r(Xdt,"from_pretrained()"),Xdt.forEach(t),Nhr=r(uV,"to load the model weights."),uV.forEach(t),Dhr=i(Kl),y_e=n(Kl,"P",{});var zdt=s(y_e);qhr=r(zdt,"Examples:"),zdt.forEach(t),Ghr=i(Kl),m(lA.$$.fragment,Kl),Kl.forEach(t),Ohr=i(Yl),Eo=n(Yl,"DIV",{class:!0});var Fa=s(Eo);m(iA.$$.fragment,Fa),Xhr=i(Fa),w_e=n(Fa,"P",{});var Vdt=s(w_e);zhr=r(Vdt,"Instantiate one of the model classes of the library (with a vision-to-text modeling head) from a pretrained model."),Vdt.forEach(t),Vhr=i(Fa),vn=n(Fa,"P",{});var z4=s(vn);Whr=r(z4,"The model class to instantiate is selected based on the "),A_e=n(z4,"CODE",{});var Wdt=s(A_e);Qhr=r(Wdt,"model_type"),Wdt.forEach(t),Hhr=r(z4,` property of the config object (either
passed as an argument or loaded from `),L_e=n(z4,"CODE",{});var Qdt=s(L_e);Uhr=r(Qdt,"pretrained_model_name_or_path"),Qdt.forEach(t),Jhr=r(z4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),B_e=n(z4,"CODE",{});var Hdt=s(B_e);Yhr=r(Hdt,"pretrained_model_name_or_path"),Hdt.forEach(t),Khr=r(z4,":"),z4.forEach(t),Zhr=i(Fa),k_e=n(Fa,"UL",{});var Udt=s(k_e);iF=n(Udt,"LI",{});var R0e=s(iF);x_e=n(R0e,"STRONG",{});var Jdt=s(x_e);epr=r(Jdt,"vision-encoder-decoder"),Jdt.forEach(t),opr=r(R0e," \u2014 "),fO=n(R0e,"A",{href:!0});var Ydt=s(fO);rpr=r(Ydt,"TFVisionEncoderDecoderModel"),Ydt.forEach(t),tpr=r(R0e," (Vision Encoder decoder model)"),R0e.forEach(t),Udt.forEach(t),apr=i(Fa),R_e=n(Fa,"P",{});var Kdt=s(R_e);npr=r(Kdt,"Examples:"),Kdt.forEach(t),spr=i(Fa),m(dA.$$.fragment,Fa),Fa.forEach(t),Yl.forEach(t),W9e=i(d),jc=n(d,"H2",{class:!0});var rxe=s(jc);dF=n(rxe,"A",{id:!0,class:!0,href:!0});var Zdt=s(dF);S_e=n(Zdt,"SPAN",{});var ect=s(S_e);m(cA.$$.fragment,ect),ect.forEach(t),Zdt.forEach(t),lpr=i(rxe),P_e=n(rxe,"SPAN",{});var oct=s(P_e);ipr=r(oct,"TFAutoModelForSpeechSeq2Seq"),oct.forEach(t),rxe.forEach(t),Q9e=i(d),yr=n(d,"DIV",{class:!0});var Zl=s(yr);m(fA.$$.fragment,Zl),dpr=i(Zl),Nc=n(Zl,"P",{});var bV=s(Nc);cpr=r(bV,`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) when created
with the `),$_e=n(bV,"CODE",{});var rct=s($_e);fpr=r(rct,"from_pretrained()"),rct.forEach(t),mpr=r(bV,"class method or the "),I_e=n(bV,"CODE",{});var tct=s(I_e);gpr=r(tct,"from_config()"),tct.forEach(t),hpr=r(bV,`class
method.`),bV.forEach(t),ppr=i(Zl),mA=n(Zl,"P",{});var txe=s(mA);_pr=r(txe,"This class cannot be instantiated directly using "),j_e=n(txe,"CODE",{});var act=s(j_e);upr=r(act,"__init__()"),act.forEach(t),bpr=r(txe," (throws an error)."),txe.forEach(t),vpr=i(Zl),bt=n(Zl,"DIV",{class:!0});var ei=s(bt);m(gA.$$.fragment,ei),Tpr=i(ei),N_e=n(ei,"P",{});var nct=s(N_e);Fpr=r(nct,"Instantiates one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) from a configuration."),nct.forEach(t),Cpr=i(ei),Dc=n(ei,"P",{});var vV=s(Dc);Mpr=r(vV,`Note:
Loading a model from its configuration file does `),D_e=n(vV,"STRONG",{});var sct=s(D_e);Epr=r(sct,"not"),sct.forEach(t),ypr=r(vV,` load the model weights. It only affects the
model\u2019s configuration. Use `),q_e=n(vV,"CODE",{});var lct=s(q_e);wpr=r(lct,"from_pretrained()"),lct.forEach(t),Apr=r(vV,"to load the model weights."),vV.forEach(t),Lpr=i(ei),G_e=n(ei,"P",{});var ict=s(G_e);Bpr=r(ict,"Examples:"),ict.forEach(t),kpr=i(ei),m(hA.$$.fragment,ei),ei.forEach(t),xpr=i(Zl),yo=n(Zl,"DIV",{class:!0});var Ca=s(yo);m(pA.$$.fragment,Ca),Rpr=i(Ca),O_e=n(Ca,"P",{});var dct=s(O_e);Spr=r(dct,"Instantiate one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) from a pretrained model."),dct.forEach(t),Ppr=i(Ca),Tn=n(Ca,"P",{});var V4=s(Tn);$pr=r(V4,"The model class to instantiate is selected based on the "),X_e=n(V4,"CODE",{});var cct=s(X_e);Ipr=r(cct,"model_type"),cct.forEach(t),jpr=r(V4,` property of the config object (either
passed as an argument or loaded from `),z_e=n(V4,"CODE",{});var fct=s(z_e);Npr=r(fct,"pretrained_model_name_or_path"),fct.forEach(t),Dpr=r(V4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),V_e=n(V4,"CODE",{});var mct=s(V_e);qpr=r(mct,"pretrained_model_name_or_path"),mct.forEach(t),Gpr=r(V4,":"),V4.forEach(t),Opr=i(Ca),W_e=n(Ca,"UL",{});var gct=s(W_e);cF=n(gct,"LI",{});var S0e=s(cF);Q_e=n(S0e,"STRONG",{});var hct=s(Q_e);Xpr=r(hct,"speech_to_text"),hct.forEach(t),zpr=r(S0e," \u2014 "),mO=n(S0e,"A",{href:!0});var pct=s(mO);Vpr=r(pct,"TFSpeech2TextForConditionalGeneration"),pct.forEach(t),Wpr=r(S0e," (Speech2Text model)"),S0e.forEach(t),gct.forEach(t),Qpr=i(Ca),H_e=n(Ca,"P",{});var _ct=s(H_e);Hpr=r(_ct,"Examples:"),_ct.forEach(t),Upr=i(Ca),m(_A.$$.fragment,Ca),Ca.forEach(t),Zl.forEach(t),H9e=i(d),qc=n(d,"H2",{class:!0});var axe=s(qc);fF=n(axe,"A",{id:!0,class:!0,href:!0});var uct=s(fF);U_e=n(uct,"SPAN",{});var bct=s(U_e);m(uA.$$.fragment,bct),bct.forEach(t),uct.forEach(t),Jpr=i(axe),J_e=n(axe,"SPAN",{});var vct=s(J_e);Ypr=r(vct,"FlaxAutoModel"),vct.forEach(t),axe.forEach(t),U9e=i(d),wr=n(d,"DIV",{class:!0});var oi=s(wr);m(bA.$$.fragment,oi),Kpr=i(oi),Gc=n(oi,"P",{});var TV=s(Gc);Zpr=r(TV,`This is a generic model class that will be instantiated as one of the base model classes of the library when created
with the `),Y_e=n(TV,"CODE",{});var Tct=s(Y_e);e_r=r(Tct,"from_pretrained()"),Tct.forEach(t),o_r=r(TV,"class method or the "),K_e=n(TV,"CODE",{});var Fct=s(K_e);r_r=r(Fct,"from_config()"),Fct.forEach(t),t_r=r(TV,`class
method.`),TV.forEach(t),a_r=i(oi),vA=n(oi,"P",{});var nxe=s(vA);n_r=r(nxe,"This class cannot be instantiated directly using "),Z_e=n(nxe,"CODE",{});var Cct=s(Z_e);s_r=r(Cct,"__init__()"),Cct.forEach(t),l_r=r(nxe," (throws an error)."),nxe.forEach(t),i_r=i(oi),vt=n(oi,"DIV",{class:!0});var ri=s(vt);m(TA.$$.fragment,ri),d_r=i(ri),eue=n(ri,"P",{});var Mct=s(eue);c_r=r(Mct,"Instantiates one of the base model classes of the library from a configuration."),Mct.forEach(t),f_r=i(ri),Oc=n(ri,"P",{});var FV=s(Oc);m_r=r(FV,`Note:
Loading a model from its configuration file does `),oue=n(FV,"STRONG",{});var Ect=s(oue);g_r=r(Ect,"not"),Ect.forEach(t),h_r=r(FV,` load the model weights. It only affects the
model\u2019s configuration. Use `),rue=n(FV,"CODE",{});var yct=s(rue);p_r=r(yct,"from_pretrained()"),yct.forEach(t),__r=r(FV,"to load the model weights."),FV.forEach(t),u_r=i(ri),tue=n(ri,"P",{});var wct=s(tue);b_r=r(wct,"Examples:"),wct.forEach(t),v_r=i(ri),m(FA.$$.fragment,ri),ri.forEach(t),T_r=i(oi),wo=n(oi,"DIV",{class:!0});var Ma=s(wo);m(CA.$$.fragment,Ma),F_r=i(Ma),aue=n(Ma,"P",{});var Act=s(aue);C_r=r(Act,"Instantiate one of the base model classes of the library from a pretrained model."),Act.forEach(t),M_r=i(Ma),Fn=n(Ma,"P",{});var W4=s(Fn);E_r=r(W4,"The model class to instantiate is selected based on the "),nue=n(W4,"CODE",{});var Lct=s(nue);y_r=r(Lct,"model_type"),Lct.forEach(t),w_r=r(W4,` property of the config object (either
passed as an argument or loaded from `),sue=n(W4,"CODE",{});var Bct=s(sue);A_r=r(Bct,"pretrained_model_name_or_path"),Bct.forEach(t),L_r=r(W4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),lue=n(W4,"CODE",{});var kct=s(lue);B_r=r(kct,"pretrained_model_name_or_path"),kct.forEach(t),k_r=r(W4,":"),W4.forEach(t),x_r=i(Ma),V=n(Ma,"UL",{});var Q=s(V);mF=n(Q,"LI",{});var P0e=s(mF);iue=n(P0e,"STRONG",{});var xct=s(iue);R_r=r(xct,"albert"),xct.forEach(t),S_r=r(P0e," \u2014 "),gO=n(P0e,"A",{href:!0});var Rct=s(gO);P_r=r(Rct,"FlaxAlbertModel"),Rct.forEach(t),$_r=r(P0e," (ALBERT model)"),P0e.forEach(t),I_r=i(Q),gF=n(Q,"LI",{});var $0e=s(gF);due=n($0e,"STRONG",{});var Sct=s(due);j_r=r(Sct,"bart"),Sct.forEach(t),N_r=r($0e," \u2014 "),hO=n($0e,"A",{href:!0});var Pct=s(hO);D_r=r(Pct,"FlaxBartModel"),Pct.forEach(t),q_r=r($0e," (BART model)"),$0e.forEach(t),G_r=i(Q),hF=n(Q,"LI",{});var I0e=s(hF);cue=n(I0e,"STRONG",{});var $ct=s(cue);O_r=r($ct,"beit"),$ct.forEach(t),X_r=r(I0e," \u2014 "),pO=n(I0e,"A",{href:!0});var Ict=s(pO);z_r=r(Ict,"FlaxBeitModel"),Ict.forEach(t),V_r=r(I0e," (BEiT model)"),I0e.forEach(t),W_r=i(Q),pF=n(Q,"LI",{});var j0e=s(pF);fue=n(j0e,"STRONG",{});var jct=s(fue);Q_r=r(jct,"bert"),jct.forEach(t),H_r=r(j0e," \u2014 "),_O=n(j0e,"A",{href:!0});var Nct=s(_O);U_r=r(Nct,"FlaxBertModel"),Nct.forEach(t),J_r=r(j0e," (BERT model)"),j0e.forEach(t),Y_r=i(Q),_F=n(Q,"LI",{});var N0e=s(_F);mue=n(N0e,"STRONG",{});var Dct=s(mue);K_r=r(Dct,"big_bird"),Dct.forEach(t),Z_r=r(N0e," \u2014 "),uO=n(N0e,"A",{href:!0});var qct=s(uO);eur=r(qct,"FlaxBigBirdModel"),qct.forEach(t),our=r(N0e," (BigBird model)"),N0e.forEach(t),rur=i(Q),uF=n(Q,"LI",{});var D0e=s(uF);gue=n(D0e,"STRONG",{});var Gct=s(gue);tur=r(Gct,"blenderbot"),Gct.forEach(t),aur=r(D0e," \u2014 "),bO=n(D0e,"A",{href:!0});var Oct=s(bO);nur=r(Oct,"FlaxBlenderbotModel"),Oct.forEach(t),sur=r(D0e," (Blenderbot model)"),D0e.forEach(t),lur=i(Q),bF=n(Q,"LI",{});var q0e=s(bF);hue=n(q0e,"STRONG",{});var Xct=s(hue);iur=r(Xct,"blenderbot-small"),Xct.forEach(t),dur=r(q0e," \u2014 "),vO=n(q0e,"A",{href:!0});var zct=s(vO);cur=r(zct,"FlaxBlenderbotSmallModel"),zct.forEach(t),fur=r(q0e," (BlenderbotSmall model)"),q0e.forEach(t),mur=i(Q),vF=n(Q,"LI",{});var G0e=s(vF);pue=n(G0e,"STRONG",{});var Vct=s(pue);gur=r(Vct,"clip"),Vct.forEach(t),hur=r(G0e," \u2014 "),TO=n(G0e,"A",{href:!0});var Wct=s(TO);pur=r(Wct,"FlaxCLIPModel"),Wct.forEach(t),_ur=r(G0e," (CLIP model)"),G0e.forEach(t),uur=i(Q),TF=n(Q,"LI",{});var O0e=s(TF);_ue=n(O0e,"STRONG",{});var Qct=s(_ue);bur=r(Qct,"distilbert"),Qct.forEach(t),vur=r(O0e," \u2014 "),FO=n(O0e,"A",{href:!0});var Hct=s(FO);Tur=r(Hct,"FlaxDistilBertModel"),Hct.forEach(t),Fur=r(O0e," (DistilBERT model)"),O0e.forEach(t),Cur=i(Q),FF=n(Q,"LI",{});var X0e=s(FF);uue=n(X0e,"STRONG",{});var Uct=s(uue);Mur=r(Uct,"electra"),Uct.forEach(t),Eur=r(X0e," \u2014 "),CO=n(X0e,"A",{href:!0});var Jct=s(CO);yur=r(Jct,"FlaxElectraModel"),Jct.forEach(t),wur=r(X0e," (ELECTRA model)"),X0e.forEach(t),Aur=i(Q),CF=n(Q,"LI",{});var z0e=s(CF);bue=n(z0e,"STRONG",{});var Yct=s(bue);Lur=r(Yct,"gpt2"),Yct.forEach(t),Bur=r(z0e," \u2014 "),MO=n(z0e,"A",{href:!0});var Kct=s(MO);kur=r(Kct,"FlaxGPT2Model"),Kct.forEach(t),xur=r(z0e," (OpenAI GPT-2 model)"),z0e.forEach(t),Rur=i(Q),MF=n(Q,"LI",{});var V0e=s(MF);vue=n(V0e,"STRONG",{});var Zct=s(vue);Sur=r(Zct,"gpt_neo"),Zct.forEach(t),Pur=r(V0e," \u2014 "),EO=n(V0e,"A",{href:!0});var eft=s(EO);$ur=r(eft,"FlaxGPTNeoModel"),eft.forEach(t),Iur=r(V0e," (GPT Neo model)"),V0e.forEach(t),jur=i(Q),EF=n(Q,"LI",{});var W0e=s(EF);Tue=n(W0e,"STRONG",{});var oft=s(Tue);Nur=r(oft,"gptj"),oft.forEach(t),Dur=r(W0e," \u2014 "),yO=n(W0e,"A",{href:!0});var rft=s(yO);qur=r(rft,"FlaxGPTJModel"),rft.forEach(t),Gur=r(W0e," (GPT-J model)"),W0e.forEach(t),Our=i(Q),yF=n(Q,"LI",{});var Q0e=s(yF);Fue=n(Q0e,"STRONG",{});var tft=s(Fue);Xur=r(tft,"marian"),tft.forEach(t),zur=r(Q0e," \u2014 "),wO=n(Q0e,"A",{href:!0});var aft=s(wO);Vur=r(aft,"FlaxMarianModel"),aft.forEach(t),Wur=r(Q0e," (Marian model)"),Q0e.forEach(t),Qur=i(Q),wF=n(Q,"LI",{});var H0e=s(wF);Cue=n(H0e,"STRONG",{});var nft=s(Cue);Hur=r(nft,"mbart"),nft.forEach(t),Uur=r(H0e," \u2014 "),AO=n(H0e,"A",{href:!0});var sft=s(AO);Jur=r(sft,"FlaxMBartModel"),sft.forEach(t),Yur=r(H0e," (mBART model)"),H0e.forEach(t),Kur=i(Q),AF=n(Q,"LI",{});var U0e=s(AF);Mue=n(U0e,"STRONG",{});var lft=s(Mue);Zur=r(lft,"mt5"),lft.forEach(t),e2r=r(U0e," \u2014 "),LO=n(U0e,"A",{href:!0});var ift=s(LO);o2r=r(ift,"FlaxMT5Model"),ift.forEach(t),r2r=r(U0e," (mT5 model)"),U0e.forEach(t),t2r=i(Q),LF=n(Q,"LI",{});var J0e=s(LF);Eue=n(J0e,"STRONG",{});var dft=s(Eue);a2r=r(dft,"pegasus"),dft.forEach(t),n2r=r(J0e," \u2014 "),BO=n(J0e,"A",{href:!0});var cft=s(BO);s2r=r(cft,"FlaxPegasusModel"),cft.forEach(t),l2r=r(J0e," (Pegasus model)"),J0e.forEach(t),i2r=i(Q),BF=n(Q,"LI",{});var Y0e=s(BF);yue=n(Y0e,"STRONG",{});var fft=s(yue);d2r=r(fft,"roberta"),fft.forEach(t),c2r=r(Y0e," \u2014 "),kO=n(Y0e,"A",{href:!0});var mft=s(kO);f2r=r(mft,"FlaxRobertaModel"),mft.forEach(t),m2r=r(Y0e," (RoBERTa model)"),Y0e.forEach(t),g2r=i(Q),kF=n(Q,"LI",{});var K0e=s(kF);wue=n(K0e,"STRONG",{});var gft=s(wue);h2r=r(gft,"roformer"),gft.forEach(t),p2r=r(K0e," \u2014 "),xO=n(K0e,"A",{href:!0});var hft=s(xO);_2r=r(hft,"FlaxRoFormerModel"),hft.forEach(t),u2r=r(K0e," (RoFormer model)"),K0e.forEach(t),b2r=i(Q),xF=n(Q,"LI",{});var Z0e=s(xF);Aue=n(Z0e,"STRONG",{});var pft=s(Aue);v2r=r(pft,"t5"),pft.forEach(t),T2r=r(Z0e," \u2014 "),RO=n(Z0e,"A",{href:!0});var _ft=s(RO);F2r=r(_ft,"FlaxT5Model"),_ft.forEach(t),C2r=r(Z0e," (T5 model)"),Z0e.forEach(t),M2r=i(Q),RF=n(Q,"LI",{});var eLe=s(RF);Lue=n(eLe,"STRONG",{});var uft=s(Lue);E2r=r(uft,"vision-text-dual-encoder"),uft.forEach(t),y2r=r(eLe," \u2014 "),SO=n(eLe,"A",{href:!0});var bft=s(SO);w2r=r(bft,"FlaxVisionTextDualEncoderModel"),bft.forEach(t),A2r=r(eLe," (VisionTextDualEncoder model)"),eLe.forEach(t),L2r=i(Q),SF=n(Q,"LI",{});var oLe=s(SF);Bue=n(oLe,"STRONG",{});var vft=s(Bue);B2r=r(vft,"vit"),vft.forEach(t),k2r=r(oLe," \u2014 "),PO=n(oLe,"A",{href:!0});var Tft=s(PO);x2r=r(Tft,"FlaxViTModel"),Tft.forEach(t),R2r=r(oLe," (ViT model)"),oLe.forEach(t),S2r=i(Q),PF=n(Q,"LI",{});var rLe=s(PF);kue=n(rLe,"STRONG",{});var Fft=s(kue);P2r=r(Fft,"wav2vec2"),Fft.forEach(t),$2r=r(rLe," \u2014 "),$O=n(rLe,"A",{href:!0});var Cft=s($O);I2r=r(Cft,"FlaxWav2Vec2Model"),Cft.forEach(t),j2r=r(rLe," (Wav2Vec2 model)"),rLe.forEach(t),N2r=i(Q),$F=n(Q,"LI",{});var tLe=s($F);xue=n(tLe,"STRONG",{});var Mft=s(xue);D2r=r(Mft,"xglm"),Mft.forEach(t),q2r=r(tLe," \u2014 "),IO=n(tLe,"A",{href:!0});var Eft=s(IO);G2r=r(Eft,"FlaxXGLMModel"),Eft.forEach(t),O2r=r(tLe," (XGLM model)"),tLe.forEach(t),Q.forEach(t),X2r=i(Ma),Rue=n(Ma,"P",{});var yft=s(Rue);z2r=r(yft,"Examples:"),yft.forEach(t),V2r=i(Ma),m(MA.$$.fragment,Ma),Ma.forEach(t),oi.forEach(t),J9e=i(d),Xc=n(d,"H2",{class:!0});var sxe=s(Xc);IF=n(sxe,"A",{id:!0,class:!0,href:!0});var wft=s(IF);Sue=n(wft,"SPAN",{});var Aft=s(Sue);m(EA.$$.fragment,Aft),Aft.forEach(t),wft.forEach(t),W2r=i(sxe),Pue=n(sxe,"SPAN",{});var Lft=s(Pue);Q2r=r(Lft,"FlaxAutoModelForCausalLM"),Lft.forEach(t),sxe.forEach(t),Y9e=i(d),Ar=n(d,"DIV",{class:!0});var ti=s(Ar);m(yA.$$.fragment,ti),H2r=i(ti),zc=n(ti,"P",{});var CV=s(zc);U2r=r(CV,`This is a generic model class that will be instantiated as one of the model classes of the library (with a causal language modeling head) when created
with the `),$ue=n(CV,"CODE",{});var Bft=s($ue);J2r=r(Bft,"from_pretrained()"),Bft.forEach(t),Y2r=r(CV,"class method or the "),Iue=n(CV,"CODE",{});var kft=s(Iue);K2r=r(kft,"from_config()"),kft.forEach(t),Z2r=r(CV,`class
method.`),CV.forEach(t),e1r=i(ti),wA=n(ti,"P",{});var lxe=s(wA);o1r=r(lxe,"This class cannot be instantiated directly using "),jue=n(lxe,"CODE",{});var xft=s(jue);r1r=r(xft,"__init__()"),xft.forEach(t),t1r=r(lxe," (throws an error)."),lxe.forEach(t),a1r=i(ti),Tt=n(ti,"DIV",{class:!0});var ai=s(Tt);m(AA.$$.fragment,ai),n1r=i(ai),Nue=n(ai,"P",{});var Rft=s(Nue);s1r=r(Rft,"Instantiates one of the model classes of the library (with a causal language modeling head) from a configuration."),Rft.forEach(t),l1r=i(ai),Vc=n(ai,"P",{});var MV=s(Vc);i1r=r(MV,`Note:
Loading a model from its configuration file does `),Due=n(MV,"STRONG",{});var Sft=s(Due);d1r=r(Sft,"not"),Sft.forEach(t),c1r=r(MV,` load the model weights. It only affects the
model\u2019s configuration. Use `),que=n(MV,"CODE",{});var Pft=s(que);f1r=r(Pft,"from_pretrained()"),Pft.forEach(t),m1r=r(MV,"to load the model weights."),MV.forEach(t),g1r=i(ai),Gue=n(ai,"P",{});var $ft=s(Gue);h1r=r($ft,"Examples:"),$ft.forEach(t),p1r=i(ai),m(LA.$$.fragment,ai),ai.forEach(t),_1r=i(ti),Ao=n(ti,"DIV",{class:!0});var Ea=s(Ao);m(BA.$$.fragment,Ea),u1r=i(Ea),Oue=n(Ea,"P",{});var Ift=s(Oue);b1r=r(Ift,"Instantiate one of the model classes of the library (with a causal language modeling head) from a pretrained model."),Ift.forEach(t),v1r=i(Ea),Cn=n(Ea,"P",{});var Q4=s(Cn);T1r=r(Q4,"The model class to instantiate is selected based on the "),Xue=n(Q4,"CODE",{});var jft=s(Xue);F1r=r(jft,"model_type"),jft.forEach(t),C1r=r(Q4,` property of the config object (either
passed as an argument or loaded from `),zue=n(Q4,"CODE",{});var Nft=s(zue);M1r=r(Nft,"pretrained_model_name_or_path"),Nft.forEach(t),E1r=r(Q4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Vue=n(Q4,"CODE",{});var Dft=s(Vue);y1r=r(Dft,"pretrained_model_name_or_path"),Dft.forEach(t),w1r=r(Q4,":"),Q4.forEach(t),A1r=i(Ea),Mn=n(Ea,"UL",{});var H4=s(Mn);jF=n(H4,"LI",{});var aLe=s(jF);Wue=n(aLe,"STRONG",{});var qft=s(Wue);L1r=r(qft,"gpt2"),qft.forEach(t),B1r=r(aLe," \u2014 "),jO=n(aLe,"A",{href:!0});var Gft=s(jO);k1r=r(Gft,"FlaxGPT2LMHeadModel"),Gft.forEach(t),x1r=r(aLe," (OpenAI GPT-2 model)"),aLe.forEach(t),R1r=i(H4),NF=n(H4,"LI",{});var nLe=s(NF);Que=n(nLe,"STRONG",{});var Oft=s(Que);S1r=r(Oft,"gpt_neo"),Oft.forEach(t),P1r=r(nLe," \u2014 "),NO=n(nLe,"A",{href:!0});var Xft=s(NO);$1r=r(Xft,"FlaxGPTNeoForCausalLM"),Xft.forEach(t),I1r=r(nLe," (GPT Neo model)"),nLe.forEach(t),j1r=i(H4),DF=n(H4,"LI",{});var sLe=s(DF);Hue=n(sLe,"STRONG",{});var zft=s(Hue);N1r=r(zft,"gptj"),zft.forEach(t),D1r=r(sLe," \u2014 "),DO=n(sLe,"A",{href:!0});var Vft=s(DO);q1r=r(Vft,"FlaxGPTJForCausalLM"),Vft.forEach(t),G1r=r(sLe," (GPT-J model)"),sLe.forEach(t),O1r=i(H4),qF=n(H4,"LI",{});var lLe=s(qF);Uue=n(lLe,"STRONG",{});var Wft=s(Uue);X1r=r(Wft,"xglm"),Wft.forEach(t),z1r=r(lLe," \u2014 "),qO=n(lLe,"A",{href:!0});var Qft=s(qO);V1r=r(Qft,"FlaxXGLMForCausalLM"),Qft.forEach(t),W1r=r(lLe," (XGLM model)"),lLe.forEach(t),H4.forEach(t),Q1r=i(Ea),Jue=n(Ea,"P",{});var Hft=s(Jue);H1r=r(Hft,"Examples:"),Hft.forEach(t),U1r=i(Ea),m(kA.$$.fragment,Ea),Ea.forEach(t),ti.forEach(t),K9e=i(d),Wc=n(d,"H2",{class:!0});var ixe=s(Wc);GF=n(ixe,"A",{id:!0,class:!0,href:!0});var Uft=s(GF);Yue=n(Uft,"SPAN",{});var Jft=s(Yue);m(xA.$$.fragment,Jft),Jft.forEach(t),Uft.forEach(t),J1r=i(ixe),Kue=n(ixe,"SPAN",{});var Yft=s(Kue);Y1r=r(Yft,"FlaxAutoModelForPreTraining"),Yft.forEach(t),ixe.forEach(t),Z9e=i(d),Lr=n(d,"DIV",{class:!0});var ni=s(Lr);m(RA.$$.fragment,ni),K1r=i(ni),Qc=n(ni,"P",{});var EV=s(Qc);Z1r=r(EV,`This is a generic model class that will be instantiated as one of the model classes of the library (with a pretraining head) when created
with the `),Zue=n(EV,"CODE",{});var Kft=s(Zue);ebr=r(Kft,"from_pretrained()"),Kft.forEach(t),obr=r(EV,"class method or the "),e2e=n(EV,"CODE",{});var Zft=s(e2e);rbr=r(Zft,"from_config()"),Zft.forEach(t),tbr=r(EV,`class
method.`),EV.forEach(t),abr=i(ni),SA=n(ni,"P",{});var dxe=s(SA);nbr=r(dxe,"This class cannot be instantiated directly using "),o2e=n(dxe,"CODE",{});var emt=s(o2e);sbr=r(emt,"__init__()"),emt.forEach(t),lbr=r(dxe," (throws an error)."),dxe.forEach(t),ibr=i(ni),Ft=n(ni,"DIV",{class:!0});var si=s(Ft);m(PA.$$.fragment,si),dbr=i(si),r2e=n(si,"P",{});var omt=s(r2e);cbr=r(omt,"Instantiates one of the model classes of the library (with a pretraining head) from a configuration."),omt.forEach(t),fbr=i(si),Hc=n(si,"P",{});var yV=s(Hc);mbr=r(yV,`Note:
Loading a model from its configuration file does `),t2e=n(yV,"STRONG",{});var rmt=s(t2e);gbr=r(rmt,"not"),rmt.forEach(t),hbr=r(yV,` load the model weights. It only affects the
model\u2019s configuration. Use `),a2e=n(yV,"CODE",{});var tmt=s(a2e);pbr=r(tmt,"from_pretrained()"),tmt.forEach(t),_br=r(yV,"to load the model weights."),yV.forEach(t),ubr=i(si),n2e=n(si,"P",{});var amt=s(n2e);bbr=r(amt,"Examples:"),amt.forEach(t),vbr=i(si),m($A.$$.fragment,si),si.forEach(t),Tbr=i(ni),Lo=n(ni,"DIV",{class:!0});var ya=s(Lo);m(IA.$$.fragment,ya),Fbr=i(ya),s2e=n(ya,"P",{});var nmt=s(s2e);Cbr=r(nmt,"Instantiate one of the model classes of the library (with a pretraining head) from a pretrained model."),nmt.forEach(t),Mbr=i(ya),En=n(ya,"P",{});var U4=s(En);Ebr=r(U4,"The model class to instantiate is selected based on the "),l2e=n(U4,"CODE",{});var smt=s(l2e);ybr=r(smt,"model_type"),smt.forEach(t),wbr=r(U4,` property of the config object (either
passed as an argument or loaded from `),i2e=n(U4,"CODE",{});var lmt=s(i2e);Abr=r(lmt,"pretrained_model_name_or_path"),lmt.forEach(t),Lbr=r(U4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),d2e=n(U4,"CODE",{});var imt=s(d2e);Bbr=r(imt,"pretrained_model_name_or_path"),imt.forEach(t),kbr=r(U4,":"),U4.forEach(t),xbr=i(ya),fe=n(ya,"UL",{});var _e=s(fe);OF=n(_e,"LI",{});var iLe=s(OF);c2e=n(iLe,"STRONG",{});var dmt=s(c2e);Rbr=r(dmt,"albert"),dmt.forEach(t),Sbr=r(iLe," \u2014 "),GO=n(iLe,"A",{href:!0});var cmt=s(GO);Pbr=r(cmt,"FlaxAlbertForPreTraining"),cmt.forEach(t),$br=r(iLe," (ALBERT model)"),iLe.forEach(t),Ibr=i(_e),XF=n(_e,"LI",{});var dLe=s(XF);f2e=n(dLe,"STRONG",{});var fmt=s(f2e);jbr=r(fmt,"bart"),fmt.forEach(t),Nbr=r(dLe," \u2014 "),OO=n(dLe,"A",{href:!0});var mmt=s(OO);Dbr=r(mmt,"FlaxBartForConditionalGeneration"),mmt.forEach(t),qbr=r(dLe," (BART model)"),dLe.forEach(t),Gbr=i(_e),zF=n(_e,"LI",{});var cLe=s(zF);m2e=n(cLe,"STRONG",{});var gmt=s(m2e);Obr=r(gmt,"bert"),gmt.forEach(t),Xbr=r(cLe," \u2014 "),XO=n(cLe,"A",{href:!0});var hmt=s(XO);zbr=r(hmt,"FlaxBertForPreTraining"),hmt.forEach(t),Vbr=r(cLe," (BERT model)"),cLe.forEach(t),Wbr=i(_e),VF=n(_e,"LI",{});var fLe=s(VF);g2e=n(fLe,"STRONG",{});var pmt=s(g2e);Qbr=r(pmt,"big_bird"),pmt.forEach(t),Hbr=r(fLe," \u2014 "),zO=n(fLe,"A",{href:!0});var _mt=s(zO);Ubr=r(_mt,"FlaxBigBirdForPreTraining"),_mt.forEach(t),Jbr=r(fLe," (BigBird model)"),fLe.forEach(t),Ybr=i(_e),WF=n(_e,"LI",{});var mLe=s(WF);h2e=n(mLe,"STRONG",{});var umt=s(h2e);Kbr=r(umt,"electra"),umt.forEach(t),Zbr=r(mLe," \u2014 "),VO=n(mLe,"A",{href:!0});var bmt=s(VO);e5r=r(bmt,"FlaxElectraForPreTraining"),bmt.forEach(t),o5r=r(mLe," (ELECTRA model)"),mLe.forEach(t),r5r=i(_e),QF=n(_e,"LI",{});var gLe=s(QF);p2e=n(gLe,"STRONG",{});var vmt=s(p2e);t5r=r(vmt,"mbart"),vmt.forEach(t),a5r=r(gLe," \u2014 "),WO=n(gLe,"A",{href:!0});var Tmt=s(WO);n5r=r(Tmt,"FlaxMBartForConditionalGeneration"),Tmt.forEach(t),s5r=r(gLe," (mBART model)"),gLe.forEach(t),l5r=i(_e),HF=n(_e,"LI",{});var hLe=s(HF);_2e=n(hLe,"STRONG",{});var Fmt=s(_2e);i5r=r(Fmt,"mt5"),Fmt.forEach(t),d5r=r(hLe," \u2014 "),QO=n(hLe,"A",{href:!0});var Cmt=s(QO);c5r=r(Cmt,"FlaxMT5ForConditionalGeneration"),Cmt.forEach(t),f5r=r(hLe," (mT5 model)"),hLe.forEach(t),m5r=i(_e),UF=n(_e,"LI",{});var pLe=s(UF);u2e=n(pLe,"STRONG",{});var Mmt=s(u2e);g5r=r(Mmt,"roberta"),Mmt.forEach(t),h5r=r(pLe," \u2014 "),HO=n(pLe,"A",{href:!0});var Emt=s(HO);p5r=r(Emt,"FlaxRobertaForMaskedLM"),Emt.forEach(t),_5r=r(pLe," (RoBERTa model)"),pLe.forEach(t),u5r=i(_e),JF=n(_e,"LI",{});var _Le=s(JF);b2e=n(_Le,"STRONG",{});var ymt=s(b2e);b5r=r(ymt,"roformer"),ymt.forEach(t),v5r=r(_Le," \u2014 "),UO=n(_Le,"A",{href:!0});var wmt=s(UO);T5r=r(wmt,"FlaxRoFormerForMaskedLM"),wmt.forEach(t),F5r=r(_Le," (RoFormer model)"),_Le.forEach(t),C5r=i(_e),YF=n(_e,"LI",{});var uLe=s(YF);v2e=n(uLe,"STRONG",{});var Amt=s(v2e);M5r=r(Amt,"t5"),Amt.forEach(t),E5r=r(uLe," \u2014 "),JO=n(uLe,"A",{href:!0});var Lmt=s(JO);y5r=r(Lmt,"FlaxT5ForConditionalGeneration"),Lmt.forEach(t),w5r=r(uLe," (T5 model)"),uLe.forEach(t),A5r=i(_e),KF=n(_e,"LI",{});var bLe=s(KF);T2e=n(bLe,"STRONG",{});var Bmt=s(T2e);L5r=r(Bmt,"wav2vec2"),Bmt.forEach(t),B5r=r(bLe," \u2014 "),YO=n(bLe,"A",{href:!0});var kmt=s(YO);k5r=r(kmt,"FlaxWav2Vec2ForPreTraining"),kmt.forEach(t),x5r=r(bLe," (Wav2Vec2 model)"),bLe.forEach(t),_e.forEach(t),R5r=i(ya),F2e=n(ya,"P",{});var xmt=s(F2e);S5r=r(xmt,"Examples:"),xmt.forEach(t),P5r=i(ya),m(jA.$$.fragment,ya),ya.forEach(t),ni.forEach(t),eBe=i(d),Uc=n(d,"H2",{class:!0});var cxe=s(Uc);ZF=n(cxe,"A",{id:!0,class:!0,href:!0});var Rmt=s(ZF);C2e=n(Rmt,"SPAN",{});var Smt=s(C2e);m(NA.$$.fragment,Smt),Smt.forEach(t),Rmt.forEach(t),$5r=i(cxe),M2e=n(cxe,"SPAN",{});var Pmt=s(M2e);I5r=r(Pmt,"FlaxAutoModelForMaskedLM"),Pmt.forEach(t),cxe.forEach(t),oBe=i(d),Br=n(d,"DIV",{class:!0});var li=s(Br);m(DA.$$.fragment,li),j5r=i(li),Jc=n(li,"P",{});var wV=s(Jc);N5r=r(wV,`This is a generic model class that will be instantiated as one of the model classes of the library (with a masked language modeling head) when created
with the `),E2e=n(wV,"CODE",{});var $mt=s(E2e);D5r=r($mt,"from_pretrained()"),$mt.forEach(t),q5r=r(wV,"class method or the "),y2e=n(wV,"CODE",{});var Imt=s(y2e);G5r=r(Imt,"from_config()"),Imt.forEach(t),O5r=r(wV,`class
method.`),wV.forEach(t),X5r=i(li),qA=n(li,"P",{});var fxe=s(qA);z5r=r(fxe,"This class cannot be instantiated directly using "),w2e=n(fxe,"CODE",{});var jmt=s(w2e);V5r=r(jmt,"__init__()"),jmt.forEach(t),W5r=r(fxe," (throws an error)."),fxe.forEach(t),Q5r=i(li),Ct=n(li,"DIV",{class:!0});var ii=s(Ct);m(GA.$$.fragment,ii),H5r=i(ii),A2e=n(ii,"P",{});var Nmt=s(A2e);U5r=r(Nmt,"Instantiates one of the model classes of the library (with a masked language modeling head) from a configuration."),Nmt.forEach(t),J5r=i(ii),Yc=n(ii,"P",{});var AV=s(Yc);Y5r=r(AV,`Note:
Loading a model from its configuration file does `),L2e=n(AV,"STRONG",{});var Dmt=s(L2e);K5r=r(Dmt,"not"),Dmt.forEach(t),Z5r=r(AV,` load the model weights. It only affects the
model\u2019s configuration. Use `),B2e=n(AV,"CODE",{});var qmt=s(B2e);evr=r(qmt,"from_pretrained()"),qmt.forEach(t),ovr=r(AV,"to load the model weights."),AV.forEach(t),rvr=i(ii),k2e=n(ii,"P",{});var Gmt=s(k2e);tvr=r(Gmt,"Examples:"),Gmt.forEach(t),avr=i(ii),m(OA.$$.fragment,ii),ii.forEach(t),nvr=i(li),Bo=n(li,"DIV",{class:!0});var wa=s(Bo);m(XA.$$.fragment,wa),svr=i(wa),x2e=n(wa,"P",{});var Omt=s(x2e);lvr=r(Omt,"Instantiate one of the model classes of the library (with a masked language modeling head) from a pretrained model."),Omt.forEach(t),ivr=i(wa),yn=n(wa,"P",{});var J4=s(yn);dvr=r(J4,"The model class to instantiate is selected based on the "),R2e=n(J4,"CODE",{});var Xmt=s(R2e);cvr=r(Xmt,"model_type"),Xmt.forEach(t),fvr=r(J4,` property of the config object (either
passed as an argument or loaded from `),S2e=n(J4,"CODE",{});var zmt=s(S2e);mvr=r(zmt,"pretrained_model_name_or_path"),zmt.forEach(t),gvr=r(J4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),P2e=n(J4,"CODE",{});var Vmt=s(P2e);hvr=r(Vmt,"pretrained_model_name_or_path"),Vmt.forEach(t),pvr=r(J4,":"),J4.forEach(t),_vr=i(wa),ve=n(wa,"UL",{});var Ze=s(ve);eC=n(Ze,"LI",{});var vLe=s(eC);$2e=n(vLe,"STRONG",{});var Wmt=s($2e);uvr=r(Wmt,"albert"),Wmt.forEach(t),bvr=r(vLe," \u2014 "),KO=n(vLe,"A",{href:!0});var Qmt=s(KO);vvr=r(Qmt,"FlaxAlbertForMaskedLM"),Qmt.forEach(t),Tvr=r(vLe," (ALBERT model)"),vLe.forEach(t),Fvr=i(Ze),oC=n(Ze,"LI",{});var TLe=s(oC);I2e=n(TLe,"STRONG",{});var Hmt=s(I2e);Cvr=r(Hmt,"bart"),Hmt.forEach(t),Mvr=r(TLe," \u2014 "),ZO=n(TLe,"A",{href:!0});var Umt=s(ZO);Evr=r(Umt,"FlaxBartForConditionalGeneration"),Umt.forEach(t),yvr=r(TLe," (BART model)"),TLe.forEach(t),wvr=i(Ze),rC=n(Ze,"LI",{});var FLe=s(rC);j2e=n(FLe,"STRONG",{});var Jmt=s(j2e);Avr=r(Jmt,"bert"),Jmt.forEach(t),Lvr=r(FLe," \u2014 "),eX=n(FLe,"A",{href:!0});var Ymt=s(eX);Bvr=r(Ymt,"FlaxBertForMaskedLM"),Ymt.forEach(t),kvr=r(FLe," (BERT model)"),FLe.forEach(t),xvr=i(Ze),tC=n(Ze,"LI",{});var CLe=s(tC);N2e=n(CLe,"STRONG",{});var Kmt=s(N2e);Rvr=r(Kmt,"big_bird"),Kmt.forEach(t),Svr=r(CLe," \u2014 "),oX=n(CLe,"A",{href:!0});var Zmt=s(oX);Pvr=r(Zmt,"FlaxBigBirdForMaskedLM"),Zmt.forEach(t),$vr=r(CLe," (BigBird model)"),CLe.forEach(t),Ivr=i(Ze),aC=n(Ze,"LI",{});var MLe=s(aC);D2e=n(MLe,"STRONG",{});var egt=s(D2e);jvr=r(egt,"distilbert"),egt.forEach(t),Nvr=r(MLe," \u2014 "),rX=n(MLe,"A",{href:!0});var ogt=s(rX);Dvr=r(ogt,"FlaxDistilBertForMaskedLM"),ogt.forEach(t),qvr=r(MLe," (DistilBERT model)"),MLe.forEach(t),Gvr=i(Ze),nC=n(Ze,"LI",{});var ELe=s(nC);q2e=n(ELe,"STRONG",{});var rgt=s(q2e);Ovr=r(rgt,"electra"),rgt.forEach(t),Xvr=r(ELe," \u2014 "),tX=n(ELe,"A",{href:!0});var tgt=s(tX);zvr=r(tgt,"FlaxElectraForMaskedLM"),tgt.forEach(t),Vvr=r(ELe," (ELECTRA model)"),ELe.forEach(t),Wvr=i(Ze),sC=n(Ze,"LI",{});var yLe=s(sC);G2e=n(yLe,"STRONG",{});var agt=s(G2e);Qvr=r(agt,"mbart"),agt.forEach(t),Hvr=r(yLe," \u2014 "),aX=n(yLe,"A",{href:!0});var ngt=s(aX);Uvr=r(ngt,"FlaxMBartForConditionalGeneration"),ngt.forEach(t),Jvr=r(yLe," (mBART model)"),yLe.forEach(t),Yvr=i(Ze),lC=n(Ze,"LI",{});var wLe=s(lC);O2e=n(wLe,"STRONG",{});var sgt=s(O2e);Kvr=r(sgt,"roberta"),sgt.forEach(t),Zvr=r(wLe," \u2014 "),nX=n(wLe,"A",{href:!0});var lgt=s(nX);e6r=r(lgt,"FlaxRobertaForMaskedLM"),lgt.forEach(t),o6r=r(wLe," (RoBERTa model)"),wLe.forEach(t),r6r=i(Ze),iC=n(Ze,"LI",{});var ALe=s(iC);X2e=n(ALe,"STRONG",{});var igt=s(X2e);t6r=r(igt,"roformer"),igt.forEach(t),a6r=r(ALe," \u2014 "),sX=n(ALe,"A",{href:!0});var dgt=s(sX);n6r=r(dgt,"FlaxRoFormerForMaskedLM"),dgt.forEach(t),s6r=r(ALe," (RoFormer model)"),ALe.forEach(t),Ze.forEach(t),l6r=i(wa),z2e=n(wa,"P",{});var cgt=s(z2e);i6r=r(cgt,"Examples:"),cgt.forEach(t),d6r=i(wa),m(zA.$$.fragment,wa),wa.forEach(t),li.forEach(t),rBe=i(d),Kc=n(d,"H2",{class:!0});var mxe=s(Kc);dC=n(mxe,"A",{id:!0,class:!0,href:!0});var fgt=s(dC);V2e=n(fgt,"SPAN",{});var mgt=s(V2e);m(VA.$$.fragment,mgt),mgt.forEach(t),fgt.forEach(t),c6r=i(mxe),W2e=n(mxe,"SPAN",{});var ggt=s(W2e);f6r=r(ggt,"FlaxAutoModelForSeq2SeqLM"),ggt.forEach(t),mxe.forEach(t),tBe=i(d),kr=n(d,"DIV",{class:!0});var di=s(kr);m(WA.$$.fragment,di),m6r=i(di),Zc=n(di,"P",{});var LV=s(Zc);g6r=r(LV,`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence language modeling head) when created
with the `),Q2e=n(LV,"CODE",{});var hgt=s(Q2e);h6r=r(hgt,"from_pretrained()"),hgt.forEach(t),p6r=r(LV,"class method or the "),H2e=n(LV,"CODE",{});var pgt=s(H2e);_6r=r(pgt,"from_config()"),pgt.forEach(t),u6r=r(LV,`class
method.`),LV.forEach(t),b6r=i(di),QA=n(di,"P",{});var gxe=s(QA);v6r=r(gxe,"This class cannot be instantiated directly using "),U2e=n(gxe,"CODE",{});var _gt=s(U2e);T6r=r(_gt,"__init__()"),_gt.forEach(t),F6r=r(gxe," (throws an error)."),gxe.forEach(t),C6r=i(di),Mt=n(di,"DIV",{class:!0});var ci=s(Mt);m(HA.$$.fragment,ci),M6r=i(ci),J2e=n(ci,"P",{});var ugt=s(J2e);E6r=r(ugt,"Instantiates one of the model classes of the library (with a sequence-to-sequence language modeling head) from a configuration."),ugt.forEach(t),y6r=i(ci),ef=n(ci,"P",{});var BV=s(ef);w6r=r(BV,`Note:
Loading a model from its configuration file does `),Y2e=n(BV,"STRONG",{});var bgt=s(Y2e);A6r=r(bgt,"not"),bgt.forEach(t),L6r=r(BV,` load the model weights. It only affects the
model\u2019s configuration. Use `),K2e=n(BV,"CODE",{});var vgt=s(K2e);B6r=r(vgt,"from_pretrained()"),vgt.forEach(t),k6r=r(BV,"to load the model weights."),BV.forEach(t),x6r=i(ci),Z2e=n(ci,"P",{});var Tgt=s(Z2e);R6r=r(Tgt,"Examples:"),Tgt.forEach(t),S6r=i(ci),m(UA.$$.fragment,ci),ci.forEach(t),P6r=i(di),ko=n(di,"DIV",{class:!0});var Aa=s(ko);m(JA.$$.fragment,Aa),$6r=i(Aa),e1e=n(Aa,"P",{});var Fgt=s(e1e);I6r=r(Fgt,"Instantiate one of the model classes of the library (with a sequence-to-sequence language modeling head) from a pretrained model."),Fgt.forEach(t),j6r=i(Aa),wn=n(Aa,"P",{});var Y4=s(wn);N6r=r(Y4,"The model class to instantiate is selected based on the "),o1e=n(Y4,"CODE",{});var Cgt=s(o1e);D6r=r(Cgt,"model_type"),Cgt.forEach(t),q6r=r(Y4,` property of the config object (either
passed as an argument or loaded from `),r1e=n(Y4,"CODE",{});var Mgt=s(r1e);G6r=r(Mgt,"pretrained_model_name_or_path"),Mgt.forEach(t),O6r=r(Y4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),t1e=n(Y4,"CODE",{});var Egt=s(t1e);X6r=r(Egt,"pretrained_model_name_or_path"),Egt.forEach(t),z6r=r(Y4,":"),Y4.forEach(t),V6r=i(Aa),Te=n(Aa,"UL",{});var eo=s(Te);cC=n(eo,"LI",{});var LLe=s(cC);a1e=n(LLe,"STRONG",{});var ygt=s(a1e);W6r=r(ygt,"bart"),ygt.forEach(t),Q6r=r(LLe," \u2014 "),lX=n(LLe,"A",{href:!0});var wgt=s(lX);H6r=r(wgt,"FlaxBartForConditionalGeneration"),wgt.forEach(t),U6r=r(LLe," (BART model)"),LLe.forEach(t),J6r=i(eo),fC=n(eo,"LI",{});var BLe=s(fC);n1e=n(BLe,"STRONG",{});var Agt=s(n1e);Y6r=r(Agt,"blenderbot"),Agt.forEach(t),K6r=r(BLe," \u2014 "),iX=n(BLe,"A",{href:!0});var Lgt=s(iX);Z6r=r(Lgt,"FlaxBlenderbotForConditionalGeneration"),Lgt.forEach(t),eTr=r(BLe," (Blenderbot model)"),BLe.forEach(t),oTr=i(eo),mC=n(eo,"LI",{});var kLe=s(mC);s1e=n(kLe,"STRONG",{});var Bgt=s(s1e);rTr=r(Bgt,"blenderbot-small"),Bgt.forEach(t),tTr=r(kLe," \u2014 "),dX=n(kLe,"A",{href:!0});var kgt=s(dX);aTr=r(kgt,"FlaxBlenderbotSmallForConditionalGeneration"),kgt.forEach(t),nTr=r(kLe," (BlenderbotSmall model)"),kLe.forEach(t),sTr=i(eo),gC=n(eo,"LI",{});var xLe=s(gC);l1e=n(xLe,"STRONG",{});var xgt=s(l1e);lTr=r(xgt,"encoder-decoder"),xgt.forEach(t),iTr=r(xLe," \u2014 "),cX=n(xLe,"A",{href:!0});var Rgt=s(cX);dTr=r(Rgt,"FlaxEncoderDecoderModel"),Rgt.forEach(t),cTr=r(xLe," (Encoder decoder model)"),xLe.forEach(t),fTr=i(eo),hC=n(eo,"LI",{});var RLe=s(hC);i1e=n(RLe,"STRONG",{});var Sgt=s(i1e);mTr=r(Sgt,"marian"),Sgt.forEach(t),gTr=r(RLe," \u2014 "),fX=n(RLe,"A",{href:!0});var Pgt=s(fX);hTr=r(Pgt,"FlaxMarianMTModel"),Pgt.forEach(t),pTr=r(RLe," (Marian model)"),RLe.forEach(t),_Tr=i(eo),pC=n(eo,"LI",{});var SLe=s(pC);d1e=n(SLe,"STRONG",{});var $gt=s(d1e);uTr=r($gt,"mbart"),$gt.forEach(t),bTr=r(SLe," \u2014 "),mX=n(SLe,"A",{href:!0});var Igt=s(mX);vTr=r(Igt,"FlaxMBartForConditionalGeneration"),Igt.forEach(t),TTr=r(SLe," (mBART model)"),SLe.forEach(t),FTr=i(eo),_C=n(eo,"LI",{});var PLe=s(_C);c1e=n(PLe,"STRONG",{});var jgt=s(c1e);CTr=r(jgt,"mt5"),jgt.forEach(t),MTr=r(PLe," \u2014 "),gX=n(PLe,"A",{href:!0});var Ngt=s(gX);ETr=r(Ngt,"FlaxMT5ForConditionalGeneration"),Ngt.forEach(t),yTr=r(PLe," (mT5 model)"),PLe.forEach(t),wTr=i(eo),uC=n(eo,"LI",{});var $Le=s(uC);f1e=n($Le,"STRONG",{});var Dgt=s(f1e);ATr=r(Dgt,"pegasus"),Dgt.forEach(t),LTr=r($Le," \u2014 "),hX=n($Le,"A",{href:!0});var qgt=s(hX);BTr=r(qgt,"FlaxPegasusForConditionalGeneration"),qgt.forEach(t),kTr=r($Le," (Pegasus model)"),$Le.forEach(t),xTr=i(eo),bC=n(eo,"LI",{});var ILe=s(bC);m1e=n(ILe,"STRONG",{});var Ggt=s(m1e);RTr=r(Ggt,"t5"),Ggt.forEach(t),STr=r(ILe," \u2014 "),pX=n(ILe,"A",{href:!0});var Ogt=s(pX);PTr=r(Ogt,"FlaxT5ForConditionalGeneration"),Ogt.forEach(t),$Tr=r(ILe," (T5 model)"),ILe.forEach(t),eo.forEach(t),ITr=i(Aa),g1e=n(Aa,"P",{});var Xgt=s(g1e);jTr=r(Xgt,"Examples:"),Xgt.forEach(t),NTr=i(Aa),m(YA.$$.fragment,Aa),Aa.forEach(t),di.forEach(t),aBe=i(d),of=n(d,"H2",{class:!0});var hxe=s(of);vC=n(hxe,"A",{id:!0,class:!0,href:!0});var zgt=s(vC);h1e=n(zgt,"SPAN",{});var Vgt=s(h1e);m(KA.$$.fragment,Vgt),Vgt.forEach(t),zgt.forEach(t),DTr=i(hxe),p1e=n(hxe,"SPAN",{});var Wgt=s(p1e);qTr=r(Wgt,"FlaxAutoModelForSequenceClassification"),Wgt.forEach(t),hxe.forEach(t),nBe=i(d),xr=n(d,"DIV",{class:!0});var fi=s(xr);m(ZA.$$.fragment,fi),GTr=i(fi),rf=n(fi,"P",{});var kV=s(rf);OTr=r(kV,`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence classification head) when created
with the `),_1e=n(kV,"CODE",{});var Qgt=s(_1e);XTr=r(Qgt,"from_pretrained()"),Qgt.forEach(t),zTr=r(kV,"class method or the "),u1e=n(kV,"CODE",{});var Hgt=s(u1e);VTr=r(Hgt,"from_config()"),Hgt.forEach(t),WTr=r(kV,`class
method.`),kV.forEach(t),QTr=i(fi),e0=n(fi,"P",{});var pxe=s(e0);HTr=r(pxe,"This class cannot be instantiated directly using "),b1e=n(pxe,"CODE",{});var Ugt=s(b1e);UTr=r(Ugt,"__init__()"),Ugt.forEach(t),JTr=r(pxe," (throws an error)."),pxe.forEach(t),YTr=i(fi),Et=n(fi,"DIV",{class:!0});var mi=s(Et);m(o0.$$.fragment,mi),KTr=i(mi),v1e=n(mi,"P",{});var Jgt=s(v1e);ZTr=r(Jgt,"Instantiates one of the model classes of the library (with a sequence classification head) from a configuration."),Jgt.forEach(t),e8r=i(mi),tf=n(mi,"P",{});var xV=s(tf);o8r=r(xV,`Note:
Loading a model from its configuration file does `),T1e=n(xV,"STRONG",{});var Ygt=s(T1e);r8r=r(Ygt,"not"),Ygt.forEach(t),t8r=r(xV,` load the model weights. It only affects the
model\u2019s configuration. Use `),F1e=n(xV,"CODE",{});var Kgt=s(F1e);a8r=r(Kgt,"from_pretrained()"),Kgt.forEach(t),n8r=r(xV,"to load the model weights."),xV.forEach(t),s8r=i(mi),C1e=n(mi,"P",{});var Zgt=s(C1e);l8r=r(Zgt,"Examples:"),Zgt.forEach(t),i8r=i(mi),m(r0.$$.fragment,mi),mi.forEach(t),d8r=i(fi),xo=n(fi,"DIV",{class:!0});var La=s(xo);m(t0.$$.fragment,La),c8r=i(La),M1e=n(La,"P",{});var eht=s(M1e);f8r=r(eht,"Instantiate one of the model classes of the library (with a sequence classification head) from a pretrained model."),eht.forEach(t),m8r=i(La),An=n(La,"P",{});var K4=s(An);g8r=r(K4,"The model class to instantiate is selected based on the "),E1e=n(K4,"CODE",{});var oht=s(E1e);h8r=r(oht,"model_type"),oht.forEach(t),p8r=r(K4,` property of the config object (either
passed as an argument or loaded from `),y1e=n(K4,"CODE",{});var rht=s(y1e);_8r=r(rht,"pretrained_model_name_or_path"),rht.forEach(t),u8r=r(K4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),w1e=n(K4,"CODE",{});var tht=s(w1e);b8r=r(tht,"pretrained_model_name_or_path"),tht.forEach(t),v8r=r(K4,":"),K4.forEach(t),T8r=i(La),Fe=n(La,"UL",{});var oo=s(Fe);TC=n(oo,"LI",{});var jLe=s(TC);A1e=n(jLe,"STRONG",{});var aht=s(A1e);F8r=r(aht,"albert"),aht.forEach(t),C8r=r(jLe," \u2014 "),_X=n(jLe,"A",{href:!0});var nht=s(_X);M8r=r(nht,"FlaxAlbertForSequenceClassification"),nht.forEach(t),E8r=r(jLe," (ALBERT model)"),jLe.forEach(t),y8r=i(oo),FC=n(oo,"LI",{});var NLe=s(FC);L1e=n(NLe,"STRONG",{});var sht=s(L1e);w8r=r(sht,"bart"),sht.forEach(t),A8r=r(NLe," \u2014 "),uX=n(NLe,"A",{href:!0});var lht=s(uX);L8r=r(lht,"FlaxBartForSequenceClassification"),lht.forEach(t),B8r=r(NLe," (BART model)"),NLe.forEach(t),k8r=i(oo),CC=n(oo,"LI",{});var DLe=s(CC);B1e=n(DLe,"STRONG",{});var iht=s(B1e);x8r=r(iht,"bert"),iht.forEach(t),R8r=r(DLe," \u2014 "),bX=n(DLe,"A",{href:!0});var dht=s(bX);S8r=r(dht,"FlaxBertForSequenceClassification"),dht.forEach(t),P8r=r(DLe," (BERT model)"),DLe.forEach(t),$8r=i(oo),MC=n(oo,"LI",{});var qLe=s(MC);k1e=n(qLe,"STRONG",{});var cht=s(k1e);I8r=r(cht,"big_bird"),cht.forEach(t),j8r=r(qLe," \u2014 "),vX=n(qLe,"A",{href:!0});var fht=s(vX);N8r=r(fht,"FlaxBigBirdForSequenceClassification"),fht.forEach(t),D8r=r(qLe," (BigBird model)"),qLe.forEach(t),q8r=i(oo),EC=n(oo,"LI",{});var GLe=s(EC);x1e=n(GLe,"STRONG",{});var mht=s(x1e);G8r=r(mht,"distilbert"),mht.forEach(t),O8r=r(GLe," \u2014 "),TX=n(GLe,"A",{href:!0});var ght=s(TX);X8r=r(ght,"FlaxDistilBertForSequenceClassification"),ght.forEach(t),z8r=r(GLe," (DistilBERT model)"),GLe.forEach(t),V8r=i(oo),yC=n(oo,"LI",{});var OLe=s(yC);R1e=n(OLe,"STRONG",{});var hht=s(R1e);W8r=r(hht,"electra"),hht.forEach(t),Q8r=r(OLe," \u2014 "),FX=n(OLe,"A",{href:!0});var pht=s(FX);H8r=r(pht,"FlaxElectraForSequenceClassification"),pht.forEach(t),U8r=r(OLe," (ELECTRA model)"),OLe.forEach(t),J8r=i(oo),wC=n(oo,"LI",{});var XLe=s(wC);S1e=n(XLe,"STRONG",{});var _ht=s(S1e);Y8r=r(_ht,"mbart"),_ht.forEach(t),K8r=r(XLe," \u2014 "),CX=n(XLe,"A",{href:!0});var uht=s(CX);Z8r=r(uht,"FlaxMBartForSequenceClassification"),uht.forEach(t),eFr=r(XLe," (mBART model)"),XLe.forEach(t),oFr=i(oo),AC=n(oo,"LI",{});var zLe=s(AC);P1e=n(zLe,"STRONG",{});var bht=s(P1e);rFr=r(bht,"roberta"),bht.forEach(t),tFr=r(zLe," \u2014 "),MX=n(zLe,"A",{href:!0});var vht=s(MX);aFr=r(vht,"FlaxRobertaForSequenceClassification"),vht.forEach(t),nFr=r(zLe," (RoBERTa model)"),zLe.forEach(t),sFr=i(oo),LC=n(oo,"LI",{});var VLe=s(LC);$1e=n(VLe,"STRONG",{});var Tht=s($1e);lFr=r(Tht,"roformer"),Tht.forEach(t),iFr=r(VLe," \u2014 "),EX=n(VLe,"A",{href:!0});var Fht=s(EX);dFr=r(Fht,"FlaxRoFormerForSequenceClassification"),Fht.forEach(t),cFr=r(VLe," (RoFormer model)"),VLe.forEach(t),oo.forEach(t),fFr=i(La),I1e=n(La,"P",{});var Cht=s(I1e);mFr=r(Cht,"Examples:"),Cht.forEach(t),gFr=i(La),m(a0.$$.fragment,La),La.forEach(t),fi.forEach(t),sBe=i(d),af=n(d,"H2",{class:!0});var _xe=s(af);BC=n(_xe,"A",{id:!0,class:!0,href:!0});var Mht=s(BC);j1e=n(Mht,"SPAN",{});var Eht=s(j1e);m(n0.$$.fragment,Eht),Eht.forEach(t),Mht.forEach(t),hFr=i(_xe),N1e=n(_xe,"SPAN",{});var yht=s(N1e);pFr=r(yht,"FlaxAutoModelForQuestionAnswering"),yht.forEach(t),_xe.forEach(t),lBe=i(d),Rr=n(d,"DIV",{class:!0});var gi=s(Rr);m(s0.$$.fragment,gi),_Fr=i(gi),nf=n(gi,"P",{});var RV=s(nf);uFr=r(RV,`This is a generic model class that will be instantiated as one of the model classes of the library (with a question answering head) when created
with the `),D1e=n(RV,"CODE",{});var wht=s(D1e);bFr=r(wht,"from_pretrained()"),wht.forEach(t),vFr=r(RV,"class method or the "),q1e=n(RV,"CODE",{});var Aht=s(q1e);TFr=r(Aht,"from_config()"),Aht.forEach(t),FFr=r(RV,`class
method.`),RV.forEach(t),CFr=i(gi),l0=n(gi,"P",{});var uxe=s(l0);MFr=r(uxe,"This class cannot be instantiated directly using "),G1e=n(uxe,"CODE",{});var Lht=s(G1e);EFr=r(Lht,"__init__()"),Lht.forEach(t),yFr=r(uxe," (throws an error)."),uxe.forEach(t),wFr=i(gi),yt=n(gi,"DIV",{class:!0});var hi=s(yt);m(i0.$$.fragment,hi),AFr=i(hi),O1e=n(hi,"P",{});var Bht=s(O1e);LFr=r(Bht,"Instantiates one of the model classes of the library (with a question answering head) from a configuration."),Bht.forEach(t),BFr=i(hi),sf=n(hi,"P",{});var SV=s(sf);kFr=r(SV,`Note:
Loading a model from its configuration file does `),X1e=n(SV,"STRONG",{});var kht=s(X1e);xFr=r(kht,"not"),kht.forEach(t),RFr=r(SV,` load the model weights. It only affects the
model\u2019s configuration. Use `),z1e=n(SV,"CODE",{});var xht=s(z1e);SFr=r(xht,"from_pretrained()"),xht.forEach(t),PFr=r(SV,"to load the model weights."),SV.forEach(t),$Fr=i(hi),V1e=n(hi,"P",{});var Rht=s(V1e);IFr=r(Rht,"Examples:"),Rht.forEach(t),jFr=i(hi),m(d0.$$.fragment,hi),hi.forEach(t),NFr=i(gi),Ro=n(gi,"DIV",{class:!0});var Ba=s(Ro);m(c0.$$.fragment,Ba),DFr=i(Ba),W1e=n(Ba,"P",{});var Sht=s(W1e);qFr=r(Sht,"Instantiate one of the model classes of the library (with a question answering head) from a pretrained model."),Sht.forEach(t),GFr=i(Ba),Ln=n(Ba,"P",{});var Z4=s(Ln);OFr=r(Z4,"The model class to instantiate is selected based on the "),Q1e=n(Z4,"CODE",{});var Pht=s(Q1e);XFr=r(Pht,"model_type"),Pht.forEach(t),zFr=r(Z4,` property of the config object (either
passed as an argument or loaded from `),H1e=n(Z4,"CODE",{});var $ht=s(H1e);VFr=r($ht,"pretrained_model_name_or_path"),$ht.forEach(t),WFr=r(Z4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),U1e=n(Z4,"CODE",{});var Iht=s(U1e);QFr=r(Iht,"pretrained_model_name_or_path"),Iht.forEach(t),HFr=r(Z4,":"),Z4.forEach(t),UFr=i(Ba),Ce=n(Ba,"UL",{});var ro=s(Ce);kC=n(ro,"LI",{});var WLe=s(kC);J1e=n(WLe,"STRONG",{});var jht=s(J1e);JFr=r(jht,"albert"),jht.forEach(t),YFr=r(WLe," \u2014 "),yX=n(WLe,"A",{href:!0});var Nht=s(yX);KFr=r(Nht,"FlaxAlbertForQuestionAnswering"),Nht.forEach(t),ZFr=r(WLe," (ALBERT model)"),WLe.forEach(t),eCr=i(ro),xC=n(ro,"LI",{});var QLe=s(xC);Y1e=n(QLe,"STRONG",{});var Dht=s(Y1e);oCr=r(Dht,"bart"),Dht.forEach(t),rCr=r(QLe," \u2014 "),wX=n(QLe,"A",{href:!0});var qht=s(wX);tCr=r(qht,"FlaxBartForQuestionAnswering"),qht.forEach(t),aCr=r(QLe," (BART model)"),QLe.forEach(t),nCr=i(ro),RC=n(ro,"LI",{});var HLe=s(RC);K1e=n(HLe,"STRONG",{});var Ght=s(K1e);sCr=r(Ght,"bert"),Ght.forEach(t),lCr=r(HLe," \u2014 "),AX=n(HLe,"A",{href:!0});var Oht=s(AX);iCr=r(Oht,"FlaxBertForQuestionAnswering"),Oht.forEach(t),dCr=r(HLe," (BERT model)"),HLe.forEach(t),cCr=i(ro),SC=n(ro,"LI",{});var ULe=s(SC);Z1e=n(ULe,"STRONG",{});var Xht=s(Z1e);fCr=r(Xht,"big_bird"),Xht.forEach(t),mCr=r(ULe," \u2014 "),LX=n(ULe,"A",{href:!0});var zht=s(LX);gCr=r(zht,"FlaxBigBirdForQuestionAnswering"),zht.forEach(t),hCr=r(ULe," (BigBird model)"),ULe.forEach(t),pCr=i(ro),PC=n(ro,"LI",{});var JLe=s(PC);ebe=n(JLe,"STRONG",{});var Vht=s(ebe);_Cr=r(Vht,"distilbert"),Vht.forEach(t),uCr=r(JLe," \u2014 "),BX=n(JLe,"A",{href:!0});var Wht=s(BX);bCr=r(Wht,"FlaxDistilBertForQuestionAnswering"),Wht.forEach(t),vCr=r(JLe," (DistilBERT model)"),JLe.forEach(t),TCr=i(ro),$C=n(ro,"LI",{});var YLe=s($C);obe=n(YLe,"STRONG",{});var Qht=s(obe);FCr=r(Qht,"electra"),Qht.forEach(t),CCr=r(YLe," \u2014 "),kX=n(YLe,"A",{href:!0});var Hht=s(kX);MCr=r(Hht,"FlaxElectraForQuestionAnswering"),Hht.forEach(t),ECr=r(YLe," (ELECTRA model)"),YLe.forEach(t),yCr=i(ro),IC=n(ro,"LI",{});var KLe=s(IC);rbe=n(KLe,"STRONG",{});var Uht=s(rbe);wCr=r(Uht,"mbart"),Uht.forEach(t),ACr=r(KLe," \u2014 "),xX=n(KLe,"A",{href:!0});var Jht=s(xX);LCr=r(Jht,"FlaxMBartForQuestionAnswering"),Jht.forEach(t),BCr=r(KLe," (mBART model)"),KLe.forEach(t),kCr=i(ro),jC=n(ro,"LI",{});var ZLe=s(jC);tbe=n(ZLe,"STRONG",{});var Yht=s(tbe);xCr=r(Yht,"roberta"),Yht.forEach(t),RCr=r(ZLe," \u2014 "),RX=n(ZLe,"A",{href:!0});var Kht=s(RX);SCr=r(Kht,"FlaxRobertaForQuestionAnswering"),Kht.forEach(t),PCr=r(ZLe," (RoBERTa model)"),ZLe.forEach(t),$Cr=i(ro),NC=n(ro,"LI",{});var e7e=s(NC);abe=n(e7e,"STRONG",{});var Zht=s(abe);ICr=r(Zht,"roformer"),Zht.forEach(t),jCr=r(e7e," \u2014 "),SX=n(e7e,"A",{href:!0});var ept=s(SX);NCr=r(ept,"FlaxRoFormerForQuestionAnswering"),ept.forEach(t),DCr=r(e7e," (RoFormer model)"),e7e.forEach(t),ro.forEach(t),qCr=i(Ba),nbe=n(Ba,"P",{});var opt=s(nbe);GCr=r(opt,"Examples:"),opt.forEach(t),OCr=i(Ba),m(f0.$$.fragment,Ba),Ba.forEach(t),gi.forEach(t),iBe=i(d),lf=n(d,"H2",{class:!0});var bxe=s(lf);DC=n(bxe,"A",{id:!0,class:!0,href:!0});var rpt=s(DC);sbe=n(rpt,"SPAN",{});var tpt=s(sbe);m(m0.$$.fragment,tpt),tpt.forEach(t),rpt.forEach(t),XCr=i(bxe),lbe=n(bxe,"SPAN",{});var apt=s(lbe);zCr=r(apt,"FlaxAutoModelForTokenClassification"),apt.forEach(t),bxe.forEach(t),dBe=i(d),Sr=n(d,"DIV",{class:!0});var pi=s(Sr);m(g0.$$.fragment,pi),VCr=i(pi),df=n(pi,"P",{});var PV=s(df);WCr=r(PV,`This is a generic model class that will be instantiated as one of the model classes of the library (with a token classification head) when created
with the `),ibe=n(PV,"CODE",{});var npt=s(ibe);QCr=r(npt,"from_pretrained()"),npt.forEach(t),HCr=r(PV,"class method or the "),dbe=n(PV,"CODE",{});var spt=s(dbe);UCr=r(spt,"from_config()"),spt.forEach(t),JCr=r(PV,`class
method.`),PV.forEach(t),YCr=i(pi),h0=n(pi,"P",{});var vxe=s(h0);KCr=r(vxe,"This class cannot be instantiated directly using "),cbe=n(vxe,"CODE",{});var lpt=s(cbe);ZCr=r(lpt,"__init__()"),lpt.forEach(t),e4r=r(vxe," (throws an error)."),vxe.forEach(t),o4r=i(pi),wt=n(pi,"DIV",{class:!0});var _i=s(wt);m(p0.$$.fragment,_i),r4r=i(_i),fbe=n(_i,"P",{});var ipt=s(fbe);t4r=r(ipt,"Instantiates one of the model classes of the library (with a token classification head) from a configuration."),ipt.forEach(t),a4r=i(_i),cf=n(_i,"P",{});var $V=s(cf);n4r=r($V,`Note:
Loading a model from its configuration file does `),mbe=n($V,"STRONG",{});var dpt=s(mbe);s4r=r(dpt,"not"),dpt.forEach(t),l4r=r($V,` load the model weights. It only affects the
model\u2019s configuration. Use `),gbe=n($V,"CODE",{});var cpt=s(gbe);i4r=r(cpt,"from_pretrained()"),cpt.forEach(t),d4r=r($V,"to load the model weights."),$V.forEach(t),c4r=i(_i),hbe=n(_i,"P",{});var fpt=s(hbe);f4r=r(fpt,"Examples:"),fpt.forEach(t),m4r=i(_i),m(_0.$$.fragment,_i),_i.forEach(t),g4r=i(pi),So=n(pi,"DIV",{class:!0});var ka=s(So);m(u0.$$.fragment,ka),h4r=i(ka),pbe=n(ka,"P",{});var mpt=s(pbe);p4r=r(mpt,"Instantiate one of the model classes of the library (with a token classification head) from a pretrained model."),mpt.forEach(t),_4r=i(ka),Bn=n(ka,"P",{});var eM=s(Bn);u4r=r(eM,"The model class to instantiate is selected based on the "),_be=n(eM,"CODE",{});var gpt=s(_be);b4r=r(gpt,"model_type"),gpt.forEach(t),v4r=r(eM,` property of the config object (either
passed as an argument or loaded from `),ube=n(eM,"CODE",{});var hpt=s(ube);T4r=r(hpt,"pretrained_model_name_or_path"),hpt.forEach(t),F4r=r(eM,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),bbe=n(eM,"CODE",{});var ppt=s(bbe);C4r=r(ppt,"pretrained_model_name_or_path"),ppt.forEach(t),M4r=r(eM,":"),eM.forEach(t),E4r=i(ka),so=n(ka,"UL",{});var ta=s(so);qC=n(ta,"LI",{});var o7e=s(qC);vbe=n(o7e,"STRONG",{});var _pt=s(vbe);y4r=r(_pt,"albert"),_pt.forEach(t),w4r=r(o7e," \u2014 "),PX=n(o7e,"A",{href:!0});var upt=s(PX);A4r=r(upt,"FlaxAlbertForTokenClassification"),upt.forEach(t),L4r=r(o7e," (ALBERT model)"),o7e.forEach(t),B4r=i(ta),GC=n(ta,"LI",{});var r7e=s(GC);Tbe=n(r7e,"STRONG",{});var bpt=s(Tbe);k4r=r(bpt,"bert"),bpt.forEach(t),x4r=r(r7e," \u2014 "),$X=n(r7e,"A",{href:!0});var vpt=s($X);R4r=r(vpt,"FlaxBertForTokenClassification"),vpt.forEach(t),S4r=r(r7e," (BERT model)"),r7e.forEach(t),P4r=i(ta),OC=n(ta,"LI",{});var t7e=s(OC);Fbe=n(t7e,"STRONG",{});var Tpt=s(Fbe);$4r=r(Tpt,"big_bird"),Tpt.forEach(t),I4r=r(t7e," \u2014 "),IX=n(t7e,"A",{href:!0});var Fpt=s(IX);j4r=r(Fpt,"FlaxBigBirdForTokenClassification"),Fpt.forEach(t),N4r=r(t7e," (BigBird model)"),t7e.forEach(t),D4r=i(ta),XC=n(ta,"LI",{});var a7e=s(XC);Cbe=n(a7e,"STRONG",{});var Cpt=s(Cbe);q4r=r(Cpt,"distilbert"),Cpt.forEach(t),G4r=r(a7e," \u2014 "),jX=n(a7e,"A",{href:!0});var Mpt=s(jX);O4r=r(Mpt,"FlaxDistilBertForTokenClassification"),Mpt.forEach(t),X4r=r(a7e," (DistilBERT model)"),a7e.forEach(t),z4r=i(ta),zC=n(ta,"LI",{});var n7e=s(zC);Mbe=n(n7e,"STRONG",{});var Ept=s(Mbe);V4r=r(Ept,"electra"),Ept.forEach(t),W4r=r(n7e," \u2014 "),NX=n(n7e,"A",{href:!0});var ypt=s(NX);Q4r=r(ypt,"FlaxElectraForTokenClassification"),ypt.forEach(t),H4r=r(n7e," (ELECTRA model)"),n7e.forEach(t),U4r=i(ta),VC=n(ta,"LI",{});var s7e=s(VC);Ebe=n(s7e,"STRONG",{});var wpt=s(Ebe);J4r=r(wpt,"roberta"),wpt.forEach(t),Y4r=r(s7e," \u2014 "),DX=n(s7e,"A",{href:!0});var Apt=s(DX);K4r=r(Apt,"FlaxRobertaForTokenClassification"),Apt.forEach(t),Z4r=r(s7e," (RoBERTa model)"),s7e.forEach(t),eMr=i(ta),WC=n(ta,"LI",{});var l7e=s(WC);ybe=n(l7e,"STRONG",{});var Lpt=s(ybe);oMr=r(Lpt,"roformer"),Lpt.forEach(t),rMr=r(l7e," \u2014 "),qX=n(l7e,"A",{href:!0});var Bpt=s(qX);tMr=r(Bpt,"FlaxRoFormerForTokenClassification"),Bpt.forEach(t),aMr=r(l7e," (RoFormer model)"),l7e.forEach(t),ta.forEach(t),nMr=i(ka),wbe=n(ka,"P",{});var kpt=s(wbe);sMr=r(kpt,"Examples:"),kpt.forEach(t),lMr=i(ka),m(b0.$$.fragment,ka),ka.forEach(t),pi.forEach(t),cBe=i(d),ff=n(d,"H2",{class:!0});var Txe=s(ff);QC=n(Txe,"A",{id:!0,class:!0,href:!0});var xpt=s(QC);Abe=n(xpt,"SPAN",{});var Rpt=s(Abe);m(v0.$$.fragment,Rpt),Rpt.forEach(t),xpt.forEach(t),iMr=i(Txe),Lbe=n(Txe,"SPAN",{});var Spt=s(Lbe);dMr=r(Spt,"FlaxAutoModelForMultipleChoice"),Spt.forEach(t),Txe.forEach(t),fBe=i(d),Pr=n(d,"DIV",{class:!0});var ui=s(Pr);m(T0.$$.fragment,ui),cMr=i(ui),mf=n(ui,"P",{});var IV=s(mf);fMr=r(IV,`This is a generic model class that will be instantiated as one of the model classes of the library (with a multiple choice head) when created
with the `),Bbe=n(IV,"CODE",{});var Ppt=s(Bbe);mMr=r(Ppt,"from_pretrained()"),Ppt.forEach(t),gMr=r(IV,"class method or the "),kbe=n(IV,"CODE",{});var $pt=s(kbe);hMr=r($pt,"from_config()"),$pt.forEach(t),pMr=r(IV,`class
method.`),IV.forEach(t),_Mr=i(ui),F0=n(ui,"P",{});var Fxe=s(F0);uMr=r(Fxe,"This class cannot be instantiated directly using "),xbe=n(Fxe,"CODE",{});var Ipt=s(xbe);bMr=r(Ipt,"__init__()"),Ipt.forEach(t),vMr=r(Fxe," (throws an error)."),Fxe.forEach(t),TMr=i(ui),At=n(ui,"DIV",{class:!0});var bi=s(At);m(C0.$$.fragment,bi),FMr=i(bi),Rbe=n(bi,"P",{});var jpt=s(Rbe);CMr=r(jpt,"Instantiates one of the model classes of the library (with a multiple choice head) from a configuration."),jpt.forEach(t),MMr=i(bi),gf=n(bi,"P",{});var jV=s(gf);EMr=r(jV,`Note:
Loading a model from its configuration file does `),Sbe=n(jV,"STRONG",{});var Npt=s(Sbe);yMr=r(Npt,"not"),Npt.forEach(t),wMr=r(jV,` load the model weights. It only affects the
model\u2019s configuration. Use `),Pbe=n(jV,"CODE",{});var Dpt=s(Pbe);AMr=r(Dpt,"from_pretrained()"),Dpt.forEach(t),LMr=r(jV,"to load the model weights."),jV.forEach(t),BMr=i(bi),$be=n(bi,"P",{});var qpt=s($be);kMr=r(qpt,"Examples:"),qpt.forEach(t),xMr=i(bi),m(M0.$$.fragment,bi),bi.forEach(t),RMr=i(ui),Po=n(ui,"DIV",{class:!0});var xa=s(Po);m(E0.$$.fragment,xa),SMr=i(xa),Ibe=n(xa,"P",{});var Gpt=s(Ibe);PMr=r(Gpt,"Instantiate one of the model classes of the library (with a multiple choice head) from a pretrained model."),Gpt.forEach(t),$Mr=i(xa),kn=n(xa,"P",{});var oM=s(kn);IMr=r(oM,"The model class to instantiate is selected based on the "),jbe=n(oM,"CODE",{});var Opt=s(jbe);jMr=r(Opt,"model_type"),Opt.forEach(t),NMr=r(oM,` property of the config object (either
passed as an argument or loaded from `),Nbe=n(oM,"CODE",{});var Xpt=s(Nbe);DMr=r(Xpt,"pretrained_model_name_or_path"),Xpt.forEach(t),qMr=r(oM,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Dbe=n(oM,"CODE",{});var zpt=s(Dbe);GMr=r(zpt,"pretrained_model_name_or_path"),zpt.forEach(t),OMr=r(oM,":"),oM.forEach(t),XMr=i(xa),lo=n(xa,"UL",{});var aa=s(lo);HC=n(aa,"LI",{});var i7e=s(HC);qbe=n(i7e,"STRONG",{});var Vpt=s(qbe);zMr=r(Vpt,"albert"),Vpt.forEach(t),VMr=r(i7e," \u2014 "),GX=n(i7e,"A",{href:!0});var Wpt=s(GX);WMr=r(Wpt,"FlaxAlbertForMultipleChoice"),Wpt.forEach(t),QMr=r(i7e," (ALBERT model)"),i7e.forEach(t),HMr=i(aa),UC=n(aa,"LI",{});var d7e=s(UC);Gbe=n(d7e,"STRONG",{});var Qpt=s(Gbe);UMr=r(Qpt,"bert"),Qpt.forEach(t),JMr=r(d7e," \u2014 "),OX=n(d7e,"A",{href:!0});var Hpt=s(OX);YMr=r(Hpt,"FlaxBertForMultipleChoice"),Hpt.forEach(t),KMr=r(d7e," (BERT model)"),d7e.forEach(t),ZMr=i(aa),JC=n(aa,"LI",{});var c7e=s(JC);Obe=n(c7e,"STRONG",{});var Upt=s(Obe);eEr=r(Upt,"big_bird"),Upt.forEach(t),oEr=r(c7e," \u2014 "),XX=n(c7e,"A",{href:!0});var Jpt=s(XX);rEr=r(Jpt,"FlaxBigBirdForMultipleChoice"),Jpt.forEach(t),tEr=r(c7e," (BigBird model)"),c7e.forEach(t),aEr=i(aa),YC=n(aa,"LI",{});var f7e=s(YC);Xbe=n(f7e,"STRONG",{});var Ypt=s(Xbe);nEr=r(Ypt,"distilbert"),Ypt.forEach(t),sEr=r(f7e," \u2014 "),zX=n(f7e,"A",{href:!0});var Kpt=s(zX);lEr=r(Kpt,"FlaxDistilBertForMultipleChoice"),Kpt.forEach(t),iEr=r(f7e," (DistilBERT model)"),f7e.forEach(t),dEr=i(aa),KC=n(aa,"LI",{});var m7e=s(KC);zbe=n(m7e,"STRONG",{});var Zpt=s(zbe);cEr=r(Zpt,"electra"),Zpt.forEach(t),fEr=r(m7e," \u2014 "),VX=n(m7e,"A",{href:!0});var e_t=s(VX);mEr=r(e_t,"FlaxElectraForMultipleChoice"),e_t.forEach(t),gEr=r(m7e," (ELECTRA model)"),m7e.forEach(t),hEr=i(aa),ZC=n(aa,"LI",{});var g7e=s(ZC);Vbe=n(g7e,"STRONG",{});var o_t=s(Vbe);pEr=r(o_t,"roberta"),o_t.forEach(t),_Er=r(g7e," \u2014 "),WX=n(g7e,"A",{href:!0});var r_t=s(WX);uEr=r(r_t,"FlaxRobertaForMultipleChoice"),r_t.forEach(t),bEr=r(g7e," (RoBERTa model)"),g7e.forEach(t),vEr=i(aa),e4=n(aa,"LI",{});var h7e=s(e4);Wbe=n(h7e,"STRONG",{});var t_t=s(Wbe);TEr=r(t_t,"roformer"),t_t.forEach(t),FEr=r(h7e," \u2014 "),QX=n(h7e,"A",{href:!0});var a_t=s(QX);CEr=r(a_t,"FlaxRoFormerForMultipleChoice"),a_t.forEach(t),MEr=r(h7e," (RoFormer model)"),h7e.forEach(t),aa.forEach(t),EEr=i(xa),Qbe=n(xa,"P",{});var n_t=s(Qbe);yEr=r(n_t,"Examples:"),n_t.forEach(t),wEr=i(xa),m(y0.$$.fragment,xa),xa.forEach(t),ui.forEach(t),mBe=i(d),hf=n(d,"H2",{class:!0});var Cxe=s(hf);o4=n(Cxe,"A",{id:!0,class:!0,href:!0});var s_t=s(o4);Hbe=n(s_t,"SPAN",{});var l_t=s(Hbe);m(w0.$$.fragment,l_t),l_t.forEach(t),s_t.forEach(t),AEr=i(Cxe),Ube=n(Cxe,"SPAN",{});var i_t=s(Ube);LEr=r(i_t,"FlaxAutoModelForNextSentencePrediction"),i_t.forEach(t),Cxe.forEach(t),gBe=i(d),$r=n(d,"DIV",{class:!0});var vi=s($r);m(A0.$$.fragment,vi),BEr=i(vi),pf=n(vi,"P",{});var NV=s(pf);kEr=r(NV,`This is a generic model class that will be instantiated as one of the model classes of the library (with a next sentence prediction head) when created
with the `),Jbe=n(NV,"CODE",{});var d_t=s(Jbe);xEr=r(d_t,"from_pretrained()"),d_t.forEach(t),REr=r(NV,"class method or the "),Ybe=n(NV,"CODE",{});var c_t=s(Ybe);SEr=r(c_t,"from_config()"),c_t.forEach(t),PEr=r(NV,`class
method.`),NV.forEach(t),$Er=i(vi),L0=n(vi,"P",{});var Mxe=s(L0);IEr=r(Mxe,"This class cannot be instantiated directly using "),Kbe=n(Mxe,"CODE",{});var f_t=s(Kbe);jEr=r(f_t,"__init__()"),f_t.forEach(t),NEr=r(Mxe," (throws an error)."),Mxe.forEach(t),DEr=i(vi),Lt=n(vi,"DIV",{class:!0});var Ti=s(Lt);m(B0.$$.fragment,Ti),qEr=i(Ti),Zbe=n(Ti,"P",{});var m_t=s(Zbe);GEr=r(m_t,"Instantiates one of the model classes of the library (with a next sentence prediction head) from a configuration."),m_t.forEach(t),OEr=i(Ti),_f=n(Ti,"P",{});var DV=s(_f);XEr=r(DV,`Note:
Loading a model from its configuration file does `),e5e=n(DV,"STRONG",{});var g_t=s(e5e);zEr=r(g_t,"not"),g_t.forEach(t),VEr=r(DV,` load the model weights. It only affects the
model\u2019s configuration. Use `),o5e=n(DV,"CODE",{});var h_t=s(o5e);WEr=r(h_t,"from_pretrained()"),h_t.forEach(t),QEr=r(DV,"to load the model weights."),DV.forEach(t),HEr=i(Ti),r5e=n(Ti,"P",{});var p_t=s(r5e);UEr=r(p_t,"Examples:"),p_t.forEach(t),JEr=i(Ti),m(k0.$$.fragment,Ti),Ti.forEach(t),YEr=i(vi),$o=n(vi,"DIV",{class:!0});var Ra=s($o);m(x0.$$.fragment,Ra),KEr=i(Ra),t5e=n(Ra,"P",{});var __t=s(t5e);ZEr=r(__t,"Instantiate one of the model classes of the library (with a next sentence prediction head) from a pretrained model."),__t.forEach(t),e3r=i(Ra),xn=n(Ra,"P",{});var rM=s(xn);o3r=r(rM,"The model class to instantiate is selected based on the "),a5e=n(rM,"CODE",{});var u_t=s(a5e);r3r=r(u_t,"model_type"),u_t.forEach(t),t3r=r(rM,` property of the config object (either
passed as an argument or loaded from `),n5e=n(rM,"CODE",{});var b_t=s(n5e);a3r=r(b_t,"pretrained_model_name_or_path"),b_t.forEach(t),n3r=r(rM,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),s5e=n(rM,"CODE",{});var v_t=s(s5e);s3r=r(v_t,"pretrained_model_name_or_path"),v_t.forEach(t),l3r=r(rM,":"),rM.forEach(t),i3r=i(Ra),l5e=n(Ra,"UL",{});var T_t=s(l5e);r4=n(T_t,"LI",{});var p7e=s(r4);i5e=n(p7e,"STRONG",{});var F_t=s(i5e);d3r=r(F_t,"bert"),F_t.forEach(t),c3r=r(p7e," \u2014 "),HX=n(p7e,"A",{href:!0});var C_t=s(HX);f3r=r(C_t,"FlaxBertForNextSentencePrediction"),C_t.forEach(t),m3r=r(p7e," (BERT model)"),p7e.forEach(t),T_t.forEach(t),g3r=i(Ra),d5e=n(Ra,"P",{});var M_t=s(d5e);h3r=r(M_t,"Examples:"),M_t.forEach(t),p3r=i(Ra),m(R0.$$.fragment,Ra),Ra.forEach(t),vi.forEach(t),hBe=i(d),uf=n(d,"H2",{class:!0});var Exe=s(uf);t4=n(Exe,"A",{id:!0,class:!0,href:!0});var E_t=s(t4);c5e=n(E_t,"SPAN",{});var y_t=s(c5e);m(S0.$$.fragment,y_t),y_t.forEach(t),E_t.forEach(t),_3r=i(Exe),f5e=n(Exe,"SPAN",{});var w_t=s(f5e);u3r=r(w_t,"FlaxAutoModelForImageClassification"),w_t.forEach(t),Exe.forEach(t),pBe=i(d),Ir=n(d,"DIV",{class:!0});var Fi=s(Ir);m(P0.$$.fragment,Fi),b3r=i(Fi),bf=n(Fi,"P",{});var qV=s(bf);v3r=r(qV,`This is a generic model class that will be instantiated as one of the model classes of the library (with a image classification head) when created
with the `),m5e=n(qV,"CODE",{});var A_t=s(m5e);T3r=r(A_t,"from_pretrained()"),A_t.forEach(t),F3r=r(qV,"class method or the "),g5e=n(qV,"CODE",{});var L_t=s(g5e);C3r=r(L_t,"from_config()"),L_t.forEach(t),M3r=r(qV,`class
method.`),qV.forEach(t),E3r=i(Fi),$0=n(Fi,"P",{});var yxe=s($0);y3r=r(yxe,"This class cannot be instantiated directly using "),h5e=n(yxe,"CODE",{});var B_t=s(h5e);w3r=r(B_t,"__init__()"),B_t.forEach(t),A3r=r(yxe," (throws an error)."),yxe.forEach(t),L3r=i(Fi),Bt=n(Fi,"DIV",{class:!0});var Ci=s(Bt);m(I0.$$.fragment,Ci),B3r=i(Ci),p5e=n(Ci,"P",{});var k_t=s(p5e);k3r=r(k_t,"Instantiates one of the model classes of the library (with a image classification head) from a configuration."),k_t.forEach(t),x3r=i(Ci),vf=n(Ci,"P",{});var GV=s(vf);R3r=r(GV,`Note:
Loading a model from its configuration file does `),_5e=n(GV,"STRONG",{});var x_t=s(_5e);S3r=r(x_t,"not"),x_t.forEach(t),P3r=r(GV,` load the model weights. It only affects the
model\u2019s configuration. Use `),u5e=n(GV,"CODE",{});var R_t=s(u5e);$3r=r(R_t,"from_pretrained()"),R_t.forEach(t),I3r=r(GV,"to load the model weights."),GV.forEach(t),j3r=i(Ci),b5e=n(Ci,"P",{});var S_t=s(b5e);N3r=r(S_t,"Examples:"),S_t.forEach(t),D3r=i(Ci),m(j0.$$.fragment,Ci),Ci.forEach(t),q3r=i(Fi),Io=n(Fi,"DIV",{class:!0});var Sa=s(Io);m(N0.$$.fragment,Sa),G3r=i(Sa),v5e=n(Sa,"P",{});var P_t=s(v5e);O3r=r(P_t,"Instantiate one of the model classes of the library (with a image classification head) from a pretrained model."),P_t.forEach(t),X3r=i(Sa),Rn=n(Sa,"P",{});var tM=s(Rn);z3r=r(tM,"The model class to instantiate is selected based on the "),T5e=n(tM,"CODE",{});var $_t=s(T5e);V3r=r($_t,"model_type"),$_t.forEach(t),W3r=r(tM,` property of the config object (either
passed as an argument or loaded from `),F5e=n(tM,"CODE",{});var I_t=s(F5e);Q3r=r(I_t,"pretrained_model_name_or_path"),I_t.forEach(t),H3r=r(tM,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),C5e=n(tM,"CODE",{});var j_t=s(C5e);U3r=r(j_t,"pretrained_model_name_or_path"),j_t.forEach(t),J3r=r(tM,":"),tM.forEach(t),Y3r=i(Sa),D0=n(Sa,"UL",{});var wxe=s(D0);a4=n(wxe,"LI",{});var _7e=s(a4);M5e=n(_7e,"STRONG",{});var N_t=s(M5e);K3r=r(N_t,"beit"),N_t.forEach(t),Z3r=r(_7e," \u2014 "),UX=n(_7e,"A",{href:!0});var D_t=s(UX);eyr=r(D_t,"FlaxBeitForImageClassification"),D_t.forEach(t),oyr=r(_7e," (BEiT model)"),_7e.forEach(t),ryr=i(wxe),n4=n(wxe,"LI",{});var u7e=s(n4);E5e=n(u7e,"STRONG",{});var q_t=s(E5e);tyr=r(q_t,"vit"),q_t.forEach(t),ayr=r(u7e," \u2014 "),JX=n(u7e,"A",{href:!0});var G_t=s(JX);nyr=r(G_t,"FlaxViTForImageClassification"),G_t.forEach(t),syr=r(u7e," (ViT model)"),u7e.forEach(t),wxe.forEach(t),lyr=i(Sa),y5e=n(Sa,"P",{});var O_t=s(y5e);iyr=r(O_t,"Examples:"),O_t.forEach(t),dyr=i(Sa),m(q0.$$.fragment,Sa),Sa.forEach(t),Fi.forEach(t),_Be=i(d),Tf=n(d,"H2",{class:!0});var Axe=s(Tf);s4=n(Axe,"A",{id:!0,class:!0,href:!0});var X_t=s(s4);w5e=n(X_t,"SPAN",{});var z_t=s(w5e);m(G0.$$.fragment,z_t),z_t.forEach(t),X_t.forEach(t),cyr=i(Axe),A5e=n(Axe,"SPAN",{});var V_t=s(A5e);fyr=r(V_t,"FlaxAutoModelForVision2Seq"),V_t.forEach(t),Axe.forEach(t),uBe=i(d),jr=n(d,"DIV",{class:!0});var Mi=s(jr);m(O0.$$.fragment,Mi),myr=i(Mi),Ff=n(Mi,"P",{});var OV=s(Ff);gyr=r(OV,`This is a generic model class that will be instantiated as one of the model classes of the library (with a vision-to-text modeling head) when created
with the `),L5e=n(OV,"CODE",{});var W_t=s(L5e);hyr=r(W_t,"from_pretrained()"),W_t.forEach(t),pyr=r(OV,"class method or the "),B5e=n(OV,"CODE",{});var Q_t=s(B5e);_yr=r(Q_t,"from_config()"),Q_t.forEach(t),uyr=r(OV,`class
method.`),OV.forEach(t),byr=i(Mi),X0=n(Mi,"P",{});var Lxe=s(X0);vyr=r(Lxe,"This class cannot be instantiated directly using "),k5e=n(Lxe,"CODE",{});var H_t=s(k5e);Tyr=r(H_t,"__init__()"),H_t.forEach(t),Fyr=r(Lxe," (throws an error)."),Lxe.forEach(t),Cyr=i(Mi),kt=n(Mi,"DIV",{class:!0});var Ei=s(kt);m(z0.$$.fragment,Ei),Myr=i(Ei),x5e=n(Ei,"P",{});var U_t=s(x5e);Eyr=r(U_t,"Instantiates one of the model classes of the library (with a vision-to-text modeling head) from a configuration."),U_t.forEach(t),yyr=i(Ei),Cf=n(Ei,"P",{});var XV=s(Cf);wyr=r(XV,`Note:
Loading a model from its configuration file does `),R5e=n(XV,"STRONG",{});var J_t=s(R5e);Ayr=r(J_t,"not"),J_t.forEach(t),Lyr=r(XV,` load the model weights. It only affects the
model\u2019s configuration. Use `),S5e=n(XV,"CODE",{});var Y_t=s(S5e);Byr=r(Y_t,"from_pretrained()"),Y_t.forEach(t),kyr=r(XV,"to load the model weights."),XV.forEach(t),xyr=i(Ei),P5e=n(Ei,"P",{});var K_t=s(P5e);Ryr=r(K_t,"Examples:"),K_t.forEach(t),Syr=i(Ei),m(V0.$$.fragment,Ei),Ei.forEach(t),Pyr=i(Mi),jo=n(Mi,"DIV",{class:!0});var Pa=s(jo);m(W0.$$.fragment,Pa),$yr=i(Pa),$5e=n(Pa,"P",{});var Z_t=s($5e);Iyr=r(Z_t,"Instantiate one of the model classes of the library (with a vision-to-text modeling head) from a pretrained model."),Z_t.forEach(t),jyr=i(Pa),Sn=n(Pa,"P",{});var aM=s(Sn);Nyr=r(aM,"The model class to instantiate is selected based on the "),I5e=n(aM,"CODE",{});var eut=s(I5e);Dyr=r(eut,"model_type"),eut.forEach(t),qyr=r(aM,` property of the config object (either
passed as an argument or loaded from `),j5e=n(aM,"CODE",{});var out=s(j5e);Gyr=r(out,"pretrained_model_name_or_path"),out.forEach(t),Oyr=r(aM,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),N5e=n(aM,"CODE",{});var rut=s(N5e);Xyr=r(rut,"pretrained_model_name_or_path"),rut.forEach(t),zyr=r(aM,":"),aM.forEach(t),Vyr=i(Pa),D5e=n(Pa,"UL",{});var tut=s(D5e);l4=n(tut,"LI",{});var b7e=s(l4);q5e=n(b7e,"STRONG",{});var aut=s(q5e);Wyr=r(aut,"vision-encoder-decoder"),aut.forEach(t),Qyr=r(b7e," \u2014 "),YX=n(b7e,"A",{href:!0});var nut=s(YX);Hyr=r(nut,"FlaxVisionEncoderDecoderModel"),nut.forEach(t),Uyr=r(b7e," (Vision Encoder decoder model)"),b7e.forEach(t),tut.forEach(t),Jyr=i(Pa),G5e=n(Pa,"P",{});var sut=s(G5e);Yyr=r(sut,"Examples:"),sut.forEach(t),Kyr=i(Pa),m(Q0.$$.fragment,Pa),Pa.forEach(t),Mi.forEach(t),this.h()},h(){c(J,"name","hf:doc:metadata"),c(J,"content",JSON.stringify(put)),c(me,"id","auto-classes"),c(me,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(me,"href","#auto-classes"),c(ie,"class","relative group"),c(Pn,"href","/docs/transformers/pr_15682/en/model_doc/auto#transformers.AutoConfig"),c(In,"href","/docs/transformers/pr_15682/en/model_doc/auto#transformers.AutoModel"),c(jn,"href","/docs/transformers/pr_15682/en/model_doc/auto#transformers.AutoTokenizer"),c(Ri,"href","/docs/transformers/pr_15682/en/model_doc/bert#transformers.BertModel"),c(Lf,"id","extending-the-auto-classes"),c(Lf,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Lf,"href","#extending-the-auto-classes"),c(Si,"class","relative group"),c(kf,"id","transformers.AutoConfig"),c(kf,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(kf,"href","#transformers.AutoConfig"),c(Pi,"class","relative group"),c(JL,"href","/docs/transformers/pr_15682/en/model_doc/auto#transformers.AutoConfig.from_pretrained"),c(YL,"href","/docs/transformers/pr_15682/en/model_doc/albert#transformers.AlbertConfig"),c(KL,"href","/docs/transformers/pr_15682/en/model_doc/bart#transformers.BartConfig"),c(ZL,"href","/docs/transformers/pr_15682/en/model_doc/beit#transformers.BeitConfig"),c(e7,"href","/docs/transformers/pr_15682/en/model_doc/bert#transformers.BertConfig"),c(o7,"href","/docs/transformers/pr_15682/en/model_doc/bert-generation#transformers.BertGenerationConfig"),c(r7,"href","/docs/transformers/pr_15682/en/model_doc/big_bird#transformers.BigBirdConfig"),c(t7,"href","/docs/transformers/pr_15682/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusConfig"),c(a7,"href","/docs/transformers/pr_15682/en/model_doc/blenderbot#transformers.BlenderbotConfig"),c(n7,"href","/docs/transformers/pr_15682/en/model_doc/blenderbot-small#transformers.BlenderbotSmallConfig"),c(s7,"href","/docs/transformers/pr_15682/en/model_doc/camembert#transformers.CamembertConfig"),c(l7,"href","/docs/transformers/pr_15682/en/model_doc/canine#transformers.CanineConfig"),c(i7,"href","/docs/transformers/pr_15682/en/model_doc/clip#transformers.CLIPConfig"),c(d7,"href","/docs/transformers/pr_15682/en/model_doc/convbert#transformers.ConvBertConfig"),c(c7,"href","/docs/transformers/pr_15682/en/model_doc/convnext#transformers.ConvNextConfig"),c(f7,"href","/docs/transformers/pr_15682/en/model_doc/ctrl#transformers.CTRLConfig"),c(m7,"href","/docs/transformers/pr_15682/en/model_doc/deberta#transformers.DebertaConfig"),c(g7,"href","/docs/transformers/pr_15682/en/model_doc/deberta-v2#transformers.DebertaV2Config"),c(h7,"href","/docs/transformers/pr_15682/en/model_doc/deit#transformers.DeiTConfig"),c(p7,"href","/docs/transformers/pr_15682/en/model_doc/detr#transformers.DetrConfig"),c(_7,"href","/docs/transformers/pr_15682/en/model_doc/distilbert#transformers.DistilBertConfig"),c(u7,"href","/docs/transformers/pr_15682/en/model_doc/dpr#transformers.DPRConfig"),c(b7,"href","/docs/transformers/pr_15682/en/model_doc/electra#transformers.ElectraConfig"),c(v7,"href","/docs/transformers/pr_15682/en/model_doc/encoder-decoder#transformers.EncoderDecoderConfig"),c(T7,"href","/docs/transformers/pr_15682/en/model_doc/flaubert#transformers.FlaubertConfig"),c(F7,"href","/docs/transformers/pr_15682/en/model_doc/fnet#transformers.FNetConfig"),c(C7,"href","/docs/transformers/pr_15682/en/model_doc/fsmt#transformers.FSMTConfig"),c(M7,"href","/docs/transformers/pr_15682/en/model_doc/funnel#transformers.FunnelConfig"),c(E7,"href","/docs/transformers/pr_15682/en/model_doc/gpt2#transformers.GPT2Config"),c(y7,"href","/docs/transformers/pr_15682/en/model_doc/gpt_neo#transformers.GPTNeoConfig"),c(w7,"href","/docs/transformers/pr_15682/en/model_doc/gptj#transformers.GPTJConfig"),c(A7,"href","/docs/transformers/pr_15682/en/model_doc/hubert#transformers.HubertConfig"),c(L7,"href","/docs/transformers/pr_15682/en/model_doc/ibert#transformers.IBertConfig"),c(B7,"href","/docs/transformers/pr_15682/en/model_doc/imagegpt#transformers.ImageGPTConfig"),c(k7,"href","/docs/transformers/pr_15682/en/model_doc/layoutlm#transformers.LayoutLMConfig"),c(x7,"href","/docs/transformers/pr_15682/en/model_doc/layoutlmv2#transformers.LayoutLMv2Config"),c(R7,"href","/docs/transformers/pr_15682/en/model_doc/led#transformers.LEDConfig"),c(S7,"href","/docs/transformers/pr_15682/en/model_doc/longformer#transformers.LongformerConfig"),c(P7,"href","/docs/transformers/pr_15682/en/model_doc/luke#transformers.LukeConfig"),c($7,"href","/docs/transformers/pr_15682/en/model_doc/lxmert#transformers.LxmertConfig"),c(I7,"href","/docs/transformers/pr_15682/en/model_doc/m2m_100#transformers.M2M100Config"),c(j7,"href","/docs/transformers/pr_15682/en/model_doc/marian#transformers.MarianConfig"),c(N7,"href","/docs/transformers/pr_15682/en/model_doc/maskformer#transformers.MaskFormerConfig"),c(D7,"href","/docs/transformers/pr_15682/en/model_doc/mbart#transformers.MBartConfig"),c(q7,"href","/docs/transformers/pr_15682/en/model_doc/megatron-bert#transformers.MegatronBertConfig"),c(G7,"href","/docs/transformers/pr_15682/en/model_doc/mobilebert#transformers.MobileBertConfig"),c(O7,"href","/docs/transformers/pr_15682/en/model_doc/mpnet#transformers.MPNetConfig"),c(X7,"href","/docs/transformers/pr_15682/en/model_doc/mt5#transformers.MT5Config"),c(z7,"href","/docs/transformers/pr_15682/en/model_doc/nystromformer#transformers.NystromformerConfig"),c(V7,"href","/docs/transformers/pr_15682/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig"),c(W7,"href","/docs/transformers/pr_15682/en/model_doc/pegasus#transformers.PegasusConfig"),c(Q7,"href","/docs/transformers/pr_15682/en/model_doc/perceiver#transformers.PerceiverConfig"),c(H7,"href","/docs/transformers/pr_15682/en/model_doc/plbart#transformers.PLBartConfig"),c(U7,"href","/docs/transformers/pr_15682/en/model_doc/poolformer#transformers.PoolFormerConfig"),c(J7,"href","/docs/transformers/pr_15682/en/model_doc/prophetnet#transformers.ProphetNetConfig"),c(Y7,"href","/docs/transformers/pr_15682/en/model_doc/qdqbert#transformers.QDQBertConfig"),c(K7,"href","/docs/transformers/pr_15682/en/model_doc/rag#transformers.RagConfig"),c(Z7,"href","/docs/transformers/pr_15682/en/model_doc/realm#transformers.RealmConfig"),c(e9,"href","/docs/transformers/pr_15682/en/model_doc/reformer#transformers.ReformerConfig"),c(o9,"href","/docs/transformers/pr_15682/en/model_doc/rembert#transformers.RemBertConfig"),c(r9,"href","/docs/transformers/pr_15682/en/model_doc/retribert#transformers.RetriBertConfig"),c(t9,"href","/docs/transformers/pr_15682/en/model_doc/roberta#transformers.RobertaConfig"),c(a9,"href","/docs/transformers/pr_15682/en/model_doc/roformer#transformers.RoFormerConfig"),c(n9,"href","/docs/transformers/pr_15682/en/model_doc/segformer#transformers.SegformerConfig"),c(s9,"href","/docs/transformers/pr_15682/en/model_doc/sew#transformers.SEWConfig"),c(l9,"href","/docs/transformers/pr_15682/en/model_doc/sew-d#transformers.SEWDConfig"),c(i9,"href","/docs/transformers/pr_15682/en/model_doc/speech-encoder-decoder#transformers.SpeechEncoderDecoderConfig"),c(d9,"href","/docs/transformers/pr_15682/en/model_doc/speech_to_text#transformers.Speech2TextConfig"),c(c9,"href","/docs/transformers/pr_15682/en/model_doc/speech_to_text_2#transformers.Speech2Text2Config"),c(f9,"href","/docs/transformers/pr_15682/en/model_doc/splinter#transformers.SplinterConfig"),c(m9,"href","/docs/transformers/pr_15682/en/model_doc/squeezebert#transformers.SqueezeBertConfig"),c(g9,"href","/docs/transformers/pr_15682/en/model_doc/swin#transformers.SwinConfig"),c(h9,"href","/docs/transformers/pr_15682/en/model_doc/t5#transformers.T5Config"),c(p9,"href","/docs/transformers/pr_15682/en/model_doc/tapas#transformers.TapasConfig"),c(_9,"href","/docs/transformers/pr_15682/en/model_doc/transfo-xl#transformers.TransfoXLConfig"),c(u9,"href","/docs/transformers/pr_15682/en/model_doc/trocr#transformers.TrOCRConfig"),c(b9,"href","/docs/transformers/pr_15682/en/model_doc/unispeech#transformers.UniSpeechConfig"),c(v9,"href","/docs/transformers/pr_15682/en/model_doc/unispeech-sat#transformers.UniSpeechSatConfig"),c(T9,"href","/docs/transformers/pr_15682/en/model_doc/vilt#transformers.ViltConfig"),c(F9,"href","/docs/transformers/pr_15682/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderConfig"),c(C9,"href","/docs/transformers/pr_15682/en/model_doc/vision-text-dual-encoder#transformers.VisionTextDualEncoderConfig"),c(M9,"href","/docs/transformers/pr_15682/en/model_doc/visual_bert#transformers.VisualBertConfig"),c(E9,"href","/docs/transformers/pr_15682/en/model_doc/vit#transformers.ViTConfig"),c(y9,"href","/docs/transformers/pr_15682/en/model_doc/vit_mae#transformers.ViTMAEConfig"),c(w9,"href","/docs/transformers/pr_15682/en/model_doc/wav2vec2#transformers.Wav2Vec2Config"),c(A9,"href","/docs/transformers/pr_15682/en/model_doc/wavlm#transformers.WavLMConfig"),c(L9,"href","/docs/transformers/pr_15682/en/model_doc/xglm#transformers.XGLMConfig"),c(B9,"href","/docs/transformers/pr_15682/en/model_doc/xlm#transformers.XLMConfig"),c(k9,"href","/docs/transformers/pr_15682/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetConfig"),c(x9,"href","/docs/transformers/pr_15682/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig"),c(R9,"href","/docs/transformers/pr_15682/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLConfig"),c(S9,"href","/docs/transformers/pr_15682/en/model_doc/xlnet#transformers.XLNetConfig"),c(P9,"href","/docs/transformers/pr_15682/en/model_doc/yoso#transformers.YosoConfig"),c(fo,"class","docstring"),c(pg,"class","docstring"),c(Go,"class","docstring"),c(_g,"id","transformers.AutoTokenizer"),c(_g,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(_g,"href","#transformers.AutoTokenizer"),c(Ii,"class","relative group"),c($9,"href","/docs/transformers/pr_15682/en/model_doc/auto#transformers.AutoTokenizer.from_pretrained"),c(I9,"href","/docs/transformers/pr_15682/en/model_doc/albert#transformers.AlbertTokenizer"),c(j9,"href","/docs/transformers/pr_15682/en/model_doc/albert#transformers.AlbertTokenizerFast"),c(N9,"href","/docs/transformers/pr_15682/en/model_doc/bart#transformers.BartTokenizer"),c(D9,"href","/docs/transformers/pr_15682/en/model_doc/bart#transformers.BartTokenizerFast"),c(q9,"href","/docs/transformers/pr_15682/en/model_doc/barthez#transformers.BarthezTokenizer"),c(G9,"href","/docs/transformers/pr_15682/en/model_doc/barthez#transformers.BarthezTokenizerFast"),c(O9,"href","/docs/transformers/pr_15682/en/model_doc/bartpho#transformers.BartphoTokenizer"),c(X9,"href","/docs/transformers/pr_15682/en/model_doc/bert#transformers.BertTokenizer"),c(z9,"href","/docs/transformers/pr_15682/en/model_doc/bert#transformers.BertTokenizerFast"),c(V9,"href","/docs/transformers/pr_15682/en/model_doc/bert-generation#transformers.BertGenerationTokenizer"),c(W9,"href","/docs/transformers/pr_15682/en/model_doc/bert-japanese#transformers.BertJapaneseTokenizer"),c(Q9,"href","/docs/transformers/pr_15682/en/model_doc/bertweet#transformers.BertweetTokenizer"),c(H9,"href","/docs/transformers/pr_15682/en/model_doc/big_bird#transformers.BigBirdTokenizer"),c(U9,"href","/docs/transformers/pr_15682/en/model_doc/big_bird#transformers.BigBirdTokenizerFast"),c(J9,"href","/docs/transformers/pr_15682/en/model_doc/pegasus#transformers.PegasusTokenizer"),c(Y9,"href","/docs/transformers/pr_15682/en/model_doc/pegasus#transformers.PegasusTokenizerFast"),c(K9,"href","/docs/transformers/pr_15682/en/model_doc/blenderbot#transformers.BlenderbotTokenizer"),c(Z9,"href","/docs/transformers/pr_15682/en/model_doc/blenderbot#transformers.BlenderbotTokenizerFast"),c(eB,"href","/docs/transformers/pr_15682/en/model_doc/blenderbot-small#transformers.BlenderbotSmallTokenizer"),c(oB,"href","/docs/transformers/pr_15682/en/model_doc/byt5#transformers.ByT5Tokenizer"),c(rB,"href","/docs/transformers/pr_15682/en/model_doc/camembert#transformers.CamembertTokenizer"),c(tB,"href","/docs/transformers/pr_15682/en/model_doc/camembert#transformers.CamembertTokenizerFast"),c(aB,"href","/docs/transformers/pr_15682/en/model_doc/canine#transformers.CanineTokenizer"),c(nB,"href","/docs/transformers/pr_15682/en/model_doc/clip#transformers.CLIPTokenizer"),c(sB,"href","/docs/transformers/pr_15682/en/model_doc/clip#transformers.CLIPTokenizerFast"),c(lB,"href","/docs/transformers/pr_15682/en/model_doc/convbert#transformers.ConvBertTokenizer"),c(iB,"href","/docs/transformers/pr_15682/en/model_doc/convbert#transformers.ConvBertTokenizerFast"),c(dB,"href","/docs/transformers/pr_15682/en/model_doc/cpm#transformers.CpmTokenizer"),c(cB,"href","/docs/transformers/pr_15682/en/model_doc/ctrl#transformers.CTRLTokenizer"),c(fB,"href","/docs/transformers/pr_15682/en/model_doc/deberta#transformers.DebertaTokenizer"),c(mB,"href","/docs/transformers/pr_15682/en/model_doc/deberta#transformers.DebertaTokenizerFast"),c(gB,"href","/docs/transformers/pr_15682/en/model_doc/deberta-v2#transformers.DebertaV2Tokenizer"),c(hB,"href","/docs/transformers/pr_15682/en/model_doc/distilbert#transformers.DistilBertTokenizer"),c(pB,"href","/docs/transformers/pr_15682/en/model_doc/distilbert#transformers.DistilBertTokenizerFast"),c(_B,"href","/docs/transformers/pr_15682/en/model_doc/dpr#transformers.DPRQuestionEncoderTokenizer"),c(uB,"href","/docs/transformers/pr_15682/en/model_doc/dpr#transformers.DPRQuestionEncoderTokenizerFast"),c(bB,"href","/docs/transformers/pr_15682/en/model_doc/electra#transformers.ElectraTokenizer"),c(vB,"href","/docs/transformers/pr_15682/en/model_doc/electra#transformers.ElectraTokenizerFast"),c(TB,"href","/docs/transformers/pr_15682/en/model_doc/flaubert#transformers.FlaubertTokenizer"),c(FB,"href","/docs/transformers/pr_15682/en/model_doc/fnet#transformers.FNetTokenizer"),c(CB,"href","/docs/transformers/pr_15682/en/model_doc/fnet#transformers.FNetTokenizerFast"),c(MB,"href","/docs/transformers/pr_15682/en/model_doc/fsmt#transformers.FSMTTokenizer"),c(EB,"href","/docs/transformers/pr_15682/en/model_doc/funnel#transformers.FunnelTokenizer"),c(yB,"href","/docs/transformers/pr_15682/en/model_doc/funnel#transformers.FunnelTokenizerFast"),c(wB,"href","/docs/transformers/pr_15682/en/model_doc/gpt2#transformers.GPT2Tokenizer"),c(AB,"href","/docs/transformers/pr_15682/en/model_doc/gpt2#transformers.GPT2TokenizerFast"),c(LB,"href","/docs/transformers/pr_15682/en/model_doc/gpt2#transformers.GPT2Tokenizer"),c(BB,"href","/docs/transformers/pr_15682/en/model_doc/gpt2#transformers.GPT2TokenizerFast"),c(kB,"href","/docs/transformers/pr_15682/en/model_doc/herbert#transformers.HerbertTokenizer"),c(xB,"href","/docs/transformers/pr_15682/en/model_doc/herbert#transformers.HerbertTokenizerFast"),c(RB,"href","/docs/transformers/pr_15682/en/model_doc/wav2vec2#transformers.Wav2Vec2CTCTokenizer"),c(SB,"href","/docs/transformers/pr_15682/en/model_doc/roberta#transformers.RobertaTokenizer"),c(PB,"href","/docs/transformers/pr_15682/en/model_doc/roberta#transformers.RobertaTokenizerFast"),c($B,"href","/docs/transformers/pr_15682/en/model_doc/layoutlm#transformers.LayoutLMTokenizer"),c(IB,"href","/docs/transformers/pr_15682/en/model_doc/layoutlm#transformers.LayoutLMTokenizerFast"),c(jB,"href","/docs/transformers/pr_15682/en/model_doc/layoutlmv2#transformers.LayoutLMv2Tokenizer"),c(NB,"href","/docs/transformers/pr_15682/en/model_doc/layoutlmv2#transformers.LayoutLMv2TokenizerFast"),c(DB,"href","/docs/transformers/pr_15682/en/model_doc/layoutxlm#transformers.LayoutXLMTokenizer"),c(qB,"href","/docs/transformers/pr_15682/en/model_doc/layoutxlm#transformers.LayoutXLMTokenizerFast"),c(GB,"href","/docs/transformers/pr_15682/en/model_doc/led#transformers.LEDTokenizer"),c(OB,"href","/docs/transformers/pr_15682/en/model_doc/led#transformers.LEDTokenizerFast"),c(XB,"href","/docs/transformers/pr_15682/en/model_doc/longformer#transformers.LongformerTokenizer"),c(zB,"href","/docs/transformers/pr_15682/en/model_doc/longformer#transformers.LongformerTokenizerFast"),c(VB,"href","/docs/transformers/pr_15682/en/model_doc/luke#transformers.LukeTokenizer"),c(WB,"href","/docs/transformers/pr_15682/en/model_doc/lxmert#transformers.LxmertTokenizer"),c(QB,"href","/docs/transformers/pr_15682/en/model_doc/lxmert#transformers.LxmertTokenizerFast"),c(HB,"href","/docs/transformers/pr_15682/en/model_doc/m2m_100#transformers.M2M100Tokenizer"),c(UB,"href","/docs/transformers/pr_15682/en/model_doc/marian#transformers.MarianTokenizer"),c(JB,"href","/docs/transformers/pr_15682/en/model_doc/mbart#transformers.MBartTokenizer"),c(YB,"href","/docs/transformers/pr_15682/en/model_doc/mbart#transformers.MBartTokenizerFast"),c(KB,"href","/docs/transformers/pr_15682/en/model_doc/mbart#transformers.MBart50Tokenizer"),c(ZB,"href","/docs/transformers/pr_15682/en/model_doc/mbart#transformers.MBart50TokenizerFast"),c(ek,"href","/docs/transformers/pr_15682/en/model_doc/mluke#transformers.MLukeTokenizer"),c(ok,"href","/docs/transformers/pr_15682/en/model_doc/mobilebert#transformers.MobileBertTokenizer"),c(rk,"href","/docs/transformers/pr_15682/en/model_doc/mobilebert#transformers.MobileBertTokenizerFast"),c(tk,"href","/docs/transformers/pr_15682/en/model_doc/mpnet#transformers.MPNetTokenizer"),c(ak,"href","/docs/transformers/pr_15682/en/model_doc/mpnet#transformers.MPNetTokenizerFast"),c(nk,"href","/docs/transformers/pr_15682/en/model_doc/mt5#transformers.T5Tokenizer"),c(sk,"href","/docs/transformers/pr_15682/en/model_doc/mt5#transformers.T5TokenizerFast"),c(lk,"href","/docs/transformers/pr_15682/en/model_doc/openai-gpt#transformers.OpenAIGPTTokenizer"),c(ik,"href","/docs/transformers/pr_15682/en/model_doc/openai-gpt#transformers.OpenAIGPTTokenizerFast"),c(dk,"href","/docs/transformers/pr_15682/en/model_doc/pegasus#transformers.PegasusTokenizer"),c(ck,"href","/docs/transformers/pr_15682/en/model_doc/pegasus#transformers.PegasusTokenizerFast"),c(fk,"href","/docs/transformers/pr_15682/en/model_doc/perceiver#transformers.PerceiverTokenizer"),c(mk,"href","/docs/transformers/pr_15682/en/model_doc/phobert#transformers.PhobertTokenizer"),c(gk,"href","/docs/transformers/pr_15682/en/model_doc/plbart#transformers.PLBartTokenizer"),c(hk,"href","/docs/transformers/pr_15682/en/model_doc/prophetnet#transformers.ProphetNetTokenizer"),c(pk,"href","/docs/transformers/pr_15682/en/model_doc/bert#transformers.BertTokenizer"),c(_k,"href","/docs/transformers/pr_15682/en/model_doc/bert#transformers.BertTokenizerFast"),c(uk,"href","/docs/transformers/pr_15682/en/model_doc/rag#transformers.RagTokenizer"),c(bk,"href","/docs/transformers/pr_15682/en/model_doc/reformer#transformers.ReformerTokenizer"),c(vk,"href","/docs/transformers/pr_15682/en/model_doc/reformer#transformers.ReformerTokenizerFast"),c(Tk,"href","/docs/transformers/pr_15682/en/model_doc/rembert#transformers.RemBertTokenizer"),c(Fk,"href","/docs/transformers/pr_15682/en/model_doc/rembert#transformers.RemBertTokenizerFast"),c(Ck,"href","/docs/transformers/pr_15682/en/model_doc/retribert#transformers.RetriBertTokenizer"),c(Mk,"href","/docs/transformers/pr_15682/en/model_doc/retribert#transformers.RetriBertTokenizerFast"),c(Ek,"href","/docs/transformers/pr_15682/en/model_doc/roberta#transformers.RobertaTokenizer"),c(yk,"href","/docs/transformers/pr_15682/en/model_doc/roberta#transformers.RobertaTokenizerFast"),c(wk,"href","/docs/transformers/pr_15682/en/model_doc/roformer#transformers.RoFormerTokenizer"),c(Ak,"href","/docs/transformers/pr_15682/en/model_doc/roformer#transformers.RoFormerTokenizerFast"),c(Lk,"href","/docs/transformers/pr_15682/en/model_doc/speech_to_text#transformers.Speech2TextTokenizer"),c(Bk,"href","/docs/transformers/pr_15682/en/model_doc/speech_to_text_2#transformers.Speech2Text2Tokenizer"),c(kk,"href","/docs/transformers/pr_15682/en/model_doc/splinter#transformers.SplinterTokenizer"),c(xk,"href","/docs/transformers/pr_15682/en/model_doc/splinter#transformers.SplinterTokenizerFast"),c(Rk,"href","/docs/transformers/pr_15682/en/model_doc/squeezebert#transformers.SqueezeBertTokenizer"),c(Sk,"href","/docs/transformers/pr_15682/en/model_doc/squeezebert#transformers.SqueezeBertTokenizerFast"),c(Pk,"href","/docs/transformers/pr_15682/en/model_doc/mt5#transformers.T5Tokenizer"),c($k,"href","/docs/transformers/pr_15682/en/model_doc/mt5#transformers.T5TokenizerFast"),c(Ik,"href","/docs/transformers/pr_15682/en/model_doc/tapas#transformers.TapasTokenizer"),c(jk,"href","/docs/transformers/pr_15682/en/model_doc/transfo-xl#transformers.TransfoXLTokenizer"),c(Nk,"href","/docs/transformers/pr_15682/en/model_doc/wav2vec2#transformers.Wav2Vec2CTCTokenizer"),c(Dk,"href","/docs/transformers/pr_15682/en/model_doc/wav2vec2_phoneme#transformers.Wav2Vec2PhonemeCTCTokenizer"),c(qk,"href","/docs/transformers/pr_15682/en/model_doc/xglm#transformers.XGLMTokenizer"),c(Gk,"href","/docs/transformers/pr_15682/en/model_doc/xglm#transformers.XGLMTokenizerFast"),c(Ok,"href","/docs/transformers/pr_15682/en/model_doc/xlm#transformers.XLMTokenizer"),c(Xk,"href","/docs/transformers/pr_15682/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetTokenizer"),c(zk,"href","/docs/transformers/pr_15682/en/model_doc/xlm-roberta#transformers.XLMRobertaTokenizer"),c(Vk,"href","/docs/transformers/pr_15682/en/model_doc/xlm-roberta#transformers.XLMRobertaTokenizerFast"),c(Wk,"href","/docs/transformers/pr_15682/en/model_doc/xlnet#transformers.XLNetTokenizer"),c(Qk,"href","/docs/transformers/pr_15682/en/model_doc/xlnet#transformers.XLNetTokenizerFast"),c(mo,"class","docstring"),c(Wg,"class","docstring"),c(Oo,"class","docstring"),c(Qg,"id","transformers.AutoFeatureExtractor"),c(Qg,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Qg,"href","#transformers.AutoFeatureExtractor"),c(ji,"class","relative group"),c(Hk,"href","/docs/transformers/pr_15682/en/model_doc/auto#transformers.AutoFeatureExtractor.from_pretrained"),c(Uk,"href","/docs/transformers/pr_15682/en/model_doc/beit#transformers.BeitFeatureExtractor"),c(Jk,"href","/docs/transformers/pr_15682/en/model_doc/clip#transformers.CLIPFeatureExtractor"),c(Yk,"href","/docs/transformers/pr_15682/en/model_doc/convnext#transformers.ConvNextFeatureExtractor"),c(Kk,"href","/docs/transformers/pr_15682/en/model_doc/deit#transformers.DeiTFeatureExtractor"),c(Zk,"href","/docs/transformers/pr_15682/en/model_doc/detr#transformers.DetrFeatureExtractor"),c(ex,"href","/docs/transformers/pr_15682/en/model_doc/wav2vec2#transformers.Wav2Vec2FeatureExtractor"),c(ox,"href","/docs/transformers/pr_15682/en/model_doc/layoutlmv2#transformers.LayoutLMv2FeatureExtractor"),c(rx,"href","/docs/transformers/pr_15682/en/model_doc/perceiver#transformers.PerceiverFeatureExtractor"),c(tx,"href","/docs/transformers/pr_15682/en/model_doc/poolformer#transformers.PoolFormerFeatureExtractor"),c(ax,"href","/docs/transformers/pr_15682/en/model_doc/segformer#transformers.SegformerFeatureExtractor"),c(nx,"href","/docs/transformers/pr_15682/en/model_doc/speech_to_text#transformers.Speech2TextFeatureExtractor"),c(sx,"href","/docs/transformers/pr_15682/en/model_doc/vit#transformers.ViTFeatureExtractor"),c(lx,"href","/docs/transformers/pr_15682/en/model_doc/vit#transformers.ViTFeatureExtractor"),c(ix,"href","/docs/transformers/pr_15682/en/model_doc/vit#transformers.ViTFeatureExtractor"),c(dx,"href","/docs/transformers/pr_15682/en/model_doc/wav2vec2#transformers.Wav2Vec2FeatureExtractor"),c(Le,"class","docstring"),c(ch,"class","docstring"),c(Xo,"class","docstring"),c(fh,"id","transformers.AutoProcessor"),c(fh,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(fh,"href","#transformers.AutoProcessor"),c(Ni,"class","relative group"),c(cx,"href","/docs/transformers/pr_15682/en/model_doc/auto#transformers.AutoProcessor.from_pretrained"),c(fx,"href","/docs/transformers/pr_15682/en/model_doc/clip#transformers.CLIPProcessor"),c(mx,"href","/docs/transformers/pr_15682/en/model_doc/layoutlmv2#transformers.LayoutLMv2Processor"),c(gx,"href","/docs/transformers/pr_15682/en/model_doc/layoutxlm#transformers.LayoutXLMProcessor"),c(hx,"href","/docs/transformers/pr_15682/en/model_doc/speech_to_text#transformers.Speech2TextProcessor"),c(px,"href","/docs/transformers/pr_15682/en/model_doc/speech_to_text_2#transformers.Speech2Text2Processor"),c(_x,"href","/docs/transformers/pr_15682/en/model_doc/trocr#transformers.TrOCRProcessor"),c(ux,"href","/docs/transformers/pr_15682/en/model_doc/vision-text-dual-encoder#transformers.VisionTextDualEncoderProcessor"),c(bx,"href","/docs/transformers/pr_15682/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor"),c(Be,"class","docstring"),c(Fh,"class","docstring"),c(zo,"class","docstring"),c(Ch,"id","transformers.AutoModel"),c(Ch,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Ch,"href","#transformers.AutoModel"),c(qi,"class","relative group"),c(Nr,"class","docstring"),c(vx,"href","/docs/transformers/pr_15682/en/model_doc/albert#transformers.AlbertModel"),c(Tx,"href","/docs/transformers/pr_15682/en/model_doc/bart#transformers.BartModel"),c(Fx,"href","/docs/transformers/pr_15682/en/model_doc/beit#transformers.BeitModel"),c(Cx,"href","/docs/transformers/pr_15682/en/model_doc/bert#transformers.BertModel"),c(Mx,"href","/docs/transformers/pr_15682/en/model_doc/bert-generation#transformers.BertGenerationEncoder"),c(Ex,"href","/docs/transformers/pr_15682/en/model_doc/big_bird#transformers.BigBirdModel"),c(yx,"href","/docs/transformers/pr_15682/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusModel"),c(wx,"href","/docs/transformers/pr_15682/en/model_doc/blenderbot#transformers.BlenderbotModel"),c(Ax,"href","/docs/transformers/pr_15682/en/model_doc/blenderbot-small#transformers.BlenderbotSmallModel"),c(Lx,"href","/docs/transformers/pr_15682/en/model_doc/camembert#transformers.CamembertModel"),c(Bx,"href","/docs/transformers/pr_15682/en/model_doc/canine#transformers.CanineModel"),c(kx,"href","/docs/transformers/pr_15682/en/model_doc/clip#transformers.CLIPModel"),c(xx,"href","/docs/transformers/pr_15682/en/model_doc/convbert#transformers.ConvBertModel"),c(Rx,"href","/docs/transformers/pr_15682/en/model_doc/convnext#transformers.ConvNextModel"),c(Sx,"href","/docs/transformers/pr_15682/en/model_doc/ctrl#transformers.CTRLModel"),c(Px,"href","/docs/transformers/pr_15682/en/model_doc/deberta#transformers.DebertaModel"),c($x,"href","/docs/transformers/pr_15682/en/model_doc/deberta-v2#transformers.DebertaV2Model"),c(Ix,"href","/docs/transformers/pr_15682/en/model_doc/deit#transformers.DeiTModel"),c(jx,"href","/docs/transformers/pr_15682/en/model_doc/detr#transformers.DetrModel"),c(Nx,"href","/docs/transformers/pr_15682/en/model_doc/distilbert#transformers.DistilBertModel"),c(Dx,"href","/docs/transformers/pr_15682/en/model_doc/dpr#transformers.DPRQuestionEncoder"),c(qx,"href","/docs/transformers/pr_15682/en/model_doc/electra#transformers.ElectraModel"),c(Gx,"href","/docs/transformers/pr_15682/en/model_doc/flaubert#transformers.FlaubertModel"),c(Ox,"href","/docs/transformers/pr_15682/en/model_doc/fnet#transformers.FNetModel"),c(Xx,"href","/docs/transformers/pr_15682/en/model_doc/fsmt#transformers.FSMTModel"),c(zx,"href","/docs/transformers/pr_15682/en/model_doc/funnel#transformers.FunnelModel"),c(Vx,"href","/docs/transformers/pr_15682/en/model_doc/funnel#transformers.FunnelBaseModel"),c(Wx,"href","/docs/transformers/pr_15682/en/model_doc/gpt2#transformers.GPT2Model"),c(Qx,"href","/docs/transformers/pr_15682/en/model_doc/gpt_neo#transformers.GPTNeoModel"),c(Hx,"href","/docs/transformers/pr_15682/en/model_doc/gptj#transformers.GPTJModel"),c(Ux,"href","/docs/transformers/pr_15682/en/model_doc/hubert#transformers.HubertModel"),c(Jx,"href","/docs/transformers/pr_15682/en/model_doc/ibert#transformers.IBertModel"),c(Yx,"href","/docs/transformers/pr_15682/en/model_doc/imagegpt#transformers.ImageGPTModel"),c(Kx,"href","/docs/transformers/pr_15682/en/model_doc/layoutlm#transformers.LayoutLMModel"),c(Zx,"href","/docs/transformers/pr_15682/en/model_doc/layoutlmv2#transformers.LayoutLMv2Model"),c(eR,"href","/docs/transformers/pr_15682/en/model_doc/led#transformers.LEDModel"),c(oR,"href","/docs/transformers/pr_15682/en/model_doc/longformer#transformers.LongformerModel"),c(rR,"href","/docs/transformers/pr_15682/en/model_doc/luke#transformers.LukeModel"),c(tR,"href","/docs/transformers/pr_15682/en/model_doc/lxmert#transformers.LxmertModel"),c(aR,"href","/docs/transformers/pr_15682/en/model_doc/m2m_100#transformers.M2M100Model"),c(nR,"href","/docs/transformers/pr_15682/en/model_doc/marian#transformers.MarianModel"),c(sR,"href","/docs/transformers/pr_15682/en/model_doc/maskformer#transformers.MaskFormerModel"),c(lR,"href","/docs/transformers/pr_15682/en/model_doc/mbart#transformers.MBartModel"),c(iR,"href","/docs/transformers/pr_15682/en/model_doc/megatron-bert#transformers.MegatronBertModel"),c(dR,"href","/docs/transformers/pr_15682/en/model_doc/mobilebert#transformers.MobileBertModel"),c(cR,"href","/docs/transformers/pr_15682/en/model_doc/mpnet#transformers.MPNetModel"),c(fR,"href","/docs/transformers/pr_15682/en/model_doc/mt5#transformers.MT5Model"),c(mR,"href","/docs/transformers/pr_15682/en/model_doc/nystromformer#transformers.NystromformerModel"),c(gR,"href","/docs/transformers/pr_15682/en/model_doc/openai-gpt#transformers.OpenAIGPTModel"),c(hR,"href","/docs/transformers/pr_15682/en/model_doc/pegasus#transformers.PegasusModel"),c(pR,"href","/docs/transformers/pr_15682/en/model_doc/perceiver#transformers.PerceiverModel"),c(_R,"href","/docs/transformers/pr_15682/en/model_doc/plbart#transformers.PLBartModel"),c(uR,"href","/docs/transformers/pr_15682/en/model_doc/poolformer#transformers.PoolFormerModel"),c(bR,"href","/docs/transformers/pr_15682/en/model_doc/prophetnet#transformers.ProphetNetModel"),c(vR,"href","/docs/transformers/pr_15682/en/model_doc/qdqbert#transformers.QDQBertModel"),c(TR,"href","/docs/transformers/pr_15682/en/model_doc/reformer#transformers.ReformerModel"),c(FR,"href","/docs/transformers/pr_15682/en/model_doc/rembert#transformers.RemBertModel"),c(CR,"href","/docs/transformers/pr_15682/en/model_doc/retribert#transformers.RetriBertModel"),c(MR,"href","/docs/transformers/pr_15682/en/model_doc/roberta#transformers.RobertaModel"),c(ER,"href","/docs/transformers/pr_15682/en/model_doc/roformer#transformers.RoFormerModel"),c(yR,"href","/docs/transformers/pr_15682/en/model_doc/segformer#transformers.SegformerModel"),c(wR,"href","/docs/transformers/pr_15682/en/model_doc/sew#transformers.SEWModel"),c(AR,"href","/docs/transformers/pr_15682/en/model_doc/sew-d#transformers.SEWDModel"),c(LR,"href","/docs/transformers/pr_15682/en/model_doc/speech_to_text#transformers.Speech2TextModel"),c(BR,"href","/docs/transformers/pr_15682/en/model_doc/splinter#transformers.SplinterModel"),c(kR,"href","/docs/transformers/pr_15682/en/model_doc/squeezebert#transformers.SqueezeBertModel"),c(xR,"href","/docs/transformers/pr_15682/en/model_doc/swin#transformers.SwinModel"),c(RR,"href","/docs/transformers/pr_15682/en/model_doc/t5#transformers.T5Model"),c(SR,"href","/docs/transformers/pr_15682/en/model_doc/tapas#transformers.TapasModel"),c(PR,"href","/docs/transformers/pr_15682/en/model_doc/transfo-xl#transformers.TransfoXLModel"),c($R,"href","/docs/transformers/pr_15682/en/model_doc/unispeech#transformers.UniSpeechModel"),c(IR,"href","/docs/transformers/pr_15682/en/model_doc/unispeech-sat#transformers.UniSpeechSatModel"),c(jR,"href","/docs/transformers/pr_15682/en/model_doc/vilt#transformers.ViltModel"),c(NR,"href","/docs/transformers/pr_15682/en/model_doc/vision-text-dual-encoder#transformers.VisionTextDualEncoderModel"),c(DR,"href","/docs/transformers/pr_15682/en/model_doc/visual_bert#transformers.VisualBertModel"),c(qR,"href","/docs/transformers/pr_15682/en/model_doc/vit#transformers.ViTModel"),c(GR,"href","/docs/transformers/pr_15682/en/model_doc/vit_mae#transformers.ViTMAEModel"),c(OR,"href","/docs/transformers/pr_15682/en/model_doc/wav2vec2#transformers.Wav2Vec2Model"),c(XR,"href","/docs/transformers/pr_15682/en/model_doc/wavlm#transformers.WavLMModel"),c(zR,"href","/docs/transformers/pr_15682/en/model_doc/xglm#transformers.XGLMModel"),c(VR,"href","/docs/transformers/pr_15682/en/model_doc/xlm#transformers.XLMModel"),c(WR,"href","/docs/transformers/pr_15682/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetModel"),c(QR,"href","/docs/transformers/pr_15682/en/model_doc/xlm-roberta#transformers.XLMRobertaModel"),c(HR,"href","/docs/transformers/pr_15682/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLModel"),c(UR,"href","/docs/transformers/pr_15682/en/model_doc/xlnet#transformers.XLNetModel"),c(JR,"href","/docs/transformers/pr_15682/en/model_doc/yoso#transformers.YosoModel"),c(ke,"class","docstring"),c(Vo,"class","docstring"),c(e_,"id","transformers.AutoModelForPreTraining"),c(e_,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(e_,"href","#transformers.AutoModelForPreTraining"),c(Xi,"class","relative group"),c(Dr,"class","docstring"),c(YR,"href","/docs/transformers/pr_15682/en/model_doc/albert#transformers.AlbertForPreTraining"),c(KR,"href","/docs/transformers/pr_15682/en/model_doc/bart#transformers.BartForConditionalGeneration"),c(ZR,"href","/docs/transformers/pr_15682/en/model_doc/bert#transformers.BertForPreTraining"),c(eS,"href","/docs/transformers/pr_15682/en/model_doc/big_bird#transformers.BigBirdForPreTraining"),c(oS,"href","/docs/transformers/pr_15682/en/model_doc/camembert#transformers.CamembertForMaskedLM"),c(rS,"href","/docs/transformers/pr_15682/en/model_doc/ctrl#transformers.CTRLLMHeadModel"),c(tS,"href","/docs/transformers/pr_15682/en/model_doc/deberta#transformers.DebertaForMaskedLM"),c(aS,"href","/docs/transformers/pr_15682/en/model_doc/deberta-v2#transformers.DebertaV2ForMaskedLM"),c(nS,"href","/docs/transformers/pr_15682/en/model_doc/distilbert#transformers.DistilBertForMaskedLM"),c(sS,"href","/docs/transformers/pr_15682/en/model_doc/electra#transformers.ElectraForPreTraining"),c(lS,"href","/docs/transformers/pr_15682/en/model_doc/flaubert#transformers.FlaubertWithLMHeadModel"),c(iS,"href","/docs/transformers/pr_15682/en/model_doc/fnet#transformers.FNetForPreTraining"),c(dS,"href","/docs/transformers/pr_15682/en/model_doc/fsmt#transformers.FSMTForConditionalGeneration"),c(cS,"href","/docs/transformers/pr_15682/en/model_doc/funnel#transformers.FunnelForPreTraining"),c(fS,"href","/docs/transformers/pr_15682/en/model_doc/gpt2#transformers.GPT2LMHeadModel"),c(mS,"href","/docs/transformers/pr_15682/en/model_doc/ibert#transformers.IBertForMaskedLM"),c(gS,"href","/docs/transformers/pr_15682/en/model_doc/layoutlm#transformers.LayoutLMForMaskedLM"),c(hS,"href","/docs/transformers/pr_15682/en/model_doc/longformer#transformers.LongformerForMaskedLM"),c(pS,"href","/docs/transformers/pr_15682/en/model_doc/lxmert#transformers.LxmertForPreTraining"),c(_S,"href","/docs/transformers/pr_15682/en/model_doc/megatron-bert#transformers.MegatronBertForPreTraining"),c(uS,"href","/docs/transformers/pr_15682/en/model_doc/mobilebert#transformers.MobileBertForPreTraining"),c(bS,"href","/docs/transformers/pr_15682/en/model_doc/mpnet#transformers.MPNetForMaskedLM"),c(vS,"href","/docs/transformers/pr_15682/en/model_doc/openai-gpt#transformers.OpenAIGPTLMHeadModel"),c(TS,"href","/docs/transformers/pr_15682/en/model_doc/retribert#transformers.RetriBertModel"),c(FS,"href","/docs/transformers/pr_15682/en/model_doc/roberta#transformers.RobertaForMaskedLM"),c(CS,"href","/docs/transformers/pr_15682/en/model_doc/squeezebert#transformers.SqueezeBertForMaskedLM"),c(MS,"href","/docs/transformers/pr_15682/en/model_doc/t5#transformers.T5ForConditionalGeneration"),c(ES,"href","/docs/transformers/pr_15682/en/model_doc/tapas#transformers.TapasForMaskedLM"),c(yS,"href","/docs/transformers/pr_15682/en/model_doc/transfo-xl#transformers.TransfoXLLMHeadModel"),c(wS,"href","/docs/transformers/pr_15682/en/model_doc/unispeech#transformers.UniSpeechForPreTraining"),c(AS,"href","/docs/transformers/pr_15682/en/model_doc/unispeech-sat#transformers.UniSpeechSatForPreTraining"),c(LS,"href","/docs/transformers/pr_15682/en/model_doc/visual_bert#transformers.VisualBertForPreTraining"),c(BS,"href","/docs/transformers/pr_15682/en/model_doc/vit_mae#transformers.ViTMAEForPreTraining"),c(kS,"href","/docs/transformers/pr_15682/en/model_doc/wav2vec2#transformers.Wav2Vec2ForPreTraining"),c(xS,"href","/docs/transformers/pr_15682/en/model_doc/xlm#transformers.XLMWithLMHeadModel"),c(RS,"href","/docs/transformers/pr_15682/en/model_doc/xlm-roberta#transformers.XLMRobertaForMaskedLM"),c(SS,"href","/docs/transformers/pr_15682/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForMaskedLM"),c(PS,"href","/docs/transformers/pr_15682/en/model_doc/xlnet#transformers.XLNetLMHeadModel"),c(xe,"class","docstring"),c(Wo,"class","docstring"),c(q_,"id","transformers.AutoModelForCausalLM"),c(q_,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(q_,"href","#transformers.AutoModelForCausalLM"),c(Wi,"class","relative group"),c(qr,"class","docstring"),c($S,"href","/docs/transformers/pr_15682/en/model_doc/bart#transformers.BartForCausalLM"),c(IS,"href","/docs/transformers/pr_15682/en/model_doc/bert#transformers.BertLMHeadModel"),c(jS,"href","/docs/transformers/pr_15682/en/model_doc/bert-generation#transformers.BertGenerationDecoder"),c(NS,"href","/docs/transformers/pr_15682/en/model_doc/big_bird#transformers.BigBirdForCausalLM"),c(DS,"href","/docs/transformers/pr_15682/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusForCausalLM"),c(qS,"href","/docs/transformers/pr_15682/en/model_doc/blenderbot#transformers.BlenderbotForCausalLM"),c(GS,"href","/docs/transformers/pr_15682/en/model_doc/blenderbot-small#transformers.BlenderbotSmallForCausalLM"),c(OS,"href","/docs/transformers/pr_15682/en/model_doc/camembert#transformers.CamembertForCausalLM"),c(XS,"href","/docs/transformers/pr_15682/en/model_doc/ctrl#transformers.CTRLLMHeadModel"),c(zS,"href","/docs/transformers/pr_15682/en/model_doc/electra#transformers.ElectraForCausalLM"),c(VS,"href","/docs/transformers/pr_15682/en/model_doc/gpt2#transformers.GPT2LMHeadModel"),c(WS,"href","/docs/transformers/pr_15682/en/model_doc/gpt_neo#transformers.GPTNeoForCausalLM"),c(QS,"href","/docs/transformers/pr_15682/en/model_doc/gptj#transformers.GPTJForCausalLM"),c(HS,"href","/docs/transformers/pr_15682/en/model_doc/marian#transformers.MarianForCausalLM"),c(US,"href","/docs/transformers/pr_15682/en/model_doc/mbart#transformers.MBartForCausalLM"),c(JS,"href","/docs/transformers/pr_15682/en/model_doc/megatron-bert#transformers.MegatronBertForCausalLM"),c(YS,"href","/docs/transformers/pr_15682/en/model_doc/openai-gpt#transformers.OpenAIGPTLMHeadModel"),c(KS,"href","/docs/transformers/pr_15682/en/model_doc/pegasus#transformers.PegasusForCausalLM"),c(ZS,"href","/docs/transformers/pr_15682/en/model_doc/plbart#transformers.PLBartForCausalLM"),c(eP,"href","/docs/transformers/pr_15682/en/model_doc/prophetnet#transformers.ProphetNetForCausalLM"),c(oP,"href","/docs/transformers/pr_15682/en/model_doc/qdqbert#transformers.QDQBertLMHeadModel"),c(rP,"href","/docs/transformers/pr_15682/en/model_doc/reformer#transformers.ReformerModelWithLMHead"),c(tP,"href","/docs/transformers/pr_15682/en/model_doc/rembert#transformers.RemBertForCausalLM"),c(aP,"href","/docs/transformers/pr_15682/en/model_doc/roberta#transformers.RobertaForCausalLM"),c(nP,"href","/docs/transformers/pr_15682/en/model_doc/roformer#transformers.RoFormerForCausalLM"),c(sP,"href","/docs/transformers/pr_15682/en/model_doc/speech_to_text_2#transformers.Speech2Text2ForCausalLM"),c(lP,"href","/docs/transformers/pr_15682/en/model_doc/transfo-xl#transformers.TransfoXLLMHeadModel"),c(iP,"href","/docs/transformers/pr_15682/en/model_doc/trocr#transformers.TrOCRForCausalLM"),c(dP,"href","/docs/transformers/pr_15682/en/model_doc/xglm#transformers.XGLMForCausalLM"),c(cP,"href","/docs/transformers/pr_15682/en/model_doc/xlm#transformers.XLMWithLMHeadModel"),c(fP,"href","/docs/transformers/pr_15682/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetForCausalLM"),c(mP,"href","/docs/transformers/pr_15682/en/model_doc/xlm-roberta#transformers.XLMRobertaForCausalLM"),c(gP,"href","/docs/transformers/pr_15682/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForCausalLM"),c(hP,"href","/docs/transformers/pr_15682/en/model_doc/xlnet#transformers.XLNetLMHeadModel"),c(Re,"class","docstring"),c(Qo,"class","docstring"),c(Cu,"id","transformers.AutoModelForMaskedLM"),c(Cu,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Cu,"href","#transformers.AutoModelForMaskedLM"),c(Ui,"class","relative group"),c(Gr,"class","docstring"),c(pP,"href","/docs/transformers/pr_15682/en/model_doc/albert#transformers.AlbertForMaskedLM"),c(_P,"href","/docs/transformers/pr_15682/en/model_doc/bart#transformers.BartForConditionalGeneration"),c(uP,"href","/docs/transformers/pr_15682/en/model_doc/bert#transformers.BertForMaskedLM"),c(bP,"href","/docs/transformers/pr_15682/en/model_doc/big_bird#transformers.BigBirdForMaskedLM"),c(vP,"href","/docs/transformers/pr_15682/en/model_doc/camembert#transformers.CamembertForMaskedLM"),c(TP,"href","/docs/transformers/pr_15682/en/model_doc/convbert#transformers.ConvBertForMaskedLM"),c(FP,"href","/docs/transformers/pr_15682/en/model_doc/deberta#transformers.DebertaForMaskedLM"),c(CP,"href","/docs/transformers/pr_15682/en/model_doc/deberta-v2#transformers.DebertaV2ForMaskedLM"),c(MP,"href","/docs/transformers/pr_15682/en/model_doc/distilbert#transformers.DistilBertForMaskedLM"),c(EP,"href","/docs/transformers/pr_15682/en/model_doc/electra#transformers.ElectraForMaskedLM"),c(yP,"href","/docs/transformers/pr_15682/en/model_doc/flaubert#transformers.FlaubertWithLMHeadModel"),c(wP,"href","/docs/transformers/pr_15682/en/model_doc/fnet#transformers.FNetForMaskedLM"),c(AP,"href","/docs/transformers/pr_15682/en/model_doc/funnel#transformers.FunnelForMaskedLM"),c(LP,"href","/docs/transformers/pr_15682/en/model_doc/ibert#transformers.IBertForMaskedLM"),c(BP,"href","/docs/transformers/pr_15682/en/model_doc/layoutlm#transformers.LayoutLMForMaskedLM"),c(kP,"href","/docs/transformers/pr_15682/en/model_doc/longformer#transformers.LongformerForMaskedLM"),c(xP,"href","/docs/transformers/pr_15682/en/model_doc/mbart#transformers.MBartForConditionalGeneration"),c(RP,"href","/docs/transformers/pr_15682/en/model_doc/megatron-bert#transformers.MegatronBertForMaskedLM"),c(SP,"href","/docs/transformers/pr_15682/en/model_doc/mobilebert#transformers.MobileBertForMaskedLM"),c(PP,"href","/docs/transformers/pr_15682/en/model_doc/mpnet#transformers.MPNetForMaskedLM"),c($P,"href","/docs/transformers/pr_15682/en/model_doc/nystromformer#transformers.NystromformerForMaskedLM"),c(IP,"href","/docs/transformers/pr_15682/en/model_doc/perceiver#transformers.PerceiverForMaskedLM"),c(jP,"href","/docs/transformers/pr_15682/en/model_doc/qdqbert#transformers.QDQBertForMaskedLM"),c(NP,"href","/docs/transformers/pr_15682/en/model_doc/reformer#transformers.ReformerForMaskedLM"),c(DP,"href","/docs/transformers/pr_15682/en/model_doc/rembert#transformers.RemBertForMaskedLM"),c(qP,"href","/docs/transformers/pr_15682/en/model_doc/roberta#transformers.RobertaForMaskedLM"),c(GP,"href","/docs/transformers/pr_15682/en/model_doc/roformer#transformers.RoFormerForMaskedLM"),c(OP,"href","/docs/transformers/pr_15682/en/model_doc/squeezebert#transformers.SqueezeBertForMaskedLM"),c(XP,"href","/docs/transformers/pr_15682/en/model_doc/tapas#transformers.TapasForMaskedLM"),c(zP,"href","/docs/transformers/pr_15682/en/model_doc/xlm#transformers.XLMWithLMHeadModel"),c(VP,"href","/docs/transformers/pr_15682/en/model_doc/xlm-roberta#transformers.XLMRobertaForMaskedLM"),c(WP,"href","/docs/transformers/pr_15682/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForMaskedLM"),c(QP,"href","/docs/transformers/pr_15682/en/model_doc/yoso#transformers.YosoForMaskedLM"),c(Se,"class","docstring"),c(Ho,"class","docstring"),c(a2,"id","transformers.AutoModelForSeq2SeqLM"),c(a2,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(a2,"href","#transformers.AutoModelForSeq2SeqLM"),c(Ki,"class","relative group"),c(Or,"class","docstring"),c(HP,"href","/docs/transformers/pr_15682/en/model_doc/bart#transformers.BartForConditionalGeneration"),c(UP,"href","/docs/transformers/pr_15682/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusForConditionalGeneration"),c(JP,"href","/docs/transformers/pr_15682/en/model_doc/blenderbot#transformers.BlenderbotForConditionalGeneration"),c(YP,"href","/docs/transformers/pr_15682/en/model_doc/blenderbot-small#transformers.BlenderbotSmallForConditionalGeneration"),c(KP,"href","/docs/transformers/pr_15682/en/model_doc/encoder-decoder#transformers.EncoderDecoderModel"),c(ZP,"href","/docs/transformers/pr_15682/en/model_doc/fsmt#transformers.FSMTForConditionalGeneration"),c(e$,"href","/docs/transformers/pr_15682/en/model_doc/led#transformers.LEDForConditionalGeneration"),c(o$,"href","/docs/transformers/pr_15682/en/model_doc/m2m_100#transformers.M2M100ForConditionalGeneration"),c(r$,"href","/docs/transformers/pr_15682/en/model_doc/marian#transformers.MarianMTModel"),c(t$,"href","/docs/transformers/pr_15682/en/model_doc/mbart#transformers.MBartForConditionalGeneration"),c(a$,"href","/docs/transformers/pr_15682/en/model_doc/mt5#transformers.MT5ForConditionalGeneration"),c(n$,"href","/docs/transformers/pr_15682/en/model_doc/pegasus#transformers.PegasusForConditionalGeneration"),c(s$,"href","/docs/transformers/pr_15682/en/model_doc/plbart#transformers.PLBartForConditionalGeneration"),c(l$,"href","/docs/transformers/pr_15682/en/model_doc/prophetnet#transformers.ProphetNetForConditionalGeneration"),c(i$,"href","/docs/transformers/pr_15682/en/model_doc/t5#transformers.T5ForConditionalGeneration"),c(d$,"href","/docs/transformers/pr_15682/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetForConditionalGeneration"),c(Pe,"class","docstring"),c(Uo,"class","docstring"),c(C2,"id","transformers.AutoModelForSequenceClassification"),c(C2,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(C2,"href","#transformers.AutoModelForSequenceClassification"),c(od,"class","relative group"),c(Xr,"class","docstring"),c(c$,"href","/docs/transformers/pr_15682/en/model_doc/albert#transformers.AlbertForSequenceClassification"),c(f$,"href","/docs/transformers/pr_15682/en/model_doc/bart#transformers.BartForSequenceClassification"),c(m$,"href","/docs/transformers/pr_15682/en/model_doc/bert#transformers.BertForSequenceClassification"),c(g$,"href","/docs/transformers/pr_15682/en/model_doc/big_bird#transformers.BigBirdForSequenceClassification"),c(h$,"href","/docs/transformers/pr_15682/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusForSequenceClassification"),c(p$,"href","/docs/transformers/pr_15682/en/model_doc/camembert#transformers.CamembertForSequenceClassification"),c(_$,"href","/docs/transformers/pr_15682/en/model_doc/canine#transformers.CanineForSequenceClassification"),c(u$,"href","/docs/transformers/pr_15682/en/model_doc/convbert#transformers.ConvBertForSequenceClassification"),c(b$,"href","/docs/transformers/pr_15682/en/model_doc/ctrl#transformers.CTRLForSequenceClassification"),c(v$,"href","/docs/transformers/pr_15682/en/model_doc/deberta#transformers.DebertaForSequenceClassification"),c(T$,"href","/docs/transformers/pr_15682/en/model_doc/deberta-v2#transformers.DebertaV2ForSequenceClassification"),c(F$,"href","/docs/transformers/pr_15682/en/model_doc/distilbert#transformers.DistilBertForSequenceClassification"),c(C$,"href","/docs/transformers/pr_15682/en/model_doc/electra#transformers.ElectraForSequenceClassification"),c(M$,"href","/docs/transformers/pr_15682/en/model_doc/flaubert#transformers.FlaubertForSequenceClassification"),c(E$,"href","/docs/transformers/pr_15682/en/model_doc/fnet#transformers.FNetForSequenceClassification"),c(y$,"href","/docs/transformers/pr_15682/en/model_doc/funnel#transformers.FunnelForSequenceClassification"),c(w$,"href","/docs/transformers/pr_15682/en/model_doc/gpt2#transformers.GPT2ForSequenceClassification"),c(A$,"href","/docs/transformers/pr_15682/en/model_doc/gpt_neo#transformers.GPTNeoForSequenceClassification"),c(L$,"href","/docs/transformers/pr_15682/en/model_doc/gptj#transformers.GPTJForSequenceClassification"),c(B$,"href","/docs/transformers/pr_15682/en/model_doc/ibert#transformers.IBertForSequenceClassification"),c(k$,"href","/docs/transformers/pr_15682/en/model_doc/layoutlm#transformers.LayoutLMForSequenceClassification"),c(x$,"href","/docs/transformers/pr_15682/en/model_doc/layoutlmv2#transformers.LayoutLMv2ForSequenceClassification"),c(R$,"href","/docs/transformers/pr_15682/en/model_doc/led#transformers.LEDForSequenceClassification"),c(S$,"href","/docs/transformers/pr_15682/en/model_doc/longformer#transformers.LongformerForSequenceClassification"),c(P$,"href","/docs/transformers/pr_15682/en/model_doc/mbart#transformers.MBartForSequenceClassification"),c($$,"href","/docs/transformers/pr_15682/en/model_doc/megatron-bert#transformers.MegatronBertForSequenceClassification"),c(I$,"href","/docs/transformers/pr_15682/en/model_doc/mobilebert#transformers.MobileBertForSequenceClassification"),c(j$,"href","/docs/transformers/pr_15682/en/model_doc/mpnet#transformers.MPNetForSequenceClassification"),c(N$,"href","/docs/transformers/pr_15682/en/model_doc/nystromformer#transformers.NystromformerForSequenceClassification"),c(D$,"href","/docs/transformers/pr_15682/en/model_doc/openai-gpt#transformers.OpenAIGPTForSequenceClassification"),c(q$,"href","/docs/transformers/pr_15682/en/model_doc/perceiver#transformers.PerceiverForSequenceClassification"),c(G$,"href","/docs/transformers/pr_15682/en/model_doc/plbart#transformers.PLBartForSequenceClassification"),c(O$,"href","/docs/transformers/pr_15682/en/model_doc/qdqbert#transformers.QDQBertForSequenceClassification"),c(X$,"href","/docs/transformers/pr_15682/en/model_doc/reformer#transformers.ReformerForSequenceClassification"),c(z$,"href","/docs/transformers/pr_15682/en/model_doc/rembert#transformers.RemBertForSequenceClassification"),c(V$,"href","/docs/transformers/pr_15682/en/model_doc/roberta#transformers.RobertaForSequenceClassification"),c(W$,"href","/docs/transformers/pr_15682/en/model_doc/roformer#transformers.RoFormerForSequenceClassification"),c(Q$,"href","/docs/transformers/pr_15682/en/model_doc/squeezebert#transformers.SqueezeBertForSequenceClassification"),c(H$,"href","/docs/transformers/pr_15682/en/model_doc/tapas#transformers.TapasForSequenceClassification"),c(U$,"href","/docs/transformers/pr_15682/en/model_doc/transfo-xl#transformers.TransfoXLForSequenceClassification"),c(J$,"href","/docs/transformers/pr_15682/en/model_doc/xlm#transformers.XLMForSequenceClassification"),c(Y$,"href","/docs/transformers/pr_15682/en/model_doc/xlm-roberta#transformers.XLMRobertaForSequenceClassification"),c(K$,"href","/docs/transformers/pr_15682/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForSequenceClassification"),c(Z$,"href","/docs/transformers/pr_15682/en/model_doc/xlnet#transformers.XLNetForSequenceClassification"),c(eI,"href","/docs/transformers/pr_15682/en/model_doc/yoso#transformers.YosoForSequenceClassification"),c($e,"class","docstring"),c(Jo,"class","docstring"),c(p1,"id","transformers.AutoModelForMultipleChoice"),c(p1,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(p1,"href","#transformers.AutoModelForMultipleChoice"),c(ad,"class","relative group"),c(zr,"class","docstring"),c(oI,"href","/docs/transformers/pr_15682/en/model_doc/albert#transformers.AlbertForMultipleChoice"),c(rI,"href","/docs/transformers/pr_15682/en/model_doc/bert#transformers.BertForMultipleChoice"),c(tI,"href","/docs/transformers/pr_15682/en/model_doc/big_bird#transformers.BigBirdForMultipleChoice"),c(aI,"href","/docs/transformers/pr_15682/en/model_doc/camembert#transformers.CamembertForMultipleChoice"),c(nI,"href","/docs/transformers/pr_15682/en/model_doc/canine#transformers.CanineForMultipleChoice"),c(sI,"href","/docs/transformers/pr_15682/en/model_doc/convbert#transformers.ConvBertForMultipleChoice"),c(lI,"href","/docs/transformers/pr_15682/en/model_doc/distilbert#transformers.DistilBertForMultipleChoice"),c(iI,"href","/docs/transformers/pr_15682/en/model_doc/electra#transformers.ElectraForMultipleChoice"),c(dI,"href","/docs/transformers/pr_15682/en/model_doc/flaubert#transformers.FlaubertForMultipleChoice"),c(cI,"href","/docs/transformers/pr_15682/en/model_doc/fnet#transformers.FNetForMultipleChoice"),c(fI,"href","/docs/transformers/pr_15682/en/model_doc/funnel#transformers.FunnelForMultipleChoice"),c(mI,"href","/docs/transformers/pr_15682/en/model_doc/ibert#transformers.IBertForMultipleChoice"),c(gI,"href","/docs/transformers/pr_15682/en/model_doc/longformer#transformers.LongformerForMultipleChoice"),c(hI,"href","/docs/transformers/pr_15682/en/model_doc/megatron-bert#transformers.MegatronBertForMultipleChoice"),c(pI,"href","/docs/transformers/pr_15682/en/model_doc/mobilebert#transformers.MobileBertForMultipleChoice"),c(_I,"href","/docs/transformers/pr_15682/en/model_doc/mpnet#transformers.MPNetForMultipleChoice"),c(uI,"href","/docs/transformers/pr_15682/en/model_doc/nystromformer#transformers.NystromformerForMultipleChoice"),c(bI,"href","/docs/transformers/pr_15682/en/model_doc/qdqbert#transformers.QDQBertForMultipleChoice"),c(vI,"href","/docs/transformers/pr_15682/en/model_doc/rembert#transformers.RemBertForMultipleChoice"),c(TI,"href","/docs/transformers/pr_15682/en/model_doc/roberta#transformers.RobertaForMultipleChoice"),c(FI,"href","/docs/transformers/pr_15682/en/model_doc/roformer#transformers.RoFormerForMultipleChoice"),c(CI,"href","/docs/transformers/pr_15682/en/model_doc/squeezebert#transformers.SqueezeBertForMultipleChoice"),c(MI,"href","/docs/transformers/pr_15682/en/model_doc/xlm#transformers.XLMForMultipleChoice"),c(EI,"href","/docs/transformers/pr_15682/en/model_doc/xlm-roberta#transformers.XLMRobertaForMultipleChoice"),c(yI,"href","/docs/transformers/pr_15682/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForMultipleChoice"),c(wI,"href","/docs/transformers/pr_15682/en/model_doc/xlnet#transformers.XLNetForMultipleChoice"),c(AI,"href","/docs/transformers/pr_15682/en/model_doc/yoso#transformers.YosoForMultipleChoice"),c(Ie,"class","docstring"),c(Yo,"class","docstring"),c(z1,"id","transformers.AutoModelForNextSentencePrediction"),c(z1,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(z1,"href","#transformers.AutoModelForNextSentencePrediction"),c(ld,"class","relative group"),c(Vr,"class","docstring"),c(LI,"href","/docs/transformers/pr_15682/en/model_doc/bert#transformers.BertForNextSentencePrediction"),c(BI,"href","/docs/transformers/pr_15682/en/model_doc/fnet#transformers.FNetForNextSentencePrediction"),c(kI,"href","/docs/transformers/pr_15682/en/model_doc/megatron-bert#transformers.MegatronBertForNextSentencePrediction"),c(xI,"href","/docs/transformers/pr_15682/en/model_doc/mobilebert#transformers.MobileBertForNextSentencePrediction"),c(RI,"href","/docs/transformers/pr_15682/en/model_doc/qdqbert#transformers.QDQBertForNextSentencePrediction"),c(je,"class","docstring"),c(Ko,"class","docstring"),c(Y1,"id","transformers.AutoModelForTokenClassification"),c(Y1,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Y1,"href","#transformers.AutoModelForTokenClassification"),c(cd,"class","relative group"),c(Wr,"class","docstring"),c(SI,"href","/docs/transformers/pr_15682/en/model_doc/albert#transformers.AlbertForTokenClassification"),c(PI,"href","/docs/transformers/pr_15682/en/model_doc/bert#transformers.BertForTokenClassification"),c($I,"href","/docs/transformers/pr_15682/en/model_doc/big_bird#transformers.BigBirdForTokenClassification"),c(II,"href","/docs/transformers/pr_15682/en/model_doc/camembert#transformers.CamembertForTokenClassification"),c(jI,"href","/docs/transformers/pr_15682/en/model_doc/canine#transformers.CanineForTokenClassification"),c(NI,"href","/docs/transformers/pr_15682/en/model_doc/convbert#transformers.ConvBertForTokenClassification"),c(DI,"href","/docs/transformers/pr_15682/en/model_doc/deberta#transformers.DebertaForTokenClassification"),c(qI,"href","/docs/transformers/pr_15682/en/model_doc/deberta-v2#transformers.DebertaV2ForTokenClassification"),c(GI,"href","/docs/transformers/pr_15682/en/model_doc/distilbert#transformers.DistilBertForTokenClassification"),c(OI,"href","/docs/transformers/pr_15682/en/model_doc/electra#transformers.ElectraForTokenClassification"),c(XI,"href","/docs/transformers/pr_15682/en/model_doc/flaubert#transformers.FlaubertForTokenClassification"),c(zI,"href","/docs/transformers/pr_15682/en/model_doc/fnet#transformers.FNetForTokenClassification"),c(VI,"href","/docs/transformers/pr_15682/en/model_doc/funnel#transformers.FunnelForTokenClassification"),c(WI,"href","/docs/transformers/pr_15682/en/model_doc/gpt2#transformers.GPT2ForTokenClassification"),c(QI,"href","/docs/transformers/pr_15682/en/model_doc/ibert#transformers.IBertForTokenClassification"),c(HI,"href","/docs/transformers/pr_15682/en/model_doc/layoutlm#transformers.LayoutLMForTokenClassification"),c(UI,"href","/docs/transformers/pr_15682/en/model_doc/layoutlmv2#transformers.LayoutLMv2ForTokenClassification"),c(JI,"href","/docs/transformers/pr_15682/en/model_doc/longformer#transformers.LongformerForTokenClassification"),c(YI,"href","/docs/transformers/pr_15682/en/model_doc/megatron-bert#transformers.MegatronBertForTokenClassification"),c(KI,"href","/docs/transformers/pr_15682/en/model_doc/mobilebert#transformers.MobileBertForTokenClassification"),c(ZI,"href","/docs/transformers/pr_15682/en/model_doc/mpnet#transformers.MPNetForTokenClassification"),c(ej,"href","/docs/transformers/pr_15682/en/model_doc/nystromformer#transformers.NystromformerForTokenClassification"),c(oj,"href","/docs/transformers/pr_15682/en/model_doc/qdqbert#transformers.QDQBertForTokenClassification"),c(rj,"href","/docs/transformers/pr_15682/en/model_doc/rembert#transformers.RemBertForTokenClassification"),c(tj,"href","/docs/transformers/pr_15682/en/model_doc/roberta#transformers.RobertaForTokenClassification"),c(aj,"href","/docs/transformers/pr_15682/en/model_doc/roformer#transformers.RoFormerForTokenClassification"),c(nj,"href","/docs/transformers/pr_15682/en/model_doc/squeezebert#transformers.SqueezeBertForTokenClassification"),c(sj,"href","/docs/transformers/pr_15682/en/model_doc/xlm#transformers.XLMForTokenClassification"),c(lj,"href","/docs/transformers/pr_15682/en/model_doc/xlm-roberta#transformers.XLMRobertaForTokenClassification"),c(ij,"href","/docs/transformers/pr_15682/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForTokenClassification"),c(dj,"href","/docs/transformers/pr_15682/en/model_doc/xlnet#transformers.XLNetForTokenClassification"),c(cj,"href","/docs/transformers/pr_15682/en/model_doc/yoso#transformers.YosoForTokenClassification"),c(Ne,"class","docstring"),c(Zo,"class","docstring"),c(xb,"id","transformers.AutoModelForQuestionAnswering"),c(xb,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(xb,"href","#transformers.AutoModelForQuestionAnswering"),c(gd,"class","relative group"),c(Qr,"class","docstring"),c(fj,"href","/docs/transformers/pr_15682/en/model_doc/albert#transformers.AlbertForQuestionAnswering"),c(mj,"href","/docs/transformers/pr_15682/en/model_doc/bart#transformers.BartForQuestionAnswering"),c(gj,"href","/docs/transformers/pr_15682/en/model_doc/bert#transformers.BertForQuestionAnswering"),c(hj,"href","/docs/transformers/pr_15682/en/model_doc/big_bird#transformers.BigBirdForQuestionAnswering"),c(pj,"href","/docs/transformers/pr_15682/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusForQuestionAnswering"),c(_j,"href","/docs/transformers/pr_15682/en/model_doc/camembert#transformers.CamembertForQuestionAnswering"),c(uj,"href","/docs/transformers/pr_15682/en/model_doc/canine#transformers.CanineForQuestionAnswering"),c(bj,"href","/docs/transformers/pr_15682/en/model_doc/convbert#transformers.ConvBertForQuestionAnswering"),c(vj,"href","/docs/transformers/pr_15682/en/model_doc/deberta#transformers.DebertaForQuestionAnswering"),c(Tj,"href","/docs/transformers/pr_15682/en/model_doc/deberta-v2#transformers.DebertaV2ForQuestionAnswering"),c(Fj,"href","/docs/transformers/pr_15682/en/model_doc/distilbert#transformers.DistilBertForQuestionAnswering"),c(Cj,"href","/docs/transformers/pr_15682/en/model_doc/electra#transformers.ElectraForQuestionAnswering"),c(Mj,"href","/docs/transformers/pr_15682/en/model_doc/flaubert#transformers.FlaubertForQuestionAnsweringSimple"),c(Ej,"href","/docs/transformers/pr_15682/en/model_doc/fnet#transformers.FNetForQuestionAnswering"),c(yj,"href","/docs/transformers/pr_15682/en/model_doc/funnel#transformers.FunnelForQuestionAnswering"),c(wj,"href","/docs/transformers/pr_15682/en/model_doc/gptj#transformers.GPTJForQuestionAnswering"),c(Aj,"href","/docs/transformers/pr_15682/en/model_doc/ibert#transformers.IBertForQuestionAnswering"),c(Lj,"href","/docs/transformers/pr_15682/en/model_doc/layoutlmv2#transformers.LayoutLMv2ForQuestionAnswering"),c(Bj,"href","/docs/transformers/pr_15682/en/model_doc/led#transformers.LEDForQuestionAnswering"),c(kj,"href","/docs/transformers/pr_15682/en/model_doc/longformer#transformers.LongformerForQuestionAnswering"),c(xj,"href","/docs/transformers/pr_15682/en/model_doc/lxmert#transformers.LxmertForQuestionAnswering"),c(Rj,"href","/docs/transformers/pr_15682/en/model_doc/mbart#transformers.MBartForQuestionAnswering"),c(Sj,"href","/docs/transformers/pr_15682/en/model_doc/megatron-bert#transformers.MegatronBertForQuestionAnswering"),c(Pj,"href","/docs/transformers/pr_15682/en/model_doc/mobilebert#transformers.MobileBertForQuestionAnswering"),c($j,"href","/docs/transformers/pr_15682/en/model_doc/mpnet#transformers.MPNetForQuestionAnswering"),c(Ij,"href","/docs/transformers/pr_15682/en/model_doc/nystromformer#transformers.NystromformerForQuestionAnswering"),c(jj,"href","/docs/transformers/pr_15682/en/model_doc/qdqbert#transformers.QDQBertForQuestionAnswering"),c(Nj,"href","/docs/transformers/pr_15682/en/model_doc/reformer#transformers.ReformerForQuestionAnswering"),c(Dj,"href","/docs/transformers/pr_15682/en/model_doc/rembert#transformers.RemBertForQuestionAnswering"),c(qj,"href","/docs/transformers/pr_15682/en/model_doc/roberta#transformers.RobertaForQuestionAnswering"),c(Gj,"href","/docs/transformers/pr_15682/en/model_doc/roformer#transformers.RoFormerForQuestionAnswering"),c(Oj,"href","/docs/transformers/pr_15682/en/model_doc/splinter#transformers.SplinterForQuestionAnswering"),c(Xj,"href","/docs/transformers/pr_15682/en/model_doc/squeezebert#transformers.SqueezeBertForQuestionAnswering"),c(zj,"href","/docs/transformers/pr_15682/en/model_doc/xlm#transformers.XLMForQuestionAnsweringSimple"),c(Vj,"href","/docs/transformers/pr_15682/en/model_doc/xlm-roberta#transformers.XLMRobertaForQuestionAnswering"),c(Wj,"href","/docs/transformers/pr_15682/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForQuestionAnswering"),c(Qj,"href","/docs/transformers/pr_15682/en/model_doc/xlnet#transformers.XLNetForQuestionAnsweringSimple"),c(Hj,"href","/docs/transformers/pr_15682/en/model_doc/yoso#transformers.YosoForQuestionAnswering"),c(De,"class","docstring"),c(er,"class","docstring"),c(u5,"id","transformers.AutoModelForTableQuestionAnswering"),c(u5,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(u5,"href","#transformers.AutoModelForTableQuestionAnswering"),c(_d,"class","relative group"),c(Hr,"class","docstring"),c(Uj,"href","/docs/transformers/pr_15682/en/model_doc/tapas#transformers.TapasForQuestionAnswering"),c(qe,"class","docstring"),c(or,"class","docstring"),c(T5,"id","transformers.AutoModelForImageClassification"),c(T5,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(T5,"href","#transformers.AutoModelForImageClassification"),c(vd,"class","relative group"),c(Ur,"class","docstring"),c(Jj,"href","/docs/transformers/pr_15682/en/model_doc/beit#transformers.BeitForImageClassification"),c(Yj,"href","/docs/transformers/pr_15682/en/model_doc/convnext#transformers.ConvNextForImageClassification"),c(Kj,"href","/docs/transformers/pr_15682/en/model_doc/deit#transformers.DeiTForImageClassification"),c(Zj,"href","/docs/transformers/pr_15682/en/model_doc/deit#transformers.DeiTForImageClassificationWithTeacher"),c(eN,"href","/docs/transformers/pr_15682/en/model_doc/imagegpt#transformers.ImageGPTForImageClassification"),c(oN,"href","/docs/transformers/pr_15682/en/model_doc/perceiver#transformers.PerceiverForImageClassificationLearned"),c(rN,"href","/docs/transformers/pr_15682/en/model_doc/perceiver#transformers.PerceiverForImageClassificationFourier"),c(tN,"href","/docs/transformers/pr_15682/en/model_doc/perceiver#transformers.PerceiverForImageClassificationConvProcessing"),c(aN,"href","/docs/transformers/pr_15682/en/model_doc/poolformer#transformers.PoolFormerForImageClassification"),c(nN,"href","/docs/transformers/pr_15682/en/model_doc/segformer#transformers.SegformerForImageClassification"),c(sN,"href","/docs/transformers/pr_15682/en/model_doc/swin#transformers.SwinForImageClassification"),c(lN,"href","/docs/transformers/pr_15682/en/model_doc/vit#transformers.ViTForImageClassification"),c(Ge,"class","docstring"),c(rr,"class","docstring"),c(B5,"id","transformers.AutoModelForVision2Seq"),c(B5,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(B5,"href","#transformers.AutoModelForVision2Seq"),c(Cd,"class","relative group"),c(Jr,"class","docstring"),c(iN,"href","/docs/transformers/pr_15682/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderModel"),c(Oe,"class","docstring"),c(tr,"class","docstring"),c(R5,"id","transformers.AutoModelForAudioClassification"),c(R5,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(R5,"href","#transformers.AutoModelForAudioClassification"),c(yd,"class","relative group"),c(Yr,"class","docstring"),c(dN,"href","/docs/transformers/pr_15682/en/model_doc/hubert#transformers.HubertForSequenceClassification"),c(cN,"href","/docs/transformers/pr_15682/en/model_doc/sew#transformers.SEWForSequenceClassification"),c(fN,"href","/docs/transformers/pr_15682/en/model_doc/sew-d#transformers.SEWDForSequenceClassification"),c(mN,"href","/docs/transformers/pr_15682/en/model_doc/unispeech#transformers.UniSpeechForSequenceClassification"),c(gN,"href","/docs/transformers/pr_15682/en/model_doc/unispeech-sat#transformers.UniSpeechSatForSequenceClassification"),c(hN,"href","/docs/transformers/pr_15682/en/model_doc/wav2vec2#transformers.Wav2Vec2ForSequenceClassification"),c(pN,"href","/docs/transformers/pr_15682/en/model_doc/wavlm#transformers.WavLMForSequenceClassification"),c(Xe,"class","docstring"),c(ar,"class","docstring"),c(G5,"id","transformers.AutoModelForAudioFrameClassification"),c(G5,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(G5,"href","#transformers.AutoModelForAudioFrameClassification"),c(Ld,"class","relative group"),c(Kr,"class","docstring"),c(_N,"href","/docs/transformers/pr_15682/en/model_doc/unispeech-sat#transformers.UniSpeechSatForAudioFrameClassification"),c(uN,"href","/docs/transformers/pr_15682/en/model_doc/wav2vec2#transformers.Wav2Vec2ForAudioFrameClassification"),c(bN,"href","/docs/transformers/pr_15682/en/model_doc/wavlm#transformers.WavLMForAudioFrameClassification"),c(ze,"class","docstring"),c(nr,"class","docstring"),c(W5,"id","transformers.AutoModelForCTC"),c(W5,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(W5,"href","#transformers.AutoModelForCTC"),c(Rd,"class","relative group"),c(Zr,"class","docstring"),c(vN,"href","/docs/transformers/pr_15682/en/model_doc/hubert#transformers.HubertForCTC"),c(TN,"href","/docs/transformers/pr_15682/en/model_doc/sew#transformers.SEWForCTC"),c(FN,"href","/docs/transformers/pr_15682/en/model_doc/sew-d#transformers.SEWDForCTC"),c(CN,"href","/docs/transformers/pr_15682/en/model_doc/unispeech#transformers.UniSpeechForCTC"),c(MN,"href","/docs/transformers/pr_15682/en/model_doc/unispeech-sat#transformers.UniSpeechSatForCTC"),c(EN,"href","/docs/transformers/pr_15682/en/model_doc/wav2vec2#transformers.Wav2Vec2ForCTC"),c(yN,"href","/docs/transformers/pr_15682/en/model_doc/wavlm#transformers.WavLMForCTC"),c(Ve,"class","docstring"),c(sr,"class","docstring"),c(ov,"id","transformers.AutoModelForSpeechSeq2Seq"),c(ov,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(ov,"href","#transformers.AutoModelForSpeechSeq2Seq"),c($d,"class","relative group"),c(et,"class","docstring"),c(wN,"href","/docs/transformers/pr_15682/en/model_doc/speech-encoder-decoder#transformers.SpeechEncoderDecoderModel"),c(AN,"href","/docs/transformers/pr_15682/en/model_doc/speech_to_text#transformers.Speech2TextForConditionalGeneration"),c(We,"class","docstring"),c(lr,"class","docstring"),c(nv,"id","transformers.AutoModelForAudioXVector"),c(nv,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(nv,"href","#transformers.AutoModelForAudioXVector"),c(Nd,"class","relative group"),c(ot,"class","docstring"),c(LN,"href","/docs/transformers/pr_15682/en/model_doc/unispeech-sat#transformers.UniSpeechSatForXVector"),c(BN,"href","/docs/transformers/pr_15682/en/model_doc/wav2vec2#transformers.Wav2Vec2ForXVector"),c(kN,"href","/docs/transformers/pr_15682/en/model_doc/wavlm#transformers.WavLMForXVector"),c(Qe,"class","docstring"),c(ir,"class","docstring"),c(cv,"id","transformers.AutoModelForMaskedImageModeling"),c(cv,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(cv,"href","#transformers.AutoModelForMaskedImageModeling"),c(Od,"class","relative group"),c(rt,"class","docstring"),c(xN,"href","/docs/transformers/pr_15682/en/model_doc/deit#transformers.DeiTForMaskedImageModeling"),c(RN,"href","/docs/transformers/pr_15682/en/model_doc/swin#transformers.SwinForMaskedImageModeling"),c(SN,"href","/docs/transformers/pr_15682/en/model_doc/vit#transformers.ViTForMaskedImageModeling"),c(He,"class","docstring"),c(dr,"class","docstring"),c(pv,"id","transformers.AutoModelForObjectDetection"),c(pv,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(pv,"href","#transformers.AutoModelForObjectDetection"),c(Wd,"class","relative group"),c(tt,"class","docstring"),c(PN,"href","/docs/transformers/pr_15682/en/model_doc/detr#transformers.DetrForObjectDetection"),c(Ue,"class","docstring"),c(cr,"class","docstring"),c(bv,"id","transformers.AutoModelForImageSegmentation"),c(bv,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(bv,"href","#transformers.AutoModelForImageSegmentation"),c(Ud,"class","relative group"),c(at,"class","docstring"),c($N,"href","/docs/transformers/pr_15682/en/model_doc/detr#transformers.DetrForSegmentation"),c(Je,"class","docstring"),c(fr,"class","docstring"),c(Fv,"id","transformers.AutoModelForSemanticSegmentation"),c(Fv,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Fv,"href","#transformers.AutoModelForSemanticSegmentation"),c(Kd,"class","relative group"),c(nt,"class","docstring"),c(IN,"href","/docs/transformers/pr_15682/en/model_doc/beit#transformers.BeitForSemanticSegmentation"),c(jN,"href","/docs/transformers/pr_15682/en/model_doc/segformer#transformers.SegformerForSemanticSegmentation"),c(Ye,"class","docstring"),c(mr,"class","docstring"),c(yv,"id","transformers.TFAutoModel"),c(yv,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(yv,"href","#transformers.TFAutoModel"),c(oc,"class","relative group"),c(st,"class","docstring"),c(NN,"href","/docs/transformers/pr_15682/en/model_doc/albert#transformers.TFAlbertModel"),c(DN,"href","/docs/transformers/pr_15682/en/model_doc/bart#transformers.TFBartModel"),c(qN,"href","/docs/transformers/pr_15682/en/model_doc/bert#transformers.TFBertModel"),c(GN,"href","/docs/transformers/pr_15682/en/model_doc/blenderbot#transformers.TFBlenderbotModel"),c(ON,"href","/docs/transformers/pr_15682/en/model_doc/blenderbot-small#transformers.TFBlenderbotSmallModel"),c(XN,"href","/docs/transformers/pr_15682/en/model_doc/camembert#transformers.TFCamembertModel"),c(zN,"href","/docs/transformers/pr_15682/en/model_doc/clip#transformers.TFCLIPModel"),c(VN,"href","/docs/transformers/pr_15682/en/model_doc/convbert#transformers.TFConvBertModel"),c(WN,"href","/docs/transformers/pr_15682/en/model_doc/convnext#transformers.TFConvNextModel"),c(QN,"href","/docs/transformers/pr_15682/en/model_doc/ctrl#transformers.TFCTRLModel"),c(HN,"href","/docs/transformers/pr_15682/en/model_doc/deberta#transformers.TFDebertaModel"),c(UN,"href","/docs/transformers/pr_15682/en/model_doc/deberta-v2#transformers.TFDebertaV2Model"),c(JN,"href","/docs/transformers/pr_15682/en/model_doc/distilbert#transformers.TFDistilBertModel"),c(YN,"href","/docs/transformers/pr_15682/en/model_doc/dpr#transformers.TFDPRQuestionEncoder"),c(KN,"href","/docs/transformers/pr_15682/en/model_doc/electra#transformers.TFElectraModel"),c(ZN,"href","/docs/transformers/pr_15682/en/model_doc/flaubert#transformers.TFFlaubertModel"),c(eD,"href","/docs/transformers/pr_15682/en/model_doc/funnel#transformers.TFFunnelModel"),c(oD,"href","/docs/transformers/pr_15682/en/model_doc/funnel#transformers.TFFunnelBaseModel"),c(rD,"href","/docs/transformers/pr_15682/en/model_doc/gpt2#transformers.TFGPT2Model"),c(tD,"href","/docs/transformers/pr_15682/en/model_doc/hubert#transformers.TFHubertModel"),c(aD,"href","/docs/transformers/pr_15682/en/model_doc/layoutlm#transformers.TFLayoutLMModel"),c(nD,"href","/docs/transformers/pr_15682/en/model_doc/led#transformers.TFLEDModel"),c(sD,"href","/docs/transformers/pr_15682/en/model_doc/longformer#transformers.TFLongformerModel"),c(lD,"href","/docs/transformers/pr_15682/en/model_doc/lxmert#transformers.TFLxmertModel"),c(iD,"href","/docs/transformers/pr_15682/en/model_doc/marian#transformers.TFMarianModel"),c(dD,"href","/docs/transformers/pr_15682/en/model_doc/mbart#transformers.TFMBartModel"),c(cD,"href","/docs/transformers/pr_15682/en/model_doc/mobilebert#transformers.TFMobileBertModel"),c(fD,"href","/docs/transformers/pr_15682/en/model_doc/mpnet#transformers.TFMPNetModel"),c(mD,"href","/docs/transformers/pr_15682/en/model_doc/mt5#transformers.TFMT5Model"),c(gD,"href","/docs/transformers/pr_15682/en/model_doc/openai-gpt#transformers.TFOpenAIGPTModel"),c(hD,"href","/docs/transformers/pr_15682/en/model_doc/pegasus#transformers.TFPegasusModel"),c(pD,"href","/docs/transformers/pr_15682/en/model_doc/rembert#transformers.TFRemBertModel"),c(_D,"href","/docs/transformers/pr_15682/en/model_doc/roberta#transformers.TFRobertaModel"),c(uD,"href","/docs/transformers/pr_15682/en/model_doc/roformer#transformers.TFRoFormerModel"),c(bD,"href","/docs/transformers/pr_15682/en/model_doc/speech_to_text#transformers.TFSpeech2TextModel"),c(vD,"href","/docs/transformers/pr_15682/en/model_doc/t5#transformers.TFT5Model"),c(TD,"href","/docs/transformers/pr_15682/en/model_doc/tapas#transformers.TFTapasModel"),c(FD,"href","/docs/transformers/pr_15682/en/model_doc/transfo-xl#transformers.TFTransfoXLModel"),c(CD,"href","/docs/transformers/pr_15682/en/model_doc/vit#transformers.TFViTModel"),c(MD,"href","/docs/transformers/pr_15682/en/model_doc/wav2vec2#transformers.TFWav2Vec2Model"),c(ED,"href","/docs/transformers/pr_15682/en/model_doc/xlm#transformers.TFXLMModel"),c(yD,"href","/docs/transformers/pr_15682/en/model_doc/xlm-roberta#transformers.TFXLMRobertaModel"),c(wD,"href","/docs/transformers/pr_15682/en/model_doc/xlnet#transformers.TFXLNetModel"),c(go,"class","docstring"),c(gr,"class","docstring"),c(g6,"id","transformers.TFAutoModelForPreTraining"),c(g6,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(g6,"href","#transformers.TFAutoModelForPreTraining"),c(ac,"class","relative group"),c(lt,"class","docstring"),c(AD,"href","/docs/transformers/pr_15682/en/model_doc/albert#transformers.TFAlbertForPreTraining"),c(LD,"href","/docs/transformers/pr_15682/en/model_doc/bart#transformers.TFBartForConditionalGeneration"),c(BD,"href","/docs/transformers/pr_15682/en/model_doc/bert#transformers.TFBertForPreTraining"),c(kD,"href","/docs/transformers/pr_15682/en/model_doc/camembert#transformers.TFCamembertForMaskedLM"),c(xD,"href","/docs/transformers/pr_15682/en/model_doc/ctrl#transformers.TFCTRLLMHeadModel"),c(RD,"href","/docs/transformers/pr_15682/en/model_doc/distilbert#transformers.TFDistilBertForMaskedLM"),c(SD,"href","/docs/transformers/pr_15682/en/model_doc/electra#transformers.TFElectraForPreTraining"),c(PD,"href","/docs/transformers/pr_15682/en/model_doc/flaubert#transformers.TFFlaubertWithLMHeadModel"),c($D,"href","/docs/transformers/pr_15682/en/model_doc/funnel#transformers.TFFunnelForPreTraining"),c(ID,"href","/docs/transformers/pr_15682/en/model_doc/gpt2#transformers.TFGPT2LMHeadModel"),c(jD,"href","/docs/transformers/pr_15682/en/model_doc/layoutlm#transformers.TFLayoutLMForMaskedLM"),c(ND,"href","/docs/transformers/pr_15682/en/model_doc/lxmert#transformers.TFLxmertForPreTraining"),c(DD,"href","/docs/transformers/pr_15682/en/model_doc/mobilebert#transformers.TFMobileBertForPreTraining"),c(qD,"href","/docs/transformers/pr_15682/en/model_doc/mpnet#transformers.TFMPNetForMaskedLM"),c(GD,"href","/docs/transformers/pr_15682/en/model_doc/openai-gpt#transformers.TFOpenAIGPTLMHeadModel"),c(OD,"href","/docs/transformers/pr_15682/en/model_doc/roberta#transformers.TFRobertaForMaskedLM"),c(XD,"href","/docs/transformers/pr_15682/en/model_doc/t5#transformers.TFT5ForConditionalGeneration"),c(zD,"href","/docs/transformers/pr_15682/en/model_doc/tapas#transformers.TFTapasForMaskedLM"),c(VD,"href","/docs/transformers/pr_15682/en/model_doc/transfo-xl#transformers.TFTransfoXLLMHeadModel"),c(WD,"href","/docs/transformers/pr_15682/en/model_doc/xlm#transformers.TFXLMWithLMHeadModel"),c(QD,"href","/docs/transformers/pr_15682/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForMaskedLM"),c(HD,"href","/docs/transformers/pr_15682/en/model_doc/xlnet#transformers.TFXLNetLMHeadModel"),c(ho,"class","docstring"),c(hr,"class","docstring"),c(I6,"id","transformers.TFAutoModelForCausalLM"),c(I6,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(I6,"href","#transformers.TFAutoModelForCausalLM"),c(lc,"class","relative group"),c(it,"class","docstring"),c(UD,"href","/docs/transformers/pr_15682/en/model_doc/bert#transformers.TFBertLMHeadModel"),c(JD,"href","/docs/transformers/pr_15682/en/model_doc/ctrl#transformers.TFCTRLLMHeadModel"),c(YD,"href","/docs/transformers/pr_15682/en/model_doc/gpt2#transformers.TFGPT2LMHeadModel"),c(KD,"href","/docs/transformers/pr_15682/en/model_doc/openai-gpt#transformers.TFOpenAIGPTLMHeadModel"),c(ZD,"href","/docs/transformers/pr_15682/en/model_doc/rembert#transformers.TFRemBertForCausalLM"),c(eq,"href","/docs/transformers/pr_15682/en/model_doc/roberta#transformers.TFRobertaForCausalLM"),c(oq,"href","/docs/transformers/pr_15682/en/model_doc/roformer#transformers.TFRoFormerForCausalLM"),c(rq,"href","/docs/transformers/pr_15682/en/model_doc/transfo-xl#transformers.TFTransfoXLLMHeadModel"),c(tq,"href","/docs/transformers/pr_15682/en/model_doc/xlm#transformers.TFXLMWithLMHeadModel"),c(aq,"href","/docs/transformers/pr_15682/en/model_doc/xlnet#transformers.TFXLNetLMHeadModel"),c(po,"class","docstring"),c(pr,"class","docstring"),c(Q6,"id","transformers.TFAutoModelForImageClassification"),c(Q6,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Q6,"href","#transformers.TFAutoModelForImageClassification"),c(cc,"class","relative group"),c(dt,"class","docstring"),c(nq,"href","/docs/transformers/pr_15682/en/model_doc/convnext#transformers.TFConvNextForImageClassification"),c(sq,"href","/docs/transformers/pr_15682/en/model_doc/vit#transformers.TFViTForImageClassification"),c(_o,"class","docstring"),c(_r,"class","docstring"),c(J6,"id","transformers.TFAutoModelForMaskedLM"),c(J6,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(J6,"href","#transformers.TFAutoModelForMaskedLM"),c(gc,"class","relative group"),c(ct,"class","docstring"),c(lq,"href","/docs/transformers/pr_15682/en/model_doc/albert#transformers.TFAlbertForMaskedLM"),c(iq,"href","/docs/transformers/pr_15682/en/model_doc/bert#transformers.TFBertForMaskedLM"),c(dq,"href","/docs/transformers/pr_15682/en/model_doc/camembert#transformers.TFCamembertForMaskedLM"),c(cq,"href","/docs/transformers/pr_15682/en/model_doc/convbert#transformers.TFConvBertForMaskedLM"),c(fq,"href","/docs/transformers/pr_15682/en/model_doc/deberta#transformers.TFDebertaForMaskedLM"),c(mq,"href","/docs/transformers/pr_15682/en/model_doc/deberta-v2#transformers.TFDebertaV2ForMaskedLM"),c(gq,"href","/docs/transformers/pr_15682/en/model_doc/distilbert#transformers.TFDistilBertForMaskedLM"),c(hq,"href","/docs/transformers/pr_15682/en/model_doc/electra#transformers.TFElectraForMaskedLM"),c(pq,"href","/docs/transformers/pr_15682/en/model_doc/flaubert#transformers.TFFlaubertWithLMHeadModel"),c(_q,"href","/docs/transformers/pr_15682/en/model_doc/funnel#transformers.TFFunnelForMaskedLM"),c(uq,"href","/docs/transformers/pr_15682/en/model_doc/layoutlm#transformers.TFLayoutLMForMaskedLM"),c(bq,"href","/docs/transformers/pr_15682/en/model_doc/longformer#transformers.TFLongformerForMaskedLM"),c(vq,"href","/docs/transformers/pr_15682/en/model_doc/mobilebert#transformers.TFMobileBertForMaskedLM"),c(Tq,"href","/docs/transformers/pr_15682/en/model_doc/mpnet#transformers.TFMPNetForMaskedLM"),c(Fq,"href","/docs/transformers/pr_15682/en/model_doc/rembert#transformers.TFRemBertForMaskedLM"),c(Cq,"href","/docs/transformers/pr_15682/en/model_doc/roberta#transformers.TFRobertaForMaskedLM"),c(Mq,"href","/docs/transformers/pr_15682/en/model_doc/roformer#transformers.TFRoFormerForMaskedLM"),c(Eq,"href","/docs/transformers/pr_15682/en/model_doc/tapas#transformers.TFTapasForMaskedLM"),c(yq,"href","/docs/transformers/pr_15682/en/model_doc/xlm#transformers.TFXLMWithLMHeadModel"),c(wq,"href","/docs/transformers/pr_15682/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForMaskedLM"),c(uo,"class","docstring"),c(ur,"class","docstring"),c(uT,"id","transformers.TFAutoModelForSeq2SeqLM"),c(uT,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(uT,"href","#transformers.TFAutoModelForSeq2SeqLM"),c(_c,"class","relative group"),c(ft,"class","docstring"),c(Aq,"href","/docs/transformers/pr_15682/en/model_doc/bart#transformers.TFBartForConditionalGeneration"),c(Lq,"href","/docs/transformers/pr_15682/en/model_doc/blenderbot#transformers.TFBlenderbotForConditionalGeneration"),c(Bq,"href","/docs/transformers/pr_15682/en/model_doc/blenderbot-small#transformers.TFBlenderbotSmallForConditionalGeneration"),c(kq,"href","/docs/transformers/pr_15682/en/model_doc/encoder-decoder#transformers.TFEncoderDecoderModel"),c(xq,"href","/docs/transformers/pr_15682/en/model_doc/led#transformers.TFLEDForConditionalGeneration"),c(Rq,"href","/docs/transformers/pr_15682/en/model_doc/marian#transformers.TFMarianMTModel"),c(Sq,"href","/docs/transformers/pr_15682/en/model_doc/mbart#transformers.TFMBartForConditionalGeneration"),c(Pq,"href","/docs/transformers/pr_15682/en/model_doc/mt5#transformers.TFMT5ForConditionalGeneration"),c($q,"href","/docs/transformers/pr_15682/en/model_doc/pegasus#transformers.TFPegasusForConditionalGeneration"),c(Iq,"href","/docs/transformers/pr_15682/en/model_doc/t5#transformers.TFT5ForConditionalGeneration"),c(bo,"class","docstring"),c(br,"class","docstring"),c(LT,"id","transformers.TFAutoModelForSequenceClassification"),c(LT,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(LT,"href","#transformers.TFAutoModelForSequenceClassification"),c(vc,"class","relative group"),c(mt,"class","docstring"),c(jq,"href","/docs/transformers/pr_15682/en/model_doc/albert#transformers.TFAlbertForSequenceClassification"),c(Nq,"href","/docs/transformers/pr_15682/en/model_doc/bert#transformers.TFBertForSequenceClassification"),c(Dq,"href","/docs/transformers/pr_15682/en/model_doc/camembert#transformers.TFCamembertForSequenceClassification"),c(qq,"href","/docs/transformers/pr_15682/en/model_doc/convbert#transformers.TFConvBertForSequenceClassification"),c(Gq,"href","/docs/transformers/pr_15682/en/model_doc/ctrl#transformers.TFCTRLForSequenceClassification"),c(Oq,"href","/docs/transformers/pr_15682/en/model_doc/deberta#transformers.TFDebertaForSequenceClassification"),c(Xq,"href","/docs/transformers/pr_15682/en/model_doc/deberta-v2#transformers.TFDebertaV2ForSequenceClassification"),c(zq,"href","/docs/transformers/pr_15682/en/model_doc/distilbert#transformers.TFDistilBertForSequenceClassification"),c(Vq,"href","/docs/transformers/pr_15682/en/model_doc/electra#transformers.TFElectraForSequenceClassification"),c(Wq,"href","/docs/transformers/pr_15682/en/model_doc/flaubert#transformers.TFFlaubertForSequenceClassification"),c(Qq,"href","/docs/transformers/pr_15682/en/model_doc/funnel#transformers.TFFunnelForSequenceClassification"),c(Hq,"href","/docs/transformers/pr_15682/en/model_doc/gpt2#transformers.TFGPT2ForSequenceClassification"),c(Uq,"href","/docs/transformers/pr_15682/en/model_doc/layoutlm#transformers.TFLayoutLMForSequenceClassification"),c(Jq,"href","/docs/transformers/pr_15682/en/model_doc/longformer#transformers.TFLongformerForSequenceClassification"),c(Yq,"href","/docs/transformers/pr_15682/en/model_doc/mobilebert#transformers.TFMobileBertForSequenceClassification"),c(Kq,"href","/docs/transformers/pr_15682/en/model_doc/mpnet#transformers.TFMPNetForSequenceClassification"),c(Zq,"href","/docs/transformers/pr_15682/en/model_doc/openai-gpt#transformers.TFOpenAIGPTForSequenceClassification"),c(eG,"href","/docs/transformers/pr_15682/en/model_doc/rembert#transformers.TFRemBertForSequenceClassification"),c(oG,"href","/docs/transformers/pr_15682/en/model_doc/roberta#transformers.TFRobertaForSequenceClassification"),c(rG,"href","/docs/transformers/pr_15682/en/model_doc/roformer#transformers.TFRoFormerForSequenceClassification"),c(tG,"href","/docs/transformers/pr_15682/en/model_doc/tapas#transformers.TFTapasForSequenceClassification"),c(aG,"href","/docs/transformers/pr_15682/en/model_doc/transfo-xl#transformers.TFTransfoXLForSequenceClassification"),c(nG,"href","/docs/transformers/pr_15682/en/model_doc/xlm#transformers.TFXLMForSequenceClassification"),c(sG,"href","/docs/transformers/pr_15682/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForSequenceClassification"),c(lG,"href","/docs/transformers/pr_15682/en/model_doc/xlnet#transformers.TFXLNetForSequenceClassification"),c(vo,"class","docstring"),c(vr,"class","docstring"),c(e8,"id","transformers.TFAutoModelForMultipleChoice"),c(e8,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(e8,"href","#transformers.TFAutoModelForMultipleChoice"),c(Cc,"class","relative group"),c(gt,"class","docstring"),c(iG,"href","/docs/transformers/pr_15682/en/model_doc/albert#transformers.TFAlbertForMultipleChoice"),c(dG,"href","/docs/transformers/pr_15682/en/model_doc/bert#transformers.TFBertForMultipleChoice"),c(cG,"href","/docs/transformers/pr_15682/en/model_doc/camembert#transformers.TFCamembertForMultipleChoice"),c(fG,"href","/docs/transformers/pr_15682/en/model_doc/convbert#transformers.TFConvBertForMultipleChoice"),c(mG,"href","/docs/transformers/pr_15682/en/model_doc/distilbert#transformers.TFDistilBertForMultipleChoice"),c(gG,"href","/docs/transformers/pr_15682/en/model_doc/electra#transformers.TFElectraForMultipleChoice"),c(hG,"href","/docs/transformers/pr_15682/en/model_doc/flaubert#transformers.TFFlaubertForMultipleChoice"),c(pG,"href","/docs/transformers/pr_15682/en/model_doc/funnel#transformers.TFFunnelForMultipleChoice"),c(_G,"href","/docs/transformers/pr_15682/en/model_doc/longformer#transformers.TFLongformerForMultipleChoice"),c(uG,"href","/docs/transformers/pr_15682/en/model_doc/mobilebert#transformers.TFMobileBertForMultipleChoice"),c(bG,"href","/docs/transformers/pr_15682/en/model_doc/mpnet#transformers.TFMPNetForMultipleChoice"),c(vG,"href","/docs/transformers/pr_15682/en/model_doc/rembert#transformers.TFRemBertForMultipleChoice"),c(TG,"href","/docs/transformers/pr_15682/en/model_doc/roberta#transformers.TFRobertaForMultipleChoice"),c(FG,"href","/docs/transformers/pr_15682/en/model_doc/roformer#transformers.TFRoFormerForMultipleChoice"),c(CG,"href","/docs/transformers/pr_15682/en/model_doc/xlm#transformers.TFXLMForMultipleChoice"),c(MG,"href","/docs/transformers/pr_15682/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForMultipleChoice"),c(EG,"href","/docs/transformers/pr_15682/en/model_doc/xlnet#transformers.TFXLNetForMultipleChoice"),c(To,"class","docstring"),c(Tr,"class","docstring"),c(b8,"id","transformers.TFAutoModelForTableQuestionAnswering"),c(b8,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(b8,"href","#transformers.TFAutoModelForTableQuestionAnswering"),c(yc,"class","relative group"),c(ht,"class","docstring"),c(yG,"href","/docs/transformers/pr_15682/en/model_doc/tapas#transformers.TFTapasForQuestionAnswering"),c(Fo,"class","docstring"),c(Fr,"class","docstring"),c(T8,"id","transformers.TFAutoModelForTokenClassification"),c(T8,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(T8,"href","#transformers.TFAutoModelForTokenClassification"),c(Lc,"class","relative group"),c(pt,"class","docstring"),c(wG,"href","/docs/transformers/pr_15682/en/model_doc/albert#transformers.TFAlbertForTokenClassification"),c(AG,"href","/docs/transformers/pr_15682/en/model_doc/bert#transformers.TFBertForTokenClassification"),c(LG,"href","/docs/transformers/pr_15682/en/model_doc/camembert#transformers.TFCamembertForTokenClassification"),c(BG,"href","/docs/transformers/pr_15682/en/model_doc/convbert#transformers.TFConvBertForTokenClassification"),c(kG,"href","/docs/transformers/pr_15682/en/model_doc/deberta#transformers.TFDebertaForTokenClassification"),c(xG,"href","/docs/transformers/pr_15682/en/model_doc/deberta-v2#transformers.TFDebertaV2ForTokenClassification"),c(RG,"href","/docs/transformers/pr_15682/en/model_doc/distilbert#transformers.TFDistilBertForTokenClassification"),c(SG,"href","/docs/transformers/pr_15682/en/model_doc/electra#transformers.TFElectraForTokenClassification"),c(PG,"href","/docs/transformers/pr_15682/en/model_doc/flaubert#transformers.TFFlaubertForTokenClassification"),c($G,"href","/docs/transformers/pr_15682/en/model_doc/funnel#transformers.TFFunnelForTokenClassification"),c(IG,"href","/docs/transformers/pr_15682/en/model_doc/layoutlm#transformers.TFLayoutLMForTokenClassification"),c(jG,"href","/docs/transformers/pr_15682/en/model_doc/longformer#transformers.TFLongformerForTokenClassification"),c(NG,"href","/docs/transformers/pr_15682/en/model_doc/mobilebert#transformers.TFMobileBertForTokenClassification"),c(DG,"href","/docs/transformers/pr_15682/en/model_doc/mpnet#transformers.TFMPNetForTokenClassification"),c(qG,"href","/docs/transformers/pr_15682/en/model_doc/rembert#transformers.TFRemBertForTokenClassification"),c(GG,"href","/docs/transformers/pr_15682/en/model_doc/roberta#transformers.TFRobertaForTokenClassification"),c(OG,"href","/docs/transformers/pr_15682/en/model_doc/roformer#transformers.TFRoFormerForTokenClassification"),c(XG,"href","/docs/transformers/pr_15682/en/model_doc/xlm#transformers.TFXLMForTokenClassification"),c(zG,"href","/docs/transformers/pr_15682/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForTokenClassification"),c(VG,"href","/docs/transformers/pr_15682/en/model_doc/xlnet#transformers.TFXLNetForTokenClassification"),c(Co,"class","docstring"),c(Cr,"class","docstring"),c(G8,"id","transformers.TFAutoModelForQuestionAnswering"),c(G8,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(G8,"href","#transformers.TFAutoModelForQuestionAnswering"),c(xc,"class","relative group"),c(_t,"class","docstring"),c(WG,"href","/docs/transformers/pr_15682/en/model_doc/albert#transformers.TFAlbertForQuestionAnswering"),c(QG,"href","/docs/transformers/pr_15682/en/model_doc/bert#transformers.TFBertForQuestionAnswering"),c(HG,"href","/docs/transformers/pr_15682/en/model_doc/camembert#transformers.TFCamembertForQuestionAnswering"),c(UG,"href","/docs/transformers/pr_15682/en/model_doc/convbert#transformers.TFConvBertForQuestionAnswering"),c(JG,"href","/docs/transformers/pr_15682/en/model_doc/deberta#transformers.TFDebertaForQuestionAnswering"),c(YG,"href","/docs/transformers/pr_15682/en/model_doc/deberta-v2#transformers.TFDebertaV2ForQuestionAnswering"),c(KG,"href","/docs/transformers/pr_15682/en/model_doc/distilbert#transformers.TFDistilBertForQuestionAnswering"),c(ZG,"href","/docs/transformers/pr_15682/en/model_doc/electra#transformers.TFElectraForQuestionAnswering"),c(eO,"href","/docs/transformers/pr_15682/en/model_doc/flaubert#transformers.TFFlaubertForQuestionAnsweringSimple"),c(oO,"href","/docs/transformers/pr_15682/en/model_doc/funnel#transformers.TFFunnelForQuestionAnswering"),c(rO,"href","/docs/transformers/pr_15682/en/model_doc/longformer#transformers.TFLongformerForQuestionAnswering"),c(tO,"href","/docs/transformers/pr_15682/en/model_doc/mobilebert#transformers.TFMobileBertForQuestionAnswering"),c(aO,"href","/docs/transformers/pr_15682/en/model_doc/mpnet#transformers.TFMPNetForQuestionAnswering"),c(nO,"href","/docs/transformers/pr_15682/en/model_doc/rembert#transformers.TFRemBertForQuestionAnswering"),c(sO,"href","/docs/transformers/pr_15682/en/model_doc/roberta#transformers.TFRobertaForQuestionAnswering"),c(lO,"href","/docs/transformers/pr_15682/en/model_doc/roformer#transformers.TFRoFormerForQuestionAnswering"),c(iO,"href","/docs/transformers/pr_15682/en/model_doc/xlm#transformers.TFXLMForQuestionAnsweringSimple"),c(dO,"href","/docs/transformers/pr_15682/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForQuestionAnswering"),c(cO,"href","/docs/transformers/pr_15682/en/model_doc/xlnet#transformers.TFXLNetForQuestionAnsweringSimple"),c(Mo,"class","docstring"),c(Mr,"class","docstring"),c(lF,"id","transformers.TFAutoModelForVision2Seq"),c(lF,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(lF,"href","#transformers.TFAutoModelForVision2Seq"),c(Pc,"class","relative group"),c(ut,"class","docstring"),c(fO,"href","/docs/transformers/pr_15682/en/model_doc/vision-encoder-decoder#transformers.TFVisionEncoderDecoderModel"),c(Eo,"class","docstring"),c(Er,"class","docstring"),c(dF,"id","transformers.TFAutoModelForSpeechSeq2Seq"),c(dF,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(dF,"href","#transformers.TFAutoModelForSpeechSeq2Seq"),c(jc,"class","relative group"),c(bt,"class","docstring"),c(mO,"href","/docs/transformers/pr_15682/en/model_doc/speech_to_text#transformers.TFSpeech2TextForConditionalGeneration"),c(yo,"class","docstring"),c(yr,"class","docstring"),c(fF,"id","transformers.FlaxAutoModel"),c(fF,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(fF,"href","#transformers.FlaxAutoModel"),c(qc,"class","relative group"),c(vt,"class","docstring"),c(gO,"href","/docs/transformers/pr_15682/en/model_doc/albert#transformers.FlaxAlbertModel"),c(hO,"href","/docs/transformers/pr_15682/en/model_doc/bart#transformers.FlaxBartModel"),c(pO,"href","/docs/transformers/pr_15682/en/model_doc/beit#transformers.FlaxBeitModel"),c(_O,"href","/docs/transformers/pr_15682/en/model_doc/bert#transformers.FlaxBertModel"),c(uO,"href","/docs/transformers/pr_15682/en/model_doc/big_bird#transformers.FlaxBigBirdModel"),c(bO,"href","/docs/transformers/pr_15682/en/model_doc/blenderbot#transformers.FlaxBlenderbotModel"),c(vO,"href","/docs/transformers/pr_15682/en/model_doc/blenderbot-small#transformers.FlaxBlenderbotSmallModel"),c(TO,"href","/docs/transformers/pr_15682/en/model_doc/clip#transformers.FlaxCLIPModel"),c(FO,"href","/docs/transformers/pr_15682/en/model_doc/distilbert#transformers.FlaxDistilBertModel"),c(CO,"href","/docs/transformers/pr_15682/en/model_doc/electra#transformers.FlaxElectraModel"),c(MO,"href","/docs/transformers/pr_15682/en/model_doc/gpt2#transformers.FlaxGPT2Model"),c(EO,"href","/docs/transformers/pr_15682/en/model_doc/gpt_neo#transformers.FlaxGPTNeoModel"),c(yO,"href","/docs/transformers/pr_15682/en/model_doc/gptj#transformers.FlaxGPTJModel"),c(wO,"href","/docs/transformers/pr_15682/en/model_doc/marian#transformers.FlaxMarianModel"),c(AO,"href","/docs/transformers/pr_15682/en/model_doc/mbart#transformers.FlaxMBartModel"),c(LO,"href","/docs/transformers/pr_15682/en/model_doc/mt5#transformers.FlaxMT5Model"),c(BO,"href","/docs/transformers/pr_15682/en/model_doc/pegasus#transformers.FlaxPegasusModel"),c(kO,"href","/docs/transformers/pr_15682/en/model_doc/roberta#transformers.FlaxRobertaModel"),c(xO,"href","/docs/transformers/pr_15682/en/model_doc/roformer#transformers.FlaxRoFormerModel"),c(RO,"href","/docs/transformers/pr_15682/en/model_doc/t5#transformers.FlaxT5Model"),c(SO,"href","/docs/transformers/pr_15682/en/model_doc/vision-text-dual-encoder#transformers.FlaxVisionTextDualEncoderModel"),c(PO,"href","/docs/transformers/pr_15682/en/model_doc/vit#transformers.FlaxViTModel"),c($O,"href","/docs/transformers/pr_15682/en/model_doc/wav2vec2#transformers.FlaxWav2Vec2Model"),c(IO,"href","/docs/transformers/pr_15682/en/model_doc/xglm#transformers.FlaxXGLMModel"),c(wo,"class","docstring"),c(wr,"class","docstring"),c(IF,"id","transformers.FlaxAutoModelForCausalLM"),c(IF,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(IF,"href","#transformers.FlaxAutoModelForCausalLM"),c(Xc,"class","relative group"),c(Tt,"class","docstring"),c(jO,"href","/docs/transformers/pr_15682/en/model_doc/gpt2#transformers.FlaxGPT2LMHeadModel"),c(NO,"href","/docs/transformers/pr_15682/en/model_doc/gpt_neo#transformers.FlaxGPTNeoForCausalLM"),c(DO,"href","/docs/transformers/pr_15682/en/model_doc/gptj#transformers.FlaxGPTJForCausalLM"),c(qO,"href","/docs/transformers/pr_15682/en/model_doc/xglm#transformers.FlaxXGLMForCausalLM"),c(Ao,"class","docstring"),c(Ar,"class","docstring"),c(GF,"id","transformers.FlaxAutoModelForPreTraining"),c(GF,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(GF,"href","#transformers.FlaxAutoModelForPreTraining"),c(Wc,"class","relative group"),c(Ft,"class","docstring"),c(GO,"href","/docs/transformers/pr_15682/en/model_doc/albert#transformers.FlaxAlbertForPreTraining"),c(OO,"href","/docs/transformers/pr_15682/en/model_doc/bart#transformers.FlaxBartForConditionalGeneration"),c(XO,"href","/docs/transformers/pr_15682/en/model_doc/bert#transformers.FlaxBertForPreTraining"),c(zO,"href","/docs/transformers/pr_15682/en/model_doc/big_bird#transformers.FlaxBigBirdForPreTraining"),c(VO,"href","/docs/transformers/pr_15682/en/model_doc/electra#transformers.FlaxElectraForPreTraining"),c(WO,"href","/docs/transformers/pr_15682/en/model_doc/mbart#transformers.FlaxMBartForConditionalGeneration"),c(QO,"href","/docs/transformers/pr_15682/en/model_doc/mt5#transformers.FlaxMT5ForConditionalGeneration"),c(HO,"href","/docs/transformers/pr_15682/en/model_doc/roberta#transformers.FlaxRobertaForMaskedLM"),c(UO,"href","/docs/transformers/pr_15682/en/model_doc/roformer#transformers.FlaxRoFormerForMaskedLM"),c(JO,"href","/docs/transformers/pr_15682/en/model_doc/t5#transformers.FlaxT5ForConditionalGeneration"),c(YO,"href","/docs/transformers/pr_15682/en/model_doc/wav2vec2#transformers.FlaxWav2Vec2ForPreTraining"),c(Lo,"class","docstring"),c(Lr,"class","docstring"),c(ZF,"id","transformers.FlaxAutoModelForMaskedLM"),c(ZF,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(ZF,"href","#transformers.FlaxAutoModelForMaskedLM"),c(Uc,"class","relative group"),c(Ct,"class","docstring"),c(KO,"href","/docs/transformers/pr_15682/en/model_doc/albert#transformers.FlaxAlbertForMaskedLM"),c(ZO,"href","/docs/transformers/pr_15682/en/model_doc/bart#transformers.FlaxBartForConditionalGeneration"),c(eX,"href","/docs/transformers/pr_15682/en/model_doc/bert#transformers.FlaxBertForMaskedLM"),c(oX,"href","/docs/transformers/pr_15682/en/model_doc/big_bird#transformers.FlaxBigBirdForMaskedLM"),c(rX,"href","/docs/transformers/pr_15682/en/model_doc/distilbert#transformers.FlaxDistilBertForMaskedLM"),c(tX,"href","/docs/transformers/pr_15682/en/model_doc/electra#transformers.FlaxElectraForMaskedLM"),c(aX,"href","/docs/transformers/pr_15682/en/model_doc/mbart#transformers.FlaxMBartForConditionalGeneration"),c(nX,"href","/docs/transformers/pr_15682/en/model_doc/roberta#transformers.FlaxRobertaForMaskedLM"),c(sX,"href","/docs/transformers/pr_15682/en/model_doc/roformer#transformers.FlaxRoFormerForMaskedLM"),c(Bo,"class","docstring"),c(Br,"class","docstring"),c(dC,"id","transformers.FlaxAutoModelForSeq2SeqLM"),c(dC,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(dC,"href","#transformers.FlaxAutoModelForSeq2SeqLM"),c(Kc,"class","relative group"),c(Mt,"class","docstring"),c(lX,"href","/docs/transformers/pr_15682/en/model_doc/bart#transformers.FlaxBartForConditionalGeneration"),c(iX,"href","/docs/transformers/pr_15682/en/model_doc/blenderbot#transformers.FlaxBlenderbotForConditionalGeneration"),c(dX,"href","/docs/transformers/pr_15682/en/model_doc/blenderbot-small#transformers.FlaxBlenderbotSmallForConditionalGeneration"),c(cX,"href","/docs/transformers/pr_15682/en/model_doc/encoder-decoder#transformers.FlaxEncoderDecoderModel"),c(fX,"href","/docs/transformers/pr_15682/en/model_doc/marian#transformers.FlaxMarianMTModel"),c(mX,"href","/docs/transformers/pr_15682/en/model_doc/mbart#transformers.FlaxMBartForConditionalGeneration"),c(gX,"href","/docs/transformers/pr_15682/en/model_doc/mt5#transformers.FlaxMT5ForConditionalGeneration"),c(hX,"href","/docs/transformers/pr_15682/en/model_doc/pegasus#transformers.FlaxPegasusForConditionalGeneration"),c(pX,"href","/docs/transformers/pr_15682/en/model_doc/t5#transformers.FlaxT5ForConditionalGeneration"),c(ko,"class","docstring"),c(kr,"class","docstring"),c(vC,"id","transformers.FlaxAutoModelForSequenceClassification"),c(vC,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(vC,"href","#transformers.FlaxAutoModelForSequenceClassification"),c(of,"class","relative group"),c(Et,"class","docstring"),c(_X,"href","/docs/transformers/pr_15682/en/model_doc/albert#transformers.FlaxAlbertForSequenceClassification"),c(uX,"href","/docs/transformers/pr_15682/en/model_doc/bart#transformers.FlaxBartForSequenceClassification"),c(bX,"href","/docs/transformers/pr_15682/en/model_doc/bert#transformers.FlaxBertForSequenceClassification"),c(vX,"href","/docs/transformers/pr_15682/en/model_doc/big_bird#transformers.FlaxBigBirdForSequenceClassification"),c(TX,"href","/docs/transformers/pr_15682/en/model_doc/distilbert#transformers.FlaxDistilBertForSequenceClassification"),c(FX,"href","/docs/transformers/pr_15682/en/model_doc/electra#transformers.FlaxElectraForSequenceClassification"),c(CX,"href","/docs/transformers/pr_15682/en/model_doc/mbart#transformers.FlaxMBartForSequenceClassification"),c(MX,"href","/docs/transformers/pr_15682/en/model_doc/roberta#transformers.FlaxRobertaForSequenceClassification"),c(EX,"href","/docs/transformers/pr_15682/en/model_doc/roformer#transformers.FlaxRoFormerForSequenceClassification"),c(xo,"class","docstring"),c(xr,"class","docstring"),c(BC,"id","transformers.FlaxAutoModelForQuestionAnswering"),c(BC,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(BC,"href","#transformers.FlaxAutoModelForQuestionAnswering"),c(af,"class","relative group"),c(yt,"class","docstring"),c(yX,"href","/docs/transformers/pr_15682/en/model_doc/albert#transformers.FlaxAlbertForQuestionAnswering"),c(wX,"href","/docs/transformers/pr_15682/en/model_doc/bart#transformers.FlaxBartForQuestionAnswering"),c(AX,"href","/docs/transformers/pr_15682/en/model_doc/bert#transformers.FlaxBertForQuestionAnswering"),c(LX,"href","/docs/transformers/pr_15682/en/model_doc/big_bird#transformers.FlaxBigBirdForQuestionAnswering"),c(BX,"href","/docs/transformers/pr_15682/en/model_doc/distilbert#transformers.FlaxDistilBertForQuestionAnswering"),c(kX,"href","/docs/transformers/pr_15682/en/model_doc/electra#transformers.FlaxElectraForQuestionAnswering"),c(xX,"href","/docs/transformers/pr_15682/en/model_doc/mbart#transformers.FlaxMBartForQuestionAnswering"),c(RX,"href","/docs/transformers/pr_15682/en/model_doc/roberta#transformers.FlaxRobertaForQuestionAnswering"),c(SX,"href","/docs/transformers/pr_15682/en/model_doc/roformer#transformers.FlaxRoFormerForQuestionAnswering"),c(Ro,"class","docstring"),c(Rr,"class","docstring"),c(DC,"id","transformers.FlaxAutoModelForTokenClassification"),c(DC,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(DC,"href","#transformers.FlaxAutoModelForTokenClassification"),c(lf,"class","relative group"),c(wt,"class","docstring"),c(PX,"href","/docs/transformers/pr_15682/en/model_doc/albert#transformers.FlaxAlbertForTokenClassification"),c($X,"href","/docs/transformers/pr_15682/en/model_doc/bert#transformers.FlaxBertForTokenClassification"),c(IX,"href","/docs/transformers/pr_15682/en/model_doc/big_bird#transformers.FlaxBigBirdForTokenClassification"),c(jX,"href","/docs/transformers/pr_15682/en/model_doc/distilbert#transformers.FlaxDistilBertForTokenClassification"),c(NX,"href","/docs/transformers/pr_15682/en/model_doc/electra#transformers.FlaxElectraForTokenClassification"),c(DX,"href","/docs/transformers/pr_15682/en/model_doc/roberta#transformers.FlaxRobertaForTokenClassification"),c(qX,"href","/docs/transformers/pr_15682/en/model_doc/roformer#transformers.FlaxRoFormerForTokenClassification"),c(So,"class","docstring"),c(Sr,"class","docstring"),c(QC,"id","transformers.FlaxAutoModelForMultipleChoice"),c(QC,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(QC,"href","#transformers.FlaxAutoModelForMultipleChoice"),c(ff,"class","relative group"),c(At,"class","docstring"),c(GX,"href","/docs/transformers/pr_15682/en/model_doc/albert#transformers.FlaxAlbertForMultipleChoice"),c(OX,"href","/docs/transformers/pr_15682/en/model_doc/bert#transformers.FlaxBertForMultipleChoice"),c(XX,"href","/docs/transformers/pr_15682/en/model_doc/big_bird#transformers.FlaxBigBirdForMultipleChoice"),c(zX,"href","/docs/transformers/pr_15682/en/model_doc/distilbert#transformers.FlaxDistilBertForMultipleChoice"),c(VX,"href","/docs/transformers/pr_15682/en/model_doc/electra#transformers.FlaxElectraForMultipleChoice"),c(WX,"href","/docs/transformers/pr_15682/en/model_doc/roberta#transformers.FlaxRobertaForMultipleChoice"),c(QX,"href","/docs/transformers/pr_15682/en/model_doc/roformer#transformers.FlaxRoFormerForMultipleChoice"),c(Po,"class","docstring"),c(Pr,"class","docstring"),c(o4,"id","transformers.FlaxAutoModelForNextSentencePrediction"),c(o4,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(o4,"href","#transformers.FlaxAutoModelForNextSentencePrediction"),c(hf,"class","relative group"),c(Lt,"class","docstring"),c(HX,"href","/docs/transformers/pr_15682/en/model_doc/bert#transformers.FlaxBertForNextSentencePrediction"),c($o,"class","docstring"),c($r,"class","docstring"),c(t4,"id","transformers.FlaxAutoModelForImageClassification"),c(t4,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(t4,"href","#transformers.FlaxAutoModelForImageClassification"),c(uf,"class","relative group"),c(Bt,"class","docstring"),c(UX,"href","/docs/transformers/pr_15682/en/model_doc/beit#transformers.FlaxBeitForImageClassification"),c(JX,"href","/docs/transformers/pr_15682/en/model_doc/vit#transformers.FlaxViTForImageClassification"),c(Io,"class","docstring"),c(Ir,"class","docstring"),c(s4,"id","transformers.FlaxAutoModelForVision2Seq"),c(s4,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(s4,"href","#transformers.FlaxAutoModelForVision2Seq"),c(Tf,"class","relative group"),c(kt,"class","docstring"),c(YX,"href","/docs/transformers/pr_15682/en/model_doc/vision-encoder-decoder#transformers.FlaxVisionEncoderDecoderModel"),c(jo,"class","docstring"),c(jr,"class","docstring")},m(d,u){e(document.head,J),b(d,Ae,u),b(d,ie,u),e(ie,me),e(me,to),g(ce,to,null),e(ie,ue),e(ie,Do),e(Do,wi),b(d,Ef,u),b(d,sa,u),e(sa,Ai),e(sa,Li),e(Li,nM),e(sa,yf),b(d,ye,u),b(d,io,u),e(io,Bi),e(io,Pn),e(Pn,sM),e(io,$n),e(io,In),e(In,lM),e(io,ki),e(io,jn),e(jn,iM),e(io,xi),b(d,wf,u),g($a,d,u),b(d,co,u),b(d,ge,u),e(ge,zL),e(ge,Ri),e(Ri,VL),e(ge,WL),b(d,qo,u),b(d,Ia,u),e(Ia,QL),e(Ia,Af),e(Af,HL),e(Ia,Bxe),b(d,v7e,u),b(d,Si,u),e(Si,Lf),e(Lf,zV),g(dM,zV,null),e(Si,kxe),e(Si,VV),e(VV,xxe),b(d,T7e,u),b(d,Nn,u),e(Nn,Rxe),e(Nn,WV),e(WV,Sxe),e(Nn,Pxe),e(Nn,QV),e(QV,$xe),e(Nn,Ixe),b(d,F7e,u),g(cM,d,u),b(d,C7e,u),b(d,UL,u),e(UL,jxe),b(d,M7e,u),g(Bf,d,u),b(d,E7e,u),b(d,Pi,u),e(Pi,kf),e(kf,HV),g(fM,HV,null),e(Pi,Nxe),e(Pi,UV),e(UV,Dxe),b(d,y7e,u),b(d,Go,u),g(mM,Go,null),e(Go,qxe),e(Go,gM),e(gM,Gxe),e(gM,JL),e(JL,Oxe),e(gM,Xxe),e(Go,zxe),e(Go,hM),e(hM,Vxe),e(hM,JV),e(JV,Wxe),e(hM,Qxe),e(Go,Hxe),e(Go,fo),g(pM,fo,null),e(fo,Uxe),e(fo,YV),e(YV,Jxe),e(fo,Yxe),e(fo,$i),e($i,Kxe),e($i,KV),e(KV,Zxe),e($i,eRe),e($i,ZV),e(ZV,oRe),e($i,rRe),e(fo,tRe),e(fo,v),e(v,xf),e(xf,eW),e(eW,aRe),e(xf,nRe),e(xf,YL),e(YL,sRe),e(xf,lRe),e(v,iRe),e(v,Rf),e(Rf,oW),e(oW,dRe),e(Rf,cRe),e(Rf,KL),e(KL,fRe),e(Rf,mRe),e(v,gRe),e(v,Sf),e(Sf,rW),e(rW,hRe),e(Sf,pRe),e(Sf,ZL),e(ZL,_Re),e(Sf,uRe),e(v,bRe),e(v,Pf),e(Pf,tW),e(tW,vRe),e(Pf,TRe),e(Pf,e7),e(e7,FRe),e(Pf,CRe),e(v,MRe),e(v,$f),e($f,aW),e(aW,ERe),e($f,yRe),e($f,o7),e(o7,wRe),e($f,ARe),e(v,LRe),e(v,If),e(If,nW),e(nW,BRe),e(If,kRe),e(If,r7),e(r7,xRe),e(If,RRe),e(v,SRe),e(v,jf),e(jf,sW),e(sW,PRe),e(jf,$Re),e(jf,t7),e(t7,IRe),e(jf,jRe),e(v,NRe),e(v,Nf),e(Nf,lW),e(lW,DRe),e(Nf,qRe),e(Nf,a7),e(a7,GRe),e(Nf,ORe),e(v,XRe),e(v,Df),e(Df,iW),e(iW,zRe),e(Df,VRe),e(Df,n7),e(n7,WRe),e(Df,QRe),e(v,HRe),e(v,qf),e(qf,dW),e(dW,URe),e(qf,JRe),e(qf,s7),e(s7,YRe),e(qf,KRe),e(v,ZRe),e(v,Gf),e(Gf,cW),e(cW,eSe),e(Gf,oSe),e(Gf,l7),e(l7,rSe),e(Gf,tSe),e(v,aSe),e(v,Of),e(Of,fW),e(fW,nSe),e(Of,sSe),e(Of,i7),e(i7,lSe),e(Of,iSe),e(v,dSe),e(v,Xf),e(Xf,mW),e(mW,cSe),e(Xf,fSe),e(Xf,d7),e(d7,mSe),e(Xf,gSe),e(v,hSe),e(v,zf),e(zf,gW),e(gW,pSe),e(zf,_Se),e(zf,c7),e(c7,uSe),e(zf,bSe),e(v,vSe),e(v,Vf),e(Vf,hW),e(hW,TSe),e(Vf,FSe),e(Vf,f7),e(f7,CSe),e(Vf,MSe),e(v,ESe),e(v,Wf),e(Wf,pW),e(pW,ySe),e(Wf,wSe),e(Wf,m7),e(m7,ASe),e(Wf,LSe),e(v,BSe),e(v,Qf),e(Qf,_W),e(_W,kSe),e(Qf,xSe),e(Qf,g7),e(g7,RSe),e(Qf,SSe),e(v,PSe),e(v,Hf),e(Hf,uW),e(uW,$Se),e(Hf,ISe),e(Hf,h7),e(h7,jSe),e(Hf,NSe),e(v,DSe),e(v,Uf),e(Uf,bW),e(bW,qSe),e(Uf,GSe),e(Uf,p7),e(p7,OSe),e(Uf,XSe),e(v,zSe),e(v,Jf),e(Jf,vW),e(vW,VSe),e(Jf,WSe),e(Jf,_7),e(_7,QSe),e(Jf,HSe),e(v,USe),e(v,Yf),e(Yf,TW),e(TW,JSe),e(Yf,YSe),e(Yf,u7),e(u7,KSe),e(Yf,ZSe),e(v,ePe),e(v,Kf),e(Kf,FW),e(FW,oPe),e(Kf,rPe),e(Kf,b7),e(b7,tPe),e(Kf,aPe),e(v,nPe),e(v,Zf),e(Zf,CW),e(CW,sPe),e(Zf,lPe),e(Zf,v7),e(v7,iPe),e(Zf,dPe),e(v,cPe),e(v,em),e(em,MW),e(MW,fPe),e(em,mPe),e(em,T7),e(T7,gPe),e(em,hPe),e(v,pPe),e(v,om),e(om,EW),e(EW,_Pe),e(om,uPe),e(om,F7),e(F7,bPe),e(om,vPe),e(v,TPe),e(v,rm),e(rm,yW),e(yW,FPe),e(rm,CPe),e(rm,C7),e(C7,MPe),e(rm,EPe),e(v,yPe),e(v,tm),e(tm,wW),e(wW,wPe),e(tm,APe),e(tm,M7),e(M7,LPe),e(tm,BPe),e(v,kPe),e(v,am),e(am,AW),e(AW,xPe),e(am,RPe),e(am,E7),e(E7,SPe),e(am,PPe),e(v,$Pe),e(v,nm),e(nm,LW),e(LW,IPe),e(nm,jPe),e(nm,y7),e(y7,NPe),e(nm,DPe),e(v,qPe),e(v,sm),e(sm,BW),e(BW,GPe),e(sm,OPe),e(sm,w7),e(w7,XPe),e(sm,zPe),e(v,VPe),e(v,lm),e(lm,kW),e(kW,WPe),e(lm,QPe),e(lm,A7),e(A7,HPe),e(lm,UPe),e(v,JPe),e(v,im),e(im,xW),e(xW,YPe),e(im,KPe),e(im,L7),e(L7,ZPe),e(im,e$e),e(v,o$e),e(v,dm),e(dm,RW),e(RW,r$e),e(dm,t$e),e(dm,B7),e(B7,a$e),e(dm,n$e),e(v,s$e),e(v,cm),e(cm,SW),e(SW,l$e),e(cm,i$e),e(cm,k7),e(k7,d$e),e(cm,c$e),e(v,f$e),e(v,fm),e(fm,PW),e(PW,m$e),e(fm,g$e),e(fm,x7),e(x7,h$e),e(fm,p$e),e(v,_$e),e(v,mm),e(mm,$W),e($W,u$e),e(mm,b$e),e(mm,R7),e(R7,v$e),e(mm,T$e),e(v,F$e),e(v,gm),e(gm,IW),e(IW,C$e),e(gm,M$e),e(gm,S7),e(S7,E$e),e(gm,y$e),e(v,w$e),e(v,hm),e(hm,jW),e(jW,A$e),e(hm,L$e),e(hm,P7),e(P7,B$e),e(hm,k$e),e(v,x$e),e(v,pm),e(pm,NW),e(NW,R$e),e(pm,S$e),e(pm,$7),e($7,P$e),e(pm,$$e),e(v,I$e),e(v,_m),e(_m,DW),e(DW,j$e),e(_m,N$e),e(_m,I7),e(I7,D$e),e(_m,q$e),e(v,G$e),e(v,um),e(um,qW),e(qW,O$e),e(um,X$e),e(um,j7),e(j7,z$e),e(um,V$e),e(v,W$e),e(v,bm),e(bm,GW),e(GW,Q$e),e(bm,H$e),e(bm,N7),e(N7,U$e),e(bm,J$e),e(v,Y$e),e(v,vm),e(vm,OW),e(OW,K$e),e(vm,Z$e),e(vm,D7),e(D7,eIe),e(vm,oIe),e(v,rIe),e(v,Tm),e(Tm,XW),e(XW,tIe),e(Tm,aIe),e(Tm,q7),e(q7,nIe),e(Tm,sIe),e(v,lIe),e(v,Fm),e(Fm,zW),e(zW,iIe),e(Fm,dIe),e(Fm,G7),e(G7,cIe),e(Fm,fIe),e(v,mIe),e(v,Cm),e(Cm,VW),e(VW,gIe),e(Cm,hIe),e(Cm,O7),e(O7,pIe),e(Cm,_Ie),e(v,uIe),e(v,Mm),e(Mm,WW),e(WW,bIe),e(Mm,vIe),e(Mm,X7),e(X7,TIe),e(Mm,FIe),e(v,CIe),e(v,Em),e(Em,QW),e(QW,MIe),e(Em,EIe),e(Em,z7),e(z7,yIe),e(Em,wIe),e(v,AIe),e(v,ym),e(ym,HW),e(HW,LIe),e(ym,BIe),e(ym,V7),e(V7,kIe),e(ym,xIe),e(v,RIe),e(v,wm),e(wm,UW),e(UW,SIe),e(wm,PIe),e(wm,W7),e(W7,$Ie),e(wm,IIe),e(v,jIe),e(v,Am),e(Am,JW),e(JW,NIe),e(Am,DIe),e(Am,Q7),e(Q7,qIe),e(Am,GIe),e(v,OIe),e(v,Lm),e(Lm,YW),e(YW,XIe),e(Lm,zIe),e(Lm,H7),e(H7,VIe),e(Lm,WIe),e(v,QIe),e(v,Bm),e(Bm,KW),e(KW,HIe),e(Bm,UIe),e(Bm,U7),e(U7,JIe),e(Bm,YIe),e(v,KIe),e(v,km),e(km,ZW),e(ZW,ZIe),e(km,eje),e(km,J7),e(J7,oje),e(km,rje),e(v,tje),e(v,xm),e(xm,eQ),e(eQ,aje),e(xm,nje),e(xm,Y7),e(Y7,sje),e(xm,lje),e(v,ije),e(v,Rm),e(Rm,oQ),e(oQ,dje),e(Rm,cje),e(Rm,K7),e(K7,fje),e(Rm,mje),e(v,gje),e(v,Sm),e(Sm,rQ),e(rQ,hje),e(Sm,pje),e(Sm,Z7),e(Z7,_je),e(Sm,uje),e(v,bje),e(v,Pm),e(Pm,tQ),e(tQ,vje),e(Pm,Tje),e(Pm,e9),e(e9,Fje),e(Pm,Cje),e(v,Mje),e(v,$m),e($m,aQ),e(aQ,Eje),e($m,yje),e($m,o9),e(o9,wje),e($m,Aje),e(v,Lje),e(v,Im),e(Im,nQ),e(nQ,Bje),e(Im,kje),e(Im,r9),e(r9,xje),e(Im,Rje),e(v,Sje),e(v,jm),e(jm,sQ),e(sQ,Pje),e(jm,$je),e(jm,t9),e(t9,Ije),e(jm,jje),e(v,Nje),e(v,Nm),e(Nm,lQ),e(lQ,Dje),e(Nm,qje),e(Nm,a9),e(a9,Gje),e(Nm,Oje),e(v,Xje),e(v,Dm),e(Dm,iQ),e(iQ,zje),e(Dm,Vje),e(Dm,n9),e(n9,Wje),e(Dm,Qje),e(v,Hje),e(v,qm),e(qm,dQ),e(dQ,Uje),e(qm,Jje),e(qm,s9),e(s9,Yje),e(qm,Kje),e(v,Zje),e(v,Gm),e(Gm,cQ),e(cQ,eNe),e(Gm,oNe),e(Gm,l9),e(l9,rNe),e(Gm,tNe),e(v,aNe),e(v,Om),e(Om,fQ),e(fQ,nNe),e(Om,sNe),e(Om,i9),e(i9,lNe),e(Om,iNe),e(v,dNe),e(v,Xm),e(Xm,mQ),e(mQ,cNe),e(Xm,fNe),e(Xm,d9),e(d9,mNe),e(Xm,gNe),e(v,hNe),e(v,zm),e(zm,gQ),e(gQ,pNe),e(zm,_Ne),e(zm,c9),e(c9,uNe),e(zm,bNe),e(v,vNe),e(v,Vm),e(Vm,hQ),e(hQ,TNe),e(Vm,FNe),e(Vm,f9),e(f9,CNe),e(Vm,MNe),e(v,ENe),e(v,Wm),e(Wm,pQ),e(pQ,yNe),e(Wm,wNe),e(Wm,m9),e(m9,ANe),e(Wm,LNe),e(v,BNe),e(v,Qm),e(Qm,_Q),e(_Q,kNe),e(Qm,xNe),e(Qm,g9),e(g9,RNe),e(Qm,SNe),e(v,PNe),e(v,Hm),e(Hm,uQ),e(uQ,$Ne),e(Hm,INe),e(Hm,h9),e(h9,jNe),e(Hm,NNe),e(v,DNe),e(v,Um),e(Um,bQ),e(bQ,qNe),e(Um,GNe),e(Um,p9),e(p9,ONe),e(Um,XNe),e(v,zNe),e(v,Jm),e(Jm,vQ),e(vQ,VNe),e(Jm,WNe),e(Jm,_9),e(_9,QNe),e(Jm,HNe),e(v,UNe),e(v,Ym),e(Ym,TQ),e(TQ,JNe),e(Ym,YNe),e(Ym,u9),e(u9,KNe),e(Ym,ZNe),e(v,eDe),e(v,Km),e(Km,FQ),e(FQ,oDe),e(Km,rDe),e(Km,b9),e(b9,tDe),e(Km,aDe),e(v,nDe),e(v,Zm),e(Zm,CQ),e(CQ,sDe),e(Zm,lDe),e(Zm,v9),e(v9,iDe),e(Zm,dDe),e(v,cDe),e(v,eg),e(eg,MQ),e(MQ,fDe),e(eg,mDe),e(eg,T9),e(T9,gDe),e(eg,hDe),e(v,pDe),e(v,og),e(og,EQ),e(EQ,_De),e(og,uDe),e(og,F9),e(F9,bDe),e(og,vDe),e(v,TDe),e(v,rg),e(rg,yQ),e(yQ,FDe),e(rg,CDe),e(rg,C9),e(C9,MDe),e(rg,EDe),e(v,yDe),e(v,tg),e(tg,wQ),e(wQ,wDe),e(tg,ADe),e(tg,M9),e(M9,LDe),e(tg,BDe),e(v,kDe),e(v,ag),e(ag,AQ),e(AQ,xDe),e(ag,RDe),e(ag,E9),e(E9,SDe),e(ag,PDe),e(v,$De),e(v,ng),e(ng,LQ),e(LQ,IDe),e(ng,jDe),e(ng,y9),e(y9,NDe),e(ng,DDe),e(v,qDe),e(v,sg),e(sg,BQ),e(BQ,GDe),e(sg,ODe),e(sg,w9),e(w9,XDe),e(sg,zDe),e(v,VDe),e(v,lg),e(lg,kQ),e(kQ,WDe),e(lg,QDe),e(lg,A9),e(A9,HDe),e(lg,UDe),e(v,JDe),e(v,ig),e(ig,xQ),e(xQ,YDe),e(ig,KDe),e(ig,L9),e(L9,ZDe),e(ig,eqe),e(v,oqe),e(v,dg),e(dg,RQ),e(RQ,rqe),e(dg,tqe),e(dg,B9),e(B9,aqe),e(dg,nqe),e(v,sqe),e(v,cg),e(cg,SQ),e(SQ,lqe),e(cg,iqe),e(cg,k9),e(k9,dqe),e(cg,cqe),e(v,fqe),e(v,fg),e(fg,PQ),e(PQ,mqe),e(fg,gqe),e(fg,x9),e(x9,hqe),e(fg,pqe),e(v,_qe),e(v,mg),e(mg,$Q),e($Q,uqe),e(mg,bqe),e(mg,R9),e(R9,vqe),e(mg,Tqe),e(v,Fqe),e(v,gg),e(gg,IQ),e(IQ,Cqe),e(gg,Mqe),e(gg,S9),e(S9,Eqe),e(gg,yqe),e(v,wqe),e(v,hg),e(hg,jQ),e(jQ,Aqe),e(hg,Lqe),e(hg,P9),e(P9,Bqe),e(hg,kqe),e(fo,xqe),e(fo,NQ),e(NQ,Rqe),e(fo,Sqe),g(_M,fo,null),e(Go,Pqe),e(Go,pg),g(uM,pg,null),e(pg,$qe),e(pg,DQ),e(DQ,Iqe),b(d,w7e,u),b(d,Ii,u),e(Ii,_g),e(_g,qQ),g(bM,qQ,null),e(Ii,jqe),e(Ii,GQ),e(GQ,Nqe),b(d,A7e,u),b(d,Oo,u),g(vM,Oo,null),e(Oo,Dqe),e(Oo,TM),e(TM,qqe),e(TM,$9),e($9,Gqe),e(TM,Oqe),e(Oo,Xqe),e(Oo,FM),e(FM,zqe),e(FM,OQ),e(OQ,Vqe),e(FM,Wqe),e(Oo,Qqe),e(Oo,mo),g(CM,mo,null),e(mo,Hqe),e(mo,XQ),e(XQ,Uqe),e(mo,Jqe),e(mo,ja),e(ja,Yqe),e(ja,zQ),e(zQ,Kqe),e(ja,Zqe),e(ja,VQ),e(VQ,eGe),e(ja,oGe),e(ja,WQ),e(WQ,rGe),e(ja,tGe),e(mo,aGe),e(mo,M),e(M,Dn),e(Dn,QQ),e(QQ,nGe),e(Dn,sGe),e(Dn,I9),e(I9,lGe),e(Dn,iGe),e(Dn,j9),e(j9,dGe),e(Dn,cGe),e(M,fGe),e(M,qn),e(qn,HQ),e(HQ,mGe),e(qn,gGe),e(qn,N9),e(N9,hGe),e(qn,pGe),e(qn,D9),e(D9,_Ge),e(qn,uGe),e(M,bGe),e(M,Gn),e(Gn,UQ),e(UQ,vGe),e(Gn,TGe),e(Gn,q9),e(q9,FGe),e(Gn,CGe),e(Gn,G9),e(G9,MGe),e(Gn,EGe),e(M,yGe),e(M,ug),e(ug,JQ),e(JQ,wGe),e(ug,AGe),e(ug,O9),e(O9,LGe),e(ug,BGe),e(M,kGe),e(M,On),e(On,YQ),e(YQ,xGe),e(On,RGe),e(On,X9),e(X9,SGe),e(On,PGe),e(On,z9),e(z9,$Ge),e(On,IGe),e(M,jGe),e(M,bg),e(bg,KQ),e(KQ,NGe),e(bg,DGe),e(bg,V9),e(V9,qGe),e(bg,GGe),e(M,OGe),e(M,vg),e(vg,ZQ),e(ZQ,XGe),e(vg,zGe),e(vg,W9),e(W9,VGe),e(vg,WGe),e(M,QGe),e(M,Tg),e(Tg,eH),e(eH,HGe),e(Tg,UGe),e(Tg,Q9),e(Q9,JGe),e(Tg,YGe),e(M,KGe),e(M,Xn),e(Xn,oH),e(oH,ZGe),e(Xn,eOe),e(Xn,H9),e(H9,oOe),e(Xn,rOe),e(Xn,U9),e(U9,tOe),e(Xn,aOe),e(M,nOe),e(M,zn),e(zn,rH),e(rH,sOe),e(zn,lOe),e(zn,J9),e(J9,iOe),e(zn,dOe),e(zn,Y9),e(Y9,cOe),e(zn,fOe),e(M,mOe),e(M,Vn),e(Vn,tH),e(tH,gOe),e(Vn,hOe),e(Vn,K9),e(K9,pOe),e(Vn,_Oe),e(Vn,Z9),e(Z9,uOe),e(Vn,bOe),e(M,vOe),e(M,Fg),e(Fg,aH),e(aH,TOe),e(Fg,FOe),e(Fg,eB),e(eB,COe),e(Fg,MOe),e(M,EOe),e(M,Cg),e(Cg,nH),e(nH,yOe),e(Cg,wOe),e(Cg,oB),e(oB,AOe),e(Cg,LOe),e(M,BOe),e(M,Wn),e(Wn,sH),e(sH,kOe),e(Wn,xOe),e(Wn,rB),e(rB,ROe),e(Wn,SOe),e(Wn,tB),e(tB,POe),e(Wn,$Oe),e(M,IOe),e(M,Mg),e(Mg,lH),e(lH,jOe),e(Mg,NOe),e(Mg,aB),e(aB,DOe),e(Mg,qOe),e(M,GOe),e(M,Qn),e(Qn,iH),e(iH,OOe),e(Qn,XOe),e(Qn,nB),e(nB,zOe),e(Qn,VOe),e(Qn,sB),e(sB,WOe),e(Qn,QOe),e(M,HOe),e(M,Hn),e(Hn,dH),e(dH,UOe),e(Hn,JOe),e(Hn,lB),e(lB,YOe),e(Hn,KOe),e(Hn,iB),e(iB,ZOe),e(Hn,eXe),e(M,oXe),e(M,Un),e(Un,cH),e(cH,rXe),e(Un,tXe),e(Un,dB),e(dB,aXe),e(Un,nXe),e(Un,fH),e(fH,sXe),e(Un,lXe),e(M,iXe),e(M,Eg),e(Eg,mH),e(mH,dXe),e(Eg,cXe),e(Eg,cB),e(cB,fXe),e(Eg,mXe),e(M,gXe),e(M,Jn),e(Jn,gH),e(gH,hXe),e(Jn,pXe),e(Jn,fB),e(fB,_Xe),e(Jn,uXe),e(Jn,mB),e(mB,bXe),e(Jn,vXe),e(M,TXe),e(M,yg),e(yg,hH),e(hH,FXe),e(yg,CXe),e(yg,gB),e(gB,MXe),e(yg,EXe),e(M,yXe),e(M,Yn),e(Yn,pH),e(pH,wXe),e(Yn,AXe),e(Yn,hB),e(hB,LXe),e(Yn,BXe),e(Yn,pB),e(pB,kXe),e(Yn,xXe),e(M,RXe),e(M,Kn),e(Kn,_H),e(_H,SXe),e(Kn,PXe),e(Kn,_B),e(_B,$Xe),e(Kn,IXe),e(Kn,uB),e(uB,jXe),e(Kn,NXe),e(M,DXe),e(M,Zn),e(Zn,uH),e(uH,qXe),e(Zn,GXe),e(Zn,bB),e(bB,OXe),e(Zn,XXe),e(Zn,vB),e(vB,zXe),e(Zn,VXe),e(M,WXe),e(M,wg),e(wg,bH),e(bH,QXe),e(wg,HXe),e(wg,TB),e(TB,UXe),e(wg,JXe),e(M,YXe),e(M,es),e(es,vH),e(vH,KXe),e(es,ZXe),e(es,FB),e(FB,eze),e(es,oze),e(es,CB),e(CB,rze),e(es,tze),e(M,aze),e(M,Ag),e(Ag,TH),e(TH,nze),e(Ag,sze),e(Ag,MB),e(MB,lze),e(Ag,ize),e(M,dze),e(M,os),e(os,FH),e(FH,cze),e(os,fze),e(os,EB),e(EB,mze),e(os,gze),e(os,yB),e(yB,hze),e(os,pze),e(M,_ze),e(M,rs),e(rs,CH),e(CH,uze),e(rs,bze),e(rs,wB),e(wB,vze),e(rs,Tze),e(rs,AB),e(AB,Fze),e(rs,Cze),e(M,Mze),e(M,ts),e(ts,MH),e(MH,Eze),e(ts,yze),e(ts,LB),e(LB,wze),e(ts,Aze),e(ts,BB),e(BB,Lze),e(ts,Bze),e(M,kze),e(M,as),e(as,EH),e(EH,xze),e(as,Rze),e(as,kB),e(kB,Sze),e(as,Pze),e(as,xB),e(xB,$ze),e(as,Ize),e(M,jze),e(M,Lg),e(Lg,yH),e(yH,Nze),e(Lg,Dze),e(Lg,RB),e(RB,qze),e(Lg,Gze),e(M,Oze),e(M,ns),e(ns,wH),e(wH,Xze),e(ns,zze),e(ns,SB),e(SB,Vze),e(ns,Wze),e(ns,PB),e(PB,Qze),e(ns,Hze),e(M,Uze),e(M,ss),e(ss,AH),e(AH,Jze),e(ss,Yze),e(ss,$B),e($B,Kze),e(ss,Zze),e(ss,IB),e(IB,eVe),e(ss,oVe),e(M,rVe),e(M,ls),e(ls,LH),e(LH,tVe),e(ls,aVe),e(ls,jB),e(jB,nVe),e(ls,sVe),e(ls,NB),e(NB,lVe),e(ls,iVe),e(M,dVe),e(M,is),e(is,BH),e(BH,cVe),e(is,fVe),e(is,DB),e(DB,mVe),e(is,gVe),e(is,qB),e(qB,hVe),e(is,pVe),e(M,_Ve),e(M,ds),e(ds,kH),e(kH,uVe),e(ds,bVe),e(ds,GB),e(GB,vVe),e(ds,TVe),e(ds,OB),e(OB,FVe),e(ds,CVe),e(M,MVe),e(M,cs),e(cs,xH),e(xH,EVe),e(cs,yVe),e(cs,XB),e(XB,wVe),e(cs,AVe),e(cs,zB),e(zB,LVe),e(cs,BVe),e(M,kVe),e(M,Bg),e(Bg,RH),e(RH,xVe),e(Bg,RVe),e(Bg,VB),e(VB,SVe),e(Bg,PVe),e(M,$Ve),e(M,fs),e(fs,SH),e(SH,IVe),e(fs,jVe),e(fs,WB),e(WB,NVe),e(fs,DVe),e(fs,QB),e(QB,qVe),e(fs,GVe),e(M,OVe),e(M,kg),e(kg,PH),e(PH,XVe),e(kg,zVe),e(kg,HB),e(HB,VVe),e(kg,WVe),e(M,QVe),e(M,xg),e(xg,$H),e($H,HVe),e(xg,UVe),e(xg,UB),e(UB,JVe),e(xg,YVe),e(M,KVe),e(M,ms),e(ms,IH),e(IH,ZVe),e(ms,eWe),e(ms,JB),e(JB,oWe),e(ms,rWe),e(ms,YB),e(YB,tWe),e(ms,aWe),e(M,nWe),e(M,gs),e(gs,jH),e(jH,sWe),e(gs,lWe),e(gs,KB),e(KB,iWe),e(gs,dWe),e(gs,ZB),e(ZB,cWe),e(gs,fWe),e(M,mWe),e(M,Rg),e(Rg,NH),e(NH,gWe),e(Rg,hWe),e(Rg,ek),e(ek,pWe),e(Rg,_We),e(M,uWe),e(M,hs),e(hs,DH),e(DH,bWe),e(hs,vWe),e(hs,ok),e(ok,TWe),e(hs,FWe),e(hs,rk),e(rk,CWe),e(hs,MWe),e(M,EWe),e(M,ps),e(ps,qH),e(qH,yWe),e(ps,wWe),e(ps,tk),e(tk,AWe),e(ps,LWe),e(ps,ak),e(ak,BWe),e(ps,kWe),e(M,xWe),e(M,_s),e(_s,GH),e(GH,RWe),e(_s,SWe),e(_s,nk),e(nk,PWe),e(_s,$We),e(_s,sk),e(sk,IWe),e(_s,jWe),e(M,NWe),e(M,us),e(us,OH),e(OH,DWe),e(us,qWe),e(us,lk),e(lk,GWe),e(us,OWe),e(us,ik),e(ik,XWe),e(us,zWe),e(M,VWe),e(M,bs),e(bs,XH),e(XH,WWe),e(bs,QWe),e(bs,dk),e(dk,HWe),e(bs,UWe),e(bs,ck),e(ck,JWe),e(bs,YWe),e(M,KWe),e(M,Sg),e(Sg,zH),e(zH,ZWe),e(Sg,eQe),e(Sg,fk),e(fk,oQe),e(Sg,rQe),e(M,tQe),e(M,Pg),e(Pg,VH),e(VH,aQe),e(Pg,nQe),e(Pg,mk),e(mk,sQe),e(Pg,lQe),e(M,iQe),e(M,$g),e($g,WH),e(WH,dQe),e($g,cQe),e($g,gk),e(gk,fQe),e($g,mQe),e(M,gQe),e(M,Ig),e(Ig,QH),e(QH,hQe),e(Ig,pQe),e(Ig,hk),e(hk,_Qe),e(Ig,uQe),e(M,bQe),e(M,vs),e(vs,HH),e(HH,vQe),e(vs,TQe),e(vs,pk),e(pk,FQe),e(vs,CQe),e(vs,_k),e(_k,MQe),e(vs,EQe),e(M,yQe),e(M,jg),e(jg,UH),e(UH,wQe),e(jg,AQe),e(jg,uk),e(uk,LQe),e(jg,BQe),e(M,kQe),e(M,Ts),e(Ts,JH),e(JH,xQe),e(Ts,RQe),e(Ts,bk),e(bk,SQe),e(Ts,PQe),e(Ts,vk),e(vk,$Qe),e(Ts,IQe),e(M,jQe),e(M,Fs),e(Fs,YH),e(YH,NQe),e(Fs,DQe),e(Fs,Tk),e(Tk,qQe),e(Fs,GQe),e(Fs,Fk),e(Fk,OQe),e(Fs,XQe),e(M,zQe),e(M,Cs),e(Cs,KH),e(KH,VQe),e(Cs,WQe),e(Cs,Ck),e(Ck,QQe),e(Cs,HQe),e(Cs,Mk),e(Mk,UQe),e(Cs,JQe),e(M,YQe),e(M,Ms),e(Ms,ZH),e(ZH,KQe),e(Ms,ZQe),e(Ms,Ek),e(Ek,eHe),e(Ms,oHe),e(Ms,yk),e(yk,rHe),e(Ms,tHe),e(M,aHe),e(M,Es),e(Es,eU),e(eU,nHe),e(Es,sHe),e(Es,wk),e(wk,lHe),e(Es,iHe),e(Es,Ak),e(Ak,dHe),e(Es,cHe),e(M,fHe),e(M,Ng),e(Ng,oU),e(oU,mHe),e(Ng,gHe),e(Ng,Lk),e(Lk,hHe),e(Ng,pHe),e(M,_He),e(M,Dg),e(Dg,rU),e(rU,uHe),e(Dg,bHe),e(Dg,Bk),e(Bk,vHe),e(Dg,THe),e(M,FHe),e(M,ys),e(ys,tU),e(tU,CHe),e(ys,MHe),e(ys,kk),e(kk,EHe),e(ys,yHe),e(ys,xk),e(xk,wHe),e(ys,AHe),e(M,LHe),e(M,ws),e(ws,aU),e(aU,BHe),e(ws,kHe),e(ws,Rk),e(Rk,xHe),e(ws,RHe),e(ws,Sk),e(Sk,SHe),e(ws,PHe),e(M,$He),e(M,As),e(As,nU),e(nU,IHe),e(As,jHe),e(As,Pk),e(Pk,NHe),e(As,DHe),e(As,$k),e($k,qHe),e(As,GHe),e(M,OHe),e(M,qg),e(qg,sU),e(sU,XHe),e(qg,zHe),e(qg,Ik),e(Ik,VHe),e(qg,WHe),e(M,QHe),e(M,Gg),e(Gg,lU),e(lU,HHe),e(Gg,UHe),e(Gg,jk),e(jk,JHe),e(Gg,YHe),e(M,KHe),e(M,Og),e(Og,iU),e(iU,ZHe),e(Og,eUe),e(Og,Nk),e(Nk,oUe),e(Og,rUe),e(M,tUe),e(M,Xg),e(Xg,dU),e(dU,aUe),e(Xg,nUe),e(Xg,Dk),e(Dk,sUe),e(Xg,lUe),e(M,iUe),e(M,Ls),e(Ls,cU),e(cU,dUe),e(Ls,cUe),e(Ls,qk),e(qk,fUe),e(Ls,mUe),e(Ls,Gk),e(Gk,gUe),e(Ls,hUe),e(M,pUe),e(M,zg),e(zg,fU),e(fU,_Ue),e(zg,uUe),e(zg,Ok),e(Ok,bUe),e(zg,vUe),e(M,TUe),e(M,Vg),e(Vg,mU),e(mU,FUe),e(Vg,CUe),e(Vg,Xk),e(Xk,MUe),e(Vg,EUe),e(M,yUe),e(M,Bs),e(Bs,gU),e(gU,wUe),e(Bs,AUe),e(Bs,zk),e(zk,LUe),e(Bs,BUe),e(Bs,Vk),e(Vk,kUe),e(Bs,xUe),e(M,RUe),e(M,ks),e(ks,hU),e(hU,SUe),e(ks,PUe),e(ks,Wk),e(Wk,$Ue),e(ks,IUe),e(ks,Qk),e(Qk,jUe),e(ks,NUe),e(mo,DUe),e(mo,pU),e(pU,qUe),e(mo,GUe),g(MM,mo,null),e(Oo,OUe),e(Oo,Wg),g(EM,Wg,null),e(Wg,XUe),e(Wg,_U),e(_U,zUe),b(d,L7e,u),b(d,ji,u),e(ji,Qg),e(Qg,uU),g(yM,uU,null),e(ji,VUe),e(ji,bU),e(bU,WUe),b(d,B7e,u),b(d,Xo,u),g(wM,Xo,null),e(Xo,QUe),e(Xo,AM),e(AM,HUe),e(AM,Hk),e(Hk,UUe),e(AM,JUe),e(Xo,YUe),e(Xo,LM),e(LM,KUe),e(LM,vU),e(vU,ZUe),e(LM,eJe),e(Xo,oJe),e(Xo,Le),g(BM,Le,null),e(Le,rJe),e(Le,TU),e(TU,tJe),e(Le,aJe),e(Le,Na),e(Na,nJe),e(Na,FU),e(FU,sJe),e(Na,lJe),e(Na,CU),e(CU,iJe),e(Na,dJe),e(Na,MU),e(MU,cJe),e(Na,fJe),e(Le,mJe),e(Le,se),e(se,Hg),e(Hg,EU),e(EU,gJe),e(Hg,hJe),e(Hg,Uk),e(Uk,pJe),e(Hg,_Je),e(se,uJe),e(se,Ug),e(Ug,yU),e(yU,bJe),e(Ug,vJe),e(Ug,Jk),e(Jk,TJe),e(Ug,FJe),e(se,CJe),e(se,Jg),e(Jg,wU),e(wU,MJe),e(Jg,EJe),e(Jg,Yk),e(Yk,yJe),e(Jg,wJe),e(se,AJe),e(se,Yg),e(Yg,AU),e(AU,LJe),e(Yg,BJe),e(Yg,Kk),e(Kk,kJe),e(Yg,xJe),e(se,RJe),e(se,Kg),e(Kg,LU),e(LU,SJe),e(Kg,PJe),e(Kg,Zk),e(Zk,$Je),e(Kg,IJe),e(se,jJe),e(se,Zg),e(Zg,BU),e(BU,NJe),e(Zg,DJe),e(Zg,ex),e(ex,qJe),e(Zg,GJe),e(se,OJe),e(se,eh),e(eh,kU),e(kU,XJe),e(eh,zJe),e(eh,ox),e(ox,VJe),e(eh,WJe),e(se,QJe),e(se,oh),e(oh,xU),e(xU,HJe),e(oh,UJe),e(oh,rx),e(rx,JJe),e(oh,YJe),e(se,KJe),e(se,rh),e(rh,RU),e(RU,ZJe),e(rh,eYe),e(rh,tx),e(tx,oYe),e(rh,rYe),e(se,tYe),e(se,th),e(th,SU),e(SU,aYe),e(th,nYe),e(th,ax),e(ax,sYe),e(th,lYe),e(se,iYe),e(se,ah),e(ah,PU),e(PU,dYe),e(ah,cYe),e(ah,nx),e(nx,fYe),e(ah,mYe),e(se,gYe),e(se,nh),e(nh,$U),e($U,hYe),e(nh,pYe),e(nh,sx),e(sx,_Ye),e(nh,uYe),e(se,bYe),e(se,sh),e(sh,IU),e(IU,vYe),e(sh,TYe),e(sh,lx),e(lx,FYe),e(sh,CYe),e(se,MYe),e(se,lh),e(lh,jU),e(jU,EYe),e(lh,yYe),e(lh,ix),e(ix,wYe),e(lh,AYe),e(se,LYe),e(se,ih),e(ih,NU),e(NU,BYe),e(ih,kYe),e(ih,dx),e(dx,xYe),e(ih,RYe),e(Le,SYe),g(dh,Le,null),e(Le,PYe),e(Le,DU),e(DU,$Ye),e(Le,IYe),g(kM,Le,null),e(Xo,jYe),e(Xo,ch),g(xM,ch,null),e(ch,NYe),e(ch,qU),e(qU,DYe),b(d,k7e,u),b(d,Ni,u),e(Ni,fh),e(fh,GU),g(RM,GU,null),e(Ni,qYe),e(Ni,OU),e(OU,GYe),b(d,x7e,u),b(d,zo,u),g(SM,zo,null),e(zo,OYe),e(zo,PM),e(PM,XYe),e(PM,cx),e(cx,zYe),e(PM,VYe),e(zo,WYe),e(zo,$M),e($M,QYe),e($M,XU),e(XU,HYe),e($M,UYe),e(zo,JYe),e(zo,Be),g(IM,Be,null),e(Be,YYe),e(Be,zU),e(zU,KYe),e(Be,ZYe),e(Be,Di),e(Di,eKe),e(Di,VU),e(VU,oKe),e(Di,rKe),e(Di,WU),e(WU,tKe),e(Di,aKe),e(Be,nKe),e(Be,we),e(we,mh),e(mh,QU),e(QU,sKe),e(mh,lKe),e(mh,fx),e(fx,iKe),e(mh,dKe),e(we,cKe),e(we,gh),e(gh,HU),e(HU,fKe),e(gh,mKe),e(gh,mx),e(mx,gKe),e(gh,hKe),e(we,pKe),e(we,hh),e(hh,UU),e(UU,_Ke),e(hh,uKe),e(hh,gx),e(gx,bKe),e(hh,vKe),e(we,TKe),e(we,ph),e(ph,JU),e(JU,FKe),e(ph,CKe),e(ph,hx),e(hx,MKe),e(ph,EKe),e(we,yKe),e(we,_h),e(_h,YU),e(YU,wKe),e(_h,AKe),e(_h,px),e(px,LKe),e(_h,BKe),e(we,kKe),e(we,uh),e(uh,KU),e(KU,xKe),e(uh,RKe),e(uh,_x),e(_x,SKe),e(uh,PKe),e(we,$Ke),e(we,bh),e(bh,ZU),e(ZU,IKe),e(bh,jKe),e(bh,ux),e(ux,NKe),e(bh,DKe),e(we,qKe),e(we,vh),e(vh,eJ),e(eJ,GKe),e(vh,OKe),e(vh,bx),e(bx,XKe),e(vh,zKe),e(Be,VKe),g(Th,Be,null),e(Be,WKe),e(Be,oJ),e(oJ,QKe),e(Be,HKe),g(jM,Be,null),e(zo,UKe),e(zo,Fh),g(NM,Fh,null),e(Fh,JKe),e(Fh,rJ),e(rJ,YKe),b(d,R7e,u),b(d,qi,u),e(qi,Ch),e(Ch,tJ),g(DM,tJ,null),e(qi,KKe),e(qi,aJ),e(aJ,ZKe),b(d,S7e,u),b(d,Vo,u),g(qM,Vo,null),e(Vo,eZe),e(Vo,Gi),e(Gi,oZe),e(Gi,nJ),e(nJ,rZe),e(Gi,tZe),e(Gi,sJ),e(sJ,aZe),e(Gi,nZe),e(Vo,sZe),e(Vo,GM),e(GM,lZe),e(GM,lJ),e(lJ,iZe),e(GM,dZe),e(Vo,cZe),e(Vo,Nr),g(OM,Nr,null),e(Nr,fZe),e(Nr,iJ),e(iJ,mZe),e(Nr,gZe),e(Nr,Oi),e(Oi,hZe),e(Oi,dJ),e(dJ,pZe),e(Oi,_Ze),e(Oi,cJ),e(cJ,uZe),e(Oi,bZe),e(Nr,vZe),e(Nr,fJ),e(fJ,TZe),e(Nr,FZe),g(XM,Nr,null),e(Vo,CZe),e(Vo,ke),g(zM,ke,null),e(ke,MZe),e(ke,mJ),e(mJ,EZe),e(ke,yZe),e(ke,Da),e(Da,wZe),e(Da,gJ),e(gJ,AZe),e(Da,LZe),e(Da,hJ),e(hJ,BZe),e(Da,kZe),e(Da,pJ),e(pJ,xZe),e(Da,RZe),e(ke,SZe),e(ke,F),e(F,Mh),e(Mh,_J),e(_J,PZe),e(Mh,$Ze),e(Mh,vx),e(vx,IZe),e(Mh,jZe),e(F,NZe),e(F,Eh),e(Eh,uJ),e(uJ,DZe),e(Eh,qZe),e(Eh,Tx),e(Tx,GZe),e(Eh,OZe),e(F,XZe),e(F,yh),e(yh,bJ),e(bJ,zZe),e(yh,VZe),e(yh,Fx),e(Fx,WZe),e(yh,QZe),e(F,HZe),e(F,wh),e(wh,vJ),e(vJ,UZe),e(wh,JZe),e(wh,Cx),e(Cx,YZe),e(wh,KZe),e(F,ZZe),e(F,Ah),e(Ah,TJ),e(TJ,eeo),e(Ah,oeo),e(Ah,Mx),e(Mx,reo),e(Ah,teo),e(F,aeo),e(F,Lh),e(Lh,FJ),e(FJ,neo),e(Lh,seo),e(Lh,Ex),e(Ex,leo),e(Lh,ieo),e(F,deo),e(F,Bh),e(Bh,CJ),e(CJ,ceo),e(Bh,feo),e(Bh,yx),e(yx,meo),e(Bh,geo),e(F,heo),e(F,kh),e(kh,MJ),e(MJ,peo),e(kh,_eo),e(kh,wx),e(wx,ueo),e(kh,beo),e(F,veo),e(F,xh),e(xh,EJ),e(EJ,Teo),e(xh,Feo),e(xh,Ax),e(Ax,Ceo),e(xh,Meo),e(F,Eeo),e(F,Rh),e(Rh,yJ),e(yJ,yeo),e(Rh,weo),e(Rh,Lx),e(Lx,Aeo),e(Rh,Leo),e(F,Beo),e(F,Sh),e(Sh,wJ),e(wJ,keo),e(Sh,xeo),e(Sh,Bx),e(Bx,Reo),e(Sh,Seo),e(F,Peo),e(F,Ph),e(Ph,AJ),e(AJ,$eo),e(Ph,Ieo),e(Ph,kx),e(kx,jeo),e(Ph,Neo),e(F,Deo),e(F,$h),e($h,LJ),e(LJ,qeo),e($h,Geo),e($h,xx),e(xx,Oeo),e($h,Xeo),e(F,zeo),e(F,Ih),e(Ih,BJ),e(BJ,Veo),e(Ih,Weo),e(Ih,Rx),e(Rx,Qeo),e(Ih,Heo),e(F,Ueo),e(F,jh),e(jh,kJ),e(kJ,Jeo),e(jh,Yeo),e(jh,Sx),e(Sx,Keo),e(jh,Zeo),e(F,eoo),e(F,Nh),e(Nh,xJ),e(xJ,ooo),e(Nh,roo),e(Nh,Px),e(Px,too),e(Nh,aoo),e(F,noo),e(F,Dh),e(Dh,RJ),e(RJ,soo),e(Dh,loo),e(Dh,$x),e($x,ioo),e(Dh,doo),e(F,coo),e(F,qh),e(qh,SJ),e(SJ,foo),e(qh,moo),e(qh,Ix),e(Ix,goo),e(qh,hoo),e(F,poo),e(F,Gh),e(Gh,PJ),e(PJ,_oo),e(Gh,uoo),e(Gh,jx),e(jx,boo),e(Gh,voo),e(F,Too),e(F,Oh),e(Oh,$J),e($J,Foo),e(Oh,Coo),e(Oh,Nx),e(Nx,Moo),e(Oh,Eoo),e(F,yoo),e(F,Xh),e(Xh,IJ),e(IJ,woo),e(Xh,Aoo),e(Xh,Dx),e(Dx,Loo),e(Xh,Boo),e(F,koo),e(F,zh),e(zh,jJ),e(jJ,xoo),e(zh,Roo),e(zh,qx),e(qx,Soo),e(zh,Poo),e(F,$oo),e(F,Vh),e(Vh,NJ),e(NJ,Ioo),e(Vh,joo),e(Vh,Gx),e(Gx,Noo),e(Vh,Doo),e(F,qoo),e(F,Wh),e(Wh,DJ),e(DJ,Goo),e(Wh,Ooo),e(Wh,Ox),e(Ox,Xoo),e(Wh,zoo),e(F,Voo),e(F,Qh),e(Qh,qJ),e(qJ,Woo),e(Qh,Qoo),e(Qh,Xx),e(Xx,Hoo),e(Qh,Uoo),e(F,Joo),e(F,xs),e(xs,GJ),e(GJ,Yoo),e(xs,Koo),e(xs,zx),e(zx,Zoo),e(xs,ero),e(xs,Vx),e(Vx,oro),e(xs,rro),e(F,tro),e(F,Hh),e(Hh,OJ),e(OJ,aro),e(Hh,nro),e(Hh,Wx),e(Wx,sro),e(Hh,lro),e(F,iro),e(F,Uh),e(Uh,XJ),e(XJ,dro),e(Uh,cro),e(Uh,Qx),e(Qx,fro),e(Uh,mro),e(F,gro),e(F,Jh),e(Jh,zJ),e(zJ,hro),e(Jh,pro),e(Jh,Hx),e(Hx,_ro),e(Jh,uro),e(F,bro),e(F,Yh),e(Yh,VJ),e(VJ,vro),e(Yh,Tro),e(Yh,Ux),e(Ux,Fro),e(Yh,Cro),e(F,Mro),e(F,Kh),e(Kh,WJ),e(WJ,Ero),e(Kh,yro),e(Kh,Jx),e(Jx,wro),e(Kh,Aro),e(F,Lro),e(F,Zh),e(Zh,QJ),e(QJ,Bro),e(Zh,kro),e(Zh,Yx),e(Yx,xro),e(Zh,Rro),e(F,Sro),e(F,ep),e(ep,HJ),e(HJ,Pro),e(ep,$ro),e(ep,Kx),e(Kx,Iro),e(ep,jro),e(F,Nro),e(F,op),e(op,UJ),e(UJ,Dro),e(op,qro),e(op,Zx),e(Zx,Gro),e(op,Oro),e(F,Xro),e(F,rp),e(rp,JJ),e(JJ,zro),e(rp,Vro),e(rp,eR),e(eR,Wro),e(rp,Qro),e(F,Hro),e(F,tp),e(tp,YJ),e(YJ,Uro),e(tp,Jro),e(tp,oR),e(oR,Yro),e(tp,Kro),e(F,Zro),e(F,ap),e(ap,KJ),e(KJ,eto),e(ap,oto),e(ap,rR),e(rR,rto),e(ap,tto),e(F,ato),e(F,np),e(np,ZJ),e(ZJ,nto),e(np,sto),e(np,tR),e(tR,lto),e(np,ito),e(F,dto),e(F,sp),e(sp,eY),e(eY,cto),e(sp,fto),e(sp,aR),e(aR,mto),e(sp,gto),e(F,hto),e(F,lp),e(lp,oY),e(oY,pto),e(lp,_to),e(lp,nR),e(nR,uto),e(lp,bto),e(F,vto),e(F,ip),e(ip,rY),e(rY,Tto),e(ip,Fto),e(ip,sR),e(sR,Cto),e(ip,Mto),e(F,Eto),e(F,dp),e(dp,tY),e(tY,yto),e(dp,wto),e(dp,lR),e(lR,Ato),e(dp,Lto),e(F,Bto),e(F,cp),e(cp,aY),e(aY,kto),e(cp,xto),e(cp,iR),e(iR,Rto),e(cp,Sto),e(F,Pto),e(F,fp),e(fp,nY),e(nY,$to),e(fp,Ito),e(fp,dR),e(dR,jto),e(fp,Nto),e(F,Dto),e(F,mp),e(mp,sY),e(sY,qto),e(mp,Gto),e(mp,cR),e(cR,Oto),e(mp,Xto),e(F,zto),e(F,gp),e(gp,lY),e(lY,Vto),e(gp,Wto),e(gp,fR),e(fR,Qto),e(gp,Hto),e(F,Uto),e(F,hp),e(hp,iY),e(iY,Jto),e(hp,Yto),e(hp,mR),e(mR,Kto),e(hp,Zto),e(F,eao),e(F,pp),e(pp,dY),e(dY,oao),e(pp,rao),e(pp,gR),e(gR,tao),e(pp,aao),e(F,nao),e(F,_p),e(_p,cY),e(cY,sao),e(_p,lao),e(_p,hR),e(hR,iao),e(_p,dao),e(F,cao),e(F,up),e(up,fY),e(fY,fao),e(up,mao),e(up,pR),e(pR,gao),e(up,hao),e(F,pao),e(F,bp),e(bp,mY),e(mY,_ao),e(bp,uao),e(bp,_R),e(_R,bao),e(bp,vao),e(F,Tao),e(F,vp),e(vp,gY),e(gY,Fao),e(vp,Cao),e(vp,uR),e(uR,Mao),e(vp,Eao),e(F,yao),e(F,Tp),e(Tp,hY),e(hY,wao),e(Tp,Aao),e(Tp,bR),e(bR,Lao),e(Tp,Bao),e(F,kao),e(F,Fp),e(Fp,pY),e(pY,xao),e(Fp,Rao),e(Fp,vR),e(vR,Sao),e(Fp,Pao),e(F,$ao),e(F,Cp),e(Cp,_Y),e(_Y,Iao),e(Cp,jao),e(Cp,TR),e(TR,Nao),e(Cp,Dao),e(F,qao),e(F,Mp),e(Mp,uY),e(uY,Gao),e(Mp,Oao),e(Mp,FR),e(FR,Xao),e(Mp,zao),e(F,Vao),e(F,Ep),e(Ep,bY),e(bY,Wao),e(Ep,Qao),e(Ep,CR),e(CR,Hao),e(Ep,Uao),e(F,Jao),e(F,yp),e(yp,vY),e(vY,Yao),e(yp,Kao),e(yp,MR),e(MR,Zao),e(yp,eno),e(F,ono),e(F,wp),e(wp,TY),e(TY,rno),e(wp,tno),e(wp,ER),e(ER,ano),e(wp,nno),e(F,sno),e(F,Ap),e(Ap,FY),e(FY,lno),e(Ap,ino),e(Ap,yR),e(yR,dno),e(Ap,cno),e(F,fno),e(F,Lp),e(Lp,CY),e(CY,mno),e(Lp,gno),e(Lp,wR),e(wR,hno),e(Lp,pno),e(F,_no),e(F,Bp),e(Bp,MY),e(MY,uno),e(Bp,bno),e(Bp,AR),e(AR,vno),e(Bp,Tno),e(F,Fno),e(F,kp),e(kp,EY),e(EY,Cno),e(kp,Mno),e(kp,LR),e(LR,Eno),e(kp,yno),e(F,wno),e(F,xp),e(xp,yY),e(yY,Ano),e(xp,Lno),e(xp,BR),e(BR,Bno),e(xp,kno),e(F,xno),e(F,Rp),e(Rp,wY),e(wY,Rno),e(Rp,Sno),e(Rp,kR),e(kR,Pno),e(Rp,$no),e(F,Ino),e(F,Sp),e(Sp,AY),e(AY,jno),e(Sp,Nno),e(Sp,xR),e(xR,Dno),e(Sp,qno),e(F,Gno),e(F,Pp),e(Pp,LY),e(LY,Ono),e(Pp,Xno),e(Pp,RR),e(RR,zno),e(Pp,Vno),e(F,Wno),e(F,$p),e($p,BY),e(BY,Qno),e($p,Hno),e($p,SR),e(SR,Uno),e($p,Jno),e(F,Yno),e(F,Ip),e(Ip,kY),e(kY,Kno),e(Ip,Zno),e(Ip,PR),e(PR,eso),e(Ip,oso),e(F,rso),e(F,jp),e(jp,xY),e(xY,tso),e(jp,aso),e(jp,$R),e($R,nso),e(jp,sso),e(F,lso),e(F,Np),e(Np,RY),e(RY,iso),e(Np,dso),e(Np,IR),e(IR,cso),e(Np,fso),e(F,mso),e(F,Dp),e(Dp,SY),e(SY,gso),e(Dp,hso),e(Dp,jR),e(jR,pso),e(Dp,_so),e(F,uso),e(F,qp),e(qp,PY),e(PY,bso),e(qp,vso),e(qp,NR),e(NR,Tso),e(qp,Fso),e(F,Cso),e(F,Gp),e(Gp,$Y),e($Y,Mso),e(Gp,Eso),e(Gp,DR),e(DR,yso),e(Gp,wso),e(F,Aso),e(F,Op),e(Op,IY),e(IY,Lso),e(Op,Bso),e(Op,qR),e(qR,kso),e(Op,xso),e(F,Rso),e(F,Xp),e(Xp,jY),e(jY,Sso),e(Xp,Pso),e(Xp,GR),e(GR,$so),e(Xp,Iso),e(F,jso),e(F,zp),e(zp,NY),e(NY,Nso),e(zp,Dso),e(zp,OR),e(OR,qso),e(zp,Gso),e(F,Oso),e(F,Vp),e(Vp,DY),e(DY,Xso),e(Vp,zso),e(Vp,XR),e(XR,Vso),e(Vp,Wso),e(F,Qso),e(F,Wp),e(Wp,qY),e(qY,Hso),e(Wp,Uso),e(Wp,zR),e(zR,Jso),e(Wp,Yso),e(F,Kso),e(F,Qp),e(Qp,GY),e(GY,Zso),e(Qp,elo),e(Qp,VR),e(VR,olo),e(Qp,rlo),e(F,tlo),e(F,Hp),e(Hp,OY),e(OY,alo),e(Hp,nlo),e(Hp,WR),e(WR,slo),e(Hp,llo),e(F,ilo),e(F,Up),e(Up,XY),e(XY,dlo),e(Up,clo),e(Up,QR),e(QR,flo),e(Up,mlo),e(F,glo),e(F,Jp),e(Jp,zY),e(zY,hlo),e(Jp,plo),e(Jp,HR),e(HR,_lo),e(Jp,ulo),e(F,blo),e(F,Yp),e(Yp,VY),e(VY,vlo),e(Yp,Tlo),e(Yp,UR),e(UR,Flo),e(Yp,Clo),e(F,Mlo),e(F,Kp),e(Kp,WY),e(WY,Elo),e(Kp,ylo),e(Kp,JR),e(JR,wlo),e(Kp,Alo),e(ke,Llo),e(ke,Zp),e(Zp,Blo),e(Zp,QY),e(QY,klo),e(Zp,xlo),e(Zp,HY),e(HY,Rlo),e(ke,Slo),e(ke,UY),e(UY,Plo),e(ke,$lo),g(VM,ke,null),b(d,P7e,u),b(d,Xi,u),e(Xi,e_),e(e_,JY),g(WM,JY,null),e(Xi,Ilo),e(Xi,YY),e(YY,jlo),b(d,$7e,u),b(d,Wo,u),g(QM,Wo,null),e(Wo,Nlo),e(Wo,zi),e(zi,Dlo),e(zi,KY),e(KY,qlo),e(zi,Glo),e(zi,ZY),e(ZY,Olo),e(zi,Xlo),e(Wo,zlo),e(Wo,HM),e(HM,Vlo),e(HM,eK),e(eK,Wlo),e(HM,Qlo),e(Wo,Hlo),e(Wo,Dr),g(UM,Dr,null),e(Dr,Ulo),e(Dr,oK),e(oK,Jlo),e(Dr,Ylo),e(Dr,Vi),e(Vi,Klo),e(Vi,rK),e(rK,Zlo),e(Vi,eio),e(Vi,tK),e(tK,oio),e(Vi,rio),e(Dr,tio),e(Dr,aK),e(aK,aio),e(Dr,nio),g(JM,Dr,null),e(Wo,sio),e(Wo,xe),g(YM,xe,null),e(xe,lio),e(xe,nK),e(nK,iio),e(xe,dio),e(xe,qa),e(qa,cio),e(qa,sK),e(sK,fio),e(qa,mio),e(qa,lK),e(lK,gio),e(qa,hio),e(qa,iK),e(iK,pio),e(qa,_io),e(xe,uio),e(xe,x),e(x,o_),e(o_,dK),e(dK,bio),e(o_,vio),e(o_,YR),e(YR,Tio),e(o_,Fio),e(x,Cio),e(x,r_),e(r_,cK),e(cK,Mio),e(r_,Eio),e(r_,KR),e(KR,yio),e(r_,wio),e(x,Aio),e(x,t_),e(t_,fK),e(fK,Lio),e(t_,Bio),e(t_,ZR),e(ZR,kio),e(t_,xio),e(x,Rio),e(x,a_),e(a_,mK),e(mK,Sio),e(a_,Pio),e(a_,eS),e(eS,$io),e(a_,Iio),e(x,jio),e(x,n_),e(n_,gK),e(gK,Nio),e(n_,Dio),e(n_,oS),e(oS,qio),e(n_,Gio),e(x,Oio),e(x,s_),e(s_,hK),e(hK,Xio),e(s_,zio),e(s_,rS),e(rS,Vio),e(s_,Wio),e(x,Qio),e(x,l_),e(l_,pK),e(pK,Hio),e(l_,Uio),e(l_,tS),e(tS,Jio),e(l_,Yio),e(x,Kio),e(x,i_),e(i_,_K),e(_K,Zio),e(i_,edo),e(i_,aS),e(aS,odo),e(i_,rdo),e(x,tdo),e(x,d_),e(d_,uK),e(uK,ado),e(d_,ndo),e(d_,nS),e(nS,sdo),e(d_,ldo),e(x,ido),e(x,c_),e(c_,bK),e(bK,ddo),e(c_,cdo),e(c_,sS),e(sS,fdo),e(c_,mdo),e(x,gdo),e(x,f_),e(f_,vK),e(vK,hdo),e(f_,pdo),e(f_,lS),e(lS,_do),e(f_,udo),e(x,bdo),e(x,m_),e(m_,TK),e(TK,vdo),e(m_,Tdo),e(m_,iS),e(iS,Fdo),e(m_,Cdo),e(x,Mdo),e(x,g_),e(g_,FK),e(FK,Edo),e(g_,ydo),e(g_,dS),e(dS,wdo),e(g_,Ado),e(x,Ldo),e(x,h_),e(h_,CK),e(CK,Bdo),e(h_,kdo),e(h_,cS),e(cS,xdo),e(h_,Rdo),e(x,Sdo),e(x,p_),e(p_,MK),e(MK,Pdo),e(p_,$do),e(p_,fS),e(fS,Ido),e(p_,jdo),e(x,Ndo),e(x,__),e(__,EK),e(EK,Ddo),e(__,qdo),e(__,mS),e(mS,Gdo),e(__,Odo),e(x,Xdo),e(x,u_),e(u_,yK),e(yK,zdo),e(u_,Vdo),e(u_,gS),e(gS,Wdo),e(u_,Qdo),e(x,Hdo),e(x,b_),e(b_,wK),e(wK,Udo),e(b_,Jdo),e(b_,hS),e(hS,Ydo),e(b_,Kdo),e(x,Zdo),e(x,v_),e(v_,AK),e(AK,eco),e(v_,oco),e(v_,pS),e(pS,rco),e(v_,tco),e(x,aco),e(x,T_),e(T_,LK),e(LK,nco),e(T_,sco),e(T_,_S),e(_S,lco),e(T_,ico),e(x,dco),e(x,F_),e(F_,BK),e(BK,cco),e(F_,fco),e(F_,uS),e(uS,mco),e(F_,gco),e(x,hco),e(x,C_),e(C_,kK),e(kK,pco),e(C_,_co),e(C_,bS),e(bS,uco),e(C_,bco),e(x,vco),e(x,M_),e(M_,xK),e(xK,Tco),e(M_,Fco),e(M_,vS),e(vS,Cco),e(M_,Mco),e(x,Eco),e(x,E_),e(E_,RK),e(RK,yco),e(E_,wco),e(E_,TS),e(TS,Aco),e(E_,Lco),e(x,Bco),e(x,y_),e(y_,SK),e(SK,kco),e(y_,xco),e(y_,FS),e(FS,Rco),e(y_,Sco),e(x,Pco),e(x,w_),e(w_,PK),e(PK,$co),e(w_,Ico),e(w_,CS),e(CS,jco),e(w_,Nco),e(x,Dco),e(x,A_),e(A_,$K),e($K,qco),e(A_,Gco),e(A_,MS),e(MS,Oco),e(A_,Xco),e(x,zco),e(x,L_),e(L_,IK),e(IK,Vco),e(L_,Wco),e(L_,ES),e(ES,Qco),e(L_,Hco),e(x,Uco),e(x,B_),e(B_,jK),e(jK,Jco),e(B_,Yco),e(B_,yS),e(yS,Kco),e(B_,Zco),e(x,efo),e(x,k_),e(k_,NK),e(NK,ofo),e(k_,rfo),e(k_,wS),e(wS,tfo),e(k_,afo),e(x,nfo),e(x,x_),e(x_,DK),e(DK,sfo),e(x_,lfo),e(x_,AS),e(AS,ifo),e(x_,dfo),e(x,cfo),e(x,R_),e(R_,qK),e(qK,ffo),e(R_,mfo),e(R_,LS),e(LS,gfo),e(R_,hfo),e(x,pfo),e(x,S_),e(S_,GK),e(GK,_fo),e(S_,ufo),e(S_,BS),e(BS,bfo),e(S_,vfo),e(x,Tfo),e(x,P_),e(P_,OK),e(OK,Ffo),e(P_,Cfo),e(P_,kS),e(kS,Mfo),e(P_,Efo),e(x,yfo),e(x,$_),e($_,XK),e(XK,wfo),e($_,Afo),e($_,xS),e(xS,Lfo),e($_,Bfo),e(x,kfo),e(x,I_),e(I_,zK),e(zK,xfo),e(I_,Rfo),e(I_,RS),e(RS,Sfo),e(I_,Pfo),e(x,$fo),e(x,j_),e(j_,VK),e(VK,Ifo),e(j_,jfo),e(j_,SS),e(SS,Nfo),e(j_,Dfo),e(x,qfo),e(x,N_),e(N_,WK),e(WK,Gfo),e(N_,Ofo),e(N_,PS),e(PS,Xfo),e(N_,zfo),e(xe,Vfo),e(xe,D_),e(D_,Wfo),e(D_,QK),e(QK,Qfo),e(D_,Hfo),e(D_,HK),e(HK,Ufo),e(xe,Jfo),e(xe,UK),e(UK,Yfo),e(xe,Kfo),g(KM,xe,null),b(d,I7e,u),b(d,Wi,u),e(Wi,q_),e(q_,JK),g(ZM,JK,null),e(Wi,Zfo),e(Wi,YK),e(YK,emo),b(d,j7e,u),b(d,Qo,u),g(eE,Qo,null),e(Qo,omo),e(Qo,Qi),e(Qi,rmo),e(Qi,KK),e(KK,tmo),e(Qi,amo),e(Qi,ZK),e(ZK,nmo),e(Qi,smo),e(Qo,lmo),e(Qo,oE),e(oE,imo),e(oE,eZ),e(eZ,dmo),e(oE,cmo),e(Qo,fmo),e(Qo,qr),g(rE,qr,null),e(qr,mmo),e(qr,oZ),e(oZ,gmo),e(qr,hmo),e(qr,Hi),e(Hi,pmo),e(Hi,rZ),e(rZ,_mo),e(Hi,umo),e(Hi,tZ),e(tZ,bmo),e(Hi,vmo),e(qr,Tmo),e(qr,aZ),e(aZ,Fmo),e(qr,Cmo),g(tE,qr,null),e(Qo,Mmo),e(Qo,Re),g(aE,Re,null),e(Re,Emo),e(Re,nZ),e(nZ,ymo),e(Re,wmo),e(Re,Ga),e(Ga,Amo),e(Ga,sZ),e(sZ,Lmo),e(Ga,Bmo),e(Ga,lZ),e(lZ,kmo),e(Ga,xmo),e(Ga,iZ),e(iZ,Rmo),e(Ga,Smo),e(Re,Pmo),e(Re,$),e($,G_),e(G_,dZ),e(dZ,$mo),e(G_,Imo),e(G_,$S),e($S,jmo),e(G_,Nmo),e($,Dmo),e($,O_),e(O_,cZ),e(cZ,qmo),e(O_,Gmo),e(O_,IS),e(IS,Omo),e(O_,Xmo),e($,zmo),e($,X_),e(X_,fZ),e(fZ,Vmo),e(X_,Wmo),e(X_,jS),e(jS,Qmo),e(X_,Hmo),e($,Umo),e($,z_),e(z_,mZ),e(mZ,Jmo),e(z_,Ymo),e(z_,NS),e(NS,Kmo),e(z_,Zmo),e($,ego),e($,V_),e(V_,gZ),e(gZ,ogo),e(V_,rgo),e(V_,DS),e(DS,tgo),e(V_,ago),e($,ngo),e($,W_),e(W_,hZ),e(hZ,sgo),e(W_,lgo),e(W_,qS),e(qS,igo),e(W_,dgo),e($,cgo),e($,Q_),e(Q_,pZ),e(pZ,fgo),e(Q_,mgo),e(Q_,GS),e(GS,ggo),e(Q_,hgo),e($,pgo),e($,H_),e(H_,_Z),e(_Z,_go),e(H_,ugo),e(H_,OS),e(OS,bgo),e(H_,vgo),e($,Tgo),e($,U_),e(U_,uZ),e(uZ,Fgo),e(U_,Cgo),e(U_,XS),e(XS,Mgo),e(U_,Ego),e($,ygo),e($,J_),e(J_,bZ),e(bZ,wgo),e(J_,Ago),e(J_,zS),e(zS,Lgo),e(J_,Bgo),e($,kgo),e($,Y_),e(Y_,vZ),e(vZ,xgo),e(Y_,Rgo),e(Y_,VS),e(VS,Sgo),e(Y_,Pgo),e($,$go),e($,K_),e(K_,TZ),e(TZ,Igo),e(K_,jgo),e(K_,WS),e(WS,Ngo),e(K_,Dgo),e($,qgo),e($,Z_),e(Z_,FZ),e(FZ,Ggo),e(Z_,Ogo),e(Z_,QS),e(QS,Xgo),e(Z_,zgo),e($,Vgo),e($,eu),e(eu,CZ),e(CZ,Wgo),e(eu,Qgo),e(eu,HS),e(HS,Hgo),e(eu,Ugo),e($,Jgo),e($,ou),e(ou,MZ),e(MZ,Ygo),e(ou,Kgo),e(ou,US),e(US,Zgo),e(ou,eho),e($,oho),e($,ru),e(ru,EZ),e(EZ,rho),e(ru,tho),e(ru,JS),e(JS,aho),e(ru,nho),e($,sho),e($,tu),e(tu,yZ),e(yZ,lho),e(tu,iho),e(tu,YS),e(YS,dho),e(tu,cho),e($,fho),e($,au),e(au,wZ),e(wZ,mho),e(au,gho),e(au,KS),e(KS,hho),e(au,pho),e($,_ho),e($,nu),e(nu,AZ),e(AZ,uho),e(nu,bho),e(nu,ZS),e(ZS,vho),e(nu,Tho),e($,Fho),e($,su),e(su,LZ),e(LZ,Cho),e(su,Mho),e(su,eP),e(eP,Eho),e(su,yho),e($,who),e($,lu),e(lu,BZ),e(BZ,Aho),e(lu,Lho),e(lu,oP),e(oP,Bho),e(lu,kho),e($,xho),e($,iu),e(iu,kZ),e(kZ,Rho),e(iu,Sho),e(iu,rP),e(rP,Pho),e(iu,$ho),e($,Iho),e($,du),e(du,xZ),e(xZ,jho),e(du,Nho),e(du,tP),e(tP,Dho),e(du,qho),e($,Gho),e($,cu),e(cu,RZ),e(RZ,Oho),e(cu,Xho),e(cu,aP),e(aP,zho),e(cu,Vho),e($,Who),e($,fu),e(fu,SZ),e(SZ,Qho),e(fu,Hho),e(fu,nP),e(nP,Uho),e(fu,Jho),e($,Yho),e($,mu),e(mu,PZ),e(PZ,Kho),e(mu,Zho),e(mu,sP),e(sP,epo),e(mu,opo),e($,rpo),e($,gu),e(gu,$Z),e($Z,tpo),e(gu,apo),e(gu,lP),e(lP,npo),e(gu,spo),e($,lpo),e($,hu),e(hu,IZ),e(IZ,ipo),e(hu,dpo),e(hu,iP),e(iP,cpo),e(hu,fpo),e($,mpo),e($,pu),e(pu,jZ),e(jZ,gpo),e(pu,hpo),e(pu,dP),e(dP,ppo),e(pu,_po),e($,upo),e($,_u),e(_u,NZ),e(NZ,bpo),e(_u,vpo),e(_u,cP),e(cP,Tpo),e(_u,Fpo),e($,Cpo),e($,uu),e(uu,DZ),e(DZ,Mpo),e(uu,Epo),e(uu,fP),e(fP,ypo),e(uu,wpo),e($,Apo),e($,bu),e(bu,qZ),e(qZ,Lpo),e(bu,Bpo),e(bu,mP),e(mP,kpo),e(bu,xpo),e($,Rpo),e($,vu),e(vu,GZ),e(GZ,Spo),e(vu,Ppo),e(vu,gP),e(gP,$po),e(vu,Ipo),e($,jpo),e($,Tu),e(Tu,OZ),e(OZ,Npo),e(Tu,Dpo),e(Tu,hP),e(hP,qpo),e(Tu,Gpo),e(Re,Opo),e(Re,Fu),e(Fu,Xpo),e(Fu,XZ),e(XZ,zpo),e(Fu,Vpo),e(Fu,zZ),e(zZ,Wpo),e(Re,Qpo),e(Re,VZ),e(VZ,Hpo),e(Re,Upo),g(nE,Re,null),b(d,N7e,u),b(d,Ui,u),e(Ui,Cu),e(Cu,WZ),g(sE,WZ,null),e(Ui,Jpo),e(Ui,QZ),e(QZ,Ypo),b(d,D7e,u),b(d,Ho,u),g(lE,Ho,null),e(Ho,Kpo),e(Ho,Ji),e(Ji,Zpo),e(Ji,HZ),e(HZ,e_o),e(Ji,o_o),e(Ji,UZ),e(UZ,r_o),e(Ji,t_o),e(Ho,a_o),e(Ho,iE),e(iE,n_o),e(iE,JZ),e(JZ,s_o),e(iE,l_o),e(Ho,i_o),e(Ho,Gr),g(dE,Gr,null),e(Gr,d_o),e(Gr,YZ),e(YZ,c_o),e(Gr,f_o),e(Gr,Yi),e(Yi,m_o),e(Yi,KZ),e(KZ,g_o),e(Yi,h_o),e(Yi,ZZ),e(ZZ,p_o),e(Yi,__o),e(Gr,u_o),e(Gr,eee),e(eee,b_o),e(Gr,v_o),g(cE,Gr,null),e(Ho,T_o),e(Ho,Se),g(fE,Se,null),e(Se,F_o),e(Se,oee),e(oee,C_o),e(Se,M_o),e(Se,Oa),e(Oa,E_o),e(Oa,ree),e(ree,y_o),e(Oa,w_o),e(Oa,tee),e(tee,A_o),e(Oa,L_o),e(Oa,aee),e(aee,B_o),e(Oa,k_o),e(Se,x_o),e(Se,I),e(I,Mu),e(Mu,nee),e(nee,R_o),e(Mu,S_o),e(Mu,pP),e(pP,P_o),e(Mu,$_o),e(I,I_o),e(I,Eu),e(Eu,see),e(see,j_o),e(Eu,N_o),e(Eu,_P),e(_P,D_o),e(Eu,q_o),e(I,G_o),e(I,yu),e(yu,lee),e(lee,O_o),e(yu,X_o),e(yu,uP),e(uP,z_o),e(yu,V_o),e(I,W_o),e(I,wu),e(wu,iee),e(iee,Q_o),e(wu,H_o),e(wu,bP),e(bP,U_o),e(wu,J_o),e(I,Y_o),e(I,Au),e(Au,dee),e(dee,K_o),e(Au,Z_o),e(Au,vP),e(vP,euo),e(Au,ouo),e(I,ruo),e(I,Lu),e(Lu,cee),e(cee,tuo),e(Lu,auo),e(Lu,TP),e(TP,nuo),e(Lu,suo),e(I,luo),e(I,Bu),e(Bu,fee),e(fee,iuo),e(Bu,duo),e(Bu,FP),e(FP,cuo),e(Bu,fuo),e(I,muo),e(I,ku),e(ku,mee),e(mee,guo),e(ku,huo),e(ku,CP),e(CP,puo),e(ku,_uo),e(I,uuo),e(I,xu),e(xu,gee),e(gee,buo),e(xu,vuo),e(xu,MP),e(MP,Tuo),e(xu,Fuo),e(I,Cuo),e(I,Ru),e(Ru,hee),e(hee,Muo),e(Ru,Euo),e(Ru,EP),e(EP,yuo),e(Ru,wuo),e(I,Auo),e(I,Su),e(Su,pee),e(pee,Luo),e(Su,Buo),e(Su,yP),e(yP,kuo),e(Su,xuo),e(I,Ruo),e(I,Pu),e(Pu,_ee),e(_ee,Suo),e(Pu,Puo),e(Pu,wP),e(wP,$uo),e(Pu,Iuo),e(I,juo),e(I,$u),e($u,uee),e(uee,Nuo),e($u,Duo),e($u,AP),e(AP,quo),e($u,Guo),e(I,Ouo),e(I,Iu),e(Iu,bee),e(bee,Xuo),e(Iu,zuo),e(Iu,LP),e(LP,Vuo),e(Iu,Wuo),e(I,Quo),e(I,ju),e(ju,vee),e(vee,Huo),e(ju,Uuo),e(ju,BP),e(BP,Juo),e(ju,Yuo),e(I,Kuo),e(I,Nu),e(Nu,Tee),e(Tee,Zuo),e(Nu,e2o),e(Nu,kP),e(kP,o2o),e(Nu,r2o),e(I,t2o),e(I,Du),e(Du,Fee),e(Fee,a2o),e(Du,n2o),e(Du,xP),e(xP,s2o),e(Du,l2o),e(I,i2o),e(I,qu),e(qu,Cee),e(Cee,d2o),e(qu,c2o),e(qu,RP),e(RP,f2o),e(qu,m2o),e(I,g2o),e(I,Gu),e(Gu,Mee),e(Mee,h2o),e(Gu,p2o),e(Gu,SP),e(SP,_2o),e(Gu,u2o),e(I,b2o),e(I,Ou),e(Ou,Eee),e(Eee,v2o),e(Ou,T2o),e(Ou,PP),e(PP,F2o),e(Ou,C2o),e(I,M2o),e(I,Xu),e(Xu,yee),e(yee,E2o),e(Xu,y2o),e(Xu,$P),e($P,w2o),e(Xu,A2o),e(I,L2o),e(I,zu),e(zu,wee),e(wee,B2o),e(zu,k2o),e(zu,IP),e(IP,x2o),e(zu,R2o),e(I,S2o),e(I,Vu),e(Vu,Aee),e(Aee,P2o),e(Vu,$2o),e(Vu,jP),e(jP,I2o),e(Vu,j2o),e(I,N2o),e(I,Wu),e(Wu,Lee),e(Lee,D2o),e(Wu,q2o),e(Wu,NP),e(NP,G2o),e(Wu,O2o),e(I,X2o),e(I,Qu),e(Qu,Bee),e(Bee,z2o),e(Qu,V2o),e(Qu,DP),e(DP,W2o),e(Qu,Q2o),e(I,H2o),e(I,Hu),e(Hu,kee),e(kee,U2o),e(Hu,J2o),e(Hu,qP),e(qP,Y2o),e(Hu,K2o),e(I,Z2o),e(I,Uu),e(Uu,xee),e(xee,e1o),e(Uu,o1o),e(Uu,GP),e(GP,r1o),e(Uu,t1o),e(I,a1o),e(I,Ju),e(Ju,Ree),e(Ree,n1o),e(Ju,s1o),e(Ju,OP),e(OP,l1o),e(Ju,i1o),e(I,d1o),e(I,Yu),e(Yu,See),e(See,c1o),e(Yu,f1o),e(Yu,XP),e(XP,m1o),e(Yu,g1o),e(I,h1o),e(I,Ku),e(Ku,Pee),e(Pee,p1o),e(Ku,_1o),e(Ku,$ee),e($ee,u1o),e(Ku,b1o),e(I,v1o),e(I,Zu),e(Zu,Iee),e(Iee,T1o),e(Zu,F1o),e(Zu,zP),e(zP,C1o),e(Zu,M1o),e(I,E1o),e(I,e2),e(e2,jee),e(jee,y1o),e(e2,w1o),e(e2,VP),e(VP,A1o),e(e2,L1o),e(I,B1o),e(I,o2),e(o2,Nee),e(Nee,k1o),e(o2,x1o),e(o2,WP),e(WP,R1o),e(o2,S1o),e(I,P1o),e(I,r2),e(r2,Dee),e(Dee,$1o),e(r2,I1o),e(r2,QP),e(QP,j1o),e(r2,N1o),e(Se,D1o),e(Se,t2),e(t2,q1o),e(t2,qee),e(qee,G1o),e(t2,O1o),e(t2,Gee),e(Gee,X1o),e(Se,z1o),e(Se,Oee),e(Oee,V1o),e(Se,W1o),g(mE,Se,null),b(d,q7e,u),b(d,Ki,u),e(Ki,a2),e(a2,Xee),g(gE,Xee,null),e(Ki,Q1o),e(Ki,zee),e(zee,H1o),b(d,G7e,u),b(d,Uo,u),g(hE,Uo,null),e(Uo,U1o),e(Uo,Zi),e(Zi,J1o),e(Zi,Vee),e(Vee,Y1o),e(Zi,K1o),e(Zi,Wee),e(Wee,Z1o),e(Zi,ebo),e(Uo,obo),e(Uo,pE),e(pE,rbo),e(pE,Qee),e(Qee,tbo),e(pE,abo),e(Uo,nbo),e(Uo,Or),g(_E,Or,null),e(Or,sbo),e(Or,Hee),e(Hee,lbo),e(Or,ibo),e(Or,ed),e(ed,dbo),e(ed,Uee),e(Uee,cbo),e(ed,fbo),e(ed,Jee),e(Jee,mbo),e(ed,gbo),e(Or,hbo),e(Or,Yee),e(Yee,pbo),e(Or,_bo),g(uE,Or,null),e(Uo,ubo),e(Uo,Pe),g(bE,Pe,null),e(Pe,bbo),e(Pe,Kee),e(Kee,vbo),e(Pe,Tbo),e(Pe,Xa),e(Xa,Fbo),e(Xa,Zee),e(Zee,Cbo),e(Xa,Mbo),e(Xa,eoe),e(eoe,Ebo),e(Xa,ybo),e(Xa,ooe),e(ooe,wbo),e(Xa,Abo),e(Pe,Lbo),e(Pe,ae),e(ae,n2),e(n2,roe),e(roe,Bbo),e(n2,kbo),e(n2,HP),e(HP,xbo),e(n2,Rbo),e(ae,Sbo),e(ae,s2),e(s2,toe),e(toe,Pbo),e(s2,$bo),e(s2,UP),e(UP,Ibo),e(s2,jbo),e(ae,Nbo),e(ae,l2),e(l2,aoe),e(aoe,Dbo),e(l2,qbo),e(l2,JP),e(JP,Gbo),e(l2,Obo),e(ae,Xbo),e(ae,i2),e(i2,noe),e(noe,zbo),e(i2,Vbo),e(i2,YP),e(YP,Wbo),e(i2,Qbo),e(ae,Hbo),e(ae,d2),e(d2,soe),e(soe,Ubo),e(d2,Jbo),e(d2,KP),e(KP,Ybo),e(d2,Kbo),e(ae,Zbo),e(ae,c2),e(c2,loe),e(loe,e5o),e(c2,o5o),e(c2,ZP),e(ZP,r5o),e(c2,t5o),e(ae,a5o),e(ae,f2),e(f2,ioe),e(ioe,n5o),e(f2,s5o),e(f2,e$),e(e$,l5o),e(f2,i5o),e(ae,d5o),e(ae,m2),e(m2,doe),e(doe,c5o),e(m2,f5o),e(m2,o$),e(o$,m5o),e(m2,g5o),e(ae,h5o),e(ae,g2),e(g2,coe),e(coe,p5o),e(g2,_5o),e(g2,r$),e(r$,u5o),e(g2,b5o),e(ae,v5o),e(ae,h2),e(h2,foe),e(foe,T5o),e(h2,F5o),e(h2,t$),e(t$,C5o),e(h2,M5o),e(ae,E5o),e(ae,p2),e(p2,moe),e(moe,y5o),e(p2,w5o),e(p2,a$),e(a$,A5o),e(p2,L5o),e(ae,B5o),e(ae,_2),e(_2,goe),e(goe,k5o),e(_2,x5o),e(_2,n$),e(n$,R5o),e(_2,S5o),e(ae,P5o),e(ae,u2),e(u2,hoe),e(hoe,$5o),e(u2,I5o),e(u2,s$),e(s$,j5o),e(u2,N5o),e(ae,D5o),e(ae,b2),e(b2,poe),e(poe,q5o),e(b2,G5o),e(b2,l$),e(l$,O5o),e(b2,X5o),e(ae,z5o),e(ae,v2),e(v2,_oe),e(_oe,V5o),e(v2,W5o),e(v2,i$),e(i$,Q5o),e(v2,H5o),e(ae,U5o),e(ae,T2),e(T2,uoe),e(uoe,J5o),e(T2,Y5o),e(T2,d$),e(d$,K5o),e(T2,Z5o),e(Pe,evo),e(Pe,F2),e(F2,ovo),e(F2,boe),e(boe,rvo),e(F2,tvo),e(F2,voe),e(voe,avo),e(Pe,nvo),e(Pe,Toe),e(Toe,svo),e(Pe,lvo),g(vE,Pe,null),b(d,O7e,u),b(d,od,u),e(od,C2),e(C2,Foe),g(TE,Foe,null),e(od,ivo),e(od,Coe),e(Coe,dvo),b(d,X7e,u),b(d,Jo,u),g(FE,Jo,null),e(Jo,cvo),e(Jo,rd),e(rd,fvo),e(rd,Moe),e(Moe,mvo),e(rd,gvo),e(rd,Eoe),e(Eoe,hvo),e(rd,pvo),e(Jo,_vo),e(Jo,CE),e(CE,uvo),e(CE,yoe),e(yoe,bvo),e(CE,vvo),e(Jo,Tvo),e(Jo,Xr),g(ME,Xr,null),e(Xr,Fvo),e(Xr,woe),e(woe,Cvo),e(Xr,Mvo),e(Xr,td),e(td,Evo),e(td,Aoe),e(Aoe,yvo),e(td,wvo),e(td,Loe),e(Loe,Avo),e(td,Lvo),e(Xr,Bvo),e(Xr,Boe),e(Boe,kvo),e(Xr,xvo),g(EE,Xr,null),e(Jo,Rvo),e(Jo,$e),g(yE,$e,null),e($e,Svo),e($e,koe),e(koe,Pvo),e($e,$vo),e($e,za),e(za,Ivo),e(za,xoe),e(xoe,jvo),e(za,Nvo),e(za,Roe),e(Roe,Dvo),e(za,qvo),e(za,Soe),e(Soe,Gvo),e(za,Ovo),e($e,Xvo),e($e,A),e(A,M2),e(M2,Poe),e(Poe,zvo),e(M2,Vvo),e(M2,c$),e(c$,Wvo),e(M2,Qvo),e(A,Hvo),e(A,E2),e(E2,$oe),e($oe,Uvo),e(E2,Jvo),e(E2,f$),e(f$,Yvo),e(E2,Kvo),e(A,Zvo),e(A,y2),e(y2,Ioe),e(Ioe,e6o),e(y2,o6o),e(y2,m$),e(m$,r6o),e(y2,t6o),e(A,a6o),e(A,w2),e(w2,joe),e(joe,n6o),e(w2,s6o),e(w2,g$),e(g$,l6o),e(w2,i6o),e(A,d6o),e(A,A2),e(A2,Noe),e(Noe,c6o),e(A2,f6o),e(A2,h$),e(h$,m6o),e(A2,g6o),e(A,h6o),e(A,L2),e(L2,Doe),e(Doe,p6o),e(L2,_6o),e(L2,p$),e(p$,u6o),e(L2,b6o),e(A,v6o),e(A,B2),e(B2,qoe),e(qoe,T6o),e(B2,F6o),e(B2,_$),e(_$,C6o),e(B2,M6o),e(A,E6o),e(A,k2),e(k2,Goe),e(Goe,y6o),e(k2,w6o),e(k2,u$),e(u$,A6o),e(k2,L6o),e(A,B6o),e(A,x2),e(x2,Ooe),e(Ooe,k6o),e(x2,x6o),e(x2,b$),e(b$,R6o),e(x2,S6o),e(A,P6o),e(A,R2),e(R2,Xoe),e(Xoe,$6o),e(R2,I6o),e(R2,v$),e(v$,j6o),e(R2,N6o),e(A,D6o),e(A,S2),e(S2,zoe),e(zoe,q6o),e(S2,G6o),e(S2,T$),e(T$,O6o),e(S2,X6o),e(A,z6o),e(A,P2),e(P2,Voe),e(Voe,V6o),e(P2,W6o),e(P2,F$),e(F$,Q6o),e(P2,H6o),e(A,U6o),e(A,$2),e($2,Woe),e(Woe,J6o),e($2,Y6o),e($2,C$),e(C$,K6o),e($2,Z6o),e(A,eTo),e(A,I2),e(I2,Qoe),e(Qoe,oTo),e(I2,rTo),e(I2,M$),e(M$,tTo),e(I2,aTo),e(A,nTo),e(A,j2),e(j2,Hoe),e(Hoe,sTo),e(j2,lTo),e(j2,E$),e(E$,iTo),e(j2,dTo),e(A,cTo),e(A,N2),e(N2,Uoe),e(Uoe,fTo),e(N2,mTo),e(N2,y$),e(y$,gTo),e(N2,hTo),e(A,pTo),e(A,D2),e(D2,Joe),e(Joe,_To),e(D2,uTo),e(D2,w$),e(w$,bTo),e(D2,vTo),e(A,TTo),e(A,q2),e(q2,Yoe),e(Yoe,FTo),e(q2,CTo),e(q2,A$),e(A$,MTo),e(q2,ETo),e(A,yTo),e(A,G2),e(G2,Koe),e(Koe,wTo),e(G2,ATo),e(G2,L$),e(L$,LTo),e(G2,BTo),e(A,kTo),e(A,O2),e(O2,Zoe),e(Zoe,xTo),e(O2,RTo),e(O2,B$),e(B$,STo),e(O2,PTo),e(A,$To),e(A,X2),e(X2,ere),e(ere,ITo),e(X2,jTo),e(X2,k$),e(k$,NTo),e(X2,DTo),e(A,qTo),e(A,z2),e(z2,ore),e(ore,GTo),e(z2,OTo),e(z2,x$),e(x$,XTo),e(z2,zTo),e(A,VTo),e(A,V2),e(V2,rre),e(rre,WTo),e(V2,QTo),e(V2,R$),e(R$,HTo),e(V2,UTo),e(A,JTo),e(A,W2),e(W2,tre),e(tre,YTo),e(W2,KTo),e(W2,S$),e(S$,ZTo),e(W2,e8o),e(A,o8o),e(A,Q2),e(Q2,are),e(are,r8o),e(Q2,t8o),e(Q2,P$),e(P$,a8o),e(Q2,n8o),e(A,s8o),e(A,H2),e(H2,nre),e(nre,l8o),e(H2,i8o),e(H2,$$),e($$,d8o),e(H2,c8o),e(A,f8o),e(A,U2),e(U2,sre),e(sre,m8o),e(U2,g8o),e(U2,I$),e(I$,h8o),e(U2,p8o),e(A,_8o),e(A,J2),e(J2,lre),e(lre,u8o),e(J2,b8o),e(J2,j$),e(j$,v8o),e(J2,T8o),e(A,F8o),e(A,Y2),e(Y2,ire),e(ire,C8o),e(Y2,M8o),e(Y2,N$),e(N$,E8o),e(Y2,y8o),e(A,w8o),e(A,K2),e(K2,dre),e(dre,A8o),e(K2,L8o),e(K2,D$),e(D$,B8o),e(K2,k8o),e(A,x8o),e(A,Z2),e(Z2,cre),e(cre,R8o),e(Z2,S8o),e(Z2,q$),e(q$,P8o),e(Z2,$8o),e(A,I8o),e(A,e1),e(e1,fre),e(fre,j8o),e(e1,N8o),e(e1,G$),e(G$,D8o),e(e1,q8o),e(A,G8o),e(A,o1),e(o1,mre),e(mre,O8o),e(o1,X8o),e(o1,O$),e(O$,z8o),e(o1,V8o),e(A,W8o),e(A,r1),e(r1,gre),e(gre,Q8o),e(r1,H8o),e(r1,X$),e(X$,U8o),e(r1,J8o),e(A,Y8o),e(A,t1),e(t1,hre),e(hre,K8o),e(t1,Z8o),e(t1,z$),e(z$,eFo),e(t1,oFo),e(A,rFo),e(A,a1),e(a1,pre),e(pre,tFo),e(a1,aFo),e(a1,V$),e(V$,nFo),e(a1,sFo),e(A,lFo),e(A,n1),e(n1,_re),e(_re,iFo),e(n1,dFo),e(n1,W$),e(W$,cFo),e(n1,fFo),e(A,mFo),e(A,s1),e(s1,ure),e(ure,gFo),e(s1,hFo),e(s1,Q$),e(Q$,pFo),e(s1,_Fo),e(A,uFo),e(A,l1),e(l1,bre),e(bre,bFo),e(l1,vFo),e(l1,H$),e(H$,TFo),e(l1,FFo),e(A,CFo),e(A,i1),e(i1,vre),e(vre,MFo),e(i1,EFo),e(i1,U$),e(U$,yFo),e(i1,wFo),e(A,AFo),e(A,d1),e(d1,Tre),e(Tre,LFo),e(d1,BFo),e(d1,J$),e(J$,kFo),e(d1,xFo),e(A,RFo),e(A,c1),e(c1,Fre),e(Fre,SFo),e(c1,PFo),e(c1,Y$),e(Y$,$Fo),e(c1,IFo),e(A,jFo),e(A,f1),e(f1,Cre),e(Cre,NFo),e(f1,DFo),e(f1,K$),e(K$,qFo),e(f1,GFo),e(A,OFo),e(A,m1),e(m1,Mre),e(Mre,XFo),e(m1,zFo),e(m1,Z$),e(Z$,VFo),e(m1,WFo),e(A,QFo),e(A,g1),e(g1,Ere),e(Ere,HFo),e(g1,UFo),e(g1,eI),e(eI,JFo),e(g1,YFo),e($e,KFo),e($e,h1),e(h1,ZFo),e(h1,yre),e(yre,eCo),e(h1,oCo),e(h1,wre),e(wre,rCo),e($e,tCo),e($e,Are),e(Are,aCo),e($e,nCo),g(wE,$e,null),b(d,z7e,u),b(d,ad,u),e(ad,p1),e(p1,Lre),g(AE,Lre,null),e(ad,sCo),e(ad,Bre),e(Bre,lCo),b(d,V7e,u),b(d,Yo,u),g(LE,Yo,null),e(Yo,iCo),e(Yo,nd),e(nd,dCo),e(nd,kre),e(kre,cCo),e(nd,fCo),e(nd,xre),e(xre,mCo),e(nd,gCo),e(Yo,hCo),e(Yo,BE),e(BE,pCo),e(BE,Rre),e(Rre,_Co),e(BE,uCo),e(Yo,bCo),e(Yo,zr),g(kE,zr,null),e(zr,vCo),e(zr,Sre),e(Sre,TCo),e(zr,FCo),e(zr,sd),e(sd,CCo),e(sd,Pre),e(Pre,MCo),e(sd,ECo),e(sd,$re),e($re,yCo),e(sd,wCo),e(zr,ACo),e(zr,Ire),e(Ire,LCo),e(zr,BCo),g(xE,zr,null),e(Yo,kCo),e(Yo,Ie),g(RE,Ie,null),e(Ie,xCo),e(Ie,jre),e(jre,RCo),e(Ie,SCo),e(Ie,Va),e(Va,PCo),e(Va,Nre),e(Nre,$Co),e(Va,ICo),e(Va,Dre),e(Dre,jCo),e(Va,NCo),e(Va,qre),e(qre,DCo),e(Va,qCo),e(Ie,GCo),e(Ie,G),e(G,_1),e(_1,Gre),e(Gre,OCo),e(_1,XCo),e(_1,oI),e(oI,zCo),e(_1,VCo),e(G,WCo),e(G,u1),e(u1,Ore),e(Ore,QCo),e(u1,HCo),e(u1,rI),e(rI,UCo),e(u1,JCo),e(G,YCo),e(G,b1),e(b1,Xre),e(Xre,KCo),e(b1,ZCo),e(b1,tI),e(tI,e4o),e(b1,o4o),e(G,r4o),e(G,v1),e(v1,zre),e(zre,t4o),e(v1,a4o),e(v1,aI),e(aI,n4o),e(v1,s4o),e(G,l4o),e(G,T1),e(T1,Vre),e(Vre,i4o),e(T1,d4o),e(T1,nI),e(nI,c4o),e(T1,f4o),e(G,m4o),e(G,F1),e(F1,Wre),e(Wre,g4o),e(F1,h4o),e(F1,sI),e(sI,p4o),e(F1,_4o),e(G,u4o),e(G,C1),e(C1,Qre),e(Qre,b4o),e(C1,v4o),e(C1,lI),e(lI,T4o),e(C1,F4o),e(G,C4o),e(G,M1),e(M1,Hre),e(Hre,M4o),e(M1,E4o),e(M1,iI),e(iI,y4o),e(M1,w4o),e(G,A4o),e(G,E1),e(E1,Ure),e(Ure,L4o),e(E1,B4o),e(E1,dI),e(dI,k4o),e(E1,x4o),e(G,R4o),e(G,y1),e(y1,Jre),e(Jre,S4o),e(y1,P4o),e(y1,cI),e(cI,$4o),e(y1,I4o),e(G,j4o),e(G,w1),e(w1,Yre),e(Yre,N4o),e(w1,D4o),e(w1,fI),e(fI,q4o),e(w1,G4o),e(G,O4o),e(G,A1),e(A1,Kre),e(Kre,X4o),e(A1,z4o),e(A1,mI),e(mI,V4o),e(A1,W4o),e(G,Q4o),e(G,L1),e(L1,Zre),e(Zre,H4o),e(L1,U4o),e(L1,gI),e(gI,J4o),e(L1,Y4o),e(G,K4o),e(G,B1),e(B1,ete),e(ete,Z4o),e(B1,eMo),e(B1,hI),e(hI,oMo),e(B1,rMo),e(G,tMo),e(G,k1),e(k1,ote),e(ote,aMo),e(k1,nMo),e(k1,pI),e(pI,sMo),e(k1,lMo),e(G,iMo),e(G,x1),e(x1,rte),e(rte,dMo),e(x1,cMo),e(x1,_I),e(_I,fMo),e(x1,mMo),e(G,gMo),e(G,R1),e(R1,tte),e(tte,hMo),e(R1,pMo),e(R1,uI),e(uI,_Mo),e(R1,uMo),e(G,bMo),e(G,S1),e(S1,ate),e(ate,vMo),e(S1,TMo),e(S1,bI),e(bI,FMo),e(S1,CMo),e(G,MMo),e(G,P1),e(P1,nte),e(nte,EMo),e(P1,yMo),e(P1,vI),e(vI,wMo),e(P1,AMo),e(G,LMo),e(G,$1),e($1,ste),e(ste,BMo),e($1,kMo),e($1,TI),e(TI,xMo),e($1,RMo),e(G,SMo),e(G,I1),e(I1,lte),e(lte,PMo),e(I1,$Mo),e(I1,FI),e(FI,IMo),e(I1,jMo),e(G,NMo),e(G,j1),e(j1,ite),e(ite,DMo),e(j1,qMo),e(j1,CI),e(CI,GMo),e(j1,OMo),e(G,XMo),e(G,N1),e(N1,dte),e(dte,zMo),e(N1,VMo),e(N1,MI),e(MI,WMo),e(N1,QMo),e(G,HMo),e(G,D1),e(D1,cte),e(cte,UMo),e(D1,JMo),e(D1,EI),e(EI,YMo),e(D1,KMo),e(G,ZMo),e(G,q1),e(q1,fte),e(fte,eEo),e(q1,oEo),e(q1,yI),e(yI,rEo),e(q1,tEo),e(G,aEo),e(G,G1),e(G1,mte),e(mte,nEo),e(G1,sEo),e(G1,wI),e(wI,lEo),e(G1,iEo),e(G,dEo),e(G,O1),e(O1,gte),e(gte,cEo),e(O1,fEo),e(O1,AI),e(AI,mEo),e(O1,gEo),e(Ie,hEo),e(Ie,X1),e(X1,pEo),e(X1,hte),e(hte,_Eo),e(X1,uEo),e(X1,pte),e(pte,bEo),e(Ie,vEo),e(Ie,_te),e(_te,TEo),e(Ie,FEo),g(SE,Ie,null),b(d,W7e,u),b(d,ld,u),e(ld,z1),e(z1,ute),g(PE,ute,null),e(ld,CEo),e(ld,bte),e(bte,MEo),b(d,Q7e,u),b(d,Ko,u),g($E,Ko,null),e(Ko,EEo),e(Ko,id),e(id,yEo),e(id,vte),e(vte,wEo),e(id,AEo),e(id,Tte),e(Tte,LEo),e(id,BEo),e(Ko,kEo),e(Ko,IE),e(IE,xEo),e(IE,Fte),e(Fte,REo),e(IE,SEo),e(Ko,PEo),e(Ko,Vr),g(jE,Vr,null),e(Vr,$Eo),e(Vr,Cte),e(Cte,IEo),e(Vr,jEo),e(Vr,dd),e(dd,NEo),e(dd,Mte),e(Mte,DEo),e(dd,qEo),e(dd,Ete),e(Ete,GEo),e(dd,OEo),e(Vr,XEo),e(Vr,yte),e(yte,zEo),e(Vr,VEo),g(NE,Vr,null),e(Ko,WEo),e(Ko,je),g(DE,je,null),e(je,QEo),e(je,wte),e(wte,HEo),e(je,UEo),e(je,Wa),e(Wa,JEo),e(Wa,Ate),e(Ate,YEo),e(Wa,KEo),e(Wa,Lte),e(Lte,ZEo),e(Wa,e3o),e(Wa,Bte),e(Bte,o3o),e(Wa,r3o),e(je,t3o),e(je,na),e(na,V1),e(V1,kte),e(kte,a3o),e(V1,n3o),e(V1,LI),e(LI,s3o),e(V1,l3o),e(na,i3o),e(na,W1),e(W1,xte),e(xte,d3o),e(W1,c3o),e(W1,BI),e(BI,f3o),e(W1,m3o),e(na,g3o),e(na,Q1),e(Q1,Rte),e(Rte,h3o),e(Q1,p3o),e(Q1,kI),e(kI,_3o),e(Q1,u3o),e(na,b3o),e(na,H1),e(H1,Ste),e(Ste,v3o),e(H1,T3o),e(H1,xI),e(xI,F3o),e(H1,C3o),e(na,M3o),e(na,U1),e(U1,Pte),e(Pte,E3o),e(U1,y3o),e(U1,RI),e(RI,w3o),e(U1,A3o),e(je,L3o),e(je,J1),e(J1,B3o),e(J1,$te),e($te,k3o),e(J1,x3o),e(J1,Ite),e(Ite,R3o),e(je,S3o),e(je,jte),e(jte,P3o),e(je,$3o),g(qE,je,null),b(d,H7e,u),b(d,cd,u),e(cd,Y1),e(Y1,Nte),g(GE,Nte,null),e(cd,I3o),e(cd,Dte),e(Dte,j3o),b(d,U7e,u),b(d,Zo,u),g(OE,Zo,null),e(Zo,N3o),e(Zo,fd),e(fd,D3o),e(fd,qte),e(qte,q3o),e(fd,G3o),e(fd,Gte),e(Gte,O3o),e(fd,X3o),e(Zo,z3o),e(Zo,XE),e(XE,V3o),e(XE,Ote),e(Ote,W3o),e(XE,Q3o),e(Zo,H3o),e(Zo,Wr),g(zE,Wr,null),e(Wr,U3o),e(Wr,Xte),e(Xte,J3o),e(Wr,Y3o),e(Wr,md),e(md,K3o),e(md,zte),e(zte,Z3o),e(md,eyo),e(md,Vte),e(Vte,oyo),e(md,ryo),e(Wr,tyo),e(Wr,Wte),e(Wte,ayo),e(Wr,nyo),g(VE,Wr,null),e(Zo,syo),e(Zo,Ne),g(WE,Ne,null),e(Ne,lyo),e(Ne,Qte),e(Qte,iyo),e(Ne,dyo),e(Ne,Qa),e(Qa,cyo),e(Qa,Hte),e(Hte,fyo),e(Qa,myo),e(Qa,Ute),e(Ute,gyo),e(Qa,hyo),e(Qa,Jte),e(Jte,pyo),e(Qa,_yo),e(Ne,uyo),e(Ne,D),e(D,K1),e(K1,Yte),e(Yte,byo),e(K1,vyo),e(K1,SI),e(SI,Tyo),e(K1,Fyo),e(D,Cyo),e(D,Z1),e(Z1,Kte),e(Kte,Myo),e(Z1,Eyo),e(Z1,PI),e(PI,yyo),e(Z1,wyo),e(D,Ayo),e(D,eb),e(eb,Zte),e(Zte,Lyo),e(eb,Byo),e(eb,$I),e($I,kyo),e(eb,xyo),e(D,Ryo),e(D,ob),e(ob,eae),e(eae,Syo),e(ob,Pyo),e(ob,II),e(II,$yo),e(ob,Iyo),e(D,jyo),e(D,rb),e(rb,oae),e(oae,Nyo),e(rb,Dyo),e(rb,jI),e(jI,qyo),e(rb,Gyo),e(D,Oyo),e(D,tb),e(tb,rae),e(rae,Xyo),e(tb,zyo),e(tb,NI),e(NI,Vyo),e(tb,Wyo),e(D,Qyo),e(D,ab),e(ab,tae),e(tae,Hyo),e(ab,Uyo),e(ab,DI),e(DI,Jyo),e(ab,Yyo),e(D,Kyo),e(D,nb),e(nb,aae),e(aae,Zyo),e(nb,ewo),e(nb,qI),e(qI,owo),e(nb,rwo),e(D,two),e(D,sb),e(sb,nae),e(nae,awo),e(sb,nwo),e(sb,GI),e(GI,swo),e(sb,lwo),e(D,iwo),e(D,lb),e(lb,sae),e(sae,dwo),e(lb,cwo),e(lb,OI),e(OI,fwo),e(lb,mwo),e(D,gwo),e(D,ib),e(ib,lae),e(lae,hwo),e(ib,pwo),e(ib,XI),e(XI,_wo),e(ib,uwo),e(D,bwo),e(D,db),e(db,iae),e(iae,vwo),e(db,Two),e(db,zI),e(zI,Fwo),e(db,Cwo),e(D,Mwo),e(D,cb),e(cb,dae),e(dae,Ewo),e(cb,ywo),e(cb,VI),e(VI,wwo),e(cb,Awo),e(D,Lwo),e(D,fb),e(fb,cae),e(cae,Bwo),e(fb,kwo),e(fb,WI),e(WI,xwo),e(fb,Rwo),e(D,Swo),e(D,mb),e(mb,fae),e(fae,Pwo),e(mb,$wo),e(mb,QI),e(QI,Iwo),e(mb,jwo),e(D,Nwo),e(D,gb),e(gb,mae),e(mae,Dwo),e(gb,qwo),e(gb,HI),e(HI,Gwo),e(gb,Owo),e(D,Xwo),e(D,hb),e(hb,gae),e(gae,zwo),e(hb,Vwo),e(hb,UI),e(UI,Wwo),e(hb,Qwo),e(D,Hwo),e(D,pb),e(pb,hae),e(hae,Uwo),e(pb,Jwo),e(pb,JI),e(JI,Ywo),e(pb,Kwo),e(D,Zwo),e(D,_b),e(_b,pae),e(pae,eAo),e(_b,oAo),e(_b,YI),e(YI,rAo),e(_b,tAo),e(D,aAo),e(D,ub),e(ub,_ae),e(_ae,nAo),e(ub,sAo),e(ub,KI),e(KI,lAo),e(ub,iAo),e(D,dAo),e(D,bb),e(bb,uae),e(uae,cAo),e(bb,fAo),e(bb,ZI),e(ZI,mAo),e(bb,gAo),e(D,hAo),e(D,vb),e(vb,bae),e(bae,pAo),e(vb,_Ao),e(vb,ej),e(ej,uAo),e(vb,bAo),e(D,vAo),e(D,Tb),e(Tb,vae),e(vae,TAo),e(Tb,FAo),e(Tb,oj),e(oj,CAo),e(Tb,MAo),e(D,EAo),e(D,Fb),e(Fb,Tae),e(Tae,yAo),e(Fb,wAo),e(Fb,rj),e(rj,AAo),e(Fb,LAo),e(D,BAo),e(D,Cb),e(Cb,Fae),e(Fae,kAo),e(Cb,xAo),e(Cb,tj),e(tj,RAo),e(Cb,SAo),e(D,PAo),e(D,Mb),e(Mb,Cae),e(Cae,$Ao),e(Mb,IAo),e(Mb,aj),e(aj,jAo),e(Mb,NAo),e(D,DAo),e(D,Eb),e(Eb,Mae),e(Mae,qAo),e(Eb,GAo),e(Eb,nj),e(nj,OAo),e(Eb,XAo),e(D,zAo),e(D,yb),e(yb,Eae),e(Eae,VAo),e(yb,WAo),e(yb,sj),e(sj,QAo),e(yb,HAo),e(D,UAo),e(D,wb),e(wb,yae),e(yae,JAo),e(wb,YAo),e(wb,lj),e(lj,KAo),e(wb,ZAo),e(D,e0o),e(D,Ab),e(Ab,wae),e(wae,o0o),e(Ab,r0o),e(Ab,ij),e(ij,t0o),e(Ab,a0o),e(D,n0o),e(D,Lb),e(Lb,Aae),e(Aae,s0o),e(Lb,l0o),e(Lb,dj),e(dj,i0o),e(Lb,d0o),e(D,c0o),e(D,Bb),e(Bb,Lae),e(Lae,f0o),e(Bb,m0o),e(Bb,cj),e(cj,g0o),e(Bb,h0o),e(Ne,p0o),e(Ne,kb),e(kb,_0o),e(kb,Bae),e(Bae,u0o),e(kb,b0o),e(kb,kae),e(kae,v0o),e(Ne,T0o),e(Ne,xae),e(xae,F0o),e(Ne,C0o),g(QE,Ne,null),b(d,J7e,u),b(d,gd,u),e(gd,xb),e(xb,Rae),g(HE,Rae,null),e(gd,M0o),e(gd,Sae),e(Sae,E0o),b(d,Y7e,u),b(d,er,u),g(UE,er,null),e(er,y0o),e(er,hd),e(hd,w0o),e(hd,Pae),e(Pae,A0o),e(hd,L0o),e(hd,$ae),e($ae,B0o),e(hd,k0o),e(er,x0o),e(er,JE),e(JE,R0o),e(JE,Iae),e(Iae,S0o),e(JE,P0o),e(er,$0o),e(er,Qr),g(YE,Qr,null),e(Qr,I0o),e(Qr,jae),e(jae,j0o),e(Qr,N0o),e(Qr,pd),e(pd,D0o),e(pd,Nae),e(Nae,q0o),e(pd,G0o),e(pd,Dae),e(Dae,O0o),e(pd,X0o),e(Qr,z0o),e(Qr,qae),e(qae,V0o),e(Qr,W0o),g(KE,Qr,null),e(er,Q0o),e(er,De),g(ZE,De,null),e(De,H0o),e(De,Gae),e(Gae,U0o),e(De,J0o),e(De,Ha),e(Ha,Y0o),e(Ha,Oae),e(Oae,K0o),e(Ha,Z0o),e(Ha,Xae),e(Xae,eLo),e(Ha,oLo),e(Ha,zae),e(zae,rLo),e(Ha,tLo),e(De,aLo),e(De,R),e(R,Rb),e(Rb,Vae),e(Vae,nLo),e(Rb,sLo),e(Rb,fj),e(fj,lLo),e(Rb,iLo),e(R,dLo),e(R,Sb),e(Sb,Wae),e(Wae,cLo),e(Sb,fLo),e(Sb,mj),e(mj,mLo),e(Sb,gLo),e(R,hLo),e(R,Pb),e(Pb,Qae),e(Qae,pLo),e(Pb,_Lo),e(Pb,gj),e(gj,uLo),e(Pb,bLo),e(R,vLo),e(R,$b),e($b,Hae),e(Hae,TLo),e($b,FLo),e($b,hj),e(hj,CLo),e($b,MLo),e(R,ELo),e(R,Ib),e(Ib,Uae),e(Uae,yLo),e(Ib,wLo),e(Ib,pj),e(pj,ALo),e(Ib,LLo),e(R,BLo),e(R,jb),e(jb,Jae),e(Jae,kLo),e(jb,xLo),e(jb,_j),e(_j,RLo),e(jb,SLo),e(R,PLo),e(R,Nb),e(Nb,Yae),e(Yae,$Lo),e(Nb,ILo),e(Nb,uj),e(uj,jLo),e(Nb,NLo),e(R,DLo),e(R,Db),e(Db,Kae),e(Kae,qLo),e(Db,GLo),e(Db,bj),e(bj,OLo),e(Db,XLo),e(R,zLo),e(R,qb),e(qb,Zae),e(Zae,VLo),e(qb,WLo),e(qb,vj),e(vj,QLo),e(qb,HLo),e(R,ULo),e(R,Gb),e(Gb,ene),e(ene,JLo),e(Gb,YLo),e(Gb,Tj),e(Tj,KLo),e(Gb,ZLo),e(R,e7o),e(R,Ob),e(Ob,one),e(one,o7o),e(Ob,r7o),e(Ob,Fj),e(Fj,t7o),e(Ob,a7o),e(R,n7o),e(R,Xb),e(Xb,rne),e(rne,s7o),e(Xb,l7o),e(Xb,Cj),e(Cj,i7o),e(Xb,d7o),e(R,c7o),e(R,zb),e(zb,tne),e(tne,f7o),e(zb,m7o),e(zb,Mj),e(Mj,g7o),e(zb,h7o),e(R,p7o),e(R,Vb),e(Vb,ane),e(ane,_7o),e(Vb,u7o),e(Vb,Ej),e(Ej,b7o),e(Vb,v7o),e(R,T7o),e(R,Wb),e(Wb,nne),e(nne,F7o),e(Wb,C7o),e(Wb,yj),e(yj,M7o),e(Wb,E7o),e(R,y7o),e(R,Qb),e(Qb,sne),e(sne,w7o),e(Qb,A7o),e(Qb,wj),e(wj,L7o),e(Qb,B7o),e(R,k7o),e(R,Hb),e(Hb,lne),e(lne,x7o),e(Hb,R7o),e(Hb,Aj),e(Aj,S7o),e(Hb,P7o),e(R,$7o),e(R,Ub),e(Ub,ine),e(ine,I7o),e(Ub,j7o),e(Ub,Lj),e(Lj,N7o),e(Ub,D7o),e(R,q7o),e(R,Jb),e(Jb,dne),e(dne,G7o),e(Jb,O7o),e(Jb,Bj),e(Bj,X7o),e(Jb,z7o),e(R,V7o),e(R,Yb),e(Yb,cne),e(cne,W7o),e(Yb,Q7o),e(Yb,kj),e(kj,H7o),e(Yb,U7o),e(R,J7o),e(R,Kb),e(Kb,fne),e(fne,Y7o),e(Kb,K7o),e(Kb,xj),e(xj,Z7o),e(Kb,e9o),e(R,o9o),e(R,Zb),e(Zb,mne),e(mne,r9o),e(Zb,t9o),e(Zb,Rj),e(Rj,a9o),e(Zb,n9o),e(R,s9o),e(R,e5),e(e5,gne),e(gne,l9o),e(e5,i9o),e(e5,Sj),e(Sj,d9o),e(e5,c9o),e(R,f9o),e(R,o5),e(o5,hne),e(hne,m9o),e(o5,g9o),e(o5,Pj),e(Pj,h9o),e(o5,p9o),e(R,_9o),e(R,r5),e(r5,pne),e(pne,u9o),e(r5,b9o),e(r5,$j),e($j,v9o),e(r5,T9o),e(R,F9o),e(R,t5),e(t5,_ne),e(_ne,C9o),e(t5,M9o),e(t5,Ij),e(Ij,E9o),e(t5,y9o),e(R,w9o),e(R,a5),e(a5,une),e(une,A9o),e(a5,L9o),e(a5,jj),e(jj,B9o),e(a5,k9o),e(R,x9o),e(R,n5),e(n5,bne),e(bne,R9o),e(n5,S9o),e(n5,Nj),e(Nj,P9o),e(n5,$9o),e(R,I9o),e(R,s5),e(s5,vne),e(vne,j9o),e(s5,N9o),e(s5,Dj),e(Dj,D9o),e(s5,q9o),e(R,G9o),e(R,l5),e(l5,Tne),e(Tne,O9o),e(l5,X9o),e(l5,qj),e(qj,z9o),e(l5,V9o),e(R,W9o),e(R,i5),e(i5,Fne),e(Fne,Q9o),e(i5,H9o),e(i5,Gj),e(Gj,U9o),e(i5,J9o),e(R,Y9o),e(R,d5),e(d5,Cne),e(Cne,K9o),e(d5,Z9o),e(d5,Oj),e(Oj,eBo),e(d5,oBo),e(R,rBo),e(R,c5),e(c5,Mne),e(Mne,tBo),e(c5,aBo),e(c5,Xj),e(Xj,nBo),e(c5,sBo),e(R,lBo),e(R,f5),e(f5,Ene),e(Ene,iBo),e(f5,dBo),e(f5,zj),e(zj,cBo),e(f5,fBo),e(R,mBo),e(R,m5),e(m5,yne),e(yne,gBo),e(m5,hBo),e(m5,Vj),e(Vj,pBo),e(m5,_Bo),e(R,uBo),e(R,g5),e(g5,wne),e(wne,bBo),e(g5,vBo),e(g5,Wj),e(Wj,TBo),e(g5,FBo),e(R,CBo),e(R,h5),e(h5,Ane),e(Ane,MBo),e(h5,EBo),e(h5,Qj),e(Qj,yBo),e(h5,wBo),e(R,ABo),e(R,p5),e(p5,Lne),e(Lne,LBo),e(p5,BBo),e(p5,Hj),e(Hj,kBo),e(p5,xBo),e(De,RBo),e(De,_5),e(_5,SBo),e(_5,Bne),e(Bne,PBo),e(_5,$Bo),e(_5,kne),e(kne,IBo),e(De,jBo),e(De,xne),e(xne,NBo),e(De,DBo),g(e3,De,null),b(d,K7e,u),b(d,_d,u),e(_d,u5),e(u5,Rne),g(o3,Rne,null),e(_d,qBo),e(_d,Sne),e(Sne,GBo),b(d,Z7e,u),b(d,or,u),g(r3,or,null),e(or,OBo),e(or,ud),e(ud,XBo),e(ud,Pne),e(Pne,zBo),e(ud,VBo),e(ud,$ne),e($ne,WBo),e(ud,QBo),e(or,HBo),e(or,t3),e(t3,UBo),e(t3,Ine),e(Ine,JBo),e(t3,YBo),e(or,KBo),e(or,Hr),g(a3,Hr,null),e(Hr,ZBo),e(Hr,jne),e(jne,eko),e(Hr,oko),e(Hr,bd),e(bd,rko),e(bd,Nne),e(Nne,tko),e(bd,ako),e(bd,Dne),e(Dne,nko),e(bd,sko),e(Hr,lko),e(Hr,qne),e(qne,iko),e(Hr,dko),g(n3,Hr,null),e(or,cko),e(or,qe),g(s3,qe,null),e(qe,fko),e(qe,Gne),e(Gne,mko),e(qe,gko),e(qe,Ua),e(Ua,hko),e(Ua,One),e(One,pko),e(Ua,_ko),e(Ua,Xne),e(Xne,uko),e(Ua,bko),e(Ua,zne),e(zne,vko),e(Ua,Tko),e(qe,Fko),e(qe,Vne),e(Vne,b5),e(b5,Wne),e(Wne,Cko),e(b5,Mko),e(b5,Uj),e(Uj,Eko),e(b5,yko),e(qe,wko),e(qe,v5),e(v5,Ako),e(v5,Qne),e(Qne,Lko),e(v5,Bko),e(v5,Hne),e(Hne,kko),e(qe,xko),e(qe,Une),e(Une,Rko),e(qe,Sko),g(l3,qe,null),b(d,e9e,u),b(d,vd,u),e(vd,T5),e(T5,Jne),g(i3,Jne,null),e(vd,Pko),e(vd,Yne),e(Yne,$ko),b(d,o9e,u),b(d,rr,u),g(d3,rr,null),e(rr,Iko),e(rr,Td),e(Td,jko),e(Td,Kne),e(Kne,Nko),e(Td,Dko),e(Td,Zne),e(Zne,qko),e(Td,Gko),e(rr,Oko),e(rr,c3),e(c3,Xko),e(c3,ese),e(ese,zko),e(c3,Vko),e(rr,Wko),e(rr,Ur),g(f3,Ur,null),e(Ur,Qko),e(Ur,ose),e(ose,Hko),e(Ur,Uko),e(Ur,Fd),e(Fd,Jko),e(Fd,rse),e(rse,Yko),e(Fd,Kko),e(Fd,tse),e(tse,Zko),e(Fd,exo),e(Ur,oxo),e(Ur,ase),e(ase,rxo),e(Ur,txo),g(m3,Ur,null),e(rr,axo),e(rr,Ge),g(g3,Ge,null),e(Ge,nxo),e(Ge,nse),e(nse,sxo),e(Ge,lxo),e(Ge,Ja),e(Ja,ixo),e(Ja,sse),e(sse,dxo),e(Ja,cxo),e(Ja,lse),e(lse,fxo),e(Ja,mxo),e(Ja,ise),e(ise,gxo),e(Ja,hxo),e(Ge,pxo),e(Ge,be),e(be,F5),e(F5,dse),e(dse,_xo),e(F5,uxo),e(F5,Jj),e(Jj,bxo),e(F5,vxo),e(be,Txo),e(be,C5),e(C5,cse),e(cse,Fxo),e(C5,Cxo),e(C5,Yj),e(Yj,Mxo),e(C5,Exo),e(be,yxo),e(be,Rs),e(Rs,fse),e(fse,wxo),e(Rs,Axo),e(Rs,Kj),e(Kj,Lxo),e(Rs,Bxo),e(Rs,Zj),e(Zj,kxo),e(Rs,xxo),e(be,Rxo),e(be,M5),e(M5,mse),e(mse,Sxo),e(M5,Pxo),e(M5,eN),e(eN,$xo),e(M5,Ixo),e(be,jxo),e(be,la),e(la,gse),e(gse,Nxo),e(la,Dxo),e(la,oN),e(oN,qxo),e(la,Gxo),e(la,rN),e(rN,Oxo),e(la,Xxo),e(la,tN),e(tN,zxo),e(la,Vxo),e(be,Wxo),e(be,E5),e(E5,hse),e(hse,Qxo),e(E5,Hxo),e(E5,aN),e(aN,Uxo),e(E5,Jxo),e(be,Yxo),e(be,y5),e(y5,pse),e(pse,Kxo),e(y5,Zxo),e(y5,nN),e(nN,eRo),e(y5,oRo),e(be,rRo),e(be,w5),e(w5,_se),e(_se,tRo),e(w5,aRo),e(w5,sN),e(sN,nRo),e(w5,sRo),e(be,lRo),e(be,A5),e(A5,use),e(use,iRo),e(A5,dRo),e(A5,lN),e(lN,cRo),e(A5,fRo),e(Ge,mRo),e(Ge,L5),e(L5,gRo),e(L5,bse),e(bse,hRo),e(L5,pRo),e(L5,vse),e(vse,_Ro),e(Ge,uRo),e(Ge,Tse),e(Tse,bRo),e(Ge,vRo),g(h3,Ge,null),b(d,r9e,u),b(d,Cd,u),e(Cd,B5),e(B5,Fse),g(p3,Fse,null),e(Cd,TRo),e(Cd,Cse),e(Cse,FRo),b(d,t9e,u),b(d,tr,u),g(_3,tr,null),e(tr,CRo),e(tr,Md),e(Md,MRo),e(Md,Mse),e(Mse,ERo),e(Md,yRo),e(Md,Ese),e(Ese,wRo),e(Md,ARo),e(tr,LRo),e(tr,u3),e(u3,BRo),e(u3,yse),e(yse,kRo),e(u3,xRo),e(tr,RRo),e(tr,Jr),g(b3,Jr,null),e(Jr,SRo),e(Jr,wse),e(wse,PRo),e(Jr,$Ro),e(Jr,Ed),e(Ed,IRo),e(Ed,Ase),e(Ase,jRo),e(Ed,NRo),e(Ed,Lse),e(Lse,DRo),e(Ed,qRo),e(Jr,GRo),e(Jr,Bse),e(Bse,ORo),e(Jr,XRo),g(v3,Jr,null),e(tr,zRo),e(tr,Oe),g(T3,Oe,null),e(Oe,VRo),e(Oe,kse),e(kse,WRo),e(Oe,QRo),e(Oe,Ya),e(Ya,HRo),e(Ya,xse),e(xse,URo),e(Ya,JRo),e(Ya,Rse),e(Rse,YRo),e(Ya,KRo),e(Ya,Sse),e(Sse,ZRo),e(Ya,eSo),e(Oe,oSo),e(Oe,Pse),e(Pse,k5),e(k5,$se),e($se,rSo),e(k5,tSo),e(k5,iN),e(iN,aSo),e(k5,nSo),e(Oe,sSo),e(Oe,x5),e(x5,lSo),e(x5,Ise),e(Ise,iSo),e(x5,dSo),e(x5,jse),e(jse,cSo),e(Oe,fSo),e(Oe,Nse),e(Nse,mSo),e(Oe,gSo),g(F3,Oe,null),b(d,a9e,u),b(d,yd,u),e(yd,R5),e(R5,Dse),g(C3,Dse,null),e(yd,hSo),e(yd,qse),e(qse,pSo),b(d,n9e,u),b(d,ar,u),g(M3,ar,null),e(ar,_So),e(ar,wd),e(wd,uSo),e(wd,Gse),e(Gse,bSo),e(wd,vSo),e(wd,Ose),e(Ose,TSo),e(wd,FSo),e(ar,CSo),e(ar,E3),e(E3,MSo),e(E3,Xse),e(Xse,ESo),e(E3,ySo),e(ar,wSo),e(ar,Yr),g(y3,Yr,null),e(Yr,ASo),e(Yr,zse),e(zse,LSo),e(Yr,BSo),e(Yr,Ad),e(Ad,kSo),e(Ad,Vse),e(Vse,xSo),e(Ad,RSo),e(Ad,Wse),e(Wse,SSo),e(Ad,PSo),e(Yr,$So),e(Yr,Qse),e(Qse,ISo),e(Yr,jSo),g(w3,Yr,null),e(ar,NSo),e(ar,Xe),g(A3,Xe,null),e(Xe,DSo),e(Xe,Hse),e(Hse,qSo),e(Xe,GSo),e(Xe,Ka),e(Ka,OSo),e(Ka,Use),e(Use,XSo),e(Ka,zSo),e(Ka,Jse),e(Jse,VSo),e(Ka,WSo),e(Ka,Yse),e(Yse,QSo),e(Ka,HSo),e(Xe,USo),e(Xe,ao),e(ao,S5),e(S5,Kse),e(Kse,JSo),e(S5,YSo),e(S5,dN),e(dN,KSo),e(S5,ZSo),e(ao,ePo),e(ao,P5),e(P5,Zse),e(Zse,oPo),e(P5,rPo),e(P5,cN),e(cN,tPo),e(P5,aPo),e(ao,nPo),e(ao,$5),e($5,ele),e(ele,sPo),e($5,lPo),e($5,fN),e(fN,iPo),e($5,dPo),e(ao,cPo),e(ao,I5),e(I5,ole),e(ole,fPo),e(I5,mPo),e(I5,mN),e(mN,gPo),e(I5,hPo),e(ao,pPo),e(ao,j5),e(j5,rle),e(rle,_Po),e(j5,uPo),e(j5,gN),e(gN,bPo),e(j5,vPo),e(ao,TPo),e(ao,N5),e(N5,tle),e(tle,FPo),e(N5,CPo),e(N5,hN),e(hN,MPo),e(N5,EPo),e(ao,yPo),e(ao,D5),e(D5,ale),e(ale,wPo),e(D5,APo),e(D5,pN),e(pN,LPo),e(D5,BPo),e(Xe,kPo),e(Xe,q5),e(q5,xPo),e(q5,nle),e(nle,RPo),e(q5,SPo),e(q5,sle),e(sle,PPo),e(Xe,$Po),e(Xe,lle),e(lle,IPo),e(Xe,jPo),g(L3,Xe,null),b(d,s9e,u),b(d,Ld,u),e(Ld,G5),e(G5,ile),g(B3,ile,null),e(Ld,NPo),e(Ld,dle),e(dle,DPo),b(d,l9e,u),b(d,nr,u),g(k3,nr,null),e(nr,qPo),e(nr,Bd),e(Bd,GPo),e(Bd,cle),e(cle,OPo),e(Bd,XPo),e(Bd,fle),e(fle,zPo),e(Bd,VPo),e(nr,WPo),e(nr,x3),e(x3,QPo),e(x3,mle),e(mle,HPo),e(x3,UPo),e(nr,JPo),e(nr,Kr),g(R3,Kr,null),e(Kr,YPo),e(Kr,gle),e(gle,KPo),e(Kr,ZPo),e(Kr,kd),e(kd,e$o),e(kd,hle),e(hle,o$o),e(kd,r$o),e(kd,ple),e(ple,t$o),e(kd,a$o),e(Kr,n$o),e(Kr,_le),e(_le,s$o),e(Kr,l$o),g(S3,Kr,null),e(nr,i$o),e(nr,ze),g(P3,ze,null),e(ze,d$o),e(ze,ule),e(ule,c$o),e(ze,f$o),e(ze,Za),e(Za,m$o),e(Za,ble),e(ble,g$o),e(Za,h$o),e(Za,vle),e(vle,p$o),e(Za,_$o),e(Za,Tle),e(Tle,u$o),e(Za,b$o),e(ze,v$o),e(ze,xd),e(xd,O5),e(O5,Fle),e(Fle,T$o),e(O5,F$o),e(O5,_N),e(_N,C$o),e(O5,M$o),e(xd,E$o),e(xd,X5),e(X5,Cle),e(Cle,y$o),e(X5,w$o),e(X5,uN),e(uN,A$o),e(X5,L$o),e(xd,B$o),e(xd,z5),e(z5,Mle),e(Mle,k$o),e(z5,x$o),e(z5,bN),e(bN,R$o),e(z5,S$o),e(ze,P$o),e(ze,V5),e(V5,$$o),e(V5,Ele),e(Ele,I$o),e(V5,j$o),e(V5,yle),e(yle,N$o),e(ze,D$o),e(ze,wle),e(wle,q$o),e(ze,G$o),g($3,ze,null),b(d,i9e,u),b(d,Rd,u),e(Rd,W5),e(W5,Ale),g(I3,Ale,null),e(Rd,O$o),e(Rd,Lle),e(Lle,X$o),b(d,d9e,u),b(d,sr,u),g(j3,sr,null),e(sr,z$o),e(sr,Sd),e(Sd,V$o),e(Sd,Ble),e(Ble,W$o),e(Sd,Q$o),e(Sd,kle),e(kle,H$o),e(Sd,U$o),e(sr,J$o),e(sr,N3),e(N3,Y$o),e(N3,xle),e(xle,K$o),e(N3,Z$o),e(sr,eIo),e(sr,Zr),g(D3,Zr,null),e(Zr,oIo),e(Zr,Rle),e(Rle,rIo),e(Zr,tIo),e(Zr,Pd),e(Pd,aIo),e(Pd,Sle),e(Sle,nIo),e(Pd,sIo),e(Pd,Ple),e(Ple,lIo),e(Pd,iIo),e(Zr,dIo),e(Zr,$le),e($le,cIo),e(Zr,fIo),g(q3,Zr,null),e(sr,mIo),e(sr,Ve),g(G3,Ve,null),e(Ve,gIo),e(Ve,Ile),e(Ile,hIo),e(Ve,pIo),e(Ve,en),e(en,_Io),e(en,jle),e(jle,uIo),e(en,bIo),e(en,Nle),e(Nle,vIo),e(en,TIo),e(en,Dle),e(Dle,FIo),e(en,CIo),e(Ve,MIo),e(Ve,no),e(no,Q5),e(Q5,qle),e(qle,EIo),e(Q5,yIo),e(Q5,vN),e(vN,wIo),e(Q5,AIo),e(no,LIo),e(no,H5),e(H5,Gle),e(Gle,BIo),e(H5,kIo),e(H5,TN),e(TN,xIo),e(H5,RIo),e(no,SIo),e(no,U5),e(U5,Ole),e(Ole,PIo),e(U5,$Io),e(U5,FN),e(FN,IIo),e(U5,jIo),e(no,NIo),e(no,J5),e(J5,Xle),e(Xle,DIo),e(J5,qIo),e(J5,CN),e(CN,GIo),e(J5,OIo),e(no,XIo),e(no,Y5),e(Y5,zle),e(zle,zIo),e(Y5,VIo),e(Y5,MN),e(MN,WIo),e(Y5,QIo),e(no,HIo),e(no,K5),e(K5,Vle),e(Vle,UIo),e(K5,JIo),e(K5,EN),e(EN,YIo),e(K5,KIo),e(no,ZIo),e(no,Z5),e(Z5,Wle),e(Wle,ejo),e(Z5,ojo),e(Z5,yN),e(yN,rjo),e(Z5,tjo),e(Ve,ajo),e(Ve,ev),e(ev,njo),e(ev,Qle),e(Qle,sjo),e(ev,ljo),e(ev,Hle),e(Hle,ijo),e(Ve,djo),e(Ve,Ule),e(Ule,cjo),e(Ve,fjo),g(O3,Ve,null),b(d,c9e,u),b(d,$d,u),e($d,ov),e(ov,Jle),g(X3,Jle,null),e($d,mjo),e($d,Yle),e(Yle,gjo),b(d,f9e,u),b(d,lr,u),g(z3,lr,null),e(lr,hjo),e(lr,Id),e(Id,pjo),e(Id,Kle),e(Kle,_jo),e(Id,ujo),e(Id,Zle),e(Zle,bjo),e(Id,vjo),e(lr,Tjo),e(lr,V3),e(V3,Fjo),e(V3,eie),e(eie,Cjo),e(V3,Mjo),e(lr,Ejo),e(lr,et),g(W3,et,null),e(et,yjo),e(et,oie),e(oie,wjo),e(et,Ajo),e(et,jd),e(jd,Ljo),e(jd,rie),e(rie,Bjo),e(jd,kjo),e(jd,tie),e(tie,xjo),e(jd,Rjo),e(et,Sjo),e(et,aie),e(aie,Pjo),e(et,$jo),g(Q3,et,null),e(lr,Ijo),e(lr,We),g(H3,We,null),e(We,jjo),e(We,nie),e(nie,Njo),e(We,Djo),e(We,on),e(on,qjo),e(on,sie),e(sie,Gjo),e(on,Ojo),e(on,lie),e(lie,Xjo),e(on,zjo),e(on,iie),e(iie,Vjo),e(on,Wjo),e(We,Qjo),e(We,U3),e(U3,rv),e(rv,die),e(die,Hjo),e(rv,Ujo),e(rv,wN),e(wN,Jjo),e(rv,Yjo),e(U3,Kjo),e(U3,tv),e(tv,cie),e(cie,Zjo),e(tv,eNo),e(tv,AN),e(AN,oNo),e(tv,rNo),e(We,tNo),e(We,av),e(av,aNo),e(av,fie),e(fie,nNo),e(av,sNo),e(av,mie),e(mie,lNo),e(We,iNo),e(We,gie),e(gie,dNo),e(We,cNo),g(J3,We,null),b(d,m9e,u),b(d,Nd,u),e(Nd,nv),e(nv,hie),g(Y3,hie,null),e(Nd,fNo),e(Nd,pie),e(pie,mNo),b(d,g9e,u),b(d,ir,u),g(K3,ir,null),e(ir,gNo),e(ir,Dd),e(Dd,hNo),e(Dd,_ie),e(_ie,pNo),e(Dd,_No),e(Dd,uie),e(uie,uNo),e(Dd,bNo),e(ir,vNo),e(ir,Z3),e(Z3,TNo),e(Z3,bie),e(bie,FNo),e(Z3,CNo),e(ir,MNo),e(ir,ot),g(ey,ot,null),e(ot,ENo),e(ot,vie),e(vie,yNo),e(ot,wNo),e(ot,qd),e(qd,ANo),e(qd,Tie),e(Tie,LNo),e(qd,BNo),e(qd,Fie),e(Fie,kNo),e(qd,xNo),e(ot,RNo),e(ot,Cie),e(Cie,SNo),e(ot,PNo),g(oy,ot,null),e(ir,$No),e(ir,Qe),g(ry,Qe,null),e(Qe,INo),e(Qe,Mie),e(Mie,jNo),e(Qe,NNo),e(Qe,rn),e(rn,DNo),e(rn,Eie),e(Eie,qNo),e(rn,GNo),e(rn,yie),e(yie,ONo),e(rn,XNo),e(rn,wie),e(wie,zNo),e(rn,VNo),e(Qe,WNo),e(Qe,Gd),e(Gd,sv),e(sv,Aie),e(Aie,QNo),e(sv,HNo),e(sv,LN),e(LN,UNo),e(sv,JNo),e(Gd,YNo),e(Gd,lv),e(lv,Lie),e(Lie,KNo),e(lv,ZNo),e(lv,BN),e(BN,eDo),e(lv,oDo),e(Gd,rDo),e(Gd,iv),e(iv,Bie),e(Bie,tDo),e(iv,aDo),e(iv,kN),e(kN,nDo),e(iv,sDo),e(Qe,lDo),e(Qe,dv),e(dv,iDo),e(dv,kie),e(kie,dDo),e(dv,cDo),e(dv,xie),e(xie,fDo),e(Qe,mDo),e(Qe,Rie),e(Rie,gDo),e(Qe,hDo),g(ty,Qe,null),b(d,h9e,u),b(d,Od,u),e(Od,cv),e(cv,Sie),g(ay,Sie,null),e(Od,pDo),e(Od,Pie),e(Pie,_Do),b(d,p9e,u),b(d,dr,u),g(ny,dr,null),e(dr,uDo),e(dr,Xd),e(Xd,bDo),e(Xd,$ie),e($ie,vDo),e(Xd,TDo),e(Xd,Iie),e(Iie,FDo),e(Xd,CDo),e(dr,MDo),e(dr,sy),e(sy,EDo),e(sy,jie),e(jie,yDo),e(sy,wDo),e(dr,ADo),e(dr,rt),g(ly,rt,null),e(rt,LDo),e(rt,Nie),e(Nie,BDo),e(rt,kDo),e(rt,zd),e(zd,xDo),e(zd,Die),e(Die,RDo),e(zd,SDo),e(zd,qie),e(qie,PDo),e(zd,$Do),e(rt,IDo),e(rt,Gie),e(Gie,jDo),e(rt,NDo),g(iy,rt,null),e(dr,DDo),e(dr,He),g(dy,He,null),e(He,qDo),e(He,Oie),e(Oie,GDo),e(He,ODo),e(He,tn),e(tn,XDo),e(tn,Xie),e(Xie,zDo),e(tn,VDo),e(tn,zie),e(zie,WDo),e(tn,QDo),e(tn,Vie),e(Vie,HDo),e(tn,UDo),e(He,JDo),e(He,Vd),e(Vd,fv),e(fv,Wie),e(Wie,YDo),e(fv,KDo),e(fv,xN),e(xN,ZDo),e(fv,eqo),e(Vd,oqo),e(Vd,mv),e(mv,Qie),e(Qie,rqo),e(mv,tqo),e(mv,RN),e(RN,aqo),e(mv,nqo),e(Vd,sqo),e(Vd,gv),e(gv,Hie),e(Hie,lqo),e(gv,iqo),e(gv,SN),e(SN,dqo),e(gv,cqo),e(He,fqo),e(He,hv),e(hv,mqo),e(hv,Uie),e(Uie,gqo),e(hv,hqo),e(hv,Jie),e(Jie,pqo),e(He,_qo),e(He,Yie),e(Yie,uqo),e(He,bqo),g(cy,He,null),b(d,_9e,u),b(d,Wd,u),e(Wd,pv),e(pv,Kie),g(fy,Kie,null),e(Wd,vqo),e(Wd,Zie),e(Zie,Tqo),b(d,u9e,u),b(d,cr,u),g(my,cr,null),e(cr,Fqo),e(cr,Qd),e(Qd,Cqo),e(Qd,ede),e(ede,Mqo),e(Qd,Eqo),e(Qd,ode),e(ode,yqo),e(Qd,wqo),e(cr,Aqo),e(cr,gy),e(gy,Lqo),e(gy,rde),e(rde,Bqo),e(gy,kqo),e(cr,xqo),e(cr,tt),g(hy,tt,null),e(tt,Rqo),e(tt,tde),e(tde,Sqo),e(tt,Pqo),e(tt,Hd),e(Hd,$qo),e(Hd,ade),e(ade,Iqo),e(Hd,jqo),e(Hd,nde),e(nde,Nqo),e(Hd,Dqo),e(tt,qqo),e(tt,sde),e(sde,Gqo),e(tt,Oqo),g(py,tt,null),e(cr,Xqo),e(cr,Ue),g(_y,Ue,null),e(Ue,zqo),e(Ue,lde),e(lde,Vqo),e(Ue,Wqo),e(Ue,an),e(an,Qqo),e(an,ide),e(ide,Hqo),e(an,Uqo),e(an,dde),e(dde,Jqo),e(an,Yqo),e(an,cde),e(cde,Kqo),e(an,Zqo),e(Ue,eGo),e(Ue,fde),e(fde,_v),e(_v,mde),e(mde,oGo),e(_v,rGo),e(_v,PN),e(PN,tGo),e(_v,aGo),e(Ue,nGo),e(Ue,uv),e(uv,sGo),e(uv,gde),e(gde,lGo),e(uv,iGo),e(uv,hde),e(hde,dGo),e(Ue,cGo),e(Ue,pde),e(pde,fGo),e(Ue,mGo),g(uy,Ue,null),b(d,b9e,u),b(d,Ud,u),e(Ud,bv),e(bv,_de),g(by,_de,null),e(Ud,gGo),e(Ud,ude),e(ude,hGo),b(d,v9e,u),b(d,fr,u),g(vy,fr,null),e(fr,pGo),e(fr,Jd),e(Jd,_Go),e(Jd,bde),e(bde,uGo),e(Jd,bGo),e(Jd,vde),e(vde,vGo),e(Jd,TGo),e(fr,FGo),e(fr,Ty),e(Ty,CGo),e(Ty,Tde),e(Tde,MGo),e(Ty,EGo),e(fr,yGo),e(fr,at),g(Fy,at,null),e(at,wGo),e(at,Fde),e(Fde,AGo),e(at,LGo),e(at,Yd),e(Yd,BGo),e(Yd,Cde),e(Cde,kGo),e(Yd,xGo),e(Yd,Mde),e(Mde,RGo),e(Yd,SGo),e(at,PGo),e(at,Ede),e(Ede,$Go),e(at,IGo),g(Cy,at,null),e(fr,jGo),e(fr,Je),g(My,Je,null),e(Je,NGo),e(Je,yde),e(yde,DGo),e(Je,qGo),e(Je,nn),e(nn,GGo),e(nn,wde),e(wde,OGo),e(nn,XGo),e(nn,Ade),e(Ade,zGo),e(nn,VGo),e(nn,Lde),e(Lde,WGo),e(nn,QGo),e(Je,HGo),e(Je,Bde),e(Bde,vv),e(vv,kde),e(kde,UGo),e(vv,JGo),e(vv,$N),e($N,YGo),e(vv,KGo),e(Je,ZGo),e(Je,Tv),e(Tv,eOo),e(Tv,xde),e(xde,oOo),e(Tv,rOo),e(Tv,Rde),e(Rde,tOo),e(Je,aOo),e(Je,Sde),e(Sde,nOo),e(Je,sOo),g(Ey,Je,null),b(d,T9e,u),b(d,Kd,u),e(Kd,Fv),e(Fv,Pde),g(yy,Pde,null),e(Kd,lOo),e(Kd,$de),e($de,iOo),b(d,F9e,u),b(d,mr,u),g(wy,mr,null),e(mr,dOo),e(mr,Zd),e(Zd,cOo),e(Zd,Ide),e(Ide,fOo),e(Zd,mOo),e(Zd,jde),e(jde,gOo),e(Zd,hOo),e(mr,pOo),e(mr,Ay),e(Ay,_Oo),e(Ay,Nde),e(Nde,uOo),e(Ay,bOo),e(mr,vOo),e(mr,nt),g(Ly,nt,null),e(nt,TOo),e(nt,Dde),e(Dde,FOo),e(nt,COo),e(nt,ec),e(ec,MOo),e(ec,qde),e(qde,EOo),e(ec,yOo),e(ec,Gde),e(Gde,wOo),e(ec,AOo),e(nt,LOo),e(nt,Ode),e(Ode,BOo),e(nt,kOo),g(By,nt,null),e(mr,xOo),e(mr,Ye),g(ky,Ye,null),e(Ye,ROo),e(Ye,Xde),e(Xde,SOo),e(Ye,POo),e(Ye,sn),e(sn,$Oo),e(sn,zde),e(zde,IOo),e(sn,jOo),e(sn,Vde),e(Vde,NOo),e(sn,DOo),e(sn,Wde),e(Wde,qOo),e(sn,GOo),e(Ye,OOo),e(Ye,xy),e(xy,Cv),e(Cv,Qde),e(Qde,XOo),e(Cv,zOo),e(Cv,IN),e(IN,VOo),e(Cv,WOo),e(xy,QOo),e(xy,Mv),e(Mv,Hde),e(Hde,HOo),e(Mv,UOo),e(Mv,jN),e(jN,JOo),e(Mv,YOo),e(Ye,KOo),e(Ye,Ev),e(Ev,ZOo),e(Ev,Ude),e(Ude,eXo),e(Ev,oXo),e(Ev,Jde),e(Jde,rXo),e(Ye,tXo),e(Ye,Yde),e(Yde,aXo),e(Ye,nXo),g(Ry,Ye,null),b(d,C9e,u),b(d,oc,u),e(oc,yv),e(yv,Kde),g(Sy,Kde,null),e(oc,sXo),e(oc,Zde),e(Zde,lXo),b(d,M9e,u),b(d,gr,u),g(Py,gr,null),e(gr,iXo),e(gr,rc),e(rc,dXo),e(rc,ece),e(ece,cXo),e(rc,fXo),e(rc,oce),e(oce,mXo),e(rc,gXo),e(gr,hXo),e(gr,$y),e($y,pXo),e($y,rce),e(rce,_Xo),e($y,uXo),e(gr,bXo),e(gr,st),g(Iy,st,null),e(st,vXo),e(st,tce),e(tce,TXo),e(st,FXo),e(st,tc),e(tc,CXo),e(tc,ace),e(ace,MXo),e(tc,EXo),e(tc,nce),e(nce,yXo),e(tc,wXo),e(st,AXo),e(st,sce),e(sce,LXo),e(st,BXo),g(jy,st,null),e(gr,kXo),e(gr,go),g(Ny,go,null),e(go,xXo),e(go,lce),e(lce,RXo),e(go,SXo),e(go,ln),e(ln,PXo),e(ln,ice),e(ice,$Xo),e(ln,IXo),e(ln,dce),e(dce,jXo),e(ln,NXo),e(ln,cce),e(cce,DXo),e(ln,qXo),e(go,GXo),e(go,B),e(B,wv),e(wv,fce),e(fce,OXo),e(wv,XXo),e(wv,NN),e(NN,zXo),e(wv,VXo),e(B,WXo),e(B,Av),e(Av,mce),e(mce,QXo),e(Av,HXo),e(Av,DN),e(DN,UXo),e(Av,JXo),e(B,YXo),e(B,Lv),e(Lv,gce),e(gce,KXo),e(Lv,ZXo),e(Lv,qN),e(qN,ezo),e(Lv,ozo),e(B,rzo),e(B,Bv),e(Bv,hce),e(hce,tzo),e(Bv,azo),e(Bv,GN),e(GN,nzo),e(Bv,szo),e(B,lzo),e(B,kv),e(kv,pce),e(pce,izo),e(kv,dzo),e(kv,ON),e(ON,czo),e(kv,fzo),e(B,mzo),e(B,xv),e(xv,_ce),e(_ce,gzo),e(xv,hzo),e(xv,XN),e(XN,pzo),e(xv,_zo),e(B,uzo),e(B,Rv),e(Rv,uce),e(uce,bzo),e(Rv,vzo),e(Rv,zN),e(zN,Tzo),e(Rv,Fzo),e(B,Czo),e(B,Sv),e(Sv,bce),e(bce,Mzo),e(Sv,Ezo),e(Sv,VN),e(VN,yzo),e(Sv,wzo),e(B,Azo),e(B,Pv),e(Pv,vce),e(vce,Lzo),e(Pv,Bzo),e(Pv,WN),e(WN,kzo),e(Pv,xzo),e(B,Rzo),e(B,$v),e($v,Tce),e(Tce,Szo),e($v,Pzo),e($v,QN),e(QN,$zo),e($v,Izo),e(B,jzo),e(B,Iv),e(Iv,Fce),e(Fce,Nzo),e(Iv,Dzo),e(Iv,HN),e(HN,qzo),e(Iv,Gzo),e(B,Ozo),e(B,jv),e(jv,Cce),e(Cce,Xzo),e(jv,zzo),e(jv,UN),e(UN,Vzo),e(jv,Wzo),e(B,Qzo),e(B,Nv),e(Nv,Mce),e(Mce,Hzo),e(Nv,Uzo),e(Nv,JN),e(JN,Jzo),e(Nv,Yzo),e(B,Kzo),e(B,Dv),e(Dv,Ece),e(Ece,Zzo),e(Dv,eVo),e(Dv,YN),e(YN,oVo),e(Dv,rVo),e(B,tVo),e(B,qv),e(qv,yce),e(yce,aVo),e(qv,nVo),e(qv,KN),e(KN,sVo),e(qv,lVo),e(B,iVo),e(B,Gv),e(Gv,wce),e(wce,dVo),e(Gv,cVo),e(Gv,ZN),e(ZN,fVo),e(Gv,mVo),e(B,gVo),e(B,Ss),e(Ss,Ace),e(Ace,hVo),e(Ss,pVo),e(Ss,eD),e(eD,_Vo),e(Ss,uVo),e(Ss,oD),e(oD,bVo),e(Ss,vVo),e(B,TVo),e(B,Ov),e(Ov,Lce),e(Lce,FVo),e(Ov,CVo),e(Ov,rD),e(rD,MVo),e(Ov,EVo),e(B,yVo),e(B,Xv),e(Xv,Bce),e(Bce,wVo),e(Xv,AVo),e(Xv,tD),e(tD,LVo),e(Xv,BVo),e(B,kVo),e(B,zv),e(zv,kce),e(kce,xVo),e(zv,RVo),e(zv,aD),e(aD,SVo),e(zv,PVo),e(B,$Vo),e(B,Vv),e(Vv,xce),e(xce,IVo),e(Vv,jVo),e(Vv,nD),e(nD,NVo),e(Vv,DVo),e(B,qVo),e(B,Wv),e(Wv,Rce),e(Rce,GVo),e(Wv,OVo),e(Wv,sD),e(sD,XVo),e(Wv,zVo),e(B,VVo),e(B,Qv),e(Qv,Sce),e(Sce,WVo),e(Qv,QVo),e(Qv,lD),e(lD,HVo),e(Qv,UVo),e(B,JVo),e(B,Hv),e(Hv,Pce),e(Pce,YVo),e(Hv,KVo),e(Hv,iD),e(iD,ZVo),e(Hv,eWo),e(B,oWo),e(B,Uv),e(Uv,$ce),e($ce,rWo),e(Uv,tWo),e(Uv,dD),e(dD,aWo),e(Uv,nWo),e(B,sWo),e(B,Jv),e(Jv,Ice),e(Ice,lWo),e(Jv,iWo),e(Jv,cD),e(cD,dWo),e(Jv,cWo),e(B,fWo),e(B,Yv),e(Yv,jce),e(jce,mWo),e(Yv,gWo),e(Yv,fD),e(fD,hWo),e(Yv,pWo),e(B,_Wo),e(B,Kv),e(Kv,Nce),e(Nce,uWo),e(Kv,bWo),e(Kv,mD),e(mD,vWo),e(Kv,TWo),e(B,FWo),e(B,Zv),e(Zv,Dce),e(Dce,CWo),e(Zv,MWo),e(Zv,gD),e(gD,EWo),e(Zv,yWo),e(B,wWo),e(B,e6),e(e6,qce),e(qce,AWo),e(e6,LWo),e(e6,hD),e(hD,BWo),e(e6,kWo),e(B,xWo),e(B,o6),e(o6,Gce),e(Gce,RWo),e(o6,SWo),e(o6,pD),e(pD,PWo),e(o6,$Wo),e(B,IWo),e(B,r6),e(r6,Oce),e(Oce,jWo),e(r6,NWo),e(r6,_D),e(_D,DWo),e(r6,qWo),e(B,GWo),e(B,t6),e(t6,Xce),e(Xce,OWo),e(t6,XWo),e(t6,uD),e(uD,zWo),e(t6,VWo),e(B,WWo),e(B,a6),e(a6,zce),e(zce,QWo),e(a6,HWo),e(a6,bD),e(bD,UWo),e(a6,JWo),e(B,YWo),e(B,n6),e(n6,Vce),e(Vce,KWo),e(n6,ZWo),e(n6,vD),e(vD,eQo),e(n6,oQo),e(B,rQo),e(B,s6),e(s6,Wce),e(Wce,tQo),e(s6,aQo),e(s6,TD),e(TD,nQo),e(s6,sQo),e(B,lQo),e(B,l6),e(l6,Qce),e(Qce,iQo),e(l6,dQo),e(l6,FD),e(FD,cQo),e(l6,fQo),e(B,mQo),e(B,i6),e(i6,Hce),e(Hce,gQo),e(i6,hQo),e(i6,CD),e(CD,pQo),e(i6,_Qo),e(B,uQo),e(B,d6),e(d6,Uce),e(Uce,bQo),e(d6,vQo),e(d6,MD),e(MD,TQo),e(d6,FQo),e(B,CQo),e(B,c6),e(c6,Jce),e(Jce,MQo),e(c6,EQo),e(c6,ED),e(ED,yQo),e(c6,wQo),e(B,AQo),e(B,f6),e(f6,Yce),e(Yce,LQo),e(f6,BQo),e(f6,yD),e(yD,kQo),e(f6,xQo),e(B,RQo),e(B,m6),e(m6,Kce),e(Kce,SQo),e(m6,PQo),e(m6,wD),e(wD,$Qo),e(m6,IQo),e(go,jQo),e(go,Zce),e(Zce,NQo),e(go,DQo),g(Dy,go,null),b(d,E9e,u),b(d,ac,u),e(ac,g6),e(g6,efe),g(qy,efe,null),e(ac,qQo),e(ac,ofe),e(ofe,GQo),b(d,y9e,u),b(d,hr,u),g(Gy,hr,null),e(hr,OQo),e(hr,nc),e(nc,XQo),e(nc,rfe),e(rfe,zQo),e(nc,VQo),e(nc,tfe),e(tfe,WQo),e(nc,QQo),e(hr,HQo),e(hr,Oy),e(Oy,UQo),e(Oy,afe),e(afe,JQo),e(Oy,YQo),e(hr,KQo),e(hr,lt),g(Xy,lt,null),e(lt,ZQo),e(lt,nfe),e(nfe,eHo),e(lt,oHo),e(lt,sc),e(sc,rHo),e(sc,sfe),e(sfe,tHo),e(sc,aHo),e(sc,lfe),e(lfe,nHo),e(sc,sHo),e(lt,lHo),e(lt,ife),e(ife,iHo),e(lt,dHo),g(zy,lt,null),e(hr,cHo),e(hr,ho),g(Vy,ho,null),e(ho,fHo),e(ho,dfe),e(dfe,mHo),e(ho,gHo),e(ho,dn),e(dn,hHo),e(dn,cfe),e(cfe,pHo),e(dn,_Ho),e(dn,ffe),e(ffe,uHo),e(dn,bHo),e(dn,mfe),e(mfe,vHo),e(dn,THo),e(ho,FHo),e(ho,H),e(H,h6),e(h6,gfe),e(gfe,CHo),e(h6,MHo),e(h6,AD),e(AD,EHo),e(h6,yHo),e(H,wHo),e(H,p6),e(p6,hfe),e(hfe,AHo),e(p6,LHo),e(p6,LD),e(LD,BHo),e(p6,kHo),e(H,xHo),e(H,_6),e(_6,pfe),e(pfe,RHo),e(_6,SHo),e(_6,BD),e(BD,PHo),e(_6,$Ho),e(H,IHo),e(H,u6),e(u6,_fe),e(_fe,jHo),e(u6,NHo),e(u6,kD),e(kD,DHo),e(u6,qHo),e(H,GHo),e(H,b6),e(b6,ufe),e(ufe,OHo),e(b6,XHo),e(b6,xD),e(xD,zHo),e(b6,VHo),e(H,WHo),e(H,v6),e(v6,bfe),e(bfe,QHo),e(v6,HHo),e(v6,RD),e(RD,UHo),e(v6,JHo),e(H,YHo),e(H,T6),e(T6,vfe),e(vfe,KHo),e(T6,ZHo),e(T6,SD),e(SD,eUo),e(T6,oUo),e(H,rUo),e(H,F6),e(F6,Tfe),e(Tfe,tUo),e(F6,aUo),e(F6,PD),e(PD,nUo),e(F6,sUo),e(H,lUo),e(H,C6),e(C6,Ffe),e(Ffe,iUo),e(C6,dUo),e(C6,$D),e($D,cUo),e(C6,fUo),e(H,mUo),e(H,M6),e(M6,Cfe),e(Cfe,gUo),e(M6,hUo),e(M6,ID),e(ID,pUo),e(M6,_Uo),e(H,uUo),e(H,E6),e(E6,Mfe),e(Mfe,bUo),e(E6,vUo),e(E6,jD),e(jD,TUo),e(E6,FUo),e(H,CUo),e(H,y6),e(y6,Efe),e(Efe,MUo),e(y6,EUo),e(y6,ND),e(ND,yUo),e(y6,wUo),e(H,AUo),e(H,w6),e(w6,yfe),e(yfe,LUo),e(w6,BUo),e(w6,DD),e(DD,kUo),e(w6,xUo),e(H,RUo),e(H,A6),e(A6,wfe),e(wfe,SUo),e(A6,PUo),e(A6,qD),e(qD,$Uo),e(A6,IUo),e(H,jUo),e(H,L6),e(L6,Afe),e(Afe,NUo),e(L6,DUo),e(L6,GD),e(GD,qUo),e(L6,GUo),e(H,OUo),e(H,B6),e(B6,Lfe),e(Lfe,XUo),e(B6,zUo),e(B6,OD),e(OD,VUo),e(B6,WUo),e(H,QUo),e(H,k6),e(k6,Bfe),e(Bfe,HUo),e(k6,UUo),e(k6,XD),e(XD,JUo),e(k6,YUo),e(H,KUo),e(H,x6),e(x6,kfe),e(kfe,ZUo),e(x6,eJo),e(x6,zD),e(zD,oJo),e(x6,rJo),e(H,tJo),e(H,R6),e(R6,xfe),e(xfe,aJo),e(R6,nJo),e(R6,VD),e(VD,sJo),e(R6,lJo),e(H,iJo),e(H,S6),e(S6,Rfe),e(Rfe,dJo),e(S6,cJo),e(S6,WD),e(WD,fJo),e(S6,mJo),e(H,gJo),e(H,P6),e(P6,Sfe),e(Sfe,hJo),e(P6,pJo),e(P6,QD),e(QD,_Jo),e(P6,uJo),e(H,bJo),e(H,$6),e($6,Pfe),e(Pfe,vJo),e($6,TJo),e($6,HD),e(HD,FJo),e($6,CJo),e(ho,MJo),e(ho,$fe),e($fe,EJo),e(ho,yJo),g(Wy,ho,null),b(d,w9e,u),b(d,lc,u),e(lc,I6),e(I6,Ife),g(Qy,Ife,null),e(lc,wJo),e(lc,jfe),e(jfe,AJo),b(d,A9e,u),b(d,pr,u),g(Hy,pr,null),e(pr,LJo),e(pr,ic),e(ic,BJo),e(ic,Nfe),e(Nfe,kJo),e(ic,xJo),e(ic,Dfe),e(Dfe,RJo),e(ic,SJo),e(pr,PJo),e(pr,Uy),e(Uy,$Jo),e(Uy,qfe),e(qfe,IJo),e(Uy,jJo),e(pr,NJo),e(pr,it),g(Jy,it,null),e(it,DJo),e(it,Gfe),e(Gfe,qJo),e(it,GJo),e(it,dc),e(dc,OJo),e(dc,Ofe),e(Ofe,XJo),e(dc,zJo),e(dc,Xfe),e(Xfe,VJo),e(dc,WJo),e(it,QJo),e(it,zfe),e(zfe,HJo),e(it,UJo),g(Yy,it,null),e(pr,JJo),e(pr,po),g(Ky,po,null),e(po,YJo),e(po,Vfe),e(Vfe,KJo),e(po,ZJo),e(po,cn),e(cn,eYo),e(cn,Wfe),e(Wfe,oYo),e(cn,rYo),e(cn,Qfe),e(Qfe,tYo),e(cn,aYo),e(cn,Hfe),e(Hfe,nYo),e(cn,sYo),e(po,lYo),e(po,he),e(he,j6),e(j6,Ufe),e(Ufe,iYo),e(j6,dYo),e(j6,UD),e(UD,cYo),e(j6,fYo),e(he,mYo),e(he,N6),e(N6,Jfe),e(Jfe,gYo),e(N6,hYo),e(N6,JD),e(JD,pYo),e(N6,_Yo),e(he,uYo),e(he,D6),e(D6,Yfe),e(Yfe,bYo),e(D6,vYo),e(D6,YD),e(YD,TYo),e(D6,FYo),e(he,CYo),e(he,q6),e(q6,Kfe),e(Kfe,MYo),e(q6,EYo),e(q6,KD),e(KD,yYo),e(q6,wYo),e(he,AYo),e(he,G6),e(G6,Zfe),e(Zfe,LYo),e(G6,BYo),e(G6,ZD),e(ZD,kYo),e(G6,xYo),e(he,RYo),e(he,O6),e(O6,eme),e(eme,SYo),e(O6,PYo),e(O6,eq),e(eq,$Yo),e(O6,IYo),e(he,jYo),e(he,X6),e(X6,ome),e(ome,NYo),e(X6,DYo),e(X6,oq),e(oq,qYo),e(X6,GYo),e(he,OYo),e(he,z6),e(z6,rme),e(rme,XYo),e(z6,zYo),e(z6,rq),e(rq,VYo),e(z6,WYo),e(he,QYo),e(he,V6),e(V6,tme),e(tme,HYo),e(V6,UYo),e(V6,tq),e(tq,JYo),e(V6,YYo),e(he,KYo),e(he,W6),e(W6,ame),e(ame,ZYo),e(W6,eKo),e(W6,aq),e(aq,oKo),e(W6,rKo),e(po,tKo),e(po,nme),e(nme,aKo),e(po,nKo),g(Zy,po,null),b(d,L9e,u),b(d,cc,u),e(cc,Q6),e(Q6,sme),g(ew,sme,null),e(cc,sKo),e(cc,lme),e(lme,lKo),b(d,B9e,u),b(d,_r,u),g(ow,_r,null),e(_r,iKo),e(_r,fc),e(fc,dKo),e(fc,ime),e(ime,cKo),e(fc,fKo),e(fc,dme),e(dme,mKo),e(fc,gKo),e(_r,hKo),e(_r,rw),e(rw,pKo),e(rw,cme),e(cme,_Ko),e(rw,uKo),e(_r,bKo),e(_r,dt),g(tw,dt,null),e(dt,vKo),e(dt,fme),e(fme,TKo),e(dt,FKo),e(dt,mc),e(mc,CKo),e(mc,mme),e(mme,MKo),e(mc,EKo),e(mc,gme),e(gme,yKo),e(mc,wKo),e(dt,AKo),e(dt,hme),e(hme,LKo),e(dt,BKo),g(aw,dt,null),e(_r,kKo),e(_r,_o),g(nw,_o,null),e(_o,xKo),e(_o,pme),e(pme,RKo),e(_o,SKo),e(_o,fn),e(fn,PKo),e(fn,_me),e(_me,$Ko),e(fn,IKo),e(fn,ume),e(ume,jKo),e(fn,NKo),e(fn,bme),e(bme,DKo),e(fn,qKo),e(_o,GKo),e(_o,sw),e(sw,H6),e(H6,vme),e(vme,OKo),e(H6,XKo),e(H6,nq),e(nq,zKo),e(H6,VKo),e(sw,WKo),e(sw,U6),e(U6,Tme),e(Tme,QKo),e(U6,HKo),e(U6,sq),e(sq,UKo),e(U6,JKo),e(_o,YKo),e(_o,Fme),e(Fme,KKo),e(_o,ZKo),g(lw,_o,null),b(d,k9e,u),b(d,gc,u),e(gc,J6),e(J6,Cme),g(iw,Cme,null),e(gc,eZo),e(gc,Mme),e(Mme,oZo),b(d,x9e,u),b(d,ur,u),g(dw,ur,null),e(ur,rZo),e(ur,hc),e(hc,tZo),e(hc,Eme),e(Eme,aZo),e(hc,nZo),e(hc,yme),e(yme,sZo),e(hc,lZo),e(ur,iZo),e(ur,cw),e(cw,dZo),e(cw,wme),e(wme,cZo),e(cw,fZo),e(ur,mZo),e(ur,ct),g(fw,ct,null),e(ct,gZo),e(ct,Ame),e(Ame,hZo),e(ct,pZo),e(ct,pc),e(pc,_Zo),e(pc,Lme),e(Lme,uZo),e(pc,bZo),e(pc,Bme),e(Bme,vZo),e(pc,TZo),e(ct,FZo),e(ct,kme),e(kme,CZo),e(ct,MZo),g(mw,ct,null),e(ur,EZo),e(ur,uo),g(gw,uo,null),e(uo,yZo),e(uo,xme),e(xme,wZo),e(uo,AZo),e(uo,mn),e(mn,LZo),e(mn,Rme),e(Rme,BZo),e(mn,kZo),e(mn,Sme),e(Sme,xZo),e(mn,RZo),e(mn,Pme),e(Pme,SZo),e(mn,PZo),e(uo,$Zo),e(uo,Y),e(Y,Y6),e(Y6,$me),e($me,IZo),e(Y6,jZo),e(Y6,lq),e(lq,NZo),e(Y6,DZo),e(Y,qZo),e(Y,K6),e(K6,Ime),e(Ime,GZo),e(K6,OZo),e(K6,iq),e(iq,XZo),e(K6,zZo),e(Y,VZo),e(Y,Z6),e(Z6,jme),e(jme,WZo),e(Z6,QZo),e(Z6,dq),e(dq,HZo),e(Z6,UZo),e(Y,JZo),e(Y,eT),e(eT,Nme),e(Nme,YZo),e(eT,KZo),e(eT,cq),e(cq,ZZo),e(eT,eer),e(Y,oer),e(Y,oT),e(oT,Dme),e(Dme,rer),e(oT,ter),e(oT,fq),e(fq,aer),e(oT,ner),e(Y,ser),e(Y,rT),e(rT,qme),e(qme,ler),e(rT,ier),e(rT,mq),e(mq,der),e(rT,cer),e(Y,fer),e(Y,tT),e(tT,Gme),e(Gme,mer),e(tT,ger),e(tT,gq),e(gq,her),e(tT,per),e(Y,_er),e(Y,aT),e(aT,Ome),e(Ome,uer),e(aT,ber),e(aT,hq),e(hq,ver),e(aT,Ter),e(Y,Fer),e(Y,nT),e(nT,Xme),e(Xme,Cer),e(nT,Mer),e(nT,pq),e(pq,Eer),e(nT,yer),e(Y,wer),e(Y,sT),e(sT,zme),e(zme,Aer),e(sT,Ler),e(sT,_q),e(_q,Ber),e(sT,ker),e(Y,xer),e(Y,lT),e(lT,Vme),e(Vme,Rer),e(lT,Ser),e(lT,uq),e(uq,Per),e(lT,$er),e(Y,Ier),e(Y,iT),e(iT,Wme),e(Wme,jer),e(iT,Ner),e(iT,bq),e(bq,Der),e(iT,qer),e(Y,Ger),e(Y,dT),e(dT,Qme),e(Qme,Oer),e(dT,Xer),e(dT,vq),e(vq,zer),e(dT,Ver),e(Y,Wer),e(Y,cT),e(cT,Hme),e(Hme,Qer),e(cT,Her),e(cT,Tq),e(Tq,Uer),e(cT,Jer),e(Y,Yer),e(Y,fT),e(fT,Ume),e(Ume,Ker),e(fT,Zer),e(fT,Fq),e(Fq,eor),e(fT,oor),e(Y,ror),e(Y,mT),e(mT,Jme),e(Jme,tor),e(mT,aor),e(mT,Cq),e(Cq,nor),e(mT,sor),e(Y,lor),e(Y,gT),e(gT,Yme),e(Yme,ior),e(gT,dor),e(gT,Mq),e(Mq,cor),e(gT,mor),e(Y,gor),e(Y,hT),e(hT,Kme),e(Kme,hor),e(hT,por),e(hT,Eq),e(Eq,_or),e(hT,uor),e(Y,bor),e(Y,pT),e(pT,Zme),e(Zme,vor),e(pT,Tor),e(pT,yq),e(yq,For),e(pT,Cor),e(Y,Mor),e(Y,_T),e(_T,ege),e(ege,Eor),e(_T,yor),e(_T,wq),e(wq,wor),e(_T,Aor),e(uo,Lor),e(uo,oge),e(oge,Bor),e(uo,kor),g(hw,uo,null),b(d,R9e,u),b(d,_c,u),e(_c,uT),e(uT,rge),g(pw,rge,null),e(_c,xor),e(_c,tge),e(tge,Ror),b(d,S9e,u),b(d,br,u),g(_w,br,null),e(br,Sor),e(br,uc),e(uc,Por),e(uc,age),e(age,$or),e(uc,Ior),e(uc,nge),e(nge,jor),e(uc,Nor),e(br,Dor),e(br,uw),e(uw,qor),e(uw,sge),e(sge,Gor),e(uw,Oor),e(br,Xor),e(br,ft),g(bw,ft,null),e(ft,zor),e(ft,lge),e(lge,Vor),e(ft,Wor),e(ft,bc),e(bc,Qor),e(bc,ige),e(ige,Hor),e(bc,Uor),e(bc,dge),e(dge,Jor),e(bc,Yor),e(ft,Kor),e(ft,cge),e(cge,Zor),e(ft,err),g(vw,ft,null),e(br,orr),e(br,bo),g(Tw,bo,null),e(bo,rrr),e(bo,fge),e(fge,trr),e(bo,arr),e(bo,gn),e(gn,nrr),e(gn,mge),e(mge,srr),e(gn,lrr),e(gn,gge),e(gge,irr),e(gn,drr),e(gn,hge),e(hge,crr),e(gn,frr),e(bo,mrr),e(bo,pe),e(pe,bT),e(bT,pge),e(pge,grr),e(bT,hrr),e(bT,Aq),e(Aq,prr),e(bT,_rr),e(pe,urr),e(pe,vT),e(vT,_ge),e(_ge,brr),e(vT,vrr),e(vT,Lq),e(Lq,Trr),e(vT,Frr),e(pe,Crr),e(pe,TT),e(TT,uge),e(uge,Mrr),e(TT,Err),e(TT,Bq),e(Bq,yrr),e(TT,wrr),e(pe,Arr),e(pe,FT),e(FT,bge),e(bge,Lrr),e(FT,Brr),e(FT,kq),e(kq,krr),e(FT,xrr),e(pe,Rrr),e(pe,CT),e(CT,vge),e(vge,Srr),e(CT,Prr),e(CT,xq),e(xq,$rr),e(CT,Irr),e(pe,jrr),e(pe,MT),e(MT,Tge),e(Tge,Nrr),e(MT,Drr),e(MT,Rq),e(Rq,qrr),e(MT,Grr),e(pe,Orr),e(pe,ET),e(ET,Fge),e(Fge,Xrr),e(ET,zrr),e(ET,Sq),e(Sq,Vrr),e(ET,Wrr),e(pe,Qrr),e(pe,yT),e(yT,Cge),e(Cge,Hrr),e(yT,Urr),e(yT,Pq),e(Pq,Jrr),e(yT,Yrr),e(pe,Krr),e(pe,wT),e(wT,Mge),e(Mge,Zrr),e(wT,etr),e(wT,$q),e($q,otr),e(wT,rtr),e(pe,ttr),e(pe,AT),e(AT,Ege),e(Ege,atr),e(AT,ntr),e(AT,Iq),e(Iq,str),e(AT,ltr),e(bo,itr),e(bo,yge),e(yge,dtr),e(bo,ctr),g(Fw,bo,null),b(d,P9e,u),b(d,vc,u),e(vc,LT),e(LT,wge),g(Cw,wge,null),e(vc,ftr),e(vc,Age),e(Age,mtr),b(d,$9e,u),b(d,vr,u),g(Mw,vr,null),e(vr,gtr),e(vr,Tc),e(Tc,htr),e(Tc,Lge),e(Lge,ptr),e(Tc,_tr),e(Tc,Bge),e(Bge,utr),e(Tc,btr),e(vr,vtr),e(vr,Ew),e(Ew,Ttr),e(Ew,kge),e(kge,Ftr),e(Ew,Ctr),e(vr,Mtr),e(vr,mt),g(yw,mt,null),e(mt,Etr),e(mt,xge),e(xge,ytr),e(mt,wtr),e(mt,Fc),e(Fc,Atr),e(Fc,Rge),e(Rge,Ltr),e(Fc,Btr),e(Fc,Sge),e(Sge,ktr),e(Fc,xtr),e(mt,Rtr),e(mt,Pge),e(Pge,Str),e(mt,Ptr),g(ww,mt,null),e(vr,$tr),e(vr,vo),g(Aw,vo,null),e(vo,Itr),e(vo,$ge),e($ge,jtr),e(vo,Ntr),e(vo,hn),e(hn,Dtr),e(hn,Ige),e(Ige,qtr),e(hn,Gtr),e(hn,jge),e(jge,Otr),e(hn,Xtr),e(hn,Nge),e(Nge,ztr),e(hn,Vtr),e(vo,Wtr),e(vo,X),e(X,BT),e(BT,Dge),e(Dge,Qtr),e(BT,Htr),e(BT,jq),e(jq,Utr),e(BT,Jtr),e(X,Ytr),e(X,kT),e(kT,qge),e(qge,Ktr),e(kT,Ztr),e(kT,Nq),e(Nq,ear),e(kT,oar),e(X,rar),e(X,xT),e(xT,Gge),e(Gge,tar),e(xT,aar),e(xT,Dq),e(Dq,nar),e(xT,sar),e(X,lar),e(X,RT),e(RT,Oge),e(Oge,iar),e(RT,dar),e(RT,qq),e(qq,car),e(RT,far),e(X,mar),e(X,ST),e(ST,Xge),e(Xge,gar),e(ST,har),e(ST,Gq),e(Gq,par),e(ST,_ar),e(X,uar),e(X,PT),e(PT,zge),e(zge,bar),e(PT,Tar),e(PT,Oq),e(Oq,Far),e(PT,Car),e(X,Mar),e(X,$T),e($T,Vge),e(Vge,Ear),e($T,yar),e($T,Xq),e(Xq,war),e($T,Aar),e(X,Lar),e(X,IT),e(IT,Wge),e(Wge,Bar),e(IT,kar),e(IT,zq),e(zq,xar),e(IT,Rar),e(X,Sar),e(X,jT),e(jT,Qge),e(Qge,Par),e(jT,$ar),e(jT,Vq),e(Vq,Iar),e(jT,jar),e(X,Nar),e(X,NT),e(NT,Hge),e(Hge,Dar),e(NT,qar),e(NT,Wq),e(Wq,Gar),e(NT,Oar),e(X,Xar),e(X,DT),e(DT,Uge),e(Uge,zar),e(DT,Var),e(DT,Qq),e(Qq,War),e(DT,Qar),e(X,Har),e(X,qT),e(qT,Jge),e(Jge,Uar),e(qT,Jar),e(qT,Hq),e(Hq,Yar),e(qT,Kar),e(X,Zar),e(X,GT),e(GT,Yge),e(Yge,enr),e(GT,onr),e(GT,Uq),e(Uq,rnr),e(GT,tnr),e(X,anr),e(X,OT),e(OT,Kge),e(Kge,nnr),e(OT,snr),e(OT,Jq),e(Jq,lnr),e(OT,inr),e(X,dnr),e(X,XT),e(XT,Zge),e(Zge,cnr),e(XT,fnr),e(XT,Yq),e(Yq,mnr),e(XT,gnr),e(X,hnr),e(X,zT),e(zT,ehe),e(ehe,pnr),e(zT,_nr),e(zT,Kq),e(Kq,unr),e(zT,bnr),e(X,vnr),e(X,VT),e(VT,ohe),e(ohe,Tnr),e(VT,Fnr),e(VT,Zq),e(Zq,Cnr),e(VT,Mnr),e(X,Enr),e(X,WT),e(WT,rhe),e(rhe,ynr),e(WT,wnr),e(WT,eG),e(eG,Anr),e(WT,Lnr),e(X,Bnr),e(X,QT),e(QT,the),e(the,knr),e(QT,xnr),e(QT,oG),e(oG,Rnr),e(QT,Snr),e(X,Pnr),e(X,HT),e(HT,ahe),e(ahe,$nr),e(HT,Inr),e(HT,rG),e(rG,jnr),e(HT,Nnr),e(X,Dnr),e(X,UT),e(UT,nhe),e(nhe,qnr),e(UT,Gnr),e(UT,tG),e(tG,Onr),e(UT,Xnr),e(X,znr),e(X,JT),e(JT,she),e(she,Vnr),e(JT,Wnr),e(JT,aG),e(aG,Qnr),e(JT,Hnr),e(X,Unr),e(X,YT),e(YT,lhe),e(lhe,Jnr),e(YT,Ynr),e(YT,nG),e(nG,Knr),e(YT,Znr),e(X,esr),e(X,KT),e(KT,ihe),e(ihe,osr),e(KT,rsr),e(KT,sG),e(sG,tsr),e(KT,asr),e(X,nsr),e(X,ZT),e(ZT,dhe),e(dhe,ssr),e(ZT,lsr),e(ZT,lG),e(lG,isr),e(ZT,dsr),e(vo,csr),e(vo,che),e(che,fsr),e(vo,msr),g(Lw,vo,null),b(d,I9e,u),b(d,Cc,u),e(Cc,e8),e(e8,fhe),g(Bw,fhe,null),e(Cc,gsr),e(Cc,mhe),e(mhe,hsr),b(d,j9e,u),b(d,Tr,u),g(kw,Tr,null),e(Tr,psr),e(Tr,Mc),e(Mc,_sr),e(Mc,ghe),e(ghe,usr),e(Mc,bsr),e(Mc,hhe),e(hhe,vsr),e(Mc,Tsr),e(Tr,Fsr),e(Tr,xw),e(xw,Csr),e(xw,phe),e(phe,Msr),e(xw,Esr),e(Tr,ysr),e(Tr,gt),g(Rw,gt,null),e(gt,wsr),e(gt,_he),e(_he,Asr),e(gt,Lsr),e(gt,Ec),e(Ec,Bsr),e(Ec,uhe),e(uhe,ksr),e(Ec,xsr),e(Ec,bhe),e(bhe,Rsr),e(Ec,Ssr),e(gt,Psr),e(gt,vhe),e(vhe,$sr),e(gt,Isr),g(Sw,gt,null),e(Tr,jsr),e(Tr,To),g(Pw,To,null),e(To,Nsr),e(To,The),e(The,Dsr),e(To,qsr),e(To,pn),e(pn,Gsr),e(pn,Fhe),e(Fhe,Osr),e(pn,Xsr),e(pn,Che),e(Che,zsr),e(pn,Vsr),e(pn,Mhe),e(Mhe,Wsr),e(pn,Qsr),e(To,Hsr),e(To,te),e(te,o8),e(o8,Ehe),e(Ehe,Usr),e(o8,Jsr),e(o8,iG),e(iG,Ysr),e(o8,Ksr),e(te,Zsr),e(te,r8),e(r8,yhe),e(yhe,elr),e(r8,olr),e(r8,dG),e(dG,rlr),e(r8,tlr),e(te,alr),e(te,t8),e(t8,whe),e(whe,nlr),e(t8,slr),e(t8,cG),e(cG,llr),e(t8,ilr),e(te,dlr),e(te,a8),e(a8,Ahe),e(Ahe,clr),e(a8,flr),e(a8,fG),e(fG,mlr),e(a8,glr),e(te,hlr),e(te,n8),e(n8,Lhe),e(Lhe,plr),e(n8,_lr),e(n8,mG),e(mG,ulr),e(n8,blr),e(te,vlr),e(te,s8),e(s8,Bhe),e(Bhe,Tlr),e(s8,Flr),e(s8,gG),e(gG,Clr),e(s8,Mlr),e(te,Elr),e(te,l8),e(l8,khe),e(khe,ylr),e(l8,wlr),e(l8,hG),e(hG,Alr),e(l8,Llr),e(te,Blr),e(te,i8),e(i8,xhe),e(xhe,klr),e(i8,xlr),e(i8,pG),e(pG,Rlr),e(i8,Slr),e(te,Plr),e(te,d8),e(d8,Rhe),e(Rhe,$lr),e(d8,Ilr),e(d8,_G),e(_G,jlr),e(d8,Nlr),e(te,Dlr),e(te,c8),e(c8,She),e(She,qlr),e(c8,Glr),e(c8,uG),e(uG,Olr),e(c8,Xlr),e(te,zlr),e(te,f8),e(f8,Phe),e(Phe,Vlr),e(f8,Wlr),e(f8,bG),e(bG,Qlr),e(f8,Hlr),e(te,Ulr),e(te,m8),e(m8,$he),e($he,Jlr),e(m8,Ylr),e(m8,vG),e(vG,Klr),e(m8,Zlr),e(te,eir),e(te,g8),e(g8,Ihe),e(Ihe,oir),e(g8,rir),e(g8,TG),e(TG,tir),e(g8,air),e(te,nir),e(te,h8),e(h8,jhe),e(jhe,sir),e(h8,lir),e(h8,FG),e(FG,iir),e(h8,dir),e(te,cir),e(te,p8),e(p8,Nhe),e(Nhe,fir),e(p8,mir),e(p8,CG),e(CG,gir),e(p8,hir),e(te,pir),e(te,_8),e(_8,Dhe),e(Dhe,_ir),e(_8,uir),e(_8,MG),e(MG,bir),e(_8,vir),e(te,Tir),e(te,u8),e(u8,qhe),e(qhe,Fir),e(u8,Cir),e(u8,EG),e(EG,Mir),e(u8,Eir),e(To,yir),e(To,Ghe),e(Ghe,wir),e(To,Air),g($w,To,null),b(d,N9e,u),b(d,yc,u),e(yc,b8),e(b8,Ohe),g(Iw,Ohe,null),e(yc,Lir),e(yc,Xhe),e(Xhe,Bir),b(d,D9e,u),b(d,Fr,u),g(jw,Fr,null),e(Fr,kir),e(Fr,wc),e(wc,xir),e(wc,zhe),e(zhe,Rir),e(wc,Sir),e(wc,Vhe),e(Vhe,Pir),e(wc,$ir),e(Fr,Iir),e(Fr,Nw),e(Nw,jir),e(Nw,Whe),e(Whe,Nir),e(Nw,Dir),e(Fr,qir),e(Fr,ht),g(Dw,ht,null),e(ht,Gir),e(ht,Qhe),e(Qhe,Oir),e(ht,Xir),e(ht,Ac),e(Ac,zir),e(Ac,Hhe),e(Hhe,Vir),e(Ac,Wir),e(Ac,Uhe),e(Uhe,Qir),e(Ac,Hir),e(ht,Uir),e(ht,Jhe),e(Jhe,Jir),e(ht,Yir),g(qw,ht,null),e(Fr,Kir),e(Fr,Fo),g(Gw,Fo,null),e(Fo,Zir),e(Fo,Yhe),e(Yhe,edr),e(Fo,odr),e(Fo,_n),e(_n,rdr),e(_n,Khe),e(Khe,tdr),e(_n,adr),e(_n,Zhe),e(Zhe,ndr),e(_n,sdr),e(_n,epe),e(epe,ldr),e(_n,idr),e(Fo,ddr),e(Fo,ope),e(ope,v8),e(v8,rpe),e(rpe,cdr),e(v8,fdr),e(v8,yG),e(yG,mdr),e(v8,gdr),e(Fo,hdr),e(Fo,tpe),e(tpe,pdr),e(Fo,_dr),g(Ow,Fo,null),b(d,q9e,u),b(d,Lc,u),e(Lc,T8),e(T8,ape),g(Xw,ape,null),e(Lc,udr),e(Lc,npe),e(npe,bdr),b(d,G9e,u),b(d,Cr,u),g(zw,Cr,null),e(Cr,vdr),e(Cr,Bc),e(Bc,Tdr),e(Bc,spe),e(spe,Fdr),e(Bc,Cdr),e(Bc,lpe),e(lpe,Mdr),e(Bc,Edr),e(Cr,ydr),e(Cr,Vw),e(Vw,wdr),e(Vw,ipe),e(ipe,Adr),e(Vw,Ldr),e(Cr,Bdr),e(Cr,pt),g(Ww,pt,null),e(pt,kdr),e(pt,dpe),e(dpe,xdr),e(pt,Rdr),e(pt,kc),e(kc,Sdr),e(kc,cpe),e(cpe,Pdr),e(kc,$dr),e(kc,fpe),e(fpe,Idr),e(kc,jdr),e(pt,Ndr),e(pt,mpe),e(mpe,Ddr),e(pt,qdr),g(Qw,pt,null),e(Cr,Gdr),e(Cr,Co),g(Hw,Co,null),e(Co,Odr),e(Co,gpe),e(gpe,Xdr),e(Co,zdr),e(Co,un),e(un,Vdr),e(un,hpe),e(hpe,Wdr),e(un,Qdr),e(un,ppe),e(ppe,Hdr),e(un,Udr),e(un,_pe),e(_pe,Jdr),e(un,Ydr),e(Co,Kdr),e(Co,K),e(K,F8),e(F8,upe),e(upe,Zdr),e(F8,ecr),e(F8,wG),e(wG,ocr),e(F8,rcr),e(K,tcr),e(K,C8),e(C8,bpe),e(bpe,acr),e(C8,ncr),e(C8,AG),e(AG,scr),e(C8,lcr),e(K,icr),e(K,M8),e(M8,vpe),e(vpe,dcr),e(M8,ccr),e(M8,LG),e(LG,fcr),e(M8,mcr),e(K,gcr),e(K,E8),e(E8,Tpe),e(Tpe,hcr),e(E8,pcr),e(E8,BG),e(BG,_cr),e(E8,ucr),e(K,bcr),e(K,y8),e(y8,Fpe),e(Fpe,vcr),e(y8,Tcr),e(y8,kG),e(kG,Fcr),e(y8,Ccr),e(K,Mcr),e(K,w8),e(w8,Cpe),e(Cpe,Ecr),e(w8,ycr),e(w8,xG),e(xG,wcr),e(w8,Acr),e(K,Lcr),e(K,A8),e(A8,Mpe),e(Mpe,Bcr),e(A8,kcr),e(A8,RG),e(RG,xcr),e(A8,Rcr),e(K,Scr),e(K,L8),e(L8,Epe),e(Epe,Pcr),e(L8,$cr),e(L8,SG),e(SG,Icr),e(L8,jcr),e(K,Ncr),e(K,B8),e(B8,ype),e(ype,Dcr),e(B8,qcr),e(B8,PG),e(PG,Gcr),e(B8,Ocr),e(K,Xcr),e(K,k8),e(k8,wpe),e(wpe,zcr),e(k8,Vcr),e(k8,$G),e($G,Wcr),e(k8,Qcr),e(K,Hcr),e(K,x8),e(x8,Ape),e(Ape,Ucr),e(x8,Jcr),e(x8,IG),e(IG,Ycr),e(x8,Kcr),e(K,Zcr),e(K,R8),e(R8,Lpe),e(Lpe,efr),e(R8,ofr),e(R8,jG),e(jG,rfr),e(R8,tfr),e(K,afr),e(K,S8),e(S8,Bpe),e(Bpe,nfr),e(S8,sfr),e(S8,NG),e(NG,lfr),e(S8,ifr),e(K,dfr),e(K,P8),e(P8,kpe),e(kpe,cfr),e(P8,ffr),e(P8,DG),e(DG,mfr),e(P8,gfr),e(K,hfr),e(K,$8),e($8,xpe),e(xpe,pfr),e($8,_fr),e($8,qG),e(qG,ufr),e($8,bfr),e(K,vfr),e(K,I8),e(I8,Rpe),e(Rpe,Tfr),e(I8,Ffr),e(I8,GG),e(GG,Cfr),e(I8,Mfr),e(K,Efr),e(K,j8),e(j8,Spe),e(Spe,yfr),e(j8,wfr),e(j8,OG),e(OG,Afr),e(j8,Lfr),e(K,Bfr),e(K,N8),e(N8,Ppe),e(Ppe,kfr),e(N8,xfr),e(N8,XG),e(XG,Rfr),e(N8,Sfr),e(K,Pfr),e(K,D8),e(D8,$pe),e($pe,$fr),e(D8,Ifr),e(D8,zG),e(zG,jfr),e(D8,Nfr),e(K,Dfr),e(K,q8),e(q8,Ipe),e(Ipe,qfr),e(q8,Gfr),e(q8,VG),e(VG,Ofr),e(q8,Xfr),e(Co,zfr),e(Co,jpe),e(jpe,Vfr),e(Co,Wfr),g(Uw,Co,null),b(d,O9e,u),b(d,xc,u),e(xc,G8),e(G8,Npe),g(Jw,Npe,null),e(xc,Qfr),e(xc,Dpe),e(Dpe,Hfr),b(d,X9e,u),b(d,Mr,u),g(Yw,Mr,null),e(Mr,Ufr),e(Mr,Rc),e(Rc,Jfr),e(Rc,qpe),e(qpe,Yfr),e(Rc,Kfr),e(Rc,Gpe),e(Gpe,Zfr),e(Rc,emr),e(Mr,omr),e(Mr,Kw),e(Kw,rmr),e(Kw,Ope),e(Ope,tmr),e(Kw,amr),e(Mr,nmr),e(Mr,_t),g(Zw,_t,null),e(_t,smr),e(_t,Xpe),e(Xpe,lmr),e(_t,imr),e(_t,Sc),e(Sc,dmr),e(Sc,zpe),e(zpe,cmr),e(Sc,fmr),e(Sc,Vpe),e(Vpe,mmr),e(Sc,gmr),e(_t,hmr),e(_t,Wpe),e(Wpe,pmr),e(_t,_mr),g(eA,_t,null),e(Mr,umr),e(Mr,Mo),g(oA,Mo,null),e(Mo,bmr),e(Mo,Qpe),e(Qpe,vmr),e(Mo,Tmr),e(Mo,bn),e(bn,Fmr),e(bn,Hpe),e(Hpe,Cmr),e(bn,Mmr),e(bn,Upe),e(Upe,Emr),e(bn,ymr),e(bn,Jpe),e(Jpe,wmr),e(bn,Amr),e(Mo,Lmr),e(Mo,Z),e(Z,O8),e(O8,Ype),e(Ype,Bmr),e(O8,kmr),e(O8,WG),e(WG,xmr),e(O8,Rmr),e(Z,Smr),e(Z,X8),e(X8,Kpe),e(Kpe,Pmr),e(X8,$mr),e(X8,QG),e(QG,Imr),e(X8,jmr),e(Z,Nmr),e(Z,z8),e(z8,Zpe),e(Zpe,Dmr),e(z8,qmr),e(z8,HG),e(HG,Gmr),e(z8,Omr),e(Z,Xmr),e(Z,V8),e(V8,e_e),e(e_e,zmr),e(V8,Vmr),e(V8,UG),e(UG,Wmr),e(V8,Qmr),e(Z,Hmr),e(Z,W8),e(W8,o_e),e(o_e,Umr),e(W8,Jmr),e(W8,JG),e(JG,Ymr),e(W8,Kmr),e(Z,Zmr),e(Z,Q8),e(Q8,r_e),e(r_e,egr),e(Q8,ogr),e(Q8,YG),e(YG,rgr),e(Q8,tgr),e(Z,agr),e(Z,H8),e(H8,t_e),e(t_e,ngr),e(H8,sgr),e(H8,KG),e(KG,lgr),e(H8,igr),e(Z,dgr),e(Z,U8),e(U8,a_e),e(a_e,cgr),e(U8,fgr),e(U8,ZG),e(ZG,mgr),e(U8,ggr),e(Z,hgr),e(Z,J8),e(J8,n_e),e(n_e,pgr),e(J8,_gr),e(J8,eO),e(eO,ugr),e(J8,bgr),e(Z,vgr),e(Z,Y8),e(Y8,s_e),e(s_e,Tgr),e(Y8,Fgr),e(Y8,oO),e(oO,Cgr),e(Y8,Mgr),e(Z,Egr),e(Z,K8),e(K8,l_e),e(l_e,ygr),e(K8,wgr),e(K8,rO),e(rO,Agr),e(K8,Lgr),e(Z,Bgr),e(Z,Z8),e(Z8,i_e),e(i_e,kgr),e(Z8,xgr),e(Z8,tO),e(tO,Rgr),e(Z8,Sgr),e(Z,Pgr),e(Z,eF),e(eF,d_e),e(d_e,$gr),e(eF,Igr),e(eF,aO),e(aO,jgr),e(eF,Ngr),e(Z,Dgr),e(Z,oF),e(oF,c_e),e(c_e,qgr),e(oF,Ggr),e(oF,nO),e(nO,Ogr),e(oF,Xgr),e(Z,zgr),e(Z,rF),e(rF,f_e),e(f_e,Vgr),e(rF,Wgr),e(rF,sO),e(sO,Qgr),e(rF,Hgr),e(Z,Ugr),e(Z,tF),e(tF,m_e),e(m_e,Jgr),e(tF,Ygr),e(tF,lO),e(lO,Kgr),e(tF,Zgr),e(Z,ehr),e(Z,aF),e(aF,g_e),e(g_e,ohr),e(aF,rhr),e(aF,iO),e(iO,thr),e(aF,ahr),e(Z,nhr),e(Z,nF),e(nF,h_e),e(h_e,shr),e(nF,lhr),e(nF,dO),e(dO,ihr),e(nF,dhr),e(Z,chr),e(Z,sF),e(sF,p_e),e(p_e,fhr),e(sF,mhr),e(sF,cO),e(cO,ghr),e(sF,hhr),e(Mo,phr),e(Mo,__e),e(__e,_hr),e(Mo,uhr),g(rA,Mo,null),b(d,z9e,u),b(d,Pc,u),e(Pc,lF),e(lF,u_e),g(tA,u_e,null),e(Pc,bhr),e(Pc,b_e),e(b_e,vhr),b(d,V9e,u),b(d,Er,u),g(aA,Er,null),e(Er,Thr),e(Er,$c),e($c,Fhr),e($c,v_e),e(v_e,Chr),e($c,Mhr),e($c,T_e),e(T_e,Ehr),e($c,yhr),e(Er,whr),e(Er,nA),e(nA,Ahr),e(nA,F_e),e(F_e,Lhr),e(nA,Bhr),e(Er,khr),e(Er,ut),g(sA,ut,null),e(ut,xhr),e(ut,C_e),e(C_e,Rhr),e(ut,Shr),e(ut,Ic),e(Ic,Phr),e(Ic,M_e),e(M_e,$hr),e(Ic,Ihr),e(Ic,E_e),e(E_e,jhr),e(Ic,Nhr),e(ut,Dhr),e(ut,y_e),e(y_e,qhr),e(ut,Ghr),g(lA,ut,null),e(Er,Ohr),e(Er,Eo),g(iA,Eo,null),e(Eo,Xhr),e(Eo,w_e),e(w_e,zhr),e(Eo,Vhr),e(Eo,vn),e(vn,Whr),e(vn,A_e),e(A_e,Qhr),e(vn,Hhr),e(vn,L_e),e(L_e,Uhr),e(vn,Jhr),e(vn,B_e),e(B_e,Yhr),e(vn,Khr),e(Eo,Zhr),e(Eo,k_e),e(k_e,iF),e(iF,x_e),e(x_e,epr),e(iF,opr),e(iF,fO),e(fO,rpr),e(iF,tpr),e(Eo,apr),e(Eo,R_e),e(R_e,npr),e(Eo,spr),g(dA,Eo,null),b(d,W9e,u),b(d,jc,u),e(jc,dF),e(dF,S_e),g(cA,S_e,null),e(jc,lpr),e(jc,P_e),e(P_e,ipr),b(d,Q9e,u),b(d,yr,u),g(fA,yr,null),e(yr,dpr),e(yr,Nc),e(Nc,cpr),e(Nc,$_e),e($_e,fpr),e(Nc,mpr),e(Nc,I_e),e(I_e,gpr),e(Nc,hpr),e(yr,ppr),e(yr,mA),e(mA,_pr),e(mA,j_e),e(j_e,upr),e(mA,bpr),e(yr,vpr),e(yr,bt),g(gA,bt,null),e(bt,Tpr),e(bt,N_e),e(N_e,Fpr),e(bt,Cpr),e(bt,Dc),e(Dc,Mpr),e(Dc,D_e),e(D_e,Epr),e(Dc,ypr),e(Dc,q_e),e(q_e,wpr),e(Dc,Apr),e(bt,Lpr),e(bt,G_e),e(G_e,Bpr),e(bt,kpr),g(hA,bt,null),e(yr,xpr),e(yr,yo),g(pA,yo,null),e(yo,Rpr),e(yo,O_e),e(O_e,Spr),e(yo,Ppr),e(yo,Tn),e(Tn,$pr),e(Tn,X_e),e(X_e,Ipr),e(Tn,jpr),e(Tn,z_e),e(z_e,Npr),e(Tn,Dpr),e(Tn,V_e),e(V_e,qpr),e(Tn,Gpr),e(yo,Opr),e(yo,W_e),e(W_e,cF),e(cF,Q_e),e(Q_e,Xpr),e(cF,zpr),e(cF,mO),e(mO,Vpr),e(cF,Wpr),e(yo,Qpr),e(yo,H_e),e(H_e,Hpr),e(yo,Upr),g(_A,yo,null),b(d,H9e,u),b(d,qc,u),e(qc,fF),e(fF,U_e),g(uA,U_e,null),e(qc,Jpr),e(qc,J_e),e(J_e,Ypr),b(d,U9e,u),b(d,wr,u),g(bA,wr,null),e(wr,Kpr),e(wr,Gc),e(Gc,Zpr),e(Gc,Y_e),e(Y_e,e_r),e(Gc,o_r),e(Gc,K_e),e(K_e,r_r),e(Gc,t_r),e(wr,a_r),e(wr,vA),e(vA,n_r),e(vA,Z_e),e(Z_e,s_r),e(vA,l_r),e(wr,i_r),e(wr,vt),g(TA,vt,null),e(vt,d_r),e(vt,eue),e(eue,c_r),e(vt,f_r),e(vt,Oc),e(Oc,m_r),e(Oc,oue),e(oue,g_r),e(Oc,h_r),e(Oc,rue),e(rue,p_r),e(Oc,__r),e(vt,u_r),e(vt,tue),e(tue,b_r),e(vt,v_r),g(FA,vt,null),e(wr,T_r),e(wr,wo),g(CA,wo,null),e(wo,F_r),e(wo,aue),e(aue,C_r),e(wo,M_r),e(wo,Fn),e(Fn,E_r),e(Fn,nue),e(nue,y_r),e(Fn,w_r),e(Fn,sue),e(sue,A_r),e(Fn,L_r),e(Fn,lue),e(lue,B_r),e(Fn,k_r),e(wo,x_r),e(wo,V),e(V,mF),e(mF,iue),e(iue,R_r),e(mF,S_r),e(mF,gO),e(gO,P_r),e(mF,$_r),e(V,I_r),e(V,gF),e(gF,due),e(due,j_r),e(gF,N_r),e(gF,hO),e(hO,D_r),e(gF,q_r),e(V,G_r),e(V,hF),e(hF,cue),e(cue,O_r),e(hF,X_r),e(hF,pO),e(pO,z_r),e(hF,V_r),e(V,W_r),e(V,pF),e(pF,fue),e(fue,Q_r),e(pF,H_r),e(pF,_O),e(_O,U_r),e(pF,J_r),e(V,Y_r),e(V,_F),e(_F,mue),e(mue,K_r),e(_F,Z_r),e(_F,uO),e(uO,eur),e(_F,our),e(V,rur),e(V,uF),e(uF,gue),e(gue,tur),e(uF,aur),e(uF,bO),e(bO,nur),e(uF,sur),e(V,lur),e(V,bF),e(bF,hue),e(hue,iur),e(bF,dur),e(bF,vO),e(vO,cur),e(bF,fur),e(V,mur),e(V,vF),e(vF,pue),e(pue,gur),e(vF,hur),e(vF,TO),e(TO,pur),e(vF,_ur),e(V,uur),e(V,TF),e(TF,_ue),e(_ue,bur),e(TF,vur),e(TF,FO),e(FO,Tur),e(TF,Fur),e(V,Cur),e(V,FF),e(FF,uue),e(uue,Mur),e(FF,Eur),e(FF,CO),e(CO,yur),e(FF,wur),e(V,Aur),e(V,CF),e(CF,bue),e(bue,Lur),e(CF,Bur),e(CF,MO),e(MO,kur),e(CF,xur),e(V,Rur),e(V,MF),e(MF,vue),e(vue,Sur),e(MF,Pur),e(MF,EO),e(EO,$ur),e(MF,Iur),e(V,jur),e(V,EF),e(EF,Tue),e(Tue,Nur),e(EF,Dur),e(EF,yO),e(yO,qur),e(EF,Gur),e(V,Our),e(V,yF),e(yF,Fue),e(Fue,Xur),e(yF,zur),e(yF,wO),e(wO,Vur),e(yF,Wur),e(V,Qur),e(V,wF),e(wF,Cue),e(Cue,Hur),e(wF,Uur),e(wF,AO),e(AO,Jur),e(wF,Yur),e(V,Kur),e(V,AF),e(AF,Mue),e(Mue,Zur),e(AF,e2r),e(AF,LO),e(LO,o2r),e(AF,r2r),e(V,t2r),e(V,LF),e(LF,Eue),e(Eue,a2r),e(LF,n2r),e(LF,BO),e(BO,s2r),e(LF,l2r),e(V,i2r),e(V,BF),e(BF,yue),e(yue,d2r),e(BF,c2r),e(BF,kO),e(kO,f2r),e(BF,m2r),e(V,g2r),e(V,kF),e(kF,wue),e(wue,h2r),e(kF,p2r),e(kF,xO),e(xO,_2r),e(kF,u2r),e(V,b2r),e(V,xF),e(xF,Aue),e(Aue,v2r),e(xF,T2r),e(xF,RO),e(RO,F2r),e(xF,C2r),e(V,M2r),e(V,RF),e(RF,Lue),e(Lue,E2r),e(RF,y2r),e(RF,SO),e(SO,w2r),e(RF,A2r),e(V,L2r),e(V,SF),e(SF,Bue),e(Bue,B2r),e(SF,k2r),e(SF,PO),e(PO,x2r),e(SF,R2r),e(V,S2r),e(V,PF),e(PF,kue),e(kue,P2r),e(PF,$2r),e(PF,$O),e($O,I2r),e(PF,j2r),e(V,N2r),e(V,$F),e($F,xue),e(xue,D2r),e($F,q2r),e($F,IO),e(IO,G2r),e($F,O2r),e(wo,X2r),e(wo,Rue),e(Rue,z2r),e(wo,V2r),g(MA,wo,null),b(d,J9e,u),b(d,Xc,u),e(Xc,IF),e(IF,Sue),g(EA,Sue,null),e(Xc,W2r),e(Xc,Pue),e(Pue,Q2r),b(d,Y9e,u),b(d,Ar,u),g(yA,Ar,null),e(Ar,H2r),e(Ar,zc),e(zc,U2r),e(zc,$ue),e($ue,J2r),e(zc,Y2r),e(zc,Iue),e(Iue,K2r),e(zc,Z2r),e(Ar,e1r),e(Ar,wA),e(wA,o1r),e(wA,jue),e(jue,r1r),e(wA,t1r),e(Ar,a1r),e(Ar,Tt),g(AA,Tt,null),e(Tt,n1r),e(Tt,Nue),e(Nue,s1r),e(Tt,l1r),e(Tt,Vc),e(Vc,i1r),e(Vc,Due),e(Due,d1r),e(Vc,c1r),e(Vc,que),e(que,f1r),e(Vc,m1r),e(Tt,g1r),e(Tt,Gue),e(Gue,h1r),e(Tt,p1r),g(LA,Tt,null),e(Ar,_1r),e(Ar,Ao),g(BA,Ao,null),e(Ao,u1r),e(Ao,Oue),e(Oue,b1r),e(Ao,v1r),e(Ao,Cn),e(Cn,T1r),e(Cn,Xue),e(Xue,F1r),e(Cn,C1r),e(Cn,zue),e(zue,M1r),e(Cn,E1r),e(Cn,Vue),e(Vue,y1r),e(Cn,w1r),e(Ao,A1r),e(Ao,Mn),e(Mn,jF),e(jF,Wue),e(Wue,L1r),e(jF,B1r),e(jF,jO),e(jO,k1r),e(jF,x1r),e(Mn,R1r),e(Mn,NF),e(NF,Que),e(Que,S1r),e(NF,P1r),e(NF,NO),e(NO,$1r),e(NF,I1r),e(Mn,j1r),e(Mn,DF),e(DF,Hue),e(Hue,N1r),e(DF,D1r),e(DF,DO),e(DO,q1r),e(DF,G1r),e(Mn,O1r),e(Mn,qF),e(qF,Uue),e(Uue,X1r),e(qF,z1r),e(qF,qO),e(qO,V1r),e(qF,W1r),e(Ao,Q1r),e(Ao,Jue),e(Jue,H1r),e(Ao,U1r),g(kA,Ao,null),b(d,K9e,u),b(d,Wc,u),e(Wc,GF),e(GF,Yue),g(xA,Yue,null),e(Wc,J1r),e(Wc,Kue),e(Kue,Y1r),b(d,Z9e,u),b(d,Lr,u),g(RA,Lr,null),e(Lr,K1r),e(Lr,Qc),e(Qc,Z1r),e(Qc,Zue),e(Zue,ebr),e(Qc,obr),e(Qc,e2e),e(e2e,rbr),e(Qc,tbr),e(Lr,abr),e(Lr,SA),e(SA,nbr),e(SA,o2e),e(o2e,sbr),e(SA,lbr),e(Lr,ibr),e(Lr,Ft),g(PA,Ft,null),e(Ft,dbr),e(Ft,r2e),e(r2e,cbr),e(Ft,fbr),e(Ft,Hc),e(Hc,mbr),e(Hc,t2e),e(t2e,gbr),e(Hc,hbr),e(Hc,a2e),e(a2e,pbr),e(Hc,_br),e(Ft,ubr),e(Ft,n2e),e(n2e,bbr),e(Ft,vbr),g($A,Ft,null),e(Lr,Tbr),e(Lr,Lo),g(IA,Lo,null),e(Lo,Fbr),e(Lo,s2e),e(s2e,Cbr),e(Lo,Mbr),e(Lo,En),e(En,Ebr),e(En,l2e),e(l2e,ybr),e(En,wbr),e(En,i2e),e(i2e,Abr),e(En,Lbr),e(En,d2e),e(d2e,Bbr),e(En,kbr),e(Lo,xbr),e(Lo,fe),e(fe,OF),e(OF,c2e),e(c2e,Rbr),e(OF,Sbr),e(OF,GO),e(GO,Pbr),e(OF,$br),e(fe,Ibr),e(fe,XF),e(XF,f2e),e(f2e,jbr),e(XF,Nbr),e(XF,OO),e(OO,Dbr),e(XF,qbr),e(fe,Gbr),e(fe,zF),e(zF,m2e),e(m2e,Obr),e(zF,Xbr),e(zF,XO),e(XO,zbr),e(zF,Vbr),e(fe,Wbr),e(fe,VF),e(VF,g2e),e(g2e,Qbr),e(VF,Hbr),e(VF,zO),e(zO,Ubr),e(VF,Jbr),e(fe,Ybr),e(fe,WF),e(WF,h2e),e(h2e,Kbr),e(WF,Zbr),e(WF,VO),e(VO,e5r),e(WF,o5r),e(fe,r5r),e(fe,QF),e(QF,p2e),e(p2e,t5r),e(QF,a5r),e(QF,WO),e(WO,n5r),e(QF,s5r),e(fe,l5r),e(fe,HF),e(HF,_2e),e(_2e,i5r),e(HF,d5r),e(HF,QO),e(QO,c5r),e(HF,f5r),e(fe,m5r),e(fe,UF),e(UF,u2e),e(u2e,g5r),e(UF,h5r),e(UF,HO),e(HO,p5r),e(UF,_5r),e(fe,u5r),e(fe,JF),e(JF,b2e),e(b2e,b5r),e(JF,v5r),e(JF,UO),e(UO,T5r),e(JF,F5r),e(fe,C5r),e(fe,YF),e(YF,v2e),e(v2e,M5r),e(YF,E5r),e(YF,JO),e(JO,y5r),e(YF,w5r),e(fe,A5r),e(fe,KF),e(KF,T2e),e(T2e,L5r),e(KF,B5r),e(KF,YO),e(YO,k5r),e(KF,x5r),e(Lo,R5r),e(Lo,F2e),e(F2e,S5r),e(Lo,P5r),g(jA,Lo,null),b(d,eBe,u),b(d,Uc,u),e(Uc,ZF),e(ZF,C2e),g(NA,C2e,null),e(Uc,$5r),e(Uc,M2e),e(M2e,I5r),b(d,oBe,u),b(d,Br,u),g(DA,Br,null),e(Br,j5r),e(Br,Jc),e(Jc,N5r),e(Jc,E2e),e(E2e,D5r),e(Jc,q5r),e(Jc,y2e),e(y2e,G5r),e(Jc,O5r),e(Br,X5r),e(Br,qA),e(qA,z5r),e(qA,w2e),e(w2e,V5r),e(qA,W5r),e(Br,Q5r),e(Br,Ct),g(GA,Ct,null),e(Ct,H5r),e(Ct,A2e),e(A2e,U5r),e(Ct,J5r),e(Ct,Yc),e(Yc,Y5r),e(Yc,L2e),e(L2e,K5r),e(Yc,Z5r),e(Yc,B2e),e(B2e,evr),e(Yc,ovr),e(Ct,rvr),e(Ct,k2e),e(k2e,tvr),e(Ct,avr),g(OA,Ct,null),e(Br,nvr),e(Br,Bo),g(XA,Bo,null),e(Bo,svr),e(Bo,x2e),e(x2e,lvr),e(Bo,ivr),e(Bo,yn),e(yn,dvr),e(yn,R2e),e(R2e,cvr),e(yn,fvr),e(yn,S2e),e(S2e,mvr),e(yn,gvr),e(yn,P2e),e(P2e,hvr),e(yn,pvr),e(Bo,_vr),e(Bo,ve),e(ve,eC),e(eC,$2e),e($2e,uvr),e(eC,bvr),e(eC,KO),e(KO,vvr),e(eC,Tvr),e(ve,Fvr),e(ve,oC),e(oC,I2e),e(I2e,Cvr),e(oC,Mvr),e(oC,ZO),e(ZO,Evr),e(oC,yvr),e(ve,wvr),e(ve,rC),e(rC,j2e),e(j2e,Avr),e(rC,Lvr),e(rC,eX),e(eX,Bvr),e(rC,kvr),e(ve,xvr),e(ve,tC),e(tC,N2e),e(N2e,Rvr),e(tC,Svr),e(tC,oX),e(oX,Pvr),e(tC,$vr),e(ve,Ivr),e(ve,aC),e(aC,D2e),e(D2e,jvr),e(aC,Nvr),e(aC,rX),e(rX,Dvr),e(aC,qvr),e(ve,Gvr),e(ve,nC),e(nC,q2e),e(q2e,Ovr),e(nC,Xvr),e(nC,tX),e(tX,zvr),e(nC,Vvr),e(ve,Wvr),e(ve,sC),e(sC,G2e),e(G2e,Qvr),e(sC,Hvr),e(sC,aX),e(aX,Uvr),e(sC,Jvr),e(ve,Yvr),e(ve,lC),e(lC,O2e),e(O2e,Kvr),e(lC,Zvr),e(lC,nX),e(nX,e6r),e(lC,o6r),e(ve,r6r),e(ve,iC),e(iC,X2e),e(X2e,t6r),e(iC,a6r),e(iC,sX),e(sX,n6r),e(iC,s6r),e(Bo,l6r),e(Bo,z2e),e(z2e,i6r),e(Bo,d6r),g(zA,Bo,null),b(d,rBe,u),b(d,Kc,u),e(Kc,dC),e(dC,V2e),g(VA,V2e,null),e(Kc,c6r),e(Kc,W2e),e(W2e,f6r),b(d,tBe,u),b(d,kr,u),g(WA,kr,null),e(kr,m6r),e(kr,Zc),e(Zc,g6r),e(Zc,Q2e),e(Q2e,h6r),e(Zc,p6r),e(Zc,H2e),e(H2e,_6r),e(Zc,u6r),e(kr,b6r),e(kr,QA),e(QA,v6r),e(QA,U2e),e(U2e,T6r),e(QA,F6r),e(kr,C6r),e(kr,Mt),g(HA,Mt,null),e(Mt,M6r),e(Mt,J2e),e(J2e,E6r),e(Mt,y6r),e(Mt,ef),e(ef,w6r),e(ef,Y2e),e(Y2e,A6r),e(ef,L6r),e(ef,K2e),e(K2e,B6r),e(ef,k6r),e(Mt,x6r),e(Mt,Z2e),e(Z2e,R6r),e(Mt,S6r),g(UA,Mt,null),e(kr,P6r),e(kr,ko),g(JA,ko,null),e(ko,$6r),e(ko,e1e),e(e1e,I6r),e(ko,j6r),e(ko,wn),e(wn,N6r),e(wn,o1e),e(o1e,D6r),e(wn,q6r),e(wn,r1e),e(r1e,G6r),e(wn,O6r),e(wn,t1e),e(t1e,X6r),e(wn,z6r),e(ko,V6r),e(ko,Te),e(Te,cC),e(cC,a1e),e(a1e,W6r),e(cC,Q6r),e(cC,lX),e(lX,H6r),e(cC,U6r),e(Te,J6r),e(Te,fC),e(fC,n1e),e(n1e,Y6r),e(fC,K6r),e(fC,iX),e(iX,Z6r),e(fC,eTr),e(Te,oTr),e(Te,mC),e(mC,s1e),e(s1e,rTr),e(mC,tTr),e(mC,dX),e(dX,aTr),e(mC,nTr),e(Te,sTr),e(Te,gC),e(gC,l1e),e(l1e,lTr),e(gC,iTr),e(gC,cX),e(cX,dTr),e(gC,cTr),e(Te,fTr),e(Te,hC),e(hC,i1e),e(i1e,mTr),e(hC,gTr),e(hC,fX),e(fX,hTr),e(hC,pTr),e(Te,_Tr),e(Te,pC),e(pC,d1e),e(d1e,uTr),e(pC,bTr),e(pC,mX),e(mX,vTr),e(pC,TTr),e(Te,FTr),e(Te,_C),e(_C,c1e),e(c1e,CTr),e(_C,MTr),e(_C,gX),e(gX,ETr),e(_C,yTr),e(Te,wTr),e(Te,uC),e(uC,f1e),e(f1e,ATr),e(uC,LTr),e(uC,hX),e(hX,BTr),e(uC,kTr),e(Te,xTr),e(Te,bC),e(bC,m1e),e(m1e,RTr),e(bC,STr),e(bC,pX),e(pX,PTr),e(bC,$Tr),e(ko,ITr),e(ko,g1e),e(g1e,jTr),e(ko,NTr),g(YA,ko,null),b(d,aBe,u),b(d,of,u),e(of,vC),e(vC,h1e),g(KA,h1e,null),e(of,DTr),e(of,p1e),e(p1e,qTr),b(d,nBe,u),b(d,xr,u),g(ZA,xr,null),e(xr,GTr),e(xr,rf),e(rf,OTr),e(rf,_1e),e(_1e,XTr),e(rf,zTr),e(rf,u1e),e(u1e,VTr),e(rf,WTr),e(xr,QTr),e(xr,e0),e(e0,HTr),e(e0,b1e),e(b1e,UTr),e(e0,JTr),e(xr,YTr),e(xr,Et),g(o0,Et,null),e(Et,KTr),e(Et,v1e),e(v1e,ZTr),e(Et,e8r),e(Et,tf),e(tf,o8r),e(tf,T1e),e(T1e,r8r),e(tf,t8r),e(tf,F1e),e(F1e,a8r),e(tf,n8r),e(Et,s8r),e(Et,C1e),e(C1e,l8r),e(Et,i8r),g(r0,Et,null),e(xr,d8r),e(xr,xo),g(t0,xo,null),e(xo,c8r),e(xo,M1e),e(M1e,f8r),e(xo,m8r),e(xo,An),e(An,g8r),e(An,E1e),e(E1e,h8r),e(An,p8r),e(An,y1e),e(y1e,_8r),e(An,u8r),e(An,w1e),e(w1e,b8r),e(An,v8r),e(xo,T8r),e(xo,Fe),e(Fe,TC),e(TC,A1e),e(A1e,F8r),e(TC,C8r),e(TC,_X),e(_X,M8r),e(TC,E8r),e(Fe,y8r),e(Fe,FC),e(FC,L1e),e(L1e,w8r),e(FC,A8r),e(FC,uX),e(uX,L8r),e(FC,B8r),e(Fe,k8r),e(Fe,CC),e(CC,B1e),e(B1e,x8r),e(CC,R8r),e(CC,bX),e(bX,S8r),e(CC,P8r),e(Fe,$8r),e(Fe,MC),e(MC,k1e),e(k1e,I8r),e(MC,j8r),e(MC,vX),e(vX,N8r),e(MC,D8r),e(Fe,q8r),e(Fe,EC),e(EC,x1e),e(x1e,G8r),e(EC,O8r),e(EC,TX),e(TX,X8r),e(EC,z8r),e(Fe,V8r),e(Fe,yC),e(yC,R1e),e(R1e,W8r),e(yC,Q8r),e(yC,FX),e(FX,H8r),e(yC,U8r),e(Fe,J8r),e(Fe,wC),e(wC,S1e),e(S1e,Y8r),e(wC,K8r),e(wC,CX),e(CX,Z8r),e(wC,eFr),e(Fe,oFr),e(Fe,AC),e(AC,P1e),e(P1e,rFr),e(AC,tFr),e(AC,MX),e(MX,aFr),e(AC,nFr),e(Fe,sFr),e(Fe,LC),e(LC,$1e),e($1e,lFr),e(LC,iFr),e(LC,EX),e(EX,dFr),e(LC,cFr),e(xo,fFr),e(xo,I1e),e(I1e,mFr),e(xo,gFr),g(a0,xo,null),b(d,sBe,u),b(d,af,u),e(af,BC),e(BC,j1e),g(n0,j1e,null),e(af,hFr),e(af,N1e),e(N1e,pFr),b(d,lBe,u),b(d,Rr,u),g(s0,Rr,null),e(Rr,_Fr),e(Rr,nf),e(nf,uFr),e(nf,D1e),e(D1e,bFr),e(nf,vFr),e(nf,q1e),e(q1e,TFr),e(nf,FFr),e(Rr,CFr),e(Rr,l0),e(l0,MFr),e(l0,G1e),e(G1e,EFr),e(l0,yFr),e(Rr,wFr),e(Rr,yt),g(i0,yt,null),e(yt,AFr),e(yt,O1e),e(O1e,LFr),e(yt,BFr),e(yt,sf),e(sf,kFr),e(sf,X1e),e(X1e,xFr),e(sf,RFr),e(sf,z1e),e(z1e,SFr),e(sf,PFr),e(yt,$Fr),e(yt,V1e),e(V1e,IFr),e(yt,jFr),g(d0,yt,null),e(Rr,NFr),e(Rr,Ro),g(c0,Ro,null),e(Ro,DFr),e(Ro,W1e),e(W1e,qFr),e(Ro,GFr),e(Ro,Ln),e(Ln,OFr),e(Ln,Q1e),e(Q1e,XFr),e(Ln,zFr),e(Ln,H1e),e(H1e,VFr),e(Ln,WFr),e(Ln,U1e),e(U1e,QFr),e(Ln,HFr),e(Ro,UFr),e(Ro,Ce),e(Ce,kC),e(kC,J1e),e(J1e,JFr),e(kC,YFr),e(kC,yX),e(yX,KFr),e(kC,ZFr),e(Ce,eCr),e(Ce,xC),e(xC,Y1e),e(Y1e,oCr),e(xC,rCr),e(xC,wX),e(wX,tCr),e(xC,aCr),e(Ce,nCr),e(Ce,RC),e(RC,K1e),e(K1e,sCr),e(RC,lCr),e(RC,AX),e(AX,iCr),e(RC,dCr),e(Ce,cCr),e(Ce,SC),e(SC,Z1e),e(Z1e,fCr),e(SC,mCr),e(SC,LX),e(LX,gCr),e(SC,hCr),e(Ce,pCr),e(Ce,PC),e(PC,ebe),e(ebe,_Cr),e(PC,uCr),e(PC,BX),e(BX,bCr),e(PC,vCr),e(Ce,TCr),e(Ce,$C),e($C,obe),e(obe,FCr),e($C,CCr),e($C,kX),e(kX,MCr),e($C,ECr),e(Ce,yCr),e(Ce,IC),e(IC,rbe),e(rbe,wCr),e(IC,ACr),e(IC,xX),e(xX,LCr),e(IC,BCr),e(Ce,kCr),e(Ce,jC),e(jC,tbe),e(tbe,xCr),e(jC,RCr),e(jC,RX),e(RX,SCr),e(jC,PCr),e(Ce,$Cr),e(Ce,NC),e(NC,abe),e(abe,ICr),e(NC,jCr),e(NC,SX),e(SX,NCr),e(NC,DCr),e(Ro,qCr),e(Ro,nbe),e(nbe,GCr),e(Ro,OCr),g(f0,Ro,null),b(d,iBe,u),b(d,lf,u),e(lf,DC),e(DC,sbe),g(m0,sbe,null),e(lf,XCr),e(lf,lbe),e(lbe,zCr),b(d,dBe,u),b(d,Sr,u),g(g0,Sr,null),e(Sr,VCr),e(Sr,df),e(df,WCr),e(df,ibe),e(ibe,QCr),e(df,HCr),e(df,dbe),e(dbe,UCr),e(df,JCr),e(Sr,YCr),e(Sr,h0),e(h0,KCr),e(h0,cbe),e(cbe,ZCr),e(h0,e4r),e(Sr,o4r),e(Sr,wt),g(p0,wt,null),e(wt,r4r),e(wt,fbe),e(fbe,t4r),e(wt,a4r),e(wt,cf),e(cf,n4r),e(cf,mbe),e(mbe,s4r),e(cf,l4r),e(cf,gbe),e(gbe,i4r),e(cf,d4r),e(wt,c4r),e(wt,hbe),e(hbe,f4r),e(wt,m4r),g(_0,wt,null),e(Sr,g4r),e(Sr,So),g(u0,So,null),e(So,h4r),e(So,pbe),e(pbe,p4r),e(So,_4r),e(So,Bn),e(Bn,u4r),e(Bn,_be),e(_be,b4r),e(Bn,v4r),e(Bn,ube),e(ube,T4r),e(Bn,F4r),e(Bn,bbe),e(bbe,C4r),e(Bn,M4r),e(So,E4r),e(So,so),e(so,qC),e(qC,vbe),e(vbe,y4r),e(qC,w4r),e(qC,PX),e(PX,A4r),e(qC,L4r),e(so,B4r),e(so,GC),e(GC,Tbe),e(Tbe,k4r),e(GC,x4r),e(GC,$X),e($X,R4r),e(GC,S4r),e(so,P4r),e(so,OC),e(OC,Fbe),e(Fbe,$4r),e(OC,I4r),e(OC,IX),e(IX,j4r),e(OC,N4r),e(so,D4r),e(so,XC),e(XC,Cbe),e(Cbe,q4r),e(XC,G4r),e(XC,jX),e(jX,O4r),e(XC,X4r),e(so,z4r),e(so,zC),e(zC,Mbe),e(Mbe,V4r),e(zC,W4r),e(zC,NX),e(NX,Q4r),e(zC,H4r),e(so,U4r),e(so,VC),e(VC,Ebe),e(Ebe,J4r),e(VC,Y4r),e(VC,DX),e(DX,K4r),e(VC,Z4r),e(so,eMr),e(so,WC),e(WC,ybe),e(ybe,oMr),e(WC,rMr),e(WC,qX),e(qX,tMr),e(WC,aMr),e(So,nMr),e(So,wbe),e(wbe,sMr),e(So,lMr),g(b0,So,null),b(d,cBe,u),b(d,ff,u),e(ff,QC),e(QC,Abe),g(v0,Abe,null),e(ff,iMr),e(ff,Lbe),e(Lbe,dMr),b(d,fBe,u),b(d,Pr,u),g(T0,Pr,null),e(Pr,cMr),e(Pr,mf),e(mf,fMr),e(mf,Bbe),e(Bbe,mMr),e(mf,gMr),e(mf,kbe),e(kbe,hMr),e(mf,pMr),e(Pr,_Mr),e(Pr,F0),e(F0,uMr),e(F0,xbe),e(xbe,bMr),e(F0,vMr),e(Pr,TMr),e(Pr,At),g(C0,At,null),e(At,FMr),e(At,Rbe),e(Rbe,CMr),e(At,MMr),e(At,gf),e(gf,EMr),e(gf,Sbe),e(Sbe,yMr),e(gf,wMr),e(gf,Pbe),e(Pbe,AMr),e(gf,LMr),e(At,BMr),e(At,$be),e($be,kMr),e(At,xMr),g(M0,At,null),e(Pr,RMr),e(Pr,Po),g(E0,Po,null),e(Po,SMr),e(Po,Ibe),e(Ibe,PMr),e(Po,$Mr),e(Po,kn),e(kn,IMr),e(kn,jbe),e(jbe,jMr),e(kn,NMr),e(kn,Nbe),e(Nbe,DMr),e(kn,qMr),e(kn,Dbe),e(Dbe,GMr),e(kn,OMr),e(Po,XMr),e(Po,lo),e(lo,HC),e(HC,qbe),e(qbe,zMr),e(HC,VMr),e(HC,GX),e(GX,WMr),e(HC,QMr),e(lo,HMr),e(lo,UC),e(UC,Gbe),e(Gbe,UMr),e(UC,JMr),e(UC,OX),e(OX,YMr),e(UC,KMr),e(lo,ZMr),e(lo,JC),e(JC,Obe),e(Obe,eEr),e(JC,oEr),e(JC,XX),e(XX,rEr),e(JC,tEr),e(lo,aEr),e(lo,YC),e(YC,Xbe),e(Xbe,nEr),e(YC,sEr),e(YC,zX),e(zX,lEr),e(YC,iEr),e(lo,dEr),e(lo,KC),e(KC,zbe),e(zbe,cEr),e(KC,fEr),e(KC,VX),e(VX,mEr),e(KC,gEr),e(lo,hEr),e(lo,ZC),e(ZC,Vbe),e(Vbe,pEr),e(ZC,_Er),e(ZC,WX),e(WX,uEr),e(ZC,bEr),e(lo,vEr),e(lo,e4),e(e4,Wbe),e(Wbe,TEr),e(e4,FEr),e(e4,QX),e(QX,CEr),e(e4,MEr),e(Po,EEr),e(Po,Qbe),e(Qbe,yEr),e(Po,wEr),g(y0,Po,null),b(d,mBe,u),b(d,hf,u),e(hf,o4),e(o4,Hbe),g(w0,Hbe,null),e(hf,AEr),e(hf,Ube),e(Ube,LEr),b(d,gBe,u),b(d,$r,u),g(A0,$r,null),e($r,BEr),e($r,pf),e(pf,kEr),e(pf,Jbe),e(Jbe,xEr),e(pf,REr),e(pf,Ybe),e(Ybe,SEr),e(pf,PEr),e($r,$Er),e($r,L0),e(L0,IEr),e(L0,Kbe),e(Kbe,jEr),e(L0,NEr),e($r,DEr),e($r,Lt),g(B0,Lt,null),e(Lt,qEr),e(Lt,Zbe),e(Zbe,GEr),e(Lt,OEr),e(Lt,_f),e(_f,XEr),e(_f,e5e),e(e5e,zEr),e(_f,VEr),e(_f,o5e),e(o5e,WEr),e(_f,QEr),e(Lt,HEr),e(Lt,r5e),e(r5e,UEr),e(Lt,JEr),g(k0,Lt,null),e($r,YEr),e($r,$o),g(x0,$o,null),e($o,KEr),e($o,t5e),e(t5e,ZEr),e($o,e3r),e($o,xn),e(xn,o3r),e(xn,a5e),e(a5e,r3r),e(xn,t3r),e(xn,n5e),e(n5e,a3r),e(xn,n3r),e(xn,s5e),e(s5e,s3r),e(xn,l3r),e($o,i3r),e($o,l5e),e(l5e,r4),e(r4,i5e),e(i5e,d3r),e(r4,c3r),e(r4,HX),e(HX,f3r),e(r4,m3r),e($o,g3r),e($o,d5e),e(d5e,h3r),e($o,p3r),g(R0,$o,null),b(d,hBe,u),b(d,uf,u),e(uf,t4),e(t4,c5e),g(S0,c5e,null),e(uf,_3r),e(uf,f5e),e(f5e,u3r),b(d,pBe,u),b(d,Ir,u),g(P0,Ir,null),e(Ir,b3r),e(Ir,bf),e(bf,v3r),e(bf,m5e),e(m5e,T3r),e(bf,F3r),e(bf,g5e),e(g5e,C3r),e(bf,M3r),e(Ir,E3r),e(Ir,$0),e($0,y3r),e($0,h5e),e(h5e,w3r),e($0,A3r),e(Ir,L3r),e(Ir,Bt),g(I0,Bt,null),e(Bt,B3r),e(Bt,p5e),e(p5e,k3r),e(Bt,x3r),e(Bt,vf),e(vf,R3r),e(vf,_5e),e(_5e,S3r),e(vf,P3r),e(vf,u5e),e(u5e,$3r),e(vf,I3r),e(Bt,j3r),e(Bt,b5e),e(b5e,N3r),e(Bt,D3r),g(j0,Bt,null),e(Ir,q3r),e(Ir,Io),g(N0,Io,null),e(Io,G3r),e(Io,v5e),e(v5e,O3r),e(Io,X3r),e(Io,Rn),e(Rn,z3r),e(Rn,T5e),e(T5e,V3r),e(Rn,W3r),e(Rn,F5e),e(F5e,Q3r),e(Rn,H3r),e(Rn,C5e),e(C5e,U3r),e(Rn,J3r),e(Io,Y3r),e(Io,D0),e(D0,a4),e(a4,M5e),e(M5e,K3r),e(a4,Z3r),e(a4,UX),e(UX,eyr),e(a4,oyr),e(D0,ryr),e(D0,n4),e(n4,E5e),e(E5e,tyr),e(n4,ayr),e(n4,JX),e(JX,nyr),e(n4,syr),e(Io,lyr),e(Io,y5e),e(y5e,iyr),e(Io,dyr),g(q0,Io,null),b(d,_Be,u),b(d,Tf,u),e(Tf,s4),e(s4,w5e),g(G0,w5e,null),e(Tf,cyr),e(Tf,A5e),e(A5e,fyr),b(d,uBe,u),b(d,jr,u),g(O0,jr,null),e(jr,myr),e(jr,Ff),e(Ff,gyr),e(Ff,L5e),e(L5e,hyr),e(Ff,pyr),e(Ff,B5e),e(B5e,_yr),e(Ff,uyr),e(jr,byr),e(jr,X0),e(X0,vyr),e(X0,k5e),e(k5e,Tyr),e(X0,Fyr),e(jr,Cyr),e(jr,kt),g(z0,kt,null),e(kt,Myr),e(kt,x5e),e(x5e,Eyr),e(kt,yyr),e(kt,Cf),e(Cf,wyr),e(Cf,R5e),e(R5e,Ayr),e(Cf,Lyr),e(Cf,S5e),e(S5e,Byr),e(Cf,kyr),e(kt,xyr),e(kt,P5e),e(P5e,Ryr),e(kt,Syr),g(V0,kt,null),e(jr,Pyr),e(jr,jo),g(W0,jo,null),e(jo,$yr),e(jo,$5e),e($5e,Iyr),e(jo,jyr),e(jo,Sn),e(Sn,Nyr),e(Sn,I5e),e(I5e,Dyr),e(Sn,qyr),e(Sn,j5e),e(j5e,Gyr),e(Sn,Oyr),e(Sn,N5e),e(N5e,Xyr),e(Sn,zyr),e(jo,Vyr),e(jo,D5e),e(D5e,l4),e(l4,q5e),e(q5e,Wyr),e(l4,Qyr),e(l4,YX),e(YX,Hyr),e(l4,Uyr),e(jo,Jyr),e(jo,G5e),e(G5e,Yyr),e(jo,Kyr),g(Q0,jo,null),bBe=!0},p(d,[u]){const H0={};u&2&&(H0.$$scope={dirty:u,ctx:d}),Bf.$set(H0);const O5e={};u&2&&(O5e.$$scope={dirty:u,ctx:d}),dh.$set(O5e);const X5e={};u&2&&(X5e.$$scope={dirty:u,ctx:d}),Th.$set(X5e)},i(d){bBe||(h(ce.$$.fragment,d),h($a.$$.fragment,d),h(dM.$$.fragment,d),h(cM.$$.fragment,d),h(Bf.$$.fragment,d),h(fM.$$.fragment,d),h(mM.$$.fragment,d),h(pM.$$.fragment,d),h(_M.$$.fragment,d),h(uM.$$.fragment,d),h(bM.$$.fragment,d),h(vM.$$.fragment,d),h(CM.$$.fragment,d),h(MM.$$.fragment,d),h(EM.$$.fragment,d),h(yM.$$.fragment,d),h(wM.$$.fragment,d),h(BM.$$.fragment,d),h(dh.$$.fragment,d),h(kM.$$.fragment,d),h(xM.$$.fragment,d),h(RM.$$.fragment,d),h(SM.$$.fragment,d),h(IM.$$.fragment,d),h(Th.$$.fragment,d),h(jM.$$.fragment,d),h(NM.$$.fragment,d),h(DM.$$.fragment,d),h(qM.$$.fragment,d),h(OM.$$.fragment,d),h(XM.$$.fragment,d),h(zM.$$.fragment,d),h(VM.$$.fragment,d),h(WM.$$.fragment,d),h(QM.$$.fragment,d),h(UM.$$.fragment,d),h(JM.$$.fragment,d),h(YM.$$.fragment,d),h(KM.$$.fragment,d),h(ZM.$$.fragment,d),h(eE.$$.fragment,d),h(rE.$$.fragment,d),h(tE.$$.fragment,d),h(aE.$$.fragment,d),h(nE.$$.fragment,d),h(sE.$$.fragment,d),h(lE.$$.fragment,d),h(dE.$$.fragment,d),h(cE.$$.fragment,d),h(fE.$$.fragment,d),h(mE.$$.fragment,d),h(gE.$$.fragment,d),h(hE.$$.fragment,d),h(_E.$$.fragment,d),h(uE.$$.fragment,d),h(bE.$$.fragment,d),h(vE.$$.fragment,d),h(TE.$$.fragment,d),h(FE.$$.fragment,d),h(ME.$$.fragment,d),h(EE.$$.fragment,d),h(yE.$$.fragment,d),h(wE.$$.fragment,d),h(AE.$$.fragment,d),h(LE.$$.fragment,d),h(kE.$$.fragment,d),h(xE.$$.fragment,d),h(RE.$$.fragment,d),h(SE.$$.fragment,d),h(PE.$$.fragment,d),h($E.$$.fragment,d),h(jE.$$.fragment,d),h(NE.$$.fragment,d),h(DE.$$.fragment,d),h(qE.$$.fragment,d),h(GE.$$.fragment,d),h(OE.$$.fragment,d),h(zE.$$.fragment,d),h(VE.$$.fragment,d),h(WE.$$.fragment,d),h(QE.$$.fragment,d),h(HE.$$.fragment,d),h(UE.$$.fragment,d),h(YE.$$.fragment,d),h(KE.$$.fragment,d),h(ZE.$$.fragment,d),h(e3.$$.fragment,d),h(o3.$$.fragment,d),h(r3.$$.fragment,d),h(a3.$$.fragment,d),h(n3.$$.fragment,d),h(s3.$$.fragment,d),h(l3.$$.fragment,d),h(i3.$$.fragment,d),h(d3.$$.fragment,d),h(f3.$$.fragment,d),h(m3.$$.fragment,d),h(g3.$$.fragment,d),h(h3.$$.fragment,d),h(p3.$$.fragment,d),h(_3.$$.fragment,d),h(b3.$$.fragment,d),h(v3.$$.fragment,d),h(T3.$$.fragment,d),h(F3.$$.fragment,d),h(C3.$$.fragment,d),h(M3.$$.fragment,d),h(y3.$$.fragment,d),h(w3.$$.fragment,d),h(A3.$$.fragment,d),h(L3.$$.fragment,d),h(B3.$$.fragment,d),h(k3.$$.fragment,d),h(R3.$$.fragment,d),h(S3.$$.fragment,d),h(P3.$$.fragment,d),h($3.$$.fragment,d),h(I3.$$.fragment,d),h(j3.$$.fragment,d),h(D3.$$.fragment,d),h(q3.$$.fragment,d),h(G3.$$.fragment,d),h(O3.$$.fragment,d),h(X3.$$.fragment,d),h(z3.$$.fragment,d),h(W3.$$.fragment,d),h(Q3.$$.fragment,d),h(H3.$$.fragment,d),h(J3.$$.fragment,d),h(Y3.$$.fragment,d),h(K3.$$.fragment,d),h(ey.$$.fragment,d),h(oy.$$.fragment,d),h(ry.$$.fragment,d),h(ty.$$.fragment,d),h(ay.$$.fragment,d),h(ny.$$.fragment,d),h(ly.$$.fragment,d),h(iy.$$.fragment,d),h(dy.$$.fragment,d),h(cy.$$.fragment,d),h(fy.$$.fragment,d),h(my.$$.fragment,d),h(hy.$$.fragment,d),h(py.$$.fragment,d),h(_y.$$.fragment,d),h(uy.$$.fragment,d),h(by.$$.fragment,d),h(vy.$$.fragment,d),h(Fy.$$.fragment,d),h(Cy.$$.fragment,d),h(My.$$.fragment,d),h(Ey.$$.fragment,d),h(yy.$$.fragment,d),h(wy.$$.fragment,d),h(Ly.$$.fragment,d),h(By.$$.fragment,d),h(ky.$$.fragment,d),h(Ry.$$.fragment,d),h(Sy.$$.fragment,d),h(Py.$$.fragment,d),h(Iy.$$.fragment,d),h(jy.$$.fragment,d),h(Ny.$$.fragment,d),h(Dy.$$.fragment,d),h(qy.$$.fragment,d),h(Gy.$$.fragment,d),h(Xy.$$.fragment,d),h(zy.$$.fragment,d),h(Vy.$$.fragment,d),h(Wy.$$.fragment,d),h(Qy.$$.fragment,d),h(Hy.$$.fragment,d),h(Jy.$$.fragment,d),h(Yy.$$.fragment,d),h(Ky.$$.fragment,d),h(Zy.$$.fragment,d),h(ew.$$.fragment,d),h(ow.$$.fragment,d),h(tw.$$.fragment,d),h(aw.$$.fragment,d),h(nw.$$.fragment,d),h(lw.$$.fragment,d),h(iw.$$.fragment,d),h(dw.$$.fragment,d),h(fw.$$.fragment,d),h(mw.$$.fragment,d),h(gw.$$.fragment,d),h(hw.$$.fragment,d),h(pw.$$.fragment,d),h(_w.$$.fragment,d),h(bw.$$.fragment,d),h(vw.$$.fragment,d),h(Tw.$$.fragment,d),h(Fw.$$.fragment,d),h(Cw.$$.fragment,d),h(Mw.$$.fragment,d),h(yw.$$.fragment,d),h(ww.$$.fragment,d),h(Aw.$$.fragment,d),h(Lw.$$.fragment,d),h(Bw.$$.fragment,d),h(kw.$$.fragment,d),h(Rw.$$.fragment,d),h(Sw.$$.fragment,d),h(Pw.$$.fragment,d),h($w.$$.fragment,d),h(Iw.$$.fragment,d),h(jw.$$.fragment,d),h(Dw.$$.fragment,d),h(qw.$$.fragment,d),h(Gw.$$.fragment,d),h(Ow.$$.fragment,d),h(Xw.$$.fragment,d),h(zw.$$.fragment,d),h(Ww.$$.fragment,d),h(Qw.$$.fragment,d),h(Hw.$$.fragment,d),h(Uw.$$.fragment,d),h(Jw.$$.fragment,d),h(Yw.$$.fragment,d),h(Zw.$$.fragment,d),h(eA.$$.fragment,d),h(oA.$$.fragment,d),h(rA.$$.fragment,d),h(tA.$$.fragment,d),h(aA.$$.fragment,d),h(sA.$$.fragment,d),h(lA.$$.fragment,d),h(iA.$$.fragment,d),h(dA.$$.fragment,d),h(cA.$$.fragment,d),h(fA.$$.fragment,d),h(gA.$$.fragment,d),h(hA.$$.fragment,d),h(pA.$$.fragment,d),h(_A.$$.fragment,d),h(uA.$$.fragment,d),h(bA.$$.fragment,d),h(TA.$$.fragment,d),h(FA.$$.fragment,d),h(CA.$$.fragment,d),h(MA.$$.fragment,d),h(EA.$$.fragment,d),h(yA.$$.fragment,d),h(AA.$$.fragment,d),h(LA.$$.fragment,d),h(BA.$$.fragment,d),h(kA.$$.fragment,d),h(xA.$$.fragment,d),h(RA.$$.fragment,d),h(PA.$$.fragment,d),h($A.$$.fragment,d),h(IA.$$.fragment,d),h(jA.$$.fragment,d),h(NA.$$.fragment,d),h(DA.$$.fragment,d),h(GA.$$.fragment,d),h(OA.$$.fragment,d),h(XA.$$.fragment,d),h(zA.$$.fragment,d),h(VA.$$.fragment,d),h(WA.$$.fragment,d),h(HA.$$.fragment,d),h(UA.$$.fragment,d),h(JA.$$.fragment,d),h(YA.$$.fragment,d),h(KA.$$.fragment,d),h(ZA.$$.fragment,d),h(o0.$$.fragment,d),h(r0.$$.fragment,d),h(t0.$$.fragment,d),h(a0.$$.fragment,d),h(n0.$$.fragment,d),h(s0.$$.fragment,d),h(i0.$$.fragment,d),h(d0.$$.fragment,d),h(c0.$$.fragment,d),h(f0.$$.fragment,d),h(m0.$$.fragment,d),h(g0.$$.fragment,d),h(p0.$$.fragment,d),h(_0.$$.fragment,d),h(u0.$$.fragment,d),h(b0.$$.fragment,d),h(v0.$$.fragment,d),h(T0.$$.fragment,d),h(C0.$$.fragment,d),h(M0.$$.fragment,d),h(E0.$$.fragment,d),h(y0.$$.fragment,d),h(w0.$$.fragment,d),h(A0.$$.fragment,d),h(B0.$$.fragment,d),h(k0.$$.fragment,d),h(x0.$$.fragment,d),h(R0.$$.fragment,d),h(S0.$$.fragment,d),h(P0.$$.fragment,d),h(I0.$$.fragment,d),h(j0.$$.fragment,d),h(N0.$$.fragment,d),h(q0.$$.fragment,d),h(G0.$$.fragment,d),h(O0.$$.fragment,d),h(z0.$$.fragment,d),h(V0.$$.fragment,d),h(W0.$$.fragment,d),h(Q0.$$.fragment,d),bBe=!0)},o(d){p(ce.$$.fragment,d),p($a.$$.fragment,d),p(dM.$$.fragment,d),p(cM.$$.fragment,d),p(Bf.$$.fragment,d),p(fM.$$.fragment,d),p(mM.$$.fragment,d),p(pM.$$.fragment,d),p(_M.$$.fragment,d),p(uM.$$.fragment,d),p(bM.$$.fragment,d),p(vM.$$.fragment,d),p(CM.$$.fragment,d),p(MM.$$.fragment,d),p(EM.$$.fragment,d),p(yM.$$.fragment,d),p(wM.$$.fragment,d),p(BM.$$.fragment,d),p(dh.$$.fragment,d),p(kM.$$.fragment,d),p(xM.$$.fragment,d),p(RM.$$.fragment,d),p(SM.$$.fragment,d),p(IM.$$.fragment,d),p(Th.$$.fragment,d),p(jM.$$.fragment,d),p(NM.$$.fragment,d),p(DM.$$.fragment,d),p(qM.$$.fragment,d),p(OM.$$.fragment,d),p(XM.$$.fragment,d),p(zM.$$.fragment,d),p(VM.$$.fragment,d),p(WM.$$.fragment,d),p(QM.$$.fragment,d),p(UM.$$.fragment,d),p(JM.$$.fragment,d),p(YM.$$.fragment,d),p(KM.$$.fragment,d),p(ZM.$$.fragment,d),p(eE.$$.fragment,d),p(rE.$$.fragment,d),p(tE.$$.fragment,d),p(aE.$$.fragment,d),p(nE.$$.fragment,d),p(sE.$$.fragment,d),p(lE.$$.fragment,d),p(dE.$$.fragment,d),p(cE.$$.fragment,d),p(fE.$$.fragment,d),p(mE.$$.fragment,d),p(gE.$$.fragment,d),p(hE.$$.fragment,d),p(_E.$$.fragment,d),p(uE.$$.fragment,d),p(bE.$$.fragment,d),p(vE.$$.fragment,d),p(TE.$$.fragment,d),p(FE.$$.fragment,d),p(ME.$$.fragment,d),p(EE.$$.fragment,d),p(yE.$$.fragment,d),p(wE.$$.fragment,d),p(AE.$$.fragment,d),p(LE.$$.fragment,d),p(kE.$$.fragment,d),p(xE.$$.fragment,d),p(RE.$$.fragment,d),p(SE.$$.fragment,d),p(PE.$$.fragment,d),p($E.$$.fragment,d),p(jE.$$.fragment,d),p(NE.$$.fragment,d),p(DE.$$.fragment,d),p(qE.$$.fragment,d),p(GE.$$.fragment,d),p(OE.$$.fragment,d),p(zE.$$.fragment,d),p(VE.$$.fragment,d),p(WE.$$.fragment,d),p(QE.$$.fragment,d),p(HE.$$.fragment,d),p(UE.$$.fragment,d),p(YE.$$.fragment,d),p(KE.$$.fragment,d),p(ZE.$$.fragment,d),p(e3.$$.fragment,d),p(o3.$$.fragment,d),p(r3.$$.fragment,d),p(a3.$$.fragment,d),p(n3.$$.fragment,d),p(s3.$$.fragment,d),p(l3.$$.fragment,d),p(i3.$$.fragment,d),p(d3.$$.fragment,d),p(f3.$$.fragment,d),p(m3.$$.fragment,d),p(g3.$$.fragment,d),p(h3.$$.fragment,d),p(p3.$$.fragment,d),p(_3.$$.fragment,d),p(b3.$$.fragment,d),p(v3.$$.fragment,d),p(T3.$$.fragment,d),p(F3.$$.fragment,d),p(C3.$$.fragment,d),p(M3.$$.fragment,d),p(y3.$$.fragment,d),p(w3.$$.fragment,d),p(A3.$$.fragment,d),p(L3.$$.fragment,d),p(B3.$$.fragment,d),p(k3.$$.fragment,d),p(R3.$$.fragment,d),p(S3.$$.fragment,d),p(P3.$$.fragment,d),p($3.$$.fragment,d),p(I3.$$.fragment,d),p(j3.$$.fragment,d),p(D3.$$.fragment,d),p(q3.$$.fragment,d),p(G3.$$.fragment,d),p(O3.$$.fragment,d),p(X3.$$.fragment,d),p(z3.$$.fragment,d),p(W3.$$.fragment,d),p(Q3.$$.fragment,d),p(H3.$$.fragment,d),p(J3.$$.fragment,d),p(Y3.$$.fragment,d),p(K3.$$.fragment,d),p(ey.$$.fragment,d),p(oy.$$.fragment,d),p(ry.$$.fragment,d),p(ty.$$.fragment,d),p(ay.$$.fragment,d),p(ny.$$.fragment,d),p(ly.$$.fragment,d),p(iy.$$.fragment,d),p(dy.$$.fragment,d),p(cy.$$.fragment,d),p(fy.$$.fragment,d),p(my.$$.fragment,d),p(hy.$$.fragment,d),p(py.$$.fragment,d),p(_y.$$.fragment,d),p(uy.$$.fragment,d),p(by.$$.fragment,d),p(vy.$$.fragment,d),p(Fy.$$.fragment,d),p(Cy.$$.fragment,d),p(My.$$.fragment,d),p(Ey.$$.fragment,d),p(yy.$$.fragment,d),p(wy.$$.fragment,d),p(Ly.$$.fragment,d),p(By.$$.fragment,d),p(ky.$$.fragment,d),p(Ry.$$.fragment,d),p(Sy.$$.fragment,d),p(Py.$$.fragment,d),p(Iy.$$.fragment,d),p(jy.$$.fragment,d),p(Ny.$$.fragment,d),p(Dy.$$.fragment,d),p(qy.$$.fragment,d),p(Gy.$$.fragment,d),p(Xy.$$.fragment,d),p(zy.$$.fragment,d),p(Vy.$$.fragment,d),p(Wy.$$.fragment,d),p(Qy.$$.fragment,d),p(Hy.$$.fragment,d),p(Jy.$$.fragment,d),p(Yy.$$.fragment,d),p(Ky.$$.fragment,d),p(Zy.$$.fragment,d),p(ew.$$.fragment,d),p(ow.$$.fragment,d),p(tw.$$.fragment,d),p(aw.$$.fragment,d),p(nw.$$.fragment,d),p(lw.$$.fragment,d),p(iw.$$.fragment,d),p(dw.$$.fragment,d),p(fw.$$.fragment,d),p(mw.$$.fragment,d),p(gw.$$.fragment,d),p(hw.$$.fragment,d),p(pw.$$.fragment,d),p(_w.$$.fragment,d),p(bw.$$.fragment,d),p(vw.$$.fragment,d),p(Tw.$$.fragment,d),p(Fw.$$.fragment,d),p(Cw.$$.fragment,d),p(Mw.$$.fragment,d),p(yw.$$.fragment,d),p(ww.$$.fragment,d),p(Aw.$$.fragment,d),p(Lw.$$.fragment,d),p(Bw.$$.fragment,d),p(kw.$$.fragment,d),p(Rw.$$.fragment,d),p(Sw.$$.fragment,d),p(Pw.$$.fragment,d),p($w.$$.fragment,d),p(Iw.$$.fragment,d),p(jw.$$.fragment,d),p(Dw.$$.fragment,d),p(qw.$$.fragment,d),p(Gw.$$.fragment,d),p(Ow.$$.fragment,d),p(Xw.$$.fragment,d),p(zw.$$.fragment,d),p(Ww.$$.fragment,d),p(Qw.$$.fragment,d),p(Hw.$$.fragment,d),p(Uw.$$.fragment,d),p(Jw.$$.fragment,d),p(Yw.$$.fragment,d),p(Zw.$$.fragment,d),p(eA.$$.fragment,d),p(oA.$$.fragment,d),p(rA.$$.fragment,d),p(tA.$$.fragment,d),p(aA.$$.fragment,d),p(sA.$$.fragment,d),p(lA.$$.fragment,d),p(iA.$$.fragment,d),p(dA.$$.fragment,d),p(cA.$$.fragment,d),p(fA.$$.fragment,d),p(gA.$$.fragment,d),p(hA.$$.fragment,d),p(pA.$$.fragment,d),p(_A.$$.fragment,d),p(uA.$$.fragment,d),p(bA.$$.fragment,d),p(TA.$$.fragment,d),p(FA.$$.fragment,d),p(CA.$$.fragment,d),p(MA.$$.fragment,d),p(EA.$$.fragment,d),p(yA.$$.fragment,d),p(AA.$$.fragment,d),p(LA.$$.fragment,d),p(BA.$$.fragment,d),p(kA.$$.fragment,d),p(xA.$$.fragment,d),p(RA.$$.fragment,d),p(PA.$$.fragment,d),p($A.$$.fragment,d),p(IA.$$.fragment,d),p(jA.$$.fragment,d),p(NA.$$.fragment,d),p(DA.$$.fragment,d),p(GA.$$.fragment,d),p(OA.$$.fragment,d),p(XA.$$.fragment,d),p(zA.$$.fragment,d),p(VA.$$.fragment,d),p(WA.$$.fragment,d),p(HA.$$.fragment,d),p(UA.$$.fragment,d),p(JA.$$.fragment,d),p(YA.$$.fragment,d),p(KA.$$.fragment,d),p(ZA.$$.fragment,d),p(o0.$$.fragment,d),p(r0.$$.fragment,d),p(t0.$$.fragment,d),p(a0.$$.fragment,d),p(n0.$$.fragment,d),p(s0.$$.fragment,d),p(i0.$$.fragment,d),p(d0.$$.fragment,d),p(c0.$$.fragment,d),p(f0.$$.fragment,d),p(m0.$$.fragment,d),p(g0.$$.fragment,d),p(p0.$$.fragment,d),p(_0.$$.fragment,d),p(u0.$$.fragment,d),p(b0.$$.fragment,d),p(v0.$$.fragment,d),p(T0.$$.fragment,d),p(C0.$$.fragment,d),p(M0.$$.fragment,d),p(E0.$$.fragment,d),p(y0.$$.fragment,d),p(w0.$$.fragment,d),p(A0.$$.fragment,d),p(B0.$$.fragment,d),p(k0.$$.fragment,d),p(x0.$$.fragment,d),p(R0.$$.fragment,d),p(S0.$$.fragment,d),p(P0.$$.fragment,d),p(I0.$$.fragment,d),p(j0.$$.fragment,d),p(N0.$$.fragment,d),p(q0.$$.fragment,d),p(G0.$$.fragment,d),p(O0.$$.fragment,d),p(z0.$$.fragment,d),p(V0.$$.fragment,d),p(W0.$$.fragment,d),p(Q0.$$.fragment,d),bBe=!1},d(d){t(J),d&&t(Ae),d&&t(ie),_(ce),d&&t(Ef),d&&t(sa),d&&t(ye),d&&t(io),d&&t(wf),_($a,d),d&&t(co),d&&t(ge),d&&t(qo),d&&t(Ia),d&&t(v7e),d&&t(Si),_(dM),d&&t(T7e),d&&t(Nn),d&&t(F7e),_(cM,d),d&&t(C7e),d&&t(UL),d&&t(M7e),_(Bf,d),d&&t(E7e),d&&t(Pi),_(fM),d&&t(y7e),d&&t(Go),_(mM),_(pM),_(_M),_(uM),d&&t(w7e),d&&t(Ii),_(bM),d&&t(A7e),d&&t(Oo),_(vM),_(CM),_(MM),_(EM),d&&t(L7e),d&&t(ji),_(yM),d&&t(B7e),d&&t(Xo),_(wM),_(BM),_(dh),_(kM),_(xM),d&&t(k7e),d&&t(Ni),_(RM),d&&t(x7e),d&&t(zo),_(SM),_(IM),_(Th),_(jM),_(NM),d&&t(R7e),d&&t(qi),_(DM),d&&t(S7e),d&&t(Vo),_(qM),_(OM),_(XM),_(zM),_(VM),d&&t(P7e),d&&t(Xi),_(WM),d&&t($7e),d&&t(Wo),_(QM),_(UM),_(JM),_(YM),_(KM),d&&t(I7e),d&&t(Wi),_(ZM),d&&t(j7e),d&&t(Qo),_(eE),_(rE),_(tE),_(aE),_(nE),d&&t(N7e),d&&t(Ui),_(sE),d&&t(D7e),d&&t(Ho),_(lE),_(dE),_(cE),_(fE),_(mE),d&&t(q7e),d&&t(Ki),_(gE),d&&t(G7e),d&&t(Uo),_(hE),_(_E),_(uE),_(bE),_(vE),d&&t(O7e),d&&t(od),_(TE),d&&t(X7e),d&&t(Jo),_(FE),_(ME),_(EE),_(yE),_(wE),d&&t(z7e),d&&t(ad),_(AE),d&&t(V7e),d&&t(Yo),_(LE),_(kE),_(xE),_(RE),_(SE),d&&t(W7e),d&&t(ld),_(PE),d&&t(Q7e),d&&t(Ko),_($E),_(jE),_(NE),_(DE),_(qE),d&&t(H7e),d&&t(cd),_(GE),d&&t(U7e),d&&t(Zo),_(OE),_(zE),_(VE),_(WE),_(QE),d&&t(J7e),d&&t(gd),_(HE),d&&t(Y7e),d&&t(er),_(UE),_(YE),_(KE),_(ZE),_(e3),d&&t(K7e),d&&t(_d),_(o3),d&&t(Z7e),d&&t(or),_(r3),_(a3),_(n3),_(s3),_(l3),d&&t(e9e),d&&t(vd),_(i3),d&&t(o9e),d&&t(rr),_(d3),_(f3),_(m3),_(g3),_(h3),d&&t(r9e),d&&t(Cd),_(p3),d&&t(t9e),d&&t(tr),_(_3),_(b3),_(v3),_(T3),_(F3),d&&t(a9e),d&&t(yd),_(C3),d&&t(n9e),d&&t(ar),_(M3),_(y3),_(w3),_(A3),_(L3),d&&t(s9e),d&&t(Ld),_(B3),d&&t(l9e),d&&t(nr),_(k3),_(R3),_(S3),_(P3),_($3),d&&t(i9e),d&&t(Rd),_(I3),d&&t(d9e),d&&t(sr),_(j3),_(D3),_(q3),_(G3),_(O3),d&&t(c9e),d&&t($d),_(X3),d&&t(f9e),d&&t(lr),_(z3),_(W3),_(Q3),_(H3),_(J3),d&&t(m9e),d&&t(Nd),_(Y3),d&&t(g9e),d&&t(ir),_(K3),_(ey),_(oy),_(ry),_(ty),d&&t(h9e),d&&t(Od),_(ay),d&&t(p9e),d&&t(dr),_(ny),_(ly),_(iy),_(dy),_(cy),d&&t(_9e),d&&t(Wd),_(fy),d&&t(u9e),d&&t(cr),_(my),_(hy),_(py),_(_y),_(uy),d&&t(b9e),d&&t(Ud),_(by),d&&t(v9e),d&&t(fr),_(vy),_(Fy),_(Cy),_(My),_(Ey),d&&t(T9e),d&&t(Kd),_(yy),d&&t(F9e),d&&t(mr),_(wy),_(Ly),_(By),_(ky),_(Ry),d&&t(C9e),d&&t(oc),_(Sy),d&&t(M9e),d&&t(gr),_(Py),_(Iy),_(jy),_(Ny),_(Dy),d&&t(E9e),d&&t(ac),_(qy),d&&t(y9e),d&&t(hr),_(Gy),_(Xy),_(zy),_(Vy),_(Wy),d&&t(w9e),d&&t(lc),_(Qy),d&&t(A9e),d&&t(pr),_(Hy),_(Jy),_(Yy),_(Ky),_(Zy),d&&t(L9e),d&&t(cc),_(ew),d&&t(B9e),d&&t(_r),_(ow),_(tw),_(aw),_(nw),_(lw),d&&t(k9e),d&&t(gc),_(iw),d&&t(x9e),d&&t(ur),_(dw),_(fw),_(mw),_(gw),_(hw),d&&t(R9e),d&&t(_c),_(pw),d&&t(S9e),d&&t(br),_(_w),_(bw),_(vw),_(Tw),_(Fw),d&&t(P9e),d&&t(vc),_(Cw),d&&t($9e),d&&t(vr),_(Mw),_(yw),_(ww),_(Aw),_(Lw),d&&t(I9e),d&&t(Cc),_(Bw),d&&t(j9e),d&&t(Tr),_(kw),_(Rw),_(Sw),_(Pw),_($w),d&&t(N9e),d&&t(yc),_(Iw),d&&t(D9e),d&&t(Fr),_(jw),_(Dw),_(qw),_(Gw),_(Ow),d&&t(q9e),d&&t(Lc),_(Xw),d&&t(G9e),d&&t(Cr),_(zw),_(Ww),_(Qw),_(Hw),_(Uw),d&&t(O9e),d&&t(xc),_(Jw),d&&t(X9e),d&&t(Mr),_(Yw),_(Zw),_(eA),_(oA),_(rA),d&&t(z9e),d&&t(Pc),_(tA),d&&t(V9e),d&&t(Er),_(aA),_(sA),_(lA),_(iA),_(dA),d&&t(W9e),d&&t(jc),_(cA),d&&t(Q9e),d&&t(yr),_(fA),_(gA),_(hA),_(pA),_(_A),d&&t(H9e),d&&t(qc),_(uA),d&&t(U9e),d&&t(wr),_(bA),_(TA),_(FA),_(CA),_(MA),d&&t(J9e),d&&t(Xc),_(EA),d&&t(Y9e),d&&t(Ar),_(yA),_(AA),_(LA),_(BA),_(kA),d&&t(K9e),d&&t(Wc),_(xA),d&&t(Z9e),d&&t(Lr),_(RA),_(PA),_($A),_(IA),_(jA),d&&t(eBe),d&&t(Uc),_(NA),d&&t(oBe),d&&t(Br),_(DA),_(GA),_(OA),_(XA),_(zA),d&&t(rBe),d&&t(Kc),_(VA),d&&t(tBe),d&&t(kr),_(WA),_(HA),_(UA),_(JA),_(YA),d&&t(aBe),d&&t(of),_(KA),d&&t(nBe),d&&t(xr),_(ZA),_(o0),_(r0),_(t0),_(a0),d&&t(sBe),d&&t(af),_(n0),d&&t(lBe),d&&t(Rr),_(s0),_(i0),_(d0),_(c0),_(f0),d&&t(iBe),d&&t(lf),_(m0),d&&t(dBe),d&&t(Sr),_(g0),_(p0),_(_0),_(u0),_(b0),d&&t(cBe),d&&t(ff),_(v0),d&&t(fBe),d&&t(Pr),_(T0),_(C0),_(M0),_(E0),_(y0),d&&t(mBe),d&&t(hf),_(w0),d&&t(gBe),d&&t($r),_(A0),_(B0),_(k0),_(x0),_(R0),d&&t(hBe),d&&t(uf),_(S0),d&&t(pBe),d&&t(Ir),_(P0),_(I0),_(j0),_(N0),_(q0),d&&t(_Be),d&&t(Tf),_(G0),d&&t(uBe),d&&t(jr),_(O0),_(z0),_(V0),_(W0),_(Q0)}}}const put={local:"auto-classes",sections:[{local:"extending-the-auto-classes",title:"Extending the Auto Classes"},{local:"transformers.AutoConfig",title:"AutoConfig"},{local:"transformers.AutoTokenizer",title:"AutoTokenizer"},{local:"transformers.AutoFeatureExtractor",title:"AutoFeatureExtractor"},{local:"transformers.AutoProcessor",title:"AutoProcessor"},{local:"transformers.AutoModel",title:"AutoModel"},{local:"transformers.AutoModelForPreTraining",title:"AutoModelForPreTraining"},{local:"transformers.AutoModelForCausalLM",title:"AutoModelForCausalLM"},{local:"transformers.AutoModelForMaskedLM",title:"AutoModelForMaskedLM"},{local:"transformers.AutoModelForSeq2SeqLM",title:"AutoModelForSeq2SeqLM"},{local:"transformers.AutoModelForSequenceClassification",title:"AutoModelForSequenceClassification"},{local:"transformers.AutoModelForMultipleChoice",title:"AutoModelForMultipleChoice"},{local:"transformers.AutoModelForNextSentencePrediction",title:"AutoModelForNextSentencePrediction"},{local:"transformers.AutoModelForTokenClassification",title:"AutoModelForTokenClassification"},{local:"transformers.AutoModelForQuestionAnswering",title:"AutoModelForQuestionAnswering"},{local:"transformers.AutoModelForTableQuestionAnswering",title:"AutoModelForTableQuestionAnswering"},{local:"transformers.AutoModelForImageClassification",title:"AutoModelForImageClassification"},{local:"transformers.AutoModelForVision2Seq",title:"AutoModelForVision2Seq"},{local:"transformers.AutoModelForAudioClassification",title:"AutoModelForAudioClassification"},{local:"transformers.AutoModelForAudioFrameClassification",title:"AutoModelForAudioFrameClassification"},{local:"transformers.AutoModelForCTC",title:"AutoModelForCTC"},{local:"transformers.AutoModelForSpeechSeq2Seq",title:"AutoModelForSpeechSeq2Seq"},{local:"transformers.AutoModelForAudioXVector",title:"AutoModelForAudioXVector"},{local:"transformers.AutoModelForMaskedImageModeling",title:"AutoModelForMaskedImageModeling"},{local:"transformers.AutoModelForObjectDetection",title:"AutoModelForObjectDetection"},{local:"transformers.AutoModelForImageSegmentation",title:"AutoModelForImageSegmentation"},{local:"transformers.AutoModelForSemanticSegmentation",title:"AutoModelForSemanticSegmentation"},{local:"transformers.TFAutoModel",title:"TFAutoModel"},{local:"transformers.TFAutoModelForPreTraining",title:"TFAutoModelForPreTraining"},{local:"transformers.TFAutoModelForCausalLM",title:"TFAutoModelForCausalLM"},{local:"transformers.TFAutoModelForImageClassification",title:"TFAutoModelForImageClassification"},{local:"transformers.TFAutoModelForMaskedLM",title:"TFAutoModelForMaskedLM"},{local:"transformers.TFAutoModelForSeq2SeqLM",title:"TFAutoModelForSeq2SeqLM"},{local:"transformers.TFAutoModelForSequenceClassification",title:"TFAutoModelForSequenceClassification"},{local:"transformers.TFAutoModelForMultipleChoice",title:"TFAutoModelForMultipleChoice"},{local:"transformers.TFAutoModelForTableQuestionAnswering",title:"TFAutoModelForTableQuestionAnswering"},{local:"transformers.TFAutoModelForTokenClassification",title:"TFAutoModelForTokenClassification"},{local:"transformers.TFAutoModelForQuestionAnswering",title:"TFAutoModelForQuestionAnswering"},{local:"transformers.TFAutoModelForVision2Seq",title:"TFAutoModelForVision2Seq"},{local:"transformers.TFAutoModelForSpeechSeq2Seq",title:"TFAutoModelForSpeechSeq2Seq"},{local:"transformers.FlaxAutoModel",title:"FlaxAutoModel"},{local:"transformers.FlaxAutoModelForCausalLM",title:"FlaxAutoModelForCausalLM"},{local:"transformers.FlaxAutoModelForPreTraining",title:"FlaxAutoModelForPreTraining"},{local:"transformers.FlaxAutoModelForMaskedLM",title:"FlaxAutoModelForMaskedLM"},{local:"transformers.FlaxAutoModelForSeq2SeqLM",title:"FlaxAutoModelForSeq2SeqLM"},{local:"transformers.FlaxAutoModelForSequenceClassification",title:"FlaxAutoModelForSequenceClassification"},{local:"transformers.FlaxAutoModelForQuestionAnswering",title:"FlaxAutoModelForQuestionAnswering"},{local:"transformers.FlaxAutoModelForTokenClassification",title:"FlaxAutoModelForTokenClassification"},{local:"transformers.FlaxAutoModelForMultipleChoice",title:"FlaxAutoModelForMultipleChoice"},{local:"transformers.FlaxAutoModelForNextSentencePrediction",title:"FlaxAutoModelForNextSentencePrediction"},{local:"transformers.FlaxAutoModelForImageClassification",title:"FlaxAutoModelForImageClassification"},{local:"transformers.FlaxAutoModelForVision2Seq",title:"FlaxAutoModelForVision2Seq"}],title:"Auto Classes"};function _ut(yi,J,Ae){let{fw:ie}=J;return yi.$$set=me=>{"fw"in me&&Ae(0,ie=me.fw)},[ie]}class Mut extends lut{constructor(J){super();iut(this,J,_ut,hut,dut,{fw:0})}}export{Mut as default,put as metadata};
