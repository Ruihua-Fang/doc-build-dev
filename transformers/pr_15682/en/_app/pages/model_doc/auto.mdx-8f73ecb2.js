import{S as D_t,i as q_t,s as G_t,e as a,k as l,w as f,t as o,M as O_t,c as n,d as t,m as i,a as s,x as m,h as r,b as c,F as e,g as b,y as g,q as h,o as p,B as _}from"../../chunks/vendor-4833417e.js";import{T as Pyr}from"../../chunks/Tip-fffd6df1.js";import{D as E}from"../../chunks/Docstring-44c5af16.js";import{C as w}from"../../chunks/CodeBlock-90ffda97.js";import{I as z}from"../../chunks/IconCopyLink-4b81c553.js";import"../../chunks/CopyButton-04a16537.js";function X_t(yi){let J,Ae,ie,me,to,ce,ue,Do,wi,Ef,sa,Ai,Li,tM,yf,ye,io,Bi,Pn,aM,$n,In,nM,ki,jn,sM,xi,wf,$a;return{c(){J=a("p"),Ae=o("If your "),ie=a("code"),me=o("NewModelConfig"),to=o(" is a subclass of "),ce=a("code"),ue=o("PretrainedConfig"),Do=o(`, make sure its
`),wi=a("code"),Ef=o("model_type"),sa=o(" attribute is set to the same key you use when registering the config (here "),Ai=a("code"),Li=o('"new-model"'),tM=o(")."),yf=l(),ye=a("p"),io=o("Likewise, if your "),Bi=a("code"),Pn=o("NewModel"),aM=o(" is a subclass of "),$n=a("a"),In=o("PreTrainedModel"),nM=o(`, make sure its
`),ki=a("code"),jn=o("config_class"),sM=o(` attribute is set to the same class you use when registering the model (here
`),xi=a("code"),wf=o("NewModelConfig"),$a=o(")."),this.h()},l(co){J=n(co,"P",{});var ge=s(J);Ae=r(ge,"If your "),ie=n(ge,"CODE",{});var GL=s(ie);me=r(GL,"NewModelConfig"),GL.forEach(t),to=r(ge," is a subclass of "),ce=n(ge,"CODE",{});var Ri=s(ce);ue=r(Ri,"PretrainedConfig"),Ri.forEach(t),Do=r(ge,`, make sure its
`),wi=n(ge,"CODE",{});var OL=s(wi);Ef=r(OL,"model_type"),OL.forEach(t),sa=r(ge," attribute is set to the same key you use when registering the config (here "),Ai=n(ge,"CODE",{});var XL=s(Ai);Li=r(XL,'"new-model"'),XL.forEach(t),tM=r(ge,")."),ge.forEach(t),yf=i(co),ye=n(co,"P",{});var qo=s(ye);io=r(qo,"Likewise, if your "),Bi=n(qo,"CODE",{});var Ia=s(Bi);Pn=r(Ia,"NewModel"),Ia.forEach(t),aM=r(qo," is a subclass of "),$n=n(qo,"A",{href:!0});var zL=s($n);In=r(zL,"PreTrainedModel"),zL.forEach(t),nM=r(qo,`, make sure its
`),ki=n(qo,"CODE",{});var Af=s(ki);jn=r(Af,"config_class"),Af.forEach(t),sM=r(qo,` attribute is set to the same class you use when registering the model (here
`),xi=n(qo,"CODE",{});var VL=s(xi);wf=r(VL,"NewModelConfig"),VL.forEach(t),$a=r(qo,")."),qo.forEach(t),this.h()},h(){c($n,"href","/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel")},m(co,ge){b(co,J,ge),e(J,Ae),e(J,ie),e(ie,me),e(J,to),e(J,ce),e(ce,ue),e(J,Do),e(J,wi),e(wi,Ef),e(J,sa),e(J,Ai),e(Ai,Li),e(J,tM),b(co,yf,ge),b(co,ye,ge),e(ye,io),e(ye,Bi),e(Bi,Pn),e(ye,aM),e(ye,$n),e($n,In),e(ye,nM),e(ye,ki),e(ki,jn),e(ye,sM),e(ye,xi),e(xi,wf),e(ye,$a)},d(co){co&&t(J),co&&t(yf),co&&t(ye)}}}function z_t(yi){let J,Ae,ie,me,to;return{c(){J=a("p"),Ae=o("Passing "),ie=a("code"),me=o("use_auth_token=True"),to=o(" is required when you want to use a private model.")},l(ce){J=n(ce,"P",{});var ue=s(J);Ae=r(ue,"Passing "),ie=n(ue,"CODE",{});var Do=s(ie);me=r(Do,"use_auth_token=True"),Do.forEach(t),to=r(ue," is required when you want to use a private model."),ue.forEach(t)},m(ce,ue){b(ce,J,ue),e(J,Ae),e(J,ie),e(ie,me),e(J,to)},d(ce){ce&&t(J)}}}function V_t(yi){let J,Ae,ie,me,to;return{c(){J=a("p"),Ae=o("Passing "),ie=a("code"),me=o("use_auth_token=True"),to=o(" is required when you want to use a private model.")},l(ce){J=n(ce,"P",{});var ue=s(J);Ae=r(ue,"Passing "),ie=n(ue,"CODE",{});var Do=s(ie);me=r(Do,"use_auth_token=True"),Do.forEach(t),to=r(ue," is required when you want to use a private model."),ue.forEach(t)},m(ce,ue){b(ce,J,ue),e(J,Ae),e(J,ie),e(ie,me),e(J,to)},d(ce){ce&&t(J)}}}function W_t(yi){let J,Ae,ie,me,to,ce,ue,Do,wi,Ef,sa,Ai,Li,tM,yf,ye,io,Bi,Pn,aM,$n,In,nM,ki,jn,sM,xi,wf,$a,co,ge,GL,Ri,OL,XL,qo,Ia,zL,Af,VL,Txe,f7e,Si,Lf,DV,lM,Fxe,qV,Cxe,m7e,Nn,Mxe,GV,Exe,yxe,OV,wxe,Axe,g7e,iM,h7e,WL,Lxe,p7e,Bf,_7e,Pi,kf,XV,dM,Bxe,zV,kxe,u7e,Go,cM,xxe,fM,Rxe,QL,Sxe,Pxe,$xe,mM,Ixe,VV,jxe,Nxe,Dxe,fo,gM,qxe,WV,Gxe,Oxe,$i,Xxe,QV,zxe,Vxe,HV,Wxe,Qxe,Hxe,v,xf,UV,Uxe,Jxe,HL,Yxe,Kxe,Zxe,Rf,JV,eRe,oRe,UL,rRe,tRe,aRe,Sf,YV,nRe,sRe,JL,lRe,iRe,dRe,Pf,KV,cRe,fRe,YL,mRe,gRe,hRe,$f,ZV,pRe,_Re,KL,uRe,bRe,vRe,If,eW,TRe,FRe,ZL,CRe,MRe,ERe,jf,oW,yRe,wRe,e7,ARe,LRe,BRe,Nf,rW,kRe,xRe,o7,RRe,SRe,PRe,Df,tW,$Re,IRe,r7,jRe,NRe,DRe,qf,aW,qRe,GRe,t7,ORe,XRe,zRe,Gf,nW,VRe,WRe,a7,QRe,HRe,URe,Of,sW,JRe,YRe,n7,KRe,ZRe,eSe,Xf,lW,oSe,rSe,s7,tSe,aSe,nSe,zf,iW,sSe,lSe,l7,iSe,dSe,cSe,Vf,dW,fSe,mSe,i7,gSe,hSe,pSe,Wf,cW,_Se,uSe,d7,bSe,vSe,TSe,Qf,fW,FSe,CSe,c7,MSe,ESe,ySe,Hf,mW,wSe,ASe,f7,LSe,BSe,kSe,Uf,gW,xSe,RSe,m7,SSe,PSe,$Se,Jf,hW,ISe,jSe,g7,NSe,DSe,qSe,Yf,pW,GSe,OSe,h7,XSe,zSe,VSe,Kf,_W,WSe,QSe,p7,HSe,USe,JSe,Zf,uW,YSe,KSe,_7,ZSe,ePe,oPe,em,bW,rPe,tPe,u7,aPe,nPe,sPe,om,vW,lPe,iPe,b7,dPe,cPe,fPe,rm,TW,mPe,gPe,v7,hPe,pPe,_Pe,tm,FW,uPe,bPe,T7,vPe,TPe,FPe,am,CW,CPe,MPe,F7,EPe,yPe,wPe,nm,MW,APe,LPe,C7,BPe,kPe,xPe,sm,EW,RPe,SPe,M7,PPe,$Pe,IPe,lm,yW,jPe,NPe,E7,DPe,qPe,GPe,im,wW,OPe,XPe,y7,zPe,VPe,WPe,dm,AW,QPe,HPe,w7,UPe,JPe,YPe,cm,LW,KPe,ZPe,A7,e$e,o$e,r$e,fm,BW,t$e,a$e,L7,n$e,s$e,l$e,mm,kW,i$e,d$e,B7,c$e,f$e,m$e,gm,xW,g$e,h$e,k7,p$e,_$e,u$e,hm,RW,b$e,v$e,x7,T$e,F$e,C$e,pm,SW,M$e,E$e,R7,y$e,w$e,A$e,_m,PW,L$e,B$e,S7,k$e,x$e,R$e,um,$W,S$e,P$e,P7,$$e,I$e,j$e,bm,IW,N$e,D$e,$7,q$e,G$e,O$e,vm,jW,X$e,z$e,I7,V$e,W$e,Q$e,Tm,NW,H$e,U$e,j7,J$e,Y$e,K$e,Fm,DW,Z$e,eIe,N7,oIe,rIe,tIe,Cm,qW,aIe,nIe,D7,sIe,lIe,iIe,Mm,GW,dIe,cIe,q7,fIe,mIe,gIe,Em,OW,hIe,pIe,G7,_Ie,uIe,bIe,ym,XW,vIe,TIe,O7,FIe,CIe,MIe,wm,zW,EIe,yIe,X7,wIe,AIe,LIe,Am,VW,BIe,kIe,z7,xIe,RIe,SIe,Lm,WW,PIe,$Ie,V7,IIe,jIe,NIe,Bm,QW,DIe,qIe,W7,GIe,OIe,XIe,km,HW,zIe,VIe,Q7,WIe,QIe,HIe,xm,UW,UIe,JIe,H7,YIe,KIe,ZIe,Rm,JW,eje,oje,U7,rje,tje,aje,Sm,YW,nje,sje,J7,lje,ije,dje,Pm,KW,cje,fje,Y7,mje,gje,hje,$m,ZW,pje,_je,K7,uje,bje,vje,Im,eQ,Tje,Fje,Z7,Cje,Mje,Eje,jm,oQ,yje,wje,e9,Aje,Lje,Bje,Nm,rQ,kje,xje,o9,Rje,Sje,Pje,Dm,tQ,$je,Ije,r9,jje,Nje,Dje,qm,aQ,qje,Gje,t9,Oje,Xje,zje,Gm,nQ,Vje,Wje,a9,Qje,Hje,Uje,Om,sQ,Jje,Yje,n9,Kje,Zje,eNe,Xm,lQ,oNe,rNe,s9,tNe,aNe,nNe,zm,iQ,sNe,lNe,l9,iNe,dNe,cNe,Vm,dQ,fNe,mNe,i9,gNe,hNe,pNe,Wm,cQ,_Ne,uNe,d9,bNe,vNe,TNe,Qm,fQ,FNe,CNe,c9,MNe,ENe,yNe,Hm,mQ,wNe,ANe,f9,LNe,BNe,kNe,Um,gQ,xNe,RNe,m9,SNe,PNe,$Ne,Jm,hQ,INe,jNe,g9,NNe,DNe,qNe,Ym,pQ,GNe,ONe,h9,XNe,zNe,VNe,Km,_Q,WNe,QNe,p9,HNe,UNe,JNe,Zm,uQ,YNe,KNe,_9,ZNe,eDe,oDe,eg,bQ,rDe,tDe,u9,aDe,nDe,sDe,og,vQ,lDe,iDe,b9,dDe,cDe,fDe,rg,TQ,mDe,gDe,v9,hDe,pDe,_De,tg,FQ,uDe,bDe,T9,vDe,TDe,FDe,ag,CQ,CDe,MDe,F9,EDe,yDe,wDe,ng,MQ,ADe,LDe,C9,BDe,kDe,xDe,sg,EQ,RDe,SDe,M9,PDe,$De,IDe,lg,yQ,jDe,NDe,E9,DDe,qDe,GDe,ig,wQ,ODe,XDe,y9,zDe,VDe,WDe,dg,AQ,QDe,HDe,w9,UDe,JDe,YDe,cg,LQ,KDe,ZDe,A9,eqe,oqe,rqe,fg,BQ,tqe,aqe,L9,nqe,sqe,lqe,mg,kQ,iqe,dqe,B9,cqe,fqe,mqe,gg,xQ,gqe,hqe,k9,pqe,_qe,uqe,hg,RQ,bqe,vqe,x9,Tqe,Fqe,Cqe,SQ,Mqe,Eqe,hM,yqe,pg,pM,wqe,PQ,Aqe,b7e,Ii,_g,$Q,_M,Lqe,IQ,Bqe,v7e,Oo,uM,kqe,bM,xqe,R9,Rqe,Sqe,Pqe,vM,$qe,jQ,Iqe,jqe,Nqe,mo,TM,Dqe,NQ,qqe,Gqe,ja,Oqe,DQ,Xqe,zqe,qQ,Vqe,Wqe,GQ,Qqe,Hqe,Uqe,M,Dn,OQ,Jqe,Yqe,S9,Kqe,Zqe,P9,eGe,oGe,rGe,qn,XQ,tGe,aGe,$9,nGe,sGe,I9,lGe,iGe,dGe,Gn,zQ,cGe,fGe,j9,mGe,gGe,N9,hGe,pGe,_Ge,ug,VQ,uGe,bGe,D9,vGe,TGe,FGe,On,WQ,CGe,MGe,q9,EGe,yGe,G9,wGe,AGe,LGe,bg,QQ,BGe,kGe,O9,xGe,RGe,SGe,vg,HQ,PGe,$Ge,X9,IGe,jGe,NGe,Tg,UQ,DGe,qGe,z9,GGe,OGe,XGe,Xn,JQ,zGe,VGe,V9,WGe,QGe,W9,HGe,UGe,JGe,zn,YQ,YGe,KGe,Q9,ZGe,eOe,H9,oOe,rOe,tOe,Vn,KQ,aOe,nOe,U9,sOe,lOe,J9,iOe,dOe,cOe,Fg,ZQ,fOe,mOe,Y9,gOe,hOe,pOe,Cg,eH,_Oe,uOe,K9,bOe,vOe,TOe,Wn,oH,FOe,COe,Z9,MOe,EOe,eB,yOe,wOe,AOe,Mg,rH,LOe,BOe,oB,kOe,xOe,ROe,Qn,tH,SOe,POe,rB,$Oe,IOe,tB,jOe,NOe,DOe,Hn,aH,qOe,GOe,aB,OOe,XOe,nB,zOe,VOe,WOe,Un,nH,QOe,HOe,sB,UOe,JOe,sH,YOe,KOe,ZOe,Eg,lH,eXe,oXe,lB,rXe,tXe,aXe,Jn,iH,nXe,sXe,iB,lXe,iXe,dB,dXe,cXe,fXe,yg,dH,mXe,gXe,cB,hXe,pXe,_Xe,Yn,cH,uXe,bXe,fB,vXe,TXe,mB,FXe,CXe,MXe,Kn,fH,EXe,yXe,gB,wXe,AXe,hB,LXe,BXe,kXe,Zn,mH,xXe,RXe,pB,SXe,PXe,_B,$Xe,IXe,jXe,wg,gH,NXe,DXe,uB,qXe,GXe,OXe,es,hH,XXe,zXe,bB,VXe,WXe,vB,QXe,HXe,UXe,Ag,pH,JXe,YXe,TB,KXe,ZXe,eze,os,_H,oze,rze,FB,tze,aze,CB,nze,sze,lze,rs,uH,ize,dze,MB,cze,fze,EB,mze,gze,hze,ts,bH,pze,_ze,yB,uze,bze,wB,vze,Tze,Fze,as,vH,Cze,Mze,AB,Eze,yze,LB,wze,Aze,Lze,Lg,TH,Bze,kze,BB,xze,Rze,Sze,ns,FH,Pze,$ze,kB,Ize,jze,xB,Nze,Dze,qze,ss,CH,Gze,Oze,RB,Xze,zze,SB,Vze,Wze,Qze,ls,MH,Hze,Uze,PB,Jze,Yze,$B,Kze,Zze,eVe,is,EH,oVe,rVe,IB,tVe,aVe,jB,nVe,sVe,lVe,ds,yH,iVe,dVe,NB,cVe,fVe,DB,mVe,gVe,hVe,cs,wH,pVe,_Ve,qB,uVe,bVe,GB,vVe,TVe,FVe,Bg,AH,CVe,MVe,OB,EVe,yVe,wVe,fs,LH,AVe,LVe,XB,BVe,kVe,zB,xVe,RVe,SVe,kg,BH,PVe,$Ve,VB,IVe,jVe,NVe,xg,kH,DVe,qVe,WB,GVe,OVe,XVe,ms,xH,zVe,VVe,QB,WVe,QVe,HB,HVe,UVe,JVe,gs,RH,YVe,KVe,UB,ZVe,eWe,JB,oWe,rWe,tWe,Rg,SH,aWe,nWe,YB,sWe,lWe,iWe,hs,PH,dWe,cWe,KB,fWe,mWe,ZB,gWe,hWe,pWe,ps,$H,_We,uWe,ek,bWe,vWe,ok,TWe,FWe,CWe,_s,IH,MWe,EWe,rk,yWe,wWe,tk,AWe,LWe,BWe,us,jH,kWe,xWe,ak,RWe,SWe,nk,PWe,$We,IWe,bs,NH,jWe,NWe,sk,DWe,qWe,lk,GWe,OWe,XWe,Sg,DH,zWe,VWe,ik,WWe,QWe,HWe,Pg,qH,UWe,JWe,dk,YWe,KWe,ZWe,$g,GH,eQe,oQe,ck,rQe,tQe,aQe,Ig,OH,nQe,sQe,fk,lQe,iQe,dQe,vs,XH,cQe,fQe,mk,mQe,gQe,gk,hQe,pQe,_Qe,jg,zH,uQe,bQe,hk,vQe,TQe,FQe,Ts,VH,CQe,MQe,pk,EQe,yQe,_k,wQe,AQe,LQe,Fs,WH,BQe,kQe,uk,xQe,RQe,bk,SQe,PQe,$Qe,Cs,QH,IQe,jQe,vk,NQe,DQe,Tk,qQe,GQe,OQe,Ms,HH,XQe,zQe,Fk,VQe,WQe,Ck,QQe,HQe,UQe,Es,UH,JQe,YQe,Mk,KQe,ZQe,Ek,eHe,oHe,rHe,Ng,JH,tHe,aHe,yk,nHe,sHe,lHe,Dg,YH,iHe,dHe,wk,cHe,fHe,mHe,ys,KH,gHe,hHe,Ak,pHe,_He,Lk,uHe,bHe,vHe,ws,ZH,THe,FHe,Bk,CHe,MHe,kk,EHe,yHe,wHe,As,eU,AHe,LHe,xk,BHe,kHe,Rk,xHe,RHe,SHe,qg,oU,PHe,$He,Sk,IHe,jHe,NHe,Gg,rU,DHe,qHe,Pk,GHe,OHe,XHe,Og,tU,zHe,VHe,$k,WHe,QHe,HHe,Xg,aU,UHe,JHe,Ik,YHe,KHe,ZHe,Ls,nU,eUe,oUe,jk,rUe,tUe,Nk,aUe,nUe,sUe,zg,sU,lUe,iUe,Dk,dUe,cUe,fUe,Vg,lU,mUe,gUe,qk,hUe,pUe,_Ue,Bs,iU,uUe,bUe,Gk,vUe,TUe,Ok,FUe,CUe,MUe,ks,dU,EUe,yUe,Xk,wUe,AUe,zk,LUe,BUe,kUe,cU,xUe,RUe,FM,SUe,Wg,CM,PUe,fU,$Ue,T7e,ji,Qg,mU,MM,IUe,gU,jUe,F7e,Xo,EM,NUe,yM,DUe,Vk,qUe,GUe,OUe,wM,XUe,hU,zUe,VUe,WUe,Le,AM,QUe,pU,HUe,UUe,Na,JUe,_U,YUe,KUe,uU,ZUe,eJe,bU,oJe,rJe,tJe,se,Hg,vU,aJe,nJe,Wk,sJe,lJe,iJe,Ug,TU,dJe,cJe,Qk,fJe,mJe,gJe,Jg,FU,hJe,pJe,Hk,_Je,uJe,bJe,Yg,CU,vJe,TJe,Uk,FJe,CJe,MJe,Kg,MU,EJe,yJe,Jk,wJe,AJe,LJe,Zg,EU,BJe,kJe,Yk,xJe,RJe,SJe,eh,yU,PJe,$Je,Kk,IJe,jJe,NJe,oh,wU,DJe,qJe,Zk,GJe,OJe,XJe,rh,AU,zJe,VJe,ex,WJe,QJe,HJe,th,LU,UJe,JJe,ox,YJe,KJe,ZJe,ah,BU,eYe,oYe,rx,rYe,tYe,aYe,nh,kU,nYe,sYe,tx,lYe,iYe,dYe,sh,xU,cYe,fYe,ax,mYe,gYe,hYe,lh,RU,pYe,_Ye,nx,uYe,bYe,vYe,ih,SU,TYe,FYe,sx,CYe,MYe,EYe,dh,yYe,PU,wYe,AYe,LM,LYe,ch,BM,BYe,$U,kYe,C7e,Ni,fh,IU,kM,xYe,jU,RYe,M7e,zo,xM,SYe,RM,PYe,lx,$Ye,IYe,jYe,SM,NYe,NU,DYe,qYe,GYe,Be,PM,OYe,DU,XYe,zYe,Di,VYe,qU,WYe,QYe,GU,HYe,UYe,JYe,we,mh,OU,YYe,KYe,ix,ZYe,eKe,oKe,gh,XU,rKe,tKe,dx,aKe,nKe,sKe,hh,zU,lKe,iKe,cx,dKe,cKe,fKe,ph,VU,mKe,gKe,fx,hKe,pKe,_Ke,_h,WU,uKe,bKe,mx,vKe,TKe,FKe,uh,QU,CKe,MKe,gx,EKe,yKe,wKe,bh,HU,AKe,LKe,hx,BKe,kKe,xKe,vh,UU,RKe,SKe,px,PKe,$Ke,IKe,Th,jKe,JU,NKe,DKe,$M,qKe,Fh,IM,GKe,YU,OKe,E7e,qi,Ch,KU,jM,XKe,ZU,zKe,y7e,Vo,NM,VKe,Gi,WKe,eJ,QKe,HKe,oJ,UKe,JKe,YKe,DM,KKe,rJ,ZKe,eZe,oZe,Nr,qM,rZe,tJ,tZe,aZe,Oi,nZe,aJ,sZe,lZe,nJ,iZe,dZe,cZe,sJ,fZe,mZe,GM,gZe,ke,OM,hZe,lJ,pZe,_Ze,Da,uZe,iJ,bZe,vZe,dJ,TZe,FZe,cJ,CZe,MZe,EZe,F,Mh,fJ,yZe,wZe,_x,AZe,LZe,BZe,Eh,mJ,kZe,xZe,ux,RZe,SZe,PZe,yh,gJ,$Ze,IZe,bx,jZe,NZe,DZe,wh,hJ,qZe,GZe,vx,OZe,XZe,zZe,Ah,pJ,VZe,WZe,Tx,QZe,HZe,UZe,Lh,_J,JZe,YZe,Fx,KZe,ZZe,eeo,Bh,uJ,oeo,reo,Cx,teo,aeo,neo,kh,bJ,seo,leo,Mx,ieo,deo,ceo,xh,vJ,feo,meo,Ex,geo,heo,peo,Rh,TJ,_eo,ueo,yx,beo,veo,Teo,Sh,FJ,Feo,Ceo,wx,Meo,Eeo,yeo,Ph,CJ,weo,Aeo,Ax,Leo,Beo,keo,$h,MJ,xeo,Reo,Lx,Seo,Peo,$eo,Ih,EJ,Ieo,jeo,Bx,Neo,Deo,qeo,jh,yJ,Geo,Oeo,kx,Xeo,zeo,Veo,Nh,wJ,Weo,Qeo,xx,Heo,Ueo,Jeo,Dh,AJ,Yeo,Keo,Rx,Zeo,eoo,ooo,qh,LJ,roo,too,Sx,aoo,noo,soo,Gh,BJ,loo,ioo,Px,doo,coo,foo,Oh,kJ,moo,goo,$x,hoo,poo,_oo,Xh,xJ,uoo,boo,Ix,voo,Too,Foo,zh,RJ,Coo,Moo,jx,Eoo,yoo,woo,Vh,SJ,Aoo,Loo,Nx,Boo,koo,xoo,Wh,PJ,Roo,Soo,Dx,Poo,$oo,Ioo,Qh,$J,joo,Noo,qx,Doo,qoo,Goo,xs,IJ,Ooo,Xoo,Gx,zoo,Voo,Ox,Woo,Qoo,Hoo,Hh,jJ,Uoo,Joo,Xx,Yoo,Koo,Zoo,Uh,NJ,ero,oro,zx,rro,tro,aro,Jh,DJ,nro,sro,Vx,lro,iro,dro,Yh,qJ,cro,fro,Wx,mro,gro,hro,Kh,GJ,pro,_ro,Qx,uro,bro,vro,Zh,OJ,Tro,Fro,Hx,Cro,Mro,Ero,ep,XJ,yro,wro,Ux,Aro,Lro,Bro,op,zJ,kro,xro,Jx,Rro,Sro,Pro,rp,VJ,$ro,Iro,Yx,jro,Nro,Dro,tp,WJ,qro,Gro,Kx,Oro,Xro,zro,ap,QJ,Vro,Wro,Zx,Qro,Hro,Uro,np,HJ,Jro,Yro,eR,Kro,Zro,eto,sp,UJ,oto,rto,oR,tto,ato,nto,lp,JJ,sto,lto,rR,ito,dto,cto,ip,YJ,fto,mto,tR,gto,hto,pto,dp,KJ,_to,uto,aR,bto,vto,Tto,cp,ZJ,Fto,Cto,nR,Mto,Eto,yto,fp,eY,wto,Ato,sR,Lto,Bto,kto,mp,oY,xto,Rto,lR,Sto,Pto,$to,gp,rY,Ito,jto,iR,Nto,Dto,qto,hp,tY,Gto,Oto,dR,Xto,zto,Vto,pp,aY,Wto,Qto,cR,Hto,Uto,Jto,_p,nY,Yto,Kto,fR,Zto,eao,oao,up,sY,rao,tao,mR,aao,nao,sao,bp,lY,lao,iao,gR,dao,cao,fao,vp,iY,mao,gao,hR,hao,pao,_ao,Tp,dY,uao,bao,pR,vao,Tao,Fao,Fp,cY,Cao,Mao,_R,Eao,yao,wao,Cp,fY,Aao,Lao,uR,Bao,kao,xao,Mp,mY,Rao,Sao,bR,Pao,$ao,Iao,Ep,gY,jao,Nao,vR,Dao,qao,Gao,yp,hY,Oao,Xao,TR,zao,Vao,Wao,wp,pY,Qao,Hao,FR,Uao,Jao,Yao,Ap,_Y,Kao,Zao,CR,eno,ono,rno,Lp,uY,tno,ano,MR,nno,sno,lno,Bp,bY,ino,dno,ER,cno,fno,mno,kp,vY,gno,hno,yR,pno,_no,uno,xp,TY,bno,vno,wR,Tno,Fno,Cno,Rp,FY,Mno,Eno,AR,yno,wno,Ano,Sp,CY,Lno,Bno,LR,kno,xno,Rno,Pp,MY,Sno,Pno,BR,$no,Ino,jno,$p,EY,Nno,Dno,kR,qno,Gno,Ono,Ip,yY,Xno,zno,xR,Vno,Wno,Qno,jp,wY,Hno,Uno,RR,Jno,Yno,Kno,Np,AY,Zno,eso,SR,oso,rso,tso,Dp,LY,aso,nso,PR,sso,lso,iso,qp,BY,dso,cso,$R,fso,mso,gso,Gp,kY,hso,pso,IR,_so,uso,bso,Op,xY,vso,Tso,jR,Fso,Cso,Mso,Xp,RY,Eso,yso,NR,wso,Aso,Lso,zp,SY,Bso,kso,DR,xso,Rso,Sso,Vp,PY,Pso,$so,qR,Iso,jso,Nso,Wp,$Y,Dso,qso,GR,Gso,Oso,Xso,Qp,IY,zso,Vso,OR,Wso,Qso,Hso,Hp,jY,Uso,Jso,XR,Yso,Kso,Zso,Up,NY,elo,olo,zR,rlo,tlo,alo,Jp,DY,nlo,slo,VR,llo,ilo,dlo,Yp,qY,clo,flo,WR,mlo,glo,hlo,Kp,GY,plo,_lo,QR,ulo,blo,vlo,Zp,Tlo,OY,Flo,Clo,XY,Mlo,Elo,zY,ylo,wlo,XM,w7e,Xi,e_,VY,zM,Alo,WY,Llo,A7e,Wo,VM,Blo,zi,klo,QY,xlo,Rlo,HY,Slo,Plo,$lo,WM,Ilo,UY,jlo,Nlo,Dlo,Dr,QM,qlo,JY,Glo,Olo,Vi,Xlo,YY,zlo,Vlo,KY,Wlo,Qlo,Hlo,ZY,Ulo,Jlo,HM,Ylo,xe,UM,Klo,eK,Zlo,eio,qa,oio,oK,rio,tio,rK,aio,nio,tK,sio,lio,iio,x,o_,aK,dio,cio,HR,fio,mio,gio,r_,nK,hio,pio,UR,_io,uio,bio,t_,sK,vio,Tio,JR,Fio,Cio,Mio,a_,lK,Eio,yio,YR,wio,Aio,Lio,n_,iK,Bio,kio,KR,xio,Rio,Sio,s_,dK,Pio,$io,ZR,Iio,jio,Nio,l_,cK,Dio,qio,eS,Gio,Oio,Xio,i_,fK,zio,Vio,oS,Wio,Qio,Hio,d_,mK,Uio,Jio,rS,Yio,Kio,Zio,c_,gK,edo,odo,tS,rdo,tdo,ado,f_,hK,ndo,sdo,aS,ldo,ido,ddo,m_,pK,cdo,fdo,nS,mdo,gdo,hdo,g_,_K,pdo,_do,sS,udo,bdo,vdo,h_,uK,Tdo,Fdo,lS,Cdo,Mdo,Edo,p_,bK,ydo,wdo,iS,Ado,Ldo,Bdo,__,vK,kdo,xdo,dS,Rdo,Sdo,Pdo,u_,TK,$do,Ido,cS,jdo,Ndo,Ddo,b_,FK,qdo,Gdo,fS,Odo,Xdo,zdo,v_,CK,Vdo,Wdo,mS,Qdo,Hdo,Udo,T_,MK,Jdo,Ydo,gS,Kdo,Zdo,eco,F_,EK,oco,rco,hS,tco,aco,nco,C_,yK,sco,lco,pS,ico,dco,cco,M_,wK,fco,mco,_S,gco,hco,pco,E_,AK,_co,uco,uS,bco,vco,Tco,y_,LK,Fco,Cco,bS,Mco,Eco,yco,w_,BK,wco,Aco,vS,Lco,Bco,kco,A_,kK,xco,Rco,TS,Sco,Pco,$co,L_,xK,Ico,jco,FS,Nco,Dco,qco,B_,RK,Gco,Oco,CS,Xco,zco,Vco,k_,SK,Wco,Qco,MS,Hco,Uco,Jco,x_,PK,Yco,Kco,ES,Zco,efo,ofo,R_,$K,rfo,tfo,yS,afo,nfo,sfo,S_,IK,lfo,ifo,wS,dfo,cfo,ffo,P_,jK,mfo,gfo,AS,hfo,pfo,_fo,$_,NK,ufo,bfo,LS,vfo,Tfo,Ffo,I_,DK,Cfo,Mfo,BS,Efo,yfo,wfo,j_,qK,Afo,Lfo,kS,Bfo,kfo,xfo,N_,GK,Rfo,Sfo,xS,Pfo,$fo,Ifo,D_,jfo,OK,Nfo,Dfo,XK,qfo,Gfo,zK,Ofo,Xfo,JM,L7e,Wi,q_,VK,YM,zfo,WK,Vfo,B7e,Qo,KM,Wfo,Qi,Qfo,QK,Hfo,Ufo,HK,Jfo,Yfo,Kfo,ZM,Zfo,UK,emo,omo,rmo,qr,eE,tmo,JK,amo,nmo,Hi,smo,YK,lmo,imo,KK,dmo,cmo,fmo,ZK,mmo,gmo,oE,hmo,Re,rE,pmo,eZ,_mo,umo,Ga,bmo,oZ,vmo,Tmo,rZ,Fmo,Cmo,tZ,Mmo,Emo,ymo,$,G_,aZ,wmo,Amo,RS,Lmo,Bmo,kmo,O_,nZ,xmo,Rmo,SS,Smo,Pmo,$mo,X_,sZ,Imo,jmo,PS,Nmo,Dmo,qmo,z_,lZ,Gmo,Omo,$S,Xmo,zmo,Vmo,V_,iZ,Wmo,Qmo,IS,Hmo,Umo,Jmo,W_,dZ,Ymo,Kmo,jS,Zmo,ego,ogo,Q_,cZ,rgo,tgo,NS,ago,ngo,sgo,H_,fZ,lgo,igo,DS,dgo,cgo,fgo,U_,mZ,mgo,ggo,qS,hgo,pgo,_go,J_,gZ,ugo,bgo,GS,vgo,Tgo,Fgo,Y_,hZ,Cgo,Mgo,OS,Ego,ygo,wgo,K_,pZ,Ago,Lgo,XS,Bgo,kgo,xgo,Z_,_Z,Rgo,Sgo,zS,Pgo,$go,Igo,eu,uZ,jgo,Ngo,VS,Dgo,qgo,Ggo,ou,bZ,Ogo,Xgo,WS,zgo,Vgo,Wgo,ru,vZ,Qgo,Hgo,QS,Ugo,Jgo,Ygo,tu,TZ,Kgo,Zgo,HS,eho,oho,rho,au,FZ,tho,aho,US,nho,sho,lho,nu,CZ,iho,dho,JS,cho,fho,mho,su,MZ,gho,hho,YS,pho,_ho,uho,lu,EZ,bho,vho,KS,Tho,Fho,Cho,iu,yZ,Mho,Eho,ZS,yho,who,Aho,du,wZ,Lho,Bho,eP,kho,xho,Rho,cu,AZ,Sho,Pho,oP,$ho,Iho,jho,fu,LZ,Nho,Dho,rP,qho,Gho,Oho,mu,BZ,Xho,zho,tP,Vho,Who,Qho,gu,kZ,Hho,Uho,aP,Jho,Yho,Kho,hu,xZ,Zho,epo,nP,opo,rpo,tpo,pu,RZ,apo,npo,sP,spo,lpo,ipo,_u,SZ,dpo,cpo,lP,fpo,mpo,gpo,uu,PZ,hpo,ppo,iP,_po,upo,bpo,bu,$Z,vpo,Tpo,dP,Fpo,Cpo,Mpo,vu,IZ,Epo,ypo,cP,wpo,Apo,Lpo,Tu,jZ,Bpo,kpo,fP,xpo,Rpo,Spo,Fu,Ppo,NZ,$po,Ipo,DZ,jpo,Npo,qZ,Dpo,qpo,tE,k7e,Ui,Cu,GZ,aE,Gpo,OZ,Opo,x7e,Ho,nE,Xpo,Ji,zpo,XZ,Vpo,Wpo,zZ,Qpo,Hpo,Upo,sE,Jpo,VZ,Ypo,Kpo,Zpo,Gr,lE,e_o,WZ,o_o,r_o,Yi,t_o,QZ,a_o,n_o,HZ,s_o,l_o,i_o,UZ,d_o,c_o,iE,f_o,Se,dE,m_o,JZ,g_o,h_o,Oa,p_o,YZ,__o,u_o,KZ,b_o,v_o,ZZ,T_o,F_o,C_o,I,Mu,eee,M_o,E_o,mP,y_o,w_o,A_o,Eu,oee,L_o,B_o,gP,k_o,x_o,R_o,yu,ree,S_o,P_o,hP,$_o,I_o,j_o,wu,tee,N_o,D_o,pP,q_o,G_o,O_o,Au,aee,X_o,z_o,_P,V_o,W_o,Q_o,Lu,nee,H_o,U_o,uP,J_o,Y_o,K_o,Bu,see,Z_o,euo,bP,ouo,ruo,tuo,ku,lee,auo,nuo,vP,suo,luo,iuo,xu,iee,duo,cuo,TP,fuo,muo,guo,Ru,dee,huo,puo,FP,_uo,uuo,buo,Su,cee,vuo,Tuo,CP,Fuo,Cuo,Muo,Pu,fee,Euo,yuo,MP,wuo,Auo,Luo,$u,mee,Buo,kuo,EP,xuo,Ruo,Suo,Iu,gee,Puo,$uo,yP,Iuo,juo,Nuo,ju,hee,Duo,quo,wP,Guo,Ouo,Xuo,Nu,pee,zuo,Vuo,AP,Wuo,Quo,Huo,Du,_ee,Uuo,Juo,LP,Yuo,Kuo,Zuo,qu,uee,e2o,o2o,BP,r2o,t2o,a2o,Gu,bee,n2o,s2o,kP,l2o,i2o,d2o,Ou,vee,c2o,f2o,xP,m2o,g2o,h2o,Xu,Tee,p2o,_2o,RP,u2o,b2o,v2o,zu,Fee,T2o,F2o,SP,C2o,M2o,E2o,Vu,Cee,y2o,w2o,PP,A2o,L2o,B2o,Wu,Mee,k2o,x2o,$P,R2o,S2o,P2o,Qu,Eee,$2o,I2o,IP,j2o,N2o,D2o,Hu,yee,q2o,G2o,jP,O2o,X2o,z2o,Uu,wee,V2o,W2o,NP,Q2o,H2o,U2o,Ju,Aee,J2o,Y2o,DP,K2o,Z2o,e1o,Yu,Lee,o1o,r1o,qP,t1o,a1o,n1o,Ku,Bee,s1o,l1o,kee,i1o,d1o,c1o,Zu,xee,f1o,m1o,GP,g1o,h1o,p1o,e2,Ree,_1o,u1o,OP,b1o,v1o,T1o,o2,See,F1o,C1o,XP,M1o,E1o,y1o,r2,Pee,w1o,A1o,zP,L1o,B1o,k1o,t2,x1o,$ee,R1o,S1o,Iee,P1o,$1o,jee,I1o,j1o,cE,R7e,Ki,a2,Nee,fE,N1o,Dee,D1o,S7e,Uo,mE,q1o,Zi,G1o,qee,O1o,X1o,Gee,z1o,V1o,W1o,gE,Q1o,Oee,H1o,U1o,J1o,Or,hE,Y1o,Xee,K1o,Z1o,ed,ebo,zee,obo,rbo,Vee,tbo,abo,nbo,Wee,sbo,lbo,pE,ibo,Pe,_E,dbo,Qee,cbo,fbo,Xa,mbo,Hee,gbo,hbo,Uee,pbo,_bo,Jee,ubo,bbo,vbo,ae,n2,Yee,Tbo,Fbo,VP,Cbo,Mbo,Ebo,s2,Kee,ybo,wbo,WP,Abo,Lbo,Bbo,l2,Zee,kbo,xbo,QP,Rbo,Sbo,Pbo,i2,eoe,$bo,Ibo,HP,jbo,Nbo,Dbo,d2,ooe,qbo,Gbo,UP,Obo,Xbo,zbo,c2,roe,Vbo,Wbo,JP,Qbo,Hbo,Ubo,f2,toe,Jbo,Ybo,YP,Kbo,Zbo,e5o,m2,aoe,o5o,r5o,KP,t5o,a5o,n5o,g2,noe,s5o,l5o,ZP,i5o,d5o,c5o,h2,soe,f5o,m5o,e$,g5o,h5o,p5o,p2,loe,_5o,u5o,o$,b5o,v5o,T5o,_2,ioe,F5o,C5o,r$,M5o,E5o,y5o,u2,doe,w5o,A5o,t$,L5o,B5o,k5o,b2,coe,x5o,R5o,a$,S5o,P5o,$5o,v2,foe,I5o,j5o,n$,N5o,D5o,q5o,T2,moe,G5o,O5o,s$,X5o,z5o,V5o,F2,W5o,goe,Q5o,H5o,hoe,U5o,J5o,poe,Y5o,K5o,uE,P7e,od,C2,_oe,bE,Z5o,uoe,evo,$7e,Jo,vE,ovo,rd,rvo,boe,tvo,avo,voe,nvo,svo,lvo,TE,ivo,Toe,dvo,cvo,fvo,Xr,FE,mvo,Foe,gvo,hvo,td,pvo,Coe,_vo,uvo,Moe,bvo,vvo,Tvo,Eoe,Fvo,Cvo,CE,Mvo,$e,ME,Evo,yoe,yvo,wvo,za,Avo,woe,Lvo,Bvo,Aoe,kvo,xvo,Loe,Rvo,Svo,Pvo,A,M2,Boe,$vo,Ivo,l$,jvo,Nvo,Dvo,E2,koe,qvo,Gvo,i$,Ovo,Xvo,zvo,y2,xoe,Vvo,Wvo,d$,Qvo,Hvo,Uvo,w2,Roe,Jvo,Yvo,c$,Kvo,Zvo,e6o,A2,Soe,o6o,r6o,f$,t6o,a6o,n6o,L2,Poe,s6o,l6o,m$,i6o,d6o,c6o,B2,$oe,f6o,m6o,g$,g6o,h6o,p6o,k2,Ioe,_6o,u6o,h$,b6o,v6o,T6o,x2,joe,F6o,C6o,p$,M6o,E6o,y6o,R2,Noe,w6o,A6o,_$,L6o,B6o,k6o,S2,Doe,x6o,R6o,u$,S6o,P6o,$6o,P2,qoe,I6o,j6o,b$,N6o,D6o,q6o,$2,Goe,G6o,O6o,v$,X6o,z6o,V6o,I2,Ooe,W6o,Q6o,T$,H6o,U6o,J6o,j2,Xoe,Y6o,K6o,F$,Z6o,eTo,oTo,N2,zoe,rTo,tTo,C$,aTo,nTo,sTo,D2,Voe,lTo,iTo,M$,dTo,cTo,fTo,q2,Woe,mTo,gTo,E$,hTo,pTo,_To,G2,Qoe,uTo,bTo,y$,vTo,TTo,FTo,O2,Hoe,CTo,MTo,w$,ETo,yTo,wTo,X2,Uoe,ATo,LTo,A$,BTo,kTo,xTo,z2,Joe,RTo,STo,L$,PTo,$To,ITo,V2,Yoe,jTo,NTo,B$,DTo,qTo,GTo,W2,Koe,OTo,XTo,k$,zTo,VTo,WTo,Q2,Zoe,QTo,HTo,x$,UTo,JTo,YTo,H2,ere,KTo,ZTo,R$,e8o,o8o,r8o,U2,ore,t8o,a8o,S$,n8o,s8o,l8o,J2,rre,i8o,d8o,P$,c8o,f8o,m8o,Y2,tre,g8o,h8o,$$,p8o,_8o,u8o,K2,are,b8o,v8o,I$,T8o,F8o,C8o,Z2,nre,M8o,E8o,j$,y8o,w8o,A8o,e1,sre,L8o,B8o,N$,k8o,x8o,R8o,o1,lre,S8o,P8o,D$,$8o,I8o,j8o,r1,ire,N8o,D8o,q$,q8o,G8o,O8o,t1,dre,X8o,z8o,G$,V8o,W8o,Q8o,a1,cre,H8o,U8o,O$,J8o,Y8o,K8o,n1,fre,Z8o,eFo,X$,oFo,rFo,tFo,s1,mre,aFo,nFo,z$,sFo,lFo,iFo,l1,gre,dFo,cFo,V$,fFo,mFo,gFo,i1,hre,hFo,pFo,W$,_Fo,uFo,bFo,d1,pre,vFo,TFo,Q$,FFo,CFo,MFo,c1,_re,EFo,yFo,H$,wFo,AFo,LFo,f1,ure,BFo,kFo,U$,xFo,RFo,SFo,m1,bre,PFo,$Fo,J$,IFo,jFo,NFo,g1,vre,DFo,qFo,Y$,GFo,OFo,XFo,h1,zFo,Tre,VFo,WFo,Fre,QFo,HFo,Cre,UFo,JFo,EE,I7e,ad,p1,Mre,yE,YFo,Ere,KFo,j7e,Yo,wE,ZFo,nd,eCo,yre,oCo,rCo,wre,tCo,aCo,nCo,AE,sCo,Are,lCo,iCo,dCo,zr,LE,cCo,Lre,fCo,mCo,sd,gCo,Bre,hCo,pCo,kre,_Co,uCo,bCo,xre,vCo,TCo,BE,FCo,Ie,kE,CCo,Rre,MCo,ECo,Va,yCo,Sre,wCo,ACo,Pre,LCo,BCo,$re,kCo,xCo,RCo,G,_1,Ire,SCo,PCo,K$,$Co,ICo,jCo,u1,jre,NCo,DCo,Z$,qCo,GCo,OCo,b1,Nre,XCo,zCo,eI,VCo,WCo,QCo,v1,Dre,HCo,UCo,oI,JCo,YCo,KCo,T1,qre,ZCo,e4o,rI,o4o,r4o,t4o,F1,Gre,a4o,n4o,tI,s4o,l4o,i4o,C1,Ore,d4o,c4o,aI,f4o,m4o,g4o,M1,Xre,h4o,p4o,nI,_4o,u4o,b4o,E1,zre,v4o,T4o,sI,F4o,C4o,M4o,y1,Vre,E4o,y4o,lI,w4o,A4o,L4o,w1,Wre,B4o,k4o,iI,x4o,R4o,S4o,A1,Qre,P4o,$4o,dI,I4o,j4o,N4o,L1,Hre,D4o,q4o,cI,G4o,O4o,X4o,B1,Ure,z4o,V4o,fI,W4o,Q4o,H4o,k1,Jre,U4o,J4o,mI,Y4o,K4o,Z4o,x1,Yre,eMo,oMo,gI,rMo,tMo,aMo,R1,Kre,nMo,sMo,hI,lMo,iMo,dMo,S1,Zre,cMo,fMo,pI,mMo,gMo,hMo,P1,ete,pMo,_Mo,_I,uMo,bMo,vMo,$1,ote,TMo,FMo,uI,CMo,MMo,EMo,I1,rte,yMo,wMo,bI,AMo,LMo,BMo,j1,tte,kMo,xMo,vI,RMo,SMo,PMo,N1,ate,$Mo,IMo,TI,jMo,NMo,DMo,D1,nte,qMo,GMo,FI,OMo,XMo,zMo,q1,ste,VMo,WMo,CI,QMo,HMo,UMo,G1,lte,JMo,YMo,MI,KMo,ZMo,eEo,O1,ite,oEo,rEo,EI,tEo,aEo,nEo,X1,sEo,dte,lEo,iEo,cte,dEo,cEo,fte,fEo,mEo,xE,N7e,ld,z1,mte,RE,gEo,gte,hEo,D7e,Ko,SE,pEo,id,_Eo,hte,uEo,bEo,pte,vEo,TEo,FEo,PE,CEo,_te,MEo,EEo,yEo,Vr,$E,wEo,ute,AEo,LEo,dd,BEo,bte,kEo,xEo,vte,REo,SEo,PEo,Tte,$Eo,IEo,IE,jEo,je,jE,NEo,Fte,DEo,qEo,Wa,GEo,Cte,OEo,XEo,Mte,zEo,VEo,Ete,WEo,QEo,HEo,na,V1,yte,UEo,JEo,yI,YEo,KEo,ZEo,W1,wte,e3o,o3o,wI,r3o,t3o,a3o,Q1,Ate,n3o,s3o,AI,l3o,i3o,d3o,H1,Lte,c3o,f3o,LI,m3o,g3o,h3o,U1,Bte,p3o,_3o,BI,u3o,b3o,v3o,J1,T3o,kte,F3o,C3o,xte,M3o,E3o,Rte,y3o,w3o,NE,q7e,cd,Y1,Ste,DE,A3o,Pte,L3o,G7e,Zo,qE,B3o,fd,k3o,$te,x3o,R3o,Ite,S3o,P3o,$3o,GE,I3o,jte,j3o,N3o,D3o,Wr,OE,q3o,Nte,G3o,O3o,md,X3o,Dte,z3o,V3o,qte,W3o,Q3o,H3o,Gte,U3o,J3o,XE,Y3o,Ne,zE,K3o,Ote,Z3o,eyo,Qa,oyo,Xte,ryo,tyo,zte,ayo,nyo,Vte,syo,lyo,iyo,D,K1,Wte,dyo,cyo,kI,fyo,myo,gyo,Z1,Qte,hyo,pyo,xI,_yo,uyo,byo,eb,Hte,vyo,Tyo,RI,Fyo,Cyo,Myo,ob,Ute,Eyo,yyo,SI,wyo,Ayo,Lyo,rb,Jte,Byo,kyo,PI,xyo,Ryo,Syo,tb,Yte,Pyo,$yo,$I,Iyo,jyo,Nyo,ab,Kte,Dyo,qyo,II,Gyo,Oyo,Xyo,nb,Zte,zyo,Vyo,jI,Wyo,Qyo,Hyo,sb,eae,Uyo,Jyo,NI,Yyo,Kyo,Zyo,lb,oae,ewo,owo,DI,rwo,two,awo,ib,rae,nwo,swo,qI,lwo,iwo,dwo,db,tae,cwo,fwo,GI,mwo,gwo,hwo,cb,aae,pwo,_wo,OI,uwo,bwo,vwo,fb,nae,Two,Fwo,XI,Cwo,Mwo,Ewo,mb,sae,ywo,wwo,zI,Awo,Lwo,Bwo,gb,lae,kwo,xwo,VI,Rwo,Swo,Pwo,hb,iae,$wo,Iwo,WI,jwo,Nwo,Dwo,pb,dae,qwo,Gwo,QI,Owo,Xwo,zwo,_b,cae,Vwo,Wwo,HI,Qwo,Hwo,Uwo,ub,fae,Jwo,Ywo,UI,Kwo,Zwo,eAo,bb,mae,oAo,rAo,JI,tAo,aAo,nAo,vb,gae,sAo,lAo,YI,iAo,dAo,cAo,Tb,hae,fAo,mAo,KI,gAo,hAo,pAo,Fb,pae,_Ao,uAo,ZI,bAo,vAo,TAo,Cb,_ae,FAo,CAo,ej,MAo,EAo,yAo,Mb,uae,wAo,AAo,oj,LAo,BAo,kAo,Eb,bae,xAo,RAo,rj,SAo,PAo,$Ao,yb,vae,IAo,jAo,tj,NAo,DAo,qAo,wb,Tae,GAo,OAo,aj,XAo,zAo,VAo,Ab,Fae,WAo,QAo,nj,HAo,UAo,JAo,Lb,Cae,YAo,KAo,sj,ZAo,e0o,o0o,Bb,Mae,r0o,t0o,lj,a0o,n0o,s0o,kb,l0o,Eae,i0o,d0o,yae,c0o,f0o,wae,m0o,g0o,VE,O7e,gd,xb,Aae,WE,h0o,Lae,p0o,X7e,er,QE,_0o,hd,u0o,Bae,b0o,v0o,kae,T0o,F0o,C0o,HE,M0o,xae,E0o,y0o,w0o,Qr,UE,A0o,Rae,L0o,B0o,pd,k0o,Sae,x0o,R0o,Pae,S0o,P0o,$0o,$ae,I0o,j0o,JE,N0o,De,YE,D0o,Iae,q0o,G0o,Ha,O0o,jae,X0o,z0o,Nae,V0o,W0o,Dae,Q0o,H0o,U0o,R,Rb,qae,J0o,Y0o,ij,K0o,Z0o,eLo,Sb,Gae,oLo,rLo,dj,tLo,aLo,nLo,Pb,Oae,sLo,lLo,cj,iLo,dLo,cLo,$b,Xae,fLo,mLo,fj,gLo,hLo,pLo,Ib,zae,_Lo,uLo,mj,bLo,vLo,TLo,jb,Vae,FLo,CLo,gj,MLo,ELo,yLo,Nb,Wae,wLo,ALo,hj,LLo,BLo,kLo,Db,Qae,xLo,RLo,pj,SLo,PLo,$Lo,qb,Hae,ILo,jLo,_j,NLo,DLo,qLo,Gb,Uae,GLo,OLo,uj,XLo,zLo,VLo,Ob,Jae,WLo,QLo,bj,HLo,ULo,JLo,Xb,Yae,YLo,KLo,vj,ZLo,e7o,o7o,zb,Kae,r7o,t7o,Tj,a7o,n7o,s7o,Vb,Zae,l7o,i7o,Fj,d7o,c7o,f7o,Wb,ene,m7o,g7o,Cj,h7o,p7o,_7o,Qb,one,u7o,b7o,Mj,v7o,T7o,F7o,Hb,rne,C7o,M7o,Ej,E7o,y7o,w7o,Ub,tne,A7o,L7o,yj,B7o,k7o,x7o,Jb,ane,R7o,S7o,wj,P7o,$7o,I7o,Yb,nne,j7o,N7o,Aj,D7o,q7o,G7o,Kb,sne,O7o,X7o,Lj,z7o,V7o,W7o,Zb,lne,Q7o,H7o,Bj,U7o,J7o,Y7o,e5,ine,K7o,Z7o,kj,e9o,o9o,r9o,o5,dne,t9o,a9o,xj,n9o,s9o,l9o,r5,cne,i9o,d9o,Rj,c9o,f9o,m9o,t5,fne,g9o,h9o,Sj,p9o,_9o,u9o,a5,mne,b9o,v9o,Pj,T9o,F9o,C9o,n5,gne,M9o,E9o,$j,y9o,w9o,A9o,s5,hne,L9o,B9o,Ij,k9o,x9o,R9o,l5,pne,S9o,P9o,jj,$9o,I9o,j9o,i5,_ne,N9o,D9o,Nj,q9o,G9o,O9o,d5,une,X9o,z9o,Dj,V9o,W9o,Q9o,c5,bne,H9o,U9o,qj,J9o,Y9o,K9o,f5,vne,Z9o,eBo,Gj,oBo,rBo,tBo,m5,Tne,aBo,nBo,Oj,sBo,lBo,iBo,g5,Fne,dBo,cBo,Xj,fBo,mBo,gBo,h5,Cne,hBo,pBo,zj,_Bo,uBo,bBo,p5,Mne,vBo,TBo,Vj,FBo,CBo,MBo,_5,EBo,Ene,yBo,wBo,yne,ABo,LBo,wne,BBo,kBo,KE,z7e,_d,u5,Ane,ZE,xBo,Lne,RBo,V7e,or,e3,SBo,ud,PBo,Bne,$Bo,IBo,kne,jBo,NBo,DBo,o3,qBo,xne,GBo,OBo,XBo,Hr,r3,zBo,Rne,VBo,WBo,bd,QBo,Sne,HBo,UBo,Pne,JBo,YBo,KBo,$ne,ZBo,eko,t3,oko,qe,a3,rko,Ine,tko,ako,Ua,nko,jne,sko,lko,Nne,iko,dko,Dne,cko,fko,mko,qne,b5,Gne,gko,hko,Wj,pko,_ko,uko,v5,bko,One,vko,Tko,Xne,Fko,Cko,zne,Mko,Eko,n3,W7e,vd,T5,Vne,s3,yko,Wne,wko,Q7e,rr,l3,Ako,Td,Lko,Qne,Bko,kko,Hne,xko,Rko,Sko,i3,Pko,Une,$ko,Iko,jko,Ur,d3,Nko,Jne,Dko,qko,Fd,Gko,Yne,Oko,Xko,Kne,zko,Vko,Wko,Zne,Qko,Hko,c3,Uko,Ge,f3,Jko,ese,Yko,Kko,Ja,Zko,ose,exo,oxo,rse,rxo,txo,tse,axo,nxo,sxo,be,F5,ase,lxo,ixo,Qj,dxo,cxo,fxo,C5,nse,mxo,gxo,Hj,hxo,pxo,_xo,Rs,sse,uxo,bxo,Uj,vxo,Txo,Jj,Fxo,Cxo,Mxo,M5,lse,Exo,yxo,Yj,wxo,Axo,Lxo,la,ise,Bxo,kxo,Kj,xxo,Rxo,Zj,Sxo,Pxo,eN,$xo,Ixo,jxo,E5,dse,Nxo,Dxo,oN,qxo,Gxo,Oxo,y5,cse,Xxo,zxo,rN,Vxo,Wxo,Qxo,w5,fse,Hxo,Uxo,tN,Jxo,Yxo,Kxo,A5,mse,Zxo,eRo,aN,oRo,rRo,tRo,L5,aRo,gse,nRo,sRo,hse,lRo,iRo,pse,dRo,cRo,m3,H7e,Cd,B5,_se,g3,fRo,use,mRo,U7e,tr,h3,gRo,Md,hRo,bse,pRo,_Ro,vse,uRo,bRo,vRo,p3,TRo,Tse,FRo,CRo,MRo,Jr,_3,ERo,Fse,yRo,wRo,Ed,ARo,Cse,LRo,BRo,Mse,kRo,xRo,RRo,Ese,SRo,PRo,u3,$Ro,Oe,b3,IRo,yse,jRo,NRo,Ya,DRo,wse,qRo,GRo,Ase,ORo,XRo,Lse,zRo,VRo,WRo,Bse,k5,kse,QRo,HRo,nN,URo,JRo,YRo,x5,KRo,xse,ZRo,eSo,Rse,oSo,rSo,Sse,tSo,aSo,v3,J7e,yd,R5,Pse,T3,nSo,$se,sSo,Y7e,ar,F3,lSo,wd,iSo,Ise,dSo,cSo,jse,fSo,mSo,gSo,C3,hSo,Nse,pSo,_So,uSo,Yr,M3,bSo,Dse,vSo,TSo,Ad,FSo,qse,CSo,MSo,Gse,ESo,ySo,wSo,Ose,ASo,LSo,E3,BSo,Xe,y3,kSo,Xse,xSo,RSo,Ka,SSo,zse,PSo,$So,Vse,ISo,jSo,Wse,NSo,DSo,qSo,ao,S5,Qse,GSo,OSo,sN,XSo,zSo,VSo,P5,Hse,WSo,QSo,lN,HSo,USo,JSo,$5,Use,YSo,KSo,iN,ZSo,ePo,oPo,I5,Jse,rPo,tPo,dN,aPo,nPo,sPo,j5,Yse,lPo,iPo,cN,dPo,cPo,fPo,N5,Kse,mPo,gPo,fN,hPo,pPo,_Po,D5,Zse,uPo,bPo,mN,vPo,TPo,FPo,q5,CPo,ele,MPo,EPo,ole,yPo,wPo,rle,APo,LPo,w3,K7e,Ld,G5,tle,A3,BPo,ale,kPo,Z7e,nr,L3,xPo,Bd,RPo,nle,SPo,PPo,sle,$Po,IPo,jPo,B3,NPo,lle,DPo,qPo,GPo,Kr,k3,OPo,ile,XPo,zPo,kd,VPo,dle,WPo,QPo,cle,HPo,UPo,JPo,fle,YPo,KPo,x3,ZPo,ze,R3,e$o,mle,o$o,r$o,Za,t$o,gle,a$o,n$o,hle,s$o,l$o,ple,i$o,d$o,c$o,xd,O5,_le,f$o,m$o,gN,g$o,h$o,p$o,X5,ule,_$o,u$o,hN,b$o,v$o,T$o,z5,ble,F$o,C$o,pN,M$o,E$o,y$o,V5,w$o,vle,A$o,L$o,Tle,B$o,k$o,Fle,x$o,R$o,S3,e9e,Rd,W5,Cle,P3,S$o,Mle,P$o,o9e,sr,$3,$$o,Sd,I$o,Ele,j$o,N$o,yle,D$o,q$o,G$o,I3,O$o,wle,X$o,z$o,V$o,Zr,j3,W$o,Ale,Q$o,H$o,Pd,U$o,Lle,J$o,Y$o,Ble,K$o,Z$o,eIo,kle,oIo,rIo,N3,tIo,Ve,D3,aIo,xle,nIo,sIo,en,lIo,Rle,iIo,dIo,Sle,cIo,fIo,Ple,mIo,gIo,hIo,no,Q5,$le,pIo,_Io,_N,uIo,bIo,vIo,H5,Ile,TIo,FIo,uN,CIo,MIo,EIo,U5,jle,yIo,wIo,bN,AIo,LIo,BIo,J5,Nle,kIo,xIo,vN,RIo,SIo,PIo,Y5,Dle,$Io,IIo,TN,jIo,NIo,DIo,K5,qle,qIo,GIo,FN,OIo,XIo,zIo,Z5,Gle,VIo,WIo,CN,QIo,HIo,UIo,ev,JIo,Ole,YIo,KIo,Xle,ZIo,ejo,zle,ojo,rjo,q3,r9e,$d,ov,Vle,G3,tjo,Wle,ajo,t9e,lr,O3,njo,Id,sjo,Qle,ljo,ijo,Hle,djo,cjo,fjo,X3,mjo,Ule,gjo,hjo,pjo,et,z3,_jo,Jle,ujo,bjo,jd,vjo,Yle,Tjo,Fjo,Kle,Cjo,Mjo,Ejo,Zle,yjo,wjo,V3,Ajo,We,W3,Ljo,eie,Bjo,kjo,on,xjo,oie,Rjo,Sjo,rie,Pjo,$jo,tie,Ijo,jjo,Njo,Q3,rv,aie,Djo,qjo,MN,Gjo,Ojo,Xjo,tv,nie,zjo,Vjo,EN,Wjo,Qjo,Hjo,av,Ujo,sie,Jjo,Yjo,lie,Kjo,Zjo,iie,eNo,oNo,H3,a9e,Nd,nv,die,U3,rNo,cie,tNo,n9e,ir,J3,aNo,Dd,nNo,fie,sNo,lNo,mie,iNo,dNo,cNo,Y3,fNo,gie,mNo,gNo,hNo,ot,K3,pNo,hie,_No,uNo,qd,bNo,pie,vNo,TNo,_ie,FNo,CNo,MNo,uie,ENo,yNo,Z3,wNo,Qe,ey,ANo,bie,LNo,BNo,rn,kNo,vie,xNo,RNo,Tie,SNo,PNo,Fie,$No,INo,jNo,Gd,sv,Cie,NNo,DNo,yN,qNo,GNo,ONo,lv,Mie,XNo,zNo,wN,VNo,WNo,QNo,iv,Eie,HNo,UNo,AN,JNo,YNo,KNo,dv,ZNo,yie,eDo,oDo,wie,rDo,tDo,Aie,aDo,nDo,oy,s9e,Od,cv,Lie,ry,sDo,Bie,lDo,l9e,dr,ty,iDo,Xd,dDo,kie,cDo,fDo,xie,mDo,gDo,hDo,ay,pDo,Rie,_Do,uDo,bDo,rt,ny,vDo,Sie,TDo,FDo,zd,CDo,Pie,MDo,EDo,$ie,yDo,wDo,ADo,Iie,LDo,BDo,sy,kDo,He,ly,xDo,jie,RDo,SDo,tn,PDo,Nie,$Do,IDo,Die,jDo,NDo,qie,DDo,qDo,GDo,Vd,fv,Gie,ODo,XDo,LN,zDo,VDo,WDo,mv,Oie,QDo,HDo,BN,UDo,JDo,YDo,gv,Xie,KDo,ZDo,kN,eqo,oqo,rqo,hv,tqo,zie,aqo,nqo,Vie,sqo,lqo,Wie,iqo,dqo,iy,i9e,Wd,pv,Qie,dy,cqo,Hie,fqo,d9e,cr,cy,mqo,Qd,gqo,Uie,hqo,pqo,Jie,_qo,uqo,bqo,fy,vqo,Yie,Tqo,Fqo,Cqo,tt,my,Mqo,Kie,Eqo,yqo,Hd,wqo,Zie,Aqo,Lqo,ede,Bqo,kqo,xqo,ode,Rqo,Sqo,gy,Pqo,Ue,hy,$qo,rde,Iqo,jqo,an,Nqo,tde,Dqo,qqo,ade,Gqo,Oqo,nde,Xqo,zqo,Vqo,sde,_v,lde,Wqo,Qqo,xN,Hqo,Uqo,Jqo,uv,Yqo,ide,Kqo,Zqo,dde,eGo,oGo,cde,rGo,tGo,py,c9e,Ud,bv,fde,_y,aGo,mde,nGo,f9e,fr,uy,sGo,Jd,lGo,gde,iGo,dGo,hde,cGo,fGo,mGo,by,gGo,pde,hGo,pGo,_Go,at,vy,uGo,_de,bGo,vGo,Yd,TGo,ude,FGo,CGo,bde,MGo,EGo,yGo,vde,wGo,AGo,Ty,LGo,Je,Fy,BGo,Tde,kGo,xGo,nn,RGo,Fde,SGo,PGo,Cde,$Go,IGo,Mde,jGo,NGo,DGo,Ede,vv,yde,qGo,GGo,RN,OGo,XGo,zGo,Tv,VGo,wde,WGo,QGo,Ade,HGo,UGo,Lde,JGo,YGo,Cy,m9e,Kd,Fv,Bde,My,KGo,kde,ZGo,g9e,mr,Ey,eOo,Zd,oOo,xde,rOo,tOo,Rde,aOo,nOo,sOo,yy,lOo,Sde,iOo,dOo,cOo,nt,wy,fOo,Pde,mOo,gOo,ec,hOo,$de,pOo,_Oo,Ide,uOo,bOo,vOo,jde,TOo,FOo,Ay,COo,Ye,Ly,MOo,Nde,EOo,yOo,sn,wOo,Dde,AOo,LOo,qde,BOo,kOo,Gde,xOo,ROo,SOo,By,Cv,Ode,POo,$Oo,SN,IOo,jOo,NOo,Mv,Xde,DOo,qOo,PN,GOo,OOo,XOo,Ev,zOo,zde,VOo,WOo,Vde,QOo,HOo,Wde,UOo,JOo,ky,h9e,oc,yv,Qde,xy,YOo,Hde,KOo,p9e,gr,Ry,ZOo,rc,eXo,Ude,oXo,rXo,Jde,tXo,aXo,nXo,Sy,sXo,Yde,lXo,iXo,dXo,st,Py,cXo,Kde,fXo,mXo,tc,gXo,Zde,hXo,pXo,ece,_Xo,uXo,bXo,oce,vXo,TXo,$y,FXo,go,Iy,CXo,rce,MXo,EXo,ln,yXo,tce,wXo,AXo,ace,LXo,BXo,nce,kXo,xXo,RXo,B,wv,sce,SXo,PXo,$N,$Xo,IXo,jXo,Av,lce,NXo,DXo,IN,qXo,GXo,OXo,Lv,ice,XXo,zXo,jN,VXo,WXo,QXo,Bv,dce,HXo,UXo,NN,JXo,YXo,KXo,kv,cce,ZXo,ezo,DN,ozo,rzo,tzo,xv,fce,azo,nzo,qN,szo,lzo,izo,Rv,mce,dzo,czo,GN,fzo,mzo,gzo,Sv,gce,hzo,pzo,ON,_zo,uzo,bzo,Pv,hce,vzo,Tzo,XN,Fzo,Czo,Mzo,$v,pce,Ezo,yzo,zN,wzo,Azo,Lzo,Iv,_ce,Bzo,kzo,VN,xzo,Rzo,Szo,jv,uce,Pzo,$zo,WN,Izo,jzo,Nzo,Nv,bce,Dzo,qzo,QN,Gzo,Ozo,Xzo,Dv,vce,zzo,Vzo,HN,Wzo,Qzo,Hzo,qv,Tce,Uzo,Jzo,UN,Yzo,Kzo,Zzo,Ss,Fce,eVo,oVo,JN,rVo,tVo,YN,aVo,nVo,sVo,Gv,Cce,lVo,iVo,KN,dVo,cVo,fVo,Ov,Mce,mVo,gVo,ZN,hVo,pVo,_Vo,Xv,Ece,uVo,bVo,eD,vVo,TVo,FVo,zv,yce,CVo,MVo,oD,EVo,yVo,wVo,Vv,wce,AVo,LVo,rD,BVo,kVo,xVo,Wv,Ace,RVo,SVo,tD,PVo,$Vo,IVo,Qv,Lce,jVo,NVo,aD,DVo,qVo,GVo,Hv,Bce,OVo,XVo,nD,zVo,VVo,WVo,Uv,kce,QVo,HVo,sD,UVo,JVo,YVo,Jv,xce,KVo,ZVo,lD,eWo,oWo,rWo,Yv,Rce,tWo,aWo,iD,nWo,sWo,lWo,Kv,Sce,iWo,dWo,dD,cWo,fWo,mWo,Zv,Pce,gWo,hWo,cD,pWo,_Wo,uWo,e6,$ce,bWo,vWo,fD,TWo,FWo,CWo,o6,Ice,MWo,EWo,mD,yWo,wWo,AWo,r6,jce,LWo,BWo,gD,kWo,xWo,RWo,t6,Nce,SWo,PWo,hD,$Wo,IWo,jWo,a6,Dce,NWo,DWo,pD,qWo,GWo,OWo,n6,qce,XWo,zWo,_D,VWo,WWo,QWo,s6,Gce,HWo,UWo,uD,JWo,YWo,KWo,l6,Oce,ZWo,eQo,bD,oQo,rQo,tQo,i6,Xce,aQo,nQo,vD,sQo,lQo,iQo,d6,zce,dQo,cQo,TD,fQo,mQo,gQo,c6,Vce,hQo,pQo,FD,_Qo,uQo,bQo,f6,Wce,vQo,TQo,CD,FQo,CQo,MQo,Qce,EQo,yQo,jy,_9e,ac,m6,Hce,Ny,wQo,Uce,AQo,u9e,hr,Dy,LQo,nc,BQo,Jce,kQo,xQo,Yce,RQo,SQo,PQo,qy,$Qo,Kce,IQo,jQo,NQo,lt,Gy,DQo,Zce,qQo,GQo,sc,OQo,efe,XQo,zQo,ofe,VQo,WQo,QQo,rfe,HQo,UQo,Oy,JQo,ho,Xy,YQo,tfe,KQo,ZQo,dn,eHo,afe,oHo,rHo,nfe,tHo,aHo,sfe,nHo,sHo,lHo,H,g6,lfe,iHo,dHo,MD,cHo,fHo,mHo,h6,ife,gHo,hHo,ED,pHo,_Ho,uHo,p6,dfe,bHo,vHo,yD,THo,FHo,CHo,_6,cfe,MHo,EHo,wD,yHo,wHo,AHo,u6,ffe,LHo,BHo,AD,kHo,xHo,RHo,b6,mfe,SHo,PHo,LD,$Ho,IHo,jHo,v6,gfe,NHo,DHo,BD,qHo,GHo,OHo,T6,hfe,XHo,zHo,kD,VHo,WHo,QHo,F6,pfe,HHo,UHo,xD,JHo,YHo,KHo,C6,_fe,ZHo,eUo,RD,oUo,rUo,tUo,M6,ufe,aUo,nUo,SD,sUo,lUo,iUo,E6,bfe,dUo,cUo,PD,fUo,mUo,gUo,y6,vfe,hUo,pUo,$D,_Uo,uUo,bUo,w6,Tfe,vUo,TUo,ID,FUo,CUo,MUo,A6,Ffe,EUo,yUo,jD,wUo,AUo,LUo,L6,Cfe,BUo,kUo,ND,xUo,RUo,SUo,B6,Mfe,PUo,$Uo,DD,IUo,jUo,NUo,k6,Efe,DUo,qUo,qD,GUo,OUo,XUo,x6,yfe,zUo,VUo,GD,WUo,QUo,HUo,R6,wfe,UUo,JUo,OD,YUo,KUo,ZUo,S6,Afe,eJo,oJo,XD,rJo,tJo,aJo,P6,Lfe,nJo,sJo,zD,lJo,iJo,dJo,Bfe,cJo,fJo,zy,b9e,lc,$6,kfe,Vy,mJo,xfe,gJo,v9e,pr,Wy,hJo,ic,pJo,Rfe,_Jo,uJo,Sfe,bJo,vJo,TJo,Qy,FJo,Pfe,CJo,MJo,EJo,it,Hy,yJo,$fe,wJo,AJo,dc,LJo,Ife,BJo,kJo,jfe,xJo,RJo,SJo,Nfe,PJo,$Jo,Uy,IJo,po,Jy,jJo,Dfe,NJo,DJo,cn,qJo,qfe,GJo,OJo,Gfe,XJo,zJo,Ofe,VJo,WJo,QJo,he,I6,Xfe,HJo,UJo,VD,JJo,YJo,KJo,j6,zfe,ZJo,eYo,WD,oYo,rYo,tYo,N6,Vfe,aYo,nYo,QD,sYo,lYo,iYo,D6,Wfe,dYo,cYo,HD,fYo,mYo,gYo,q6,Qfe,hYo,pYo,UD,_Yo,uYo,bYo,G6,Hfe,vYo,TYo,JD,FYo,CYo,MYo,O6,Ufe,EYo,yYo,YD,wYo,AYo,LYo,X6,Jfe,BYo,kYo,KD,xYo,RYo,SYo,z6,Yfe,PYo,$Yo,ZD,IYo,jYo,NYo,V6,Kfe,DYo,qYo,eq,GYo,OYo,XYo,Zfe,zYo,VYo,Yy,T9e,cc,W6,eme,Ky,WYo,ome,QYo,F9e,_r,Zy,HYo,fc,UYo,rme,JYo,YYo,tme,KYo,ZYo,eKo,ew,oKo,ame,rKo,tKo,aKo,dt,ow,nKo,nme,sKo,lKo,mc,iKo,sme,dKo,cKo,lme,fKo,mKo,gKo,ime,hKo,pKo,rw,_Ko,_o,tw,uKo,dme,bKo,vKo,fn,TKo,cme,FKo,CKo,fme,MKo,EKo,mme,yKo,wKo,AKo,gme,Q6,hme,LKo,BKo,oq,kKo,xKo,RKo,pme,SKo,PKo,aw,C9e,gc,H6,_me,nw,$Ko,ume,IKo,M9e,ur,sw,jKo,hc,NKo,bme,DKo,qKo,vme,GKo,OKo,XKo,lw,zKo,Tme,VKo,WKo,QKo,ct,iw,HKo,Fme,UKo,JKo,pc,YKo,Cme,KKo,ZKo,Mme,eZo,oZo,rZo,Eme,tZo,aZo,dw,nZo,uo,cw,sZo,yme,lZo,iZo,mn,dZo,wme,cZo,fZo,Ame,mZo,gZo,Lme,hZo,pZo,_Zo,Y,U6,Bme,uZo,bZo,rq,vZo,TZo,FZo,J6,kme,CZo,MZo,tq,EZo,yZo,wZo,Y6,xme,AZo,LZo,aq,BZo,kZo,xZo,K6,Rme,RZo,SZo,nq,PZo,$Zo,IZo,Z6,Sme,jZo,NZo,sq,DZo,qZo,GZo,eT,Pme,OZo,XZo,lq,zZo,VZo,WZo,oT,$me,QZo,HZo,iq,UZo,JZo,YZo,rT,Ime,KZo,ZZo,dq,eer,oer,rer,tT,jme,ter,aer,cq,ner,ser,ler,aT,Nme,ier,der,fq,cer,fer,mer,nT,Dme,ger,her,mq,per,_er,uer,sT,qme,ber,ver,gq,Ter,Fer,Cer,lT,Gme,Mer,Eer,hq,yer,wer,Aer,iT,Ome,Ler,Ber,pq,ker,xer,Rer,dT,Xme,Ser,Per,_q,$er,Ier,jer,cT,zme,Ner,Der,uq,qer,Ger,Oer,fT,Vme,Xer,zer,bq,Ver,Wer,Qer,mT,Wme,Her,Uer,vq,Jer,Yer,Ker,gT,Qme,Zer,eor,Tq,oor,ror,tor,hT,Hme,aor,nor,Fq,sor,lor,ior,Ume,dor,cor,fw,E9e,_c,pT,Jme,mw,mor,Yme,gor,y9e,br,gw,hor,uc,por,Kme,_or,uor,Zme,bor,vor,Tor,hw,For,ege,Cor,Mor,Eor,ft,pw,yor,oge,wor,Aor,bc,Lor,rge,Bor,kor,tge,xor,Ror,Sor,age,Por,$or,_w,Ior,bo,uw,jor,nge,Nor,Dor,gn,qor,sge,Gor,Oor,lge,Xor,zor,ige,Vor,Wor,Qor,pe,_T,dge,Hor,Uor,Cq,Jor,Yor,Kor,uT,cge,Zor,err,Mq,orr,rrr,trr,bT,fge,arr,nrr,Eq,srr,lrr,irr,vT,mge,drr,crr,yq,frr,mrr,grr,TT,gge,hrr,prr,wq,_rr,urr,brr,FT,hge,vrr,Trr,Aq,Frr,Crr,Mrr,CT,pge,Err,yrr,Lq,wrr,Arr,Lrr,MT,_ge,Brr,krr,Bq,xrr,Rrr,Srr,ET,uge,Prr,$rr,kq,Irr,jrr,Nrr,yT,bge,Drr,qrr,xq,Grr,Orr,Xrr,vge,zrr,Vrr,bw,w9e,vc,wT,Tge,vw,Wrr,Fge,Qrr,A9e,vr,Tw,Hrr,Tc,Urr,Cge,Jrr,Yrr,Mge,Krr,Zrr,etr,Fw,otr,Ege,rtr,ttr,atr,mt,Cw,ntr,yge,str,ltr,Fc,itr,wge,dtr,ctr,Age,ftr,mtr,gtr,Lge,htr,ptr,Mw,_tr,vo,Ew,utr,Bge,btr,vtr,hn,Ttr,kge,Ftr,Ctr,xge,Mtr,Etr,Rge,ytr,wtr,Atr,X,AT,Sge,Ltr,Btr,Rq,ktr,xtr,Rtr,LT,Pge,Str,Ptr,Sq,$tr,Itr,jtr,BT,$ge,Ntr,Dtr,Pq,qtr,Gtr,Otr,kT,Ige,Xtr,ztr,$q,Vtr,Wtr,Qtr,xT,jge,Htr,Utr,Iq,Jtr,Ytr,Ktr,RT,Nge,Ztr,ear,jq,oar,rar,tar,ST,Dge,aar,nar,Nq,sar,lar,iar,PT,qge,dar,car,Dq,far,mar,gar,$T,Gge,har,par,qq,_ar,uar,bar,IT,Oge,Tar,Far,Gq,Car,Mar,Ear,jT,Xge,yar,war,Oq,Aar,Lar,Bar,NT,zge,kar,xar,Xq,Rar,Sar,Par,DT,Vge,$ar,Iar,zq,jar,Nar,Dar,qT,Wge,qar,Gar,Vq,Oar,Xar,zar,GT,Qge,Var,War,Wq,Qar,Har,Uar,OT,Hge,Jar,Yar,Qq,Kar,Zar,enr,XT,Uge,onr,rnr,Hq,tnr,anr,nnr,zT,Jge,snr,lnr,Uq,inr,dnr,cnr,VT,Yge,fnr,mnr,Jq,gnr,hnr,pnr,WT,Kge,_nr,unr,Yq,bnr,vnr,Tnr,QT,Zge,Fnr,Cnr,Kq,Mnr,Enr,ynr,HT,ehe,wnr,Anr,Zq,Lnr,Bnr,knr,UT,ohe,xnr,Rnr,eG,Snr,Pnr,$nr,JT,rhe,Inr,jnr,oG,Nnr,Dnr,qnr,YT,the,Gnr,Onr,rG,Xnr,znr,Vnr,ahe,Wnr,Qnr,yw,L9e,Cc,KT,nhe,ww,Hnr,she,Unr,B9e,Tr,Aw,Jnr,Mc,Ynr,lhe,Knr,Znr,ihe,esr,osr,rsr,Lw,tsr,dhe,asr,nsr,ssr,gt,Bw,lsr,che,isr,dsr,Ec,csr,fhe,fsr,msr,mhe,gsr,hsr,psr,ghe,_sr,usr,kw,bsr,To,xw,vsr,hhe,Tsr,Fsr,pn,Csr,phe,Msr,Esr,_he,ysr,wsr,uhe,Asr,Lsr,Bsr,te,ZT,bhe,ksr,xsr,tG,Rsr,Ssr,Psr,e8,vhe,$sr,Isr,aG,jsr,Nsr,Dsr,o8,The,qsr,Gsr,nG,Osr,Xsr,zsr,r8,Fhe,Vsr,Wsr,sG,Qsr,Hsr,Usr,t8,Che,Jsr,Ysr,lG,Ksr,Zsr,elr,a8,Mhe,olr,rlr,iG,tlr,alr,nlr,n8,Ehe,slr,llr,dG,ilr,dlr,clr,s8,yhe,flr,mlr,cG,glr,hlr,plr,l8,whe,_lr,ulr,fG,blr,vlr,Tlr,i8,Ahe,Flr,Clr,mG,Mlr,Elr,ylr,d8,Lhe,wlr,Alr,gG,Llr,Blr,klr,c8,Bhe,xlr,Rlr,hG,Slr,Plr,$lr,f8,khe,Ilr,jlr,pG,Nlr,Dlr,qlr,m8,xhe,Glr,Olr,_G,Xlr,zlr,Vlr,g8,Rhe,Wlr,Qlr,uG,Hlr,Ulr,Jlr,h8,She,Ylr,Klr,bG,Zlr,eir,oir,p8,Phe,rir,tir,vG,air,nir,sir,$he,lir,iir,Rw,k9e,yc,_8,Ihe,Sw,dir,jhe,cir,x9e,Fr,Pw,fir,wc,mir,Nhe,gir,hir,Dhe,pir,_ir,uir,$w,bir,qhe,vir,Tir,Fir,ht,Iw,Cir,Ghe,Mir,Eir,Ac,yir,Ohe,wir,Air,Xhe,Lir,Bir,kir,zhe,xir,Rir,jw,Sir,Fo,Nw,Pir,Vhe,$ir,Iir,_n,jir,Whe,Nir,Dir,Qhe,qir,Gir,Hhe,Oir,Xir,zir,Uhe,u8,Jhe,Vir,Wir,TG,Qir,Hir,Uir,Yhe,Jir,Yir,Dw,R9e,Lc,b8,Khe,qw,Kir,Zhe,Zir,S9e,Cr,Gw,edr,Bc,odr,epe,rdr,tdr,ope,adr,ndr,sdr,Ow,ldr,rpe,idr,ddr,cdr,pt,Xw,fdr,tpe,mdr,gdr,kc,hdr,ape,pdr,_dr,npe,udr,bdr,vdr,spe,Tdr,Fdr,zw,Cdr,Co,Vw,Mdr,lpe,Edr,ydr,un,wdr,ipe,Adr,Ldr,dpe,Bdr,kdr,cpe,xdr,Rdr,Sdr,K,v8,fpe,Pdr,$dr,FG,Idr,jdr,Ndr,T8,mpe,Ddr,qdr,CG,Gdr,Odr,Xdr,F8,gpe,zdr,Vdr,MG,Wdr,Qdr,Hdr,C8,hpe,Udr,Jdr,EG,Ydr,Kdr,Zdr,M8,ppe,ecr,ocr,yG,rcr,tcr,acr,E8,_pe,ncr,scr,wG,lcr,icr,dcr,y8,upe,ccr,fcr,AG,mcr,gcr,hcr,w8,bpe,pcr,_cr,LG,ucr,bcr,vcr,A8,vpe,Tcr,Fcr,BG,Ccr,Mcr,Ecr,L8,Tpe,ycr,wcr,kG,Acr,Lcr,Bcr,B8,Fpe,kcr,xcr,xG,Rcr,Scr,Pcr,k8,Cpe,$cr,Icr,RG,jcr,Ncr,Dcr,x8,Mpe,qcr,Gcr,SG,Ocr,Xcr,zcr,R8,Epe,Vcr,Wcr,PG,Qcr,Hcr,Ucr,S8,ype,Jcr,Ycr,$G,Kcr,Zcr,efr,P8,wpe,ofr,rfr,IG,tfr,afr,nfr,$8,Ape,sfr,lfr,jG,ifr,dfr,cfr,I8,Lpe,ffr,mfr,NG,gfr,hfr,pfr,j8,Bpe,_fr,ufr,DG,bfr,vfr,Tfr,N8,kpe,Ffr,Cfr,qG,Mfr,Efr,yfr,xpe,wfr,Afr,Ww,P9e,xc,D8,Rpe,Qw,Lfr,Spe,Bfr,$9e,Mr,Hw,kfr,Rc,xfr,Ppe,Rfr,Sfr,$pe,Pfr,$fr,Ifr,Uw,jfr,Ipe,Nfr,Dfr,qfr,_t,Jw,Gfr,jpe,Ofr,Xfr,Sc,zfr,Npe,Vfr,Wfr,Dpe,Qfr,Hfr,Ufr,qpe,Jfr,Yfr,Yw,Kfr,Mo,Kw,Zfr,Gpe,emr,omr,bn,rmr,Ope,tmr,amr,Xpe,nmr,smr,zpe,lmr,imr,dmr,Z,q8,Vpe,cmr,fmr,GG,mmr,gmr,hmr,G8,Wpe,pmr,_mr,OG,umr,bmr,vmr,O8,Qpe,Tmr,Fmr,XG,Cmr,Mmr,Emr,X8,Hpe,ymr,wmr,zG,Amr,Lmr,Bmr,z8,Upe,kmr,xmr,VG,Rmr,Smr,Pmr,V8,Jpe,$mr,Imr,WG,jmr,Nmr,Dmr,W8,Ype,qmr,Gmr,QG,Omr,Xmr,zmr,Q8,Kpe,Vmr,Wmr,HG,Qmr,Hmr,Umr,H8,Zpe,Jmr,Ymr,UG,Kmr,Zmr,egr,U8,e_e,ogr,rgr,JG,tgr,agr,ngr,J8,o_e,sgr,lgr,YG,igr,dgr,cgr,Y8,r_e,fgr,mgr,KG,ggr,hgr,pgr,K8,t_e,_gr,ugr,ZG,bgr,vgr,Tgr,Z8,a_e,Fgr,Cgr,eO,Mgr,Egr,ygr,eF,n_e,wgr,Agr,oO,Lgr,Bgr,kgr,oF,s_e,xgr,Rgr,rO,Sgr,Pgr,$gr,rF,l_e,Igr,jgr,tO,Ngr,Dgr,qgr,tF,i_e,Ggr,Ogr,aO,Xgr,zgr,Vgr,aF,d_e,Wgr,Qgr,nO,Hgr,Ugr,Jgr,c_e,Ygr,Kgr,Zw,I9e,Pc,nF,f_e,eA,Zgr,m_e,ehr,j9e,Er,oA,ohr,$c,rhr,g_e,thr,ahr,h_e,nhr,shr,lhr,rA,ihr,p_e,dhr,chr,fhr,ut,tA,mhr,__e,ghr,hhr,Ic,phr,u_e,_hr,uhr,b_e,bhr,vhr,Thr,v_e,Fhr,Chr,aA,Mhr,Eo,nA,Ehr,T_e,yhr,whr,vn,Ahr,F_e,Lhr,Bhr,C_e,khr,xhr,M_e,Rhr,Shr,Phr,E_e,sF,y_e,$hr,Ihr,sO,jhr,Nhr,Dhr,w_e,qhr,Ghr,sA,N9e,jc,lF,A_e,lA,Ohr,L_e,Xhr,D9e,yr,iA,zhr,Nc,Vhr,B_e,Whr,Qhr,k_e,Hhr,Uhr,Jhr,dA,Yhr,x_e,Khr,Zhr,epr,bt,cA,opr,R_e,rpr,tpr,Dc,apr,S_e,npr,spr,P_e,lpr,ipr,dpr,$_e,cpr,fpr,fA,mpr,yo,mA,gpr,I_e,hpr,ppr,Tn,_pr,j_e,upr,bpr,N_e,vpr,Tpr,D_e,Fpr,Cpr,Mpr,q_e,iF,G_e,Epr,ypr,lO,wpr,Apr,Lpr,O_e,Bpr,kpr,gA,q9e,qc,dF,X_e,hA,xpr,z_e,Rpr,G9e,wr,pA,Spr,Gc,Ppr,V_e,$pr,Ipr,W_e,jpr,Npr,Dpr,_A,qpr,Q_e,Gpr,Opr,Xpr,vt,uA,zpr,H_e,Vpr,Wpr,Oc,Qpr,U_e,Hpr,Upr,J_e,Jpr,Ypr,Kpr,Y_e,Zpr,e_r,bA,o_r,wo,vA,r_r,K_e,t_r,a_r,Fn,n_r,Z_e,s_r,l_r,eue,i_r,d_r,oue,c_r,f_r,m_r,V,cF,rue,g_r,h_r,iO,p_r,__r,u_r,fF,tue,b_r,v_r,dO,T_r,F_r,C_r,mF,aue,M_r,E_r,cO,y_r,w_r,A_r,gF,nue,L_r,B_r,fO,k_r,x_r,R_r,hF,sue,S_r,P_r,mO,$_r,I_r,j_r,pF,lue,N_r,D_r,gO,q_r,G_r,O_r,_F,iue,X_r,z_r,hO,V_r,W_r,Q_r,uF,due,H_r,U_r,pO,J_r,Y_r,K_r,bF,cue,Z_r,eur,_O,our,rur,tur,vF,fue,aur,nur,uO,sur,lur,iur,TF,mue,dur,cur,bO,fur,mur,gur,FF,gue,hur,pur,vO,_ur,uur,bur,CF,hue,vur,Tur,TO,Fur,Cur,Mur,MF,pue,Eur,yur,FO,wur,Aur,Lur,EF,_ue,Bur,kur,CO,xur,Rur,Sur,yF,uue,Pur,$ur,MO,Iur,jur,Nur,wF,bue,Dur,qur,EO,Gur,Our,Xur,AF,vue,zur,Vur,yO,Wur,Qur,Hur,LF,Tue,Uur,Jur,wO,Yur,Kur,Zur,BF,Fue,e2r,o2r,AO,r2r,t2r,a2r,kF,Cue,n2r,s2r,LO,l2r,i2r,d2r,xF,Mue,c2r,f2r,BO,m2r,g2r,h2r,RF,Eue,p2r,_2r,kO,u2r,b2r,v2r,SF,yue,T2r,F2r,xO,C2r,M2r,E2r,wue,y2r,w2r,TA,O9e,Xc,PF,Aue,FA,A2r,Lue,L2r,X9e,Ar,CA,B2r,zc,k2r,Bue,x2r,R2r,kue,S2r,P2r,$2r,MA,I2r,xue,j2r,N2r,D2r,Tt,EA,q2r,Rue,G2r,O2r,Vc,X2r,Sue,z2r,V2r,Pue,W2r,Q2r,H2r,$ue,U2r,J2r,yA,Y2r,Ao,wA,K2r,Iue,Z2r,e1r,Cn,o1r,jue,r1r,t1r,Nue,a1r,n1r,Due,s1r,l1r,i1r,Mn,$F,que,d1r,c1r,RO,f1r,m1r,g1r,IF,Gue,h1r,p1r,SO,_1r,u1r,b1r,jF,Oue,v1r,T1r,PO,F1r,C1r,M1r,NF,Xue,E1r,y1r,$O,w1r,A1r,L1r,zue,B1r,k1r,AA,z9e,Wc,DF,Vue,LA,x1r,Wue,R1r,V9e,Lr,BA,S1r,Qc,P1r,Que,$1r,I1r,Hue,j1r,N1r,D1r,kA,q1r,Uue,G1r,O1r,X1r,Ft,xA,z1r,Jue,V1r,W1r,Hc,Q1r,Yue,H1r,U1r,Kue,J1r,Y1r,K1r,Zue,Z1r,ebr,RA,obr,Lo,SA,rbr,e2e,tbr,abr,En,nbr,o2e,sbr,lbr,r2e,ibr,dbr,t2e,cbr,fbr,mbr,fe,qF,a2e,gbr,hbr,IO,pbr,_br,ubr,GF,n2e,bbr,vbr,jO,Tbr,Fbr,Cbr,OF,s2e,Mbr,Ebr,NO,ybr,wbr,Abr,XF,l2e,Lbr,Bbr,DO,kbr,xbr,Rbr,zF,i2e,Sbr,Pbr,qO,$br,Ibr,jbr,VF,d2e,Nbr,Dbr,GO,qbr,Gbr,Obr,WF,c2e,Xbr,zbr,OO,Vbr,Wbr,Qbr,QF,f2e,Hbr,Ubr,XO,Jbr,Ybr,Kbr,HF,m2e,Zbr,e5r,zO,o5r,r5r,t5r,UF,g2e,a5r,n5r,VO,s5r,l5r,i5r,JF,h2e,d5r,c5r,WO,f5r,m5r,g5r,p2e,h5r,p5r,PA,W9e,Uc,YF,_2e,$A,_5r,u2e,u5r,Q9e,Br,IA,b5r,Jc,v5r,b2e,T5r,F5r,v2e,C5r,M5r,E5r,jA,y5r,T2e,w5r,A5r,L5r,Ct,NA,B5r,F2e,k5r,x5r,Yc,R5r,C2e,S5r,P5r,M2e,$5r,I5r,j5r,E2e,N5r,D5r,DA,q5r,Bo,qA,G5r,y2e,O5r,X5r,yn,z5r,w2e,V5r,W5r,A2e,Q5r,H5r,L2e,U5r,J5r,Y5r,ve,KF,B2e,K5r,Z5r,QO,evr,ovr,rvr,ZF,k2e,tvr,avr,HO,nvr,svr,lvr,eC,x2e,ivr,dvr,UO,cvr,fvr,mvr,oC,R2e,gvr,hvr,JO,pvr,_vr,uvr,rC,S2e,bvr,vvr,YO,Tvr,Fvr,Cvr,tC,P2e,Mvr,Evr,KO,yvr,wvr,Avr,aC,$2e,Lvr,Bvr,ZO,kvr,xvr,Rvr,nC,I2e,Svr,Pvr,eX,$vr,Ivr,jvr,sC,j2e,Nvr,Dvr,oX,qvr,Gvr,Ovr,N2e,Xvr,zvr,GA,H9e,Kc,lC,D2e,OA,Vvr,q2e,Wvr,U9e,kr,XA,Qvr,Zc,Hvr,G2e,Uvr,Jvr,O2e,Yvr,Kvr,Zvr,zA,e6r,X2e,o6r,r6r,t6r,Mt,VA,a6r,z2e,n6r,s6r,ef,l6r,V2e,i6r,d6r,W2e,c6r,f6r,m6r,Q2e,g6r,h6r,WA,p6r,ko,QA,_6r,H2e,u6r,b6r,wn,v6r,U2e,T6r,F6r,J2e,C6r,M6r,Y2e,E6r,y6r,w6r,Te,iC,K2e,A6r,L6r,rX,B6r,k6r,x6r,dC,Z2e,R6r,S6r,tX,P6r,$6r,I6r,cC,e1e,j6r,N6r,aX,D6r,q6r,G6r,fC,o1e,O6r,X6r,nX,z6r,V6r,W6r,mC,r1e,Q6r,H6r,sX,U6r,J6r,Y6r,gC,t1e,K6r,Z6r,lX,eTr,oTr,rTr,hC,a1e,tTr,aTr,iX,nTr,sTr,lTr,pC,n1e,iTr,dTr,dX,cTr,fTr,mTr,_C,s1e,gTr,hTr,cX,pTr,_Tr,uTr,l1e,bTr,vTr,HA,J9e,of,uC,i1e,UA,TTr,d1e,FTr,Y9e,xr,JA,CTr,rf,MTr,c1e,ETr,yTr,f1e,wTr,ATr,LTr,YA,BTr,m1e,kTr,xTr,RTr,Et,KA,STr,g1e,PTr,$Tr,tf,ITr,h1e,jTr,NTr,p1e,DTr,qTr,GTr,_1e,OTr,XTr,ZA,zTr,xo,e0,VTr,u1e,WTr,QTr,An,HTr,b1e,UTr,JTr,v1e,YTr,KTr,T1e,ZTr,e8r,o8r,Fe,bC,F1e,r8r,t8r,fX,a8r,n8r,s8r,vC,C1e,l8r,i8r,mX,d8r,c8r,f8r,TC,M1e,m8r,g8r,gX,h8r,p8r,_8r,FC,E1e,u8r,b8r,hX,v8r,T8r,F8r,CC,y1e,C8r,M8r,pX,E8r,y8r,w8r,MC,w1e,A8r,L8r,_X,B8r,k8r,x8r,EC,A1e,R8r,S8r,uX,P8r,$8r,I8r,yC,L1e,j8r,N8r,bX,D8r,q8r,G8r,wC,B1e,O8r,X8r,vX,z8r,V8r,W8r,k1e,Q8r,H8r,o0,K9e,af,AC,x1e,r0,U8r,R1e,J8r,Z9e,Rr,t0,Y8r,nf,K8r,S1e,Z8r,eFr,P1e,oFr,rFr,tFr,a0,aFr,$1e,nFr,sFr,lFr,yt,n0,iFr,I1e,dFr,cFr,sf,fFr,j1e,mFr,gFr,N1e,hFr,pFr,_Fr,D1e,uFr,bFr,s0,vFr,Ro,l0,TFr,q1e,FFr,CFr,Ln,MFr,G1e,EFr,yFr,O1e,wFr,AFr,X1e,LFr,BFr,kFr,Ce,LC,z1e,xFr,RFr,TX,SFr,PFr,$Fr,BC,V1e,IFr,jFr,FX,NFr,DFr,qFr,kC,W1e,GFr,OFr,CX,XFr,zFr,VFr,xC,Q1e,WFr,QFr,MX,HFr,UFr,JFr,RC,H1e,YFr,KFr,EX,ZFr,eCr,oCr,SC,U1e,rCr,tCr,yX,aCr,nCr,sCr,PC,J1e,lCr,iCr,wX,dCr,cCr,fCr,$C,Y1e,mCr,gCr,AX,hCr,pCr,_Cr,IC,K1e,uCr,bCr,LX,vCr,TCr,FCr,Z1e,CCr,MCr,i0,eBe,lf,jC,ebe,d0,ECr,obe,yCr,oBe,Sr,c0,wCr,df,ACr,rbe,LCr,BCr,tbe,kCr,xCr,RCr,f0,SCr,abe,PCr,$Cr,ICr,wt,m0,jCr,nbe,NCr,DCr,cf,qCr,sbe,GCr,OCr,lbe,XCr,zCr,VCr,ibe,WCr,QCr,g0,HCr,So,h0,UCr,dbe,JCr,YCr,Bn,KCr,cbe,ZCr,e4r,fbe,o4r,r4r,mbe,t4r,a4r,n4r,so,NC,gbe,s4r,l4r,BX,i4r,d4r,c4r,DC,hbe,f4r,m4r,kX,g4r,h4r,p4r,qC,pbe,_4r,u4r,xX,b4r,v4r,T4r,GC,_be,F4r,C4r,RX,M4r,E4r,y4r,OC,ube,w4r,A4r,SX,L4r,B4r,k4r,XC,bbe,x4r,R4r,PX,S4r,P4r,$4r,zC,vbe,I4r,j4r,$X,N4r,D4r,q4r,Tbe,G4r,O4r,p0,rBe,ff,VC,Fbe,_0,X4r,Cbe,z4r,tBe,Pr,u0,V4r,mf,W4r,Mbe,Q4r,H4r,Ebe,U4r,J4r,Y4r,b0,K4r,ybe,Z4r,eMr,oMr,At,v0,rMr,wbe,tMr,aMr,gf,nMr,Abe,sMr,lMr,Lbe,iMr,dMr,cMr,Bbe,fMr,mMr,T0,gMr,Po,F0,hMr,kbe,pMr,_Mr,kn,uMr,xbe,bMr,vMr,Rbe,TMr,FMr,Sbe,CMr,MMr,EMr,lo,WC,Pbe,yMr,wMr,IX,AMr,LMr,BMr,QC,$be,kMr,xMr,jX,RMr,SMr,PMr,HC,Ibe,$Mr,IMr,NX,jMr,NMr,DMr,UC,jbe,qMr,GMr,DX,OMr,XMr,zMr,JC,Nbe,VMr,WMr,qX,QMr,HMr,UMr,YC,Dbe,JMr,YMr,GX,KMr,ZMr,eEr,KC,qbe,oEr,rEr,OX,tEr,aEr,nEr,Gbe,sEr,lEr,C0,aBe,hf,ZC,Obe,M0,iEr,Xbe,dEr,nBe,$r,E0,cEr,pf,fEr,zbe,mEr,gEr,Vbe,hEr,pEr,_Er,y0,uEr,Wbe,bEr,vEr,TEr,Lt,w0,FEr,Qbe,CEr,MEr,_f,EEr,Hbe,yEr,wEr,Ube,AEr,LEr,BEr,Jbe,kEr,xEr,A0,REr,$o,L0,SEr,Ybe,PEr,$Er,xn,IEr,Kbe,jEr,NEr,Zbe,DEr,qEr,e5e,GEr,OEr,XEr,o5e,e4,r5e,zEr,VEr,XX,WEr,QEr,HEr,t5e,UEr,JEr,B0,sBe,uf,o4,a5e,k0,YEr,n5e,KEr,lBe,Ir,x0,ZEr,bf,e3r,s5e,o3r,r3r,l5e,t3r,a3r,n3r,R0,s3r,i5e,l3r,i3r,d3r,Bt,S0,c3r,d5e,f3r,m3r,vf,g3r,c5e,h3r,p3r,f5e,_3r,u3r,b3r,m5e,v3r,T3r,P0,F3r,Io,$0,C3r,g5e,M3r,E3r,Rn,y3r,h5e,w3r,A3r,p5e,L3r,B3r,_5e,k3r,x3r,R3r,I0,r4,u5e,S3r,P3r,zX,$3r,I3r,j3r,t4,b5e,N3r,D3r,VX,q3r,G3r,O3r,v5e,X3r,z3r,j0,iBe,Tf,a4,T5e,N0,V3r,F5e,W3r,dBe,jr,D0,Q3r,Ff,H3r,C5e,U3r,J3r,M5e,Y3r,K3r,Z3r,q0,eyr,E5e,oyr,ryr,tyr,kt,G0,ayr,y5e,nyr,syr,Cf,lyr,w5e,iyr,dyr,A5e,cyr,fyr,myr,L5e,gyr,hyr,O0,pyr,jo,X0,_yr,B5e,uyr,byr,Sn,vyr,k5e,Tyr,Fyr,x5e,Cyr,Myr,R5e,Eyr,yyr,wyr,S5e,n4,P5e,Ayr,Lyr,WX,Byr,kyr,xyr,$5e,Ryr,Syr,z0,cBe;return ce=new z({}),$a=new w({props:{code:'model = AutoModel.from_pretrained("bert-base-cased"),',highlighted:'model = AutoModel.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)'}}),lM=new z({}),iM=new w({props:{code:`from transformers import AutoConfig, AutoModel

AutoConfig.register("new-model", NewModelConfig)
AutoModel.register(NewModelConfig, NewModel),`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModel

AutoConfig.register(<span class="hljs-string">&quot;new-model&quot;</span>, NewModelConfig)
AutoModel.register(NewModelConfig, NewModel)`}}),Bf=new Pyr({props:{warning:"&lcub;true}",$$slots:{default:[X_t]},$$scope:{ctx:yi}}}),dM=new z({}),cM=new E({props:{name:"class transformers.AutoConfig",anchor:"transformers.AutoConfig",parameters:[],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/configuration_auto.py#L518"}}),gM=new E({props:{name:"from_pretrained",anchor:"transformers.AutoConfig.from_pretrained",parameters:[{name:"pretrained_model_name_or_path",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/configuration_auto.py#L541",parametersDescription:[{anchor:"transformers.AutoConfig.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model configuration hosted inside a model repo on
huggingface.co. Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or
namespaced under a user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing a configuration file saved using the
<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig.save_pretrained">save_pretrained()</a> method, or the <a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> method,
e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a saved configuration JSON <em>file</em>, e.g.,
<code>./my_model_directory/configuration.json</code>.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.AutoConfig.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.AutoConfig.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download the model weights and configuration files and override the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.AutoConfig.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.AutoConfig.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.AutoConfig.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.AutoConfig.from_pretrained.return_unused_kwargs",description:`<strong>return_unused_kwargs</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
If <code>False</code>, then this function returns just the final configuration object.</p>
<p>If <code>True</code>, then this functions returns a <code>Tuple(config, unused_kwargs)</code> where <em>unused_kwargs</em> is a
dictionary consisting of the key/value pairs whose keys are not configuration attributes: i.e., the
part of <code>kwargs</code> which has not been used to update <code>config</code> and is otherwise ignored.`,name:"return_unused_kwargs"},{anchor:"transformers.AutoConfig.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.AutoConfig.from_pretrained.kwargs(additional",description:`<strong>kwargs(additional</strong> keyword arguments, <em>optional</em>) &#x2014;
The values in kwargs of any keys which are configuration attributes will be used to override the loaded
values. Behavior concerning key/value pairs whose keys are <em>not</em> configuration attributes is controlled
by the <code>return_unused_kwargs</code> keyword parameter.`,name:"kwargs(additional"}]}}),hM=new w({props:{code:`from transformers import AutoConfig

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-uncased")

# Download configuration from huggingface.co (user-uploaded) and cache.
config = AutoConfig.from_pretrained("dbmdz/bert-base-german-cased")

# If configuration file is in a directory (e.g., was saved using *save_pretrained('./test/saved_model/')*).
config = AutoConfig.from_pretrained("./test/bert_saved_model/")

# Load a specific configuration file.
config = AutoConfig.from_pretrained("./test/bert_saved_model/my_configuration.json")

# Change some config attributes when loading a pretrained config.
config = AutoConfig.from_pretrained("bert-base-uncased", output_attentions=True, foo=False)
config.output_attentions

config, unused_kwargs = AutoConfig.from_pretrained(
    "bert-base-uncased", output_attentions=True, foo=False, return_unused_kwargs=True
)
config.output_attentions

config.unused_kwargs,`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-uncased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co (user-uploaded) and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;dbmdz/bert-base-german-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># If configuration file is in a directory (e.g., was saved using *save_pretrained(&#x27;./test/saved_model/&#x27;)*).</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./test/bert_saved_model/&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Load a specific configuration file.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./test/bert_saved_model/my_configuration.json&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Change some config attributes when loading a pretrained config.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-uncased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>, foo=<span class="hljs-literal">False</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>config, unused_kwargs = AutoConfig.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;bert-base-uncased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>, foo=<span class="hljs-literal">False</span>, return_unused_kwargs=<span class="hljs-literal">True</span>
<span class="hljs-meta">... </span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>config.unused_kwargs
{<span class="hljs-string">&#x27;foo&#x27;</span>: <span class="hljs-literal">False</span>}`}}),pM=new E({props:{name:"register",anchor:"transformers.AutoConfig.register",parameters:[{name:"model_type",val:""},{name:"config",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/configuration_auto.py#L663",parametersDescription:[{anchor:"transformers.AutoConfig.register.model_type",description:"<strong>model_type</strong> (<code>str</code>) &#x2014; The model type like &#x201C;bert&#x201D; or &#x201C;gpt&#x201D;.",name:"model_type"},{anchor:"transformers.AutoConfig.register.config",description:'<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014; The config to register.',name:"config"}]}}),_M=new z({}),uM=new E({props:{name:"class transformers.AutoTokenizer",anchor:"transformers.AutoTokenizer",parameters:[],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/tokenization_auto.py#L351"}}),TM=new E({props:{name:"from_pretrained",anchor:"transformers.AutoTokenizer.from_pretrained",parameters:[{name:"pretrained_model_name_or_path",val:""},{name:"*inputs",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/tokenization_auto.py#L365",parametersDescription:[{anchor:"transformers.AutoTokenizer.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a predefined tokenizer hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing vocabulary files required by the tokenizer, for instance saved
using the <a href="/docs/transformers/pr_15682/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.save_pretrained">save_pretrained()</a> method, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a single saved vocabulary file if and only if the tokenizer only requires a
single vocabulary file (like Bert or XLNet), e.g.: <code>./my_model_directory/vocab.txt</code>. (Not
applicable to all derived classes)</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.AutoTokenizer.from_pretrained.inputs",description:`<strong>inputs</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the Tokenizer <code>__init__()</code> method.`,name:"inputs"},{anchor:"transformers.AutoTokenizer.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
The configuration object used to dertermine the tokenizer class to instantiate.`,name:"config"},{anchor:"transformers.AutoTokenizer.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.AutoTokenizer.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download the model weights and configuration files and override the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.AutoTokenizer.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.AutoTokenizer.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.AutoTokenizer.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.AutoTokenizer.from_pretrained.subfolder",description:`<strong>subfolder</strong> (<code>str</code>, <em>optional</em>) &#x2014;
In case the relevant files are located inside a subfolder of the model repo on huggingface.co (e.g. for
facebook/rag-token-base), specify it here.`,name:"subfolder"},{anchor:"transformers.AutoTokenizer.from_pretrained.use_fast",description:`<strong>use_fast</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to try to load the fast version of the tokenizer.`,name:"use_fast"},{anchor:"transformers.AutoTokenizer.from_pretrained.tokenizer_type",description:`<strong>tokenizer_type</strong> (<code>str</code>, <em>optional</em>) &#x2014;
Tokenizer type to be loaded.`,name:"tokenizer_type"},{anchor:"transformers.AutoTokenizer.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.AutoTokenizer.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Will be passed to the Tokenizer <code>__init__()</code> method. Can be used to set special tokens like
<code>bos_token</code>, <code>eos_token</code>, <code>unk_token</code>, <code>sep_token</code>, <code>pad_token</code>, <code>cls_token</code>, <code>mask_token</code>,
<code>additional_special_tokens</code>. See parameters in the <code>__init__()</code> for more details.`,name:"kwargs"}]}}),FM=new w({props:{code:`from transformers import AutoTokenizer

# Download vocabulary from huggingface.co and cache.
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

# Download vocabulary from huggingface.co (user-uploaded) and cache.
tokenizer = AutoTokenizer.from_pretrained("dbmdz/bert-base-german-cased")

# If vocabulary files are in a directory (e.g. tokenizer was saved using *save_pretrained('./test/saved_model/')*)
tokenizer = AutoTokenizer.from_pretrained("./test/bert_saved_model/"),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download vocabulary from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;bert-base-uncased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download vocabulary from huggingface.co (user-uploaded) and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;dbmdz/bert-base-german-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># If vocabulary files are in a directory (e.g. tokenizer was saved using *save_pretrained(&#x27;./test/saved_model/&#x27;)*)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;./test/bert_saved_model/&quot;</span>)`}}),CM=new E({props:{name:"register",anchor:"transformers.AutoTokenizer.register",parameters:[{name:"config_class",val:""},{name:"slow_tokenizer_class",val:" = None"},{name:"fast_tokenizer_class",val:" = None"}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/tokenization_auto.py#L561",parametersDescription:[{anchor:"transformers.AutoTokenizer.register.config_class",description:`<strong>config_class</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The configuration corresponding to the model to register.`,name:"config_class"},{anchor:"transformers.AutoTokenizer.register.slow_tokenizer_class",description:`<strong>slow_tokenizer_class</strong> (<code>PretrainedTokenizer</code>, <em>optional</em>) &#x2014;
The slow tokenizer to register.`,name:"slow_tokenizer_class"},{anchor:"transformers.AutoTokenizer.register.slow_tokenizer_class",description:`<strong>slow_tokenizer_class</strong> (<code>PretrainedTokenizerFast</code>, <em>optional</em>) &#x2014;
The fast tokenizer to register.`,name:"slow_tokenizer_class"}]}}),MM=new z({}),EM=new E({props:{name:"class transformers.AutoFeatureExtractor",anchor:"transformers.AutoFeatureExtractor",parameters:[],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/feature_extraction_auto.py#L169"}}),AM=new E({props:{name:"from_pretrained",anchor:"transformers.AutoFeatureExtractor.from_pretrained",parameters:[{name:"pretrained_model_name_or_path",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/feature_extraction_auto.py#L183",parametersDescription:[{anchor:"transformers.AutoFeatureExtractor.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
This can be either:</p>
<ul>
<li>a string, the <em>model id</em> of a pretrained feature_extractor hosted inside a model repo on
huggingface.co. Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or
namespaced under a user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>a path to a <em>directory</em> containing a feature extractor file saved using the
<a href="/docs/transformers/pr_15682/en/main_classes/feature_extractor#transformers.FeatureExtractionMixin.save_pretrained">save_pretrained()</a> method, e.g.,
<code>./my_model_directory/</code>.</li>
<li>a path or url to a saved feature extractor JSON <em>file</em>, e.g.,
<code>./my_model_directory/preprocessor_config.json</code>.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.AutoFeatureExtractor.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model feature extractor should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.AutoFeatureExtractor.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force to (re-)download the feature extractor files and override the cached versions
if they exist.`,name:"force_download"},{anchor:"transformers.AutoFeatureExtractor.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received file. Attempts to resume the download if such a file
exists.`,name:"resume_download"},{anchor:"transformers.AutoFeatureExtractor.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}.</code> The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.AutoFeatureExtractor.from_pretrained.use_auth_token",description:`<strong>use_auth_token</strong> (<code>str</code> or <em>bool</em>, <em>optional</em>) &#x2014;
The token to use as HTTP bearer authorization for remote files. If <code>True</code>, will use the token generated
when running <code>transformers-cli login</code> (stored in <code>~/.huggingface</code>).`,name:"use_auth_token"},{anchor:"transformers.AutoFeatureExtractor.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.AutoFeatureExtractor.from_pretrained.return_unused_kwargs",description:`<strong>return_unused_kwargs</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
If <code>False</code>, then this function returns just the final feature extractor object. If <code>True</code>, then this
functions returns a <code>Tuple(feature_extractor, unused_kwargs)</code> where <em>unused_kwargs</em> is a dictionary
consisting of the key/value pairs whose keys are not feature extractor attributes: i.e., the part of
<code>kwargs</code> which has not been used to update <code>feature_extractor</code> and is otherwise ignored.`,name:"return_unused_kwargs"},{anchor:"transformers.AutoFeatureExtractor.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.AutoFeatureExtractor.from_pretrained.kwargs",description:`<strong>kwargs</strong> (<code>Dict[str, Any]</code>, <em>optional</em>) &#x2014;
The values in kwargs of any keys which are feature extractor attributes will be used to override the
loaded values. Behavior concerning key/value pairs whose keys are <em>not</em> feature extractor attributes is
controlled by the <code>return_unused_kwargs</code> keyword parameter.`,name:"kwargs"}]}}),dh=new Pyr({props:{$$slots:{default:[z_t]},$$scope:{ctx:yi}}}),LM=new w({props:{code:`from transformers import AutoFeatureExtractor

# Download feature extractor from huggingface.co and cache.
feature_extractor = AutoFeatureExtractor.from_pretrained("facebook/wav2vec2-base-960h")

# If feature extractor files are in a directory (e.g. feature extractor was saved using *save_pretrained('./test/saved_model/')*)
feature_extractor = AutoFeatureExtractor.from_pretrained("./test/saved_model/"),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoFeatureExtractor

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download feature extractor from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>feature_extractor = AutoFeatureExtractor.from_pretrained(<span class="hljs-string">&quot;facebook/wav2vec2-base-960h&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># If feature extractor files are in a directory (e.g. feature extractor was saved using *save_pretrained(&#x27;./test/saved_model/&#x27;)*)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>feature_extractor = AutoFeatureExtractor.from_pretrained(<span class="hljs-string">&quot;./test/saved_model/&quot;</span>)`}}),BM=new E({props:{name:"register",anchor:"transformers.AutoFeatureExtractor.register",parameters:[{name:"config_class",val:""},{name:"feature_extractor_class",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/feature_extraction_auto.py#L310",parametersDescription:[{anchor:"transformers.AutoFeatureExtractor.register.config_class",description:`<strong>config_class</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The configuration corresponding to the model to register.`,name:"config_class"},{anchor:"transformers.AutoFeatureExtractor.register.feature_extractor_class",description:"<strong>feature_extractor_class</strong> (<code>FeatureExtractorMixin</code>) &#x2014; The feature extractor to register.",name:"feature_extractor_class"}]}}),kM=new z({}),xM=new E({props:{name:"class transformers.AutoProcessor",anchor:"transformers.AutoProcessor",parameters:[],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/processing_auto.py#L71"}}),PM=new E({props:{name:"from_pretrained",anchor:"transformers.AutoProcessor.from_pretrained",parameters:[{name:"pretrained_model_name_or_path",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/processing_auto.py#L85",parametersDescription:[{anchor:"transformers.AutoProcessor.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
This can be either:</p>
<ul>
<li>a string, the <em>model id</em> of a pretrained feature_extractor hosted inside a model repo on
huggingface.co. Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or
namespaced under a user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>a path to a <em>directory</em> containing a processor files saved using the <code>save_pretrained()</code> method,
e.g., <code>./my_model_directory/</code>.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.AutoProcessor.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model feature extractor should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.AutoProcessor.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force to (re-)download the feature extractor files and override the cached versions
if they exist.`,name:"force_download"},{anchor:"transformers.AutoProcessor.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received file. Attempts to resume the download if such a file
exists.`,name:"resume_download"},{anchor:"transformers.AutoProcessor.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}.</code> The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.AutoProcessor.from_pretrained.use_auth_token",description:`<strong>use_auth_token</strong> (<code>str</code> or <em>bool</em>, <em>optional</em>) &#x2014;
The token to use as HTTP bearer authorization for remote files. If <code>True</code>, will use the token generated
when running <code>transformers-cli login</code> (stored in <code>~/.huggingface</code>).`,name:"use_auth_token"},{anchor:"transformers.AutoProcessor.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.AutoProcessor.from_pretrained.return_unused_kwargs",description:`<strong>return_unused_kwargs</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
If <code>False</code>, then this function returns just the final feature extractor object. If <code>True</code>, then this
functions returns a <code>Tuple(feature_extractor, unused_kwargs)</code> where <em>unused_kwargs</em> is a dictionary
consisting of the key/value pairs whose keys are not feature extractor attributes: i.e., the part of
<code>kwargs</code> which has not been used to update <code>feature_extractor</code> and is otherwise ignored.`,name:"return_unused_kwargs"},{anchor:"transformers.AutoProcessor.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.AutoProcessor.from_pretrained.kwargs",description:`<strong>kwargs</strong> (<code>Dict[str, Any]</code>, <em>optional</em>) &#x2014;
The values in kwargs of any keys which are feature extractor attributes will be used to override the
loaded values. Behavior concerning key/value pairs whose keys are <em>not</em> feature extractor attributes is
controlled by the <code>return_unused_kwargs</code> keyword parameter.`,name:"kwargs"}]}}),Th=new Pyr({props:{$$slots:{default:[V_t]},$$scope:{ctx:yi}}}),$M=new w({props:{code:`from transformers import AutoProcessor

# Download processor from huggingface.co and cache.
processor = AutoProcessor.from_pretrained("facebook/wav2vec2-base-960h")

# If processor files are in a directory (e.g. processor was saved using *save_pretrained('./test/saved_model/')*)
processor = AutoProcessor.from_pretrained("./test/saved_model/"),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoProcessor

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download processor from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>processor = AutoProcessor.from_pretrained(<span class="hljs-string">&quot;facebook/wav2vec2-base-960h&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># If processor files are in a directory (e.g. processor was saved using *save_pretrained(&#x27;./test/saved_model/&#x27;)*)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>processor = AutoProcessor.from_pretrained(<span class="hljs-string">&quot;./test/saved_model/&quot;</span>)`}}),IM=new E({props:{name:"register",anchor:"transformers.AutoProcessor.register",parameters:[{name:"config_class",val:""},{name:"processor_class",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/processing_auto.py#L238",parametersDescription:[{anchor:"transformers.AutoProcessor.register.config_class",description:`<strong>config_class</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The configuration corresponding to the model to register.`,name:"config_class"},{anchor:"transformers.AutoProcessor.register.processor_class",description:"<strong>processor_class</strong> (<code>FeatureExtractorMixin</code>) &#x2014; The processor to register.",name:"processor_class"}]}}),jM=new z({}),NM=new E({props:{name:"class transformers.AutoModel",anchor:"transformers.AutoModel",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/modeling_auto.py#L673"}}),qM=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15682/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/albert#transformers.AlbertModel">AlbertModel</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/bart#transformers.BartModel">BartModel</a> (BART model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/beit#transformers.BeitConfig">BeitConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/beit#transformers.BeitModel">BeitModel</a> (BEiT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/bert#transformers.BertModel">BertModel</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/bert-generation#transformers.BertGenerationConfig">BertGenerationConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/bert-generation#transformers.BertGenerationEncoder">BertGenerationEncoder</a> (Bert Generation model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/big_bird#transformers.BigBirdModel">BigBirdModel</a> (BigBird model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusConfig">BigBirdPegasusConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusModel">BigBirdPegasusModel</a> (BigBirdPegasus model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/blenderbot#transformers.BlenderbotConfig">BlenderbotConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/blenderbot#transformers.BlenderbotModel">BlenderbotModel</a> (Blenderbot model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/blenderbot-small#transformers.BlenderbotSmallConfig">BlenderbotSmallConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/blenderbot-small#transformers.BlenderbotSmallModel">BlenderbotSmallModel</a> (BlenderbotSmall model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/clip#transformers.CLIPConfig">CLIPConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/clip#transformers.CLIPModel">CLIPModel</a> (CLIP model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/ctrl#transformers.CTRLConfig">CTRLConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/ctrl#transformers.CTRLModel">CTRLModel</a> (CTRL model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/camembert#transformers.CamembertModel">CamembertModel</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/canine#transformers.CanineConfig">CanineConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/canine#transformers.CanineModel">CanineModel</a> (Canine model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/convbert#transformers.ConvBertModel">ConvBertModel</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/convnext#transformers.ConvNextConfig">ConvNextConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/convnext#transformers.ConvNextModel">ConvNextModel</a> (ConvNext model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/dpr#transformers.DPRConfig">DPRConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/dpr#transformers.DPRQuestionEncoder">DPRQuestionEncoder</a> (DPR model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/deberta#transformers.DebertaModel">DebertaModel</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/deberta-v2#transformers.DebertaV2Model">DebertaV2Model</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/deit#transformers.DeiTConfig">DeiTConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/deit#transformers.DeiTModel">DeiTModel</a> (DeiT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/detr#transformers.DetrConfig">DetrConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/detr#transformers.DetrModel">DetrModel</a> (DETR model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/distilbert#transformers.DistilBertModel">DistilBertModel</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/electra#transformers.ElectraModel">ElectraModel</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/fnet#transformers.FNetConfig">FNetConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/fnet#transformers.FNetModel">FNetModel</a> (FNet model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/fsmt#transformers.FSMTConfig">FSMTConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/fsmt#transformers.FSMTModel">FSMTModel</a> (FairSeq Machine-Translation model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/flaubert#transformers.FlaubertModel">FlaubertModel</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/funnel#transformers.FunnelModel">FunnelModel</a> or <a href="/docs/transformers/pr_15682/en/model_doc/funnel#transformers.FunnelBaseModel">FunnelBaseModel</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/gpt2#transformers.GPT2Model">GPT2Model</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/gptj#transformers.GPTJConfig">GPTJConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/gptj#transformers.GPTJModel">GPTJModel</a> (GPT-J model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/gpt_neo#transformers.GPTNeoConfig">GPTNeoConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/gpt_neo#transformers.GPTNeoModel">GPTNeoModel</a> (GPT Neo model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/hubert#transformers.HubertConfig">HubertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/hubert#transformers.HubertModel">HubertModel</a> (Hubert model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/ibert#transformers.IBertConfig">IBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/ibert#transformers.IBertModel">IBertModel</a> (I-BERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/imagegpt#transformers.ImageGPTConfig">ImageGPTConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/imagegpt#transformers.ImageGPTModel">ImageGPTModel</a> (ImageGPT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/led#transformers.LEDConfig">LEDConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/led#transformers.LEDModel">LEDModel</a> (LED model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/layoutlm#transformers.LayoutLMModel">LayoutLMModel</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/layoutlmv2#transformers.LayoutLMv2Config">LayoutLMv2Config</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/layoutlmv2#transformers.LayoutLMv2Model">LayoutLMv2Model</a> (LayoutLMv2 model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/longformer#transformers.LongformerModel">LongformerModel</a> (Longformer model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/luke#transformers.LukeConfig">LukeConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/luke#transformers.LukeModel">LukeModel</a> (LUKE model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/lxmert#transformers.LxmertConfig">LxmertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/lxmert#transformers.LxmertModel">LxmertModel</a> (LXMERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/m2m_100#transformers.M2M100Config">M2M100Config</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/m2m_100#transformers.M2M100Model">M2M100Model</a> (M2M100 model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/mbart#transformers.MBartModel">MBartModel</a> (mBART model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/mpnet#transformers.MPNetModel">MPNetModel</a> (MPNet model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/mt5#transformers.MT5Config">MT5Config</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/mt5#transformers.MT5Model">MT5Model</a> (mT5 model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/marian#transformers.MarianConfig">MarianConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/marian#transformers.MarianModel">MarianModel</a> (Marian model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/maskformer#transformers.MaskFormerConfig">MaskFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/maskformer#transformers.MaskFormerModel">MaskFormerModel</a> (MaskFormer model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/megatron-bert#transformers.MegatronBertConfig">MegatronBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/megatron-bert#transformers.MegatronBertModel">MegatronBertModel</a> (MegatronBert model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/mobilebert#transformers.MobileBertModel">MobileBertModel</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/nystromformer#transformers.NystromformerConfig">NystromformerConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/nystromformer#transformers.NystromformerModel">NystromformerModel</a> (Nystromformer model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig">OpenAIGPTConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/openai-gpt#transformers.OpenAIGPTModel">OpenAIGPTModel</a> (OpenAI GPT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/plbart#transformers.PLBartConfig">PLBartConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/plbart#transformers.PLBartModel">PLBartModel</a> (PLBart model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/pegasus#transformers.PegasusConfig">PegasusConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/pegasus#transformers.PegasusModel">PegasusModel</a> (Pegasus model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/perceiver#transformers.PerceiverConfig">PerceiverConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/perceiver#transformers.PerceiverModel">PerceiverModel</a> (Perceiver model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/poolformer#transformers.PoolFormerConfig">PoolFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/poolformer#transformers.PoolFormerModel">PoolFormerModel</a> (PoolFormer model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/prophetnet#transformers.ProphetNetConfig">ProphetNetConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/prophetnet#transformers.ProphetNetModel">ProphetNetModel</a> (ProphetNet model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/qdqbert#transformers.QDQBertConfig">QDQBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/qdqbert#transformers.QDQBertModel">QDQBertModel</a> (QDQBert model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/reformer#transformers.ReformerConfig">ReformerConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/reformer#transformers.ReformerModel">ReformerModel</a> (Reformer model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/rembert#transformers.RemBertModel">RemBertModel</a> (RemBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/retribert#transformers.RetriBertConfig">RetriBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/retribert#transformers.RetriBertModel">RetriBertModel</a> (RetriBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/roformer#transformers.RoFormerModel">RoFormerModel</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/roberta#transformers.RobertaModel">RobertaModel</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/sew#transformers.SEWConfig">SEWConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/sew#transformers.SEWModel">SEWModel</a> (SEW model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/sew-d#transformers.SEWDConfig">SEWDConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/sew-d#transformers.SEWDModel">SEWDModel</a> (SEW-D model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/segformer#transformers.SegformerConfig">SegformerConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/segformer#transformers.SegformerModel">SegformerModel</a> (SegFormer model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/speech_to_text#transformers.Speech2TextConfig">Speech2TextConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/speech_to_text#transformers.Speech2TextModel">Speech2TextModel</a> (Speech2Text model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/splinter#transformers.SplinterConfig">SplinterConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/splinter#transformers.SplinterModel">SplinterModel</a> (Splinter model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/squeezebert#transformers.SqueezeBertConfig">SqueezeBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/squeezebert#transformers.SqueezeBertModel">SqueezeBertModel</a> (SqueezeBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/swin#transformers.SwinConfig">SwinConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/swin#transformers.SwinModel">SwinModel</a> (Swin model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/t5#transformers.T5Config">T5Config</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/t5#transformers.T5Model">T5Model</a> (T5 model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/tapas#transformers.TapasModel">TapasModel</a> (TAPAS model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/transfo-xl#transformers.TransfoXLConfig">TransfoXLConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/transfo-xl#transformers.TransfoXLModel">TransfoXLModel</a> (Transformer-XL model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/unispeech#transformers.UniSpeechConfig">UniSpeechConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/unispeech#transformers.UniSpeechModel">UniSpeechModel</a> (UniSpeech model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/unispeech-sat#transformers.UniSpeechSatConfig">UniSpeechSatConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/unispeech-sat#transformers.UniSpeechSatModel">UniSpeechSatModel</a> (UniSpeechSat model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/vit#transformers.ViTConfig">ViTConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/vit#transformers.ViTModel">ViTModel</a> (ViT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/vit_mae#transformers.ViTMAEConfig">ViTMAEConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/vit_mae#transformers.ViTMAEModel">ViTMAEModel</a> (ViTMAE model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/vilt#transformers.ViltConfig">ViltConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/vilt#transformers.ViltModel">ViltModel</a> (ViLT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/vision-text-dual-encoder#transformers.VisionTextDualEncoderConfig">VisionTextDualEncoderConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/vision-text-dual-encoder#transformers.VisionTextDualEncoderModel">VisionTextDualEncoderModel</a> (VisionTextDualEncoder model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/visual_bert#transformers.VisualBertConfig">VisualBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/visual_bert#transformers.VisualBertModel">VisualBertModel</a> (VisualBert model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/wav2vec2#transformers.Wav2Vec2Model">Wav2Vec2Model</a> (Wav2Vec2 model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/wavlm#transformers.WavLMConfig">WavLMConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/wavlm#transformers.WavLMModel">WavLMModel</a> (WavLM model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/xglm#transformers.XGLMConfig">XGLMConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/xglm#transformers.XGLMModel">XGLMModel</a> (XGLM model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/xlm#transformers.XLMModel">XLMModel</a> (XLM model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetConfig">XLMProphetNetConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetModel">XLMProphetNetModel</a> (XLMProphetNet model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/xlm-roberta#transformers.XLMRobertaModel">XLMRobertaModel</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLConfig">XLMRobertaXLConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLModel">XLMRobertaXLModel</a> (XLM-RoBERTa-XL model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/xlnet#transformers.XLNetModel">XLNetModel</a> (XLNet model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/yoso#transformers.YosoConfig">YosoConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/yoso#transformers.YosoModel">YosoModel</a> (YOSO model)</li>
</ul>`,name:"config"}]}}),GM=new w({props:{code:`from transformers import AutoConfig, AutoModel

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModel.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModel.from_config(config)`}}),OM=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),XM=new w({props:{code:`from transformers import AutoConfig, AutoModel

# Download model and configuration from huggingface.co and cache.
model = AutoModel.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModel.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModel.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModel.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModel.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModel.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),zM=new z({}),VM=new E({props:{name:"class transformers.AutoModelForPreTraining",anchor:"transformers.AutoModelForPreTraining",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/modeling_auto.py#L680"}}),QM=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15682/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/albert#transformers.AlbertForPreTraining">AlbertForPreTraining</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/bart#transformers.BartForConditionalGeneration">BartForConditionalGeneration</a> (BART model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/bert#transformers.BertForPreTraining">BertForPreTraining</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/big_bird#transformers.BigBirdForPreTraining">BigBirdForPreTraining</a> (BigBird model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/ctrl#transformers.CTRLConfig">CTRLConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/ctrl#transformers.CTRLLMHeadModel">CTRLLMHeadModel</a> (CTRL model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/camembert#transformers.CamembertForMaskedLM">CamembertForMaskedLM</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/deberta#transformers.DebertaForMaskedLM">DebertaForMaskedLM</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/deberta-v2#transformers.DebertaV2ForMaskedLM">DebertaV2ForMaskedLM</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/distilbert#transformers.DistilBertForMaskedLM">DistilBertForMaskedLM</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/electra#transformers.ElectraForPreTraining">ElectraForPreTraining</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/fnet#transformers.FNetConfig">FNetConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/fnet#transformers.FNetForPreTraining">FNetForPreTraining</a> (FNet model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/fsmt#transformers.FSMTConfig">FSMTConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/fsmt#transformers.FSMTForConditionalGeneration">FSMTForConditionalGeneration</a> (FairSeq Machine-Translation model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/flaubert#transformers.FlaubertWithLMHeadModel">FlaubertWithLMHeadModel</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/funnel#transformers.FunnelForPreTraining">FunnelForPreTraining</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/gpt2#transformers.GPT2LMHeadModel">GPT2LMHeadModel</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/ibert#transformers.IBertConfig">IBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/ibert#transformers.IBertForMaskedLM">IBertForMaskedLM</a> (I-BERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/layoutlm#transformers.LayoutLMForMaskedLM">LayoutLMForMaskedLM</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/longformer#transformers.LongformerForMaskedLM">LongformerForMaskedLM</a> (Longformer model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/lxmert#transformers.LxmertConfig">LxmertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/lxmert#transformers.LxmertForPreTraining">LxmertForPreTraining</a> (LXMERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/mpnet#transformers.MPNetForMaskedLM">MPNetForMaskedLM</a> (MPNet model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/megatron-bert#transformers.MegatronBertConfig">MegatronBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/megatron-bert#transformers.MegatronBertForPreTraining">MegatronBertForPreTraining</a> (MegatronBert model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/mobilebert#transformers.MobileBertForPreTraining">MobileBertForPreTraining</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig">OpenAIGPTConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/openai-gpt#transformers.OpenAIGPTLMHeadModel">OpenAIGPTLMHeadModel</a> (OpenAI GPT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/retribert#transformers.RetriBertConfig">RetriBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/retribert#transformers.RetriBertModel">RetriBertModel</a> (RetriBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/roberta#transformers.RobertaForMaskedLM">RobertaForMaskedLM</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/squeezebert#transformers.SqueezeBertConfig">SqueezeBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/squeezebert#transformers.SqueezeBertForMaskedLM">SqueezeBertForMaskedLM</a> (SqueezeBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/t5#transformers.T5Config">T5Config</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/t5#transformers.T5ForConditionalGeneration">T5ForConditionalGeneration</a> (T5 model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/tapas#transformers.TapasForMaskedLM">TapasForMaskedLM</a> (TAPAS model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/transfo-xl#transformers.TransfoXLConfig">TransfoXLConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/transfo-xl#transformers.TransfoXLLMHeadModel">TransfoXLLMHeadModel</a> (Transformer-XL model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/unispeech#transformers.UniSpeechConfig">UniSpeechConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/unispeech#transformers.UniSpeechForPreTraining">UniSpeechForPreTraining</a> (UniSpeech model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/unispeech-sat#transformers.UniSpeechSatConfig">UniSpeechSatConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/unispeech-sat#transformers.UniSpeechSatForPreTraining">UniSpeechSatForPreTraining</a> (UniSpeechSat model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/vit_mae#transformers.ViTMAEConfig">ViTMAEConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/vit_mae#transformers.ViTMAEForPreTraining">ViTMAEForPreTraining</a> (ViTMAE model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/visual_bert#transformers.VisualBertConfig">VisualBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/visual_bert#transformers.VisualBertForPreTraining">VisualBertForPreTraining</a> (VisualBert model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/wav2vec2#transformers.Wav2Vec2ForPreTraining">Wav2Vec2ForPreTraining</a> (Wav2Vec2 model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/xlm#transformers.XLMWithLMHeadModel">XLMWithLMHeadModel</a> (XLM model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/xlm-roberta#transformers.XLMRobertaForMaskedLM">XLMRobertaForMaskedLM</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLConfig">XLMRobertaXLConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForMaskedLM">XLMRobertaXLForMaskedLM</a> (XLM-RoBERTa-XL model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/xlnet#transformers.XLNetLMHeadModel">XLNetLMHeadModel</a> (XLNet model)</li>
</ul>`,name:"config"}]}}),HM=new w({props:{code:`from transformers import AutoConfig, AutoModelForPreTraining

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForPreTraining.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForPreTraining

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForPreTraining.from_config(config)`}}),UM=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),JM=new w({props:{code:`from transformers import AutoConfig, AutoModelForPreTraining

# Download model and configuration from huggingface.co and cache.
model = AutoModelForPreTraining.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForPreTraining.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForPreTraining.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForPreTraining

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForPreTraining.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForPreTraining.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForPreTraining.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),YM=new z({}),KM=new E({props:{name:"class transformers.AutoModelForCausalLM",anchor:"transformers.AutoModelForCausalLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/modeling_auto.py#L695"}}),eE=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15682/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/bart#transformers.BartForCausalLM">BartForCausalLM</a> (BART model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/bert#transformers.BertLMHeadModel">BertLMHeadModel</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/bert-generation#transformers.BertGenerationConfig">BertGenerationConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/bert-generation#transformers.BertGenerationDecoder">BertGenerationDecoder</a> (Bert Generation model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/big_bird#transformers.BigBirdForCausalLM">BigBirdForCausalLM</a> (BigBird model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusConfig">BigBirdPegasusConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusForCausalLM">BigBirdPegasusForCausalLM</a> (BigBirdPegasus model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/blenderbot#transformers.BlenderbotConfig">BlenderbotConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/blenderbot#transformers.BlenderbotForCausalLM">BlenderbotForCausalLM</a> (Blenderbot model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/blenderbot-small#transformers.BlenderbotSmallConfig">BlenderbotSmallConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/blenderbot-small#transformers.BlenderbotSmallForCausalLM">BlenderbotSmallForCausalLM</a> (BlenderbotSmall model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/ctrl#transformers.CTRLConfig">CTRLConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/ctrl#transformers.CTRLLMHeadModel">CTRLLMHeadModel</a> (CTRL model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/camembert#transformers.CamembertForCausalLM">CamembertForCausalLM</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/electra#transformers.ElectraForCausalLM">ElectraForCausalLM</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/gpt2#transformers.GPT2LMHeadModel">GPT2LMHeadModel</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/gptj#transformers.GPTJConfig">GPTJConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/gptj#transformers.GPTJForCausalLM">GPTJForCausalLM</a> (GPT-J model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/gpt_neo#transformers.GPTNeoConfig">GPTNeoConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/gpt_neo#transformers.GPTNeoForCausalLM">GPTNeoForCausalLM</a> (GPT Neo model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/mbart#transformers.MBartForCausalLM">MBartForCausalLM</a> (mBART model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/marian#transformers.MarianConfig">MarianConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/marian#transformers.MarianForCausalLM">MarianForCausalLM</a> (Marian model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/megatron-bert#transformers.MegatronBertConfig">MegatronBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/megatron-bert#transformers.MegatronBertForCausalLM">MegatronBertForCausalLM</a> (MegatronBert model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig">OpenAIGPTConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/openai-gpt#transformers.OpenAIGPTLMHeadModel">OpenAIGPTLMHeadModel</a> (OpenAI GPT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/plbart#transformers.PLBartConfig">PLBartConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/plbart#transformers.PLBartForCausalLM">PLBartForCausalLM</a> (PLBart model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/pegasus#transformers.PegasusConfig">PegasusConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/pegasus#transformers.PegasusForCausalLM">PegasusForCausalLM</a> (Pegasus model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/prophetnet#transformers.ProphetNetConfig">ProphetNetConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/prophetnet#transformers.ProphetNetForCausalLM">ProphetNetForCausalLM</a> (ProphetNet model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/qdqbert#transformers.QDQBertConfig">QDQBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/qdqbert#transformers.QDQBertLMHeadModel">QDQBertLMHeadModel</a> (QDQBert model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/reformer#transformers.ReformerConfig">ReformerConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/reformer#transformers.ReformerModelWithLMHead">ReformerModelWithLMHead</a> (Reformer model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/rembert#transformers.RemBertForCausalLM">RemBertForCausalLM</a> (RemBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/roformer#transformers.RoFormerForCausalLM">RoFormerForCausalLM</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/roberta#transformers.RobertaForCausalLM">RobertaForCausalLM</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/speech_to_text_2#transformers.Speech2Text2Config">Speech2Text2Config</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/speech_to_text_2#transformers.Speech2Text2ForCausalLM">Speech2Text2ForCausalLM</a> (Speech2Text2 model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/trocr#transformers.TrOCRConfig">TrOCRConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/trocr#transformers.TrOCRForCausalLM">TrOCRForCausalLM</a> (TrOCR model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/transfo-xl#transformers.TransfoXLConfig">TransfoXLConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/transfo-xl#transformers.TransfoXLLMHeadModel">TransfoXLLMHeadModel</a> (Transformer-XL model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/xglm#transformers.XGLMConfig">XGLMConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/xglm#transformers.XGLMForCausalLM">XGLMForCausalLM</a> (XGLM model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/xlm#transformers.XLMWithLMHeadModel">XLMWithLMHeadModel</a> (XLM model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetConfig">XLMProphetNetConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetForCausalLM">XLMProphetNetForCausalLM</a> (XLMProphetNet model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/xlm-roberta#transformers.XLMRobertaForCausalLM">XLMRobertaForCausalLM</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLConfig">XLMRobertaXLConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForCausalLM">XLMRobertaXLForCausalLM</a> (XLM-RoBERTa-XL model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/xlnet#transformers.XLNetLMHeadModel">XLNetLMHeadModel</a> (XLNet model)</li>
</ul>`,name:"config"}]}}),oE=new w({props:{code:`from transformers import AutoConfig, AutoModelForCausalLM

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForCausalLM.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForCausalLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCausalLM.from_config(config)`}}),rE=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),tE=new w({props:{code:`from transformers import AutoConfig, AutoModelForCausalLM

# Download model and configuration from huggingface.co and cache.
model = AutoModelForCausalLM.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForCausalLM.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForCausalLM.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForCausalLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCausalLM.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),aE=new z({}),nE=new E({props:{name:"class transformers.AutoModelForMaskedLM",anchor:"transformers.AutoModelForMaskedLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/modeling_auto.py#L702"}}),lE=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15682/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/albert#transformers.AlbertForMaskedLM">AlbertForMaskedLM</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/bart#transformers.BartForConditionalGeneration">BartForConditionalGeneration</a> (BART model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/bert#transformers.BertForMaskedLM">BertForMaskedLM</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/big_bird#transformers.BigBirdForMaskedLM">BigBirdForMaskedLM</a> (BigBird model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/camembert#transformers.CamembertForMaskedLM">CamembertForMaskedLM</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/convbert#transformers.ConvBertForMaskedLM">ConvBertForMaskedLM</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/deberta#transformers.DebertaForMaskedLM">DebertaForMaskedLM</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/deberta-v2#transformers.DebertaV2ForMaskedLM">DebertaV2ForMaskedLM</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/distilbert#transformers.DistilBertForMaskedLM">DistilBertForMaskedLM</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/electra#transformers.ElectraForMaskedLM">ElectraForMaskedLM</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/fnet#transformers.FNetConfig">FNetConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/fnet#transformers.FNetForMaskedLM">FNetForMaskedLM</a> (FNet model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/flaubert#transformers.FlaubertWithLMHeadModel">FlaubertWithLMHeadModel</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/funnel#transformers.FunnelForMaskedLM">FunnelForMaskedLM</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/ibert#transformers.IBertConfig">IBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/ibert#transformers.IBertForMaskedLM">IBertForMaskedLM</a> (I-BERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/layoutlm#transformers.LayoutLMForMaskedLM">LayoutLMForMaskedLM</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/longformer#transformers.LongformerForMaskedLM">LongformerForMaskedLM</a> (Longformer model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/mbart#transformers.MBartForConditionalGeneration">MBartForConditionalGeneration</a> (mBART model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/mpnet#transformers.MPNetForMaskedLM">MPNetForMaskedLM</a> (MPNet model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/megatron-bert#transformers.MegatronBertConfig">MegatronBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/megatron-bert#transformers.MegatronBertForMaskedLM">MegatronBertForMaskedLM</a> (MegatronBert model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/mobilebert#transformers.MobileBertForMaskedLM">MobileBertForMaskedLM</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/nystromformer#transformers.NystromformerConfig">NystromformerConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/nystromformer#transformers.NystromformerForMaskedLM">NystromformerForMaskedLM</a> (Nystromformer model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/perceiver#transformers.PerceiverConfig">PerceiverConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/perceiver#transformers.PerceiverForMaskedLM">PerceiverForMaskedLM</a> (Perceiver model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/qdqbert#transformers.QDQBertConfig">QDQBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/qdqbert#transformers.QDQBertForMaskedLM">QDQBertForMaskedLM</a> (QDQBert model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/reformer#transformers.ReformerConfig">ReformerConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/reformer#transformers.ReformerForMaskedLM">ReformerForMaskedLM</a> (Reformer model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/rembert#transformers.RemBertForMaskedLM">RemBertForMaskedLM</a> (RemBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/roformer#transformers.RoFormerForMaskedLM">RoFormerForMaskedLM</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/roberta#transformers.RobertaForMaskedLM">RobertaForMaskedLM</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/squeezebert#transformers.SqueezeBertConfig">SqueezeBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/squeezebert#transformers.SqueezeBertForMaskedLM">SqueezeBertForMaskedLM</a> (SqueezeBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/tapas#transformers.TapasForMaskedLM">TapasForMaskedLM</a> (TAPAS model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <code>Wav2Vec2ForMaskedLM</code>(Wav2Vec2 model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/xlm#transformers.XLMWithLMHeadModel">XLMWithLMHeadModel</a> (XLM model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/xlm-roberta#transformers.XLMRobertaForMaskedLM">XLMRobertaForMaskedLM</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLConfig">XLMRobertaXLConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForMaskedLM">XLMRobertaXLForMaskedLM</a> (XLM-RoBERTa-XL model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/yoso#transformers.YosoConfig">YosoConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/yoso#transformers.YosoForMaskedLM">YosoForMaskedLM</a> (YOSO model)</li>
</ul>`,name:"config"}]}}),iE=new w({props:{code:`from transformers import AutoConfig, AutoModelForMaskedLM

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForMaskedLM.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForMaskedLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMaskedLM.from_config(config)`}}),dE=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),cE=new w({props:{code:`from transformers import AutoConfig, AutoModelForMaskedLM

# Download model and configuration from huggingface.co and cache.
model = AutoModelForMaskedLM.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForMaskedLM.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForMaskedLM.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForMaskedLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMaskedLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMaskedLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMaskedLM.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),fE=new z({}),mE=new E({props:{name:"class transformers.AutoModelForSeq2SeqLM",anchor:"transformers.AutoModelForSeq2SeqLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/modeling_auto.py#L709"}}),hE=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15682/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/bart#transformers.BartForConditionalGeneration">BartForConditionalGeneration</a> (BART model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusConfig">BigBirdPegasusConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusForConditionalGeneration">BigBirdPegasusForConditionalGeneration</a> (BigBirdPegasus model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/blenderbot#transformers.BlenderbotConfig">BlenderbotConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/blenderbot#transformers.BlenderbotForConditionalGeneration">BlenderbotForConditionalGeneration</a> (Blenderbot model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/blenderbot-small#transformers.BlenderbotSmallConfig">BlenderbotSmallConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/blenderbot-small#transformers.BlenderbotSmallForConditionalGeneration">BlenderbotSmallForConditionalGeneration</a> (BlenderbotSmall model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/encoder-decoder#transformers.EncoderDecoderConfig">EncoderDecoderConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/encoder-decoder#transformers.EncoderDecoderModel">EncoderDecoderModel</a> (Encoder decoder model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/fsmt#transformers.FSMTConfig">FSMTConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/fsmt#transformers.FSMTForConditionalGeneration">FSMTForConditionalGeneration</a> (FairSeq Machine-Translation model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/led#transformers.LEDConfig">LEDConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/led#transformers.LEDForConditionalGeneration">LEDForConditionalGeneration</a> (LED model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/m2m_100#transformers.M2M100Config">M2M100Config</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/m2m_100#transformers.M2M100ForConditionalGeneration">M2M100ForConditionalGeneration</a> (M2M100 model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/mbart#transformers.MBartForConditionalGeneration">MBartForConditionalGeneration</a> (mBART model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/mt5#transformers.MT5Config">MT5Config</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/mt5#transformers.MT5ForConditionalGeneration">MT5ForConditionalGeneration</a> (mT5 model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/marian#transformers.MarianConfig">MarianConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/marian#transformers.MarianMTModel">MarianMTModel</a> (Marian model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/plbart#transformers.PLBartConfig">PLBartConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/plbart#transformers.PLBartForConditionalGeneration">PLBartForConditionalGeneration</a> (PLBart model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/pegasus#transformers.PegasusConfig">PegasusConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/pegasus#transformers.PegasusForConditionalGeneration">PegasusForConditionalGeneration</a> (Pegasus model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/prophetnet#transformers.ProphetNetConfig">ProphetNetConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/prophetnet#transformers.ProphetNetForConditionalGeneration">ProphetNetForConditionalGeneration</a> (ProphetNet model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/t5#transformers.T5Config">T5Config</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/t5#transformers.T5ForConditionalGeneration">T5ForConditionalGeneration</a> (T5 model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetConfig">XLMProphetNetConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetForConditionalGeneration">XLMProphetNetForConditionalGeneration</a> (XLMProphetNet model)</li>
</ul>`,name:"config"}]}}),pE=new w({props:{code:`from transformers import AutoConfig, AutoModelForSeq2SeqLM

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("t5-base")
model = AutoModelForSeq2SeqLM.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForSeq2SeqLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;t5-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSeq2SeqLM.from_config(config)`}}),_E=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),uE=new w({props:{code:`from transformers import AutoConfig, AutoModelForSeq2SeqLM

# Download model and configuration from huggingface.co and cache.
model = AutoModelForSeq2SeqLM.from_pretrained("t5-base")

# Update configuration during loading
model = AutoModelForSeq2SeqLM.from_pretrained("t5-base", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/t5_tf_model_config.json")
model = AutoModelForSeq2SeqLM.from_pretrained(
    "./tf_model/t5_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForSeq2SeqLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSeq2SeqLM.from_pretrained(<span class="hljs-string">&quot;t5-base&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSeq2SeqLM.from_pretrained(<span class="hljs-string">&quot;t5-base&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/t5_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSeq2SeqLM.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/t5_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),bE=new z({}),vE=new E({props:{name:"class transformers.AutoModelForSequenceClassification",anchor:"transformers.AutoModelForSequenceClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/modeling_auto.py#L718"}}),FE=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15682/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/albert#transformers.AlbertForSequenceClassification">AlbertForSequenceClassification</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/bart#transformers.BartForSequenceClassification">BartForSequenceClassification</a> (BART model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/bert#transformers.BertForSequenceClassification">BertForSequenceClassification</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/big_bird#transformers.BigBirdForSequenceClassification">BigBirdForSequenceClassification</a> (BigBird model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusConfig">BigBirdPegasusConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusForSequenceClassification">BigBirdPegasusForSequenceClassification</a> (BigBirdPegasus model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/ctrl#transformers.CTRLConfig">CTRLConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/ctrl#transformers.CTRLForSequenceClassification">CTRLForSequenceClassification</a> (CTRL model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/camembert#transformers.CamembertForSequenceClassification">CamembertForSequenceClassification</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/canine#transformers.CanineConfig">CanineConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/canine#transformers.CanineForSequenceClassification">CanineForSequenceClassification</a> (Canine model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/convbert#transformers.ConvBertForSequenceClassification">ConvBertForSequenceClassification</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/deberta#transformers.DebertaForSequenceClassification">DebertaForSequenceClassification</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/deberta-v2#transformers.DebertaV2ForSequenceClassification">DebertaV2ForSequenceClassification</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/distilbert#transformers.DistilBertForSequenceClassification">DistilBertForSequenceClassification</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/electra#transformers.ElectraForSequenceClassification">ElectraForSequenceClassification</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/fnet#transformers.FNetConfig">FNetConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/fnet#transformers.FNetForSequenceClassification">FNetForSequenceClassification</a> (FNet model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/flaubert#transformers.FlaubertForSequenceClassification">FlaubertForSequenceClassification</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/funnel#transformers.FunnelForSequenceClassification">FunnelForSequenceClassification</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/gpt2#transformers.GPT2ForSequenceClassification">GPT2ForSequenceClassification</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/gptj#transformers.GPTJConfig">GPTJConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/gptj#transformers.GPTJForSequenceClassification">GPTJForSequenceClassification</a> (GPT-J model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/gpt_neo#transformers.GPTNeoConfig">GPTNeoConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/gpt_neo#transformers.GPTNeoForSequenceClassification">GPTNeoForSequenceClassification</a> (GPT Neo model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/ibert#transformers.IBertConfig">IBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/ibert#transformers.IBertForSequenceClassification">IBertForSequenceClassification</a> (I-BERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/led#transformers.LEDConfig">LEDConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/led#transformers.LEDForSequenceClassification">LEDForSequenceClassification</a> (LED model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/layoutlm#transformers.LayoutLMForSequenceClassification">LayoutLMForSequenceClassification</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/layoutlmv2#transformers.LayoutLMv2Config">LayoutLMv2Config</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/layoutlmv2#transformers.LayoutLMv2ForSequenceClassification">LayoutLMv2ForSequenceClassification</a> (LayoutLMv2 model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/longformer#transformers.LongformerForSequenceClassification">LongformerForSequenceClassification</a> (Longformer model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/mbart#transformers.MBartForSequenceClassification">MBartForSequenceClassification</a> (mBART model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/mpnet#transformers.MPNetForSequenceClassification">MPNetForSequenceClassification</a> (MPNet model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/megatron-bert#transformers.MegatronBertConfig">MegatronBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/megatron-bert#transformers.MegatronBertForSequenceClassification">MegatronBertForSequenceClassification</a> (MegatronBert model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/mobilebert#transformers.MobileBertForSequenceClassification">MobileBertForSequenceClassification</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/nystromformer#transformers.NystromformerConfig">NystromformerConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/nystromformer#transformers.NystromformerForSequenceClassification">NystromformerForSequenceClassification</a> (Nystromformer model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig">OpenAIGPTConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/openai-gpt#transformers.OpenAIGPTForSequenceClassification">OpenAIGPTForSequenceClassification</a> (OpenAI GPT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/plbart#transformers.PLBartConfig">PLBartConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/plbart#transformers.PLBartForSequenceClassification">PLBartForSequenceClassification</a> (PLBart model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/perceiver#transformers.PerceiverConfig">PerceiverConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/perceiver#transformers.PerceiverForSequenceClassification">PerceiverForSequenceClassification</a> (Perceiver model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/qdqbert#transformers.QDQBertConfig">QDQBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/qdqbert#transformers.QDQBertForSequenceClassification">QDQBertForSequenceClassification</a> (QDQBert model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/reformer#transformers.ReformerConfig">ReformerConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/reformer#transformers.ReformerForSequenceClassification">ReformerForSequenceClassification</a> (Reformer model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/rembert#transformers.RemBertForSequenceClassification">RemBertForSequenceClassification</a> (RemBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/roformer#transformers.RoFormerForSequenceClassification">RoFormerForSequenceClassification</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/roberta#transformers.RobertaForSequenceClassification">RobertaForSequenceClassification</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/squeezebert#transformers.SqueezeBertConfig">SqueezeBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/squeezebert#transformers.SqueezeBertForSequenceClassification">SqueezeBertForSequenceClassification</a> (SqueezeBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/tapas#transformers.TapasForSequenceClassification">TapasForSequenceClassification</a> (TAPAS model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/transfo-xl#transformers.TransfoXLConfig">TransfoXLConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/transfo-xl#transformers.TransfoXLForSequenceClassification">TransfoXLForSequenceClassification</a> (Transformer-XL model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/xlm#transformers.XLMForSequenceClassification">XLMForSequenceClassification</a> (XLM model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/xlm-roberta#transformers.XLMRobertaForSequenceClassification">XLMRobertaForSequenceClassification</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLConfig">XLMRobertaXLConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForSequenceClassification">XLMRobertaXLForSequenceClassification</a> (XLM-RoBERTa-XL model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/xlnet#transformers.XLNetForSequenceClassification">XLNetForSequenceClassification</a> (XLNet model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/yoso#transformers.YosoConfig">YosoConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/yoso#transformers.YosoForSequenceClassification">YosoForSequenceClassification</a> (YOSO model)</li>
</ul>`,name:"config"}]}}),CE=new w({props:{code:`from transformers import AutoConfig, AutoModelForSequenceClassification

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForSequenceClassification.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSequenceClassification.from_config(config)`}}),ME=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),EE=new w({props:{code:`from transformers import AutoConfig, AutoModelForSequenceClassification

# Download model and configuration from huggingface.co and cache.
model = AutoModelForSequenceClassification.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForSequenceClassification.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForSequenceClassification.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSequenceClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),yE=new z({}),wE=new E({props:{name:"class transformers.AutoModelForMultipleChoice",anchor:"transformers.AutoModelForMultipleChoice",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/modeling_auto.py#L752"}}),LE=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15682/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/albert#transformers.AlbertForMultipleChoice">AlbertForMultipleChoice</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/bert#transformers.BertForMultipleChoice">BertForMultipleChoice</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/big_bird#transformers.BigBirdForMultipleChoice">BigBirdForMultipleChoice</a> (BigBird model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/camembert#transformers.CamembertForMultipleChoice">CamembertForMultipleChoice</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/canine#transformers.CanineConfig">CanineConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/canine#transformers.CanineForMultipleChoice">CanineForMultipleChoice</a> (Canine model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/convbert#transformers.ConvBertForMultipleChoice">ConvBertForMultipleChoice</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/distilbert#transformers.DistilBertForMultipleChoice">DistilBertForMultipleChoice</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/electra#transformers.ElectraForMultipleChoice">ElectraForMultipleChoice</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/fnet#transformers.FNetConfig">FNetConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/fnet#transformers.FNetForMultipleChoice">FNetForMultipleChoice</a> (FNet model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/flaubert#transformers.FlaubertForMultipleChoice">FlaubertForMultipleChoice</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/funnel#transformers.FunnelForMultipleChoice">FunnelForMultipleChoice</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/ibert#transformers.IBertConfig">IBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/ibert#transformers.IBertForMultipleChoice">IBertForMultipleChoice</a> (I-BERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/longformer#transformers.LongformerForMultipleChoice">LongformerForMultipleChoice</a> (Longformer model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/mpnet#transformers.MPNetForMultipleChoice">MPNetForMultipleChoice</a> (MPNet model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/megatron-bert#transformers.MegatronBertConfig">MegatronBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/megatron-bert#transformers.MegatronBertForMultipleChoice">MegatronBertForMultipleChoice</a> (MegatronBert model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/mobilebert#transformers.MobileBertForMultipleChoice">MobileBertForMultipleChoice</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/nystromformer#transformers.NystromformerConfig">NystromformerConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/nystromformer#transformers.NystromformerForMultipleChoice">NystromformerForMultipleChoice</a> (Nystromformer model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/qdqbert#transformers.QDQBertConfig">QDQBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/qdqbert#transformers.QDQBertForMultipleChoice">QDQBertForMultipleChoice</a> (QDQBert model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/rembert#transformers.RemBertForMultipleChoice">RemBertForMultipleChoice</a> (RemBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/roformer#transformers.RoFormerForMultipleChoice">RoFormerForMultipleChoice</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/roberta#transformers.RobertaForMultipleChoice">RobertaForMultipleChoice</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/squeezebert#transformers.SqueezeBertConfig">SqueezeBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/squeezebert#transformers.SqueezeBertForMultipleChoice">SqueezeBertForMultipleChoice</a> (SqueezeBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/xlm#transformers.XLMForMultipleChoice">XLMForMultipleChoice</a> (XLM model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/xlm-roberta#transformers.XLMRobertaForMultipleChoice">XLMRobertaForMultipleChoice</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLConfig">XLMRobertaXLConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForMultipleChoice">XLMRobertaXLForMultipleChoice</a> (XLM-RoBERTa-XL model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/xlnet#transformers.XLNetForMultipleChoice">XLNetForMultipleChoice</a> (XLNet model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/yoso#transformers.YosoConfig">YosoConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/yoso#transformers.YosoForMultipleChoice">YosoForMultipleChoice</a> (YOSO model)</li>
</ul>`,name:"config"}]}}),BE=new w({props:{code:`from transformers import AutoConfig, AutoModelForMultipleChoice

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForMultipleChoice.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForMultipleChoice

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMultipleChoice.from_config(config)`}}),kE=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),xE=new w({props:{code:`from transformers import AutoConfig, AutoModelForMultipleChoice

# Download model and configuration from huggingface.co and cache.
model = AutoModelForMultipleChoice.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForMultipleChoice.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForMultipleChoice.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForMultipleChoice

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMultipleChoice.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMultipleChoice.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMultipleChoice.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),RE=new z({}),SE=new E({props:{name:"class transformers.AutoModelForNextSentencePrediction",anchor:"transformers.AutoModelForNextSentencePrediction",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/modeling_auto.py#L759"}}),$E=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15682/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/bert#transformers.BertForNextSentencePrediction">BertForNextSentencePrediction</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/fnet#transformers.FNetConfig">FNetConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/fnet#transformers.FNetForNextSentencePrediction">FNetForNextSentencePrediction</a> (FNet model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/megatron-bert#transformers.MegatronBertConfig">MegatronBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/megatron-bert#transformers.MegatronBertForNextSentencePrediction">MegatronBertForNextSentencePrediction</a> (MegatronBert model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/mobilebert#transformers.MobileBertForNextSentencePrediction">MobileBertForNextSentencePrediction</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/qdqbert#transformers.QDQBertConfig">QDQBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/qdqbert#transformers.QDQBertForNextSentencePrediction">QDQBertForNextSentencePrediction</a> (QDQBert model)</li>
</ul>`,name:"config"}]}}),IE=new w({props:{code:`from transformers import AutoConfig, AutoModelForNextSentencePrediction

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForNextSentencePrediction.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForNextSentencePrediction

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForNextSentencePrediction.from_config(config)`}}),jE=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),NE=new w({props:{code:`from transformers import AutoConfig, AutoModelForNextSentencePrediction

# Download model and configuration from huggingface.co and cache.
model = AutoModelForNextSentencePrediction.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForNextSentencePrediction.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForNextSentencePrediction.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForNextSentencePrediction

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForNextSentencePrediction.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForNextSentencePrediction.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForNextSentencePrediction.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),DE=new z({}),qE=new E({props:{name:"class transformers.AutoModelForTokenClassification",anchor:"transformers.AutoModelForTokenClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/modeling_auto.py#L745"}}),OE=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15682/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/albert#transformers.AlbertForTokenClassification">AlbertForTokenClassification</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/bert#transformers.BertForTokenClassification">BertForTokenClassification</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/big_bird#transformers.BigBirdForTokenClassification">BigBirdForTokenClassification</a> (BigBird model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/camembert#transformers.CamembertForTokenClassification">CamembertForTokenClassification</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/canine#transformers.CanineConfig">CanineConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/canine#transformers.CanineForTokenClassification">CanineForTokenClassification</a> (Canine model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/convbert#transformers.ConvBertForTokenClassification">ConvBertForTokenClassification</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/deberta#transformers.DebertaForTokenClassification">DebertaForTokenClassification</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/deberta-v2#transformers.DebertaV2ForTokenClassification">DebertaV2ForTokenClassification</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/distilbert#transformers.DistilBertForTokenClassification">DistilBertForTokenClassification</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/electra#transformers.ElectraForTokenClassification">ElectraForTokenClassification</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/fnet#transformers.FNetConfig">FNetConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/fnet#transformers.FNetForTokenClassification">FNetForTokenClassification</a> (FNet model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/flaubert#transformers.FlaubertForTokenClassification">FlaubertForTokenClassification</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/funnel#transformers.FunnelForTokenClassification">FunnelForTokenClassification</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/gpt2#transformers.GPT2ForTokenClassification">GPT2ForTokenClassification</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/ibert#transformers.IBertConfig">IBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/ibert#transformers.IBertForTokenClassification">IBertForTokenClassification</a> (I-BERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/layoutlm#transformers.LayoutLMForTokenClassification">LayoutLMForTokenClassification</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/layoutlmv2#transformers.LayoutLMv2Config">LayoutLMv2Config</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/layoutlmv2#transformers.LayoutLMv2ForTokenClassification">LayoutLMv2ForTokenClassification</a> (LayoutLMv2 model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/longformer#transformers.LongformerForTokenClassification">LongformerForTokenClassification</a> (Longformer model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/mpnet#transformers.MPNetForTokenClassification">MPNetForTokenClassification</a> (MPNet model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/megatron-bert#transformers.MegatronBertConfig">MegatronBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/megatron-bert#transformers.MegatronBertForTokenClassification">MegatronBertForTokenClassification</a> (MegatronBert model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/mobilebert#transformers.MobileBertForTokenClassification">MobileBertForTokenClassification</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/nystromformer#transformers.NystromformerConfig">NystromformerConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/nystromformer#transformers.NystromformerForTokenClassification">NystromformerForTokenClassification</a> (Nystromformer model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/qdqbert#transformers.QDQBertConfig">QDQBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/qdqbert#transformers.QDQBertForTokenClassification">QDQBertForTokenClassification</a> (QDQBert model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/rembert#transformers.RemBertForTokenClassification">RemBertForTokenClassification</a> (RemBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/roformer#transformers.RoFormerForTokenClassification">RoFormerForTokenClassification</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/roberta#transformers.RobertaForTokenClassification">RobertaForTokenClassification</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/squeezebert#transformers.SqueezeBertConfig">SqueezeBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/squeezebert#transformers.SqueezeBertForTokenClassification">SqueezeBertForTokenClassification</a> (SqueezeBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/xlm#transformers.XLMForTokenClassification">XLMForTokenClassification</a> (XLM model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/xlm-roberta#transformers.XLMRobertaForTokenClassification">XLMRobertaForTokenClassification</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLConfig">XLMRobertaXLConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForTokenClassification">XLMRobertaXLForTokenClassification</a> (XLM-RoBERTa-XL model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/xlnet#transformers.XLNetForTokenClassification">XLNetForTokenClassification</a> (XLNet model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/yoso#transformers.YosoConfig">YosoConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/yoso#transformers.YosoForTokenClassification">YosoForTokenClassification</a> (YOSO model)</li>
</ul>`,name:"config"}]}}),XE=new w({props:{code:`from transformers import AutoConfig, AutoModelForTokenClassification

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForTokenClassification.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForTokenClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForTokenClassification.from_config(config)`}}),zE=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),VE=new w({props:{code:`from transformers import AutoConfig, AutoModelForTokenClassification

# Download model and configuration from huggingface.co and cache.
model = AutoModelForTokenClassification.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForTokenClassification.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForTokenClassification.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForTokenClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForTokenClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForTokenClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForTokenClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),WE=new z({}),QE=new E({props:{name:"class transformers.AutoModelForQuestionAnswering",anchor:"transformers.AutoModelForQuestionAnswering",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/modeling_auto.py#L727"}}),UE=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15682/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/albert#transformers.AlbertForQuestionAnswering">AlbertForQuestionAnswering</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/bart#transformers.BartForQuestionAnswering">BartForQuestionAnswering</a> (BART model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/bert#transformers.BertForQuestionAnswering">BertForQuestionAnswering</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/big_bird#transformers.BigBirdForQuestionAnswering">BigBirdForQuestionAnswering</a> (BigBird model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusConfig">BigBirdPegasusConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusForQuestionAnswering">BigBirdPegasusForQuestionAnswering</a> (BigBirdPegasus model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/camembert#transformers.CamembertForQuestionAnswering">CamembertForQuestionAnswering</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/canine#transformers.CanineConfig">CanineConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/canine#transformers.CanineForQuestionAnswering">CanineForQuestionAnswering</a> (Canine model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/convbert#transformers.ConvBertForQuestionAnswering">ConvBertForQuestionAnswering</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/deberta#transformers.DebertaForQuestionAnswering">DebertaForQuestionAnswering</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/deberta-v2#transformers.DebertaV2ForQuestionAnswering">DebertaV2ForQuestionAnswering</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/distilbert#transformers.DistilBertForQuestionAnswering">DistilBertForQuestionAnswering</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/electra#transformers.ElectraForQuestionAnswering">ElectraForQuestionAnswering</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/fnet#transformers.FNetConfig">FNetConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/fnet#transformers.FNetForQuestionAnswering">FNetForQuestionAnswering</a> (FNet model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/flaubert#transformers.FlaubertForQuestionAnsweringSimple">FlaubertForQuestionAnsweringSimple</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/funnel#transformers.FunnelForQuestionAnswering">FunnelForQuestionAnswering</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/gptj#transformers.GPTJConfig">GPTJConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/gptj#transformers.GPTJForQuestionAnswering">GPTJForQuestionAnswering</a> (GPT-J model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/ibert#transformers.IBertConfig">IBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/ibert#transformers.IBertForQuestionAnswering">IBertForQuestionAnswering</a> (I-BERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/led#transformers.LEDConfig">LEDConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/led#transformers.LEDForQuestionAnswering">LEDForQuestionAnswering</a> (LED model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/layoutlmv2#transformers.LayoutLMv2Config">LayoutLMv2Config</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/layoutlmv2#transformers.LayoutLMv2ForQuestionAnswering">LayoutLMv2ForQuestionAnswering</a> (LayoutLMv2 model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/longformer#transformers.LongformerForQuestionAnswering">LongformerForQuestionAnswering</a> (Longformer model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/lxmert#transformers.LxmertConfig">LxmertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/lxmert#transformers.LxmertForQuestionAnswering">LxmertForQuestionAnswering</a> (LXMERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/mbart#transformers.MBartForQuestionAnswering">MBartForQuestionAnswering</a> (mBART model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/mpnet#transformers.MPNetForQuestionAnswering">MPNetForQuestionAnswering</a> (MPNet model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/megatron-bert#transformers.MegatronBertConfig">MegatronBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/megatron-bert#transformers.MegatronBertForQuestionAnswering">MegatronBertForQuestionAnswering</a> (MegatronBert model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/mobilebert#transformers.MobileBertForQuestionAnswering">MobileBertForQuestionAnswering</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/nystromformer#transformers.NystromformerConfig">NystromformerConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/nystromformer#transformers.NystromformerForQuestionAnswering">NystromformerForQuestionAnswering</a> (Nystromformer model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/qdqbert#transformers.QDQBertConfig">QDQBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/qdqbert#transformers.QDQBertForQuestionAnswering">QDQBertForQuestionAnswering</a> (QDQBert model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/reformer#transformers.ReformerConfig">ReformerConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/reformer#transformers.ReformerForQuestionAnswering">ReformerForQuestionAnswering</a> (Reformer model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/rembert#transformers.RemBertForQuestionAnswering">RemBertForQuestionAnswering</a> (RemBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/roformer#transformers.RoFormerForQuestionAnswering">RoFormerForQuestionAnswering</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/roberta#transformers.RobertaForQuestionAnswering">RobertaForQuestionAnswering</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/splinter#transformers.SplinterConfig">SplinterConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/splinter#transformers.SplinterForQuestionAnswering">SplinterForQuestionAnswering</a> (Splinter model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/squeezebert#transformers.SqueezeBertConfig">SqueezeBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/squeezebert#transformers.SqueezeBertForQuestionAnswering">SqueezeBertForQuestionAnswering</a> (SqueezeBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/xlm#transformers.XLMForQuestionAnsweringSimple">XLMForQuestionAnsweringSimple</a> (XLM model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/xlm-roberta#transformers.XLMRobertaForQuestionAnswering">XLMRobertaForQuestionAnswering</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLConfig">XLMRobertaXLConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForQuestionAnswering">XLMRobertaXLForQuestionAnswering</a> (XLM-RoBERTa-XL model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/xlnet#transformers.XLNetForQuestionAnsweringSimple">XLNetForQuestionAnsweringSimple</a> (XLNet model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/yoso#transformers.YosoConfig">YosoConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/yoso#transformers.YosoForQuestionAnswering">YosoForQuestionAnswering</a> (YOSO model)</li>
</ul>`,name:"config"}]}}),JE=new w({props:{code:`from transformers import AutoConfig, AutoModelForQuestionAnswering

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForQuestionAnswering.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForQuestionAnswering.from_config(config)`}}),YE=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),KE=new w({props:{code:`from transformers import AutoConfig, AutoModelForQuestionAnswering

# Download model and configuration from huggingface.co and cache.
model = AutoModelForQuestionAnswering.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForQuestionAnswering.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForQuestionAnswering.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForQuestionAnswering.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),ZE=new z({}),e3=new E({props:{name:"class transformers.AutoModelForTableQuestionAnswering",anchor:"transformers.AutoModelForTableQuestionAnswering",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/modeling_auto.py#L734"}}),r3=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15682/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/tapas#transformers.TapasForQuestionAnswering">TapasForQuestionAnswering</a> (TAPAS model)</li>
</ul>`,name:"config"}]}}),t3=new w({props:{code:`from transformers import AutoConfig, AutoModelForTableQuestionAnswering

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("google/tapas-base-finetuned-wtq")
model = AutoModelForTableQuestionAnswering.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForTableQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;google/tapas-base-finetuned-wtq&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForTableQuestionAnswering.from_config(config)`}}),a3=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),n3=new w({props:{code:`from transformers import AutoConfig, AutoModelForTableQuestionAnswering

# Download model and configuration from huggingface.co and cache.
model = AutoModelForTableQuestionAnswering.from_pretrained("google/tapas-base-finetuned-wtq")

# Update configuration during loading
model = AutoModelForTableQuestionAnswering.from_pretrained("google/tapas-base-finetuned-wtq", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/tapas_tf_model_config.json")
model = AutoModelForTableQuestionAnswering.from_pretrained(
    "./tf_model/tapas_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForTableQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForTableQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;google/tapas-base-finetuned-wtq&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForTableQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;google/tapas-base-finetuned-wtq&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/tapas_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForTableQuestionAnswering.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/tapas_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),s3=new z({}),l3=new E({props:{name:"class transformers.AutoModelForImageClassification",anchor:"transformers.AutoModelForImageClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/modeling_auto.py#L768"}}),d3=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15682/en/model_doc/beit#transformers.BeitConfig">BeitConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/beit#transformers.BeitForImageClassification">BeitForImageClassification</a> (BEiT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/convnext#transformers.ConvNextConfig">ConvNextConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/convnext#transformers.ConvNextForImageClassification">ConvNextForImageClassification</a> (ConvNext model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/deit#transformers.DeiTConfig">DeiTConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/deit#transformers.DeiTForImageClassification">DeiTForImageClassification</a> or <a href="/docs/transformers/pr_15682/en/model_doc/deit#transformers.DeiTForImageClassificationWithTeacher">DeiTForImageClassificationWithTeacher</a> (DeiT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/imagegpt#transformers.ImageGPTConfig">ImageGPTConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/imagegpt#transformers.ImageGPTForImageClassification">ImageGPTForImageClassification</a> (ImageGPT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/perceiver#transformers.PerceiverConfig">PerceiverConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/perceiver#transformers.PerceiverForImageClassificationLearned">PerceiverForImageClassificationLearned</a> or <a href="/docs/transformers/pr_15682/en/model_doc/perceiver#transformers.PerceiverForImageClassificationFourier">PerceiverForImageClassificationFourier</a> or <a href="/docs/transformers/pr_15682/en/model_doc/perceiver#transformers.PerceiverForImageClassificationConvProcessing">PerceiverForImageClassificationConvProcessing</a> (Perceiver model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/poolformer#transformers.PoolFormerConfig">PoolFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/poolformer#transformers.PoolFormerForImageClassification">PoolFormerForImageClassification</a> (PoolFormer model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/segformer#transformers.SegformerConfig">SegformerConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/segformer#transformers.SegformerForImageClassification">SegformerForImageClassification</a> (SegFormer model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/swin#transformers.SwinConfig">SwinConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/swin#transformers.SwinForImageClassification">SwinForImageClassification</a> (Swin model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/vit#transformers.ViTConfig">ViTConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/vit#transformers.ViTForImageClassification">ViTForImageClassification</a> (ViT model)</li>
</ul>`,name:"config"}]}}),c3=new w({props:{code:`from transformers import AutoConfig, AutoModelForImageClassification

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForImageClassification.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForImageClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForImageClassification.from_config(config)`}}),f3=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),m3=new w({props:{code:`from transformers import AutoConfig, AutoModelForImageClassification

# Download model and configuration from huggingface.co and cache.
model = AutoModelForImageClassification.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForImageClassification.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForImageClassification.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForImageClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForImageClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForImageClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForImageClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),g3=new z({}),h3=new E({props:{name:"class transformers.AutoModelForVision2Seq",anchor:"transformers.AutoModelForVision2Seq",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/modeling_auto.py#L798"}}),_3=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15682/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderConfig">VisionEncoderDecoderConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderModel">VisionEncoderDecoderModel</a> (Vision Encoder decoder model)</li>
</ul>`,name:"config"}]}}),u3=new w({props:{code:`from transformers import AutoConfig, AutoModelForVision2Seq

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForVision2Seq.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForVision2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForVision2Seq.from_config(config)`}}),b3=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),v3=new w({props:{code:`from transformers import AutoConfig, AutoModelForVision2Seq

# Download model and configuration from huggingface.co and cache.
model = AutoModelForVision2Seq.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForVision2Seq.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForVision2Seq.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForVision2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForVision2Seq.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForVision2Seq.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForVision2Seq.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),T3=new z({}),F3=new E({props:{name:"class transformers.AutoModelForAudioClassification",anchor:"transformers.AutoModelForAudioClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/modeling_auto.py#L805"}}),M3=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15682/en/model_doc/hubert#transformers.HubertConfig">HubertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/hubert#transformers.HubertForSequenceClassification">HubertForSequenceClassification</a> (Hubert model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/sew#transformers.SEWConfig">SEWConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/sew#transformers.SEWForSequenceClassification">SEWForSequenceClassification</a> (SEW model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/sew-d#transformers.SEWDConfig">SEWDConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/sew-d#transformers.SEWDForSequenceClassification">SEWDForSequenceClassification</a> (SEW-D model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/unispeech#transformers.UniSpeechConfig">UniSpeechConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/unispeech#transformers.UniSpeechForSequenceClassification">UniSpeechForSequenceClassification</a> (UniSpeech model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/unispeech-sat#transformers.UniSpeechSatConfig">UniSpeechSatConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/unispeech-sat#transformers.UniSpeechSatForSequenceClassification">UniSpeechSatForSequenceClassification</a> (UniSpeechSat model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/wav2vec2#transformers.Wav2Vec2ForSequenceClassification">Wav2Vec2ForSequenceClassification</a> (Wav2Vec2 model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/wavlm#transformers.WavLMConfig">WavLMConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/wavlm#transformers.WavLMForSequenceClassification">WavLMForSequenceClassification</a> (WavLM model)</li>
</ul>`,name:"config"}]}}),E3=new w({props:{code:`from transformers import AutoConfig, AutoModelForAudioClassification

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForAudioClassification.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForAudioClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioClassification.from_config(config)`}}),y3=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),w3=new w({props:{code:`from transformers import AutoConfig, AutoModelForAudioClassification

# Download model and configuration from huggingface.co and cache.
model = AutoModelForAudioClassification.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForAudioClassification.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForAudioClassification.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForAudioClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),A3=new z({}),L3=new E({props:{name:"class transformers.AutoModelForAudioFrameClassification",anchor:"transformers.AutoModelForAudioFrameClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/modeling_auto.py#L828"}}),k3=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15682/en/model_doc/unispeech-sat#transformers.UniSpeechSatConfig">UniSpeechSatConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/unispeech-sat#transformers.UniSpeechSatForAudioFrameClassification">UniSpeechSatForAudioFrameClassification</a> (UniSpeechSat model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/wav2vec2#transformers.Wav2Vec2ForAudioFrameClassification">Wav2Vec2ForAudioFrameClassification</a> (Wav2Vec2 model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/wavlm#transformers.WavLMConfig">WavLMConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/wavlm#transformers.WavLMForAudioFrameClassification">WavLMForAudioFrameClassification</a> (WavLM model)</li>
</ul>`,name:"config"}]}}),x3=new w({props:{code:`from transformers import AutoConfig, AutoModelForAudioFrameClassification

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForAudioFrameClassification.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForAudioFrameClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioFrameClassification.from_config(config)`}}),R3=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),S3=new w({props:{code:`from transformers import AutoConfig, AutoModelForAudioFrameClassification

# Download model and configuration from huggingface.co and cache.
model = AutoModelForAudioFrameClassification.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForAudioFrameClassification.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForAudioFrameClassification.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForAudioFrameClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioFrameClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioFrameClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioFrameClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),P3=new z({}),$3=new E({props:{name:"class transformers.AutoModelForCTC",anchor:"transformers.AutoModelForCTC",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/modeling_auto.py#L812"}}),j3=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15682/en/model_doc/hubert#transformers.HubertConfig">HubertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/hubert#transformers.HubertForCTC">HubertForCTC</a> (Hubert model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/sew#transformers.SEWConfig">SEWConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/sew#transformers.SEWForCTC">SEWForCTC</a> (SEW model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/sew-d#transformers.SEWDConfig">SEWDConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/sew-d#transformers.SEWDForCTC">SEWDForCTC</a> (SEW-D model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/unispeech#transformers.UniSpeechConfig">UniSpeechConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/unispeech#transformers.UniSpeechForCTC">UniSpeechForCTC</a> (UniSpeech model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/unispeech-sat#transformers.UniSpeechSatConfig">UniSpeechSatConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/unispeech-sat#transformers.UniSpeechSatForCTC">UniSpeechSatForCTC</a> (UniSpeechSat model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/wav2vec2#transformers.Wav2Vec2ForCTC">Wav2Vec2ForCTC</a> (Wav2Vec2 model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/wavlm#transformers.WavLMConfig">WavLMConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/wavlm#transformers.WavLMForCTC">WavLMForCTC</a> (WavLM model)</li>
</ul>`,name:"config"}]}}),N3=new w({props:{code:`from transformers import AutoConfig, AutoModelForCTC

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForCTC.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForCTC

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCTC.from_config(config)`}}),D3=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),q3=new w({props:{code:`from transformers import AutoConfig, AutoModelForCTC

# Download model and configuration from huggingface.co and cache.
model = AutoModelForCTC.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForCTC.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForCTC.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForCTC

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCTC.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCTC.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCTC.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),G3=new z({}),O3=new E({props:{name:"class transformers.AutoModelForSpeechSeq2Seq",anchor:"transformers.AutoModelForSpeechSeq2Seq",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/modeling_auto.py#L819"}}),z3=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15682/en/model_doc/speech_to_text#transformers.Speech2TextConfig">Speech2TextConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/speech_to_text#transformers.Speech2TextForConditionalGeneration">Speech2TextForConditionalGeneration</a> (Speech2Text model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/speech-encoder-decoder#transformers.SpeechEncoderDecoderConfig">SpeechEncoderDecoderConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/speech-encoder-decoder#transformers.SpeechEncoderDecoderModel">SpeechEncoderDecoderModel</a> (Speech Encoder decoder model)</li>
</ul>`,name:"config"}]}}),V3=new w({props:{code:`from transformers import AutoConfig, AutoModelForSpeechSeq2Seq

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForSpeechSeq2Seq.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForSpeechSeq2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSpeechSeq2Seq.from_config(config)`}}),W3=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),H3=new w({props:{code:`from transformers import AutoConfig, AutoModelForSpeechSeq2Seq

# Download model and configuration from huggingface.co and cache.
model = AutoModelForSpeechSeq2Seq.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForSpeechSeq2Seq.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForSpeechSeq2Seq.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForSpeechSeq2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSpeechSeq2Seq.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSpeechSeq2Seq.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSpeechSeq2Seq.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),U3=new z({}),J3=new E({props:{name:"class transformers.AutoModelForAudioXVector",anchor:"transformers.AutoModelForAudioXVector",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/modeling_auto.py#L837"}}),K3=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15682/en/model_doc/unispeech-sat#transformers.UniSpeechSatConfig">UniSpeechSatConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/unispeech-sat#transformers.UniSpeechSatForXVector">UniSpeechSatForXVector</a> (UniSpeechSat model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/wav2vec2#transformers.Wav2Vec2ForXVector">Wav2Vec2ForXVector</a> (Wav2Vec2 model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/wavlm#transformers.WavLMConfig">WavLMConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/wavlm#transformers.WavLMForXVector">WavLMForXVector</a> (WavLM model)</li>
</ul>`,name:"config"}]}}),Z3=new w({props:{code:`from transformers import AutoConfig, AutoModelForAudioXVector

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForAudioXVector.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForAudioXVector

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioXVector.from_config(config)`}}),ey=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),oy=new w({props:{code:`from transformers import AutoConfig, AutoModelForAudioXVector

# Download model and configuration from huggingface.co and cache.
model = AutoModelForAudioXVector.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForAudioXVector.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForAudioXVector.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForAudioXVector

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioXVector.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioXVector.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioXVector.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),ry=new z({}),ty=new E({props:{name:"class transformers.AutoModelForMaskedImageModeling",anchor:"transformers.AutoModelForMaskedImageModeling",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/modeling_auto.py#L844"}}),ny=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15682/en/model_doc/deit#transformers.DeiTConfig">DeiTConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/deit#transformers.DeiTForMaskedImageModeling">DeiTForMaskedImageModeling</a> (DeiT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/swin#transformers.SwinConfig">SwinConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/swin#transformers.SwinForMaskedImageModeling">SwinForMaskedImageModeling</a> (Swin model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/vit#transformers.ViTConfig">ViTConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/vit#transformers.ViTForMaskedImageModeling">ViTForMaskedImageModeling</a> (ViT model)</li>
</ul>`,name:"config"}]}}),sy=new w({props:{code:`from transformers import AutoConfig, AutoModelForMaskedImageModeling

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForMaskedImageModeling.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForMaskedImageModeling

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMaskedImageModeling.from_config(config)`}}),ly=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),iy=new w({props:{code:`from transformers import AutoConfig, AutoModelForMaskedImageModeling

# Download model and configuration from huggingface.co and cache.
model = AutoModelForMaskedImageModeling.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForMaskedImageModeling.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForMaskedImageModeling.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForMaskedImageModeling

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMaskedImageModeling.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMaskedImageModeling.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMaskedImageModeling.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),dy=new z({}),cy=new E({props:{name:"class transformers.AutoModelForObjectDetection",anchor:"transformers.AutoModelForObjectDetection",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/modeling_auto.py#L791"}}),my=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15682/en/model_doc/detr#transformers.DetrConfig">DetrConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/detr#transformers.DetrForObjectDetection">DetrForObjectDetection</a> (DETR model)</li>
</ul>`,name:"config"}]}}),gy=new w({props:{code:`from transformers import AutoConfig, AutoModelForObjectDetection

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForObjectDetection.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForObjectDetection

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForObjectDetection.from_config(config)`}}),hy=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),py=new w({props:{code:`from transformers import AutoConfig, AutoModelForObjectDetection

# Download model and configuration from huggingface.co and cache.
model = AutoModelForObjectDetection.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForObjectDetection.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForObjectDetection.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForObjectDetection

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForObjectDetection.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForObjectDetection.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForObjectDetection.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),_y=new z({}),uy=new E({props:{name:"class transformers.AutoModelForImageSegmentation",anchor:"transformers.AutoModelForImageSegmentation",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/modeling_auto.py#L775"}}),vy=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15682/en/model_doc/detr#transformers.DetrConfig">DetrConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/detr#transformers.DetrForSegmentation">DetrForSegmentation</a> (DETR model)</li>
</ul>`,name:"config"}]}}),Ty=new w({props:{code:`from transformers import AutoConfig, AutoModelForImageSegmentation

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForImageSegmentation.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForImageSegmentation

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForImageSegmentation.from_config(config)`}}),Fy=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),Cy=new w({props:{code:`from transformers import AutoConfig, AutoModelForImageSegmentation

# Download model and configuration from huggingface.co and cache.
model = AutoModelForImageSegmentation.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForImageSegmentation.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForImageSegmentation.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForImageSegmentation

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForImageSegmentation.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForImageSegmentation.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForImageSegmentation.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),My=new z({}),Ey=new E({props:{name:"class transformers.AutoModelForSemanticSegmentation",anchor:"transformers.AutoModelForSemanticSegmentation",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/modeling_auto.py#L782"}}),wy=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15682/en/model_doc/beit#transformers.BeitConfig">BeitConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/beit#transformers.BeitForSemanticSegmentation">BeitForSemanticSegmentation</a> (BEiT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/segformer#transformers.SegformerConfig">SegformerConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/segformer#transformers.SegformerForSemanticSegmentation">SegformerForSemanticSegmentation</a> (SegFormer model)</li>
</ul>`,name:"config"}]}}),Ay=new w({props:{code:`from transformers import AutoConfig, AutoModelForSemanticSegmentation

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForSemanticSegmentation.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForSemanticSegmentation

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSemanticSegmentation.from_config(config)`}}),Ly=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),ky=new w({props:{code:`from transformers import AutoConfig, AutoModelForSemanticSegmentation

# Download model and configuration from huggingface.co and cache.
model = AutoModelForSemanticSegmentation.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForSemanticSegmentation.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForSemanticSegmentation.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForSemanticSegmentation

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSemanticSegmentation.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSemanticSegmentation.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSemanticSegmentation.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),xy=new z({}),Ry=new E({props:{name:"class transformers.TFAutoModel",anchor:"transformers.TFAutoModel",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/modeling_tf_auto.py#L371"}}),Py=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15682/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/albert#transformers.TFAlbertModel">TFAlbertModel</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/bart#transformers.TFBartModel">TFBartModel</a> (BART model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/bert#transformers.TFBertModel">TFBertModel</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/blenderbot#transformers.BlenderbotConfig">BlenderbotConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/blenderbot#transformers.TFBlenderbotModel">TFBlenderbotModel</a> (Blenderbot model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/blenderbot-small#transformers.BlenderbotSmallConfig">BlenderbotSmallConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/blenderbot-small#transformers.TFBlenderbotSmallModel">TFBlenderbotSmallModel</a> (BlenderbotSmall model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/clip#transformers.CLIPConfig">CLIPConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/clip#transformers.TFCLIPModel">TFCLIPModel</a> (CLIP model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/ctrl#transformers.CTRLConfig">CTRLConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/ctrl#transformers.TFCTRLModel">TFCTRLModel</a> (CTRL model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/camembert#transformers.TFCamembertModel">TFCamembertModel</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/convbert#transformers.TFConvBertModel">TFConvBertModel</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/dpr#transformers.DPRConfig">DPRConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/dpr#transformers.TFDPRQuestionEncoder">TFDPRQuestionEncoder</a> (DPR model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/deberta#transformers.TFDebertaModel">TFDebertaModel</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/deberta-v2#transformers.TFDebertaV2Model">TFDebertaV2Model</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/distilbert#transformers.TFDistilBertModel">TFDistilBertModel</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/electra#transformers.TFElectraModel">TFElectraModel</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/flaubert#transformers.TFFlaubertModel">TFFlaubertModel</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/funnel#transformers.TFFunnelModel">TFFunnelModel</a> or <a href="/docs/transformers/pr_15682/en/model_doc/funnel#transformers.TFFunnelBaseModel">TFFunnelBaseModel</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/gpt2#transformers.TFGPT2Model">TFGPT2Model</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/hubert#transformers.HubertConfig">HubertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/hubert#transformers.TFHubertModel">TFHubertModel</a> (Hubert model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/led#transformers.LEDConfig">LEDConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/led#transformers.TFLEDModel">TFLEDModel</a> (LED model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/layoutlm#transformers.TFLayoutLMModel">TFLayoutLMModel</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/longformer#transformers.TFLongformerModel">TFLongformerModel</a> (Longformer model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/lxmert#transformers.LxmertConfig">LxmertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/lxmert#transformers.TFLxmertModel">TFLxmertModel</a> (LXMERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/mbart#transformers.TFMBartModel">TFMBartModel</a> (mBART model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/mpnet#transformers.TFMPNetModel">TFMPNetModel</a> (MPNet model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/mt5#transformers.MT5Config">MT5Config</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/mt5#transformers.TFMT5Model">TFMT5Model</a> (mT5 model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/marian#transformers.MarianConfig">MarianConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/marian#transformers.TFMarianModel">TFMarianModel</a> (Marian model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/mobilebert#transformers.TFMobileBertModel">TFMobileBertModel</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig">OpenAIGPTConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/openai-gpt#transformers.TFOpenAIGPTModel">TFOpenAIGPTModel</a> (OpenAI GPT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/pegasus#transformers.PegasusConfig">PegasusConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/pegasus#transformers.TFPegasusModel">TFPegasusModel</a> (Pegasus model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/rembert#transformers.TFRemBertModel">TFRemBertModel</a> (RemBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/roformer#transformers.TFRoFormerModel">TFRoFormerModel</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/roberta#transformers.TFRobertaModel">TFRobertaModel</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/speech_to_text#transformers.Speech2TextConfig">Speech2TextConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/speech_to_text#transformers.TFSpeech2TextModel">TFSpeech2TextModel</a> (Speech2Text model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/t5#transformers.T5Config">T5Config</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/t5#transformers.TFT5Model">TFT5Model</a> (T5 model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/tapas#transformers.TFTapasModel">TFTapasModel</a> (TAPAS model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/transfo-xl#transformers.TransfoXLConfig">TransfoXLConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/transfo-xl#transformers.TFTransfoXLModel">TFTransfoXLModel</a> (Transformer-XL model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/vit#transformers.ViTConfig">ViTConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/vit#transformers.TFViTModel">TFViTModel</a> (ViT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/wav2vec2#transformers.TFWav2Vec2Model">TFWav2Vec2Model</a> (Wav2Vec2 model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/xlm#transformers.TFXLMModel">TFXLMModel</a> (XLM model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/xlm-roberta#transformers.TFXLMRobertaModel">TFXLMRobertaModel</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/xlnet#transformers.TFXLNetModel">TFXLNetModel</a> (XLNet model)</li>
</ul>`,name:"config"}]}}),$y=new w({props:{code:`from transformers import AutoConfig, TFAutoModel

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = TFAutoModel.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModel.from_config(config)`}}),Iy=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),jy=new w({props:{code:`from transformers import AutoConfig, TFAutoModel

# Download model and configuration from huggingface.co and cache.
model = TFAutoModel.from_pretrained("bert-base-cased")

# Update configuration during loading
model = TFAutoModel.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = TFAutoModel.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModel.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModel.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModel.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),Ny=new z({}),Dy=new E({props:{name:"class transformers.TFAutoModelForPreTraining",anchor:"transformers.TFAutoModelForPreTraining",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/modeling_tf_auto.py#L378"}}),Gy=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15682/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/albert#transformers.TFAlbertForPreTraining">TFAlbertForPreTraining</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/bart#transformers.TFBartForConditionalGeneration">TFBartForConditionalGeneration</a> (BART model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/bert#transformers.TFBertForPreTraining">TFBertForPreTraining</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/ctrl#transformers.CTRLConfig">CTRLConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/ctrl#transformers.TFCTRLLMHeadModel">TFCTRLLMHeadModel</a> (CTRL model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/camembert#transformers.TFCamembertForMaskedLM">TFCamembertForMaskedLM</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/distilbert#transformers.TFDistilBertForMaskedLM">TFDistilBertForMaskedLM</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/electra#transformers.TFElectraForPreTraining">TFElectraForPreTraining</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/flaubert#transformers.TFFlaubertWithLMHeadModel">TFFlaubertWithLMHeadModel</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/funnel#transformers.TFFunnelForPreTraining">TFFunnelForPreTraining</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/gpt2#transformers.TFGPT2LMHeadModel">TFGPT2LMHeadModel</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/layoutlm#transformers.TFLayoutLMForMaskedLM">TFLayoutLMForMaskedLM</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/lxmert#transformers.LxmertConfig">LxmertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/lxmert#transformers.TFLxmertForPreTraining">TFLxmertForPreTraining</a> (LXMERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/mpnet#transformers.TFMPNetForMaskedLM">TFMPNetForMaskedLM</a> (MPNet model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/mobilebert#transformers.TFMobileBertForPreTraining">TFMobileBertForPreTraining</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig">OpenAIGPTConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/openai-gpt#transformers.TFOpenAIGPTLMHeadModel">TFOpenAIGPTLMHeadModel</a> (OpenAI GPT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/roberta#transformers.TFRobertaForMaskedLM">TFRobertaForMaskedLM</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/t5#transformers.T5Config">T5Config</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/t5#transformers.TFT5ForConditionalGeneration">TFT5ForConditionalGeneration</a> (T5 model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/tapas#transformers.TFTapasForMaskedLM">TFTapasForMaskedLM</a> (TAPAS model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/transfo-xl#transformers.TransfoXLConfig">TransfoXLConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/transfo-xl#transformers.TFTransfoXLLMHeadModel">TFTransfoXLLMHeadModel</a> (Transformer-XL model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/xlm#transformers.TFXLMWithLMHeadModel">TFXLMWithLMHeadModel</a> (XLM model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForMaskedLM">TFXLMRobertaForMaskedLM</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/xlnet#transformers.TFXLNetLMHeadModel">TFXLNetLMHeadModel</a> (XLNet model)</li>
</ul>`,name:"config"}]}}),Oy=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForPreTraining

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = TFAutoModelForPreTraining.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForPreTraining

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForPreTraining.from_config(config)`}}),Xy=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),zy=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForPreTraining

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForPreTraining.from_pretrained("bert-base-cased")

# Update configuration during loading
model = TFAutoModelForPreTraining.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = TFAutoModelForPreTraining.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForPreTraining

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForPreTraining.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForPreTraining.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForPreTraining.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),Vy=new z({}),Wy=new E({props:{name:"class transformers.TFAutoModelForCausalLM",anchor:"transformers.TFAutoModelForCausalLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/modeling_tf_auto.py#L393"}}),Hy=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15682/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/bert#transformers.TFBertLMHeadModel">TFBertLMHeadModel</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/ctrl#transformers.CTRLConfig">CTRLConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/ctrl#transformers.TFCTRLLMHeadModel">TFCTRLLMHeadModel</a> (CTRL model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/gpt2#transformers.TFGPT2LMHeadModel">TFGPT2LMHeadModel</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig">OpenAIGPTConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/openai-gpt#transformers.TFOpenAIGPTLMHeadModel">TFOpenAIGPTLMHeadModel</a> (OpenAI GPT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/rembert#transformers.TFRemBertForCausalLM">TFRemBertForCausalLM</a> (RemBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/roformer#transformers.TFRoFormerForCausalLM">TFRoFormerForCausalLM</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/roberta#transformers.TFRobertaForCausalLM">TFRobertaForCausalLM</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/transfo-xl#transformers.TransfoXLConfig">TransfoXLConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/transfo-xl#transformers.TFTransfoXLLMHeadModel">TFTransfoXLLMHeadModel</a> (Transformer-XL model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/xlm#transformers.TFXLMWithLMHeadModel">TFXLMWithLMHeadModel</a> (XLM model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/xlnet#transformers.TFXLNetLMHeadModel">TFXLNetLMHeadModel</a> (XLNet model)</li>
</ul>`,name:"config"}]}}),Uy=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForCausalLM

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = TFAutoModelForCausalLM.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForCausalLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForCausalLM.from_config(config)`}}),Jy=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),Yy=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForCausalLM

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForCausalLM.from_pretrained("bert-base-cased")

# Update configuration during loading
model = TFAutoModelForCausalLM.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = TFAutoModelForCausalLM.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForCausalLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForCausalLM.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),Ky=new z({}),Zy=new E({props:{name:"class transformers.TFAutoModelForImageClassification",anchor:"transformers.TFAutoModelForImageClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/modeling_tf_auto.py#L400"}}),ow=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15682/en/model_doc/vit#transformers.ViTConfig">ViTConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/vit#transformers.TFViTForImageClassification">TFViTForImageClassification</a> (ViT model)</li>
</ul>`,name:"config"}]}}),rw=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForImageClassification

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = TFAutoModelForImageClassification.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForImageClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForImageClassification.from_config(config)`}}),tw=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),aw=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForImageClassification

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForImageClassification.from_pretrained("bert-base-cased")

# Update configuration during loading
model = TFAutoModelForImageClassification.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = TFAutoModelForImageClassification.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForImageClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForImageClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForImageClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForImageClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),nw=new z({}),sw=new E({props:{name:"class transformers.TFAutoModelForMaskedLM",anchor:"transformers.TFAutoModelForMaskedLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/modeling_tf_auto.py#L414"}}),iw=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15682/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/albert#transformers.TFAlbertForMaskedLM">TFAlbertForMaskedLM</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/bert#transformers.TFBertForMaskedLM">TFBertForMaskedLM</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/camembert#transformers.TFCamembertForMaskedLM">TFCamembertForMaskedLM</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/convbert#transformers.TFConvBertForMaskedLM">TFConvBertForMaskedLM</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/deberta#transformers.TFDebertaForMaskedLM">TFDebertaForMaskedLM</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/deberta-v2#transformers.TFDebertaV2ForMaskedLM">TFDebertaV2ForMaskedLM</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/distilbert#transformers.TFDistilBertForMaskedLM">TFDistilBertForMaskedLM</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/electra#transformers.TFElectraForMaskedLM">TFElectraForMaskedLM</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/flaubert#transformers.TFFlaubertWithLMHeadModel">TFFlaubertWithLMHeadModel</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/funnel#transformers.TFFunnelForMaskedLM">TFFunnelForMaskedLM</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/layoutlm#transformers.TFLayoutLMForMaskedLM">TFLayoutLMForMaskedLM</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/longformer#transformers.TFLongformerForMaskedLM">TFLongformerForMaskedLM</a> (Longformer model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/mpnet#transformers.TFMPNetForMaskedLM">TFMPNetForMaskedLM</a> (MPNet model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/mobilebert#transformers.TFMobileBertForMaskedLM">TFMobileBertForMaskedLM</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/rembert#transformers.TFRemBertForMaskedLM">TFRemBertForMaskedLM</a> (RemBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/roformer#transformers.TFRoFormerForMaskedLM">TFRoFormerForMaskedLM</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/roberta#transformers.TFRobertaForMaskedLM">TFRobertaForMaskedLM</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/tapas#transformers.TFTapasForMaskedLM">TFTapasForMaskedLM</a> (TAPAS model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/xlm#transformers.TFXLMWithLMHeadModel">TFXLMWithLMHeadModel</a> (XLM model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForMaskedLM">TFXLMRobertaForMaskedLM</a> (XLM-RoBERTa model)</li>
</ul>`,name:"config"}]}}),dw=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForMaskedLM

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = TFAutoModelForMaskedLM.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForMaskedLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForMaskedLM.from_config(config)`}}),cw=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),fw=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForMaskedLM

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForMaskedLM.from_pretrained("bert-base-cased")

# Update configuration during loading
model = TFAutoModelForMaskedLM.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = TFAutoModelForMaskedLM.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForMaskedLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForMaskedLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForMaskedLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForMaskedLM.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),mw=new z({}),gw=new E({props:{name:"class transformers.TFAutoModelForSeq2SeqLM",anchor:"transformers.TFAutoModelForSeq2SeqLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/modeling_tf_auto.py#L421"}}),pw=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15682/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/bart#transformers.TFBartForConditionalGeneration">TFBartForConditionalGeneration</a> (BART model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/blenderbot#transformers.BlenderbotConfig">BlenderbotConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/blenderbot#transformers.TFBlenderbotForConditionalGeneration">TFBlenderbotForConditionalGeneration</a> (Blenderbot model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/blenderbot-small#transformers.BlenderbotSmallConfig">BlenderbotSmallConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/blenderbot-small#transformers.TFBlenderbotSmallForConditionalGeneration">TFBlenderbotSmallForConditionalGeneration</a> (BlenderbotSmall model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/encoder-decoder#transformers.EncoderDecoderConfig">EncoderDecoderConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/encoder-decoder#transformers.TFEncoderDecoderModel">TFEncoderDecoderModel</a> (Encoder decoder model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/led#transformers.LEDConfig">LEDConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/led#transformers.TFLEDForConditionalGeneration">TFLEDForConditionalGeneration</a> (LED model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/mbart#transformers.TFMBartForConditionalGeneration">TFMBartForConditionalGeneration</a> (mBART model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/mt5#transformers.MT5Config">MT5Config</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/mt5#transformers.TFMT5ForConditionalGeneration">TFMT5ForConditionalGeneration</a> (mT5 model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/marian#transformers.MarianConfig">MarianConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/marian#transformers.TFMarianMTModel">TFMarianMTModel</a> (Marian model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/pegasus#transformers.PegasusConfig">PegasusConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/pegasus#transformers.TFPegasusForConditionalGeneration">TFPegasusForConditionalGeneration</a> (Pegasus model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/t5#transformers.T5Config">T5Config</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/t5#transformers.TFT5ForConditionalGeneration">TFT5ForConditionalGeneration</a> (T5 model)</li>
</ul>`,name:"config"}]}}),_w=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForSeq2SeqLM

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("t5-base")
model = TFAutoModelForSeq2SeqLM.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForSeq2SeqLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;t5-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSeq2SeqLM.from_config(config)`}}),uw=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),bw=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForSeq2SeqLM

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForSeq2SeqLM.from_pretrained("t5-base")

# Update configuration during loading
model = TFAutoModelForSeq2SeqLM.from_pretrained("t5-base", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/t5_pt_model_config.json")
model = TFAutoModelForSeq2SeqLM.from_pretrained(
    "./pt_model/t5_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForSeq2SeqLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSeq2SeqLM.from_pretrained(<span class="hljs-string">&quot;t5-base&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSeq2SeqLM.from_pretrained(<span class="hljs-string">&quot;t5-base&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/t5_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSeq2SeqLM.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/t5_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),vw=new z({}),Tw=new E({props:{name:"class transformers.TFAutoModelForSequenceClassification",anchor:"transformers.TFAutoModelForSequenceClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/modeling_tf_auto.py#L430"}}),Cw=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15682/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/albert#transformers.TFAlbertForSequenceClassification">TFAlbertForSequenceClassification</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/bert#transformers.TFBertForSequenceClassification">TFBertForSequenceClassification</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/ctrl#transformers.CTRLConfig">CTRLConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/ctrl#transformers.TFCTRLForSequenceClassification">TFCTRLForSequenceClassification</a> (CTRL model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/camembert#transformers.TFCamembertForSequenceClassification">TFCamembertForSequenceClassification</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/convbert#transformers.TFConvBertForSequenceClassification">TFConvBertForSequenceClassification</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/deberta#transformers.TFDebertaForSequenceClassification">TFDebertaForSequenceClassification</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/deberta-v2#transformers.TFDebertaV2ForSequenceClassification">TFDebertaV2ForSequenceClassification</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/distilbert#transformers.TFDistilBertForSequenceClassification">TFDistilBertForSequenceClassification</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/electra#transformers.TFElectraForSequenceClassification">TFElectraForSequenceClassification</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/flaubert#transformers.TFFlaubertForSequenceClassification">TFFlaubertForSequenceClassification</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/funnel#transformers.TFFunnelForSequenceClassification">TFFunnelForSequenceClassification</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/gpt2#transformers.TFGPT2ForSequenceClassification">TFGPT2ForSequenceClassification</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/layoutlm#transformers.TFLayoutLMForSequenceClassification">TFLayoutLMForSequenceClassification</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/longformer#transformers.TFLongformerForSequenceClassification">TFLongformerForSequenceClassification</a> (Longformer model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/mpnet#transformers.TFMPNetForSequenceClassification">TFMPNetForSequenceClassification</a> (MPNet model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/mobilebert#transformers.TFMobileBertForSequenceClassification">TFMobileBertForSequenceClassification</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig">OpenAIGPTConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/openai-gpt#transformers.TFOpenAIGPTForSequenceClassification">TFOpenAIGPTForSequenceClassification</a> (OpenAI GPT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/rembert#transformers.TFRemBertForSequenceClassification">TFRemBertForSequenceClassification</a> (RemBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/roformer#transformers.TFRoFormerForSequenceClassification">TFRoFormerForSequenceClassification</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/roberta#transformers.TFRobertaForSequenceClassification">TFRobertaForSequenceClassification</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/tapas#transformers.TFTapasForSequenceClassification">TFTapasForSequenceClassification</a> (TAPAS model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/transfo-xl#transformers.TransfoXLConfig">TransfoXLConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/transfo-xl#transformers.TFTransfoXLForSequenceClassification">TFTransfoXLForSequenceClassification</a> (Transformer-XL model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/xlm#transformers.TFXLMForSequenceClassification">TFXLMForSequenceClassification</a> (XLM model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForSequenceClassification">TFXLMRobertaForSequenceClassification</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/xlnet#transformers.TFXLNetForSequenceClassification">TFXLNetForSequenceClassification</a> (XLNet model)</li>
</ul>`,name:"config"}]}}),Mw=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForSequenceClassification

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = TFAutoModelForSequenceClassification.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSequenceClassification.from_config(config)`}}),Ew=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),yw=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForSequenceClassification

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForSequenceClassification.from_pretrained("bert-base-cased")

# Update configuration during loading
model = TFAutoModelForSequenceClassification.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = TFAutoModelForSequenceClassification.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSequenceClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),ww=new z({}),Aw=new E({props:{name:"class transformers.TFAutoModelForMultipleChoice",anchor:"transformers.TFAutoModelForMultipleChoice",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/modeling_tf_auto.py#L466"}}),Bw=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15682/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/albert#transformers.TFAlbertForMultipleChoice">TFAlbertForMultipleChoice</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/bert#transformers.TFBertForMultipleChoice">TFBertForMultipleChoice</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/camembert#transformers.TFCamembertForMultipleChoice">TFCamembertForMultipleChoice</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/convbert#transformers.TFConvBertForMultipleChoice">TFConvBertForMultipleChoice</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/distilbert#transformers.TFDistilBertForMultipleChoice">TFDistilBertForMultipleChoice</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/electra#transformers.TFElectraForMultipleChoice">TFElectraForMultipleChoice</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/flaubert#transformers.TFFlaubertForMultipleChoice">TFFlaubertForMultipleChoice</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/funnel#transformers.TFFunnelForMultipleChoice">TFFunnelForMultipleChoice</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/longformer#transformers.TFLongformerForMultipleChoice">TFLongformerForMultipleChoice</a> (Longformer model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/mpnet#transformers.TFMPNetForMultipleChoice">TFMPNetForMultipleChoice</a> (MPNet model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/mobilebert#transformers.TFMobileBertForMultipleChoice">TFMobileBertForMultipleChoice</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/rembert#transformers.TFRemBertForMultipleChoice">TFRemBertForMultipleChoice</a> (RemBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/roformer#transformers.TFRoFormerForMultipleChoice">TFRoFormerForMultipleChoice</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/roberta#transformers.TFRobertaForMultipleChoice">TFRobertaForMultipleChoice</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/xlm#transformers.TFXLMForMultipleChoice">TFXLMForMultipleChoice</a> (XLM model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForMultipleChoice">TFXLMRobertaForMultipleChoice</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/xlnet#transformers.TFXLNetForMultipleChoice">TFXLNetForMultipleChoice</a> (XLNet model)</li>
</ul>`,name:"config"}]}}),kw=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForMultipleChoice

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = TFAutoModelForMultipleChoice.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForMultipleChoice

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForMultipleChoice.from_config(config)`}}),xw=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),Rw=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForMultipleChoice

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForMultipleChoice.from_pretrained("bert-base-cased")

# Update configuration during loading
model = TFAutoModelForMultipleChoice.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = TFAutoModelForMultipleChoice.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForMultipleChoice

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForMultipleChoice.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForMultipleChoice.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForMultipleChoice.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),Sw=new z({}),Pw=new E({props:{name:"class transformers.TFAutoModelForTableQuestionAnswering",anchor:"transformers.TFAutoModelForTableQuestionAnswering",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/modeling_tf_auto.py#L446"}}),Iw=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15682/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/tapas#transformers.TFTapasForQuestionAnswering">TFTapasForQuestionAnswering</a> (TAPAS model)</li>
</ul>`,name:"config"}]}}),jw=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForTableQuestionAnswering

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("google/tapas-base-finetuned-wtq")
model = TFAutoModelForTableQuestionAnswering.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForTableQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;google/tapas-base-finetuned-wtq&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForTableQuestionAnswering.from_config(config)`}}),Nw=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),Dw=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForTableQuestionAnswering

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForTableQuestionAnswering.from_pretrained("google/tapas-base-finetuned-wtq")

# Update configuration during loading
model = TFAutoModelForTableQuestionAnswering.from_pretrained("google/tapas-base-finetuned-wtq", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/tapas_pt_model_config.json")
model = TFAutoModelForTableQuestionAnswering.from_pretrained(
    "./pt_model/tapas_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForTableQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForTableQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;google/tapas-base-finetuned-wtq&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForTableQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;google/tapas-base-finetuned-wtq&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/tapas_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForTableQuestionAnswering.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/tapas_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),qw=new z({}),Gw=new E({props:{name:"class transformers.TFAutoModelForTokenClassification",anchor:"transformers.TFAutoModelForTokenClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/modeling_tf_auto.py#L457"}}),Xw=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15682/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/albert#transformers.TFAlbertForTokenClassification">TFAlbertForTokenClassification</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/bert#transformers.TFBertForTokenClassification">TFBertForTokenClassification</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/camembert#transformers.TFCamembertForTokenClassification">TFCamembertForTokenClassification</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/convbert#transformers.TFConvBertForTokenClassification">TFConvBertForTokenClassification</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/deberta#transformers.TFDebertaForTokenClassification">TFDebertaForTokenClassification</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/deberta-v2#transformers.TFDebertaV2ForTokenClassification">TFDebertaV2ForTokenClassification</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/distilbert#transformers.TFDistilBertForTokenClassification">TFDistilBertForTokenClassification</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/electra#transformers.TFElectraForTokenClassification">TFElectraForTokenClassification</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/flaubert#transformers.TFFlaubertForTokenClassification">TFFlaubertForTokenClassification</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/funnel#transformers.TFFunnelForTokenClassification">TFFunnelForTokenClassification</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/layoutlm#transformers.TFLayoutLMForTokenClassification">TFLayoutLMForTokenClassification</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/longformer#transformers.TFLongformerForTokenClassification">TFLongformerForTokenClassification</a> (Longformer model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/mpnet#transformers.TFMPNetForTokenClassification">TFMPNetForTokenClassification</a> (MPNet model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/mobilebert#transformers.TFMobileBertForTokenClassification">TFMobileBertForTokenClassification</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/rembert#transformers.TFRemBertForTokenClassification">TFRemBertForTokenClassification</a> (RemBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/roformer#transformers.TFRoFormerForTokenClassification">TFRoFormerForTokenClassification</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/roberta#transformers.TFRobertaForTokenClassification">TFRobertaForTokenClassification</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/xlm#transformers.TFXLMForTokenClassification">TFXLMForTokenClassification</a> (XLM model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForTokenClassification">TFXLMRobertaForTokenClassification</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/xlnet#transformers.TFXLNetForTokenClassification">TFXLNetForTokenClassification</a> (XLNet model)</li>
</ul>`,name:"config"}]}}),zw=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForTokenClassification

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = TFAutoModelForTokenClassification.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForTokenClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForTokenClassification.from_config(config)`}}),Vw=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),Ww=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForTokenClassification

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForTokenClassification.from_pretrained("bert-base-cased")

# Update configuration during loading
model = TFAutoModelForTokenClassification.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = TFAutoModelForTokenClassification.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForTokenClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForTokenClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForTokenClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForTokenClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),Qw=new z({}),Hw=new E({props:{name:"class transformers.TFAutoModelForQuestionAnswering",anchor:"transformers.TFAutoModelForQuestionAnswering",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/modeling_tf_auto.py#L439"}}),Jw=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15682/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/albert#transformers.TFAlbertForQuestionAnswering">TFAlbertForQuestionAnswering</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/bert#transformers.TFBertForQuestionAnswering">TFBertForQuestionAnswering</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/camembert#transformers.TFCamembertForQuestionAnswering">TFCamembertForQuestionAnswering</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/convbert#transformers.TFConvBertForQuestionAnswering">TFConvBertForQuestionAnswering</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/deberta#transformers.TFDebertaForQuestionAnswering">TFDebertaForQuestionAnswering</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/deberta-v2#transformers.TFDebertaV2ForQuestionAnswering">TFDebertaV2ForQuestionAnswering</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/distilbert#transformers.TFDistilBertForQuestionAnswering">TFDistilBertForQuestionAnswering</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/electra#transformers.TFElectraForQuestionAnswering">TFElectraForQuestionAnswering</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/flaubert#transformers.TFFlaubertForQuestionAnsweringSimple">TFFlaubertForQuestionAnsweringSimple</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/funnel#transformers.TFFunnelForQuestionAnswering">TFFunnelForQuestionAnswering</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/longformer#transformers.TFLongformerForQuestionAnswering">TFLongformerForQuestionAnswering</a> (Longformer model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/mpnet#transformers.TFMPNetForQuestionAnswering">TFMPNetForQuestionAnswering</a> (MPNet model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/mobilebert#transformers.TFMobileBertForQuestionAnswering">TFMobileBertForQuestionAnswering</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/rembert#transformers.TFRemBertForQuestionAnswering">TFRemBertForQuestionAnswering</a> (RemBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/roformer#transformers.TFRoFormerForQuestionAnswering">TFRoFormerForQuestionAnswering</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/roberta#transformers.TFRobertaForQuestionAnswering">TFRobertaForQuestionAnswering</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/xlm#transformers.TFXLMForQuestionAnsweringSimple">TFXLMForQuestionAnsweringSimple</a> (XLM model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForQuestionAnswering">TFXLMRobertaForQuestionAnswering</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/xlnet#transformers.TFXLNetForQuestionAnsweringSimple">TFXLNetForQuestionAnsweringSimple</a> (XLNet model)</li>
</ul>`,name:"config"}]}}),Yw=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForQuestionAnswering

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = TFAutoModelForQuestionAnswering.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForQuestionAnswering.from_config(config)`}}),Kw=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),Zw=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForQuestionAnswering

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForQuestionAnswering.from_pretrained("bert-base-cased")

# Update configuration during loading
model = TFAutoModelForQuestionAnswering.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = TFAutoModelForQuestionAnswering.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForQuestionAnswering.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),eA=new z({}),oA=new E({props:{name:"class transformers.TFAutoModelForVision2Seq",anchor:"transformers.TFAutoModelForVision2Seq",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/modeling_tf_auto.py#L407"}}),tA=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15682/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderConfig">VisionEncoderDecoderConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/vision-encoder-decoder#transformers.TFVisionEncoderDecoderModel">TFVisionEncoderDecoderModel</a> (Vision Encoder decoder model)</li>
</ul>`,name:"config"}]}}),aA=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForVision2Seq

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = TFAutoModelForVision2Seq.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForVision2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForVision2Seq.from_config(config)`}}),nA=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),sA=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForVision2Seq

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForVision2Seq.from_pretrained("bert-base-cased")

# Update configuration during loading
model = TFAutoModelForVision2Seq.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = TFAutoModelForVision2Seq.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForVision2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForVision2Seq.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForVision2Seq.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForVision2Seq.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),lA=new z({}),iA=new E({props:{name:"class transformers.TFAutoModelForSpeechSeq2Seq",anchor:"transformers.TFAutoModelForSpeechSeq2Seq",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/modeling_tf_auto.py#L482"}}),cA=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15682/en/model_doc/speech_to_text#transformers.Speech2TextConfig">Speech2TextConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/speech_to_text#transformers.TFSpeech2TextForConditionalGeneration">TFSpeech2TextForConditionalGeneration</a> (Speech2Text model)</li>
</ul>`,name:"config"}]}}),fA=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForSpeechSeq2Seq

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = TFAutoModelForSpeechSeq2Seq.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForSpeechSeq2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSpeechSeq2Seq.from_config(config)`}}),mA=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),gA=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForSpeechSeq2Seq

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForSpeechSeq2Seq.from_pretrained("bert-base-cased")

# Update configuration during loading
model = TFAutoModelForSpeechSeq2Seq.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = TFAutoModelForSpeechSeq2Seq.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForSpeechSeq2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSpeechSeq2Seq.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSpeechSeq2Seq.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSpeechSeq2Seq.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),hA=new z({}),pA=new E({props:{name:"class transformers.FlaxAutoModel",anchor:"transformers.FlaxAutoModel",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/modeling_flax_auto.py#L220"}}),uA=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15682/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/albert#transformers.FlaxAlbertModel">FlaxAlbertModel</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/bart#transformers.FlaxBartModel">FlaxBartModel</a> (BART model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/beit#transformers.BeitConfig">BeitConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/beit#transformers.FlaxBeitModel">FlaxBeitModel</a> (BEiT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/bert#transformers.FlaxBertModel">FlaxBertModel</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/big_bird#transformers.FlaxBigBirdModel">FlaxBigBirdModel</a> (BigBird model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/blenderbot#transformers.BlenderbotConfig">BlenderbotConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/blenderbot#transformers.FlaxBlenderbotModel">FlaxBlenderbotModel</a> (Blenderbot model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/blenderbot-small#transformers.BlenderbotSmallConfig">BlenderbotSmallConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/blenderbot-small#transformers.FlaxBlenderbotSmallModel">FlaxBlenderbotSmallModel</a> (BlenderbotSmall model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/clip#transformers.CLIPConfig">CLIPConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/clip#transformers.FlaxCLIPModel">FlaxCLIPModel</a> (CLIP model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/distilbert#transformers.FlaxDistilBertModel">FlaxDistilBertModel</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/electra#transformers.FlaxElectraModel">FlaxElectraModel</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/gpt2#transformers.FlaxGPT2Model">FlaxGPT2Model</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/gptj#transformers.GPTJConfig">GPTJConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/gptj#transformers.FlaxGPTJModel">FlaxGPTJModel</a> (GPT-J model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/gpt_neo#transformers.GPTNeoConfig">GPTNeoConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/gpt_neo#transformers.FlaxGPTNeoModel">FlaxGPTNeoModel</a> (GPT Neo model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/mbart#transformers.FlaxMBartModel">FlaxMBartModel</a> (mBART model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/mt5#transformers.MT5Config">MT5Config</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/mt5#transformers.FlaxMT5Model">FlaxMT5Model</a> (mT5 model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/marian#transformers.MarianConfig">MarianConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/marian#transformers.FlaxMarianModel">FlaxMarianModel</a> (Marian model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/pegasus#transformers.PegasusConfig">PegasusConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/pegasus#transformers.FlaxPegasusModel">FlaxPegasusModel</a> (Pegasus model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/roformer#transformers.FlaxRoFormerModel">FlaxRoFormerModel</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/roberta#transformers.FlaxRobertaModel">FlaxRobertaModel</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/t5#transformers.T5Config">T5Config</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/t5#transformers.FlaxT5Model">FlaxT5Model</a> (T5 model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/vit#transformers.ViTConfig">ViTConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/vit#transformers.FlaxViTModel">FlaxViTModel</a> (ViT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/vision-text-dual-encoder#transformers.VisionTextDualEncoderConfig">VisionTextDualEncoderConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/vision-text-dual-encoder#transformers.FlaxVisionTextDualEncoderModel">FlaxVisionTextDualEncoderModel</a> (VisionTextDualEncoder model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/wav2vec2#transformers.FlaxWav2Vec2Model">FlaxWav2Vec2Model</a> (Wav2Vec2 model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/xglm#transformers.XGLMConfig">XGLMConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/xglm#transformers.FlaxXGLMModel">FlaxXGLMModel</a> (XGLM model)</li>
</ul>`,name:"config"}]}}),bA=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModel

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = FlaxAutoModel.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModel.from_config(config)`}}),vA=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),TA=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModel

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModel.from_pretrained("bert-base-cased")

# Update configuration during loading
model = FlaxAutoModel.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = FlaxAutoModel.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModel.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModel.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModel.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),FA=new z({}),CA=new E({props:{name:"class transformers.FlaxAutoModelForCausalLM",anchor:"transformers.FlaxAutoModelForCausalLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/modeling_flax_auto.py#L234"}}),EA=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15682/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/gpt2#transformers.FlaxGPT2LMHeadModel">FlaxGPT2LMHeadModel</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/gptj#transformers.GPTJConfig">GPTJConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/gptj#transformers.FlaxGPTJForCausalLM">FlaxGPTJForCausalLM</a> (GPT-J model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/gpt_neo#transformers.GPTNeoConfig">GPTNeoConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/gpt_neo#transformers.FlaxGPTNeoForCausalLM">FlaxGPTNeoForCausalLM</a> (GPT Neo model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/xglm#transformers.XGLMConfig">XGLMConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/xglm#transformers.FlaxXGLMForCausalLM">FlaxXGLMForCausalLM</a> (XGLM model)</li>
</ul>`,name:"config"}]}}),yA=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForCausalLM

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = FlaxAutoModelForCausalLM.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForCausalLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForCausalLM.from_config(config)`}}),wA=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),AA=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForCausalLM

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForCausalLM.from_pretrained("bert-base-cased")

# Update configuration during loading
model = FlaxAutoModelForCausalLM.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = FlaxAutoModelForCausalLM.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForCausalLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForCausalLM.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),LA=new z({}),BA=new E({props:{name:"class transformers.FlaxAutoModelForPreTraining",anchor:"transformers.FlaxAutoModelForPreTraining",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/modeling_flax_auto.py#L227"}}),xA=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15682/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/albert#transformers.FlaxAlbertForPreTraining">FlaxAlbertForPreTraining</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/bart#transformers.FlaxBartForConditionalGeneration">FlaxBartForConditionalGeneration</a> (BART model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/bert#transformers.FlaxBertForPreTraining">FlaxBertForPreTraining</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/big_bird#transformers.FlaxBigBirdForPreTraining">FlaxBigBirdForPreTraining</a> (BigBird model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/electra#transformers.FlaxElectraForPreTraining">FlaxElectraForPreTraining</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/mbart#transformers.FlaxMBartForConditionalGeneration">FlaxMBartForConditionalGeneration</a> (mBART model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/mt5#transformers.MT5Config">MT5Config</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/mt5#transformers.FlaxMT5ForConditionalGeneration">FlaxMT5ForConditionalGeneration</a> (mT5 model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/roformer#transformers.FlaxRoFormerForMaskedLM">FlaxRoFormerForMaskedLM</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/roberta#transformers.FlaxRobertaForMaskedLM">FlaxRobertaForMaskedLM</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/t5#transformers.T5Config">T5Config</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/t5#transformers.FlaxT5ForConditionalGeneration">FlaxT5ForConditionalGeneration</a> (T5 model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/wav2vec2#transformers.FlaxWav2Vec2ForPreTraining">FlaxWav2Vec2ForPreTraining</a> (Wav2Vec2 model)</li>
</ul>`,name:"config"}]}}),RA=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForPreTraining

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = FlaxAutoModelForPreTraining.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForPreTraining

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForPreTraining.from_config(config)`}}),SA=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),PA=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForPreTraining

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForPreTraining.from_pretrained("bert-base-cased")

# Update configuration during loading
model = FlaxAutoModelForPreTraining.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = FlaxAutoModelForPreTraining.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForPreTraining

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForPreTraining.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForPreTraining.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForPreTraining.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),$A=new z({}),IA=new E({props:{name:"class transformers.FlaxAutoModelForMaskedLM",anchor:"transformers.FlaxAutoModelForMaskedLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/modeling_flax_auto.py#L241"}}),NA=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15682/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/albert#transformers.FlaxAlbertForMaskedLM">FlaxAlbertForMaskedLM</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/bart#transformers.FlaxBartForConditionalGeneration">FlaxBartForConditionalGeneration</a> (BART model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/bert#transformers.FlaxBertForMaskedLM">FlaxBertForMaskedLM</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/big_bird#transformers.FlaxBigBirdForMaskedLM">FlaxBigBirdForMaskedLM</a> (BigBird model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/distilbert#transformers.FlaxDistilBertForMaskedLM">FlaxDistilBertForMaskedLM</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/electra#transformers.FlaxElectraForMaskedLM">FlaxElectraForMaskedLM</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/mbart#transformers.FlaxMBartForConditionalGeneration">FlaxMBartForConditionalGeneration</a> (mBART model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/roformer#transformers.FlaxRoFormerForMaskedLM">FlaxRoFormerForMaskedLM</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/roberta#transformers.FlaxRobertaForMaskedLM">FlaxRobertaForMaskedLM</a> (RoBERTa model)</li>
</ul>`,name:"config"}]}}),DA=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForMaskedLM

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = FlaxAutoModelForMaskedLM.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForMaskedLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForMaskedLM.from_config(config)`}}),qA=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),GA=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForMaskedLM

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForMaskedLM.from_pretrained("bert-base-cased")

# Update configuration during loading
model = FlaxAutoModelForMaskedLM.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = FlaxAutoModelForMaskedLM.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForMaskedLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForMaskedLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForMaskedLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForMaskedLM.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),OA=new z({}),XA=new E({props:{name:"class transformers.FlaxAutoModelForSeq2SeqLM",anchor:"transformers.FlaxAutoModelForSeq2SeqLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/modeling_flax_auto.py#L248"}}),VA=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15682/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/bart#transformers.FlaxBartForConditionalGeneration">FlaxBartForConditionalGeneration</a> (BART model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/blenderbot#transformers.BlenderbotConfig">BlenderbotConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/blenderbot#transformers.FlaxBlenderbotForConditionalGeneration">FlaxBlenderbotForConditionalGeneration</a> (Blenderbot model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/blenderbot-small#transformers.BlenderbotSmallConfig">BlenderbotSmallConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/blenderbot-small#transformers.FlaxBlenderbotSmallForConditionalGeneration">FlaxBlenderbotSmallForConditionalGeneration</a> (BlenderbotSmall model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/encoder-decoder#transformers.EncoderDecoderConfig">EncoderDecoderConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/encoder-decoder#transformers.FlaxEncoderDecoderModel">FlaxEncoderDecoderModel</a> (Encoder decoder model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/mbart#transformers.FlaxMBartForConditionalGeneration">FlaxMBartForConditionalGeneration</a> (mBART model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/mt5#transformers.MT5Config">MT5Config</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/mt5#transformers.FlaxMT5ForConditionalGeneration">FlaxMT5ForConditionalGeneration</a> (mT5 model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/marian#transformers.MarianConfig">MarianConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/marian#transformers.FlaxMarianMTModel">FlaxMarianMTModel</a> (Marian model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/pegasus#transformers.PegasusConfig">PegasusConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/pegasus#transformers.FlaxPegasusForConditionalGeneration">FlaxPegasusForConditionalGeneration</a> (Pegasus model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/t5#transformers.T5Config">T5Config</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/t5#transformers.FlaxT5ForConditionalGeneration">FlaxT5ForConditionalGeneration</a> (T5 model)</li>
</ul>`,name:"config"}]}}),WA=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForSeq2SeqLM

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("t5-base")
model = FlaxAutoModelForSeq2SeqLM.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForSeq2SeqLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;t5-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForSeq2SeqLM.from_config(config)`}}),QA=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),HA=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForSeq2SeqLM

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForSeq2SeqLM.from_pretrained("t5-base")

# Update configuration during loading
model = FlaxAutoModelForSeq2SeqLM.from_pretrained("t5-base", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/t5_pt_model_config.json")
model = FlaxAutoModelForSeq2SeqLM.from_pretrained(
    "./pt_model/t5_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForSeq2SeqLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForSeq2SeqLM.from_pretrained(<span class="hljs-string">&quot;t5-base&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForSeq2SeqLM.from_pretrained(<span class="hljs-string">&quot;t5-base&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/t5_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForSeq2SeqLM.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/t5_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),UA=new z({}),JA=new E({props:{name:"class transformers.FlaxAutoModelForSequenceClassification",anchor:"transformers.FlaxAutoModelForSequenceClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/modeling_flax_auto.py#L257"}}),KA=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15682/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/albert#transformers.FlaxAlbertForSequenceClassification">FlaxAlbertForSequenceClassification</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/bart#transformers.FlaxBartForSequenceClassification">FlaxBartForSequenceClassification</a> (BART model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/bert#transformers.FlaxBertForSequenceClassification">FlaxBertForSequenceClassification</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/big_bird#transformers.FlaxBigBirdForSequenceClassification">FlaxBigBirdForSequenceClassification</a> (BigBird model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/distilbert#transformers.FlaxDistilBertForSequenceClassification">FlaxDistilBertForSequenceClassification</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/electra#transformers.FlaxElectraForSequenceClassification">FlaxElectraForSequenceClassification</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/mbart#transformers.FlaxMBartForSequenceClassification">FlaxMBartForSequenceClassification</a> (mBART model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/roformer#transformers.FlaxRoFormerForSequenceClassification">FlaxRoFormerForSequenceClassification</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/roberta#transformers.FlaxRobertaForSequenceClassification">FlaxRobertaForSequenceClassification</a> (RoBERTa model)</li>
</ul>`,name:"config"}]}}),ZA=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForSequenceClassification

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = FlaxAutoModelForSequenceClassification.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForSequenceClassification.from_config(config)`}}),e0=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),o0=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForSequenceClassification

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForSequenceClassification.from_pretrained("bert-base-cased")

# Update configuration during loading
model = FlaxAutoModelForSequenceClassification.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = FlaxAutoModelForSequenceClassification.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForSequenceClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),r0=new z({}),t0=new E({props:{name:"class transformers.FlaxAutoModelForQuestionAnswering",anchor:"transformers.FlaxAutoModelForQuestionAnswering",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/modeling_flax_auto.py#L266"}}),n0=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15682/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/albert#transformers.FlaxAlbertForQuestionAnswering">FlaxAlbertForQuestionAnswering</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/bart#transformers.FlaxBartForQuestionAnswering">FlaxBartForQuestionAnswering</a> (BART model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/bert#transformers.FlaxBertForQuestionAnswering">FlaxBertForQuestionAnswering</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/big_bird#transformers.FlaxBigBirdForQuestionAnswering">FlaxBigBirdForQuestionAnswering</a> (BigBird model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/distilbert#transformers.FlaxDistilBertForQuestionAnswering">FlaxDistilBertForQuestionAnswering</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/electra#transformers.FlaxElectraForQuestionAnswering">FlaxElectraForQuestionAnswering</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/mbart#transformers.FlaxMBartForQuestionAnswering">FlaxMBartForQuestionAnswering</a> (mBART model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/roformer#transformers.FlaxRoFormerForQuestionAnswering">FlaxRoFormerForQuestionAnswering</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/roberta#transformers.FlaxRobertaForQuestionAnswering">FlaxRobertaForQuestionAnswering</a> (RoBERTa model)</li>
</ul>`,name:"config"}]}}),s0=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForQuestionAnswering

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = FlaxAutoModelForQuestionAnswering.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForQuestionAnswering.from_config(config)`}}),l0=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),i0=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForQuestionAnswering

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForQuestionAnswering.from_pretrained("bert-base-cased")

# Update configuration during loading
model = FlaxAutoModelForQuestionAnswering.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = FlaxAutoModelForQuestionAnswering.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForQuestionAnswering.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),d0=new z({}),c0=new E({props:{name:"class transformers.FlaxAutoModelForTokenClassification",anchor:"transformers.FlaxAutoModelForTokenClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/modeling_flax_auto.py#L273"}}),m0=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15682/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/albert#transformers.FlaxAlbertForTokenClassification">FlaxAlbertForTokenClassification</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/bert#transformers.FlaxBertForTokenClassification">FlaxBertForTokenClassification</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/big_bird#transformers.FlaxBigBirdForTokenClassification">FlaxBigBirdForTokenClassification</a> (BigBird model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/distilbert#transformers.FlaxDistilBertForTokenClassification">FlaxDistilBertForTokenClassification</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/electra#transformers.FlaxElectraForTokenClassification">FlaxElectraForTokenClassification</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/roformer#transformers.FlaxRoFormerForTokenClassification">FlaxRoFormerForTokenClassification</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/roberta#transformers.FlaxRobertaForTokenClassification">FlaxRobertaForTokenClassification</a> (RoBERTa model)</li>
</ul>`,name:"config"}]}}),g0=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForTokenClassification

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = FlaxAutoModelForTokenClassification.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForTokenClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForTokenClassification.from_config(config)`}}),h0=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),p0=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForTokenClassification

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForTokenClassification.from_pretrained("bert-base-cased")

# Update configuration during loading
model = FlaxAutoModelForTokenClassification.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = FlaxAutoModelForTokenClassification.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForTokenClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForTokenClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForTokenClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForTokenClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),_0=new z({}),u0=new E({props:{name:"class transformers.FlaxAutoModelForMultipleChoice",anchor:"transformers.FlaxAutoModelForMultipleChoice",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/modeling_flax_auto.py#L282"}}),v0=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15682/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/albert#transformers.FlaxAlbertForMultipleChoice">FlaxAlbertForMultipleChoice</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/bert#transformers.FlaxBertForMultipleChoice">FlaxBertForMultipleChoice</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/big_bird#transformers.FlaxBigBirdForMultipleChoice">FlaxBigBirdForMultipleChoice</a> (BigBird model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/distilbert#transformers.FlaxDistilBertForMultipleChoice">FlaxDistilBertForMultipleChoice</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/electra#transformers.FlaxElectraForMultipleChoice">FlaxElectraForMultipleChoice</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/roformer#transformers.FlaxRoFormerForMultipleChoice">FlaxRoFormerForMultipleChoice</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/roberta#transformers.FlaxRobertaForMultipleChoice">FlaxRobertaForMultipleChoice</a> (RoBERTa model)</li>
</ul>`,name:"config"}]}}),T0=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForMultipleChoice

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = FlaxAutoModelForMultipleChoice.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForMultipleChoice

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForMultipleChoice.from_config(config)`}}),F0=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),C0=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForMultipleChoice

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForMultipleChoice.from_pretrained("bert-base-cased")

# Update configuration during loading
model = FlaxAutoModelForMultipleChoice.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = FlaxAutoModelForMultipleChoice.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForMultipleChoice

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForMultipleChoice.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForMultipleChoice.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForMultipleChoice.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),M0=new z({}),E0=new E({props:{name:"class transformers.FlaxAutoModelForNextSentencePrediction",anchor:"transformers.FlaxAutoModelForNextSentencePrediction",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/modeling_flax_auto.py#L289"}}),w0=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15682/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/bert#transformers.FlaxBertForNextSentencePrediction">FlaxBertForNextSentencePrediction</a> (BERT model)</li>
</ul>`,name:"config"}]}}),A0=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForNextSentencePrediction

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = FlaxAutoModelForNextSentencePrediction.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForNextSentencePrediction

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForNextSentencePrediction.from_config(config)`}}),L0=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),B0=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForNextSentencePrediction

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForNextSentencePrediction.from_pretrained("bert-base-cased")

# Update configuration during loading
model = FlaxAutoModelForNextSentencePrediction.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = FlaxAutoModelForNextSentencePrediction.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForNextSentencePrediction

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForNextSentencePrediction.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForNextSentencePrediction.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForNextSentencePrediction.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),k0=new z({}),x0=new E({props:{name:"class transformers.FlaxAutoModelForImageClassification",anchor:"transformers.FlaxAutoModelForImageClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/modeling_flax_auto.py#L298"}}),S0=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15682/en/model_doc/beit#transformers.BeitConfig">BeitConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/beit#transformers.FlaxBeitForImageClassification">FlaxBeitForImageClassification</a> (BEiT model)</li>
<li><a href="/docs/transformers/pr_15682/en/model_doc/vit#transformers.ViTConfig">ViTConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/vit#transformers.FlaxViTForImageClassification">FlaxViTForImageClassification</a> (ViT model)</li>
</ul>`,name:"config"}]}}),P0=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForImageClassification

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = FlaxAutoModelForImageClassification.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForImageClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForImageClassification.from_config(config)`}}),$0=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),j0=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForImageClassification

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForImageClassification.from_pretrained("bert-base-cased")

# Update configuration during loading
model = FlaxAutoModelForImageClassification.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = FlaxAutoModelForImageClassification.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForImageClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForImageClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForImageClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForImageClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),N0=new z({}),D0=new E({props:{name:"class transformers.FlaxAutoModelForVision2Seq",anchor:"transformers.FlaxAutoModelForVision2Seq",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/modeling_flax_auto.py#L307"}}),G0=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15682/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderConfig">VisionEncoderDecoderConfig</a> configuration class: <a href="/docs/transformers/pr_15682/en/model_doc/vision-encoder-decoder#transformers.FlaxVisionEncoderDecoderModel">FlaxVisionEncoderDecoderModel</a> (Vision Encoder decoder model)</li>
</ul>`,name:"config"}]}}),O0=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForVision2Seq

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = FlaxAutoModelForVision2Seq.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForVision2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForVision2Seq.from_config(config)`}}),X0=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15682/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15682/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15682/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),z0=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForVision2Seq

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForVision2Seq.from_pretrained("bert-base-cased")

# Update configuration during loading
model = FlaxAutoModelForVision2Seq.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = FlaxAutoModelForVision2Seq.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForVision2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForVision2Seq.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForVision2Seq.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForVision2Seq.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),{c(){J=a("meta"),Ae=l(),ie=a("h1"),me=a("a"),to=a("span"),f(ce.$$.fragment),ue=l(),Do=a("span"),wi=o("Auto Classes"),Ef=l(),sa=a("p"),Ai=o(`In many cases, the architecture you want to use can be guessed from the name or the path of the pretrained model you
are supplying to the `),Li=a("code"),tM=o("from_pretrained()"),yf=o(` method. AutoClasses are here to do this job for you so that you
automatically retrieve the relevant model given the name/path to the pretrained weights/config/vocabulary.`),ye=l(),io=a("p"),Bi=o("Instantiating one of "),Pn=a("a"),aM=o("AutoConfig"),$n=o(", "),In=a("a"),nM=o("AutoModel"),ki=o(`, and
`),jn=a("a"),sM=o("AutoTokenizer"),xi=o(" will directly create a class of the relevant architecture. For instance"),wf=l(),f($a.$$.fragment),co=l(),ge=a("p"),GL=o("will create a model that is an instance of "),Ri=a("a"),OL=o("BertModel"),XL=o("."),qo=l(),Ia=a("p"),zL=o("There is one class of "),Af=a("code"),VL=o("AutoModel"),Txe=o(" for each task, and for each backend (PyTorch, TensorFlow, or Flax)."),f7e=l(),Si=a("h2"),Lf=a("a"),DV=a("span"),f(lM.$$.fragment),Fxe=l(),qV=a("span"),Cxe=o("Extending the Auto Classes"),m7e=l(),Nn=a("p"),Mxe=o(`Each of the auto classes has a method to be extended with your custom classes. For instance, if you have defined a
custom class of model `),GV=a("code"),Exe=o("NewModel"),yxe=o(", make sure you have a "),OV=a("code"),wxe=o("NewModelConfig"),Axe=o(` then you can add those to the auto
classes like this:`),g7e=l(),f(iM.$$.fragment),h7e=l(),WL=a("p"),Lxe=o("You will then be able to use the auto classes like you would usually do!"),p7e=l(),f(Bf.$$.fragment),_7e=l(),Pi=a("h2"),kf=a("a"),XV=a("span"),f(dM.$$.fragment),Bxe=l(),zV=a("span"),kxe=o("AutoConfig"),u7e=l(),Go=a("div"),f(cM.$$.fragment),xxe=l(),fM=a("p"),Rxe=o(`This is a generic configuration class that will be instantiated as one of the configuration classes of the library
when created with the `),QL=a("a"),Sxe=o("from_pretrained()"),Pxe=o(" class method."),$xe=l(),mM=a("p"),Ixe=o("This class cannot be instantiated directly using "),VV=a("code"),jxe=o("__init__()"),Nxe=o(" (throws an error)."),Dxe=l(),fo=a("div"),f(gM.$$.fragment),qxe=l(),WV=a("p"),Gxe=o("Instantiate one of the configuration classes of the library from a pretrained model configuration."),Oxe=l(),$i=a("p"),Xxe=o("The configuration class to instantiate is selected based on the "),QV=a("code"),zxe=o("model_type"),Vxe=o(` property of the config object that
is loaded, or when it\u2019s missing, by falling back to using pattern matching on `),HV=a("code"),Wxe=o("pretrained_model_name_or_path"),Qxe=o(":"),Hxe=l(),v=a("ul"),xf=a("li"),UV=a("strong"),Uxe=o("albert"),Jxe=o(" \u2014 "),HL=a("a"),Yxe=o("AlbertConfig"),Kxe=o(" (ALBERT model)"),Zxe=l(),Rf=a("li"),JV=a("strong"),eRe=o("bart"),oRe=o(" \u2014 "),UL=a("a"),rRe=o("BartConfig"),tRe=o(" (BART model)"),aRe=l(),Sf=a("li"),YV=a("strong"),nRe=o("beit"),sRe=o(" \u2014 "),JL=a("a"),lRe=o("BeitConfig"),iRe=o(" (BEiT model)"),dRe=l(),Pf=a("li"),KV=a("strong"),cRe=o("bert"),fRe=o(" \u2014 "),YL=a("a"),mRe=o("BertConfig"),gRe=o(" (BERT model)"),hRe=l(),$f=a("li"),ZV=a("strong"),pRe=o("bert-generation"),_Re=o(" \u2014 "),KL=a("a"),uRe=o("BertGenerationConfig"),bRe=o(" (Bert Generation model)"),vRe=l(),If=a("li"),eW=a("strong"),TRe=o("big_bird"),FRe=o(" \u2014 "),ZL=a("a"),CRe=o("BigBirdConfig"),MRe=o(" (BigBird model)"),ERe=l(),jf=a("li"),oW=a("strong"),yRe=o("bigbird_pegasus"),wRe=o(" \u2014 "),e7=a("a"),ARe=o("BigBirdPegasusConfig"),LRe=o(" (BigBirdPegasus model)"),BRe=l(),Nf=a("li"),rW=a("strong"),kRe=o("blenderbot"),xRe=o(" \u2014 "),o7=a("a"),RRe=o("BlenderbotConfig"),SRe=o(" (Blenderbot model)"),PRe=l(),Df=a("li"),tW=a("strong"),$Re=o("blenderbot-small"),IRe=o(" \u2014 "),r7=a("a"),jRe=o("BlenderbotSmallConfig"),NRe=o(" (BlenderbotSmall model)"),DRe=l(),qf=a("li"),aW=a("strong"),qRe=o("camembert"),GRe=o(" \u2014 "),t7=a("a"),ORe=o("CamembertConfig"),XRe=o(" (CamemBERT model)"),zRe=l(),Gf=a("li"),nW=a("strong"),VRe=o("canine"),WRe=o(" \u2014 "),a7=a("a"),QRe=o("CanineConfig"),HRe=o(" (Canine model)"),URe=l(),Of=a("li"),sW=a("strong"),JRe=o("clip"),YRe=o(" \u2014 "),n7=a("a"),KRe=o("CLIPConfig"),ZRe=o(" (CLIP model)"),eSe=l(),Xf=a("li"),lW=a("strong"),oSe=o("convbert"),rSe=o(" \u2014 "),s7=a("a"),tSe=o("ConvBertConfig"),aSe=o(" (ConvBERT model)"),nSe=l(),zf=a("li"),iW=a("strong"),sSe=o("convnext"),lSe=o(" \u2014 "),l7=a("a"),iSe=o("ConvNextConfig"),dSe=o(" (ConvNext model)"),cSe=l(),Vf=a("li"),dW=a("strong"),fSe=o("ctrl"),mSe=o(" \u2014 "),i7=a("a"),gSe=o("CTRLConfig"),hSe=o(" (CTRL model)"),pSe=l(),Wf=a("li"),cW=a("strong"),_Se=o("deberta"),uSe=o(" \u2014 "),d7=a("a"),bSe=o("DebertaConfig"),vSe=o(" (DeBERTa model)"),TSe=l(),Qf=a("li"),fW=a("strong"),FSe=o("deberta-v2"),CSe=o(" \u2014 "),c7=a("a"),MSe=o("DebertaV2Config"),ESe=o(" (DeBERTa-v2 model)"),ySe=l(),Hf=a("li"),mW=a("strong"),wSe=o("deit"),ASe=o(" \u2014 "),f7=a("a"),LSe=o("DeiTConfig"),BSe=o(" (DeiT model)"),kSe=l(),Uf=a("li"),gW=a("strong"),xSe=o("detr"),RSe=o(" \u2014 "),m7=a("a"),SSe=o("DetrConfig"),PSe=o(" (DETR model)"),$Se=l(),Jf=a("li"),hW=a("strong"),ISe=o("distilbert"),jSe=o(" \u2014 "),g7=a("a"),NSe=o("DistilBertConfig"),DSe=o(" (DistilBERT model)"),qSe=l(),Yf=a("li"),pW=a("strong"),GSe=o("dpr"),OSe=o(" \u2014 "),h7=a("a"),XSe=o("DPRConfig"),zSe=o(" (DPR model)"),VSe=l(),Kf=a("li"),_W=a("strong"),WSe=o("electra"),QSe=o(" \u2014 "),p7=a("a"),HSe=o("ElectraConfig"),USe=o(" (ELECTRA model)"),JSe=l(),Zf=a("li"),uW=a("strong"),YSe=o("encoder-decoder"),KSe=o(" \u2014 "),_7=a("a"),ZSe=o("EncoderDecoderConfig"),ePe=o(" (Encoder decoder model)"),oPe=l(),em=a("li"),bW=a("strong"),rPe=o("flaubert"),tPe=o(" \u2014 "),u7=a("a"),aPe=o("FlaubertConfig"),nPe=o(" (FlauBERT model)"),sPe=l(),om=a("li"),vW=a("strong"),lPe=o("fnet"),iPe=o(" \u2014 "),b7=a("a"),dPe=o("FNetConfig"),cPe=o(" (FNet model)"),fPe=l(),rm=a("li"),TW=a("strong"),mPe=o("fsmt"),gPe=o(" \u2014 "),v7=a("a"),hPe=o("FSMTConfig"),pPe=o(" (FairSeq Machine-Translation model)"),_Pe=l(),tm=a("li"),FW=a("strong"),uPe=o("funnel"),bPe=o(" \u2014 "),T7=a("a"),vPe=o("FunnelConfig"),TPe=o(" (Funnel Transformer model)"),FPe=l(),am=a("li"),CW=a("strong"),CPe=o("gpt2"),MPe=o(" \u2014 "),F7=a("a"),EPe=o("GPT2Config"),yPe=o(" (OpenAI GPT-2 model)"),wPe=l(),nm=a("li"),MW=a("strong"),APe=o("gpt_neo"),LPe=o(" \u2014 "),C7=a("a"),BPe=o("GPTNeoConfig"),kPe=o(" (GPT Neo model)"),xPe=l(),sm=a("li"),EW=a("strong"),RPe=o("gptj"),SPe=o(" \u2014 "),M7=a("a"),PPe=o("GPTJConfig"),$Pe=o(" (GPT-J model)"),IPe=l(),lm=a("li"),yW=a("strong"),jPe=o("hubert"),NPe=o(" \u2014 "),E7=a("a"),DPe=o("HubertConfig"),qPe=o(" (Hubert model)"),GPe=l(),im=a("li"),wW=a("strong"),OPe=o("ibert"),XPe=o(" \u2014 "),y7=a("a"),zPe=o("IBertConfig"),VPe=o(" (I-BERT model)"),WPe=l(),dm=a("li"),AW=a("strong"),QPe=o("imagegpt"),HPe=o(" \u2014 "),w7=a("a"),UPe=o("ImageGPTConfig"),JPe=o(" (ImageGPT model)"),YPe=l(),cm=a("li"),LW=a("strong"),KPe=o("layoutlm"),ZPe=o(" \u2014 "),A7=a("a"),e$e=o("LayoutLMConfig"),o$e=o(" (LayoutLM model)"),r$e=l(),fm=a("li"),BW=a("strong"),t$e=o("layoutlmv2"),a$e=o(" \u2014 "),L7=a("a"),n$e=o("LayoutLMv2Config"),s$e=o(" (LayoutLMv2 model)"),l$e=l(),mm=a("li"),kW=a("strong"),i$e=o("led"),d$e=o(" \u2014 "),B7=a("a"),c$e=o("LEDConfig"),f$e=o(" (LED model)"),m$e=l(),gm=a("li"),xW=a("strong"),g$e=o("longformer"),h$e=o(" \u2014 "),k7=a("a"),p$e=o("LongformerConfig"),_$e=o(" (Longformer model)"),u$e=l(),hm=a("li"),RW=a("strong"),b$e=o("luke"),v$e=o(" \u2014 "),x7=a("a"),T$e=o("LukeConfig"),F$e=o(" (LUKE model)"),C$e=l(),pm=a("li"),SW=a("strong"),M$e=o("lxmert"),E$e=o(" \u2014 "),R7=a("a"),y$e=o("LxmertConfig"),w$e=o(" (LXMERT model)"),A$e=l(),_m=a("li"),PW=a("strong"),L$e=o("m2m_100"),B$e=o(" \u2014 "),S7=a("a"),k$e=o("M2M100Config"),x$e=o(" (M2M100 model)"),R$e=l(),um=a("li"),$W=a("strong"),S$e=o("marian"),P$e=o(" \u2014 "),P7=a("a"),$$e=o("MarianConfig"),I$e=o(" (Marian model)"),j$e=l(),bm=a("li"),IW=a("strong"),N$e=o("maskformer"),D$e=o(" \u2014 "),$7=a("a"),q$e=o("MaskFormerConfig"),G$e=o(" (MaskFormer model)"),O$e=l(),vm=a("li"),jW=a("strong"),X$e=o("mbart"),z$e=o(" \u2014 "),I7=a("a"),V$e=o("MBartConfig"),W$e=o(" (mBART model)"),Q$e=l(),Tm=a("li"),NW=a("strong"),H$e=o("megatron-bert"),U$e=o(" \u2014 "),j7=a("a"),J$e=o("MegatronBertConfig"),Y$e=o(" (MegatronBert model)"),K$e=l(),Fm=a("li"),DW=a("strong"),Z$e=o("mobilebert"),eIe=o(" \u2014 "),N7=a("a"),oIe=o("MobileBertConfig"),rIe=o(" (MobileBERT model)"),tIe=l(),Cm=a("li"),qW=a("strong"),aIe=o("mpnet"),nIe=o(" \u2014 "),D7=a("a"),sIe=o("MPNetConfig"),lIe=o(" (MPNet model)"),iIe=l(),Mm=a("li"),GW=a("strong"),dIe=o("mt5"),cIe=o(" \u2014 "),q7=a("a"),fIe=o("MT5Config"),mIe=o(" (mT5 model)"),gIe=l(),Em=a("li"),OW=a("strong"),hIe=o("nystromformer"),pIe=o(" \u2014 "),G7=a("a"),_Ie=o("NystromformerConfig"),uIe=o(" (Nystromformer model)"),bIe=l(),ym=a("li"),XW=a("strong"),vIe=o("openai-gpt"),TIe=o(" \u2014 "),O7=a("a"),FIe=o("OpenAIGPTConfig"),CIe=o(" (OpenAI GPT model)"),MIe=l(),wm=a("li"),zW=a("strong"),EIe=o("pegasus"),yIe=o(" \u2014 "),X7=a("a"),wIe=o("PegasusConfig"),AIe=o(" (Pegasus model)"),LIe=l(),Am=a("li"),VW=a("strong"),BIe=o("perceiver"),kIe=o(" \u2014 "),z7=a("a"),xIe=o("PerceiverConfig"),RIe=o(" (Perceiver model)"),SIe=l(),Lm=a("li"),WW=a("strong"),PIe=o("plbart"),$Ie=o(" \u2014 "),V7=a("a"),IIe=o("PLBartConfig"),jIe=o(" (PLBart model)"),NIe=l(),Bm=a("li"),QW=a("strong"),DIe=o("poolformer"),qIe=o(" \u2014 "),W7=a("a"),GIe=o("PoolFormerConfig"),OIe=o(" (PoolFormer model)"),XIe=l(),km=a("li"),HW=a("strong"),zIe=o("prophetnet"),VIe=o(" \u2014 "),Q7=a("a"),WIe=o("ProphetNetConfig"),QIe=o(" (ProphetNet model)"),HIe=l(),xm=a("li"),UW=a("strong"),UIe=o("qdqbert"),JIe=o(" \u2014 "),H7=a("a"),YIe=o("QDQBertConfig"),KIe=o(" (QDQBert model)"),ZIe=l(),Rm=a("li"),JW=a("strong"),eje=o("rag"),oje=o(" \u2014 "),U7=a("a"),rje=o("RagConfig"),tje=o(" (RAG model)"),aje=l(),Sm=a("li"),YW=a("strong"),nje=o("realm"),sje=o(" \u2014 "),J7=a("a"),lje=o("RealmConfig"),ije=o(" (Realm model)"),dje=l(),Pm=a("li"),KW=a("strong"),cje=o("reformer"),fje=o(" \u2014 "),Y7=a("a"),mje=o("ReformerConfig"),gje=o(" (Reformer model)"),hje=l(),$m=a("li"),ZW=a("strong"),pje=o("rembert"),_je=o(" \u2014 "),K7=a("a"),uje=o("RemBertConfig"),bje=o(" (RemBERT model)"),vje=l(),Im=a("li"),eQ=a("strong"),Tje=o("retribert"),Fje=o(" \u2014 "),Z7=a("a"),Cje=o("RetriBertConfig"),Mje=o(" (RetriBERT model)"),Eje=l(),jm=a("li"),oQ=a("strong"),yje=o("roberta"),wje=o(" \u2014 "),e9=a("a"),Aje=o("RobertaConfig"),Lje=o(" (RoBERTa model)"),Bje=l(),Nm=a("li"),rQ=a("strong"),kje=o("roformer"),xje=o(" \u2014 "),o9=a("a"),Rje=o("RoFormerConfig"),Sje=o(" (RoFormer model)"),Pje=l(),Dm=a("li"),tQ=a("strong"),$je=o("segformer"),Ije=o(" \u2014 "),r9=a("a"),jje=o("SegformerConfig"),Nje=o(" (SegFormer model)"),Dje=l(),qm=a("li"),aQ=a("strong"),qje=o("sew"),Gje=o(" \u2014 "),t9=a("a"),Oje=o("SEWConfig"),Xje=o(" (SEW model)"),zje=l(),Gm=a("li"),nQ=a("strong"),Vje=o("sew-d"),Wje=o(" \u2014 "),a9=a("a"),Qje=o("SEWDConfig"),Hje=o(" (SEW-D model)"),Uje=l(),Om=a("li"),sQ=a("strong"),Jje=o("speech-encoder-decoder"),Yje=o(" \u2014 "),n9=a("a"),Kje=o("SpeechEncoderDecoderConfig"),Zje=o(" (Speech Encoder decoder model)"),eNe=l(),Xm=a("li"),lQ=a("strong"),oNe=o("speech_to_text"),rNe=o(" \u2014 "),s9=a("a"),tNe=o("Speech2TextConfig"),aNe=o(" (Speech2Text model)"),nNe=l(),zm=a("li"),iQ=a("strong"),sNe=o("speech_to_text_2"),lNe=o(" \u2014 "),l9=a("a"),iNe=o("Speech2Text2Config"),dNe=o(" (Speech2Text2 model)"),cNe=l(),Vm=a("li"),dQ=a("strong"),fNe=o("splinter"),mNe=o(" \u2014 "),i9=a("a"),gNe=o("SplinterConfig"),hNe=o(" (Splinter model)"),pNe=l(),Wm=a("li"),cQ=a("strong"),_Ne=o("squeezebert"),uNe=o(" \u2014 "),d9=a("a"),bNe=o("SqueezeBertConfig"),vNe=o(" (SqueezeBERT model)"),TNe=l(),Qm=a("li"),fQ=a("strong"),FNe=o("swin"),CNe=o(" \u2014 "),c9=a("a"),MNe=o("SwinConfig"),ENe=o(" (Swin model)"),yNe=l(),Hm=a("li"),mQ=a("strong"),wNe=o("t5"),ANe=o(" \u2014 "),f9=a("a"),LNe=o("T5Config"),BNe=o(" (T5 model)"),kNe=l(),Um=a("li"),gQ=a("strong"),xNe=o("tapas"),RNe=o(" \u2014 "),m9=a("a"),SNe=o("TapasConfig"),PNe=o(" (TAPAS model)"),$Ne=l(),Jm=a("li"),hQ=a("strong"),INe=o("transfo-xl"),jNe=o(" \u2014 "),g9=a("a"),NNe=o("TransfoXLConfig"),DNe=o(" (Transformer-XL model)"),qNe=l(),Ym=a("li"),pQ=a("strong"),GNe=o("trocr"),ONe=o(" \u2014 "),h9=a("a"),XNe=o("TrOCRConfig"),zNe=o(" (TrOCR model)"),VNe=l(),Km=a("li"),_Q=a("strong"),WNe=o("unispeech"),QNe=o(" \u2014 "),p9=a("a"),HNe=o("UniSpeechConfig"),UNe=o(" (UniSpeech model)"),JNe=l(),Zm=a("li"),uQ=a("strong"),YNe=o("unispeech-sat"),KNe=o(" \u2014 "),_9=a("a"),ZNe=o("UniSpeechSatConfig"),eDe=o(" (UniSpeechSat model)"),oDe=l(),eg=a("li"),bQ=a("strong"),rDe=o("vilt"),tDe=o(" \u2014 "),u9=a("a"),aDe=o("ViltConfig"),nDe=o(" (ViLT model)"),sDe=l(),og=a("li"),vQ=a("strong"),lDe=o("vision-encoder-decoder"),iDe=o(" \u2014 "),b9=a("a"),dDe=o("VisionEncoderDecoderConfig"),cDe=o(" (Vision Encoder decoder model)"),fDe=l(),rg=a("li"),TQ=a("strong"),mDe=o("vision-text-dual-encoder"),gDe=o(" \u2014 "),v9=a("a"),hDe=o("VisionTextDualEncoderConfig"),pDe=o(" (VisionTextDualEncoder model)"),_De=l(),tg=a("li"),FQ=a("strong"),uDe=o("visual_bert"),bDe=o(" \u2014 "),T9=a("a"),vDe=o("VisualBertConfig"),TDe=o(" (VisualBert model)"),FDe=l(),ag=a("li"),CQ=a("strong"),CDe=o("vit"),MDe=o(" \u2014 "),F9=a("a"),EDe=o("ViTConfig"),yDe=o(" (ViT model)"),wDe=l(),ng=a("li"),MQ=a("strong"),ADe=o("vit_mae"),LDe=o(" \u2014 "),C9=a("a"),BDe=o("ViTMAEConfig"),kDe=o(" (ViTMAE model)"),xDe=l(),sg=a("li"),EQ=a("strong"),RDe=o("wav2vec2"),SDe=o(" \u2014 "),M9=a("a"),PDe=o("Wav2Vec2Config"),$De=o(" (Wav2Vec2 model)"),IDe=l(),lg=a("li"),yQ=a("strong"),jDe=o("wavlm"),NDe=o(" \u2014 "),E9=a("a"),DDe=o("WavLMConfig"),qDe=o(" (WavLM model)"),GDe=l(),ig=a("li"),wQ=a("strong"),ODe=o("xglm"),XDe=o(" \u2014 "),y9=a("a"),zDe=o("XGLMConfig"),VDe=o(" (XGLM model)"),WDe=l(),dg=a("li"),AQ=a("strong"),QDe=o("xlm"),HDe=o(" \u2014 "),w9=a("a"),UDe=o("XLMConfig"),JDe=o(" (XLM model)"),YDe=l(),cg=a("li"),LQ=a("strong"),KDe=o("xlm-prophetnet"),ZDe=o(" \u2014 "),A9=a("a"),eqe=o("XLMProphetNetConfig"),oqe=o(" (XLMProphetNet model)"),rqe=l(),fg=a("li"),BQ=a("strong"),tqe=o("xlm-roberta"),aqe=o(" \u2014 "),L9=a("a"),nqe=o("XLMRobertaConfig"),sqe=o(" (XLM-RoBERTa model)"),lqe=l(),mg=a("li"),kQ=a("strong"),iqe=o("xlm-roberta-xl"),dqe=o(" \u2014 "),B9=a("a"),cqe=o("XLMRobertaXLConfig"),fqe=o(" (XLM-RoBERTa-XL model)"),mqe=l(),gg=a("li"),xQ=a("strong"),gqe=o("xlnet"),hqe=o(" \u2014 "),k9=a("a"),pqe=o("XLNetConfig"),_qe=o(" (XLNet model)"),uqe=l(),hg=a("li"),RQ=a("strong"),bqe=o("yoso"),vqe=o(" \u2014 "),x9=a("a"),Tqe=o("YosoConfig"),Fqe=o(" (YOSO model)"),Cqe=l(),SQ=a("p"),Mqe=o("Examples:"),Eqe=l(),f(hM.$$.fragment),yqe=l(),pg=a("div"),f(pM.$$.fragment),wqe=l(),PQ=a("p"),Aqe=o("Register a new configuration for this class."),b7e=l(),Ii=a("h2"),_g=a("a"),$Q=a("span"),f(_M.$$.fragment),Lqe=l(),IQ=a("span"),Bqe=o("AutoTokenizer"),v7e=l(),Oo=a("div"),f(uM.$$.fragment),kqe=l(),bM=a("p"),xqe=o(`This is a generic tokenizer class that will be instantiated as one of the tokenizer classes of the library when
created with the `),R9=a("a"),Rqe=o("AutoTokenizer.from_pretrained()"),Sqe=o(" class method."),Pqe=l(),vM=a("p"),$qe=o("This class cannot be instantiated directly using "),jQ=a("code"),Iqe=o("__init__()"),jqe=o(" (throws an error)."),Nqe=l(),mo=a("div"),f(TM.$$.fragment),Dqe=l(),NQ=a("p"),qqe=o("Instantiate one of the tokenizer classes of the library from a pretrained model vocabulary."),Gqe=l(),ja=a("p"),Oqe=o("The tokenizer class to instantiate is selected based on the "),DQ=a("code"),Xqe=o("model_type"),zqe=o(` property of the config object (either
passed as an argument or loaded from `),qQ=a("code"),Vqe=o("pretrained_model_name_or_path"),Wqe=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),GQ=a("code"),Qqe=o("pretrained_model_name_or_path"),Hqe=o(":"),Uqe=l(),M=a("ul"),Dn=a("li"),OQ=a("strong"),Jqe=o("albert"),Yqe=o(" \u2014 "),S9=a("a"),Kqe=o("AlbertTokenizer"),Zqe=o(" or "),P9=a("a"),eGe=o("AlbertTokenizerFast"),oGe=o(" (ALBERT model)"),rGe=l(),qn=a("li"),XQ=a("strong"),tGe=o("bart"),aGe=o(" \u2014 "),$9=a("a"),nGe=o("BartTokenizer"),sGe=o(" or "),I9=a("a"),lGe=o("BartTokenizerFast"),iGe=o(" (BART model)"),dGe=l(),Gn=a("li"),zQ=a("strong"),cGe=o("barthez"),fGe=o(" \u2014 "),j9=a("a"),mGe=o("BarthezTokenizer"),gGe=o(" or "),N9=a("a"),hGe=o("BarthezTokenizerFast"),pGe=o(" (BARThez model)"),_Ge=l(),ug=a("li"),VQ=a("strong"),uGe=o("bartpho"),bGe=o(" \u2014 "),D9=a("a"),vGe=o("BartphoTokenizer"),TGe=o(" (BARTpho model)"),FGe=l(),On=a("li"),WQ=a("strong"),CGe=o("bert"),MGe=o(" \u2014 "),q9=a("a"),EGe=o("BertTokenizer"),yGe=o(" or "),G9=a("a"),wGe=o("BertTokenizerFast"),AGe=o(" (BERT model)"),LGe=l(),bg=a("li"),QQ=a("strong"),BGe=o("bert-generation"),kGe=o(" \u2014 "),O9=a("a"),xGe=o("BertGenerationTokenizer"),RGe=o(" (Bert Generation model)"),SGe=l(),vg=a("li"),HQ=a("strong"),PGe=o("bert-japanese"),$Ge=o(" \u2014 "),X9=a("a"),IGe=o("BertJapaneseTokenizer"),jGe=o(" (BertJapanese model)"),NGe=l(),Tg=a("li"),UQ=a("strong"),DGe=o("bertweet"),qGe=o(" \u2014 "),z9=a("a"),GGe=o("BertweetTokenizer"),OGe=o(" (Bertweet model)"),XGe=l(),Xn=a("li"),JQ=a("strong"),zGe=o("big_bird"),VGe=o(" \u2014 "),V9=a("a"),WGe=o("BigBirdTokenizer"),QGe=o(" or "),W9=a("a"),HGe=o("BigBirdTokenizerFast"),UGe=o(" (BigBird model)"),JGe=l(),zn=a("li"),YQ=a("strong"),YGe=o("bigbird_pegasus"),KGe=o(" \u2014 "),Q9=a("a"),ZGe=o("PegasusTokenizer"),eOe=o(" or "),H9=a("a"),oOe=o("PegasusTokenizerFast"),rOe=o(" (BigBirdPegasus model)"),tOe=l(),Vn=a("li"),KQ=a("strong"),aOe=o("blenderbot"),nOe=o(" \u2014 "),U9=a("a"),sOe=o("BlenderbotTokenizer"),lOe=o(" or "),J9=a("a"),iOe=o("BlenderbotTokenizerFast"),dOe=o(" (Blenderbot model)"),cOe=l(),Fg=a("li"),ZQ=a("strong"),fOe=o("blenderbot-small"),mOe=o(" \u2014 "),Y9=a("a"),gOe=o("BlenderbotSmallTokenizer"),hOe=o(" (BlenderbotSmall model)"),pOe=l(),Cg=a("li"),eH=a("strong"),_Oe=o("byt5"),uOe=o(" \u2014 "),K9=a("a"),bOe=o("ByT5Tokenizer"),vOe=o(" (ByT5 model)"),TOe=l(),Wn=a("li"),oH=a("strong"),FOe=o("camembert"),COe=o(" \u2014 "),Z9=a("a"),MOe=o("CamembertTokenizer"),EOe=o(" or "),eB=a("a"),yOe=o("CamembertTokenizerFast"),wOe=o(" (CamemBERT model)"),AOe=l(),Mg=a("li"),rH=a("strong"),LOe=o("canine"),BOe=o(" \u2014 "),oB=a("a"),kOe=o("CanineTokenizer"),xOe=o(" (Canine model)"),ROe=l(),Qn=a("li"),tH=a("strong"),SOe=o("clip"),POe=o(" \u2014 "),rB=a("a"),$Oe=o("CLIPTokenizer"),IOe=o(" or "),tB=a("a"),jOe=o("CLIPTokenizerFast"),NOe=o(" (CLIP model)"),DOe=l(),Hn=a("li"),aH=a("strong"),qOe=o("convbert"),GOe=o(" \u2014 "),aB=a("a"),OOe=o("ConvBertTokenizer"),XOe=o(" or "),nB=a("a"),zOe=o("ConvBertTokenizerFast"),VOe=o(" (ConvBERT model)"),WOe=l(),Un=a("li"),nH=a("strong"),QOe=o("cpm"),HOe=o(" \u2014 "),sB=a("a"),UOe=o("CpmTokenizer"),JOe=o(" or "),sH=a("code"),YOe=o("CpmTokenizerFast"),KOe=o(" (CPM model)"),ZOe=l(),Eg=a("li"),lH=a("strong"),eXe=o("ctrl"),oXe=o(" \u2014 "),lB=a("a"),rXe=o("CTRLTokenizer"),tXe=o(" (CTRL model)"),aXe=l(),Jn=a("li"),iH=a("strong"),nXe=o("deberta"),sXe=o(" \u2014 "),iB=a("a"),lXe=o("DebertaTokenizer"),iXe=o(" or "),dB=a("a"),dXe=o("DebertaTokenizerFast"),cXe=o(" (DeBERTa model)"),fXe=l(),yg=a("li"),dH=a("strong"),mXe=o("deberta-v2"),gXe=o(" \u2014 "),cB=a("a"),hXe=o("DebertaV2Tokenizer"),pXe=o(" (DeBERTa-v2 model)"),_Xe=l(),Yn=a("li"),cH=a("strong"),uXe=o("distilbert"),bXe=o(" \u2014 "),fB=a("a"),vXe=o("DistilBertTokenizer"),TXe=o(" or "),mB=a("a"),FXe=o("DistilBertTokenizerFast"),CXe=o(" (DistilBERT model)"),MXe=l(),Kn=a("li"),fH=a("strong"),EXe=o("dpr"),yXe=o(" \u2014 "),gB=a("a"),wXe=o("DPRQuestionEncoderTokenizer"),AXe=o(" or "),hB=a("a"),LXe=o("DPRQuestionEncoderTokenizerFast"),BXe=o(" (DPR model)"),kXe=l(),Zn=a("li"),mH=a("strong"),xXe=o("electra"),RXe=o(" \u2014 "),pB=a("a"),SXe=o("ElectraTokenizer"),PXe=o(" or "),_B=a("a"),$Xe=o("ElectraTokenizerFast"),IXe=o(" (ELECTRA model)"),jXe=l(),wg=a("li"),gH=a("strong"),NXe=o("flaubert"),DXe=o(" \u2014 "),uB=a("a"),qXe=o("FlaubertTokenizer"),GXe=o(" (FlauBERT model)"),OXe=l(),es=a("li"),hH=a("strong"),XXe=o("fnet"),zXe=o(" \u2014 "),bB=a("a"),VXe=o("FNetTokenizer"),WXe=o(" or "),vB=a("a"),QXe=o("FNetTokenizerFast"),HXe=o(" (FNet model)"),UXe=l(),Ag=a("li"),pH=a("strong"),JXe=o("fsmt"),YXe=o(" \u2014 "),TB=a("a"),KXe=o("FSMTTokenizer"),ZXe=o(" (FairSeq Machine-Translation model)"),eze=l(),os=a("li"),_H=a("strong"),oze=o("funnel"),rze=o(" \u2014 "),FB=a("a"),tze=o("FunnelTokenizer"),aze=o(" or "),CB=a("a"),nze=o("FunnelTokenizerFast"),sze=o(" (Funnel Transformer model)"),lze=l(),rs=a("li"),uH=a("strong"),ize=o("gpt2"),dze=o(" \u2014 "),MB=a("a"),cze=o("GPT2Tokenizer"),fze=o(" or "),EB=a("a"),mze=o("GPT2TokenizerFast"),gze=o(" (OpenAI GPT-2 model)"),hze=l(),ts=a("li"),bH=a("strong"),pze=o("gpt_neo"),_ze=o(" \u2014 "),yB=a("a"),uze=o("GPT2Tokenizer"),bze=o(" or "),wB=a("a"),vze=o("GPT2TokenizerFast"),Tze=o(" (GPT Neo model)"),Fze=l(),as=a("li"),vH=a("strong"),Cze=o("herbert"),Mze=o(" \u2014 "),AB=a("a"),Eze=o("HerbertTokenizer"),yze=o(" or "),LB=a("a"),wze=o("HerbertTokenizerFast"),Aze=o(" (HerBERT model)"),Lze=l(),Lg=a("li"),TH=a("strong"),Bze=o("hubert"),kze=o(" \u2014 "),BB=a("a"),xze=o("Wav2Vec2CTCTokenizer"),Rze=o(" (Hubert model)"),Sze=l(),ns=a("li"),FH=a("strong"),Pze=o("ibert"),$ze=o(" \u2014 "),kB=a("a"),Ize=o("RobertaTokenizer"),jze=o(" or "),xB=a("a"),Nze=o("RobertaTokenizerFast"),Dze=o(" (I-BERT model)"),qze=l(),ss=a("li"),CH=a("strong"),Gze=o("layoutlm"),Oze=o(" \u2014 "),RB=a("a"),Xze=o("LayoutLMTokenizer"),zze=o(" or "),SB=a("a"),Vze=o("LayoutLMTokenizerFast"),Wze=o(" (LayoutLM model)"),Qze=l(),ls=a("li"),MH=a("strong"),Hze=o("layoutlmv2"),Uze=o(" \u2014 "),PB=a("a"),Jze=o("LayoutLMv2Tokenizer"),Yze=o(" or "),$B=a("a"),Kze=o("LayoutLMv2TokenizerFast"),Zze=o(" (LayoutLMv2 model)"),eVe=l(),is=a("li"),EH=a("strong"),oVe=o("layoutxlm"),rVe=o(" \u2014 "),IB=a("a"),tVe=o("LayoutXLMTokenizer"),aVe=o(" or "),jB=a("a"),nVe=o("LayoutXLMTokenizerFast"),sVe=o(" (LayoutXLM model)"),lVe=l(),ds=a("li"),yH=a("strong"),iVe=o("led"),dVe=o(" \u2014 "),NB=a("a"),cVe=o("LEDTokenizer"),fVe=o(" or "),DB=a("a"),mVe=o("LEDTokenizerFast"),gVe=o(" (LED model)"),hVe=l(),cs=a("li"),wH=a("strong"),pVe=o("longformer"),_Ve=o(" \u2014 "),qB=a("a"),uVe=o("LongformerTokenizer"),bVe=o(" or "),GB=a("a"),vVe=o("LongformerTokenizerFast"),TVe=o(" (Longformer model)"),FVe=l(),Bg=a("li"),AH=a("strong"),CVe=o("luke"),MVe=o(" \u2014 "),OB=a("a"),EVe=o("LukeTokenizer"),yVe=o(" (LUKE model)"),wVe=l(),fs=a("li"),LH=a("strong"),AVe=o("lxmert"),LVe=o(" \u2014 "),XB=a("a"),BVe=o("LxmertTokenizer"),kVe=o(" or "),zB=a("a"),xVe=o("LxmertTokenizerFast"),RVe=o(" (LXMERT model)"),SVe=l(),kg=a("li"),BH=a("strong"),PVe=o("m2m_100"),$Ve=o(" \u2014 "),VB=a("a"),IVe=o("M2M100Tokenizer"),jVe=o(" (M2M100 model)"),NVe=l(),xg=a("li"),kH=a("strong"),DVe=o("marian"),qVe=o(" \u2014 "),WB=a("a"),GVe=o("MarianTokenizer"),OVe=o(" (Marian model)"),XVe=l(),ms=a("li"),xH=a("strong"),zVe=o("mbart"),VVe=o(" \u2014 "),QB=a("a"),WVe=o("MBartTokenizer"),QVe=o(" or "),HB=a("a"),HVe=o("MBartTokenizerFast"),UVe=o(" (mBART model)"),JVe=l(),gs=a("li"),RH=a("strong"),YVe=o("mbart50"),KVe=o(" \u2014 "),UB=a("a"),ZVe=o("MBart50Tokenizer"),eWe=o(" or "),JB=a("a"),oWe=o("MBart50TokenizerFast"),rWe=o(" (mBART-50 model)"),tWe=l(),Rg=a("li"),SH=a("strong"),aWe=o("mluke"),nWe=o(" \u2014 "),YB=a("a"),sWe=o("MLukeTokenizer"),lWe=o(" (mLUKE model)"),iWe=l(),hs=a("li"),PH=a("strong"),dWe=o("mobilebert"),cWe=o(" \u2014 "),KB=a("a"),fWe=o("MobileBertTokenizer"),mWe=o(" or "),ZB=a("a"),gWe=o("MobileBertTokenizerFast"),hWe=o(" (MobileBERT model)"),pWe=l(),ps=a("li"),$H=a("strong"),_We=o("mpnet"),uWe=o(" \u2014 "),ek=a("a"),bWe=o("MPNetTokenizer"),vWe=o(" or "),ok=a("a"),TWe=o("MPNetTokenizerFast"),FWe=o(" (MPNet model)"),CWe=l(),_s=a("li"),IH=a("strong"),MWe=o("mt5"),EWe=o(" \u2014 "),rk=a("a"),yWe=o("MT5Tokenizer"),wWe=o(" or "),tk=a("a"),AWe=o("MT5TokenizerFast"),LWe=o(" (mT5 model)"),BWe=l(),us=a("li"),jH=a("strong"),kWe=o("openai-gpt"),xWe=o(" \u2014 "),ak=a("a"),RWe=o("OpenAIGPTTokenizer"),SWe=o(" or "),nk=a("a"),PWe=o("OpenAIGPTTokenizerFast"),$We=o(" (OpenAI GPT model)"),IWe=l(),bs=a("li"),NH=a("strong"),jWe=o("pegasus"),NWe=o(" \u2014 "),sk=a("a"),DWe=o("PegasusTokenizer"),qWe=o(" or "),lk=a("a"),GWe=o("PegasusTokenizerFast"),OWe=o(" (Pegasus model)"),XWe=l(),Sg=a("li"),DH=a("strong"),zWe=o("perceiver"),VWe=o(" \u2014 "),ik=a("a"),WWe=o("PerceiverTokenizer"),QWe=o(" (Perceiver model)"),HWe=l(),Pg=a("li"),qH=a("strong"),UWe=o("phobert"),JWe=o(" \u2014 "),dk=a("a"),YWe=o("PhobertTokenizer"),KWe=o(" (PhoBERT model)"),ZWe=l(),$g=a("li"),GH=a("strong"),eQe=o("plbart"),oQe=o(" \u2014 "),ck=a("a"),rQe=o("PLBartTokenizer"),tQe=o(" (PLBart model)"),aQe=l(),Ig=a("li"),OH=a("strong"),nQe=o("prophetnet"),sQe=o(" \u2014 "),fk=a("a"),lQe=o("ProphetNetTokenizer"),iQe=o(" (ProphetNet model)"),dQe=l(),vs=a("li"),XH=a("strong"),cQe=o("qdqbert"),fQe=o(" \u2014 "),mk=a("a"),mQe=o("BertTokenizer"),gQe=o(" or "),gk=a("a"),hQe=o("BertTokenizerFast"),pQe=o(" (QDQBert model)"),_Qe=l(),jg=a("li"),zH=a("strong"),uQe=o("rag"),bQe=o(" \u2014 "),hk=a("a"),vQe=o("RagTokenizer"),TQe=o(" (RAG model)"),FQe=l(),Ts=a("li"),VH=a("strong"),CQe=o("reformer"),MQe=o(" \u2014 "),pk=a("a"),EQe=o("ReformerTokenizer"),yQe=o(" or "),_k=a("a"),wQe=o("ReformerTokenizerFast"),AQe=o(" (Reformer model)"),LQe=l(),Fs=a("li"),WH=a("strong"),BQe=o("rembert"),kQe=o(" \u2014 "),uk=a("a"),xQe=o("RemBertTokenizer"),RQe=o(" or "),bk=a("a"),SQe=o("RemBertTokenizerFast"),PQe=o(" (RemBERT model)"),$Qe=l(),Cs=a("li"),QH=a("strong"),IQe=o("retribert"),jQe=o(" \u2014 "),vk=a("a"),NQe=o("RetriBertTokenizer"),DQe=o(" or "),Tk=a("a"),qQe=o("RetriBertTokenizerFast"),GQe=o(" (RetriBERT model)"),OQe=l(),Ms=a("li"),HH=a("strong"),XQe=o("roberta"),zQe=o(" \u2014 "),Fk=a("a"),VQe=o("RobertaTokenizer"),WQe=o(" or "),Ck=a("a"),QQe=o("RobertaTokenizerFast"),HQe=o(" (RoBERTa model)"),UQe=l(),Es=a("li"),UH=a("strong"),JQe=o("roformer"),YQe=o(" \u2014 "),Mk=a("a"),KQe=o("RoFormerTokenizer"),ZQe=o(" or "),Ek=a("a"),eHe=o("RoFormerTokenizerFast"),oHe=o(" (RoFormer model)"),rHe=l(),Ng=a("li"),JH=a("strong"),tHe=o("speech_to_text"),aHe=o(" \u2014 "),yk=a("a"),nHe=o("Speech2TextTokenizer"),sHe=o(" (Speech2Text model)"),lHe=l(),Dg=a("li"),YH=a("strong"),iHe=o("speech_to_text_2"),dHe=o(" \u2014 "),wk=a("a"),cHe=o("Speech2Text2Tokenizer"),fHe=o(" (Speech2Text2 model)"),mHe=l(),ys=a("li"),KH=a("strong"),gHe=o("splinter"),hHe=o(" \u2014 "),Ak=a("a"),pHe=o("SplinterTokenizer"),_He=o(" or "),Lk=a("a"),uHe=o("SplinterTokenizerFast"),bHe=o(" (Splinter model)"),vHe=l(),ws=a("li"),ZH=a("strong"),THe=o("squeezebert"),FHe=o(" \u2014 "),Bk=a("a"),CHe=o("SqueezeBertTokenizer"),MHe=o(" or "),kk=a("a"),EHe=o("SqueezeBertTokenizerFast"),yHe=o(" (SqueezeBERT model)"),wHe=l(),As=a("li"),eU=a("strong"),AHe=o("t5"),LHe=o(" \u2014 "),xk=a("a"),BHe=o("T5Tokenizer"),kHe=o(" or "),Rk=a("a"),xHe=o("T5TokenizerFast"),RHe=o(" (T5 model)"),SHe=l(),qg=a("li"),oU=a("strong"),PHe=o("tapas"),$He=o(" \u2014 "),Sk=a("a"),IHe=o("TapasTokenizer"),jHe=o(" (TAPAS model)"),NHe=l(),Gg=a("li"),rU=a("strong"),DHe=o("transfo-xl"),qHe=o(" \u2014 "),Pk=a("a"),GHe=o("TransfoXLTokenizer"),OHe=o(" (Transformer-XL model)"),XHe=l(),Og=a("li"),tU=a("strong"),zHe=o("wav2vec2"),VHe=o(" \u2014 "),$k=a("a"),WHe=o("Wav2Vec2CTCTokenizer"),QHe=o(" (Wav2Vec2 model)"),HHe=l(),Xg=a("li"),aU=a("strong"),UHe=o("wav2vec2_phoneme"),JHe=o(" \u2014 "),Ik=a("a"),YHe=o("Wav2Vec2PhonemeCTCTokenizer"),KHe=o(" (Wav2Vec2Phoneme model)"),ZHe=l(),Ls=a("li"),nU=a("strong"),eUe=o("xglm"),oUe=o(" \u2014 "),jk=a("a"),rUe=o("XGLMTokenizer"),tUe=o(" or "),Nk=a("a"),aUe=o("XGLMTokenizerFast"),nUe=o(" (XGLM model)"),sUe=l(),zg=a("li"),sU=a("strong"),lUe=o("xlm"),iUe=o(" \u2014 "),Dk=a("a"),dUe=o("XLMTokenizer"),cUe=o(" (XLM model)"),fUe=l(),Vg=a("li"),lU=a("strong"),mUe=o("xlm-prophetnet"),gUe=o(" \u2014 "),qk=a("a"),hUe=o("XLMProphetNetTokenizer"),pUe=o(" (XLMProphetNet model)"),_Ue=l(),Bs=a("li"),iU=a("strong"),uUe=o("xlm-roberta"),bUe=o(" \u2014 "),Gk=a("a"),vUe=o("XLMRobertaTokenizer"),TUe=o(" or "),Ok=a("a"),FUe=o("XLMRobertaTokenizerFast"),CUe=o(" (XLM-RoBERTa model)"),MUe=l(),ks=a("li"),dU=a("strong"),EUe=o("xlnet"),yUe=o(" \u2014 "),Xk=a("a"),wUe=o("XLNetTokenizer"),AUe=o(" or "),zk=a("a"),LUe=o("XLNetTokenizerFast"),BUe=o(" (XLNet model)"),kUe=l(),cU=a("p"),xUe=o("Examples:"),RUe=l(),f(FM.$$.fragment),SUe=l(),Wg=a("div"),f(CM.$$.fragment),PUe=l(),fU=a("p"),$Ue=o("Register a new tokenizer in this mapping."),T7e=l(),ji=a("h2"),Qg=a("a"),mU=a("span"),f(MM.$$.fragment),IUe=l(),gU=a("span"),jUe=o("AutoFeatureExtractor"),F7e=l(),Xo=a("div"),f(EM.$$.fragment),NUe=l(),yM=a("p"),DUe=o(`This is a generic feature extractor class that will be instantiated as one of the feature extractor classes of the
library when created with the `),Vk=a("a"),qUe=o("AutoFeatureExtractor.from_pretrained()"),GUe=o(" class method."),OUe=l(),wM=a("p"),XUe=o("This class cannot be instantiated directly using "),hU=a("code"),zUe=o("__init__()"),VUe=o(" (throws an error)."),WUe=l(),Le=a("div"),f(AM.$$.fragment),QUe=l(),pU=a("p"),HUe=o("Instantiate one of the feature extractor classes of the library from a pretrained model vocabulary."),UUe=l(),Na=a("p"),JUe=o("The feature extractor class to instantiate is selected based on the "),_U=a("code"),YUe=o("model_type"),KUe=o(` property of the config object
(either passed as an argument or loaded from `),uU=a("code"),ZUe=o("pretrained_model_name_or_path"),eJe=o(` if possible), or when it\u2019s
missing, by falling back to using pattern matching on `),bU=a("code"),oJe=o("pretrained_model_name_or_path"),rJe=o(":"),tJe=l(),se=a("ul"),Hg=a("li"),vU=a("strong"),aJe=o("beit"),nJe=o(" \u2014 "),Wk=a("a"),sJe=o("BeitFeatureExtractor"),lJe=o(" (BEiT model)"),iJe=l(),Ug=a("li"),TU=a("strong"),dJe=o("clip"),cJe=o(" \u2014 "),Qk=a("a"),fJe=o("CLIPFeatureExtractor"),mJe=o(" (CLIP model)"),gJe=l(),Jg=a("li"),FU=a("strong"),hJe=o("convnext"),pJe=o(" \u2014 "),Hk=a("a"),_Je=o("ConvNextFeatureExtractor"),uJe=o(" (ConvNext model)"),bJe=l(),Yg=a("li"),CU=a("strong"),vJe=o("deit"),TJe=o(" \u2014 "),Uk=a("a"),FJe=o("DeiTFeatureExtractor"),CJe=o(" (DeiT model)"),MJe=l(),Kg=a("li"),MU=a("strong"),EJe=o("detr"),yJe=o(" \u2014 "),Jk=a("a"),wJe=o("DetrFeatureExtractor"),AJe=o(" (DETR model)"),LJe=l(),Zg=a("li"),EU=a("strong"),BJe=o("hubert"),kJe=o(" \u2014 "),Yk=a("a"),xJe=o("Wav2Vec2FeatureExtractor"),RJe=o(" (Hubert model)"),SJe=l(),eh=a("li"),yU=a("strong"),PJe=o("layoutlmv2"),$Je=o(" \u2014 "),Kk=a("a"),IJe=o("LayoutLMv2FeatureExtractor"),jJe=o(" (LayoutLMv2 model)"),NJe=l(),oh=a("li"),wU=a("strong"),DJe=o("perceiver"),qJe=o(" \u2014 "),Zk=a("a"),GJe=o("PerceiverFeatureExtractor"),OJe=o(" (Perceiver model)"),XJe=l(),rh=a("li"),AU=a("strong"),zJe=o("poolformer"),VJe=o(" \u2014 "),ex=a("a"),WJe=o("PoolFormerFeatureExtractor"),QJe=o(" (PoolFormer model)"),HJe=l(),th=a("li"),LU=a("strong"),UJe=o("segformer"),JJe=o(" \u2014 "),ox=a("a"),YJe=o("SegformerFeatureExtractor"),KJe=o(" (SegFormer model)"),ZJe=l(),ah=a("li"),BU=a("strong"),eYe=o("speech_to_text"),oYe=o(" \u2014 "),rx=a("a"),rYe=o("Speech2TextFeatureExtractor"),tYe=o(" (Speech2Text model)"),aYe=l(),nh=a("li"),kU=a("strong"),nYe=o("swin"),sYe=o(" \u2014 "),tx=a("a"),lYe=o("ViTFeatureExtractor"),iYe=o(" (Swin model)"),dYe=l(),sh=a("li"),xU=a("strong"),cYe=o("vit"),fYe=o(" \u2014 "),ax=a("a"),mYe=o("ViTFeatureExtractor"),gYe=o(" (ViT model)"),hYe=l(),lh=a("li"),RU=a("strong"),pYe=o("vit_mae"),_Ye=o(" \u2014 "),nx=a("a"),uYe=o("ViTFeatureExtractor"),bYe=o(" (ViTMAE model)"),vYe=l(),ih=a("li"),SU=a("strong"),TYe=o("wav2vec2"),FYe=o(" \u2014 "),sx=a("a"),CYe=o("Wav2Vec2FeatureExtractor"),MYe=o(" (Wav2Vec2 model)"),EYe=l(),f(dh.$$.fragment),yYe=l(),PU=a("p"),wYe=o("Examples:"),AYe=l(),f(LM.$$.fragment),LYe=l(),ch=a("div"),f(BM.$$.fragment),BYe=l(),$U=a("p"),kYe=o("Register a new feature extractor for this class."),C7e=l(),Ni=a("h2"),fh=a("a"),IU=a("span"),f(kM.$$.fragment),xYe=l(),jU=a("span"),RYe=o("AutoProcessor"),M7e=l(),zo=a("div"),f(xM.$$.fragment),SYe=l(),RM=a("p"),PYe=o(`This is a generic processor class that will be instantiated as one of the processor classes of the library when
created with the `),lx=a("a"),$Ye=o("AutoProcessor.from_pretrained()"),IYe=o(" class method."),jYe=l(),SM=a("p"),NYe=o("This class cannot be instantiated directly using "),NU=a("code"),DYe=o("__init__()"),qYe=o(" (throws an error)."),GYe=l(),Be=a("div"),f(PM.$$.fragment),OYe=l(),DU=a("p"),XYe=o("Instantiate one of the processor classes of the library from a pretrained model vocabulary."),zYe=l(),Di=a("p"),VYe=o("The processor class to instantiate is selected based on the "),qU=a("code"),WYe=o("model_type"),QYe=o(` property of the config object (either
passed as an argument or loaded from `),GU=a("code"),HYe=o("pretrained_model_name_or_path"),UYe=o(" if possible):"),JYe=l(),we=a("ul"),mh=a("li"),OU=a("strong"),YYe=o("clip"),KYe=o(" \u2014 "),ix=a("a"),ZYe=o("CLIPProcessor"),eKe=o(" (CLIP model)"),oKe=l(),gh=a("li"),XU=a("strong"),rKe=o("layoutlmv2"),tKe=o(" \u2014 "),dx=a("a"),aKe=o("LayoutLMv2Processor"),nKe=o(" (LayoutLMv2 model)"),sKe=l(),hh=a("li"),zU=a("strong"),lKe=o("layoutxlm"),iKe=o(" \u2014 "),cx=a("a"),dKe=o("LayoutXLMProcessor"),cKe=o(" (LayoutXLM model)"),fKe=l(),ph=a("li"),VU=a("strong"),mKe=o("speech_to_text"),gKe=o(" \u2014 "),fx=a("a"),hKe=o("Speech2TextProcessor"),pKe=o(" (Speech2Text model)"),_Ke=l(),_h=a("li"),WU=a("strong"),uKe=o("speech_to_text_2"),bKe=o(" \u2014 "),mx=a("a"),vKe=o("Speech2Text2Processor"),TKe=o(" (Speech2Text2 model)"),FKe=l(),uh=a("li"),QU=a("strong"),CKe=o("trocr"),MKe=o(" \u2014 "),gx=a("a"),EKe=o("TrOCRProcessor"),yKe=o(" (TrOCR model)"),wKe=l(),bh=a("li"),HU=a("strong"),AKe=o("vision-text-dual-encoder"),LKe=o(" \u2014 "),hx=a("a"),BKe=o("VisionTextDualEncoderProcessor"),kKe=o(" (VisionTextDualEncoder model)"),xKe=l(),vh=a("li"),UU=a("strong"),RKe=o("wav2vec2"),SKe=o(" \u2014 "),px=a("a"),PKe=o("Wav2Vec2Processor"),$Ke=o(" (Wav2Vec2 model)"),IKe=l(),f(Th.$$.fragment),jKe=l(),JU=a("p"),NKe=o("Examples:"),DKe=l(),f($M.$$.fragment),qKe=l(),Fh=a("div"),f(IM.$$.fragment),GKe=l(),YU=a("p"),OKe=o("Register a new processor for this class."),E7e=l(),qi=a("h2"),Ch=a("a"),KU=a("span"),f(jM.$$.fragment),XKe=l(),ZU=a("span"),zKe=o("AutoModel"),y7e=l(),Vo=a("div"),f(NM.$$.fragment),VKe=l(),Gi=a("p"),WKe=o(`This is a generic model class that will be instantiated as one of the base model classes of the library when created
with the `),eJ=a("code"),QKe=o("from_pretrained()"),HKe=o("class method or the "),oJ=a("code"),UKe=o("from_config()"),JKe=o(`class
method.`),YKe=l(),DM=a("p"),KKe=o("This class cannot be instantiated directly using "),rJ=a("code"),ZKe=o("__init__()"),eZe=o(" (throws an error)."),oZe=l(),Nr=a("div"),f(qM.$$.fragment),rZe=l(),tJ=a("p"),tZe=o("Instantiates one of the base model classes of the library from a configuration."),aZe=l(),Oi=a("p"),nZe=o(`Note:
Loading a model from its configuration file does `),aJ=a("strong"),sZe=o("not"),lZe=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),nJ=a("code"),iZe=o("from_pretrained()"),dZe=o("to load the model weights."),cZe=l(),sJ=a("p"),fZe=o("Examples:"),mZe=l(),f(GM.$$.fragment),gZe=l(),ke=a("div"),f(OM.$$.fragment),hZe=l(),lJ=a("p"),pZe=o("Instantiate one of the base model classes of the library from a pretrained model."),_Ze=l(),Da=a("p"),uZe=o("The model class to instantiate is selected based on the "),iJ=a("code"),bZe=o("model_type"),vZe=o(` property of the config object (either
passed as an argument or loaded from `),dJ=a("code"),TZe=o("pretrained_model_name_or_path"),FZe=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),cJ=a("code"),CZe=o("pretrained_model_name_or_path"),MZe=o(":"),EZe=l(),F=a("ul"),Mh=a("li"),fJ=a("strong"),yZe=o("albert"),wZe=o(" \u2014 "),_x=a("a"),AZe=o("AlbertModel"),LZe=o(" (ALBERT model)"),BZe=l(),Eh=a("li"),mJ=a("strong"),kZe=o("bart"),xZe=o(" \u2014 "),ux=a("a"),RZe=o("BartModel"),SZe=o(" (BART model)"),PZe=l(),yh=a("li"),gJ=a("strong"),$Ze=o("beit"),IZe=o(" \u2014 "),bx=a("a"),jZe=o("BeitModel"),NZe=o(" (BEiT model)"),DZe=l(),wh=a("li"),hJ=a("strong"),qZe=o("bert"),GZe=o(" \u2014 "),vx=a("a"),OZe=o("BertModel"),XZe=o(" (BERT model)"),zZe=l(),Ah=a("li"),pJ=a("strong"),VZe=o("bert-generation"),WZe=o(" \u2014 "),Tx=a("a"),QZe=o("BertGenerationEncoder"),HZe=o(" (Bert Generation model)"),UZe=l(),Lh=a("li"),_J=a("strong"),JZe=o("big_bird"),YZe=o(" \u2014 "),Fx=a("a"),KZe=o("BigBirdModel"),ZZe=o(" (BigBird model)"),eeo=l(),Bh=a("li"),uJ=a("strong"),oeo=o("bigbird_pegasus"),reo=o(" \u2014 "),Cx=a("a"),teo=o("BigBirdPegasusModel"),aeo=o(" (BigBirdPegasus model)"),neo=l(),kh=a("li"),bJ=a("strong"),seo=o("blenderbot"),leo=o(" \u2014 "),Mx=a("a"),ieo=o("BlenderbotModel"),deo=o(" (Blenderbot model)"),ceo=l(),xh=a("li"),vJ=a("strong"),feo=o("blenderbot-small"),meo=o(" \u2014 "),Ex=a("a"),geo=o("BlenderbotSmallModel"),heo=o(" (BlenderbotSmall model)"),peo=l(),Rh=a("li"),TJ=a("strong"),_eo=o("camembert"),ueo=o(" \u2014 "),yx=a("a"),beo=o("CamembertModel"),veo=o(" (CamemBERT model)"),Teo=l(),Sh=a("li"),FJ=a("strong"),Feo=o("canine"),Ceo=o(" \u2014 "),wx=a("a"),Meo=o("CanineModel"),Eeo=o(" (Canine model)"),yeo=l(),Ph=a("li"),CJ=a("strong"),weo=o("clip"),Aeo=o(" \u2014 "),Ax=a("a"),Leo=o("CLIPModel"),Beo=o(" (CLIP model)"),keo=l(),$h=a("li"),MJ=a("strong"),xeo=o("convbert"),Reo=o(" \u2014 "),Lx=a("a"),Seo=o("ConvBertModel"),Peo=o(" (ConvBERT model)"),$eo=l(),Ih=a("li"),EJ=a("strong"),Ieo=o("convnext"),jeo=o(" \u2014 "),Bx=a("a"),Neo=o("ConvNextModel"),Deo=o(" (ConvNext model)"),qeo=l(),jh=a("li"),yJ=a("strong"),Geo=o("ctrl"),Oeo=o(" \u2014 "),kx=a("a"),Xeo=o("CTRLModel"),zeo=o(" (CTRL model)"),Veo=l(),Nh=a("li"),wJ=a("strong"),Weo=o("deberta"),Qeo=o(" \u2014 "),xx=a("a"),Heo=o("DebertaModel"),Ueo=o(" (DeBERTa model)"),Jeo=l(),Dh=a("li"),AJ=a("strong"),Yeo=o("deberta-v2"),Keo=o(" \u2014 "),Rx=a("a"),Zeo=o("DebertaV2Model"),eoo=o(" (DeBERTa-v2 model)"),ooo=l(),qh=a("li"),LJ=a("strong"),roo=o("deit"),too=o(" \u2014 "),Sx=a("a"),aoo=o("DeiTModel"),noo=o(" (DeiT model)"),soo=l(),Gh=a("li"),BJ=a("strong"),loo=o("detr"),ioo=o(" \u2014 "),Px=a("a"),doo=o("DetrModel"),coo=o(" (DETR model)"),foo=l(),Oh=a("li"),kJ=a("strong"),moo=o("distilbert"),goo=o(" \u2014 "),$x=a("a"),hoo=o("DistilBertModel"),poo=o(" (DistilBERT model)"),_oo=l(),Xh=a("li"),xJ=a("strong"),uoo=o("dpr"),boo=o(" \u2014 "),Ix=a("a"),voo=o("DPRQuestionEncoder"),Too=o(" (DPR model)"),Foo=l(),zh=a("li"),RJ=a("strong"),Coo=o("electra"),Moo=o(" \u2014 "),jx=a("a"),Eoo=o("ElectraModel"),yoo=o(" (ELECTRA model)"),woo=l(),Vh=a("li"),SJ=a("strong"),Aoo=o("flaubert"),Loo=o(" \u2014 "),Nx=a("a"),Boo=o("FlaubertModel"),koo=o(" (FlauBERT model)"),xoo=l(),Wh=a("li"),PJ=a("strong"),Roo=o("fnet"),Soo=o(" \u2014 "),Dx=a("a"),Poo=o("FNetModel"),$oo=o(" (FNet model)"),Ioo=l(),Qh=a("li"),$J=a("strong"),joo=o("fsmt"),Noo=o(" \u2014 "),qx=a("a"),Doo=o("FSMTModel"),qoo=o(" (FairSeq Machine-Translation model)"),Goo=l(),xs=a("li"),IJ=a("strong"),Ooo=o("funnel"),Xoo=o(" \u2014 "),Gx=a("a"),zoo=o("FunnelModel"),Voo=o(" or "),Ox=a("a"),Woo=o("FunnelBaseModel"),Qoo=o(" (Funnel Transformer model)"),Hoo=l(),Hh=a("li"),jJ=a("strong"),Uoo=o("gpt2"),Joo=o(" \u2014 "),Xx=a("a"),Yoo=o("GPT2Model"),Koo=o(" (OpenAI GPT-2 model)"),Zoo=l(),Uh=a("li"),NJ=a("strong"),ero=o("gpt_neo"),oro=o(" \u2014 "),zx=a("a"),rro=o("GPTNeoModel"),tro=o(" (GPT Neo model)"),aro=l(),Jh=a("li"),DJ=a("strong"),nro=o("gptj"),sro=o(" \u2014 "),Vx=a("a"),lro=o("GPTJModel"),iro=o(" (GPT-J model)"),dro=l(),Yh=a("li"),qJ=a("strong"),cro=o("hubert"),fro=o(" \u2014 "),Wx=a("a"),mro=o("HubertModel"),gro=o(" (Hubert model)"),hro=l(),Kh=a("li"),GJ=a("strong"),pro=o("ibert"),_ro=o(" \u2014 "),Qx=a("a"),uro=o("IBertModel"),bro=o(" (I-BERT model)"),vro=l(),Zh=a("li"),OJ=a("strong"),Tro=o("imagegpt"),Fro=o(" \u2014 "),Hx=a("a"),Cro=o("ImageGPTModel"),Mro=o(" (ImageGPT model)"),Ero=l(),ep=a("li"),XJ=a("strong"),yro=o("layoutlm"),wro=o(" \u2014 "),Ux=a("a"),Aro=o("LayoutLMModel"),Lro=o(" (LayoutLM model)"),Bro=l(),op=a("li"),zJ=a("strong"),kro=o("layoutlmv2"),xro=o(" \u2014 "),Jx=a("a"),Rro=o("LayoutLMv2Model"),Sro=o(" (LayoutLMv2 model)"),Pro=l(),rp=a("li"),VJ=a("strong"),$ro=o("led"),Iro=o(" \u2014 "),Yx=a("a"),jro=o("LEDModel"),Nro=o(" (LED model)"),Dro=l(),tp=a("li"),WJ=a("strong"),qro=o("longformer"),Gro=o(" \u2014 "),Kx=a("a"),Oro=o("LongformerModel"),Xro=o(" (Longformer model)"),zro=l(),ap=a("li"),QJ=a("strong"),Vro=o("luke"),Wro=o(" \u2014 "),Zx=a("a"),Qro=o("LukeModel"),Hro=o(" (LUKE model)"),Uro=l(),np=a("li"),HJ=a("strong"),Jro=o("lxmert"),Yro=o(" \u2014 "),eR=a("a"),Kro=o("LxmertModel"),Zro=o(" (LXMERT model)"),eto=l(),sp=a("li"),UJ=a("strong"),oto=o("m2m_100"),rto=o(" \u2014 "),oR=a("a"),tto=o("M2M100Model"),ato=o(" (M2M100 model)"),nto=l(),lp=a("li"),JJ=a("strong"),sto=o("marian"),lto=o(" \u2014 "),rR=a("a"),ito=o("MarianModel"),dto=o(" (Marian model)"),cto=l(),ip=a("li"),YJ=a("strong"),fto=o("maskformer"),mto=o(" \u2014 "),tR=a("a"),gto=o("MaskFormerModel"),hto=o(" (MaskFormer model)"),pto=l(),dp=a("li"),KJ=a("strong"),_to=o("mbart"),uto=o(" \u2014 "),aR=a("a"),bto=o("MBartModel"),vto=o(" (mBART model)"),Tto=l(),cp=a("li"),ZJ=a("strong"),Fto=o("megatron-bert"),Cto=o(" \u2014 "),nR=a("a"),Mto=o("MegatronBertModel"),Eto=o(" (MegatronBert model)"),yto=l(),fp=a("li"),eY=a("strong"),wto=o("mobilebert"),Ato=o(" \u2014 "),sR=a("a"),Lto=o("MobileBertModel"),Bto=o(" (MobileBERT model)"),kto=l(),mp=a("li"),oY=a("strong"),xto=o("mpnet"),Rto=o(" \u2014 "),lR=a("a"),Sto=o("MPNetModel"),Pto=o(" (MPNet model)"),$to=l(),gp=a("li"),rY=a("strong"),Ito=o("mt5"),jto=o(" \u2014 "),iR=a("a"),Nto=o("MT5Model"),Dto=o(" (mT5 model)"),qto=l(),hp=a("li"),tY=a("strong"),Gto=o("nystromformer"),Oto=o(" \u2014 "),dR=a("a"),Xto=o("NystromformerModel"),zto=o(" (Nystromformer model)"),Vto=l(),pp=a("li"),aY=a("strong"),Wto=o("openai-gpt"),Qto=o(" \u2014 "),cR=a("a"),Hto=o("OpenAIGPTModel"),Uto=o(" (OpenAI GPT model)"),Jto=l(),_p=a("li"),nY=a("strong"),Yto=o("pegasus"),Kto=o(" \u2014 "),fR=a("a"),Zto=o("PegasusModel"),eao=o(" (Pegasus model)"),oao=l(),up=a("li"),sY=a("strong"),rao=o("perceiver"),tao=o(" \u2014 "),mR=a("a"),aao=o("PerceiverModel"),nao=o(" (Perceiver model)"),sao=l(),bp=a("li"),lY=a("strong"),lao=o("plbart"),iao=o(" \u2014 "),gR=a("a"),dao=o("PLBartModel"),cao=o(" (PLBart model)"),fao=l(),vp=a("li"),iY=a("strong"),mao=o("poolformer"),gao=o(" \u2014 "),hR=a("a"),hao=o("PoolFormerModel"),pao=o(" (PoolFormer model)"),_ao=l(),Tp=a("li"),dY=a("strong"),uao=o("prophetnet"),bao=o(" \u2014 "),pR=a("a"),vao=o("ProphetNetModel"),Tao=o(" (ProphetNet model)"),Fao=l(),Fp=a("li"),cY=a("strong"),Cao=o("qdqbert"),Mao=o(" \u2014 "),_R=a("a"),Eao=o("QDQBertModel"),yao=o(" (QDQBert model)"),wao=l(),Cp=a("li"),fY=a("strong"),Aao=o("reformer"),Lao=o(" \u2014 "),uR=a("a"),Bao=o("ReformerModel"),kao=o(" (Reformer model)"),xao=l(),Mp=a("li"),mY=a("strong"),Rao=o("rembert"),Sao=o(" \u2014 "),bR=a("a"),Pao=o("RemBertModel"),$ao=o(" (RemBERT model)"),Iao=l(),Ep=a("li"),gY=a("strong"),jao=o("retribert"),Nao=o(" \u2014 "),vR=a("a"),Dao=o("RetriBertModel"),qao=o(" (RetriBERT model)"),Gao=l(),yp=a("li"),hY=a("strong"),Oao=o("roberta"),Xao=o(" \u2014 "),TR=a("a"),zao=o("RobertaModel"),Vao=o(" (RoBERTa model)"),Wao=l(),wp=a("li"),pY=a("strong"),Qao=o("roformer"),Hao=o(" \u2014 "),FR=a("a"),Uao=o("RoFormerModel"),Jao=o(" (RoFormer model)"),Yao=l(),Ap=a("li"),_Y=a("strong"),Kao=o("segformer"),Zao=o(" \u2014 "),CR=a("a"),eno=o("SegformerModel"),ono=o(" (SegFormer model)"),rno=l(),Lp=a("li"),uY=a("strong"),tno=o("sew"),ano=o(" \u2014 "),MR=a("a"),nno=o("SEWModel"),sno=o(" (SEW model)"),lno=l(),Bp=a("li"),bY=a("strong"),ino=o("sew-d"),dno=o(" \u2014 "),ER=a("a"),cno=o("SEWDModel"),fno=o(" (SEW-D model)"),mno=l(),kp=a("li"),vY=a("strong"),gno=o("speech_to_text"),hno=o(" \u2014 "),yR=a("a"),pno=o("Speech2TextModel"),_no=o(" (Speech2Text model)"),uno=l(),xp=a("li"),TY=a("strong"),bno=o("splinter"),vno=o(" \u2014 "),wR=a("a"),Tno=o("SplinterModel"),Fno=o(" (Splinter model)"),Cno=l(),Rp=a("li"),FY=a("strong"),Mno=o("squeezebert"),Eno=o(" \u2014 "),AR=a("a"),yno=o("SqueezeBertModel"),wno=o(" (SqueezeBERT model)"),Ano=l(),Sp=a("li"),CY=a("strong"),Lno=o("swin"),Bno=o(" \u2014 "),LR=a("a"),kno=o("SwinModel"),xno=o(" (Swin model)"),Rno=l(),Pp=a("li"),MY=a("strong"),Sno=o("t5"),Pno=o(" \u2014 "),BR=a("a"),$no=o("T5Model"),Ino=o(" (T5 model)"),jno=l(),$p=a("li"),EY=a("strong"),Nno=o("tapas"),Dno=o(" \u2014 "),kR=a("a"),qno=o("TapasModel"),Gno=o(" (TAPAS model)"),Ono=l(),Ip=a("li"),yY=a("strong"),Xno=o("transfo-xl"),zno=o(" \u2014 "),xR=a("a"),Vno=o("TransfoXLModel"),Wno=o(" (Transformer-XL model)"),Qno=l(),jp=a("li"),wY=a("strong"),Hno=o("unispeech"),Uno=o(" \u2014 "),RR=a("a"),Jno=o("UniSpeechModel"),Yno=o(" (UniSpeech model)"),Kno=l(),Np=a("li"),AY=a("strong"),Zno=o("unispeech-sat"),eso=o(" \u2014 "),SR=a("a"),oso=o("UniSpeechSatModel"),rso=o(" (UniSpeechSat model)"),tso=l(),Dp=a("li"),LY=a("strong"),aso=o("vilt"),nso=o(" \u2014 "),PR=a("a"),sso=o("ViltModel"),lso=o(" (ViLT model)"),iso=l(),qp=a("li"),BY=a("strong"),dso=o("vision-text-dual-encoder"),cso=o(" \u2014 "),$R=a("a"),fso=o("VisionTextDualEncoderModel"),mso=o(" (VisionTextDualEncoder model)"),gso=l(),Gp=a("li"),kY=a("strong"),hso=o("visual_bert"),pso=o(" \u2014 "),IR=a("a"),_so=o("VisualBertModel"),uso=o(" (VisualBert model)"),bso=l(),Op=a("li"),xY=a("strong"),vso=o("vit"),Tso=o(" \u2014 "),jR=a("a"),Fso=o("ViTModel"),Cso=o(" (ViT model)"),Mso=l(),Xp=a("li"),RY=a("strong"),Eso=o("vit_mae"),yso=o(" \u2014 "),NR=a("a"),wso=o("ViTMAEModel"),Aso=o(" (ViTMAE model)"),Lso=l(),zp=a("li"),SY=a("strong"),Bso=o("wav2vec2"),kso=o(" \u2014 "),DR=a("a"),xso=o("Wav2Vec2Model"),Rso=o(" (Wav2Vec2 model)"),Sso=l(),Vp=a("li"),PY=a("strong"),Pso=o("wavlm"),$so=o(" \u2014 "),qR=a("a"),Iso=o("WavLMModel"),jso=o(" (WavLM model)"),Nso=l(),Wp=a("li"),$Y=a("strong"),Dso=o("xglm"),qso=o(" \u2014 "),GR=a("a"),Gso=o("XGLMModel"),Oso=o(" (XGLM model)"),Xso=l(),Qp=a("li"),IY=a("strong"),zso=o("xlm"),Vso=o(" \u2014 "),OR=a("a"),Wso=o("XLMModel"),Qso=o(" (XLM model)"),Hso=l(),Hp=a("li"),jY=a("strong"),Uso=o("xlm-prophetnet"),Jso=o(" \u2014 "),XR=a("a"),Yso=o("XLMProphetNetModel"),Kso=o(" (XLMProphetNet model)"),Zso=l(),Up=a("li"),NY=a("strong"),elo=o("xlm-roberta"),olo=o(" \u2014 "),zR=a("a"),rlo=o("XLMRobertaModel"),tlo=o(" (XLM-RoBERTa model)"),alo=l(),Jp=a("li"),DY=a("strong"),nlo=o("xlm-roberta-xl"),slo=o(" \u2014 "),VR=a("a"),llo=o("XLMRobertaXLModel"),ilo=o(" (XLM-RoBERTa-XL model)"),dlo=l(),Yp=a("li"),qY=a("strong"),clo=o("xlnet"),flo=o(" \u2014 "),WR=a("a"),mlo=o("XLNetModel"),glo=o(" (XLNet model)"),hlo=l(),Kp=a("li"),GY=a("strong"),plo=o("yoso"),_lo=o(" \u2014 "),QR=a("a"),ulo=o("YosoModel"),blo=o(" (YOSO model)"),vlo=l(),Zp=a("p"),Tlo=o("The model is set in evaluation mode by default using "),OY=a("code"),Flo=o("model.eval()"),Clo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),XY=a("code"),Mlo=o("model.train()"),Elo=l(),zY=a("p"),ylo=o("Examples:"),wlo=l(),f(XM.$$.fragment),w7e=l(),Xi=a("h2"),e_=a("a"),VY=a("span"),f(zM.$$.fragment),Alo=l(),WY=a("span"),Llo=o("AutoModelForPreTraining"),A7e=l(),Wo=a("div"),f(VM.$$.fragment),Blo=l(),zi=a("p"),klo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a pretraining head) when created
with the `),QY=a("code"),xlo=o("from_pretrained()"),Rlo=o("class method or the "),HY=a("code"),Slo=o("from_config()"),Plo=o(`class
method.`),$lo=l(),WM=a("p"),Ilo=o("This class cannot be instantiated directly using "),UY=a("code"),jlo=o("__init__()"),Nlo=o(" (throws an error)."),Dlo=l(),Dr=a("div"),f(QM.$$.fragment),qlo=l(),JY=a("p"),Glo=o("Instantiates one of the model classes of the library (with a pretraining head) from a configuration."),Olo=l(),Vi=a("p"),Xlo=o(`Note:
Loading a model from its configuration file does `),YY=a("strong"),zlo=o("not"),Vlo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),KY=a("code"),Wlo=o("from_pretrained()"),Qlo=o("to load the model weights."),Hlo=l(),ZY=a("p"),Ulo=o("Examples:"),Jlo=l(),f(HM.$$.fragment),Ylo=l(),xe=a("div"),f(UM.$$.fragment),Klo=l(),eK=a("p"),Zlo=o("Instantiate one of the model classes of the library (with a pretraining head) from a pretrained model."),eio=l(),qa=a("p"),oio=o("The model class to instantiate is selected based on the "),oK=a("code"),rio=o("model_type"),tio=o(` property of the config object (either
passed as an argument or loaded from `),rK=a("code"),aio=o("pretrained_model_name_or_path"),nio=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),tK=a("code"),sio=o("pretrained_model_name_or_path"),lio=o(":"),iio=l(),x=a("ul"),o_=a("li"),aK=a("strong"),dio=o("albert"),cio=o(" \u2014 "),HR=a("a"),fio=o("AlbertForPreTraining"),mio=o(" (ALBERT model)"),gio=l(),r_=a("li"),nK=a("strong"),hio=o("bart"),pio=o(" \u2014 "),UR=a("a"),_io=o("BartForConditionalGeneration"),uio=o(" (BART model)"),bio=l(),t_=a("li"),sK=a("strong"),vio=o("bert"),Tio=o(" \u2014 "),JR=a("a"),Fio=o("BertForPreTraining"),Cio=o(" (BERT model)"),Mio=l(),a_=a("li"),lK=a("strong"),Eio=o("big_bird"),yio=o(" \u2014 "),YR=a("a"),wio=o("BigBirdForPreTraining"),Aio=o(" (BigBird model)"),Lio=l(),n_=a("li"),iK=a("strong"),Bio=o("camembert"),kio=o(" \u2014 "),KR=a("a"),xio=o("CamembertForMaskedLM"),Rio=o(" (CamemBERT model)"),Sio=l(),s_=a("li"),dK=a("strong"),Pio=o("ctrl"),$io=o(" \u2014 "),ZR=a("a"),Iio=o("CTRLLMHeadModel"),jio=o(" (CTRL model)"),Nio=l(),l_=a("li"),cK=a("strong"),Dio=o("deberta"),qio=o(" \u2014 "),eS=a("a"),Gio=o("DebertaForMaskedLM"),Oio=o(" (DeBERTa model)"),Xio=l(),i_=a("li"),fK=a("strong"),zio=o("deberta-v2"),Vio=o(" \u2014 "),oS=a("a"),Wio=o("DebertaV2ForMaskedLM"),Qio=o(" (DeBERTa-v2 model)"),Hio=l(),d_=a("li"),mK=a("strong"),Uio=o("distilbert"),Jio=o(" \u2014 "),rS=a("a"),Yio=o("DistilBertForMaskedLM"),Kio=o(" (DistilBERT model)"),Zio=l(),c_=a("li"),gK=a("strong"),edo=o("electra"),odo=o(" \u2014 "),tS=a("a"),rdo=o("ElectraForPreTraining"),tdo=o(" (ELECTRA model)"),ado=l(),f_=a("li"),hK=a("strong"),ndo=o("flaubert"),sdo=o(" \u2014 "),aS=a("a"),ldo=o("FlaubertWithLMHeadModel"),ido=o(" (FlauBERT model)"),ddo=l(),m_=a("li"),pK=a("strong"),cdo=o("fnet"),fdo=o(" \u2014 "),nS=a("a"),mdo=o("FNetForPreTraining"),gdo=o(" (FNet model)"),hdo=l(),g_=a("li"),_K=a("strong"),pdo=o("fsmt"),_do=o(" \u2014 "),sS=a("a"),udo=o("FSMTForConditionalGeneration"),bdo=o(" (FairSeq Machine-Translation model)"),vdo=l(),h_=a("li"),uK=a("strong"),Tdo=o("funnel"),Fdo=o(" \u2014 "),lS=a("a"),Cdo=o("FunnelForPreTraining"),Mdo=o(" (Funnel Transformer model)"),Edo=l(),p_=a("li"),bK=a("strong"),ydo=o("gpt2"),wdo=o(" \u2014 "),iS=a("a"),Ado=o("GPT2LMHeadModel"),Ldo=o(" (OpenAI GPT-2 model)"),Bdo=l(),__=a("li"),vK=a("strong"),kdo=o("ibert"),xdo=o(" \u2014 "),dS=a("a"),Rdo=o("IBertForMaskedLM"),Sdo=o(" (I-BERT model)"),Pdo=l(),u_=a("li"),TK=a("strong"),$do=o("layoutlm"),Ido=o(" \u2014 "),cS=a("a"),jdo=o("LayoutLMForMaskedLM"),Ndo=o(" (LayoutLM model)"),Ddo=l(),b_=a("li"),FK=a("strong"),qdo=o("longformer"),Gdo=o(" \u2014 "),fS=a("a"),Odo=o("LongformerForMaskedLM"),Xdo=o(" (Longformer model)"),zdo=l(),v_=a("li"),CK=a("strong"),Vdo=o("lxmert"),Wdo=o(" \u2014 "),mS=a("a"),Qdo=o("LxmertForPreTraining"),Hdo=o(" (LXMERT model)"),Udo=l(),T_=a("li"),MK=a("strong"),Jdo=o("megatron-bert"),Ydo=o(" \u2014 "),gS=a("a"),Kdo=o("MegatronBertForPreTraining"),Zdo=o(" (MegatronBert model)"),eco=l(),F_=a("li"),EK=a("strong"),oco=o("mobilebert"),rco=o(" \u2014 "),hS=a("a"),tco=o("MobileBertForPreTraining"),aco=o(" (MobileBERT model)"),nco=l(),C_=a("li"),yK=a("strong"),sco=o("mpnet"),lco=o(" \u2014 "),pS=a("a"),ico=o("MPNetForMaskedLM"),dco=o(" (MPNet model)"),cco=l(),M_=a("li"),wK=a("strong"),fco=o("openai-gpt"),mco=o(" \u2014 "),_S=a("a"),gco=o("OpenAIGPTLMHeadModel"),hco=o(" (OpenAI GPT model)"),pco=l(),E_=a("li"),AK=a("strong"),_co=o("retribert"),uco=o(" \u2014 "),uS=a("a"),bco=o("RetriBertModel"),vco=o(" (RetriBERT model)"),Tco=l(),y_=a("li"),LK=a("strong"),Fco=o("roberta"),Cco=o(" \u2014 "),bS=a("a"),Mco=o("RobertaForMaskedLM"),Eco=o(" (RoBERTa model)"),yco=l(),w_=a("li"),BK=a("strong"),wco=o("squeezebert"),Aco=o(" \u2014 "),vS=a("a"),Lco=o("SqueezeBertForMaskedLM"),Bco=o(" (SqueezeBERT model)"),kco=l(),A_=a("li"),kK=a("strong"),xco=o("t5"),Rco=o(" \u2014 "),TS=a("a"),Sco=o("T5ForConditionalGeneration"),Pco=o(" (T5 model)"),$co=l(),L_=a("li"),xK=a("strong"),Ico=o("tapas"),jco=o(" \u2014 "),FS=a("a"),Nco=o("TapasForMaskedLM"),Dco=o(" (TAPAS model)"),qco=l(),B_=a("li"),RK=a("strong"),Gco=o("transfo-xl"),Oco=o(" \u2014 "),CS=a("a"),Xco=o("TransfoXLLMHeadModel"),zco=o(" (Transformer-XL model)"),Vco=l(),k_=a("li"),SK=a("strong"),Wco=o("unispeech"),Qco=o(" \u2014 "),MS=a("a"),Hco=o("UniSpeechForPreTraining"),Uco=o(" (UniSpeech model)"),Jco=l(),x_=a("li"),PK=a("strong"),Yco=o("unispeech-sat"),Kco=o(" \u2014 "),ES=a("a"),Zco=o("UniSpeechSatForPreTraining"),efo=o(" (UniSpeechSat model)"),ofo=l(),R_=a("li"),$K=a("strong"),rfo=o("visual_bert"),tfo=o(" \u2014 "),yS=a("a"),afo=o("VisualBertForPreTraining"),nfo=o(" (VisualBert model)"),sfo=l(),S_=a("li"),IK=a("strong"),lfo=o("vit_mae"),ifo=o(" \u2014 "),wS=a("a"),dfo=o("ViTMAEForPreTraining"),cfo=o(" (ViTMAE model)"),ffo=l(),P_=a("li"),jK=a("strong"),mfo=o("wav2vec2"),gfo=o(" \u2014 "),AS=a("a"),hfo=o("Wav2Vec2ForPreTraining"),pfo=o(" (Wav2Vec2 model)"),_fo=l(),$_=a("li"),NK=a("strong"),ufo=o("xlm"),bfo=o(" \u2014 "),LS=a("a"),vfo=o("XLMWithLMHeadModel"),Tfo=o(" (XLM model)"),Ffo=l(),I_=a("li"),DK=a("strong"),Cfo=o("xlm-roberta"),Mfo=o(" \u2014 "),BS=a("a"),Efo=o("XLMRobertaForMaskedLM"),yfo=o(" (XLM-RoBERTa model)"),wfo=l(),j_=a("li"),qK=a("strong"),Afo=o("xlm-roberta-xl"),Lfo=o(" \u2014 "),kS=a("a"),Bfo=o("XLMRobertaXLForMaskedLM"),kfo=o(" (XLM-RoBERTa-XL model)"),xfo=l(),N_=a("li"),GK=a("strong"),Rfo=o("xlnet"),Sfo=o(" \u2014 "),xS=a("a"),Pfo=o("XLNetLMHeadModel"),$fo=o(" (XLNet model)"),Ifo=l(),D_=a("p"),jfo=o("The model is set in evaluation mode by default using "),OK=a("code"),Nfo=o("model.eval()"),Dfo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),XK=a("code"),qfo=o("model.train()"),Gfo=l(),zK=a("p"),Ofo=o("Examples:"),Xfo=l(),f(JM.$$.fragment),L7e=l(),Wi=a("h2"),q_=a("a"),VK=a("span"),f(YM.$$.fragment),zfo=l(),WK=a("span"),Vfo=o("AutoModelForCausalLM"),B7e=l(),Qo=a("div"),f(KM.$$.fragment),Wfo=l(),Qi=a("p"),Qfo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a causal language modeling head) when created
with the `),QK=a("code"),Hfo=o("from_pretrained()"),Ufo=o("class method or the "),HK=a("code"),Jfo=o("from_config()"),Yfo=o(`class
method.`),Kfo=l(),ZM=a("p"),Zfo=o("This class cannot be instantiated directly using "),UK=a("code"),emo=o("__init__()"),omo=o(" (throws an error)."),rmo=l(),qr=a("div"),f(eE.$$.fragment),tmo=l(),JK=a("p"),amo=o("Instantiates one of the model classes of the library (with a causal language modeling head) from a configuration."),nmo=l(),Hi=a("p"),smo=o(`Note:
Loading a model from its configuration file does `),YK=a("strong"),lmo=o("not"),imo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),KK=a("code"),dmo=o("from_pretrained()"),cmo=o("to load the model weights."),fmo=l(),ZK=a("p"),mmo=o("Examples:"),gmo=l(),f(oE.$$.fragment),hmo=l(),Re=a("div"),f(rE.$$.fragment),pmo=l(),eZ=a("p"),_mo=o("Instantiate one of the model classes of the library (with a causal language modeling head) from a pretrained model."),umo=l(),Ga=a("p"),bmo=o("The model class to instantiate is selected based on the "),oZ=a("code"),vmo=o("model_type"),Tmo=o(` property of the config object (either
passed as an argument or loaded from `),rZ=a("code"),Fmo=o("pretrained_model_name_or_path"),Cmo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),tZ=a("code"),Mmo=o("pretrained_model_name_or_path"),Emo=o(":"),ymo=l(),$=a("ul"),G_=a("li"),aZ=a("strong"),wmo=o("bart"),Amo=o(" \u2014 "),RS=a("a"),Lmo=o("BartForCausalLM"),Bmo=o(" (BART model)"),kmo=l(),O_=a("li"),nZ=a("strong"),xmo=o("bert"),Rmo=o(" \u2014 "),SS=a("a"),Smo=o("BertLMHeadModel"),Pmo=o(" (BERT model)"),$mo=l(),X_=a("li"),sZ=a("strong"),Imo=o("bert-generation"),jmo=o(" \u2014 "),PS=a("a"),Nmo=o("BertGenerationDecoder"),Dmo=o(" (Bert Generation model)"),qmo=l(),z_=a("li"),lZ=a("strong"),Gmo=o("big_bird"),Omo=o(" \u2014 "),$S=a("a"),Xmo=o("BigBirdForCausalLM"),zmo=o(" (BigBird model)"),Vmo=l(),V_=a("li"),iZ=a("strong"),Wmo=o("bigbird_pegasus"),Qmo=o(" \u2014 "),IS=a("a"),Hmo=o("BigBirdPegasusForCausalLM"),Umo=o(" (BigBirdPegasus model)"),Jmo=l(),W_=a("li"),dZ=a("strong"),Ymo=o("blenderbot"),Kmo=o(" \u2014 "),jS=a("a"),Zmo=o("BlenderbotForCausalLM"),ego=o(" (Blenderbot model)"),ogo=l(),Q_=a("li"),cZ=a("strong"),rgo=o("blenderbot-small"),tgo=o(" \u2014 "),NS=a("a"),ago=o("BlenderbotSmallForCausalLM"),ngo=o(" (BlenderbotSmall model)"),sgo=l(),H_=a("li"),fZ=a("strong"),lgo=o("camembert"),igo=o(" \u2014 "),DS=a("a"),dgo=o("CamembertForCausalLM"),cgo=o(" (CamemBERT model)"),fgo=l(),U_=a("li"),mZ=a("strong"),mgo=o("ctrl"),ggo=o(" \u2014 "),qS=a("a"),hgo=o("CTRLLMHeadModel"),pgo=o(" (CTRL model)"),_go=l(),J_=a("li"),gZ=a("strong"),ugo=o("electra"),bgo=o(" \u2014 "),GS=a("a"),vgo=o("ElectraForCausalLM"),Tgo=o(" (ELECTRA model)"),Fgo=l(),Y_=a("li"),hZ=a("strong"),Cgo=o("gpt2"),Mgo=o(" \u2014 "),OS=a("a"),Ego=o("GPT2LMHeadModel"),ygo=o(" (OpenAI GPT-2 model)"),wgo=l(),K_=a("li"),pZ=a("strong"),Ago=o("gpt_neo"),Lgo=o(" \u2014 "),XS=a("a"),Bgo=o("GPTNeoForCausalLM"),kgo=o(" (GPT Neo model)"),xgo=l(),Z_=a("li"),_Z=a("strong"),Rgo=o("gptj"),Sgo=o(" \u2014 "),zS=a("a"),Pgo=o("GPTJForCausalLM"),$go=o(" (GPT-J model)"),Igo=l(),eu=a("li"),uZ=a("strong"),jgo=o("marian"),Ngo=o(" \u2014 "),VS=a("a"),Dgo=o("MarianForCausalLM"),qgo=o(" (Marian model)"),Ggo=l(),ou=a("li"),bZ=a("strong"),Ogo=o("mbart"),Xgo=o(" \u2014 "),WS=a("a"),zgo=o("MBartForCausalLM"),Vgo=o(" (mBART model)"),Wgo=l(),ru=a("li"),vZ=a("strong"),Qgo=o("megatron-bert"),Hgo=o(" \u2014 "),QS=a("a"),Ugo=o("MegatronBertForCausalLM"),Jgo=o(" (MegatronBert model)"),Ygo=l(),tu=a("li"),TZ=a("strong"),Kgo=o("openai-gpt"),Zgo=o(" \u2014 "),HS=a("a"),eho=o("OpenAIGPTLMHeadModel"),oho=o(" (OpenAI GPT model)"),rho=l(),au=a("li"),FZ=a("strong"),tho=o("pegasus"),aho=o(" \u2014 "),US=a("a"),nho=o("PegasusForCausalLM"),sho=o(" (Pegasus model)"),lho=l(),nu=a("li"),CZ=a("strong"),iho=o("plbart"),dho=o(" \u2014 "),JS=a("a"),cho=o("PLBartForCausalLM"),fho=o(" (PLBart model)"),mho=l(),su=a("li"),MZ=a("strong"),gho=o("prophetnet"),hho=o(" \u2014 "),YS=a("a"),pho=o("ProphetNetForCausalLM"),_ho=o(" (ProphetNet model)"),uho=l(),lu=a("li"),EZ=a("strong"),bho=o("qdqbert"),vho=o(" \u2014 "),KS=a("a"),Tho=o("QDQBertLMHeadModel"),Fho=o(" (QDQBert model)"),Cho=l(),iu=a("li"),yZ=a("strong"),Mho=o("reformer"),Eho=o(" \u2014 "),ZS=a("a"),yho=o("ReformerModelWithLMHead"),who=o(" (Reformer model)"),Aho=l(),du=a("li"),wZ=a("strong"),Lho=o("rembert"),Bho=o(" \u2014 "),eP=a("a"),kho=o("RemBertForCausalLM"),xho=o(" (RemBERT model)"),Rho=l(),cu=a("li"),AZ=a("strong"),Sho=o("roberta"),Pho=o(" \u2014 "),oP=a("a"),$ho=o("RobertaForCausalLM"),Iho=o(" (RoBERTa model)"),jho=l(),fu=a("li"),LZ=a("strong"),Nho=o("roformer"),Dho=o(" \u2014 "),rP=a("a"),qho=o("RoFormerForCausalLM"),Gho=o(" (RoFormer model)"),Oho=l(),mu=a("li"),BZ=a("strong"),Xho=o("speech_to_text_2"),zho=o(" \u2014 "),tP=a("a"),Vho=o("Speech2Text2ForCausalLM"),Who=o(" (Speech2Text2 model)"),Qho=l(),gu=a("li"),kZ=a("strong"),Hho=o("transfo-xl"),Uho=o(" \u2014 "),aP=a("a"),Jho=o("TransfoXLLMHeadModel"),Yho=o(" (Transformer-XL model)"),Kho=l(),hu=a("li"),xZ=a("strong"),Zho=o("trocr"),epo=o(" \u2014 "),nP=a("a"),opo=o("TrOCRForCausalLM"),rpo=o(" (TrOCR model)"),tpo=l(),pu=a("li"),RZ=a("strong"),apo=o("xglm"),npo=o(" \u2014 "),sP=a("a"),spo=o("XGLMForCausalLM"),lpo=o(" (XGLM model)"),ipo=l(),_u=a("li"),SZ=a("strong"),dpo=o("xlm"),cpo=o(" \u2014 "),lP=a("a"),fpo=o("XLMWithLMHeadModel"),mpo=o(" (XLM model)"),gpo=l(),uu=a("li"),PZ=a("strong"),hpo=o("xlm-prophetnet"),ppo=o(" \u2014 "),iP=a("a"),_po=o("XLMProphetNetForCausalLM"),upo=o(" (XLMProphetNet model)"),bpo=l(),bu=a("li"),$Z=a("strong"),vpo=o("xlm-roberta"),Tpo=o(" \u2014 "),dP=a("a"),Fpo=o("XLMRobertaForCausalLM"),Cpo=o(" (XLM-RoBERTa model)"),Mpo=l(),vu=a("li"),IZ=a("strong"),Epo=o("xlm-roberta-xl"),ypo=o(" \u2014 "),cP=a("a"),wpo=o("XLMRobertaXLForCausalLM"),Apo=o(" (XLM-RoBERTa-XL model)"),Lpo=l(),Tu=a("li"),jZ=a("strong"),Bpo=o("xlnet"),kpo=o(" \u2014 "),fP=a("a"),xpo=o("XLNetLMHeadModel"),Rpo=o(" (XLNet model)"),Spo=l(),Fu=a("p"),Ppo=o("The model is set in evaluation mode by default using "),NZ=a("code"),$po=o("model.eval()"),Ipo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),DZ=a("code"),jpo=o("model.train()"),Npo=l(),qZ=a("p"),Dpo=o("Examples:"),qpo=l(),f(tE.$$.fragment),k7e=l(),Ui=a("h2"),Cu=a("a"),GZ=a("span"),f(aE.$$.fragment),Gpo=l(),OZ=a("span"),Opo=o("AutoModelForMaskedLM"),x7e=l(),Ho=a("div"),f(nE.$$.fragment),Xpo=l(),Ji=a("p"),zpo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a masked language modeling head) when created
with the `),XZ=a("code"),Vpo=o("from_pretrained()"),Wpo=o("class method or the "),zZ=a("code"),Qpo=o("from_config()"),Hpo=o(`class
method.`),Upo=l(),sE=a("p"),Jpo=o("This class cannot be instantiated directly using "),VZ=a("code"),Ypo=o("__init__()"),Kpo=o(" (throws an error)."),Zpo=l(),Gr=a("div"),f(lE.$$.fragment),e_o=l(),WZ=a("p"),o_o=o("Instantiates one of the model classes of the library (with a masked language modeling head) from a configuration."),r_o=l(),Yi=a("p"),t_o=o(`Note:
Loading a model from its configuration file does `),QZ=a("strong"),a_o=o("not"),n_o=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),HZ=a("code"),s_o=o("from_pretrained()"),l_o=o("to load the model weights."),i_o=l(),UZ=a("p"),d_o=o("Examples:"),c_o=l(),f(iE.$$.fragment),f_o=l(),Se=a("div"),f(dE.$$.fragment),m_o=l(),JZ=a("p"),g_o=o("Instantiate one of the model classes of the library (with a masked language modeling head) from a pretrained model."),h_o=l(),Oa=a("p"),p_o=o("The model class to instantiate is selected based on the "),YZ=a("code"),__o=o("model_type"),u_o=o(` property of the config object (either
passed as an argument or loaded from `),KZ=a("code"),b_o=o("pretrained_model_name_or_path"),v_o=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),ZZ=a("code"),T_o=o("pretrained_model_name_or_path"),F_o=o(":"),C_o=l(),I=a("ul"),Mu=a("li"),eee=a("strong"),M_o=o("albert"),E_o=o(" \u2014 "),mP=a("a"),y_o=o("AlbertForMaskedLM"),w_o=o(" (ALBERT model)"),A_o=l(),Eu=a("li"),oee=a("strong"),L_o=o("bart"),B_o=o(" \u2014 "),gP=a("a"),k_o=o("BartForConditionalGeneration"),x_o=o(" (BART model)"),R_o=l(),yu=a("li"),ree=a("strong"),S_o=o("bert"),P_o=o(" \u2014 "),hP=a("a"),$_o=o("BertForMaskedLM"),I_o=o(" (BERT model)"),j_o=l(),wu=a("li"),tee=a("strong"),N_o=o("big_bird"),D_o=o(" \u2014 "),pP=a("a"),q_o=o("BigBirdForMaskedLM"),G_o=o(" (BigBird model)"),O_o=l(),Au=a("li"),aee=a("strong"),X_o=o("camembert"),z_o=o(" \u2014 "),_P=a("a"),V_o=o("CamembertForMaskedLM"),W_o=o(" (CamemBERT model)"),Q_o=l(),Lu=a("li"),nee=a("strong"),H_o=o("convbert"),U_o=o(" \u2014 "),uP=a("a"),J_o=o("ConvBertForMaskedLM"),Y_o=o(" (ConvBERT model)"),K_o=l(),Bu=a("li"),see=a("strong"),Z_o=o("deberta"),euo=o(" \u2014 "),bP=a("a"),ouo=o("DebertaForMaskedLM"),ruo=o(" (DeBERTa model)"),tuo=l(),ku=a("li"),lee=a("strong"),auo=o("deberta-v2"),nuo=o(" \u2014 "),vP=a("a"),suo=o("DebertaV2ForMaskedLM"),luo=o(" (DeBERTa-v2 model)"),iuo=l(),xu=a("li"),iee=a("strong"),duo=o("distilbert"),cuo=o(" \u2014 "),TP=a("a"),fuo=o("DistilBertForMaskedLM"),muo=o(" (DistilBERT model)"),guo=l(),Ru=a("li"),dee=a("strong"),huo=o("electra"),puo=o(" \u2014 "),FP=a("a"),_uo=o("ElectraForMaskedLM"),uuo=o(" (ELECTRA model)"),buo=l(),Su=a("li"),cee=a("strong"),vuo=o("flaubert"),Tuo=o(" \u2014 "),CP=a("a"),Fuo=o("FlaubertWithLMHeadModel"),Cuo=o(" (FlauBERT model)"),Muo=l(),Pu=a("li"),fee=a("strong"),Euo=o("fnet"),yuo=o(" \u2014 "),MP=a("a"),wuo=o("FNetForMaskedLM"),Auo=o(" (FNet model)"),Luo=l(),$u=a("li"),mee=a("strong"),Buo=o("funnel"),kuo=o(" \u2014 "),EP=a("a"),xuo=o("FunnelForMaskedLM"),Ruo=o(" (Funnel Transformer model)"),Suo=l(),Iu=a("li"),gee=a("strong"),Puo=o("ibert"),$uo=o(" \u2014 "),yP=a("a"),Iuo=o("IBertForMaskedLM"),juo=o(" (I-BERT model)"),Nuo=l(),ju=a("li"),hee=a("strong"),Duo=o("layoutlm"),quo=o(" \u2014 "),wP=a("a"),Guo=o("LayoutLMForMaskedLM"),Ouo=o(" (LayoutLM model)"),Xuo=l(),Nu=a("li"),pee=a("strong"),zuo=o("longformer"),Vuo=o(" \u2014 "),AP=a("a"),Wuo=o("LongformerForMaskedLM"),Quo=o(" (Longformer model)"),Huo=l(),Du=a("li"),_ee=a("strong"),Uuo=o("mbart"),Juo=o(" \u2014 "),LP=a("a"),Yuo=o("MBartForConditionalGeneration"),Kuo=o(" (mBART model)"),Zuo=l(),qu=a("li"),uee=a("strong"),e2o=o("megatron-bert"),o2o=o(" \u2014 "),BP=a("a"),r2o=o("MegatronBertForMaskedLM"),t2o=o(" (MegatronBert model)"),a2o=l(),Gu=a("li"),bee=a("strong"),n2o=o("mobilebert"),s2o=o(" \u2014 "),kP=a("a"),l2o=o("MobileBertForMaskedLM"),i2o=o(" (MobileBERT model)"),d2o=l(),Ou=a("li"),vee=a("strong"),c2o=o("mpnet"),f2o=o(" \u2014 "),xP=a("a"),m2o=o("MPNetForMaskedLM"),g2o=o(" (MPNet model)"),h2o=l(),Xu=a("li"),Tee=a("strong"),p2o=o("nystromformer"),_2o=o(" \u2014 "),RP=a("a"),u2o=o("NystromformerForMaskedLM"),b2o=o(" (Nystromformer model)"),v2o=l(),zu=a("li"),Fee=a("strong"),T2o=o("perceiver"),F2o=o(" \u2014 "),SP=a("a"),C2o=o("PerceiverForMaskedLM"),M2o=o(" (Perceiver model)"),E2o=l(),Vu=a("li"),Cee=a("strong"),y2o=o("qdqbert"),w2o=o(" \u2014 "),PP=a("a"),A2o=o("QDQBertForMaskedLM"),L2o=o(" (QDQBert model)"),B2o=l(),Wu=a("li"),Mee=a("strong"),k2o=o("reformer"),x2o=o(" \u2014 "),$P=a("a"),R2o=o("ReformerForMaskedLM"),S2o=o(" (Reformer model)"),P2o=l(),Qu=a("li"),Eee=a("strong"),$2o=o("rembert"),I2o=o(" \u2014 "),IP=a("a"),j2o=o("RemBertForMaskedLM"),N2o=o(" (RemBERT model)"),D2o=l(),Hu=a("li"),yee=a("strong"),q2o=o("roberta"),G2o=o(" \u2014 "),jP=a("a"),O2o=o("RobertaForMaskedLM"),X2o=o(" (RoBERTa model)"),z2o=l(),Uu=a("li"),wee=a("strong"),V2o=o("roformer"),W2o=o(" \u2014 "),NP=a("a"),Q2o=o("RoFormerForMaskedLM"),H2o=o(" (RoFormer model)"),U2o=l(),Ju=a("li"),Aee=a("strong"),J2o=o("squeezebert"),Y2o=o(" \u2014 "),DP=a("a"),K2o=o("SqueezeBertForMaskedLM"),Z2o=o(" (SqueezeBERT model)"),e1o=l(),Yu=a("li"),Lee=a("strong"),o1o=o("tapas"),r1o=o(" \u2014 "),qP=a("a"),t1o=o("TapasForMaskedLM"),a1o=o(" (TAPAS model)"),n1o=l(),Ku=a("li"),Bee=a("strong"),s1o=o("wav2vec2"),l1o=o(" \u2014 "),kee=a("code"),i1o=o("Wav2Vec2ForMaskedLM"),d1o=o("(Wav2Vec2 model)"),c1o=l(),Zu=a("li"),xee=a("strong"),f1o=o("xlm"),m1o=o(" \u2014 "),GP=a("a"),g1o=o("XLMWithLMHeadModel"),h1o=o(" (XLM model)"),p1o=l(),e2=a("li"),Ree=a("strong"),_1o=o("xlm-roberta"),u1o=o(" \u2014 "),OP=a("a"),b1o=o("XLMRobertaForMaskedLM"),v1o=o(" (XLM-RoBERTa model)"),T1o=l(),o2=a("li"),See=a("strong"),F1o=o("xlm-roberta-xl"),C1o=o(" \u2014 "),XP=a("a"),M1o=o("XLMRobertaXLForMaskedLM"),E1o=o(" (XLM-RoBERTa-XL model)"),y1o=l(),r2=a("li"),Pee=a("strong"),w1o=o("yoso"),A1o=o(" \u2014 "),zP=a("a"),L1o=o("YosoForMaskedLM"),B1o=o(" (YOSO model)"),k1o=l(),t2=a("p"),x1o=o("The model is set in evaluation mode by default using "),$ee=a("code"),R1o=o("model.eval()"),S1o=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Iee=a("code"),P1o=o("model.train()"),$1o=l(),jee=a("p"),I1o=o("Examples:"),j1o=l(),f(cE.$$.fragment),R7e=l(),Ki=a("h2"),a2=a("a"),Nee=a("span"),f(fE.$$.fragment),N1o=l(),Dee=a("span"),D1o=o("AutoModelForSeq2SeqLM"),S7e=l(),Uo=a("div"),f(mE.$$.fragment),q1o=l(),Zi=a("p"),G1o=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence language modeling head) when created
with the `),qee=a("code"),O1o=o("from_pretrained()"),X1o=o("class method or the "),Gee=a("code"),z1o=o("from_config()"),V1o=o(`class
method.`),W1o=l(),gE=a("p"),Q1o=o("This class cannot be instantiated directly using "),Oee=a("code"),H1o=o("__init__()"),U1o=o(" (throws an error)."),J1o=l(),Or=a("div"),f(hE.$$.fragment),Y1o=l(),Xee=a("p"),K1o=o("Instantiates one of the model classes of the library (with a sequence-to-sequence language modeling head) from a configuration."),Z1o=l(),ed=a("p"),ebo=o(`Note:
Loading a model from its configuration file does `),zee=a("strong"),obo=o("not"),rbo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Vee=a("code"),tbo=o("from_pretrained()"),abo=o("to load the model weights."),nbo=l(),Wee=a("p"),sbo=o("Examples:"),lbo=l(),f(pE.$$.fragment),ibo=l(),Pe=a("div"),f(_E.$$.fragment),dbo=l(),Qee=a("p"),cbo=o("Instantiate one of the model classes of the library (with a sequence-to-sequence language modeling head) from a pretrained model."),fbo=l(),Xa=a("p"),mbo=o("The model class to instantiate is selected based on the "),Hee=a("code"),gbo=o("model_type"),hbo=o(` property of the config object (either
passed as an argument or loaded from `),Uee=a("code"),pbo=o("pretrained_model_name_or_path"),_bo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Jee=a("code"),ubo=o("pretrained_model_name_or_path"),bbo=o(":"),vbo=l(),ae=a("ul"),n2=a("li"),Yee=a("strong"),Tbo=o("bart"),Fbo=o(" \u2014 "),VP=a("a"),Cbo=o("BartForConditionalGeneration"),Mbo=o(" (BART model)"),Ebo=l(),s2=a("li"),Kee=a("strong"),ybo=o("bigbird_pegasus"),wbo=o(" \u2014 "),WP=a("a"),Abo=o("BigBirdPegasusForConditionalGeneration"),Lbo=o(" (BigBirdPegasus model)"),Bbo=l(),l2=a("li"),Zee=a("strong"),kbo=o("blenderbot"),xbo=o(" \u2014 "),QP=a("a"),Rbo=o("BlenderbotForConditionalGeneration"),Sbo=o(" (Blenderbot model)"),Pbo=l(),i2=a("li"),eoe=a("strong"),$bo=o("blenderbot-small"),Ibo=o(" \u2014 "),HP=a("a"),jbo=o("BlenderbotSmallForConditionalGeneration"),Nbo=o(" (BlenderbotSmall model)"),Dbo=l(),d2=a("li"),ooe=a("strong"),qbo=o("encoder-decoder"),Gbo=o(" \u2014 "),UP=a("a"),Obo=o("EncoderDecoderModel"),Xbo=o(" (Encoder decoder model)"),zbo=l(),c2=a("li"),roe=a("strong"),Vbo=o("fsmt"),Wbo=o(" \u2014 "),JP=a("a"),Qbo=o("FSMTForConditionalGeneration"),Hbo=o(" (FairSeq Machine-Translation model)"),Ubo=l(),f2=a("li"),toe=a("strong"),Jbo=o("led"),Ybo=o(" \u2014 "),YP=a("a"),Kbo=o("LEDForConditionalGeneration"),Zbo=o(" (LED model)"),e5o=l(),m2=a("li"),aoe=a("strong"),o5o=o("m2m_100"),r5o=o(" \u2014 "),KP=a("a"),t5o=o("M2M100ForConditionalGeneration"),a5o=o(" (M2M100 model)"),n5o=l(),g2=a("li"),noe=a("strong"),s5o=o("marian"),l5o=o(" \u2014 "),ZP=a("a"),i5o=o("MarianMTModel"),d5o=o(" (Marian model)"),c5o=l(),h2=a("li"),soe=a("strong"),f5o=o("mbart"),m5o=o(" \u2014 "),e$=a("a"),g5o=o("MBartForConditionalGeneration"),h5o=o(" (mBART model)"),p5o=l(),p2=a("li"),loe=a("strong"),_5o=o("mt5"),u5o=o(" \u2014 "),o$=a("a"),b5o=o("MT5ForConditionalGeneration"),v5o=o(" (mT5 model)"),T5o=l(),_2=a("li"),ioe=a("strong"),F5o=o("pegasus"),C5o=o(" \u2014 "),r$=a("a"),M5o=o("PegasusForConditionalGeneration"),E5o=o(" (Pegasus model)"),y5o=l(),u2=a("li"),doe=a("strong"),w5o=o("plbart"),A5o=o(" \u2014 "),t$=a("a"),L5o=o("PLBartForConditionalGeneration"),B5o=o(" (PLBart model)"),k5o=l(),b2=a("li"),coe=a("strong"),x5o=o("prophetnet"),R5o=o(" \u2014 "),a$=a("a"),S5o=o("ProphetNetForConditionalGeneration"),P5o=o(" (ProphetNet model)"),$5o=l(),v2=a("li"),foe=a("strong"),I5o=o("t5"),j5o=o(" \u2014 "),n$=a("a"),N5o=o("T5ForConditionalGeneration"),D5o=o(" (T5 model)"),q5o=l(),T2=a("li"),moe=a("strong"),G5o=o("xlm-prophetnet"),O5o=o(" \u2014 "),s$=a("a"),X5o=o("XLMProphetNetForConditionalGeneration"),z5o=o(" (XLMProphetNet model)"),V5o=l(),F2=a("p"),W5o=o("The model is set in evaluation mode by default using "),goe=a("code"),Q5o=o("model.eval()"),H5o=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),hoe=a("code"),U5o=o("model.train()"),J5o=l(),poe=a("p"),Y5o=o("Examples:"),K5o=l(),f(uE.$$.fragment),P7e=l(),od=a("h2"),C2=a("a"),_oe=a("span"),f(bE.$$.fragment),Z5o=l(),uoe=a("span"),evo=o("AutoModelForSequenceClassification"),$7e=l(),Jo=a("div"),f(vE.$$.fragment),ovo=l(),rd=a("p"),rvo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence classification head) when created
with the `),boe=a("code"),tvo=o("from_pretrained()"),avo=o("class method or the "),voe=a("code"),nvo=o("from_config()"),svo=o(`class
method.`),lvo=l(),TE=a("p"),ivo=o("This class cannot be instantiated directly using "),Toe=a("code"),dvo=o("__init__()"),cvo=o(" (throws an error)."),fvo=l(),Xr=a("div"),f(FE.$$.fragment),mvo=l(),Foe=a("p"),gvo=o("Instantiates one of the model classes of the library (with a sequence classification head) from a configuration."),hvo=l(),td=a("p"),pvo=o(`Note:
Loading a model from its configuration file does `),Coe=a("strong"),_vo=o("not"),uvo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Moe=a("code"),bvo=o("from_pretrained()"),vvo=o("to load the model weights."),Tvo=l(),Eoe=a("p"),Fvo=o("Examples:"),Cvo=l(),f(CE.$$.fragment),Mvo=l(),$e=a("div"),f(ME.$$.fragment),Evo=l(),yoe=a("p"),yvo=o("Instantiate one of the model classes of the library (with a sequence classification head) from a pretrained model."),wvo=l(),za=a("p"),Avo=o("The model class to instantiate is selected based on the "),woe=a("code"),Lvo=o("model_type"),Bvo=o(` property of the config object (either
passed as an argument or loaded from `),Aoe=a("code"),kvo=o("pretrained_model_name_or_path"),xvo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Loe=a("code"),Rvo=o("pretrained_model_name_or_path"),Svo=o(":"),Pvo=l(),A=a("ul"),M2=a("li"),Boe=a("strong"),$vo=o("albert"),Ivo=o(" \u2014 "),l$=a("a"),jvo=o("AlbertForSequenceClassification"),Nvo=o(" (ALBERT model)"),Dvo=l(),E2=a("li"),koe=a("strong"),qvo=o("bart"),Gvo=o(" \u2014 "),i$=a("a"),Ovo=o("BartForSequenceClassification"),Xvo=o(" (BART model)"),zvo=l(),y2=a("li"),xoe=a("strong"),Vvo=o("bert"),Wvo=o(" \u2014 "),d$=a("a"),Qvo=o("BertForSequenceClassification"),Hvo=o(" (BERT model)"),Uvo=l(),w2=a("li"),Roe=a("strong"),Jvo=o("big_bird"),Yvo=o(" \u2014 "),c$=a("a"),Kvo=o("BigBirdForSequenceClassification"),Zvo=o(" (BigBird model)"),e6o=l(),A2=a("li"),Soe=a("strong"),o6o=o("bigbird_pegasus"),r6o=o(" \u2014 "),f$=a("a"),t6o=o("BigBirdPegasusForSequenceClassification"),a6o=o(" (BigBirdPegasus model)"),n6o=l(),L2=a("li"),Poe=a("strong"),s6o=o("camembert"),l6o=o(" \u2014 "),m$=a("a"),i6o=o("CamembertForSequenceClassification"),d6o=o(" (CamemBERT model)"),c6o=l(),B2=a("li"),$oe=a("strong"),f6o=o("canine"),m6o=o(" \u2014 "),g$=a("a"),g6o=o("CanineForSequenceClassification"),h6o=o(" (Canine model)"),p6o=l(),k2=a("li"),Ioe=a("strong"),_6o=o("convbert"),u6o=o(" \u2014 "),h$=a("a"),b6o=o("ConvBertForSequenceClassification"),v6o=o(" (ConvBERT model)"),T6o=l(),x2=a("li"),joe=a("strong"),F6o=o("ctrl"),C6o=o(" \u2014 "),p$=a("a"),M6o=o("CTRLForSequenceClassification"),E6o=o(" (CTRL model)"),y6o=l(),R2=a("li"),Noe=a("strong"),w6o=o("deberta"),A6o=o(" \u2014 "),_$=a("a"),L6o=o("DebertaForSequenceClassification"),B6o=o(" (DeBERTa model)"),k6o=l(),S2=a("li"),Doe=a("strong"),x6o=o("deberta-v2"),R6o=o(" \u2014 "),u$=a("a"),S6o=o("DebertaV2ForSequenceClassification"),P6o=o(" (DeBERTa-v2 model)"),$6o=l(),P2=a("li"),qoe=a("strong"),I6o=o("distilbert"),j6o=o(" \u2014 "),b$=a("a"),N6o=o("DistilBertForSequenceClassification"),D6o=o(" (DistilBERT model)"),q6o=l(),$2=a("li"),Goe=a("strong"),G6o=o("electra"),O6o=o(" \u2014 "),v$=a("a"),X6o=o("ElectraForSequenceClassification"),z6o=o(" (ELECTRA model)"),V6o=l(),I2=a("li"),Ooe=a("strong"),W6o=o("flaubert"),Q6o=o(" \u2014 "),T$=a("a"),H6o=o("FlaubertForSequenceClassification"),U6o=o(" (FlauBERT model)"),J6o=l(),j2=a("li"),Xoe=a("strong"),Y6o=o("fnet"),K6o=o(" \u2014 "),F$=a("a"),Z6o=o("FNetForSequenceClassification"),eTo=o(" (FNet model)"),oTo=l(),N2=a("li"),zoe=a("strong"),rTo=o("funnel"),tTo=o(" \u2014 "),C$=a("a"),aTo=o("FunnelForSequenceClassification"),nTo=o(" (Funnel Transformer model)"),sTo=l(),D2=a("li"),Voe=a("strong"),lTo=o("gpt2"),iTo=o(" \u2014 "),M$=a("a"),dTo=o("GPT2ForSequenceClassification"),cTo=o(" (OpenAI GPT-2 model)"),fTo=l(),q2=a("li"),Woe=a("strong"),mTo=o("gpt_neo"),gTo=o(" \u2014 "),E$=a("a"),hTo=o("GPTNeoForSequenceClassification"),pTo=o(" (GPT Neo model)"),_To=l(),G2=a("li"),Qoe=a("strong"),uTo=o("gptj"),bTo=o(" \u2014 "),y$=a("a"),vTo=o("GPTJForSequenceClassification"),TTo=o(" (GPT-J model)"),FTo=l(),O2=a("li"),Hoe=a("strong"),CTo=o("ibert"),MTo=o(" \u2014 "),w$=a("a"),ETo=o("IBertForSequenceClassification"),yTo=o(" (I-BERT model)"),wTo=l(),X2=a("li"),Uoe=a("strong"),ATo=o("layoutlm"),LTo=o(" \u2014 "),A$=a("a"),BTo=o("LayoutLMForSequenceClassification"),kTo=o(" (LayoutLM model)"),xTo=l(),z2=a("li"),Joe=a("strong"),RTo=o("layoutlmv2"),STo=o(" \u2014 "),L$=a("a"),PTo=o("LayoutLMv2ForSequenceClassification"),$To=o(" (LayoutLMv2 model)"),ITo=l(),V2=a("li"),Yoe=a("strong"),jTo=o("led"),NTo=o(" \u2014 "),B$=a("a"),DTo=o("LEDForSequenceClassification"),qTo=o(" (LED model)"),GTo=l(),W2=a("li"),Koe=a("strong"),OTo=o("longformer"),XTo=o(" \u2014 "),k$=a("a"),zTo=o("LongformerForSequenceClassification"),VTo=o(" (Longformer model)"),WTo=l(),Q2=a("li"),Zoe=a("strong"),QTo=o("mbart"),HTo=o(" \u2014 "),x$=a("a"),UTo=o("MBartForSequenceClassification"),JTo=o(" (mBART model)"),YTo=l(),H2=a("li"),ere=a("strong"),KTo=o("megatron-bert"),ZTo=o(" \u2014 "),R$=a("a"),e8o=o("MegatronBertForSequenceClassification"),o8o=o(" (MegatronBert model)"),r8o=l(),U2=a("li"),ore=a("strong"),t8o=o("mobilebert"),a8o=o(" \u2014 "),S$=a("a"),n8o=o("MobileBertForSequenceClassification"),s8o=o(" (MobileBERT model)"),l8o=l(),J2=a("li"),rre=a("strong"),i8o=o("mpnet"),d8o=o(" \u2014 "),P$=a("a"),c8o=o("MPNetForSequenceClassification"),f8o=o(" (MPNet model)"),m8o=l(),Y2=a("li"),tre=a("strong"),g8o=o("nystromformer"),h8o=o(" \u2014 "),$$=a("a"),p8o=o("NystromformerForSequenceClassification"),_8o=o(" (Nystromformer model)"),u8o=l(),K2=a("li"),are=a("strong"),b8o=o("openai-gpt"),v8o=o(" \u2014 "),I$=a("a"),T8o=o("OpenAIGPTForSequenceClassification"),F8o=o(" (OpenAI GPT model)"),C8o=l(),Z2=a("li"),nre=a("strong"),M8o=o("perceiver"),E8o=o(" \u2014 "),j$=a("a"),y8o=o("PerceiverForSequenceClassification"),w8o=o(" (Perceiver model)"),A8o=l(),e1=a("li"),sre=a("strong"),L8o=o("plbart"),B8o=o(" \u2014 "),N$=a("a"),k8o=o("PLBartForSequenceClassification"),x8o=o(" (PLBart model)"),R8o=l(),o1=a("li"),lre=a("strong"),S8o=o("qdqbert"),P8o=o(" \u2014 "),D$=a("a"),$8o=o("QDQBertForSequenceClassification"),I8o=o(" (QDQBert model)"),j8o=l(),r1=a("li"),ire=a("strong"),N8o=o("reformer"),D8o=o(" \u2014 "),q$=a("a"),q8o=o("ReformerForSequenceClassification"),G8o=o(" (Reformer model)"),O8o=l(),t1=a("li"),dre=a("strong"),X8o=o("rembert"),z8o=o(" \u2014 "),G$=a("a"),V8o=o("RemBertForSequenceClassification"),W8o=o(" (RemBERT model)"),Q8o=l(),a1=a("li"),cre=a("strong"),H8o=o("roberta"),U8o=o(" \u2014 "),O$=a("a"),J8o=o("RobertaForSequenceClassification"),Y8o=o(" (RoBERTa model)"),K8o=l(),n1=a("li"),fre=a("strong"),Z8o=o("roformer"),eFo=o(" \u2014 "),X$=a("a"),oFo=o("RoFormerForSequenceClassification"),rFo=o(" (RoFormer model)"),tFo=l(),s1=a("li"),mre=a("strong"),aFo=o("squeezebert"),nFo=o(" \u2014 "),z$=a("a"),sFo=o("SqueezeBertForSequenceClassification"),lFo=o(" (SqueezeBERT model)"),iFo=l(),l1=a("li"),gre=a("strong"),dFo=o("tapas"),cFo=o(" \u2014 "),V$=a("a"),fFo=o("TapasForSequenceClassification"),mFo=o(" (TAPAS model)"),gFo=l(),i1=a("li"),hre=a("strong"),hFo=o("transfo-xl"),pFo=o(" \u2014 "),W$=a("a"),_Fo=o("TransfoXLForSequenceClassification"),uFo=o(" (Transformer-XL model)"),bFo=l(),d1=a("li"),pre=a("strong"),vFo=o("xlm"),TFo=o(" \u2014 "),Q$=a("a"),FFo=o("XLMForSequenceClassification"),CFo=o(" (XLM model)"),MFo=l(),c1=a("li"),_re=a("strong"),EFo=o("xlm-roberta"),yFo=o(" \u2014 "),H$=a("a"),wFo=o("XLMRobertaForSequenceClassification"),AFo=o(" (XLM-RoBERTa model)"),LFo=l(),f1=a("li"),ure=a("strong"),BFo=o("xlm-roberta-xl"),kFo=o(" \u2014 "),U$=a("a"),xFo=o("XLMRobertaXLForSequenceClassification"),RFo=o(" (XLM-RoBERTa-XL model)"),SFo=l(),m1=a("li"),bre=a("strong"),PFo=o("xlnet"),$Fo=o(" \u2014 "),J$=a("a"),IFo=o("XLNetForSequenceClassification"),jFo=o(" (XLNet model)"),NFo=l(),g1=a("li"),vre=a("strong"),DFo=o("yoso"),qFo=o(" \u2014 "),Y$=a("a"),GFo=o("YosoForSequenceClassification"),OFo=o(" (YOSO model)"),XFo=l(),h1=a("p"),zFo=o("The model is set in evaluation mode by default using "),Tre=a("code"),VFo=o("model.eval()"),WFo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Fre=a("code"),QFo=o("model.train()"),HFo=l(),Cre=a("p"),UFo=o("Examples:"),JFo=l(),f(EE.$$.fragment),I7e=l(),ad=a("h2"),p1=a("a"),Mre=a("span"),f(yE.$$.fragment),YFo=l(),Ere=a("span"),KFo=o("AutoModelForMultipleChoice"),j7e=l(),Yo=a("div"),f(wE.$$.fragment),ZFo=l(),nd=a("p"),eCo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a multiple choice head) when created
with the `),yre=a("code"),oCo=o("from_pretrained()"),rCo=o("class method or the "),wre=a("code"),tCo=o("from_config()"),aCo=o(`class
method.`),nCo=l(),AE=a("p"),sCo=o("This class cannot be instantiated directly using "),Are=a("code"),lCo=o("__init__()"),iCo=o(" (throws an error)."),dCo=l(),zr=a("div"),f(LE.$$.fragment),cCo=l(),Lre=a("p"),fCo=o("Instantiates one of the model classes of the library (with a multiple choice head) from a configuration."),mCo=l(),sd=a("p"),gCo=o(`Note:
Loading a model from its configuration file does `),Bre=a("strong"),hCo=o("not"),pCo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),kre=a("code"),_Co=o("from_pretrained()"),uCo=o("to load the model weights."),bCo=l(),xre=a("p"),vCo=o("Examples:"),TCo=l(),f(BE.$$.fragment),FCo=l(),Ie=a("div"),f(kE.$$.fragment),CCo=l(),Rre=a("p"),MCo=o("Instantiate one of the model classes of the library (with a multiple choice head) from a pretrained model."),ECo=l(),Va=a("p"),yCo=o("The model class to instantiate is selected based on the "),Sre=a("code"),wCo=o("model_type"),ACo=o(` property of the config object (either
passed as an argument or loaded from `),Pre=a("code"),LCo=o("pretrained_model_name_or_path"),BCo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),$re=a("code"),kCo=o("pretrained_model_name_or_path"),xCo=o(":"),RCo=l(),G=a("ul"),_1=a("li"),Ire=a("strong"),SCo=o("albert"),PCo=o(" \u2014 "),K$=a("a"),$Co=o("AlbertForMultipleChoice"),ICo=o(" (ALBERT model)"),jCo=l(),u1=a("li"),jre=a("strong"),NCo=o("bert"),DCo=o(" \u2014 "),Z$=a("a"),qCo=o("BertForMultipleChoice"),GCo=o(" (BERT model)"),OCo=l(),b1=a("li"),Nre=a("strong"),XCo=o("big_bird"),zCo=o(" \u2014 "),eI=a("a"),VCo=o("BigBirdForMultipleChoice"),WCo=o(" (BigBird model)"),QCo=l(),v1=a("li"),Dre=a("strong"),HCo=o("camembert"),UCo=o(" \u2014 "),oI=a("a"),JCo=o("CamembertForMultipleChoice"),YCo=o(" (CamemBERT model)"),KCo=l(),T1=a("li"),qre=a("strong"),ZCo=o("canine"),e4o=o(" \u2014 "),rI=a("a"),o4o=o("CanineForMultipleChoice"),r4o=o(" (Canine model)"),t4o=l(),F1=a("li"),Gre=a("strong"),a4o=o("convbert"),n4o=o(" \u2014 "),tI=a("a"),s4o=o("ConvBertForMultipleChoice"),l4o=o(" (ConvBERT model)"),i4o=l(),C1=a("li"),Ore=a("strong"),d4o=o("distilbert"),c4o=o(" \u2014 "),aI=a("a"),f4o=o("DistilBertForMultipleChoice"),m4o=o(" (DistilBERT model)"),g4o=l(),M1=a("li"),Xre=a("strong"),h4o=o("electra"),p4o=o(" \u2014 "),nI=a("a"),_4o=o("ElectraForMultipleChoice"),u4o=o(" (ELECTRA model)"),b4o=l(),E1=a("li"),zre=a("strong"),v4o=o("flaubert"),T4o=o(" \u2014 "),sI=a("a"),F4o=o("FlaubertForMultipleChoice"),C4o=o(" (FlauBERT model)"),M4o=l(),y1=a("li"),Vre=a("strong"),E4o=o("fnet"),y4o=o(" \u2014 "),lI=a("a"),w4o=o("FNetForMultipleChoice"),A4o=o(" (FNet model)"),L4o=l(),w1=a("li"),Wre=a("strong"),B4o=o("funnel"),k4o=o(" \u2014 "),iI=a("a"),x4o=o("FunnelForMultipleChoice"),R4o=o(" (Funnel Transformer model)"),S4o=l(),A1=a("li"),Qre=a("strong"),P4o=o("ibert"),$4o=o(" \u2014 "),dI=a("a"),I4o=o("IBertForMultipleChoice"),j4o=o(" (I-BERT model)"),N4o=l(),L1=a("li"),Hre=a("strong"),D4o=o("longformer"),q4o=o(" \u2014 "),cI=a("a"),G4o=o("LongformerForMultipleChoice"),O4o=o(" (Longformer model)"),X4o=l(),B1=a("li"),Ure=a("strong"),z4o=o("megatron-bert"),V4o=o(" \u2014 "),fI=a("a"),W4o=o("MegatronBertForMultipleChoice"),Q4o=o(" (MegatronBert model)"),H4o=l(),k1=a("li"),Jre=a("strong"),U4o=o("mobilebert"),J4o=o(" \u2014 "),mI=a("a"),Y4o=o("MobileBertForMultipleChoice"),K4o=o(" (MobileBERT model)"),Z4o=l(),x1=a("li"),Yre=a("strong"),eMo=o("mpnet"),oMo=o(" \u2014 "),gI=a("a"),rMo=o("MPNetForMultipleChoice"),tMo=o(" (MPNet model)"),aMo=l(),R1=a("li"),Kre=a("strong"),nMo=o("nystromformer"),sMo=o(" \u2014 "),hI=a("a"),lMo=o("NystromformerForMultipleChoice"),iMo=o(" (Nystromformer model)"),dMo=l(),S1=a("li"),Zre=a("strong"),cMo=o("qdqbert"),fMo=o(" \u2014 "),pI=a("a"),mMo=o("QDQBertForMultipleChoice"),gMo=o(" (QDQBert model)"),hMo=l(),P1=a("li"),ete=a("strong"),pMo=o("rembert"),_Mo=o(" \u2014 "),_I=a("a"),uMo=o("RemBertForMultipleChoice"),bMo=o(" (RemBERT model)"),vMo=l(),$1=a("li"),ote=a("strong"),TMo=o("roberta"),FMo=o(" \u2014 "),uI=a("a"),CMo=o("RobertaForMultipleChoice"),MMo=o(" (RoBERTa model)"),EMo=l(),I1=a("li"),rte=a("strong"),yMo=o("roformer"),wMo=o(" \u2014 "),bI=a("a"),AMo=o("RoFormerForMultipleChoice"),LMo=o(" (RoFormer model)"),BMo=l(),j1=a("li"),tte=a("strong"),kMo=o("squeezebert"),xMo=o(" \u2014 "),vI=a("a"),RMo=o("SqueezeBertForMultipleChoice"),SMo=o(" (SqueezeBERT model)"),PMo=l(),N1=a("li"),ate=a("strong"),$Mo=o("xlm"),IMo=o(" \u2014 "),TI=a("a"),jMo=o("XLMForMultipleChoice"),NMo=o(" (XLM model)"),DMo=l(),D1=a("li"),nte=a("strong"),qMo=o("xlm-roberta"),GMo=o(" \u2014 "),FI=a("a"),OMo=o("XLMRobertaForMultipleChoice"),XMo=o(" (XLM-RoBERTa model)"),zMo=l(),q1=a("li"),ste=a("strong"),VMo=o("xlm-roberta-xl"),WMo=o(" \u2014 "),CI=a("a"),QMo=o("XLMRobertaXLForMultipleChoice"),HMo=o(" (XLM-RoBERTa-XL model)"),UMo=l(),G1=a("li"),lte=a("strong"),JMo=o("xlnet"),YMo=o(" \u2014 "),MI=a("a"),KMo=o("XLNetForMultipleChoice"),ZMo=o(" (XLNet model)"),eEo=l(),O1=a("li"),ite=a("strong"),oEo=o("yoso"),rEo=o(" \u2014 "),EI=a("a"),tEo=o("YosoForMultipleChoice"),aEo=o(" (YOSO model)"),nEo=l(),X1=a("p"),sEo=o("The model is set in evaluation mode by default using "),dte=a("code"),lEo=o("model.eval()"),iEo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),cte=a("code"),dEo=o("model.train()"),cEo=l(),fte=a("p"),fEo=o("Examples:"),mEo=l(),f(xE.$$.fragment),N7e=l(),ld=a("h2"),z1=a("a"),mte=a("span"),f(RE.$$.fragment),gEo=l(),gte=a("span"),hEo=o("AutoModelForNextSentencePrediction"),D7e=l(),Ko=a("div"),f(SE.$$.fragment),pEo=l(),id=a("p"),_Eo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a next sentence prediction head) when created
with the `),hte=a("code"),uEo=o("from_pretrained()"),bEo=o("class method or the "),pte=a("code"),vEo=o("from_config()"),TEo=o(`class
method.`),FEo=l(),PE=a("p"),CEo=o("This class cannot be instantiated directly using "),_te=a("code"),MEo=o("__init__()"),EEo=o(" (throws an error)."),yEo=l(),Vr=a("div"),f($E.$$.fragment),wEo=l(),ute=a("p"),AEo=o("Instantiates one of the model classes of the library (with a next sentence prediction head) from a configuration."),LEo=l(),dd=a("p"),BEo=o(`Note:
Loading a model from its configuration file does `),bte=a("strong"),kEo=o("not"),xEo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),vte=a("code"),REo=o("from_pretrained()"),SEo=o("to load the model weights."),PEo=l(),Tte=a("p"),$Eo=o("Examples:"),IEo=l(),f(IE.$$.fragment),jEo=l(),je=a("div"),f(jE.$$.fragment),NEo=l(),Fte=a("p"),DEo=o("Instantiate one of the model classes of the library (with a next sentence prediction head) from a pretrained model."),qEo=l(),Wa=a("p"),GEo=o("The model class to instantiate is selected based on the "),Cte=a("code"),OEo=o("model_type"),XEo=o(` property of the config object (either
passed as an argument or loaded from `),Mte=a("code"),zEo=o("pretrained_model_name_or_path"),VEo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Ete=a("code"),WEo=o("pretrained_model_name_or_path"),QEo=o(":"),HEo=l(),na=a("ul"),V1=a("li"),yte=a("strong"),UEo=o("bert"),JEo=o(" \u2014 "),yI=a("a"),YEo=o("BertForNextSentencePrediction"),KEo=o(" (BERT model)"),ZEo=l(),W1=a("li"),wte=a("strong"),e3o=o("fnet"),o3o=o(" \u2014 "),wI=a("a"),r3o=o("FNetForNextSentencePrediction"),t3o=o(" (FNet model)"),a3o=l(),Q1=a("li"),Ate=a("strong"),n3o=o("megatron-bert"),s3o=o(" \u2014 "),AI=a("a"),l3o=o("MegatronBertForNextSentencePrediction"),i3o=o(" (MegatronBert model)"),d3o=l(),H1=a("li"),Lte=a("strong"),c3o=o("mobilebert"),f3o=o(" \u2014 "),LI=a("a"),m3o=o("MobileBertForNextSentencePrediction"),g3o=o(" (MobileBERT model)"),h3o=l(),U1=a("li"),Bte=a("strong"),p3o=o("qdqbert"),_3o=o(" \u2014 "),BI=a("a"),u3o=o("QDQBertForNextSentencePrediction"),b3o=o(" (QDQBert model)"),v3o=l(),J1=a("p"),T3o=o("The model is set in evaluation mode by default using "),kte=a("code"),F3o=o("model.eval()"),C3o=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),xte=a("code"),M3o=o("model.train()"),E3o=l(),Rte=a("p"),y3o=o("Examples:"),w3o=l(),f(NE.$$.fragment),q7e=l(),cd=a("h2"),Y1=a("a"),Ste=a("span"),f(DE.$$.fragment),A3o=l(),Pte=a("span"),L3o=o("AutoModelForTokenClassification"),G7e=l(),Zo=a("div"),f(qE.$$.fragment),B3o=l(),fd=a("p"),k3o=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a token classification head) when created
with the `),$te=a("code"),x3o=o("from_pretrained()"),R3o=o("class method or the "),Ite=a("code"),S3o=o("from_config()"),P3o=o(`class
method.`),$3o=l(),GE=a("p"),I3o=o("This class cannot be instantiated directly using "),jte=a("code"),j3o=o("__init__()"),N3o=o(" (throws an error)."),D3o=l(),Wr=a("div"),f(OE.$$.fragment),q3o=l(),Nte=a("p"),G3o=o("Instantiates one of the model classes of the library (with a token classification head) from a configuration."),O3o=l(),md=a("p"),X3o=o(`Note:
Loading a model from its configuration file does `),Dte=a("strong"),z3o=o("not"),V3o=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),qte=a("code"),W3o=o("from_pretrained()"),Q3o=o("to load the model weights."),H3o=l(),Gte=a("p"),U3o=o("Examples:"),J3o=l(),f(XE.$$.fragment),Y3o=l(),Ne=a("div"),f(zE.$$.fragment),K3o=l(),Ote=a("p"),Z3o=o("Instantiate one of the model classes of the library (with a token classification head) from a pretrained model."),eyo=l(),Qa=a("p"),oyo=o("The model class to instantiate is selected based on the "),Xte=a("code"),ryo=o("model_type"),tyo=o(` property of the config object (either
passed as an argument or loaded from `),zte=a("code"),ayo=o("pretrained_model_name_or_path"),nyo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Vte=a("code"),syo=o("pretrained_model_name_or_path"),lyo=o(":"),iyo=l(),D=a("ul"),K1=a("li"),Wte=a("strong"),dyo=o("albert"),cyo=o(" \u2014 "),kI=a("a"),fyo=o("AlbertForTokenClassification"),myo=o(" (ALBERT model)"),gyo=l(),Z1=a("li"),Qte=a("strong"),hyo=o("bert"),pyo=o(" \u2014 "),xI=a("a"),_yo=o("BertForTokenClassification"),uyo=o(" (BERT model)"),byo=l(),eb=a("li"),Hte=a("strong"),vyo=o("big_bird"),Tyo=o(" \u2014 "),RI=a("a"),Fyo=o("BigBirdForTokenClassification"),Cyo=o(" (BigBird model)"),Myo=l(),ob=a("li"),Ute=a("strong"),Eyo=o("camembert"),yyo=o(" \u2014 "),SI=a("a"),wyo=o("CamembertForTokenClassification"),Ayo=o(" (CamemBERT model)"),Lyo=l(),rb=a("li"),Jte=a("strong"),Byo=o("canine"),kyo=o(" \u2014 "),PI=a("a"),xyo=o("CanineForTokenClassification"),Ryo=o(" (Canine model)"),Syo=l(),tb=a("li"),Yte=a("strong"),Pyo=o("convbert"),$yo=o(" \u2014 "),$I=a("a"),Iyo=o("ConvBertForTokenClassification"),jyo=o(" (ConvBERT model)"),Nyo=l(),ab=a("li"),Kte=a("strong"),Dyo=o("deberta"),qyo=o(" \u2014 "),II=a("a"),Gyo=o("DebertaForTokenClassification"),Oyo=o(" (DeBERTa model)"),Xyo=l(),nb=a("li"),Zte=a("strong"),zyo=o("deberta-v2"),Vyo=o(" \u2014 "),jI=a("a"),Wyo=o("DebertaV2ForTokenClassification"),Qyo=o(" (DeBERTa-v2 model)"),Hyo=l(),sb=a("li"),eae=a("strong"),Uyo=o("distilbert"),Jyo=o(" \u2014 "),NI=a("a"),Yyo=o("DistilBertForTokenClassification"),Kyo=o(" (DistilBERT model)"),Zyo=l(),lb=a("li"),oae=a("strong"),ewo=o("electra"),owo=o(" \u2014 "),DI=a("a"),rwo=o("ElectraForTokenClassification"),two=o(" (ELECTRA model)"),awo=l(),ib=a("li"),rae=a("strong"),nwo=o("flaubert"),swo=o(" \u2014 "),qI=a("a"),lwo=o("FlaubertForTokenClassification"),iwo=o(" (FlauBERT model)"),dwo=l(),db=a("li"),tae=a("strong"),cwo=o("fnet"),fwo=o(" \u2014 "),GI=a("a"),mwo=o("FNetForTokenClassification"),gwo=o(" (FNet model)"),hwo=l(),cb=a("li"),aae=a("strong"),pwo=o("funnel"),_wo=o(" \u2014 "),OI=a("a"),uwo=o("FunnelForTokenClassification"),bwo=o(" (Funnel Transformer model)"),vwo=l(),fb=a("li"),nae=a("strong"),Two=o("gpt2"),Fwo=o(" \u2014 "),XI=a("a"),Cwo=o("GPT2ForTokenClassification"),Mwo=o(" (OpenAI GPT-2 model)"),Ewo=l(),mb=a("li"),sae=a("strong"),ywo=o("ibert"),wwo=o(" \u2014 "),zI=a("a"),Awo=o("IBertForTokenClassification"),Lwo=o(" (I-BERT model)"),Bwo=l(),gb=a("li"),lae=a("strong"),kwo=o("layoutlm"),xwo=o(" \u2014 "),VI=a("a"),Rwo=o("LayoutLMForTokenClassification"),Swo=o(" (LayoutLM model)"),Pwo=l(),hb=a("li"),iae=a("strong"),$wo=o("layoutlmv2"),Iwo=o(" \u2014 "),WI=a("a"),jwo=o("LayoutLMv2ForTokenClassification"),Nwo=o(" (LayoutLMv2 model)"),Dwo=l(),pb=a("li"),dae=a("strong"),qwo=o("longformer"),Gwo=o(" \u2014 "),QI=a("a"),Owo=o("LongformerForTokenClassification"),Xwo=o(" (Longformer model)"),zwo=l(),_b=a("li"),cae=a("strong"),Vwo=o("megatron-bert"),Wwo=o(" \u2014 "),HI=a("a"),Qwo=o("MegatronBertForTokenClassification"),Hwo=o(" (MegatronBert model)"),Uwo=l(),ub=a("li"),fae=a("strong"),Jwo=o("mobilebert"),Ywo=o(" \u2014 "),UI=a("a"),Kwo=o("MobileBertForTokenClassification"),Zwo=o(" (MobileBERT model)"),eAo=l(),bb=a("li"),mae=a("strong"),oAo=o("mpnet"),rAo=o(" \u2014 "),JI=a("a"),tAo=o("MPNetForTokenClassification"),aAo=o(" (MPNet model)"),nAo=l(),vb=a("li"),gae=a("strong"),sAo=o("nystromformer"),lAo=o(" \u2014 "),YI=a("a"),iAo=o("NystromformerForTokenClassification"),dAo=o(" (Nystromformer model)"),cAo=l(),Tb=a("li"),hae=a("strong"),fAo=o("qdqbert"),mAo=o(" \u2014 "),KI=a("a"),gAo=o("QDQBertForTokenClassification"),hAo=o(" (QDQBert model)"),pAo=l(),Fb=a("li"),pae=a("strong"),_Ao=o("rembert"),uAo=o(" \u2014 "),ZI=a("a"),bAo=o("RemBertForTokenClassification"),vAo=o(" (RemBERT model)"),TAo=l(),Cb=a("li"),_ae=a("strong"),FAo=o("roberta"),CAo=o(" \u2014 "),ej=a("a"),MAo=o("RobertaForTokenClassification"),EAo=o(" (RoBERTa model)"),yAo=l(),Mb=a("li"),uae=a("strong"),wAo=o("roformer"),AAo=o(" \u2014 "),oj=a("a"),LAo=o("RoFormerForTokenClassification"),BAo=o(" (RoFormer model)"),kAo=l(),Eb=a("li"),bae=a("strong"),xAo=o("squeezebert"),RAo=o(" \u2014 "),rj=a("a"),SAo=o("SqueezeBertForTokenClassification"),PAo=o(" (SqueezeBERT model)"),$Ao=l(),yb=a("li"),vae=a("strong"),IAo=o("xlm"),jAo=o(" \u2014 "),tj=a("a"),NAo=o("XLMForTokenClassification"),DAo=o(" (XLM model)"),qAo=l(),wb=a("li"),Tae=a("strong"),GAo=o("xlm-roberta"),OAo=o(" \u2014 "),aj=a("a"),XAo=o("XLMRobertaForTokenClassification"),zAo=o(" (XLM-RoBERTa model)"),VAo=l(),Ab=a("li"),Fae=a("strong"),WAo=o("xlm-roberta-xl"),QAo=o(" \u2014 "),nj=a("a"),HAo=o("XLMRobertaXLForTokenClassification"),UAo=o(" (XLM-RoBERTa-XL model)"),JAo=l(),Lb=a("li"),Cae=a("strong"),YAo=o("xlnet"),KAo=o(" \u2014 "),sj=a("a"),ZAo=o("XLNetForTokenClassification"),e0o=o(" (XLNet model)"),o0o=l(),Bb=a("li"),Mae=a("strong"),r0o=o("yoso"),t0o=o(" \u2014 "),lj=a("a"),a0o=o("YosoForTokenClassification"),n0o=o(" (YOSO model)"),s0o=l(),kb=a("p"),l0o=o("The model is set in evaluation mode by default using "),Eae=a("code"),i0o=o("model.eval()"),d0o=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),yae=a("code"),c0o=o("model.train()"),f0o=l(),wae=a("p"),m0o=o("Examples:"),g0o=l(),f(VE.$$.fragment),O7e=l(),gd=a("h2"),xb=a("a"),Aae=a("span"),f(WE.$$.fragment),h0o=l(),Lae=a("span"),p0o=o("AutoModelForQuestionAnswering"),X7e=l(),er=a("div"),f(QE.$$.fragment),_0o=l(),hd=a("p"),u0o=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a question answering head) when created
with the `),Bae=a("code"),b0o=o("from_pretrained()"),v0o=o("class method or the "),kae=a("code"),T0o=o("from_config()"),F0o=o(`class
method.`),C0o=l(),HE=a("p"),M0o=o("This class cannot be instantiated directly using "),xae=a("code"),E0o=o("__init__()"),y0o=o(" (throws an error)."),w0o=l(),Qr=a("div"),f(UE.$$.fragment),A0o=l(),Rae=a("p"),L0o=o("Instantiates one of the model classes of the library (with a question answering head) from a configuration."),B0o=l(),pd=a("p"),k0o=o(`Note:
Loading a model from its configuration file does `),Sae=a("strong"),x0o=o("not"),R0o=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Pae=a("code"),S0o=o("from_pretrained()"),P0o=o("to load the model weights."),$0o=l(),$ae=a("p"),I0o=o("Examples:"),j0o=l(),f(JE.$$.fragment),N0o=l(),De=a("div"),f(YE.$$.fragment),D0o=l(),Iae=a("p"),q0o=o("Instantiate one of the model classes of the library (with a question answering head) from a pretrained model."),G0o=l(),Ha=a("p"),O0o=o("The model class to instantiate is selected based on the "),jae=a("code"),X0o=o("model_type"),z0o=o(` property of the config object (either
passed as an argument or loaded from `),Nae=a("code"),V0o=o("pretrained_model_name_or_path"),W0o=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Dae=a("code"),Q0o=o("pretrained_model_name_or_path"),H0o=o(":"),U0o=l(),R=a("ul"),Rb=a("li"),qae=a("strong"),J0o=o("albert"),Y0o=o(" \u2014 "),ij=a("a"),K0o=o("AlbertForQuestionAnswering"),Z0o=o(" (ALBERT model)"),eLo=l(),Sb=a("li"),Gae=a("strong"),oLo=o("bart"),rLo=o(" \u2014 "),dj=a("a"),tLo=o("BartForQuestionAnswering"),aLo=o(" (BART model)"),nLo=l(),Pb=a("li"),Oae=a("strong"),sLo=o("bert"),lLo=o(" \u2014 "),cj=a("a"),iLo=o("BertForQuestionAnswering"),dLo=o(" (BERT model)"),cLo=l(),$b=a("li"),Xae=a("strong"),fLo=o("big_bird"),mLo=o(" \u2014 "),fj=a("a"),gLo=o("BigBirdForQuestionAnswering"),hLo=o(" (BigBird model)"),pLo=l(),Ib=a("li"),zae=a("strong"),_Lo=o("bigbird_pegasus"),uLo=o(" \u2014 "),mj=a("a"),bLo=o("BigBirdPegasusForQuestionAnswering"),vLo=o(" (BigBirdPegasus model)"),TLo=l(),jb=a("li"),Vae=a("strong"),FLo=o("camembert"),CLo=o(" \u2014 "),gj=a("a"),MLo=o("CamembertForQuestionAnswering"),ELo=o(" (CamemBERT model)"),yLo=l(),Nb=a("li"),Wae=a("strong"),wLo=o("canine"),ALo=o(" \u2014 "),hj=a("a"),LLo=o("CanineForQuestionAnswering"),BLo=o(" (Canine model)"),kLo=l(),Db=a("li"),Qae=a("strong"),xLo=o("convbert"),RLo=o(" \u2014 "),pj=a("a"),SLo=o("ConvBertForQuestionAnswering"),PLo=o(" (ConvBERT model)"),$Lo=l(),qb=a("li"),Hae=a("strong"),ILo=o("deberta"),jLo=o(" \u2014 "),_j=a("a"),NLo=o("DebertaForQuestionAnswering"),DLo=o(" (DeBERTa model)"),qLo=l(),Gb=a("li"),Uae=a("strong"),GLo=o("deberta-v2"),OLo=o(" \u2014 "),uj=a("a"),XLo=o("DebertaV2ForQuestionAnswering"),zLo=o(" (DeBERTa-v2 model)"),VLo=l(),Ob=a("li"),Jae=a("strong"),WLo=o("distilbert"),QLo=o(" \u2014 "),bj=a("a"),HLo=o("DistilBertForQuestionAnswering"),ULo=o(" (DistilBERT model)"),JLo=l(),Xb=a("li"),Yae=a("strong"),YLo=o("electra"),KLo=o(" \u2014 "),vj=a("a"),ZLo=o("ElectraForQuestionAnswering"),e7o=o(" (ELECTRA model)"),o7o=l(),zb=a("li"),Kae=a("strong"),r7o=o("flaubert"),t7o=o(" \u2014 "),Tj=a("a"),a7o=o("FlaubertForQuestionAnsweringSimple"),n7o=o(" (FlauBERT model)"),s7o=l(),Vb=a("li"),Zae=a("strong"),l7o=o("fnet"),i7o=o(" \u2014 "),Fj=a("a"),d7o=o("FNetForQuestionAnswering"),c7o=o(" (FNet model)"),f7o=l(),Wb=a("li"),ene=a("strong"),m7o=o("funnel"),g7o=o(" \u2014 "),Cj=a("a"),h7o=o("FunnelForQuestionAnswering"),p7o=o(" (Funnel Transformer model)"),_7o=l(),Qb=a("li"),one=a("strong"),u7o=o("gptj"),b7o=o(" \u2014 "),Mj=a("a"),v7o=o("GPTJForQuestionAnswering"),T7o=o(" (GPT-J model)"),F7o=l(),Hb=a("li"),rne=a("strong"),C7o=o("ibert"),M7o=o(" \u2014 "),Ej=a("a"),E7o=o("IBertForQuestionAnswering"),y7o=o(" (I-BERT model)"),w7o=l(),Ub=a("li"),tne=a("strong"),A7o=o("layoutlmv2"),L7o=o(" \u2014 "),yj=a("a"),B7o=o("LayoutLMv2ForQuestionAnswering"),k7o=o(" (LayoutLMv2 model)"),x7o=l(),Jb=a("li"),ane=a("strong"),R7o=o("led"),S7o=o(" \u2014 "),wj=a("a"),P7o=o("LEDForQuestionAnswering"),$7o=o(" (LED model)"),I7o=l(),Yb=a("li"),nne=a("strong"),j7o=o("longformer"),N7o=o(" \u2014 "),Aj=a("a"),D7o=o("LongformerForQuestionAnswering"),q7o=o(" (Longformer model)"),G7o=l(),Kb=a("li"),sne=a("strong"),O7o=o("lxmert"),X7o=o(" \u2014 "),Lj=a("a"),z7o=o("LxmertForQuestionAnswering"),V7o=o(" (LXMERT model)"),W7o=l(),Zb=a("li"),lne=a("strong"),Q7o=o("mbart"),H7o=o(" \u2014 "),Bj=a("a"),U7o=o("MBartForQuestionAnswering"),J7o=o(" (mBART model)"),Y7o=l(),e5=a("li"),ine=a("strong"),K7o=o("megatron-bert"),Z7o=o(" \u2014 "),kj=a("a"),e9o=o("MegatronBertForQuestionAnswering"),o9o=o(" (MegatronBert model)"),r9o=l(),o5=a("li"),dne=a("strong"),t9o=o("mobilebert"),a9o=o(" \u2014 "),xj=a("a"),n9o=o("MobileBertForQuestionAnswering"),s9o=o(" (MobileBERT model)"),l9o=l(),r5=a("li"),cne=a("strong"),i9o=o("mpnet"),d9o=o(" \u2014 "),Rj=a("a"),c9o=o("MPNetForQuestionAnswering"),f9o=o(" (MPNet model)"),m9o=l(),t5=a("li"),fne=a("strong"),g9o=o("nystromformer"),h9o=o(" \u2014 "),Sj=a("a"),p9o=o("NystromformerForQuestionAnswering"),_9o=o(" (Nystromformer model)"),u9o=l(),a5=a("li"),mne=a("strong"),b9o=o("qdqbert"),v9o=o(" \u2014 "),Pj=a("a"),T9o=o("QDQBertForQuestionAnswering"),F9o=o(" (QDQBert model)"),C9o=l(),n5=a("li"),gne=a("strong"),M9o=o("reformer"),E9o=o(" \u2014 "),$j=a("a"),y9o=o("ReformerForQuestionAnswering"),w9o=o(" (Reformer model)"),A9o=l(),s5=a("li"),hne=a("strong"),L9o=o("rembert"),B9o=o(" \u2014 "),Ij=a("a"),k9o=o("RemBertForQuestionAnswering"),x9o=o(" (RemBERT model)"),R9o=l(),l5=a("li"),pne=a("strong"),S9o=o("roberta"),P9o=o(" \u2014 "),jj=a("a"),$9o=o("RobertaForQuestionAnswering"),I9o=o(" (RoBERTa model)"),j9o=l(),i5=a("li"),_ne=a("strong"),N9o=o("roformer"),D9o=o(" \u2014 "),Nj=a("a"),q9o=o("RoFormerForQuestionAnswering"),G9o=o(" (RoFormer model)"),O9o=l(),d5=a("li"),une=a("strong"),X9o=o("splinter"),z9o=o(" \u2014 "),Dj=a("a"),V9o=o("SplinterForQuestionAnswering"),W9o=o(" (Splinter model)"),Q9o=l(),c5=a("li"),bne=a("strong"),H9o=o("squeezebert"),U9o=o(" \u2014 "),qj=a("a"),J9o=o("SqueezeBertForQuestionAnswering"),Y9o=o(" (SqueezeBERT model)"),K9o=l(),f5=a("li"),vne=a("strong"),Z9o=o("xlm"),eBo=o(" \u2014 "),Gj=a("a"),oBo=o("XLMForQuestionAnsweringSimple"),rBo=o(" (XLM model)"),tBo=l(),m5=a("li"),Tne=a("strong"),aBo=o("xlm-roberta"),nBo=o(" \u2014 "),Oj=a("a"),sBo=o("XLMRobertaForQuestionAnswering"),lBo=o(" (XLM-RoBERTa model)"),iBo=l(),g5=a("li"),Fne=a("strong"),dBo=o("xlm-roberta-xl"),cBo=o(" \u2014 "),Xj=a("a"),fBo=o("XLMRobertaXLForQuestionAnswering"),mBo=o(" (XLM-RoBERTa-XL model)"),gBo=l(),h5=a("li"),Cne=a("strong"),hBo=o("xlnet"),pBo=o(" \u2014 "),zj=a("a"),_Bo=o("XLNetForQuestionAnsweringSimple"),uBo=o(" (XLNet model)"),bBo=l(),p5=a("li"),Mne=a("strong"),vBo=o("yoso"),TBo=o(" \u2014 "),Vj=a("a"),FBo=o("YosoForQuestionAnswering"),CBo=o(" (YOSO model)"),MBo=l(),_5=a("p"),EBo=o("The model is set in evaluation mode by default using "),Ene=a("code"),yBo=o("model.eval()"),wBo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),yne=a("code"),ABo=o("model.train()"),LBo=l(),wne=a("p"),BBo=o("Examples:"),kBo=l(),f(KE.$$.fragment),z7e=l(),_d=a("h2"),u5=a("a"),Ane=a("span"),f(ZE.$$.fragment),xBo=l(),Lne=a("span"),RBo=o("AutoModelForTableQuestionAnswering"),V7e=l(),or=a("div"),f(e3.$$.fragment),SBo=l(),ud=a("p"),PBo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a table question answering head) when created
with the `),Bne=a("code"),$Bo=o("from_pretrained()"),IBo=o("class method or the "),kne=a("code"),jBo=o("from_config()"),NBo=o(`class
method.`),DBo=l(),o3=a("p"),qBo=o("This class cannot be instantiated directly using "),xne=a("code"),GBo=o("__init__()"),OBo=o(" (throws an error)."),XBo=l(),Hr=a("div"),f(r3.$$.fragment),zBo=l(),Rne=a("p"),VBo=o("Instantiates one of the model classes of the library (with a table question answering head) from a configuration."),WBo=l(),bd=a("p"),QBo=o(`Note:
Loading a model from its configuration file does `),Sne=a("strong"),HBo=o("not"),UBo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Pne=a("code"),JBo=o("from_pretrained()"),YBo=o("to load the model weights."),KBo=l(),$ne=a("p"),ZBo=o("Examples:"),eko=l(),f(t3.$$.fragment),oko=l(),qe=a("div"),f(a3.$$.fragment),rko=l(),Ine=a("p"),tko=o("Instantiate one of the model classes of the library (with a table question answering head) from a pretrained model."),ako=l(),Ua=a("p"),nko=o("The model class to instantiate is selected based on the "),jne=a("code"),sko=o("model_type"),lko=o(` property of the config object (either
passed as an argument or loaded from `),Nne=a("code"),iko=o("pretrained_model_name_or_path"),dko=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Dne=a("code"),cko=o("pretrained_model_name_or_path"),fko=o(":"),mko=l(),qne=a("ul"),b5=a("li"),Gne=a("strong"),gko=o("tapas"),hko=o(" \u2014 "),Wj=a("a"),pko=o("TapasForQuestionAnswering"),_ko=o(" (TAPAS model)"),uko=l(),v5=a("p"),bko=o("The model is set in evaluation mode by default using "),One=a("code"),vko=o("model.eval()"),Tko=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Xne=a("code"),Fko=o("model.train()"),Cko=l(),zne=a("p"),Mko=o("Examples:"),Eko=l(),f(n3.$$.fragment),W7e=l(),vd=a("h2"),T5=a("a"),Vne=a("span"),f(s3.$$.fragment),yko=l(),Wne=a("span"),wko=o("AutoModelForImageClassification"),Q7e=l(),rr=a("div"),f(l3.$$.fragment),Ako=l(),Td=a("p"),Lko=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a image classification head) when created
with the `),Qne=a("code"),Bko=o("from_pretrained()"),kko=o("class method or the "),Hne=a("code"),xko=o("from_config()"),Rko=o(`class
method.`),Sko=l(),i3=a("p"),Pko=o("This class cannot be instantiated directly using "),Une=a("code"),$ko=o("__init__()"),Iko=o(" (throws an error)."),jko=l(),Ur=a("div"),f(d3.$$.fragment),Nko=l(),Jne=a("p"),Dko=o("Instantiates one of the model classes of the library (with a image classification head) from a configuration."),qko=l(),Fd=a("p"),Gko=o(`Note:
Loading a model from its configuration file does `),Yne=a("strong"),Oko=o("not"),Xko=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Kne=a("code"),zko=o("from_pretrained()"),Vko=o("to load the model weights."),Wko=l(),Zne=a("p"),Qko=o("Examples:"),Hko=l(),f(c3.$$.fragment),Uko=l(),Ge=a("div"),f(f3.$$.fragment),Jko=l(),ese=a("p"),Yko=o("Instantiate one of the model classes of the library (with a image classification head) from a pretrained model."),Kko=l(),Ja=a("p"),Zko=o("The model class to instantiate is selected based on the "),ose=a("code"),exo=o("model_type"),oxo=o(` property of the config object (either
passed as an argument or loaded from `),rse=a("code"),rxo=o("pretrained_model_name_or_path"),txo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),tse=a("code"),axo=o("pretrained_model_name_or_path"),nxo=o(":"),sxo=l(),be=a("ul"),F5=a("li"),ase=a("strong"),lxo=o("beit"),ixo=o(" \u2014 "),Qj=a("a"),dxo=o("BeitForImageClassification"),cxo=o(" (BEiT model)"),fxo=l(),C5=a("li"),nse=a("strong"),mxo=o("convnext"),gxo=o(" \u2014 "),Hj=a("a"),hxo=o("ConvNextForImageClassification"),pxo=o(" (ConvNext model)"),_xo=l(),Rs=a("li"),sse=a("strong"),uxo=o("deit"),bxo=o(" \u2014 "),Uj=a("a"),vxo=o("DeiTForImageClassification"),Txo=o(" or "),Jj=a("a"),Fxo=o("DeiTForImageClassificationWithTeacher"),Cxo=o(" (DeiT model)"),Mxo=l(),M5=a("li"),lse=a("strong"),Exo=o("imagegpt"),yxo=o(" \u2014 "),Yj=a("a"),wxo=o("ImageGPTForImageClassification"),Axo=o(" (ImageGPT model)"),Lxo=l(),la=a("li"),ise=a("strong"),Bxo=o("perceiver"),kxo=o(" \u2014 "),Kj=a("a"),xxo=o("PerceiverForImageClassificationLearned"),Rxo=o(" or "),Zj=a("a"),Sxo=o("PerceiverForImageClassificationFourier"),Pxo=o(" or "),eN=a("a"),$xo=o("PerceiverForImageClassificationConvProcessing"),Ixo=o(" (Perceiver model)"),jxo=l(),E5=a("li"),dse=a("strong"),Nxo=o("poolformer"),Dxo=o(" \u2014 "),oN=a("a"),qxo=o("PoolFormerForImageClassification"),Gxo=o(" (PoolFormer model)"),Oxo=l(),y5=a("li"),cse=a("strong"),Xxo=o("segformer"),zxo=o(" \u2014 "),rN=a("a"),Vxo=o("SegformerForImageClassification"),Wxo=o(" (SegFormer model)"),Qxo=l(),w5=a("li"),fse=a("strong"),Hxo=o("swin"),Uxo=o(" \u2014 "),tN=a("a"),Jxo=o("SwinForImageClassification"),Yxo=o(" (Swin model)"),Kxo=l(),A5=a("li"),mse=a("strong"),Zxo=o("vit"),eRo=o(" \u2014 "),aN=a("a"),oRo=o("ViTForImageClassification"),rRo=o(" (ViT model)"),tRo=l(),L5=a("p"),aRo=o("The model is set in evaluation mode by default using "),gse=a("code"),nRo=o("model.eval()"),sRo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),hse=a("code"),lRo=o("model.train()"),iRo=l(),pse=a("p"),dRo=o("Examples:"),cRo=l(),f(m3.$$.fragment),H7e=l(),Cd=a("h2"),B5=a("a"),_se=a("span"),f(g3.$$.fragment),fRo=l(),use=a("span"),mRo=o("AutoModelForVision2Seq"),U7e=l(),tr=a("div"),f(h3.$$.fragment),gRo=l(),Md=a("p"),hRo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a vision-to-text modeling head) when created
with the `),bse=a("code"),pRo=o("from_pretrained()"),_Ro=o("class method or the "),vse=a("code"),uRo=o("from_config()"),bRo=o(`class
method.`),vRo=l(),p3=a("p"),TRo=o("This class cannot be instantiated directly using "),Tse=a("code"),FRo=o("__init__()"),CRo=o(" (throws an error)."),MRo=l(),Jr=a("div"),f(_3.$$.fragment),ERo=l(),Fse=a("p"),yRo=o("Instantiates one of the model classes of the library (with a vision-to-text modeling head) from a configuration."),wRo=l(),Ed=a("p"),ARo=o(`Note:
Loading a model from its configuration file does `),Cse=a("strong"),LRo=o("not"),BRo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Mse=a("code"),kRo=o("from_pretrained()"),xRo=o("to load the model weights."),RRo=l(),Ese=a("p"),SRo=o("Examples:"),PRo=l(),f(u3.$$.fragment),$Ro=l(),Oe=a("div"),f(b3.$$.fragment),IRo=l(),yse=a("p"),jRo=o("Instantiate one of the model classes of the library (with a vision-to-text modeling head) from a pretrained model."),NRo=l(),Ya=a("p"),DRo=o("The model class to instantiate is selected based on the "),wse=a("code"),qRo=o("model_type"),GRo=o(` property of the config object (either
passed as an argument or loaded from `),Ase=a("code"),ORo=o("pretrained_model_name_or_path"),XRo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Lse=a("code"),zRo=o("pretrained_model_name_or_path"),VRo=o(":"),WRo=l(),Bse=a("ul"),k5=a("li"),kse=a("strong"),QRo=o("vision-encoder-decoder"),HRo=o(" \u2014 "),nN=a("a"),URo=o("VisionEncoderDecoderModel"),JRo=o(" (Vision Encoder decoder model)"),YRo=l(),x5=a("p"),KRo=o("The model is set in evaluation mode by default using "),xse=a("code"),ZRo=o("model.eval()"),eSo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Rse=a("code"),oSo=o("model.train()"),rSo=l(),Sse=a("p"),tSo=o("Examples:"),aSo=l(),f(v3.$$.fragment),J7e=l(),yd=a("h2"),R5=a("a"),Pse=a("span"),f(T3.$$.fragment),nSo=l(),$se=a("span"),sSo=o("AutoModelForAudioClassification"),Y7e=l(),ar=a("div"),f(F3.$$.fragment),lSo=l(),wd=a("p"),iSo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a audio classification head) when created
with the `),Ise=a("code"),dSo=o("from_pretrained()"),cSo=o("class method or the "),jse=a("code"),fSo=o("from_config()"),mSo=o(`class
method.`),gSo=l(),C3=a("p"),hSo=o("This class cannot be instantiated directly using "),Nse=a("code"),pSo=o("__init__()"),_So=o(" (throws an error)."),uSo=l(),Yr=a("div"),f(M3.$$.fragment),bSo=l(),Dse=a("p"),vSo=o("Instantiates one of the model classes of the library (with a audio classification head) from a configuration."),TSo=l(),Ad=a("p"),FSo=o(`Note:
Loading a model from its configuration file does `),qse=a("strong"),CSo=o("not"),MSo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Gse=a("code"),ESo=o("from_pretrained()"),ySo=o("to load the model weights."),wSo=l(),Ose=a("p"),ASo=o("Examples:"),LSo=l(),f(E3.$$.fragment),BSo=l(),Xe=a("div"),f(y3.$$.fragment),kSo=l(),Xse=a("p"),xSo=o("Instantiate one of the model classes of the library (with a audio classification head) from a pretrained model."),RSo=l(),Ka=a("p"),SSo=o("The model class to instantiate is selected based on the "),zse=a("code"),PSo=o("model_type"),$So=o(` property of the config object (either
passed as an argument or loaded from `),Vse=a("code"),ISo=o("pretrained_model_name_or_path"),jSo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Wse=a("code"),NSo=o("pretrained_model_name_or_path"),DSo=o(":"),qSo=l(),ao=a("ul"),S5=a("li"),Qse=a("strong"),GSo=o("hubert"),OSo=o(" \u2014 "),sN=a("a"),XSo=o("HubertForSequenceClassification"),zSo=o(" (Hubert model)"),VSo=l(),P5=a("li"),Hse=a("strong"),WSo=o("sew"),QSo=o(" \u2014 "),lN=a("a"),HSo=o("SEWForSequenceClassification"),USo=o(" (SEW model)"),JSo=l(),$5=a("li"),Use=a("strong"),YSo=o("sew-d"),KSo=o(" \u2014 "),iN=a("a"),ZSo=o("SEWDForSequenceClassification"),ePo=o(" (SEW-D model)"),oPo=l(),I5=a("li"),Jse=a("strong"),rPo=o("unispeech"),tPo=o(" \u2014 "),dN=a("a"),aPo=o("UniSpeechForSequenceClassification"),nPo=o(" (UniSpeech model)"),sPo=l(),j5=a("li"),Yse=a("strong"),lPo=o("unispeech-sat"),iPo=o(" \u2014 "),cN=a("a"),dPo=o("UniSpeechSatForSequenceClassification"),cPo=o(" (UniSpeechSat model)"),fPo=l(),N5=a("li"),Kse=a("strong"),mPo=o("wav2vec2"),gPo=o(" \u2014 "),fN=a("a"),hPo=o("Wav2Vec2ForSequenceClassification"),pPo=o(" (Wav2Vec2 model)"),_Po=l(),D5=a("li"),Zse=a("strong"),uPo=o("wavlm"),bPo=o(" \u2014 "),mN=a("a"),vPo=o("WavLMForSequenceClassification"),TPo=o(" (WavLM model)"),FPo=l(),q5=a("p"),CPo=o("The model is set in evaluation mode by default using "),ele=a("code"),MPo=o("model.eval()"),EPo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),ole=a("code"),yPo=o("model.train()"),wPo=l(),rle=a("p"),APo=o("Examples:"),LPo=l(),f(w3.$$.fragment),K7e=l(),Ld=a("h2"),G5=a("a"),tle=a("span"),f(A3.$$.fragment),BPo=l(),ale=a("span"),kPo=o("AutoModelForAudioFrameClassification"),Z7e=l(),nr=a("div"),f(L3.$$.fragment),xPo=l(),Bd=a("p"),RPo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a audio frame (token) classification head) when created
with the `),nle=a("code"),SPo=o("from_pretrained()"),PPo=o("class method or the "),sle=a("code"),$Po=o("from_config()"),IPo=o(`class
method.`),jPo=l(),B3=a("p"),NPo=o("This class cannot be instantiated directly using "),lle=a("code"),DPo=o("__init__()"),qPo=o(" (throws an error)."),GPo=l(),Kr=a("div"),f(k3.$$.fragment),OPo=l(),ile=a("p"),XPo=o("Instantiates one of the model classes of the library (with a audio frame (token) classification head) from a configuration."),zPo=l(),kd=a("p"),VPo=o(`Note:
Loading a model from its configuration file does `),dle=a("strong"),WPo=o("not"),QPo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),cle=a("code"),HPo=o("from_pretrained()"),UPo=o("to load the model weights."),JPo=l(),fle=a("p"),YPo=o("Examples:"),KPo=l(),f(x3.$$.fragment),ZPo=l(),ze=a("div"),f(R3.$$.fragment),e$o=l(),mle=a("p"),o$o=o("Instantiate one of the model classes of the library (with a audio frame (token) classification head) from a pretrained model."),r$o=l(),Za=a("p"),t$o=o("The model class to instantiate is selected based on the "),gle=a("code"),a$o=o("model_type"),n$o=o(` property of the config object (either
passed as an argument or loaded from `),hle=a("code"),s$o=o("pretrained_model_name_or_path"),l$o=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),ple=a("code"),i$o=o("pretrained_model_name_or_path"),d$o=o(":"),c$o=l(),xd=a("ul"),O5=a("li"),_le=a("strong"),f$o=o("unispeech-sat"),m$o=o(" \u2014 "),gN=a("a"),g$o=o("UniSpeechSatForAudioFrameClassification"),h$o=o(" (UniSpeechSat model)"),p$o=l(),X5=a("li"),ule=a("strong"),_$o=o("wav2vec2"),u$o=o(" \u2014 "),hN=a("a"),b$o=o("Wav2Vec2ForAudioFrameClassification"),v$o=o(" (Wav2Vec2 model)"),T$o=l(),z5=a("li"),ble=a("strong"),F$o=o("wavlm"),C$o=o(" \u2014 "),pN=a("a"),M$o=o("WavLMForAudioFrameClassification"),E$o=o(" (WavLM model)"),y$o=l(),V5=a("p"),w$o=o("The model is set in evaluation mode by default using "),vle=a("code"),A$o=o("model.eval()"),L$o=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Tle=a("code"),B$o=o("model.train()"),k$o=l(),Fle=a("p"),x$o=o("Examples:"),R$o=l(),f(S3.$$.fragment),e9e=l(),Rd=a("h2"),W5=a("a"),Cle=a("span"),f(P3.$$.fragment),S$o=l(),Mle=a("span"),P$o=o("AutoModelForCTC"),o9e=l(),sr=a("div"),f($3.$$.fragment),$$o=l(),Sd=a("p"),I$o=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a connectionist temporal classification head) when created
with the `),Ele=a("code"),j$o=o("from_pretrained()"),N$o=o("class method or the "),yle=a("code"),D$o=o("from_config()"),q$o=o(`class
method.`),G$o=l(),I3=a("p"),O$o=o("This class cannot be instantiated directly using "),wle=a("code"),X$o=o("__init__()"),z$o=o(" (throws an error)."),V$o=l(),Zr=a("div"),f(j3.$$.fragment),W$o=l(),Ale=a("p"),Q$o=o("Instantiates one of the model classes of the library (with a connectionist temporal classification head) from a configuration."),H$o=l(),Pd=a("p"),U$o=o(`Note:
Loading a model from its configuration file does `),Lle=a("strong"),J$o=o("not"),Y$o=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Ble=a("code"),K$o=o("from_pretrained()"),Z$o=o("to load the model weights."),eIo=l(),kle=a("p"),oIo=o("Examples:"),rIo=l(),f(N3.$$.fragment),tIo=l(),Ve=a("div"),f(D3.$$.fragment),aIo=l(),xle=a("p"),nIo=o("Instantiate one of the model classes of the library (with a connectionist temporal classification head) from a pretrained model."),sIo=l(),en=a("p"),lIo=o("The model class to instantiate is selected based on the "),Rle=a("code"),iIo=o("model_type"),dIo=o(` property of the config object (either
passed as an argument or loaded from `),Sle=a("code"),cIo=o("pretrained_model_name_or_path"),fIo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Ple=a("code"),mIo=o("pretrained_model_name_or_path"),gIo=o(":"),hIo=l(),no=a("ul"),Q5=a("li"),$le=a("strong"),pIo=o("hubert"),_Io=o(" \u2014 "),_N=a("a"),uIo=o("HubertForCTC"),bIo=o(" (Hubert model)"),vIo=l(),H5=a("li"),Ile=a("strong"),TIo=o("sew"),FIo=o(" \u2014 "),uN=a("a"),CIo=o("SEWForCTC"),MIo=o(" (SEW model)"),EIo=l(),U5=a("li"),jle=a("strong"),yIo=o("sew-d"),wIo=o(" \u2014 "),bN=a("a"),AIo=o("SEWDForCTC"),LIo=o(" (SEW-D model)"),BIo=l(),J5=a("li"),Nle=a("strong"),kIo=o("unispeech"),xIo=o(" \u2014 "),vN=a("a"),RIo=o("UniSpeechForCTC"),SIo=o(" (UniSpeech model)"),PIo=l(),Y5=a("li"),Dle=a("strong"),$Io=o("unispeech-sat"),IIo=o(" \u2014 "),TN=a("a"),jIo=o("UniSpeechSatForCTC"),NIo=o(" (UniSpeechSat model)"),DIo=l(),K5=a("li"),qle=a("strong"),qIo=o("wav2vec2"),GIo=o(" \u2014 "),FN=a("a"),OIo=o("Wav2Vec2ForCTC"),XIo=o(" (Wav2Vec2 model)"),zIo=l(),Z5=a("li"),Gle=a("strong"),VIo=o("wavlm"),WIo=o(" \u2014 "),CN=a("a"),QIo=o("WavLMForCTC"),HIo=o(" (WavLM model)"),UIo=l(),ev=a("p"),JIo=o("The model is set in evaluation mode by default using "),Ole=a("code"),YIo=o("model.eval()"),KIo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Xle=a("code"),ZIo=o("model.train()"),ejo=l(),zle=a("p"),ojo=o("Examples:"),rjo=l(),f(q3.$$.fragment),r9e=l(),$d=a("h2"),ov=a("a"),Vle=a("span"),f(G3.$$.fragment),tjo=l(),Wle=a("span"),ajo=o("AutoModelForSpeechSeq2Seq"),t9e=l(),lr=a("div"),f(O3.$$.fragment),njo=l(),Id=a("p"),sjo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) when created
with the `),Qle=a("code"),ljo=o("from_pretrained()"),ijo=o("class method or the "),Hle=a("code"),djo=o("from_config()"),cjo=o(`class
method.`),fjo=l(),X3=a("p"),mjo=o("This class cannot be instantiated directly using "),Ule=a("code"),gjo=o("__init__()"),hjo=o(" (throws an error)."),pjo=l(),et=a("div"),f(z3.$$.fragment),_jo=l(),Jle=a("p"),ujo=o("Instantiates one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) from a configuration."),bjo=l(),jd=a("p"),vjo=o(`Note:
Loading a model from its configuration file does `),Yle=a("strong"),Tjo=o("not"),Fjo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Kle=a("code"),Cjo=o("from_pretrained()"),Mjo=o("to load the model weights."),Ejo=l(),Zle=a("p"),yjo=o("Examples:"),wjo=l(),f(V3.$$.fragment),Ajo=l(),We=a("div"),f(W3.$$.fragment),Ljo=l(),eie=a("p"),Bjo=o("Instantiate one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) from a pretrained model."),kjo=l(),on=a("p"),xjo=o("The model class to instantiate is selected based on the "),oie=a("code"),Rjo=o("model_type"),Sjo=o(` property of the config object (either
passed as an argument or loaded from `),rie=a("code"),Pjo=o("pretrained_model_name_or_path"),$jo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),tie=a("code"),Ijo=o("pretrained_model_name_or_path"),jjo=o(":"),Njo=l(),Q3=a("ul"),rv=a("li"),aie=a("strong"),Djo=o("speech-encoder-decoder"),qjo=o(" \u2014 "),MN=a("a"),Gjo=o("SpeechEncoderDecoderModel"),Ojo=o(" (Speech Encoder decoder model)"),Xjo=l(),tv=a("li"),nie=a("strong"),zjo=o("speech_to_text"),Vjo=o(" \u2014 "),EN=a("a"),Wjo=o("Speech2TextForConditionalGeneration"),Qjo=o(" (Speech2Text model)"),Hjo=l(),av=a("p"),Ujo=o("The model is set in evaluation mode by default using "),sie=a("code"),Jjo=o("model.eval()"),Yjo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),lie=a("code"),Kjo=o("model.train()"),Zjo=l(),iie=a("p"),eNo=o("Examples:"),oNo=l(),f(H3.$$.fragment),a9e=l(),Nd=a("h2"),nv=a("a"),die=a("span"),f(U3.$$.fragment),rNo=l(),cie=a("span"),tNo=o("AutoModelForAudioXVector"),n9e=l(),ir=a("div"),f(J3.$$.fragment),aNo=l(),Dd=a("p"),nNo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a audio retrieval via x-vector head) when created
with the `),fie=a("code"),sNo=o("from_pretrained()"),lNo=o("class method or the "),mie=a("code"),iNo=o("from_config()"),dNo=o(`class
method.`),cNo=l(),Y3=a("p"),fNo=o("This class cannot be instantiated directly using "),gie=a("code"),mNo=o("__init__()"),gNo=o(" (throws an error)."),hNo=l(),ot=a("div"),f(K3.$$.fragment),pNo=l(),hie=a("p"),_No=o("Instantiates one of the model classes of the library (with a audio retrieval via x-vector head) from a configuration."),uNo=l(),qd=a("p"),bNo=o(`Note:
Loading a model from its configuration file does `),pie=a("strong"),vNo=o("not"),TNo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),_ie=a("code"),FNo=o("from_pretrained()"),CNo=o("to load the model weights."),MNo=l(),uie=a("p"),ENo=o("Examples:"),yNo=l(),f(Z3.$$.fragment),wNo=l(),Qe=a("div"),f(ey.$$.fragment),ANo=l(),bie=a("p"),LNo=o("Instantiate one of the model classes of the library (with a audio retrieval via x-vector head) from a pretrained model."),BNo=l(),rn=a("p"),kNo=o("The model class to instantiate is selected based on the "),vie=a("code"),xNo=o("model_type"),RNo=o(` property of the config object (either
passed as an argument or loaded from `),Tie=a("code"),SNo=o("pretrained_model_name_or_path"),PNo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Fie=a("code"),$No=o("pretrained_model_name_or_path"),INo=o(":"),jNo=l(),Gd=a("ul"),sv=a("li"),Cie=a("strong"),NNo=o("unispeech-sat"),DNo=o(" \u2014 "),yN=a("a"),qNo=o("UniSpeechSatForXVector"),GNo=o(" (UniSpeechSat model)"),ONo=l(),lv=a("li"),Mie=a("strong"),XNo=o("wav2vec2"),zNo=o(" \u2014 "),wN=a("a"),VNo=o("Wav2Vec2ForXVector"),WNo=o(" (Wav2Vec2 model)"),QNo=l(),iv=a("li"),Eie=a("strong"),HNo=o("wavlm"),UNo=o(" \u2014 "),AN=a("a"),JNo=o("WavLMForXVector"),YNo=o(" (WavLM model)"),KNo=l(),dv=a("p"),ZNo=o("The model is set in evaluation mode by default using "),yie=a("code"),eDo=o("model.eval()"),oDo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),wie=a("code"),rDo=o("model.train()"),tDo=l(),Aie=a("p"),aDo=o("Examples:"),nDo=l(),f(oy.$$.fragment),s9e=l(),Od=a("h2"),cv=a("a"),Lie=a("span"),f(ry.$$.fragment),sDo=l(),Bie=a("span"),lDo=o("AutoModelForMaskedImageModeling"),l9e=l(),dr=a("div"),f(ty.$$.fragment),iDo=l(),Xd=a("p"),dDo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a masked image modeling head) when created
with the `),kie=a("code"),cDo=o("from_pretrained()"),fDo=o("class method or the "),xie=a("code"),mDo=o("from_config()"),gDo=o(`class
method.`),hDo=l(),ay=a("p"),pDo=o("This class cannot be instantiated directly using "),Rie=a("code"),_Do=o("__init__()"),uDo=o(" (throws an error)."),bDo=l(),rt=a("div"),f(ny.$$.fragment),vDo=l(),Sie=a("p"),TDo=o("Instantiates one of the model classes of the library (with a masked image modeling head) from a configuration."),FDo=l(),zd=a("p"),CDo=o(`Note:
Loading a model from its configuration file does `),Pie=a("strong"),MDo=o("not"),EDo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),$ie=a("code"),yDo=o("from_pretrained()"),wDo=o("to load the model weights."),ADo=l(),Iie=a("p"),LDo=o("Examples:"),BDo=l(),f(sy.$$.fragment),kDo=l(),He=a("div"),f(ly.$$.fragment),xDo=l(),jie=a("p"),RDo=o("Instantiate one of the model classes of the library (with a masked image modeling head) from a pretrained model."),SDo=l(),tn=a("p"),PDo=o("The model class to instantiate is selected based on the "),Nie=a("code"),$Do=o("model_type"),IDo=o(` property of the config object (either
passed as an argument or loaded from `),Die=a("code"),jDo=o("pretrained_model_name_or_path"),NDo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),qie=a("code"),DDo=o("pretrained_model_name_or_path"),qDo=o(":"),GDo=l(),Vd=a("ul"),fv=a("li"),Gie=a("strong"),ODo=o("deit"),XDo=o(" \u2014 "),LN=a("a"),zDo=o("DeiTForMaskedImageModeling"),VDo=o(" (DeiT model)"),WDo=l(),mv=a("li"),Oie=a("strong"),QDo=o("swin"),HDo=o(" \u2014 "),BN=a("a"),UDo=o("SwinForMaskedImageModeling"),JDo=o(" (Swin model)"),YDo=l(),gv=a("li"),Xie=a("strong"),KDo=o("vit"),ZDo=o(" \u2014 "),kN=a("a"),eqo=o("ViTForMaskedImageModeling"),oqo=o(" (ViT model)"),rqo=l(),hv=a("p"),tqo=o("The model is set in evaluation mode by default using "),zie=a("code"),aqo=o("model.eval()"),nqo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Vie=a("code"),sqo=o("model.train()"),lqo=l(),Wie=a("p"),iqo=o("Examples:"),dqo=l(),f(iy.$$.fragment),i9e=l(),Wd=a("h2"),pv=a("a"),Qie=a("span"),f(dy.$$.fragment),cqo=l(),Hie=a("span"),fqo=o("AutoModelForObjectDetection"),d9e=l(),cr=a("div"),f(cy.$$.fragment),mqo=l(),Qd=a("p"),gqo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a object detection head) when created
with the `),Uie=a("code"),hqo=o("from_pretrained()"),pqo=o("class method or the "),Jie=a("code"),_qo=o("from_config()"),uqo=o(`class
method.`),bqo=l(),fy=a("p"),vqo=o("This class cannot be instantiated directly using "),Yie=a("code"),Tqo=o("__init__()"),Fqo=o(" (throws an error)."),Cqo=l(),tt=a("div"),f(my.$$.fragment),Mqo=l(),Kie=a("p"),Eqo=o("Instantiates one of the model classes of the library (with a object detection head) from a configuration."),yqo=l(),Hd=a("p"),wqo=o(`Note:
Loading a model from its configuration file does `),Zie=a("strong"),Aqo=o("not"),Lqo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),ede=a("code"),Bqo=o("from_pretrained()"),kqo=o("to load the model weights."),xqo=l(),ode=a("p"),Rqo=o("Examples:"),Sqo=l(),f(gy.$$.fragment),Pqo=l(),Ue=a("div"),f(hy.$$.fragment),$qo=l(),rde=a("p"),Iqo=o("Instantiate one of the model classes of the library (with a object detection head) from a pretrained model."),jqo=l(),an=a("p"),Nqo=o("The model class to instantiate is selected based on the "),tde=a("code"),Dqo=o("model_type"),qqo=o(` property of the config object (either
passed as an argument or loaded from `),ade=a("code"),Gqo=o("pretrained_model_name_or_path"),Oqo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),nde=a("code"),Xqo=o("pretrained_model_name_or_path"),zqo=o(":"),Vqo=l(),sde=a("ul"),_v=a("li"),lde=a("strong"),Wqo=o("detr"),Qqo=o(" \u2014 "),xN=a("a"),Hqo=o("DetrForObjectDetection"),Uqo=o(" (DETR model)"),Jqo=l(),uv=a("p"),Yqo=o("The model is set in evaluation mode by default using "),ide=a("code"),Kqo=o("model.eval()"),Zqo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),dde=a("code"),eGo=o("model.train()"),oGo=l(),cde=a("p"),rGo=o("Examples:"),tGo=l(),f(py.$$.fragment),c9e=l(),Ud=a("h2"),bv=a("a"),fde=a("span"),f(_y.$$.fragment),aGo=l(),mde=a("span"),nGo=o("AutoModelForImageSegmentation"),f9e=l(),fr=a("div"),f(uy.$$.fragment),sGo=l(),Jd=a("p"),lGo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a image segmentation head) when created
with the `),gde=a("code"),iGo=o("from_pretrained()"),dGo=o("class method or the "),hde=a("code"),cGo=o("from_config()"),fGo=o(`class
method.`),mGo=l(),by=a("p"),gGo=o("This class cannot be instantiated directly using "),pde=a("code"),hGo=o("__init__()"),pGo=o(" (throws an error)."),_Go=l(),at=a("div"),f(vy.$$.fragment),uGo=l(),_de=a("p"),bGo=o("Instantiates one of the model classes of the library (with a image segmentation head) from a configuration."),vGo=l(),Yd=a("p"),TGo=o(`Note:
Loading a model from its configuration file does `),ude=a("strong"),FGo=o("not"),CGo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),bde=a("code"),MGo=o("from_pretrained()"),EGo=o("to load the model weights."),yGo=l(),vde=a("p"),wGo=o("Examples:"),AGo=l(),f(Ty.$$.fragment),LGo=l(),Je=a("div"),f(Fy.$$.fragment),BGo=l(),Tde=a("p"),kGo=o("Instantiate one of the model classes of the library (with a image segmentation head) from a pretrained model."),xGo=l(),nn=a("p"),RGo=o("The model class to instantiate is selected based on the "),Fde=a("code"),SGo=o("model_type"),PGo=o(` property of the config object (either
passed as an argument or loaded from `),Cde=a("code"),$Go=o("pretrained_model_name_or_path"),IGo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Mde=a("code"),jGo=o("pretrained_model_name_or_path"),NGo=o(":"),DGo=l(),Ede=a("ul"),vv=a("li"),yde=a("strong"),qGo=o("detr"),GGo=o(" \u2014 "),RN=a("a"),OGo=o("DetrForSegmentation"),XGo=o(" (DETR model)"),zGo=l(),Tv=a("p"),VGo=o("The model is set in evaluation mode by default using "),wde=a("code"),WGo=o("model.eval()"),QGo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Ade=a("code"),HGo=o("model.train()"),UGo=l(),Lde=a("p"),JGo=o("Examples:"),YGo=l(),f(Cy.$$.fragment),m9e=l(),Kd=a("h2"),Fv=a("a"),Bde=a("span"),f(My.$$.fragment),KGo=l(),kde=a("span"),ZGo=o("AutoModelForSemanticSegmentation"),g9e=l(),mr=a("div"),f(Ey.$$.fragment),eOo=l(),Zd=a("p"),oOo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a semantic segmentation head) when created
with the `),xde=a("code"),rOo=o("from_pretrained()"),tOo=o("class method or the "),Rde=a("code"),aOo=o("from_config()"),nOo=o(`class
method.`),sOo=l(),yy=a("p"),lOo=o("This class cannot be instantiated directly using "),Sde=a("code"),iOo=o("__init__()"),dOo=o(" (throws an error)."),cOo=l(),nt=a("div"),f(wy.$$.fragment),fOo=l(),Pde=a("p"),mOo=o("Instantiates one of the model classes of the library (with a semantic segmentation head) from a configuration."),gOo=l(),ec=a("p"),hOo=o(`Note:
Loading a model from its configuration file does `),$de=a("strong"),pOo=o("not"),_Oo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Ide=a("code"),uOo=o("from_pretrained()"),bOo=o("to load the model weights."),vOo=l(),jde=a("p"),TOo=o("Examples:"),FOo=l(),f(Ay.$$.fragment),COo=l(),Ye=a("div"),f(Ly.$$.fragment),MOo=l(),Nde=a("p"),EOo=o("Instantiate one of the model classes of the library (with a semantic segmentation head) from a pretrained model."),yOo=l(),sn=a("p"),wOo=o("The model class to instantiate is selected based on the "),Dde=a("code"),AOo=o("model_type"),LOo=o(` property of the config object (either
passed as an argument or loaded from `),qde=a("code"),BOo=o("pretrained_model_name_or_path"),kOo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Gde=a("code"),xOo=o("pretrained_model_name_or_path"),ROo=o(":"),SOo=l(),By=a("ul"),Cv=a("li"),Ode=a("strong"),POo=o("beit"),$Oo=o(" \u2014 "),SN=a("a"),IOo=o("BeitForSemanticSegmentation"),jOo=o(" (BEiT model)"),NOo=l(),Mv=a("li"),Xde=a("strong"),DOo=o("segformer"),qOo=o(" \u2014 "),PN=a("a"),GOo=o("SegformerForSemanticSegmentation"),OOo=o(" (SegFormer model)"),XOo=l(),Ev=a("p"),zOo=o("The model is set in evaluation mode by default using "),zde=a("code"),VOo=o("model.eval()"),WOo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Vde=a("code"),QOo=o("model.train()"),HOo=l(),Wde=a("p"),UOo=o("Examples:"),JOo=l(),f(ky.$$.fragment),h9e=l(),oc=a("h2"),yv=a("a"),Qde=a("span"),f(xy.$$.fragment),YOo=l(),Hde=a("span"),KOo=o("TFAutoModel"),p9e=l(),gr=a("div"),f(Ry.$$.fragment),ZOo=l(),rc=a("p"),eXo=o(`This is a generic model class that will be instantiated as one of the base model classes of the library when created
with the `),Ude=a("code"),oXo=o("from_pretrained()"),rXo=o("class method or the "),Jde=a("code"),tXo=o("from_config()"),aXo=o(`class
method.`),nXo=l(),Sy=a("p"),sXo=o("This class cannot be instantiated directly using "),Yde=a("code"),lXo=o("__init__()"),iXo=o(" (throws an error)."),dXo=l(),st=a("div"),f(Py.$$.fragment),cXo=l(),Kde=a("p"),fXo=o("Instantiates one of the base model classes of the library from a configuration."),mXo=l(),tc=a("p"),gXo=o(`Note:
Loading a model from its configuration file does `),Zde=a("strong"),hXo=o("not"),pXo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),ece=a("code"),_Xo=o("from_pretrained()"),uXo=o("to load the model weights."),bXo=l(),oce=a("p"),vXo=o("Examples:"),TXo=l(),f($y.$$.fragment),FXo=l(),go=a("div"),f(Iy.$$.fragment),CXo=l(),rce=a("p"),MXo=o("Instantiate one of the base model classes of the library from a pretrained model."),EXo=l(),ln=a("p"),yXo=o("The model class to instantiate is selected based on the "),tce=a("code"),wXo=o("model_type"),AXo=o(` property of the config object (either
passed as an argument or loaded from `),ace=a("code"),LXo=o("pretrained_model_name_or_path"),BXo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),nce=a("code"),kXo=o("pretrained_model_name_or_path"),xXo=o(":"),RXo=l(),B=a("ul"),wv=a("li"),sce=a("strong"),SXo=o("albert"),PXo=o(" \u2014 "),$N=a("a"),$Xo=o("TFAlbertModel"),IXo=o(" (ALBERT model)"),jXo=l(),Av=a("li"),lce=a("strong"),NXo=o("bart"),DXo=o(" \u2014 "),IN=a("a"),qXo=o("TFBartModel"),GXo=o(" (BART model)"),OXo=l(),Lv=a("li"),ice=a("strong"),XXo=o("bert"),zXo=o(" \u2014 "),jN=a("a"),VXo=o("TFBertModel"),WXo=o(" (BERT model)"),QXo=l(),Bv=a("li"),dce=a("strong"),HXo=o("blenderbot"),UXo=o(" \u2014 "),NN=a("a"),JXo=o("TFBlenderbotModel"),YXo=o(" (Blenderbot model)"),KXo=l(),kv=a("li"),cce=a("strong"),ZXo=o("blenderbot-small"),ezo=o(" \u2014 "),DN=a("a"),ozo=o("TFBlenderbotSmallModel"),rzo=o(" (BlenderbotSmall model)"),tzo=l(),xv=a("li"),fce=a("strong"),azo=o("camembert"),nzo=o(" \u2014 "),qN=a("a"),szo=o("TFCamembertModel"),lzo=o(" (CamemBERT model)"),izo=l(),Rv=a("li"),mce=a("strong"),dzo=o("clip"),czo=o(" \u2014 "),GN=a("a"),fzo=o("TFCLIPModel"),mzo=o(" (CLIP model)"),gzo=l(),Sv=a("li"),gce=a("strong"),hzo=o("convbert"),pzo=o(" \u2014 "),ON=a("a"),_zo=o("TFConvBertModel"),uzo=o(" (ConvBERT model)"),bzo=l(),Pv=a("li"),hce=a("strong"),vzo=o("ctrl"),Tzo=o(" \u2014 "),XN=a("a"),Fzo=o("TFCTRLModel"),Czo=o(" (CTRL model)"),Mzo=l(),$v=a("li"),pce=a("strong"),Ezo=o("deberta"),yzo=o(" \u2014 "),zN=a("a"),wzo=o("TFDebertaModel"),Azo=o(" (DeBERTa model)"),Lzo=l(),Iv=a("li"),_ce=a("strong"),Bzo=o("deberta-v2"),kzo=o(" \u2014 "),VN=a("a"),xzo=o("TFDebertaV2Model"),Rzo=o(" (DeBERTa-v2 model)"),Szo=l(),jv=a("li"),uce=a("strong"),Pzo=o("distilbert"),$zo=o(" \u2014 "),WN=a("a"),Izo=o("TFDistilBertModel"),jzo=o(" (DistilBERT model)"),Nzo=l(),Nv=a("li"),bce=a("strong"),Dzo=o("dpr"),qzo=o(" \u2014 "),QN=a("a"),Gzo=o("TFDPRQuestionEncoder"),Ozo=o(" (DPR model)"),Xzo=l(),Dv=a("li"),vce=a("strong"),zzo=o("electra"),Vzo=o(" \u2014 "),HN=a("a"),Wzo=o("TFElectraModel"),Qzo=o(" (ELECTRA model)"),Hzo=l(),qv=a("li"),Tce=a("strong"),Uzo=o("flaubert"),Jzo=o(" \u2014 "),UN=a("a"),Yzo=o("TFFlaubertModel"),Kzo=o(" (FlauBERT model)"),Zzo=l(),Ss=a("li"),Fce=a("strong"),eVo=o("funnel"),oVo=o(" \u2014 "),JN=a("a"),rVo=o("TFFunnelModel"),tVo=o(" or "),YN=a("a"),aVo=o("TFFunnelBaseModel"),nVo=o(" (Funnel Transformer model)"),sVo=l(),Gv=a("li"),Cce=a("strong"),lVo=o("gpt2"),iVo=o(" \u2014 "),KN=a("a"),dVo=o("TFGPT2Model"),cVo=o(" (OpenAI GPT-2 model)"),fVo=l(),Ov=a("li"),Mce=a("strong"),mVo=o("hubert"),gVo=o(" \u2014 "),ZN=a("a"),hVo=o("TFHubertModel"),pVo=o(" (Hubert model)"),_Vo=l(),Xv=a("li"),Ece=a("strong"),uVo=o("layoutlm"),bVo=o(" \u2014 "),eD=a("a"),vVo=o("TFLayoutLMModel"),TVo=o(" (LayoutLM model)"),FVo=l(),zv=a("li"),yce=a("strong"),CVo=o("led"),MVo=o(" \u2014 "),oD=a("a"),EVo=o("TFLEDModel"),yVo=o(" (LED model)"),wVo=l(),Vv=a("li"),wce=a("strong"),AVo=o("longformer"),LVo=o(" \u2014 "),rD=a("a"),BVo=o("TFLongformerModel"),kVo=o(" (Longformer model)"),xVo=l(),Wv=a("li"),Ace=a("strong"),RVo=o("lxmert"),SVo=o(" \u2014 "),tD=a("a"),PVo=o("TFLxmertModel"),$Vo=o(" (LXMERT model)"),IVo=l(),Qv=a("li"),Lce=a("strong"),jVo=o("marian"),NVo=o(" \u2014 "),aD=a("a"),DVo=o("TFMarianModel"),qVo=o(" (Marian model)"),GVo=l(),Hv=a("li"),Bce=a("strong"),OVo=o("mbart"),XVo=o(" \u2014 "),nD=a("a"),zVo=o("TFMBartModel"),VVo=o(" (mBART model)"),WVo=l(),Uv=a("li"),kce=a("strong"),QVo=o("mobilebert"),HVo=o(" \u2014 "),sD=a("a"),UVo=o("TFMobileBertModel"),JVo=o(" (MobileBERT model)"),YVo=l(),Jv=a("li"),xce=a("strong"),KVo=o("mpnet"),ZVo=o(" \u2014 "),lD=a("a"),eWo=o("TFMPNetModel"),oWo=o(" (MPNet model)"),rWo=l(),Yv=a("li"),Rce=a("strong"),tWo=o("mt5"),aWo=o(" \u2014 "),iD=a("a"),nWo=o("TFMT5Model"),sWo=o(" (mT5 model)"),lWo=l(),Kv=a("li"),Sce=a("strong"),iWo=o("openai-gpt"),dWo=o(" \u2014 "),dD=a("a"),cWo=o("TFOpenAIGPTModel"),fWo=o(" (OpenAI GPT model)"),mWo=l(),Zv=a("li"),Pce=a("strong"),gWo=o("pegasus"),hWo=o(" \u2014 "),cD=a("a"),pWo=o("TFPegasusModel"),_Wo=o(" (Pegasus model)"),uWo=l(),e6=a("li"),$ce=a("strong"),bWo=o("rembert"),vWo=o(" \u2014 "),fD=a("a"),TWo=o("TFRemBertModel"),FWo=o(" (RemBERT model)"),CWo=l(),o6=a("li"),Ice=a("strong"),MWo=o("roberta"),EWo=o(" \u2014 "),mD=a("a"),yWo=o("TFRobertaModel"),wWo=o(" (RoBERTa model)"),AWo=l(),r6=a("li"),jce=a("strong"),LWo=o("roformer"),BWo=o(" \u2014 "),gD=a("a"),kWo=o("TFRoFormerModel"),xWo=o(" (RoFormer model)"),RWo=l(),t6=a("li"),Nce=a("strong"),SWo=o("speech_to_text"),PWo=o(" \u2014 "),hD=a("a"),$Wo=o("TFSpeech2TextModel"),IWo=o(" (Speech2Text model)"),jWo=l(),a6=a("li"),Dce=a("strong"),NWo=o("t5"),DWo=o(" \u2014 "),pD=a("a"),qWo=o("TFT5Model"),GWo=o(" (T5 model)"),OWo=l(),n6=a("li"),qce=a("strong"),XWo=o("tapas"),zWo=o(" \u2014 "),_D=a("a"),VWo=o("TFTapasModel"),WWo=o(" (TAPAS model)"),QWo=l(),s6=a("li"),Gce=a("strong"),HWo=o("transfo-xl"),UWo=o(" \u2014 "),uD=a("a"),JWo=o("TFTransfoXLModel"),YWo=o(" (Transformer-XL model)"),KWo=l(),l6=a("li"),Oce=a("strong"),ZWo=o("vit"),eQo=o(" \u2014 "),bD=a("a"),oQo=o("TFViTModel"),rQo=o(" (ViT model)"),tQo=l(),i6=a("li"),Xce=a("strong"),aQo=o("wav2vec2"),nQo=o(" \u2014 "),vD=a("a"),sQo=o("TFWav2Vec2Model"),lQo=o(" (Wav2Vec2 model)"),iQo=l(),d6=a("li"),zce=a("strong"),dQo=o("xlm"),cQo=o(" \u2014 "),TD=a("a"),fQo=o("TFXLMModel"),mQo=o(" (XLM model)"),gQo=l(),c6=a("li"),Vce=a("strong"),hQo=o("xlm-roberta"),pQo=o(" \u2014 "),FD=a("a"),_Qo=o("TFXLMRobertaModel"),uQo=o(" (XLM-RoBERTa model)"),bQo=l(),f6=a("li"),Wce=a("strong"),vQo=o("xlnet"),TQo=o(" \u2014 "),CD=a("a"),FQo=o("TFXLNetModel"),CQo=o(" (XLNet model)"),MQo=l(),Qce=a("p"),EQo=o("Examples:"),yQo=l(),f(jy.$$.fragment),_9e=l(),ac=a("h2"),m6=a("a"),Hce=a("span"),f(Ny.$$.fragment),wQo=l(),Uce=a("span"),AQo=o("TFAutoModelForPreTraining"),u9e=l(),hr=a("div"),f(Dy.$$.fragment),LQo=l(),nc=a("p"),BQo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a pretraining head) when created
with the `),Jce=a("code"),kQo=o("from_pretrained()"),xQo=o("class method or the "),Yce=a("code"),RQo=o("from_config()"),SQo=o(`class
method.`),PQo=l(),qy=a("p"),$Qo=o("This class cannot be instantiated directly using "),Kce=a("code"),IQo=o("__init__()"),jQo=o(" (throws an error)."),NQo=l(),lt=a("div"),f(Gy.$$.fragment),DQo=l(),Zce=a("p"),qQo=o("Instantiates one of the model classes of the library (with a pretraining head) from a configuration."),GQo=l(),sc=a("p"),OQo=o(`Note:
Loading a model from its configuration file does `),efe=a("strong"),XQo=o("not"),zQo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),ofe=a("code"),VQo=o("from_pretrained()"),WQo=o("to load the model weights."),QQo=l(),rfe=a("p"),HQo=o("Examples:"),UQo=l(),f(Oy.$$.fragment),JQo=l(),ho=a("div"),f(Xy.$$.fragment),YQo=l(),tfe=a("p"),KQo=o("Instantiate one of the model classes of the library (with a pretraining head) from a pretrained model."),ZQo=l(),dn=a("p"),eHo=o("The model class to instantiate is selected based on the "),afe=a("code"),oHo=o("model_type"),rHo=o(` property of the config object (either
passed as an argument or loaded from `),nfe=a("code"),tHo=o("pretrained_model_name_or_path"),aHo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),sfe=a("code"),nHo=o("pretrained_model_name_or_path"),sHo=o(":"),lHo=l(),H=a("ul"),g6=a("li"),lfe=a("strong"),iHo=o("albert"),dHo=o(" \u2014 "),MD=a("a"),cHo=o("TFAlbertForPreTraining"),fHo=o(" (ALBERT model)"),mHo=l(),h6=a("li"),ife=a("strong"),gHo=o("bart"),hHo=o(" \u2014 "),ED=a("a"),pHo=o("TFBartForConditionalGeneration"),_Ho=o(" (BART model)"),uHo=l(),p6=a("li"),dfe=a("strong"),bHo=o("bert"),vHo=o(" \u2014 "),yD=a("a"),THo=o("TFBertForPreTraining"),FHo=o(" (BERT model)"),CHo=l(),_6=a("li"),cfe=a("strong"),MHo=o("camembert"),EHo=o(" \u2014 "),wD=a("a"),yHo=o("TFCamembertForMaskedLM"),wHo=o(" (CamemBERT model)"),AHo=l(),u6=a("li"),ffe=a("strong"),LHo=o("ctrl"),BHo=o(" \u2014 "),AD=a("a"),kHo=o("TFCTRLLMHeadModel"),xHo=o(" (CTRL model)"),RHo=l(),b6=a("li"),mfe=a("strong"),SHo=o("distilbert"),PHo=o(" \u2014 "),LD=a("a"),$Ho=o("TFDistilBertForMaskedLM"),IHo=o(" (DistilBERT model)"),jHo=l(),v6=a("li"),gfe=a("strong"),NHo=o("electra"),DHo=o(" \u2014 "),BD=a("a"),qHo=o("TFElectraForPreTraining"),GHo=o(" (ELECTRA model)"),OHo=l(),T6=a("li"),hfe=a("strong"),XHo=o("flaubert"),zHo=o(" \u2014 "),kD=a("a"),VHo=o("TFFlaubertWithLMHeadModel"),WHo=o(" (FlauBERT model)"),QHo=l(),F6=a("li"),pfe=a("strong"),HHo=o("funnel"),UHo=o(" \u2014 "),xD=a("a"),JHo=o("TFFunnelForPreTraining"),YHo=o(" (Funnel Transformer model)"),KHo=l(),C6=a("li"),_fe=a("strong"),ZHo=o("gpt2"),eUo=o(" \u2014 "),RD=a("a"),oUo=o("TFGPT2LMHeadModel"),rUo=o(" (OpenAI GPT-2 model)"),tUo=l(),M6=a("li"),ufe=a("strong"),aUo=o("layoutlm"),nUo=o(" \u2014 "),SD=a("a"),sUo=o("TFLayoutLMForMaskedLM"),lUo=o(" (LayoutLM model)"),iUo=l(),E6=a("li"),bfe=a("strong"),dUo=o("lxmert"),cUo=o(" \u2014 "),PD=a("a"),fUo=o("TFLxmertForPreTraining"),mUo=o(" (LXMERT model)"),gUo=l(),y6=a("li"),vfe=a("strong"),hUo=o("mobilebert"),pUo=o(" \u2014 "),$D=a("a"),_Uo=o("TFMobileBertForPreTraining"),uUo=o(" (MobileBERT model)"),bUo=l(),w6=a("li"),Tfe=a("strong"),vUo=o("mpnet"),TUo=o(" \u2014 "),ID=a("a"),FUo=o("TFMPNetForMaskedLM"),CUo=o(" (MPNet model)"),MUo=l(),A6=a("li"),Ffe=a("strong"),EUo=o("openai-gpt"),yUo=o(" \u2014 "),jD=a("a"),wUo=o("TFOpenAIGPTLMHeadModel"),AUo=o(" (OpenAI GPT model)"),LUo=l(),L6=a("li"),Cfe=a("strong"),BUo=o("roberta"),kUo=o(" \u2014 "),ND=a("a"),xUo=o("TFRobertaForMaskedLM"),RUo=o(" (RoBERTa model)"),SUo=l(),B6=a("li"),Mfe=a("strong"),PUo=o("t5"),$Uo=o(" \u2014 "),DD=a("a"),IUo=o("TFT5ForConditionalGeneration"),jUo=o(" (T5 model)"),NUo=l(),k6=a("li"),Efe=a("strong"),DUo=o("tapas"),qUo=o(" \u2014 "),qD=a("a"),GUo=o("TFTapasForMaskedLM"),OUo=o(" (TAPAS model)"),XUo=l(),x6=a("li"),yfe=a("strong"),zUo=o("transfo-xl"),VUo=o(" \u2014 "),GD=a("a"),WUo=o("TFTransfoXLLMHeadModel"),QUo=o(" (Transformer-XL model)"),HUo=l(),R6=a("li"),wfe=a("strong"),UUo=o("xlm"),JUo=o(" \u2014 "),OD=a("a"),YUo=o("TFXLMWithLMHeadModel"),KUo=o(" (XLM model)"),ZUo=l(),S6=a("li"),Afe=a("strong"),eJo=o("xlm-roberta"),oJo=o(" \u2014 "),XD=a("a"),rJo=o("TFXLMRobertaForMaskedLM"),tJo=o(" (XLM-RoBERTa model)"),aJo=l(),P6=a("li"),Lfe=a("strong"),nJo=o("xlnet"),sJo=o(" \u2014 "),zD=a("a"),lJo=o("TFXLNetLMHeadModel"),iJo=o(" (XLNet model)"),dJo=l(),Bfe=a("p"),cJo=o("Examples:"),fJo=l(),f(zy.$$.fragment),b9e=l(),lc=a("h2"),$6=a("a"),kfe=a("span"),f(Vy.$$.fragment),mJo=l(),xfe=a("span"),gJo=o("TFAutoModelForCausalLM"),v9e=l(),pr=a("div"),f(Wy.$$.fragment),hJo=l(),ic=a("p"),pJo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a causal language modeling head) when created
with the `),Rfe=a("code"),_Jo=o("from_pretrained()"),uJo=o("class method or the "),Sfe=a("code"),bJo=o("from_config()"),vJo=o(`class
method.`),TJo=l(),Qy=a("p"),FJo=o("This class cannot be instantiated directly using "),Pfe=a("code"),CJo=o("__init__()"),MJo=o(" (throws an error)."),EJo=l(),it=a("div"),f(Hy.$$.fragment),yJo=l(),$fe=a("p"),wJo=o("Instantiates one of the model classes of the library (with a causal language modeling head) from a configuration."),AJo=l(),dc=a("p"),LJo=o(`Note:
Loading a model from its configuration file does `),Ife=a("strong"),BJo=o("not"),kJo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),jfe=a("code"),xJo=o("from_pretrained()"),RJo=o("to load the model weights."),SJo=l(),Nfe=a("p"),PJo=o("Examples:"),$Jo=l(),f(Uy.$$.fragment),IJo=l(),po=a("div"),f(Jy.$$.fragment),jJo=l(),Dfe=a("p"),NJo=o("Instantiate one of the model classes of the library (with a causal language modeling head) from a pretrained model."),DJo=l(),cn=a("p"),qJo=o("The model class to instantiate is selected based on the "),qfe=a("code"),GJo=o("model_type"),OJo=o(` property of the config object (either
passed as an argument or loaded from `),Gfe=a("code"),XJo=o("pretrained_model_name_or_path"),zJo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Ofe=a("code"),VJo=o("pretrained_model_name_or_path"),WJo=o(":"),QJo=l(),he=a("ul"),I6=a("li"),Xfe=a("strong"),HJo=o("bert"),UJo=o(" \u2014 "),VD=a("a"),JJo=o("TFBertLMHeadModel"),YJo=o(" (BERT model)"),KJo=l(),j6=a("li"),zfe=a("strong"),ZJo=o("ctrl"),eYo=o(" \u2014 "),WD=a("a"),oYo=o("TFCTRLLMHeadModel"),rYo=o(" (CTRL model)"),tYo=l(),N6=a("li"),Vfe=a("strong"),aYo=o("gpt2"),nYo=o(" \u2014 "),QD=a("a"),sYo=o("TFGPT2LMHeadModel"),lYo=o(" (OpenAI GPT-2 model)"),iYo=l(),D6=a("li"),Wfe=a("strong"),dYo=o("openai-gpt"),cYo=o(" \u2014 "),HD=a("a"),fYo=o("TFOpenAIGPTLMHeadModel"),mYo=o(" (OpenAI GPT model)"),gYo=l(),q6=a("li"),Qfe=a("strong"),hYo=o("rembert"),pYo=o(" \u2014 "),UD=a("a"),_Yo=o("TFRemBertForCausalLM"),uYo=o(" (RemBERT model)"),bYo=l(),G6=a("li"),Hfe=a("strong"),vYo=o("roberta"),TYo=o(" \u2014 "),JD=a("a"),FYo=o("TFRobertaForCausalLM"),CYo=o(" (RoBERTa model)"),MYo=l(),O6=a("li"),Ufe=a("strong"),EYo=o("roformer"),yYo=o(" \u2014 "),YD=a("a"),wYo=o("TFRoFormerForCausalLM"),AYo=o(" (RoFormer model)"),LYo=l(),X6=a("li"),Jfe=a("strong"),BYo=o("transfo-xl"),kYo=o(" \u2014 "),KD=a("a"),xYo=o("TFTransfoXLLMHeadModel"),RYo=o(" (Transformer-XL model)"),SYo=l(),z6=a("li"),Yfe=a("strong"),PYo=o("xlm"),$Yo=o(" \u2014 "),ZD=a("a"),IYo=o("TFXLMWithLMHeadModel"),jYo=o(" (XLM model)"),NYo=l(),V6=a("li"),Kfe=a("strong"),DYo=o("xlnet"),qYo=o(" \u2014 "),eq=a("a"),GYo=o("TFXLNetLMHeadModel"),OYo=o(" (XLNet model)"),XYo=l(),Zfe=a("p"),zYo=o("Examples:"),VYo=l(),f(Yy.$$.fragment),T9e=l(),cc=a("h2"),W6=a("a"),eme=a("span"),f(Ky.$$.fragment),WYo=l(),ome=a("span"),QYo=o("TFAutoModelForImageClassification"),F9e=l(),_r=a("div"),f(Zy.$$.fragment),HYo=l(),fc=a("p"),UYo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a image classification head) when created
with the `),rme=a("code"),JYo=o("from_pretrained()"),YYo=o("class method or the "),tme=a("code"),KYo=o("from_config()"),ZYo=o(`class
method.`),eKo=l(),ew=a("p"),oKo=o("This class cannot be instantiated directly using "),ame=a("code"),rKo=o("__init__()"),tKo=o(" (throws an error)."),aKo=l(),dt=a("div"),f(ow.$$.fragment),nKo=l(),nme=a("p"),sKo=o("Instantiates one of the model classes of the library (with a image classification head) from a configuration."),lKo=l(),mc=a("p"),iKo=o(`Note:
Loading a model from its configuration file does `),sme=a("strong"),dKo=o("not"),cKo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),lme=a("code"),fKo=o("from_pretrained()"),mKo=o("to load the model weights."),gKo=l(),ime=a("p"),hKo=o("Examples:"),pKo=l(),f(rw.$$.fragment),_Ko=l(),_o=a("div"),f(tw.$$.fragment),uKo=l(),dme=a("p"),bKo=o("Instantiate one of the model classes of the library (with a image classification head) from a pretrained model."),vKo=l(),fn=a("p"),TKo=o("The model class to instantiate is selected based on the "),cme=a("code"),FKo=o("model_type"),CKo=o(` property of the config object (either
passed as an argument or loaded from `),fme=a("code"),MKo=o("pretrained_model_name_or_path"),EKo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),mme=a("code"),yKo=o("pretrained_model_name_or_path"),wKo=o(":"),AKo=l(),gme=a("ul"),Q6=a("li"),hme=a("strong"),LKo=o("vit"),BKo=o(" \u2014 "),oq=a("a"),kKo=o("TFViTForImageClassification"),xKo=o(" (ViT model)"),RKo=l(),pme=a("p"),SKo=o("Examples:"),PKo=l(),f(aw.$$.fragment),C9e=l(),gc=a("h2"),H6=a("a"),_me=a("span"),f(nw.$$.fragment),$Ko=l(),ume=a("span"),IKo=o("TFAutoModelForMaskedLM"),M9e=l(),ur=a("div"),f(sw.$$.fragment),jKo=l(),hc=a("p"),NKo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a masked language modeling head) when created
with the `),bme=a("code"),DKo=o("from_pretrained()"),qKo=o("class method or the "),vme=a("code"),GKo=o("from_config()"),OKo=o(`class
method.`),XKo=l(),lw=a("p"),zKo=o("This class cannot be instantiated directly using "),Tme=a("code"),VKo=o("__init__()"),WKo=o(" (throws an error)."),QKo=l(),ct=a("div"),f(iw.$$.fragment),HKo=l(),Fme=a("p"),UKo=o("Instantiates one of the model classes of the library (with a masked language modeling head) from a configuration."),JKo=l(),pc=a("p"),YKo=o(`Note:
Loading a model from its configuration file does `),Cme=a("strong"),KKo=o("not"),ZKo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Mme=a("code"),eZo=o("from_pretrained()"),oZo=o("to load the model weights."),rZo=l(),Eme=a("p"),tZo=o("Examples:"),aZo=l(),f(dw.$$.fragment),nZo=l(),uo=a("div"),f(cw.$$.fragment),sZo=l(),yme=a("p"),lZo=o("Instantiate one of the model classes of the library (with a masked language modeling head) from a pretrained model."),iZo=l(),mn=a("p"),dZo=o("The model class to instantiate is selected based on the "),wme=a("code"),cZo=o("model_type"),fZo=o(` property of the config object (either
passed as an argument or loaded from `),Ame=a("code"),mZo=o("pretrained_model_name_or_path"),gZo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Lme=a("code"),hZo=o("pretrained_model_name_or_path"),pZo=o(":"),_Zo=l(),Y=a("ul"),U6=a("li"),Bme=a("strong"),uZo=o("albert"),bZo=o(" \u2014 "),rq=a("a"),vZo=o("TFAlbertForMaskedLM"),TZo=o(" (ALBERT model)"),FZo=l(),J6=a("li"),kme=a("strong"),CZo=o("bert"),MZo=o(" \u2014 "),tq=a("a"),EZo=o("TFBertForMaskedLM"),yZo=o(" (BERT model)"),wZo=l(),Y6=a("li"),xme=a("strong"),AZo=o("camembert"),LZo=o(" \u2014 "),aq=a("a"),BZo=o("TFCamembertForMaskedLM"),kZo=o(" (CamemBERT model)"),xZo=l(),K6=a("li"),Rme=a("strong"),RZo=o("convbert"),SZo=o(" \u2014 "),nq=a("a"),PZo=o("TFConvBertForMaskedLM"),$Zo=o(" (ConvBERT model)"),IZo=l(),Z6=a("li"),Sme=a("strong"),jZo=o("deberta"),NZo=o(" \u2014 "),sq=a("a"),DZo=o("TFDebertaForMaskedLM"),qZo=o(" (DeBERTa model)"),GZo=l(),eT=a("li"),Pme=a("strong"),OZo=o("deberta-v2"),XZo=o(" \u2014 "),lq=a("a"),zZo=o("TFDebertaV2ForMaskedLM"),VZo=o(" (DeBERTa-v2 model)"),WZo=l(),oT=a("li"),$me=a("strong"),QZo=o("distilbert"),HZo=o(" \u2014 "),iq=a("a"),UZo=o("TFDistilBertForMaskedLM"),JZo=o(" (DistilBERT model)"),YZo=l(),rT=a("li"),Ime=a("strong"),KZo=o("electra"),ZZo=o(" \u2014 "),dq=a("a"),eer=o("TFElectraForMaskedLM"),oer=o(" (ELECTRA model)"),rer=l(),tT=a("li"),jme=a("strong"),ter=o("flaubert"),aer=o(" \u2014 "),cq=a("a"),ner=o("TFFlaubertWithLMHeadModel"),ser=o(" (FlauBERT model)"),ler=l(),aT=a("li"),Nme=a("strong"),ier=o("funnel"),der=o(" \u2014 "),fq=a("a"),cer=o("TFFunnelForMaskedLM"),fer=o(" (Funnel Transformer model)"),mer=l(),nT=a("li"),Dme=a("strong"),ger=o("layoutlm"),her=o(" \u2014 "),mq=a("a"),per=o("TFLayoutLMForMaskedLM"),_er=o(" (LayoutLM model)"),uer=l(),sT=a("li"),qme=a("strong"),ber=o("longformer"),ver=o(" \u2014 "),gq=a("a"),Ter=o("TFLongformerForMaskedLM"),Fer=o(" (Longformer model)"),Cer=l(),lT=a("li"),Gme=a("strong"),Mer=o("mobilebert"),Eer=o(" \u2014 "),hq=a("a"),yer=o("TFMobileBertForMaskedLM"),wer=o(" (MobileBERT model)"),Aer=l(),iT=a("li"),Ome=a("strong"),Ler=o("mpnet"),Ber=o(" \u2014 "),pq=a("a"),ker=o("TFMPNetForMaskedLM"),xer=o(" (MPNet model)"),Rer=l(),dT=a("li"),Xme=a("strong"),Ser=o("rembert"),Per=o(" \u2014 "),_q=a("a"),$er=o("TFRemBertForMaskedLM"),Ier=o(" (RemBERT model)"),jer=l(),cT=a("li"),zme=a("strong"),Ner=o("roberta"),Der=o(" \u2014 "),uq=a("a"),qer=o("TFRobertaForMaskedLM"),Ger=o(" (RoBERTa model)"),Oer=l(),fT=a("li"),Vme=a("strong"),Xer=o("roformer"),zer=o(" \u2014 "),bq=a("a"),Ver=o("TFRoFormerForMaskedLM"),Wer=o(" (RoFormer model)"),Qer=l(),mT=a("li"),Wme=a("strong"),Her=o("tapas"),Uer=o(" \u2014 "),vq=a("a"),Jer=o("TFTapasForMaskedLM"),Yer=o(" (TAPAS model)"),Ker=l(),gT=a("li"),Qme=a("strong"),Zer=o("xlm"),eor=o(" \u2014 "),Tq=a("a"),oor=o("TFXLMWithLMHeadModel"),ror=o(" (XLM model)"),tor=l(),hT=a("li"),Hme=a("strong"),aor=o("xlm-roberta"),nor=o(" \u2014 "),Fq=a("a"),sor=o("TFXLMRobertaForMaskedLM"),lor=o(" (XLM-RoBERTa model)"),ior=l(),Ume=a("p"),dor=o("Examples:"),cor=l(),f(fw.$$.fragment),E9e=l(),_c=a("h2"),pT=a("a"),Jme=a("span"),f(mw.$$.fragment),mor=l(),Yme=a("span"),gor=o("TFAutoModelForSeq2SeqLM"),y9e=l(),br=a("div"),f(gw.$$.fragment),hor=l(),uc=a("p"),por=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence language modeling head) when created
with the `),Kme=a("code"),_or=o("from_pretrained()"),uor=o("class method or the "),Zme=a("code"),bor=o("from_config()"),vor=o(`class
method.`),Tor=l(),hw=a("p"),For=o("This class cannot be instantiated directly using "),ege=a("code"),Cor=o("__init__()"),Mor=o(" (throws an error)."),Eor=l(),ft=a("div"),f(pw.$$.fragment),yor=l(),oge=a("p"),wor=o("Instantiates one of the model classes of the library (with a sequence-to-sequence language modeling head) from a configuration."),Aor=l(),bc=a("p"),Lor=o(`Note:
Loading a model from its configuration file does `),rge=a("strong"),Bor=o("not"),kor=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),tge=a("code"),xor=o("from_pretrained()"),Ror=o("to load the model weights."),Sor=l(),age=a("p"),Por=o("Examples:"),$or=l(),f(_w.$$.fragment),Ior=l(),bo=a("div"),f(uw.$$.fragment),jor=l(),nge=a("p"),Nor=o("Instantiate one of the model classes of the library (with a sequence-to-sequence language modeling head) from a pretrained model."),Dor=l(),gn=a("p"),qor=o("The model class to instantiate is selected based on the "),sge=a("code"),Gor=o("model_type"),Oor=o(` property of the config object (either
passed as an argument or loaded from `),lge=a("code"),Xor=o("pretrained_model_name_or_path"),zor=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),ige=a("code"),Vor=o("pretrained_model_name_or_path"),Wor=o(":"),Qor=l(),pe=a("ul"),_T=a("li"),dge=a("strong"),Hor=o("bart"),Uor=o(" \u2014 "),Cq=a("a"),Jor=o("TFBartForConditionalGeneration"),Yor=o(" (BART model)"),Kor=l(),uT=a("li"),cge=a("strong"),Zor=o("blenderbot"),err=o(" \u2014 "),Mq=a("a"),orr=o("TFBlenderbotForConditionalGeneration"),rrr=o(" (Blenderbot model)"),trr=l(),bT=a("li"),fge=a("strong"),arr=o("blenderbot-small"),nrr=o(" \u2014 "),Eq=a("a"),srr=o("TFBlenderbotSmallForConditionalGeneration"),lrr=o(" (BlenderbotSmall model)"),irr=l(),vT=a("li"),mge=a("strong"),drr=o("encoder-decoder"),crr=o(" \u2014 "),yq=a("a"),frr=o("TFEncoderDecoderModel"),mrr=o(" (Encoder decoder model)"),grr=l(),TT=a("li"),gge=a("strong"),hrr=o("led"),prr=o(" \u2014 "),wq=a("a"),_rr=o("TFLEDForConditionalGeneration"),urr=o(" (LED model)"),brr=l(),FT=a("li"),hge=a("strong"),vrr=o("marian"),Trr=o(" \u2014 "),Aq=a("a"),Frr=o("TFMarianMTModel"),Crr=o(" (Marian model)"),Mrr=l(),CT=a("li"),pge=a("strong"),Err=o("mbart"),yrr=o(" \u2014 "),Lq=a("a"),wrr=o("TFMBartForConditionalGeneration"),Arr=o(" (mBART model)"),Lrr=l(),MT=a("li"),_ge=a("strong"),Brr=o("mt5"),krr=o(" \u2014 "),Bq=a("a"),xrr=o("TFMT5ForConditionalGeneration"),Rrr=o(" (mT5 model)"),Srr=l(),ET=a("li"),uge=a("strong"),Prr=o("pegasus"),$rr=o(" \u2014 "),kq=a("a"),Irr=o("TFPegasusForConditionalGeneration"),jrr=o(" (Pegasus model)"),Nrr=l(),yT=a("li"),bge=a("strong"),Drr=o("t5"),qrr=o(" \u2014 "),xq=a("a"),Grr=o("TFT5ForConditionalGeneration"),Orr=o(" (T5 model)"),Xrr=l(),vge=a("p"),zrr=o("Examples:"),Vrr=l(),f(bw.$$.fragment),w9e=l(),vc=a("h2"),wT=a("a"),Tge=a("span"),f(vw.$$.fragment),Wrr=l(),Fge=a("span"),Qrr=o("TFAutoModelForSequenceClassification"),A9e=l(),vr=a("div"),f(Tw.$$.fragment),Hrr=l(),Tc=a("p"),Urr=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence classification head) when created
with the `),Cge=a("code"),Jrr=o("from_pretrained()"),Yrr=o("class method or the "),Mge=a("code"),Krr=o("from_config()"),Zrr=o(`class
method.`),etr=l(),Fw=a("p"),otr=o("This class cannot be instantiated directly using "),Ege=a("code"),rtr=o("__init__()"),ttr=o(" (throws an error)."),atr=l(),mt=a("div"),f(Cw.$$.fragment),ntr=l(),yge=a("p"),str=o("Instantiates one of the model classes of the library (with a sequence classification head) from a configuration."),ltr=l(),Fc=a("p"),itr=o(`Note:
Loading a model from its configuration file does `),wge=a("strong"),dtr=o("not"),ctr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Age=a("code"),ftr=o("from_pretrained()"),mtr=o("to load the model weights."),gtr=l(),Lge=a("p"),htr=o("Examples:"),ptr=l(),f(Mw.$$.fragment),_tr=l(),vo=a("div"),f(Ew.$$.fragment),utr=l(),Bge=a("p"),btr=o("Instantiate one of the model classes of the library (with a sequence classification head) from a pretrained model."),vtr=l(),hn=a("p"),Ttr=o("The model class to instantiate is selected based on the "),kge=a("code"),Ftr=o("model_type"),Ctr=o(` property of the config object (either
passed as an argument or loaded from `),xge=a("code"),Mtr=o("pretrained_model_name_or_path"),Etr=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Rge=a("code"),ytr=o("pretrained_model_name_or_path"),wtr=o(":"),Atr=l(),X=a("ul"),AT=a("li"),Sge=a("strong"),Ltr=o("albert"),Btr=o(" \u2014 "),Rq=a("a"),ktr=o("TFAlbertForSequenceClassification"),xtr=o(" (ALBERT model)"),Rtr=l(),LT=a("li"),Pge=a("strong"),Str=o("bert"),Ptr=o(" \u2014 "),Sq=a("a"),$tr=o("TFBertForSequenceClassification"),Itr=o(" (BERT model)"),jtr=l(),BT=a("li"),$ge=a("strong"),Ntr=o("camembert"),Dtr=o(" \u2014 "),Pq=a("a"),qtr=o("TFCamembertForSequenceClassification"),Gtr=o(" (CamemBERT model)"),Otr=l(),kT=a("li"),Ige=a("strong"),Xtr=o("convbert"),ztr=o(" \u2014 "),$q=a("a"),Vtr=o("TFConvBertForSequenceClassification"),Wtr=o(" (ConvBERT model)"),Qtr=l(),xT=a("li"),jge=a("strong"),Htr=o("ctrl"),Utr=o(" \u2014 "),Iq=a("a"),Jtr=o("TFCTRLForSequenceClassification"),Ytr=o(" (CTRL model)"),Ktr=l(),RT=a("li"),Nge=a("strong"),Ztr=o("deberta"),ear=o(" \u2014 "),jq=a("a"),oar=o("TFDebertaForSequenceClassification"),rar=o(" (DeBERTa model)"),tar=l(),ST=a("li"),Dge=a("strong"),aar=o("deberta-v2"),nar=o(" \u2014 "),Nq=a("a"),sar=o("TFDebertaV2ForSequenceClassification"),lar=o(" (DeBERTa-v2 model)"),iar=l(),PT=a("li"),qge=a("strong"),dar=o("distilbert"),car=o(" \u2014 "),Dq=a("a"),far=o("TFDistilBertForSequenceClassification"),mar=o(" (DistilBERT model)"),gar=l(),$T=a("li"),Gge=a("strong"),har=o("electra"),par=o(" \u2014 "),qq=a("a"),_ar=o("TFElectraForSequenceClassification"),uar=o(" (ELECTRA model)"),bar=l(),IT=a("li"),Oge=a("strong"),Tar=o("flaubert"),Far=o(" \u2014 "),Gq=a("a"),Car=o("TFFlaubertForSequenceClassification"),Mar=o(" (FlauBERT model)"),Ear=l(),jT=a("li"),Xge=a("strong"),yar=o("funnel"),war=o(" \u2014 "),Oq=a("a"),Aar=o("TFFunnelForSequenceClassification"),Lar=o(" (Funnel Transformer model)"),Bar=l(),NT=a("li"),zge=a("strong"),kar=o("gpt2"),xar=o(" \u2014 "),Xq=a("a"),Rar=o("TFGPT2ForSequenceClassification"),Sar=o(" (OpenAI GPT-2 model)"),Par=l(),DT=a("li"),Vge=a("strong"),$ar=o("layoutlm"),Iar=o(" \u2014 "),zq=a("a"),jar=o("TFLayoutLMForSequenceClassification"),Nar=o(" (LayoutLM model)"),Dar=l(),qT=a("li"),Wge=a("strong"),qar=o("longformer"),Gar=o(" \u2014 "),Vq=a("a"),Oar=o("TFLongformerForSequenceClassification"),Xar=o(" (Longformer model)"),zar=l(),GT=a("li"),Qge=a("strong"),Var=o("mobilebert"),War=o(" \u2014 "),Wq=a("a"),Qar=o("TFMobileBertForSequenceClassification"),Har=o(" (MobileBERT model)"),Uar=l(),OT=a("li"),Hge=a("strong"),Jar=o("mpnet"),Yar=o(" \u2014 "),Qq=a("a"),Kar=o("TFMPNetForSequenceClassification"),Zar=o(" (MPNet model)"),enr=l(),XT=a("li"),Uge=a("strong"),onr=o("openai-gpt"),rnr=o(" \u2014 "),Hq=a("a"),tnr=o("TFOpenAIGPTForSequenceClassification"),anr=o(" (OpenAI GPT model)"),nnr=l(),zT=a("li"),Jge=a("strong"),snr=o("rembert"),lnr=o(" \u2014 "),Uq=a("a"),inr=o("TFRemBertForSequenceClassification"),dnr=o(" (RemBERT model)"),cnr=l(),VT=a("li"),Yge=a("strong"),fnr=o("roberta"),mnr=o(" \u2014 "),Jq=a("a"),gnr=o("TFRobertaForSequenceClassification"),hnr=o(" (RoBERTa model)"),pnr=l(),WT=a("li"),Kge=a("strong"),_nr=o("roformer"),unr=o(" \u2014 "),Yq=a("a"),bnr=o("TFRoFormerForSequenceClassification"),vnr=o(" (RoFormer model)"),Tnr=l(),QT=a("li"),Zge=a("strong"),Fnr=o("tapas"),Cnr=o(" \u2014 "),Kq=a("a"),Mnr=o("TFTapasForSequenceClassification"),Enr=o(" (TAPAS model)"),ynr=l(),HT=a("li"),ehe=a("strong"),wnr=o("transfo-xl"),Anr=o(" \u2014 "),Zq=a("a"),Lnr=o("TFTransfoXLForSequenceClassification"),Bnr=o(" (Transformer-XL model)"),knr=l(),UT=a("li"),ohe=a("strong"),xnr=o("xlm"),Rnr=o(" \u2014 "),eG=a("a"),Snr=o("TFXLMForSequenceClassification"),Pnr=o(" (XLM model)"),$nr=l(),JT=a("li"),rhe=a("strong"),Inr=o("xlm-roberta"),jnr=o(" \u2014 "),oG=a("a"),Nnr=o("TFXLMRobertaForSequenceClassification"),Dnr=o(" (XLM-RoBERTa model)"),qnr=l(),YT=a("li"),the=a("strong"),Gnr=o("xlnet"),Onr=o(" \u2014 "),rG=a("a"),Xnr=o("TFXLNetForSequenceClassification"),znr=o(" (XLNet model)"),Vnr=l(),ahe=a("p"),Wnr=o("Examples:"),Qnr=l(),f(yw.$$.fragment),L9e=l(),Cc=a("h2"),KT=a("a"),nhe=a("span"),f(ww.$$.fragment),Hnr=l(),she=a("span"),Unr=o("TFAutoModelForMultipleChoice"),B9e=l(),Tr=a("div"),f(Aw.$$.fragment),Jnr=l(),Mc=a("p"),Ynr=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a multiple choice head) when created
with the `),lhe=a("code"),Knr=o("from_pretrained()"),Znr=o("class method or the "),ihe=a("code"),esr=o("from_config()"),osr=o(`class
method.`),rsr=l(),Lw=a("p"),tsr=o("This class cannot be instantiated directly using "),dhe=a("code"),asr=o("__init__()"),nsr=o(" (throws an error)."),ssr=l(),gt=a("div"),f(Bw.$$.fragment),lsr=l(),che=a("p"),isr=o("Instantiates one of the model classes of the library (with a multiple choice head) from a configuration."),dsr=l(),Ec=a("p"),csr=o(`Note:
Loading a model from its configuration file does `),fhe=a("strong"),fsr=o("not"),msr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),mhe=a("code"),gsr=o("from_pretrained()"),hsr=o("to load the model weights."),psr=l(),ghe=a("p"),_sr=o("Examples:"),usr=l(),f(kw.$$.fragment),bsr=l(),To=a("div"),f(xw.$$.fragment),vsr=l(),hhe=a("p"),Tsr=o("Instantiate one of the model classes of the library (with a multiple choice head) from a pretrained model."),Fsr=l(),pn=a("p"),Csr=o("The model class to instantiate is selected based on the "),phe=a("code"),Msr=o("model_type"),Esr=o(` property of the config object (either
passed as an argument or loaded from `),_he=a("code"),ysr=o("pretrained_model_name_or_path"),wsr=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),uhe=a("code"),Asr=o("pretrained_model_name_or_path"),Lsr=o(":"),Bsr=l(),te=a("ul"),ZT=a("li"),bhe=a("strong"),ksr=o("albert"),xsr=o(" \u2014 "),tG=a("a"),Rsr=o("TFAlbertForMultipleChoice"),Ssr=o(" (ALBERT model)"),Psr=l(),e8=a("li"),vhe=a("strong"),$sr=o("bert"),Isr=o(" \u2014 "),aG=a("a"),jsr=o("TFBertForMultipleChoice"),Nsr=o(" (BERT model)"),Dsr=l(),o8=a("li"),The=a("strong"),qsr=o("camembert"),Gsr=o(" \u2014 "),nG=a("a"),Osr=o("TFCamembertForMultipleChoice"),Xsr=o(" (CamemBERT model)"),zsr=l(),r8=a("li"),Fhe=a("strong"),Vsr=o("convbert"),Wsr=o(" \u2014 "),sG=a("a"),Qsr=o("TFConvBertForMultipleChoice"),Hsr=o(" (ConvBERT model)"),Usr=l(),t8=a("li"),Che=a("strong"),Jsr=o("distilbert"),Ysr=o(" \u2014 "),lG=a("a"),Ksr=o("TFDistilBertForMultipleChoice"),Zsr=o(" (DistilBERT model)"),elr=l(),a8=a("li"),Mhe=a("strong"),olr=o("electra"),rlr=o(" \u2014 "),iG=a("a"),tlr=o("TFElectraForMultipleChoice"),alr=o(" (ELECTRA model)"),nlr=l(),n8=a("li"),Ehe=a("strong"),slr=o("flaubert"),llr=o(" \u2014 "),dG=a("a"),ilr=o("TFFlaubertForMultipleChoice"),dlr=o(" (FlauBERT model)"),clr=l(),s8=a("li"),yhe=a("strong"),flr=o("funnel"),mlr=o(" \u2014 "),cG=a("a"),glr=o("TFFunnelForMultipleChoice"),hlr=o(" (Funnel Transformer model)"),plr=l(),l8=a("li"),whe=a("strong"),_lr=o("longformer"),ulr=o(" \u2014 "),fG=a("a"),blr=o("TFLongformerForMultipleChoice"),vlr=o(" (Longformer model)"),Tlr=l(),i8=a("li"),Ahe=a("strong"),Flr=o("mobilebert"),Clr=o(" \u2014 "),mG=a("a"),Mlr=o("TFMobileBertForMultipleChoice"),Elr=o(" (MobileBERT model)"),ylr=l(),d8=a("li"),Lhe=a("strong"),wlr=o("mpnet"),Alr=o(" \u2014 "),gG=a("a"),Llr=o("TFMPNetForMultipleChoice"),Blr=o(" (MPNet model)"),klr=l(),c8=a("li"),Bhe=a("strong"),xlr=o("rembert"),Rlr=o(" \u2014 "),hG=a("a"),Slr=o("TFRemBertForMultipleChoice"),Plr=o(" (RemBERT model)"),$lr=l(),f8=a("li"),khe=a("strong"),Ilr=o("roberta"),jlr=o(" \u2014 "),pG=a("a"),Nlr=o("TFRobertaForMultipleChoice"),Dlr=o(" (RoBERTa model)"),qlr=l(),m8=a("li"),xhe=a("strong"),Glr=o("roformer"),Olr=o(" \u2014 "),_G=a("a"),Xlr=o("TFRoFormerForMultipleChoice"),zlr=o(" (RoFormer model)"),Vlr=l(),g8=a("li"),Rhe=a("strong"),Wlr=o("xlm"),Qlr=o(" \u2014 "),uG=a("a"),Hlr=o("TFXLMForMultipleChoice"),Ulr=o(" (XLM model)"),Jlr=l(),h8=a("li"),She=a("strong"),Ylr=o("xlm-roberta"),Klr=o(" \u2014 "),bG=a("a"),Zlr=o("TFXLMRobertaForMultipleChoice"),eir=o(" (XLM-RoBERTa model)"),oir=l(),p8=a("li"),Phe=a("strong"),rir=o("xlnet"),tir=o(" \u2014 "),vG=a("a"),air=o("TFXLNetForMultipleChoice"),nir=o(" (XLNet model)"),sir=l(),$he=a("p"),lir=o("Examples:"),iir=l(),f(Rw.$$.fragment),k9e=l(),yc=a("h2"),_8=a("a"),Ihe=a("span"),f(Sw.$$.fragment),dir=l(),jhe=a("span"),cir=o("TFAutoModelForTableQuestionAnswering"),x9e=l(),Fr=a("div"),f(Pw.$$.fragment),fir=l(),wc=a("p"),mir=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a table question answering head) when created
with the `),Nhe=a("code"),gir=o("from_pretrained()"),hir=o("class method or the "),Dhe=a("code"),pir=o("from_config()"),_ir=o(`class
method.`),uir=l(),$w=a("p"),bir=o("This class cannot be instantiated directly using "),qhe=a("code"),vir=o("__init__()"),Tir=o(" (throws an error)."),Fir=l(),ht=a("div"),f(Iw.$$.fragment),Cir=l(),Ghe=a("p"),Mir=o("Instantiates one of the model classes of the library (with a table question answering head) from a configuration."),Eir=l(),Ac=a("p"),yir=o(`Note:
Loading a model from its configuration file does `),Ohe=a("strong"),wir=o("not"),Air=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Xhe=a("code"),Lir=o("from_pretrained()"),Bir=o("to load the model weights."),kir=l(),zhe=a("p"),xir=o("Examples:"),Rir=l(),f(jw.$$.fragment),Sir=l(),Fo=a("div"),f(Nw.$$.fragment),Pir=l(),Vhe=a("p"),$ir=o("Instantiate one of the model classes of the library (with a table question answering head) from a pretrained model."),Iir=l(),_n=a("p"),jir=o("The model class to instantiate is selected based on the "),Whe=a("code"),Nir=o("model_type"),Dir=o(` property of the config object (either
passed as an argument or loaded from `),Qhe=a("code"),qir=o("pretrained_model_name_or_path"),Gir=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Hhe=a("code"),Oir=o("pretrained_model_name_or_path"),Xir=o(":"),zir=l(),Uhe=a("ul"),u8=a("li"),Jhe=a("strong"),Vir=o("tapas"),Wir=o(" \u2014 "),TG=a("a"),Qir=o("TFTapasForQuestionAnswering"),Hir=o(" (TAPAS model)"),Uir=l(),Yhe=a("p"),Jir=o("Examples:"),Yir=l(),f(Dw.$$.fragment),R9e=l(),Lc=a("h2"),b8=a("a"),Khe=a("span"),f(qw.$$.fragment),Kir=l(),Zhe=a("span"),Zir=o("TFAutoModelForTokenClassification"),S9e=l(),Cr=a("div"),f(Gw.$$.fragment),edr=l(),Bc=a("p"),odr=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a token classification head) when created
with the `),epe=a("code"),rdr=o("from_pretrained()"),tdr=o("class method or the "),ope=a("code"),adr=o("from_config()"),ndr=o(`class
method.`),sdr=l(),Ow=a("p"),ldr=o("This class cannot be instantiated directly using "),rpe=a("code"),idr=o("__init__()"),ddr=o(" (throws an error)."),cdr=l(),pt=a("div"),f(Xw.$$.fragment),fdr=l(),tpe=a("p"),mdr=o("Instantiates one of the model classes of the library (with a token classification head) from a configuration."),gdr=l(),kc=a("p"),hdr=o(`Note:
Loading a model from its configuration file does `),ape=a("strong"),pdr=o("not"),_dr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),npe=a("code"),udr=o("from_pretrained()"),bdr=o("to load the model weights."),vdr=l(),spe=a("p"),Tdr=o("Examples:"),Fdr=l(),f(zw.$$.fragment),Cdr=l(),Co=a("div"),f(Vw.$$.fragment),Mdr=l(),lpe=a("p"),Edr=o("Instantiate one of the model classes of the library (with a token classification head) from a pretrained model."),ydr=l(),un=a("p"),wdr=o("The model class to instantiate is selected based on the "),ipe=a("code"),Adr=o("model_type"),Ldr=o(` property of the config object (either
passed as an argument or loaded from `),dpe=a("code"),Bdr=o("pretrained_model_name_or_path"),kdr=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),cpe=a("code"),xdr=o("pretrained_model_name_or_path"),Rdr=o(":"),Sdr=l(),K=a("ul"),v8=a("li"),fpe=a("strong"),Pdr=o("albert"),$dr=o(" \u2014 "),FG=a("a"),Idr=o("TFAlbertForTokenClassification"),jdr=o(" (ALBERT model)"),Ndr=l(),T8=a("li"),mpe=a("strong"),Ddr=o("bert"),qdr=o(" \u2014 "),CG=a("a"),Gdr=o("TFBertForTokenClassification"),Odr=o(" (BERT model)"),Xdr=l(),F8=a("li"),gpe=a("strong"),zdr=o("camembert"),Vdr=o(" \u2014 "),MG=a("a"),Wdr=o("TFCamembertForTokenClassification"),Qdr=o(" (CamemBERT model)"),Hdr=l(),C8=a("li"),hpe=a("strong"),Udr=o("convbert"),Jdr=o(" \u2014 "),EG=a("a"),Ydr=o("TFConvBertForTokenClassification"),Kdr=o(" (ConvBERT model)"),Zdr=l(),M8=a("li"),ppe=a("strong"),ecr=o("deberta"),ocr=o(" \u2014 "),yG=a("a"),rcr=o("TFDebertaForTokenClassification"),tcr=o(" (DeBERTa model)"),acr=l(),E8=a("li"),_pe=a("strong"),ncr=o("deberta-v2"),scr=o(" \u2014 "),wG=a("a"),lcr=o("TFDebertaV2ForTokenClassification"),icr=o(" (DeBERTa-v2 model)"),dcr=l(),y8=a("li"),upe=a("strong"),ccr=o("distilbert"),fcr=o(" \u2014 "),AG=a("a"),mcr=o("TFDistilBertForTokenClassification"),gcr=o(" (DistilBERT model)"),hcr=l(),w8=a("li"),bpe=a("strong"),pcr=o("electra"),_cr=o(" \u2014 "),LG=a("a"),ucr=o("TFElectraForTokenClassification"),bcr=o(" (ELECTRA model)"),vcr=l(),A8=a("li"),vpe=a("strong"),Tcr=o("flaubert"),Fcr=o(" \u2014 "),BG=a("a"),Ccr=o("TFFlaubertForTokenClassification"),Mcr=o(" (FlauBERT model)"),Ecr=l(),L8=a("li"),Tpe=a("strong"),ycr=o("funnel"),wcr=o(" \u2014 "),kG=a("a"),Acr=o("TFFunnelForTokenClassification"),Lcr=o(" (Funnel Transformer model)"),Bcr=l(),B8=a("li"),Fpe=a("strong"),kcr=o("layoutlm"),xcr=o(" \u2014 "),xG=a("a"),Rcr=o("TFLayoutLMForTokenClassification"),Scr=o(" (LayoutLM model)"),Pcr=l(),k8=a("li"),Cpe=a("strong"),$cr=o("longformer"),Icr=o(" \u2014 "),RG=a("a"),jcr=o("TFLongformerForTokenClassification"),Ncr=o(" (Longformer model)"),Dcr=l(),x8=a("li"),Mpe=a("strong"),qcr=o("mobilebert"),Gcr=o(" \u2014 "),SG=a("a"),Ocr=o("TFMobileBertForTokenClassification"),Xcr=o(" (MobileBERT model)"),zcr=l(),R8=a("li"),Epe=a("strong"),Vcr=o("mpnet"),Wcr=o(" \u2014 "),PG=a("a"),Qcr=o("TFMPNetForTokenClassification"),Hcr=o(" (MPNet model)"),Ucr=l(),S8=a("li"),ype=a("strong"),Jcr=o("rembert"),Ycr=o(" \u2014 "),$G=a("a"),Kcr=o("TFRemBertForTokenClassification"),Zcr=o(" (RemBERT model)"),efr=l(),P8=a("li"),wpe=a("strong"),ofr=o("roberta"),rfr=o(" \u2014 "),IG=a("a"),tfr=o("TFRobertaForTokenClassification"),afr=o(" (RoBERTa model)"),nfr=l(),$8=a("li"),Ape=a("strong"),sfr=o("roformer"),lfr=o(" \u2014 "),jG=a("a"),ifr=o("TFRoFormerForTokenClassification"),dfr=o(" (RoFormer model)"),cfr=l(),I8=a("li"),Lpe=a("strong"),ffr=o("xlm"),mfr=o(" \u2014 "),NG=a("a"),gfr=o("TFXLMForTokenClassification"),hfr=o(" (XLM model)"),pfr=l(),j8=a("li"),Bpe=a("strong"),_fr=o("xlm-roberta"),ufr=o(" \u2014 "),DG=a("a"),bfr=o("TFXLMRobertaForTokenClassification"),vfr=o(" (XLM-RoBERTa model)"),Tfr=l(),N8=a("li"),kpe=a("strong"),Ffr=o("xlnet"),Cfr=o(" \u2014 "),qG=a("a"),Mfr=o("TFXLNetForTokenClassification"),Efr=o(" (XLNet model)"),yfr=l(),xpe=a("p"),wfr=o("Examples:"),Afr=l(),f(Ww.$$.fragment),P9e=l(),xc=a("h2"),D8=a("a"),Rpe=a("span"),f(Qw.$$.fragment),Lfr=l(),Spe=a("span"),Bfr=o("TFAutoModelForQuestionAnswering"),$9e=l(),Mr=a("div"),f(Hw.$$.fragment),kfr=l(),Rc=a("p"),xfr=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a question answering head) when created
with the `),Ppe=a("code"),Rfr=o("from_pretrained()"),Sfr=o("class method or the "),$pe=a("code"),Pfr=o("from_config()"),$fr=o(`class
method.`),Ifr=l(),Uw=a("p"),jfr=o("This class cannot be instantiated directly using "),Ipe=a("code"),Nfr=o("__init__()"),Dfr=o(" (throws an error)."),qfr=l(),_t=a("div"),f(Jw.$$.fragment),Gfr=l(),jpe=a("p"),Ofr=o("Instantiates one of the model classes of the library (with a question answering head) from a configuration."),Xfr=l(),Sc=a("p"),zfr=o(`Note:
Loading a model from its configuration file does `),Npe=a("strong"),Vfr=o("not"),Wfr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Dpe=a("code"),Qfr=o("from_pretrained()"),Hfr=o("to load the model weights."),Ufr=l(),qpe=a("p"),Jfr=o("Examples:"),Yfr=l(),f(Yw.$$.fragment),Kfr=l(),Mo=a("div"),f(Kw.$$.fragment),Zfr=l(),Gpe=a("p"),emr=o("Instantiate one of the model classes of the library (with a question answering head) from a pretrained model."),omr=l(),bn=a("p"),rmr=o("The model class to instantiate is selected based on the "),Ope=a("code"),tmr=o("model_type"),amr=o(` property of the config object (either
passed as an argument or loaded from `),Xpe=a("code"),nmr=o("pretrained_model_name_or_path"),smr=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),zpe=a("code"),lmr=o("pretrained_model_name_or_path"),imr=o(":"),dmr=l(),Z=a("ul"),q8=a("li"),Vpe=a("strong"),cmr=o("albert"),fmr=o(" \u2014 "),GG=a("a"),mmr=o("TFAlbertForQuestionAnswering"),gmr=o(" (ALBERT model)"),hmr=l(),G8=a("li"),Wpe=a("strong"),pmr=o("bert"),_mr=o(" \u2014 "),OG=a("a"),umr=o("TFBertForQuestionAnswering"),bmr=o(" (BERT model)"),vmr=l(),O8=a("li"),Qpe=a("strong"),Tmr=o("camembert"),Fmr=o(" \u2014 "),XG=a("a"),Cmr=o("TFCamembertForQuestionAnswering"),Mmr=o(" (CamemBERT model)"),Emr=l(),X8=a("li"),Hpe=a("strong"),ymr=o("convbert"),wmr=o(" \u2014 "),zG=a("a"),Amr=o("TFConvBertForQuestionAnswering"),Lmr=o(" (ConvBERT model)"),Bmr=l(),z8=a("li"),Upe=a("strong"),kmr=o("deberta"),xmr=o(" \u2014 "),VG=a("a"),Rmr=o("TFDebertaForQuestionAnswering"),Smr=o(" (DeBERTa model)"),Pmr=l(),V8=a("li"),Jpe=a("strong"),$mr=o("deberta-v2"),Imr=o(" \u2014 "),WG=a("a"),jmr=o("TFDebertaV2ForQuestionAnswering"),Nmr=o(" (DeBERTa-v2 model)"),Dmr=l(),W8=a("li"),Ype=a("strong"),qmr=o("distilbert"),Gmr=o(" \u2014 "),QG=a("a"),Omr=o("TFDistilBertForQuestionAnswering"),Xmr=o(" (DistilBERT model)"),zmr=l(),Q8=a("li"),Kpe=a("strong"),Vmr=o("electra"),Wmr=o(" \u2014 "),HG=a("a"),Qmr=o("TFElectraForQuestionAnswering"),Hmr=o(" (ELECTRA model)"),Umr=l(),H8=a("li"),Zpe=a("strong"),Jmr=o("flaubert"),Ymr=o(" \u2014 "),UG=a("a"),Kmr=o("TFFlaubertForQuestionAnsweringSimple"),Zmr=o(" (FlauBERT model)"),egr=l(),U8=a("li"),e_e=a("strong"),ogr=o("funnel"),rgr=o(" \u2014 "),JG=a("a"),tgr=o("TFFunnelForQuestionAnswering"),agr=o(" (Funnel Transformer model)"),ngr=l(),J8=a("li"),o_e=a("strong"),sgr=o("longformer"),lgr=o(" \u2014 "),YG=a("a"),igr=o("TFLongformerForQuestionAnswering"),dgr=o(" (Longformer model)"),cgr=l(),Y8=a("li"),r_e=a("strong"),fgr=o("mobilebert"),mgr=o(" \u2014 "),KG=a("a"),ggr=o("TFMobileBertForQuestionAnswering"),hgr=o(" (MobileBERT model)"),pgr=l(),K8=a("li"),t_e=a("strong"),_gr=o("mpnet"),ugr=o(" \u2014 "),ZG=a("a"),bgr=o("TFMPNetForQuestionAnswering"),vgr=o(" (MPNet model)"),Tgr=l(),Z8=a("li"),a_e=a("strong"),Fgr=o("rembert"),Cgr=o(" \u2014 "),eO=a("a"),Mgr=o("TFRemBertForQuestionAnswering"),Egr=o(" (RemBERT model)"),ygr=l(),eF=a("li"),n_e=a("strong"),wgr=o("roberta"),Agr=o(" \u2014 "),oO=a("a"),Lgr=o("TFRobertaForQuestionAnswering"),Bgr=o(" (RoBERTa model)"),kgr=l(),oF=a("li"),s_e=a("strong"),xgr=o("roformer"),Rgr=o(" \u2014 "),rO=a("a"),Sgr=o("TFRoFormerForQuestionAnswering"),Pgr=o(" (RoFormer model)"),$gr=l(),rF=a("li"),l_e=a("strong"),Igr=o("xlm"),jgr=o(" \u2014 "),tO=a("a"),Ngr=o("TFXLMForQuestionAnsweringSimple"),Dgr=o(" (XLM model)"),qgr=l(),tF=a("li"),i_e=a("strong"),Ggr=o("xlm-roberta"),Ogr=o(" \u2014 "),aO=a("a"),Xgr=o("TFXLMRobertaForQuestionAnswering"),zgr=o(" (XLM-RoBERTa model)"),Vgr=l(),aF=a("li"),d_e=a("strong"),Wgr=o("xlnet"),Qgr=o(" \u2014 "),nO=a("a"),Hgr=o("TFXLNetForQuestionAnsweringSimple"),Ugr=o(" (XLNet model)"),Jgr=l(),c_e=a("p"),Ygr=o("Examples:"),Kgr=l(),f(Zw.$$.fragment),I9e=l(),Pc=a("h2"),nF=a("a"),f_e=a("span"),f(eA.$$.fragment),Zgr=l(),m_e=a("span"),ehr=o("TFAutoModelForVision2Seq"),j9e=l(),Er=a("div"),f(oA.$$.fragment),ohr=l(),$c=a("p"),rhr=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a vision-to-text modeling head) when created
with the `),g_e=a("code"),thr=o("from_pretrained()"),ahr=o("class method or the "),h_e=a("code"),nhr=o("from_config()"),shr=o(`class
method.`),lhr=l(),rA=a("p"),ihr=o("This class cannot be instantiated directly using "),p_e=a("code"),dhr=o("__init__()"),chr=o(" (throws an error)."),fhr=l(),ut=a("div"),f(tA.$$.fragment),mhr=l(),__e=a("p"),ghr=o("Instantiates one of the model classes of the library (with a vision-to-text modeling head) from a configuration."),hhr=l(),Ic=a("p"),phr=o(`Note:
Loading a model from its configuration file does `),u_e=a("strong"),_hr=o("not"),uhr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),b_e=a("code"),bhr=o("from_pretrained()"),vhr=o("to load the model weights."),Thr=l(),v_e=a("p"),Fhr=o("Examples:"),Chr=l(),f(aA.$$.fragment),Mhr=l(),Eo=a("div"),f(nA.$$.fragment),Ehr=l(),T_e=a("p"),yhr=o("Instantiate one of the model classes of the library (with a vision-to-text modeling head) from a pretrained model."),whr=l(),vn=a("p"),Ahr=o("The model class to instantiate is selected based on the "),F_e=a("code"),Lhr=o("model_type"),Bhr=o(` property of the config object (either
passed as an argument or loaded from `),C_e=a("code"),khr=o("pretrained_model_name_or_path"),xhr=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),M_e=a("code"),Rhr=o("pretrained_model_name_or_path"),Shr=o(":"),Phr=l(),E_e=a("ul"),sF=a("li"),y_e=a("strong"),$hr=o("vision-encoder-decoder"),Ihr=o(" \u2014 "),sO=a("a"),jhr=o("TFVisionEncoderDecoderModel"),Nhr=o(" (Vision Encoder decoder model)"),Dhr=l(),w_e=a("p"),qhr=o("Examples:"),Ghr=l(),f(sA.$$.fragment),N9e=l(),jc=a("h2"),lF=a("a"),A_e=a("span"),f(lA.$$.fragment),Ohr=l(),L_e=a("span"),Xhr=o("TFAutoModelForSpeechSeq2Seq"),D9e=l(),yr=a("div"),f(iA.$$.fragment),zhr=l(),Nc=a("p"),Vhr=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) when created
with the `),B_e=a("code"),Whr=o("from_pretrained()"),Qhr=o("class method or the "),k_e=a("code"),Hhr=o("from_config()"),Uhr=o(`class
method.`),Jhr=l(),dA=a("p"),Yhr=o("This class cannot be instantiated directly using "),x_e=a("code"),Khr=o("__init__()"),Zhr=o(" (throws an error)."),epr=l(),bt=a("div"),f(cA.$$.fragment),opr=l(),R_e=a("p"),rpr=o("Instantiates one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) from a configuration."),tpr=l(),Dc=a("p"),apr=o(`Note:
Loading a model from its configuration file does `),S_e=a("strong"),npr=o("not"),spr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),P_e=a("code"),lpr=o("from_pretrained()"),ipr=o("to load the model weights."),dpr=l(),$_e=a("p"),cpr=o("Examples:"),fpr=l(),f(fA.$$.fragment),mpr=l(),yo=a("div"),f(mA.$$.fragment),gpr=l(),I_e=a("p"),hpr=o("Instantiate one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) from a pretrained model."),ppr=l(),Tn=a("p"),_pr=o("The model class to instantiate is selected based on the "),j_e=a("code"),upr=o("model_type"),bpr=o(` property of the config object (either
passed as an argument or loaded from `),N_e=a("code"),vpr=o("pretrained_model_name_or_path"),Tpr=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),D_e=a("code"),Fpr=o("pretrained_model_name_or_path"),Cpr=o(":"),Mpr=l(),q_e=a("ul"),iF=a("li"),G_e=a("strong"),Epr=o("speech_to_text"),ypr=o(" \u2014 "),lO=a("a"),wpr=o("TFSpeech2TextForConditionalGeneration"),Apr=o(" (Speech2Text model)"),Lpr=l(),O_e=a("p"),Bpr=o("Examples:"),kpr=l(),f(gA.$$.fragment),q9e=l(),qc=a("h2"),dF=a("a"),X_e=a("span"),f(hA.$$.fragment),xpr=l(),z_e=a("span"),Rpr=o("FlaxAutoModel"),G9e=l(),wr=a("div"),f(pA.$$.fragment),Spr=l(),Gc=a("p"),Ppr=o(`This is a generic model class that will be instantiated as one of the base model classes of the library when created
with the `),V_e=a("code"),$pr=o("from_pretrained()"),Ipr=o("class method or the "),W_e=a("code"),jpr=o("from_config()"),Npr=o(`class
method.`),Dpr=l(),_A=a("p"),qpr=o("This class cannot be instantiated directly using "),Q_e=a("code"),Gpr=o("__init__()"),Opr=o(" (throws an error)."),Xpr=l(),vt=a("div"),f(uA.$$.fragment),zpr=l(),H_e=a("p"),Vpr=o("Instantiates one of the base model classes of the library from a configuration."),Wpr=l(),Oc=a("p"),Qpr=o(`Note:
Loading a model from its configuration file does `),U_e=a("strong"),Hpr=o("not"),Upr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),J_e=a("code"),Jpr=o("from_pretrained()"),Ypr=o("to load the model weights."),Kpr=l(),Y_e=a("p"),Zpr=o("Examples:"),e_r=l(),f(bA.$$.fragment),o_r=l(),wo=a("div"),f(vA.$$.fragment),r_r=l(),K_e=a("p"),t_r=o("Instantiate one of the base model classes of the library from a pretrained model."),a_r=l(),Fn=a("p"),n_r=o("The model class to instantiate is selected based on the "),Z_e=a("code"),s_r=o("model_type"),l_r=o(` property of the config object (either
passed as an argument or loaded from `),eue=a("code"),i_r=o("pretrained_model_name_or_path"),d_r=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),oue=a("code"),c_r=o("pretrained_model_name_or_path"),f_r=o(":"),m_r=l(),V=a("ul"),cF=a("li"),rue=a("strong"),g_r=o("albert"),h_r=o(" \u2014 "),iO=a("a"),p_r=o("FlaxAlbertModel"),__r=o(" (ALBERT model)"),u_r=l(),fF=a("li"),tue=a("strong"),b_r=o("bart"),v_r=o(" \u2014 "),dO=a("a"),T_r=o("FlaxBartModel"),F_r=o(" (BART model)"),C_r=l(),mF=a("li"),aue=a("strong"),M_r=o("beit"),E_r=o(" \u2014 "),cO=a("a"),y_r=o("FlaxBeitModel"),w_r=o(" (BEiT model)"),A_r=l(),gF=a("li"),nue=a("strong"),L_r=o("bert"),B_r=o(" \u2014 "),fO=a("a"),k_r=o("FlaxBertModel"),x_r=o(" (BERT model)"),R_r=l(),hF=a("li"),sue=a("strong"),S_r=o("big_bird"),P_r=o(" \u2014 "),mO=a("a"),$_r=o("FlaxBigBirdModel"),I_r=o(" (BigBird model)"),j_r=l(),pF=a("li"),lue=a("strong"),N_r=o("blenderbot"),D_r=o(" \u2014 "),gO=a("a"),q_r=o("FlaxBlenderbotModel"),G_r=o(" (Blenderbot model)"),O_r=l(),_F=a("li"),iue=a("strong"),X_r=o("blenderbot-small"),z_r=o(" \u2014 "),hO=a("a"),V_r=o("FlaxBlenderbotSmallModel"),W_r=o(" (BlenderbotSmall model)"),Q_r=l(),uF=a("li"),due=a("strong"),H_r=o("clip"),U_r=o(" \u2014 "),pO=a("a"),J_r=o("FlaxCLIPModel"),Y_r=o(" (CLIP model)"),K_r=l(),bF=a("li"),cue=a("strong"),Z_r=o("distilbert"),eur=o(" \u2014 "),_O=a("a"),our=o("FlaxDistilBertModel"),rur=o(" (DistilBERT model)"),tur=l(),vF=a("li"),fue=a("strong"),aur=o("electra"),nur=o(" \u2014 "),uO=a("a"),sur=o("FlaxElectraModel"),lur=o(" (ELECTRA model)"),iur=l(),TF=a("li"),mue=a("strong"),dur=o("gpt2"),cur=o(" \u2014 "),bO=a("a"),fur=o("FlaxGPT2Model"),mur=o(" (OpenAI GPT-2 model)"),gur=l(),FF=a("li"),gue=a("strong"),hur=o("gpt_neo"),pur=o(" \u2014 "),vO=a("a"),_ur=o("FlaxGPTNeoModel"),uur=o(" (GPT Neo model)"),bur=l(),CF=a("li"),hue=a("strong"),vur=o("gptj"),Tur=o(" \u2014 "),TO=a("a"),Fur=o("FlaxGPTJModel"),Cur=o(" (GPT-J model)"),Mur=l(),MF=a("li"),pue=a("strong"),Eur=o("marian"),yur=o(" \u2014 "),FO=a("a"),wur=o("FlaxMarianModel"),Aur=o(" (Marian model)"),Lur=l(),EF=a("li"),_ue=a("strong"),Bur=o("mbart"),kur=o(" \u2014 "),CO=a("a"),xur=o("FlaxMBartModel"),Rur=o(" (mBART model)"),Sur=l(),yF=a("li"),uue=a("strong"),Pur=o("mt5"),$ur=o(" \u2014 "),MO=a("a"),Iur=o("FlaxMT5Model"),jur=o(" (mT5 model)"),Nur=l(),wF=a("li"),bue=a("strong"),Dur=o("pegasus"),qur=o(" \u2014 "),EO=a("a"),Gur=o("FlaxPegasusModel"),Our=o(" (Pegasus model)"),Xur=l(),AF=a("li"),vue=a("strong"),zur=o("roberta"),Vur=o(" \u2014 "),yO=a("a"),Wur=o("FlaxRobertaModel"),Qur=o(" (RoBERTa model)"),Hur=l(),LF=a("li"),Tue=a("strong"),Uur=o("roformer"),Jur=o(" \u2014 "),wO=a("a"),Yur=o("FlaxRoFormerModel"),Kur=o(" (RoFormer model)"),Zur=l(),BF=a("li"),Fue=a("strong"),e2r=o("t5"),o2r=o(" \u2014 "),AO=a("a"),r2r=o("FlaxT5Model"),t2r=o(" (T5 model)"),a2r=l(),kF=a("li"),Cue=a("strong"),n2r=o("vision-text-dual-encoder"),s2r=o(" \u2014 "),LO=a("a"),l2r=o("FlaxVisionTextDualEncoderModel"),i2r=o(" (VisionTextDualEncoder model)"),d2r=l(),xF=a("li"),Mue=a("strong"),c2r=o("vit"),f2r=o(" \u2014 "),BO=a("a"),m2r=o("FlaxViTModel"),g2r=o(" (ViT model)"),h2r=l(),RF=a("li"),Eue=a("strong"),p2r=o("wav2vec2"),_2r=o(" \u2014 "),kO=a("a"),u2r=o("FlaxWav2Vec2Model"),b2r=o(" (Wav2Vec2 model)"),v2r=l(),SF=a("li"),yue=a("strong"),T2r=o("xglm"),F2r=o(" \u2014 "),xO=a("a"),C2r=o("FlaxXGLMModel"),M2r=o(" (XGLM model)"),E2r=l(),wue=a("p"),y2r=o("Examples:"),w2r=l(),f(TA.$$.fragment),O9e=l(),Xc=a("h2"),PF=a("a"),Aue=a("span"),f(FA.$$.fragment),A2r=l(),Lue=a("span"),L2r=o("FlaxAutoModelForCausalLM"),X9e=l(),Ar=a("div"),f(CA.$$.fragment),B2r=l(),zc=a("p"),k2r=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a causal language modeling head) when created
with the `),Bue=a("code"),x2r=o("from_pretrained()"),R2r=o("class method or the "),kue=a("code"),S2r=o("from_config()"),P2r=o(`class
method.`),$2r=l(),MA=a("p"),I2r=o("This class cannot be instantiated directly using "),xue=a("code"),j2r=o("__init__()"),N2r=o(" (throws an error)."),D2r=l(),Tt=a("div"),f(EA.$$.fragment),q2r=l(),Rue=a("p"),G2r=o("Instantiates one of the model classes of the library (with a causal language modeling head) from a configuration."),O2r=l(),Vc=a("p"),X2r=o(`Note:
Loading a model from its configuration file does `),Sue=a("strong"),z2r=o("not"),V2r=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Pue=a("code"),W2r=o("from_pretrained()"),Q2r=o("to load the model weights."),H2r=l(),$ue=a("p"),U2r=o("Examples:"),J2r=l(),f(yA.$$.fragment),Y2r=l(),Ao=a("div"),f(wA.$$.fragment),K2r=l(),Iue=a("p"),Z2r=o("Instantiate one of the model classes of the library (with a causal language modeling head) from a pretrained model."),e1r=l(),Cn=a("p"),o1r=o("The model class to instantiate is selected based on the "),jue=a("code"),r1r=o("model_type"),t1r=o(` property of the config object (either
passed as an argument or loaded from `),Nue=a("code"),a1r=o("pretrained_model_name_or_path"),n1r=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Due=a("code"),s1r=o("pretrained_model_name_or_path"),l1r=o(":"),i1r=l(),Mn=a("ul"),$F=a("li"),que=a("strong"),d1r=o("gpt2"),c1r=o(" \u2014 "),RO=a("a"),f1r=o("FlaxGPT2LMHeadModel"),m1r=o(" (OpenAI GPT-2 model)"),g1r=l(),IF=a("li"),Gue=a("strong"),h1r=o("gpt_neo"),p1r=o(" \u2014 "),SO=a("a"),_1r=o("FlaxGPTNeoForCausalLM"),u1r=o(" (GPT Neo model)"),b1r=l(),jF=a("li"),Oue=a("strong"),v1r=o("gptj"),T1r=o(" \u2014 "),PO=a("a"),F1r=o("FlaxGPTJForCausalLM"),C1r=o(" (GPT-J model)"),M1r=l(),NF=a("li"),Xue=a("strong"),E1r=o("xglm"),y1r=o(" \u2014 "),$O=a("a"),w1r=o("FlaxXGLMForCausalLM"),A1r=o(" (XGLM model)"),L1r=l(),zue=a("p"),B1r=o("Examples:"),k1r=l(),f(AA.$$.fragment),z9e=l(),Wc=a("h2"),DF=a("a"),Vue=a("span"),f(LA.$$.fragment),x1r=l(),Wue=a("span"),R1r=o("FlaxAutoModelForPreTraining"),V9e=l(),Lr=a("div"),f(BA.$$.fragment),S1r=l(),Qc=a("p"),P1r=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a pretraining head) when created
with the `),Que=a("code"),$1r=o("from_pretrained()"),I1r=o("class method or the "),Hue=a("code"),j1r=o("from_config()"),N1r=o(`class
method.`),D1r=l(),kA=a("p"),q1r=o("This class cannot be instantiated directly using "),Uue=a("code"),G1r=o("__init__()"),O1r=o(" (throws an error)."),X1r=l(),Ft=a("div"),f(xA.$$.fragment),z1r=l(),Jue=a("p"),V1r=o("Instantiates one of the model classes of the library (with a pretraining head) from a configuration."),W1r=l(),Hc=a("p"),Q1r=o(`Note:
Loading a model from its configuration file does `),Yue=a("strong"),H1r=o("not"),U1r=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Kue=a("code"),J1r=o("from_pretrained()"),Y1r=o("to load the model weights."),K1r=l(),Zue=a("p"),Z1r=o("Examples:"),ebr=l(),f(RA.$$.fragment),obr=l(),Lo=a("div"),f(SA.$$.fragment),rbr=l(),e2e=a("p"),tbr=o("Instantiate one of the model classes of the library (with a pretraining head) from a pretrained model."),abr=l(),En=a("p"),nbr=o("The model class to instantiate is selected based on the "),o2e=a("code"),sbr=o("model_type"),lbr=o(` property of the config object (either
passed as an argument or loaded from `),r2e=a("code"),ibr=o("pretrained_model_name_or_path"),dbr=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),t2e=a("code"),cbr=o("pretrained_model_name_or_path"),fbr=o(":"),mbr=l(),fe=a("ul"),qF=a("li"),a2e=a("strong"),gbr=o("albert"),hbr=o(" \u2014 "),IO=a("a"),pbr=o("FlaxAlbertForPreTraining"),_br=o(" (ALBERT model)"),ubr=l(),GF=a("li"),n2e=a("strong"),bbr=o("bart"),vbr=o(" \u2014 "),jO=a("a"),Tbr=o("FlaxBartForConditionalGeneration"),Fbr=o(" (BART model)"),Cbr=l(),OF=a("li"),s2e=a("strong"),Mbr=o("bert"),Ebr=o(" \u2014 "),NO=a("a"),ybr=o("FlaxBertForPreTraining"),wbr=o(" (BERT model)"),Abr=l(),XF=a("li"),l2e=a("strong"),Lbr=o("big_bird"),Bbr=o(" \u2014 "),DO=a("a"),kbr=o("FlaxBigBirdForPreTraining"),xbr=o(" (BigBird model)"),Rbr=l(),zF=a("li"),i2e=a("strong"),Sbr=o("electra"),Pbr=o(" \u2014 "),qO=a("a"),$br=o("FlaxElectraForPreTraining"),Ibr=o(" (ELECTRA model)"),jbr=l(),VF=a("li"),d2e=a("strong"),Nbr=o("mbart"),Dbr=o(" \u2014 "),GO=a("a"),qbr=o("FlaxMBartForConditionalGeneration"),Gbr=o(" (mBART model)"),Obr=l(),WF=a("li"),c2e=a("strong"),Xbr=o("mt5"),zbr=o(" \u2014 "),OO=a("a"),Vbr=o("FlaxMT5ForConditionalGeneration"),Wbr=o(" (mT5 model)"),Qbr=l(),QF=a("li"),f2e=a("strong"),Hbr=o("roberta"),Ubr=o(" \u2014 "),XO=a("a"),Jbr=o("FlaxRobertaForMaskedLM"),Ybr=o(" (RoBERTa model)"),Kbr=l(),HF=a("li"),m2e=a("strong"),Zbr=o("roformer"),e5r=o(" \u2014 "),zO=a("a"),o5r=o("FlaxRoFormerForMaskedLM"),r5r=o(" (RoFormer model)"),t5r=l(),UF=a("li"),g2e=a("strong"),a5r=o("t5"),n5r=o(" \u2014 "),VO=a("a"),s5r=o("FlaxT5ForConditionalGeneration"),l5r=o(" (T5 model)"),i5r=l(),JF=a("li"),h2e=a("strong"),d5r=o("wav2vec2"),c5r=o(" \u2014 "),WO=a("a"),f5r=o("FlaxWav2Vec2ForPreTraining"),m5r=o(" (Wav2Vec2 model)"),g5r=l(),p2e=a("p"),h5r=o("Examples:"),p5r=l(),f(PA.$$.fragment),W9e=l(),Uc=a("h2"),YF=a("a"),_2e=a("span"),f($A.$$.fragment),_5r=l(),u2e=a("span"),u5r=o("FlaxAutoModelForMaskedLM"),Q9e=l(),Br=a("div"),f(IA.$$.fragment),b5r=l(),Jc=a("p"),v5r=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a masked language modeling head) when created
with the `),b2e=a("code"),T5r=o("from_pretrained()"),F5r=o("class method or the "),v2e=a("code"),C5r=o("from_config()"),M5r=o(`class
method.`),E5r=l(),jA=a("p"),y5r=o("This class cannot be instantiated directly using "),T2e=a("code"),w5r=o("__init__()"),A5r=o(" (throws an error)."),L5r=l(),Ct=a("div"),f(NA.$$.fragment),B5r=l(),F2e=a("p"),k5r=o("Instantiates one of the model classes of the library (with a masked language modeling head) from a configuration."),x5r=l(),Yc=a("p"),R5r=o(`Note:
Loading a model from its configuration file does `),C2e=a("strong"),S5r=o("not"),P5r=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),M2e=a("code"),$5r=o("from_pretrained()"),I5r=o("to load the model weights."),j5r=l(),E2e=a("p"),N5r=o("Examples:"),D5r=l(),f(DA.$$.fragment),q5r=l(),Bo=a("div"),f(qA.$$.fragment),G5r=l(),y2e=a("p"),O5r=o("Instantiate one of the model classes of the library (with a masked language modeling head) from a pretrained model."),X5r=l(),yn=a("p"),z5r=o("The model class to instantiate is selected based on the "),w2e=a("code"),V5r=o("model_type"),W5r=o(` property of the config object (either
passed as an argument or loaded from `),A2e=a("code"),Q5r=o("pretrained_model_name_or_path"),H5r=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),L2e=a("code"),U5r=o("pretrained_model_name_or_path"),J5r=o(":"),Y5r=l(),ve=a("ul"),KF=a("li"),B2e=a("strong"),K5r=o("albert"),Z5r=o(" \u2014 "),QO=a("a"),evr=o("FlaxAlbertForMaskedLM"),ovr=o(" (ALBERT model)"),rvr=l(),ZF=a("li"),k2e=a("strong"),tvr=o("bart"),avr=o(" \u2014 "),HO=a("a"),nvr=o("FlaxBartForConditionalGeneration"),svr=o(" (BART model)"),lvr=l(),eC=a("li"),x2e=a("strong"),ivr=o("bert"),dvr=o(" \u2014 "),UO=a("a"),cvr=o("FlaxBertForMaskedLM"),fvr=o(" (BERT model)"),mvr=l(),oC=a("li"),R2e=a("strong"),gvr=o("big_bird"),hvr=o(" \u2014 "),JO=a("a"),pvr=o("FlaxBigBirdForMaskedLM"),_vr=o(" (BigBird model)"),uvr=l(),rC=a("li"),S2e=a("strong"),bvr=o("distilbert"),vvr=o(" \u2014 "),YO=a("a"),Tvr=o("FlaxDistilBertForMaskedLM"),Fvr=o(" (DistilBERT model)"),Cvr=l(),tC=a("li"),P2e=a("strong"),Mvr=o("electra"),Evr=o(" \u2014 "),KO=a("a"),yvr=o("FlaxElectraForMaskedLM"),wvr=o(" (ELECTRA model)"),Avr=l(),aC=a("li"),$2e=a("strong"),Lvr=o("mbart"),Bvr=o(" \u2014 "),ZO=a("a"),kvr=o("FlaxMBartForConditionalGeneration"),xvr=o(" (mBART model)"),Rvr=l(),nC=a("li"),I2e=a("strong"),Svr=o("roberta"),Pvr=o(" \u2014 "),eX=a("a"),$vr=o("FlaxRobertaForMaskedLM"),Ivr=o(" (RoBERTa model)"),jvr=l(),sC=a("li"),j2e=a("strong"),Nvr=o("roformer"),Dvr=o(" \u2014 "),oX=a("a"),qvr=o("FlaxRoFormerForMaskedLM"),Gvr=o(" (RoFormer model)"),Ovr=l(),N2e=a("p"),Xvr=o("Examples:"),zvr=l(),f(GA.$$.fragment),H9e=l(),Kc=a("h2"),lC=a("a"),D2e=a("span"),f(OA.$$.fragment),Vvr=l(),q2e=a("span"),Wvr=o("FlaxAutoModelForSeq2SeqLM"),U9e=l(),kr=a("div"),f(XA.$$.fragment),Qvr=l(),Zc=a("p"),Hvr=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence language modeling head) when created
with the `),G2e=a("code"),Uvr=o("from_pretrained()"),Jvr=o("class method or the "),O2e=a("code"),Yvr=o("from_config()"),Kvr=o(`class
method.`),Zvr=l(),zA=a("p"),e6r=o("This class cannot be instantiated directly using "),X2e=a("code"),o6r=o("__init__()"),r6r=o(" (throws an error)."),t6r=l(),Mt=a("div"),f(VA.$$.fragment),a6r=l(),z2e=a("p"),n6r=o("Instantiates one of the model classes of the library (with a sequence-to-sequence language modeling head) from a configuration."),s6r=l(),ef=a("p"),l6r=o(`Note:
Loading a model from its configuration file does `),V2e=a("strong"),i6r=o("not"),d6r=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),W2e=a("code"),c6r=o("from_pretrained()"),f6r=o("to load the model weights."),m6r=l(),Q2e=a("p"),g6r=o("Examples:"),h6r=l(),f(WA.$$.fragment),p6r=l(),ko=a("div"),f(QA.$$.fragment),_6r=l(),H2e=a("p"),u6r=o("Instantiate one of the model classes of the library (with a sequence-to-sequence language modeling head) from a pretrained model."),b6r=l(),wn=a("p"),v6r=o("The model class to instantiate is selected based on the "),U2e=a("code"),T6r=o("model_type"),F6r=o(` property of the config object (either
passed as an argument or loaded from `),J2e=a("code"),C6r=o("pretrained_model_name_or_path"),M6r=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Y2e=a("code"),E6r=o("pretrained_model_name_or_path"),y6r=o(":"),w6r=l(),Te=a("ul"),iC=a("li"),K2e=a("strong"),A6r=o("bart"),L6r=o(" \u2014 "),rX=a("a"),B6r=o("FlaxBartForConditionalGeneration"),k6r=o(" (BART model)"),x6r=l(),dC=a("li"),Z2e=a("strong"),R6r=o("blenderbot"),S6r=o(" \u2014 "),tX=a("a"),P6r=o("FlaxBlenderbotForConditionalGeneration"),$6r=o(" (Blenderbot model)"),I6r=l(),cC=a("li"),e1e=a("strong"),j6r=o("blenderbot-small"),N6r=o(" \u2014 "),aX=a("a"),D6r=o("FlaxBlenderbotSmallForConditionalGeneration"),q6r=o(" (BlenderbotSmall model)"),G6r=l(),fC=a("li"),o1e=a("strong"),O6r=o("encoder-decoder"),X6r=o(" \u2014 "),nX=a("a"),z6r=o("FlaxEncoderDecoderModel"),V6r=o(" (Encoder decoder model)"),W6r=l(),mC=a("li"),r1e=a("strong"),Q6r=o("marian"),H6r=o(" \u2014 "),sX=a("a"),U6r=o("FlaxMarianMTModel"),J6r=o(" (Marian model)"),Y6r=l(),gC=a("li"),t1e=a("strong"),K6r=o("mbart"),Z6r=o(" \u2014 "),lX=a("a"),eTr=o("FlaxMBartForConditionalGeneration"),oTr=o(" (mBART model)"),rTr=l(),hC=a("li"),a1e=a("strong"),tTr=o("mt5"),aTr=o(" \u2014 "),iX=a("a"),nTr=o("FlaxMT5ForConditionalGeneration"),sTr=o(" (mT5 model)"),lTr=l(),pC=a("li"),n1e=a("strong"),iTr=o("pegasus"),dTr=o(" \u2014 "),dX=a("a"),cTr=o("FlaxPegasusForConditionalGeneration"),fTr=o(" (Pegasus model)"),mTr=l(),_C=a("li"),s1e=a("strong"),gTr=o("t5"),hTr=o(" \u2014 "),cX=a("a"),pTr=o("FlaxT5ForConditionalGeneration"),_Tr=o(" (T5 model)"),uTr=l(),l1e=a("p"),bTr=o("Examples:"),vTr=l(),f(HA.$$.fragment),J9e=l(),of=a("h2"),uC=a("a"),i1e=a("span"),f(UA.$$.fragment),TTr=l(),d1e=a("span"),FTr=o("FlaxAutoModelForSequenceClassification"),Y9e=l(),xr=a("div"),f(JA.$$.fragment),CTr=l(),rf=a("p"),MTr=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence classification head) when created
with the `),c1e=a("code"),ETr=o("from_pretrained()"),yTr=o("class method or the "),f1e=a("code"),wTr=o("from_config()"),ATr=o(`class
method.`),LTr=l(),YA=a("p"),BTr=o("This class cannot be instantiated directly using "),m1e=a("code"),kTr=o("__init__()"),xTr=o(" (throws an error)."),RTr=l(),Et=a("div"),f(KA.$$.fragment),STr=l(),g1e=a("p"),PTr=o("Instantiates one of the model classes of the library (with a sequence classification head) from a configuration."),$Tr=l(),tf=a("p"),ITr=o(`Note:
Loading a model from its configuration file does `),h1e=a("strong"),jTr=o("not"),NTr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),p1e=a("code"),DTr=o("from_pretrained()"),qTr=o("to load the model weights."),GTr=l(),_1e=a("p"),OTr=o("Examples:"),XTr=l(),f(ZA.$$.fragment),zTr=l(),xo=a("div"),f(e0.$$.fragment),VTr=l(),u1e=a("p"),WTr=o("Instantiate one of the model classes of the library (with a sequence classification head) from a pretrained model."),QTr=l(),An=a("p"),HTr=o("The model class to instantiate is selected based on the "),b1e=a("code"),UTr=o("model_type"),JTr=o(` property of the config object (either
passed as an argument or loaded from `),v1e=a("code"),YTr=o("pretrained_model_name_or_path"),KTr=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),T1e=a("code"),ZTr=o("pretrained_model_name_or_path"),e8r=o(":"),o8r=l(),Fe=a("ul"),bC=a("li"),F1e=a("strong"),r8r=o("albert"),t8r=o(" \u2014 "),fX=a("a"),a8r=o("FlaxAlbertForSequenceClassification"),n8r=o(" (ALBERT model)"),s8r=l(),vC=a("li"),C1e=a("strong"),l8r=o("bart"),i8r=o(" \u2014 "),mX=a("a"),d8r=o("FlaxBartForSequenceClassification"),c8r=o(" (BART model)"),f8r=l(),TC=a("li"),M1e=a("strong"),m8r=o("bert"),g8r=o(" \u2014 "),gX=a("a"),h8r=o("FlaxBertForSequenceClassification"),p8r=o(" (BERT model)"),_8r=l(),FC=a("li"),E1e=a("strong"),u8r=o("big_bird"),b8r=o(" \u2014 "),hX=a("a"),v8r=o("FlaxBigBirdForSequenceClassification"),T8r=o(" (BigBird model)"),F8r=l(),CC=a("li"),y1e=a("strong"),C8r=o("distilbert"),M8r=o(" \u2014 "),pX=a("a"),E8r=o("FlaxDistilBertForSequenceClassification"),y8r=o(" (DistilBERT model)"),w8r=l(),MC=a("li"),w1e=a("strong"),A8r=o("electra"),L8r=o(" \u2014 "),_X=a("a"),B8r=o("FlaxElectraForSequenceClassification"),k8r=o(" (ELECTRA model)"),x8r=l(),EC=a("li"),A1e=a("strong"),R8r=o("mbart"),S8r=o(" \u2014 "),uX=a("a"),P8r=o("FlaxMBartForSequenceClassification"),$8r=o(" (mBART model)"),I8r=l(),yC=a("li"),L1e=a("strong"),j8r=o("roberta"),N8r=o(" \u2014 "),bX=a("a"),D8r=o("FlaxRobertaForSequenceClassification"),q8r=o(" (RoBERTa model)"),G8r=l(),wC=a("li"),B1e=a("strong"),O8r=o("roformer"),X8r=o(" \u2014 "),vX=a("a"),z8r=o("FlaxRoFormerForSequenceClassification"),V8r=o(" (RoFormer model)"),W8r=l(),k1e=a("p"),Q8r=o("Examples:"),H8r=l(),f(o0.$$.fragment),K9e=l(),af=a("h2"),AC=a("a"),x1e=a("span"),f(r0.$$.fragment),U8r=l(),R1e=a("span"),J8r=o("FlaxAutoModelForQuestionAnswering"),Z9e=l(),Rr=a("div"),f(t0.$$.fragment),Y8r=l(),nf=a("p"),K8r=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a question answering head) when created
with the `),S1e=a("code"),Z8r=o("from_pretrained()"),eFr=o("class method or the "),P1e=a("code"),oFr=o("from_config()"),rFr=o(`class
method.`),tFr=l(),a0=a("p"),aFr=o("This class cannot be instantiated directly using "),$1e=a("code"),nFr=o("__init__()"),sFr=o(" (throws an error)."),lFr=l(),yt=a("div"),f(n0.$$.fragment),iFr=l(),I1e=a("p"),dFr=o("Instantiates one of the model classes of the library (with a question answering head) from a configuration."),cFr=l(),sf=a("p"),fFr=o(`Note:
Loading a model from its configuration file does `),j1e=a("strong"),mFr=o("not"),gFr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),N1e=a("code"),hFr=o("from_pretrained()"),pFr=o("to load the model weights."),_Fr=l(),D1e=a("p"),uFr=o("Examples:"),bFr=l(),f(s0.$$.fragment),vFr=l(),Ro=a("div"),f(l0.$$.fragment),TFr=l(),q1e=a("p"),FFr=o("Instantiate one of the model classes of the library (with a question answering head) from a pretrained model."),CFr=l(),Ln=a("p"),MFr=o("The model class to instantiate is selected based on the "),G1e=a("code"),EFr=o("model_type"),yFr=o(` property of the config object (either
passed as an argument or loaded from `),O1e=a("code"),wFr=o("pretrained_model_name_or_path"),AFr=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),X1e=a("code"),LFr=o("pretrained_model_name_or_path"),BFr=o(":"),kFr=l(),Ce=a("ul"),LC=a("li"),z1e=a("strong"),xFr=o("albert"),RFr=o(" \u2014 "),TX=a("a"),SFr=o("FlaxAlbertForQuestionAnswering"),PFr=o(" (ALBERT model)"),$Fr=l(),BC=a("li"),V1e=a("strong"),IFr=o("bart"),jFr=o(" \u2014 "),FX=a("a"),NFr=o("FlaxBartForQuestionAnswering"),DFr=o(" (BART model)"),qFr=l(),kC=a("li"),W1e=a("strong"),GFr=o("bert"),OFr=o(" \u2014 "),CX=a("a"),XFr=o("FlaxBertForQuestionAnswering"),zFr=o(" (BERT model)"),VFr=l(),xC=a("li"),Q1e=a("strong"),WFr=o("big_bird"),QFr=o(" \u2014 "),MX=a("a"),HFr=o("FlaxBigBirdForQuestionAnswering"),UFr=o(" (BigBird model)"),JFr=l(),RC=a("li"),H1e=a("strong"),YFr=o("distilbert"),KFr=o(" \u2014 "),EX=a("a"),ZFr=o("FlaxDistilBertForQuestionAnswering"),eCr=o(" (DistilBERT model)"),oCr=l(),SC=a("li"),U1e=a("strong"),rCr=o("electra"),tCr=o(" \u2014 "),yX=a("a"),aCr=o("FlaxElectraForQuestionAnswering"),nCr=o(" (ELECTRA model)"),sCr=l(),PC=a("li"),J1e=a("strong"),lCr=o("mbart"),iCr=o(" \u2014 "),wX=a("a"),dCr=o("FlaxMBartForQuestionAnswering"),cCr=o(" (mBART model)"),fCr=l(),$C=a("li"),Y1e=a("strong"),mCr=o("roberta"),gCr=o(" \u2014 "),AX=a("a"),hCr=o("FlaxRobertaForQuestionAnswering"),pCr=o(" (RoBERTa model)"),_Cr=l(),IC=a("li"),K1e=a("strong"),uCr=o("roformer"),bCr=o(" \u2014 "),LX=a("a"),vCr=o("FlaxRoFormerForQuestionAnswering"),TCr=o(" (RoFormer model)"),FCr=l(),Z1e=a("p"),CCr=o("Examples:"),MCr=l(),f(i0.$$.fragment),eBe=l(),lf=a("h2"),jC=a("a"),ebe=a("span"),f(d0.$$.fragment),ECr=l(),obe=a("span"),yCr=o("FlaxAutoModelForTokenClassification"),oBe=l(),Sr=a("div"),f(c0.$$.fragment),wCr=l(),df=a("p"),ACr=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a token classification head) when created
with the `),rbe=a("code"),LCr=o("from_pretrained()"),BCr=o("class method or the "),tbe=a("code"),kCr=o("from_config()"),xCr=o(`class
method.`),RCr=l(),f0=a("p"),SCr=o("This class cannot be instantiated directly using "),abe=a("code"),PCr=o("__init__()"),$Cr=o(" (throws an error)."),ICr=l(),wt=a("div"),f(m0.$$.fragment),jCr=l(),nbe=a("p"),NCr=o("Instantiates one of the model classes of the library (with a token classification head) from a configuration."),DCr=l(),cf=a("p"),qCr=o(`Note:
Loading a model from its configuration file does `),sbe=a("strong"),GCr=o("not"),OCr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),lbe=a("code"),XCr=o("from_pretrained()"),zCr=o("to load the model weights."),VCr=l(),ibe=a("p"),WCr=o("Examples:"),QCr=l(),f(g0.$$.fragment),HCr=l(),So=a("div"),f(h0.$$.fragment),UCr=l(),dbe=a("p"),JCr=o("Instantiate one of the model classes of the library (with a token classification head) from a pretrained model."),YCr=l(),Bn=a("p"),KCr=o("The model class to instantiate is selected based on the "),cbe=a("code"),ZCr=o("model_type"),e4r=o(` property of the config object (either
passed as an argument or loaded from `),fbe=a("code"),o4r=o("pretrained_model_name_or_path"),r4r=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),mbe=a("code"),t4r=o("pretrained_model_name_or_path"),a4r=o(":"),n4r=l(),so=a("ul"),NC=a("li"),gbe=a("strong"),s4r=o("albert"),l4r=o(" \u2014 "),BX=a("a"),i4r=o("FlaxAlbertForTokenClassification"),d4r=o(" (ALBERT model)"),c4r=l(),DC=a("li"),hbe=a("strong"),f4r=o("bert"),m4r=o(" \u2014 "),kX=a("a"),g4r=o("FlaxBertForTokenClassification"),h4r=o(" (BERT model)"),p4r=l(),qC=a("li"),pbe=a("strong"),_4r=o("big_bird"),u4r=o(" \u2014 "),xX=a("a"),b4r=o("FlaxBigBirdForTokenClassification"),v4r=o(" (BigBird model)"),T4r=l(),GC=a("li"),_be=a("strong"),F4r=o("distilbert"),C4r=o(" \u2014 "),RX=a("a"),M4r=o("FlaxDistilBertForTokenClassification"),E4r=o(" (DistilBERT model)"),y4r=l(),OC=a("li"),ube=a("strong"),w4r=o("electra"),A4r=o(" \u2014 "),SX=a("a"),L4r=o("FlaxElectraForTokenClassification"),B4r=o(" (ELECTRA model)"),k4r=l(),XC=a("li"),bbe=a("strong"),x4r=o("roberta"),R4r=o(" \u2014 "),PX=a("a"),S4r=o("FlaxRobertaForTokenClassification"),P4r=o(" (RoBERTa model)"),$4r=l(),zC=a("li"),vbe=a("strong"),I4r=o("roformer"),j4r=o(" \u2014 "),$X=a("a"),N4r=o("FlaxRoFormerForTokenClassification"),D4r=o(" (RoFormer model)"),q4r=l(),Tbe=a("p"),G4r=o("Examples:"),O4r=l(),f(p0.$$.fragment),rBe=l(),ff=a("h2"),VC=a("a"),Fbe=a("span"),f(_0.$$.fragment),X4r=l(),Cbe=a("span"),z4r=o("FlaxAutoModelForMultipleChoice"),tBe=l(),Pr=a("div"),f(u0.$$.fragment),V4r=l(),mf=a("p"),W4r=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a multiple choice head) when created
with the `),Mbe=a("code"),Q4r=o("from_pretrained()"),H4r=o("class method or the "),Ebe=a("code"),U4r=o("from_config()"),J4r=o(`class
method.`),Y4r=l(),b0=a("p"),K4r=o("This class cannot be instantiated directly using "),ybe=a("code"),Z4r=o("__init__()"),eMr=o(" (throws an error)."),oMr=l(),At=a("div"),f(v0.$$.fragment),rMr=l(),wbe=a("p"),tMr=o("Instantiates one of the model classes of the library (with a multiple choice head) from a configuration."),aMr=l(),gf=a("p"),nMr=o(`Note:
Loading a model from its configuration file does `),Abe=a("strong"),sMr=o("not"),lMr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Lbe=a("code"),iMr=o("from_pretrained()"),dMr=o("to load the model weights."),cMr=l(),Bbe=a("p"),fMr=o("Examples:"),mMr=l(),f(T0.$$.fragment),gMr=l(),Po=a("div"),f(F0.$$.fragment),hMr=l(),kbe=a("p"),pMr=o("Instantiate one of the model classes of the library (with a multiple choice head) from a pretrained model."),_Mr=l(),kn=a("p"),uMr=o("The model class to instantiate is selected based on the "),xbe=a("code"),bMr=o("model_type"),vMr=o(` property of the config object (either
passed as an argument or loaded from `),Rbe=a("code"),TMr=o("pretrained_model_name_or_path"),FMr=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Sbe=a("code"),CMr=o("pretrained_model_name_or_path"),MMr=o(":"),EMr=l(),lo=a("ul"),WC=a("li"),Pbe=a("strong"),yMr=o("albert"),wMr=o(" \u2014 "),IX=a("a"),AMr=o("FlaxAlbertForMultipleChoice"),LMr=o(" (ALBERT model)"),BMr=l(),QC=a("li"),$be=a("strong"),kMr=o("bert"),xMr=o(" \u2014 "),jX=a("a"),RMr=o("FlaxBertForMultipleChoice"),SMr=o(" (BERT model)"),PMr=l(),HC=a("li"),Ibe=a("strong"),$Mr=o("big_bird"),IMr=o(" \u2014 "),NX=a("a"),jMr=o("FlaxBigBirdForMultipleChoice"),NMr=o(" (BigBird model)"),DMr=l(),UC=a("li"),jbe=a("strong"),qMr=o("distilbert"),GMr=o(" \u2014 "),DX=a("a"),OMr=o("FlaxDistilBertForMultipleChoice"),XMr=o(" (DistilBERT model)"),zMr=l(),JC=a("li"),Nbe=a("strong"),VMr=o("electra"),WMr=o(" \u2014 "),qX=a("a"),QMr=o("FlaxElectraForMultipleChoice"),HMr=o(" (ELECTRA model)"),UMr=l(),YC=a("li"),Dbe=a("strong"),JMr=o("roberta"),YMr=o(" \u2014 "),GX=a("a"),KMr=o("FlaxRobertaForMultipleChoice"),ZMr=o(" (RoBERTa model)"),eEr=l(),KC=a("li"),qbe=a("strong"),oEr=o("roformer"),rEr=o(" \u2014 "),OX=a("a"),tEr=o("FlaxRoFormerForMultipleChoice"),aEr=o(" (RoFormer model)"),nEr=l(),Gbe=a("p"),sEr=o("Examples:"),lEr=l(),f(C0.$$.fragment),aBe=l(),hf=a("h2"),ZC=a("a"),Obe=a("span"),f(M0.$$.fragment),iEr=l(),Xbe=a("span"),dEr=o("FlaxAutoModelForNextSentencePrediction"),nBe=l(),$r=a("div"),f(E0.$$.fragment),cEr=l(),pf=a("p"),fEr=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a next sentence prediction head) when created
with the `),zbe=a("code"),mEr=o("from_pretrained()"),gEr=o("class method or the "),Vbe=a("code"),hEr=o("from_config()"),pEr=o(`class
method.`),_Er=l(),y0=a("p"),uEr=o("This class cannot be instantiated directly using "),Wbe=a("code"),bEr=o("__init__()"),vEr=o(" (throws an error)."),TEr=l(),Lt=a("div"),f(w0.$$.fragment),FEr=l(),Qbe=a("p"),CEr=o("Instantiates one of the model classes of the library (with a next sentence prediction head) from a configuration."),MEr=l(),_f=a("p"),EEr=o(`Note:
Loading a model from its configuration file does `),Hbe=a("strong"),yEr=o("not"),wEr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Ube=a("code"),AEr=o("from_pretrained()"),LEr=o("to load the model weights."),BEr=l(),Jbe=a("p"),kEr=o("Examples:"),xEr=l(),f(A0.$$.fragment),REr=l(),$o=a("div"),f(L0.$$.fragment),SEr=l(),Ybe=a("p"),PEr=o("Instantiate one of the model classes of the library (with a next sentence prediction head) from a pretrained model."),$Er=l(),xn=a("p"),IEr=o("The model class to instantiate is selected based on the "),Kbe=a("code"),jEr=o("model_type"),NEr=o(` property of the config object (either
passed as an argument or loaded from `),Zbe=a("code"),DEr=o("pretrained_model_name_or_path"),qEr=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),e5e=a("code"),GEr=o("pretrained_model_name_or_path"),OEr=o(":"),XEr=l(),o5e=a("ul"),e4=a("li"),r5e=a("strong"),zEr=o("bert"),VEr=o(" \u2014 "),XX=a("a"),WEr=o("FlaxBertForNextSentencePrediction"),QEr=o(" (BERT model)"),HEr=l(),t5e=a("p"),UEr=o("Examples:"),JEr=l(),f(B0.$$.fragment),sBe=l(),uf=a("h2"),o4=a("a"),a5e=a("span"),f(k0.$$.fragment),YEr=l(),n5e=a("span"),KEr=o("FlaxAutoModelForImageClassification"),lBe=l(),Ir=a("div"),f(x0.$$.fragment),ZEr=l(),bf=a("p"),e3r=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a image classification head) when created
with the `),s5e=a("code"),o3r=o("from_pretrained()"),r3r=o("class method or the "),l5e=a("code"),t3r=o("from_config()"),a3r=o(`class
method.`),n3r=l(),R0=a("p"),s3r=o("This class cannot be instantiated directly using "),i5e=a("code"),l3r=o("__init__()"),i3r=o(" (throws an error)."),d3r=l(),Bt=a("div"),f(S0.$$.fragment),c3r=l(),d5e=a("p"),f3r=o("Instantiates one of the model classes of the library (with a image classification head) from a configuration."),m3r=l(),vf=a("p"),g3r=o(`Note:
Loading a model from its configuration file does `),c5e=a("strong"),h3r=o("not"),p3r=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),f5e=a("code"),_3r=o("from_pretrained()"),u3r=o("to load the model weights."),b3r=l(),m5e=a("p"),v3r=o("Examples:"),T3r=l(),f(P0.$$.fragment),F3r=l(),Io=a("div"),f($0.$$.fragment),C3r=l(),g5e=a("p"),M3r=o("Instantiate one of the model classes of the library (with a image classification head) from a pretrained model."),E3r=l(),Rn=a("p"),y3r=o("The model class to instantiate is selected based on the "),h5e=a("code"),w3r=o("model_type"),A3r=o(` property of the config object (either
passed as an argument or loaded from `),p5e=a("code"),L3r=o("pretrained_model_name_or_path"),B3r=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),_5e=a("code"),k3r=o("pretrained_model_name_or_path"),x3r=o(":"),R3r=l(),I0=a("ul"),r4=a("li"),u5e=a("strong"),S3r=o("beit"),P3r=o(" \u2014 "),zX=a("a"),$3r=o("FlaxBeitForImageClassification"),I3r=o(" (BEiT model)"),j3r=l(),t4=a("li"),b5e=a("strong"),N3r=o("vit"),D3r=o(" \u2014 "),VX=a("a"),q3r=o("FlaxViTForImageClassification"),G3r=o(" (ViT model)"),O3r=l(),v5e=a("p"),X3r=o("Examples:"),z3r=l(),f(j0.$$.fragment),iBe=l(),Tf=a("h2"),a4=a("a"),T5e=a("span"),f(N0.$$.fragment),V3r=l(),F5e=a("span"),W3r=o("FlaxAutoModelForVision2Seq"),dBe=l(),jr=a("div"),f(D0.$$.fragment),Q3r=l(),Ff=a("p"),H3r=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a vision-to-text modeling head) when created
with the `),C5e=a("code"),U3r=o("from_pretrained()"),J3r=o("class method or the "),M5e=a("code"),Y3r=o("from_config()"),K3r=o(`class
method.`),Z3r=l(),q0=a("p"),eyr=o("This class cannot be instantiated directly using "),E5e=a("code"),oyr=o("__init__()"),ryr=o(" (throws an error)."),tyr=l(),kt=a("div"),f(G0.$$.fragment),ayr=l(),y5e=a("p"),nyr=o("Instantiates one of the model classes of the library (with a vision-to-text modeling head) from a configuration."),syr=l(),Cf=a("p"),lyr=o(`Note:
Loading a model from its configuration file does `),w5e=a("strong"),iyr=o("not"),dyr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),A5e=a("code"),cyr=o("from_pretrained()"),fyr=o("to load the model weights."),myr=l(),L5e=a("p"),gyr=o("Examples:"),hyr=l(),f(O0.$$.fragment),pyr=l(),jo=a("div"),f(X0.$$.fragment),_yr=l(),B5e=a("p"),uyr=o("Instantiate one of the model classes of the library (with a vision-to-text modeling head) from a pretrained model."),byr=l(),Sn=a("p"),vyr=o("The model class to instantiate is selected based on the "),k5e=a("code"),Tyr=o("model_type"),Fyr=o(` property of the config object (either
passed as an argument or loaded from `),x5e=a("code"),Cyr=o("pretrained_model_name_or_path"),Myr=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),R5e=a("code"),Eyr=o("pretrained_model_name_or_path"),yyr=o(":"),wyr=l(),S5e=a("ul"),n4=a("li"),P5e=a("strong"),Ayr=o("vision-encoder-decoder"),Lyr=o(" \u2014 "),WX=a("a"),Byr=o("FlaxVisionEncoderDecoderModel"),kyr=o(" (Vision Encoder decoder model)"),xyr=l(),$5e=a("p"),Ryr=o("Examples:"),Syr=l(),f(z0.$$.fragment),this.h()},l(d){const u=O_t('[data-svelte="svelte-1phssyn"]',document.head);J=n(u,"META",{name:!0,content:!0}),u.forEach(t),Ae=i(d),ie=n(d,"H1",{class:!0});var V0=s(ie);me=n(V0,"A",{id:!0,class:!0,href:!0});var I5e=s(me);to=n(I5e,"SPAN",{});var j5e=s(to);m(ce.$$.fragment,j5e),j5e.forEach(t),I5e.forEach(t),ue=i(V0),Do=n(V0,"SPAN",{});var $yr=s(Do);wi=r($yr,"Auto Classes"),$yr.forEach(t),V0.forEach(t),Ef=i(d),sa=n(d,"P",{});var fBe=s(sa);Ai=r(fBe,`In many cases, the architecture you want to use can be guessed from the name or the path of the pretrained model you
are supplying to the `),Li=n(fBe,"CODE",{});var Iyr=s(Li);tM=r(Iyr,"from_pretrained()"),Iyr.forEach(t),yf=r(fBe,` method. AutoClasses are here to do this job for you so that you
automatically retrieve the relevant model given the name/path to the pretrained weights/config/vocabulary.`),fBe.forEach(t),ye=i(d),io=n(d,"P",{});var s4=s(io);Bi=r(s4,"Instantiating one of "),Pn=n(s4,"A",{href:!0});var jyr=s(Pn);aM=r(jyr,"AutoConfig"),jyr.forEach(t),$n=r(s4,", "),In=n(s4,"A",{href:!0});var Nyr=s(In);nM=r(Nyr,"AutoModel"),Nyr.forEach(t),ki=r(s4,`, and
`),jn=n(s4,"A",{href:!0});var Dyr=s(jn);sM=r(Dyr,"AutoTokenizer"),Dyr.forEach(t),xi=r(s4," will directly create a class of the relevant architecture. For instance"),s4.forEach(t),wf=i(d),m($a.$$.fragment,d),co=i(d),ge=n(d,"P",{});var mBe=s(ge);GL=r(mBe,"will create a model that is an instance of "),Ri=n(mBe,"A",{href:!0});var qyr=s(Ri);OL=r(qyr,"BertModel"),qyr.forEach(t),XL=r(mBe,"."),mBe.forEach(t),qo=i(d),Ia=n(d,"P",{});var gBe=s(Ia);zL=r(gBe,"There is one class of "),Af=n(gBe,"CODE",{});var Gyr=s(Af);VL=r(Gyr,"AutoModel"),Gyr.forEach(t),Txe=r(gBe," for each task, and for each backend (PyTorch, TensorFlow, or Flax)."),gBe.forEach(t),f7e=i(d),Si=n(d,"H2",{class:!0});var hBe=s(Si);Lf=n(hBe,"A",{id:!0,class:!0,href:!0});var Oyr=s(Lf);DV=n(Oyr,"SPAN",{});var Xyr=s(DV);m(lM.$$.fragment,Xyr),Xyr.forEach(t),Oyr.forEach(t),Fxe=i(hBe),qV=n(hBe,"SPAN",{});var zyr=s(qV);Cxe=r(zyr,"Extending the Auto Classes"),zyr.forEach(t),hBe.forEach(t),m7e=i(d),Nn=n(d,"P",{});var QX=s(Nn);Mxe=r(QX,`Each of the auto classes has a method to be extended with your custom classes. For instance, if you have defined a
custom class of model `),GV=n(QX,"CODE",{});var Vyr=s(GV);Exe=r(Vyr,"NewModel"),Vyr.forEach(t),yxe=r(QX,", make sure you have a "),OV=n(QX,"CODE",{});var Wyr=s(OV);wxe=r(Wyr,"NewModelConfig"),Wyr.forEach(t),Axe=r(QX,` then you can add those to the auto
classes like this:`),QX.forEach(t),g7e=i(d),m(iM.$$.fragment,d),h7e=i(d),WL=n(d,"P",{});var Qyr=s(WL);Lxe=r(Qyr,"You will then be able to use the auto classes like you would usually do!"),Qyr.forEach(t),p7e=i(d),m(Bf.$$.fragment,d),_7e=i(d),Pi=n(d,"H2",{class:!0});var pBe=s(Pi);kf=n(pBe,"A",{id:!0,class:!0,href:!0});var Hyr=s(kf);XV=n(Hyr,"SPAN",{});var Uyr=s(XV);m(dM.$$.fragment,Uyr),Uyr.forEach(t),Hyr.forEach(t),Bxe=i(pBe),zV=n(pBe,"SPAN",{});var Jyr=s(zV);kxe=r(Jyr,"AutoConfig"),Jyr.forEach(t),pBe.forEach(t),u7e=i(d),Go=n(d,"DIV",{class:!0});var Ps=s(Go);m(cM.$$.fragment,Ps),xxe=i(Ps),fM=n(Ps,"P",{});var _Be=s(fM);Rxe=r(_Be,`This is a generic configuration class that will be instantiated as one of the configuration classes of the library
when created with the `),QL=n(_Be,"A",{href:!0});var Yyr=s(QL);Sxe=r(Yyr,"from_pretrained()"),Yyr.forEach(t),Pxe=r(_Be," class method."),_Be.forEach(t),$xe=i(Ps),mM=n(Ps,"P",{});var uBe=s(mM);Ixe=r(uBe,"This class cannot be instantiated directly using "),VV=n(uBe,"CODE",{});var Kyr=s(VV);jxe=r(Kyr,"__init__()"),Kyr.forEach(t),Nxe=r(uBe," (throws an error)."),uBe.forEach(t),Dxe=i(Ps),fo=n(Ps,"DIV",{class:!0});var ia=s(fo);m(gM.$$.fragment,ia),qxe=i(ia),WV=n(ia,"P",{});var Zyr=s(WV);Gxe=r(Zyr,"Instantiate one of the configuration classes of the library from a pretrained model configuration."),Zyr.forEach(t),Oxe=i(ia),$i=n(ia,"P",{});var HX=s($i);Xxe=r(HX,"The configuration class to instantiate is selected based on the "),QV=n(HX,"CODE",{});var ewr=s(QV);zxe=r(ewr,"model_type"),ewr.forEach(t),Vxe=r(HX,` property of the config object that
is loaded, or when it\u2019s missing, by falling back to using pattern matching on `),HV=n(HX,"CODE",{});var owr=s(HV);Wxe=r(owr,"pretrained_model_name_or_path"),owr.forEach(t),Qxe=r(HX,":"),HX.forEach(t),Hxe=i(ia),v=n(ia,"UL",{});var T=s(v);xf=n(T,"LI",{});var N5e=s(xf);UV=n(N5e,"STRONG",{});var rwr=s(UV);Uxe=r(rwr,"albert"),rwr.forEach(t),Jxe=r(N5e," \u2014 "),HL=n(N5e,"A",{href:!0});var twr=s(HL);Yxe=r(twr,"AlbertConfig"),twr.forEach(t),Kxe=r(N5e," (ALBERT model)"),N5e.forEach(t),Zxe=i(T),Rf=n(T,"LI",{});var D5e=s(Rf);JV=n(D5e,"STRONG",{});var awr=s(JV);eRe=r(awr,"bart"),awr.forEach(t),oRe=r(D5e," \u2014 "),UL=n(D5e,"A",{href:!0});var nwr=s(UL);rRe=r(nwr,"BartConfig"),nwr.forEach(t),tRe=r(D5e," (BART model)"),D5e.forEach(t),aRe=i(T),Sf=n(T,"LI",{});var q5e=s(Sf);YV=n(q5e,"STRONG",{});var swr=s(YV);nRe=r(swr,"beit"),swr.forEach(t),sRe=r(q5e," \u2014 "),JL=n(q5e,"A",{href:!0});var lwr=s(JL);lRe=r(lwr,"BeitConfig"),lwr.forEach(t),iRe=r(q5e," (BEiT model)"),q5e.forEach(t),dRe=i(T),Pf=n(T,"LI",{});var G5e=s(Pf);KV=n(G5e,"STRONG",{});var iwr=s(KV);cRe=r(iwr,"bert"),iwr.forEach(t),fRe=r(G5e," \u2014 "),YL=n(G5e,"A",{href:!0});var dwr=s(YL);mRe=r(dwr,"BertConfig"),dwr.forEach(t),gRe=r(G5e," (BERT model)"),G5e.forEach(t),hRe=i(T),$f=n(T,"LI",{});var O5e=s($f);ZV=n(O5e,"STRONG",{});var cwr=s(ZV);pRe=r(cwr,"bert-generation"),cwr.forEach(t),_Re=r(O5e," \u2014 "),KL=n(O5e,"A",{href:!0});var fwr=s(KL);uRe=r(fwr,"BertGenerationConfig"),fwr.forEach(t),bRe=r(O5e," (Bert Generation model)"),O5e.forEach(t),vRe=i(T),If=n(T,"LI",{});var X5e=s(If);eW=n(X5e,"STRONG",{});var mwr=s(eW);TRe=r(mwr,"big_bird"),mwr.forEach(t),FRe=r(X5e," \u2014 "),ZL=n(X5e,"A",{href:!0});var gwr=s(ZL);CRe=r(gwr,"BigBirdConfig"),gwr.forEach(t),MRe=r(X5e," (BigBird model)"),X5e.forEach(t),ERe=i(T),jf=n(T,"LI",{});var z5e=s(jf);oW=n(z5e,"STRONG",{});var hwr=s(oW);yRe=r(hwr,"bigbird_pegasus"),hwr.forEach(t),wRe=r(z5e," \u2014 "),e7=n(z5e,"A",{href:!0});var pwr=s(e7);ARe=r(pwr,"BigBirdPegasusConfig"),pwr.forEach(t),LRe=r(z5e," (BigBirdPegasus model)"),z5e.forEach(t),BRe=i(T),Nf=n(T,"LI",{});var V5e=s(Nf);rW=n(V5e,"STRONG",{});var _wr=s(rW);kRe=r(_wr,"blenderbot"),_wr.forEach(t),xRe=r(V5e," \u2014 "),o7=n(V5e,"A",{href:!0});var uwr=s(o7);RRe=r(uwr,"BlenderbotConfig"),uwr.forEach(t),SRe=r(V5e," (Blenderbot model)"),V5e.forEach(t),PRe=i(T),Df=n(T,"LI",{});var W5e=s(Df);tW=n(W5e,"STRONG",{});var bwr=s(tW);$Re=r(bwr,"blenderbot-small"),bwr.forEach(t),IRe=r(W5e," \u2014 "),r7=n(W5e,"A",{href:!0});var vwr=s(r7);jRe=r(vwr,"BlenderbotSmallConfig"),vwr.forEach(t),NRe=r(W5e," (BlenderbotSmall model)"),W5e.forEach(t),DRe=i(T),qf=n(T,"LI",{});var Q5e=s(qf);aW=n(Q5e,"STRONG",{});var Twr=s(aW);qRe=r(Twr,"camembert"),Twr.forEach(t),GRe=r(Q5e," \u2014 "),t7=n(Q5e,"A",{href:!0});var Fwr=s(t7);ORe=r(Fwr,"CamembertConfig"),Fwr.forEach(t),XRe=r(Q5e," (CamemBERT model)"),Q5e.forEach(t),zRe=i(T),Gf=n(T,"LI",{});var H5e=s(Gf);nW=n(H5e,"STRONG",{});var Cwr=s(nW);VRe=r(Cwr,"canine"),Cwr.forEach(t),WRe=r(H5e," \u2014 "),a7=n(H5e,"A",{href:!0});var Mwr=s(a7);QRe=r(Mwr,"CanineConfig"),Mwr.forEach(t),HRe=r(H5e," (Canine model)"),H5e.forEach(t),URe=i(T),Of=n(T,"LI",{});var U5e=s(Of);sW=n(U5e,"STRONG",{});var Ewr=s(sW);JRe=r(Ewr,"clip"),Ewr.forEach(t),YRe=r(U5e," \u2014 "),n7=n(U5e,"A",{href:!0});var ywr=s(n7);KRe=r(ywr,"CLIPConfig"),ywr.forEach(t),ZRe=r(U5e," (CLIP model)"),U5e.forEach(t),eSe=i(T),Xf=n(T,"LI",{});var J5e=s(Xf);lW=n(J5e,"STRONG",{});var wwr=s(lW);oSe=r(wwr,"convbert"),wwr.forEach(t),rSe=r(J5e," \u2014 "),s7=n(J5e,"A",{href:!0});var Awr=s(s7);tSe=r(Awr,"ConvBertConfig"),Awr.forEach(t),aSe=r(J5e," (ConvBERT model)"),J5e.forEach(t),nSe=i(T),zf=n(T,"LI",{});var Y5e=s(zf);iW=n(Y5e,"STRONG",{});var Lwr=s(iW);sSe=r(Lwr,"convnext"),Lwr.forEach(t),lSe=r(Y5e," \u2014 "),l7=n(Y5e,"A",{href:!0});var Bwr=s(l7);iSe=r(Bwr,"ConvNextConfig"),Bwr.forEach(t),dSe=r(Y5e," (ConvNext model)"),Y5e.forEach(t),cSe=i(T),Vf=n(T,"LI",{});var K5e=s(Vf);dW=n(K5e,"STRONG",{});var kwr=s(dW);fSe=r(kwr,"ctrl"),kwr.forEach(t),mSe=r(K5e," \u2014 "),i7=n(K5e,"A",{href:!0});var xwr=s(i7);gSe=r(xwr,"CTRLConfig"),xwr.forEach(t),hSe=r(K5e," (CTRL model)"),K5e.forEach(t),pSe=i(T),Wf=n(T,"LI",{});var Z5e=s(Wf);cW=n(Z5e,"STRONG",{});var Rwr=s(cW);_Se=r(Rwr,"deberta"),Rwr.forEach(t),uSe=r(Z5e," \u2014 "),d7=n(Z5e,"A",{href:!0});var Swr=s(d7);bSe=r(Swr,"DebertaConfig"),Swr.forEach(t),vSe=r(Z5e," (DeBERTa model)"),Z5e.forEach(t),TSe=i(T),Qf=n(T,"LI",{});var eve=s(Qf);fW=n(eve,"STRONG",{});var Pwr=s(fW);FSe=r(Pwr,"deberta-v2"),Pwr.forEach(t),CSe=r(eve," \u2014 "),c7=n(eve,"A",{href:!0});var $wr=s(c7);MSe=r($wr,"DebertaV2Config"),$wr.forEach(t),ESe=r(eve," (DeBERTa-v2 model)"),eve.forEach(t),ySe=i(T),Hf=n(T,"LI",{});var ove=s(Hf);mW=n(ove,"STRONG",{});var Iwr=s(mW);wSe=r(Iwr,"deit"),Iwr.forEach(t),ASe=r(ove," \u2014 "),f7=n(ove,"A",{href:!0});var jwr=s(f7);LSe=r(jwr,"DeiTConfig"),jwr.forEach(t),BSe=r(ove," (DeiT model)"),ove.forEach(t),kSe=i(T),Uf=n(T,"LI",{});var rve=s(Uf);gW=n(rve,"STRONG",{});var Nwr=s(gW);xSe=r(Nwr,"detr"),Nwr.forEach(t),RSe=r(rve," \u2014 "),m7=n(rve,"A",{href:!0});var Dwr=s(m7);SSe=r(Dwr,"DetrConfig"),Dwr.forEach(t),PSe=r(rve," (DETR model)"),rve.forEach(t),$Se=i(T),Jf=n(T,"LI",{});var tve=s(Jf);hW=n(tve,"STRONG",{});var qwr=s(hW);ISe=r(qwr,"distilbert"),qwr.forEach(t),jSe=r(tve," \u2014 "),g7=n(tve,"A",{href:!0});var Gwr=s(g7);NSe=r(Gwr,"DistilBertConfig"),Gwr.forEach(t),DSe=r(tve," (DistilBERT model)"),tve.forEach(t),qSe=i(T),Yf=n(T,"LI",{});var ave=s(Yf);pW=n(ave,"STRONG",{});var Owr=s(pW);GSe=r(Owr,"dpr"),Owr.forEach(t),OSe=r(ave," \u2014 "),h7=n(ave,"A",{href:!0});var Xwr=s(h7);XSe=r(Xwr,"DPRConfig"),Xwr.forEach(t),zSe=r(ave," (DPR model)"),ave.forEach(t),VSe=i(T),Kf=n(T,"LI",{});var nve=s(Kf);_W=n(nve,"STRONG",{});var zwr=s(_W);WSe=r(zwr,"electra"),zwr.forEach(t),QSe=r(nve," \u2014 "),p7=n(nve,"A",{href:!0});var Vwr=s(p7);HSe=r(Vwr,"ElectraConfig"),Vwr.forEach(t),USe=r(nve," (ELECTRA model)"),nve.forEach(t),JSe=i(T),Zf=n(T,"LI",{});var sve=s(Zf);uW=n(sve,"STRONG",{});var Wwr=s(uW);YSe=r(Wwr,"encoder-decoder"),Wwr.forEach(t),KSe=r(sve," \u2014 "),_7=n(sve,"A",{href:!0});var Qwr=s(_7);ZSe=r(Qwr,"EncoderDecoderConfig"),Qwr.forEach(t),ePe=r(sve," (Encoder decoder model)"),sve.forEach(t),oPe=i(T),em=n(T,"LI",{});var lve=s(em);bW=n(lve,"STRONG",{});var Hwr=s(bW);rPe=r(Hwr,"flaubert"),Hwr.forEach(t),tPe=r(lve," \u2014 "),u7=n(lve,"A",{href:!0});var Uwr=s(u7);aPe=r(Uwr,"FlaubertConfig"),Uwr.forEach(t),nPe=r(lve," (FlauBERT model)"),lve.forEach(t),sPe=i(T),om=n(T,"LI",{});var ive=s(om);vW=n(ive,"STRONG",{});var Jwr=s(vW);lPe=r(Jwr,"fnet"),Jwr.forEach(t),iPe=r(ive," \u2014 "),b7=n(ive,"A",{href:!0});var Ywr=s(b7);dPe=r(Ywr,"FNetConfig"),Ywr.forEach(t),cPe=r(ive," (FNet model)"),ive.forEach(t),fPe=i(T),rm=n(T,"LI",{});var dve=s(rm);TW=n(dve,"STRONG",{});var Kwr=s(TW);mPe=r(Kwr,"fsmt"),Kwr.forEach(t),gPe=r(dve," \u2014 "),v7=n(dve,"A",{href:!0});var Zwr=s(v7);hPe=r(Zwr,"FSMTConfig"),Zwr.forEach(t),pPe=r(dve," (FairSeq Machine-Translation model)"),dve.forEach(t),_Pe=i(T),tm=n(T,"LI",{});var cve=s(tm);FW=n(cve,"STRONG",{});var eAr=s(FW);uPe=r(eAr,"funnel"),eAr.forEach(t),bPe=r(cve," \u2014 "),T7=n(cve,"A",{href:!0});var oAr=s(T7);vPe=r(oAr,"FunnelConfig"),oAr.forEach(t),TPe=r(cve," (Funnel Transformer model)"),cve.forEach(t),FPe=i(T),am=n(T,"LI",{});var fve=s(am);CW=n(fve,"STRONG",{});var rAr=s(CW);CPe=r(rAr,"gpt2"),rAr.forEach(t),MPe=r(fve," \u2014 "),F7=n(fve,"A",{href:!0});var tAr=s(F7);EPe=r(tAr,"GPT2Config"),tAr.forEach(t),yPe=r(fve," (OpenAI GPT-2 model)"),fve.forEach(t),wPe=i(T),nm=n(T,"LI",{});var mve=s(nm);MW=n(mve,"STRONG",{});var aAr=s(MW);APe=r(aAr,"gpt_neo"),aAr.forEach(t),LPe=r(mve," \u2014 "),C7=n(mve,"A",{href:!0});var nAr=s(C7);BPe=r(nAr,"GPTNeoConfig"),nAr.forEach(t),kPe=r(mve," (GPT Neo model)"),mve.forEach(t),xPe=i(T),sm=n(T,"LI",{});var gve=s(sm);EW=n(gve,"STRONG",{});var sAr=s(EW);RPe=r(sAr,"gptj"),sAr.forEach(t),SPe=r(gve," \u2014 "),M7=n(gve,"A",{href:!0});var lAr=s(M7);PPe=r(lAr,"GPTJConfig"),lAr.forEach(t),$Pe=r(gve," (GPT-J model)"),gve.forEach(t),IPe=i(T),lm=n(T,"LI",{});var hve=s(lm);yW=n(hve,"STRONG",{});var iAr=s(yW);jPe=r(iAr,"hubert"),iAr.forEach(t),NPe=r(hve," \u2014 "),E7=n(hve,"A",{href:!0});var dAr=s(E7);DPe=r(dAr,"HubertConfig"),dAr.forEach(t),qPe=r(hve," (Hubert model)"),hve.forEach(t),GPe=i(T),im=n(T,"LI",{});var pve=s(im);wW=n(pve,"STRONG",{});var cAr=s(wW);OPe=r(cAr,"ibert"),cAr.forEach(t),XPe=r(pve," \u2014 "),y7=n(pve,"A",{href:!0});var fAr=s(y7);zPe=r(fAr,"IBertConfig"),fAr.forEach(t),VPe=r(pve," (I-BERT model)"),pve.forEach(t),WPe=i(T),dm=n(T,"LI",{});var _ve=s(dm);AW=n(_ve,"STRONG",{});var mAr=s(AW);QPe=r(mAr,"imagegpt"),mAr.forEach(t),HPe=r(_ve," \u2014 "),w7=n(_ve,"A",{href:!0});var gAr=s(w7);UPe=r(gAr,"ImageGPTConfig"),gAr.forEach(t),JPe=r(_ve," (ImageGPT model)"),_ve.forEach(t),YPe=i(T),cm=n(T,"LI",{});var uve=s(cm);LW=n(uve,"STRONG",{});var hAr=s(LW);KPe=r(hAr,"layoutlm"),hAr.forEach(t),ZPe=r(uve," \u2014 "),A7=n(uve,"A",{href:!0});var pAr=s(A7);e$e=r(pAr,"LayoutLMConfig"),pAr.forEach(t),o$e=r(uve," (LayoutLM model)"),uve.forEach(t),r$e=i(T),fm=n(T,"LI",{});var bve=s(fm);BW=n(bve,"STRONG",{});var _Ar=s(BW);t$e=r(_Ar,"layoutlmv2"),_Ar.forEach(t),a$e=r(bve," \u2014 "),L7=n(bve,"A",{href:!0});var uAr=s(L7);n$e=r(uAr,"LayoutLMv2Config"),uAr.forEach(t),s$e=r(bve," (LayoutLMv2 model)"),bve.forEach(t),l$e=i(T),mm=n(T,"LI",{});var vve=s(mm);kW=n(vve,"STRONG",{});var bAr=s(kW);i$e=r(bAr,"led"),bAr.forEach(t),d$e=r(vve," \u2014 "),B7=n(vve,"A",{href:!0});var vAr=s(B7);c$e=r(vAr,"LEDConfig"),vAr.forEach(t),f$e=r(vve," (LED model)"),vve.forEach(t),m$e=i(T),gm=n(T,"LI",{});var Tve=s(gm);xW=n(Tve,"STRONG",{});var TAr=s(xW);g$e=r(TAr,"longformer"),TAr.forEach(t),h$e=r(Tve," \u2014 "),k7=n(Tve,"A",{href:!0});var FAr=s(k7);p$e=r(FAr,"LongformerConfig"),FAr.forEach(t),_$e=r(Tve," (Longformer model)"),Tve.forEach(t),u$e=i(T),hm=n(T,"LI",{});var Fve=s(hm);RW=n(Fve,"STRONG",{});var CAr=s(RW);b$e=r(CAr,"luke"),CAr.forEach(t),v$e=r(Fve," \u2014 "),x7=n(Fve,"A",{href:!0});var MAr=s(x7);T$e=r(MAr,"LukeConfig"),MAr.forEach(t),F$e=r(Fve," (LUKE model)"),Fve.forEach(t),C$e=i(T),pm=n(T,"LI",{});var Cve=s(pm);SW=n(Cve,"STRONG",{});var EAr=s(SW);M$e=r(EAr,"lxmert"),EAr.forEach(t),E$e=r(Cve," \u2014 "),R7=n(Cve,"A",{href:!0});var yAr=s(R7);y$e=r(yAr,"LxmertConfig"),yAr.forEach(t),w$e=r(Cve," (LXMERT model)"),Cve.forEach(t),A$e=i(T),_m=n(T,"LI",{});var Mve=s(_m);PW=n(Mve,"STRONG",{});var wAr=s(PW);L$e=r(wAr,"m2m_100"),wAr.forEach(t),B$e=r(Mve," \u2014 "),S7=n(Mve,"A",{href:!0});var AAr=s(S7);k$e=r(AAr,"M2M100Config"),AAr.forEach(t),x$e=r(Mve," (M2M100 model)"),Mve.forEach(t),R$e=i(T),um=n(T,"LI",{});var Eve=s(um);$W=n(Eve,"STRONG",{});var LAr=s($W);S$e=r(LAr,"marian"),LAr.forEach(t),P$e=r(Eve," \u2014 "),P7=n(Eve,"A",{href:!0});var BAr=s(P7);$$e=r(BAr,"MarianConfig"),BAr.forEach(t),I$e=r(Eve," (Marian model)"),Eve.forEach(t),j$e=i(T),bm=n(T,"LI",{});var yve=s(bm);IW=n(yve,"STRONG",{});var kAr=s(IW);N$e=r(kAr,"maskformer"),kAr.forEach(t),D$e=r(yve," \u2014 "),$7=n(yve,"A",{href:!0});var xAr=s($7);q$e=r(xAr,"MaskFormerConfig"),xAr.forEach(t),G$e=r(yve," (MaskFormer model)"),yve.forEach(t),O$e=i(T),vm=n(T,"LI",{});var wve=s(vm);jW=n(wve,"STRONG",{});var RAr=s(jW);X$e=r(RAr,"mbart"),RAr.forEach(t),z$e=r(wve," \u2014 "),I7=n(wve,"A",{href:!0});var SAr=s(I7);V$e=r(SAr,"MBartConfig"),SAr.forEach(t),W$e=r(wve," (mBART model)"),wve.forEach(t),Q$e=i(T),Tm=n(T,"LI",{});var Ave=s(Tm);NW=n(Ave,"STRONG",{});var PAr=s(NW);H$e=r(PAr,"megatron-bert"),PAr.forEach(t),U$e=r(Ave," \u2014 "),j7=n(Ave,"A",{href:!0});var $Ar=s(j7);J$e=r($Ar,"MegatronBertConfig"),$Ar.forEach(t),Y$e=r(Ave," (MegatronBert model)"),Ave.forEach(t),K$e=i(T),Fm=n(T,"LI",{});var Lve=s(Fm);DW=n(Lve,"STRONG",{});var IAr=s(DW);Z$e=r(IAr,"mobilebert"),IAr.forEach(t),eIe=r(Lve," \u2014 "),N7=n(Lve,"A",{href:!0});var jAr=s(N7);oIe=r(jAr,"MobileBertConfig"),jAr.forEach(t),rIe=r(Lve," (MobileBERT model)"),Lve.forEach(t),tIe=i(T),Cm=n(T,"LI",{});var Bve=s(Cm);qW=n(Bve,"STRONG",{});var NAr=s(qW);aIe=r(NAr,"mpnet"),NAr.forEach(t),nIe=r(Bve," \u2014 "),D7=n(Bve,"A",{href:!0});var DAr=s(D7);sIe=r(DAr,"MPNetConfig"),DAr.forEach(t),lIe=r(Bve," (MPNet model)"),Bve.forEach(t),iIe=i(T),Mm=n(T,"LI",{});var kve=s(Mm);GW=n(kve,"STRONG",{});var qAr=s(GW);dIe=r(qAr,"mt5"),qAr.forEach(t),cIe=r(kve," \u2014 "),q7=n(kve,"A",{href:!0});var GAr=s(q7);fIe=r(GAr,"MT5Config"),GAr.forEach(t),mIe=r(kve," (mT5 model)"),kve.forEach(t),gIe=i(T),Em=n(T,"LI",{});var xve=s(Em);OW=n(xve,"STRONG",{});var OAr=s(OW);hIe=r(OAr,"nystromformer"),OAr.forEach(t),pIe=r(xve," \u2014 "),G7=n(xve,"A",{href:!0});var XAr=s(G7);_Ie=r(XAr,"NystromformerConfig"),XAr.forEach(t),uIe=r(xve," (Nystromformer model)"),xve.forEach(t),bIe=i(T),ym=n(T,"LI",{});var Rve=s(ym);XW=n(Rve,"STRONG",{});var zAr=s(XW);vIe=r(zAr,"openai-gpt"),zAr.forEach(t),TIe=r(Rve," \u2014 "),O7=n(Rve,"A",{href:!0});var VAr=s(O7);FIe=r(VAr,"OpenAIGPTConfig"),VAr.forEach(t),CIe=r(Rve," (OpenAI GPT model)"),Rve.forEach(t),MIe=i(T),wm=n(T,"LI",{});var Sve=s(wm);zW=n(Sve,"STRONG",{});var WAr=s(zW);EIe=r(WAr,"pegasus"),WAr.forEach(t),yIe=r(Sve," \u2014 "),X7=n(Sve,"A",{href:!0});var QAr=s(X7);wIe=r(QAr,"PegasusConfig"),QAr.forEach(t),AIe=r(Sve," (Pegasus model)"),Sve.forEach(t),LIe=i(T),Am=n(T,"LI",{});var Pve=s(Am);VW=n(Pve,"STRONG",{});var HAr=s(VW);BIe=r(HAr,"perceiver"),HAr.forEach(t),kIe=r(Pve," \u2014 "),z7=n(Pve,"A",{href:!0});var UAr=s(z7);xIe=r(UAr,"PerceiverConfig"),UAr.forEach(t),RIe=r(Pve," (Perceiver model)"),Pve.forEach(t),SIe=i(T),Lm=n(T,"LI",{});var $ve=s(Lm);WW=n($ve,"STRONG",{});var JAr=s(WW);PIe=r(JAr,"plbart"),JAr.forEach(t),$Ie=r($ve," \u2014 "),V7=n($ve,"A",{href:!0});var YAr=s(V7);IIe=r(YAr,"PLBartConfig"),YAr.forEach(t),jIe=r($ve," (PLBart model)"),$ve.forEach(t),NIe=i(T),Bm=n(T,"LI",{});var Ive=s(Bm);QW=n(Ive,"STRONG",{});var KAr=s(QW);DIe=r(KAr,"poolformer"),KAr.forEach(t),qIe=r(Ive," \u2014 "),W7=n(Ive,"A",{href:!0});var ZAr=s(W7);GIe=r(ZAr,"PoolFormerConfig"),ZAr.forEach(t),OIe=r(Ive," (PoolFormer model)"),Ive.forEach(t),XIe=i(T),km=n(T,"LI",{});var jve=s(km);HW=n(jve,"STRONG",{});var e0r=s(HW);zIe=r(e0r,"prophetnet"),e0r.forEach(t),VIe=r(jve," \u2014 "),Q7=n(jve,"A",{href:!0});var o0r=s(Q7);WIe=r(o0r,"ProphetNetConfig"),o0r.forEach(t),QIe=r(jve," (ProphetNet model)"),jve.forEach(t),HIe=i(T),xm=n(T,"LI",{});var Nve=s(xm);UW=n(Nve,"STRONG",{});var r0r=s(UW);UIe=r(r0r,"qdqbert"),r0r.forEach(t),JIe=r(Nve," \u2014 "),H7=n(Nve,"A",{href:!0});var t0r=s(H7);YIe=r(t0r,"QDQBertConfig"),t0r.forEach(t),KIe=r(Nve," (QDQBert model)"),Nve.forEach(t),ZIe=i(T),Rm=n(T,"LI",{});var Dve=s(Rm);JW=n(Dve,"STRONG",{});var a0r=s(JW);eje=r(a0r,"rag"),a0r.forEach(t),oje=r(Dve," \u2014 "),U7=n(Dve,"A",{href:!0});var n0r=s(U7);rje=r(n0r,"RagConfig"),n0r.forEach(t),tje=r(Dve," (RAG model)"),Dve.forEach(t),aje=i(T),Sm=n(T,"LI",{});var qve=s(Sm);YW=n(qve,"STRONG",{});var s0r=s(YW);nje=r(s0r,"realm"),s0r.forEach(t),sje=r(qve," \u2014 "),J7=n(qve,"A",{href:!0});var l0r=s(J7);lje=r(l0r,"RealmConfig"),l0r.forEach(t),ije=r(qve," (Realm model)"),qve.forEach(t),dje=i(T),Pm=n(T,"LI",{});var Gve=s(Pm);KW=n(Gve,"STRONG",{});var i0r=s(KW);cje=r(i0r,"reformer"),i0r.forEach(t),fje=r(Gve," \u2014 "),Y7=n(Gve,"A",{href:!0});var d0r=s(Y7);mje=r(d0r,"ReformerConfig"),d0r.forEach(t),gje=r(Gve," (Reformer model)"),Gve.forEach(t),hje=i(T),$m=n(T,"LI",{});var Ove=s($m);ZW=n(Ove,"STRONG",{});var c0r=s(ZW);pje=r(c0r,"rembert"),c0r.forEach(t),_je=r(Ove," \u2014 "),K7=n(Ove,"A",{href:!0});var f0r=s(K7);uje=r(f0r,"RemBertConfig"),f0r.forEach(t),bje=r(Ove," (RemBERT model)"),Ove.forEach(t),vje=i(T),Im=n(T,"LI",{});var Xve=s(Im);eQ=n(Xve,"STRONG",{});var m0r=s(eQ);Tje=r(m0r,"retribert"),m0r.forEach(t),Fje=r(Xve," \u2014 "),Z7=n(Xve,"A",{href:!0});var g0r=s(Z7);Cje=r(g0r,"RetriBertConfig"),g0r.forEach(t),Mje=r(Xve," (RetriBERT model)"),Xve.forEach(t),Eje=i(T),jm=n(T,"LI",{});var zve=s(jm);oQ=n(zve,"STRONG",{});var h0r=s(oQ);yje=r(h0r,"roberta"),h0r.forEach(t),wje=r(zve," \u2014 "),e9=n(zve,"A",{href:!0});var p0r=s(e9);Aje=r(p0r,"RobertaConfig"),p0r.forEach(t),Lje=r(zve," (RoBERTa model)"),zve.forEach(t),Bje=i(T),Nm=n(T,"LI",{});var Vve=s(Nm);rQ=n(Vve,"STRONG",{});var _0r=s(rQ);kje=r(_0r,"roformer"),_0r.forEach(t),xje=r(Vve," \u2014 "),o9=n(Vve,"A",{href:!0});var u0r=s(o9);Rje=r(u0r,"RoFormerConfig"),u0r.forEach(t),Sje=r(Vve," (RoFormer model)"),Vve.forEach(t),Pje=i(T),Dm=n(T,"LI",{});var Wve=s(Dm);tQ=n(Wve,"STRONG",{});var b0r=s(tQ);$je=r(b0r,"segformer"),b0r.forEach(t),Ije=r(Wve," \u2014 "),r9=n(Wve,"A",{href:!0});var v0r=s(r9);jje=r(v0r,"SegformerConfig"),v0r.forEach(t),Nje=r(Wve," (SegFormer model)"),Wve.forEach(t),Dje=i(T),qm=n(T,"LI",{});var Qve=s(qm);aQ=n(Qve,"STRONG",{});var T0r=s(aQ);qje=r(T0r,"sew"),T0r.forEach(t),Gje=r(Qve," \u2014 "),t9=n(Qve,"A",{href:!0});var F0r=s(t9);Oje=r(F0r,"SEWConfig"),F0r.forEach(t),Xje=r(Qve," (SEW model)"),Qve.forEach(t),zje=i(T),Gm=n(T,"LI",{});var Hve=s(Gm);nQ=n(Hve,"STRONG",{});var C0r=s(nQ);Vje=r(C0r,"sew-d"),C0r.forEach(t),Wje=r(Hve," \u2014 "),a9=n(Hve,"A",{href:!0});var M0r=s(a9);Qje=r(M0r,"SEWDConfig"),M0r.forEach(t),Hje=r(Hve," (SEW-D model)"),Hve.forEach(t),Uje=i(T),Om=n(T,"LI",{});var Uve=s(Om);sQ=n(Uve,"STRONG",{});var E0r=s(sQ);Jje=r(E0r,"speech-encoder-decoder"),E0r.forEach(t),Yje=r(Uve," \u2014 "),n9=n(Uve,"A",{href:!0});var y0r=s(n9);Kje=r(y0r,"SpeechEncoderDecoderConfig"),y0r.forEach(t),Zje=r(Uve," (Speech Encoder decoder model)"),Uve.forEach(t),eNe=i(T),Xm=n(T,"LI",{});var Jve=s(Xm);lQ=n(Jve,"STRONG",{});var w0r=s(lQ);oNe=r(w0r,"speech_to_text"),w0r.forEach(t),rNe=r(Jve," \u2014 "),s9=n(Jve,"A",{href:!0});var A0r=s(s9);tNe=r(A0r,"Speech2TextConfig"),A0r.forEach(t),aNe=r(Jve," (Speech2Text model)"),Jve.forEach(t),nNe=i(T),zm=n(T,"LI",{});var Yve=s(zm);iQ=n(Yve,"STRONG",{});var L0r=s(iQ);sNe=r(L0r,"speech_to_text_2"),L0r.forEach(t),lNe=r(Yve," \u2014 "),l9=n(Yve,"A",{href:!0});var B0r=s(l9);iNe=r(B0r,"Speech2Text2Config"),B0r.forEach(t),dNe=r(Yve," (Speech2Text2 model)"),Yve.forEach(t),cNe=i(T),Vm=n(T,"LI",{});var Kve=s(Vm);dQ=n(Kve,"STRONG",{});var k0r=s(dQ);fNe=r(k0r,"splinter"),k0r.forEach(t),mNe=r(Kve," \u2014 "),i9=n(Kve,"A",{href:!0});var x0r=s(i9);gNe=r(x0r,"SplinterConfig"),x0r.forEach(t),hNe=r(Kve," (Splinter model)"),Kve.forEach(t),pNe=i(T),Wm=n(T,"LI",{});var Zve=s(Wm);cQ=n(Zve,"STRONG",{});var R0r=s(cQ);_Ne=r(R0r,"squeezebert"),R0r.forEach(t),uNe=r(Zve," \u2014 "),d9=n(Zve,"A",{href:!0});var S0r=s(d9);bNe=r(S0r,"SqueezeBertConfig"),S0r.forEach(t),vNe=r(Zve," (SqueezeBERT model)"),Zve.forEach(t),TNe=i(T),Qm=n(T,"LI",{});var e6e=s(Qm);fQ=n(e6e,"STRONG",{});var P0r=s(fQ);FNe=r(P0r,"swin"),P0r.forEach(t),CNe=r(e6e," \u2014 "),c9=n(e6e,"A",{href:!0});var $0r=s(c9);MNe=r($0r,"SwinConfig"),$0r.forEach(t),ENe=r(e6e," (Swin model)"),e6e.forEach(t),yNe=i(T),Hm=n(T,"LI",{});var o6e=s(Hm);mQ=n(o6e,"STRONG",{});var I0r=s(mQ);wNe=r(I0r,"t5"),I0r.forEach(t),ANe=r(o6e," \u2014 "),f9=n(o6e,"A",{href:!0});var j0r=s(f9);LNe=r(j0r,"T5Config"),j0r.forEach(t),BNe=r(o6e," (T5 model)"),o6e.forEach(t),kNe=i(T),Um=n(T,"LI",{});var r6e=s(Um);gQ=n(r6e,"STRONG",{});var N0r=s(gQ);xNe=r(N0r,"tapas"),N0r.forEach(t),RNe=r(r6e," \u2014 "),m9=n(r6e,"A",{href:!0});var D0r=s(m9);SNe=r(D0r,"TapasConfig"),D0r.forEach(t),PNe=r(r6e," (TAPAS model)"),r6e.forEach(t),$Ne=i(T),Jm=n(T,"LI",{});var t6e=s(Jm);hQ=n(t6e,"STRONG",{});var q0r=s(hQ);INe=r(q0r,"transfo-xl"),q0r.forEach(t),jNe=r(t6e," \u2014 "),g9=n(t6e,"A",{href:!0});var G0r=s(g9);NNe=r(G0r,"TransfoXLConfig"),G0r.forEach(t),DNe=r(t6e," (Transformer-XL model)"),t6e.forEach(t),qNe=i(T),Ym=n(T,"LI",{});var a6e=s(Ym);pQ=n(a6e,"STRONG",{});var O0r=s(pQ);GNe=r(O0r,"trocr"),O0r.forEach(t),ONe=r(a6e," \u2014 "),h9=n(a6e,"A",{href:!0});var X0r=s(h9);XNe=r(X0r,"TrOCRConfig"),X0r.forEach(t),zNe=r(a6e," (TrOCR model)"),a6e.forEach(t),VNe=i(T),Km=n(T,"LI",{});var n6e=s(Km);_Q=n(n6e,"STRONG",{});var z0r=s(_Q);WNe=r(z0r,"unispeech"),z0r.forEach(t),QNe=r(n6e," \u2014 "),p9=n(n6e,"A",{href:!0});var V0r=s(p9);HNe=r(V0r,"UniSpeechConfig"),V0r.forEach(t),UNe=r(n6e," (UniSpeech model)"),n6e.forEach(t),JNe=i(T),Zm=n(T,"LI",{});var s6e=s(Zm);uQ=n(s6e,"STRONG",{});var W0r=s(uQ);YNe=r(W0r,"unispeech-sat"),W0r.forEach(t),KNe=r(s6e," \u2014 "),_9=n(s6e,"A",{href:!0});var Q0r=s(_9);ZNe=r(Q0r,"UniSpeechSatConfig"),Q0r.forEach(t),eDe=r(s6e," (UniSpeechSat model)"),s6e.forEach(t),oDe=i(T),eg=n(T,"LI",{});var l6e=s(eg);bQ=n(l6e,"STRONG",{});var H0r=s(bQ);rDe=r(H0r,"vilt"),H0r.forEach(t),tDe=r(l6e," \u2014 "),u9=n(l6e,"A",{href:!0});var U0r=s(u9);aDe=r(U0r,"ViltConfig"),U0r.forEach(t),nDe=r(l6e," (ViLT model)"),l6e.forEach(t),sDe=i(T),og=n(T,"LI",{});var i6e=s(og);vQ=n(i6e,"STRONG",{});var J0r=s(vQ);lDe=r(J0r,"vision-encoder-decoder"),J0r.forEach(t),iDe=r(i6e," \u2014 "),b9=n(i6e,"A",{href:!0});var Y0r=s(b9);dDe=r(Y0r,"VisionEncoderDecoderConfig"),Y0r.forEach(t),cDe=r(i6e," (Vision Encoder decoder model)"),i6e.forEach(t),fDe=i(T),rg=n(T,"LI",{});var d6e=s(rg);TQ=n(d6e,"STRONG",{});var K0r=s(TQ);mDe=r(K0r,"vision-text-dual-encoder"),K0r.forEach(t),gDe=r(d6e," \u2014 "),v9=n(d6e,"A",{href:!0});var Z0r=s(v9);hDe=r(Z0r,"VisionTextDualEncoderConfig"),Z0r.forEach(t),pDe=r(d6e," (VisionTextDualEncoder model)"),d6e.forEach(t),_De=i(T),tg=n(T,"LI",{});var c6e=s(tg);FQ=n(c6e,"STRONG",{});var eLr=s(FQ);uDe=r(eLr,"visual_bert"),eLr.forEach(t),bDe=r(c6e," \u2014 "),T9=n(c6e,"A",{href:!0});var oLr=s(T9);vDe=r(oLr,"VisualBertConfig"),oLr.forEach(t),TDe=r(c6e," (VisualBert model)"),c6e.forEach(t),FDe=i(T),ag=n(T,"LI",{});var f6e=s(ag);CQ=n(f6e,"STRONG",{});var rLr=s(CQ);CDe=r(rLr,"vit"),rLr.forEach(t),MDe=r(f6e," \u2014 "),F9=n(f6e,"A",{href:!0});var tLr=s(F9);EDe=r(tLr,"ViTConfig"),tLr.forEach(t),yDe=r(f6e," (ViT model)"),f6e.forEach(t),wDe=i(T),ng=n(T,"LI",{});var m6e=s(ng);MQ=n(m6e,"STRONG",{});var aLr=s(MQ);ADe=r(aLr,"vit_mae"),aLr.forEach(t),LDe=r(m6e," \u2014 "),C9=n(m6e,"A",{href:!0});var nLr=s(C9);BDe=r(nLr,"ViTMAEConfig"),nLr.forEach(t),kDe=r(m6e," (ViTMAE model)"),m6e.forEach(t),xDe=i(T),sg=n(T,"LI",{});var g6e=s(sg);EQ=n(g6e,"STRONG",{});var sLr=s(EQ);RDe=r(sLr,"wav2vec2"),sLr.forEach(t),SDe=r(g6e," \u2014 "),M9=n(g6e,"A",{href:!0});var lLr=s(M9);PDe=r(lLr,"Wav2Vec2Config"),lLr.forEach(t),$De=r(g6e," (Wav2Vec2 model)"),g6e.forEach(t),IDe=i(T),lg=n(T,"LI",{});var h6e=s(lg);yQ=n(h6e,"STRONG",{});var iLr=s(yQ);jDe=r(iLr,"wavlm"),iLr.forEach(t),NDe=r(h6e," \u2014 "),E9=n(h6e,"A",{href:!0});var dLr=s(E9);DDe=r(dLr,"WavLMConfig"),dLr.forEach(t),qDe=r(h6e," (WavLM model)"),h6e.forEach(t),GDe=i(T),ig=n(T,"LI",{});var p6e=s(ig);wQ=n(p6e,"STRONG",{});var cLr=s(wQ);ODe=r(cLr,"xglm"),cLr.forEach(t),XDe=r(p6e," \u2014 "),y9=n(p6e,"A",{href:!0});var fLr=s(y9);zDe=r(fLr,"XGLMConfig"),fLr.forEach(t),VDe=r(p6e," (XGLM model)"),p6e.forEach(t),WDe=i(T),dg=n(T,"LI",{});var _6e=s(dg);AQ=n(_6e,"STRONG",{});var mLr=s(AQ);QDe=r(mLr,"xlm"),mLr.forEach(t),HDe=r(_6e," \u2014 "),w9=n(_6e,"A",{href:!0});var gLr=s(w9);UDe=r(gLr,"XLMConfig"),gLr.forEach(t),JDe=r(_6e," (XLM model)"),_6e.forEach(t),YDe=i(T),cg=n(T,"LI",{});var u6e=s(cg);LQ=n(u6e,"STRONG",{});var hLr=s(LQ);KDe=r(hLr,"xlm-prophetnet"),hLr.forEach(t),ZDe=r(u6e," \u2014 "),A9=n(u6e,"A",{href:!0});var pLr=s(A9);eqe=r(pLr,"XLMProphetNetConfig"),pLr.forEach(t),oqe=r(u6e," (XLMProphetNet model)"),u6e.forEach(t),rqe=i(T),fg=n(T,"LI",{});var b6e=s(fg);BQ=n(b6e,"STRONG",{});var _Lr=s(BQ);tqe=r(_Lr,"xlm-roberta"),_Lr.forEach(t),aqe=r(b6e," \u2014 "),L9=n(b6e,"A",{href:!0});var uLr=s(L9);nqe=r(uLr,"XLMRobertaConfig"),uLr.forEach(t),sqe=r(b6e," (XLM-RoBERTa model)"),b6e.forEach(t),lqe=i(T),mg=n(T,"LI",{});var v6e=s(mg);kQ=n(v6e,"STRONG",{});var bLr=s(kQ);iqe=r(bLr,"xlm-roberta-xl"),bLr.forEach(t),dqe=r(v6e," \u2014 "),B9=n(v6e,"A",{href:!0});var vLr=s(B9);cqe=r(vLr,"XLMRobertaXLConfig"),vLr.forEach(t),fqe=r(v6e," (XLM-RoBERTa-XL model)"),v6e.forEach(t),mqe=i(T),gg=n(T,"LI",{});var T6e=s(gg);xQ=n(T6e,"STRONG",{});var TLr=s(xQ);gqe=r(TLr,"xlnet"),TLr.forEach(t),hqe=r(T6e," \u2014 "),k9=n(T6e,"A",{href:!0});var FLr=s(k9);pqe=r(FLr,"XLNetConfig"),FLr.forEach(t),_qe=r(T6e," (XLNet model)"),T6e.forEach(t),uqe=i(T),hg=n(T,"LI",{});var F6e=s(hg);RQ=n(F6e,"STRONG",{});var CLr=s(RQ);bqe=r(CLr,"yoso"),CLr.forEach(t),vqe=r(F6e," \u2014 "),x9=n(F6e,"A",{href:!0});var MLr=s(x9);Tqe=r(MLr,"YosoConfig"),MLr.forEach(t),Fqe=r(F6e," (YOSO model)"),F6e.forEach(t),T.forEach(t),Cqe=i(ia),SQ=n(ia,"P",{});var ELr=s(SQ);Mqe=r(ELr,"Examples:"),ELr.forEach(t),Eqe=i(ia),m(hM.$$.fragment,ia),ia.forEach(t),yqe=i(Ps),pg=n(Ps,"DIV",{class:!0});var bBe=s(pg);m(pM.$$.fragment,bBe),wqe=i(bBe),PQ=n(bBe,"P",{});var yLr=s(PQ);Aqe=r(yLr,"Register a new configuration for this class."),yLr.forEach(t),bBe.forEach(t),Ps.forEach(t),b7e=i(d),Ii=n(d,"H2",{class:!0});var vBe=s(Ii);_g=n(vBe,"A",{id:!0,class:!0,href:!0});var wLr=s(_g);$Q=n(wLr,"SPAN",{});var ALr=s($Q);m(_M.$$.fragment,ALr),ALr.forEach(t),wLr.forEach(t),Lqe=i(vBe),IQ=n(vBe,"SPAN",{});var LLr=s(IQ);Bqe=r(LLr,"AutoTokenizer"),LLr.forEach(t),vBe.forEach(t),v7e=i(d),Oo=n(d,"DIV",{class:!0});var $s=s(Oo);m(uM.$$.fragment,$s),kqe=i($s),bM=n($s,"P",{});var TBe=s(bM);xqe=r(TBe,`This is a generic tokenizer class that will be instantiated as one of the tokenizer classes of the library when
created with the `),R9=n(TBe,"A",{href:!0});var BLr=s(R9);Rqe=r(BLr,"AutoTokenizer.from_pretrained()"),BLr.forEach(t),Sqe=r(TBe," class method."),TBe.forEach(t),Pqe=i($s),vM=n($s,"P",{});var FBe=s(vM);$qe=r(FBe,"This class cannot be instantiated directly using "),jQ=n(FBe,"CODE",{});var kLr=s(jQ);Iqe=r(kLr,"__init__()"),kLr.forEach(t),jqe=r(FBe," (throws an error)."),FBe.forEach(t),Nqe=i($s),mo=n($s,"DIV",{class:!0});var da=s(mo);m(TM.$$.fragment,da),Dqe=i(da),NQ=n(da,"P",{});var xLr=s(NQ);qqe=r(xLr,"Instantiate one of the tokenizer classes of the library from a pretrained model vocabulary."),xLr.forEach(t),Gqe=i(da),ja=n(da,"P",{});var l4=s(ja);Oqe=r(l4,"The tokenizer class to instantiate is selected based on the "),DQ=n(l4,"CODE",{});var RLr=s(DQ);Xqe=r(RLr,"model_type"),RLr.forEach(t),zqe=r(l4,` property of the config object (either
passed as an argument or loaded from `),qQ=n(l4,"CODE",{});var SLr=s(qQ);Vqe=r(SLr,"pretrained_model_name_or_path"),SLr.forEach(t),Wqe=r(l4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),GQ=n(l4,"CODE",{});var PLr=s(GQ);Qqe=r(PLr,"pretrained_model_name_or_path"),PLr.forEach(t),Hqe=r(l4,":"),l4.forEach(t),Uqe=i(da),M=n(da,"UL",{});var y=s(M);Dn=n(y,"LI",{});var W0=s(Dn);OQ=n(W0,"STRONG",{});var $Lr=s(OQ);Jqe=r($Lr,"albert"),$Lr.forEach(t),Yqe=r(W0," \u2014 "),S9=n(W0,"A",{href:!0});var ILr=s(S9);Kqe=r(ILr,"AlbertTokenizer"),ILr.forEach(t),Zqe=r(W0," or "),P9=n(W0,"A",{href:!0});var jLr=s(P9);eGe=r(jLr,"AlbertTokenizerFast"),jLr.forEach(t),oGe=r(W0," (ALBERT model)"),W0.forEach(t),rGe=i(y),qn=n(y,"LI",{});var Q0=s(qn);XQ=n(Q0,"STRONG",{});var NLr=s(XQ);tGe=r(NLr,"bart"),NLr.forEach(t),aGe=r(Q0," \u2014 "),$9=n(Q0,"A",{href:!0});var DLr=s($9);nGe=r(DLr,"BartTokenizer"),DLr.forEach(t),sGe=r(Q0," or "),I9=n(Q0,"A",{href:!0});var qLr=s(I9);lGe=r(qLr,"BartTokenizerFast"),qLr.forEach(t),iGe=r(Q0," (BART model)"),Q0.forEach(t),dGe=i(y),Gn=n(y,"LI",{});var H0=s(Gn);zQ=n(H0,"STRONG",{});var GLr=s(zQ);cGe=r(GLr,"barthez"),GLr.forEach(t),fGe=r(H0," \u2014 "),j9=n(H0,"A",{href:!0});var OLr=s(j9);mGe=r(OLr,"BarthezTokenizer"),OLr.forEach(t),gGe=r(H0," or "),N9=n(H0,"A",{href:!0});var XLr=s(N9);hGe=r(XLr,"BarthezTokenizerFast"),XLr.forEach(t),pGe=r(H0," (BARThez model)"),H0.forEach(t),_Ge=i(y),ug=n(y,"LI",{});var C6e=s(ug);VQ=n(C6e,"STRONG",{});var zLr=s(VQ);uGe=r(zLr,"bartpho"),zLr.forEach(t),bGe=r(C6e," \u2014 "),D9=n(C6e,"A",{href:!0});var VLr=s(D9);vGe=r(VLr,"BartphoTokenizer"),VLr.forEach(t),TGe=r(C6e," (BARTpho model)"),C6e.forEach(t),FGe=i(y),On=n(y,"LI",{});var U0=s(On);WQ=n(U0,"STRONG",{});var WLr=s(WQ);CGe=r(WLr,"bert"),WLr.forEach(t),MGe=r(U0," \u2014 "),q9=n(U0,"A",{href:!0});var QLr=s(q9);EGe=r(QLr,"BertTokenizer"),QLr.forEach(t),yGe=r(U0," or "),G9=n(U0,"A",{href:!0});var HLr=s(G9);wGe=r(HLr,"BertTokenizerFast"),HLr.forEach(t),AGe=r(U0," (BERT model)"),U0.forEach(t),LGe=i(y),bg=n(y,"LI",{});var M6e=s(bg);QQ=n(M6e,"STRONG",{});var ULr=s(QQ);BGe=r(ULr,"bert-generation"),ULr.forEach(t),kGe=r(M6e," \u2014 "),O9=n(M6e,"A",{href:!0});var JLr=s(O9);xGe=r(JLr,"BertGenerationTokenizer"),JLr.forEach(t),RGe=r(M6e," (Bert Generation model)"),M6e.forEach(t),SGe=i(y),vg=n(y,"LI",{});var E6e=s(vg);HQ=n(E6e,"STRONG",{});var YLr=s(HQ);PGe=r(YLr,"bert-japanese"),YLr.forEach(t),$Ge=r(E6e," \u2014 "),X9=n(E6e,"A",{href:!0});var KLr=s(X9);IGe=r(KLr,"BertJapaneseTokenizer"),KLr.forEach(t),jGe=r(E6e," (BertJapanese model)"),E6e.forEach(t),NGe=i(y),Tg=n(y,"LI",{});var y6e=s(Tg);UQ=n(y6e,"STRONG",{});var ZLr=s(UQ);DGe=r(ZLr,"bertweet"),ZLr.forEach(t),qGe=r(y6e," \u2014 "),z9=n(y6e,"A",{href:!0});var e7r=s(z9);GGe=r(e7r,"BertweetTokenizer"),e7r.forEach(t),OGe=r(y6e," (Bertweet model)"),y6e.forEach(t),XGe=i(y),Xn=n(y,"LI",{});var J0=s(Xn);JQ=n(J0,"STRONG",{});var o7r=s(JQ);zGe=r(o7r,"big_bird"),o7r.forEach(t),VGe=r(J0," \u2014 "),V9=n(J0,"A",{href:!0});var r7r=s(V9);WGe=r(r7r,"BigBirdTokenizer"),r7r.forEach(t),QGe=r(J0," or "),W9=n(J0,"A",{href:!0});var t7r=s(W9);HGe=r(t7r,"BigBirdTokenizerFast"),t7r.forEach(t),UGe=r(J0," (BigBird model)"),J0.forEach(t),JGe=i(y),zn=n(y,"LI",{});var Y0=s(zn);YQ=n(Y0,"STRONG",{});var a7r=s(YQ);YGe=r(a7r,"bigbird_pegasus"),a7r.forEach(t),KGe=r(Y0," \u2014 "),Q9=n(Y0,"A",{href:!0});var n7r=s(Q9);ZGe=r(n7r,"PegasusTokenizer"),n7r.forEach(t),eOe=r(Y0," or "),H9=n(Y0,"A",{href:!0});var s7r=s(H9);oOe=r(s7r,"PegasusTokenizerFast"),s7r.forEach(t),rOe=r(Y0," (BigBirdPegasus model)"),Y0.forEach(t),tOe=i(y),Vn=n(y,"LI",{});var K0=s(Vn);KQ=n(K0,"STRONG",{});var l7r=s(KQ);aOe=r(l7r,"blenderbot"),l7r.forEach(t),nOe=r(K0," \u2014 "),U9=n(K0,"A",{href:!0});var i7r=s(U9);sOe=r(i7r,"BlenderbotTokenizer"),i7r.forEach(t),lOe=r(K0," or "),J9=n(K0,"A",{href:!0});var d7r=s(J9);iOe=r(d7r,"BlenderbotTokenizerFast"),d7r.forEach(t),dOe=r(K0," (Blenderbot model)"),K0.forEach(t),cOe=i(y),Fg=n(y,"LI",{});var w6e=s(Fg);ZQ=n(w6e,"STRONG",{});var c7r=s(ZQ);fOe=r(c7r,"blenderbot-small"),c7r.forEach(t),mOe=r(w6e," \u2014 "),Y9=n(w6e,"A",{href:!0});var f7r=s(Y9);gOe=r(f7r,"BlenderbotSmallTokenizer"),f7r.forEach(t),hOe=r(w6e," (BlenderbotSmall model)"),w6e.forEach(t),pOe=i(y),Cg=n(y,"LI",{});var A6e=s(Cg);eH=n(A6e,"STRONG",{});var m7r=s(eH);_Oe=r(m7r,"byt5"),m7r.forEach(t),uOe=r(A6e," \u2014 "),K9=n(A6e,"A",{href:!0});var g7r=s(K9);bOe=r(g7r,"ByT5Tokenizer"),g7r.forEach(t),vOe=r(A6e," (ByT5 model)"),A6e.forEach(t),TOe=i(y),Wn=n(y,"LI",{});var Z0=s(Wn);oH=n(Z0,"STRONG",{});var h7r=s(oH);FOe=r(h7r,"camembert"),h7r.forEach(t),COe=r(Z0," \u2014 "),Z9=n(Z0,"A",{href:!0});var p7r=s(Z9);MOe=r(p7r,"CamembertTokenizer"),p7r.forEach(t),EOe=r(Z0," or "),eB=n(Z0,"A",{href:!0});var _7r=s(eB);yOe=r(_7r,"CamembertTokenizerFast"),_7r.forEach(t),wOe=r(Z0," (CamemBERT model)"),Z0.forEach(t),AOe=i(y),Mg=n(y,"LI",{});var L6e=s(Mg);rH=n(L6e,"STRONG",{});var u7r=s(rH);LOe=r(u7r,"canine"),u7r.forEach(t),BOe=r(L6e," \u2014 "),oB=n(L6e,"A",{href:!0});var b7r=s(oB);kOe=r(b7r,"CanineTokenizer"),b7r.forEach(t),xOe=r(L6e," (Canine model)"),L6e.forEach(t),ROe=i(y),Qn=n(y,"LI",{});var eL=s(Qn);tH=n(eL,"STRONG",{});var v7r=s(tH);SOe=r(v7r,"clip"),v7r.forEach(t),POe=r(eL," \u2014 "),rB=n(eL,"A",{href:!0});var T7r=s(rB);$Oe=r(T7r,"CLIPTokenizer"),T7r.forEach(t),IOe=r(eL," or "),tB=n(eL,"A",{href:!0});var F7r=s(tB);jOe=r(F7r,"CLIPTokenizerFast"),F7r.forEach(t),NOe=r(eL," (CLIP model)"),eL.forEach(t),DOe=i(y),Hn=n(y,"LI",{});var oL=s(Hn);aH=n(oL,"STRONG",{});var C7r=s(aH);qOe=r(C7r,"convbert"),C7r.forEach(t),GOe=r(oL," \u2014 "),aB=n(oL,"A",{href:!0});var M7r=s(aB);OOe=r(M7r,"ConvBertTokenizer"),M7r.forEach(t),XOe=r(oL," or "),nB=n(oL,"A",{href:!0});var E7r=s(nB);zOe=r(E7r,"ConvBertTokenizerFast"),E7r.forEach(t),VOe=r(oL," (ConvBERT model)"),oL.forEach(t),WOe=i(y),Un=n(y,"LI",{});var rL=s(Un);nH=n(rL,"STRONG",{});var y7r=s(nH);QOe=r(y7r,"cpm"),y7r.forEach(t),HOe=r(rL," \u2014 "),sB=n(rL,"A",{href:!0});var w7r=s(sB);UOe=r(w7r,"CpmTokenizer"),w7r.forEach(t),JOe=r(rL," or "),sH=n(rL,"CODE",{});var A7r=s(sH);YOe=r(A7r,"CpmTokenizerFast"),A7r.forEach(t),KOe=r(rL," (CPM model)"),rL.forEach(t),ZOe=i(y),Eg=n(y,"LI",{});var B6e=s(Eg);lH=n(B6e,"STRONG",{});var L7r=s(lH);eXe=r(L7r,"ctrl"),L7r.forEach(t),oXe=r(B6e," \u2014 "),lB=n(B6e,"A",{href:!0});var B7r=s(lB);rXe=r(B7r,"CTRLTokenizer"),B7r.forEach(t),tXe=r(B6e," (CTRL model)"),B6e.forEach(t),aXe=i(y),Jn=n(y,"LI",{});var tL=s(Jn);iH=n(tL,"STRONG",{});var k7r=s(iH);nXe=r(k7r,"deberta"),k7r.forEach(t),sXe=r(tL," \u2014 "),iB=n(tL,"A",{href:!0});var x7r=s(iB);lXe=r(x7r,"DebertaTokenizer"),x7r.forEach(t),iXe=r(tL," or "),dB=n(tL,"A",{href:!0});var R7r=s(dB);dXe=r(R7r,"DebertaTokenizerFast"),R7r.forEach(t),cXe=r(tL," (DeBERTa model)"),tL.forEach(t),fXe=i(y),yg=n(y,"LI",{});var k6e=s(yg);dH=n(k6e,"STRONG",{});var S7r=s(dH);mXe=r(S7r,"deberta-v2"),S7r.forEach(t),gXe=r(k6e," \u2014 "),cB=n(k6e,"A",{href:!0});var P7r=s(cB);hXe=r(P7r,"DebertaV2Tokenizer"),P7r.forEach(t),pXe=r(k6e," (DeBERTa-v2 model)"),k6e.forEach(t),_Xe=i(y),Yn=n(y,"LI",{});var aL=s(Yn);cH=n(aL,"STRONG",{});var $7r=s(cH);uXe=r($7r,"distilbert"),$7r.forEach(t),bXe=r(aL," \u2014 "),fB=n(aL,"A",{href:!0});var I7r=s(fB);vXe=r(I7r,"DistilBertTokenizer"),I7r.forEach(t),TXe=r(aL," or "),mB=n(aL,"A",{href:!0});var j7r=s(mB);FXe=r(j7r,"DistilBertTokenizerFast"),j7r.forEach(t),CXe=r(aL," (DistilBERT model)"),aL.forEach(t),MXe=i(y),Kn=n(y,"LI",{});var nL=s(Kn);fH=n(nL,"STRONG",{});var N7r=s(fH);EXe=r(N7r,"dpr"),N7r.forEach(t),yXe=r(nL," \u2014 "),gB=n(nL,"A",{href:!0});var D7r=s(gB);wXe=r(D7r,"DPRQuestionEncoderTokenizer"),D7r.forEach(t),AXe=r(nL," or "),hB=n(nL,"A",{href:!0});var q7r=s(hB);LXe=r(q7r,"DPRQuestionEncoderTokenizerFast"),q7r.forEach(t),BXe=r(nL," (DPR model)"),nL.forEach(t),kXe=i(y),Zn=n(y,"LI",{});var sL=s(Zn);mH=n(sL,"STRONG",{});var G7r=s(mH);xXe=r(G7r,"electra"),G7r.forEach(t),RXe=r(sL," \u2014 "),pB=n(sL,"A",{href:!0});var O7r=s(pB);SXe=r(O7r,"ElectraTokenizer"),O7r.forEach(t),PXe=r(sL," or "),_B=n(sL,"A",{href:!0});var X7r=s(_B);$Xe=r(X7r,"ElectraTokenizerFast"),X7r.forEach(t),IXe=r(sL," (ELECTRA model)"),sL.forEach(t),jXe=i(y),wg=n(y,"LI",{});var x6e=s(wg);gH=n(x6e,"STRONG",{});var z7r=s(gH);NXe=r(z7r,"flaubert"),z7r.forEach(t),DXe=r(x6e," \u2014 "),uB=n(x6e,"A",{href:!0});var V7r=s(uB);qXe=r(V7r,"FlaubertTokenizer"),V7r.forEach(t),GXe=r(x6e," (FlauBERT model)"),x6e.forEach(t),OXe=i(y),es=n(y,"LI",{});var lL=s(es);hH=n(lL,"STRONG",{});var W7r=s(hH);XXe=r(W7r,"fnet"),W7r.forEach(t),zXe=r(lL," \u2014 "),bB=n(lL,"A",{href:!0});var Q7r=s(bB);VXe=r(Q7r,"FNetTokenizer"),Q7r.forEach(t),WXe=r(lL," or "),vB=n(lL,"A",{href:!0});var H7r=s(vB);QXe=r(H7r,"FNetTokenizerFast"),H7r.forEach(t),HXe=r(lL," (FNet model)"),lL.forEach(t),UXe=i(y),Ag=n(y,"LI",{});var R6e=s(Ag);pH=n(R6e,"STRONG",{});var U7r=s(pH);JXe=r(U7r,"fsmt"),U7r.forEach(t),YXe=r(R6e," \u2014 "),TB=n(R6e,"A",{href:!0});var J7r=s(TB);KXe=r(J7r,"FSMTTokenizer"),J7r.forEach(t),ZXe=r(R6e," (FairSeq Machine-Translation model)"),R6e.forEach(t),eze=i(y),os=n(y,"LI",{});var iL=s(os);_H=n(iL,"STRONG",{});var Y7r=s(_H);oze=r(Y7r,"funnel"),Y7r.forEach(t),rze=r(iL," \u2014 "),FB=n(iL,"A",{href:!0});var K7r=s(FB);tze=r(K7r,"FunnelTokenizer"),K7r.forEach(t),aze=r(iL," or "),CB=n(iL,"A",{href:!0});var Z7r=s(CB);nze=r(Z7r,"FunnelTokenizerFast"),Z7r.forEach(t),sze=r(iL," (Funnel Transformer model)"),iL.forEach(t),lze=i(y),rs=n(y,"LI",{});var dL=s(rs);uH=n(dL,"STRONG",{});var e9r=s(uH);ize=r(e9r,"gpt2"),e9r.forEach(t),dze=r(dL," \u2014 "),MB=n(dL,"A",{href:!0});var o9r=s(MB);cze=r(o9r,"GPT2Tokenizer"),o9r.forEach(t),fze=r(dL," or "),EB=n(dL,"A",{href:!0});var r9r=s(EB);mze=r(r9r,"GPT2TokenizerFast"),r9r.forEach(t),gze=r(dL," (OpenAI GPT-2 model)"),dL.forEach(t),hze=i(y),ts=n(y,"LI",{});var cL=s(ts);bH=n(cL,"STRONG",{});var t9r=s(bH);pze=r(t9r,"gpt_neo"),t9r.forEach(t),_ze=r(cL," \u2014 "),yB=n(cL,"A",{href:!0});var a9r=s(yB);uze=r(a9r,"GPT2Tokenizer"),a9r.forEach(t),bze=r(cL," or "),wB=n(cL,"A",{href:!0});var n9r=s(wB);vze=r(n9r,"GPT2TokenizerFast"),n9r.forEach(t),Tze=r(cL," (GPT Neo model)"),cL.forEach(t),Fze=i(y),as=n(y,"LI",{});var fL=s(as);vH=n(fL,"STRONG",{});var s9r=s(vH);Cze=r(s9r,"herbert"),s9r.forEach(t),Mze=r(fL," \u2014 "),AB=n(fL,"A",{href:!0});var l9r=s(AB);Eze=r(l9r,"HerbertTokenizer"),l9r.forEach(t),yze=r(fL," or "),LB=n(fL,"A",{href:!0});var i9r=s(LB);wze=r(i9r,"HerbertTokenizerFast"),i9r.forEach(t),Aze=r(fL," (HerBERT model)"),fL.forEach(t),Lze=i(y),Lg=n(y,"LI",{});var S6e=s(Lg);TH=n(S6e,"STRONG",{});var d9r=s(TH);Bze=r(d9r,"hubert"),d9r.forEach(t),kze=r(S6e," \u2014 "),BB=n(S6e,"A",{href:!0});var c9r=s(BB);xze=r(c9r,"Wav2Vec2CTCTokenizer"),c9r.forEach(t),Rze=r(S6e," (Hubert model)"),S6e.forEach(t),Sze=i(y),ns=n(y,"LI",{});var mL=s(ns);FH=n(mL,"STRONG",{});var f9r=s(FH);Pze=r(f9r,"ibert"),f9r.forEach(t),$ze=r(mL," \u2014 "),kB=n(mL,"A",{href:!0});var m9r=s(kB);Ize=r(m9r,"RobertaTokenizer"),m9r.forEach(t),jze=r(mL," or "),xB=n(mL,"A",{href:!0});var g9r=s(xB);Nze=r(g9r,"RobertaTokenizerFast"),g9r.forEach(t),Dze=r(mL," (I-BERT model)"),mL.forEach(t),qze=i(y),ss=n(y,"LI",{});var gL=s(ss);CH=n(gL,"STRONG",{});var h9r=s(CH);Gze=r(h9r,"layoutlm"),h9r.forEach(t),Oze=r(gL," \u2014 "),RB=n(gL,"A",{href:!0});var p9r=s(RB);Xze=r(p9r,"LayoutLMTokenizer"),p9r.forEach(t),zze=r(gL," or "),SB=n(gL,"A",{href:!0});var _9r=s(SB);Vze=r(_9r,"LayoutLMTokenizerFast"),_9r.forEach(t),Wze=r(gL," (LayoutLM model)"),gL.forEach(t),Qze=i(y),ls=n(y,"LI",{});var hL=s(ls);MH=n(hL,"STRONG",{});var u9r=s(MH);Hze=r(u9r,"layoutlmv2"),u9r.forEach(t),Uze=r(hL," \u2014 "),PB=n(hL,"A",{href:!0});var b9r=s(PB);Jze=r(b9r,"LayoutLMv2Tokenizer"),b9r.forEach(t),Yze=r(hL," or "),$B=n(hL,"A",{href:!0});var v9r=s($B);Kze=r(v9r,"LayoutLMv2TokenizerFast"),v9r.forEach(t),Zze=r(hL," (LayoutLMv2 model)"),hL.forEach(t),eVe=i(y),is=n(y,"LI",{});var pL=s(is);EH=n(pL,"STRONG",{});var T9r=s(EH);oVe=r(T9r,"layoutxlm"),T9r.forEach(t),rVe=r(pL," \u2014 "),IB=n(pL,"A",{href:!0});var F9r=s(IB);tVe=r(F9r,"LayoutXLMTokenizer"),F9r.forEach(t),aVe=r(pL," or "),jB=n(pL,"A",{href:!0});var C9r=s(jB);nVe=r(C9r,"LayoutXLMTokenizerFast"),C9r.forEach(t),sVe=r(pL," (LayoutXLM model)"),pL.forEach(t),lVe=i(y),ds=n(y,"LI",{});var _L=s(ds);yH=n(_L,"STRONG",{});var M9r=s(yH);iVe=r(M9r,"led"),M9r.forEach(t),dVe=r(_L," \u2014 "),NB=n(_L,"A",{href:!0});var E9r=s(NB);cVe=r(E9r,"LEDTokenizer"),E9r.forEach(t),fVe=r(_L," or "),DB=n(_L,"A",{href:!0});var y9r=s(DB);mVe=r(y9r,"LEDTokenizerFast"),y9r.forEach(t),gVe=r(_L," (LED model)"),_L.forEach(t),hVe=i(y),cs=n(y,"LI",{});var uL=s(cs);wH=n(uL,"STRONG",{});var w9r=s(wH);pVe=r(w9r,"longformer"),w9r.forEach(t),_Ve=r(uL," \u2014 "),qB=n(uL,"A",{href:!0});var A9r=s(qB);uVe=r(A9r,"LongformerTokenizer"),A9r.forEach(t),bVe=r(uL," or "),GB=n(uL,"A",{href:!0});var L9r=s(GB);vVe=r(L9r,"LongformerTokenizerFast"),L9r.forEach(t),TVe=r(uL," (Longformer model)"),uL.forEach(t),FVe=i(y),Bg=n(y,"LI",{});var P6e=s(Bg);AH=n(P6e,"STRONG",{});var B9r=s(AH);CVe=r(B9r,"luke"),B9r.forEach(t),MVe=r(P6e," \u2014 "),OB=n(P6e,"A",{href:!0});var k9r=s(OB);EVe=r(k9r,"LukeTokenizer"),k9r.forEach(t),yVe=r(P6e," (LUKE model)"),P6e.forEach(t),wVe=i(y),fs=n(y,"LI",{});var bL=s(fs);LH=n(bL,"STRONG",{});var x9r=s(LH);AVe=r(x9r,"lxmert"),x9r.forEach(t),LVe=r(bL," \u2014 "),XB=n(bL,"A",{href:!0});var R9r=s(XB);BVe=r(R9r,"LxmertTokenizer"),R9r.forEach(t),kVe=r(bL," or "),zB=n(bL,"A",{href:!0});var S9r=s(zB);xVe=r(S9r,"LxmertTokenizerFast"),S9r.forEach(t),RVe=r(bL," (LXMERT model)"),bL.forEach(t),SVe=i(y),kg=n(y,"LI",{});var $6e=s(kg);BH=n($6e,"STRONG",{});var P9r=s(BH);PVe=r(P9r,"m2m_100"),P9r.forEach(t),$Ve=r($6e," \u2014 "),VB=n($6e,"A",{href:!0});var $9r=s(VB);IVe=r($9r,"M2M100Tokenizer"),$9r.forEach(t),jVe=r($6e," (M2M100 model)"),$6e.forEach(t),NVe=i(y),xg=n(y,"LI",{});var I6e=s(xg);kH=n(I6e,"STRONG",{});var I9r=s(kH);DVe=r(I9r,"marian"),I9r.forEach(t),qVe=r(I6e," \u2014 "),WB=n(I6e,"A",{href:!0});var j9r=s(WB);GVe=r(j9r,"MarianTokenizer"),j9r.forEach(t),OVe=r(I6e," (Marian model)"),I6e.forEach(t),XVe=i(y),ms=n(y,"LI",{});var vL=s(ms);xH=n(vL,"STRONG",{});var N9r=s(xH);zVe=r(N9r,"mbart"),N9r.forEach(t),VVe=r(vL," \u2014 "),QB=n(vL,"A",{href:!0});var D9r=s(QB);WVe=r(D9r,"MBartTokenizer"),D9r.forEach(t),QVe=r(vL," or "),HB=n(vL,"A",{href:!0});var q9r=s(HB);HVe=r(q9r,"MBartTokenizerFast"),q9r.forEach(t),UVe=r(vL," (mBART model)"),vL.forEach(t),JVe=i(y),gs=n(y,"LI",{});var TL=s(gs);RH=n(TL,"STRONG",{});var G9r=s(RH);YVe=r(G9r,"mbart50"),G9r.forEach(t),KVe=r(TL," \u2014 "),UB=n(TL,"A",{href:!0});var O9r=s(UB);ZVe=r(O9r,"MBart50Tokenizer"),O9r.forEach(t),eWe=r(TL," or "),JB=n(TL,"A",{href:!0});var X9r=s(JB);oWe=r(X9r,"MBart50TokenizerFast"),X9r.forEach(t),rWe=r(TL," (mBART-50 model)"),TL.forEach(t),tWe=i(y),Rg=n(y,"LI",{});var j6e=s(Rg);SH=n(j6e,"STRONG",{});var z9r=s(SH);aWe=r(z9r,"mluke"),z9r.forEach(t),nWe=r(j6e," \u2014 "),YB=n(j6e,"A",{href:!0});var V9r=s(YB);sWe=r(V9r,"MLukeTokenizer"),V9r.forEach(t),lWe=r(j6e," (mLUKE model)"),j6e.forEach(t),iWe=i(y),hs=n(y,"LI",{});var FL=s(hs);PH=n(FL,"STRONG",{});var W9r=s(PH);dWe=r(W9r,"mobilebert"),W9r.forEach(t),cWe=r(FL," \u2014 "),KB=n(FL,"A",{href:!0});var Q9r=s(KB);fWe=r(Q9r,"MobileBertTokenizer"),Q9r.forEach(t),mWe=r(FL," or "),ZB=n(FL,"A",{href:!0});var H9r=s(ZB);gWe=r(H9r,"MobileBertTokenizerFast"),H9r.forEach(t),hWe=r(FL," (MobileBERT model)"),FL.forEach(t),pWe=i(y),ps=n(y,"LI",{});var CL=s(ps);$H=n(CL,"STRONG",{});var U9r=s($H);_We=r(U9r,"mpnet"),U9r.forEach(t),uWe=r(CL," \u2014 "),ek=n(CL,"A",{href:!0});var J9r=s(ek);bWe=r(J9r,"MPNetTokenizer"),J9r.forEach(t),vWe=r(CL," or "),ok=n(CL,"A",{href:!0});var Y9r=s(ok);TWe=r(Y9r,"MPNetTokenizerFast"),Y9r.forEach(t),FWe=r(CL," (MPNet model)"),CL.forEach(t),CWe=i(y),_s=n(y,"LI",{});var ML=s(_s);IH=n(ML,"STRONG",{});var K9r=s(IH);MWe=r(K9r,"mt5"),K9r.forEach(t),EWe=r(ML," \u2014 "),rk=n(ML,"A",{href:!0});var Z9r=s(rk);yWe=r(Z9r,"MT5Tokenizer"),Z9r.forEach(t),wWe=r(ML," or "),tk=n(ML,"A",{href:!0});var eBr=s(tk);AWe=r(eBr,"MT5TokenizerFast"),eBr.forEach(t),LWe=r(ML," (mT5 model)"),ML.forEach(t),BWe=i(y),us=n(y,"LI",{});var EL=s(us);jH=n(EL,"STRONG",{});var oBr=s(jH);kWe=r(oBr,"openai-gpt"),oBr.forEach(t),xWe=r(EL," \u2014 "),ak=n(EL,"A",{href:!0});var rBr=s(ak);RWe=r(rBr,"OpenAIGPTTokenizer"),rBr.forEach(t),SWe=r(EL," or "),nk=n(EL,"A",{href:!0});var tBr=s(nk);PWe=r(tBr,"OpenAIGPTTokenizerFast"),tBr.forEach(t),$We=r(EL," (OpenAI GPT model)"),EL.forEach(t),IWe=i(y),bs=n(y,"LI",{});var yL=s(bs);NH=n(yL,"STRONG",{});var aBr=s(NH);jWe=r(aBr,"pegasus"),aBr.forEach(t),NWe=r(yL," \u2014 "),sk=n(yL,"A",{href:!0});var nBr=s(sk);DWe=r(nBr,"PegasusTokenizer"),nBr.forEach(t),qWe=r(yL," or "),lk=n(yL,"A",{href:!0});var sBr=s(lk);GWe=r(sBr,"PegasusTokenizerFast"),sBr.forEach(t),OWe=r(yL," (Pegasus model)"),yL.forEach(t),XWe=i(y),Sg=n(y,"LI",{});var N6e=s(Sg);DH=n(N6e,"STRONG",{});var lBr=s(DH);zWe=r(lBr,"perceiver"),lBr.forEach(t),VWe=r(N6e," \u2014 "),ik=n(N6e,"A",{href:!0});var iBr=s(ik);WWe=r(iBr,"PerceiverTokenizer"),iBr.forEach(t),QWe=r(N6e," (Perceiver model)"),N6e.forEach(t),HWe=i(y),Pg=n(y,"LI",{});var D6e=s(Pg);qH=n(D6e,"STRONG",{});var dBr=s(qH);UWe=r(dBr,"phobert"),dBr.forEach(t),JWe=r(D6e," \u2014 "),dk=n(D6e,"A",{href:!0});var cBr=s(dk);YWe=r(cBr,"PhobertTokenizer"),cBr.forEach(t),KWe=r(D6e," (PhoBERT model)"),D6e.forEach(t),ZWe=i(y),$g=n(y,"LI",{});var q6e=s($g);GH=n(q6e,"STRONG",{});var fBr=s(GH);eQe=r(fBr,"plbart"),fBr.forEach(t),oQe=r(q6e," \u2014 "),ck=n(q6e,"A",{href:!0});var mBr=s(ck);rQe=r(mBr,"PLBartTokenizer"),mBr.forEach(t),tQe=r(q6e," (PLBart model)"),q6e.forEach(t),aQe=i(y),Ig=n(y,"LI",{});var G6e=s(Ig);OH=n(G6e,"STRONG",{});var gBr=s(OH);nQe=r(gBr,"prophetnet"),gBr.forEach(t),sQe=r(G6e," \u2014 "),fk=n(G6e,"A",{href:!0});var hBr=s(fk);lQe=r(hBr,"ProphetNetTokenizer"),hBr.forEach(t),iQe=r(G6e," (ProphetNet model)"),G6e.forEach(t),dQe=i(y),vs=n(y,"LI",{});var wL=s(vs);XH=n(wL,"STRONG",{});var pBr=s(XH);cQe=r(pBr,"qdqbert"),pBr.forEach(t),fQe=r(wL," \u2014 "),mk=n(wL,"A",{href:!0});var _Br=s(mk);mQe=r(_Br,"BertTokenizer"),_Br.forEach(t),gQe=r(wL," or "),gk=n(wL,"A",{href:!0});var uBr=s(gk);hQe=r(uBr,"BertTokenizerFast"),uBr.forEach(t),pQe=r(wL," (QDQBert model)"),wL.forEach(t),_Qe=i(y),jg=n(y,"LI",{});var O6e=s(jg);zH=n(O6e,"STRONG",{});var bBr=s(zH);uQe=r(bBr,"rag"),bBr.forEach(t),bQe=r(O6e," \u2014 "),hk=n(O6e,"A",{href:!0});var vBr=s(hk);vQe=r(vBr,"RagTokenizer"),vBr.forEach(t),TQe=r(O6e," (RAG model)"),O6e.forEach(t),FQe=i(y),Ts=n(y,"LI",{});var AL=s(Ts);VH=n(AL,"STRONG",{});var TBr=s(VH);CQe=r(TBr,"reformer"),TBr.forEach(t),MQe=r(AL," \u2014 "),pk=n(AL,"A",{href:!0});var FBr=s(pk);EQe=r(FBr,"ReformerTokenizer"),FBr.forEach(t),yQe=r(AL," or "),_k=n(AL,"A",{href:!0});var CBr=s(_k);wQe=r(CBr,"ReformerTokenizerFast"),CBr.forEach(t),AQe=r(AL," (Reformer model)"),AL.forEach(t),LQe=i(y),Fs=n(y,"LI",{});var LL=s(Fs);WH=n(LL,"STRONG",{});var MBr=s(WH);BQe=r(MBr,"rembert"),MBr.forEach(t),kQe=r(LL," \u2014 "),uk=n(LL,"A",{href:!0});var EBr=s(uk);xQe=r(EBr,"RemBertTokenizer"),EBr.forEach(t),RQe=r(LL," or "),bk=n(LL,"A",{href:!0});var yBr=s(bk);SQe=r(yBr,"RemBertTokenizerFast"),yBr.forEach(t),PQe=r(LL," (RemBERT model)"),LL.forEach(t),$Qe=i(y),Cs=n(y,"LI",{});var BL=s(Cs);QH=n(BL,"STRONG",{});var wBr=s(QH);IQe=r(wBr,"retribert"),wBr.forEach(t),jQe=r(BL," \u2014 "),vk=n(BL,"A",{href:!0});var ABr=s(vk);NQe=r(ABr,"RetriBertTokenizer"),ABr.forEach(t),DQe=r(BL," or "),Tk=n(BL,"A",{href:!0});var LBr=s(Tk);qQe=r(LBr,"RetriBertTokenizerFast"),LBr.forEach(t),GQe=r(BL," (RetriBERT model)"),BL.forEach(t),OQe=i(y),Ms=n(y,"LI",{});var kL=s(Ms);HH=n(kL,"STRONG",{});var BBr=s(HH);XQe=r(BBr,"roberta"),BBr.forEach(t),zQe=r(kL," \u2014 "),Fk=n(kL,"A",{href:!0});var kBr=s(Fk);VQe=r(kBr,"RobertaTokenizer"),kBr.forEach(t),WQe=r(kL," or "),Ck=n(kL,"A",{href:!0});var xBr=s(Ck);QQe=r(xBr,"RobertaTokenizerFast"),xBr.forEach(t),HQe=r(kL," (RoBERTa model)"),kL.forEach(t),UQe=i(y),Es=n(y,"LI",{});var xL=s(Es);UH=n(xL,"STRONG",{});var RBr=s(UH);JQe=r(RBr,"roformer"),RBr.forEach(t),YQe=r(xL," \u2014 "),Mk=n(xL,"A",{href:!0});var SBr=s(Mk);KQe=r(SBr,"RoFormerTokenizer"),SBr.forEach(t),ZQe=r(xL," or "),Ek=n(xL,"A",{href:!0});var PBr=s(Ek);eHe=r(PBr,"RoFormerTokenizerFast"),PBr.forEach(t),oHe=r(xL," (RoFormer model)"),xL.forEach(t),rHe=i(y),Ng=n(y,"LI",{});var X6e=s(Ng);JH=n(X6e,"STRONG",{});var $Br=s(JH);tHe=r($Br,"speech_to_text"),$Br.forEach(t),aHe=r(X6e," \u2014 "),yk=n(X6e,"A",{href:!0});var IBr=s(yk);nHe=r(IBr,"Speech2TextTokenizer"),IBr.forEach(t),sHe=r(X6e," (Speech2Text model)"),X6e.forEach(t),lHe=i(y),Dg=n(y,"LI",{});var z6e=s(Dg);YH=n(z6e,"STRONG",{});var jBr=s(YH);iHe=r(jBr,"speech_to_text_2"),jBr.forEach(t),dHe=r(z6e," \u2014 "),wk=n(z6e,"A",{href:!0});var NBr=s(wk);cHe=r(NBr,"Speech2Text2Tokenizer"),NBr.forEach(t),fHe=r(z6e," (Speech2Text2 model)"),z6e.forEach(t),mHe=i(y),ys=n(y,"LI",{});var RL=s(ys);KH=n(RL,"STRONG",{});var DBr=s(KH);gHe=r(DBr,"splinter"),DBr.forEach(t),hHe=r(RL," \u2014 "),Ak=n(RL,"A",{href:!0});var qBr=s(Ak);pHe=r(qBr,"SplinterTokenizer"),qBr.forEach(t),_He=r(RL," or "),Lk=n(RL,"A",{href:!0});var GBr=s(Lk);uHe=r(GBr,"SplinterTokenizerFast"),GBr.forEach(t),bHe=r(RL," (Splinter model)"),RL.forEach(t),vHe=i(y),ws=n(y,"LI",{});var SL=s(ws);ZH=n(SL,"STRONG",{});var OBr=s(ZH);THe=r(OBr,"squeezebert"),OBr.forEach(t),FHe=r(SL," \u2014 "),Bk=n(SL,"A",{href:!0});var XBr=s(Bk);CHe=r(XBr,"SqueezeBertTokenizer"),XBr.forEach(t),MHe=r(SL," or "),kk=n(SL,"A",{href:!0});var zBr=s(kk);EHe=r(zBr,"SqueezeBertTokenizerFast"),zBr.forEach(t),yHe=r(SL," (SqueezeBERT model)"),SL.forEach(t),wHe=i(y),As=n(y,"LI",{});var PL=s(As);eU=n(PL,"STRONG",{});var VBr=s(eU);AHe=r(VBr,"t5"),VBr.forEach(t),LHe=r(PL," \u2014 "),xk=n(PL,"A",{href:!0});var WBr=s(xk);BHe=r(WBr,"T5Tokenizer"),WBr.forEach(t),kHe=r(PL," or "),Rk=n(PL,"A",{href:!0});var QBr=s(Rk);xHe=r(QBr,"T5TokenizerFast"),QBr.forEach(t),RHe=r(PL," (T5 model)"),PL.forEach(t),SHe=i(y),qg=n(y,"LI",{});var V6e=s(qg);oU=n(V6e,"STRONG",{});var HBr=s(oU);PHe=r(HBr,"tapas"),HBr.forEach(t),$He=r(V6e," \u2014 "),Sk=n(V6e,"A",{href:!0});var UBr=s(Sk);IHe=r(UBr,"TapasTokenizer"),UBr.forEach(t),jHe=r(V6e," (TAPAS model)"),V6e.forEach(t),NHe=i(y),Gg=n(y,"LI",{});var W6e=s(Gg);rU=n(W6e,"STRONG",{});var JBr=s(rU);DHe=r(JBr,"transfo-xl"),JBr.forEach(t),qHe=r(W6e," \u2014 "),Pk=n(W6e,"A",{href:!0});var YBr=s(Pk);GHe=r(YBr,"TransfoXLTokenizer"),YBr.forEach(t),OHe=r(W6e," (Transformer-XL model)"),W6e.forEach(t),XHe=i(y),Og=n(y,"LI",{});var Q6e=s(Og);tU=n(Q6e,"STRONG",{});var KBr=s(tU);zHe=r(KBr,"wav2vec2"),KBr.forEach(t),VHe=r(Q6e," \u2014 "),$k=n(Q6e,"A",{href:!0});var ZBr=s($k);WHe=r(ZBr,"Wav2Vec2CTCTokenizer"),ZBr.forEach(t),QHe=r(Q6e," (Wav2Vec2 model)"),Q6e.forEach(t),HHe=i(y),Xg=n(y,"LI",{});var H6e=s(Xg);aU=n(H6e,"STRONG",{});var ekr=s(aU);UHe=r(ekr,"wav2vec2_phoneme"),ekr.forEach(t),JHe=r(H6e," \u2014 "),Ik=n(H6e,"A",{href:!0});var okr=s(Ik);YHe=r(okr,"Wav2Vec2PhonemeCTCTokenizer"),okr.forEach(t),KHe=r(H6e," (Wav2Vec2Phoneme model)"),H6e.forEach(t),ZHe=i(y),Ls=n(y,"LI",{});var $L=s(Ls);nU=n($L,"STRONG",{});var rkr=s(nU);eUe=r(rkr,"xglm"),rkr.forEach(t),oUe=r($L," \u2014 "),jk=n($L,"A",{href:!0});var tkr=s(jk);rUe=r(tkr,"XGLMTokenizer"),tkr.forEach(t),tUe=r($L," or "),Nk=n($L,"A",{href:!0});var akr=s(Nk);aUe=r(akr,"XGLMTokenizerFast"),akr.forEach(t),nUe=r($L," (XGLM model)"),$L.forEach(t),sUe=i(y),zg=n(y,"LI",{});var U6e=s(zg);sU=n(U6e,"STRONG",{});var nkr=s(sU);lUe=r(nkr,"xlm"),nkr.forEach(t),iUe=r(U6e," \u2014 "),Dk=n(U6e,"A",{href:!0});var skr=s(Dk);dUe=r(skr,"XLMTokenizer"),skr.forEach(t),cUe=r(U6e," (XLM model)"),U6e.forEach(t),fUe=i(y),Vg=n(y,"LI",{});var J6e=s(Vg);lU=n(J6e,"STRONG",{});var lkr=s(lU);mUe=r(lkr,"xlm-prophetnet"),lkr.forEach(t),gUe=r(J6e," \u2014 "),qk=n(J6e,"A",{href:!0});var ikr=s(qk);hUe=r(ikr,"XLMProphetNetTokenizer"),ikr.forEach(t),pUe=r(J6e," (XLMProphetNet model)"),J6e.forEach(t),_Ue=i(y),Bs=n(y,"LI",{});var IL=s(Bs);iU=n(IL,"STRONG",{});var dkr=s(iU);uUe=r(dkr,"xlm-roberta"),dkr.forEach(t),bUe=r(IL," \u2014 "),Gk=n(IL,"A",{href:!0});var ckr=s(Gk);vUe=r(ckr,"XLMRobertaTokenizer"),ckr.forEach(t),TUe=r(IL," or "),Ok=n(IL,"A",{href:!0});var fkr=s(Ok);FUe=r(fkr,"XLMRobertaTokenizerFast"),fkr.forEach(t),CUe=r(IL," (XLM-RoBERTa model)"),IL.forEach(t),MUe=i(y),ks=n(y,"LI",{});var jL=s(ks);dU=n(jL,"STRONG",{});var mkr=s(dU);EUe=r(mkr,"xlnet"),mkr.forEach(t),yUe=r(jL," \u2014 "),Xk=n(jL,"A",{href:!0});var gkr=s(Xk);wUe=r(gkr,"XLNetTokenizer"),gkr.forEach(t),AUe=r(jL," or "),zk=n(jL,"A",{href:!0});var hkr=s(zk);LUe=r(hkr,"XLNetTokenizerFast"),hkr.forEach(t),BUe=r(jL," (XLNet model)"),jL.forEach(t),y.forEach(t),kUe=i(da),cU=n(da,"P",{});var pkr=s(cU);xUe=r(pkr,"Examples:"),pkr.forEach(t),RUe=i(da),m(FM.$$.fragment,da),da.forEach(t),SUe=i($s),Wg=n($s,"DIV",{class:!0});var CBe=s(Wg);m(CM.$$.fragment,CBe),PUe=i(CBe),fU=n(CBe,"P",{});var _kr=s(fU);$Ue=r(_kr,"Register a new tokenizer in this mapping."),_kr.forEach(t),CBe.forEach(t),$s.forEach(t),T7e=i(d),ji=n(d,"H2",{class:!0});var MBe=s(ji);Qg=n(MBe,"A",{id:!0,class:!0,href:!0});var ukr=s(Qg);mU=n(ukr,"SPAN",{});var bkr=s(mU);m(MM.$$.fragment,bkr),bkr.forEach(t),ukr.forEach(t),IUe=i(MBe),gU=n(MBe,"SPAN",{});var vkr=s(gU);jUe=r(vkr,"AutoFeatureExtractor"),vkr.forEach(t),MBe.forEach(t),F7e=i(d),Xo=n(d,"DIV",{class:!0});var Is=s(Xo);m(EM.$$.fragment,Is),NUe=i(Is),yM=n(Is,"P",{});var EBe=s(yM);DUe=r(EBe,`This is a generic feature extractor class that will be instantiated as one of the feature extractor classes of the
library when created with the `),Vk=n(EBe,"A",{href:!0});var Tkr=s(Vk);qUe=r(Tkr,"AutoFeatureExtractor.from_pretrained()"),Tkr.forEach(t),GUe=r(EBe," class method."),EBe.forEach(t),OUe=i(Is),wM=n(Is,"P",{});var yBe=s(wM);XUe=r(yBe,"This class cannot be instantiated directly using "),hU=n(yBe,"CODE",{});var Fkr=s(hU);zUe=r(Fkr,"__init__()"),Fkr.forEach(t),VUe=r(yBe," (throws an error)."),yBe.forEach(t),WUe=i(Is),Le=n(Is,"DIV",{class:!0});var xt=s(Le);m(AM.$$.fragment,xt),QUe=i(xt),pU=n(xt,"P",{});var Ckr=s(pU);HUe=r(Ckr,"Instantiate one of the feature extractor classes of the library from a pretrained model vocabulary."),Ckr.forEach(t),UUe=i(xt),Na=n(xt,"P",{});var i4=s(Na);JUe=r(i4,"The feature extractor class to instantiate is selected based on the "),_U=n(i4,"CODE",{});var Mkr=s(_U);YUe=r(Mkr,"model_type"),Mkr.forEach(t),KUe=r(i4,` property of the config object
(either passed as an argument or loaded from `),uU=n(i4,"CODE",{});var Ekr=s(uU);ZUe=r(Ekr,"pretrained_model_name_or_path"),Ekr.forEach(t),eJe=r(i4,` if possible), or when it\u2019s
missing, by falling back to using pattern matching on `),bU=n(i4,"CODE",{});var ykr=s(bU);oJe=r(ykr,"pretrained_model_name_or_path"),ykr.forEach(t),rJe=r(i4,":"),i4.forEach(t),tJe=i(xt),se=n(xt,"UL",{});var de=s(se);Hg=n(de,"LI",{});var Y6e=s(Hg);vU=n(Y6e,"STRONG",{});var wkr=s(vU);aJe=r(wkr,"beit"),wkr.forEach(t),nJe=r(Y6e," \u2014 "),Wk=n(Y6e,"A",{href:!0});var Akr=s(Wk);sJe=r(Akr,"BeitFeatureExtractor"),Akr.forEach(t),lJe=r(Y6e," (BEiT model)"),Y6e.forEach(t),iJe=i(de),Ug=n(de,"LI",{});var K6e=s(Ug);TU=n(K6e,"STRONG",{});var Lkr=s(TU);dJe=r(Lkr,"clip"),Lkr.forEach(t),cJe=r(K6e," \u2014 "),Qk=n(K6e,"A",{href:!0});var Bkr=s(Qk);fJe=r(Bkr,"CLIPFeatureExtractor"),Bkr.forEach(t),mJe=r(K6e," (CLIP model)"),K6e.forEach(t),gJe=i(de),Jg=n(de,"LI",{});var Z6e=s(Jg);FU=n(Z6e,"STRONG",{});var kkr=s(FU);hJe=r(kkr,"convnext"),kkr.forEach(t),pJe=r(Z6e," \u2014 "),Hk=n(Z6e,"A",{href:!0});var xkr=s(Hk);_Je=r(xkr,"ConvNextFeatureExtractor"),xkr.forEach(t),uJe=r(Z6e," (ConvNext model)"),Z6e.forEach(t),bJe=i(de),Yg=n(de,"LI",{});var eTe=s(Yg);CU=n(eTe,"STRONG",{});var Rkr=s(CU);vJe=r(Rkr,"deit"),Rkr.forEach(t),TJe=r(eTe," \u2014 "),Uk=n(eTe,"A",{href:!0});var Skr=s(Uk);FJe=r(Skr,"DeiTFeatureExtractor"),Skr.forEach(t),CJe=r(eTe," (DeiT model)"),eTe.forEach(t),MJe=i(de),Kg=n(de,"LI",{});var oTe=s(Kg);MU=n(oTe,"STRONG",{});var Pkr=s(MU);EJe=r(Pkr,"detr"),Pkr.forEach(t),yJe=r(oTe," \u2014 "),Jk=n(oTe,"A",{href:!0});var $kr=s(Jk);wJe=r($kr,"DetrFeatureExtractor"),$kr.forEach(t),AJe=r(oTe," (DETR model)"),oTe.forEach(t),LJe=i(de),Zg=n(de,"LI",{});var rTe=s(Zg);EU=n(rTe,"STRONG",{});var Ikr=s(EU);BJe=r(Ikr,"hubert"),Ikr.forEach(t),kJe=r(rTe," \u2014 "),Yk=n(rTe,"A",{href:!0});var jkr=s(Yk);xJe=r(jkr,"Wav2Vec2FeatureExtractor"),jkr.forEach(t),RJe=r(rTe," (Hubert model)"),rTe.forEach(t),SJe=i(de),eh=n(de,"LI",{});var tTe=s(eh);yU=n(tTe,"STRONG",{});var Nkr=s(yU);PJe=r(Nkr,"layoutlmv2"),Nkr.forEach(t),$Je=r(tTe," \u2014 "),Kk=n(tTe,"A",{href:!0});var Dkr=s(Kk);IJe=r(Dkr,"LayoutLMv2FeatureExtractor"),Dkr.forEach(t),jJe=r(tTe," (LayoutLMv2 model)"),tTe.forEach(t),NJe=i(de),oh=n(de,"LI",{});var aTe=s(oh);wU=n(aTe,"STRONG",{});var qkr=s(wU);DJe=r(qkr,"perceiver"),qkr.forEach(t),qJe=r(aTe," \u2014 "),Zk=n(aTe,"A",{href:!0});var Gkr=s(Zk);GJe=r(Gkr,"PerceiverFeatureExtractor"),Gkr.forEach(t),OJe=r(aTe," (Perceiver model)"),aTe.forEach(t),XJe=i(de),rh=n(de,"LI",{});var nTe=s(rh);AU=n(nTe,"STRONG",{});var Okr=s(AU);zJe=r(Okr,"poolformer"),Okr.forEach(t),VJe=r(nTe," \u2014 "),ex=n(nTe,"A",{href:!0});var Xkr=s(ex);WJe=r(Xkr,"PoolFormerFeatureExtractor"),Xkr.forEach(t),QJe=r(nTe," (PoolFormer model)"),nTe.forEach(t),HJe=i(de),th=n(de,"LI",{});var sTe=s(th);LU=n(sTe,"STRONG",{});var zkr=s(LU);UJe=r(zkr,"segformer"),zkr.forEach(t),JJe=r(sTe," \u2014 "),ox=n(sTe,"A",{href:!0});var Vkr=s(ox);YJe=r(Vkr,"SegformerFeatureExtractor"),Vkr.forEach(t),KJe=r(sTe," (SegFormer model)"),sTe.forEach(t),ZJe=i(de),ah=n(de,"LI",{});var lTe=s(ah);BU=n(lTe,"STRONG",{});var Wkr=s(BU);eYe=r(Wkr,"speech_to_text"),Wkr.forEach(t),oYe=r(lTe," \u2014 "),rx=n(lTe,"A",{href:!0});var Qkr=s(rx);rYe=r(Qkr,"Speech2TextFeatureExtractor"),Qkr.forEach(t),tYe=r(lTe," (Speech2Text model)"),lTe.forEach(t),aYe=i(de),nh=n(de,"LI",{});var iTe=s(nh);kU=n(iTe,"STRONG",{});var Hkr=s(kU);nYe=r(Hkr,"swin"),Hkr.forEach(t),sYe=r(iTe," \u2014 "),tx=n(iTe,"A",{href:!0});var Ukr=s(tx);lYe=r(Ukr,"ViTFeatureExtractor"),Ukr.forEach(t),iYe=r(iTe," (Swin model)"),iTe.forEach(t),dYe=i(de),sh=n(de,"LI",{});var dTe=s(sh);xU=n(dTe,"STRONG",{});var Jkr=s(xU);cYe=r(Jkr,"vit"),Jkr.forEach(t),fYe=r(dTe," \u2014 "),ax=n(dTe,"A",{href:!0});var Ykr=s(ax);mYe=r(Ykr,"ViTFeatureExtractor"),Ykr.forEach(t),gYe=r(dTe," (ViT model)"),dTe.forEach(t),hYe=i(de),lh=n(de,"LI",{});var cTe=s(lh);RU=n(cTe,"STRONG",{});var Kkr=s(RU);pYe=r(Kkr,"vit_mae"),Kkr.forEach(t),_Ye=r(cTe," \u2014 "),nx=n(cTe,"A",{href:!0});var Zkr=s(nx);uYe=r(Zkr,"ViTFeatureExtractor"),Zkr.forEach(t),bYe=r(cTe," (ViTMAE model)"),cTe.forEach(t),vYe=i(de),ih=n(de,"LI",{});var fTe=s(ih);SU=n(fTe,"STRONG",{});var exr=s(SU);TYe=r(exr,"wav2vec2"),exr.forEach(t),FYe=r(fTe," \u2014 "),sx=n(fTe,"A",{href:!0});var oxr=s(sx);CYe=r(oxr,"Wav2Vec2FeatureExtractor"),oxr.forEach(t),MYe=r(fTe," (Wav2Vec2 model)"),fTe.forEach(t),de.forEach(t),EYe=i(xt),m(dh.$$.fragment,xt),yYe=i(xt),PU=n(xt,"P",{});var rxr=s(PU);wYe=r(rxr,"Examples:"),rxr.forEach(t),AYe=i(xt),m(LM.$$.fragment,xt),xt.forEach(t),LYe=i(Is),ch=n(Is,"DIV",{class:!0});var wBe=s(ch);m(BM.$$.fragment,wBe),BYe=i(wBe),$U=n(wBe,"P",{});var txr=s($U);kYe=r(txr,"Register a new feature extractor for this class."),txr.forEach(t),wBe.forEach(t),Is.forEach(t),C7e=i(d),Ni=n(d,"H2",{class:!0});var ABe=s(Ni);fh=n(ABe,"A",{id:!0,class:!0,href:!0});var axr=s(fh);IU=n(axr,"SPAN",{});var nxr=s(IU);m(kM.$$.fragment,nxr),nxr.forEach(t),axr.forEach(t),xYe=i(ABe),jU=n(ABe,"SPAN",{});var sxr=s(jU);RYe=r(sxr,"AutoProcessor"),sxr.forEach(t),ABe.forEach(t),M7e=i(d),zo=n(d,"DIV",{class:!0});var js=s(zo);m(xM.$$.fragment,js),SYe=i(js),RM=n(js,"P",{});var LBe=s(RM);PYe=r(LBe,`This is a generic processor class that will be instantiated as one of the processor classes of the library when
created with the `),lx=n(LBe,"A",{href:!0});var lxr=s(lx);$Ye=r(lxr,"AutoProcessor.from_pretrained()"),lxr.forEach(t),IYe=r(LBe," class method."),LBe.forEach(t),jYe=i(js),SM=n(js,"P",{});var BBe=s(SM);NYe=r(BBe,"This class cannot be instantiated directly using "),NU=n(BBe,"CODE",{});var ixr=s(NU);DYe=r(ixr,"__init__()"),ixr.forEach(t),qYe=r(BBe," (throws an error)."),BBe.forEach(t),GYe=i(js),Be=n(js,"DIV",{class:!0});var Rt=s(Be);m(PM.$$.fragment,Rt),OYe=i(Rt),DU=n(Rt,"P",{});var dxr=s(DU);XYe=r(dxr,"Instantiate one of the processor classes of the library from a pretrained model vocabulary."),dxr.forEach(t),zYe=i(Rt),Di=n(Rt,"P",{});var UX=s(Di);VYe=r(UX,"The processor class to instantiate is selected based on the "),qU=n(UX,"CODE",{});var cxr=s(qU);WYe=r(cxr,"model_type"),cxr.forEach(t),QYe=r(UX,` property of the config object (either
passed as an argument or loaded from `),GU=n(UX,"CODE",{});var fxr=s(GU);HYe=r(fxr,"pretrained_model_name_or_path"),fxr.forEach(t),UYe=r(UX," if possible):"),UX.forEach(t),JYe=i(Rt),we=n(Rt,"UL",{});var No=s(we);mh=n(No,"LI",{});var mTe=s(mh);OU=n(mTe,"STRONG",{});var mxr=s(OU);YYe=r(mxr,"clip"),mxr.forEach(t),KYe=r(mTe," \u2014 "),ix=n(mTe,"A",{href:!0});var gxr=s(ix);ZYe=r(gxr,"CLIPProcessor"),gxr.forEach(t),eKe=r(mTe," (CLIP model)"),mTe.forEach(t),oKe=i(No),gh=n(No,"LI",{});var gTe=s(gh);XU=n(gTe,"STRONG",{});var hxr=s(XU);rKe=r(hxr,"layoutlmv2"),hxr.forEach(t),tKe=r(gTe," \u2014 "),dx=n(gTe,"A",{href:!0});var pxr=s(dx);aKe=r(pxr,"LayoutLMv2Processor"),pxr.forEach(t),nKe=r(gTe," (LayoutLMv2 model)"),gTe.forEach(t),sKe=i(No),hh=n(No,"LI",{});var hTe=s(hh);zU=n(hTe,"STRONG",{});var _xr=s(zU);lKe=r(_xr,"layoutxlm"),_xr.forEach(t),iKe=r(hTe," \u2014 "),cx=n(hTe,"A",{href:!0});var uxr=s(cx);dKe=r(uxr,"LayoutXLMProcessor"),uxr.forEach(t),cKe=r(hTe," (LayoutXLM model)"),hTe.forEach(t),fKe=i(No),ph=n(No,"LI",{});var pTe=s(ph);VU=n(pTe,"STRONG",{});var bxr=s(VU);mKe=r(bxr,"speech_to_text"),bxr.forEach(t),gKe=r(pTe," \u2014 "),fx=n(pTe,"A",{href:!0});var vxr=s(fx);hKe=r(vxr,"Speech2TextProcessor"),vxr.forEach(t),pKe=r(pTe," (Speech2Text model)"),pTe.forEach(t),_Ke=i(No),_h=n(No,"LI",{});var _Te=s(_h);WU=n(_Te,"STRONG",{});var Txr=s(WU);uKe=r(Txr,"speech_to_text_2"),Txr.forEach(t),bKe=r(_Te," \u2014 "),mx=n(_Te,"A",{href:!0});var Fxr=s(mx);vKe=r(Fxr,"Speech2Text2Processor"),Fxr.forEach(t),TKe=r(_Te," (Speech2Text2 model)"),_Te.forEach(t),FKe=i(No),uh=n(No,"LI",{});var uTe=s(uh);QU=n(uTe,"STRONG",{});var Cxr=s(QU);CKe=r(Cxr,"trocr"),Cxr.forEach(t),MKe=r(uTe," \u2014 "),gx=n(uTe,"A",{href:!0});var Mxr=s(gx);EKe=r(Mxr,"TrOCRProcessor"),Mxr.forEach(t),yKe=r(uTe," (TrOCR model)"),uTe.forEach(t),wKe=i(No),bh=n(No,"LI",{});var bTe=s(bh);HU=n(bTe,"STRONG",{});var Exr=s(HU);AKe=r(Exr,"vision-text-dual-encoder"),Exr.forEach(t),LKe=r(bTe," \u2014 "),hx=n(bTe,"A",{href:!0});var yxr=s(hx);BKe=r(yxr,"VisionTextDualEncoderProcessor"),yxr.forEach(t),kKe=r(bTe," (VisionTextDualEncoder model)"),bTe.forEach(t),xKe=i(No),vh=n(No,"LI",{});var vTe=s(vh);UU=n(vTe,"STRONG",{});var wxr=s(UU);RKe=r(wxr,"wav2vec2"),wxr.forEach(t),SKe=r(vTe," \u2014 "),px=n(vTe,"A",{href:!0});var Axr=s(px);PKe=r(Axr,"Wav2Vec2Processor"),Axr.forEach(t),$Ke=r(vTe," (Wav2Vec2 model)"),vTe.forEach(t),No.forEach(t),IKe=i(Rt),m(Th.$$.fragment,Rt),jKe=i(Rt),JU=n(Rt,"P",{});var Lxr=s(JU);NKe=r(Lxr,"Examples:"),Lxr.forEach(t),DKe=i(Rt),m($M.$$.fragment,Rt),Rt.forEach(t),qKe=i(js),Fh=n(js,"DIV",{class:!0});var kBe=s(Fh);m(IM.$$.fragment,kBe),GKe=i(kBe),YU=n(kBe,"P",{});var Bxr=s(YU);OKe=r(Bxr,"Register a new processor for this class."),Bxr.forEach(t),kBe.forEach(t),js.forEach(t),E7e=i(d),qi=n(d,"H2",{class:!0});var xBe=s(qi);Ch=n(xBe,"A",{id:!0,class:!0,href:!0});var kxr=s(Ch);KU=n(kxr,"SPAN",{});var xxr=s(KU);m(jM.$$.fragment,xxr),xxr.forEach(t),kxr.forEach(t),XKe=i(xBe),ZU=n(xBe,"SPAN",{});var Rxr=s(ZU);zKe=r(Rxr,"AutoModel"),Rxr.forEach(t),xBe.forEach(t),y7e=i(d),Vo=n(d,"DIV",{class:!0});var Ns=s(Vo);m(NM.$$.fragment,Ns),VKe=i(Ns),Gi=n(Ns,"P",{});var JX=s(Gi);WKe=r(JX,`This is a generic model class that will be instantiated as one of the base model classes of the library when created
with the `),eJ=n(JX,"CODE",{});var Sxr=s(eJ);QKe=r(Sxr,"from_pretrained()"),Sxr.forEach(t),HKe=r(JX,"class method or the "),oJ=n(JX,"CODE",{});var Pxr=s(oJ);UKe=r(Pxr,"from_config()"),Pxr.forEach(t),JKe=r(JX,`class
method.`),JX.forEach(t),YKe=i(Ns),DM=n(Ns,"P",{});var RBe=s(DM);KKe=r(RBe,"This class cannot be instantiated directly using "),rJ=n(RBe,"CODE",{});var $xr=s(rJ);ZKe=r($xr,"__init__()"),$xr.forEach(t),eZe=r(RBe," (throws an error)."),RBe.forEach(t),oZe=i(Ns),Nr=n(Ns,"DIV",{class:!0});var Ds=s(Nr);m(qM.$$.fragment,Ds),rZe=i(Ds),tJ=n(Ds,"P",{});var Ixr=s(tJ);tZe=r(Ixr,"Instantiates one of the base model classes of the library from a configuration."),Ixr.forEach(t),aZe=i(Ds),Oi=n(Ds,"P",{});var YX=s(Oi);nZe=r(YX,`Note:
Loading a model from its configuration file does `),aJ=n(YX,"STRONG",{});var jxr=s(aJ);sZe=r(jxr,"not"),jxr.forEach(t),lZe=r(YX,` load the model weights. It only affects the
model\u2019s configuration. Use `),nJ=n(YX,"CODE",{});var Nxr=s(nJ);iZe=r(Nxr,"from_pretrained()"),Nxr.forEach(t),dZe=r(YX,"to load the model weights."),YX.forEach(t),cZe=i(Ds),sJ=n(Ds,"P",{});var Dxr=s(sJ);fZe=r(Dxr,"Examples:"),Dxr.forEach(t),mZe=i(Ds),m(GM.$$.fragment,Ds),Ds.forEach(t),gZe=i(Ns),ke=n(Ns,"DIV",{class:!0});var St=s(ke);m(OM.$$.fragment,St),hZe=i(St),lJ=n(St,"P",{});var qxr=s(lJ);pZe=r(qxr,"Instantiate one of the base model classes of the library from a pretrained model."),qxr.forEach(t),_Ze=i(St),Da=n(St,"P",{});var d4=s(Da);uZe=r(d4,"The model class to instantiate is selected based on the "),iJ=n(d4,"CODE",{});var Gxr=s(iJ);bZe=r(Gxr,"model_type"),Gxr.forEach(t),vZe=r(d4,` property of the config object (either
passed as an argument or loaded from `),dJ=n(d4,"CODE",{});var Oxr=s(dJ);TZe=r(Oxr,"pretrained_model_name_or_path"),Oxr.forEach(t),FZe=r(d4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),cJ=n(d4,"CODE",{});var Xxr=s(cJ);CZe=r(Xxr,"pretrained_model_name_or_path"),Xxr.forEach(t),MZe=r(d4,":"),d4.forEach(t),EZe=i(St),F=n(St,"UL",{});var C=s(F);Mh=n(C,"LI",{});var TTe=s(Mh);fJ=n(TTe,"STRONG",{});var zxr=s(fJ);yZe=r(zxr,"albert"),zxr.forEach(t),wZe=r(TTe," \u2014 "),_x=n(TTe,"A",{href:!0});var Vxr=s(_x);AZe=r(Vxr,"AlbertModel"),Vxr.forEach(t),LZe=r(TTe," (ALBERT model)"),TTe.forEach(t),BZe=i(C),Eh=n(C,"LI",{});var FTe=s(Eh);mJ=n(FTe,"STRONG",{});var Wxr=s(mJ);kZe=r(Wxr,"bart"),Wxr.forEach(t),xZe=r(FTe," \u2014 "),ux=n(FTe,"A",{href:!0});var Qxr=s(ux);RZe=r(Qxr,"BartModel"),Qxr.forEach(t),SZe=r(FTe," (BART model)"),FTe.forEach(t),PZe=i(C),yh=n(C,"LI",{});var CTe=s(yh);gJ=n(CTe,"STRONG",{});var Hxr=s(gJ);$Ze=r(Hxr,"beit"),Hxr.forEach(t),IZe=r(CTe," \u2014 "),bx=n(CTe,"A",{href:!0});var Uxr=s(bx);jZe=r(Uxr,"BeitModel"),Uxr.forEach(t),NZe=r(CTe," (BEiT model)"),CTe.forEach(t),DZe=i(C),wh=n(C,"LI",{});var MTe=s(wh);hJ=n(MTe,"STRONG",{});var Jxr=s(hJ);qZe=r(Jxr,"bert"),Jxr.forEach(t),GZe=r(MTe," \u2014 "),vx=n(MTe,"A",{href:!0});var Yxr=s(vx);OZe=r(Yxr,"BertModel"),Yxr.forEach(t),XZe=r(MTe," (BERT model)"),MTe.forEach(t),zZe=i(C),Ah=n(C,"LI",{});var ETe=s(Ah);pJ=n(ETe,"STRONG",{});var Kxr=s(pJ);VZe=r(Kxr,"bert-generation"),Kxr.forEach(t),WZe=r(ETe," \u2014 "),Tx=n(ETe,"A",{href:!0});var Zxr=s(Tx);QZe=r(Zxr,"BertGenerationEncoder"),Zxr.forEach(t),HZe=r(ETe," (Bert Generation model)"),ETe.forEach(t),UZe=i(C),Lh=n(C,"LI",{});var yTe=s(Lh);_J=n(yTe,"STRONG",{});var eRr=s(_J);JZe=r(eRr,"big_bird"),eRr.forEach(t),YZe=r(yTe," \u2014 "),Fx=n(yTe,"A",{href:!0});var oRr=s(Fx);KZe=r(oRr,"BigBirdModel"),oRr.forEach(t),ZZe=r(yTe," (BigBird model)"),yTe.forEach(t),eeo=i(C),Bh=n(C,"LI",{});var wTe=s(Bh);uJ=n(wTe,"STRONG",{});var rRr=s(uJ);oeo=r(rRr,"bigbird_pegasus"),rRr.forEach(t),reo=r(wTe," \u2014 "),Cx=n(wTe,"A",{href:!0});var tRr=s(Cx);teo=r(tRr,"BigBirdPegasusModel"),tRr.forEach(t),aeo=r(wTe," (BigBirdPegasus model)"),wTe.forEach(t),neo=i(C),kh=n(C,"LI",{});var ATe=s(kh);bJ=n(ATe,"STRONG",{});var aRr=s(bJ);seo=r(aRr,"blenderbot"),aRr.forEach(t),leo=r(ATe," \u2014 "),Mx=n(ATe,"A",{href:!0});var nRr=s(Mx);ieo=r(nRr,"BlenderbotModel"),nRr.forEach(t),deo=r(ATe," (Blenderbot model)"),ATe.forEach(t),ceo=i(C),xh=n(C,"LI",{});var LTe=s(xh);vJ=n(LTe,"STRONG",{});var sRr=s(vJ);feo=r(sRr,"blenderbot-small"),sRr.forEach(t),meo=r(LTe," \u2014 "),Ex=n(LTe,"A",{href:!0});var lRr=s(Ex);geo=r(lRr,"BlenderbotSmallModel"),lRr.forEach(t),heo=r(LTe," (BlenderbotSmall model)"),LTe.forEach(t),peo=i(C),Rh=n(C,"LI",{});var BTe=s(Rh);TJ=n(BTe,"STRONG",{});var iRr=s(TJ);_eo=r(iRr,"camembert"),iRr.forEach(t),ueo=r(BTe," \u2014 "),yx=n(BTe,"A",{href:!0});var dRr=s(yx);beo=r(dRr,"CamembertModel"),dRr.forEach(t),veo=r(BTe," (CamemBERT model)"),BTe.forEach(t),Teo=i(C),Sh=n(C,"LI",{});var kTe=s(Sh);FJ=n(kTe,"STRONG",{});var cRr=s(FJ);Feo=r(cRr,"canine"),cRr.forEach(t),Ceo=r(kTe," \u2014 "),wx=n(kTe,"A",{href:!0});var fRr=s(wx);Meo=r(fRr,"CanineModel"),fRr.forEach(t),Eeo=r(kTe," (Canine model)"),kTe.forEach(t),yeo=i(C),Ph=n(C,"LI",{});var xTe=s(Ph);CJ=n(xTe,"STRONG",{});var mRr=s(CJ);weo=r(mRr,"clip"),mRr.forEach(t),Aeo=r(xTe," \u2014 "),Ax=n(xTe,"A",{href:!0});var gRr=s(Ax);Leo=r(gRr,"CLIPModel"),gRr.forEach(t),Beo=r(xTe," (CLIP model)"),xTe.forEach(t),keo=i(C),$h=n(C,"LI",{});var RTe=s($h);MJ=n(RTe,"STRONG",{});var hRr=s(MJ);xeo=r(hRr,"convbert"),hRr.forEach(t),Reo=r(RTe," \u2014 "),Lx=n(RTe,"A",{href:!0});var pRr=s(Lx);Seo=r(pRr,"ConvBertModel"),pRr.forEach(t),Peo=r(RTe," (ConvBERT model)"),RTe.forEach(t),$eo=i(C),Ih=n(C,"LI",{});var STe=s(Ih);EJ=n(STe,"STRONG",{});var _Rr=s(EJ);Ieo=r(_Rr,"convnext"),_Rr.forEach(t),jeo=r(STe," \u2014 "),Bx=n(STe,"A",{href:!0});var uRr=s(Bx);Neo=r(uRr,"ConvNextModel"),uRr.forEach(t),Deo=r(STe," (ConvNext model)"),STe.forEach(t),qeo=i(C),jh=n(C,"LI",{});var PTe=s(jh);yJ=n(PTe,"STRONG",{});var bRr=s(yJ);Geo=r(bRr,"ctrl"),bRr.forEach(t),Oeo=r(PTe," \u2014 "),kx=n(PTe,"A",{href:!0});var vRr=s(kx);Xeo=r(vRr,"CTRLModel"),vRr.forEach(t),zeo=r(PTe," (CTRL model)"),PTe.forEach(t),Veo=i(C),Nh=n(C,"LI",{});var $Te=s(Nh);wJ=n($Te,"STRONG",{});var TRr=s(wJ);Weo=r(TRr,"deberta"),TRr.forEach(t),Qeo=r($Te," \u2014 "),xx=n($Te,"A",{href:!0});var FRr=s(xx);Heo=r(FRr,"DebertaModel"),FRr.forEach(t),Ueo=r($Te," (DeBERTa model)"),$Te.forEach(t),Jeo=i(C),Dh=n(C,"LI",{});var ITe=s(Dh);AJ=n(ITe,"STRONG",{});var CRr=s(AJ);Yeo=r(CRr,"deberta-v2"),CRr.forEach(t),Keo=r(ITe," \u2014 "),Rx=n(ITe,"A",{href:!0});var MRr=s(Rx);Zeo=r(MRr,"DebertaV2Model"),MRr.forEach(t),eoo=r(ITe," (DeBERTa-v2 model)"),ITe.forEach(t),ooo=i(C),qh=n(C,"LI",{});var jTe=s(qh);LJ=n(jTe,"STRONG",{});var ERr=s(LJ);roo=r(ERr,"deit"),ERr.forEach(t),too=r(jTe," \u2014 "),Sx=n(jTe,"A",{href:!0});var yRr=s(Sx);aoo=r(yRr,"DeiTModel"),yRr.forEach(t),noo=r(jTe," (DeiT model)"),jTe.forEach(t),soo=i(C),Gh=n(C,"LI",{});var NTe=s(Gh);BJ=n(NTe,"STRONG",{});var wRr=s(BJ);loo=r(wRr,"detr"),wRr.forEach(t),ioo=r(NTe," \u2014 "),Px=n(NTe,"A",{href:!0});var ARr=s(Px);doo=r(ARr,"DetrModel"),ARr.forEach(t),coo=r(NTe," (DETR model)"),NTe.forEach(t),foo=i(C),Oh=n(C,"LI",{});var DTe=s(Oh);kJ=n(DTe,"STRONG",{});var LRr=s(kJ);moo=r(LRr,"distilbert"),LRr.forEach(t),goo=r(DTe," \u2014 "),$x=n(DTe,"A",{href:!0});var BRr=s($x);hoo=r(BRr,"DistilBertModel"),BRr.forEach(t),poo=r(DTe," (DistilBERT model)"),DTe.forEach(t),_oo=i(C),Xh=n(C,"LI",{});var qTe=s(Xh);xJ=n(qTe,"STRONG",{});var kRr=s(xJ);uoo=r(kRr,"dpr"),kRr.forEach(t),boo=r(qTe," \u2014 "),Ix=n(qTe,"A",{href:!0});var xRr=s(Ix);voo=r(xRr,"DPRQuestionEncoder"),xRr.forEach(t),Too=r(qTe," (DPR model)"),qTe.forEach(t),Foo=i(C),zh=n(C,"LI",{});var GTe=s(zh);RJ=n(GTe,"STRONG",{});var RRr=s(RJ);Coo=r(RRr,"electra"),RRr.forEach(t),Moo=r(GTe," \u2014 "),jx=n(GTe,"A",{href:!0});var SRr=s(jx);Eoo=r(SRr,"ElectraModel"),SRr.forEach(t),yoo=r(GTe," (ELECTRA model)"),GTe.forEach(t),woo=i(C),Vh=n(C,"LI",{});var OTe=s(Vh);SJ=n(OTe,"STRONG",{});var PRr=s(SJ);Aoo=r(PRr,"flaubert"),PRr.forEach(t),Loo=r(OTe," \u2014 "),Nx=n(OTe,"A",{href:!0});var $Rr=s(Nx);Boo=r($Rr,"FlaubertModel"),$Rr.forEach(t),koo=r(OTe," (FlauBERT model)"),OTe.forEach(t),xoo=i(C),Wh=n(C,"LI",{});var XTe=s(Wh);PJ=n(XTe,"STRONG",{});var IRr=s(PJ);Roo=r(IRr,"fnet"),IRr.forEach(t),Soo=r(XTe," \u2014 "),Dx=n(XTe,"A",{href:!0});var jRr=s(Dx);Poo=r(jRr,"FNetModel"),jRr.forEach(t),$oo=r(XTe," (FNet model)"),XTe.forEach(t),Ioo=i(C),Qh=n(C,"LI",{});var zTe=s(Qh);$J=n(zTe,"STRONG",{});var NRr=s($J);joo=r(NRr,"fsmt"),NRr.forEach(t),Noo=r(zTe," \u2014 "),qx=n(zTe,"A",{href:!0});var DRr=s(qx);Doo=r(DRr,"FSMTModel"),DRr.forEach(t),qoo=r(zTe," (FairSeq Machine-Translation model)"),zTe.forEach(t),Goo=i(C),xs=n(C,"LI",{});var NL=s(xs);IJ=n(NL,"STRONG",{});var qRr=s(IJ);Ooo=r(qRr,"funnel"),qRr.forEach(t),Xoo=r(NL," \u2014 "),Gx=n(NL,"A",{href:!0});var GRr=s(Gx);zoo=r(GRr,"FunnelModel"),GRr.forEach(t),Voo=r(NL," or "),Ox=n(NL,"A",{href:!0});var ORr=s(Ox);Woo=r(ORr,"FunnelBaseModel"),ORr.forEach(t),Qoo=r(NL," (Funnel Transformer model)"),NL.forEach(t),Hoo=i(C),Hh=n(C,"LI",{});var VTe=s(Hh);jJ=n(VTe,"STRONG",{});var XRr=s(jJ);Uoo=r(XRr,"gpt2"),XRr.forEach(t),Joo=r(VTe," \u2014 "),Xx=n(VTe,"A",{href:!0});var zRr=s(Xx);Yoo=r(zRr,"GPT2Model"),zRr.forEach(t),Koo=r(VTe," (OpenAI GPT-2 model)"),VTe.forEach(t),Zoo=i(C),Uh=n(C,"LI",{});var WTe=s(Uh);NJ=n(WTe,"STRONG",{});var VRr=s(NJ);ero=r(VRr,"gpt_neo"),VRr.forEach(t),oro=r(WTe," \u2014 "),zx=n(WTe,"A",{href:!0});var WRr=s(zx);rro=r(WRr,"GPTNeoModel"),WRr.forEach(t),tro=r(WTe," (GPT Neo model)"),WTe.forEach(t),aro=i(C),Jh=n(C,"LI",{});var QTe=s(Jh);DJ=n(QTe,"STRONG",{});var QRr=s(DJ);nro=r(QRr,"gptj"),QRr.forEach(t),sro=r(QTe," \u2014 "),Vx=n(QTe,"A",{href:!0});var HRr=s(Vx);lro=r(HRr,"GPTJModel"),HRr.forEach(t),iro=r(QTe," (GPT-J model)"),QTe.forEach(t),dro=i(C),Yh=n(C,"LI",{});var HTe=s(Yh);qJ=n(HTe,"STRONG",{});var URr=s(qJ);cro=r(URr,"hubert"),URr.forEach(t),fro=r(HTe," \u2014 "),Wx=n(HTe,"A",{href:!0});var JRr=s(Wx);mro=r(JRr,"HubertModel"),JRr.forEach(t),gro=r(HTe," (Hubert model)"),HTe.forEach(t),hro=i(C),Kh=n(C,"LI",{});var UTe=s(Kh);GJ=n(UTe,"STRONG",{});var YRr=s(GJ);pro=r(YRr,"ibert"),YRr.forEach(t),_ro=r(UTe," \u2014 "),Qx=n(UTe,"A",{href:!0});var KRr=s(Qx);uro=r(KRr,"IBertModel"),KRr.forEach(t),bro=r(UTe," (I-BERT model)"),UTe.forEach(t),vro=i(C),Zh=n(C,"LI",{});var JTe=s(Zh);OJ=n(JTe,"STRONG",{});var ZRr=s(OJ);Tro=r(ZRr,"imagegpt"),ZRr.forEach(t),Fro=r(JTe," \u2014 "),Hx=n(JTe,"A",{href:!0});var eSr=s(Hx);Cro=r(eSr,"ImageGPTModel"),eSr.forEach(t),Mro=r(JTe," (ImageGPT model)"),JTe.forEach(t),Ero=i(C),ep=n(C,"LI",{});var YTe=s(ep);XJ=n(YTe,"STRONG",{});var oSr=s(XJ);yro=r(oSr,"layoutlm"),oSr.forEach(t),wro=r(YTe," \u2014 "),Ux=n(YTe,"A",{href:!0});var rSr=s(Ux);Aro=r(rSr,"LayoutLMModel"),rSr.forEach(t),Lro=r(YTe," (LayoutLM model)"),YTe.forEach(t),Bro=i(C),op=n(C,"LI",{});var KTe=s(op);zJ=n(KTe,"STRONG",{});var tSr=s(zJ);kro=r(tSr,"layoutlmv2"),tSr.forEach(t),xro=r(KTe," \u2014 "),Jx=n(KTe,"A",{href:!0});var aSr=s(Jx);Rro=r(aSr,"LayoutLMv2Model"),aSr.forEach(t),Sro=r(KTe," (LayoutLMv2 model)"),KTe.forEach(t),Pro=i(C),rp=n(C,"LI",{});var ZTe=s(rp);VJ=n(ZTe,"STRONG",{});var nSr=s(VJ);$ro=r(nSr,"led"),nSr.forEach(t),Iro=r(ZTe," \u2014 "),Yx=n(ZTe,"A",{href:!0});var sSr=s(Yx);jro=r(sSr,"LEDModel"),sSr.forEach(t),Nro=r(ZTe," (LED model)"),ZTe.forEach(t),Dro=i(C),tp=n(C,"LI",{});var e8e=s(tp);WJ=n(e8e,"STRONG",{});var lSr=s(WJ);qro=r(lSr,"longformer"),lSr.forEach(t),Gro=r(e8e," \u2014 "),Kx=n(e8e,"A",{href:!0});var iSr=s(Kx);Oro=r(iSr,"LongformerModel"),iSr.forEach(t),Xro=r(e8e," (Longformer model)"),e8e.forEach(t),zro=i(C),ap=n(C,"LI",{});var o8e=s(ap);QJ=n(o8e,"STRONG",{});var dSr=s(QJ);Vro=r(dSr,"luke"),dSr.forEach(t),Wro=r(o8e," \u2014 "),Zx=n(o8e,"A",{href:!0});var cSr=s(Zx);Qro=r(cSr,"LukeModel"),cSr.forEach(t),Hro=r(o8e," (LUKE model)"),o8e.forEach(t),Uro=i(C),np=n(C,"LI",{});var r8e=s(np);HJ=n(r8e,"STRONG",{});var fSr=s(HJ);Jro=r(fSr,"lxmert"),fSr.forEach(t),Yro=r(r8e," \u2014 "),eR=n(r8e,"A",{href:!0});var mSr=s(eR);Kro=r(mSr,"LxmertModel"),mSr.forEach(t),Zro=r(r8e," (LXMERT model)"),r8e.forEach(t),eto=i(C),sp=n(C,"LI",{});var t8e=s(sp);UJ=n(t8e,"STRONG",{});var gSr=s(UJ);oto=r(gSr,"m2m_100"),gSr.forEach(t),rto=r(t8e," \u2014 "),oR=n(t8e,"A",{href:!0});var hSr=s(oR);tto=r(hSr,"M2M100Model"),hSr.forEach(t),ato=r(t8e," (M2M100 model)"),t8e.forEach(t),nto=i(C),lp=n(C,"LI",{});var a8e=s(lp);JJ=n(a8e,"STRONG",{});var pSr=s(JJ);sto=r(pSr,"marian"),pSr.forEach(t),lto=r(a8e," \u2014 "),rR=n(a8e,"A",{href:!0});var _Sr=s(rR);ito=r(_Sr,"MarianModel"),_Sr.forEach(t),dto=r(a8e," (Marian model)"),a8e.forEach(t),cto=i(C),ip=n(C,"LI",{});var n8e=s(ip);YJ=n(n8e,"STRONG",{});var uSr=s(YJ);fto=r(uSr,"maskformer"),uSr.forEach(t),mto=r(n8e," \u2014 "),tR=n(n8e,"A",{href:!0});var bSr=s(tR);gto=r(bSr,"MaskFormerModel"),bSr.forEach(t),hto=r(n8e," (MaskFormer model)"),n8e.forEach(t),pto=i(C),dp=n(C,"LI",{});var s8e=s(dp);KJ=n(s8e,"STRONG",{});var vSr=s(KJ);_to=r(vSr,"mbart"),vSr.forEach(t),uto=r(s8e," \u2014 "),aR=n(s8e,"A",{href:!0});var TSr=s(aR);bto=r(TSr,"MBartModel"),TSr.forEach(t),vto=r(s8e," (mBART model)"),s8e.forEach(t),Tto=i(C),cp=n(C,"LI",{});var l8e=s(cp);ZJ=n(l8e,"STRONG",{});var FSr=s(ZJ);Fto=r(FSr,"megatron-bert"),FSr.forEach(t),Cto=r(l8e," \u2014 "),nR=n(l8e,"A",{href:!0});var CSr=s(nR);Mto=r(CSr,"MegatronBertModel"),CSr.forEach(t),Eto=r(l8e," (MegatronBert model)"),l8e.forEach(t),yto=i(C),fp=n(C,"LI",{});var i8e=s(fp);eY=n(i8e,"STRONG",{});var MSr=s(eY);wto=r(MSr,"mobilebert"),MSr.forEach(t),Ato=r(i8e," \u2014 "),sR=n(i8e,"A",{href:!0});var ESr=s(sR);Lto=r(ESr,"MobileBertModel"),ESr.forEach(t),Bto=r(i8e," (MobileBERT model)"),i8e.forEach(t),kto=i(C),mp=n(C,"LI",{});var d8e=s(mp);oY=n(d8e,"STRONG",{});var ySr=s(oY);xto=r(ySr,"mpnet"),ySr.forEach(t),Rto=r(d8e," \u2014 "),lR=n(d8e,"A",{href:!0});var wSr=s(lR);Sto=r(wSr,"MPNetModel"),wSr.forEach(t),Pto=r(d8e," (MPNet model)"),d8e.forEach(t),$to=i(C),gp=n(C,"LI",{});var c8e=s(gp);rY=n(c8e,"STRONG",{});var ASr=s(rY);Ito=r(ASr,"mt5"),ASr.forEach(t),jto=r(c8e," \u2014 "),iR=n(c8e,"A",{href:!0});var LSr=s(iR);Nto=r(LSr,"MT5Model"),LSr.forEach(t),Dto=r(c8e," (mT5 model)"),c8e.forEach(t),qto=i(C),hp=n(C,"LI",{});var f8e=s(hp);tY=n(f8e,"STRONG",{});var BSr=s(tY);Gto=r(BSr,"nystromformer"),BSr.forEach(t),Oto=r(f8e," \u2014 "),dR=n(f8e,"A",{href:!0});var kSr=s(dR);Xto=r(kSr,"NystromformerModel"),kSr.forEach(t),zto=r(f8e," (Nystromformer model)"),f8e.forEach(t),Vto=i(C),pp=n(C,"LI",{});var m8e=s(pp);aY=n(m8e,"STRONG",{});var xSr=s(aY);Wto=r(xSr,"openai-gpt"),xSr.forEach(t),Qto=r(m8e," \u2014 "),cR=n(m8e,"A",{href:!0});var RSr=s(cR);Hto=r(RSr,"OpenAIGPTModel"),RSr.forEach(t),Uto=r(m8e," (OpenAI GPT model)"),m8e.forEach(t),Jto=i(C),_p=n(C,"LI",{});var g8e=s(_p);nY=n(g8e,"STRONG",{});var SSr=s(nY);Yto=r(SSr,"pegasus"),SSr.forEach(t),Kto=r(g8e," \u2014 "),fR=n(g8e,"A",{href:!0});var PSr=s(fR);Zto=r(PSr,"PegasusModel"),PSr.forEach(t),eao=r(g8e," (Pegasus model)"),g8e.forEach(t),oao=i(C),up=n(C,"LI",{});var h8e=s(up);sY=n(h8e,"STRONG",{});var $Sr=s(sY);rao=r($Sr,"perceiver"),$Sr.forEach(t),tao=r(h8e," \u2014 "),mR=n(h8e,"A",{href:!0});var ISr=s(mR);aao=r(ISr,"PerceiverModel"),ISr.forEach(t),nao=r(h8e," (Perceiver model)"),h8e.forEach(t),sao=i(C),bp=n(C,"LI",{});var p8e=s(bp);lY=n(p8e,"STRONG",{});var jSr=s(lY);lao=r(jSr,"plbart"),jSr.forEach(t),iao=r(p8e," \u2014 "),gR=n(p8e,"A",{href:!0});var NSr=s(gR);dao=r(NSr,"PLBartModel"),NSr.forEach(t),cao=r(p8e," (PLBart model)"),p8e.forEach(t),fao=i(C),vp=n(C,"LI",{});var _8e=s(vp);iY=n(_8e,"STRONG",{});var DSr=s(iY);mao=r(DSr,"poolformer"),DSr.forEach(t),gao=r(_8e," \u2014 "),hR=n(_8e,"A",{href:!0});var qSr=s(hR);hao=r(qSr,"PoolFormerModel"),qSr.forEach(t),pao=r(_8e," (PoolFormer model)"),_8e.forEach(t),_ao=i(C),Tp=n(C,"LI",{});var u8e=s(Tp);dY=n(u8e,"STRONG",{});var GSr=s(dY);uao=r(GSr,"prophetnet"),GSr.forEach(t),bao=r(u8e," \u2014 "),pR=n(u8e,"A",{href:!0});var OSr=s(pR);vao=r(OSr,"ProphetNetModel"),OSr.forEach(t),Tao=r(u8e," (ProphetNet model)"),u8e.forEach(t),Fao=i(C),Fp=n(C,"LI",{});var b8e=s(Fp);cY=n(b8e,"STRONG",{});var XSr=s(cY);Cao=r(XSr,"qdqbert"),XSr.forEach(t),Mao=r(b8e," \u2014 "),_R=n(b8e,"A",{href:!0});var zSr=s(_R);Eao=r(zSr,"QDQBertModel"),zSr.forEach(t),yao=r(b8e," (QDQBert model)"),b8e.forEach(t),wao=i(C),Cp=n(C,"LI",{});var v8e=s(Cp);fY=n(v8e,"STRONG",{});var VSr=s(fY);Aao=r(VSr,"reformer"),VSr.forEach(t),Lao=r(v8e," \u2014 "),uR=n(v8e,"A",{href:!0});var WSr=s(uR);Bao=r(WSr,"ReformerModel"),WSr.forEach(t),kao=r(v8e," (Reformer model)"),v8e.forEach(t),xao=i(C),Mp=n(C,"LI",{});var T8e=s(Mp);mY=n(T8e,"STRONG",{});var QSr=s(mY);Rao=r(QSr,"rembert"),QSr.forEach(t),Sao=r(T8e," \u2014 "),bR=n(T8e,"A",{href:!0});var HSr=s(bR);Pao=r(HSr,"RemBertModel"),HSr.forEach(t),$ao=r(T8e," (RemBERT model)"),T8e.forEach(t),Iao=i(C),Ep=n(C,"LI",{});var F8e=s(Ep);gY=n(F8e,"STRONG",{});var USr=s(gY);jao=r(USr,"retribert"),USr.forEach(t),Nao=r(F8e," \u2014 "),vR=n(F8e,"A",{href:!0});var JSr=s(vR);Dao=r(JSr,"RetriBertModel"),JSr.forEach(t),qao=r(F8e," (RetriBERT model)"),F8e.forEach(t),Gao=i(C),yp=n(C,"LI",{});var C8e=s(yp);hY=n(C8e,"STRONG",{});var YSr=s(hY);Oao=r(YSr,"roberta"),YSr.forEach(t),Xao=r(C8e," \u2014 "),TR=n(C8e,"A",{href:!0});var KSr=s(TR);zao=r(KSr,"RobertaModel"),KSr.forEach(t),Vao=r(C8e," (RoBERTa model)"),C8e.forEach(t),Wao=i(C),wp=n(C,"LI",{});var M8e=s(wp);pY=n(M8e,"STRONG",{});var ZSr=s(pY);Qao=r(ZSr,"roformer"),ZSr.forEach(t),Hao=r(M8e," \u2014 "),FR=n(M8e,"A",{href:!0});var ePr=s(FR);Uao=r(ePr,"RoFormerModel"),ePr.forEach(t),Jao=r(M8e," (RoFormer model)"),M8e.forEach(t),Yao=i(C),Ap=n(C,"LI",{});var E8e=s(Ap);_Y=n(E8e,"STRONG",{});var oPr=s(_Y);Kao=r(oPr,"segformer"),oPr.forEach(t),Zao=r(E8e," \u2014 "),CR=n(E8e,"A",{href:!0});var rPr=s(CR);eno=r(rPr,"SegformerModel"),rPr.forEach(t),ono=r(E8e," (SegFormer model)"),E8e.forEach(t),rno=i(C),Lp=n(C,"LI",{});var y8e=s(Lp);uY=n(y8e,"STRONG",{});var tPr=s(uY);tno=r(tPr,"sew"),tPr.forEach(t),ano=r(y8e," \u2014 "),MR=n(y8e,"A",{href:!0});var aPr=s(MR);nno=r(aPr,"SEWModel"),aPr.forEach(t),sno=r(y8e," (SEW model)"),y8e.forEach(t),lno=i(C),Bp=n(C,"LI",{});var w8e=s(Bp);bY=n(w8e,"STRONG",{});var nPr=s(bY);ino=r(nPr,"sew-d"),nPr.forEach(t),dno=r(w8e," \u2014 "),ER=n(w8e,"A",{href:!0});var sPr=s(ER);cno=r(sPr,"SEWDModel"),sPr.forEach(t),fno=r(w8e," (SEW-D model)"),w8e.forEach(t),mno=i(C),kp=n(C,"LI",{});var A8e=s(kp);vY=n(A8e,"STRONG",{});var lPr=s(vY);gno=r(lPr,"speech_to_text"),lPr.forEach(t),hno=r(A8e," \u2014 "),yR=n(A8e,"A",{href:!0});var iPr=s(yR);pno=r(iPr,"Speech2TextModel"),iPr.forEach(t),_no=r(A8e," (Speech2Text model)"),A8e.forEach(t),uno=i(C),xp=n(C,"LI",{});var L8e=s(xp);TY=n(L8e,"STRONG",{});var dPr=s(TY);bno=r(dPr,"splinter"),dPr.forEach(t),vno=r(L8e," \u2014 "),wR=n(L8e,"A",{href:!0});var cPr=s(wR);Tno=r(cPr,"SplinterModel"),cPr.forEach(t),Fno=r(L8e," (Splinter model)"),L8e.forEach(t),Cno=i(C),Rp=n(C,"LI",{});var B8e=s(Rp);FY=n(B8e,"STRONG",{});var fPr=s(FY);Mno=r(fPr,"squeezebert"),fPr.forEach(t),Eno=r(B8e," \u2014 "),AR=n(B8e,"A",{href:!0});var mPr=s(AR);yno=r(mPr,"SqueezeBertModel"),mPr.forEach(t),wno=r(B8e," (SqueezeBERT model)"),B8e.forEach(t),Ano=i(C),Sp=n(C,"LI",{});var k8e=s(Sp);CY=n(k8e,"STRONG",{});var gPr=s(CY);Lno=r(gPr,"swin"),gPr.forEach(t),Bno=r(k8e," \u2014 "),LR=n(k8e,"A",{href:!0});var hPr=s(LR);kno=r(hPr,"SwinModel"),hPr.forEach(t),xno=r(k8e," (Swin model)"),k8e.forEach(t),Rno=i(C),Pp=n(C,"LI",{});var x8e=s(Pp);MY=n(x8e,"STRONG",{});var pPr=s(MY);Sno=r(pPr,"t5"),pPr.forEach(t),Pno=r(x8e," \u2014 "),BR=n(x8e,"A",{href:!0});var _Pr=s(BR);$no=r(_Pr,"T5Model"),_Pr.forEach(t),Ino=r(x8e," (T5 model)"),x8e.forEach(t),jno=i(C),$p=n(C,"LI",{});var R8e=s($p);EY=n(R8e,"STRONG",{});var uPr=s(EY);Nno=r(uPr,"tapas"),uPr.forEach(t),Dno=r(R8e," \u2014 "),kR=n(R8e,"A",{href:!0});var bPr=s(kR);qno=r(bPr,"TapasModel"),bPr.forEach(t),Gno=r(R8e," (TAPAS model)"),R8e.forEach(t),Ono=i(C),Ip=n(C,"LI",{});var S8e=s(Ip);yY=n(S8e,"STRONG",{});var vPr=s(yY);Xno=r(vPr,"transfo-xl"),vPr.forEach(t),zno=r(S8e," \u2014 "),xR=n(S8e,"A",{href:!0});var TPr=s(xR);Vno=r(TPr,"TransfoXLModel"),TPr.forEach(t),Wno=r(S8e," (Transformer-XL model)"),S8e.forEach(t),Qno=i(C),jp=n(C,"LI",{});var P8e=s(jp);wY=n(P8e,"STRONG",{});var FPr=s(wY);Hno=r(FPr,"unispeech"),FPr.forEach(t),Uno=r(P8e," \u2014 "),RR=n(P8e,"A",{href:!0});var CPr=s(RR);Jno=r(CPr,"UniSpeechModel"),CPr.forEach(t),Yno=r(P8e," (UniSpeech model)"),P8e.forEach(t),Kno=i(C),Np=n(C,"LI",{});var $8e=s(Np);AY=n($8e,"STRONG",{});var MPr=s(AY);Zno=r(MPr,"unispeech-sat"),MPr.forEach(t),eso=r($8e," \u2014 "),SR=n($8e,"A",{href:!0});var EPr=s(SR);oso=r(EPr,"UniSpeechSatModel"),EPr.forEach(t),rso=r($8e," (UniSpeechSat model)"),$8e.forEach(t),tso=i(C),Dp=n(C,"LI",{});var I8e=s(Dp);LY=n(I8e,"STRONG",{});var yPr=s(LY);aso=r(yPr,"vilt"),yPr.forEach(t),nso=r(I8e," \u2014 "),PR=n(I8e,"A",{href:!0});var wPr=s(PR);sso=r(wPr,"ViltModel"),wPr.forEach(t),lso=r(I8e," (ViLT model)"),I8e.forEach(t),iso=i(C),qp=n(C,"LI",{});var j8e=s(qp);BY=n(j8e,"STRONG",{});var APr=s(BY);dso=r(APr,"vision-text-dual-encoder"),APr.forEach(t),cso=r(j8e," \u2014 "),$R=n(j8e,"A",{href:!0});var LPr=s($R);fso=r(LPr,"VisionTextDualEncoderModel"),LPr.forEach(t),mso=r(j8e," (VisionTextDualEncoder model)"),j8e.forEach(t),gso=i(C),Gp=n(C,"LI",{});var N8e=s(Gp);kY=n(N8e,"STRONG",{});var BPr=s(kY);hso=r(BPr,"visual_bert"),BPr.forEach(t),pso=r(N8e," \u2014 "),IR=n(N8e,"A",{href:!0});var kPr=s(IR);_so=r(kPr,"VisualBertModel"),kPr.forEach(t),uso=r(N8e," (VisualBert model)"),N8e.forEach(t),bso=i(C),Op=n(C,"LI",{});var D8e=s(Op);xY=n(D8e,"STRONG",{});var xPr=s(xY);vso=r(xPr,"vit"),xPr.forEach(t),Tso=r(D8e," \u2014 "),jR=n(D8e,"A",{href:!0});var RPr=s(jR);Fso=r(RPr,"ViTModel"),RPr.forEach(t),Cso=r(D8e," (ViT model)"),D8e.forEach(t),Mso=i(C),Xp=n(C,"LI",{});var q8e=s(Xp);RY=n(q8e,"STRONG",{});var SPr=s(RY);Eso=r(SPr,"vit_mae"),SPr.forEach(t),yso=r(q8e," \u2014 "),NR=n(q8e,"A",{href:!0});var PPr=s(NR);wso=r(PPr,"ViTMAEModel"),PPr.forEach(t),Aso=r(q8e," (ViTMAE model)"),q8e.forEach(t),Lso=i(C),zp=n(C,"LI",{});var G8e=s(zp);SY=n(G8e,"STRONG",{});var $Pr=s(SY);Bso=r($Pr,"wav2vec2"),$Pr.forEach(t),kso=r(G8e," \u2014 "),DR=n(G8e,"A",{href:!0});var IPr=s(DR);xso=r(IPr,"Wav2Vec2Model"),IPr.forEach(t),Rso=r(G8e," (Wav2Vec2 model)"),G8e.forEach(t),Sso=i(C),Vp=n(C,"LI",{});var O8e=s(Vp);PY=n(O8e,"STRONG",{});var jPr=s(PY);Pso=r(jPr,"wavlm"),jPr.forEach(t),$so=r(O8e," \u2014 "),qR=n(O8e,"A",{href:!0});var NPr=s(qR);Iso=r(NPr,"WavLMModel"),NPr.forEach(t),jso=r(O8e," (WavLM model)"),O8e.forEach(t),Nso=i(C),Wp=n(C,"LI",{});var X8e=s(Wp);$Y=n(X8e,"STRONG",{});var DPr=s($Y);Dso=r(DPr,"xglm"),DPr.forEach(t),qso=r(X8e," \u2014 "),GR=n(X8e,"A",{href:!0});var qPr=s(GR);Gso=r(qPr,"XGLMModel"),qPr.forEach(t),Oso=r(X8e," (XGLM model)"),X8e.forEach(t),Xso=i(C),Qp=n(C,"LI",{});var z8e=s(Qp);IY=n(z8e,"STRONG",{});var GPr=s(IY);zso=r(GPr,"xlm"),GPr.forEach(t),Vso=r(z8e," \u2014 "),OR=n(z8e,"A",{href:!0});var OPr=s(OR);Wso=r(OPr,"XLMModel"),OPr.forEach(t),Qso=r(z8e," (XLM model)"),z8e.forEach(t),Hso=i(C),Hp=n(C,"LI",{});var V8e=s(Hp);jY=n(V8e,"STRONG",{});var XPr=s(jY);Uso=r(XPr,"xlm-prophetnet"),XPr.forEach(t),Jso=r(V8e," \u2014 "),XR=n(V8e,"A",{href:!0});var zPr=s(XR);Yso=r(zPr,"XLMProphetNetModel"),zPr.forEach(t),Kso=r(V8e," (XLMProphetNet model)"),V8e.forEach(t),Zso=i(C),Up=n(C,"LI",{});var W8e=s(Up);NY=n(W8e,"STRONG",{});var VPr=s(NY);elo=r(VPr,"xlm-roberta"),VPr.forEach(t),olo=r(W8e," \u2014 "),zR=n(W8e,"A",{href:!0});var WPr=s(zR);rlo=r(WPr,"XLMRobertaModel"),WPr.forEach(t),tlo=r(W8e," (XLM-RoBERTa model)"),W8e.forEach(t),alo=i(C),Jp=n(C,"LI",{});var Q8e=s(Jp);DY=n(Q8e,"STRONG",{});var QPr=s(DY);nlo=r(QPr,"xlm-roberta-xl"),QPr.forEach(t),slo=r(Q8e," \u2014 "),VR=n(Q8e,"A",{href:!0});var HPr=s(VR);llo=r(HPr,"XLMRobertaXLModel"),HPr.forEach(t),ilo=r(Q8e," (XLM-RoBERTa-XL model)"),Q8e.forEach(t),dlo=i(C),Yp=n(C,"LI",{});var H8e=s(Yp);qY=n(H8e,"STRONG",{});var UPr=s(qY);clo=r(UPr,"xlnet"),UPr.forEach(t),flo=r(H8e," \u2014 "),WR=n(H8e,"A",{href:!0});var JPr=s(WR);mlo=r(JPr,"XLNetModel"),JPr.forEach(t),glo=r(H8e," (XLNet model)"),H8e.forEach(t),hlo=i(C),Kp=n(C,"LI",{});var U8e=s(Kp);GY=n(U8e,"STRONG",{});var YPr=s(GY);plo=r(YPr,"yoso"),YPr.forEach(t),_lo=r(U8e," \u2014 "),QR=n(U8e,"A",{href:!0});var KPr=s(QR);ulo=r(KPr,"YosoModel"),KPr.forEach(t),blo=r(U8e," (YOSO model)"),U8e.forEach(t),C.forEach(t),vlo=i(St),Zp=n(St,"P",{});var J8e=s(Zp);Tlo=r(J8e,"The model is set in evaluation mode by default using "),OY=n(J8e,"CODE",{});var ZPr=s(OY);Flo=r(ZPr,"model.eval()"),ZPr.forEach(t),Clo=r(J8e,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),XY=n(J8e,"CODE",{});var e$r=s(XY);Mlo=r(e$r,"model.train()"),e$r.forEach(t),J8e.forEach(t),Elo=i(St),zY=n(St,"P",{});var o$r=s(zY);ylo=r(o$r,"Examples:"),o$r.forEach(t),wlo=i(St),m(XM.$$.fragment,St),St.forEach(t),Ns.forEach(t),w7e=i(d),Xi=n(d,"H2",{class:!0});var SBe=s(Xi);e_=n(SBe,"A",{id:!0,class:!0,href:!0});var r$r=s(e_);VY=n(r$r,"SPAN",{});var t$r=s(VY);m(zM.$$.fragment,t$r),t$r.forEach(t),r$r.forEach(t),Alo=i(SBe),WY=n(SBe,"SPAN",{});var a$r=s(WY);Llo=r(a$r,"AutoModelForPreTraining"),a$r.forEach(t),SBe.forEach(t),A7e=i(d),Wo=n(d,"DIV",{class:!0});var qs=s(Wo);m(VM.$$.fragment,qs),Blo=i(qs),zi=n(qs,"P",{});var KX=s(zi);klo=r(KX,`This is a generic model class that will be instantiated as one of the model classes of the library (with a pretraining head) when created
with the `),QY=n(KX,"CODE",{});var n$r=s(QY);xlo=r(n$r,"from_pretrained()"),n$r.forEach(t),Rlo=r(KX,"class method or the "),HY=n(KX,"CODE",{});var s$r=s(HY);Slo=r(s$r,"from_config()"),s$r.forEach(t),Plo=r(KX,`class
method.`),KX.forEach(t),$lo=i(qs),WM=n(qs,"P",{});var PBe=s(WM);Ilo=r(PBe,"This class cannot be instantiated directly using "),UY=n(PBe,"CODE",{});var l$r=s(UY);jlo=r(l$r,"__init__()"),l$r.forEach(t),Nlo=r(PBe," (throws an error)."),PBe.forEach(t),Dlo=i(qs),Dr=n(qs,"DIV",{class:!0});var Gs=s(Dr);m(QM.$$.fragment,Gs),qlo=i(Gs),JY=n(Gs,"P",{});var i$r=s(JY);Glo=r(i$r,"Instantiates one of the model classes of the library (with a pretraining head) from a configuration."),i$r.forEach(t),Olo=i(Gs),Vi=n(Gs,"P",{});var ZX=s(Vi);Xlo=r(ZX,`Note:
Loading a model from its configuration file does `),YY=n(ZX,"STRONG",{});var d$r=s(YY);zlo=r(d$r,"not"),d$r.forEach(t),Vlo=r(ZX,` load the model weights. It only affects the
model\u2019s configuration. Use `),KY=n(ZX,"CODE",{});var c$r=s(KY);Wlo=r(c$r,"from_pretrained()"),c$r.forEach(t),Qlo=r(ZX,"to load the model weights."),ZX.forEach(t),Hlo=i(Gs),ZY=n(Gs,"P",{});var f$r=s(ZY);Ulo=r(f$r,"Examples:"),f$r.forEach(t),Jlo=i(Gs),m(HM.$$.fragment,Gs),Gs.forEach(t),Ylo=i(qs),xe=n(qs,"DIV",{class:!0});var Pt=s(xe);m(UM.$$.fragment,Pt),Klo=i(Pt),eK=n(Pt,"P",{});var m$r=s(eK);Zlo=r(m$r,"Instantiate one of the model classes of the library (with a pretraining head) from a pretrained model."),m$r.forEach(t),eio=i(Pt),qa=n(Pt,"P",{});var c4=s(qa);oio=r(c4,"The model class to instantiate is selected based on the "),oK=n(c4,"CODE",{});var g$r=s(oK);rio=r(g$r,"model_type"),g$r.forEach(t),tio=r(c4,` property of the config object (either
passed as an argument or loaded from `),rK=n(c4,"CODE",{});var h$r=s(rK);aio=r(h$r,"pretrained_model_name_or_path"),h$r.forEach(t),nio=r(c4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),tK=n(c4,"CODE",{});var p$r=s(tK);sio=r(p$r,"pretrained_model_name_or_path"),p$r.forEach(t),lio=r(c4,":"),c4.forEach(t),iio=i(Pt),x=n(Pt,"UL",{});var S=s(x);o_=n(S,"LI",{});var Y8e=s(o_);aK=n(Y8e,"STRONG",{});var _$r=s(aK);dio=r(_$r,"albert"),_$r.forEach(t),cio=r(Y8e," \u2014 "),HR=n(Y8e,"A",{href:!0});var u$r=s(HR);fio=r(u$r,"AlbertForPreTraining"),u$r.forEach(t),mio=r(Y8e," (ALBERT model)"),Y8e.forEach(t),gio=i(S),r_=n(S,"LI",{});var K8e=s(r_);nK=n(K8e,"STRONG",{});var b$r=s(nK);hio=r(b$r,"bart"),b$r.forEach(t),pio=r(K8e," \u2014 "),UR=n(K8e,"A",{href:!0});var v$r=s(UR);_io=r(v$r,"BartForConditionalGeneration"),v$r.forEach(t),uio=r(K8e," (BART model)"),K8e.forEach(t),bio=i(S),t_=n(S,"LI",{});var Z8e=s(t_);sK=n(Z8e,"STRONG",{});var T$r=s(sK);vio=r(T$r,"bert"),T$r.forEach(t),Tio=r(Z8e," \u2014 "),JR=n(Z8e,"A",{href:!0});var F$r=s(JR);Fio=r(F$r,"BertForPreTraining"),F$r.forEach(t),Cio=r(Z8e," (BERT model)"),Z8e.forEach(t),Mio=i(S),a_=n(S,"LI",{});var eFe=s(a_);lK=n(eFe,"STRONG",{});var C$r=s(lK);Eio=r(C$r,"big_bird"),C$r.forEach(t),yio=r(eFe," \u2014 "),YR=n(eFe,"A",{href:!0});var M$r=s(YR);wio=r(M$r,"BigBirdForPreTraining"),M$r.forEach(t),Aio=r(eFe," (BigBird model)"),eFe.forEach(t),Lio=i(S),n_=n(S,"LI",{});var oFe=s(n_);iK=n(oFe,"STRONG",{});var E$r=s(iK);Bio=r(E$r,"camembert"),E$r.forEach(t),kio=r(oFe," \u2014 "),KR=n(oFe,"A",{href:!0});var y$r=s(KR);xio=r(y$r,"CamembertForMaskedLM"),y$r.forEach(t),Rio=r(oFe," (CamemBERT model)"),oFe.forEach(t),Sio=i(S),s_=n(S,"LI",{});var rFe=s(s_);dK=n(rFe,"STRONG",{});var w$r=s(dK);Pio=r(w$r,"ctrl"),w$r.forEach(t),$io=r(rFe," \u2014 "),ZR=n(rFe,"A",{href:!0});var A$r=s(ZR);Iio=r(A$r,"CTRLLMHeadModel"),A$r.forEach(t),jio=r(rFe," (CTRL model)"),rFe.forEach(t),Nio=i(S),l_=n(S,"LI",{});var tFe=s(l_);cK=n(tFe,"STRONG",{});var L$r=s(cK);Dio=r(L$r,"deberta"),L$r.forEach(t),qio=r(tFe," \u2014 "),eS=n(tFe,"A",{href:!0});var B$r=s(eS);Gio=r(B$r,"DebertaForMaskedLM"),B$r.forEach(t),Oio=r(tFe," (DeBERTa model)"),tFe.forEach(t),Xio=i(S),i_=n(S,"LI",{});var aFe=s(i_);fK=n(aFe,"STRONG",{});var k$r=s(fK);zio=r(k$r,"deberta-v2"),k$r.forEach(t),Vio=r(aFe," \u2014 "),oS=n(aFe,"A",{href:!0});var x$r=s(oS);Wio=r(x$r,"DebertaV2ForMaskedLM"),x$r.forEach(t),Qio=r(aFe," (DeBERTa-v2 model)"),aFe.forEach(t),Hio=i(S),d_=n(S,"LI",{});var nFe=s(d_);mK=n(nFe,"STRONG",{});var R$r=s(mK);Uio=r(R$r,"distilbert"),R$r.forEach(t),Jio=r(nFe," \u2014 "),rS=n(nFe,"A",{href:!0});var S$r=s(rS);Yio=r(S$r,"DistilBertForMaskedLM"),S$r.forEach(t),Kio=r(nFe," (DistilBERT model)"),nFe.forEach(t),Zio=i(S),c_=n(S,"LI",{});var sFe=s(c_);gK=n(sFe,"STRONG",{});var P$r=s(gK);edo=r(P$r,"electra"),P$r.forEach(t),odo=r(sFe," \u2014 "),tS=n(sFe,"A",{href:!0});var $$r=s(tS);rdo=r($$r,"ElectraForPreTraining"),$$r.forEach(t),tdo=r(sFe," (ELECTRA model)"),sFe.forEach(t),ado=i(S),f_=n(S,"LI",{});var lFe=s(f_);hK=n(lFe,"STRONG",{});var I$r=s(hK);ndo=r(I$r,"flaubert"),I$r.forEach(t),sdo=r(lFe," \u2014 "),aS=n(lFe,"A",{href:!0});var j$r=s(aS);ldo=r(j$r,"FlaubertWithLMHeadModel"),j$r.forEach(t),ido=r(lFe," (FlauBERT model)"),lFe.forEach(t),ddo=i(S),m_=n(S,"LI",{});var iFe=s(m_);pK=n(iFe,"STRONG",{});var N$r=s(pK);cdo=r(N$r,"fnet"),N$r.forEach(t),fdo=r(iFe," \u2014 "),nS=n(iFe,"A",{href:!0});var D$r=s(nS);mdo=r(D$r,"FNetForPreTraining"),D$r.forEach(t),gdo=r(iFe," (FNet model)"),iFe.forEach(t),hdo=i(S),g_=n(S,"LI",{});var dFe=s(g_);_K=n(dFe,"STRONG",{});var q$r=s(_K);pdo=r(q$r,"fsmt"),q$r.forEach(t),_do=r(dFe," \u2014 "),sS=n(dFe,"A",{href:!0});var G$r=s(sS);udo=r(G$r,"FSMTForConditionalGeneration"),G$r.forEach(t),bdo=r(dFe," (FairSeq Machine-Translation model)"),dFe.forEach(t),vdo=i(S),h_=n(S,"LI",{});var cFe=s(h_);uK=n(cFe,"STRONG",{});var O$r=s(uK);Tdo=r(O$r,"funnel"),O$r.forEach(t),Fdo=r(cFe," \u2014 "),lS=n(cFe,"A",{href:!0});var X$r=s(lS);Cdo=r(X$r,"FunnelForPreTraining"),X$r.forEach(t),Mdo=r(cFe," (Funnel Transformer model)"),cFe.forEach(t),Edo=i(S),p_=n(S,"LI",{});var fFe=s(p_);bK=n(fFe,"STRONG",{});var z$r=s(bK);ydo=r(z$r,"gpt2"),z$r.forEach(t),wdo=r(fFe," \u2014 "),iS=n(fFe,"A",{href:!0});var V$r=s(iS);Ado=r(V$r,"GPT2LMHeadModel"),V$r.forEach(t),Ldo=r(fFe," (OpenAI GPT-2 model)"),fFe.forEach(t),Bdo=i(S),__=n(S,"LI",{});var mFe=s(__);vK=n(mFe,"STRONG",{});var W$r=s(vK);kdo=r(W$r,"ibert"),W$r.forEach(t),xdo=r(mFe," \u2014 "),dS=n(mFe,"A",{href:!0});var Q$r=s(dS);Rdo=r(Q$r,"IBertForMaskedLM"),Q$r.forEach(t),Sdo=r(mFe," (I-BERT model)"),mFe.forEach(t),Pdo=i(S),u_=n(S,"LI",{});var gFe=s(u_);TK=n(gFe,"STRONG",{});var H$r=s(TK);$do=r(H$r,"layoutlm"),H$r.forEach(t),Ido=r(gFe," \u2014 "),cS=n(gFe,"A",{href:!0});var U$r=s(cS);jdo=r(U$r,"LayoutLMForMaskedLM"),U$r.forEach(t),Ndo=r(gFe," (LayoutLM model)"),gFe.forEach(t),Ddo=i(S),b_=n(S,"LI",{});var hFe=s(b_);FK=n(hFe,"STRONG",{});var J$r=s(FK);qdo=r(J$r,"longformer"),J$r.forEach(t),Gdo=r(hFe," \u2014 "),fS=n(hFe,"A",{href:!0});var Y$r=s(fS);Odo=r(Y$r,"LongformerForMaskedLM"),Y$r.forEach(t),Xdo=r(hFe," (Longformer model)"),hFe.forEach(t),zdo=i(S),v_=n(S,"LI",{});var pFe=s(v_);CK=n(pFe,"STRONG",{});var K$r=s(CK);Vdo=r(K$r,"lxmert"),K$r.forEach(t),Wdo=r(pFe," \u2014 "),mS=n(pFe,"A",{href:!0});var Z$r=s(mS);Qdo=r(Z$r,"LxmertForPreTraining"),Z$r.forEach(t),Hdo=r(pFe," (LXMERT model)"),pFe.forEach(t),Udo=i(S),T_=n(S,"LI",{});var _Fe=s(T_);MK=n(_Fe,"STRONG",{});var eIr=s(MK);Jdo=r(eIr,"megatron-bert"),eIr.forEach(t),Ydo=r(_Fe," \u2014 "),gS=n(_Fe,"A",{href:!0});var oIr=s(gS);Kdo=r(oIr,"MegatronBertForPreTraining"),oIr.forEach(t),Zdo=r(_Fe," (MegatronBert model)"),_Fe.forEach(t),eco=i(S),F_=n(S,"LI",{});var uFe=s(F_);EK=n(uFe,"STRONG",{});var rIr=s(EK);oco=r(rIr,"mobilebert"),rIr.forEach(t),rco=r(uFe," \u2014 "),hS=n(uFe,"A",{href:!0});var tIr=s(hS);tco=r(tIr,"MobileBertForPreTraining"),tIr.forEach(t),aco=r(uFe," (MobileBERT model)"),uFe.forEach(t),nco=i(S),C_=n(S,"LI",{});var bFe=s(C_);yK=n(bFe,"STRONG",{});var aIr=s(yK);sco=r(aIr,"mpnet"),aIr.forEach(t),lco=r(bFe," \u2014 "),pS=n(bFe,"A",{href:!0});var nIr=s(pS);ico=r(nIr,"MPNetForMaskedLM"),nIr.forEach(t),dco=r(bFe," (MPNet model)"),bFe.forEach(t),cco=i(S),M_=n(S,"LI",{});var vFe=s(M_);wK=n(vFe,"STRONG",{});var sIr=s(wK);fco=r(sIr,"openai-gpt"),sIr.forEach(t),mco=r(vFe," \u2014 "),_S=n(vFe,"A",{href:!0});var lIr=s(_S);gco=r(lIr,"OpenAIGPTLMHeadModel"),lIr.forEach(t),hco=r(vFe," (OpenAI GPT model)"),vFe.forEach(t),pco=i(S),E_=n(S,"LI",{});var TFe=s(E_);AK=n(TFe,"STRONG",{});var iIr=s(AK);_co=r(iIr,"retribert"),iIr.forEach(t),uco=r(TFe," \u2014 "),uS=n(TFe,"A",{href:!0});var dIr=s(uS);bco=r(dIr,"RetriBertModel"),dIr.forEach(t),vco=r(TFe," (RetriBERT model)"),TFe.forEach(t),Tco=i(S),y_=n(S,"LI",{});var FFe=s(y_);LK=n(FFe,"STRONG",{});var cIr=s(LK);Fco=r(cIr,"roberta"),cIr.forEach(t),Cco=r(FFe," \u2014 "),bS=n(FFe,"A",{href:!0});var fIr=s(bS);Mco=r(fIr,"RobertaForMaskedLM"),fIr.forEach(t),Eco=r(FFe," (RoBERTa model)"),FFe.forEach(t),yco=i(S),w_=n(S,"LI",{});var CFe=s(w_);BK=n(CFe,"STRONG",{});var mIr=s(BK);wco=r(mIr,"squeezebert"),mIr.forEach(t),Aco=r(CFe," \u2014 "),vS=n(CFe,"A",{href:!0});var gIr=s(vS);Lco=r(gIr,"SqueezeBertForMaskedLM"),gIr.forEach(t),Bco=r(CFe," (SqueezeBERT model)"),CFe.forEach(t),kco=i(S),A_=n(S,"LI",{});var MFe=s(A_);kK=n(MFe,"STRONG",{});var hIr=s(kK);xco=r(hIr,"t5"),hIr.forEach(t),Rco=r(MFe," \u2014 "),TS=n(MFe,"A",{href:!0});var pIr=s(TS);Sco=r(pIr,"T5ForConditionalGeneration"),pIr.forEach(t),Pco=r(MFe," (T5 model)"),MFe.forEach(t),$co=i(S),L_=n(S,"LI",{});var EFe=s(L_);xK=n(EFe,"STRONG",{});var _Ir=s(xK);Ico=r(_Ir,"tapas"),_Ir.forEach(t),jco=r(EFe," \u2014 "),FS=n(EFe,"A",{href:!0});var uIr=s(FS);Nco=r(uIr,"TapasForMaskedLM"),uIr.forEach(t),Dco=r(EFe," (TAPAS model)"),EFe.forEach(t),qco=i(S),B_=n(S,"LI",{});var yFe=s(B_);RK=n(yFe,"STRONG",{});var bIr=s(RK);Gco=r(bIr,"transfo-xl"),bIr.forEach(t),Oco=r(yFe," \u2014 "),CS=n(yFe,"A",{href:!0});var vIr=s(CS);Xco=r(vIr,"TransfoXLLMHeadModel"),vIr.forEach(t),zco=r(yFe," (Transformer-XL model)"),yFe.forEach(t),Vco=i(S),k_=n(S,"LI",{});var wFe=s(k_);SK=n(wFe,"STRONG",{});var TIr=s(SK);Wco=r(TIr,"unispeech"),TIr.forEach(t),Qco=r(wFe," \u2014 "),MS=n(wFe,"A",{href:!0});var FIr=s(MS);Hco=r(FIr,"UniSpeechForPreTraining"),FIr.forEach(t),Uco=r(wFe," (UniSpeech model)"),wFe.forEach(t),Jco=i(S),x_=n(S,"LI",{});var AFe=s(x_);PK=n(AFe,"STRONG",{});var CIr=s(PK);Yco=r(CIr,"unispeech-sat"),CIr.forEach(t),Kco=r(AFe," \u2014 "),ES=n(AFe,"A",{href:!0});var MIr=s(ES);Zco=r(MIr,"UniSpeechSatForPreTraining"),MIr.forEach(t),efo=r(AFe," (UniSpeechSat model)"),AFe.forEach(t),ofo=i(S),R_=n(S,"LI",{});var LFe=s(R_);$K=n(LFe,"STRONG",{});var EIr=s($K);rfo=r(EIr,"visual_bert"),EIr.forEach(t),tfo=r(LFe," \u2014 "),yS=n(LFe,"A",{href:!0});var yIr=s(yS);afo=r(yIr,"VisualBertForPreTraining"),yIr.forEach(t),nfo=r(LFe," (VisualBert model)"),LFe.forEach(t),sfo=i(S),S_=n(S,"LI",{});var BFe=s(S_);IK=n(BFe,"STRONG",{});var wIr=s(IK);lfo=r(wIr,"vit_mae"),wIr.forEach(t),ifo=r(BFe," \u2014 "),wS=n(BFe,"A",{href:!0});var AIr=s(wS);dfo=r(AIr,"ViTMAEForPreTraining"),AIr.forEach(t),cfo=r(BFe," (ViTMAE model)"),BFe.forEach(t),ffo=i(S),P_=n(S,"LI",{});var kFe=s(P_);jK=n(kFe,"STRONG",{});var LIr=s(jK);mfo=r(LIr,"wav2vec2"),LIr.forEach(t),gfo=r(kFe," \u2014 "),AS=n(kFe,"A",{href:!0});var BIr=s(AS);hfo=r(BIr,"Wav2Vec2ForPreTraining"),BIr.forEach(t),pfo=r(kFe," (Wav2Vec2 model)"),kFe.forEach(t),_fo=i(S),$_=n(S,"LI",{});var xFe=s($_);NK=n(xFe,"STRONG",{});var kIr=s(NK);ufo=r(kIr,"xlm"),kIr.forEach(t),bfo=r(xFe," \u2014 "),LS=n(xFe,"A",{href:!0});var xIr=s(LS);vfo=r(xIr,"XLMWithLMHeadModel"),xIr.forEach(t),Tfo=r(xFe," (XLM model)"),xFe.forEach(t),Ffo=i(S),I_=n(S,"LI",{});var RFe=s(I_);DK=n(RFe,"STRONG",{});var RIr=s(DK);Cfo=r(RIr,"xlm-roberta"),RIr.forEach(t),Mfo=r(RFe," \u2014 "),BS=n(RFe,"A",{href:!0});var SIr=s(BS);Efo=r(SIr,"XLMRobertaForMaskedLM"),SIr.forEach(t),yfo=r(RFe," (XLM-RoBERTa model)"),RFe.forEach(t),wfo=i(S),j_=n(S,"LI",{});var SFe=s(j_);qK=n(SFe,"STRONG",{});var PIr=s(qK);Afo=r(PIr,"xlm-roberta-xl"),PIr.forEach(t),Lfo=r(SFe," \u2014 "),kS=n(SFe,"A",{href:!0});var $Ir=s(kS);Bfo=r($Ir,"XLMRobertaXLForMaskedLM"),$Ir.forEach(t),kfo=r(SFe," (XLM-RoBERTa-XL model)"),SFe.forEach(t),xfo=i(S),N_=n(S,"LI",{});var PFe=s(N_);GK=n(PFe,"STRONG",{});var IIr=s(GK);Rfo=r(IIr,"xlnet"),IIr.forEach(t),Sfo=r(PFe," \u2014 "),xS=n(PFe,"A",{href:!0});var jIr=s(xS);Pfo=r(jIr,"XLNetLMHeadModel"),jIr.forEach(t),$fo=r(PFe," (XLNet model)"),PFe.forEach(t),S.forEach(t),Ifo=i(Pt),D_=n(Pt,"P",{});var $Fe=s(D_);jfo=r($Fe,"The model is set in evaluation mode by default using "),OK=n($Fe,"CODE",{});var NIr=s(OK);Nfo=r(NIr,"model.eval()"),NIr.forEach(t),Dfo=r($Fe,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),XK=n($Fe,"CODE",{});var DIr=s(XK);qfo=r(DIr,"model.train()"),DIr.forEach(t),$Fe.forEach(t),Gfo=i(Pt),zK=n(Pt,"P",{});var qIr=s(zK);Ofo=r(qIr,"Examples:"),qIr.forEach(t),Xfo=i(Pt),m(JM.$$.fragment,Pt),Pt.forEach(t),qs.forEach(t),L7e=i(d),Wi=n(d,"H2",{class:!0});var $Be=s(Wi);q_=n($Be,"A",{id:!0,class:!0,href:!0});var GIr=s(q_);VK=n(GIr,"SPAN",{});var OIr=s(VK);m(YM.$$.fragment,OIr),OIr.forEach(t),GIr.forEach(t),zfo=i($Be),WK=n($Be,"SPAN",{});var XIr=s(WK);Vfo=r(XIr,"AutoModelForCausalLM"),XIr.forEach(t),$Be.forEach(t),B7e=i(d),Qo=n(d,"DIV",{class:!0});var Os=s(Qo);m(KM.$$.fragment,Os),Wfo=i(Os),Qi=n(Os,"P",{});var ez=s(Qi);Qfo=r(ez,`This is a generic model class that will be instantiated as one of the model classes of the library (with a causal language modeling head) when created
with the `),QK=n(ez,"CODE",{});var zIr=s(QK);Hfo=r(zIr,"from_pretrained()"),zIr.forEach(t),Ufo=r(ez,"class method or the "),HK=n(ez,"CODE",{});var VIr=s(HK);Jfo=r(VIr,"from_config()"),VIr.forEach(t),Yfo=r(ez,`class
method.`),ez.forEach(t),Kfo=i(Os),ZM=n(Os,"P",{});var IBe=s(ZM);Zfo=r(IBe,"This class cannot be instantiated directly using "),UK=n(IBe,"CODE",{});var WIr=s(UK);emo=r(WIr,"__init__()"),WIr.forEach(t),omo=r(IBe," (throws an error)."),IBe.forEach(t),rmo=i(Os),qr=n(Os,"DIV",{class:!0});var Xs=s(qr);m(eE.$$.fragment,Xs),tmo=i(Xs),JK=n(Xs,"P",{});var QIr=s(JK);amo=r(QIr,"Instantiates one of the model classes of the library (with a causal language modeling head) from a configuration."),QIr.forEach(t),nmo=i(Xs),Hi=n(Xs,"P",{});var oz=s(Hi);smo=r(oz,`Note:
Loading a model from its configuration file does `),YK=n(oz,"STRONG",{});var HIr=s(YK);lmo=r(HIr,"not"),HIr.forEach(t),imo=r(oz,` load the model weights. It only affects the
model\u2019s configuration. Use `),KK=n(oz,"CODE",{});var UIr=s(KK);dmo=r(UIr,"from_pretrained()"),UIr.forEach(t),cmo=r(oz,"to load the model weights."),oz.forEach(t),fmo=i(Xs),ZK=n(Xs,"P",{});var JIr=s(ZK);mmo=r(JIr,"Examples:"),JIr.forEach(t),gmo=i(Xs),m(oE.$$.fragment,Xs),Xs.forEach(t),hmo=i(Os),Re=n(Os,"DIV",{class:!0});var $t=s(Re);m(rE.$$.fragment,$t),pmo=i($t),eZ=n($t,"P",{});var YIr=s(eZ);_mo=r(YIr,"Instantiate one of the model classes of the library (with a causal language modeling head) from a pretrained model."),YIr.forEach(t),umo=i($t),Ga=n($t,"P",{});var f4=s(Ga);bmo=r(f4,"The model class to instantiate is selected based on the "),oZ=n(f4,"CODE",{});var KIr=s(oZ);vmo=r(KIr,"model_type"),KIr.forEach(t),Tmo=r(f4,` property of the config object (either
passed as an argument or loaded from `),rZ=n(f4,"CODE",{});var ZIr=s(rZ);Fmo=r(ZIr,"pretrained_model_name_or_path"),ZIr.forEach(t),Cmo=r(f4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),tZ=n(f4,"CODE",{});var ejr=s(tZ);Mmo=r(ejr,"pretrained_model_name_or_path"),ejr.forEach(t),Emo=r(f4,":"),f4.forEach(t),ymo=i($t),$=n($t,"UL",{});var j=s($);G_=n(j,"LI",{});var IFe=s(G_);aZ=n(IFe,"STRONG",{});var ojr=s(aZ);wmo=r(ojr,"bart"),ojr.forEach(t),Amo=r(IFe," \u2014 "),RS=n(IFe,"A",{href:!0});var rjr=s(RS);Lmo=r(rjr,"BartForCausalLM"),rjr.forEach(t),Bmo=r(IFe," (BART model)"),IFe.forEach(t),kmo=i(j),O_=n(j,"LI",{});var jFe=s(O_);nZ=n(jFe,"STRONG",{});var tjr=s(nZ);xmo=r(tjr,"bert"),tjr.forEach(t),Rmo=r(jFe," \u2014 "),SS=n(jFe,"A",{href:!0});var ajr=s(SS);Smo=r(ajr,"BertLMHeadModel"),ajr.forEach(t),Pmo=r(jFe," (BERT model)"),jFe.forEach(t),$mo=i(j),X_=n(j,"LI",{});var NFe=s(X_);sZ=n(NFe,"STRONG",{});var njr=s(sZ);Imo=r(njr,"bert-generation"),njr.forEach(t),jmo=r(NFe," \u2014 "),PS=n(NFe,"A",{href:!0});var sjr=s(PS);Nmo=r(sjr,"BertGenerationDecoder"),sjr.forEach(t),Dmo=r(NFe," (Bert Generation model)"),NFe.forEach(t),qmo=i(j),z_=n(j,"LI",{});var DFe=s(z_);lZ=n(DFe,"STRONG",{});var ljr=s(lZ);Gmo=r(ljr,"big_bird"),ljr.forEach(t),Omo=r(DFe," \u2014 "),$S=n(DFe,"A",{href:!0});var ijr=s($S);Xmo=r(ijr,"BigBirdForCausalLM"),ijr.forEach(t),zmo=r(DFe," (BigBird model)"),DFe.forEach(t),Vmo=i(j),V_=n(j,"LI",{});var qFe=s(V_);iZ=n(qFe,"STRONG",{});var djr=s(iZ);Wmo=r(djr,"bigbird_pegasus"),djr.forEach(t),Qmo=r(qFe," \u2014 "),IS=n(qFe,"A",{href:!0});var cjr=s(IS);Hmo=r(cjr,"BigBirdPegasusForCausalLM"),cjr.forEach(t),Umo=r(qFe," (BigBirdPegasus model)"),qFe.forEach(t),Jmo=i(j),W_=n(j,"LI",{});var GFe=s(W_);dZ=n(GFe,"STRONG",{});var fjr=s(dZ);Ymo=r(fjr,"blenderbot"),fjr.forEach(t),Kmo=r(GFe," \u2014 "),jS=n(GFe,"A",{href:!0});var mjr=s(jS);Zmo=r(mjr,"BlenderbotForCausalLM"),mjr.forEach(t),ego=r(GFe," (Blenderbot model)"),GFe.forEach(t),ogo=i(j),Q_=n(j,"LI",{});var OFe=s(Q_);cZ=n(OFe,"STRONG",{});var gjr=s(cZ);rgo=r(gjr,"blenderbot-small"),gjr.forEach(t),tgo=r(OFe," \u2014 "),NS=n(OFe,"A",{href:!0});var hjr=s(NS);ago=r(hjr,"BlenderbotSmallForCausalLM"),hjr.forEach(t),ngo=r(OFe," (BlenderbotSmall model)"),OFe.forEach(t),sgo=i(j),H_=n(j,"LI",{});var XFe=s(H_);fZ=n(XFe,"STRONG",{});var pjr=s(fZ);lgo=r(pjr,"camembert"),pjr.forEach(t),igo=r(XFe," \u2014 "),DS=n(XFe,"A",{href:!0});var _jr=s(DS);dgo=r(_jr,"CamembertForCausalLM"),_jr.forEach(t),cgo=r(XFe," (CamemBERT model)"),XFe.forEach(t),fgo=i(j),U_=n(j,"LI",{});var zFe=s(U_);mZ=n(zFe,"STRONG",{});var ujr=s(mZ);mgo=r(ujr,"ctrl"),ujr.forEach(t),ggo=r(zFe," \u2014 "),qS=n(zFe,"A",{href:!0});var bjr=s(qS);hgo=r(bjr,"CTRLLMHeadModel"),bjr.forEach(t),pgo=r(zFe," (CTRL model)"),zFe.forEach(t),_go=i(j),J_=n(j,"LI",{});var VFe=s(J_);gZ=n(VFe,"STRONG",{});var vjr=s(gZ);ugo=r(vjr,"electra"),vjr.forEach(t),bgo=r(VFe," \u2014 "),GS=n(VFe,"A",{href:!0});var Tjr=s(GS);vgo=r(Tjr,"ElectraForCausalLM"),Tjr.forEach(t),Tgo=r(VFe," (ELECTRA model)"),VFe.forEach(t),Fgo=i(j),Y_=n(j,"LI",{});var WFe=s(Y_);hZ=n(WFe,"STRONG",{});var Fjr=s(hZ);Cgo=r(Fjr,"gpt2"),Fjr.forEach(t),Mgo=r(WFe," \u2014 "),OS=n(WFe,"A",{href:!0});var Cjr=s(OS);Ego=r(Cjr,"GPT2LMHeadModel"),Cjr.forEach(t),ygo=r(WFe," (OpenAI GPT-2 model)"),WFe.forEach(t),wgo=i(j),K_=n(j,"LI",{});var QFe=s(K_);pZ=n(QFe,"STRONG",{});var Mjr=s(pZ);Ago=r(Mjr,"gpt_neo"),Mjr.forEach(t),Lgo=r(QFe," \u2014 "),XS=n(QFe,"A",{href:!0});var Ejr=s(XS);Bgo=r(Ejr,"GPTNeoForCausalLM"),Ejr.forEach(t),kgo=r(QFe," (GPT Neo model)"),QFe.forEach(t),xgo=i(j),Z_=n(j,"LI",{});var HFe=s(Z_);_Z=n(HFe,"STRONG",{});var yjr=s(_Z);Rgo=r(yjr,"gptj"),yjr.forEach(t),Sgo=r(HFe," \u2014 "),zS=n(HFe,"A",{href:!0});var wjr=s(zS);Pgo=r(wjr,"GPTJForCausalLM"),wjr.forEach(t),$go=r(HFe," (GPT-J model)"),HFe.forEach(t),Igo=i(j),eu=n(j,"LI",{});var UFe=s(eu);uZ=n(UFe,"STRONG",{});var Ajr=s(uZ);jgo=r(Ajr,"marian"),Ajr.forEach(t),Ngo=r(UFe," \u2014 "),VS=n(UFe,"A",{href:!0});var Ljr=s(VS);Dgo=r(Ljr,"MarianForCausalLM"),Ljr.forEach(t),qgo=r(UFe," (Marian model)"),UFe.forEach(t),Ggo=i(j),ou=n(j,"LI",{});var JFe=s(ou);bZ=n(JFe,"STRONG",{});var Bjr=s(bZ);Ogo=r(Bjr,"mbart"),Bjr.forEach(t),Xgo=r(JFe," \u2014 "),WS=n(JFe,"A",{href:!0});var kjr=s(WS);zgo=r(kjr,"MBartForCausalLM"),kjr.forEach(t),Vgo=r(JFe," (mBART model)"),JFe.forEach(t),Wgo=i(j),ru=n(j,"LI",{});var YFe=s(ru);vZ=n(YFe,"STRONG",{});var xjr=s(vZ);Qgo=r(xjr,"megatron-bert"),xjr.forEach(t),Hgo=r(YFe," \u2014 "),QS=n(YFe,"A",{href:!0});var Rjr=s(QS);Ugo=r(Rjr,"MegatronBertForCausalLM"),Rjr.forEach(t),Jgo=r(YFe," (MegatronBert model)"),YFe.forEach(t),Ygo=i(j),tu=n(j,"LI",{});var KFe=s(tu);TZ=n(KFe,"STRONG",{});var Sjr=s(TZ);Kgo=r(Sjr,"openai-gpt"),Sjr.forEach(t),Zgo=r(KFe," \u2014 "),HS=n(KFe,"A",{href:!0});var Pjr=s(HS);eho=r(Pjr,"OpenAIGPTLMHeadModel"),Pjr.forEach(t),oho=r(KFe," (OpenAI GPT model)"),KFe.forEach(t),rho=i(j),au=n(j,"LI",{});var ZFe=s(au);FZ=n(ZFe,"STRONG",{});var $jr=s(FZ);tho=r($jr,"pegasus"),$jr.forEach(t),aho=r(ZFe," \u2014 "),US=n(ZFe,"A",{href:!0});var Ijr=s(US);nho=r(Ijr,"PegasusForCausalLM"),Ijr.forEach(t),sho=r(ZFe," (Pegasus model)"),ZFe.forEach(t),lho=i(j),nu=n(j,"LI",{});var eCe=s(nu);CZ=n(eCe,"STRONG",{});var jjr=s(CZ);iho=r(jjr,"plbart"),jjr.forEach(t),dho=r(eCe," \u2014 "),JS=n(eCe,"A",{href:!0});var Njr=s(JS);cho=r(Njr,"PLBartForCausalLM"),Njr.forEach(t),fho=r(eCe," (PLBart model)"),eCe.forEach(t),mho=i(j),su=n(j,"LI",{});var oCe=s(su);MZ=n(oCe,"STRONG",{});var Djr=s(MZ);gho=r(Djr,"prophetnet"),Djr.forEach(t),hho=r(oCe," \u2014 "),YS=n(oCe,"A",{href:!0});var qjr=s(YS);pho=r(qjr,"ProphetNetForCausalLM"),qjr.forEach(t),_ho=r(oCe," (ProphetNet model)"),oCe.forEach(t),uho=i(j),lu=n(j,"LI",{});var rCe=s(lu);EZ=n(rCe,"STRONG",{});var Gjr=s(EZ);bho=r(Gjr,"qdqbert"),Gjr.forEach(t),vho=r(rCe," \u2014 "),KS=n(rCe,"A",{href:!0});var Ojr=s(KS);Tho=r(Ojr,"QDQBertLMHeadModel"),Ojr.forEach(t),Fho=r(rCe," (QDQBert model)"),rCe.forEach(t),Cho=i(j),iu=n(j,"LI",{});var tCe=s(iu);yZ=n(tCe,"STRONG",{});var Xjr=s(yZ);Mho=r(Xjr,"reformer"),Xjr.forEach(t),Eho=r(tCe," \u2014 "),ZS=n(tCe,"A",{href:!0});var zjr=s(ZS);yho=r(zjr,"ReformerModelWithLMHead"),zjr.forEach(t),who=r(tCe," (Reformer model)"),tCe.forEach(t),Aho=i(j),du=n(j,"LI",{});var aCe=s(du);wZ=n(aCe,"STRONG",{});var Vjr=s(wZ);Lho=r(Vjr,"rembert"),Vjr.forEach(t),Bho=r(aCe," \u2014 "),eP=n(aCe,"A",{href:!0});var Wjr=s(eP);kho=r(Wjr,"RemBertForCausalLM"),Wjr.forEach(t),xho=r(aCe," (RemBERT model)"),aCe.forEach(t),Rho=i(j),cu=n(j,"LI",{});var nCe=s(cu);AZ=n(nCe,"STRONG",{});var Qjr=s(AZ);Sho=r(Qjr,"roberta"),Qjr.forEach(t),Pho=r(nCe," \u2014 "),oP=n(nCe,"A",{href:!0});var Hjr=s(oP);$ho=r(Hjr,"RobertaForCausalLM"),Hjr.forEach(t),Iho=r(nCe," (RoBERTa model)"),nCe.forEach(t),jho=i(j),fu=n(j,"LI",{});var sCe=s(fu);LZ=n(sCe,"STRONG",{});var Ujr=s(LZ);Nho=r(Ujr,"roformer"),Ujr.forEach(t),Dho=r(sCe," \u2014 "),rP=n(sCe,"A",{href:!0});var Jjr=s(rP);qho=r(Jjr,"RoFormerForCausalLM"),Jjr.forEach(t),Gho=r(sCe," (RoFormer model)"),sCe.forEach(t),Oho=i(j),mu=n(j,"LI",{});var lCe=s(mu);BZ=n(lCe,"STRONG",{});var Yjr=s(BZ);Xho=r(Yjr,"speech_to_text_2"),Yjr.forEach(t),zho=r(lCe," \u2014 "),tP=n(lCe,"A",{href:!0});var Kjr=s(tP);Vho=r(Kjr,"Speech2Text2ForCausalLM"),Kjr.forEach(t),Who=r(lCe," (Speech2Text2 model)"),lCe.forEach(t),Qho=i(j),gu=n(j,"LI",{});var iCe=s(gu);kZ=n(iCe,"STRONG",{});var Zjr=s(kZ);Hho=r(Zjr,"transfo-xl"),Zjr.forEach(t),Uho=r(iCe," \u2014 "),aP=n(iCe,"A",{href:!0});var eNr=s(aP);Jho=r(eNr,"TransfoXLLMHeadModel"),eNr.forEach(t),Yho=r(iCe," (Transformer-XL model)"),iCe.forEach(t),Kho=i(j),hu=n(j,"LI",{});var dCe=s(hu);xZ=n(dCe,"STRONG",{});var oNr=s(xZ);Zho=r(oNr,"trocr"),oNr.forEach(t),epo=r(dCe," \u2014 "),nP=n(dCe,"A",{href:!0});var rNr=s(nP);opo=r(rNr,"TrOCRForCausalLM"),rNr.forEach(t),rpo=r(dCe," (TrOCR model)"),dCe.forEach(t),tpo=i(j),pu=n(j,"LI",{});var cCe=s(pu);RZ=n(cCe,"STRONG",{});var tNr=s(RZ);apo=r(tNr,"xglm"),tNr.forEach(t),npo=r(cCe," \u2014 "),sP=n(cCe,"A",{href:!0});var aNr=s(sP);spo=r(aNr,"XGLMForCausalLM"),aNr.forEach(t),lpo=r(cCe," (XGLM model)"),cCe.forEach(t),ipo=i(j),_u=n(j,"LI",{});var fCe=s(_u);SZ=n(fCe,"STRONG",{});var nNr=s(SZ);dpo=r(nNr,"xlm"),nNr.forEach(t),cpo=r(fCe," \u2014 "),lP=n(fCe,"A",{href:!0});var sNr=s(lP);fpo=r(sNr,"XLMWithLMHeadModel"),sNr.forEach(t),mpo=r(fCe," (XLM model)"),fCe.forEach(t),gpo=i(j),uu=n(j,"LI",{});var mCe=s(uu);PZ=n(mCe,"STRONG",{});var lNr=s(PZ);hpo=r(lNr,"xlm-prophetnet"),lNr.forEach(t),ppo=r(mCe," \u2014 "),iP=n(mCe,"A",{href:!0});var iNr=s(iP);_po=r(iNr,"XLMProphetNetForCausalLM"),iNr.forEach(t),upo=r(mCe," (XLMProphetNet model)"),mCe.forEach(t),bpo=i(j),bu=n(j,"LI",{});var gCe=s(bu);$Z=n(gCe,"STRONG",{});var dNr=s($Z);vpo=r(dNr,"xlm-roberta"),dNr.forEach(t),Tpo=r(gCe," \u2014 "),dP=n(gCe,"A",{href:!0});var cNr=s(dP);Fpo=r(cNr,"XLMRobertaForCausalLM"),cNr.forEach(t),Cpo=r(gCe," (XLM-RoBERTa model)"),gCe.forEach(t),Mpo=i(j),vu=n(j,"LI",{});var hCe=s(vu);IZ=n(hCe,"STRONG",{});var fNr=s(IZ);Epo=r(fNr,"xlm-roberta-xl"),fNr.forEach(t),ypo=r(hCe," \u2014 "),cP=n(hCe,"A",{href:!0});var mNr=s(cP);wpo=r(mNr,"XLMRobertaXLForCausalLM"),mNr.forEach(t),Apo=r(hCe," (XLM-RoBERTa-XL model)"),hCe.forEach(t),Lpo=i(j),Tu=n(j,"LI",{});var pCe=s(Tu);jZ=n(pCe,"STRONG",{});var gNr=s(jZ);Bpo=r(gNr,"xlnet"),gNr.forEach(t),kpo=r(pCe," \u2014 "),fP=n(pCe,"A",{href:!0});var hNr=s(fP);xpo=r(hNr,"XLNetLMHeadModel"),hNr.forEach(t),Rpo=r(pCe," (XLNet model)"),pCe.forEach(t),j.forEach(t),Spo=i($t),Fu=n($t,"P",{});var _Ce=s(Fu);Ppo=r(_Ce,"The model is set in evaluation mode by default using "),NZ=n(_Ce,"CODE",{});var pNr=s(NZ);$po=r(pNr,"model.eval()"),pNr.forEach(t),Ipo=r(_Ce,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),DZ=n(_Ce,"CODE",{});var _Nr=s(DZ);jpo=r(_Nr,"model.train()"),_Nr.forEach(t),_Ce.forEach(t),Npo=i($t),qZ=n($t,"P",{});var uNr=s(qZ);Dpo=r(uNr,"Examples:"),uNr.forEach(t),qpo=i($t),m(tE.$$.fragment,$t),$t.forEach(t),Os.forEach(t),k7e=i(d),Ui=n(d,"H2",{class:!0});var jBe=s(Ui);Cu=n(jBe,"A",{id:!0,class:!0,href:!0});var bNr=s(Cu);GZ=n(bNr,"SPAN",{});var vNr=s(GZ);m(aE.$$.fragment,vNr),vNr.forEach(t),bNr.forEach(t),Gpo=i(jBe),OZ=n(jBe,"SPAN",{});var TNr=s(OZ);Opo=r(TNr,"AutoModelForMaskedLM"),TNr.forEach(t),jBe.forEach(t),x7e=i(d),Ho=n(d,"DIV",{class:!0});var zs=s(Ho);m(nE.$$.fragment,zs),Xpo=i(zs),Ji=n(zs,"P",{});var rz=s(Ji);zpo=r(rz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a masked language modeling head) when created
with the `),XZ=n(rz,"CODE",{});var FNr=s(XZ);Vpo=r(FNr,"from_pretrained()"),FNr.forEach(t),Wpo=r(rz,"class method or the "),zZ=n(rz,"CODE",{});var CNr=s(zZ);Qpo=r(CNr,"from_config()"),CNr.forEach(t),Hpo=r(rz,`class
method.`),rz.forEach(t),Upo=i(zs),sE=n(zs,"P",{});var NBe=s(sE);Jpo=r(NBe,"This class cannot be instantiated directly using "),VZ=n(NBe,"CODE",{});var MNr=s(VZ);Ypo=r(MNr,"__init__()"),MNr.forEach(t),Kpo=r(NBe," (throws an error)."),NBe.forEach(t),Zpo=i(zs),Gr=n(zs,"DIV",{class:!0});var Vs=s(Gr);m(lE.$$.fragment,Vs),e_o=i(Vs),WZ=n(Vs,"P",{});var ENr=s(WZ);o_o=r(ENr,"Instantiates one of the model classes of the library (with a masked language modeling head) from a configuration."),ENr.forEach(t),r_o=i(Vs),Yi=n(Vs,"P",{});var tz=s(Yi);t_o=r(tz,`Note:
Loading a model from its configuration file does `),QZ=n(tz,"STRONG",{});var yNr=s(QZ);a_o=r(yNr,"not"),yNr.forEach(t),n_o=r(tz,` load the model weights. It only affects the
model\u2019s configuration. Use `),HZ=n(tz,"CODE",{});var wNr=s(HZ);s_o=r(wNr,"from_pretrained()"),wNr.forEach(t),l_o=r(tz,"to load the model weights."),tz.forEach(t),i_o=i(Vs),UZ=n(Vs,"P",{});var ANr=s(UZ);d_o=r(ANr,"Examples:"),ANr.forEach(t),c_o=i(Vs),m(iE.$$.fragment,Vs),Vs.forEach(t),f_o=i(zs),Se=n(zs,"DIV",{class:!0});var It=s(Se);m(dE.$$.fragment,It),m_o=i(It),JZ=n(It,"P",{});var LNr=s(JZ);g_o=r(LNr,"Instantiate one of the model classes of the library (with a masked language modeling head) from a pretrained model."),LNr.forEach(t),h_o=i(It),Oa=n(It,"P",{});var m4=s(Oa);p_o=r(m4,"The model class to instantiate is selected based on the "),YZ=n(m4,"CODE",{});var BNr=s(YZ);__o=r(BNr,"model_type"),BNr.forEach(t),u_o=r(m4,` property of the config object (either
passed as an argument or loaded from `),KZ=n(m4,"CODE",{});var kNr=s(KZ);b_o=r(kNr,"pretrained_model_name_or_path"),kNr.forEach(t),v_o=r(m4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),ZZ=n(m4,"CODE",{});var xNr=s(ZZ);T_o=r(xNr,"pretrained_model_name_or_path"),xNr.forEach(t),F_o=r(m4,":"),m4.forEach(t),C_o=i(It),I=n(It,"UL",{});var N=s(I);Mu=n(N,"LI",{});var uCe=s(Mu);eee=n(uCe,"STRONG",{});var RNr=s(eee);M_o=r(RNr,"albert"),RNr.forEach(t),E_o=r(uCe," \u2014 "),mP=n(uCe,"A",{href:!0});var SNr=s(mP);y_o=r(SNr,"AlbertForMaskedLM"),SNr.forEach(t),w_o=r(uCe," (ALBERT model)"),uCe.forEach(t),A_o=i(N),Eu=n(N,"LI",{});var bCe=s(Eu);oee=n(bCe,"STRONG",{});var PNr=s(oee);L_o=r(PNr,"bart"),PNr.forEach(t),B_o=r(bCe," \u2014 "),gP=n(bCe,"A",{href:!0});var $Nr=s(gP);k_o=r($Nr,"BartForConditionalGeneration"),$Nr.forEach(t),x_o=r(bCe," (BART model)"),bCe.forEach(t),R_o=i(N),yu=n(N,"LI",{});var vCe=s(yu);ree=n(vCe,"STRONG",{});var INr=s(ree);S_o=r(INr,"bert"),INr.forEach(t),P_o=r(vCe," \u2014 "),hP=n(vCe,"A",{href:!0});var jNr=s(hP);$_o=r(jNr,"BertForMaskedLM"),jNr.forEach(t),I_o=r(vCe," (BERT model)"),vCe.forEach(t),j_o=i(N),wu=n(N,"LI",{});var TCe=s(wu);tee=n(TCe,"STRONG",{});var NNr=s(tee);N_o=r(NNr,"big_bird"),NNr.forEach(t),D_o=r(TCe," \u2014 "),pP=n(TCe,"A",{href:!0});var DNr=s(pP);q_o=r(DNr,"BigBirdForMaskedLM"),DNr.forEach(t),G_o=r(TCe," (BigBird model)"),TCe.forEach(t),O_o=i(N),Au=n(N,"LI",{});var FCe=s(Au);aee=n(FCe,"STRONG",{});var qNr=s(aee);X_o=r(qNr,"camembert"),qNr.forEach(t),z_o=r(FCe," \u2014 "),_P=n(FCe,"A",{href:!0});var GNr=s(_P);V_o=r(GNr,"CamembertForMaskedLM"),GNr.forEach(t),W_o=r(FCe," (CamemBERT model)"),FCe.forEach(t),Q_o=i(N),Lu=n(N,"LI",{});var CCe=s(Lu);nee=n(CCe,"STRONG",{});var ONr=s(nee);H_o=r(ONr,"convbert"),ONr.forEach(t),U_o=r(CCe," \u2014 "),uP=n(CCe,"A",{href:!0});var XNr=s(uP);J_o=r(XNr,"ConvBertForMaskedLM"),XNr.forEach(t),Y_o=r(CCe," (ConvBERT model)"),CCe.forEach(t),K_o=i(N),Bu=n(N,"LI",{});var MCe=s(Bu);see=n(MCe,"STRONG",{});var zNr=s(see);Z_o=r(zNr,"deberta"),zNr.forEach(t),euo=r(MCe," \u2014 "),bP=n(MCe,"A",{href:!0});var VNr=s(bP);ouo=r(VNr,"DebertaForMaskedLM"),VNr.forEach(t),ruo=r(MCe," (DeBERTa model)"),MCe.forEach(t),tuo=i(N),ku=n(N,"LI",{});var ECe=s(ku);lee=n(ECe,"STRONG",{});var WNr=s(lee);auo=r(WNr,"deberta-v2"),WNr.forEach(t),nuo=r(ECe," \u2014 "),vP=n(ECe,"A",{href:!0});var QNr=s(vP);suo=r(QNr,"DebertaV2ForMaskedLM"),QNr.forEach(t),luo=r(ECe," (DeBERTa-v2 model)"),ECe.forEach(t),iuo=i(N),xu=n(N,"LI",{});var yCe=s(xu);iee=n(yCe,"STRONG",{});var HNr=s(iee);duo=r(HNr,"distilbert"),HNr.forEach(t),cuo=r(yCe," \u2014 "),TP=n(yCe,"A",{href:!0});var UNr=s(TP);fuo=r(UNr,"DistilBertForMaskedLM"),UNr.forEach(t),muo=r(yCe," (DistilBERT model)"),yCe.forEach(t),guo=i(N),Ru=n(N,"LI",{});var wCe=s(Ru);dee=n(wCe,"STRONG",{});var JNr=s(dee);huo=r(JNr,"electra"),JNr.forEach(t),puo=r(wCe," \u2014 "),FP=n(wCe,"A",{href:!0});var YNr=s(FP);_uo=r(YNr,"ElectraForMaskedLM"),YNr.forEach(t),uuo=r(wCe," (ELECTRA model)"),wCe.forEach(t),buo=i(N),Su=n(N,"LI",{});var ACe=s(Su);cee=n(ACe,"STRONG",{});var KNr=s(cee);vuo=r(KNr,"flaubert"),KNr.forEach(t),Tuo=r(ACe," \u2014 "),CP=n(ACe,"A",{href:!0});var ZNr=s(CP);Fuo=r(ZNr,"FlaubertWithLMHeadModel"),ZNr.forEach(t),Cuo=r(ACe," (FlauBERT model)"),ACe.forEach(t),Muo=i(N),Pu=n(N,"LI",{});var LCe=s(Pu);fee=n(LCe,"STRONG",{});var eDr=s(fee);Euo=r(eDr,"fnet"),eDr.forEach(t),yuo=r(LCe," \u2014 "),MP=n(LCe,"A",{href:!0});var oDr=s(MP);wuo=r(oDr,"FNetForMaskedLM"),oDr.forEach(t),Auo=r(LCe," (FNet model)"),LCe.forEach(t),Luo=i(N),$u=n(N,"LI",{});var BCe=s($u);mee=n(BCe,"STRONG",{});var rDr=s(mee);Buo=r(rDr,"funnel"),rDr.forEach(t),kuo=r(BCe," \u2014 "),EP=n(BCe,"A",{href:!0});var tDr=s(EP);xuo=r(tDr,"FunnelForMaskedLM"),tDr.forEach(t),Ruo=r(BCe," (Funnel Transformer model)"),BCe.forEach(t),Suo=i(N),Iu=n(N,"LI",{});var kCe=s(Iu);gee=n(kCe,"STRONG",{});var aDr=s(gee);Puo=r(aDr,"ibert"),aDr.forEach(t),$uo=r(kCe," \u2014 "),yP=n(kCe,"A",{href:!0});var nDr=s(yP);Iuo=r(nDr,"IBertForMaskedLM"),nDr.forEach(t),juo=r(kCe," (I-BERT model)"),kCe.forEach(t),Nuo=i(N),ju=n(N,"LI",{});var xCe=s(ju);hee=n(xCe,"STRONG",{});var sDr=s(hee);Duo=r(sDr,"layoutlm"),sDr.forEach(t),quo=r(xCe," \u2014 "),wP=n(xCe,"A",{href:!0});var lDr=s(wP);Guo=r(lDr,"LayoutLMForMaskedLM"),lDr.forEach(t),Ouo=r(xCe," (LayoutLM model)"),xCe.forEach(t),Xuo=i(N),Nu=n(N,"LI",{});var RCe=s(Nu);pee=n(RCe,"STRONG",{});var iDr=s(pee);zuo=r(iDr,"longformer"),iDr.forEach(t),Vuo=r(RCe," \u2014 "),AP=n(RCe,"A",{href:!0});var dDr=s(AP);Wuo=r(dDr,"LongformerForMaskedLM"),dDr.forEach(t),Quo=r(RCe," (Longformer model)"),RCe.forEach(t),Huo=i(N),Du=n(N,"LI",{});var SCe=s(Du);_ee=n(SCe,"STRONG",{});var cDr=s(_ee);Uuo=r(cDr,"mbart"),cDr.forEach(t),Juo=r(SCe," \u2014 "),LP=n(SCe,"A",{href:!0});var fDr=s(LP);Yuo=r(fDr,"MBartForConditionalGeneration"),fDr.forEach(t),Kuo=r(SCe," (mBART model)"),SCe.forEach(t),Zuo=i(N),qu=n(N,"LI",{});var PCe=s(qu);uee=n(PCe,"STRONG",{});var mDr=s(uee);e2o=r(mDr,"megatron-bert"),mDr.forEach(t),o2o=r(PCe," \u2014 "),BP=n(PCe,"A",{href:!0});var gDr=s(BP);r2o=r(gDr,"MegatronBertForMaskedLM"),gDr.forEach(t),t2o=r(PCe," (MegatronBert model)"),PCe.forEach(t),a2o=i(N),Gu=n(N,"LI",{});var $Ce=s(Gu);bee=n($Ce,"STRONG",{});var hDr=s(bee);n2o=r(hDr,"mobilebert"),hDr.forEach(t),s2o=r($Ce," \u2014 "),kP=n($Ce,"A",{href:!0});var pDr=s(kP);l2o=r(pDr,"MobileBertForMaskedLM"),pDr.forEach(t),i2o=r($Ce," (MobileBERT model)"),$Ce.forEach(t),d2o=i(N),Ou=n(N,"LI",{});var ICe=s(Ou);vee=n(ICe,"STRONG",{});var _Dr=s(vee);c2o=r(_Dr,"mpnet"),_Dr.forEach(t),f2o=r(ICe," \u2014 "),xP=n(ICe,"A",{href:!0});var uDr=s(xP);m2o=r(uDr,"MPNetForMaskedLM"),uDr.forEach(t),g2o=r(ICe," (MPNet model)"),ICe.forEach(t),h2o=i(N),Xu=n(N,"LI",{});var jCe=s(Xu);Tee=n(jCe,"STRONG",{});var bDr=s(Tee);p2o=r(bDr,"nystromformer"),bDr.forEach(t),_2o=r(jCe," \u2014 "),RP=n(jCe,"A",{href:!0});var vDr=s(RP);u2o=r(vDr,"NystromformerForMaskedLM"),vDr.forEach(t),b2o=r(jCe," (Nystromformer model)"),jCe.forEach(t),v2o=i(N),zu=n(N,"LI",{});var NCe=s(zu);Fee=n(NCe,"STRONG",{});var TDr=s(Fee);T2o=r(TDr,"perceiver"),TDr.forEach(t),F2o=r(NCe," \u2014 "),SP=n(NCe,"A",{href:!0});var FDr=s(SP);C2o=r(FDr,"PerceiverForMaskedLM"),FDr.forEach(t),M2o=r(NCe," (Perceiver model)"),NCe.forEach(t),E2o=i(N),Vu=n(N,"LI",{});var DCe=s(Vu);Cee=n(DCe,"STRONG",{});var CDr=s(Cee);y2o=r(CDr,"qdqbert"),CDr.forEach(t),w2o=r(DCe," \u2014 "),PP=n(DCe,"A",{href:!0});var MDr=s(PP);A2o=r(MDr,"QDQBertForMaskedLM"),MDr.forEach(t),L2o=r(DCe," (QDQBert model)"),DCe.forEach(t),B2o=i(N),Wu=n(N,"LI",{});var qCe=s(Wu);Mee=n(qCe,"STRONG",{});var EDr=s(Mee);k2o=r(EDr,"reformer"),EDr.forEach(t),x2o=r(qCe," \u2014 "),$P=n(qCe,"A",{href:!0});var yDr=s($P);R2o=r(yDr,"ReformerForMaskedLM"),yDr.forEach(t),S2o=r(qCe," (Reformer model)"),qCe.forEach(t),P2o=i(N),Qu=n(N,"LI",{});var GCe=s(Qu);Eee=n(GCe,"STRONG",{});var wDr=s(Eee);$2o=r(wDr,"rembert"),wDr.forEach(t),I2o=r(GCe," \u2014 "),IP=n(GCe,"A",{href:!0});var ADr=s(IP);j2o=r(ADr,"RemBertForMaskedLM"),ADr.forEach(t),N2o=r(GCe," (RemBERT model)"),GCe.forEach(t),D2o=i(N),Hu=n(N,"LI",{});var OCe=s(Hu);yee=n(OCe,"STRONG",{});var LDr=s(yee);q2o=r(LDr,"roberta"),LDr.forEach(t),G2o=r(OCe," \u2014 "),jP=n(OCe,"A",{href:!0});var BDr=s(jP);O2o=r(BDr,"RobertaForMaskedLM"),BDr.forEach(t),X2o=r(OCe," (RoBERTa model)"),OCe.forEach(t),z2o=i(N),Uu=n(N,"LI",{});var XCe=s(Uu);wee=n(XCe,"STRONG",{});var kDr=s(wee);V2o=r(kDr,"roformer"),kDr.forEach(t),W2o=r(XCe," \u2014 "),NP=n(XCe,"A",{href:!0});var xDr=s(NP);Q2o=r(xDr,"RoFormerForMaskedLM"),xDr.forEach(t),H2o=r(XCe," (RoFormer model)"),XCe.forEach(t),U2o=i(N),Ju=n(N,"LI",{});var zCe=s(Ju);Aee=n(zCe,"STRONG",{});var RDr=s(Aee);J2o=r(RDr,"squeezebert"),RDr.forEach(t),Y2o=r(zCe," \u2014 "),DP=n(zCe,"A",{href:!0});var SDr=s(DP);K2o=r(SDr,"SqueezeBertForMaskedLM"),SDr.forEach(t),Z2o=r(zCe," (SqueezeBERT model)"),zCe.forEach(t),e1o=i(N),Yu=n(N,"LI",{});var VCe=s(Yu);Lee=n(VCe,"STRONG",{});var PDr=s(Lee);o1o=r(PDr,"tapas"),PDr.forEach(t),r1o=r(VCe," \u2014 "),qP=n(VCe,"A",{href:!0});var $Dr=s(qP);t1o=r($Dr,"TapasForMaskedLM"),$Dr.forEach(t),a1o=r(VCe," (TAPAS model)"),VCe.forEach(t),n1o=i(N),Ku=n(N,"LI",{});var WCe=s(Ku);Bee=n(WCe,"STRONG",{});var IDr=s(Bee);s1o=r(IDr,"wav2vec2"),IDr.forEach(t),l1o=r(WCe," \u2014 "),kee=n(WCe,"CODE",{});var jDr=s(kee);i1o=r(jDr,"Wav2Vec2ForMaskedLM"),jDr.forEach(t),d1o=r(WCe,"(Wav2Vec2 model)"),WCe.forEach(t),c1o=i(N),Zu=n(N,"LI",{});var QCe=s(Zu);xee=n(QCe,"STRONG",{});var NDr=s(xee);f1o=r(NDr,"xlm"),NDr.forEach(t),m1o=r(QCe," \u2014 "),GP=n(QCe,"A",{href:!0});var DDr=s(GP);g1o=r(DDr,"XLMWithLMHeadModel"),DDr.forEach(t),h1o=r(QCe," (XLM model)"),QCe.forEach(t),p1o=i(N),e2=n(N,"LI",{});var HCe=s(e2);Ree=n(HCe,"STRONG",{});var qDr=s(Ree);_1o=r(qDr,"xlm-roberta"),qDr.forEach(t),u1o=r(HCe," \u2014 "),OP=n(HCe,"A",{href:!0});var GDr=s(OP);b1o=r(GDr,"XLMRobertaForMaskedLM"),GDr.forEach(t),v1o=r(HCe," (XLM-RoBERTa model)"),HCe.forEach(t),T1o=i(N),o2=n(N,"LI",{});var UCe=s(o2);See=n(UCe,"STRONG",{});var ODr=s(See);F1o=r(ODr,"xlm-roberta-xl"),ODr.forEach(t),C1o=r(UCe," \u2014 "),XP=n(UCe,"A",{href:!0});var XDr=s(XP);M1o=r(XDr,"XLMRobertaXLForMaskedLM"),XDr.forEach(t),E1o=r(UCe," (XLM-RoBERTa-XL model)"),UCe.forEach(t),y1o=i(N),r2=n(N,"LI",{});var JCe=s(r2);Pee=n(JCe,"STRONG",{});var zDr=s(Pee);w1o=r(zDr,"yoso"),zDr.forEach(t),A1o=r(JCe," \u2014 "),zP=n(JCe,"A",{href:!0});var VDr=s(zP);L1o=r(VDr,"YosoForMaskedLM"),VDr.forEach(t),B1o=r(JCe," (YOSO model)"),JCe.forEach(t),N.forEach(t),k1o=i(It),t2=n(It,"P",{});var YCe=s(t2);x1o=r(YCe,"The model is set in evaluation mode by default using "),$ee=n(YCe,"CODE",{});var WDr=s($ee);R1o=r(WDr,"model.eval()"),WDr.forEach(t),S1o=r(YCe,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Iee=n(YCe,"CODE",{});var QDr=s(Iee);P1o=r(QDr,"model.train()"),QDr.forEach(t),YCe.forEach(t),$1o=i(It),jee=n(It,"P",{});var HDr=s(jee);I1o=r(HDr,"Examples:"),HDr.forEach(t),j1o=i(It),m(cE.$$.fragment,It),It.forEach(t),zs.forEach(t),R7e=i(d),Ki=n(d,"H2",{class:!0});var DBe=s(Ki);a2=n(DBe,"A",{id:!0,class:!0,href:!0});var UDr=s(a2);Nee=n(UDr,"SPAN",{});var JDr=s(Nee);m(fE.$$.fragment,JDr),JDr.forEach(t),UDr.forEach(t),N1o=i(DBe),Dee=n(DBe,"SPAN",{});var YDr=s(Dee);D1o=r(YDr,"AutoModelForSeq2SeqLM"),YDr.forEach(t),DBe.forEach(t),S7e=i(d),Uo=n(d,"DIV",{class:!0});var Ws=s(Uo);m(mE.$$.fragment,Ws),q1o=i(Ws),Zi=n(Ws,"P",{});var az=s(Zi);G1o=r(az,`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence language modeling head) when created
with the `),qee=n(az,"CODE",{});var KDr=s(qee);O1o=r(KDr,"from_pretrained()"),KDr.forEach(t),X1o=r(az,"class method or the "),Gee=n(az,"CODE",{});var ZDr=s(Gee);z1o=r(ZDr,"from_config()"),ZDr.forEach(t),V1o=r(az,`class
method.`),az.forEach(t),W1o=i(Ws),gE=n(Ws,"P",{});var qBe=s(gE);Q1o=r(qBe,"This class cannot be instantiated directly using "),Oee=n(qBe,"CODE",{});var eqr=s(Oee);H1o=r(eqr,"__init__()"),eqr.forEach(t),U1o=r(qBe," (throws an error)."),qBe.forEach(t),J1o=i(Ws),Or=n(Ws,"DIV",{class:!0});var Qs=s(Or);m(hE.$$.fragment,Qs),Y1o=i(Qs),Xee=n(Qs,"P",{});var oqr=s(Xee);K1o=r(oqr,"Instantiates one of the model classes of the library (with a sequence-to-sequence language modeling head) from a configuration."),oqr.forEach(t),Z1o=i(Qs),ed=n(Qs,"P",{});var nz=s(ed);ebo=r(nz,`Note:
Loading a model from its configuration file does `),zee=n(nz,"STRONG",{});var rqr=s(zee);obo=r(rqr,"not"),rqr.forEach(t),rbo=r(nz,` load the model weights. It only affects the
model\u2019s configuration. Use `),Vee=n(nz,"CODE",{});var tqr=s(Vee);tbo=r(tqr,"from_pretrained()"),tqr.forEach(t),abo=r(nz,"to load the model weights."),nz.forEach(t),nbo=i(Qs),Wee=n(Qs,"P",{});var aqr=s(Wee);sbo=r(aqr,"Examples:"),aqr.forEach(t),lbo=i(Qs),m(pE.$$.fragment,Qs),Qs.forEach(t),ibo=i(Ws),Pe=n(Ws,"DIV",{class:!0});var jt=s(Pe);m(_E.$$.fragment,jt),dbo=i(jt),Qee=n(jt,"P",{});var nqr=s(Qee);cbo=r(nqr,"Instantiate one of the model classes of the library (with a sequence-to-sequence language modeling head) from a pretrained model."),nqr.forEach(t),fbo=i(jt),Xa=n(jt,"P",{});var g4=s(Xa);mbo=r(g4,"The model class to instantiate is selected based on the "),Hee=n(g4,"CODE",{});var sqr=s(Hee);gbo=r(sqr,"model_type"),sqr.forEach(t),hbo=r(g4,` property of the config object (either
passed as an argument or loaded from `),Uee=n(g4,"CODE",{});var lqr=s(Uee);pbo=r(lqr,"pretrained_model_name_or_path"),lqr.forEach(t),_bo=r(g4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Jee=n(g4,"CODE",{});var iqr=s(Jee);ubo=r(iqr,"pretrained_model_name_or_path"),iqr.forEach(t),bbo=r(g4,":"),g4.forEach(t),vbo=i(jt),ae=n(jt,"UL",{});var le=s(ae);n2=n(le,"LI",{});var KCe=s(n2);Yee=n(KCe,"STRONG",{});var dqr=s(Yee);Tbo=r(dqr,"bart"),dqr.forEach(t),Fbo=r(KCe," \u2014 "),VP=n(KCe,"A",{href:!0});var cqr=s(VP);Cbo=r(cqr,"BartForConditionalGeneration"),cqr.forEach(t),Mbo=r(KCe," (BART model)"),KCe.forEach(t),Ebo=i(le),s2=n(le,"LI",{});var ZCe=s(s2);Kee=n(ZCe,"STRONG",{});var fqr=s(Kee);ybo=r(fqr,"bigbird_pegasus"),fqr.forEach(t),wbo=r(ZCe," \u2014 "),WP=n(ZCe,"A",{href:!0});var mqr=s(WP);Abo=r(mqr,"BigBirdPegasusForConditionalGeneration"),mqr.forEach(t),Lbo=r(ZCe," (BigBirdPegasus model)"),ZCe.forEach(t),Bbo=i(le),l2=n(le,"LI",{});var e4e=s(l2);Zee=n(e4e,"STRONG",{});var gqr=s(Zee);kbo=r(gqr,"blenderbot"),gqr.forEach(t),xbo=r(e4e," \u2014 "),QP=n(e4e,"A",{href:!0});var hqr=s(QP);Rbo=r(hqr,"BlenderbotForConditionalGeneration"),hqr.forEach(t),Sbo=r(e4e," (Blenderbot model)"),e4e.forEach(t),Pbo=i(le),i2=n(le,"LI",{});var o4e=s(i2);eoe=n(o4e,"STRONG",{});var pqr=s(eoe);$bo=r(pqr,"blenderbot-small"),pqr.forEach(t),Ibo=r(o4e," \u2014 "),HP=n(o4e,"A",{href:!0});var _qr=s(HP);jbo=r(_qr,"BlenderbotSmallForConditionalGeneration"),_qr.forEach(t),Nbo=r(o4e," (BlenderbotSmall model)"),o4e.forEach(t),Dbo=i(le),d2=n(le,"LI",{});var r4e=s(d2);ooe=n(r4e,"STRONG",{});var uqr=s(ooe);qbo=r(uqr,"encoder-decoder"),uqr.forEach(t),Gbo=r(r4e," \u2014 "),UP=n(r4e,"A",{href:!0});var bqr=s(UP);Obo=r(bqr,"EncoderDecoderModel"),bqr.forEach(t),Xbo=r(r4e," (Encoder decoder model)"),r4e.forEach(t),zbo=i(le),c2=n(le,"LI",{});var t4e=s(c2);roe=n(t4e,"STRONG",{});var vqr=s(roe);Vbo=r(vqr,"fsmt"),vqr.forEach(t),Wbo=r(t4e," \u2014 "),JP=n(t4e,"A",{href:!0});var Tqr=s(JP);Qbo=r(Tqr,"FSMTForConditionalGeneration"),Tqr.forEach(t),Hbo=r(t4e," (FairSeq Machine-Translation model)"),t4e.forEach(t),Ubo=i(le),f2=n(le,"LI",{});var a4e=s(f2);toe=n(a4e,"STRONG",{});var Fqr=s(toe);Jbo=r(Fqr,"led"),Fqr.forEach(t),Ybo=r(a4e," \u2014 "),YP=n(a4e,"A",{href:!0});var Cqr=s(YP);Kbo=r(Cqr,"LEDForConditionalGeneration"),Cqr.forEach(t),Zbo=r(a4e," (LED model)"),a4e.forEach(t),e5o=i(le),m2=n(le,"LI",{});var n4e=s(m2);aoe=n(n4e,"STRONG",{});var Mqr=s(aoe);o5o=r(Mqr,"m2m_100"),Mqr.forEach(t),r5o=r(n4e," \u2014 "),KP=n(n4e,"A",{href:!0});var Eqr=s(KP);t5o=r(Eqr,"M2M100ForConditionalGeneration"),Eqr.forEach(t),a5o=r(n4e," (M2M100 model)"),n4e.forEach(t),n5o=i(le),g2=n(le,"LI",{});var s4e=s(g2);noe=n(s4e,"STRONG",{});var yqr=s(noe);s5o=r(yqr,"marian"),yqr.forEach(t),l5o=r(s4e," \u2014 "),ZP=n(s4e,"A",{href:!0});var wqr=s(ZP);i5o=r(wqr,"MarianMTModel"),wqr.forEach(t),d5o=r(s4e," (Marian model)"),s4e.forEach(t),c5o=i(le),h2=n(le,"LI",{});var l4e=s(h2);soe=n(l4e,"STRONG",{});var Aqr=s(soe);f5o=r(Aqr,"mbart"),Aqr.forEach(t),m5o=r(l4e," \u2014 "),e$=n(l4e,"A",{href:!0});var Lqr=s(e$);g5o=r(Lqr,"MBartForConditionalGeneration"),Lqr.forEach(t),h5o=r(l4e," (mBART model)"),l4e.forEach(t),p5o=i(le),p2=n(le,"LI",{});var i4e=s(p2);loe=n(i4e,"STRONG",{});var Bqr=s(loe);_5o=r(Bqr,"mt5"),Bqr.forEach(t),u5o=r(i4e," \u2014 "),o$=n(i4e,"A",{href:!0});var kqr=s(o$);b5o=r(kqr,"MT5ForConditionalGeneration"),kqr.forEach(t),v5o=r(i4e," (mT5 model)"),i4e.forEach(t),T5o=i(le),_2=n(le,"LI",{});var d4e=s(_2);ioe=n(d4e,"STRONG",{});var xqr=s(ioe);F5o=r(xqr,"pegasus"),xqr.forEach(t),C5o=r(d4e," \u2014 "),r$=n(d4e,"A",{href:!0});var Rqr=s(r$);M5o=r(Rqr,"PegasusForConditionalGeneration"),Rqr.forEach(t),E5o=r(d4e," (Pegasus model)"),d4e.forEach(t),y5o=i(le),u2=n(le,"LI",{});var c4e=s(u2);doe=n(c4e,"STRONG",{});var Sqr=s(doe);w5o=r(Sqr,"plbart"),Sqr.forEach(t),A5o=r(c4e," \u2014 "),t$=n(c4e,"A",{href:!0});var Pqr=s(t$);L5o=r(Pqr,"PLBartForConditionalGeneration"),Pqr.forEach(t),B5o=r(c4e," (PLBart model)"),c4e.forEach(t),k5o=i(le),b2=n(le,"LI",{});var f4e=s(b2);coe=n(f4e,"STRONG",{});var $qr=s(coe);x5o=r($qr,"prophetnet"),$qr.forEach(t),R5o=r(f4e," \u2014 "),a$=n(f4e,"A",{href:!0});var Iqr=s(a$);S5o=r(Iqr,"ProphetNetForConditionalGeneration"),Iqr.forEach(t),P5o=r(f4e," (ProphetNet model)"),f4e.forEach(t),$5o=i(le),v2=n(le,"LI",{});var m4e=s(v2);foe=n(m4e,"STRONG",{});var jqr=s(foe);I5o=r(jqr,"t5"),jqr.forEach(t),j5o=r(m4e," \u2014 "),n$=n(m4e,"A",{href:!0});var Nqr=s(n$);N5o=r(Nqr,"T5ForConditionalGeneration"),Nqr.forEach(t),D5o=r(m4e," (T5 model)"),m4e.forEach(t),q5o=i(le),T2=n(le,"LI",{});var g4e=s(T2);moe=n(g4e,"STRONG",{});var Dqr=s(moe);G5o=r(Dqr,"xlm-prophetnet"),Dqr.forEach(t),O5o=r(g4e," \u2014 "),s$=n(g4e,"A",{href:!0});var qqr=s(s$);X5o=r(qqr,"XLMProphetNetForConditionalGeneration"),qqr.forEach(t),z5o=r(g4e," (XLMProphetNet model)"),g4e.forEach(t),le.forEach(t),V5o=i(jt),F2=n(jt,"P",{});var h4e=s(F2);W5o=r(h4e,"The model is set in evaluation mode by default using "),goe=n(h4e,"CODE",{});var Gqr=s(goe);Q5o=r(Gqr,"model.eval()"),Gqr.forEach(t),H5o=r(h4e,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),hoe=n(h4e,"CODE",{});var Oqr=s(hoe);U5o=r(Oqr,"model.train()"),Oqr.forEach(t),h4e.forEach(t),J5o=i(jt),poe=n(jt,"P",{});var Xqr=s(poe);Y5o=r(Xqr,"Examples:"),Xqr.forEach(t),K5o=i(jt),m(uE.$$.fragment,jt),jt.forEach(t),Ws.forEach(t),P7e=i(d),od=n(d,"H2",{class:!0});var GBe=s(od);C2=n(GBe,"A",{id:!0,class:!0,href:!0});var zqr=s(C2);_oe=n(zqr,"SPAN",{});var Vqr=s(_oe);m(bE.$$.fragment,Vqr),Vqr.forEach(t),zqr.forEach(t),Z5o=i(GBe),uoe=n(GBe,"SPAN",{});var Wqr=s(uoe);evo=r(Wqr,"AutoModelForSequenceClassification"),Wqr.forEach(t),GBe.forEach(t),$7e=i(d),Jo=n(d,"DIV",{class:!0});var Hs=s(Jo);m(vE.$$.fragment,Hs),ovo=i(Hs),rd=n(Hs,"P",{});var sz=s(rd);rvo=r(sz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence classification head) when created
with the `),boe=n(sz,"CODE",{});var Qqr=s(boe);tvo=r(Qqr,"from_pretrained()"),Qqr.forEach(t),avo=r(sz,"class method or the "),voe=n(sz,"CODE",{});var Hqr=s(voe);nvo=r(Hqr,"from_config()"),Hqr.forEach(t),svo=r(sz,`class
method.`),sz.forEach(t),lvo=i(Hs),TE=n(Hs,"P",{});var OBe=s(TE);ivo=r(OBe,"This class cannot be instantiated directly using "),Toe=n(OBe,"CODE",{});var Uqr=s(Toe);dvo=r(Uqr,"__init__()"),Uqr.forEach(t),cvo=r(OBe," (throws an error)."),OBe.forEach(t),fvo=i(Hs),Xr=n(Hs,"DIV",{class:!0});var Us=s(Xr);m(FE.$$.fragment,Us),mvo=i(Us),Foe=n(Us,"P",{});var Jqr=s(Foe);gvo=r(Jqr,"Instantiates one of the model classes of the library (with a sequence classification head) from a configuration."),Jqr.forEach(t),hvo=i(Us),td=n(Us,"P",{});var lz=s(td);pvo=r(lz,`Note:
Loading a model from its configuration file does `),Coe=n(lz,"STRONG",{});var Yqr=s(Coe);_vo=r(Yqr,"not"),Yqr.forEach(t),uvo=r(lz,` load the model weights. It only affects the
model\u2019s configuration. Use `),Moe=n(lz,"CODE",{});var Kqr=s(Moe);bvo=r(Kqr,"from_pretrained()"),Kqr.forEach(t),vvo=r(lz,"to load the model weights."),lz.forEach(t),Tvo=i(Us),Eoe=n(Us,"P",{});var Zqr=s(Eoe);Fvo=r(Zqr,"Examples:"),Zqr.forEach(t),Cvo=i(Us),m(CE.$$.fragment,Us),Us.forEach(t),Mvo=i(Hs),$e=n(Hs,"DIV",{class:!0});var Nt=s($e);m(ME.$$.fragment,Nt),Evo=i(Nt),yoe=n(Nt,"P",{});var eGr=s(yoe);yvo=r(eGr,"Instantiate one of the model classes of the library (with a sequence classification head) from a pretrained model."),eGr.forEach(t),wvo=i(Nt),za=n(Nt,"P",{});var h4=s(za);Avo=r(h4,"The model class to instantiate is selected based on the "),woe=n(h4,"CODE",{});var oGr=s(woe);Lvo=r(oGr,"model_type"),oGr.forEach(t),Bvo=r(h4,` property of the config object (either
passed as an argument or loaded from `),Aoe=n(h4,"CODE",{});var rGr=s(Aoe);kvo=r(rGr,"pretrained_model_name_or_path"),rGr.forEach(t),xvo=r(h4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Loe=n(h4,"CODE",{});var tGr=s(Loe);Rvo=r(tGr,"pretrained_model_name_or_path"),tGr.forEach(t),Svo=r(h4,":"),h4.forEach(t),Pvo=i(Nt),A=n(Nt,"UL",{});var L=s(A);M2=n(L,"LI",{});var p4e=s(M2);Boe=n(p4e,"STRONG",{});var aGr=s(Boe);$vo=r(aGr,"albert"),aGr.forEach(t),Ivo=r(p4e," \u2014 "),l$=n(p4e,"A",{href:!0});var nGr=s(l$);jvo=r(nGr,"AlbertForSequenceClassification"),nGr.forEach(t),Nvo=r(p4e," (ALBERT model)"),p4e.forEach(t),Dvo=i(L),E2=n(L,"LI",{});var _4e=s(E2);koe=n(_4e,"STRONG",{});var sGr=s(koe);qvo=r(sGr,"bart"),sGr.forEach(t),Gvo=r(_4e," \u2014 "),i$=n(_4e,"A",{href:!0});var lGr=s(i$);Ovo=r(lGr,"BartForSequenceClassification"),lGr.forEach(t),Xvo=r(_4e," (BART model)"),_4e.forEach(t),zvo=i(L),y2=n(L,"LI",{});var u4e=s(y2);xoe=n(u4e,"STRONG",{});var iGr=s(xoe);Vvo=r(iGr,"bert"),iGr.forEach(t),Wvo=r(u4e," \u2014 "),d$=n(u4e,"A",{href:!0});var dGr=s(d$);Qvo=r(dGr,"BertForSequenceClassification"),dGr.forEach(t),Hvo=r(u4e," (BERT model)"),u4e.forEach(t),Uvo=i(L),w2=n(L,"LI",{});var b4e=s(w2);Roe=n(b4e,"STRONG",{});var cGr=s(Roe);Jvo=r(cGr,"big_bird"),cGr.forEach(t),Yvo=r(b4e," \u2014 "),c$=n(b4e,"A",{href:!0});var fGr=s(c$);Kvo=r(fGr,"BigBirdForSequenceClassification"),fGr.forEach(t),Zvo=r(b4e," (BigBird model)"),b4e.forEach(t),e6o=i(L),A2=n(L,"LI",{});var v4e=s(A2);Soe=n(v4e,"STRONG",{});var mGr=s(Soe);o6o=r(mGr,"bigbird_pegasus"),mGr.forEach(t),r6o=r(v4e," \u2014 "),f$=n(v4e,"A",{href:!0});var gGr=s(f$);t6o=r(gGr,"BigBirdPegasusForSequenceClassification"),gGr.forEach(t),a6o=r(v4e," (BigBirdPegasus model)"),v4e.forEach(t),n6o=i(L),L2=n(L,"LI",{});var T4e=s(L2);Poe=n(T4e,"STRONG",{});var hGr=s(Poe);s6o=r(hGr,"camembert"),hGr.forEach(t),l6o=r(T4e," \u2014 "),m$=n(T4e,"A",{href:!0});var pGr=s(m$);i6o=r(pGr,"CamembertForSequenceClassification"),pGr.forEach(t),d6o=r(T4e," (CamemBERT model)"),T4e.forEach(t),c6o=i(L),B2=n(L,"LI",{});var F4e=s(B2);$oe=n(F4e,"STRONG",{});var _Gr=s($oe);f6o=r(_Gr,"canine"),_Gr.forEach(t),m6o=r(F4e," \u2014 "),g$=n(F4e,"A",{href:!0});var uGr=s(g$);g6o=r(uGr,"CanineForSequenceClassification"),uGr.forEach(t),h6o=r(F4e," (Canine model)"),F4e.forEach(t),p6o=i(L),k2=n(L,"LI",{});var C4e=s(k2);Ioe=n(C4e,"STRONG",{});var bGr=s(Ioe);_6o=r(bGr,"convbert"),bGr.forEach(t),u6o=r(C4e," \u2014 "),h$=n(C4e,"A",{href:!0});var vGr=s(h$);b6o=r(vGr,"ConvBertForSequenceClassification"),vGr.forEach(t),v6o=r(C4e," (ConvBERT model)"),C4e.forEach(t),T6o=i(L),x2=n(L,"LI",{});var M4e=s(x2);joe=n(M4e,"STRONG",{});var TGr=s(joe);F6o=r(TGr,"ctrl"),TGr.forEach(t),C6o=r(M4e," \u2014 "),p$=n(M4e,"A",{href:!0});var FGr=s(p$);M6o=r(FGr,"CTRLForSequenceClassification"),FGr.forEach(t),E6o=r(M4e," (CTRL model)"),M4e.forEach(t),y6o=i(L),R2=n(L,"LI",{});var E4e=s(R2);Noe=n(E4e,"STRONG",{});var CGr=s(Noe);w6o=r(CGr,"deberta"),CGr.forEach(t),A6o=r(E4e," \u2014 "),_$=n(E4e,"A",{href:!0});var MGr=s(_$);L6o=r(MGr,"DebertaForSequenceClassification"),MGr.forEach(t),B6o=r(E4e," (DeBERTa model)"),E4e.forEach(t),k6o=i(L),S2=n(L,"LI",{});var y4e=s(S2);Doe=n(y4e,"STRONG",{});var EGr=s(Doe);x6o=r(EGr,"deberta-v2"),EGr.forEach(t),R6o=r(y4e," \u2014 "),u$=n(y4e,"A",{href:!0});var yGr=s(u$);S6o=r(yGr,"DebertaV2ForSequenceClassification"),yGr.forEach(t),P6o=r(y4e," (DeBERTa-v2 model)"),y4e.forEach(t),$6o=i(L),P2=n(L,"LI",{});var w4e=s(P2);qoe=n(w4e,"STRONG",{});var wGr=s(qoe);I6o=r(wGr,"distilbert"),wGr.forEach(t),j6o=r(w4e," \u2014 "),b$=n(w4e,"A",{href:!0});var AGr=s(b$);N6o=r(AGr,"DistilBertForSequenceClassification"),AGr.forEach(t),D6o=r(w4e," (DistilBERT model)"),w4e.forEach(t),q6o=i(L),$2=n(L,"LI",{});var A4e=s($2);Goe=n(A4e,"STRONG",{});var LGr=s(Goe);G6o=r(LGr,"electra"),LGr.forEach(t),O6o=r(A4e," \u2014 "),v$=n(A4e,"A",{href:!0});var BGr=s(v$);X6o=r(BGr,"ElectraForSequenceClassification"),BGr.forEach(t),z6o=r(A4e," (ELECTRA model)"),A4e.forEach(t),V6o=i(L),I2=n(L,"LI",{});var L4e=s(I2);Ooe=n(L4e,"STRONG",{});var kGr=s(Ooe);W6o=r(kGr,"flaubert"),kGr.forEach(t),Q6o=r(L4e," \u2014 "),T$=n(L4e,"A",{href:!0});var xGr=s(T$);H6o=r(xGr,"FlaubertForSequenceClassification"),xGr.forEach(t),U6o=r(L4e," (FlauBERT model)"),L4e.forEach(t),J6o=i(L),j2=n(L,"LI",{});var B4e=s(j2);Xoe=n(B4e,"STRONG",{});var RGr=s(Xoe);Y6o=r(RGr,"fnet"),RGr.forEach(t),K6o=r(B4e," \u2014 "),F$=n(B4e,"A",{href:!0});var SGr=s(F$);Z6o=r(SGr,"FNetForSequenceClassification"),SGr.forEach(t),eTo=r(B4e," (FNet model)"),B4e.forEach(t),oTo=i(L),N2=n(L,"LI",{});var k4e=s(N2);zoe=n(k4e,"STRONG",{});var PGr=s(zoe);rTo=r(PGr,"funnel"),PGr.forEach(t),tTo=r(k4e," \u2014 "),C$=n(k4e,"A",{href:!0});var $Gr=s(C$);aTo=r($Gr,"FunnelForSequenceClassification"),$Gr.forEach(t),nTo=r(k4e," (Funnel Transformer model)"),k4e.forEach(t),sTo=i(L),D2=n(L,"LI",{});var x4e=s(D2);Voe=n(x4e,"STRONG",{});var IGr=s(Voe);lTo=r(IGr,"gpt2"),IGr.forEach(t),iTo=r(x4e," \u2014 "),M$=n(x4e,"A",{href:!0});var jGr=s(M$);dTo=r(jGr,"GPT2ForSequenceClassification"),jGr.forEach(t),cTo=r(x4e," (OpenAI GPT-2 model)"),x4e.forEach(t),fTo=i(L),q2=n(L,"LI",{});var R4e=s(q2);Woe=n(R4e,"STRONG",{});var NGr=s(Woe);mTo=r(NGr,"gpt_neo"),NGr.forEach(t),gTo=r(R4e," \u2014 "),E$=n(R4e,"A",{href:!0});var DGr=s(E$);hTo=r(DGr,"GPTNeoForSequenceClassification"),DGr.forEach(t),pTo=r(R4e," (GPT Neo model)"),R4e.forEach(t),_To=i(L),G2=n(L,"LI",{});var S4e=s(G2);Qoe=n(S4e,"STRONG",{});var qGr=s(Qoe);uTo=r(qGr,"gptj"),qGr.forEach(t),bTo=r(S4e," \u2014 "),y$=n(S4e,"A",{href:!0});var GGr=s(y$);vTo=r(GGr,"GPTJForSequenceClassification"),GGr.forEach(t),TTo=r(S4e," (GPT-J model)"),S4e.forEach(t),FTo=i(L),O2=n(L,"LI",{});var P4e=s(O2);Hoe=n(P4e,"STRONG",{});var OGr=s(Hoe);CTo=r(OGr,"ibert"),OGr.forEach(t),MTo=r(P4e," \u2014 "),w$=n(P4e,"A",{href:!0});var XGr=s(w$);ETo=r(XGr,"IBertForSequenceClassification"),XGr.forEach(t),yTo=r(P4e," (I-BERT model)"),P4e.forEach(t),wTo=i(L),X2=n(L,"LI",{});var $4e=s(X2);Uoe=n($4e,"STRONG",{});var zGr=s(Uoe);ATo=r(zGr,"layoutlm"),zGr.forEach(t),LTo=r($4e," \u2014 "),A$=n($4e,"A",{href:!0});var VGr=s(A$);BTo=r(VGr,"LayoutLMForSequenceClassification"),VGr.forEach(t),kTo=r($4e," (LayoutLM model)"),$4e.forEach(t),xTo=i(L),z2=n(L,"LI",{});var I4e=s(z2);Joe=n(I4e,"STRONG",{});var WGr=s(Joe);RTo=r(WGr,"layoutlmv2"),WGr.forEach(t),STo=r(I4e," \u2014 "),L$=n(I4e,"A",{href:!0});var QGr=s(L$);PTo=r(QGr,"LayoutLMv2ForSequenceClassification"),QGr.forEach(t),$To=r(I4e," (LayoutLMv2 model)"),I4e.forEach(t),ITo=i(L),V2=n(L,"LI",{});var j4e=s(V2);Yoe=n(j4e,"STRONG",{});var HGr=s(Yoe);jTo=r(HGr,"led"),HGr.forEach(t),NTo=r(j4e," \u2014 "),B$=n(j4e,"A",{href:!0});var UGr=s(B$);DTo=r(UGr,"LEDForSequenceClassification"),UGr.forEach(t),qTo=r(j4e," (LED model)"),j4e.forEach(t),GTo=i(L),W2=n(L,"LI",{});var N4e=s(W2);Koe=n(N4e,"STRONG",{});var JGr=s(Koe);OTo=r(JGr,"longformer"),JGr.forEach(t),XTo=r(N4e," \u2014 "),k$=n(N4e,"A",{href:!0});var YGr=s(k$);zTo=r(YGr,"LongformerForSequenceClassification"),YGr.forEach(t),VTo=r(N4e," (Longformer model)"),N4e.forEach(t),WTo=i(L),Q2=n(L,"LI",{});var D4e=s(Q2);Zoe=n(D4e,"STRONG",{});var KGr=s(Zoe);QTo=r(KGr,"mbart"),KGr.forEach(t),HTo=r(D4e," \u2014 "),x$=n(D4e,"A",{href:!0});var ZGr=s(x$);UTo=r(ZGr,"MBartForSequenceClassification"),ZGr.forEach(t),JTo=r(D4e," (mBART model)"),D4e.forEach(t),YTo=i(L),H2=n(L,"LI",{});var q4e=s(H2);ere=n(q4e,"STRONG",{});var eOr=s(ere);KTo=r(eOr,"megatron-bert"),eOr.forEach(t),ZTo=r(q4e," \u2014 "),R$=n(q4e,"A",{href:!0});var oOr=s(R$);e8o=r(oOr,"MegatronBertForSequenceClassification"),oOr.forEach(t),o8o=r(q4e," (MegatronBert model)"),q4e.forEach(t),r8o=i(L),U2=n(L,"LI",{});var G4e=s(U2);ore=n(G4e,"STRONG",{});var rOr=s(ore);t8o=r(rOr,"mobilebert"),rOr.forEach(t),a8o=r(G4e," \u2014 "),S$=n(G4e,"A",{href:!0});var tOr=s(S$);n8o=r(tOr,"MobileBertForSequenceClassification"),tOr.forEach(t),s8o=r(G4e," (MobileBERT model)"),G4e.forEach(t),l8o=i(L),J2=n(L,"LI",{});var O4e=s(J2);rre=n(O4e,"STRONG",{});var aOr=s(rre);i8o=r(aOr,"mpnet"),aOr.forEach(t),d8o=r(O4e," \u2014 "),P$=n(O4e,"A",{href:!0});var nOr=s(P$);c8o=r(nOr,"MPNetForSequenceClassification"),nOr.forEach(t),f8o=r(O4e," (MPNet model)"),O4e.forEach(t),m8o=i(L),Y2=n(L,"LI",{});var X4e=s(Y2);tre=n(X4e,"STRONG",{});var sOr=s(tre);g8o=r(sOr,"nystromformer"),sOr.forEach(t),h8o=r(X4e," \u2014 "),$$=n(X4e,"A",{href:!0});var lOr=s($$);p8o=r(lOr,"NystromformerForSequenceClassification"),lOr.forEach(t),_8o=r(X4e," (Nystromformer model)"),X4e.forEach(t),u8o=i(L),K2=n(L,"LI",{});var z4e=s(K2);are=n(z4e,"STRONG",{});var iOr=s(are);b8o=r(iOr,"openai-gpt"),iOr.forEach(t),v8o=r(z4e," \u2014 "),I$=n(z4e,"A",{href:!0});var dOr=s(I$);T8o=r(dOr,"OpenAIGPTForSequenceClassification"),dOr.forEach(t),F8o=r(z4e," (OpenAI GPT model)"),z4e.forEach(t),C8o=i(L),Z2=n(L,"LI",{});var V4e=s(Z2);nre=n(V4e,"STRONG",{});var cOr=s(nre);M8o=r(cOr,"perceiver"),cOr.forEach(t),E8o=r(V4e," \u2014 "),j$=n(V4e,"A",{href:!0});var fOr=s(j$);y8o=r(fOr,"PerceiverForSequenceClassification"),fOr.forEach(t),w8o=r(V4e," (Perceiver model)"),V4e.forEach(t),A8o=i(L),e1=n(L,"LI",{});var W4e=s(e1);sre=n(W4e,"STRONG",{});var mOr=s(sre);L8o=r(mOr,"plbart"),mOr.forEach(t),B8o=r(W4e," \u2014 "),N$=n(W4e,"A",{href:!0});var gOr=s(N$);k8o=r(gOr,"PLBartForSequenceClassification"),gOr.forEach(t),x8o=r(W4e," (PLBart model)"),W4e.forEach(t),R8o=i(L),o1=n(L,"LI",{});var Q4e=s(o1);lre=n(Q4e,"STRONG",{});var hOr=s(lre);S8o=r(hOr,"qdqbert"),hOr.forEach(t),P8o=r(Q4e," \u2014 "),D$=n(Q4e,"A",{href:!0});var pOr=s(D$);$8o=r(pOr,"QDQBertForSequenceClassification"),pOr.forEach(t),I8o=r(Q4e," (QDQBert model)"),Q4e.forEach(t),j8o=i(L),r1=n(L,"LI",{});var H4e=s(r1);ire=n(H4e,"STRONG",{});var _Or=s(ire);N8o=r(_Or,"reformer"),_Or.forEach(t),D8o=r(H4e," \u2014 "),q$=n(H4e,"A",{href:!0});var uOr=s(q$);q8o=r(uOr,"ReformerForSequenceClassification"),uOr.forEach(t),G8o=r(H4e," (Reformer model)"),H4e.forEach(t),O8o=i(L),t1=n(L,"LI",{});var U4e=s(t1);dre=n(U4e,"STRONG",{});var bOr=s(dre);X8o=r(bOr,"rembert"),bOr.forEach(t),z8o=r(U4e," \u2014 "),G$=n(U4e,"A",{href:!0});var vOr=s(G$);V8o=r(vOr,"RemBertForSequenceClassification"),vOr.forEach(t),W8o=r(U4e," (RemBERT model)"),U4e.forEach(t),Q8o=i(L),a1=n(L,"LI",{});var J4e=s(a1);cre=n(J4e,"STRONG",{});var TOr=s(cre);H8o=r(TOr,"roberta"),TOr.forEach(t),U8o=r(J4e," \u2014 "),O$=n(J4e,"A",{href:!0});var FOr=s(O$);J8o=r(FOr,"RobertaForSequenceClassification"),FOr.forEach(t),Y8o=r(J4e," (RoBERTa model)"),J4e.forEach(t),K8o=i(L),n1=n(L,"LI",{});var Y4e=s(n1);fre=n(Y4e,"STRONG",{});var COr=s(fre);Z8o=r(COr,"roformer"),COr.forEach(t),eFo=r(Y4e," \u2014 "),X$=n(Y4e,"A",{href:!0});var MOr=s(X$);oFo=r(MOr,"RoFormerForSequenceClassification"),MOr.forEach(t),rFo=r(Y4e," (RoFormer model)"),Y4e.forEach(t),tFo=i(L),s1=n(L,"LI",{});var K4e=s(s1);mre=n(K4e,"STRONG",{});var EOr=s(mre);aFo=r(EOr,"squeezebert"),EOr.forEach(t),nFo=r(K4e," \u2014 "),z$=n(K4e,"A",{href:!0});var yOr=s(z$);sFo=r(yOr,"SqueezeBertForSequenceClassification"),yOr.forEach(t),lFo=r(K4e," (SqueezeBERT model)"),K4e.forEach(t),iFo=i(L),l1=n(L,"LI",{});var Z4e=s(l1);gre=n(Z4e,"STRONG",{});var wOr=s(gre);dFo=r(wOr,"tapas"),wOr.forEach(t),cFo=r(Z4e," \u2014 "),V$=n(Z4e,"A",{href:!0});var AOr=s(V$);fFo=r(AOr,"TapasForSequenceClassification"),AOr.forEach(t),mFo=r(Z4e," (TAPAS model)"),Z4e.forEach(t),gFo=i(L),i1=n(L,"LI",{});var eMe=s(i1);hre=n(eMe,"STRONG",{});var LOr=s(hre);hFo=r(LOr,"transfo-xl"),LOr.forEach(t),pFo=r(eMe," \u2014 "),W$=n(eMe,"A",{href:!0});var BOr=s(W$);_Fo=r(BOr,"TransfoXLForSequenceClassification"),BOr.forEach(t),uFo=r(eMe," (Transformer-XL model)"),eMe.forEach(t),bFo=i(L),d1=n(L,"LI",{});var oMe=s(d1);pre=n(oMe,"STRONG",{});var kOr=s(pre);vFo=r(kOr,"xlm"),kOr.forEach(t),TFo=r(oMe," \u2014 "),Q$=n(oMe,"A",{href:!0});var xOr=s(Q$);FFo=r(xOr,"XLMForSequenceClassification"),xOr.forEach(t),CFo=r(oMe," (XLM model)"),oMe.forEach(t),MFo=i(L),c1=n(L,"LI",{});var rMe=s(c1);_re=n(rMe,"STRONG",{});var ROr=s(_re);EFo=r(ROr,"xlm-roberta"),ROr.forEach(t),yFo=r(rMe," \u2014 "),H$=n(rMe,"A",{href:!0});var SOr=s(H$);wFo=r(SOr,"XLMRobertaForSequenceClassification"),SOr.forEach(t),AFo=r(rMe," (XLM-RoBERTa model)"),rMe.forEach(t),LFo=i(L),f1=n(L,"LI",{});var tMe=s(f1);ure=n(tMe,"STRONG",{});var POr=s(ure);BFo=r(POr,"xlm-roberta-xl"),POr.forEach(t),kFo=r(tMe," \u2014 "),U$=n(tMe,"A",{href:!0});var $Or=s(U$);xFo=r($Or,"XLMRobertaXLForSequenceClassification"),$Or.forEach(t),RFo=r(tMe," (XLM-RoBERTa-XL model)"),tMe.forEach(t),SFo=i(L),m1=n(L,"LI",{});var aMe=s(m1);bre=n(aMe,"STRONG",{});var IOr=s(bre);PFo=r(IOr,"xlnet"),IOr.forEach(t),$Fo=r(aMe," \u2014 "),J$=n(aMe,"A",{href:!0});var jOr=s(J$);IFo=r(jOr,"XLNetForSequenceClassification"),jOr.forEach(t),jFo=r(aMe," (XLNet model)"),aMe.forEach(t),NFo=i(L),g1=n(L,"LI",{});var nMe=s(g1);vre=n(nMe,"STRONG",{});var NOr=s(vre);DFo=r(NOr,"yoso"),NOr.forEach(t),qFo=r(nMe," \u2014 "),Y$=n(nMe,"A",{href:!0});var DOr=s(Y$);GFo=r(DOr,"YosoForSequenceClassification"),DOr.forEach(t),OFo=r(nMe," (YOSO model)"),nMe.forEach(t),L.forEach(t),XFo=i(Nt),h1=n(Nt,"P",{});var sMe=s(h1);zFo=r(sMe,"The model is set in evaluation mode by default using "),Tre=n(sMe,"CODE",{});var qOr=s(Tre);VFo=r(qOr,"model.eval()"),qOr.forEach(t),WFo=r(sMe,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Fre=n(sMe,"CODE",{});var GOr=s(Fre);QFo=r(GOr,"model.train()"),GOr.forEach(t),sMe.forEach(t),HFo=i(Nt),Cre=n(Nt,"P",{});var OOr=s(Cre);UFo=r(OOr,"Examples:"),OOr.forEach(t),JFo=i(Nt),m(EE.$$.fragment,Nt),Nt.forEach(t),Hs.forEach(t),I7e=i(d),ad=n(d,"H2",{class:!0});var XBe=s(ad);p1=n(XBe,"A",{id:!0,class:!0,href:!0});var XOr=s(p1);Mre=n(XOr,"SPAN",{});var zOr=s(Mre);m(yE.$$.fragment,zOr),zOr.forEach(t),XOr.forEach(t),YFo=i(XBe),Ere=n(XBe,"SPAN",{});var VOr=s(Ere);KFo=r(VOr,"AutoModelForMultipleChoice"),VOr.forEach(t),XBe.forEach(t),j7e=i(d),Yo=n(d,"DIV",{class:!0});var Js=s(Yo);m(wE.$$.fragment,Js),ZFo=i(Js),nd=n(Js,"P",{});var iz=s(nd);eCo=r(iz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a multiple choice head) when created
with the `),yre=n(iz,"CODE",{});var WOr=s(yre);oCo=r(WOr,"from_pretrained()"),WOr.forEach(t),rCo=r(iz,"class method or the "),wre=n(iz,"CODE",{});var QOr=s(wre);tCo=r(QOr,"from_config()"),QOr.forEach(t),aCo=r(iz,`class
method.`),iz.forEach(t),nCo=i(Js),AE=n(Js,"P",{});var zBe=s(AE);sCo=r(zBe,"This class cannot be instantiated directly using "),Are=n(zBe,"CODE",{});var HOr=s(Are);lCo=r(HOr,"__init__()"),HOr.forEach(t),iCo=r(zBe," (throws an error)."),zBe.forEach(t),dCo=i(Js),zr=n(Js,"DIV",{class:!0});var Ys=s(zr);m(LE.$$.fragment,Ys),cCo=i(Ys),Lre=n(Ys,"P",{});var UOr=s(Lre);fCo=r(UOr,"Instantiates one of the model classes of the library (with a multiple choice head) from a configuration."),UOr.forEach(t),mCo=i(Ys),sd=n(Ys,"P",{});var dz=s(sd);gCo=r(dz,`Note:
Loading a model from its configuration file does `),Bre=n(dz,"STRONG",{});var JOr=s(Bre);hCo=r(JOr,"not"),JOr.forEach(t),pCo=r(dz,` load the model weights. It only affects the
model\u2019s configuration. Use `),kre=n(dz,"CODE",{});var YOr=s(kre);_Co=r(YOr,"from_pretrained()"),YOr.forEach(t),uCo=r(dz,"to load the model weights."),dz.forEach(t),bCo=i(Ys),xre=n(Ys,"P",{});var KOr=s(xre);vCo=r(KOr,"Examples:"),KOr.forEach(t),TCo=i(Ys),m(BE.$$.fragment,Ys),Ys.forEach(t),FCo=i(Js),Ie=n(Js,"DIV",{class:!0});var Dt=s(Ie);m(kE.$$.fragment,Dt),CCo=i(Dt),Rre=n(Dt,"P",{});var ZOr=s(Rre);MCo=r(ZOr,"Instantiate one of the model classes of the library (with a multiple choice head) from a pretrained model."),ZOr.forEach(t),ECo=i(Dt),Va=n(Dt,"P",{});var p4=s(Va);yCo=r(p4,"The model class to instantiate is selected based on the "),Sre=n(p4,"CODE",{});var eXr=s(Sre);wCo=r(eXr,"model_type"),eXr.forEach(t),ACo=r(p4,` property of the config object (either
passed as an argument or loaded from `),Pre=n(p4,"CODE",{});var oXr=s(Pre);LCo=r(oXr,"pretrained_model_name_or_path"),oXr.forEach(t),BCo=r(p4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),$re=n(p4,"CODE",{});var rXr=s($re);kCo=r(rXr,"pretrained_model_name_or_path"),rXr.forEach(t),xCo=r(p4,":"),p4.forEach(t),RCo=i(Dt),G=n(Dt,"UL",{});var O=s(G);_1=n(O,"LI",{});var lMe=s(_1);Ire=n(lMe,"STRONG",{});var tXr=s(Ire);SCo=r(tXr,"albert"),tXr.forEach(t),PCo=r(lMe," \u2014 "),K$=n(lMe,"A",{href:!0});var aXr=s(K$);$Co=r(aXr,"AlbertForMultipleChoice"),aXr.forEach(t),ICo=r(lMe," (ALBERT model)"),lMe.forEach(t),jCo=i(O),u1=n(O,"LI",{});var iMe=s(u1);jre=n(iMe,"STRONG",{});var nXr=s(jre);NCo=r(nXr,"bert"),nXr.forEach(t),DCo=r(iMe," \u2014 "),Z$=n(iMe,"A",{href:!0});var sXr=s(Z$);qCo=r(sXr,"BertForMultipleChoice"),sXr.forEach(t),GCo=r(iMe," (BERT model)"),iMe.forEach(t),OCo=i(O),b1=n(O,"LI",{});var dMe=s(b1);Nre=n(dMe,"STRONG",{});var lXr=s(Nre);XCo=r(lXr,"big_bird"),lXr.forEach(t),zCo=r(dMe," \u2014 "),eI=n(dMe,"A",{href:!0});var iXr=s(eI);VCo=r(iXr,"BigBirdForMultipleChoice"),iXr.forEach(t),WCo=r(dMe," (BigBird model)"),dMe.forEach(t),QCo=i(O),v1=n(O,"LI",{});var cMe=s(v1);Dre=n(cMe,"STRONG",{});var dXr=s(Dre);HCo=r(dXr,"camembert"),dXr.forEach(t),UCo=r(cMe," \u2014 "),oI=n(cMe,"A",{href:!0});var cXr=s(oI);JCo=r(cXr,"CamembertForMultipleChoice"),cXr.forEach(t),YCo=r(cMe," (CamemBERT model)"),cMe.forEach(t),KCo=i(O),T1=n(O,"LI",{});var fMe=s(T1);qre=n(fMe,"STRONG",{});var fXr=s(qre);ZCo=r(fXr,"canine"),fXr.forEach(t),e4o=r(fMe," \u2014 "),rI=n(fMe,"A",{href:!0});var mXr=s(rI);o4o=r(mXr,"CanineForMultipleChoice"),mXr.forEach(t),r4o=r(fMe," (Canine model)"),fMe.forEach(t),t4o=i(O),F1=n(O,"LI",{});var mMe=s(F1);Gre=n(mMe,"STRONG",{});var gXr=s(Gre);a4o=r(gXr,"convbert"),gXr.forEach(t),n4o=r(mMe," \u2014 "),tI=n(mMe,"A",{href:!0});var hXr=s(tI);s4o=r(hXr,"ConvBertForMultipleChoice"),hXr.forEach(t),l4o=r(mMe," (ConvBERT model)"),mMe.forEach(t),i4o=i(O),C1=n(O,"LI",{});var gMe=s(C1);Ore=n(gMe,"STRONG",{});var pXr=s(Ore);d4o=r(pXr,"distilbert"),pXr.forEach(t),c4o=r(gMe," \u2014 "),aI=n(gMe,"A",{href:!0});var _Xr=s(aI);f4o=r(_Xr,"DistilBertForMultipleChoice"),_Xr.forEach(t),m4o=r(gMe," (DistilBERT model)"),gMe.forEach(t),g4o=i(O),M1=n(O,"LI",{});var hMe=s(M1);Xre=n(hMe,"STRONG",{});var uXr=s(Xre);h4o=r(uXr,"electra"),uXr.forEach(t),p4o=r(hMe," \u2014 "),nI=n(hMe,"A",{href:!0});var bXr=s(nI);_4o=r(bXr,"ElectraForMultipleChoice"),bXr.forEach(t),u4o=r(hMe," (ELECTRA model)"),hMe.forEach(t),b4o=i(O),E1=n(O,"LI",{});var pMe=s(E1);zre=n(pMe,"STRONG",{});var vXr=s(zre);v4o=r(vXr,"flaubert"),vXr.forEach(t),T4o=r(pMe," \u2014 "),sI=n(pMe,"A",{href:!0});var TXr=s(sI);F4o=r(TXr,"FlaubertForMultipleChoice"),TXr.forEach(t),C4o=r(pMe," (FlauBERT model)"),pMe.forEach(t),M4o=i(O),y1=n(O,"LI",{});var _Me=s(y1);Vre=n(_Me,"STRONG",{});var FXr=s(Vre);E4o=r(FXr,"fnet"),FXr.forEach(t),y4o=r(_Me," \u2014 "),lI=n(_Me,"A",{href:!0});var CXr=s(lI);w4o=r(CXr,"FNetForMultipleChoice"),CXr.forEach(t),A4o=r(_Me," (FNet model)"),_Me.forEach(t),L4o=i(O),w1=n(O,"LI",{});var uMe=s(w1);Wre=n(uMe,"STRONG",{});var MXr=s(Wre);B4o=r(MXr,"funnel"),MXr.forEach(t),k4o=r(uMe," \u2014 "),iI=n(uMe,"A",{href:!0});var EXr=s(iI);x4o=r(EXr,"FunnelForMultipleChoice"),EXr.forEach(t),R4o=r(uMe," (Funnel Transformer model)"),uMe.forEach(t),S4o=i(O),A1=n(O,"LI",{});var bMe=s(A1);Qre=n(bMe,"STRONG",{});var yXr=s(Qre);P4o=r(yXr,"ibert"),yXr.forEach(t),$4o=r(bMe," \u2014 "),dI=n(bMe,"A",{href:!0});var wXr=s(dI);I4o=r(wXr,"IBertForMultipleChoice"),wXr.forEach(t),j4o=r(bMe," (I-BERT model)"),bMe.forEach(t),N4o=i(O),L1=n(O,"LI",{});var vMe=s(L1);Hre=n(vMe,"STRONG",{});var AXr=s(Hre);D4o=r(AXr,"longformer"),AXr.forEach(t),q4o=r(vMe," \u2014 "),cI=n(vMe,"A",{href:!0});var LXr=s(cI);G4o=r(LXr,"LongformerForMultipleChoice"),LXr.forEach(t),O4o=r(vMe," (Longformer model)"),vMe.forEach(t),X4o=i(O),B1=n(O,"LI",{});var TMe=s(B1);Ure=n(TMe,"STRONG",{});var BXr=s(Ure);z4o=r(BXr,"megatron-bert"),BXr.forEach(t),V4o=r(TMe," \u2014 "),fI=n(TMe,"A",{href:!0});var kXr=s(fI);W4o=r(kXr,"MegatronBertForMultipleChoice"),kXr.forEach(t),Q4o=r(TMe," (MegatronBert model)"),TMe.forEach(t),H4o=i(O),k1=n(O,"LI",{});var FMe=s(k1);Jre=n(FMe,"STRONG",{});var xXr=s(Jre);U4o=r(xXr,"mobilebert"),xXr.forEach(t),J4o=r(FMe," \u2014 "),mI=n(FMe,"A",{href:!0});var RXr=s(mI);Y4o=r(RXr,"MobileBertForMultipleChoice"),RXr.forEach(t),K4o=r(FMe," (MobileBERT model)"),FMe.forEach(t),Z4o=i(O),x1=n(O,"LI",{});var CMe=s(x1);Yre=n(CMe,"STRONG",{});var SXr=s(Yre);eMo=r(SXr,"mpnet"),SXr.forEach(t),oMo=r(CMe," \u2014 "),gI=n(CMe,"A",{href:!0});var PXr=s(gI);rMo=r(PXr,"MPNetForMultipleChoice"),PXr.forEach(t),tMo=r(CMe," (MPNet model)"),CMe.forEach(t),aMo=i(O),R1=n(O,"LI",{});var MMe=s(R1);Kre=n(MMe,"STRONG",{});var $Xr=s(Kre);nMo=r($Xr,"nystromformer"),$Xr.forEach(t),sMo=r(MMe," \u2014 "),hI=n(MMe,"A",{href:!0});var IXr=s(hI);lMo=r(IXr,"NystromformerForMultipleChoice"),IXr.forEach(t),iMo=r(MMe," (Nystromformer model)"),MMe.forEach(t),dMo=i(O),S1=n(O,"LI",{});var EMe=s(S1);Zre=n(EMe,"STRONG",{});var jXr=s(Zre);cMo=r(jXr,"qdqbert"),jXr.forEach(t),fMo=r(EMe," \u2014 "),pI=n(EMe,"A",{href:!0});var NXr=s(pI);mMo=r(NXr,"QDQBertForMultipleChoice"),NXr.forEach(t),gMo=r(EMe," (QDQBert model)"),EMe.forEach(t),hMo=i(O),P1=n(O,"LI",{});var yMe=s(P1);ete=n(yMe,"STRONG",{});var DXr=s(ete);pMo=r(DXr,"rembert"),DXr.forEach(t),_Mo=r(yMe," \u2014 "),_I=n(yMe,"A",{href:!0});var qXr=s(_I);uMo=r(qXr,"RemBertForMultipleChoice"),qXr.forEach(t),bMo=r(yMe," (RemBERT model)"),yMe.forEach(t),vMo=i(O),$1=n(O,"LI",{});var wMe=s($1);ote=n(wMe,"STRONG",{});var GXr=s(ote);TMo=r(GXr,"roberta"),GXr.forEach(t),FMo=r(wMe," \u2014 "),uI=n(wMe,"A",{href:!0});var OXr=s(uI);CMo=r(OXr,"RobertaForMultipleChoice"),OXr.forEach(t),MMo=r(wMe," (RoBERTa model)"),wMe.forEach(t),EMo=i(O),I1=n(O,"LI",{});var AMe=s(I1);rte=n(AMe,"STRONG",{});var XXr=s(rte);yMo=r(XXr,"roformer"),XXr.forEach(t),wMo=r(AMe," \u2014 "),bI=n(AMe,"A",{href:!0});var zXr=s(bI);AMo=r(zXr,"RoFormerForMultipleChoice"),zXr.forEach(t),LMo=r(AMe," (RoFormer model)"),AMe.forEach(t),BMo=i(O),j1=n(O,"LI",{});var LMe=s(j1);tte=n(LMe,"STRONG",{});var VXr=s(tte);kMo=r(VXr,"squeezebert"),VXr.forEach(t),xMo=r(LMe," \u2014 "),vI=n(LMe,"A",{href:!0});var WXr=s(vI);RMo=r(WXr,"SqueezeBertForMultipleChoice"),WXr.forEach(t),SMo=r(LMe," (SqueezeBERT model)"),LMe.forEach(t),PMo=i(O),N1=n(O,"LI",{});var BMe=s(N1);ate=n(BMe,"STRONG",{});var QXr=s(ate);$Mo=r(QXr,"xlm"),QXr.forEach(t),IMo=r(BMe," \u2014 "),TI=n(BMe,"A",{href:!0});var HXr=s(TI);jMo=r(HXr,"XLMForMultipleChoice"),HXr.forEach(t),NMo=r(BMe," (XLM model)"),BMe.forEach(t),DMo=i(O),D1=n(O,"LI",{});var kMe=s(D1);nte=n(kMe,"STRONG",{});var UXr=s(nte);qMo=r(UXr,"xlm-roberta"),UXr.forEach(t),GMo=r(kMe," \u2014 "),FI=n(kMe,"A",{href:!0});var JXr=s(FI);OMo=r(JXr,"XLMRobertaForMultipleChoice"),JXr.forEach(t),XMo=r(kMe," (XLM-RoBERTa model)"),kMe.forEach(t),zMo=i(O),q1=n(O,"LI",{});var xMe=s(q1);ste=n(xMe,"STRONG",{});var YXr=s(ste);VMo=r(YXr,"xlm-roberta-xl"),YXr.forEach(t),WMo=r(xMe," \u2014 "),CI=n(xMe,"A",{href:!0});var KXr=s(CI);QMo=r(KXr,"XLMRobertaXLForMultipleChoice"),KXr.forEach(t),HMo=r(xMe," (XLM-RoBERTa-XL model)"),xMe.forEach(t),UMo=i(O),G1=n(O,"LI",{});var RMe=s(G1);lte=n(RMe,"STRONG",{});var ZXr=s(lte);JMo=r(ZXr,"xlnet"),ZXr.forEach(t),YMo=r(RMe," \u2014 "),MI=n(RMe,"A",{href:!0});var ezr=s(MI);KMo=r(ezr,"XLNetForMultipleChoice"),ezr.forEach(t),ZMo=r(RMe," (XLNet model)"),RMe.forEach(t),eEo=i(O),O1=n(O,"LI",{});var SMe=s(O1);ite=n(SMe,"STRONG",{});var ozr=s(ite);oEo=r(ozr,"yoso"),ozr.forEach(t),rEo=r(SMe," \u2014 "),EI=n(SMe,"A",{href:!0});var rzr=s(EI);tEo=r(rzr,"YosoForMultipleChoice"),rzr.forEach(t),aEo=r(SMe," (YOSO model)"),SMe.forEach(t),O.forEach(t),nEo=i(Dt),X1=n(Dt,"P",{});var PMe=s(X1);sEo=r(PMe,"The model is set in evaluation mode by default using "),dte=n(PMe,"CODE",{});var tzr=s(dte);lEo=r(tzr,"model.eval()"),tzr.forEach(t),iEo=r(PMe,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),cte=n(PMe,"CODE",{});var azr=s(cte);dEo=r(azr,"model.train()"),azr.forEach(t),PMe.forEach(t),cEo=i(Dt),fte=n(Dt,"P",{});var nzr=s(fte);fEo=r(nzr,"Examples:"),nzr.forEach(t),mEo=i(Dt),m(xE.$$.fragment,Dt),Dt.forEach(t),Js.forEach(t),N7e=i(d),ld=n(d,"H2",{class:!0});var VBe=s(ld);z1=n(VBe,"A",{id:!0,class:!0,href:!0});var szr=s(z1);mte=n(szr,"SPAN",{});var lzr=s(mte);m(RE.$$.fragment,lzr),lzr.forEach(t),szr.forEach(t),gEo=i(VBe),gte=n(VBe,"SPAN",{});var izr=s(gte);hEo=r(izr,"AutoModelForNextSentencePrediction"),izr.forEach(t),VBe.forEach(t),D7e=i(d),Ko=n(d,"DIV",{class:!0});var Ks=s(Ko);m(SE.$$.fragment,Ks),pEo=i(Ks),id=n(Ks,"P",{});var cz=s(id);_Eo=r(cz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a next sentence prediction head) when created
with the `),hte=n(cz,"CODE",{});var dzr=s(hte);uEo=r(dzr,"from_pretrained()"),dzr.forEach(t),bEo=r(cz,"class method or the "),pte=n(cz,"CODE",{});var czr=s(pte);vEo=r(czr,"from_config()"),czr.forEach(t),TEo=r(cz,`class
method.`),cz.forEach(t),FEo=i(Ks),PE=n(Ks,"P",{});var WBe=s(PE);CEo=r(WBe,"This class cannot be instantiated directly using "),_te=n(WBe,"CODE",{});var fzr=s(_te);MEo=r(fzr,"__init__()"),fzr.forEach(t),EEo=r(WBe," (throws an error)."),WBe.forEach(t),yEo=i(Ks),Vr=n(Ks,"DIV",{class:!0});var Zs=s(Vr);m($E.$$.fragment,Zs),wEo=i(Zs),ute=n(Zs,"P",{});var mzr=s(ute);AEo=r(mzr,"Instantiates one of the model classes of the library (with a next sentence prediction head) from a configuration."),mzr.forEach(t),LEo=i(Zs),dd=n(Zs,"P",{});var fz=s(dd);BEo=r(fz,`Note:
Loading a model from its configuration file does `),bte=n(fz,"STRONG",{});var gzr=s(bte);kEo=r(gzr,"not"),gzr.forEach(t),xEo=r(fz,` load the model weights. It only affects the
model\u2019s configuration. Use `),vte=n(fz,"CODE",{});var hzr=s(vte);REo=r(hzr,"from_pretrained()"),hzr.forEach(t),SEo=r(fz,"to load the model weights."),fz.forEach(t),PEo=i(Zs),Tte=n(Zs,"P",{});var pzr=s(Tte);$Eo=r(pzr,"Examples:"),pzr.forEach(t),IEo=i(Zs),m(IE.$$.fragment,Zs),Zs.forEach(t),jEo=i(Ks),je=n(Ks,"DIV",{class:!0});var qt=s(je);m(jE.$$.fragment,qt),NEo=i(qt),Fte=n(qt,"P",{});var _zr=s(Fte);DEo=r(_zr,"Instantiate one of the model classes of the library (with a next sentence prediction head) from a pretrained model."),_zr.forEach(t),qEo=i(qt),Wa=n(qt,"P",{});var _4=s(Wa);GEo=r(_4,"The model class to instantiate is selected based on the "),Cte=n(_4,"CODE",{});var uzr=s(Cte);OEo=r(uzr,"model_type"),uzr.forEach(t),XEo=r(_4,` property of the config object (either
passed as an argument or loaded from `),Mte=n(_4,"CODE",{});var bzr=s(Mte);zEo=r(bzr,"pretrained_model_name_or_path"),bzr.forEach(t),VEo=r(_4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Ete=n(_4,"CODE",{});var vzr=s(Ete);WEo=r(vzr,"pretrained_model_name_or_path"),vzr.forEach(t),QEo=r(_4,":"),_4.forEach(t),HEo=i(qt),na=n(qt,"UL",{});var el=s(na);V1=n(el,"LI",{});var $Me=s(V1);yte=n($Me,"STRONG",{});var Tzr=s(yte);UEo=r(Tzr,"bert"),Tzr.forEach(t),JEo=r($Me," \u2014 "),yI=n($Me,"A",{href:!0});var Fzr=s(yI);YEo=r(Fzr,"BertForNextSentencePrediction"),Fzr.forEach(t),KEo=r($Me," (BERT model)"),$Me.forEach(t),ZEo=i(el),W1=n(el,"LI",{});var IMe=s(W1);wte=n(IMe,"STRONG",{});var Czr=s(wte);e3o=r(Czr,"fnet"),Czr.forEach(t),o3o=r(IMe," \u2014 "),wI=n(IMe,"A",{href:!0});var Mzr=s(wI);r3o=r(Mzr,"FNetForNextSentencePrediction"),Mzr.forEach(t),t3o=r(IMe," (FNet model)"),IMe.forEach(t),a3o=i(el),Q1=n(el,"LI",{});var jMe=s(Q1);Ate=n(jMe,"STRONG",{});var Ezr=s(Ate);n3o=r(Ezr,"megatron-bert"),Ezr.forEach(t),s3o=r(jMe," \u2014 "),AI=n(jMe,"A",{href:!0});var yzr=s(AI);l3o=r(yzr,"MegatronBertForNextSentencePrediction"),yzr.forEach(t),i3o=r(jMe," (MegatronBert model)"),jMe.forEach(t),d3o=i(el),H1=n(el,"LI",{});var NMe=s(H1);Lte=n(NMe,"STRONG",{});var wzr=s(Lte);c3o=r(wzr,"mobilebert"),wzr.forEach(t),f3o=r(NMe," \u2014 "),LI=n(NMe,"A",{href:!0});var Azr=s(LI);m3o=r(Azr,"MobileBertForNextSentencePrediction"),Azr.forEach(t),g3o=r(NMe," (MobileBERT model)"),NMe.forEach(t),h3o=i(el),U1=n(el,"LI",{});var DMe=s(U1);Bte=n(DMe,"STRONG",{});var Lzr=s(Bte);p3o=r(Lzr,"qdqbert"),Lzr.forEach(t),_3o=r(DMe," \u2014 "),BI=n(DMe,"A",{href:!0});var Bzr=s(BI);u3o=r(Bzr,"QDQBertForNextSentencePrediction"),Bzr.forEach(t),b3o=r(DMe," (QDQBert model)"),DMe.forEach(t),el.forEach(t),v3o=i(qt),J1=n(qt,"P",{});var qMe=s(J1);T3o=r(qMe,"The model is set in evaluation mode by default using "),kte=n(qMe,"CODE",{});var kzr=s(kte);F3o=r(kzr,"model.eval()"),kzr.forEach(t),C3o=r(qMe,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),xte=n(qMe,"CODE",{});var xzr=s(xte);M3o=r(xzr,"model.train()"),xzr.forEach(t),qMe.forEach(t),E3o=i(qt),Rte=n(qt,"P",{});var Rzr=s(Rte);y3o=r(Rzr,"Examples:"),Rzr.forEach(t),w3o=i(qt),m(NE.$$.fragment,qt),qt.forEach(t),Ks.forEach(t),q7e=i(d),cd=n(d,"H2",{class:!0});var QBe=s(cd);Y1=n(QBe,"A",{id:!0,class:!0,href:!0});var Szr=s(Y1);Ste=n(Szr,"SPAN",{});var Pzr=s(Ste);m(DE.$$.fragment,Pzr),Pzr.forEach(t),Szr.forEach(t),A3o=i(QBe),Pte=n(QBe,"SPAN",{});var $zr=s(Pte);L3o=r($zr,"AutoModelForTokenClassification"),$zr.forEach(t),QBe.forEach(t),G7e=i(d),Zo=n(d,"DIV",{class:!0});var ol=s(Zo);m(qE.$$.fragment,ol),B3o=i(ol),fd=n(ol,"P",{});var mz=s(fd);k3o=r(mz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a token classification head) when created
with the `),$te=n(mz,"CODE",{});var Izr=s($te);x3o=r(Izr,"from_pretrained()"),Izr.forEach(t),R3o=r(mz,"class method or the "),Ite=n(mz,"CODE",{});var jzr=s(Ite);S3o=r(jzr,"from_config()"),jzr.forEach(t),P3o=r(mz,`class
method.`),mz.forEach(t),$3o=i(ol),GE=n(ol,"P",{});var HBe=s(GE);I3o=r(HBe,"This class cannot be instantiated directly using "),jte=n(HBe,"CODE",{});var Nzr=s(jte);j3o=r(Nzr,"__init__()"),Nzr.forEach(t),N3o=r(HBe," (throws an error)."),HBe.forEach(t),D3o=i(ol),Wr=n(ol,"DIV",{class:!0});var rl=s(Wr);m(OE.$$.fragment,rl),q3o=i(rl),Nte=n(rl,"P",{});var Dzr=s(Nte);G3o=r(Dzr,"Instantiates one of the model classes of the library (with a token classification head) from a configuration."),Dzr.forEach(t),O3o=i(rl),md=n(rl,"P",{});var gz=s(md);X3o=r(gz,`Note:
Loading a model from its configuration file does `),Dte=n(gz,"STRONG",{});var qzr=s(Dte);z3o=r(qzr,"not"),qzr.forEach(t),V3o=r(gz,` load the model weights. It only affects the
model\u2019s configuration. Use `),qte=n(gz,"CODE",{});var Gzr=s(qte);W3o=r(Gzr,"from_pretrained()"),Gzr.forEach(t),Q3o=r(gz,"to load the model weights."),gz.forEach(t),H3o=i(rl),Gte=n(rl,"P",{});var Ozr=s(Gte);U3o=r(Ozr,"Examples:"),Ozr.forEach(t),J3o=i(rl),m(XE.$$.fragment,rl),rl.forEach(t),Y3o=i(ol),Ne=n(ol,"DIV",{class:!0});var Gt=s(Ne);m(zE.$$.fragment,Gt),K3o=i(Gt),Ote=n(Gt,"P",{});var Xzr=s(Ote);Z3o=r(Xzr,"Instantiate one of the model classes of the library (with a token classification head) from a pretrained model."),Xzr.forEach(t),eyo=i(Gt),Qa=n(Gt,"P",{});var u4=s(Qa);oyo=r(u4,"The model class to instantiate is selected based on the "),Xte=n(u4,"CODE",{});var zzr=s(Xte);ryo=r(zzr,"model_type"),zzr.forEach(t),tyo=r(u4,` property of the config object (either
passed as an argument or loaded from `),zte=n(u4,"CODE",{});var Vzr=s(zte);ayo=r(Vzr,"pretrained_model_name_or_path"),Vzr.forEach(t),nyo=r(u4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Vte=n(u4,"CODE",{});var Wzr=s(Vte);syo=r(Wzr,"pretrained_model_name_or_path"),Wzr.forEach(t),lyo=r(u4,":"),u4.forEach(t),iyo=i(Gt),D=n(Gt,"UL",{});var q=s(D);K1=n(q,"LI",{});var GMe=s(K1);Wte=n(GMe,"STRONG",{});var Qzr=s(Wte);dyo=r(Qzr,"albert"),Qzr.forEach(t),cyo=r(GMe," \u2014 "),kI=n(GMe,"A",{href:!0});var Hzr=s(kI);fyo=r(Hzr,"AlbertForTokenClassification"),Hzr.forEach(t),myo=r(GMe," (ALBERT model)"),GMe.forEach(t),gyo=i(q),Z1=n(q,"LI",{});var OMe=s(Z1);Qte=n(OMe,"STRONG",{});var Uzr=s(Qte);hyo=r(Uzr,"bert"),Uzr.forEach(t),pyo=r(OMe," \u2014 "),xI=n(OMe,"A",{href:!0});var Jzr=s(xI);_yo=r(Jzr,"BertForTokenClassification"),Jzr.forEach(t),uyo=r(OMe," (BERT model)"),OMe.forEach(t),byo=i(q),eb=n(q,"LI",{});var XMe=s(eb);Hte=n(XMe,"STRONG",{});var Yzr=s(Hte);vyo=r(Yzr,"big_bird"),Yzr.forEach(t),Tyo=r(XMe," \u2014 "),RI=n(XMe,"A",{href:!0});var Kzr=s(RI);Fyo=r(Kzr,"BigBirdForTokenClassification"),Kzr.forEach(t),Cyo=r(XMe," (BigBird model)"),XMe.forEach(t),Myo=i(q),ob=n(q,"LI",{});var zMe=s(ob);Ute=n(zMe,"STRONG",{});var Zzr=s(Ute);Eyo=r(Zzr,"camembert"),Zzr.forEach(t),yyo=r(zMe," \u2014 "),SI=n(zMe,"A",{href:!0});var eVr=s(SI);wyo=r(eVr,"CamembertForTokenClassification"),eVr.forEach(t),Ayo=r(zMe," (CamemBERT model)"),zMe.forEach(t),Lyo=i(q),rb=n(q,"LI",{});var VMe=s(rb);Jte=n(VMe,"STRONG",{});var oVr=s(Jte);Byo=r(oVr,"canine"),oVr.forEach(t),kyo=r(VMe," \u2014 "),PI=n(VMe,"A",{href:!0});var rVr=s(PI);xyo=r(rVr,"CanineForTokenClassification"),rVr.forEach(t),Ryo=r(VMe," (Canine model)"),VMe.forEach(t),Syo=i(q),tb=n(q,"LI",{});var WMe=s(tb);Yte=n(WMe,"STRONG",{});var tVr=s(Yte);Pyo=r(tVr,"convbert"),tVr.forEach(t),$yo=r(WMe," \u2014 "),$I=n(WMe,"A",{href:!0});var aVr=s($I);Iyo=r(aVr,"ConvBertForTokenClassification"),aVr.forEach(t),jyo=r(WMe," (ConvBERT model)"),WMe.forEach(t),Nyo=i(q),ab=n(q,"LI",{});var QMe=s(ab);Kte=n(QMe,"STRONG",{});var nVr=s(Kte);Dyo=r(nVr,"deberta"),nVr.forEach(t),qyo=r(QMe," \u2014 "),II=n(QMe,"A",{href:!0});var sVr=s(II);Gyo=r(sVr,"DebertaForTokenClassification"),sVr.forEach(t),Oyo=r(QMe," (DeBERTa model)"),QMe.forEach(t),Xyo=i(q),nb=n(q,"LI",{});var HMe=s(nb);Zte=n(HMe,"STRONG",{});var lVr=s(Zte);zyo=r(lVr,"deberta-v2"),lVr.forEach(t),Vyo=r(HMe," \u2014 "),jI=n(HMe,"A",{href:!0});var iVr=s(jI);Wyo=r(iVr,"DebertaV2ForTokenClassification"),iVr.forEach(t),Qyo=r(HMe," (DeBERTa-v2 model)"),HMe.forEach(t),Hyo=i(q),sb=n(q,"LI",{});var UMe=s(sb);eae=n(UMe,"STRONG",{});var dVr=s(eae);Uyo=r(dVr,"distilbert"),dVr.forEach(t),Jyo=r(UMe," \u2014 "),NI=n(UMe,"A",{href:!0});var cVr=s(NI);Yyo=r(cVr,"DistilBertForTokenClassification"),cVr.forEach(t),Kyo=r(UMe," (DistilBERT model)"),UMe.forEach(t),Zyo=i(q),lb=n(q,"LI",{});var JMe=s(lb);oae=n(JMe,"STRONG",{});var fVr=s(oae);ewo=r(fVr,"electra"),fVr.forEach(t),owo=r(JMe," \u2014 "),DI=n(JMe,"A",{href:!0});var mVr=s(DI);rwo=r(mVr,"ElectraForTokenClassification"),mVr.forEach(t),two=r(JMe," (ELECTRA model)"),JMe.forEach(t),awo=i(q),ib=n(q,"LI",{});var YMe=s(ib);rae=n(YMe,"STRONG",{});var gVr=s(rae);nwo=r(gVr,"flaubert"),gVr.forEach(t),swo=r(YMe," \u2014 "),qI=n(YMe,"A",{href:!0});var hVr=s(qI);lwo=r(hVr,"FlaubertForTokenClassification"),hVr.forEach(t),iwo=r(YMe," (FlauBERT model)"),YMe.forEach(t),dwo=i(q),db=n(q,"LI",{});var KMe=s(db);tae=n(KMe,"STRONG",{});var pVr=s(tae);cwo=r(pVr,"fnet"),pVr.forEach(t),fwo=r(KMe," \u2014 "),GI=n(KMe,"A",{href:!0});var _Vr=s(GI);mwo=r(_Vr,"FNetForTokenClassification"),_Vr.forEach(t),gwo=r(KMe," (FNet model)"),KMe.forEach(t),hwo=i(q),cb=n(q,"LI",{});var ZMe=s(cb);aae=n(ZMe,"STRONG",{});var uVr=s(aae);pwo=r(uVr,"funnel"),uVr.forEach(t),_wo=r(ZMe," \u2014 "),OI=n(ZMe,"A",{href:!0});var bVr=s(OI);uwo=r(bVr,"FunnelForTokenClassification"),bVr.forEach(t),bwo=r(ZMe," (Funnel Transformer model)"),ZMe.forEach(t),vwo=i(q),fb=n(q,"LI",{});var eEe=s(fb);nae=n(eEe,"STRONG",{});var vVr=s(nae);Two=r(vVr,"gpt2"),vVr.forEach(t),Fwo=r(eEe," \u2014 "),XI=n(eEe,"A",{href:!0});var TVr=s(XI);Cwo=r(TVr,"GPT2ForTokenClassification"),TVr.forEach(t),Mwo=r(eEe," (OpenAI GPT-2 model)"),eEe.forEach(t),Ewo=i(q),mb=n(q,"LI",{});var oEe=s(mb);sae=n(oEe,"STRONG",{});var FVr=s(sae);ywo=r(FVr,"ibert"),FVr.forEach(t),wwo=r(oEe," \u2014 "),zI=n(oEe,"A",{href:!0});var CVr=s(zI);Awo=r(CVr,"IBertForTokenClassification"),CVr.forEach(t),Lwo=r(oEe," (I-BERT model)"),oEe.forEach(t),Bwo=i(q),gb=n(q,"LI",{});var rEe=s(gb);lae=n(rEe,"STRONG",{});var MVr=s(lae);kwo=r(MVr,"layoutlm"),MVr.forEach(t),xwo=r(rEe," \u2014 "),VI=n(rEe,"A",{href:!0});var EVr=s(VI);Rwo=r(EVr,"LayoutLMForTokenClassification"),EVr.forEach(t),Swo=r(rEe," (LayoutLM model)"),rEe.forEach(t),Pwo=i(q),hb=n(q,"LI",{});var tEe=s(hb);iae=n(tEe,"STRONG",{});var yVr=s(iae);$wo=r(yVr,"layoutlmv2"),yVr.forEach(t),Iwo=r(tEe," \u2014 "),WI=n(tEe,"A",{href:!0});var wVr=s(WI);jwo=r(wVr,"LayoutLMv2ForTokenClassification"),wVr.forEach(t),Nwo=r(tEe," (LayoutLMv2 model)"),tEe.forEach(t),Dwo=i(q),pb=n(q,"LI",{});var aEe=s(pb);dae=n(aEe,"STRONG",{});var AVr=s(dae);qwo=r(AVr,"longformer"),AVr.forEach(t),Gwo=r(aEe," \u2014 "),QI=n(aEe,"A",{href:!0});var LVr=s(QI);Owo=r(LVr,"LongformerForTokenClassification"),LVr.forEach(t),Xwo=r(aEe," (Longformer model)"),aEe.forEach(t),zwo=i(q),_b=n(q,"LI",{});var nEe=s(_b);cae=n(nEe,"STRONG",{});var BVr=s(cae);Vwo=r(BVr,"megatron-bert"),BVr.forEach(t),Wwo=r(nEe," \u2014 "),HI=n(nEe,"A",{href:!0});var kVr=s(HI);Qwo=r(kVr,"MegatronBertForTokenClassification"),kVr.forEach(t),Hwo=r(nEe," (MegatronBert model)"),nEe.forEach(t),Uwo=i(q),ub=n(q,"LI",{});var sEe=s(ub);fae=n(sEe,"STRONG",{});var xVr=s(fae);Jwo=r(xVr,"mobilebert"),xVr.forEach(t),Ywo=r(sEe," \u2014 "),UI=n(sEe,"A",{href:!0});var RVr=s(UI);Kwo=r(RVr,"MobileBertForTokenClassification"),RVr.forEach(t),Zwo=r(sEe," (MobileBERT model)"),sEe.forEach(t),eAo=i(q),bb=n(q,"LI",{});var lEe=s(bb);mae=n(lEe,"STRONG",{});var SVr=s(mae);oAo=r(SVr,"mpnet"),SVr.forEach(t),rAo=r(lEe," \u2014 "),JI=n(lEe,"A",{href:!0});var PVr=s(JI);tAo=r(PVr,"MPNetForTokenClassification"),PVr.forEach(t),aAo=r(lEe," (MPNet model)"),lEe.forEach(t),nAo=i(q),vb=n(q,"LI",{});var iEe=s(vb);gae=n(iEe,"STRONG",{});var $Vr=s(gae);sAo=r($Vr,"nystromformer"),$Vr.forEach(t),lAo=r(iEe," \u2014 "),YI=n(iEe,"A",{href:!0});var IVr=s(YI);iAo=r(IVr,"NystromformerForTokenClassification"),IVr.forEach(t),dAo=r(iEe," (Nystromformer model)"),iEe.forEach(t),cAo=i(q),Tb=n(q,"LI",{});var dEe=s(Tb);hae=n(dEe,"STRONG",{});var jVr=s(hae);fAo=r(jVr,"qdqbert"),jVr.forEach(t),mAo=r(dEe," \u2014 "),KI=n(dEe,"A",{href:!0});var NVr=s(KI);gAo=r(NVr,"QDQBertForTokenClassification"),NVr.forEach(t),hAo=r(dEe," (QDQBert model)"),dEe.forEach(t),pAo=i(q),Fb=n(q,"LI",{});var cEe=s(Fb);pae=n(cEe,"STRONG",{});var DVr=s(pae);_Ao=r(DVr,"rembert"),DVr.forEach(t),uAo=r(cEe," \u2014 "),ZI=n(cEe,"A",{href:!0});var qVr=s(ZI);bAo=r(qVr,"RemBertForTokenClassification"),qVr.forEach(t),vAo=r(cEe," (RemBERT model)"),cEe.forEach(t),TAo=i(q),Cb=n(q,"LI",{});var fEe=s(Cb);_ae=n(fEe,"STRONG",{});var GVr=s(_ae);FAo=r(GVr,"roberta"),GVr.forEach(t),CAo=r(fEe," \u2014 "),ej=n(fEe,"A",{href:!0});var OVr=s(ej);MAo=r(OVr,"RobertaForTokenClassification"),OVr.forEach(t),EAo=r(fEe," (RoBERTa model)"),fEe.forEach(t),yAo=i(q),Mb=n(q,"LI",{});var mEe=s(Mb);uae=n(mEe,"STRONG",{});var XVr=s(uae);wAo=r(XVr,"roformer"),XVr.forEach(t),AAo=r(mEe," \u2014 "),oj=n(mEe,"A",{href:!0});var zVr=s(oj);LAo=r(zVr,"RoFormerForTokenClassification"),zVr.forEach(t),BAo=r(mEe," (RoFormer model)"),mEe.forEach(t),kAo=i(q),Eb=n(q,"LI",{});var gEe=s(Eb);bae=n(gEe,"STRONG",{});var VVr=s(bae);xAo=r(VVr,"squeezebert"),VVr.forEach(t),RAo=r(gEe," \u2014 "),rj=n(gEe,"A",{href:!0});var WVr=s(rj);SAo=r(WVr,"SqueezeBertForTokenClassification"),WVr.forEach(t),PAo=r(gEe," (SqueezeBERT model)"),gEe.forEach(t),$Ao=i(q),yb=n(q,"LI",{});var hEe=s(yb);vae=n(hEe,"STRONG",{});var QVr=s(vae);IAo=r(QVr,"xlm"),QVr.forEach(t),jAo=r(hEe," \u2014 "),tj=n(hEe,"A",{href:!0});var HVr=s(tj);NAo=r(HVr,"XLMForTokenClassification"),HVr.forEach(t),DAo=r(hEe," (XLM model)"),hEe.forEach(t),qAo=i(q),wb=n(q,"LI",{});var pEe=s(wb);Tae=n(pEe,"STRONG",{});var UVr=s(Tae);GAo=r(UVr,"xlm-roberta"),UVr.forEach(t),OAo=r(pEe," \u2014 "),aj=n(pEe,"A",{href:!0});var JVr=s(aj);XAo=r(JVr,"XLMRobertaForTokenClassification"),JVr.forEach(t),zAo=r(pEe," (XLM-RoBERTa model)"),pEe.forEach(t),VAo=i(q),Ab=n(q,"LI",{});var _Ee=s(Ab);Fae=n(_Ee,"STRONG",{});var YVr=s(Fae);WAo=r(YVr,"xlm-roberta-xl"),YVr.forEach(t),QAo=r(_Ee," \u2014 "),nj=n(_Ee,"A",{href:!0});var KVr=s(nj);HAo=r(KVr,"XLMRobertaXLForTokenClassification"),KVr.forEach(t),UAo=r(_Ee," (XLM-RoBERTa-XL model)"),_Ee.forEach(t),JAo=i(q),Lb=n(q,"LI",{});var uEe=s(Lb);Cae=n(uEe,"STRONG",{});var ZVr=s(Cae);YAo=r(ZVr,"xlnet"),ZVr.forEach(t),KAo=r(uEe," \u2014 "),sj=n(uEe,"A",{href:!0});var eWr=s(sj);ZAo=r(eWr,"XLNetForTokenClassification"),eWr.forEach(t),e0o=r(uEe," (XLNet model)"),uEe.forEach(t),o0o=i(q),Bb=n(q,"LI",{});var bEe=s(Bb);Mae=n(bEe,"STRONG",{});var oWr=s(Mae);r0o=r(oWr,"yoso"),oWr.forEach(t),t0o=r(bEe," \u2014 "),lj=n(bEe,"A",{href:!0});var rWr=s(lj);a0o=r(rWr,"YosoForTokenClassification"),rWr.forEach(t),n0o=r(bEe," (YOSO model)"),bEe.forEach(t),q.forEach(t),s0o=i(Gt),kb=n(Gt,"P",{});var vEe=s(kb);l0o=r(vEe,"The model is set in evaluation mode by default using "),Eae=n(vEe,"CODE",{});var tWr=s(Eae);i0o=r(tWr,"model.eval()"),tWr.forEach(t),d0o=r(vEe,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),yae=n(vEe,"CODE",{});var aWr=s(yae);c0o=r(aWr,"model.train()"),aWr.forEach(t),vEe.forEach(t),f0o=i(Gt),wae=n(Gt,"P",{});var nWr=s(wae);m0o=r(nWr,"Examples:"),nWr.forEach(t),g0o=i(Gt),m(VE.$$.fragment,Gt),Gt.forEach(t),ol.forEach(t),O7e=i(d),gd=n(d,"H2",{class:!0});var UBe=s(gd);xb=n(UBe,"A",{id:!0,class:!0,href:!0});var sWr=s(xb);Aae=n(sWr,"SPAN",{});var lWr=s(Aae);m(WE.$$.fragment,lWr),lWr.forEach(t),sWr.forEach(t),h0o=i(UBe),Lae=n(UBe,"SPAN",{});var iWr=s(Lae);p0o=r(iWr,"AutoModelForQuestionAnswering"),iWr.forEach(t),UBe.forEach(t),X7e=i(d),er=n(d,"DIV",{class:!0});var tl=s(er);m(QE.$$.fragment,tl),_0o=i(tl),hd=n(tl,"P",{});var hz=s(hd);u0o=r(hz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a question answering head) when created
with the `),Bae=n(hz,"CODE",{});var dWr=s(Bae);b0o=r(dWr,"from_pretrained()"),dWr.forEach(t),v0o=r(hz,"class method or the "),kae=n(hz,"CODE",{});var cWr=s(kae);T0o=r(cWr,"from_config()"),cWr.forEach(t),F0o=r(hz,`class
method.`),hz.forEach(t),C0o=i(tl),HE=n(tl,"P",{});var JBe=s(HE);M0o=r(JBe,"This class cannot be instantiated directly using "),xae=n(JBe,"CODE",{});var fWr=s(xae);E0o=r(fWr,"__init__()"),fWr.forEach(t),y0o=r(JBe," (throws an error)."),JBe.forEach(t),w0o=i(tl),Qr=n(tl,"DIV",{class:!0});var al=s(Qr);m(UE.$$.fragment,al),A0o=i(al),Rae=n(al,"P",{});var mWr=s(Rae);L0o=r(mWr,"Instantiates one of the model classes of the library (with a question answering head) from a configuration."),mWr.forEach(t),B0o=i(al),pd=n(al,"P",{});var pz=s(pd);k0o=r(pz,`Note:
Loading a model from its configuration file does `),Sae=n(pz,"STRONG",{});var gWr=s(Sae);x0o=r(gWr,"not"),gWr.forEach(t),R0o=r(pz,` load the model weights. It only affects the
model\u2019s configuration. Use `),Pae=n(pz,"CODE",{});var hWr=s(Pae);S0o=r(hWr,"from_pretrained()"),hWr.forEach(t),P0o=r(pz,"to load the model weights."),pz.forEach(t),$0o=i(al),$ae=n(al,"P",{});var pWr=s($ae);I0o=r(pWr,"Examples:"),pWr.forEach(t),j0o=i(al),m(JE.$$.fragment,al),al.forEach(t),N0o=i(tl),De=n(tl,"DIV",{class:!0});var Ot=s(De);m(YE.$$.fragment,Ot),D0o=i(Ot),Iae=n(Ot,"P",{});var _Wr=s(Iae);q0o=r(_Wr,"Instantiate one of the model classes of the library (with a question answering head) from a pretrained model."),_Wr.forEach(t),G0o=i(Ot),Ha=n(Ot,"P",{});var b4=s(Ha);O0o=r(b4,"The model class to instantiate is selected based on the "),jae=n(b4,"CODE",{});var uWr=s(jae);X0o=r(uWr,"model_type"),uWr.forEach(t),z0o=r(b4,` property of the config object (either
passed as an argument or loaded from `),Nae=n(b4,"CODE",{});var bWr=s(Nae);V0o=r(bWr,"pretrained_model_name_or_path"),bWr.forEach(t),W0o=r(b4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Dae=n(b4,"CODE",{});var vWr=s(Dae);Q0o=r(vWr,"pretrained_model_name_or_path"),vWr.forEach(t),H0o=r(b4,":"),b4.forEach(t),U0o=i(Ot),R=n(Ot,"UL",{});var P=s(R);Rb=n(P,"LI",{});var TEe=s(Rb);qae=n(TEe,"STRONG",{});var TWr=s(qae);J0o=r(TWr,"albert"),TWr.forEach(t),Y0o=r(TEe," \u2014 "),ij=n(TEe,"A",{href:!0});var FWr=s(ij);K0o=r(FWr,"AlbertForQuestionAnswering"),FWr.forEach(t),Z0o=r(TEe," (ALBERT model)"),TEe.forEach(t),eLo=i(P),Sb=n(P,"LI",{});var FEe=s(Sb);Gae=n(FEe,"STRONG",{});var CWr=s(Gae);oLo=r(CWr,"bart"),CWr.forEach(t),rLo=r(FEe," \u2014 "),dj=n(FEe,"A",{href:!0});var MWr=s(dj);tLo=r(MWr,"BartForQuestionAnswering"),MWr.forEach(t),aLo=r(FEe," (BART model)"),FEe.forEach(t),nLo=i(P),Pb=n(P,"LI",{});var CEe=s(Pb);Oae=n(CEe,"STRONG",{});var EWr=s(Oae);sLo=r(EWr,"bert"),EWr.forEach(t),lLo=r(CEe," \u2014 "),cj=n(CEe,"A",{href:!0});var yWr=s(cj);iLo=r(yWr,"BertForQuestionAnswering"),yWr.forEach(t),dLo=r(CEe," (BERT model)"),CEe.forEach(t),cLo=i(P),$b=n(P,"LI",{});var MEe=s($b);Xae=n(MEe,"STRONG",{});var wWr=s(Xae);fLo=r(wWr,"big_bird"),wWr.forEach(t),mLo=r(MEe," \u2014 "),fj=n(MEe,"A",{href:!0});var AWr=s(fj);gLo=r(AWr,"BigBirdForQuestionAnswering"),AWr.forEach(t),hLo=r(MEe," (BigBird model)"),MEe.forEach(t),pLo=i(P),Ib=n(P,"LI",{});var EEe=s(Ib);zae=n(EEe,"STRONG",{});var LWr=s(zae);_Lo=r(LWr,"bigbird_pegasus"),LWr.forEach(t),uLo=r(EEe," \u2014 "),mj=n(EEe,"A",{href:!0});var BWr=s(mj);bLo=r(BWr,"BigBirdPegasusForQuestionAnswering"),BWr.forEach(t),vLo=r(EEe," (BigBirdPegasus model)"),EEe.forEach(t),TLo=i(P),jb=n(P,"LI",{});var yEe=s(jb);Vae=n(yEe,"STRONG",{});var kWr=s(Vae);FLo=r(kWr,"camembert"),kWr.forEach(t),CLo=r(yEe," \u2014 "),gj=n(yEe,"A",{href:!0});var xWr=s(gj);MLo=r(xWr,"CamembertForQuestionAnswering"),xWr.forEach(t),ELo=r(yEe," (CamemBERT model)"),yEe.forEach(t),yLo=i(P),Nb=n(P,"LI",{});var wEe=s(Nb);Wae=n(wEe,"STRONG",{});var RWr=s(Wae);wLo=r(RWr,"canine"),RWr.forEach(t),ALo=r(wEe," \u2014 "),hj=n(wEe,"A",{href:!0});var SWr=s(hj);LLo=r(SWr,"CanineForQuestionAnswering"),SWr.forEach(t),BLo=r(wEe," (Canine model)"),wEe.forEach(t),kLo=i(P),Db=n(P,"LI",{});var AEe=s(Db);Qae=n(AEe,"STRONG",{});var PWr=s(Qae);xLo=r(PWr,"convbert"),PWr.forEach(t),RLo=r(AEe," \u2014 "),pj=n(AEe,"A",{href:!0});var $Wr=s(pj);SLo=r($Wr,"ConvBertForQuestionAnswering"),$Wr.forEach(t),PLo=r(AEe," (ConvBERT model)"),AEe.forEach(t),$Lo=i(P),qb=n(P,"LI",{});var LEe=s(qb);Hae=n(LEe,"STRONG",{});var IWr=s(Hae);ILo=r(IWr,"deberta"),IWr.forEach(t),jLo=r(LEe," \u2014 "),_j=n(LEe,"A",{href:!0});var jWr=s(_j);NLo=r(jWr,"DebertaForQuestionAnswering"),jWr.forEach(t),DLo=r(LEe," (DeBERTa model)"),LEe.forEach(t),qLo=i(P),Gb=n(P,"LI",{});var BEe=s(Gb);Uae=n(BEe,"STRONG",{});var NWr=s(Uae);GLo=r(NWr,"deberta-v2"),NWr.forEach(t),OLo=r(BEe," \u2014 "),uj=n(BEe,"A",{href:!0});var DWr=s(uj);XLo=r(DWr,"DebertaV2ForQuestionAnswering"),DWr.forEach(t),zLo=r(BEe," (DeBERTa-v2 model)"),BEe.forEach(t),VLo=i(P),Ob=n(P,"LI",{});var kEe=s(Ob);Jae=n(kEe,"STRONG",{});var qWr=s(Jae);WLo=r(qWr,"distilbert"),qWr.forEach(t),QLo=r(kEe," \u2014 "),bj=n(kEe,"A",{href:!0});var GWr=s(bj);HLo=r(GWr,"DistilBertForQuestionAnswering"),GWr.forEach(t),ULo=r(kEe," (DistilBERT model)"),kEe.forEach(t),JLo=i(P),Xb=n(P,"LI",{});var xEe=s(Xb);Yae=n(xEe,"STRONG",{});var OWr=s(Yae);YLo=r(OWr,"electra"),OWr.forEach(t),KLo=r(xEe," \u2014 "),vj=n(xEe,"A",{href:!0});var XWr=s(vj);ZLo=r(XWr,"ElectraForQuestionAnswering"),XWr.forEach(t),e7o=r(xEe," (ELECTRA model)"),xEe.forEach(t),o7o=i(P),zb=n(P,"LI",{});var REe=s(zb);Kae=n(REe,"STRONG",{});var zWr=s(Kae);r7o=r(zWr,"flaubert"),zWr.forEach(t),t7o=r(REe," \u2014 "),Tj=n(REe,"A",{href:!0});var VWr=s(Tj);a7o=r(VWr,"FlaubertForQuestionAnsweringSimple"),VWr.forEach(t),n7o=r(REe," (FlauBERT model)"),REe.forEach(t),s7o=i(P),Vb=n(P,"LI",{});var SEe=s(Vb);Zae=n(SEe,"STRONG",{});var WWr=s(Zae);l7o=r(WWr,"fnet"),WWr.forEach(t),i7o=r(SEe," \u2014 "),Fj=n(SEe,"A",{href:!0});var QWr=s(Fj);d7o=r(QWr,"FNetForQuestionAnswering"),QWr.forEach(t),c7o=r(SEe," (FNet model)"),SEe.forEach(t),f7o=i(P),Wb=n(P,"LI",{});var PEe=s(Wb);ene=n(PEe,"STRONG",{});var HWr=s(ene);m7o=r(HWr,"funnel"),HWr.forEach(t),g7o=r(PEe," \u2014 "),Cj=n(PEe,"A",{href:!0});var UWr=s(Cj);h7o=r(UWr,"FunnelForQuestionAnswering"),UWr.forEach(t),p7o=r(PEe," (Funnel Transformer model)"),PEe.forEach(t),_7o=i(P),Qb=n(P,"LI",{});var $Ee=s(Qb);one=n($Ee,"STRONG",{});var JWr=s(one);u7o=r(JWr,"gptj"),JWr.forEach(t),b7o=r($Ee," \u2014 "),Mj=n($Ee,"A",{href:!0});var YWr=s(Mj);v7o=r(YWr,"GPTJForQuestionAnswering"),YWr.forEach(t),T7o=r($Ee," (GPT-J model)"),$Ee.forEach(t),F7o=i(P),Hb=n(P,"LI",{});var IEe=s(Hb);rne=n(IEe,"STRONG",{});var KWr=s(rne);C7o=r(KWr,"ibert"),KWr.forEach(t),M7o=r(IEe," \u2014 "),Ej=n(IEe,"A",{href:!0});var ZWr=s(Ej);E7o=r(ZWr,"IBertForQuestionAnswering"),ZWr.forEach(t),y7o=r(IEe," (I-BERT model)"),IEe.forEach(t),w7o=i(P),Ub=n(P,"LI",{});var jEe=s(Ub);tne=n(jEe,"STRONG",{});var eQr=s(tne);A7o=r(eQr,"layoutlmv2"),eQr.forEach(t),L7o=r(jEe," \u2014 "),yj=n(jEe,"A",{href:!0});var oQr=s(yj);B7o=r(oQr,"LayoutLMv2ForQuestionAnswering"),oQr.forEach(t),k7o=r(jEe," (LayoutLMv2 model)"),jEe.forEach(t),x7o=i(P),Jb=n(P,"LI",{});var NEe=s(Jb);ane=n(NEe,"STRONG",{});var rQr=s(ane);R7o=r(rQr,"led"),rQr.forEach(t),S7o=r(NEe," \u2014 "),wj=n(NEe,"A",{href:!0});var tQr=s(wj);P7o=r(tQr,"LEDForQuestionAnswering"),tQr.forEach(t),$7o=r(NEe," (LED model)"),NEe.forEach(t),I7o=i(P),Yb=n(P,"LI",{});var DEe=s(Yb);nne=n(DEe,"STRONG",{});var aQr=s(nne);j7o=r(aQr,"longformer"),aQr.forEach(t),N7o=r(DEe," \u2014 "),Aj=n(DEe,"A",{href:!0});var nQr=s(Aj);D7o=r(nQr,"LongformerForQuestionAnswering"),nQr.forEach(t),q7o=r(DEe," (Longformer model)"),DEe.forEach(t),G7o=i(P),Kb=n(P,"LI",{});var qEe=s(Kb);sne=n(qEe,"STRONG",{});var sQr=s(sne);O7o=r(sQr,"lxmert"),sQr.forEach(t),X7o=r(qEe," \u2014 "),Lj=n(qEe,"A",{href:!0});var lQr=s(Lj);z7o=r(lQr,"LxmertForQuestionAnswering"),lQr.forEach(t),V7o=r(qEe," (LXMERT model)"),qEe.forEach(t),W7o=i(P),Zb=n(P,"LI",{});var GEe=s(Zb);lne=n(GEe,"STRONG",{});var iQr=s(lne);Q7o=r(iQr,"mbart"),iQr.forEach(t),H7o=r(GEe," \u2014 "),Bj=n(GEe,"A",{href:!0});var dQr=s(Bj);U7o=r(dQr,"MBartForQuestionAnswering"),dQr.forEach(t),J7o=r(GEe," (mBART model)"),GEe.forEach(t),Y7o=i(P),e5=n(P,"LI",{});var OEe=s(e5);ine=n(OEe,"STRONG",{});var cQr=s(ine);K7o=r(cQr,"megatron-bert"),cQr.forEach(t),Z7o=r(OEe," \u2014 "),kj=n(OEe,"A",{href:!0});var fQr=s(kj);e9o=r(fQr,"MegatronBertForQuestionAnswering"),fQr.forEach(t),o9o=r(OEe," (MegatronBert model)"),OEe.forEach(t),r9o=i(P),o5=n(P,"LI",{});var XEe=s(o5);dne=n(XEe,"STRONG",{});var mQr=s(dne);t9o=r(mQr,"mobilebert"),mQr.forEach(t),a9o=r(XEe," \u2014 "),xj=n(XEe,"A",{href:!0});var gQr=s(xj);n9o=r(gQr,"MobileBertForQuestionAnswering"),gQr.forEach(t),s9o=r(XEe," (MobileBERT model)"),XEe.forEach(t),l9o=i(P),r5=n(P,"LI",{});var zEe=s(r5);cne=n(zEe,"STRONG",{});var hQr=s(cne);i9o=r(hQr,"mpnet"),hQr.forEach(t),d9o=r(zEe," \u2014 "),Rj=n(zEe,"A",{href:!0});var pQr=s(Rj);c9o=r(pQr,"MPNetForQuestionAnswering"),pQr.forEach(t),f9o=r(zEe," (MPNet model)"),zEe.forEach(t),m9o=i(P),t5=n(P,"LI",{});var VEe=s(t5);fne=n(VEe,"STRONG",{});var _Qr=s(fne);g9o=r(_Qr,"nystromformer"),_Qr.forEach(t),h9o=r(VEe," \u2014 "),Sj=n(VEe,"A",{href:!0});var uQr=s(Sj);p9o=r(uQr,"NystromformerForQuestionAnswering"),uQr.forEach(t),_9o=r(VEe," (Nystromformer model)"),VEe.forEach(t),u9o=i(P),a5=n(P,"LI",{});var WEe=s(a5);mne=n(WEe,"STRONG",{});var bQr=s(mne);b9o=r(bQr,"qdqbert"),bQr.forEach(t),v9o=r(WEe," \u2014 "),Pj=n(WEe,"A",{href:!0});var vQr=s(Pj);T9o=r(vQr,"QDQBertForQuestionAnswering"),vQr.forEach(t),F9o=r(WEe," (QDQBert model)"),WEe.forEach(t),C9o=i(P),n5=n(P,"LI",{});var QEe=s(n5);gne=n(QEe,"STRONG",{});var TQr=s(gne);M9o=r(TQr,"reformer"),TQr.forEach(t),E9o=r(QEe," \u2014 "),$j=n(QEe,"A",{href:!0});var FQr=s($j);y9o=r(FQr,"ReformerForQuestionAnswering"),FQr.forEach(t),w9o=r(QEe," (Reformer model)"),QEe.forEach(t),A9o=i(P),s5=n(P,"LI",{});var HEe=s(s5);hne=n(HEe,"STRONG",{});var CQr=s(hne);L9o=r(CQr,"rembert"),CQr.forEach(t),B9o=r(HEe," \u2014 "),Ij=n(HEe,"A",{href:!0});var MQr=s(Ij);k9o=r(MQr,"RemBertForQuestionAnswering"),MQr.forEach(t),x9o=r(HEe," (RemBERT model)"),HEe.forEach(t),R9o=i(P),l5=n(P,"LI",{});var UEe=s(l5);pne=n(UEe,"STRONG",{});var EQr=s(pne);S9o=r(EQr,"roberta"),EQr.forEach(t),P9o=r(UEe," \u2014 "),jj=n(UEe,"A",{href:!0});var yQr=s(jj);$9o=r(yQr,"RobertaForQuestionAnswering"),yQr.forEach(t),I9o=r(UEe," (RoBERTa model)"),UEe.forEach(t),j9o=i(P),i5=n(P,"LI",{});var JEe=s(i5);_ne=n(JEe,"STRONG",{});var wQr=s(_ne);N9o=r(wQr,"roformer"),wQr.forEach(t),D9o=r(JEe," \u2014 "),Nj=n(JEe,"A",{href:!0});var AQr=s(Nj);q9o=r(AQr,"RoFormerForQuestionAnswering"),AQr.forEach(t),G9o=r(JEe," (RoFormer model)"),JEe.forEach(t),O9o=i(P),d5=n(P,"LI",{});var YEe=s(d5);une=n(YEe,"STRONG",{});var LQr=s(une);X9o=r(LQr,"splinter"),LQr.forEach(t),z9o=r(YEe," \u2014 "),Dj=n(YEe,"A",{href:!0});var BQr=s(Dj);V9o=r(BQr,"SplinterForQuestionAnswering"),BQr.forEach(t),W9o=r(YEe," (Splinter model)"),YEe.forEach(t),Q9o=i(P),c5=n(P,"LI",{});var KEe=s(c5);bne=n(KEe,"STRONG",{});var kQr=s(bne);H9o=r(kQr,"squeezebert"),kQr.forEach(t),U9o=r(KEe," \u2014 "),qj=n(KEe,"A",{href:!0});var xQr=s(qj);J9o=r(xQr,"SqueezeBertForQuestionAnswering"),xQr.forEach(t),Y9o=r(KEe," (SqueezeBERT model)"),KEe.forEach(t),K9o=i(P),f5=n(P,"LI",{});var ZEe=s(f5);vne=n(ZEe,"STRONG",{});var RQr=s(vne);Z9o=r(RQr,"xlm"),RQr.forEach(t),eBo=r(ZEe," \u2014 "),Gj=n(ZEe,"A",{href:!0});var SQr=s(Gj);oBo=r(SQr,"XLMForQuestionAnsweringSimple"),SQr.forEach(t),rBo=r(ZEe," (XLM model)"),ZEe.forEach(t),tBo=i(P),m5=n(P,"LI",{});var e3e=s(m5);Tne=n(e3e,"STRONG",{});var PQr=s(Tne);aBo=r(PQr,"xlm-roberta"),PQr.forEach(t),nBo=r(e3e," \u2014 "),Oj=n(e3e,"A",{href:!0});var $Qr=s(Oj);sBo=r($Qr,"XLMRobertaForQuestionAnswering"),$Qr.forEach(t),lBo=r(e3e," (XLM-RoBERTa model)"),e3e.forEach(t),iBo=i(P),g5=n(P,"LI",{});var o3e=s(g5);Fne=n(o3e,"STRONG",{});var IQr=s(Fne);dBo=r(IQr,"xlm-roberta-xl"),IQr.forEach(t),cBo=r(o3e," \u2014 "),Xj=n(o3e,"A",{href:!0});var jQr=s(Xj);fBo=r(jQr,"XLMRobertaXLForQuestionAnswering"),jQr.forEach(t),mBo=r(o3e," (XLM-RoBERTa-XL model)"),o3e.forEach(t),gBo=i(P),h5=n(P,"LI",{});var r3e=s(h5);Cne=n(r3e,"STRONG",{});var NQr=s(Cne);hBo=r(NQr,"xlnet"),NQr.forEach(t),pBo=r(r3e," \u2014 "),zj=n(r3e,"A",{href:!0});var DQr=s(zj);_Bo=r(DQr,"XLNetForQuestionAnsweringSimple"),DQr.forEach(t),uBo=r(r3e," (XLNet model)"),r3e.forEach(t),bBo=i(P),p5=n(P,"LI",{});var t3e=s(p5);Mne=n(t3e,"STRONG",{});var qQr=s(Mne);vBo=r(qQr,"yoso"),qQr.forEach(t),TBo=r(t3e," \u2014 "),Vj=n(t3e,"A",{href:!0});var GQr=s(Vj);FBo=r(GQr,"YosoForQuestionAnswering"),GQr.forEach(t),CBo=r(t3e," (YOSO model)"),t3e.forEach(t),P.forEach(t),MBo=i(Ot),_5=n(Ot,"P",{});var a3e=s(_5);EBo=r(a3e,"The model is set in evaluation mode by default using "),Ene=n(a3e,"CODE",{});var OQr=s(Ene);yBo=r(OQr,"model.eval()"),OQr.forEach(t),wBo=r(a3e,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),yne=n(a3e,"CODE",{});var XQr=s(yne);ABo=r(XQr,"model.train()"),XQr.forEach(t),a3e.forEach(t),LBo=i(Ot),wne=n(Ot,"P",{});var zQr=s(wne);BBo=r(zQr,"Examples:"),zQr.forEach(t),kBo=i(Ot),m(KE.$$.fragment,Ot),Ot.forEach(t),tl.forEach(t),z7e=i(d),_d=n(d,"H2",{class:!0});var YBe=s(_d);u5=n(YBe,"A",{id:!0,class:!0,href:!0});var VQr=s(u5);Ane=n(VQr,"SPAN",{});var WQr=s(Ane);m(ZE.$$.fragment,WQr),WQr.forEach(t),VQr.forEach(t),xBo=i(YBe),Lne=n(YBe,"SPAN",{});var QQr=s(Lne);RBo=r(QQr,"AutoModelForTableQuestionAnswering"),QQr.forEach(t),YBe.forEach(t),V7e=i(d),or=n(d,"DIV",{class:!0});var nl=s(or);m(e3.$$.fragment,nl),SBo=i(nl),ud=n(nl,"P",{});var _z=s(ud);PBo=r(_z,`This is a generic model class that will be instantiated as one of the model classes of the library (with a table question answering head) when created
with the `),Bne=n(_z,"CODE",{});var HQr=s(Bne);$Bo=r(HQr,"from_pretrained()"),HQr.forEach(t),IBo=r(_z,"class method or the "),kne=n(_z,"CODE",{});var UQr=s(kne);jBo=r(UQr,"from_config()"),UQr.forEach(t),NBo=r(_z,`class
method.`),_z.forEach(t),DBo=i(nl),o3=n(nl,"P",{});var KBe=s(o3);qBo=r(KBe,"This class cannot be instantiated directly using "),xne=n(KBe,"CODE",{});var JQr=s(xne);GBo=r(JQr,"__init__()"),JQr.forEach(t),OBo=r(KBe," (throws an error)."),KBe.forEach(t),XBo=i(nl),Hr=n(nl,"DIV",{class:!0});var sl=s(Hr);m(r3.$$.fragment,sl),zBo=i(sl),Rne=n(sl,"P",{});var YQr=s(Rne);VBo=r(YQr,"Instantiates one of the model classes of the library (with a table question answering head) from a configuration."),YQr.forEach(t),WBo=i(sl),bd=n(sl,"P",{});var uz=s(bd);QBo=r(uz,`Note:
Loading a model from its configuration file does `),Sne=n(uz,"STRONG",{});var KQr=s(Sne);HBo=r(KQr,"not"),KQr.forEach(t),UBo=r(uz,` load the model weights. It only affects the
model\u2019s configuration. Use `),Pne=n(uz,"CODE",{});var ZQr=s(Pne);JBo=r(ZQr,"from_pretrained()"),ZQr.forEach(t),YBo=r(uz,"to load the model weights."),uz.forEach(t),KBo=i(sl),$ne=n(sl,"P",{});var eHr=s($ne);ZBo=r(eHr,"Examples:"),eHr.forEach(t),eko=i(sl),m(t3.$$.fragment,sl),sl.forEach(t),oko=i(nl),qe=n(nl,"DIV",{class:!0});var Xt=s(qe);m(a3.$$.fragment,Xt),rko=i(Xt),Ine=n(Xt,"P",{});var oHr=s(Ine);tko=r(oHr,"Instantiate one of the model classes of the library (with a table question answering head) from a pretrained model."),oHr.forEach(t),ako=i(Xt),Ua=n(Xt,"P",{});var v4=s(Ua);nko=r(v4,"The model class to instantiate is selected based on the "),jne=n(v4,"CODE",{});var rHr=s(jne);sko=r(rHr,"model_type"),rHr.forEach(t),lko=r(v4,` property of the config object (either
passed as an argument or loaded from `),Nne=n(v4,"CODE",{});var tHr=s(Nne);iko=r(tHr,"pretrained_model_name_or_path"),tHr.forEach(t),dko=r(v4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Dne=n(v4,"CODE",{});var aHr=s(Dne);cko=r(aHr,"pretrained_model_name_or_path"),aHr.forEach(t),fko=r(v4,":"),v4.forEach(t),mko=i(Xt),qne=n(Xt,"UL",{});var nHr=s(qne);b5=n(nHr,"LI",{});var n3e=s(b5);Gne=n(n3e,"STRONG",{});var sHr=s(Gne);gko=r(sHr,"tapas"),sHr.forEach(t),hko=r(n3e," \u2014 "),Wj=n(n3e,"A",{href:!0});var lHr=s(Wj);pko=r(lHr,"TapasForQuestionAnswering"),lHr.forEach(t),_ko=r(n3e," (TAPAS model)"),n3e.forEach(t),nHr.forEach(t),uko=i(Xt),v5=n(Xt,"P",{});var s3e=s(v5);bko=r(s3e,"The model is set in evaluation mode by default using "),One=n(s3e,"CODE",{});var iHr=s(One);vko=r(iHr,"model.eval()"),iHr.forEach(t),Tko=r(s3e,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Xne=n(s3e,"CODE",{});var dHr=s(Xne);Fko=r(dHr,"model.train()"),dHr.forEach(t),s3e.forEach(t),Cko=i(Xt),zne=n(Xt,"P",{});var cHr=s(zne);Mko=r(cHr,"Examples:"),cHr.forEach(t),Eko=i(Xt),m(n3.$$.fragment,Xt),Xt.forEach(t),nl.forEach(t),W7e=i(d),vd=n(d,"H2",{class:!0});var ZBe=s(vd);T5=n(ZBe,"A",{id:!0,class:!0,href:!0});var fHr=s(T5);Vne=n(fHr,"SPAN",{});var mHr=s(Vne);m(s3.$$.fragment,mHr),mHr.forEach(t),fHr.forEach(t),yko=i(ZBe),Wne=n(ZBe,"SPAN",{});var gHr=s(Wne);wko=r(gHr,"AutoModelForImageClassification"),gHr.forEach(t),ZBe.forEach(t),Q7e=i(d),rr=n(d,"DIV",{class:!0});var ll=s(rr);m(l3.$$.fragment,ll),Ako=i(ll),Td=n(ll,"P",{});var bz=s(Td);Lko=r(bz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a image classification head) when created
with the `),Qne=n(bz,"CODE",{});var hHr=s(Qne);Bko=r(hHr,"from_pretrained()"),hHr.forEach(t),kko=r(bz,"class method or the "),Hne=n(bz,"CODE",{});var pHr=s(Hne);xko=r(pHr,"from_config()"),pHr.forEach(t),Rko=r(bz,`class
method.`),bz.forEach(t),Sko=i(ll),i3=n(ll,"P",{});var eke=s(i3);Pko=r(eke,"This class cannot be instantiated directly using "),Une=n(eke,"CODE",{});var _Hr=s(Une);$ko=r(_Hr,"__init__()"),_Hr.forEach(t),Iko=r(eke," (throws an error)."),eke.forEach(t),jko=i(ll),Ur=n(ll,"DIV",{class:!0});var il=s(Ur);m(d3.$$.fragment,il),Nko=i(il),Jne=n(il,"P",{});var uHr=s(Jne);Dko=r(uHr,"Instantiates one of the model classes of the library (with a image classification head) from a configuration."),uHr.forEach(t),qko=i(il),Fd=n(il,"P",{});var vz=s(Fd);Gko=r(vz,`Note:
Loading a model from its configuration file does `),Yne=n(vz,"STRONG",{});var bHr=s(Yne);Oko=r(bHr,"not"),bHr.forEach(t),Xko=r(vz,` load the model weights. It only affects the
model\u2019s configuration. Use `),Kne=n(vz,"CODE",{});var vHr=s(Kne);zko=r(vHr,"from_pretrained()"),vHr.forEach(t),Vko=r(vz,"to load the model weights."),vz.forEach(t),Wko=i(il),Zne=n(il,"P",{});var THr=s(Zne);Qko=r(THr,"Examples:"),THr.forEach(t),Hko=i(il),m(c3.$$.fragment,il),il.forEach(t),Uko=i(ll),Ge=n(ll,"DIV",{class:!0});var zt=s(Ge);m(f3.$$.fragment,zt),Jko=i(zt),ese=n(zt,"P",{});var FHr=s(ese);Yko=r(FHr,"Instantiate one of the model classes of the library (with a image classification head) from a pretrained model."),FHr.forEach(t),Kko=i(zt),Ja=n(zt,"P",{});var T4=s(Ja);Zko=r(T4,"The model class to instantiate is selected based on the "),ose=n(T4,"CODE",{});var CHr=s(ose);exo=r(CHr,"model_type"),CHr.forEach(t),oxo=r(T4,` property of the config object (either
passed as an argument or loaded from `),rse=n(T4,"CODE",{});var MHr=s(rse);rxo=r(MHr,"pretrained_model_name_or_path"),MHr.forEach(t),txo=r(T4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),tse=n(T4,"CODE",{});var EHr=s(tse);axo=r(EHr,"pretrained_model_name_or_path"),EHr.forEach(t),nxo=r(T4,":"),T4.forEach(t),sxo=i(zt),be=n(zt,"UL",{});var Ke=s(be);F5=n(Ke,"LI",{});var l3e=s(F5);ase=n(l3e,"STRONG",{});var yHr=s(ase);lxo=r(yHr,"beit"),yHr.forEach(t),ixo=r(l3e," \u2014 "),Qj=n(l3e,"A",{href:!0});var wHr=s(Qj);dxo=r(wHr,"BeitForImageClassification"),wHr.forEach(t),cxo=r(l3e," (BEiT model)"),l3e.forEach(t),fxo=i(Ke),C5=n(Ke,"LI",{});var i3e=s(C5);nse=n(i3e,"STRONG",{});var AHr=s(nse);mxo=r(AHr,"convnext"),AHr.forEach(t),gxo=r(i3e," \u2014 "),Hj=n(i3e,"A",{href:!0});var LHr=s(Hj);hxo=r(LHr,"ConvNextForImageClassification"),LHr.forEach(t),pxo=r(i3e," (ConvNext model)"),i3e.forEach(t),_xo=i(Ke),Rs=n(Ke,"LI",{});var DL=s(Rs);sse=n(DL,"STRONG",{});var BHr=s(sse);uxo=r(BHr,"deit"),BHr.forEach(t),bxo=r(DL," \u2014 "),Uj=n(DL,"A",{href:!0});var kHr=s(Uj);vxo=r(kHr,"DeiTForImageClassification"),kHr.forEach(t),Txo=r(DL," or "),Jj=n(DL,"A",{href:!0});var xHr=s(Jj);Fxo=r(xHr,"DeiTForImageClassificationWithTeacher"),xHr.forEach(t),Cxo=r(DL," (DeiT model)"),DL.forEach(t),Mxo=i(Ke),M5=n(Ke,"LI",{});var d3e=s(M5);lse=n(d3e,"STRONG",{});var RHr=s(lse);Exo=r(RHr,"imagegpt"),RHr.forEach(t),yxo=r(d3e," \u2014 "),Yj=n(d3e,"A",{href:!0});var SHr=s(Yj);wxo=r(SHr,"ImageGPTForImageClassification"),SHr.forEach(t),Axo=r(d3e," (ImageGPT model)"),d3e.forEach(t),Lxo=i(Ke),la=n(Ke,"LI",{});var Mf=s(la);ise=n(Mf,"STRONG",{});var PHr=s(ise);Bxo=r(PHr,"perceiver"),PHr.forEach(t),kxo=r(Mf," \u2014 "),Kj=n(Mf,"A",{href:!0});var $Hr=s(Kj);xxo=r($Hr,"PerceiverForImageClassificationLearned"),$Hr.forEach(t),Rxo=r(Mf," or "),Zj=n(Mf,"A",{href:!0});var IHr=s(Zj);Sxo=r(IHr,"PerceiverForImageClassificationFourier"),IHr.forEach(t),Pxo=r(Mf," or "),eN=n(Mf,"A",{href:!0});var jHr=s(eN);$xo=r(jHr,"PerceiverForImageClassificationConvProcessing"),jHr.forEach(t),Ixo=r(Mf," (Perceiver model)"),Mf.forEach(t),jxo=i(Ke),E5=n(Ke,"LI",{});var c3e=s(E5);dse=n(c3e,"STRONG",{});var NHr=s(dse);Nxo=r(NHr,"poolformer"),NHr.forEach(t),Dxo=r(c3e," \u2014 "),oN=n(c3e,"A",{href:!0});var DHr=s(oN);qxo=r(DHr,"PoolFormerForImageClassification"),DHr.forEach(t),Gxo=r(c3e," (PoolFormer model)"),c3e.forEach(t),Oxo=i(Ke),y5=n(Ke,"LI",{});var f3e=s(y5);cse=n(f3e,"STRONG",{});var qHr=s(cse);Xxo=r(qHr,"segformer"),qHr.forEach(t),zxo=r(f3e," \u2014 "),rN=n(f3e,"A",{href:!0});var GHr=s(rN);Vxo=r(GHr,"SegformerForImageClassification"),GHr.forEach(t),Wxo=r(f3e," (SegFormer model)"),f3e.forEach(t),Qxo=i(Ke),w5=n(Ke,"LI",{});var m3e=s(w5);fse=n(m3e,"STRONG",{});var OHr=s(fse);Hxo=r(OHr,"swin"),OHr.forEach(t),Uxo=r(m3e," \u2014 "),tN=n(m3e,"A",{href:!0});var XHr=s(tN);Jxo=r(XHr,"SwinForImageClassification"),XHr.forEach(t),Yxo=r(m3e," (Swin model)"),m3e.forEach(t),Kxo=i(Ke),A5=n(Ke,"LI",{});var g3e=s(A5);mse=n(g3e,"STRONG",{});var zHr=s(mse);Zxo=r(zHr,"vit"),zHr.forEach(t),eRo=r(g3e," \u2014 "),aN=n(g3e,"A",{href:!0});var VHr=s(aN);oRo=r(VHr,"ViTForImageClassification"),VHr.forEach(t),rRo=r(g3e," (ViT model)"),g3e.forEach(t),Ke.forEach(t),tRo=i(zt),L5=n(zt,"P",{});var h3e=s(L5);aRo=r(h3e,"The model is set in evaluation mode by default using "),gse=n(h3e,"CODE",{});var WHr=s(gse);nRo=r(WHr,"model.eval()"),WHr.forEach(t),sRo=r(h3e,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),hse=n(h3e,"CODE",{});var QHr=s(hse);lRo=r(QHr,"model.train()"),QHr.forEach(t),h3e.forEach(t),iRo=i(zt),pse=n(zt,"P",{});var HHr=s(pse);dRo=r(HHr,"Examples:"),HHr.forEach(t),cRo=i(zt),m(m3.$$.fragment,zt),zt.forEach(t),ll.forEach(t),H7e=i(d),Cd=n(d,"H2",{class:!0});var oke=s(Cd);B5=n(oke,"A",{id:!0,class:!0,href:!0});var UHr=s(B5);_se=n(UHr,"SPAN",{});var JHr=s(_se);m(g3.$$.fragment,JHr),JHr.forEach(t),UHr.forEach(t),fRo=i(oke),use=n(oke,"SPAN",{});var YHr=s(use);mRo=r(YHr,"AutoModelForVision2Seq"),YHr.forEach(t),oke.forEach(t),U7e=i(d),tr=n(d,"DIV",{class:!0});var dl=s(tr);m(h3.$$.fragment,dl),gRo=i(dl),Md=n(dl,"P",{});var Tz=s(Md);hRo=r(Tz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a vision-to-text modeling head) when created
with the `),bse=n(Tz,"CODE",{});var KHr=s(bse);pRo=r(KHr,"from_pretrained()"),KHr.forEach(t),_Ro=r(Tz,"class method or the "),vse=n(Tz,"CODE",{});var ZHr=s(vse);uRo=r(ZHr,"from_config()"),ZHr.forEach(t),bRo=r(Tz,`class
method.`),Tz.forEach(t),vRo=i(dl),p3=n(dl,"P",{});var rke=s(p3);TRo=r(rke,"This class cannot be instantiated directly using "),Tse=n(rke,"CODE",{});var eUr=s(Tse);FRo=r(eUr,"__init__()"),eUr.forEach(t),CRo=r(rke," (throws an error)."),rke.forEach(t),MRo=i(dl),Jr=n(dl,"DIV",{class:!0});var cl=s(Jr);m(_3.$$.fragment,cl),ERo=i(cl),Fse=n(cl,"P",{});var oUr=s(Fse);yRo=r(oUr,"Instantiates one of the model classes of the library (with a vision-to-text modeling head) from a configuration."),oUr.forEach(t),wRo=i(cl),Ed=n(cl,"P",{});var Fz=s(Ed);ARo=r(Fz,`Note:
Loading a model from its configuration file does `),Cse=n(Fz,"STRONG",{});var rUr=s(Cse);LRo=r(rUr,"not"),rUr.forEach(t),BRo=r(Fz,` load the model weights. It only affects the
model\u2019s configuration. Use `),Mse=n(Fz,"CODE",{});var tUr=s(Mse);kRo=r(tUr,"from_pretrained()"),tUr.forEach(t),xRo=r(Fz,"to load the model weights."),Fz.forEach(t),RRo=i(cl),Ese=n(cl,"P",{});var aUr=s(Ese);SRo=r(aUr,"Examples:"),aUr.forEach(t),PRo=i(cl),m(u3.$$.fragment,cl),cl.forEach(t),$Ro=i(dl),Oe=n(dl,"DIV",{class:!0});var Vt=s(Oe);m(b3.$$.fragment,Vt),IRo=i(Vt),yse=n(Vt,"P",{});var nUr=s(yse);jRo=r(nUr,"Instantiate one of the model classes of the library (with a vision-to-text modeling head) from a pretrained model."),nUr.forEach(t),NRo=i(Vt),Ya=n(Vt,"P",{});var F4=s(Ya);DRo=r(F4,"The model class to instantiate is selected based on the "),wse=n(F4,"CODE",{});var sUr=s(wse);qRo=r(sUr,"model_type"),sUr.forEach(t),GRo=r(F4,` property of the config object (either
passed as an argument or loaded from `),Ase=n(F4,"CODE",{});var lUr=s(Ase);ORo=r(lUr,"pretrained_model_name_or_path"),lUr.forEach(t),XRo=r(F4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Lse=n(F4,"CODE",{});var iUr=s(Lse);zRo=r(iUr,"pretrained_model_name_or_path"),iUr.forEach(t),VRo=r(F4,":"),F4.forEach(t),WRo=i(Vt),Bse=n(Vt,"UL",{});var dUr=s(Bse);k5=n(dUr,"LI",{});var p3e=s(k5);kse=n(p3e,"STRONG",{});var cUr=s(kse);QRo=r(cUr,"vision-encoder-decoder"),cUr.forEach(t),HRo=r(p3e," \u2014 "),nN=n(p3e,"A",{href:!0});var fUr=s(nN);URo=r(fUr,"VisionEncoderDecoderModel"),fUr.forEach(t),JRo=r(p3e," (Vision Encoder decoder model)"),p3e.forEach(t),dUr.forEach(t),YRo=i(Vt),x5=n(Vt,"P",{});var _3e=s(x5);KRo=r(_3e,"The model is set in evaluation mode by default using "),xse=n(_3e,"CODE",{});var mUr=s(xse);ZRo=r(mUr,"model.eval()"),mUr.forEach(t),eSo=r(_3e,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Rse=n(_3e,"CODE",{});var gUr=s(Rse);oSo=r(gUr,"model.train()"),gUr.forEach(t),_3e.forEach(t),rSo=i(Vt),Sse=n(Vt,"P",{});var hUr=s(Sse);tSo=r(hUr,"Examples:"),hUr.forEach(t),aSo=i(Vt),m(v3.$$.fragment,Vt),Vt.forEach(t),dl.forEach(t),J7e=i(d),yd=n(d,"H2",{class:!0});var tke=s(yd);R5=n(tke,"A",{id:!0,class:!0,href:!0});var pUr=s(R5);Pse=n(pUr,"SPAN",{});var _Ur=s(Pse);m(T3.$$.fragment,_Ur),_Ur.forEach(t),pUr.forEach(t),nSo=i(tke),$se=n(tke,"SPAN",{});var uUr=s($se);sSo=r(uUr,"AutoModelForAudioClassification"),uUr.forEach(t),tke.forEach(t),Y7e=i(d),ar=n(d,"DIV",{class:!0});var fl=s(ar);m(F3.$$.fragment,fl),lSo=i(fl),wd=n(fl,"P",{});var Cz=s(wd);iSo=r(Cz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a audio classification head) when created
with the `),Ise=n(Cz,"CODE",{});var bUr=s(Ise);dSo=r(bUr,"from_pretrained()"),bUr.forEach(t),cSo=r(Cz,"class method or the "),jse=n(Cz,"CODE",{});var vUr=s(jse);fSo=r(vUr,"from_config()"),vUr.forEach(t),mSo=r(Cz,`class
method.`),Cz.forEach(t),gSo=i(fl),C3=n(fl,"P",{});var ake=s(C3);hSo=r(ake,"This class cannot be instantiated directly using "),Nse=n(ake,"CODE",{});var TUr=s(Nse);pSo=r(TUr,"__init__()"),TUr.forEach(t),_So=r(ake," (throws an error)."),ake.forEach(t),uSo=i(fl),Yr=n(fl,"DIV",{class:!0});var ml=s(Yr);m(M3.$$.fragment,ml),bSo=i(ml),Dse=n(ml,"P",{});var FUr=s(Dse);vSo=r(FUr,"Instantiates one of the model classes of the library (with a audio classification head) from a configuration."),FUr.forEach(t),TSo=i(ml),Ad=n(ml,"P",{});var Mz=s(Ad);FSo=r(Mz,`Note:
Loading a model from its configuration file does `),qse=n(Mz,"STRONG",{});var CUr=s(qse);CSo=r(CUr,"not"),CUr.forEach(t),MSo=r(Mz,` load the model weights. It only affects the
model\u2019s configuration. Use `),Gse=n(Mz,"CODE",{});var MUr=s(Gse);ESo=r(MUr,"from_pretrained()"),MUr.forEach(t),ySo=r(Mz,"to load the model weights."),Mz.forEach(t),wSo=i(ml),Ose=n(ml,"P",{});var EUr=s(Ose);ASo=r(EUr,"Examples:"),EUr.forEach(t),LSo=i(ml),m(E3.$$.fragment,ml),ml.forEach(t),BSo=i(fl),Xe=n(fl,"DIV",{class:!0});var Wt=s(Xe);m(y3.$$.fragment,Wt),kSo=i(Wt),Xse=n(Wt,"P",{});var yUr=s(Xse);xSo=r(yUr,"Instantiate one of the model classes of the library (with a audio classification head) from a pretrained model."),yUr.forEach(t),RSo=i(Wt),Ka=n(Wt,"P",{});var C4=s(Ka);SSo=r(C4,"The model class to instantiate is selected based on the "),zse=n(C4,"CODE",{});var wUr=s(zse);PSo=r(wUr,"model_type"),wUr.forEach(t),$So=r(C4,` property of the config object (either
passed as an argument or loaded from `),Vse=n(C4,"CODE",{});var AUr=s(Vse);ISo=r(AUr,"pretrained_model_name_or_path"),AUr.forEach(t),jSo=r(C4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Wse=n(C4,"CODE",{});var LUr=s(Wse);NSo=r(LUr,"pretrained_model_name_or_path"),LUr.forEach(t),DSo=r(C4,":"),C4.forEach(t),qSo=i(Wt),ao=n(Wt,"UL",{});var Qt=s(ao);S5=n(Qt,"LI",{});var u3e=s(S5);Qse=n(u3e,"STRONG",{});var BUr=s(Qse);GSo=r(BUr,"hubert"),BUr.forEach(t),OSo=r(u3e," \u2014 "),sN=n(u3e,"A",{href:!0});var kUr=s(sN);XSo=r(kUr,"HubertForSequenceClassification"),kUr.forEach(t),zSo=r(u3e," (Hubert model)"),u3e.forEach(t),VSo=i(Qt),P5=n(Qt,"LI",{});var b3e=s(P5);Hse=n(b3e,"STRONG",{});var xUr=s(Hse);WSo=r(xUr,"sew"),xUr.forEach(t),QSo=r(b3e," \u2014 "),lN=n(b3e,"A",{href:!0});var RUr=s(lN);HSo=r(RUr,"SEWForSequenceClassification"),RUr.forEach(t),USo=r(b3e," (SEW model)"),b3e.forEach(t),JSo=i(Qt),$5=n(Qt,"LI",{});var v3e=s($5);Use=n(v3e,"STRONG",{});var SUr=s(Use);YSo=r(SUr,"sew-d"),SUr.forEach(t),KSo=r(v3e," \u2014 "),iN=n(v3e,"A",{href:!0});var PUr=s(iN);ZSo=r(PUr,"SEWDForSequenceClassification"),PUr.forEach(t),ePo=r(v3e," (SEW-D model)"),v3e.forEach(t),oPo=i(Qt),I5=n(Qt,"LI",{});var T3e=s(I5);Jse=n(T3e,"STRONG",{});var $Ur=s(Jse);rPo=r($Ur,"unispeech"),$Ur.forEach(t),tPo=r(T3e," \u2014 "),dN=n(T3e,"A",{href:!0});var IUr=s(dN);aPo=r(IUr,"UniSpeechForSequenceClassification"),IUr.forEach(t),nPo=r(T3e," (UniSpeech model)"),T3e.forEach(t),sPo=i(Qt),j5=n(Qt,"LI",{});var F3e=s(j5);Yse=n(F3e,"STRONG",{});var jUr=s(Yse);lPo=r(jUr,"unispeech-sat"),jUr.forEach(t),iPo=r(F3e," \u2014 "),cN=n(F3e,"A",{href:!0});var NUr=s(cN);dPo=r(NUr,"UniSpeechSatForSequenceClassification"),NUr.forEach(t),cPo=r(F3e," (UniSpeechSat model)"),F3e.forEach(t),fPo=i(Qt),N5=n(Qt,"LI",{});var C3e=s(N5);Kse=n(C3e,"STRONG",{});var DUr=s(Kse);mPo=r(DUr,"wav2vec2"),DUr.forEach(t),gPo=r(C3e," \u2014 "),fN=n(C3e,"A",{href:!0});var qUr=s(fN);hPo=r(qUr,"Wav2Vec2ForSequenceClassification"),qUr.forEach(t),pPo=r(C3e," (Wav2Vec2 model)"),C3e.forEach(t),_Po=i(Qt),D5=n(Qt,"LI",{});var M3e=s(D5);Zse=n(M3e,"STRONG",{});var GUr=s(Zse);uPo=r(GUr,"wavlm"),GUr.forEach(t),bPo=r(M3e," \u2014 "),mN=n(M3e,"A",{href:!0});var OUr=s(mN);vPo=r(OUr,"WavLMForSequenceClassification"),OUr.forEach(t),TPo=r(M3e," (WavLM model)"),M3e.forEach(t),Qt.forEach(t),FPo=i(Wt),q5=n(Wt,"P",{});var E3e=s(q5);CPo=r(E3e,"The model is set in evaluation mode by default using "),ele=n(E3e,"CODE",{});var XUr=s(ele);MPo=r(XUr,"model.eval()"),XUr.forEach(t),EPo=r(E3e,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),ole=n(E3e,"CODE",{});var zUr=s(ole);yPo=r(zUr,"model.train()"),zUr.forEach(t),E3e.forEach(t),wPo=i(Wt),rle=n(Wt,"P",{});var VUr=s(rle);APo=r(VUr,"Examples:"),VUr.forEach(t),LPo=i(Wt),m(w3.$$.fragment,Wt),Wt.forEach(t),fl.forEach(t),K7e=i(d),Ld=n(d,"H2",{class:!0});var nke=s(Ld);G5=n(nke,"A",{id:!0,class:!0,href:!0});var WUr=s(G5);tle=n(WUr,"SPAN",{});var QUr=s(tle);m(A3.$$.fragment,QUr),QUr.forEach(t),WUr.forEach(t),BPo=i(nke),ale=n(nke,"SPAN",{});var HUr=s(ale);kPo=r(HUr,"AutoModelForAudioFrameClassification"),HUr.forEach(t),nke.forEach(t),Z7e=i(d),nr=n(d,"DIV",{class:!0});var gl=s(nr);m(L3.$$.fragment,gl),xPo=i(gl),Bd=n(gl,"P",{});var Ez=s(Bd);RPo=r(Ez,`This is a generic model class that will be instantiated as one of the model classes of the library (with a audio frame (token) classification head) when created
with the `),nle=n(Ez,"CODE",{});var UUr=s(nle);SPo=r(UUr,"from_pretrained()"),UUr.forEach(t),PPo=r(Ez,"class method or the "),sle=n(Ez,"CODE",{});var JUr=s(sle);$Po=r(JUr,"from_config()"),JUr.forEach(t),IPo=r(Ez,`class
method.`),Ez.forEach(t),jPo=i(gl),B3=n(gl,"P",{});var ske=s(B3);NPo=r(ske,"This class cannot be instantiated directly using "),lle=n(ske,"CODE",{});var YUr=s(lle);DPo=r(YUr,"__init__()"),YUr.forEach(t),qPo=r(ske," (throws an error)."),ske.forEach(t),GPo=i(gl),Kr=n(gl,"DIV",{class:!0});var hl=s(Kr);m(k3.$$.fragment,hl),OPo=i(hl),ile=n(hl,"P",{});var KUr=s(ile);XPo=r(KUr,"Instantiates one of the model classes of the library (with a audio frame (token) classification head) from a configuration."),KUr.forEach(t),zPo=i(hl),kd=n(hl,"P",{});var yz=s(kd);VPo=r(yz,`Note:
Loading a model from its configuration file does `),dle=n(yz,"STRONG",{});var ZUr=s(dle);WPo=r(ZUr,"not"),ZUr.forEach(t),QPo=r(yz,` load the model weights. It only affects the
model\u2019s configuration. Use `),cle=n(yz,"CODE",{});var eJr=s(cle);HPo=r(eJr,"from_pretrained()"),eJr.forEach(t),UPo=r(yz,"to load the model weights."),yz.forEach(t),JPo=i(hl),fle=n(hl,"P",{});var oJr=s(fle);YPo=r(oJr,"Examples:"),oJr.forEach(t),KPo=i(hl),m(x3.$$.fragment,hl),hl.forEach(t),ZPo=i(gl),ze=n(gl,"DIV",{class:!0});var Ht=s(ze);m(R3.$$.fragment,Ht),e$o=i(Ht),mle=n(Ht,"P",{});var rJr=s(mle);o$o=r(rJr,"Instantiate one of the model classes of the library (with a audio frame (token) classification head) from a pretrained model."),rJr.forEach(t),r$o=i(Ht),Za=n(Ht,"P",{});var M4=s(Za);t$o=r(M4,"The model class to instantiate is selected based on the "),gle=n(M4,"CODE",{});var tJr=s(gle);a$o=r(tJr,"model_type"),tJr.forEach(t),n$o=r(M4,` property of the config object (either
passed as an argument or loaded from `),hle=n(M4,"CODE",{});var aJr=s(hle);s$o=r(aJr,"pretrained_model_name_or_path"),aJr.forEach(t),l$o=r(M4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),ple=n(M4,"CODE",{});var nJr=s(ple);i$o=r(nJr,"pretrained_model_name_or_path"),nJr.forEach(t),d$o=r(M4,":"),M4.forEach(t),c$o=i(Ht),xd=n(Ht,"UL",{});var wz=s(xd);O5=n(wz,"LI",{});var y3e=s(O5);_le=n(y3e,"STRONG",{});var sJr=s(_le);f$o=r(sJr,"unispeech-sat"),sJr.forEach(t),m$o=r(y3e," \u2014 "),gN=n(y3e,"A",{href:!0});var lJr=s(gN);g$o=r(lJr,"UniSpeechSatForAudioFrameClassification"),lJr.forEach(t),h$o=r(y3e," (UniSpeechSat model)"),y3e.forEach(t),p$o=i(wz),X5=n(wz,"LI",{});var w3e=s(X5);ule=n(w3e,"STRONG",{});var iJr=s(ule);_$o=r(iJr,"wav2vec2"),iJr.forEach(t),u$o=r(w3e," \u2014 "),hN=n(w3e,"A",{href:!0});var dJr=s(hN);b$o=r(dJr,"Wav2Vec2ForAudioFrameClassification"),dJr.forEach(t),v$o=r(w3e," (Wav2Vec2 model)"),w3e.forEach(t),T$o=i(wz),z5=n(wz,"LI",{});var A3e=s(z5);ble=n(A3e,"STRONG",{});var cJr=s(ble);F$o=r(cJr,"wavlm"),cJr.forEach(t),C$o=r(A3e," \u2014 "),pN=n(A3e,"A",{href:!0});var fJr=s(pN);M$o=r(fJr,"WavLMForAudioFrameClassification"),fJr.forEach(t),E$o=r(A3e," (WavLM model)"),A3e.forEach(t),wz.forEach(t),y$o=i(Ht),V5=n(Ht,"P",{});var L3e=s(V5);w$o=r(L3e,"The model is set in evaluation mode by default using "),vle=n(L3e,"CODE",{});var mJr=s(vle);A$o=r(mJr,"model.eval()"),mJr.forEach(t),L$o=r(L3e,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Tle=n(L3e,"CODE",{});var gJr=s(Tle);B$o=r(gJr,"model.train()"),gJr.forEach(t),L3e.forEach(t),k$o=i(Ht),Fle=n(Ht,"P",{});var hJr=s(Fle);x$o=r(hJr,"Examples:"),hJr.forEach(t),R$o=i(Ht),m(S3.$$.fragment,Ht),Ht.forEach(t),gl.forEach(t),e9e=i(d),Rd=n(d,"H2",{class:!0});var lke=s(Rd);W5=n(lke,"A",{id:!0,class:!0,href:!0});var pJr=s(W5);Cle=n(pJr,"SPAN",{});var _Jr=s(Cle);m(P3.$$.fragment,_Jr),_Jr.forEach(t),pJr.forEach(t),S$o=i(lke),Mle=n(lke,"SPAN",{});var uJr=s(Mle);P$o=r(uJr,"AutoModelForCTC"),uJr.forEach(t),lke.forEach(t),o9e=i(d),sr=n(d,"DIV",{class:!0});var pl=s(sr);m($3.$$.fragment,pl),$$o=i(pl),Sd=n(pl,"P",{});var Az=s(Sd);I$o=r(Az,`This is a generic model class that will be instantiated as one of the model classes of the library (with a connectionist temporal classification head) when created
with the `),Ele=n(Az,"CODE",{});var bJr=s(Ele);j$o=r(bJr,"from_pretrained()"),bJr.forEach(t),N$o=r(Az,"class method or the "),yle=n(Az,"CODE",{});var vJr=s(yle);D$o=r(vJr,"from_config()"),vJr.forEach(t),q$o=r(Az,`class
method.`),Az.forEach(t),G$o=i(pl),I3=n(pl,"P",{});var ike=s(I3);O$o=r(ike,"This class cannot be instantiated directly using "),wle=n(ike,"CODE",{});var TJr=s(wle);X$o=r(TJr,"__init__()"),TJr.forEach(t),z$o=r(ike," (throws an error)."),ike.forEach(t),V$o=i(pl),Zr=n(pl,"DIV",{class:!0});var _l=s(Zr);m(j3.$$.fragment,_l),W$o=i(_l),Ale=n(_l,"P",{});var FJr=s(Ale);Q$o=r(FJr,"Instantiates one of the model classes of the library (with a connectionist temporal classification head) from a configuration."),FJr.forEach(t),H$o=i(_l),Pd=n(_l,"P",{});var Lz=s(Pd);U$o=r(Lz,`Note:
Loading a model from its configuration file does `),Lle=n(Lz,"STRONG",{});var CJr=s(Lle);J$o=r(CJr,"not"),CJr.forEach(t),Y$o=r(Lz,` load the model weights. It only affects the
model\u2019s configuration. Use `),Ble=n(Lz,"CODE",{});var MJr=s(Ble);K$o=r(MJr,"from_pretrained()"),MJr.forEach(t),Z$o=r(Lz,"to load the model weights."),Lz.forEach(t),eIo=i(_l),kle=n(_l,"P",{});var EJr=s(kle);oIo=r(EJr,"Examples:"),EJr.forEach(t),rIo=i(_l),m(N3.$$.fragment,_l),_l.forEach(t),tIo=i(pl),Ve=n(pl,"DIV",{class:!0});var Ut=s(Ve);m(D3.$$.fragment,Ut),aIo=i(Ut),xle=n(Ut,"P",{});var yJr=s(xle);nIo=r(yJr,"Instantiate one of the model classes of the library (with a connectionist temporal classification head) from a pretrained model."),yJr.forEach(t),sIo=i(Ut),en=n(Ut,"P",{});var E4=s(en);lIo=r(E4,"The model class to instantiate is selected based on the "),Rle=n(E4,"CODE",{});var wJr=s(Rle);iIo=r(wJr,"model_type"),wJr.forEach(t),dIo=r(E4,` property of the config object (either
passed as an argument or loaded from `),Sle=n(E4,"CODE",{});var AJr=s(Sle);cIo=r(AJr,"pretrained_model_name_or_path"),AJr.forEach(t),fIo=r(E4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Ple=n(E4,"CODE",{});var LJr=s(Ple);mIo=r(LJr,"pretrained_model_name_or_path"),LJr.forEach(t),gIo=r(E4,":"),E4.forEach(t),hIo=i(Ut),no=n(Ut,"UL",{});var Jt=s(no);Q5=n(Jt,"LI",{});var B3e=s(Q5);$le=n(B3e,"STRONG",{});var BJr=s($le);pIo=r(BJr,"hubert"),BJr.forEach(t),_Io=r(B3e," \u2014 "),_N=n(B3e,"A",{href:!0});var kJr=s(_N);uIo=r(kJr,"HubertForCTC"),kJr.forEach(t),bIo=r(B3e," (Hubert model)"),B3e.forEach(t),vIo=i(Jt),H5=n(Jt,"LI",{});var k3e=s(H5);Ile=n(k3e,"STRONG",{});var xJr=s(Ile);TIo=r(xJr,"sew"),xJr.forEach(t),FIo=r(k3e," \u2014 "),uN=n(k3e,"A",{href:!0});var RJr=s(uN);CIo=r(RJr,"SEWForCTC"),RJr.forEach(t),MIo=r(k3e," (SEW model)"),k3e.forEach(t),EIo=i(Jt),U5=n(Jt,"LI",{});var x3e=s(U5);jle=n(x3e,"STRONG",{});var SJr=s(jle);yIo=r(SJr,"sew-d"),SJr.forEach(t),wIo=r(x3e," \u2014 "),bN=n(x3e,"A",{href:!0});var PJr=s(bN);AIo=r(PJr,"SEWDForCTC"),PJr.forEach(t),LIo=r(x3e," (SEW-D model)"),x3e.forEach(t),BIo=i(Jt),J5=n(Jt,"LI",{});var R3e=s(J5);Nle=n(R3e,"STRONG",{});var $Jr=s(Nle);kIo=r($Jr,"unispeech"),$Jr.forEach(t),xIo=r(R3e," \u2014 "),vN=n(R3e,"A",{href:!0});var IJr=s(vN);RIo=r(IJr,"UniSpeechForCTC"),IJr.forEach(t),SIo=r(R3e," (UniSpeech model)"),R3e.forEach(t),PIo=i(Jt),Y5=n(Jt,"LI",{});var S3e=s(Y5);Dle=n(S3e,"STRONG",{});var jJr=s(Dle);$Io=r(jJr,"unispeech-sat"),jJr.forEach(t),IIo=r(S3e," \u2014 "),TN=n(S3e,"A",{href:!0});var NJr=s(TN);jIo=r(NJr,"UniSpeechSatForCTC"),NJr.forEach(t),NIo=r(S3e," (UniSpeechSat model)"),S3e.forEach(t),DIo=i(Jt),K5=n(Jt,"LI",{});var P3e=s(K5);qle=n(P3e,"STRONG",{});var DJr=s(qle);qIo=r(DJr,"wav2vec2"),DJr.forEach(t),GIo=r(P3e," \u2014 "),FN=n(P3e,"A",{href:!0});var qJr=s(FN);OIo=r(qJr,"Wav2Vec2ForCTC"),qJr.forEach(t),XIo=r(P3e," (Wav2Vec2 model)"),P3e.forEach(t),zIo=i(Jt),Z5=n(Jt,"LI",{});var $3e=s(Z5);Gle=n($3e,"STRONG",{});var GJr=s(Gle);VIo=r(GJr,"wavlm"),GJr.forEach(t),WIo=r($3e," \u2014 "),CN=n($3e,"A",{href:!0});var OJr=s(CN);QIo=r(OJr,"WavLMForCTC"),OJr.forEach(t),HIo=r($3e," (WavLM model)"),$3e.forEach(t),Jt.forEach(t),UIo=i(Ut),ev=n(Ut,"P",{});var I3e=s(ev);JIo=r(I3e,"The model is set in evaluation mode by default using "),Ole=n(I3e,"CODE",{});var XJr=s(Ole);YIo=r(XJr,"model.eval()"),XJr.forEach(t),KIo=r(I3e,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Xle=n(I3e,"CODE",{});var zJr=s(Xle);ZIo=r(zJr,"model.train()"),zJr.forEach(t),I3e.forEach(t),ejo=i(Ut),zle=n(Ut,"P",{});var VJr=s(zle);ojo=r(VJr,"Examples:"),VJr.forEach(t),rjo=i(Ut),m(q3.$$.fragment,Ut),Ut.forEach(t),pl.forEach(t),r9e=i(d),$d=n(d,"H2",{class:!0});var dke=s($d);ov=n(dke,"A",{id:!0,class:!0,href:!0});var WJr=s(ov);Vle=n(WJr,"SPAN",{});var QJr=s(Vle);m(G3.$$.fragment,QJr),QJr.forEach(t),WJr.forEach(t),tjo=i(dke),Wle=n(dke,"SPAN",{});var HJr=s(Wle);ajo=r(HJr,"AutoModelForSpeechSeq2Seq"),HJr.forEach(t),dke.forEach(t),t9e=i(d),lr=n(d,"DIV",{class:!0});var ul=s(lr);m(O3.$$.fragment,ul),njo=i(ul),Id=n(ul,"P",{});var Bz=s(Id);sjo=r(Bz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) when created
with the `),Qle=n(Bz,"CODE",{});var UJr=s(Qle);ljo=r(UJr,"from_pretrained()"),UJr.forEach(t),ijo=r(Bz,"class method or the "),Hle=n(Bz,"CODE",{});var JJr=s(Hle);djo=r(JJr,"from_config()"),JJr.forEach(t),cjo=r(Bz,`class
method.`),Bz.forEach(t),fjo=i(ul),X3=n(ul,"P",{});var cke=s(X3);mjo=r(cke,"This class cannot be instantiated directly using "),Ule=n(cke,"CODE",{});var YJr=s(Ule);gjo=r(YJr,"__init__()"),YJr.forEach(t),hjo=r(cke," (throws an error)."),cke.forEach(t),pjo=i(ul),et=n(ul,"DIV",{class:!0});var bl=s(et);m(z3.$$.fragment,bl),_jo=i(bl),Jle=n(bl,"P",{});var KJr=s(Jle);ujo=r(KJr,"Instantiates one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) from a configuration."),KJr.forEach(t),bjo=i(bl),jd=n(bl,"P",{});var kz=s(jd);vjo=r(kz,`Note:
Loading a model from its configuration file does `),Yle=n(kz,"STRONG",{});var ZJr=s(Yle);Tjo=r(ZJr,"not"),ZJr.forEach(t),Fjo=r(kz,` load the model weights. It only affects the
model\u2019s configuration. Use `),Kle=n(kz,"CODE",{});var eYr=s(Kle);Cjo=r(eYr,"from_pretrained()"),eYr.forEach(t),Mjo=r(kz,"to load the model weights."),kz.forEach(t),Ejo=i(bl),Zle=n(bl,"P",{});var oYr=s(Zle);yjo=r(oYr,"Examples:"),oYr.forEach(t),wjo=i(bl),m(V3.$$.fragment,bl),bl.forEach(t),Ajo=i(ul),We=n(ul,"DIV",{class:!0});var Yt=s(We);m(W3.$$.fragment,Yt),Ljo=i(Yt),eie=n(Yt,"P",{});var rYr=s(eie);Bjo=r(rYr,"Instantiate one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) from a pretrained model."),rYr.forEach(t),kjo=i(Yt),on=n(Yt,"P",{});var y4=s(on);xjo=r(y4,"The model class to instantiate is selected based on the "),oie=n(y4,"CODE",{});var tYr=s(oie);Rjo=r(tYr,"model_type"),tYr.forEach(t),Sjo=r(y4,` property of the config object (either
passed as an argument or loaded from `),rie=n(y4,"CODE",{});var aYr=s(rie);Pjo=r(aYr,"pretrained_model_name_or_path"),aYr.forEach(t),$jo=r(y4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),tie=n(y4,"CODE",{});var nYr=s(tie);Ijo=r(nYr,"pretrained_model_name_or_path"),nYr.forEach(t),jjo=r(y4,":"),y4.forEach(t),Njo=i(Yt),Q3=n(Yt,"UL",{});var fke=s(Q3);rv=n(fke,"LI",{});var j3e=s(rv);aie=n(j3e,"STRONG",{});var sYr=s(aie);Djo=r(sYr,"speech-encoder-decoder"),sYr.forEach(t),qjo=r(j3e," \u2014 "),MN=n(j3e,"A",{href:!0});var lYr=s(MN);Gjo=r(lYr,"SpeechEncoderDecoderModel"),lYr.forEach(t),Ojo=r(j3e," (Speech Encoder decoder model)"),j3e.forEach(t),Xjo=i(fke),tv=n(fke,"LI",{});var N3e=s(tv);nie=n(N3e,"STRONG",{});var iYr=s(nie);zjo=r(iYr,"speech_to_text"),iYr.forEach(t),Vjo=r(N3e," \u2014 "),EN=n(N3e,"A",{href:!0});var dYr=s(EN);Wjo=r(dYr,"Speech2TextForConditionalGeneration"),dYr.forEach(t),Qjo=r(N3e," (Speech2Text model)"),N3e.forEach(t),fke.forEach(t),Hjo=i(Yt),av=n(Yt,"P",{});var D3e=s(av);Ujo=r(D3e,"The model is set in evaluation mode by default using "),sie=n(D3e,"CODE",{});var cYr=s(sie);Jjo=r(cYr,"model.eval()"),cYr.forEach(t),Yjo=r(D3e,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),lie=n(D3e,"CODE",{});var fYr=s(lie);Kjo=r(fYr,"model.train()"),fYr.forEach(t),D3e.forEach(t),Zjo=i(Yt),iie=n(Yt,"P",{});var mYr=s(iie);eNo=r(mYr,"Examples:"),mYr.forEach(t),oNo=i(Yt),m(H3.$$.fragment,Yt),Yt.forEach(t),ul.forEach(t),a9e=i(d),Nd=n(d,"H2",{class:!0});var mke=s(Nd);nv=n(mke,"A",{id:!0,class:!0,href:!0});var gYr=s(nv);die=n(gYr,"SPAN",{});var hYr=s(die);m(U3.$$.fragment,hYr),hYr.forEach(t),gYr.forEach(t),rNo=i(mke),cie=n(mke,"SPAN",{});var pYr=s(cie);tNo=r(pYr,"AutoModelForAudioXVector"),pYr.forEach(t),mke.forEach(t),n9e=i(d),ir=n(d,"DIV",{class:!0});var vl=s(ir);m(J3.$$.fragment,vl),aNo=i(vl),Dd=n(vl,"P",{});var xz=s(Dd);nNo=r(xz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a audio retrieval via x-vector head) when created
with the `),fie=n(xz,"CODE",{});var _Yr=s(fie);sNo=r(_Yr,"from_pretrained()"),_Yr.forEach(t),lNo=r(xz,"class method or the "),mie=n(xz,"CODE",{});var uYr=s(mie);iNo=r(uYr,"from_config()"),uYr.forEach(t),dNo=r(xz,`class
method.`),xz.forEach(t),cNo=i(vl),Y3=n(vl,"P",{});var gke=s(Y3);fNo=r(gke,"This class cannot be instantiated directly using "),gie=n(gke,"CODE",{});var bYr=s(gie);mNo=r(bYr,"__init__()"),bYr.forEach(t),gNo=r(gke," (throws an error)."),gke.forEach(t),hNo=i(vl),ot=n(vl,"DIV",{class:!0});var Tl=s(ot);m(K3.$$.fragment,Tl),pNo=i(Tl),hie=n(Tl,"P",{});var vYr=s(hie);_No=r(vYr,"Instantiates one of the model classes of the library (with a audio retrieval via x-vector head) from a configuration."),vYr.forEach(t),uNo=i(Tl),qd=n(Tl,"P",{});var Rz=s(qd);bNo=r(Rz,`Note:
Loading a model from its configuration file does `),pie=n(Rz,"STRONG",{});var TYr=s(pie);vNo=r(TYr,"not"),TYr.forEach(t),TNo=r(Rz,` load the model weights. It only affects the
model\u2019s configuration. Use `),_ie=n(Rz,"CODE",{});var FYr=s(_ie);FNo=r(FYr,"from_pretrained()"),FYr.forEach(t),CNo=r(Rz,"to load the model weights."),Rz.forEach(t),MNo=i(Tl),uie=n(Tl,"P",{});var CYr=s(uie);ENo=r(CYr,"Examples:"),CYr.forEach(t),yNo=i(Tl),m(Z3.$$.fragment,Tl),Tl.forEach(t),wNo=i(vl),Qe=n(vl,"DIV",{class:!0});var Kt=s(Qe);m(ey.$$.fragment,Kt),ANo=i(Kt),bie=n(Kt,"P",{});var MYr=s(bie);LNo=r(MYr,"Instantiate one of the model classes of the library (with a audio retrieval via x-vector head) from a pretrained model."),MYr.forEach(t),BNo=i(Kt),rn=n(Kt,"P",{});var w4=s(rn);kNo=r(w4,"The model class to instantiate is selected based on the "),vie=n(w4,"CODE",{});var EYr=s(vie);xNo=r(EYr,"model_type"),EYr.forEach(t),RNo=r(w4,` property of the config object (either
passed as an argument or loaded from `),Tie=n(w4,"CODE",{});var yYr=s(Tie);SNo=r(yYr,"pretrained_model_name_or_path"),yYr.forEach(t),PNo=r(w4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Fie=n(w4,"CODE",{});var wYr=s(Fie);$No=r(wYr,"pretrained_model_name_or_path"),wYr.forEach(t),INo=r(w4,":"),w4.forEach(t),jNo=i(Kt),Gd=n(Kt,"UL",{});var Sz=s(Gd);sv=n(Sz,"LI",{});var q3e=s(sv);Cie=n(q3e,"STRONG",{});var AYr=s(Cie);NNo=r(AYr,"unispeech-sat"),AYr.forEach(t),DNo=r(q3e," \u2014 "),yN=n(q3e,"A",{href:!0});var LYr=s(yN);qNo=r(LYr,"UniSpeechSatForXVector"),LYr.forEach(t),GNo=r(q3e," (UniSpeechSat model)"),q3e.forEach(t),ONo=i(Sz),lv=n(Sz,"LI",{});var G3e=s(lv);Mie=n(G3e,"STRONG",{});var BYr=s(Mie);XNo=r(BYr,"wav2vec2"),BYr.forEach(t),zNo=r(G3e," \u2014 "),wN=n(G3e,"A",{href:!0});var kYr=s(wN);VNo=r(kYr,"Wav2Vec2ForXVector"),kYr.forEach(t),WNo=r(G3e," (Wav2Vec2 model)"),G3e.forEach(t),QNo=i(Sz),iv=n(Sz,"LI",{});var O3e=s(iv);Eie=n(O3e,"STRONG",{});var xYr=s(Eie);HNo=r(xYr,"wavlm"),xYr.forEach(t),UNo=r(O3e," \u2014 "),AN=n(O3e,"A",{href:!0});var RYr=s(AN);JNo=r(RYr,"WavLMForXVector"),RYr.forEach(t),YNo=r(O3e," (WavLM model)"),O3e.forEach(t),Sz.forEach(t),KNo=i(Kt),dv=n(Kt,"P",{});var X3e=s(dv);ZNo=r(X3e,"The model is set in evaluation mode by default using "),yie=n(X3e,"CODE",{});var SYr=s(yie);eDo=r(SYr,"model.eval()"),SYr.forEach(t),oDo=r(X3e,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),wie=n(X3e,"CODE",{});var PYr=s(wie);rDo=r(PYr,"model.train()"),PYr.forEach(t),X3e.forEach(t),tDo=i(Kt),Aie=n(Kt,"P",{});var $Yr=s(Aie);aDo=r($Yr,"Examples:"),$Yr.forEach(t),nDo=i(Kt),m(oy.$$.fragment,Kt),Kt.forEach(t),vl.forEach(t),s9e=i(d),Od=n(d,"H2",{class:!0});var hke=s(Od);cv=n(hke,"A",{id:!0,class:!0,href:!0});var IYr=s(cv);Lie=n(IYr,"SPAN",{});var jYr=s(Lie);m(ry.$$.fragment,jYr),jYr.forEach(t),IYr.forEach(t),sDo=i(hke),Bie=n(hke,"SPAN",{});var NYr=s(Bie);lDo=r(NYr,"AutoModelForMaskedImageModeling"),NYr.forEach(t),hke.forEach(t),l9e=i(d),dr=n(d,"DIV",{class:!0});var Fl=s(dr);m(ty.$$.fragment,Fl),iDo=i(Fl),Xd=n(Fl,"P",{});var Pz=s(Xd);dDo=r(Pz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a masked image modeling head) when created
with the `),kie=n(Pz,"CODE",{});var DYr=s(kie);cDo=r(DYr,"from_pretrained()"),DYr.forEach(t),fDo=r(Pz,"class method or the "),xie=n(Pz,"CODE",{});var qYr=s(xie);mDo=r(qYr,"from_config()"),qYr.forEach(t),gDo=r(Pz,`class
method.`),Pz.forEach(t),hDo=i(Fl),ay=n(Fl,"P",{});var pke=s(ay);pDo=r(pke,"This class cannot be instantiated directly using "),Rie=n(pke,"CODE",{});var GYr=s(Rie);_Do=r(GYr,"__init__()"),GYr.forEach(t),uDo=r(pke," (throws an error)."),pke.forEach(t),bDo=i(Fl),rt=n(Fl,"DIV",{class:!0});var Cl=s(rt);m(ny.$$.fragment,Cl),vDo=i(Cl),Sie=n(Cl,"P",{});var OYr=s(Sie);TDo=r(OYr,"Instantiates one of the model classes of the library (with a masked image modeling head) from a configuration."),OYr.forEach(t),FDo=i(Cl),zd=n(Cl,"P",{});var $z=s(zd);CDo=r($z,`Note:
Loading a model from its configuration file does `),Pie=n($z,"STRONG",{});var XYr=s(Pie);MDo=r(XYr,"not"),XYr.forEach(t),EDo=r($z,` load the model weights. It only affects the
model\u2019s configuration. Use `),$ie=n($z,"CODE",{});var zYr=s($ie);yDo=r(zYr,"from_pretrained()"),zYr.forEach(t),wDo=r($z,"to load the model weights."),$z.forEach(t),ADo=i(Cl),Iie=n(Cl,"P",{});var VYr=s(Iie);LDo=r(VYr,"Examples:"),VYr.forEach(t),BDo=i(Cl),m(sy.$$.fragment,Cl),Cl.forEach(t),kDo=i(Fl),He=n(Fl,"DIV",{class:!0});var Zt=s(He);m(ly.$$.fragment,Zt),xDo=i(Zt),jie=n(Zt,"P",{});var WYr=s(jie);RDo=r(WYr,"Instantiate one of the model classes of the library (with a masked image modeling head) from a pretrained model."),WYr.forEach(t),SDo=i(Zt),tn=n(Zt,"P",{});var A4=s(tn);PDo=r(A4,"The model class to instantiate is selected based on the "),Nie=n(A4,"CODE",{});var QYr=s(Nie);$Do=r(QYr,"model_type"),QYr.forEach(t),IDo=r(A4,` property of the config object (either
passed as an argument or loaded from `),Die=n(A4,"CODE",{});var HYr=s(Die);jDo=r(HYr,"pretrained_model_name_or_path"),HYr.forEach(t),NDo=r(A4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),qie=n(A4,"CODE",{});var UYr=s(qie);DDo=r(UYr,"pretrained_model_name_or_path"),UYr.forEach(t),qDo=r(A4,":"),A4.forEach(t),GDo=i(Zt),Vd=n(Zt,"UL",{});var Iz=s(Vd);fv=n(Iz,"LI",{});var z3e=s(fv);Gie=n(z3e,"STRONG",{});var JYr=s(Gie);ODo=r(JYr,"deit"),JYr.forEach(t),XDo=r(z3e," \u2014 "),LN=n(z3e,"A",{href:!0});var YYr=s(LN);zDo=r(YYr,"DeiTForMaskedImageModeling"),YYr.forEach(t),VDo=r(z3e," (DeiT model)"),z3e.forEach(t),WDo=i(Iz),mv=n(Iz,"LI",{});var V3e=s(mv);Oie=n(V3e,"STRONG",{});var KYr=s(Oie);QDo=r(KYr,"swin"),KYr.forEach(t),HDo=r(V3e," \u2014 "),BN=n(V3e,"A",{href:!0});var ZYr=s(BN);UDo=r(ZYr,"SwinForMaskedImageModeling"),ZYr.forEach(t),JDo=r(V3e," (Swin model)"),V3e.forEach(t),YDo=i(Iz),gv=n(Iz,"LI",{});var W3e=s(gv);Xie=n(W3e,"STRONG",{});var eKr=s(Xie);KDo=r(eKr,"vit"),eKr.forEach(t),ZDo=r(W3e," \u2014 "),kN=n(W3e,"A",{href:!0});var oKr=s(kN);eqo=r(oKr,"ViTForMaskedImageModeling"),oKr.forEach(t),oqo=r(W3e," (ViT model)"),W3e.forEach(t),Iz.forEach(t),rqo=i(Zt),hv=n(Zt,"P",{});var Q3e=s(hv);tqo=r(Q3e,"The model is set in evaluation mode by default using "),zie=n(Q3e,"CODE",{});var rKr=s(zie);aqo=r(rKr,"model.eval()"),rKr.forEach(t),nqo=r(Q3e,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Vie=n(Q3e,"CODE",{});var tKr=s(Vie);sqo=r(tKr,"model.train()"),tKr.forEach(t),Q3e.forEach(t),lqo=i(Zt),Wie=n(Zt,"P",{});var aKr=s(Wie);iqo=r(aKr,"Examples:"),aKr.forEach(t),dqo=i(Zt),m(iy.$$.fragment,Zt),Zt.forEach(t),Fl.forEach(t),i9e=i(d),Wd=n(d,"H2",{class:!0});var _ke=s(Wd);pv=n(_ke,"A",{id:!0,class:!0,href:!0});var nKr=s(pv);Qie=n(nKr,"SPAN",{});var sKr=s(Qie);m(dy.$$.fragment,sKr),sKr.forEach(t),nKr.forEach(t),cqo=i(_ke),Hie=n(_ke,"SPAN",{});var lKr=s(Hie);fqo=r(lKr,"AutoModelForObjectDetection"),lKr.forEach(t),_ke.forEach(t),d9e=i(d),cr=n(d,"DIV",{class:!0});var Ml=s(cr);m(cy.$$.fragment,Ml),mqo=i(Ml),Qd=n(Ml,"P",{});var jz=s(Qd);gqo=r(jz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a object detection head) when created
with the `),Uie=n(jz,"CODE",{});var iKr=s(Uie);hqo=r(iKr,"from_pretrained()"),iKr.forEach(t),pqo=r(jz,"class method or the "),Jie=n(jz,"CODE",{});var dKr=s(Jie);_qo=r(dKr,"from_config()"),dKr.forEach(t),uqo=r(jz,`class
method.`),jz.forEach(t),bqo=i(Ml),fy=n(Ml,"P",{});var uke=s(fy);vqo=r(uke,"This class cannot be instantiated directly using "),Yie=n(uke,"CODE",{});var cKr=s(Yie);Tqo=r(cKr,"__init__()"),cKr.forEach(t),Fqo=r(uke," (throws an error)."),uke.forEach(t),Cqo=i(Ml),tt=n(Ml,"DIV",{class:!0});var El=s(tt);m(my.$$.fragment,El),Mqo=i(El),Kie=n(El,"P",{});var fKr=s(Kie);Eqo=r(fKr,"Instantiates one of the model classes of the library (with a object detection head) from a configuration."),fKr.forEach(t),yqo=i(El),Hd=n(El,"P",{});var Nz=s(Hd);wqo=r(Nz,`Note:
Loading a model from its configuration file does `),Zie=n(Nz,"STRONG",{});var mKr=s(Zie);Aqo=r(mKr,"not"),mKr.forEach(t),Lqo=r(Nz,` load the model weights. It only affects the
model\u2019s configuration. Use `),ede=n(Nz,"CODE",{});var gKr=s(ede);Bqo=r(gKr,"from_pretrained()"),gKr.forEach(t),kqo=r(Nz,"to load the model weights."),Nz.forEach(t),xqo=i(El),ode=n(El,"P",{});var hKr=s(ode);Rqo=r(hKr,"Examples:"),hKr.forEach(t),Sqo=i(El),m(gy.$$.fragment,El),El.forEach(t),Pqo=i(Ml),Ue=n(Ml,"DIV",{class:!0});var ea=s(Ue);m(hy.$$.fragment,ea),$qo=i(ea),rde=n(ea,"P",{});var pKr=s(rde);Iqo=r(pKr,"Instantiate one of the model classes of the library (with a object detection head) from a pretrained model."),pKr.forEach(t),jqo=i(ea),an=n(ea,"P",{});var L4=s(an);Nqo=r(L4,"The model class to instantiate is selected based on the "),tde=n(L4,"CODE",{});var _Kr=s(tde);Dqo=r(_Kr,"model_type"),_Kr.forEach(t),qqo=r(L4,` property of the config object (either
passed as an argument or loaded from `),ade=n(L4,"CODE",{});var uKr=s(ade);Gqo=r(uKr,"pretrained_model_name_or_path"),uKr.forEach(t),Oqo=r(L4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),nde=n(L4,"CODE",{});var bKr=s(nde);Xqo=r(bKr,"pretrained_model_name_or_path"),bKr.forEach(t),zqo=r(L4,":"),L4.forEach(t),Vqo=i(ea),sde=n(ea,"UL",{});var vKr=s(sde);_v=n(vKr,"LI",{});var H3e=s(_v);lde=n(H3e,"STRONG",{});var TKr=s(lde);Wqo=r(TKr,"detr"),TKr.forEach(t),Qqo=r(H3e," \u2014 "),xN=n(H3e,"A",{href:!0});var FKr=s(xN);Hqo=r(FKr,"DetrForObjectDetection"),FKr.forEach(t),Uqo=r(H3e," (DETR model)"),H3e.forEach(t),vKr.forEach(t),Jqo=i(ea),uv=n(ea,"P",{});var U3e=s(uv);Yqo=r(U3e,"The model is set in evaluation mode by default using "),ide=n(U3e,"CODE",{});var CKr=s(ide);Kqo=r(CKr,"model.eval()"),CKr.forEach(t),Zqo=r(U3e,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),dde=n(U3e,"CODE",{});var MKr=s(dde);eGo=r(MKr,"model.train()"),MKr.forEach(t),U3e.forEach(t),oGo=i(ea),cde=n(ea,"P",{});var EKr=s(cde);rGo=r(EKr,"Examples:"),EKr.forEach(t),tGo=i(ea),m(py.$$.fragment,ea),ea.forEach(t),Ml.forEach(t),c9e=i(d),Ud=n(d,"H2",{class:!0});var bke=s(Ud);bv=n(bke,"A",{id:!0,class:!0,href:!0});var yKr=s(bv);fde=n(yKr,"SPAN",{});var wKr=s(fde);m(_y.$$.fragment,wKr),wKr.forEach(t),yKr.forEach(t),aGo=i(bke),mde=n(bke,"SPAN",{});var AKr=s(mde);nGo=r(AKr,"AutoModelForImageSegmentation"),AKr.forEach(t),bke.forEach(t),f9e=i(d),fr=n(d,"DIV",{class:!0});var yl=s(fr);m(uy.$$.fragment,yl),sGo=i(yl),Jd=n(yl,"P",{});var Dz=s(Jd);lGo=r(Dz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a image segmentation head) when created
with the `),gde=n(Dz,"CODE",{});var LKr=s(gde);iGo=r(LKr,"from_pretrained()"),LKr.forEach(t),dGo=r(Dz,"class method or the "),hde=n(Dz,"CODE",{});var BKr=s(hde);cGo=r(BKr,"from_config()"),BKr.forEach(t),fGo=r(Dz,`class
method.`),Dz.forEach(t),mGo=i(yl),by=n(yl,"P",{});var vke=s(by);gGo=r(vke,"This class cannot be instantiated directly using "),pde=n(vke,"CODE",{});var kKr=s(pde);hGo=r(kKr,"__init__()"),kKr.forEach(t),pGo=r(vke," (throws an error)."),vke.forEach(t),_Go=i(yl),at=n(yl,"DIV",{class:!0});var wl=s(at);m(vy.$$.fragment,wl),uGo=i(wl),_de=n(wl,"P",{});var xKr=s(_de);bGo=r(xKr,"Instantiates one of the model classes of the library (with a image segmentation head) from a configuration."),xKr.forEach(t),vGo=i(wl),Yd=n(wl,"P",{});var qz=s(Yd);TGo=r(qz,`Note:
Loading a model from its configuration file does `),ude=n(qz,"STRONG",{});var RKr=s(ude);FGo=r(RKr,"not"),RKr.forEach(t),CGo=r(qz,` load the model weights. It only affects the
model\u2019s configuration. Use `),bde=n(qz,"CODE",{});var SKr=s(bde);MGo=r(SKr,"from_pretrained()"),SKr.forEach(t),EGo=r(qz,"to load the model weights."),qz.forEach(t),yGo=i(wl),vde=n(wl,"P",{});var PKr=s(vde);wGo=r(PKr,"Examples:"),PKr.forEach(t),AGo=i(wl),m(Ty.$$.fragment,wl),wl.forEach(t),LGo=i(yl),Je=n(yl,"DIV",{class:!0});var oa=s(Je);m(Fy.$$.fragment,oa),BGo=i(oa),Tde=n(oa,"P",{});var $Kr=s(Tde);kGo=r($Kr,"Instantiate one of the model classes of the library (with a image segmentation head) from a pretrained model."),$Kr.forEach(t),xGo=i(oa),nn=n(oa,"P",{});var B4=s(nn);RGo=r(B4,"The model class to instantiate is selected based on the "),Fde=n(B4,"CODE",{});var IKr=s(Fde);SGo=r(IKr,"model_type"),IKr.forEach(t),PGo=r(B4,` property of the config object (either
passed as an argument or loaded from `),Cde=n(B4,"CODE",{});var jKr=s(Cde);$Go=r(jKr,"pretrained_model_name_or_path"),jKr.forEach(t),IGo=r(B4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Mde=n(B4,"CODE",{});var NKr=s(Mde);jGo=r(NKr,"pretrained_model_name_or_path"),NKr.forEach(t),NGo=r(B4,":"),B4.forEach(t),DGo=i(oa),Ede=n(oa,"UL",{});var DKr=s(Ede);vv=n(DKr,"LI",{});var J3e=s(vv);yde=n(J3e,"STRONG",{});var qKr=s(yde);qGo=r(qKr,"detr"),qKr.forEach(t),GGo=r(J3e," \u2014 "),RN=n(J3e,"A",{href:!0});var GKr=s(RN);OGo=r(GKr,"DetrForSegmentation"),GKr.forEach(t),XGo=r(J3e," (DETR model)"),J3e.forEach(t),DKr.forEach(t),zGo=i(oa),Tv=n(oa,"P",{});var Y3e=s(Tv);VGo=r(Y3e,"The model is set in evaluation mode by default using "),wde=n(Y3e,"CODE",{});var OKr=s(wde);WGo=r(OKr,"model.eval()"),OKr.forEach(t),QGo=r(Y3e,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Ade=n(Y3e,"CODE",{});var XKr=s(Ade);HGo=r(XKr,"model.train()"),XKr.forEach(t),Y3e.forEach(t),UGo=i(oa),Lde=n(oa,"P",{});var zKr=s(Lde);JGo=r(zKr,"Examples:"),zKr.forEach(t),YGo=i(oa),m(Cy.$$.fragment,oa),oa.forEach(t),yl.forEach(t),m9e=i(d),Kd=n(d,"H2",{class:!0});var Tke=s(Kd);Fv=n(Tke,"A",{id:!0,class:!0,href:!0});var VKr=s(Fv);Bde=n(VKr,"SPAN",{});var WKr=s(Bde);m(My.$$.fragment,WKr),WKr.forEach(t),VKr.forEach(t),KGo=i(Tke),kde=n(Tke,"SPAN",{});var QKr=s(kde);ZGo=r(QKr,"AutoModelForSemanticSegmentation"),QKr.forEach(t),Tke.forEach(t),g9e=i(d),mr=n(d,"DIV",{class:!0});var Al=s(mr);m(Ey.$$.fragment,Al),eOo=i(Al),Zd=n(Al,"P",{});var Gz=s(Zd);oOo=r(Gz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a semantic segmentation head) when created
with the `),xde=n(Gz,"CODE",{});var HKr=s(xde);rOo=r(HKr,"from_pretrained()"),HKr.forEach(t),tOo=r(Gz,"class method or the "),Rde=n(Gz,"CODE",{});var UKr=s(Rde);aOo=r(UKr,"from_config()"),UKr.forEach(t),nOo=r(Gz,`class
method.`),Gz.forEach(t),sOo=i(Al),yy=n(Al,"P",{});var Fke=s(yy);lOo=r(Fke,"This class cannot be instantiated directly using "),Sde=n(Fke,"CODE",{});var JKr=s(Sde);iOo=r(JKr,"__init__()"),JKr.forEach(t),dOo=r(Fke," (throws an error)."),Fke.forEach(t),cOo=i(Al),nt=n(Al,"DIV",{class:!0});var Ll=s(nt);m(wy.$$.fragment,Ll),fOo=i(Ll),Pde=n(Ll,"P",{});var YKr=s(Pde);mOo=r(YKr,"Instantiates one of the model classes of the library (with a semantic segmentation head) from a configuration."),YKr.forEach(t),gOo=i(Ll),ec=n(Ll,"P",{});var Oz=s(ec);hOo=r(Oz,`Note:
Loading a model from its configuration file does `),$de=n(Oz,"STRONG",{});var KKr=s($de);pOo=r(KKr,"not"),KKr.forEach(t),_Oo=r(Oz,` load the model weights. It only affects the
model\u2019s configuration. Use `),Ide=n(Oz,"CODE",{});var ZKr=s(Ide);uOo=r(ZKr,"from_pretrained()"),ZKr.forEach(t),bOo=r(Oz,"to load the model weights."),Oz.forEach(t),vOo=i(Ll),jde=n(Ll,"P",{});var eZr=s(jde);TOo=r(eZr,"Examples:"),eZr.forEach(t),FOo=i(Ll),m(Ay.$$.fragment,Ll),Ll.forEach(t),COo=i(Al),Ye=n(Al,"DIV",{class:!0});var ra=s(Ye);m(Ly.$$.fragment,ra),MOo=i(ra),Nde=n(ra,"P",{});var oZr=s(Nde);EOo=r(oZr,"Instantiate one of the model classes of the library (with a semantic segmentation head) from a pretrained model."),oZr.forEach(t),yOo=i(ra),sn=n(ra,"P",{});var k4=s(sn);wOo=r(k4,"The model class to instantiate is selected based on the "),Dde=n(k4,"CODE",{});var rZr=s(Dde);AOo=r(rZr,"model_type"),rZr.forEach(t),LOo=r(k4,` property of the config object (either
passed as an argument or loaded from `),qde=n(k4,"CODE",{});var tZr=s(qde);BOo=r(tZr,"pretrained_model_name_or_path"),tZr.forEach(t),kOo=r(k4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Gde=n(k4,"CODE",{});var aZr=s(Gde);xOo=r(aZr,"pretrained_model_name_or_path"),aZr.forEach(t),ROo=r(k4,":"),k4.forEach(t),SOo=i(ra),By=n(ra,"UL",{});var Cke=s(By);Cv=n(Cke,"LI",{});var K3e=s(Cv);Ode=n(K3e,"STRONG",{});var nZr=s(Ode);POo=r(nZr,"beit"),nZr.forEach(t),$Oo=r(K3e," \u2014 "),SN=n(K3e,"A",{href:!0});var sZr=s(SN);IOo=r(sZr,"BeitForSemanticSegmentation"),sZr.forEach(t),jOo=r(K3e," (BEiT model)"),K3e.forEach(t),NOo=i(Cke),Mv=n(Cke,"LI",{});var Z3e=s(Mv);Xde=n(Z3e,"STRONG",{});var lZr=s(Xde);DOo=r(lZr,"segformer"),lZr.forEach(t),qOo=r(Z3e," \u2014 "),PN=n(Z3e,"A",{href:!0});var iZr=s(PN);GOo=r(iZr,"SegformerForSemanticSegmentation"),iZr.forEach(t),OOo=r(Z3e," (SegFormer model)"),Z3e.forEach(t),Cke.forEach(t),XOo=i(ra),Ev=n(ra,"P",{});var eye=s(Ev);zOo=r(eye,"The model is set in evaluation mode by default using "),zde=n(eye,"CODE",{});var dZr=s(zde);VOo=r(dZr,"model.eval()"),dZr.forEach(t),WOo=r(eye,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Vde=n(eye,"CODE",{});var cZr=s(Vde);QOo=r(cZr,"model.train()"),cZr.forEach(t),eye.forEach(t),HOo=i(ra),Wde=n(ra,"P",{});var fZr=s(Wde);UOo=r(fZr,"Examples:"),fZr.forEach(t),JOo=i(ra),m(ky.$$.fragment,ra),ra.forEach(t),Al.forEach(t),h9e=i(d),oc=n(d,"H2",{class:!0});var Mke=s(oc);yv=n(Mke,"A",{id:!0,class:!0,href:!0});var mZr=s(yv);Qde=n(mZr,"SPAN",{});var gZr=s(Qde);m(xy.$$.fragment,gZr),gZr.forEach(t),mZr.forEach(t),YOo=i(Mke),Hde=n(Mke,"SPAN",{});var hZr=s(Hde);KOo=r(hZr,"TFAutoModel"),hZr.forEach(t),Mke.forEach(t),p9e=i(d),gr=n(d,"DIV",{class:!0});var Bl=s(gr);m(Ry.$$.fragment,Bl),ZOo=i(Bl),rc=n(Bl,"P",{});var Xz=s(rc);eXo=r(Xz,`This is a generic model class that will be instantiated as one of the base model classes of the library when created
with the `),Ude=n(Xz,"CODE",{});var pZr=s(Ude);oXo=r(pZr,"from_pretrained()"),pZr.forEach(t),rXo=r(Xz,"class method or the "),Jde=n(Xz,"CODE",{});var _Zr=s(Jde);tXo=r(_Zr,"from_config()"),_Zr.forEach(t),aXo=r(Xz,`class
method.`),Xz.forEach(t),nXo=i(Bl),Sy=n(Bl,"P",{});var Eke=s(Sy);sXo=r(Eke,"This class cannot be instantiated directly using "),Yde=n(Eke,"CODE",{});var uZr=s(Yde);lXo=r(uZr,"__init__()"),uZr.forEach(t),iXo=r(Eke," (throws an error)."),Eke.forEach(t),dXo=i(Bl),st=n(Bl,"DIV",{class:!0});var kl=s(st);m(Py.$$.fragment,kl),cXo=i(kl),Kde=n(kl,"P",{});var bZr=s(Kde);fXo=r(bZr,"Instantiates one of the base model classes of the library from a configuration."),bZr.forEach(t),mXo=i(kl),tc=n(kl,"P",{});var zz=s(tc);gXo=r(zz,`Note:
Loading a model from its configuration file does `),Zde=n(zz,"STRONG",{});var vZr=s(Zde);hXo=r(vZr,"not"),vZr.forEach(t),pXo=r(zz,` load the model weights. It only affects the
model\u2019s configuration. Use `),ece=n(zz,"CODE",{});var TZr=s(ece);_Xo=r(TZr,"from_pretrained()"),TZr.forEach(t),uXo=r(zz,"to load the model weights."),zz.forEach(t),bXo=i(kl),oce=n(kl,"P",{});var FZr=s(oce);vXo=r(FZr,"Examples:"),FZr.forEach(t),TXo=i(kl),m($y.$$.fragment,kl),kl.forEach(t),FXo=i(Bl),go=n(Bl,"DIV",{class:!0});var ca=s(go);m(Iy.$$.fragment,ca),CXo=i(ca),rce=n(ca,"P",{});var CZr=s(rce);MXo=r(CZr,"Instantiate one of the base model classes of the library from a pretrained model."),CZr.forEach(t),EXo=i(ca),ln=n(ca,"P",{});var x4=s(ln);yXo=r(x4,"The model class to instantiate is selected based on the "),tce=n(x4,"CODE",{});var MZr=s(tce);wXo=r(MZr,"model_type"),MZr.forEach(t),AXo=r(x4,` property of the config object (either
passed as an argument or loaded from `),ace=n(x4,"CODE",{});var EZr=s(ace);LXo=r(EZr,"pretrained_model_name_or_path"),EZr.forEach(t),BXo=r(x4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),nce=n(x4,"CODE",{});var yZr=s(nce);kXo=r(yZr,"pretrained_model_name_or_path"),yZr.forEach(t),xXo=r(x4,":"),x4.forEach(t),RXo=i(ca),B=n(ca,"UL",{});var k=s(B);wv=n(k,"LI",{});var oye=s(wv);sce=n(oye,"STRONG",{});var wZr=s(sce);SXo=r(wZr,"albert"),wZr.forEach(t),PXo=r(oye," \u2014 "),$N=n(oye,"A",{href:!0});var AZr=s($N);$Xo=r(AZr,"TFAlbertModel"),AZr.forEach(t),IXo=r(oye," (ALBERT model)"),oye.forEach(t),jXo=i(k),Av=n(k,"LI",{});var rye=s(Av);lce=n(rye,"STRONG",{});var LZr=s(lce);NXo=r(LZr,"bart"),LZr.forEach(t),DXo=r(rye," \u2014 "),IN=n(rye,"A",{href:!0});var BZr=s(IN);qXo=r(BZr,"TFBartModel"),BZr.forEach(t),GXo=r(rye," (BART model)"),rye.forEach(t),OXo=i(k),Lv=n(k,"LI",{});var tye=s(Lv);ice=n(tye,"STRONG",{});var kZr=s(ice);XXo=r(kZr,"bert"),kZr.forEach(t),zXo=r(tye," \u2014 "),jN=n(tye,"A",{href:!0});var xZr=s(jN);VXo=r(xZr,"TFBertModel"),xZr.forEach(t),WXo=r(tye," (BERT model)"),tye.forEach(t),QXo=i(k),Bv=n(k,"LI",{});var aye=s(Bv);dce=n(aye,"STRONG",{});var RZr=s(dce);HXo=r(RZr,"blenderbot"),RZr.forEach(t),UXo=r(aye," \u2014 "),NN=n(aye,"A",{href:!0});var SZr=s(NN);JXo=r(SZr,"TFBlenderbotModel"),SZr.forEach(t),YXo=r(aye," (Blenderbot model)"),aye.forEach(t),KXo=i(k),kv=n(k,"LI",{});var nye=s(kv);cce=n(nye,"STRONG",{});var PZr=s(cce);ZXo=r(PZr,"blenderbot-small"),PZr.forEach(t),ezo=r(nye," \u2014 "),DN=n(nye,"A",{href:!0});var $Zr=s(DN);ozo=r($Zr,"TFBlenderbotSmallModel"),$Zr.forEach(t),rzo=r(nye," (BlenderbotSmall model)"),nye.forEach(t),tzo=i(k),xv=n(k,"LI",{});var sye=s(xv);fce=n(sye,"STRONG",{});var IZr=s(fce);azo=r(IZr,"camembert"),IZr.forEach(t),nzo=r(sye," \u2014 "),qN=n(sye,"A",{href:!0});var jZr=s(qN);szo=r(jZr,"TFCamembertModel"),jZr.forEach(t),lzo=r(sye," (CamemBERT model)"),sye.forEach(t),izo=i(k),Rv=n(k,"LI",{});var lye=s(Rv);mce=n(lye,"STRONG",{});var NZr=s(mce);dzo=r(NZr,"clip"),NZr.forEach(t),czo=r(lye," \u2014 "),GN=n(lye,"A",{href:!0});var DZr=s(GN);fzo=r(DZr,"TFCLIPModel"),DZr.forEach(t),mzo=r(lye," (CLIP model)"),lye.forEach(t),gzo=i(k),Sv=n(k,"LI",{});var iye=s(Sv);gce=n(iye,"STRONG",{});var qZr=s(gce);hzo=r(qZr,"convbert"),qZr.forEach(t),pzo=r(iye," \u2014 "),ON=n(iye,"A",{href:!0});var GZr=s(ON);_zo=r(GZr,"TFConvBertModel"),GZr.forEach(t),uzo=r(iye," (ConvBERT model)"),iye.forEach(t),bzo=i(k),Pv=n(k,"LI",{});var dye=s(Pv);hce=n(dye,"STRONG",{});var OZr=s(hce);vzo=r(OZr,"ctrl"),OZr.forEach(t),Tzo=r(dye," \u2014 "),XN=n(dye,"A",{href:!0});var XZr=s(XN);Fzo=r(XZr,"TFCTRLModel"),XZr.forEach(t),Czo=r(dye," (CTRL model)"),dye.forEach(t),Mzo=i(k),$v=n(k,"LI",{});var cye=s($v);pce=n(cye,"STRONG",{});var zZr=s(pce);Ezo=r(zZr,"deberta"),zZr.forEach(t),yzo=r(cye," \u2014 "),zN=n(cye,"A",{href:!0});var VZr=s(zN);wzo=r(VZr,"TFDebertaModel"),VZr.forEach(t),Azo=r(cye," (DeBERTa model)"),cye.forEach(t),Lzo=i(k),Iv=n(k,"LI",{});var fye=s(Iv);_ce=n(fye,"STRONG",{});var WZr=s(_ce);Bzo=r(WZr,"deberta-v2"),WZr.forEach(t),kzo=r(fye," \u2014 "),VN=n(fye,"A",{href:!0});var QZr=s(VN);xzo=r(QZr,"TFDebertaV2Model"),QZr.forEach(t),Rzo=r(fye," (DeBERTa-v2 model)"),fye.forEach(t),Szo=i(k),jv=n(k,"LI",{});var mye=s(jv);uce=n(mye,"STRONG",{});var HZr=s(uce);Pzo=r(HZr,"distilbert"),HZr.forEach(t),$zo=r(mye," \u2014 "),WN=n(mye,"A",{href:!0});var UZr=s(WN);Izo=r(UZr,"TFDistilBertModel"),UZr.forEach(t),jzo=r(mye," (DistilBERT model)"),mye.forEach(t),Nzo=i(k),Nv=n(k,"LI",{});var gye=s(Nv);bce=n(gye,"STRONG",{});var JZr=s(bce);Dzo=r(JZr,"dpr"),JZr.forEach(t),qzo=r(gye," \u2014 "),QN=n(gye,"A",{href:!0});var YZr=s(QN);Gzo=r(YZr,"TFDPRQuestionEncoder"),YZr.forEach(t),Ozo=r(gye," (DPR model)"),gye.forEach(t),Xzo=i(k),Dv=n(k,"LI",{});var hye=s(Dv);vce=n(hye,"STRONG",{});var KZr=s(vce);zzo=r(KZr,"electra"),KZr.forEach(t),Vzo=r(hye," \u2014 "),HN=n(hye,"A",{href:!0});var ZZr=s(HN);Wzo=r(ZZr,"TFElectraModel"),ZZr.forEach(t),Qzo=r(hye," (ELECTRA model)"),hye.forEach(t),Hzo=i(k),qv=n(k,"LI",{});var pye=s(qv);Tce=n(pye,"STRONG",{});var eet=s(Tce);Uzo=r(eet,"flaubert"),eet.forEach(t),Jzo=r(pye," \u2014 "),UN=n(pye,"A",{href:!0});var oet=s(UN);Yzo=r(oet,"TFFlaubertModel"),oet.forEach(t),Kzo=r(pye," (FlauBERT model)"),pye.forEach(t),Zzo=i(k),Ss=n(k,"LI",{});var qL=s(Ss);Fce=n(qL,"STRONG",{});var ret=s(Fce);eVo=r(ret,"funnel"),ret.forEach(t),oVo=r(qL," \u2014 "),JN=n(qL,"A",{href:!0});var tet=s(JN);rVo=r(tet,"TFFunnelModel"),tet.forEach(t),tVo=r(qL," or "),YN=n(qL,"A",{href:!0});var aet=s(YN);aVo=r(aet,"TFFunnelBaseModel"),aet.forEach(t),nVo=r(qL," (Funnel Transformer model)"),qL.forEach(t),sVo=i(k),Gv=n(k,"LI",{});var _ye=s(Gv);Cce=n(_ye,"STRONG",{});var net=s(Cce);lVo=r(net,"gpt2"),net.forEach(t),iVo=r(_ye," \u2014 "),KN=n(_ye,"A",{href:!0});var set=s(KN);dVo=r(set,"TFGPT2Model"),set.forEach(t),cVo=r(_ye," (OpenAI GPT-2 model)"),_ye.forEach(t),fVo=i(k),Ov=n(k,"LI",{});var uye=s(Ov);Mce=n(uye,"STRONG",{});var iet=s(Mce);mVo=r(iet,"hubert"),iet.forEach(t),gVo=r(uye," \u2014 "),ZN=n(uye,"A",{href:!0});var det=s(ZN);hVo=r(det,"TFHubertModel"),det.forEach(t),pVo=r(uye," (Hubert model)"),uye.forEach(t),_Vo=i(k),Xv=n(k,"LI",{});var bye=s(Xv);Ece=n(bye,"STRONG",{});var cet=s(Ece);uVo=r(cet,"layoutlm"),cet.forEach(t),bVo=r(bye," \u2014 "),eD=n(bye,"A",{href:!0});var fet=s(eD);vVo=r(fet,"TFLayoutLMModel"),fet.forEach(t),TVo=r(bye," (LayoutLM model)"),bye.forEach(t),FVo=i(k),zv=n(k,"LI",{});var vye=s(zv);yce=n(vye,"STRONG",{});var met=s(yce);CVo=r(met,"led"),met.forEach(t),MVo=r(vye," \u2014 "),oD=n(vye,"A",{href:!0});var get=s(oD);EVo=r(get,"TFLEDModel"),get.forEach(t),yVo=r(vye," (LED model)"),vye.forEach(t),wVo=i(k),Vv=n(k,"LI",{});var Tye=s(Vv);wce=n(Tye,"STRONG",{});var het=s(wce);AVo=r(het,"longformer"),het.forEach(t),LVo=r(Tye," \u2014 "),rD=n(Tye,"A",{href:!0});var pet=s(rD);BVo=r(pet,"TFLongformerModel"),pet.forEach(t),kVo=r(Tye," (Longformer model)"),Tye.forEach(t),xVo=i(k),Wv=n(k,"LI",{});var Fye=s(Wv);Ace=n(Fye,"STRONG",{});var _et=s(Ace);RVo=r(_et,"lxmert"),_et.forEach(t),SVo=r(Fye," \u2014 "),tD=n(Fye,"A",{href:!0});var uet=s(tD);PVo=r(uet,"TFLxmertModel"),uet.forEach(t),$Vo=r(Fye," (LXMERT model)"),Fye.forEach(t),IVo=i(k),Qv=n(k,"LI",{});var Cye=s(Qv);Lce=n(Cye,"STRONG",{});var bet=s(Lce);jVo=r(bet,"marian"),bet.forEach(t),NVo=r(Cye," \u2014 "),aD=n(Cye,"A",{href:!0});var vet=s(aD);DVo=r(vet,"TFMarianModel"),vet.forEach(t),qVo=r(Cye," (Marian model)"),Cye.forEach(t),GVo=i(k),Hv=n(k,"LI",{});var Mye=s(Hv);Bce=n(Mye,"STRONG",{});var Tet=s(Bce);OVo=r(Tet,"mbart"),Tet.forEach(t),XVo=r(Mye," \u2014 "),nD=n(Mye,"A",{href:!0});var Fet=s(nD);zVo=r(Fet,"TFMBartModel"),Fet.forEach(t),VVo=r(Mye," (mBART model)"),Mye.forEach(t),WVo=i(k),Uv=n(k,"LI",{});var Eye=s(Uv);kce=n(Eye,"STRONG",{});var Cet=s(kce);QVo=r(Cet,"mobilebert"),Cet.forEach(t),HVo=r(Eye," \u2014 "),sD=n(Eye,"A",{href:!0});var Met=s(sD);UVo=r(Met,"TFMobileBertModel"),Met.forEach(t),JVo=r(Eye," (MobileBERT model)"),Eye.forEach(t),YVo=i(k),Jv=n(k,"LI",{});var yye=s(Jv);xce=n(yye,"STRONG",{});var Eet=s(xce);KVo=r(Eet,"mpnet"),Eet.forEach(t),ZVo=r(yye," \u2014 "),lD=n(yye,"A",{href:!0});var yet=s(lD);eWo=r(yet,"TFMPNetModel"),yet.forEach(t),oWo=r(yye," (MPNet model)"),yye.forEach(t),rWo=i(k),Yv=n(k,"LI",{});var wye=s(Yv);Rce=n(wye,"STRONG",{});var wet=s(Rce);tWo=r(wet,"mt5"),wet.forEach(t),aWo=r(wye," \u2014 "),iD=n(wye,"A",{href:!0});var Aet=s(iD);nWo=r(Aet,"TFMT5Model"),Aet.forEach(t),sWo=r(wye," (mT5 model)"),wye.forEach(t),lWo=i(k),Kv=n(k,"LI",{});var Aye=s(Kv);Sce=n(Aye,"STRONG",{});var Let=s(Sce);iWo=r(Let,"openai-gpt"),Let.forEach(t),dWo=r(Aye," \u2014 "),dD=n(Aye,"A",{href:!0});var Bet=s(dD);cWo=r(Bet,"TFOpenAIGPTModel"),Bet.forEach(t),fWo=r(Aye," (OpenAI GPT model)"),Aye.forEach(t),mWo=i(k),Zv=n(k,"LI",{});var Lye=s(Zv);Pce=n(Lye,"STRONG",{});var ket=s(Pce);gWo=r(ket,"pegasus"),ket.forEach(t),hWo=r(Lye," \u2014 "),cD=n(Lye,"A",{href:!0});var xet=s(cD);pWo=r(xet,"TFPegasusModel"),xet.forEach(t),_Wo=r(Lye," (Pegasus model)"),Lye.forEach(t),uWo=i(k),e6=n(k,"LI",{});var Bye=s(e6);$ce=n(Bye,"STRONG",{});var Ret=s($ce);bWo=r(Ret,"rembert"),Ret.forEach(t),vWo=r(Bye," \u2014 "),fD=n(Bye,"A",{href:!0});var Set=s(fD);TWo=r(Set,"TFRemBertModel"),Set.forEach(t),FWo=r(Bye," (RemBERT model)"),Bye.forEach(t),CWo=i(k),o6=n(k,"LI",{});var kye=s(o6);Ice=n(kye,"STRONG",{});var Pet=s(Ice);MWo=r(Pet,"roberta"),Pet.forEach(t),EWo=r(kye," \u2014 "),mD=n(kye,"A",{href:!0});var $et=s(mD);yWo=r($et,"TFRobertaModel"),$et.forEach(t),wWo=r(kye," (RoBERTa model)"),kye.forEach(t),AWo=i(k),r6=n(k,"LI",{});var xye=s(r6);jce=n(xye,"STRONG",{});var Iet=s(jce);LWo=r(Iet,"roformer"),Iet.forEach(t),BWo=r(xye," \u2014 "),gD=n(xye,"A",{href:!0});var jet=s(gD);kWo=r(jet,"TFRoFormerModel"),jet.forEach(t),xWo=r(xye," (RoFormer model)"),xye.forEach(t),RWo=i(k),t6=n(k,"LI",{});var Rye=s(t6);Nce=n(Rye,"STRONG",{});var Net=s(Nce);SWo=r(Net,"speech_to_text"),Net.forEach(t),PWo=r(Rye," \u2014 "),hD=n(Rye,"A",{href:!0});var Det=s(hD);$Wo=r(Det,"TFSpeech2TextModel"),Det.forEach(t),IWo=r(Rye," (Speech2Text model)"),Rye.forEach(t),jWo=i(k),a6=n(k,"LI",{});var Sye=s(a6);Dce=n(Sye,"STRONG",{});var qet=s(Dce);NWo=r(qet,"t5"),qet.forEach(t),DWo=r(Sye," \u2014 "),pD=n(Sye,"A",{href:!0});var Get=s(pD);qWo=r(Get,"TFT5Model"),Get.forEach(t),GWo=r(Sye," (T5 model)"),Sye.forEach(t),OWo=i(k),n6=n(k,"LI",{});var Pye=s(n6);qce=n(Pye,"STRONG",{});var Oet=s(qce);XWo=r(Oet,"tapas"),Oet.forEach(t),zWo=r(Pye," \u2014 "),_D=n(Pye,"A",{href:!0});var Xet=s(_D);VWo=r(Xet,"TFTapasModel"),Xet.forEach(t),WWo=r(Pye," (TAPAS model)"),Pye.forEach(t),QWo=i(k),s6=n(k,"LI",{});var $ye=s(s6);Gce=n($ye,"STRONG",{});var zet=s(Gce);HWo=r(zet,"transfo-xl"),zet.forEach(t),UWo=r($ye," \u2014 "),uD=n($ye,"A",{href:!0});var Vet=s(uD);JWo=r(Vet,"TFTransfoXLModel"),Vet.forEach(t),YWo=r($ye," (Transformer-XL model)"),$ye.forEach(t),KWo=i(k),l6=n(k,"LI",{});var Iye=s(l6);Oce=n(Iye,"STRONG",{});var Wet=s(Oce);ZWo=r(Wet,"vit"),Wet.forEach(t),eQo=r(Iye," \u2014 "),bD=n(Iye,"A",{href:!0});var Qet=s(bD);oQo=r(Qet,"TFViTModel"),Qet.forEach(t),rQo=r(Iye," (ViT model)"),Iye.forEach(t),tQo=i(k),i6=n(k,"LI",{});var jye=s(i6);Xce=n(jye,"STRONG",{});var Het=s(Xce);aQo=r(Het,"wav2vec2"),Het.forEach(t),nQo=r(jye," \u2014 "),vD=n(jye,"A",{href:!0});var Uet=s(vD);sQo=r(Uet,"TFWav2Vec2Model"),Uet.forEach(t),lQo=r(jye," (Wav2Vec2 model)"),jye.forEach(t),iQo=i(k),d6=n(k,"LI",{});var Nye=s(d6);zce=n(Nye,"STRONG",{});var Jet=s(zce);dQo=r(Jet,"xlm"),Jet.forEach(t),cQo=r(Nye," \u2014 "),TD=n(Nye,"A",{href:!0});var Yet=s(TD);fQo=r(Yet,"TFXLMModel"),Yet.forEach(t),mQo=r(Nye," (XLM model)"),Nye.forEach(t),gQo=i(k),c6=n(k,"LI",{});var Dye=s(c6);Vce=n(Dye,"STRONG",{});var Ket=s(Vce);hQo=r(Ket,"xlm-roberta"),Ket.forEach(t),pQo=r(Dye," \u2014 "),FD=n(Dye,"A",{href:!0});var Zet=s(FD);_Qo=r(Zet,"TFXLMRobertaModel"),Zet.forEach(t),uQo=r(Dye," (XLM-RoBERTa model)"),Dye.forEach(t),bQo=i(k),f6=n(k,"LI",{});var qye=s(f6);Wce=n(qye,"STRONG",{});var eot=s(Wce);vQo=r(eot,"xlnet"),eot.forEach(t),TQo=r(qye," \u2014 "),CD=n(qye,"A",{href:!0});var oot=s(CD);FQo=r(oot,"TFXLNetModel"),oot.forEach(t),CQo=r(qye," (XLNet model)"),qye.forEach(t),k.forEach(t),MQo=i(ca),Qce=n(ca,"P",{});var rot=s(Qce);EQo=r(rot,"Examples:"),rot.forEach(t),yQo=i(ca),m(jy.$$.fragment,ca),ca.forEach(t),Bl.forEach(t),_9e=i(d),ac=n(d,"H2",{class:!0});var yke=s(ac);m6=n(yke,"A",{id:!0,class:!0,href:!0});var tot=s(m6);Hce=n(tot,"SPAN",{});var aot=s(Hce);m(Ny.$$.fragment,aot),aot.forEach(t),tot.forEach(t),wQo=i(yke),Uce=n(yke,"SPAN",{});var not=s(Uce);AQo=r(not,"TFAutoModelForPreTraining"),not.forEach(t),yke.forEach(t),u9e=i(d),hr=n(d,"DIV",{class:!0});var xl=s(hr);m(Dy.$$.fragment,xl),LQo=i(xl),nc=n(xl,"P",{});var Vz=s(nc);BQo=r(Vz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a pretraining head) when created
with the `),Jce=n(Vz,"CODE",{});var sot=s(Jce);kQo=r(sot,"from_pretrained()"),sot.forEach(t),xQo=r(Vz,"class method or the "),Yce=n(Vz,"CODE",{});var lot=s(Yce);RQo=r(lot,"from_config()"),lot.forEach(t),SQo=r(Vz,`class
method.`),Vz.forEach(t),PQo=i(xl),qy=n(xl,"P",{});var wke=s(qy);$Qo=r(wke,"This class cannot be instantiated directly using "),Kce=n(wke,"CODE",{});var iot=s(Kce);IQo=r(iot,"__init__()"),iot.forEach(t),jQo=r(wke," (throws an error)."),wke.forEach(t),NQo=i(xl),lt=n(xl,"DIV",{class:!0});var Rl=s(lt);m(Gy.$$.fragment,Rl),DQo=i(Rl),Zce=n(Rl,"P",{});var dot=s(Zce);qQo=r(dot,"Instantiates one of the model classes of the library (with a pretraining head) from a configuration."),dot.forEach(t),GQo=i(Rl),sc=n(Rl,"P",{});var Wz=s(sc);OQo=r(Wz,`Note:
Loading a model from its configuration file does `),efe=n(Wz,"STRONG",{});var cot=s(efe);XQo=r(cot,"not"),cot.forEach(t),zQo=r(Wz,` load the model weights. It only affects the
model\u2019s configuration. Use `),ofe=n(Wz,"CODE",{});var fot=s(ofe);VQo=r(fot,"from_pretrained()"),fot.forEach(t),WQo=r(Wz,"to load the model weights."),Wz.forEach(t),QQo=i(Rl),rfe=n(Rl,"P",{});var mot=s(rfe);HQo=r(mot,"Examples:"),mot.forEach(t),UQo=i(Rl),m(Oy.$$.fragment,Rl),Rl.forEach(t),JQo=i(xl),ho=n(xl,"DIV",{class:!0});var fa=s(ho);m(Xy.$$.fragment,fa),YQo=i(fa),tfe=n(fa,"P",{});var got=s(tfe);KQo=r(got,"Instantiate one of the model classes of the library (with a pretraining head) from a pretrained model."),got.forEach(t),ZQo=i(fa),dn=n(fa,"P",{});var R4=s(dn);eHo=r(R4,"The model class to instantiate is selected based on the "),afe=n(R4,"CODE",{});var hot=s(afe);oHo=r(hot,"model_type"),hot.forEach(t),rHo=r(R4,` property of the config object (either
passed as an argument or loaded from `),nfe=n(R4,"CODE",{});var pot=s(nfe);tHo=r(pot,"pretrained_model_name_or_path"),pot.forEach(t),aHo=r(R4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),sfe=n(R4,"CODE",{});var _ot=s(sfe);nHo=r(_ot,"pretrained_model_name_or_path"),_ot.forEach(t),sHo=r(R4,":"),R4.forEach(t),lHo=i(fa),H=n(fa,"UL",{});var U=s(H);g6=n(U,"LI",{});var Gye=s(g6);lfe=n(Gye,"STRONG",{});var uot=s(lfe);iHo=r(uot,"albert"),uot.forEach(t),dHo=r(Gye," \u2014 "),MD=n(Gye,"A",{href:!0});var bot=s(MD);cHo=r(bot,"TFAlbertForPreTraining"),bot.forEach(t),fHo=r(Gye," (ALBERT model)"),Gye.forEach(t),mHo=i(U),h6=n(U,"LI",{});var Oye=s(h6);ife=n(Oye,"STRONG",{});var vot=s(ife);gHo=r(vot,"bart"),vot.forEach(t),hHo=r(Oye," \u2014 "),ED=n(Oye,"A",{href:!0});var Tot=s(ED);pHo=r(Tot,"TFBartForConditionalGeneration"),Tot.forEach(t),_Ho=r(Oye," (BART model)"),Oye.forEach(t),uHo=i(U),p6=n(U,"LI",{});var Xye=s(p6);dfe=n(Xye,"STRONG",{});var Fot=s(dfe);bHo=r(Fot,"bert"),Fot.forEach(t),vHo=r(Xye," \u2014 "),yD=n(Xye,"A",{href:!0});var Cot=s(yD);THo=r(Cot,"TFBertForPreTraining"),Cot.forEach(t),FHo=r(Xye," (BERT model)"),Xye.forEach(t),CHo=i(U),_6=n(U,"LI",{});var zye=s(_6);cfe=n(zye,"STRONG",{});var Mot=s(cfe);MHo=r(Mot,"camembert"),Mot.forEach(t),EHo=r(zye," \u2014 "),wD=n(zye,"A",{href:!0});var Eot=s(wD);yHo=r(Eot,"TFCamembertForMaskedLM"),Eot.forEach(t),wHo=r(zye," (CamemBERT model)"),zye.forEach(t),AHo=i(U),u6=n(U,"LI",{});var Vye=s(u6);ffe=n(Vye,"STRONG",{});var yot=s(ffe);LHo=r(yot,"ctrl"),yot.forEach(t),BHo=r(Vye," \u2014 "),AD=n(Vye,"A",{href:!0});var wot=s(AD);kHo=r(wot,"TFCTRLLMHeadModel"),wot.forEach(t),xHo=r(Vye," (CTRL model)"),Vye.forEach(t),RHo=i(U),b6=n(U,"LI",{});var Wye=s(b6);mfe=n(Wye,"STRONG",{});var Aot=s(mfe);SHo=r(Aot,"distilbert"),Aot.forEach(t),PHo=r(Wye," \u2014 "),LD=n(Wye,"A",{href:!0});var Lot=s(LD);$Ho=r(Lot,"TFDistilBertForMaskedLM"),Lot.forEach(t),IHo=r(Wye," (DistilBERT model)"),Wye.forEach(t),jHo=i(U),v6=n(U,"LI",{});var Qye=s(v6);gfe=n(Qye,"STRONG",{});var Bot=s(gfe);NHo=r(Bot,"electra"),Bot.forEach(t),DHo=r(Qye," \u2014 "),BD=n(Qye,"A",{href:!0});var kot=s(BD);qHo=r(kot,"TFElectraForPreTraining"),kot.forEach(t),GHo=r(Qye," (ELECTRA model)"),Qye.forEach(t),OHo=i(U),T6=n(U,"LI",{});var Hye=s(T6);hfe=n(Hye,"STRONG",{});var xot=s(hfe);XHo=r(xot,"flaubert"),xot.forEach(t),zHo=r(Hye," \u2014 "),kD=n(Hye,"A",{href:!0});var Rot=s(kD);VHo=r(Rot,"TFFlaubertWithLMHeadModel"),Rot.forEach(t),WHo=r(Hye," (FlauBERT model)"),Hye.forEach(t),QHo=i(U),F6=n(U,"LI",{});var Uye=s(F6);pfe=n(Uye,"STRONG",{});var Sot=s(pfe);HHo=r(Sot,"funnel"),Sot.forEach(t),UHo=r(Uye," \u2014 "),xD=n(Uye,"A",{href:!0});var Pot=s(xD);JHo=r(Pot,"TFFunnelForPreTraining"),Pot.forEach(t),YHo=r(Uye," (Funnel Transformer model)"),Uye.forEach(t),KHo=i(U),C6=n(U,"LI",{});var Jye=s(C6);_fe=n(Jye,"STRONG",{});var $ot=s(_fe);ZHo=r($ot,"gpt2"),$ot.forEach(t),eUo=r(Jye," \u2014 "),RD=n(Jye,"A",{href:!0});var Iot=s(RD);oUo=r(Iot,"TFGPT2LMHeadModel"),Iot.forEach(t),rUo=r(Jye," (OpenAI GPT-2 model)"),Jye.forEach(t),tUo=i(U),M6=n(U,"LI",{});var Yye=s(M6);ufe=n(Yye,"STRONG",{});var jot=s(ufe);aUo=r(jot,"layoutlm"),jot.forEach(t),nUo=r(Yye," \u2014 "),SD=n(Yye,"A",{href:!0});var Not=s(SD);sUo=r(Not,"TFLayoutLMForMaskedLM"),Not.forEach(t),lUo=r(Yye," (LayoutLM model)"),Yye.forEach(t),iUo=i(U),E6=n(U,"LI",{});var Kye=s(E6);bfe=n(Kye,"STRONG",{});var Dot=s(bfe);dUo=r(Dot,"lxmert"),Dot.forEach(t),cUo=r(Kye," \u2014 "),PD=n(Kye,"A",{href:!0});var qot=s(PD);fUo=r(qot,"TFLxmertForPreTraining"),qot.forEach(t),mUo=r(Kye," (LXMERT model)"),Kye.forEach(t),gUo=i(U),y6=n(U,"LI",{});var Zye=s(y6);vfe=n(Zye,"STRONG",{});var Got=s(vfe);hUo=r(Got,"mobilebert"),Got.forEach(t),pUo=r(Zye," \u2014 "),$D=n(Zye,"A",{href:!0});var Oot=s($D);_Uo=r(Oot,"TFMobileBertForPreTraining"),Oot.forEach(t),uUo=r(Zye," (MobileBERT model)"),Zye.forEach(t),bUo=i(U),w6=n(U,"LI",{});var ewe=s(w6);Tfe=n(ewe,"STRONG",{});var Xot=s(Tfe);vUo=r(Xot,"mpnet"),Xot.forEach(t),TUo=r(ewe," \u2014 "),ID=n(ewe,"A",{href:!0});var zot=s(ID);FUo=r(zot,"TFMPNetForMaskedLM"),zot.forEach(t),CUo=r(ewe," (MPNet model)"),ewe.forEach(t),MUo=i(U),A6=n(U,"LI",{});var owe=s(A6);Ffe=n(owe,"STRONG",{});var Vot=s(Ffe);EUo=r(Vot,"openai-gpt"),Vot.forEach(t),yUo=r(owe," \u2014 "),jD=n(owe,"A",{href:!0});var Wot=s(jD);wUo=r(Wot,"TFOpenAIGPTLMHeadModel"),Wot.forEach(t),AUo=r(owe," (OpenAI GPT model)"),owe.forEach(t),LUo=i(U),L6=n(U,"LI",{});var rwe=s(L6);Cfe=n(rwe,"STRONG",{});var Qot=s(Cfe);BUo=r(Qot,"roberta"),Qot.forEach(t),kUo=r(rwe," \u2014 "),ND=n(rwe,"A",{href:!0});var Hot=s(ND);xUo=r(Hot,"TFRobertaForMaskedLM"),Hot.forEach(t),RUo=r(rwe," (RoBERTa model)"),rwe.forEach(t),SUo=i(U),B6=n(U,"LI",{});var twe=s(B6);Mfe=n(twe,"STRONG",{});var Uot=s(Mfe);PUo=r(Uot,"t5"),Uot.forEach(t),$Uo=r(twe," \u2014 "),DD=n(twe,"A",{href:!0});var Jot=s(DD);IUo=r(Jot,"TFT5ForConditionalGeneration"),Jot.forEach(t),jUo=r(twe," (T5 model)"),twe.forEach(t),NUo=i(U),k6=n(U,"LI",{});var awe=s(k6);Efe=n(awe,"STRONG",{});var Yot=s(Efe);DUo=r(Yot,"tapas"),Yot.forEach(t),qUo=r(awe," \u2014 "),qD=n(awe,"A",{href:!0});var Kot=s(qD);GUo=r(Kot,"TFTapasForMaskedLM"),Kot.forEach(t),OUo=r(awe," (TAPAS model)"),awe.forEach(t),XUo=i(U),x6=n(U,"LI",{});var nwe=s(x6);yfe=n(nwe,"STRONG",{});var Zot=s(yfe);zUo=r(Zot,"transfo-xl"),Zot.forEach(t),VUo=r(nwe," \u2014 "),GD=n(nwe,"A",{href:!0});var ert=s(GD);WUo=r(ert,"TFTransfoXLLMHeadModel"),ert.forEach(t),QUo=r(nwe," (Transformer-XL model)"),nwe.forEach(t),HUo=i(U),R6=n(U,"LI",{});var swe=s(R6);wfe=n(swe,"STRONG",{});var ort=s(wfe);UUo=r(ort,"xlm"),ort.forEach(t),JUo=r(swe," \u2014 "),OD=n(swe,"A",{href:!0});var rrt=s(OD);YUo=r(rrt,"TFXLMWithLMHeadModel"),rrt.forEach(t),KUo=r(swe," (XLM model)"),swe.forEach(t),ZUo=i(U),S6=n(U,"LI",{});var lwe=s(S6);Afe=n(lwe,"STRONG",{});var trt=s(Afe);eJo=r(trt,"xlm-roberta"),trt.forEach(t),oJo=r(lwe," \u2014 "),XD=n(lwe,"A",{href:!0});var art=s(XD);rJo=r(art,"TFXLMRobertaForMaskedLM"),art.forEach(t),tJo=r(lwe," (XLM-RoBERTa model)"),lwe.forEach(t),aJo=i(U),P6=n(U,"LI",{});var iwe=s(P6);Lfe=n(iwe,"STRONG",{});var nrt=s(Lfe);nJo=r(nrt,"xlnet"),nrt.forEach(t),sJo=r(iwe," \u2014 "),zD=n(iwe,"A",{href:!0});var srt=s(zD);lJo=r(srt,"TFXLNetLMHeadModel"),srt.forEach(t),iJo=r(iwe," (XLNet model)"),iwe.forEach(t),U.forEach(t),dJo=i(fa),Bfe=n(fa,"P",{});var lrt=s(Bfe);cJo=r(lrt,"Examples:"),lrt.forEach(t),fJo=i(fa),m(zy.$$.fragment,fa),fa.forEach(t),xl.forEach(t),b9e=i(d),lc=n(d,"H2",{class:!0});var Ake=s(lc);$6=n(Ake,"A",{id:!0,class:!0,href:!0});var irt=s($6);kfe=n(irt,"SPAN",{});var drt=s(kfe);m(Vy.$$.fragment,drt),drt.forEach(t),irt.forEach(t),mJo=i(Ake),xfe=n(Ake,"SPAN",{});var crt=s(xfe);gJo=r(crt,"TFAutoModelForCausalLM"),crt.forEach(t),Ake.forEach(t),v9e=i(d),pr=n(d,"DIV",{class:!0});var Sl=s(pr);m(Wy.$$.fragment,Sl),hJo=i(Sl),ic=n(Sl,"P",{});var Qz=s(ic);pJo=r(Qz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a causal language modeling head) when created
with the `),Rfe=n(Qz,"CODE",{});var frt=s(Rfe);_Jo=r(frt,"from_pretrained()"),frt.forEach(t),uJo=r(Qz,"class method or the "),Sfe=n(Qz,"CODE",{});var mrt=s(Sfe);bJo=r(mrt,"from_config()"),mrt.forEach(t),vJo=r(Qz,`class
method.`),Qz.forEach(t),TJo=i(Sl),Qy=n(Sl,"P",{});var Lke=s(Qy);FJo=r(Lke,"This class cannot be instantiated directly using "),Pfe=n(Lke,"CODE",{});var grt=s(Pfe);CJo=r(grt,"__init__()"),grt.forEach(t),MJo=r(Lke," (throws an error)."),Lke.forEach(t),EJo=i(Sl),it=n(Sl,"DIV",{class:!0});var Pl=s(it);m(Hy.$$.fragment,Pl),yJo=i(Pl),$fe=n(Pl,"P",{});var hrt=s($fe);wJo=r(hrt,"Instantiates one of the model classes of the library (with a causal language modeling head) from a configuration."),hrt.forEach(t),AJo=i(Pl),dc=n(Pl,"P",{});var Hz=s(dc);LJo=r(Hz,`Note:
Loading a model from its configuration file does `),Ife=n(Hz,"STRONG",{});var prt=s(Ife);BJo=r(prt,"not"),prt.forEach(t),kJo=r(Hz,` load the model weights. It only affects the
model\u2019s configuration. Use `),jfe=n(Hz,"CODE",{});var _rt=s(jfe);xJo=r(_rt,"from_pretrained()"),_rt.forEach(t),RJo=r(Hz,"to load the model weights."),Hz.forEach(t),SJo=i(Pl),Nfe=n(Pl,"P",{});var urt=s(Nfe);PJo=r(urt,"Examples:"),urt.forEach(t),$Jo=i(Pl),m(Uy.$$.fragment,Pl),Pl.forEach(t),IJo=i(Sl),po=n(Sl,"DIV",{class:!0});var ma=s(po);m(Jy.$$.fragment,ma),jJo=i(ma),Dfe=n(ma,"P",{});var brt=s(Dfe);NJo=r(brt,"Instantiate one of the model classes of the library (with a causal language modeling head) from a pretrained model."),brt.forEach(t),DJo=i(ma),cn=n(ma,"P",{});var S4=s(cn);qJo=r(S4,"The model class to instantiate is selected based on the "),qfe=n(S4,"CODE",{});var vrt=s(qfe);GJo=r(vrt,"model_type"),vrt.forEach(t),OJo=r(S4,` property of the config object (either
passed as an argument or loaded from `),Gfe=n(S4,"CODE",{});var Trt=s(Gfe);XJo=r(Trt,"pretrained_model_name_or_path"),Trt.forEach(t),zJo=r(S4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Ofe=n(S4,"CODE",{});var Frt=s(Ofe);VJo=r(Frt,"pretrained_model_name_or_path"),Frt.forEach(t),WJo=r(S4,":"),S4.forEach(t),QJo=i(ma),he=n(ma,"UL",{});var Me=s(he);I6=n(Me,"LI",{});var dwe=s(I6);Xfe=n(dwe,"STRONG",{});var Crt=s(Xfe);HJo=r(Crt,"bert"),Crt.forEach(t),UJo=r(dwe," \u2014 "),VD=n(dwe,"A",{href:!0});var Mrt=s(VD);JJo=r(Mrt,"TFBertLMHeadModel"),Mrt.forEach(t),YJo=r(dwe," (BERT model)"),dwe.forEach(t),KJo=i(Me),j6=n(Me,"LI",{});var cwe=s(j6);zfe=n(cwe,"STRONG",{});var Ert=s(zfe);ZJo=r(Ert,"ctrl"),Ert.forEach(t),eYo=r(cwe," \u2014 "),WD=n(cwe,"A",{href:!0});var yrt=s(WD);oYo=r(yrt,"TFCTRLLMHeadModel"),yrt.forEach(t),rYo=r(cwe," (CTRL model)"),cwe.forEach(t),tYo=i(Me),N6=n(Me,"LI",{});var fwe=s(N6);Vfe=n(fwe,"STRONG",{});var wrt=s(Vfe);aYo=r(wrt,"gpt2"),wrt.forEach(t),nYo=r(fwe," \u2014 "),QD=n(fwe,"A",{href:!0});var Art=s(QD);sYo=r(Art,"TFGPT2LMHeadModel"),Art.forEach(t),lYo=r(fwe," (OpenAI GPT-2 model)"),fwe.forEach(t),iYo=i(Me),D6=n(Me,"LI",{});var mwe=s(D6);Wfe=n(mwe,"STRONG",{});var Lrt=s(Wfe);dYo=r(Lrt,"openai-gpt"),Lrt.forEach(t),cYo=r(mwe," \u2014 "),HD=n(mwe,"A",{href:!0});var Brt=s(HD);fYo=r(Brt,"TFOpenAIGPTLMHeadModel"),Brt.forEach(t),mYo=r(mwe," (OpenAI GPT model)"),mwe.forEach(t),gYo=i(Me),q6=n(Me,"LI",{});var gwe=s(q6);Qfe=n(gwe,"STRONG",{});var krt=s(Qfe);hYo=r(krt,"rembert"),krt.forEach(t),pYo=r(gwe," \u2014 "),UD=n(gwe,"A",{href:!0});var xrt=s(UD);_Yo=r(xrt,"TFRemBertForCausalLM"),xrt.forEach(t),uYo=r(gwe," (RemBERT model)"),gwe.forEach(t),bYo=i(Me),G6=n(Me,"LI",{});var hwe=s(G6);Hfe=n(hwe,"STRONG",{});var Rrt=s(Hfe);vYo=r(Rrt,"roberta"),Rrt.forEach(t),TYo=r(hwe," \u2014 "),JD=n(hwe,"A",{href:!0});var Srt=s(JD);FYo=r(Srt,"TFRobertaForCausalLM"),Srt.forEach(t),CYo=r(hwe," (RoBERTa model)"),hwe.forEach(t),MYo=i(Me),O6=n(Me,"LI",{});var pwe=s(O6);Ufe=n(pwe,"STRONG",{});var Prt=s(Ufe);EYo=r(Prt,"roformer"),Prt.forEach(t),yYo=r(pwe," \u2014 "),YD=n(pwe,"A",{href:!0});var $rt=s(YD);wYo=r($rt,"TFRoFormerForCausalLM"),$rt.forEach(t),AYo=r(pwe," (RoFormer model)"),pwe.forEach(t),LYo=i(Me),X6=n(Me,"LI",{});var _we=s(X6);Jfe=n(_we,"STRONG",{});var Irt=s(Jfe);BYo=r(Irt,"transfo-xl"),Irt.forEach(t),kYo=r(_we," \u2014 "),KD=n(_we,"A",{href:!0});var jrt=s(KD);xYo=r(jrt,"TFTransfoXLLMHeadModel"),jrt.forEach(t),RYo=r(_we," (Transformer-XL model)"),_we.forEach(t),SYo=i(Me),z6=n(Me,"LI",{});var uwe=s(z6);Yfe=n(uwe,"STRONG",{});var Nrt=s(Yfe);PYo=r(Nrt,"xlm"),Nrt.forEach(t),$Yo=r(uwe," \u2014 "),ZD=n(uwe,"A",{href:!0});var Drt=s(ZD);IYo=r(Drt,"TFXLMWithLMHeadModel"),Drt.forEach(t),jYo=r(uwe," (XLM model)"),uwe.forEach(t),NYo=i(Me),V6=n(Me,"LI",{});var bwe=s(V6);Kfe=n(bwe,"STRONG",{});var qrt=s(Kfe);DYo=r(qrt,"xlnet"),qrt.forEach(t),qYo=r(bwe," \u2014 "),eq=n(bwe,"A",{href:!0});var Grt=s(eq);GYo=r(Grt,"TFXLNetLMHeadModel"),Grt.forEach(t),OYo=r(bwe," (XLNet model)"),bwe.forEach(t),Me.forEach(t),XYo=i(ma),Zfe=n(ma,"P",{});var Ort=s(Zfe);zYo=r(Ort,"Examples:"),Ort.forEach(t),VYo=i(ma),m(Yy.$$.fragment,ma),ma.forEach(t),Sl.forEach(t),T9e=i(d),cc=n(d,"H2",{class:!0});var Bke=s(cc);W6=n(Bke,"A",{id:!0,class:!0,href:!0});var Xrt=s(W6);eme=n(Xrt,"SPAN",{});var zrt=s(eme);m(Ky.$$.fragment,zrt),zrt.forEach(t),Xrt.forEach(t),WYo=i(Bke),ome=n(Bke,"SPAN",{});var Vrt=s(ome);QYo=r(Vrt,"TFAutoModelForImageClassification"),Vrt.forEach(t),Bke.forEach(t),F9e=i(d),_r=n(d,"DIV",{class:!0});var $l=s(_r);m(Zy.$$.fragment,$l),HYo=i($l),fc=n($l,"P",{});var Uz=s(fc);UYo=r(Uz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a image classification head) when created
with the `),rme=n(Uz,"CODE",{});var Wrt=s(rme);JYo=r(Wrt,"from_pretrained()"),Wrt.forEach(t),YYo=r(Uz,"class method or the "),tme=n(Uz,"CODE",{});var Qrt=s(tme);KYo=r(Qrt,"from_config()"),Qrt.forEach(t),ZYo=r(Uz,`class
method.`),Uz.forEach(t),eKo=i($l),ew=n($l,"P",{});var kke=s(ew);oKo=r(kke,"This class cannot be instantiated directly using "),ame=n(kke,"CODE",{});var Hrt=s(ame);rKo=r(Hrt,"__init__()"),Hrt.forEach(t),tKo=r(kke," (throws an error)."),kke.forEach(t),aKo=i($l),dt=n($l,"DIV",{class:!0});var Il=s(dt);m(ow.$$.fragment,Il),nKo=i(Il),nme=n(Il,"P",{});var Urt=s(nme);sKo=r(Urt,"Instantiates one of the model classes of the library (with a image classification head) from a configuration."),Urt.forEach(t),lKo=i(Il),mc=n(Il,"P",{});var Jz=s(mc);iKo=r(Jz,`Note:
Loading a model from its configuration file does `),sme=n(Jz,"STRONG",{});var Jrt=s(sme);dKo=r(Jrt,"not"),Jrt.forEach(t),cKo=r(Jz,` load the model weights. It only affects the
model\u2019s configuration. Use `),lme=n(Jz,"CODE",{});var Yrt=s(lme);fKo=r(Yrt,"from_pretrained()"),Yrt.forEach(t),mKo=r(Jz,"to load the model weights."),Jz.forEach(t),gKo=i(Il),ime=n(Il,"P",{});var Krt=s(ime);hKo=r(Krt,"Examples:"),Krt.forEach(t),pKo=i(Il),m(rw.$$.fragment,Il),Il.forEach(t),_Ko=i($l),_o=n($l,"DIV",{class:!0});var ga=s(_o);m(tw.$$.fragment,ga),uKo=i(ga),dme=n(ga,"P",{});var Zrt=s(dme);bKo=r(Zrt,"Instantiate one of the model classes of the library (with a image classification head) from a pretrained model."),Zrt.forEach(t),vKo=i(ga),fn=n(ga,"P",{});var P4=s(fn);TKo=r(P4,"The model class to instantiate is selected based on the "),cme=n(P4,"CODE",{});var ett=s(cme);FKo=r(ett,"model_type"),ett.forEach(t),CKo=r(P4,` property of the config object (either
passed as an argument or loaded from `),fme=n(P4,"CODE",{});var ott=s(fme);MKo=r(ott,"pretrained_model_name_or_path"),ott.forEach(t),EKo=r(P4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),mme=n(P4,"CODE",{});var rtt=s(mme);yKo=r(rtt,"pretrained_model_name_or_path"),rtt.forEach(t),wKo=r(P4,":"),P4.forEach(t),AKo=i(ga),gme=n(ga,"UL",{});var ttt=s(gme);Q6=n(ttt,"LI",{});var vwe=s(Q6);hme=n(vwe,"STRONG",{});var att=s(hme);LKo=r(att,"vit"),att.forEach(t),BKo=r(vwe," \u2014 "),oq=n(vwe,"A",{href:!0});var ntt=s(oq);kKo=r(ntt,"TFViTForImageClassification"),ntt.forEach(t),xKo=r(vwe," (ViT model)"),vwe.forEach(t),ttt.forEach(t),RKo=i(ga),pme=n(ga,"P",{});var stt=s(pme);SKo=r(stt,"Examples:"),stt.forEach(t),PKo=i(ga),m(aw.$$.fragment,ga),ga.forEach(t),$l.forEach(t),C9e=i(d),gc=n(d,"H2",{class:!0});var xke=s(gc);H6=n(xke,"A",{id:!0,class:!0,href:!0});var ltt=s(H6);_me=n(ltt,"SPAN",{});var itt=s(_me);m(nw.$$.fragment,itt),itt.forEach(t),ltt.forEach(t),$Ko=i(xke),ume=n(xke,"SPAN",{});var dtt=s(ume);IKo=r(dtt,"TFAutoModelForMaskedLM"),dtt.forEach(t),xke.forEach(t),M9e=i(d),ur=n(d,"DIV",{class:!0});var jl=s(ur);m(sw.$$.fragment,jl),jKo=i(jl),hc=n(jl,"P",{});var Yz=s(hc);NKo=r(Yz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a masked language modeling head) when created
with the `),bme=n(Yz,"CODE",{});var ctt=s(bme);DKo=r(ctt,"from_pretrained()"),ctt.forEach(t),qKo=r(Yz,"class method or the "),vme=n(Yz,"CODE",{});var ftt=s(vme);GKo=r(ftt,"from_config()"),ftt.forEach(t),OKo=r(Yz,`class
method.`),Yz.forEach(t),XKo=i(jl),lw=n(jl,"P",{});var Rke=s(lw);zKo=r(Rke,"This class cannot be instantiated directly using "),Tme=n(Rke,"CODE",{});var mtt=s(Tme);VKo=r(mtt,"__init__()"),mtt.forEach(t),WKo=r(Rke," (throws an error)."),Rke.forEach(t),QKo=i(jl),ct=n(jl,"DIV",{class:!0});var Nl=s(ct);m(iw.$$.fragment,Nl),HKo=i(Nl),Fme=n(Nl,"P",{});var gtt=s(Fme);UKo=r(gtt,"Instantiates one of the model classes of the library (with a masked language modeling head) from a configuration."),gtt.forEach(t),JKo=i(Nl),pc=n(Nl,"P",{});var Kz=s(pc);YKo=r(Kz,`Note:
Loading a model from its configuration file does `),Cme=n(Kz,"STRONG",{});var htt=s(Cme);KKo=r(htt,"not"),htt.forEach(t),ZKo=r(Kz,` load the model weights. It only affects the
model\u2019s configuration. Use `),Mme=n(Kz,"CODE",{});var ptt=s(Mme);eZo=r(ptt,"from_pretrained()"),ptt.forEach(t),oZo=r(Kz,"to load the model weights."),Kz.forEach(t),rZo=i(Nl),Eme=n(Nl,"P",{});var _tt=s(Eme);tZo=r(_tt,"Examples:"),_tt.forEach(t),aZo=i(Nl),m(dw.$$.fragment,Nl),Nl.forEach(t),nZo=i(jl),uo=n(jl,"DIV",{class:!0});var ha=s(uo);m(cw.$$.fragment,ha),sZo=i(ha),yme=n(ha,"P",{});var utt=s(yme);lZo=r(utt,"Instantiate one of the model classes of the library (with a masked language modeling head) from a pretrained model."),utt.forEach(t),iZo=i(ha),mn=n(ha,"P",{});var $4=s(mn);dZo=r($4,"The model class to instantiate is selected based on the "),wme=n($4,"CODE",{});var btt=s(wme);cZo=r(btt,"model_type"),btt.forEach(t),fZo=r($4,` property of the config object (either
passed as an argument or loaded from `),Ame=n($4,"CODE",{});var vtt=s(Ame);mZo=r(vtt,"pretrained_model_name_or_path"),vtt.forEach(t),gZo=r($4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Lme=n($4,"CODE",{});var Ttt=s(Lme);hZo=r(Ttt,"pretrained_model_name_or_path"),Ttt.forEach(t),pZo=r($4,":"),$4.forEach(t),_Zo=i(ha),Y=n(ha,"UL",{});var ee=s(Y);U6=n(ee,"LI",{});var Twe=s(U6);Bme=n(Twe,"STRONG",{});var Ftt=s(Bme);uZo=r(Ftt,"albert"),Ftt.forEach(t),bZo=r(Twe," \u2014 "),rq=n(Twe,"A",{href:!0});var Ctt=s(rq);vZo=r(Ctt,"TFAlbertForMaskedLM"),Ctt.forEach(t),TZo=r(Twe," (ALBERT model)"),Twe.forEach(t),FZo=i(ee),J6=n(ee,"LI",{});var Fwe=s(J6);kme=n(Fwe,"STRONG",{});var Mtt=s(kme);CZo=r(Mtt,"bert"),Mtt.forEach(t),MZo=r(Fwe," \u2014 "),tq=n(Fwe,"A",{href:!0});var Ett=s(tq);EZo=r(Ett,"TFBertForMaskedLM"),Ett.forEach(t),yZo=r(Fwe," (BERT model)"),Fwe.forEach(t),wZo=i(ee),Y6=n(ee,"LI",{});var Cwe=s(Y6);xme=n(Cwe,"STRONG",{});var ytt=s(xme);AZo=r(ytt,"camembert"),ytt.forEach(t),LZo=r(Cwe," \u2014 "),aq=n(Cwe,"A",{href:!0});var wtt=s(aq);BZo=r(wtt,"TFCamembertForMaskedLM"),wtt.forEach(t),kZo=r(Cwe," (CamemBERT model)"),Cwe.forEach(t),xZo=i(ee),K6=n(ee,"LI",{});var Mwe=s(K6);Rme=n(Mwe,"STRONG",{});var Att=s(Rme);RZo=r(Att,"convbert"),Att.forEach(t),SZo=r(Mwe," \u2014 "),nq=n(Mwe,"A",{href:!0});var Ltt=s(nq);PZo=r(Ltt,"TFConvBertForMaskedLM"),Ltt.forEach(t),$Zo=r(Mwe," (ConvBERT model)"),Mwe.forEach(t),IZo=i(ee),Z6=n(ee,"LI",{});var Ewe=s(Z6);Sme=n(Ewe,"STRONG",{});var Btt=s(Sme);jZo=r(Btt,"deberta"),Btt.forEach(t),NZo=r(Ewe," \u2014 "),sq=n(Ewe,"A",{href:!0});var ktt=s(sq);DZo=r(ktt,"TFDebertaForMaskedLM"),ktt.forEach(t),qZo=r(Ewe," (DeBERTa model)"),Ewe.forEach(t),GZo=i(ee),eT=n(ee,"LI",{});var ywe=s(eT);Pme=n(ywe,"STRONG",{});var xtt=s(Pme);OZo=r(xtt,"deberta-v2"),xtt.forEach(t),XZo=r(ywe," \u2014 "),lq=n(ywe,"A",{href:!0});var Rtt=s(lq);zZo=r(Rtt,"TFDebertaV2ForMaskedLM"),Rtt.forEach(t),VZo=r(ywe," (DeBERTa-v2 model)"),ywe.forEach(t),WZo=i(ee),oT=n(ee,"LI",{});var wwe=s(oT);$me=n(wwe,"STRONG",{});var Stt=s($me);QZo=r(Stt,"distilbert"),Stt.forEach(t),HZo=r(wwe," \u2014 "),iq=n(wwe,"A",{href:!0});var Ptt=s(iq);UZo=r(Ptt,"TFDistilBertForMaskedLM"),Ptt.forEach(t),JZo=r(wwe," (DistilBERT model)"),wwe.forEach(t),YZo=i(ee),rT=n(ee,"LI",{});var Awe=s(rT);Ime=n(Awe,"STRONG",{});var $tt=s(Ime);KZo=r($tt,"electra"),$tt.forEach(t),ZZo=r(Awe," \u2014 "),dq=n(Awe,"A",{href:!0});var Itt=s(dq);eer=r(Itt,"TFElectraForMaskedLM"),Itt.forEach(t),oer=r(Awe," (ELECTRA model)"),Awe.forEach(t),rer=i(ee),tT=n(ee,"LI",{});var Lwe=s(tT);jme=n(Lwe,"STRONG",{});var jtt=s(jme);ter=r(jtt,"flaubert"),jtt.forEach(t),aer=r(Lwe," \u2014 "),cq=n(Lwe,"A",{href:!0});var Ntt=s(cq);ner=r(Ntt,"TFFlaubertWithLMHeadModel"),Ntt.forEach(t),ser=r(Lwe," (FlauBERT model)"),Lwe.forEach(t),ler=i(ee),aT=n(ee,"LI",{});var Bwe=s(aT);Nme=n(Bwe,"STRONG",{});var Dtt=s(Nme);ier=r(Dtt,"funnel"),Dtt.forEach(t),der=r(Bwe," \u2014 "),fq=n(Bwe,"A",{href:!0});var qtt=s(fq);cer=r(qtt,"TFFunnelForMaskedLM"),qtt.forEach(t),fer=r(Bwe," (Funnel Transformer model)"),Bwe.forEach(t),mer=i(ee),nT=n(ee,"LI",{});var kwe=s(nT);Dme=n(kwe,"STRONG",{});var Gtt=s(Dme);ger=r(Gtt,"layoutlm"),Gtt.forEach(t),her=r(kwe," \u2014 "),mq=n(kwe,"A",{href:!0});var Ott=s(mq);per=r(Ott,"TFLayoutLMForMaskedLM"),Ott.forEach(t),_er=r(kwe," (LayoutLM model)"),kwe.forEach(t),uer=i(ee),sT=n(ee,"LI",{});var xwe=s(sT);qme=n(xwe,"STRONG",{});var Xtt=s(qme);ber=r(Xtt,"longformer"),Xtt.forEach(t),ver=r(xwe," \u2014 "),gq=n(xwe,"A",{href:!0});var ztt=s(gq);Ter=r(ztt,"TFLongformerForMaskedLM"),ztt.forEach(t),Fer=r(xwe," (Longformer model)"),xwe.forEach(t),Cer=i(ee),lT=n(ee,"LI",{});var Rwe=s(lT);Gme=n(Rwe,"STRONG",{});var Vtt=s(Gme);Mer=r(Vtt,"mobilebert"),Vtt.forEach(t),Eer=r(Rwe," \u2014 "),hq=n(Rwe,"A",{href:!0});var Wtt=s(hq);yer=r(Wtt,"TFMobileBertForMaskedLM"),Wtt.forEach(t),wer=r(Rwe," (MobileBERT model)"),Rwe.forEach(t),Aer=i(ee),iT=n(ee,"LI",{});var Swe=s(iT);Ome=n(Swe,"STRONG",{});var Qtt=s(Ome);Ler=r(Qtt,"mpnet"),Qtt.forEach(t),Ber=r(Swe," \u2014 "),pq=n(Swe,"A",{href:!0});var Htt=s(pq);ker=r(Htt,"TFMPNetForMaskedLM"),Htt.forEach(t),xer=r(Swe," (MPNet model)"),Swe.forEach(t),Rer=i(ee),dT=n(ee,"LI",{});var Pwe=s(dT);Xme=n(Pwe,"STRONG",{});var Utt=s(Xme);Ser=r(Utt,"rembert"),Utt.forEach(t),Per=r(Pwe," \u2014 "),_q=n(Pwe,"A",{href:!0});var Jtt=s(_q);$er=r(Jtt,"TFRemBertForMaskedLM"),Jtt.forEach(t),Ier=r(Pwe," (RemBERT model)"),Pwe.forEach(t),jer=i(ee),cT=n(ee,"LI",{});var $we=s(cT);zme=n($we,"STRONG",{});var Ytt=s(zme);Ner=r(Ytt,"roberta"),Ytt.forEach(t),Der=r($we," \u2014 "),uq=n($we,"A",{href:!0});var Ktt=s(uq);qer=r(Ktt,"TFRobertaForMaskedLM"),Ktt.forEach(t),Ger=r($we," (RoBERTa model)"),$we.forEach(t),Oer=i(ee),fT=n(ee,"LI",{});var Iwe=s(fT);Vme=n(Iwe,"STRONG",{});var Ztt=s(Vme);Xer=r(Ztt,"roformer"),Ztt.forEach(t),zer=r(Iwe," \u2014 "),bq=n(Iwe,"A",{href:!0});var eat=s(bq);Ver=r(eat,"TFRoFormerForMaskedLM"),eat.forEach(t),Wer=r(Iwe," (RoFormer model)"),Iwe.forEach(t),Qer=i(ee),mT=n(ee,"LI",{});var jwe=s(mT);Wme=n(jwe,"STRONG",{});var oat=s(Wme);Her=r(oat,"tapas"),oat.forEach(t),Uer=r(jwe," \u2014 "),vq=n(jwe,"A",{href:!0});var rat=s(vq);Jer=r(rat,"TFTapasForMaskedLM"),rat.forEach(t),Yer=r(jwe," (TAPAS model)"),jwe.forEach(t),Ker=i(ee),gT=n(ee,"LI",{});var Nwe=s(gT);Qme=n(Nwe,"STRONG",{});var tat=s(Qme);Zer=r(tat,"xlm"),tat.forEach(t),eor=r(Nwe," \u2014 "),Tq=n(Nwe,"A",{href:!0});var aat=s(Tq);oor=r(aat,"TFXLMWithLMHeadModel"),aat.forEach(t),ror=r(Nwe," (XLM model)"),Nwe.forEach(t),tor=i(ee),hT=n(ee,"LI",{});var Dwe=s(hT);Hme=n(Dwe,"STRONG",{});var nat=s(Hme);aor=r(nat,"xlm-roberta"),nat.forEach(t),nor=r(Dwe," \u2014 "),Fq=n(Dwe,"A",{href:!0});var sat=s(Fq);sor=r(sat,"TFXLMRobertaForMaskedLM"),sat.forEach(t),lor=r(Dwe," (XLM-RoBERTa model)"),Dwe.forEach(t),ee.forEach(t),ior=i(ha),Ume=n(ha,"P",{});var lat=s(Ume);dor=r(lat,"Examples:"),lat.forEach(t),cor=i(ha),m(fw.$$.fragment,ha),ha.forEach(t),jl.forEach(t),E9e=i(d),_c=n(d,"H2",{class:!0});var Ske=s(_c);pT=n(Ske,"A",{id:!0,class:!0,href:!0});var iat=s(pT);Jme=n(iat,"SPAN",{});var dat=s(Jme);m(mw.$$.fragment,dat),dat.forEach(t),iat.forEach(t),mor=i(Ske),Yme=n(Ske,"SPAN",{});var cat=s(Yme);gor=r(cat,"TFAutoModelForSeq2SeqLM"),cat.forEach(t),Ske.forEach(t),y9e=i(d),br=n(d,"DIV",{class:!0});var Dl=s(br);m(gw.$$.fragment,Dl),hor=i(Dl),uc=n(Dl,"P",{});var Zz=s(uc);por=r(Zz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence language modeling head) when created
with the `),Kme=n(Zz,"CODE",{});var fat=s(Kme);_or=r(fat,"from_pretrained()"),fat.forEach(t),uor=r(Zz,"class method or the "),Zme=n(Zz,"CODE",{});var mat=s(Zme);bor=r(mat,"from_config()"),mat.forEach(t),vor=r(Zz,`class
method.`),Zz.forEach(t),Tor=i(Dl),hw=n(Dl,"P",{});var Pke=s(hw);For=r(Pke,"This class cannot be instantiated directly using "),ege=n(Pke,"CODE",{});var gat=s(ege);Cor=r(gat,"__init__()"),gat.forEach(t),Mor=r(Pke," (throws an error)."),Pke.forEach(t),Eor=i(Dl),ft=n(Dl,"DIV",{class:!0});var ql=s(ft);m(pw.$$.fragment,ql),yor=i(ql),oge=n(ql,"P",{});var hat=s(oge);wor=r(hat,"Instantiates one of the model classes of the library (with a sequence-to-sequence language modeling head) from a configuration."),hat.forEach(t),Aor=i(ql),bc=n(ql,"P",{});var eV=s(bc);Lor=r(eV,`Note:
Loading a model from its configuration file does `),rge=n(eV,"STRONG",{});var pat=s(rge);Bor=r(pat,"not"),pat.forEach(t),kor=r(eV,` load the model weights. It only affects the
model\u2019s configuration. Use `),tge=n(eV,"CODE",{});var _at=s(tge);xor=r(_at,"from_pretrained()"),_at.forEach(t),Ror=r(eV,"to load the model weights."),eV.forEach(t),Sor=i(ql),age=n(ql,"P",{});var uat=s(age);Por=r(uat,"Examples:"),uat.forEach(t),$or=i(ql),m(_w.$$.fragment,ql),ql.forEach(t),Ior=i(Dl),bo=n(Dl,"DIV",{class:!0});var pa=s(bo);m(uw.$$.fragment,pa),jor=i(pa),nge=n(pa,"P",{});var bat=s(nge);Nor=r(bat,"Instantiate one of the model classes of the library (with a sequence-to-sequence language modeling head) from a pretrained model."),bat.forEach(t),Dor=i(pa),gn=n(pa,"P",{});var I4=s(gn);qor=r(I4,"The model class to instantiate is selected based on the "),sge=n(I4,"CODE",{});var vat=s(sge);Gor=r(vat,"model_type"),vat.forEach(t),Oor=r(I4,` property of the config object (either
passed as an argument or loaded from `),lge=n(I4,"CODE",{});var Tat=s(lge);Xor=r(Tat,"pretrained_model_name_or_path"),Tat.forEach(t),zor=r(I4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),ige=n(I4,"CODE",{});var Fat=s(ige);Vor=r(Fat,"pretrained_model_name_or_path"),Fat.forEach(t),Wor=r(I4,":"),I4.forEach(t),Qor=i(pa),pe=n(pa,"UL",{});var Ee=s(pe);_T=n(Ee,"LI",{});var qwe=s(_T);dge=n(qwe,"STRONG",{});var Cat=s(dge);Hor=r(Cat,"bart"),Cat.forEach(t),Uor=r(qwe," \u2014 "),Cq=n(qwe,"A",{href:!0});var Mat=s(Cq);Jor=r(Mat,"TFBartForConditionalGeneration"),Mat.forEach(t),Yor=r(qwe," (BART model)"),qwe.forEach(t),Kor=i(Ee),uT=n(Ee,"LI",{});var Gwe=s(uT);cge=n(Gwe,"STRONG",{});var Eat=s(cge);Zor=r(Eat,"blenderbot"),Eat.forEach(t),err=r(Gwe," \u2014 "),Mq=n(Gwe,"A",{href:!0});var yat=s(Mq);orr=r(yat,"TFBlenderbotForConditionalGeneration"),yat.forEach(t),rrr=r(Gwe," (Blenderbot model)"),Gwe.forEach(t),trr=i(Ee),bT=n(Ee,"LI",{});var Owe=s(bT);fge=n(Owe,"STRONG",{});var wat=s(fge);arr=r(wat,"blenderbot-small"),wat.forEach(t),nrr=r(Owe," \u2014 "),Eq=n(Owe,"A",{href:!0});var Aat=s(Eq);srr=r(Aat,"TFBlenderbotSmallForConditionalGeneration"),Aat.forEach(t),lrr=r(Owe," (BlenderbotSmall model)"),Owe.forEach(t),irr=i(Ee),vT=n(Ee,"LI",{});var Xwe=s(vT);mge=n(Xwe,"STRONG",{});var Lat=s(mge);drr=r(Lat,"encoder-decoder"),Lat.forEach(t),crr=r(Xwe," \u2014 "),yq=n(Xwe,"A",{href:!0});var Bat=s(yq);frr=r(Bat,"TFEncoderDecoderModel"),Bat.forEach(t),mrr=r(Xwe," (Encoder decoder model)"),Xwe.forEach(t),grr=i(Ee),TT=n(Ee,"LI",{});var zwe=s(TT);gge=n(zwe,"STRONG",{});var kat=s(gge);hrr=r(kat,"led"),kat.forEach(t),prr=r(zwe," \u2014 "),wq=n(zwe,"A",{href:!0});var xat=s(wq);_rr=r(xat,"TFLEDForConditionalGeneration"),xat.forEach(t),urr=r(zwe," (LED model)"),zwe.forEach(t),brr=i(Ee),FT=n(Ee,"LI",{});var Vwe=s(FT);hge=n(Vwe,"STRONG",{});var Rat=s(hge);vrr=r(Rat,"marian"),Rat.forEach(t),Trr=r(Vwe," \u2014 "),Aq=n(Vwe,"A",{href:!0});var Sat=s(Aq);Frr=r(Sat,"TFMarianMTModel"),Sat.forEach(t),Crr=r(Vwe," (Marian model)"),Vwe.forEach(t),Mrr=i(Ee),CT=n(Ee,"LI",{});var Wwe=s(CT);pge=n(Wwe,"STRONG",{});var Pat=s(pge);Err=r(Pat,"mbart"),Pat.forEach(t),yrr=r(Wwe," \u2014 "),Lq=n(Wwe,"A",{href:!0});var $at=s(Lq);wrr=r($at,"TFMBartForConditionalGeneration"),$at.forEach(t),Arr=r(Wwe," (mBART model)"),Wwe.forEach(t),Lrr=i(Ee),MT=n(Ee,"LI",{});var Qwe=s(MT);_ge=n(Qwe,"STRONG",{});var Iat=s(_ge);Brr=r(Iat,"mt5"),Iat.forEach(t),krr=r(Qwe," \u2014 "),Bq=n(Qwe,"A",{href:!0});var jat=s(Bq);xrr=r(jat,"TFMT5ForConditionalGeneration"),jat.forEach(t),Rrr=r(Qwe," (mT5 model)"),Qwe.forEach(t),Srr=i(Ee),ET=n(Ee,"LI",{});var Hwe=s(ET);uge=n(Hwe,"STRONG",{});var Nat=s(uge);Prr=r(Nat,"pegasus"),Nat.forEach(t),$rr=r(Hwe," \u2014 "),kq=n(Hwe,"A",{href:!0});var Dat=s(kq);Irr=r(Dat,"TFPegasusForConditionalGeneration"),Dat.forEach(t),jrr=r(Hwe," (Pegasus model)"),Hwe.forEach(t),Nrr=i(Ee),yT=n(Ee,"LI",{});var Uwe=s(yT);bge=n(Uwe,"STRONG",{});var qat=s(bge);Drr=r(qat,"t5"),qat.forEach(t),qrr=r(Uwe," \u2014 "),xq=n(Uwe,"A",{href:!0});var Gat=s(xq);Grr=r(Gat,"TFT5ForConditionalGeneration"),Gat.forEach(t),Orr=r(Uwe," (T5 model)"),Uwe.forEach(t),Ee.forEach(t),Xrr=i(pa),vge=n(pa,"P",{});var Oat=s(vge);zrr=r(Oat,"Examples:"),Oat.forEach(t),Vrr=i(pa),m(bw.$$.fragment,pa),pa.forEach(t),Dl.forEach(t),w9e=i(d),vc=n(d,"H2",{class:!0});var $ke=s(vc);wT=n($ke,"A",{id:!0,class:!0,href:!0});var Xat=s(wT);Tge=n(Xat,"SPAN",{});var zat=s(Tge);m(vw.$$.fragment,zat),zat.forEach(t),Xat.forEach(t),Wrr=i($ke),Fge=n($ke,"SPAN",{});var Vat=s(Fge);Qrr=r(Vat,"TFAutoModelForSequenceClassification"),Vat.forEach(t),$ke.forEach(t),A9e=i(d),vr=n(d,"DIV",{class:!0});var Gl=s(vr);m(Tw.$$.fragment,Gl),Hrr=i(Gl),Tc=n(Gl,"P",{});var oV=s(Tc);Urr=r(oV,`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence classification head) when created
with the `),Cge=n(oV,"CODE",{});var Wat=s(Cge);Jrr=r(Wat,"from_pretrained()"),Wat.forEach(t),Yrr=r(oV,"class method or the "),Mge=n(oV,"CODE",{});var Qat=s(Mge);Krr=r(Qat,"from_config()"),Qat.forEach(t),Zrr=r(oV,`class
method.`),oV.forEach(t),etr=i(Gl),Fw=n(Gl,"P",{});var Ike=s(Fw);otr=r(Ike,"This class cannot be instantiated directly using "),Ege=n(Ike,"CODE",{});var Hat=s(Ege);rtr=r(Hat,"__init__()"),Hat.forEach(t),ttr=r(Ike," (throws an error)."),Ike.forEach(t),atr=i(Gl),mt=n(Gl,"DIV",{class:!0});var Ol=s(mt);m(Cw.$$.fragment,Ol),ntr=i(Ol),yge=n(Ol,"P",{});var Uat=s(yge);str=r(Uat,"Instantiates one of the model classes of the library (with a sequence classification head) from a configuration."),Uat.forEach(t),ltr=i(Ol),Fc=n(Ol,"P",{});var rV=s(Fc);itr=r(rV,`Note:
Loading a model from its configuration file does `),wge=n(rV,"STRONG",{});var Jat=s(wge);dtr=r(Jat,"not"),Jat.forEach(t),ctr=r(rV,` load the model weights. It only affects the
model\u2019s configuration. Use `),Age=n(rV,"CODE",{});var Yat=s(Age);ftr=r(Yat,"from_pretrained()"),Yat.forEach(t),mtr=r(rV,"to load the model weights."),rV.forEach(t),gtr=i(Ol),Lge=n(Ol,"P",{});var Kat=s(Lge);htr=r(Kat,"Examples:"),Kat.forEach(t),ptr=i(Ol),m(Mw.$$.fragment,Ol),Ol.forEach(t),_tr=i(Gl),vo=n(Gl,"DIV",{class:!0});var _a=s(vo);m(Ew.$$.fragment,_a),utr=i(_a),Bge=n(_a,"P",{});var Zat=s(Bge);btr=r(Zat,"Instantiate one of the model classes of the library (with a sequence classification head) from a pretrained model."),Zat.forEach(t),vtr=i(_a),hn=n(_a,"P",{});var j4=s(hn);Ttr=r(j4,"The model class to instantiate is selected based on the "),kge=n(j4,"CODE",{});var ent=s(kge);Ftr=r(ent,"model_type"),ent.forEach(t),Ctr=r(j4,` property of the config object (either
passed as an argument or loaded from `),xge=n(j4,"CODE",{});var ont=s(xge);Mtr=r(ont,"pretrained_model_name_or_path"),ont.forEach(t),Etr=r(j4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Rge=n(j4,"CODE",{});var rnt=s(Rge);ytr=r(rnt,"pretrained_model_name_or_path"),rnt.forEach(t),wtr=r(j4,":"),j4.forEach(t),Atr=i(_a),X=n(_a,"UL",{});var W=s(X);AT=n(W,"LI",{});var Jwe=s(AT);Sge=n(Jwe,"STRONG",{});var tnt=s(Sge);Ltr=r(tnt,"albert"),tnt.forEach(t),Btr=r(Jwe," \u2014 "),Rq=n(Jwe,"A",{href:!0});var ant=s(Rq);ktr=r(ant,"TFAlbertForSequenceClassification"),ant.forEach(t),xtr=r(Jwe," (ALBERT model)"),Jwe.forEach(t),Rtr=i(W),LT=n(W,"LI",{});var Ywe=s(LT);Pge=n(Ywe,"STRONG",{});var nnt=s(Pge);Str=r(nnt,"bert"),nnt.forEach(t),Ptr=r(Ywe," \u2014 "),Sq=n(Ywe,"A",{href:!0});var snt=s(Sq);$tr=r(snt,"TFBertForSequenceClassification"),snt.forEach(t),Itr=r(Ywe," (BERT model)"),Ywe.forEach(t),jtr=i(W),BT=n(W,"LI",{});var Kwe=s(BT);$ge=n(Kwe,"STRONG",{});var lnt=s($ge);Ntr=r(lnt,"camembert"),lnt.forEach(t),Dtr=r(Kwe," \u2014 "),Pq=n(Kwe,"A",{href:!0});var int=s(Pq);qtr=r(int,"TFCamembertForSequenceClassification"),int.forEach(t),Gtr=r(Kwe," (CamemBERT model)"),Kwe.forEach(t),Otr=i(W),kT=n(W,"LI",{});var Zwe=s(kT);Ige=n(Zwe,"STRONG",{});var dnt=s(Ige);Xtr=r(dnt,"convbert"),dnt.forEach(t),ztr=r(Zwe," \u2014 "),$q=n(Zwe,"A",{href:!0});var cnt=s($q);Vtr=r(cnt,"TFConvBertForSequenceClassification"),cnt.forEach(t),Wtr=r(Zwe," (ConvBERT model)"),Zwe.forEach(t),Qtr=i(W),xT=n(W,"LI",{});var eAe=s(xT);jge=n(eAe,"STRONG",{});var fnt=s(jge);Htr=r(fnt,"ctrl"),fnt.forEach(t),Utr=r(eAe," \u2014 "),Iq=n(eAe,"A",{href:!0});var mnt=s(Iq);Jtr=r(mnt,"TFCTRLForSequenceClassification"),mnt.forEach(t),Ytr=r(eAe," (CTRL model)"),eAe.forEach(t),Ktr=i(W),RT=n(W,"LI",{});var oAe=s(RT);Nge=n(oAe,"STRONG",{});var gnt=s(Nge);Ztr=r(gnt,"deberta"),gnt.forEach(t),ear=r(oAe," \u2014 "),jq=n(oAe,"A",{href:!0});var hnt=s(jq);oar=r(hnt,"TFDebertaForSequenceClassification"),hnt.forEach(t),rar=r(oAe," (DeBERTa model)"),oAe.forEach(t),tar=i(W),ST=n(W,"LI",{});var rAe=s(ST);Dge=n(rAe,"STRONG",{});var pnt=s(Dge);aar=r(pnt,"deberta-v2"),pnt.forEach(t),nar=r(rAe," \u2014 "),Nq=n(rAe,"A",{href:!0});var _nt=s(Nq);sar=r(_nt,"TFDebertaV2ForSequenceClassification"),_nt.forEach(t),lar=r(rAe," (DeBERTa-v2 model)"),rAe.forEach(t),iar=i(W),PT=n(W,"LI",{});var tAe=s(PT);qge=n(tAe,"STRONG",{});var unt=s(qge);dar=r(unt,"distilbert"),unt.forEach(t),car=r(tAe," \u2014 "),Dq=n(tAe,"A",{href:!0});var bnt=s(Dq);far=r(bnt,"TFDistilBertForSequenceClassification"),bnt.forEach(t),mar=r(tAe," (DistilBERT model)"),tAe.forEach(t),gar=i(W),$T=n(W,"LI",{});var aAe=s($T);Gge=n(aAe,"STRONG",{});var vnt=s(Gge);har=r(vnt,"electra"),vnt.forEach(t),par=r(aAe," \u2014 "),qq=n(aAe,"A",{href:!0});var Tnt=s(qq);_ar=r(Tnt,"TFElectraForSequenceClassification"),Tnt.forEach(t),uar=r(aAe," (ELECTRA model)"),aAe.forEach(t),bar=i(W),IT=n(W,"LI",{});var nAe=s(IT);Oge=n(nAe,"STRONG",{});var Fnt=s(Oge);Tar=r(Fnt,"flaubert"),Fnt.forEach(t),Far=r(nAe," \u2014 "),Gq=n(nAe,"A",{href:!0});var Cnt=s(Gq);Car=r(Cnt,"TFFlaubertForSequenceClassification"),Cnt.forEach(t),Mar=r(nAe," (FlauBERT model)"),nAe.forEach(t),Ear=i(W),jT=n(W,"LI",{});var sAe=s(jT);Xge=n(sAe,"STRONG",{});var Mnt=s(Xge);yar=r(Mnt,"funnel"),Mnt.forEach(t),war=r(sAe," \u2014 "),Oq=n(sAe,"A",{href:!0});var Ent=s(Oq);Aar=r(Ent,"TFFunnelForSequenceClassification"),Ent.forEach(t),Lar=r(sAe," (Funnel Transformer model)"),sAe.forEach(t),Bar=i(W),NT=n(W,"LI",{});var lAe=s(NT);zge=n(lAe,"STRONG",{});var ynt=s(zge);kar=r(ynt,"gpt2"),ynt.forEach(t),xar=r(lAe," \u2014 "),Xq=n(lAe,"A",{href:!0});var wnt=s(Xq);Rar=r(wnt,"TFGPT2ForSequenceClassification"),wnt.forEach(t),Sar=r(lAe," (OpenAI GPT-2 model)"),lAe.forEach(t),Par=i(W),DT=n(W,"LI",{});var iAe=s(DT);Vge=n(iAe,"STRONG",{});var Ant=s(Vge);$ar=r(Ant,"layoutlm"),Ant.forEach(t),Iar=r(iAe," \u2014 "),zq=n(iAe,"A",{href:!0});var Lnt=s(zq);jar=r(Lnt,"TFLayoutLMForSequenceClassification"),Lnt.forEach(t),Nar=r(iAe," (LayoutLM model)"),iAe.forEach(t),Dar=i(W),qT=n(W,"LI",{});var dAe=s(qT);Wge=n(dAe,"STRONG",{});var Bnt=s(Wge);qar=r(Bnt,"longformer"),Bnt.forEach(t),Gar=r(dAe," \u2014 "),Vq=n(dAe,"A",{href:!0});var knt=s(Vq);Oar=r(knt,"TFLongformerForSequenceClassification"),knt.forEach(t),Xar=r(dAe," (Longformer model)"),dAe.forEach(t),zar=i(W),GT=n(W,"LI",{});var cAe=s(GT);Qge=n(cAe,"STRONG",{});var xnt=s(Qge);Var=r(xnt,"mobilebert"),xnt.forEach(t),War=r(cAe," \u2014 "),Wq=n(cAe,"A",{href:!0});var Rnt=s(Wq);Qar=r(Rnt,"TFMobileBertForSequenceClassification"),Rnt.forEach(t),Har=r(cAe," (MobileBERT model)"),cAe.forEach(t),Uar=i(W),OT=n(W,"LI",{});var fAe=s(OT);Hge=n(fAe,"STRONG",{});var Snt=s(Hge);Jar=r(Snt,"mpnet"),Snt.forEach(t),Yar=r(fAe," \u2014 "),Qq=n(fAe,"A",{href:!0});var Pnt=s(Qq);Kar=r(Pnt,"TFMPNetForSequenceClassification"),Pnt.forEach(t),Zar=r(fAe," (MPNet model)"),fAe.forEach(t),enr=i(W),XT=n(W,"LI",{});var mAe=s(XT);Uge=n(mAe,"STRONG",{});var $nt=s(Uge);onr=r($nt,"openai-gpt"),$nt.forEach(t),rnr=r(mAe," \u2014 "),Hq=n(mAe,"A",{href:!0});var Int=s(Hq);tnr=r(Int,"TFOpenAIGPTForSequenceClassification"),Int.forEach(t),anr=r(mAe," (OpenAI GPT model)"),mAe.forEach(t),nnr=i(W),zT=n(W,"LI",{});var gAe=s(zT);Jge=n(gAe,"STRONG",{});var jnt=s(Jge);snr=r(jnt,"rembert"),jnt.forEach(t),lnr=r(gAe," \u2014 "),Uq=n(gAe,"A",{href:!0});var Nnt=s(Uq);inr=r(Nnt,"TFRemBertForSequenceClassification"),Nnt.forEach(t),dnr=r(gAe," (RemBERT model)"),gAe.forEach(t),cnr=i(W),VT=n(W,"LI",{});var hAe=s(VT);Yge=n(hAe,"STRONG",{});var Dnt=s(Yge);fnr=r(Dnt,"roberta"),Dnt.forEach(t),mnr=r(hAe," \u2014 "),Jq=n(hAe,"A",{href:!0});var qnt=s(Jq);gnr=r(qnt,"TFRobertaForSequenceClassification"),qnt.forEach(t),hnr=r(hAe," (RoBERTa model)"),hAe.forEach(t),pnr=i(W),WT=n(W,"LI",{});var pAe=s(WT);Kge=n(pAe,"STRONG",{});var Gnt=s(Kge);_nr=r(Gnt,"roformer"),Gnt.forEach(t),unr=r(pAe," \u2014 "),Yq=n(pAe,"A",{href:!0});var Ont=s(Yq);bnr=r(Ont,"TFRoFormerForSequenceClassification"),Ont.forEach(t),vnr=r(pAe," (RoFormer model)"),pAe.forEach(t),Tnr=i(W),QT=n(W,"LI",{});var _Ae=s(QT);Zge=n(_Ae,"STRONG",{});var Xnt=s(Zge);Fnr=r(Xnt,"tapas"),Xnt.forEach(t),Cnr=r(_Ae," \u2014 "),Kq=n(_Ae,"A",{href:!0});var znt=s(Kq);Mnr=r(znt,"TFTapasForSequenceClassification"),znt.forEach(t),Enr=r(_Ae," (TAPAS model)"),_Ae.forEach(t),ynr=i(W),HT=n(W,"LI",{});var uAe=s(HT);ehe=n(uAe,"STRONG",{});var Vnt=s(ehe);wnr=r(Vnt,"transfo-xl"),Vnt.forEach(t),Anr=r(uAe," \u2014 "),Zq=n(uAe,"A",{href:!0});var Wnt=s(Zq);Lnr=r(Wnt,"TFTransfoXLForSequenceClassification"),Wnt.forEach(t),Bnr=r(uAe," (Transformer-XL model)"),uAe.forEach(t),knr=i(W),UT=n(W,"LI",{});var bAe=s(UT);ohe=n(bAe,"STRONG",{});var Qnt=s(ohe);xnr=r(Qnt,"xlm"),Qnt.forEach(t),Rnr=r(bAe," \u2014 "),eG=n(bAe,"A",{href:!0});var Hnt=s(eG);Snr=r(Hnt,"TFXLMForSequenceClassification"),Hnt.forEach(t),Pnr=r(bAe," (XLM model)"),bAe.forEach(t),$nr=i(W),JT=n(W,"LI",{});var vAe=s(JT);rhe=n(vAe,"STRONG",{});var Unt=s(rhe);Inr=r(Unt,"xlm-roberta"),Unt.forEach(t),jnr=r(vAe," \u2014 "),oG=n(vAe,"A",{href:!0});var Jnt=s(oG);Nnr=r(Jnt,"TFXLMRobertaForSequenceClassification"),Jnt.forEach(t),Dnr=r(vAe," (XLM-RoBERTa model)"),vAe.forEach(t),qnr=i(W),YT=n(W,"LI",{});var TAe=s(YT);the=n(TAe,"STRONG",{});var Ynt=s(the);Gnr=r(Ynt,"xlnet"),Ynt.forEach(t),Onr=r(TAe," \u2014 "),rG=n(TAe,"A",{href:!0});var Knt=s(rG);Xnr=r(Knt,"TFXLNetForSequenceClassification"),Knt.forEach(t),znr=r(TAe," (XLNet model)"),TAe.forEach(t),W.forEach(t),Vnr=i(_a),ahe=n(_a,"P",{});var Znt=s(ahe);Wnr=r(Znt,"Examples:"),Znt.forEach(t),Qnr=i(_a),m(yw.$$.fragment,_a),_a.forEach(t),Gl.forEach(t),L9e=i(d),Cc=n(d,"H2",{class:!0});var jke=s(Cc);KT=n(jke,"A",{id:!0,class:!0,href:!0});var est=s(KT);nhe=n(est,"SPAN",{});var ost=s(nhe);m(ww.$$.fragment,ost),ost.forEach(t),est.forEach(t),Hnr=i(jke),she=n(jke,"SPAN",{});var rst=s(she);Unr=r(rst,"TFAutoModelForMultipleChoice"),rst.forEach(t),jke.forEach(t),B9e=i(d),Tr=n(d,"DIV",{class:!0});var Xl=s(Tr);m(Aw.$$.fragment,Xl),Jnr=i(Xl),Mc=n(Xl,"P",{});var tV=s(Mc);Ynr=r(tV,`This is a generic model class that will be instantiated as one of the model classes of the library (with a multiple choice head) when created
with the `),lhe=n(tV,"CODE",{});var tst=s(lhe);Knr=r(tst,"from_pretrained()"),tst.forEach(t),Znr=r(tV,"class method or the "),ihe=n(tV,"CODE",{});var ast=s(ihe);esr=r(ast,"from_config()"),ast.forEach(t),osr=r(tV,`class
method.`),tV.forEach(t),rsr=i(Xl),Lw=n(Xl,"P",{});var Nke=s(Lw);tsr=r(Nke,"This class cannot be instantiated directly using "),dhe=n(Nke,"CODE",{});var nst=s(dhe);asr=r(nst,"__init__()"),nst.forEach(t),nsr=r(Nke," (throws an error)."),Nke.forEach(t),ssr=i(Xl),gt=n(Xl,"DIV",{class:!0});var zl=s(gt);m(Bw.$$.fragment,zl),lsr=i(zl),che=n(zl,"P",{});var sst=s(che);isr=r(sst,"Instantiates one of the model classes of the library (with a multiple choice head) from a configuration."),sst.forEach(t),dsr=i(zl),Ec=n(zl,"P",{});var aV=s(Ec);csr=r(aV,`Note:
Loading a model from its configuration file does `),fhe=n(aV,"STRONG",{});var lst=s(fhe);fsr=r(lst,"not"),lst.forEach(t),msr=r(aV,` load the model weights. It only affects the
model\u2019s configuration. Use `),mhe=n(aV,"CODE",{});var ist=s(mhe);gsr=r(ist,"from_pretrained()"),ist.forEach(t),hsr=r(aV,"to load the model weights."),aV.forEach(t),psr=i(zl),ghe=n(zl,"P",{});var dst=s(ghe);_sr=r(dst,"Examples:"),dst.forEach(t),usr=i(zl),m(kw.$$.fragment,zl),zl.forEach(t),bsr=i(Xl),To=n(Xl,"DIV",{class:!0});var ua=s(To);m(xw.$$.fragment,ua),vsr=i(ua),hhe=n(ua,"P",{});var cst=s(hhe);Tsr=r(cst,"Instantiate one of the model classes of the library (with a multiple choice head) from a pretrained model."),cst.forEach(t),Fsr=i(ua),pn=n(ua,"P",{});var N4=s(pn);Csr=r(N4,"The model class to instantiate is selected based on the "),phe=n(N4,"CODE",{});var fst=s(phe);Msr=r(fst,"model_type"),fst.forEach(t),Esr=r(N4,` property of the config object (either
passed as an argument or loaded from `),_he=n(N4,"CODE",{});var mst=s(_he);ysr=r(mst,"pretrained_model_name_or_path"),mst.forEach(t),wsr=r(N4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),uhe=n(N4,"CODE",{});var gst=s(uhe);Asr=r(gst,"pretrained_model_name_or_path"),gst.forEach(t),Lsr=r(N4,":"),N4.forEach(t),Bsr=i(ua),te=n(ua,"UL",{});var ne=s(te);ZT=n(ne,"LI",{});var FAe=s(ZT);bhe=n(FAe,"STRONG",{});var hst=s(bhe);ksr=r(hst,"albert"),hst.forEach(t),xsr=r(FAe," \u2014 "),tG=n(FAe,"A",{href:!0});var pst=s(tG);Rsr=r(pst,"TFAlbertForMultipleChoice"),pst.forEach(t),Ssr=r(FAe," (ALBERT model)"),FAe.forEach(t),Psr=i(ne),e8=n(ne,"LI",{});var CAe=s(e8);vhe=n(CAe,"STRONG",{});var _st=s(vhe);$sr=r(_st,"bert"),_st.forEach(t),Isr=r(CAe," \u2014 "),aG=n(CAe,"A",{href:!0});var ust=s(aG);jsr=r(ust,"TFBertForMultipleChoice"),ust.forEach(t),Nsr=r(CAe," (BERT model)"),CAe.forEach(t),Dsr=i(ne),o8=n(ne,"LI",{});var MAe=s(o8);The=n(MAe,"STRONG",{});var bst=s(The);qsr=r(bst,"camembert"),bst.forEach(t),Gsr=r(MAe," \u2014 "),nG=n(MAe,"A",{href:!0});var vst=s(nG);Osr=r(vst,"TFCamembertForMultipleChoice"),vst.forEach(t),Xsr=r(MAe," (CamemBERT model)"),MAe.forEach(t),zsr=i(ne),r8=n(ne,"LI",{});var EAe=s(r8);Fhe=n(EAe,"STRONG",{});var Tst=s(Fhe);Vsr=r(Tst,"convbert"),Tst.forEach(t),Wsr=r(EAe," \u2014 "),sG=n(EAe,"A",{href:!0});var Fst=s(sG);Qsr=r(Fst,"TFConvBertForMultipleChoice"),Fst.forEach(t),Hsr=r(EAe," (ConvBERT model)"),EAe.forEach(t),Usr=i(ne),t8=n(ne,"LI",{});var yAe=s(t8);Che=n(yAe,"STRONG",{});var Cst=s(Che);Jsr=r(Cst,"distilbert"),Cst.forEach(t),Ysr=r(yAe," \u2014 "),lG=n(yAe,"A",{href:!0});var Mst=s(lG);Ksr=r(Mst,"TFDistilBertForMultipleChoice"),Mst.forEach(t),Zsr=r(yAe," (DistilBERT model)"),yAe.forEach(t),elr=i(ne),a8=n(ne,"LI",{});var wAe=s(a8);Mhe=n(wAe,"STRONG",{});var Est=s(Mhe);olr=r(Est,"electra"),Est.forEach(t),rlr=r(wAe," \u2014 "),iG=n(wAe,"A",{href:!0});var yst=s(iG);tlr=r(yst,"TFElectraForMultipleChoice"),yst.forEach(t),alr=r(wAe," (ELECTRA model)"),wAe.forEach(t),nlr=i(ne),n8=n(ne,"LI",{});var AAe=s(n8);Ehe=n(AAe,"STRONG",{});var wst=s(Ehe);slr=r(wst,"flaubert"),wst.forEach(t),llr=r(AAe," \u2014 "),dG=n(AAe,"A",{href:!0});var Ast=s(dG);ilr=r(Ast,"TFFlaubertForMultipleChoice"),Ast.forEach(t),dlr=r(AAe," (FlauBERT model)"),AAe.forEach(t),clr=i(ne),s8=n(ne,"LI",{});var LAe=s(s8);yhe=n(LAe,"STRONG",{});var Lst=s(yhe);flr=r(Lst,"funnel"),Lst.forEach(t),mlr=r(LAe," \u2014 "),cG=n(LAe,"A",{href:!0});var Bst=s(cG);glr=r(Bst,"TFFunnelForMultipleChoice"),Bst.forEach(t),hlr=r(LAe," (Funnel Transformer model)"),LAe.forEach(t),plr=i(ne),l8=n(ne,"LI",{});var BAe=s(l8);whe=n(BAe,"STRONG",{});var kst=s(whe);_lr=r(kst,"longformer"),kst.forEach(t),ulr=r(BAe," \u2014 "),fG=n(BAe,"A",{href:!0});var xst=s(fG);blr=r(xst,"TFLongformerForMultipleChoice"),xst.forEach(t),vlr=r(BAe," (Longformer model)"),BAe.forEach(t),Tlr=i(ne),i8=n(ne,"LI",{});var kAe=s(i8);Ahe=n(kAe,"STRONG",{});var Rst=s(Ahe);Flr=r(Rst,"mobilebert"),Rst.forEach(t),Clr=r(kAe," \u2014 "),mG=n(kAe,"A",{href:!0});var Sst=s(mG);Mlr=r(Sst,"TFMobileBertForMultipleChoice"),Sst.forEach(t),Elr=r(kAe," (MobileBERT model)"),kAe.forEach(t),ylr=i(ne),d8=n(ne,"LI",{});var xAe=s(d8);Lhe=n(xAe,"STRONG",{});var Pst=s(Lhe);wlr=r(Pst,"mpnet"),Pst.forEach(t),Alr=r(xAe," \u2014 "),gG=n(xAe,"A",{href:!0});var $st=s(gG);Llr=r($st,"TFMPNetForMultipleChoice"),$st.forEach(t),Blr=r(xAe," (MPNet model)"),xAe.forEach(t),klr=i(ne),c8=n(ne,"LI",{});var RAe=s(c8);Bhe=n(RAe,"STRONG",{});var Ist=s(Bhe);xlr=r(Ist,"rembert"),Ist.forEach(t),Rlr=r(RAe," \u2014 "),hG=n(RAe,"A",{href:!0});var jst=s(hG);Slr=r(jst,"TFRemBertForMultipleChoice"),jst.forEach(t),Plr=r(RAe," (RemBERT model)"),RAe.forEach(t),$lr=i(ne),f8=n(ne,"LI",{});var SAe=s(f8);khe=n(SAe,"STRONG",{});var Nst=s(khe);Ilr=r(Nst,"roberta"),Nst.forEach(t),jlr=r(SAe," \u2014 "),pG=n(SAe,"A",{href:!0});var Dst=s(pG);Nlr=r(Dst,"TFRobertaForMultipleChoice"),Dst.forEach(t),Dlr=r(SAe," (RoBERTa model)"),SAe.forEach(t),qlr=i(ne),m8=n(ne,"LI",{});var PAe=s(m8);xhe=n(PAe,"STRONG",{});var qst=s(xhe);Glr=r(qst,"roformer"),qst.forEach(t),Olr=r(PAe," \u2014 "),_G=n(PAe,"A",{href:!0});var Gst=s(_G);Xlr=r(Gst,"TFRoFormerForMultipleChoice"),Gst.forEach(t),zlr=r(PAe," (RoFormer model)"),PAe.forEach(t),Vlr=i(ne),g8=n(ne,"LI",{});var $Ae=s(g8);Rhe=n($Ae,"STRONG",{});var Ost=s(Rhe);Wlr=r(Ost,"xlm"),Ost.forEach(t),Qlr=r($Ae," \u2014 "),uG=n($Ae,"A",{href:!0});var Xst=s(uG);Hlr=r(Xst,"TFXLMForMultipleChoice"),Xst.forEach(t),Ulr=r($Ae," (XLM model)"),$Ae.forEach(t),Jlr=i(ne),h8=n(ne,"LI",{});var IAe=s(h8);She=n(IAe,"STRONG",{});var zst=s(She);Ylr=r(zst,"xlm-roberta"),zst.forEach(t),Klr=r(IAe," \u2014 "),bG=n(IAe,"A",{href:!0});var Vst=s(bG);Zlr=r(Vst,"TFXLMRobertaForMultipleChoice"),Vst.forEach(t),eir=r(IAe," (XLM-RoBERTa model)"),IAe.forEach(t),oir=i(ne),p8=n(ne,"LI",{});var jAe=s(p8);Phe=n(jAe,"STRONG",{});var Wst=s(Phe);rir=r(Wst,"xlnet"),Wst.forEach(t),tir=r(jAe," \u2014 "),vG=n(jAe,"A",{href:!0});var Qst=s(vG);air=r(Qst,"TFXLNetForMultipleChoice"),Qst.forEach(t),nir=r(jAe," (XLNet model)"),jAe.forEach(t),ne.forEach(t),sir=i(ua),$he=n(ua,"P",{});var Hst=s($he);lir=r(Hst,"Examples:"),Hst.forEach(t),iir=i(ua),m(Rw.$$.fragment,ua),ua.forEach(t),Xl.forEach(t),k9e=i(d),yc=n(d,"H2",{class:!0});var Dke=s(yc);_8=n(Dke,"A",{id:!0,class:!0,href:!0});var Ust=s(_8);Ihe=n(Ust,"SPAN",{});var Jst=s(Ihe);m(Sw.$$.fragment,Jst),Jst.forEach(t),Ust.forEach(t),dir=i(Dke),jhe=n(Dke,"SPAN",{});var Yst=s(jhe);cir=r(Yst,"TFAutoModelForTableQuestionAnswering"),Yst.forEach(t),Dke.forEach(t),x9e=i(d),Fr=n(d,"DIV",{class:!0});var Vl=s(Fr);m(Pw.$$.fragment,Vl),fir=i(Vl),wc=n(Vl,"P",{});var nV=s(wc);mir=r(nV,`This is a generic model class that will be instantiated as one of the model classes of the library (with a table question answering head) when created
with the `),Nhe=n(nV,"CODE",{});var Kst=s(Nhe);gir=r(Kst,"from_pretrained()"),Kst.forEach(t),hir=r(nV,"class method or the "),Dhe=n(nV,"CODE",{});var Zst=s(Dhe);pir=r(Zst,"from_config()"),Zst.forEach(t),_ir=r(nV,`class
method.`),nV.forEach(t),uir=i(Vl),$w=n(Vl,"P",{});var qke=s($w);bir=r(qke,"This class cannot be instantiated directly using "),qhe=n(qke,"CODE",{});var elt=s(qhe);vir=r(elt,"__init__()"),elt.forEach(t),Tir=r(qke," (throws an error)."),qke.forEach(t),Fir=i(Vl),ht=n(Vl,"DIV",{class:!0});var Wl=s(ht);m(Iw.$$.fragment,Wl),Cir=i(Wl),Ghe=n(Wl,"P",{});var olt=s(Ghe);Mir=r(olt,"Instantiates one of the model classes of the library (with a table question answering head) from a configuration."),olt.forEach(t),Eir=i(Wl),Ac=n(Wl,"P",{});var sV=s(Ac);yir=r(sV,`Note:
Loading a model from its configuration file does `),Ohe=n(sV,"STRONG",{});var rlt=s(Ohe);wir=r(rlt,"not"),rlt.forEach(t),Air=r(sV,` load the model weights. It only affects the
model\u2019s configuration. Use `),Xhe=n(sV,"CODE",{});var tlt=s(Xhe);Lir=r(tlt,"from_pretrained()"),tlt.forEach(t),Bir=r(sV,"to load the model weights."),sV.forEach(t),kir=i(Wl),zhe=n(Wl,"P",{});var alt=s(zhe);xir=r(alt,"Examples:"),alt.forEach(t),Rir=i(Wl),m(jw.$$.fragment,Wl),Wl.forEach(t),Sir=i(Vl),Fo=n(Vl,"DIV",{class:!0});var ba=s(Fo);m(Nw.$$.fragment,ba),Pir=i(ba),Vhe=n(ba,"P",{});var nlt=s(Vhe);$ir=r(nlt,"Instantiate one of the model classes of the library (with a table question answering head) from a pretrained model."),nlt.forEach(t),Iir=i(ba),_n=n(ba,"P",{});var D4=s(_n);jir=r(D4,"The model class to instantiate is selected based on the "),Whe=n(D4,"CODE",{});var slt=s(Whe);Nir=r(slt,"model_type"),slt.forEach(t),Dir=r(D4,` property of the config object (either
passed as an argument or loaded from `),Qhe=n(D4,"CODE",{});var llt=s(Qhe);qir=r(llt,"pretrained_model_name_or_path"),llt.forEach(t),Gir=r(D4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Hhe=n(D4,"CODE",{});var ilt=s(Hhe);Oir=r(ilt,"pretrained_model_name_or_path"),ilt.forEach(t),Xir=r(D4,":"),D4.forEach(t),zir=i(ba),Uhe=n(ba,"UL",{});var dlt=s(Uhe);u8=n(dlt,"LI",{});var NAe=s(u8);Jhe=n(NAe,"STRONG",{});var clt=s(Jhe);Vir=r(clt,"tapas"),clt.forEach(t),Wir=r(NAe," \u2014 "),TG=n(NAe,"A",{href:!0});var flt=s(TG);Qir=r(flt,"TFTapasForQuestionAnswering"),flt.forEach(t),Hir=r(NAe," (TAPAS model)"),NAe.forEach(t),dlt.forEach(t),Uir=i(ba),Yhe=n(ba,"P",{});var mlt=s(Yhe);Jir=r(mlt,"Examples:"),mlt.forEach(t),Yir=i(ba),m(Dw.$$.fragment,ba),ba.forEach(t),Vl.forEach(t),R9e=i(d),Lc=n(d,"H2",{class:!0});var Gke=s(Lc);b8=n(Gke,"A",{id:!0,class:!0,href:!0});var glt=s(b8);Khe=n(glt,"SPAN",{});var hlt=s(Khe);m(qw.$$.fragment,hlt),hlt.forEach(t),glt.forEach(t),Kir=i(Gke),Zhe=n(Gke,"SPAN",{});var plt=s(Zhe);Zir=r(plt,"TFAutoModelForTokenClassification"),plt.forEach(t),Gke.forEach(t),S9e=i(d),Cr=n(d,"DIV",{class:!0});var Ql=s(Cr);m(Gw.$$.fragment,Ql),edr=i(Ql),Bc=n(Ql,"P",{});var lV=s(Bc);odr=r(lV,`This is a generic model class that will be instantiated as one of the model classes of the library (with a token classification head) when created
with the `),epe=n(lV,"CODE",{});var _lt=s(epe);rdr=r(_lt,"from_pretrained()"),_lt.forEach(t),tdr=r(lV,"class method or the "),ope=n(lV,"CODE",{});var ult=s(ope);adr=r(ult,"from_config()"),ult.forEach(t),ndr=r(lV,`class
method.`),lV.forEach(t),sdr=i(Ql),Ow=n(Ql,"P",{});var Oke=s(Ow);ldr=r(Oke,"This class cannot be instantiated directly using "),rpe=n(Oke,"CODE",{});var blt=s(rpe);idr=r(blt,"__init__()"),blt.forEach(t),ddr=r(Oke," (throws an error)."),Oke.forEach(t),cdr=i(Ql),pt=n(Ql,"DIV",{class:!0});var Hl=s(pt);m(Xw.$$.fragment,Hl),fdr=i(Hl),tpe=n(Hl,"P",{});var vlt=s(tpe);mdr=r(vlt,"Instantiates one of the model classes of the library (with a token classification head) from a configuration."),vlt.forEach(t),gdr=i(Hl),kc=n(Hl,"P",{});var iV=s(kc);hdr=r(iV,`Note:
Loading a model from its configuration file does `),ape=n(iV,"STRONG",{});var Tlt=s(ape);pdr=r(Tlt,"not"),Tlt.forEach(t),_dr=r(iV,` load the model weights. It only affects the
model\u2019s configuration. Use `),npe=n(iV,"CODE",{});var Flt=s(npe);udr=r(Flt,"from_pretrained()"),Flt.forEach(t),bdr=r(iV,"to load the model weights."),iV.forEach(t),vdr=i(Hl),spe=n(Hl,"P",{});var Clt=s(spe);Tdr=r(Clt,"Examples:"),Clt.forEach(t),Fdr=i(Hl),m(zw.$$.fragment,Hl),Hl.forEach(t),Cdr=i(Ql),Co=n(Ql,"DIV",{class:!0});var va=s(Co);m(Vw.$$.fragment,va),Mdr=i(va),lpe=n(va,"P",{});var Mlt=s(lpe);Edr=r(Mlt,"Instantiate one of the model classes of the library (with a token classification head) from a pretrained model."),Mlt.forEach(t),ydr=i(va),un=n(va,"P",{});var q4=s(un);wdr=r(q4,"The model class to instantiate is selected based on the "),ipe=n(q4,"CODE",{});var Elt=s(ipe);Adr=r(Elt,"model_type"),Elt.forEach(t),Ldr=r(q4,` property of the config object (either
passed as an argument or loaded from `),dpe=n(q4,"CODE",{});var ylt=s(dpe);Bdr=r(ylt,"pretrained_model_name_or_path"),ylt.forEach(t),kdr=r(q4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),cpe=n(q4,"CODE",{});var wlt=s(cpe);xdr=r(wlt,"pretrained_model_name_or_path"),wlt.forEach(t),Rdr=r(q4,":"),q4.forEach(t),Sdr=i(va),K=n(va,"UL",{});var oe=s(K);v8=n(oe,"LI",{});var DAe=s(v8);fpe=n(DAe,"STRONG",{});var Alt=s(fpe);Pdr=r(Alt,"albert"),Alt.forEach(t),$dr=r(DAe," \u2014 "),FG=n(DAe,"A",{href:!0});var Llt=s(FG);Idr=r(Llt,"TFAlbertForTokenClassification"),Llt.forEach(t),jdr=r(DAe," (ALBERT model)"),DAe.forEach(t),Ndr=i(oe),T8=n(oe,"LI",{});var qAe=s(T8);mpe=n(qAe,"STRONG",{});var Blt=s(mpe);Ddr=r(Blt,"bert"),Blt.forEach(t),qdr=r(qAe," \u2014 "),CG=n(qAe,"A",{href:!0});var klt=s(CG);Gdr=r(klt,"TFBertForTokenClassification"),klt.forEach(t),Odr=r(qAe," (BERT model)"),qAe.forEach(t),Xdr=i(oe),F8=n(oe,"LI",{});var GAe=s(F8);gpe=n(GAe,"STRONG",{});var xlt=s(gpe);zdr=r(xlt,"camembert"),xlt.forEach(t),Vdr=r(GAe," \u2014 "),MG=n(GAe,"A",{href:!0});var Rlt=s(MG);Wdr=r(Rlt,"TFCamembertForTokenClassification"),Rlt.forEach(t),Qdr=r(GAe," (CamemBERT model)"),GAe.forEach(t),Hdr=i(oe),C8=n(oe,"LI",{});var OAe=s(C8);hpe=n(OAe,"STRONG",{});var Slt=s(hpe);Udr=r(Slt,"convbert"),Slt.forEach(t),Jdr=r(OAe," \u2014 "),EG=n(OAe,"A",{href:!0});var Plt=s(EG);Ydr=r(Plt,"TFConvBertForTokenClassification"),Plt.forEach(t),Kdr=r(OAe," (ConvBERT model)"),OAe.forEach(t),Zdr=i(oe),M8=n(oe,"LI",{});var XAe=s(M8);ppe=n(XAe,"STRONG",{});var $lt=s(ppe);ecr=r($lt,"deberta"),$lt.forEach(t),ocr=r(XAe," \u2014 "),yG=n(XAe,"A",{href:!0});var Ilt=s(yG);rcr=r(Ilt,"TFDebertaForTokenClassification"),Ilt.forEach(t),tcr=r(XAe," (DeBERTa model)"),XAe.forEach(t),acr=i(oe),E8=n(oe,"LI",{});var zAe=s(E8);_pe=n(zAe,"STRONG",{});var jlt=s(_pe);ncr=r(jlt,"deberta-v2"),jlt.forEach(t),scr=r(zAe," \u2014 "),wG=n(zAe,"A",{href:!0});var Nlt=s(wG);lcr=r(Nlt,"TFDebertaV2ForTokenClassification"),Nlt.forEach(t),icr=r(zAe," (DeBERTa-v2 model)"),zAe.forEach(t),dcr=i(oe),y8=n(oe,"LI",{});var VAe=s(y8);upe=n(VAe,"STRONG",{});var Dlt=s(upe);ccr=r(Dlt,"distilbert"),Dlt.forEach(t),fcr=r(VAe," \u2014 "),AG=n(VAe,"A",{href:!0});var qlt=s(AG);mcr=r(qlt,"TFDistilBertForTokenClassification"),qlt.forEach(t),gcr=r(VAe," (DistilBERT model)"),VAe.forEach(t),hcr=i(oe),w8=n(oe,"LI",{});var WAe=s(w8);bpe=n(WAe,"STRONG",{});var Glt=s(bpe);pcr=r(Glt,"electra"),Glt.forEach(t),_cr=r(WAe," \u2014 "),LG=n(WAe,"A",{href:!0});var Olt=s(LG);ucr=r(Olt,"TFElectraForTokenClassification"),Olt.forEach(t),bcr=r(WAe," (ELECTRA model)"),WAe.forEach(t),vcr=i(oe),A8=n(oe,"LI",{});var QAe=s(A8);vpe=n(QAe,"STRONG",{});var Xlt=s(vpe);Tcr=r(Xlt,"flaubert"),Xlt.forEach(t),Fcr=r(QAe," \u2014 "),BG=n(QAe,"A",{href:!0});var zlt=s(BG);Ccr=r(zlt,"TFFlaubertForTokenClassification"),zlt.forEach(t),Mcr=r(QAe," (FlauBERT model)"),QAe.forEach(t),Ecr=i(oe),L8=n(oe,"LI",{});var HAe=s(L8);Tpe=n(HAe,"STRONG",{});var Vlt=s(Tpe);ycr=r(Vlt,"funnel"),Vlt.forEach(t),wcr=r(HAe," \u2014 "),kG=n(HAe,"A",{href:!0});var Wlt=s(kG);Acr=r(Wlt,"TFFunnelForTokenClassification"),Wlt.forEach(t),Lcr=r(HAe," (Funnel Transformer model)"),HAe.forEach(t),Bcr=i(oe),B8=n(oe,"LI",{});var UAe=s(B8);Fpe=n(UAe,"STRONG",{});var Qlt=s(Fpe);kcr=r(Qlt,"layoutlm"),Qlt.forEach(t),xcr=r(UAe," \u2014 "),xG=n(UAe,"A",{href:!0});var Hlt=s(xG);Rcr=r(Hlt,"TFLayoutLMForTokenClassification"),Hlt.forEach(t),Scr=r(UAe," (LayoutLM model)"),UAe.forEach(t),Pcr=i(oe),k8=n(oe,"LI",{});var JAe=s(k8);Cpe=n(JAe,"STRONG",{});var Ult=s(Cpe);$cr=r(Ult,"longformer"),Ult.forEach(t),Icr=r(JAe," \u2014 "),RG=n(JAe,"A",{href:!0});var Jlt=s(RG);jcr=r(Jlt,"TFLongformerForTokenClassification"),Jlt.forEach(t),Ncr=r(JAe," (Longformer model)"),JAe.forEach(t),Dcr=i(oe),x8=n(oe,"LI",{});var YAe=s(x8);Mpe=n(YAe,"STRONG",{});var Ylt=s(Mpe);qcr=r(Ylt,"mobilebert"),Ylt.forEach(t),Gcr=r(YAe," \u2014 "),SG=n(YAe,"A",{href:!0});var Klt=s(SG);Ocr=r(Klt,"TFMobileBertForTokenClassification"),Klt.forEach(t),Xcr=r(YAe," (MobileBERT model)"),YAe.forEach(t),zcr=i(oe),R8=n(oe,"LI",{});var KAe=s(R8);Epe=n(KAe,"STRONG",{});var Zlt=s(Epe);Vcr=r(Zlt,"mpnet"),Zlt.forEach(t),Wcr=r(KAe," \u2014 "),PG=n(KAe,"A",{href:!0});var eit=s(PG);Qcr=r(eit,"TFMPNetForTokenClassification"),eit.forEach(t),Hcr=r(KAe," (MPNet model)"),KAe.forEach(t),Ucr=i(oe),S8=n(oe,"LI",{});var ZAe=s(S8);ype=n(ZAe,"STRONG",{});var oit=s(ype);Jcr=r(oit,"rembert"),oit.forEach(t),Ycr=r(ZAe," \u2014 "),$G=n(ZAe,"A",{href:!0});var rit=s($G);Kcr=r(rit,"TFRemBertForTokenClassification"),rit.forEach(t),Zcr=r(ZAe," (RemBERT model)"),ZAe.forEach(t),efr=i(oe),P8=n(oe,"LI",{});var e0e=s(P8);wpe=n(e0e,"STRONG",{});var tit=s(wpe);ofr=r(tit,"roberta"),tit.forEach(t),rfr=r(e0e," \u2014 "),IG=n(e0e,"A",{href:!0});var ait=s(IG);tfr=r(ait,"TFRobertaForTokenClassification"),ait.forEach(t),afr=r(e0e," (RoBERTa model)"),e0e.forEach(t),nfr=i(oe),$8=n(oe,"LI",{});var o0e=s($8);Ape=n(o0e,"STRONG",{});var nit=s(Ape);sfr=r(nit,"roformer"),nit.forEach(t),lfr=r(o0e," \u2014 "),jG=n(o0e,"A",{href:!0});var sit=s(jG);ifr=r(sit,"TFRoFormerForTokenClassification"),sit.forEach(t),dfr=r(o0e," (RoFormer model)"),o0e.forEach(t),cfr=i(oe),I8=n(oe,"LI",{});var r0e=s(I8);Lpe=n(r0e,"STRONG",{});var lit=s(Lpe);ffr=r(lit,"xlm"),lit.forEach(t),mfr=r(r0e," \u2014 "),NG=n(r0e,"A",{href:!0});var iit=s(NG);gfr=r(iit,"TFXLMForTokenClassification"),iit.forEach(t),hfr=r(r0e," (XLM model)"),r0e.forEach(t),pfr=i(oe),j8=n(oe,"LI",{});var t0e=s(j8);Bpe=n(t0e,"STRONG",{});var dit=s(Bpe);_fr=r(dit,"xlm-roberta"),dit.forEach(t),ufr=r(t0e," \u2014 "),DG=n(t0e,"A",{href:!0});var cit=s(DG);bfr=r(cit,"TFXLMRobertaForTokenClassification"),cit.forEach(t),vfr=r(t0e," (XLM-RoBERTa model)"),t0e.forEach(t),Tfr=i(oe),N8=n(oe,"LI",{});var a0e=s(N8);kpe=n(a0e,"STRONG",{});var fit=s(kpe);Ffr=r(fit,"xlnet"),fit.forEach(t),Cfr=r(a0e," \u2014 "),qG=n(a0e,"A",{href:!0});var mit=s(qG);Mfr=r(mit,"TFXLNetForTokenClassification"),mit.forEach(t),Efr=r(a0e," (XLNet model)"),a0e.forEach(t),oe.forEach(t),yfr=i(va),xpe=n(va,"P",{});var git=s(xpe);wfr=r(git,"Examples:"),git.forEach(t),Afr=i(va),m(Ww.$$.fragment,va),va.forEach(t),Ql.forEach(t),P9e=i(d),xc=n(d,"H2",{class:!0});var Xke=s(xc);D8=n(Xke,"A",{id:!0,class:!0,href:!0});var hit=s(D8);Rpe=n(hit,"SPAN",{});var pit=s(Rpe);m(Qw.$$.fragment,pit),pit.forEach(t),hit.forEach(t),Lfr=i(Xke),Spe=n(Xke,"SPAN",{});var _it=s(Spe);Bfr=r(_it,"TFAutoModelForQuestionAnswering"),_it.forEach(t),Xke.forEach(t),$9e=i(d),Mr=n(d,"DIV",{class:!0});var Ul=s(Mr);m(Hw.$$.fragment,Ul),kfr=i(Ul),Rc=n(Ul,"P",{});var dV=s(Rc);xfr=r(dV,`This is a generic model class that will be instantiated as one of the model classes of the library (with a question answering head) when created
with the `),Ppe=n(dV,"CODE",{});var uit=s(Ppe);Rfr=r(uit,"from_pretrained()"),uit.forEach(t),Sfr=r(dV,"class method or the "),$pe=n(dV,"CODE",{});var bit=s($pe);Pfr=r(bit,"from_config()"),bit.forEach(t),$fr=r(dV,`class
method.`),dV.forEach(t),Ifr=i(Ul),Uw=n(Ul,"P",{});var zke=s(Uw);jfr=r(zke,"This class cannot be instantiated directly using "),Ipe=n(zke,"CODE",{});var vit=s(Ipe);Nfr=r(vit,"__init__()"),vit.forEach(t),Dfr=r(zke," (throws an error)."),zke.forEach(t),qfr=i(Ul),_t=n(Ul,"DIV",{class:!0});var Jl=s(_t);m(Jw.$$.fragment,Jl),Gfr=i(Jl),jpe=n(Jl,"P",{});var Tit=s(jpe);Ofr=r(Tit,"Instantiates one of the model classes of the library (with a question answering head) from a configuration."),Tit.forEach(t),Xfr=i(Jl),Sc=n(Jl,"P",{});var cV=s(Sc);zfr=r(cV,`Note:
Loading a model from its configuration file does `),Npe=n(cV,"STRONG",{});var Fit=s(Npe);Vfr=r(Fit,"not"),Fit.forEach(t),Wfr=r(cV,` load the model weights. It only affects the
model\u2019s configuration. Use `),Dpe=n(cV,"CODE",{});var Cit=s(Dpe);Qfr=r(Cit,"from_pretrained()"),Cit.forEach(t),Hfr=r(cV,"to load the model weights."),cV.forEach(t),Ufr=i(Jl),qpe=n(Jl,"P",{});var Mit=s(qpe);Jfr=r(Mit,"Examples:"),Mit.forEach(t),Yfr=i(Jl),m(Yw.$$.fragment,Jl),Jl.forEach(t),Kfr=i(Ul),Mo=n(Ul,"DIV",{class:!0});var Ta=s(Mo);m(Kw.$$.fragment,Ta),Zfr=i(Ta),Gpe=n(Ta,"P",{});var Eit=s(Gpe);emr=r(Eit,"Instantiate one of the model classes of the library (with a question answering head) from a pretrained model."),Eit.forEach(t),omr=i(Ta),bn=n(Ta,"P",{});var G4=s(bn);rmr=r(G4,"The model class to instantiate is selected based on the "),Ope=n(G4,"CODE",{});var yit=s(Ope);tmr=r(yit,"model_type"),yit.forEach(t),amr=r(G4,` property of the config object (either
passed as an argument or loaded from `),Xpe=n(G4,"CODE",{});var wit=s(Xpe);nmr=r(wit,"pretrained_model_name_or_path"),wit.forEach(t),smr=r(G4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),zpe=n(G4,"CODE",{});var Ait=s(zpe);lmr=r(Ait,"pretrained_model_name_or_path"),Ait.forEach(t),imr=r(G4,":"),G4.forEach(t),dmr=i(Ta),Z=n(Ta,"UL",{});var re=s(Z);q8=n(re,"LI",{});var n0e=s(q8);Vpe=n(n0e,"STRONG",{});var Lit=s(Vpe);cmr=r(Lit,"albert"),Lit.forEach(t),fmr=r(n0e," \u2014 "),GG=n(n0e,"A",{href:!0});var Bit=s(GG);mmr=r(Bit,"TFAlbertForQuestionAnswering"),Bit.forEach(t),gmr=r(n0e," (ALBERT model)"),n0e.forEach(t),hmr=i(re),G8=n(re,"LI",{});var s0e=s(G8);Wpe=n(s0e,"STRONG",{});var kit=s(Wpe);pmr=r(kit,"bert"),kit.forEach(t),_mr=r(s0e," \u2014 "),OG=n(s0e,"A",{href:!0});var xit=s(OG);umr=r(xit,"TFBertForQuestionAnswering"),xit.forEach(t),bmr=r(s0e," (BERT model)"),s0e.forEach(t),vmr=i(re),O8=n(re,"LI",{});var l0e=s(O8);Qpe=n(l0e,"STRONG",{});var Rit=s(Qpe);Tmr=r(Rit,"camembert"),Rit.forEach(t),Fmr=r(l0e," \u2014 "),XG=n(l0e,"A",{href:!0});var Sit=s(XG);Cmr=r(Sit,"TFCamembertForQuestionAnswering"),Sit.forEach(t),Mmr=r(l0e," (CamemBERT model)"),l0e.forEach(t),Emr=i(re),X8=n(re,"LI",{});var i0e=s(X8);Hpe=n(i0e,"STRONG",{});var Pit=s(Hpe);ymr=r(Pit,"convbert"),Pit.forEach(t),wmr=r(i0e," \u2014 "),zG=n(i0e,"A",{href:!0});var $it=s(zG);Amr=r($it,"TFConvBertForQuestionAnswering"),$it.forEach(t),Lmr=r(i0e," (ConvBERT model)"),i0e.forEach(t),Bmr=i(re),z8=n(re,"LI",{});var d0e=s(z8);Upe=n(d0e,"STRONG",{});var Iit=s(Upe);kmr=r(Iit,"deberta"),Iit.forEach(t),xmr=r(d0e," \u2014 "),VG=n(d0e,"A",{href:!0});var jit=s(VG);Rmr=r(jit,"TFDebertaForQuestionAnswering"),jit.forEach(t),Smr=r(d0e," (DeBERTa model)"),d0e.forEach(t),Pmr=i(re),V8=n(re,"LI",{});var c0e=s(V8);Jpe=n(c0e,"STRONG",{});var Nit=s(Jpe);$mr=r(Nit,"deberta-v2"),Nit.forEach(t),Imr=r(c0e," \u2014 "),WG=n(c0e,"A",{href:!0});var Dit=s(WG);jmr=r(Dit,"TFDebertaV2ForQuestionAnswering"),Dit.forEach(t),Nmr=r(c0e," (DeBERTa-v2 model)"),c0e.forEach(t),Dmr=i(re),W8=n(re,"LI",{});var f0e=s(W8);Ype=n(f0e,"STRONG",{});var qit=s(Ype);qmr=r(qit,"distilbert"),qit.forEach(t),Gmr=r(f0e," \u2014 "),QG=n(f0e,"A",{href:!0});var Git=s(QG);Omr=r(Git,"TFDistilBertForQuestionAnswering"),Git.forEach(t),Xmr=r(f0e," (DistilBERT model)"),f0e.forEach(t),zmr=i(re),Q8=n(re,"LI",{});var m0e=s(Q8);Kpe=n(m0e,"STRONG",{});var Oit=s(Kpe);Vmr=r(Oit,"electra"),Oit.forEach(t),Wmr=r(m0e," \u2014 "),HG=n(m0e,"A",{href:!0});var Xit=s(HG);Qmr=r(Xit,"TFElectraForQuestionAnswering"),Xit.forEach(t),Hmr=r(m0e," (ELECTRA model)"),m0e.forEach(t),Umr=i(re),H8=n(re,"LI",{});var g0e=s(H8);Zpe=n(g0e,"STRONG",{});var zit=s(Zpe);Jmr=r(zit,"flaubert"),zit.forEach(t),Ymr=r(g0e," \u2014 "),UG=n(g0e,"A",{href:!0});var Vit=s(UG);Kmr=r(Vit,"TFFlaubertForQuestionAnsweringSimple"),Vit.forEach(t),Zmr=r(g0e," (FlauBERT model)"),g0e.forEach(t),egr=i(re),U8=n(re,"LI",{});var h0e=s(U8);e_e=n(h0e,"STRONG",{});var Wit=s(e_e);ogr=r(Wit,"funnel"),Wit.forEach(t),rgr=r(h0e," \u2014 "),JG=n(h0e,"A",{href:!0});var Qit=s(JG);tgr=r(Qit,"TFFunnelForQuestionAnswering"),Qit.forEach(t),agr=r(h0e," (Funnel Transformer model)"),h0e.forEach(t),ngr=i(re),J8=n(re,"LI",{});var p0e=s(J8);o_e=n(p0e,"STRONG",{});var Hit=s(o_e);sgr=r(Hit,"longformer"),Hit.forEach(t),lgr=r(p0e," \u2014 "),YG=n(p0e,"A",{href:!0});var Uit=s(YG);igr=r(Uit,"TFLongformerForQuestionAnswering"),Uit.forEach(t),dgr=r(p0e," (Longformer model)"),p0e.forEach(t),cgr=i(re),Y8=n(re,"LI",{});var _0e=s(Y8);r_e=n(_0e,"STRONG",{});var Jit=s(r_e);fgr=r(Jit,"mobilebert"),Jit.forEach(t),mgr=r(_0e," \u2014 "),KG=n(_0e,"A",{href:!0});var Yit=s(KG);ggr=r(Yit,"TFMobileBertForQuestionAnswering"),Yit.forEach(t),hgr=r(_0e," (MobileBERT model)"),_0e.forEach(t),pgr=i(re),K8=n(re,"LI",{});var u0e=s(K8);t_e=n(u0e,"STRONG",{});var Kit=s(t_e);_gr=r(Kit,"mpnet"),Kit.forEach(t),ugr=r(u0e," \u2014 "),ZG=n(u0e,"A",{href:!0});var Zit=s(ZG);bgr=r(Zit,"TFMPNetForQuestionAnswering"),Zit.forEach(t),vgr=r(u0e," (MPNet model)"),u0e.forEach(t),Tgr=i(re),Z8=n(re,"LI",{});var b0e=s(Z8);a_e=n(b0e,"STRONG",{});var edt=s(a_e);Fgr=r(edt,"rembert"),edt.forEach(t),Cgr=r(b0e," \u2014 "),eO=n(b0e,"A",{href:!0});var odt=s(eO);Mgr=r(odt,"TFRemBertForQuestionAnswering"),odt.forEach(t),Egr=r(b0e," (RemBERT model)"),b0e.forEach(t),ygr=i(re),eF=n(re,"LI",{});var v0e=s(eF);n_e=n(v0e,"STRONG",{});var rdt=s(n_e);wgr=r(rdt,"roberta"),rdt.forEach(t),Agr=r(v0e," \u2014 "),oO=n(v0e,"A",{href:!0});var tdt=s(oO);Lgr=r(tdt,"TFRobertaForQuestionAnswering"),tdt.forEach(t),Bgr=r(v0e," (RoBERTa model)"),v0e.forEach(t),kgr=i(re),oF=n(re,"LI",{});var T0e=s(oF);s_e=n(T0e,"STRONG",{});var adt=s(s_e);xgr=r(adt,"roformer"),adt.forEach(t),Rgr=r(T0e," \u2014 "),rO=n(T0e,"A",{href:!0});var ndt=s(rO);Sgr=r(ndt,"TFRoFormerForQuestionAnswering"),ndt.forEach(t),Pgr=r(T0e," (RoFormer model)"),T0e.forEach(t),$gr=i(re),rF=n(re,"LI",{});var F0e=s(rF);l_e=n(F0e,"STRONG",{});var sdt=s(l_e);Igr=r(sdt,"xlm"),sdt.forEach(t),jgr=r(F0e," \u2014 "),tO=n(F0e,"A",{href:!0});var ldt=s(tO);Ngr=r(ldt,"TFXLMForQuestionAnsweringSimple"),ldt.forEach(t),Dgr=r(F0e," (XLM model)"),F0e.forEach(t),qgr=i(re),tF=n(re,"LI",{});var C0e=s(tF);i_e=n(C0e,"STRONG",{});var idt=s(i_e);Ggr=r(idt,"xlm-roberta"),idt.forEach(t),Ogr=r(C0e," \u2014 "),aO=n(C0e,"A",{href:!0});var ddt=s(aO);Xgr=r(ddt,"TFXLMRobertaForQuestionAnswering"),ddt.forEach(t),zgr=r(C0e," (XLM-RoBERTa model)"),C0e.forEach(t),Vgr=i(re),aF=n(re,"LI",{});var M0e=s(aF);d_e=n(M0e,"STRONG",{});var cdt=s(d_e);Wgr=r(cdt,"xlnet"),cdt.forEach(t),Qgr=r(M0e," \u2014 "),nO=n(M0e,"A",{href:!0});var fdt=s(nO);Hgr=r(fdt,"TFXLNetForQuestionAnsweringSimple"),fdt.forEach(t),Ugr=r(M0e," (XLNet model)"),M0e.forEach(t),re.forEach(t),Jgr=i(Ta),c_e=n(Ta,"P",{});var mdt=s(c_e);Ygr=r(mdt,"Examples:"),mdt.forEach(t),Kgr=i(Ta),m(Zw.$$.fragment,Ta),Ta.forEach(t),Ul.forEach(t),I9e=i(d),Pc=n(d,"H2",{class:!0});var Vke=s(Pc);nF=n(Vke,"A",{id:!0,class:!0,href:!0});var gdt=s(nF);f_e=n(gdt,"SPAN",{});var hdt=s(f_e);m(eA.$$.fragment,hdt),hdt.forEach(t),gdt.forEach(t),Zgr=i(Vke),m_e=n(Vke,"SPAN",{});var pdt=s(m_e);ehr=r(pdt,"TFAutoModelForVision2Seq"),pdt.forEach(t),Vke.forEach(t),j9e=i(d),Er=n(d,"DIV",{class:!0});var Yl=s(Er);m(oA.$$.fragment,Yl),ohr=i(Yl),$c=n(Yl,"P",{});var fV=s($c);rhr=r(fV,`This is a generic model class that will be instantiated as one of the model classes of the library (with a vision-to-text modeling head) when created
with the `),g_e=n(fV,"CODE",{});var _dt=s(g_e);thr=r(_dt,"from_pretrained()"),_dt.forEach(t),ahr=r(fV,"class method or the "),h_e=n(fV,"CODE",{});var udt=s(h_e);nhr=r(udt,"from_config()"),udt.forEach(t),shr=r(fV,`class
method.`),fV.forEach(t),lhr=i(Yl),rA=n(Yl,"P",{});var Wke=s(rA);ihr=r(Wke,"This class cannot be instantiated directly using "),p_e=n(Wke,"CODE",{});var bdt=s(p_e);dhr=r(bdt,"__init__()"),bdt.forEach(t),chr=r(Wke," (throws an error)."),Wke.forEach(t),fhr=i(Yl),ut=n(Yl,"DIV",{class:!0});var Kl=s(ut);m(tA.$$.fragment,Kl),mhr=i(Kl),__e=n(Kl,"P",{});var vdt=s(__e);ghr=r(vdt,"Instantiates one of the model classes of the library (with a vision-to-text modeling head) from a configuration."),vdt.forEach(t),hhr=i(Kl),Ic=n(Kl,"P",{});var mV=s(Ic);phr=r(mV,`Note:
Loading a model from its configuration file does `),u_e=n(mV,"STRONG",{});var Tdt=s(u_e);_hr=r(Tdt,"not"),Tdt.forEach(t),uhr=r(mV,` load the model weights. It only affects the
model\u2019s configuration. Use `),b_e=n(mV,"CODE",{});var Fdt=s(b_e);bhr=r(Fdt,"from_pretrained()"),Fdt.forEach(t),vhr=r(mV,"to load the model weights."),mV.forEach(t),Thr=i(Kl),v_e=n(Kl,"P",{});var Cdt=s(v_e);Fhr=r(Cdt,"Examples:"),Cdt.forEach(t),Chr=i(Kl),m(aA.$$.fragment,Kl),Kl.forEach(t),Mhr=i(Yl),Eo=n(Yl,"DIV",{class:!0});var Fa=s(Eo);m(nA.$$.fragment,Fa),Ehr=i(Fa),T_e=n(Fa,"P",{});var Mdt=s(T_e);yhr=r(Mdt,"Instantiate one of the model classes of the library (with a vision-to-text modeling head) from a pretrained model."),Mdt.forEach(t),whr=i(Fa),vn=n(Fa,"P",{});var O4=s(vn);Ahr=r(O4,"The model class to instantiate is selected based on the "),F_e=n(O4,"CODE",{});var Edt=s(F_e);Lhr=r(Edt,"model_type"),Edt.forEach(t),Bhr=r(O4,` property of the config object (either
passed as an argument or loaded from `),C_e=n(O4,"CODE",{});var ydt=s(C_e);khr=r(ydt,"pretrained_model_name_or_path"),ydt.forEach(t),xhr=r(O4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),M_e=n(O4,"CODE",{});var wdt=s(M_e);Rhr=r(wdt,"pretrained_model_name_or_path"),wdt.forEach(t),Shr=r(O4,":"),O4.forEach(t),Phr=i(Fa),E_e=n(Fa,"UL",{});var Adt=s(E_e);sF=n(Adt,"LI",{});var E0e=s(sF);y_e=n(E0e,"STRONG",{});var Ldt=s(y_e);$hr=r(Ldt,"vision-encoder-decoder"),Ldt.forEach(t),Ihr=r(E0e," \u2014 "),sO=n(E0e,"A",{href:!0});var Bdt=s(sO);jhr=r(Bdt,"TFVisionEncoderDecoderModel"),Bdt.forEach(t),Nhr=r(E0e," (Vision Encoder decoder model)"),E0e.forEach(t),Adt.forEach(t),Dhr=i(Fa),w_e=n(Fa,"P",{});var kdt=s(w_e);qhr=r(kdt,"Examples:"),kdt.forEach(t),Ghr=i(Fa),m(sA.$$.fragment,Fa),Fa.forEach(t),Yl.forEach(t),N9e=i(d),jc=n(d,"H2",{class:!0});var Qke=s(jc);lF=n(Qke,"A",{id:!0,class:!0,href:!0});var xdt=s(lF);A_e=n(xdt,"SPAN",{});var Rdt=s(A_e);m(lA.$$.fragment,Rdt),Rdt.forEach(t),xdt.forEach(t),Ohr=i(Qke),L_e=n(Qke,"SPAN",{});var Sdt=s(L_e);Xhr=r(Sdt,"TFAutoModelForSpeechSeq2Seq"),Sdt.forEach(t),Qke.forEach(t),D9e=i(d),yr=n(d,"DIV",{class:!0});var Zl=s(yr);m(iA.$$.fragment,Zl),zhr=i(Zl),Nc=n(Zl,"P",{});var gV=s(Nc);Vhr=r(gV,`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) when created
with the `),B_e=n(gV,"CODE",{});var Pdt=s(B_e);Whr=r(Pdt,"from_pretrained()"),Pdt.forEach(t),Qhr=r(gV,"class method or the "),k_e=n(gV,"CODE",{});var $dt=s(k_e);Hhr=r($dt,"from_config()"),$dt.forEach(t),Uhr=r(gV,`class
method.`),gV.forEach(t),Jhr=i(Zl),dA=n(Zl,"P",{});var Hke=s(dA);Yhr=r(Hke,"This class cannot be instantiated directly using "),x_e=n(Hke,"CODE",{});var Idt=s(x_e);Khr=r(Idt,"__init__()"),Idt.forEach(t),Zhr=r(Hke," (throws an error)."),Hke.forEach(t),epr=i(Zl),bt=n(Zl,"DIV",{class:!0});var ei=s(bt);m(cA.$$.fragment,ei),opr=i(ei),R_e=n(ei,"P",{});var jdt=s(R_e);rpr=r(jdt,"Instantiates one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) from a configuration."),jdt.forEach(t),tpr=i(ei),Dc=n(ei,"P",{});var hV=s(Dc);apr=r(hV,`Note:
Loading a model from its configuration file does `),S_e=n(hV,"STRONG",{});var Ndt=s(S_e);npr=r(Ndt,"not"),Ndt.forEach(t),spr=r(hV,` load the model weights. It only affects the
model\u2019s configuration. Use `),P_e=n(hV,"CODE",{});var Ddt=s(P_e);lpr=r(Ddt,"from_pretrained()"),Ddt.forEach(t),ipr=r(hV,"to load the model weights."),hV.forEach(t),dpr=i(ei),$_e=n(ei,"P",{});var qdt=s($_e);cpr=r(qdt,"Examples:"),qdt.forEach(t),fpr=i(ei),m(fA.$$.fragment,ei),ei.forEach(t),mpr=i(Zl),yo=n(Zl,"DIV",{class:!0});var Ca=s(yo);m(mA.$$.fragment,Ca),gpr=i(Ca),I_e=n(Ca,"P",{});var Gdt=s(I_e);hpr=r(Gdt,"Instantiate one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) from a pretrained model."),Gdt.forEach(t),ppr=i(Ca),Tn=n(Ca,"P",{});var X4=s(Tn);_pr=r(X4,"The model class to instantiate is selected based on the "),j_e=n(X4,"CODE",{});var Odt=s(j_e);upr=r(Odt,"model_type"),Odt.forEach(t),bpr=r(X4,` property of the config object (either
passed as an argument or loaded from `),N_e=n(X4,"CODE",{});var Xdt=s(N_e);vpr=r(Xdt,"pretrained_model_name_or_path"),Xdt.forEach(t),Tpr=r(X4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),D_e=n(X4,"CODE",{});var zdt=s(D_e);Fpr=r(zdt,"pretrained_model_name_or_path"),zdt.forEach(t),Cpr=r(X4,":"),X4.forEach(t),Mpr=i(Ca),q_e=n(Ca,"UL",{});var Vdt=s(q_e);iF=n(Vdt,"LI",{});var y0e=s(iF);G_e=n(y0e,"STRONG",{});var Wdt=s(G_e);Epr=r(Wdt,"speech_to_text"),Wdt.forEach(t),ypr=r(y0e," \u2014 "),lO=n(y0e,"A",{href:!0});var Qdt=s(lO);wpr=r(Qdt,"TFSpeech2TextForConditionalGeneration"),Qdt.forEach(t),Apr=r(y0e," (Speech2Text model)"),y0e.forEach(t),Vdt.forEach(t),Lpr=i(Ca),O_e=n(Ca,"P",{});var Hdt=s(O_e);Bpr=r(Hdt,"Examples:"),Hdt.forEach(t),kpr=i(Ca),m(gA.$$.fragment,Ca),Ca.forEach(t),Zl.forEach(t),q9e=i(d),qc=n(d,"H2",{class:!0});var Uke=s(qc);dF=n(Uke,"A",{id:!0,class:!0,href:!0});var Udt=s(dF);X_e=n(Udt,"SPAN",{});var Jdt=s(X_e);m(hA.$$.fragment,Jdt),Jdt.forEach(t),Udt.forEach(t),xpr=i(Uke),z_e=n(Uke,"SPAN",{});var Ydt=s(z_e);Rpr=r(Ydt,"FlaxAutoModel"),Ydt.forEach(t),Uke.forEach(t),G9e=i(d),wr=n(d,"DIV",{class:!0});var oi=s(wr);m(pA.$$.fragment,oi),Spr=i(oi),Gc=n(oi,"P",{});var pV=s(Gc);Ppr=r(pV,`This is a generic model class that will be instantiated as one of the base model classes of the library when created
with the `),V_e=n(pV,"CODE",{});var Kdt=s(V_e);$pr=r(Kdt,"from_pretrained()"),Kdt.forEach(t),Ipr=r(pV,"class method or the "),W_e=n(pV,"CODE",{});var Zdt=s(W_e);jpr=r(Zdt,"from_config()"),Zdt.forEach(t),Npr=r(pV,`class
method.`),pV.forEach(t),Dpr=i(oi),_A=n(oi,"P",{});var Jke=s(_A);qpr=r(Jke,"This class cannot be instantiated directly using "),Q_e=n(Jke,"CODE",{});var ect=s(Q_e);Gpr=r(ect,"__init__()"),ect.forEach(t),Opr=r(Jke," (throws an error)."),Jke.forEach(t),Xpr=i(oi),vt=n(oi,"DIV",{class:!0});var ri=s(vt);m(uA.$$.fragment,ri),zpr=i(ri),H_e=n(ri,"P",{});var oct=s(H_e);Vpr=r(oct,"Instantiates one of the base model classes of the library from a configuration."),oct.forEach(t),Wpr=i(ri),Oc=n(ri,"P",{});var _V=s(Oc);Qpr=r(_V,`Note:
Loading a model from its configuration file does `),U_e=n(_V,"STRONG",{});var rct=s(U_e);Hpr=r(rct,"not"),rct.forEach(t),Upr=r(_V,` load the model weights. It only affects the
model\u2019s configuration. Use `),J_e=n(_V,"CODE",{});var tct=s(J_e);Jpr=r(tct,"from_pretrained()"),tct.forEach(t),Ypr=r(_V,"to load the model weights."),_V.forEach(t),Kpr=i(ri),Y_e=n(ri,"P",{});var act=s(Y_e);Zpr=r(act,"Examples:"),act.forEach(t),e_r=i(ri),m(bA.$$.fragment,ri),ri.forEach(t),o_r=i(oi),wo=n(oi,"DIV",{class:!0});var Ma=s(wo);m(vA.$$.fragment,Ma),r_r=i(Ma),K_e=n(Ma,"P",{});var nct=s(K_e);t_r=r(nct,"Instantiate one of the base model classes of the library from a pretrained model."),nct.forEach(t),a_r=i(Ma),Fn=n(Ma,"P",{});var z4=s(Fn);n_r=r(z4,"The model class to instantiate is selected based on the "),Z_e=n(z4,"CODE",{});var sct=s(Z_e);s_r=r(sct,"model_type"),sct.forEach(t),l_r=r(z4,` property of the config object (either
passed as an argument or loaded from `),eue=n(z4,"CODE",{});var lct=s(eue);i_r=r(lct,"pretrained_model_name_or_path"),lct.forEach(t),d_r=r(z4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),oue=n(z4,"CODE",{});var ict=s(oue);c_r=r(ict,"pretrained_model_name_or_path"),ict.forEach(t),f_r=r(z4,":"),z4.forEach(t),m_r=i(Ma),V=n(Ma,"UL",{});var Q=s(V);cF=n(Q,"LI",{});var w0e=s(cF);rue=n(w0e,"STRONG",{});var dct=s(rue);g_r=r(dct,"albert"),dct.forEach(t),h_r=r(w0e," \u2014 "),iO=n(w0e,"A",{href:!0});var cct=s(iO);p_r=r(cct,"FlaxAlbertModel"),cct.forEach(t),__r=r(w0e," (ALBERT model)"),w0e.forEach(t),u_r=i(Q),fF=n(Q,"LI",{});var A0e=s(fF);tue=n(A0e,"STRONG",{});var fct=s(tue);b_r=r(fct,"bart"),fct.forEach(t),v_r=r(A0e," \u2014 "),dO=n(A0e,"A",{href:!0});var mct=s(dO);T_r=r(mct,"FlaxBartModel"),mct.forEach(t),F_r=r(A0e," (BART model)"),A0e.forEach(t),C_r=i(Q),mF=n(Q,"LI",{});var L0e=s(mF);aue=n(L0e,"STRONG",{});var gct=s(aue);M_r=r(gct,"beit"),gct.forEach(t),E_r=r(L0e," \u2014 "),cO=n(L0e,"A",{href:!0});var hct=s(cO);y_r=r(hct,"FlaxBeitModel"),hct.forEach(t),w_r=r(L0e," (BEiT model)"),L0e.forEach(t),A_r=i(Q),gF=n(Q,"LI",{});var B0e=s(gF);nue=n(B0e,"STRONG",{});var pct=s(nue);L_r=r(pct,"bert"),pct.forEach(t),B_r=r(B0e," \u2014 "),fO=n(B0e,"A",{href:!0});var _ct=s(fO);k_r=r(_ct,"FlaxBertModel"),_ct.forEach(t),x_r=r(B0e," (BERT model)"),B0e.forEach(t),R_r=i(Q),hF=n(Q,"LI",{});var k0e=s(hF);sue=n(k0e,"STRONG",{});var uct=s(sue);S_r=r(uct,"big_bird"),uct.forEach(t),P_r=r(k0e," \u2014 "),mO=n(k0e,"A",{href:!0});var bct=s(mO);$_r=r(bct,"FlaxBigBirdModel"),bct.forEach(t),I_r=r(k0e," (BigBird model)"),k0e.forEach(t),j_r=i(Q),pF=n(Q,"LI",{});var x0e=s(pF);lue=n(x0e,"STRONG",{});var vct=s(lue);N_r=r(vct,"blenderbot"),vct.forEach(t),D_r=r(x0e," \u2014 "),gO=n(x0e,"A",{href:!0});var Tct=s(gO);q_r=r(Tct,"FlaxBlenderbotModel"),Tct.forEach(t),G_r=r(x0e," (Blenderbot model)"),x0e.forEach(t),O_r=i(Q),_F=n(Q,"LI",{});var R0e=s(_F);iue=n(R0e,"STRONG",{});var Fct=s(iue);X_r=r(Fct,"blenderbot-small"),Fct.forEach(t),z_r=r(R0e," \u2014 "),hO=n(R0e,"A",{href:!0});var Cct=s(hO);V_r=r(Cct,"FlaxBlenderbotSmallModel"),Cct.forEach(t),W_r=r(R0e," (BlenderbotSmall model)"),R0e.forEach(t),Q_r=i(Q),uF=n(Q,"LI",{});var S0e=s(uF);due=n(S0e,"STRONG",{});var Mct=s(due);H_r=r(Mct,"clip"),Mct.forEach(t),U_r=r(S0e," \u2014 "),pO=n(S0e,"A",{href:!0});var Ect=s(pO);J_r=r(Ect,"FlaxCLIPModel"),Ect.forEach(t),Y_r=r(S0e," (CLIP model)"),S0e.forEach(t),K_r=i(Q),bF=n(Q,"LI",{});var P0e=s(bF);cue=n(P0e,"STRONG",{});var yct=s(cue);Z_r=r(yct,"distilbert"),yct.forEach(t),eur=r(P0e," \u2014 "),_O=n(P0e,"A",{href:!0});var wct=s(_O);our=r(wct,"FlaxDistilBertModel"),wct.forEach(t),rur=r(P0e," (DistilBERT model)"),P0e.forEach(t),tur=i(Q),vF=n(Q,"LI",{});var $0e=s(vF);fue=n($0e,"STRONG",{});var Act=s(fue);aur=r(Act,"electra"),Act.forEach(t),nur=r($0e," \u2014 "),uO=n($0e,"A",{href:!0});var Lct=s(uO);sur=r(Lct,"FlaxElectraModel"),Lct.forEach(t),lur=r($0e," (ELECTRA model)"),$0e.forEach(t),iur=i(Q),TF=n(Q,"LI",{});var I0e=s(TF);mue=n(I0e,"STRONG",{});var Bct=s(mue);dur=r(Bct,"gpt2"),Bct.forEach(t),cur=r(I0e," \u2014 "),bO=n(I0e,"A",{href:!0});var kct=s(bO);fur=r(kct,"FlaxGPT2Model"),kct.forEach(t),mur=r(I0e," (OpenAI GPT-2 model)"),I0e.forEach(t),gur=i(Q),FF=n(Q,"LI",{});var j0e=s(FF);gue=n(j0e,"STRONG",{});var xct=s(gue);hur=r(xct,"gpt_neo"),xct.forEach(t),pur=r(j0e," \u2014 "),vO=n(j0e,"A",{href:!0});var Rct=s(vO);_ur=r(Rct,"FlaxGPTNeoModel"),Rct.forEach(t),uur=r(j0e," (GPT Neo model)"),j0e.forEach(t),bur=i(Q),CF=n(Q,"LI",{});var N0e=s(CF);hue=n(N0e,"STRONG",{});var Sct=s(hue);vur=r(Sct,"gptj"),Sct.forEach(t),Tur=r(N0e," \u2014 "),TO=n(N0e,"A",{href:!0});var Pct=s(TO);Fur=r(Pct,"FlaxGPTJModel"),Pct.forEach(t),Cur=r(N0e," (GPT-J model)"),N0e.forEach(t),Mur=i(Q),MF=n(Q,"LI",{});var D0e=s(MF);pue=n(D0e,"STRONG",{});var $ct=s(pue);Eur=r($ct,"marian"),$ct.forEach(t),yur=r(D0e," \u2014 "),FO=n(D0e,"A",{href:!0});var Ict=s(FO);wur=r(Ict,"FlaxMarianModel"),Ict.forEach(t),Aur=r(D0e," (Marian model)"),D0e.forEach(t),Lur=i(Q),EF=n(Q,"LI",{});var q0e=s(EF);_ue=n(q0e,"STRONG",{});var jct=s(_ue);Bur=r(jct,"mbart"),jct.forEach(t),kur=r(q0e," \u2014 "),CO=n(q0e,"A",{href:!0});var Nct=s(CO);xur=r(Nct,"FlaxMBartModel"),Nct.forEach(t),Rur=r(q0e," (mBART model)"),q0e.forEach(t),Sur=i(Q),yF=n(Q,"LI",{});var G0e=s(yF);uue=n(G0e,"STRONG",{});var Dct=s(uue);Pur=r(Dct,"mt5"),Dct.forEach(t),$ur=r(G0e," \u2014 "),MO=n(G0e,"A",{href:!0});var qct=s(MO);Iur=r(qct,"FlaxMT5Model"),qct.forEach(t),jur=r(G0e," (mT5 model)"),G0e.forEach(t),Nur=i(Q),wF=n(Q,"LI",{});var O0e=s(wF);bue=n(O0e,"STRONG",{});var Gct=s(bue);Dur=r(Gct,"pegasus"),Gct.forEach(t),qur=r(O0e," \u2014 "),EO=n(O0e,"A",{href:!0});var Oct=s(EO);Gur=r(Oct,"FlaxPegasusModel"),Oct.forEach(t),Our=r(O0e," (Pegasus model)"),O0e.forEach(t),Xur=i(Q),AF=n(Q,"LI",{});var X0e=s(AF);vue=n(X0e,"STRONG",{});var Xct=s(vue);zur=r(Xct,"roberta"),Xct.forEach(t),Vur=r(X0e," \u2014 "),yO=n(X0e,"A",{href:!0});var zct=s(yO);Wur=r(zct,"FlaxRobertaModel"),zct.forEach(t),Qur=r(X0e," (RoBERTa model)"),X0e.forEach(t),Hur=i(Q),LF=n(Q,"LI",{});var z0e=s(LF);Tue=n(z0e,"STRONG",{});var Vct=s(Tue);Uur=r(Vct,"roformer"),Vct.forEach(t),Jur=r(z0e," \u2014 "),wO=n(z0e,"A",{href:!0});var Wct=s(wO);Yur=r(Wct,"FlaxRoFormerModel"),Wct.forEach(t),Kur=r(z0e," (RoFormer model)"),z0e.forEach(t),Zur=i(Q),BF=n(Q,"LI",{});var V0e=s(BF);Fue=n(V0e,"STRONG",{});var Qct=s(Fue);e2r=r(Qct,"t5"),Qct.forEach(t),o2r=r(V0e," \u2014 "),AO=n(V0e,"A",{href:!0});var Hct=s(AO);r2r=r(Hct,"FlaxT5Model"),Hct.forEach(t),t2r=r(V0e," (T5 model)"),V0e.forEach(t),a2r=i(Q),kF=n(Q,"LI",{});var W0e=s(kF);Cue=n(W0e,"STRONG",{});var Uct=s(Cue);n2r=r(Uct,"vision-text-dual-encoder"),Uct.forEach(t),s2r=r(W0e," \u2014 "),LO=n(W0e,"A",{href:!0});var Jct=s(LO);l2r=r(Jct,"FlaxVisionTextDualEncoderModel"),Jct.forEach(t),i2r=r(W0e," (VisionTextDualEncoder model)"),W0e.forEach(t),d2r=i(Q),xF=n(Q,"LI",{});var Q0e=s(xF);Mue=n(Q0e,"STRONG",{});var Yct=s(Mue);c2r=r(Yct,"vit"),Yct.forEach(t),f2r=r(Q0e," \u2014 "),BO=n(Q0e,"A",{href:!0});var Kct=s(BO);m2r=r(Kct,"FlaxViTModel"),Kct.forEach(t),g2r=r(Q0e," (ViT model)"),Q0e.forEach(t),h2r=i(Q),RF=n(Q,"LI",{});var H0e=s(RF);Eue=n(H0e,"STRONG",{});var Zct=s(Eue);p2r=r(Zct,"wav2vec2"),Zct.forEach(t),_2r=r(H0e," \u2014 "),kO=n(H0e,"A",{href:!0});var eft=s(kO);u2r=r(eft,"FlaxWav2Vec2Model"),eft.forEach(t),b2r=r(H0e," (Wav2Vec2 model)"),H0e.forEach(t),v2r=i(Q),SF=n(Q,"LI",{});var U0e=s(SF);yue=n(U0e,"STRONG",{});var oft=s(yue);T2r=r(oft,"xglm"),oft.forEach(t),F2r=r(U0e," \u2014 "),xO=n(U0e,"A",{href:!0});var rft=s(xO);C2r=r(rft,"FlaxXGLMModel"),rft.forEach(t),M2r=r(U0e," (XGLM model)"),U0e.forEach(t),Q.forEach(t),E2r=i(Ma),wue=n(Ma,"P",{});var tft=s(wue);y2r=r(tft,"Examples:"),tft.forEach(t),w2r=i(Ma),m(TA.$$.fragment,Ma),Ma.forEach(t),oi.forEach(t),O9e=i(d),Xc=n(d,"H2",{class:!0});var Yke=s(Xc);PF=n(Yke,"A",{id:!0,class:!0,href:!0});var aft=s(PF);Aue=n(aft,"SPAN",{});var nft=s(Aue);m(FA.$$.fragment,nft),nft.forEach(t),aft.forEach(t),A2r=i(Yke),Lue=n(Yke,"SPAN",{});var sft=s(Lue);L2r=r(sft,"FlaxAutoModelForCausalLM"),sft.forEach(t),Yke.forEach(t),X9e=i(d),Ar=n(d,"DIV",{class:!0});var ti=s(Ar);m(CA.$$.fragment,ti),B2r=i(ti),zc=n(ti,"P",{});var uV=s(zc);k2r=r(uV,`This is a generic model class that will be instantiated as one of the model classes of the library (with a causal language modeling head) when created
with the `),Bue=n(uV,"CODE",{});var lft=s(Bue);x2r=r(lft,"from_pretrained()"),lft.forEach(t),R2r=r(uV,"class method or the "),kue=n(uV,"CODE",{});var ift=s(kue);S2r=r(ift,"from_config()"),ift.forEach(t),P2r=r(uV,`class
method.`),uV.forEach(t),$2r=i(ti),MA=n(ti,"P",{});var Kke=s(MA);I2r=r(Kke,"This class cannot be instantiated directly using "),xue=n(Kke,"CODE",{});var dft=s(xue);j2r=r(dft,"__init__()"),dft.forEach(t),N2r=r(Kke," (throws an error)."),Kke.forEach(t),D2r=i(ti),Tt=n(ti,"DIV",{class:!0});var ai=s(Tt);m(EA.$$.fragment,ai),q2r=i(ai),Rue=n(ai,"P",{});var cft=s(Rue);G2r=r(cft,"Instantiates one of the model classes of the library (with a causal language modeling head) from a configuration."),cft.forEach(t),O2r=i(ai),Vc=n(ai,"P",{});var bV=s(Vc);X2r=r(bV,`Note:
Loading a model from its configuration file does `),Sue=n(bV,"STRONG",{});var fft=s(Sue);z2r=r(fft,"not"),fft.forEach(t),V2r=r(bV,` load the model weights. It only affects the
model\u2019s configuration. Use `),Pue=n(bV,"CODE",{});var mft=s(Pue);W2r=r(mft,"from_pretrained()"),mft.forEach(t),Q2r=r(bV,"to load the model weights."),bV.forEach(t),H2r=i(ai),$ue=n(ai,"P",{});var gft=s($ue);U2r=r(gft,"Examples:"),gft.forEach(t),J2r=i(ai),m(yA.$$.fragment,ai),ai.forEach(t),Y2r=i(ti),Ao=n(ti,"DIV",{class:!0});var Ea=s(Ao);m(wA.$$.fragment,Ea),K2r=i(Ea),Iue=n(Ea,"P",{});var hft=s(Iue);Z2r=r(hft,"Instantiate one of the model classes of the library (with a causal language modeling head) from a pretrained model."),hft.forEach(t),e1r=i(Ea),Cn=n(Ea,"P",{});var V4=s(Cn);o1r=r(V4,"The model class to instantiate is selected based on the "),jue=n(V4,"CODE",{});var pft=s(jue);r1r=r(pft,"model_type"),pft.forEach(t),t1r=r(V4,` property of the config object (either
passed as an argument or loaded from `),Nue=n(V4,"CODE",{});var _ft=s(Nue);a1r=r(_ft,"pretrained_model_name_or_path"),_ft.forEach(t),n1r=r(V4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Due=n(V4,"CODE",{});var uft=s(Due);s1r=r(uft,"pretrained_model_name_or_path"),uft.forEach(t),l1r=r(V4,":"),V4.forEach(t),i1r=i(Ea),Mn=n(Ea,"UL",{});var W4=s(Mn);$F=n(W4,"LI",{});var J0e=s($F);que=n(J0e,"STRONG",{});var bft=s(que);d1r=r(bft,"gpt2"),bft.forEach(t),c1r=r(J0e," \u2014 "),RO=n(J0e,"A",{href:!0});var vft=s(RO);f1r=r(vft,"FlaxGPT2LMHeadModel"),vft.forEach(t),m1r=r(J0e," (OpenAI GPT-2 model)"),J0e.forEach(t),g1r=i(W4),IF=n(W4,"LI",{});var Y0e=s(IF);Gue=n(Y0e,"STRONG",{});var Tft=s(Gue);h1r=r(Tft,"gpt_neo"),Tft.forEach(t),p1r=r(Y0e," \u2014 "),SO=n(Y0e,"A",{href:!0});var Fft=s(SO);_1r=r(Fft,"FlaxGPTNeoForCausalLM"),Fft.forEach(t),u1r=r(Y0e," (GPT Neo model)"),Y0e.forEach(t),b1r=i(W4),jF=n(W4,"LI",{});var K0e=s(jF);Oue=n(K0e,"STRONG",{});var Cft=s(Oue);v1r=r(Cft,"gptj"),Cft.forEach(t),T1r=r(K0e," \u2014 "),PO=n(K0e,"A",{href:!0});var Mft=s(PO);F1r=r(Mft,"FlaxGPTJForCausalLM"),Mft.forEach(t),C1r=r(K0e," (GPT-J model)"),K0e.forEach(t),M1r=i(W4),NF=n(W4,"LI",{});var Z0e=s(NF);Xue=n(Z0e,"STRONG",{});var Eft=s(Xue);E1r=r(Eft,"xglm"),Eft.forEach(t),y1r=r(Z0e," \u2014 "),$O=n(Z0e,"A",{href:!0});var yft=s($O);w1r=r(yft,"FlaxXGLMForCausalLM"),yft.forEach(t),A1r=r(Z0e," (XGLM model)"),Z0e.forEach(t),W4.forEach(t),L1r=i(Ea),zue=n(Ea,"P",{});var wft=s(zue);B1r=r(wft,"Examples:"),wft.forEach(t),k1r=i(Ea),m(AA.$$.fragment,Ea),Ea.forEach(t),ti.forEach(t),z9e=i(d),Wc=n(d,"H2",{class:!0});var Zke=s(Wc);DF=n(Zke,"A",{id:!0,class:!0,href:!0});var Aft=s(DF);Vue=n(Aft,"SPAN",{});var Lft=s(Vue);m(LA.$$.fragment,Lft),Lft.forEach(t),Aft.forEach(t),x1r=i(Zke),Wue=n(Zke,"SPAN",{});var Bft=s(Wue);R1r=r(Bft,"FlaxAutoModelForPreTraining"),Bft.forEach(t),Zke.forEach(t),V9e=i(d),Lr=n(d,"DIV",{class:!0});var ni=s(Lr);m(BA.$$.fragment,ni),S1r=i(ni),Qc=n(ni,"P",{});var vV=s(Qc);P1r=r(vV,`This is a generic model class that will be instantiated as one of the model classes of the library (with a pretraining head) when created
with the `),Que=n(vV,"CODE",{});var kft=s(Que);$1r=r(kft,"from_pretrained()"),kft.forEach(t),I1r=r(vV,"class method or the "),Hue=n(vV,"CODE",{});var xft=s(Hue);j1r=r(xft,"from_config()"),xft.forEach(t),N1r=r(vV,`class
method.`),vV.forEach(t),D1r=i(ni),kA=n(ni,"P",{});var exe=s(kA);q1r=r(exe,"This class cannot be instantiated directly using "),Uue=n(exe,"CODE",{});var Rft=s(Uue);G1r=r(Rft,"__init__()"),Rft.forEach(t),O1r=r(exe," (throws an error)."),exe.forEach(t),X1r=i(ni),Ft=n(ni,"DIV",{class:!0});var si=s(Ft);m(xA.$$.fragment,si),z1r=i(si),Jue=n(si,"P",{});var Sft=s(Jue);V1r=r(Sft,"Instantiates one of the model classes of the library (with a pretraining head) from a configuration."),Sft.forEach(t),W1r=i(si),Hc=n(si,"P",{});var TV=s(Hc);Q1r=r(TV,`Note:
Loading a model from its configuration file does `),Yue=n(TV,"STRONG",{});var Pft=s(Yue);H1r=r(Pft,"not"),Pft.forEach(t),U1r=r(TV,` load the model weights. It only affects the
model\u2019s configuration. Use `),Kue=n(TV,"CODE",{});var $ft=s(Kue);J1r=r($ft,"from_pretrained()"),$ft.forEach(t),Y1r=r(TV,"to load the model weights."),TV.forEach(t),K1r=i(si),Zue=n(si,"P",{});var Ift=s(Zue);Z1r=r(Ift,"Examples:"),Ift.forEach(t),ebr=i(si),m(RA.$$.fragment,si),si.forEach(t),obr=i(ni),Lo=n(ni,"DIV",{class:!0});var ya=s(Lo);m(SA.$$.fragment,ya),rbr=i(ya),e2e=n(ya,"P",{});var jft=s(e2e);tbr=r(jft,"Instantiate one of the model classes of the library (with a pretraining head) from a pretrained model."),jft.forEach(t),abr=i(ya),En=n(ya,"P",{});var Q4=s(En);nbr=r(Q4,"The model class to instantiate is selected based on the "),o2e=n(Q4,"CODE",{});var Nft=s(o2e);sbr=r(Nft,"model_type"),Nft.forEach(t),lbr=r(Q4,` property of the config object (either
passed as an argument or loaded from `),r2e=n(Q4,"CODE",{});var Dft=s(r2e);ibr=r(Dft,"pretrained_model_name_or_path"),Dft.forEach(t),dbr=r(Q4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),t2e=n(Q4,"CODE",{});var qft=s(t2e);cbr=r(qft,"pretrained_model_name_or_path"),qft.forEach(t),fbr=r(Q4,":"),Q4.forEach(t),mbr=i(ya),fe=n(ya,"UL",{});var _e=s(fe);qF=n(_e,"LI",{});var eLe=s(qF);a2e=n(eLe,"STRONG",{});var Gft=s(a2e);gbr=r(Gft,"albert"),Gft.forEach(t),hbr=r(eLe," \u2014 "),IO=n(eLe,"A",{href:!0});var Oft=s(IO);pbr=r(Oft,"FlaxAlbertForPreTraining"),Oft.forEach(t),_br=r(eLe," (ALBERT model)"),eLe.forEach(t),ubr=i(_e),GF=n(_e,"LI",{});var oLe=s(GF);n2e=n(oLe,"STRONG",{});var Xft=s(n2e);bbr=r(Xft,"bart"),Xft.forEach(t),vbr=r(oLe," \u2014 "),jO=n(oLe,"A",{href:!0});var zft=s(jO);Tbr=r(zft,"FlaxBartForConditionalGeneration"),zft.forEach(t),Fbr=r(oLe," (BART model)"),oLe.forEach(t),Cbr=i(_e),OF=n(_e,"LI",{});var rLe=s(OF);s2e=n(rLe,"STRONG",{});var Vft=s(s2e);Mbr=r(Vft,"bert"),Vft.forEach(t),Ebr=r(rLe," \u2014 "),NO=n(rLe,"A",{href:!0});var Wft=s(NO);ybr=r(Wft,"FlaxBertForPreTraining"),Wft.forEach(t),wbr=r(rLe," (BERT model)"),rLe.forEach(t),Abr=i(_e),XF=n(_e,"LI",{});var tLe=s(XF);l2e=n(tLe,"STRONG",{});var Qft=s(l2e);Lbr=r(Qft,"big_bird"),Qft.forEach(t),Bbr=r(tLe," \u2014 "),DO=n(tLe,"A",{href:!0});var Hft=s(DO);kbr=r(Hft,"FlaxBigBirdForPreTraining"),Hft.forEach(t),xbr=r(tLe," (BigBird model)"),tLe.forEach(t),Rbr=i(_e),zF=n(_e,"LI",{});var aLe=s(zF);i2e=n(aLe,"STRONG",{});var Uft=s(i2e);Sbr=r(Uft,"electra"),Uft.forEach(t),Pbr=r(aLe," \u2014 "),qO=n(aLe,"A",{href:!0});var Jft=s(qO);$br=r(Jft,"FlaxElectraForPreTraining"),Jft.forEach(t),Ibr=r(aLe," (ELECTRA model)"),aLe.forEach(t),jbr=i(_e),VF=n(_e,"LI",{});var nLe=s(VF);d2e=n(nLe,"STRONG",{});var Yft=s(d2e);Nbr=r(Yft,"mbart"),Yft.forEach(t),Dbr=r(nLe," \u2014 "),GO=n(nLe,"A",{href:!0});var Kft=s(GO);qbr=r(Kft,"FlaxMBartForConditionalGeneration"),Kft.forEach(t),Gbr=r(nLe," (mBART model)"),nLe.forEach(t),Obr=i(_e),WF=n(_e,"LI",{});var sLe=s(WF);c2e=n(sLe,"STRONG",{});var Zft=s(c2e);Xbr=r(Zft,"mt5"),Zft.forEach(t),zbr=r(sLe," \u2014 "),OO=n(sLe,"A",{href:!0});var emt=s(OO);Vbr=r(emt,"FlaxMT5ForConditionalGeneration"),emt.forEach(t),Wbr=r(sLe," (mT5 model)"),sLe.forEach(t),Qbr=i(_e),QF=n(_e,"LI",{});var lLe=s(QF);f2e=n(lLe,"STRONG",{});var omt=s(f2e);Hbr=r(omt,"roberta"),omt.forEach(t),Ubr=r(lLe," \u2014 "),XO=n(lLe,"A",{href:!0});var rmt=s(XO);Jbr=r(rmt,"FlaxRobertaForMaskedLM"),rmt.forEach(t),Ybr=r(lLe," (RoBERTa model)"),lLe.forEach(t),Kbr=i(_e),HF=n(_e,"LI",{});var iLe=s(HF);m2e=n(iLe,"STRONG",{});var tmt=s(m2e);Zbr=r(tmt,"roformer"),tmt.forEach(t),e5r=r(iLe," \u2014 "),zO=n(iLe,"A",{href:!0});var amt=s(zO);o5r=r(amt,"FlaxRoFormerForMaskedLM"),amt.forEach(t),r5r=r(iLe," (RoFormer model)"),iLe.forEach(t),t5r=i(_e),UF=n(_e,"LI",{});var dLe=s(UF);g2e=n(dLe,"STRONG",{});var nmt=s(g2e);a5r=r(nmt,"t5"),nmt.forEach(t),n5r=r(dLe," \u2014 "),VO=n(dLe,"A",{href:!0});var smt=s(VO);s5r=r(smt,"FlaxT5ForConditionalGeneration"),smt.forEach(t),l5r=r(dLe," (T5 model)"),dLe.forEach(t),i5r=i(_e),JF=n(_e,"LI",{});var cLe=s(JF);h2e=n(cLe,"STRONG",{});var lmt=s(h2e);d5r=r(lmt,"wav2vec2"),lmt.forEach(t),c5r=r(cLe," \u2014 "),WO=n(cLe,"A",{href:!0});var imt=s(WO);f5r=r(imt,"FlaxWav2Vec2ForPreTraining"),imt.forEach(t),m5r=r(cLe," (Wav2Vec2 model)"),cLe.forEach(t),_e.forEach(t),g5r=i(ya),p2e=n(ya,"P",{});var dmt=s(p2e);h5r=r(dmt,"Examples:"),dmt.forEach(t),p5r=i(ya),m(PA.$$.fragment,ya),ya.forEach(t),ni.forEach(t),W9e=i(d),Uc=n(d,"H2",{class:!0});var oxe=s(Uc);YF=n(oxe,"A",{id:!0,class:!0,href:!0});var cmt=s(YF);_2e=n(cmt,"SPAN",{});var fmt=s(_2e);m($A.$$.fragment,fmt),fmt.forEach(t),cmt.forEach(t),_5r=i(oxe),u2e=n(oxe,"SPAN",{});var mmt=s(u2e);u5r=r(mmt,"FlaxAutoModelForMaskedLM"),mmt.forEach(t),oxe.forEach(t),Q9e=i(d),Br=n(d,"DIV",{class:!0});var li=s(Br);m(IA.$$.fragment,li),b5r=i(li),Jc=n(li,"P",{});var FV=s(Jc);v5r=r(FV,`This is a generic model class that will be instantiated as one of the model classes of the library (with a masked language modeling head) when created
with the `),b2e=n(FV,"CODE",{});var gmt=s(b2e);T5r=r(gmt,"from_pretrained()"),gmt.forEach(t),F5r=r(FV,"class method or the "),v2e=n(FV,"CODE",{});var hmt=s(v2e);C5r=r(hmt,"from_config()"),hmt.forEach(t),M5r=r(FV,`class
method.`),FV.forEach(t),E5r=i(li),jA=n(li,"P",{});var rxe=s(jA);y5r=r(rxe,"This class cannot be instantiated directly using "),T2e=n(rxe,"CODE",{});var pmt=s(T2e);w5r=r(pmt,"__init__()"),pmt.forEach(t),A5r=r(rxe," (throws an error)."),rxe.forEach(t),L5r=i(li),Ct=n(li,"DIV",{class:!0});var ii=s(Ct);m(NA.$$.fragment,ii),B5r=i(ii),F2e=n(ii,"P",{});var _mt=s(F2e);k5r=r(_mt,"Instantiates one of the model classes of the library (with a masked language modeling head) from a configuration."),_mt.forEach(t),x5r=i(ii),Yc=n(ii,"P",{});var CV=s(Yc);R5r=r(CV,`Note:
Loading a model from its configuration file does `),C2e=n(CV,"STRONG",{});var umt=s(C2e);S5r=r(umt,"not"),umt.forEach(t),P5r=r(CV,` load the model weights. It only affects the
model\u2019s configuration. Use `),M2e=n(CV,"CODE",{});var bmt=s(M2e);$5r=r(bmt,"from_pretrained()"),bmt.forEach(t),I5r=r(CV,"to load the model weights."),CV.forEach(t),j5r=i(ii),E2e=n(ii,"P",{});var vmt=s(E2e);N5r=r(vmt,"Examples:"),vmt.forEach(t),D5r=i(ii),m(DA.$$.fragment,ii),ii.forEach(t),q5r=i(li),Bo=n(li,"DIV",{class:!0});var wa=s(Bo);m(qA.$$.fragment,wa),G5r=i(wa),y2e=n(wa,"P",{});var Tmt=s(y2e);O5r=r(Tmt,"Instantiate one of the model classes of the library (with a masked language modeling head) from a pretrained model."),Tmt.forEach(t),X5r=i(wa),yn=n(wa,"P",{});var H4=s(yn);z5r=r(H4,"The model class to instantiate is selected based on the "),w2e=n(H4,"CODE",{});var Fmt=s(w2e);V5r=r(Fmt,"model_type"),Fmt.forEach(t),W5r=r(H4,` property of the config object (either
passed as an argument or loaded from `),A2e=n(H4,"CODE",{});var Cmt=s(A2e);Q5r=r(Cmt,"pretrained_model_name_or_path"),Cmt.forEach(t),H5r=r(H4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),L2e=n(H4,"CODE",{});var Mmt=s(L2e);U5r=r(Mmt,"pretrained_model_name_or_path"),Mmt.forEach(t),J5r=r(H4,":"),H4.forEach(t),Y5r=i(wa),ve=n(wa,"UL",{});var Ze=s(ve);KF=n(Ze,"LI",{});var fLe=s(KF);B2e=n(fLe,"STRONG",{});var Emt=s(B2e);K5r=r(Emt,"albert"),Emt.forEach(t),Z5r=r(fLe," \u2014 "),QO=n(fLe,"A",{href:!0});var ymt=s(QO);evr=r(ymt,"FlaxAlbertForMaskedLM"),ymt.forEach(t),ovr=r(fLe," (ALBERT model)"),fLe.forEach(t),rvr=i(Ze),ZF=n(Ze,"LI",{});var mLe=s(ZF);k2e=n(mLe,"STRONG",{});var wmt=s(k2e);tvr=r(wmt,"bart"),wmt.forEach(t),avr=r(mLe," \u2014 "),HO=n(mLe,"A",{href:!0});var Amt=s(HO);nvr=r(Amt,"FlaxBartForConditionalGeneration"),Amt.forEach(t),svr=r(mLe," (BART model)"),mLe.forEach(t),lvr=i(Ze),eC=n(Ze,"LI",{});var gLe=s(eC);x2e=n(gLe,"STRONG",{});var Lmt=s(x2e);ivr=r(Lmt,"bert"),Lmt.forEach(t),dvr=r(gLe," \u2014 "),UO=n(gLe,"A",{href:!0});var Bmt=s(UO);cvr=r(Bmt,"FlaxBertForMaskedLM"),Bmt.forEach(t),fvr=r(gLe," (BERT model)"),gLe.forEach(t),mvr=i(Ze),oC=n(Ze,"LI",{});var hLe=s(oC);R2e=n(hLe,"STRONG",{});var kmt=s(R2e);gvr=r(kmt,"big_bird"),kmt.forEach(t),hvr=r(hLe," \u2014 "),JO=n(hLe,"A",{href:!0});var xmt=s(JO);pvr=r(xmt,"FlaxBigBirdForMaskedLM"),xmt.forEach(t),_vr=r(hLe," (BigBird model)"),hLe.forEach(t),uvr=i(Ze),rC=n(Ze,"LI",{});var pLe=s(rC);S2e=n(pLe,"STRONG",{});var Rmt=s(S2e);bvr=r(Rmt,"distilbert"),Rmt.forEach(t),vvr=r(pLe," \u2014 "),YO=n(pLe,"A",{href:!0});var Smt=s(YO);Tvr=r(Smt,"FlaxDistilBertForMaskedLM"),Smt.forEach(t),Fvr=r(pLe," (DistilBERT model)"),pLe.forEach(t),Cvr=i(Ze),tC=n(Ze,"LI",{});var _Le=s(tC);P2e=n(_Le,"STRONG",{});var Pmt=s(P2e);Mvr=r(Pmt,"electra"),Pmt.forEach(t),Evr=r(_Le," \u2014 "),KO=n(_Le,"A",{href:!0});var $mt=s(KO);yvr=r($mt,"FlaxElectraForMaskedLM"),$mt.forEach(t),wvr=r(_Le," (ELECTRA model)"),_Le.forEach(t),Avr=i(Ze),aC=n(Ze,"LI",{});var uLe=s(aC);$2e=n(uLe,"STRONG",{});var Imt=s($2e);Lvr=r(Imt,"mbart"),Imt.forEach(t),Bvr=r(uLe," \u2014 "),ZO=n(uLe,"A",{href:!0});var jmt=s(ZO);kvr=r(jmt,"FlaxMBartForConditionalGeneration"),jmt.forEach(t),xvr=r(uLe," (mBART model)"),uLe.forEach(t),Rvr=i(Ze),nC=n(Ze,"LI",{});var bLe=s(nC);I2e=n(bLe,"STRONG",{});var Nmt=s(I2e);Svr=r(Nmt,"roberta"),Nmt.forEach(t),Pvr=r(bLe," \u2014 "),eX=n(bLe,"A",{href:!0});var Dmt=s(eX);$vr=r(Dmt,"FlaxRobertaForMaskedLM"),Dmt.forEach(t),Ivr=r(bLe," (RoBERTa model)"),bLe.forEach(t),jvr=i(Ze),sC=n(Ze,"LI",{});var vLe=s(sC);j2e=n(vLe,"STRONG",{});var qmt=s(j2e);Nvr=r(qmt,"roformer"),qmt.forEach(t),Dvr=r(vLe," \u2014 "),oX=n(vLe,"A",{href:!0});var Gmt=s(oX);qvr=r(Gmt,"FlaxRoFormerForMaskedLM"),Gmt.forEach(t),Gvr=r(vLe," (RoFormer model)"),vLe.forEach(t),Ze.forEach(t),Ovr=i(wa),N2e=n(wa,"P",{});var Omt=s(N2e);Xvr=r(Omt,"Examples:"),Omt.forEach(t),zvr=i(wa),m(GA.$$.fragment,wa),wa.forEach(t),li.forEach(t),H9e=i(d),Kc=n(d,"H2",{class:!0});var txe=s(Kc);lC=n(txe,"A",{id:!0,class:!0,href:!0});var Xmt=s(lC);D2e=n(Xmt,"SPAN",{});var zmt=s(D2e);m(OA.$$.fragment,zmt),zmt.forEach(t),Xmt.forEach(t),Vvr=i(txe),q2e=n(txe,"SPAN",{});var Vmt=s(q2e);Wvr=r(Vmt,"FlaxAutoModelForSeq2SeqLM"),Vmt.forEach(t),txe.forEach(t),U9e=i(d),kr=n(d,"DIV",{class:!0});var di=s(kr);m(XA.$$.fragment,di),Qvr=i(di),Zc=n(di,"P",{});var MV=s(Zc);Hvr=r(MV,`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence language modeling head) when created
with the `),G2e=n(MV,"CODE",{});var Wmt=s(G2e);Uvr=r(Wmt,"from_pretrained()"),Wmt.forEach(t),Jvr=r(MV,"class method or the "),O2e=n(MV,"CODE",{});var Qmt=s(O2e);Yvr=r(Qmt,"from_config()"),Qmt.forEach(t),Kvr=r(MV,`class
method.`),MV.forEach(t),Zvr=i(di),zA=n(di,"P",{});var axe=s(zA);e6r=r(axe,"This class cannot be instantiated directly using "),X2e=n(axe,"CODE",{});var Hmt=s(X2e);o6r=r(Hmt,"__init__()"),Hmt.forEach(t),r6r=r(axe," (throws an error)."),axe.forEach(t),t6r=i(di),Mt=n(di,"DIV",{class:!0});var ci=s(Mt);m(VA.$$.fragment,ci),a6r=i(ci),z2e=n(ci,"P",{});var Umt=s(z2e);n6r=r(Umt,"Instantiates one of the model classes of the library (with a sequence-to-sequence language modeling head) from a configuration."),Umt.forEach(t),s6r=i(ci),ef=n(ci,"P",{});var EV=s(ef);l6r=r(EV,`Note:
Loading a model from its configuration file does `),V2e=n(EV,"STRONG",{});var Jmt=s(V2e);i6r=r(Jmt,"not"),Jmt.forEach(t),d6r=r(EV,` load the model weights. It only affects the
model\u2019s configuration. Use `),W2e=n(EV,"CODE",{});var Ymt=s(W2e);c6r=r(Ymt,"from_pretrained()"),Ymt.forEach(t),f6r=r(EV,"to load the model weights."),EV.forEach(t),m6r=i(ci),Q2e=n(ci,"P",{});var Kmt=s(Q2e);g6r=r(Kmt,"Examples:"),Kmt.forEach(t),h6r=i(ci),m(WA.$$.fragment,ci),ci.forEach(t),p6r=i(di),ko=n(di,"DIV",{class:!0});var Aa=s(ko);m(QA.$$.fragment,Aa),_6r=i(Aa),H2e=n(Aa,"P",{});var Zmt=s(H2e);u6r=r(Zmt,"Instantiate one of the model classes of the library (with a sequence-to-sequence language modeling head) from a pretrained model."),Zmt.forEach(t),b6r=i(Aa),wn=n(Aa,"P",{});var U4=s(wn);v6r=r(U4,"The model class to instantiate is selected based on the "),U2e=n(U4,"CODE",{});var egt=s(U2e);T6r=r(egt,"model_type"),egt.forEach(t),F6r=r(U4,` property of the config object (either
passed as an argument or loaded from `),J2e=n(U4,"CODE",{});var ogt=s(J2e);C6r=r(ogt,"pretrained_model_name_or_path"),ogt.forEach(t),M6r=r(U4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Y2e=n(U4,"CODE",{});var rgt=s(Y2e);E6r=r(rgt,"pretrained_model_name_or_path"),rgt.forEach(t),y6r=r(U4,":"),U4.forEach(t),w6r=i(Aa),Te=n(Aa,"UL",{});var eo=s(Te);iC=n(eo,"LI",{});var TLe=s(iC);K2e=n(TLe,"STRONG",{});var tgt=s(K2e);A6r=r(tgt,"bart"),tgt.forEach(t),L6r=r(TLe," \u2014 "),rX=n(TLe,"A",{href:!0});var agt=s(rX);B6r=r(agt,"FlaxBartForConditionalGeneration"),agt.forEach(t),k6r=r(TLe," (BART model)"),TLe.forEach(t),x6r=i(eo),dC=n(eo,"LI",{});var FLe=s(dC);Z2e=n(FLe,"STRONG",{});var ngt=s(Z2e);R6r=r(ngt,"blenderbot"),ngt.forEach(t),S6r=r(FLe," \u2014 "),tX=n(FLe,"A",{href:!0});var sgt=s(tX);P6r=r(sgt,"FlaxBlenderbotForConditionalGeneration"),sgt.forEach(t),$6r=r(FLe," (Blenderbot model)"),FLe.forEach(t),I6r=i(eo),cC=n(eo,"LI",{});var CLe=s(cC);e1e=n(CLe,"STRONG",{});var lgt=s(e1e);j6r=r(lgt,"blenderbot-small"),lgt.forEach(t),N6r=r(CLe," \u2014 "),aX=n(CLe,"A",{href:!0});var igt=s(aX);D6r=r(igt,"FlaxBlenderbotSmallForConditionalGeneration"),igt.forEach(t),q6r=r(CLe," (BlenderbotSmall model)"),CLe.forEach(t),G6r=i(eo),fC=n(eo,"LI",{});var MLe=s(fC);o1e=n(MLe,"STRONG",{});var dgt=s(o1e);O6r=r(dgt,"encoder-decoder"),dgt.forEach(t),X6r=r(MLe," \u2014 "),nX=n(MLe,"A",{href:!0});var cgt=s(nX);z6r=r(cgt,"FlaxEncoderDecoderModel"),cgt.forEach(t),V6r=r(MLe," (Encoder decoder model)"),MLe.forEach(t),W6r=i(eo),mC=n(eo,"LI",{});var ELe=s(mC);r1e=n(ELe,"STRONG",{});var fgt=s(r1e);Q6r=r(fgt,"marian"),fgt.forEach(t),H6r=r(ELe," \u2014 "),sX=n(ELe,"A",{href:!0});var mgt=s(sX);U6r=r(mgt,"FlaxMarianMTModel"),mgt.forEach(t),J6r=r(ELe," (Marian model)"),ELe.forEach(t),Y6r=i(eo),gC=n(eo,"LI",{});var yLe=s(gC);t1e=n(yLe,"STRONG",{});var ggt=s(t1e);K6r=r(ggt,"mbart"),ggt.forEach(t),Z6r=r(yLe," \u2014 "),lX=n(yLe,"A",{href:!0});var hgt=s(lX);eTr=r(hgt,"FlaxMBartForConditionalGeneration"),hgt.forEach(t),oTr=r(yLe," (mBART model)"),yLe.forEach(t),rTr=i(eo),hC=n(eo,"LI",{});var wLe=s(hC);a1e=n(wLe,"STRONG",{});var pgt=s(a1e);tTr=r(pgt,"mt5"),pgt.forEach(t),aTr=r(wLe," \u2014 "),iX=n(wLe,"A",{href:!0});var _gt=s(iX);nTr=r(_gt,"FlaxMT5ForConditionalGeneration"),_gt.forEach(t),sTr=r(wLe," (mT5 model)"),wLe.forEach(t),lTr=i(eo),pC=n(eo,"LI",{});var ALe=s(pC);n1e=n(ALe,"STRONG",{});var ugt=s(n1e);iTr=r(ugt,"pegasus"),ugt.forEach(t),dTr=r(ALe," \u2014 "),dX=n(ALe,"A",{href:!0});var bgt=s(dX);cTr=r(bgt,"FlaxPegasusForConditionalGeneration"),bgt.forEach(t),fTr=r(ALe," (Pegasus model)"),ALe.forEach(t),mTr=i(eo),_C=n(eo,"LI",{});var LLe=s(_C);s1e=n(LLe,"STRONG",{});var vgt=s(s1e);gTr=r(vgt,"t5"),vgt.forEach(t),hTr=r(LLe," \u2014 "),cX=n(LLe,"A",{href:!0});var Tgt=s(cX);pTr=r(Tgt,"FlaxT5ForConditionalGeneration"),Tgt.forEach(t),_Tr=r(LLe," (T5 model)"),LLe.forEach(t),eo.forEach(t),uTr=i(Aa),l1e=n(Aa,"P",{});var Fgt=s(l1e);bTr=r(Fgt,"Examples:"),Fgt.forEach(t),vTr=i(Aa),m(HA.$$.fragment,Aa),Aa.forEach(t),di.forEach(t),J9e=i(d),of=n(d,"H2",{class:!0});var nxe=s(of);uC=n(nxe,"A",{id:!0,class:!0,href:!0});var Cgt=s(uC);i1e=n(Cgt,"SPAN",{});var Mgt=s(i1e);m(UA.$$.fragment,Mgt),Mgt.forEach(t),Cgt.forEach(t),TTr=i(nxe),d1e=n(nxe,"SPAN",{});var Egt=s(d1e);FTr=r(Egt,"FlaxAutoModelForSequenceClassification"),Egt.forEach(t),nxe.forEach(t),Y9e=i(d),xr=n(d,"DIV",{class:!0});var fi=s(xr);m(JA.$$.fragment,fi),CTr=i(fi),rf=n(fi,"P",{});var yV=s(rf);MTr=r(yV,`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence classification head) when created
with the `),c1e=n(yV,"CODE",{});var ygt=s(c1e);ETr=r(ygt,"from_pretrained()"),ygt.forEach(t),yTr=r(yV,"class method or the "),f1e=n(yV,"CODE",{});var wgt=s(f1e);wTr=r(wgt,"from_config()"),wgt.forEach(t),ATr=r(yV,`class
method.`),yV.forEach(t),LTr=i(fi),YA=n(fi,"P",{});var sxe=s(YA);BTr=r(sxe,"This class cannot be instantiated directly using "),m1e=n(sxe,"CODE",{});var Agt=s(m1e);kTr=r(Agt,"__init__()"),Agt.forEach(t),xTr=r(sxe," (throws an error)."),sxe.forEach(t),RTr=i(fi),Et=n(fi,"DIV",{class:!0});var mi=s(Et);m(KA.$$.fragment,mi),STr=i(mi),g1e=n(mi,"P",{});var Lgt=s(g1e);PTr=r(Lgt,"Instantiates one of the model classes of the library (with a sequence classification head) from a configuration."),Lgt.forEach(t),$Tr=i(mi),tf=n(mi,"P",{});var wV=s(tf);ITr=r(wV,`Note:
Loading a model from its configuration file does `),h1e=n(wV,"STRONG",{});var Bgt=s(h1e);jTr=r(Bgt,"not"),Bgt.forEach(t),NTr=r(wV,` load the model weights. It only affects the
model\u2019s configuration. Use `),p1e=n(wV,"CODE",{});var kgt=s(p1e);DTr=r(kgt,"from_pretrained()"),kgt.forEach(t),qTr=r(wV,"to load the model weights."),wV.forEach(t),GTr=i(mi),_1e=n(mi,"P",{});var xgt=s(_1e);OTr=r(xgt,"Examples:"),xgt.forEach(t),XTr=i(mi),m(ZA.$$.fragment,mi),mi.forEach(t),zTr=i(fi),xo=n(fi,"DIV",{class:!0});var La=s(xo);m(e0.$$.fragment,La),VTr=i(La),u1e=n(La,"P",{});var Rgt=s(u1e);WTr=r(Rgt,"Instantiate one of the model classes of the library (with a sequence classification head) from a pretrained model."),Rgt.forEach(t),QTr=i(La),An=n(La,"P",{});var J4=s(An);HTr=r(J4,"The model class to instantiate is selected based on the "),b1e=n(J4,"CODE",{});var Sgt=s(b1e);UTr=r(Sgt,"model_type"),Sgt.forEach(t),JTr=r(J4,` property of the config object (either
passed as an argument or loaded from `),v1e=n(J4,"CODE",{});var Pgt=s(v1e);YTr=r(Pgt,"pretrained_model_name_or_path"),Pgt.forEach(t),KTr=r(J4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),T1e=n(J4,"CODE",{});var $gt=s(T1e);ZTr=r($gt,"pretrained_model_name_or_path"),$gt.forEach(t),e8r=r(J4,":"),J4.forEach(t),o8r=i(La),Fe=n(La,"UL",{});var oo=s(Fe);bC=n(oo,"LI",{});var BLe=s(bC);F1e=n(BLe,"STRONG",{});var Igt=s(F1e);r8r=r(Igt,"albert"),Igt.forEach(t),t8r=r(BLe," \u2014 "),fX=n(BLe,"A",{href:!0});var jgt=s(fX);a8r=r(jgt,"FlaxAlbertForSequenceClassification"),jgt.forEach(t),n8r=r(BLe," (ALBERT model)"),BLe.forEach(t),s8r=i(oo),vC=n(oo,"LI",{});var kLe=s(vC);C1e=n(kLe,"STRONG",{});var Ngt=s(C1e);l8r=r(Ngt,"bart"),Ngt.forEach(t),i8r=r(kLe," \u2014 "),mX=n(kLe,"A",{href:!0});var Dgt=s(mX);d8r=r(Dgt,"FlaxBartForSequenceClassification"),Dgt.forEach(t),c8r=r(kLe," (BART model)"),kLe.forEach(t),f8r=i(oo),TC=n(oo,"LI",{});var xLe=s(TC);M1e=n(xLe,"STRONG",{});var qgt=s(M1e);m8r=r(qgt,"bert"),qgt.forEach(t),g8r=r(xLe," \u2014 "),gX=n(xLe,"A",{href:!0});var Ggt=s(gX);h8r=r(Ggt,"FlaxBertForSequenceClassification"),Ggt.forEach(t),p8r=r(xLe," (BERT model)"),xLe.forEach(t),_8r=i(oo),FC=n(oo,"LI",{});var RLe=s(FC);E1e=n(RLe,"STRONG",{});var Ogt=s(E1e);u8r=r(Ogt,"big_bird"),Ogt.forEach(t),b8r=r(RLe," \u2014 "),hX=n(RLe,"A",{href:!0});var Xgt=s(hX);v8r=r(Xgt,"FlaxBigBirdForSequenceClassification"),Xgt.forEach(t),T8r=r(RLe," (BigBird model)"),RLe.forEach(t),F8r=i(oo),CC=n(oo,"LI",{});var SLe=s(CC);y1e=n(SLe,"STRONG",{});var zgt=s(y1e);C8r=r(zgt,"distilbert"),zgt.forEach(t),M8r=r(SLe," \u2014 "),pX=n(SLe,"A",{href:!0});var Vgt=s(pX);E8r=r(Vgt,"FlaxDistilBertForSequenceClassification"),Vgt.forEach(t),y8r=r(SLe," (DistilBERT model)"),SLe.forEach(t),w8r=i(oo),MC=n(oo,"LI",{});var PLe=s(MC);w1e=n(PLe,"STRONG",{});var Wgt=s(w1e);A8r=r(Wgt,"electra"),Wgt.forEach(t),L8r=r(PLe," \u2014 "),_X=n(PLe,"A",{href:!0});var Qgt=s(_X);B8r=r(Qgt,"FlaxElectraForSequenceClassification"),Qgt.forEach(t),k8r=r(PLe," (ELECTRA model)"),PLe.forEach(t),x8r=i(oo),EC=n(oo,"LI",{});var $Le=s(EC);A1e=n($Le,"STRONG",{});var Hgt=s(A1e);R8r=r(Hgt,"mbart"),Hgt.forEach(t),S8r=r($Le," \u2014 "),uX=n($Le,"A",{href:!0});var Ugt=s(uX);P8r=r(Ugt,"FlaxMBartForSequenceClassification"),Ugt.forEach(t),$8r=r($Le," (mBART model)"),$Le.forEach(t),I8r=i(oo),yC=n(oo,"LI",{});var ILe=s(yC);L1e=n(ILe,"STRONG",{});var Jgt=s(L1e);j8r=r(Jgt,"roberta"),Jgt.forEach(t),N8r=r(ILe," \u2014 "),bX=n(ILe,"A",{href:!0});var Ygt=s(bX);D8r=r(Ygt,"FlaxRobertaForSequenceClassification"),Ygt.forEach(t),q8r=r(ILe," (RoBERTa model)"),ILe.forEach(t),G8r=i(oo),wC=n(oo,"LI",{});var jLe=s(wC);B1e=n(jLe,"STRONG",{});var Kgt=s(B1e);O8r=r(Kgt,"roformer"),Kgt.forEach(t),X8r=r(jLe," \u2014 "),vX=n(jLe,"A",{href:!0});var Zgt=s(vX);z8r=r(Zgt,"FlaxRoFormerForSequenceClassification"),Zgt.forEach(t),V8r=r(jLe," (RoFormer model)"),jLe.forEach(t),oo.forEach(t),W8r=i(La),k1e=n(La,"P",{});var eht=s(k1e);Q8r=r(eht,"Examples:"),eht.forEach(t),H8r=i(La),m(o0.$$.fragment,La),La.forEach(t),fi.forEach(t),K9e=i(d),af=n(d,"H2",{class:!0});var lxe=s(af);AC=n(lxe,"A",{id:!0,class:!0,href:!0});var oht=s(AC);x1e=n(oht,"SPAN",{});var rht=s(x1e);m(r0.$$.fragment,rht),rht.forEach(t),oht.forEach(t),U8r=i(lxe),R1e=n(lxe,"SPAN",{});var tht=s(R1e);J8r=r(tht,"FlaxAutoModelForQuestionAnswering"),tht.forEach(t),lxe.forEach(t),Z9e=i(d),Rr=n(d,"DIV",{class:!0});var gi=s(Rr);m(t0.$$.fragment,gi),Y8r=i(gi),nf=n(gi,"P",{});var AV=s(nf);K8r=r(AV,`This is a generic model class that will be instantiated as one of the model classes of the library (with a question answering head) when created
with the `),S1e=n(AV,"CODE",{});var aht=s(S1e);Z8r=r(aht,"from_pretrained()"),aht.forEach(t),eFr=r(AV,"class method or the "),P1e=n(AV,"CODE",{});var nht=s(P1e);oFr=r(nht,"from_config()"),nht.forEach(t),rFr=r(AV,`class
method.`),AV.forEach(t),tFr=i(gi),a0=n(gi,"P",{});var ixe=s(a0);aFr=r(ixe,"This class cannot be instantiated directly using "),$1e=n(ixe,"CODE",{});var sht=s($1e);nFr=r(sht,"__init__()"),sht.forEach(t),sFr=r(ixe," (throws an error)."),ixe.forEach(t),lFr=i(gi),yt=n(gi,"DIV",{class:!0});var hi=s(yt);m(n0.$$.fragment,hi),iFr=i(hi),I1e=n(hi,"P",{});var lht=s(I1e);dFr=r(lht,"Instantiates one of the model classes of the library (with a question answering head) from a configuration."),lht.forEach(t),cFr=i(hi),sf=n(hi,"P",{});var LV=s(sf);fFr=r(LV,`Note:
Loading a model from its configuration file does `),j1e=n(LV,"STRONG",{});var iht=s(j1e);mFr=r(iht,"not"),iht.forEach(t),gFr=r(LV,` load the model weights. It only affects the
model\u2019s configuration. Use `),N1e=n(LV,"CODE",{});var dht=s(N1e);hFr=r(dht,"from_pretrained()"),dht.forEach(t),pFr=r(LV,"to load the model weights."),LV.forEach(t),_Fr=i(hi),D1e=n(hi,"P",{});var cht=s(D1e);uFr=r(cht,"Examples:"),cht.forEach(t),bFr=i(hi),m(s0.$$.fragment,hi),hi.forEach(t),vFr=i(gi),Ro=n(gi,"DIV",{class:!0});var Ba=s(Ro);m(l0.$$.fragment,Ba),TFr=i(Ba),q1e=n(Ba,"P",{});var fht=s(q1e);FFr=r(fht,"Instantiate one of the model classes of the library (with a question answering head) from a pretrained model."),fht.forEach(t),CFr=i(Ba),Ln=n(Ba,"P",{});var Y4=s(Ln);MFr=r(Y4,"The model class to instantiate is selected based on the "),G1e=n(Y4,"CODE",{});var mht=s(G1e);EFr=r(mht,"model_type"),mht.forEach(t),yFr=r(Y4,` property of the config object (either
passed as an argument or loaded from `),O1e=n(Y4,"CODE",{});var ght=s(O1e);wFr=r(ght,"pretrained_model_name_or_path"),ght.forEach(t),AFr=r(Y4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),X1e=n(Y4,"CODE",{});var hht=s(X1e);LFr=r(hht,"pretrained_model_name_or_path"),hht.forEach(t),BFr=r(Y4,":"),Y4.forEach(t),kFr=i(Ba),Ce=n(Ba,"UL",{});var ro=s(Ce);LC=n(ro,"LI",{});var NLe=s(LC);z1e=n(NLe,"STRONG",{});var pht=s(z1e);xFr=r(pht,"albert"),pht.forEach(t),RFr=r(NLe," \u2014 "),TX=n(NLe,"A",{href:!0});var _ht=s(TX);SFr=r(_ht,"FlaxAlbertForQuestionAnswering"),_ht.forEach(t),PFr=r(NLe," (ALBERT model)"),NLe.forEach(t),$Fr=i(ro),BC=n(ro,"LI",{});var DLe=s(BC);V1e=n(DLe,"STRONG",{});var uht=s(V1e);IFr=r(uht,"bart"),uht.forEach(t),jFr=r(DLe," \u2014 "),FX=n(DLe,"A",{href:!0});var bht=s(FX);NFr=r(bht,"FlaxBartForQuestionAnswering"),bht.forEach(t),DFr=r(DLe," (BART model)"),DLe.forEach(t),qFr=i(ro),kC=n(ro,"LI",{});var qLe=s(kC);W1e=n(qLe,"STRONG",{});var vht=s(W1e);GFr=r(vht,"bert"),vht.forEach(t),OFr=r(qLe," \u2014 "),CX=n(qLe,"A",{href:!0});var Tht=s(CX);XFr=r(Tht,"FlaxBertForQuestionAnswering"),Tht.forEach(t),zFr=r(qLe," (BERT model)"),qLe.forEach(t),VFr=i(ro),xC=n(ro,"LI",{});var GLe=s(xC);Q1e=n(GLe,"STRONG",{});var Fht=s(Q1e);WFr=r(Fht,"big_bird"),Fht.forEach(t),QFr=r(GLe," \u2014 "),MX=n(GLe,"A",{href:!0});var Cht=s(MX);HFr=r(Cht,"FlaxBigBirdForQuestionAnswering"),Cht.forEach(t),UFr=r(GLe," (BigBird model)"),GLe.forEach(t),JFr=i(ro),RC=n(ro,"LI",{});var OLe=s(RC);H1e=n(OLe,"STRONG",{});var Mht=s(H1e);YFr=r(Mht,"distilbert"),Mht.forEach(t),KFr=r(OLe," \u2014 "),EX=n(OLe,"A",{href:!0});var Eht=s(EX);ZFr=r(Eht,"FlaxDistilBertForQuestionAnswering"),Eht.forEach(t),eCr=r(OLe," (DistilBERT model)"),OLe.forEach(t),oCr=i(ro),SC=n(ro,"LI",{});var XLe=s(SC);U1e=n(XLe,"STRONG",{});var yht=s(U1e);rCr=r(yht,"electra"),yht.forEach(t),tCr=r(XLe," \u2014 "),yX=n(XLe,"A",{href:!0});var wht=s(yX);aCr=r(wht,"FlaxElectraForQuestionAnswering"),wht.forEach(t),nCr=r(XLe," (ELECTRA model)"),XLe.forEach(t),sCr=i(ro),PC=n(ro,"LI",{});var zLe=s(PC);J1e=n(zLe,"STRONG",{});var Aht=s(J1e);lCr=r(Aht,"mbart"),Aht.forEach(t),iCr=r(zLe," \u2014 "),wX=n(zLe,"A",{href:!0});var Lht=s(wX);dCr=r(Lht,"FlaxMBartForQuestionAnswering"),Lht.forEach(t),cCr=r(zLe," (mBART model)"),zLe.forEach(t),fCr=i(ro),$C=n(ro,"LI",{});var VLe=s($C);Y1e=n(VLe,"STRONG",{});var Bht=s(Y1e);mCr=r(Bht,"roberta"),Bht.forEach(t),gCr=r(VLe," \u2014 "),AX=n(VLe,"A",{href:!0});var kht=s(AX);hCr=r(kht,"FlaxRobertaForQuestionAnswering"),kht.forEach(t),pCr=r(VLe," (RoBERTa model)"),VLe.forEach(t),_Cr=i(ro),IC=n(ro,"LI",{});var WLe=s(IC);K1e=n(WLe,"STRONG",{});var xht=s(K1e);uCr=r(xht,"roformer"),xht.forEach(t),bCr=r(WLe," \u2014 "),LX=n(WLe,"A",{href:!0});var Rht=s(LX);vCr=r(Rht,"FlaxRoFormerForQuestionAnswering"),Rht.forEach(t),TCr=r(WLe," (RoFormer model)"),WLe.forEach(t),ro.forEach(t),FCr=i(Ba),Z1e=n(Ba,"P",{});var Sht=s(Z1e);CCr=r(Sht,"Examples:"),Sht.forEach(t),MCr=i(Ba),m(i0.$$.fragment,Ba),Ba.forEach(t),gi.forEach(t),eBe=i(d),lf=n(d,"H2",{class:!0});var dxe=s(lf);jC=n(dxe,"A",{id:!0,class:!0,href:!0});var Pht=s(jC);ebe=n(Pht,"SPAN",{});var $ht=s(ebe);m(d0.$$.fragment,$ht),$ht.forEach(t),Pht.forEach(t),ECr=i(dxe),obe=n(dxe,"SPAN",{});var Iht=s(obe);yCr=r(Iht,"FlaxAutoModelForTokenClassification"),Iht.forEach(t),dxe.forEach(t),oBe=i(d),Sr=n(d,"DIV",{class:!0});var pi=s(Sr);m(c0.$$.fragment,pi),wCr=i(pi),df=n(pi,"P",{});var BV=s(df);ACr=r(BV,`This is a generic model class that will be instantiated as one of the model classes of the library (with a token classification head) when created
with the `),rbe=n(BV,"CODE",{});var jht=s(rbe);LCr=r(jht,"from_pretrained()"),jht.forEach(t),BCr=r(BV,"class method or the "),tbe=n(BV,"CODE",{});var Nht=s(tbe);kCr=r(Nht,"from_config()"),Nht.forEach(t),xCr=r(BV,`class
method.`),BV.forEach(t),RCr=i(pi),f0=n(pi,"P",{});var cxe=s(f0);SCr=r(cxe,"This class cannot be instantiated directly using "),abe=n(cxe,"CODE",{});var Dht=s(abe);PCr=r(Dht,"__init__()"),Dht.forEach(t),$Cr=r(cxe," (throws an error)."),cxe.forEach(t),ICr=i(pi),wt=n(pi,"DIV",{class:!0});var _i=s(wt);m(m0.$$.fragment,_i),jCr=i(_i),nbe=n(_i,"P",{});var qht=s(nbe);NCr=r(qht,"Instantiates one of the model classes of the library (with a token classification head) from a configuration."),qht.forEach(t),DCr=i(_i),cf=n(_i,"P",{});var kV=s(cf);qCr=r(kV,`Note:
Loading a model from its configuration file does `),sbe=n(kV,"STRONG",{});var Ght=s(sbe);GCr=r(Ght,"not"),Ght.forEach(t),OCr=r(kV,` load the model weights. It only affects the
model\u2019s configuration. Use `),lbe=n(kV,"CODE",{});var Oht=s(lbe);XCr=r(Oht,"from_pretrained()"),Oht.forEach(t),zCr=r(kV,"to load the model weights."),kV.forEach(t),VCr=i(_i),ibe=n(_i,"P",{});var Xht=s(ibe);WCr=r(Xht,"Examples:"),Xht.forEach(t),QCr=i(_i),m(g0.$$.fragment,_i),_i.forEach(t),HCr=i(pi),So=n(pi,"DIV",{class:!0});var ka=s(So);m(h0.$$.fragment,ka),UCr=i(ka),dbe=n(ka,"P",{});var zht=s(dbe);JCr=r(zht,"Instantiate one of the model classes of the library (with a token classification head) from a pretrained model."),zht.forEach(t),YCr=i(ka),Bn=n(ka,"P",{});var K4=s(Bn);KCr=r(K4,"The model class to instantiate is selected based on the "),cbe=n(K4,"CODE",{});var Vht=s(cbe);ZCr=r(Vht,"model_type"),Vht.forEach(t),e4r=r(K4,` property of the config object (either
passed as an argument or loaded from `),fbe=n(K4,"CODE",{});var Wht=s(fbe);o4r=r(Wht,"pretrained_model_name_or_path"),Wht.forEach(t),r4r=r(K4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),mbe=n(K4,"CODE",{});var Qht=s(mbe);t4r=r(Qht,"pretrained_model_name_or_path"),Qht.forEach(t),a4r=r(K4,":"),K4.forEach(t),n4r=i(ka),so=n(ka,"UL",{});var ta=s(so);NC=n(ta,"LI",{});var QLe=s(NC);gbe=n(QLe,"STRONG",{});var Hht=s(gbe);s4r=r(Hht,"albert"),Hht.forEach(t),l4r=r(QLe," \u2014 "),BX=n(QLe,"A",{href:!0});var Uht=s(BX);i4r=r(Uht,"FlaxAlbertForTokenClassification"),Uht.forEach(t),d4r=r(QLe," (ALBERT model)"),QLe.forEach(t),c4r=i(ta),DC=n(ta,"LI",{});var HLe=s(DC);hbe=n(HLe,"STRONG",{});var Jht=s(hbe);f4r=r(Jht,"bert"),Jht.forEach(t),m4r=r(HLe," \u2014 "),kX=n(HLe,"A",{href:!0});var Yht=s(kX);g4r=r(Yht,"FlaxBertForTokenClassification"),Yht.forEach(t),h4r=r(HLe," (BERT model)"),HLe.forEach(t),p4r=i(ta),qC=n(ta,"LI",{});var ULe=s(qC);pbe=n(ULe,"STRONG",{});var Kht=s(pbe);_4r=r(Kht,"big_bird"),Kht.forEach(t),u4r=r(ULe," \u2014 "),xX=n(ULe,"A",{href:!0});var Zht=s(xX);b4r=r(Zht,"FlaxBigBirdForTokenClassification"),Zht.forEach(t),v4r=r(ULe," (BigBird model)"),ULe.forEach(t),T4r=i(ta),GC=n(ta,"LI",{});var JLe=s(GC);_be=n(JLe,"STRONG",{});var ept=s(_be);F4r=r(ept,"distilbert"),ept.forEach(t),C4r=r(JLe," \u2014 "),RX=n(JLe,"A",{href:!0});var opt=s(RX);M4r=r(opt,"FlaxDistilBertForTokenClassification"),opt.forEach(t),E4r=r(JLe," (DistilBERT model)"),JLe.forEach(t),y4r=i(ta),OC=n(ta,"LI",{});var YLe=s(OC);ube=n(YLe,"STRONG",{});var rpt=s(ube);w4r=r(rpt,"electra"),rpt.forEach(t),A4r=r(YLe," \u2014 "),SX=n(YLe,"A",{href:!0});var tpt=s(SX);L4r=r(tpt,"FlaxElectraForTokenClassification"),tpt.forEach(t),B4r=r(YLe," (ELECTRA model)"),YLe.forEach(t),k4r=i(ta),XC=n(ta,"LI",{});var KLe=s(XC);bbe=n(KLe,"STRONG",{});var apt=s(bbe);x4r=r(apt,"roberta"),apt.forEach(t),R4r=r(KLe," \u2014 "),PX=n(KLe,"A",{href:!0});var npt=s(PX);S4r=r(npt,"FlaxRobertaForTokenClassification"),npt.forEach(t),P4r=r(KLe," (RoBERTa model)"),KLe.forEach(t),$4r=i(ta),zC=n(ta,"LI",{});var ZLe=s(zC);vbe=n(ZLe,"STRONG",{});var spt=s(vbe);I4r=r(spt,"roformer"),spt.forEach(t),j4r=r(ZLe," \u2014 "),$X=n(ZLe,"A",{href:!0});var lpt=s($X);N4r=r(lpt,"FlaxRoFormerForTokenClassification"),lpt.forEach(t),D4r=r(ZLe," (RoFormer model)"),ZLe.forEach(t),ta.forEach(t),q4r=i(ka),Tbe=n(ka,"P",{});var ipt=s(Tbe);G4r=r(ipt,"Examples:"),ipt.forEach(t),O4r=i(ka),m(p0.$$.fragment,ka),ka.forEach(t),pi.forEach(t),rBe=i(d),ff=n(d,"H2",{class:!0});var fxe=s(ff);VC=n(fxe,"A",{id:!0,class:!0,href:!0});var dpt=s(VC);Fbe=n(dpt,"SPAN",{});var cpt=s(Fbe);m(_0.$$.fragment,cpt),cpt.forEach(t),dpt.forEach(t),X4r=i(fxe),Cbe=n(fxe,"SPAN",{});var fpt=s(Cbe);z4r=r(fpt,"FlaxAutoModelForMultipleChoice"),fpt.forEach(t),fxe.forEach(t),tBe=i(d),Pr=n(d,"DIV",{class:!0});var ui=s(Pr);m(u0.$$.fragment,ui),V4r=i(ui),mf=n(ui,"P",{});var xV=s(mf);W4r=r(xV,`This is a generic model class that will be instantiated as one of the model classes of the library (with a multiple choice head) when created
with the `),Mbe=n(xV,"CODE",{});var mpt=s(Mbe);Q4r=r(mpt,"from_pretrained()"),mpt.forEach(t),H4r=r(xV,"class method or the "),Ebe=n(xV,"CODE",{});var gpt=s(Ebe);U4r=r(gpt,"from_config()"),gpt.forEach(t),J4r=r(xV,`class
method.`),xV.forEach(t),Y4r=i(ui),b0=n(ui,"P",{});var mxe=s(b0);K4r=r(mxe,"This class cannot be instantiated directly using "),ybe=n(mxe,"CODE",{});var hpt=s(ybe);Z4r=r(hpt,"__init__()"),hpt.forEach(t),eMr=r(mxe," (throws an error)."),mxe.forEach(t),oMr=i(ui),At=n(ui,"DIV",{class:!0});var bi=s(At);m(v0.$$.fragment,bi),rMr=i(bi),wbe=n(bi,"P",{});var ppt=s(wbe);tMr=r(ppt,"Instantiates one of the model classes of the library (with a multiple choice head) from a configuration."),ppt.forEach(t),aMr=i(bi),gf=n(bi,"P",{});var RV=s(gf);nMr=r(RV,`Note:
Loading a model from its configuration file does `),Abe=n(RV,"STRONG",{});var _pt=s(Abe);sMr=r(_pt,"not"),_pt.forEach(t),lMr=r(RV,` load the model weights. It only affects the
model\u2019s configuration. Use `),Lbe=n(RV,"CODE",{});var upt=s(Lbe);iMr=r(upt,"from_pretrained()"),upt.forEach(t),dMr=r(RV,"to load the model weights."),RV.forEach(t),cMr=i(bi),Bbe=n(bi,"P",{});var bpt=s(Bbe);fMr=r(bpt,"Examples:"),bpt.forEach(t),mMr=i(bi),m(T0.$$.fragment,bi),bi.forEach(t),gMr=i(ui),Po=n(ui,"DIV",{class:!0});var xa=s(Po);m(F0.$$.fragment,xa),hMr=i(xa),kbe=n(xa,"P",{});var vpt=s(kbe);pMr=r(vpt,"Instantiate one of the model classes of the library (with a multiple choice head) from a pretrained model."),vpt.forEach(t),_Mr=i(xa),kn=n(xa,"P",{});var Z4=s(kn);uMr=r(Z4,"The model class to instantiate is selected based on the "),xbe=n(Z4,"CODE",{});var Tpt=s(xbe);bMr=r(Tpt,"model_type"),Tpt.forEach(t),vMr=r(Z4,` property of the config object (either
passed as an argument or loaded from `),Rbe=n(Z4,"CODE",{});var Fpt=s(Rbe);TMr=r(Fpt,"pretrained_model_name_or_path"),Fpt.forEach(t),FMr=r(Z4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Sbe=n(Z4,"CODE",{});var Cpt=s(Sbe);CMr=r(Cpt,"pretrained_model_name_or_path"),Cpt.forEach(t),MMr=r(Z4,":"),Z4.forEach(t),EMr=i(xa),lo=n(xa,"UL",{});var aa=s(lo);WC=n(aa,"LI",{});var e7e=s(WC);Pbe=n(e7e,"STRONG",{});var Mpt=s(Pbe);yMr=r(Mpt,"albert"),Mpt.forEach(t),wMr=r(e7e," \u2014 "),IX=n(e7e,"A",{href:!0});var Ept=s(IX);AMr=r(Ept,"FlaxAlbertForMultipleChoice"),Ept.forEach(t),LMr=r(e7e," (ALBERT model)"),e7e.forEach(t),BMr=i(aa),QC=n(aa,"LI",{});var o7e=s(QC);$be=n(o7e,"STRONG",{});var ypt=s($be);kMr=r(ypt,"bert"),ypt.forEach(t),xMr=r(o7e," \u2014 "),jX=n(o7e,"A",{href:!0});var wpt=s(jX);RMr=r(wpt,"FlaxBertForMultipleChoice"),wpt.forEach(t),SMr=r(o7e," (BERT model)"),o7e.forEach(t),PMr=i(aa),HC=n(aa,"LI",{});var r7e=s(HC);Ibe=n(r7e,"STRONG",{});var Apt=s(Ibe);$Mr=r(Apt,"big_bird"),Apt.forEach(t),IMr=r(r7e," \u2014 "),NX=n(r7e,"A",{href:!0});var Lpt=s(NX);jMr=r(Lpt,"FlaxBigBirdForMultipleChoice"),Lpt.forEach(t),NMr=r(r7e," (BigBird model)"),r7e.forEach(t),DMr=i(aa),UC=n(aa,"LI",{});var t7e=s(UC);jbe=n(t7e,"STRONG",{});var Bpt=s(jbe);qMr=r(Bpt,"distilbert"),Bpt.forEach(t),GMr=r(t7e," \u2014 "),DX=n(t7e,"A",{href:!0});var kpt=s(DX);OMr=r(kpt,"FlaxDistilBertForMultipleChoice"),kpt.forEach(t),XMr=r(t7e," (DistilBERT model)"),t7e.forEach(t),zMr=i(aa),JC=n(aa,"LI",{});var a7e=s(JC);Nbe=n(a7e,"STRONG",{});var xpt=s(Nbe);VMr=r(xpt,"electra"),xpt.forEach(t),WMr=r(a7e," \u2014 "),qX=n(a7e,"A",{href:!0});var Rpt=s(qX);QMr=r(Rpt,"FlaxElectraForMultipleChoice"),Rpt.forEach(t),HMr=r(a7e," (ELECTRA model)"),a7e.forEach(t),UMr=i(aa),YC=n(aa,"LI",{});var n7e=s(YC);Dbe=n(n7e,"STRONG",{});var Spt=s(Dbe);JMr=r(Spt,"roberta"),Spt.forEach(t),YMr=r(n7e," \u2014 "),GX=n(n7e,"A",{href:!0});var Ppt=s(GX);KMr=r(Ppt,"FlaxRobertaForMultipleChoice"),Ppt.forEach(t),ZMr=r(n7e," (RoBERTa model)"),n7e.forEach(t),eEr=i(aa),KC=n(aa,"LI",{});var s7e=s(KC);qbe=n(s7e,"STRONG",{});var $pt=s(qbe);oEr=r($pt,"roformer"),$pt.forEach(t),rEr=r(s7e," \u2014 "),OX=n(s7e,"A",{href:!0});var Ipt=s(OX);tEr=r(Ipt,"FlaxRoFormerForMultipleChoice"),Ipt.forEach(t),aEr=r(s7e," (RoFormer model)"),s7e.forEach(t),aa.forEach(t),nEr=i(xa),Gbe=n(xa,"P",{});var jpt=s(Gbe);sEr=r(jpt,"Examples:"),jpt.forEach(t),lEr=i(xa),m(C0.$$.fragment,xa),xa.forEach(t),ui.forEach(t),aBe=i(d),hf=n(d,"H2",{class:!0});var gxe=s(hf);ZC=n(gxe,"A",{id:!0,class:!0,href:!0});var Npt=s(ZC);Obe=n(Npt,"SPAN",{});var Dpt=s(Obe);m(M0.$$.fragment,Dpt),Dpt.forEach(t),Npt.forEach(t),iEr=i(gxe),Xbe=n(gxe,"SPAN",{});var qpt=s(Xbe);dEr=r(qpt,"FlaxAutoModelForNextSentencePrediction"),qpt.forEach(t),gxe.forEach(t),nBe=i(d),$r=n(d,"DIV",{class:!0});var vi=s($r);m(E0.$$.fragment,vi),cEr=i(vi),pf=n(vi,"P",{});var SV=s(pf);fEr=r(SV,`This is a generic model class that will be instantiated as one of the model classes of the library (with a next sentence prediction head) when created
with the `),zbe=n(SV,"CODE",{});var Gpt=s(zbe);mEr=r(Gpt,"from_pretrained()"),Gpt.forEach(t),gEr=r(SV,"class method or the "),Vbe=n(SV,"CODE",{});var Opt=s(Vbe);hEr=r(Opt,"from_config()"),Opt.forEach(t),pEr=r(SV,`class
method.`),SV.forEach(t),_Er=i(vi),y0=n(vi,"P",{});var hxe=s(y0);uEr=r(hxe,"This class cannot be instantiated directly using "),Wbe=n(hxe,"CODE",{});var Xpt=s(Wbe);bEr=r(Xpt,"__init__()"),Xpt.forEach(t),vEr=r(hxe," (throws an error)."),hxe.forEach(t),TEr=i(vi),Lt=n(vi,"DIV",{class:!0});var Ti=s(Lt);m(w0.$$.fragment,Ti),FEr=i(Ti),Qbe=n(Ti,"P",{});var zpt=s(Qbe);CEr=r(zpt,"Instantiates one of the model classes of the library (with a next sentence prediction head) from a configuration."),zpt.forEach(t),MEr=i(Ti),_f=n(Ti,"P",{});var PV=s(_f);EEr=r(PV,`Note:
Loading a model from its configuration file does `),Hbe=n(PV,"STRONG",{});var Vpt=s(Hbe);yEr=r(Vpt,"not"),Vpt.forEach(t),wEr=r(PV,` load the model weights. It only affects the
model\u2019s configuration. Use `),Ube=n(PV,"CODE",{});var Wpt=s(Ube);AEr=r(Wpt,"from_pretrained()"),Wpt.forEach(t),LEr=r(PV,"to load the model weights."),PV.forEach(t),BEr=i(Ti),Jbe=n(Ti,"P",{});var Qpt=s(Jbe);kEr=r(Qpt,"Examples:"),Qpt.forEach(t),xEr=i(Ti),m(A0.$$.fragment,Ti),Ti.forEach(t),REr=i(vi),$o=n(vi,"DIV",{class:!0});var Ra=s($o);m(L0.$$.fragment,Ra),SEr=i(Ra),Ybe=n(Ra,"P",{});var Hpt=s(Ybe);PEr=r(Hpt,"Instantiate one of the model classes of the library (with a next sentence prediction head) from a pretrained model."),Hpt.forEach(t),$Er=i(Ra),xn=n(Ra,"P",{});var eM=s(xn);IEr=r(eM,"The model class to instantiate is selected based on the "),Kbe=n(eM,"CODE",{});var Upt=s(Kbe);jEr=r(Upt,"model_type"),Upt.forEach(t),NEr=r(eM,` property of the config object (either
passed as an argument or loaded from `),Zbe=n(eM,"CODE",{});var Jpt=s(Zbe);DEr=r(Jpt,"pretrained_model_name_or_path"),Jpt.forEach(t),qEr=r(eM,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),e5e=n(eM,"CODE",{});var Ypt=s(e5e);GEr=r(Ypt,"pretrained_model_name_or_path"),Ypt.forEach(t),OEr=r(eM,":"),eM.forEach(t),XEr=i(Ra),o5e=n(Ra,"UL",{});var Kpt=s(o5e);e4=n(Kpt,"LI",{});var l7e=s(e4);r5e=n(l7e,"STRONG",{});var Zpt=s(r5e);zEr=r(Zpt,"bert"),Zpt.forEach(t),VEr=r(l7e," \u2014 "),XX=n(l7e,"A",{href:!0});var e_t=s(XX);WEr=r(e_t,"FlaxBertForNextSentencePrediction"),e_t.forEach(t),QEr=r(l7e," (BERT model)"),l7e.forEach(t),Kpt.forEach(t),HEr=i(Ra),t5e=n(Ra,"P",{});var o_t=s(t5e);UEr=r(o_t,"Examples:"),o_t.forEach(t),JEr=i(Ra),m(B0.$$.fragment,Ra),Ra.forEach(t),vi.forEach(t),sBe=i(d),uf=n(d,"H2",{class:!0});var pxe=s(uf);o4=n(pxe,"A",{id:!0,class:!0,href:!0});var r_t=s(o4);a5e=n(r_t,"SPAN",{});var t_t=s(a5e);m(k0.$$.fragment,t_t),t_t.forEach(t),r_t.forEach(t),YEr=i(pxe),n5e=n(pxe,"SPAN",{});var a_t=s(n5e);KEr=r(a_t,"FlaxAutoModelForImageClassification"),a_t.forEach(t),pxe.forEach(t),lBe=i(d),Ir=n(d,"DIV",{class:!0});var Fi=s(Ir);m(x0.$$.fragment,Fi),ZEr=i(Fi),bf=n(Fi,"P",{});var $V=s(bf);e3r=r($V,`This is a generic model class that will be instantiated as one of the model classes of the library (with a image classification head) when created
with the `),s5e=n($V,"CODE",{});var n_t=s(s5e);o3r=r(n_t,"from_pretrained()"),n_t.forEach(t),r3r=r($V,"class method or the "),l5e=n($V,"CODE",{});var s_t=s(l5e);t3r=r(s_t,"from_config()"),s_t.forEach(t),a3r=r($V,`class
method.`),$V.forEach(t),n3r=i(Fi),R0=n(Fi,"P",{});var _xe=s(R0);s3r=r(_xe,"This class cannot be instantiated directly using "),i5e=n(_xe,"CODE",{});var l_t=s(i5e);l3r=r(l_t,"__init__()"),l_t.forEach(t),i3r=r(_xe," (throws an error)."),_xe.forEach(t),d3r=i(Fi),Bt=n(Fi,"DIV",{class:!0});var Ci=s(Bt);m(S0.$$.fragment,Ci),c3r=i(Ci),d5e=n(Ci,"P",{});var i_t=s(d5e);f3r=r(i_t,"Instantiates one of the model classes of the library (with a image classification head) from a configuration."),i_t.forEach(t),m3r=i(Ci),vf=n(Ci,"P",{});var IV=s(vf);g3r=r(IV,`Note:
Loading a model from its configuration file does `),c5e=n(IV,"STRONG",{});var d_t=s(c5e);h3r=r(d_t,"not"),d_t.forEach(t),p3r=r(IV,` load the model weights. It only affects the
model\u2019s configuration. Use `),f5e=n(IV,"CODE",{});var c_t=s(f5e);_3r=r(c_t,"from_pretrained()"),c_t.forEach(t),u3r=r(IV,"to load the model weights."),IV.forEach(t),b3r=i(Ci),m5e=n(Ci,"P",{});var f_t=s(m5e);v3r=r(f_t,"Examples:"),f_t.forEach(t),T3r=i(Ci),m(P0.$$.fragment,Ci),Ci.forEach(t),F3r=i(Fi),Io=n(Fi,"DIV",{class:!0});var Sa=s(Io);m($0.$$.fragment,Sa),C3r=i(Sa),g5e=n(Sa,"P",{});var m_t=s(g5e);M3r=r(m_t,"Instantiate one of the model classes of the library (with a image classification head) from a pretrained model."),m_t.forEach(t),E3r=i(Sa),Rn=n(Sa,"P",{});var oM=s(Rn);y3r=r(oM,"The model class to instantiate is selected based on the "),h5e=n(oM,"CODE",{});var g_t=s(h5e);w3r=r(g_t,"model_type"),g_t.forEach(t),A3r=r(oM,` property of the config object (either
passed as an argument or loaded from `),p5e=n(oM,"CODE",{});var h_t=s(p5e);L3r=r(h_t,"pretrained_model_name_or_path"),h_t.forEach(t),B3r=r(oM,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),_5e=n(oM,"CODE",{});var p_t=s(_5e);k3r=r(p_t,"pretrained_model_name_or_path"),p_t.forEach(t),x3r=r(oM,":"),oM.forEach(t),R3r=i(Sa),I0=n(Sa,"UL",{});var uxe=s(I0);r4=n(uxe,"LI",{});var i7e=s(r4);u5e=n(i7e,"STRONG",{});var __t=s(u5e);S3r=r(__t,"beit"),__t.forEach(t),P3r=r(i7e," \u2014 "),zX=n(i7e,"A",{href:!0});var u_t=s(zX);$3r=r(u_t,"FlaxBeitForImageClassification"),u_t.forEach(t),I3r=r(i7e," (BEiT model)"),i7e.forEach(t),j3r=i(uxe),t4=n(uxe,"LI",{});var d7e=s(t4);b5e=n(d7e,"STRONG",{});var b_t=s(b5e);N3r=r(b_t,"vit"),b_t.forEach(t),D3r=r(d7e," \u2014 "),VX=n(d7e,"A",{href:!0});var v_t=s(VX);q3r=r(v_t,"FlaxViTForImageClassification"),v_t.forEach(t),G3r=r(d7e," (ViT model)"),d7e.forEach(t),uxe.forEach(t),O3r=i(Sa),v5e=n(Sa,"P",{});var T_t=s(v5e);X3r=r(T_t,"Examples:"),T_t.forEach(t),z3r=i(Sa),m(j0.$$.fragment,Sa),Sa.forEach(t),Fi.forEach(t),iBe=i(d),Tf=n(d,"H2",{class:!0});var bxe=s(Tf);a4=n(bxe,"A",{id:!0,class:!0,href:!0});var F_t=s(a4);T5e=n(F_t,"SPAN",{});var C_t=s(T5e);m(N0.$$.fragment,C_t),C_t.forEach(t),F_t.forEach(t),V3r=i(bxe),F5e=n(bxe,"SPAN",{});var M_t=s(F5e);W3r=r(M_t,"FlaxAutoModelForVision2Seq"),M_t.forEach(t),bxe.forEach(t),dBe=i(d),jr=n(d,"DIV",{class:!0});var Mi=s(jr);m(D0.$$.fragment,Mi),Q3r=i(Mi),Ff=n(Mi,"P",{});var jV=s(Ff);H3r=r(jV,`This is a generic model class that will be instantiated as one of the model classes of the library (with a vision-to-text modeling head) when created
with the `),C5e=n(jV,"CODE",{});var E_t=s(C5e);U3r=r(E_t,"from_pretrained()"),E_t.forEach(t),J3r=r(jV,"class method or the "),M5e=n(jV,"CODE",{});var y_t=s(M5e);Y3r=r(y_t,"from_config()"),y_t.forEach(t),K3r=r(jV,`class
method.`),jV.forEach(t),Z3r=i(Mi),q0=n(Mi,"P",{});var vxe=s(q0);eyr=r(vxe,"This class cannot be instantiated directly using "),E5e=n(vxe,"CODE",{});var w_t=s(E5e);oyr=r(w_t,"__init__()"),w_t.forEach(t),ryr=r(vxe," (throws an error)."),vxe.forEach(t),tyr=i(Mi),kt=n(Mi,"DIV",{class:!0});var Ei=s(kt);m(G0.$$.fragment,Ei),ayr=i(Ei),y5e=n(Ei,"P",{});var A_t=s(y5e);nyr=r(A_t,"Instantiates one of the model classes of the library (with a vision-to-text modeling head) from a configuration."),A_t.forEach(t),syr=i(Ei),Cf=n(Ei,"P",{});var NV=s(Cf);lyr=r(NV,`Note:
Loading a model from its configuration file does `),w5e=n(NV,"STRONG",{});var L_t=s(w5e);iyr=r(L_t,"not"),L_t.forEach(t),dyr=r(NV,` load the model weights. It only affects the
model\u2019s configuration. Use `),A5e=n(NV,"CODE",{});var B_t=s(A5e);cyr=r(B_t,"from_pretrained()"),B_t.forEach(t),fyr=r(NV,"to load the model weights."),NV.forEach(t),myr=i(Ei),L5e=n(Ei,"P",{});var k_t=s(L5e);gyr=r(k_t,"Examples:"),k_t.forEach(t),hyr=i(Ei),m(O0.$$.fragment,Ei),Ei.forEach(t),pyr=i(Mi),jo=n(Mi,"DIV",{class:!0});var Pa=s(jo);m(X0.$$.fragment,Pa),_yr=i(Pa),B5e=n(Pa,"P",{});var x_t=s(B5e);uyr=r(x_t,"Instantiate one of the model classes of the library (with a vision-to-text modeling head) from a pretrained model."),x_t.forEach(t),byr=i(Pa),Sn=n(Pa,"P",{});var rM=s(Sn);vyr=r(rM,"The model class to instantiate is selected based on the "),k5e=n(rM,"CODE",{});var R_t=s(k5e);Tyr=r(R_t,"model_type"),R_t.forEach(t),Fyr=r(rM,` property of the config object (either
passed as an argument or loaded from `),x5e=n(rM,"CODE",{});var S_t=s(x5e);Cyr=r(S_t,"pretrained_model_name_or_path"),S_t.forEach(t),Myr=r(rM,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),R5e=n(rM,"CODE",{});var P_t=s(R5e);Eyr=r(P_t,"pretrained_model_name_or_path"),P_t.forEach(t),yyr=r(rM,":"),rM.forEach(t),wyr=i(Pa),S5e=n(Pa,"UL",{});var $_t=s(S5e);n4=n($_t,"LI",{});var c7e=s(n4);P5e=n(c7e,"STRONG",{});var I_t=s(P5e);Ayr=r(I_t,"vision-encoder-decoder"),I_t.forEach(t),Lyr=r(c7e," \u2014 "),WX=n(c7e,"A",{href:!0});var j_t=s(WX);Byr=r(j_t,"FlaxVisionEncoderDecoderModel"),j_t.forEach(t),kyr=r(c7e," (Vision Encoder decoder model)"),c7e.forEach(t),$_t.forEach(t),xyr=i(Pa),$5e=n(Pa,"P",{});var N_t=s($5e);Ryr=r(N_t,"Examples:"),N_t.forEach(t),Syr=i(Pa),m(z0.$$.fragment,Pa),Pa.forEach(t),Mi.forEach(t),this.h()},h(){c(J,"name","hf:doc:metadata"),c(J,"content",JSON.stringify(Q_t)),c(me,"id","auto-classes"),c(me,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(me,"href","#auto-classes"),c(ie,"class","relative group"),c(Pn,"href","/docs/transformers/pr_15682/en/model_doc/auto#transformers.AutoConfig"),c(In,"href","/docs/transformers/pr_15682/en/model_doc/auto#transformers.AutoModel"),c(jn,"href","/docs/transformers/pr_15682/en/model_doc/auto#transformers.AutoTokenizer"),c(Ri,"href","/docs/transformers/pr_15682/en/model_doc/bert#transformers.BertModel"),c(Lf,"id","extending-the-auto-classes"),c(Lf,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Lf,"href","#extending-the-auto-classes"),c(Si,"class","relative group"),c(kf,"id","transformers.AutoConfig"),c(kf,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(kf,"href","#transformers.AutoConfig"),c(Pi,"class","relative group"),c(QL,"href","/docs/transformers/pr_15682/en/model_doc/auto#transformers.AutoConfig.from_pretrained"),c(HL,"href","/docs/transformers/pr_15682/en/model_doc/albert#transformers.AlbertConfig"),c(UL,"href","/docs/transformers/pr_15682/en/model_doc/bart#transformers.BartConfig"),c(JL,"href","/docs/transformers/pr_15682/en/model_doc/beit#transformers.BeitConfig"),c(YL,"href","/docs/transformers/pr_15682/en/model_doc/bert#transformers.BertConfig"),c(KL,"href","/docs/transformers/pr_15682/en/model_doc/bert-generation#transformers.BertGenerationConfig"),c(ZL,"href","/docs/transformers/pr_15682/en/model_doc/big_bird#transformers.BigBirdConfig"),c(e7,"href","/docs/transformers/pr_15682/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusConfig"),c(o7,"href","/docs/transformers/pr_15682/en/model_doc/blenderbot#transformers.BlenderbotConfig"),c(r7,"href","/docs/transformers/pr_15682/en/model_doc/blenderbot-small#transformers.BlenderbotSmallConfig"),c(t7,"href","/docs/transformers/pr_15682/en/model_doc/camembert#transformers.CamembertConfig"),c(a7,"href","/docs/transformers/pr_15682/en/model_doc/canine#transformers.CanineConfig"),c(n7,"href","/docs/transformers/pr_15682/en/model_doc/clip#transformers.CLIPConfig"),c(s7,"href","/docs/transformers/pr_15682/en/model_doc/convbert#transformers.ConvBertConfig"),c(l7,"href","/docs/transformers/pr_15682/en/model_doc/convnext#transformers.ConvNextConfig"),c(i7,"href","/docs/transformers/pr_15682/en/model_doc/ctrl#transformers.CTRLConfig"),c(d7,"href","/docs/transformers/pr_15682/en/model_doc/deberta#transformers.DebertaConfig"),c(c7,"href","/docs/transformers/pr_15682/en/model_doc/deberta-v2#transformers.DebertaV2Config"),c(f7,"href","/docs/transformers/pr_15682/en/model_doc/deit#transformers.DeiTConfig"),c(m7,"href","/docs/transformers/pr_15682/en/model_doc/detr#transformers.DetrConfig"),c(g7,"href","/docs/transformers/pr_15682/en/model_doc/distilbert#transformers.DistilBertConfig"),c(h7,"href","/docs/transformers/pr_15682/en/model_doc/dpr#transformers.DPRConfig"),c(p7,"href","/docs/transformers/pr_15682/en/model_doc/electra#transformers.ElectraConfig"),c(_7,"href","/docs/transformers/pr_15682/en/model_doc/encoder-decoder#transformers.EncoderDecoderConfig"),c(u7,"href","/docs/transformers/pr_15682/en/model_doc/flaubert#transformers.FlaubertConfig"),c(b7,"href","/docs/transformers/pr_15682/en/model_doc/fnet#transformers.FNetConfig"),c(v7,"href","/docs/transformers/pr_15682/en/model_doc/fsmt#transformers.FSMTConfig"),c(T7,"href","/docs/transformers/pr_15682/en/model_doc/funnel#transformers.FunnelConfig"),c(F7,"href","/docs/transformers/pr_15682/en/model_doc/gpt2#transformers.GPT2Config"),c(C7,"href","/docs/transformers/pr_15682/en/model_doc/gpt_neo#transformers.GPTNeoConfig"),c(M7,"href","/docs/transformers/pr_15682/en/model_doc/gptj#transformers.GPTJConfig"),c(E7,"href","/docs/transformers/pr_15682/en/model_doc/hubert#transformers.HubertConfig"),c(y7,"href","/docs/transformers/pr_15682/en/model_doc/ibert#transformers.IBertConfig"),c(w7,"href","/docs/transformers/pr_15682/en/model_doc/imagegpt#transformers.ImageGPTConfig"),c(A7,"href","/docs/transformers/pr_15682/en/model_doc/layoutlm#transformers.LayoutLMConfig"),c(L7,"href","/docs/transformers/pr_15682/en/model_doc/layoutlmv2#transformers.LayoutLMv2Config"),c(B7,"href","/docs/transformers/pr_15682/en/model_doc/led#transformers.LEDConfig"),c(k7,"href","/docs/transformers/pr_15682/en/model_doc/longformer#transformers.LongformerConfig"),c(x7,"href","/docs/transformers/pr_15682/en/model_doc/luke#transformers.LukeConfig"),c(R7,"href","/docs/transformers/pr_15682/en/model_doc/lxmert#transformers.LxmertConfig"),c(S7,"href","/docs/transformers/pr_15682/en/model_doc/m2m_100#transformers.M2M100Config"),c(P7,"href","/docs/transformers/pr_15682/en/model_doc/marian#transformers.MarianConfig"),c($7,"href","/docs/transformers/pr_15682/en/model_doc/maskformer#transformers.MaskFormerConfig"),c(I7,"href","/docs/transformers/pr_15682/en/model_doc/mbart#transformers.MBartConfig"),c(j7,"href","/docs/transformers/pr_15682/en/model_doc/megatron-bert#transformers.MegatronBertConfig"),c(N7,"href","/docs/transformers/pr_15682/en/model_doc/mobilebert#transformers.MobileBertConfig"),c(D7,"href","/docs/transformers/pr_15682/en/model_doc/mpnet#transformers.MPNetConfig"),c(q7,"href","/docs/transformers/pr_15682/en/model_doc/mt5#transformers.MT5Config"),c(G7,"href","/docs/transformers/pr_15682/en/model_doc/nystromformer#transformers.NystromformerConfig"),c(O7,"href","/docs/transformers/pr_15682/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig"),c(X7,"href","/docs/transformers/pr_15682/en/model_doc/pegasus#transformers.PegasusConfig"),c(z7,"href","/docs/transformers/pr_15682/en/model_doc/perceiver#transformers.PerceiverConfig"),c(V7,"href","/docs/transformers/pr_15682/en/model_doc/plbart#transformers.PLBartConfig"),c(W7,"href","/docs/transformers/pr_15682/en/model_doc/poolformer#transformers.PoolFormerConfig"),c(Q7,"href","/docs/transformers/pr_15682/en/model_doc/prophetnet#transformers.ProphetNetConfig"),c(H7,"href","/docs/transformers/pr_15682/en/model_doc/qdqbert#transformers.QDQBertConfig"),c(U7,"href","/docs/transformers/pr_15682/en/model_doc/rag#transformers.RagConfig"),c(J7,"href","/docs/transformers/pr_15682/en/model_doc/realm#transformers.RealmConfig"),c(Y7,"href","/docs/transformers/pr_15682/en/model_doc/reformer#transformers.ReformerConfig"),c(K7,"href","/docs/transformers/pr_15682/en/model_doc/rembert#transformers.RemBertConfig"),c(Z7,"href","/docs/transformers/pr_15682/en/model_doc/retribert#transformers.RetriBertConfig"),c(e9,"href","/docs/transformers/pr_15682/en/model_doc/roberta#transformers.RobertaConfig"),c(o9,"href","/docs/transformers/pr_15682/en/model_doc/roformer#transformers.RoFormerConfig"),c(r9,"href","/docs/transformers/pr_15682/en/model_doc/segformer#transformers.SegformerConfig"),c(t9,"href","/docs/transformers/pr_15682/en/model_doc/sew#transformers.SEWConfig"),c(a9,"href","/docs/transformers/pr_15682/en/model_doc/sew-d#transformers.SEWDConfig"),c(n9,"href","/docs/transformers/pr_15682/en/model_doc/speech-encoder-decoder#transformers.SpeechEncoderDecoderConfig"),c(s9,"href","/docs/transformers/pr_15682/en/model_doc/speech_to_text#transformers.Speech2TextConfig"),c(l9,"href","/docs/transformers/pr_15682/en/model_doc/speech_to_text_2#transformers.Speech2Text2Config"),c(i9,"href","/docs/transformers/pr_15682/en/model_doc/splinter#transformers.SplinterConfig"),c(d9,"href","/docs/transformers/pr_15682/en/model_doc/squeezebert#transformers.SqueezeBertConfig"),c(c9,"href","/docs/transformers/pr_15682/en/model_doc/swin#transformers.SwinConfig"),c(f9,"href","/docs/transformers/pr_15682/en/model_doc/t5#transformers.T5Config"),c(m9,"href","/docs/transformers/pr_15682/en/model_doc/tapas#transformers.TapasConfig"),c(g9,"href","/docs/transformers/pr_15682/en/model_doc/transfo-xl#transformers.TransfoXLConfig"),c(h9,"href","/docs/transformers/pr_15682/en/model_doc/trocr#transformers.TrOCRConfig"),c(p9,"href","/docs/transformers/pr_15682/en/model_doc/unispeech#transformers.UniSpeechConfig"),c(_9,"href","/docs/transformers/pr_15682/en/model_doc/unispeech-sat#transformers.UniSpeechSatConfig"),c(u9,"href","/docs/transformers/pr_15682/en/model_doc/vilt#transformers.ViltConfig"),c(b9,"href","/docs/transformers/pr_15682/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderConfig"),c(v9,"href","/docs/transformers/pr_15682/en/model_doc/vision-text-dual-encoder#transformers.VisionTextDualEncoderConfig"),c(T9,"href","/docs/transformers/pr_15682/en/model_doc/visual_bert#transformers.VisualBertConfig"),c(F9,"href","/docs/transformers/pr_15682/en/model_doc/vit#transformers.ViTConfig"),c(C9,"href","/docs/transformers/pr_15682/en/model_doc/vit_mae#transformers.ViTMAEConfig"),c(M9,"href","/docs/transformers/pr_15682/en/model_doc/wav2vec2#transformers.Wav2Vec2Config"),c(E9,"href","/docs/transformers/pr_15682/en/model_doc/wavlm#transformers.WavLMConfig"),c(y9,"href","/docs/transformers/pr_15682/en/model_doc/xglm#transformers.XGLMConfig"),c(w9,"href","/docs/transformers/pr_15682/en/model_doc/xlm#transformers.XLMConfig"),c(A9,"href","/docs/transformers/pr_15682/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetConfig"),c(L9,"href","/docs/transformers/pr_15682/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig"),c(B9,"href","/docs/transformers/pr_15682/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLConfig"),c(k9,"href","/docs/transformers/pr_15682/en/model_doc/xlnet#transformers.XLNetConfig"),c(x9,"href","/docs/transformers/pr_15682/en/model_doc/yoso#transformers.YosoConfig"),c(fo,"class","docstring"),c(pg,"class","docstring"),c(Go,"class","docstring"),c(_g,"id","transformers.AutoTokenizer"),c(_g,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(_g,"href","#transformers.AutoTokenizer"),c(Ii,"class","relative group"),c(R9,"href","/docs/transformers/pr_15682/en/model_doc/auto#transformers.AutoTokenizer.from_pretrained"),c(S9,"href","/docs/transformers/pr_15682/en/model_doc/albert#transformers.AlbertTokenizer"),c(P9,"href","/docs/transformers/pr_15682/en/model_doc/albert#transformers.AlbertTokenizerFast"),c($9,"href","/docs/transformers/pr_15682/en/model_doc/bart#transformers.BartTokenizer"),c(I9,"href","/docs/transformers/pr_15682/en/model_doc/bart#transformers.BartTokenizerFast"),c(j9,"href","/docs/transformers/pr_15682/en/model_doc/barthez#transformers.BarthezTokenizer"),c(N9,"href","/docs/transformers/pr_15682/en/model_doc/barthez#transformers.BarthezTokenizerFast"),c(D9,"href","/docs/transformers/pr_15682/en/model_doc/bartpho#transformers.BartphoTokenizer"),c(q9,"href","/docs/transformers/pr_15682/en/model_doc/bert#transformers.BertTokenizer"),c(G9,"href","/docs/transformers/pr_15682/en/model_doc/bert#transformers.BertTokenizerFast"),c(O9,"href","/docs/transformers/pr_15682/en/model_doc/bert-generation#transformers.BertGenerationTokenizer"),c(X9,"href","/docs/transformers/pr_15682/en/model_doc/bert-japanese#transformers.BertJapaneseTokenizer"),c(z9,"href","/docs/transformers/pr_15682/en/model_doc/bertweet#transformers.BertweetTokenizer"),c(V9,"href","/docs/transformers/pr_15682/en/model_doc/big_bird#transformers.BigBirdTokenizer"),c(W9,"href","/docs/transformers/pr_15682/en/model_doc/big_bird#transformers.BigBirdTokenizerFast"),c(Q9,"href","/docs/transformers/pr_15682/en/model_doc/pegasus#transformers.PegasusTokenizer"),c(H9,"href","/docs/transformers/pr_15682/en/model_doc/pegasus#transformers.PegasusTokenizerFast"),c(U9,"href","/docs/transformers/pr_15682/en/model_doc/blenderbot#transformers.BlenderbotTokenizer"),c(J9,"href","/docs/transformers/pr_15682/en/model_doc/blenderbot#transformers.BlenderbotTokenizerFast"),c(Y9,"href","/docs/transformers/pr_15682/en/model_doc/blenderbot-small#transformers.BlenderbotSmallTokenizer"),c(K9,"href","/docs/transformers/pr_15682/en/model_doc/byt5#transformers.ByT5Tokenizer"),c(Z9,"href","/docs/transformers/pr_15682/en/model_doc/camembert#transformers.CamembertTokenizer"),c(eB,"href","/docs/transformers/pr_15682/en/model_doc/camembert#transformers.CamembertTokenizerFast"),c(oB,"href","/docs/transformers/pr_15682/en/model_doc/canine#transformers.CanineTokenizer"),c(rB,"href","/docs/transformers/pr_15682/en/model_doc/clip#transformers.CLIPTokenizer"),c(tB,"href","/docs/transformers/pr_15682/en/model_doc/clip#transformers.CLIPTokenizerFast"),c(aB,"href","/docs/transformers/pr_15682/en/model_doc/convbert#transformers.ConvBertTokenizer"),c(nB,"href","/docs/transformers/pr_15682/en/model_doc/convbert#transformers.ConvBertTokenizerFast"),c(sB,"href","/docs/transformers/pr_15682/en/model_doc/cpm#transformers.CpmTokenizer"),c(lB,"href","/docs/transformers/pr_15682/en/model_doc/ctrl#transformers.CTRLTokenizer"),c(iB,"href","/docs/transformers/pr_15682/en/model_doc/deberta#transformers.DebertaTokenizer"),c(dB,"href","/docs/transformers/pr_15682/en/model_doc/deberta#transformers.DebertaTokenizerFast"),c(cB,"href","/docs/transformers/pr_15682/en/model_doc/deberta-v2#transformers.DebertaV2Tokenizer"),c(fB,"href","/docs/transformers/pr_15682/en/model_doc/distilbert#transformers.DistilBertTokenizer"),c(mB,"href","/docs/transformers/pr_15682/en/model_doc/distilbert#transformers.DistilBertTokenizerFast"),c(gB,"href","/docs/transformers/pr_15682/en/model_doc/dpr#transformers.DPRQuestionEncoderTokenizer"),c(hB,"href","/docs/transformers/pr_15682/en/model_doc/dpr#transformers.DPRQuestionEncoderTokenizerFast"),c(pB,"href","/docs/transformers/pr_15682/en/model_doc/electra#transformers.ElectraTokenizer"),c(_B,"href","/docs/transformers/pr_15682/en/model_doc/electra#transformers.ElectraTokenizerFast"),c(uB,"href","/docs/transformers/pr_15682/en/model_doc/flaubert#transformers.FlaubertTokenizer"),c(bB,"href","/docs/transformers/pr_15682/en/model_doc/fnet#transformers.FNetTokenizer"),c(vB,"href","/docs/transformers/pr_15682/en/model_doc/fnet#transformers.FNetTokenizerFast"),c(TB,"href","/docs/transformers/pr_15682/en/model_doc/fsmt#transformers.FSMTTokenizer"),c(FB,"href","/docs/transformers/pr_15682/en/model_doc/funnel#transformers.FunnelTokenizer"),c(CB,"href","/docs/transformers/pr_15682/en/model_doc/funnel#transformers.FunnelTokenizerFast"),c(MB,"href","/docs/transformers/pr_15682/en/model_doc/gpt2#transformers.GPT2Tokenizer"),c(EB,"href","/docs/transformers/pr_15682/en/model_doc/gpt2#transformers.GPT2TokenizerFast"),c(yB,"href","/docs/transformers/pr_15682/en/model_doc/gpt2#transformers.GPT2Tokenizer"),c(wB,"href","/docs/transformers/pr_15682/en/model_doc/gpt2#transformers.GPT2TokenizerFast"),c(AB,"href","/docs/transformers/pr_15682/en/model_doc/herbert#transformers.HerbertTokenizer"),c(LB,"href","/docs/transformers/pr_15682/en/model_doc/herbert#transformers.HerbertTokenizerFast"),c(BB,"href","/docs/transformers/pr_15682/en/model_doc/wav2vec2#transformers.Wav2Vec2CTCTokenizer"),c(kB,"href","/docs/transformers/pr_15682/en/model_doc/roberta#transformers.RobertaTokenizer"),c(xB,"href","/docs/transformers/pr_15682/en/model_doc/roberta#transformers.RobertaTokenizerFast"),c(RB,"href","/docs/transformers/pr_15682/en/model_doc/layoutlm#transformers.LayoutLMTokenizer"),c(SB,"href","/docs/transformers/pr_15682/en/model_doc/layoutlm#transformers.LayoutLMTokenizerFast"),c(PB,"href","/docs/transformers/pr_15682/en/model_doc/layoutlmv2#transformers.LayoutLMv2Tokenizer"),c($B,"href","/docs/transformers/pr_15682/en/model_doc/layoutlmv2#transformers.LayoutLMv2TokenizerFast"),c(IB,"href","/docs/transformers/pr_15682/en/model_doc/layoutxlm#transformers.LayoutXLMTokenizer"),c(jB,"href","/docs/transformers/pr_15682/en/model_doc/layoutxlm#transformers.LayoutXLMTokenizerFast"),c(NB,"href","/docs/transformers/pr_15682/en/model_doc/led#transformers.LEDTokenizer"),c(DB,"href","/docs/transformers/pr_15682/en/model_doc/led#transformers.LEDTokenizerFast"),c(qB,"href","/docs/transformers/pr_15682/en/model_doc/longformer#transformers.LongformerTokenizer"),c(GB,"href","/docs/transformers/pr_15682/en/model_doc/longformer#transformers.LongformerTokenizerFast"),c(OB,"href","/docs/transformers/pr_15682/en/model_doc/luke#transformers.LukeTokenizer"),c(XB,"href","/docs/transformers/pr_15682/en/model_doc/lxmert#transformers.LxmertTokenizer"),c(zB,"href","/docs/transformers/pr_15682/en/model_doc/lxmert#transformers.LxmertTokenizerFast"),c(VB,"href","/docs/transformers/pr_15682/en/model_doc/m2m_100#transformers.M2M100Tokenizer"),c(WB,"href","/docs/transformers/pr_15682/en/model_doc/marian#transformers.MarianTokenizer"),c(QB,"href","/docs/transformers/pr_15682/en/model_doc/mbart#transformers.MBartTokenizer"),c(HB,"href","/docs/transformers/pr_15682/en/model_doc/mbart#transformers.MBartTokenizerFast"),c(UB,"href","/docs/transformers/pr_15682/en/model_doc/mbart#transformers.MBart50Tokenizer"),c(JB,"href","/docs/transformers/pr_15682/en/model_doc/mbart#transformers.MBart50TokenizerFast"),c(YB,"href","/docs/transformers/pr_15682/en/model_doc/mluke#transformers.MLukeTokenizer"),c(KB,"href","/docs/transformers/pr_15682/en/model_doc/mobilebert#transformers.MobileBertTokenizer"),c(ZB,"href","/docs/transformers/pr_15682/en/model_doc/mobilebert#transformers.MobileBertTokenizerFast"),c(ek,"href","/docs/transformers/pr_15682/en/model_doc/mpnet#transformers.MPNetTokenizer"),c(ok,"href","/docs/transformers/pr_15682/en/model_doc/mpnet#transformers.MPNetTokenizerFast"),c(rk,"href","/docs/transformers/pr_15682/en/model_doc/mt5#transformers.T5Tokenizer"),c(tk,"href","/docs/transformers/pr_15682/en/model_doc/mt5#transformers.T5TokenizerFast"),c(ak,"href","/docs/transformers/pr_15682/en/model_doc/openai-gpt#transformers.OpenAIGPTTokenizer"),c(nk,"href","/docs/transformers/pr_15682/en/model_doc/openai-gpt#transformers.OpenAIGPTTokenizerFast"),c(sk,"href","/docs/transformers/pr_15682/en/model_doc/pegasus#transformers.PegasusTokenizer"),c(lk,"href","/docs/transformers/pr_15682/en/model_doc/pegasus#transformers.PegasusTokenizerFast"),c(ik,"href","/docs/transformers/pr_15682/en/model_doc/perceiver#transformers.PerceiverTokenizer"),c(dk,"href","/docs/transformers/pr_15682/en/model_doc/phobert#transformers.PhobertTokenizer"),c(ck,"href","/docs/transformers/pr_15682/en/model_doc/plbart#transformers.PLBartTokenizer"),c(fk,"href","/docs/transformers/pr_15682/en/model_doc/prophetnet#transformers.ProphetNetTokenizer"),c(mk,"href","/docs/transformers/pr_15682/en/model_doc/bert#transformers.BertTokenizer"),c(gk,"href","/docs/transformers/pr_15682/en/model_doc/bert#transformers.BertTokenizerFast"),c(hk,"href","/docs/transformers/pr_15682/en/model_doc/rag#transformers.RagTokenizer"),c(pk,"href","/docs/transformers/pr_15682/en/model_doc/reformer#transformers.ReformerTokenizer"),c(_k,"href","/docs/transformers/pr_15682/en/model_doc/reformer#transformers.ReformerTokenizerFast"),c(uk,"href","/docs/transformers/pr_15682/en/model_doc/rembert#transformers.RemBertTokenizer"),c(bk,"href","/docs/transformers/pr_15682/en/model_doc/rembert#transformers.RemBertTokenizerFast"),c(vk,"href","/docs/transformers/pr_15682/en/model_doc/retribert#transformers.RetriBertTokenizer"),c(Tk,"href","/docs/transformers/pr_15682/en/model_doc/retribert#transformers.RetriBertTokenizerFast"),c(Fk,"href","/docs/transformers/pr_15682/en/model_doc/roberta#transformers.RobertaTokenizer"),c(Ck,"href","/docs/transformers/pr_15682/en/model_doc/roberta#transformers.RobertaTokenizerFast"),c(Mk,"href","/docs/transformers/pr_15682/en/model_doc/roformer#transformers.RoFormerTokenizer"),c(Ek,"href","/docs/transformers/pr_15682/en/model_doc/roformer#transformers.RoFormerTokenizerFast"),c(yk,"href","/docs/transformers/pr_15682/en/model_doc/speech_to_text#transformers.Speech2TextTokenizer"),c(wk,"href","/docs/transformers/pr_15682/en/model_doc/speech_to_text_2#transformers.Speech2Text2Tokenizer"),c(Ak,"href","/docs/transformers/pr_15682/en/model_doc/splinter#transformers.SplinterTokenizer"),c(Lk,"href","/docs/transformers/pr_15682/en/model_doc/splinter#transformers.SplinterTokenizerFast"),c(Bk,"href","/docs/transformers/pr_15682/en/model_doc/squeezebert#transformers.SqueezeBertTokenizer"),c(kk,"href","/docs/transformers/pr_15682/en/model_doc/squeezebert#transformers.SqueezeBertTokenizerFast"),c(xk,"href","/docs/transformers/pr_15682/en/model_doc/mt5#transformers.T5Tokenizer"),c(Rk,"href","/docs/transformers/pr_15682/en/model_doc/mt5#transformers.T5TokenizerFast"),c(Sk,"href","/docs/transformers/pr_15682/en/model_doc/tapas#transformers.TapasTokenizer"),c(Pk,"href","/docs/transformers/pr_15682/en/model_doc/transfo-xl#transformers.TransfoXLTokenizer"),c($k,"href","/docs/transformers/pr_15682/en/model_doc/wav2vec2#transformers.Wav2Vec2CTCTokenizer"),c(Ik,"href","/docs/transformers/pr_15682/en/model_doc/wav2vec2_phoneme#transformers.Wav2Vec2PhonemeCTCTokenizer"),c(jk,"href","/docs/transformers/pr_15682/en/model_doc/xglm#transformers.XGLMTokenizer"),c(Nk,"href","/docs/transformers/pr_15682/en/model_doc/xglm#transformers.XGLMTokenizerFast"),c(Dk,"href","/docs/transformers/pr_15682/en/model_doc/xlm#transformers.XLMTokenizer"),c(qk,"href","/docs/transformers/pr_15682/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetTokenizer"),c(Gk,"href","/docs/transformers/pr_15682/en/model_doc/xlm-roberta#transformers.XLMRobertaTokenizer"),c(Ok,"href","/docs/transformers/pr_15682/en/model_doc/xlm-roberta#transformers.XLMRobertaTokenizerFast"),c(Xk,"href","/docs/transformers/pr_15682/en/model_doc/xlnet#transformers.XLNetTokenizer"),c(zk,"href","/docs/transformers/pr_15682/en/model_doc/xlnet#transformers.XLNetTokenizerFast"),c(mo,"class","docstring"),c(Wg,"class","docstring"),c(Oo,"class","docstring"),c(Qg,"id","transformers.AutoFeatureExtractor"),c(Qg,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Qg,"href","#transformers.AutoFeatureExtractor"),c(ji,"class","relative group"),c(Vk,"href","/docs/transformers/pr_15682/en/model_doc/auto#transformers.AutoFeatureExtractor.from_pretrained"),c(Wk,"href","/docs/transformers/pr_15682/en/model_doc/beit#transformers.BeitFeatureExtractor"),c(Qk,"href","/docs/transformers/pr_15682/en/model_doc/clip#transformers.CLIPFeatureExtractor"),c(Hk,"href","/docs/transformers/pr_15682/en/model_doc/convnext#transformers.ConvNextFeatureExtractor"),c(Uk,"href","/docs/transformers/pr_15682/en/model_doc/deit#transformers.DeiTFeatureExtractor"),c(Jk,"href","/docs/transformers/pr_15682/en/model_doc/detr#transformers.DetrFeatureExtractor"),c(Yk,"href","/docs/transformers/pr_15682/en/model_doc/wav2vec2#transformers.Wav2Vec2FeatureExtractor"),c(Kk,"href","/docs/transformers/pr_15682/en/model_doc/layoutlmv2#transformers.LayoutLMv2FeatureExtractor"),c(Zk,"href","/docs/transformers/pr_15682/en/model_doc/perceiver#transformers.PerceiverFeatureExtractor"),c(ex,"href","/docs/transformers/pr_15682/en/model_doc/poolformer#transformers.PoolFormerFeatureExtractor"),c(ox,"href","/docs/transformers/pr_15682/en/model_doc/segformer#transformers.SegformerFeatureExtractor"),c(rx,"href","/docs/transformers/pr_15682/en/model_doc/speech_to_text#transformers.Speech2TextFeatureExtractor"),c(tx,"href","/docs/transformers/pr_15682/en/model_doc/vit#transformers.ViTFeatureExtractor"),c(ax,"href","/docs/transformers/pr_15682/en/model_doc/vit#transformers.ViTFeatureExtractor"),c(nx,"href","/docs/transformers/pr_15682/en/model_doc/vit#transformers.ViTFeatureExtractor"),c(sx,"href","/docs/transformers/pr_15682/en/model_doc/wav2vec2#transformers.Wav2Vec2FeatureExtractor"),c(Le,"class","docstring"),c(ch,"class","docstring"),c(Xo,"class","docstring"),c(fh,"id","transformers.AutoProcessor"),c(fh,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(fh,"href","#transformers.AutoProcessor"),c(Ni,"class","relative group"),c(lx,"href","/docs/transformers/pr_15682/en/model_doc/auto#transformers.AutoProcessor.from_pretrained"),c(ix,"href","/docs/transformers/pr_15682/en/model_doc/clip#transformers.CLIPProcessor"),c(dx,"href","/docs/transformers/pr_15682/en/model_doc/layoutlmv2#transformers.LayoutLMv2Processor"),c(cx,"href","/docs/transformers/pr_15682/en/model_doc/layoutxlm#transformers.LayoutXLMProcessor"),c(fx,"href","/docs/transformers/pr_15682/en/model_doc/speech_to_text#transformers.Speech2TextProcessor"),c(mx,"href","/docs/transformers/pr_15682/en/model_doc/speech_to_text_2#transformers.Speech2Text2Processor"),c(gx,"href","/docs/transformers/pr_15682/en/model_doc/trocr#transformers.TrOCRProcessor"),c(hx,"href","/docs/transformers/pr_15682/en/model_doc/vision-text-dual-encoder#transformers.VisionTextDualEncoderProcessor"),c(px,"href","/docs/transformers/pr_15682/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor"),c(Be,"class","docstring"),c(Fh,"class","docstring"),c(zo,"class","docstring"),c(Ch,"id","transformers.AutoModel"),c(Ch,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Ch,"href","#transformers.AutoModel"),c(qi,"class","relative group"),c(Nr,"class","docstring"),c(_x,"href","/docs/transformers/pr_15682/en/model_doc/albert#transformers.AlbertModel"),c(ux,"href","/docs/transformers/pr_15682/en/model_doc/bart#transformers.BartModel"),c(bx,"href","/docs/transformers/pr_15682/en/model_doc/beit#transformers.BeitModel"),c(vx,"href","/docs/transformers/pr_15682/en/model_doc/bert#transformers.BertModel"),c(Tx,"href","/docs/transformers/pr_15682/en/model_doc/bert-generation#transformers.BertGenerationEncoder"),c(Fx,"href","/docs/transformers/pr_15682/en/model_doc/big_bird#transformers.BigBirdModel"),c(Cx,"href","/docs/transformers/pr_15682/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusModel"),c(Mx,"href","/docs/transformers/pr_15682/en/model_doc/blenderbot#transformers.BlenderbotModel"),c(Ex,"href","/docs/transformers/pr_15682/en/model_doc/blenderbot-small#transformers.BlenderbotSmallModel"),c(yx,"href","/docs/transformers/pr_15682/en/model_doc/camembert#transformers.CamembertModel"),c(wx,"href","/docs/transformers/pr_15682/en/model_doc/canine#transformers.CanineModel"),c(Ax,"href","/docs/transformers/pr_15682/en/model_doc/clip#transformers.CLIPModel"),c(Lx,"href","/docs/transformers/pr_15682/en/model_doc/convbert#transformers.ConvBertModel"),c(Bx,"href","/docs/transformers/pr_15682/en/model_doc/convnext#transformers.ConvNextModel"),c(kx,"href","/docs/transformers/pr_15682/en/model_doc/ctrl#transformers.CTRLModel"),c(xx,"href","/docs/transformers/pr_15682/en/model_doc/deberta#transformers.DebertaModel"),c(Rx,"href","/docs/transformers/pr_15682/en/model_doc/deberta-v2#transformers.DebertaV2Model"),c(Sx,"href","/docs/transformers/pr_15682/en/model_doc/deit#transformers.DeiTModel"),c(Px,"href","/docs/transformers/pr_15682/en/model_doc/detr#transformers.DetrModel"),c($x,"href","/docs/transformers/pr_15682/en/model_doc/distilbert#transformers.DistilBertModel"),c(Ix,"href","/docs/transformers/pr_15682/en/model_doc/dpr#transformers.DPRQuestionEncoder"),c(jx,"href","/docs/transformers/pr_15682/en/model_doc/electra#transformers.ElectraModel"),c(Nx,"href","/docs/transformers/pr_15682/en/model_doc/flaubert#transformers.FlaubertModel"),c(Dx,"href","/docs/transformers/pr_15682/en/model_doc/fnet#transformers.FNetModel"),c(qx,"href","/docs/transformers/pr_15682/en/model_doc/fsmt#transformers.FSMTModel"),c(Gx,"href","/docs/transformers/pr_15682/en/model_doc/funnel#transformers.FunnelModel"),c(Ox,"href","/docs/transformers/pr_15682/en/model_doc/funnel#transformers.FunnelBaseModel"),c(Xx,"href","/docs/transformers/pr_15682/en/model_doc/gpt2#transformers.GPT2Model"),c(zx,"href","/docs/transformers/pr_15682/en/model_doc/gpt_neo#transformers.GPTNeoModel"),c(Vx,"href","/docs/transformers/pr_15682/en/model_doc/gptj#transformers.GPTJModel"),c(Wx,"href","/docs/transformers/pr_15682/en/model_doc/hubert#transformers.HubertModel"),c(Qx,"href","/docs/transformers/pr_15682/en/model_doc/ibert#transformers.IBertModel"),c(Hx,"href","/docs/transformers/pr_15682/en/model_doc/imagegpt#transformers.ImageGPTModel"),c(Ux,"href","/docs/transformers/pr_15682/en/model_doc/layoutlm#transformers.LayoutLMModel"),c(Jx,"href","/docs/transformers/pr_15682/en/model_doc/layoutlmv2#transformers.LayoutLMv2Model"),c(Yx,"href","/docs/transformers/pr_15682/en/model_doc/led#transformers.LEDModel"),c(Kx,"href","/docs/transformers/pr_15682/en/model_doc/longformer#transformers.LongformerModel"),c(Zx,"href","/docs/transformers/pr_15682/en/model_doc/luke#transformers.LukeModel"),c(eR,"href","/docs/transformers/pr_15682/en/model_doc/lxmert#transformers.LxmertModel"),c(oR,"href","/docs/transformers/pr_15682/en/model_doc/m2m_100#transformers.M2M100Model"),c(rR,"href","/docs/transformers/pr_15682/en/model_doc/marian#transformers.MarianModel"),c(tR,"href","/docs/transformers/pr_15682/en/model_doc/maskformer#transformers.MaskFormerModel"),c(aR,"href","/docs/transformers/pr_15682/en/model_doc/mbart#transformers.MBartModel"),c(nR,"href","/docs/transformers/pr_15682/en/model_doc/megatron-bert#transformers.MegatronBertModel"),c(sR,"href","/docs/transformers/pr_15682/en/model_doc/mobilebert#transformers.MobileBertModel"),c(lR,"href","/docs/transformers/pr_15682/en/model_doc/mpnet#transformers.MPNetModel"),c(iR,"href","/docs/transformers/pr_15682/en/model_doc/mt5#transformers.MT5Model"),c(dR,"href","/docs/transformers/pr_15682/en/model_doc/nystromformer#transformers.NystromformerModel"),c(cR,"href","/docs/transformers/pr_15682/en/model_doc/openai-gpt#transformers.OpenAIGPTModel"),c(fR,"href","/docs/transformers/pr_15682/en/model_doc/pegasus#transformers.PegasusModel"),c(mR,"href","/docs/transformers/pr_15682/en/model_doc/perceiver#transformers.PerceiverModel"),c(gR,"href","/docs/transformers/pr_15682/en/model_doc/plbart#transformers.PLBartModel"),c(hR,"href","/docs/transformers/pr_15682/en/model_doc/poolformer#transformers.PoolFormerModel"),c(pR,"href","/docs/transformers/pr_15682/en/model_doc/prophetnet#transformers.ProphetNetModel"),c(_R,"href","/docs/transformers/pr_15682/en/model_doc/qdqbert#transformers.QDQBertModel"),c(uR,"href","/docs/transformers/pr_15682/en/model_doc/reformer#transformers.ReformerModel"),c(bR,"href","/docs/transformers/pr_15682/en/model_doc/rembert#transformers.RemBertModel"),c(vR,"href","/docs/transformers/pr_15682/en/model_doc/retribert#transformers.RetriBertModel"),c(TR,"href","/docs/transformers/pr_15682/en/model_doc/roberta#transformers.RobertaModel"),c(FR,"href","/docs/transformers/pr_15682/en/model_doc/roformer#transformers.RoFormerModel"),c(CR,"href","/docs/transformers/pr_15682/en/model_doc/segformer#transformers.SegformerModel"),c(MR,"href","/docs/transformers/pr_15682/en/model_doc/sew#transformers.SEWModel"),c(ER,"href","/docs/transformers/pr_15682/en/model_doc/sew-d#transformers.SEWDModel"),c(yR,"href","/docs/transformers/pr_15682/en/model_doc/speech_to_text#transformers.Speech2TextModel"),c(wR,"href","/docs/transformers/pr_15682/en/model_doc/splinter#transformers.SplinterModel"),c(AR,"href","/docs/transformers/pr_15682/en/model_doc/squeezebert#transformers.SqueezeBertModel"),c(LR,"href","/docs/transformers/pr_15682/en/model_doc/swin#transformers.SwinModel"),c(BR,"href","/docs/transformers/pr_15682/en/model_doc/t5#transformers.T5Model"),c(kR,"href","/docs/transformers/pr_15682/en/model_doc/tapas#transformers.TapasModel"),c(xR,"href","/docs/transformers/pr_15682/en/model_doc/transfo-xl#transformers.TransfoXLModel"),c(RR,"href","/docs/transformers/pr_15682/en/model_doc/unispeech#transformers.UniSpeechModel"),c(SR,"href","/docs/transformers/pr_15682/en/model_doc/unispeech-sat#transformers.UniSpeechSatModel"),c(PR,"href","/docs/transformers/pr_15682/en/model_doc/vilt#transformers.ViltModel"),c($R,"href","/docs/transformers/pr_15682/en/model_doc/vision-text-dual-encoder#transformers.VisionTextDualEncoderModel"),c(IR,"href","/docs/transformers/pr_15682/en/model_doc/visual_bert#transformers.VisualBertModel"),c(jR,"href","/docs/transformers/pr_15682/en/model_doc/vit#transformers.ViTModel"),c(NR,"href","/docs/transformers/pr_15682/en/model_doc/vit_mae#transformers.ViTMAEModel"),c(DR,"href","/docs/transformers/pr_15682/en/model_doc/wav2vec2#transformers.Wav2Vec2Model"),c(qR,"href","/docs/transformers/pr_15682/en/model_doc/wavlm#transformers.WavLMModel"),c(GR,"href","/docs/transformers/pr_15682/en/model_doc/xglm#transformers.XGLMModel"),c(OR,"href","/docs/transformers/pr_15682/en/model_doc/xlm#transformers.XLMModel"),c(XR,"href","/docs/transformers/pr_15682/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetModel"),c(zR,"href","/docs/transformers/pr_15682/en/model_doc/xlm-roberta#transformers.XLMRobertaModel"),c(VR,"href","/docs/transformers/pr_15682/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLModel"),c(WR,"href","/docs/transformers/pr_15682/en/model_doc/xlnet#transformers.XLNetModel"),c(QR,"href","/docs/transformers/pr_15682/en/model_doc/yoso#transformers.YosoModel"),c(ke,"class","docstring"),c(Vo,"class","docstring"),c(e_,"id","transformers.AutoModelForPreTraining"),c(e_,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(e_,"href","#transformers.AutoModelForPreTraining"),c(Xi,"class","relative group"),c(Dr,"class","docstring"),c(HR,"href","/docs/transformers/pr_15682/en/model_doc/albert#transformers.AlbertForPreTraining"),c(UR,"href","/docs/transformers/pr_15682/en/model_doc/bart#transformers.BartForConditionalGeneration"),c(JR,"href","/docs/transformers/pr_15682/en/model_doc/bert#transformers.BertForPreTraining"),c(YR,"href","/docs/transformers/pr_15682/en/model_doc/big_bird#transformers.BigBirdForPreTraining"),c(KR,"href","/docs/transformers/pr_15682/en/model_doc/camembert#transformers.CamembertForMaskedLM"),c(ZR,"href","/docs/transformers/pr_15682/en/model_doc/ctrl#transformers.CTRLLMHeadModel"),c(eS,"href","/docs/transformers/pr_15682/en/model_doc/deberta#transformers.DebertaForMaskedLM"),c(oS,"href","/docs/transformers/pr_15682/en/model_doc/deberta-v2#transformers.DebertaV2ForMaskedLM"),c(rS,"href","/docs/transformers/pr_15682/en/model_doc/distilbert#transformers.DistilBertForMaskedLM"),c(tS,"href","/docs/transformers/pr_15682/en/model_doc/electra#transformers.ElectraForPreTraining"),c(aS,"href","/docs/transformers/pr_15682/en/model_doc/flaubert#transformers.FlaubertWithLMHeadModel"),c(nS,"href","/docs/transformers/pr_15682/en/model_doc/fnet#transformers.FNetForPreTraining"),c(sS,"href","/docs/transformers/pr_15682/en/model_doc/fsmt#transformers.FSMTForConditionalGeneration"),c(lS,"href","/docs/transformers/pr_15682/en/model_doc/funnel#transformers.FunnelForPreTraining"),c(iS,"href","/docs/transformers/pr_15682/en/model_doc/gpt2#transformers.GPT2LMHeadModel"),c(dS,"href","/docs/transformers/pr_15682/en/model_doc/ibert#transformers.IBertForMaskedLM"),c(cS,"href","/docs/transformers/pr_15682/en/model_doc/layoutlm#transformers.LayoutLMForMaskedLM"),c(fS,"href","/docs/transformers/pr_15682/en/model_doc/longformer#transformers.LongformerForMaskedLM"),c(mS,"href","/docs/transformers/pr_15682/en/model_doc/lxmert#transformers.LxmertForPreTraining"),c(gS,"href","/docs/transformers/pr_15682/en/model_doc/megatron-bert#transformers.MegatronBertForPreTraining"),c(hS,"href","/docs/transformers/pr_15682/en/model_doc/mobilebert#transformers.MobileBertForPreTraining"),c(pS,"href","/docs/transformers/pr_15682/en/model_doc/mpnet#transformers.MPNetForMaskedLM"),c(_S,"href","/docs/transformers/pr_15682/en/model_doc/openai-gpt#transformers.OpenAIGPTLMHeadModel"),c(uS,"href","/docs/transformers/pr_15682/en/model_doc/retribert#transformers.RetriBertModel"),c(bS,"href","/docs/transformers/pr_15682/en/model_doc/roberta#transformers.RobertaForMaskedLM"),c(vS,"href","/docs/transformers/pr_15682/en/model_doc/squeezebert#transformers.SqueezeBertForMaskedLM"),c(TS,"href","/docs/transformers/pr_15682/en/model_doc/t5#transformers.T5ForConditionalGeneration"),c(FS,"href","/docs/transformers/pr_15682/en/model_doc/tapas#transformers.TapasForMaskedLM"),c(CS,"href","/docs/transformers/pr_15682/en/model_doc/transfo-xl#transformers.TransfoXLLMHeadModel"),c(MS,"href","/docs/transformers/pr_15682/en/model_doc/unispeech#transformers.UniSpeechForPreTraining"),c(ES,"href","/docs/transformers/pr_15682/en/model_doc/unispeech-sat#transformers.UniSpeechSatForPreTraining"),c(yS,"href","/docs/transformers/pr_15682/en/model_doc/visual_bert#transformers.VisualBertForPreTraining"),c(wS,"href","/docs/transformers/pr_15682/en/model_doc/vit_mae#transformers.ViTMAEForPreTraining"),c(AS,"href","/docs/transformers/pr_15682/en/model_doc/wav2vec2#transformers.Wav2Vec2ForPreTraining"),c(LS,"href","/docs/transformers/pr_15682/en/model_doc/xlm#transformers.XLMWithLMHeadModel"),c(BS,"href","/docs/transformers/pr_15682/en/model_doc/xlm-roberta#transformers.XLMRobertaForMaskedLM"),c(kS,"href","/docs/transformers/pr_15682/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForMaskedLM"),c(xS,"href","/docs/transformers/pr_15682/en/model_doc/xlnet#transformers.XLNetLMHeadModel"),c(xe,"class","docstring"),c(Wo,"class","docstring"),c(q_,"id","transformers.AutoModelForCausalLM"),c(q_,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(q_,"href","#transformers.AutoModelForCausalLM"),c(Wi,"class","relative group"),c(qr,"class","docstring"),c(RS,"href","/docs/transformers/pr_15682/en/model_doc/bart#transformers.BartForCausalLM"),c(SS,"href","/docs/transformers/pr_15682/en/model_doc/bert#transformers.BertLMHeadModel"),c(PS,"href","/docs/transformers/pr_15682/en/model_doc/bert-generation#transformers.BertGenerationDecoder"),c($S,"href","/docs/transformers/pr_15682/en/model_doc/big_bird#transformers.BigBirdForCausalLM"),c(IS,"href","/docs/transformers/pr_15682/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusForCausalLM"),c(jS,"href","/docs/transformers/pr_15682/en/model_doc/blenderbot#transformers.BlenderbotForCausalLM"),c(NS,"href","/docs/transformers/pr_15682/en/model_doc/blenderbot-small#transformers.BlenderbotSmallForCausalLM"),c(DS,"href","/docs/transformers/pr_15682/en/model_doc/camembert#transformers.CamembertForCausalLM"),c(qS,"href","/docs/transformers/pr_15682/en/model_doc/ctrl#transformers.CTRLLMHeadModel"),c(GS,"href","/docs/transformers/pr_15682/en/model_doc/electra#transformers.ElectraForCausalLM"),c(OS,"href","/docs/transformers/pr_15682/en/model_doc/gpt2#transformers.GPT2LMHeadModel"),c(XS,"href","/docs/transformers/pr_15682/en/model_doc/gpt_neo#transformers.GPTNeoForCausalLM"),c(zS,"href","/docs/transformers/pr_15682/en/model_doc/gptj#transformers.GPTJForCausalLM"),c(VS,"href","/docs/transformers/pr_15682/en/model_doc/marian#transformers.MarianForCausalLM"),c(WS,"href","/docs/transformers/pr_15682/en/model_doc/mbart#transformers.MBartForCausalLM"),c(QS,"href","/docs/transformers/pr_15682/en/model_doc/megatron-bert#transformers.MegatronBertForCausalLM"),c(HS,"href","/docs/transformers/pr_15682/en/model_doc/openai-gpt#transformers.OpenAIGPTLMHeadModel"),c(US,"href","/docs/transformers/pr_15682/en/model_doc/pegasus#transformers.PegasusForCausalLM"),c(JS,"href","/docs/transformers/pr_15682/en/model_doc/plbart#transformers.PLBartForCausalLM"),c(YS,"href","/docs/transformers/pr_15682/en/model_doc/prophetnet#transformers.ProphetNetForCausalLM"),c(KS,"href","/docs/transformers/pr_15682/en/model_doc/qdqbert#transformers.QDQBertLMHeadModel"),c(ZS,"href","/docs/transformers/pr_15682/en/model_doc/reformer#transformers.ReformerModelWithLMHead"),c(eP,"href","/docs/transformers/pr_15682/en/model_doc/rembert#transformers.RemBertForCausalLM"),c(oP,"href","/docs/transformers/pr_15682/en/model_doc/roberta#transformers.RobertaForCausalLM"),c(rP,"href","/docs/transformers/pr_15682/en/model_doc/roformer#transformers.RoFormerForCausalLM"),c(tP,"href","/docs/transformers/pr_15682/en/model_doc/speech_to_text_2#transformers.Speech2Text2ForCausalLM"),c(aP,"href","/docs/transformers/pr_15682/en/model_doc/transfo-xl#transformers.TransfoXLLMHeadModel"),c(nP,"href","/docs/transformers/pr_15682/en/model_doc/trocr#transformers.TrOCRForCausalLM"),c(sP,"href","/docs/transformers/pr_15682/en/model_doc/xglm#transformers.XGLMForCausalLM"),c(lP,"href","/docs/transformers/pr_15682/en/model_doc/xlm#transformers.XLMWithLMHeadModel"),c(iP,"href","/docs/transformers/pr_15682/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetForCausalLM"),c(dP,"href","/docs/transformers/pr_15682/en/model_doc/xlm-roberta#transformers.XLMRobertaForCausalLM"),c(cP,"href","/docs/transformers/pr_15682/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForCausalLM"),c(fP,"href","/docs/transformers/pr_15682/en/model_doc/xlnet#transformers.XLNetLMHeadModel"),c(Re,"class","docstring"),c(Qo,"class","docstring"),c(Cu,"id","transformers.AutoModelForMaskedLM"),c(Cu,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Cu,"href","#transformers.AutoModelForMaskedLM"),c(Ui,"class","relative group"),c(Gr,"class","docstring"),c(mP,"href","/docs/transformers/pr_15682/en/model_doc/albert#transformers.AlbertForMaskedLM"),c(gP,"href","/docs/transformers/pr_15682/en/model_doc/bart#transformers.BartForConditionalGeneration"),c(hP,"href","/docs/transformers/pr_15682/en/model_doc/bert#transformers.BertForMaskedLM"),c(pP,"href","/docs/transformers/pr_15682/en/model_doc/big_bird#transformers.BigBirdForMaskedLM"),c(_P,"href","/docs/transformers/pr_15682/en/model_doc/camembert#transformers.CamembertForMaskedLM"),c(uP,"href","/docs/transformers/pr_15682/en/model_doc/convbert#transformers.ConvBertForMaskedLM"),c(bP,"href","/docs/transformers/pr_15682/en/model_doc/deberta#transformers.DebertaForMaskedLM"),c(vP,"href","/docs/transformers/pr_15682/en/model_doc/deberta-v2#transformers.DebertaV2ForMaskedLM"),c(TP,"href","/docs/transformers/pr_15682/en/model_doc/distilbert#transformers.DistilBertForMaskedLM"),c(FP,"href","/docs/transformers/pr_15682/en/model_doc/electra#transformers.ElectraForMaskedLM"),c(CP,"href","/docs/transformers/pr_15682/en/model_doc/flaubert#transformers.FlaubertWithLMHeadModel"),c(MP,"href","/docs/transformers/pr_15682/en/model_doc/fnet#transformers.FNetForMaskedLM"),c(EP,"href","/docs/transformers/pr_15682/en/model_doc/funnel#transformers.FunnelForMaskedLM"),c(yP,"href","/docs/transformers/pr_15682/en/model_doc/ibert#transformers.IBertForMaskedLM"),c(wP,"href","/docs/transformers/pr_15682/en/model_doc/layoutlm#transformers.LayoutLMForMaskedLM"),c(AP,"href","/docs/transformers/pr_15682/en/model_doc/longformer#transformers.LongformerForMaskedLM"),c(LP,"href","/docs/transformers/pr_15682/en/model_doc/mbart#transformers.MBartForConditionalGeneration"),c(BP,"href","/docs/transformers/pr_15682/en/model_doc/megatron-bert#transformers.MegatronBertForMaskedLM"),c(kP,"href","/docs/transformers/pr_15682/en/model_doc/mobilebert#transformers.MobileBertForMaskedLM"),c(xP,"href","/docs/transformers/pr_15682/en/model_doc/mpnet#transformers.MPNetForMaskedLM"),c(RP,"href","/docs/transformers/pr_15682/en/model_doc/nystromformer#transformers.NystromformerForMaskedLM"),c(SP,"href","/docs/transformers/pr_15682/en/model_doc/perceiver#transformers.PerceiverForMaskedLM"),c(PP,"href","/docs/transformers/pr_15682/en/model_doc/qdqbert#transformers.QDQBertForMaskedLM"),c($P,"href","/docs/transformers/pr_15682/en/model_doc/reformer#transformers.ReformerForMaskedLM"),c(IP,"href","/docs/transformers/pr_15682/en/model_doc/rembert#transformers.RemBertForMaskedLM"),c(jP,"href","/docs/transformers/pr_15682/en/model_doc/roberta#transformers.RobertaForMaskedLM"),c(NP,"href","/docs/transformers/pr_15682/en/model_doc/roformer#transformers.RoFormerForMaskedLM"),c(DP,"href","/docs/transformers/pr_15682/en/model_doc/squeezebert#transformers.SqueezeBertForMaskedLM"),c(qP,"href","/docs/transformers/pr_15682/en/model_doc/tapas#transformers.TapasForMaskedLM"),c(GP,"href","/docs/transformers/pr_15682/en/model_doc/xlm#transformers.XLMWithLMHeadModel"),c(OP,"href","/docs/transformers/pr_15682/en/model_doc/xlm-roberta#transformers.XLMRobertaForMaskedLM"),c(XP,"href","/docs/transformers/pr_15682/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForMaskedLM"),c(zP,"href","/docs/transformers/pr_15682/en/model_doc/yoso#transformers.YosoForMaskedLM"),c(Se,"class","docstring"),c(Ho,"class","docstring"),c(a2,"id","transformers.AutoModelForSeq2SeqLM"),c(a2,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(a2,"href","#transformers.AutoModelForSeq2SeqLM"),c(Ki,"class","relative group"),c(Or,"class","docstring"),c(VP,"href","/docs/transformers/pr_15682/en/model_doc/bart#transformers.BartForConditionalGeneration"),c(WP,"href","/docs/transformers/pr_15682/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusForConditionalGeneration"),c(QP,"href","/docs/transformers/pr_15682/en/model_doc/blenderbot#transformers.BlenderbotForConditionalGeneration"),c(HP,"href","/docs/transformers/pr_15682/en/model_doc/blenderbot-small#transformers.BlenderbotSmallForConditionalGeneration"),c(UP,"href","/docs/transformers/pr_15682/en/model_doc/encoder-decoder#transformers.EncoderDecoderModel"),c(JP,"href","/docs/transformers/pr_15682/en/model_doc/fsmt#transformers.FSMTForConditionalGeneration"),c(YP,"href","/docs/transformers/pr_15682/en/model_doc/led#transformers.LEDForConditionalGeneration"),c(KP,"href","/docs/transformers/pr_15682/en/model_doc/m2m_100#transformers.M2M100ForConditionalGeneration"),c(ZP,"href","/docs/transformers/pr_15682/en/model_doc/marian#transformers.MarianMTModel"),c(e$,"href","/docs/transformers/pr_15682/en/model_doc/mbart#transformers.MBartForConditionalGeneration"),c(o$,"href","/docs/transformers/pr_15682/en/model_doc/mt5#transformers.MT5ForConditionalGeneration"),c(r$,"href","/docs/transformers/pr_15682/en/model_doc/pegasus#transformers.PegasusForConditionalGeneration"),c(t$,"href","/docs/transformers/pr_15682/en/model_doc/plbart#transformers.PLBartForConditionalGeneration"),c(a$,"href","/docs/transformers/pr_15682/en/model_doc/prophetnet#transformers.ProphetNetForConditionalGeneration"),c(n$,"href","/docs/transformers/pr_15682/en/model_doc/t5#transformers.T5ForConditionalGeneration"),c(s$,"href","/docs/transformers/pr_15682/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetForConditionalGeneration"),c(Pe,"class","docstring"),c(Uo,"class","docstring"),c(C2,"id","transformers.AutoModelForSequenceClassification"),c(C2,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(C2,"href","#transformers.AutoModelForSequenceClassification"),c(od,"class","relative group"),c(Xr,"class","docstring"),c(l$,"href","/docs/transformers/pr_15682/en/model_doc/albert#transformers.AlbertForSequenceClassification"),c(i$,"href","/docs/transformers/pr_15682/en/model_doc/bart#transformers.BartForSequenceClassification"),c(d$,"href","/docs/transformers/pr_15682/en/model_doc/bert#transformers.BertForSequenceClassification"),c(c$,"href","/docs/transformers/pr_15682/en/model_doc/big_bird#transformers.BigBirdForSequenceClassification"),c(f$,"href","/docs/transformers/pr_15682/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusForSequenceClassification"),c(m$,"href","/docs/transformers/pr_15682/en/model_doc/camembert#transformers.CamembertForSequenceClassification"),c(g$,"href","/docs/transformers/pr_15682/en/model_doc/canine#transformers.CanineForSequenceClassification"),c(h$,"href","/docs/transformers/pr_15682/en/model_doc/convbert#transformers.ConvBertForSequenceClassification"),c(p$,"href","/docs/transformers/pr_15682/en/model_doc/ctrl#transformers.CTRLForSequenceClassification"),c(_$,"href","/docs/transformers/pr_15682/en/model_doc/deberta#transformers.DebertaForSequenceClassification"),c(u$,"href","/docs/transformers/pr_15682/en/model_doc/deberta-v2#transformers.DebertaV2ForSequenceClassification"),c(b$,"href","/docs/transformers/pr_15682/en/model_doc/distilbert#transformers.DistilBertForSequenceClassification"),c(v$,"href","/docs/transformers/pr_15682/en/model_doc/electra#transformers.ElectraForSequenceClassification"),c(T$,"href","/docs/transformers/pr_15682/en/model_doc/flaubert#transformers.FlaubertForSequenceClassification"),c(F$,"href","/docs/transformers/pr_15682/en/model_doc/fnet#transformers.FNetForSequenceClassification"),c(C$,"href","/docs/transformers/pr_15682/en/model_doc/funnel#transformers.FunnelForSequenceClassification"),c(M$,"href","/docs/transformers/pr_15682/en/model_doc/gpt2#transformers.GPT2ForSequenceClassification"),c(E$,"href","/docs/transformers/pr_15682/en/model_doc/gpt_neo#transformers.GPTNeoForSequenceClassification"),c(y$,"href","/docs/transformers/pr_15682/en/model_doc/gptj#transformers.GPTJForSequenceClassification"),c(w$,"href","/docs/transformers/pr_15682/en/model_doc/ibert#transformers.IBertForSequenceClassification"),c(A$,"href","/docs/transformers/pr_15682/en/model_doc/layoutlm#transformers.LayoutLMForSequenceClassification"),c(L$,"href","/docs/transformers/pr_15682/en/model_doc/layoutlmv2#transformers.LayoutLMv2ForSequenceClassification"),c(B$,"href","/docs/transformers/pr_15682/en/model_doc/led#transformers.LEDForSequenceClassification"),c(k$,"href","/docs/transformers/pr_15682/en/model_doc/longformer#transformers.LongformerForSequenceClassification"),c(x$,"href","/docs/transformers/pr_15682/en/model_doc/mbart#transformers.MBartForSequenceClassification"),c(R$,"href","/docs/transformers/pr_15682/en/model_doc/megatron-bert#transformers.MegatronBertForSequenceClassification"),c(S$,"href","/docs/transformers/pr_15682/en/model_doc/mobilebert#transformers.MobileBertForSequenceClassification"),c(P$,"href","/docs/transformers/pr_15682/en/model_doc/mpnet#transformers.MPNetForSequenceClassification"),c($$,"href","/docs/transformers/pr_15682/en/model_doc/nystromformer#transformers.NystromformerForSequenceClassification"),c(I$,"href","/docs/transformers/pr_15682/en/model_doc/openai-gpt#transformers.OpenAIGPTForSequenceClassification"),c(j$,"href","/docs/transformers/pr_15682/en/model_doc/perceiver#transformers.PerceiverForSequenceClassification"),c(N$,"href","/docs/transformers/pr_15682/en/model_doc/plbart#transformers.PLBartForSequenceClassification"),c(D$,"href","/docs/transformers/pr_15682/en/model_doc/qdqbert#transformers.QDQBertForSequenceClassification"),c(q$,"href","/docs/transformers/pr_15682/en/model_doc/reformer#transformers.ReformerForSequenceClassification"),c(G$,"href","/docs/transformers/pr_15682/en/model_doc/rembert#transformers.RemBertForSequenceClassification"),c(O$,"href","/docs/transformers/pr_15682/en/model_doc/roberta#transformers.RobertaForSequenceClassification"),c(X$,"href","/docs/transformers/pr_15682/en/model_doc/roformer#transformers.RoFormerForSequenceClassification"),c(z$,"href","/docs/transformers/pr_15682/en/model_doc/squeezebert#transformers.SqueezeBertForSequenceClassification"),c(V$,"href","/docs/transformers/pr_15682/en/model_doc/tapas#transformers.TapasForSequenceClassification"),c(W$,"href","/docs/transformers/pr_15682/en/model_doc/transfo-xl#transformers.TransfoXLForSequenceClassification"),c(Q$,"href","/docs/transformers/pr_15682/en/model_doc/xlm#transformers.XLMForSequenceClassification"),c(H$,"href","/docs/transformers/pr_15682/en/model_doc/xlm-roberta#transformers.XLMRobertaForSequenceClassification"),c(U$,"href","/docs/transformers/pr_15682/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForSequenceClassification"),c(J$,"href","/docs/transformers/pr_15682/en/model_doc/xlnet#transformers.XLNetForSequenceClassification"),c(Y$,"href","/docs/transformers/pr_15682/en/model_doc/yoso#transformers.YosoForSequenceClassification"),c($e,"class","docstring"),c(Jo,"class","docstring"),c(p1,"id","transformers.AutoModelForMultipleChoice"),c(p1,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(p1,"href","#transformers.AutoModelForMultipleChoice"),c(ad,"class","relative group"),c(zr,"class","docstring"),c(K$,"href","/docs/transformers/pr_15682/en/model_doc/albert#transformers.AlbertForMultipleChoice"),c(Z$,"href","/docs/transformers/pr_15682/en/model_doc/bert#transformers.BertForMultipleChoice"),c(eI,"href","/docs/transformers/pr_15682/en/model_doc/big_bird#transformers.BigBirdForMultipleChoice"),c(oI,"href","/docs/transformers/pr_15682/en/model_doc/camembert#transformers.CamembertForMultipleChoice"),c(rI,"href","/docs/transformers/pr_15682/en/model_doc/canine#transformers.CanineForMultipleChoice"),c(tI,"href","/docs/transformers/pr_15682/en/model_doc/convbert#transformers.ConvBertForMultipleChoice"),c(aI,"href","/docs/transformers/pr_15682/en/model_doc/distilbert#transformers.DistilBertForMultipleChoice"),c(nI,"href","/docs/transformers/pr_15682/en/model_doc/electra#transformers.ElectraForMultipleChoice"),c(sI,"href","/docs/transformers/pr_15682/en/model_doc/flaubert#transformers.FlaubertForMultipleChoice"),c(lI,"href","/docs/transformers/pr_15682/en/model_doc/fnet#transformers.FNetForMultipleChoice"),c(iI,"href","/docs/transformers/pr_15682/en/model_doc/funnel#transformers.FunnelForMultipleChoice"),c(dI,"href","/docs/transformers/pr_15682/en/model_doc/ibert#transformers.IBertForMultipleChoice"),c(cI,"href","/docs/transformers/pr_15682/en/model_doc/longformer#transformers.LongformerForMultipleChoice"),c(fI,"href","/docs/transformers/pr_15682/en/model_doc/megatron-bert#transformers.MegatronBertForMultipleChoice"),c(mI,"href","/docs/transformers/pr_15682/en/model_doc/mobilebert#transformers.MobileBertForMultipleChoice"),c(gI,"href","/docs/transformers/pr_15682/en/model_doc/mpnet#transformers.MPNetForMultipleChoice"),c(hI,"href","/docs/transformers/pr_15682/en/model_doc/nystromformer#transformers.NystromformerForMultipleChoice"),c(pI,"href","/docs/transformers/pr_15682/en/model_doc/qdqbert#transformers.QDQBertForMultipleChoice"),c(_I,"href","/docs/transformers/pr_15682/en/model_doc/rembert#transformers.RemBertForMultipleChoice"),c(uI,"href","/docs/transformers/pr_15682/en/model_doc/roberta#transformers.RobertaForMultipleChoice"),c(bI,"href","/docs/transformers/pr_15682/en/model_doc/roformer#transformers.RoFormerForMultipleChoice"),c(vI,"href","/docs/transformers/pr_15682/en/model_doc/squeezebert#transformers.SqueezeBertForMultipleChoice"),c(TI,"href","/docs/transformers/pr_15682/en/model_doc/xlm#transformers.XLMForMultipleChoice"),c(FI,"href","/docs/transformers/pr_15682/en/model_doc/xlm-roberta#transformers.XLMRobertaForMultipleChoice"),c(CI,"href","/docs/transformers/pr_15682/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForMultipleChoice"),c(MI,"href","/docs/transformers/pr_15682/en/model_doc/xlnet#transformers.XLNetForMultipleChoice"),c(EI,"href","/docs/transformers/pr_15682/en/model_doc/yoso#transformers.YosoForMultipleChoice"),c(Ie,"class","docstring"),c(Yo,"class","docstring"),c(z1,"id","transformers.AutoModelForNextSentencePrediction"),c(z1,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(z1,"href","#transformers.AutoModelForNextSentencePrediction"),c(ld,"class","relative group"),c(Vr,"class","docstring"),c(yI,"href","/docs/transformers/pr_15682/en/model_doc/bert#transformers.BertForNextSentencePrediction"),c(wI,"href","/docs/transformers/pr_15682/en/model_doc/fnet#transformers.FNetForNextSentencePrediction"),c(AI,"href","/docs/transformers/pr_15682/en/model_doc/megatron-bert#transformers.MegatronBertForNextSentencePrediction"),c(LI,"href","/docs/transformers/pr_15682/en/model_doc/mobilebert#transformers.MobileBertForNextSentencePrediction"),c(BI,"href","/docs/transformers/pr_15682/en/model_doc/qdqbert#transformers.QDQBertForNextSentencePrediction"),c(je,"class","docstring"),c(Ko,"class","docstring"),c(Y1,"id","transformers.AutoModelForTokenClassification"),c(Y1,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Y1,"href","#transformers.AutoModelForTokenClassification"),c(cd,"class","relative group"),c(Wr,"class","docstring"),c(kI,"href","/docs/transformers/pr_15682/en/model_doc/albert#transformers.AlbertForTokenClassification"),c(xI,"href","/docs/transformers/pr_15682/en/model_doc/bert#transformers.BertForTokenClassification"),c(RI,"href","/docs/transformers/pr_15682/en/model_doc/big_bird#transformers.BigBirdForTokenClassification"),c(SI,"href","/docs/transformers/pr_15682/en/model_doc/camembert#transformers.CamembertForTokenClassification"),c(PI,"href","/docs/transformers/pr_15682/en/model_doc/canine#transformers.CanineForTokenClassification"),c($I,"href","/docs/transformers/pr_15682/en/model_doc/convbert#transformers.ConvBertForTokenClassification"),c(II,"href","/docs/transformers/pr_15682/en/model_doc/deberta#transformers.DebertaForTokenClassification"),c(jI,"href","/docs/transformers/pr_15682/en/model_doc/deberta-v2#transformers.DebertaV2ForTokenClassification"),c(NI,"href","/docs/transformers/pr_15682/en/model_doc/distilbert#transformers.DistilBertForTokenClassification"),c(DI,"href","/docs/transformers/pr_15682/en/model_doc/electra#transformers.ElectraForTokenClassification"),c(qI,"href","/docs/transformers/pr_15682/en/model_doc/flaubert#transformers.FlaubertForTokenClassification"),c(GI,"href","/docs/transformers/pr_15682/en/model_doc/fnet#transformers.FNetForTokenClassification"),c(OI,"href","/docs/transformers/pr_15682/en/model_doc/funnel#transformers.FunnelForTokenClassification"),c(XI,"href","/docs/transformers/pr_15682/en/model_doc/gpt2#transformers.GPT2ForTokenClassification"),c(zI,"href","/docs/transformers/pr_15682/en/model_doc/ibert#transformers.IBertForTokenClassification"),c(VI,"href","/docs/transformers/pr_15682/en/model_doc/layoutlm#transformers.LayoutLMForTokenClassification"),c(WI,"href","/docs/transformers/pr_15682/en/model_doc/layoutlmv2#transformers.LayoutLMv2ForTokenClassification"),c(QI,"href","/docs/transformers/pr_15682/en/model_doc/longformer#transformers.LongformerForTokenClassification"),c(HI,"href","/docs/transformers/pr_15682/en/model_doc/megatron-bert#transformers.MegatronBertForTokenClassification"),c(UI,"href","/docs/transformers/pr_15682/en/model_doc/mobilebert#transformers.MobileBertForTokenClassification"),c(JI,"href","/docs/transformers/pr_15682/en/model_doc/mpnet#transformers.MPNetForTokenClassification"),c(YI,"href","/docs/transformers/pr_15682/en/model_doc/nystromformer#transformers.NystromformerForTokenClassification"),c(KI,"href","/docs/transformers/pr_15682/en/model_doc/qdqbert#transformers.QDQBertForTokenClassification"),c(ZI,"href","/docs/transformers/pr_15682/en/model_doc/rembert#transformers.RemBertForTokenClassification"),c(ej,"href","/docs/transformers/pr_15682/en/model_doc/roberta#transformers.RobertaForTokenClassification"),c(oj,"href","/docs/transformers/pr_15682/en/model_doc/roformer#transformers.RoFormerForTokenClassification"),c(rj,"href","/docs/transformers/pr_15682/en/model_doc/squeezebert#transformers.SqueezeBertForTokenClassification"),c(tj,"href","/docs/transformers/pr_15682/en/model_doc/xlm#transformers.XLMForTokenClassification"),c(aj,"href","/docs/transformers/pr_15682/en/model_doc/xlm-roberta#transformers.XLMRobertaForTokenClassification"),c(nj,"href","/docs/transformers/pr_15682/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForTokenClassification"),c(sj,"href","/docs/transformers/pr_15682/en/model_doc/xlnet#transformers.XLNetForTokenClassification"),c(lj,"href","/docs/transformers/pr_15682/en/model_doc/yoso#transformers.YosoForTokenClassification"),c(Ne,"class","docstring"),c(Zo,"class","docstring"),c(xb,"id","transformers.AutoModelForQuestionAnswering"),c(xb,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(xb,"href","#transformers.AutoModelForQuestionAnswering"),c(gd,"class","relative group"),c(Qr,"class","docstring"),c(ij,"href","/docs/transformers/pr_15682/en/model_doc/albert#transformers.AlbertForQuestionAnswering"),c(dj,"href","/docs/transformers/pr_15682/en/model_doc/bart#transformers.BartForQuestionAnswering"),c(cj,"href","/docs/transformers/pr_15682/en/model_doc/bert#transformers.BertForQuestionAnswering"),c(fj,"href","/docs/transformers/pr_15682/en/model_doc/big_bird#transformers.BigBirdForQuestionAnswering"),c(mj,"href","/docs/transformers/pr_15682/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusForQuestionAnswering"),c(gj,"href","/docs/transformers/pr_15682/en/model_doc/camembert#transformers.CamembertForQuestionAnswering"),c(hj,"href","/docs/transformers/pr_15682/en/model_doc/canine#transformers.CanineForQuestionAnswering"),c(pj,"href","/docs/transformers/pr_15682/en/model_doc/convbert#transformers.ConvBertForQuestionAnswering"),c(_j,"href","/docs/transformers/pr_15682/en/model_doc/deberta#transformers.DebertaForQuestionAnswering"),c(uj,"href","/docs/transformers/pr_15682/en/model_doc/deberta-v2#transformers.DebertaV2ForQuestionAnswering"),c(bj,"href","/docs/transformers/pr_15682/en/model_doc/distilbert#transformers.DistilBertForQuestionAnswering"),c(vj,"href","/docs/transformers/pr_15682/en/model_doc/electra#transformers.ElectraForQuestionAnswering"),c(Tj,"href","/docs/transformers/pr_15682/en/model_doc/flaubert#transformers.FlaubertForQuestionAnsweringSimple"),c(Fj,"href","/docs/transformers/pr_15682/en/model_doc/fnet#transformers.FNetForQuestionAnswering"),c(Cj,"href","/docs/transformers/pr_15682/en/model_doc/funnel#transformers.FunnelForQuestionAnswering"),c(Mj,"href","/docs/transformers/pr_15682/en/model_doc/gptj#transformers.GPTJForQuestionAnswering"),c(Ej,"href","/docs/transformers/pr_15682/en/model_doc/ibert#transformers.IBertForQuestionAnswering"),c(yj,"href","/docs/transformers/pr_15682/en/model_doc/layoutlmv2#transformers.LayoutLMv2ForQuestionAnswering"),c(wj,"href","/docs/transformers/pr_15682/en/model_doc/led#transformers.LEDForQuestionAnswering"),c(Aj,"href","/docs/transformers/pr_15682/en/model_doc/longformer#transformers.LongformerForQuestionAnswering"),c(Lj,"href","/docs/transformers/pr_15682/en/model_doc/lxmert#transformers.LxmertForQuestionAnswering"),c(Bj,"href","/docs/transformers/pr_15682/en/model_doc/mbart#transformers.MBartForQuestionAnswering"),c(kj,"href","/docs/transformers/pr_15682/en/model_doc/megatron-bert#transformers.MegatronBertForQuestionAnswering"),c(xj,"href","/docs/transformers/pr_15682/en/model_doc/mobilebert#transformers.MobileBertForQuestionAnswering"),c(Rj,"href","/docs/transformers/pr_15682/en/model_doc/mpnet#transformers.MPNetForQuestionAnswering"),c(Sj,"href","/docs/transformers/pr_15682/en/model_doc/nystromformer#transformers.NystromformerForQuestionAnswering"),c(Pj,"href","/docs/transformers/pr_15682/en/model_doc/qdqbert#transformers.QDQBertForQuestionAnswering"),c($j,"href","/docs/transformers/pr_15682/en/model_doc/reformer#transformers.ReformerForQuestionAnswering"),c(Ij,"href","/docs/transformers/pr_15682/en/model_doc/rembert#transformers.RemBertForQuestionAnswering"),c(jj,"href","/docs/transformers/pr_15682/en/model_doc/roberta#transformers.RobertaForQuestionAnswering"),c(Nj,"href","/docs/transformers/pr_15682/en/model_doc/roformer#transformers.RoFormerForQuestionAnswering"),c(Dj,"href","/docs/transformers/pr_15682/en/model_doc/splinter#transformers.SplinterForQuestionAnswering"),c(qj,"href","/docs/transformers/pr_15682/en/model_doc/squeezebert#transformers.SqueezeBertForQuestionAnswering"),c(Gj,"href","/docs/transformers/pr_15682/en/model_doc/xlm#transformers.XLMForQuestionAnsweringSimple"),c(Oj,"href","/docs/transformers/pr_15682/en/model_doc/xlm-roberta#transformers.XLMRobertaForQuestionAnswering"),c(Xj,"href","/docs/transformers/pr_15682/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForQuestionAnswering"),c(zj,"href","/docs/transformers/pr_15682/en/model_doc/xlnet#transformers.XLNetForQuestionAnsweringSimple"),c(Vj,"href","/docs/transformers/pr_15682/en/model_doc/yoso#transformers.YosoForQuestionAnswering"),c(De,"class","docstring"),c(er,"class","docstring"),c(u5,"id","transformers.AutoModelForTableQuestionAnswering"),c(u5,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(u5,"href","#transformers.AutoModelForTableQuestionAnswering"),c(_d,"class","relative group"),c(Hr,"class","docstring"),c(Wj,"href","/docs/transformers/pr_15682/en/model_doc/tapas#transformers.TapasForQuestionAnswering"),c(qe,"class","docstring"),c(or,"class","docstring"),c(T5,"id","transformers.AutoModelForImageClassification"),c(T5,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(T5,"href","#transformers.AutoModelForImageClassification"),c(vd,"class","relative group"),c(Ur,"class","docstring"),c(Qj,"href","/docs/transformers/pr_15682/en/model_doc/beit#transformers.BeitForImageClassification"),c(Hj,"href","/docs/transformers/pr_15682/en/model_doc/convnext#transformers.ConvNextForImageClassification"),c(Uj,"href","/docs/transformers/pr_15682/en/model_doc/deit#transformers.DeiTForImageClassification"),c(Jj,"href","/docs/transformers/pr_15682/en/model_doc/deit#transformers.DeiTForImageClassificationWithTeacher"),c(Yj,"href","/docs/transformers/pr_15682/en/model_doc/imagegpt#transformers.ImageGPTForImageClassification"),c(Kj,"href","/docs/transformers/pr_15682/en/model_doc/perceiver#transformers.PerceiverForImageClassificationLearned"),c(Zj,"href","/docs/transformers/pr_15682/en/model_doc/perceiver#transformers.PerceiverForImageClassificationFourier"),c(eN,"href","/docs/transformers/pr_15682/en/model_doc/perceiver#transformers.PerceiverForImageClassificationConvProcessing"),c(oN,"href","/docs/transformers/pr_15682/en/model_doc/poolformer#transformers.PoolFormerForImageClassification"),c(rN,"href","/docs/transformers/pr_15682/en/model_doc/segformer#transformers.SegformerForImageClassification"),c(tN,"href","/docs/transformers/pr_15682/en/model_doc/swin#transformers.SwinForImageClassification"),c(aN,"href","/docs/transformers/pr_15682/en/model_doc/vit#transformers.ViTForImageClassification"),c(Ge,"class","docstring"),c(rr,"class","docstring"),c(B5,"id","transformers.AutoModelForVision2Seq"),c(B5,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(B5,"href","#transformers.AutoModelForVision2Seq"),c(Cd,"class","relative group"),c(Jr,"class","docstring"),c(nN,"href","/docs/transformers/pr_15682/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderModel"),c(Oe,"class","docstring"),c(tr,"class","docstring"),c(R5,"id","transformers.AutoModelForAudioClassification"),c(R5,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(R5,"href","#transformers.AutoModelForAudioClassification"),c(yd,"class","relative group"),c(Yr,"class","docstring"),c(sN,"href","/docs/transformers/pr_15682/en/model_doc/hubert#transformers.HubertForSequenceClassification"),c(lN,"href","/docs/transformers/pr_15682/en/model_doc/sew#transformers.SEWForSequenceClassification"),c(iN,"href","/docs/transformers/pr_15682/en/model_doc/sew-d#transformers.SEWDForSequenceClassification"),c(dN,"href","/docs/transformers/pr_15682/en/model_doc/unispeech#transformers.UniSpeechForSequenceClassification"),c(cN,"href","/docs/transformers/pr_15682/en/model_doc/unispeech-sat#transformers.UniSpeechSatForSequenceClassification"),c(fN,"href","/docs/transformers/pr_15682/en/model_doc/wav2vec2#transformers.Wav2Vec2ForSequenceClassification"),c(mN,"href","/docs/transformers/pr_15682/en/model_doc/wavlm#transformers.WavLMForSequenceClassification"),c(Xe,"class","docstring"),c(ar,"class","docstring"),c(G5,"id","transformers.AutoModelForAudioFrameClassification"),c(G5,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(G5,"href","#transformers.AutoModelForAudioFrameClassification"),c(Ld,"class","relative group"),c(Kr,"class","docstring"),c(gN,"href","/docs/transformers/pr_15682/en/model_doc/unispeech-sat#transformers.UniSpeechSatForAudioFrameClassification"),c(hN,"href","/docs/transformers/pr_15682/en/model_doc/wav2vec2#transformers.Wav2Vec2ForAudioFrameClassification"),c(pN,"href","/docs/transformers/pr_15682/en/model_doc/wavlm#transformers.WavLMForAudioFrameClassification"),c(ze,"class","docstring"),c(nr,"class","docstring"),c(W5,"id","transformers.AutoModelForCTC"),c(W5,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(W5,"href","#transformers.AutoModelForCTC"),c(Rd,"class","relative group"),c(Zr,"class","docstring"),c(_N,"href","/docs/transformers/pr_15682/en/model_doc/hubert#transformers.HubertForCTC"),c(uN,"href","/docs/transformers/pr_15682/en/model_doc/sew#transformers.SEWForCTC"),c(bN,"href","/docs/transformers/pr_15682/en/model_doc/sew-d#transformers.SEWDForCTC"),c(vN,"href","/docs/transformers/pr_15682/en/model_doc/unispeech#transformers.UniSpeechForCTC"),c(TN,"href","/docs/transformers/pr_15682/en/model_doc/unispeech-sat#transformers.UniSpeechSatForCTC"),c(FN,"href","/docs/transformers/pr_15682/en/model_doc/wav2vec2#transformers.Wav2Vec2ForCTC"),c(CN,"href","/docs/transformers/pr_15682/en/model_doc/wavlm#transformers.WavLMForCTC"),c(Ve,"class","docstring"),c(sr,"class","docstring"),c(ov,"id","transformers.AutoModelForSpeechSeq2Seq"),c(ov,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(ov,"href","#transformers.AutoModelForSpeechSeq2Seq"),c($d,"class","relative group"),c(et,"class","docstring"),c(MN,"href","/docs/transformers/pr_15682/en/model_doc/speech-encoder-decoder#transformers.SpeechEncoderDecoderModel"),c(EN,"href","/docs/transformers/pr_15682/en/model_doc/speech_to_text#transformers.Speech2TextForConditionalGeneration"),c(We,"class","docstring"),c(lr,"class","docstring"),c(nv,"id","transformers.AutoModelForAudioXVector"),c(nv,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(nv,"href","#transformers.AutoModelForAudioXVector"),c(Nd,"class","relative group"),c(ot,"class","docstring"),c(yN,"href","/docs/transformers/pr_15682/en/model_doc/unispeech-sat#transformers.UniSpeechSatForXVector"),c(wN,"href","/docs/transformers/pr_15682/en/model_doc/wav2vec2#transformers.Wav2Vec2ForXVector"),c(AN,"href","/docs/transformers/pr_15682/en/model_doc/wavlm#transformers.WavLMForXVector"),c(Qe,"class","docstring"),c(ir,"class","docstring"),c(cv,"id","transformers.AutoModelForMaskedImageModeling"),c(cv,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(cv,"href","#transformers.AutoModelForMaskedImageModeling"),c(Od,"class","relative group"),c(rt,"class","docstring"),c(LN,"href","/docs/transformers/pr_15682/en/model_doc/deit#transformers.DeiTForMaskedImageModeling"),c(BN,"href","/docs/transformers/pr_15682/en/model_doc/swin#transformers.SwinForMaskedImageModeling"),c(kN,"href","/docs/transformers/pr_15682/en/model_doc/vit#transformers.ViTForMaskedImageModeling"),c(He,"class","docstring"),c(dr,"class","docstring"),c(pv,"id","transformers.AutoModelForObjectDetection"),c(pv,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(pv,"href","#transformers.AutoModelForObjectDetection"),c(Wd,"class","relative group"),c(tt,"class","docstring"),c(xN,"href","/docs/transformers/pr_15682/en/model_doc/detr#transformers.DetrForObjectDetection"),c(Ue,"class","docstring"),c(cr,"class","docstring"),c(bv,"id","transformers.AutoModelForImageSegmentation"),c(bv,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(bv,"href","#transformers.AutoModelForImageSegmentation"),c(Ud,"class","relative group"),c(at,"class","docstring"),c(RN,"href","/docs/transformers/pr_15682/en/model_doc/detr#transformers.DetrForSegmentation"),c(Je,"class","docstring"),c(fr,"class","docstring"),c(Fv,"id","transformers.AutoModelForSemanticSegmentation"),c(Fv,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Fv,"href","#transformers.AutoModelForSemanticSegmentation"),c(Kd,"class","relative group"),c(nt,"class","docstring"),c(SN,"href","/docs/transformers/pr_15682/en/model_doc/beit#transformers.BeitForSemanticSegmentation"),c(PN,"href","/docs/transformers/pr_15682/en/model_doc/segformer#transformers.SegformerForSemanticSegmentation"),c(Ye,"class","docstring"),c(mr,"class","docstring"),c(yv,"id","transformers.TFAutoModel"),c(yv,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(yv,"href","#transformers.TFAutoModel"),c(oc,"class","relative group"),c(st,"class","docstring"),c($N,"href","/docs/transformers/pr_15682/en/model_doc/albert#transformers.TFAlbertModel"),c(IN,"href","/docs/transformers/pr_15682/en/model_doc/bart#transformers.TFBartModel"),c(jN,"href","/docs/transformers/pr_15682/en/model_doc/bert#transformers.TFBertModel"),c(NN,"href","/docs/transformers/pr_15682/en/model_doc/blenderbot#transformers.TFBlenderbotModel"),c(DN,"href","/docs/transformers/pr_15682/en/model_doc/blenderbot-small#transformers.TFBlenderbotSmallModel"),c(qN,"href","/docs/transformers/pr_15682/en/model_doc/camembert#transformers.TFCamembertModel"),c(GN,"href","/docs/transformers/pr_15682/en/model_doc/clip#transformers.TFCLIPModel"),c(ON,"href","/docs/transformers/pr_15682/en/model_doc/convbert#transformers.TFConvBertModel"),c(XN,"href","/docs/transformers/pr_15682/en/model_doc/ctrl#transformers.TFCTRLModel"),c(zN,"href","/docs/transformers/pr_15682/en/model_doc/deberta#transformers.TFDebertaModel"),c(VN,"href","/docs/transformers/pr_15682/en/model_doc/deberta-v2#transformers.TFDebertaV2Model"),c(WN,"href","/docs/transformers/pr_15682/en/model_doc/distilbert#transformers.TFDistilBertModel"),c(QN,"href","/docs/transformers/pr_15682/en/model_doc/dpr#transformers.TFDPRQuestionEncoder"),c(HN,"href","/docs/transformers/pr_15682/en/model_doc/electra#transformers.TFElectraModel"),c(UN,"href","/docs/transformers/pr_15682/en/model_doc/flaubert#transformers.TFFlaubertModel"),c(JN,"href","/docs/transformers/pr_15682/en/model_doc/funnel#transformers.TFFunnelModel"),c(YN,"href","/docs/transformers/pr_15682/en/model_doc/funnel#transformers.TFFunnelBaseModel"),c(KN,"href","/docs/transformers/pr_15682/en/model_doc/gpt2#transformers.TFGPT2Model"),c(ZN,"href","/docs/transformers/pr_15682/en/model_doc/hubert#transformers.TFHubertModel"),c(eD,"href","/docs/transformers/pr_15682/en/model_doc/layoutlm#transformers.TFLayoutLMModel"),c(oD,"href","/docs/transformers/pr_15682/en/model_doc/led#transformers.TFLEDModel"),c(rD,"href","/docs/transformers/pr_15682/en/model_doc/longformer#transformers.TFLongformerModel"),c(tD,"href","/docs/transformers/pr_15682/en/model_doc/lxmert#transformers.TFLxmertModel"),c(aD,"href","/docs/transformers/pr_15682/en/model_doc/marian#transformers.TFMarianModel"),c(nD,"href","/docs/transformers/pr_15682/en/model_doc/mbart#transformers.TFMBartModel"),c(sD,"href","/docs/transformers/pr_15682/en/model_doc/mobilebert#transformers.TFMobileBertModel"),c(lD,"href","/docs/transformers/pr_15682/en/model_doc/mpnet#transformers.TFMPNetModel"),c(iD,"href","/docs/transformers/pr_15682/en/model_doc/mt5#transformers.TFMT5Model"),c(dD,"href","/docs/transformers/pr_15682/en/model_doc/openai-gpt#transformers.TFOpenAIGPTModel"),c(cD,"href","/docs/transformers/pr_15682/en/model_doc/pegasus#transformers.TFPegasusModel"),c(fD,"href","/docs/transformers/pr_15682/en/model_doc/rembert#transformers.TFRemBertModel"),c(mD,"href","/docs/transformers/pr_15682/en/model_doc/roberta#transformers.TFRobertaModel"),c(gD,"href","/docs/transformers/pr_15682/en/model_doc/roformer#transformers.TFRoFormerModel"),c(hD,"href","/docs/transformers/pr_15682/en/model_doc/speech_to_text#transformers.TFSpeech2TextModel"),c(pD,"href","/docs/transformers/pr_15682/en/model_doc/t5#transformers.TFT5Model"),c(_D,"href","/docs/transformers/pr_15682/en/model_doc/tapas#transformers.TFTapasModel"),c(uD,"href","/docs/transformers/pr_15682/en/model_doc/transfo-xl#transformers.TFTransfoXLModel"),c(bD,"href","/docs/transformers/pr_15682/en/model_doc/vit#transformers.TFViTModel"),c(vD,"href","/docs/transformers/pr_15682/en/model_doc/wav2vec2#transformers.TFWav2Vec2Model"),c(TD,"href","/docs/transformers/pr_15682/en/model_doc/xlm#transformers.TFXLMModel"),c(FD,"href","/docs/transformers/pr_15682/en/model_doc/xlm-roberta#transformers.TFXLMRobertaModel"),c(CD,"href","/docs/transformers/pr_15682/en/model_doc/xlnet#transformers.TFXLNetModel"),c(go,"class","docstring"),c(gr,"class","docstring"),c(m6,"id","transformers.TFAutoModelForPreTraining"),c(m6,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(m6,"href","#transformers.TFAutoModelForPreTraining"),c(ac,"class","relative group"),c(lt,"class","docstring"),c(MD,"href","/docs/transformers/pr_15682/en/model_doc/albert#transformers.TFAlbertForPreTraining"),c(ED,"href","/docs/transformers/pr_15682/en/model_doc/bart#transformers.TFBartForConditionalGeneration"),c(yD,"href","/docs/transformers/pr_15682/en/model_doc/bert#transformers.TFBertForPreTraining"),c(wD,"href","/docs/transformers/pr_15682/en/model_doc/camembert#transformers.TFCamembertForMaskedLM"),c(AD,"href","/docs/transformers/pr_15682/en/model_doc/ctrl#transformers.TFCTRLLMHeadModel"),c(LD,"href","/docs/transformers/pr_15682/en/model_doc/distilbert#transformers.TFDistilBertForMaskedLM"),c(BD,"href","/docs/transformers/pr_15682/en/model_doc/electra#transformers.TFElectraForPreTraining"),c(kD,"href","/docs/transformers/pr_15682/en/model_doc/flaubert#transformers.TFFlaubertWithLMHeadModel"),c(xD,"href","/docs/transformers/pr_15682/en/model_doc/funnel#transformers.TFFunnelForPreTraining"),c(RD,"href","/docs/transformers/pr_15682/en/model_doc/gpt2#transformers.TFGPT2LMHeadModel"),c(SD,"href","/docs/transformers/pr_15682/en/model_doc/layoutlm#transformers.TFLayoutLMForMaskedLM"),c(PD,"href","/docs/transformers/pr_15682/en/model_doc/lxmert#transformers.TFLxmertForPreTraining"),c($D,"href","/docs/transformers/pr_15682/en/model_doc/mobilebert#transformers.TFMobileBertForPreTraining"),c(ID,"href","/docs/transformers/pr_15682/en/model_doc/mpnet#transformers.TFMPNetForMaskedLM"),c(jD,"href","/docs/transformers/pr_15682/en/model_doc/openai-gpt#transformers.TFOpenAIGPTLMHeadModel"),c(ND,"href","/docs/transformers/pr_15682/en/model_doc/roberta#transformers.TFRobertaForMaskedLM"),c(DD,"href","/docs/transformers/pr_15682/en/model_doc/t5#transformers.TFT5ForConditionalGeneration"),c(qD,"href","/docs/transformers/pr_15682/en/model_doc/tapas#transformers.TFTapasForMaskedLM"),c(GD,"href","/docs/transformers/pr_15682/en/model_doc/transfo-xl#transformers.TFTransfoXLLMHeadModel"),c(OD,"href","/docs/transformers/pr_15682/en/model_doc/xlm#transformers.TFXLMWithLMHeadModel"),c(XD,"href","/docs/transformers/pr_15682/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForMaskedLM"),c(zD,"href","/docs/transformers/pr_15682/en/model_doc/xlnet#transformers.TFXLNetLMHeadModel"),c(ho,"class","docstring"),c(hr,"class","docstring"),c($6,"id","transformers.TFAutoModelForCausalLM"),c($6,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c($6,"href","#transformers.TFAutoModelForCausalLM"),c(lc,"class","relative group"),c(it,"class","docstring"),c(VD,"href","/docs/transformers/pr_15682/en/model_doc/bert#transformers.TFBertLMHeadModel"),c(WD,"href","/docs/transformers/pr_15682/en/model_doc/ctrl#transformers.TFCTRLLMHeadModel"),c(QD,"href","/docs/transformers/pr_15682/en/model_doc/gpt2#transformers.TFGPT2LMHeadModel"),c(HD,"href","/docs/transformers/pr_15682/en/model_doc/openai-gpt#transformers.TFOpenAIGPTLMHeadModel"),c(UD,"href","/docs/transformers/pr_15682/en/model_doc/rembert#transformers.TFRemBertForCausalLM"),c(JD,"href","/docs/transformers/pr_15682/en/model_doc/roberta#transformers.TFRobertaForCausalLM"),c(YD,"href","/docs/transformers/pr_15682/en/model_doc/roformer#transformers.TFRoFormerForCausalLM"),c(KD,"href","/docs/transformers/pr_15682/en/model_doc/transfo-xl#transformers.TFTransfoXLLMHeadModel"),c(ZD,"href","/docs/transformers/pr_15682/en/model_doc/xlm#transformers.TFXLMWithLMHeadModel"),c(eq,"href","/docs/transformers/pr_15682/en/model_doc/xlnet#transformers.TFXLNetLMHeadModel"),c(po,"class","docstring"),c(pr,"class","docstring"),c(W6,"id","transformers.TFAutoModelForImageClassification"),c(W6,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(W6,"href","#transformers.TFAutoModelForImageClassification"),c(cc,"class","relative group"),c(dt,"class","docstring"),c(oq,"href","/docs/transformers/pr_15682/en/model_doc/vit#transformers.TFViTForImageClassification"),c(_o,"class","docstring"),c(_r,"class","docstring"),c(H6,"id","transformers.TFAutoModelForMaskedLM"),c(H6,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(H6,"href","#transformers.TFAutoModelForMaskedLM"),c(gc,"class","relative group"),c(ct,"class","docstring"),c(rq,"href","/docs/transformers/pr_15682/en/model_doc/albert#transformers.TFAlbertForMaskedLM"),c(tq,"href","/docs/transformers/pr_15682/en/model_doc/bert#transformers.TFBertForMaskedLM"),c(aq,"href","/docs/transformers/pr_15682/en/model_doc/camembert#transformers.TFCamembertForMaskedLM"),c(nq,"href","/docs/transformers/pr_15682/en/model_doc/convbert#transformers.TFConvBertForMaskedLM"),c(sq,"href","/docs/transformers/pr_15682/en/model_doc/deberta#transformers.TFDebertaForMaskedLM"),c(lq,"href","/docs/transformers/pr_15682/en/model_doc/deberta-v2#transformers.TFDebertaV2ForMaskedLM"),c(iq,"href","/docs/transformers/pr_15682/en/model_doc/distilbert#transformers.TFDistilBertForMaskedLM"),c(dq,"href","/docs/transformers/pr_15682/en/model_doc/electra#transformers.TFElectraForMaskedLM"),c(cq,"href","/docs/transformers/pr_15682/en/model_doc/flaubert#transformers.TFFlaubertWithLMHeadModel"),c(fq,"href","/docs/transformers/pr_15682/en/model_doc/funnel#transformers.TFFunnelForMaskedLM"),c(mq,"href","/docs/transformers/pr_15682/en/model_doc/layoutlm#transformers.TFLayoutLMForMaskedLM"),c(gq,"href","/docs/transformers/pr_15682/en/model_doc/longformer#transformers.TFLongformerForMaskedLM"),c(hq,"href","/docs/transformers/pr_15682/en/model_doc/mobilebert#transformers.TFMobileBertForMaskedLM"),c(pq,"href","/docs/transformers/pr_15682/en/model_doc/mpnet#transformers.TFMPNetForMaskedLM"),c(_q,"href","/docs/transformers/pr_15682/en/model_doc/rembert#transformers.TFRemBertForMaskedLM"),c(uq,"href","/docs/transformers/pr_15682/en/model_doc/roberta#transformers.TFRobertaForMaskedLM"),c(bq,"href","/docs/transformers/pr_15682/en/model_doc/roformer#transformers.TFRoFormerForMaskedLM"),c(vq,"href","/docs/transformers/pr_15682/en/model_doc/tapas#transformers.TFTapasForMaskedLM"),c(Tq,"href","/docs/transformers/pr_15682/en/model_doc/xlm#transformers.TFXLMWithLMHeadModel"),c(Fq,"href","/docs/transformers/pr_15682/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForMaskedLM"),c(uo,"class","docstring"),c(ur,"class","docstring"),c(pT,"id","transformers.TFAutoModelForSeq2SeqLM"),c(pT,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(pT,"href","#transformers.TFAutoModelForSeq2SeqLM"),c(_c,"class","relative group"),c(ft,"class","docstring"),c(Cq,"href","/docs/transformers/pr_15682/en/model_doc/bart#transformers.TFBartForConditionalGeneration"),c(Mq,"href","/docs/transformers/pr_15682/en/model_doc/blenderbot#transformers.TFBlenderbotForConditionalGeneration"),c(Eq,"href","/docs/transformers/pr_15682/en/model_doc/blenderbot-small#transformers.TFBlenderbotSmallForConditionalGeneration"),c(yq,"href","/docs/transformers/pr_15682/en/model_doc/encoder-decoder#transformers.TFEncoderDecoderModel"),c(wq,"href","/docs/transformers/pr_15682/en/model_doc/led#transformers.TFLEDForConditionalGeneration"),c(Aq,"href","/docs/transformers/pr_15682/en/model_doc/marian#transformers.TFMarianMTModel"),c(Lq,"href","/docs/transformers/pr_15682/en/model_doc/mbart#transformers.TFMBartForConditionalGeneration"),c(Bq,"href","/docs/transformers/pr_15682/en/model_doc/mt5#transformers.TFMT5ForConditionalGeneration"),c(kq,"href","/docs/transformers/pr_15682/en/model_doc/pegasus#transformers.TFPegasusForConditionalGeneration"),c(xq,"href","/docs/transformers/pr_15682/en/model_doc/t5#transformers.TFT5ForConditionalGeneration"),c(bo,"class","docstring"),c(br,"class","docstring"),c(wT,"id","transformers.TFAutoModelForSequenceClassification"),c(wT,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(wT,"href","#transformers.TFAutoModelForSequenceClassification"),c(vc,"class","relative group"),c(mt,"class","docstring"),c(Rq,"href","/docs/transformers/pr_15682/en/model_doc/albert#transformers.TFAlbertForSequenceClassification"),c(Sq,"href","/docs/transformers/pr_15682/en/model_doc/bert#transformers.TFBertForSequenceClassification"),c(Pq,"href","/docs/transformers/pr_15682/en/model_doc/camembert#transformers.TFCamembertForSequenceClassification"),c($q,"href","/docs/transformers/pr_15682/en/model_doc/convbert#transformers.TFConvBertForSequenceClassification"),c(Iq,"href","/docs/transformers/pr_15682/en/model_doc/ctrl#transformers.TFCTRLForSequenceClassification"),c(jq,"href","/docs/transformers/pr_15682/en/model_doc/deberta#transformers.TFDebertaForSequenceClassification"),c(Nq,"href","/docs/transformers/pr_15682/en/model_doc/deberta-v2#transformers.TFDebertaV2ForSequenceClassification"),c(Dq,"href","/docs/transformers/pr_15682/en/model_doc/distilbert#transformers.TFDistilBertForSequenceClassification"),c(qq,"href","/docs/transformers/pr_15682/en/model_doc/electra#transformers.TFElectraForSequenceClassification"),c(Gq,"href","/docs/transformers/pr_15682/en/model_doc/flaubert#transformers.TFFlaubertForSequenceClassification"),c(Oq,"href","/docs/transformers/pr_15682/en/model_doc/funnel#transformers.TFFunnelForSequenceClassification"),c(Xq,"href","/docs/transformers/pr_15682/en/model_doc/gpt2#transformers.TFGPT2ForSequenceClassification"),c(zq,"href","/docs/transformers/pr_15682/en/model_doc/layoutlm#transformers.TFLayoutLMForSequenceClassification"),c(Vq,"href","/docs/transformers/pr_15682/en/model_doc/longformer#transformers.TFLongformerForSequenceClassification"),c(Wq,"href","/docs/transformers/pr_15682/en/model_doc/mobilebert#transformers.TFMobileBertForSequenceClassification"),c(Qq,"href","/docs/transformers/pr_15682/en/model_doc/mpnet#transformers.TFMPNetForSequenceClassification"),c(Hq,"href","/docs/transformers/pr_15682/en/model_doc/openai-gpt#transformers.TFOpenAIGPTForSequenceClassification"),c(Uq,"href","/docs/transformers/pr_15682/en/model_doc/rembert#transformers.TFRemBertForSequenceClassification"),c(Jq,"href","/docs/transformers/pr_15682/en/model_doc/roberta#transformers.TFRobertaForSequenceClassification"),c(Yq,"href","/docs/transformers/pr_15682/en/model_doc/roformer#transformers.TFRoFormerForSequenceClassification"),c(Kq,"href","/docs/transformers/pr_15682/en/model_doc/tapas#transformers.TFTapasForSequenceClassification"),c(Zq,"href","/docs/transformers/pr_15682/en/model_doc/transfo-xl#transformers.TFTransfoXLForSequenceClassification"),c(eG,"href","/docs/transformers/pr_15682/en/model_doc/xlm#transformers.TFXLMForSequenceClassification"),c(oG,"href","/docs/transformers/pr_15682/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForSequenceClassification"),c(rG,"href","/docs/transformers/pr_15682/en/model_doc/xlnet#transformers.TFXLNetForSequenceClassification"),c(vo,"class","docstring"),c(vr,"class","docstring"),c(KT,"id","transformers.TFAutoModelForMultipleChoice"),c(KT,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(KT,"href","#transformers.TFAutoModelForMultipleChoice"),c(Cc,"class","relative group"),c(gt,"class","docstring"),c(tG,"href","/docs/transformers/pr_15682/en/model_doc/albert#transformers.TFAlbertForMultipleChoice"),c(aG,"href","/docs/transformers/pr_15682/en/model_doc/bert#transformers.TFBertForMultipleChoice"),c(nG,"href","/docs/transformers/pr_15682/en/model_doc/camembert#transformers.TFCamembertForMultipleChoice"),c(sG,"href","/docs/transformers/pr_15682/en/model_doc/convbert#transformers.TFConvBertForMultipleChoice"),c(lG,"href","/docs/transformers/pr_15682/en/model_doc/distilbert#transformers.TFDistilBertForMultipleChoice"),c(iG,"href","/docs/transformers/pr_15682/en/model_doc/electra#transformers.TFElectraForMultipleChoice"),c(dG,"href","/docs/transformers/pr_15682/en/model_doc/flaubert#transformers.TFFlaubertForMultipleChoice"),c(cG,"href","/docs/transformers/pr_15682/en/model_doc/funnel#transformers.TFFunnelForMultipleChoice"),c(fG,"href","/docs/transformers/pr_15682/en/model_doc/longformer#transformers.TFLongformerForMultipleChoice"),c(mG,"href","/docs/transformers/pr_15682/en/model_doc/mobilebert#transformers.TFMobileBertForMultipleChoice"),c(gG,"href","/docs/transformers/pr_15682/en/model_doc/mpnet#transformers.TFMPNetForMultipleChoice"),c(hG,"href","/docs/transformers/pr_15682/en/model_doc/rembert#transformers.TFRemBertForMultipleChoice"),c(pG,"href","/docs/transformers/pr_15682/en/model_doc/roberta#transformers.TFRobertaForMultipleChoice"),c(_G,"href","/docs/transformers/pr_15682/en/model_doc/roformer#transformers.TFRoFormerForMultipleChoice"),c(uG,"href","/docs/transformers/pr_15682/en/model_doc/xlm#transformers.TFXLMForMultipleChoice"),c(bG,"href","/docs/transformers/pr_15682/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForMultipleChoice"),c(vG,"href","/docs/transformers/pr_15682/en/model_doc/xlnet#transformers.TFXLNetForMultipleChoice"),c(To,"class","docstring"),c(Tr,"class","docstring"),c(_8,"id","transformers.TFAutoModelForTableQuestionAnswering"),c(_8,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(_8,"href","#transformers.TFAutoModelForTableQuestionAnswering"),c(yc,"class","relative group"),c(ht,"class","docstring"),c(TG,"href","/docs/transformers/pr_15682/en/model_doc/tapas#transformers.TFTapasForQuestionAnswering"),c(Fo,"class","docstring"),c(Fr,"class","docstring"),c(b8,"id","transformers.TFAutoModelForTokenClassification"),c(b8,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(b8,"href","#transformers.TFAutoModelForTokenClassification"),c(Lc,"class","relative group"),c(pt,"class","docstring"),c(FG,"href","/docs/transformers/pr_15682/en/model_doc/albert#transformers.TFAlbertForTokenClassification"),c(CG,"href","/docs/transformers/pr_15682/en/model_doc/bert#transformers.TFBertForTokenClassification"),c(MG,"href","/docs/transformers/pr_15682/en/model_doc/camembert#transformers.TFCamembertForTokenClassification"),c(EG,"href","/docs/transformers/pr_15682/en/model_doc/convbert#transformers.TFConvBertForTokenClassification"),c(yG,"href","/docs/transformers/pr_15682/en/model_doc/deberta#transformers.TFDebertaForTokenClassification"),c(wG,"href","/docs/transformers/pr_15682/en/model_doc/deberta-v2#transformers.TFDebertaV2ForTokenClassification"),c(AG,"href","/docs/transformers/pr_15682/en/model_doc/distilbert#transformers.TFDistilBertForTokenClassification"),c(LG,"href","/docs/transformers/pr_15682/en/model_doc/electra#transformers.TFElectraForTokenClassification"),c(BG,"href","/docs/transformers/pr_15682/en/model_doc/flaubert#transformers.TFFlaubertForTokenClassification"),c(kG,"href","/docs/transformers/pr_15682/en/model_doc/funnel#transformers.TFFunnelForTokenClassification"),c(xG,"href","/docs/transformers/pr_15682/en/model_doc/layoutlm#transformers.TFLayoutLMForTokenClassification"),c(RG,"href","/docs/transformers/pr_15682/en/model_doc/longformer#transformers.TFLongformerForTokenClassification"),c(SG,"href","/docs/transformers/pr_15682/en/model_doc/mobilebert#transformers.TFMobileBertForTokenClassification"),c(PG,"href","/docs/transformers/pr_15682/en/model_doc/mpnet#transformers.TFMPNetForTokenClassification"),c($G,"href","/docs/transformers/pr_15682/en/model_doc/rembert#transformers.TFRemBertForTokenClassification"),c(IG,"href","/docs/transformers/pr_15682/en/model_doc/roberta#transformers.TFRobertaForTokenClassification"),c(jG,"href","/docs/transformers/pr_15682/en/model_doc/roformer#transformers.TFRoFormerForTokenClassification"),c(NG,"href","/docs/transformers/pr_15682/en/model_doc/xlm#transformers.TFXLMForTokenClassification"),c(DG,"href","/docs/transformers/pr_15682/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForTokenClassification"),c(qG,"href","/docs/transformers/pr_15682/en/model_doc/xlnet#transformers.TFXLNetForTokenClassification"),c(Co,"class","docstring"),c(Cr,"class","docstring"),c(D8,"id","transformers.TFAutoModelForQuestionAnswering"),c(D8,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(D8,"href","#transformers.TFAutoModelForQuestionAnswering"),c(xc,"class","relative group"),c(_t,"class","docstring"),c(GG,"href","/docs/transformers/pr_15682/en/model_doc/albert#transformers.TFAlbertForQuestionAnswering"),c(OG,"href","/docs/transformers/pr_15682/en/model_doc/bert#transformers.TFBertForQuestionAnswering"),c(XG,"href","/docs/transformers/pr_15682/en/model_doc/camembert#transformers.TFCamembertForQuestionAnswering"),c(zG,"href","/docs/transformers/pr_15682/en/model_doc/convbert#transformers.TFConvBertForQuestionAnswering"),c(VG,"href","/docs/transformers/pr_15682/en/model_doc/deberta#transformers.TFDebertaForQuestionAnswering"),c(WG,"href","/docs/transformers/pr_15682/en/model_doc/deberta-v2#transformers.TFDebertaV2ForQuestionAnswering"),c(QG,"href","/docs/transformers/pr_15682/en/model_doc/distilbert#transformers.TFDistilBertForQuestionAnswering"),c(HG,"href","/docs/transformers/pr_15682/en/model_doc/electra#transformers.TFElectraForQuestionAnswering"),c(UG,"href","/docs/transformers/pr_15682/en/model_doc/flaubert#transformers.TFFlaubertForQuestionAnsweringSimple"),c(JG,"href","/docs/transformers/pr_15682/en/model_doc/funnel#transformers.TFFunnelForQuestionAnswering"),c(YG,"href","/docs/transformers/pr_15682/en/model_doc/longformer#transformers.TFLongformerForQuestionAnswering"),c(KG,"href","/docs/transformers/pr_15682/en/model_doc/mobilebert#transformers.TFMobileBertForQuestionAnswering"),c(ZG,"href","/docs/transformers/pr_15682/en/model_doc/mpnet#transformers.TFMPNetForQuestionAnswering"),c(eO,"href","/docs/transformers/pr_15682/en/model_doc/rembert#transformers.TFRemBertForQuestionAnswering"),c(oO,"href","/docs/transformers/pr_15682/en/model_doc/roberta#transformers.TFRobertaForQuestionAnswering"),c(rO,"href","/docs/transformers/pr_15682/en/model_doc/roformer#transformers.TFRoFormerForQuestionAnswering"),c(tO,"href","/docs/transformers/pr_15682/en/model_doc/xlm#transformers.TFXLMForQuestionAnsweringSimple"),c(aO,"href","/docs/transformers/pr_15682/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForQuestionAnswering"),c(nO,"href","/docs/transformers/pr_15682/en/model_doc/xlnet#transformers.TFXLNetForQuestionAnsweringSimple"),c(Mo,"class","docstring"),c(Mr,"class","docstring"),c(nF,"id","transformers.TFAutoModelForVision2Seq"),c(nF,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(nF,"href","#transformers.TFAutoModelForVision2Seq"),c(Pc,"class","relative group"),c(ut,"class","docstring"),c(sO,"href","/docs/transformers/pr_15682/en/model_doc/vision-encoder-decoder#transformers.TFVisionEncoderDecoderModel"),c(Eo,"class","docstring"),c(Er,"class","docstring"),c(lF,"id","transformers.TFAutoModelForSpeechSeq2Seq"),c(lF,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(lF,"href","#transformers.TFAutoModelForSpeechSeq2Seq"),c(jc,"class","relative group"),c(bt,"class","docstring"),c(lO,"href","/docs/transformers/pr_15682/en/model_doc/speech_to_text#transformers.TFSpeech2TextForConditionalGeneration"),c(yo,"class","docstring"),c(yr,"class","docstring"),c(dF,"id","transformers.FlaxAutoModel"),c(dF,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(dF,"href","#transformers.FlaxAutoModel"),c(qc,"class","relative group"),c(vt,"class","docstring"),c(iO,"href","/docs/transformers/pr_15682/en/model_doc/albert#transformers.FlaxAlbertModel"),c(dO,"href","/docs/transformers/pr_15682/en/model_doc/bart#transformers.FlaxBartModel"),c(cO,"href","/docs/transformers/pr_15682/en/model_doc/beit#transformers.FlaxBeitModel"),c(fO,"href","/docs/transformers/pr_15682/en/model_doc/bert#transformers.FlaxBertModel"),c(mO,"href","/docs/transformers/pr_15682/en/model_doc/big_bird#transformers.FlaxBigBirdModel"),c(gO,"href","/docs/transformers/pr_15682/en/model_doc/blenderbot#transformers.FlaxBlenderbotModel"),c(hO,"href","/docs/transformers/pr_15682/en/model_doc/blenderbot-small#transformers.FlaxBlenderbotSmallModel"),c(pO,"href","/docs/transformers/pr_15682/en/model_doc/clip#transformers.FlaxCLIPModel"),c(_O,"href","/docs/transformers/pr_15682/en/model_doc/distilbert#transformers.FlaxDistilBertModel"),c(uO,"href","/docs/transformers/pr_15682/en/model_doc/electra#transformers.FlaxElectraModel"),c(bO,"href","/docs/transformers/pr_15682/en/model_doc/gpt2#transformers.FlaxGPT2Model"),c(vO,"href","/docs/transformers/pr_15682/en/model_doc/gpt_neo#transformers.FlaxGPTNeoModel"),c(TO,"href","/docs/transformers/pr_15682/en/model_doc/gptj#transformers.FlaxGPTJModel"),c(FO,"href","/docs/transformers/pr_15682/en/model_doc/marian#transformers.FlaxMarianModel"),c(CO,"href","/docs/transformers/pr_15682/en/model_doc/mbart#transformers.FlaxMBartModel"),c(MO,"href","/docs/transformers/pr_15682/en/model_doc/mt5#transformers.FlaxMT5Model"),c(EO,"href","/docs/transformers/pr_15682/en/model_doc/pegasus#transformers.FlaxPegasusModel"),c(yO,"href","/docs/transformers/pr_15682/en/model_doc/roberta#transformers.FlaxRobertaModel"),c(wO,"href","/docs/transformers/pr_15682/en/model_doc/roformer#transformers.FlaxRoFormerModel"),c(AO,"href","/docs/transformers/pr_15682/en/model_doc/t5#transformers.FlaxT5Model"),c(LO,"href","/docs/transformers/pr_15682/en/model_doc/vision-text-dual-encoder#transformers.FlaxVisionTextDualEncoderModel"),c(BO,"href","/docs/transformers/pr_15682/en/model_doc/vit#transformers.FlaxViTModel"),c(kO,"href","/docs/transformers/pr_15682/en/model_doc/wav2vec2#transformers.FlaxWav2Vec2Model"),c(xO,"href","/docs/transformers/pr_15682/en/model_doc/xglm#transformers.FlaxXGLMModel"),c(wo,"class","docstring"),c(wr,"class","docstring"),c(PF,"id","transformers.FlaxAutoModelForCausalLM"),c(PF,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(PF,"href","#transformers.FlaxAutoModelForCausalLM"),c(Xc,"class","relative group"),c(Tt,"class","docstring"),c(RO,"href","/docs/transformers/pr_15682/en/model_doc/gpt2#transformers.FlaxGPT2LMHeadModel"),c(SO,"href","/docs/transformers/pr_15682/en/model_doc/gpt_neo#transformers.FlaxGPTNeoForCausalLM"),c(PO,"href","/docs/transformers/pr_15682/en/model_doc/gptj#transformers.FlaxGPTJForCausalLM"),c($O,"href","/docs/transformers/pr_15682/en/model_doc/xglm#transformers.FlaxXGLMForCausalLM"),c(Ao,"class","docstring"),c(Ar,"class","docstring"),c(DF,"id","transformers.FlaxAutoModelForPreTraining"),c(DF,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(DF,"href","#transformers.FlaxAutoModelForPreTraining"),c(Wc,"class","relative group"),c(Ft,"class","docstring"),c(IO,"href","/docs/transformers/pr_15682/en/model_doc/albert#transformers.FlaxAlbertForPreTraining"),c(jO,"href","/docs/transformers/pr_15682/en/model_doc/bart#transformers.FlaxBartForConditionalGeneration"),c(NO,"href","/docs/transformers/pr_15682/en/model_doc/bert#transformers.FlaxBertForPreTraining"),c(DO,"href","/docs/transformers/pr_15682/en/model_doc/big_bird#transformers.FlaxBigBirdForPreTraining"),c(qO,"href","/docs/transformers/pr_15682/en/model_doc/electra#transformers.FlaxElectraForPreTraining"),c(GO,"href","/docs/transformers/pr_15682/en/model_doc/mbart#transformers.FlaxMBartForConditionalGeneration"),c(OO,"href","/docs/transformers/pr_15682/en/model_doc/mt5#transformers.FlaxMT5ForConditionalGeneration"),c(XO,"href","/docs/transformers/pr_15682/en/model_doc/roberta#transformers.FlaxRobertaForMaskedLM"),c(zO,"href","/docs/transformers/pr_15682/en/model_doc/roformer#transformers.FlaxRoFormerForMaskedLM"),c(VO,"href","/docs/transformers/pr_15682/en/model_doc/t5#transformers.FlaxT5ForConditionalGeneration"),c(WO,"href","/docs/transformers/pr_15682/en/model_doc/wav2vec2#transformers.FlaxWav2Vec2ForPreTraining"),c(Lo,"class","docstring"),c(Lr,"class","docstring"),c(YF,"id","transformers.FlaxAutoModelForMaskedLM"),c(YF,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(YF,"href","#transformers.FlaxAutoModelForMaskedLM"),c(Uc,"class","relative group"),c(Ct,"class","docstring"),c(QO,"href","/docs/transformers/pr_15682/en/model_doc/albert#transformers.FlaxAlbertForMaskedLM"),c(HO,"href","/docs/transformers/pr_15682/en/model_doc/bart#transformers.FlaxBartForConditionalGeneration"),c(UO,"href","/docs/transformers/pr_15682/en/model_doc/bert#transformers.FlaxBertForMaskedLM"),c(JO,"href","/docs/transformers/pr_15682/en/model_doc/big_bird#transformers.FlaxBigBirdForMaskedLM"),c(YO,"href","/docs/transformers/pr_15682/en/model_doc/distilbert#transformers.FlaxDistilBertForMaskedLM"),c(KO,"href","/docs/transformers/pr_15682/en/model_doc/electra#transformers.FlaxElectraForMaskedLM"),c(ZO,"href","/docs/transformers/pr_15682/en/model_doc/mbart#transformers.FlaxMBartForConditionalGeneration"),c(eX,"href","/docs/transformers/pr_15682/en/model_doc/roberta#transformers.FlaxRobertaForMaskedLM"),c(oX,"href","/docs/transformers/pr_15682/en/model_doc/roformer#transformers.FlaxRoFormerForMaskedLM"),c(Bo,"class","docstring"),c(Br,"class","docstring"),c(lC,"id","transformers.FlaxAutoModelForSeq2SeqLM"),c(lC,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(lC,"href","#transformers.FlaxAutoModelForSeq2SeqLM"),c(Kc,"class","relative group"),c(Mt,"class","docstring"),c(rX,"href","/docs/transformers/pr_15682/en/model_doc/bart#transformers.FlaxBartForConditionalGeneration"),c(tX,"href","/docs/transformers/pr_15682/en/model_doc/blenderbot#transformers.FlaxBlenderbotForConditionalGeneration"),c(aX,"href","/docs/transformers/pr_15682/en/model_doc/blenderbot-small#transformers.FlaxBlenderbotSmallForConditionalGeneration"),c(nX,"href","/docs/transformers/pr_15682/en/model_doc/encoder-decoder#transformers.FlaxEncoderDecoderModel"),c(sX,"href","/docs/transformers/pr_15682/en/model_doc/marian#transformers.FlaxMarianMTModel"),c(lX,"href","/docs/transformers/pr_15682/en/model_doc/mbart#transformers.FlaxMBartForConditionalGeneration"),c(iX,"href","/docs/transformers/pr_15682/en/model_doc/mt5#transformers.FlaxMT5ForConditionalGeneration"),c(dX,"href","/docs/transformers/pr_15682/en/model_doc/pegasus#transformers.FlaxPegasusForConditionalGeneration"),c(cX,"href","/docs/transformers/pr_15682/en/model_doc/t5#transformers.FlaxT5ForConditionalGeneration"),c(ko,"class","docstring"),c(kr,"class","docstring"),c(uC,"id","transformers.FlaxAutoModelForSequenceClassification"),c(uC,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(uC,"href","#transformers.FlaxAutoModelForSequenceClassification"),c(of,"class","relative group"),c(Et,"class","docstring"),c(fX,"href","/docs/transformers/pr_15682/en/model_doc/albert#transformers.FlaxAlbertForSequenceClassification"),c(mX,"href","/docs/transformers/pr_15682/en/model_doc/bart#transformers.FlaxBartForSequenceClassification"),c(gX,"href","/docs/transformers/pr_15682/en/model_doc/bert#transformers.FlaxBertForSequenceClassification"),c(hX,"href","/docs/transformers/pr_15682/en/model_doc/big_bird#transformers.FlaxBigBirdForSequenceClassification"),c(pX,"href","/docs/transformers/pr_15682/en/model_doc/distilbert#transformers.FlaxDistilBertForSequenceClassification"),c(_X,"href","/docs/transformers/pr_15682/en/model_doc/electra#transformers.FlaxElectraForSequenceClassification"),c(uX,"href","/docs/transformers/pr_15682/en/model_doc/mbart#transformers.FlaxMBartForSequenceClassification"),c(bX,"href","/docs/transformers/pr_15682/en/model_doc/roberta#transformers.FlaxRobertaForSequenceClassification"),c(vX,"href","/docs/transformers/pr_15682/en/model_doc/roformer#transformers.FlaxRoFormerForSequenceClassification"),c(xo,"class","docstring"),c(xr,"class","docstring"),c(AC,"id","transformers.FlaxAutoModelForQuestionAnswering"),c(AC,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(AC,"href","#transformers.FlaxAutoModelForQuestionAnswering"),c(af,"class","relative group"),c(yt,"class","docstring"),c(TX,"href","/docs/transformers/pr_15682/en/model_doc/albert#transformers.FlaxAlbertForQuestionAnswering"),c(FX,"href","/docs/transformers/pr_15682/en/model_doc/bart#transformers.FlaxBartForQuestionAnswering"),c(CX,"href","/docs/transformers/pr_15682/en/model_doc/bert#transformers.FlaxBertForQuestionAnswering"),c(MX,"href","/docs/transformers/pr_15682/en/model_doc/big_bird#transformers.FlaxBigBirdForQuestionAnswering"),c(EX,"href","/docs/transformers/pr_15682/en/model_doc/distilbert#transformers.FlaxDistilBertForQuestionAnswering"),c(yX,"href","/docs/transformers/pr_15682/en/model_doc/electra#transformers.FlaxElectraForQuestionAnswering"),c(wX,"href","/docs/transformers/pr_15682/en/model_doc/mbart#transformers.FlaxMBartForQuestionAnswering"),c(AX,"href","/docs/transformers/pr_15682/en/model_doc/roberta#transformers.FlaxRobertaForQuestionAnswering"),c(LX,"href","/docs/transformers/pr_15682/en/model_doc/roformer#transformers.FlaxRoFormerForQuestionAnswering"),c(Ro,"class","docstring"),c(Rr,"class","docstring"),c(jC,"id","transformers.FlaxAutoModelForTokenClassification"),c(jC,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(jC,"href","#transformers.FlaxAutoModelForTokenClassification"),c(lf,"class","relative group"),c(wt,"class","docstring"),c(BX,"href","/docs/transformers/pr_15682/en/model_doc/albert#transformers.FlaxAlbertForTokenClassification"),c(kX,"href","/docs/transformers/pr_15682/en/model_doc/bert#transformers.FlaxBertForTokenClassification"),c(xX,"href","/docs/transformers/pr_15682/en/model_doc/big_bird#transformers.FlaxBigBirdForTokenClassification"),c(RX,"href","/docs/transformers/pr_15682/en/model_doc/distilbert#transformers.FlaxDistilBertForTokenClassification"),c(SX,"href","/docs/transformers/pr_15682/en/model_doc/electra#transformers.FlaxElectraForTokenClassification"),c(PX,"href","/docs/transformers/pr_15682/en/model_doc/roberta#transformers.FlaxRobertaForTokenClassification"),c($X,"href","/docs/transformers/pr_15682/en/model_doc/roformer#transformers.FlaxRoFormerForTokenClassification"),c(So,"class","docstring"),c(Sr,"class","docstring"),c(VC,"id","transformers.FlaxAutoModelForMultipleChoice"),c(VC,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(VC,"href","#transformers.FlaxAutoModelForMultipleChoice"),c(ff,"class","relative group"),c(At,"class","docstring"),c(IX,"href","/docs/transformers/pr_15682/en/model_doc/albert#transformers.FlaxAlbertForMultipleChoice"),c(jX,"href","/docs/transformers/pr_15682/en/model_doc/bert#transformers.FlaxBertForMultipleChoice"),c(NX,"href","/docs/transformers/pr_15682/en/model_doc/big_bird#transformers.FlaxBigBirdForMultipleChoice"),c(DX,"href","/docs/transformers/pr_15682/en/model_doc/distilbert#transformers.FlaxDistilBertForMultipleChoice"),c(qX,"href","/docs/transformers/pr_15682/en/model_doc/electra#transformers.FlaxElectraForMultipleChoice"),c(GX,"href","/docs/transformers/pr_15682/en/model_doc/roberta#transformers.FlaxRobertaForMultipleChoice"),c(OX,"href","/docs/transformers/pr_15682/en/model_doc/roformer#transformers.FlaxRoFormerForMultipleChoice"),c(Po,"class","docstring"),c(Pr,"class","docstring"),c(ZC,"id","transformers.FlaxAutoModelForNextSentencePrediction"),c(ZC,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(ZC,"href","#transformers.FlaxAutoModelForNextSentencePrediction"),c(hf,"class","relative group"),c(Lt,"class","docstring"),c(XX,"href","/docs/transformers/pr_15682/en/model_doc/bert#transformers.FlaxBertForNextSentencePrediction"),c($o,"class","docstring"),c($r,"class","docstring"),c(o4,"id","transformers.FlaxAutoModelForImageClassification"),c(o4,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(o4,"href","#transformers.FlaxAutoModelForImageClassification"),c(uf,"class","relative group"),c(Bt,"class","docstring"),c(zX,"href","/docs/transformers/pr_15682/en/model_doc/beit#transformers.FlaxBeitForImageClassification"),c(VX,"href","/docs/transformers/pr_15682/en/model_doc/vit#transformers.FlaxViTForImageClassification"),c(Io,"class","docstring"),c(Ir,"class","docstring"),c(a4,"id","transformers.FlaxAutoModelForVision2Seq"),c(a4,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(a4,"href","#transformers.FlaxAutoModelForVision2Seq"),c(Tf,"class","relative group"),c(kt,"class","docstring"),c(WX,"href","/docs/transformers/pr_15682/en/model_doc/vision-encoder-decoder#transformers.FlaxVisionEncoderDecoderModel"),c(jo,"class","docstring"),c(jr,"class","docstring")},m(d,u){e(document.head,J),b(d,Ae,u),b(d,ie,u),e(ie,me),e(me,to),g(ce,to,null),e(ie,ue),e(ie,Do),e(Do,wi),b(d,Ef,u),b(d,sa,u),e(sa,Ai),e(sa,Li),e(Li,tM),e(sa,yf),b(d,ye,u),b(d,io,u),e(io,Bi),e(io,Pn),e(Pn,aM),e(io,$n),e(io,In),e(In,nM),e(io,ki),e(io,jn),e(jn,sM),e(io,xi),b(d,wf,u),g($a,d,u),b(d,co,u),b(d,ge,u),e(ge,GL),e(ge,Ri),e(Ri,OL),e(ge,XL),b(d,qo,u),b(d,Ia,u),e(Ia,zL),e(Ia,Af),e(Af,VL),e(Ia,Txe),b(d,f7e,u),b(d,Si,u),e(Si,Lf),e(Lf,DV),g(lM,DV,null),e(Si,Fxe),e(Si,qV),e(qV,Cxe),b(d,m7e,u),b(d,Nn,u),e(Nn,Mxe),e(Nn,GV),e(GV,Exe),e(Nn,yxe),e(Nn,OV),e(OV,wxe),e(Nn,Axe),b(d,g7e,u),g(iM,d,u),b(d,h7e,u),b(d,WL,u),e(WL,Lxe),b(d,p7e,u),g(Bf,d,u),b(d,_7e,u),b(d,Pi,u),e(Pi,kf),e(kf,XV),g(dM,XV,null),e(Pi,Bxe),e(Pi,zV),e(zV,kxe),b(d,u7e,u),b(d,Go,u),g(cM,Go,null),e(Go,xxe),e(Go,fM),e(fM,Rxe),e(fM,QL),e(QL,Sxe),e(fM,Pxe),e(Go,$xe),e(Go,mM),e(mM,Ixe),e(mM,VV),e(VV,jxe),e(mM,Nxe),e(Go,Dxe),e(Go,fo),g(gM,fo,null),e(fo,qxe),e(fo,WV),e(WV,Gxe),e(fo,Oxe),e(fo,$i),e($i,Xxe),e($i,QV),e(QV,zxe),e($i,Vxe),e($i,HV),e(HV,Wxe),e($i,Qxe),e(fo,Hxe),e(fo,v),e(v,xf),e(xf,UV),e(UV,Uxe),e(xf,Jxe),e(xf,HL),e(HL,Yxe),e(xf,Kxe),e(v,Zxe),e(v,Rf),e(Rf,JV),e(JV,eRe),e(Rf,oRe),e(Rf,UL),e(UL,rRe),e(Rf,tRe),e(v,aRe),e(v,Sf),e(Sf,YV),e(YV,nRe),e(Sf,sRe),e(Sf,JL),e(JL,lRe),e(Sf,iRe),e(v,dRe),e(v,Pf),e(Pf,KV),e(KV,cRe),e(Pf,fRe),e(Pf,YL),e(YL,mRe),e(Pf,gRe),e(v,hRe),e(v,$f),e($f,ZV),e(ZV,pRe),e($f,_Re),e($f,KL),e(KL,uRe),e($f,bRe),e(v,vRe),e(v,If),e(If,eW),e(eW,TRe),e(If,FRe),e(If,ZL),e(ZL,CRe),e(If,MRe),e(v,ERe),e(v,jf),e(jf,oW),e(oW,yRe),e(jf,wRe),e(jf,e7),e(e7,ARe),e(jf,LRe),e(v,BRe),e(v,Nf),e(Nf,rW),e(rW,kRe),e(Nf,xRe),e(Nf,o7),e(o7,RRe),e(Nf,SRe),e(v,PRe),e(v,Df),e(Df,tW),e(tW,$Re),e(Df,IRe),e(Df,r7),e(r7,jRe),e(Df,NRe),e(v,DRe),e(v,qf),e(qf,aW),e(aW,qRe),e(qf,GRe),e(qf,t7),e(t7,ORe),e(qf,XRe),e(v,zRe),e(v,Gf),e(Gf,nW),e(nW,VRe),e(Gf,WRe),e(Gf,a7),e(a7,QRe),e(Gf,HRe),e(v,URe),e(v,Of),e(Of,sW),e(sW,JRe),e(Of,YRe),e(Of,n7),e(n7,KRe),e(Of,ZRe),e(v,eSe),e(v,Xf),e(Xf,lW),e(lW,oSe),e(Xf,rSe),e(Xf,s7),e(s7,tSe),e(Xf,aSe),e(v,nSe),e(v,zf),e(zf,iW),e(iW,sSe),e(zf,lSe),e(zf,l7),e(l7,iSe),e(zf,dSe),e(v,cSe),e(v,Vf),e(Vf,dW),e(dW,fSe),e(Vf,mSe),e(Vf,i7),e(i7,gSe),e(Vf,hSe),e(v,pSe),e(v,Wf),e(Wf,cW),e(cW,_Se),e(Wf,uSe),e(Wf,d7),e(d7,bSe),e(Wf,vSe),e(v,TSe),e(v,Qf),e(Qf,fW),e(fW,FSe),e(Qf,CSe),e(Qf,c7),e(c7,MSe),e(Qf,ESe),e(v,ySe),e(v,Hf),e(Hf,mW),e(mW,wSe),e(Hf,ASe),e(Hf,f7),e(f7,LSe),e(Hf,BSe),e(v,kSe),e(v,Uf),e(Uf,gW),e(gW,xSe),e(Uf,RSe),e(Uf,m7),e(m7,SSe),e(Uf,PSe),e(v,$Se),e(v,Jf),e(Jf,hW),e(hW,ISe),e(Jf,jSe),e(Jf,g7),e(g7,NSe),e(Jf,DSe),e(v,qSe),e(v,Yf),e(Yf,pW),e(pW,GSe),e(Yf,OSe),e(Yf,h7),e(h7,XSe),e(Yf,zSe),e(v,VSe),e(v,Kf),e(Kf,_W),e(_W,WSe),e(Kf,QSe),e(Kf,p7),e(p7,HSe),e(Kf,USe),e(v,JSe),e(v,Zf),e(Zf,uW),e(uW,YSe),e(Zf,KSe),e(Zf,_7),e(_7,ZSe),e(Zf,ePe),e(v,oPe),e(v,em),e(em,bW),e(bW,rPe),e(em,tPe),e(em,u7),e(u7,aPe),e(em,nPe),e(v,sPe),e(v,om),e(om,vW),e(vW,lPe),e(om,iPe),e(om,b7),e(b7,dPe),e(om,cPe),e(v,fPe),e(v,rm),e(rm,TW),e(TW,mPe),e(rm,gPe),e(rm,v7),e(v7,hPe),e(rm,pPe),e(v,_Pe),e(v,tm),e(tm,FW),e(FW,uPe),e(tm,bPe),e(tm,T7),e(T7,vPe),e(tm,TPe),e(v,FPe),e(v,am),e(am,CW),e(CW,CPe),e(am,MPe),e(am,F7),e(F7,EPe),e(am,yPe),e(v,wPe),e(v,nm),e(nm,MW),e(MW,APe),e(nm,LPe),e(nm,C7),e(C7,BPe),e(nm,kPe),e(v,xPe),e(v,sm),e(sm,EW),e(EW,RPe),e(sm,SPe),e(sm,M7),e(M7,PPe),e(sm,$Pe),e(v,IPe),e(v,lm),e(lm,yW),e(yW,jPe),e(lm,NPe),e(lm,E7),e(E7,DPe),e(lm,qPe),e(v,GPe),e(v,im),e(im,wW),e(wW,OPe),e(im,XPe),e(im,y7),e(y7,zPe),e(im,VPe),e(v,WPe),e(v,dm),e(dm,AW),e(AW,QPe),e(dm,HPe),e(dm,w7),e(w7,UPe),e(dm,JPe),e(v,YPe),e(v,cm),e(cm,LW),e(LW,KPe),e(cm,ZPe),e(cm,A7),e(A7,e$e),e(cm,o$e),e(v,r$e),e(v,fm),e(fm,BW),e(BW,t$e),e(fm,a$e),e(fm,L7),e(L7,n$e),e(fm,s$e),e(v,l$e),e(v,mm),e(mm,kW),e(kW,i$e),e(mm,d$e),e(mm,B7),e(B7,c$e),e(mm,f$e),e(v,m$e),e(v,gm),e(gm,xW),e(xW,g$e),e(gm,h$e),e(gm,k7),e(k7,p$e),e(gm,_$e),e(v,u$e),e(v,hm),e(hm,RW),e(RW,b$e),e(hm,v$e),e(hm,x7),e(x7,T$e),e(hm,F$e),e(v,C$e),e(v,pm),e(pm,SW),e(SW,M$e),e(pm,E$e),e(pm,R7),e(R7,y$e),e(pm,w$e),e(v,A$e),e(v,_m),e(_m,PW),e(PW,L$e),e(_m,B$e),e(_m,S7),e(S7,k$e),e(_m,x$e),e(v,R$e),e(v,um),e(um,$W),e($W,S$e),e(um,P$e),e(um,P7),e(P7,$$e),e(um,I$e),e(v,j$e),e(v,bm),e(bm,IW),e(IW,N$e),e(bm,D$e),e(bm,$7),e($7,q$e),e(bm,G$e),e(v,O$e),e(v,vm),e(vm,jW),e(jW,X$e),e(vm,z$e),e(vm,I7),e(I7,V$e),e(vm,W$e),e(v,Q$e),e(v,Tm),e(Tm,NW),e(NW,H$e),e(Tm,U$e),e(Tm,j7),e(j7,J$e),e(Tm,Y$e),e(v,K$e),e(v,Fm),e(Fm,DW),e(DW,Z$e),e(Fm,eIe),e(Fm,N7),e(N7,oIe),e(Fm,rIe),e(v,tIe),e(v,Cm),e(Cm,qW),e(qW,aIe),e(Cm,nIe),e(Cm,D7),e(D7,sIe),e(Cm,lIe),e(v,iIe),e(v,Mm),e(Mm,GW),e(GW,dIe),e(Mm,cIe),e(Mm,q7),e(q7,fIe),e(Mm,mIe),e(v,gIe),e(v,Em),e(Em,OW),e(OW,hIe),e(Em,pIe),e(Em,G7),e(G7,_Ie),e(Em,uIe),e(v,bIe),e(v,ym),e(ym,XW),e(XW,vIe),e(ym,TIe),e(ym,O7),e(O7,FIe),e(ym,CIe),e(v,MIe),e(v,wm),e(wm,zW),e(zW,EIe),e(wm,yIe),e(wm,X7),e(X7,wIe),e(wm,AIe),e(v,LIe),e(v,Am),e(Am,VW),e(VW,BIe),e(Am,kIe),e(Am,z7),e(z7,xIe),e(Am,RIe),e(v,SIe),e(v,Lm),e(Lm,WW),e(WW,PIe),e(Lm,$Ie),e(Lm,V7),e(V7,IIe),e(Lm,jIe),e(v,NIe),e(v,Bm),e(Bm,QW),e(QW,DIe),e(Bm,qIe),e(Bm,W7),e(W7,GIe),e(Bm,OIe),e(v,XIe),e(v,km),e(km,HW),e(HW,zIe),e(km,VIe),e(km,Q7),e(Q7,WIe),e(km,QIe),e(v,HIe),e(v,xm),e(xm,UW),e(UW,UIe),e(xm,JIe),e(xm,H7),e(H7,YIe),e(xm,KIe),e(v,ZIe),e(v,Rm),e(Rm,JW),e(JW,eje),e(Rm,oje),e(Rm,U7),e(U7,rje),e(Rm,tje),e(v,aje),e(v,Sm),e(Sm,YW),e(YW,nje),e(Sm,sje),e(Sm,J7),e(J7,lje),e(Sm,ije),e(v,dje),e(v,Pm),e(Pm,KW),e(KW,cje),e(Pm,fje),e(Pm,Y7),e(Y7,mje),e(Pm,gje),e(v,hje),e(v,$m),e($m,ZW),e(ZW,pje),e($m,_je),e($m,K7),e(K7,uje),e($m,bje),e(v,vje),e(v,Im),e(Im,eQ),e(eQ,Tje),e(Im,Fje),e(Im,Z7),e(Z7,Cje),e(Im,Mje),e(v,Eje),e(v,jm),e(jm,oQ),e(oQ,yje),e(jm,wje),e(jm,e9),e(e9,Aje),e(jm,Lje),e(v,Bje),e(v,Nm),e(Nm,rQ),e(rQ,kje),e(Nm,xje),e(Nm,o9),e(o9,Rje),e(Nm,Sje),e(v,Pje),e(v,Dm),e(Dm,tQ),e(tQ,$je),e(Dm,Ije),e(Dm,r9),e(r9,jje),e(Dm,Nje),e(v,Dje),e(v,qm),e(qm,aQ),e(aQ,qje),e(qm,Gje),e(qm,t9),e(t9,Oje),e(qm,Xje),e(v,zje),e(v,Gm),e(Gm,nQ),e(nQ,Vje),e(Gm,Wje),e(Gm,a9),e(a9,Qje),e(Gm,Hje),e(v,Uje),e(v,Om),e(Om,sQ),e(sQ,Jje),e(Om,Yje),e(Om,n9),e(n9,Kje),e(Om,Zje),e(v,eNe),e(v,Xm),e(Xm,lQ),e(lQ,oNe),e(Xm,rNe),e(Xm,s9),e(s9,tNe),e(Xm,aNe),e(v,nNe),e(v,zm),e(zm,iQ),e(iQ,sNe),e(zm,lNe),e(zm,l9),e(l9,iNe),e(zm,dNe),e(v,cNe),e(v,Vm),e(Vm,dQ),e(dQ,fNe),e(Vm,mNe),e(Vm,i9),e(i9,gNe),e(Vm,hNe),e(v,pNe),e(v,Wm),e(Wm,cQ),e(cQ,_Ne),e(Wm,uNe),e(Wm,d9),e(d9,bNe),e(Wm,vNe),e(v,TNe),e(v,Qm),e(Qm,fQ),e(fQ,FNe),e(Qm,CNe),e(Qm,c9),e(c9,MNe),e(Qm,ENe),e(v,yNe),e(v,Hm),e(Hm,mQ),e(mQ,wNe),e(Hm,ANe),e(Hm,f9),e(f9,LNe),e(Hm,BNe),e(v,kNe),e(v,Um),e(Um,gQ),e(gQ,xNe),e(Um,RNe),e(Um,m9),e(m9,SNe),e(Um,PNe),e(v,$Ne),e(v,Jm),e(Jm,hQ),e(hQ,INe),e(Jm,jNe),e(Jm,g9),e(g9,NNe),e(Jm,DNe),e(v,qNe),e(v,Ym),e(Ym,pQ),e(pQ,GNe),e(Ym,ONe),e(Ym,h9),e(h9,XNe),e(Ym,zNe),e(v,VNe),e(v,Km),e(Km,_Q),e(_Q,WNe),e(Km,QNe),e(Km,p9),e(p9,HNe),e(Km,UNe),e(v,JNe),e(v,Zm),e(Zm,uQ),e(uQ,YNe),e(Zm,KNe),e(Zm,_9),e(_9,ZNe),e(Zm,eDe),e(v,oDe),e(v,eg),e(eg,bQ),e(bQ,rDe),e(eg,tDe),e(eg,u9),e(u9,aDe),e(eg,nDe),e(v,sDe),e(v,og),e(og,vQ),e(vQ,lDe),e(og,iDe),e(og,b9),e(b9,dDe),e(og,cDe),e(v,fDe),e(v,rg),e(rg,TQ),e(TQ,mDe),e(rg,gDe),e(rg,v9),e(v9,hDe),e(rg,pDe),e(v,_De),e(v,tg),e(tg,FQ),e(FQ,uDe),e(tg,bDe),e(tg,T9),e(T9,vDe),e(tg,TDe),e(v,FDe),e(v,ag),e(ag,CQ),e(CQ,CDe),e(ag,MDe),e(ag,F9),e(F9,EDe),e(ag,yDe),e(v,wDe),e(v,ng),e(ng,MQ),e(MQ,ADe),e(ng,LDe),e(ng,C9),e(C9,BDe),e(ng,kDe),e(v,xDe),e(v,sg),e(sg,EQ),e(EQ,RDe),e(sg,SDe),e(sg,M9),e(M9,PDe),e(sg,$De),e(v,IDe),e(v,lg),e(lg,yQ),e(yQ,jDe),e(lg,NDe),e(lg,E9),e(E9,DDe),e(lg,qDe),e(v,GDe),e(v,ig),e(ig,wQ),e(wQ,ODe),e(ig,XDe),e(ig,y9),e(y9,zDe),e(ig,VDe),e(v,WDe),e(v,dg),e(dg,AQ),e(AQ,QDe),e(dg,HDe),e(dg,w9),e(w9,UDe),e(dg,JDe),e(v,YDe),e(v,cg),e(cg,LQ),e(LQ,KDe),e(cg,ZDe),e(cg,A9),e(A9,eqe),e(cg,oqe),e(v,rqe),e(v,fg),e(fg,BQ),e(BQ,tqe),e(fg,aqe),e(fg,L9),e(L9,nqe),e(fg,sqe),e(v,lqe),e(v,mg),e(mg,kQ),e(kQ,iqe),e(mg,dqe),e(mg,B9),e(B9,cqe),e(mg,fqe),e(v,mqe),e(v,gg),e(gg,xQ),e(xQ,gqe),e(gg,hqe),e(gg,k9),e(k9,pqe),e(gg,_qe),e(v,uqe),e(v,hg),e(hg,RQ),e(RQ,bqe),e(hg,vqe),e(hg,x9),e(x9,Tqe),e(hg,Fqe),e(fo,Cqe),e(fo,SQ),e(SQ,Mqe),e(fo,Eqe),g(hM,fo,null),e(Go,yqe),e(Go,pg),g(pM,pg,null),e(pg,wqe),e(pg,PQ),e(PQ,Aqe),b(d,b7e,u),b(d,Ii,u),e(Ii,_g),e(_g,$Q),g(_M,$Q,null),e(Ii,Lqe),e(Ii,IQ),e(IQ,Bqe),b(d,v7e,u),b(d,Oo,u),g(uM,Oo,null),e(Oo,kqe),e(Oo,bM),e(bM,xqe),e(bM,R9),e(R9,Rqe),e(bM,Sqe),e(Oo,Pqe),e(Oo,vM),e(vM,$qe),e(vM,jQ),e(jQ,Iqe),e(vM,jqe),e(Oo,Nqe),e(Oo,mo),g(TM,mo,null),e(mo,Dqe),e(mo,NQ),e(NQ,qqe),e(mo,Gqe),e(mo,ja),e(ja,Oqe),e(ja,DQ),e(DQ,Xqe),e(ja,zqe),e(ja,qQ),e(qQ,Vqe),e(ja,Wqe),e(ja,GQ),e(GQ,Qqe),e(ja,Hqe),e(mo,Uqe),e(mo,M),e(M,Dn),e(Dn,OQ),e(OQ,Jqe),e(Dn,Yqe),e(Dn,S9),e(S9,Kqe),e(Dn,Zqe),e(Dn,P9),e(P9,eGe),e(Dn,oGe),e(M,rGe),e(M,qn),e(qn,XQ),e(XQ,tGe),e(qn,aGe),e(qn,$9),e($9,nGe),e(qn,sGe),e(qn,I9),e(I9,lGe),e(qn,iGe),e(M,dGe),e(M,Gn),e(Gn,zQ),e(zQ,cGe),e(Gn,fGe),e(Gn,j9),e(j9,mGe),e(Gn,gGe),e(Gn,N9),e(N9,hGe),e(Gn,pGe),e(M,_Ge),e(M,ug),e(ug,VQ),e(VQ,uGe),e(ug,bGe),e(ug,D9),e(D9,vGe),e(ug,TGe),e(M,FGe),e(M,On),e(On,WQ),e(WQ,CGe),e(On,MGe),e(On,q9),e(q9,EGe),e(On,yGe),e(On,G9),e(G9,wGe),e(On,AGe),e(M,LGe),e(M,bg),e(bg,QQ),e(QQ,BGe),e(bg,kGe),e(bg,O9),e(O9,xGe),e(bg,RGe),e(M,SGe),e(M,vg),e(vg,HQ),e(HQ,PGe),e(vg,$Ge),e(vg,X9),e(X9,IGe),e(vg,jGe),e(M,NGe),e(M,Tg),e(Tg,UQ),e(UQ,DGe),e(Tg,qGe),e(Tg,z9),e(z9,GGe),e(Tg,OGe),e(M,XGe),e(M,Xn),e(Xn,JQ),e(JQ,zGe),e(Xn,VGe),e(Xn,V9),e(V9,WGe),e(Xn,QGe),e(Xn,W9),e(W9,HGe),e(Xn,UGe),e(M,JGe),e(M,zn),e(zn,YQ),e(YQ,YGe),e(zn,KGe),e(zn,Q9),e(Q9,ZGe),e(zn,eOe),e(zn,H9),e(H9,oOe),e(zn,rOe),e(M,tOe),e(M,Vn),e(Vn,KQ),e(KQ,aOe),e(Vn,nOe),e(Vn,U9),e(U9,sOe),e(Vn,lOe),e(Vn,J9),e(J9,iOe),e(Vn,dOe),e(M,cOe),e(M,Fg),e(Fg,ZQ),e(ZQ,fOe),e(Fg,mOe),e(Fg,Y9),e(Y9,gOe),e(Fg,hOe),e(M,pOe),e(M,Cg),e(Cg,eH),e(eH,_Oe),e(Cg,uOe),e(Cg,K9),e(K9,bOe),e(Cg,vOe),e(M,TOe),e(M,Wn),e(Wn,oH),e(oH,FOe),e(Wn,COe),e(Wn,Z9),e(Z9,MOe),e(Wn,EOe),e(Wn,eB),e(eB,yOe),e(Wn,wOe),e(M,AOe),e(M,Mg),e(Mg,rH),e(rH,LOe),e(Mg,BOe),e(Mg,oB),e(oB,kOe),e(Mg,xOe),e(M,ROe),e(M,Qn),e(Qn,tH),e(tH,SOe),e(Qn,POe),e(Qn,rB),e(rB,$Oe),e(Qn,IOe),e(Qn,tB),e(tB,jOe),e(Qn,NOe),e(M,DOe),e(M,Hn),e(Hn,aH),e(aH,qOe),e(Hn,GOe),e(Hn,aB),e(aB,OOe),e(Hn,XOe),e(Hn,nB),e(nB,zOe),e(Hn,VOe),e(M,WOe),e(M,Un),e(Un,nH),e(nH,QOe),e(Un,HOe),e(Un,sB),e(sB,UOe),e(Un,JOe),e(Un,sH),e(sH,YOe),e(Un,KOe),e(M,ZOe),e(M,Eg),e(Eg,lH),e(lH,eXe),e(Eg,oXe),e(Eg,lB),e(lB,rXe),e(Eg,tXe),e(M,aXe),e(M,Jn),e(Jn,iH),e(iH,nXe),e(Jn,sXe),e(Jn,iB),e(iB,lXe),e(Jn,iXe),e(Jn,dB),e(dB,dXe),e(Jn,cXe),e(M,fXe),e(M,yg),e(yg,dH),e(dH,mXe),e(yg,gXe),e(yg,cB),e(cB,hXe),e(yg,pXe),e(M,_Xe),e(M,Yn),e(Yn,cH),e(cH,uXe),e(Yn,bXe),e(Yn,fB),e(fB,vXe),e(Yn,TXe),e(Yn,mB),e(mB,FXe),e(Yn,CXe),e(M,MXe),e(M,Kn),e(Kn,fH),e(fH,EXe),e(Kn,yXe),e(Kn,gB),e(gB,wXe),e(Kn,AXe),e(Kn,hB),e(hB,LXe),e(Kn,BXe),e(M,kXe),e(M,Zn),e(Zn,mH),e(mH,xXe),e(Zn,RXe),e(Zn,pB),e(pB,SXe),e(Zn,PXe),e(Zn,_B),e(_B,$Xe),e(Zn,IXe),e(M,jXe),e(M,wg),e(wg,gH),e(gH,NXe),e(wg,DXe),e(wg,uB),e(uB,qXe),e(wg,GXe),e(M,OXe),e(M,es),e(es,hH),e(hH,XXe),e(es,zXe),e(es,bB),e(bB,VXe),e(es,WXe),e(es,vB),e(vB,QXe),e(es,HXe),e(M,UXe),e(M,Ag),e(Ag,pH),e(pH,JXe),e(Ag,YXe),e(Ag,TB),e(TB,KXe),e(Ag,ZXe),e(M,eze),e(M,os),e(os,_H),e(_H,oze),e(os,rze),e(os,FB),e(FB,tze),e(os,aze),e(os,CB),e(CB,nze),e(os,sze),e(M,lze),e(M,rs),e(rs,uH),e(uH,ize),e(rs,dze),e(rs,MB),e(MB,cze),e(rs,fze),e(rs,EB),e(EB,mze),e(rs,gze),e(M,hze),e(M,ts),e(ts,bH),e(bH,pze),e(ts,_ze),e(ts,yB),e(yB,uze),e(ts,bze),e(ts,wB),e(wB,vze),e(ts,Tze),e(M,Fze),e(M,as),e(as,vH),e(vH,Cze),e(as,Mze),e(as,AB),e(AB,Eze),e(as,yze),e(as,LB),e(LB,wze),e(as,Aze),e(M,Lze),e(M,Lg),e(Lg,TH),e(TH,Bze),e(Lg,kze),e(Lg,BB),e(BB,xze),e(Lg,Rze),e(M,Sze),e(M,ns),e(ns,FH),e(FH,Pze),e(ns,$ze),e(ns,kB),e(kB,Ize),e(ns,jze),e(ns,xB),e(xB,Nze),e(ns,Dze),e(M,qze),e(M,ss),e(ss,CH),e(CH,Gze),e(ss,Oze),e(ss,RB),e(RB,Xze),e(ss,zze),e(ss,SB),e(SB,Vze),e(ss,Wze),e(M,Qze),e(M,ls),e(ls,MH),e(MH,Hze),e(ls,Uze),e(ls,PB),e(PB,Jze),e(ls,Yze),e(ls,$B),e($B,Kze),e(ls,Zze),e(M,eVe),e(M,is),e(is,EH),e(EH,oVe),e(is,rVe),e(is,IB),e(IB,tVe),e(is,aVe),e(is,jB),e(jB,nVe),e(is,sVe),e(M,lVe),e(M,ds),e(ds,yH),e(yH,iVe),e(ds,dVe),e(ds,NB),e(NB,cVe),e(ds,fVe),e(ds,DB),e(DB,mVe),e(ds,gVe),e(M,hVe),e(M,cs),e(cs,wH),e(wH,pVe),e(cs,_Ve),e(cs,qB),e(qB,uVe),e(cs,bVe),e(cs,GB),e(GB,vVe),e(cs,TVe),e(M,FVe),e(M,Bg),e(Bg,AH),e(AH,CVe),e(Bg,MVe),e(Bg,OB),e(OB,EVe),e(Bg,yVe),e(M,wVe),e(M,fs),e(fs,LH),e(LH,AVe),e(fs,LVe),e(fs,XB),e(XB,BVe),e(fs,kVe),e(fs,zB),e(zB,xVe),e(fs,RVe),e(M,SVe),e(M,kg),e(kg,BH),e(BH,PVe),e(kg,$Ve),e(kg,VB),e(VB,IVe),e(kg,jVe),e(M,NVe),e(M,xg),e(xg,kH),e(kH,DVe),e(xg,qVe),e(xg,WB),e(WB,GVe),e(xg,OVe),e(M,XVe),e(M,ms),e(ms,xH),e(xH,zVe),e(ms,VVe),e(ms,QB),e(QB,WVe),e(ms,QVe),e(ms,HB),e(HB,HVe),e(ms,UVe),e(M,JVe),e(M,gs),e(gs,RH),e(RH,YVe),e(gs,KVe),e(gs,UB),e(UB,ZVe),e(gs,eWe),e(gs,JB),e(JB,oWe),e(gs,rWe),e(M,tWe),e(M,Rg),e(Rg,SH),e(SH,aWe),e(Rg,nWe),e(Rg,YB),e(YB,sWe),e(Rg,lWe),e(M,iWe),e(M,hs),e(hs,PH),e(PH,dWe),e(hs,cWe),e(hs,KB),e(KB,fWe),e(hs,mWe),e(hs,ZB),e(ZB,gWe),e(hs,hWe),e(M,pWe),e(M,ps),e(ps,$H),e($H,_We),e(ps,uWe),e(ps,ek),e(ek,bWe),e(ps,vWe),e(ps,ok),e(ok,TWe),e(ps,FWe),e(M,CWe),e(M,_s),e(_s,IH),e(IH,MWe),e(_s,EWe),e(_s,rk),e(rk,yWe),e(_s,wWe),e(_s,tk),e(tk,AWe),e(_s,LWe),e(M,BWe),e(M,us),e(us,jH),e(jH,kWe),e(us,xWe),e(us,ak),e(ak,RWe),e(us,SWe),e(us,nk),e(nk,PWe),e(us,$We),e(M,IWe),e(M,bs),e(bs,NH),e(NH,jWe),e(bs,NWe),e(bs,sk),e(sk,DWe),e(bs,qWe),e(bs,lk),e(lk,GWe),e(bs,OWe),e(M,XWe),e(M,Sg),e(Sg,DH),e(DH,zWe),e(Sg,VWe),e(Sg,ik),e(ik,WWe),e(Sg,QWe),e(M,HWe),e(M,Pg),e(Pg,qH),e(qH,UWe),e(Pg,JWe),e(Pg,dk),e(dk,YWe),e(Pg,KWe),e(M,ZWe),e(M,$g),e($g,GH),e(GH,eQe),e($g,oQe),e($g,ck),e(ck,rQe),e($g,tQe),e(M,aQe),e(M,Ig),e(Ig,OH),e(OH,nQe),e(Ig,sQe),e(Ig,fk),e(fk,lQe),e(Ig,iQe),e(M,dQe),e(M,vs),e(vs,XH),e(XH,cQe),e(vs,fQe),e(vs,mk),e(mk,mQe),e(vs,gQe),e(vs,gk),e(gk,hQe),e(vs,pQe),e(M,_Qe),e(M,jg),e(jg,zH),e(zH,uQe),e(jg,bQe),e(jg,hk),e(hk,vQe),e(jg,TQe),e(M,FQe),e(M,Ts),e(Ts,VH),e(VH,CQe),e(Ts,MQe),e(Ts,pk),e(pk,EQe),e(Ts,yQe),e(Ts,_k),e(_k,wQe),e(Ts,AQe),e(M,LQe),e(M,Fs),e(Fs,WH),e(WH,BQe),e(Fs,kQe),e(Fs,uk),e(uk,xQe),e(Fs,RQe),e(Fs,bk),e(bk,SQe),e(Fs,PQe),e(M,$Qe),e(M,Cs),e(Cs,QH),e(QH,IQe),e(Cs,jQe),e(Cs,vk),e(vk,NQe),e(Cs,DQe),e(Cs,Tk),e(Tk,qQe),e(Cs,GQe),e(M,OQe),e(M,Ms),e(Ms,HH),e(HH,XQe),e(Ms,zQe),e(Ms,Fk),e(Fk,VQe),e(Ms,WQe),e(Ms,Ck),e(Ck,QQe),e(Ms,HQe),e(M,UQe),e(M,Es),e(Es,UH),e(UH,JQe),e(Es,YQe),e(Es,Mk),e(Mk,KQe),e(Es,ZQe),e(Es,Ek),e(Ek,eHe),e(Es,oHe),e(M,rHe),e(M,Ng),e(Ng,JH),e(JH,tHe),e(Ng,aHe),e(Ng,yk),e(yk,nHe),e(Ng,sHe),e(M,lHe),e(M,Dg),e(Dg,YH),e(YH,iHe),e(Dg,dHe),e(Dg,wk),e(wk,cHe),e(Dg,fHe),e(M,mHe),e(M,ys),e(ys,KH),e(KH,gHe),e(ys,hHe),e(ys,Ak),e(Ak,pHe),e(ys,_He),e(ys,Lk),e(Lk,uHe),e(ys,bHe),e(M,vHe),e(M,ws),e(ws,ZH),e(ZH,THe),e(ws,FHe),e(ws,Bk),e(Bk,CHe),e(ws,MHe),e(ws,kk),e(kk,EHe),e(ws,yHe),e(M,wHe),e(M,As),e(As,eU),e(eU,AHe),e(As,LHe),e(As,xk),e(xk,BHe),e(As,kHe),e(As,Rk),e(Rk,xHe),e(As,RHe),e(M,SHe),e(M,qg),e(qg,oU),e(oU,PHe),e(qg,$He),e(qg,Sk),e(Sk,IHe),e(qg,jHe),e(M,NHe),e(M,Gg),e(Gg,rU),e(rU,DHe),e(Gg,qHe),e(Gg,Pk),e(Pk,GHe),e(Gg,OHe),e(M,XHe),e(M,Og),e(Og,tU),e(tU,zHe),e(Og,VHe),e(Og,$k),e($k,WHe),e(Og,QHe),e(M,HHe),e(M,Xg),e(Xg,aU),e(aU,UHe),e(Xg,JHe),e(Xg,Ik),e(Ik,YHe),e(Xg,KHe),e(M,ZHe),e(M,Ls),e(Ls,nU),e(nU,eUe),e(Ls,oUe),e(Ls,jk),e(jk,rUe),e(Ls,tUe),e(Ls,Nk),e(Nk,aUe),e(Ls,nUe),e(M,sUe),e(M,zg),e(zg,sU),e(sU,lUe),e(zg,iUe),e(zg,Dk),e(Dk,dUe),e(zg,cUe),e(M,fUe),e(M,Vg),e(Vg,lU),e(lU,mUe),e(Vg,gUe),e(Vg,qk),e(qk,hUe),e(Vg,pUe),e(M,_Ue),e(M,Bs),e(Bs,iU),e(iU,uUe),e(Bs,bUe),e(Bs,Gk),e(Gk,vUe),e(Bs,TUe),e(Bs,Ok),e(Ok,FUe),e(Bs,CUe),e(M,MUe),e(M,ks),e(ks,dU),e(dU,EUe),e(ks,yUe),e(ks,Xk),e(Xk,wUe),e(ks,AUe),e(ks,zk),e(zk,LUe),e(ks,BUe),e(mo,kUe),e(mo,cU),e(cU,xUe),e(mo,RUe),g(FM,mo,null),e(Oo,SUe),e(Oo,Wg),g(CM,Wg,null),e(Wg,PUe),e(Wg,fU),e(fU,$Ue),b(d,T7e,u),b(d,ji,u),e(ji,Qg),e(Qg,mU),g(MM,mU,null),e(ji,IUe),e(ji,gU),e(gU,jUe),b(d,F7e,u),b(d,Xo,u),g(EM,Xo,null),e(Xo,NUe),e(Xo,yM),e(yM,DUe),e(yM,Vk),e(Vk,qUe),e(yM,GUe),e(Xo,OUe),e(Xo,wM),e(wM,XUe),e(wM,hU),e(hU,zUe),e(wM,VUe),e(Xo,WUe),e(Xo,Le),g(AM,Le,null),e(Le,QUe),e(Le,pU),e(pU,HUe),e(Le,UUe),e(Le,Na),e(Na,JUe),e(Na,_U),e(_U,YUe),e(Na,KUe),e(Na,uU),e(uU,ZUe),e(Na,eJe),e(Na,bU),e(bU,oJe),e(Na,rJe),e(Le,tJe),e(Le,se),e(se,Hg),e(Hg,vU),e(vU,aJe),e(Hg,nJe),e(Hg,Wk),e(Wk,sJe),e(Hg,lJe),e(se,iJe),e(se,Ug),e(Ug,TU),e(TU,dJe),e(Ug,cJe),e(Ug,Qk),e(Qk,fJe),e(Ug,mJe),e(se,gJe),e(se,Jg),e(Jg,FU),e(FU,hJe),e(Jg,pJe),e(Jg,Hk),e(Hk,_Je),e(Jg,uJe),e(se,bJe),e(se,Yg),e(Yg,CU),e(CU,vJe),e(Yg,TJe),e(Yg,Uk),e(Uk,FJe),e(Yg,CJe),e(se,MJe),e(se,Kg),e(Kg,MU),e(MU,EJe),e(Kg,yJe),e(Kg,Jk),e(Jk,wJe),e(Kg,AJe),e(se,LJe),e(se,Zg),e(Zg,EU),e(EU,BJe),e(Zg,kJe),e(Zg,Yk),e(Yk,xJe),e(Zg,RJe),e(se,SJe),e(se,eh),e(eh,yU),e(yU,PJe),e(eh,$Je),e(eh,Kk),e(Kk,IJe),e(eh,jJe),e(se,NJe),e(se,oh),e(oh,wU),e(wU,DJe),e(oh,qJe),e(oh,Zk),e(Zk,GJe),e(oh,OJe),e(se,XJe),e(se,rh),e(rh,AU),e(AU,zJe),e(rh,VJe),e(rh,ex),e(ex,WJe),e(rh,QJe),e(se,HJe),e(se,th),e(th,LU),e(LU,UJe),e(th,JJe),e(th,ox),e(ox,YJe),e(th,KJe),e(se,ZJe),e(se,ah),e(ah,BU),e(BU,eYe),e(ah,oYe),e(ah,rx),e(rx,rYe),e(ah,tYe),e(se,aYe),e(se,nh),e(nh,kU),e(kU,nYe),e(nh,sYe),e(nh,tx),e(tx,lYe),e(nh,iYe),e(se,dYe),e(se,sh),e(sh,xU),e(xU,cYe),e(sh,fYe),e(sh,ax),e(ax,mYe),e(sh,gYe),e(se,hYe),e(se,lh),e(lh,RU),e(RU,pYe),e(lh,_Ye),e(lh,nx),e(nx,uYe),e(lh,bYe),e(se,vYe),e(se,ih),e(ih,SU),e(SU,TYe),e(ih,FYe),e(ih,sx),e(sx,CYe),e(ih,MYe),e(Le,EYe),g(dh,Le,null),e(Le,yYe),e(Le,PU),e(PU,wYe),e(Le,AYe),g(LM,Le,null),e(Xo,LYe),e(Xo,ch),g(BM,ch,null),e(ch,BYe),e(ch,$U),e($U,kYe),b(d,C7e,u),b(d,Ni,u),e(Ni,fh),e(fh,IU),g(kM,IU,null),e(Ni,xYe),e(Ni,jU),e(jU,RYe),b(d,M7e,u),b(d,zo,u),g(xM,zo,null),e(zo,SYe),e(zo,RM),e(RM,PYe),e(RM,lx),e(lx,$Ye),e(RM,IYe),e(zo,jYe),e(zo,SM),e(SM,NYe),e(SM,NU),e(NU,DYe),e(SM,qYe),e(zo,GYe),e(zo,Be),g(PM,Be,null),e(Be,OYe),e(Be,DU),e(DU,XYe),e(Be,zYe),e(Be,Di),e(Di,VYe),e(Di,qU),e(qU,WYe),e(Di,QYe),e(Di,GU),e(GU,HYe),e(Di,UYe),e(Be,JYe),e(Be,we),e(we,mh),e(mh,OU),e(OU,YYe),e(mh,KYe),e(mh,ix),e(ix,ZYe),e(mh,eKe),e(we,oKe),e(we,gh),e(gh,XU),e(XU,rKe),e(gh,tKe),e(gh,dx),e(dx,aKe),e(gh,nKe),e(we,sKe),e(we,hh),e(hh,zU),e(zU,lKe),e(hh,iKe),e(hh,cx),e(cx,dKe),e(hh,cKe),e(we,fKe),e(we,ph),e(ph,VU),e(VU,mKe),e(ph,gKe),e(ph,fx),e(fx,hKe),e(ph,pKe),e(we,_Ke),e(we,_h),e(_h,WU),e(WU,uKe),e(_h,bKe),e(_h,mx),e(mx,vKe),e(_h,TKe),e(we,FKe),e(we,uh),e(uh,QU),e(QU,CKe),e(uh,MKe),e(uh,gx),e(gx,EKe),e(uh,yKe),e(we,wKe),e(we,bh),e(bh,HU),e(HU,AKe),e(bh,LKe),e(bh,hx),e(hx,BKe),e(bh,kKe),e(we,xKe),e(we,vh),e(vh,UU),e(UU,RKe),e(vh,SKe),e(vh,px),e(px,PKe),e(vh,$Ke),e(Be,IKe),g(Th,Be,null),e(Be,jKe),e(Be,JU),e(JU,NKe),e(Be,DKe),g($M,Be,null),e(zo,qKe),e(zo,Fh),g(IM,Fh,null),e(Fh,GKe),e(Fh,YU),e(YU,OKe),b(d,E7e,u),b(d,qi,u),e(qi,Ch),e(Ch,KU),g(jM,KU,null),e(qi,XKe),e(qi,ZU),e(ZU,zKe),b(d,y7e,u),b(d,Vo,u),g(NM,Vo,null),e(Vo,VKe),e(Vo,Gi),e(Gi,WKe),e(Gi,eJ),e(eJ,QKe),e(Gi,HKe),e(Gi,oJ),e(oJ,UKe),e(Gi,JKe),e(Vo,YKe),e(Vo,DM),e(DM,KKe),e(DM,rJ),e(rJ,ZKe),e(DM,eZe),e(Vo,oZe),e(Vo,Nr),g(qM,Nr,null),e(Nr,rZe),e(Nr,tJ),e(tJ,tZe),e(Nr,aZe),e(Nr,Oi),e(Oi,nZe),e(Oi,aJ),e(aJ,sZe),e(Oi,lZe),e(Oi,nJ),e(nJ,iZe),e(Oi,dZe),e(Nr,cZe),e(Nr,sJ),e(sJ,fZe),e(Nr,mZe),g(GM,Nr,null),e(Vo,gZe),e(Vo,ke),g(OM,ke,null),e(ke,hZe),e(ke,lJ),e(lJ,pZe),e(ke,_Ze),e(ke,Da),e(Da,uZe),e(Da,iJ),e(iJ,bZe),e(Da,vZe),e(Da,dJ),e(dJ,TZe),e(Da,FZe),e(Da,cJ),e(cJ,CZe),e(Da,MZe),e(ke,EZe),e(ke,F),e(F,Mh),e(Mh,fJ),e(fJ,yZe),e(Mh,wZe),e(Mh,_x),e(_x,AZe),e(Mh,LZe),e(F,BZe),e(F,Eh),e(Eh,mJ),e(mJ,kZe),e(Eh,xZe),e(Eh,ux),e(ux,RZe),e(Eh,SZe),e(F,PZe),e(F,yh),e(yh,gJ),e(gJ,$Ze),e(yh,IZe),e(yh,bx),e(bx,jZe),e(yh,NZe),e(F,DZe),e(F,wh),e(wh,hJ),e(hJ,qZe),e(wh,GZe),e(wh,vx),e(vx,OZe),e(wh,XZe),e(F,zZe),e(F,Ah),e(Ah,pJ),e(pJ,VZe),e(Ah,WZe),e(Ah,Tx),e(Tx,QZe),e(Ah,HZe),e(F,UZe),e(F,Lh),e(Lh,_J),e(_J,JZe),e(Lh,YZe),e(Lh,Fx),e(Fx,KZe),e(Lh,ZZe),e(F,eeo),e(F,Bh),e(Bh,uJ),e(uJ,oeo),e(Bh,reo),e(Bh,Cx),e(Cx,teo),e(Bh,aeo),e(F,neo),e(F,kh),e(kh,bJ),e(bJ,seo),e(kh,leo),e(kh,Mx),e(Mx,ieo),e(kh,deo),e(F,ceo),e(F,xh),e(xh,vJ),e(vJ,feo),e(xh,meo),e(xh,Ex),e(Ex,geo),e(xh,heo),e(F,peo),e(F,Rh),e(Rh,TJ),e(TJ,_eo),e(Rh,ueo),e(Rh,yx),e(yx,beo),e(Rh,veo),e(F,Teo),e(F,Sh),e(Sh,FJ),e(FJ,Feo),e(Sh,Ceo),e(Sh,wx),e(wx,Meo),e(Sh,Eeo),e(F,yeo),e(F,Ph),e(Ph,CJ),e(CJ,weo),e(Ph,Aeo),e(Ph,Ax),e(Ax,Leo),e(Ph,Beo),e(F,keo),e(F,$h),e($h,MJ),e(MJ,xeo),e($h,Reo),e($h,Lx),e(Lx,Seo),e($h,Peo),e(F,$eo),e(F,Ih),e(Ih,EJ),e(EJ,Ieo),e(Ih,jeo),e(Ih,Bx),e(Bx,Neo),e(Ih,Deo),e(F,qeo),e(F,jh),e(jh,yJ),e(yJ,Geo),e(jh,Oeo),e(jh,kx),e(kx,Xeo),e(jh,zeo),e(F,Veo),e(F,Nh),e(Nh,wJ),e(wJ,Weo),e(Nh,Qeo),e(Nh,xx),e(xx,Heo),e(Nh,Ueo),e(F,Jeo),e(F,Dh),e(Dh,AJ),e(AJ,Yeo),e(Dh,Keo),e(Dh,Rx),e(Rx,Zeo),e(Dh,eoo),e(F,ooo),e(F,qh),e(qh,LJ),e(LJ,roo),e(qh,too),e(qh,Sx),e(Sx,aoo),e(qh,noo),e(F,soo),e(F,Gh),e(Gh,BJ),e(BJ,loo),e(Gh,ioo),e(Gh,Px),e(Px,doo),e(Gh,coo),e(F,foo),e(F,Oh),e(Oh,kJ),e(kJ,moo),e(Oh,goo),e(Oh,$x),e($x,hoo),e(Oh,poo),e(F,_oo),e(F,Xh),e(Xh,xJ),e(xJ,uoo),e(Xh,boo),e(Xh,Ix),e(Ix,voo),e(Xh,Too),e(F,Foo),e(F,zh),e(zh,RJ),e(RJ,Coo),e(zh,Moo),e(zh,jx),e(jx,Eoo),e(zh,yoo),e(F,woo),e(F,Vh),e(Vh,SJ),e(SJ,Aoo),e(Vh,Loo),e(Vh,Nx),e(Nx,Boo),e(Vh,koo),e(F,xoo),e(F,Wh),e(Wh,PJ),e(PJ,Roo),e(Wh,Soo),e(Wh,Dx),e(Dx,Poo),e(Wh,$oo),e(F,Ioo),e(F,Qh),e(Qh,$J),e($J,joo),e(Qh,Noo),e(Qh,qx),e(qx,Doo),e(Qh,qoo),e(F,Goo),e(F,xs),e(xs,IJ),e(IJ,Ooo),e(xs,Xoo),e(xs,Gx),e(Gx,zoo),e(xs,Voo),e(xs,Ox),e(Ox,Woo),e(xs,Qoo),e(F,Hoo),e(F,Hh),e(Hh,jJ),e(jJ,Uoo),e(Hh,Joo),e(Hh,Xx),e(Xx,Yoo),e(Hh,Koo),e(F,Zoo),e(F,Uh),e(Uh,NJ),e(NJ,ero),e(Uh,oro),e(Uh,zx),e(zx,rro),e(Uh,tro),e(F,aro),e(F,Jh),e(Jh,DJ),e(DJ,nro),e(Jh,sro),e(Jh,Vx),e(Vx,lro),e(Jh,iro),e(F,dro),e(F,Yh),e(Yh,qJ),e(qJ,cro),e(Yh,fro),e(Yh,Wx),e(Wx,mro),e(Yh,gro),e(F,hro),e(F,Kh),e(Kh,GJ),e(GJ,pro),e(Kh,_ro),e(Kh,Qx),e(Qx,uro),e(Kh,bro),e(F,vro),e(F,Zh),e(Zh,OJ),e(OJ,Tro),e(Zh,Fro),e(Zh,Hx),e(Hx,Cro),e(Zh,Mro),e(F,Ero),e(F,ep),e(ep,XJ),e(XJ,yro),e(ep,wro),e(ep,Ux),e(Ux,Aro),e(ep,Lro),e(F,Bro),e(F,op),e(op,zJ),e(zJ,kro),e(op,xro),e(op,Jx),e(Jx,Rro),e(op,Sro),e(F,Pro),e(F,rp),e(rp,VJ),e(VJ,$ro),e(rp,Iro),e(rp,Yx),e(Yx,jro),e(rp,Nro),e(F,Dro),e(F,tp),e(tp,WJ),e(WJ,qro),e(tp,Gro),e(tp,Kx),e(Kx,Oro),e(tp,Xro),e(F,zro),e(F,ap),e(ap,QJ),e(QJ,Vro),e(ap,Wro),e(ap,Zx),e(Zx,Qro),e(ap,Hro),e(F,Uro),e(F,np),e(np,HJ),e(HJ,Jro),e(np,Yro),e(np,eR),e(eR,Kro),e(np,Zro),e(F,eto),e(F,sp),e(sp,UJ),e(UJ,oto),e(sp,rto),e(sp,oR),e(oR,tto),e(sp,ato),e(F,nto),e(F,lp),e(lp,JJ),e(JJ,sto),e(lp,lto),e(lp,rR),e(rR,ito),e(lp,dto),e(F,cto),e(F,ip),e(ip,YJ),e(YJ,fto),e(ip,mto),e(ip,tR),e(tR,gto),e(ip,hto),e(F,pto),e(F,dp),e(dp,KJ),e(KJ,_to),e(dp,uto),e(dp,aR),e(aR,bto),e(dp,vto),e(F,Tto),e(F,cp),e(cp,ZJ),e(ZJ,Fto),e(cp,Cto),e(cp,nR),e(nR,Mto),e(cp,Eto),e(F,yto),e(F,fp),e(fp,eY),e(eY,wto),e(fp,Ato),e(fp,sR),e(sR,Lto),e(fp,Bto),e(F,kto),e(F,mp),e(mp,oY),e(oY,xto),e(mp,Rto),e(mp,lR),e(lR,Sto),e(mp,Pto),e(F,$to),e(F,gp),e(gp,rY),e(rY,Ito),e(gp,jto),e(gp,iR),e(iR,Nto),e(gp,Dto),e(F,qto),e(F,hp),e(hp,tY),e(tY,Gto),e(hp,Oto),e(hp,dR),e(dR,Xto),e(hp,zto),e(F,Vto),e(F,pp),e(pp,aY),e(aY,Wto),e(pp,Qto),e(pp,cR),e(cR,Hto),e(pp,Uto),e(F,Jto),e(F,_p),e(_p,nY),e(nY,Yto),e(_p,Kto),e(_p,fR),e(fR,Zto),e(_p,eao),e(F,oao),e(F,up),e(up,sY),e(sY,rao),e(up,tao),e(up,mR),e(mR,aao),e(up,nao),e(F,sao),e(F,bp),e(bp,lY),e(lY,lao),e(bp,iao),e(bp,gR),e(gR,dao),e(bp,cao),e(F,fao),e(F,vp),e(vp,iY),e(iY,mao),e(vp,gao),e(vp,hR),e(hR,hao),e(vp,pao),e(F,_ao),e(F,Tp),e(Tp,dY),e(dY,uao),e(Tp,bao),e(Tp,pR),e(pR,vao),e(Tp,Tao),e(F,Fao),e(F,Fp),e(Fp,cY),e(cY,Cao),e(Fp,Mao),e(Fp,_R),e(_R,Eao),e(Fp,yao),e(F,wao),e(F,Cp),e(Cp,fY),e(fY,Aao),e(Cp,Lao),e(Cp,uR),e(uR,Bao),e(Cp,kao),e(F,xao),e(F,Mp),e(Mp,mY),e(mY,Rao),e(Mp,Sao),e(Mp,bR),e(bR,Pao),e(Mp,$ao),e(F,Iao),e(F,Ep),e(Ep,gY),e(gY,jao),e(Ep,Nao),e(Ep,vR),e(vR,Dao),e(Ep,qao),e(F,Gao),e(F,yp),e(yp,hY),e(hY,Oao),e(yp,Xao),e(yp,TR),e(TR,zao),e(yp,Vao),e(F,Wao),e(F,wp),e(wp,pY),e(pY,Qao),e(wp,Hao),e(wp,FR),e(FR,Uao),e(wp,Jao),e(F,Yao),e(F,Ap),e(Ap,_Y),e(_Y,Kao),e(Ap,Zao),e(Ap,CR),e(CR,eno),e(Ap,ono),e(F,rno),e(F,Lp),e(Lp,uY),e(uY,tno),e(Lp,ano),e(Lp,MR),e(MR,nno),e(Lp,sno),e(F,lno),e(F,Bp),e(Bp,bY),e(bY,ino),e(Bp,dno),e(Bp,ER),e(ER,cno),e(Bp,fno),e(F,mno),e(F,kp),e(kp,vY),e(vY,gno),e(kp,hno),e(kp,yR),e(yR,pno),e(kp,_no),e(F,uno),e(F,xp),e(xp,TY),e(TY,bno),e(xp,vno),e(xp,wR),e(wR,Tno),e(xp,Fno),e(F,Cno),e(F,Rp),e(Rp,FY),e(FY,Mno),e(Rp,Eno),e(Rp,AR),e(AR,yno),e(Rp,wno),e(F,Ano),e(F,Sp),e(Sp,CY),e(CY,Lno),e(Sp,Bno),e(Sp,LR),e(LR,kno),e(Sp,xno),e(F,Rno),e(F,Pp),e(Pp,MY),e(MY,Sno),e(Pp,Pno),e(Pp,BR),e(BR,$no),e(Pp,Ino),e(F,jno),e(F,$p),e($p,EY),e(EY,Nno),e($p,Dno),e($p,kR),e(kR,qno),e($p,Gno),e(F,Ono),e(F,Ip),e(Ip,yY),e(yY,Xno),e(Ip,zno),e(Ip,xR),e(xR,Vno),e(Ip,Wno),e(F,Qno),e(F,jp),e(jp,wY),e(wY,Hno),e(jp,Uno),e(jp,RR),e(RR,Jno),e(jp,Yno),e(F,Kno),e(F,Np),e(Np,AY),e(AY,Zno),e(Np,eso),e(Np,SR),e(SR,oso),e(Np,rso),e(F,tso),e(F,Dp),e(Dp,LY),e(LY,aso),e(Dp,nso),e(Dp,PR),e(PR,sso),e(Dp,lso),e(F,iso),e(F,qp),e(qp,BY),e(BY,dso),e(qp,cso),e(qp,$R),e($R,fso),e(qp,mso),e(F,gso),e(F,Gp),e(Gp,kY),e(kY,hso),e(Gp,pso),e(Gp,IR),e(IR,_so),e(Gp,uso),e(F,bso),e(F,Op),e(Op,xY),e(xY,vso),e(Op,Tso),e(Op,jR),e(jR,Fso),e(Op,Cso),e(F,Mso),e(F,Xp),e(Xp,RY),e(RY,Eso),e(Xp,yso),e(Xp,NR),e(NR,wso),e(Xp,Aso),e(F,Lso),e(F,zp),e(zp,SY),e(SY,Bso),e(zp,kso),e(zp,DR),e(DR,xso),e(zp,Rso),e(F,Sso),e(F,Vp),e(Vp,PY),e(PY,Pso),e(Vp,$so),e(Vp,qR),e(qR,Iso),e(Vp,jso),e(F,Nso),e(F,Wp),e(Wp,$Y),e($Y,Dso),e(Wp,qso),e(Wp,GR),e(GR,Gso),e(Wp,Oso),e(F,Xso),e(F,Qp),e(Qp,IY),e(IY,zso),e(Qp,Vso),e(Qp,OR),e(OR,Wso),e(Qp,Qso),e(F,Hso),e(F,Hp),e(Hp,jY),e(jY,Uso),e(Hp,Jso),e(Hp,XR),e(XR,Yso),e(Hp,Kso),e(F,Zso),e(F,Up),e(Up,NY),e(NY,elo),e(Up,olo),e(Up,zR),e(zR,rlo),e(Up,tlo),e(F,alo),e(F,Jp),e(Jp,DY),e(DY,nlo),e(Jp,slo),e(Jp,VR),e(VR,llo),e(Jp,ilo),e(F,dlo),e(F,Yp),e(Yp,qY),e(qY,clo),e(Yp,flo),e(Yp,WR),e(WR,mlo),e(Yp,glo),e(F,hlo),e(F,Kp),e(Kp,GY),e(GY,plo),e(Kp,_lo),e(Kp,QR),e(QR,ulo),e(Kp,blo),e(ke,vlo),e(ke,Zp),e(Zp,Tlo),e(Zp,OY),e(OY,Flo),e(Zp,Clo),e(Zp,XY),e(XY,Mlo),e(ke,Elo),e(ke,zY),e(zY,ylo),e(ke,wlo),g(XM,ke,null),b(d,w7e,u),b(d,Xi,u),e(Xi,e_),e(e_,VY),g(zM,VY,null),e(Xi,Alo),e(Xi,WY),e(WY,Llo),b(d,A7e,u),b(d,Wo,u),g(VM,Wo,null),e(Wo,Blo),e(Wo,zi),e(zi,klo),e(zi,QY),e(QY,xlo),e(zi,Rlo),e(zi,HY),e(HY,Slo),e(zi,Plo),e(Wo,$lo),e(Wo,WM),e(WM,Ilo),e(WM,UY),e(UY,jlo),e(WM,Nlo),e(Wo,Dlo),e(Wo,Dr),g(QM,Dr,null),e(Dr,qlo),e(Dr,JY),e(JY,Glo),e(Dr,Olo),e(Dr,Vi),e(Vi,Xlo),e(Vi,YY),e(YY,zlo),e(Vi,Vlo),e(Vi,KY),e(KY,Wlo),e(Vi,Qlo),e(Dr,Hlo),e(Dr,ZY),e(ZY,Ulo),e(Dr,Jlo),g(HM,Dr,null),e(Wo,Ylo),e(Wo,xe),g(UM,xe,null),e(xe,Klo),e(xe,eK),e(eK,Zlo),e(xe,eio),e(xe,qa),e(qa,oio),e(qa,oK),e(oK,rio),e(qa,tio),e(qa,rK),e(rK,aio),e(qa,nio),e(qa,tK),e(tK,sio),e(qa,lio),e(xe,iio),e(xe,x),e(x,o_),e(o_,aK),e(aK,dio),e(o_,cio),e(o_,HR),e(HR,fio),e(o_,mio),e(x,gio),e(x,r_),e(r_,nK),e(nK,hio),e(r_,pio),e(r_,UR),e(UR,_io),e(r_,uio),e(x,bio),e(x,t_),e(t_,sK),e(sK,vio),e(t_,Tio),e(t_,JR),e(JR,Fio),e(t_,Cio),e(x,Mio),e(x,a_),e(a_,lK),e(lK,Eio),e(a_,yio),e(a_,YR),e(YR,wio),e(a_,Aio),e(x,Lio),e(x,n_),e(n_,iK),e(iK,Bio),e(n_,kio),e(n_,KR),e(KR,xio),e(n_,Rio),e(x,Sio),e(x,s_),e(s_,dK),e(dK,Pio),e(s_,$io),e(s_,ZR),e(ZR,Iio),e(s_,jio),e(x,Nio),e(x,l_),e(l_,cK),e(cK,Dio),e(l_,qio),e(l_,eS),e(eS,Gio),e(l_,Oio),e(x,Xio),e(x,i_),e(i_,fK),e(fK,zio),e(i_,Vio),e(i_,oS),e(oS,Wio),e(i_,Qio),e(x,Hio),e(x,d_),e(d_,mK),e(mK,Uio),e(d_,Jio),e(d_,rS),e(rS,Yio),e(d_,Kio),e(x,Zio),e(x,c_),e(c_,gK),e(gK,edo),e(c_,odo),e(c_,tS),e(tS,rdo),e(c_,tdo),e(x,ado),e(x,f_),e(f_,hK),e(hK,ndo),e(f_,sdo),e(f_,aS),e(aS,ldo),e(f_,ido),e(x,ddo),e(x,m_),e(m_,pK),e(pK,cdo),e(m_,fdo),e(m_,nS),e(nS,mdo),e(m_,gdo),e(x,hdo),e(x,g_),e(g_,_K),e(_K,pdo),e(g_,_do),e(g_,sS),e(sS,udo),e(g_,bdo),e(x,vdo),e(x,h_),e(h_,uK),e(uK,Tdo),e(h_,Fdo),e(h_,lS),e(lS,Cdo),e(h_,Mdo),e(x,Edo),e(x,p_),e(p_,bK),e(bK,ydo),e(p_,wdo),e(p_,iS),e(iS,Ado),e(p_,Ldo),e(x,Bdo),e(x,__),e(__,vK),e(vK,kdo),e(__,xdo),e(__,dS),e(dS,Rdo),e(__,Sdo),e(x,Pdo),e(x,u_),e(u_,TK),e(TK,$do),e(u_,Ido),e(u_,cS),e(cS,jdo),e(u_,Ndo),e(x,Ddo),e(x,b_),e(b_,FK),e(FK,qdo),e(b_,Gdo),e(b_,fS),e(fS,Odo),e(b_,Xdo),e(x,zdo),e(x,v_),e(v_,CK),e(CK,Vdo),e(v_,Wdo),e(v_,mS),e(mS,Qdo),e(v_,Hdo),e(x,Udo),e(x,T_),e(T_,MK),e(MK,Jdo),e(T_,Ydo),e(T_,gS),e(gS,Kdo),e(T_,Zdo),e(x,eco),e(x,F_),e(F_,EK),e(EK,oco),e(F_,rco),e(F_,hS),e(hS,tco),e(F_,aco),e(x,nco),e(x,C_),e(C_,yK),e(yK,sco),e(C_,lco),e(C_,pS),e(pS,ico),e(C_,dco),e(x,cco),e(x,M_),e(M_,wK),e(wK,fco),e(M_,mco),e(M_,_S),e(_S,gco),e(M_,hco),e(x,pco),e(x,E_),e(E_,AK),e(AK,_co),e(E_,uco),e(E_,uS),e(uS,bco),e(E_,vco),e(x,Tco),e(x,y_),e(y_,LK),e(LK,Fco),e(y_,Cco),e(y_,bS),e(bS,Mco),e(y_,Eco),e(x,yco),e(x,w_),e(w_,BK),e(BK,wco),e(w_,Aco),e(w_,vS),e(vS,Lco),e(w_,Bco),e(x,kco),e(x,A_),e(A_,kK),e(kK,xco),e(A_,Rco),e(A_,TS),e(TS,Sco),e(A_,Pco),e(x,$co),e(x,L_),e(L_,xK),e(xK,Ico),e(L_,jco),e(L_,FS),e(FS,Nco),e(L_,Dco),e(x,qco),e(x,B_),e(B_,RK),e(RK,Gco),e(B_,Oco),e(B_,CS),e(CS,Xco),e(B_,zco),e(x,Vco),e(x,k_),e(k_,SK),e(SK,Wco),e(k_,Qco),e(k_,MS),e(MS,Hco),e(k_,Uco),e(x,Jco),e(x,x_),e(x_,PK),e(PK,Yco),e(x_,Kco),e(x_,ES),e(ES,Zco),e(x_,efo),e(x,ofo),e(x,R_),e(R_,$K),e($K,rfo),e(R_,tfo),e(R_,yS),e(yS,afo),e(R_,nfo),e(x,sfo),e(x,S_),e(S_,IK),e(IK,lfo),e(S_,ifo),e(S_,wS),e(wS,dfo),e(S_,cfo),e(x,ffo),e(x,P_),e(P_,jK),e(jK,mfo),e(P_,gfo),e(P_,AS),e(AS,hfo),e(P_,pfo),e(x,_fo),e(x,$_),e($_,NK),e(NK,ufo),e($_,bfo),e($_,LS),e(LS,vfo),e($_,Tfo),e(x,Ffo),e(x,I_),e(I_,DK),e(DK,Cfo),e(I_,Mfo),e(I_,BS),e(BS,Efo),e(I_,yfo),e(x,wfo),e(x,j_),e(j_,qK),e(qK,Afo),e(j_,Lfo),e(j_,kS),e(kS,Bfo),e(j_,kfo),e(x,xfo),e(x,N_),e(N_,GK),e(GK,Rfo),e(N_,Sfo),e(N_,xS),e(xS,Pfo),e(N_,$fo),e(xe,Ifo),e(xe,D_),e(D_,jfo),e(D_,OK),e(OK,Nfo),e(D_,Dfo),e(D_,XK),e(XK,qfo),e(xe,Gfo),e(xe,zK),e(zK,Ofo),e(xe,Xfo),g(JM,xe,null),b(d,L7e,u),b(d,Wi,u),e(Wi,q_),e(q_,VK),g(YM,VK,null),e(Wi,zfo),e(Wi,WK),e(WK,Vfo),b(d,B7e,u),b(d,Qo,u),g(KM,Qo,null),e(Qo,Wfo),e(Qo,Qi),e(Qi,Qfo),e(Qi,QK),e(QK,Hfo),e(Qi,Ufo),e(Qi,HK),e(HK,Jfo),e(Qi,Yfo),e(Qo,Kfo),e(Qo,ZM),e(ZM,Zfo),e(ZM,UK),e(UK,emo),e(ZM,omo),e(Qo,rmo),e(Qo,qr),g(eE,qr,null),e(qr,tmo),e(qr,JK),e(JK,amo),e(qr,nmo),e(qr,Hi),e(Hi,smo),e(Hi,YK),e(YK,lmo),e(Hi,imo),e(Hi,KK),e(KK,dmo),e(Hi,cmo),e(qr,fmo),e(qr,ZK),e(ZK,mmo),e(qr,gmo),g(oE,qr,null),e(Qo,hmo),e(Qo,Re),g(rE,Re,null),e(Re,pmo),e(Re,eZ),e(eZ,_mo),e(Re,umo),e(Re,Ga),e(Ga,bmo),e(Ga,oZ),e(oZ,vmo),e(Ga,Tmo),e(Ga,rZ),e(rZ,Fmo),e(Ga,Cmo),e(Ga,tZ),e(tZ,Mmo),e(Ga,Emo),e(Re,ymo),e(Re,$),e($,G_),e(G_,aZ),e(aZ,wmo),e(G_,Amo),e(G_,RS),e(RS,Lmo),e(G_,Bmo),e($,kmo),e($,O_),e(O_,nZ),e(nZ,xmo),e(O_,Rmo),e(O_,SS),e(SS,Smo),e(O_,Pmo),e($,$mo),e($,X_),e(X_,sZ),e(sZ,Imo),e(X_,jmo),e(X_,PS),e(PS,Nmo),e(X_,Dmo),e($,qmo),e($,z_),e(z_,lZ),e(lZ,Gmo),e(z_,Omo),e(z_,$S),e($S,Xmo),e(z_,zmo),e($,Vmo),e($,V_),e(V_,iZ),e(iZ,Wmo),e(V_,Qmo),e(V_,IS),e(IS,Hmo),e(V_,Umo),e($,Jmo),e($,W_),e(W_,dZ),e(dZ,Ymo),e(W_,Kmo),e(W_,jS),e(jS,Zmo),e(W_,ego),e($,ogo),e($,Q_),e(Q_,cZ),e(cZ,rgo),e(Q_,tgo),e(Q_,NS),e(NS,ago),e(Q_,ngo),e($,sgo),e($,H_),e(H_,fZ),e(fZ,lgo),e(H_,igo),e(H_,DS),e(DS,dgo),e(H_,cgo),e($,fgo),e($,U_),e(U_,mZ),e(mZ,mgo),e(U_,ggo),e(U_,qS),e(qS,hgo),e(U_,pgo),e($,_go),e($,J_),e(J_,gZ),e(gZ,ugo),e(J_,bgo),e(J_,GS),e(GS,vgo),e(J_,Tgo),e($,Fgo),e($,Y_),e(Y_,hZ),e(hZ,Cgo),e(Y_,Mgo),e(Y_,OS),e(OS,Ego),e(Y_,ygo),e($,wgo),e($,K_),e(K_,pZ),e(pZ,Ago),e(K_,Lgo),e(K_,XS),e(XS,Bgo),e(K_,kgo),e($,xgo),e($,Z_),e(Z_,_Z),e(_Z,Rgo),e(Z_,Sgo),e(Z_,zS),e(zS,Pgo),e(Z_,$go),e($,Igo),e($,eu),e(eu,uZ),e(uZ,jgo),e(eu,Ngo),e(eu,VS),e(VS,Dgo),e(eu,qgo),e($,Ggo),e($,ou),e(ou,bZ),e(bZ,Ogo),e(ou,Xgo),e(ou,WS),e(WS,zgo),e(ou,Vgo),e($,Wgo),e($,ru),e(ru,vZ),e(vZ,Qgo),e(ru,Hgo),e(ru,QS),e(QS,Ugo),e(ru,Jgo),e($,Ygo),e($,tu),e(tu,TZ),e(TZ,Kgo),e(tu,Zgo),e(tu,HS),e(HS,eho),e(tu,oho),e($,rho),e($,au),e(au,FZ),e(FZ,tho),e(au,aho),e(au,US),e(US,nho),e(au,sho),e($,lho),e($,nu),e(nu,CZ),e(CZ,iho),e(nu,dho),e(nu,JS),e(JS,cho),e(nu,fho),e($,mho),e($,su),e(su,MZ),e(MZ,gho),e(su,hho),e(su,YS),e(YS,pho),e(su,_ho),e($,uho),e($,lu),e(lu,EZ),e(EZ,bho),e(lu,vho),e(lu,KS),e(KS,Tho),e(lu,Fho),e($,Cho),e($,iu),e(iu,yZ),e(yZ,Mho),e(iu,Eho),e(iu,ZS),e(ZS,yho),e(iu,who),e($,Aho),e($,du),e(du,wZ),e(wZ,Lho),e(du,Bho),e(du,eP),e(eP,kho),e(du,xho),e($,Rho),e($,cu),e(cu,AZ),e(AZ,Sho),e(cu,Pho),e(cu,oP),e(oP,$ho),e(cu,Iho),e($,jho),e($,fu),e(fu,LZ),e(LZ,Nho),e(fu,Dho),e(fu,rP),e(rP,qho),e(fu,Gho),e($,Oho),e($,mu),e(mu,BZ),e(BZ,Xho),e(mu,zho),e(mu,tP),e(tP,Vho),e(mu,Who),e($,Qho),e($,gu),e(gu,kZ),e(kZ,Hho),e(gu,Uho),e(gu,aP),e(aP,Jho),e(gu,Yho),e($,Kho),e($,hu),e(hu,xZ),e(xZ,Zho),e(hu,epo),e(hu,nP),e(nP,opo),e(hu,rpo),e($,tpo),e($,pu),e(pu,RZ),e(RZ,apo),e(pu,npo),e(pu,sP),e(sP,spo),e(pu,lpo),e($,ipo),e($,_u),e(_u,SZ),e(SZ,dpo),e(_u,cpo),e(_u,lP),e(lP,fpo),e(_u,mpo),e($,gpo),e($,uu),e(uu,PZ),e(PZ,hpo),e(uu,ppo),e(uu,iP),e(iP,_po),e(uu,upo),e($,bpo),e($,bu),e(bu,$Z),e($Z,vpo),e(bu,Tpo),e(bu,dP),e(dP,Fpo),e(bu,Cpo),e($,Mpo),e($,vu),e(vu,IZ),e(IZ,Epo),e(vu,ypo),e(vu,cP),e(cP,wpo),e(vu,Apo),e($,Lpo),e($,Tu),e(Tu,jZ),e(jZ,Bpo),e(Tu,kpo),e(Tu,fP),e(fP,xpo),e(Tu,Rpo),e(Re,Spo),e(Re,Fu),e(Fu,Ppo),e(Fu,NZ),e(NZ,$po),e(Fu,Ipo),e(Fu,DZ),e(DZ,jpo),e(Re,Npo),e(Re,qZ),e(qZ,Dpo),e(Re,qpo),g(tE,Re,null),b(d,k7e,u),b(d,Ui,u),e(Ui,Cu),e(Cu,GZ),g(aE,GZ,null),e(Ui,Gpo),e(Ui,OZ),e(OZ,Opo),b(d,x7e,u),b(d,Ho,u),g(nE,Ho,null),e(Ho,Xpo),e(Ho,Ji),e(Ji,zpo),e(Ji,XZ),e(XZ,Vpo),e(Ji,Wpo),e(Ji,zZ),e(zZ,Qpo),e(Ji,Hpo),e(Ho,Upo),e(Ho,sE),e(sE,Jpo),e(sE,VZ),e(VZ,Ypo),e(sE,Kpo),e(Ho,Zpo),e(Ho,Gr),g(lE,Gr,null),e(Gr,e_o),e(Gr,WZ),e(WZ,o_o),e(Gr,r_o),e(Gr,Yi),e(Yi,t_o),e(Yi,QZ),e(QZ,a_o),e(Yi,n_o),e(Yi,HZ),e(HZ,s_o),e(Yi,l_o),e(Gr,i_o),e(Gr,UZ),e(UZ,d_o),e(Gr,c_o),g(iE,Gr,null),e(Ho,f_o),e(Ho,Se),g(dE,Se,null),e(Se,m_o),e(Se,JZ),e(JZ,g_o),e(Se,h_o),e(Se,Oa),e(Oa,p_o),e(Oa,YZ),e(YZ,__o),e(Oa,u_o),e(Oa,KZ),e(KZ,b_o),e(Oa,v_o),e(Oa,ZZ),e(ZZ,T_o),e(Oa,F_o),e(Se,C_o),e(Se,I),e(I,Mu),e(Mu,eee),e(eee,M_o),e(Mu,E_o),e(Mu,mP),e(mP,y_o),e(Mu,w_o),e(I,A_o),e(I,Eu),e(Eu,oee),e(oee,L_o),e(Eu,B_o),e(Eu,gP),e(gP,k_o),e(Eu,x_o),e(I,R_o),e(I,yu),e(yu,ree),e(ree,S_o),e(yu,P_o),e(yu,hP),e(hP,$_o),e(yu,I_o),e(I,j_o),e(I,wu),e(wu,tee),e(tee,N_o),e(wu,D_o),e(wu,pP),e(pP,q_o),e(wu,G_o),e(I,O_o),e(I,Au),e(Au,aee),e(aee,X_o),e(Au,z_o),e(Au,_P),e(_P,V_o),e(Au,W_o),e(I,Q_o),e(I,Lu),e(Lu,nee),e(nee,H_o),e(Lu,U_o),e(Lu,uP),e(uP,J_o),e(Lu,Y_o),e(I,K_o),e(I,Bu),e(Bu,see),e(see,Z_o),e(Bu,euo),e(Bu,bP),e(bP,ouo),e(Bu,ruo),e(I,tuo),e(I,ku),e(ku,lee),e(lee,auo),e(ku,nuo),e(ku,vP),e(vP,suo),e(ku,luo),e(I,iuo),e(I,xu),e(xu,iee),e(iee,duo),e(xu,cuo),e(xu,TP),e(TP,fuo),e(xu,muo),e(I,guo),e(I,Ru),e(Ru,dee),e(dee,huo),e(Ru,puo),e(Ru,FP),e(FP,_uo),e(Ru,uuo),e(I,buo),e(I,Su),e(Su,cee),e(cee,vuo),e(Su,Tuo),e(Su,CP),e(CP,Fuo),e(Su,Cuo),e(I,Muo),e(I,Pu),e(Pu,fee),e(fee,Euo),e(Pu,yuo),e(Pu,MP),e(MP,wuo),e(Pu,Auo),e(I,Luo),e(I,$u),e($u,mee),e(mee,Buo),e($u,kuo),e($u,EP),e(EP,xuo),e($u,Ruo),e(I,Suo),e(I,Iu),e(Iu,gee),e(gee,Puo),e(Iu,$uo),e(Iu,yP),e(yP,Iuo),e(Iu,juo),e(I,Nuo),e(I,ju),e(ju,hee),e(hee,Duo),e(ju,quo),e(ju,wP),e(wP,Guo),e(ju,Ouo),e(I,Xuo),e(I,Nu),e(Nu,pee),e(pee,zuo),e(Nu,Vuo),e(Nu,AP),e(AP,Wuo),e(Nu,Quo),e(I,Huo),e(I,Du),e(Du,_ee),e(_ee,Uuo),e(Du,Juo),e(Du,LP),e(LP,Yuo),e(Du,Kuo),e(I,Zuo),e(I,qu),e(qu,uee),e(uee,e2o),e(qu,o2o),e(qu,BP),e(BP,r2o),e(qu,t2o),e(I,a2o),e(I,Gu),e(Gu,bee),e(bee,n2o),e(Gu,s2o),e(Gu,kP),e(kP,l2o),e(Gu,i2o),e(I,d2o),e(I,Ou),e(Ou,vee),e(vee,c2o),e(Ou,f2o),e(Ou,xP),e(xP,m2o),e(Ou,g2o),e(I,h2o),e(I,Xu),e(Xu,Tee),e(Tee,p2o),e(Xu,_2o),e(Xu,RP),e(RP,u2o),e(Xu,b2o),e(I,v2o),e(I,zu),e(zu,Fee),e(Fee,T2o),e(zu,F2o),e(zu,SP),e(SP,C2o),e(zu,M2o),e(I,E2o),e(I,Vu),e(Vu,Cee),e(Cee,y2o),e(Vu,w2o),e(Vu,PP),e(PP,A2o),e(Vu,L2o),e(I,B2o),e(I,Wu),e(Wu,Mee),e(Mee,k2o),e(Wu,x2o),e(Wu,$P),e($P,R2o),e(Wu,S2o),e(I,P2o),e(I,Qu),e(Qu,Eee),e(Eee,$2o),e(Qu,I2o),e(Qu,IP),e(IP,j2o),e(Qu,N2o),e(I,D2o),e(I,Hu),e(Hu,yee),e(yee,q2o),e(Hu,G2o),e(Hu,jP),e(jP,O2o),e(Hu,X2o),e(I,z2o),e(I,Uu),e(Uu,wee),e(wee,V2o),e(Uu,W2o),e(Uu,NP),e(NP,Q2o),e(Uu,H2o),e(I,U2o),e(I,Ju),e(Ju,Aee),e(Aee,J2o),e(Ju,Y2o),e(Ju,DP),e(DP,K2o),e(Ju,Z2o),e(I,e1o),e(I,Yu),e(Yu,Lee),e(Lee,o1o),e(Yu,r1o),e(Yu,qP),e(qP,t1o),e(Yu,a1o),e(I,n1o),e(I,Ku),e(Ku,Bee),e(Bee,s1o),e(Ku,l1o),e(Ku,kee),e(kee,i1o),e(Ku,d1o),e(I,c1o),e(I,Zu),e(Zu,xee),e(xee,f1o),e(Zu,m1o),e(Zu,GP),e(GP,g1o),e(Zu,h1o),e(I,p1o),e(I,e2),e(e2,Ree),e(Ree,_1o),e(e2,u1o),e(e2,OP),e(OP,b1o),e(e2,v1o),e(I,T1o),e(I,o2),e(o2,See),e(See,F1o),e(o2,C1o),e(o2,XP),e(XP,M1o),e(o2,E1o),e(I,y1o),e(I,r2),e(r2,Pee),e(Pee,w1o),e(r2,A1o),e(r2,zP),e(zP,L1o),e(r2,B1o),e(Se,k1o),e(Se,t2),e(t2,x1o),e(t2,$ee),e($ee,R1o),e(t2,S1o),e(t2,Iee),e(Iee,P1o),e(Se,$1o),e(Se,jee),e(jee,I1o),e(Se,j1o),g(cE,Se,null),b(d,R7e,u),b(d,Ki,u),e(Ki,a2),e(a2,Nee),g(fE,Nee,null),e(Ki,N1o),e(Ki,Dee),e(Dee,D1o),b(d,S7e,u),b(d,Uo,u),g(mE,Uo,null),e(Uo,q1o),e(Uo,Zi),e(Zi,G1o),e(Zi,qee),e(qee,O1o),e(Zi,X1o),e(Zi,Gee),e(Gee,z1o),e(Zi,V1o),e(Uo,W1o),e(Uo,gE),e(gE,Q1o),e(gE,Oee),e(Oee,H1o),e(gE,U1o),e(Uo,J1o),e(Uo,Or),g(hE,Or,null),e(Or,Y1o),e(Or,Xee),e(Xee,K1o),e(Or,Z1o),e(Or,ed),e(ed,ebo),e(ed,zee),e(zee,obo),e(ed,rbo),e(ed,Vee),e(Vee,tbo),e(ed,abo),e(Or,nbo),e(Or,Wee),e(Wee,sbo),e(Or,lbo),g(pE,Or,null),e(Uo,ibo),e(Uo,Pe),g(_E,Pe,null),e(Pe,dbo),e(Pe,Qee),e(Qee,cbo),e(Pe,fbo),e(Pe,Xa),e(Xa,mbo),e(Xa,Hee),e(Hee,gbo),e(Xa,hbo),e(Xa,Uee),e(Uee,pbo),e(Xa,_bo),e(Xa,Jee),e(Jee,ubo),e(Xa,bbo),e(Pe,vbo),e(Pe,ae),e(ae,n2),e(n2,Yee),e(Yee,Tbo),e(n2,Fbo),e(n2,VP),e(VP,Cbo),e(n2,Mbo),e(ae,Ebo),e(ae,s2),e(s2,Kee),e(Kee,ybo),e(s2,wbo),e(s2,WP),e(WP,Abo),e(s2,Lbo),e(ae,Bbo),e(ae,l2),e(l2,Zee),e(Zee,kbo),e(l2,xbo),e(l2,QP),e(QP,Rbo),e(l2,Sbo),e(ae,Pbo),e(ae,i2),e(i2,eoe),e(eoe,$bo),e(i2,Ibo),e(i2,HP),e(HP,jbo),e(i2,Nbo),e(ae,Dbo),e(ae,d2),e(d2,ooe),e(ooe,qbo),e(d2,Gbo),e(d2,UP),e(UP,Obo),e(d2,Xbo),e(ae,zbo),e(ae,c2),e(c2,roe),e(roe,Vbo),e(c2,Wbo),e(c2,JP),e(JP,Qbo),e(c2,Hbo),e(ae,Ubo),e(ae,f2),e(f2,toe),e(toe,Jbo),e(f2,Ybo),e(f2,YP),e(YP,Kbo),e(f2,Zbo),e(ae,e5o),e(ae,m2),e(m2,aoe),e(aoe,o5o),e(m2,r5o),e(m2,KP),e(KP,t5o),e(m2,a5o),e(ae,n5o),e(ae,g2),e(g2,noe),e(noe,s5o),e(g2,l5o),e(g2,ZP),e(ZP,i5o),e(g2,d5o),e(ae,c5o),e(ae,h2),e(h2,soe),e(soe,f5o),e(h2,m5o),e(h2,e$),e(e$,g5o),e(h2,h5o),e(ae,p5o),e(ae,p2),e(p2,loe),e(loe,_5o),e(p2,u5o),e(p2,o$),e(o$,b5o),e(p2,v5o),e(ae,T5o),e(ae,_2),e(_2,ioe),e(ioe,F5o),e(_2,C5o),e(_2,r$),e(r$,M5o),e(_2,E5o),e(ae,y5o),e(ae,u2),e(u2,doe),e(doe,w5o),e(u2,A5o),e(u2,t$),e(t$,L5o),e(u2,B5o),e(ae,k5o),e(ae,b2),e(b2,coe),e(coe,x5o),e(b2,R5o),e(b2,a$),e(a$,S5o),e(b2,P5o),e(ae,$5o),e(ae,v2),e(v2,foe),e(foe,I5o),e(v2,j5o),e(v2,n$),e(n$,N5o),e(v2,D5o),e(ae,q5o),e(ae,T2),e(T2,moe),e(moe,G5o),e(T2,O5o),e(T2,s$),e(s$,X5o),e(T2,z5o),e(Pe,V5o),e(Pe,F2),e(F2,W5o),e(F2,goe),e(goe,Q5o),e(F2,H5o),e(F2,hoe),e(hoe,U5o),e(Pe,J5o),e(Pe,poe),e(poe,Y5o),e(Pe,K5o),g(uE,Pe,null),b(d,P7e,u),b(d,od,u),e(od,C2),e(C2,_oe),g(bE,_oe,null),e(od,Z5o),e(od,uoe),e(uoe,evo),b(d,$7e,u),b(d,Jo,u),g(vE,Jo,null),e(Jo,ovo),e(Jo,rd),e(rd,rvo),e(rd,boe),e(boe,tvo),e(rd,avo),e(rd,voe),e(voe,nvo),e(rd,svo),e(Jo,lvo),e(Jo,TE),e(TE,ivo),e(TE,Toe),e(Toe,dvo),e(TE,cvo),e(Jo,fvo),e(Jo,Xr),g(FE,Xr,null),e(Xr,mvo),e(Xr,Foe),e(Foe,gvo),e(Xr,hvo),e(Xr,td),e(td,pvo),e(td,Coe),e(Coe,_vo),e(td,uvo),e(td,Moe),e(Moe,bvo),e(td,vvo),e(Xr,Tvo),e(Xr,Eoe),e(Eoe,Fvo),e(Xr,Cvo),g(CE,Xr,null),e(Jo,Mvo),e(Jo,$e),g(ME,$e,null),e($e,Evo),e($e,yoe),e(yoe,yvo),e($e,wvo),e($e,za),e(za,Avo),e(za,woe),e(woe,Lvo),e(za,Bvo),e(za,Aoe),e(Aoe,kvo),e(za,xvo),e(za,Loe),e(Loe,Rvo),e(za,Svo),e($e,Pvo),e($e,A),e(A,M2),e(M2,Boe),e(Boe,$vo),e(M2,Ivo),e(M2,l$),e(l$,jvo),e(M2,Nvo),e(A,Dvo),e(A,E2),e(E2,koe),e(koe,qvo),e(E2,Gvo),e(E2,i$),e(i$,Ovo),e(E2,Xvo),e(A,zvo),e(A,y2),e(y2,xoe),e(xoe,Vvo),e(y2,Wvo),e(y2,d$),e(d$,Qvo),e(y2,Hvo),e(A,Uvo),e(A,w2),e(w2,Roe),e(Roe,Jvo),e(w2,Yvo),e(w2,c$),e(c$,Kvo),e(w2,Zvo),e(A,e6o),e(A,A2),e(A2,Soe),e(Soe,o6o),e(A2,r6o),e(A2,f$),e(f$,t6o),e(A2,a6o),e(A,n6o),e(A,L2),e(L2,Poe),e(Poe,s6o),e(L2,l6o),e(L2,m$),e(m$,i6o),e(L2,d6o),e(A,c6o),e(A,B2),e(B2,$oe),e($oe,f6o),e(B2,m6o),e(B2,g$),e(g$,g6o),e(B2,h6o),e(A,p6o),e(A,k2),e(k2,Ioe),e(Ioe,_6o),e(k2,u6o),e(k2,h$),e(h$,b6o),e(k2,v6o),e(A,T6o),e(A,x2),e(x2,joe),e(joe,F6o),e(x2,C6o),e(x2,p$),e(p$,M6o),e(x2,E6o),e(A,y6o),e(A,R2),e(R2,Noe),e(Noe,w6o),e(R2,A6o),e(R2,_$),e(_$,L6o),e(R2,B6o),e(A,k6o),e(A,S2),e(S2,Doe),e(Doe,x6o),e(S2,R6o),e(S2,u$),e(u$,S6o),e(S2,P6o),e(A,$6o),e(A,P2),e(P2,qoe),e(qoe,I6o),e(P2,j6o),e(P2,b$),e(b$,N6o),e(P2,D6o),e(A,q6o),e(A,$2),e($2,Goe),e(Goe,G6o),e($2,O6o),e($2,v$),e(v$,X6o),e($2,z6o),e(A,V6o),e(A,I2),e(I2,Ooe),e(Ooe,W6o),e(I2,Q6o),e(I2,T$),e(T$,H6o),e(I2,U6o),e(A,J6o),e(A,j2),e(j2,Xoe),e(Xoe,Y6o),e(j2,K6o),e(j2,F$),e(F$,Z6o),e(j2,eTo),e(A,oTo),e(A,N2),e(N2,zoe),e(zoe,rTo),e(N2,tTo),e(N2,C$),e(C$,aTo),e(N2,nTo),e(A,sTo),e(A,D2),e(D2,Voe),e(Voe,lTo),e(D2,iTo),e(D2,M$),e(M$,dTo),e(D2,cTo),e(A,fTo),e(A,q2),e(q2,Woe),e(Woe,mTo),e(q2,gTo),e(q2,E$),e(E$,hTo),e(q2,pTo),e(A,_To),e(A,G2),e(G2,Qoe),e(Qoe,uTo),e(G2,bTo),e(G2,y$),e(y$,vTo),e(G2,TTo),e(A,FTo),e(A,O2),e(O2,Hoe),e(Hoe,CTo),e(O2,MTo),e(O2,w$),e(w$,ETo),e(O2,yTo),e(A,wTo),e(A,X2),e(X2,Uoe),e(Uoe,ATo),e(X2,LTo),e(X2,A$),e(A$,BTo),e(X2,kTo),e(A,xTo),e(A,z2),e(z2,Joe),e(Joe,RTo),e(z2,STo),e(z2,L$),e(L$,PTo),e(z2,$To),e(A,ITo),e(A,V2),e(V2,Yoe),e(Yoe,jTo),e(V2,NTo),e(V2,B$),e(B$,DTo),e(V2,qTo),e(A,GTo),e(A,W2),e(W2,Koe),e(Koe,OTo),e(W2,XTo),e(W2,k$),e(k$,zTo),e(W2,VTo),e(A,WTo),e(A,Q2),e(Q2,Zoe),e(Zoe,QTo),e(Q2,HTo),e(Q2,x$),e(x$,UTo),e(Q2,JTo),e(A,YTo),e(A,H2),e(H2,ere),e(ere,KTo),e(H2,ZTo),e(H2,R$),e(R$,e8o),e(H2,o8o),e(A,r8o),e(A,U2),e(U2,ore),e(ore,t8o),e(U2,a8o),e(U2,S$),e(S$,n8o),e(U2,s8o),e(A,l8o),e(A,J2),e(J2,rre),e(rre,i8o),e(J2,d8o),e(J2,P$),e(P$,c8o),e(J2,f8o),e(A,m8o),e(A,Y2),e(Y2,tre),e(tre,g8o),e(Y2,h8o),e(Y2,$$),e($$,p8o),e(Y2,_8o),e(A,u8o),e(A,K2),e(K2,are),e(are,b8o),e(K2,v8o),e(K2,I$),e(I$,T8o),e(K2,F8o),e(A,C8o),e(A,Z2),e(Z2,nre),e(nre,M8o),e(Z2,E8o),e(Z2,j$),e(j$,y8o),e(Z2,w8o),e(A,A8o),e(A,e1),e(e1,sre),e(sre,L8o),e(e1,B8o),e(e1,N$),e(N$,k8o),e(e1,x8o),e(A,R8o),e(A,o1),e(o1,lre),e(lre,S8o),e(o1,P8o),e(o1,D$),e(D$,$8o),e(o1,I8o),e(A,j8o),e(A,r1),e(r1,ire),e(ire,N8o),e(r1,D8o),e(r1,q$),e(q$,q8o),e(r1,G8o),e(A,O8o),e(A,t1),e(t1,dre),e(dre,X8o),e(t1,z8o),e(t1,G$),e(G$,V8o),e(t1,W8o),e(A,Q8o),e(A,a1),e(a1,cre),e(cre,H8o),e(a1,U8o),e(a1,O$),e(O$,J8o),e(a1,Y8o),e(A,K8o),e(A,n1),e(n1,fre),e(fre,Z8o),e(n1,eFo),e(n1,X$),e(X$,oFo),e(n1,rFo),e(A,tFo),e(A,s1),e(s1,mre),e(mre,aFo),e(s1,nFo),e(s1,z$),e(z$,sFo),e(s1,lFo),e(A,iFo),e(A,l1),e(l1,gre),e(gre,dFo),e(l1,cFo),e(l1,V$),e(V$,fFo),e(l1,mFo),e(A,gFo),e(A,i1),e(i1,hre),e(hre,hFo),e(i1,pFo),e(i1,W$),e(W$,_Fo),e(i1,uFo),e(A,bFo),e(A,d1),e(d1,pre),e(pre,vFo),e(d1,TFo),e(d1,Q$),e(Q$,FFo),e(d1,CFo),e(A,MFo),e(A,c1),e(c1,_re),e(_re,EFo),e(c1,yFo),e(c1,H$),e(H$,wFo),e(c1,AFo),e(A,LFo),e(A,f1),e(f1,ure),e(ure,BFo),e(f1,kFo),e(f1,U$),e(U$,xFo),e(f1,RFo),e(A,SFo),e(A,m1),e(m1,bre),e(bre,PFo),e(m1,$Fo),e(m1,J$),e(J$,IFo),e(m1,jFo),e(A,NFo),e(A,g1),e(g1,vre),e(vre,DFo),e(g1,qFo),e(g1,Y$),e(Y$,GFo),e(g1,OFo),e($e,XFo),e($e,h1),e(h1,zFo),e(h1,Tre),e(Tre,VFo),e(h1,WFo),e(h1,Fre),e(Fre,QFo),e($e,HFo),e($e,Cre),e(Cre,UFo),e($e,JFo),g(EE,$e,null),b(d,I7e,u),b(d,ad,u),e(ad,p1),e(p1,Mre),g(yE,Mre,null),e(ad,YFo),e(ad,Ere),e(Ere,KFo),b(d,j7e,u),b(d,Yo,u),g(wE,Yo,null),e(Yo,ZFo),e(Yo,nd),e(nd,eCo),e(nd,yre),e(yre,oCo),e(nd,rCo),e(nd,wre),e(wre,tCo),e(nd,aCo),e(Yo,nCo),e(Yo,AE),e(AE,sCo),e(AE,Are),e(Are,lCo),e(AE,iCo),e(Yo,dCo),e(Yo,zr),g(LE,zr,null),e(zr,cCo),e(zr,Lre),e(Lre,fCo),e(zr,mCo),e(zr,sd),e(sd,gCo),e(sd,Bre),e(Bre,hCo),e(sd,pCo),e(sd,kre),e(kre,_Co),e(sd,uCo),e(zr,bCo),e(zr,xre),e(xre,vCo),e(zr,TCo),g(BE,zr,null),e(Yo,FCo),e(Yo,Ie),g(kE,Ie,null),e(Ie,CCo),e(Ie,Rre),e(Rre,MCo),e(Ie,ECo),e(Ie,Va),e(Va,yCo),e(Va,Sre),e(Sre,wCo),e(Va,ACo),e(Va,Pre),e(Pre,LCo),e(Va,BCo),e(Va,$re),e($re,kCo),e(Va,xCo),e(Ie,RCo),e(Ie,G),e(G,_1),e(_1,Ire),e(Ire,SCo),e(_1,PCo),e(_1,K$),e(K$,$Co),e(_1,ICo),e(G,jCo),e(G,u1),e(u1,jre),e(jre,NCo),e(u1,DCo),e(u1,Z$),e(Z$,qCo),e(u1,GCo),e(G,OCo),e(G,b1),e(b1,Nre),e(Nre,XCo),e(b1,zCo),e(b1,eI),e(eI,VCo),e(b1,WCo),e(G,QCo),e(G,v1),e(v1,Dre),e(Dre,HCo),e(v1,UCo),e(v1,oI),e(oI,JCo),e(v1,YCo),e(G,KCo),e(G,T1),e(T1,qre),e(qre,ZCo),e(T1,e4o),e(T1,rI),e(rI,o4o),e(T1,r4o),e(G,t4o),e(G,F1),e(F1,Gre),e(Gre,a4o),e(F1,n4o),e(F1,tI),e(tI,s4o),e(F1,l4o),e(G,i4o),e(G,C1),e(C1,Ore),e(Ore,d4o),e(C1,c4o),e(C1,aI),e(aI,f4o),e(C1,m4o),e(G,g4o),e(G,M1),e(M1,Xre),e(Xre,h4o),e(M1,p4o),e(M1,nI),e(nI,_4o),e(M1,u4o),e(G,b4o),e(G,E1),e(E1,zre),e(zre,v4o),e(E1,T4o),e(E1,sI),e(sI,F4o),e(E1,C4o),e(G,M4o),e(G,y1),e(y1,Vre),e(Vre,E4o),e(y1,y4o),e(y1,lI),e(lI,w4o),e(y1,A4o),e(G,L4o),e(G,w1),e(w1,Wre),e(Wre,B4o),e(w1,k4o),e(w1,iI),e(iI,x4o),e(w1,R4o),e(G,S4o),e(G,A1),e(A1,Qre),e(Qre,P4o),e(A1,$4o),e(A1,dI),e(dI,I4o),e(A1,j4o),e(G,N4o),e(G,L1),e(L1,Hre),e(Hre,D4o),e(L1,q4o),e(L1,cI),e(cI,G4o),e(L1,O4o),e(G,X4o),e(G,B1),e(B1,Ure),e(Ure,z4o),e(B1,V4o),e(B1,fI),e(fI,W4o),e(B1,Q4o),e(G,H4o),e(G,k1),e(k1,Jre),e(Jre,U4o),e(k1,J4o),e(k1,mI),e(mI,Y4o),e(k1,K4o),e(G,Z4o),e(G,x1),e(x1,Yre),e(Yre,eMo),e(x1,oMo),e(x1,gI),e(gI,rMo),e(x1,tMo),e(G,aMo),e(G,R1),e(R1,Kre),e(Kre,nMo),e(R1,sMo),e(R1,hI),e(hI,lMo),e(R1,iMo),e(G,dMo),e(G,S1),e(S1,Zre),e(Zre,cMo),e(S1,fMo),e(S1,pI),e(pI,mMo),e(S1,gMo),e(G,hMo),e(G,P1),e(P1,ete),e(ete,pMo),e(P1,_Mo),e(P1,_I),e(_I,uMo),e(P1,bMo),e(G,vMo),e(G,$1),e($1,ote),e(ote,TMo),e($1,FMo),e($1,uI),e(uI,CMo),e($1,MMo),e(G,EMo),e(G,I1),e(I1,rte),e(rte,yMo),e(I1,wMo),e(I1,bI),e(bI,AMo),e(I1,LMo),e(G,BMo),e(G,j1),e(j1,tte),e(tte,kMo),e(j1,xMo),e(j1,vI),e(vI,RMo),e(j1,SMo),e(G,PMo),e(G,N1),e(N1,ate),e(ate,$Mo),e(N1,IMo),e(N1,TI),e(TI,jMo),e(N1,NMo),e(G,DMo),e(G,D1),e(D1,nte),e(nte,qMo),e(D1,GMo),e(D1,FI),e(FI,OMo),e(D1,XMo),e(G,zMo),e(G,q1),e(q1,ste),e(ste,VMo),e(q1,WMo),e(q1,CI),e(CI,QMo),e(q1,HMo),e(G,UMo),e(G,G1),e(G1,lte),e(lte,JMo),e(G1,YMo),e(G1,MI),e(MI,KMo),e(G1,ZMo),e(G,eEo),e(G,O1),e(O1,ite),e(ite,oEo),e(O1,rEo),e(O1,EI),e(EI,tEo),e(O1,aEo),e(Ie,nEo),e(Ie,X1),e(X1,sEo),e(X1,dte),e(dte,lEo),e(X1,iEo),e(X1,cte),e(cte,dEo),e(Ie,cEo),e(Ie,fte),e(fte,fEo),e(Ie,mEo),g(xE,Ie,null),b(d,N7e,u),b(d,ld,u),e(ld,z1),e(z1,mte),g(RE,mte,null),e(ld,gEo),e(ld,gte),e(gte,hEo),b(d,D7e,u),b(d,Ko,u),g(SE,Ko,null),e(Ko,pEo),e(Ko,id),e(id,_Eo),e(id,hte),e(hte,uEo),e(id,bEo),e(id,pte),e(pte,vEo),e(id,TEo),e(Ko,FEo),e(Ko,PE),e(PE,CEo),e(PE,_te),e(_te,MEo),e(PE,EEo),e(Ko,yEo),e(Ko,Vr),g($E,Vr,null),e(Vr,wEo),e(Vr,ute),e(ute,AEo),e(Vr,LEo),e(Vr,dd),e(dd,BEo),e(dd,bte),e(bte,kEo),e(dd,xEo),e(dd,vte),e(vte,REo),e(dd,SEo),e(Vr,PEo),e(Vr,Tte),e(Tte,$Eo),e(Vr,IEo),g(IE,Vr,null),e(Ko,jEo),e(Ko,je),g(jE,je,null),e(je,NEo),e(je,Fte),e(Fte,DEo),e(je,qEo),e(je,Wa),e(Wa,GEo),e(Wa,Cte),e(Cte,OEo),e(Wa,XEo),e(Wa,Mte),e(Mte,zEo),e(Wa,VEo),e(Wa,Ete),e(Ete,WEo),e(Wa,QEo),e(je,HEo),e(je,na),e(na,V1),e(V1,yte),e(yte,UEo),e(V1,JEo),e(V1,yI),e(yI,YEo),e(V1,KEo),e(na,ZEo),e(na,W1),e(W1,wte),e(wte,e3o),e(W1,o3o),e(W1,wI),e(wI,r3o),e(W1,t3o),e(na,a3o),e(na,Q1),e(Q1,Ate),e(Ate,n3o),e(Q1,s3o),e(Q1,AI),e(AI,l3o),e(Q1,i3o),e(na,d3o),e(na,H1),e(H1,Lte),e(Lte,c3o),e(H1,f3o),e(H1,LI),e(LI,m3o),e(H1,g3o),e(na,h3o),e(na,U1),e(U1,Bte),e(Bte,p3o),e(U1,_3o),e(U1,BI),e(BI,u3o),e(U1,b3o),e(je,v3o),e(je,J1),e(J1,T3o),e(J1,kte),e(kte,F3o),e(J1,C3o),e(J1,xte),e(xte,M3o),e(je,E3o),e(je,Rte),e(Rte,y3o),e(je,w3o),g(NE,je,null),b(d,q7e,u),b(d,cd,u),e(cd,Y1),e(Y1,Ste),g(DE,Ste,null),e(cd,A3o),e(cd,Pte),e(Pte,L3o),b(d,G7e,u),b(d,Zo,u),g(qE,Zo,null),e(Zo,B3o),e(Zo,fd),e(fd,k3o),e(fd,$te),e($te,x3o),e(fd,R3o),e(fd,Ite),e(Ite,S3o),e(fd,P3o),e(Zo,$3o),e(Zo,GE),e(GE,I3o),e(GE,jte),e(jte,j3o),e(GE,N3o),e(Zo,D3o),e(Zo,Wr),g(OE,Wr,null),e(Wr,q3o),e(Wr,Nte),e(Nte,G3o),e(Wr,O3o),e(Wr,md),e(md,X3o),e(md,Dte),e(Dte,z3o),e(md,V3o),e(md,qte),e(qte,W3o),e(md,Q3o),e(Wr,H3o),e(Wr,Gte),e(Gte,U3o),e(Wr,J3o),g(XE,Wr,null),e(Zo,Y3o),e(Zo,Ne),g(zE,Ne,null),e(Ne,K3o),e(Ne,Ote),e(Ote,Z3o),e(Ne,eyo),e(Ne,Qa),e(Qa,oyo),e(Qa,Xte),e(Xte,ryo),e(Qa,tyo),e(Qa,zte),e(zte,ayo),e(Qa,nyo),e(Qa,Vte),e(Vte,syo),e(Qa,lyo),e(Ne,iyo),e(Ne,D),e(D,K1),e(K1,Wte),e(Wte,dyo),e(K1,cyo),e(K1,kI),e(kI,fyo),e(K1,myo),e(D,gyo),e(D,Z1),e(Z1,Qte),e(Qte,hyo),e(Z1,pyo),e(Z1,xI),e(xI,_yo),e(Z1,uyo),e(D,byo),e(D,eb),e(eb,Hte),e(Hte,vyo),e(eb,Tyo),e(eb,RI),e(RI,Fyo),e(eb,Cyo),e(D,Myo),e(D,ob),e(ob,Ute),e(Ute,Eyo),e(ob,yyo),e(ob,SI),e(SI,wyo),e(ob,Ayo),e(D,Lyo),e(D,rb),e(rb,Jte),e(Jte,Byo),e(rb,kyo),e(rb,PI),e(PI,xyo),e(rb,Ryo),e(D,Syo),e(D,tb),e(tb,Yte),e(Yte,Pyo),e(tb,$yo),e(tb,$I),e($I,Iyo),e(tb,jyo),e(D,Nyo),e(D,ab),e(ab,Kte),e(Kte,Dyo),e(ab,qyo),e(ab,II),e(II,Gyo),e(ab,Oyo),e(D,Xyo),e(D,nb),e(nb,Zte),e(Zte,zyo),e(nb,Vyo),e(nb,jI),e(jI,Wyo),e(nb,Qyo),e(D,Hyo),e(D,sb),e(sb,eae),e(eae,Uyo),e(sb,Jyo),e(sb,NI),e(NI,Yyo),e(sb,Kyo),e(D,Zyo),e(D,lb),e(lb,oae),e(oae,ewo),e(lb,owo),e(lb,DI),e(DI,rwo),e(lb,two),e(D,awo),e(D,ib),e(ib,rae),e(rae,nwo),e(ib,swo),e(ib,qI),e(qI,lwo),e(ib,iwo),e(D,dwo),e(D,db),e(db,tae),e(tae,cwo),e(db,fwo),e(db,GI),e(GI,mwo),e(db,gwo),e(D,hwo),e(D,cb),e(cb,aae),e(aae,pwo),e(cb,_wo),e(cb,OI),e(OI,uwo),e(cb,bwo),e(D,vwo),e(D,fb),e(fb,nae),e(nae,Two),e(fb,Fwo),e(fb,XI),e(XI,Cwo),e(fb,Mwo),e(D,Ewo),e(D,mb),e(mb,sae),e(sae,ywo),e(mb,wwo),e(mb,zI),e(zI,Awo),e(mb,Lwo),e(D,Bwo),e(D,gb),e(gb,lae),e(lae,kwo),e(gb,xwo),e(gb,VI),e(VI,Rwo),e(gb,Swo),e(D,Pwo),e(D,hb),e(hb,iae),e(iae,$wo),e(hb,Iwo),e(hb,WI),e(WI,jwo),e(hb,Nwo),e(D,Dwo),e(D,pb),e(pb,dae),e(dae,qwo),e(pb,Gwo),e(pb,QI),e(QI,Owo),e(pb,Xwo),e(D,zwo),e(D,_b),e(_b,cae),e(cae,Vwo),e(_b,Wwo),e(_b,HI),e(HI,Qwo),e(_b,Hwo),e(D,Uwo),e(D,ub),e(ub,fae),e(fae,Jwo),e(ub,Ywo),e(ub,UI),e(UI,Kwo),e(ub,Zwo),e(D,eAo),e(D,bb),e(bb,mae),e(mae,oAo),e(bb,rAo),e(bb,JI),e(JI,tAo),e(bb,aAo),e(D,nAo),e(D,vb),e(vb,gae),e(gae,sAo),e(vb,lAo),e(vb,YI),e(YI,iAo),e(vb,dAo),e(D,cAo),e(D,Tb),e(Tb,hae),e(hae,fAo),e(Tb,mAo),e(Tb,KI),e(KI,gAo),e(Tb,hAo),e(D,pAo),e(D,Fb),e(Fb,pae),e(pae,_Ao),e(Fb,uAo),e(Fb,ZI),e(ZI,bAo),e(Fb,vAo),e(D,TAo),e(D,Cb),e(Cb,_ae),e(_ae,FAo),e(Cb,CAo),e(Cb,ej),e(ej,MAo),e(Cb,EAo),e(D,yAo),e(D,Mb),e(Mb,uae),e(uae,wAo),e(Mb,AAo),e(Mb,oj),e(oj,LAo),e(Mb,BAo),e(D,kAo),e(D,Eb),e(Eb,bae),e(bae,xAo),e(Eb,RAo),e(Eb,rj),e(rj,SAo),e(Eb,PAo),e(D,$Ao),e(D,yb),e(yb,vae),e(vae,IAo),e(yb,jAo),e(yb,tj),e(tj,NAo),e(yb,DAo),e(D,qAo),e(D,wb),e(wb,Tae),e(Tae,GAo),e(wb,OAo),e(wb,aj),e(aj,XAo),e(wb,zAo),e(D,VAo),e(D,Ab),e(Ab,Fae),e(Fae,WAo),e(Ab,QAo),e(Ab,nj),e(nj,HAo),e(Ab,UAo),e(D,JAo),e(D,Lb),e(Lb,Cae),e(Cae,YAo),e(Lb,KAo),e(Lb,sj),e(sj,ZAo),e(Lb,e0o),e(D,o0o),e(D,Bb),e(Bb,Mae),e(Mae,r0o),e(Bb,t0o),e(Bb,lj),e(lj,a0o),e(Bb,n0o),e(Ne,s0o),e(Ne,kb),e(kb,l0o),e(kb,Eae),e(Eae,i0o),e(kb,d0o),e(kb,yae),e(yae,c0o),e(Ne,f0o),e(Ne,wae),e(wae,m0o),e(Ne,g0o),g(VE,Ne,null),b(d,O7e,u),b(d,gd,u),e(gd,xb),e(xb,Aae),g(WE,Aae,null),e(gd,h0o),e(gd,Lae),e(Lae,p0o),b(d,X7e,u),b(d,er,u),g(QE,er,null),e(er,_0o),e(er,hd),e(hd,u0o),e(hd,Bae),e(Bae,b0o),e(hd,v0o),e(hd,kae),e(kae,T0o),e(hd,F0o),e(er,C0o),e(er,HE),e(HE,M0o),e(HE,xae),e(xae,E0o),e(HE,y0o),e(er,w0o),e(er,Qr),g(UE,Qr,null),e(Qr,A0o),e(Qr,Rae),e(Rae,L0o),e(Qr,B0o),e(Qr,pd),e(pd,k0o),e(pd,Sae),e(Sae,x0o),e(pd,R0o),e(pd,Pae),e(Pae,S0o),e(pd,P0o),e(Qr,$0o),e(Qr,$ae),e($ae,I0o),e(Qr,j0o),g(JE,Qr,null),e(er,N0o),e(er,De),g(YE,De,null),e(De,D0o),e(De,Iae),e(Iae,q0o),e(De,G0o),e(De,Ha),e(Ha,O0o),e(Ha,jae),e(jae,X0o),e(Ha,z0o),e(Ha,Nae),e(Nae,V0o),e(Ha,W0o),e(Ha,Dae),e(Dae,Q0o),e(Ha,H0o),e(De,U0o),e(De,R),e(R,Rb),e(Rb,qae),e(qae,J0o),e(Rb,Y0o),e(Rb,ij),e(ij,K0o),e(Rb,Z0o),e(R,eLo),e(R,Sb),e(Sb,Gae),e(Gae,oLo),e(Sb,rLo),e(Sb,dj),e(dj,tLo),e(Sb,aLo),e(R,nLo),e(R,Pb),e(Pb,Oae),e(Oae,sLo),e(Pb,lLo),e(Pb,cj),e(cj,iLo),e(Pb,dLo),e(R,cLo),e(R,$b),e($b,Xae),e(Xae,fLo),e($b,mLo),e($b,fj),e(fj,gLo),e($b,hLo),e(R,pLo),e(R,Ib),e(Ib,zae),e(zae,_Lo),e(Ib,uLo),e(Ib,mj),e(mj,bLo),e(Ib,vLo),e(R,TLo),e(R,jb),e(jb,Vae),e(Vae,FLo),e(jb,CLo),e(jb,gj),e(gj,MLo),e(jb,ELo),e(R,yLo),e(R,Nb),e(Nb,Wae),e(Wae,wLo),e(Nb,ALo),e(Nb,hj),e(hj,LLo),e(Nb,BLo),e(R,kLo),e(R,Db),e(Db,Qae),e(Qae,xLo),e(Db,RLo),e(Db,pj),e(pj,SLo),e(Db,PLo),e(R,$Lo),e(R,qb),e(qb,Hae),e(Hae,ILo),e(qb,jLo),e(qb,_j),e(_j,NLo),e(qb,DLo),e(R,qLo),e(R,Gb),e(Gb,Uae),e(Uae,GLo),e(Gb,OLo),e(Gb,uj),e(uj,XLo),e(Gb,zLo),e(R,VLo),e(R,Ob),e(Ob,Jae),e(Jae,WLo),e(Ob,QLo),e(Ob,bj),e(bj,HLo),e(Ob,ULo),e(R,JLo),e(R,Xb),e(Xb,Yae),e(Yae,YLo),e(Xb,KLo),e(Xb,vj),e(vj,ZLo),e(Xb,e7o),e(R,o7o),e(R,zb),e(zb,Kae),e(Kae,r7o),e(zb,t7o),e(zb,Tj),e(Tj,a7o),e(zb,n7o),e(R,s7o),e(R,Vb),e(Vb,Zae),e(Zae,l7o),e(Vb,i7o),e(Vb,Fj),e(Fj,d7o),e(Vb,c7o),e(R,f7o),e(R,Wb),e(Wb,ene),e(ene,m7o),e(Wb,g7o),e(Wb,Cj),e(Cj,h7o),e(Wb,p7o),e(R,_7o),e(R,Qb),e(Qb,one),e(one,u7o),e(Qb,b7o),e(Qb,Mj),e(Mj,v7o),e(Qb,T7o),e(R,F7o),e(R,Hb),e(Hb,rne),e(rne,C7o),e(Hb,M7o),e(Hb,Ej),e(Ej,E7o),e(Hb,y7o),e(R,w7o),e(R,Ub),e(Ub,tne),e(tne,A7o),e(Ub,L7o),e(Ub,yj),e(yj,B7o),e(Ub,k7o),e(R,x7o),e(R,Jb),e(Jb,ane),e(ane,R7o),e(Jb,S7o),e(Jb,wj),e(wj,P7o),e(Jb,$7o),e(R,I7o),e(R,Yb),e(Yb,nne),e(nne,j7o),e(Yb,N7o),e(Yb,Aj),e(Aj,D7o),e(Yb,q7o),e(R,G7o),e(R,Kb),e(Kb,sne),e(sne,O7o),e(Kb,X7o),e(Kb,Lj),e(Lj,z7o),e(Kb,V7o),e(R,W7o),e(R,Zb),e(Zb,lne),e(lne,Q7o),e(Zb,H7o),e(Zb,Bj),e(Bj,U7o),e(Zb,J7o),e(R,Y7o),e(R,e5),e(e5,ine),e(ine,K7o),e(e5,Z7o),e(e5,kj),e(kj,e9o),e(e5,o9o),e(R,r9o),e(R,o5),e(o5,dne),e(dne,t9o),e(o5,a9o),e(o5,xj),e(xj,n9o),e(o5,s9o),e(R,l9o),e(R,r5),e(r5,cne),e(cne,i9o),e(r5,d9o),e(r5,Rj),e(Rj,c9o),e(r5,f9o),e(R,m9o),e(R,t5),e(t5,fne),e(fne,g9o),e(t5,h9o),e(t5,Sj),e(Sj,p9o),e(t5,_9o),e(R,u9o),e(R,a5),e(a5,mne),e(mne,b9o),e(a5,v9o),e(a5,Pj),e(Pj,T9o),e(a5,F9o),e(R,C9o),e(R,n5),e(n5,gne),e(gne,M9o),e(n5,E9o),e(n5,$j),e($j,y9o),e(n5,w9o),e(R,A9o),e(R,s5),e(s5,hne),e(hne,L9o),e(s5,B9o),e(s5,Ij),e(Ij,k9o),e(s5,x9o),e(R,R9o),e(R,l5),e(l5,pne),e(pne,S9o),e(l5,P9o),e(l5,jj),e(jj,$9o),e(l5,I9o),e(R,j9o),e(R,i5),e(i5,_ne),e(_ne,N9o),e(i5,D9o),e(i5,Nj),e(Nj,q9o),e(i5,G9o),e(R,O9o),e(R,d5),e(d5,une),e(une,X9o),e(d5,z9o),e(d5,Dj),e(Dj,V9o),e(d5,W9o),e(R,Q9o),e(R,c5),e(c5,bne),e(bne,H9o),e(c5,U9o),e(c5,qj),e(qj,J9o),e(c5,Y9o),e(R,K9o),e(R,f5),e(f5,vne),e(vne,Z9o),e(f5,eBo),e(f5,Gj),e(Gj,oBo),e(f5,rBo),e(R,tBo),e(R,m5),e(m5,Tne),e(Tne,aBo),e(m5,nBo),e(m5,Oj),e(Oj,sBo),e(m5,lBo),e(R,iBo),e(R,g5),e(g5,Fne),e(Fne,dBo),e(g5,cBo),e(g5,Xj),e(Xj,fBo),e(g5,mBo),e(R,gBo),e(R,h5),e(h5,Cne),e(Cne,hBo),e(h5,pBo),e(h5,zj),e(zj,_Bo),e(h5,uBo),e(R,bBo),e(R,p5),e(p5,Mne),e(Mne,vBo),e(p5,TBo),e(p5,Vj),e(Vj,FBo),e(p5,CBo),e(De,MBo),e(De,_5),e(_5,EBo),e(_5,Ene),e(Ene,yBo),e(_5,wBo),e(_5,yne),e(yne,ABo),e(De,LBo),e(De,wne),e(wne,BBo),e(De,kBo),g(KE,De,null),b(d,z7e,u),b(d,_d,u),e(_d,u5),e(u5,Ane),g(ZE,Ane,null),e(_d,xBo),e(_d,Lne),e(Lne,RBo),b(d,V7e,u),b(d,or,u),g(e3,or,null),e(or,SBo),e(or,ud),e(ud,PBo),e(ud,Bne),e(Bne,$Bo),e(ud,IBo),e(ud,kne),e(kne,jBo),e(ud,NBo),e(or,DBo),e(or,o3),e(o3,qBo),e(o3,xne),e(xne,GBo),e(o3,OBo),e(or,XBo),e(or,Hr),g(r3,Hr,null),e(Hr,zBo),e(Hr,Rne),e(Rne,VBo),e(Hr,WBo),e(Hr,bd),e(bd,QBo),e(bd,Sne),e(Sne,HBo),e(bd,UBo),e(bd,Pne),e(Pne,JBo),e(bd,YBo),e(Hr,KBo),e(Hr,$ne),e($ne,ZBo),e(Hr,eko),g(t3,Hr,null),e(or,oko),e(or,qe),g(a3,qe,null),e(qe,rko),e(qe,Ine),e(Ine,tko),e(qe,ako),e(qe,Ua),e(Ua,nko),e(Ua,jne),e(jne,sko),e(Ua,lko),e(Ua,Nne),e(Nne,iko),e(Ua,dko),e(Ua,Dne),e(Dne,cko),e(Ua,fko),e(qe,mko),e(qe,qne),e(qne,b5),e(b5,Gne),e(Gne,gko),e(b5,hko),e(b5,Wj),e(Wj,pko),e(b5,_ko),e(qe,uko),e(qe,v5),e(v5,bko),e(v5,One),e(One,vko),e(v5,Tko),e(v5,Xne),e(Xne,Fko),e(qe,Cko),e(qe,zne),e(zne,Mko),e(qe,Eko),g(n3,qe,null),b(d,W7e,u),b(d,vd,u),e(vd,T5),e(T5,Vne),g(s3,Vne,null),e(vd,yko),e(vd,Wne),e(Wne,wko),b(d,Q7e,u),b(d,rr,u),g(l3,rr,null),e(rr,Ako),e(rr,Td),e(Td,Lko),e(Td,Qne),e(Qne,Bko),e(Td,kko),e(Td,Hne),e(Hne,xko),e(Td,Rko),e(rr,Sko),e(rr,i3),e(i3,Pko),e(i3,Une),e(Une,$ko),e(i3,Iko),e(rr,jko),e(rr,Ur),g(d3,Ur,null),e(Ur,Nko),e(Ur,Jne),e(Jne,Dko),e(Ur,qko),e(Ur,Fd),e(Fd,Gko),e(Fd,Yne),e(Yne,Oko),e(Fd,Xko),e(Fd,Kne),e(Kne,zko),e(Fd,Vko),e(Ur,Wko),e(Ur,Zne),e(Zne,Qko),e(Ur,Hko),g(c3,Ur,null),e(rr,Uko),e(rr,Ge),g(f3,Ge,null),e(Ge,Jko),e(Ge,ese),e(ese,Yko),e(Ge,Kko),e(Ge,Ja),e(Ja,Zko),e(Ja,ose),e(ose,exo),e(Ja,oxo),e(Ja,rse),e(rse,rxo),e(Ja,txo),e(Ja,tse),e(tse,axo),e(Ja,nxo),e(Ge,sxo),e(Ge,be),e(be,F5),e(F5,ase),e(ase,lxo),e(F5,ixo),e(F5,Qj),e(Qj,dxo),e(F5,cxo),e(be,fxo),e(be,C5),e(C5,nse),e(nse,mxo),e(C5,gxo),e(C5,Hj),e(Hj,hxo),e(C5,pxo),e(be,_xo),e(be,Rs),e(Rs,sse),e(sse,uxo),e(Rs,bxo),e(Rs,Uj),e(Uj,vxo),e(Rs,Txo),e(Rs,Jj),e(Jj,Fxo),e(Rs,Cxo),e(be,Mxo),e(be,M5),e(M5,lse),e(lse,Exo),e(M5,yxo),e(M5,Yj),e(Yj,wxo),e(M5,Axo),e(be,Lxo),e(be,la),e(la,ise),e(ise,Bxo),e(la,kxo),e(la,Kj),e(Kj,xxo),e(la,Rxo),e(la,Zj),e(Zj,Sxo),e(la,Pxo),e(la,eN),e(eN,$xo),e(la,Ixo),e(be,jxo),e(be,E5),e(E5,dse),e(dse,Nxo),e(E5,Dxo),e(E5,oN),e(oN,qxo),e(E5,Gxo),e(be,Oxo),e(be,y5),e(y5,cse),e(cse,Xxo),e(y5,zxo),e(y5,rN),e(rN,Vxo),e(y5,Wxo),e(be,Qxo),e(be,w5),e(w5,fse),e(fse,Hxo),e(w5,Uxo),e(w5,tN),e(tN,Jxo),e(w5,Yxo),e(be,Kxo),e(be,A5),e(A5,mse),e(mse,Zxo),e(A5,eRo),e(A5,aN),e(aN,oRo),e(A5,rRo),e(Ge,tRo),e(Ge,L5),e(L5,aRo),e(L5,gse),e(gse,nRo),e(L5,sRo),e(L5,hse),e(hse,lRo),e(Ge,iRo),e(Ge,pse),e(pse,dRo),e(Ge,cRo),g(m3,Ge,null),b(d,H7e,u),b(d,Cd,u),e(Cd,B5),e(B5,_se),g(g3,_se,null),e(Cd,fRo),e(Cd,use),e(use,mRo),b(d,U7e,u),b(d,tr,u),g(h3,tr,null),e(tr,gRo),e(tr,Md),e(Md,hRo),e(Md,bse),e(bse,pRo),e(Md,_Ro),e(Md,vse),e(vse,uRo),e(Md,bRo),e(tr,vRo),e(tr,p3),e(p3,TRo),e(p3,Tse),e(Tse,FRo),e(p3,CRo),e(tr,MRo),e(tr,Jr),g(_3,Jr,null),e(Jr,ERo),e(Jr,Fse),e(Fse,yRo),e(Jr,wRo),e(Jr,Ed),e(Ed,ARo),e(Ed,Cse),e(Cse,LRo),e(Ed,BRo),e(Ed,Mse),e(Mse,kRo),e(Ed,xRo),e(Jr,RRo),e(Jr,Ese),e(Ese,SRo),e(Jr,PRo),g(u3,Jr,null),e(tr,$Ro),e(tr,Oe),g(b3,Oe,null),e(Oe,IRo),e(Oe,yse),e(yse,jRo),e(Oe,NRo),e(Oe,Ya),e(Ya,DRo),e(Ya,wse),e(wse,qRo),e(Ya,GRo),e(Ya,Ase),e(Ase,ORo),e(Ya,XRo),e(Ya,Lse),e(Lse,zRo),e(Ya,VRo),e(Oe,WRo),e(Oe,Bse),e(Bse,k5),e(k5,kse),e(kse,QRo),e(k5,HRo),e(k5,nN),e(nN,URo),e(k5,JRo),e(Oe,YRo),e(Oe,x5),e(x5,KRo),e(x5,xse),e(xse,ZRo),e(x5,eSo),e(x5,Rse),e(Rse,oSo),e(Oe,rSo),e(Oe,Sse),e(Sse,tSo),e(Oe,aSo),g(v3,Oe,null),b(d,J7e,u),b(d,yd,u),e(yd,R5),e(R5,Pse),g(T3,Pse,null),e(yd,nSo),e(yd,$se),e($se,sSo),b(d,Y7e,u),b(d,ar,u),g(F3,ar,null),e(ar,lSo),e(ar,wd),e(wd,iSo),e(wd,Ise),e(Ise,dSo),e(wd,cSo),e(wd,jse),e(jse,fSo),e(wd,mSo),e(ar,gSo),e(ar,C3),e(C3,hSo),e(C3,Nse),e(Nse,pSo),e(C3,_So),e(ar,uSo),e(ar,Yr),g(M3,Yr,null),e(Yr,bSo),e(Yr,Dse),e(Dse,vSo),e(Yr,TSo),e(Yr,Ad),e(Ad,FSo),e(Ad,qse),e(qse,CSo),e(Ad,MSo),e(Ad,Gse),e(Gse,ESo),e(Ad,ySo),e(Yr,wSo),e(Yr,Ose),e(Ose,ASo),e(Yr,LSo),g(E3,Yr,null),e(ar,BSo),e(ar,Xe),g(y3,Xe,null),e(Xe,kSo),e(Xe,Xse),e(Xse,xSo),e(Xe,RSo),e(Xe,Ka),e(Ka,SSo),e(Ka,zse),e(zse,PSo),e(Ka,$So),e(Ka,Vse),e(Vse,ISo),e(Ka,jSo),e(Ka,Wse),e(Wse,NSo),e(Ka,DSo),e(Xe,qSo),e(Xe,ao),e(ao,S5),e(S5,Qse),e(Qse,GSo),e(S5,OSo),e(S5,sN),e(sN,XSo),e(S5,zSo),e(ao,VSo),e(ao,P5),e(P5,Hse),e(Hse,WSo),e(P5,QSo),e(P5,lN),e(lN,HSo),e(P5,USo),e(ao,JSo),e(ao,$5),e($5,Use),e(Use,YSo),e($5,KSo),e($5,iN),e(iN,ZSo),e($5,ePo),e(ao,oPo),e(ao,I5),e(I5,Jse),e(Jse,rPo),e(I5,tPo),e(I5,dN),e(dN,aPo),e(I5,nPo),e(ao,sPo),e(ao,j5),e(j5,Yse),e(Yse,lPo),e(j5,iPo),e(j5,cN),e(cN,dPo),e(j5,cPo),e(ao,fPo),e(ao,N5),e(N5,Kse),e(Kse,mPo),e(N5,gPo),e(N5,fN),e(fN,hPo),e(N5,pPo),e(ao,_Po),e(ao,D5),e(D5,Zse),e(Zse,uPo),e(D5,bPo),e(D5,mN),e(mN,vPo),e(D5,TPo),e(Xe,FPo),e(Xe,q5),e(q5,CPo),e(q5,ele),e(ele,MPo),e(q5,EPo),e(q5,ole),e(ole,yPo),e(Xe,wPo),e(Xe,rle),e(rle,APo),e(Xe,LPo),g(w3,Xe,null),b(d,K7e,u),b(d,Ld,u),e(Ld,G5),e(G5,tle),g(A3,tle,null),e(Ld,BPo),e(Ld,ale),e(ale,kPo),b(d,Z7e,u),b(d,nr,u),g(L3,nr,null),e(nr,xPo),e(nr,Bd),e(Bd,RPo),e(Bd,nle),e(nle,SPo),e(Bd,PPo),e(Bd,sle),e(sle,$Po),e(Bd,IPo),e(nr,jPo),e(nr,B3),e(B3,NPo),e(B3,lle),e(lle,DPo),e(B3,qPo),e(nr,GPo),e(nr,Kr),g(k3,Kr,null),e(Kr,OPo),e(Kr,ile),e(ile,XPo),e(Kr,zPo),e(Kr,kd),e(kd,VPo),e(kd,dle),e(dle,WPo),e(kd,QPo),e(kd,cle),e(cle,HPo),e(kd,UPo),e(Kr,JPo),e(Kr,fle),e(fle,YPo),e(Kr,KPo),g(x3,Kr,null),e(nr,ZPo),e(nr,ze),g(R3,ze,null),e(ze,e$o),e(ze,mle),e(mle,o$o),e(ze,r$o),e(ze,Za),e(Za,t$o),e(Za,gle),e(gle,a$o),e(Za,n$o),e(Za,hle),e(hle,s$o),e(Za,l$o),e(Za,ple),e(ple,i$o),e(Za,d$o),e(ze,c$o),e(ze,xd),e(xd,O5),e(O5,_le),e(_le,f$o),e(O5,m$o),e(O5,gN),e(gN,g$o),e(O5,h$o),e(xd,p$o),e(xd,X5),e(X5,ule),e(ule,_$o),e(X5,u$o),e(X5,hN),e(hN,b$o),e(X5,v$o),e(xd,T$o),e(xd,z5),e(z5,ble),e(ble,F$o),e(z5,C$o),e(z5,pN),e(pN,M$o),e(z5,E$o),e(ze,y$o),e(ze,V5),e(V5,w$o),e(V5,vle),e(vle,A$o),e(V5,L$o),e(V5,Tle),e(Tle,B$o),e(ze,k$o),e(ze,Fle),e(Fle,x$o),e(ze,R$o),g(S3,ze,null),b(d,e9e,u),b(d,Rd,u),e(Rd,W5),e(W5,Cle),g(P3,Cle,null),e(Rd,S$o),e(Rd,Mle),e(Mle,P$o),b(d,o9e,u),b(d,sr,u),g($3,sr,null),e(sr,$$o),e(sr,Sd),e(Sd,I$o),e(Sd,Ele),e(Ele,j$o),e(Sd,N$o),e(Sd,yle),e(yle,D$o),e(Sd,q$o),e(sr,G$o),e(sr,I3),e(I3,O$o),e(I3,wle),e(wle,X$o),e(I3,z$o),e(sr,V$o),e(sr,Zr),g(j3,Zr,null),e(Zr,W$o),e(Zr,Ale),e(Ale,Q$o),e(Zr,H$o),e(Zr,Pd),e(Pd,U$o),e(Pd,Lle),e(Lle,J$o),e(Pd,Y$o),e(Pd,Ble),e(Ble,K$o),e(Pd,Z$o),e(Zr,eIo),e(Zr,kle),e(kle,oIo),e(Zr,rIo),g(N3,Zr,null),e(sr,tIo),e(sr,Ve),g(D3,Ve,null),e(Ve,aIo),e(Ve,xle),e(xle,nIo),e(Ve,sIo),e(Ve,en),e(en,lIo),e(en,Rle),e(Rle,iIo),e(en,dIo),e(en,Sle),e(Sle,cIo),e(en,fIo),e(en,Ple),e(Ple,mIo),e(en,gIo),e(Ve,hIo),e(Ve,no),e(no,Q5),e(Q5,$le),e($le,pIo),e(Q5,_Io),e(Q5,_N),e(_N,uIo),e(Q5,bIo),e(no,vIo),e(no,H5),e(H5,Ile),e(Ile,TIo),e(H5,FIo),e(H5,uN),e(uN,CIo),e(H5,MIo),e(no,EIo),e(no,U5),e(U5,jle),e(jle,yIo),e(U5,wIo),e(U5,bN),e(bN,AIo),e(U5,LIo),e(no,BIo),e(no,J5),e(J5,Nle),e(Nle,kIo),e(J5,xIo),e(J5,vN),e(vN,RIo),e(J5,SIo),e(no,PIo),e(no,Y5),e(Y5,Dle),e(Dle,$Io),e(Y5,IIo),e(Y5,TN),e(TN,jIo),e(Y5,NIo),e(no,DIo),e(no,K5),e(K5,qle),e(qle,qIo),e(K5,GIo),e(K5,FN),e(FN,OIo),e(K5,XIo),e(no,zIo),e(no,Z5),e(Z5,Gle),e(Gle,VIo),e(Z5,WIo),e(Z5,CN),e(CN,QIo),e(Z5,HIo),e(Ve,UIo),e(Ve,ev),e(ev,JIo),e(ev,Ole),e(Ole,YIo),e(ev,KIo),e(ev,Xle),e(Xle,ZIo),e(Ve,ejo),e(Ve,zle),e(zle,ojo),e(Ve,rjo),g(q3,Ve,null),b(d,r9e,u),b(d,$d,u),e($d,ov),e(ov,Vle),g(G3,Vle,null),e($d,tjo),e($d,Wle),e(Wle,ajo),b(d,t9e,u),b(d,lr,u),g(O3,lr,null),e(lr,njo),e(lr,Id),e(Id,sjo),e(Id,Qle),e(Qle,ljo),e(Id,ijo),e(Id,Hle),e(Hle,djo),e(Id,cjo),e(lr,fjo),e(lr,X3),e(X3,mjo),e(X3,Ule),e(Ule,gjo),e(X3,hjo),e(lr,pjo),e(lr,et),g(z3,et,null),e(et,_jo),e(et,Jle),e(Jle,ujo),e(et,bjo),e(et,jd),e(jd,vjo),e(jd,Yle),e(Yle,Tjo),e(jd,Fjo),e(jd,Kle),e(Kle,Cjo),e(jd,Mjo),e(et,Ejo),e(et,Zle),e(Zle,yjo),e(et,wjo),g(V3,et,null),e(lr,Ajo),e(lr,We),g(W3,We,null),e(We,Ljo),e(We,eie),e(eie,Bjo),e(We,kjo),e(We,on),e(on,xjo),e(on,oie),e(oie,Rjo),e(on,Sjo),e(on,rie),e(rie,Pjo),e(on,$jo),e(on,tie),e(tie,Ijo),e(on,jjo),e(We,Njo),e(We,Q3),e(Q3,rv),e(rv,aie),e(aie,Djo),e(rv,qjo),e(rv,MN),e(MN,Gjo),e(rv,Ojo),e(Q3,Xjo),e(Q3,tv),e(tv,nie),e(nie,zjo),e(tv,Vjo),e(tv,EN),e(EN,Wjo),e(tv,Qjo),e(We,Hjo),e(We,av),e(av,Ujo),e(av,sie),e(sie,Jjo),e(av,Yjo),e(av,lie),e(lie,Kjo),e(We,Zjo),e(We,iie),e(iie,eNo),e(We,oNo),g(H3,We,null),b(d,a9e,u),b(d,Nd,u),e(Nd,nv),e(nv,die),g(U3,die,null),e(Nd,rNo),e(Nd,cie),e(cie,tNo),b(d,n9e,u),b(d,ir,u),g(J3,ir,null),e(ir,aNo),e(ir,Dd),e(Dd,nNo),e(Dd,fie),e(fie,sNo),e(Dd,lNo),e(Dd,mie),e(mie,iNo),e(Dd,dNo),e(ir,cNo),e(ir,Y3),e(Y3,fNo),e(Y3,gie),e(gie,mNo),e(Y3,gNo),e(ir,hNo),e(ir,ot),g(K3,ot,null),e(ot,pNo),e(ot,hie),e(hie,_No),e(ot,uNo),e(ot,qd),e(qd,bNo),e(qd,pie),e(pie,vNo),e(qd,TNo),e(qd,_ie),e(_ie,FNo),e(qd,CNo),e(ot,MNo),e(ot,uie),e(uie,ENo),e(ot,yNo),g(Z3,ot,null),e(ir,wNo),e(ir,Qe),g(ey,Qe,null),e(Qe,ANo),e(Qe,bie),e(bie,LNo),e(Qe,BNo),e(Qe,rn),e(rn,kNo),e(rn,vie),e(vie,xNo),e(rn,RNo),e(rn,Tie),e(Tie,SNo),e(rn,PNo),e(rn,Fie),e(Fie,$No),e(rn,INo),e(Qe,jNo),e(Qe,Gd),e(Gd,sv),e(sv,Cie),e(Cie,NNo),e(sv,DNo),e(sv,yN),e(yN,qNo),e(sv,GNo),e(Gd,ONo),e(Gd,lv),e(lv,Mie),e(Mie,XNo),e(lv,zNo),e(lv,wN),e(wN,VNo),e(lv,WNo),e(Gd,QNo),e(Gd,iv),e(iv,Eie),e(Eie,HNo),e(iv,UNo),e(iv,AN),e(AN,JNo),e(iv,YNo),e(Qe,KNo),e(Qe,dv),e(dv,ZNo),e(dv,yie),e(yie,eDo),e(dv,oDo),e(dv,wie),e(wie,rDo),e(Qe,tDo),e(Qe,Aie),e(Aie,aDo),e(Qe,nDo),g(oy,Qe,null),b(d,s9e,u),b(d,Od,u),e(Od,cv),e(cv,Lie),g(ry,Lie,null),e(Od,sDo),e(Od,Bie),e(Bie,lDo),b(d,l9e,u),b(d,dr,u),g(ty,dr,null),e(dr,iDo),e(dr,Xd),e(Xd,dDo),e(Xd,kie),e(kie,cDo),e(Xd,fDo),e(Xd,xie),e(xie,mDo),e(Xd,gDo),e(dr,hDo),e(dr,ay),e(ay,pDo),e(ay,Rie),e(Rie,_Do),e(ay,uDo),e(dr,bDo),e(dr,rt),g(ny,rt,null),e(rt,vDo),e(rt,Sie),e(Sie,TDo),e(rt,FDo),e(rt,zd),e(zd,CDo),e(zd,Pie),e(Pie,MDo),e(zd,EDo),e(zd,$ie),e($ie,yDo),e(zd,wDo),e(rt,ADo),e(rt,Iie),e(Iie,LDo),e(rt,BDo),g(sy,rt,null),e(dr,kDo),e(dr,He),g(ly,He,null),e(He,xDo),e(He,jie),e(jie,RDo),e(He,SDo),e(He,tn),e(tn,PDo),e(tn,Nie),e(Nie,$Do),e(tn,IDo),e(tn,Die),e(Die,jDo),e(tn,NDo),e(tn,qie),e(qie,DDo),e(tn,qDo),e(He,GDo),e(He,Vd),e(Vd,fv),e(fv,Gie),e(Gie,ODo),e(fv,XDo),e(fv,LN),e(LN,zDo),e(fv,VDo),e(Vd,WDo),e(Vd,mv),e(mv,Oie),e(Oie,QDo),e(mv,HDo),e(mv,BN),e(BN,UDo),e(mv,JDo),e(Vd,YDo),e(Vd,gv),e(gv,Xie),e(Xie,KDo),e(gv,ZDo),e(gv,kN),e(kN,eqo),e(gv,oqo),e(He,rqo),e(He,hv),e(hv,tqo),e(hv,zie),e(zie,aqo),e(hv,nqo),e(hv,Vie),e(Vie,sqo),e(He,lqo),e(He,Wie),e(Wie,iqo),e(He,dqo),g(iy,He,null),b(d,i9e,u),b(d,Wd,u),e(Wd,pv),e(pv,Qie),g(dy,Qie,null),e(Wd,cqo),e(Wd,Hie),e(Hie,fqo),b(d,d9e,u),b(d,cr,u),g(cy,cr,null),e(cr,mqo),e(cr,Qd),e(Qd,gqo),e(Qd,Uie),e(Uie,hqo),e(Qd,pqo),e(Qd,Jie),e(Jie,_qo),e(Qd,uqo),e(cr,bqo),e(cr,fy),e(fy,vqo),e(fy,Yie),e(Yie,Tqo),e(fy,Fqo),e(cr,Cqo),e(cr,tt),g(my,tt,null),e(tt,Mqo),e(tt,Kie),e(Kie,Eqo),e(tt,yqo),e(tt,Hd),e(Hd,wqo),e(Hd,Zie),e(Zie,Aqo),e(Hd,Lqo),e(Hd,ede),e(ede,Bqo),e(Hd,kqo),e(tt,xqo),e(tt,ode),e(ode,Rqo),e(tt,Sqo),g(gy,tt,null),e(cr,Pqo),e(cr,Ue),g(hy,Ue,null),e(Ue,$qo),e(Ue,rde),e(rde,Iqo),e(Ue,jqo),e(Ue,an),e(an,Nqo),e(an,tde),e(tde,Dqo),e(an,qqo),e(an,ade),e(ade,Gqo),e(an,Oqo),e(an,nde),e(nde,Xqo),e(an,zqo),e(Ue,Vqo),e(Ue,sde),e(sde,_v),e(_v,lde),e(lde,Wqo),e(_v,Qqo),e(_v,xN),e(xN,Hqo),e(_v,Uqo),e(Ue,Jqo),e(Ue,uv),e(uv,Yqo),e(uv,ide),e(ide,Kqo),e(uv,Zqo),e(uv,dde),e(dde,eGo),e(Ue,oGo),e(Ue,cde),e(cde,rGo),e(Ue,tGo),g(py,Ue,null),b(d,c9e,u),b(d,Ud,u),e(Ud,bv),e(bv,fde),g(_y,fde,null),e(Ud,aGo),e(Ud,mde),e(mde,nGo),b(d,f9e,u),b(d,fr,u),g(uy,fr,null),e(fr,sGo),e(fr,Jd),e(Jd,lGo),e(Jd,gde),e(gde,iGo),e(Jd,dGo),e(Jd,hde),e(hde,cGo),e(Jd,fGo),e(fr,mGo),e(fr,by),e(by,gGo),e(by,pde),e(pde,hGo),e(by,pGo),e(fr,_Go),e(fr,at),g(vy,at,null),e(at,uGo),e(at,_de),e(_de,bGo),e(at,vGo),e(at,Yd),e(Yd,TGo),e(Yd,ude),e(ude,FGo),e(Yd,CGo),e(Yd,bde),e(bde,MGo),e(Yd,EGo),e(at,yGo),e(at,vde),e(vde,wGo),e(at,AGo),g(Ty,at,null),e(fr,LGo),e(fr,Je),g(Fy,Je,null),e(Je,BGo),e(Je,Tde),e(Tde,kGo),e(Je,xGo),e(Je,nn),e(nn,RGo),e(nn,Fde),e(Fde,SGo),e(nn,PGo),e(nn,Cde),e(Cde,$Go),e(nn,IGo),e(nn,Mde),e(Mde,jGo),e(nn,NGo),e(Je,DGo),e(Je,Ede),e(Ede,vv),e(vv,yde),e(yde,qGo),e(vv,GGo),e(vv,RN),e(RN,OGo),e(vv,XGo),e(Je,zGo),e(Je,Tv),e(Tv,VGo),e(Tv,wde),e(wde,WGo),e(Tv,QGo),e(Tv,Ade),e(Ade,HGo),e(Je,UGo),e(Je,Lde),e(Lde,JGo),e(Je,YGo),g(Cy,Je,null),b(d,m9e,u),b(d,Kd,u),e(Kd,Fv),e(Fv,Bde),g(My,Bde,null),e(Kd,KGo),e(Kd,kde),e(kde,ZGo),b(d,g9e,u),b(d,mr,u),g(Ey,mr,null),e(mr,eOo),e(mr,Zd),e(Zd,oOo),e(Zd,xde),e(xde,rOo),e(Zd,tOo),e(Zd,Rde),e(Rde,aOo),e(Zd,nOo),e(mr,sOo),e(mr,yy),e(yy,lOo),e(yy,Sde),e(Sde,iOo),e(yy,dOo),e(mr,cOo),e(mr,nt),g(wy,nt,null),e(nt,fOo),e(nt,Pde),e(Pde,mOo),e(nt,gOo),e(nt,ec),e(ec,hOo),e(ec,$de),e($de,pOo),e(ec,_Oo),e(ec,Ide),e(Ide,uOo),e(ec,bOo),e(nt,vOo),e(nt,jde),e(jde,TOo),e(nt,FOo),g(Ay,nt,null),e(mr,COo),e(mr,Ye),g(Ly,Ye,null),e(Ye,MOo),e(Ye,Nde),e(Nde,EOo),e(Ye,yOo),e(Ye,sn),e(sn,wOo),e(sn,Dde),e(Dde,AOo),e(sn,LOo),e(sn,qde),e(qde,BOo),e(sn,kOo),e(sn,Gde),e(Gde,xOo),e(sn,ROo),e(Ye,SOo),e(Ye,By),e(By,Cv),e(Cv,Ode),e(Ode,POo),e(Cv,$Oo),e(Cv,SN),e(SN,IOo),e(Cv,jOo),e(By,NOo),e(By,Mv),e(Mv,Xde),e(Xde,DOo),e(Mv,qOo),e(Mv,PN),e(PN,GOo),e(Mv,OOo),e(Ye,XOo),e(Ye,Ev),e(Ev,zOo),e(Ev,zde),e(zde,VOo),e(Ev,WOo),e(Ev,Vde),e(Vde,QOo),e(Ye,HOo),e(Ye,Wde),e(Wde,UOo),e(Ye,JOo),g(ky,Ye,null),b(d,h9e,u),b(d,oc,u),e(oc,yv),e(yv,Qde),g(xy,Qde,null),e(oc,YOo),e(oc,Hde),e(Hde,KOo),b(d,p9e,u),b(d,gr,u),g(Ry,gr,null),e(gr,ZOo),e(gr,rc),e(rc,eXo),e(rc,Ude),e(Ude,oXo),e(rc,rXo),e(rc,Jde),e(Jde,tXo),e(rc,aXo),e(gr,nXo),e(gr,Sy),e(Sy,sXo),e(Sy,Yde),e(Yde,lXo),e(Sy,iXo),e(gr,dXo),e(gr,st),g(Py,st,null),e(st,cXo),e(st,Kde),e(Kde,fXo),e(st,mXo),e(st,tc),e(tc,gXo),e(tc,Zde),e(Zde,hXo),e(tc,pXo),e(tc,ece),e(ece,_Xo),e(tc,uXo),e(st,bXo),e(st,oce),e(oce,vXo),e(st,TXo),g($y,st,null),e(gr,FXo),e(gr,go),g(Iy,go,null),e(go,CXo),e(go,rce),e(rce,MXo),e(go,EXo),e(go,ln),e(ln,yXo),e(ln,tce),e(tce,wXo),e(ln,AXo),e(ln,ace),e(ace,LXo),e(ln,BXo),e(ln,nce),e(nce,kXo),e(ln,xXo),e(go,RXo),e(go,B),e(B,wv),e(wv,sce),e(sce,SXo),e(wv,PXo),e(wv,$N),e($N,$Xo),e(wv,IXo),e(B,jXo),e(B,Av),e(Av,lce),e(lce,NXo),e(Av,DXo),e(Av,IN),e(IN,qXo),e(Av,GXo),e(B,OXo),e(B,Lv),e(Lv,ice),e(ice,XXo),e(Lv,zXo),e(Lv,jN),e(jN,VXo),e(Lv,WXo),e(B,QXo),e(B,Bv),e(Bv,dce),e(dce,HXo),e(Bv,UXo),e(Bv,NN),e(NN,JXo),e(Bv,YXo),e(B,KXo),e(B,kv),e(kv,cce),e(cce,ZXo),e(kv,ezo),e(kv,DN),e(DN,ozo),e(kv,rzo),e(B,tzo),e(B,xv),e(xv,fce),e(fce,azo),e(xv,nzo),e(xv,qN),e(qN,szo),e(xv,lzo),e(B,izo),e(B,Rv),e(Rv,mce),e(mce,dzo),e(Rv,czo),e(Rv,GN),e(GN,fzo),e(Rv,mzo),e(B,gzo),e(B,Sv),e(Sv,gce),e(gce,hzo),e(Sv,pzo),e(Sv,ON),e(ON,_zo),e(Sv,uzo),e(B,bzo),e(B,Pv),e(Pv,hce),e(hce,vzo),e(Pv,Tzo),e(Pv,XN),e(XN,Fzo),e(Pv,Czo),e(B,Mzo),e(B,$v),e($v,pce),e(pce,Ezo),e($v,yzo),e($v,zN),e(zN,wzo),e($v,Azo),e(B,Lzo),e(B,Iv),e(Iv,_ce),e(_ce,Bzo),e(Iv,kzo),e(Iv,VN),e(VN,xzo),e(Iv,Rzo),e(B,Szo),e(B,jv),e(jv,uce),e(uce,Pzo),e(jv,$zo),e(jv,WN),e(WN,Izo),e(jv,jzo),e(B,Nzo),e(B,Nv),e(Nv,bce),e(bce,Dzo),e(Nv,qzo),e(Nv,QN),e(QN,Gzo),e(Nv,Ozo),e(B,Xzo),e(B,Dv),e(Dv,vce),e(vce,zzo),e(Dv,Vzo),e(Dv,HN),e(HN,Wzo),e(Dv,Qzo),e(B,Hzo),e(B,qv),e(qv,Tce),e(Tce,Uzo),e(qv,Jzo),e(qv,UN),e(UN,Yzo),e(qv,Kzo),e(B,Zzo),e(B,Ss),e(Ss,Fce),e(Fce,eVo),e(Ss,oVo),e(Ss,JN),e(JN,rVo),e(Ss,tVo),e(Ss,YN),e(YN,aVo),e(Ss,nVo),e(B,sVo),e(B,Gv),e(Gv,Cce),e(Cce,lVo),e(Gv,iVo),e(Gv,KN),e(KN,dVo),e(Gv,cVo),e(B,fVo),e(B,Ov),e(Ov,Mce),e(Mce,mVo),e(Ov,gVo),e(Ov,ZN),e(ZN,hVo),e(Ov,pVo),e(B,_Vo),e(B,Xv),e(Xv,Ece),e(Ece,uVo),e(Xv,bVo),e(Xv,eD),e(eD,vVo),e(Xv,TVo),e(B,FVo),e(B,zv),e(zv,yce),e(yce,CVo),e(zv,MVo),e(zv,oD),e(oD,EVo),e(zv,yVo),e(B,wVo),e(B,Vv),e(Vv,wce),e(wce,AVo),e(Vv,LVo),e(Vv,rD),e(rD,BVo),e(Vv,kVo),e(B,xVo),e(B,Wv),e(Wv,Ace),e(Ace,RVo),e(Wv,SVo),e(Wv,tD),e(tD,PVo),e(Wv,$Vo),e(B,IVo),e(B,Qv),e(Qv,Lce),e(Lce,jVo),e(Qv,NVo),e(Qv,aD),e(aD,DVo),e(Qv,qVo),e(B,GVo),e(B,Hv),e(Hv,Bce),e(Bce,OVo),e(Hv,XVo),e(Hv,nD),e(nD,zVo),e(Hv,VVo),e(B,WVo),e(B,Uv),e(Uv,kce),e(kce,QVo),e(Uv,HVo),e(Uv,sD),e(sD,UVo),e(Uv,JVo),e(B,YVo),e(B,Jv),e(Jv,xce),e(xce,KVo),e(Jv,ZVo),e(Jv,lD),e(lD,eWo),e(Jv,oWo),e(B,rWo),e(B,Yv),e(Yv,Rce),e(Rce,tWo),e(Yv,aWo),e(Yv,iD),e(iD,nWo),e(Yv,sWo),e(B,lWo),e(B,Kv),e(Kv,Sce),e(Sce,iWo),e(Kv,dWo),e(Kv,dD),e(dD,cWo),e(Kv,fWo),e(B,mWo),e(B,Zv),e(Zv,Pce),e(Pce,gWo),e(Zv,hWo),e(Zv,cD),e(cD,pWo),e(Zv,_Wo),e(B,uWo),e(B,e6),e(e6,$ce),e($ce,bWo),e(e6,vWo),e(e6,fD),e(fD,TWo),e(e6,FWo),e(B,CWo),e(B,o6),e(o6,Ice),e(Ice,MWo),e(o6,EWo),e(o6,mD),e(mD,yWo),e(o6,wWo),e(B,AWo),e(B,r6),e(r6,jce),e(jce,LWo),e(r6,BWo),e(r6,gD),e(gD,kWo),e(r6,xWo),e(B,RWo),e(B,t6),e(t6,Nce),e(Nce,SWo),e(t6,PWo),e(t6,hD),e(hD,$Wo),e(t6,IWo),e(B,jWo),e(B,a6),e(a6,Dce),e(Dce,NWo),e(a6,DWo),e(a6,pD),e(pD,qWo),e(a6,GWo),e(B,OWo),e(B,n6),e(n6,qce),e(qce,XWo),e(n6,zWo),e(n6,_D),e(_D,VWo),e(n6,WWo),e(B,QWo),e(B,s6),e(s6,Gce),e(Gce,HWo),e(s6,UWo),e(s6,uD),e(uD,JWo),e(s6,YWo),e(B,KWo),e(B,l6),e(l6,Oce),e(Oce,ZWo),e(l6,eQo),e(l6,bD),e(bD,oQo),e(l6,rQo),e(B,tQo),e(B,i6),e(i6,Xce),e(Xce,aQo),e(i6,nQo),e(i6,vD),e(vD,sQo),e(i6,lQo),e(B,iQo),e(B,d6),e(d6,zce),e(zce,dQo),e(d6,cQo),e(d6,TD),e(TD,fQo),e(d6,mQo),e(B,gQo),e(B,c6),e(c6,Vce),e(Vce,hQo),e(c6,pQo),e(c6,FD),e(FD,_Qo),e(c6,uQo),e(B,bQo),e(B,f6),e(f6,Wce),e(Wce,vQo),e(f6,TQo),e(f6,CD),e(CD,FQo),e(f6,CQo),e(go,MQo),e(go,Qce),e(Qce,EQo),e(go,yQo),g(jy,go,null),b(d,_9e,u),b(d,ac,u),e(ac,m6),e(m6,Hce),g(Ny,Hce,null),e(ac,wQo),e(ac,Uce),e(Uce,AQo),b(d,u9e,u),b(d,hr,u),g(Dy,hr,null),e(hr,LQo),e(hr,nc),e(nc,BQo),e(nc,Jce),e(Jce,kQo),e(nc,xQo),e(nc,Yce),e(Yce,RQo),e(nc,SQo),e(hr,PQo),e(hr,qy),e(qy,$Qo),e(qy,Kce),e(Kce,IQo),e(qy,jQo),e(hr,NQo),e(hr,lt),g(Gy,lt,null),e(lt,DQo),e(lt,Zce),e(Zce,qQo),e(lt,GQo),e(lt,sc),e(sc,OQo),e(sc,efe),e(efe,XQo),e(sc,zQo),e(sc,ofe),e(ofe,VQo),e(sc,WQo),e(lt,QQo),e(lt,rfe),e(rfe,HQo),e(lt,UQo),g(Oy,lt,null),e(hr,JQo),e(hr,ho),g(Xy,ho,null),e(ho,YQo),e(ho,tfe),e(tfe,KQo),e(ho,ZQo),e(ho,dn),e(dn,eHo),e(dn,afe),e(afe,oHo),e(dn,rHo),e(dn,nfe),e(nfe,tHo),e(dn,aHo),e(dn,sfe),e(sfe,nHo),e(dn,sHo),e(ho,lHo),e(ho,H),e(H,g6),e(g6,lfe),e(lfe,iHo),e(g6,dHo),e(g6,MD),e(MD,cHo),e(g6,fHo),e(H,mHo),e(H,h6),e(h6,ife),e(ife,gHo),e(h6,hHo),e(h6,ED),e(ED,pHo),e(h6,_Ho),e(H,uHo),e(H,p6),e(p6,dfe),e(dfe,bHo),e(p6,vHo),e(p6,yD),e(yD,THo),e(p6,FHo),e(H,CHo),e(H,_6),e(_6,cfe),e(cfe,MHo),e(_6,EHo),e(_6,wD),e(wD,yHo),e(_6,wHo),e(H,AHo),e(H,u6),e(u6,ffe),e(ffe,LHo),e(u6,BHo),e(u6,AD),e(AD,kHo),e(u6,xHo),e(H,RHo),e(H,b6),e(b6,mfe),e(mfe,SHo),e(b6,PHo),e(b6,LD),e(LD,$Ho),e(b6,IHo),e(H,jHo),e(H,v6),e(v6,gfe),e(gfe,NHo),e(v6,DHo),e(v6,BD),e(BD,qHo),e(v6,GHo),e(H,OHo),e(H,T6),e(T6,hfe),e(hfe,XHo),e(T6,zHo),e(T6,kD),e(kD,VHo),e(T6,WHo),e(H,QHo),e(H,F6),e(F6,pfe),e(pfe,HHo),e(F6,UHo),e(F6,xD),e(xD,JHo),e(F6,YHo),e(H,KHo),e(H,C6),e(C6,_fe),e(_fe,ZHo),e(C6,eUo),e(C6,RD),e(RD,oUo),e(C6,rUo),e(H,tUo),e(H,M6),e(M6,ufe),e(ufe,aUo),e(M6,nUo),e(M6,SD),e(SD,sUo),e(M6,lUo),e(H,iUo),e(H,E6),e(E6,bfe),e(bfe,dUo),e(E6,cUo),e(E6,PD),e(PD,fUo),e(E6,mUo),e(H,gUo),e(H,y6),e(y6,vfe),e(vfe,hUo),e(y6,pUo),e(y6,$D),e($D,_Uo),e(y6,uUo),e(H,bUo),e(H,w6),e(w6,Tfe),e(Tfe,vUo),e(w6,TUo),e(w6,ID),e(ID,FUo),e(w6,CUo),e(H,MUo),e(H,A6),e(A6,Ffe),e(Ffe,EUo),e(A6,yUo),e(A6,jD),e(jD,wUo),e(A6,AUo),e(H,LUo),e(H,L6),e(L6,Cfe),e(Cfe,BUo),e(L6,kUo),e(L6,ND),e(ND,xUo),e(L6,RUo),e(H,SUo),e(H,B6),e(B6,Mfe),e(Mfe,PUo),e(B6,$Uo),e(B6,DD),e(DD,IUo),e(B6,jUo),e(H,NUo),e(H,k6),e(k6,Efe),e(Efe,DUo),e(k6,qUo),e(k6,qD),e(qD,GUo),e(k6,OUo),e(H,XUo),e(H,x6),e(x6,yfe),e(yfe,zUo),e(x6,VUo),e(x6,GD),e(GD,WUo),e(x6,QUo),e(H,HUo),e(H,R6),e(R6,wfe),e(wfe,UUo),e(R6,JUo),e(R6,OD),e(OD,YUo),e(R6,KUo),e(H,ZUo),e(H,S6),e(S6,Afe),e(Afe,eJo),e(S6,oJo),e(S6,XD),e(XD,rJo),e(S6,tJo),e(H,aJo),e(H,P6),e(P6,Lfe),e(Lfe,nJo),e(P6,sJo),e(P6,zD),e(zD,lJo),e(P6,iJo),e(ho,dJo),e(ho,Bfe),e(Bfe,cJo),e(ho,fJo),g(zy,ho,null),b(d,b9e,u),b(d,lc,u),e(lc,$6),e($6,kfe),g(Vy,kfe,null),e(lc,mJo),e(lc,xfe),e(xfe,gJo),b(d,v9e,u),b(d,pr,u),g(Wy,pr,null),e(pr,hJo),e(pr,ic),e(ic,pJo),e(ic,Rfe),e(Rfe,_Jo),e(ic,uJo),e(ic,Sfe),e(Sfe,bJo),e(ic,vJo),e(pr,TJo),e(pr,Qy),e(Qy,FJo),e(Qy,Pfe),e(Pfe,CJo),e(Qy,MJo),e(pr,EJo),e(pr,it),g(Hy,it,null),e(it,yJo),e(it,$fe),e($fe,wJo),e(it,AJo),e(it,dc),e(dc,LJo),e(dc,Ife),e(Ife,BJo),e(dc,kJo),e(dc,jfe),e(jfe,xJo),e(dc,RJo),e(it,SJo),e(it,Nfe),e(Nfe,PJo),e(it,$Jo),g(Uy,it,null),e(pr,IJo),e(pr,po),g(Jy,po,null),e(po,jJo),e(po,Dfe),e(Dfe,NJo),e(po,DJo),e(po,cn),e(cn,qJo),e(cn,qfe),e(qfe,GJo),e(cn,OJo),e(cn,Gfe),e(Gfe,XJo),e(cn,zJo),e(cn,Ofe),e(Ofe,VJo),e(cn,WJo),e(po,QJo),e(po,he),e(he,I6),e(I6,Xfe),e(Xfe,HJo),e(I6,UJo),e(I6,VD),e(VD,JJo),e(I6,YJo),e(he,KJo),e(he,j6),e(j6,zfe),e(zfe,ZJo),e(j6,eYo),e(j6,WD),e(WD,oYo),e(j6,rYo),e(he,tYo),e(he,N6),e(N6,Vfe),e(Vfe,aYo),e(N6,nYo),e(N6,QD),e(QD,sYo),e(N6,lYo),e(he,iYo),e(he,D6),e(D6,Wfe),e(Wfe,dYo),e(D6,cYo),e(D6,HD),e(HD,fYo),e(D6,mYo),e(he,gYo),e(he,q6),e(q6,Qfe),e(Qfe,hYo),e(q6,pYo),e(q6,UD),e(UD,_Yo),e(q6,uYo),e(he,bYo),e(he,G6),e(G6,Hfe),e(Hfe,vYo),e(G6,TYo),e(G6,JD),e(JD,FYo),e(G6,CYo),e(he,MYo),e(he,O6),e(O6,Ufe),e(Ufe,EYo),e(O6,yYo),e(O6,YD),e(YD,wYo),e(O6,AYo),e(he,LYo),e(he,X6),e(X6,Jfe),e(Jfe,BYo),e(X6,kYo),e(X6,KD),e(KD,xYo),e(X6,RYo),e(he,SYo),e(he,z6),e(z6,Yfe),e(Yfe,PYo),e(z6,$Yo),e(z6,ZD),e(ZD,IYo),e(z6,jYo),e(he,NYo),e(he,V6),e(V6,Kfe),e(Kfe,DYo),e(V6,qYo),e(V6,eq),e(eq,GYo),e(V6,OYo),e(po,XYo),e(po,Zfe),e(Zfe,zYo),e(po,VYo),g(Yy,po,null),b(d,T9e,u),b(d,cc,u),e(cc,W6),e(W6,eme),g(Ky,eme,null),e(cc,WYo),e(cc,ome),e(ome,QYo),b(d,F9e,u),b(d,_r,u),g(Zy,_r,null),e(_r,HYo),e(_r,fc),e(fc,UYo),e(fc,rme),e(rme,JYo),e(fc,YYo),e(fc,tme),e(tme,KYo),e(fc,ZYo),e(_r,eKo),e(_r,ew),e(ew,oKo),e(ew,ame),e(ame,rKo),e(ew,tKo),e(_r,aKo),e(_r,dt),g(ow,dt,null),e(dt,nKo),e(dt,nme),e(nme,sKo),e(dt,lKo),e(dt,mc),e(mc,iKo),e(mc,sme),e(sme,dKo),e(mc,cKo),e(mc,lme),e(lme,fKo),e(mc,mKo),e(dt,gKo),e(dt,ime),e(ime,hKo),e(dt,pKo),g(rw,dt,null),e(_r,_Ko),e(_r,_o),g(tw,_o,null),e(_o,uKo),e(_o,dme),e(dme,bKo),e(_o,vKo),e(_o,fn),e(fn,TKo),e(fn,cme),e(cme,FKo),e(fn,CKo),e(fn,fme),e(fme,MKo),e(fn,EKo),e(fn,mme),e(mme,yKo),e(fn,wKo),e(_o,AKo),e(_o,gme),e(gme,Q6),e(Q6,hme),e(hme,LKo),e(Q6,BKo),e(Q6,oq),e(oq,kKo),e(Q6,xKo),e(_o,RKo),e(_o,pme),e(pme,SKo),e(_o,PKo),g(aw,_o,null),b(d,C9e,u),b(d,gc,u),e(gc,H6),e(H6,_me),g(nw,_me,null),e(gc,$Ko),e(gc,ume),e(ume,IKo),b(d,M9e,u),b(d,ur,u),g(sw,ur,null),e(ur,jKo),e(ur,hc),e(hc,NKo),e(hc,bme),e(bme,DKo),e(hc,qKo),e(hc,vme),e(vme,GKo),e(hc,OKo),e(ur,XKo),e(ur,lw),e(lw,zKo),e(lw,Tme),e(Tme,VKo),e(lw,WKo),e(ur,QKo),e(ur,ct),g(iw,ct,null),e(ct,HKo),e(ct,Fme),e(Fme,UKo),e(ct,JKo),e(ct,pc),e(pc,YKo),e(pc,Cme),e(Cme,KKo),e(pc,ZKo),e(pc,Mme),e(Mme,eZo),e(pc,oZo),e(ct,rZo),e(ct,Eme),e(Eme,tZo),e(ct,aZo),g(dw,ct,null),e(ur,nZo),e(ur,uo),g(cw,uo,null),e(uo,sZo),e(uo,yme),e(yme,lZo),e(uo,iZo),e(uo,mn),e(mn,dZo),e(mn,wme),e(wme,cZo),e(mn,fZo),e(mn,Ame),e(Ame,mZo),e(mn,gZo),e(mn,Lme),e(Lme,hZo),e(mn,pZo),e(uo,_Zo),e(uo,Y),e(Y,U6),e(U6,Bme),e(Bme,uZo),e(U6,bZo),e(U6,rq),e(rq,vZo),e(U6,TZo),e(Y,FZo),e(Y,J6),e(J6,kme),e(kme,CZo),e(J6,MZo),e(J6,tq),e(tq,EZo),e(J6,yZo),e(Y,wZo),e(Y,Y6),e(Y6,xme),e(xme,AZo),e(Y6,LZo),e(Y6,aq),e(aq,BZo),e(Y6,kZo),e(Y,xZo),e(Y,K6),e(K6,Rme),e(Rme,RZo),e(K6,SZo),e(K6,nq),e(nq,PZo),e(K6,$Zo),e(Y,IZo),e(Y,Z6),e(Z6,Sme),e(Sme,jZo),e(Z6,NZo),e(Z6,sq),e(sq,DZo),e(Z6,qZo),e(Y,GZo),e(Y,eT),e(eT,Pme),e(Pme,OZo),e(eT,XZo),e(eT,lq),e(lq,zZo),e(eT,VZo),e(Y,WZo),e(Y,oT),e(oT,$me),e($me,QZo),e(oT,HZo),e(oT,iq),e(iq,UZo),e(oT,JZo),e(Y,YZo),e(Y,rT),e(rT,Ime),e(Ime,KZo),e(rT,ZZo),e(rT,dq),e(dq,eer),e(rT,oer),e(Y,rer),e(Y,tT),e(tT,jme),e(jme,ter),e(tT,aer),e(tT,cq),e(cq,ner),e(tT,ser),e(Y,ler),e(Y,aT),e(aT,Nme),e(Nme,ier),e(aT,der),e(aT,fq),e(fq,cer),e(aT,fer),e(Y,mer),e(Y,nT),e(nT,Dme),e(Dme,ger),e(nT,her),e(nT,mq),e(mq,per),e(nT,_er),e(Y,uer),e(Y,sT),e(sT,qme),e(qme,ber),e(sT,ver),e(sT,gq),e(gq,Ter),e(sT,Fer),e(Y,Cer),e(Y,lT),e(lT,Gme),e(Gme,Mer),e(lT,Eer),e(lT,hq),e(hq,yer),e(lT,wer),e(Y,Aer),e(Y,iT),e(iT,Ome),e(Ome,Ler),e(iT,Ber),e(iT,pq),e(pq,ker),e(iT,xer),e(Y,Rer),e(Y,dT),e(dT,Xme),e(Xme,Ser),e(dT,Per),e(dT,_q),e(_q,$er),e(dT,Ier),e(Y,jer),e(Y,cT),e(cT,zme),e(zme,Ner),e(cT,Der),e(cT,uq),e(uq,qer),e(cT,Ger),e(Y,Oer),e(Y,fT),e(fT,Vme),e(Vme,Xer),e(fT,zer),e(fT,bq),e(bq,Ver),e(fT,Wer),e(Y,Qer),e(Y,mT),e(mT,Wme),e(Wme,Her),e(mT,Uer),e(mT,vq),e(vq,Jer),e(mT,Yer),e(Y,Ker),e(Y,gT),e(gT,Qme),e(Qme,Zer),e(gT,eor),e(gT,Tq),e(Tq,oor),e(gT,ror),e(Y,tor),e(Y,hT),e(hT,Hme),e(Hme,aor),e(hT,nor),e(hT,Fq),e(Fq,sor),e(hT,lor),e(uo,ior),e(uo,Ume),e(Ume,dor),e(uo,cor),g(fw,uo,null),b(d,E9e,u),b(d,_c,u),e(_c,pT),e(pT,Jme),g(mw,Jme,null),e(_c,mor),e(_c,Yme),e(Yme,gor),b(d,y9e,u),b(d,br,u),g(gw,br,null),e(br,hor),e(br,uc),e(uc,por),e(uc,Kme),e(Kme,_or),e(uc,uor),e(uc,Zme),e(Zme,bor),e(uc,vor),e(br,Tor),e(br,hw),e(hw,For),e(hw,ege),e(ege,Cor),e(hw,Mor),e(br,Eor),e(br,ft),g(pw,ft,null),e(ft,yor),e(ft,oge),e(oge,wor),e(ft,Aor),e(ft,bc),e(bc,Lor),e(bc,rge),e(rge,Bor),e(bc,kor),e(bc,tge),e(tge,xor),e(bc,Ror),e(ft,Sor),e(ft,age),e(age,Por),e(ft,$or),g(_w,ft,null),e(br,Ior),e(br,bo),g(uw,bo,null),e(bo,jor),e(bo,nge),e(nge,Nor),e(bo,Dor),e(bo,gn),e(gn,qor),e(gn,sge),e(sge,Gor),e(gn,Oor),e(gn,lge),e(lge,Xor),e(gn,zor),e(gn,ige),e(ige,Vor),e(gn,Wor),e(bo,Qor),e(bo,pe),e(pe,_T),e(_T,dge),e(dge,Hor),e(_T,Uor),e(_T,Cq),e(Cq,Jor),e(_T,Yor),e(pe,Kor),e(pe,uT),e(uT,cge),e(cge,Zor),e(uT,err),e(uT,Mq),e(Mq,orr),e(uT,rrr),e(pe,trr),e(pe,bT),e(bT,fge),e(fge,arr),e(bT,nrr),e(bT,Eq),e(Eq,srr),e(bT,lrr),e(pe,irr),e(pe,vT),e(vT,mge),e(mge,drr),e(vT,crr),e(vT,yq),e(yq,frr),e(vT,mrr),e(pe,grr),e(pe,TT),e(TT,gge),e(gge,hrr),e(TT,prr),e(TT,wq),e(wq,_rr),e(TT,urr),e(pe,brr),e(pe,FT),e(FT,hge),e(hge,vrr),e(FT,Trr),e(FT,Aq),e(Aq,Frr),e(FT,Crr),e(pe,Mrr),e(pe,CT),e(CT,pge),e(pge,Err),e(CT,yrr),e(CT,Lq),e(Lq,wrr),e(CT,Arr),e(pe,Lrr),e(pe,MT),e(MT,_ge),e(_ge,Brr),e(MT,krr),e(MT,Bq),e(Bq,xrr),e(MT,Rrr),e(pe,Srr),e(pe,ET),e(ET,uge),e(uge,Prr),e(ET,$rr),e(ET,kq),e(kq,Irr),e(ET,jrr),e(pe,Nrr),e(pe,yT),e(yT,bge),e(bge,Drr),e(yT,qrr),e(yT,xq),e(xq,Grr),e(yT,Orr),e(bo,Xrr),e(bo,vge),e(vge,zrr),e(bo,Vrr),g(bw,bo,null),b(d,w9e,u),b(d,vc,u),e(vc,wT),e(wT,Tge),g(vw,Tge,null),e(vc,Wrr),e(vc,Fge),e(Fge,Qrr),b(d,A9e,u),b(d,vr,u),g(Tw,vr,null),e(vr,Hrr),e(vr,Tc),e(Tc,Urr),e(Tc,Cge),e(Cge,Jrr),e(Tc,Yrr),e(Tc,Mge),e(Mge,Krr),e(Tc,Zrr),e(vr,etr),e(vr,Fw),e(Fw,otr),e(Fw,Ege),e(Ege,rtr),e(Fw,ttr),e(vr,atr),e(vr,mt),g(Cw,mt,null),e(mt,ntr),e(mt,yge),e(yge,str),e(mt,ltr),e(mt,Fc),e(Fc,itr),e(Fc,wge),e(wge,dtr),e(Fc,ctr),e(Fc,Age),e(Age,ftr),e(Fc,mtr),e(mt,gtr),e(mt,Lge),e(Lge,htr),e(mt,ptr),g(Mw,mt,null),e(vr,_tr),e(vr,vo),g(Ew,vo,null),e(vo,utr),e(vo,Bge),e(Bge,btr),e(vo,vtr),e(vo,hn),e(hn,Ttr),e(hn,kge),e(kge,Ftr),e(hn,Ctr),e(hn,xge),e(xge,Mtr),e(hn,Etr),e(hn,Rge),e(Rge,ytr),e(hn,wtr),e(vo,Atr),e(vo,X),e(X,AT),e(AT,Sge),e(Sge,Ltr),e(AT,Btr),e(AT,Rq),e(Rq,ktr),e(AT,xtr),e(X,Rtr),e(X,LT),e(LT,Pge),e(Pge,Str),e(LT,Ptr),e(LT,Sq),e(Sq,$tr),e(LT,Itr),e(X,jtr),e(X,BT),e(BT,$ge),e($ge,Ntr),e(BT,Dtr),e(BT,Pq),e(Pq,qtr),e(BT,Gtr),e(X,Otr),e(X,kT),e(kT,Ige),e(Ige,Xtr),e(kT,ztr),e(kT,$q),e($q,Vtr),e(kT,Wtr),e(X,Qtr),e(X,xT),e(xT,jge),e(jge,Htr),e(xT,Utr),e(xT,Iq),e(Iq,Jtr),e(xT,Ytr),e(X,Ktr),e(X,RT),e(RT,Nge),e(Nge,Ztr),e(RT,ear),e(RT,jq),e(jq,oar),e(RT,rar),e(X,tar),e(X,ST),e(ST,Dge),e(Dge,aar),e(ST,nar),e(ST,Nq),e(Nq,sar),e(ST,lar),e(X,iar),e(X,PT),e(PT,qge),e(qge,dar),e(PT,car),e(PT,Dq),e(Dq,far),e(PT,mar),e(X,gar),e(X,$T),e($T,Gge),e(Gge,har),e($T,par),e($T,qq),e(qq,_ar),e($T,uar),e(X,bar),e(X,IT),e(IT,Oge),e(Oge,Tar),e(IT,Far),e(IT,Gq),e(Gq,Car),e(IT,Mar),e(X,Ear),e(X,jT),e(jT,Xge),e(Xge,yar),e(jT,war),e(jT,Oq),e(Oq,Aar),e(jT,Lar),e(X,Bar),e(X,NT),e(NT,zge),e(zge,kar),e(NT,xar),e(NT,Xq),e(Xq,Rar),e(NT,Sar),e(X,Par),e(X,DT),e(DT,Vge),e(Vge,$ar),e(DT,Iar),e(DT,zq),e(zq,jar),e(DT,Nar),e(X,Dar),e(X,qT),e(qT,Wge),e(Wge,qar),e(qT,Gar),e(qT,Vq),e(Vq,Oar),e(qT,Xar),e(X,zar),e(X,GT),e(GT,Qge),e(Qge,Var),e(GT,War),e(GT,Wq),e(Wq,Qar),e(GT,Har),e(X,Uar),e(X,OT),e(OT,Hge),e(Hge,Jar),e(OT,Yar),e(OT,Qq),e(Qq,Kar),e(OT,Zar),e(X,enr),e(X,XT),e(XT,Uge),e(Uge,onr),e(XT,rnr),e(XT,Hq),e(Hq,tnr),e(XT,anr),e(X,nnr),e(X,zT),e(zT,Jge),e(Jge,snr),e(zT,lnr),e(zT,Uq),e(Uq,inr),e(zT,dnr),e(X,cnr),e(X,VT),e(VT,Yge),e(Yge,fnr),e(VT,mnr),e(VT,Jq),e(Jq,gnr),e(VT,hnr),e(X,pnr),e(X,WT),e(WT,Kge),e(Kge,_nr),e(WT,unr),e(WT,Yq),e(Yq,bnr),e(WT,vnr),e(X,Tnr),e(X,QT),e(QT,Zge),e(Zge,Fnr),e(QT,Cnr),e(QT,Kq),e(Kq,Mnr),e(QT,Enr),e(X,ynr),e(X,HT),e(HT,ehe),e(ehe,wnr),e(HT,Anr),e(HT,Zq),e(Zq,Lnr),e(HT,Bnr),e(X,knr),e(X,UT),e(UT,ohe),e(ohe,xnr),e(UT,Rnr),e(UT,eG),e(eG,Snr),e(UT,Pnr),e(X,$nr),e(X,JT),e(JT,rhe),e(rhe,Inr),e(JT,jnr),e(JT,oG),e(oG,Nnr),e(JT,Dnr),e(X,qnr),e(X,YT),e(YT,the),e(the,Gnr),e(YT,Onr),e(YT,rG),e(rG,Xnr),e(YT,znr),e(vo,Vnr),e(vo,ahe),e(ahe,Wnr),e(vo,Qnr),g(yw,vo,null),b(d,L9e,u),b(d,Cc,u),e(Cc,KT),e(KT,nhe),g(ww,nhe,null),e(Cc,Hnr),e(Cc,she),e(she,Unr),b(d,B9e,u),b(d,Tr,u),g(Aw,Tr,null),e(Tr,Jnr),e(Tr,Mc),e(Mc,Ynr),e(Mc,lhe),e(lhe,Knr),e(Mc,Znr),e(Mc,ihe),e(ihe,esr),e(Mc,osr),e(Tr,rsr),e(Tr,Lw),e(Lw,tsr),e(Lw,dhe),e(dhe,asr),e(Lw,nsr),e(Tr,ssr),e(Tr,gt),g(Bw,gt,null),e(gt,lsr),e(gt,che),e(che,isr),e(gt,dsr),e(gt,Ec),e(Ec,csr),e(Ec,fhe),e(fhe,fsr),e(Ec,msr),e(Ec,mhe),e(mhe,gsr),e(Ec,hsr),e(gt,psr),e(gt,ghe),e(ghe,_sr),e(gt,usr),g(kw,gt,null),e(Tr,bsr),e(Tr,To),g(xw,To,null),e(To,vsr),e(To,hhe),e(hhe,Tsr),e(To,Fsr),e(To,pn),e(pn,Csr),e(pn,phe),e(phe,Msr),e(pn,Esr),e(pn,_he),e(_he,ysr),e(pn,wsr),e(pn,uhe),e(uhe,Asr),e(pn,Lsr),e(To,Bsr),e(To,te),e(te,ZT),e(ZT,bhe),e(bhe,ksr),e(ZT,xsr),e(ZT,tG),e(tG,Rsr),e(ZT,Ssr),e(te,Psr),e(te,e8),e(e8,vhe),e(vhe,$sr),e(e8,Isr),e(e8,aG),e(aG,jsr),e(e8,Nsr),e(te,Dsr),e(te,o8),e(o8,The),e(The,qsr),e(o8,Gsr),e(o8,nG),e(nG,Osr),e(o8,Xsr),e(te,zsr),e(te,r8),e(r8,Fhe),e(Fhe,Vsr),e(r8,Wsr),e(r8,sG),e(sG,Qsr),e(r8,Hsr),e(te,Usr),e(te,t8),e(t8,Che),e(Che,Jsr),e(t8,Ysr),e(t8,lG),e(lG,Ksr),e(t8,Zsr),e(te,elr),e(te,a8),e(a8,Mhe),e(Mhe,olr),e(a8,rlr),e(a8,iG),e(iG,tlr),e(a8,alr),e(te,nlr),e(te,n8),e(n8,Ehe),e(Ehe,slr),e(n8,llr),e(n8,dG),e(dG,ilr),e(n8,dlr),e(te,clr),e(te,s8),e(s8,yhe),e(yhe,flr),e(s8,mlr),e(s8,cG),e(cG,glr),e(s8,hlr),e(te,plr),e(te,l8),e(l8,whe),e(whe,_lr),e(l8,ulr),e(l8,fG),e(fG,blr),e(l8,vlr),e(te,Tlr),e(te,i8),e(i8,Ahe),e(Ahe,Flr),e(i8,Clr),e(i8,mG),e(mG,Mlr),e(i8,Elr),e(te,ylr),e(te,d8),e(d8,Lhe),e(Lhe,wlr),e(d8,Alr),e(d8,gG),e(gG,Llr),e(d8,Blr),e(te,klr),e(te,c8),e(c8,Bhe),e(Bhe,xlr),e(c8,Rlr),e(c8,hG),e(hG,Slr),e(c8,Plr),e(te,$lr),e(te,f8),e(f8,khe),e(khe,Ilr),e(f8,jlr),e(f8,pG),e(pG,Nlr),e(f8,Dlr),e(te,qlr),e(te,m8),e(m8,xhe),e(xhe,Glr),e(m8,Olr),e(m8,_G),e(_G,Xlr),e(m8,zlr),e(te,Vlr),e(te,g8),e(g8,Rhe),e(Rhe,Wlr),e(g8,Qlr),e(g8,uG),e(uG,Hlr),e(g8,Ulr),e(te,Jlr),e(te,h8),e(h8,She),e(She,Ylr),e(h8,Klr),e(h8,bG),e(bG,Zlr),e(h8,eir),e(te,oir),e(te,p8),e(p8,Phe),e(Phe,rir),e(p8,tir),e(p8,vG),e(vG,air),e(p8,nir),e(To,sir),e(To,$he),e($he,lir),e(To,iir),g(Rw,To,null),b(d,k9e,u),b(d,yc,u),e(yc,_8),e(_8,Ihe),g(Sw,Ihe,null),e(yc,dir),e(yc,jhe),e(jhe,cir),b(d,x9e,u),b(d,Fr,u),g(Pw,Fr,null),e(Fr,fir),e(Fr,wc),e(wc,mir),e(wc,Nhe),e(Nhe,gir),e(wc,hir),e(wc,Dhe),e(Dhe,pir),e(wc,_ir),e(Fr,uir),e(Fr,$w),e($w,bir),e($w,qhe),e(qhe,vir),e($w,Tir),e(Fr,Fir),e(Fr,ht),g(Iw,ht,null),e(ht,Cir),e(ht,Ghe),e(Ghe,Mir),e(ht,Eir),e(ht,Ac),e(Ac,yir),e(Ac,Ohe),e(Ohe,wir),e(Ac,Air),e(Ac,Xhe),e(Xhe,Lir),e(Ac,Bir),e(ht,kir),e(ht,zhe),e(zhe,xir),e(ht,Rir),g(jw,ht,null),e(Fr,Sir),e(Fr,Fo),g(Nw,Fo,null),e(Fo,Pir),e(Fo,Vhe),e(Vhe,$ir),e(Fo,Iir),e(Fo,_n),e(_n,jir),e(_n,Whe),e(Whe,Nir),e(_n,Dir),e(_n,Qhe),e(Qhe,qir),e(_n,Gir),e(_n,Hhe),e(Hhe,Oir),e(_n,Xir),e(Fo,zir),e(Fo,Uhe),e(Uhe,u8),e(u8,Jhe),e(Jhe,Vir),e(u8,Wir),e(u8,TG),e(TG,Qir),e(u8,Hir),e(Fo,Uir),e(Fo,Yhe),e(Yhe,Jir),e(Fo,Yir),g(Dw,Fo,null),b(d,R9e,u),b(d,Lc,u),e(Lc,b8),e(b8,Khe),g(qw,Khe,null),e(Lc,Kir),e(Lc,Zhe),e(Zhe,Zir),b(d,S9e,u),b(d,Cr,u),g(Gw,Cr,null),e(Cr,edr),e(Cr,Bc),e(Bc,odr),e(Bc,epe),e(epe,rdr),e(Bc,tdr),e(Bc,ope),e(ope,adr),e(Bc,ndr),e(Cr,sdr),e(Cr,Ow),e(Ow,ldr),e(Ow,rpe),e(rpe,idr),e(Ow,ddr),e(Cr,cdr),e(Cr,pt),g(Xw,pt,null),e(pt,fdr),e(pt,tpe),e(tpe,mdr),e(pt,gdr),e(pt,kc),e(kc,hdr),e(kc,ape),e(ape,pdr),e(kc,_dr),e(kc,npe),e(npe,udr),e(kc,bdr),e(pt,vdr),e(pt,spe),e(spe,Tdr),e(pt,Fdr),g(zw,pt,null),e(Cr,Cdr),e(Cr,Co),g(Vw,Co,null),e(Co,Mdr),e(Co,lpe),e(lpe,Edr),e(Co,ydr),e(Co,un),e(un,wdr),e(un,ipe),e(ipe,Adr),e(un,Ldr),e(un,dpe),e(dpe,Bdr),e(un,kdr),e(un,cpe),e(cpe,xdr),e(un,Rdr),e(Co,Sdr),e(Co,K),e(K,v8),e(v8,fpe),e(fpe,Pdr),e(v8,$dr),e(v8,FG),e(FG,Idr),e(v8,jdr),e(K,Ndr),e(K,T8),e(T8,mpe),e(mpe,Ddr),e(T8,qdr),e(T8,CG),e(CG,Gdr),e(T8,Odr),e(K,Xdr),e(K,F8),e(F8,gpe),e(gpe,zdr),e(F8,Vdr),e(F8,MG),e(MG,Wdr),e(F8,Qdr),e(K,Hdr),e(K,C8),e(C8,hpe),e(hpe,Udr),e(C8,Jdr),e(C8,EG),e(EG,Ydr),e(C8,Kdr),e(K,Zdr),e(K,M8),e(M8,ppe),e(ppe,ecr),e(M8,ocr),e(M8,yG),e(yG,rcr),e(M8,tcr),e(K,acr),e(K,E8),e(E8,_pe),e(_pe,ncr),e(E8,scr),e(E8,wG),e(wG,lcr),e(E8,icr),e(K,dcr),e(K,y8),e(y8,upe),e(upe,ccr),e(y8,fcr),e(y8,AG),e(AG,mcr),e(y8,gcr),e(K,hcr),e(K,w8),e(w8,bpe),e(bpe,pcr),e(w8,_cr),e(w8,LG),e(LG,ucr),e(w8,bcr),e(K,vcr),e(K,A8),e(A8,vpe),e(vpe,Tcr),e(A8,Fcr),e(A8,BG),e(BG,Ccr),e(A8,Mcr),e(K,Ecr),e(K,L8),e(L8,Tpe),e(Tpe,ycr),e(L8,wcr),e(L8,kG),e(kG,Acr),e(L8,Lcr),e(K,Bcr),e(K,B8),e(B8,Fpe),e(Fpe,kcr),e(B8,xcr),e(B8,xG),e(xG,Rcr),e(B8,Scr),e(K,Pcr),e(K,k8),e(k8,Cpe),e(Cpe,$cr),e(k8,Icr),e(k8,RG),e(RG,jcr),e(k8,Ncr),e(K,Dcr),e(K,x8),e(x8,Mpe),e(Mpe,qcr),e(x8,Gcr),e(x8,SG),e(SG,Ocr),e(x8,Xcr),e(K,zcr),e(K,R8),e(R8,Epe),e(Epe,Vcr),e(R8,Wcr),e(R8,PG),e(PG,Qcr),e(R8,Hcr),e(K,Ucr),e(K,S8),e(S8,ype),e(ype,Jcr),e(S8,Ycr),e(S8,$G),e($G,Kcr),e(S8,Zcr),e(K,efr),e(K,P8),e(P8,wpe),e(wpe,ofr),e(P8,rfr),e(P8,IG),e(IG,tfr),e(P8,afr),e(K,nfr),e(K,$8),e($8,Ape),e(Ape,sfr),e($8,lfr),e($8,jG),e(jG,ifr),e($8,dfr),e(K,cfr),e(K,I8),e(I8,Lpe),e(Lpe,ffr),e(I8,mfr),e(I8,NG),e(NG,gfr),e(I8,hfr),e(K,pfr),e(K,j8),e(j8,Bpe),e(Bpe,_fr),e(j8,ufr),e(j8,DG),e(DG,bfr),e(j8,vfr),e(K,Tfr),e(K,N8),e(N8,kpe),e(kpe,Ffr),e(N8,Cfr),e(N8,qG),e(qG,Mfr),e(N8,Efr),e(Co,yfr),e(Co,xpe),e(xpe,wfr),e(Co,Afr),g(Ww,Co,null),b(d,P9e,u),b(d,xc,u),e(xc,D8),e(D8,Rpe),g(Qw,Rpe,null),e(xc,Lfr),e(xc,Spe),e(Spe,Bfr),b(d,$9e,u),b(d,Mr,u),g(Hw,Mr,null),e(Mr,kfr),e(Mr,Rc),e(Rc,xfr),e(Rc,Ppe),e(Ppe,Rfr),e(Rc,Sfr),e(Rc,$pe),e($pe,Pfr),e(Rc,$fr),e(Mr,Ifr),e(Mr,Uw),e(Uw,jfr),e(Uw,Ipe),e(Ipe,Nfr),e(Uw,Dfr),e(Mr,qfr),e(Mr,_t),g(Jw,_t,null),e(_t,Gfr),e(_t,jpe),e(jpe,Ofr),e(_t,Xfr),e(_t,Sc),e(Sc,zfr),e(Sc,Npe),e(Npe,Vfr),e(Sc,Wfr),e(Sc,Dpe),e(Dpe,Qfr),e(Sc,Hfr),e(_t,Ufr),e(_t,qpe),e(qpe,Jfr),e(_t,Yfr),g(Yw,_t,null),e(Mr,Kfr),e(Mr,Mo),g(Kw,Mo,null),e(Mo,Zfr),e(Mo,Gpe),e(Gpe,emr),e(Mo,omr),e(Mo,bn),e(bn,rmr),e(bn,Ope),e(Ope,tmr),e(bn,amr),e(bn,Xpe),e(Xpe,nmr),e(bn,smr),e(bn,zpe),e(zpe,lmr),e(bn,imr),e(Mo,dmr),e(Mo,Z),e(Z,q8),e(q8,Vpe),e(Vpe,cmr),e(q8,fmr),e(q8,GG),e(GG,mmr),e(q8,gmr),e(Z,hmr),e(Z,G8),e(G8,Wpe),e(Wpe,pmr),e(G8,_mr),e(G8,OG),e(OG,umr),e(G8,bmr),e(Z,vmr),e(Z,O8),e(O8,Qpe),e(Qpe,Tmr),e(O8,Fmr),e(O8,XG),e(XG,Cmr),e(O8,Mmr),e(Z,Emr),e(Z,X8),e(X8,Hpe),e(Hpe,ymr),e(X8,wmr),e(X8,zG),e(zG,Amr),e(X8,Lmr),e(Z,Bmr),e(Z,z8),e(z8,Upe),e(Upe,kmr),e(z8,xmr),e(z8,VG),e(VG,Rmr),e(z8,Smr),e(Z,Pmr),e(Z,V8),e(V8,Jpe),e(Jpe,$mr),e(V8,Imr),e(V8,WG),e(WG,jmr),e(V8,Nmr),e(Z,Dmr),e(Z,W8),e(W8,Ype),e(Ype,qmr),e(W8,Gmr),e(W8,QG),e(QG,Omr),e(W8,Xmr),e(Z,zmr),e(Z,Q8),e(Q8,Kpe),e(Kpe,Vmr),e(Q8,Wmr),e(Q8,HG),e(HG,Qmr),e(Q8,Hmr),e(Z,Umr),e(Z,H8),e(H8,Zpe),e(Zpe,Jmr),e(H8,Ymr),e(H8,UG),e(UG,Kmr),e(H8,Zmr),e(Z,egr),e(Z,U8),e(U8,e_e),e(e_e,ogr),e(U8,rgr),e(U8,JG),e(JG,tgr),e(U8,agr),e(Z,ngr),e(Z,J8),e(J8,o_e),e(o_e,sgr),e(J8,lgr),e(J8,YG),e(YG,igr),e(J8,dgr),e(Z,cgr),e(Z,Y8),e(Y8,r_e),e(r_e,fgr),e(Y8,mgr),e(Y8,KG),e(KG,ggr),e(Y8,hgr),e(Z,pgr),e(Z,K8),e(K8,t_e),e(t_e,_gr),e(K8,ugr),e(K8,ZG),e(ZG,bgr),e(K8,vgr),e(Z,Tgr),e(Z,Z8),e(Z8,a_e),e(a_e,Fgr),e(Z8,Cgr),e(Z8,eO),e(eO,Mgr),e(Z8,Egr),e(Z,ygr),e(Z,eF),e(eF,n_e),e(n_e,wgr),e(eF,Agr),e(eF,oO),e(oO,Lgr),e(eF,Bgr),e(Z,kgr),e(Z,oF),e(oF,s_e),e(s_e,xgr),e(oF,Rgr),e(oF,rO),e(rO,Sgr),e(oF,Pgr),e(Z,$gr),e(Z,rF),e(rF,l_e),e(l_e,Igr),e(rF,jgr),e(rF,tO),e(tO,Ngr),e(rF,Dgr),e(Z,qgr),e(Z,tF),e(tF,i_e),e(i_e,Ggr),e(tF,Ogr),e(tF,aO),e(aO,Xgr),e(tF,zgr),e(Z,Vgr),e(Z,aF),e(aF,d_e),e(d_e,Wgr),e(aF,Qgr),e(aF,nO),e(nO,Hgr),e(aF,Ugr),e(Mo,Jgr),e(Mo,c_e),e(c_e,Ygr),e(Mo,Kgr),g(Zw,Mo,null),b(d,I9e,u),b(d,Pc,u),e(Pc,nF),e(nF,f_e),g(eA,f_e,null),e(Pc,Zgr),e(Pc,m_e),e(m_e,ehr),b(d,j9e,u),b(d,Er,u),g(oA,Er,null),e(Er,ohr),e(Er,$c),e($c,rhr),e($c,g_e),e(g_e,thr),e($c,ahr),e($c,h_e),e(h_e,nhr),e($c,shr),e(Er,lhr),e(Er,rA),e(rA,ihr),e(rA,p_e),e(p_e,dhr),e(rA,chr),e(Er,fhr),e(Er,ut),g(tA,ut,null),e(ut,mhr),e(ut,__e),e(__e,ghr),e(ut,hhr),e(ut,Ic),e(Ic,phr),e(Ic,u_e),e(u_e,_hr),e(Ic,uhr),e(Ic,b_e),e(b_e,bhr),e(Ic,vhr),e(ut,Thr),e(ut,v_e),e(v_e,Fhr),e(ut,Chr),g(aA,ut,null),e(Er,Mhr),e(Er,Eo),g(nA,Eo,null),e(Eo,Ehr),e(Eo,T_e),e(T_e,yhr),e(Eo,whr),e(Eo,vn),e(vn,Ahr),e(vn,F_e),e(F_e,Lhr),e(vn,Bhr),e(vn,C_e),e(C_e,khr),e(vn,xhr),e(vn,M_e),e(M_e,Rhr),e(vn,Shr),e(Eo,Phr),e(Eo,E_e),e(E_e,sF),e(sF,y_e),e(y_e,$hr),e(sF,Ihr),e(sF,sO),e(sO,jhr),e(sF,Nhr),e(Eo,Dhr),e(Eo,w_e),e(w_e,qhr),e(Eo,Ghr),g(sA,Eo,null),b(d,N9e,u),b(d,jc,u),e(jc,lF),e(lF,A_e),g(lA,A_e,null),e(jc,Ohr),e(jc,L_e),e(L_e,Xhr),b(d,D9e,u),b(d,yr,u),g(iA,yr,null),e(yr,zhr),e(yr,Nc),e(Nc,Vhr),e(Nc,B_e),e(B_e,Whr),e(Nc,Qhr),e(Nc,k_e),e(k_e,Hhr),e(Nc,Uhr),e(yr,Jhr),e(yr,dA),e(dA,Yhr),e(dA,x_e),e(x_e,Khr),e(dA,Zhr),e(yr,epr),e(yr,bt),g(cA,bt,null),e(bt,opr),e(bt,R_e),e(R_e,rpr),e(bt,tpr),e(bt,Dc),e(Dc,apr),e(Dc,S_e),e(S_e,npr),e(Dc,spr),e(Dc,P_e),e(P_e,lpr),e(Dc,ipr),e(bt,dpr),e(bt,$_e),e($_e,cpr),e(bt,fpr),g(fA,bt,null),e(yr,mpr),e(yr,yo),g(mA,yo,null),e(yo,gpr),e(yo,I_e),e(I_e,hpr),e(yo,ppr),e(yo,Tn),e(Tn,_pr),e(Tn,j_e),e(j_e,upr),e(Tn,bpr),e(Tn,N_e),e(N_e,vpr),e(Tn,Tpr),e(Tn,D_e),e(D_e,Fpr),e(Tn,Cpr),e(yo,Mpr),e(yo,q_e),e(q_e,iF),e(iF,G_e),e(G_e,Epr),e(iF,ypr),e(iF,lO),e(lO,wpr),e(iF,Apr),e(yo,Lpr),e(yo,O_e),e(O_e,Bpr),e(yo,kpr),g(gA,yo,null),b(d,q9e,u),b(d,qc,u),e(qc,dF),e(dF,X_e),g(hA,X_e,null),e(qc,xpr),e(qc,z_e),e(z_e,Rpr),b(d,G9e,u),b(d,wr,u),g(pA,wr,null),e(wr,Spr),e(wr,Gc),e(Gc,Ppr),e(Gc,V_e),e(V_e,$pr),e(Gc,Ipr),e(Gc,W_e),e(W_e,jpr),e(Gc,Npr),e(wr,Dpr),e(wr,_A),e(_A,qpr),e(_A,Q_e),e(Q_e,Gpr),e(_A,Opr),e(wr,Xpr),e(wr,vt),g(uA,vt,null),e(vt,zpr),e(vt,H_e),e(H_e,Vpr),e(vt,Wpr),e(vt,Oc),e(Oc,Qpr),e(Oc,U_e),e(U_e,Hpr),e(Oc,Upr),e(Oc,J_e),e(J_e,Jpr),e(Oc,Ypr),e(vt,Kpr),e(vt,Y_e),e(Y_e,Zpr),e(vt,e_r),g(bA,vt,null),e(wr,o_r),e(wr,wo),g(vA,wo,null),e(wo,r_r),e(wo,K_e),e(K_e,t_r),e(wo,a_r),e(wo,Fn),e(Fn,n_r),e(Fn,Z_e),e(Z_e,s_r),e(Fn,l_r),e(Fn,eue),e(eue,i_r),e(Fn,d_r),e(Fn,oue),e(oue,c_r),e(Fn,f_r),e(wo,m_r),e(wo,V),e(V,cF),e(cF,rue),e(rue,g_r),e(cF,h_r),e(cF,iO),e(iO,p_r),e(cF,__r),e(V,u_r),e(V,fF),e(fF,tue),e(tue,b_r),e(fF,v_r),e(fF,dO),e(dO,T_r),e(fF,F_r),e(V,C_r),e(V,mF),e(mF,aue),e(aue,M_r),e(mF,E_r),e(mF,cO),e(cO,y_r),e(mF,w_r),e(V,A_r),e(V,gF),e(gF,nue),e(nue,L_r),e(gF,B_r),e(gF,fO),e(fO,k_r),e(gF,x_r),e(V,R_r),e(V,hF),e(hF,sue),e(sue,S_r),e(hF,P_r),e(hF,mO),e(mO,$_r),e(hF,I_r),e(V,j_r),e(V,pF),e(pF,lue),e(lue,N_r),e(pF,D_r),e(pF,gO),e(gO,q_r),e(pF,G_r),e(V,O_r),e(V,_F),e(_F,iue),e(iue,X_r),e(_F,z_r),e(_F,hO),e(hO,V_r),e(_F,W_r),e(V,Q_r),e(V,uF),e(uF,due),e(due,H_r),e(uF,U_r),e(uF,pO),e(pO,J_r),e(uF,Y_r),e(V,K_r),e(V,bF),e(bF,cue),e(cue,Z_r),e(bF,eur),e(bF,_O),e(_O,our),e(bF,rur),e(V,tur),e(V,vF),e(vF,fue),e(fue,aur),e(vF,nur),e(vF,uO),e(uO,sur),e(vF,lur),e(V,iur),e(V,TF),e(TF,mue),e(mue,dur),e(TF,cur),e(TF,bO),e(bO,fur),e(TF,mur),e(V,gur),e(V,FF),e(FF,gue),e(gue,hur),e(FF,pur),e(FF,vO),e(vO,_ur),e(FF,uur),e(V,bur),e(V,CF),e(CF,hue),e(hue,vur),e(CF,Tur),e(CF,TO),e(TO,Fur),e(CF,Cur),e(V,Mur),e(V,MF),e(MF,pue),e(pue,Eur),e(MF,yur),e(MF,FO),e(FO,wur),e(MF,Aur),e(V,Lur),e(V,EF),e(EF,_ue),e(_ue,Bur),e(EF,kur),e(EF,CO),e(CO,xur),e(EF,Rur),e(V,Sur),e(V,yF),e(yF,uue),e(uue,Pur),e(yF,$ur),e(yF,MO),e(MO,Iur),e(yF,jur),e(V,Nur),e(V,wF),e(wF,bue),e(bue,Dur),e(wF,qur),e(wF,EO),e(EO,Gur),e(wF,Our),e(V,Xur),e(V,AF),e(AF,vue),e(vue,zur),e(AF,Vur),e(AF,yO),e(yO,Wur),e(AF,Qur),e(V,Hur),e(V,LF),e(LF,Tue),e(Tue,Uur),e(LF,Jur),e(LF,wO),e(wO,Yur),e(LF,Kur),e(V,Zur),e(V,BF),e(BF,Fue),e(Fue,e2r),e(BF,o2r),e(BF,AO),e(AO,r2r),e(BF,t2r),e(V,a2r),e(V,kF),e(kF,Cue),e(Cue,n2r),e(kF,s2r),e(kF,LO),e(LO,l2r),e(kF,i2r),e(V,d2r),e(V,xF),e(xF,Mue),e(Mue,c2r),e(xF,f2r),e(xF,BO),e(BO,m2r),e(xF,g2r),e(V,h2r),e(V,RF),e(RF,Eue),e(Eue,p2r),e(RF,_2r),e(RF,kO),e(kO,u2r),e(RF,b2r),e(V,v2r),e(V,SF),e(SF,yue),e(yue,T2r),e(SF,F2r),e(SF,xO),e(xO,C2r),e(SF,M2r),e(wo,E2r),e(wo,wue),e(wue,y2r),e(wo,w2r),g(TA,wo,null),b(d,O9e,u),b(d,Xc,u),e(Xc,PF),e(PF,Aue),g(FA,Aue,null),e(Xc,A2r),e(Xc,Lue),e(Lue,L2r),b(d,X9e,u),b(d,Ar,u),g(CA,Ar,null),e(Ar,B2r),e(Ar,zc),e(zc,k2r),e(zc,Bue),e(Bue,x2r),e(zc,R2r),e(zc,kue),e(kue,S2r),e(zc,P2r),e(Ar,$2r),e(Ar,MA),e(MA,I2r),e(MA,xue),e(xue,j2r),e(MA,N2r),e(Ar,D2r),e(Ar,Tt),g(EA,Tt,null),e(Tt,q2r),e(Tt,Rue),e(Rue,G2r),e(Tt,O2r),e(Tt,Vc),e(Vc,X2r),e(Vc,Sue),e(Sue,z2r),e(Vc,V2r),e(Vc,Pue),e(Pue,W2r),e(Vc,Q2r),e(Tt,H2r),e(Tt,$ue),e($ue,U2r),e(Tt,J2r),g(yA,Tt,null),e(Ar,Y2r),e(Ar,Ao),g(wA,Ao,null),e(Ao,K2r),e(Ao,Iue),e(Iue,Z2r),e(Ao,e1r),e(Ao,Cn),e(Cn,o1r),e(Cn,jue),e(jue,r1r),e(Cn,t1r),e(Cn,Nue),e(Nue,a1r),e(Cn,n1r),e(Cn,Due),e(Due,s1r),e(Cn,l1r),e(Ao,i1r),e(Ao,Mn),e(Mn,$F),e($F,que),e(que,d1r),e($F,c1r),e($F,RO),e(RO,f1r),e($F,m1r),e(Mn,g1r),e(Mn,IF),e(IF,Gue),e(Gue,h1r),e(IF,p1r),e(IF,SO),e(SO,_1r),e(IF,u1r),e(Mn,b1r),e(Mn,jF),e(jF,Oue),e(Oue,v1r),e(jF,T1r),e(jF,PO),e(PO,F1r),e(jF,C1r),e(Mn,M1r),e(Mn,NF),e(NF,Xue),e(Xue,E1r),e(NF,y1r),e(NF,$O),e($O,w1r),e(NF,A1r),e(Ao,L1r),e(Ao,zue),e(zue,B1r),e(Ao,k1r),g(AA,Ao,null),b(d,z9e,u),b(d,Wc,u),e(Wc,DF),e(DF,Vue),g(LA,Vue,null),e(Wc,x1r),e(Wc,Wue),e(Wue,R1r),b(d,V9e,u),b(d,Lr,u),g(BA,Lr,null),e(Lr,S1r),e(Lr,Qc),e(Qc,P1r),e(Qc,Que),e(Que,$1r),e(Qc,I1r),e(Qc,Hue),e(Hue,j1r),e(Qc,N1r),e(Lr,D1r),e(Lr,kA),e(kA,q1r),e(kA,Uue),e(Uue,G1r),e(kA,O1r),e(Lr,X1r),e(Lr,Ft),g(xA,Ft,null),e(Ft,z1r),e(Ft,Jue),e(Jue,V1r),e(Ft,W1r),e(Ft,Hc),e(Hc,Q1r),e(Hc,Yue),e(Yue,H1r),e(Hc,U1r),e(Hc,Kue),e(Kue,J1r),e(Hc,Y1r),e(Ft,K1r),e(Ft,Zue),e(Zue,Z1r),e(Ft,ebr),g(RA,Ft,null),e(Lr,obr),e(Lr,Lo),g(SA,Lo,null),e(Lo,rbr),e(Lo,e2e),e(e2e,tbr),e(Lo,abr),e(Lo,En),e(En,nbr),e(En,o2e),e(o2e,sbr),e(En,lbr),e(En,r2e),e(r2e,ibr),e(En,dbr),e(En,t2e),e(t2e,cbr),e(En,fbr),e(Lo,mbr),e(Lo,fe),e(fe,qF),e(qF,a2e),e(a2e,gbr),e(qF,hbr),e(qF,IO),e(IO,pbr),e(qF,_br),e(fe,ubr),e(fe,GF),e(GF,n2e),e(n2e,bbr),e(GF,vbr),e(GF,jO),e(jO,Tbr),e(GF,Fbr),e(fe,Cbr),e(fe,OF),e(OF,s2e),e(s2e,Mbr),e(OF,Ebr),e(OF,NO),e(NO,ybr),e(OF,wbr),e(fe,Abr),e(fe,XF),e(XF,l2e),e(l2e,Lbr),e(XF,Bbr),e(XF,DO),e(DO,kbr),e(XF,xbr),e(fe,Rbr),e(fe,zF),e(zF,i2e),e(i2e,Sbr),e(zF,Pbr),e(zF,qO),e(qO,$br),e(zF,Ibr),e(fe,jbr),e(fe,VF),e(VF,d2e),e(d2e,Nbr),e(VF,Dbr),e(VF,GO),e(GO,qbr),e(VF,Gbr),e(fe,Obr),e(fe,WF),e(WF,c2e),e(c2e,Xbr),e(WF,zbr),e(WF,OO),e(OO,Vbr),e(WF,Wbr),e(fe,Qbr),e(fe,QF),e(QF,f2e),e(f2e,Hbr),e(QF,Ubr),e(QF,XO),e(XO,Jbr),e(QF,Ybr),e(fe,Kbr),e(fe,HF),e(HF,m2e),e(m2e,Zbr),e(HF,e5r),e(HF,zO),e(zO,o5r),e(HF,r5r),e(fe,t5r),e(fe,UF),e(UF,g2e),e(g2e,a5r),e(UF,n5r),e(UF,VO),e(VO,s5r),e(UF,l5r),e(fe,i5r),e(fe,JF),e(JF,h2e),e(h2e,d5r),e(JF,c5r),e(JF,WO),e(WO,f5r),e(JF,m5r),e(Lo,g5r),e(Lo,p2e),e(p2e,h5r),e(Lo,p5r),g(PA,Lo,null),b(d,W9e,u),b(d,Uc,u),e(Uc,YF),e(YF,_2e),g($A,_2e,null),e(Uc,_5r),e(Uc,u2e),e(u2e,u5r),b(d,Q9e,u),b(d,Br,u),g(IA,Br,null),e(Br,b5r),e(Br,Jc),e(Jc,v5r),e(Jc,b2e),e(b2e,T5r),e(Jc,F5r),e(Jc,v2e),e(v2e,C5r),e(Jc,M5r),e(Br,E5r),e(Br,jA),e(jA,y5r),e(jA,T2e),e(T2e,w5r),e(jA,A5r),e(Br,L5r),e(Br,Ct),g(NA,Ct,null),e(Ct,B5r),e(Ct,F2e),e(F2e,k5r),e(Ct,x5r),e(Ct,Yc),e(Yc,R5r),e(Yc,C2e),e(C2e,S5r),e(Yc,P5r),e(Yc,M2e),e(M2e,$5r),e(Yc,I5r),e(Ct,j5r),e(Ct,E2e),e(E2e,N5r),e(Ct,D5r),g(DA,Ct,null),e(Br,q5r),e(Br,Bo),g(qA,Bo,null),e(Bo,G5r),e(Bo,y2e),e(y2e,O5r),e(Bo,X5r),e(Bo,yn),e(yn,z5r),e(yn,w2e),e(w2e,V5r),e(yn,W5r),e(yn,A2e),e(A2e,Q5r),e(yn,H5r),e(yn,L2e),e(L2e,U5r),e(yn,J5r),e(Bo,Y5r),e(Bo,ve),e(ve,KF),e(KF,B2e),e(B2e,K5r),e(KF,Z5r),e(KF,QO),e(QO,evr),e(KF,ovr),e(ve,rvr),e(ve,ZF),e(ZF,k2e),e(k2e,tvr),e(ZF,avr),e(ZF,HO),e(HO,nvr),e(ZF,svr),e(ve,lvr),e(ve,eC),e(eC,x2e),e(x2e,ivr),e(eC,dvr),e(eC,UO),e(UO,cvr),e(eC,fvr),e(ve,mvr),e(ve,oC),e(oC,R2e),e(R2e,gvr),e(oC,hvr),e(oC,JO),e(JO,pvr),e(oC,_vr),e(ve,uvr),e(ve,rC),e(rC,S2e),e(S2e,bvr),e(rC,vvr),e(rC,YO),e(YO,Tvr),e(rC,Fvr),e(ve,Cvr),e(ve,tC),e(tC,P2e),e(P2e,Mvr),e(tC,Evr),e(tC,KO),e(KO,yvr),e(tC,wvr),e(ve,Avr),e(ve,aC),e(aC,$2e),e($2e,Lvr),e(aC,Bvr),e(aC,ZO),e(ZO,kvr),e(aC,xvr),e(ve,Rvr),e(ve,nC),e(nC,I2e),e(I2e,Svr),e(nC,Pvr),e(nC,eX),e(eX,$vr),e(nC,Ivr),e(ve,jvr),e(ve,sC),e(sC,j2e),e(j2e,Nvr),e(sC,Dvr),e(sC,oX),e(oX,qvr),e(sC,Gvr),e(Bo,Ovr),e(Bo,N2e),e(N2e,Xvr),e(Bo,zvr),g(GA,Bo,null),b(d,H9e,u),b(d,Kc,u),e(Kc,lC),e(lC,D2e),g(OA,D2e,null),e(Kc,Vvr),e(Kc,q2e),e(q2e,Wvr),b(d,U9e,u),b(d,kr,u),g(XA,kr,null),e(kr,Qvr),e(kr,Zc),e(Zc,Hvr),e(Zc,G2e),e(G2e,Uvr),e(Zc,Jvr),e(Zc,O2e),e(O2e,Yvr),e(Zc,Kvr),e(kr,Zvr),e(kr,zA),e(zA,e6r),e(zA,X2e),e(X2e,o6r),e(zA,r6r),e(kr,t6r),e(kr,Mt),g(VA,Mt,null),e(Mt,a6r),e(Mt,z2e),e(z2e,n6r),e(Mt,s6r),e(Mt,ef),e(ef,l6r),e(ef,V2e),e(V2e,i6r),e(ef,d6r),e(ef,W2e),e(W2e,c6r),e(ef,f6r),e(Mt,m6r),e(Mt,Q2e),e(Q2e,g6r),e(Mt,h6r),g(WA,Mt,null),e(kr,p6r),e(kr,ko),g(QA,ko,null),e(ko,_6r),e(ko,H2e),e(H2e,u6r),e(ko,b6r),e(ko,wn),e(wn,v6r),e(wn,U2e),e(U2e,T6r),e(wn,F6r),e(wn,J2e),e(J2e,C6r),e(wn,M6r),e(wn,Y2e),e(Y2e,E6r),e(wn,y6r),e(ko,w6r),e(ko,Te),e(Te,iC),e(iC,K2e),e(K2e,A6r),e(iC,L6r),e(iC,rX),e(rX,B6r),e(iC,k6r),e(Te,x6r),e(Te,dC),e(dC,Z2e),e(Z2e,R6r),e(dC,S6r),e(dC,tX),e(tX,P6r),e(dC,$6r),e(Te,I6r),e(Te,cC),e(cC,e1e),e(e1e,j6r),e(cC,N6r),e(cC,aX),e(aX,D6r),e(cC,q6r),e(Te,G6r),e(Te,fC),e(fC,o1e),e(o1e,O6r),e(fC,X6r),e(fC,nX),e(nX,z6r),e(fC,V6r),e(Te,W6r),e(Te,mC),e(mC,r1e),e(r1e,Q6r),e(mC,H6r),e(mC,sX),e(sX,U6r),e(mC,J6r),e(Te,Y6r),e(Te,gC),e(gC,t1e),e(t1e,K6r),e(gC,Z6r),e(gC,lX),e(lX,eTr),e(gC,oTr),e(Te,rTr),e(Te,hC),e(hC,a1e),e(a1e,tTr),e(hC,aTr),e(hC,iX),e(iX,nTr),e(hC,sTr),e(Te,lTr),e(Te,pC),e(pC,n1e),e(n1e,iTr),e(pC,dTr),e(pC,dX),e(dX,cTr),e(pC,fTr),e(Te,mTr),e(Te,_C),e(_C,s1e),e(s1e,gTr),e(_C,hTr),e(_C,cX),e(cX,pTr),e(_C,_Tr),e(ko,uTr),e(ko,l1e),e(l1e,bTr),e(ko,vTr),g(HA,ko,null),b(d,J9e,u),b(d,of,u),e(of,uC),e(uC,i1e),g(UA,i1e,null),e(of,TTr),e(of,d1e),e(d1e,FTr),b(d,Y9e,u),b(d,xr,u),g(JA,xr,null),e(xr,CTr),e(xr,rf),e(rf,MTr),e(rf,c1e),e(c1e,ETr),e(rf,yTr),e(rf,f1e),e(f1e,wTr),e(rf,ATr),e(xr,LTr),e(xr,YA),e(YA,BTr),e(YA,m1e),e(m1e,kTr),e(YA,xTr),e(xr,RTr),e(xr,Et),g(KA,Et,null),e(Et,STr),e(Et,g1e),e(g1e,PTr),e(Et,$Tr),e(Et,tf),e(tf,ITr),e(tf,h1e),e(h1e,jTr),e(tf,NTr),e(tf,p1e),e(p1e,DTr),e(tf,qTr),e(Et,GTr),e(Et,_1e),e(_1e,OTr),e(Et,XTr),g(ZA,Et,null),e(xr,zTr),e(xr,xo),g(e0,xo,null),e(xo,VTr),e(xo,u1e),e(u1e,WTr),e(xo,QTr),e(xo,An),e(An,HTr),e(An,b1e),e(b1e,UTr),e(An,JTr),e(An,v1e),e(v1e,YTr),e(An,KTr),e(An,T1e),e(T1e,ZTr),e(An,e8r),e(xo,o8r),e(xo,Fe),e(Fe,bC),e(bC,F1e),e(F1e,r8r),e(bC,t8r),e(bC,fX),e(fX,a8r),e(bC,n8r),e(Fe,s8r),e(Fe,vC),e(vC,C1e),e(C1e,l8r),e(vC,i8r),e(vC,mX),e(mX,d8r),e(vC,c8r),e(Fe,f8r),e(Fe,TC),e(TC,M1e),e(M1e,m8r),e(TC,g8r),e(TC,gX),e(gX,h8r),e(TC,p8r),e(Fe,_8r),e(Fe,FC),e(FC,E1e),e(E1e,u8r),e(FC,b8r),e(FC,hX),e(hX,v8r),e(FC,T8r),e(Fe,F8r),e(Fe,CC),e(CC,y1e),e(y1e,C8r),e(CC,M8r),e(CC,pX),e(pX,E8r),e(CC,y8r),e(Fe,w8r),e(Fe,MC),e(MC,w1e),e(w1e,A8r),e(MC,L8r),e(MC,_X),e(_X,B8r),e(MC,k8r),e(Fe,x8r),e(Fe,EC),e(EC,A1e),e(A1e,R8r),e(EC,S8r),e(EC,uX),e(uX,P8r),e(EC,$8r),e(Fe,I8r),e(Fe,yC),e(yC,L1e),e(L1e,j8r),e(yC,N8r),e(yC,bX),e(bX,D8r),e(yC,q8r),e(Fe,G8r),e(Fe,wC),e(wC,B1e),e(B1e,O8r),e(wC,X8r),e(wC,vX),e(vX,z8r),e(wC,V8r),e(xo,W8r),e(xo,k1e),e(k1e,Q8r),e(xo,H8r),g(o0,xo,null),b(d,K9e,u),b(d,af,u),e(af,AC),e(AC,x1e),g(r0,x1e,null),e(af,U8r),e(af,R1e),e(R1e,J8r),b(d,Z9e,u),b(d,Rr,u),g(t0,Rr,null),e(Rr,Y8r),e(Rr,nf),e(nf,K8r),e(nf,S1e),e(S1e,Z8r),e(nf,eFr),e(nf,P1e),e(P1e,oFr),e(nf,rFr),e(Rr,tFr),e(Rr,a0),e(a0,aFr),e(a0,$1e),e($1e,nFr),e(a0,sFr),e(Rr,lFr),e(Rr,yt),g(n0,yt,null),e(yt,iFr),e(yt,I1e),e(I1e,dFr),e(yt,cFr),e(yt,sf),e(sf,fFr),e(sf,j1e),e(j1e,mFr),e(sf,gFr),e(sf,N1e),e(N1e,hFr),e(sf,pFr),e(yt,_Fr),e(yt,D1e),e(D1e,uFr),e(yt,bFr),g(s0,yt,null),e(Rr,vFr),e(Rr,Ro),g(l0,Ro,null),e(Ro,TFr),e(Ro,q1e),e(q1e,FFr),e(Ro,CFr),e(Ro,Ln),e(Ln,MFr),e(Ln,G1e),e(G1e,EFr),e(Ln,yFr),e(Ln,O1e),e(O1e,wFr),e(Ln,AFr),e(Ln,X1e),e(X1e,LFr),e(Ln,BFr),e(Ro,kFr),e(Ro,Ce),e(Ce,LC),e(LC,z1e),e(z1e,xFr),e(LC,RFr),e(LC,TX),e(TX,SFr),e(LC,PFr),e(Ce,$Fr),e(Ce,BC),e(BC,V1e),e(V1e,IFr),e(BC,jFr),e(BC,FX),e(FX,NFr),e(BC,DFr),e(Ce,qFr),e(Ce,kC),e(kC,W1e),e(W1e,GFr),e(kC,OFr),e(kC,CX),e(CX,XFr),e(kC,zFr),e(Ce,VFr),e(Ce,xC),e(xC,Q1e),e(Q1e,WFr),e(xC,QFr),e(xC,MX),e(MX,HFr),e(xC,UFr),e(Ce,JFr),e(Ce,RC),e(RC,H1e),e(H1e,YFr),e(RC,KFr),e(RC,EX),e(EX,ZFr),e(RC,eCr),e(Ce,oCr),e(Ce,SC),e(SC,U1e),e(U1e,rCr),e(SC,tCr),e(SC,yX),e(yX,aCr),e(SC,nCr),e(Ce,sCr),e(Ce,PC),e(PC,J1e),e(J1e,lCr),e(PC,iCr),e(PC,wX),e(wX,dCr),e(PC,cCr),e(Ce,fCr),e(Ce,$C),e($C,Y1e),e(Y1e,mCr),e($C,gCr),e($C,AX),e(AX,hCr),e($C,pCr),e(Ce,_Cr),e(Ce,IC),e(IC,K1e),e(K1e,uCr),e(IC,bCr),e(IC,LX),e(LX,vCr),e(IC,TCr),e(Ro,FCr),e(Ro,Z1e),e(Z1e,CCr),e(Ro,MCr),g(i0,Ro,null),b(d,eBe,u),b(d,lf,u),e(lf,jC),e(jC,ebe),g(d0,ebe,null),e(lf,ECr),e(lf,obe),e(obe,yCr),b(d,oBe,u),b(d,Sr,u),g(c0,Sr,null),e(Sr,wCr),e(Sr,df),e(df,ACr),e(df,rbe),e(rbe,LCr),e(df,BCr),e(df,tbe),e(tbe,kCr),e(df,xCr),e(Sr,RCr),e(Sr,f0),e(f0,SCr),e(f0,abe),e(abe,PCr),e(f0,$Cr),e(Sr,ICr),e(Sr,wt),g(m0,wt,null),e(wt,jCr),e(wt,nbe),e(nbe,NCr),e(wt,DCr),e(wt,cf),e(cf,qCr),e(cf,sbe),e(sbe,GCr),e(cf,OCr),e(cf,lbe),e(lbe,XCr),e(cf,zCr),e(wt,VCr),e(wt,ibe),e(ibe,WCr),e(wt,QCr),g(g0,wt,null),e(Sr,HCr),e(Sr,So),g(h0,So,null),e(So,UCr),e(So,dbe),e(dbe,JCr),e(So,YCr),e(So,Bn),e(Bn,KCr),e(Bn,cbe),e(cbe,ZCr),e(Bn,e4r),e(Bn,fbe),e(fbe,o4r),e(Bn,r4r),e(Bn,mbe),e(mbe,t4r),e(Bn,a4r),e(So,n4r),e(So,so),e(so,NC),e(NC,gbe),e(gbe,s4r),e(NC,l4r),e(NC,BX),e(BX,i4r),e(NC,d4r),e(so,c4r),e(so,DC),e(DC,hbe),e(hbe,f4r),e(DC,m4r),e(DC,kX),e(kX,g4r),e(DC,h4r),e(so,p4r),e(so,qC),e(qC,pbe),e(pbe,_4r),e(qC,u4r),e(qC,xX),e(xX,b4r),e(qC,v4r),e(so,T4r),e(so,GC),e(GC,_be),e(_be,F4r),e(GC,C4r),e(GC,RX),e(RX,M4r),e(GC,E4r),e(so,y4r),e(so,OC),e(OC,ube),e(ube,w4r),e(OC,A4r),e(OC,SX),e(SX,L4r),e(OC,B4r),e(so,k4r),e(so,XC),e(XC,bbe),e(bbe,x4r),e(XC,R4r),e(XC,PX),e(PX,S4r),e(XC,P4r),e(so,$4r),e(so,zC),e(zC,vbe),e(vbe,I4r),e(zC,j4r),e(zC,$X),e($X,N4r),e(zC,D4r),e(So,q4r),e(So,Tbe),e(Tbe,G4r),e(So,O4r),g(p0,So,null),b(d,rBe,u),b(d,ff,u),e(ff,VC),e(VC,Fbe),g(_0,Fbe,null),e(ff,X4r),e(ff,Cbe),e(Cbe,z4r),b(d,tBe,u),b(d,Pr,u),g(u0,Pr,null),e(Pr,V4r),e(Pr,mf),e(mf,W4r),e(mf,Mbe),e(Mbe,Q4r),e(mf,H4r),e(mf,Ebe),e(Ebe,U4r),e(mf,J4r),e(Pr,Y4r),e(Pr,b0),e(b0,K4r),e(b0,ybe),e(ybe,Z4r),e(b0,eMr),e(Pr,oMr),e(Pr,At),g(v0,At,null),e(At,rMr),e(At,wbe),e(wbe,tMr),e(At,aMr),e(At,gf),e(gf,nMr),e(gf,Abe),e(Abe,sMr),e(gf,lMr),e(gf,Lbe),e(Lbe,iMr),e(gf,dMr),e(At,cMr),e(At,Bbe),e(Bbe,fMr),e(At,mMr),g(T0,At,null),e(Pr,gMr),e(Pr,Po),g(F0,Po,null),e(Po,hMr),e(Po,kbe),e(kbe,pMr),e(Po,_Mr),e(Po,kn),e(kn,uMr),e(kn,xbe),e(xbe,bMr),e(kn,vMr),e(kn,Rbe),e(Rbe,TMr),e(kn,FMr),e(kn,Sbe),e(Sbe,CMr),e(kn,MMr),e(Po,EMr),e(Po,lo),e(lo,WC),e(WC,Pbe),e(Pbe,yMr),e(WC,wMr),e(WC,IX),e(IX,AMr),e(WC,LMr),e(lo,BMr),e(lo,QC),e(QC,$be),e($be,kMr),e(QC,xMr),e(QC,jX),e(jX,RMr),e(QC,SMr),e(lo,PMr),e(lo,HC),e(HC,Ibe),e(Ibe,$Mr),e(HC,IMr),e(HC,NX),e(NX,jMr),e(HC,NMr),e(lo,DMr),e(lo,UC),e(UC,jbe),e(jbe,qMr),e(UC,GMr),e(UC,DX),e(DX,OMr),e(UC,XMr),e(lo,zMr),e(lo,JC),e(JC,Nbe),e(Nbe,VMr),e(JC,WMr),e(JC,qX),e(qX,QMr),e(JC,HMr),e(lo,UMr),e(lo,YC),e(YC,Dbe),e(Dbe,JMr),e(YC,YMr),e(YC,GX),e(GX,KMr),e(YC,ZMr),e(lo,eEr),e(lo,KC),e(KC,qbe),e(qbe,oEr),e(KC,rEr),e(KC,OX),e(OX,tEr),e(KC,aEr),e(Po,nEr),e(Po,Gbe),e(Gbe,sEr),e(Po,lEr),g(C0,Po,null),b(d,aBe,u),b(d,hf,u),e(hf,ZC),e(ZC,Obe),g(M0,Obe,null),e(hf,iEr),e(hf,Xbe),e(Xbe,dEr),b(d,nBe,u),b(d,$r,u),g(E0,$r,null),e($r,cEr),e($r,pf),e(pf,fEr),e(pf,zbe),e(zbe,mEr),e(pf,gEr),e(pf,Vbe),e(Vbe,hEr),e(pf,pEr),e($r,_Er),e($r,y0),e(y0,uEr),e(y0,Wbe),e(Wbe,bEr),e(y0,vEr),e($r,TEr),e($r,Lt),g(w0,Lt,null),e(Lt,FEr),e(Lt,Qbe),e(Qbe,CEr),e(Lt,MEr),e(Lt,_f),e(_f,EEr),e(_f,Hbe),e(Hbe,yEr),e(_f,wEr),e(_f,Ube),e(Ube,AEr),e(_f,LEr),e(Lt,BEr),e(Lt,Jbe),e(Jbe,kEr),e(Lt,xEr),g(A0,Lt,null),e($r,REr),e($r,$o),g(L0,$o,null),e($o,SEr),e($o,Ybe),e(Ybe,PEr),e($o,$Er),e($o,xn),e(xn,IEr),e(xn,Kbe),e(Kbe,jEr),e(xn,NEr),e(xn,Zbe),e(Zbe,DEr),e(xn,qEr),e(xn,e5e),e(e5e,GEr),e(xn,OEr),e($o,XEr),e($o,o5e),e(o5e,e4),e(e4,r5e),e(r5e,zEr),e(e4,VEr),e(e4,XX),e(XX,WEr),e(e4,QEr),e($o,HEr),e($o,t5e),e(t5e,UEr),e($o,JEr),g(B0,$o,null),b(d,sBe,u),b(d,uf,u),e(uf,o4),e(o4,a5e),g(k0,a5e,null),e(uf,YEr),e(uf,n5e),e(n5e,KEr),b(d,lBe,u),b(d,Ir,u),g(x0,Ir,null),e(Ir,ZEr),e(Ir,bf),e(bf,e3r),e(bf,s5e),e(s5e,o3r),e(bf,r3r),e(bf,l5e),e(l5e,t3r),e(bf,a3r),e(Ir,n3r),e(Ir,R0),e(R0,s3r),e(R0,i5e),e(i5e,l3r),e(R0,i3r),e(Ir,d3r),e(Ir,Bt),g(S0,Bt,null),e(Bt,c3r),e(Bt,d5e),e(d5e,f3r),e(Bt,m3r),e(Bt,vf),e(vf,g3r),e(vf,c5e),e(c5e,h3r),e(vf,p3r),e(vf,f5e),e(f5e,_3r),e(vf,u3r),e(Bt,b3r),e(Bt,m5e),e(m5e,v3r),e(Bt,T3r),g(P0,Bt,null),e(Ir,F3r),e(Ir,Io),g($0,Io,null),e(Io,C3r),e(Io,g5e),e(g5e,M3r),e(Io,E3r),e(Io,Rn),e(Rn,y3r),e(Rn,h5e),e(h5e,w3r),e(Rn,A3r),e(Rn,p5e),e(p5e,L3r),e(Rn,B3r),e(Rn,_5e),e(_5e,k3r),e(Rn,x3r),e(Io,R3r),e(Io,I0),e(I0,r4),e(r4,u5e),e(u5e,S3r),e(r4,P3r),e(r4,zX),e(zX,$3r),e(r4,I3r),e(I0,j3r),e(I0,t4),e(t4,b5e),e(b5e,N3r),e(t4,D3r),e(t4,VX),e(VX,q3r),e(t4,G3r),e(Io,O3r),e(Io,v5e),e(v5e,X3r),e(Io,z3r),g(j0,Io,null),b(d,iBe,u),b(d,Tf,u),e(Tf,a4),e(a4,T5e),g(N0,T5e,null),e(Tf,V3r),e(Tf,F5e),e(F5e,W3r),b(d,dBe,u),b(d,jr,u),g(D0,jr,null),e(jr,Q3r),e(jr,Ff),e(Ff,H3r),e(Ff,C5e),e(C5e,U3r),e(Ff,J3r),e(Ff,M5e),e(M5e,Y3r),e(Ff,K3r),e(jr,Z3r),e(jr,q0),e(q0,eyr),e(q0,E5e),e(E5e,oyr),e(q0,ryr),e(jr,tyr),e(jr,kt),g(G0,kt,null),e(kt,ayr),e(kt,y5e),e(y5e,nyr),e(kt,syr),e(kt,Cf),e(Cf,lyr),e(Cf,w5e),e(w5e,iyr),e(Cf,dyr),e(Cf,A5e),e(A5e,cyr),e(Cf,fyr),e(kt,myr),e(kt,L5e),e(L5e,gyr),e(kt,hyr),g(O0,kt,null),e(jr,pyr),e(jr,jo),g(X0,jo,null),e(jo,_yr),e(jo,B5e),e(B5e,uyr),e(jo,byr),e(jo,Sn),e(Sn,vyr),e(Sn,k5e),e(k5e,Tyr),e(Sn,Fyr),e(Sn,x5e),e(x5e,Cyr),e(Sn,Myr),e(Sn,R5e),e(R5e,Eyr),e(Sn,yyr),e(jo,wyr),e(jo,S5e),e(S5e,n4),e(n4,P5e),e(P5e,Ayr),e(n4,Lyr),e(n4,WX),e(WX,Byr),e(n4,kyr),e(jo,xyr),e(jo,$5e),e($5e,Ryr),e(jo,Syr),g(z0,jo,null),cBe=!0},p(d,[u]){const V0={};u&2&&(V0.$$scope={dirty:u,ctx:d}),Bf.$set(V0);const I5e={};u&2&&(I5e.$$scope={dirty:u,ctx:d}),dh.$set(I5e);const j5e={};u&2&&(j5e.$$scope={dirty:u,ctx:d}),Th.$set(j5e)},i(d){cBe||(h(ce.$$.fragment,d),h($a.$$.fragment,d),h(lM.$$.fragment,d),h(iM.$$.fragment,d),h(Bf.$$.fragment,d),h(dM.$$.fragment,d),h(cM.$$.fragment,d),h(gM.$$.fragment,d),h(hM.$$.fragment,d),h(pM.$$.fragment,d),h(_M.$$.fragment,d),h(uM.$$.fragment,d),h(TM.$$.fragment,d),h(FM.$$.fragment,d),h(CM.$$.fragment,d),h(MM.$$.fragment,d),h(EM.$$.fragment,d),h(AM.$$.fragment,d),h(dh.$$.fragment,d),h(LM.$$.fragment,d),h(BM.$$.fragment,d),h(kM.$$.fragment,d),h(xM.$$.fragment,d),h(PM.$$.fragment,d),h(Th.$$.fragment,d),h($M.$$.fragment,d),h(IM.$$.fragment,d),h(jM.$$.fragment,d),h(NM.$$.fragment,d),h(qM.$$.fragment,d),h(GM.$$.fragment,d),h(OM.$$.fragment,d),h(XM.$$.fragment,d),h(zM.$$.fragment,d),h(VM.$$.fragment,d),h(QM.$$.fragment,d),h(HM.$$.fragment,d),h(UM.$$.fragment,d),h(JM.$$.fragment,d),h(YM.$$.fragment,d),h(KM.$$.fragment,d),h(eE.$$.fragment,d),h(oE.$$.fragment,d),h(rE.$$.fragment,d),h(tE.$$.fragment,d),h(aE.$$.fragment,d),h(nE.$$.fragment,d),h(lE.$$.fragment,d),h(iE.$$.fragment,d),h(dE.$$.fragment,d),h(cE.$$.fragment,d),h(fE.$$.fragment,d),h(mE.$$.fragment,d),h(hE.$$.fragment,d),h(pE.$$.fragment,d),h(_E.$$.fragment,d),h(uE.$$.fragment,d),h(bE.$$.fragment,d),h(vE.$$.fragment,d),h(FE.$$.fragment,d),h(CE.$$.fragment,d),h(ME.$$.fragment,d),h(EE.$$.fragment,d),h(yE.$$.fragment,d),h(wE.$$.fragment,d),h(LE.$$.fragment,d),h(BE.$$.fragment,d),h(kE.$$.fragment,d),h(xE.$$.fragment,d),h(RE.$$.fragment,d),h(SE.$$.fragment,d),h($E.$$.fragment,d),h(IE.$$.fragment,d),h(jE.$$.fragment,d),h(NE.$$.fragment,d),h(DE.$$.fragment,d),h(qE.$$.fragment,d),h(OE.$$.fragment,d),h(XE.$$.fragment,d),h(zE.$$.fragment,d),h(VE.$$.fragment,d),h(WE.$$.fragment,d),h(QE.$$.fragment,d),h(UE.$$.fragment,d),h(JE.$$.fragment,d),h(YE.$$.fragment,d),h(KE.$$.fragment,d),h(ZE.$$.fragment,d),h(e3.$$.fragment,d),h(r3.$$.fragment,d),h(t3.$$.fragment,d),h(a3.$$.fragment,d),h(n3.$$.fragment,d),h(s3.$$.fragment,d),h(l3.$$.fragment,d),h(d3.$$.fragment,d),h(c3.$$.fragment,d),h(f3.$$.fragment,d),h(m3.$$.fragment,d),h(g3.$$.fragment,d),h(h3.$$.fragment,d),h(_3.$$.fragment,d),h(u3.$$.fragment,d),h(b3.$$.fragment,d),h(v3.$$.fragment,d),h(T3.$$.fragment,d),h(F3.$$.fragment,d),h(M3.$$.fragment,d),h(E3.$$.fragment,d),h(y3.$$.fragment,d),h(w3.$$.fragment,d),h(A3.$$.fragment,d),h(L3.$$.fragment,d),h(k3.$$.fragment,d),h(x3.$$.fragment,d),h(R3.$$.fragment,d),h(S3.$$.fragment,d),h(P3.$$.fragment,d),h($3.$$.fragment,d),h(j3.$$.fragment,d),h(N3.$$.fragment,d),h(D3.$$.fragment,d),h(q3.$$.fragment,d),h(G3.$$.fragment,d),h(O3.$$.fragment,d),h(z3.$$.fragment,d),h(V3.$$.fragment,d),h(W3.$$.fragment,d),h(H3.$$.fragment,d),h(U3.$$.fragment,d),h(J3.$$.fragment,d),h(K3.$$.fragment,d),h(Z3.$$.fragment,d),h(ey.$$.fragment,d),h(oy.$$.fragment,d),h(ry.$$.fragment,d),h(ty.$$.fragment,d),h(ny.$$.fragment,d),h(sy.$$.fragment,d),h(ly.$$.fragment,d),h(iy.$$.fragment,d),h(dy.$$.fragment,d),h(cy.$$.fragment,d),h(my.$$.fragment,d),h(gy.$$.fragment,d),h(hy.$$.fragment,d),h(py.$$.fragment,d),h(_y.$$.fragment,d),h(uy.$$.fragment,d),h(vy.$$.fragment,d),h(Ty.$$.fragment,d),h(Fy.$$.fragment,d),h(Cy.$$.fragment,d),h(My.$$.fragment,d),h(Ey.$$.fragment,d),h(wy.$$.fragment,d),h(Ay.$$.fragment,d),h(Ly.$$.fragment,d),h(ky.$$.fragment,d),h(xy.$$.fragment,d),h(Ry.$$.fragment,d),h(Py.$$.fragment,d),h($y.$$.fragment,d),h(Iy.$$.fragment,d),h(jy.$$.fragment,d),h(Ny.$$.fragment,d),h(Dy.$$.fragment,d),h(Gy.$$.fragment,d),h(Oy.$$.fragment,d),h(Xy.$$.fragment,d),h(zy.$$.fragment,d),h(Vy.$$.fragment,d),h(Wy.$$.fragment,d),h(Hy.$$.fragment,d),h(Uy.$$.fragment,d),h(Jy.$$.fragment,d),h(Yy.$$.fragment,d),h(Ky.$$.fragment,d),h(Zy.$$.fragment,d),h(ow.$$.fragment,d),h(rw.$$.fragment,d),h(tw.$$.fragment,d),h(aw.$$.fragment,d),h(nw.$$.fragment,d),h(sw.$$.fragment,d),h(iw.$$.fragment,d),h(dw.$$.fragment,d),h(cw.$$.fragment,d),h(fw.$$.fragment,d),h(mw.$$.fragment,d),h(gw.$$.fragment,d),h(pw.$$.fragment,d),h(_w.$$.fragment,d),h(uw.$$.fragment,d),h(bw.$$.fragment,d),h(vw.$$.fragment,d),h(Tw.$$.fragment,d),h(Cw.$$.fragment,d),h(Mw.$$.fragment,d),h(Ew.$$.fragment,d),h(yw.$$.fragment,d),h(ww.$$.fragment,d),h(Aw.$$.fragment,d),h(Bw.$$.fragment,d),h(kw.$$.fragment,d),h(xw.$$.fragment,d),h(Rw.$$.fragment,d),h(Sw.$$.fragment,d),h(Pw.$$.fragment,d),h(Iw.$$.fragment,d),h(jw.$$.fragment,d),h(Nw.$$.fragment,d),h(Dw.$$.fragment,d),h(qw.$$.fragment,d),h(Gw.$$.fragment,d),h(Xw.$$.fragment,d),h(zw.$$.fragment,d),h(Vw.$$.fragment,d),h(Ww.$$.fragment,d),h(Qw.$$.fragment,d),h(Hw.$$.fragment,d),h(Jw.$$.fragment,d),h(Yw.$$.fragment,d),h(Kw.$$.fragment,d),h(Zw.$$.fragment,d),h(eA.$$.fragment,d),h(oA.$$.fragment,d),h(tA.$$.fragment,d),h(aA.$$.fragment,d),h(nA.$$.fragment,d),h(sA.$$.fragment,d),h(lA.$$.fragment,d),h(iA.$$.fragment,d),h(cA.$$.fragment,d),h(fA.$$.fragment,d),h(mA.$$.fragment,d),h(gA.$$.fragment,d),h(hA.$$.fragment,d),h(pA.$$.fragment,d),h(uA.$$.fragment,d),h(bA.$$.fragment,d),h(vA.$$.fragment,d),h(TA.$$.fragment,d),h(FA.$$.fragment,d),h(CA.$$.fragment,d),h(EA.$$.fragment,d),h(yA.$$.fragment,d),h(wA.$$.fragment,d),h(AA.$$.fragment,d),h(LA.$$.fragment,d),h(BA.$$.fragment,d),h(xA.$$.fragment,d),h(RA.$$.fragment,d),h(SA.$$.fragment,d),h(PA.$$.fragment,d),h($A.$$.fragment,d),h(IA.$$.fragment,d),h(NA.$$.fragment,d),h(DA.$$.fragment,d),h(qA.$$.fragment,d),h(GA.$$.fragment,d),h(OA.$$.fragment,d),h(XA.$$.fragment,d),h(VA.$$.fragment,d),h(WA.$$.fragment,d),h(QA.$$.fragment,d),h(HA.$$.fragment,d),h(UA.$$.fragment,d),h(JA.$$.fragment,d),h(KA.$$.fragment,d),h(ZA.$$.fragment,d),h(e0.$$.fragment,d),h(o0.$$.fragment,d),h(r0.$$.fragment,d),h(t0.$$.fragment,d),h(n0.$$.fragment,d),h(s0.$$.fragment,d),h(l0.$$.fragment,d),h(i0.$$.fragment,d),h(d0.$$.fragment,d),h(c0.$$.fragment,d),h(m0.$$.fragment,d),h(g0.$$.fragment,d),h(h0.$$.fragment,d),h(p0.$$.fragment,d),h(_0.$$.fragment,d),h(u0.$$.fragment,d),h(v0.$$.fragment,d),h(T0.$$.fragment,d),h(F0.$$.fragment,d),h(C0.$$.fragment,d),h(M0.$$.fragment,d),h(E0.$$.fragment,d),h(w0.$$.fragment,d),h(A0.$$.fragment,d),h(L0.$$.fragment,d),h(B0.$$.fragment,d),h(k0.$$.fragment,d),h(x0.$$.fragment,d),h(S0.$$.fragment,d),h(P0.$$.fragment,d),h($0.$$.fragment,d),h(j0.$$.fragment,d),h(N0.$$.fragment,d),h(D0.$$.fragment,d),h(G0.$$.fragment,d),h(O0.$$.fragment,d),h(X0.$$.fragment,d),h(z0.$$.fragment,d),cBe=!0)},o(d){p(ce.$$.fragment,d),p($a.$$.fragment,d),p(lM.$$.fragment,d),p(iM.$$.fragment,d),p(Bf.$$.fragment,d),p(dM.$$.fragment,d),p(cM.$$.fragment,d),p(gM.$$.fragment,d),p(hM.$$.fragment,d),p(pM.$$.fragment,d),p(_M.$$.fragment,d),p(uM.$$.fragment,d),p(TM.$$.fragment,d),p(FM.$$.fragment,d),p(CM.$$.fragment,d),p(MM.$$.fragment,d),p(EM.$$.fragment,d),p(AM.$$.fragment,d),p(dh.$$.fragment,d),p(LM.$$.fragment,d),p(BM.$$.fragment,d),p(kM.$$.fragment,d),p(xM.$$.fragment,d),p(PM.$$.fragment,d),p(Th.$$.fragment,d),p($M.$$.fragment,d),p(IM.$$.fragment,d),p(jM.$$.fragment,d),p(NM.$$.fragment,d),p(qM.$$.fragment,d),p(GM.$$.fragment,d),p(OM.$$.fragment,d),p(XM.$$.fragment,d),p(zM.$$.fragment,d),p(VM.$$.fragment,d),p(QM.$$.fragment,d),p(HM.$$.fragment,d),p(UM.$$.fragment,d),p(JM.$$.fragment,d),p(YM.$$.fragment,d),p(KM.$$.fragment,d),p(eE.$$.fragment,d),p(oE.$$.fragment,d),p(rE.$$.fragment,d),p(tE.$$.fragment,d),p(aE.$$.fragment,d),p(nE.$$.fragment,d),p(lE.$$.fragment,d),p(iE.$$.fragment,d),p(dE.$$.fragment,d),p(cE.$$.fragment,d),p(fE.$$.fragment,d),p(mE.$$.fragment,d),p(hE.$$.fragment,d),p(pE.$$.fragment,d),p(_E.$$.fragment,d),p(uE.$$.fragment,d),p(bE.$$.fragment,d),p(vE.$$.fragment,d),p(FE.$$.fragment,d),p(CE.$$.fragment,d),p(ME.$$.fragment,d),p(EE.$$.fragment,d),p(yE.$$.fragment,d),p(wE.$$.fragment,d),p(LE.$$.fragment,d),p(BE.$$.fragment,d),p(kE.$$.fragment,d),p(xE.$$.fragment,d),p(RE.$$.fragment,d),p(SE.$$.fragment,d),p($E.$$.fragment,d),p(IE.$$.fragment,d),p(jE.$$.fragment,d),p(NE.$$.fragment,d),p(DE.$$.fragment,d),p(qE.$$.fragment,d),p(OE.$$.fragment,d),p(XE.$$.fragment,d),p(zE.$$.fragment,d),p(VE.$$.fragment,d),p(WE.$$.fragment,d),p(QE.$$.fragment,d),p(UE.$$.fragment,d),p(JE.$$.fragment,d),p(YE.$$.fragment,d),p(KE.$$.fragment,d),p(ZE.$$.fragment,d),p(e3.$$.fragment,d),p(r3.$$.fragment,d),p(t3.$$.fragment,d),p(a3.$$.fragment,d),p(n3.$$.fragment,d),p(s3.$$.fragment,d),p(l3.$$.fragment,d),p(d3.$$.fragment,d),p(c3.$$.fragment,d),p(f3.$$.fragment,d),p(m3.$$.fragment,d),p(g3.$$.fragment,d),p(h3.$$.fragment,d),p(_3.$$.fragment,d),p(u3.$$.fragment,d),p(b3.$$.fragment,d),p(v3.$$.fragment,d),p(T3.$$.fragment,d),p(F3.$$.fragment,d),p(M3.$$.fragment,d),p(E3.$$.fragment,d),p(y3.$$.fragment,d),p(w3.$$.fragment,d),p(A3.$$.fragment,d),p(L3.$$.fragment,d),p(k3.$$.fragment,d),p(x3.$$.fragment,d),p(R3.$$.fragment,d),p(S3.$$.fragment,d),p(P3.$$.fragment,d),p($3.$$.fragment,d),p(j3.$$.fragment,d),p(N3.$$.fragment,d),p(D3.$$.fragment,d),p(q3.$$.fragment,d),p(G3.$$.fragment,d),p(O3.$$.fragment,d),p(z3.$$.fragment,d),p(V3.$$.fragment,d),p(W3.$$.fragment,d),p(H3.$$.fragment,d),p(U3.$$.fragment,d),p(J3.$$.fragment,d),p(K3.$$.fragment,d),p(Z3.$$.fragment,d),p(ey.$$.fragment,d),p(oy.$$.fragment,d),p(ry.$$.fragment,d),p(ty.$$.fragment,d),p(ny.$$.fragment,d),p(sy.$$.fragment,d),p(ly.$$.fragment,d),p(iy.$$.fragment,d),p(dy.$$.fragment,d),p(cy.$$.fragment,d),p(my.$$.fragment,d),p(gy.$$.fragment,d),p(hy.$$.fragment,d),p(py.$$.fragment,d),p(_y.$$.fragment,d),p(uy.$$.fragment,d),p(vy.$$.fragment,d),p(Ty.$$.fragment,d),p(Fy.$$.fragment,d),p(Cy.$$.fragment,d),p(My.$$.fragment,d),p(Ey.$$.fragment,d),p(wy.$$.fragment,d),p(Ay.$$.fragment,d),p(Ly.$$.fragment,d),p(ky.$$.fragment,d),p(xy.$$.fragment,d),p(Ry.$$.fragment,d),p(Py.$$.fragment,d),p($y.$$.fragment,d),p(Iy.$$.fragment,d),p(jy.$$.fragment,d),p(Ny.$$.fragment,d),p(Dy.$$.fragment,d),p(Gy.$$.fragment,d),p(Oy.$$.fragment,d),p(Xy.$$.fragment,d),p(zy.$$.fragment,d),p(Vy.$$.fragment,d),p(Wy.$$.fragment,d),p(Hy.$$.fragment,d),p(Uy.$$.fragment,d),p(Jy.$$.fragment,d),p(Yy.$$.fragment,d),p(Ky.$$.fragment,d),p(Zy.$$.fragment,d),p(ow.$$.fragment,d),p(rw.$$.fragment,d),p(tw.$$.fragment,d),p(aw.$$.fragment,d),p(nw.$$.fragment,d),p(sw.$$.fragment,d),p(iw.$$.fragment,d),p(dw.$$.fragment,d),p(cw.$$.fragment,d),p(fw.$$.fragment,d),p(mw.$$.fragment,d),p(gw.$$.fragment,d),p(pw.$$.fragment,d),p(_w.$$.fragment,d),p(uw.$$.fragment,d),p(bw.$$.fragment,d),p(vw.$$.fragment,d),p(Tw.$$.fragment,d),p(Cw.$$.fragment,d),p(Mw.$$.fragment,d),p(Ew.$$.fragment,d),p(yw.$$.fragment,d),p(ww.$$.fragment,d),p(Aw.$$.fragment,d),p(Bw.$$.fragment,d),p(kw.$$.fragment,d),p(xw.$$.fragment,d),p(Rw.$$.fragment,d),p(Sw.$$.fragment,d),p(Pw.$$.fragment,d),p(Iw.$$.fragment,d),p(jw.$$.fragment,d),p(Nw.$$.fragment,d),p(Dw.$$.fragment,d),p(qw.$$.fragment,d),p(Gw.$$.fragment,d),p(Xw.$$.fragment,d),p(zw.$$.fragment,d),p(Vw.$$.fragment,d),p(Ww.$$.fragment,d),p(Qw.$$.fragment,d),p(Hw.$$.fragment,d),p(Jw.$$.fragment,d),p(Yw.$$.fragment,d),p(Kw.$$.fragment,d),p(Zw.$$.fragment,d),p(eA.$$.fragment,d),p(oA.$$.fragment,d),p(tA.$$.fragment,d),p(aA.$$.fragment,d),p(nA.$$.fragment,d),p(sA.$$.fragment,d),p(lA.$$.fragment,d),p(iA.$$.fragment,d),p(cA.$$.fragment,d),p(fA.$$.fragment,d),p(mA.$$.fragment,d),p(gA.$$.fragment,d),p(hA.$$.fragment,d),p(pA.$$.fragment,d),p(uA.$$.fragment,d),p(bA.$$.fragment,d),p(vA.$$.fragment,d),p(TA.$$.fragment,d),p(FA.$$.fragment,d),p(CA.$$.fragment,d),p(EA.$$.fragment,d),p(yA.$$.fragment,d),p(wA.$$.fragment,d),p(AA.$$.fragment,d),p(LA.$$.fragment,d),p(BA.$$.fragment,d),p(xA.$$.fragment,d),p(RA.$$.fragment,d),p(SA.$$.fragment,d),p(PA.$$.fragment,d),p($A.$$.fragment,d),p(IA.$$.fragment,d),p(NA.$$.fragment,d),p(DA.$$.fragment,d),p(qA.$$.fragment,d),p(GA.$$.fragment,d),p(OA.$$.fragment,d),p(XA.$$.fragment,d),p(VA.$$.fragment,d),p(WA.$$.fragment,d),p(QA.$$.fragment,d),p(HA.$$.fragment,d),p(UA.$$.fragment,d),p(JA.$$.fragment,d),p(KA.$$.fragment,d),p(ZA.$$.fragment,d),p(e0.$$.fragment,d),p(o0.$$.fragment,d),p(r0.$$.fragment,d),p(t0.$$.fragment,d),p(n0.$$.fragment,d),p(s0.$$.fragment,d),p(l0.$$.fragment,d),p(i0.$$.fragment,d),p(d0.$$.fragment,d),p(c0.$$.fragment,d),p(m0.$$.fragment,d),p(g0.$$.fragment,d),p(h0.$$.fragment,d),p(p0.$$.fragment,d),p(_0.$$.fragment,d),p(u0.$$.fragment,d),p(v0.$$.fragment,d),p(T0.$$.fragment,d),p(F0.$$.fragment,d),p(C0.$$.fragment,d),p(M0.$$.fragment,d),p(E0.$$.fragment,d),p(w0.$$.fragment,d),p(A0.$$.fragment,d),p(L0.$$.fragment,d),p(B0.$$.fragment,d),p(k0.$$.fragment,d),p(x0.$$.fragment,d),p(S0.$$.fragment,d),p(P0.$$.fragment,d),p($0.$$.fragment,d),p(j0.$$.fragment,d),p(N0.$$.fragment,d),p(D0.$$.fragment,d),p(G0.$$.fragment,d),p(O0.$$.fragment,d),p(X0.$$.fragment,d),p(z0.$$.fragment,d),cBe=!1},d(d){t(J),d&&t(Ae),d&&t(ie),_(ce),d&&t(Ef),d&&t(sa),d&&t(ye),d&&t(io),d&&t(wf),_($a,d),d&&t(co),d&&t(ge),d&&t(qo),d&&t(Ia),d&&t(f7e),d&&t(Si),_(lM),d&&t(m7e),d&&t(Nn),d&&t(g7e),_(iM,d),d&&t(h7e),d&&t(WL),d&&t(p7e),_(Bf,d),d&&t(_7e),d&&t(Pi),_(dM),d&&t(u7e),d&&t(Go),_(cM),_(gM),_(hM),_(pM),d&&t(b7e),d&&t(Ii),_(_M),d&&t(v7e),d&&t(Oo),_(uM),_(TM),_(FM),_(CM),d&&t(T7e),d&&t(ji),_(MM),d&&t(F7e),d&&t(Xo),_(EM),_(AM),_(dh),_(LM),_(BM),d&&t(C7e),d&&t(Ni),_(kM),d&&t(M7e),d&&t(zo),_(xM),_(PM),_(Th),_($M),_(IM),d&&t(E7e),d&&t(qi),_(jM),d&&t(y7e),d&&t(Vo),_(NM),_(qM),_(GM),_(OM),_(XM),d&&t(w7e),d&&t(Xi),_(zM),d&&t(A7e),d&&t(Wo),_(VM),_(QM),_(HM),_(UM),_(JM),d&&t(L7e),d&&t(Wi),_(YM),d&&t(B7e),d&&t(Qo),_(KM),_(eE),_(oE),_(rE),_(tE),d&&t(k7e),d&&t(Ui),_(aE),d&&t(x7e),d&&t(Ho),_(nE),_(lE),_(iE),_(dE),_(cE),d&&t(R7e),d&&t(Ki),_(fE),d&&t(S7e),d&&t(Uo),_(mE),_(hE),_(pE),_(_E),_(uE),d&&t(P7e),d&&t(od),_(bE),d&&t($7e),d&&t(Jo),_(vE),_(FE),_(CE),_(ME),_(EE),d&&t(I7e),d&&t(ad),_(yE),d&&t(j7e),d&&t(Yo),_(wE),_(LE),_(BE),_(kE),_(xE),d&&t(N7e),d&&t(ld),_(RE),d&&t(D7e),d&&t(Ko),_(SE),_($E),_(IE),_(jE),_(NE),d&&t(q7e),d&&t(cd),_(DE),d&&t(G7e),d&&t(Zo),_(qE),_(OE),_(XE),_(zE),_(VE),d&&t(O7e),d&&t(gd),_(WE),d&&t(X7e),d&&t(er),_(QE),_(UE),_(JE),_(YE),_(KE),d&&t(z7e),d&&t(_d),_(ZE),d&&t(V7e),d&&t(or),_(e3),_(r3),_(t3),_(a3),_(n3),d&&t(W7e),d&&t(vd),_(s3),d&&t(Q7e),d&&t(rr),_(l3),_(d3),_(c3),_(f3),_(m3),d&&t(H7e),d&&t(Cd),_(g3),d&&t(U7e),d&&t(tr),_(h3),_(_3),_(u3),_(b3),_(v3),d&&t(J7e),d&&t(yd),_(T3),d&&t(Y7e),d&&t(ar),_(F3),_(M3),_(E3),_(y3),_(w3),d&&t(K7e),d&&t(Ld),_(A3),d&&t(Z7e),d&&t(nr),_(L3),_(k3),_(x3),_(R3),_(S3),d&&t(e9e),d&&t(Rd),_(P3),d&&t(o9e),d&&t(sr),_($3),_(j3),_(N3),_(D3),_(q3),d&&t(r9e),d&&t($d),_(G3),d&&t(t9e),d&&t(lr),_(O3),_(z3),_(V3),_(W3),_(H3),d&&t(a9e),d&&t(Nd),_(U3),d&&t(n9e),d&&t(ir),_(J3),_(K3),_(Z3),_(ey),_(oy),d&&t(s9e),d&&t(Od),_(ry),d&&t(l9e),d&&t(dr),_(ty),_(ny),_(sy),_(ly),_(iy),d&&t(i9e),d&&t(Wd),_(dy),d&&t(d9e),d&&t(cr),_(cy),_(my),_(gy),_(hy),_(py),d&&t(c9e),d&&t(Ud),_(_y),d&&t(f9e),d&&t(fr),_(uy),_(vy),_(Ty),_(Fy),_(Cy),d&&t(m9e),d&&t(Kd),_(My),d&&t(g9e),d&&t(mr),_(Ey),_(wy),_(Ay),_(Ly),_(ky),d&&t(h9e),d&&t(oc),_(xy),d&&t(p9e),d&&t(gr),_(Ry),_(Py),_($y),_(Iy),_(jy),d&&t(_9e),d&&t(ac),_(Ny),d&&t(u9e),d&&t(hr),_(Dy),_(Gy),_(Oy),_(Xy),_(zy),d&&t(b9e),d&&t(lc),_(Vy),d&&t(v9e),d&&t(pr),_(Wy),_(Hy),_(Uy),_(Jy),_(Yy),d&&t(T9e),d&&t(cc),_(Ky),d&&t(F9e),d&&t(_r),_(Zy),_(ow),_(rw),_(tw),_(aw),d&&t(C9e),d&&t(gc),_(nw),d&&t(M9e),d&&t(ur),_(sw),_(iw),_(dw),_(cw),_(fw),d&&t(E9e),d&&t(_c),_(mw),d&&t(y9e),d&&t(br),_(gw),_(pw),_(_w),_(uw),_(bw),d&&t(w9e),d&&t(vc),_(vw),d&&t(A9e),d&&t(vr),_(Tw),_(Cw),_(Mw),_(Ew),_(yw),d&&t(L9e),d&&t(Cc),_(ww),d&&t(B9e),d&&t(Tr),_(Aw),_(Bw),_(kw),_(xw),_(Rw),d&&t(k9e),d&&t(yc),_(Sw),d&&t(x9e),d&&t(Fr),_(Pw),_(Iw),_(jw),_(Nw),_(Dw),d&&t(R9e),d&&t(Lc),_(qw),d&&t(S9e),d&&t(Cr),_(Gw),_(Xw),_(zw),_(Vw),_(Ww),d&&t(P9e),d&&t(xc),_(Qw),d&&t($9e),d&&t(Mr),_(Hw),_(Jw),_(Yw),_(Kw),_(Zw),d&&t(I9e),d&&t(Pc),_(eA),d&&t(j9e),d&&t(Er),_(oA),_(tA),_(aA),_(nA),_(sA),d&&t(N9e),d&&t(jc),_(lA),d&&t(D9e),d&&t(yr),_(iA),_(cA),_(fA),_(mA),_(gA),d&&t(q9e),d&&t(qc),_(hA),d&&t(G9e),d&&t(wr),_(pA),_(uA),_(bA),_(vA),_(TA),d&&t(O9e),d&&t(Xc),_(FA),d&&t(X9e),d&&t(Ar),_(CA),_(EA),_(yA),_(wA),_(AA),d&&t(z9e),d&&t(Wc),_(LA),d&&t(V9e),d&&t(Lr),_(BA),_(xA),_(RA),_(SA),_(PA),d&&t(W9e),d&&t(Uc),_($A),d&&t(Q9e),d&&t(Br),_(IA),_(NA),_(DA),_(qA),_(GA),d&&t(H9e),d&&t(Kc),_(OA),d&&t(U9e),d&&t(kr),_(XA),_(VA),_(WA),_(QA),_(HA),d&&t(J9e),d&&t(of),_(UA),d&&t(Y9e),d&&t(xr),_(JA),_(KA),_(ZA),_(e0),_(o0),d&&t(K9e),d&&t(af),_(r0),d&&t(Z9e),d&&t(Rr),_(t0),_(n0),_(s0),_(l0),_(i0),d&&t(eBe),d&&t(lf),_(d0),d&&t(oBe),d&&t(Sr),_(c0),_(m0),_(g0),_(h0),_(p0),d&&t(rBe),d&&t(ff),_(_0),d&&t(tBe),d&&t(Pr),_(u0),_(v0),_(T0),_(F0),_(C0),d&&t(aBe),d&&t(hf),_(M0),d&&t(nBe),d&&t($r),_(E0),_(w0),_(A0),_(L0),_(B0),d&&t(sBe),d&&t(uf),_(k0),d&&t(lBe),d&&t(Ir),_(x0),_(S0),_(P0),_($0),_(j0),d&&t(iBe),d&&t(Tf),_(N0),d&&t(dBe),d&&t(jr),_(D0),_(G0),_(O0),_(X0),_(z0)}}}const Q_t={local:"auto-classes",sections:[{local:"extending-the-auto-classes",title:"Extending the Auto Classes"},{local:"transformers.AutoConfig",title:"AutoConfig"},{local:"transformers.AutoTokenizer",title:"AutoTokenizer"},{local:"transformers.AutoFeatureExtractor",title:"AutoFeatureExtractor"},{local:"transformers.AutoProcessor",title:"AutoProcessor"},{local:"transformers.AutoModel",title:"AutoModel"},{local:"transformers.AutoModelForPreTraining",title:"AutoModelForPreTraining"},{local:"transformers.AutoModelForCausalLM",title:"AutoModelForCausalLM"},{local:"transformers.AutoModelForMaskedLM",title:"AutoModelForMaskedLM"},{local:"transformers.AutoModelForSeq2SeqLM",title:"AutoModelForSeq2SeqLM"},{local:"transformers.AutoModelForSequenceClassification",title:"AutoModelForSequenceClassification"},{local:"transformers.AutoModelForMultipleChoice",title:"AutoModelForMultipleChoice"},{local:"transformers.AutoModelForNextSentencePrediction",title:"AutoModelForNextSentencePrediction"},{local:"transformers.AutoModelForTokenClassification",title:"AutoModelForTokenClassification"},{local:"transformers.AutoModelForQuestionAnswering",title:"AutoModelForQuestionAnswering"},{local:"transformers.AutoModelForTableQuestionAnswering",title:"AutoModelForTableQuestionAnswering"},{local:"transformers.AutoModelForImageClassification",title:"AutoModelForImageClassification"},{local:"transformers.AutoModelForVision2Seq",title:"AutoModelForVision2Seq"},{local:"transformers.AutoModelForAudioClassification",title:"AutoModelForAudioClassification"},{local:"transformers.AutoModelForAudioFrameClassification",title:"AutoModelForAudioFrameClassification"},{local:"transformers.AutoModelForCTC",title:"AutoModelForCTC"},{local:"transformers.AutoModelForSpeechSeq2Seq",title:"AutoModelForSpeechSeq2Seq"},{local:"transformers.AutoModelForAudioXVector",title:"AutoModelForAudioXVector"},{local:"transformers.AutoModelForMaskedImageModeling",title:"AutoModelForMaskedImageModeling"},{local:"transformers.AutoModelForObjectDetection",title:"AutoModelForObjectDetection"},{local:"transformers.AutoModelForImageSegmentation",title:"AutoModelForImageSegmentation"},{local:"transformers.AutoModelForSemanticSegmentation",title:"AutoModelForSemanticSegmentation"},{local:"transformers.TFAutoModel",title:"TFAutoModel"},{local:"transformers.TFAutoModelForPreTraining",title:"TFAutoModelForPreTraining"},{local:"transformers.TFAutoModelForCausalLM",title:"TFAutoModelForCausalLM"},{local:"transformers.TFAutoModelForImageClassification",title:"TFAutoModelForImageClassification"},{local:"transformers.TFAutoModelForMaskedLM",title:"TFAutoModelForMaskedLM"},{local:"transformers.TFAutoModelForSeq2SeqLM",title:"TFAutoModelForSeq2SeqLM"},{local:"transformers.TFAutoModelForSequenceClassification",title:"TFAutoModelForSequenceClassification"},{local:"transformers.TFAutoModelForMultipleChoice",title:"TFAutoModelForMultipleChoice"},{local:"transformers.TFAutoModelForTableQuestionAnswering",title:"TFAutoModelForTableQuestionAnswering"},{local:"transformers.TFAutoModelForTokenClassification",title:"TFAutoModelForTokenClassification"},{local:"transformers.TFAutoModelForQuestionAnswering",title:"TFAutoModelForQuestionAnswering"},{local:"transformers.TFAutoModelForVision2Seq",title:"TFAutoModelForVision2Seq"},{local:"transformers.TFAutoModelForSpeechSeq2Seq",title:"TFAutoModelForSpeechSeq2Seq"},{local:"transformers.FlaxAutoModel",title:"FlaxAutoModel"},{local:"transformers.FlaxAutoModelForCausalLM",title:"FlaxAutoModelForCausalLM"},{local:"transformers.FlaxAutoModelForPreTraining",title:"FlaxAutoModelForPreTraining"},{local:"transformers.FlaxAutoModelForMaskedLM",title:"FlaxAutoModelForMaskedLM"},{local:"transformers.FlaxAutoModelForSeq2SeqLM",title:"FlaxAutoModelForSeq2SeqLM"},{local:"transformers.FlaxAutoModelForSequenceClassification",title:"FlaxAutoModelForSequenceClassification"},{local:"transformers.FlaxAutoModelForQuestionAnswering",title:"FlaxAutoModelForQuestionAnswering"},{local:"transformers.FlaxAutoModelForTokenClassification",title:"FlaxAutoModelForTokenClassification"},{local:"transformers.FlaxAutoModelForMultipleChoice",title:"FlaxAutoModelForMultipleChoice"},{local:"transformers.FlaxAutoModelForNextSentencePrediction",title:"FlaxAutoModelForNextSentencePrediction"},{local:"transformers.FlaxAutoModelForImageClassification",title:"FlaxAutoModelForImageClassification"},{local:"transformers.FlaxAutoModelForVision2Seq",title:"FlaxAutoModelForVision2Seq"}],title:"Auto Classes"};function H_t(yi,J,Ae){let{fw:ie}=J;return yi.$$set=me=>{"fw"in me&&Ae(0,ie=me.fw)},[ie]}class out extends D_t{constructor(J){super();q_t(this,J,H_t,W_t,G_t,{fw:0})}}export{out as default,Q_t as metadata};
