import{S as Hi,i as Bi,s as Ji,e as a,k as l,w as f,t as s,M as Ki,c as n,d as r,m as c,a as o,x as g,h as i,b as m,N as Oa,F as t,g as d,y as _,q as w,o as v,B as b,v as Qi,L as ja}from"../../chunks/vendor-6b77c823.js";import{D as T}from"../../chunks/Docstring-1088f2fb.js";import{C as qa}from"../../chunks/CodeBlock-3a8b25a8.js";import{I as ne}from"../../chunks/IconCopyLink-7a11ce68.js";import{E as Ra}from"../../chunks/ExampleCodeBlock-5212b321.js";function Xi(N){let u,A,h,z,F;return z=new qa({props:{code:"Adafactor(model.parameters(), scale_parameter=False, relative_step=False, warmup_init=False, lr=1e-3)",highlighted:'Adafactor(model.parameters(), scale_parameter=<span class="hljs-literal">False</span>, relative_step=<span class="hljs-literal">False</span>, warmup_init=<span class="hljs-literal">False</span>, lr=<span class="hljs-number">1e-3</span>)'}}),{c(){u=a("p"),A=s("Example:"),h=l(),f(z.$$.fragment)},l(x){u=n(x,"P",{});var C=o(u);A=i(C,"Example:"),C.forEach(r),h=c(x),g(z.$$.fragment,x)},m(x,C){d(x,u,C),t(u,A),d(x,h,C),_(z,x,C),F=!0},p:ja,i(x){F||(w(z.$$.fragment,x),F=!0)},o(x){v(z.$$.fragment,x),F=!1},d(x){x&&r(u),x&&r(h),b(z,x)}}}function Yi(N){let u,A;return u=new qa({props:{code:"Adafactor(model.parameters(), scale_parameter=True, relative_step=True, warmup_init=True, lr=None)",highlighted:'Adafactor(model.parameters(), scale_parameter=<span class="hljs-literal">True</span>, relative_step=<span class="hljs-literal">True</span>, warmup_init=<span class="hljs-literal">True</span>, lr=<span class="hljs-literal">None</span>)'}}),{c(){f(u.$$.fragment)},l(h){g(u.$$.fragment,h)},m(h,z){_(u,h,z),A=!0},p:ja,i(h){A||(w(u.$$.fragment,h),A=!0)},o(h){v(u.$$.fragment,h),A=!1},d(h){b(u,h)}}}function Zi(N){let u,A;return u=new qa({props:{code:`from transformers.optimization import Adafactor, AdafactorSchedule

optimizer = Adafactor(model.parameters(), scale_parameter=True, relative_step=True, warmup_init=True, lr=None)
lr_scheduler = AdafactorSchedule(optimizer)
trainer = Trainer(..., optimizers=(optimizer, lr_scheduler))`,highlighted:`<span class="hljs-keyword">from</span> transformers.optimization <span class="hljs-keyword">import</span> Adafactor, AdafactorSchedule

optimizer = Adafactor(model.parameters(), scale_parameter=<span class="hljs-literal">True</span>, relative_step=<span class="hljs-literal">True</span>, warmup_init=<span class="hljs-literal">True</span>, lr=<span class="hljs-literal">None</span>)
lr_scheduler = AdafactorSchedule(optimizer)
trainer = Trainer(..., optimizers=(optimizer, lr_scheduler))`}}),{c(){f(u.$$.fragment)},l(h){g(u.$$.fragment,h)},m(h,z){_(u,h,z),A=!0},p:ja,i(h){A||(w(u.$$.fragment,h),A=!0)},o(h){v(u.$$.fragment,h),A=!1},d(h){b(u,h)}}}function el(N){let u,A;return u=new qa({props:{code:`# replace AdamW with Adafactor
optimizer = Adafactor(
    model.parameters(),
    lr=1e-3,
    eps=(1e-30, 1e-3),
    clip_threshold=1.0,
    decay_rate=-0.8,
    beta1=None,
    weight_decay=0.0,
    relative_step=False,
    scale_parameter=False,
    warmup_init=False,
)`,highlighted:`<span class="hljs-comment"># replace AdamW with Adafactor</span>
optimizer = Adafactor(
    model.parameters(),
    lr=<span class="hljs-number">1e-3</span>,
    eps=(<span class="hljs-number">1e-30</span>, <span class="hljs-number">1e-3</span>),
    clip_threshold=<span class="hljs-number">1.0</span>,
    decay_rate=-<span class="hljs-number">0.8</span>,
    beta1=<span class="hljs-literal">None</span>,
    weight_decay=<span class="hljs-number">0.0</span>,
    relative_step=<span class="hljs-literal">False</span>,
    scale_parameter=<span class="hljs-literal">False</span>,
    warmup_init=<span class="hljs-literal">False</span>,
)`}}),{c(){f(u.$$.fragment)},l(h){g(u.$$.fragment,h)},m(h,z){_(u,h,z),A=!0},p:ja,i(h){A||(w(u.$$.fragment,h),A=!0)},o(h){v(u.$$.fragment,h),A=!1},d(h){b(u,h)}}}function tl(N){let u,A,h,z,F,x,C,zt,Ua,Ir,oe,Ga,Et,Va,Ma,Nr,O,Tt,Ha,Ba,Ee,Ja,Dt,Ka,Qa,Xa,Lt,Ya,Fr,R,se,Pt,Te,Za,Wt,en,Cr,P,De,tn,Le,rn,Pe,an,nn,on,ie,We,sn,kt,ln,Or,j,le,St,ke,cn,It,mn,Rr,y,Se,pn,_t,dn,Ie,hn,un,E,fn,Nt,gn,_n,Ne,wn,vn,Ft,bn,yn,Ct,$n,An,Ot,xn,zn,Rt,En,Tn,jt,Dn,Ln,Pn,qt,Wn,kn,Fe,Sn,Ce,In,Nn,Fn,W,Oe,Ut,Cn,On,Re,Gt,Rn,jn,je,qn,qe,Un,Gn,Vn,Vt,Mt,Mn,Hn,Ht,Bt,Bn,Jn,Jt,Kt,Kn,Qn,ce,Xn,Qt,Yn,Zn,me,eo,k,to,Xt,ro,ao,wt,no,oo,Yt,so,io,lo,pe,co,Zt,mo,po,de,ho,he,Ue,uo,er,fo,jr,q,ue,tr,Ge,go,rr,_o,qr,L,Ve,wo,U,vo,ar,bo,yo,Me,$o,Ao,xo,nr,zo,Eo,fe,He,To,or,Do,Ur,G,Be,Lo,sr,Po,Gr,V,ge,ir,Je,Wo,lr,ko,Vr,M,_e,cr,Ke,So,mr,Io,Mr,H,Qe,No,pr,Fo,Hr,B,Xe,Co,dr,Oo,Br,J,Ye,Ro,hr,jo,Jr,K,Ze,qo,ur,Uo,Kr,et,$s,Qr,Q,tt,Go,fr,Vo,Xr,rt,As,Yr,X,at,Mo,gr,Ho,Zr,nt,xs,ea,Y,ot,Bo,_r,Jo,ta,st,zs,ra,S,it,Ko,lt,Qo,wr,Xo,Yo,Zo,we,es,vr,ts,rs,ct,as,aa,Z,ve,br,mt,ns,yr,os,na,ee,pt,ss,$r,is,oa,te,be,Ar,dt,ls,xr,cs,sa,re,ye,zr,ht,ms,Er,ps,ia,I,ut,ds,ae,hs,Tr,us,fs,Dr,gs,_s,ws,$e,ft,vs,Lr,bs,la;return x=new ne({}),Te=new ne({}),De=new T({props:{name:"class transformers.AdamW",anchor:"transformers.AdamW",parameters:[{name:"params",val:": typing.Iterable[torch.nn.parameter.Parameter]"},{name:"lr",val:": float = 0.001"},{name:"betas",val:": typing.Tuple[float, float] = (0.9, 0.999)"},{name:"eps",val:": float = 1e-06"},{name:"weight_decay",val:": float = 0.0"},{name:"correct_bias",val:": bool = True"},{name:"no_deprecation_warning",val:": bool = False"}],parametersDescription:[{anchor:"transformers.AdamW.params",description:`<strong>params</strong> (<code>Iterable[nn.parameter.Parameter]</code>) &#x2014;
Iterable of parameters to optimize or dictionaries defining parameter groups.`,name:"params"},{anchor:"transformers.AdamW.lr",description:`<strong>lr</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-3) &#x2014;
The learning rate to use.`,name:"lr"},{anchor:"transformers.AdamW.betas",description:`<strong>betas</strong> (<code>Tuple[float,float]</code>, <em>optional</em>, defaults to (0.9, 0.999)) &#x2014;
Adam&#x2019;s betas parameters (b1, b2).`,name:"betas"},{anchor:"transformers.AdamW.eps",description:`<strong>eps</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-6) &#x2014;
Adam&#x2019;s epsilon for numerical stability.`,name:"eps"},{anchor:"transformers.AdamW.weight_decay",description:`<strong>weight_decay</strong> (<code>float</code>, <em>optional</em>, defaults to 0) &#x2014;
Decoupled weight decay to apply.`,name:"weight_decay"},{anchor:"transformers.AdamW.correct_bias",description:`<strong>correct_bias</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to correct bias in Adam (for instance, in Bert TF repository they use <code>False</code>).`,name:"correct_bias"},{anchor:"transformers.AdamW.no_deprecation_warning",description:`<strong>no_deprecation_warning</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
A flag used to disable the deprecation warning (set to <code>True</code> to disable the warning).`,name:"no_deprecation_warning"}],source:"https://github.com/huggingface/transformers/blob/pr_16919/src/transformers/optimization.py#L273"}}),We=new T({props:{name:"step",anchor:"transformers.AdamW.step",parameters:[{name:"closure",val:": typing.Callable = None"}],parametersDescription:[{anchor:"transformers.AdamW.step.closure",description:"<strong>closure</strong> (<code>Callable</code>, <em>optional</em>) &#x2014; A closure that reevaluates the model and returns the loss.",name:"closure"}],source:"https://github.com/huggingface/transformers/blob/pr_16919/src/transformers/optimization.py#L323"}}),ke=new ne({}),Se=new T({props:{name:"class transformers.Adafactor",anchor:"transformers.Adafactor",parameters:[{name:"params",val:""},{name:"lr",val:" = None"},{name:"eps",val:" = (1e-30, 0.001)"},{name:"clip_threshold",val:" = 1.0"},{name:"decay_rate",val:" = -0.8"},{name:"beta1",val:" = None"},{name:"weight_decay",val:" = 0.0"},{name:"scale_parameter",val:" = True"},{name:"relative_step",val:" = True"},{name:"warmup_init",val:" = False"}],parametersDescription:[{anchor:"transformers.Adafactor.params",description:`<strong>params</strong> (<code>Iterable[nn.parameter.Parameter]</code>) &#x2014;
Iterable of parameters to optimize or dictionaries defining parameter groups.`,name:"params"},{anchor:"transformers.Adafactor.lr",description:`<strong>lr</strong> (<code>float</code>, <em>optional</em>) &#x2014;
The external learning rate.`,name:"lr"},{anchor:"transformers.Adafactor.eps",description:`<strong>eps</strong> (<code>Tuple[float, float]</code>, <em>optional</em>, defaults to (1e-30, 1e-3)) &#x2014;
Regularization constants for square gradient and parameter scale respectively`,name:"eps"},{anchor:"transformers.Adafactor.clip_threshold",description:`<strong>clip_threshold</strong> (<code>float</code>, <em>optional</em>, defaults 1.0) &#x2014;
Threshold of root mean square of final gradient update`,name:"clip_threshold"},{anchor:"transformers.Adafactor.decay_rate",description:`<strong>decay_rate</strong> (<code>float</code>, <em>optional</em>, defaults to -0.8) &#x2014;
Coefficient used to compute running averages of square`,name:"decay_rate"},{anchor:"transformers.Adafactor.beta1",description:`<strong>beta1</strong> (<code>float</code>, <em>optional</em>) &#x2014;
Coefficient used for computing running averages of gradient`,name:"beta1"},{anchor:"transformers.Adafactor.weight_decay",description:`<strong>weight_decay</strong> (<code>float</code>, <em>optional</em>, defaults to 0) &#x2014;
Weight decay (L2 penalty)`,name:"weight_decay"},{anchor:"transformers.Adafactor.scale_parameter",description:`<strong>scale_parameter</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
If True, learning rate is scaled by root mean square`,name:"scale_parameter"},{anchor:"transformers.Adafactor.relative_step",description:`<strong>relative_step</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
If True, time-dependent learning rate is computed instead of external learning rate`,name:"relative_step"},{anchor:"transformers.Adafactor.warmup_init",description:`<strong>warmup_init</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Time-dependent learning rate computation depends on whether warm-up initialization is being used`,name:"warmup_init"}],source:"https://github.com/huggingface/transformers/blob/pr_16919/src/transformers/optimization.py#L385"}}),ce=new Ra({props:{anchor:"transformers.Adafactor.example",$$slots:{default:[Xi]},$$scope:{ctx:N}}}),me=new Ra({props:{anchor:"transformers.Adafactor.example-2",$$slots:{default:[Yi]},$$scope:{ctx:N}}}),pe=new Ra({props:{anchor:"transformers.Adafactor.example-3",$$slots:{default:[Zi]},$$scope:{ctx:N}}}),de=new Ra({props:{anchor:"transformers.Adafactor.example-4",$$slots:{default:[el]},$$scope:{ctx:N}}}),Ue=new T({props:{name:"step",anchor:"transformers.Adafactor.step",parameters:[{name:"closure",val:" = None"}],parametersDescription:[{anchor:"transformers.Adafactor.step.closure",description:`<strong>closure</strong> (callable, optional) &#x2014; A closure that reevaluates the model
and returns the loss.`,name:"closure"}],source:"https://github.com/huggingface/transformers/blob/pr_16919/src/transformers/optimization.py#L531"}}),Ge=new ne({}),Ve=new T({props:{name:"class transformers.AdamWeightDecay",anchor:"transformers.AdamWeightDecay",parameters:[{name:"learning_rate",val:": typing.Union[float, tensorflow.python.keras.optimizer_v2.learning_rate_schedule.LearningRateSchedule] = 0.001"},{name:"beta_1",val:": float = 0.9"},{name:"beta_2",val:": float = 0.999"},{name:"epsilon",val:": float = 1e-07"},{name:"amsgrad",val:": bool = False"},{name:"weight_decay_rate",val:": float = 0.0"},{name:"include_in_weight_decay",val:": typing.Optional[typing.List[str]] = None"},{name:"exclude_from_weight_decay",val:": typing.Optional[typing.List[str]] = None"},{name:"name",val:": str = 'AdamWeightDecay'"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AdamWeightDecay.learning_rate",description:`<strong>learning_rate</strong> (<code>Union[float, tf.keras.optimizers.schedules.LearningRateSchedule]</code>, <em>optional</em>, defaults to 1e-3) &#x2014;
The learning rate to use or a schedule.`,name:"learning_rate"},{anchor:"transformers.AdamWeightDecay.beta_1",description:`<strong>beta_1</strong> (<code>float</code>, <em>optional</em>, defaults to 0.9) &#x2014;
The beta1 parameter in Adam, which is the exponential decay rate for the 1st momentum estimates.`,name:"beta_1"},{anchor:"transformers.AdamWeightDecay.beta_2",description:`<strong>beta_2</strong> (<code>float</code>, <em>optional</em>, defaults to 0.999) &#x2014;
The beta2 parameter in Adam, which is the exponential decay rate for the 2nd momentum estimates.`,name:"beta_2"},{anchor:"transformers.AdamWeightDecay.epsilon",description:`<strong>epsilon</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-7) &#x2014;
The epsilon parameter in Adam, which is a small constant for numerical stability.`,name:"epsilon"},{anchor:"transformers.AdamWeightDecay.amsgrad",description:`<strong>amsgrad</strong> (<code>bool</code>, <em>optional</em>, default to <code>False</code>) &#x2014;
Whether to apply AMSGrad variant of this algorithm or not, see <a href="https://arxiv.org/abs/1904.09237" rel="nofollow">On the Convergence of Adam and
Beyond</a>.`,name:"amsgrad"},{anchor:"transformers.AdamWeightDecay.weight_decay_rate",description:`<strong>weight_decay_rate</strong> (<code>float</code>, <em>optional</em>, defaults to 0) &#x2014;
The weight decay to apply.`,name:"weight_decay_rate"},{anchor:"transformers.AdamWeightDecay.include_in_weight_decay",description:`<strong>include_in_weight_decay</strong> (<code>List[str]</code>, <em>optional</em>) &#x2014;
List of the parameter names (or re patterns) to apply weight decay to. If none is passed, weight decay is
applied to all parameters by default (unless they are in <code>exclude_from_weight_decay</code>).`,name:"include_in_weight_decay"},{anchor:"transformers.AdamWeightDecay.exclude_from_weight_decay",description:`<strong>exclude_from_weight_decay</strong> (<code>List[str]</code>, <em>optional</em>) &#x2014;
List of the parameter names (or re patterns) to exclude from applying weight decay to. If a
<code>include_in_weight_decay</code> is passed, the names in it will supersede this list.`,name:"exclude_from_weight_decay"},{anchor:"transformers.AdamWeightDecay.name",description:`<strong>name</strong> (<code>str</code>, <em>optional</em>, defaults to &#x2018;AdamWeightDecay&#x2019;) &#x2014;
Optional name for the operations created when applying gradients.
kwargs &#x2014;
Keyword arguments. Allowed to be {<code>clipnorm</code>, <code>clipvalue</code>, <code>lr</code>, <code>decay</code>}. <code>clipnorm</code> is clip gradients by
norm; <code>clipvalue</code> is clip gradients by value, <code>decay</code> is included for backward compatibility to allow time
inverse decay of learning rate. <code>lr</code> is included for backward compatibility, recommended to use
<code>learning_rate</code> instead.`,name:"name"}],source:"https://github.com/huggingface/transformers/blob/pr_16919/src/transformers/optimization_tf.py#L152"}}),He=new T({props:{name:"from_config",anchor:"transformers.AdamWeightDecay.from_config",parameters:[{name:"config",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_16919/src/transformers/optimization_tf.py#L209"}}),Be=new T({props:{name:"transformers.create_optimizer",anchor:"transformers.create_optimizer",parameters:[{name:"init_lr",val:": float"},{name:"num_train_steps",val:": int"},{name:"num_warmup_steps",val:": int"},{name:"min_lr_ratio",val:": float = 0.0"},{name:"adam_beta1",val:": float = 0.9"},{name:"adam_beta2",val:": float = 0.999"},{name:"adam_epsilon",val:": float = 1e-08"},{name:"weight_decay_rate",val:": float = 0.0"},{name:"power",val:": float = 1.0"},{name:"include_in_weight_decay",val:": typing.Optional[typing.List[str]] = None"}],parametersDescription:[{anchor:"transformers.create_optimizer.init_lr",description:`<strong>init_lr</strong> (<code>float</code>) &#x2014;
The desired learning rate at the end of the warmup phase.`,name:"init_lr"},{anchor:"transformers.create_optimizer.num_train_steps",description:`<strong>num_train_steps</strong> (<code>int</code>) &#x2014;
The total number of training steps.`,name:"num_train_steps"},{anchor:"transformers.create_optimizer.num_warmup_steps",description:`<strong>num_warmup_steps</strong> (<code>int</code>) &#x2014;
The number of warmup steps.`,name:"num_warmup_steps"},{anchor:"transformers.create_optimizer.min_lr_ratio",description:`<strong>min_lr_ratio</strong> (<code>float</code>, <em>optional</em>, defaults to 0) &#x2014;
The final learning rate at the end of the linear decay will be <code>init_lr * min_lr_ratio</code>.`,name:"min_lr_ratio"},{anchor:"transformers.create_optimizer.adam_beta1",description:`<strong>adam_beta1</strong> (<code>float</code>, <em>optional</em>, defaults to 0.9) &#x2014;
The beta1 to use in Adam.`,name:"adam_beta1"},{anchor:"transformers.create_optimizer.adam_beta2",description:`<strong>adam_beta2</strong> (<code>float</code>, <em>optional</em>, defaults to 0.999) &#x2014;
The beta2 to use in Adam.`,name:"adam_beta2"},{anchor:"transformers.create_optimizer.adam_epsilon",description:`<strong>adam_epsilon</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-8) &#x2014;
The epsilon to use in Adam.`,name:"adam_epsilon"},{anchor:"transformers.create_optimizer.weight_decay_rate",description:`<strong>weight_decay_rate</strong> (<code>float</code>, <em>optional</em>, defaults to 0) &#x2014;
The weight decay to use.`,name:"weight_decay_rate"},{anchor:"transformers.create_optimizer.power",description:`<strong>power</strong> (<code>float</code>, <em>optional</em>, defaults to 1.0) &#x2014;
The power to use for PolynomialDecay.`,name:"power"},{anchor:"transformers.create_optimizer.include_in_weight_decay",description:`<strong>include_in_weight_decay</strong> (<code>List[str]</code>, <em>optional</em>) &#x2014;
List of the parameter names (or re patterns) to apply weight decay to. If none is passed, weight decay is
applied to all parameters except bias and layer norm parameters.`,name:"include_in_weight_decay"}],source:"https://github.com/huggingface/transformers/blob/pr_16919/src/transformers/optimization_tf.py#L82"}}),Je=new ne({}),Ke=new ne({}),Qe=new T({props:{name:"class transformers.SchedulerType",anchor:"transformers.SchedulerType",parameters:[{name:"value",val:""},{name:"names",val:" = None"},{name:"module",val:" = None"},{name:"qualname",val:" = None"},{name:"type",val:" = None"},{name:"start",val:" = 1"}],source:"https://github.com/huggingface/transformers/blob/pr_16919/src/transformers/trainer_utils.py#L301"}}),Xe=new T({props:{name:"transformers.get_scheduler",anchor:"transformers.get_scheduler",parameters:[{name:"name",val:": typing.Union[str, transformers.trainer_utils.SchedulerType]"},{name:"optimizer",val:": Optimizer"},{name:"num_warmup_steps",val:": typing.Optional[int] = None"},{name:"num_training_steps",val:": typing.Optional[int] = None"}],parametersDescription:[{anchor:"transformers.get_scheduler.name",description:`<strong>name</strong> (<code>str</code> or <code>SchedulerType</code>) &#x2014;
The name of the scheduler to use.`,name:"name"},{anchor:"transformers.get_scheduler.optimizer",description:`<strong>optimizer</strong> (<code>torch.optim.Optimizer</code>) &#x2014;
The optimizer that will be used during training.`,name:"optimizer"},{anchor:"transformers.get_scheduler.num_warmup_steps",description:`<strong>num_warmup_steps</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The number of warmup steps to do. This is not required by all schedulers (hence the argument being
optional), the function will raise an error if it&#x2019;s unset and the scheduler type requires it.`,name:"num_warmup_steps"},{anchor:"transformers.get_scheduler.num_training_steps",description:`<strong>num_training_steps</strong> (\`int&#x201C;, <em>optional</em>) &#x2014;
The number of training steps to do. This is not required by all schedulers (hence the argument being
optional), the function will raise an error if it&#x2019;s unset and the scheduler type requires it.`,name:"num_training_steps"}],source:"https://github.com/huggingface/transformers/blob/pr_16919/src/transformers/optimization.py#L233"}}),Ye=new T({props:{name:"transformers.get_constant_schedule",anchor:"transformers.get_constant_schedule",parameters:[{name:"optimizer",val:": Optimizer"},{name:"last_epoch",val:": int = -1"}],parametersDescription:[{anchor:"transformers.get_constant_schedule.optimizer",description:`<strong>optimizer</strong> (<code>Optimizer</code>) &#x2014;
The optimizer for which to schedule the learning rate.`,name:"optimizer"},{anchor:"transformers.get_constant_schedule.last_epoch",description:`<strong>last_epoch</strong> (<code>int</code>, <em>optional</em>, defaults to -1) &#x2014;
The index of the last epoch when resuming training.`,name:"last_epoch"}],source:"https://github.com/huggingface/transformers/blob/pr_16919/src/transformers/optimization.py#L34",returnDescription:`
<p><code>torch.optim.lr_scheduler.LambdaLR</code> with the appropriate schedule.</p>
`}}),Ze=new T({props:{name:"transformers.get_constant_schedule_with_warmup",anchor:"transformers.get_constant_schedule_with_warmup",parameters:[{name:"optimizer",val:": Optimizer"},{name:"num_warmup_steps",val:": int"},{name:"last_epoch",val:": int = -1"}],parametersDescription:[{anchor:"transformers.get_constant_schedule_with_warmup.optimizer",description:`<strong>optimizer</strong> (<code>Optimizer</code>) &#x2014;
The optimizer for which to schedule the learning rate.`,name:"optimizer"},{anchor:"transformers.get_constant_schedule_with_warmup.num_warmup_steps",description:`<strong>num_warmup_steps</strong> (<code>int</code>) &#x2014;
The number of steps for the warmup phase.`,name:"num_warmup_steps"},{anchor:"transformers.get_constant_schedule_with_warmup.last_epoch",description:`<strong>last_epoch</strong> (<code>int</code>, <em>optional</em>, defaults to -1) &#x2014;
The index of the last epoch when resuming training.`,name:"last_epoch"}],source:"https://github.com/huggingface/transformers/blob/pr_16919/src/transformers/optimization.py#L50",returnDescription:`
<p><code>torch.optim.lr_scheduler.LambdaLR</code> with the appropriate schedule.</p>
`}}),tt=new T({props:{name:"transformers.get_cosine_schedule_with_warmup",anchor:"transformers.get_cosine_schedule_with_warmup",parameters:[{name:"optimizer",val:": Optimizer"},{name:"num_warmup_steps",val:": int"},{name:"num_training_steps",val:": int"},{name:"num_cycles",val:": float = 0.5"},{name:"last_epoch",val:": int = -1"}],parametersDescription:[{anchor:"transformers.get_cosine_schedule_with_warmup.optimizer",description:`<strong>optimizer</strong> (<code>Optimizer</code>) &#x2014;
The optimizer for which to schedule the learning rate.`,name:"optimizer"},{anchor:"transformers.get_cosine_schedule_with_warmup.num_warmup_steps",description:`<strong>num_warmup_steps</strong> (<code>int</code>) &#x2014;
The number of steps for the warmup phase.`,name:"num_warmup_steps"},{anchor:"transformers.get_cosine_schedule_with_warmup.num_training_steps",description:`<strong>num_training_steps</strong> (<code>int</code>) &#x2014;
The total number of training steps.`,name:"num_training_steps"},{anchor:"transformers.get_cosine_schedule_with_warmup.num_cycles",description:`<strong>num_cycles</strong> (<code>float</code>, <em>optional</em>, defaults to 0.5) &#x2014;
The number of waves in the cosine schedule (the defaults is to just decrease from the max value to 0
following a half-cosine).`,name:"num_cycles"},{anchor:"transformers.get_cosine_schedule_with_warmup.last_epoch",description:`<strong>last_epoch</strong> (<code>int</code>, <em>optional</em>, defaults to -1) &#x2014;
The index of the last epoch when resuming training.`,name:"last_epoch"}],source:"https://github.com/huggingface/transformers/blob/pr_16919/src/transformers/optimization.py#L104",returnDescription:`
<p><code>torch.optim.lr_scheduler.LambdaLR</code> with the appropriate schedule.</p>
`}}),at=new T({props:{name:"transformers.get_cosine_with_hard_restarts_schedule_with_warmup",anchor:"transformers.get_cosine_with_hard_restarts_schedule_with_warmup",parameters:[{name:"optimizer",val:": Optimizer"},{name:"num_warmup_steps",val:": int"},{name:"num_training_steps",val:": int"},{name:"num_cycles",val:": int = 1"},{name:"last_epoch",val:": int = -1"}],parametersDescription:[{anchor:"transformers.get_cosine_with_hard_restarts_schedule_with_warmup.optimizer",description:`<strong>optimizer</strong> (<code>Optimizer</code>) &#x2014;
The optimizer for which to schedule the learning rate.`,name:"optimizer"},{anchor:"transformers.get_cosine_with_hard_restarts_schedule_with_warmup.num_warmup_steps",description:`<strong>num_warmup_steps</strong> (<code>int</code>) &#x2014;
The number of steps for the warmup phase.`,name:"num_warmup_steps"},{anchor:"transformers.get_cosine_with_hard_restarts_schedule_with_warmup.num_training_steps",description:`<strong>num_training_steps</strong> (<code>int</code>) &#x2014;
The total number of training steps.`,name:"num_training_steps"},{anchor:"transformers.get_cosine_with_hard_restarts_schedule_with_warmup.num_cycles",description:`<strong>num_cycles</strong> (<code>int</code>, <em>optional</em>, defaults to 1) &#x2014;
The number of hard restarts to use.`,name:"num_cycles"},{anchor:"transformers.get_cosine_with_hard_restarts_schedule_with_warmup.last_epoch",description:`<strong>last_epoch</strong> (<code>int</code>, <em>optional</em>, defaults to -1) &#x2014;
The index of the last epoch when resuming training.`,name:"last_epoch"}],source:"https://github.com/huggingface/transformers/blob/pr_16919/src/transformers/optimization.py#L138",returnDescription:`
<p><code>torch.optim.lr_scheduler.LambdaLR</code> with the appropriate schedule.</p>
`}}),ot=new T({props:{name:"transformers.get_linear_schedule_with_warmup",anchor:"transformers.get_linear_schedule_with_warmup",parameters:[{name:"optimizer",val:""},{name:"num_warmup_steps",val:""},{name:"num_training_steps",val:""},{name:"last_epoch",val:" = -1"}],parametersDescription:[{anchor:"transformers.get_linear_schedule_with_warmup.optimizer",description:`<strong>optimizer</strong> (<code>Optimizer</code>) &#x2014;
The optimizer for which to schedule the learning rate.`,name:"optimizer"},{anchor:"transformers.get_linear_schedule_with_warmup.num_warmup_steps",description:`<strong>num_warmup_steps</strong> (<code>int</code>) &#x2014;
The number of steps for the warmup phase.`,name:"num_warmup_steps"},{anchor:"transformers.get_linear_schedule_with_warmup.num_training_steps",description:`<strong>num_training_steps</strong> (<code>int</code>) &#x2014;
The total number of training steps.`,name:"num_training_steps"},{anchor:"transformers.get_linear_schedule_with_warmup.last_epoch",description:`<strong>last_epoch</strong> (<code>int</code>, <em>optional</em>, defaults to -1) &#x2014;
The index of the last epoch when resuming training.`,name:"last_epoch"}],source:"https://github.com/huggingface/transformers/blob/pr_16919/src/transformers/optimization.py#L75",returnDescription:`
<p><code>torch.optim.lr_scheduler.LambdaLR</code> with the appropriate schedule.</p>
`}}),it=new T({props:{name:"transformers.get_polynomial_decay_schedule_with_warmup",anchor:"transformers.get_polynomial_decay_schedule_with_warmup",parameters:[{name:"optimizer",val:""},{name:"num_warmup_steps",val:""},{name:"num_training_steps",val:""},{name:"lr_end",val:" = 1e-07"},{name:"power",val:" = 1.0"},{name:"last_epoch",val:" = -1"}],parametersDescription:[{anchor:"transformers.get_polynomial_decay_schedule_with_warmup.optimizer",description:`<strong>optimizer</strong> (<code>Optimizer</code>) &#x2014;
The optimizer for which to schedule the learning rate.`,name:"optimizer"},{anchor:"transformers.get_polynomial_decay_schedule_with_warmup.num_warmup_steps",description:`<strong>num_warmup_steps</strong> (<code>int</code>) &#x2014;
The number of steps for the warmup phase.`,name:"num_warmup_steps"},{anchor:"transformers.get_polynomial_decay_schedule_with_warmup.num_training_steps",description:`<strong>num_training_steps</strong> (<code>int</code>) &#x2014;
The total number of training steps.`,name:"num_training_steps"},{anchor:"transformers.get_polynomial_decay_schedule_with_warmup.lr_end",description:`<strong>lr_end</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-7) &#x2014;
The end LR.`,name:"lr_end"},{anchor:"transformers.get_polynomial_decay_schedule_with_warmup.power",description:`<strong>power</strong> (<code>float</code>, <em>optional</em>, defaults to 1.0) &#x2014;
Power factor.`,name:"power"},{anchor:"transformers.get_polynomial_decay_schedule_with_warmup.last_epoch",description:`<strong>last_epoch</strong> (<code>int</code>, <em>optional</em>, defaults to -1) &#x2014;
The index of the last epoch when resuming training.`,name:"last_epoch"}],source:"https://github.com/huggingface/transformers/blob/pr_16919/src/transformers/optimization.py#L173",returnDescription:`
<p><code>torch.optim.lr_scheduler.LambdaLR</code> with the appropriate schedule.</p>
`}}),mt=new ne({}),pt=new T({props:{name:"class transformers.WarmUp",anchor:"transformers.WarmUp",parameters:[{name:"initial_learning_rate",val:": float"},{name:"decay_schedule_fn",val:": typing.Callable"},{name:"warmup_steps",val:": int"},{name:"power",val:": float = 1.0"},{name:"name",val:": str = None"}],parametersDescription:[{anchor:"transformers.WarmUp.initial_learning_rate",description:`<strong>initial_learning_rate</strong> (<code>float</code>) &#x2014;
The initial learning rate for the schedule after the warmup (so this will be the learning rate at the end
of the warmup).`,name:"initial_learning_rate"},{anchor:"transformers.WarmUp.decay_schedule_fn",description:`<strong>decay_schedule_fn</strong> (<code>Callable</code>) &#x2014;
The schedule function to apply after the warmup for the rest of training.`,name:"decay_schedule_fn"},{anchor:"transformers.WarmUp.warmup_steps",description:`<strong>warmup_steps</strong> (<code>int</code>) &#x2014;
The number of steps for the warmup part of training.`,name:"warmup_steps"},{anchor:"transformers.WarmUp.power",description:`<strong>power</strong> (<code>float</code>, <em>optional</em>, defaults to 1) &#x2014;
The power to use for the polynomial warmup (defaults is a linear warmup).`,name:"power"},{anchor:"transformers.WarmUp.name",description:`<strong>name</strong> (<code>str</code>, <em>optional</em>) &#x2014;
Optional name prefix for the returned tensors during the schedule.`,name:"name"}],source:"https://github.com/huggingface/transformers/blob/pr_16919/src/transformers/optimization_tf.py#L24"}}),dt=new ne({}),ht=new ne({}),ut=new T({props:{name:"class transformers.GradientAccumulator",anchor:"transformers.GradientAccumulator",parameters:[],source:"https://github.com/huggingface/transformers/blob/pr_16919/src/transformers/optimization_tf.py#L282"}}),ft=new T({props:{name:"reset",anchor:"transformers.GradientAccumulator.reset",parameters:[],source:"https://github.com/huggingface/transformers/blob/pr_16919/src/transformers/optimization_tf.py#L344"}}),{c(){u=a("meta"),A=l(),h=a("h1"),z=a("a"),F=a("span"),f(x.$$.fragment),C=l(),zt=a("span"),Ua=s("Optimization"),Ir=l(),oe=a("p"),Ga=s("The "),Et=a("code"),Va=s(".optimization"),Ma=s(" module provides:"),Nr=l(),O=a("ul"),Tt=a("li"),Ha=s("an optimizer with weight decay fixed that can be used to fine-tuned models, and"),Ba=l(),Ee=a("li"),Ja=s("several schedules in the form of schedule objects that inherit from "),Dt=a("code"),Ka=s("_LRSchedule"),Qa=s(":"),Xa=l(),Lt=a("li"),Ya=s("a gradient accumulation class to accumulate the gradients of multiple batches"),Fr=l(),R=a("h2"),se=a("a"),Pt=a("span"),f(Te.$$.fragment),Za=l(),Wt=a("span"),en=s("AdamW (PyTorch)"),Cr=l(),P=a("div"),f(De.$$.fragment),tn=l(),Le=a("p"),rn=s("Implements Adam algorithm with weight decay fix as introduced in "),Pe=a("a"),an=s(`Decoupled Weight Decay
Regularization`),nn=s("."),on=l(),ie=a("div"),f(We.$$.fragment),sn=l(),kt=a("p"),ln=s("Performs a single optimization step."),Or=l(),j=a("h2"),le=a("a"),St=a("span"),f(ke.$$.fragment),cn=l(),It=a("span"),mn=s("AdaFactor (PyTorch)"),Rr=l(),y=a("div"),f(Se.$$.fragment),pn=l(),_t=a("p"),dn=s(`AdaFactor pytorch implementation can be used as a drop in replacement for Adam original fairseq code:
`),Ie=a("a"),hn=s("https://github.com/pytorch/fairseq/blob/master/fairseq/optim/adafactor.py"),un=l(),E=a("p"),fn=s("Paper: "),Nt=a("em"),gn=s("Adafactor: Adaptive Learning Rates with Sublinear Memory Cost"),_n=l(),Ne=a("a"),wn=s("https://arxiv.org/abs/1804.04235"),vn=s(` Note that
this optimizer internally adjusts the learning rate depending on the `),Ft=a("code"),bn=s("scale_parameter"),yn=s(", "),Ct=a("code"),$n=s("relative_step"),An=s(` and
`),Ot=a("code"),xn=s("warmup_init"),zn=s(" options. To use a manual (external) learning rate schedule you should set "),Rt=a("code"),En=s("scale_parameter=False"),Tn=s(` and
`),jt=a("code"),Dn=s("relative_step=False"),Ln=s("."),Pn=l(),qt=a("p"),Wn=s("This implementation handles low-precision (FP16, bfloat) values, but we have not thoroughly tested."),kn=l(),Fe=a("p"),Sn=s("Recommended T5 finetuning settings ("),Ce=a("a"),In=s("https://discuss.huggingface.co/t/t5-finetuning-tips/684/3"),Nn=s("):"),Fn=l(),W=a("ul"),Oe=a("li"),Ut=a("p"),Cn=s("Training without LR warmup or clip_threshold is not recommended."),On=l(),Re=a("ul"),Gt=a("li"),Rn=s("use scheduled LR warm-up to fixed LR"),jn=l(),je=a("li"),qn=s("use clip_threshold=1.0 ("),qe=a("a"),Un=s("https://arxiv.org/abs/1804.04235"),Gn=s(")"),Vn=l(),Vt=a("li"),Mt=a("p"),Mn=s("Disable relative updates"),Hn=l(),Ht=a("li"),Bt=a("p"),Bn=s("Use scale_parameter=False"),Jn=l(),Jt=a("li"),Kt=a("p"),Kn=s("Additional optimizer operations like gradient clipping should not be used alongside Adafactor"),Qn=l(),f(ce.$$.fragment),Xn=l(),Qt=a("p"),Yn=s("Others reported the following combination to work well:"),Zn=l(),f(me.$$.fragment),eo=l(),k=a("p"),to=s("When using "),Xt=a("code"),ro=s("lr=None"),ao=s(" with "),wt=a("a"),no=s("Trainer"),oo=s(" you will most likely need to use "),Yt=a("code"),so=s("AdafactorSchedule"),io=s(`
scheduler as following:`),lo=l(),f(pe.$$.fragment),co=l(),Zt=a("p"),mo=s("Usage:"),po=l(),f(de.$$.fragment),ho=l(),he=a("div"),f(Ue.$$.fragment),uo=l(),er=a("p"),fo=s("Performs a single optimization step"),jr=l(),q=a("h2"),ue=a("a"),tr=a("span"),f(Ge.$$.fragment),go=l(),rr=a("span"),_o=s("AdamWeightDecay (TensorFlow)"),qr=l(),L=a("div"),f(Ve.$$.fragment),wo=l(),U=a("p"),vo=s(`Adam enables L2 weight decay and clip_by_global_norm on gradients. Just adding the square of the weights to the
loss function is `),ar=a("em"),bo=s("not"),yo=s(` the correct way of using L2 regularization/weight decay with Adam, since that will interact
with the m and v parameters in strange ways as shown in `),Me=a("a"),$o=s(`Decoupled Weight Decay
Regularization`),Ao=s("."),xo=l(),nr=a("p"),zo=s(`Instead we want ot decay the weights in a manner that doesn\u2019t interact with the m/v parameters. This is equivalent
to adding the square of the weights to the loss with plain (non-momentum) SGD.`),Eo=l(),fe=a("div"),f(He.$$.fragment),To=l(),or=a("p"),Do=s("Creates an optimizer from its config with WarmUp custom object."),Ur=l(),G=a("div"),f(Be.$$.fragment),Lo=l(),sr=a("p"),Po=s("Creates an optimizer with a learning rate schedule using a warmup phase followed by a linear decay."),Gr=l(),V=a("h2"),ge=a("a"),ir=a("span"),f(Je.$$.fragment),Wo=l(),lr=a("span"),ko=s("Schedules"),Vr=l(),M=a("h3"),_e=a("a"),cr=a("span"),f(Ke.$$.fragment),So=l(),mr=a("span"),Io=s("Learning Rate Schedules (Pytorch)"),Mr=l(),H=a("div"),f(Qe.$$.fragment),No=l(),pr=a("p"),Fo=s("An enumeration."),Hr=l(),B=a("div"),f(Xe.$$.fragment),Co=l(),dr=a("p"),Oo=s("Unified API to get any scheduler from its name."),Br=l(),J=a("div"),f(Ye.$$.fragment),Ro=l(),hr=a("p"),jo=s("Create a schedule with a constant learning rate, using the learning rate set in optimizer."),Jr=l(),K=a("div"),f(Ze.$$.fragment),qo=l(),ur=a("p"),Uo=s(`Create a schedule with a constant learning rate preceded by a warmup period during which the learning rate
increases linearly between 0 and the initial lr set in the optimizer.`),Kr=l(),et=a("img"),Qr=l(),Q=a("div"),f(tt.$$.fragment),Go=l(),fr=a("p"),Vo=s(`Create a schedule with a learning rate that decreases following the values of the cosine function between the
initial lr set in the optimizer to 0, after a warmup period during which it increases linearly between 0 and the
initial lr set in the optimizer.`),Xr=l(),rt=a("img"),Yr=l(),X=a("div"),f(at.$$.fragment),Mo=l(),gr=a("p"),Ho=s(`Create a schedule with a learning rate that decreases following the values of the cosine function between the
initial lr set in the optimizer to 0, with several hard restarts, after a warmup period during which it increases
linearly between 0 and the initial lr set in the optimizer.`),Zr=l(),nt=a("img"),ea=l(),Y=a("div"),f(ot.$$.fragment),Bo=l(),_r=a("p"),Jo=s(`Create a schedule with a learning rate that decreases linearly from the initial lr set in the optimizer to 0, after
a warmup period during which it increases linearly from 0 to the initial lr set in the optimizer.`),ta=l(),st=a("img"),ra=l(),S=a("div"),f(it.$$.fragment),Ko=l(),lt=a("p"),Qo=s(`Create a schedule with a learning rate that decreases as a polynomial decay from the initial lr set in the
optimizer to end lr defined by `),wr=a("em"),Xo=s("lr_end"),Yo=s(`, after a warmup period during which it increases linearly from 0 to the
initial lr set in the optimizer.`),Zo=l(),we=a("p"),es=s("Note: "),vr=a("em"),ts=s("power"),rs=s(` defaults to 1.0 as in the fairseq implementation, which in turn is based on the original BERT
implementation at
`),ct=a("a"),as=s("https://github.com/google-research/bert/blob/f39e881b169b9d53bea03d2d341b31707a6c052b/optimization.py#L37"),aa=l(),Z=a("h3"),ve=a("a"),br=a("span"),f(mt.$$.fragment),ns=l(),yr=a("span"),os=s("Warmup (TensorFlow)"),na=l(),ee=a("div"),f(pt.$$.fragment),ss=l(),$r=a("p"),is=s("Applies a warmup schedule on a given learning rate decay schedule."),oa=l(),te=a("h2"),be=a("a"),Ar=a("span"),f(dt.$$.fragment),ls=l(),xr=a("span"),cs=s("Gradient Strategies"),sa=l(),re=a("h3"),ye=a("a"),zr=a("span"),f(ht.$$.fragment),ms=l(),Er=a("span"),ps=s("GradientAccumulator (TensorFlow)"),ia=l(),I=a("div"),f(ut.$$.fragment),ds=l(),ae=a("p"),hs=s(`Gradient accumulation utility. When used with a distribution strategy, the accumulator should be called in a
replica context. Gradients will be accumulated locally on each replica and without synchronization. Users should
then call `),Tr=a("code"),us=s(".gradients"),fs=s(", scale the gradients if required, and pass the result to "),Dr=a("code"),gs=s("apply_gradients"),_s=s("."),ws=l(),$e=a("div"),f(ft.$$.fragment),vs=l(),Lr=a("p"),bs=s("Resets the accumulated gradients on the current replica."),this.h()},l(e){const p=Ki('[data-svelte="svelte-1phssyn"]',document.head);u=n(p,"META",{name:!0,content:!0}),p.forEach(r),A=c(e),h=n(e,"H1",{class:!0});var gt=o(h);z=n(gt,"A",{id:!0,class:!0,href:!0});var Pr=o(z);F=n(Pr,"SPAN",{});var Wr=o(F);g(x.$$.fragment,Wr),Wr.forEach(r),Pr.forEach(r),C=c(gt),zt=n(gt,"SPAN",{});var kr=o(zt);Ua=i(kr,"Optimization"),kr.forEach(r),gt.forEach(r),Ir=c(e),oe=n(e,"P",{});var ca=o(oe);Ga=i(ca,"The "),Et=n(ca,"CODE",{});var Es=o(Et);Va=i(Es,".optimization"),Es.forEach(r),Ma=i(ca," module provides:"),ca.forEach(r),Nr=c(e),O=n(e,"UL",{});var vt=o(O);Tt=n(vt,"LI",{});var Ts=o(Tt);Ha=i(Ts,"an optimizer with weight decay fixed that can be used to fine-tuned models, and"),Ts.forEach(r),Ba=c(vt),Ee=n(vt,"LI",{});var ma=o(Ee);Ja=i(ma,"several schedules in the form of schedule objects that inherit from "),Dt=n(ma,"CODE",{});var Ds=o(Dt);Ka=i(Ds,"_LRSchedule"),Ds.forEach(r),Qa=i(ma,":"),ma.forEach(r),Xa=c(vt),Lt=n(vt,"LI",{});var Ls=o(Lt);Ya=i(Ls,"a gradient accumulation class to accumulate the gradients of multiple batches"),Ls.forEach(r),vt.forEach(r),Fr=c(e),R=n(e,"H2",{class:!0});var pa=o(R);se=n(pa,"A",{id:!0,class:!0,href:!0});var Ps=o(se);Pt=n(Ps,"SPAN",{});var Ws=o(Pt);g(Te.$$.fragment,Ws),Ws.forEach(r),Ps.forEach(r),Za=c(pa),Wt=n(pa,"SPAN",{});var ks=o(Wt);en=i(ks,"AdamW (PyTorch)"),ks.forEach(r),pa.forEach(r),Cr=c(e),P=n(e,"DIV",{class:!0});var bt=o(P);g(De.$$.fragment,bt),tn=c(bt),Le=n(bt,"P",{});var da=o(Le);rn=i(da,"Implements Adam algorithm with weight decay fix as introduced in "),Pe=n(da,"A",{href:!0,rel:!0});var Ss=o(Pe);an=i(Ss,`Decoupled Weight Decay
Regularization`),Ss.forEach(r),nn=i(da,"."),da.forEach(r),on=c(bt),ie=n(bt,"DIV",{class:!0});var ha=o(ie);g(We.$$.fragment,ha),sn=c(ha),kt=n(ha,"P",{});var Is=o(kt);ln=i(Is,"Performs a single optimization step."),Is.forEach(r),ha.forEach(r),bt.forEach(r),Or=c(e),j=n(e,"H2",{class:!0});var ua=o(j);le=n(ua,"A",{id:!0,class:!0,href:!0});var Ns=o(le);St=n(Ns,"SPAN",{});var Fs=o(St);g(ke.$$.fragment,Fs),Fs.forEach(r),Ns.forEach(r),cn=c(ua),It=n(ua,"SPAN",{});var Cs=o(It);mn=i(Cs,"AdaFactor (PyTorch)"),Cs.forEach(r),ua.forEach(r),Rr=c(e),y=n(e,"DIV",{class:!0});var $=o(y);g(Se.$$.fragment,$),pn=c($),_t=n($,"P",{});var ys=o(_t);dn=i(ys,`AdaFactor pytorch implementation can be used as a drop in replacement for Adam original fairseq code:
`),Ie=n(ys,"A",{href:!0,rel:!0});var Os=o(Ie);hn=i(Os,"https://github.com/pytorch/fairseq/blob/master/fairseq/optim/adafactor.py"),Os.forEach(r),ys.forEach(r),un=c($),E=n($,"P",{});var D=o(E);fn=i(D,"Paper: "),Nt=n(D,"EM",{});var Rs=o(Nt);gn=i(Rs,"Adafactor: Adaptive Learning Rates with Sublinear Memory Cost"),Rs.forEach(r),_n=c(D),Ne=n(D,"A",{href:!0,rel:!0});var js=o(Ne);wn=i(js,"https://arxiv.org/abs/1804.04235"),js.forEach(r),vn=i(D,` Note that
this optimizer internally adjusts the learning rate depending on the `),Ft=n(D,"CODE",{});var qs=o(Ft);bn=i(qs,"scale_parameter"),qs.forEach(r),yn=i(D,", "),Ct=n(D,"CODE",{});var Us=o(Ct);$n=i(Us,"relative_step"),Us.forEach(r),An=i(D,` and
`),Ot=n(D,"CODE",{});var Gs=o(Ot);xn=i(Gs,"warmup_init"),Gs.forEach(r),zn=i(D," options. To use a manual (external) learning rate schedule you should set "),Rt=n(D,"CODE",{});var Vs=o(Rt);En=i(Vs,"scale_parameter=False"),Vs.forEach(r),Tn=i(D,` and
`),jt=n(D,"CODE",{});var Ms=o(jt);Dn=i(Ms,"relative_step=False"),Ms.forEach(r),Ln=i(D,"."),D.forEach(r),Pn=c($),qt=n($,"P",{});var Hs=o(qt);Wn=i(Hs,"This implementation handles low-precision (FP16, bfloat) values, but we have not thoroughly tested."),Hs.forEach(r),kn=c($),Fe=n($,"P",{});var fa=o(Fe);Sn=i(fa,"Recommended T5 finetuning settings ("),Ce=n(fa,"A",{href:!0,rel:!0});var Bs=o(Ce);In=i(Bs,"https://discuss.huggingface.co/t/t5-finetuning-tips/684/3"),Bs.forEach(r),Nn=i(fa,"):"),fa.forEach(r),Fn=c($),W=n($,"UL",{});var Ae=o(W);Oe=n(Ae,"LI",{});var ga=o(Oe);Ut=n(ga,"P",{});var Js=o(Ut);Cn=i(Js,"Training without LR warmup or clip_threshold is not recommended."),Js.forEach(r),On=c(ga),Re=n(ga,"UL",{});var _a=o(Re);Gt=n(_a,"LI",{});var Ks=o(Gt);Rn=i(Ks,"use scheduled LR warm-up to fixed LR"),Ks.forEach(r),jn=c(_a),je=n(_a,"LI",{});var wa=o(je);qn=i(wa,"use clip_threshold=1.0 ("),qe=n(wa,"A",{href:!0,rel:!0});var Qs=o(qe);Un=i(Qs,"https://arxiv.org/abs/1804.04235"),Qs.forEach(r),Gn=i(wa,")"),wa.forEach(r),_a.forEach(r),ga.forEach(r),Vn=c(Ae),Vt=n(Ae,"LI",{});var Xs=o(Vt);Mt=n(Xs,"P",{});var Ys=o(Mt);Mn=i(Ys,"Disable relative updates"),Ys.forEach(r),Xs.forEach(r),Hn=c(Ae),Ht=n(Ae,"LI",{});var Zs=o(Ht);Bt=n(Zs,"P",{});var ei=o(Bt);Bn=i(ei,"Use scale_parameter=False"),ei.forEach(r),Zs.forEach(r),Jn=c(Ae),Jt=n(Ae,"LI",{});var ti=o(Jt);Kt=n(ti,"P",{});var ri=o(Kt);Kn=i(ri,"Additional optimizer operations like gradient clipping should not be used alongside Adafactor"),ri.forEach(r),ti.forEach(r),Ae.forEach(r),Qn=c($),g(ce.$$.fragment,$),Xn=c($),Qt=n($,"P",{});var ai=o(Qt);Yn=i(ai,"Others reported the following combination to work well:"),ai.forEach(r),Zn=c($),g(me.$$.fragment,$),eo=c($),k=n($,"P",{});var xe=o(k);to=i(xe,"When using "),Xt=n(xe,"CODE",{});var ni=o(Xt);ro=i(ni,"lr=None"),ni.forEach(r),ao=i(xe," with "),wt=n(xe,"A",{href:!0});var oi=o(wt);no=i(oi,"Trainer"),oi.forEach(r),oo=i(xe," you will most likely need to use "),Yt=n(xe,"CODE",{});var si=o(Yt);so=i(si,"AdafactorSchedule"),si.forEach(r),io=i(xe,`
scheduler as following:`),xe.forEach(r),lo=c($),g(pe.$$.fragment,$),co=c($),Zt=n($,"P",{});var ii=o(Zt);mo=i(ii,"Usage:"),ii.forEach(r),po=c($),g(de.$$.fragment,$),ho=c($),he=n($,"DIV",{class:!0});var va=o(he);g(Ue.$$.fragment,va),uo=c(va),er=n(va,"P",{});var li=o(er);fo=i(li,"Performs a single optimization step"),li.forEach(r),va.forEach(r),$.forEach(r),jr=c(e),q=n(e,"H2",{class:!0});var ba=o(q);ue=n(ba,"A",{id:!0,class:!0,href:!0});var ci=o(ue);tr=n(ci,"SPAN",{});var mi=o(tr);g(Ge.$$.fragment,mi),mi.forEach(r),ci.forEach(r),go=c(ba),rr=n(ba,"SPAN",{});var pi=o(rr);_o=i(pi,"AdamWeightDecay (TensorFlow)"),pi.forEach(r),ba.forEach(r),qr=c(e),L=n(e,"DIV",{class:!0});var ze=o(L);g(Ve.$$.fragment,ze),wo=c(ze),U=n(ze,"P",{});var yt=o(U);vo=i(yt,`Adam enables L2 weight decay and clip_by_global_norm on gradients. Just adding the square of the weights to the
loss function is `),ar=n(yt,"EM",{});var di=o(ar);bo=i(di,"not"),di.forEach(r),yo=i(yt,` the correct way of using L2 regularization/weight decay with Adam, since that will interact
with the m and v parameters in strange ways as shown in `),Me=n(yt,"A",{href:!0,rel:!0});var hi=o(Me);$o=i(hi,`Decoupled Weight Decay
Regularization`),hi.forEach(r),Ao=i(yt,"."),yt.forEach(r),xo=c(ze),nr=n(ze,"P",{});var ui=o(nr);zo=i(ui,`Instead we want ot decay the weights in a manner that doesn\u2019t interact with the m/v parameters. This is equivalent
to adding the square of the weights to the loss with plain (non-momentum) SGD.`),ui.forEach(r),Eo=c(ze),fe=n(ze,"DIV",{class:!0});var ya=o(fe);g(He.$$.fragment,ya),To=c(ya),or=n(ya,"P",{});var fi=o(or);Do=i(fi,"Creates an optimizer from its config with WarmUp custom object."),fi.forEach(r),ya.forEach(r),ze.forEach(r),Ur=c(e),G=n(e,"DIV",{class:!0});var $a=o(G);g(Be.$$.fragment,$a),Lo=c($a),sr=n($a,"P",{});var gi=o(sr);Po=i(gi,"Creates an optimizer with a learning rate schedule using a warmup phase followed by a linear decay."),gi.forEach(r),$a.forEach(r),Gr=c(e),V=n(e,"H2",{class:!0});var Aa=o(V);ge=n(Aa,"A",{id:!0,class:!0,href:!0});var _i=o(ge);ir=n(_i,"SPAN",{});var wi=o(ir);g(Je.$$.fragment,wi),wi.forEach(r),_i.forEach(r),Wo=c(Aa),lr=n(Aa,"SPAN",{});var vi=o(lr);ko=i(vi,"Schedules"),vi.forEach(r),Aa.forEach(r),Vr=c(e),M=n(e,"H3",{class:!0});var xa=o(M);_e=n(xa,"A",{id:!0,class:!0,href:!0});var bi=o(_e);cr=n(bi,"SPAN",{});var yi=o(cr);g(Ke.$$.fragment,yi),yi.forEach(r),bi.forEach(r),So=c(xa),mr=n(xa,"SPAN",{});var $i=o(mr);Io=i($i,"Learning Rate Schedules (Pytorch)"),$i.forEach(r),xa.forEach(r),Mr=c(e),H=n(e,"DIV",{class:!0});var za=o(H);g(Qe.$$.fragment,za),No=c(za),pr=n(za,"P",{});var Ai=o(pr);Fo=i(Ai,"An enumeration."),Ai.forEach(r),za.forEach(r),Hr=c(e),B=n(e,"DIV",{class:!0});var Ea=o(B);g(Xe.$$.fragment,Ea),Co=c(Ea),dr=n(Ea,"P",{});var xi=o(dr);Oo=i(xi,"Unified API to get any scheduler from its name."),xi.forEach(r),Ea.forEach(r),Br=c(e),J=n(e,"DIV",{class:!0});var Ta=o(J);g(Ye.$$.fragment,Ta),Ro=c(Ta),hr=n(Ta,"P",{});var zi=o(hr);jo=i(zi,"Create a schedule with a constant learning rate, using the learning rate set in optimizer."),zi.forEach(r),Ta.forEach(r),Jr=c(e),K=n(e,"DIV",{class:!0});var Da=o(K);g(Ze.$$.fragment,Da),qo=c(Da),ur=n(Da,"P",{});var Ei=o(ur);Uo=i(Ei,`Create a schedule with a constant learning rate preceded by a warmup period during which the learning rate
increases linearly between 0 and the initial lr set in the optimizer.`),Ei.forEach(r),Da.forEach(r),Kr=c(e),et=n(e,"IMG",{alt:!0,src:!0}),Qr=c(e),Q=n(e,"DIV",{class:!0});var La=o(Q);g(tt.$$.fragment,La),Go=c(La),fr=n(La,"P",{});var Ti=o(fr);Vo=i(Ti,`Create a schedule with a learning rate that decreases following the values of the cosine function between the
initial lr set in the optimizer to 0, after a warmup period during which it increases linearly between 0 and the
initial lr set in the optimizer.`),Ti.forEach(r),La.forEach(r),Xr=c(e),rt=n(e,"IMG",{alt:!0,src:!0}),Yr=c(e),X=n(e,"DIV",{class:!0});var Pa=o(X);g(at.$$.fragment,Pa),Mo=c(Pa),gr=n(Pa,"P",{});var Di=o(gr);Ho=i(Di,`Create a schedule with a learning rate that decreases following the values of the cosine function between the
initial lr set in the optimizer to 0, with several hard restarts, after a warmup period during which it increases
linearly between 0 and the initial lr set in the optimizer.`),Di.forEach(r),Pa.forEach(r),Zr=c(e),nt=n(e,"IMG",{alt:!0,src:!0}),ea=c(e),Y=n(e,"DIV",{class:!0});var Wa=o(Y);g(ot.$$.fragment,Wa),Bo=c(Wa),_r=n(Wa,"P",{});var Li=o(_r);Jo=i(Li,`Create a schedule with a learning rate that decreases linearly from the initial lr set in the optimizer to 0, after
a warmup period during which it increases linearly from 0 to the initial lr set in the optimizer.`),Li.forEach(r),Wa.forEach(r),ta=c(e),st=n(e,"IMG",{alt:!0,src:!0}),ra=c(e),S=n(e,"DIV",{class:!0});var $t=o(S);g(it.$$.fragment,$t),Ko=c($t),lt=n($t,"P",{});var ka=o(lt);Qo=i(ka,`Create a schedule with a learning rate that decreases as a polynomial decay from the initial lr set in the
optimizer to end lr defined by `),wr=n(ka,"EM",{});var Pi=o(wr);Xo=i(Pi,"lr_end"),Pi.forEach(r),Yo=i(ka,`, after a warmup period during which it increases linearly from 0 to the
initial lr set in the optimizer.`),ka.forEach(r),Zo=c($t),we=n($t,"P",{});var Sr=o(we);es=i(Sr,"Note: "),vr=n(Sr,"EM",{});var Wi=o(vr);ts=i(Wi,"power"),Wi.forEach(r),rs=i(Sr,` defaults to 1.0 as in the fairseq implementation, which in turn is based on the original BERT
implementation at
`),ct=n(Sr,"A",{href:!0,rel:!0});var ki=o(ct);as=i(ki,"https://github.com/google-research/bert/blob/f39e881b169b9d53bea03d2d341b31707a6c052b/optimization.py#L37"),ki.forEach(r),Sr.forEach(r),$t.forEach(r),aa=c(e),Z=n(e,"H3",{class:!0});var Sa=o(Z);ve=n(Sa,"A",{id:!0,class:!0,href:!0});var Si=o(ve);br=n(Si,"SPAN",{});var Ii=o(br);g(mt.$$.fragment,Ii),Ii.forEach(r),Si.forEach(r),ns=c(Sa),yr=n(Sa,"SPAN",{});var Ni=o(yr);os=i(Ni,"Warmup (TensorFlow)"),Ni.forEach(r),Sa.forEach(r),na=c(e),ee=n(e,"DIV",{class:!0});var Ia=o(ee);g(pt.$$.fragment,Ia),ss=c(Ia),$r=n(Ia,"P",{});var Fi=o($r);is=i(Fi,"Applies a warmup schedule on a given learning rate decay schedule."),Fi.forEach(r),Ia.forEach(r),oa=c(e),te=n(e,"H2",{class:!0});var Na=o(te);be=n(Na,"A",{id:!0,class:!0,href:!0});var Ci=o(be);Ar=n(Ci,"SPAN",{});var Oi=o(Ar);g(dt.$$.fragment,Oi),Oi.forEach(r),Ci.forEach(r),ls=c(Na),xr=n(Na,"SPAN",{});var Ri=o(xr);cs=i(Ri,"Gradient Strategies"),Ri.forEach(r),Na.forEach(r),sa=c(e),re=n(e,"H3",{class:!0});var Fa=o(re);ye=n(Fa,"A",{id:!0,class:!0,href:!0});var ji=o(ye);zr=n(ji,"SPAN",{});var qi=o(zr);g(ht.$$.fragment,qi),qi.forEach(r),ji.forEach(r),ms=c(Fa),Er=n(Fa,"SPAN",{});var Ui=o(Er);ps=i(Ui,"GradientAccumulator (TensorFlow)"),Ui.forEach(r),Fa.forEach(r),ia=c(e),I=n(e,"DIV",{class:!0});var At=o(I);g(ut.$$.fragment,At),ds=c(At),ae=n(At,"P",{});var xt=o(ae);hs=i(xt,`Gradient accumulation utility. When used with a distribution strategy, the accumulator should be called in a
replica context. Gradients will be accumulated locally on each replica and without synchronization. Users should
then call `),Tr=n(xt,"CODE",{});var Gi=o(Tr);us=i(Gi,".gradients"),Gi.forEach(r),fs=i(xt,", scale the gradients if required, and pass the result to "),Dr=n(xt,"CODE",{});var Vi=o(Dr);gs=i(Vi,"apply_gradients"),Vi.forEach(r),_s=i(xt,"."),xt.forEach(r),ws=c(At),$e=n(At,"DIV",{class:!0});var Ca=o($e);g(ft.$$.fragment,Ca),vs=c(Ca),Lr=n(Ca,"P",{});var Mi=o(Lr);bs=i(Mi,"Resets the accumulated gradients on the current replica."),Mi.forEach(r),Ca.forEach(r),At.forEach(r),this.h()},h(){m(u,"name","hf:doc:metadata"),m(u,"content",JSON.stringify(rl)),m(z,"id","optimization"),m(z,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(z,"href","#optimization"),m(h,"class","relative group"),m(se,"id","transformers.AdamW"),m(se,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(se,"href","#transformers.AdamW"),m(R,"class","relative group"),m(Pe,"href","https://arxiv.org/abs/1711.05101"),m(Pe,"rel","nofollow"),m(ie,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),m(P,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),m(le,"id","transformers.Adafactor"),m(le,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(le,"href","#transformers.Adafactor"),m(j,"class","relative group"),m(Ie,"href","https://github.com/pytorch/fairseq/blob/master/fairseq/optim/adafactor.py"),m(Ie,"rel","nofollow"),m(Ne,"href","https://arxiv.org/abs/1804.04235"),m(Ne,"rel","nofollow"),m(Ce,"href","https://discuss.huggingface.co/t/t5-finetuning-tips/684/3"),m(Ce,"rel","nofollow"),m(qe,"href","https://arxiv.org/abs/1804.04235"),m(qe,"rel","nofollow"),m(wt,"href","/docs/transformers/pr_16919/en/main_classes/trainer#transformers.Trainer"),m(he,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),m(y,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),m(ue,"id","transformers.AdamWeightDecay"),m(ue,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(ue,"href","#transformers.AdamWeightDecay"),m(q,"class","relative group"),m(Me,"href","https://arxiv.org/abs/1711.05101"),m(Me,"rel","nofollow"),m(fe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),m(L,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),m(G,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),m(ge,"id","schedules"),m(ge,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(ge,"href","#schedules"),m(V,"class","relative group"),m(_e,"id","transformers.SchedulerType"),m(_e,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(_e,"href","#transformers.SchedulerType"),m(M,"class","relative group"),m(H,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),m(B,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),m(J,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),m(K,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),m(et,"alt",""),Oa(et.src,$s="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/warmup_constant_schedule.png")||m(et,"src",$s),m(Q,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),m(rt,"alt",""),Oa(rt.src,As="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/warmup_cosine_schedule.png")||m(rt,"src",As),m(X,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),m(nt,"alt",""),Oa(nt.src,xs="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/warmup_cosine_hard_restarts_schedule.png")||m(nt,"src",xs),m(Y,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),m(st,"alt",""),Oa(st.src,zs="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/warmup_linear_schedule.png")||m(st,"src",zs),m(ct,"href","https://github.com/google-research/bert/blob/f39e881b169b9d53bea03d2d341b31707a6c052b/optimization.py#L37"),m(ct,"rel","nofollow"),m(S,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),m(ve,"id","transformers.WarmUp"),m(ve,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(ve,"href","#transformers.WarmUp"),m(Z,"class","relative group"),m(ee,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),m(be,"id","gradient-strategies"),m(be,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(be,"href","#gradient-strategies"),m(te,"class","relative group"),m(ye,"id","transformers.GradientAccumulator"),m(ye,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(ye,"href","#transformers.GradientAccumulator"),m(re,"class","relative group"),m($e,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),m(I,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(e,p){t(document.head,u),d(e,A,p),d(e,h,p),t(h,z),t(z,F),_(x,F,null),t(h,C),t(h,zt),t(zt,Ua),d(e,Ir,p),d(e,oe,p),t(oe,Ga),t(oe,Et),t(Et,Va),t(oe,Ma),d(e,Nr,p),d(e,O,p),t(O,Tt),t(Tt,Ha),t(O,Ba),t(O,Ee),t(Ee,Ja),t(Ee,Dt),t(Dt,Ka),t(Ee,Qa),t(O,Xa),t(O,Lt),t(Lt,Ya),d(e,Fr,p),d(e,R,p),t(R,se),t(se,Pt),_(Te,Pt,null),t(R,Za),t(R,Wt),t(Wt,en),d(e,Cr,p),d(e,P,p),_(De,P,null),t(P,tn),t(P,Le),t(Le,rn),t(Le,Pe),t(Pe,an),t(Le,nn),t(P,on),t(P,ie),_(We,ie,null),t(ie,sn),t(ie,kt),t(kt,ln),d(e,Or,p),d(e,j,p),t(j,le),t(le,St),_(ke,St,null),t(j,cn),t(j,It),t(It,mn),d(e,Rr,p),d(e,y,p),_(Se,y,null),t(y,pn),t(y,_t),t(_t,dn),t(_t,Ie),t(Ie,hn),t(y,un),t(y,E),t(E,fn),t(E,Nt),t(Nt,gn),t(E,_n),t(E,Ne),t(Ne,wn),t(E,vn),t(E,Ft),t(Ft,bn),t(E,yn),t(E,Ct),t(Ct,$n),t(E,An),t(E,Ot),t(Ot,xn),t(E,zn),t(E,Rt),t(Rt,En),t(E,Tn),t(E,jt),t(jt,Dn),t(E,Ln),t(y,Pn),t(y,qt),t(qt,Wn),t(y,kn),t(y,Fe),t(Fe,Sn),t(Fe,Ce),t(Ce,In),t(Fe,Nn),t(y,Fn),t(y,W),t(W,Oe),t(Oe,Ut),t(Ut,Cn),t(Oe,On),t(Oe,Re),t(Re,Gt),t(Gt,Rn),t(Re,jn),t(Re,je),t(je,qn),t(je,qe),t(qe,Un),t(je,Gn),t(W,Vn),t(W,Vt),t(Vt,Mt),t(Mt,Mn),t(W,Hn),t(W,Ht),t(Ht,Bt),t(Bt,Bn),t(W,Jn),t(W,Jt),t(Jt,Kt),t(Kt,Kn),t(y,Qn),_(ce,y,null),t(y,Xn),t(y,Qt),t(Qt,Yn),t(y,Zn),_(me,y,null),t(y,eo),t(y,k),t(k,to),t(k,Xt),t(Xt,ro),t(k,ao),t(k,wt),t(wt,no),t(k,oo),t(k,Yt),t(Yt,so),t(k,io),t(y,lo),_(pe,y,null),t(y,co),t(y,Zt),t(Zt,mo),t(y,po),_(de,y,null),t(y,ho),t(y,he),_(Ue,he,null),t(he,uo),t(he,er),t(er,fo),d(e,jr,p),d(e,q,p),t(q,ue),t(ue,tr),_(Ge,tr,null),t(q,go),t(q,rr),t(rr,_o),d(e,qr,p),d(e,L,p),_(Ve,L,null),t(L,wo),t(L,U),t(U,vo),t(U,ar),t(ar,bo),t(U,yo),t(U,Me),t(Me,$o),t(U,Ao),t(L,xo),t(L,nr),t(nr,zo),t(L,Eo),t(L,fe),_(He,fe,null),t(fe,To),t(fe,or),t(or,Do),d(e,Ur,p),d(e,G,p),_(Be,G,null),t(G,Lo),t(G,sr),t(sr,Po),d(e,Gr,p),d(e,V,p),t(V,ge),t(ge,ir),_(Je,ir,null),t(V,Wo),t(V,lr),t(lr,ko),d(e,Vr,p),d(e,M,p),t(M,_e),t(_e,cr),_(Ke,cr,null),t(M,So),t(M,mr),t(mr,Io),d(e,Mr,p),d(e,H,p),_(Qe,H,null),t(H,No),t(H,pr),t(pr,Fo),d(e,Hr,p),d(e,B,p),_(Xe,B,null),t(B,Co),t(B,dr),t(dr,Oo),d(e,Br,p),d(e,J,p),_(Ye,J,null),t(J,Ro),t(J,hr),t(hr,jo),d(e,Jr,p),d(e,K,p),_(Ze,K,null),t(K,qo),t(K,ur),t(ur,Uo),d(e,Kr,p),d(e,et,p),d(e,Qr,p),d(e,Q,p),_(tt,Q,null),t(Q,Go),t(Q,fr),t(fr,Vo),d(e,Xr,p),d(e,rt,p),d(e,Yr,p),d(e,X,p),_(at,X,null),t(X,Mo),t(X,gr),t(gr,Ho),d(e,Zr,p),d(e,nt,p),d(e,ea,p),d(e,Y,p),_(ot,Y,null),t(Y,Bo),t(Y,_r),t(_r,Jo),d(e,ta,p),d(e,st,p),d(e,ra,p),d(e,S,p),_(it,S,null),t(S,Ko),t(S,lt),t(lt,Qo),t(lt,wr),t(wr,Xo),t(lt,Yo),t(S,Zo),t(S,we),t(we,es),t(we,vr),t(vr,ts),t(we,rs),t(we,ct),t(ct,as),d(e,aa,p),d(e,Z,p),t(Z,ve),t(ve,br),_(mt,br,null),t(Z,ns),t(Z,yr),t(yr,os),d(e,na,p),d(e,ee,p),_(pt,ee,null),t(ee,ss),t(ee,$r),t($r,is),d(e,oa,p),d(e,te,p),t(te,be),t(be,Ar),_(dt,Ar,null),t(te,ls),t(te,xr),t(xr,cs),d(e,sa,p),d(e,re,p),t(re,ye),t(ye,zr),_(ht,zr,null),t(re,ms),t(re,Er),t(Er,ps),d(e,ia,p),d(e,I,p),_(ut,I,null),t(I,ds),t(I,ae),t(ae,hs),t(ae,Tr),t(Tr,us),t(ae,fs),t(ae,Dr),t(Dr,gs),t(ae,_s),t(I,ws),t(I,$e),_(ft,$e,null),t($e,vs),t($e,Lr),t(Lr,bs),la=!0},p(e,[p]){const gt={};p&2&&(gt.$$scope={dirty:p,ctx:e}),ce.$set(gt);const Pr={};p&2&&(Pr.$$scope={dirty:p,ctx:e}),me.$set(Pr);const Wr={};p&2&&(Wr.$$scope={dirty:p,ctx:e}),pe.$set(Wr);const kr={};p&2&&(kr.$$scope={dirty:p,ctx:e}),de.$set(kr)},i(e){la||(w(x.$$.fragment,e),w(Te.$$.fragment,e),w(De.$$.fragment,e),w(We.$$.fragment,e),w(ke.$$.fragment,e),w(Se.$$.fragment,e),w(ce.$$.fragment,e),w(me.$$.fragment,e),w(pe.$$.fragment,e),w(de.$$.fragment,e),w(Ue.$$.fragment,e),w(Ge.$$.fragment,e),w(Ve.$$.fragment,e),w(He.$$.fragment,e),w(Be.$$.fragment,e),w(Je.$$.fragment,e),w(Ke.$$.fragment,e),w(Qe.$$.fragment,e),w(Xe.$$.fragment,e),w(Ye.$$.fragment,e),w(Ze.$$.fragment,e),w(tt.$$.fragment,e),w(at.$$.fragment,e),w(ot.$$.fragment,e),w(it.$$.fragment,e),w(mt.$$.fragment,e),w(pt.$$.fragment,e),w(dt.$$.fragment,e),w(ht.$$.fragment,e),w(ut.$$.fragment,e),w(ft.$$.fragment,e),la=!0)},o(e){v(x.$$.fragment,e),v(Te.$$.fragment,e),v(De.$$.fragment,e),v(We.$$.fragment,e),v(ke.$$.fragment,e),v(Se.$$.fragment,e),v(ce.$$.fragment,e),v(me.$$.fragment,e),v(pe.$$.fragment,e),v(de.$$.fragment,e),v(Ue.$$.fragment,e),v(Ge.$$.fragment,e),v(Ve.$$.fragment,e),v(He.$$.fragment,e),v(Be.$$.fragment,e),v(Je.$$.fragment,e),v(Ke.$$.fragment,e),v(Qe.$$.fragment,e),v(Xe.$$.fragment,e),v(Ye.$$.fragment,e),v(Ze.$$.fragment,e),v(tt.$$.fragment,e),v(at.$$.fragment,e),v(ot.$$.fragment,e),v(it.$$.fragment,e),v(mt.$$.fragment,e),v(pt.$$.fragment,e),v(dt.$$.fragment,e),v(ht.$$.fragment,e),v(ut.$$.fragment,e),v(ft.$$.fragment,e),la=!1},d(e){r(u),e&&r(A),e&&r(h),b(x),e&&r(Ir),e&&r(oe),e&&r(Nr),e&&r(O),e&&r(Fr),e&&r(R),b(Te),e&&r(Cr),e&&r(P),b(De),b(We),e&&r(Or),e&&r(j),b(ke),e&&r(Rr),e&&r(y),b(Se),b(ce),b(me),b(pe),b(de),b(Ue),e&&r(jr),e&&r(q),b(Ge),e&&r(qr),e&&r(L),b(Ve),b(He),e&&r(Ur),e&&r(G),b(Be),e&&r(Gr),e&&r(V),b(Je),e&&r(Vr),e&&r(M),b(Ke),e&&r(Mr),e&&r(H),b(Qe),e&&r(Hr),e&&r(B),b(Xe),e&&r(Br),e&&r(J),b(Ye),e&&r(Jr),e&&r(K),b(Ze),e&&r(Kr),e&&r(et),e&&r(Qr),e&&r(Q),b(tt),e&&r(Xr),e&&r(rt),e&&r(Yr),e&&r(X),b(at),e&&r(Zr),e&&r(nt),e&&r(ea),e&&r(Y),b(ot),e&&r(ta),e&&r(st),e&&r(ra),e&&r(S),b(it),e&&r(aa),e&&r(Z),b(mt),e&&r(na),e&&r(ee),b(pt),e&&r(oa),e&&r(te),b(dt),e&&r(sa),e&&r(re),b(ht),e&&r(ia),e&&r(I),b(ut),b(ft)}}}const rl={local:"optimization",sections:[{local:"transformers.AdamW",title:"AdamW (PyTorch)"},{local:"transformers.Adafactor",title:"AdaFactor (PyTorch)"},{local:"transformers.AdamWeightDecay",title:"AdamWeightDecay (TensorFlow)"},{local:"schedules",sections:[{local:"transformers.SchedulerType",title:"Learning Rate Schedules (Pytorch)"},{local:"transformers.WarmUp",title:"Warmup (TensorFlow)"}],title:"Schedules"},{local:"gradient-strategies",sections:[{local:"transformers.GradientAccumulator",title:"GradientAccumulator (TensorFlow)"}],title:"Gradient Strategies"}],title:"Optimization"};function al(N){return Qi(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class cl extends Hi{constructor(u){super();Bi(this,u,al,tl,Ji,{})}}export{cl as default,rl as metadata};
