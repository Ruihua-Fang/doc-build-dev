import{S as u_t,i as b_t,s as v_t,e as a,k as l,w as f,t as o,M as T_t,c as n,d as t,m as i,a as s,x as m,h as r,b as c,F as e,g as b,y as g,q as h,o as p,B as _}from"../../chunks/vendor-4833417e.js";import{T as _yr}from"../../chunks/Tip-fffd6df1.js";import{D as E}from"../../chunks/Docstring-44c5af16.js";import{C as w}from"../../chunks/CodeBlock-90ffda97.js";import{I as z}from"../../chunks/IconCopyLink-4b81c553.js";import"../../chunks/CopyButton-04a16537.js";function F_t(yi){let J,Ae,ie,me,to,ce,ue,Do,wi,Ef,sa,Ai,Li,oM,yf,ye,io,Bi,Pn,rM,$n,In,tM,ki,jn,aM,xi,wf,$a;return{c(){J=a("p"),Ae=o("If your "),ie=a("code"),me=o("NewModelConfig"),to=o(" is a subclass of "),ce=a("code"),ue=o("PretrainedConfig"),Do=o(`, make sure its
`),wi=a("code"),Ef=o("model_type"),sa=o(" attribute is set to the same key you use when registering the config (here "),Ai=a("code"),Li=o('"new-model"'),oM=o(")."),yf=l(),ye=a("p"),io=o("Likewise, if your "),Bi=a("code"),Pn=o("NewModel"),rM=o(" is a subclass of "),$n=a("a"),In=o("PreTrainedModel"),tM=o(`, make sure its
`),ki=a("code"),jn=o("config_class"),aM=o(` attribute is set to the same class you use when registering the model (here
`),xi=a("code"),wf=o("NewModelConfig"),$a=o(")."),this.h()},l(co){J=n(co,"P",{});var ge=s(J);Ae=r(ge,"If your "),ie=n(ge,"CODE",{});var D0=s(ie);me=r(D0,"NewModelConfig"),D0.forEach(t),to=r(ge," is a subclass of "),ce=n(ge,"CODE",{});var Ri=s(ce);ue=r(Ri,"PretrainedConfig"),Ri.forEach(t),Do=r(ge,`, make sure its
`),wi=n(ge,"CODE",{});var q0=s(wi);Ef=r(q0,"model_type"),q0.forEach(t),sa=r(ge," attribute is set to the same key you use when registering the config (here "),Ai=n(ge,"CODE",{});var G0=s(Ai);Li=r(G0,'"new-model"'),G0.forEach(t),oM=r(ge,")."),ge.forEach(t),yf=i(co),ye=n(co,"P",{});var qo=s(ye);io=r(qo,"Likewise, if your "),Bi=n(qo,"CODE",{});var Ia=s(Bi);Pn=r(Ia,"NewModel"),Ia.forEach(t),rM=r(qo," is a subclass of "),$n=n(qo,"A",{href:!0});var O0=s($n);In=r(O0,"PreTrainedModel"),O0.forEach(t),tM=r(qo,`, make sure its
`),ki=n(qo,"CODE",{});var Af=s(ki);jn=r(Af,"config_class"),Af.forEach(t),aM=r(qo,` attribute is set to the same class you use when registering the model (here
`),xi=n(qo,"CODE",{});var X0=s(xi);wf=r(X0,"NewModelConfig"),X0.forEach(t),$a=r(qo,")."),qo.forEach(t),this.h()},h(){c($n,"href","/docs/transformers/pr_15792/en/main_classes/model#transformers.PreTrainedModel")},m(co,ge){b(co,J,ge),e(J,Ae),e(J,ie),e(ie,me),e(J,to),e(J,ce),e(ce,ue),e(J,Do),e(J,wi),e(wi,Ef),e(J,sa),e(J,Ai),e(Ai,Li),e(J,oM),b(co,yf,ge),b(co,ye,ge),e(ye,io),e(ye,Bi),e(Bi,Pn),e(ye,rM),e(ye,$n),e($n,In),e(ye,tM),e(ye,ki),e(ki,jn),e(ye,aM),e(ye,xi),e(xi,wf),e(ye,$a)},d(co){co&&t(J),co&&t(yf),co&&t(ye)}}}function C_t(yi){let J,Ae,ie,me,to;return{c(){J=a("p"),Ae=o("Passing "),ie=a("code"),me=o("use_auth_token=True"),to=o(" is required when you want to use a private model.")},l(ce){J=n(ce,"P",{});var ue=s(J);Ae=r(ue,"Passing "),ie=n(ue,"CODE",{});var Do=s(ie);me=r(Do,"use_auth_token=True"),Do.forEach(t),to=r(ue," is required when you want to use a private model."),ue.forEach(t)},m(ce,ue){b(ce,J,ue),e(J,Ae),e(J,ie),e(ie,me),e(J,to)},d(ce){ce&&t(J)}}}function M_t(yi){let J,Ae,ie,me,to;return{c(){J=a("p"),Ae=o("Passing "),ie=a("code"),me=o("use_auth_token=True"),to=o(" is required when you want to use a private model.")},l(ce){J=n(ce,"P",{});var ue=s(J);Ae=r(ue,"Passing "),ie=n(ue,"CODE",{});var Do=s(ie);me=r(Do,"use_auth_token=True"),Do.forEach(t),to=r(ue," is required when you want to use a private model."),ue.forEach(t)},m(ce,ue){b(ce,J,ue),e(J,Ae),e(J,ie),e(ie,me),e(J,to)},d(ce){ce&&t(J)}}}function E_t(yi){let J,Ae,ie,me,to,ce,ue,Do,wi,Ef,sa,Ai,Li,oM,yf,ye,io,Bi,Pn,rM,$n,In,tM,ki,jn,aM,xi,wf,$a,co,ge,D0,Ri,q0,G0,qo,Ia,O0,Af,X0,mxe,tLe,Si,Lf,$V,nM,gxe,IV,hxe,aLe,Nn,pxe,jV,_xe,uxe,NV,bxe,vxe,nLe,sM,sLe,z0,Txe,lLe,Bf,iLe,Pi,kf,DV,lM,Fxe,qV,Cxe,dLe,Go,iM,Mxe,dM,Exe,V0,yxe,wxe,Axe,cM,Lxe,GV,Bxe,kxe,xxe,fo,fM,Rxe,OV,Sxe,Pxe,$i,$xe,XV,Ixe,jxe,zV,Nxe,Dxe,qxe,v,xf,VV,Gxe,Oxe,W0,Xxe,zxe,Vxe,Rf,WV,Wxe,Qxe,Q0,Hxe,Uxe,Jxe,Sf,QV,Yxe,Kxe,H0,Zxe,eRe,oRe,Pf,HV,rRe,tRe,U0,aRe,nRe,sRe,$f,UV,lRe,iRe,J0,dRe,cRe,fRe,If,JV,mRe,gRe,Y0,hRe,pRe,_Re,jf,YV,uRe,bRe,K0,vRe,TRe,FRe,Nf,KV,CRe,MRe,Z0,ERe,yRe,wRe,Df,ZV,ARe,LRe,eL,BRe,kRe,xRe,qf,eW,RRe,SRe,oL,PRe,$Re,IRe,Gf,oW,jRe,NRe,rL,DRe,qRe,GRe,Of,rW,ORe,XRe,tL,zRe,VRe,WRe,Xf,tW,QRe,HRe,aL,URe,JRe,YRe,zf,aW,KRe,ZRe,nL,eSe,oSe,rSe,Vf,nW,tSe,aSe,sL,nSe,sSe,lSe,Wf,sW,iSe,dSe,lL,cSe,fSe,mSe,Qf,lW,gSe,hSe,iL,pSe,_Se,uSe,Hf,iW,bSe,vSe,dL,TSe,FSe,CSe,Uf,dW,MSe,ESe,cL,ySe,wSe,ASe,Jf,cW,LSe,BSe,fL,kSe,xSe,RSe,Yf,fW,SSe,PSe,mL,$Se,ISe,jSe,Kf,mW,NSe,DSe,gL,qSe,GSe,OSe,Zf,gW,XSe,zSe,hL,VSe,WSe,QSe,em,hW,HSe,USe,pL,JSe,YSe,KSe,om,pW,ZSe,ePe,_L,oPe,rPe,tPe,rm,_W,aPe,nPe,uL,sPe,lPe,iPe,tm,uW,dPe,cPe,bL,fPe,mPe,gPe,am,bW,hPe,pPe,vL,_Pe,uPe,bPe,nm,vW,vPe,TPe,TL,FPe,CPe,MPe,sm,TW,EPe,yPe,FL,wPe,APe,LPe,lm,FW,BPe,kPe,CL,xPe,RPe,SPe,im,CW,PPe,$Pe,ML,IPe,jPe,NPe,dm,MW,DPe,qPe,EL,GPe,OPe,XPe,cm,EW,zPe,VPe,yL,WPe,QPe,HPe,fm,yW,UPe,JPe,wL,YPe,KPe,ZPe,mm,wW,e$e,o$e,AL,r$e,t$e,a$e,gm,AW,n$e,s$e,LL,l$e,i$e,d$e,hm,LW,c$e,f$e,BL,m$e,g$e,h$e,pm,BW,p$e,_$e,kL,u$e,b$e,v$e,_m,kW,T$e,F$e,xL,C$e,M$e,E$e,um,xW,y$e,w$e,RL,A$e,L$e,B$e,bm,RW,k$e,x$e,SL,R$e,S$e,P$e,vm,SW,$$e,I$e,PL,j$e,N$e,D$e,Tm,PW,q$e,G$e,$L,O$e,X$e,z$e,Fm,$W,V$e,W$e,IL,Q$e,H$e,U$e,Cm,IW,J$e,Y$e,jL,K$e,Z$e,eIe,Mm,jW,oIe,rIe,NL,tIe,aIe,nIe,Em,NW,sIe,lIe,DL,iIe,dIe,cIe,ym,DW,fIe,mIe,qL,gIe,hIe,pIe,wm,qW,_Ie,uIe,GL,bIe,vIe,TIe,Am,GW,FIe,CIe,OL,MIe,EIe,yIe,Lm,OW,wIe,AIe,XL,LIe,BIe,kIe,Bm,XW,xIe,RIe,zL,SIe,PIe,$Ie,km,zW,IIe,jIe,VL,NIe,DIe,qIe,xm,VW,GIe,OIe,WL,XIe,zIe,VIe,Rm,WW,WIe,QIe,QL,HIe,UIe,JIe,Sm,QW,YIe,KIe,HL,ZIe,eje,oje,Pm,HW,rje,tje,UL,aje,nje,sje,$m,UW,lje,ije,JL,dje,cje,fje,Im,JW,mje,gje,YL,hje,pje,_je,jm,YW,uje,bje,KL,vje,Tje,Fje,Nm,KW,Cje,Mje,ZL,Eje,yje,wje,Dm,ZW,Aje,Lje,e8,Bje,kje,xje,qm,eQ,Rje,Sje,o8,Pje,$je,Ije,Gm,oQ,jje,Nje,r8,Dje,qje,Gje,Om,rQ,Oje,Xje,t8,zje,Vje,Wje,Xm,tQ,Qje,Hje,a8,Uje,Jje,Yje,zm,aQ,Kje,Zje,n8,eNe,oNe,rNe,Vm,nQ,tNe,aNe,s8,nNe,sNe,lNe,Wm,sQ,iNe,dNe,l8,cNe,fNe,mNe,Qm,lQ,gNe,hNe,i8,pNe,_Ne,uNe,Hm,iQ,bNe,vNe,d8,TNe,FNe,CNe,Um,dQ,MNe,ENe,c8,yNe,wNe,ANe,Jm,cQ,LNe,BNe,f8,kNe,xNe,RNe,Ym,fQ,SNe,PNe,m8,$Ne,INe,jNe,Km,mQ,NNe,DNe,g8,qNe,GNe,ONe,Zm,gQ,XNe,zNe,h8,VNe,WNe,QNe,eg,hQ,HNe,UNe,p8,JNe,YNe,KNe,og,pQ,ZNe,eDe,_8,oDe,rDe,tDe,rg,_Q,aDe,nDe,u8,sDe,lDe,iDe,tg,uQ,dDe,cDe,b8,fDe,mDe,gDe,ag,bQ,hDe,pDe,v8,_De,uDe,bDe,ng,vQ,vDe,TDe,T8,FDe,CDe,MDe,sg,TQ,EDe,yDe,F8,wDe,ADe,LDe,lg,FQ,BDe,kDe,C8,xDe,RDe,SDe,ig,CQ,PDe,$De,M8,IDe,jDe,NDe,dg,MQ,DDe,qDe,E8,GDe,ODe,XDe,cg,EQ,zDe,VDe,y8,WDe,QDe,HDe,fg,yQ,UDe,JDe,w8,YDe,KDe,ZDe,mg,wQ,eqe,oqe,A8,rqe,tqe,aqe,gg,AQ,nqe,sqe,L8,lqe,iqe,dqe,LQ,cqe,fqe,mM,mqe,hg,gM,gqe,BQ,hqe,cLe,Ii,pg,kQ,hM,pqe,xQ,_qe,fLe,Oo,pM,uqe,_M,bqe,B8,vqe,Tqe,Fqe,uM,Cqe,RQ,Mqe,Eqe,yqe,mo,bM,wqe,SQ,Aqe,Lqe,ja,Bqe,PQ,kqe,xqe,$Q,Rqe,Sqe,IQ,Pqe,$qe,Iqe,M,Dn,jQ,jqe,Nqe,k8,Dqe,qqe,x8,Gqe,Oqe,Xqe,qn,NQ,zqe,Vqe,R8,Wqe,Qqe,S8,Hqe,Uqe,Jqe,Gn,DQ,Yqe,Kqe,P8,Zqe,eGe,$8,oGe,rGe,tGe,_g,qQ,aGe,nGe,I8,sGe,lGe,iGe,On,GQ,dGe,cGe,j8,fGe,mGe,N8,gGe,hGe,pGe,ug,OQ,_Ge,uGe,D8,bGe,vGe,TGe,bg,XQ,FGe,CGe,q8,MGe,EGe,yGe,vg,zQ,wGe,AGe,G8,LGe,BGe,kGe,Xn,VQ,xGe,RGe,O8,SGe,PGe,X8,$Ge,IGe,jGe,zn,WQ,NGe,DGe,z8,qGe,GGe,V8,OGe,XGe,zGe,Vn,QQ,VGe,WGe,W8,QGe,HGe,Q8,UGe,JGe,YGe,Tg,HQ,KGe,ZGe,H8,eOe,oOe,rOe,Fg,UQ,tOe,aOe,U8,nOe,sOe,lOe,Wn,JQ,iOe,dOe,J8,cOe,fOe,Y8,mOe,gOe,hOe,Cg,YQ,pOe,_Oe,K8,uOe,bOe,vOe,Qn,KQ,TOe,FOe,Z8,COe,MOe,eB,EOe,yOe,wOe,Hn,ZQ,AOe,LOe,oB,BOe,kOe,rB,xOe,ROe,SOe,Un,eH,POe,$Oe,tB,IOe,jOe,oH,NOe,DOe,qOe,Mg,rH,GOe,OOe,aB,XOe,zOe,VOe,Jn,tH,WOe,QOe,nB,HOe,UOe,sB,JOe,YOe,KOe,Eg,aH,ZOe,eXe,lB,oXe,rXe,tXe,Yn,nH,aXe,nXe,iB,sXe,lXe,dB,iXe,dXe,cXe,Kn,sH,fXe,mXe,cB,gXe,hXe,fB,pXe,_Xe,uXe,Zn,lH,bXe,vXe,mB,TXe,FXe,gB,CXe,MXe,EXe,yg,iH,yXe,wXe,hB,AXe,LXe,BXe,es,dH,kXe,xXe,pB,RXe,SXe,_B,PXe,$Xe,IXe,wg,cH,jXe,NXe,uB,DXe,qXe,GXe,os,fH,OXe,XXe,bB,zXe,VXe,vB,WXe,QXe,HXe,rs,mH,UXe,JXe,TB,YXe,KXe,FB,ZXe,eze,oze,ts,gH,rze,tze,CB,aze,nze,MB,sze,lze,ize,as,hH,dze,cze,EB,fze,mze,yB,gze,hze,pze,Ag,pH,_ze,uze,wB,bze,vze,Tze,ns,_H,Fze,Cze,AB,Mze,Eze,LB,yze,wze,Aze,ss,uH,Lze,Bze,BB,kze,xze,kB,Rze,Sze,Pze,ls,bH,$ze,Ize,xB,jze,Nze,RB,Dze,qze,Gze,is,vH,Oze,Xze,SB,zze,Vze,PB,Wze,Qze,Hze,ds,TH,Uze,Jze,$B,Yze,Kze,IB,Zze,eVe,oVe,cs,FH,rVe,tVe,jB,aVe,nVe,NB,sVe,lVe,iVe,Lg,CH,dVe,cVe,DB,fVe,mVe,gVe,fs,MH,hVe,pVe,qB,_Ve,uVe,GB,bVe,vVe,TVe,Bg,EH,FVe,CVe,OB,MVe,EVe,yVe,kg,yH,wVe,AVe,XB,LVe,BVe,kVe,ms,wH,xVe,RVe,zB,SVe,PVe,VB,$Ve,IVe,jVe,gs,AH,NVe,DVe,WB,qVe,GVe,QB,OVe,XVe,zVe,xg,LH,VVe,WVe,HB,QVe,HVe,UVe,hs,BH,JVe,YVe,UB,KVe,ZVe,JB,eWe,oWe,rWe,ps,kH,tWe,aWe,YB,nWe,sWe,KB,lWe,iWe,dWe,_s,xH,cWe,fWe,ZB,mWe,gWe,ek,hWe,pWe,_We,us,RH,uWe,bWe,ok,vWe,TWe,rk,FWe,CWe,MWe,bs,SH,EWe,yWe,tk,wWe,AWe,ak,LWe,BWe,kWe,Rg,PH,xWe,RWe,nk,SWe,PWe,$We,Sg,$H,IWe,jWe,sk,NWe,DWe,qWe,Pg,IH,GWe,OWe,lk,XWe,zWe,VWe,$g,jH,WWe,QWe,ik,HWe,UWe,JWe,vs,NH,YWe,KWe,dk,ZWe,eQe,ck,oQe,rQe,tQe,Ig,DH,aQe,nQe,fk,sQe,lQe,iQe,Ts,qH,dQe,cQe,mk,fQe,mQe,gk,gQe,hQe,pQe,Fs,GH,_Qe,uQe,hk,bQe,vQe,pk,TQe,FQe,CQe,Cs,OH,MQe,EQe,_k,yQe,wQe,uk,AQe,LQe,BQe,Ms,XH,kQe,xQe,bk,RQe,SQe,vk,PQe,$Qe,IQe,Es,zH,jQe,NQe,Tk,DQe,qQe,Fk,GQe,OQe,XQe,jg,VH,zQe,VQe,Ck,WQe,QQe,HQe,Ng,WH,UQe,JQe,Mk,YQe,KQe,ZQe,ys,QH,eHe,oHe,Ek,rHe,tHe,yk,aHe,nHe,sHe,ws,HH,lHe,iHe,wk,dHe,cHe,Ak,fHe,mHe,gHe,As,UH,hHe,pHe,Lk,_He,uHe,Bk,bHe,vHe,THe,Dg,JH,FHe,CHe,kk,MHe,EHe,yHe,qg,YH,wHe,AHe,xk,LHe,BHe,kHe,Gg,KH,xHe,RHe,Rk,SHe,PHe,$He,Og,ZH,IHe,jHe,Sk,NHe,DHe,qHe,Ls,eU,GHe,OHe,Pk,XHe,zHe,$k,VHe,WHe,QHe,Xg,oU,HHe,UHe,Ik,JHe,YHe,KHe,zg,rU,ZHe,eUe,jk,oUe,rUe,tUe,Bs,tU,aUe,nUe,Nk,sUe,lUe,Dk,iUe,dUe,cUe,ks,aU,fUe,mUe,qk,gUe,hUe,Gk,pUe,_Ue,uUe,nU,bUe,vUe,vM,TUe,Vg,TM,FUe,sU,CUe,mLe,ji,Wg,lU,FM,MUe,iU,EUe,gLe,Xo,CM,yUe,MM,wUe,Ok,AUe,LUe,BUe,EM,kUe,dU,xUe,RUe,SUe,Le,yM,PUe,cU,$Ue,IUe,Na,jUe,fU,NUe,DUe,mU,qUe,GUe,gU,OUe,XUe,zUe,se,Qg,hU,VUe,WUe,Xk,QUe,HUe,UUe,Hg,pU,JUe,YUe,zk,KUe,ZUe,eJe,Ug,_U,oJe,rJe,Vk,tJe,aJe,nJe,Jg,uU,sJe,lJe,Wk,iJe,dJe,cJe,Yg,bU,fJe,mJe,Qk,gJe,hJe,pJe,Kg,vU,_Je,uJe,Hk,bJe,vJe,TJe,Zg,TU,FJe,CJe,Uk,MJe,EJe,yJe,eh,FU,wJe,AJe,Jk,LJe,BJe,kJe,oh,CU,xJe,RJe,Yk,SJe,PJe,$Je,rh,MU,IJe,jJe,Kk,NJe,DJe,qJe,th,EU,GJe,OJe,Zk,XJe,zJe,VJe,ah,yU,WJe,QJe,ex,HJe,UJe,JJe,nh,wU,YJe,KJe,ox,ZJe,eYe,oYe,sh,AU,rYe,tYe,rx,aYe,nYe,sYe,lh,LU,lYe,iYe,tx,dYe,cYe,fYe,ih,mYe,BU,gYe,hYe,wM,pYe,dh,AM,_Ye,kU,uYe,hLe,Ni,ch,xU,LM,bYe,RU,vYe,pLe,zo,BM,TYe,kM,FYe,ax,CYe,MYe,EYe,xM,yYe,SU,wYe,AYe,LYe,Be,RM,BYe,PU,kYe,xYe,Di,RYe,$U,SYe,PYe,IU,$Ye,IYe,jYe,we,fh,jU,NYe,DYe,nx,qYe,GYe,OYe,mh,NU,XYe,zYe,sx,VYe,WYe,QYe,gh,DU,HYe,UYe,lx,JYe,YYe,KYe,hh,qU,ZYe,eKe,ix,oKe,rKe,tKe,ph,GU,aKe,nKe,dx,sKe,lKe,iKe,_h,OU,dKe,cKe,cx,fKe,mKe,gKe,uh,XU,hKe,pKe,fx,_Ke,uKe,bKe,bh,zU,vKe,TKe,mx,FKe,CKe,MKe,vh,EKe,VU,yKe,wKe,SM,AKe,Th,PM,LKe,WU,BKe,_Le,qi,Fh,QU,$M,kKe,HU,xKe,uLe,Vo,IM,RKe,Gi,SKe,UU,PKe,$Ke,JU,IKe,jKe,NKe,jM,DKe,YU,qKe,GKe,OKe,Nr,NM,XKe,KU,zKe,VKe,Oi,WKe,ZU,QKe,HKe,eJ,UKe,JKe,YKe,oJ,KKe,ZKe,DM,eZe,ke,qM,oZe,rJ,rZe,tZe,Da,aZe,tJ,nZe,sZe,aJ,lZe,iZe,nJ,dZe,cZe,fZe,F,Ch,sJ,mZe,gZe,gx,hZe,pZe,_Ze,Mh,lJ,uZe,bZe,hx,vZe,TZe,FZe,Eh,iJ,CZe,MZe,px,EZe,yZe,wZe,yh,dJ,AZe,LZe,_x,BZe,kZe,xZe,wh,cJ,RZe,SZe,ux,PZe,$Ze,IZe,Ah,fJ,jZe,NZe,bx,DZe,qZe,GZe,Lh,mJ,OZe,XZe,vx,zZe,VZe,WZe,Bh,gJ,QZe,HZe,Tx,UZe,JZe,YZe,kh,hJ,KZe,ZZe,Fx,eeo,oeo,reo,xh,pJ,teo,aeo,Cx,neo,seo,leo,Rh,_J,ieo,deo,Mx,ceo,feo,meo,Sh,uJ,geo,heo,Ex,peo,_eo,ueo,Ph,bJ,beo,veo,yx,Teo,Feo,Ceo,$h,vJ,Meo,Eeo,wx,yeo,weo,Aeo,Ih,TJ,Leo,Beo,Ax,keo,xeo,Reo,jh,FJ,Seo,Peo,Lx,$eo,Ieo,jeo,Nh,CJ,Neo,Deo,Bx,qeo,Geo,Oeo,Dh,MJ,Xeo,zeo,kx,Veo,Weo,Qeo,qh,EJ,Heo,Ueo,xx,Jeo,Yeo,Keo,Gh,yJ,Zeo,eoo,Rx,ooo,roo,too,Oh,wJ,aoo,noo,Sx,soo,loo,ioo,Xh,AJ,doo,coo,Px,foo,moo,goo,zh,LJ,hoo,poo,$x,_oo,uoo,boo,Vh,BJ,voo,Too,Ix,Foo,Coo,Moo,Wh,kJ,Eoo,yoo,jx,woo,Aoo,Loo,xs,xJ,Boo,koo,Nx,xoo,Roo,Dx,Soo,Poo,$oo,Qh,RJ,Ioo,joo,qx,Noo,Doo,qoo,Hh,SJ,Goo,Ooo,Gx,Xoo,zoo,Voo,Uh,PJ,Woo,Qoo,Ox,Hoo,Uoo,Joo,Jh,$J,Yoo,Koo,Xx,Zoo,ero,oro,Yh,IJ,rro,tro,zx,aro,nro,sro,Kh,jJ,lro,iro,Vx,dro,cro,fro,Zh,NJ,mro,gro,Wx,hro,pro,_ro,ep,DJ,uro,bro,Qx,vro,Tro,Fro,op,qJ,Cro,Mro,Hx,Ero,yro,wro,rp,GJ,Aro,Lro,Ux,Bro,kro,xro,tp,OJ,Rro,Sro,Jx,Pro,$ro,Iro,ap,XJ,jro,Nro,Yx,Dro,qro,Gro,np,zJ,Oro,Xro,Kx,zro,Vro,Wro,sp,VJ,Qro,Hro,Zx,Uro,Jro,Yro,lp,WJ,Kro,Zro,eR,eto,oto,rto,ip,QJ,tto,ato,oR,nto,sto,lto,dp,HJ,ito,dto,rR,cto,fto,mto,cp,UJ,gto,hto,tR,pto,_to,uto,fp,JJ,bto,vto,aR,Tto,Fto,Cto,mp,YJ,Mto,Eto,nR,yto,wto,Ato,gp,KJ,Lto,Bto,sR,kto,xto,Rto,hp,ZJ,Sto,Pto,lR,$to,Ito,jto,pp,eY,Nto,Dto,iR,qto,Gto,Oto,_p,oY,Xto,zto,dR,Vto,Wto,Qto,up,rY,Hto,Uto,cR,Jto,Yto,Kto,bp,tY,Zto,eao,fR,oao,rao,tao,vp,aY,aao,nao,mR,sao,lao,iao,Tp,nY,dao,cao,gR,fao,mao,gao,Fp,sY,hao,pao,hR,_ao,uao,bao,Cp,lY,vao,Tao,pR,Fao,Cao,Mao,Mp,iY,Eao,yao,_R,wao,Aao,Lao,Ep,dY,Bao,kao,uR,xao,Rao,Sao,yp,cY,Pao,$ao,bR,Iao,jao,Nao,wp,fY,Dao,qao,vR,Gao,Oao,Xao,Ap,mY,zao,Vao,TR,Wao,Qao,Hao,Lp,gY,Uao,Jao,FR,Yao,Kao,Zao,Bp,hY,eno,ono,CR,rno,tno,ano,kp,pY,nno,sno,MR,lno,ino,dno,xp,_Y,cno,fno,ER,mno,gno,hno,Rp,uY,pno,_no,yR,uno,bno,vno,Sp,bY,Tno,Fno,wR,Cno,Mno,Eno,Pp,vY,yno,wno,AR,Ano,Lno,Bno,$p,TY,kno,xno,LR,Rno,Sno,Pno,Ip,FY,$no,Ino,BR,jno,Nno,Dno,jp,CY,qno,Gno,kR,Ono,Xno,zno,Np,MY,Vno,Wno,xR,Qno,Hno,Uno,Dp,EY,Jno,Yno,RR,Kno,Zno,eso,qp,yY,oso,rso,SR,tso,aso,nso,Gp,wY,sso,lso,PR,iso,dso,cso,Op,AY,fso,mso,$R,gso,hso,pso,Xp,LY,_so,uso,IR,bso,vso,Tso,zp,BY,Fso,Cso,jR,Mso,Eso,yso,Vp,kY,wso,Aso,NR,Lso,Bso,kso,Wp,xY,xso,Rso,DR,Sso,Pso,$so,Qp,RY,Iso,jso,qR,Nso,Dso,qso,Hp,SY,Gso,Oso,GR,Xso,zso,Vso,Up,PY,Wso,Qso,OR,Hso,Uso,Jso,Jp,$Y,Yso,Kso,XR,Zso,elo,olo,Yp,rlo,IY,tlo,alo,jY,nlo,slo,NY,llo,ilo,GM,bLe,Xi,Kp,DY,OM,dlo,qY,clo,vLe,Wo,XM,flo,zi,mlo,GY,glo,hlo,OY,plo,_lo,ulo,zM,blo,XY,vlo,Tlo,Flo,Dr,VM,Clo,zY,Mlo,Elo,Vi,ylo,VY,wlo,Alo,WY,Llo,Blo,klo,QY,xlo,Rlo,WM,Slo,xe,QM,Plo,HY,$lo,Ilo,qa,jlo,UY,Nlo,Dlo,JY,qlo,Glo,YY,Olo,Xlo,zlo,x,Zp,KY,Vlo,Wlo,zR,Qlo,Hlo,Ulo,e_,ZY,Jlo,Ylo,VR,Klo,Zlo,eio,o_,eK,oio,rio,WR,tio,aio,nio,r_,oK,sio,lio,QR,iio,dio,cio,t_,rK,fio,mio,HR,gio,hio,pio,a_,tK,_io,uio,UR,bio,vio,Tio,n_,aK,Fio,Cio,JR,Mio,Eio,yio,s_,nK,wio,Aio,YR,Lio,Bio,kio,l_,sK,xio,Rio,KR,Sio,Pio,$io,i_,lK,Iio,jio,ZR,Nio,Dio,qio,d_,iK,Gio,Oio,eS,Xio,zio,Vio,c_,dK,Wio,Qio,oS,Hio,Uio,Jio,f_,cK,Yio,Kio,rS,Zio,edo,odo,m_,fK,rdo,tdo,tS,ado,ndo,sdo,g_,mK,ldo,ido,aS,ddo,cdo,fdo,h_,gK,mdo,gdo,nS,hdo,pdo,_do,p_,hK,udo,bdo,sS,vdo,Tdo,Fdo,__,pK,Cdo,Mdo,lS,Edo,ydo,wdo,u_,_K,Ado,Ldo,iS,Bdo,kdo,xdo,b_,uK,Rdo,Sdo,dS,Pdo,$do,Ido,v_,bK,jdo,Ndo,cS,Ddo,qdo,Gdo,T_,vK,Odo,Xdo,fS,zdo,Vdo,Wdo,F_,TK,Qdo,Hdo,mS,Udo,Jdo,Ydo,C_,FK,Kdo,Zdo,gS,eco,oco,rco,M_,CK,tco,aco,hS,nco,sco,lco,E_,MK,ico,dco,pS,cco,fco,mco,y_,EK,gco,hco,_S,pco,_co,uco,w_,yK,bco,vco,uS,Tco,Fco,Cco,A_,wK,Mco,Eco,bS,yco,wco,Aco,L_,AK,Lco,Bco,vS,kco,xco,Rco,B_,LK,Sco,Pco,TS,$co,Ico,jco,k_,BK,Nco,Dco,FS,qco,Gco,Oco,x_,kK,Xco,zco,CS,Vco,Wco,Qco,R_,xK,Hco,Uco,MS,Jco,Yco,Kco,S_,RK,Zco,efo,ES,ofo,rfo,tfo,P_,SK,afo,nfo,yS,sfo,lfo,ifo,$_,PK,dfo,cfo,wS,ffo,mfo,gfo,I_,$K,hfo,pfo,AS,_fo,ufo,bfo,j_,vfo,IK,Tfo,Ffo,jK,Cfo,Mfo,NK,Efo,yfo,HM,TLe,Wi,N_,DK,UM,wfo,qK,Afo,FLe,Qo,JM,Lfo,Qi,Bfo,GK,kfo,xfo,OK,Rfo,Sfo,Pfo,YM,$fo,XK,Ifo,jfo,Nfo,qr,KM,Dfo,zK,qfo,Gfo,Hi,Ofo,VK,Xfo,zfo,WK,Vfo,Wfo,Qfo,QK,Hfo,Ufo,ZM,Jfo,Re,eE,Yfo,HK,Kfo,Zfo,Ga,emo,UK,omo,rmo,JK,tmo,amo,YK,nmo,smo,lmo,$,D_,KK,imo,dmo,LS,cmo,fmo,mmo,q_,ZK,gmo,hmo,BS,pmo,_mo,umo,G_,eZ,bmo,vmo,kS,Tmo,Fmo,Cmo,O_,oZ,Mmo,Emo,xS,ymo,wmo,Amo,X_,rZ,Lmo,Bmo,RS,kmo,xmo,Rmo,z_,tZ,Smo,Pmo,SS,$mo,Imo,jmo,V_,aZ,Nmo,Dmo,PS,qmo,Gmo,Omo,W_,nZ,Xmo,zmo,$S,Vmo,Wmo,Qmo,Q_,sZ,Hmo,Umo,IS,Jmo,Ymo,Kmo,H_,lZ,Zmo,ego,jS,ogo,rgo,tgo,U_,iZ,ago,ngo,NS,sgo,lgo,igo,J_,dZ,dgo,cgo,DS,fgo,mgo,ggo,Y_,cZ,hgo,pgo,qS,_go,ugo,bgo,K_,fZ,vgo,Tgo,GS,Fgo,Cgo,Mgo,Z_,mZ,Ego,ygo,OS,wgo,Ago,Lgo,eu,gZ,Bgo,kgo,XS,xgo,Rgo,Sgo,ou,hZ,Pgo,$go,zS,Igo,jgo,Ngo,ru,pZ,Dgo,qgo,VS,Ggo,Ogo,Xgo,tu,_Z,zgo,Vgo,WS,Wgo,Qgo,Hgo,au,uZ,Ugo,Jgo,QS,Ygo,Kgo,Zgo,nu,bZ,eho,oho,HS,rho,tho,aho,su,vZ,nho,sho,US,lho,iho,dho,lu,TZ,cho,fho,JS,mho,gho,hho,iu,FZ,pho,_ho,YS,uho,bho,vho,du,CZ,Tho,Fho,KS,Cho,Mho,Eho,cu,MZ,yho,who,ZS,Aho,Lho,Bho,fu,EZ,kho,xho,eP,Rho,Sho,Pho,mu,yZ,$ho,Iho,oP,jho,Nho,Dho,gu,wZ,qho,Gho,rP,Oho,Xho,zho,hu,AZ,Vho,Who,tP,Qho,Hho,Uho,pu,LZ,Jho,Yho,aP,Kho,Zho,epo,_u,BZ,opo,rpo,nP,tpo,apo,npo,uu,kZ,spo,lpo,sP,ipo,dpo,cpo,bu,xZ,fpo,mpo,lP,gpo,hpo,ppo,vu,_po,RZ,upo,bpo,SZ,vpo,Tpo,PZ,Fpo,Cpo,oE,CLe,Ui,Tu,$Z,rE,Mpo,IZ,Epo,MLe,Ho,tE,ypo,Ji,wpo,jZ,Apo,Lpo,NZ,Bpo,kpo,xpo,aE,Rpo,DZ,Spo,Ppo,$po,Gr,nE,Ipo,qZ,jpo,Npo,Yi,Dpo,GZ,qpo,Gpo,OZ,Opo,Xpo,zpo,XZ,Vpo,Wpo,sE,Qpo,Se,lE,Hpo,zZ,Upo,Jpo,Oa,Ypo,VZ,Kpo,Zpo,WZ,e_o,o_o,QZ,r_o,t_o,a_o,I,Fu,HZ,n_o,s_o,iP,l_o,i_o,d_o,Cu,UZ,c_o,f_o,dP,m_o,g_o,h_o,Mu,JZ,p_o,__o,cP,u_o,b_o,v_o,Eu,YZ,T_o,F_o,fP,C_o,M_o,E_o,yu,KZ,y_o,w_o,mP,A_o,L_o,B_o,wu,ZZ,k_o,x_o,gP,R_o,S_o,P_o,Au,eee,$_o,I_o,hP,j_o,N_o,D_o,Lu,oee,q_o,G_o,pP,O_o,X_o,z_o,Bu,ree,V_o,W_o,_P,Q_o,H_o,U_o,ku,tee,J_o,Y_o,uP,K_o,Z_o,euo,xu,aee,ouo,ruo,bP,tuo,auo,nuo,Ru,nee,suo,luo,vP,iuo,duo,cuo,Su,see,fuo,muo,TP,guo,huo,puo,Pu,lee,_uo,uuo,FP,buo,vuo,Tuo,$u,iee,Fuo,Cuo,CP,Muo,Euo,yuo,Iu,dee,wuo,Auo,MP,Luo,Buo,kuo,ju,cee,xuo,Ruo,EP,Suo,Puo,$uo,Nu,fee,Iuo,juo,yP,Nuo,Duo,quo,Du,mee,Guo,Ouo,wP,Xuo,zuo,Vuo,qu,gee,Wuo,Quo,AP,Huo,Uuo,Juo,Gu,hee,Yuo,Kuo,LP,Zuo,e2o,o2o,Ou,pee,r2o,t2o,BP,a2o,n2o,s2o,Xu,_ee,l2o,i2o,kP,d2o,c2o,f2o,zu,uee,m2o,g2o,xP,h2o,p2o,_2o,Vu,bee,u2o,b2o,RP,v2o,T2o,F2o,Wu,vee,C2o,M2o,SP,E2o,y2o,w2o,Qu,Tee,A2o,L2o,PP,B2o,k2o,x2o,Hu,Fee,R2o,S2o,$P,P2o,$2o,I2o,Uu,Cee,j2o,N2o,IP,D2o,q2o,G2o,Ju,Mee,O2o,X2o,Eee,z2o,V2o,W2o,Yu,yee,Q2o,H2o,jP,U2o,J2o,Y2o,Ku,wee,K2o,Z2o,NP,e1o,o1o,r1o,Zu,Aee,t1o,a1o,DP,n1o,s1o,l1o,e2,Lee,i1o,d1o,qP,c1o,f1o,m1o,o2,g1o,Bee,h1o,p1o,kee,_1o,u1o,xee,b1o,v1o,iE,ELe,Ki,r2,Ree,dE,T1o,See,F1o,yLe,Uo,cE,C1o,Zi,M1o,Pee,E1o,y1o,$ee,w1o,A1o,L1o,fE,B1o,Iee,k1o,x1o,R1o,Or,mE,S1o,jee,P1o,$1o,ed,I1o,Nee,j1o,N1o,Dee,D1o,q1o,G1o,qee,O1o,X1o,gE,z1o,Pe,hE,V1o,Gee,W1o,Q1o,Xa,H1o,Oee,U1o,J1o,Xee,Y1o,K1o,zee,Z1o,ebo,obo,ae,t2,Vee,rbo,tbo,GP,abo,nbo,sbo,a2,Wee,lbo,ibo,OP,dbo,cbo,fbo,n2,Qee,mbo,gbo,XP,hbo,pbo,_bo,s2,Hee,ubo,bbo,zP,vbo,Tbo,Fbo,l2,Uee,Cbo,Mbo,VP,Ebo,ybo,wbo,i2,Jee,Abo,Lbo,WP,Bbo,kbo,xbo,d2,Yee,Rbo,Sbo,QP,Pbo,$bo,Ibo,c2,Kee,jbo,Nbo,HP,Dbo,qbo,Gbo,f2,Zee,Obo,Xbo,UP,zbo,Vbo,Wbo,m2,eoe,Qbo,Hbo,JP,Ubo,Jbo,Ybo,g2,ooe,Kbo,Zbo,YP,e5o,o5o,r5o,h2,roe,t5o,a5o,KP,n5o,s5o,l5o,p2,toe,i5o,d5o,ZP,c5o,f5o,m5o,_2,aoe,g5o,h5o,e$,p5o,_5o,u5o,u2,noe,b5o,v5o,o$,T5o,F5o,C5o,b2,soe,M5o,E5o,r$,y5o,w5o,A5o,v2,L5o,loe,B5o,k5o,ioe,x5o,R5o,doe,S5o,P5o,pE,wLe,od,T2,coe,_E,$5o,foe,I5o,ALe,Jo,uE,j5o,rd,N5o,moe,D5o,q5o,goe,G5o,O5o,X5o,bE,z5o,hoe,V5o,W5o,Q5o,Xr,vE,H5o,poe,U5o,J5o,td,Y5o,_oe,K5o,Z5o,uoe,evo,ovo,rvo,boe,tvo,avo,TE,nvo,$e,FE,svo,voe,lvo,ivo,za,dvo,Toe,cvo,fvo,Foe,mvo,gvo,Coe,hvo,pvo,_vo,A,F2,Moe,uvo,bvo,t$,vvo,Tvo,Fvo,C2,Eoe,Cvo,Mvo,a$,Evo,yvo,wvo,M2,yoe,Avo,Lvo,n$,Bvo,kvo,xvo,E2,woe,Rvo,Svo,s$,Pvo,$vo,Ivo,y2,Aoe,jvo,Nvo,l$,Dvo,qvo,Gvo,w2,Loe,Ovo,Xvo,i$,zvo,Vvo,Wvo,A2,Boe,Qvo,Hvo,d$,Uvo,Jvo,Yvo,L2,koe,Kvo,Zvo,c$,eTo,oTo,rTo,B2,xoe,tTo,aTo,f$,nTo,sTo,lTo,k2,Roe,iTo,dTo,m$,cTo,fTo,mTo,x2,Soe,gTo,hTo,g$,pTo,_To,uTo,R2,Poe,bTo,vTo,h$,TTo,FTo,CTo,S2,$oe,MTo,ETo,p$,yTo,wTo,ATo,P2,Ioe,LTo,BTo,_$,kTo,xTo,RTo,$2,joe,STo,PTo,u$,$To,ITo,jTo,I2,Noe,NTo,DTo,b$,qTo,GTo,OTo,j2,Doe,XTo,zTo,v$,VTo,WTo,QTo,N2,qoe,HTo,UTo,T$,JTo,YTo,KTo,D2,Goe,ZTo,e7o,F$,o7o,r7o,t7o,q2,Ooe,a7o,n7o,C$,s7o,l7o,i7o,G2,Xoe,d7o,c7o,M$,f7o,m7o,g7o,O2,zoe,h7o,p7o,E$,_7o,u7o,b7o,X2,Voe,v7o,T7o,y$,F7o,C7o,M7o,z2,Woe,E7o,y7o,w$,w7o,A7o,L7o,V2,Qoe,B7o,k7o,A$,x7o,R7o,S7o,W2,Hoe,P7o,$7o,L$,I7o,j7o,N7o,Q2,Uoe,D7o,q7o,B$,G7o,O7o,X7o,H2,Joe,z7o,V7o,k$,W7o,Q7o,H7o,U2,Yoe,U7o,J7o,x$,Y7o,K7o,Z7o,J2,Koe,eFo,oFo,R$,rFo,tFo,aFo,Y2,Zoe,nFo,sFo,S$,lFo,iFo,dFo,K2,ere,cFo,fFo,P$,mFo,gFo,hFo,Z2,ore,pFo,_Fo,$$,uFo,bFo,vFo,e1,rre,TFo,FFo,I$,CFo,MFo,EFo,o1,tre,yFo,wFo,j$,AFo,LFo,BFo,r1,are,kFo,xFo,N$,RFo,SFo,PFo,t1,nre,$Fo,IFo,D$,jFo,NFo,DFo,a1,sre,qFo,GFo,q$,OFo,XFo,zFo,n1,lre,VFo,WFo,G$,QFo,HFo,UFo,s1,ire,JFo,YFo,O$,KFo,ZFo,e9o,l1,dre,o9o,r9o,X$,t9o,a9o,n9o,i1,cre,s9o,l9o,z$,i9o,d9o,c9o,d1,fre,f9o,m9o,V$,g9o,h9o,p9o,c1,mre,_9o,u9o,W$,b9o,v9o,T9o,f1,gre,F9o,C9o,Q$,M9o,E9o,y9o,m1,w9o,hre,A9o,L9o,pre,B9o,k9o,_re,x9o,R9o,CE,LLe,ad,g1,ure,ME,S9o,bre,P9o,BLe,Yo,EE,$9o,nd,I9o,vre,j9o,N9o,Tre,D9o,q9o,G9o,yE,O9o,Fre,X9o,z9o,V9o,zr,wE,W9o,Cre,Q9o,H9o,sd,U9o,Mre,J9o,Y9o,Ere,K9o,Z9o,eCo,yre,oCo,rCo,AE,tCo,Ie,LE,aCo,wre,nCo,sCo,Va,lCo,Are,iCo,dCo,Lre,cCo,fCo,Bre,mCo,gCo,hCo,G,h1,kre,pCo,_Co,H$,uCo,bCo,vCo,p1,xre,TCo,FCo,U$,CCo,MCo,ECo,_1,Rre,yCo,wCo,J$,ACo,LCo,BCo,u1,Sre,kCo,xCo,Y$,RCo,SCo,PCo,b1,Pre,$Co,ICo,K$,jCo,NCo,DCo,v1,$re,qCo,GCo,Z$,OCo,XCo,zCo,T1,Ire,VCo,WCo,eI,QCo,HCo,UCo,F1,jre,JCo,YCo,oI,KCo,ZCo,e4o,C1,Nre,o4o,r4o,rI,t4o,a4o,n4o,M1,Dre,s4o,l4o,tI,i4o,d4o,c4o,E1,qre,f4o,m4o,aI,g4o,h4o,p4o,y1,Gre,_4o,u4o,nI,b4o,v4o,T4o,w1,Ore,F4o,C4o,sI,M4o,E4o,y4o,A1,Xre,w4o,A4o,lI,L4o,B4o,k4o,L1,zre,x4o,R4o,iI,S4o,P4o,$4o,B1,Vre,I4o,j4o,dI,N4o,D4o,q4o,k1,Wre,G4o,O4o,cI,X4o,z4o,V4o,x1,Qre,W4o,Q4o,fI,H4o,U4o,J4o,R1,Hre,Y4o,K4o,mI,Z4o,eMo,oMo,S1,Ure,rMo,tMo,gI,aMo,nMo,sMo,P1,Jre,lMo,iMo,hI,dMo,cMo,fMo,$1,Yre,mMo,gMo,pI,hMo,pMo,_Mo,I1,Kre,uMo,bMo,_I,vMo,TMo,FMo,j1,Zre,CMo,MMo,uI,EMo,yMo,wMo,N1,ete,AMo,LMo,bI,BMo,kMo,xMo,D1,ote,RMo,SMo,vI,PMo,$Mo,IMo,q1,rte,jMo,NMo,TI,DMo,qMo,GMo,G1,OMo,tte,XMo,zMo,ate,VMo,WMo,nte,QMo,HMo,BE,kLe,ld,O1,ste,kE,UMo,lte,JMo,xLe,Ko,xE,YMo,id,KMo,ite,ZMo,eEo,dte,oEo,rEo,tEo,RE,aEo,cte,nEo,sEo,lEo,Vr,SE,iEo,fte,dEo,cEo,dd,fEo,mte,mEo,gEo,gte,hEo,pEo,_Eo,hte,uEo,bEo,PE,vEo,je,$E,TEo,pte,FEo,CEo,Wa,MEo,_te,EEo,yEo,ute,wEo,AEo,bte,LEo,BEo,kEo,na,X1,vte,xEo,REo,FI,SEo,PEo,$Eo,z1,Tte,IEo,jEo,CI,NEo,DEo,qEo,V1,Fte,GEo,OEo,MI,XEo,zEo,VEo,W1,Cte,WEo,QEo,EI,HEo,UEo,JEo,Q1,Mte,YEo,KEo,yI,ZEo,e3o,o3o,H1,r3o,Ete,t3o,a3o,yte,n3o,s3o,wte,l3o,i3o,IE,RLe,cd,U1,Ate,jE,d3o,Lte,c3o,SLe,Zo,NE,f3o,fd,m3o,Bte,g3o,h3o,kte,p3o,_3o,u3o,DE,b3o,xte,v3o,T3o,F3o,Wr,qE,C3o,Rte,M3o,E3o,md,y3o,Ste,w3o,A3o,Pte,L3o,B3o,k3o,$te,x3o,R3o,GE,S3o,Ne,OE,P3o,Ite,$3o,I3o,Qa,j3o,jte,N3o,D3o,Nte,q3o,G3o,Dte,O3o,X3o,z3o,D,J1,qte,V3o,W3o,wI,Q3o,H3o,U3o,Y1,Gte,J3o,Y3o,AI,K3o,Z3o,eyo,K1,Ote,oyo,ryo,LI,tyo,ayo,nyo,Z1,Xte,syo,lyo,BI,iyo,dyo,cyo,eb,zte,fyo,myo,kI,gyo,hyo,pyo,ob,Vte,_yo,uyo,xI,byo,vyo,Tyo,rb,Wte,Fyo,Cyo,RI,Myo,Eyo,yyo,tb,Qte,wyo,Ayo,SI,Lyo,Byo,kyo,ab,Hte,xyo,Ryo,PI,Syo,Pyo,$yo,nb,Ute,Iyo,jyo,$I,Nyo,Dyo,qyo,sb,Jte,Gyo,Oyo,II,Xyo,zyo,Vyo,lb,Yte,Wyo,Qyo,jI,Hyo,Uyo,Jyo,ib,Kte,Yyo,Kyo,NI,Zyo,ewo,owo,db,Zte,rwo,two,DI,awo,nwo,swo,cb,eae,lwo,iwo,qI,dwo,cwo,fwo,fb,oae,mwo,gwo,GI,hwo,pwo,_wo,mb,rae,uwo,bwo,OI,vwo,Two,Fwo,gb,tae,Cwo,Mwo,XI,Ewo,ywo,wwo,hb,aae,Awo,Lwo,zI,Bwo,kwo,xwo,pb,nae,Rwo,Swo,VI,Pwo,$wo,Iwo,_b,sae,jwo,Nwo,WI,Dwo,qwo,Gwo,ub,lae,Owo,Xwo,QI,zwo,Vwo,Wwo,bb,iae,Qwo,Hwo,HI,Uwo,Jwo,Ywo,vb,dae,Kwo,Zwo,UI,eAo,oAo,rAo,Tb,cae,tAo,aAo,JI,nAo,sAo,lAo,Fb,fae,iAo,dAo,YI,cAo,fAo,mAo,Cb,mae,gAo,hAo,KI,pAo,_Ao,uAo,Mb,gae,bAo,vAo,ZI,TAo,FAo,CAo,Eb,hae,MAo,EAo,ej,yAo,wAo,AAo,yb,pae,LAo,BAo,oj,kAo,xAo,RAo,wb,_ae,SAo,PAo,rj,$Ao,IAo,jAo,Ab,uae,NAo,DAo,tj,qAo,GAo,OAo,Lb,XAo,bae,zAo,VAo,vae,WAo,QAo,Tae,HAo,UAo,XE,PLe,gd,Bb,Fae,zE,JAo,Cae,YAo,$Le,er,VE,KAo,hd,ZAo,Mae,e6o,o6o,Eae,r6o,t6o,a6o,WE,n6o,yae,s6o,l6o,i6o,Qr,QE,d6o,wae,c6o,f6o,pd,m6o,Aae,g6o,h6o,Lae,p6o,_6o,u6o,Bae,b6o,v6o,HE,T6o,De,UE,F6o,kae,C6o,M6o,Ha,E6o,xae,y6o,w6o,Rae,A6o,L6o,Sae,B6o,k6o,x6o,R,kb,Pae,R6o,S6o,aj,P6o,$6o,I6o,xb,$ae,j6o,N6o,nj,D6o,q6o,G6o,Rb,Iae,O6o,X6o,sj,z6o,V6o,W6o,Sb,jae,Q6o,H6o,lj,U6o,J6o,Y6o,Pb,Nae,K6o,Z6o,ij,e0o,o0o,r0o,$b,Dae,t0o,a0o,dj,n0o,s0o,l0o,Ib,qae,i0o,d0o,cj,c0o,f0o,m0o,jb,Gae,g0o,h0o,fj,p0o,_0o,u0o,Nb,Oae,b0o,v0o,mj,T0o,F0o,C0o,Db,Xae,M0o,E0o,gj,y0o,w0o,A0o,qb,zae,L0o,B0o,hj,k0o,x0o,R0o,Gb,Vae,S0o,P0o,pj,$0o,I0o,j0o,Ob,Wae,N0o,D0o,_j,q0o,G0o,O0o,Xb,Qae,X0o,z0o,uj,V0o,W0o,Q0o,zb,Hae,H0o,U0o,bj,J0o,Y0o,K0o,Vb,Uae,Z0o,eLo,vj,oLo,rLo,tLo,Wb,Jae,aLo,nLo,Tj,sLo,lLo,iLo,Qb,Yae,dLo,cLo,Fj,fLo,mLo,gLo,Hb,Kae,hLo,pLo,Cj,_Lo,uLo,bLo,Ub,Zae,vLo,TLo,Mj,FLo,CLo,MLo,Jb,ene,ELo,yLo,Ej,wLo,ALo,LLo,Yb,one,BLo,kLo,yj,xLo,RLo,SLo,Kb,rne,PLo,$Lo,wj,ILo,jLo,NLo,Zb,tne,DLo,qLo,Aj,GLo,OLo,XLo,e5,ane,zLo,VLo,Lj,WLo,QLo,HLo,o5,nne,ULo,JLo,Bj,YLo,KLo,ZLo,r5,sne,e8o,o8o,kj,r8o,t8o,a8o,t5,lne,n8o,s8o,xj,l8o,i8o,d8o,a5,ine,c8o,f8o,Rj,m8o,g8o,h8o,n5,dne,p8o,_8o,Sj,u8o,b8o,v8o,s5,cne,T8o,F8o,Pj,C8o,M8o,E8o,l5,fne,y8o,w8o,$j,A8o,L8o,B8o,i5,mne,k8o,x8o,Ij,R8o,S8o,P8o,d5,gne,$8o,I8o,jj,j8o,N8o,D8o,c5,hne,q8o,G8o,Nj,O8o,X8o,z8o,f5,pne,V8o,W8o,Dj,Q8o,H8o,U8o,m5,_ne,J8o,Y8o,qj,K8o,Z8o,eBo,g5,une,oBo,rBo,Gj,tBo,aBo,nBo,h5,sBo,bne,lBo,iBo,vne,dBo,cBo,Tne,fBo,mBo,JE,ILe,_d,p5,Fne,YE,gBo,Cne,hBo,jLe,or,KE,pBo,ud,_Bo,Mne,uBo,bBo,Ene,vBo,TBo,FBo,ZE,CBo,yne,MBo,EBo,yBo,Hr,e3,wBo,wne,ABo,LBo,bd,BBo,Ane,kBo,xBo,Lne,RBo,SBo,PBo,Bne,$Bo,IBo,o3,jBo,qe,r3,NBo,kne,DBo,qBo,Ua,GBo,xne,OBo,XBo,Rne,zBo,VBo,Sne,WBo,QBo,HBo,Pne,_5,$ne,UBo,JBo,Oj,YBo,KBo,ZBo,u5,eko,Ine,oko,rko,jne,tko,ako,Nne,nko,sko,t3,NLe,vd,b5,Dne,a3,lko,qne,iko,DLe,rr,n3,dko,Td,cko,Gne,fko,mko,One,gko,hko,pko,s3,_ko,Xne,uko,bko,vko,Ur,l3,Tko,zne,Fko,Cko,Fd,Mko,Vne,Eko,yko,Wne,wko,Ako,Lko,Qne,Bko,kko,i3,xko,Ge,d3,Rko,Hne,Sko,Pko,Ja,$ko,Une,Iko,jko,Jne,Nko,Dko,Yne,qko,Gko,Oko,be,v5,Kne,Xko,zko,Xj,Vko,Wko,Qko,T5,Zne,Hko,Uko,zj,Jko,Yko,Kko,Rs,ese,Zko,exo,Vj,oxo,rxo,Wj,txo,axo,nxo,F5,ose,sxo,lxo,Qj,ixo,dxo,cxo,la,rse,fxo,mxo,Hj,gxo,hxo,Uj,pxo,_xo,Jj,uxo,bxo,vxo,C5,tse,Txo,Fxo,Yj,Cxo,Mxo,Exo,M5,ase,yxo,wxo,Kj,Axo,Lxo,Bxo,E5,nse,kxo,xxo,Zj,Rxo,Sxo,Pxo,y5,sse,$xo,Ixo,eN,jxo,Nxo,Dxo,w5,qxo,lse,Gxo,Oxo,ise,Xxo,zxo,dse,Vxo,Wxo,c3,qLe,Cd,A5,cse,f3,Qxo,fse,Hxo,GLe,tr,m3,Uxo,Md,Jxo,mse,Yxo,Kxo,gse,Zxo,eRo,oRo,g3,rRo,hse,tRo,aRo,nRo,Jr,h3,sRo,pse,lRo,iRo,Ed,dRo,_se,cRo,fRo,use,mRo,gRo,hRo,bse,pRo,_Ro,p3,uRo,Oe,_3,bRo,vse,vRo,TRo,Ya,FRo,Tse,CRo,MRo,Fse,ERo,yRo,Cse,wRo,ARo,LRo,Mse,L5,Ese,BRo,kRo,oN,xRo,RRo,SRo,B5,PRo,yse,$Ro,IRo,wse,jRo,NRo,Ase,DRo,qRo,u3,OLe,yd,k5,Lse,b3,GRo,Bse,ORo,XLe,ar,v3,XRo,wd,zRo,kse,VRo,WRo,xse,QRo,HRo,URo,T3,JRo,Rse,YRo,KRo,ZRo,Yr,F3,eSo,Sse,oSo,rSo,Ad,tSo,Pse,aSo,nSo,$se,sSo,lSo,iSo,Ise,dSo,cSo,C3,fSo,Xe,M3,mSo,jse,gSo,hSo,Ka,pSo,Nse,_So,uSo,Dse,bSo,vSo,qse,TSo,FSo,CSo,ao,x5,Gse,MSo,ESo,rN,ySo,wSo,ASo,R5,Ose,LSo,BSo,tN,kSo,xSo,RSo,S5,Xse,SSo,PSo,aN,$So,ISo,jSo,P5,zse,NSo,DSo,nN,qSo,GSo,OSo,$5,Vse,XSo,zSo,sN,VSo,WSo,QSo,I5,Wse,HSo,USo,lN,JSo,YSo,KSo,j5,Qse,ZSo,ePo,iN,oPo,rPo,tPo,N5,aPo,Hse,nPo,sPo,Use,lPo,iPo,Jse,dPo,cPo,E3,zLe,Ld,D5,Yse,y3,fPo,Kse,mPo,VLe,nr,w3,gPo,Bd,hPo,Zse,pPo,_Po,ele,uPo,bPo,vPo,A3,TPo,ole,FPo,CPo,MPo,Kr,L3,EPo,rle,yPo,wPo,kd,APo,tle,LPo,BPo,ale,kPo,xPo,RPo,nle,SPo,PPo,B3,$Po,ze,k3,IPo,sle,jPo,NPo,Za,DPo,lle,qPo,GPo,ile,OPo,XPo,dle,zPo,VPo,WPo,xd,q5,cle,QPo,HPo,dN,UPo,JPo,YPo,G5,fle,KPo,ZPo,cN,e$o,o$o,r$o,O5,mle,t$o,a$o,fN,n$o,s$o,l$o,X5,i$o,gle,d$o,c$o,hle,f$o,m$o,ple,g$o,h$o,x3,WLe,Rd,z5,_le,R3,p$o,ule,_$o,QLe,sr,S3,u$o,Sd,b$o,ble,v$o,T$o,vle,F$o,C$o,M$o,P3,E$o,Tle,y$o,w$o,A$o,Zr,$3,L$o,Fle,B$o,k$o,Pd,x$o,Cle,R$o,S$o,Mle,P$o,$$o,I$o,Ele,j$o,N$o,I3,D$o,Ve,j3,q$o,yle,G$o,O$o,en,X$o,wle,z$o,V$o,Ale,W$o,Q$o,Lle,H$o,U$o,J$o,no,V5,Ble,Y$o,K$o,mN,Z$o,eIo,oIo,W5,kle,rIo,tIo,gN,aIo,nIo,sIo,Q5,xle,lIo,iIo,hN,dIo,cIo,fIo,H5,Rle,mIo,gIo,pN,hIo,pIo,_Io,U5,Sle,uIo,bIo,_N,vIo,TIo,FIo,J5,Ple,CIo,MIo,uN,EIo,yIo,wIo,Y5,$le,AIo,LIo,bN,BIo,kIo,xIo,K5,RIo,Ile,SIo,PIo,jle,$Io,IIo,Nle,jIo,NIo,N3,HLe,$d,Z5,Dle,D3,DIo,qle,qIo,ULe,lr,q3,GIo,Id,OIo,Gle,XIo,zIo,Ole,VIo,WIo,QIo,G3,HIo,Xle,UIo,JIo,YIo,et,O3,KIo,zle,ZIo,ejo,jd,ojo,Vle,rjo,tjo,Wle,ajo,njo,sjo,Qle,ljo,ijo,X3,djo,We,z3,cjo,Hle,fjo,mjo,on,gjo,Ule,hjo,pjo,Jle,_jo,ujo,Yle,bjo,vjo,Tjo,V3,ev,Kle,Fjo,Cjo,vN,Mjo,Ejo,yjo,ov,Zle,wjo,Ajo,TN,Ljo,Bjo,kjo,rv,xjo,eie,Rjo,Sjo,oie,Pjo,$jo,rie,Ijo,jjo,W3,JLe,Nd,tv,tie,Q3,Njo,aie,Djo,YLe,ir,H3,qjo,Dd,Gjo,nie,Ojo,Xjo,sie,zjo,Vjo,Wjo,U3,Qjo,lie,Hjo,Ujo,Jjo,ot,J3,Yjo,iie,Kjo,Zjo,qd,eNo,die,oNo,rNo,cie,tNo,aNo,nNo,fie,sNo,lNo,Y3,iNo,Qe,K3,dNo,mie,cNo,fNo,rn,mNo,gie,gNo,hNo,hie,pNo,_No,pie,uNo,bNo,vNo,Gd,av,_ie,TNo,FNo,FN,CNo,MNo,ENo,nv,uie,yNo,wNo,CN,ANo,LNo,BNo,sv,bie,kNo,xNo,MN,RNo,SNo,PNo,lv,$No,vie,INo,jNo,Tie,NNo,DNo,Fie,qNo,GNo,Z3,KLe,Od,iv,Cie,ey,ONo,Mie,XNo,ZLe,dr,oy,zNo,Xd,VNo,Eie,WNo,QNo,yie,HNo,UNo,JNo,ry,YNo,wie,KNo,ZNo,eDo,rt,ty,oDo,Aie,rDo,tDo,zd,aDo,Lie,nDo,sDo,Bie,lDo,iDo,dDo,kie,cDo,fDo,ay,mDo,He,ny,gDo,xie,hDo,pDo,tn,_Do,Rie,uDo,bDo,Sie,vDo,TDo,Pie,FDo,CDo,MDo,Vd,dv,$ie,EDo,yDo,EN,wDo,ADo,LDo,cv,Iie,BDo,kDo,yN,xDo,RDo,SDo,fv,jie,PDo,$Do,wN,IDo,jDo,NDo,mv,DDo,Nie,qDo,GDo,Die,ODo,XDo,qie,zDo,VDo,sy,e8e,Wd,gv,Gie,ly,WDo,Oie,QDo,o8e,cr,iy,HDo,Qd,UDo,Xie,JDo,YDo,zie,KDo,ZDo,eqo,dy,oqo,Vie,rqo,tqo,aqo,tt,cy,nqo,Wie,sqo,lqo,Hd,iqo,Qie,dqo,cqo,Hie,fqo,mqo,gqo,Uie,hqo,pqo,fy,_qo,Ue,my,uqo,Jie,bqo,vqo,an,Tqo,Yie,Fqo,Cqo,Kie,Mqo,Eqo,Zie,yqo,wqo,Aqo,ede,hv,ode,Lqo,Bqo,AN,kqo,xqo,Rqo,pv,Sqo,rde,Pqo,$qo,tde,Iqo,jqo,ade,Nqo,Dqo,gy,r8e,Ud,_v,nde,hy,qqo,sde,Gqo,t8e,fr,py,Oqo,Jd,Xqo,lde,zqo,Vqo,ide,Wqo,Qqo,Hqo,_y,Uqo,dde,Jqo,Yqo,Kqo,at,uy,Zqo,cde,eGo,oGo,Yd,rGo,fde,tGo,aGo,mde,nGo,sGo,lGo,gde,iGo,dGo,by,cGo,Je,vy,fGo,hde,mGo,gGo,nn,hGo,pde,pGo,_Go,_de,uGo,bGo,ude,vGo,TGo,FGo,bde,uv,vde,CGo,MGo,LN,EGo,yGo,wGo,bv,AGo,Tde,LGo,BGo,Fde,kGo,xGo,Cde,RGo,SGo,Ty,a8e,Kd,vv,Mde,Fy,PGo,Ede,$Go,n8e,mr,Cy,IGo,Zd,jGo,yde,NGo,DGo,wde,qGo,GGo,OGo,My,XGo,Ade,zGo,VGo,WGo,nt,Ey,QGo,Lde,HGo,UGo,ec,JGo,Bde,YGo,KGo,kde,ZGo,eOo,oOo,xde,rOo,tOo,yy,aOo,Ye,wy,nOo,Rde,sOo,lOo,sn,iOo,Sde,dOo,cOo,Pde,fOo,mOo,$de,gOo,hOo,pOo,Ay,Tv,Ide,_Oo,uOo,BN,bOo,vOo,TOo,Fv,jde,FOo,COo,kN,MOo,EOo,yOo,Cv,wOo,Nde,AOo,LOo,Dde,BOo,kOo,qde,xOo,ROo,Ly,s8e,oc,Mv,Gde,By,SOo,Ode,POo,l8e,gr,ky,$Oo,rc,IOo,Xde,jOo,NOo,zde,DOo,qOo,GOo,xy,OOo,Vde,XOo,zOo,VOo,st,Ry,WOo,Wde,QOo,HOo,tc,UOo,Qde,JOo,YOo,Hde,KOo,ZOo,eXo,Ude,oXo,rXo,Sy,tXo,go,Py,aXo,Jde,nXo,sXo,ln,lXo,Yde,iXo,dXo,Kde,cXo,fXo,Zde,mXo,gXo,hXo,B,Ev,ece,pXo,_Xo,xN,uXo,bXo,vXo,yv,oce,TXo,FXo,RN,CXo,MXo,EXo,wv,rce,yXo,wXo,SN,AXo,LXo,BXo,Av,tce,kXo,xXo,PN,RXo,SXo,PXo,Lv,ace,$Xo,IXo,$N,jXo,NXo,DXo,Bv,nce,qXo,GXo,IN,OXo,XXo,zXo,kv,sce,VXo,WXo,jN,QXo,HXo,UXo,xv,lce,JXo,YXo,NN,KXo,ZXo,ezo,Rv,ice,ozo,rzo,DN,tzo,azo,nzo,Sv,dce,szo,lzo,qN,izo,dzo,czo,Pv,cce,fzo,mzo,GN,gzo,hzo,pzo,$v,fce,_zo,uzo,ON,bzo,vzo,Tzo,Iv,mce,Fzo,Czo,XN,Mzo,Ezo,yzo,jv,gce,wzo,Azo,zN,Lzo,Bzo,kzo,Nv,hce,xzo,Rzo,VN,Szo,Pzo,$zo,Ss,pce,Izo,jzo,WN,Nzo,Dzo,QN,qzo,Gzo,Ozo,Dv,_ce,Xzo,zzo,HN,Vzo,Wzo,Qzo,qv,uce,Hzo,Uzo,UN,Jzo,Yzo,Kzo,Gv,bce,Zzo,eVo,JN,oVo,rVo,tVo,Ov,vce,aVo,nVo,YN,sVo,lVo,iVo,Xv,Tce,dVo,cVo,KN,fVo,mVo,gVo,zv,Fce,hVo,pVo,ZN,_Vo,uVo,bVo,Vv,Cce,vVo,TVo,eD,FVo,CVo,MVo,Wv,Mce,EVo,yVo,oD,wVo,AVo,LVo,Qv,Ece,BVo,kVo,rD,xVo,RVo,SVo,Hv,yce,PVo,$Vo,tD,IVo,jVo,NVo,Uv,wce,DVo,qVo,aD,GVo,OVo,XVo,Jv,Ace,zVo,VVo,nD,WVo,QVo,HVo,Yv,Lce,UVo,JVo,sD,YVo,KVo,ZVo,Kv,Bce,eWo,oWo,lD,rWo,tWo,aWo,Zv,kce,nWo,sWo,iD,lWo,iWo,dWo,eT,xce,cWo,fWo,dD,mWo,gWo,hWo,oT,Rce,pWo,_Wo,cD,uWo,bWo,vWo,rT,Sce,TWo,FWo,fD,CWo,MWo,EWo,tT,Pce,yWo,wWo,mD,AWo,LWo,BWo,aT,$ce,kWo,xWo,gD,RWo,SWo,PWo,nT,Ice,$Wo,IWo,hD,jWo,NWo,DWo,sT,jce,qWo,GWo,pD,OWo,XWo,zWo,lT,Nce,VWo,WWo,_D,QWo,HWo,UWo,iT,Dce,JWo,YWo,uD,KWo,ZWo,eQo,dT,qce,oQo,rQo,bD,tQo,aQo,nQo,Gce,sQo,lQo,$y,i8e,ac,cT,Oce,Iy,iQo,Xce,dQo,d8e,hr,jy,cQo,nc,fQo,zce,mQo,gQo,Vce,hQo,pQo,_Qo,Ny,uQo,Wce,bQo,vQo,TQo,lt,Dy,FQo,Qce,CQo,MQo,sc,EQo,Hce,yQo,wQo,Uce,AQo,LQo,BQo,Jce,kQo,xQo,qy,RQo,ho,Gy,SQo,Yce,PQo,$Qo,dn,IQo,Kce,jQo,NQo,Zce,DQo,qQo,efe,GQo,OQo,XQo,H,fT,ofe,zQo,VQo,vD,WQo,QQo,HQo,mT,rfe,UQo,JQo,TD,YQo,KQo,ZQo,gT,tfe,eHo,oHo,FD,rHo,tHo,aHo,hT,afe,nHo,sHo,CD,lHo,iHo,dHo,pT,nfe,cHo,fHo,MD,mHo,gHo,hHo,_T,sfe,pHo,_Ho,ED,uHo,bHo,vHo,uT,lfe,THo,FHo,yD,CHo,MHo,EHo,bT,ife,yHo,wHo,wD,AHo,LHo,BHo,vT,dfe,kHo,xHo,AD,RHo,SHo,PHo,TT,cfe,$Ho,IHo,LD,jHo,NHo,DHo,FT,ffe,qHo,GHo,BD,OHo,XHo,zHo,CT,mfe,VHo,WHo,kD,QHo,HHo,UHo,MT,gfe,JHo,YHo,xD,KHo,ZHo,eUo,ET,hfe,oUo,rUo,RD,tUo,aUo,nUo,yT,pfe,sUo,lUo,SD,iUo,dUo,cUo,wT,_fe,fUo,mUo,PD,gUo,hUo,pUo,AT,ufe,_Uo,uUo,$D,bUo,vUo,TUo,LT,bfe,FUo,CUo,ID,MUo,EUo,yUo,BT,vfe,wUo,AUo,jD,LUo,BUo,kUo,kT,Tfe,xUo,RUo,ND,SUo,PUo,$Uo,xT,Ffe,IUo,jUo,DD,NUo,DUo,qUo,RT,Cfe,GUo,OUo,qD,XUo,zUo,VUo,Mfe,WUo,QUo,Oy,c8e,lc,ST,Efe,Xy,HUo,yfe,UUo,f8e,pr,zy,JUo,ic,YUo,wfe,KUo,ZUo,Afe,eJo,oJo,rJo,Vy,tJo,Lfe,aJo,nJo,sJo,it,Wy,lJo,Bfe,iJo,dJo,dc,cJo,kfe,fJo,mJo,xfe,gJo,hJo,pJo,Rfe,_Jo,uJo,Qy,bJo,po,Hy,vJo,Sfe,TJo,FJo,cn,CJo,Pfe,MJo,EJo,$fe,yJo,wJo,Ife,AJo,LJo,BJo,he,PT,jfe,kJo,xJo,GD,RJo,SJo,PJo,$T,Nfe,$Jo,IJo,OD,jJo,NJo,DJo,IT,Dfe,qJo,GJo,XD,OJo,XJo,zJo,jT,qfe,VJo,WJo,zD,QJo,HJo,UJo,NT,Gfe,JJo,YJo,VD,KJo,ZJo,eYo,DT,Ofe,oYo,rYo,WD,tYo,aYo,nYo,qT,Xfe,sYo,lYo,QD,iYo,dYo,cYo,GT,zfe,fYo,mYo,HD,gYo,hYo,pYo,OT,Vfe,_Yo,uYo,UD,bYo,vYo,TYo,XT,Wfe,FYo,CYo,JD,MYo,EYo,yYo,Qfe,wYo,AYo,Uy,m8e,cc,zT,Hfe,Jy,LYo,Ufe,BYo,g8e,_r,Yy,kYo,fc,xYo,Jfe,RYo,SYo,Yfe,PYo,$Yo,IYo,Ky,jYo,Kfe,NYo,DYo,qYo,dt,Zy,GYo,Zfe,OYo,XYo,mc,zYo,eme,VYo,WYo,ome,QYo,HYo,UYo,rme,JYo,YYo,ew,KYo,_o,ow,ZYo,tme,eKo,oKo,fn,rKo,ame,tKo,aKo,nme,nKo,sKo,sme,lKo,iKo,dKo,lme,VT,ime,cKo,fKo,YD,mKo,gKo,hKo,dme,pKo,_Ko,rw,h8e,gc,WT,cme,tw,uKo,fme,bKo,p8e,ur,aw,vKo,hc,TKo,mme,FKo,CKo,gme,MKo,EKo,yKo,nw,wKo,hme,AKo,LKo,BKo,ct,sw,kKo,pme,xKo,RKo,pc,SKo,_me,PKo,$Ko,ume,IKo,jKo,NKo,bme,DKo,qKo,lw,GKo,uo,iw,OKo,vme,XKo,zKo,mn,VKo,Tme,WKo,QKo,Fme,HKo,UKo,Cme,JKo,YKo,KKo,Y,QT,Mme,ZKo,eZo,KD,oZo,rZo,tZo,HT,Eme,aZo,nZo,ZD,sZo,lZo,iZo,UT,yme,dZo,cZo,eq,fZo,mZo,gZo,JT,wme,hZo,pZo,oq,_Zo,uZo,bZo,YT,Ame,vZo,TZo,rq,FZo,CZo,MZo,KT,Lme,EZo,yZo,tq,wZo,AZo,LZo,ZT,Bme,BZo,kZo,aq,xZo,RZo,SZo,e7,kme,PZo,$Zo,nq,IZo,jZo,NZo,o7,xme,DZo,qZo,sq,GZo,OZo,XZo,r7,Rme,zZo,VZo,lq,WZo,QZo,HZo,t7,Sme,UZo,JZo,iq,YZo,KZo,ZZo,a7,Pme,eer,oer,dq,rer,ter,aer,n7,$me,ner,ser,cq,ler,ier,der,s7,Ime,cer,fer,fq,mer,ger,her,l7,jme,per,_er,mq,uer,ber,ver,i7,Nme,Ter,Fer,gq,Cer,Mer,Eer,d7,Dme,yer,wer,hq,Aer,Ler,Ber,c7,qme,ker,xer,pq,Rer,Ser,Per,f7,Gme,$er,Ier,_q,jer,Ner,Der,m7,Ome,qer,Ger,uq,Oer,Xer,zer,Xme,Ver,Wer,dw,_8e,_c,g7,zme,cw,Qer,Vme,Her,u8e,br,fw,Uer,uc,Jer,Wme,Yer,Ker,Qme,Zer,eor,oor,mw,ror,Hme,tor,aor,nor,ft,gw,sor,Ume,lor,ior,bc,dor,Jme,cor,mor,Yme,gor,hor,por,Kme,_or,uor,hw,bor,bo,pw,vor,Zme,Tor,For,gn,Cor,ege,Mor,Eor,oge,yor,wor,rge,Aor,Lor,Bor,pe,h7,tge,kor,xor,bq,Ror,Sor,Por,p7,age,$or,Ior,vq,jor,Nor,Dor,_7,nge,qor,Gor,Tq,Oor,Xor,zor,u7,sge,Vor,Wor,Fq,Qor,Hor,Uor,b7,lge,Jor,Yor,Cq,Kor,Zor,err,v7,ige,orr,rrr,Mq,trr,arr,nrr,T7,dge,srr,lrr,Eq,irr,drr,crr,F7,cge,frr,mrr,yq,grr,hrr,prr,C7,fge,_rr,urr,wq,brr,vrr,Trr,M7,mge,Frr,Crr,Aq,Mrr,Err,yrr,gge,wrr,Arr,_w,b8e,vc,E7,hge,uw,Lrr,pge,Brr,v8e,vr,bw,krr,Tc,xrr,_ge,Rrr,Srr,uge,Prr,$rr,Irr,vw,jrr,bge,Nrr,Drr,qrr,mt,Tw,Grr,vge,Orr,Xrr,Fc,zrr,Tge,Vrr,Wrr,Fge,Qrr,Hrr,Urr,Cge,Jrr,Yrr,Fw,Krr,vo,Cw,Zrr,Mge,etr,otr,hn,rtr,Ege,ttr,atr,yge,ntr,str,wge,ltr,itr,dtr,X,y7,Age,ctr,ftr,Lq,mtr,gtr,htr,w7,Lge,ptr,_tr,Bq,utr,btr,vtr,A7,Bge,Ttr,Ftr,kq,Ctr,Mtr,Etr,L7,kge,ytr,wtr,xq,Atr,Ltr,Btr,B7,xge,ktr,xtr,Rq,Rtr,Str,Ptr,k7,Rge,$tr,Itr,Sq,jtr,Ntr,Dtr,x7,Sge,qtr,Gtr,Pq,Otr,Xtr,ztr,R7,Pge,Vtr,Wtr,$q,Qtr,Htr,Utr,S7,$ge,Jtr,Ytr,Iq,Ktr,Ztr,ear,P7,Ige,oar,rar,jq,tar,aar,nar,$7,jge,sar,lar,Nq,iar,dar,car,I7,Nge,far,mar,Dq,gar,har,par,j7,Dge,_ar,uar,qq,bar,Tar,Far,N7,qge,Car,Mar,Gq,Ear,yar,war,D7,Gge,Aar,Lar,Oq,Bar,kar,xar,q7,Oge,Rar,Sar,Xq,Par,$ar,Iar,G7,Xge,jar,Nar,zq,Dar,qar,Gar,O7,zge,Oar,Xar,Vq,zar,Var,War,X7,Vge,Qar,Har,Wq,Uar,Jar,Yar,z7,Wge,Kar,Zar,Qq,enr,onr,rnr,V7,Qge,tnr,anr,Hq,nnr,snr,lnr,W7,Hge,inr,dnr,Uq,cnr,fnr,mnr,Q7,Uge,gnr,hnr,Jq,pnr,_nr,unr,H7,Jge,bnr,vnr,Yq,Tnr,Fnr,Cnr,U7,Yge,Mnr,Enr,Kq,ynr,wnr,Anr,Kge,Lnr,Bnr,Mw,T8e,Cc,J7,Zge,Ew,knr,ehe,xnr,F8e,Tr,yw,Rnr,Mc,Snr,ohe,Pnr,$nr,rhe,Inr,jnr,Nnr,ww,Dnr,the,qnr,Gnr,Onr,gt,Aw,Xnr,ahe,znr,Vnr,Ec,Wnr,nhe,Qnr,Hnr,she,Unr,Jnr,Ynr,lhe,Knr,Znr,Lw,esr,To,Bw,osr,ihe,rsr,tsr,pn,asr,dhe,nsr,ssr,che,lsr,isr,fhe,dsr,csr,fsr,te,Y7,mhe,msr,gsr,Zq,hsr,psr,_sr,K7,ghe,usr,bsr,eG,vsr,Tsr,Fsr,Z7,hhe,Csr,Msr,oG,Esr,ysr,wsr,eF,phe,Asr,Lsr,rG,Bsr,ksr,xsr,oF,_he,Rsr,Ssr,tG,Psr,$sr,Isr,rF,uhe,jsr,Nsr,aG,Dsr,qsr,Gsr,tF,bhe,Osr,Xsr,nG,zsr,Vsr,Wsr,aF,vhe,Qsr,Hsr,sG,Usr,Jsr,Ysr,nF,The,Ksr,Zsr,lG,elr,olr,rlr,sF,Fhe,tlr,alr,iG,nlr,slr,llr,lF,Che,ilr,dlr,dG,clr,flr,mlr,iF,Mhe,glr,hlr,cG,plr,_lr,ulr,dF,Ehe,blr,vlr,fG,Tlr,Flr,Clr,cF,yhe,Mlr,Elr,mG,ylr,wlr,Alr,fF,whe,Llr,Blr,gG,klr,xlr,Rlr,mF,Ahe,Slr,Plr,hG,$lr,Ilr,jlr,gF,Lhe,Nlr,Dlr,pG,qlr,Glr,Olr,Bhe,Xlr,zlr,kw,C8e,yc,hF,khe,xw,Vlr,xhe,Wlr,M8e,Fr,Rw,Qlr,wc,Hlr,Rhe,Ulr,Jlr,She,Ylr,Klr,Zlr,Sw,eir,Phe,oir,rir,tir,ht,Pw,air,$he,nir,sir,Ac,lir,Ihe,iir,dir,jhe,cir,fir,mir,Nhe,gir,hir,$w,pir,Fo,Iw,_ir,Dhe,uir,bir,_n,vir,qhe,Tir,Fir,Ghe,Cir,Mir,Ohe,Eir,yir,wir,Xhe,pF,zhe,Air,Lir,_G,Bir,kir,xir,Vhe,Rir,Sir,jw,E8e,Lc,_F,Whe,Nw,Pir,Qhe,$ir,y8e,Cr,Dw,Iir,Bc,jir,Hhe,Nir,Dir,Uhe,qir,Gir,Oir,qw,Xir,Jhe,zir,Vir,Wir,pt,Gw,Qir,Yhe,Hir,Uir,kc,Jir,Khe,Yir,Kir,Zhe,Zir,edr,odr,epe,rdr,tdr,Ow,adr,Co,Xw,ndr,ope,sdr,ldr,un,idr,rpe,ddr,cdr,tpe,fdr,mdr,ape,gdr,hdr,pdr,K,uF,npe,_dr,udr,uG,bdr,vdr,Tdr,bF,spe,Fdr,Cdr,bG,Mdr,Edr,ydr,vF,lpe,wdr,Adr,vG,Ldr,Bdr,kdr,TF,ipe,xdr,Rdr,TG,Sdr,Pdr,$dr,FF,dpe,Idr,jdr,FG,Ndr,Ddr,qdr,CF,cpe,Gdr,Odr,CG,Xdr,zdr,Vdr,MF,fpe,Wdr,Qdr,MG,Hdr,Udr,Jdr,EF,mpe,Ydr,Kdr,EG,Zdr,ecr,ocr,yF,gpe,rcr,tcr,yG,acr,ncr,scr,wF,hpe,lcr,icr,wG,dcr,ccr,fcr,AF,ppe,mcr,gcr,AG,hcr,pcr,_cr,LF,_pe,ucr,bcr,LG,vcr,Tcr,Fcr,BF,upe,Ccr,Mcr,BG,Ecr,ycr,wcr,kF,bpe,Acr,Lcr,kG,Bcr,kcr,xcr,xF,vpe,Rcr,Scr,xG,Pcr,$cr,Icr,RF,Tpe,jcr,Ncr,RG,Dcr,qcr,Gcr,SF,Fpe,Ocr,Xcr,SG,zcr,Vcr,Wcr,PF,Cpe,Qcr,Hcr,PG,Ucr,Jcr,Ycr,$F,Mpe,Kcr,Zcr,$G,efr,ofr,rfr,IF,Epe,tfr,afr,IG,nfr,sfr,lfr,ype,ifr,dfr,zw,w8e,xc,jF,wpe,Vw,cfr,Ape,ffr,A8e,Mr,Ww,mfr,Rc,gfr,Lpe,hfr,pfr,Bpe,_fr,ufr,bfr,Qw,vfr,kpe,Tfr,Ffr,Cfr,_t,Hw,Mfr,xpe,Efr,yfr,Sc,wfr,Rpe,Afr,Lfr,Spe,Bfr,kfr,xfr,Ppe,Rfr,Sfr,Uw,Pfr,Mo,Jw,$fr,$pe,Ifr,jfr,bn,Nfr,Ipe,Dfr,qfr,jpe,Gfr,Ofr,Npe,Xfr,zfr,Vfr,Z,NF,Dpe,Wfr,Qfr,jG,Hfr,Ufr,Jfr,DF,qpe,Yfr,Kfr,NG,Zfr,emr,omr,qF,Gpe,rmr,tmr,DG,amr,nmr,smr,GF,Ope,lmr,imr,qG,dmr,cmr,fmr,OF,Xpe,mmr,gmr,GG,hmr,pmr,_mr,XF,zpe,umr,bmr,OG,vmr,Tmr,Fmr,zF,Vpe,Cmr,Mmr,XG,Emr,ymr,wmr,VF,Wpe,Amr,Lmr,zG,Bmr,kmr,xmr,WF,Qpe,Rmr,Smr,VG,Pmr,$mr,Imr,QF,Hpe,jmr,Nmr,WG,Dmr,qmr,Gmr,HF,Upe,Omr,Xmr,QG,zmr,Vmr,Wmr,UF,Jpe,Qmr,Hmr,HG,Umr,Jmr,Ymr,JF,Ype,Kmr,Zmr,UG,egr,ogr,rgr,YF,Kpe,tgr,agr,JG,ngr,sgr,lgr,KF,Zpe,igr,dgr,YG,cgr,fgr,mgr,ZF,e_e,ggr,hgr,KG,pgr,_gr,ugr,e9,o_e,bgr,vgr,ZG,Tgr,Fgr,Cgr,o9,r_e,Mgr,Egr,eO,ygr,wgr,Agr,r9,t_e,Lgr,Bgr,oO,kgr,xgr,Rgr,a_e,Sgr,Pgr,Yw,L8e,Pc,t9,n_e,Kw,$gr,s_e,Igr,B8e,Er,Zw,jgr,$c,Ngr,l_e,Dgr,qgr,i_e,Ggr,Ogr,Xgr,eA,zgr,d_e,Vgr,Wgr,Qgr,ut,oA,Hgr,c_e,Ugr,Jgr,Ic,Ygr,f_e,Kgr,Zgr,m_e,ehr,ohr,rhr,g_e,thr,ahr,rA,nhr,Eo,tA,shr,h_e,lhr,ihr,vn,dhr,p_e,chr,fhr,__e,mhr,ghr,u_e,hhr,phr,_hr,b_e,a9,v_e,uhr,bhr,rO,vhr,Thr,Fhr,T_e,Chr,Mhr,aA,k8e,jc,n9,F_e,nA,Ehr,C_e,yhr,x8e,yr,sA,whr,Nc,Ahr,M_e,Lhr,Bhr,E_e,khr,xhr,Rhr,lA,Shr,y_e,Phr,$hr,Ihr,bt,iA,jhr,w_e,Nhr,Dhr,Dc,qhr,A_e,Ghr,Ohr,L_e,Xhr,zhr,Vhr,B_e,Whr,Qhr,dA,Hhr,yo,cA,Uhr,k_e,Jhr,Yhr,Tn,Khr,x_e,Zhr,epr,R_e,opr,rpr,S_e,tpr,apr,npr,P_e,s9,$_e,spr,lpr,tO,ipr,dpr,cpr,I_e,fpr,mpr,fA,R8e,qc,l9,j_e,mA,gpr,N_e,hpr,S8e,wr,gA,ppr,Gc,_pr,D_e,upr,bpr,q_e,vpr,Tpr,Fpr,hA,Cpr,G_e,Mpr,Epr,ypr,vt,pA,wpr,O_e,Apr,Lpr,Oc,Bpr,X_e,kpr,xpr,z_e,Rpr,Spr,Ppr,V_e,$pr,Ipr,_A,jpr,wo,uA,Npr,W_e,Dpr,qpr,Fn,Gpr,Q_e,Opr,Xpr,H_e,zpr,Vpr,U_e,Wpr,Qpr,Hpr,V,i9,J_e,Upr,Jpr,aO,Ypr,Kpr,Zpr,d9,Y_e,e_r,o_r,nO,r_r,t_r,a_r,c9,K_e,n_r,s_r,sO,l_r,i_r,d_r,f9,Z_e,c_r,f_r,lO,m_r,g_r,h_r,m9,eue,p_r,__r,iO,u_r,b_r,v_r,g9,oue,T_r,F_r,dO,C_r,M_r,E_r,h9,rue,y_r,w_r,cO,A_r,L_r,B_r,p9,tue,k_r,x_r,fO,R_r,S_r,P_r,_9,aue,$_r,I_r,mO,j_r,N_r,D_r,u9,nue,q_r,G_r,gO,O_r,X_r,z_r,b9,sue,V_r,W_r,hO,Q_r,H_r,U_r,v9,lue,J_r,Y_r,pO,K_r,Z_r,eur,T9,iue,our,rur,_O,tur,aur,nur,F9,due,sur,lur,uO,iur,dur,cur,C9,cue,fur,mur,bO,gur,hur,pur,M9,fue,_ur,uur,vO,bur,vur,Tur,E9,mue,Fur,Cur,TO,Mur,Eur,yur,y9,gue,wur,Aur,FO,Lur,Bur,kur,w9,hue,xur,Rur,CO,Sur,Pur,$ur,A9,pue,Iur,jur,MO,Nur,Dur,qur,L9,_ue,Gur,Our,EO,Xur,zur,Vur,B9,uue,Wur,Qur,yO,Hur,Uur,Jur,k9,bue,Yur,Kur,wO,Zur,e2r,o2r,x9,vue,r2r,t2r,AO,a2r,n2r,s2r,Tue,l2r,i2r,bA,P8e,Xc,R9,Fue,vA,d2r,Cue,c2r,$8e,Ar,TA,f2r,zc,m2r,Mue,g2r,h2r,Eue,p2r,_2r,u2r,FA,b2r,yue,v2r,T2r,F2r,Tt,CA,C2r,wue,M2r,E2r,Vc,y2r,Aue,w2r,A2r,Lue,L2r,B2r,k2r,Bue,x2r,R2r,MA,S2r,Ao,EA,P2r,kue,$2r,I2r,Cn,j2r,xue,N2r,D2r,Rue,q2r,G2r,Sue,O2r,X2r,z2r,Mn,S9,Pue,V2r,W2r,LO,Q2r,H2r,U2r,P9,$ue,J2r,Y2r,BO,K2r,Z2r,e1r,$9,Iue,o1r,r1r,kO,t1r,a1r,n1r,I9,jue,s1r,l1r,xO,i1r,d1r,c1r,Nue,f1r,m1r,yA,I8e,Wc,j9,Due,wA,g1r,que,h1r,j8e,Lr,AA,p1r,Qc,_1r,Gue,u1r,b1r,Oue,v1r,T1r,F1r,LA,C1r,Xue,M1r,E1r,y1r,Ft,BA,w1r,zue,A1r,L1r,Hc,B1r,Vue,k1r,x1r,Wue,R1r,S1r,P1r,Que,$1r,I1r,kA,j1r,Lo,xA,N1r,Hue,D1r,q1r,En,G1r,Uue,O1r,X1r,Jue,z1r,V1r,Yue,W1r,Q1r,H1r,fe,N9,Kue,U1r,J1r,RO,Y1r,K1r,Z1r,D9,Zue,ebr,obr,SO,rbr,tbr,abr,q9,e2e,nbr,sbr,PO,lbr,ibr,dbr,G9,o2e,cbr,fbr,$O,mbr,gbr,hbr,O9,r2e,pbr,_br,IO,ubr,bbr,vbr,X9,t2e,Tbr,Fbr,jO,Cbr,Mbr,Ebr,z9,a2e,ybr,wbr,NO,Abr,Lbr,Bbr,V9,n2e,kbr,xbr,DO,Rbr,Sbr,Pbr,W9,s2e,$br,Ibr,qO,jbr,Nbr,Dbr,Q9,l2e,qbr,Gbr,GO,Obr,Xbr,zbr,H9,i2e,Vbr,Wbr,OO,Qbr,Hbr,Ubr,d2e,Jbr,Ybr,RA,N8e,Uc,U9,c2e,SA,Kbr,f2e,Zbr,D8e,Br,PA,e5r,Jc,o5r,m2e,r5r,t5r,g2e,a5r,n5r,s5r,$A,l5r,h2e,i5r,d5r,c5r,Ct,IA,f5r,p2e,m5r,g5r,Yc,h5r,_2e,p5r,_5r,u2e,u5r,b5r,v5r,b2e,T5r,F5r,jA,C5r,Bo,NA,M5r,v2e,E5r,y5r,yn,w5r,T2e,A5r,L5r,F2e,B5r,k5r,C2e,x5r,R5r,S5r,ve,J9,M2e,P5r,$5r,XO,I5r,j5r,N5r,Y9,E2e,D5r,q5r,zO,G5r,O5r,X5r,K9,y2e,z5r,V5r,VO,W5r,Q5r,H5r,Z9,w2e,U5r,J5r,WO,Y5r,K5r,Z5r,eC,A2e,evr,ovr,QO,rvr,tvr,avr,oC,L2e,nvr,svr,HO,lvr,ivr,dvr,rC,B2e,cvr,fvr,UO,mvr,gvr,hvr,tC,k2e,pvr,_vr,JO,uvr,bvr,vvr,aC,x2e,Tvr,Fvr,YO,Cvr,Mvr,Evr,R2e,yvr,wvr,DA,q8e,Kc,nC,S2e,qA,Avr,P2e,Lvr,G8e,kr,GA,Bvr,Zc,kvr,$2e,xvr,Rvr,I2e,Svr,Pvr,$vr,OA,Ivr,j2e,jvr,Nvr,Dvr,Mt,XA,qvr,N2e,Gvr,Ovr,ef,Xvr,D2e,zvr,Vvr,q2e,Wvr,Qvr,Hvr,G2e,Uvr,Jvr,zA,Yvr,ko,VA,Kvr,O2e,Zvr,eTr,wn,oTr,X2e,rTr,tTr,z2e,aTr,nTr,V2e,sTr,lTr,iTr,Te,sC,W2e,dTr,cTr,KO,fTr,mTr,gTr,lC,Q2e,hTr,pTr,ZO,_Tr,uTr,bTr,iC,H2e,vTr,TTr,eX,FTr,CTr,MTr,dC,U2e,ETr,yTr,oX,wTr,ATr,LTr,cC,J2e,BTr,kTr,rX,xTr,RTr,STr,fC,Y2e,PTr,$Tr,tX,ITr,jTr,NTr,mC,K2e,DTr,qTr,aX,GTr,OTr,XTr,gC,Z2e,zTr,VTr,nX,WTr,QTr,HTr,hC,e1e,UTr,JTr,sX,YTr,KTr,ZTr,o1e,e7r,o7r,WA,O8e,of,pC,r1e,QA,r7r,t1e,t7r,X8e,xr,HA,a7r,rf,n7r,a1e,s7r,l7r,n1e,i7r,d7r,c7r,UA,f7r,s1e,m7r,g7r,h7r,Et,JA,p7r,l1e,_7r,u7r,tf,b7r,i1e,v7r,T7r,d1e,F7r,C7r,M7r,c1e,E7r,y7r,YA,w7r,xo,KA,A7r,f1e,L7r,B7r,An,k7r,m1e,x7r,R7r,g1e,S7r,P7r,h1e,$7r,I7r,j7r,Fe,_C,p1e,N7r,D7r,lX,q7r,G7r,O7r,uC,_1e,X7r,z7r,iX,V7r,W7r,Q7r,bC,u1e,H7r,U7r,dX,J7r,Y7r,K7r,vC,b1e,Z7r,eFr,cX,oFr,rFr,tFr,TC,v1e,aFr,nFr,fX,sFr,lFr,iFr,FC,T1e,dFr,cFr,mX,fFr,mFr,gFr,CC,F1e,hFr,pFr,gX,_Fr,uFr,bFr,MC,C1e,vFr,TFr,hX,FFr,CFr,MFr,EC,M1e,EFr,yFr,pX,wFr,AFr,LFr,E1e,BFr,kFr,ZA,z8e,af,yC,y1e,e6,xFr,w1e,RFr,V8e,Rr,o6,SFr,nf,PFr,A1e,$Fr,IFr,L1e,jFr,NFr,DFr,r6,qFr,B1e,GFr,OFr,XFr,yt,t6,zFr,k1e,VFr,WFr,sf,QFr,x1e,HFr,UFr,R1e,JFr,YFr,KFr,S1e,ZFr,e9r,a6,o9r,Ro,n6,r9r,P1e,t9r,a9r,Ln,n9r,$1e,s9r,l9r,I1e,i9r,d9r,j1e,c9r,f9r,m9r,Ce,wC,N1e,g9r,h9r,_X,p9r,_9r,u9r,AC,D1e,b9r,v9r,uX,T9r,F9r,C9r,LC,q1e,M9r,E9r,bX,y9r,w9r,A9r,BC,G1e,L9r,B9r,vX,k9r,x9r,R9r,kC,O1e,S9r,P9r,TX,$9r,I9r,j9r,xC,X1e,N9r,D9r,FX,q9r,G9r,O9r,RC,z1e,X9r,z9r,CX,V9r,W9r,Q9r,SC,V1e,H9r,U9r,MX,J9r,Y9r,K9r,PC,W1e,Z9r,eCr,EX,oCr,rCr,tCr,Q1e,aCr,nCr,s6,W8e,lf,$C,H1e,l6,sCr,U1e,lCr,Q8e,Sr,i6,iCr,df,dCr,J1e,cCr,fCr,Y1e,mCr,gCr,hCr,d6,pCr,K1e,_Cr,uCr,bCr,wt,c6,vCr,Z1e,TCr,FCr,cf,CCr,ebe,MCr,ECr,obe,yCr,wCr,ACr,rbe,LCr,BCr,f6,kCr,So,m6,xCr,tbe,RCr,SCr,Bn,PCr,abe,$Cr,ICr,nbe,jCr,NCr,sbe,DCr,qCr,GCr,so,IC,lbe,OCr,XCr,yX,zCr,VCr,WCr,jC,ibe,QCr,HCr,wX,UCr,JCr,YCr,NC,dbe,KCr,ZCr,AX,e4r,o4r,r4r,DC,cbe,t4r,a4r,LX,n4r,s4r,l4r,qC,fbe,i4r,d4r,BX,c4r,f4r,m4r,GC,mbe,g4r,h4r,kX,p4r,_4r,u4r,OC,gbe,b4r,v4r,xX,T4r,F4r,C4r,hbe,M4r,E4r,g6,H8e,ff,XC,pbe,h6,y4r,_be,w4r,U8e,Pr,p6,A4r,mf,L4r,ube,B4r,k4r,bbe,x4r,R4r,S4r,_6,P4r,vbe,$4r,I4r,j4r,At,u6,N4r,Tbe,D4r,q4r,gf,G4r,Fbe,O4r,X4r,Cbe,z4r,V4r,W4r,Mbe,Q4r,H4r,b6,U4r,Po,v6,J4r,Ebe,Y4r,K4r,kn,Z4r,ybe,eMr,oMr,wbe,rMr,tMr,Abe,aMr,nMr,sMr,lo,zC,Lbe,lMr,iMr,RX,dMr,cMr,fMr,VC,Bbe,mMr,gMr,SX,hMr,pMr,_Mr,WC,kbe,uMr,bMr,PX,vMr,TMr,FMr,QC,xbe,CMr,MMr,$X,EMr,yMr,wMr,HC,Rbe,AMr,LMr,IX,BMr,kMr,xMr,UC,Sbe,RMr,SMr,jX,PMr,$Mr,IMr,JC,Pbe,jMr,NMr,NX,DMr,qMr,GMr,$be,OMr,XMr,T6,J8e,hf,YC,Ibe,F6,zMr,jbe,VMr,Y8e,$r,C6,WMr,pf,QMr,Nbe,HMr,UMr,Dbe,JMr,YMr,KMr,M6,ZMr,qbe,eEr,oEr,rEr,Lt,E6,tEr,Gbe,aEr,nEr,_f,sEr,Obe,lEr,iEr,Xbe,dEr,cEr,fEr,zbe,mEr,gEr,y6,hEr,$o,w6,pEr,Vbe,_Er,uEr,xn,bEr,Wbe,vEr,TEr,Qbe,FEr,CEr,Hbe,MEr,EEr,yEr,Ube,KC,Jbe,wEr,AEr,DX,LEr,BEr,kEr,Ybe,xEr,REr,A6,K8e,uf,ZC,Kbe,L6,SEr,Zbe,PEr,Z8e,Ir,B6,$Er,bf,IEr,e5e,jEr,NEr,o5e,DEr,qEr,GEr,k6,OEr,r5e,XEr,zEr,VEr,Bt,x6,WEr,t5e,QEr,HEr,vf,UEr,a5e,JEr,YEr,n5e,KEr,ZEr,e3r,s5e,o3r,r3r,R6,t3r,Io,S6,a3r,l5e,n3r,s3r,Rn,l3r,i5e,i3r,d3r,d5e,c3r,f3r,c5e,m3r,g3r,h3r,P6,e4,f5e,p3r,_3r,qX,u3r,b3r,v3r,o4,m5e,T3r,F3r,GX,C3r,M3r,E3r,g5e,y3r,w3r,$6,eBe,Tf,r4,h5e,I6,A3r,p5e,L3r,oBe,jr,j6,B3r,Ff,k3r,_5e,x3r,R3r,u5e,S3r,P3r,$3r,N6,I3r,b5e,j3r,N3r,D3r,kt,D6,q3r,v5e,G3r,O3r,Cf,X3r,T5e,z3r,V3r,F5e,W3r,Q3r,H3r,C5e,U3r,J3r,q6,Y3r,jo,G6,K3r,M5e,Z3r,eyr,Sn,oyr,E5e,ryr,tyr,y5e,ayr,nyr,w5e,syr,lyr,iyr,A5e,t4,L5e,dyr,cyr,OX,fyr,myr,gyr,B5e,hyr,pyr,O6,rBe;return ce=new z({}),$a=new w({props:{code:'model = AutoModel.from_pretrained("bert-base-cased"),',highlighted:'model = AutoModel.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)'}}),nM=new z({}),sM=new w({props:{code:`from transformers import AutoConfig, AutoModel

AutoConfig.register("new-model", NewModelConfig)
AutoModel.register(NewModelConfig, NewModel),`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModel

AutoConfig.register(<span class="hljs-string">&quot;new-model&quot;</span>, NewModelConfig)
AutoModel.register(NewModelConfig, NewModel)`}}),Bf=new _yr({props:{warning:"&lcub;true}",$$slots:{default:[F_t]},$$scope:{ctx:yi}}}),lM=new z({}),iM=new E({props:{name:"class transformers.AutoConfig",anchor:"transformers.AutoConfig",parameters:[],source:"https://github.com/huggingface/transformers/blob/pr_15792/src/transformers/models/auto/configuration_auto.py#L515"}}),fM=new E({props:{name:"from_pretrained",anchor:"transformers.AutoConfig.from_pretrained",parameters:[{name:"pretrained_model_name_or_path",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15792/src/transformers/models/auto/configuration_auto.py#L538",parametersDescription:[{anchor:"transformers.AutoConfig.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model configuration hosted inside a model repo on
huggingface.co. Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or
namespaced under a user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing a configuration file saved using the
<a href="/docs/transformers/pr_15792/en/main_classes/configuration#transformers.PretrainedConfig.save_pretrained">save_pretrained()</a> method, or the <a href="/docs/transformers/pr_15792/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> method,
e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a saved configuration JSON <em>file</em>, e.g.,
<code>./my_model_directory/configuration.json</code>.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.AutoConfig.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.AutoConfig.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download the model weights and configuration files and override the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.AutoConfig.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.AutoConfig.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.AutoConfig.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.AutoConfig.from_pretrained.return_unused_kwargs",description:`<strong>return_unused_kwargs</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
If <code>False</code>, then this function returns just the final configuration object.</p>
<p>If <code>True</code>, then this functions returns a <code>Tuple(config, unused_kwargs)</code> where <em>unused_kwargs</em> is a
dictionary consisting of the key/value pairs whose keys are not configuration attributes: i.e., the
part of <code>kwargs</code> which has not been used to update <code>config</code> and is otherwise ignored.`,name:"return_unused_kwargs"},{anchor:"transformers.AutoConfig.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.AutoConfig.from_pretrained.kwargs(additional",description:`<strong>kwargs(additional</strong> keyword arguments, <em>optional</em>) &#x2014;
The values in kwargs of any keys which are configuration attributes will be used to override the loaded
values. Behavior concerning key/value pairs whose keys are <em>not</em> configuration attributes is controlled
by the <code>return_unused_kwargs</code> keyword parameter.`,name:"kwargs(additional"}]}}),mM=new w({props:{code:`from transformers import AutoConfig

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-uncased")

# Download configuration from huggingface.co (user-uploaded) and cache.
config = AutoConfig.from_pretrained("dbmdz/bert-base-german-cased")

# If configuration file is in a directory (e.g., was saved using *save_pretrained('./test/saved_model/')*).
config = AutoConfig.from_pretrained("./test/bert_saved_model/")

# Load a specific configuration file.
config = AutoConfig.from_pretrained("./test/bert_saved_model/my_configuration.json")

# Change some config attributes when loading a pretrained config.
config = AutoConfig.from_pretrained("bert-base-uncased", output_attentions=True, foo=False)
config.output_attentions

config, unused_kwargs = AutoConfig.from_pretrained(
    "bert-base-uncased", output_attentions=True, foo=False, return_unused_kwargs=True
)
config.output_attentions

config.unused_kwargs,`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-uncased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co (user-uploaded) and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;dbmdz/bert-base-german-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># If configuration file is in a directory (e.g., was saved using *save_pretrained(&#x27;./test/saved_model/&#x27;)*).</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./test/bert_saved_model/&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Load a specific configuration file.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./test/bert_saved_model/my_configuration.json&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Change some config attributes when loading a pretrained config.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-uncased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>, foo=<span class="hljs-literal">False</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>config, unused_kwargs = AutoConfig.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;bert-base-uncased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>, foo=<span class="hljs-literal">False</span>, return_unused_kwargs=<span class="hljs-literal">True</span>
<span class="hljs-meta">... </span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>config.unused_kwargs
{<span class="hljs-string">&#x27;foo&#x27;</span>: <span class="hljs-literal">False</span>}`}}),gM=new E({props:{name:"register",anchor:"transformers.AutoConfig.register",parameters:[{name:"model_type",val:""},{name:"config",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15792/src/transformers/models/auto/configuration_auto.py#L660",parametersDescription:[{anchor:"transformers.AutoConfig.register.model_type",description:"<strong>model_type</strong> (<code>str</code>) &#x2014; The model type like &#x201C;bert&#x201D; or &#x201C;gpt&#x201D;.",name:"model_type"},{anchor:"transformers.AutoConfig.register.config",description:'<strong>config</strong> (<a href="/docs/transformers/pr_15792/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014; The config to register.',name:"config"}]}}),hM=new z({}),pM=new E({props:{name:"class transformers.AutoTokenizer",anchor:"transformers.AutoTokenizer",parameters:[],source:"https://github.com/huggingface/transformers/blob/pr_15792/src/transformers/models/auto/tokenization_auto.py#L351"}}),bM=new E({props:{name:"from_pretrained",anchor:"transformers.AutoTokenizer.from_pretrained",parameters:[{name:"pretrained_model_name_or_path",val:""},{name:"*inputs",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15792/src/transformers/models/auto/tokenization_auto.py#L365",parametersDescription:[{anchor:"transformers.AutoTokenizer.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a predefined tokenizer hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing vocabulary files required by the tokenizer, for instance saved
using the <a href="/docs/transformers/pr_15792/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.save_pretrained">save_pretrained()</a> method, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a single saved vocabulary file if and only if the tokenizer only requires a
single vocabulary file (like Bert or XLNet), e.g.: <code>./my_model_directory/vocab.txt</code>. (Not
applicable to all derived classes)</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.AutoTokenizer.from_pretrained.inputs",description:`<strong>inputs</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the Tokenizer <code>__init__()</code> method.`,name:"inputs"},{anchor:"transformers.AutoTokenizer.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15792/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
The configuration object used to dertermine the tokenizer class to instantiate.`,name:"config"},{anchor:"transformers.AutoTokenizer.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.AutoTokenizer.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download the model weights and configuration files and override the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.AutoTokenizer.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.AutoTokenizer.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.AutoTokenizer.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.AutoTokenizer.from_pretrained.subfolder",description:`<strong>subfolder</strong> (<code>str</code>, <em>optional</em>) &#x2014;
In case the relevant files are located inside a subfolder of the model repo on huggingface.co (e.g. for
facebook/rag-token-base), specify it here.`,name:"subfolder"},{anchor:"transformers.AutoTokenizer.from_pretrained.use_fast",description:`<strong>use_fast</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to try to load the fast version of the tokenizer.`,name:"use_fast"},{anchor:"transformers.AutoTokenizer.from_pretrained.tokenizer_type",description:`<strong>tokenizer_type</strong> (<code>str</code>, <em>optional</em>) &#x2014;
Tokenizer type to be loaded.`,name:"tokenizer_type"},{anchor:"transformers.AutoTokenizer.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.AutoTokenizer.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Will be passed to the Tokenizer <code>__init__()</code> method. Can be used to set special tokens like
<code>bos_token</code>, <code>eos_token</code>, <code>unk_token</code>, <code>sep_token</code>, <code>pad_token</code>, <code>cls_token</code>, <code>mask_token</code>,
<code>additional_special_tokens</code>. See parameters in the <code>__init__()</code> for more details.`,name:"kwargs"}]}}),vM=new w({props:{code:`from transformers import AutoTokenizer

# Download vocabulary from huggingface.co and cache.
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

# Download vocabulary from huggingface.co (user-uploaded) and cache.
tokenizer = AutoTokenizer.from_pretrained("dbmdz/bert-base-german-cased")

# If vocabulary files are in a directory (e.g. tokenizer was saved using *save_pretrained('./test/saved_model/')*)
tokenizer = AutoTokenizer.from_pretrained("./test/bert_saved_model/"),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download vocabulary from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;bert-base-uncased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download vocabulary from huggingface.co (user-uploaded) and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;dbmdz/bert-base-german-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># If vocabulary files are in a directory (e.g. tokenizer was saved using *save_pretrained(&#x27;./test/saved_model/&#x27;)*)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;./test/bert_saved_model/&quot;</span>)`}}),TM=new E({props:{name:"register",anchor:"transformers.AutoTokenizer.register",parameters:[{name:"config_class",val:""},{name:"slow_tokenizer_class",val:" = None"},{name:"fast_tokenizer_class",val:" = None"}],source:"https://github.com/huggingface/transformers/blob/pr_15792/src/transformers/models/auto/tokenization_auto.py#L561",parametersDescription:[{anchor:"transformers.AutoTokenizer.register.config_class",description:`<strong>config_class</strong> (<a href="/docs/transformers/pr_15792/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The configuration corresponding to the model to register.`,name:"config_class"},{anchor:"transformers.AutoTokenizer.register.slow_tokenizer_class",description:`<strong>slow_tokenizer_class</strong> (<code>PretrainedTokenizer</code>, <em>optional</em>) &#x2014;
The slow tokenizer to register.`,name:"slow_tokenizer_class"},{anchor:"transformers.AutoTokenizer.register.slow_tokenizer_class",description:`<strong>slow_tokenizer_class</strong> (<code>PretrainedTokenizerFast</code>, <em>optional</em>) &#x2014;
The fast tokenizer to register.`,name:"slow_tokenizer_class"}]}}),FM=new z({}),CM=new E({props:{name:"class transformers.AutoFeatureExtractor",anchor:"transformers.AutoFeatureExtractor",parameters:[],source:"https://github.com/huggingface/transformers/blob/pr_15792/src/transformers/models/auto/feature_extraction_auto.py#L169"}}),yM=new E({props:{name:"from_pretrained",anchor:"transformers.AutoFeatureExtractor.from_pretrained",parameters:[{name:"pretrained_model_name_or_path",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15792/src/transformers/models/auto/feature_extraction_auto.py#L183",parametersDescription:[{anchor:"transformers.AutoFeatureExtractor.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
This can be either:</p>
<ul>
<li>a string, the <em>model id</em> of a pretrained feature_extractor hosted inside a model repo on
huggingface.co. Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or
namespaced under a user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>a path to a <em>directory</em> containing a feature extractor file saved using the
<a href="/docs/transformers/pr_15792/en/main_classes/feature_extractor#transformers.FeatureExtractionMixin.save_pretrained">save_pretrained()</a> method, e.g.,
<code>./my_model_directory/</code>.</li>
<li>a path or url to a saved feature extractor JSON <em>file</em>, e.g.,
<code>./my_model_directory/preprocessor_config.json</code>.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.AutoFeatureExtractor.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model feature extractor should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.AutoFeatureExtractor.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force to (re-)download the feature extractor files and override the cached versions
if they exist.`,name:"force_download"},{anchor:"transformers.AutoFeatureExtractor.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received file. Attempts to resume the download if such a file
exists.`,name:"resume_download"},{anchor:"transformers.AutoFeatureExtractor.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}.</code> The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.AutoFeatureExtractor.from_pretrained.use_auth_token",description:`<strong>use_auth_token</strong> (<code>str</code> or <em>bool</em>, <em>optional</em>) &#x2014;
The token to use as HTTP bearer authorization for remote files. If <code>True</code>, will use the token generated
when running <code>transformers-cli login</code> (stored in <code>~/.huggingface</code>).`,name:"use_auth_token"},{anchor:"transformers.AutoFeatureExtractor.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.AutoFeatureExtractor.from_pretrained.return_unused_kwargs",description:`<strong>return_unused_kwargs</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
If <code>False</code>, then this function returns just the final feature extractor object. If <code>True</code>, then this
functions returns a <code>Tuple(feature_extractor, unused_kwargs)</code> where <em>unused_kwargs</em> is a dictionary
consisting of the key/value pairs whose keys are not feature extractor attributes: i.e., the part of
<code>kwargs</code> which has not been used to update <code>feature_extractor</code> and is otherwise ignored.`,name:"return_unused_kwargs"},{anchor:"transformers.AutoFeatureExtractor.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.AutoFeatureExtractor.from_pretrained.kwargs",description:`<strong>kwargs</strong> (<code>Dict[str, Any]</code>, <em>optional</em>) &#x2014;
The values in kwargs of any keys which are feature extractor attributes will be used to override the
loaded values. Behavior concerning key/value pairs whose keys are <em>not</em> feature extractor attributes is
controlled by the <code>return_unused_kwargs</code> keyword parameter.`,name:"kwargs"}]}}),ih=new _yr({props:{$$slots:{default:[C_t]},$$scope:{ctx:yi}}}),wM=new w({props:{code:`from transformers import AutoFeatureExtractor

# Download feature extractor from huggingface.co and cache.
feature_extractor = AutoFeatureExtractor.from_pretrained("facebook/wav2vec2-base-960h")

# If feature extractor files are in a directory (e.g. feature extractor was saved using *save_pretrained('./test/saved_model/')*)
feature_extractor = AutoFeatureExtractor.from_pretrained("./test/saved_model/"),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoFeatureExtractor

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download feature extractor from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>feature_extractor = AutoFeatureExtractor.from_pretrained(<span class="hljs-string">&quot;facebook/wav2vec2-base-960h&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># If feature extractor files are in a directory (e.g. feature extractor was saved using *save_pretrained(&#x27;./test/saved_model/&#x27;)*)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>feature_extractor = AutoFeatureExtractor.from_pretrained(<span class="hljs-string">&quot;./test/saved_model/&quot;</span>)`}}),AM=new E({props:{name:"register",anchor:"transformers.AutoFeatureExtractor.register",parameters:[{name:"config_class",val:""},{name:"feature_extractor_class",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15792/src/transformers/models/auto/feature_extraction_auto.py#L310",parametersDescription:[{anchor:"transformers.AutoFeatureExtractor.register.config_class",description:`<strong>config_class</strong> (<a href="/docs/transformers/pr_15792/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The configuration corresponding to the model to register.`,name:"config_class"},{anchor:"transformers.AutoFeatureExtractor.register.feature_extractor_class",description:"<strong>feature_extractor_class</strong> (<code>FeatureExtractorMixin</code>) &#x2014; The feature extractor to register.",name:"feature_extractor_class"}]}}),LM=new z({}),BM=new E({props:{name:"class transformers.AutoProcessor",anchor:"transformers.AutoProcessor",parameters:[],source:"https://github.com/huggingface/transformers/blob/pr_15792/src/transformers/models/auto/processing_auto.py#L71"}}),RM=new E({props:{name:"from_pretrained",anchor:"transformers.AutoProcessor.from_pretrained",parameters:[{name:"pretrained_model_name_or_path",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15792/src/transformers/models/auto/processing_auto.py#L85",parametersDescription:[{anchor:"transformers.AutoProcessor.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
This can be either:</p>
<ul>
<li>a string, the <em>model id</em> of a pretrained feature_extractor hosted inside a model repo on
huggingface.co. Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or
namespaced under a user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>a path to a <em>directory</em> containing a processor files saved using the <code>save_pretrained()</code> method,
e.g., <code>./my_model_directory/</code>.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.AutoProcessor.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model feature extractor should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.AutoProcessor.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force to (re-)download the feature extractor files and override the cached versions
if they exist.`,name:"force_download"},{anchor:"transformers.AutoProcessor.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received file. Attempts to resume the download if such a file
exists.`,name:"resume_download"},{anchor:"transformers.AutoProcessor.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}.</code> The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.AutoProcessor.from_pretrained.use_auth_token",description:`<strong>use_auth_token</strong> (<code>str</code> or <em>bool</em>, <em>optional</em>) &#x2014;
The token to use as HTTP bearer authorization for remote files. If <code>True</code>, will use the token generated
when running <code>transformers-cli login</code> (stored in <code>~/.huggingface</code>).`,name:"use_auth_token"},{anchor:"transformers.AutoProcessor.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.AutoProcessor.from_pretrained.return_unused_kwargs",description:`<strong>return_unused_kwargs</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
If <code>False</code>, then this function returns just the final feature extractor object. If <code>True</code>, then this
functions returns a <code>Tuple(feature_extractor, unused_kwargs)</code> where <em>unused_kwargs</em> is a dictionary
consisting of the key/value pairs whose keys are not feature extractor attributes: i.e., the part of
<code>kwargs</code> which has not been used to update <code>feature_extractor</code> and is otherwise ignored.`,name:"return_unused_kwargs"},{anchor:"transformers.AutoProcessor.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.AutoProcessor.from_pretrained.kwargs",description:`<strong>kwargs</strong> (<code>Dict[str, Any]</code>, <em>optional</em>) &#x2014;
The values in kwargs of any keys which are feature extractor attributes will be used to override the
loaded values. Behavior concerning key/value pairs whose keys are <em>not</em> feature extractor attributes is
controlled by the <code>return_unused_kwargs</code> keyword parameter.`,name:"kwargs"}]}}),vh=new _yr({props:{$$slots:{default:[M_t]},$$scope:{ctx:yi}}}),SM=new w({props:{code:`from transformers import AutoProcessor

# Download processor from huggingface.co and cache.
processor = AutoProcessor.from_pretrained("facebook/wav2vec2-base-960h")

# If processor files are in a directory (e.g. processor was saved using *save_pretrained('./test/saved_model/')*)
processor = AutoProcessor.from_pretrained("./test/saved_model/"),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoProcessor

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download processor from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>processor = AutoProcessor.from_pretrained(<span class="hljs-string">&quot;facebook/wav2vec2-base-960h&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># If processor files are in a directory (e.g. processor was saved using *save_pretrained(&#x27;./test/saved_model/&#x27;)*)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>processor = AutoProcessor.from_pretrained(<span class="hljs-string">&quot;./test/saved_model/&quot;</span>)`}}),PM=new E({props:{name:"register",anchor:"transformers.AutoProcessor.register",parameters:[{name:"config_class",val:""},{name:"processor_class",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15792/src/transformers/models/auto/processing_auto.py#L238",parametersDescription:[{anchor:"transformers.AutoProcessor.register.config_class",description:`<strong>config_class</strong> (<a href="/docs/transformers/pr_15792/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The configuration corresponding to the model to register.`,name:"config_class"},{anchor:"transformers.AutoProcessor.register.processor_class",description:"<strong>processor_class</strong> (<code>FeatureExtractorMixin</code>) &#x2014; The processor to register.",name:"processor_class"}]}}),$M=new z({}),IM=new E({props:{name:"class transformers.AutoModel",anchor:"transformers.AutoModel",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15792/src/transformers/models/auto/modeling_auto.py#L672"}}),NM=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15792/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15792/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15792/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/albert#transformers.AlbertModel">AlbertModel</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/bart#transformers.BartModel">BartModel</a> (BART model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/beit#transformers.BeitConfig">BeitConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/beit#transformers.BeitModel">BeitModel</a> (BEiT model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/bert#transformers.BertModel">BertModel</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/bert-generation#transformers.BertGenerationConfig">BertGenerationConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/bert-generation#transformers.BertGenerationEncoder">BertGenerationEncoder</a> (Bert Generation model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/big_bird#transformers.BigBirdModel">BigBirdModel</a> (BigBird model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusConfig">BigBirdPegasusConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusModel">BigBirdPegasusModel</a> (BigBirdPegasus model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/blenderbot#transformers.BlenderbotConfig">BlenderbotConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/blenderbot#transformers.BlenderbotModel">BlenderbotModel</a> (Blenderbot model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/blenderbot-small#transformers.BlenderbotSmallConfig">BlenderbotSmallConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/blenderbot-small#transformers.BlenderbotSmallModel">BlenderbotSmallModel</a> (BlenderbotSmall model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/clip#transformers.CLIPConfig">CLIPConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/clip#transformers.CLIPModel">CLIPModel</a> (CLIP model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/ctrl#transformers.CTRLConfig">CTRLConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/ctrl#transformers.CTRLModel">CTRLModel</a> (CTRL model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/camembert#transformers.CamembertModel">CamembertModel</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/canine#transformers.CanineConfig">CanineConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/canine#transformers.CanineModel">CanineModel</a> (Canine model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/convbert#transformers.ConvBertModel">ConvBertModel</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/convnext#transformers.ConvNextConfig">ConvNextConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/convnext#transformers.ConvNextModel">ConvNextModel</a> (ConvNext model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/dpr#transformers.DPRConfig">DPRConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/dpr#transformers.DPRQuestionEncoder">DPRQuestionEncoder</a> (DPR model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/deberta#transformers.DebertaModel">DebertaModel</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/deberta-v2#transformers.DebertaV2Model">DebertaV2Model</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/deit#transformers.DeiTConfig">DeiTConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/deit#transformers.DeiTModel">DeiTModel</a> (DeiT model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/detr#transformers.DetrConfig">DetrConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/detr#transformers.DetrModel">DetrModel</a> (DETR model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/distilbert#transformers.DistilBertModel">DistilBertModel</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/electra#transformers.ElectraModel">ElectraModel</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/fnet#transformers.FNetConfig">FNetConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/fnet#transformers.FNetModel">FNetModel</a> (FNet model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/fsmt#transformers.FSMTConfig">FSMTConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/fsmt#transformers.FSMTModel">FSMTModel</a> (FairSeq Machine-Translation model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/flaubert#transformers.FlaubertModel">FlaubertModel</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/funnel#transformers.FunnelModel">FunnelModel</a> or <a href="/docs/transformers/pr_15792/en/model_doc/funnel#transformers.FunnelBaseModel">FunnelBaseModel</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/gpt2#transformers.GPT2Model">GPT2Model</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/gptj#transformers.GPTJConfig">GPTJConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/gptj#transformers.GPTJModel">GPTJModel</a> (GPT-J model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/gpt_neo#transformers.GPTNeoConfig">GPTNeoConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/gpt_neo#transformers.GPTNeoModel">GPTNeoModel</a> (GPT Neo model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/hubert#transformers.HubertConfig">HubertConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/hubert#transformers.HubertModel">HubertModel</a> (Hubert model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/ibert#transformers.IBertConfig">IBertConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/ibert#transformers.IBertModel">IBertModel</a> (I-BERT model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/imagegpt#transformers.ImageGPTConfig">ImageGPTConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/imagegpt#transformers.ImageGPTModel">ImageGPTModel</a> (ImageGPT model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/led#transformers.LEDConfig">LEDConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/led#transformers.LEDModel">LEDModel</a> (LED model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/layoutlm#transformers.LayoutLMModel">LayoutLMModel</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/layoutlmv2#transformers.LayoutLMv2Config">LayoutLMv2Config</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/layoutlmv2#transformers.LayoutLMv2Model">LayoutLMv2Model</a> (LayoutLMv2 model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/longformer#transformers.LongformerModel">LongformerModel</a> (Longformer model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/luke#transformers.LukeConfig">LukeConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/luke#transformers.LukeModel">LukeModel</a> (LUKE model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/lxmert#transformers.LxmertConfig">LxmertConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/lxmert#transformers.LxmertModel">LxmertModel</a> (LXMERT model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/m2m_100#transformers.M2M100Config">M2M100Config</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/m2m_100#transformers.M2M100Model">M2M100Model</a> (M2M100 model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/mbart#transformers.MBartModel">MBartModel</a> (mBART model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/mpnet#transformers.MPNetModel">MPNetModel</a> (MPNet model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/mt5#transformers.MT5Config">MT5Config</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/mt5#transformers.MT5Model">MT5Model</a> (mT5 model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/marian#transformers.MarianConfig">MarianConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/marian#transformers.MarianModel">MarianModel</a> (Marian model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/megatron-bert#transformers.MegatronBertConfig">MegatronBertConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/megatron-bert#transformers.MegatronBertModel">MegatronBertModel</a> (MegatronBert model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/mobilebert#transformers.MobileBertModel">MobileBertModel</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/nystromformer#transformers.NystromformerConfig">NystromformerConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/nystromformer#transformers.NystromformerModel">NystromformerModel</a> (Nystromformer model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig">OpenAIGPTConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/openai-gpt#transformers.OpenAIGPTModel">OpenAIGPTModel</a> (OpenAI GPT model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/plbart#transformers.PLBartConfig">PLBartConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/plbart#transformers.PLBartModel">PLBartModel</a> (PLBart model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/pegasus#transformers.PegasusConfig">PegasusConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/pegasus#transformers.PegasusModel">PegasusModel</a> (Pegasus model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/perceiver#transformers.PerceiverConfig">PerceiverConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/perceiver#transformers.PerceiverModel">PerceiverModel</a> (Perceiver model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/poolformer#transformers.PoolFormerConfig">PoolFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/poolformer#transformers.PoolFormerModel">PoolFormerModel</a> (PoolFormer model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/prophetnet#transformers.ProphetNetConfig">ProphetNetConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/prophetnet#transformers.ProphetNetModel">ProphetNetModel</a> (ProphetNet model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/qdqbert#transformers.QDQBertConfig">QDQBertConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/qdqbert#transformers.QDQBertModel">QDQBertModel</a> (QDQBert model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/reformer#transformers.ReformerConfig">ReformerConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/reformer#transformers.ReformerModel">ReformerModel</a> (Reformer model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/rembert#transformers.RemBertModel">RemBertModel</a> (RemBERT model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/retribert#transformers.RetriBertConfig">RetriBertConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/retribert#transformers.RetriBertModel">RetriBertModel</a> (RetriBERT model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/roformer#transformers.RoFormerModel">RoFormerModel</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/roberta#transformers.RobertaModel">RobertaModel</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/sew#transformers.SEWConfig">SEWConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/sew#transformers.SEWModel">SEWModel</a> (SEW model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/sew-d#transformers.SEWDConfig">SEWDConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/sew-d#transformers.SEWDModel">SEWDModel</a> (SEW-D model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/segformer#transformers.SegformerConfig">SegformerConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/segformer#transformers.SegformerModel">SegformerModel</a> (SegFormer model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/speech_to_text#transformers.Speech2TextConfig">Speech2TextConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/speech_to_text#transformers.Speech2TextModel">Speech2TextModel</a> (Speech2Text model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/splinter#transformers.SplinterConfig">SplinterConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/splinter#transformers.SplinterModel">SplinterModel</a> (Splinter model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/squeezebert#transformers.SqueezeBertConfig">SqueezeBertConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/squeezebert#transformers.SqueezeBertModel">SqueezeBertModel</a> (SqueezeBERT model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/swin#transformers.SwinConfig">SwinConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/swin#transformers.SwinModel">SwinModel</a> (Swin model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/t5#transformers.T5Config">T5Config</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/t5#transformers.T5Model">T5Model</a> (T5 model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/tapas#transformers.TapasModel">TapasModel</a> (TAPAS model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/transfo-xl#transformers.TransfoXLConfig">TransfoXLConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/transfo-xl#transformers.TransfoXLModel">TransfoXLModel</a> (Transformer-XL model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/unispeech#transformers.UniSpeechConfig">UniSpeechConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/unispeech#transformers.UniSpeechModel">UniSpeechModel</a> (UniSpeech model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/unispeech-sat#transformers.UniSpeechSatConfig">UniSpeechSatConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/unispeech-sat#transformers.UniSpeechSatModel">UniSpeechSatModel</a> (UniSpeechSat model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/vit#transformers.ViTConfig">ViTConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/vit#transformers.ViTModel">ViTModel</a> (ViT model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/vit_mae#transformers.ViTMAEConfig">ViTMAEConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/vit_mae#transformers.ViTMAEModel">ViTMAEModel</a> (ViTMAE model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/vilt#transformers.ViltConfig">ViltConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/vilt#transformers.ViltModel">ViltModel</a> (ViLT model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/vision-text-dual-encoder#transformers.VisionTextDualEncoderConfig">VisionTextDualEncoderConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/vision-text-dual-encoder#transformers.VisionTextDualEncoderModel">VisionTextDualEncoderModel</a> (VisionTextDualEncoder model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/visual_bert#transformers.VisualBertConfig">VisualBertConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/visual_bert#transformers.VisualBertModel">VisualBertModel</a> (VisualBert model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/wav2vec2#transformers.Wav2Vec2Model">Wav2Vec2Model</a> (Wav2Vec2 model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/wavlm#transformers.WavLMConfig">WavLMConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/wavlm#transformers.WavLMModel">WavLMModel</a> (WavLM model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/xglm#transformers.XGLMConfig">XGLMConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/xglm#transformers.XGLMModel">XGLMModel</a> (XGLM model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/xlm#transformers.XLMModel">XLMModel</a> (XLM model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetConfig">XLMProphetNetConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetModel">XLMProphetNetModel</a> (XLMProphetNet model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/xlm-roberta#transformers.XLMRobertaModel">XLMRobertaModel</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLConfig">XLMRobertaXLConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLModel">XLMRobertaXLModel</a> (XLM-RoBERTa-XL model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/xlnet#transformers.XLNetModel">XLNetModel</a> (XLNet model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/yoso#transformers.YosoConfig">YosoConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/yoso#transformers.YosoModel">YosoModel</a> (YOSO model)</li>
</ul>`,name:"config"}]}}),DM=new w({props:{code:`from transformers import AutoConfig, AutoModel

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModel.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModel.from_config(config)`}}),qM=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15792/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15792/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15792/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15792/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15792/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15792/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15792/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),GM=new w({props:{code:`from transformers import AutoConfig, AutoModel

# Download model and configuration from huggingface.co and cache.
model = AutoModel.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModel.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModel.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModel.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModel.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModel.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),OM=new z({}),XM=new E({props:{name:"class transformers.AutoModelForPreTraining",anchor:"transformers.AutoModelForPreTraining",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15792/src/transformers/models/auto/modeling_auto.py#L679"}}),VM=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15792/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15792/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15792/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/albert#transformers.AlbertForPreTraining">AlbertForPreTraining</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/bart#transformers.BartForConditionalGeneration">BartForConditionalGeneration</a> (BART model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/bert#transformers.BertForPreTraining">BertForPreTraining</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/big_bird#transformers.BigBirdForPreTraining">BigBirdForPreTraining</a> (BigBird model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/ctrl#transformers.CTRLConfig">CTRLConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/ctrl#transformers.CTRLLMHeadModel">CTRLLMHeadModel</a> (CTRL model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/camembert#transformers.CamembertForMaskedLM">CamembertForMaskedLM</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/deberta#transformers.DebertaForMaskedLM">DebertaForMaskedLM</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/deberta-v2#transformers.DebertaV2ForMaskedLM">DebertaV2ForMaskedLM</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/distilbert#transformers.DistilBertForMaskedLM">DistilBertForMaskedLM</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/electra#transformers.ElectraForPreTraining">ElectraForPreTraining</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/fnet#transformers.FNetConfig">FNetConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/fnet#transformers.FNetForPreTraining">FNetForPreTraining</a> (FNet model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/fsmt#transformers.FSMTConfig">FSMTConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/fsmt#transformers.FSMTForConditionalGeneration">FSMTForConditionalGeneration</a> (FairSeq Machine-Translation model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/flaubert#transformers.FlaubertWithLMHeadModel">FlaubertWithLMHeadModel</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/funnel#transformers.FunnelForPreTraining">FunnelForPreTraining</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/gpt2#transformers.GPT2LMHeadModel">GPT2LMHeadModel</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/ibert#transformers.IBertConfig">IBertConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/ibert#transformers.IBertForMaskedLM">IBertForMaskedLM</a> (I-BERT model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/layoutlm#transformers.LayoutLMForMaskedLM">LayoutLMForMaskedLM</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/longformer#transformers.LongformerForMaskedLM">LongformerForMaskedLM</a> (Longformer model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/lxmert#transformers.LxmertConfig">LxmertConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/lxmert#transformers.LxmertForPreTraining">LxmertForPreTraining</a> (LXMERT model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/mpnet#transformers.MPNetForMaskedLM">MPNetForMaskedLM</a> (MPNet model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/megatron-bert#transformers.MegatronBertConfig">MegatronBertConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/megatron-bert#transformers.MegatronBertForPreTraining">MegatronBertForPreTraining</a> (MegatronBert model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/mobilebert#transformers.MobileBertForPreTraining">MobileBertForPreTraining</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig">OpenAIGPTConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/openai-gpt#transformers.OpenAIGPTLMHeadModel">OpenAIGPTLMHeadModel</a> (OpenAI GPT model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/retribert#transformers.RetriBertConfig">RetriBertConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/retribert#transformers.RetriBertModel">RetriBertModel</a> (RetriBERT model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/roberta#transformers.RobertaForMaskedLM">RobertaForMaskedLM</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/squeezebert#transformers.SqueezeBertConfig">SqueezeBertConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/squeezebert#transformers.SqueezeBertForMaskedLM">SqueezeBertForMaskedLM</a> (SqueezeBERT model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/t5#transformers.T5Config">T5Config</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/t5#transformers.T5ForConditionalGeneration">T5ForConditionalGeneration</a> (T5 model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/tapas#transformers.TapasForMaskedLM">TapasForMaskedLM</a> (TAPAS model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/transfo-xl#transformers.TransfoXLConfig">TransfoXLConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/transfo-xl#transformers.TransfoXLLMHeadModel">TransfoXLLMHeadModel</a> (Transformer-XL model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/unispeech#transformers.UniSpeechConfig">UniSpeechConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/unispeech#transformers.UniSpeechForPreTraining">UniSpeechForPreTraining</a> (UniSpeech model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/unispeech-sat#transformers.UniSpeechSatConfig">UniSpeechSatConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/unispeech-sat#transformers.UniSpeechSatForPreTraining">UniSpeechSatForPreTraining</a> (UniSpeechSat model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/vit_mae#transformers.ViTMAEConfig">ViTMAEConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/vit_mae#transformers.ViTMAEForPreTraining">ViTMAEForPreTraining</a> (ViTMAE model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/visual_bert#transformers.VisualBertConfig">VisualBertConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/visual_bert#transformers.VisualBertForPreTraining">VisualBertForPreTraining</a> (VisualBert model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/wav2vec2#transformers.Wav2Vec2ForPreTraining">Wav2Vec2ForPreTraining</a> (Wav2Vec2 model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/xlm#transformers.XLMWithLMHeadModel">XLMWithLMHeadModel</a> (XLM model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/xlm-roberta#transformers.XLMRobertaForMaskedLM">XLMRobertaForMaskedLM</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLConfig">XLMRobertaXLConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForMaskedLM">XLMRobertaXLForMaskedLM</a> (XLM-RoBERTa-XL model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/xlnet#transformers.XLNetLMHeadModel">XLNetLMHeadModel</a> (XLNet model)</li>
</ul>`,name:"config"}]}}),WM=new w({props:{code:`from transformers import AutoConfig, AutoModelForPreTraining

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForPreTraining.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForPreTraining

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForPreTraining.from_config(config)`}}),QM=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15792/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15792/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15792/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15792/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15792/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15792/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15792/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),HM=new w({props:{code:`from transformers import AutoConfig, AutoModelForPreTraining

# Download model and configuration from huggingface.co and cache.
model = AutoModelForPreTraining.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForPreTraining.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForPreTraining.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForPreTraining

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForPreTraining.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForPreTraining.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForPreTraining.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),UM=new z({}),JM=new E({props:{name:"class transformers.AutoModelForCausalLM",anchor:"transformers.AutoModelForCausalLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15792/src/transformers/models/auto/modeling_auto.py#L694"}}),KM=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15792/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15792/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15792/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/bart#transformers.BartForCausalLM">BartForCausalLM</a> (BART model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/bert#transformers.BertLMHeadModel">BertLMHeadModel</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/bert-generation#transformers.BertGenerationConfig">BertGenerationConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/bert-generation#transformers.BertGenerationDecoder">BertGenerationDecoder</a> (Bert Generation model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/big_bird#transformers.BigBirdForCausalLM">BigBirdForCausalLM</a> (BigBird model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusConfig">BigBirdPegasusConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusForCausalLM">BigBirdPegasusForCausalLM</a> (BigBirdPegasus model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/blenderbot#transformers.BlenderbotConfig">BlenderbotConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/blenderbot#transformers.BlenderbotForCausalLM">BlenderbotForCausalLM</a> (Blenderbot model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/blenderbot-small#transformers.BlenderbotSmallConfig">BlenderbotSmallConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/blenderbot-small#transformers.BlenderbotSmallForCausalLM">BlenderbotSmallForCausalLM</a> (BlenderbotSmall model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/ctrl#transformers.CTRLConfig">CTRLConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/ctrl#transformers.CTRLLMHeadModel">CTRLLMHeadModel</a> (CTRL model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/camembert#transformers.CamembertForCausalLM">CamembertForCausalLM</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/electra#transformers.ElectraForCausalLM">ElectraForCausalLM</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/gpt2#transformers.GPT2LMHeadModel">GPT2LMHeadModel</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/gptj#transformers.GPTJConfig">GPTJConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/gptj#transformers.GPTJForCausalLM">GPTJForCausalLM</a> (GPT-J model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/gpt_neo#transformers.GPTNeoConfig">GPTNeoConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/gpt_neo#transformers.GPTNeoForCausalLM">GPTNeoForCausalLM</a> (GPT Neo model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/mbart#transformers.MBartForCausalLM">MBartForCausalLM</a> (mBART model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/marian#transformers.MarianConfig">MarianConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/marian#transformers.MarianForCausalLM">MarianForCausalLM</a> (Marian model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/megatron-bert#transformers.MegatronBertConfig">MegatronBertConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/megatron-bert#transformers.MegatronBertForCausalLM">MegatronBertForCausalLM</a> (MegatronBert model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig">OpenAIGPTConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/openai-gpt#transformers.OpenAIGPTLMHeadModel">OpenAIGPTLMHeadModel</a> (OpenAI GPT model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/plbart#transformers.PLBartConfig">PLBartConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/plbart#transformers.PLBartForCausalLM">PLBartForCausalLM</a> (PLBart model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/pegasus#transformers.PegasusConfig">PegasusConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/pegasus#transformers.PegasusForCausalLM">PegasusForCausalLM</a> (Pegasus model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/prophetnet#transformers.ProphetNetConfig">ProphetNetConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/prophetnet#transformers.ProphetNetForCausalLM">ProphetNetForCausalLM</a> (ProphetNet model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/qdqbert#transformers.QDQBertConfig">QDQBertConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/qdqbert#transformers.QDQBertLMHeadModel">QDQBertLMHeadModel</a> (QDQBert model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/reformer#transformers.ReformerConfig">ReformerConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/reformer#transformers.ReformerModelWithLMHead">ReformerModelWithLMHead</a> (Reformer model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/rembert#transformers.RemBertForCausalLM">RemBertForCausalLM</a> (RemBERT model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/roformer#transformers.RoFormerForCausalLM">RoFormerForCausalLM</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/roberta#transformers.RobertaForCausalLM">RobertaForCausalLM</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/speech_to_text_2#transformers.Speech2Text2Config">Speech2Text2Config</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/speech_to_text_2#transformers.Speech2Text2ForCausalLM">Speech2Text2ForCausalLM</a> (Speech2Text2 model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/trocr#transformers.TrOCRConfig">TrOCRConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/trocr#transformers.TrOCRForCausalLM">TrOCRForCausalLM</a> (TrOCR model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/transfo-xl#transformers.TransfoXLConfig">TransfoXLConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/transfo-xl#transformers.TransfoXLLMHeadModel">TransfoXLLMHeadModel</a> (Transformer-XL model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/xglm#transformers.XGLMConfig">XGLMConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/xglm#transformers.XGLMForCausalLM">XGLMForCausalLM</a> (XGLM model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/xlm#transformers.XLMWithLMHeadModel">XLMWithLMHeadModel</a> (XLM model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetConfig">XLMProphetNetConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetForCausalLM">XLMProphetNetForCausalLM</a> (XLMProphetNet model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/xlm-roberta#transformers.XLMRobertaForCausalLM">XLMRobertaForCausalLM</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLConfig">XLMRobertaXLConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForCausalLM">XLMRobertaXLForCausalLM</a> (XLM-RoBERTa-XL model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/xlnet#transformers.XLNetLMHeadModel">XLNetLMHeadModel</a> (XLNet model)</li>
</ul>`,name:"config"}]}}),ZM=new w({props:{code:`from transformers import AutoConfig, AutoModelForCausalLM

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForCausalLM.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForCausalLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCausalLM.from_config(config)`}}),eE=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15792/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15792/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15792/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15792/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15792/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15792/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15792/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),oE=new w({props:{code:`from transformers import AutoConfig, AutoModelForCausalLM

# Download model and configuration from huggingface.co and cache.
model = AutoModelForCausalLM.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForCausalLM.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForCausalLM.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForCausalLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCausalLM.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),rE=new z({}),tE=new E({props:{name:"class transformers.AutoModelForMaskedLM",anchor:"transformers.AutoModelForMaskedLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15792/src/transformers/models/auto/modeling_auto.py#L701"}}),nE=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15792/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15792/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15792/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/albert#transformers.AlbertForMaskedLM">AlbertForMaskedLM</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/bart#transformers.BartForConditionalGeneration">BartForConditionalGeneration</a> (BART model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/bert#transformers.BertForMaskedLM">BertForMaskedLM</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/big_bird#transformers.BigBirdForMaskedLM">BigBirdForMaskedLM</a> (BigBird model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/camembert#transformers.CamembertForMaskedLM">CamembertForMaskedLM</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/convbert#transformers.ConvBertForMaskedLM">ConvBertForMaskedLM</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/deberta#transformers.DebertaForMaskedLM">DebertaForMaskedLM</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/deberta-v2#transformers.DebertaV2ForMaskedLM">DebertaV2ForMaskedLM</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/distilbert#transformers.DistilBertForMaskedLM">DistilBertForMaskedLM</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/electra#transformers.ElectraForMaskedLM">ElectraForMaskedLM</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/fnet#transformers.FNetConfig">FNetConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/fnet#transformers.FNetForMaskedLM">FNetForMaskedLM</a> (FNet model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/flaubert#transformers.FlaubertWithLMHeadModel">FlaubertWithLMHeadModel</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/funnel#transformers.FunnelForMaskedLM">FunnelForMaskedLM</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/ibert#transformers.IBertConfig">IBertConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/ibert#transformers.IBertForMaskedLM">IBertForMaskedLM</a> (I-BERT model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/layoutlm#transformers.LayoutLMForMaskedLM">LayoutLMForMaskedLM</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/longformer#transformers.LongformerForMaskedLM">LongformerForMaskedLM</a> (Longformer model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/mbart#transformers.MBartForConditionalGeneration">MBartForConditionalGeneration</a> (mBART model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/mpnet#transformers.MPNetForMaskedLM">MPNetForMaskedLM</a> (MPNet model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/megatron-bert#transformers.MegatronBertConfig">MegatronBertConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/megatron-bert#transformers.MegatronBertForMaskedLM">MegatronBertForMaskedLM</a> (MegatronBert model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/mobilebert#transformers.MobileBertForMaskedLM">MobileBertForMaskedLM</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/nystromformer#transformers.NystromformerConfig">NystromformerConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/nystromformer#transformers.NystromformerForMaskedLM">NystromformerForMaskedLM</a> (Nystromformer model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/perceiver#transformers.PerceiverConfig">PerceiverConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/perceiver#transformers.PerceiverForMaskedLM">PerceiverForMaskedLM</a> (Perceiver model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/qdqbert#transformers.QDQBertConfig">QDQBertConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/qdqbert#transformers.QDQBertForMaskedLM">QDQBertForMaskedLM</a> (QDQBert model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/reformer#transformers.ReformerConfig">ReformerConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/reformer#transformers.ReformerForMaskedLM">ReformerForMaskedLM</a> (Reformer model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/rembert#transformers.RemBertForMaskedLM">RemBertForMaskedLM</a> (RemBERT model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/roformer#transformers.RoFormerForMaskedLM">RoFormerForMaskedLM</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/roberta#transformers.RobertaForMaskedLM">RobertaForMaskedLM</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/squeezebert#transformers.SqueezeBertConfig">SqueezeBertConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/squeezebert#transformers.SqueezeBertForMaskedLM">SqueezeBertForMaskedLM</a> (SqueezeBERT model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/tapas#transformers.TapasForMaskedLM">TapasForMaskedLM</a> (TAPAS model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <code>Wav2Vec2ForMaskedLM</code>(Wav2Vec2 model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/xlm#transformers.XLMWithLMHeadModel">XLMWithLMHeadModel</a> (XLM model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/xlm-roberta#transformers.XLMRobertaForMaskedLM">XLMRobertaForMaskedLM</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLConfig">XLMRobertaXLConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForMaskedLM">XLMRobertaXLForMaskedLM</a> (XLM-RoBERTa-XL model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/yoso#transformers.YosoConfig">YosoConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/yoso#transformers.YosoForMaskedLM">YosoForMaskedLM</a> (YOSO model)</li>
</ul>`,name:"config"}]}}),sE=new w({props:{code:`from transformers import AutoConfig, AutoModelForMaskedLM

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForMaskedLM.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForMaskedLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMaskedLM.from_config(config)`}}),lE=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15792/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15792/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15792/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15792/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15792/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15792/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15792/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),iE=new w({props:{code:`from transformers import AutoConfig, AutoModelForMaskedLM

# Download model and configuration from huggingface.co and cache.
model = AutoModelForMaskedLM.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForMaskedLM.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForMaskedLM.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForMaskedLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMaskedLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMaskedLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMaskedLM.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),dE=new z({}),cE=new E({props:{name:"class transformers.AutoModelForSeq2SeqLM",anchor:"transformers.AutoModelForSeq2SeqLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15792/src/transformers/models/auto/modeling_auto.py#L708"}}),mE=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15792/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15792/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15792/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/bart#transformers.BartForConditionalGeneration">BartForConditionalGeneration</a> (BART model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusConfig">BigBirdPegasusConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusForConditionalGeneration">BigBirdPegasusForConditionalGeneration</a> (BigBirdPegasus model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/blenderbot#transformers.BlenderbotConfig">BlenderbotConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/blenderbot#transformers.BlenderbotForConditionalGeneration">BlenderbotForConditionalGeneration</a> (Blenderbot model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/blenderbot-small#transformers.BlenderbotSmallConfig">BlenderbotSmallConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/blenderbot-small#transformers.BlenderbotSmallForConditionalGeneration">BlenderbotSmallForConditionalGeneration</a> (BlenderbotSmall model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/encoder-decoder#transformers.EncoderDecoderConfig">EncoderDecoderConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/encoder-decoder#transformers.EncoderDecoderModel">EncoderDecoderModel</a> (Encoder decoder model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/fsmt#transformers.FSMTConfig">FSMTConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/fsmt#transformers.FSMTForConditionalGeneration">FSMTForConditionalGeneration</a> (FairSeq Machine-Translation model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/led#transformers.LEDConfig">LEDConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/led#transformers.LEDForConditionalGeneration">LEDForConditionalGeneration</a> (LED model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/m2m_100#transformers.M2M100Config">M2M100Config</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/m2m_100#transformers.M2M100ForConditionalGeneration">M2M100ForConditionalGeneration</a> (M2M100 model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/mbart#transformers.MBartForConditionalGeneration">MBartForConditionalGeneration</a> (mBART model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/mt5#transformers.MT5Config">MT5Config</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/mt5#transformers.MT5ForConditionalGeneration">MT5ForConditionalGeneration</a> (mT5 model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/marian#transformers.MarianConfig">MarianConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/marian#transformers.MarianMTModel">MarianMTModel</a> (Marian model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/plbart#transformers.PLBartConfig">PLBartConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/plbart#transformers.PLBartForConditionalGeneration">PLBartForConditionalGeneration</a> (PLBart model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/pegasus#transformers.PegasusConfig">PegasusConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/pegasus#transformers.PegasusForConditionalGeneration">PegasusForConditionalGeneration</a> (Pegasus model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/prophetnet#transformers.ProphetNetConfig">ProphetNetConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/prophetnet#transformers.ProphetNetForConditionalGeneration">ProphetNetForConditionalGeneration</a> (ProphetNet model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/t5#transformers.T5Config">T5Config</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/t5#transformers.T5ForConditionalGeneration">T5ForConditionalGeneration</a> (T5 model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetConfig">XLMProphetNetConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetForConditionalGeneration">XLMProphetNetForConditionalGeneration</a> (XLMProphetNet model)</li>
</ul>`,name:"config"}]}}),gE=new w({props:{code:`from transformers import AutoConfig, AutoModelForSeq2SeqLM

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("t5-base")
model = AutoModelForSeq2SeqLM.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForSeq2SeqLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;t5-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSeq2SeqLM.from_config(config)`}}),hE=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15792/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15792/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15792/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15792/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15792/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15792/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15792/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),pE=new w({props:{code:`from transformers import AutoConfig, AutoModelForSeq2SeqLM

# Download model and configuration from huggingface.co and cache.
model = AutoModelForSeq2SeqLM.from_pretrained("t5-base")

# Update configuration during loading
model = AutoModelForSeq2SeqLM.from_pretrained("t5-base", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/t5_tf_model_config.json")
model = AutoModelForSeq2SeqLM.from_pretrained(
    "./tf_model/t5_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForSeq2SeqLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSeq2SeqLM.from_pretrained(<span class="hljs-string">&quot;t5-base&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSeq2SeqLM.from_pretrained(<span class="hljs-string">&quot;t5-base&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/t5_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSeq2SeqLM.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/t5_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),_E=new z({}),uE=new E({props:{name:"class transformers.AutoModelForSequenceClassification",anchor:"transformers.AutoModelForSequenceClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15792/src/transformers/models/auto/modeling_auto.py#L717"}}),vE=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15792/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15792/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15792/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/albert#transformers.AlbertForSequenceClassification">AlbertForSequenceClassification</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/bart#transformers.BartForSequenceClassification">BartForSequenceClassification</a> (BART model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/bert#transformers.BertForSequenceClassification">BertForSequenceClassification</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/big_bird#transformers.BigBirdForSequenceClassification">BigBirdForSequenceClassification</a> (BigBird model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusConfig">BigBirdPegasusConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusForSequenceClassification">BigBirdPegasusForSequenceClassification</a> (BigBirdPegasus model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/ctrl#transformers.CTRLConfig">CTRLConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/ctrl#transformers.CTRLForSequenceClassification">CTRLForSequenceClassification</a> (CTRL model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/camembert#transformers.CamembertForSequenceClassification">CamembertForSequenceClassification</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/canine#transformers.CanineConfig">CanineConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/canine#transformers.CanineForSequenceClassification">CanineForSequenceClassification</a> (Canine model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/convbert#transformers.ConvBertForSequenceClassification">ConvBertForSequenceClassification</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/deberta#transformers.DebertaForSequenceClassification">DebertaForSequenceClassification</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/deberta-v2#transformers.DebertaV2ForSequenceClassification">DebertaV2ForSequenceClassification</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/distilbert#transformers.DistilBertForSequenceClassification">DistilBertForSequenceClassification</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/electra#transformers.ElectraForSequenceClassification">ElectraForSequenceClassification</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/fnet#transformers.FNetConfig">FNetConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/fnet#transformers.FNetForSequenceClassification">FNetForSequenceClassification</a> (FNet model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/flaubert#transformers.FlaubertForSequenceClassification">FlaubertForSequenceClassification</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/funnel#transformers.FunnelForSequenceClassification">FunnelForSequenceClassification</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/gpt2#transformers.GPT2ForSequenceClassification">GPT2ForSequenceClassification</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/gptj#transformers.GPTJConfig">GPTJConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/gptj#transformers.GPTJForSequenceClassification">GPTJForSequenceClassification</a> (GPT-J model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/gpt_neo#transformers.GPTNeoConfig">GPTNeoConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/gpt_neo#transformers.GPTNeoForSequenceClassification">GPTNeoForSequenceClassification</a> (GPT Neo model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/ibert#transformers.IBertConfig">IBertConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/ibert#transformers.IBertForSequenceClassification">IBertForSequenceClassification</a> (I-BERT model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/led#transformers.LEDConfig">LEDConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/led#transformers.LEDForSequenceClassification">LEDForSequenceClassification</a> (LED model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/layoutlm#transformers.LayoutLMForSequenceClassification">LayoutLMForSequenceClassification</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/layoutlmv2#transformers.LayoutLMv2Config">LayoutLMv2Config</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/layoutlmv2#transformers.LayoutLMv2ForSequenceClassification">LayoutLMv2ForSequenceClassification</a> (LayoutLMv2 model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/longformer#transformers.LongformerForSequenceClassification">LongformerForSequenceClassification</a> (Longformer model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/mbart#transformers.MBartForSequenceClassification">MBartForSequenceClassification</a> (mBART model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/mpnet#transformers.MPNetForSequenceClassification">MPNetForSequenceClassification</a> (MPNet model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/megatron-bert#transformers.MegatronBertConfig">MegatronBertConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/megatron-bert#transformers.MegatronBertForSequenceClassification">MegatronBertForSequenceClassification</a> (MegatronBert model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/mobilebert#transformers.MobileBertForSequenceClassification">MobileBertForSequenceClassification</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/nystromformer#transformers.NystromformerConfig">NystromformerConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/nystromformer#transformers.NystromformerForSequenceClassification">NystromformerForSequenceClassification</a> (Nystromformer model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig">OpenAIGPTConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/openai-gpt#transformers.OpenAIGPTForSequenceClassification">OpenAIGPTForSequenceClassification</a> (OpenAI GPT model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/plbart#transformers.PLBartConfig">PLBartConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/plbart#transformers.PLBartForSequenceClassification">PLBartForSequenceClassification</a> (PLBart model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/perceiver#transformers.PerceiverConfig">PerceiverConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/perceiver#transformers.PerceiverForSequenceClassification">PerceiverForSequenceClassification</a> (Perceiver model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/qdqbert#transformers.QDQBertConfig">QDQBertConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/qdqbert#transformers.QDQBertForSequenceClassification">QDQBertForSequenceClassification</a> (QDQBert model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/reformer#transformers.ReformerConfig">ReformerConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/reformer#transformers.ReformerForSequenceClassification">ReformerForSequenceClassification</a> (Reformer model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/rembert#transformers.RemBertForSequenceClassification">RemBertForSequenceClassification</a> (RemBERT model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/roformer#transformers.RoFormerForSequenceClassification">RoFormerForSequenceClassification</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/roberta#transformers.RobertaForSequenceClassification">RobertaForSequenceClassification</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/squeezebert#transformers.SqueezeBertConfig">SqueezeBertConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/squeezebert#transformers.SqueezeBertForSequenceClassification">SqueezeBertForSequenceClassification</a> (SqueezeBERT model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/tapas#transformers.TapasForSequenceClassification">TapasForSequenceClassification</a> (TAPAS model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/transfo-xl#transformers.TransfoXLConfig">TransfoXLConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/transfo-xl#transformers.TransfoXLForSequenceClassification">TransfoXLForSequenceClassification</a> (Transformer-XL model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/xlm#transformers.XLMForSequenceClassification">XLMForSequenceClassification</a> (XLM model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/xlm-roberta#transformers.XLMRobertaForSequenceClassification">XLMRobertaForSequenceClassification</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLConfig">XLMRobertaXLConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForSequenceClassification">XLMRobertaXLForSequenceClassification</a> (XLM-RoBERTa-XL model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/xlnet#transformers.XLNetForSequenceClassification">XLNetForSequenceClassification</a> (XLNet model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/yoso#transformers.YosoConfig">YosoConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/yoso#transformers.YosoForSequenceClassification">YosoForSequenceClassification</a> (YOSO model)</li>
</ul>`,name:"config"}]}}),TE=new w({props:{code:`from transformers import AutoConfig, AutoModelForSequenceClassification

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForSequenceClassification.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSequenceClassification.from_config(config)`}}),FE=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15792/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15792/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15792/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15792/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15792/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15792/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15792/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),CE=new w({props:{code:`from transformers import AutoConfig, AutoModelForSequenceClassification

# Download model and configuration from huggingface.co and cache.
model = AutoModelForSequenceClassification.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForSequenceClassification.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForSequenceClassification.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSequenceClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),ME=new z({}),EE=new E({props:{name:"class transformers.AutoModelForMultipleChoice",anchor:"transformers.AutoModelForMultipleChoice",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15792/src/transformers/models/auto/modeling_auto.py#L751"}}),wE=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15792/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15792/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15792/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/albert#transformers.AlbertForMultipleChoice">AlbertForMultipleChoice</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/bert#transformers.BertForMultipleChoice">BertForMultipleChoice</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/big_bird#transformers.BigBirdForMultipleChoice">BigBirdForMultipleChoice</a> (BigBird model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/camembert#transformers.CamembertForMultipleChoice">CamembertForMultipleChoice</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/canine#transformers.CanineConfig">CanineConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/canine#transformers.CanineForMultipleChoice">CanineForMultipleChoice</a> (Canine model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/convbert#transformers.ConvBertForMultipleChoice">ConvBertForMultipleChoice</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/distilbert#transformers.DistilBertForMultipleChoice">DistilBertForMultipleChoice</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/electra#transformers.ElectraForMultipleChoice">ElectraForMultipleChoice</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/fnet#transformers.FNetConfig">FNetConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/fnet#transformers.FNetForMultipleChoice">FNetForMultipleChoice</a> (FNet model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/flaubert#transformers.FlaubertForMultipleChoice">FlaubertForMultipleChoice</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/funnel#transformers.FunnelForMultipleChoice">FunnelForMultipleChoice</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/ibert#transformers.IBertConfig">IBertConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/ibert#transformers.IBertForMultipleChoice">IBertForMultipleChoice</a> (I-BERT model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/longformer#transformers.LongformerForMultipleChoice">LongformerForMultipleChoice</a> (Longformer model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/mpnet#transformers.MPNetForMultipleChoice">MPNetForMultipleChoice</a> (MPNet model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/megatron-bert#transformers.MegatronBertConfig">MegatronBertConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/megatron-bert#transformers.MegatronBertForMultipleChoice">MegatronBertForMultipleChoice</a> (MegatronBert model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/mobilebert#transformers.MobileBertForMultipleChoice">MobileBertForMultipleChoice</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/nystromformer#transformers.NystromformerConfig">NystromformerConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/nystromformer#transformers.NystromformerForMultipleChoice">NystromformerForMultipleChoice</a> (Nystromformer model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/qdqbert#transformers.QDQBertConfig">QDQBertConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/qdqbert#transformers.QDQBertForMultipleChoice">QDQBertForMultipleChoice</a> (QDQBert model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/rembert#transformers.RemBertForMultipleChoice">RemBertForMultipleChoice</a> (RemBERT model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/roformer#transformers.RoFormerForMultipleChoice">RoFormerForMultipleChoice</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/roberta#transformers.RobertaForMultipleChoice">RobertaForMultipleChoice</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/squeezebert#transformers.SqueezeBertConfig">SqueezeBertConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/squeezebert#transformers.SqueezeBertForMultipleChoice">SqueezeBertForMultipleChoice</a> (SqueezeBERT model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/xlm#transformers.XLMForMultipleChoice">XLMForMultipleChoice</a> (XLM model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/xlm-roberta#transformers.XLMRobertaForMultipleChoice">XLMRobertaForMultipleChoice</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLConfig">XLMRobertaXLConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForMultipleChoice">XLMRobertaXLForMultipleChoice</a> (XLM-RoBERTa-XL model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/xlnet#transformers.XLNetForMultipleChoice">XLNetForMultipleChoice</a> (XLNet model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/yoso#transformers.YosoConfig">YosoConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/yoso#transformers.YosoForMultipleChoice">YosoForMultipleChoice</a> (YOSO model)</li>
</ul>`,name:"config"}]}}),AE=new w({props:{code:`from transformers import AutoConfig, AutoModelForMultipleChoice

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForMultipleChoice.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForMultipleChoice

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMultipleChoice.from_config(config)`}}),LE=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15792/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15792/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15792/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15792/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15792/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15792/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15792/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),BE=new w({props:{code:`from transformers import AutoConfig, AutoModelForMultipleChoice

# Download model and configuration from huggingface.co and cache.
model = AutoModelForMultipleChoice.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForMultipleChoice.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForMultipleChoice.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForMultipleChoice

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMultipleChoice.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMultipleChoice.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMultipleChoice.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),kE=new z({}),xE=new E({props:{name:"class transformers.AutoModelForNextSentencePrediction",anchor:"transformers.AutoModelForNextSentencePrediction",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15792/src/transformers/models/auto/modeling_auto.py#L758"}}),SE=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15792/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15792/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15792/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/bert#transformers.BertForNextSentencePrediction">BertForNextSentencePrediction</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/fnet#transformers.FNetConfig">FNetConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/fnet#transformers.FNetForNextSentencePrediction">FNetForNextSentencePrediction</a> (FNet model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/megatron-bert#transformers.MegatronBertConfig">MegatronBertConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/megatron-bert#transformers.MegatronBertForNextSentencePrediction">MegatronBertForNextSentencePrediction</a> (MegatronBert model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/mobilebert#transformers.MobileBertForNextSentencePrediction">MobileBertForNextSentencePrediction</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/qdqbert#transformers.QDQBertConfig">QDQBertConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/qdqbert#transformers.QDQBertForNextSentencePrediction">QDQBertForNextSentencePrediction</a> (QDQBert model)</li>
</ul>`,name:"config"}]}}),PE=new w({props:{code:`from transformers import AutoConfig, AutoModelForNextSentencePrediction

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForNextSentencePrediction.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForNextSentencePrediction

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForNextSentencePrediction.from_config(config)`}}),$E=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15792/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15792/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15792/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15792/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15792/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15792/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15792/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),IE=new w({props:{code:`from transformers import AutoConfig, AutoModelForNextSentencePrediction

# Download model and configuration from huggingface.co and cache.
model = AutoModelForNextSentencePrediction.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForNextSentencePrediction.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForNextSentencePrediction.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForNextSentencePrediction

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForNextSentencePrediction.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForNextSentencePrediction.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForNextSentencePrediction.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),jE=new z({}),NE=new E({props:{name:"class transformers.AutoModelForTokenClassification",anchor:"transformers.AutoModelForTokenClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15792/src/transformers/models/auto/modeling_auto.py#L744"}}),qE=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15792/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15792/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15792/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/albert#transformers.AlbertForTokenClassification">AlbertForTokenClassification</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/bert#transformers.BertForTokenClassification">BertForTokenClassification</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/big_bird#transformers.BigBirdForTokenClassification">BigBirdForTokenClassification</a> (BigBird model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/camembert#transformers.CamembertForTokenClassification">CamembertForTokenClassification</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/canine#transformers.CanineConfig">CanineConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/canine#transformers.CanineForTokenClassification">CanineForTokenClassification</a> (Canine model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/convbert#transformers.ConvBertForTokenClassification">ConvBertForTokenClassification</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/deberta#transformers.DebertaForTokenClassification">DebertaForTokenClassification</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/deberta-v2#transformers.DebertaV2ForTokenClassification">DebertaV2ForTokenClassification</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/distilbert#transformers.DistilBertForTokenClassification">DistilBertForTokenClassification</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/electra#transformers.ElectraForTokenClassification">ElectraForTokenClassification</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/fnet#transformers.FNetConfig">FNetConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/fnet#transformers.FNetForTokenClassification">FNetForTokenClassification</a> (FNet model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/flaubert#transformers.FlaubertForTokenClassification">FlaubertForTokenClassification</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/funnel#transformers.FunnelForTokenClassification">FunnelForTokenClassification</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/gpt2#transformers.GPT2ForTokenClassification">GPT2ForTokenClassification</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/ibert#transformers.IBertConfig">IBertConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/ibert#transformers.IBertForTokenClassification">IBertForTokenClassification</a> (I-BERT model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/layoutlm#transformers.LayoutLMForTokenClassification">LayoutLMForTokenClassification</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/layoutlmv2#transformers.LayoutLMv2Config">LayoutLMv2Config</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/layoutlmv2#transformers.LayoutLMv2ForTokenClassification">LayoutLMv2ForTokenClassification</a> (LayoutLMv2 model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/longformer#transformers.LongformerForTokenClassification">LongformerForTokenClassification</a> (Longformer model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/mpnet#transformers.MPNetForTokenClassification">MPNetForTokenClassification</a> (MPNet model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/megatron-bert#transformers.MegatronBertConfig">MegatronBertConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/megatron-bert#transformers.MegatronBertForTokenClassification">MegatronBertForTokenClassification</a> (MegatronBert model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/mobilebert#transformers.MobileBertForTokenClassification">MobileBertForTokenClassification</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/nystromformer#transformers.NystromformerConfig">NystromformerConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/nystromformer#transformers.NystromformerForTokenClassification">NystromformerForTokenClassification</a> (Nystromformer model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/qdqbert#transformers.QDQBertConfig">QDQBertConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/qdqbert#transformers.QDQBertForTokenClassification">QDQBertForTokenClassification</a> (QDQBert model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/rembert#transformers.RemBertForTokenClassification">RemBertForTokenClassification</a> (RemBERT model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/roformer#transformers.RoFormerForTokenClassification">RoFormerForTokenClassification</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/roberta#transformers.RobertaForTokenClassification">RobertaForTokenClassification</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/squeezebert#transformers.SqueezeBertConfig">SqueezeBertConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/squeezebert#transformers.SqueezeBertForTokenClassification">SqueezeBertForTokenClassification</a> (SqueezeBERT model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/xlm#transformers.XLMForTokenClassification">XLMForTokenClassification</a> (XLM model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/xlm-roberta#transformers.XLMRobertaForTokenClassification">XLMRobertaForTokenClassification</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLConfig">XLMRobertaXLConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForTokenClassification">XLMRobertaXLForTokenClassification</a> (XLM-RoBERTa-XL model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/xlnet#transformers.XLNetForTokenClassification">XLNetForTokenClassification</a> (XLNet model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/yoso#transformers.YosoConfig">YosoConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/yoso#transformers.YosoForTokenClassification">YosoForTokenClassification</a> (YOSO model)</li>
</ul>`,name:"config"}]}}),GE=new w({props:{code:`from transformers import AutoConfig, AutoModelForTokenClassification

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForTokenClassification.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForTokenClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForTokenClassification.from_config(config)`}}),OE=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15792/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15792/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15792/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15792/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15792/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15792/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15792/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),XE=new w({props:{code:`from transformers import AutoConfig, AutoModelForTokenClassification

# Download model and configuration from huggingface.co and cache.
model = AutoModelForTokenClassification.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForTokenClassification.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForTokenClassification.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForTokenClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForTokenClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForTokenClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForTokenClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),zE=new z({}),VE=new E({props:{name:"class transformers.AutoModelForQuestionAnswering",anchor:"transformers.AutoModelForQuestionAnswering",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15792/src/transformers/models/auto/modeling_auto.py#L726"}}),QE=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15792/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15792/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15792/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/albert#transformers.AlbertForQuestionAnswering">AlbertForQuestionAnswering</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/bart#transformers.BartForQuestionAnswering">BartForQuestionAnswering</a> (BART model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/bert#transformers.BertForQuestionAnswering">BertForQuestionAnswering</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/big_bird#transformers.BigBirdForQuestionAnswering">BigBirdForQuestionAnswering</a> (BigBird model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusConfig">BigBirdPegasusConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusForQuestionAnswering">BigBirdPegasusForQuestionAnswering</a> (BigBirdPegasus model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/camembert#transformers.CamembertForQuestionAnswering">CamembertForQuestionAnswering</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/canine#transformers.CanineConfig">CanineConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/canine#transformers.CanineForQuestionAnswering">CanineForQuestionAnswering</a> (Canine model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/convbert#transformers.ConvBertForQuestionAnswering">ConvBertForQuestionAnswering</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/deberta#transformers.DebertaForQuestionAnswering">DebertaForQuestionAnswering</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/deberta-v2#transformers.DebertaV2ForQuestionAnswering">DebertaV2ForQuestionAnswering</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/distilbert#transformers.DistilBertForQuestionAnswering">DistilBertForQuestionAnswering</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/electra#transformers.ElectraForQuestionAnswering">ElectraForQuestionAnswering</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/fnet#transformers.FNetConfig">FNetConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/fnet#transformers.FNetForQuestionAnswering">FNetForQuestionAnswering</a> (FNet model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/flaubert#transformers.FlaubertForQuestionAnsweringSimple">FlaubertForQuestionAnsweringSimple</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/funnel#transformers.FunnelForQuestionAnswering">FunnelForQuestionAnswering</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/gptj#transformers.GPTJConfig">GPTJConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/gptj#transformers.GPTJForQuestionAnswering">GPTJForQuestionAnswering</a> (GPT-J model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/ibert#transformers.IBertConfig">IBertConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/ibert#transformers.IBertForQuestionAnswering">IBertForQuestionAnswering</a> (I-BERT model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/led#transformers.LEDConfig">LEDConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/led#transformers.LEDForQuestionAnswering">LEDForQuestionAnswering</a> (LED model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/layoutlmv2#transformers.LayoutLMv2Config">LayoutLMv2Config</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/layoutlmv2#transformers.LayoutLMv2ForQuestionAnswering">LayoutLMv2ForQuestionAnswering</a> (LayoutLMv2 model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/longformer#transformers.LongformerForQuestionAnswering">LongformerForQuestionAnswering</a> (Longformer model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/lxmert#transformers.LxmertConfig">LxmertConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/lxmert#transformers.LxmertForQuestionAnswering">LxmertForQuestionAnswering</a> (LXMERT model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/mbart#transformers.MBartForQuestionAnswering">MBartForQuestionAnswering</a> (mBART model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/mpnet#transformers.MPNetForQuestionAnswering">MPNetForQuestionAnswering</a> (MPNet model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/megatron-bert#transformers.MegatronBertConfig">MegatronBertConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/megatron-bert#transformers.MegatronBertForQuestionAnswering">MegatronBertForQuestionAnswering</a> (MegatronBert model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/mobilebert#transformers.MobileBertForQuestionAnswering">MobileBertForQuestionAnswering</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/nystromformer#transformers.NystromformerConfig">NystromformerConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/nystromformer#transformers.NystromformerForQuestionAnswering">NystromformerForQuestionAnswering</a> (Nystromformer model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/qdqbert#transformers.QDQBertConfig">QDQBertConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/qdqbert#transformers.QDQBertForQuestionAnswering">QDQBertForQuestionAnswering</a> (QDQBert model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/reformer#transformers.ReformerConfig">ReformerConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/reformer#transformers.ReformerForQuestionAnswering">ReformerForQuestionAnswering</a> (Reformer model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/rembert#transformers.RemBertForQuestionAnswering">RemBertForQuestionAnswering</a> (RemBERT model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/roformer#transformers.RoFormerForQuestionAnswering">RoFormerForQuestionAnswering</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/roberta#transformers.RobertaForQuestionAnswering">RobertaForQuestionAnswering</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/splinter#transformers.SplinterConfig">SplinterConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/splinter#transformers.SplinterForQuestionAnswering">SplinterForQuestionAnswering</a> (Splinter model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/squeezebert#transformers.SqueezeBertConfig">SqueezeBertConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/squeezebert#transformers.SqueezeBertForQuestionAnswering">SqueezeBertForQuestionAnswering</a> (SqueezeBERT model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/xlm#transformers.XLMForQuestionAnsweringSimple">XLMForQuestionAnsweringSimple</a> (XLM model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/xlm-roberta#transformers.XLMRobertaForQuestionAnswering">XLMRobertaForQuestionAnswering</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLConfig">XLMRobertaXLConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForQuestionAnswering">XLMRobertaXLForQuestionAnswering</a> (XLM-RoBERTa-XL model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/xlnet#transformers.XLNetForQuestionAnsweringSimple">XLNetForQuestionAnsweringSimple</a> (XLNet model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/yoso#transformers.YosoConfig">YosoConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/yoso#transformers.YosoForQuestionAnswering">YosoForQuestionAnswering</a> (YOSO model)</li>
</ul>`,name:"config"}]}}),HE=new w({props:{code:`from transformers import AutoConfig, AutoModelForQuestionAnswering

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForQuestionAnswering.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForQuestionAnswering.from_config(config)`}}),UE=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15792/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15792/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15792/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15792/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15792/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15792/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15792/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),JE=new w({props:{code:`from transformers import AutoConfig, AutoModelForQuestionAnswering

# Download model and configuration from huggingface.co and cache.
model = AutoModelForQuestionAnswering.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForQuestionAnswering.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForQuestionAnswering.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForQuestionAnswering.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),YE=new z({}),KE=new E({props:{name:"class transformers.AutoModelForTableQuestionAnswering",anchor:"transformers.AutoModelForTableQuestionAnswering",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15792/src/transformers/models/auto/modeling_auto.py#L733"}}),e3=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15792/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15792/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15792/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/tapas#transformers.TapasForQuestionAnswering">TapasForQuestionAnswering</a> (TAPAS model)</li>
</ul>`,name:"config"}]}}),o3=new w({props:{code:`from transformers import AutoConfig, AutoModelForTableQuestionAnswering

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("google/tapas-base-finetuned-wtq")
model = AutoModelForTableQuestionAnswering.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForTableQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;google/tapas-base-finetuned-wtq&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForTableQuestionAnswering.from_config(config)`}}),r3=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15792/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15792/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15792/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15792/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15792/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15792/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15792/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),t3=new w({props:{code:`from transformers import AutoConfig, AutoModelForTableQuestionAnswering

# Download model and configuration from huggingface.co and cache.
model = AutoModelForTableQuestionAnswering.from_pretrained("google/tapas-base-finetuned-wtq")

# Update configuration during loading
model = AutoModelForTableQuestionAnswering.from_pretrained("google/tapas-base-finetuned-wtq", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/tapas_tf_model_config.json")
model = AutoModelForTableQuestionAnswering.from_pretrained(
    "./tf_model/tapas_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForTableQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForTableQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;google/tapas-base-finetuned-wtq&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForTableQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;google/tapas-base-finetuned-wtq&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/tapas_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForTableQuestionAnswering.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/tapas_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),a3=new z({}),n3=new E({props:{name:"class transformers.AutoModelForImageClassification",anchor:"transformers.AutoModelForImageClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15792/src/transformers/models/auto/modeling_auto.py#L767"}}),l3=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15792/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15792/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15792/en/model_doc/beit#transformers.BeitConfig">BeitConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/beit#transformers.BeitForImageClassification">BeitForImageClassification</a> (BEiT model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/convnext#transformers.ConvNextConfig">ConvNextConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/convnext#transformers.ConvNextForImageClassification">ConvNextForImageClassification</a> (ConvNext model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/deit#transformers.DeiTConfig">DeiTConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/deit#transformers.DeiTForImageClassification">DeiTForImageClassification</a> or <a href="/docs/transformers/pr_15792/en/model_doc/deit#transformers.DeiTForImageClassificationWithTeacher">DeiTForImageClassificationWithTeacher</a> (DeiT model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/imagegpt#transformers.ImageGPTConfig">ImageGPTConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/imagegpt#transformers.ImageGPTForImageClassification">ImageGPTForImageClassification</a> (ImageGPT model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/perceiver#transformers.PerceiverConfig">PerceiverConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/perceiver#transformers.PerceiverForImageClassificationLearned">PerceiverForImageClassificationLearned</a> or <a href="/docs/transformers/pr_15792/en/model_doc/perceiver#transformers.PerceiverForImageClassificationFourier">PerceiverForImageClassificationFourier</a> or <a href="/docs/transformers/pr_15792/en/model_doc/perceiver#transformers.PerceiverForImageClassificationConvProcessing">PerceiverForImageClassificationConvProcessing</a> (Perceiver model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/poolformer#transformers.PoolFormerConfig">PoolFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/poolformer#transformers.PoolFormerForImageClassification">PoolFormerForImageClassification</a> (PoolFormer model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/segformer#transformers.SegformerConfig">SegformerConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/segformer#transformers.SegformerForImageClassification">SegformerForImageClassification</a> (SegFormer model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/swin#transformers.SwinConfig">SwinConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/swin#transformers.SwinForImageClassification">SwinForImageClassification</a> (Swin model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/vit#transformers.ViTConfig">ViTConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/vit#transformers.ViTForImageClassification">ViTForImageClassification</a> (ViT model)</li>
</ul>`,name:"config"}]}}),i3=new w({props:{code:`from transformers import AutoConfig, AutoModelForImageClassification

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForImageClassification.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForImageClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForImageClassification.from_config(config)`}}),d3=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15792/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15792/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15792/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15792/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15792/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15792/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15792/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),c3=new w({props:{code:`from transformers import AutoConfig, AutoModelForImageClassification

# Download model and configuration from huggingface.co and cache.
model = AutoModelForImageClassification.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForImageClassification.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForImageClassification.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForImageClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForImageClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForImageClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForImageClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),f3=new z({}),m3=new E({props:{name:"class transformers.AutoModelForVision2Seq",anchor:"transformers.AutoModelForVision2Seq",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15792/src/transformers/models/auto/modeling_auto.py#L797"}}),h3=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15792/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15792/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15792/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderConfig">VisionEncoderDecoderConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderModel">VisionEncoderDecoderModel</a> (Vision Encoder decoder model)</li>
</ul>`,name:"config"}]}}),p3=new w({props:{code:`from transformers import AutoConfig, AutoModelForVision2Seq

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForVision2Seq.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForVision2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForVision2Seq.from_config(config)`}}),_3=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15792/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15792/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15792/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15792/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15792/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15792/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15792/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),u3=new w({props:{code:`from transformers import AutoConfig, AutoModelForVision2Seq

# Download model and configuration from huggingface.co and cache.
model = AutoModelForVision2Seq.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForVision2Seq.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForVision2Seq.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForVision2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForVision2Seq.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForVision2Seq.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForVision2Seq.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),b3=new z({}),v3=new E({props:{name:"class transformers.AutoModelForAudioClassification",anchor:"transformers.AutoModelForAudioClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15792/src/transformers/models/auto/modeling_auto.py#L804"}}),F3=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15792/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15792/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15792/en/model_doc/hubert#transformers.HubertConfig">HubertConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/hubert#transformers.HubertForSequenceClassification">HubertForSequenceClassification</a> (Hubert model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/sew#transformers.SEWConfig">SEWConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/sew#transformers.SEWForSequenceClassification">SEWForSequenceClassification</a> (SEW model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/sew-d#transformers.SEWDConfig">SEWDConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/sew-d#transformers.SEWDForSequenceClassification">SEWDForSequenceClassification</a> (SEW-D model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/unispeech#transformers.UniSpeechConfig">UniSpeechConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/unispeech#transformers.UniSpeechForSequenceClassification">UniSpeechForSequenceClassification</a> (UniSpeech model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/unispeech-sat#transformers.UniSpeechSatConfig">UniSpeechSatConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/unispeech-sat#transformers.UniSpeechSatForSequenceClassification">UniSpeechSatForSequenceClassification</a> (UniSpeechSat model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/wav2vec2#transformers.Wav2Vec2ForSequenceClassification">Wav2Vec2ForSequenceClassification</a> (Wav2Vec2 model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/wavlm#transformers.WavLMConfig">WavLMConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/wavlm#transformers.WavLMForSequenceClassification">WavLMForSequenceClassification</a> (WavLM model)</li>
</ul>`,name:"config"}]}}),C3=new w({props:{code:`from transformers import AutoConfig, AutoModelForAudioClassification

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForAudioClassification.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForAudioClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioClassification.from_config(config)`}}),M3=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15792/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15792/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15792/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15792/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15792/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15792/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15792/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),E3=new w({props:{code:`from transformers import AutoConfig, AutoModelForAudioClassification

# Download model and configuration from huggingface.co and cache.
model = AutoModelForAudioClassification.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForAudioClassification.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForAudioClassification.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForAudioClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),y3=new z({}),w3=new E({props:{name:"class transformers.AutoModelForAudioFrameClassification",anchor:"transformers.AutoModelForAudioFrameClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15792/src/transformers/models/auto/modeling_auto.py#L827"}}),L3=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15792/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15792/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15792/en/model_doc/unispeech-sat#transformers.UniSpeechSatConfig">UniSpeechSatConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/unispeech-sat#transformers.UniSpeechSatForAudioFrameClassification">UniSpeechSatForAudioFrameClassification</a> (UniSpeechSat model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/wav2vec2#transformers.Wav2Vec2ForAudioFrameClassification">Wav2Vec2ForAudioFrameClassification</a> (Wav2Vec2 model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/wavlm#transformers.WavLMConfig">WavLMConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/wavlm#transformers.WavLMForAudioFrameClassification">WavLMForAudioFrameClassification</a> (WavLM model)</li>
</ul>`,name:"config"}]}}),B3=new w({props:{code:`from transformers import AutoConfig, AutoModelForAudioFrameClassification

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForAudioFrameClassification.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForAudioFrameClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioFrameClassification.from_config(config)`}}),k3=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15792/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15792/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15792/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15792/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15792/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15792/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15792/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),x3=new w({props:{code:`from transformers import AutoConfig, AutoModelForAudioFrameClassification

# Download model and configuration from huggingface.co and cache.
model = AutoModelForAudioFrameClassification.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForAudioFrameClassification.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForAudioFrameClassification.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForAudioFrameClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioFrameClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioFrameClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioFrameClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),R3=new z({}),S3=new E({props:{name:"class transformers.AutoModelForCTC",anchor:"transformers.AutoModelForCTC",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15792/src/transformers/models/auto/modeling_auto.py#L811"}}),$3=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15792/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15792/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15792/en/model_doc/hubert#transformers.HubertConfig">HubertConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/hubert#transformers.HubertForCTC">HubertForCTC</a> (Hubert model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/sew#transformers.SEWConfig">SEWConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/sew#transformers.SEWForCTC">SEWForCTC</a> (SEW model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/sew-d#transformers.SEWDConfig">SEWDConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/sew-d#transformers.SEWDForCTC">SEWDForCTC</a> (SEW-D model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/unispeech#transformers.UniSpeechConfig">UniSpeechConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/unispeech#transformers.UniSpeechForCTC">UniSpeechForCTC</a> (UniSpeech model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/unispeech-sat#transformers.UniSpeechSatConfig">UniSpeechSatConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/unispeech-sat#transformers.UniSpeechSatForCTC">UniSpeechSatForCTC</a> (UniSpeechSat model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/wav2vec2#transformers.Wav2Vec2ForCTC">Wav2Vec2ForCTC</a> (Wav2Vec2 model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/wavlm#transformers.WavLMConfig">WavLMConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/wavlm#transformers.WavLMForCTC">WavLMForCTC</a> (WavLM model)</li>
</ul>`,name:"config"}]}}),I3=new w({props:{code:`from transformers import AutoConfig, AutoModelForCTC

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForCTC.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForCTC

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCTC.from_config(config)`}}),j3=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15792/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15792/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15792/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15792/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15792/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15792/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15792/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),N3=new w({props:{code:`from transformers import AutoConfig, AutoModelForCTC

# Download model and configuration from huggingface.co and cache.
model = AutoModelForCTC.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForCTC.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForCTC.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForCTC

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCTC.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCTC.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCTC.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),D3=new z({}),q3=new E({props:{name:"class transformers.AutoModelForSpeechSeq2Seq",anchor:"transformers.AutoModelForSpeechSeq2Seq",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15792/src/transformers/models/auto/modeling_auto.py#L818"}}),O3=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15792/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15792/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15792/en/model_doc/speech_to_text#transformers.Speech2TextConfig">Speech2TextConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/speech_to_text#transformers.Speech2TextForConditionalGeneration">Speech2TextForConditionalGeneration</a> (Speech2Text model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/speech-encoder-decoder#transformers.SpeechEncoderDecoderConfig">SpeechEncoderDecoderConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/speech-encoder-decoder#transformers.SpeechEncoderDecoderModel">SpeechEncoderDecoderModel</a> (Speech Encoder decoder model)</li>
</ul>`,name:"config"}]}}),X3=new w({props:{code:`from transformers import AutoConfig, AutoModelForSpeechSeq2Seq

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForSpeechSeq2Seq.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForSpeechSeq2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSpeechSeq2Seq.from_config(config)`}}),z3=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15792/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15792/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15792/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15792/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15792/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15792/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15792/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),W3=new w({props:{code:`from transformers import AutoConfig, AutoModelForSpeechSeq2Seq

# Download model and configuration from huggingface.co and cache.
model = AutoModelForSpeechSeq2Seq.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForSpeechSeq2Seq.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForSpeechSeq2Seq.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForSpeechSeq2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSpeechSeq2Seq.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSpeechSeq2Seq.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSpeechSeq2Seq.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),Q3=new z({}),H3=new E({props:{name:"class transformers.AutoModelForAudioXVector",anchor:"transformers.AutoModelForAudioXVector",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15792/src/transformers/models/auto/modeling_auto.py#L836"}}),J3=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15792/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15792/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15792/en/model_doc/unispeech-sat#transformers.UniSpeechSatConfig">UniSpeechSatConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/unispeech-sat#transformers.UniSpeechSatForXVector">UniSpeechSatForXVector</a> (UniSpeechSat model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/wav2vec2#transformers.Wav2Vec2ForXVector">Wav2Vec2ForXVector</a> (Wav2Vec2 model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/wavlm#transformers.WavLMConfig">WavLMConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/wavlm#transformers.WavLMForXVector">WavLMForXVector</a> (WavLM model)</li>
</ul>`,name:"config"}]}}),Y3=new w({props:{code:`from transformers import AutoConfig, AutoModelForAudioXVector

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForAudioXVector.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForAudioXVector

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioXVector.from_config(config)`}}),K3=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15792/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15792/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15792/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15792/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15792/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15792/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15792/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),Z3=new w({props:{code:`from transformers import AutoConfig, AutoModelForAudioXVector

# Download model and configuration from huggingface.co and cache.
model = AutoModelForAudioXVector.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForAudioXVector.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForAudioXVector.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForAudioXVector

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioXVector.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioXVector.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioXVector.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),ey=new z({}),oy=new E({props:{name:"class transformers.AutoModelForMaskedImageModeling",anchor:"transformers.AutoModelForMaskedImageModeling",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15792/src/transformers/models/auto/modeling_auto.py#L843"}}),ty=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15792/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15792/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15792/en/model_doc/deit#transformers.DeiTConfig">DeiTConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/deit#transformers.DeiTForMaskedImageModeling">DeiTForMaskedImageModeling</a> (DeiT model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/swin#transformers.SwinConfig">SwinConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/swin#transformers.SwinForMaskedImageModeling">SwinForMaskedImageModeling</a> (Swin model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/vit#transformers.ViTConfig">ViTConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/vit#transformers.ViTForMaskedImageModeling">ViTForMaskedImageModeling</a> (ViT model)</li>
</ul>`,name:"config"}]}}),ay=new w({props:{code:`from transformers import AutoConfig, AutoModelForMaskedImageModeling

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForMaskedImageModeling.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForMaskedImageModeling

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMaskedImageModeling.from_config(config)`}}),ny=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15792/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15792/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15792/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15792/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15792/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15792/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15792/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),sy=new w({props:{code:`from transformers import AutoConfig, AutoModelForMaskedImageModeling

# Download model and configuration from huggingface.co and cache.
model = AutoModelForMaskedImageModeling.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForMaskedImageModeling.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForMaskedImageModeling.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForMaskedImageModeling

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMaskedImageModeling.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMaskedImageModeling.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMaskedImageModeling.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),ly=new z({}),iy=new E({props:{name:"class transformers.AutoModelForObjectDetection",anchor:"transformers.AutoModelForObjectDetection",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15792/src/transformers/models/auto/modeling_auto.py#L790"}}),cy=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15792/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15792/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15792/en/model_doc/detr#transformers.DetrConfig">DetrConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/detr#transformers.DetrForObjectDetection">DetrForObjectDetection</a> (DETR model)</li>
</ul>`,name:"config"}]}}),fy=new w({props:{code:`from transformers import AutoConfig, AutoModelForObjectDetection

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForObjectDetection.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForObjectDetection

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForObjectDetection.from_config(config)`}}),my=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15792/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15792/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15792/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15792/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15792/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15792/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15792/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),gy=new w({props:{code:`from transformers import AutoConfig, AutoModelForObjectDetection

# Download model and configuration from huggingface.co and cache.
model = AutoModelForObjectDetection.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForObjectDetection.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForObjectDetection.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForObjectDetection

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForObjectDetection.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForObjectDetection.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForObjectDetection.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),hy=new z({}),py=new E({props:{name:"class transformers.AutoModelForImageSegmentation",anchor:"transformers.AutoModelForImageSegmentation",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15792/src/transformers/models/auto/modeling_auto.py#L774"}}),uy=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15792/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15792/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15792/en/model_doc/detr#transformers.DetrConfig">DetrConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/detr#transformers.DetrForSegmentation">DetrForSegmentation</a> (DETR model)</li>
</ul>`,name:"config"}]}}),by=new w({props:{code:`from transformers import AutoConfig, AutoModelForImageSegmentation

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForImageSegmentation.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForImageSegmentation

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForImageSegmentation.from_config(config)`}}),vy=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15792/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15792/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15792/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15792/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15792/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15792/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15792/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),Ty=new w({props:{code:`from transformers import AutoConfig, AutoModelForImageSegmentation

# Download model and configuration from huggingface.co and cache.
model = AutoModelForImageSegmentation.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForImageSegmentation.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForImageSegmentation.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForImageSegmentation

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForImageSegmentation.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForImageSegmentation.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForImageSegmentation.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),Fy=new z({}),Cy=new E({props:{name:"class transformers.AutoModelForSemanticSegmentation",anchor:"transformers.AutoModelForSemanticSegmentation",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15792/src/transformers/models/auto/modeling_auto.py#L781"}}),Ey=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15792/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15792/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15792/en/model_doc/beit#transformers.BeitConfig">BeitConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/beit#transformers.BeitForSemanticSegmentation">BeitForSemanticSegmentation</a> (BEiT model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/segformer#transformers.SegformerConfig">SegformerConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/segformer#transformers.SegformerForSemanticSegmentation">SegformerForSemanticSegmentation</a> (SegFormer model)</li>
</ul>`,name:"config"}]}}),yy=new w({props:{code:`from transformers import AutoConfig, AutoModelForSemanticSegmentation

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForSemanticSegmentation.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForSemanticSegmentation

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSemanticSegmentation.from_config(config)`}}),wy=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15792/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15792/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15792/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15792/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15792/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15792/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15792/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),Ly=new w({props:{code:`from transformers import AutoConfig, AutoModelForSemanticSegmentation

# Download model and configuration from huggingface.co and cache.
model = AutoModelForSemanticSegmentation.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForSemanticSegmentation.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForSemanticSegmentation.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForSemanticSegmentation

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSemanticSegmentation.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSemanticSegmentation.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSemanticSegmentation.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),By=new z({}),ky=new E({props:{name:"class transformers.TFAutoModel",anchor:"transformers.TFAutoModel",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15792/src/transformers/models/auto/modeling_tf_auto.py#L371"}}),Ry=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15792/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15792/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15792/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/albert#transformers.TFAlbertModel">TFAlbertModel</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/bart#transformers.TFBartModel">TFBartModel</a> (BART model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/bert#transformers.TFBertModel">TFBertModel</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/blenderbot#transformers.BlenderbotConfig">BlenderbotConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/blenderbot#transformers.TFBlenderbotModel">TFBlenderbotModel</a> (Blenderbot model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/blenderbot-small#transformers.BlenderbotSmallConfig">BlenderbotSmallConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/blenderbot-small#transformers.TFBlenderbotSmallModel">TFBlenderbotSmallModel</a> (BlenderbotSmall model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/clip#transformers.CLIPConfig">CLIPConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/clip#transformers.TFCLIPModel">TFCLIPModel</a> (CLIP model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/ctrl#transformers.CTRLConfig">CTRLConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/ctrl#transformers.TFCTRLModel">TFCTRLModel</a> (CTRL model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/camembert#transformers.TFCamembertModel">TFCamembertModel</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/convbert#transformers.TFConvBertModel">TFConvBertModel</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/dpr#transformers.DPRConfig">DPRConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/dpr#transformers.TFDPRQuestionEncoder">TFDPRQuestionEncoder</a> (DPR model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/deberta#transformers.TFDebertaModel">TFDebertaModel</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/deberta-v2#transformers.TFDebertaV2Model">TFDebertaV2Model</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/distilbert#transformers.TFDistilBertModel">TFDistilBertModel</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/electra#transformers.TFElectraModel">TFElectraModel</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/flaubert#transformers.TFFlaubertModel">TFFlaubertModel</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/funnel#transformers.TFFunnelModel">TFFunnelModel</a> or <a href="/docs/transformers/pr_15792/en/model_doc/funnel#transformers.TFFunnelBaseModel">TFFunnelBaseModel</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/gpt2#transformers.TFGPT2Model">TFGPT2Model</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/hubert#transformers.HubertConfig">HubertConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/hubert#transformers.TFHubertModel">TFHubertModel</a> (Hubert model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/led#transformers.LEDConfig">LEDConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/led#transformers.TFLEDModel">TFLEDModel</a> (LED model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/layoutlm#transformers.TFLayoutLMModel">TFLayoutLMModel</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/longformer#transformers.TFLongformerModel">TFLongformerModel</a> (Longformer model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/lxmert#transformers.LxmertConfig">LxmertConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/lxmert#transformers.TFLxmertModel">TFLxmertModel</a> (LXMERT model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/mbart#transformers.TFMBartModel">TFMBartModel</a> (mBART model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/mpnet#transformers.TFMPNetModel">TFMPNetModel</a> (MPNet model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/mt5#transformers.MT5Config">MT5Config</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/mt5#transformers.TFMT5Model">TFMT5Model</a> (mT5 model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/marian#transformers.MarianConfig">MarianConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/marian#transformers.TFMarianModel">TFMarianModel</a> (Marian model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/mobilebert#transformers.TFMobileBertModel">TFMobileBertModel</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig">OpenAIGPTConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/openai-gpt#transformers.TFOpenAIGPTModel">TFOpenAIGPTModel</a> (OpenAI GPT model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/pegasus#transformers.PegasusConfig">PegasusConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/pegasus#transformers.TFPegasusModel">TFPegasusModel</a> (Pegasus model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/rembert#transformers.TFRemBertModel">TFRemBertModel</a> (RemBERT model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/roformer#transformers.TFRoFormerModel">TFRoFormerModel</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/roberta#transformers.TFRobertaModel">TFRobertaModel</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/speech_to_text#transformers.Speech2TextConfig">Speech2TextConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/speech_to_text#transformers.TFSpeech2TextModel">TFSpeech2TextModel</a> (Speech2Text model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/t5#transformers.T5Config">T5Config</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/t5#transformers.TFT5Model">TFT5Model</a> (T5 model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/tapas#transformers.TFTapasModel">TFTapasModel</a> (TAPAS model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/transfo-xl#transformers.TransfoXLConfig">TransfoXLConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/transfo-xl#transformers.TFTransfoXLModel">TFTransfoXLModel</a> (Transformer-XL model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/vit#transformers.ViTConfig">ViTConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/vit#transformers.TFViTModel">TFViTModel</a> (ViT model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/wav2vec2#transformers.TFWav2Vec2Model">TFWav2Vec2Model</a> (Wav2Vec2 model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/xlm#transformers.TFXLMModel">TFXLMModel</a> (XLM model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/xlm-roberta#transformers.TFXLMRobertaModel">TFXLMRobertaModel</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/xlnet#transformers.TFXLNetModel">TFXLNetModel</a> (XLNet model)</li>
</ul>`,name:"config"}]}}),Sy=new w({props:{code:`from transformers import AutoConfig, TFAutoModel

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = TFAutoModel.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModel.from_config(config)`}}),Py=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15792/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15792/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15792/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15792/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15792/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),$y=new w({props:{code:`from transformers import AutoConfig, TFAutoModel

# Download model and configuration from huggingface.co and cache.
model = TFAutoModel.from_pretrained("bert-base-cased")

# Update configuration during loading
model = TFAutoModel.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = TFAutoModel.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModel.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModel.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModel.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),Iy=new z({}),jy=new E({props:{name:"class transformers.TFAutoModelForPreTraining",anchor:"transformers.TFAutoModelForPreTraining",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15792/src/transformers/models/auto/modeling_tf_auto.py#L378"}}),Dy=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15792/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15792/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15792/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/albert#transformers.TFAlbertForPreTraining">TFAlbertForPreTraining</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/bart#transformers.TFBartForConditionalGeneration">TFBartForConditionalGeneration</a> (BART model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/bert#transformers.TFBertForPreTraining">TFBertForPreTraining</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/ctrl#transformers.CTRLConfig">CTRLConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/ctrl#transformers.TFCTRLLMHeadModel">TFCTRLLMHeadModel</a> (CTRL model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/camembert#transformers.TFCamembertForMaskedLM">TFCamembertForMaskedLM</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/distilbert#transformers.TFDistilBertForMaskedLM">TFDistilBertForMaskedLM</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/electra#transformers.TFElectraForPreTraining">TFElectraForPreTraining</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/flaubert#transformers.TFFlaubertWithLMHeadModel">TFFlaubertWithLMHeadModel</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/funnel#transformers.TFFunnelForPreTraining">TFFunnelForPreTraining</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/gpt2#transformers.TFGPT2LMHeadModel">TFGPT2LMHeadModel</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/layoutlm#transformers.TFLayoutLMForMaskedLM">TFLayoutLMForMaskedLM</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/lxmert#transformers.LxmertConfig">LxmertConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/lxmert#transformers.TFLxmertForPreTraining">TFLxmertForPreTraining</a> (LXMERT model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/mpnet#transformers.TFMPNetForMaskedLM">TFMPNetForMaskedLM</a> (MPNet model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/mobilebert#transformers.TFMobileBertForPreTraining">TFMobileBertForPreTraining</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig">OpenAIGPTConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/openai-gpt#transformers.TFOpenAIGPTLMHeadModel">TFOpenAIGPTLMHeadModel</a> (OpenAI GPT model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/roberta#transformers.TFRobertaForMaskedLM">TFRobertaForMaskedLM</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/t5#transformers.T5Config">T5Config</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/t5#transformers.TFT5ForConditionalGeneration">TFT5ForConditionalGeneration</a> (T5 model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/tapas#transformers.TFTapasForMaskedLM">TFTapasForMaskedLM</a> (TAPAS model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/transfo-xl#transformers.TransfoXLConfig">TransfoXLConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/transfo-xl#transformers.TFTransfoXLLMHeadModel">TFTransfoXLLMHeadModel</a> (Transformer-XL model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/xlm#transformers.TFXLMWithLMHeadModel">TFXLMWithLMHeadModel</a> (XLM model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForMaskedLM">TFXLMRobertaForMaskedLM</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/xlnet#transformers.TFXLNetLMHeadModel">TFXLNetLMHeadModel</a> (XLNet model)</li>
</ul>`,name:"config"}]}}),qy=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForPreTraining

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = TFAutoModelForPreTraining.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForPreTraining

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForPreTraining.from_config(config)`}}),Gy=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15792/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15792/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15792/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15792/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15792/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),Oy=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForPreTraining

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForPreTraining.from_pretrained("bert-base-cased")

# Update configuration during loading
model = TFAutoModelForPreTraining.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = TFAutoModelForPreTraining.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForPreTraining

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForPreTraining.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForPreTraining.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForPreTraining.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),Xy=new z({}),zy=new E({props:{name:"class transformers.TFAutoModelForCausalLM",anchor:"transformers.TFAutoModelForCausalLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15792/src/transformers/models/auto/modeling_tf_auto.py#L393"}}),Wy=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15792/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15792/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15792/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/bert#transformers.TFBertLMHeadModel">TFBertLMHeadModel</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/ctrl#transformers.CTRLConfig">CTRLConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/ctrl#transformers.TFCTRLLMHeadModel">TFCTRLLMHeadModel</a> (CTRL model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/gpt2#transformers.TFGPT2LMHeadModel">TFGPT2LMHeadModel</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig">OpenAIGPTConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/openai-gpt#transformers.TFOpenAIGPTLMHeadModel">TFOpenAIGPTLMHeadModel</a> (OpenAI GPT model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/rembert#transformers.TFRemBertForCausalLM">TFRemBertForCausalLM</a> (RemBERT model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/roformer#transformers.TFRoFormerForCausalLM">TFRoFormerForCausalLM</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/roberta#transformers.TFRobertaForCausalLM">TFRobertaForCausalLM</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/transfo-xl#transformers.TransfoXLConfig">TransfoXLConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/transfo-xl#transformers.TFTransfoXLLMHeadModel">TFTransfoXLLMHeadModel</a> (Transformer-XL model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/xlm#transformers.TFXLMWithLMHeadModel">TFXLMWithLMHeadModel</a> (XLM model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/xlnet#transformers.TFXLNetLMHeadModel">TFXLNetLMHeadModel</a> (XLNet model)</li>
</ul>`,name:"config"}]}}),Qy=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForCausalLM

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = TFAutoModelForCausalLM.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForCausalLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForCausalLM.from_config(config)`}}),Hy=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15792/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15792/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15792/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15792/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15792/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),Uy=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForCausalLM

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForCausalLM.from_pretrained("bert-base-cased")

# Update configuration during loading
model = TFAutoModelForCausalLM.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = TFAutoModelForCausalLM.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForCausalLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForCausalLM.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),Jy=new z({}),Yy=new E({props:{name:"class transformers.TFAutoModelForImageClassification",anchor:"transformers.TFAutoModelForImageClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15792/src/transformers/models/auto/modeling_tf_auto.py#L400"}}),Zy=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15792/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15792/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15792/en/model_doc/vit#transformers.ViTConfig">ViTConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/vit#transformers.TFViTForImageClassification">TFViTForImageClassification</a> (ViT model)</li>
</ul>`,name:"config"}]}}),ew=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForImageClassification

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = TFAutoModelForImageClassification.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForImageClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForImageClassification.from_config(config)`}}),ow=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15792/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15792/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15792/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15792/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15792/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),rw=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForImageClassification

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForImageClassification.from_pretrained("bert-base-cased")

# Update configuration during loading
model = TFAutoModelForImageClassification.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = TFAutoModelForImageClassification.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForImageClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForImageClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForImageClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForImageClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),tw=new z({}),aw=new E({props:{name:"class transformers.TFAutoModelForMaskedLM",anchor:"transformers.TFAutoModelForMaskedLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15792/src/transformers/models/auto/modeling_tf_auto.py#L414"}}),sw=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15792/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15792/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15792/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/albert#transformers.TFAlbertForMaskedLM">TFAlbertForMaskedLM</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/bert#transformers.TFBertForMaskedLM">TFBertForMaskedLM</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/camembert#transformers.TFCamembertForMaskedLM">TFCamembertForMaskedLM</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/convbert#transformers.TFConvBertForMaskedLM">TFConvBertForMaskedLM</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/deberta#transformers.TFDebertaForMaskedLM">TFDebertaForMaskedLM</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/deberta-v2#transformers.TFDebertaV2ForMaskedLM">TFDebertaV2ForMaskedLM</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/distilbert#transformers.TFDistilBertForMaskedLM">TFDistilBertForMaskedLM</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/electra#transformers.TFElectraForMaskedLM">TFElectraForMaskedLM</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/flaubert#transformers.TFFlaubertWithLMHeadModel">TFFlaubertWithLMHeadModel</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/funnel#transformers.TFFunnelForMaskedLM">TFFunnelForMaskedLM</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/layoutlm#transformers.TFLayoutLMForMaskedLM">TFLayoutLMForMaskedLM</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/longformer#transformers.TFLongformerForMaskedLM">TFLongformerForMaskedLM</a> (Longformer model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/mpnet#transformers.TFMPNetForMaskedLM">TFMPNetForMaskedLM</a> (MPNet model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/mobilebert#transformers.TFMobileBertForMaskedLM">TFMobileBertForMaskedLM</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/rembert#transformers.TFRemBertForMaskedLM">TFRemBertForMaskedLM</a> (RemBERT model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/roformer#transformers.TFRoFormerForMaskedLM">TFRoFormerForMaskedLM</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/roberta#transformers.TFRobertaForMaskedLM">TFRobertaForMaskedLM</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/tapas#transformers.TFTapasForMaskedLM">TFTapasForMaskedLM</a> (TAPAS model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/xlm#transformers.TFXLMWithLMHeadModel">TFXLMWithLMHeadModel</a> (XLM model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForMaskedLM">TFXLMRobertaForMaskedLM</a> (XLM-RoBERTa model)</li>
</ul>`,name:"config"}]}}),lw=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForMaskedLM

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = TFAutoModelForMaskedLM.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForMaskedLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForMaskedLM.from_config(config)`}}),iw=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15792/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15792/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15792/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15792/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15792/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),dw=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForMaskedLM

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForMaskedLM.from_pretrained("bert-base-cased")

# Update configuration during loading
model = TFAutoModelForMaskedLM.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = TFAutoModelForMaskedLM.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForMaskedLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForMaskedLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForMaskedLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForMaskedLM.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),cw=new z({}),fw=new E({props:{name:"class transformers.TFAutoModelForSeq2SeqLM",anchor:"transformers.TFAutoModelForSeq2SeqLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15792/src/transformers/models/auto/modeling_tf_auto.py#L421"}}),gw=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15792/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15792/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15792/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/bart#transformers.TFBartForConditionalGeneration">TFBartForConditionalGeneration</a> (BART model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/blenderbot#transformers.BlenderbotConfig">BlenderbotConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/blenderbot#transformers.TFBlenderbotForConditionalGeneration">TFBlenderbotForConditionalGeneration</a> (Blenderbot model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/blenderbot-small#transformers.BlenderbotSmallConfig">BlenderbotSmallConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/blenderbot-small#transformers.TFBlenderbotSmallForConditionalGeneration">TFBlenderbotSmallForConditionalGeneration</a> (BlenderbotSmall model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/encoder-decoder#transformers.EncoderDecoderConfig">EncoderDecoderConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/encoder-decoder#transformers.TFEncoderDecoderModel">TFEncoderDecoderModel</a> (Encoder decoder model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/led#transformers.LEDConfig">LEDConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/led#transformers.TFLEDForConditionalGeneration">TFLEDForConditionalGeneration</a> (LED model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/mbart#transformers.TFMBartForConditionalGeneration">TFMBartForConditionalGeneration</a> (mBART model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/mt5#transformers.MT5Config">MT5Config</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/mt5#transformers.TFMT5ForConditionalGeneration">TFMT5ForConditionalGeneration</a> (mT5 model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/marian#transformers.MarianConfig">MarianConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/marian#transformers.TFMarianMTModel">TFMarianMTModel</a> (Marian model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/pegasus#transformers.PegasusConfig">PegasusConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/pegasus#transformers.TFPegasusForConditionalGeneration">TFPegasusForConditionalGeneration</a> (Pegasus model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/t5#transformers.T5Config">T5Config</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/t5#transformers.TFT5ForConditionalGeneration">TFT5ForConditionalGeneration</a> (T5 model)</li>
</ul>`,name:"config"}]}}),hw=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForSeq2SeqLM

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("t5-base")
model = TFAutoModelForSeq2SeqLM.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForSeq2SeqLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;t5-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSeq2SeqLM.from_config(config)`}}),pw=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15792/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15792/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15792/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15792/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15792/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),_w=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForSeq2SeqLM

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForSeq2SeqLM.from_pretrained("t5-base")

# Update configuration during loading
model = TFAutoModelForSeq2SeqLM.from_pretrained("t5-base", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/t5_pt_model_config.json")
model = TFAutoModelForSeq2SeqLM.from_pretrained(
    "./pt_model/t5_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForSeq2SeqLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSeq2SeqLM.from_pretrained(<span class="hljs-string">&quot;t5-base&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSeq2SeqLM.from_pretrained(<span class="hljs-string">&quot;t5-base&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/t5_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSeq2SeqLM.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/t5_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),uw=new z({}),bw=new E({props:{name:"class transformers.TFAutoModelForSequenceClassification",anchor:"transformers.TFAutoModelForSequenceClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15792/src/transformers/models/auto/modeling_tf_auto.py#L430"}}),Tw=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15792/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15792/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15792/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/albert#transformers.TFAlbertForSequenceClassification">TFAlbertForSequenceClassification</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/bert#transformers.TFBertForSequenceClassification">TFBertForSequenceClassification</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/ctrl#transformers.CTRLConfig">CTRLConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/ctrl#transformers.TFCTRLForSequenceClassification">TFCTRLForSequenceClassification</a> (CTRL model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/camembert#transformers.TFCamembertForSequenceClassification">TFCamembertForSequenceClassification</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/convbert#transformers.TFConvBertForSequenceClassification">TFConvBertForSequenceClassification</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/deberta#transformers.TFDebertaForSequenceClassification">TFDebertaForSequenceClassification</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/deberta-v2#transformers.TFDebertaV2ForSequenceClassification">TFDebertaV2ForSequenceClassification</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/distilbert#transformers.TFDistilBertForSequenceClassification">TFDistilBertForSequenceClassification</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/electra#transformers.TFElectraForSequenceClassification">TFElectraForSequenceClassification</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/flaubert#transformers.TFFlaubertForSequenceClassification">TFFlaubertForSequenceClassification</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/funnel#transformers.TFFunnelForSequenceClassification">TFFunnelForSequenceClassification</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/gpt2#transformers.TFGPT2ForSequenceClassification">TFGPT2ForSequenceClassification</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/layoutlm#transformers.TFLayoutLMForSequenceClassification">TFLayoutLMForSequenceClassification</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/longformer#transformers.TFLongformerForSequenceClassification">TFLongformerForSequenceClassification</a> (Longformer model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/mpnet#transformers.TFMPNetForSequenceClassification">TFMPNetForSequenceClassification</a> (MPNet model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/mobilebert#transformers.TFMobileBertForSequenceClassification">TFMobileBertForSequenceClassification</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig">OpenAIGPTConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/openai-gpt#transformers.TFOpenAIGPTForSequenceClassification">TFOpenAIGPTForSequenceClassification</a> (OpenAI GPT model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/rembert#transformers.TFRemBertForSequenceClassification">TFRemBertForSequenceClassification</a> (RemBERT model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/roformer#transformers.TFRoFormerForSequenceClassification">TFRoFormerForSequenceClassification</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/roberta#transformers.TFRobertaForSequenceClassification">TFRobertaForSequenceClassification</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/tapas#transformers.TFTapasForSequenceClassification">TFTapasForSequenceClassification</a> (TAPAS model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/transfo-xl#transformers.TransfoXLConfig">TransfoXLConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/transfo-xl#transformers.TFTransfoXLForSequenceClassification">TFTransfoXLForSequenceClassification</a> (Transformer-XL model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/xlm#transformers.TFXLMForSequenceClassification">TFXLMForSequenceClassification</a> (XLM model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForSequenceClassification">TFXLMRobertaForSequenceClassification</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/xlnet#transformers.TFXLNetForSequenceClassification">TFXLNetForSequenceClassification</a> (XLNet model)</li>
</ul>`,name:"config"}]}}),Fw=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForSequenceClassification

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = TFAutoModelForSequenceClassification.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSequenceClassification.from_config(config)`}}),Cw=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15792/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15792/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15792/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15792/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15792/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),Mw=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForSequenceClassification

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForSequenceClassification.from_pretrained("bert-base-cased")

# Update configuration during loading
model = TFAutoModelForSequenceClassification.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = TFAutoModelForSequenceClassification.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSequenceClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),Ew=new z({}),yw=new E({props:{name:"class transformers.TFAutoModelForMultipleChoice",anchor:"transformers.TFAutoModelForMultipleChoice",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15792/src/transformers/models/auto/modeling_tf_auto.py#L466"}}),Aw=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15792/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15792/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15792/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/albert#transformers.TFAlbertForMultipleChoice">TFAlbertForMultipleChoice</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/bert#transformers.TFBertForMultipleChoice">TFBertForMultipleChoice</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/camembert#transformers.TFCamembertForMultipleChoice">TFCamembertForMultipleChoice</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/convbert#transformers.TFConvBertForMultipleChoice">TFConvBertForMultipleChoice</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/distilbert#transformers.TFDistilBertForMultipleChoice">TFDistilBertForMultipleChoice</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/electra#transformers.TFElectraForMultipleChoice">TFElectraForMultipleChoice</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/flaubert#transformers.TFFlaubertForMultipleChoice">TFFlaubertForMultipleChoice</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/funnel#transformers.TFFunnelForMultipleChoice">TFFunnelForMultipleChoice</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/longformer#transformers.TFLongformerForMultipleChoice">TFLongformerForMultipleChoice</a> (Longformer model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/mpnet#transformers.TFMPNetForMultipleChoice">TFMPNetForMultipleChoice</a> (MPNet model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/mobilebert#transformers.TFMobileBertForMultipleChoice">TFMobileBertForMultipleChoice</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/rembert#transformers.TFRemBertForMultipleChoice">TFRemBertForMultipleChoice</a> (RemBERT model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/roformer#transformers.TFRoFormerForMultipleChoice">TFRoFormerForMultipleChoice</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/roberta#transformers.TFRobertaForMultipleChoice">TFRobertaForMultipleChoice</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/xlm#transformers.TFXLMForMultipleChoice">TFXLMForMultipleChoice</a> (XLM model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForMultipleChoice">TFXLMRobertaForMultipleChoice</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/xlnet#transformers.TFXLNetForMultipleChoice">TFXLNetForMultipleChoice</a> (XLNet model)</li>
</ul>`,name:"config"}]}}),Lw=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForMultipleChoice

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = TFAutoModelForMultipleChoice.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForMultipleChoice

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForMultipleChoice.from_config(config)`}}),Bw=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15792/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15792/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15792/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15792/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15792/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),kw=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForMultipleChoice

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForMultipleChoice.from_pretrained("bert-base-cased")

# Update configuration during loading
model = TFAutoModelForMultipleChoice.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = TFAutoModelForMultipleChoice.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForMultipleChoice

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForMultipleChoice.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForMultipleChoice.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForMultipleChoice.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),xw=new z({}),Rw=new E({props:{name:"class transformers.TFAutoModelForTableQuestionAnswering",anchor:"transformers.TFAutoModelForTableQuestionAnswering",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15792/src/transformers/models/auto/modeling_tf_auto.py#L446"}}),Pw=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15792/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15792/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15792/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/tapas#transformers.TFTapasForQuestionAnswering">TFTapasForQuestionAnswering</a> (TAPAS model)</li>
</ul>`,name:"config"}]}}),$w=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForTableQuestionAnswering

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("google/tapas-base-finetuned-wtq")
model = TFAutoModelForTableQuestionAnswering.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForTableQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;google/tapas-base-finetuned-wtq&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForTableQuestionAnswering.from_config(config)`}}),Iw=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15792/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15792/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15792/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15792/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15792/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),jw=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForTableQuestionAnswering

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForTableQuestionAnswering.from_pretrained("google/tapas-base-finetuned-wtq")

# Update configuration during loading
model = TFAutoModelForTableQuestionAnswering.from_pretrained("google/tapas-base-finetuned-wtq", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/tapas_pt_model_config.json")
model = TFAutoModelForTableQuestionAnswering.from_pretrained(
    "./pt_model/tapas_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForTableQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForTableQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;google/tapas-base-finetuned-wtq&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForTableQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;google/tapas-base-finetuned-wtq&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/tapas_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForTableQuestionAnswering.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/tapas_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),Nw=new z({}),Dw=new E({props:{name:"class transformers.TFAutoModelForTokenClassification",anchor:"transformers.TFAutoModelForTokenClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15792/src/transformers/models/auto/modeling_tf_auto.py#L457"}}),Gw=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15792/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15792/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15792/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/albert#transformers.TFAlbertForTokenClassification">TFAlbertForTokenClassification</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/bert#transformers.TFBertForTokenClassification">TFBertForTokenClassification</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/camembert#transformers.TFCamembertForTokenClassification">TFCamembertForTokenClassification</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/convbert#transformers.TFConvBertForTokenClassification">TFConvBertForTokenClassification</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/deberta#transformers.TFDebertaForTokenClassification">TFDebertaForTokenClassification</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/deberta-v2#transformers.TFDebertaV2ForTokenClassification">TFDebertaV2ForTokenClassification</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/distilbert#transformers.TFDistilBertForTokenClassification">TFDistilBertForTokenClassification</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/electra#transformers.TFElectraForTokenClassification">TFElectraForTokenClassification</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/flaubert#transformers.TFFlaubertForTokenClassification">TFFlaubertForTokenClassification</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/funnel#transformers.TFFunnelForTokenClassification">TFFunnelForTokenClassification</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/layoutlm#transformers.TFLayoutLMForTokenClassification">TFLayoutLMForTokenClassification</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/longformer#transformers.TFLongformerForTokenClassification">TFLongformerForTokenClassification</a> (Longformer model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/mpnet#transformers.TFMPNetForTokenClassification">TFMPNetForTokenClassification</a> (MPNet model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/mobilebert#transformers.TFMobileBertForTokenClassification">TFMobileBertForTokenClassification</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/rembert#transformers.TFRemBertForTokenClassification">TFRemBertForTokenClassification</a> (RemBERT model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/roformer#transformers.TFRoFormerForTokenClassification">TFRoFormerForTokenClassification</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/roberta#transformers.TFRobertaForTokenClassification">TFRobertaForTokenClassification</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/xlm#transformers.TFXLMForTokenClassification">TFXLMForTokenClassification</a> (XLM model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForTokenClassification">TFXLMRobertaForTokenClassification</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/xlnet#transformers.TFXLNetForTokenClassification">TFXLNetForTokenClassification</a> (XLNet model)</li>
</ul>`,name:"config"}]}}),Ow=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForTokenClassification

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = TFAutoModelForTokenClassification.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForTokenClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForTokenClassification.from_config(config)`}}),Xw=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15792/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15792/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15792/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15792/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15792/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),zw=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForTokenClassification

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForTokenClassification.from_pretrained("bert-base-cased")

# Update configuration during loading
model = TFAutoModelForTokenClassification.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = TFAutoModelForTokenClassification.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForTokenClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForTokenClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForTokenClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForTokenClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),Vw=new z({}),Ww=new E({props:{name:"class transformers.TFAutoModelForQuestionAnswering",anchor:"transformers.TFAutoModelForQuestionAnswering",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15792/src/transformers/models/auto/modeling_tf_auto.py#L439"}}),Hw=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15792/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15792/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15792/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/albert#transformers.TFAlbertForQuestionAnswering">TFAlbertForQuestionAnswering</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/bert#transformers.TFBertForQuestionAnswering">TFBertForQuestionAnswering</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/camembert#transformers.TFCamembertForQuestionAnswering">TFCamembertForQuestionAnswering</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/convbert#transformers.TFConvBertForQuestionAnswering">TFConvBertForQuestionAnswering</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/deberta#transformers.TFDebertaForQuestionAnswering">TFDebertaForQuestionAnswering</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/deberta-v2#transformers.TFDebertaV2ForQuestionAnswering">TFDebertaV2ForQuestionAnswering</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/distilbert#transformers.TFDistilBertForQuestionAnswering">TFDistilBertForQuestionAnswering</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/electra#transformers.TFElectraForQuestionAnswering">TFElectraForQuestionAnswering</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/flaubert#transformers.TFFlaubertForQuestionAnsweringSimple">TFFlaubertForQuestionAnsweringSimple</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/funnel#transformers.TFFunnelForQuestionAnswering">TFFunnelForQuestionAnswering</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/longformer#transformers.TFLongformerForQuestionAnswering">TFLongformerForQuestionAnswering</a> (Longformer model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/mpnet#transformers.TFMPNetForQuestionAnswering">TFMPNetForQuestionAnswering</a> (MPNet model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/mobilebert#transformers.TFMobileBertForQuestionAnswering">TFMobileBertForQuestionAnswering</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/rembert#transformers.TFRemBertForQuestionAnswering">TFRemBertForQuestionAnswering</a> (RemBERT model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/roformer#transformers.TFRoFormerForQuestionAnswering">TFRoFormerForQuestionAnswering</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/roberta#transformers.TFRobertaForQuestionAnswering">TFRobertaForQuestionAnswering</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/xlm#transformers.TFXLMForQuestionAnsweringSimple">TFXLMForQuestionAnsweringSimple</a> (XLM model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForQuestionAnswering">TFXLMRobertaForQuestionAnswering</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/xlnet#transformers.TFXLNetForQuestionAnsweringSimple">TFXLNetForQuestionAnsweringSimple</a> (XLNet model)</li>
</ul>`,name:"config"}]}}),Uw=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForQuestionAnswering

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = TFAutoModelForQuestionAnswering.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForQuestionAnswering.from_config(config)`}}),Jw=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15792/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15792/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15792/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15792/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15792/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),Yw=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForQuestionAnswering

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForQuestionAnswering.from_pretrained("bert-base-cased")

# Update configuration during loading
model = TFAutoModelForQuestionAnswering.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = TFAutoModelForQuestionAnswering.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForQuestionAnswering.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),Kw=new z({}),Zw=new E({props:{name:"class transformers.TFAutoModelForVision2Seq",anchor:"transformers.TFAutoModelForVision2Seq",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15792/src/transformers/models/auto/modeling_tf_auto.py#L407"}}),oA=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15792/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15792/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15792/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderConfig">VisionEncoderDecoderConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/vision-encoder-decoder#transformers.TFVisionEncoderDecoderModel">TFVisionEncoderDecoderModel</a> (Vision Encoder decoder model)</li>
</ul>`,name:"config"}]}}),rA=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForVision2Seq

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = TFAutoModelForVision2Seq.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForVision2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForVision2Seq.from_config(config)`}}),tA=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15792/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15792/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15792/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15792/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15792/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),aA=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForVision2Seq

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForVision2Seq.from_pretrained("bert-base-cased")

# Update configuration during loading
model = TFAutoModelForVision2Seq.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = TFAutoModelForVision2Seq.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForVision2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForVision2Seq.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForVision2Seq.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForVision2Seq.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),nA=new z({}),sA=new E({props:{name:"class transformers.TFAutoModelForSpeechSeq2Seq",anchor:"transformers.TFAutoModelForSpeechSeq2Seq",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15792/src/transformers/models/auto/modeling_tf_auto.py#L482"}}),iA=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15792/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15792/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15792/en/model_doc/speech_to_text#transformers.Speech2TextConfig">Speech2TextConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/speech_to_text#transformers.TFSpeech2TextForConditionalGeneration">TFSpeech2TextForConditionalGeneration</a> (Speech2Text model)</li>
</ul>`,name:"config"}]}}),dA=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForSpeechSeq2Seq

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = TFAutoModelForSpeechSeq2Seq.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForSpeechSeq2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSpeechSeq2Seq.from_config(config)`}}),cA=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15792/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15792/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15792/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15792/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15792/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),fA=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForSpeechSeq2Seq

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForSpeechSeq2Seq.from_pretrained("bert-base-cased")

# Update configuration during loading
model = TFAutoModelForSpeechSeq2Seq.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = TFAutoModelForSpeechSeq2Seq.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForSpeechSeq2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSpeechSeq2Seq.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSpeechSeq2Seq.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSpeechSeq2Seq.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),mA=new z({}),gA=new E({props:{name:"class transformers.FlaxAutoModel",anchor:"transformers.FlaxAutoModel",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15792/src/transformers/models/auto/modeling_flax_auto.py#L220"}}),pA=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15792/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15792/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15792/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/albert#transformers.FlaxAlbertModel">FlaxAlbertModel</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/bart#transformers.FlaxBartModel">FlaxBartModel</a> (BART model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/beit#transformers.BeitConfig">BeitConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/beit#transformers.FlaxBeitModel">FlaxBeitModel</a> (BEiT model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/bert#transformers.FlaxBertModel">FlaxBertModel</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/big_bird#transformers.FlaxBigBirdModel">FlaxBigBirdModel</a> (BigBird model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/blenderbot#transformers.BlenderbotConfig">BlenderbotConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/blenderbot#transformers.FlaxBlenderbotModel">FlaxBlenderbotModel</a> (Blenderbot model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/blenderbot-small#transformers.BlenderbotSmallConfig">BlenderbotSmallConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/blenderbot-small#transformers.FlaxBlenderbotSmallModel">FlaxBlenderbotSmallModel</a> (BlenderbotSmall model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/clip#transformers.CLIPConfig">CLIPConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/clip#transformers.FlaxCLIPModel">FlaxCLIPModel</a> (CLIP model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/distilbert#transformers.FlaxDistilBertModel">FlaxDistilBertModel</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/electra#transformers.FlaxElectraModel">FlaxElectraModel</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/gpt2#transformers.FlaxGPT2Model">FlaxGPT2Model</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/gptj#transformers.GPTJConfig">GPTJConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/gptj#transformers.FlaxGPTJModel">FlaxGPTJModel</a> (GPT-J model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/gpt_neo#transformers.GPTNeoConfig">GPTNeoConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/gpt_neo#transformers.FlaxGPTNeoModel">FlaxGPTNeoModel</a> (GPT Neo model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/mbart#transformers.FlaxMBartModel">FlaxMBartModel</a> (mBART model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/mt5#transformers.MT5Config">MT5Config</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/mt5#transformers.FlaxMT5Model">FlaxMT5Model</a> (mT5 model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/marian#transformers.MarianConfig">MarianConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/marian#transformers.FlaxMarianModel">FlaxMarianModel</a> (Marian model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/pegasus#transformers.PegasusConfig">PegasusConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/pegasus#transformers.FlaxPegasusModel">FlaxPegasusModel</a> (Pegasus model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/roformer#transformers.FlaxRoFormerModel">FlaxRoFormerModel</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/roberta#transformers.FlaxRobertaModel">FlaxRobertaModel</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/t5#transformers.T5Config">T5Config</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/t5#transformers.FlaxT5Model">FlaxT5Model</a> (T5 model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/vit#transformers.ViTConfig">ViTConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/vit#transformers.FlaxViTModel">FlaxViTModel</a> (ViT model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/vision-text-dual-encoder#transformers.VisionTextDualEncoderConfig">VisionTextDualEncoderConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/vision-text-dual-encoder#transformers.FlaxVisionTextDualEncoderModel">FlaxVisionTextDualEncoderModel</a> (VisionTextDualEncoder model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/wav2vec2#transformers.FlaxWav2Vec2Model">FlaxWav2Vec2Model</a> (Wav2Vec2 model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/xglm#transformers.XGLMConfig">XGLMConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/xglm#transformers.FlaxXGLMModel">FlaxXGLMModel</a> (XGLM model)</li>
</ul>`,name:"config"}]}}),_A=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModel

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = FlaxAutoModel.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModel.from_config(config)`}}),uA=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15792/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15792/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15792/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15792/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15792/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),bA=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModel

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModel.from_pretrained("bert-base-cased")

# Update configuration during loading
model = FlaxAutoModel.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = FlaxAutoModel.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModel.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModel.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModel.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),vA=new z({}),TA=new E({props:{name:"class transformers.FlaxAutoModelForCausalLM",anchor:"transformers.FlaxAutoModelForCausalLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15792/src/transformers/models/auto/modeling_flax_auto.py#L234"}}),CA=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15792/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15792/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15792/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/gpt2#transformers.FlaxGPT2LMHeadModel">FlaxGPT2LMHeadModel</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/gptj#transformers.GPTJConfig">GPTJConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/gptj#transformers.FlaxGPTJForCausalLM">FlaxGPTJForCausalLM</a> (GPT-J model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/gpt_neo#transformers.GPTNeoConfig">GPTNeoConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/gpt_neo#transformers.FlaxGPTNeoForCausalLM">FlaxGPTNeoForCausalLM</a> (GPT Neo model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/xglm#transformers.XGLMConfig">XGLMConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/xglm#transformers.FlaxXGLMForCausalLM">FlaxXGLMForCausalLM</a> (XGLM model)</li>
</ul>`,name:"config"}]}}),MA=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForCausalLM

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = FlaxAutoModelForCausalLM.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForCausalLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForCausalLM.from_config(config)`}}),EA=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15792/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15792/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15792/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15792/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15792/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),yA=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForCausalLM

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForCausalLM.from_pretrained("bert-base-cased")

# Update configuration during loading
model = FlaxAutoModelForCausalLM.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = FlaxAutoModelForCausalLM.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForCausalLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForCausalLM.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),wA=new z({}),AA=new E({props:{name:"class transformers.FlaxAutoModelForPreTraining",anchor:"transformers.FlaxAutoModelForPreTraining",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15792/src/transformers/models/auto/modeling_flax_auto.py#L227"}}),BA=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15792/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15792/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15792/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/albert#transformers.FlaxAlbertForPreTraining">FlaxAlbertForPreTraining</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/bart#transformers.FlaxBartForConditionalGeneration">FlaxBartForConditionalGeneration</a> (BART model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/bert#transformers.FlaxBertForPreTraining">FlaxBertForPreTraining</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/big_bird#transformers.FlaxBigBirdForPreTraining">FlaxBigBirdForPreTraining</a> (BigBird model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/electra#transformers.FlaxElectraForPreTraining">FlaxElectraForPreTraining</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/mbart#transformers.FlaxMBartForConditionalGeneration">FlaxMBartForConditionalGeneration</a> (mBART model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/mt5#transformers.MT5Config">MT5Config</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/mt5#transformers.FlaxMT5ForConditionalGeneration">FlaxMT5ForConditionalGeneration</a> (mT5 model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/roformer#transformers.FlaxRoFormerForMaskedLM">FlaxRoFormerForMaskedLM</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/roberta#transformers.FlaxRobertaForMaskedLM">FlaxRobertaForMaskedLM</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/t5#transformers.T5Config">T5Config</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/t5#transformers.FlaxT5ForConditionalGeneration">FlaxT5ForConditionalGeneration</a> (T5 model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/wav2vec2#transformers.FlaxWav2Vec2ForPreTraining">FlaxWav2Vec2ForPreTraining</a> (Wav2Vec2 model)</li>
</ul>`,name:"config"}]}}),kA=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForPreTraining

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = FlaxAutoModelForPreTraining.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForPreTraining

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForPreTraining.from_config(config)`}}),xA=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15792/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15792/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15792/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15792/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15792/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),RA=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForPreTraining

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForPreTraining.from_pretrained("bert-base-cased")

# Update configuration during loading
model = FlaxAutoModelForPreTraining.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = FlaxAutoModelForPreTraining.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForPreTraining

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForPreTraining.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForPreTraining.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForPreTraining.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),SA=new z({}),PA=new E({props:{name:"class transformers.FlaxAutoModelForMaskedLM",anchor:"transformers.FlaxAutoModelForMaskedLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15792/src/transformers/models/auto/modeling_flax_auto.py#L241"}}),IA=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15792/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15792/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15792/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/albert#transformers.FlaxAlbertForMaskedLM">FlaxAlbertForMaskedLM</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/bart#transformers.FlaxBartForConditionalGeneration">FlaxBartForConditionalGeneration</a> (BART model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/bert#transformers.FlaxBertForMaskedLM">FlaxBertForMaskedLM</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/big_bird#transformers.FlaxBigBirdForMaskedLM">FlaxBigBirdForMaskedLM</a> (BigBird model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/distilbert#transformers.FlaxDistilBertForMaskedLM">FlaxDistilBertForMaskedLM</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/electra#transformers.FlaxElectraForMaskedLM">FlaxElectraForMaskedLM</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/mbart#transformers.FlaxMBartForConditionalGeneration">FlaxMBartForConditionalGeneration</a> (mBART model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/roformer#transformers.FlaxRoFormerForMaskedLM">FlaxRoFormerForMaskedLM</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/roberta#transformers.FlaxRobertaForMaskedLM">FlaxRobertaForMaskedLM</a> (RoBERTa model)</li>
</ul>`,name:"config"}]}}),jA=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForMaskedLM

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = FlaxAutoModelForMaskedLM.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForMaskedLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForMaskedLM.from_config(config)`}}),NA=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15792/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15792/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15792/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15792/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15792/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),DA=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForMaskedLM

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForMaskedLM.from_pretrained("bert-base-cased")

# Update configuration during loading
model = FlaxAutoModelForMaskedLM.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = FlaxAutoModelForMaskedLM.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForMaskedLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForMaskedLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForMaskedLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForMaskedLM.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),qA=new z({}),GA=new E({props:{name:"class transformers.FlaxAutoModelForSeq2SeqLM",anchor:"transformers.FlaxAutoModelForSeq2SeqLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15792/src/transformers/models/auto/modeling_flax_auto.py#L248"}}),XA=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15792/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15792/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15792/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/bart#transformers.FlaxBartForConditionalGeneration">FlaxBartForConditionalGeneration</a> (BART model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/blenderbot#transformers.BlenderbotConfig">BlenderbotConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/blenderbot#transformers.FlaxBlenderbotForConditionalGeneration">FlaxBlenderbotForConditionalGeneration</a> (Blenderbot model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/blenderbot-small#transformers.BlenderbotSmallConfig">BlenderbotSmallConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/blenderbot-small#transformers.FlaxBlenderbotSmallForConditionalGeneration">FlaxBlenderbotSmallForConditionalGeneration</a> (BlenderbotSmall model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/encoder-decoder#transformers.EncoderDecoderConfig">EncoderDecoderConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/encoder-decoder#transformers.FlaxEncoderDecoderModel">FlaxEncoderDecoderModel</a> (Encoder decoder model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/mbart#transformers.FlaxMBartForConditionalGeneration">FlaxMBartForConditionalGeneration</a> (mBART model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/mt5#transformers.MT5Config">MT5Config</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/mt5#transformers.FlaxMT5ForConditionalGeneration">FlaxMT5ForConditionalGeneration</a> (mT5 model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/marian#transformers.MarianConfig">MarianConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/marian#transformers.FlaxMarianMTModel">FlaxMarianMTModel</a> (Marian model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/pegasus#transformers.PegasusConfig">PegasusConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/pegasus#transformers.FlaxPegasusForConditionalGeneration">FlaxPegasusForConditionalGeneration</a> (Pegasus model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/t5#transformers.T5Config">T5Config</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/t5#transformers.FlaxT5ForConditionalGeneration">FlaxT5ForConditionalGeneration</a> (T5 model)</li>
</ul>`,name:"config"}]}}),zA=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForSeq2SeqLM

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("t5-base")
model = FlaxAutoModelForSeq2SeqLM.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForSeq2SeqLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;t5-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForSeq2SeqLM.from_config(config)`}}),VA=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15792/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15792/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15792/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15792/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15792/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),WA=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForSeq2SeqLM

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForSeq2SeqLM.from_pretrained("t5-base")

# Update configuration during loading
model = FlaxAutoModelForSeq2SeqLM.from_pretrained("t5-base", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/t5_pt_model_config.json")
model = FlaxAutoModelForSeq2SeqLM.from_pretrained(
    "./pt_model/t5_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForSeq2SeqLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForSeq2SeqLM.from_pretrained(<span class="hljs-string">&quot;t5-base&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForSeq2SeqLM.from_pretrained(<span class="hljs-string">&quot;t5-base&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/t5_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForSeq2SeqLM.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/t5_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),QA=new z({}),HA=new E({props:{name:"class transformers.FlaxAutoModelForSequenceClassification",anchor:"transformers.FlaxAutoModelForSequenceClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15792/src/transformers/models/auto/modeling_flax_auto.py#L257"}}),JA=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15792/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15792/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15792/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/albert#transformers.FlaxAlbertForSequenceClassification">FlaxAlbertForSequenceClassification</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/bart#transformers.FlaxBartForSequenceClassification">FlaxBartForSequenceClassification</a> (BART model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/bert#transformers.FlaxBertForSequenceClassification">FlaxBertForSequenceClassification</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/big_bird#transformers.FlaxBigBirdForSequenceClassification">FlaxBigBirdForSequenceClassification</a> (BigBird model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/distilbert#transformers.FlaxDistilBertForSequenceClassification">FlaxDistilBertForSequenceClassification</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/electra#transformers.FlaxElectraForSequenceClassification">FlaxElectraForSequenceClassification</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/mbart#transformers.FlaxMBartForSequenceClassification">FlaxMBartForSequenceClassification</a> (mBART model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/roformer#transformers.FlaxRoFormerForSequenceClassification">FlaxRoFormerForSequenceClassification</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/roberta#transformers.FlaxRobertaForSequenceClassification">FlaxRobertaForSequenceClassification</a> (RoBERTa model)</li>
</ul>`,name:"config"}]}}),YA=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForSequenceClassification

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = FlaxAutoModelForSequenceClassification.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForSequenceClassification.from_config(config)`}}),KA=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15792/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15792/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15792/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15792/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15792/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),ZA=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForSequenceClassification

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForSequenceClassification.from_pretrained("bert-base-cased")

# Update configuration during loading
model = FlaxAutoModelForSequenceClassification.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = FlaxAutoModelForSequenceClassification.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForSequenceClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),e6=new z({}),o6=new E({props:{name:"class transformers.FlaxAutoModelForQuestionAnswering",anchor:"transformers.FlaxAutoModelForQuestionAnswering",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15792/src/transformers/models/auto/modeling_flax_auto.py#L266"}}),t6=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15792/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15792/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15792/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/albert#transformers.FlaxAlbertForQuestionAnswering">FlaxAlbertForQuestionAnswering</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/bart#transformers.FlaxBartForQuestionAnswering">FlaxBartForQuestionAnswering</a> (BART model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/bert#transformers.FlaxBertForQuestionAnswering">FlaxBertForQuestionAnswering</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/big_bird#transformers.FlaxBigBirdForQuestionAnswering">FlaxBigBirdForQuestionAnswering</a> (BigBird model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/distilbert#transformers.FlaxDistilBertForQuestionAnswering">FlaxDistilBertForQuestionAnswering</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/electra#transformers.FlaxElectraForQuestionAnswering">FlaxElectraForQuestionAnswering</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/mbart#transformers.FlaxMBartForQuestionAnswering">FlaxMBartForQuestionAnswering</a> (mBART model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/roformer#transformers.FlaxRoFormerForQuestionAnswering">FlaxRoFormerForQuestionAnswering</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/roberta#transformers.FlaxRobertaForQuestionAnswering">FlaxRobertaForQuestionAnswering</a> (RoBERTa model)</li>
</ul>`,name:"config"}]}}),a6=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForQuestionAnswering

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = FlaxAutoModelForQuestionAnswering.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForQuestionAnswering.from_config(config)`}}),n6=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15792/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15792/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15792/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15792/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15792/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),s6=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForQuestionAnswering

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForQuestionAnswering.from_pretrained("bert-base-cased")

# Update configuration during loading
model = FlaxAutoModelForQuestionAnswering.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = FlaxAutoModelForQuestionAnswering.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForQuestionAnswering.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),l6=new z({}),i6=new E({props:{name:"class transformers.FlaxAutoModelForTokenClassification",anchor:"transformers.FlaxAutoModelForTokenClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15792/src/transformers/models/auto/modeling_flax_auto.py#L273"}}),c6=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15792/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15792/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15792/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/albert#transformers.FlaxAlbertForTokenClassification">FlaxAlbertForTokenClassification</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/bert#transformers.FlaxBertForTokenClassification">FlaxBertForTokenClassification</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/big_bird#transformers.FlaxBigBirdForTokenClassification">FlaxBigBirdForTokenClassification</a> (BigBird model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/distilbert#transformers.FlaxDistilBertForTokenClassification">FlaxDistilBertForTokenClassification</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/electra#transformers.FlaxElectraForTokenClassification">FlaxElectraForTokenClassification</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/roformer#transformers.FlaxRoFormerForTokenClassification">FlaxRoFormerForTokenClassification</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/roberta#transformers.FlaxRobertaForTokenClassification">FlaxRobertaForTokenClassification</a> (RoBERTa model)</li>
</ul>`,name:"config"}]}}),f6=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForTokenClassification

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = FlaxAutoModelForTokenClassification.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForTokenClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForTokenClassification.from_config(config)`}}),m6=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15792/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15792/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15792/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15792/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15792/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),g6=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForTokenClassification

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForTokenClassification.from_pretrained("bert-base-cased")

# Update configuration during loading
model = FlaxAutoModelForTokenClassification.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = FlaxAutoModelForTokenClassification.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForTokenClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForTokenClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForTokenClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForTokenClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),h6=new z({}),p6=new E({props:{name:"class transformers.FlaxAutoModelForMultipleChoice",anchor:"transformers.FlaxAutoModelForMultipleChoice",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15792/src/transformers/models/auto/modeling_flax_auto.py#L282"}}),u6=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15792/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15792/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15792/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/albert#transformers.FlaxAlbertForMultipleChoice">FlaxAlbertForMultipleChoice</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/bert#transformers.FlaxBertForMultipleChoice">FlaxBertForMultipleChoice</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/big_bird#transformers.FlaxBigBirdForMultipleChoice">FlaxBigBirdForMultipleChoice</a> (BigBird model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/distilbert#transformers.FlaxDistilBertForMultipleChoice">FlaxDistilBertForMultipleChoice</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/electra#transformers.FlaxElectraForMultipleChoice">FlaxElectraForMultipleChoice</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/roformer#transformers.FlaxRoFormerForMultipleChoice">FlaxRoFormerForMultipleChoice</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/roberta#transformers.FlaxRobertaForMultipleChoice">FlaxRobertaForMultipleChoice</a> (RoBERTa model)</li>
</ul>`,name:"config"}]}}),b6=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForMultipleChoice

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = FlaxAutoModelForMultipleChoice.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForMultipleChoice

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForMultipleChoice.from_config(config)`}}),v6=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15792/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15792/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15792/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15792/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15792/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),T6=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForMultipleChoice

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForMultipleChoice.from_pretrained("bert-base-cased")

# Update configuration during loading
model = FlaxAutoModelForMultipleChoice.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = FlaxAutoModelForMultipleChoice.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForMultipleChoice

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForMultipleChoice.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForMultipleChoice.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForMultipleChoice.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),F6=new z({}),C6=new E({props:{name:"class transformers.FlaxAutoModelForNextSentencePrediction",anchor:"transformers.FlaxAutoModelForNextSentencePrediction",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15792/src/transformers/models/auto/modeling_flax_auto.py#L289"}}),E6=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15792/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15792/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15792/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/bert#transformers.FlaxBertForNextSentencePrediction">FlaxBertForNextSentencePrediction</a> (BERT model)</li>
</ul>`,name:"config"}]}}),y6=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForNextSentencePrediction

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = FlaxAutoModelForNextSentencePrediction.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForNextSentencePrediction

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForNextSentencePrediction.from_config(config)`}}),w6=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15792/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15792/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15792/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15792/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15792/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),A6=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForNextSentencePrediction

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForNextSentencePrediction.from_pretrained("bert-base-cased")

# Update configuration during loading
model = FlaxAutoModelForNextSentencePrediction.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = FlaxAutoModelForNextSentencePrediction.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForNextSentencePrediction

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForNextSentencePrediction.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForNextSentencePrediction.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForNextSentencePrediction.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),L6=new z({}),B6=new E({props:{name:"class transformers.FlaxAutoModelForImageClassification",anchor:"transformers.FlaxAutoModelForImageClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15792/src/transformers/models/auto/modeling_flax_auto.py#L298"}}),x6=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15792/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15792/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15792/en/model_doc/beit#transformers.BeitConfig">BeitConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/beit#transformers.FlaxBeitForImageClassification">FlaxBeitForImageClassification</a> (BEiT model)</li>
<li><a href="/docs/transformers/pr_15792/en/model_doc/vit#transformers.ViTConfig">ViTConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/vit#transformers.FlaxViTForImageClassification">FlaxViTForImageClassification</a> (ViT model)</li>
</ul>`,name:"config"}]}}),R6=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForImageClassification

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = FlaxAutoModelForImageClassification.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForImageClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForImageClassification.from_config(config)`}}),S6=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15792/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15792/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15792/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15792/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15792/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),$6=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForImageClassification

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForImageClassification.from_pretrained("bert-base-cased")

# Update configuration during loading
model = FlaxAutoModelForImageClassification.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = FlaxAutoModelForImageClassification.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForImageClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForImageClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForImageClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForImageClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),I6=new z({}),j6=new E({props:{name:"class transformers.FlaxAutoModelForVision2Seq",anchor:"transformers.FlaxAutoModelForVision2Seq",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15792/src/transformers/models/auto/modeling_flax_auto.py#L307"}}),D6=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15792/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15792/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15792/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderConfig">VisionEncoderDecoderConfig</a> configuration class: <a href="/docs/transformers/pr_15792/en/model_doc/vision-encoder-decoder#transformers.FlaxVisionEncoderDecoderModel">FlaxVisionEncoderDecoderModel</a> (Vision Encoder decoder model)</li>
</ul>`,name:"config"}]}}),q6=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForVision2Seq

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = FlaxAutoModelForVision2Seq.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForVision2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForVision2Seq.from_config(config)`}}),G6=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15792/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15792/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15792/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15792/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15792/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),O6=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForVision2Seq

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForVision2Seq.from_pretrained("bert-base-cased")

# Update configuration during loading
model = FlaxAutoModelForVision2Seq.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = FlaxAutoModelForVision2Seq.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForVision2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForVision2Seq.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForVision2Seq.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForVision2Seq.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),{c(){J=a("meta"),Ae=l(),ie=a("h1"),me=a("a"),to=a("span"),f(ce.$$.fragment),ue=l(),Do=a("span"),wi=o("Auto Classes"),Ef=l(),sa=a("p"),Ai=o(`In many cases, the architecture you want to use can be guessed from the name or the path of the pretrained model you
are supplying to the `),Li=a("code"),oM=o("from_pretrained()"),yf=o(` method. AutoClasses are here to do this job for you so that you
automatically retrieve the relevant model given the name/path to the pretrained weights/config/vocabulary.`),ye=l(),io=a("p"),Bi=o("Instantiating one of "),Pn=a("a"),rM=o("AutoConfig"),$n=o(", "),In=a("a"),tM=o("AutoModel"),ki=o(`, and
`),jn=a("a"),aM=o("AutoTokenizer"),xi=o(" will directly create a class of the relevant architecture. For instance"),wf=l(),f($a.$$.fragment),co=l(),ge=a("p"),D0=o("will create a model that is an instance of "),Ri=a("a"),q0=o("BertModel"),G0=o("."),qo=l(),Ia=a("p"),O0=o("There is one class of "),Af=a("code"),X0=o("AutoModel"),mxe=o(" for each task, and for each backend (PyTorch, TensorFlow, or Flax)."),tLe=l(),Si=a("h2"),Lf=a("a"),$V=a("span"),f(nM.$$.fragment),gxe=l(),IV=a("span"),hxe=o("Extending the Auto Classes"),aLe=l(),Nn=a("p"),pxe=o(`Each of the auto classes has a method to be extended with your custom classes. For instance, if you have defined a
custom class of model `),jV=a("code"),_xe=o("NewModel"),uxe=o(", make sure you have a "),NV=a("code"),bxe=o("NewModelConfig"),vxe=o(` then you can add those to the auto
classes like this:`),nLe=l(),f(sM.$$.fragment),sLe=l(),z0=a("p"),Txe=o("You will then be able to use the auto classes like you would usually do!"),lLe=l(),f(Bf.$$.fragment),iLe=l(),Pi=a("h2"),kf=a("a"),DV=a("span"),f(lM.$$.fragment),Fxe=l(),qV=a("span"),Cxe=o("AutoConfig"),dLe=l(),Go=a("div"),f(iM.$$.fragment),Mxe=l(),dM=a("p"),Exe=o(`This is a generic configuration class that will be instantiated as one of the configuration classes of the library
when created with the `),V0=a("a"),yxe=o("from_pretrained()"),wxe=o(" class method."),Axe=l(),cM=a("p"),Lxe=o("This class cannot be instantiated directly using "),GV=a("code"),Bxe=o("__init__()"),kxe=o(" (throws an error)."),xxe=l(),fo=a("div"),f(fM.$$.fragment),Rxe=l(),OV=a("p"),Sxe=o("Instantiate one of the configuration classes of the library from a pretrained model configuration."),Pxe=l(),$i=a("p"),$xe=o("The configuration class to instantiate is selected based on the "),XV=a("code"),Ixe=o("model_type"),jxe=o(` property of the config object that
is loaded, or when it\u2019s missing, by falling back to using pattern matching on `),zV=a("code"),Nxe=o("pretrained_model_name_or_path"),Dxe=o(":"),qxe=l(),v=a("ul"),xf=a("li"),VV=a("strong"),Gxe=o("albert"),Oxe=o(" \u2014 "),W0=a("a"),Xxe=o("AlbertConfig"),zxe=o(" (ALBERT model)"),Vxe=l(),Rf=a("li"),WV=a("strong"),Wxe=o("bart"),Qxe=o(" \u2014 "),Q0=a("a"),Hxe=o("BartConfig"),Uxe=o(" (BART model)"),Jxe=l(),Sf=a("li"),QV=a("strong"),Yxe=o("beit"),Kxe=o(" \u2014 "),H0=a("a"),Zxe=o("BeitConfig"),eRe=o(" (BEiT model)"),oRe=l(),Pf=a("li"),HV=a("strong"),rRe=o("bert"),tRe=o(" \u2014 "),U0=a("a"),aRe=o("BertConfig"),nRe=o(" (BERT model)"),sRe=l(),$f=a("li"),UV=a("strong"),lRe=o("bert-generation"),iRe=o(" \u2014 "),J0=a("a"),dRe=o("BertGenerationConfig"),cRe=o(" (Bert Generation model)"),fRe=l(),If=a("li"),JV=a("strong"),mRe=o("big_bird"),gRe=o(" \u2014 "),Y0=a("a"),hRe=o("BigBirdConfig"),pRe=o(" (BigBird model)"),_Re=l(),jf=a("li"),YV=a("strong"),uRe=o("bigbird_pegasus"),bRe=o(" \u2014 "),K0=a("a"),vRe=o("BigBirdPegasusConfig"),TRe=o(" (BigBirdPegasus model)"),FRe=l(),Nf=a("li"),KV=a("strong"),CRe=o("blenderbot"),MRe=o(" \u2014 "),Z0=a("a"),ERe=o("BlenderbotConfig"),yRe=o(" (Blenderbot model)"),wRe=l(),Df=a("li"),ZV=a("strong"),ARe=o("blenderbot-small"),LRe=o(" \u2014 "),eL=a("a"),BRe=o("BlenderbotSmallConfig"),kRe=o(" (BlenderbotSmall model)"),xRe=l(),qf=a("li"),eW=a("strong"),RRe=o("camembert"),SRe=o(" \u2014 "),oL=a("a"),PRe=o("CamembertConfig"),$Re=o(" (CamemBERT model)"),IRe=l(),Gf=a("li"),oW=a("strong"),jRe=o("canine"),NRe=o(" \u2014 "),rL=a("a"),DRe=o("CanineConfig"),qRe=o(" (Canine model)"),GRe=l(),Of=a("li"),rW=a("strong"),ORe=o("clip"),XRe=o(" \u2014 "),tL=a("a"),zRe=o("CLIPConfig"),VRe=o(" (CLIP model)"),WRe=l(),Xf=a("li"),tW=a("strong"),QRe=o("convbert"),HRe=o(" \u2014 "),aL=a("a"),URe=o("ConvBertConfig"),JRe=o(" (ConvBERT model)"),YRe=l(),zf=a("li"),aW=a("strong"),KRe=o("convnext"),ZRe=o(" \u2014 "),nL=a("a"),eSe=o("ConvNextConfig"),oSe=o(" (ConvNext model)"),rSe=l(),Vf=a("li"),nW=a("strong"),tSe=o("ctrl"),aSe=o(" \u2014 "),sL=a("a"),nSe=o("CTRLConfig"),sSe=o(" (CTRL model)"),lSe=l(),Wf=a("li"),sW=a("strong"),iSe=o("deberta"),dSe=o(" \u2014 "),lL=a("a"),cSe=o("DebertaConfig"),fSe=o(" (DeBERTa model)"),mSe=l(),Qf=a("li"),lW=a("strong"),gSe=o("deberta-v2"),hSe=o(" \u2014 "),iL=a("a"),pSe=o("DebertaV2Config"),_Se=o(" (DeBERTa-v2 model)"),uSe=l(),Hf=a("li"),iW=a("strong"),bSe=o("deit"),vSe=o(" \u2014 "),dL=a("a"),TSe=o("DeiTConfig"),FSe=o(" (DeiT model)"),CSe=l(),Uf=a("li"),dW=a("strong"),MSe=o("detr"),ESe=o(" \u2014 "),cL=a("a"),ySe=o("DetrConfig"),wSe=o(" (DETR model)"),ASe=l(),Jf=a("li"),cW=a("strong"),LSe=o("distilbert"),BSe=o(" \u2014 "),fL=a("a"),kSe=o("DistilBertConfig"),xSe=o(" (DistilBERT model)"),RSe=l(),Yf=a("li"),fW=a("strong"),SSe=o("dpr"),PSe=o(" \u2014 "),mL=a("a"),$Se=o("DPRConfig"),ISe=o(" (DPR model)"),jSe=l(),Kf=a("li"),mW=a("strong"),NSe=o("electra"),DSe=o(" \u2014 "),gL=a("a"),qSe=o("ElectraConfig"),GSe=o(" (ELECTRA model)"),OSe=l(),Zf=a("li"),gW=a("strong"),XSe=o("encoder-decoder"),zSe=o(" \u2014 "),hL=a("a"),VSe=o("EncoderDecoderConfig"),WSe=o(" (Encoder decoder model)"),QSe=l(),em=a("li"),hW=a("strong"),HSe=o("flaubert"),USe=o(" \u2014 "),pL=a("a"),JSe=o("FlaubertConfig"),YSe=o(" (FlauBERT model)"),KSe=l(),om=a("li"),pW=a("strong"),ZSe=o("fnet"),ePe=o(" \u2014 "),_L=a("a"),oPe=o("FNetConfig"),rPe=o(" (FNet model)"),tPe=l(),rm=a("li"),_W=a("strong"),aPe=o("fsmt"),nPe=o(" \u2014 "),uL=a("a"),sPe=o("FSMTConfig"),lPe=o(" (FairSeq Machine-Translation model)"),iPe=l(),tm=a("li"),uW=a("strong"),dPe=o("funnel"),cPe=o(" \u2014 "),bL=a("a"),fPe=o("FunnelConfig"),mPe=o(" (Funnel Transformer model)"),gPe=l(),am=a("li"),bW=a("strong"),hPe=o("gpt2"),pPe=o(" \u2014 "),vL=a("a"),_Pe=o("GPT2Config"),uPe=o(" (OpenAI GPT-2 model)"),bPe=l(),nm=a("li"),vW=a("strong"),vPe=o("gpt_neo"),TPe=o(" \u2014 "),TL=a("a"),FPe=o("GPTNeoConfig"),CPe=o(" (GPT Neo model)"),MPe=l(),sm=a("li"),TW=a("strong"),EPe=o("gptj"),yPe=o(" \u2014 "),FL=a("a"),wPe=o("GPTJConfig"),APe=o(" (GPT-J model)"),LPe=l(),lm=a("li"),FW=a("strong"),BPe=o("hubert"),kPe=o(" \u2014 "),CL=a("a"),xPe=o("HubertConfig"),RPe=o(" (Hubert model)"),SPe=l(),im=a("li"),CW=a("strong"),PPe=o("ibert"),$Pe=o(" \u2014 "),ML=a("a"),IPe=o("IBertConfig"),jPe=o(" (I-BERT model)"),NPe=l(),dm=a("li"),MW=a("strong"),DPe=o("imagegpt"),qPe=o(" \u2014 "),EL=a("a"),GPe=o("ImageGPTConfig"),OPe=o(" (ImageGPT model)"),XPe=l(),cm=a("li"),EW=a("strong"),zPe=o("layoutlm"),VPe=o(" \u2014 "),yL=a("a"),WPe=o("LayoutLMConfig"),QPe=o(" (LayoutLM model)"),HPe=l(),fm=a("li"),yW=a("strong"),UPe=o("layoutlmv2"),JPe=o(" \u2014 "),wL=a("a"),YPe=o("LayoutLMv2Config"),KPe=o(" (LayoutLMv2 model)"),ZPe=l(),mm=a("li"),wW=a("strong"),e$e=o("led"),o$e=o(" \u2014 "),AL=a("a"),r$e=o("LEDConfig"),t$e=o(" (LED model)"),a$e=l(),gm=a("li"),AW=a("strong"),n$e=o("longformer"),s$e=o(" \u2014 "),LL=a("a"),l$e=o("LongformerConfig"),i$e=o(" (Longformer model)"),d$e=l(),hm=a("li"),LW=a("strong"),c$e=o("luke"),f$e=o(" \u2014 "),BL=a("a"),m$e=o("LukeConfig"),g$e=o(" (LUKE model)"),h$e=l(),pm=a("li"),BW=a("strong"),p$e=o("lxmert"),_$e=o(" \u2014 "),kL=a("a"),u$e=o("LxmertConfig"),b$e=o(" (LXMERT model)"),v$e=l(),_m=a("li"),kW=a("strong"),T$e=o("m2m_100"),F$e=o(" \u2014 "),xL=a("a"),C$e=o("M2M100Config"),M$e=o(" (M2M100 model)"),E$e=l(),um=a("li"),xW=a("strong"),y$e=o("marian"),w$e=o(" \u2014 "),RL=a("a"),A$e=o("MarianConfig"),L$e=o(" (Marian model)"),B$e=l(),bm=a("li"),RW=a("strong"),k$e=o("mbart"),x$e=o(" \u2014 "),SL=a("a"),R$e=o("MBartConfig"),S$e=o(" (mBART model)"),P$e=l(),vm=a("li"),SW=a("strong"),$$e=o("megatron-bert"),I$e=o(" \u2014 "),PL=a("a"),j$e=o("MegatronBertConfig"),N$e=o(" (MegatronBert model)"),D$e=l(),Tm=a("li"),PW=a("strong"),q$e=o("mobilebert"),G$e=o(" \u2014 "),$L=a("a"),O$e=o("MobileBertConfig"),X$e=o(" (MobileBERT model)"),z$e=l(),Fm=a("li"),$W=a("strong"),V$e=o("mpnet"),W$e=o(" \u2014 "),IL=a("a"),Q$e=o("MPNetConfig"),H$e=o(" (MPNet model)"),U$e=l(),Cm=a("li"),IW=a("strong"),J$e=o("mt5"),Y$e=o(" \u2014 "),jL=a("a"),K$e=o("MT5Config"),Z$e=o(" (mT5 model)"),eIe=l(),Mm=a("li"),jW=a("strong"),oIe=o("nystromformer"),rIe=o(" \u2014 "),NL=a("a"),tIe=o("NystromformerConfig"),aIe=o(" (Nystromformer model)"),nIe=l(),Em=a("li"),NW=a("strong"),sIe=o("openai-gpt"),lIe=o(" \u2014 "),DL=a("a"),iIe=o("OpenAIGPTConfig"),dIe=o(" (OpenAI GPT model)"),cIe=l(),ym=a("li"),DW=a("strong"),fIe=o("pegasus"),mIe=o(" \u2014 "),qL=a("a"),gIe=o("PegasusConfig"),hIe=o(" (Pegasus model)"),pIe=l(),wm=a("li"),qW=a("strong"),_Ie=o("perceiver"),uIe=o(" \u2014 "),GL=a("a"),bIe=o("PerceiverConfig"),vIe=o(" (Perceiver model)"),TIe=l(),Am=a("li"),GW=a("strong"),FIe=o("plbart"),CIe=o(" \u2014 "),OL=a("a"),MIe=o("PLBartConfig"),EIe=o(" (PLBart model)"),yIe=l(),Lm=a("li"),OW=a("strong"),wIe=o("poolformer"),AIe=o(" \u2014 "),XL=a("a"),LIe=o("PoolFormerConfig"),BIe=o(" (PoolFormer model)"),kIe=l(),Bm=a("li"),XW=a("strong"),xIe=o("prophetnet"),RIe=o(" \u2014 "),zL=a("a"),SIe=o("ProphetNetConfig"),PIe=o(" (ProphetNet model)"),$Ie=l(),km=a("li"),zW=a("strong"),IIe=o("qdqbert"),jIe=o(" \u2014 "),VL=a("a"),NIe=o("QDQBertConfig"),DIe=o(" (QDQBert model)"),qIe=l(),xm=a("li"),VW=a("strong"),GIe=o("rag"),OIe=o(" \u2014 "),WL=a("a"),XIe=o("RagConfig"),zIe=o(" (RAG model)"),VIe=l(),Rm=a("li"),WW=a("strong"),WIe=o("realm"),QIe=o(" \u2014 "),QL=a("a"),HIe=o("RealmConfig"),UIe=o(" (Realm model)"),JIe=l(),Sm=a("li"),QW=a("strong"),YIe=o("reformer"),KIe=o(" \u2014 "),HL=a("a"),ZIe=o("ReformerConfig"),eje=o(" (Reformer model)"),oje=l(),Pm=a("li"),HW=a("strong"),rje=o("rembert"),tje=o(" \u2014 "),UL=a("a"),aje=o("RemBertConfig"),nje=o(" (RemBERT model)"),sje=l(),$m=a("li"),UW=a("strong"),lje=o("retribert"),ije=o(" \u2014 "),JL=a("a"),dje=o("RetriBertConfig"),cje=o(" (RetriBERT model)"),fje=l(),Im=a("li"),JW=a("strong"),mje=o("roberta"),gje=o(" \u2014 "),YL=a("a"),hje=o("RobertaConfig"),pje=o(" (RoBERTa model)"),_je=l(),jm=a("li"),YW=a("strong"),uje=o("roformer"),bje=o(" \u2014 "),KL=a("a"),vje=o("RoFormerConfig"),Tje=o(" (RoFormer model)"),Fje=l(),Nm=a("li"),KW=a("strong"),Cje=o("segformer"),Mje=o(" \u2014 "),ZL=a("a"),Eje=o("SegformerConfig"),yje=o(" (SegFormer model)"),wje=l(),Dm=a("li"),ZW=a("strong"),Aje=o("sew"),Lje=o(" \u2014 "),e8=a("a"),Bje=o("SEWConfig"),kje=o(" (SEW model)"),xje=l(),qm=a("li"),eQ=a("strong"),Rje=o("sew-d"),Sje=o(" \u2014 "),o8=a("a"),Pje=o("SEWDConfig"),$je=o(" (SEW-D model)"),Ije=l(),Gm=a("li"),oQ=a("strong"),jje=o("speech-encoder-decoder"),Nje=o(" \u2014 "),r8=a("a"),Dje=o("SpeechEncoderDecoderConfig"),qje=o(" (Speech Encoder decoder model)"),Gje=l(),Om=a("li"),rQ=a("strong"),Oje=o("speech_to_text"),Xje=o(" \u2014 "),t8=a("a"),zje=o("Speech2TextConfig"),Vje=o(" (Speech2Text model)"),Wje=l(),Xm=a("li"),tQ=a("strong"),Qje=o("speech_to_text_2"),Hje=o(" \u2014 "),a8=a("a"),Uje=o("Speech2Text2Config"),Jje=o(" (Speech2Text2 model)"),Yje=l(),zm=a("li"),aQ=a("strong"),Kje=o("splinter"),Zje=o(" \u2014 "),n8=a("a"),eNe=o("SplinterConfig"),oNe=o(" (Splinter model)"),rNe=l(),Vm=a("li"),nQ=a("strong"),tNe=o("squeezebert"),aNe=o(" \u2014 "),s8=a("a"),nNe=o("SqueezeBertConfig"),sNe=o(" (SqueezeBERT model)"),lNe=l(),Wm=a("li"),sQ=a("strong"),iNe=o("swin"),dNe=o(" \u2014 "),l8=a("a"),cNe=o("SwinConfig"),fNe=o(" (Swin model)"),mNe=l(),Qm=a("li"),lQ=a("strong"),gNe=o("t5"),hNe=o(" \u2014 "),i8=a("a"),pNe=o("T5Config"),_Ne=o(" (T5 model)"),uNe=l(),Hm=a("li"),iQ=a("strong"),bNe=o("tapas"),vNe=o(" \u2014 "),d8=a("a"),TNe=o("TapasConfig"),FNe=o(" (TAPAS model)"),CNe=l(),Um=a("li"),dQ=a("strong"),MNe=o("transfo-xl"),ENe=o(" \u2014 "),c8=a("a"),yNe=o("TransfoXLConfig"),wNe=o(" (Transformer-XL model)"),ANe=l(),Jm=a("li"),cQ=a("strong"),LNe=o("trocr"),BNe=o(" \u2014 "),f8=a("a"),kNe=o("TrOCRConfig"),xNe=o(" (TrOCR model)"),RNe=l(),Ym=a("li"),fQ=a("strong"),SNe=o("unispeech"),PNe=o(" \u2014 "),m8=a("a"),$Ne=o("UniSpeechConfig"),INe=o(" (UniSpeech model)"),jNe=l(),Km=a("li"),mQ=a("strong"),NNe=o("unispeech-sat"),DNe=o(" \u2014 "),g8=a("a"),qNe=o("UniSpeechSatConfig"),GNe=o(" (UniSpeechSat model)"),ONe=l(),Zm=a("li"),gQ=a("strong"),XNe=o("vilt"),zNe=o(" \u2014 "),h8=a("a"),VNe=o("ViltConfig"),WNe=o(" (ViLT model)"),QNe=l(),eg=a("li"),hQ=a("strong"),HNe=o("vision-encoder-decoder"),UNe=o(" \u2014 "),p8=a("a"),JNe=o("VisionEncoderDecoderConfig"),YNe=o(" (Vision Encoder decoder model)"),KNe=l(),og=a("li"),pQ=a("strong"),ZNe=o("vision-text-dual-encoder"),eDe=o(" \u2014 "),_8=a("a"),oDe=o("VisionTextDualEncoderConfig"),rDe=o(" (VisionTextDualEncoder model)"),tDe=l(),rg=a("li"),_Q=a("strong"),aDe=o("visual_bert"),nDe=o(" \u2014 "),u8=a("a"),sDe=o("VisualBertConfig"),lDe=o(" (VisualBert model)"),iDe=l(),tg=a("li"),uQ=a("strong"),dDe=o("vit"),cDe=o(" \u2014 "),b8=a("a"),fDe=o("ViTConfig"),mDe=o(" (ViT model)"),gDe=l(),ag=a("li"),bQ=a("strong"),hDe=o("vit_mae"),pDe=o(" \u2014 "),v8=a("a"),_De=o("ViTMAEConfig"),uDe=o(" (ViTMAE model)"),bDe=l(),ng=a("li"),vQ=a("strong"),vDe=o("wav2vec2"),TDe=o(" \u2014 "),T8=a("a"),FDe=o("Wav2Vec2Config"),CDe=o(" (Wav2Vec2 model)"),MDe=l(),sg=a("li"),TQ=a("strong"),EDe=o("wavlm"),yDe=o(" \u2014 "),F8=a("a"),wDe=o("WavLMConfig"),ADe=o(" (WavLM model)"),LDe=l(),lg=a("li"),FQ=a("strong"),BDe=o("xglm"),kDe=o(" \u2014 "),C8=a("a"),xDe=o("XGLMConfig"),RDe=o(" (XGLM model)"),SDe=l(),ig=a("li"),CQ=a("strong"),PDe=o("xlm"),$De=o(" \u2014 "),M8=a("a"),IDe=o("XLMConfig"),jDe=o(" (XLM model)"),NDe=l(),dg=a("li"),MQ=a("strong"),DDe=o("xlm-prophetnet"),qDe=o(" \u2014 "),E8=a("a"),GDe=o("XLMProphetNetConfig"),ODe=o(" (XLMProphetNet model)"),XDe=l(),cg=a("li"),EQ=a("strong"),zDe=o("xlm-roberta"),VDe=o(" \u2014 "),y8=a("a"),WDe=o("XLMRobertaConfig"),QDe=o(" (XLM-RoBERTa model)"),HDe=l(),fg=a("li"),yQ=a("strong"),UDe=o("xlm-roberta-xl"),JDe=o(" \u2014 "),w8=a("a"),YDe=o("XLMRobertaXLConfig"),KDe=o(" (XLM-RoBERTa-XL model)"),ZDe=l(),mg=a("li"),wQ=a("strong"),eqe=o("xlnet"),oqe=o(" \u2014 "),A8=a("a"),rqe=o("XLNetConfig"),tqe=o(" (XLNet model)"),aqe=l(),gg=a("li"),AQ=a("strong"),nqe=o("yoso"),sqe=o(" \u2014 "),L8=a("a"),lqe=o("YosoConfig"),iqe=o(" (YOSO model)"),dqe=l(),LQ=a("p"),cqe=o("Examples:"),fqe=l(),f(mM.$$.fragment),mqe=l(),hg=a("div"),f(gM.$$.fragment),gqe=l(),BQ=a("p"),hqe=o("Register a new configuration for this class."),cLe=l(),Ii=a("h2"),pg=a("a"),kQ=a("span"),f(hM.$$.fragment),pqe=l(),xQ=a("span"),_qe=o("AutoTokenizer"),fLe=l(),Oo=a("div"),f(pM.$$.fragment),uqe=l(),_M=a("p"),bqe=o(`This is a generic tokenizer class that will be instantiated as one of the tokenizer classes of the library when
created with the `),B8=a("a"),vqe=o("AutoTokenizer.from_pretrained()"),Tqe=o(" class method."),Fqe=l(),uM=a("p"),Cqe=o("This class cannot be instantiated directly using "),RQ=a("code"),Mqe=o("__init__()"),Eqe=o(" (throws an error)."),yqe=l(),mo=a("div"),f(bM.$$.fragment),wqe=l(),SQ=a("p"),Aqe=o("Instantiate one of the tokenizer classes of the library from a pretrained model vocabulary."),Lqe=l(),ja=a("p"),Bqe=o("The tokenizer class to instantiate is selected based on the "),PQ=a("code"),kqe=o("model_type"),xqe=o(` property of the config object (either
passed as an argument or loaded from `),$Q=a("code"),Rqe=o("pretrained_model_name_or_path"),Sqe=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),IQ=a("code"),Pqe=o("pretrained_model_name_or_path"),$qe=o(":"),Iqe=l(),M=a("ul"),Dn=a("li"),jQ=a("strong"),jqe=o("albert"),Nqe=o(" \u2014 "),k8=a("a"),Dqe=o("AlbertTokenizer"),qqe=o(" or "),x8=a("a"),Gqe=o("AlbertTokenizerFast"),Oqe=o(" (ALBERT model)"),Xqe=l(),qn=a("li"),NQ=a("strong"),zqe=o("bart"),Vqe=o(" \u2014 "),R8=a("a"),Wqe=o("BartTokenizer"),Qqe=o(" or "),S8=a("a"),Hqe=o("BartTokenizerFast"),Uqe=o(" (BART model)"),Jqe=l(),Gn=a("li"),DQ=a("strong"),Yqe=o("barthez"),Kqe=o(" \u2014 "),P8=a("a"),Zqe=o("BarthezTokenizer"),eGe=o(" or "),$8=a("a"),oGe=o("BarthezTokenizerFast"),rGe=o(" (BARThez model)"),tGe=l(),_g=a("li"),qQ=a("strong"),aGe=o("bartpho"),nGe=o(" \u2014 "),I8=a("a"),sGe=o("BartphoTokenizer"),lGe=o(" (BARTpho model)"),iGe=l(),On=a("li"),GQ=a("strong"),dGe=o("bert"),cGe=o(" \u2014 "),j8=a("a"),fGe=o("BertTokenizer"),mGe=o(" or "),N8=a("a"),gGe=o("BertTokenizerFast"),hGe=o(" (BERT model)"),pGe=l(),ug=a("li"),OQ=a("strong"),_Ge=o("bert-generation"),uGe=o(" \u2014 "),D8=a("a"),bGe=o("BertGenerationTokenizer"),vGe=o(" (Bert Generation model)"),TGe=l(),bg=a("li"),XQ=a("strong"),FGe=o("bert-japanese"),CGe=o(" \u2014 "),q8=a("a"),MGe=o("BertJapaneseTokenizer"),EGe=o(" (BertJapanese model)"),yGe=l(),vg=a("li"),zQ=a("strong"),wGe=o("bertweet"),AGe=o(" \u2014 "),G8=a("a"),LGe=o("BertweetTokenizer"),BGe=o(" (Bertweet model)"),kGe=l(),Xn=a("li"),VQ=a("strong"),xGe=o("big_bird"),RGe=o(" \u2014 "),O8=a("a"),SGe=o("BigBirdTokenizer"),PGe=o(" or "),X8=a("a"),$Ge=o("BigBirdTokenizerFast"),IGe=o(" (BigBird model)"),jGe=l(),zn=a("li"),WQ=a("strong"),NGe=o("bigbird_pegasus"),DGe=o(" \u2014 "),z8=a("a"),qGe=o("PegasusTokenizer"),GGe=o(" or "),V8=a("a"),OGe=o("PegasusTokenizerFast"),XGe=o(" (BigBirdPegasus model)"),zGe=l(),Vn=a("li"),QQ=a("strong"),VGe=o("blenderbot"),WGe=o(" \u2014 "),W8=a("a"),QGe=o("BlenderbotTokenizer"),HGe=o(" or "),Q8=a("a"),UGe=o("BlenderbotTokenizerFast"),JGe=o(" (Blenderbot model)"),YGe=l(),Tg=a("li"),HQ=a("strong"),KGe=o("blenderbot-small"),ZGe=o(" \u2014 "),H8=a("a"),eOe=o("BlenderbotSmallTokenizer"),oOe=o(" (BlenderbotSmall model)"),rOe=l(),Fg=a("li"),UQ=a("strong"),tOe=o("byt5"),aOe=o(" \u2014 "),U8=a("a"),nOe=o("ByT5Tokenizer"),sOe=o(" (ByT5 model)"),lOe=l(),Wn=a("li"),JQ=a("strong"),iOe=o("camembert"),dOe=o(" \u2014 "),J8=a("a"),cOe=o("CamembertTokenizer"),fOe=o(" or "),Y8=a("a"),mOe=o("CamembertTokenizerFast"),gOe=o(" (CamemBERT model)"),hOe=l(),Cg=a("li"),YQ=a("strong"),pOe=o("canine"),_Oe=o(" \u2014 "),K8=a("a"),uOe=o("CanineTokenizer"),bOe=o(" (Canine model)"),vOe=l(),Qn=a("li"),KQ=a("strong"),TOe=o("clip"),FOe=o(" \u2014 "),Z8=a("a"),COe=o("CLIPTokenizer"),MOe=o(" or "),eB=a("a"),EOe=o("CLIPTokenizerFast"),yOe=o(" (CLIP model)"),wOe=l(),Hn=a("li"),ZQ=a("strong"),AOe=o("convbert"),LOe=o(" \u2014 "),oB=a("a"),BOe=o("ConvBertTokenizer"),kOe=o(" or "),rB=a("a"),xOe=o("ConvBertTokenizerFast"),ROe=o(" (ConvBERT model)"),SOe=l(),Un=a("li"),eH=a("strong"),POe=o("cpm"),$Oe=o(" \u2014 "),tB=a("a"),IOe=o("CpmTokenizer"),jOe=o(" or "),oH=a("code"),NOe=o("CpmTokenizerFast"),DOe=o(" (CPM model)"),qOe=l(),Mg=a("li"),rH=a("strong"),GOe=o("ctrl"),OOe=o(" \u2014 "),aB=a("a"),XOe=o("CTRLTokenizer"),zOe=o(" (CTRL model)"),VOe=l(),Jn=a("li"),tH=a("strong"),WOe=o("deberta"),QOe=o(" \u2014 "),nB=a("a"),HOe=o("DebertaTokenizer"),UOe=o(" or "),sB=a("a"),JOe=o("DebertaTokenizerFast"),YOe=o(" (DeBERTa model)"),KOe=l(),Eg=a("li"),aH=a("strong"),ZOe=o("deberta-v2"),eXe=o(" \u2014 "),lB=a("a"),oXe=o("DebertaV2Tokenizer"),rXe=o(" (DeBERTa-v2 model)"),tXe=l(),Yn=a("li"),nH=a("strong"),aXe=o("distilbert"),nXe=o(" \u2014 "),iB=a("a"),sXe=o("DistilBertTokenizer"),lXe=o(" or "),dB=a("a"),iXe=o("DistilBertTokenizerFast"),dXe=o(" (DistilBERT model)"),cXe=l(),Kn=a("li"),sH=a("strong"),fXe=o("dpr"),mXe=o(" \u2014 "),cB=a("a"),gXe=o("DPRQuestionEncoderTokenizer"),hXe=o(" or "),fB=a("a"),pXe=o("DPRQuestionEncoderTokenizerFast"),_Xe=o(" (DPR model)"),uXe=l(),Zn=a("li"),lH=a("strong"),bXe=o("electra"),vXe=o(" \u2014 "),mB=a("a"),TXe=o("ElectraTokenizer"),FXe=o(" or "),gB=a("a"),CXe=o("ElectraTokenizerFast"),MXe=o(" (ELECTRA model)"),EXe=l(),yg=a("li"),iH=a("strong"),yXe=o("flaubert"),wXe=o(" \u2014 "),hB=a("a"),AXe=o("FlaubertTokenizer"),LXe=o(" (FlauBERT model)"),BXe=l(),es=a("li"),dH=a("strong"),kXe=o("fnet"),xXe=o(" \u2014 "),pB=a("a"),RXe=o("FNetTokenizer"),SXe=o(" or "),_B=a("a"),PXe=o("FNetTokenizerFast"),$Xe=o(" (FNet model)"),IXe=l(),wg=a("li"),cH=a("strong"),jXe=o("fsmt"),NXe=o(" \u2014 "),uB=a("a"),DXe=o("FSMTTokenizer"),qXe=o(" (FairSeq Machine-Translation model)"),GXe=l(),os=a("li"),fH=a("strong"),OXe=o("funnel"),XXe=o(" \u2014 "),bB=a("a"),zXe=o("FunnelTokenizer"),VXe=o(" or "),vB=a("a"),WXe=o("FunnelTokenizerFast"),QXe=o(" (Funnel Transformer model)"),HXe=l(),rs=a("li"),mH=a("strong"),UXe=o("gpt2"),JXe=o(" \u2014 "),TB=a("a"),YXe=o("GPT2Tokenizer"),KXe=o(" or "),FB=a("a"),ZXe=o("GPT2TokenizerFast"),eze=o(" (OpenAI GPT-2 model)"),oze=l(),ts=a("li"),gH=a("strong"),rze=o("gpt_neo"),tze=o(" \u2014 "),CB=a("a"),aze=o("GPT2Tokenizer"),nze=o(" or "),MB=a("a"),sze=o("GPT2TokenizerFast"),lze=o(" (GPT Neo model)"),ize=l(),as=a("li"),hH=a("strong"),dze=o("herbert"),cze=o(" \u2014 "),EB=a("a"),fze=o("HerbertTokenizer"),mze=o(" or "),yB=a("a"),gze=o("HerbertTokenizerFast"),hze=o(" (HerBERT model)"),pze=l(),Ag=a("li"),pH=a("strong"),_ze=o("hubert"),uze=o(" \u2014 "),wB=a("a"),bze=o("Wav2Vec2CTCTokenizer"),vze=o(" (Hubert model)"),Tze=l(),ns=a("li"),_H=a("strong"),Fze=o("ibert"),Cze=o(" \u2014 "),AB=a("a"),Mze=o("RobertaTokenizer"),Eze=o(" or "),LB=a("a"),yze=o("RobertaTokenizerFast"),wze=o(" (I-BERT model)"),Aze=l(),ss=a("li"),uH=a("strong"),Lze=o("layoutlm"),Bze=o(" \u2014 "),BB=a("a"),kze=o("LayoutLMTokenizer"),xze=o(" or "),kB=a("a"),Rze=o("LayoutLMTokenizerFast"),Sze=o(" (LayoutLM model)"),Pze=l(),ls=a("li"),bH=a("strong"),$ze=o("layoutlmv2"),Ize=o(" \u2014 "),xB=a("a"),jze=o("LayoutLMv2Tokenizer"),Nze=o(" or "),RB=a("a"),Dze=o("LayoutLMv2TokenizerFast"),qze=o(" (LayoutLMv2 model)"),Gze=l(),is=a("li"),vH=a("strong"),Oze=o("layoutxlm"),Xze=o(" \u2014 "),SB=a("a"),zze=o("LayoutXLMTokenizer"),Vze=o(" or "),PB=a("a"),Wze=o("LayoutXLMTokenizerFast"),Qze=o(" (LayoutXLM model)"),Hze=l(),ds=a("li"),TH=a("strong"),Uze=o("led"),Jze=o(" \u2014 "),$B=a("a"),Yze=o("LEDTokenizer"),Kze=o(" or "),IB=a("a"),Zze=o("LEDTokenizerFast"),eVe=o(" (LED model)"),oVe=l(),cs=a("li"),FH=a("strong"),rVe=o("longformer"),tVe=o(" \u2014 "),jB=a("a"),aVe=o("LongformerTokenizer"),nVe=o(" or "),NB=a("a"),sVe=o("LongformerTokenizerFast"),lVe=o(" (Longformer model)"),iVe=l(),Lg=a("li"),CH=a("strong"),dVe=o("luke"),cVe=o(" \u2014 "),DB=a("a"),fVe=o("LukeTokenizer"),mVe=o(" (LUKE model)"),gVe=l(),fs=a("li"),MH=a("strong"),hVe=o("lxmert"),pVe=o(" \u2014 "),qB=a("a"),_Ve=o("LxmertTokenizer"),uVe=o(" or "),GB=a("a"),bVe=o("LxmertTokenizerFast"),vVe=o(" (LXMERT model)"),TVe=l(),Bg=a("li"),EH=a("strong"),FVe=o("m2m_100"),CVe=o(" \u2014 "),OB=a("a"),MVe=o("M2M100Tokenizer"),EVe=o(" (M2M100 model)"),yVe=l(),kg=a("li"),yH=a("strong"),wVe=o("marian"),AVe=o(" \u2014 "),XB=a("a"),LVe=o("MarianTokenizer"),BVe=o(" (Marian model)"),kVe=l(),ms=a("li"),wH=a("strong"),xVe=o("mbart"),RVe=o(" \u2014 "),zB=a("a"),SVe=o("MBartTokenizer"),PVe=o(" or "),VB=a("a"),$Ve=o("MBartTokenizerFast"),IVe=o(" (mBART model)"),jVe=l(),gs=a("li"),AH=a("strong"),NVe=o("mbart50"),DVe=o(" \u2014 "),WB=a("a"),qVe=o("MBart50Tokenizer"),GVe=o(" or "),QB=a("a"),OVe=o("MBart50TokenizerFast"),XVe=o(" (mBART-50 model)"),zVe=l(),xg=a("li"),LH=a("strong"),VVe=o("mluke"),WVe=o(" \u2014 "),HB=a("a"),QVe=o("MLukeTokenizer"),HVe=o(" (mLUKE model)"),UVe=l(),hs=a("li"),BH=a("strong"),JVe=o("mobilebert"),YVe=o(" \u2014 "),UB=a("a"),KVe=o("MobileBertTokenizer"),ZVe=o(" or "),JB=a("a"),eWe=o("MobileBertTokenizerFast"),oWe=o(" (MobileBERT model)"),rWe=l(),ps=a("li"),kH=a("strong"),tWe=o("mpnet"),aWe=o(" \u2014 "),YB=a("a"),nWe=o("MPNetTokenizer"),sWe=o(" or "),KB=a("a"),lWe=o("MPNetTokenizerFast"),iWe=o(" (MPNet model)"),dWe=l(),_s=a("li"),xH=a("strong"),cWe=o("mt5"),fWe=o(" \u2014 "),ZB=a("a"),mWe=o("MT5Tokenizer"),gWe=o(" or "),ek=a("a"),hWe=o("MT5TokenizerFast"),pWe=o(" (mT5 model)"),_We=l(),us=a("li"),RH=a("strong"),uWe=o("openai-gpt"),bWe=o(" \u2014 "),ok=a("a"),vWe=o("OpenAIGPTTokenizer"),TWe=o(" or "),rk=a("a"),FWe=o("OpenAIGPTTokenizerFast"),CWe=o(" (OpenAI GPT model)"),MWe=l(),bs=a("li"),SH=a("strong"),EWe=o("pegasus"),yWe=o(" \u2014 "),tk=a("a"),wWe=o("PegasusTokenizer"),AWe=o(" or "),ak=a("a"),LWe=o("PegasusTokenizerFast"),BWe=o(" (Pegasus model)"),kWe=l(),Rg=a("li"),PH=a("strong"),xWe=o("perceiver"),RWe=o(" \u2014 "),nk=a("a"),SWe=o("PerceiverTokenizer"),PWe=o(" (Perceiver model)"),$We=l(),Sg=a("li"),$H=a("strong"),IWe=o("phobert"),jWe=o(" \u2014 "),sk=a("a"),NWe=o("PhobertTokenizer"),DWe=o(" (PhoBERT model)"),qWe=l(),Pg=a("li"),IH=a("strong"),GWe=o("plbart"),OWe=o(" \u2014 "),lk=a("a"),XWe=o("PLBartTokenizer"),zWe=o(" (PLBart model)"),VWe=l(),$g=a("li"),jH=a("strong"),WWe=o("prophetnet"),QWe=o(" \u2014 "),ik=a("a"),HWe=o("ProphetNetTokenizer"),UWe=o(" (ProphetNet model)"),JWe=l(),vs=a("li"),NH=a("strong"),YWe=o("qdqbert"),KWe=o(" \u2014 "),dk=a("a"),ZWe=o("BertTokenizer"),eQe=o(" or "),ck=a("a"),oQe=o("BertTokenizerFast"),rQe=o(" (QDQBert model)"),tQe=l(),Ig=a("li"),DH=a("strong"),aQe=o("rag"),nQe=o(" \u2014 "),fk=a("a"),sQe=o("RagTokenizer"),lQe=o(" (RAG model)"),iQe=l(),Ts=a("li"),qH=a("strong"),dQe=o("reformer"),cQe=o(" \u2014 "),mk=a("a"),fQe=o("ReformerTokenizer"),mQe=o(" or "),gk=a("a"),gQe=o("ReformerTokenizerFast"),hQe=o(" (Reformer model)"),pQe=l(),Fs=a("li"),GH=a("strong"),_Qe=o("rembert"),uQe=o(" \u2014 "),hk=a("a"),bQe=o("RemBertTokenizer"),vQe=o(" or "),pk=a("a"),TQe=o("RemBertTokenizerFast"),FQe=o(" (RemBERT model)"),CQe=l(),Cs=a("li"),OH=a("strong"),MQe=o("retribert"),EQe=o(" \u2014 "),_k=a("a"),yQe=o("RetriBertTokenizer"),wQe=o(" or "),uk=a("a"),AQe=o("RetriBertTokenizerFast"),LQe=o(" (RetriBERT model)"),BQe=l(),Ms=a("li"),XH=a("strong"),kQe=o("roberta"),xQe=o(" \u2014 "),bk=a("a"),RQe=o("RobertaTokenizer"),SQe=o(" or "),vk=a("a"),PQe=o("RobertaTokenizerFast"),$Qe=o(" (RoBERTa model)"),IQe=l(),Es=a("li"),zH=a("strong"),jQe=o("roformer"),NQe=o(" \u2014 "),Tk=a("a"),DQe=o("RoFormerTokenizer"),qQe=o(" or "),Fk=a("a"),GQe=o("RoFormerTokenizerFast"),OQe=o(" (RoFormer model)"),XQe=l(),jg=a("li"),VH=a("strong"),zQe=o("speech_to_text"),VQe=o(" \u2014 "),Ck=a("a"),WQe=o("Speech2TextTokenizer"),QQe=o(" (Speech2Text model)"),HQe=l(),Ng=a("li"),WH=a("strong"),UQe=o("speech_to_text_2"),JQe=o(" \u2014 "),Mk=a("a"),YQe=o("Speech2Text2Tokenizer"),KQe=o(" (Speech2Text2 model)"),ZQe=l(),ys=a("li"),QH=a("strong"),eHe=o("splinter"),oHe=o(" \u2014 "),Ek=a("a"),rHe=o("SplinterTokenizer"),tHe=o(" or "),yk=a("a"),aHe=o("SplinterTokenizerFast"),nHe=o(" (Splinter model)"),sHe=l(),ws=a("li"),HH=a("strong"),lHe=o("squeezebert"),iHe=o(" \u2014 "),wk=a("a"),dHe=o("SqueezeBertTokenizer"),cHe=o(" or "),Ak=a("a"),fHe=o("SqueezeBertTokenizerFast"),mHe=o(" (SqueezeBERT model)"),gHe=l(),As=a("li"),UH=a("strong"),hHe=o("t5"),pHe=o(" \u2014 "),Lk=a("a"),_He=o("T5Tokenizer"),uHe=o(" or "),Bk=a("a"),bHe=o("T5TokenizerFast"),vHe=o(" (T5 model)"),THe=l(),Dg=a("li"),JH=a("strong"),FHe=o("tapas"),CHe=o(" \u2014 "),kk=a("a"),MHe=o("TapasTokenizer"),EHe=o(" (TAPAS model)"),yHe=l(),qg=a("li"),YH=a("strong"),wHe=o("transfo-xl"),AHe=o(" \u2014 "),xk=a("a"),LHe=o("TransfoXLTokenizer"),BHe=o(" (Transformer-XL model)"),kHe=l(),Gg=a("li"),KH=a("strong"),xHe=o("wav2vec2"),RHe=o(" \u2014 "),Rk=a("a"),SHe=o("Wav2Vec2CTCTokenizer"),PHe=o(" (Wav2Vec2 model)"),$He=l(),Og=a("li"),ZH=a("strong"),IHe=o("wav2vec2_phoneme"),jHe=o(" \u2014 "),Sk=a("a"),NHe=o("Wav2Vec2PhonemeCTCTokenizer"),DHe=o(" (Wav2Vec2Phoneme model)"),qHe=l(),Ls=a("li"),eU=a("strong"),GHe=o("xglm"),OHe=o(" \u2014 "),Pk=a("a"),XHe=o("XGLMTokenizer"),zHe=o(" or "),$k=a("a"),VHe=o("XGLMTokenizerFast"),WHe=o(" (XGLM model)"),QHe=l(),Xg=a("li"),oU=a("strong"),HHe=o("xlm"),UHe=o(" \u2014 "),Ik=a("a"),JHe=o("XLMTokenizer"),YHe=o(" (XLM model)"),KHe=l(),zg=a("li"),rU=a("strong"),ZHe=o("xlm-prophetnet"),eUe=o(" \u2014 "),jk=a("a"),oUe=o("XLMProphetNetTokenizer"),rUe=o(" (XLMProphetNet model)"),tUe=l(),Bs=a("li"),tU=a("strong"),aUe=o("xlm-roberta"),nUe=o(" \u2014 "),Nk=a("a"),sUe=o("XLMRobertaTokenizer"),lUe=o(" or "),Dk=a("a"),iUe=o("XLMRobertaTokenizerFast"),dUe=o(" (XLM-RoBERTa model)"),cUe=l(),ks=a("li"),aU=a("strong"),fUe=o("xlnet"),mUe=o(" \u2014 "),qk=a("a"),gUe=o("XLNetTokenizer"),hUe=o(" or "),Gk=a("a"),pUe=o("XLNetTokenizerFast"),_Ue=o(" (XLNet model)"),uUe=l(),nU=a("p"),bUe=o("Examples:"),vUe=l(),f(vM.$$.fragment),TUe=l(),Vg=a("div"),f(TM.$$.fragment),FUe=l(),sU=a("p"),CUe=o("Register a new tokenizer in this mapping."),mLe=l(),ji=a("h2"),Wg=a("a"),lU=a("span"),f(FM.$$.fragment),MUe=l(),iU=a("span"),EUe=o("AutoFeatureExtractor"),gLe=l(),Xo=a("div"),f(CM.$$.fragment),yUe=l(),MM=a("p"),wUe=o(`This is a generic feature extractor class that will be instantiated as one of the feature extractor classes of the
library when created with the `),Ok=a("a"),AUe=o("AutoFeatureExtractor.from_pretrained()"),LUe=o(" class method."),BUe=l(),EM=a("p"),kUe=o("This class cannot be instantiated directly using "),dU=a("code"),xUe=o("__init__()"),RUe=o(" (throws an error)."),SUe=l(),Le=a("div"),f(yM.$$.fragment),PUe=l(),cU=a("p"),$Ue=o("Instantiate one of the feature extractor classes of the library from a pretrained model vocabulary."),IUe=l(),Na=a("p"),jUe=o("The feature extractor class to instantiate is selected based on the "),fU=a("code"),NUe=o("model_type"),DUe=o(` property of the config object
(either passed as an argument or loaded from `),mU=a("code"),qUe=o("pretrained_model_name_or_path"),GUe=o(` if possible), or when it\u2019s
missing, by falling back to using pattern matching on `),gU=a("code"),OUe=o("pretrained_model_name_or_path"),XUe=o(":"),zUe=l(),se=a("ul"),Qg=a("li"),hU=a("strong"),VUe=o("beit"),WUe=o(" \u2014 "),Xk=a("a"),QUe=o("BeitFeatureExtractor"),HUe=o(" (BEiT model)"),UUe=l(),Hg=a("li"),pU=a("strong"),JUe=o("clip"),YUe=o(" \u2014 "),zk=a("a"),KUe=o("CLIPFeatureExtractor"),ZUe=o(" (CLIP model)"),eJe=l(),Ug=a("li"),_U=a("strong"),oJe=o("convnext"),rJe=o(" \u2014 "),Vk=a("a"),tJe=o("ConvNextFeatureExtractor"),aJe=o(" (ConvNext model)"),nJe=l(),Jg=a("li"),uU=a("strong"),sJe=o("deit"),lJe=o(" \u2014 "),Wk=a("a"),iJe=o("DeiTFeatureExtractor"),dJe=o(" (DeiT model)"),cJe=l(),Yg=a("li"),bU=a("strong"),fJe=o("detr"),mJe=o(" \u2014 "),Qk=a("a"),gJe=o("DetrFeatureExtractor"),hJe=o(" (DETR model)"),pJe=l(),Kg=a("li"),vU=a("strong"),_Je=o("hubert"),uJe=o(" \u2014 "),Hk=a("a"),bJe=o("Wav2Vec2FeatureExtractor"),vJe=o(" (Hubert model)"),TJe=l(),Zg=a("li"),TU=a("strong"),FJe=o("layoutlmv2"),CJe=o(" \u2014 "),Uk=a("a"),MJe=o("LayoutLMv2FeatureExtractor"),EJe=o(" (LayoutLMv2 model)"),yJe=l(),eh=a("li"),FU=a("strong"),wJe=o("perceiver"),AJe=o(" \u2014 "),Jk=a("a"),LJe=o("PerceiverFeatureExtractor"),BJe=o(" (Perceiver model)"),kJe=l(),oh=a("li"),CU=a("strong"),xJe=o("poolformer"),RJe=o(" \u2014 "),Yk=a("a"),SJe=o("PoolFormerFeatureExtractor"),PJe=o(" (PoolFormer model)"),$Je=l(),rh=a("li"),MU=a("strong"),IJe=o("segformer"),jJe=o(" \u2014 "),Kk=a("a"),NJe=o("SegformerFeatureExtractor"),DJe=o(" (SegFormer model)"),qJe=l(),th=a("li"),EU=a("strong"),GJe=o("speech_to_text"),OJe=o(" \u2014 "),Zk=a("a"),XJe=o("Speech2TextFeatureExtractor"),zJe=o(" (Speech2Text model)"),VJe=l(),ah=a("li"),yU=a("strong"),WJe=o("swin"),QJe=o(" \u2014 "),ex=a("a"),HJe=o("ViTFeatureExtractor"),UJe=o(" (Swin model)"),JJe=l(),nh=a("li"),wU=a("strong"),YJe=o("vit"),KJe=o(" \u2014 "),ox=a("a"),ZJe=o("ViTFeatureExtractor"),eYe=o(" (ViT model)"),oYe=l(),sh=a("li"),AU=a("strong"),rYe=o("vit_mae"),tYe=o(" \u2014 "),rx=a("a"),aYe=o("ViTFeatureExtractor"),nYe=o(" (ViTMAE model)"),sYe=l(),lh=a("li"),LU=a("strong"),lYe=o("wav2vec2"),iYe=o(" \u2014 "),tx=a("a"),dYe=o("Wav2Vec2FeatureExtractor"),cYe=o(" (Wav2Vec2 model)"),fYe=l(),f(ih.$$.fragment),mYe=l(),BU=a("p"),gYe=o("Examples:"),hYe=l(),f(wM.$$.fragment),pYe=l(),dh=a("div"),f(AM.$$.fragment),_Ye=l(),kU=a("p"),uYe=o("Register a new feature extractor for this class."),hLe=l(),Ni=a("h2"),ch=a("a"),xU=a("span"),f(LM.$$.fragment),bYe=l(),RU=a("span"),vYe=o("AutoProcessor"),pLe=l(),zo=a("div"),f(BM.$$.fragment),TYe=l(),kM=a("p"),FYe=o(`This is a generic processor class that will be instantiated as one of the processor classes of the library when
created with the `),ax=a("a"),CYe=o("AutoProcessor.from_pretrained()"),MYe=o(" class method."),EYe=l(),xM=a("p"),yYe=o("This class cannot be instantiated directly using "),SU=a("code"),wYe=o("__init__()"),AYe=o(" (throws an error)."),LYe=l(),Be=a("div"),f(RM.$$.fragment),BYe=l(),PU=a("p"),kYe=o("Instantiate one of the processor classes of the library from a pretrained model vocabulary."),xYe=l(),Di=a("p"),RYe=o("The processor class to instantiate is selected based on the "),$U=a("code"),SYe=o("model_type"),PYe=o(` property of the config object (either
passed as an argument or loaded from `),IU=a("code"),$Ye=o("pretrained_model_name_or_path"),IYe=o(" if possible):"),jYe=l(),we=a("ul"),fh=a("li"),jU=a("strong"),NYe=o("clip"),DYe=o(" \u2014 "),nx=a("a"),qYe=o("CLIPProcessor"),GYe=o(" (CLIP model)"),OYe=l(),mh=a("li"),NU=a("strong"),XYe=o("layoutlmv2"),zYe=o(" \u2014 "),sx=a("a"),VYe=o("LayoutLMv2Processor"),WYe=o(" (LayoutLMv2 model)"),QYe=l(),gh=a("li"),DU=a("strong"),HYe=o("layoutxlm"),UYe=o(" \u2014 "),lx=a("a"),JYe=o("LayoutXLMProcessor"),YYe=o(" (LayoutXLM model)"),KYe=l(),hh=a("li"),qU=a("strong"),ZYe=o("speech_to_text"),eKe=o(" \u2014 "),ix=a("a"),oKe=o("Speech2TextProcessor"),rKe=o(" (Speech2Text model)"),tKe=l(),ph=a("li"),GU=a("strong"),aKe=o("speech_to_text_2"),nKe=o(" \u2014 "),dx=a("a"),sKe=o("Speech2Text2Processor"),lKe=o(" (Speech2Text2 model)"),iKe=l(),_h=a("li"),OU=a("strong"),dKe=o("trocr"),cKe=o(" \u2014 "),cx=a("a"),fKe=o("TrOCRProcessor"),mKe=o(" (TrOCR model)"),gKe=l(),uh=a("li"),XU=a("strong"),hKe=o("vision-text-dual-encoder"),pKe=o(" \u2014 "),fx=a("a"),_Ke=o("VisionTextDualEncoderProcessor"),uKe=o(" (VisionTextDualEncoder model)"),bKe=l(),bh=a("li"),zU=a("strong"),vKe=o("wav2vec2"),TKe=o(" \u2014 "),mx=a("a"),FKe=o("Wav2Vec2Processor"),CKe=o(" (Wav2Vec2 model)"),MKe=l(),f(vh.$$.fragment),EKe=l(),VU=a("p"),yKe=o("Examples:"),wKe=l(),f(SM.$$.fragment),AKe=l(),Th=a("div"),f(PM.$$.fragment),LKe=l(),WU=a("p"),BKe=o("Register a new processor for this class."),_Le=l(),qi=a("h2"),Fh=a("a"),QU=a("span"),f($M.$$.fragment),kKe=l(),HU=a("span"),xKe=o("AutoModel"),uLe=l(),Vo=a("div"),f(IM.$$.fragment),RKe=l(),Gi=a("p"),SKe=o(`This is a generic model class that will be instantiated as one of the base model classes of the library when created
with the `),UU=a("code"),PKe=o("from_pretrained()"),$Ke=o("class method or the "),JU=a("code"),IKe=o("from_config()"),jKe=o(`class
method.`),NKe=l(),jM=a("p"),DKe=o("This class cannot be instantiated directly using "),YU=a("code"),qKe=o("__init__()"),GKe=o(" (throws an error)."),OKe=l(),Nr=a("div"),f(NM.$$.fragment),XKe=l(),KU=a("p"),zKe=o("Instantiates one of the base model classes of the library from a configuration."),VKe=l(),Oi=a("p"),WKe=o(`Note:
Loading a model from its configuration file does `),ZU=a("strong"),QKe=o("not"),HKe=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),eJ=a("code"),UKe=o("from_pretrained()"),JKe=o("to load the model weights."),YKe=l(),oJ=a("p"),KKe=o("Examples:"),ZKe=l(),f(DM.$$.fragment),eZe=l(),ke=a("div"),f(qM.$$.fragment),oZe=l(),rJ=a("p"),rZe=o("Instantiate one of the base model classes of the library from a pretrained model."),tZe=l(),Da=a("p"),aZe=o("The model class to instantiate is selected based on the "),tJ=a("code"),nZe=o("model_type"),sZe=o(` property of the config object (either
passed as an argument or loaded from `),aJ=a("code"),lZe=o("pretrained_model_name_or_path"),iZe=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),nJ=a("code"),dZe=o("pretrained_model_name_or_path"),cZe=o(":"),fZe=l(),F=a("ul"),Ch=a("li"),sJ=a("strong"),mZe=o("albert"),gZe=o(" \u2014 "),gx=a("a"),hZe=o("AlbertModel"),pZe=o(" (ALBERT model)"),_Ze=l(),Mh=a("li"),lJ=a("strong"),uZe=o("bart"),bZe=o(" \u2014 "),hx=a("a"),vZe=o("BartModel"),TZe=o(" (BART model)"),FZe=l(),Eh=a("li"),iJ=a("strong"),CZe=o("beit"),MZe=o(" \u2014 "),px=a("a"),EZe=o("BeitModel"),yZe=o(" (BEiT model)"),wZe=l(),yh=a("li"),dJ=a("strong"),AZe=o("bert"),LZe=o(" \u2014 "),_x=a("a"),BZe=o("BertModel"),kZe=o(" (BERT model)"),xZe=l(),wh=a("li"),cJ=a("strong"),RZe=o("bert-generation"),SZe=o(" \u2014 "),ux=a("a"),PZe=o("BertGenerationEncoder"),$Ze=o(" (Bert Generation model)"),IZe=l(),Ah=a("li"),fJ=a("strong"),jZe=o("big_bird"),NZe=o(" \u2014 "),bx=a("a"),DZe=o("BigBirdModel"),qZe=o(" (BigBird model)"),GZe=l(),Lh=a("li"),mJ=a("strong"),OZe=o("bigbird_pegasus"),XZe=o(" \u2014 "),vx=a("a"),zZe=o("BigBirdPegasusModel"),VZe=o(" (BigBirdPegasus model)"),WZe=l(),Bh=a("li"),gJ=a("strong"),QZe=o("blenderbot"),HZe=o(" \u2014 "),Tx=a("a"),UZe=o("BlenderbotModel"),JZe=o(" (Blenderbot model)"),YZe=l(),kh=a("li"),hJ=a("strong"),KZe=o("blenderbot-small"),ZZe=o(" \u2014 "),Fx=a("a"),eeo=o("BlenderbotSmallModel"),oeo=o(" (BlenderbotSmall model)"),reo=l(),xh=a("li"),pJ=a("strong"),teo=o("camembert"),aeo=o(" \u2014 "),Cx=a("a"),neo=o("CamembertModel"),seo=o(" (CamemBERT model)"),leo=l(),Rh=a("li"),_J=a("strong"),ieo=o("canine"),deo=o(" \u2014 "),Mx=a("a"),ceo=o("CanineModel"),feo=o(" (Canine model)"),meo=l(),Sh=a("li"),uJ=a("strong"),geo=o("clip"),heo=o(" \u2014 "),Ex=a("a"),peo=o("CLIPModel"),_eo=o(" (CLIP model)"),ueo=l(),Ph=a("li"),bJ=a("strong"),beo=o("convbert"),veo=o(" \u2014 "),yx=a("a"),Teo=o("ConvBertModel"),Feo=o(" (ConvBERT model)"),Ceo=l(),$h=a("li"),vJ=a("strong"),Meo=o("convnext"),Eeo=o(" \u2014 "),wx=a("a"),yeo=o("ConvNextModel"),weo=o(" (ConvNext model)"),Aeo=l(),Ih=a("li"),TJ=a("strong"),Leo=o("ctrl"),Beo=o(" \u2014 "),Ax=a("a"),keo=o("CTRLModel"),xeo=o(" (CTRL model)"),Reo=l(),jh=a("li"),FJ=a("strong"),Seo=o("deberta"),Peo=o(" \u2014 "),Lx=a("a"),$eo=o("DebertaModel"),Ieo=o(" (DeBERTa model)"),jeo=l(),Nh=a("li"),CJ=a("strong"),Neo=o("deberta-v2"),Deo=o(" \u2014 "),Bx=a("a"),qeo=o("DebertaV2Model"),Geo=o(" (DeBERTa-v2 model)"),Oeo=l(),Dh=a("li"),MJ=a("strong"),Xeo=o("deit"),zeo=o(" \u2014 "),kx=a("a"),Veo=o("DeiTModel"),Weo=o(" (DeiT model)"),Qeo=l(),qh=a("li"),EJ=a("strong"),Heo=o("detr"),Ueo=o(" \u2014 "),xx=a("a"),Jeo=o("DetrModel"),Yeo=o(" (DETR model)"),Keo=l(),Gh=a("li"),yJ=a("strong"),Zeo=o("distilbert"),eoo=o(" \u2014 "),Rx=a("a"),ooo=o("DistilBertModel"),roo=o(" (DistilBERT model)"),too=l(),Oh=a("li"),wJ=a("strong"),aoo=o("dpr"),noo=o(" \u2014 "),Sx=a("a"),soo=o("DPRQuestionEncoder"),loo=o(" (DPR model)"),ioo=l(),Xh=a("li"),AJ=a("strong"),doo=o("electra"),coo=o(" \u2014 "),Px=a("a"),foo=o("ElectraModel"),moo=o(" (ELECTRA model)"),goo=l(),zh=a("li"),LJ=a("strong"),hoo=o("flaubert"),poo=o(" \u2014 "),$x=a("a"),_oo=o("FlaubertModel"),uoo=o(" (FlauBERT model)"),boo=l(),Vh=a("li"),BJ=a("strong"),voo=o("fnet"),Too=o(" \u2014 "),Ix=a("a"),Foo=o("FNetModel"),Coo=o(" (FNet model)"),Moo=l(),Wh=a("li"),kJ=a("strong"),Eoo=o("fsmt"),yoo=o(" \u2014 "),jx=a("a"),woo=o("FSMTModel"),Aoo=o(" (FairSeq Machine-Translation model)"),Loo=l(),xs=a("li"),xJ=a("strong"),Boo=o("funnel"),koo=o(" \u2014 "),Nx=a("a"),xoo=o("FunnelModel"),Roo=o(" or "),Dx=a("a"),Soo=o("FunnelBaseModel"),Poo=o(" (Funnel Transformer model)"),$oo=l(),Qh=a("li"),RJ=a("strong"),Ioo=o("gpt2"),joo=o(" \u2014 "),qx=a("a"),Noo=o("GPT2Model"),Doo=o(" (OpenAI GPT-2 model)"),qoo=l(),Hh=a("li"),SJ=a("strong"),Goo=o("gpt_neo"),Ooo=o(" \u2014 "),Gx=a("a"),Xoo=o("GPTNeoModel"),zoo=o(" (GPT Neo model)"),Voo=l(),Uh=a("li"),PJ=a("strong"),Woo=o("gptj"),Qoo=o(" \u2014 "),Ox=a("a"),Hoo=o("GPTJModel"),Uoo=o(" (GPT-J model)"),Joo=l(),Jh=a("li"),$J=a("strong"),Yoo=o("hubert"),Koo=o(" \u2014 "),Xx=a("a"),Zoo=o("HubertModel"),ero=o(" (Hubert model)"),oro=l(),Yh=a("li"),IJ=a("strong"),rro=o("ibert"),tro=o(" \u2014 "),zx=a("a"),aro=o("IBertModel"),nro=o(" (I-BERT model)"),sro=l(),Kh=a("li"),jJ=a("strong"),lro=o("imagegpt"),iro=o(" \u2014 "),Vx=a("a"),dro=o("ImageGPTModel"),cro=o(" (ImageGPT model)"),fro=l(),Zh=a("li"),NJ=a("strong"),mro=o("layoutlm"),gro=o(" \u2014 "),Wx=a("a"),hro=o("LayoutLMModel"),pro=o(" (LayoutLM model)"),_ro=l(),ep=a("li"),DJ=a("strong"),uro=o("layoutlmv2"),bro=o(" \u2014 "),Qx=a("a"),vro=o("LayoutLMv2Model"),Tro=o(" (LayoutLMv2 model)"),Fro=l(),op=a("li"),qJ=a("strong"),Cro=o("led"),Mro=o(" \u2014 "),Hx=a("a"),Ero=o("LEDModel"),yro=o(" (LED model)"),wro=l(),rp=a("li"),GJ=a("strong"),Aro=o("longformer"),Lro=o(" \u2014 "),Ux=a("a"),Bro=o("LongformerModel"),kro=o(" (Longformer model)"),xro=l(),tp=a("li"),OJ=a("strong"),Rro=o("luke"),Sro=o(" \u2014 "),Jx=a("a"),Pro=o("LukeModel"),$ro=o(" (LUKE model)"),Iro=l(),ap=a("li"),XJ=a("strong"),jro=o("lxmert"),Nro=o(" \u2014 "),Yx=a("a"),Dro=o("LxmertModel"),qro=o(" (LXMERT model)"),Gro=l(),np=a("li"),zJ=a("strong"),Oro=o("m2m_100"),Xro=o(" \u2014 "),Kx=a("a"),zro=o("M2M100Model"),Vro=o(" (M2M100 model)"),Wro=l(),sp=a("li"),VJ=a("strong"),Qro=o("marian"),Hro=o(" \u2014 "),Zx=a("a"),Uro=o("MarianModel"),Jro=o(" (Marian model)"),Yro=l(),lp=a("li"),WJ=a("strong"),Kro=o("mbart"),Zro=o(" \u2014 "),eR=a("a"),eto=o("MBartModel"),oto=o(" (mBART model)"),rto=l(),ip=a("li"),QJ=a("strong"),tto=o("megatron-bert"),ato=o(" \u2014 "),oR=a("a"),nto=o("MegatronBertModel"),sto=o(" (MegatronBert model)"),lto=l(),dp=a("li"),HJ=a("strong"),ito=o("mobilebert"),dto=o(" \u2014 "),rR=a("a"),cto=o("MobileBertModel"),fto=o(" (MobileBERT model)"),mto=l(),cp=a("li"),UJ=a("strong"),gto=o("mpnet"),hto=o(" \u2014 "),tR=a("a"),pto=o("MPNetModel"),_to=o(" (MPNet model)"),uto=l(),fp=a("li"),JJ=a("strong"),bto=o("mt5"),vto=o(" \u2014 "),aR=a("a"),Tto=o("MT5Model"),Fto=o(" (mT5 model)"),Cto=l(),mp=a("li"),YJ=a("strong"),Mto=o("nystromformer"),Eto=o(" \u2014 "),nR=a("a"),yto=o("NystromformerModel"),wto=o(" (Nystromformer model)"),Ato=l(),gp=a("li"),KJ=a("strong"),Lto=o("openai-gpt"),Bto=o(" \u2014 "),sR=a("a"),kto=o("OpenAIGPTModel"),xto=o(" (OpenAI GPT model)"),Rto=l(),hp=a("li"),ZJ=a("strong"),Sto=o("pegasus"),Pto=o(" \u2014 "),lR=a("a"),$to=o("PegasusModel"),Ito=o(" (Pegasus model)"),jto=l(),pp=a("li"),eY=a("strong"),Nto=o("perceiver"),Dto=o(" \u2014 "),iR=a("a"),qto=o("PerceiverModel"),Gto=o(" (Perceiver model)"),Oto=l(),_p=a("li"),oY=a("strong"),Xto=o("plbart"),zto=o(" \u2014 "),dR=a("a"),Vto=o("PLBartModel"),Wto=o(" (PLBart model)"),Qto=l(),up=a("li"),rY=a("strong"),Hto=o("poolformer"),Uto=o(" \u2014 "),cR=a("a"),Jto=o("PoolFormerModel"),Yto=o(" (PoolFormer model)"),Kto=l(),bp=a("li"),tY=a("strong"),Zto=o("prophetnet"),eao=o(" \u2014 "),fR=a("a"),oao=o("ProphetNetModel"),rao=o(" (ProphetNet model)"),tao=l(),vp=a("li"),aY=a("strong"),aao=o("qdqbert"),nao=o(" \u2014 "),mR=a("a"),sao=o("QDQBertModel"),lao=o(" (QDQBert model)"),iao=l(),Tp=a("li"),nY=a("strong"),dao=o("reformer"),cao=o(" \u2014 "),gR=a("a"),fao=o("ReformerModel"),mao=o(" (Reformer model)"),gao=l(),Fp=a("li"),sY=a("strong"),hao=o("rembert"),pao=o(" \u2014 "),hR=a("a"),_ao=o("RemBertModel"),uao=o(" (RemBERT model)"),bao=l(),Cp=a("li"),lY=a("strong"),vao=o("retribert"),Tao=o(" \u2014 "),pR=a("a"),Fao=o("RetriBertModel"),Cao=o(" (RetriBERT model)"),Mao=l(),Mp=a("li"),iY=a("strong"),Eao=o("roberta"),yao=o(" \u2014 "),_R=a("a"),wao=o("RobertaModel"),Aao=o(" (RoBERTa model)"),Lao=l(),Ep=a("li"),dY=a("strong"),Bao=o("roformer"),kao=o(" \u2014 "),uR=a("a"),xao=o("RoFormerModel"),Rao=o(" (RoFormer model)"),Sao=l(),yp=a("li"),cY=a("strong"),Pao=o("segformer"),$ao=o(" \u2014 "),bR=a("a"),Iao=o("SegformerModel"),jao=o(" (SegFormer model)"),Nao=l(),wp=a("li"),fY=a("strong"),Dao=o("sew"),qao=o(" \u2014 "),vR=a("a"),Gao=o("SEWModel"),Oao=o(" (SEW model)"),Xao=l(),Ap=a("li"),mY=a("strong"),zao=o("sew-d"),Vao=o(" \u2014 "),TR=a("a"),Wao=o("SEWDModel"),Qao=o(" (SEW-D model)"),Hao=l(),Lp=a("li"),gY=a("strong"),Uao=o("speech_to_text"),Jao=o(" \u2014 "),FR=a("a"),Yao=o("Speech2TextModel"),Kao=o(" (Speech2Text model)"),Zao=l(),Bp=a("li"),hY=a("strong"),eno=o("splinter"),ono=o(" \u2014 "),CR=a("a"),rno=o("SplinterModel"),tno=o(" (Splinter model)"),ano=l(),kp=a("li"),pY=a("strong"),nno=o("squeezebert"),sno=o(" \u2014 "),MR=a("a"),lno=o("SqueezeBertModel"),ino=o(" (SqueezeBERT model)"),dno=l(),xp=a("li"),_Y=a("strong"),cno=o("swin"),fno=o(" \u2014 "),ER=a("a"),mno=o("SwinModel"),gno=o(" (Swin model)"),hno=l(),Rp=a("li"),uY=a("strong"),pno=o("t5"),_no=o(" \u2014 "),yR=a("a"),uno=o("T5Model"),bno=o(" (T5 model)"),vno=l(),Sp=a("li"),bY=a("strong"),Tno=o("tapas"),Fno=o(" \u2014 "),wR=a("a"),Cno=o("TapasModel"),Mno=o(" (TAPAS model)"),Eno=l(),Pp=a("li"),vY=a("strong"),yno=o("transfo-xl"),wno=o(" \u2014 "),AR=a("a"),Ano=o("TransfoXLModel"),Lno=o(" (Transformer-XL model)"),Bno=l(),$p=a("li"),TY=a("strong"),kno=o("unispeech"),xno=o(" \u2014 "),LR=a("a"),Rno=o("UniSpeechModel"),Sno=o(" (UniSpeech model)"),Pno=l(),Ip=a("li"),FY=a("strong"),$no=o("unispeech-sat"),Ino=o(" \u2014 "),BR=a("a"),jno=o("UniSpeechSatModel"),Nno=o(" (UniSpeechSat model)"),Dno=l(),jp=a("li"),CY=a("strong"),qno=o("vilt"),Gno=o(" \u2014 "),kR=a("a"),Ono=o("ViltModel"),Xno=o(" (ViLT model)"),zno=l(),Np=a("li"),MY=a("strong"),Vno=o("vision-text-dual-encoder"),Wno=o(" \u2014 "),xR=a("a"),Qno=o("VisionTextDualEncoderModel"),Hno=o(" (VisionTextDualEncoder model)"),Uno=l(),Dp=a("li"),EY=a("strong"),Jno=o("visual_bert"),Yno=o(" \u2014 "),RR=a("a"),Kno=o("VisualBertModel"),Zno=o(" (VisualBert model)"),eso=l(),qp=a("li"),yY=a("strong"),oso=o("vit"),rso=o(" \u2014 "),SR=a("a"),tso=o("ViTModel"),aso=o(" (ViT model)"),nso=l(),Gp=a("li"),wY=a("strong"),sso=o("vit_mae"),lso=o(" \u2014 "),PR=a("a"),iso=o("ViTMAEModel"),dso=o(" (ViTMAE model)"),cso=l(),Op=a("li"),AY=a("strong"),fso=o("wav2vec2"),mso=o(" \u2014 "),$R=a("a"),gso=o("Wav2Vec2Model"),hso=o(" (Wav2Vec2 model)"),pso=l(),Xp=a("li"),LY=a("strong"),_so=o("wavlm"),uso=o(" \u2014 "),IR=a("a"),bso=o("WavLMModel"),vso=o(" (WavLM model)"),Tso=l(),zp=a("li"),BY=a("strong"),Fso=o("xglm"),Cso=o(" \u2014 "),jR=a("a"),Mso=o("XGLMModel"),Eso=o(" (XGLM model)"),yso=l(),Vp=a("li"),kY=a("strong"),wso=o("xlm"),Aso=o(" \u2014 "),NR=a("a"),Lso=o("XLMModel"),Bso=o(" (XLM model)"),kso=l(),Wp=a("li"),xY=a("strong"),xso=o("xlm-prophetnet"),Rso=o(" \u2014 "),DR=a("a"),Sso=o("XLMProphetNetModel"),Pso=o(" (XLMProphetNet model)"),$so=l(),Qp=a("li"),RY=a("strong"),Iso=o("xlm-roberta"),jso=o(" \u2014 "),qR=a("a"),Nso=o("XLMRobertaModel"),Dso=o(" (XLM-RoBERTa model)"),qso=l(),Hp=a("li"),SY=a("strong"),Gso=o("xlm-roberta-xl"),Oso=o(" \u2014 "),GR=a("a"),Xso=o("XLMRobertaXLModel"),zso=o(" (XLM-RoBERTa-XL model)"),Vso=l(),Up=a("li"),PY=a("strong"),Wso=o("xlnet"),Qso=o(" \u2014 "),OR=a("a"),Hso=o("XLNetModel"),Uso=o(" (XLNet model)"),Jso=l(),Jp=a("li"),$Y=a("strong"),Yso=o("yoso"),Kso=o(" \u2014 "),XR=a("a"),Zso=o("YosoModel"),elo=o(" (YOSO model)"),olo=l(),Yp=a("p"),rlo=o("The model is set in evaluation mode by default using "),IY=a("code"),tlo=o("model.eval()"),alo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),jY=a("code"),nlo=o("model.train()"),slo=l(),NY=a("p"),llo=o("Examples:"),ilo=l(),f(GM.$$.fragment),bLe=l(),Xi=a("h2"),Kp=a("a"),DY=a("span"),f(OM.$$.fragment),dlo=l(),qY=a("span"),clo=o("AutoModelForPreTraining"),vLe=l(),Wo=a("div"),f(XM.$$.fragment),flo=l(),zi=a("p"),mlo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a pretraining head) when created
with the `),GY=a("code"),glo=o("from_pretrained()"),hlo=o("class method or the "),OY=a("code"),plo=o("from_config()"),_lo=o(`class
method.`),ulo=l(),zM=a("p"),blo=o("This class cannot be instantiated directly using "),XY=a("code"),vlo=o("__init__()"),Tlo=o(" (throws an error)."),Flo=l(),Dr=a("div"),f(VM.$$.fragment),Clo=l(),zY=a("p"),Mlo=o("Instantiates one of the model classes of the library (with a pretraining head) from a configuration."),Elo=l(),Vi=a("p"),ylo=o(`Note:
Loading a model from its configuration file does `),VY=a("strong"),wlo=o("not"),Alo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),WY=a("code"),Llo=o("from_pretrained()"),Blo=o("to load the model weights."),klo=l(),QY=a("p"),xlo=o("Examples:"),Rlo=l(),f(WM.$$.fragment),Slo=l(),xe=a("div"),f(QM.$$.fragment),Plo=l(),HY=a("p"),$lo=o("Instantiate one of the model classes of the library (with a pretraining head) from a pretrained model."),Ilo=l(),qa=a("p"),jlo=o("The model class to instantiate is selected based on the "),UY=a("code"),Nlo=o("model_type"),Dlo=o(` property of the config object (either
passed as an argument or loaded from `),JY=a("code"),qlo=o("pretrained_model_name_or_path"),Glo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),YY=a("code"),Olo=o("pretrained_model_name_or_path"),Xlo=o(":"),zlo=l(),x=a("ul"),Zp=a("li"),KY=a("strong"),Vlo=o("albert"),Wlo=o(" \u2014 "),zR=a("a"),Qlo=o("AlbertForPreTraining"),Hlo=o(" (ALBERT model)"),Ulo=l(),e_=a("li"),ZY=a("strong"),Jlo=o("bart"),Ylo=o(" \u2014 "),VR=a("a"),Klo=o("BartForConditionalGeneration"),Zlo=o(" (BART model)"),eio=l(),o_=a("li"),eK=a("strong"),oio=o("bert"),rio=o(" \u2014 "),WR=a("a"),tio=o("BertForPreTraining"),aio=o(" (BERT model)"),nio=l(),r_=a("li"),oK=a("strong"),sio=o("big_bird"),lio=o(" \u2014 "),QR=a("a"),iio=o("BigBirdForPreTraining"),dio=o(" (BigBird model)"),cio=l(),t_=a("li"),rK=a("strong"),fio=o("camembert"),mio=o(" \u2014 "),HR=a("a"),gio=o("CamembertForMaskedLM"),hio=o(" (CamemBERT model)"),pio=l(),a_=a("li"),tK=a("strong"),_io=o("ctrl"),uio=o(" \u2014 "),UR=a("a"),bio=o("CTRLLMHeadModel"),vio=o(" (CTRL model)"),Tio=l(),n_=a("li"),aK=a("strong"),Fio=o("deberta"),Cio=o(" \u2014 "),JR=a("a"),Mio=o("DebertaForMaskedLM"),Eio=o(" (DeBERTa model)"),yio=l(),s_=a("li"),nK=a("strong"),wio=o("deberta-v2"),Aio=o(" \u2014 "),YR=a("a"),Lio=o("DebertaV2ForMaskedLM"),Bio=o(" (DeBERTa-v2 model)"),kio=l(),l_=a("li"),sK=a("strong"),xio=o("distilbert"),Rio=o(" \u2014 "),KR=a("a"),Sio=o("DistilBertForMaskedLM"),Pio=o(" (DistilBERT model)"),$io=l(),i_=a("li"),lK=a("strong"),Iio=o("electra"),jio=o(" \u2014 "),ZR=a("a"),Nio=o("ElectraForPreTraining"),Dio=o(" (ELECTRA model)"),qio=l(),d_=a("li"),iK=a("strong"),Gio=o("flaubert"),Oio=o(" \u2014 "),eS=a("a"),Xio=o("FlaubertWithLMHeadModel"),zio=o(" (FlauBERT model)"),Vio=l(),c_=a("li"),dK=a("strong"),Wio=o("fnet"),Qio=o(" \u2014 "),oS=a("a"),Hio=o("FNetForPreTraining"),Uio=o(" (FNet model)"),Jio=l(),f_=a("li"),cK=a("strong"),Yio=o("fsmt"),Kio=o(" \u2014 "),rS=a("a"),Zio=o("FSMTForConditionalGeneration"),edo=o(" (FairSeq Machine-Translation model)"),odo=l(),m_=a("li"),fK=a("strong"),rdo=o("funnel"),tdo=o(" \u2014 "),tS=a("a"),ado=o("FunnelForPreTraining"),ndo=o(" (Funnel Transformer model)"),sdo=l(),g_=a("li"),mK=a("strong"),ldo=o("gpt2"),ido=o(" \u2014 "),aS=a("a"),ddo=o("GPT2LMHeadModel"),cdo=o(" (OpenAI GPT-2 model)"),fdo=l(),h_=a("li"),gK=a("strong"),mdo=o("ibert"),gdo=o(" \u2014 "),nS=a("a"),hdo=o("IBertForMaskedLM"),pdo=o(" (I-BERT model)"),_do=l(),p_=a("li"),hK=a("strong"),udo=o("layoutlm"),bdo=o(" \u2014 "),sS=a("a"),vdo=o("LayoutLMForMaskedLM"),Tdo=o(" (LayoutLM model)"),Fdo=l(),__=a("li"),pK=a("strong"),Cdo=o("longformer"),Mdo=o(" \u2014 "),lS=a("a"),Edo=o("LongformerForMaskedLM"),ydo=o(" (Longformer model)"),wdo=l(),u_=a("li"),_K=a("strong"),Ado=o("lxmert"),Ldo=o(" \u2014 "),iS=a("a"),Bdo=o("LxmertForPreTraining"),kdo=o(" (LXMERT model)"),xdo=l(),b_=a("li"),uK=a("strong"),Rdo=o("megatron-bert"),Sdo=o(" \u2014 "),dS=a("a"),Pdo=o("MegatronBertForPreTraining"),$do=o(" (MegatronBert model)"),Ido=l(),v_=a("li"),bK=a("strong"),jdo=o("mobilebert"),Ndo=o(" \u2014 "),cS=a("a"),Ddo=o("MobileBertForPreTraining"),qdo=o(" (MobileBERT model)"),Gdo=l(),T_=a("li"),vK=a("strong"),Odo=o("mpnet"),Xdo=o(" \u2014 "),fS=a("a"),zdo=o("MPNetForMaskedLM"),Vdo=o(" (MPNet model)"),Wdo=l(),F_=a("li"),TK=a("strong"),Qdo=o("openai-gpt"),Hdo=o(" \u2014 "),mS=a("a"),Udo=o("OpenAIGPTLMHeadModel"),Jdo=o(" (OpenAI GPT model)"),Ydo=l(),C_=a("li"),FK=a("strong"),Kdo=o("retribert"),Zdo=o(" \u2014 "),gS=a("a"),eco=o("RetriBertModel"),oco=o(" (RetriBERT model)"),rco=l(),M_=a("li"),CK=a("strong"),tco=o("roberta"),aco=o(" \u2014 "),hS=a("a"),nco=o("RobertaForMaskedLM"),sco=o(" (RoBERTa model)"),lco=l(),E_=a("li"),MK=a("strong"),ico=o("squeezebert"),dco=o(" \u2014 "),pS=a("a"),cco=o("SqueezeBertForMaskedLM"),fco=o(" (SqueezeBERT model)"),mco=l(),y_=a("li"),EK=a("strong"),gco=o("t5"),hco=o(" \u2014 "),_S=a("a"),pco=o("T5ForConditionalGeneration"),_co=o(" (T5 model)"),uco=l(),w_=a("li"),yK=a("strong"),bco=o("tapas"),vco=o(" \u2014 "),uS=a("a"),Tco=o("TapasForMaskedLM"),Fco=o(" (TAPAS model)"),Cco=l(),A_=a("li"),wK=a("strong"),Mco=o("transfo-xl"),Eco=o(" \u2014 "),bS=a("a"),yco=o("TransfoXLLMHeadModel"),wco=o(" (Transformer-XL model)"),Aco=l(),L_=a("li"),AK=a("strong"),Lco=o("unispeech"),Bco=o(" \u2014 "),vS=a("a"),kco=o("UniSpeechForPreTraining"),xco=o(" (UniSpeech model)"),Rco=l(),B_=a("li"),LK=a("strong"),Sco=o("unispeech-sat"),Pco=o(" \u2014 "),TS=a("a"),$co=o("UniSpeechSatForPreTraining"),Ico=o(" (UniSpeechSat model)"),jco=l(),k_=a("li"),BK=a("strong"),Nco=o("visual_bert"),Dco=o(" \u2014 "),FS=a("a"),qco=o("VisualBertForPreTraining"),Gco=o(" (VisualBert model)"),Oco=l(),x_=a("li"),kK=a("strong"),Xco=o("vit_mae"),zco=o(" \u2014 "),CS=a("a"),Vco=o("ViTMAEForPreTraining"),Wco=o(" (ViTMAE model)"),Qco=l(),R_=a("li"),xK=a("strong"),Hco=o("wav2vec2"),Uco=o(" \u2014 "),MS=a("a"),Jco=o("Wav2Vec2ForPreTraining"),Yco=o(" (Wav2Vec2 model)"),Kco=l(),S_=a("li"),RK=a("strong"),Zco=o("xlm"),efo=o(" \u2014 "),ES=a("a"),ofo=o("XLMWithLMHeadModel"),rfo=o(" (XLM model)"),tfo=l(),P_=a("li"),SK=a("strong"),afo=o("xlm-roberta"),nfo=o(" \u2014 "),yS=a("a"),sfo=o("XLMRobertaForMaskedLM"),lfo=o(" (XLM-RoBERTa model)"),ifo=l(),$_=a("li"),PK=a("strong"),dfo=o("xlm-roberta-xl"),cfo=o(" \u2014 "),wS=a("a"),ffo=o("XLMRobertaXLForMaskedLM"),mfo=o(" (XLM-RoBERTa-XL model)"),gfo=l(),I_=a("li"),$K=a("strong"),hfo=o("xlnet"),pfo=o(" \u2014 "),AS=a("a"),_fo=o("XLNetLMHeadModel"),ufo=o(" (XLNet model)"),bfo=l(),j_=a("p"),vfo=o("The model is set in evaluation mode by default using "),IK=a("code"),Tfo=o("model.eval()"),Ffo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),jK=a("code"),Cfo=o("model.train()"),Mfo=l(),NK=a("p"),Efo=o("Examples:"),yfo=l(),f(HM.$$.fragment),TLe=l(),Wi=a("h2"),N_=a("a"),DK=a("span"),f(UM.$$.fragment),wfo=l(),qK=a("span"),Afo=o("AutoModelForCausalLM"),FLe=l(),Qo=a("div"),f(JM.$$.fragment),Lfo=l(),Qi=a("p"),Bfo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a causal language modeling head) when created
with the `),GK=a("code"),kfo=o("from_pretrained()"),xfo=o("class method or the "),OK=a("code"),Rfo=o("from_config()"),Sfo=o(`class
method.`),Pfo=l(),YM=a("p"),$fo=o("This class cannot be instantiated directly using "),XK=a("code"),Ifo=o("__init__()"),jfo=o(" (throws an error)."),Nfo=l(),qr=a("div"),f(KM.$$.fragment),Dfo=l(),zK=a("p"),qfo=o("Instantiates one of the model classes of the library (with a causal language modeling head) from a configuration."),Gfo=l(),Hi=a("p"),Ofo=o(`Note:
Loading a model from its configuration file does `),VK=a("strong"),Xfo=o("not"),zfo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),WK=a("code"),Vfo=o("from_pretrained()"),Wfo=o("to load the model weights."),Qfo=l(),QK=a("p"),Hfo=o("Examples:"),Ufo=l(),f(ZM.$$.fragment),Jfo=l(),Re=a("div"),f(eE.$$.fragment),Yfo=l(),HK=a("p"),Kfo=o("Instantiate one of the model classes of the library (with a causal language modeling head) from a pretrained model."),Zfo=l(),Ga=a("p"),emo=o("The model class to instantiate is selected based on the "),UK=a("code"),omo=o("model_type"),rmo=o(` property of the config object (either
passed as an argument or loaded from `),JK=a("code"),tmo=o("pretrained_model_name_or_path"),amo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),YK=a("code"),nmo=o("pretrained_model_name_or_path"),smo=o(":"),lmo=l(),$=a("ul"),D_=a("li"),KK=a("strong"),imo=o("bart"),dmo=o(" \u2014 "),LS=a("a"),cmo=o("BartForCausalLM"),fmo=o(" (BART model)"),mmo=l(),q_=a("li"),ZK=a("strong"),gmo=o("bert"),hmo=o(" \u2014 "),BS=a("a"),pmo=o("BertLMHeadModel"),_mo=o(" (BERT model)"),umo=l(),G_=a("li"),eZ=a("strong"),bmo=o("bert-generation"),vmo=o(" \u2014 "),kS=a("a"),Tmo=o("BertGenerationDecoder"),Fmo=o(" (Bert Generation model)"),Cmo=l(),O_=a("li"),oZ=a("strong"),Mmo=o("big_bird"),Emo=o(" \u2014 "),xS=a("a"),ymo=o("BigBirdForCausalLM"),wmo=o(" (BigBird model)"),Amo=l(),X_=a("li"),rZ=a("strong"),Lmo=o("bigbird_pegasus"),Bmo=o(" \u2014 "),RS=a("a"),kmo=o("BigBirdPegasusForCausalLM"),xmo=o(" (BigBirdPegasus model)"),Rmo=l(),z_=a("li"),tZ=a("strong"),Smo=o("blenderbot"),Pmo=o(" \u2014 "),SS=a("a"),$mo=o("BlenderbotForCausalLM"),Imo=o(" (Blenderbot model)"),jmo=l(),V_=a("li"),aZ=a("strong"),Nmo=o("blenderbot-small"),Dmo=o(" \u2014 "),PS=a("a"),qmo=o("BlenderbotSmallForCausalLM"),Gmo=o(" (BlenderbotSmall model)"),Omo=l(),W_=a("li"),nZ=a("strong"),Xmo=o("camembert"),zmo=o(" \u2014 "),$S=a("a"),Vmo=o("CamembertForCausalLM"),Wmo=o(" (CamemBERT model)"),Qmo=l(),Q_=a("li"),sZ=a("strong"),Hmo=o("ctrl"),Umo=o(" \u2014 "),IS=a("a"),Jmo=o("CTRLLMHeadModel"),Ymo=o(" (CTRL model)"),Kmo=l(),H_=a("li"),lZ=a("strong"),Zmo=o("electra"),ego=o(" \u2014 "),jS=a("a"),ogo=o("ElectraForCausalLM"),rgo=o(" (ELECTRA model)"),tgo=l(),U_=a("li"),iZ=a("strong"),ago=o("gpt2"),ngo=o(" \u2014 "),NS=a("a"),sgo=o("GPT2LMHeadModel"),lgo=o(" (OpenAI GPT-2 model)"),igo=l(),J_=a("li"),dZ=a("strong"),dgo=o("gpt_neo"),cgo=o(" \u2014 "),DS=a("a"),fgo=o("GPTNeoForCausalLM"),mgo=o(" (GPT Neo model)"),ggo=l(),Y_=a("li"),cZ=a("strong"),hgo=o("gptj"),pgo=o(" \u2014 "),qS=a("a"),_go=o("GPTJForCausalLM"),ugo=o(" (GPT-J model)"),bgo=l(),K_=a("li"),fZ=a("strong"),vgo=o("marian"),Tgo=o(" \u2014 "),GS=a("a"),Fgo=o("MarianForCausalLM"),Cgo=o(" (Marian model)"),Mgo=l(),Z_=a("li"),mZ=a("strong"),Ego=o("mbart"),ygo=o(" \u2014 "),OS=a("a"),wgo=o("MBartForCausalLM"),Ago=o(" (mBART model)"),Lgo=l(),eu=a("li"),gZ=a("strong"),Bgo=o("megatron-bert"),kgo=o(" \u2014 "),XS=a("a"),xgo=o("MegatronBertForCausalLM"),Rgo=o(" (MegatronBert model)"),Sgo=l(),ou=a("li"),hZ=a("strong"),Pgo=o("openai-gpt"),$go=o(" \u2014 "),zS=a("a"),Igo=o("OpenAIGPTLMHeadModel"),jgo=o(" (OpenAI GPT model)"),Ngo=l(),ru=a("li"),pZ=a("strong"),Dgo=o("pegasus"),qgo=o(" \u2014 "),VS=a("a"),Ggo=o("PegasusForCausalLM"),Ogo=o(" (Pegasus model)"),Xgo=l(),tu=a("li"),_Z=a("strong"),zgo=o("plbart"),Vgo=o(" \u2014 "),WS=a("a"),Wgo=o("PLBartForCausalLM"),Qgo=o(" (PLBart model)"),Hgo=l(),au=a("li"),uZ=a("strong"),Ugo=o("prophetnet"),Jgo=o(" \u2014 "),QS=a("a"),Ygo=o("ProphetNetForCausalLM"),Kgo=o(" (ProphetNet model)"),Zgo=l(),nu=a("li"),bZ=a("strong"),eho=o("qdqbert"),oho=o(" \u2014 "),HS=a("a"),rho=o("QDQBertLMHeadModel"),tho=o(" (QDQBert model)"),aho=l(),su=a("li"),vZ=a("strong"),nho=o("reformer"),sho=o(" \u2014 "),US=a("a"),lho=o("ReformerModelWithLMHead"),iho=o(" (Reformer model)"),dho=l(),lu=a("li"),TZ=a("strong"),cho=o("rembert"),fho=o(" \u2014 "),JS=a("a"),mho=o("RemBertForCausalLM"),gho=o(" (RemBERT model)"),hho=l(),iu=a("li"),FZ=a("strong"),pho=o("roberta"),_ho=o(" \u2014 "),YS=a("a"),uho=o("RobertaForCausalLM"),bho=o(" (RoBERTa model)"),vho=l(),du=a("li"),CZ=a("strong"),Tho=o("roformer"),Fho=o(" \u2014 "),KS=a("a"),Cho=o("RoFormerForCausalLM"),Mho=o(" (RoFormer model)"),Eho=l(),cu=a("li"),MZ=a("strong"),yho=o("speech_to_text_2"),who=o(" \u2014 "),ZS=a("a"),Aho=o("Speech2Text2ForCausalLM"),Lho=o(" (Speech2Text2 model)"),Bho=l(),fu=a("li"),EZ=a("strong"),kho=o("transfo-xl"),xho=o(" \u2014 "),eP=a("a"),Rho=o("TransfoXLLMHeadModel"),Sho=o(" (Transformer-XL model)"),Pho=l(),mu=a("li"),yZ=a("strong"),$ho=o("trocr"),Iho=o(" \u2014 "),oP=a("a"),jho=o("TrOCRForCausalLM"),Nho=o(" (TrOCR model)"),Dho=l(),gu=a("li"),wZ=a("strong"),qho=o("xglm"),Gho=o(" \u2014 "),rP=a("a"),Oho=o("XGLMForCausalLM"),Xho=o(" (XGLM model)"),zho=l(),hu=a("li"),AZ=a("strong"),Vho=o("xlm"),Who=o(" \u2014 "),tP=a("a"),Qho=o("XLMWithLMHeadModel"),Hho=o(" (XLM model)"),Uho=l(),pu=a("li"),LZ=a("strong"),Jho=o("xlm-prophetnet"),Yho=o(" \u2014 "),aP=a("a"),Kho=o("XLMProphetNetForCausalLM"),Zho=o(" (XLMProphetNet model)"),epo=l(),_u=a("li"),BZ=a("strong"),opo=o("xlm-roberta"),rpo=o(" \u2014 "),nP=a("a"),tpo=o("XLMRobertaForCausalLM"),apo=o(" (XLM-RoBERTa model)"),npo=l(),uu=a("li"),kZ=a("strong"),spo=o("xlm-roberta-xl"),lpo=o(" \u2014 "),sP=a("a"),ipo=o("XLMRobertaXLForCausalLM"),dpo=o(" (XLM-RoBERTa-XL model)"),cpo=l(),bu=a("li"),xZ=a("strong"),fpo=o("xlnet"),mpo=o(" \u2014 "),lP=a("a"),gpo=o("XLNetLMHeadModel"),hpo=o(" (XLNet model)"),ppo=l(),vu=a("p"),_po=o("The model is set in evaluation mode by default using "),RZ=a("code"),upo=o("model.eval()"),bpo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),SZ=a("code"),vpo=o("model.train()"),Tpo=l(),PZ=a("p"),Fpo=o("Examples:"),Cpo=l(),f(oE.$$.fragment),CLe=l(),Ui=a("h2"),Tu=a("a"),$Z=a("span"),f(rE.$$.fragment),Mpo=l(),IZ=a("span"),Epo=o("AutoModelForMaskedLM"),MLe=l(),Ho=a("div"),f(tE.$$.fragment),ypo=l(),Ji=a("p"),wpo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a masked language modeling head) when created
with the `),jZ=a("code"),Apo=o("from_pretrained()"),Lpo=o("class method or the "),NZ=a("code"),Bpo=o("from_config()"),kpo=o(`class
method.`),xpo=l(),aE=a("p"),Rpo=o("This class cannot be instantiated directly using "),DZ=a("code"),Spo=o("__init__()"),Ppo=o(" (throws an error)."),$po=l(),Gr=a("div"),f(nE.$$.fragment),Ipo=l(),qZ=a("p"),jpo=o("Instantiates one of the model classes of the library (with a masked language modeling head) from a configuration."),Npo=l(),Yi=a("p"),Dpo=o(`Note:
Loading a model from its configuration file does `),GZ=a("strong"),qpo=o("not"),Gpo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),OZ=a("code"),Opo=o("from_pretrained()"),Xpo=o("to load the model weights."),zpo=l(),XZ=a("p"),Vpo=o("Examples:"),Wpo=l(),f(sE.$$.fragment),Qpo=l(),Se=a("div"),f(lE.$$.fragment),Hpo=l(),zZ=a("p"),Upo=o("Instantiate one of the model classes of the library (with a masked language modeling head) from a pretrained model."),Jpo=l(),Oa=a("p"),Ypo=o("The model class to instantiate is selected based on the "),VZ=a("code"),Kpo=o("model_type"),Zpo=o(` property of the config object (either
passed as an argument or loaded from `),WZ=a("code"),e_o=o("pretrained_model_name_or_path"),o_o=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),QZ=a("code"),r_o=o("pretrained_model_name_or_path"),t_o=o(":"),a_o=l(),I=a("ul"),Fu=a("li"),HZ=a("strong"),n_o=o("albert"),s_o=o(" \u2014 "),iP=a("a"),l_o=o("AlbertForMaskedLM"),i_o=o(" (ALBERT model)"),d_o=l(),Cu=a("li"),UZ=a("strong"),c_o=o("bart"),f_o=o(" \u2014 "),dP=a("a"),m_o=o("BartForConditionalGeneration"),g_o=o(" (BART model)"),h_o=l(),Mu=a("li"),JZ=a("strong"),p_o=o("bert"),__o=o(" \u2014 "),cP=a("a"),u_o=o("BertForMaskedLM"),b_o=o(" (BERT model)"),v_o=l(),Eu=a("li"),YZ=a("strong"),T_o=o("big_bird"),F_o=o(" \u2014 "),fP=a("a"),C_o=o("BigBirdForMaskedLM"),M_o=o(" (BigBird model)"),E_o=l(),yu=a("li"),KZ=a("strong"),y_o=o("camembert"),w_o=o(" \u2014 "),mP=a("a"),A_o=o("CamembertForMaskedLM"),L_o=o(" (CamemBERT model)"),B_o=l(),wu=a("li"),ZZ=a("strong"),k_o=o("convbert"),x_o=o(" \u2014 "),gP=a("a"),R_o=o("ConvBertForMaskedLM"),S_o=o(" (ConvBERT model)"),P_o=l(),Au=a("li"),eee=a("strong"),$_o=o("deberta"),I_o=o(" \u2014 "),hP=a("a"),j_o=o("DebertaForMaskedLM"),N_o=o(" (DeBERTa model)"),D_o=l(),Lu=a("li"),oee=a("strong"),q_o=o("deberta-v2"),G_o=o(" \u2014 "),pP=a("a"),O_o=o("DebertaV2ForMaskedLM"),X_o=o(" (DeBERTa-v2 model)"),z_o=l(),Bu=a("li"),ree=a("strong"),V_o=o("distilbert"),W_o=o(" \u2014 "),_P=a("a"),Q_o=o("DistilBertForMaskedLM"),H_o=o(" (DistilBERT model)"),U_o=l(),ku=a("li"),tee=a("strong"),J_o=o("electra"),Y_o=o(" \u2014 "),uP=a("a"),K_o=o("ElectraForMaskedLM"),Z_o=o(" (ELECTRA model)"),euo=l(),xu=a("li"),aee=a("strong"),ouo=o("flaubert"),ruo=o(" \u2014 "),bP=a("a"),tuo=o("FlaubertWithLMHeadModel"),auo=o(" (FlauBERT model)"),nuo=l(),Ru=a("li"),nee=a("strong"),suo=o("fnet"),luo=o(" \u2014 "),vP=a("a"),iuo=o("FNetForMaskedLM"),duo=o(" (FNet model)"),cuo=l(),Su=a("li"),see=a("strong"),fuo=o("funnel"),muo=o(" \u2014 "),TP=a("a"),guo=o("FunnelForMaskedLM"),huo=o(" (Funnel Transformer model)"),puo=l(),Pu=a("li"),lee=a("strong"),_uo=o("ibert"),uuo=o(" \u2014 "),FP=a("a"),buo=o("IBertForMaskedLM"),vuo=o(" (I-BERT model)"),Tuo=l(),$u=a("li"),iee=a("strong"),Fuo=o("layoutlm"),Cuo=o(" \u2014 "),CP=a("a"),Muo=o("LayoutLMForMaskedLM"),Euo=o(" (LayoutLM model)"),yuo=l(),Iu=a("li"),dee=a("strong"),wuo=o("longformer"),Auo=o(" \u2014 "),MP=a("a"),Luo=o("LongformerForMaskedLM"),Buo=o(" (Longformer model)"),kuo=l(),ju=a("li"),cee=a("strong"),xuo=o("mbart"),Ruo=o(" \u2014 "),EP=a("a"),Suo=o("MBartForConditionalGeneration"),Puo=o(" (mBART model)"),$uo=l(),Nu=a("li"),fee=a("strong"),Iuo=o("megatron-bert"),juo=o(" \u2014 "),yP=a("a"),Nuo=o("MegatronBertForMaskedLM"),Duo=o(" (MegatronBert model)"),quo=l(),Du=a("li"),mee=a("strong"),Guo=o("mobilebert"),Ouo=o(" \u2014 "),wP=a("a"),Xuo=o("MobileBertForMaskedLM"),zuo=o(" (MobileBERT model)"),Vuo=l(),qu=a("li"),gee=a("strong"),Wuo=o("mpnet"),Quo=o(" \u2014 "),AP=a("a"),Huo=o("MPNetForMaskedLM"),Uuo=o(" (MPNet model)"),Juo=l(),Gu=a("li"),hee=a("strong"),Yuo=o("nystromformer"),Kuo=o(" \u2014 "),LP=a("a"),Zuo=o("NystromformerForMaskedLM"),e2o=o(" (Nystromformer model)"),o2o=l(),Ou=a("li"),pee=a("strong"),r2o=o("perceiver"),t2o=o(" \u2014 "),BP=a("a"),a2o=o("PerceiverForMaskedLM"),n2o=o(" (Perceiver model)"),s2o=l(),Xu=a("li"),_ee=a("strong"),l2o=o("qdqbert"),i2o=o(" \u2014 "),kP=a("a"),d2o=o("QDQBertForMaskedLM"),c2o=o(" (QDQBert model)"),f2o=l(),zu=a("li"),uee=a("strong"),m2o=o("reformer"),g2o=o(" \u2014 "),xP=a("a"),h2o=o("ReformerForMaskedLM"),p2o=o(" (Reformer model)"),_2o=l(),Vu=a("li"),bee=a("strong"),u2o=o("rembert"),b2o=o(" \u2014 "),RP=a("a"),v2o=o("RemBertForMaskedLM"),T2o=o(" (RemBERT model)"),F2o=l(),Wu=a("li"),vee=a("strong"),C2o=o("roberta"),M2o=o(" \u2014 "),SP=a("a"),E2o=o("RobertaForMaskedLM"),y2o=o(" (RoBERTa model)"),w2o=l(),Qu=a("li"),Tee=a("strong"),A2o=o("roformer"),L2o=o(" \u2014 "),PP=a("a"),B2o=o("RoFormerForMaskedLM"),k2o=o(" (RoFormer model)"),x2o=l(),Hu=a("li"),Fee=a("strong"),R2o=o("squeezebert"),S2o=o(" \u2014 "),$P=a("a"),P2o=o("SqueezeBertForMaskedLM"),$2o=o(" (SqueezeBERT model)"),I2o=l(),Uu=a("li"),Cee=a("strong"),j2o=o("tapas"),N2o=o(" \u2014 "),IP=a("a"),D2o=o("TapasForMaskedLM"),q2o=o(" (TAPAS model)"),G2o=l(),Ju=a("li"),Mee=a("strong"),O2o=o("wav2vec2"),X2o=o(" \u2014 "),Eee=a("code"),z2o=o("Wav2Vec2ForMaskedLM"),V2o=o("(Wav2Vec2 model)"),W2o=l(),Yu=a("li"),yee=a("strong"),Q2o=o("xlm"),H2o=o(" \u2014 "),jP=a("a"),U2o=o("XLMWithLMHeadModel"),J2o=o(" (XLM model)"),Y2o=l(),Ku=a("li"),wee=a("strong"),K2o=o("xlm-roberta"),Z2o=o(" \u2014 "),NP=a("a"),e1o=o("XLMRobertaForMaskedLM"),o1o=o(" (XLM-RoBERTa model)"),r1o=l(),Zu=a("li"),Aee=a("strong"),t1o=o("xlm-roberta-xl"),a1o=o(" \u2014 "),DP=a("a"),n1o=o("XLMRobertaXLForMaskedLM"),s1o=o(" (XLM-RoBERTa-XL model)"),l1o=l(),e2=a("li"),Lee=a("strong"),i1o=o("yoso"),d1o=o(" \u2014 "),qP=a("a"),c1o=o("YosoForMaskedLM"),f1o=o(" (YOSO model)"),m1o=l(),o2=a("p"),g1o=o("The model is set in evaluation mode by default using "),Bee=a("code"),h1o=o("model.eval()"),p1o=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),kee=a("code"),_1o=o("model.train()"),u1o=l(),xee=a("p"),b1o=o("Examples:"),v1o=l(),f(iE.$$.fragment),ELe=l(),Ki=a("h2"),r2=a("a"),Ree=a("span"),f(dE.$$.fragment),T1o=l(),See=a("span"),F1o=o("AutoModelForSeq2SeqLM"),yLe=l(),Uo=a("div"),f(cE.$$.fragment),C1o=l(),Zi=a("p"),M1o=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence language modeling head) when created
with the `),Pee=a("code"),E1o=o("from_pretrained()"),y1o=o("class method or the "),$ee=a("code"),w1o=o("from_config()"),A1o=o(`class
method.`),L1o=l(),fE=a("p"),B1o=o("This class cannot be instantiated directly using "),Iee=a("code"),k1o=o("__init__()"),x1o=o(" (throws an error)."),R1o=l(),Or=a("div"),f(mE.$$.fragment),S1o=l(),jee=a("p"),P1o=o("Instantiates one of the model classes of the library (with a sequence-to-sequence language modeling head) from a configuration."),$1o=l(),ed=a("p"),I1o=o(`Note:
Loading a model from its configuration file does `),Nee=a("strong"),j1o=o("not"),N1o=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Dee=a("code"),D1o=o("from_pretrained()"),q1o=o("to load the model weights."),G1o=l(),qee=a("p"),O1o=o("Examples:"),X1o=l(),f(gE.$$.fragment),z1o=l(),Pe=a("div"),f(hE.$$.fragment),V1o=l(),Gee=a("p"),W1o=o("Instantiate one of the model classes of the library (with a sequence-to-sequence language modeling head) from a pretrained model."),Q1o=l(),Xa=a("p"),H1o=o("The model class to instantiate is selected based on the "),Oee=a("code"),U1o=o("model_type"),J1o=o(` property of the config object (either
passed as an argument or loaded from `),Xee=a("code"),Y1o=o("pretrained_model_name_or_path"),K1o=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),zee=a("code"),Z1o=o("pretrained_model_name_or_path"),ebo=o(":"),obo=l(),ae=a("ul"),t2=a("li"),Vee=a("strong"),rbo=o("bart"),tbo=o(" \u2014 "),GP=a("a"),abo=o("BartForConditionalGeneration"),nbo=o(" (BART model)"),sbo=l(),a2=a("li"),Wee=a("strong"),lbo=o("bigbird_pegasus"),ibo=o(" \u2014 "),OP=a("a"),dbo=o("BigBirdPegasusForConditionalGeneration"),cbo=o(" (BigBirdPegasus model)"),fbo=l(),n2=a("li"),Qee=a("strong"),mbo=o("blenderbot"),gbo=o(" \u2014 "),XP=a("a"),hbo=o("BlenderbotForConditionalGeneration"),pbo=o(" (Blenderbot model)"),_bo=l(),s2=a("li"),Hee=a("strong"),ubo=o("blenderbot-small"),bbo=o(" \u2014 "),zP=a("a"),vbo=o("BlenderbotSmallForConditionalGeneration"),Tbo=o(" (BlenderbotSmall model)"),Fbo=l(),l2=a("li"),Uee=a("strong"),Cbo=o("encoder-decoder"),Mbo=o(" \u2014 "),VP=a("a"),Ebo=o("EncoderDecoderModel"),ybo=o(" (Encoder decoder model)"),wbo=l(),i2=a("li"),Jee=a("strong"),Abo=o("fsmt"),Lbo=o(" \u2014 "),WP=a("a"),Bbo=o("FSMTForConditionalGeneration"),kbo=o(" (FairSeq Machine-Translation model)"),xbo=l(),d2=a("li"),Yee=a("strong"),Rbo=o("led"),Sbo=o(" \u2014 "),QP=a("a"),Pbo=o("LEDForConditionalGeneration"),$bo=o(" (LED model)"),Ibo=l(),c2=a("li"),Kee=a("strong"),jbo=o("m2m_100"),Nbo=o(" \u2014 "),HP=a("a"),Dbo=o("M2M100ForConditionalGeneration"),qbo=o(" (M2M100 model)"),Gbo=l(),f2=a("li"),Zee=a("strong"),Obo=o("marian"),Xbo=o(" \u2014 "),UP=a("a"),zbo=o("MarianMTModel"),Vbo=o(" (Marian model)"),Wbo=l(),m2=a("li"),eoe=a("strong"),Qbo=o("mbart"),Hbo=o(" \u2014 "),JP=a("a"),Ubo=o("MBartForConditionalGeneration"),Jbo=o(" (mBART model)"),Ybo=l(),g2=a("li"),ooe=a("strong"),Kbo=o("mt5"),Zbo=o(" \u2014 "),YP=a("a"),e5o=o("MT5ForConditionalGeneration"),o5o=o(" (mT5 model)"),r5o=l(),h2=a("li"),roe=a("strong"),t5o=o("pegasus"),a5o=o(" \u2014 "),KP=a("a"),n5o=o("PegasusForConditionalGeneration"),s5o=o(" (Pegasus model)"),l5o=l(),p2=a("li"),toe=a("strong"),i5o=o("plbart"),d5o=o(" \u2014 "),ZP=a("a"),c5o=o("PLBartForConditionalGeneration"),f5o=o(" (PLBart model)"),m5o=l(),_2=a("li"),aoe=a("strong"),g5o=o("prophetnet"),h5o=o(" \u2014 "),e$=a("a"),p5o=o("ProphetNetForConditionalGeneration"),_5o=o(" (ProphetNet model)"),u5o=l(),u2=a("li"),noe=a("strong"),b5o=o("t5"),v5o=o(" \u2014 "),o$=a("a"),T5o=o("T5ForConditionalGeneration"),F5o=o(" (T5 model)"),C5o=l(),b2=a("li"),soe=a("strong"),M5o=o("xlm-prophetnet"),E5o=o(" \u2014 "),r$=a("a"),y5o=o("XLMProphetNetForConditionalGeneration"),w5o=o(" (XLMProphetNet model)"),A5o=l(),v2=a("p"),L5o=o("The model is set in evaluation mode by default using "),loe=a("code"),B5o=o("model.eval()"),k5o=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),ioe=a("code"),x5o=o("model.train()"),R5o=l(),doe=a("p"),S5o=o("Examples:"),P5o=l(),f(pE.$$.fragment),wLe=l(),od=a("h2"),T2=a("a"),coe=a("span"),f(_E.$$.fragment),$5o=l(),foe=a("span"),I5o=o("AutoModelForSequenceClassification"),ALe=l(),Jo=a("div"),f(uE.$$.fragment),j5o=l(),rd=a("p"),N5o=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence classification head) when created
with the `),moe=a("code"),D5o=o("from_pretrained()"),q5o=o("class method or the "),goe=a("code"),G5o=o("from_config()"),O5o=o(`class
method.`),X5o=l(),bE=a("p"),z5o=o("This class cannot be instantiated directly using "),hoe=a("code"),V5o=o("__init__()"),W5o=o(" (throws an error)."),Q5o=l(),Xr=a("div"),f(vE.$$.fragment),H5o=l(),poe=a("p"),U5o=o("Instantiates one of the model classes of the library (with a sequence classification head) from a configuration."),J5o=l(),td=a("p"),Y5o=o(`Note:
Loading a model from its configuration file does `),_oe=a("strong"),K5o=o("not"),Z5o=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),uoe=a("code"),evo=o("from_pretrained()"),ovo=o("to load the model weights."),rvo=l(),boe=a("p"),tvo=o("Examples:"),avo=l(),f(TE.$$.fragment),nvo=l(),$e=a("div"),f(FE.$$.fragment),svo=l(),voe=a("p"),lvo=o("Instantiate one of the model classes of the library (with a sequence classification head) from a pretrained model."),ivo=l(),za=a("p"),dvo=o("The model class to instantiate is selected based on the "),Toe=a("code"),cvo=o("model_type"),fvo=o(` property of the config object (either
passed as an argument or loaded from `),Foe=a("code"),mvo=o("pretrained_model_name_or_path"),gvo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Coe=a("code"),hvo=o("pretrained_model_name_or_path"),pvo=o(":"),_vo=l(),A=a("ul"),F2=a("li"),Moe=a("strong"),uvo=o("albert"),bvo=o(" \u2014 "),t$=a("a"),vvo=o("AlbertForSequenceClassification"),Tvo=o(" (ALBERT model)"),Fvo=l(),C2=a("li"),Eoe=a("strong"),Cvo=o("bart"),Mvo=o(" \u2014 "),a$=a("a"),Evo=o("BartForSequenceClassification"),yvo=o(" (BART model)"),wvo=l(),M2=a("li"),yoe=a("strong"),Avo=o("bert"),Lvo=o(" \u2014 "),n$=a("a"),Bvo=o("BertForSequenceClassification"),kvo=o(" (BERT model)"),xvo=l(),E2=a("li"),woe=a("strong"),Rvo=o("big_bird"),Svo=o(" \u2014 "),s$=a("a"),Pvo=o("BigBirdForSequenceClassification"),$vo=o(" (BigBird model)"),Ivo=l(),y2=a("li"),Aoe=a("strong"),jvo=o("bigbird_pegasus"),Nvo=o(" \u2014 "),l$=a("a"),Dvo=o("BigBirdPegasusForSequenceClassification"),qvo=o(" (BigBirdPegasus model)"),Gvo=l(),w2=a("li"),Loe=a("strong"),Ovo=o("camembert"),Xvo=o(" \u2014 "),i$=a("a"),zvo=o("CamembertForSequenceClassification"),Vvo=o(" (CamemBERT model)"),Wvo=l(),A2=a("li"),Boe=a("strong"),Qvo=o("canine"),Hvo=o(" \u2014 "),d$=a("a"),Uvo=o("CanineForSequenceClassification"),Jvo=o(" (Canine model)"),Yvo=l(),L2=a("li"),koe=a("strong"),Kvo=o("convbert"),Zvo=o(" \u2014 "),c$=a("a"),eTo=o("ConvBertForSequenceClassification"),oTo=o(" (ConvBERT model)"),rTo=l(),B2=a("li"),xoe=a("strong"),tTo=o("ctrl"),aTo=o(" \u2014 "),f$=a("a"),nTo=o("CTRLForSequenceClassification"),sTo=o(" (CTRL model)"),lTo=l(),k2=a("li"),Roe=a("strong"),iTo=o("deberta"),dTo=o(" \u2014 "),m$=a("a"),cTo=o("DebertaForSequenceClassification"),fTo=o(" (DeBERTa model)"),mTo=l(),x2=a("li"),Soe=a("strong"),gTo=o("deberta-v2"),hTo=o(" \u2014 "),g$=a("a"),pTo=o("DebertaV2ForSequenceClassification"),_To=o(" (DeBERTa-v2 model)"),uTo=l(),R2=a("li"),Poe=a("strong"),bTo=o("distilbert"),vTo=o(" \u2014 "),h$=a("a"),TTo=o("DistilBertForSequenceClassification"),FTo=o(" (DistilBERT model)"),CTo=l(),S2=a("li"),$oe=a("strong"),MTo=o("electra"),ETo=o(" \u2014 "),p$=a("a"),yTo=o("ElectraForSequenceClassification"),wTo=o(" (ELECTRA model)"),ATo=l(),P2=a("li"),Ioe=a("strong"),LTo=o("flaubert"),BTo=o(" \u2014 "),_$=a("a"),kTo=o("FlaubertForSequenceClassification"),xTo=o(" (FlauBERT model)"),RTo=l(),$2=a("li"),joe=a("strong"),STo=o("fnet"),PTo=o(" \u2014 "),u$=a("a"),$To=o("FNetForSequenceClassification"),ITo=o(" (FNet model)"),jTo=l(),I2=a("li"),Noe=a("strong"),NTo=o("funnel"),DTo=o(" \u2014 "),b$=a("a"),qTo=o("FunnelForSequenceClassification"),GTo=o(" (Funnel Transformer model)"),OTo=l(),j2=a("li"),Doe=a("strong"),XTo=o("gpt2"),zTo=o(" \u2014 "),v$=a("a"),VTo=o("GPT2ForSequenceClassification"),WTo=o(" (OpenAI GPT-2 model)"),QTo=l(),N2=a("li"),qoe=a("strong"),HTo=o("gpt_neo"),UTo=o(" \u2014 "),T$=a("a"),JTo=o("GPTNeoForSequenceClassification"),YTo=o(" (GPT Neo model)"),KTo=l(),D2=a("li"),Goe=a("strong"),ZTo=o("gptj"),e7o=o(" \u2014 "),F$=a("a"),o7o=o("GPTJForSequenceClassification"),r7o=o(" (GPT-J model)"),t7o=l(),q2=a("li"),Ooe=a("strong"),a7o=o("ibert"),n7o=o(" \u2014 "),C$=a("a"),s7o=o("IBertForSequenceClassification"),l7o=o(" (I-BERT model)"),i7o=l(),G2=a("li"),Xoe=a("strong"),d7o=o("layoutlm"),c7o=o(" \u2014 "),M$=a("a"),f7o=o("LayoutLMForSequenceClassification"),m7o=o(" (LayoutLM model)"),g7o=l(),O2=a("li"),zoe=a("strong"),h7o=o("layoutlmv2"),p7o=o(" \u2014 "),E$=a("a"),_7o=o("LayoutLMv2ForSequenceClassification"),u7o=o(" (LayoutLMv2 model)"),b7o=l(),X2=a("li"),Voe=a("strong"),v7o=o("led"),T7o=o(" \u2014 "),y$=a("a"),F7o=o("LEDForSequenceClassification"),C7o=o(" (LED model)"),M7o=l(),z2=a("li"),Woe=a("strong"),E7o=o("longformer"),y7o=o(" \u2014 "),w$=a("a"),w7o=o("LongformerForSequenceClassification"),A7o=o(" (Longformer model)"),L7o=l(),V2=a("li"),Qoe=a("strong"),B7o=o("mbart"),k7o=o(" \u2014 "),A$=a("a"),x7o=o("MBartForSequenceClassification"),R7o=o(" (mBART model)"),S7o=l(),W2=a("li"),Hoe=a("strong"),P7o=o("megatron-bert"),$7o=o(" \u2014 "),L$=a("a"),I7o=o("MegatronBertForSequenceClassification"),j7o=o(" (MegatronBert model)"),N7o=l(),Q2=a("li"),Uoe=a("strong"),D7o=o("mobilebert"),q7o=o(" \u2014 "),B$=a("a"),G7o=o("MobileBertForSequenceClassification"),O7o=o(" (MobileBERT model)"),X7o=l(),H2=a("li"),Joe=a("strong"),z7o=o("mpnet"),V7o=o(" \u2014 "),k$=a("a"),W7o=o("MPNetForSequenceClassification"),Q7o=o(" (MPNet model)"),H7o=l(),U2=a("li"),Yoe=a("strong"),U7o=o("nystromformer"),J7o=o(" \u2014 "),x$=a("a"),Y7o=o("NystromformerForSequenceClassification"),K7o=o(" (Nystromformer model)"),Z7o=l(),J2=a("li"),Koe=a("strong"),eFo=o("openai-gpt"),oFo=o(" \u2014 "),R$=a("a"),rFo=o("OpenAIGPTForSequenceClassification"),tFo=o(" (OpenAI GPT model)"),aFo=l(),Y2=a("li"),Zoe=a("strong"),nFo=o("perceiver"),sFo=o(" \u2014 "),S$=a("a"),lFo=o("PerceiverForSequenceClassification"),iFo=o(" (Perceiver model)"),dFo=l(),K2=a("li"),ere=a("strong"),cFo=o("plbart"),fFo=o(" \u2014 "),P$=a("a"),mFo=o("PLBartForSequenceClassification"),gFo=o(" (PLBart model)"),hFo=l(),Z2=a("li"),ore=a("strong"),pFo=o("qdqbert"),_Fo=o(" \u2014 "),$$=a("a"),uFo=o("QDQBertForSequenceClassification"),bFo=o(" (QDQBert model)"),vFo=l(),e1=a("li"),rre=a("strong"),TFo=o("reformer"),FFo=o(" \u2014 "),I$=a("a"),CFo=o("ReformerForSequenceClassification"),MFo=o(" (Reformer model)"),EFo=l(),o1=a("li"),tre=a("strong"),yFo=o("rembert"),wFo=o(" \u2014 "),j$=a("a"),AFo=o("RemBertForSequenceClassification"),LFo=o(" (RemBERT model)"),BFo=l(),r1=a("li"),are=a("strong"),kFo=o("roberta"),xFo=o(" \u2014 "),N$=a("a"),RFo=o("RobertaForSequenceClassification"),SFo=o(" (RoBERTa model)"),PFo=l(),t1=a("li"),nre=a("strong"),$Fo=o("roformer"),IFo=o(" \u2014 "),D$=a("a"),jFo=o("RoFormerForSequenceClassification"),NFo=o(" (RoFormer model)"),DFo=l(),a1=a("li"),sre=a("strong"),qFo=o("squeezebert"),GFo=o(" \u2014 "),q$=a("a"),OFo=o("SqueezeBertForSequenceClassification"),XFo=o(" (SqueezeBERT model)"),zFo=l(),n1=a("li"),lre=a("strong"),VFo=o("tapas"),WFo=o(" \u2014 "),G$=a("a"),QFo=o("TapasForSequenceClassification"),HFo=o(" (TAPAS model)"),UFo=l(),s1=a("li"),ire=a("strong"),JFo=o("transfo-xl"),YFo=o(" \u2014 "),O$=a("a"),KFo=o("TransfoXLForSequenceClassification"),ZFo=o(" (Transformer-XL model)"),e9o=l(),l1=a("li"),dre=a("strong"),o9o=o("xlm"),r9o=o(" \u2014 "),X$=a("a"),t9o=o("XLMForSequenceClassification"),a9o=o(" (XLM model)"),n9o=l(),i1=a("li"),cre=a("strong"),s9o=o("xlm-roberta"),l9o=o(" \u2014 "),z$=a("a"),i9o=o("XLMRobertaForSequenceClassification"),d9o=o(" (XLM-RoBERTa model)"),c9o=l(),d1=a("li"),fre=a("strong"),f9o=o("xlm-roberta-xl"),m9o=o(" \u2014 "),V$=a("a"),g9o=o("XLMRobertaXLForSequenceClassification"),h9o=o(" (XLM-RoBERTa-XL model)"),p9o=l(),c1=a("li"),mre=a("strong"),_9o=o("xlnet"),u9o=o(" \u2014 "),W$=a("a"),b9o=o("XLNetForSequenceClassification"),v9o=o(" (XLNet model)"),T9o=l(),f1=a("li"),gre=a("strong"),F9o=o("yoso"),C9o=o(" \u2014 "),Q$=a("a"),M9o=o("YosoForSequenceClassification"),E9o=o(" (YOSO model)"),y9o=l(),m1=a("p"),w9o=o("The model is set in evaluation mode by default using "),hre=a("code"),A9o=o("model.eval()"),L9o=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),pre=a("code"),B9o=o("model.train()"),k9o=l(),_re=a("p"),x9o=o("Examples:"),R9o=l(),f(CE.$$.fragment),LLe=l(),ad=a("h2"),g1=a("a"),ure=a("span"),f(ME.$$.fragment),S9o=l(),bre=a("span"),P9o=o("AutoModelForMultipleChoice"),BLe=l(),Yo=a("div"),f(EE.$$.fragment),$9o=l(),nd=a("p"),I9o=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a multiple choice head) when created
with the `),vre=a("code"),j9o=o("from_pretrained()"),N9o=o("class method or the "),Tre=a("code"),D9o=o("from_config()"),q9o=o(`class
method.`),G9o=l(),yE=a("p"),O9o=o("This class cannot be instantiated directly using "),Fre=a("code"),X9o=o("__init__()"),z9o=o(" (throws an error)."),V9o=l(),zr=a("div"),f(wE.$$.fragment),W9o=l(),Cre=a("p"),Q9o=o("Instantiates one of the model classes of the library (with a multiple choice head) from a configuration."),H9o=l(),sd=a("p"),U9o=o(`Note:
Loading a model from its configuration file does `),Mre=a("strong"),J9o=o("not"),Y9o=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Ere=a("code"),K9o=o("from_pretrained()"),Z9o=o("to load the model weights."),eCo=l(),yre=a("p"),oCo=o("Examples:"),rCo=l(),f(AE.$$.fragment),tCo=l(),Ie=a("div"),f(LE.$$.fragment),aCo=l(),wre=a("p"),nCo=o("Instantiate one of the model classes of the library (with a multiple choice head) from a pretrained model."),sCo=l(),Va=a("p"),lCo=o("The model class to instantiate is selected based on the "),Are=a("code"),iCo=o("model_type"),dCo=o(` property of the config object (either
passed as an argument or loaded from `),Lre=a("code"),cCo=o("pretrained_model_name_or_path"),fCo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Bre=a("code"),mCo=o("pretrained_model_name_or_path"),gCo=o(":"),hCo=l(),G=a("ul"),h1=a("li"),kre=a("strong"),pCo=o("albert"),_Co=o(" \u2014 "),H$=a("a"),uCo=o("AlbertForMultipleChoice"),bCo=o(" (ALBERT model)"),vCo=l(),p1=a("li"),xre=a("strong"),TCo=o("bert"),FCo=o(" \u2014 "),U$=a("a"),CCo=o("BertForMultipleChoice"),MCo=o(" (BERT model)"),ECo=l(),_1=a("li"),Rre=a("strong"),yCo=o("big_bird"),wCo=o(" \u2014 "),J$=a("a"),ACo=o("BigBirdForMultipleChoice"),LCo=o(" (BigBird model)"),BCo=l(),u1=a("li"),Sre=a("strong"),kCo=o("camembert"),xCo=o(" \u2014 "),Y$=a("a"),RCo=o("CamembertForMultipleChoice"),SCo=o(" (CamemBERT model)"),PCo=l(),b1=a("li"),Pre=a("strong"),$Co=o("canine"),ICo=o(" \u2014 "),K$=a("a"),jCo=o("CanineForMultipleChoice"),NCo=o(" (Canine model)"),DCo=l(),v1=a("li"),$re=a("strong"),qCo=o("convbert"),GCo=o(" \u2014 "),Z$=a("a"),OCo=o("ConvBertForMultipleChoice"),XCo=o(" (ConvBERT model)"),zCo=l(),T1=a("li"),Ire=a("strong"),VCo=o("distilbert"),WCo=o(" \u2014 "),eI=a("a"),QCo=o("DistilBertForMultipleChoice"),HCo=o(" (DistilBERT model)"),UCo=l(),F1=a("li"),jre=a("strong"),JCo=o("electra"),YCo=o(" \u2014 "),oI=a("a"),KCo=o("ElectraForMultipleChoice"),ZCo=o(" (ELECTRA model)"),e4o=l(),C1=a("li"),Nre=a("strong"),o4o=o("flaubert"),r4o=o(" \u2014 "),rI=a("a"),t4o=o("FlaubertForMultipleChoice"),a4o=o(" (FlauBERT model)"),n4o=l(),M1=a("li"),Dre=a("strong"),s4o=o("fnet"),l4o=o(" \u2014 "),tI=a("a"),i4o=o("FNetForMultipleChoice"),d4o=o(" (FNet model)"),c4o=l(),E1=a("li"),qre=a("strong"),f4o=o("funnel"),m4o=o(" \u2014 "),aI=a("a"),g4o=o("FunnelForMultipleChoice"),h4o=o(" (Funnel Transformer model)"),p4o=l(),y1=a("li"),Gre=a("strong"),_4o=o("ibert"),u4o=o(" \u2014 "),nI=a("a"),b4o=o("IBertForMultipleChoice"),v4o=o(" (I-BERT model)"),T4o=l(),w1=a("li"),Ore=a("strong"),F4o=o("longformer"),C4o=o(" \u2014 "),sI=a("a"),M4o=o("LongformerForMultipleChoice"),E4o=o(" (Longformer model)"),y4o=l(),A1=a("li"),Xre=a("strong"),w4o=o("megatron-bert"),A4o=o(" \u2014 "),lI=a("a"),L4o=o("MegatronBertForMultipleChoice"),B4o=o(" (MegatronBert model)"),k4o=l(),L1=a("li"),zre=a("strong"),x4o=o("mobilebert"),R4o=o(" \u2014 "),iI=a("a"),S4o=o("MobileBertForMultipleChoice"),P4o=o(" (MobileBERT model)"),$4o=l(),B1=a("li"),Vre=a("strong"),I4o=o("mpnet"),j4o=o(" \u2014 "),dI=a("a"),N4o=o("MPNetForMultipleChoice"),D4o=o(" (MPNet model)"),q4o=l(),k1=a("li"),Wre=a("strong"),G4o=o("nystromformer"),O4o=o(" \u2014 "),cI=a("a"),X4o=o("NystromformerForMultipleChoice"),z4o=o(" (Nystromformer model)"),V4o=l(),x1=a("li"),Qre=a("strong"),W4o=o("qdqbert"),Q4o=o(" \u2014 "),fI=a("a"),H4o=o("QDQBertForMultipleChoice"),U4o=o(" (QDQBert model)"),J4o=l(),R1=a("li"),Hre=a("strong"),Y4o=o("rembert"),K4o=o(" \u2014 "),mI=a("a"),Z4o=o("RemBertForMultipleChoice"),eMo=o(" (RemBERT model)"),oMo=l(),S1=a("li"),Ure=a("strong"),rMo=o("roberta"),tMo=o(" \u2014 "),gI=a("a"),aMo=o("RobertaForMultipleChoice"),nMo=o(" (RoBERTa model)"),sMo=l(),P1=a("li"),Jre=a("strong"),lMo=o("roformer"),iMo=o(" \u2014 "),hI=a("a"),dMo=o("RoFormerForMultipleChoice"),cMo=o(" (RoFormer model)"),fMo=l(),$1=a("li"),Yre=a("strong"),mMo=o("squeezebert"),gMo=o(" \u2014 "),pI=a("a"),hMo=o("SqueezeBertForMultipleChoice"),pMo=o(" (SqueezeBERT model)"),_Mo=l(),I1=a("li"),Kre=a("strong"),uMo=o("xlm"),bMo=o(" \u2014 "),_I=a("a"),vMo=o("XLMForMultipleChoice"),TMo=o(" (XLM model)"),FMo=l(),j1=a("li"),Zre=a("strong"),CMo=o("xlm-roberta"),MMo=o(" \u2014 "),uI=a("a"),EMo=o("XLMRobertaForMultipleChoice"),yMo=o(" (XLM-RoBERTa model)"),wMo=l(),N1=a("li"),ete=a("strong"),AMo=o("xlm-roberta-xl"),LMo=o(" \u2014 "),bI=a("a"),BMo=o("XLMRobertaXLForMultipleChoice"),kMo=o(" (XLM-RoBERTa-XL model)"),xMo=l(),D1=a("li"),ote=a("strong"),RMo=o("xlnet"),SMo=o(" \u2014 "),vI=a("a"),PMo=o("XLNetForMultipleChoice"),$Mo=o(" (XLNet model)"),IMo=l(),q1=a("li"),rte=a("strong"),jMo=o("yoso"),NMo=o(" \u2014 "),TI=a("a"),DMo=o("YosoForMultipleChoice"),qMo=o(" (YOSO model)"),GMo=l(),G1=a("p"),OMo=o("The model is set in evaluation mode by default using "),tte=a("code"),XMo=o("model.eval()"),zMo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),ate=a("code"),VMo=o("model.train()"),WMo=l(),nte=a("p"),QMo=o("Examples:"),HMo=l(),f(BE.$$.fragment),kLe=l(),ld=a("h2"),O1=a("a"),ste=a("span"),f(kE.$$.fragment),UMo=l(),lte=a("span"),JMo=o("AutoModelForNextSentencePrediction"),xLe=l(),Ko=a("div"),f(xE.$$.fragment),YMo=l(),id=a("p"),KMo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a next sentence prediction head) when created
with the `),ite=a("code"),ZMo=o("from_pretrained()"),eEo=o("class method or the "),dte=a("code"),oEo=o("from_config()"),rEo=o(`class
method.`),tEo=l(),RE=a("p"),aEo=o("This class cannot be instantiated directly using "),cte=a("code"),nEo=o("__init__()"),sEo=o(" (throws an error)."),lEo=l(),Vr=a("div"),f(SE.$$.fragment),iEo=l(),fte=a("p"),dEo=o("Instantiates one of the model classes of the library (with a next sentence prediction head) from a configuration."),cEo=l(),dd=a("p"),fEo=o(`Note:
Loading a model from its configuration file does `),mte=a("strong"),mEo=o("not"),gEo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),gte=a("code"),hEo=o("from_pretrained()"),pEo=o("to load the model weights."),_Eo=l(),hte=a("p"),uEo=o("Examples:"),bEo=l(),f(PE.$$.fragment),vEo=l(),je=a("div"),f($E.$$.fragment),TEo=l(),pte=a("p"),FEo=o("Instantiate one of the model classes of the library (with a next sentence prediction head) from a pretrained model."),CEo=l(),Wa=a("p"),MEo=o("The model class to instantiate is selected based on the "),_te=a("code"),EEo=o("model_type"),yEo=o(` property of the config object (either
passed as an argument or loaded from `),ute=a("code"),wEo=o("pretrained_model_name_or_path"),AEo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),bte=a("code"),LEo=o("pretrained_model_name_or_path"),BEo=o(":"),kEo=l(),na=a("ul"),X1=a("li"),vte=a("strong"),xEo=o("bert"),REo=o(" \u2014 "),FI=a("a"),SEo=o("BertForNextSentencePrediction"),PEo=o(" (BERT model)"),$Eo=l(),z1=a("li"),Tte=a("strong"),IEo=o("fnet"),jEo=o(" \u2014 "),CI=a("a"),NEo=o("FNetForNextSentencePrediction"),DEo=o(" (FNet model)"),qEo=l(),V1=a("li"),Fte=a("strong"),GEo=o("megatron-bert"),OEo=o(" \u2014 "),MI=a("a"),XEo=o("MegatronBertForNextSentencePrediction"),zEo=o(" (MegatronBert model)"),VEo=l(),W1=a("li"),Cte=a("strong"),WEo=o("mobilebert"),QEo=o(" \u2014 "),EI=a("a"),HEo=o("MobileBertForNextSentencePrediction"),UEo=o(" (MobileBERT model)"),JEo=l(),Q1=a("li"),Mte=a("strong"),YEo=o("qdqbert"),KEo=o(" \u2014 "),yI=a("a"),ZEo=o("QDQBertForNextSentencePrediction"),e3o=o(" (QDQBert model)"),o3o=l(),H1=a("p"),r3o=o("The model is set in evaluation mode by default using "),Ete=a("code"),t3o=o("model.eval()"),a3o=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),yte=a("code"),n3o=o("model.train()"),s3o=l(),wte=a("p"),l3o=o("Examples:"),i3o=l(),f(IE.$$.fragment),RLe=l(),cd=a("h2"),U1=a("a"),Ate=a("span"),f(jE.$$.fragment),d3o=l(),Lte=a("span"),c3o=o("AutoModelForTokenClassification"),SLe=l(),Zo=a("div"),f(NE.$$.fragment),f3o=l(),fd=a("p"),m3o=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a token classification head) when created
with the `),Bte=a("code"),g3o=o("from_pretrained()"),h3o=o("class method or the "),kte=a("code"),p3o=o("from_config()"),_3o=o(`class
method.`),u3o=l(),DE=a("p"),b3o=o("This class cannot be instantiated directly using "),xte=a("code"),v3o=o("__init__()"),T3o=o(" (throws an error)."),F3o=l(),Wr=a("div"),f(qE.$$.fragment),C3o=l(),Rte=a("p"),M3o=o("Instantiates one of the model classes of the library (with a token classification head) from a configuration."),E3o=l(),md=a("p"),y3o=o(`Note:
Loading a model from its configuration file does `),Ste=a("strong"),w3o=o("not"),A3o=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Pte=a("code"),L3o=o("from_pretrained()"),B3o=o("to load the model weights."),k3o=l(),$te=a("p"),x3o=o("Examples:"),R3o=l(),f(GE.$$.fragment),S3o=l(),Ne=a("div"),f(OE.$$.fragment),P3o=l(),Ite=a("p"),$3o=o("Instantiate one of the model classes of the library (with a token classification head) from a pretrained model."),I3o=l(),Qa=a("p"),j3o=o("The model class to instantiate is selected based on the "),jte=a("code"),N3o=o("model_type"),D3o=o(` property of the config object (either
passed as an argument or loaded from `),Nte=a("code"),q3o=o("pretrained_model_name_or_path"),G3o=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Dte=a("code"),O3o=o("pretrained_model_name_or_path"),X3o=o(":"),z3o=l(),D=a("ul"),J1=a("li"),qte=a("strong"),V3o=o("albert"),W3o=o(" \u2014 "),wI=a("a"),Q3o=o("AlbertForTokenClassification"),H3o=o(" (ALBERT model)"),U3o=l(),Y1=a("li"),Gte=a("strong"),J3o=o("bert"),Y3o=o(" \u2014 "),AI=a("a"),K3o=o("BertForTokenClassification"),Z3o=o(" (BERT model)"),eyo=l(),K1=a("li"),Ote=a("strong"),oyo=o("big_bird"),ryo=o(" \u2014 "),LI=a("a"),tyo=o("BigBirdForTokenClassification"),ayo=o(" (BigBird model)"),nyo=l(),Z1=a("li"),Xte=a("strong"),syo=o("camembert"),lyo=o(" \u2014 "),BI=a("a"),iyo=o("CamembertForTokenClassification"),dyo=o(" (CamemBERT model)"),cyo=l(),eb=a("li"),zte=a("strong"),fyo=o("canine"),myo=o(" \u2014 "),kI=a("a"),gyo=o("CanineForTokenClassification"),hyo=o(" (Canine model)"),pyo=l(),ob=a("li"),Vte=a("strong"),_yo=o("convbert"),uyo=o(" \u2014 "),xI=a("a"),byo=o("ConvBertForTokenClassification"),vyo=o(" (ConvBERT model)"),Tyo=l(),rb=a("li"),Wte=a("strong"),Fyo=o("deberta"),Cyo=o(" \u2014 "),RI=a("a"),Myo=o("DebertaForTokenClassification"),Eyo=o(" (DeBERTa model)"),yyo=l(),tb=a("li"),Qte=a("strong"),wyo=o("deberta-v2"),Ayo=o(" \u2014 "),SI=a("a"),Lyo=o("DebertaV2ForTokenClassification"),Byo=o(" (DeBERTa-v2 model)"),kyo=l(),ab=a("li"),Hte=a("strong"),xyo=o("distilbert"),Ryo=o(" \u2014 "),PI=a("a"),Syo=o("DistilBertForTokenClassification"),Pyo=o(" (DistilBERT model)"),$yo=l(),nb=a("li"),Ute=a("strong"),Iyo=o("electra"),jyo=o(" \u2014 "),$I=a("a"),Nyo=o("ElectraForTokenClassification"),Dyo=o(" (ELECTRA model)"),qyo=l(),sb=a("li"),Jte=a("strong"),Gyo=o("flaubert"),Oyo=o(" \u2014 "),II=a("a"),Xyo=o("FlaubertForTokenClassification"),zyo=o(" (FlauBERT model)"),Vyo=l(),lb=a("li"),Yte=a("strong"),Wyo=o("fnet"),Qyo=o(" \u2014 "),jI=a("a"),Hyo=o("FNetForTokenClassification"),Uyo=o(" (FNet model)"),Jyo=l(),ib=a("li"),Kte=a("strong"),Yyo=o("funnel"),Kyo=o(" \u2014 "),NI=a("a"),Zyo=o("FunnelForTokenClassification"),ewo=o(" (Funnel Transformer model)"),owo=l(),db=a("li"),Zte=a("strong"),rwo=o("gpt2"),two=o(" \u2014 "),DI=a("a"),awo=o("GPT2ForTokenClassification"),nwo=o(" (OpenAI GPT-2 model)"),swo=l(),cb=a("li"),eae=a("strong"),lwo=o("ibert"),iwo=o(" \u2014 "),qI=a("a"),dwo=o("IBertForTokenClassification"),cwo=o(" (I-BERT model)"),fwo=l(),fb=a("li"),oae=a("strong"),mwo=o("layoutlm"),gwo=o(" \u2014 "),GI=a("a"),hwo=o("LayoutLMForTokenClassification"),pwo=o(" (LayoutLM model)"),_wo=l(),mb=a("li"),rae=a("strong"),uwo=o("layoutlmv2"),bwo=o(" \u2014 "),OI=a("a"),vwo=o("LayoutLMv2ForTokenClassification"),Two=o(" (LayoutLMv2 model)"),Fwo=l(),gb=a("li"),tae=a("strong"),Cwo=o("longformer"),Mwo=o(" \u2014 "),XI=a("a"),Ewo=o("LongformerForTokenClassification"),ywo=o(" (Longformer model)"),wwo=l(),hb=a("li"),aae=a("strong"),Awo=o("megatron-bert"),Lwo=o(" \u2014 "),zI=a("a"),Bwo=o("MegatronBertForTokenClassification"),kwo=o(" (MegatronBert model)"),xwo=l(),pb=a("li"),nae=a("strong"),Rwo=o("mobilebert"),Swo=o(" \u2014 "),VI=a("a"),Pwo=o("MobileBertForTokenClassification"),$wo=o(" (MobileBERT model)"),Iwo=l(),_b=a("li"),sae=a("strong"),jwo=o("mpnet"),Nwo=o(" \u2014 "),WI=a("a"),Dwo=o("MPNetForTokenClassification"),qwo=o(" (MPNet model)"),Gwo=l(),ub=a("li"),lae=a("strong"),Owo=o("nystromformer"),Xwo=o(" \u2014 "),QI=a("a"),zwo=o("NystromformerForTokenClassification"),Vwo=o(" (Nystromformer model)"),Wwo=l(),bb=a("li"),iae=a("strong"),Qwo=o("qdqbert"),Hwo=o(" \u2014 "),HI=a("a"),Uwo=o("QDQBertForTokenClassification"),Jwo=o(" (QDQBert model)"),Ywo=l(),vb=a("li"),dae=a("strong"),Kwo=o("rembert"),Zwo=o(" \u2014 "),UI=a("a"),eAo=o("RemBertForTokenClassification"),oAo=o(" (RemBERT model)"),rAo=l(),Tb=a("li"),cae=a("strong"),tAo=o("roberta"),aAo=o(" \u2014 "),JI=a("a"),nAo=o("RobertaForTokenClassification"),sAo=o(" (RoBERTa model)"),lAo=l(),Fb=a("li"),fae=a("strong"),iAo=o("roformer"),dAo=o(" \u2014 "),YI=a("a"),cAo=o("RoFormerForTokenClassification"),fAo=o(" (RoFormer model)"),mAo=l(),Cb=a("li"),mae=a("strong"),gAo=o("squeezebert"),hAo=o(" \u2014 "),KI=a("a"),pAo=o("SqueezeBertForTokenClassification"),_Ao=o(" (SqueezeBERT model)"),uAo=l(),Mb=a("li"),gae=a("strong"),bAo=o("xlm"),vAo=o(" \u2014 "),ZI=a("a"),TAo=o("XLMForTokenClassification"),FAo=o(" (XLM model)"),CAo=l(),Eb=a("li"),hae=a("strong"),MAo=o("xlm-roberta"),EAo=o(" \u2014 "),ej=a("a"),yAo=o("XLMRobertaForTokenClassification"),wAo=o(" (XLM-RoBERTa model)"),AAo=l(),yb=a("li"),pae=a("strong"),LAo=o("xlm-roberta-xl"),BAo=o(" \u2014 "),oj=a("a"),kAo=o("XLMRobertaXLForTokenClassification"),xAo=o(" (XLM-RoBERTa-XL model)"),RAo=l(),wb=a("li"),_ae=a("strong"),SAo=o("xlnet"),PAo=o(" \u2014 "),rj=a("a"),$Ao=o("XLNetForTokenClassification"),IAo=o(" (XLNet model)"),jAo=l(),Ab=a("li"),uae=a("strong"),NAo=o("yoso"),DAo=o(" \u2014 "),tj=a("a"),qAo=o("YosoForTokenClassification"),GAo=o(" (YOSO model)"),OAo=l(),Lb=a("p"),XAo=o("The model is set in evaluation mode by default using "),bae=a("code"),zAo=o("model.eval()"),VAo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),vae=a("code"),WAo=o("model.train()"),QAo=l(),Tae=a("p"),HAo=o("Examples:"),UAo=l(),f(XE.$$.fragment),PLe=l(),gd=a("h2"),Bb=a("a"),Fae=a("span"),f(zE.$$.fragment),JAo=l(),Cae=a("span"),YAo=o("AutoModelForQuestionAnswering"),$Le=l(),er=a("div"),f(VE.$$.fragment),KAo=l(),hd=a("p"),ZAo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a question answering head) when created
with the `),Mae=a("code"),e6o=o("from_pretrained()"),o6o=o("class method or the "),Eae=a("code"),r6o=o("from_config()"),t6o=o(`class
method.`),a6o=l(),WE=a("p"),n6o=o("This class cannot be instantiated directly using "),yae=a("code"),s6o=o("__init__()"),l6o=o(" (throws an error)."),i6o=l(),Qr=a("div"),f(QE.$$.fragment),d6o=l(),wae=a("p"),c6o=o("Instantiates one of the model classes of the library (with a question answering head) from a configuration."),f6o=l(),pd=a("p"),m6o=o(`Note:
Loading a model from its configuration file does `),Aae=a("strong"),g6o=o("not"),h6o=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Lae=a("code"),p6o=o("from_pretrained()"),_6o=o("to load the model weights."),u6o=l(),Bae=a("p"),b6o=o("Examples:"),v6o=l(),f(HE.$$.fragment),T6o=l(),De=a("div"),f(UE.$$.fragment),F6o=l(),kae=a("p"),C6o=o("Instantiate one of the model classes of the library (with a question answering head) from a pretrained model."),M6o=l(),Ha=a("p"),E6o=o("The model class to instantiate is selected based on the "),xae=a("code"),y6o=o("model_type"),w6o=o(` property of the config object (either
passed as an argument or loaded from `),Rae=a("code"),A6o=o("pretrained_model_name_or_path"),L6o=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Sae=a("code"),B6o=o("pretrained_model_name_or_path"),k6o=o(":"),x6o=l(),R=a("ul"),kb=a("li"),Pae=a("strong"),R6o=o("albert"),S6o=o(" \u2014 "),aj=a("a"),P6o=o("AlbertForQuestionAnswering"),$6o=o(" (ALBERT model)"),I6o=l(),xb=a("li"),$ae=a("strong"),j6o=o("bart"),N6o=o(" \u2014 "),nj=a("a"),D6o=o("BartForQuestionAnswering"),q6o=o(" (BART model)"),G6o=l(),Rb=a("li"),Iae=a("strong"),O6o=o("bert"),X6o=o(" \u2014 "),sj=a("a"),z6o=o("BertForQuestionAnswering"),V6o=o(" (BERT model)"),W6o=l(),Sb=a("li"),jae=a("strong"),Q6o=o("big_bird"),H6o=o(" \u2014 "),lj=a("a"),U6o=o("BigBirdForQuestionAnswering"),J6o=o(" (BigBird model)"),Y6o=l(),Pb=a("li"),Nae=a("strong"),K6o=o("bigbird_pegasus"),Z6o=o(" \u2014 "),ij=a("a"),e0o=o("BigBirdPegasusForQuestionAnswering"),o0o=o(" (BigBirdPegasus model)"),r0o=l(),$b=a("li"),Dae=a("strong"),t0o=o("camembert"),a0o=o(" \u2014 "),dj=a("a"),n0o=o("CamembertForQuestionAnswering"),s0o=o(" (CamemBERT model)"),l0o=l(),Ib=a("li"),qae=a("strong"),i0o=o("canine"),d0o=o(" \u2014 "),cj=a("a"),c0o=o("CanineForQuestionAnswering"),f0o=o(" (Canine model)"),m0o=l(),jb=a("li"),Gae=a("strong"),g0o=o("convbert"),h0o=o(" \u2014 "),fj=a("a"),p0o=o("ConvBertForQuestionAnswering"),_0o=o(" (ConvBERT model)"),u0o=l(),Nb=a("li"),Oae=a("strong"),b0o=o("deberta"),v0o=o(" \u2014 "),mj=a("a"),T0o=o("DebertaForQuestionAnswering"),F0o=o(" (DeBERTa model)"),C0o=l(),Db=a("li"),Xae=a("strong"),M0o=o("deberta-v2"),E0o=o(" \u2014 "),gj=a("a"),y0o=o("DebertaV2ForQuestionAnswering"),w0o=o(" (DeBERTa-v2 model)"),A0o=l(),qb=a("li"),zae=a("strong"),L0o=o("distilbert"),B0o=o(" \u2014 "),hj=a("a"),k0o=o("DistilBertForQuestionAnswering"),x0o=o(" (DistilBERT model)"),R0o=l(),Gb=a("li"),Vae=a("strong"),S0o=o("electra"),P0o=o(" \u2014 "),pj=a("a"),$0o=o("ElectraForQuestionAnswering"),I0o=o(" (ELECTRA model)"),j0o=l(),Ob=a("li"),Wae=a("strong"),N0o=o("flaubert"),D0o=o(" \u2014 "),_j=a("a"),q0o=o("FlaubertForQuestionAnsweringSimple"),G0o=o(" (FlauBERT model)"),O0o=l(),Xb=a("li"),Qae=a("strong"),X0o=o("fnet"),z0o=o(" \u2014 "),uj=a("a"),V0o=o("FNetForQuestionAnswering"),W0o=o(" (FNet model)"),Q0o=l(),zb=a("li"),Hae=a("strong"),H0o=o("funnel"),U0o=o(" \u2014 "),bj=a("a"),J0o=o("FunnelForQuestionAnswering"),Y0o=o(" (Funnel Transformer model)"),K0o=l(),Vb=a("li"),Uae=a("strong"),Z0o=o("gptj"),eLo=o(" \u2014 "),vj=a("a"),oLo=o("GPTJForQuestionAnswering"),rLo=o(" (GPT-J model)"),tLo=l(),Wb=a("li"),Jae=a("strong"),aLo=o("ibert"),nLo=o(" \u2014 "),Tj=a("a"),sLo=o("IBertForQuestionAnswering"),lLo=o(" (I-BERT model)"),iLo=l(),Qb=a("li"),Yae=a("strong"),dLo=o("layoutlmv2"),cLo=o(" \u2014 "),Fj=a("a"),fLo=o("LayoutLMv2ForQuestionAnswering"),mLo=o(" (LayoutLMv2 model)"),gLo=l(),Hb=a("li"),Kae=a("strong"),hLo=o("led"),pLo=o(" \u2014 "),Cj=a("a"),_Lo=o("LEDForQuestionAnswering"),uLo=o(" (LED model)"),bLo=l(),Ub=a("li"),Zae=a("strong"),vLo=o("longformer"),TLo=o(" \u2014 "),Mj=a("a"),FLo=o("LongformerForQuestionAnswering"),CLo=o(" (Longformer model)"),MLo=l(),Jb=a("li"),ene=a("strong"),ELo=o("lxmert"),yLo=o(" \u2014 "),Ej=a("a"),wLo=o("LxmertForQuestionAnswering"),ALo=o(" (LXMERT model)"),LLo=l(),Yb=a("li"),one=a("strong"),BLo=o("mbart"),kLo=o(" \u2014 "),yj=a("a"),xLo=o("MBartForQuestionAnswering"),RLo=o(" (mBART model)"),SLo=l(),Kb=a("li"),rne=a("strong"),PLo=o("megatron-bert"),$Lo=o(" \u2014 "),wj=a("a"),ILo=o("MegatronBertForQuestionAnswering"),jLo=o(" (MegatronBert model)"),NLo=l(),Zb=a("li"),tne=a("strong"),DLo=o("mobilebert"),qLo=o(" \u2014 "),Aj=a("a"),GLo=o("MobileBertForQuestionAnswering"),OLo=o(" (MobileBERT model)"),XLo=l(),e5=a("li"),ane=a("strong"),zLo=o("mpnet"),VLo=o(" \u2014 "),Lj=a("a"),WLo=o("MPNetForQuestionAnswering"),QLo=o(" (MPNet model)"),HLo=l(),o5=a("li"),nne=a("strong"),ULo=o("nystromformer"),JLo=o(" \u2014 "),Bj=a("a"),YLo=o("NystromformerForQuestionAnswering"),KLo=o(" (Nystromformer model)"),ZLo=l(),r5=a("li"),sne=a("strong"),e8o=o("qdqbert"),o8o=o(" \u2014 "),kj=a("a"),r8o=o("QDQBertForQuestionAnswering"),t8o=o(" (QDQBert model)"),a8o=l(),t5=a("li"),lne=a("strong"),n8o=o("reformer"),s8o=o(" \u2014 "),xj=a("a"),l8o=o("ReformerForQuestionAnswering"),i8o=o(" (Reformer model)"),d8o=l(),a5=a("li"),ine=a("strong"),c8o=o("rembert"),f8o=o(" \u2014 "),Rj=a("a"),m8o=o("RemBertForQuestionAnswering"),g8o=o(" (RemBERT model)"),h8o=l(),n5=a("li"),dne=a("strong"),p8o=o("roberta"),_8o=o(" \u2014 "),Sj=a("a"),u8o=o("RobertaForQuestionAnswering"),b8o=o(" (RoBERTa model)"),v8o=l(),s5=a("li"),cne=a("strong"),T8o=o("roformer"),F8o=o(" \u2014 "),Pj=a("a"),C8o=o("RoFormerForQuestionAnswering"),M8o=o(" (RoFormer model)"),E8o=l(),l5=a("li"),fne=a("strong"),y8o=o("splinter"),w8o=o(" \u2014 "),$j=a("a"),A8o=o("SplinterForQuestionAnswering"),L8o=o(" (Splinter model)"),B8o=l(),i5=a("li"),mne=a("strong"),k8o=o("squeezebert"),x8o=o(" \u2014 "),Ij=a("a"),R8o=o("SqueezeBertForQuestionAnswering"),S8o=o(" (SqueezeBERT model)"),P8o=l(),d5=a("li"),gne=a("strong"),$8o=o("xlm"),I8o=o(" \u2014 "),jj=a("a"),j8o=o("XLMForQuestionAnsweringSimple"),N8o=o(" (XLM model)"),D8o=l(),c5=a("li"),hne=a("strong"),q8o=o("xlm-roberta"),G8o=o(" \u2014 "),Nj=a("a"),O8o=o("XLMRobertaForQuestionAnswering"),X8o=o(" (XLM-RoBERTa model)"),z8o=l(),f5=a("li"),pne=a("strong"),V8o=o("xlm-roberta-xl"),W8o=o(" \u2014 "),Dj=a("a"),Q8o=o("XLMRobertaXLForQuestionAnswering"),H8o=o(" (XLM-RoBERTa-XL model)"),U8o=l(),m5=a("li"),_ne=a("strong"),J8o=o("xlnet"),Y8o=o(" \u2014 "),qj=a("a"),K8o=o("XLNetForQuestionAnsweringSimple"),Z8o=o(" (XLNet model)"),eBo=l(),g5=a("li"),une=a("strong"),oBo=o("yoso"),rBo=o(" \u2014 "),Gj=a("a"),tBo=o("YosoForQuestionAnswering"),aBo=o(" (YOSO model)"),nBo=l(),h5=a("p"),sBo=o("The model is set in evaluation mode by default using "),bne=a("code"),lBo=o("model.eval()"),iBo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),vne=a("code"),dBo=o("model.train()"),cBo=l(),Tne=a("p"),fBo=o("Examples:"),mBo=l(),f(JE.$$.fragment),ILe=l(),_d=a("h2"),p5=a("a"),Fne=a("span"),f(YE.$$.fragment),gBo=l(),Cne=a("span"),hBo=o("AutoModelForTableQuestionAnswering"),jLe=l(),or=a("div"),f(KE.$$.fragment),pBo=l(),ud=a("p"),_Bo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a table question answering head) when created
with the `),Mne=a("code"),uBo=o("from_pretrained()"),bBo=o("class method or the "),Ene=a("code"),vBo=o("from_config()"),TBo=o(`class
method.`),FBo=l(),ZE=a("p"),CBo=o("This class cannot be instantiated directly using "),yne=a("code"),MBo=o("__init__()"),EBo=o(" (throws an error)."),yBo=l(),Hr=a("div"),f(e3.$$.fragment),wBo=l(),wne=a("p"),ABo=o("Instantiates one of the model classes of the library (with a table question answering head) from a configuration."),LBo=l(),bd=a("p"),BBo=o(`Note:
Loading a model from its configuration file does `),Ane=a("strong"),kBo=o("not"),xBo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Lne=a("code"),RBo=o("from_pretrained()"),SBo=o("to load the model weights."),PBo=l(),Bne=a("p"),$Bo=o("Examples:"),IBo=l(),f(o3.$$.fragment),jBo=l(),qe=a("div"),f(r3.$$.fragment),NBo=l(),kne=a("p"),DBo=o("Instantiate one of the model classes of the library (with a table question answering head) from a pretrained model."),qBo=l(),Ua=a("p"),GBo=o("The model class to instantiate is selected based on the "),xne=a("code"),OBo=o("model_type"),XBo=o(` property of the config object (either
passed as an argument or loaded from `),Rne=a("code"),zBo=o("pretrained_model_name_or_path"),VBo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Sne=a("code"),WBo=o("pretrained_model_name_or_path"),QBo=o(":"),HBo=l(),Pne=a("ul"),_5=a("li"),$ne=a("strong"),UBo=o("tapas"),JBo=o(" \u2014 "),Oj=a("a"),YBo=o("TapasForQuestionAnswering"),KBo=o(" (TAPAS model)"),ZBo=l(),u5=a("p"),eko=o("The model is set in evaluation mode by default using "),Ine=a("code"),oko=o("model.eval()"),rko=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),jne=a("code"),tko=o("model.train()"),ako=l(),Nne=a("p"),nko=o("Examples:"),sko=l(),f(t3.$$.fragment),NLe=l(),vd=a("h2"),b5=a("a"),Dne=a("span"),f(a3.$$.fragment),lko=l(),qne=a("span"),iko=o("AutoModelForImageClassification"),DLe=l(),rr=a("div"),f(n3.$$.fragment),dko=l(),Td=a("p"),cko=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a image classification head) when created
with the `),Gne=a("code"),fko=o("from_pretrained()"),mko=o("class method or the "),One=a("code"),gko=o("from_config()"),hko=o(`class
method.`),pko=l(),s3=a("p"),_ko=o("This class cannot be instantiated directly using "),Xne=a("code"),uko=o("__init__()"),bko=o(" (throws an error)."),vko=l(),Ur=a("div"),f(l3.$$.fragment),Tko=l(),zne=a("p"),Fko=o("Instantiates one of the model classes of the library (with a image classification head) from a configuration."),Cko=l(),Fd=a("p"),Mko=o(`Note:
Loading a model from its configuration file does `),Vne=a("strong"),Eko=o("not"),yko=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Wne=a("code"),wko=o("from_pretrained()"),Ako=o("to load the model weights."),Lko=l(),Qne=a("p"),Bko=o("Examples:"),kko=l(),f(i3.$$.fragment),xko=l(),Ge=a("div"),f(d3.$$.fragment),Rko=l(),Hne=a("p"),Sko=o("Instantiate one of the model classes of the library (with a image classification head) from a pretrained model."),Pko=l(),Ja=a("p"),$ko=o("The model class to instantiate is selected based on the "),Une=a("code"),Iko=o("model_type"),jko=o(` property of the config object (either
passed as an argument or loaded from `),Jne=a("code"),Nko=o("pretrained_model_name_or_path"),Dko=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Yne=a("code"),qko=o("pretrained_model_name_or_path"),Gko=o(":"),Oko=l(),be=a("ul"),v5=a("li"),Kne=a("strong"),Xko=o("beit"),zko=o(" \u2014 "),Xj=a("a"),Vko=o("BeitForImageClassification"),Wko=o(" (BEiT model)"),Qko=l(),T5=a("li"),Zne=a("strong"),Hko=o("convnext"),Uko=o(" \u2014 "),zj=a("a"),Jko=o("ConvNextForImageClassification"),Yko=o(" (ConvNext model)"),Kko=l(),Rs=a("li"),ese=a("strong"),Zko=o("deit"),exo=o(" \u2014 "),Vj=a("a"),oxo=o("DeiTForImageClassification"),rxo=o(" or "),Wj=a("a"),txo=o("DeiTForImageClassificationWithTeacher"),axo=o(" (DeiT model)"),nxo=l(),F5=a("li"),ose=a("strong"),sxo=o("imagegpt"),lxo=o(" \u2014 "),Qj=a("a"),ixo=o("ImageGPTForImageClassification"),dxo=o(" (ImageGPT model)"),cxo=l(),la=a("li"),rse=a("strong"),fxo=o("perceiver"),mxo=o(" \u2014 "),Hj=a("a"),gxo=o("PerceiverForImageClassificationLearned"),hxo=o(" or "),Uj=a("a"),pxo=o("PerceiverForImageClassificationFourier"),_xo=o(" or "),Jj=a("a"),uxo=o("PerceiverForImageClassificationConvProcessing"),bxo=o(" (Perceiver model)"),vxo=l(),C5=a("li"),tse=a("strong"),Txo=o("poolformer"),Fxo=o(" \u2014 "),Yj=a("a"),Cxo=o("PoolFormerForImageClassification"),Mxo=o(" (PoolFormer model)"),Exo=l(),M5=a("li"),ase=a("strong"),yxo=o("segformer"),wxo=o(" \u2014 "),Kj=a("a"),Axo=o("SegformerForImageClassification"),Lxo=o(" (SegFormer model)"),Bxo=l(),E5=a("li"),nse=a("strong"),kxo=o("swin"),xxo=o(" \u2014 "),Zj=a("a"),Rxo=o("SwinForImageClassification"),Sxo=o(" (Swin model)"),Pxo=l(),y5=a("li"),sse=a("strong"),$xo=o("vit"),Ixo=o(" \u2014 "),eN=a("a"),jxo=o("ViTForImageClassification"),Nxo=o(" (ViT model)"),Dxo=l(),w5=a("p"),qxo=o("The model is set in evaluation mode by default using "),lse=a("code"),Gxo=o("model.eval()"),Oxo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),ise=a("code"),Xxo=o("model.train()"),zxo=l(),dse=a("p"),Vxo=o("Examples:"),Wxo=l(),f(c3.$$.fragment),qLe=l(),Cd=a("h2"),A5=a("a"),cse=a("span"),f(f3.$$.fragment),Qxo=l(),fse=a("span"),Hxo=o("AutoModelForVision2Seq"),GLe=l(),tr=a("div"),f(m3.$$.fragment),Uxo=l(),Md=a("p"),Jxo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a vision-to-text modeling head) when created
with the `),mse=a("code"),Yxo=o("from_pretrained()"),Kxo=o("class method or the "),gse=a("code"),Zxo=o("from_config()"),eRo=o(`class
method.`),oRo=l(),g3=a("p"),rRo=o("This class cannot be instantiated directly using "),hse=a("code"),tRo=o("__init__()"),aRo=o(" (throws an error)."),nRo=l(),Jr=a("div"),f(h3.$$.fragment),sRo=l(),pse=a("p"),lRo=o("Instantiates one of the model classes of the library (with a vision-to-text modeling head) from a configuration."),iRo=l(),Ed=a("p"),dRo=o(`Note:
Loading a model from its configuration file does `),_se=a("strong"),cRo=o("not"),fRo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),use=a("code"),mRo=o("from_pretrained()"),gRo=o("to load the model weights."),hRo=l(),bse=a("p"),pRo=o("Examples:"),_Ro=l(),f(p3.$$.fragment),uRo=l(),Oe=a("div"),f(_3.$$.fragment),bRo=l(),vse=a("p"),vRo=o("Instantiate one of the model classes of the library (with a vision-to-text modeling head) from a pretrained model."),TRo=l(),Ya=a("p"),FRo=o("The model class to instantiate is selected based on the "),Tse=a("code"),CRo=o("model_type"),MRo=o(` property of the config object (either
passed as an argument or loaded from `),Fse=a("code"),ERo=o("pretrained_model_name_or_path"),yRo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Cse=a("code"),wRo=o("pretrained_model_name_or_path"),ARo=o(":"),LRo=l(),Mse=a("ul"),L5=a("li"),Ese=a("strong"),BRo=o("vision-encoder-decoder"),kRo=o(" \u2014 "),oN=a("a"),xRo=o("VisionEncoderDecoderModel"),RRo=o(" (Vision Encoder decoder model)"),SRo=l(),B5=a("p"),PRo=o("The model is set in evaluation mode by default using "),yse=a("code"),$Ro=o("model.eval()"),IRo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),wse=a("code"),jRo=o("model.train()"),NRo=l(),Ase=a("p"),DRo=o("Examples:"),qRo=l(),f(u3.$$.fragment),OLe=l(),yd=a("h2"),k5=a("a"),Lse=a("span"),f(b3.$$.fragment),GRo=l(),Bse=a("span"),ORo=o("AutoModelForAudioClassification"),XLe=l(),ar=a("div"),f(v3.$$.fragment),XRo=l(),wd=a("p"),zRo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a audio classification head) when created
with the `),kse=a("code"),VRo=o("from_pretrained()"),WRo=o("class method or the "),xse=a("code"),QRo=o("from_config()"),HRo=o(`class
method.`),URo=l(),T3=a("p"),JRo=o("This class cannot be instantiated directly using "),Rse=a("code"),YRo=o("__init__()"),KRo=o(" (throws an error)."),ZRo=l(),Yr=a("div"),f(F3.$$.fragment),eSo=l(),Sse=a("p"),oSo=o("Instantiates one of the model classes of the library (with a audio classification head) from a configuration."),rSo=l(),Ad=a("p"),tSo=o(`Note:
Loading a model from its configuration file does `),Pse=a("strong"),aSo=o("not"),nSo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),$se=a("code"),sSo=o("from_pretrained()"),lSo=o("to load the model weights."),iSo=l(),Ise=a("p"),dSo=o("Examples:"),cSo=l(),f(C3.$$.fragment),fSo=l(),Xe=a("div"),f(M3.$$.fragment),mSo=l(),jse=a("p"),gSo=o("Instantiate one of the model classes of the library (with a audio classification head) from a pretrained model."),hSo=l(),Ka=a("p"),pSo=o("The model class to instantiate is selected based on the "),Nse=a("code"),_So=o("model_type"),uSo=o(` property of the config object (either
passed as an argument or loaded from `),Dse=a("code"),bSo=o("pretrained_model_name_or_path"),vSo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),qse=a("code"),TSo=o("pretrained_model_name_or_path"),FSo=o(":"),CSo=l(),ao=a("ul"),x5=a("li"),Gse=a("strong"),MSo=o("hubert"),ESo=o(" \u2014 "),rN=a("a"),ySo=o("HubertForSequenceClassification"),wSo=o(" (Hubert model)"),ASo=l(),R5=a("li"),Ose=a("strong"),LSo=o("sew"),BSo=o(" \u2014 "),tN=a("a"),kSo=o("SEWForSequenceClassification"),xSo=o(" (SEW model)"),RSo=l(),S5=a("li"),Xse=a("strong"),SSo=o("sew-d"),PSo=o(" \u2014 "),aN=a("a"),$So=o("SEWDForSequenceClassification"),ISo=o(" (SEW-D model)"),jSo=l(),P5=a("li"),zse=a("strong"),NSo=o("unispeech"),DSo=o(" \u2014 "),nN=a("a"),qSo=o("UniSpeechForSequenceClassification"),GSo=o(" (UniSpeech model)"),OSo=l(),$5=a("li"),Vse=a("strong"),XSo=o("unispeech-sat"),zSo=o(" \u2014 "),sN=a("a"),VSo=o("UniSpeechSatForSequenceClassification"),WSo=o(" (UniSpeechSat model)"),QSo=l(),I5=a("li"),Wse=a("strong"),HSo=o("wav2vec2"),USo=o(" \u2014 "),lN=a("a"),JSo=o("Wav2Vec2ForSequenceClassification"),YSo=o(" (Wav2Vec2 model)"),KSo=l(),j5=a("li"),Qse=a("strong"),ZSo=o("wavlm"),ePo=o(" \u2014 "),iN=a("a"),oPo=o("WavLMForSequenceClassification"),rPo=o(" (WavLM model)"),tPo=l(),N5=a("p"),aPo=o("The model is set in evaluation mode by default using "),Hse=a("code"),nPo=o("model.eval()"),sPo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Use=a("code"),lPo=o("model.train()"),iPo=l(),Jse=a("p"),dPo=o("Examples:"),cPo=l(),f(E3.$$.fragment),zLe=l(),Ld=a("h2"),D5=a("a"),Yse=a("span"),f(y3.$$.fragment),fPo=l(),Kse=a("span"),mPo=o("AutoModelForAudioFrameClassification"),VLe=l(),nr=a("div"),f(w3.$$.fragment),gPo=l(),Bd=a("p"),hPo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a audio frame (token) classification head) when created
with the `),Zse=a("code"),pPo=o("from_pretrained()"),_Po=o("class method or the "),ele=a("code"),uPo=o("from_config()"),bPo=o(`class
method.`),vPo=l(),A3=a("p"),TPo=o("This class cannot be instantiated directly using "),ole=a("code"),FPo=o("__init__()"),CPo=o(" (throws an error)."),MPo=l(),Kr=a("div"),f(L3.$$.fragment),EPo=l(),rle=a("p"),yPo=o("Instantiates one of the model classes of the library (with a audio frame (token) classification head) from a configuration."),wPo=l(),kd=a("p"),APo=o(`Note:
Loading a model from its configuration file does `),tle=a("strong"),LPo=o("not"),BPo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),ale=a("code"),kPo=o("from_pretrained()"),xPo=o("to load the model weights."),RPo=l(),nle=a("p"),SPo=o("Examples:"),PPo=l(),f(B3.$$.fragment),$Po=l(),ze=a("div"),f(k3.$$.fragment),IPo=l(),sle=a("p"),jPo=o("Instantiate one of the model classes of the library (with a audio frame (token) classification head) from a pretrained model."),NPo=l(),Za=a("p"),DPo=o("The model class to instantiate is selected based on the "),lle=a("code"),qPo=o("model_type"),GPo=o(` property of the config object (either
passed as an argument or loaded from `),ile=a("code"),OPo=o("pretrained_model_name_or_path"),XPo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),dle=a("code"),zPo=o("pretrained_model_name_or_path"),VPo=o(":"),WPo=l(),xd=a("ul"),q5=a("li"),cle=a("strong"),QPo=o("unispeech-sat"),HPo=o(" \u2014 "),dN=a("a"),UPo=o("UniSpeechSatForAudioFrameClassification"),JPo=o(" (UniSpeechSat model)"),YPo=l(),G5=a("li"),fle=a("strong"),KPo=o("wav2vec2"),ZPo=o(" \u2014 "),cN=a("a"),e$o=o("Wav2Vec2ForAudioFrameClassification"),o$o=o(" (Wav2Vec2 model)"),r$o=l(),O5=a("li"),mle=a("strong"),t$o=o("wavlm"),a$o=o(" \u2014 "),fN=a("a"),n$o=o("WavLMForAudioFrameClassification"),s$o=o(" (WavLM model)"),l$o=l(),X5=a("p"),i$o=o("The model is set in evaluation mode by default using "),gle=a("code"),d$o=o("model.eval()"),c$o=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),hle=a("code"),f$o=o("model.train()"),m$o=l(),ple=a("p"),g$o=o("Examples:"),h$o=l(),f(x3.$$.fragment),WLe=l(),Rd=a("h2"),z5=a("a"),_le=a("span"),f(R3.$$.fragment),p$o=l(),ule=a("span"),_$o=o("AutoModelForCTC"),QLe=l(),sr=a("div"),f(S3.$$.fragment),u$o=l(),Sd=a("p"),b$o=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a connectionist temporal classification head) when created
with the `),ble=a("code"),v$o=o("from_pretrained()"),T$o=o("class method or the "),vle=a("code"),F$o=o("from_config()"),C$o=o(`class
method.`),M$o=l(),P3=a("p"),E$o=o("This class cannot be instantiated directly using "),Tle=a("code"),y$o=o("__init__()"),w$o=o(" (throws an error)."),A$o=l(),Zr=a("div"),f($3.$$.fragment),L$o=l(),Fle=a("p"),B$o=o("Instantiates one of the model classes of the library (with a connectionist temporal classification head) from a configuration."),k$o=l(),Pd=a("p"),x$o=o(`Note:
Loading a model from its configuration file does `),Cle=a("strong"),R$o=o("not"),S$o=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Mle=a("code"),P$o=o("from_pretrained()"),$$o=o("to load the model weights."),I$o=l(),Ele=a("p"),j$o=o("Examples:"),N$o=l(),f(I3.$$.fragment),D$o=l(),Ve=a("div"),f(j3.$$.fragment),q$o=l(),yle=a("p"),G$o=o("Instantiate one of the model classes of the library (with a connectionist temporal classification head) from a pretrained model."),O$o=l(),en=a("p"),X$o=o("The model class to instantiate is selected based on the "),wle=a("code"),z$o=o("model_type"),V$o=o(` property of the config object (either
passed as an argument or loaded from `),Ale=a("code"),W$o=o("pretrained_model_name_or_path"),Q$o=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Lle=a("code"),H$o=o("pretrained_model_name_or_path"),U$o=o(":"),J$o=l(),no=a("ul"),V5=a("li"),Ble=a("strong"),Y$o=o("hubert"),K$o=o(" \u2014 "),mN=a("a"),Z$o=o("HubertForCTC"),eIo=o(" (Hubert model)"),oIo=l(),W5=a("li"),kle=a("strong"),rIo=o("sew"),tIo=o(" \u2014 "),gN=a("a"),aIo=o("SEWForCTC"),nIo=o(" (SEW model)"),sIo=l(),Q5=a("li"),xle=a("strong"),lIo=o("sew-d"),iIo=o(" \u2014 "),hN=a("a"),dIo=o("SEWDForCTC"),cIo=o(" (SEW-D model)"),fIo=l(),H5=a("li"),Rle=a("strong"),mIo=o("unispeech"),gIo=o(" \u2014 "),pN=a("a"),hIo=o("UniSpeechForCTC"),pIo=o(" (UniSpeech model)"),_Io=l(),U5=a("li"),Sle=a("strong"),uIo=o("unispeech-sat"),bIo=o(" \u2014 "),_N=a("a"),vIo=o("UniSpeechSatForCTC"),TIo=o(" (UniSpeechSat model)"),FIo=l(),J5=a("li"),Ple=a("strong"),CIo=o("wav2vec2"),MIo=o(" \u2014 "),uN=a("a"),EIo=o("Wav2Vec2ForCTC"),yIo=o(" (Wav2Vec2 model)"),wIo=l(),Y5=a("li"),$le=a("strong"),AIo=o("wavlm"),LIo=o(" \u2014 "),bN=a("a"),BIo=o("WavLMForCTC"),kIo=o(" (WavLM model)"),xIo=l(),K5=a("p"),RIo=o("The model is set in evaluation mode by default using "),Ile=a("code"),SIo=o("model.eval()"),PIo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),jle=a("code"),$Io=o("model.train()"),IIo=l(),Nle=a("p"),jIo=o("Examples:"),NIo=l(),f(N3.$$.fragment),HLe=l(),$d=a("h2"),Z5=a("a"),Dle=a("span"),f(D3.$$.fragment),DIo=l(),qle=a("span"),qIo=o("AutoModelForSpeechSeq2Seq"),ULe=l(),lr=a("div"),f(q3.$$.fragment),GIo=l(),Id=a("p"),OIo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) when created
with the `),Gle=a("code"),XIo=o("from_pretrained()"),zIo=o("class method or the "),Ole=a("code"),VIo=o("from_config()"),WIo=o(`class
method.`),QIo=l(),G3=a("p"),HIo=o("This class cannot be instantiated directly using "),Xle=a("code"),UIo=o("__init__()"),JIo=o(" (throws an error)."),YIo=l(),et=a("div"),f(O3.$$.fragment),KIo=l(),zle=a("p"),ZIo=o("Instantiates one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) from a configuration."),ejo=l(),jd=a("p"),ojo=o(`Note:
Loading a model from its configuration file does `),Vle=a("strong"),rjo=o("not"),tjo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Wle=a("code"),ajo=o("from_pretrained()"),njo=o("to load the model weights."),sjo=l(),Qle=a("p"),ljo=o("Examples:"),ijo=l(),f(X3.$$.fragment),djo=l(),We=a("div"),f(z3.$$.fragment),cjo=l(),Hle=a("p"),fjo=o("Instantiate one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) from a pretrained model."),mjo=l(),on=a("p"),gjo=o("The model class to instantiate is selected based on the "),Ule=a("code"),hjo=o("model_type"),pjo=o(` property of the config object (either
passed as an argument or loaded from `),Jle=a("code"),_jo=o("pretrained_model_name_or_path"),ujo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Yle=a("code"),bjo=o("pretrained_model_name_or_path"),vjo=o(":"),Tjo=l(),V3=a("ul"),ev=a("li"),Kle=a("strong"),Fjo=o("speech-encoder-decoder"),Cjo=o(" \u2014 "),vN=a("a"),Mjo=o("SpeechEncoderDecoderModel"),Ejo=o(" (Speech Encoder decoder model)"),yjo=l(),ov=a("li"),Zle=a("strong"),wjo=o("speech_to_text"),Ajo=o(" \u2014 "),TN=a("a"),Ljo=o("Speech2TextForConditionalGeneration"),Bjo=o(" (Speech2Text model)"),kjo=l(),rv=a("p"),xjo=o("The model is set in evaluation mode by default using "),eie=a("code"),Rjo=o("model.eval()"),Sjo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),oie=a("code"),Pjo=o("model.train()"),$jo=l(),rie=a("p"),Ijo=o("Examples:"),jjo=l(),f(W3.$$.fragment),JLe=l(),Nd=a("h2"),tv=a("a"),tie=a("span"),f(Q3.$$.fragment),Njo=l(),aie=a("span"),Djo=o("AutoModelForAudioXVector"),YLe=l(),ir=a("div"),f(H3.$$.fragment),qjo=l(),Dd=a("p"),Gjo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a audio retrieval via x-vector head) when created
with the `),nie=a("code"),Ojo=o("from_pretrained()"),Xjo=o("class method or the "),sie=a("code"),zjo=o("from_config()"),Vjo=o(`class
method.`),Wjo=l(),U3=a("p"),Qjo=o("This class cannot be instantiated directly using "),lie=a("code"),Hjo=o("__init__()"),Ujo=o(" (throws an error)."),Jjo=l(),ot=a("div"),f(J3.$$.fragment),Yjo=l(),iie=a("p"),Kjo=o("Instantiates one of the model classes of the library (with a audio retrieval via x-vector head) from a configuration."),Zjo=l(),qd=a("p"),eNo=o(`Note:
Loading a model from its configuration file does `),die=a("strong"),oNo=o("not"),rNo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),cie=a("code"),tNo=o("from_pretrained()"),aNo=o("to load the model weights."),nNo=l(),fie=a("p"),sNo=o("Examples:"),lNo=l(),f(Y3.$$.fragment),iNo=l(),Qe=a("div"),f(K3.$$.fragment),dNo=l(),mie=a("p"),cNo=o("Instantiate one of the model classes of the library (with a audio retrieval via x-vector head) from a pretrained model."),fNo=l(),rn=a("p"),mNo=o("The model class to instantiate is selected based on the "),gie=a("code"),gNo=o("model_type"),hNo=o(` property of the config object (either
passed as an argument or loaded from `),hie=a("code"),pNo=o("pretrained_model_name_or_path"),_No=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),pie=a("code"),uNo=o("pretrained_model_name_or_path"),bNo=o(":"),vNo=l(),Gd=a("ul"),av=a("li"),_ie=a("strong"),TNo=o("unispeech-sat"),FNo=o(" \u2014 "),FN=a("a"),CNo=o("UniSpeechSatForXVector"),MNo=o(" (UniSpeechSat model)"),ENo=l(),nv=a("li"),uie=a("strong"),yNo=o("wav2vec2"),wNo=o(" \u2014 "),CN=a("a"),ANo=o("Wav2Vec2ForXVector"),LNo=o(" (Wav2Vec2 model)"),BNo=l(),sv=a("li"),bie=a("strong"),kNo=o("wavlm"),xNo=o(" \u2014 "),MN=a("a"),RNo=o("WavLMForXVector"),SNo=o(" (WavLM model)"),PNo=l(),lv=a("p"),$No=o("The model is set in evaluation mode by default using "),vie=a("code"),INo=o("model.eval()"),jNo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Tie=a("code"),NNo=o("model.train()"),DNo=l(),Fie=a("p"),qNo=o("Examples:"),GNo=l(),f(Z3.$$.fragment),KLe=l(),Od=a("h2"),iv=a("a"),Cie=a("span"),f(ey.$$.fragment),ONo=l(),Mie=a("span"),XNo=o("AutoModelForMaskedImageModeling"),ZLe=l(),dr=a("div"),f(oy.$$.fragment),zNo=l(),Xd=a("p"),VNo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a masked image modeling head) when created
with the `),Eie=a("code"),WNo=o("from_pretrained()"),QNo=o("class method or the "),yie=a("code"),HNo=o("from_config()"),UNo=o(`class
method.`),JNo=l(),ry=a("p"),YNo=o("This class cannot be instantiated directly using "),wie=a("code"),KNo=o("__init__()"),ZNo=o(" (throws an error)."),eDo=l(),rt=a("div"),f(ty.$$.fragment),oDo=l(),Aie=a("p"),rDo=o("Instantiates one of the model classes of the library (with a masked image modeling head) from a configuration."),tDo=l(),zd=a("p"),aDo=o(`Note:
Loading a model from its configuration file does `),Lie=a("strong"),nDo=o("not"),sDo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Bie=a("code"),lDo=o("from_pretrained()"),iDo=o("to load the model weights."),dDo=l(),kie=a("p"),cDo=o("Examples:"),fDo=l(),f(ay.$$.fragment),mDo=l(),He=a("div"),f(ny.$$.fragment),gDo=l(),xie=a("p"),hDo=o("Instantiate one of the model classes of the library (with a masked image modeling head) from a pretrained model."),pDo=l(),tn=a("p"),_Do=o("The model class to instantiate is selected based on the "),Rie=a("code"),uDo=o("model_type"),bDo=o(` property of the config object (either
passed as an argument or loaded from `),Sie=a("code"),vDo=o("pretrained_model_name_or_path"),TDo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Pie=a("code"),FDo=o("pretrained_model_name_or_path"),CDo=o(":"),MDo=l(),Vd=a("ul"),dv=a("li"),$ie=a("strong"),EDo=o("deit"),yDo=o(" \u2014 "),EN=a("a"),wDo=o("DeiTForMaskedImageModeling"),ADo=o(" (DeiT model)"),LDo=l(),cv=a("li"),Iie=a("strong"),BDo=o("swin"),kDo=o(" \u2014 "),yN=a("a"),xDo=o("SwinForMaskedImageModeling"),RDo=o(" (Swin model)"),SDo=l(),fv=a("li"),jie=a("strong"),PDo=o("vit"),$Do=o(" \u2014 "),wN=a("a"),IDo=o("ViTForMaskedImageModeling"),jDo=o(" (ViT model)"),NDo=l(),mv=a("p"),DDo=o("The model is set in evaluation mode by default using "),Nie=a("code"),qDo=o("model.eval()"),GDo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Die=a("code"),ODo=o("model.train()"),XDo=l(),qie=a("p"),zDo=o("Examples:"),VDo=l(),f(sy.$$.fragment),e8e=l(),Wd=a("h2"),gv=a("a"),Gie=a("span"),f(ly.$$.fragment),WDo=l(),Oie=a("span"),QDo=o("AutoModelForObjectDetection"),o8e=l(),cr=a("div"),f(iy.$$.fragment),HDo=l(),Qd=a("p"),UDo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a object detection head) when created
with the `),Xie=a("code"),JDo=o("from_pretrained()"),YDo=o("class method or the "),zie=a("code"),KDo=o("from_config()"),ZDo=o(`class
method.`),eqo=l(),dy=a("p"),oqo=o("This class cannot be instantiated directly using "),Vie=a("code"),rqo=o("__init__()"),tqo=o(" (throws an error)."),aqo=l(),tt=a("div"),f(cy.$$.fragment),nqo=l(),Wie=a("p"),sqo=o("Instantiates one of the model classes of the library (with a object detection head) from a configuration."),lqo=l(),Hd=a("p"),iqo=o(`Note:
Loading a model from its configuration file does `),Qie=a("strong"),dqo=o("not"),cqo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Hie=a("code"),fqo=o("from_pretrained()"),mqo=o("to load the model weights."),gqo=l(),Uie=a("p"),hqo=o("Examples:"),pqo=l(),f(fy.$$.fragment),_qo=l(),Ue=a("div"),f(my.$$.fragment),uqo=l(),Jie=a("p"),bqo=o("Instantiate one of the model classes of the library (with a object detection head) from a pretrained model."),vqo=l(),an=a("p"),Tqo=o("The model class to instantiate is selected based on the "),Yie=a("code"),Fqo=o("model_type"),Cqo=o(` property of the config object (either
passed as an argument or loaded from `),Kie=a("code"),Mqo=o("pretrained_model_name_or_path"),Eqo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Zie=a("code"),yqo=o("pretrained_model_name_or_path"),wqo=o(":"),Aqo=l(),ede=a("ul"),hv=a("li"),ode=a("strong"),Lqo=o("detr"),Bqo=o(" \u2014 "),AN=a("a"),kqo=o("DetrForObjectDetection"),xqo=o(" (DETR model)"),Rqo=l(),pv=a("p"),Sqo=o("The model is set in evaluation mode by default using "),rde=a("code"),Pqo=o("model.eval()"),$qo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),tde=a("code"),Iqo=o("model.train()"),jqo=l(),ade=a("p"),Nqo=o("Examples:"),Dqo=l(),f(gy.$$.fragment),r8e=l(),Ud=a("h2"),_v=a("a"),nde=a("span"),f(hy.$$.fragment),qqo=l(),sde=a("span"),Gqo=o("AutoModelForImageSegmentation"),t8e=l(),fr=a("div"),f(py.$$.fragment),Oqo=l(),Jd=a("p"),Xqo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a image segmentation head) when created
with the `),lde=a("code"),zqo=o("from_pretrained()"),Vqo=o("class method or the "),ide=a("code"),Wqo=o("from_config()"),Qqo=o(`class
method.`),Hqo=l(),_y=a("p"),Uqo=o("This class cannot be instantiated directly using "),dde=a("code"),Jqo=o("__init__()"),Yqo=o(" (throws an error)."),Kqo=l(),at=a("div"),f(uy.$$.fragment),Zqo=l(),cde=a("p"),eGo=o("Instantiates one of the model classes of the library (with a image segmentation head) from a configuration."),oGo=l(),Yd=a("p"),rGo=o(`Note:
Loading a model from its configuration file does `),fde=a("strong"),tGo=o("not"),aGo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),mde=a("code"),nGo=o("from_pretrained()"),sGo=o("to load the model weights."),lGo=l(),gde=a("p"),iGo=o("Examples:"),dGo=l(),f(by.$$.fragment),cGo=l(),Je=a("div"),f(vy.$$.fragment),fGo=l(),hde=a("p"),mGo=o("Instantiate one of the model classes of the library (with a image segmentation head) from a pretrained model."),gGo=l(),nn=a("p"),hGo=o("The model class to instantiate is selected based on the "),pde=a("code"),pGo=o("model_type"),_Go=o(` property of the config object (either
passed as an argument or loaded from `),_de=a("code"),uGo=o("pretrained_model_name_or_path"),bGo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),ude=a("code"),vGo=o("pretrained_model_name_or_path"),TGo=o(":"),FGo=l(),bde=a("ul"),uv=a("li"),vde=a("strong"),CGo=o("detr"),MGo=o(" \u2014 "),LN=a("a"),EGo=o("DetrForSegmentation"),yGo=o(" (DETR model)"),wGo=l(),bv=a("p"),AGo=o("The model is set in evaluation mode by default using "),Tde=a("code"),LGo=o("model.eval()"),BGo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Fde=a("code"),kGo=o("model.train()"),xGo=l(),Cde=a("p"),RGo=o("Examples:"),SGo=l(),f(Ty.$$.fragment),a8e=l(),Kd=a("h2"),vv=a("a"),Mde=a("span"),f(Fy.$$.fragment),PGo=l(),Ede=a("span"),$Go=o("AutoModelForSemanticSegmentation"),n8e=l(),mr=a("div"),f(Cy.$$.fragment),IGo=l(),Zd=a("p"),jGo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a semantic segmentation head) when created
with the `),yde=a("code"),NGo=o("from_pretrained()"),DGo=o("class method or the "),wde=a("code"),qGo=o("from_config()"),GGo=o(`class
method.`),OGo=l(),My=a("p"),XGo=o("This class cannot be instantiated directly using "),Ade=a("code"),zGo=o("__init__()"),VGo=o(" (throws an error)."),WGo=l(),nt=a("div"),f(Ey.$$.fragment),QGo=l(),Lde=a("p"),HGo=o("Instantiates one of the model classes of the library (with a semantic segmentation head) from a configuration."),UGo=l(),ec=a("p"),JGo=o(`Note:
Loading a model from its configuration file does `),Bde=a("strong"),YGo=o("not"),KGo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),kde=a("code"),ZGo=o("from_pretrained()"),eOo=o("to load the model weights."),oOo=l(),xde=a("p"),rOo=o("Examples:"),tOo=l(),f(yy.$$.fragment),aOo=l(),Ye=a("div"),f(wy.$$.fragment),nOo=l(),Rde=a("p"),sOo=o("Instantiate one of the model classes of the library (with a semantic segmentation head) from a pretrained model."),lOo=l(),sn=a("p"),iOo=o("The model class to instantiate is selected based on the "),Sde=a("code"),dOo=o("model_type"),cOo=o(` property of the config object (either
passed as an argument or loaded from `),Pde=a("code"),fOo=o("pretrained_model_name_or_path"),mOo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),$de=a("code"),gOo=o("pretrained_model_name_or_path"),hOo=o(":"),pOo=l(),Ay=a("ul"),Tv=a("li"),Ide=a("strong"),_Oo=o("beit"),uOo=o(" \u2014 "),BN=a("a"),bOo=o("BeitForSemanticSegmentation"),vOo=o(" (BEiT model)"),TOo=l(),Fv=a("li"),jde=a("strong"),FOo=o("segformer"),COo=o(" \u2014 "),kN=a("a"),MOo=o("SegformerForSemanticSegmentation"),EOo=o(" (SegFormer model)"),yOo=l(),Cv=a("p"),wOo=o("The model is set in evaluation mode by default using "),Nde=a("code"),AOo=o("model.eval()"),LOo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Dde=a("code"),BOo=o("model.train()"),kOo=l(),qde=a("p"),xOo=o("Examples:"),ROo=l(),f(Ly.$$.fragment),s8e=l(),oc=a("h2"),Mv=a("a"),Gde=a("span"),f(By.$$.fragment),SOo=l(),Ode=a("span"),POo=o("TFAutoModel"),l8e=l(),gr=a("div"),f(ky.$$.fragment),$Oo=l(),rc=a("p"),IOo=o(`This is a generic model class that will be instantiated as one of the base model classes of the library when created
with the `),Xde=a("code"),jOo=o("from_pretrained()"),NOo=o("class method or the "),zde=a("code"),DOo=o("from_config()"),qOo=o(`class
method.`),GOo=l(),xy=a("p"),OOo=o("This class cannot be instantiated directly using "),Vde=a("code"),XOo=o("__init__()"),zOo=o(" (throws an error)."),VOo=l(),st=a("div"),f(Ry.$$.fragment),WOo=l(),Wde=a("p"),QOo=o("Instantiates one of the base model classes of the library from a configuration."),HOo=l(),tc=a("p"),UOo=o(`Note:
Loading a model from its configuration file does `),Qde=a("strong"),JOo=o("not"),YOo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Hde=a("code"),KOo=o("from_pretrained()"),ZOo=o("to load the model weights."),eXo=l(),Ude=a("p"),oXo=o("Examples:"),rXo=l(),f(Sy.$$.fragment),tXo=l(),go=a("div"),f(Py.$$.fragment),aXo=l(),Jde=a("p"),nXo=o("Instantiate one of the base model classes of the library from a pretrained model."),sXo=l(),ln=a("p"),lXo=o("The model class to instantiate is selected based on the "),Yde=a("code"),iXo=o("model_type"),dXo=o(` property of the config object (either
passed as an argument or loaded from `),Kde=a("code"),cXo=o("pretrained_model_name_or_path"),fXo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Zde=a("code"),mXo=o("pretrained_model_name_or_path"),gXo=o(":"),hXo=l(),B=a("ul"),Ev=a("li"),ece=a("strong"),pXo=o("albert"),_Xo=o(" \u2014 "),xN=a("a"),uXo=o("TFAlbertModel"),bXo=o(" (ALBERT model)"),vXo=l(),yv=a("li"),oce=a("strong"),TXo=o("bart"),FXo=o(" \u2014 "),RN=a("a"),CXo=o("TFBartModel"),MXo=o(" (BART model)"),EXo=l(),wv=a("li"),rce=a("strong"),yXo=o("bert"),wXo=o(" \u2014 "),SN=a("a"),AXo=o("TFBertModel"),LXo=o(" (BERT model)"),BXo=l(),Av=a("li"),tce=a("strong"),kXo=o("blenderbot"),xXo=o(" \u2014 "),PN=a("a"),RXo=o("TFBlenderbotModel"),SXo=o(" (Blenderbot model)"),PXo=l(),Lv=a("li"),ace=a("strong"),$Xo=o("blenderbot-small"),IXo=o(" \u2014 "),$N=a("a"),jXo=o("TFBlenderbotSmallModel"),NXo=o(" (BlenderbotSmall model)"),DXo=l(),Bv=a("li"),nce=a("strong"),qXo=o("camembert"),GXo=o(" \u2014 "),IN=a("a"),OXo=o("TFCamembertModel"),XXo=o(" (CamemBERT model)"),zXo=l(),kv=a("li"),sce=a("strong"),VXo=o("clip"),WXo=o(" \u2014 "),jN=a("a"),QXo=o("TFCLIPModel"),HXo=o(" (CLIP model)"),UXo=l(),xv=a("li"),lce=a("strong"),JXo=o("convbert"),YXo=o(" \u2014 "),NN=a("a"),KXo=o("TFConvBertModel"),ZXo=o(" (ConvBERT model)"),ezo=l(),Rv=a("li"),ice=a("strong"),ozo=o("ctrl"),rzo=o(" \u2014 "),DN=a("a"),tzo=o("TFCTRLModel"),azo=o(" (CTRL model)"),nzo=l(),Sv=a("li"),dce=a("strong"),szo=o("deberta"),lzo=o(" \u2014 "),qN=a("a"),izo=o("TFDebertaModel"),dzo=o(" (DeBERTa model)"),czo=l(),Pv=a("li"),cce=a("strong"),fzo=o("deberta-v2"),mzo=o(" \u2014 "),GN=a("a"),gzo=o("TFDebertaV2Model"),hzo=o(" (DeBERTa-v2 model)"),pzo=l(),$v=a("li"),fce=a("strong"),_zo=o("distilbert"),uzo=o(" \u2014 "),ON=a("a"),bzo=o("TFDistilBertModel"),vzo=o(" (DistilBERT model)"),Tzo=l(),Iv=a("li"),mce=a("strong"),Fzo=o("dpr"),Czo=o(" \u2014 "),XN=a("a"),Mzo=o("TFDPRQuestionEncoder"),Ezo=o(" (DPR model)"),yzo=l(),jv=a("li"),gce=a("strong"),wzo=o("electra"),Azo=o(" \u2014 "),zN=a("a"),Lzo=o("TFElectraModel"),Bzo=o(" (ELECTRA model)"),kzo=l(),Nv=a("li"),hce=a("strong"),xzo=o("flaubert"),Rzo=o(" \u2014 "),VN=a("a"),Szo=o("TFFlaubertModel"),Pzo=o(" (FlauBERT model)"),$zo=l(),Ss=a("li"),pce=a("strong"),Izo=o("funnel"),jzo=o(" \u2014 "),WN=a("a"),Nzo=o("TFFunnelModel"),Dzo=o(" or "),QN=a("a"),qzo=o("TFFunnelBaseModel"),Gzo=o(" (Funnel Transformer model)"),Ozo=l(),Dv=a("li"),_ce=a("strong"),Xzo=o("gpt2"),zzo=o(" \u2014 "),HN=a("a"),Vzo=o("TFGPT2Model"),Wzo=o(" (OpenAI GPT-2 model)"),Qzo=l(),qv=a("li"),uce=a("strong"),Hzo=o("hubert"),Uzo=o(" \u2014 "),UN=a("a"),Jzo=o("TFHubertModel"),Yzo=o(" (Hubert model)"),Kzo=l(),Gv=a("li"),bce=a("strong"),Zzo=o("layoutlm"),eVo=o(" \u2014 "),JN=a("a"),oVo=o("TFLayoutLMModel"),rVo=o(" (LayoutLM model)"),tVo=l(),Ov=a("li"),vce=a("strong"),aVo=o("led"),nVo=o(" \u2014 "),YN=a("a"),sVo=o("TFLEDModel"),lVo=o(" (LED model)"),iVo=l(),Xv=a("li"),Tce=a("strong"),dVo=o("longformer"),cVo=o(" \u2014 "),KN=a("a"),fVo=o("TFLongformerModel"),mVo=o(" (Longformer model)"),gVo=l(),zv=a("li"),Fce=a("strong"),hVo=o("lxmert"),pVo=o(" \u2014 "),ZN=a("a"),_Vo=o("TFLxmertModel"),uVo=o(" (LXMERT model)"),bVo=l(),Vv=a("li"),Cce=a("strong"),vVo=o("marian"),TVo=o(" \u2014 "),eD=a("a"),FVo=o("TFMarianModel"),CVo=o(" (Marian model)"),MVo=l(),Wv=a("li"),Mce=a("strong"),EVo=o("mbart"),yVo=o(" \u2014 "),oD=a("a"),wVo=o("TFMBartModel"),AVo=o(" (mBART model)"),LVo=l(),Qv=a("li"),Ece=a("strong"),BVo=o("mobilebert"),kVo=o(" \u2014 "),rD=a("a"),xVo=o("TFMobileBertModel"),RVo=o(" (MobileBERT model)"),SVo=l(),Hv=a("li"),yce=a("strong"),PVo=o("mpnet"),$Vo=o(" \u2014 "),tD=a("a"),IVo=o("TFMPNetModel"),jVo=o(" (MPNet model)"),NVo=l(),Uv=a("li"),wce=a("strong"),DVo=o("mt5"),qVo=o(" \u2014 "),aD=a("a"),GVo=o("TFMT5Model"),OVo=o(" (mT5 model)"),XVo=l(),Jv=a("li"),Ace=a("strong"),zVo=o("openai-gpt"),VVo=o(" \u2014 "),nD=a("a"),WVo=o("TFOpenAIGPTModel"),QVo=o(" (OpenAI GPT model)"),HVo=l(),Yv=a("li"),Lce=a("strong"),UVo=o("pegasus"),JVo=o(" \u2014 "),sD=a("a"),YVo=o("TFPegasusModel"),KVo=o(" (Pegasus model)"),ZVo=l(),Kv=a("li"),Bce=a("strong"),eWo=o("rembert"),oWo=o(" \u2014 "),lD=a("a"),rWo=o("TFRemBertModel"),tWo=o(" (RemBERT model)"),aWo=l(),Zv=a("li"),kce=a("strong"),nWo=o("roberta"),sWo=o(" \u2014 "),iD=a("a"),lWo=o("TFRobertaModel"),iWo=o(" (RoBERTa model)"),dWo=l(),eT=a("li"),xce=a("strong"),cWo=o("roformer"),fWo=o(" \u2014 "),dD=a("a"),mWo=o("TFRoFormerModel"),gWo=o(" (RoFormer model)"),hWo=l(),oT=a("li"),Rce=a("strong"),pWo=o("speech_to_text"),_Wo=o(" \u2014 "),cD=a("a"),uWo=o("TFSpeech2TextModel"),bWo=o(" (Speech2Text model)"),vWo=l(),rT=a("li"),Sce=a("strong"),TWo=o("t5"),FWo=o(" \u2014 "),fD=a("a"),CWo=o("TFT5Model"),MWo=o(" (T5 model)"),EWo=l(),tT=a("li"),Pce=a("strong"),yWo=o("tapas"),wWo=o(" \u2014 "),mD=a("a"),AWo=o("TFTapasModel"),LWo=o(" (TAPAS model)"),BWo=l(),aT=a("li"),$ce=a("strong"),kWo=o("transfo-xl"),xWo=o(" \u2014 "),gD=a("a"),RWo=o("TFTransfoXLModel"),SWo=o(" (Transformer-XL model)"),PWo=l(),nT=a("li"),Ice=a("strong"),$Wo=o("vit"),IWo=o(" \u2014 "),hD=a("a"),jWo=o("TFViTModel"),NWo=o(" (ViT model)"),DWo=l(),sT=a("li"),jce=a("strong"),qWo=o("wav2vec2"),GWo=o(" \u2014 "),pD=a("a"),OWo=o("TFWav2Vec2Model"),XWo=o(" (Wav2Vec2 model)"),zWo=l(),lT=a("li"),Nce=a("strong"),VWo=o("xlm"),WWo=o(" \u2014 "),_D=a("a"),QWo=o("TFXLMModel"),HWo=o(" (XLM model)"),UWo=l(),iT=a("li"),Dce=a("strong"),JWo=o("xlm-roberta"),YWo=o(" \u2014 "),uD=a("a"),KWo=o("TFXLMRobertaModel"),ZWo=o(" (XLM-RoBERTa model)"),eQo=l(),dT=a("li"),qce=a("strong"),oQo=o("xlnet"),rQo=o(" \u2014 "),bD=a("a"),tQo=o("TFXLNetModel"),aQo=o(" (XLNet model)"),nQo=l(),Gce=a("p"),sQo=o("Examples:"),lQo=l(),f($y.$$.fragment),i8e=l(),ac=a("h2"),cT=a("a"),Oce=a("span"),f(Iy.$$.fragment),iQo=l(),Xce=a("span"),dQo=o("TFAutoModelForPreTraining"),d8e=l(),hr=a("div"),f(jy.$$.fragment),cQo=l(),nc=a("p"),fQo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a pretraining head) when created
with the `),zce=a("code"),mQo=o("from_pretrained()"),gQo=o("class method or the "),Vce=a("code"),hQo=o("from_config()"),pQo=o(`class
method.`),_Qo=l(),Ny=a("p"),uQo=o("This class cannot be instantiated directly using "),Wce=a("code"),bQo=o("__init__()"),vQo=o(" (throws an error)."),TQo=l(),lt=a("div"),f(Dy.$$.fragment),FQo=l(),Qce=a("p"),CQo=o("Instantiates one of the model classes of the library (with a pretraining head) from a configuration."),MQo=l(),sc=a("p"),EQo=o(`Note:
Loading a model from its configuration file does `),Hce=a("strong"),yQo=o("not"),wQo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Uce=a("code"),AQo=o("from_pretrained()"),LQo=o("to load the model weights."),BQo=l(),Jce=a("p"),kQo=o("Examples:"),xQo=l(),f(qy.$$.fragment),RQo=l(),ho=a("div"),f(Gy.$$.fragment),SQo=l(),Yce=a("p"),PQo=o("Instantiate one of the model classes of the library (with a pretraining head) from a pretrained model."),$Qo=l(),dn=a("p"),IQo=o("The model class to instantiate is selected based on the "),Kce=a("code"),jQo=o("model_type"),NQo=o(` property of the config object (either
passed as an argument or loaded from `),Zce=a("code"),DQo=o("pretrained_model_name_or_path"),qQo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),efe=a("code"),GQo=o("pretrained_model_name_or_path"),OQo=o(":"),XQo=l(),H=a("ul"),fT=a("li"),ofe=a("strong"),zQo=o("albert"),VQo=o(" \u2014 "),vD=a("a"),WQo=o("TFAlbertForPreTraining"),QQo=o(" (ALBERT model)"),HQo=l(),mT=a("li"),rfe=a("strong"),UQo=o("bart"),JQo=o(" \u2014 "),TD=a("a"),YQo=o("TFBartForConditionalGeneration"),KQo=o(" (BART model)"),ZQo=l(),gT=a("li"),tfe=a("strong"),eHo=o("bert"),oHo=o(" \u2014 "),FD=a("a"),rHo=o("TFBertForPreTraining"),tHo=o(" (BERT model)"),aHo=l(),hT=a("li"),afe=a("strong"),nHo=o("camembert"),sHo=o(" \u2014 "),CD=a("a"),lHo=o("TFCamembertForMaskedLM"),iHo=o(" (CamemBERT model)"),dHo=l(),pT=a("li"),nfe=a("strong"),cHo=o("ctrl"),fHo=o(" \u2014 "),MD=a("a"),mHo=o("TFCTRLLMHeadModel"),gHo=o(" (CTRL model)"),hHo=l(),_T=a("li"),sfe=a("strong"),pHo=o("distilbert"),_Ho=o(" \u2014 "),ED=a("a"),uHo=o("TFDistilBertForMaskedLM"),bHo=o(" (DistilBERT model)"),vHo=l(),uT=a("li"),lfe=a("strong"),THo=o("electra"),FHo=o(" \u2014 "),yD=a("a"),CHo=o("TFElectraForPreTraining"),MHo=o(" (ELECTRA model)"),EHo=l(),bT=a("li"),ife=a("strong"),yHo=o("flaubert"),wHo=o(" \u2014 "),wD=a("a"),AHo=o("TFFlaubertWithLMHeadModel"),LHo=o(" (FlauBERT model)"),BHo=l(),vT=a("li"),dfe=a("strong"),kHo=o("funnel"),xHo=o(" \u2014 "),AD=a("a"),RHo=o("TFFunnelForPreTraining"),SHo=o(" (Funnel Transformer model)"),PHo=l(),TT=a("li"),cfe=a("strong"),$Ho=o("gpt2"),IHo=o(" \u2014 "),LD=a("a"),jHo=o("TFGPT2LMHeadModel"),NHo=o(" (OpenAI GPT-2 model)"),DHo=l(),FT=a("li"),ffe=a("strong"),qHo=o("layoutlm"),GHo=o(" \u2014 "),BD=a("a"),OHo=o("TFLayoutLMForMaskedLM"),XHo=o(" (LayoutLM model)"),zHo=l(),CT=a("li"),mfe=a("strong"),VHo=o("lxmert"),WHo=o(" \u2014 "),kD=a("a"),QHo=o("TFLxmertForPreTraining"),HHo=o(" (LXMERT model)"),UHo=l(),MT=a("li"),gfe=a("strong"),JHo=o("mobilebert"),YHo=o(" \u2014 "),xD=a("a"),KHo=o("TFMobileBertForPreTraining"),ZHo=o(" (MobileBERT model)"),eUo=l(),ET=a("li"),hfe=a("strong"),oUo=o("mpnet"),rUo=o(" \u2014 "),RD=a("a"),tUo=o("TFMPNetForMaskedLM"),aUo=o(" (MPNet model)"),nUo=l(),yT=a("li"),pfe=a("strong"),sUo=o("openai-gpt"),lUo=o(" \u2014 "),SD=a("a"),iUo=o("TFOpenAIGPTLMHeadModel"),dUo=o(" (OpenAI GPT model)"),cUo=l(),wT=a("li"),_fe=a("strong"),fUo=o("roberta"),mUo=o(" \u2014 "),PD=a("a"),gUo=o("TFRobertaForMaskedLM"),hUo=o(" (RoBERTa model)"),pUo=l(),AT=a("li"),ufe=a("strong"),_Uo=o("t5"),uUo=o(" \u2014 "),$D=a("a"),bUo=o("TFT5ForConditionalGeneration"),vUo=o(" (T5 model)"),TUo=l(),LT=a("li"),bfe=a("strong"),FUo=o("tapas"),CUo=o(" \u2014 "),ID=a("a"),MUo=o("TFTapasForMaskedLM"),EUo=o(" (TAPAS model)"),yUo=l(),BT=a("li"),vfe=a("strong"),wUo=o("transfo-xl"),AUo=o(" \u2014 "),jD=a("a"),LUo=o("TFTransfoXLLMHeadModel"),BUo=o(" (Transformer-XL model)"),kUo=l(),kT=a("li"),Tfe=a("strong"),xUo=o("xlm"),RUo=o(" \u2014 "),ND=a("a"),SUo=o("TFXLMWithLMHeadModel"),PUo=o(" (XLM model)"),$Uo=l(),xT=a("li"),Ffe=a("strong"),IUo=o("xlm-roberta"),jUo=o(" \u2014 "),DD=a("a"),NUo=o("TFXLMRobertaForMaskedLM"),DUo=o(" (XLM-RoBERTa model)"),qUo=l(),RT=a("li"),Cfe=a("strong"),GUo=o("xlnet"),OUo=o(" \u2014 "),qD=a("a"),XUo=o("TFXLNetLMHeadModel"),zUo=o(" (XLNet model)"),VUo=l(),Mfe=a("p"),WUo=o("Examples:"),QUo=l(),f(Oy.$$.fragment),c8e=l(),lc=a("h2"),ST=a("a"),Efe=a("span"),f(Xy.$$.fragment),HUo=l(),yfe=a("span"),UUo=o("TFAutoModelForCausalLM"),f8e=l(),pr=a("div"),f(zy.$$.fragment),JUo=l(),ic=a("p"),YUo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a causal language modeling head) when created
with the `),wfe=a("code"),KUo=o("from_pretrained()"),ZUo=o("class method or the "),Afe=a("code"),eJo=o("from_config()"),oJo=o(`class
method.`),rJo=l(),Vy=a("p"),tJo=o("This class cannot be instantiated directly using "),Lfe=a("code"),aJo=o("__init__()"),nJo=o(" (throws an error)."),sJo=l(),it=a("div"),f(Wy.$$.fragment),lJo=l(),Bfe=a("p"),iJo=o("Instantiates one of the model classes of the library (with a causal language modeling head) from a configuration."),dJo=l(),dc=a("p"),cJo=o(`Note:
Loading a model from its configuration file does `),kfe=a("strong"),fJo=o("not"),mJo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),xfe=a("code"),gJo=o("from_pretrained()"),hJo=o("to load the model weights."),pJo=l(),Rfe=a("p"),_Jo=o("Examples:"),uJo=l(),f(Qy.$$.fragment),bJo=l(),po=a("div"),f(Hy.$$.fragment),vJo=l(),Sfe=a("p"),TJo=o("Instantiate one of the model classes of the library (with a causal language modeling head) from a pretrained model."),FJo=l(),cn=a("p"),CJo=o("The model class to instantiate is selected based on the "),Pfe=a("code"),MJo=o("model_type"),EJo=o(` property of the config object (either
passed as an argument or loaded from `),$fe=a("code"),yJo=o("pretrained_model_name_or_path"),wJo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Ife=a("code"),AJo=o("pretrained_model_name_or_path"),LJo=o(":"),BJo=l(),he=a("ul"),PT=a("li"),jfe=a("strong"),kJo=o("bert"),xJo=o(" \u2014 "),GD=a("a"),RJo=o("TFBertLMHeadModel"),SJo=o(" (BERT model)"),PJo=l(),$T=a("li"),Nfe=a("strong"),$Jo=o("ctrl"),IJo=o(" \u2014 "),OD=a("a"),jJo=o("TFCTRLLMHeadModel"),NJo=o(" (CTRL model)"),DJo=l(),IT=a("li"),Dfe=a("strong"),qJo=o("gpt2"),GJo=o(" \u2014 "),XD=a("a"),OJo=o("TFGPT2LMHeadModel"),XJo=o(" (OpenAI GPT-2 model)"),zJo=l(),jT=a("li"),qfe=a("strong"),VJo=o("openai-gpt"),WJo=o(" \u2014 "),zD=a("a"),QJo=o("TFOpenAIGPTLMHeadModel"),HJo=o(" (OpenAI GPT model)"),UJo=l(),NT=a("li"),Gfe=a("strong"),JJo=o("rembert"),YJo=o(" \u2014 "),VD=a("a"),KJo=o("TFRemBertForCausalLM"),ZJo=o(" (RemBERT model)"),eYo=l(),DT=a("li"),Ofe=a("strong"),oYo=o("roberta"),rYo=o(" \u2014 "),WD=a("a"),tYo=o("TFRobertaForCausalLM"),aYo=o(" (RoBERTa model)"),nYo=l(),qT=a("li"),Xfe=a("strong"),sYo=o("roformer"),lYo=o(" \u2014 "),QD=a("a"),iYo=o("TFRoFormerForCausalLM"),dYo=o(" (RoFormer model)"),cYo=l(),GT=a("li"),zfe=a("strong"),fYo=o("transfo-xl"),mYo=o(" \u2014 "),HD=a("a"),gYo=o("TFTransfoXLLMHeadModel"),hYo=o(" (Transformer-XL model)"),pYo=l(),OT=a("li"),Vfe=a("strong"),_Yo=o("xlm"),uYo=o(" \u2014 "),UD=a("a"),bYo=o("TFXLMWithLMHeadModel"),vYo=o(" (XLM model)"),TYo=l(),XT=a("li"),Wfe=a("strong"),FYo=o("xlnet"),CYo=o(" \u2014 "),JD=a("a"),MYo=o("TFXLNetLMHeadModel"),EYo=o(" (XLNet model)"),yYo=l(),Qfe=a("p"),wYo=o("Examples:"),AYo=l(),f(Uy.$$.fragment),m8e=l(),cc=a("h2"),zT=a("a"),Hfe=a("span"),f(Jy.$$.fragment),LYo=l(),Ufe=a("span"),BYo=o("TFAutoModelForImageClassification"),g8e=l(),_r=a("div"),f(Yy.$$.fragment),kYo=l(),fc=a("p"),xYo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a image classification head) when created
with the `),Jfe=a("code"),RYo=o("from_pretrained()"),SYo=o("class method or the "),Yfe=a("code"),PYo=o("from_config()"),$Yo=o(`class
method.`),IYo=l(),Ky=a("p"),jYo=o("This class cannot be instantiated directly using "),Kfe=a("code"),NYo=o("__init__()"),DYo=o(" (throws an error)."),qYo=l(),dt=a("div"),f(Zy.$$.fragment),GYo=l(),Zfe=a("p"),OYo=o("Instantiates one of the model classes of the library (with a image classification head) from a configuration."),XYo=l(),mc=a("p"),zYo=o(`Note:
Loading a model from its configuration file does `),eme=a("strong"),VYo=o("not"),WYo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),ome=a("code"),QYo=o("from_pretrained()"),HYo=o("to load the model weights."),UYo=l(),rme=a("p"),JYo=o("Examples:"),YYo=l(),f(ew.$$.fragment),KYo=l(),_o=a("div"),f(ow.$$.fragment),ZYo=l(),tme=a("p"),eKo=o("Instantiate one of the model classes of the library (with a image classification head) from a pretrained model."),oKo=l(),fn=a("p"),rKo=o("The model class to instantiate is selected based on the "),ame=a("code"),tKo=o("model_type"),aKo=o(` property of the config object (either
passed as an argument or loaded from `),nme=a("code"),nKo=o("pretrained_model_name_or_path"),sKo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),sme=a("code"),lKo=o("pretrained_model_name_or_path"),iKo=o(":"),dKo=l(),lme=a("ul"),VT=a("li"),ime=a("strong"),cKo=o("vit"),fKo=o(" \u2014 "),YD=a("a"),mKo=o("TFViTForImageClassification"),gKo=o(" (ViT model)"),hKo=l(),dme=a("p"),pKo=o("Examples:"),_Ko=l(),f(rw.$$.fragment),h8e=l(),gc=a("h2"),WT=a("a"),cme=a("span"),f(tw.$$.fragment),uKo=l(),fme=a("span"),bKo=o("TFAutoModelForMaskedLM"),p8e=l(),ur=a("div"),f(aw.$$.fragment),vKo=l(),hc=a("p"),TKo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a masked language modeling head) when created
with the `),mme=a("code"),FKo=o("from_pretrained()"),CKo=o("class method or the "),gme=a("code"),MKo=o("from_config()"),EKo=o(`class
method.`),yKo=l(),nw=a("p"),wKo=o("This class cannot be instantiated directly using "),hme=a("code"),AKo=o("__init__()"),LKo=o(" (throws an error)."),BKo=l(),ct=a("div"),f(sw.$$.fragment),kKo=l(),pme=a("p"),xKo=o("Instantiates one of the model classes of the library (with a masked language modeling head) from a configuration."),RKo=l(),pc=a("p"),SKo=o(`Note:
Loading a model from its configuration file does `),_me=a("strong"),PKo=o("not"),$Ko=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),ume=a("code"),IKo=o("from_pretrained()"),jKo=o("to load the model weights."),NKo=l(),bme=a("p"),DKo=o("Examples:"),qKo=l(),f(lw.$$.fragment),GKo=l(),uo=a("div"),f(iw.$$.fragment),OKo=l(),vme=a("p"),XKo=o("Instantiate one of the model classes of the library (with a masked language modeling head) from a pretrained model."),zKo=l(),mn=a("p"),VKo=o("The model class to instantiate is selected based on the "),Tme=a("code"),WKo=o("model_type"),QKo=o(` property of the config object (either
passed as an argument or loaded from `),Fme=a("code"),HKo=o("pretrained_model_name_or_path"),UKo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Cme=a("code"),JKo=o("pretrained_model_name_or_path"),YKo=o(":"),KKo=l(),Y=a("ul"),QT=a("li"),Mme=a("strong"),ZKo=o("albert"),eZo=o(" \u2014 "),KD=a("a"),oZo=o("TFAlbertForMaskedLM"),rZo=o(" (ALBERT model)"),tZo=l(),HT=a("li"),Eme=a("strong"),aZo=o("bert"),nZo=o(" \u2014 "),ZD=a("a"),sZo=o("TFBertForMaskedLM"),lZo=o(" (BERT model)"),iZo=l(),UT=a("li"),yme=a("strong"),dZo=o("camembert"),cZo=o(" \u2014 "),eq=a("a"),fZo=o("TFCamembertForMaskedLM"),mZo=o(" (CamemBERT model)"),gZo=l(),JT=a("li"),wme=a("strong"),hZo=o("convbert"),pZo=o(" \u2014 "),oq=a("a"),_Zo=o("TFConvBertForMaskedLM"),uZo=o(" (ConvBERT model)"),bZo=l(),YT=a("li"),Ame=a("strong"),vZo=o("deberta"),TZo=o(" \u2014 "),rq=a("a"),FZo=o("TFDebertaForMaskedLM"),CZo=o(" (DeBERTa model)"),MZo=l(),KT=a("li"),Lme=a("strong"),EZo=o("deberta-v2"),yZo=o(" \u2014 "),tq=a("a"),wZo=o("TFDebertaV2ForMaskedLM"),AZo=o(" (DeBERTa-v2 model)"),LZo=l(),ZT=a("li"),Bme=a("strong"),BZo=o("distilbert"),kZo=o(" \u2014 "),aq=a("a"),xZo=o("TFDistilBertForMaskedLM"),RZo=o(" (DistilBERT model)"),SZo=l(),e7=a("li"),kme=a("strong"),PZo=o("electra"),$Zo=o(" \u2014 "),nq=a("a"),IZo=o("TFElectraForMaskedLM"),jZo=o(" (ELECTRA model)"),NZo=l(),o7=a("li"),xme=a("strong"),DZo=o("flaubert"),qZo=o(" \u2014 "),sq=a("a"),GZo=o("TFFlaubertWithLMHeadModel"),OZo=o(" (FlauBERT model)"),XZo=l(),r7=a("li"),Rme=a("strong"),zZo=o("funnel"),VZo=o(" \u2014 "),lq=a("a"),WZo=o("TFFunnelForMaskedLM"),QZo=o(" (Funnel Transformer model)"),HZo=l(),t7=a("li"),Sme=a("strong"),UZo=o("layoutlm"),JZo=o(" \u2014 "),iq=a("a"),YZo=o("TFLayoutLMForMaskedLM"),KZo=o(" (LayoutLM model)"),ZZo=l(),a7=a("li"),Pme=a("strong"),eer=o("longformer"),oer=o(" \u2014 "),dq=a("a"),rer=o("TFLongformerForMaskedLM"),ter=o(" (Longformer model)"),aer=l(),n7=a("li"),$me=a("strong"),ner=o("mobilebert"),ser=o(" \u2014 "),cq=a("a"),ler=o("TFMobileBertForMaskedLM"),ier=o(" (MobileBERT model)"),der=l(),s7=a("li"),Ime=a("strong"),cer=o("mpnet"),fer=o(" \u2014 "),fq=a("a"),mer=o("TFMPNetForMaskedLM"),ger=o(" (MPNet model)"),her=l(),l7=a("li"),jme=a("strong"),per=o("rembert"),_er=o(" \u2014 "),mq=a("a"),uer=o("TFRemBertForMaskedLM"),ber=o(" (RemBERT model)"),ver=l(),i7=a("li"),Nme=a("strong"),Ter=o("roberta"),Fer=o(" \u2014 "),gq=a("a"),Cer=o("TFRobertaForMaskedLM"),Mer=o(" (RoBERTa model)"),Eer=l(),d7=a("li"),Dme=a("strong"),yer=o("roformer"),wer=o(" \u2014 "),hq=a("a"),Aer=o("TFRoFormerForMaskedLM"),Ler=o(" (RoFormer model)"),Ber=l(),c7=a("li"),qme=a("strong"),ker=o("tapas"),xer=o(" \u2014 "),pq=a("a"),Rer=o("TFTapasForMaskedLM"),Ser=o(" (TAPAS model)"),Per=l(),f7=a("li"),Gme=a("strong"),$er=o("xlm"),Ier=o(" \u2014 "),_q=a("a"),jer=o("TFXLMWithLMHeadModel"),Ner=o(" (XLM model)"),Der=l(),m7=a("li"),Ome=a("strong"),qer=o("xlm-roberta"),Ger=o(" \u2014 "),uq=a("a"),Oer=o("TFXLMRobertaForMaskedLM"),Xer=o(" (XLM-RoBERTa model)"),zer=l(),Xme=a("p"),Ver=o("Examples:"),Wer=l(),f(dw.$$.fragment),_8e=l(),_c=a("h2"),g7=a("a"),zme=a("span"),f(cw.$$.fragment),Qer=l(),Vme=a("span"),Her=o("TFAutoModelForSeq2SeqLM"),u8e=l(),br=a("div"),f(fw.$$.fragment),Uer=l(),uc=a("p"),Jer=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence language modeling head) when created
with the `),Wme=a("code"),Yer=o("from_pretrained()"),Ker=o("class method or the "),Qme=a("code"),Zer=o("from_config()"),eor=o(`class
method.`),oor=l(),mw=a("p"),ror=o("This class cannot be instantiated directly using "),Hme=a("code"),tor=o("__init__()"),aor=o(" (throws an error)."),nor=l(),ft=a("div"),f(gw.$$.fragment),sor=l(),Ume=a("p"),lor=o("Instantiates one of the model classes of the library (with a sequence-to-sequence language modeling head) from a configuration."),ior=l(),bc=a("p"),dor=o(`Note:
Loading a model from its configuration file does `),Jme=a("strong"),cor=o("not"),mor=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Yme=a("code"),gor=o("from_pretrained()"),hor=o("to load the model weights."),por=l(),Kme=a("p"),_or=o("Examples:"),uor=l(),f(hw.$$.fragment),bor=l(),bo=a("div"),f(pw.$$.fragment),vor=l(),Zme=a("p"),Tor=o("Instantiate one of the model classes of the library (with a sequence-to-sequence language modeling head) from a pretrained model."),For=l(),gn=a("p"),Cor=o("The model class to instantiate is selected based on the "),ege=a("code"),Mor=o("model_type"),Eor=o(` property of the config object (either
passed as an argument or loaded from `),oge=a("code"),yor=o("pretrained_model_name_or_path"),wor=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),rge=a("code"),Aor=o("pretrained_model_name_or_path"),Lor=o(":"),Bor=l(),pe=a("ul"),h7=a("li"),tge=a("strong"),kor=o("bart"),xor=o(" \u2014 "),bq=a("a"),Ror=o("TFBartForConditionalGeneration"),Sor=o(" (BART model)"),Por=l(),p7=a("li"),age=a("strong"),$or=o("blenderbot"),Ior=o(" \u2014 "),vq=a("a"),jor=o("TFBlenderbotForConditionalGeneration"),Nor=o(" (Blenderbot model)"),Dor=l(),_7=a("li"),nge=a("strong"),qor=o("blenderbot-small"),Gor=o(" \u2014 "),Tq=a("a"),Oor=o("TFBlenderbotSmallForConditionalGeneration"),Xor=o(" (BlenderbotSmall model)"),zor=l(),u7=a("li"),sge=a("strong"),Vor=o("encoder-decoder"),Wor=o(" \u2014 "),Fq=a("a"),Qor=o("TFEncoderDecoderModel"),Hor=o(" (Encoder decoder model)"),Uor=l(),b7=a("li"),lge=a("strong"),Jor=o("led"),Yor=o(" \u2014 "),Cq=a("a"),Kor=o("TFLEDForConditionalGeneration"),Zor=o(" (LED model)"),err=l(),v7=a("li"),ige=a("strong"),orr=o("marian"),rrr=o(" \u2014 "),Mq=a("a"),trr=o("TFMarianMTModel"),arr=o(" (Marian model)"),nrr=l(),T7=a("li"),dge=a("strong"),srr=o("mbart"),lrr=o(" \u2014 "),Eq=a("a"),irr=o("TFMBartForConditionalGeneration"),drr=o(" (mBART model)"),crr=l(),F7=a("li"),cge=a("strong"),frr=o("mt5"),mrr=o(" \u2014 "),yq=a("a"),grr=o("TFMT5ForConditionalGeneration"),hrr=o(" (mT5 model)"),prr=l(),C7=a("li"),fge=a("strong"),_rr=o("pegasus"),urr=o(" \u2014 "),wq=a("a"),brr=o("TFPegasusForConditionalGeneration"),vrr=o(" (Pegasus model)"),Trr=l(),M7=a("li"),mge=a("strong"),Frr=o("t5"),Crr=o(" \u2014 "),Aq=a("a"),Mrr=o("TFT5ForConditionalGeneration"),Err=o(" (T5 model)"),yrr=l(),gge=a("p"),wrr=o("Examples:"),Arr=l(),f(_w.$$.fragment),b8e=l(),vc=a("h2"),E7=a("a"),hge=a("span"),f(uw.$$.fragment),Lrr=l(),pge=a("span"),Brr=o("TFAutoModelForSequenceClassification"),v8e=l(),vr=a("div"),f(bw.$$.fragment),krr=l(),Tc=a("p"),xrr=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence classification head) when created
with the `),_ge=a("code"),Rrr=o("from_pretrained()"),Srr=o("class method or the "),uge=a("code"),Prr=o("from_config()"),$rr=o(`class
method.`),Irr=l(),vw=a("p"),jrr=o("This class cannot be instantiated directly using "),bge=a("code"),Nrr=o("__init__()"),Drr=o(" (throws an error)."),qrr=l(),mt=a("div"),f(Tw.$$.fragment),Grr=l(),vge=a("p"),Orr=o("Instantiates one of the model classes of the library (with a sequence classification head) from a configuration."),Xrr=l(),Fc=a("p"),zrr=o(`Note:
Loading a model from its configuration file does `),Tge=a("strong"),Vrr=o("not"),Wrr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Fge=a("code"),Qrr=o("from_pretrained()"),Hrr=o("to load the model weights."),Urr=l(),Cge=a("p"),Jrr=o("Examples:"),Yrr=l(),f(Fw.$$.fragment),Krr=l(),vo=a("div"),f(Cw.$$.fragment),Zrr=l(),Mge=a("p"),etr=o("Instantiate one of the model classes of the library (with a sequence classification head) from a pretrained model."),otr=l(),hn=a("p"),rtr=o("The model class to instantiate is selected based on the "),Ege=a("code"),ttr=o("model_type"),atr=o(` property of the config object (either
passed as an argument or loaded from `),yge=a("code"),ntr=o("pretrained_model_name_or_path"),str=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),wge=a("code"),ltr=o("pretrained_model_name_or_path"),itr=o(":"),dtr=l(),X=a("ul"),y7=a("li"),Age=a("strong"),ctr=o("albert"),ftr=o(" \u2014 "),Lq=a("a"),mtr=o("TFAlbertForSequenceClassification"),gtr=o(" (ALBERT model)"),htr=l(),w7=a("li"),Lge=a("strong"),ptr=o("bert"),_tr=o(" \u2014 "),Bq=a("a"),utr=o("TFBertForSequenceClassification"),btr=o(" (BERT model)"),vtr=l(),A7=a("li"),Bge=a("strong"),Ttr=o("camembert"),Ftr=o(" \u2014 "),kq=a("a"),Ctr=o("TFCamembertForSequenceClassification"),Mtr=o(" (CamemBERT model)"),Etr=l(),L7=a("li"),kge=a("strong"),ytr=o("convbert"),wtr=o(" \u2014 "),xq=a("a"),Atr=o("TFConvBertForSequenceClassification"),Ltr=o(" (ConvBERT model)"),Btr=l(),B7=a("li"),xge=a("strong"),ktr=o("ctrl"),xtr=o(" \u2014 "),Rq=a("a"),Rtr=o("TFCTRLForSequenceClassification"),Str=o(" (CTRL model)"),Ptr=l(),k7=a("li"),Rge=a("strong"),$tr=o("deberta"),Itr=o(" \u2014 "),Sq=a("a"),jtr=o("TFDebertaForSequenceClassification"),Ntr=o(" (DeBERTa model)"),Dtr=l(),x7=a("li"),Sge=a("strong"),qtr=o("deberta-v2"),Gtr=o(" \u2014 "),Pq=a("a"),Otr=o("TFDebertaV2ForSequenceClassification"),Xtr=o(" (DeBERTa-v2 model)"),ztr=l(),R7=a("li"),Pge=a("strong"),Vtr=o("distilbert"),Wtr=o(" \u2014 "),$q=a("a"),Qtr=o("TFDistilBertForSequenceClassification"),Htr=o(" (DistilBERT model)"),Utr=l(),S7=a("li"),$ge=a("strong"),Jtr=o("electra"),Ytr=o(" \u2014 "),Iq=a("a"),Ktr=o("TFElectraForSequenceClassification"),Ztr=o(" (ELECTRA model)"),ear=l(),P7=a("li"),Ige=a("strong"),oar=o("flaubert"),rar=o(" \u2014 "),jq=a("a"),tar=o("TFFlaubertForSequenceClassification"),aar=o(" (FlauBERT model)"),nar=l(),$7=a("li"),jge=a("strong"),sar=o("funnel"),lar=o(" \u2014 "),Nq=a("a"),iar=o("TFFunnelForSequenceClassification"),dar=o(" (Funnel Transformer model)"),car=l(),I7=a("li"),Nge=a("strong"),far=o("gpt2"),mar=o(" \u2014 "),Dq=a("a"),gar=o("TFGPT2ForSequenceClassification"),har=o(" (OpenAI GPT-2 model)"),par=l(),j7=a("li"),Dge=a("strong"),_ar=o("layoutlm"),uar=o(" \u2014 "),qq=a("a"),bar=o("TFLayoutLMForSequenceClassification"),Tar=o(" (LayoutLM model)"),Far=l(),N7=a("li"),qge=a("strong"),Car=o("longformer"),Mar=o(" \u2014 "),Gq=a("a"),Ear=o("TFLongformerForSequenceClassification"),yar=o(" (Longformer model)"),war=l(),D7=a("li"),Gge=a("strong"),Aar=o("mobilebert"),Lar=o(" \u2014 "),Oq=a("a"),Bar=o("TFMobileBertForSequenceClassification"),kar=o(" (MobileBERT model)"),xar=l(),q7=a("li"),Oge=a("strong"),Rar=o("mpnet"),Sar=o(" \u2014 "),Xq=a("a"),Par=o("TFMPNetForSequenceClassification"),$ar=o(" (MPNet model)"),Iar=l(),G7=a("li"),Xge=a("strong"),jar=o("openai-gpt"),Nar=o(" \u2014 "),zq=a("a"),Dar=o("TFOpenAIGPTForSequenceClassification"),qar=o(" (OpenAI GPT model)"),Gar=l(),O7=a("li"),zge=a("strong"),Oar=o("rembert"),Xar=o(" \u2014 "),Vq=a("a"),zar=o("TFRemBertForSequenceClassification"),Var=o(" (RemBERT model)"),War=l(),X7=a("li"),Vge=a("strong"),Qar=o("roberta"),Har=o(" \u2014 "),Wq=a("a"),Uar=o("TFRobertaForSequenceClassification"),Jar=o(" (RoBERTa model)"),Yar=l(),z7=a("li"),Wge=a("strong"),Kar=o("roformer"),Zar=o(" \u2014 "),Qq=a("a"),enr=o("TFRoFormerForSequenceClassification"),onr=o(" (RoFormer model)"),rnr=l(),V7=a("li"),Qge=a("strong"),tnr=o("tapas"),anr=o(" \u2014 "),Hq=a("a"),nnr=o("TFTapasForSequenceClassification"),snr=o(" (TAPAS model)"),lnr=l(),W7=a("li"),Hge=a("strong"),inr=o("transfo-xl"),dnr=o(" \u2014 "),Uq=a("a"),cnr=o("TFTransfoXLForSequenceClassification"),fnr=o(" (Transformer-XL model)"),mnr=l(),Q7=a("li"),Uge=a("strong"),gnr=o("xlm"),hnr=o(" \u2014 "),Jq=a("a"),pnr=o("TFXLMForSequenceClassification"),_nr=o(" (XLM model)"),unr=l(),H7=a("li"),Jge=a("strong"),bnr=o("xlm-roberta"),vnr=o(" \u2014 "),Yq=a("a"),Tnr=o("TFXLMRobertaForSequenceClassification"),Fnr=o(" (XLM-RoBERTa model)"),Cnr=l(),U7=a("li"),Yge=a("strong"),Mnr=o("xlnet"),Enr=o(" \u2014 "),Kq=a("a"),ynr=o("TFXLNetForSequenceClassification"),wnr=o(" (XLNet model)"),Anr=l(),Kge=a("p"),Lnr=o("Examples:"),Bnr=l(),f(Mw.$$.fragment),T8e=l(),Cc=a("h2"),J7=a("a"),Zge=a("span"),f(Ew.$$.fragment),knr=l(),ehe=a("span"),xnr=o("TFAutoModelForMultipleChoice"),F8e=l(),Tr=a("div"),f(yw.$$.fragment),Rnr=l(),Mc=a("p"),Snr=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a multiple choice head) when created
with the `),ohe=a("code"),Pnr=o("from_pretrained()"),$nr=o("class method or the "),rhe=a("code"),Inr=o("from_config()"),jnr=o(`class
method.`),Nnr=l(),ww=a("p"),Dnr=o("This class cannot be instantiated directly using "),the=a("code"),qnr=o("__init__()"),Gnr=o(" (throws an error)."),Onr=l(),gt=a("div"),f(Aw.$$.fragment),Xnr=l(),ahe=a("p"),znr=o("Instantiates one of the model classes of the library (with a multiple choice head) from a configuration."),Vnr=l(),Ec=a("p"),Wnr=o(`Note:
Loading a model from its configuration file does `),nhe=a("strong"),Qnr=o("not"),Hnr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),she=a("code"),Unr=o("from_pretrained()"),Jnr=o("to load the model weights."),Ynr=l(),lhe=a("p"),Knr=o("Examples:"),Znr=l(),f(Lw.$$.fragment),esr=l(),To=a("div"),f(Bw.$$.fragment),osr=l(),ihe=a("p"),rsr=o("Instantiate one of the model classes of the library (with a multiple choice head) from a pretrained model."),tsr=l(),pn=a("p"),asr=o("The model class to instantiate is selected based on the "),dhe=a("code"),nsr=o("model_type"),ssr=o(` property of the config object (either
passed as an argument or loaded from `),che=a("code"),lsr=o("pretrained_model_name_or_path"),isr=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),fhe=a("code"),dsr=o("pretrained_model_name_or_path"),csr=o(":"),fsr=l(),te=a("ul"),Y7=a("li"),mhe=a("strong"),msr=o("albert"),gsr=o(" \u2014 "),Zq=a("a"),hsr=o("TFAlbertForMultipleChoice"),psr=o(" (ALBERT model)"),_sr=l(),K7=a("li"),ghe=a("strong"),usr=o("bert"),bsr=o(" \u2014 "),eG=a("a"),vsr=o("TFBertForMultipleChoice"),Tsr=o(" (BERT model)"),Fsr=l(),Z7=a("li"),hhe=a("strong"),Csr=o("camembert"),Msr=o(" \u2014 "),oG=a("a"),Esr=o("TFCamembertForMultipleChoice"),ysr=o(" (CamemBERT model)"),wsr=l(),eF=a("li"),phe=a("strong"),Asr=o("convbert"),Lsr=o(" \u2014 "),rG=a("a"),Bsr=o("TFConvBertForMultipleChoice"),ksr=o(" (ConvBERT model)"),xsr=l(),oF=a("li"),_he=a("strong"),Rsr=o("distilbert"),Ssr=o(" \u2014 "),tG=a("a"),Psr=o("TFDistilBertForMultipleChoice"),$sr=o(" (DistilBERT model)"),Isr=l(),rF=a("li"),uhe=a("strong"),jsr=o("electra"),Nsr=o(" \u2014 "),aG=a("a"),Dsr=o("TFElectraForMultipleChoice"),qsr=o(" (ELECTRA model)"),Gsr=l(),tF=a("li"),bhe=a("strong"),Osr=o("flaubert"),Xsr=o(" \u2014 "),nG=a("a"),zsr=o("TFFlaubertForMultipleChoice"),Vsr=o(" (FlauBERT model)"),Wsr=l(),aF=a("li"),vhe=a("strong"),Qsr=o("funnel"),Hsr=o(" \u2014 "),sG=a("a"),Usr=o("TFFunnelForMultipleChoice"),Jsr=o(" (Funnel Transformer model)"),Ysr=l(),nF=a("li"),The=a("strong"),Ksr=o("longformer"),Zsr=o(" \u2014 "),lG=a("a"),elr=o("TFLongformerForMultipleChoice"),olr=o(" (Longformer model)"),rlr=l(),sF=a("li"),Fhe=a("strong"),tlr=o("mobilebert"),alr=o(" \u2014 "),iG=a("a"),nlr=o("TFMobileBertForMultipleChoice"),slr=o(" (MobileBERT model)"),llr=l(),lF=a("li"),Che=a("strong"),ilr=o("mpnet"),dlr=o(" \u2014 "),dG=a("a"),clr=o("TFMPNetForMultipleChoice"),flr=o(" (MPNet model)"),mlr=l(),iF=a("li"),Mhe=a("strong"),glr=o("rembert"),hlr=o(" \u2014 "),cG=a("a"),plr=o("TFRemBertForMultipleChoice"),_lr=o(" (RemBERT model)"),ulr=l(),dF=a("li"),Ehe=a("strong"),blr=o("roberta"),vlr=o(" \u2014 "),fG=a("a"),Tlr=o("TFRobertaForMultipleChoice"),Flr=o(" (RoBERTa model)"),Clr=l(),cF=a("li"),yhe=a("strong"),Mlr=o("roformer"),Elr=o(" \u2014 "),mG=a("a"),ylr=o("TFRoFormerForMultipleChoice"),wlr=o(" (RoFormer model)"),Alr=l(),fF=a("li"),whe=a("strong"),Llr=o("xlm"),Blr=o(" \u2014 "),gG=a("a"),klr=o("TFXLMForMultipleChoice"),xlr=o(" (XLM model)"),Rlr=l(),mF=a("li"),Ahe=a("strong"),Slr=o("xlm-roberta"),Plr=o(" \u2014 "),hG=a("a"),$lr=o("TFXLMRobertaForMultipleChoice"),Ilr=o(" (XLM-RoBERTa model)"),jlr=l(),gF=a("li"),Lhe=a("strong"),Nlr=o("xlnet"),Dlr=o(" \u2014 "),pG=a("a"),qlr=o("TFXLNetForMultipleChoice"),Glr=o(" (XLNet model)"),Olr=l(),Bhe=a("p"),Xlr=o("Examples:"),zlr=l(),f(kw.$$.fragment),C8e=l(),yc=a("h2"),hF=a("a"),khe=a("span"),f(xw.$$.fragment),Vlr=l(),xhe=a("span"),Wlr=o("TFAutoModelForTableQuestionAnswering"),M8e=l(),Fr=a("div"),f(Rw.$$.fragment),Qlr=l(),wc=a("p"),Hlr=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a table question answering head) when created
with the `),Rhe=a("code"),Ulr=o("from_pretrained()"),Jlr=o("class method or the "),She=a("code"),Ylr=o("from_config()"),Klr=o(`class
method.`),Zlr=l(),Sw=a("p"),eir=o("This class cannot be instantiated directly using "),Phe=a("code"),oir=o("__init__()"),rir=o(" (throws an error)."),tir=l(),ht=a("div"),f(Pw.$$.fragment),air=l(),$he=a("p"),nir=o("Instantiates one of the model classes of the library (with a table question answering head) from a configuration."),sir=l(),Ac=a("p"),lir=o(`Note:
Loading a model from its configuration file does `),Ihe=a("strong"),iir=o("not"),dir=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),jhe=a("code"),cir=o("from_pretrained()"),fir=o("to load the model weights."),mir=l(),Nhe=a("p"),gir=o("Examples:"),hir=l(),f($w.$$.fragment),pir=l(),Fo=a("div"),f(Iw.$$.fragment),_ir=l(),Dhe=a("p"),uir=o("Instantiate one of the model classes of the library (with a table question answering head) from a pretrained model."),bir=l(),_n=a("p"),vir=o("The model class to instantiate is selected based on the "),qhe=a("code"),Tir=o("model_type"),Fir=o(` property of the config object (either
passed as an argument or loaded from `),Ghe=a("code"),Cir=o("pretrained_model_name_or_path"),Mir=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Ohe=a("code"),Eir=o("pretrained_model_name_or_path"),yir=o(":"),wir=l(),Xhe=a("ul"),pF=a("li"),zhe=a("strong"),Air=o("tapas"),Lir=o(" \u2014 "),_G=a("a"),Bir=o("TFTapasForQuestionAnswering"),kir=o(" (TAPAS model)"),xir=l(),Vhe=a("p"),Rir=o("Examples:"),Sir=l(),f(jw.$$.fragment),E8e=l(),Lc=a("h2"),_F=a("a"),Whe=a("span"),f(Nw.$$.fragment),Pir=l(),Qhe=a("span"),$ir=o("TFAutoModelForTokenClassification"),y8e=l(),Cr=a("div"),f(Dw.$$.fragment),Iir=l(),Bc=a("p"),jir=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a token classification head) when created
with the `),Hhe=a("code"),Nir=o("from_pretrained()"),Dir=o("class method or the "),Uhe=a("code"),qir=o("from_config()"),Gir=o(`class
method.`),Oir=l(),qw=a("p"),Xir=o("This class cannot be instantiated directly using "),Jhe=a("code"),zir=o("__init__()"),Vir=o(" (throws an error)."),Wir=l(),pt=a("div"),f(Gw.$$.fragment),Qir=l(),Yhe=a("p"),Hir=o("Instantiates one of the model classes of the library (with a token classification head) from a configuration."),Uir=l(),kc=a("p"),Jir=o(`Note:
Loading a model from its configuration file does `),Khe=a("strong"),Yir=o("not"),Kir=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Zhe=a("code"),Zir=o("from_pretrained()"),edr=o("to load the model weights."),odr=l(),epe=a("p"),rdr=o("Examples:"),tdr=l(),f(Ow.$$.fragment),adr=l(),Co=a("div"),f(Xw.$$.fragment),ndr=l(),ope=a("p"),sdr=o("Instantiate one of the model classes of the library (with a token classification head) from a pretrained model."),ldr=l(),un=a("p"),idr=o("The model class to instantiate is selected based on the "),rpe=a("code"),ddr=o("model_type"),cdr=o(` property of the config object (either
passed as an argument or loaded from `),tpe=a("code"),fdr=o("pretrained_model_name_or_path"),mdr=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),ape=a("code"),gdr=o("pretrained_model_name_or_path"),hdr=o(":"),pdr=l(),K=a("ul"),uF=a("li"),npe=a("strong"),_dr=o("albert"),udr=o(" \u2014 "),uG=a("a"),bdr=o("TFAlbertForTokenClassification"),vdr=o(" (ALBERT model)"),Tdr=l(),bF=a("li"),spe=a("strong"),Fdr=o("bert"),Cdr=o(" \u2014 "),bG=a("a"),Mdr=o("TFBertForTokenClassification"),Edr=o(" (BERT model)"),ydr=l(),vF=a("li"),lpe=a("strong"),wdr=o("camembert"),Adr=o(" \u2014 "),vG=a("a"),Ldr=o("TFCamembertForTokenClassification"),Bdr=o(" (CamemBERT model)"),kdr=l(),TF=a("li"),ipe=a("strong"),xdr=o("convbert"),Rdr=o(" \u2014 "),TG=a("a"),Sdr=o("TFConvBertForTokenClassification"),Pdr=o(" (ConvBERT model)"),$dr=l(),FF=a("li"),dpe=a("strong"),Idr=o("deberta"),jdr=o(" \u2014 "),FG=a("a"),Ndr=o("TFDebertaForTokenClassification"),Ddr=o(" (DeBERTa model)"),qdr=l(),CF=a("li"),cpe=a("strong"),Gdr=o("deberta-v2"),Odr=o(" \u2014 "),CG=a("a"),Xdr=o("TFDebertaV2ForTokenClassification"),zdr=o(" (DeBERTa-v2 model)"),Vdr=l(),MF=a("li"),fpe=a("strong"),Wdr=o("distilbert"),Qdr=o(" \u2014 "),MG=a("a"),Hdr=o("TFDistilBertForTokenClassification"),Udr=o(" (DistilBERT model)"),Jdr=l(),EF=a("li"),mpe=a("strong"),Ydr=o("electra"),Kdr=o(" \u2014 "),EG=a("a"),Zdr=o("TFElectraForTokenClassification"),ecr=o(" (ELECTRA model)"),ocr=l(),yF=a("li"),gpe=a("strong"),rcr=o("flaubert"),tcr=o(" \u2014 "),yG=a("a"),acr=o("TFFlaubertForTokenClassification"),ncr=o(" (FlauBERT model)"),scr=l(),wF=a("li"),hpe=a("strong"),lcr=o("funnel"),icr=o(" \u2014 "),wG=a("a"),dcr=o("TFFunnelForTokenClassification"),ccr=o(" (Funnel Transformer model)"),fcr=l(),AF=a("li"),ppe=a("strong"),mcr=o("layoutlm"),gcr=o(" \u2014 "),AG=a("a"),hcr=o("TFLayoutLMForTokenClassification"),pcr=o(" (LayoutLM model)"),_cr=l(),LF=a("li"),_pe=a("strong"),ucr=o("longformer"),bcr=o(" \u2014 "),LG=a("a"),vcr=o("TFLongformerForTokenClassification"),Tcr=o(" (Longformer model)"),Fcr=l(),BF=a("li"),upe=a("strong"),Ccr=o("mobilebert"),Mcr=o(" \u2014 "),BG=a("a"),Ecr=o("TFMobileBertForTokenClassification"),ycr=o(" (MobileBERT model)"),wcr=l(),kF=a("li"),bpe=a("strong"),Acr=o("mpnet"),Lcr=o(" \u2014 "),kG=a("a"),Bcr=o("TFMPNetForTokenClassification"),kcr=o(" (MPNet model)"),xcr=l(),xF=a("li"),vpe=a("strong"),Rcr=o("rembert"),Scr=o(" \u2014 "),xG=a("a"),Pcr=o("TFRemBertForTokenClassification"),$cr=o(" (RemBERT model)"),Icr=l(),RF=a("li"),Tpe=a("strong"),jcr=o("roberta"),Ncr=o(" \u2014 "),RG=a("a"),Dcr=o("TFRobertaForTokenClassification"),qcr=o(" (RoBERTa model)"),Gcr=l(),SF=a("li"),Fpe=a("strong"),Ocr=o("roformer"),Xcr=o(" \u2014 "),SG=a("a"),zcr=o("TFRoFormerForTokenClassification"),Vcr=o(" (RoFormer model)"),Wcr=l(),PF=a("li"),Cpe=a("strong"),Qcr=o("xlm"),Hcr=o(" \u2014 "),PG=a("a"),Ucr=o("TFXLMForTokenClassification"),Jcr=o(" (XLM model)"),Ycr=l(),$F=a("li"),Mpe=a("strong"),Kcr=o("xlm-roberta"),Zcr=o(" \u2014 "),$G=a("a"),efr=o("TFXLMRobertaForTokenClassification"),ofr=o(" (XLM-RoBERTa model)"),rfr=l(),IF=a("li"),Epe=a("strong"),tfr=o("xlnet"),afr=o(" \u2014 "),IG=a("a"),nfr=o("TFXLNetForTokenClassification"),sfr=o(" (XLNet model)"),lfr=l(),ype=a("p"),ifr=o("Examples:"),dfr=l(),f(zw.$$.fragment),w8e=l(),xc=a("h2"),jF=a("a"),wpe=a("span"),f(Vw.$$.fragment),cfr=l(),Ape=a("span"),ffr=o("TFAutoModelForQuestionAnswering"),A8e=l(),Mr=a("div"),f(Ww.$$.fragment),mfr=l(),Rc=a("p"),gfr=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a question answering head) when created
with the `),Lpe=a("code"),hfr=o("from_pretrained()"),pfr=o("class method or the "),Bpe=a("code"),_fr=o("from_config()"),ufr=o(`class
method.`),bfr=l(),Qw=a("p"),vfr=o("This class cannot be instantiated directly using "),kpe=a("code"),Tfr=o("__init__()"),Ffr=o(" (throws an error)."),Cfr=l(),_t=a("div"),f(Hw.$$.fragment),Mfr=l(),xpe=a("p"),Efr=o("Instantiates one of the model classes of the library (with a question answering head) from a configuration."),yfr=l(),Sc=a("p"),wfr=o(`Note:
Loading a model from its configuration file does `),Rpe=a("strong"),Afr=o("not"),Lfr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Spe=a("code"),Bfr=o("from_pretrained()"),kfr=o("to load the model weights."),xfr=l(),Ppe=a("p"),Rfr=o("Examples:"),Sfr=l(),f(Uw.$$.fragment),Pfr=l(),Mo=a("div"),f(Jw.$$.fragment),$fr=l(),$pe=a("p"),Ifr=o("Instantiate one of the model classes of the library (with a question answering head) from a pretrained model."),jfr=l(),bn=a("p"),Nfr=o("The model class to instantiate is selected based on the "),Ipe=a("code"),Dfr=o("model_type"),qfr=o(` property of the config object (either
passed as an argument or loaded from `),jpe=a("code"),Gfr=o("pretrained_model_name_or_path"),Ofr=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Npe=a("code"),Xfr=o("pretrained_model_name_or_path"),zfr=o(":"),Vfr=l(),Z=a("ul"),NF=a("li"),Dpe=a("strong"),Wfr=o("albert"),Qfr=o(" \u2014 "),jG=a("a"),Hfr=o("TFAlbertForQuestionAnswering"),Ufr=o(" (ALBERT model)"),Jfr=l(),DF=a("li"),qpe=a("strong"),Yfr=o("bert"),Kfr=o(" \u2014 "),NG=a("a"),Zfr=o("TFBertForQuestionAnswering"),emr=o(" (BERT model)"),omr=l(),qF=a("li"),Gpe=a("strong"),rmr=o("camembert"),tmr=o(" \u2014 "),DG=a("a"),amr=o("TFCamembertForQuestionAnswering"),nmr=o(" (CamemBERT model)"),smr=l(),GF=a("li"),Ope=a("strong"),lmr=o("convbert"),imr=o(" \u2014 "),qG=a("a"),dmr=o("TFConvBertForQuestionAnswering"),cmr=o(" (ConvBERT model)"),fmr=l(),OF=a("li"),Xpe=a("strong"),mmr=o("deberta"),gmr=o(" \u2014 "),GG=a("a"),hmr=o("TFDebertaForQuestionAnswering"),pmr=o(" (DeBERTa model)"),_mr=l(),XF=a("li"),zpe=a("strong"),umr=o("deberta-v2"),bmr=o(" \u2014 "),OG=a("a"),vmr=o("TFDebertaV2ForQuestionAnswering"),Tmr=o(" (DeBERTa-v2 model)"),Fmr=l(),zF=a("li"),Vpe=a("strong"),Cmr=o("distilbert"),Mmr=o(" \u2014 "),XG=a("a"),Emr=o("TFDistilBertForQuestionAnswering"),ymr=o(" (DistilBERT model)"),wmr=l(),VF=a("li"),Wpe=a("strong"),Amr=o("electra"),Lmr=o(" \u2014 "),zG=a("a"),Bmr=o("TFElectraForQuestionAnswering"),kmr=o(" (ELECTRA model)"),xmr=l(),WF=a("li"),Qpe=a("strong"),Rmr=o("flaubert"),Smr=o(" \u2014 "),VG=a("a"),Pmr=o("TFFlaubertForQuestionAnsweringSimple"),$mr=o(" (FlauBERT model)"),Imr=l(),QF=a("li"),Hpe=a("strong"),jmr=o("funnel"),Nmr=o(" \u2014 "),WG=a("a"),Dmr=o("TFFunnelForQuestionAnswering"),qmr=o(" (Funnel Transformer model)"),Gmr=l(),HF=a("li"),Upe=a("strong"),Omr=o("longformer"),Xmr=o(" \u2014 "),QG=a("a"),zmr=o("TFLongformerForQuestionAnswering"),Vmr=o(" (Longformer model)"),Wmr=l(),UF=a("li"),Jpe=a("strong"),Qmr=o("mobilebert"),Hmr=o(" \u2014 "),HG=a("a"),Umr=o("TFMobileBertForQuestionAnswering"),Jmr=o(" (MobileBERT model)"),Ymr=l(),JF=a("li"),Ype=a("strong"),Kmr=o("mpnet"),Zmr=o(" \u2014 "),UG=a("a"),egr=o("TFMPNetForQuestionAnswering"),ogr=o(" (MPNet model)"),rgr=l(),YF=a("li"),Kpe=a("strong"),tgr=o("rembert"),agr=o(" \u2014 "),JG=a("a"),ngr=o("TFRemBertForQuestionAnswering"),sgr=o(" (RemBERT model)"),lgr=l(),KF=a("li"),Zpe=a("strong"),igr=o("roberta"),dgr=o(" \u2014 "),YG=a("a"),cgr=o("TFRobertaForQuestionAnswering"),fgr=o(" (RoBERTa model)"),mgr=l(),ZF=a("li"),e_e=a("strong"),ggr=o("roformer"),hgr=o(" \u2014 "),KG=a("a"),pgr=o("TFRoFormerForQuestionAnswering"),_gr=o(" (RoFormer model)"),ugr=l(),e9=a("li"),o_e=a("strong"),bgr=o("xlm"),vgr=o(" \u2014 "),ZG=a("a"),Tgr=o("TFXLMForQuestionAnsweringSimple"),Fgr=o(" (XLM model)"),Cgr=l(),o9=a("li"),r_e=a("strong"),Mgr=o("xlm-roberta"),Egr=o(" \u2014 "),eO=a("a"),ygr=o("TFXLMRobertaForQuestionAnswering"),wgr=o(" (XLM-RoBERTa model)"),Agr=l(),r9=a("li"),t_e=a("strong"),Lgr=o("xlnet"),Bgr=o(" \u2014 "),oO=a("a"),kgr=o("TFXLNetForQuestionAnsweringSimple"),xgr=o(" (XLNet model)"),Rgr=l(),a_e=a("p"),Sgr=o("Examples:"),Pgr=l(),f(Yw.$$.fragment),L8e=l(),Pc=a("h2"),t9=a("a"),n_e=a("span"),f(Kw.$$.fragment),$gr=l(),s_e=a("span"),Igr=o("TFAutoModelForVision2Seq"),B8e=l(),Er=a("div"),f(Zw.$$.fragment),jgr=l(),$c=a("p"),Ngr=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a vision-to-text modeling head) when created
with the `),l_e=a("code"),Dgr=o("from_pretrained()"),qgr=o("class method or the "),i_e=a("code"),Ggr=o("from_config()"),Ogr=o(`class
method.`),Xgr=l(),eA=a("p"),zgr=o("This class cannot be instantiated directly using "),d_e=a("code"),Vgr=o("__init__()"),Wgr=o(" (throws an error)."),Qgr=l(),ut=a("div"),f(oA.$$.fragment),Hgr=l(),c_e=a("p"),Ugr=o("Instantiates one of the model classes of the library (with a vision-to-text modeling head) from a configuration."),Jgr=l(),Ic=a("p"),Ygr=o(`Note:
Loading a model from its configuration file does `),f_e=a("strong"),Kgr=o("not"),Zgr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),m_e=a("code"),ehr=o("from_pretrained()"),ohr=o("to load the model weights."),rhr=l(),g_e=a("p"),thr=o("Examples:"),ahr=l(),f(rA.$$.fragment),nhr=l(),Eo=a("div"),f(tA.$$.fragment),shr=l(),h_e=a("p"),lhr=o("Instantiate one of the model classes of the library (with a vision-to-text modeling head) from a pretrained model."),ihr=l(),vn=a("p"),dhr=o("The model class to instantiate is selected based on the "),p_e=a("code"),chr=o("model_type"),fhr=o(` property of the config object (either
passed as an argument or loaded from `),__e=a("code"),mhr=o("pretrained_model_name_or_path"),ghr=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),u_e=a("code"),hhr=o("pretrained_model_name_or_path"),phr=o(":"),_hr=l(),b_e=a("ul"),a9=a("li"),v_e=a("strong"),uhr=o("vision-encoder-decoder"),bhr=o(" \u2014 "),rO=a("a"),vhr=o("TFVisionEncoderDecoderModel"),Thr=o(" (Vision Encoder decoder model)"),Fhr=l(),T_e=a("p"),Chr=o("Examples:"),Mhr=l(),f(aA.$$.fragment),k8e=l(),jc=a("h2"),n9=a("a"),F_e=a("span"),f(nA.$$.fragment),Ehr=l(),C_e=a("span"),yhr=o("TFAutoModelForSpeechSeq2Seq"),x8e=l(),yr=a("div"),f(sA.$$.fragment),whr=l(),Nc=a("p"),Ahr=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) when created
with the `),M_e=a("code"),Lhr=o("from_pretrained()"),Bhr=o("class method or the "),E_e=a("code"),khr=o("from_config()"),xhr=o(`class
method.`),Rhr=l(),lA=a("p"),Shr=o("This class cannot be instantiated directly using "),y_e=a("code"),Phr=o("__init__()"),$hr=o(" (throws an error)."),Ihr=l(),bt=a("div"),f(iA.$$.fragment),jhr=l(),w_e=a("p"),Nhr=o("Instantiates one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) from a configuration."),Dhr=l(),Dc=a("p"),qhr=o(`Note:
Loading a model from its configuration file does `),A_e=a("strong"),Ghr=o("not"),Ohr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),L_e=a("code"),Xhr=o("from_pretrained()"),zhr=o("to load the model weights."),Vhr=l(),B_e=a("p"),Whr=o("Examples:"),Qhr=l(),f(dA.$$.fragment),Hhr=l(),yo=a("div"),f(cA.$$.fragment),Uhr=l(),k_e=a("p"),Jhr=o("Instantiate one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) from a pretrained model."),Yhr=l(),Tn=a("p"),Khr=o("The model class to instantiate is selected based on the "),x_e=a("code"),Zhr=o("model_type"),epr=o(` property of the config object (either
passed as an argument or loaded from `),R_e=a("code"),opr=o("pretrained_model_name_or_path"),rpr=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),S_e=a("code"),tpr=o("pretrained_model_name_or_path"),apr=o(":"),npr=l(),P_e=a("ul"),s9=a("li"),$_e=a("strong"),spr=o("speech_to_text"),lpr=o(" \u2014 "),tO=a("a"),ipr=o("TFSpeech2TextForConditionalGeneration"),dpr=o(" (Speech2Text model)"),cpr=l(),I_e=a("p"),fpr=o("Examples:"),mpr=l(),f(fA.$$.fragment),R8e=l(),qc=a("h2"),l9=a("a"),j_e=a("span"),f(mA.$$.fragment),gpr=l(),N_e=a("span"),hpr=o("FlaxAutoModel"),S8e=l(),wr=a("div"),f(gA.$$.fragment),ppr=l(),Gc=a("p"),_pr=o(`This is a generic model class that will be instantiated as one of the base model classes of the library when created
with the `),D_e=a("code"),upr=o("from_pretrained()"),bpr=o("class method or the "),q_e=a("code"),vpr=o("from_config()"),Tpr=o(`class
method.`),Fpr=l(),hA=a("p"),Cpr=o("This class cannot be instantiated directly using "),G_e=a("code"),Mpr=o("__init__()"),Epr=o(" (throws an error)."),ypr=l(),vt=a("div"),f(pA.$$.fragment),wpr=l(),O_e=a("p"),Apr=o("Instantiates one of the base model classes of the library from a configuration."),Lpr=l(),Oc=a("p"),Bpr=o(`Note:
Loading a model from its configuration file does `),X_e=a("strong"),kpr=o("not"),xpr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),z_e=a("code"),Rpr=o("from_pretrained()"),Spr=o("to load the model weights."),Ppr=l(),V_e=a("p"),$pr=o("Examples:"),Ipr=l(),f(_A.$$.fragment),jpr=l(),wo=a("div"),f(uA.$$.fragment),Npr=l(),W_e=a("p"),Dpr=o("Instantiate one of the base model classes of the library from a pretrained model."),qpr=l(),Fn=a("p"),Gpr=o("The model class to instantiate is selected based on the "),Q_e=a("code"),Opr=o("model_type"),Xpr=o(` property of the config object (either
passed as an argument or loaded from `),H_e=a("code"),zpr=o("pretrained_model_name_or_path"),Vpr=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),U_e=a("code"),Wpr=o("pretrained_model_name_or_path"),Qpr=o(":"),Hpr=l(),V=a("ul"),i9=a("li"),J_e=a("strong"),Upr=o("albert"),Jpr=o(" \u2014 "),aO=a("a"),Ypr=o("FlaxAlbertModel"),Kpr=o(" (ALBERT model)"),Zpr=l(),d9=a("li"),Y_e=a("strong"),e_r=o("bart"),o_r=o(" \u2014 "),nO=a("a"),r_r=o("FlaxBartModel"),t_r=o(" (BART model)"),a_r=l(),c9=a("li"),K_e=a("strong"),n_r=o("beit"),s_r=o(" \u2014 "),sO=a("a"),l_r=o("FlaxBeitModel"),i_r=o(" (BEiT model)"),d_r=l(),f9=a("li"),Z_e=a("strong"),c_r=o("bert"),f_r=o(" \u2014 "),lO=a("a"),m_r=o("FlaxBertModel"),g_r=o(" (BERT model)"),h_r=l(),m9=a("li"),eue=a("strong"),p_r=o("big_bird"),__r=o(" \u2014 "),iO=a("a"),u_r=o("FlaxBigBirdModel"),b_r=o(" (BigBird model)"),v_r=l(),g9=a("li"),oue=a("strong"),T_r=o("blenderbot"),F_r=o(" \u2014 "),dO=a("a"),C_r=o("FlaxBlenderbotModel"),M_r=o(" (Blenderbot model)"),E_r=l(),h9=a("li"),rue=a("strong"),y_r=o("blenderbot-small"),w_r=o(" \u2014 "),cO=a("a"),A_r=o("FlaxBlenderbotSmallModel"),L_r=o(" (BlenderbotSmall model)"),B_r=l(),p9=a("li"),tue=a("strong"),k_r=o("clip"),x_r=o(" \u2014 "),fO=a("a"),R_r=o("FlaxCLIPModel"),S_r=o(" (CLIP model)"),P_r=l(),_9=a("li"),aue=a("strong"),$_r=o("distilbert"),I_r=o(" \u2014 "),mO=a("a"),j_r=o("FlaxDistilBertModel"),N_r=o(" (DistilBERT model)"),D_r=l(),u9=a("li"),nue=a("strong"),q_r=o("electra"),G_r=o(" \u2014 "),gO=a("a"),O_r=o("FlaxElectraModel"),X_r=o(" (ELECTRA model)"),z_r=l(),b9=a("li"),sue=a("strong"),V_r=o("gpt2"),W_r=o(" \u2014 "),hO=a("a"),Q_r=o("FlaxGPT2Model"),H_r=o(" (OpenAI GPT-2 model)"),U_r=l(),v9=a("li"),lue=a("strong"),J_r=o("gpt_neo"),Y_r=o(" \u2014 "),pO=a("a"),K_r=o("FlaxGPTNeoModel"),Z_r=o(" (GPT Neo model)"),eur=l(),T9=a("li"),iue=a("strong"),our=o("gptj"),rur=o(" \u2014 "),_O=a("a"),tur=o("FlaxGPTJModel"),aur=o(" (GPT-J model)"),nur=l(),F9=a("li"),due=a("strong"),sur=o("marian"),lur=o(" \u2014 "),uO=a("a"),iur=o("FlaxMarianModel"),dur=o(" (Marian model)"),cur=l(),C9=a("li"),cue=a("strong"),fur=o("mbart"),mur=o(" \u2014 "),bO=a("a"),gur=o("FlaxMBartModel"),hur=o(" (mBART model)"),pur=l(),M9=a("li"),fue=a("strong"),_ur=o("mt5"),uur=o(" \u2014 "),vO=a("a"),bur=o("FlaxMT5Model"),vur=o(" (mT5 model)"),Tur=l(),E9=a("li"),mue=a("strong"),Fur=o("pegasus"),Cur=o(" \u2014 "),TO=a("a"),Mur=o("FlaxPegasusModel"),Eur=o(" (Pegasus model)"),yur=l(),y9=a("li"),gue=a("strong"),wur=o("roberta"),Aur=o(" \u2014 "),FO=a("a"),Lur=o("FlaxRobertaModel"),Bur=o(" (RoBERTa model)"),kur=l(),w9=a("li"),hue=a("strong"),xur=o("roformer"),Rur=o(" \u2014 "),CO=a("a"),Sur=o("FlaxRoFormerModel"),Pur=o(" (RoFormer model)"),$ur=l(),A9=a("li"),pue=a("strong"),Iur=o("t5"),jur=o(" \u2014 "),MO=a("a"),Nur=o("FlaxT5Model"),Dur=o(" (T5 model)"),qur=l(),L9=a("li"),_ue=a("strong"),Gur=o("vision-text-dual-encoder"),Our=o(" \u2014 "),EO=a("a"),Xur=o("FlaxVisionTextDualEncoderModel"),zur=o(" (VisionTextDualEncoder model)"),Vur=l(),B9=a("li"),uue=a("strong"),Wur=o("vit"),Qur=o(" \u2014 "),yO=a("a"),Hur=o("FlaxViTModel"),Uur=o(" (ViT model)"),Jur=l(),k9=a("li"),bue=a("strong"),Yur=o("wav2vec2"),Kur=o(" \u2014 "),wO=a("a"),Zur=o("FlaxWav2Vec2Model"),e2r=o(" (Wav2Vec2 model)"),o2r=l(),x9=a("li"),vue=a("strong"),r2r=o("xglm"),t2r=o(" \u2014 "),AO=a("a"),a2r=o("FlaxXGLMModel"),n2r=o(" (XGLM model)"),s2r=l(),Tue=a("p"),l2r=o("Examples:"),i2r=l(),f(bA.$$.fragment),P8e=l(),Xc=a("h2"),R9=a("a"),Fue=a("span"),f(vA.$$.fragment),d2r=l(),Cue=a("span"),c2r=o("FlaxAutoModelForCausalLM"),$8e=l(),Ar=a("div"),f(TA.$$.fragment),f2r=l(),zc=a("p"),m2r=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a causal language modeling head) when created
with the `),Mue=a("code"),g2r=o("from_pretrained()"),h2r=o("class method or the "),Eue=a("code"),p2r=o("from_config()"),_2r=o(`class
method.`),u2r=l(),FA=a("p"),b2r=o("This class cannot be instantiated directly using "),yue=a("code"),v2r=o("__init__()"),T2r=o(" (throws an error)."),F2r=l(),Tt=a("div"),f(CA.$$.fragment),C2r=l(),wue=a("p"),M2r=o("Instantiates one of the model classes of the library (with a causal language modeling head) from a configuration."),E2r=l(),Vc=a("p"),y2r=o(`Note:
Loading a model from its configuration file does `),Aue=a("strong"),w2r=o("not"),A2r=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Lue=a("code"),L2r=o("from_pretrained()"),B2r=o("to load the model weights."),k2r=l(),Bue=a("p"),x2r=o("Examples:"),R2r=l(),f(MA.$$.fragment),S2r=l(),Ao=a("div"),f(EA.$$.fragment),P2r=l(),kue=a("p"),$2r=o("Instantiate one of the model classes of the library (with a causal language modeling head) from a pretrained model."),I2r=l(),Cn=a("p"),j2r=o("The model class to instantiate is selected based on the "),xue=a("code"),N2r=o("model_type"),D2r=o(` property of the config object (either
passed as an argument or loaded from `),Rue=a("code"),q2r=o("pretrained_model_name_or_path"),G2r=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Sue=a("code"),O2r=o("pretrained_model_name_or_path"),X2r=o(":"),z2r=l(),Mn=a("ul"),S9=a("li"),Pue=a("strong"),V2r=o("gpt2"),W2r=o(" \u2014 "),LO=a("a"),Q2r=o("FlaxGPT2LMHeadModel"),H2r=o(" (OpenAI GPT-2 model)"),U2r=l(),P9=a("li"),$ue=a("strong"),J2r=o("gpt_neo"),Y2r=o(" \u2014 "),BO=a("a"),K2r=o("FlaxGPTNeoForCausalLM"),Z2r=o(" (GPT Neo model)"),e1r=l(),$9=a("li"),Iue=a("strong"),o1r=o("gptj"),r1r=o(" \u2014 "),kO=a("a"),t1r=o("FlaxGPTJForCausalLM"),a1r=o(" (GPT-J model)"),n1r=l(),I9=a("li"),jue=a("strong"),s1r=o("xglm"),l1r=o(" \u2014 "),xO=a("a"),i1r=o("FlaxXGLMForCausalLM"),d1r=o(" (XGLM model)"),c1r=l(),Nue=a("p"),f1r=o("Examples:"),m1r=l(),f(yA.$$.fragment),I8e=l(),Wc=a("h2"),j9=a("a"),Due=a("span"),f(wA.$$.fragment),g1r=l(),que=a("span"),h1r=o("FlaxAutoModelForPreTraining"),j8e=l(),Lr=a("div"),f(AA.$$.fragment),p1r=l(),Qc=a("p"),_1r=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a pretraining head) when created
with the `),Gue=a("code"),u1r=o("from_pretrained()"),b1r=o("class method or the "),Oue=a("code"),v1r=o("from_config()"),T1r=o(`class
method.`),F1r=l(),LA=a("p"),C1r=o("This class cannot be instantiated directly using "),Xue=a("code"),M1r=o("__init__()"),E1r=o(" (throws an error)."),y1r=l(),Ft=a("div"),f(BA.$$.fragment),w1r=l(),zue=a("p"),A1r=o("Instantiates one of the model classes of the library (with a pretraining head) from a configuration."),L1r=l(),Hc=a("p"),B1r=o(`Note:
Loading a model from its configuration file does `),Vue=a("strong"),k1r=o("not"),x1r=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Wue=a("code"),R1r=o("from_pretrained()"),S1r=o("to load the model weights."),P1r=l(),Que=a("p"),$1r=o("Examples:"),I1r=l(),f(kA.$$.fragment),j1r=l(),Lo=a("div"),f(xA.$$.fragment),N1r=l(),Hue=a("p"),D1r=o("Instantiate one of the model classes of the library (with a pretraining head) from a pretrained model."),q1r=l(),En=a("p"),G1r=o("The model class to instantiate is selected based on the "),Uue=a("code"),O1r=o("model_type"),X1r=o(` property of the config object (either
passed as an argument or loaded from `),Jue=a("code"),z1r=o("pretrained_model_name_or_path"),V1r=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Yue=a("code"),W1r=o("pretrained_model_name_or_path"),Q1r=o(":"),H1r=l(),fe=a("ul"),N9=a("li"),Kue=a("strong"),U1r=o("albert"),J1r=o(" \u2014 "),RO=a("a"),Y1r=o("FlaxAlbertForPreTraining"),K1r=o(" (ALBERT model)"),Z1r=l(),D9=a("li"),Zue=a("strong"),ebr=o("bart"),obr=o(" \u2014 "),SO=a("a"),rbr=o("FlaxBartForConditionalGeneration"),tbr=o(" (BART model)"),abr=l(),q9=a("li"),e2e=a("strong"),nbr=o("bert"),sbr=o(" \u2014 "),PO=a("a"),lbr=o("FlaxBertForPreTraining"),ibr=o(" (BERT model)"),dbr=l(),G9=a("li"),o2e=a("strong"),cbr=o("big_bird"),fbr=o(" \u2014 "),$O=a("a"),mbr=o("FlaxBigBirdForPreTraining"),gbr=o(" (BigBird model)"),hbr=l(),O9=a("li"),r2e=a("strong"),pbr=o("electra"),_br=o(" \u2014 "),IO=a("a"),ubr=o("FlaxElectraForPreTraining"),bbr=o(" (ELECTRA model)"),vbr=l(),X9=a("li"),t2e=a("strong"),Tbr=o("mbart"),Fbr=o(" \u2014 "),jO=a("a"),Cbr=o("FlaxMBartForConditionalGeneration"),Mbr=o(" (mBART model)"),Ebr=l(),z9=a("li"),a2e=a("strong"),ybr=o("mt5"),wbr=o(" \u2014 "),NO=a("a"),Abr=o("FlaxMT5ForConditionalGeneration"),Lbr=o(" (mT5 model)"),Bbr=l(),V9=a("li"),n2e=a("strong"),kbr=o("roberta"),xbr=o(" \u2014 "),DO=a("a"),Rbr=o("FlaxRobertaForMaskedLM"),Sbr=o(" (RoBERTa model)"),Pbr=l(),W9=a("li"),s2e=a("strong"),$br=o("roformer"),Ibr=o(" \u2014 "),qO=a("a"),jbr=o("FlaxRoFormerForMaskedLM"),Nbr=o(" (RoFormer model)"),Dbr=l(),Q9=a("li"),l2e=a("strong"),qbr=o("t5"),Gbr=o(" \u2014 "),GO=a("a"),Obr=o("FlaxT5ForConditionalGeneration"),Xbr=o(" (T5 model)"),zbr=l(),H9=a("li"),i2e=a("strong"),Vbr=o("wav2vec2"),Wbr=o(" \u2014 "),OO=a("a"),Qbr=o("FlaxWav2Vec2ForPreTraining"),Hbr=o(" (Wav2Vec2 model)"),Ubr=l(),d2e=a("p"),Jbr=o("Examples:"),Ybr=l(),f(RA.$$.fragment),N8e=l(),Uc=a("h2"),U9=a("a"),c2e=a("span"),f(SA.$$.fragment),Kbr=l(),f2e=a("span"),Zbr=o("FlaxAutoModelForMaskedLM"),D8e=l(),Br=a("div"),f(PA.$$.fragment),e5r=l(),Jc=a("p"),o5r=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a masked language modeling head) when created
with the `),m2e=a("code"),r5r=o("from_pretrained()"),t5r=o("class method or the "),g2e=a("code"),a5r=o("from_config()"),n5r=o(`class
method.`),s5r=l(),$A=a("p"),l5r=o("This class cannot be instantiated directly using "),h2e=a("code"),i5r=o("__init__()"),d5r=o(" (throws an error)."),c5r=l(),Ct=a("div"),f(IA.$$.fragment),f5r=l(),p2e=a("p"),m5r=o("Instantiates one of the model classes of the library (with a masked language modeling head) from a configuration."),g5r=l(),Yc=a("p"),h5r=o(`Note:
Loading a model from its configuration file does `),_2e=a("strong"),p5r=o("not"),_5r=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),u2e=a("code"),u5r=o("from_pretrained()"),b5r=o("to load the model weights."),v5r=l(),b2e=a("p"),T5r=o("Examples:"),F5r=l(),f(jA.$$.fragment),C5r=l(),Bo=a("div"),f(NA.$$.fragment),M5r=l(),v2e=a("p"),E5r=o("Instantiate one of the model classes of the library (with a masked language modeling head) from a pretrained model."),y5r=l(),yn=a("p"),w5r=o("The model class to instantiate is selected based on the "),T2e=a("code"),A5r=o("model_type"),L5r=o(` property of the config object (either
passed as an argument or loaded from `),F2e=a("code"),B5r=o("pretrained_model_name_or_path"),k5r=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),C2e=a("code"),x5r=o("pretrained_model_name_or_path"),R5r=o(":"),S5r=l(),ve=a("ul"),J9=a("li"),M2e=a("strong"),P5r=o("albert"),$5r=o(" \u2014 "),XO=a("a"),I5r=o("FlaxAlbertForMaskedLM"),j5r=o(" (ALBERT model)"),N5r=l(),Y9=a("li"),E2e=a("strong"),D5r=o("bart"),q5r=o(" \u2014 "),zO=a("a"),G5r=o("FlaxBartForConditionalGeneration"),O5r=o(" (BART model)"),X5r=l(),K9=a("li"),y2e=a("strong"),z5r=o("bert"),V5r=o(" \u2014 "),VO=a("a"),W5r=o("FlaxBertForMaskedLM"),Q5r=o(" (BERT model)"),H5r=l(),Z9=a("li"),w2e=a("strong"),U5r=o("big_bird"),J5r=o(" \u2014 "),WO=a("a"),Y5r=o("FlaxBigBirdForMaskedLM"),K5r=o(" (BigBird model)"),Z5r=l(),eC=a("li"),A2e=a("strong"),evr=o("distilbert"),ovr=o(" \u2014 "),QO=a("a"),rvr=o("FlaxDistilBertForMaskedLM"),tvr=o(" (DistilBERT model)"),avr=l(),oC=a("li"),L2e=a("strong"),nvr=o("electra"),svr=o(" \u2014 "),HO=a("a"),lvr=o("FlaxElectraForMaskedLM"),ivr=o(" (ELECTRA model)"),dvr=l(),rC=a("li"),B2e=a("strong"),cvr=o("mbart"),fvr=o(" \u2014 "),UO=a("a"),mvr=o("FlaxMBartForConditionalGeneration"),gvr=o(" (mBART model)"),hvr=l(),tC=a("li"),k2e=a("strong"),pvr=o("roberta"),_vr=o(" \u2014 "),JO=a("a"),uvr=o("FlaxRobertaForMaskedLM"),bvr=o(" (RoBERTa model)"),vvr=l(),aC=a("li"),x2e=a("strong"),Tvr=o("roformer"),Fvr=o(" \u2014 "),YO=a("a"),Cvr=o("FlaxRoFormerForMaskedLM"),Mvr=o(" (RoFormer model)"),Evr=l(),R2e=a("p"),yvr=o("Examples:"),wvr=l(),f(DA.$$.fragment),q8e=l(),Kc=a("h2"),nC=a("a"),S2e=a("span"),f(qA.$$.fragment),Avr=l(),P2e=a("span"),Lvr=o("FlaxAutoModelForSeq2SeqLM"),G8e=l(),kr=a("div"),f(GA.$$.fragment),Bvr=l(),Zc=a("p"),kvr=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence language modeling head) when created
with the `),$2e=a("code"),xvr=o("from_pretrained()"),Rvr=o("class method or the "),I2e=a("code"),Svr=o("from_config()"),Pvr=o(`class
method.`),$vr=l(),OA=a("p"),Ivr=o("This class cannot be instantiated directly using "),j2e=a("code"),jvr=o("__init__()"),Nvr=o(" (throws an error)."),Dvr=l(),Mt=a("div"),f(XA.$$.fragment),qvr=l(),N2e=a("p"),Gvr=o("Instantiates one of the model classes of the library (with a sequence-to-sequence language modeling head) from a configuration."),Ovr=l(),ef=a("p"),Xvr=o(`Note:
Loading a model from its configuration file does `),D2e=a("strong"),zvr=o("not"),Vvr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),q2e=a("code"),Wvr=o("from_pretrained()"),Qvr=o("to load the model weights."),Hvr=l(),G2e=a("p"),Uvr=o("Examples:"),Jvr=l(),f(zA.$$.fragment),Yvr=l(),ko=a("div"),f(VA.$$.fragment),Kvr=l(),O2e=a("p"),Zvr=o("Instantiate one of the model classes of the library (with a sequence-to-sequence language modeling head) from a pretrained model."),eTr=l(),wn=a("p"),oTr=o("The model class to instantiate is selected based on the "),X2e=a("code"),rTr=o("model_type"),tTr=o(` property of the config object (either
passed as an argument or loaded from `),z2e=a("code"),aTr=o("pretrained_model_name_or_path"),nTr=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),V2e=a("code"),sTr=o("pretrained_model_name_or_path"),lTr=o(":"),iTr=l(),Te=a("ul"),sC=a("li"),W2e=a("strong"),dTr=o("bart"),cTr=o(" \u2014 "),KO=a("a"),fTr=o("FlaxBartForConditionalGeneration"),mTr=o(" (BART model)"),gTr=l(),lC=a("li"),Q2e=a("strong"),hTr=o("blenderbot"),pTr=o(" \u2014 "),ZO=a("a"),_Tr=o("FlaxBlenderbotForConditionalGeneration"),uTr=o(" (Blenderbot model)"),bTr=l(),iC=a("li"),H2e=a("strong"),vTr=o("blenderbot-small"),TTr=o(" \u2014 "),eX=a("a"),FTr=o("FlaxBlenderbotSmallForConditionalGeneration"),CTr=o(" (BlenderbotSmall model)"),MTr=l(),dC=a("li"),U2e=a("strong"),ETr=o("encoder-decoder"),yTr=o(" \u2014 "),oX=a("a"),wTr=o("FlaxEncoderDecoderModel"),ATr=o(" (Encoder decoder model)"),LTr=l(),cC=a("li"),J2e=a("strong"),BTr=o("marian"),kTr=o(" \u2014 "),rX=a("a"),xTr=o("FlaxMarianMTModel"),RTr=o(" (Marian model)"),STr=l(),fC=a("li"),Y2e=a("strong"),PTr=o("mbart"),$Tr=o(" \u2014 "),tX=a("a"),ITr=o("FlaxMBartForConditionalGeneration"),jTr=o(" (mBART model)"),NTr=l(),mC=a("li"),K2e=a("strong"),DTr=o("mt5"),qTr=o(" \u2014 "),aX=a("a"),GTr=o("FlaxMT5ForConditionalGeneration"),OTr=o(" (mT5 model)"),XTr=l(),gC=a("li"),Z2e=a("strong"),zTr=o("pegasus"),VTr=o(" \u2014 "),nX=a("a"),WTr=o("FlaxPegasusForConditionalGeneration"),QTr=o(" (Pegasus model)"),HTr=l(),hC=a("li"),e1e=a("strong"),UTr=o("t5"),JTr=o(" \u2014 "),sX=a("a"),YTr=o("FlaxT5ForConditionalGeneration"),KTr=o(" (T5 model)"),ZTr=l(),o1e=a("p"),e7r=o("Examples:"),o7r=l(),f(WA.$$.fragment),O8e=l(),of=a("h2"),pC=a("a"),r1e=a("span"),f(QA.$$.fragment),r7r=l(),t1e=a("span"),t7r=o("FlaxAutoModelForSequenceClassification"),X8e=l(),xr=a("div"),f(HA.$$.fragment),a7r=l(),rf=a("p"),n7r=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence classification head) when created
with the `),a1e=a("code"),s7r=o("from_pretrained()"),l7r=o("class method or the "),n1e=a("code"),i7r=o("from_config()"),d7r=o(`class
method.`),c7r=l(),UA=a("p"),f7r=o("This class cannot be instantiated directly using "),s1e=a("code"),m7r=o("__init__()"),g7r=o(" (throws an error)."),h7r=l(),Et=a("div"),f(JA.$$.fragment),p7r=l(),l1e=a("p"),_7r=o("Instantiates one of the model classes of the library (with a sequence classification head) from a configuration."),u7r=l(),tf=a("p"),b7r=o(`Note:
Loading a model from its configuration file does `),i1e=a("strong"),v7r=o("not"),T7r=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),d1e=a("code"),F7r=o("from_pretrained()"),C7r=o("to load the model weights."),M7r=l(),c1e=a("p"),E7r=o("Examples:"),y7r=l(),f(YA.$$.fragment),w7r=l(),xo=a("div"),f(KA.$$.fragment),A7r=l(),f1e=a("p"),L7r=o("Instantiate one of the model classes of the library (with a sequence classification head) from a pretrained model."),B7r=l(),An=a("p"),k7r=o("The model class to instantiate is selected based on the "),m1e=a("code"),x7r=o("model_type"),R7r=o(` property of the config object (either
passed as an argument or loaded from `),g1e=a("code"),S7r=o("pretrained_model_name_or_path"),P7r=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),h1e=a("code"),$7r=o("pretrained_model_name_or_path"),I7r=o(":"),j7r=l(),Fe=a("ul"),_C=a("li"),p1e=a("strong"),N7r=o("albert"),D7r=o(" \u2014 "),lX=a("a"),q7r=o("FlaxAlbertForSequenceClassification"),G7r=o(" (ALBERT model)"),O7r=l(),uC=a("li"),_1e=a("strong"),X7r=o("bart"),z7r=o(" \u2014 "),iX=a("a"),V7r=o("FlaxBartForSequenceClassification"),W7r=o(" (BART model)"),Q7r=l(),bC=a("li"),u1e=a("strong"),H7r=o("bert"),U7r=o(" \u2014 "),dX=a("a"),J7r=o("FlaxBertForSequenceClassification"),Y7r=o(" (BERT model)"),K7r=l(),vC=a("li"),b1e=a("strong"),Z7r=o("big_bird"),eFr=o(" \u2014 "),cX=a("a"),oFr=o("FlaxBigBirdForSequenceClassification"),rFr=o(" (BigBird model)"),tFr=l(),TC=a("li"),v1e=a("strong"),aFr=o("distilbert"),nFr=o(" \u2014 "),fX=a("a"),sFr=o("FlaxDistilBertForSequenceClassification"),lFr=o(" (DistilBERT model)"),iFr=l(),FC=a("li"),T1e=a("strong"),dFr=o("electra"),cFr=o(" \u2014 "),mX=a("a"),fFr=o("FlaxElectraForSequenceClassification"),mFr=o(" (ELECTRA model)"),gFr=l(),CC=a("li"),F1e=a("strong"),hFr=o("mbart"),pFr=o(" \u2014 "),gX=a("a"),_Fr=o("FlaxMBartForSequenceClassification"),uFr=o(" (mBART model)"),bFr=l(),MC=a("li"),C1e=a("strong"),vFr=o("roberta"),TFr=o(" \u2014 "),hX=a("a"),FFr=o("FlaxRobertaForSequenceClassification"),CFr=o(" (RoBERTa model)"),MFr=l(),EC=a("li"),M1e=a("strong"),EFr=o("roformer"),yFr=o(" \u2014 "),pX=a("a"),wFr=o("FlaxRoFormerForSequenceClassification"),AFr=o(" (RoFormer model)"),LFr=l(),E1e=a("p"),BFr=o("Examples:"),kFr=l(),f(ZA.$$.fragment),z8e=l(),af=a("h2"),yC=a("a"),y1e=a("span"),f(e6.$$.fragment),xFr=l(),w1e=a("span"),RFr=o("FlaxAutoModelForQuestionAnswering"),V8e=l(),Rr=a("div"),f(o6.$$.fragment),SFr=l(),nf=a("p"),PFr=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a question answering head) when created
with the `),A1e=a("code"),$Fr=o("from_pretrained()"),IFr=o("class method or the "),L1e=a("code"),jFr=o("from_config()"),NFr=o(`class
method.`),DFr=l(),r6=a("p"),qFr=o("This class cannot be instantiated directly using "),B1e=a("code"),GFr=o("__init__()"),OFr=o(" (throws an error)."),XFr=l(),yt=a("div"),f(t6.$$.fragment),zFr=l(),k1e=a("p"),VFr=o("Instantiates one of the model classes of the library (with a question answering head) from a configuration."),WFr=l(),sf=a("p"),QFr=o(`Note:
Loading a model from its configuration file does `),x1e=a("strong"),HFr=o("not"),UFr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),R1e=a("code"),JFr=o("from_pretrained()"),YFr=o("to load the model weights."),KFr=l(),S1e=a("p"),ZFr=o("Examples:"),e9r=l(),f(a6.$$.fragment),o9r=l(),Ro=a("div"),f(n6.$$.fragment),r9r=l(),P1e=a("p"),t9r=o("Instantiate one of the model classes of the library (with a question answering head) from a pretrained model."),a9r=l(),Ln=a("p"),n9r=o("The model class to instantiate is selected based on the "),$1e=a("code"),s9r=o("model_type"),l9r=o(` property of the config object (either
passed as an argument or loaded from `),I1e=a("code"),i9r=o("pretrained_model_name_or_path"),d9r=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),j1e=a("code"),c9r=o("pretrained_model_name_or_path"),f9r=o(":"),m9r=l(),Ce=a("ul"),wC=a("li"),N1e=a("strong"),g9r=o("albert"),h9r=o(" \u2014 "),_X=a("a"),p9r=o("FlaxAlbertForQuestionAnswering"),_9r=o(" (ALBERT model)"),u9r=l(),AC=a("li"),D1e=a("strong"),b9r=o("bart"),v9r=o(" \u2014 "),uX=a("a"),T9r=o("FlaxBartForQuestionAnswering"),F9r=o(" (BART model)"),C9r=l(),LC=a("li"),q1e=a("strong"),M9r=o("bert"),E9r=o(" \u2014 "),bX=a("a"),y9r=o("FlaxBertForQuestionAnswering"),w9r=o(" (BERT model)"),A9r=l(),BC=a("li"),G1e=a("strong"),L9r=o("big_bird"),B9r=o(" \u2014 "),vX=a("a"),k9r=o("FlaxBigBirdForQuestionAnswering"),x9r=o(" (BigBird model)"),R9r=l(),kC=a("li"),O1e=a("strong"),S9r=o("distilbert"),P9r=o(" \u2014 "),TX=a("a"),$9r=o("FlaxDistilBertForQuestionAnswering"),I9r=o(" (DistilBERT model)"),j9r=l(),xC=a("li"),X1e=a("strong"),N9r=o("electra"),D9r=o(" \u2014 "),FX=a("a"),q9r=o("FlaxElectraForQuestionAnswering"),G9r=o(" (ELECTRA model)"),O9r=l(),RC=a("li"),z1e=a("strong"),X9r=o("mbart"),z9r=o(" \u2014 "),CX=a("a"),V9r=o("FlaxMBartForQuestionAnswering"),W9r=o(" (mBART model)"),Q9r=l(),SC=a("li"),V1e=a("strong"),H9r=o("roberta"),U9r=o(" \u2014 "),MX=a("a"),J9r=o("FlaxRobertaForQuestionAnswering"),Y9r=o(" (RoBERTa model)"),K9r=l(),PC=a("li"),W1e=a("strong"),Z9r=o("roformer"),eCr=o(" \u2014 "),EX=a("a"),oCr=o("FlaxRoFormerForQuestionAnswering"),rCr=o(" (RoFormer model)"),tCr=l(),Q1e=a("p"),aCr=o("Examples:"),nCr=l(),f(s6.$$.fragment),W8e=l(),lf=a("h2"),$C=a("a"),H1e=a("span"),f(l6.$$.fragment),sCr=l(),U1e=a("span"),lCr=o("FlaxAutoModelForTokenClassification"),Q8e=l(),Sr=a("div"),f(i6.$$.fragment),iCr=l(),df=a("p"),dCr=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a token classification head) when created
with the `),J1e=a("code"),cCr=o("from_pretrained()"),fCr=o("class method or the "),Y1e=a("code"),mCr=o("from_config()"),gCr=o(`class
method.`),hCr=l(),d6=a("p"),pCr=o("This class cannot be instantiated directly using "),K1e=a("code"),_Cr=o("__init__()"),uCr=o(" (throws an error)."),bCr=l(),wt=a("div"),f(c6.$$.fragment),vCr=l(),Z1e=a("p"),TCr=o("Instantiates one of the model classes of the library (with a token classification head) from a configuration."),FCr=l(),cf=a("p"),CCr=o(`Note:
Loading a model from its configuration file does `),ebe=a("strong"),MCr=o("not"),ECr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),obe=a("code"),yCr=o("from_pretrained()"),wCr=o("to load the model weights."),ACr=l(),rbe=a("p"),LCr=o("Examples:"),BCr=l(),f(f6.$$.fragment),kCr=l(),So=a("div"),f(m6.$$.fragment),xCr=l(),tbe=a("p"),RCr=o("Instantiate one of the model classes of the library (with a token classification head) from a pretrained model."),SCr=l(),Bn=a("p"),PCr=o("The model class to instantiate is selected based on the "),abe=a("code"),$Cr=o("model_type"),ICr=o(` property of the config object (either
passed as an argument or loaded from `),nbe=a("code"),jCr=o("pretrained_model_name_or_path"),NCr=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),sbe=a("code"),DCr=o("pretrained_model_name_or_path"),qCr=o(":"),GCr=l(),so=a("ul"),IC=a("li"),lbe=a("strong"),OCr=o("albert"),XCr=o(" \u2014 "),yX=a("a"),zCr=o("FlaxAlbertForTokenClassification"),VCr=o(" (ALBERT model)"),WCr=l(),jC=a("li"),ibe=a("strong"),QCr=o("bert"),HCr=o(" \u2014 "),wX=a("a"),UCr=o("FlaxBertForTokenClassification"),JCr=o(" (BERT model)"),YCr=l(),NC=a("li"),dbe=a("strong"),KCr=o("big_bird"),ZCr=o(" \u2014 "),AX=a("a"),e4r=o("FlaxBigBirdForTokenClassification"),o4r=o(" (BigBird model)"),r4r=l(),DC=a("li"),cbe=a("strong"),t4r=o("distilbert"),a4r=o(" \u2014 "),LX=a("a"),n4r=o("FlaxDistilBertForTokenClassification"),s4r=o(" (DistilBERT model)"),l4r=l(),qC=a("li"),fbe=a("strong"),i4r=o("electra"),d4r=o(" \u2014 "),BX=a("a"),c4r=o("FlaxElectraForTokenClassification"),f4r=o(" (ELECTRA model)"),m4r=l(),GC=a("li"),mbe=a("strong"),g4r=o("roberta"),h4r=o(" \u2014 "),kX=a("a"),p4r=o("FlaxRobertaForTokenClassification"),_4r=o(" (RoBERTa model)"),u4r=l(),OC=a("li"),gbe=a("strong"),b4r=o("roformer"),v4r=o(" \u2014 "),xX=a("a"),T4r=o("FlaxRoFormerForTokenClassification"),F4r=o(" (RoFormer model)"),C4r=l(),hbe=a("p"),M4r=o("Examples:"),E4r=l(),f(g6.$$.fragment),H8e=l(),ff=a("h2"),XC=a("a"),pbe=a("span"),f(h6.$$.fragment),y4r=l(),_be=a("span"),w4r=o("FlaxAutoModelForMultipleChoice"),U8e=l(),Pr=a("div"),f(p6.$$.fragment),A4r=l(),mf=a("p"),L4r=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a multiple choice head) when created
with the `),ube=a("code"),B4r=o("from_pretrained()"),k4r=o("class method or the "),bbe=a("code"),x4r=o("from_config()"),R4r=o(`class
method.`),S4r=l(),_6=a("p"),P4r=o("This class cannot be instantiated directly using "),vbe=a("code"),$4r=o("__init__()"),I4r=o(" (throws an error)."),j4r=l(),At=a("div"),f(u6.$$.fragment),N4r=l(),Tbe=a("p"),D4r=o("Instantiates one of the model classes of the library (with a multiple choice head) from a configuration."),q4r=l(),gf=a("p"),G4r=o(`Note:
Loading a model from its configuration file does `),Fbe=a("strong"),O4r=o("not"),X4r=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Cbe=a("code"),z4r=o("from_pretrained()"),V4r=o("to load the model weights."),W4r=l(),Mbe=a("p"),Q4r=o("Examples:"),H4r=l(),f(b6.$$.fragment),U4r=l(),Po=a("div"),f(v6.$$.fragment),J4r=l(),Ebe=a("p"),Y4r=o("Instantiate one of the model classes of the library (with a multiple choice head) from a pretrained model."),K4r=l(),kn=a("p"),Z4r=o("The model class to instantiate is selected based on the "),ybe=a("code"),eMr=o("model_type"),oMr=o(` property of the config object (either
passed as an argument or loaded from `),wbe=a("code"),rMr=o("pretrained_model_name_or_path"),tMr=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Abe=a("code"),aMr=o("pretrained_model_name_or_path"),nMr=o(":"),sMr=l(),lo=a("ul"),zC=a("li"),Lbe=a("strong"),lMr=o("albert"),iMr=o(" \u2014 "),RX=a("a"),dMr=o("FlaxAlbertForMultipleChoice"),cMr=o(" (ALBERT model)"),fMr=l(),VC=a("li"),Bbe=a("strong"),mMr=o("bert"),gMr=o(" \u2014 "),SX=a("a"),hMr=o("FlaxBertForMultipleChoice"),pMr=o(" (BERT model)"),_Mr=l(),WC=a("li"),kbe=a("strong"),uMr=o("big_bird"),bMr=o(" \u2014 "),PX=a("a"),vMr=o("FlaxBigBirdForMultipleChoice"),TMr=o(" (BigBird model)"),FMr=l(),QC=a("li"),xbe=a("strong"),CMr=o("distilbert"),MMr=o(" \u2014 "),$X=a("a"),EMr=o("FlaxDistilBertForMultipleChoice"),yMr=o(" (DistilBERT model)"),wMr=l(),HC=a("li"),Rbe=a("strong"),AMr=o("electra"),LMr=o(" \u2014 "),IX=a("a"),BMr=o("FlaxElectraForMultipleChoice"),kMr=o(" (ELECTRA model)"),xMr=l(),UC=a("li"),Sbe=a("strong"),RMr=o("roberta"),SMr=o(" \u2014 "),jX=a("a"),PMr=o("FlaxRobertaForMultipleChoice"),$Mr=o(" (RoBERTa model)"),IMr=l(),JC=a("li"),Pbe=a("strong"),jMr=o("roformer"),NMr=o(" \u2014 "),NX=a("a"),DMr=o("FlaxRoFormerForMultipleChoice"),qMr=o(" (RoFormer model)"),GMr=l(),$be=a("p"),OMr=o("Examples:"),XMr=l(),f(T6.$$.fragment),J8e=l(),hf=a("h2"),YC=a("a"),Ibe=a("span"),f(F6.$$.fragment),zMr=l(),jbe=a("span"),VMr=o("FlaxAutoModelForNextSentencePrediction"),Y8e=l(),$r=a("div"),f(C6.$$.fragment),WMr=l(),pf=a("p"),QMr=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a next sentence prediction head) when created
with the `),Nbe=a("code"),HMr=o("from_pretrained()"),UMr=o("class method or the "),Dbe=a("code"),JMr=o("from_config()"),YMr=o(`class
method.`),KMr=l(),M6=a("p"),ZMr=o("This class cannot be instantiated directly using "),qbe=a("code"),eEr=o("__init__()"),oEr=o(" (throws an error)."),rEr=l(),Lt=a("div"),f(E6.$$.fragment),tEr=l(),Gbe=a("p"),aEr=o("Instantiates one of the model classes of the library (with a next sentence prediction head) from a configuration."),nEr=l(),_f=a("p"),sEr=o(`Note:
Loading a model from its configuration file does `),Obe=a("strong"),lEr=o("not"),iEr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Xbe=a("code"),dEr=o("from_pretrained()"),cEr=o("to load the model weights."),fEr=l(),zbe=a("p"),mEr=o("Examples:"),gEr=l(),f(y6.$$.fragment),hEr=l(),$o=a("div"),f(w6.$$.fragment),pEr=l(),Vbe=a("p"),_Er=o("Instantiate one of the model classes of the library (with a next sentence prediction head) from a pretrained model."),uEr=l(),xn=a("p"),bEr=o("The model class to instantiate is selected based on the "),Wbe=a("code"),vEr=o("model_type"),TEr=o(` property of the config object (either
passed as an argument or loaded from `),Qbe=a("code"),FEr=o("pretrained_model_name_or_path"),CEr=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Hbe=a("code"),MEr=o("pretrained_model_name_or_path"),EEr=o(":"),yEr=l(),Ube=a("ul"),KC=a("li"),Jbe=a("strong"),wEr=o("bert"),AEr=o(" \u2014 "),DX=a("a"),LEr=o("FlaxBertForNextSentencePrediction"),BEr=o(" (BERT model)"),kEr=l(),Ybe=a("p"),xEr=o("Examples:"),REr=l(),f(A6.$$.fragment),K8e=l(),uf=a("h2"),ZC=a("a"),Kbe=a("span"),f(L6.$$.fragment),SEr=l(),Zbe=a("span"),PEr=o("FlaxAutoModelForImageClassification"),Z8e=l(),Ir=a("div"),f(B6.$$.fragment),$Er=l(),bf=a("p"),IEr=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a image classification head) when created
with the `),e5e=a("code"),jEr=o("from_pretrained()"),NEr=o("class method or the "),o5e=a("code"),DEr=o("from_config()"),qEr=o(`class
method.`),GEr=l(),k6=a("p"),OEr=o("This class cannot be instantiated directly using "),r5e=a("code"),XEr=o("__init__()"),zEr=o(" (throws an error)."),VEr=l(),Bt=a("div"),f(x6.$$.fragment),WEr=l(),t5e=a("p"),QEr=o("Instantiates one of the model classes of the library (with a image classification head) from a configuration."),HEr=l(),vf=a("p"),UEr=o(`Note:
Loading a model from its configuration file does `),a5e=a("strong"),JEr=o("not"),YEr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),n5e=a("code"),KEr=o("from_pretrained()"),ZEr=o("to load the model weights."),e3r=l(),s5e=a("p"),o3r=o("Examples:"),r3r=l(),f(R6.$$.fragment),t3r=l(),Io=a("div"),f(S6.$$.fragment),a3r=l(),l5e=a("p"),n3r=o("Instantiate one of the model classes of the library (with a image classification head) from a pretrained model."),s3r=l(),Rn=a("p"),l3r=o("The model class to instantiate is selected based on the "),i5e=a("code"),i3r=o("model_type"),d3r=o(` property of the config object (either
passed as an argument or loaded from `),d5e=a("code"),c3r=o("pretrained_model_name_or_path"),f3r=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),c5e=a("code"),m3r=o("pretrained_model_name_or_path"),g3r=o(":"),h3r=l(),P6=a("ul"),e4=a("li"),f5e=a("strong"),p3r=o("beit"),_3r=o(" \u2014 "),qX=a("a"),u3r=o("FlaxBeitForImageClassification"),b3r=o(" (BEiT model)"),v3r=l(),o4=a("li"),m5e=a("strong"),T3r=o("vit"),F3r=o(" \u2014 "),GX=a("a"),C3r=o("FlaxViTForImageClassification"),M3r=o(" (ViT model)"),E3r=l(),g5e=a("p"),y3r=o("Examples:"),w3r=l(),f($6.$$.fragment),eBe=l(),Tf=a("h2"),r4=a("a"),h5e=a("span"),f(I6.$$.fragment),A3r=l(),p5e=a("span"),L3r=o("FlaxAutoModelForVision2Seq"),oBe=l(),jr=a("div"),f(j6.$$.fragment),B3r=l(),Ff=a("p"),k3r=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a vision-to-text modeling head) when created
with the `),_5e=a("code"),x3r=o("from_pretrained()"),R3r=o("class method or the "),u5e=a("code"),S3r=o("from_config()"),P3r=o(`class
method.`),$3r=l(),N6=a("p"),I3r=o("This class cannot be instantiated directly using "),b5e=a("code"),j3r=o("__init__()"),N3r=o(" (throws an error)."),D3r=l(),kt=a("div"),f(D6.$$.fragment),q3r=l(),v5e=a("p"),G3r=o("Instantiates one of the model classes of the library (with a vision-to-text modeling head) from a configuration."),O3r=l(),Cf=a("p"),X3r=o(`Note:
Loading a model from its configuration file does `),T5e=a("strong"),z3r=o("not"),V3r=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),F5e=a("code"),W3r=o("from_pretrained()"),Q3r=o("to load the model weights."),H3r=l(),C5e=a("p"),U3r=o("Examples:"),J3r=l(),f(q6.$$.fragment),Y3r=l(),jo=a("div"),f(G6.$$.fragment),K3r=l(),M5e=a("p"),Z3r=o("Instantiate one of the model classes of the library (with a vision-to-text modeling head) from a pretrained model."),eyr=l(),Sn=a("p"),oyr=o("The model class to instantiate is selected based on the "),E5e=a("code"),ryr=o("model_type"),tyr=o(` property of the config object (either
passed as an argument or loaded from `),y5e=a("code"),ayr=o("pretrained_model_name_or_path"),nyr=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),w5e=a("code"),syr=o("pretrained_model_name_or_path"),lyr=o(":"),iyr=l(),A5e=a("ul"),t4=a("li"),L5e=a("strong"),dyr=o("vision-encoder-decoder"),cyr=o(" \u2014 "),OX=a("a"),fyr=o("FlaxVisionEncoderDecoderModel"),myr=o(" (Vision Encoder decoder model)"),gyr=l(),B5e=a("p"),hyr=o("Examples:"),pyr=l(),f(O6.$$.fragment),this.h()},l(d){const u=T_t('[data-svelte="svelte-1phssyn"]',document.head);J=n(u,"META",{name:!0,content:!0}),u.forEach(t),Ae=i(d),ie=n(d,"H1",{class:!0});var X6=s(ie);me=n(X6,"A",{id:!0,class:!0,href:!0});var k5e=s(me);to=n(k5e,"SPAN",{});var x5e=s(to);m(ce.$$.fragment,x5e),x5e.forEach(t),k5e.forEach(t),ue=i(X6),Do=n(X6,"SPAN",{});var uyr=s(Do);wi=r(uyr,"Auto Classes"),uyr.forEach(t),X6.forEach(t),Ef=i(d),sa=n(d,"P",{});var tBe=s(sa);Ai=r(tBe,`In many cases, the architecture you want to use can be guessed from the name or the path of the pretrained model you
are supplying to the `),Li=n(tBe,"CODE",{});var byr=s(Li);oM=r(byr,"from_pretrained()"),byr.forEach(t),yf=r(tBe,` method. AutoClasses are here to do this job for you so that you
automatically retrieve the relevant model given the name/path to the pretrained weights/config/vocabulary.`),tBe.forEach(t),ye=i(d),io=n(d,"P",{});var a4=s(io);Bi=r(a4,"Instantiating one of "),Pn=n(a4,"A",{href:!0});var vyr=s(Pn);rM=r(vyr,"AutoConfig"),vyr.forEach(t),$n=r(a4,", "),In=n(a4,"A",{href:!0});var Tyr=s(In);tM=r(Tyr,"AutoModel"),Tyr.forEach(t),ki=r(a4,`, and
`),jn=n(a4,"A",{href:!0});var Fyr=s(jn);aM=r(Fyr,"AutoTokenizer"),Fyr.forEach(t),xi=r(a4," will directly create a class of the relevant architecture. For instance"),a4.forEach(t),wf=i(d),m($a.$$.fragment,d),co=i(d),ge=n(d,"P",{});var aBe=s(ge);D0=r(aBe,"will create a model that is an instance of "),Ri=n(aBe,"A",{href:!0});var Cyr=s(Ri);q0=r(Cyr,"BertModel"),Cyr.forEach(t),G0=r(aBe,"."),aBe.forEach(t),qo=i(d),Ia=n(d,"P",{});var nBe=s(Ia);O0=r(nBe,"There is one class of "),Af=n(nBe,"CODE",{});var Myr=s(Af);X0=r(Myr,"AutoModel"),Myr.forEach(t),mxe=r(nBe," for each task, and for each backend (PyTorch, TensorFlow, or Flax)."),nBe.forEach(t),tLe=i(d),Si=n(d,"H2",{class:!0});var sBe=s(Si);Lf=n(sBe,"A",{id:!0,class:!0,href:!0});var Eyr=s(Lf);$V=n(Eyr,"SPAN",{});var yyr=s($V);m(nM.$$.fragment,yyr),yyr.forEach(t),Eyr.forEach(t),gxe=i(sBe),IV=n(sBe,"SPAN",{});var wyr=s(IV);hxe=r(wyr,"Extending the Auto Classes"),wyr.forEach(t),sBe.forEach(t),aLe=i(d),Nn=n(d,"P",{});var XX=s(Nn);pxe=r(XX,`Each of the auto classes has a method to be extended with your custom classes. For instance, if you have defined a
custom class of model `),jV=n(XX,"CODE",{});var Ayr=s(jV);_xe=r(Ayr,"NewModel"),Ayr.forEach(t),uxe=r(XX,", make sure you have a "),NV=n(XX,"CODE",{});var Lyr=s(NV);bxe=r(Lyr,"NewModelConfig"),Lyr.forEach(t),vxe=r(XX,` then you can add those to the auto
classes like this:`),XX.forEach(t),nLe=i(d),m(sM.$$.fragment,d),sLe=i(d),z0=n(d,"P",{});var Byr=s(z0);Txe=r(Byr,"You will then be able to use the auto classes like you would usually do!"),Byr.forEach(t),lLe=i(d),m(Bf.$$.fragment,d),iLe=i(d),Pi=n(d,"H2",{class:!0});var lBe=s(Pi);kf=n(lBe,"A",{id:!0,class:!0,href:!0});var kyr=s(kf);DV=n(kyr,"SPAN",{});var xyr=s(DV);m(lM.$$.fragment,xyr),xyr.forEach(t),kyr.forEach(t),Fxe=i(lBe),qV=n(lBe,"SPAN",{});var Ryr=s(qV);Cxe=r(Ryr,"AutoConfig"),Ryr.forEach(t),lBe.forEach(t),dLe=i(d),Go=n(d,"DIV",{class:!0});var Ps=s(Go);m(iM.$$.fragment,Ps),Mxe=i(Ps),dM=n(Ps,"P",{});var iBe=s(dM);Exe=r(iBe,`This is a generic configuration class that will be instantiated as one of the configuration classes of the library
when created with the `),V0=n(iBe,"A",{href:!0});var Syr=s(V0);yxe=r(Syr,"from_pretrained()"),Syr.forEach(t),wxe=r(iBe," class method."),iBe.forEach(t),Axe=i(Ps),cM=n(Ps,"P",{});var dBe=s(cM);Lxe=r(dBe,"This class cannot be instantiated directly using "),GV=n(dBe,"CODE",{});var Pyr=s(GV);Bxe=r(Pyr,"__init__()"),Pyr.forEach(t),kxe=r(dBe," (throws an error)."),dBe.forEach(t),xxe=i(Ps),fo=n(Ps,"DIV",{class:!0});var ia=s(fo);m(fM.$$.fragment,ia),Rxe=i(ia),OV=n(ia,"P",{});var $yr=s(OV);Sxe=r($yr,"Instantiate one of the configuration classes of the library from a pretrained model configuration."),$yr.forEach(t),Pxe=i(ia),$i=n(ia,"P",{});var zX=s($i);$xe=r(zX,"The configuration class to instantiate is selected based on the "),XV=n(zX,"CODE",{});var Iyr=s(XV);Ixe=r(Iyr,"model_type"),Iyr.forEach(t),jxe=r(zX,` property of the config object that
is loaded, or when it\u2019s missing, by falling back to using pattern matching on `),zV=n(zX,"CODE",{});var jyr=s(zV);Nxe=r(jyr,"pretrained_model_name_or_path"),jyr.forEach(t),Dxe=r(zX,":"),zX.forEach(t),qxe=i(ia),v=n(ia,"UL",{});var T=s(v);xf=n(T,"LI",{});var R5e=s(xf);VV=n(R5e,"STRONG",{});var Nyr=s(VV);Gxe=r(Nyr,"albert"),Nyr.forEach(t),Oxe=r(R5e," \u2014 "),W0=n(R5e,"A",{href:!0});var Dyr=s(W0);Xxe=r(Dyr,"AlbertConfig"),Dyr.forEach(t),zxe=r(R5e," (ALBERT model)"),R5e.forEach(t),Vxe=i(T),Rf=n(T,"LI",{});var S5e=s(Rf);WV=n(S5e,"STRONG",{});var qyr=s(WV);Wxe=r(qyr,"bart"),qyr.forEach(t),Qxe=r(S5e," \u2014 "),Q0=n(S5e,"A",{href:!0});var Gyr=s(Q0);Hxe=r(Gyr,"BartConfig"),Gyr.forEach(t),Uxe=r(S5e," (BART model)"),S5e.forEach(t),Jxe=i(T),Sf=n(T,"LI",{});var P5e=s(Sf);QV=n(P5e,"STRONG",{});var Oyr=s(QV);Yxe=r(Oyr,"beit"),Oyr.forEach(t),Kxe=r(P5e," \u2014 "),H0=n(P5e,"A",{href:!0});var Xyr=s(H0);Zxe=r(Xyr,"BeitConfig"),Xyr.forEach(t),eRe=r(P5e," (BEiT model)"),P5e.forEach(t),oRe=i(T),Pf=n(T,"LI",{});var $5e=s(Pf);HV=n($5e,"STRONG",{});var zyr=s(HV);rRe=r(zyr,"bert"),zyr.forEach(t),tRe=r($5e," \u2014 "),U0=n($5e,"A",{href:!0});var Vyr=s(U0);aRe=r(Vyr,"BertConfig"),Vyr.forEach(t),nRe=r($5e," (BERT model)"),$5e.forEach(t),sRe=i(T),$f=n(T,"LI",{});var I5e=s($f);UV=n(I5e,"STRONG",{});var Wyr=s(UV);lRe=r(Wyr,"bert-generation"),Wyr.forEach(t),iRe=r(I5e," \u2014 "),J0=n(I5e,"A",{href:!0});var Qyr=s(J0);dRe=r(Qyr,"BertGenerationConfig"),Qyr.forEach(t),cRe=r(I5e," (Bert Generation model)"),I5e.forEach(t),fRe=i(T),If=n(T,"LI",{});var j5e=s(If);JV=n(j5e,"STRONG",{});var Hyr=s(JV);mRe=r(Hyr,"big_bird"),Hyr.forEach(t),gRe=r(j5e," \u2014 "),Y0=n(j5e,"A",{href:!0});var Uyr=s(Y0);hRe=r(Uyr,"BigBirdConfig"),Uyr.forEach(t),pRe=r(j5e," (BigBird model)"),j5e.forEach(t),_Re=i(T),jf=n(T,"LI",{});var N5e=s(jf);YV=n(N5e,"STRONG",{});var Jyr=s(YV);uRe=r(Jyr,"bigbird_pegasus"),Jyr.forEach(t),bRe=r(N5e," \u2014 "),K0=n(N5e,"A",{href:!0});var Yyr=s(K0);vRe=r(Yyr,"BigBirdPegasusConfig"),Yyr.forEach(t),TRe=r(N5e," (BigBirdPegasus model)"),N5e.forEach(t),FRe=i(T),Nf=n(T,"LI",{});var D5e=s(Nf);KV=n(D5e,"STRONG",{});var Kyr=s(KV);CRe=r(Kyr,"blenderbot"),Kyr.forEach(t),MRe=r(D5e," \u2014 "),Z0=n(D5e,"A",{href:!0});var Zyr=s(Z0);ERe=r(Zyr,"BlenderbotConfig"),Zyr.forEach(t),yRe=r(D5e," (Blenderbot model)"),D5e.forEach(t),wRe=i(T),Df=n(T,"LI",{});var q5e=s(Df);ZV=n(q5e,"STRONG",{});var ewr=s(ZV);ARe=r(ewr,"blenderbot-small"),ewr.forEach(t),LRe=r(q5e," \u2014 "),eL=n(q5e,"A",{href:!0});var owr=s(eL);BRe=r(owr,"BlenderbotSmallConfig"),owr.forEach(t),kRe=r(q5e," (BlenderbotSmall model)"),q5e.forEach(t),xRe=i(T),qf=n(T,"LI",{});var G5e=s(qf);eW=n(G5e,"STRONG",{});var rwr=s(eW);RRe=r(rwr,"camembert"),rwr.forEach(t),SRe=r(G5e," \u2014 "),oL=n(G5e,"A",{href:!0});var twr=s(oL);PRe=r(twr,"CamembertConfig"),twr.forEach(t),$Re=r(G5e," (CamemBERT model)"),G5e.forEach(t),IRe=i(T),Gf=n(T,"LI",{});var O5e=s(Gf);oW=n(O5e,"STRONG",{});var awr=s(oW);jRe=r(awr,"canine"),awr.forEach(t),NRe=r(O5e," \u2014 "),rL=n(O5e,"A",{href:!0});var nwr=s(rL);DRe=r(nwr,"CanineConfig"),nwr.forEach(t),qRe=r(O5e," (Canine model)"),O5e.forEach(t),GRe=i(T),Of=n(T,"LI",{});var X5e=s(Of);rW=n(X5e,"STRONG",{});var swr=s(rW);ORe=r(swr,"clip"),swr.forEach(t),XRe=r(X5e," \u2014 "),tL=n(X5e,"A",{href:!0});var lwr=s(tL);zRe=r(lwr,"CLIPConfig"),lwr.forEach(t),VRe=r(X5e," (CLIP model)"),X5e.forEach(t),WRe=i(T),Xf=n(T,"LI",{});var z5e=s(Xf);tW=n(z5e,"STRONG",{});var iwr=s(tW);QRe=r(iwr,"convbert"),iwr.forEach(t),HRe=r(z5e," \u2014 "),aL=n(z5e,"A",{href:!0});var dwr=s(aL);URe=r(dwr,"ConvBertConfig"),dwr.forEach(t),JRe=r(z5e," (ConvBERT model)"),z5e.forEach(t),YRe=i(T),zf=n(T,"LI",{});var V5e=s(zf);aW=n(V5e,"STRONG",{});var cwr=s(aW);KRe=r(cwr,"convnext"),cwr.forEach(t),ZRe=r(V5e," \u2014 "),nL=n(V5e,"A",{href:!0});var fwr=s(nL);eSe=r(fwr,"ConvNextConfig"),fwr.forEach(t),oSe=r(V5e," (ConvNext model)"),V5e.forEach(t),rSe=i(T),Vf=n(T,"LI",{});var W5e=s(Vf);nW=n(W5e,"STRONG",{});var mwr=s(nW);tSe=r(mwr,"ctrl"),mwr.forEach(t),aSe=r(W5e," \u2014 "),sL=n(W5e,"A",{href:!0});var gwr=s(sL);nSe=r(gwr,"CTRLConfig"),gwr.forEach(t),sSe=r(W5e," (CTRL model)"),W5e.forEach(t),lSe=i(T),Wf=n(T,"LI",{});var Q5e=s(Wf);sW=n(Q5e,"STRONG",{});var hwr=s(sW);iSe=r(hwr,"deberta"),hwr.forEach(t),dSe=r(Q5e," \u2014 "),lL=n(Q5e,"A",{href:!0});var pwr=s(lL);cSe=r(pwr,"DebertaConfig"),pwr.forEach(t),fSe=r(Q5e," (DeBERTa model)"),Q5e.forEach(t),mSe=i(T),Qf=n(T,"LI",{});var H5e=s(Qf);lW=n(H5e,"STRONG",{});var _wr=s(lW);gSe=r(_wr,"deberta-v2"),_wr.forEach(t),hSe=r(H5e," \u2014 "),iL=n(H5e,"A",{href:!0});var uwr=s(iL);pSe=r(uwr,"DebertaV2Config"),uwr.forEach(t),_Se=r(H5e," (DeBERTa-v2 model)"),H5e.forEach(t),uSe=i(T),Hf=n(T,"LI",{});var U5e=s(Hf);iW=n(U5e,"STRONG",{});var bwr=s(iW);bSe=r(bwr,"deit"),bwr.forEach(t),vSe=r(U5e," \u2014 "),dL=n(U5e,"A",{href:!0});var vwr=s(dL);TSe=r(vwr,"DeiTConfig"),vwr.forEach(t),FSe=r(U5e," (DeiT model)"),U5e.forEach(t),CSe=i(T),Uf=n(T,"LI",{});var J5e=s(Uf);dW=n(J5e,"STRONG",{});var Twr=s(dW);MSe=r(Twr,"detr"),Twr.forEach(t),ESe=r(J5e," \u2014 "),cL=n(J5e,"A",{href:!0});var Fwr=s(cL);ySe=r(Fwr,"DetrConfig"),Fwr.forEach(t),wSe=r(J5e," (DETR model)"),J5e.forEach(t),ASe=i(T),Jf=n(T,"LI",{});var Y5e=s(Jf);cW=n(Y5e,"STRONG",{});var Cwr=s(cW);LSe=r(Cwr,"distilbert"),Cwr.forEach(t),BSe=r(Y5e," \u2014 "),fL=n(Y5e,"A",{href:!0});var Mwr=s(fL);kSe=r(Mwr,"DistilBertConfig"),Mwr.forEach(t),xSe=r(Y5e," (DistilBERT model)"),Y5e.forEach(t),RSe=i(T),Yf=n(T,"LI",{});var K5e=s(Yf);fW=n(K5e,"STRONG",{});var Ewr=s(fW);SSe=r(Ewr,"dpr"),Ewr.forEach(t),PSe=r(K5e," \u2014 "),mL=n(K5e,"A",{href:!0});var ywr=s(mL);$Se=r(ywr,"DPRConfig"),ywr.forEach(t),ISe=r(K5e," (DPR model)"),K5e.forEach(t),jSe=i(T),Kf=n(T,"LI",{});var Z5e=s(Kf);mW=n(Z5e,"STRONG",{});var wwr=s(mW);NSe=r(wwr,"electra"),wwr.forEach(t),DSe=r(Z5e," \u2014 "),gL=n(Z5e,"A",{href:!0});var Awr=s(gL);qSe=r(Awr,"ElectraConfig"),Awr.forEach(t),GSe=r(Z5e," (ELECTRA model)"),Z5e.forEach(t),OSe=i(T),Zf=n(T,"LI",{});var eve=s(Zf);gW=n(eve,"STRONG",{});var Lwr=s(gW);XSe=r(Lwr,"encoder-decoder"),Lwr.forEach(t),zSe=r(eve," \u2014 "),hL=n(eve,"A",{href:!0});var Bwr=s(hL);VSe=r(Bwr,"EncoderDecoderConfig"),Bwr.forEach(t),WSe=r(eve," (Encoder decoder model)"),eve.forEach(t),QSe=i(T),em=n(T,"LI",{});var ove=s(em);hW=n(ove,"STRONG",{});var kwr=s(hW);HSe=r(kwr,"flaubert"),kwr.forEach(t),USe=r(ove," \u2014 "),pL=n(ove,"A",{href:!0});var xwr=s(pL);JSe=r(xwr,"FlaubertConfig"),xwr.forEach(t),YSe=r(ove," (FlauBERT model)"),ove.forEach(t),KSe=i(T),om=n(T,"LI",{});var rve=s(om);pW=n(rve,"STRONG",{});var Rwr=s(pW);ZSe=r(Rwr,"fnet"),Rwr.forEach(t),ePe=r(rve," \u2014 "),_L=n(rve,"A",{href:!0});var Swr=s(_L);oPe=r(Swr,"FNetConfig"),Swr.forEach(t),rPe=r(rve," (FNet model)"),rve.forEach(t),tPe=i(T),rm=n(T,"LI",{});var tve=s(rm);_W=n(tve,"STRONG",{});var Pwr=s(_W);aPe=r(Pwr,"fsmt"),Pwr.forEach(t),nPe=r(tve," \u2014 "),uL=n(tve,"A",{href:!0});var $wr=s(uL);sPe=r($wr,"FSMTConfig"),$wr.forEach(t),lPe=r(tve," (FairSeq Machine-Translation model)"),tve.forEach(t),iPe=i(T),tm=n(T,"LI",{});var ave=s(tm);uW=n(ave,"STRONG",{});var Iwr=s(uW);dPe=r(Iwr,"funnel"),Iwr.forEach(t),cPe=r(ave," \u2014 "),bL=n(ave,"A",{href:!0});var jwr=s(bL);fPe=r(jwr,"FunnelConfig"),jwr.forEach(t),mPe=r(ave," (Funnel Transformer model)"),ave.forEach(t),gPe=i(T),am=n(T,"LI",{});var nve=s(am);bW=n(nve,"STRONG",{});var Nwr=s(bW);hPe=r(Nwr,"gpt2"),Nwr.forEach(t),pPe=r(nve," \u2014 "),vL=n(nve,"A",{href:!0});var Dwr=s(vL);_Pe=r(Dwr,"GPT2Config"),Dwr.forEach(t),uPe=r(nve," (OpenAI GPT-2 model)"),nve.forEach(t),bPe=i(T),nm=n(T,"LI",{});var sve=s(nm);vW=n(sve,"STRONG",{});var qwr=s(vW);vPe=r(qwr,"gpt_neo"),qwr.forEach(t),TPe=r(sve," \u2014 "),TL=n(sve,"A",{href:!0});var Gwr=s(TL);FPe=r(Gwr,"GPTNeoConfig"),Gwr.forEach(t),CPe=r(sve," (GPT Neo model)"),sve.forEach(t),MPe=i(T),sm=n(T,"LI",{});var lve=s(sm);TW=n(lve,"STRONG",{});var Owr=s(TW);EPe=r(Owr,"gptj"),Owr.forEach(t),yPe=r(lve," \u2014 "),FL=n(lve,"A",{href:!0});var Xwr=s(FL);wPe=r(Xwr,"GPTJConfig"),Xwr.forEach(t),APe=r(lve," (GPT-J model)"),lve.forEach(t),LPe=i(T),lm=n(T,"LI",{});var ive=s(lm);FW=n(ive,"STRONG",{});var zwr=s(FW);BPe=r(zwr,"hubert"),zwr.forEach(t),kPe=r(ive," \u2014 "),CL=n(ive,"A",{href:!0});var Vwr=s(CL);xPe=r(Vwr,"HubertConfig"),Vwr.forEach(t),RPe=r(ive," (Hubert model)"),ive.forEach(t),SPe=i(T),im=n(T,"LI",{});var dve=s(im);CW=n(dve,"STRONG",{});var Wwr=s(CW);PPe=r(Wwr,"ibert"),Wwr.forEach(t),$Pe=r(dve," \u2014 "),ML=n(dve,"A",{href:!0});var Qwr=s(ML);IPe=r(Qwr,"IBertConfig"),Qwr.forEach(t),jPe=r(dve," (I-BERT model)"),dve.forEach(t),NPe=i(T),dm=n(T,"LI",{});var cve=s(dm);MW=n(cve,"STRONG",{});var Hwr=s(MW);DPe=r(Hwr,"imagegpt"),Hwr.forEach(t),qPe=r(cve," \u2014 "),EL=n(cve,"A",{href:!0});var Uwr=s(EL);GPe=r(Uwr,"ImageGPTConfig"),Uwr.forEach(t),OPe=r(cve," (ImageGPT model)"),cve.forEach(t),XPe=i(T),cm=n(T,"LI",{});var fve=s(cm);EW=n(fve,"STRONG",{});var Jwr=s(EW);zPe=r(Jwr,"layoutlm"),Jwr.forEach(t),VPe=r(fve," \u2014 "),yL=n(fve,"A",{href:!0});var Ywr=s(yL);WPe=r(Ywr,"LayoutLMConfig"),Ywr.forEach(t),QPe=r(fve," (LayoutLM model)"),fve.forEach(t),HPe=i(T),fm=n(T,"LI",{});var mve=s(fm);yW=n(mve,"STRONG",{});var Kwr=s(yW);UPe=r(Kwr,"layoutlmv2"),Kwr.forEach(t),JPe=r(mve," \u2014 "),wL=n(mve,"A",{href:!0});var Zwr=s(wL);YPe=r(Zwr,"LayoutLMv2Config"),Zwr.forEach(t),KPe=r(mve," (LayoutLMv2 model)"),mve.forEach(t),ZPe=i(T),mm=n(T,"LI",{});var gve=s(mm);wW=n(gve,"STRONG",{});var eAr=s(wW);e$e=r(eAr,"led"),eAr.forEach(t),o$e=r(gve," \u2014 "),AL=n(gve,"A",{href:!0});var oAr=s(AL);r$e=r(oAr,"LEDConfig"),oAr.forEach(t),t$e=r(gve," (LED model)"),gve.forEach(t),a$e=i(T),gm=n(T,"LI",{});var hve=s(gm);AW=n(hve,"STRONG",{});var rAr=s(AW);n$e=r(rAr,"longformer"),rAr.forEach(t),s$e=r(hve," \u2014 "),LL=n(hve,"A",{href:!0});var tAr=s(LL);l$e=r(tAr,"LongformerConfig"),tAr.forEach(t),i$e=r(hve," (Longformer model)"),hve.forEach(t),d$e=i(T),hm=n(T,"LI",{});var pve=s(hm);LW=n(pve,"STRONG",{});var aAr=s(LW);c$e=r(aAr,"luke"),aAr.forEach(t),f$e=r(pve," \u2014 "),BL=n(pve,"A",{href:!0});var nAr=s(BL);m$e=r(nAr,"LukeConfig"),nAr.forEach(t),g$e=r(pve," (LUKE model)"),pve.forEach(t),h$e=i(T),pm=n(T,"LI",{});var _ve=s(pm);BW=n(_ve,"STRONG",{});var sAr=s(BW);p$e=r(sAr,"lxmert"),sAr.forEach(t),_$e=r(_ve," \u2014 "),kL=n(_ve,"A",{href:!0});var lAr=s(kL);u$e=r(lAr,"LxmertConfig"),lAr.forEach(t),b$e=r(_ve," (LXMERT model)"),_ve.forEach(t),v$e=i(T),_m=n(T,"LI",{});var uve=s(_m);kW=n(uve,"STRONG",{});var iAr=s(kW);T$e=r(iAr,"m2m_100"),iAr.forEach(t),F$e=r(uve," \u2014 "),xL=n(uve,"A",{href:!0});var dAr=s(xL);C$e=r(dAr,"M2M100Config"),dAr.forEach(t),M$e=r(uve," (M2M100 model)"),uve.forEach(t),E$e=i(T),um=n(T,"LI",{});var bve=s(um);xW=n(bve,"STRONG",{});var cAr=s(xW);y$e=r(cAr,"marian"),cAr.forEach(t),w$e=r(bve," \u2014 "),RL=n(bve,"A",{href:!0});var fAr=s(RL);A$e=r(fAr,"MarianConfig"),fAr.forEach(t),L$e=r(bve," (Marian model)"),bve.forEach(t),B$e=i(T),bm=n(T,"LI",{});var vve=s(bm);RW=n(vve,"STRONG",{});var mAr=s(RW);k$e=r(mAr,"mbart"),mAr.forEach(t),x$e=r(vve," \u2014 "),SL=n(vve,"A",{href:!0});var gAr=s(SL);R$e=r(gAr,"MBartConfig"),gAr.forEach(t),S$e=r(vve," (mBART model)"),vve.forEach(t),P$e=i(T),vm=n(T,"LI",{});var Tve=s(vm);SW=n(Tve,"STRONG",{});var hAr=s(SW);$$e=r(hAr,"megatron-bert"),hAr.forEach(t),I$e=r(Tve," \u2014 "),PL=n(Tve,"A",{href:!0});var pAr=s(PL);j$e=r(pAr,"MegatronBertConfig"),pAr.forEach(t),N$e=r(Tve," (MegatronBert model)"),Tve.forEach(t),D$e=i(T),Tm=n(T,"LI",{});var Fve=s(Tm);PW=n(Fve,"STRONG",{});var _Ar=s(PW);q$e=r(_Ar,"mobilebert"),_Ar.forEach(t),G$e=r(Fve," \u2014 "),$L=n(Fve,"A",{href:!0});var uAr=s($L);O$e=r(uAr,"MobileBertConfig"),uAr.forEach(t),X$e=r(Fve," (MobileBERT model)"),Fve.forEach(t),z$e=i(T),Fm=n(T,"LI",{});var Cve=s(Fm);$W=n(Cve,"STRONG",{});var bAr=s($W);V$e=r(bAr,"mpnet"),bAr.forEach(t),W$e=r(Cve," \u2014 "),IL=n(Cve,"A",{href:!0});var vAr=s(IL);Q$e=r(vAr,"MPNetConfig"),vAr.forEach(t),H$e=r(Cve," (MPNet model)"),Cve.forEach(t),U$e=i(T),Cm=n(T,"LI",{});var Mve=s(Cm);IW=n(Mve,"STRONG",{});var TAr=s(IW);J$e=r(TAr,"mt5"),TAr.forEach(t),Y$e=r(Mve," \u2014 "),jL=n(Mve,"A",{href:!0});var FAr=s(jL);K$e=r(FAr,"MT5Config"),FAr.forEach(t),Z$e=r(Mve," (mT5 model)"),Mve.forEach(t),eIe=i(T),Mm=n(T,"LI",{});var Eve=s(Mm);jW=n(Eve,"STRONG",{});var CAr=s(jW);oIe=r(CAr,"nystromformer"),CAr.forEach(t),rIe=r(Eve," \u2014 "),NL=n(Eve,"A",{href:!0});var MAr=s(NL);tIe=r(MAr,"NystromformerConfig"),MAr.forEach(t),aIe=r(Eve," (Nystromformer model)"),Eve.forEach(t),nIe=i(T),Em=n(T,"LI",{});var yve=s(Em);NW=n(yve,"STRONG",{});var EAr=s(NW);sIe=r(EAr,"openai-gpt"),EAr.forEach(t),lIe=r(yve," \u2014 "),DL=n(yve,"A",{href:!0});var yAr=s(DL);iIe=r(yAr,"OpenAIGPTConfig"),yAr.forEach(t),dIe=r(yve," (OpenAI GPT model)"),yve.forEach(t),cIe=i(T),ym=n(T,"LI",{});var wve=s(ym);DW=n(wve,"STRONG",{});var wAr=s(DW);fIe=r(wAr,"pegasus"),wAr.forEach(t),mIe=r(wve," \u2014 "),qL=n(wve,"A",{href:!0});var AAr=s(qL);gIe=r(AAr,"PegasusConfig"),AAr.forEach(t),hIe=r(wve," (Pegasus model)"),wve.forEach(t),pIe=i(T),wm=n(T,"LI",{});var Ave=s(wm);qW=n(Ave,"STRONG",{});var LAr=s(qW);_Ie=r(LAr,"perceiver"),LAr.forEach(t),uIe=r(Ave," \u2014 "),GL=n(Ave,"A",{href:!0});var BAr=s(GL);bIe=r(BAr,"PerceiverConfig"),BAr.forEach(t),vIe=r(Ave," (Perceiver model)"),Ave.forEach(t),TIe=i(T),Am=n(T,"LI",{});var Lve=s(Am);GW=n(Lve,"STRONG",{});var kAr=s(GW);FIe=r(kAr,"plbart"),kAr.forEach(t),CIe=r(Lve," \u2014 "),OL=n(Lve,"A",{href:!0});var xAr=s(OL);MIe=r(xAr,"PLBartConfig"),xAr.forEach(t),EIe=r(Lve," (PLBart model)"),Lve.forEach(t),yIe=i(T),Lm=n(T,"LI",{});var Bve=s(Lm);OW=n(Bve,"STRONG",{});var RAr=s(OW);wIe=r(RAr,"poolformer"),RAr.forEach(t),AIe=r(Bve," \u2014 "),XL=n(Bve,"A",{href:!0});var SAr=s(XL);LIe=r(SAr,"PoolFormerConfig"),SAr.forEach(t),BIe=r(Bve," (PoolFormer model)"),Bve.forEach(t),kIe=i(T),Bm=n(T,"LI",{});var kve=s(Bm);XW=n(kve,"STRONG",{});var PAr=s(XW);xIe=r(PAr,"prophetnet"),PAr.forEach(t),RIe=r(kve," \u2014 "),zL=n(kve,"A",{href:!0});var $Ar=s(zL);SIe=r($Ar,"ProphetNetConfig"),$Ar.forEach(t),PIe=r(kve," (ProphetNet model)"),kve.forEach(t),$Ie=i(T),km=n(T,"LI",{});var xve=s(km);zW=n(xve,"STRONG",{});var IAr=s(zW);IIe=r(IAr,"qdqbert"),IAr.forEach(t),jIe=r(xve," \u2014 "),VL=n(xve,"A",{href:!0});var jAr=s(VL);NIe=r(jAr,"QDQBertConfig"),jAr.forEach(t),DIe=r(xve," (QDQBert model)"),xve.forEach(t),qIe=i(T),xm=n(T,"LI",{});var Rve=s(xm);VW=n(Rve,"STRONG",{});var NAr=s(VW);GIe=r(NAr,"rag"),NAr.forEach(t),OIe=r(Rve," \u2014 "),WL=n(Rve,"A",{href:!0});var DAr=s(WL);XIe=r(DAr,"RagConfig"),DAr.forEach(t),zIe=r(Rve," (RAG model)"),Rve.forEach(t),VIe=i(T),Rm=n(T,"LI",{});var Sve=s(Rm);WW=n(Sve,"STRONG",{});var qAr=s(WW);WIe=r(qAr,"realm"),qAr.forEach(t),QIe=r(Sve," \u2014 "),QL=n(Sve,"A",{href:!0});var GAr=s(QL);HIe=r(GAr,"RealmConfig"),GAr.forEach(t),UIe=r(Sve," (Realm model)"),Sve.forEach(t),JIe=i(T),Sm=n(T,"LI",{});var Pve=s(Sm);QW=n(Pve,"STRONG",{});var OAr=s(QW);YIe=r(OAr,"reformer"),OAr.forEach(t),KIe=r(Pve," \u2014 "),HL=n(Pve,"A",{href:!0});var XAr=s(HL);ZIe=r(XAr,"ReformerConfig"),XAr.forEach(t),eje=r(Pve," (Reformer model)"),Pve.forEach(t),oje=i(T),Pm=n(T,"LI",{});var $ve=s(Pm);HW=n($ve,"STRONG",{});var zAr=s(HW);rje=r(zAr,"rembert"),zAr.forEach(t),tje=r($ve," \u2014 "),UL=n($ve,"A",{href:!0});var VAr=s(UL);aje=r(VAr,"RemBertConfig"),VAr.forEach(t),nje=r($ve," (RemBERT model)"),$ve.forEach(t),sje=i(T),$m=n(T,"LI",{});var Ive=s($m);UW=n(Ive,"STRONG",{});var WAr=s(UW);lje=r(WAr,"retribert"),WAr.forEach(t),ije=r(Ive," \u2014 "),JL=n(Ive,"A",{href:!0});var QAr=s(JL);dje=r(QAr,"RetriBertConfig"),QAr.forEach(t),cje=r(Ive," (RetriBERT model)"),Ive.forEach(t),fje=i(T),Im=n(T,"LI",{});var jve=s(Im);JW=n(jve,"STRONG",{});var HAr=s(JW);mje=r(HAr,"roberta"),HAr.forEach(t),gje=r(jve," \u2014 "),YL=n(jve,"A",{href:!0});var UAr=s(YL);hje=r(UAr,"RobertaConfig"),UAr.forEach(t),pje=r(jve," (RoBERTa model)"),jve.forEach(t),_je=i(T),jm=n(T,"LI",{});var Nve=s(jm);YW=n(Nve,"STRONG",{});var JAr=s(YW);uje=r(JAr,"roformer"),JAr.forEach(t),bje=r(Nve," \u2014 "),KL=n(Nve,"A",{href:!0});var YAr=s(KL);vje=r(YAr,"RoFormerConfig"),YAr.forEach(t),Tje=r(Nve," (RoFormer model)"),Nve.forEach(t),Fje=i(T),Nm=n(T,"LI",{});var Dve=s(Nm);KW=n(Dve,"STRONG",{});var KAr=s(KW);Cje=r(KAr,"segformer"),KAr.forEach(t),Mje=r(Dve," \u2014 "),ZL=n(Dve,"A",{href:!0});var ZAr=s(ZL);Eje=r(ZAr,"SegformerConfig"),ZAr.forEach(t),yje=r(Dve," (SegFormer model)"),Dve.forEach(t),wje=i(T),Dm=n(T,"LI",{});var qve=s(Dm);ZW=n(qve,"STRONG",{});var e6r=s(ZW);Aje=r(e6r,"sew"),e6r.forEach(t),Lje=r(qve," \u2014 "),e8=n(qve,"A",{href:!0});var o6r=s(e8);Bje=r(o6r,"SEWConfig"),o6r.forEach(t),kje=r(qve," (SEW model)"),qve.forEach(t),xje=i(T),qm=n(T,"LI",{});var Gve=s(qm);eQ=n(Gve,"STRONG",{});var r6r=s(eQ);Rje=r(r6r,"sew-d"),r6r.forEach(t),Sje=r(Gve," \u2014 "),o8=n(Gve,"A",{href:!0});var t6r=s(o8);Pje=r(t6r,"SEWDConfig"),t6r.forEach(t),$je=r(Gve," (SEW-D model)"),Gve.forEach(t),Ije=i(T),Gm=n(T,"LI",{});var Ove=s(Gm);oQ=n(Ove,"STRONG",{});var a6r=s(oQ);jje=r(a6r,"speech-encoder-decoder"),a6r.forEach(t),Nje=r(Ove," \u2014 "),r8=n(Ove,"A",{href:!0});var n6r=s(r8);Dje=r(n6r,"SpeechEncoderDecoderConfig"),n6r.forEach(t),qje=r(Ove," (Speech Encoder decoder model)"),Ove.forEach(t),Gje=i(T),Om=n(T,"LI",{});var Xve=s(Om);rQ=n(Xve,"STRONG",{});var s6r=s(rQ);Oje=r(s6r,"speech_to_text"),s6r.forEach(t),Xje=r(Xve," \u2014 "),t8=n(Xve,"A",{href:!0});var l6r=s(t8);zje=r(l6r,"Speech2TextConfig"),l6r.forEach(t),Vje=r(Xve," (Speech2Text model)"),Xve.forEach(t),Wje=i(T),Xm=n(T,"LI",{});var zve=s(Xm);tQ=n(zve,"STRONG",{});var i6r=s(tQ);Qje=r(i6r,"speech_to_text_2"),i6r.forEach(t),Hje=r(zve," \u2014 "),a8=n(zve,"A",{href:!0});var d6r=s(a8);Uje=r(d6r,"Speech2Text2Config"),d6r.forEach(t),Jje=r(zve," (Speech2Text2 model)"),zve.forEach(t),Yje=i(T),zm=n(T,"LI",{});var Vve=s(zm);aQ=n(Vve,"STRONG",{});var c6r=s(aQ);Kje=r(c6r,"splinter"),c6r.forEach(t),Zje=r(Vve," \u2014 "),n8=n(Vve,"A",{href:!0});var f6r=s(n8);eNe=r(f6r,"SplinterConfig"),f6r.forEach(t),oNe=r(Vve," (Splinter model)"),Vve.forEach(t),rNe=i(T),Vm=n(T,"LI",{});var Wve=s(Vm);nQ=n(Wve,"STRONG",{});var m6r=s(nQ);tNe=r(m6r,"squeezebert"),m6r.forEach(t),aNe=r(Wve," \u2014 "),s8=n(Wve,"A",{href:!0});var g6r=s(s8);nNe=r(g6r,"SqueezeBertConfig"),g6r.forEach(t),sNe=r(Wve," (SqueezeBERT model)"),Wve.forEach(t),lNe=i(T),Wm=n(T,"LI",{});var Qve=s(Wm);sQ=n(Qve,"STRONG",{});var h6r=s(sQ);iNe=r(h6r,"swin"),h6r.forEach(t),dNe=r(Qve," \u2014 "),l8=n(Qve,"A",{href:!0});var p6r=s(l8);cNe=r(p6r,"SwinConfig"),p6r.forEach(t),fNe=r(Qve," (Swin model)"),Qve.forEach(t),mNe=i(T),Qm=n(T,"LI",{});var Hve=s(Qm);lQ=n(Hve,"STRONG",{});var _6r=s(lQ);gNe=r(_6r,"t5"),_6r.forEach(t),hNe=r(Hve," \u2014 "),i8=n(Hve,"A",{href:!0});var u6r=s(i8);pNe=r(u6r,"T5Config"),u6r.forEach(t),_Ne=r(Hve," (T5 model)"),Hve.forEach(t),uNe=i(T),Hm=n(T,"LI",{});var Uve=s(Hm);iQ=n(Uve,"STRONG",{});var b6r=s(iQ);bNe=r(b6r,"tapas"),b6r.forEach(t),vNe=r(Uve," \u2014 "),d8=n(Uve,"A",{href:!0});var v6r=s(d8);TNe=r(v6r,"TapasConfig"),v6r.forEach(t),FNe=r(Uve," (TAPAS model)"),Uve.forEach(t),CNe=i(T),Um=n(T,"LI",{});var Jve=s(Um);dQ=n(Jve,"STRONG",{});var T6r=s(dQ);MNe=r(T6r,"transfo-xl"),T6r.forEach(t),ENe=r(Jve," \u2014 "),c8=n(Jve,"A",{href:!0});var F6r=s(c8);yNe=r(F6r,"TransfoXLConfig"),F6r.forEach(t),wNe=r(Jve," (Transformer-XL model)"),Jve.forEach(t),ANe=i(T),Jm=n(T,"LI",{});var Yve=s(Jm);cQ=n(Yve,"STRONG",{});var C6r=s(cQ);LNe=r(C6r,"trocr"),C6r.forEach(t),BNe=r(Yve," \u2014 "),f8=n(Yve,"A",{href:!0});var M6r=s(f8);kNe=r(M6r,"TrOCRConfig"),M6r.forEach(t),xNe=r(Yve," (TrOCR model)"),Yve.forEach(t),RNe=i(T),Ym=n(T,"LI",{});var Kve=s(Ym);fQ=n(Kve,"STRONG",{});var E6r=s(fQ);SNe=r(E6r,"unispeech"),E6r.forEach(t),PNe=r(Kve," \u2014 "),m8=n(Kve,"A",{href:!0});var y6r=s(m8);$Ne=r(y6r,"UniSpeechConfig"),y6r.forEach(t),INe=r(Kve," (UniSpeech model)"),Kve.forEach(t),jNe=i(T),Km=n(T,"LI",{});var Zve=s(Km);mQ=n(Zve,"STRONG",{});var w6r=s(mQ);NNe=r(w6r,"unispeech-sat"),w6r.forEach(t),DNe=r(Zve," \u2014 "),g8=n(Zve,"A",{href:!0});var A6r=s(g8);qNe=r(A6r,"UniSpeechSatConfig"),A6r.forEach(t),GNe=r(Zve," (UniSpeechSat model)"),Zve.forEach(t),ONe=i(T),Zm=n(T,"LI",{});var eTe=s(Zm);gQ=n(eTe,"STRONG",{});var L6r=s(gQ);XNe=r(L6r,"vilt"),L6r.forEach(t),zNe=r(eTe," \u2014 "),h8=n(eTe,"A",{href:!0});var B6r=s(h8);VNe=r(B6r,"ViltConfig"),B6r.forEach(t),WNe=r(eTe," (ViLT model)"),eTe.forEach(t),QNe=i(T),eg=n(T,"LI",{});var oTe=s(eg);hQ=n(oTe,"STRONG",{});var k6r=s(hQ);HNe=r(k6r,"vision-encoder-decoder"),k6r.forEach(t),UNe=r(oTe," \u2014 "),p8=n(oTe,"A",{href:!0});var x6r=s(p8);JNe=r(x6r,"VisionEncoderDecoderConfig"),x6r.forEach(t),YNe=r(oTe," (Vision Encoder decoder model)"),oTe.forEach(t),KNe=i(T),og=n(T,"LI",{});var rTe=s(og);pQ=n(rTe,"STRONG",{});var R6r=s(pQ);ZNe=r(R6r,"vision-text-dual-encoder"),R6r.forEach(t),eDe=r(rTe," \u2014 "),_8=n(rTe,"A",{href:!0});var S6r=s(_8);oDe=r(S6r,"VisionTextDualEncoderConfig"),S6r.forEach(t),rDe=r(rTe," (VisionTextDualEncoder model)"),rTe.forEach(t),tDe=i(T),rg=n(T,"LI",{});var tTe=s(rg);_Q=n(tTe,"STRONG",{});var P6r=s(_Q);aDe=r(P6r,"visual_bert"),P6r.forEach(t),nDe=r(tTe," \u2014 "),u8=n(tTe,"A",{href:!0});var $6r=s(u8);sDe=r($6r,"VisualBertConfig"),$6r.forEach(t),lDe=r(tTe," (VisualBert model)"),tTe.forEach(t),iDe=i(T),tg=n(T,"LI",{});var aTe=s(tg);uQ=n(aTe,"STRONG",{});var I6r=s(uQ);dDe=r(I6r,"vit"),I6r.forEach(t),cDe=r(aTe," \u2014 "),b8=n(aTe,"A",{href:!0});var j6r=s(b8);fDe=r(j6r,"ViTConfig"),j6r.forEach(t),mDe=r(aTe," (ViT model)"),aTe.forEach(t),gDe=i(T),ag=n(T,"LI",{});var nTe=s(ag);bQ=n(nTe,"STRONG",{});var N6r=s(bQ);hDe=r(N6r,"vit_mae"),N6r.forEach(t),pDe=r(nTe," \u2014 "),v8=n(nTe,"A",{href:!0});var D6r=s(v8);_De=r(D6r,"ViTMAEConfig"),D6r.forEach(t),uDe=r(nTe," (ViTMAE model)"),nTe.forEach(t),bDe=i(T),ng=n(T,"LI",{});var sTe=s(ng);vQ=n(sTe,"STRONG",{});var q6r=s(vQ);vDe=r(q6r,"wav2vec2"),q6r.forEach(t),TDe=r(sTe," \u2014 "),T8=n(sTe,"A",{href:!0});var G6r=s(T8);FDe=r(G6r,"Wav2Vec2Config"),G6r.forEach(t),CDe=r(sTe," (Wav2Vec2 model)"),sTe.forEach(t),MDe=i(T),sg=n(T,"LI",{});var lTe=s(sg);TQ=n(lTe,"STRONG",{});var O6r=s(TQ);EDe=r(O6r,"wavlm"),O6r.forEach(t),yDe=r(lTe," \u2014 "),F8=n(lTe,"A",{href:!0});var X6r=s(F8);wDe=r(X6r,"WavLMConfig"),X6r.forEach(t),ADe=r(lTe," (WavLM model)"),lTe.forEach(t),LDe=i(T),lg=n(T,"LI",{});var iTe=s(lg);FQ=n(iTe,"STRONG",{});var z6r=s(FQ);BDe=r(z6r,"xglm"),z6r.forEach(t),kDe=r(iTe," \u2014 "),C8=n(iTe,"A",{href:!0});var V6r=s(C8);xDe=r(V6r,"XGLMConfig"),V6r.forEach(t),RDe=r(iTe," (XGLM model)"),iTe.forEach(t),SDe=i(T),ig=n(T,"LI",{});var dTe=s(ig);CQ=n(dTe,"STRONG",{});var W6r=s(CQ);PDe=r(W6r,"xlm"),W6r.forEach(t),$De=r(dTe," \u2014 "),M8=n(dTe,"A",{href:!0});var Q6r=s(M8);IDe=r(Q6r,"XLMConfig"),Q6r.forEach(t),jDe=r(dTe," (XLM model)"),dTe.forEach(t),NDe=i(T),dg=n(T,"LI",{});var cTe=s(dg);MQ=n(cTe,"STRONG",{});var H6r=s(MQ);DDe=r(H6r,"xlm-prophetnet"),H6r.forEach(t),qDe=r(cTe," \u2014 "),E8=n(cTe,"A",{href:!0});var U6r=s(E8);GDe=r(U6r,"XLMProphetNetConfig"),U6r.forEach(t),ODe=r(cTe," (XLMProphetNet model)"),cTe.forEach(t),XDe=i(T),cg=n(T,"LI",{});var fTe=s(cg);EQ=n(fTe,"STRONG",{});var J6r=s(EQ);zDe=r(J6r,"xlm-roberta"),J6r.forEach(t),VDe=r(fTe," \u2014 "),y8=n(fTe,"A",{href:!0});var Y6r=s(y8);WDe=r(Y6r,"XLMRobertaConfig"),Y6r.forEach(t),QDe=r(fTe," (XLM-RoBERTa model)"),fTe.forEach(t),HDe=i(T),fg=n(T,"LI",{});var mTe=s(fg);yQ=n(mTe,"STRONG",{});var K6r=s(yQ);UDe=r(K6r,"xlm-roberta-xl"),K6r.forEach(t),JDe=r(mTe," \u2014 "),w8=n(mTe,"A",{href:!0});var Z6r=s(w8);YDe=r(Z6r,"XLMRobertaXLConfig"),Z6r.forEach(t),KDe=r(mTe," (XLM-RoBERTa-XL model)"),mTe.forEach(t),ZDe=i(T),mg=n(T,"LI",{});var gTe=s(mg);wQ=n(gTe,"STRONG",{});var e0r=s(wQ);eqe=r(e0r,"xlnet"),e0r.forEach(t),oqe=r(gTe," \u2014 "),A8=n(gTe,"A",{href:!0});var o0r=s(A8);rqe=r(o0r,"XLNetConfig"),o0r.forEach(t),tqe=r(gTe," (XLNet model)"),gTe.forEach(t),aqe=i(T),gg=n(T,"LI",{});var hTe=s(gg);AQ=n(hTe,"STRONG",{});var r0r=s(AQ);nqe=r(r0r,"yoso"),r0r.forEach(t),sqe=r(hTe," \u2014 "),L8=n(hTe,"A",{href:!0});var t0r=s(L8);lqe=r(t0r,"YosoConfig"),t0r.forEach(t),iqe=r(hTe," (YOSO model)"),hTe.forEach(t),T.forEach(t),dqe=i(ia),LQ=n(ia,"P",{});var a0r=s(LQ);cqe=r(a0r,"Examples:"),a0r.forEach(t),fqe=i(ia),m(mM.$$.fragment,ia),ia.forEach(t),mqe=i(Ps),hg=n(Ps,"DIV",{class:!0});var cBe=s(hg);m(gM.$$.fragment,cBe),gqe=i(cBe),BQ=n(cBe,"P",{});var n0r=s(BQ);hqe=r(n0r,"Register a new configuration for this class."),n0r.forEach(t),cBe.forEach(t),Ps.forEach(t),cLe=i(d),Ii=n(d,"H2",{class:!0});var fBe=s(Ii);pg=n(fBe,"A",{id:!0,class:!0,href:!0});var s0r=s(pg);kQ=n(s0r,"SPAN",{});var l0r=s(kQ);m(hM.$$.fragment,l0r),l0r.forEach(t),s0r.forEach(t),pqe=i(fBe),xQ=n(fBe,"SPAN",{});var i0r=s(xQ);_qe=r(i0r,"AutoTokenizer"),i0r.forEach(t),fBe.forEach(t),fLe=i(d),Oo=n(d,"DIV",{class:!0});var $s=s(Oo);m(pM.$$.fragment,$s),uqe=i($s),_M=n($s,"P",{});var mBe=s(_M);bqe=r(mBe,`This is a generic tokenizer class that will be instantiated as one of the tokenizer classes of the library when
created with the `),B8=n(mBe,"A",{href:!0});var d0r=s(B8);vqe=r(d0r,"AutoTokenizer.from_pretrained()"),d0r.forEach(t),Tqe=r(mBe," class method."),mBe.forEach(t),Fqe=i($s),uM=n($s,"P",{});var gBe=s(uM);Cqe=r(gBe,"This class cannot be instantiated directly using "),RQ=n(gBe,"CODE",{});var c0r=s(RQ);Mqe=r(c0r,"__init__()"),c0r.forEach(t),Eqe=r(gBe," (throws an error)."),gBe.forEach(t),yqe=i($s),mo=n($s,"DIV",{class:!0});var da=s(mo);m(bM.$$.fragment,da),wqe=i(da),SQ=n(da,"P",{});var f0r=s(SQ);Aqe=r(f0r,"Instantiate one of the tokenizer classes of the library from a pretrained model vocabulary."),f0r.forEach(t),Lqe=i(da),ja=n(da,"P",{});var n4=s(ja);Bqe=r(n4,"The tokenizer class to instantiate is selected based on the "),PQ=n(n4,"CODE",{});var m0r=s(PQ);kqe=r(m0r,"model_type"),m0r.forEach(t),xqe=r(n4,` property of the config object (either
passed as an argument or loaded from `),$Q=n(n4,"CODE",{});var g0r=s($Q);Rqe=r(g0r,"pretrained_model_name_or_path"),g0r.forEach(t),Sqe=r(n4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),IQ=n(n4,"CODE",{});var h0r=s(IQ);Pqe=r(h0r,"pretrained_model_name_or_path"),h0r.forEach(t),$qe=r(n4,":"),n4.forEach(t),Iqe=i(da),M=n(da,"UL",{});var y=s(M);Dn=n(y,"LI",{});var z6=s(Dn);jQ=n(z6,"STRONG",{});var p0r=s(jQ);jqe=r(p0r,"albert"),p0r.forEach(t),Nqe=r(z6," \u2014 "),k8=n(z6,"A",{href:!0});var _0r=s(k8);Dqe=r(_0r,"AlbertTokenizer"),_0r.forEach(t),qqe=r(z6," or "),x8=n(z6,"A",{href:!0});var u0r=s(x8);Gqe=r(u0r,"AlbertTokenizerFast"),u0r.forEach(t),Oqe=r(z6," (ALBERT model)"),z6.forEach(t),Xqe=i(y),qn=n(y,"LI",{});var V6=s(qn);NQ=n(V6,"STRONG",{});var b0r=s(NQ);zqe=r(b0r,"bart"),b0r.forEach(t),Vqe=r(V6," \u2014 "),R8=n(V6,"A",{href:!0});var v0r=s(R8);Wqe=r(v0r,"BartTokenizer"),v0r.forEach(t),Qqe=r(V6," or "),S8=n(V6,"A",{href:!0});var T0r=s(S8);Hqe=r(T0r,"BartTokenizerFast"),T0r.forEach(t),Uqe=r(V6," (BART model)"),V6.forEach(t),Jqe=i(y),Gn=n(y,"LI",{});var W6=s(Gn);DQ=n(W6,"STRONG",{});var F0r=s(DQ);Yqe=r(F0r,"barthez"),F0r.forEach(t),Kqe=r(W6," \u2014 "),P8=n(W6,"A",{href:!0});var C0r=s(P8);Zqe=r(C0r,"BarthezTokenizer"),C0r.forEach(t),eGe=r(W6," or "),$8=n(W6,"A",{href:!0});var M0r=s($8);oGe=r(M0r,"BarthezTokenizerFast"),M0r.forEach(t),rGe=r(W6," (BARThez model)"),W6.forEach(t),tGe=i(y),_g=n(y,"LI",{});var pTe=s(_g);qQ=n(pTe,"STRONG",{});var E0r=s(qQ);aGe=r(E0r,"bartpho"),E0r.forEach(t),nGe=r(pTe," \u2014 "),I8=n(pTe,"A",{href:!0});var y0r=s(I8);sGe=r(y0r,"BartphoTokenizer"),y0r.forEach(t),lGe=r(pTe," (BARTpho model)"),pTe.forEach(t),iGe=i(y),On=n(y,"LI",{});var Q6=s(On);GQ=n(Q6,"STRONG",{});var w0r=s(GQ);dGe=r(w0r,"bert"),w0r.forEach(t),cGe=r(Q6," \u2014 "),j8=n(Q6,"A",{href:!0});var A0r=s(j8);fGe=r(A0r,"BertTokenizer"),A0r.forEach(t),mGe=r(Q6," or "),N8=n(Q6,"A",{href:!0});var L0r=s(N8);gGe=r(L0r,"BertTokenizerFast"),L0r.forEach(t),hGe=r(Q6," (BERT model)"),Q6.forEach(t),pGe=i(y),ug=n(y,"LI",{});var _Te=s(ug);OQ=n(_Te,"STRONG",{});var B0r=s(OQ);_Ge=r(B0r,"bert-generation"),B0r.forEach(t),uGe=r(_Te," \u2014 "),D8=n(_Te,"A",{href:!0});var k0r=s(D8);bGe=r(k0r,"BertGenerationTokenizer"),k0r.forEach(t),vGe=r(_Te," (Bert Generation model)"),_Te.forEach(t),TGe=i(y),bg=n(y,"LI",{});var uTe=s(bg);XQ=n(uTe,"STRONG",{});var x0r=s(XQ);FGe=r(x0r,"bert-japanese"),x0r.forEach(t),CGe=r(uTe," \u2014 "),q8=n(uTe,"A",{href:!0});var R0r=s(q8);MGe=r(R0r,"BertJapaneseTokenizer"),R0r.forEach(t),EGe=r(uTe," (BertJapanese model)"),uTe.forEach(t),yGe=i(y),vg=n(y,"LI",{});var bTe=s(vg);zQ=n(bTe,"STRONG",{});var S0r=s(zQ);wGe=r(S0r,"bertweet"),S0r.forEach(t),AGe=r(bTe," \u2014 "),G8=n(bTe,"A",{href:!0});var P0r=s(G8);LGe=r(P0r,"BertweetTokenizer"),P0r.forEach(t),BGe=r(bTe," (Bertweet model)"),bTe.forEach(t),kGe=i(y),Xn=n(y,"LI",{});var H6=s(Xn);VQ=n(H6,"STRONG",{});var $0r=s(VQ);xGe=r($0r,"big_bird"),$0r.forEach(t),RGe=r(H6," \u2014 "),O8=n(H6,"A",{href:!0});var I0r=s(O8);SGe=r(I0r,"BigBirdTokenizer"),I0r.forEach(t),PGe=r(H6," or "),X8=n(H6,"A",{href:!0});var j0r=s(X8);$Ge=r(j0r,"BigBirdTokenizerFast"),j0r.forEach(t),IGe=r(H6," (BigBird model)"),H6.forEach(t),jGe=i(y),zn=n(y,"LI",{});var U6=s(zn);WQ=n(U6,"STRONG",{});var N0r=s(WQ);NGe=r(N0r,"bigbird_pegasus"),N0r.forEach(t),DGe=r(U6," \u2014 "),z8=n(U6,"A",{href:!0});var D0r=s(z8);qGe=r(D0r,"PegasusTokenizer"),D0r.forEach(t),GGe=r(U6," or "),V8=n(U6,"A",{href:!0});var q0r=s(V8);OGe=r(q0r,"PegasusTokenizerFast"),q0r.forEach(t),XGe=r(U6," (BigBirdPegasus model)"),U6.forEach(t),zGe=i(y),Vn=n(y,"LI",{});var J6=s(Vn);QQ=n(J6,"STRONG",{});var G0r=s(QQ);VGe=r(G0r,"blenderbot"),G0r.forEach(t),WGe=r(J6," \u2014 "),W8=n(J6,"A",{href:!0});var O0r=s(W8);QGe=r(O0r,"BlenderbotTokenizer"),O0r.forEach(t),HGe=r(J6," or "),Q8=n(J6,"A",{href:!0});var X0r=s(Q8);UGe=r(X0r,"BlenderbotTokenizerFast"),X0r.forEach(t),JGe=r(J6," (Blenderbot model)"),J6.forEach(t),YGe=i(y),Tg=n(y,"LI",{});var vTe=s(Tg);HQ=n(vTe,"STRONG",{});var z0r=s(HQ);KGe=r(z0r,"blenderbot-small"),z0r.forEach(t),ZGe=r(vTe," \u2014 "),H8=n(vTe,"A",{href:!0});var V0r=s(H8);eOe=r(V0r,"BlenderbotSmallTokenizer"),V0r.forEach(t),oOe=r(vTe," (BlenderbotSmall model)"),vTe.forEach(t),rOe=i(y),Fg=n(y,"LI",{});var TTe=s(Fg);UQ=n(TTe,"STRONG",{});var W0r=s(UQ);tOe=r(W0r,"byt5"),W0r.forEach(t),aOe=r(TTe," \u2014 "),U8=n(TTe,"A",{href:!0});var Q0r=s(U8);nOe=r(Q0r,"ByT5Tokenizer"),Q0r.forEach(t),sOe=r(TTe," (ByT5 model)"),TTe.forEach(t),lOe=i(y),Wn=n(y,"LI",{});var Y6=s(Wn);JQ=n(Y6,"STRONG",{});var H0r=s(JQ);iOe=r(H0r,"camembert"),H0r.forEach(t),dOe=r(Y6," \u2014 "),J8=n(Y6,"A",{href:!0});var U0r=s(J8);cOe=r(U0r,"CamembertTokenizer"),U0r.forEach(t),fOe=r(Y6," or "),Y8=n(Y6,"A",{href:!0});var J0r=s(Y8);mOe=r(J0r,"CamembertTokenizerFast"),J0r.forEach(t),gOe=r(Y6," (CamemBERT model)"),Y6.forEach(t),hOe=i(y),Cg=n(y,"LI",{});var FTe=s(Cg);YQ=n(FTe,"STRONG",{});var Y0r=s(YQ);pOe=r(Y0r,"canine"),Y0r.forEach(t),_Oe=r(FTe," \u2014 "),K8=n(FTe,"A",{href:!0});var K0r=s(K8);uOe=r(K0r,"CanineTokenizer"),K0r.forEach(t),bOe=r(FTe," (Canine model)"),FTe.forEach(t),vOe=i(y),Qn=n(y,"LI",{});var K6=s(Qn);KQ=n(K6,"STRONG",{});var Z0r=s(KQ);TOe=r(Z0r,"clip"),Z0r.forEach(t),FOe=r(K6," \u2014 "),Z8=n(K6,"A",{href:!0});var eLr=s(Z8);COe=r(eLr,"CLIPTokenizer"),eLr.forEach(t),MOe=r(K6," or "),eB=n(K6,"A",{href:!0});var oLr=s(eB);EOe=r(oLr,"CLIPTokenizerFast"),oLr.forEach(t),yOe=r(K6," (CLIP model)"),K6.forEach(t),wOe=i(y),Hn=n(y,"LI",{});var Z6=s(Hn);ZQ=n(Z6,"STRONG",{});var rLr=s(ZQ);AOe=r(rLr,"convbert"),rLr.forEach(t),LOe=r(Z6," \u2014 "),oB=n(Z6,"A",{href:!0});var tLr=s(oB);BOe=r(tLr,"ConvBertTokenizer"),tLr.forEach(t),kOe=r(Z6," or "),rB=n(Z6,"A",{href:!0});var aLr=s(rB);xOe=r(aLr,"ConvBertTokenizerFast"),aLr.forEach(t),ROe=r(Z6," (ConvBERT model)"),Z6.forEach(t),SOe=i(y),Un=n(y,"LI",{});var e0=s(Un);eH=n(e0,"STRONG",{});var nLr=s(eH);POe=r(nLr,"cpm"),nLr.forEach(t),$Oe=r(e0," \u2014 "),tB=n(e0,"A",{href:!0});var sLr=s(tB);IOe=r(sLr,"CpmTokenizer"),sLr.forEach(t),jOe=r(e0," or "),oH=n(e0,"CODE",{});var lLr=s(oH);NOe=r(lLr,"CpmTokenizerFast"),lLr.forEach(t),DOe=r(e0," (CPM model)"),e0.forEach(t),qOe=i(y),Mg=n(y,"LI",{});var CTe=s(Mg);rH=n(CTe,"STRONG",{});var iLr=s(rH);GOe=r(iLr,"ctrl"),iLr.forEach(t),OOe=r(CTe," \u2014 "),aB=n(CTe,"A",{href:!0});var dLr=s(aB);XOe=r(dLr,"CTRLTokenizer"),dLr.forEach(t),zOe=r(CTe," (CTRL model)"),CTe.forEach(t),VOe=i(y),Jn=n(y,"LI",{});var o0=s(Jn);tH=n(o0,"STRONG",{});var cLr=s(tH);WOe=r(cLr,"deberta"),cLr.forEach(t),QOe=r(o0," \u2014 "),nB=n(o0,"A",{href:!0});var fLr=s(nB);HOe=r(fLr,"DebertaTokenizer"),fLr.forEach(t),UOe=r(o0," or "),sB=n(o0,"A",{href:!0});var mLr=s(sB);JOe=r(mLr,"DebertaTokenizerFast"),mLr.forEach(t),YOe=r(o0," (DeBERTa model)"),o0.forEach(t),KOe=i(y),Eg=n(y,"LI",{});var MTe=s(Eg);aH=n(MTe,"STRONG",{});var gLr=s(aH);ZOe=r(gLr,"deberta-v2"),gLr.forEach(t),eXe=r(MTe," \u2014 "),lB=n(MTe,"A",{href:!0});var hLr=s(lB);oXe=r(hLr,"DebertaV2Tokenizer"),hLr.forEach(t),rXe=r(MTe," (DeBERTa-v2 model)"),MTe.forEach(t),tXe=i(y),Yn=n(y,"LI",{});var r0=s(Yn);nH=n(r0,"STRONG",{});var pLr=s(nH);aXe=r(pLr,"distilbert"),pLr.forEach(t),nXe=r(r0," \u2014 "),iB=n(r0,"A",{href:!0});var _Lr=s(iB);sXe=r(_Lr,"DistilBertTokenizer"),_Lr.forEach(t),lXe=r(r0," or "),dB=n(r0,"A",{href:!0});var uLr=s(dB);iXe=r(uLr,"DistilBertTokenizerFast"),uLr.forEach(t),dXe=r(r0," (DistilBERT model)"),r0.forEach(t),cXe=i(y),Kn=n(y,"LI",{});var t0=s(Kn);sH=n(t0,"STRONG",{});var bLr=s(sH);fXe=r(bLr,"dpr"),bLr.forEach(t),mXe=r(t0," \u2014 "),cB=n(t0,"A",{href:!0});var vLr=s(cB);gXe=r(vLr,"DPRQuestionEncoderTokenizer"),vLr.forEach(t),hXe=r(t0," or "),fB=n(t0,"A",{href:!0});var TLr=s(fB);pXe=r(TLr,"DPRQuestionEncoderTokenizerFast"),TLr.forEach(t),_Xe=r(t0," (DPR model)"),t0.forEach(t),uXe=i(y),Zn=n(y,"LI",{});var a0=s(Zn);lH=n(a0,"STRONG",{});var FLr=s(lH);bXe=r(FLr,"electra"),FLr.forEach(t),vXe=r(a0," \u2014 "),mB=n(a0,"A",{href:!0});var CLr=s(mB);TXe=r(CLr,"ElectraTokenizer"),CLr.forEach(t),FXe=r(a0," or "),gB=n(a0,"A",{href:!0});var MLr=s(gB);CXe=r(MLr,"ElectraTokenizerFast"),MLr.forEach(t),MXe=r(a0," (ELECTRA model)"),a0.forEach(t),EXe=i(y),yg=n(y,"LI",{});var ETe=s(yg);iH=n(ETe,"STRONG",{});var ELr=s(iH);yXe=r(ELr,"flaubert"),ELr.forEach(t),wXe=r(ETe," \u2014 "),hB=n(ETe,"A",{href:!0});var yLr=s(hB);AXe=r(yLr,"FlaubertTokenizer"),yLr.forEach(t),LXe=r(ETe," (FlauBERT model)"),ETe.forEach(t),BXe=i(y),es=n(y,"LI",{});var n0=s(es);dH=n(n0,"STRONG",{});var wLr=s(dH);kXe=r(wLr,"fnet"),wLr.forEach(t),xXe=r(n0," \u2014 "),pB=n(n0,"A",{href:!0});var ALr=s(pB);RXe=r(ALr,"FNetTokenizer"),ALr.forEach(t),SXe=r(n0," or "),_B=n(n0,"A",{href:!0});var LLr=s(_B);PXe=r(LLr,"FNetTokenizerFast"),LLr.forEach(t),$Xe=r(n0," (FNet model)"),n0.forEach(t),IXe=i(y),wg=n(y,"LI",{});var yTe=s(wg);cH=n(yTe,"STRONG",{});var BLr=s(cH);jXe=r(BLr,"fsmt"),BLr.forEach(t),NXe=r(yTe," \u2014 "),uB=n(yTe,"A",{href:!0});var kLr=s(uB);DXe=r(kLr,"FSMTTokenizer"),kLr.forEach(t),qXe=r(yTe," (FairSeq Machine-Translation model)"),yTe.forEach(t),GXe=i(y),os=n(y,"LI",{});var s0=s(os);fH=n(s0,"STRONG",{});var xLr=s(fH);OXe=r(xLr,"funnel"),xLr.forEach(t),XXe=r(s0," \u2014 "),bB=n(s0,"A",{href:!0});var RLr=s(bB);zXe=r(RLr,"FunnelTokenizer"),RLr.forEach(t),VXe=r(s0," or "),vB=n(s0,"A",{href:!0});var SLr=s(vB);WXe=r(SLr,"FunnelTokenizerFast"),SLr.forEach(t),QXe=r(s0," (Funnel Transformer model)"),s0.forEach(t),HXe=i(y),rs=n(y,"LI",{});var l0=s(rs);mH=n(l0,"STRONG",{});var PLr=s(mH);UXe=r(PLr,"gpt2"),PLr.forEach(t),JXe=r(l0," \u2014 "),TB=n(l0,"A",{href:!0});var $Lr=s(TB);YXe=r($Lr,"GPT2Tokenizer"),$Lr.forEach(t),KXe=r(l0," or "),FB=n(l0,"A",{href:!0});var ILr=s(FB);ZXe=r(ILr,"GPT2TokenizerFast"),ILr.forEach(t),eze=r(l0," (OpenAI GPT-2 model)"),l0.forEach(t),oze=i(y),ts=n(y,"LI",{});var i0=s(ts);gH=n(i0,"STRONG",{});var jLr=s(gH);rze=r(jLr,"gpt_neo"),jLr.forEach(t),tze=r(i0," \u2014 "),CB=n(i0,"A",{href:!0});var NLr=s(CB);aze=r(NLr,"GPT2Tokenizer"),NLr.forEach(t),nze=r(i0," or "),MB=n(i0,"A",{href:!0});var DLr=s(MB);sze=r(DLr,"GPT2TokenizerFast"),DLr.forEach(t),lze=r(i0," (GPT Neo model)"),i0.forEach(t),ize=i(y),as=n(y,"LI",{});var d0=s(as);hH=n(d0,"STRONG",{});var qLr=s(hH);dze=r(qLr,"herbert"),qLr.forEach(t),cze=r(d0," \u2014 "),EB=n(d0,"A",{href:!0});var GLr=s(EB);fze=r(GLr,"HerbertTokenizer"),GLr.forEach(t),mze=r(d0," or "),yB=n(d0,"A",{href:!0});var OLr=s(yB);gze=r(OLr,"HerbertTokenizerFast"),OLr.forEach(t),hze=r(d0," (HerBERT model)"),d0.forEach(t),pze=i(y),Ag=n(y,"LI",{});var wTe=s(Ag);pH=n(wTe,"STRONG",{});var XLr=s(pH);_ze=r(XLr,"hubert"),XLr.forEach(t),uze=r(wTe," \u2014 "),wB=n(wTe,"A",{href:!0});var zLr=s(wB);bze=r(zLr,"Wav2Vec2CTCTokenizer"),zLr.forEach(t),vze=r(wTe," (Hubert model)"),wTe.forEach(t),Tze=i(y),ns=n(y,"LI",{});var c0=s(ns);_H=n(c0,"STRONG",{});var VLr=s(_H);Fze=r(VLr,"ibert"),VLr.forEach(t),Cze=r(c0," \u2014 "),AB=n(c0,"A",{href:!0});var WLr=s(AB);Mze=r(WLr,"RobertaTokenizer"),WLr.forEach(t),Eze=r(c0," or "),LB=n(c0,"A",{href:!0});var QLr=s(LB);yze=r(QLr,"RobertaTokenizerFast"),QLr.forEach(t),wze=r(c0," (I-BERT model)"),c0.forEach(t),Aze=i(y),ss=n(y,"LI",{});var f0=s(ss);uH=n(f0,"STRONG",{});var HLr=s(uH);Lze=r(HLr,"layoutlm"),HLr.forEach(t),Bze=r(f0," \u2014 "),BB=n(f0,"A",{href:!0});var ULr=s(BB);kze=r(ULr,"LayoutLMTokenizer"),ULr.forEach(t),xze=r(f0," or "),kB=n(f0,"A",{href:!0});var JLr=s(kB);Rze=r(JLr,"LayoutLMTokenizerFast"),JLr.forEach(t),Sze=r(f0," (LayoutLM model)"),f0.forEach(t),Pze=i(y),ls=n(y,"LI",{});var m0=s(ls);bH=n(m0,"STRONG",{});var YLr=s(bH);$ze=r(YLr,"layoutlmv2"),YLr.forEach(t),Ize=r(m0," \u2014 "),xB=n(m0,"A",{href:!0});var KLr=s(xB);jze=r(KLr,"LayoutLMv2Tokenizer"),KLr.forEach(t),Nze=r(m0," or "),RB=n(m0,"A",{href:!0});var ZLr=s(RB);Dze=r(ZLr,"LayoutLMv2TokenizerFast"),ZLr.forEach(t),qze=r(m0," (LayoutLMv2 model)"),m0.forEach(t),Gze=i(y),is=n(y,"LI",{});var g0=s(is);vH=n(g0,"STRONG",{});var e8r=s(vH);Oze=r(e8r,"layoutxlm"),e8r.forEach(t),Xze=r(g0," \u2014 "),SB=n(g0,"A",{href:!0});var o8r=s(SB);zze=r(o8r,"LayoutXLMTokenizer"),o8r.forEach(t),Vze=r(g0," or "),PB=n(g0,"A",{href:!0});var r8r=s(PB);Wze=r(r8r,"LayoutXLMTokenizerFast"),r8r.forEach(t),Qze=r(g0," (LayoutXLM model)"),g0.forEach(t),Hze=i(y),ds=n(y,"LI",{});var h0=s(ds);TH=n(h0,"STRONG",{});var t8r=s(TH);Uze=r(t8r,"led"),t8r.forEach(t),Jze=r(h0," \u2014 "),$B=n(h0,"A",{href:!0});var a8r=s($B);Yze=r(a8r,"LEDTokenizer"),a8r.forEach(t),Kze=r(h0," or "),IB=n(h0,"A",{href:!0});var n8r=s(IB);Zze=r(n8r,"LEDTokenizerFast"),n8r.forEach(t),eVe=r(h0," (LED model)"),h0.forEach(t),oVe=i(y),cs=n(y,"LI",{});var p0=s(cs);FH=n(p0,"STRONG",{});var s8r=s(FH);rVe=r(s8r,"longformer"),s8r.forEach(t),tVe=r(p0," \u2014 "),jB=n(p0,"A",{href:!0});var l8r=s(jB);aVe=r(l8r,"LongformerTokenizer"),l8r.forEach(t),nVe=r(p0," or "),NB=n(p0,"A",{href:!0});var i8r=s(NB);sVe=r(i8r,"LongformerTokenizerFast"),i8r.forEach(t),lVe=r(p0," (Longformer model)"),p0.forEach(t),iVe=i(y),Lg=n(y,"LI",{});var ATe=s(Lg);CH=n(ATe,"STRONG",{});var d8r=s(CH);dVe=r(d8r,"luke"),d8r.forEach(t),cVe=r(ATe," \u2014 "),DB=n(ATe,"A",{href:!0});var c8r=s(DB);fVe=r(c8r,"LukeTokenizer"),c8r.forEach(t),mVe=r(ATe," (LUKE model)"),ATe.forEach(t),gVe=i(y),fs=n(y,"LI",{});var _0=s(fs);MH=n(_0,"STRONG",{});var f8r=s(MH);hVe=r(f8r,"lxmert"),f8r.forEach(t),pVe=r(_0," \u2014 "),qB=n(_0,"A",{href:!0});var m8r=s(qB);_Ve=r(m8r,"LxmertTokenizer"),m8r.forEach(t),uVe=r(_0," or "),GB=n(_0,"A",{href:!0});var g8r=s(GB);bVe=r(g8r,"LxmertTokenizerFast"),g8r.forEach(t),vVe=r(_0," (LXMERT model)"),_0.forEach(t),TVe=i(y),Bg=n(y,"LI",{});var LTe=s(Bg);EH=n(LTe,"STRONG",{});var h8r=s(EH);FVe=r(h8r,"m2m_100"),h8r.forEach(t),CVe=r(LTe," \u2014 "),OB=n(LTe,"A",{href:!0});var p8r=s(OB);MVe=r(p8r,"M2M100Tokenizer"),p8r.forEach(t),EVe=r(LTe," (M2M100 model)"),LTe.forEach(t),yVe=i(y),kg=n(y,"LI",{});var BTe=s(kg);yH=n(BTe,"STRONG",{});var _8r=s(yH);wVe=r(_8r,"marian"),_8r.forEach(t),AVe=r(BTe," \u2014 "),XB=n(BTe,"A",{href:!0});var u8r=s(XB);LVe=r(u8r,"MarianTokenizer"),u8r.forEach(t),BVe=r(BTe," (Marian model)"),BTe.forEach(t),kVe=i(y),ms=n(y,"LI",{});var u0=s(ms);wH=n(u0,"STRONG",{});var b8r=s(wH);xVe=r(b8r,"mbart"),b8r.forEach(t),RVe=r(u0," \u2014 "),zB=n(u0,"A",{href:!0});var v8r=s(zB);SVe=r(v8r,"MBartTokenizer"),v8r.forEach(t),PVe=r(u0," or "),VB=n(u0,"A",{href:!0});var T8r=s(VB);$Ve=r(T8r,"MBartTokenizerFast"),T8r.forEach(t),IVe=r(u0," (mBART model)"),u0.forEach(t),jVe=i(y),gs=n(y,"LI",{});var b0=s(gs);AH=n(b0,"STRONG",{});var F8r=s(AH);NVe=r(F8r,"mbart50"),F8r.forEach(t),DVe=r(b0," \u2014 "),WB=n(b0,"A",{href:!0});var C8r=s(WB);qVe=r(C8r,"MBart50Tokenizer"),C8r.forEach(t),GVe=r(b0," or "),QB=n(b0,"A",{href:!0});var M8r=s(QB);OVe=r(M8r,"MBart50TokenizerFast"),M8r.forEach(t),XVe=r(b0," (mBART-50 model)"),b0.forEach(t),zVe=i(y),xg=n(y,"LI",{});var kTe=s(xg);LH=n(kTe,"STRONG",{});var E8r=s(LH);VVe=r(E8r,"mluke"),E8r.forEach(t),WVe=r(kTe," \u2014 "),HB=n(kTe,"A",{href:!0});var y8r=s(HB);QVe=r(y8r,"MLukeTokenizer"),y8r.forEach(t),HVe=r(kTe," (mLUKE model)"),kTe.forEach(t),UVe=i(y),hs=n(y,"LI",{});var v0=s(hs);BH=n(v0,"STRONG",{});var w8r=s(BH);JVe=r(w8r,"mobilebert"),w8r.forEach(t),YVe=r(v0," \u2014 "),UB=n(v0,"A",{href:!0});var A8r=s(UB);KVe=r(A8r,"MobileBertTokenizer"),A8r.forEach(t),ZVe=r(v0," or "),JB=n(v0,"A",{href:!0});var L8r=s(JB);eWe=r(L8r,"MobileBertTokenizerFast"),L8r.forEach(t),oWe=r(v0," (MobileBERT model)"),v0.forEach(t),rWe=i(y),ps=n(y,"LI",{});var T0=s(ps);kH=n(T0,"STRONG",{});var B8r=s(kH);tWe=r(B8r,"mpnet"),B8r.forEach(t),aWe=r(T0," \u2014 "),YB=n(T0,"A",{href:!0});var k8r=s(YB);nWe=r(k8r,"MPNetTokenizer"),k8r.forEach(t),sWe=r(T0," or "),KB=n(T0,"A",{href:!0});var x8r=s(KB);lWe=r(x8r,"MPNetTokenizerFast"),x8r.forEach(t),iWe=r(T0," (MPNet model)"),T0.forEach(t),dWe=i(y),_s=n(y,"LI",{});var F0=s(_s);xH=n(F0,"STRONG",{});var R8r=s(xH);cWe=r(R8r,"mt5"),R8r.forEach(t),fWe=r(F0," \u2014 "),ZB=n(F0,"A",{href:!0});var S8r=s(ZB);mWe=r(S8r,"MT5Tokenizer"),S8r.forEach(t),gWe=r(F0," or "),ek=n(F0,"A",{href:!0});var P8r=s(ek);hWe=r(P8r,"MT5TokenizerFast"),P8r.forEach(t),pWe=r(F0," (mT5 model)"),F0.forEach(t),_We=i(y),us=n(y,"LI",{});var C0=s(us);RH=n(C0,"STRONG",{});var $8r=s(RH);uWe=r($8r,"openai-gpt"),$8r.forEach(t),bWe=r(C0," \u2014 "),ok=n(C0,"A",{href:!0});var I8r=s(ok);vWe=r(I8r,"OpenAIGPTTokenizer"),I8r.forEach(t),TWe=r(C0," or "),rk=n(C0,"A",{href:!0});var j8r=s(rk);FWe=r(j8r,"OpenAIGPTTokenizerFast"),j8r.forEach(t),CWe=r(C0," (OpenAI GPT model)"),C0.forEach(t),MWe=i(y),bs=n(y,"LI",{});var M0=s(bs);SH=n(M0,"STRONG",{});var N8r=s(SH);EWe=r(N8r,"pegasus"),N8r.forEach(t),yWe=r(M0," \u2014 "),tk=n(M0,"A",{href:!0});var D8r=s(tk);wWe=r(D8r,"PegasusTokenizer"),D8r.forEach(t),AWe=r(M0," or "),ak=n(M0,"A",{href:!0});var q8r=s(ak);LWe=r(q8r,"PegasusTokenizerFast"),q8r.forEach(t),BWe=r(M0," (Pegasus model)"),M0.forEach(t),kWe=i(y),Rg=n(y,"LI",{});var xTe=s(Rg);PH=n(xTe,"STRONG",{});var G8r=s(PH);xWe=r(G8r,"perceiver"),G8r.forEach(t),RWe=r(xTe," \u2014 "),nk=n(xTe,"A",{href:!0});var O8r=s(nk);SWe=r(O8r,"PerceiverTokenizer"),O8r.forEach(t),PWe=r(xTe," (Perceiver model)"),xTe.forEach(t),$We=i(y),Sg=n(y,"LI",{});var RTe=s(Sg);$H=n(RTe,"STRONG",{});var X8r=s($H);IWe=r(X8r,"phobert"),X8r.forEach(t),jWe=r(RTe," \u2014 "),sk=n(RTe,"A",{href:!0});var z8r=s(sk);NWe=r(z8r,"PhobertTokenizer"),z8r.forEach(t),DWe=r(RTe," (PhoBERT model)"),RTe.forEach(t),qWe=i(y),Pg=n(y,"LI",{});var STe=s(Pg);IH=n(STe,"STRONG",{});var V8r=s(IH);GWe=r(V8r,"plbart"),V8r.forEach(t),OWe=r(STe," \u2014 "),lk=n(STe,"A",{href:!0});var W8r=s(lk);XWe=r(W8r,"PLBartTokenizer"),W8r.forEach(t),zWe=r(STe," (PLBart model)"),STe.forEach(t),VWe=i(y),$g=n(y,"LI",{});var PTe=s($g);jH=n(PTe,"STRONG",{});var Q8r=s(jH);WWe=r(Q8r,"prophetnet"),Q8r.forEach(t),QWe=r(PTe," \u2014 "),ik=n(PTe,"A",{href:!0});var H8r=s(ik);HWe=r(H8r,"ProphetNetTokenizer"),H8r.forEach(t),UWe=r(PTe," (ProphetNet model)"),PTe.forEach(t),JWe=i(y),vs=n(y,"LI",{});var E0=s(vs);NH=n(E0,"STRONG",{});var U8r=s(NH);YWe=r(U8r,"qdqbert"),U8r.forEach(t),KWe=r(E0," \u2014 "),dk=n(E0,"A",{href:!0});var J8r=s(dk);ZWe=r(J8r,"BertTokenizer"),J8r.forEach(t),eQe=r(E0," or "),ck=n(E0,"A",{href:!0});var Y8r=s(ck);oQe=r(Y8r,"BertTokenizerFast"),Y8r.forEach(t),rQe=r(E0," (QDQBert model)"),E0.forEach(t),tQe=i(y),Ig=n(y,"LI",{});var $Te=s(Ig);DH=n($Te,"STRONG",{});var K8r=s(DH);aQe=r(K8r,"rag"),K8r.forEach(t),nQe=r($Te," \u2014 "),fk=n($Te,"A",{href:!0});var Z8r=s(fk);sQe=r(Z8r,"RagTokenizer"),Z8r.forEach(t),lQe=r($Te," (RAG model)"),$Te.forEach(t),iQe=i(y),Ts=n(y,"LI",{});var y0=s(Ts);qH=n(y0,"STRONG",{});var eBr=s(qH);dQe=r(eBr,"reformer"),eBr.forEach(t),cQe=r(y0," \u2014 "),mk=n(y0,"A",{href:!0});var oBr=s(mk);fQe=r(oBr,"ReformerTokenizer"),oBr.forEach(t),mQe=r(y0," or "),gk=n(y0,"A",{href:!0});var rBr=s(gk);gQe=r(rBr,"ReformerTokenizerFast"),rBr.forEach(t),hQe=r(y0," (Reformer model)"),y0.forEach(t),pQe=i(y),Fs=n(y,"LI",{});var w0=s(Fs);GH=n(w0,"STRONG",{});var tBr=s(GH);_Qe=r(tBr,"rembert"),tBr.forEach(t),uQe=r(w0," \u2014 "),hk=n(w0,"A",{href:!0});var aBr=s(hk);bQe=r(aBr,"RemBertTokenizer"),aBr.forEach(t),vQe=r(w0," or "),pk=n(w0,"A",{href:!0});var nBr=s(pk);TQe=r(nBr,"RemBertTokenizerFast"),nBr.forEach(t),FQe=r(w0," (RemBERT model)"),w0.forEach(t),CQe=i(y),Cs=n(y,"LI",{});var A0=s(Cs);OH=n(A0,"STRONG",{});var sBr=s(OH);MQe=r(sBr,"retribert"),sBr.forEach(t),EQe=r(A0," \u2014 "),_k=n(A0,"A",{href:!0});var lBr=s(_k);yQe=r(lBr,"RetriBertTokenizer"),lBr.forEach(t),wQe=r(A0," or "),uk=n(A0,"A",{href:!0});var iBr=s(uk);AQe=r(iBr,"RetriBertTokenizerFast"),iBr.forEach(t),LQe=r(A0," (RetriBERT model)"),A0.forEach(t),BQe=i(y),Ms=n(y,"LI",{});var L0=s(Ms);XH=n(L0,"STRONG",{});var dBr=s(XH);kQe=r(dBr,"roberta"),dBr.forEach(t),xQe=r(L0," \u2014 "),bk=n(L0,"A",{href:!0});var cBr=s(bk);RQe=r(cBr,"RobertaTokenizer"),cBr.forEach(t),SQe=r(L0," or "),vk=n(L0,"A",{href:!0});var fBr=s(vk);PQe=r(fBr,"RobertaTokenizerFast"),fBr.forEach(t),$Qe=r(L0," (RoBERTa model)"),L0.forEach(t),IQe=i(y),Es=n(y,"LI",{});var B0=s(Es);zH=n(B0,"STRONG",{});var mBr=s(zH);jQe=r(mBr,"roformer"),mBr.forEach(t),NQe=r(B0," \u2014 "),Tk=n(B0,"A",{href:!0});var gBr=s(Tk);DQe=r(gBr,"RoFormerTokenizer"),gBr.forEach(t),qQe=r(B0," or "),Fk=n(B0,"A",{href:!0});var hBr=s(Fk);GQe=r(hBr,"RoFormerTokenizerFast"),hBr.forEach(t),OQe=r(B0," (RoFormer model)"),B0.forEach(t),XQe=i(y),jg=n(y,"LI",{});var ITe=s(jg);VH=n(ITe,"STRONG",{});var pBr=s(VH);zQe=r(pBr,"speech_to_text"),pBr.forEach(t),VQe=r(ITe," \u2014 "),Ck=n(ITe,"A",{href:!0});var _Br=s(Ck);WQe=r(_Br,"Speech2TextTokenizer"),_Br.forEach(t),QQe=r(ITe," (Speech2Text model)"),ITe.forEach(t),HQe=i(y),Ng=n(y,"LI",{});var jTe=s(Ng);WH=n(jTe,"STRONG",{});var uBr=s(WH);UQe=r(uBr,"speech_to_text_2"),uBr.forEach(t),JQe=r(jTe," \u2014 "),Mk=n(jTe,"A",{href:!0});var bBr=s(Mk);YQe=r(bBr,"Speech2Text2Tokenizer"),bBr.forEach(t),KQe=r(jTe," (Speech2Text2 model)"),jTe.forEach(t),ZQe=i(y),ys=n(y,"LI",{});var k0=s(ys);QH=n(k0,"STRONG",{});var vBr=s(QH);eHe=r(vBr,"splinter"),vBr.forEach(t),oHe=r(k0," \u2014 "),Ek=n(k0,"A",{href:!0});var TBr=s(Ek);rHe=r(TBr,"SplinterTokenizer"),TBr.forEach(t),tHe=r(k0," or "),yk=n(k0,"A",{href:!0});var FBr=s(yk);aHe=r(FBr,"SplinterTokenizerFast"),FBr.forEach(t),nHe=r(k0," (Splinter model)"),k0.forEach(t),sHe=i(y),ws=n(y,"LI",{});var x0=s(ws);HH=n(x0,"STRONG",{});var CBr=s(HH);lHe=r(CBr,"squeezebert"),CBr.forEach(t),iHe=r(x0," \u2014 "),wk=n(x0,"A",{href:!0});var MBr=s(wk);dHe=r(MBr,"SqueezeBertTokenizer"),MBr.forEach(t),cHe=r(x0," or "),Ak=n(x0,"A",{href:!0});var EBr=s(Ak);fHe=r(EBr,"SqueezeBertTokenizerFast"),EBr.forEach(t),mHe=r(x0," (SqueezeBERT model)"),x0.forEach(t),gHe=i(y),As=n(y,"LI",{});var R0=s(As);UH=n(R0,"STRONG",{});var yBr=s(UH);hHe=r(yBr,"t5"),yBr.forEach(t),pHe=r(R0," \u2014 "),Lk=n(R0,"A",{href:!0});var wBr=s(Lk);_He=r(wBr,"T5Tokenizer"),wBr.forEach(t),uHe=r(R0," or "),Bk=n(R0,"A",{href:!0});var ABr=s(Bk);bHe=r(ABr,"T5TokenizerFast"),ABr.forEach(t),vHe=r(R0," (T5 model)"),R0.forEach(t),THe=i(y),Dg=n(y,"LI",{});var NTe=s(Dg);JH=n(NTe,"STRONG",{});var LBr=s(JH);FHe=r(LBr,"tapas"),LBr.forEach(t),CHe=r(NTe," \u2014 "),kk=n(NTe,"A",{href:!0});var BBr=s(kk);MHe=r(BBr,"TapasTokenizer"),BBr.forEach(t),EHe=r(NTe," (TAPAS model)"),NTe.forEach(t),yHe=i(y),qg=n(y,"LI",{});var DTe=s(qg);YH=n(DTe,"STRONG",{});var kBr=s(YH);wHe=r(kBr,"transfo-xl"),kBr.forEach(t),AHe=r(DTe," \u2014 "),xk=n(DTe,"A",{href:!0});var xBr=s(xk);LHe=r(xBr,"TransfoXLTokenizer"),xBr.forEach(t),BHe=r(DTe," (Transformer-XL model)"),DTe.forEach(t),kHe=i(y),Gg=n(y,"LI",{});var qTe=s(Gg);KH=n(qTe,"STRONG",{});var RBr=s(KH);xHe=r(RBr,"wav2vec2"),RBr.forEach(t),RHe=r(qTe," \u2014 "),Rk=n(qTe,"A",{href:!0});var SBr=s(Rk);SHe=r(SBr,"Wav2Vec2CTCTokenizer"),SBr.forEach(t),PHe=r(qTe," (Wav2Vec2 model)"),qTe.forEach(t),$He=i(y),Og=n(y,"LI",{});var GTe=s(Og);ZH=n(GTe,"STRONG",{});var PBr=s(ZH);IHe=r(PBr,"wav2vec2_phoneme"),PBr.forEach(t),jHe=r(GTe," \u2014 "),Sk=n(GTe,"A",{href:!0});var $Br=s(Sk);NHe=r($Br,"Wav2Vec2PhonemeCTCTokenizer"),$Br.forEach(t),DHe=r(GTe," (Wav2Vec2Phoneme model)"),GTe.forEach(t),qHe=i(y),Ls=n(y,"LI",{});var S0=s(Ls);eU=n(S0,"STRONG",{});var IBr=s(eU);GHe=r(IBr,"xglm"),IBr.forEach(t),OHe=r(S0," \u2014 "),Pk=n(S0,"A",{href:!0});var jBr=s(Pk);XHe=r(jBr,"XGLMTokenizer"),jBr.forEach(t),zHe=r(S0," or "),$k=n(S0,"A",{href:!0});var NBr=s($k);VHe=r(NBr,"XGLMTokenizerFast"),NBr.forEach(t),WHe=r(S0," (XGLM model)"),S0.forEach(t),QHe=i(y),Xg=n(y,"LI",{});var OTe=s(Xg);oU=n(OTe,"STRONG",{});var DBr=s(oU);HHe=r(DBr,"xlm"),DBr.forEach(t),UHe=r(OTe," \u2014 "),Ik=n(OTe,"A",{href:!0});var qBr=s(Ik);JHe=r(qBr,"XLMTokenizer"),qBr.forEach(t),YHe=r(OTe," (XLM model)"),OTe.forEach(t),KHe=i(y),zg=n(y,"LI",{});var XTe=s(zg);rU=n(XTe,"STRONG",{});var GBr=s(rU);ZHe=r(GBr,"xlm-prophetnet"),GBr.forEach(t),eUe=r(XTe," \u2014 "),jk=n(XTe,"A",{href:!0});var OBr=s(jk);oUe=r(OBr,"XLMProphetNetTokenizer"),OBr.forEach(t),rUe=r(XTe," (XLMProphetNet model)"),XTe.forEach(t),tUe=i(y),Bs=n(y,"LI",{});var P0=s(Bs);tU=n(P0,"STRONG",{});var XBr=s(tU);aUe=r(XBr,"xlm-roberta"),XBr.forEach(t),nUe=r(P0," \u2014 "),Nk=n(P0,"A",{href:!0});var zBr=s(Nk);sUe=r(zBr,"XLMRobertaTokenizer"),zBr.forEach(t),lUe=r(P0," or "),Dk=n(P0,"A",{href:!0});var VBr=s(Dk);iUe=r(VBr,"XLMRobertaTokenizerFast"),VBr.forEach(t),dUe=r(P0," (XLM-RoBERTa model)"),P0.forEach(t),cUe=i(y),ks=n(y,"LI",{});var $0=s(ks);aU=n($0,"STRONG",{});var WBr=s(aU);fUe=r(WBr,"xlnet"),WBr.forEach(t),mUe=r($0," \u2014 "),qk=n($0,"A",{href:!0});var QBr=s(qk);gUe=r(QBr,"XLNetTokenizer"),QBr.forEach(t),hUe=r($0," or "),Gk=n($0,"A",{href:!0});var HBr=s(Gk);pUe=r(HBr,"XLNetTokenizerFast"),HBr.forEach(t),_Ue=r($0," (XLNet model)"),$0.forEach(t),y.forEach(t),uUe=i(da),nU=n(da,"P",{});var UBr=s(nU);bUe=r(UBr,"Examples:"),UBr.forEach(t),vUe=i(da),m(vM.$$.fragment,da),da.forEach(t),TUe=i($s),Vg=n($s,"DIV",{class:!0});var hBe=s(Vg);m(TM.$$.fragment,hBe),FUe=i(hBe),sU=n(hBe,"P",{});var JBr=s(sU);CUe=r(JBr,"Register a new tokenizer in this mapping."),JBr.forEach(t),hBe.forEach(t),$s.forEach(t),mLe=i(d),ji=n(d,"H2",{class:!0});var pBe=s(ji);Wg=n(pBe,"A",{id:!0,class:!0,href:!0});var YBr=s(Wg);lU=n(YBr,"SPAN",{});var KBr=s(lU);m(FM.$$.fragment,KBr),KBr.forEach(t),YBr.forEach(t),MUe=i(pBe),iU=n(pBe,"SPAN",{});var ZBr=s(iU);EUe=r(ZBr,"AutoFeatureExtractor"),ZBr.forEach(t),pBe.forEach(t),gLe=i(d),Xo=n(d,"DIV",{class:!0});var Is=s(Xo);m(CM.$$.fragment,Is),yUe=i(Is),MM=n(Is,"P",{});var _Be=s(MM);wUe=r(_Be,`This is a generic feature extractor class that will be instantiated as one of the feature extractor classes of the
library when created with the `),Ok=n(_Be,"A",{href:!0});var ekr=s(Ok);AUe=r(ekr,"AutoFeatureExtractor.from_pretrained()"),ekr.forEach(t),LUe=r(_Be," class method."),_Be.forEach(t),BUe=i(Is),EM=n(Is,"P",{});var uBe=s(EM);kUe=r(uBe,"This class cannot be instantiated directly using "),dU=n(uBe,"CODE",{});var okr=s(dU);xUe=r(okr,"__init__()"),okr.forEach(t),RUe=r(uBe," (throws an error)."),uBe.forEach(t),SUe=i(Is),Le=n(Is,"DIV",{class:!0});var xt=s(Le);m(yM.$$.fragment,xt),PUe=i(xt),cU=n(xt,"P",{});var rkr=s(cU);$Ue=r(rkr,"Instantiate one of the feature extractor classes of the library from a pretrained model vocabulary."),rkr.forEach(t),IUe=i(xt),Na=n(xt,"P",{});var s4=s(Na);jUe=r(s4,"The feature extractor class to instantiate is selected based on the "),fU=n(s4,"CODE",{});var tkr=s(fU);NUe=r(tkr,"model_type"),tkr.forEach(t),DUe=r(s4,` property of the config object
(either passed as an argument or loaded from `),mU=n(s4,"CODE",{});var akr=s(mU);qUe=r(akr,"pretrained_model_name_or_path"),akr.forEach(t),GUe=r(s4,` if possible), or when it\u2019s
missing, by falling back to using pattern matching on `),gU=n(s4,"CODE",{});var nkr=s(gU);OUe=r(nkr,"pretrained_model_name_or_path"),nkr.forEach(t),XUe=r(s4,":"),s4.forEach(t),zUe=i(xt),se=n(xt,"UL",{});var de=s(se);Qg=n(de,"LI",{});var zTe=s(Qg);hU=n(zTe,"STRONG",{});var skr=s(hU);VUe=r(skr,"beit"),skr.forEach(t),WUe=r(zTe," \u2014 "),Xk=n(zTe,"A",{href:!0});var lkr=s(Xk);QUe=r(lkr,"BeitFeatureExtractor"),lkr.forEach(t),HUe=r(zTe," (BEiT model)"),zTe.forEach(t),UUe=i(de),Hg=n(de,"LI",{});var VTe=s(Hg);pU=n(VTe,"STRONG",{});var ikr=s(pU);JUe=r(ikr,"clip"),ikr.forEach(t),YUe=r(VTe," \u2014 "),zk=n(VTe,"A",{href:!0});var dkr=s(zk);KUe=r(dkr,"CLIPFeatureExtractor"),dkr.forEach(t),ZUe=r(VTe," (CLIP model)"),VTe.forEach(t),eJe=i(de),Ug=n(de,"LI",{});var WTe=s(Ug);_U=n(WTe,"STRONG",{});var ckr=s(_U);oJe=r(ckr,"convnext"),ckr.forEach(t),rJe=r(WTe," \u2014 "),Vk=n(WTe,"A",{href:!0});var fkr=s(Vk);tJe=r(fkr,"ConvNextFeatureExtractor"),fkr.forEach(t),aJe=r(WTe," (ConvNext model)"),WTe.forEach(t),nJe=i(de),Jg=n(de,"LI",{});var QTe=s(Jg);uU=n(QTe,"STRONG",{});var mkr=s(uU);sJe=r(mkr,"deit"),mkr.forEach(t),lJe=r(QTe," \u2014 "),Wk=n(QTe,"A",{href:!0});var gkr=s(Wk);iJe=r(gkr,"DeiTFeatureExtractor"),gkr.forEach(t),dJe=r(QTe," (DeiT model)"),QTe.forEach(t),cJe=i(de),Yg=n(de,"LI",{});var HTe=s(Yg);bU=n(HTe,"STRONG",{});var hkr=s(bU);fJe=r(hkr,"detr"),hkr.forEach(t),mJe=r(HTe," \u2014 "),Qk=n(HTe,"A",{href:!0});var pkr=s(Qk);gJe=r(pkr,"DetrFeatureExtractor"),pkr.forEach(t),hJe=r(HTe," (DETR model)"),HTe.forEach(t),pJe=i(de),Kg=n(de,"LI",{});var UTe=s(Kg);vU=n(UTe,"STRONG",{});var _kr=s(vU);_Je=r(_kr,"hubert"),_kr.forEach(t),uJe=r(UTe," \u2014 "),Hk=n(UTe,"A",{href:!0});var ukr=s(Hk);bJe=r(ukr,"Wav2Vec2FeatureExtractor"),ukr.forEach(t),vJe=r(UTe," (Hubert model)"),UTe.forEach(t),TJe=i(de),Zg=n(de,"LI",{});var JTe=s(Zg);TU=n(JTe,"STRONG",{});var bkr=s(TU);FJe=r(bkr,"layoutlmv2"),bkr.forEach(t),CJe=r(JTe," \u2014 "),Uk=n(JTe,"A",{href:!0});var vkr=s(Uk);MJe=r(vkr,"LayoutLMv2FeatureExtractor"),vkr.forEach(t),EJe=r(JTe," (LayoutLMv2 model)"),JTe.forEach(t),yJe=i(de),eh=n(de,"LI",{});var YTe=s(eh);FU=n(YTe,"STRONG",{});var Tkr=s(FU);wJe=r(Tkr,"perceiver"),Tkr.forEach(t),AJe=r(YTe," \u2014 "),Jk=n(YTe,"A",{href:!0});var Fkr=s(Jk);LJe=r(Fkr,"PerceiverFeatureExtractor"),Fkr.forEach(t),BJe=r(YTe," (Perceiver model)"),YTe.forEach(t),kJe=i(de),oh=n(de,"LI",{});var KTe=s(oh);CU=n(KTe,"STRONG",{});var Ckr=s(CU);xJe=r(Ckr,"poolformer"),Ckr.forEach(t),RJe=r(KTe," \u2014 "),Yk=n(KTe,"A",{href:!0});var Mkr=s(Yk);SJe=r(Mkr,"PoolFormerFeatureExtractor"),Mkr.forEach(t),PJe=r(KTe," (PoolFormer model)"),KTe.forEach(t),$Je=i(de),rh=n(de,"LI",{});var ZTe=s(rh);MU=n(ZTe,"STRONG",{});var Ekr=s(MU);IJe=r(Ekr,"segformer"),Ekr.forEach(t),jJe=r(ZTe," \u2014 "),Kk=n(ZTe,"A",{href:!0});var ykr=s(Kk);NJe=r(ykr,"SegformerFeatureExtractor"),ykr.forEach(t),DJe=r(ZTe," (SegFormer model)"),ZTe.forEach(t),qJe=i(de),th=n(de,"LI",{});var e7e=s(th);EU=n(e7e,"STRONG",{});var wkr=s(EU);GJe=r(wkr,"speech_to_text"),wkr.forEach(t),OJe=r(e7e," \u2014 "),Zk=n(e7e,"A",{href:!0});var Akr=s(Zk);XJe=r(Akr,"Speech2TextFeatureExtractor"),Akr.forEach(t),zJe=r(e7e," (Speech2Text model)"),e7e.forEach(t),VJe=i(de),ah=n(de,"LI",{});var o7e=s(ah);yU=n(o7e,"STRONG",{});var Lkr=s(yU);WJe=r(Lkr,"swin"),Lkr.forEach(t),QJe=r(o7e," \u2014 "),ex=n(o7e,"A",{href:!0});var Bkr=s(ex);HJe=r(Bkr,"ViTFeatureExtractor"),Bkr.forEach(t),UJe=r(o7e," (Swin model)"),o7e.forEach(t),JJe=i(de),nh=n(de,"LI",{});var r7e=s(nh);wU=n(r7e,"STRONG",{});var kkr=s(wU);YJe=r(kkr,"vit"),kkr.forEach(t),KJe=r(r7e," \u2014 "),ox=n(r7e,"A",{href:!0});var xkr=s(ox);ZJe=r(xkr,"ViTFeatureExtractor"),xkr.forEach(t),eYe=r(r7e," (ViT model)"),r7e.forEach(t),oYe=i(de),sh=n(de,"LI",{});var t7e=s(sh);AU=n(t7e,"STRONG",{});var Rkr=s(AU);rYe=r(Rkr,"vit_mae"),Rkr.forEach(t),tYe=r(t7e," \u2014 "),rx=n(t7e,"A",{href:!0});var Skr=s(rx);aYe=r(Skr,"ViTFeatureExtractor"),Skr.forEach(t),nYe=r(t7e," (ViTMAE model)"),t7e.forEach(t),sYe=i(de),lh=n(de,"LI",{});var a7e=s(lh);LU=n(a7e,"STRONG",{});var Pkr=s(LU);lYe=r(Pkr,"wav2vec2"),Pkr.forEach(t),iYe=r(a7e," \u2014 "),tx=n(a7e,"A",{href:!0});var $kr=s(tx);dYe=r($kr,"Wav2Vec2FeatureExtractor"),$kr.forEach(t),cYe=r(a7e," (Wav2Vec2 model)"),a7e.forEach(t),de.forEach(t),fYe=i(xt),m(ih.$$.fragment,xt),mYe=i(xt),BU=n(xt,"P",{});var Ikr=s(BU);gYe=r(Ikr,"Examples:"),Ikr.forEach(t),hYe=i(xt),m(wM.$$.fragment,xt),xt.forEach(t),pYe=i(Is),dh=n(Is,"DIV",{class:!0});var bBe=s(dh);m(AM.$$.fragment,bBe),_Ye=i(bBe),kU=n(bBe,"P",{});var jkr=s(kU);uYe=r(jkr,"Register a new feature extractor for this class."),jkr.forEach(t),bBe.forEach(t),Is.forEach(t),hLe=i(d),Ni=n(d,"H2",{class:!0});var vBe=s(Ni);ch=n(vBe,"A",{id:!0,class:!0,href:!0});var Nkr=s(ch);xU=n(Nkr,"SPAN",{});var Dkr=s(xU);m(LM.$$.fragment,Dkr),Dkr.forEach(t),Nkr.forEach(t),bYe=i(vBe),RU=n(vBe,"SPAN",{});var qkr=s(RU);vYe=r(qkr,"AutoProcessor"),qkr.forEach(t),vBe.forEach(t),pLe=i(d),zo=n(d,"DIV",{class:!0});var js=s(zo);m(BM.$$.fragment,js),TYe=i(js),kM=n(js,"P",{});var TBe=s(kM);FYe=r(TBe,`This is a generic processor class that will be instantiated as one of the processor classes of the library when
created with the `),ax=n(TBe,"A",{href:!0});var Gkr=s(ax);CYe=r(Gkr,"AutoProcessor.from_pretrained()"),Gkr.forEach(t),MYe=r(TBe," class method."),TBe.forEach(t),EYe=i(js),xM=n(js,"P",{});var FBe=s(xM);yYe=r(FBe,"This class cannot be instantiated directly using "),SU=n(FBe,"CODE",{});var Okr=s(SU);wYe=r(Okr,"__init__()"),Okr.forEach(t),AYe=r(FBe," (throws an error)."),FBe.forEach(t),LYe=i(js),Be=n(js,"DIV",{class:!0});var Rt=s(Be);m(RM.$$.fragment,Rt),BYe=i(Rt),PU=n(Rt,"P",{});var Xkr=s(PU);kYe=r(Xkr,"Instantiate one of the processor classes of the library from a pretrained model vocabulary."),Xkr.forEach(t),xYe=i(Rt),Di=n(Rt,"P",{});var VX=s(Di);RYe=r(VX,"The processor class to instantiate is selected based on the "),$U=n(VX,"CODE",{});var zkr=s($U);SYe=r(zkr,"model_type"),zkr.forEach(t),PYe=r(VX,` property of the config object (either
passed as an argument or loaded from `),IU=n(VX,"CODE",{});var Vkr=s(IU);$Ye=r(Vkr,"pretrained_model_name_or_path"),Vkr.forEach(t),IYe=r(VX," if possible):"),VX.forEach(t),jYe=i(Rt),we=n(Rt,"UL",{});var No=s(we);fh=n(No,"LI",{});var n7e=s(fh);jU=n(n7e,"STRONG",{});var Wkr=s(jU);NYe=r(Wkr,"clip"),Wkr.forEach(t),DYe=r(n7e," \u2014 "),nx=n(n7e,"A",{href:!0});var Qkr=s(nx);qYe=r(Qkr,"CLIPProcessor"),Qkr.forEach(t),GYe=r(n7e," (CLIP model)"),n7e.forEach(t),OYe=i(No),mh=n(No,"LI",{});var s7e=s(mh);NU=n(s7e,"STRONG",{});var Hkr=s(NU);XYe=r(Hkr,"layoutlmv2"),Hkr.forEach(t),zYe=r(s7e," \u2014 "),sx=n(s7e,"A",{href:!0});var Ukr=s(sx);VYe=r(Ukr,"LayoutLMv2Processor"),Ukr.forEach(t),WYe=r(s7e," (LayoutLMv2 model)"),s7e.forEach(t),QYe=i(No),gh=n(No,"LI",{});var l7e=s(gh);DU=n(l7e,"STRONG",{});var Jkr=s(DU);HYe=r(Jkr,"layoutxlm"),Jkr.forEach(t),UYe=r(l7e," \u2014 "),lx=n(l7e,"A",{href:!0});var Ykr=s(lx);JYe=r(Ykr,"LayoutXLMProcessor"),Ykr.forEach(t),YYe=r(l7e," (LayoutXLM model)"),l7e.forEach(t),KYe=i(No),hh=n(No,"LI",{});var i7e=s(hh);qU=n(i7e,"STRONG",{});var Kkr=s(qU);ZYe=r(Kkr,"speech_to_text"),Kkr.forEach(t),eKe=r(i7e," \u2014 "),ix=n(i7e,"A",{href:!0});var Zkr=s(ix);oKe=r(Zkr,"Speech2TextProcessor"),Zkr.forEach(t),rKe=r(i7e," (Speech2Text model)"),i7e.forEach(t),tKe=i(No),ph=n(No,"LI",{});var d7e=s(ph);GU=n(d7e,"STRONG",{});var exr=s(GU);aKe=r(exr,"speech_to_text_2"),exr.forEach(t),nKe=r(d7e," \u2014 "),dx=n(d7e,"A",{href:!0});var oxr=s(dx);sKe=r(oxr,"Speech2Text2Processor"),oxr.forEach(t),lKe=r(d7e," (Speech2Text2 model)"),d7e.forEach(t),iKe=i(No),_h=n(No,"LI",{});var c7e=s(_h);OU=n(c7e,"STRONG",{});var rxr=s(OU);dKe=r(rxr,"trocr"),rxr.forEach(t),cKe=r(c7e," \u2014 "),cx=n(c7e,"A",{href:!0});var txr=s(cx);fKe=r(txr,"TrOCRProcessor"),txr.forEach(t),mKe=r(c7e," (TrOCR model)"),c7e.forEach(t),gKe=i(No),uh=n(No,"LI",{});var f7e=s(uh);XU=n(f7e,"STRONG",{});var axr=s(XU);hKe=r(axr,"vision-text-dual-encoder"),axr.forEach(t),pKe=r(f7e," \u2014 "),fx=n(f7e,"A",{href:!0});var nxr=s(fx);_Ke=r(nxr,"VisionTextDualEncoderProcessor"),nxr.forEach(t),uKe=r(f7e," (VisionTextDualEncoder model)"),f7e.forEach(t),bKe=i(No),bh=n(No,"LI",{});var m7e=s(bh);zU=n(m7e,"STRONG",{});var sxr=s(zU);vKe=r(sxr,"wav2vec2"),sxr.forEach(t),TKe=r(m7e," \u2014 "),mx=n(m7e,"A",{href:!0});var lxr=s(mx);FKe=r(lxr,"Wav2Vec2Processor"),lxr.forEach(t),CKe=r(m7e," (Wav2Vec2 model)"),m7e.forEach(t),No.forEach(t),MKe=i(Rt),m(vh.$$.fragment,Rt),EKe=i(Rt),VU=n(Rt,"P",{});var ixr=s(VU);yKe=r(ixr,"Examples:"),ixr.forEach(t),wKe=i(Rt),m(SM.$$.fragment,Rt),Rt.forEach(t),AKe=i(js),Th=n(js,"DIV",{class:!0});var CBe=s(Th);m(PM.$$.fragment,CBe),LKe=i(CBe),WU=n(CBe,"P",{});var dxr=s(WU);BKe=r(dxr,"Register a new processor for this class."),dxr.forEach(t),CBe.forEach(t),js.forEach(t),_Le=i(d),qi=n(d,"H2",{class:!0});var MBe=s(qi);Fh=n(MBe,"A",{id:!0,class:!0,href:!0});var cxr=s(Fh);QU=n(cxr,"SPAN",{});var fxr=s(QU);m($M.$$.fragment,fxr),fxr.forEach(t),cxr.forEach(t),kKe=i(MBe),HU=n(MBe,"SPAN",{});var mxr=s(HU);xKe=r(mxr,"AutoModel"),mxr.forEach(t),MBe.forEach(t),uLe=i(d),Vo=n(d,"DIV",{class:!0});var Ns=s(Vo);m(IM.$$.fragment,Ns),RKe=i(Ns),Gi=n(Ns,"P",{});var WX=s(Gi);SKe=r(WX,`This is a generic model class that will be instantiated as one of the base model classes of the library when created
with the `),UU=n(WX,"CODE",{});var gxr=s(UU);PKe=r(gxr,"from_pretrained()"),gxr.forEach(t),$Ke=r(WX,"class method or the "),JU=n(WX,"CODE",{});var hxr=s(JU);IKe=r(hxr,"from_config()"),hxr.forEach(t),jKe=r(WX,`class
method.`),WX.forEach(t),NKe=i(Ns),jM=n(Ns,"P",{});var EBe=s(jM);DKe=r(EBe,"This class cannot be instantiated directly using "),YU=n(EBe,"CODE",{});var pxr=s(YU);qKe=r(pxr,"__init__()"),pxr.forEach(t),GKe=r(EBe," (throws an error)."),EBe.forEach(t),OKe=i(Ns),Nr=n(Ns,"DIV",{class:!0});var Ds=s(Nr);m(NM.$$.fragment,Ds),XKe=i(Ds),KU=n(Ds,"P",{});var _xr=s(KU);zKe=r(_xr,"Instantiates one of the base model classes of the library from a configuration."),_xr.forEach(t),VKe=i(Ds),Oi=n(Ds,"P",{});var QX=s(Oi);WKe=r(QX,`Note:
Loading a model from its configuration file does `),ZU=n(QX,"STRONG",{});var uxr=s(ZU);QKe=r(uxr,"not"),uxr.forEach(t),HKe=r(QX,` load the model weights. It only affects the
model\u2019s configuration. Use `),eJ=n(QX,"CODE",{});var bxr=s(eJ);UKe=r(bxr,"from_pretrained()"),bxr.forEach(t),JKe=r(QX,"to load the model weights."),QX.forEach(t),YKe=i(Ds),oJ=n(Ds,"P",{});var vxr=s(oJ);KKe=r(vxr,"Examples:"),vxr.forEach(t),ZKe=i(Ds),m(DM.$$.fragment,Ds),Ds.forEach(t),eZe=i(Ns),ke=n(Ns,"DIV",{class:!0});var St=s(ke);m(qM.$$.fragment,St),oZe=i(St),rJ=n(St,"P",{});var Txr=s(rJ);rZe=r(Txr,"Instantiate one of the base model classes of the library from a pretrained model."),Txr.forEach(t),tZe=i(St),Da=n(St,"P",{});var l4=s(Da);aZe=r(l4,"The model class to instantiate is selected based on the "),tJ=n(l4,"CODE",{});var Fxr=s(tJ);nZe=r(Fxr,"model_type"),Fxr.forEach(t),sZe=r(l4,` property of the config object (either
passed as an argument or loaded from `),aJ=n(l4,"CODE",{});var Cxr=s(aJ);lZe=r(Cxr,"pretrained_model_name_or_path"),Cxr.forEach(t),iZe=r(l4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),nJ=n(l4,"CODE",{});var Mxr=s(nJ);dZe=r(Mxr,"pretrained_model_name_or_path"),Mxr.forEach(t),cZe=r(l4,":"),l4.forEach(t),fZe=i(St),F=n(St,"UL",{});var C=s(F);Ch=n(C,"LI",{});var g7e=s(Ch);sJ=n(g7e,"STRONG",{});var Exr=s(sJ);mZe=r(Exr,"albert"),Exr.forEach(t),gZe=r(g7e," \u2014 "),gx=n(g7e,"A",{href:!0});var yxr=s(gx);hZe=r(yxr,"AlbertModel"),yxr.forEach(t),pZe=r(g7e," (ALBERT model)"),g7e.forEach(t),_Ze=i(C),Mh=n(C,"LI",{});var h7e=s(Mh);lJ=n(h7e,"STRONG",{});var wxr=s(lJ);uZe=r(wxr,"bart"),wxr.forEach(t),bZe=r(h7e," \u2014 "),hx=n(h7e,"A",{href:!0});var Axr=s(hx);vZe=r(Axr,"BartModel"),Axr.forEach(t),TZe=r(h7e," (BART model)"),h7e.forEach(t),FZe=i(C),Eh=n(C,"LI",{});var p7e=s(Eh);iJ=n(p7e,"STRONG",{});var Lxr=s(iJ);CZe=r(Lxr,"beit"),Lxr.forEach(t),MZe=r(p7e," \u2014 "),px=n(p7e,"A",{href:!0});var Bxr=s(px);EZe=r(Bxr,"BeitModel"),Bxr.forEach(t),yZe=r(p7e," (BEiT model)"),p7e.forEach(t),wZe=i(C),yh=n(C,"LI",{});var _7e=s(yh);dJ=n(_7e,"STRONG",{});var kxr=s(dJ);AZe=r(kxr,"bert"),kxr.forEach(t),LZe=r(_7e," \u2014 "),_x=n(_7e,"A",{href:!0});var xxr=s(_x);BZe=r(xxr,"BertModel"),xxr.forEach(t),kZe=r(_7e," (BERT model)"),_7e.forEach(t),xZe=i(C),wh=n(C,"LI",{});var u7e=s(wh);cJ=n(u7e,"STRONG",{});var Rxr=s(cJ);RZe=r(Rxr,"bert-generation"),Rxr.forEach(t),SZe=r(u7e," \u2014 "),ux=n(u7e,"A",{href:!0});var Sxr=s(ux);PZe=r(Sxr,"BertGenerationEncoder"),Sxr.forEach(t),$Ze=r(u7e," (Bert Generation model)"),u7e.forEach(t),IZe=i(C),Ah=n(C,"LI",{});var b7e=s(Ah);fJ=n(b7e,"STRONG",{});var Pxr=s(fJ);jZe=r(Pxr,"big_bird"),Pxr.forEach(t),NZe=r(b7e," \u2014 "),bx=n(b7e,"A",{href:!0});var $xr=s(bx);DZe=r($xr,"BigBirdModel"),$xr.forEach(t),qZe=r(b7e," (BigBird model)"),b7e.forEach(t),GZe=i(C),Lh=n(C,"LI",{});var v7e=s(Lh);mJ=n(v7e,"STRONG",{});var Ixr=s(mJ);OZe=r(Ixr,"bigbird_pegasus"),Ixr.forEach(t),XZe=r(v7e," \u2014 "),vx=n(v7e,"A",{href:!0});var jxr=s(vx);zZe=r(jxr,"BigBirdPegasusModel"),jxr.forEach(t),VZe=r(v7e," (BigBirdPegasus model)"),v7e.forEach(t),WZe=i(C),Bh=n(C,"LI",{});var T7e=s(Bh);gJ=n(T7e,"STRONG",{});var Nxr=s(gJ);QZe=r(Nxr,"blenderbot"),Nxr.forEach(t),HZe=r(T7e," \u2014 "),Tx=n(T7e,"A",{href:!0});var Dxr=s(Tx);UZe=r(Dxr,"BlenderbotModel"),Dxr.forEach(t),JZe=r(T7e," (Blenderbot model)"),T7e.forEach(t),YZe=i(C),kh=n(C,"LI",{});var F7e=s(kh);hJ=n(F7e,"STRONG",{});var qxr=s(hJ);KZe=r(qxr,"blenderbot-small"),qxr.forEach(t),ZZe=r(F7e," \u2014 "),Fx=n(F7e,"A",{href:!0});var Gxr=s(Fx);eeo=r(Gxr,"BlenderbotSmallModel"),Gxr.forEach(t),oeo=r(F7e," (BlenderbotSmall model)"),F7e.forEach(t),reo=i(C),xh=n(C,"LI",{});var C7e=s(xh);pJ=n(C7e,"STRONG",{});var Oxr=s(pJ);teo=r(Oxr,"camembert"),Oxr.forEach(t),aeo=r(C7e," \u2014 "),Cx=n(C7e,"A",{href:!0});var Xxr=s(Cx);neo=r(Xxr,"CamembertModel"),Xxr.forEach(t),seo=r(C7e," (CamemBERT model)"),C7e.forEach(t),leo=i(C),Rh=n(C,"LI",{});var M7e=s(Rh);_J=n(M7e,"STRONG",{});var zxr=s(_J);ieo=r(zxr,"canine"),zxr.forEach(t),deo=r(M7e," \u2014 "),Mx=n(M7e,"A",{href:!0});var Vxr=s(Mx);ceo=r(Vxr,"CanineModel"),Vxr.forEach(t),feo=r(M7e," (Canine model)"),M7e.forEach(t),meo=i(C),Sh=n(C,"LI",{});var E7e=s(Sh);uJ=n(E7e,"STRONG",{});var Wxr=s(uJ);geo=r(Wxr,"clip"),Wxr.forEach(t),heo=r(E7e," \u2014 "),Ex=n(E7e,"A",{href:!0});var Qxr=s(Ex);peo=r(Qxr,"CLIPModel"),Qxr.forEach(t),_eo=r(E7e," (CLIP model)"),E7e.forEach(t),ueo=i(C),Ph=n(C,"LI",{});var y7e=s(Ph);bJ=n(y7e,"STRONG",{});var Hxr=s(bJ);beo=r(Hxr,"convbert"),Hxr.forEach(t),veo=r(y7e," \u2014 "),yx=n(y7e,"A",{href:!0});var Uxr=s(yx);Teo=r(Uxr,"ConvBertModel"),Uxr.forEach(t),Feo=r(y7e," (ConvBERT model)"),y7e.forEach(t),Ceo=i(C),$h=n(C,"LI",{});var w7e=s($h);vJ=n(w7e,"STRONG",{});var Jxr=s(vJ);Meo=r(Jxr,"convnext"),Jxr.forEach(t),Eeo=r(w7e," \u2014 "),wx=n(w7e,"A",{href:!0});var Yxr=s(wx);yeo=r(Yxr,"ConvNextModel"),Yxr.forEach(t),weo=r(w7e," (ConvNext model)"),w7e.forEach(t),Aeo=i(C),Ih=n(C,"LI",{});var A7e=s(Ih);TJ=n(A7e,"STRONG",{});var Kxr=s(TJ);Leo=r(Kxr,"ctrl"),Kxr.forEach(t),Beo=r(A7e," \u2014 "),Ax=n(A7e,"A",{href:!0});var Zxr=s(Ax);keo=r(Zxr,"CTRLModel"),Zxr.forEach(t),xeo=r(A7e," (CTRL model)"),A7e.forEach(t),Reo=i(C),jh=n(C,"LI",{});var L7e=s(jh);FJ=n(L7e,"STRONG",{});var eRr=s(FJ);Seo=r(eRr,"deberta"),eRr.forEach(t),Peo=r(L7e," \u2014 "),Lx=n(L7e,"A",{href:!0});var oRr=s(Lx);$eo=r(oRr,"DebertaModel"),oRr.forEach(t),Ieo=r(L7e," (DeBERTa model)"),L7e.forEach(t),jeo=i(C),Nh=n(C,"LI",{});var B7e=s(Nh);CJ=n(B7e,"STRONG",{});var rRr=s(CJ);Neo=r(rRr,"deberta-v2"),rRr.forEach(t),Deo=r(B7e," \u2014 "),Bx=n(B7e,"A",{href:!0});var tRr=s(Bx);qeo=r(tRr,"DebertaV2Model"),tRr.forEach(t),Geo=r(B7e," (DeBERTa-v2 model)"),B7e.forEach(t),Oeo=i(C),Dh=n(C,"LI",{});var k7e=s(Dh);MJ=n(k7e,"STRONG",{});var aRr=s(MJ);Xeo=r(aRr,"deit"),aRr.forEach(t),zeo=r(k7e," \u2014 "),kx=n(k7e,"A",{href:!0});var nRr=s(kx);Veo=r(nRr,"DeiTModel"),nRr.forEach(t),Weo=r(k7e," (DeiT model)"),k7e.forEach(t),Qeo=i(C),qh=n(C,"LI",{});var x7e=s(qh);EJ=n(x7e,"STRONG",{});var sRr=s(EJ);Heo=r(sRr,"detr"),sRr.forEach(t),Ueo=r(x7e," \u2014 "),xx=n(x7e,"A",{href:!0});var lRr=s(xx);Jeo=r(lRr,"DetrModel"),lRr.forEach(t),Yeo=r(x7e," (DETR model)"),x7e.forEach(t),Keo=i(C),Gh=n(C,"LI",{});var R7e=s(Gh);yJ=n(R7e,"STRONG",{});var iRr=s(yJ);Zeo=r(iRr,"distilbert"),iRr.forEach(t),eoo=r(R7e," \u2014 "),Rx=n(R7e,"A",{href:!0});var dRr=s(Rx);ooo=r(dRr,"DistilBertModel"),dRr.forEach(t),roo=r(R7e," (DistilBERT model)"),R7e.forEach(t),too=i(C),Oh=n(C,"LI",{});var S7e=s(Oh);wJ=n(S7e,"STRONG",{});var cRr=s(wJ);aoo=r(cRr,"dpr"),cRr.forEach(t),noo=r(S7e," \u2014 "),Sx=n(S7e,"A",{href:!0});var fRr=s(Sx);soo=r(fRr,"DPRQuestionEncoder"),fRr.forEach(t),loo=r(S7e," (DPR model)"),S7e.forEach(t),ioo=i(C),Xh=n(C,"LI",{});var P7e=s(Xh);AJ=n(P7e,"STRONG",{});var mRr=s(AJ);doo=r(mRr,"electra"),mRr.forEach(t),coo=r(P7e," \u2014 "),Px=n(P7e,"A",{href:!0});var gRr=s(Px);foo=r(gRr,"ElectraModel"),gRr.forEach(t),moo=r(P7e," (ELECTRA model)"),P7e.forEach(t),goo=i(C),zh=n(C,"LI",{});var $7e=s(zh);LJ=n($7e,"STRONG",{});var hRr=s(LJ);hoo=r(hRr,"flaubert"),hRr.forEach(t),poo=r($7e," \u2014 "),$x=n($7e,"A",{href:!0});var pRr=s($x);_oo=r(pRr,"FlaubertModel"),pRr.forEach(t),uoo=r($7e," (FlauBERT model)"),$7e.forEach(t),boo=i(C),Vh=n(C,"LI",{});var I7e=s(Vh);BJ=n(I7e,"STRONG",{});var _Rr=s(BJ);voo=r(_Rr,"fnet"),_Rr.forEach(t),Too=r(I7e," \u2014 "),Ix=n(I7e,"A",{href:!0});var uRr=s(Ix);Foo=r(uRr,"FNetModel"),uRr.forEach(t),Coo=r(I7e," (FNet model)"),I7e.forEach(t),Moo=i(C),Wh=n(C,"LI",{});var j7e=s(Wh);kJ=n(j7e,"STRONG",{});var bRr=s(kJ);Eoo=r(bRr,"fsmt"),bRr.forEach(t),yoo=r(j7e," \u2014 "),jx=n(j7e,"A",{href:!0});var vRr=s(jx);woo=r(vRr,"FSMTModel"),vRr.forEach(t),Aoo=r(j7e," (FairSeq Machine-Translation model)"),j7e.forEach(t),Loo=i(C),xs=n(C,"LI",{});var I0=s(xs);xJ=n(I0,"STRONG",{});var TRr=s(xJ);Boo=r(TRr,"funnel"),TRr.forEach(t),koo=r(I0," \u2014 "),Nx=n(I0,"A",{href:!0});var FRr=s(Nx);xoo=r(FRr,"FunnelModel"),FRr.forEach(t),Roo=r(I0," or "),Dx=n(I0,"A",{href:!0});var CRr=s(Dx);Soo=r(CRr,"FunnelBaseModel"),CRr.forEach(t),Poo=r(I0," (Funnel Transformer model)"),I0.forEach(t),$oo=i(C),Qh=n(C,"LI",{});var N7e=s(Qh);RJ=n(N7e,"STRONG",{});var MRr=s(RJ);Ioo=r(MRr,"gpt2"),MRr.forEach(t),joo=r(N7e," \u2014 "),qx=n(N7e,"A",{href:!0});var ERr=s(qx);Noo=r(ERr,"GPT2Model"),ERr.forEach(t),Doo=r(N7e," (OpenAI GPT-2 model)"),N7e.forEach(t),qoo=i(C),Hh=n(C,"LI",{});var D7e=s(Hh);SJ=n(D7e,"STRONG",{});var yRr=s(SJ);Goo=r(yRr,"gpt_neo"),yRr.forEach(t),Ooo=r(D7e," \u2014 "),Gx=n(D7e,"A",{href:!0});var wRr=s(Gx);Xoo=r(wRr,"GPTNeoModel"),wRr.forEach(t),zoo=r(D7e," (GPT Neo model)"),D7e.forEach(t),Voo=i(C),Uh=n(C,"LI",{});var q7e=s(Uh);PJ=n(q7e,"STRONG",{});var ARr=s(PJ);Woo=r(ARr,"gptj"),ARr.forEach(t),Qoo=r(q7e," \u2014 "),Ox=n(q7e,"A",{href:!0});var LRr=s(Ox);Hoo=r(LRr,"GPTJModel"),LRr.forEach(t),Uoo=r(q7e," (GPT-J model)"),q7e.forEach(t),Joo=i(C),Jh=n(C,"LI",{});var G7e=s(Jh);$J=n(G7e,"STRONG",{});var BRr=s($J);Yoo=r(BRr,"hubert"),BRr.forEach(t),Koo=r(G7e," \u2014 "),Xx=n(G7e,"A",{href:!0});var kRr=s(Xx);Zoo=r(kRr,"HubertModel"),kRr.forEach(t),ero=r(G7e," (Hubert model)"),G7e.forEach(t),oro=i(C),Yh=n(C,"LI",{});var O7e=s(Yh);IJ=n(O7e,"STRONG",{});var xRr=s(IJ);rro=r(xRr,"ibert"),xRr.forEach(t),tro=r(O7e," \u2014 "),zx=n(O7e,"A",{href:!0});var RRr=s(zx);aro=r(RRr,"IBertModel"),RRr.forEach(t),nro=r(O7e," (I-BERT model)"),O7e.forEach(t),sro=i(C),Kh=n(C,"LI",{});var X7e=s(Kh);jJ=n(X7e,"STRONG",{});var SRr=s(jJ);lro=r(SRr,"imagegpt"),SRr.forEach(t),iro=r(X7e," \u2014 "),Vx=n(X7e,"A",{href:!0});var PRr=s(Vx);dro=r(PRr,"ImageGPTModel"),PRr.forEach(t),cro=r(X7e," (ImageGPT model)"),X7e.forEach(t),fro=i(C),Zh=n(C,"LI",{});var z7e=s(Zh);NJ=n(z7e,"STRONG",{});var $Rr=s(NJ);mro=r($Rr,"layoutlm"),$Rr.forEach(t),gro=r(z7e," \u2014 "),Wx=n(z7e,"A",{href:!0});var IRr=s(Wx);hro=r(IRr,"LayoutLMModel"),IRr.forEach(t),pro=r(z7e," (LayoutLM model)"),z7e.forEach(t),_ro=i(C),ep=n(C,"LI",{});var V7e=s(ep);DJ=n(V7e,"STRONG",{});var jRr=s(DJ);uro=r(jRr,"layoutlmv2"),jRr.forEach(t),bro=r(V7e," \u2014 "),Qx=n(V7e,"A",{href:!0});var NRr=s(Qx);vro=r(NRr,"LayoutLMv2Model"),NRr.forEach(t),Tro=r(V7e," (LayoutLMv2 model)"),V7e.forEach(t),Fro=i(C),op=n(C,"LI",{});var W7e=s(op);qJ=n(W7e,"STRONG",{});var DRr=s(qJ);Cro=r(DRr,"led"),DRr.forEach(t),Mro=r(W7e," \u2014 "),Hx=n(W7e,"A",{href:!0});var qRr=s(Hx);Ero=r(qRr,"LEDModel"),qRr.forEach(t),yro=r(W7e," (LED model)"),W7e.forEach(t),wro=i(C),rp=n(C,"LI",{});var Q7e=s(rp);GJ=n(Q7e,"STRONG",{});var GRr=s(GJ);Aro=r(GRr,"longformer"),GRr.forEach(t),Lro=r(Q7e," \u2014 "),Ux=n(Q7e,"A",{href:!0});var ORr=s(Ux);Bro=r(ORr,"LongformerModel"),ORr.forEach(t),kro=r(Q7e," (Longformer model)"),Q7e.forEach(t),xro=i(C),tp=n(C,"LI",{});var H7e=s(tp);OJ=n(H7e,"STRONG",{});var XRr=s(OJ);Rro=r(XRr,"luke"),XRr.forEach(t),Sro=r(H7e," \u2014 "),Jx=n(H7e,"A",{href:!0});var zRr=s(Jx);Pro=r(zRr,"LukeModel"),zRr.forEach(t),$ro=r(H7e," (LUKE model)"),H7e.forEach(t),Iro=i(C),ap=n(C,"LI",{});var U7e=s(ap);XJ=n(U7e,"STRONG",{});var VRr=s(XJ);jro=r(VRr,"lxmert"),VRr.forEach(t),Nro=r(U7e," \u2014 "),Yx=n(U7e,"A",{href:!0});var WRr=s(Yx);Dro=r(WRr,"LxmertModel"),WRr.forEach(t),qro=r(U7e," (LXMERT model)"),U7e.forEach(t),Gro=i(C),np=n(C,"LI",{});var J7e=s(np);zJ=n(J7e,"STRONG",{});var QRr=s(zJ);Oro=r(QRr,"m2m_100"),QRr.forEach(t),Xro=r(J7e," \u2014 "),Kx=n(J7e,"A",{href:!0});var HRr=s(Kx);zro=r(HRr,"M2M100Model"),HRr.forEach(t),Vro=r(J7e," (M2M100 model)"),J7e.forEach(t),Wro=i(C),sp=n(C,"LI",{});var Y7e=s(sp);VJ=n(Y7e,"STRONG",{});var URr=s(VJ);Qro=r(URr,"marian"),URr.forEach(t),Hro=r(Y7e," \u2014 "),Zx=n(Y7e,"A",{href:!0});var JRr=s(Zx);Uro=r(JRr,"MarianModel"),JRr.forEach(t),Jro=r(Y7e," (Marian model)"),Y7e.forEach(t),Yro=i(C),lp=n(C,"LI",{});var K7e=s(lp);WJ=n(K7e,"STRONG",{});var YRr=s(WJ);Kro=r(YRr,"mbart"),YRr.forEach(t),Zro=r(K7e," \u2014 "),eR=n(K7e,"A",{href:!0});var KRr=s(eR);eto=r(KRr,"MBartModel"),KRr.forEach(t),oto=r(K7e," (mBART model)"),K7e.forEach(t),rto=i(C),ip=n(C,"LI",{});var Z7e=s(ip);QJ=n(Z7e,"STRONG",{});var ZRr=s(QJ);tto=r(ZRr,"megatron-bert"),ZRr.forEach(t),ato=r(Z7e," \u2014 "),oR=n(Z7e,"A",{href:!0});var eSr=s(oR);nto=r(eSr,"MegatronBertModel"),eSr.forEach(t),sto=r(Z7e," (MegatronBert model)"),Z7e.forEach(t),lto=i(C),dp=n(C,"LI",{});var eFe=s(dp);HJ=n(eFe,"STRONG",{});var oSr=s(HJ);ito=r(oSr,"mobilebert"),oSr.forEach(t),dto=r(eFe," \u2014 "),rR=n(eFe,"A",{href:!0});var rSr=s(rR);cto=r(rSr,"MobileBertModel"),rSr.forEach(t),fto=r(eFe," (MobileBERT model)"),eFe.forEach(t),mto=i(C),cp=n(C,"LI",{});var oFe=s(cp);UJ=n(oFe,"STRONG",{});var tSr=s(UJ);gto=r(tSr,"mpnet"),tSr.forEach(t),hto=r(oFe," \u2014 "),tR=n(oFe,"A",{href:!0});var aSr=s(tR);pto=r(aSr,"MPNetModel"),aSr.forEach(t),_to=r(oFe," (MPNet model)"),oFe.forEach(t),uto=i(C),fp=n(C,"LI",{});var rFe=s(fp);JJ=n(rFe,"STRONG",{});var nSr=s(JJ);bto=r(nSr,"mt5"),nSr.forEach(t),vto=r(rFe," \u2014 "),aR=n(rFe,"A",{href:!0});var sSr=s(aR);Tto=r(sSr,"MT5Model"),sSr.forEach(t),Fto=r(rFe," (mT5 model)"),rFe.forEach(t),Cto=i(C),mp=n(C,"LI",{});var tFe=s(mp);YJ=n(tFe,"STRONG",{});var lSr=s(YJ);Mto=r(lSr,"nystromformer"),lSr.forEach(t),Eto=r(tFe," \u2014 "),nR=n(tFe,"A",{href:!0});var iSr=s(nR);yto=r(iSr,"NystromformerModel"),iSr.forEach(t),wto=r(tFe," (Nystromformer model)"),tFe.forEach(t),Ato=i(C),gp=n(C,"LI",{});var aFe=s(gp);KJ=n(aFe,"STRONG",{});var dSr=s(KJ);Lto=r(dSr,"openai-gpt"),dSr.forEach(t),Bto=r(aFe," \u2014 "),sR=n(aFe,"A",{href:!0});var cSr=s(sR);kto=r(cSr,"OpenAIGPTModel"),cSr.forEach(t),xto=r(aFe," (OpenAI GPT model)"),aFe.forEach(t),Rto=i(C),hp=n(C,"LI",{});var nFe=s(hp);ZJ=n(nFe,"STRONG",{});var fSr=s(ZJ);Sto=r(fSr,"pegasus"),fSr.forEach(t),Pto=r(nFe," \u2014 "),lR=n(nFe,"A",{href:!0});var mSr=s(lR);$to=r(mSr,"PegasusModel"),mSr.forEach(t),Ito=r(nFe," (Pegasus model)"),nFe.forEach(t),jto=i(C),pp=n(C,"LI",{});var sFe=s(pp);eY=n(sFe,"STRONG",{});var gSr=s(eY);Nto=r(gSr,"perceiver"),gSr.forEach(t),Dto=r(sFe," \u2014 "),iR=n(sFe,"A",{href:!0});var hSr=s(iR);qto=r(hSr,"PerceiverModel"),hSr.forEach(t),Gto=r(sFe," (Perceiver model)"),sFe.forEach(t),Oto=i(C),_p=n(C,"LI",{});var lFe=s(_p);oY=n(lFe,"STRONG",{});var pSr=s(oY);Xto=r(pSr,"plbart"),pSr.forEach(t),zto=r(lFe," \u2014 "),dR=n(lFe,"A",{href:!0});var _Sr=s(dR);Vto=r(_Sr,"PLBartModel"),_Sr.forEach(t),Wto=r(lFe," (PLBart model)"),lFe.forEach(t),Qto=i(C),up=n(C,"LI",{});var iFe=s(up);rY=n(iFe,"STRONG",{});var uSr=s(rY);Hto=r(uSr,"poolformer"),uSr.forEach(t),Uto=r(iFe," \u2014 "),cR=n(iFe,"A",{href:!0});var bSr=s(cR);Jto=r(bSr,"PoolFormerModel"),bSr.forEach(t),Yto=r(iFe," (PoolFormer model)"),iFe.forEach(t),Kto=i(C),bp=n(C,"LI",{});var dFe=s(bp);tY=n(dFe,"STRONG",{});var vSr=s(tY);Zto=r(vSr,"prophetnet"),vSr.forEach(t),eao=r(dFe," \u2014 "),fR=n(dFe,"A",{href:!0});var TSr=s(fR);oao=r(TSr,"ProphetNetModel"),TSr.forEach(t),rao=r(dFe," (ProphetNet model)"),dFe.forEach(t),tao=i(C),vp=n(C,"LI",{});var cFe=s(vp);aY=n(cFe,"STRONG",{});var FSr=s(aY);aao=r(FSr,"qdqbert"),FSr.forEach(t),nao=r(cFe," \u2014 "),mR=n(cFe,"A",{href:!0});var CSr=s(mR);sao=r(CSr,"QDQBertModel"),CSr.forEach(t),lao=r(cFe," (QDQBert model)"),cFe.forEach(t),iao=i(C),Tp=n(C,"LI",{});var fFe=s(Tp);nY=n(fFe,"STRONG",{});var MSr=s(nY);dao=r(MSr,"reformer"),MSr.forEach(t),cao=r(fFe," \u2014 "),gR=n(fFe,"A",{href:!0});var ESr=s(gR);fao=r(ESr,"ReformerModel"),ESr.forEach(t),mao=r(fFe," (Reformer model)"),fFe.forEach(t),gao=i(C),Fp=n(C,"LI",{});var mFe=s(Fp);sY=n(mFe,"STRONG",{});var ySr=s(sY);hao=r(ySr,"rembert"),ySr.forEach(t),pao=r(mFe," \u2014 "),hR=n(mFe,"A",{href:!0});var wSr=s(hR);_ao=r(wSr,"RemBertModel"),wSr.forEach(t),uao=r(mFe," (RemBERT model)"),mFe.forEach(t),bao=i(C),Cp=n(C,"LI",{});var gFe=s(Cp);lY=n(gFe,"STRONG",{});var ASr=s(lY);vao=r(ASr,"retribert"),ASr.forEach(t),Tao=r(gFe," \u2014 "),pR=n(gFe,"A",{href:!0});var LSr=s(pR);Fao=r(LSr,"RetriBertModel"),LSr.forEach(t),Cao=r(gFe," (RetriBERT model)"),gFe.forEach(t),Mao=i(C),Mp=n(C,"LI",{});var hFe=s(Mp);iY=n(hFe,"STRONG",{});var BSr=s(iY);Eao=r(BSr,"roberta"),BSr.forEach(t),yao=r(hFe," \u2014 "),_R=n(hFe,"A",{href:!0});var kSr=s(_R);wao=r(kSr,"RobertaModel"),kSr.forEach(t),Aao=r(hFe," (RoBERTa model)"),hFe.forEach(t),Lao=i(C),Ep=n(C,"LI",{});var pFe=s(Ep);dY=n(pFe,"STRONG",{});var xSr=s(dY);Bao=r(xSr,"roformer"),xSr.forEach(t),kao=r(pFe," \u2014 "),uR=n(pFe,"A",{href:!0});var RSr=s(uR);xao=r(RSr,"RoFormerModel"),RSr.forEach(t),Rao=r(pFe," (RoFormer model)"),pFe.forEach(t),Sao=i(C),yp=n(C,"LI",{});var _Fe=s(yp);cY=n(_Fe,"STRONG",{});var SSr=s(cY);Pao=r(SSr,"segformer"),SSr.forEach(t),$ao=r(_Fe," \u2014 "),bR=n(_Fe,"A",{href:!0});var PSr=s(bR);Iao=r(PSr,"SegformerModel"),PSr.forEach(t),jao=r(_Fe," (SegFormer model)"),_Fe.forEach(t),Nao=i(C),wp=n(C,"LI",{});var uFe=s(wp);fY=n(uFe,"STRONG",{});var $Sr=s(fY);Dao=r($Sr,"sew"),$Sr.forEach(t),qao=r(uFe," \u2014 "),vR=n(uFe,"A",{href:!0});var ISr=s(vR);Gao=r(ISr,"SEWModel"),ISr.forEach(t),Oao=r(uFe," (SEW model)"),uFe.forEach(t),Xao=i(C),Ap=n(C,"LI",{});var bFe=s(Ap);mY=n(bFe,"STRONG",{});var jSr=s(mY);zao=r(jSr,"sew-d"),jSr.forEach(t),Vao=r(bFe," \u2014 "),TR=n(bFe,"A",{href:!0});var NSr=s(TR);Wao=r(NSr,"SEWDModel"),NSr.forEach(t),Qao=r(bFe," (SEW-D model)"),bFe.forEach(t),Hao=i(C),Lp=n(C,"LI",{});var vFe=s(Lp);gY=n(vFe,"STRONG",{});var DSr=s(gY);Uao=r(DSr,"speech_to_text"),DSr.forEach(t),Jao=r(vFe," \u2014 "),FR=n(vFe,"A",{href:!0});var qSr=s(FR);Yao=r(qSr,"Speech2TextModel"),qSr.forEach(t),Kao=r(vFe," (Speech2Text model)"),vFe.forEach(t),Zao=i(C),Bp=n(C,"LI",{});var TFe=s(Bp);hY=n(TFe,"STRONG",{});var GSr=s(hY);eno=r(GSr,"splinter"),GSr.forEach(t),ono=r(TFe," \u2014 "),CR=n(TFe,"A",{href:!0});var OSr=s(CR);rno=r(OSr,"SplinterModel"),OSr.forEach(t),tno=r(TFe," (Splinter model)"),TFe.forEach(t),ano=i(C),kp=n(C,"LI",{});var FFe=s(kp);pY=n(FFe,"STRONG",{});var XSr=s(pY);nno=r(XSr,"squeezebert"),XSr.forEach(t),sno=r(FFe," \u2014 "),MR=n(FFe,"A",{href:!0});var zSr=s(MR);lno=r(zSr,"SqueezeBertModel"),zSr.forEach(t),ino=r(FFe," (SqueezeBERT model)"),FFe.forEach(t),dno=i(C),xp=n(C,"LI",{});var CFe=s(xp);_Y=n(CFe,"STRONG",{});var VSr=s(_Y);cno=r(VSr,"swin"),VSr.forEach(t),fno=r(CFe," \u2014 "),ER=n(CFe,"A",{href:!0});var WSr=s(ER);mno=r(WSr,"SwinModel"),WSr.forEach(t),gno=r(CFe," (Swin model)"),CFe.forEach(t),hno=i(C),Rp=n(C,"LI",{});var MFe=s(Rp);uY=n(MFe,"STRONG",{});var QSr=s(uY);pno=r(QSr,"t5"),QSr.forEach(t),_no=r(MFe," \u2014 "),yR=n(MFe,"A",{href:!0});var HSr=s(yR);uno=r(HSr,"T5Model"),HSr.forEach(t),bno=r(MFe," (T5 model)"),MFe.forEach(t),vno=i(C),Sp=n(C,"LI",{});var EFe=s(Sp);bY=n(EFe,"STRONG",{});var USr=s(bY);Tno=r(USr,"tapas"),USr.forEach(t),Fno=r(EFe," \u2014 "),wR=n(EFe,"A",{href:!0});var JSr=s(wR);Cno=r(JSr,"TapasModel"),JSr.forEach(t),Mno=r(EFe," (TAPAS model)"),EFe.forEach(t),Eno=i(C),Pp=n(C,"LI",{});var yFe=s(Pp);vY=n(yFe,"STRONG",{});var YSr=s(vY);yno=r(YSr,"transfo-xl"),YSr.forEach(t),wno=r(yFe," \u2014 "),AR=n(yFe,"A",{href:!0});var KSr=s(AR);Ano=r(KSr,"TransfoXLModel"),KSr.forEach(t),Lno=r(yFe," (Transformer-XL model)"),yFe.forEach(t),Bno=i(C),$p=n(C,"LI",{});var wFe=s($p);TY=n(wFe,"STRONG",{});var ZSr=s(TY);kno=r(ZSr,"unispeech"),ZSr.forEach(t),xno=r(wFe," \u2014 "),LR=n(wFe,"A",{href:!0});var ePr=s(LR);Rno=r(ePr,"UniSpeechModel"),ePr.forEach(t),Sno=r(wFe," (UniSpeech model)"),wFe.forEach(t),Pno=i(C),Ip=n(C,"LI",{});var AFe=s(Ip);FY=n(AFe,"STRONG",{});var oPr=s(FY);$no=r(oPr,"unispeech-sat"),oPr.forEach(t),Ino=r(AFe," \u2014 "),BR=n(AFe,"A",{href:!0});var rPr=s(BR);jno=r(rPr,"UniSpeechSatModel"),rPr.forEach(t),Nno=r(AFe," (UniSpeechSat model)"),AFe.forEach(t),Dno=i(C),jp=n(C,"LI",{});var LFe=s(jp);CY=n(LFe,"STRONG",{});var tPr=s(CY);qno=r(tPr,"vilt"),tPr.forEach(t),Gno=r(LFe," \u2014 "),kR=n(LFe,"A",{href:!0});var aPr=s(kR);Ono=r(aPr,"ViltModel"),aPr.forEach(t),Xno=r(LFe," (ViLT model)"),LFe.forEach(t),zno=i(C),Np=n(C,"LI",{});var BFe=s(Np);MY=n(BFe,"STRONG",{});var nPr=s(MY);Vno=r(nPr,"vision-text-dual-encoder"),nPr.forEach(t),Wno=r(BFe," \u2014 "),xR=n(BFe,"A",{href:!0});var sPr=s(xR);Qno=r(sPr,"VisionTextDualEncoderModel"),sPr.forEach(t),Hno=r(BFe," (VisionTextDualEncoder model)"),BFe.forEach(t),Uno=i(C),Dp=n(C,"LI",{});var kFe=s(Dp);EY=n(kFe,"STRONG",{});var lPr=s(EY);Jno=r(lPr,"visual_bert"),lPr.forEach(t),Yno=r(kFe," \u2014 "),RR=n(kFe,"A",{href:!0});var iPr=s(RR);Kno=r(iPr,"VisualBertModel"),iPr.forEach(t),Zno=r(kFe," (VisualBert model)"),kFe.forEach(t),eso=i(C),qp=n(C,"LI",{});var xFe=s(qp);yY=n(xFe,"STRONG",{});var dPr=s(yY);oso=r(dPr,"vit"),dPr.forEach(t),rso=r(xFe," \u2014 "),SR=n(xFe,"A",{href:!0});var cPr=s(SR);tso=r(cPr,"ViTModel"),cPr.forEach(t),aso=r(xFe," (ViT model)"),xFe.forEach(t),nso=i(C),Gp=n(C,"LI",{});var RFe=s(Gp);wY=n(RFe,"STRONG",{});var fPr=s(wY);sso=r(fPr,"vit_mae"),fPr.forEach(t),lso=r(RFe," \u2014 "),PR=n(RFe,"A",{href:!0});var mPr=s(PR);iso=r(mPr,"ViTMAEModel"),mPr.forEach(t),dso=r(RFe," (ViTMAE model)"),RFe.forEach(t),cso=i(C),Op=n(C,"LI",{});var SFe=s(Op);AY=n(SFe,"STRONG",{});var gPr=s(AY);fso=r(gPr,"wav2vec2"),gPr.forEach(t),mso=r(SFe," \u2014 "),$R=n(SFe,"A",{href:!0});var hPr=s($R);gso=r(hPr,"Wav2Vec2Model"),hPr.forEach(t),hso=r(SFe," (Wav2Vec2 model)"),SFe.forEach(t),pso=i(C),Xp=n(C,"LI",{});var PFe=s(Xp);LY=n(PFe,"STRONG",{});var pPr=s(LY);_so=r(pPr,"wavlm"),pPr.forEach(t),uso=r(PFe," \u2014 "),IR=n(PFe,"A",{href:!0});var _Pr=s(IR);bso=r(_Pr,"WavLMModel"),_Pr.forEach(t),vso=r(PFe," (WavLM model)"),PFe.forEach(t),Tso=i(C),zp=n(C,"LI",{});var $Fe=s(zp);BY=n($Fe,"STRONG",{});var uPr=s(BY);Fso=r(uPr,"xglm"),uPr.forEach(t),Cso=r($Fe," \u2014 "),jR=n($Fe,"A",{href:!0});var bPr=s(jR);Mso=r(bPr,"XGLMModel"),bPr.forEach(t),Eso=r($Fe," (XGLM model)"),$Fe.forEach(t),yso=i(C),Vp=n(C,"LI",{});var IFe=s(Vp);kY=n(IFe,"STRONG",{});var vPr=s(kY);wso=r(vPr,"xlm"),vPr.forEach(t),Aso=r(IFe," \u2014 "),NR=n(IFe,"A",{href:!0});var TPr=s(NR);Lso=r(TPr,"XLMModel"),TPr.forEach(t),Bso=r(IFe," (XLM model)"),IFe.forEach(t),kso=i(C),Wp=n(C,"LI",{});var jFe=s(Wp);xY=n(jFe,"STRONG",{});var FPr=s(xY);xso=r(FPr,"xlm-prophetnet"),FPr.forEach(t),Rso=r(jFe," \u2014 "),DR=n(jFe,"A",{href:!0});var CPr=s(DR);Sso=r(CPr,"XLMProphetNetModel"),CPr.forEach(t),Pso=r(jFe," (XLMProphetNet model)"),jFe.forEach(t),$so=i(C),Qp=n(C,"LI",{});var NFe=s(Qp);RY=n(NFe,"STRONG",{});var MPr=s(RY);Iso=r(MPr,"xlm-roberta"),MPr.forEach(t),jso=r(NFe," \u2014 "),qR=n(NFe,"A",{href:!0});var EPr=s(qR);Nso=r(EPr,"XLMRobertaModel"),EPr.forEach(t),Dso=r(NFe," (XLM-RoBERTa model)"),NFe.forEach(t),qso=i(C),Hp=n(C,"LI",{});var DFe=s(Hp);SY=n(DFe,"STRONG",{});var yPr=s(SY);Gso=r(yPr,"xlm-roberta-xl"),yPr.forEach(t),Oso=r(DFe," \u2014 "),GR=n(DFe,"A",{href:!0});var wPr=s(GR);Xso=r(wPr,"XLMRobertaXLModel"),wPr.forEach(t),zso=r(DFe," (XLM-RoBERTa-XL model)"),DFe.forEach(t),Vso=i(C),Up=n(C,"LI",{});var qFe=s(Up);PY=n(qFe,"STRONG",{});var APr=s(PY);Wso=r(APr,"xlnet"),APr.forEach(t),Qso=r(qFe," \u2014 "),OR=n(qFe,"A",{href:!0});var LPr=s(OR);Hso=r(LPr,"XLNetModel"),LPr.forEach(t),Uso=r(qFe," (XLNet model)"),qFe.forEach(t),Jso=i(C),Jp=n(C,"LI",{});var GFe=s(Jp);$Y=n(GFe,"STRONG",{});var BPr=s($Y);Yso=r(BPr,"yoso"),BPr.forEach(t),Kso=r(GFe," \u2014 "),XR=n(GFe,"A",{href:!0});var kPr=s(XR);Zso=r(kPr,"YosoModel"),kPr.forEach(t),elo=r(GFe," (YOSO model)"),GFe.forEach(t),C.forEach(t),olo=i(St),Yp=n(St,"P",{});var OFe=s(Yp);rlo=r(OFe,"The model is set in evaluation mode by default using "),IY=n(OFe,"CODE",{});var xPr=s(IY);tlo=r(xPr,"model.eval()"),xPr.forEach(t),alo=r(OFe,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),jY=n(OFe,"CODE",{});var RPr=s(jY);nlo=r(RPr,"model.train()"),RPr.forEach(t),OFe.forEach(t),slo=i(St),NY=n(St,"P",{});var SPr=s(NY);llo=r(SPr,"Examples:"),SPr.forEach(t),ilo=i(St),m(GM.$$.fragment,St),St.forEach(t),Ns.forEach(t),bLe=i(d),Xi=n(d,"H2",{class:!0});var yBe=s(Xi);Kp=n(yBe,"A",{id:!0,class:!0,href:!0});var PPr=s(Kp);DY=n(PPr,"SPAN",{});var $Pr=s(DY);m(OM.$$.fragment,$Pr),$Pr.forEach(t),PPr.forEach(t),dlo=i(yBe),qY=n(yBe,"SPAN",{});var IPr=s(qY);clo=r(IPr,"AutoModelForPreTraining"),IPr.forEach(t),yBe.forEach(t),vLe=i(d),Wo=n(d,"DIV",{class:!0});var qs=s(Wo);m(XM.$$.fragment,qs),flo=i(qs),zi=n(qs,"P",{});var HX=s(zi);mlo=r(HX,`This is a generic model class that will be instantiated as one of the model classes of the library (with a pretraining head) when created
with the `),GY=n(HX,"CODE",{});var jPr=s(GY);glo=r(jPr,"from_pretrained()"),jPr.forEach(t),hlo=r(HX,"class method or the "),OY=n(HX,"CODE",{});var NPr=s(OY);plo=r(NPr,"from_config()"),NPr.forEach(t),_lo=r(HX,`class
method.`),HX.forEach(t),ulo=i(qs),zM=n(qs,"P",{});var wBe=s(zM);blo=r(wBe,"This class cannot be instantiated directly using "),XY=n(wBe,"CODE",{});var DPr=s(XY);vlo=r(DPr,"__init__()"),DPr.forEach(t),Tlo=r(wBe," (throws an error)."),wBe.forEach(t),Flo=i(qs),Dr=n(qs,"DIV",{class:!0});var Gs=s(Dr);m(VM.$$.fragment,Gs),Clo=i(Gs),zY=n(Gs,"P",{});var qPr=s(zY);Mlo=r(qPr,"Instantiates one of the model classes of the library (with a pretraining head) from a configuration."),qPr.forEach(t),Elo=i(Gs),Vi=n(Gs,"P",{});var UX=s(Vi);ylo=r(UX,`Note:
Loading a model from its configuration file does `),VY=n(UX,"STRONG",{});var GPr=s(VY);wlo=r(GPr,"not"),GPr.forEach(t),Alo=r(UX,` load the model weights. It only affects the
model\u2019s configuration. Use `),WY=n(UX,"CODE",{});var OPr=s(WY);Llo=r(OPr,"from_pretrained()"),OPr.forEach(t),Blo=r(UX,"to load the model weights."),UX.forEach(t),klo=i(Gs),QY=n(Gs,"P",{});var XPr=s(QY);xlo=r(XPr,"Examples:"),XPr.forEach(t),Rlo=i(Gs),m(WM.$$.fragment,Gs),Gs.forEach(t),Slo=i(qs),xe=n(qs,"DIV",{class:!0});var Pt=s(xe);m(QM.$$.fragment,Pt),Plo=i(Pt),HY=n(Pt,"P",{});var zPr=s(HY);$lo=r(zPr,"Instantiate one of the model classes of the library (with a pretraining head) from a pretrained model."),zPr.forEach(t),Ilo=i(Pt),qa=n(Pt,"P",{});var i4=s(qa);jlo=r(i4,"The model class to instantiate is selected based on the "),UY=n(i4,"CODE",{});var VPr=s(UY);Nlo=r(VPr,"model_type"),VPr.forEach(t),Dlo=r(i4,` property of the config object (either
passed as an argument or loaded from `),JY=n(i4,"CODE",{});var WPr=s(JY);qlo=r(WPr,"pretrained_model_name_or_path"),WPr.forEach(t),Glo=r(i4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),YY=n(i4,"CODE",{});var QPr=s(YY);Olo=r(QPr,"pretrained_model_name_or_path"),QPr.forEach(t),Xlo=r(i4,":"),i4.forEach(t),zlo=i(Pt),x=n(Pt,"UL",{});var S=s(x);Zp=n(S,"LI",{});var XFe=s(Zp);KY=n(XFe,"STRONG",{});var HPr=s(KY);Vlo=r(HPr,"albert"),HPr.forEach(t),Wlo=r(XFe," \u2014 "),zR=n(XFe,"A",{href:!0});var UPr=s(zR);Qlo=r(UPr,"AlbertForPreTraining"),UPr.forEach(t),Hlo=r(XFe," (ALBERT model)"),XFe.forEach(t),Ulo=i(S),e_=n(S,"LI",{});var zFe=s(e_);ZY=n(zFe,"STRONG",{});var JPr=s(ZY);Jlo=r(JPr,"bart"),JPr.forEach(t),Ylo=r(zFe," \u2014 "),VR=n(zFe,"A",{href:!0});var YPr=s(VR);Klo=r(YPr,"BartForConditionalGeneration"),YPr.forEach(t),Zlo=r(zFe," (BART model)"),zFe.forEach(t),eio=i(S),o_=n(S,"LI",{});var VFe=s(o_);eK=n(VFe,"STRONG",{});var KPr=s(eK);oio=r(KPr,"bert"),KPr.forEach(t),rio=r(VFe," \u2014 "),WR=n(VFe,"A",{href:!0});var ZPr=s(WR);tio=r(ZPr,"BertForPreTraining"),ZPr.forEach(t),aio=r(VFe," (BERT model)"),VFe.forEach(t),nio=i(S),r_=n(S,"LI",{});var WFe=s(r_);oK=n(WFe,"STRONG",{});var e$r=s(oK);sio=r(e$r,"big_bird"),e$r.forEach(t),lio=r(WFe," \u2014 "),QR=n(WFe,"A",{href:!0});var o$r=s(QR);iio=r(o$r,"BigBirdForPreTraining"),o$r.forEach(t),dio=r(WFe," (BigBird model)"),WFe.forEach(t),cio=i(S),t_=n(S,"LI",{});var QFe=s(t_);rK=n(QFe,"STRONG",{});var r$r=s(rK);fio=r(r$r,"camembert"),r$r.forEach(t),mio=r(QFe," \u2014 "),HR=n(QFe,"A",{href:!0});var t$r=s(HR);gio=r(t$r,"CamembertForMaskedLM"),t$r.forEach(t),hio=r(QFe," (CamemBERT model)"),QFe.forEach(t),pio=i(S),a_=n(S,"LI",{});var HFe=s(a_);tK=n(HFe,"STRONG",{});var a$r=s(tK);_io=r(a$r,"ctrl"),a$r.forEach(t),uio=r(HFe," \u2014 "),UR=n(HFe,"A",{href:!0});var n$r=s(UR);bio=r(n$r,"CTRLLMHeadModel"),n$r.forEach(t),vio=r(HFe," (CTRL model)"),HFe.forEach(t),Tio=i(S),n_=n(S,"LI",{});var UFe=s(n_);aK=n(UFe,"STRONG",{});var s$r=s(aK);Fio=r(s$r,"deberta"),s$r.forEach(t),Cio=r(UFe," \u2014 "),JR=n(UFe,"A",{href:!0});var l$r=s(JR);Mio=r(l$r,"DebertaForMaskedLM"),l$r.forEach(t),Eio=r(UFe," (DeBERTa model)"),UFe.forEach(t),yio=i(S),s_=n(S,"LI",{});var JFe=s(s_);nK=n(JFe,"STRONG",{});var i$r=s(nK);wio=r(i$r,"deberta-v2"),i$r.forEach(t),Aio=r(JFe," \u2014 "),YR=n(JFe,"A",{href:!0});var d$r=s(YR);Lio=r(d$r,"DebertaV2ForMaskedLM"),d$r.forEach(t),Bio=r(JFe," (DeBERTa-v2 model)"),JFe.forEach(t),kio=i(S),l_=n(S,"LI",{});var YFe=s(l_);sK=n(YFe,"STRONG",{});var c$r=s(sK);xio=r(c$r,"distilbert"),c$r.forEach(t),Rio=r(YFe," \u2014 "),KR=n(YFe,"A",{href:!0});var f$r=s(KR);Sio=r(f$r,"DistilBertForMaskedLM"),f$r.forEach(t),Pio=r(YFe," (DistilBERT model)"),YFe.forEach(t),$io=i(S),i_=n(S,"LI",{});var KFe=s(i_);lK=n(KFe,"STRONG",{});var m$r=s(lK);Iio=r(m$r,"electra"),m$r.forEach(t),jio=r(KFe," \u2014 "),ZR=n(KFe,"A",{href:!0});var g$r=s(ZR);Nio=r(g$r,"ElectraForPreTraining"),g$r.forEach(t),Dio=r(KFe," (ELECTRA model)"),KFe.forEach(t),qio=i(S),d_=n(S,"LI",{});var ZFe=s(d_);iK=n(ZFe,"STRONG",{});var h$r=s(iK);Gio=r(h$r,"flaubert"),h$r.forEach(t),Oio=r(ZFe," \u2014 "),eS=n(ZFe,"A",{href:!0});var p$r=s(eS);Xio=r(p$r,"FlaubertWithLMHeadModel"),p$r.forEach(t),zio=r(ZFe," (FlauBERT model)"),ZFe.forEach(t),Vio=i(S),c_=n(S,"LI",{});var e9e=s(c_);dK=n(e9e,"STRONG",{});var _$r=s(dK);Wio=r(_$r,"fnet"),_$r.forEach(t),Qio=r(e9e," \u2014 "),oS=n(e9e,"A",{href:!0});var u$r=s(oS);Hio=r(u$r,"FNetForPreTraining"),u$r.forEach(t),Uio=r(e9e," (FNet model)"),e9e.forEach(t),Jio=i(S),f_=n(S,"LI",{});var o9e=s(f_);cK=n(o9e,"STRONG",{});var b$r=s(cK);Yio=r(b$r,"fsmt"),b$r.forEach(t),Kio=r(o9e," \u2014 "),rS=n(o9e,"A",{href:!0});var v$r=s(rS);Zio=r(v$r,"FSMTForConditionalGeneration"),v$r.forEach(t),edo=r(o9e," (FairSeq Machine-Translation model)"),o9e.forEach(t),odo=i(S),m_=n(S,"LI",{});var r9e=s(m_);fK=n(r9e,"STRONG",{});var T$r=s(fK);rdo=r(T$r,"funnel"),T$r.forEach(t),tdo=r(r9e," \u2014 "),tS=n(r9e,"A",{href:!0});var F$r=s(tS);ado=r(F$r,"FunnelForPreTraining"),F$r.forEach(t),ndo=r(r9e," (Funnel Transformer model)"),r9e.forEach(t),sdo=i(S),g_=n(S,"LI",{});var t9e=s(g_);mK=n(t9e,"STRONG",{});var C$r=s(mK);ldo=r(C$r,"gpt2"),C$r.forEach(t),ido=r(t9e," \u2014 "),aS=n(t9e,"A",{href:!0});var M$r=s(aS);ddo=r(M$r,"GPT2LMHeadModel"),M$r.forEach(t),cdo=r(t9e," (OpenAI GPT-2 model)"),t9e.forEach(t),fdo=i(S),h_=n(S,"LI",{});var a9e=s(h_);gK=n(a9e,"STRONG",{});var E$r=s(gK);mdo=r(E$r,"ibert"),E$r.forEach(t),gdo=r(a9e," \u2014 "),nS=n(a9e,"A",{href:!0});var y$r=s(nS);hdo=r(y$r,"IBertForMaskedLM"),y$r.forEach(t),pdo=r(a9e," (I-BERT model)"),a9e.forEach(t),_do=i(S),p_=n(S,"LI",{});var n9e=s(p_);hK=n(n9e,"STRONG",{});var w$r=s(hK);udo=r(w$r,"layoutlm"),w$r.forEach(t),bdo=r(n9e," \u2014 "),sS=n(n9e,"A",{href:!0});var A$r=s(sS);vdo=r(A$r,"LayoutLMForMaskedLM"),A$r.forEach(t),Tdo=r(n9e," (LayoutLM model)"),n9e.forEach(t),Fdo=i(S),__=n(S,"LI",{});var s9e=s(__);pK=n(s9e,"STRONG",{});var L$r=s(pK);Cdo=r(L$r,"longformer"),L$r.forEach(t),Mdo=r(s9e," \u2014 "),lS=n(s9e,"A",{href:!0});var B$r=s(lS);Edo=r(B$r,"LongformerForMaskedLM"),B$r.forEach(t),ydo=r(s9e," (Longformer model)"),s9e.forEach(t),wdo=i(S),u_=n(S,"LI",{});var l9e=s(u_);_K=n(l9e,"STRONG",{});var k$r=s(_K);Ado=r(k$r,"lxmert"),k$r.forEach(t),Ldo=r(l9e," \u2014 "),iS=n(l9e,"A",{href:!0});var x$r=s(iS);Bdo=r(x$r,"LxmertForPreTraining"),x$r.forEach(t),kdo=r(l9e," (LXMERT model)"),l9e.forEach(t),xdo=i(S),b_=n(S,"LI",{});var i9e=s(b_);uK=n(i9e,"STRONG",{});var R$r=s(uK);Rdo=r(R$r,"megatron-bert"),R$r.forEach(t),Sdo=r(i9e," \u2014 "),dS=n(i9e,"A",{href:!0});var S$r=s(dS);Pdo=r(S$r,"MegatronBertForPreTraining"),S$r.forEach(t),$do=r(i9e," (MegatronBert model)"),i9e.forEach(t),Ido=i(S),v_=n(S,"LI",{});var d9e=s(v_);bK=n(d9e,"STRONG",{});var P$r=s(bK);jdo=r(P$r,"mobilebert"),P$r.forEach(t),Ndo=r(d9e," \u2014 "),cS=n(d9e,"A",{href:!0});var $$r=s(cS);Ddo=r($$r,"MobileBertForPreTraining"),$$r.forEach(t),qdo=r(d9e," (MobileBERT model)"),d9e.forEach(t),Gdo=i(S),T_=n(S,"LI",{});var c9e=s(T_);vK=n(c9e,"STRONG",{});var I$r=s(vK);Odo=r(I$r,"mpnet"),I$r.forEach(t),Xdo=r(c9e," \u2014 "),fS=n(c9e,"A",{href:!0});var j$r=s(fS);zdo=r(j$r,"MPNetForMaskedLM"),j$r.forEach(t),Vdo=r(c9e," (MPNet model)"),c9e.forEach(t),Wdo=i(S),F_=n(S,"LI",{});var f9e=s(F_);TK=n(f9e,"STRONG",{});var N$r=s(TK);Qdo=r(N$r,"openai-gpt"),N$r.forEach(t),Hdo=r(f9e," \u2014 "),mS=n(f9e,"A",{href:!0});var D$r=s(mS);Udo=r(D$r,"OpenAIGPTLMHeadModel"),D$r.forEach(t),Jdo=r(f9e," (OpenAI GPT model)"),f9e.forEach(t),Ydo=i(S),C_=n(S,"LI",{});var m9e=s(C_);FK=n(m9e,"STRONG",{});var q$r=s(FK);Kdo=r(q$r,"retribert"),q$r.forEach(t),Zdo=r(m9e," \u2014 "),gS=n(m9e,"A",{href:!0});var G$r=s(gS);eco=r(G$r,"RetriBertModel"),G$r.forEach(t),oco=r(m9e," (RetriBERT model)"),m9e.forEach(t),rco=i(S),M_=n(S,"LI",{});var g9e=s(M_);CK=n(g9e,"STRONG",{});var O$r=s(CK);tco=r(O$r,"roberta"),O$r.forEach(t),aco=r(g9e," \u2014 "),hS=n(g9e,"A",{href:!0});var X$r=s(hS);nco=r(X$r,"RobertaForMaskedLM"),X$r.forEach(t),sco=r(g9e," (RoBERTa model)"),g9e.forEach(t),lco=i(S),E_=n(S,"LI",{});var h9e=s(E_);MK=n(h9e,"STRONG",{});var z$r=s(MK);ico=r(z$r,"squeezebert"),z$r.forEach(t),dco=r(h9e," \u2014 "),pS=n(h9e,"A",{href:!0});var V$r=s(pS);cco=r(V$r,"SqueezeBertForMaskedLM"),V$r.forEach(t),fco=r(h9e," (SqueezeBERT model)"),h9e.forEach(t),mco=i(S),y_=n(S,"LI",{});var p9e=s(y_);EK=n(p9e,"STRONG",{});var W$r=s(EK);gco=r(W$r,"t5"),W$r.forEach(t),hco=r(p9e," \u2014 "),_S=n(p9e,"A",{href:!0});var Q$r=s(_S);pco=r(Q$r,"T5ForConditionalGeneration"),Q$r.forEach(t),_co=r(p9e," (T5 model)"),p9e.forEach(t),uco=i(S),w_=n(S,"LI",{});var _9e=s(w_);yK=n(_9e,"STRONG",{});var H$r=s(yK);bco=r(H$r,"tapas"),H$r.forEach(t),vco=r(_9e," \u2014 "),uS=n(_9e,"A",{href:!0});var U$r=s(uS);Tco=r(U$r,"TapasForMaskedLM"),U$r.forEach(t),Fco=r(_9e," (TAPAS model)"),_9e.forEach(t),Cco=i(S),A_=n(S,"LI",{});var u9e=s(A_);wK=n(u9e,"STRONG",{});var J$r=s(wK);Mco=r(J$r,"transfo-xl"),J$r.forEach(t),Eco=r(u9e," \u2014 "),bS=n(u9e,"A",{href:!0});var Y$r=s(bS);yco=r(Y$r,"TransfoXLLMHeadModel"),Y$r.forEach(t),wco=r(u9e," (Transformer-XL model)"),u9e.forEach(t),Aco=i(S),L_=n(S,"LI",{});var b9e=s(L_);AK=n(b9e,"STRONG",{});var K$r=s(AK);Lco=r(K$r,"unispeech"),K$r.forEach(t),Bco=r(b9e," \u2014 "),vS=n(b9e,"A",{href:!0});var Z$r=s(vS);kco=r(Z$r,"UniSpeechForPreTraining"),Z$r.forEach(t),xco=r(b9e," (UniSpeech model)"),b9e.forEach(t),Rco=i(S),B_=n(S,"LI",{});var v9e=s(B_);LK=n(v9e,"STRONG",{});var eIr=s(LK);Sco=r(eIr,"unispeech-sat"),eIr.forEach(t),Pco=r(v9e," \u2014 "),TS=n(v9e,"A",{href:!0});var oIr=s(TS);$co=r(oIr,"UniSpeechSatForPreTraining"),oIr.forEach(t),Ico=r(v9e," (UniSpeechSat model)"),v9e.forEach(t),jco=i(S),k_=n(S,"LI",{});var T9e=s(k_);BK=n(T9e,"STRONG",{});var rIr=s(BK);Nco=r(rIr,"visual_bert"),rIr.forEach(t),Dco=r(T9e," \u2014 "),FS=n(T9e,"A",{href:!0});var tIr=s(FS);qco=r(tIr,"VisualBertForPreTraining"),tIr.forEach(t),Gco=r(T9e," (VisualBert model)"),T9e.forEach(t),Oco=i(S),x_=n(S,"LI",{});var F9e=s(x_);kK=n(F9e,"STRONG",{});var aIr=s(kK);Xco=r(aIr,"vit_mae"),aIr.forEach(t),zco=r(F9e," \u2014 "),CS=n(F9e,"A",{href:!0});var nIr=s(CS);Vco=r(nIr,"ViTMAEForPreTraining"),nIr.forEach(t),Wco=r(F9e," (ViTMAE model)"),F9e.forEach(t),Qco=i(S),R_=n(S,"LI",{});var C9e=s(R_);xK=n(C9e,"STRONG",{});var sIr=s(xK);Hco=r(sIr,"wav2vec2"),sIr.forEach(t),Uco=r(C9e," \u2014 "),MS=n(C9e,"A",{href:!0});var lIr=s(MS);Jco=r(lIr,"Wav2Vec2ForPreTraining"),lIr.forEach(t),Yco=r(C9e," (Wav2Vec2 model)"),C9e.forEach(t),Kco=i(S),S_=n(S,"LI",{});var M9e=s(S_);RK=n(M9e,"STRONG",{});var iIr=s(RK);Zco=r(iIr,"xlm"),iIr.forEach(t),efo=r(M9e," \u2014 "),ES=n(M9e,"A",{href:!0});var dIr=s(ES);ofo=r(dIr,"XLMWithLMHeadModel"),dIr.forEach(t),rfo=r(M9e," (XLM model)"),M9e.forEach(t),tfo=i(S),P_=n(S,"LI",{});var E9e=s(P_);SK=n(E9e,"STRONG",{});var cIr=s(SK);afo=r(cIr,"xlm-roberta"),cIr.forEach(t),nfo=r(E9e," \u2014 "),yS=n(E9e,"A",{href:!0});var fIr=s(yS);sfo=r(fIr,"XLMRobertaForMaskedLM"),fIr.forEach(t),lfo=r(E9e," (XLM-RoBERTa model)"),E9e.forEach(t),ifo=i(S),$_=n(S,"LI",{});var y9e=s($_);PK=n(y9e,"STRONG",{});var mIr=s(PK);dfo=r(mIr,"xlm-roberta-xl"),mIr.forEach(t),cfo=r(y9e," \u2014 "),wS=n(y9e,"A",{href:!0});var gIr=s(wS);ffo=r(gIr,"XLMRobertaXLForMaskedLM"),gIr.forEach(t),mfo=r(y9e," (XLM-RoBERTa-XL model)"),y9e.forEach(t),gfo=i(S),I_=n(S,"LI",{});var w9e=s(I_);$K=n(w9e,"STRONG",{});var hIr=s($K);hfo=r(hIr,"xlnet"),hIr.forEach(t),pfo=r(w9e," \u2014 "),AS=n(w9e,"A",{href:!0});var pIr=s(AS);_fo=r(pIr,"XLNetLMHeadModel"),pIr.forEach(t),ufo=r(w9e," (XLNet model)"),w9e.forEach(t),S.forEach(t),bfo=i(Pt),j_=n(Pt,"P",{});var A9e=s(j_);vfo=r(A9e,"The model is set in evaluation mode by default using "),IK=n(A9e,"CODE",{});var _Ir=s(IK);Tfo=r(_Ir,"model.eval()"),_Ir.forEach(t),Ffo=r(A9e,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),jK=n(A9e,"CODE",{});var uIr=s(jK);Cfo=r(uIr,"model.train()"),uIr.forEach(t),A9e.forEach(t),Mfo=i(Pt),NK=n(Pt,"P",{});var bIr=s(NK);Efo=r(bIr,"Examples:"),bIr.forEach(t),yfo=i(Pt),m(HM.$$.fragment,Pt),Pt.forEach(t),qs.forEach(t),TLe=i(d),Wi=n(d,"H2",{class:!0});var ABe=s(Wi);N_=n(ABe,"A",{id:!0,class:!0,href:!0});var vIr=s(N_);DK=n(vIr,"SPAN",{});var TIr=s(DK);m(UM.$$.fragment,TIr),TIr.forEach(t),vIr.forEach(t),wfo=i(ABe),qK=n(ABe,"SPAN",{});var FIr=s(qK);Afo=r(FIr,"AutoModelForCausalLM"),FIr.forEach(t),ABe.forEach(t),FLe=i(d),Qo=n(d,"DIV",{class:!0});var Os=s(Qo);m(JM.$$.fragment,Os),Lfo=i(Os),Qi=n(Os,"P",{});var JX=s(Qi);Bfo=r(JX,`This is a generic model class that will be instantiated as one of the model classes of the library (with a causal language modeling head) when created
with the `),GK=n(JX,"CODE",{});var CIr=s(GK);kfo=r(CIr,"from_pretrained()"),CIr.forEach(t),xfo=r(JX,"class method or the "),OK=n(JX,"CODE",{});var MIr=s(OK);Rfo=r(MIr,"from_config()"),MIr.forEach(t),Sfo=r(JX,`class
method.`),JX.forEach(t),Pfo=i(Os),YM=n(Os,"P",{});var LBe=s(YM);$fo=r(LBe,"This class cannot be instantiated directly using "),XK=n(LBe,"CODE",{});var EIr=s(XK);Ifo=r(EIr,"__init__()"),EIr.forEach(t),jfo=r(LBe," (throws an error)."),LBe.forEach(t),Nfo=i(Os),qr=n(Os,"DIV",{class:!0});var Xs=s(qr);m(KM.$$.fragment,Xs),Dfo=i(Xs),zK=n(Xs,"P",{});var yIr=s(zK);qfo=r(yIr,"Instantiates one of the model classes of the library (with a causal language modeling head) from a configuration."),yIr.forEach(t),Gfo=i(Xs),Hi=n(Xs,"P",{});var YX=s(Hi);Ofo=r(YX,`Note:
Loading a model from its configuration file does `),VK=n(YX,"STRONG",{});var wIr=s(VK);Xfo=r(wIr,"not"),wIr.forEach(t),zfo=r(YX,` load the model weights. It only affects the
model\u2019s configuration. Use `),WK=n(YX,"CODE",{});var AIr=s(WK);Vfo=r(AIr,"from_pretrained()"),AIr.forEach(t),Wfo=r(YX,"to load the model weights."),YX.forEach(t),Qfo=i(Xs),QK=n(Xs,"P",{});var LIr=s(QK);Hfo=r(LIr,"Examples:"),LIr.forEach(t),Ufo=i(Xs),m(ZM.$$.fragment,Xs),Xs.forEach(t),Jfo=i(Os),Re=n(Os,"DIV",{class:!0});var $t=s(Re);m(eE.$$.fragment,$t),Yfo=i($t),HK=n($t,"P",{});var BIr=s(HK);Kfo=r(BIr,"Instantiate one of the model classes of the library (with a causal language modeling head) from a pretrained model."),BIr.forEach(t),Zfo=i($t),Ga=n($t,"P",{});var d4=s(Ga);emo=r(d4,"The model class to instantiate is selected based on the "),UK=n(d4,"CODE",{});var kIr=s(UK);omo=r(kIr,"model_type"),kIr.forEach(t),rmo=r(d4,` property of the config object (either
passed as an argument or loaded from `),JK=n(d4,"CODE",{});var xIr=s(JK);tmo=r(xIr,"pretrained_model_name_or_path"),xIr.forEach(t),amo=r(d4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),YK=n(d4,"CODE",{});var RIr=s(YK);nmo=r(RIr,"pretrained_model_name_or_path"),RIr.forEach(t),smo=r(d4,":"),d4.forEach(t),lmo=i($t),$=n($t,"UL",{});var j=s($);D_=n(j,"LI",{});var L9e=s(D_);KK=n(L9e,"STRONG",{});var SIr=s(KK);imo=r(SIr,"bart"),SIr.forEach(t),dmo=r(L9e," \u2014 "),LS=n(L9e,"A",{href:!0});var PIr=s(LS);cmo=r(PIr,"BartForCausalLM"),PIr.forEach(t),fmo=r(L9e," (BART model)"),L9e.forEach(t),mmo=i(j),q_=n(j,"LI",{});var B9e=s(q_);ZK=n(B9e,"STRONG",{});var $Ir=s(ZK);gmo=r($Ir,"bert"),$Ir.forEach(t),hmo=r(B9e," \u2014 "),BS=n(B9e,"A",{href:!0});var IIr=s(BS);pmo=r(IIr,"BertLMHeadModel"),IIr.forEach(t),_mo=r(B9e," (BERT model)"),B9e.forEach(t),umo=i(j),G_=n(j,"LI",{});var k9e=s(G_);eZ=n(k9e,"STRONG",{});var jIr=s(eZ);bmo=r(jIr,"bert-generation"),jIr.forEach(t),vmo=r(k9e," \u2014 "),kS=n(k9e,"A",{href:!0});var NIr=s(kS);Tmo=r(NIr,"BertGenerationDecoder"),NIr.forEach(t),Fmo=r(k9e," (Bert Generation model)"),k9e.forEach(t),Cmo=i(j),O_=n(j,"LI",{});var x9e=s(O_);oZ=n(x9e,"STRONG",{});var DIr=s(oZ);Mmo=r(DIr,"big_bird"),DIr.forEach(t),Emo=r(x9e," \u2014 "),xS=n(x9e,"A",{href:!0});var qIr=s(xS);ymo=r(qIr,"BigBirdForCausalLM"),qIr.forEach(t),wmo=r(x9e," (BigBird model)"),x9e.forEach(t),Amo=i(j),X_=n(j,"LI",{});var R9e=s(X_);rZ=n(R9e,"STRONG",{});var GIr=s(rZ);Lmo=r(GIr,"bigbird_pegasus"),GIr.forEach(t),Bmo=r(R9e," \u2014 "),RS=n(R9e,"A",{href:!0});var OIr=s(RS);kmo=r(OIr,"BigBirdPegasusForCausalLM"),OIr.forEach(t),xmo=r(R9e," (BigBirdPegasus model)"),R9e.forEach(t),Rmo=i(j),z_=n(j,"LI",{});var S9e=s(z_);tZ=n(S9e,"STRONG",{});var XIr=s(tZ);Smo=r(XIr,"blenderbot"),XIr.forEach(t),Pmo=r(S9e," \u2014 "),SS=n(S9e,"A",{href:!0});var zIr=s(SS);$mo=r(zIr,"BlenderbotForCausalLM"),zIr.forEach(t),Imo=r(S9e," (Blenderbot model)"),S9e.forEach(t),jmo=i(j),V_=n(j,"LI",{});var P9e=s(V_);aZ=n(P9e,"STRONG",{});var VIr=s(aZ);Nmo=r(VIr,"blenderbot-small"),VIr.forEach(t),Dmo=r(P9e," \u2014 "),PS=n(P9e,"A",{href:!0});var WIr=s(PS);qmo=r(WIr,"BlenderbotSmallForCausalLM"),WIr.forEach(t),Gmo=r(P9e," (BlenderbotSmall model)"),P9e.forEach(t),Omo=i(j),W_=n(j,"LI",{});var $9e=s(W_);nZ=n($9e,"STRONG",{});var QIr=s(nZ);Xmo=r(QIr,"camembert"),QIr.forEach(t),zmo=r($9e," \u2014 "),$S=n($9e,"A",{href:!0});var HIr=s($S);Vmo=r(HIr,"CamembertForCausalLM"),HIr.forEach(t),Wmo=r($9e," (CamemBERT model)"),$9e.forEach(t),Qmo=i(j),Q_=n(j,"LI",{});var I9e=s(Q_);sZ=n(I9e,"STRONG",{});var UIr=s(sZ);Hmo=r(UIr,"ctrl"),UIr.forEach(t),Umo=r(I9e," \u2014 "),IS=n(I9e,"A",{href:!0});var JIr=s(IS);Jmo=r(JIr,"CTRLLMHeadModel"),JIr.forEach(t),Ymo=r(I9e," (CTRL model)"),I9e.forEach(t),Kmo=i(j),H_=n(j,"LI",{});var j9e=s(H_);lZ=n(j9e,"STRONG",{});var YIr=s(lZ);Zmo=r(YIr,"electra"),YIr.forEach(t),ego=r(j9e," \u2014 "),jS=n(j9e,"A",{href:!0});var KIr=s(jS);ogo=r(KIr,"ElectraForCausalLM"),KIr.forEach(t),rgo=r(j9e," (ELECTRA model)"),j9e.forEach(t),tgo=i(j),U_=n(j,"LI",{});var N9e=s(U_);iZ=n(N9e,"STRONG",{});var ZIr=s(iZ);ago=r(ZIr,"gpt2"),ZIr.forEach(t),ngo=r(N9e," \u2014 "),NS=n(N9e,"A",{href:!0});var ejr=s(NS);sgo=r(ejr,"GPT2LMHeadModel"),ejr.forEach(t),lgo=r(N9e," (OpenAI GPT-2 model)"),N9e.forEach(t),igo=i(j),J_=n(j,"LI",{});var D9e=s(J_);dZ=n(D9e,"STRONG",{});var ojr=s(dZ);dgo=r(ojr,"gpt_neo"),ojr.forEach(t),cgo=r(D9e," \u2014 "),DS=n(D9e,"A",{href:!0});var rjr=s(DS);fgo=r(rjr,"GPTNeoForCausalLM"),rjr.forEach(t),mgo=r(D9e," (GPT Neo model)"),D9e.forEach(t),ggo=i(j),Y_=n(j,"LI",{});var q9e=s(Y_);cZ=n(q9e,"STRONG",{});var tjr=s(cZ);hgo=r(tjr,"gptj"),tjr.forEach(t),pgo=r(q9e," \u2014 "),qS=n(q9e,"A",{href:!0});var ajr=s(qS);_go=r(ajr,"GPTJForCausalLM"),ajr.forEach(t),ugo=r(q9e," (GPT-J model)"),q9e.forEach(t),bgo=i(j),K_=n(j,"LI",{});var G9e=s(K_);fZ=n(G9e,"STRONG",{});var njr=s(fZ);vgo=r(njr,"marian"),njr.forEach(t),Tgo=r(G9e," \u2014 "),GS=n(G9e,"A",{href:!0});var sjr=s(GS);Fgo=r(sjr,"MarianForCausalLM"),sjr.forEach(t),Cgo=r(G9e," (Marian model)"),G9e.forEach(t),Mgo=i(j),Z_=n(j,"LI",{});var O9e=s(Z_);mZ=n(O9e,"STRONG",{});var ljr=s(mZ);Ego=r(ljr,"mbart"),ljr.forEach(t),ygo=r(O9e," \u2014 "),OS=n(O9e,"A",{href:!0});var ijr=s(OS);wgo=r(ijr,"MBartForCausalLM"),ijr.forEach(t),Ago=r(O9e," (mBART model)"),O9e.forEach(t),Lgo=i(j),eu=n(j,"LI",{});var X9e=s(eu);gZ=n(X9e,"STRONG",{});var djr=s(gZ);Bgo=r(djr,"megatron-bert"),djr.forEach(t),kgo=r(X9e," \u2014 "),XS=n(X9e,"A",{href:!0});var cjr=s(XS);xgo=r(cjr,"MegatronBertForCausalLM"),cjr.forEach(t),Rgo=r(X9e," (MegatronBert model)"),X9e.forEach(t),Sgo=i(j),ou=n(j,"LI",{});var z9e=s(ou);hZ=n(z9e,"STRONG",{});var fjr=s(hZ);Pgo=r(fjr,"openai-gpt"),fjr.forEach(t),$go=r(z9e," \u2014 "),zS=n(z9e,"A",{href:!0});var mjr=s(zS);Igo=r(mjr,"OpenAIGPTLMHeadModel"),mjr.forEach(t),jgo=r(z9e," (OpenAI GPT model)"),z9e.forEach(t),Ngo=i(j),ru=n(j,"LI",{});var V9e=s(ru);pZ=n(V9e,"STRONG",{});var gjr=s(pZ);Dgo=r(gjr,"pegasus"),gjr.forEach(t),qgo=r(V9e," \u2014 "),VS=n(V9e,"A",{href:!0});var hjr=s(VS);Ggo=r(hjr,"PegasusForCausalLM"),hjr.forEach(t),Ogo=r(V9e," (Pegasus model)"),V9e.forEach(t),Xgo=i(j),tu=n(j,"LI",{});var W9e=s(tu);_Z=n(W9e,"STRONG",{});var pjr=s(_Z);zgo=r(pjr,"plbart"),pjr.forEach(t),Vgo=r(W9e," \u2014 "),WS=n(W9e,"A",{href:!0});var _jr=s(WS);Wgo=r(_jr,"PLBartForCausalLM"),_jr.forEach(t),Qgo=r(W9e," (PLBart model)"),W9e.forEach(t),Hgo=i(j),au=n(j,"LI",{});var Q9e=s(au);uZ=n(Q9e,"STRONG",{});var ujr=s(uZ);Ugo=r(ujr,"prophetnet"),ujr.forEach(t),Jgo=r(Q9e," \u2014 "),QS=n(Q9e,"A",{href:!0});var bjr=s(QS);Ygo=r(bjr,"ProphetNetForCausalLM"),bjr.forEach(t),Kgo=r(Q9e," (ProphetNet model)"),Q9e.forEach(t),Zgo=i(j),nu=n(j,"LI",{});var H9e=s(nu);bZ=n(H9e,"STRONG",{});var vjr=s(bZ);eho=r(vjr,"qdqbert"),vjr.forEach(t),oho=r(H9e," \u2014 "),HS=n(H9e,"A",{href:!0});var Tjr=s(HS);rho=r(Tjr,"QDQBertLMHeadModel"),Tjr.forEach(t),tho=r(H9e," (QDQBert model)"),H9e.forEach(t),aho=i(j),su=n(j,"LI",{});var U9e=s(su);vZ=n(U9e,"STRONG",{});var Fjr=s(vZ);nho=r(Fjr,"reformer"),Fjr.forEach(t),sho=r(U9e," \u2014 "),US=n(U9e,"A",{href:!0});var Cjr=s(US);lho=r(Cjr,"ReformerModelWithLMHead"),Cjr.forEach(t),iho=r(U9e," (Reformer model)"),U9e.forEach(t),dho=i(j),lu=n(j,"LI",{});var J9e=s(lu);TZ=n(J9e,"STRONG",{});var Mjr=s(TZ);cho=r(Mjr,"rembert"),Mjr.forEach(t),fho=r(J9e," \u2014 "),JS=n(J9e,"A",{href:!0});var Ejr=s(JS);mho=r(Ejr,"RemBertForCausalLM"),Ejr.forEach(t),gho=r(J9e," (RemBERT model)"),J9e.forEach(t),hho=i(j),iu=n(j,"LI",{});var Y9e=s(iu);FZ=n(Y9e,"STRONG",{});var yjr=s(FZ);pho=r(yjr,"roberta"),yjr.forEach(t),_ho=r(Y9e," \u2014 "),YS=n(Y9e,"A",{href:!0});var wjr=s(YS);uho=r(wjr,"RobertaForCausalLM"),wjr.forEach(t),bho=r(Y9e," (RoBERTa model)"),Y9e.forEach(t),vho=i(j),du=n(j,"LI",{});var K9e=s(du);CZ=n(K9e,"STRONG",{});var Ajr=s(CZ);Tho=r(Ajr,"roformer"),Ajr.forEach(t),Fho=r(K9e," \u2014 "),KS=n(K9e,"A",{href:!0});var Ljr=s(KS);Cho=r(Ljr,"RoFormerForCausalLM"),Ljr.forEach(t),Mho=r(K9e," (RoFormer model)"),K9e.forEach(t),Eho=i(j),cu=n(j,"LI",{});var Z9e=s(cu);MZ=n(Z9e,"STRONG",{});var Bjr=s(MZ);yho=r(Bjr,"speech_to_text_2"),Bjr.forEach(t),who=r(Z9e," \u2014 "),ZS=n(Z9e,"A",{href:!0});var kjr=s(ZS);Aho=r(kjr,"Speech2Text2ForCausalLM"),kjr.forEach(t),Lho=r(Z9e," (Speech2Text2 model)"),Z9e.forEach(t),Bho=i(j),fu=n(j,"LI",{});var eCe=s(fu);EZ=n(eCe,"STRONG",{});var xjr=s(EZ);kho=r(xjr,"transfo-xl"),xjr.forEach(t),xho=r(eCe," \u2014 "),eP=n(eCe,"A",{href:!0});var Rjr=s(eP);Rho=r(Rjr,"TransfoXLLMHeadModel"),Rjr.forEach(t),Sho=r(eCe," (Transformer-XL model)"),eCe.forEach(t),Pho=i(j),mu=n(j,"LI",{});var oCe=s(mu);yZ=n(oCe,"STRONG",{});var Sjr=s(yZ);$ho=r(Sjr,"trocr"),Sjr.forEach(t),Iho=r(oCe," \u2014 "),oP=n(oCe,"A",{href:!0});var Pjr=s(oP);jho=r(Pjr,"TrOCRForCausalLM"),Pjr.forEach(t),Nho=r(oCe," (TrOCR model)"),oCe.forEach(t),Dho=i(j),gu=n(j,"LI",{});var rCe=s(gu);wZ=n(rCe,"STRONG",{});var $jr=s(wZ);qho=r($jr,"xglm"),$jr.forEach(t),Gho=r(rCe," \u2014 "),rP=n(rCe,"A",{href:!0});var Ijr=s(rP);Oho=r(Ijr,"XGLMForCausalLM"),Ijr.forEach(t),Xho=r(rCe," (XGLM model)"),rCe.forEach(t),zho=i(j),hu=n(j,"LI",{});var tCe=s(hu);AZ=n(tCe,"STRONG",{});var jjr=s(AZ);Vho=r(jjr,"xlm"),jjr.forEach(t),Who=r(tCe," \u2014 "),tP=n(tCe,"A",{href:!0});var Njr=s(tP);Qho=r(Njr,"XLMWithLMHeadModel"),Njr.forEach(t),Hho=r(tCe," (XLM model)"),tCe.forEach(t),Uho=i(j),pu=n(j,"LI",{});var aCe=s(pu);LZ=n(aCe,"STRONG",{});var Djr=s(LZ);Jho=r(Djr,"xlm-prophetnet"),Djr.forEach(t),Yho=r(aCe," \u2014 "),aP=n(aCe,"A",{href:!0});var qjr=s(aP);Kho=r(qjr,"XLMProphetNetForCausalLM"),qjr.forEach(t),Zho=r(aCe," (XLMProphetNet model)"),aCe.forEach(t),epo=i(j),_u=n(j,"LI",{});var nCe=s(_u);BZ=n(nCe,"STRONG",{});var Gjr=s(BZ);opo=r(Gjr,"xlm-roberta"),Gjr.forEach(t),rpo=r(nCe," \u2014 "),nP=n(nCe,"A",{href:!0});var Ojr=s(nP);tpo=r(Ojr,"XLMRobertaForCausalLM"),Ojr.forEach(t),apo=r(nCe," (XLM-RoBERTa model)"),nCe.forEach(t),npo=i(j),uu=n(j,"LI",{});var sCe=s(uu);kZ=n(sCe,"STRONG",{});var Xjr=s(kZ);spo=r(Xjr,"xlm-roberta-xl"),Xjr.forEach(t),lpo=r(sCe," \u2014 "),sP=n(sCe,"A",{href:!0});var zjr=s(sP);ipo=r(zjr,"XLMRobertaXLForCausalLM"),zjr.forEach(t),dpo=r(sCe," (XLM-RoBERTa-XL model)"),sCe.forEach(t),cpo=i(j),bu=n(j,"LI",{});var lCe=s(bu);xZ=n(lCe,"STRONG",{});var Vjr=s(xZ);fpo=r(Vjr,"xlnet"),Vjr.forEach(t),mpo=r(lCe," \u2014 "),lP=n(lCe,"A",{href:!0});var Wjr=s(lP);gpo=r(Wjr,"XLNetLMHeadModel"),Wjr.forEach(t),hpo=r(lCe," (XLNet model)"),lCe.forEach(t),j.forEach(t),ppo=i($t),vu=n($t,"P",{});var iCe=s(vu);_po=r(iCe,"The model is set in evaluation mode by default using "),RZ=n(iCe,"CODE",{});var Qjr=s(RZ);upo=r(Qjr,"model.eval()"),Qjr.forEach(t),bpo=r(iCe,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),SZ=n(iCe,"CODE",{});var Hjr=s(SZ);vpo=r(Hjr,"model.train()"),Hjr.forEach(t),iCe.forEach(t),Tpo=i($t),PZ=n($t,"P",{});var Ujr=s(PZ);Fpo=r(Ujr,"Examples:"),Ujr.forEach(t),Cpo=i($t),m(oE.$$.fragment,$t),$t.forEach(t),Os.forEach(t),CLe=i(d),Ui=n(d,"H2",{class:!0});var BBe=s(Ui);Tu=n(BBe,"A",{id:!0,class:!0,href:!0});var Jjr=s(Tu);$Z=n(Jjr,"SPAN",{});var Yjr=s($Z);m(rE.$$.fragment,Yjr),Yjr.forEach(t),Jjr.forEach(t),Mpo=i(BBe),IZ=n(BBe,"SPAN",{});var Kjr=s(IZ);Epo=r(Kjr,"AutoModelForMaskedLM"),Kjr.forEach(t),BBe.forEach(t),MLe=i(d),Ho=n(d,"DIV",{class:!0});var zs=s(Ho);m(tE.$$.fragment,zs),ypo=i(zs),Ji=n(zs,"P",{});var KX=s(Ji);wpo=r(KX,`This is a generic model class that will be instantiated as one of the model classes of the library (with a masked language modeling head) when created
with the `),jZ=n(KX,"CODE",{});var Zjr=s(jZ);Apo=r(Zjr,"from_pretrained()"),Zjr.forEach(t),Lpo=r(KX,"class method or the "),NZ=n(KX,"CODE",{});var eNr=s(NZ);Bpo=r(eNr,"from_config()"),eNr.forEach(t),kpo=r(KX,`class
method.`),KX.forEach(t),xpo=i(zs),aE=n(zs,"P",{});var kBe=s(aE);Rpo=r(kBe,"This class cannot be instantiated directly using "),DZ=n(kBe,"CODE",{});var oNr=s(DZ);Spo=r(oNr,"__init__()"),oNr.forEach(t),Ppo=r(kBe," (throws an error)."),kBe.forEach(t),$po=i(zs),Gr=n(zs,"DIV",{class:!0});var Vs=s(Gr);m(nE.$$.fragment,Vs),Ipo=i(Vs),qZ=n(Vs,"P",{});var rNr=s(qZ);jpo=r(rNr,"Instantiates one of the model classes of the library (with a masked language modeling head) from a configuration."),rNr.forEach(t),Npo=i(Vs),Yi=n(Vs,"P",{});var ZX=s(Yi);Dpo=r(ZX,`Note:
Loading a model from its configuration file does `),GZ=n(ZX,"STRONG",{});var tNr=s(GZ);qpo=r(tNr,"not"),tNr.forEach(t),Gpo=r(ZX,` load the model weights. It only affects the
model\u2019s configuration. Use `),OZ=n(ZX,"CODE",{});var aNr=s(OZ);Opo=r(aNr,"from_pretrained()"),aNr.forEach(t),Xpo=r(ZX,"to load the model weights."),ZX.forEach(t),zpo=i(Vs),XZ=n(Vs,"P",{});var nNr=s(XZ);Vpo=r(nNr,"Examples:"),nNr.forEach(t),Wpo=i(Vs),m(sE.$$.fragment,Vs),Vs.forEach(t),Qpo=i(zs),Se=n(zs,"DIV",{class:!0});var It=s(Se);m(lE.$$.fragment,It),Hpo=i(It),zZ=n(It,"P",{});var sNr=s(zZ);Upo=r(sNr,"Instantiate one of the model classes of the library (with a masked language modeling head) from a pretrained model."),sNr.forEach(t),Jpo=i(It),Oa=n(It,"P",{});var c4=s(Oa);Ypo=r(c4,"The model class to instantiate is selected based on the "),VZ=n(c4,"CODE",{});var lNr=s(VZ);Kpo=r(lNr,"model_type"),lNr.forEach(t),Zpo=r(c4,` property of the config object (either
passed as an argument or loaded from `),WZ=n(c4,"CODE",{});var iNr=s(WZ);e_o=r(iNr,"pretrained_model_name_or_path"),iNr.forEach(t),o_o=r(c4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),QZ=n(c4,"CODE",{});var dNr=s(QZ);r_o=r(dNr,"pretrained_model_name_or_path"),dNr.forEach(t),t_o=r(c4,":"),c4.forEach(t),a_o=i(It),I=n(It,"UL",{});var N=s(I);Fu=n(N,"LI",{});var dCe=s(Fu);HZ=n(dCe,"STRONG",{});var cNr=s(HZ);n_o=r(cNr,"albert"),cNr.forEach(t),s_o=r(dCe," \u2014 "),iP=n(dCe,"A",{href:!0});var fNr=s(iP);l_o=r(fNr,"AlbertForMaskedLM"),fNr.forEach(t),i_o=r(dCe," (ALBERT model)"),dCe.forEach(t),d_o=i(N),Cu=n(N,"LI",{});var cCe=s(Cu);UZ=n(cCe,"STRONG",{});var mNr=s(UZ);c_o=r(mNr,"bart"),mNr.forEach(t),f_o=r(cCe," \u2014 "),dP=n(cCe,"A",{href:!0});var gNr=s(dP);m_o=r(gNr,"BartForConditionalGeneration"),gNr.forEach(t),g_o=r(cCe," (BART model)"),cCe.forEach(t),h_o=i(N),Mu=n(N,"LI",{});var fCe=s(Mu);JZ=n(fCe,"STRONG",{});var hNr=s(JZ);p_o=r(hNr,"bert"),hNr.forEach(t),__o=r(fCe," \u2014 "),cP=n(fCe,"A",{href:!0});var pNr=s(cP);u_o=r(pNr,"BertForMaskedLM"),pNr.forEach(t),b_o=r(fCe," (BERT model)"),fCe.forEach(t),v_o=i(N),Eu=n(N,"LI",{});var mCe=s(Eu);YZ=n(mCe,"STRONG",{});var _Nr=s(YZ);T_o=r(_Nr,"big_bird"),_Nr.forEach(t),F_o=r(mCe," \u2014 "),fP=n(mCe,"A",{href:!0});var uNr=s(fP);C_o=r(uNr,"BigBirdForMaskedLM"),uNr.forEach(t),M_o=r(mCe," (BigBird model)"),mCe.forEach(t),E_o=i(N),yu=n(N,"LI",{});var gCe=s(yu);KZ=n(gCe,"STRONG",{});var bNr=s(KZ);y_o=r(bNr,"camembert"),bNr.forEach(t),w_o=r(gCe," \u2014 "),mP=n(gCe,"A",{href:!0});var vNr=s(mP);A_o=r(vNr,"CamembertForMaskedLM"),vNr.forEach(t),L_o=r(gCe," (CamemBERT model)"),gCe.forEach(t),B_o=i(N),wu=n(N,"LI",{});var hCe=s(wu);ZZ=n(hCe,"STRONG",{});var TNr=s(ZZ);k_o=r(TNr,"convbert"),TNr.forEach(t),x_o=r(hCe," \u2014 "),gP=n(hCe,"A",{href:!0});var FNr=s(gP);R_o=r(FNr,"ConvBertForMaskedLM"),FNr.forEach(t),S_o=r(hCe," (ConvBERT model)"),hCe.forEach(t),P_o=i(N),Au=n(N,"LI",{});var pCe=s(Au);eee=n(pCe,"STRONG",{});var CNr=s(eee);$_o=r(CNr,"deberta"),CNr.forEach(t),I_o=r(pCe," \u2014 "),hP=n(pCe,"A",{href:!0});var MNr=s(hP);j_o=r(MNr,"DebertaForMaskedLM"),MNr.forEach(t),N_o=r(pCe," (DeBERTa model)"),pCe.forEach(t),D_o=i(N),Lu=n(N,"LI",{});var _Ce=s(Lu);oee=n(_Ce,"STRONG",{});var ENr=s(oee);q_o=r(ENr,"deberta-v2"),ENr.forEach(t),G_o=r(_Ce," \u2014 "),pP=n(_Ce,"A",{href:!0});var yNr=s(pP);O_o=r(yNr,"DebertaV2ForMaskedLM"),yNr.forEach(t),X_o=r(_Ce," (DeBERTa-v2 model)"),_Ce.forEach(t),z_o=i(N),Bu=n(N,"LI",{});var uCe=s(Bu);ree=n(uCe,"STRONG",{});var wNr=s(ree);V_o=r(wNr,"distilbert"),wNr.forEach(t),W_o=r(uCe," \u2014 "),_P=n(uCe,"A",{href:!0});var ANr=s(_P);Q_o=r(ANr,"DistilBertForMaskedLM"),ANr.forEach(t),H_o=r(uCe," (DistilBERT model)"),uCe.forEach(t),U_o=i(N),ku=n(N,"LI",{});var bCe=s(ku);tee=n(bCe,"STRONG",{});var LNr=s(tee);J_o=r(LNr,"electra"),LNr.forEach(t),Y_o=r(bCe," \u2014 "),uP=n(bCe,"A",{href:!0});var BNr=s(uP);K_o=r(BNr,"ElectraForMaskedLM"),BNr.forEach(t),Z_o=r(bCe," (ELECTRA model)"),bCe.forEach(t),euo=i(N),xu=n(N,"LI",{});var vCe=s(xu);aee=n(vCe,"STRONG",{});var kNr=s(aee);ouo=r(kNr,"flaubert"),kNr.forEach(t),ruo=r(vCe," \u2014 "),bP=n(vCe,"A",{href:!0});var xNr=s(bP);tuo=r(xNr,"FlaubertWithLMHeadModel"),xNr.forEach(t),auo=r(vCe," (FlauBERT model)"),vCe.forEach(t),nuo=i(N),Ru=n(N,"LI",{});var TCe=s(Ru);nee=n(TCe,"STRONG",{});var RNr=s(nee);suo=r(RNr,"fnet"),RNr.forEach(t),luo=r(TCe," \u2014 "),vP=n(TCe,"A",{href:!0});var SNr=s(vP);iuo=r(SNr,"FNetForMaskedLM"),SNr.forEach(t),duo=r(TCe," (FNet model)"),TCe.forEach(t),cuo=i(N),Su=n(N,"LI",{});var FCe=s(Su);see=n(FCe,"STRONG",{});var PNr=s(see);fuo=r(PNr,"funnel"),PNr.forEach(t),muo=r(FCe," \u2014 "),TP=n(FCe,"A",{href:!0});var $Nr=s(TP);guo=r($Nr,"FunnelForMaskedLM"),$Nr.forEach(t),huo=r(FCe," (Funnel Transformer model)"),FCe.forEach(t),puo=i(N),Pu=n(N,"LI",{});var CCe=s(Pu);lee=n(CCe,"STRONG",{});var INr=s(lee);_uo=r(INr,"ibert"),INr.forEach(t),uuo=r(CCe," \u2014 "),FP=n(CCe,"A",{href:!0});var jNr=s(FP);buo=r(jNr,"IBertForMaskedLM"),jNr.forEach(t),vuo=r(CCe," (I-BERT model)"),CCe.forEach(t),Tuo=i(N),$u=n(N,"LI",{});var MCe=s($u);iee=n(MCe,"STRONG",{});var NNr=s(iee);Fuo=r(NNr,"layoutlm"),NNr.forEach(t),Cuo=r(MCe," \u2014 "),CP=n(MCe,"A",{href:!0});var DNr=s(CP);Muo=r(DNr,"LayoutLMForMaskedLM"),DNr.forEach(t),Euo=r(MCe," (LayoutLM model)"),MCe.forEach(t),yuo=i(N),Iu=n(N,"LI",{});var ECe=s(Iu);dee=n(ECe,"STRONG",{});var qNr=s(dee);wuo=r(qNr,"longformer"),qNr.forEach(t),Auo=r(ECe," \u2014 "),MP=n(ECe,"A",{href:!0});var GNr=s(MP);Luo=r(GNr,"LongformerForMaskedLM"),GNr.forEach(t),Buo=r(ECe," (Longformer model)"),ECe.forEach(t),kuo=i(N),ju=n(N,"LI",{});var yCe=s(ju);cee=n(yCe,"STRONG",{});var ONr=s(cee);xuo=r(ONr,"mbart"),ONr.forEach(t),Ruo=r(yCe," \u2014 "),EP=n(yCe,"A",{href:!0});var XNr=s(EP);Suo=r(XNr,"MBartForConditionalGeneration"),XNr.forEach(t),Puo=r(yCe," (mBART model)"),yCe.forEach(t),$uo=i(N),Nu=n(N,"LI",{});var wCe=s(Nu);fee=n(wCe,"STRONG",{});var zNr=s(fee);Iuo=r(zNr,"megatron-bert"),zNr.forEach(t),juo=r(wCe," \u2014 "),yP=n(wCe,"A",{href:!0});var VNr=s(yP);Nuo=r(VNr,"MegatronBertForMaskedLM"),VNr.forEach(t),Duo=r(wCe," (MegatronBert model)"),wCe.forEach(t),quo=i(N),Du=n(N,"LI",{});var ACe=s(Du);mee=n(ACe,"STRONG",{});var WNr=s(mee);Guo=r(WNr,"mobilebert"),WNr.forEach(t),Ouo=r(ACe," \u2014 "),wP=n(ACe,"A",{href:!0});var QNr=s(wP);Xuo=r(QNr,"MobileBertForMaskedLM"),QNr.forEach(t),zuo=r(ACe," (MobileBERT model)"),ACe.forEach(t),Vuo=i(N),qu=n(N,"LI",{});var LCe=s(qu);gee=n(LCe,"STRONG",{});var HNr=s(gee);Wuo=r(HNr,"mpnet"),HNr.forEach(t),Quo=r(LCe," \u2014 "),AP=n(LCe,"A",{href:!0});var UNr=s(AP);Huo=r(UNr,"MPNetForMaskedLM"),UNr.forEach(t),Uuo=r(LCe," (MPNet model)"),LCe.forEach(t),Juo=i(N),Gu=n(N,"LI",{});var BCe=s(Gu);hee=n(BCe,"STRONG",{});var JNr=s(hee);Yuo=r(JNr,"nystromformer"),JNr.forEach(t),Kuo=r(BCe," \u2014 "),LP=n(BCe,"A",{href:!0});var YNr=s(LP);Zuo=r(YNr,"NystromformerForMaskedLM"),YNr.forEach(t),e2o=r(BCe," (Nystromformer model)"),BCe.forEach(t),o2o=i(N),Ou=n(N,"LI",{});var kCe=s(Ou);pee=n(kCe,"STRONG",{});var KNr=s(pee);r2o=r(KNr,"perceiver"),KNr.forEach(t),t2o=r(kCe," \u2014 "),BP=n(kCe,"A",{href:!0});var ZNr=s(BP);a2o=r(ZNr,"PerceiverForMaskedLM"),ZNr.forEach(t),n2o=r(kCe," (Perceiver model)"),kCe.forEach(t),s2o=i(N),Xu=n(N,"LI",{});var xCe=s(Xu);_ee=n(xCe,"STRONG",{});var eDr=s(_ee);l2o=r(eDr,"qdqbert"),eDr.forEach(t),i2o=r(xCe," \u2014 "),kP=n(xCe,"A",{href:!0});var oDr=s(kP);d2o=r(oDr,"QDQBertForMaskedLM"),oDr.forEach(t),c2o=r(xCe," (QDQBert model)"),xCe.forEach(t),f2o=i(N),zu=n(N,"LI",{});var RCe=s(zu);uee=n(RCe,"STRONG",{});var rDr=s(uee);m2o=r(rDr,"reformer"),rDr.forEach(t),g2o=r(RCe," \u2014 "),xP=n(RCe,"A",{href:!0});var tDr=s(xP);h2o=r(tDr,"ReformerForMaskedLM"),tDr.forEach(t),p2o=r(RCe," (Reformer model)"),RCe.forEach(t),_2o=i(N),Vu=n(N,"LI",{});var SCe=s(Vu);bee=n(SCe,"STRONG",{});var aDr=s(bee);u2o=r(aDr,"rembert"),aDr.forEach(t),b2o=r(SCe," \u2014 "),RP=n(SCe,"A",{href:!0});var nDr=s(RP);v2o=r(nDr,"RemBertForMaskedLM"),nDr.forEach(t),T2o=r(SCe," (RemBERT model)"),SCe.forEach(t),F2o=i(N),Wu=n(N,"LI",{});var PCe=s(Wu);vee=n(PCe,"STRONG",{});var sDr=s(vee);C2o=r(sDr,"roberta"),sDr.forEach(t),M2o=r(PCe," \u2014 "),SP=n(PCe,"A",{href:!0});var lDr=s(SP);E2o=r(lDr,"RobertaForMaskedLM"),lDr.forEach(t),y2o=r(PCe," (RoBERTa model)"),PCe.forEach(t),w2o=i(N),Qu=n(N,"LI",{});var $Ce=s(Qu);Tee=n($Ce,"STRONG",{});var iDr=s(Tee);A2o=r(iDr,"roformer"),iDr.forEach(t),L2o=r($Ce," \u2014 "),PP=n($Ce,"A",{href:!0});var dDr=s(PP);B2o=r(dDr,"RoFormerForMaskedLM"),dDr.forEach(t),k2o=r($Ce," (RoFormer model)"),$Ce.forEach(t),x2o=i(N),Hu=n(N,"LI",{});var ICe=s(Hu);Fee=n(ICe,"STRONG",{});var cDr=s(Fee);R2o=r(cDr,"squeezebert"),cDr.forEach(t),S2o=r(ICe," \u2014 "),$P=n(ICe,"A",{href:!0});var fDr=s($P);P2o=r(fDr,"SqueezeBertForMaskedLM"),fDr.forEach(t),$2o=r(ICe," (SqueezeBERT model)"),ICe.forEach(t),I2o=i(N),Uu=n(N,"LI",{});var jCe=s(Uu);Cee=n(jCe,"STRONG",{});var mDr=s(Cee);j2o=r(mDr,"tapas"),mDr.forEach(t),N2o=r(jCe," \u2014 "),IP=n(jCe,"A",{href:!0});var gDr=s(IP);D2o=r(gDr,"TapasForMaskedLM"),gDr.forEach(t),q2o=r(jCe," (TAPAS model)"),jCe.forEach(t),G2o=i(N),Ju=n(N,"LI",{});var NCe=s(Ju);Mee=n(NCe,"STRONG",{});var hDr=s(Mee);O2o=r(hDr,"wav2vec2"),hDr.forEach(t),X2o=r(NCe," \u2014 "),Eee=n(NCe,"CODE",{});var pDr=s(Eee);z2o=r(pDr,"Wav2Vec2ForMaskedLM"),pDr.forEach(t),V2o=r(NCe,"(Wav2Vec2 model)"),NCe.forEach(t),W2o=i(N),Yu=n(N,"LI",{});var DCe=s(Yu);yee=n(DCe,"STRONG",{});var _Dr=s(yee);Q2o=r(_Dr,"xlm"),_Dr.forEach(t),H2o=r(DCe," \u2014 "),jP=n(DCe,"A",{href:!0});var uDr=s(jP);U2o=r(uDr,"XLMWithLMHeadModel"),uDr.forEach(t),J2o=r(DCe," (XLM model)"),DCe.forEach(t),Y2o=i(N),Ku=n(N,"LI",{});var qCe=s(Ku);wee=n(qCe,"STRONG",{});var bDr=s(wee);K2o=r(bDr,"xlm-roberta"),bDr.forEach(t),Z2o=r(qCe," \u2014 "),NP=n(qCe,"A",{href:!0});var vDr=s(NP);e1o=r(vDr,"XLMRobertaForMaskedLM"),vDr.forEach(t),o1o=r(qCe," (XLM-RoBERTa model)"),qCe.forEach(t),r1o=i(N),Zu=n(N,"LI",{});var GCe=s(Zu);Aee=n(GCe,"STRONG",{});var TDr=s(Aee);t1o=r(TDr,"xlm-roberta-xl"),TDr.forEach(t),a1o=r(GCe," \u2014 "),DP=n(GCe,"A",{href:!0});var FDr=s(DP);n1o=r(FDr,"XLMRobertaXLForMaskedLM"),FDr.forEach(t),s1o=r(GCe," (XLM-RoBERTa-XL model)"),GCe.forEach(t),l1o=i(N),e2=n(N,"LI",{});var OCe=s(e2);Lee=n(OCe,"STRONG",{});var CDr=s(Lee);i1o=r(CDr,"yoso"),CDr.forEach(t),d1o=r(OCe," \u2014 "),qP=n(OCe,"A",{href:!0});var MDr=s(qP);c1o=r(MDr,"YosoForMaskedLM"),MDr.forEach(t),f1o=r(OCe," (YOSO model)"),OCe.forEach(t),N.forEach(t),m1o=i(It),o2=n(It,"P",{});var XCe=s(o2);g1o=r(XCe,"The model is set in evaluation mode by default using "),Bee=n(XCe,"CODE",{});var EDr=s(Bee);h1o=r(EDr,"model.eval()"),EDr.forEach(t),p1o=r(XCe,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),kee=n(XCe,"CODE",{});var yDr=s(kee);_1o=r(yDr,"model.train()"),yDr.forEach(t),XCe.forEach(t),u1o=i(It),xee=n(It,"P",{});var wDr=s(xee);b1o=r(wDr,"Examples:"),wDr.forEach(t),v1o=i(It),m(iE.$$.fragment,It),It.forEach(t),zs.forEach(t),ELe=i(d),Ki=n(d,"H2",{class:!0});var xBe=s(Ki);r2=n(xBe,"A",{id:!0,class:!0,href:!0});var ADr=s(r2);Ree=n(ADr,"SPAN",{});var LDr=s(Ree);m(dE.$$.fragment,LDr),LDr.forEach(t),ADr.forEach(t),T1o=i(xBe),See=n(xBe,"SPAN",{});var BDr=s(See);F1o=r(BDr,"AutoModelForSeq2SeqLM"),BDr.forEach(t),xBe.forEach(t),yLe=i(d),Uo=n(d,"DIV",{class:!0});var Ws=s(Uo);m(cE.$$.fragment,Ws),C1o=i(Ws),Zi=n(Ws,"P",{});var ez=s(Zi);M1o=r(ez,`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence language modeling head) when created
with the `),Pee=n(ez,"CODE",{});var kDr=s(Pee);E1o=r(kDr,"from_pretrained()"),kDr.forEach(t),y1o=r(ez,"class method or the "),$ee=n(ez,"CODE",{});var xDr=s($ee);w1o=r(xDr,"from_config()"),xDr.forEach(t),A1o=r(ez,`class
method.`),ez.forEach(t),L1o=i(Ws),fE=n(Ws,"P",{});var RBe=s(fE);B1o=r(RBe,"This class cannot be instantiated directly using "),Iee=n(RBe,"CODE",{});var RDr=s(Iee);k1o=r(RDr,"__init__()"),RDr.forEach(t),x1o=r(RBe," (throws an error)."),RBe.forEach(t),R1o=i(Ws),Or=n(Ws,"DIV",{class:!0});var Qs=s(Or);m(mE.$$.fragment,Qs),S1o=i(Qs),jee=n(Qs,"P",{});var SDr=s(jee);P1o=r(SDr,"Instantiates one of the model classes of the library (with a sequence-to-sequence language modeling head) from a configuration."),SDr.forEach(t),$1o=i(Qs),ed=n(Qs,"P",{});var oz=s(ed);I1o=r(oz,`Note:
Loading a model from its configuration file does `),Nee=n(oz,"STRONG",{});var PDr=s(Nee);j1o=r(PDr,"not"),PDr.forEach(t),N1o=r(oz,` load the model weights. It only affects the
model\u2019s configuration. Use `),Dee=n(oz,"CODE",{});var $Dr=s(Dee);D1o=r($Dr,"from_pretrained()"),$Dr.forEach(t),q1o=r(oz,"to load the model weights."),oz.forEach(t),G1o=i(Qs),qee=n(Qs,"P",{});var IDr=s(qee);O1o=r(IDr,"Examples:"),IDr.forEach(t),X1o=i(Qs),m(gE.$$.fragment,Qs),Qs.forEach(t),z1o=i(Ws),Pe=n(Ws,"DIV",{class:!0});var jt=s(Pe);m(hE.$$.fragment,jt),V1o=i(jt),Gee=n(jt,"P",{});var jDr=s(Gee);W1o=r(jDr,"Instantiate one of the model classes of the library (with a sequence-to-sequence language modeling head) from a pretrained model."),jDr.forEach(t),Q1o=i(jt),Xa=n(jt,"P",{});var f4=s(Xa);H1o=r(f4,"The model class to instantiate is selected based on the "),Oee=n(f4,"CODE",{});var NDr=s(Oee);U1o=r(NDr,"model_type"),NDr.forEach(t),J1o=r(f4,` property of the config object (either
passed as an argument or loaded from `),Xee=n(f4,"CODE",{});var DDr=s(Xee);Y1o=r(DDr,"pretrained_model_name_or_path"),DDr.forEach(t),K1o=r(f4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),zee=n(f4,"CODE",{});var qDr=s(zee);Z1o=r(qDr,"pretrained_model_name_or_path"),qDr.forEach(t),ebo=r(f4,":"),f4.forEach(t),obo=i(jt),ae=n(jt,"UL",{});var le=s(ae);t2=n(le,"LI",{});var zCe=s(t2);Vee=n(zCe,"STRONG",{});var GDr=s(Vee);rbo=r(GDr,"bart"),GDr.forEach(t),tbo=r(zCe," \u2014 "),GP=n(zCe,"A",{href:!0});var ODr=s(GP);abo=r(ODr,"BartForConditionalGeneration"),ODr.forEach(t),nbo=r(zCe," (BART model)"),zCe.forEach(t),sbo=i(le),a2=n(le,"LI",{});var VCe=s(a2);Wee=n(VCe,"STRONG",{});var XDr=s(Wee);lbo=r(XDr,"bigbird_pegasus"),XDr.forEach(t),ibo=r(VCe," \u2014 "),OP=n(VCe,"A",{href:!0});var zDr=s(OP);dbo=r(zDr,"BigBirdPegasusForConditionalGeneration"),zDr.forEach(t),cbo=r(VCe," (BigBirdPegasus model)"),VCe.forEach(t),fbo=i(le),n2=n(le,"LI",{});var WCe=s(n2);Qee=n(WCe,"STRONG",{});var VDr=s(Qee);mbo=r(VDr,"blenderbot"),VDr.forEach(t),gbo=r(WCe," \u2014 "),XP=n(WCe,"A",{href:!0});var WDr=s(XP);hbo=r(WDr,"BlenderbotForConditionalGeneration"),WDr.forEach(t),pbo=r(WCe," (Blenderbot model)"),WCe.forEach(t),_bo=i(le),s2=n(le,"LI",{});var QCe=s(s2);Hee=n(QCe,"STRONG",{});var QDr=s(Hee);ubo=r(QDr,"blenderbot-small"),QDr.forEach(t),bbo=r(QCe," \u2014 "),zP=n(QCe,"A",{href:!0});var HDr=s(zP);vbo=r(HDr,"BlenderbotSmallForConditionalGeneration"),HDr.forEach(t),Tbo=r(QCe," (BlenderbotSmall model)"),QCe.forEach(t),Fbo=i(le),l2=n(le,"LI",{});var HCe=s(l2);Uee=n(HCe,"STRONG",{});var UDr=s(Uee);Cbo=r(UDr,"encoder-decoder"),UDr.forEach(t),Mbo=r(HCe," \u2014 "),VP=n(HCe,"A",{href:!0});var JDr=s(VP);Ebo=r(JDr,"EncoderDecoderModel"),JDr.forEach(t),ybo=r(HCe," (Encoder decoder model)"),HCe.forEach(t),wbo=i(le),i2=n(le,"LI",{});var UCe=s(i2);Jee=n(UCe,"STRONG",{});var YDr=s(Jee);Abo=r(YDr,"fsmt"),YDr.forEach(t),Lbo=r(UCe," \u2014 "),WP=n(UCe,"A",{href:!0});var KDr=s(WP);Bbo=r(KDr,"FSMTForConditionalGeneration"),KDr.forEach(t),kbo=r(UCe," (FairSeq Machine-Translation model)"),UCe.forEach(t),xbo=i(le),d2=n(le,"LI",{});var JCe=s(d2);Yee=n(JCe,"STRONG",{});var ZDr=s(Yee);Rbo=r(ZDr,"led"),ZDr.forEach(t),Sbo=r(JCe," \u2014 "),QP=n(JCe,"A",{href:!0});var eqr=s(QP);Pbo=r(eqr,"LEDForConditionalGeneration"),eqr.forEach(t),$bo=r(JCe," (LED model)"),JCe.forEach(t),Ibo=i(le),c2=n(le,"LI",{});var YCe=s(c2);Kee=n(YCe,"STRONG",{});var oqr=s(Kee);jbo=r(oqr,"m2m_100"),oqr.forEach(t),Nbo=r(YCe," \u2014 "),HP=n(YCe,"A",{href:!0});var rqr=s(HP);Dbo=r(rqr,"M2M100ForConditionalGeneration"),rqr.forEach(t),qbo=r(YCe," (M2M100 model)"),YCe.forEach(t),Gbo=i(le),f2=n(le,"LI",{});var KCe=s(f2);Zee=n(KCe,"STRONG",{});var tqr=s(Zee);Obo=r(tqr,"marian"),tqr.forEach(t),Xbo=r(KCe," \u2014 "),UP=n(KCe,"A",{href:!0});var aqr=s(UP);zbo=r(aqr,"MarianMTModel"),aqr.forEach(t),Vbo=r(KCe," (Marian model)"),KCe.forEach(t),Wbo=i(le),m2=n(le,"LI",{});var ZCe=s(m2);eoe=n(ZCe,"STRONG",{});var nqr=s(eoe);Qbo=r(nqr,"mbart"),nqr.forEach(t),Hbo=r(ZCe," \u2014 "),JP=n(ZCe,"A",{href:!0});var sqr=s(JP);Ubo=r(sqr,"MBartForConditionalGeneration"),sqr.forEach(t),Jbo=r(ZCe," (mBART model)"),ZCe.forEach(t),Ybo=i(le),g2=n(le,"LI",{});var e4e=s(g2);ooe=n(e4e,"STRONG",{});var lqr=s(ooe);Kbo=r(lqr,"mt5"),lqr.forEach(t),Zbo=r(e4e," \u2014 "),YP=n(e4e,"A",{href:!0});var iqr=s(YP);e5o=r(iqr,"MT5ForConditionalGeneration"),iqr.forEach(t),o5o=r(e4e," (mT5 model)"),e4e.forEach(t),r5o=i(le),h2=n(le,"LI",{});var o4e=s(h2);roe=n(o4e,"STRONG",{});var dqr=s(roe);t5o=r(dqr,"pegasus"),dqr.forEach(t),a5o=r(o4e," \u2014 "),KP=n(o4e,"A",{href:!0});var cqr=s(KP);n5o=r(cqr,"PegasusForConditionalGeneration"),cqr.forEach(t),s5o=r(o4e," (Pegasus model)"),o4e.forEach(t),l5o=i(le),p2=n(le,"LI",{});var r4e=s(p2);toe=n(r4e,"STRONG",{});var fqr=s(toe);i5o=r(fqr,"plbart"),fqr.forEach(t),d5o=r(r4e," \u2014 "),ZP=n(r4e,"A",{href:!0});var mqr=s(ZP);c5o=r(mqr,"PLBartForConditionalGeneration"),mqr.forEach(t),f5o=r(r4e," (PLBart model)"),r4e.forEach(t),m5o=i(le),_2=n(le,"LI",{});var t4e=s(_2);aoe=n(t4e,"STRONG",{});var gqr=s(aoe);g5o=r(gqr,"prophetnet"),gqr.forEach(t),h5o=r(t4e," \u2014 "),e$=n(t4e,"A",{href:!0});var hqr=s(e$);p5o=r(hqr,"ProphetNetForConditionalGeneration"),hqr.forEach(t),_5o=r(t4e," (ProphetNet model)"),t4e.forEach(t),u5o=i(le),u2=n(le,"LI",{});var a4e=s(u2);noe=n(a4e,"STRONG",{});var pqr=s(noe);b5o=r(pqr,"t5"),pqr.forEach(t),v5o=r(a4e," \u2014 "),o$=n(a4e,"A",{href:!0});var _qr=s(o$);T5o=r(_qr,"T5ForConditionalGeneration"),_qr.forEach(t),F5o=r(a4e," (T5 model)"),a4e.forEach(t),C5o=i(le),b2=n(le,"LI",{});var n4e=s(b2);soe=n(n4e,"STRONG",{});var uqr=s(soe);M5o=r(uqr,"xlm-prophetnet"),uqr.forEach(t),E5o=r(n4e," \u2014 "),r$=n(n4e,"A",{href:!0});var bqr=s(r$);y5o=r(bqr,"XLMProphetNetForConditionalGeneration"),bqr.forEach(t),w5o=r(n4e," (XLMProphetNet model)"),n4e.forEach(t),le.forEach(t),A5o=i(jt),v2=n(jt,"P",{});var s4e=s(v2);L5o=r(s4e,"The model is set in evaluation mode by default using "),loe=n(s4e,"CODE",{});var vqr=s(loe);B5o=r(vqr,"model.eval()"),vqr.forEach(t),k5o=r(s4e,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),ioe=n(s4e,"CODE",{});var Tqr=s(ioe);x5o=r(Tqr,"model.train()"),Tqr.forEach(t),s4e.forEach(t),R5o=i(jt),doe=n(jt,"P",{});var Fqr=s(doe);S5o=r(Fqr,"Examples:"),Fqr.forEach(t),P5o=i(jt),m(pE.$$.fragment,jt),jt.forEach(t),Ws.forEach(t),wLe=i(d),od=n(d,"H2",{class:!0});var SBe=s(od);T2=n(SBe,"A",{id:!0,class:!0,href:!0});var Cqr=s(T2);coe=n(Cqr,"SPAN",{});var Mqr=s(coe);m(_E.$$.fragment,Mqr),Mqr.forEach(t),Cqr.forEach(t),$5o=i(SBe),foe=n(SBe,"SPAN",{});var Eqr=s(foe);I5o=r(Eqr,"AutoModelForSequenceClassification"),Eqr.forEach(t),SBe.forEach(t),ALe=i(d),Jo=n(d,"DIV",{class:!0});var Hs=s(Jo);m(uE.$$.fragment,Hs),j5o=i(Hs),rd=n(Hs,"P",{});var rz=s(rd);N5o=r(rz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence classification head) when created
with the `),moe=n(rz,"CODE",{});var yqr=s(moe);D5o=r(yqr,"from_pretrained()"),yqr.forEach(t),q5o=r(rz,"class method or the "),goe=n(rz,"CODE",{});var wqr=s(goe);G5o=r(wqr,"from_config()"),wqr.forEach(t),O5o=r(rz,`class
method.`),rz.forEach(t),X5o=i(Hs),bE=n(Hs,"P",{});var PBe=s(bE);z5o=r(PBe,"This class cannot be instantiated directly using "),hoe=n(PBe,"CODE",{});var Aqr=s(hoe);V5o=r(Aqr,"__init__()"),Aqr.forEach(t),W5o=r(PBe," (throws an error)."),PBe.forEach(t),Q5o=i(Hs),Xr=n(Hs,"DIV",{class:!0});var Us=s(Xr);m(vE.$$.fragment,Us),H5o=i(Us),poe=n(Us,"P",{});var Lqr=s(poe);U5o=r(Lqr,"Instantiates one of the model classes of the library (with a sequence classification head) from a configuration."),Lqr.forEach(t),J5o=i(Us),td=n(Us,"P",{});var tz=s(td);Y5o=r(tz,`Note:
Loading a model from its configuration file does `),_oe=n(tz,"STRONG",{});var Bqr=s(_oe);K5o=r(Bqr,"not"),Bqr.forEach(t),Z5o=r(tz,` load the model weights. It only affects the
model\u2019s configuration. Use `),uoe=n(tz,"CODE",{});var kqr=s(uoe);evo=r(kqr,"from_pretrained()"),kqr.forEach(t),ovo=r(tz,"to load the model weights."),tz.forEach(t),rvo=i(Us),boe=n(Us,"P",{});var xqr=s(boe);tvo=r(xqr,"Examples:"),xqr.forEach(t),avo=i(Us),m(TE.$$.fragment,Us),Us.forEach(t),nvo=i(Hs),$e=n(Hs,"DIV",{class:!0});var Nt=s($e);m(FE.$$.fragment,Nt),svo=i(Nt),voe=n(Nt,"P",{});var Rqr=s(voe);lvo=r(Rqr,"Instantiate one of the model classes of the library (with a sequence classification head) from a pretrained model."),Rqr.forEach(t),ivo=i(Nt),za=n(Nt,"P",{});var m4=s(za);dvo=r(m4,"The model class to instantiate is selected based on the "),Toe=n(m4,"CODE",{});var Sqr=s(Toe);cvo=r(Sqr,"model_type"),Sqr.forEach(t),fvo=r(m4,` property of the config object (either
passed as an argument or loaded from `),Foe=n(m4,"CODE",{});var Pqr=s(Foe);mvo=r(Pqr,"pretrained_model_name_or_path"),Pqr.forEach(t),gvo=r(m4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Coe=n(m4,"CODE",{});var $qr=s(Coe);hvo=r($qr,"pretrained_model_name_or_path"),$qr.forEach(t),pvo=r(m4,":"),m4.forEach(t),_vo=i(Nt),A=n(Nt,"UL",{});var L=s(A);F2=n(L,"LI",{});var l4e=s(F2);Moe=n(l4e,"STRONG",{});var Iqr=s(Moe);uvo=r(Iqr,"albert"),Iqr.forEach(t),bvo=r(l4e," \u2014 "),t$=n(l4e,"A",{href:!0});var jqr=s(t$);vvo=r(jqr,"AlbertForSequenceClassification"),jqr.forEach(t),Tvo=r(l4e," (ALBERT model)"),l4e.forEach(t),Fvo=i(L),C2=n(L,"LI",{});var i4e=s(C2);Eoe=n(i4e,"STRONG",{});var Nqr=s(Eoe);Cvo=r(Nqr,"bart"),Nqr.forEach(t),Mvo=r(i4e," \u2014 "),a$=n(i4e,"A",{href:!0});var Dqr=s(a$);Evo=r(Dqr,"BartForSequenceClassification"),Dqr.forEach(t),yvo=r(i4e," (BART model)"),i4e.forEach(t),wvo=i(L),M2=n(L,"LI",{});var d4e=s(M2);yoe=n(d4e,"STRONG",{});var qqr=s(yoe);Avo=r(qqr,"bert"),qqr.forEach(t),Lvo=r(d4e," \u2014 "),n$=n(d4e,"A",{href:!0});var Gqr=s(n$);Bvo=r(Gqr,"BertForSequenceClassification"),Gqr.forEach(t),kvo=r(d4e," (BERT model)"),d4e.forEach(t),xvo=i(L),E2=n(L,"LI",{});var c4e=s(E2);woe=n(c4e,"STRONG",{});var Oqr=s(woe);Rvo=r(Oqr,"big_bird"),Oqr.forEach(t),Svo=r(c4e," \u2014 "),s$=n(c4e,"A",{href:!0});var Xqr=s(s$);Pvo=r(Xqr,"BigBirdForSequenceClassification"),Xqr.forEach(t),$vo=r(c4e," (BigBird model)"),c4e.forEach(t),Ivo=i(L),y2=n(L,"LI",{});var f4e=s(y2);Aoe=n(f4e,"STRONG",{});var zqr=s(Aoe);jvo=r(zqr,"bigbird_pegasus"),zqr.forEach(t),Nvo=r(f4e," \u2014 "),l$=n(f4e,"A",{href:!0});var Vqr=s(l$);Dvo=r(Vqr,"BigBirdPegasusForSequenceClassification"),Vqr.forEach(t),qvo=r(f4e," (BigBirdPegasus model)"),f4e.forEach(t),Gvo=i(L),w2=n(L,"LI",{});var m4e=s(w2);Loe=n(m4e,"STRONG",{});var Wqr=s(Loe);Ovo=r(Wqr,"camembert"),Wqr.forEach(t),Xvo=r(m4e," \u2014 "),i$=n(m4e,"A",{href:!0});var Qqr=s(i$);zvo=r(Qqr,"CamembertForSequenceClassification"),Qqr.forEach(t),Vvo=r(m4e," (CamemBERT model)"),m4e.forEach(t),Wvo=i(L),A2=n(L,"LI",{});var g4e=s(A2);Boe=n(g4e,"STRONG",{});var Hqr=s(Boe);Qvo=r(Hqr,"canine"),Hqr.forEach(t),Hvo=r(g4e," \u2014 "),d$=n(g4e,"A",{href:!0});var Uqr=s(d$);Uvo=r(Uqr,"CanineForSequenceClassification"),Uqr.forEach(t),Jvo=r(g4e," (Canine model)"),g4e.forEach(t),Yvo=i(L),L2=n(L,"LI",{});var h4e=s(L2);koe=n(h4e,"STRONG",{});var Jqr=s(koe);Kvo=r(Jqr,"convbert"),Jqr.forEach(t),Zvo=r(h4e," \u2014 "),c$=n(h4e,"A",{href:!0});var Yqr=s(c$);eTo=r(Yqr,"ConvBertForSequenceClassification"),Yqr.forEach(t),oTo=r(h4e," (ConvBERT model)"),h4e.forEach(t),rTo=i(L),B2=n(L,"LI",{});var p4e=s(B2);xoe=n(p4e,"STRONG",{});var Kqr=s(xoe);tTo=r(Kqr,"ctrl"),Kqr.forEach(t),aTo=r(p4e," \u2014 "),f$=n(p4e,"A",{href:!0});var Zqr=s(f$);nTo=r(Zqr,"CTRLForSequenceClassification"),Zqr.forEach(t),sTo=r(p4e," (CTRL model)"),p4e.forEach(t),lTo=i(L),k2=n(L,"LI",{});var _4e=s(k2);Roe=n(_4e,"STRONG",{});var eGr=s(Roe);iTo=r(eGr,"deberta"),eGr.forEach(t),dTo=r(_4e," \u2014 "),m$=n(_4e,"A",{href:!0});var oGr=s(m$);cTo=r(oGr,"DebertaForSequenceClassification"),oGr.forEach(t),fTo=r(_4e," (DeBERTa model)"),_4e.forEach(t),mTo=i(L),x2=n(L,"LI",{});var u4e=s(x2);Soe=n(u4e,"STRONG",{});var rGr=s(Soe);gTo=r(rGr,"deberta-v2"),rGr.forEach(t),hTo=r(u4e," \u2014 "),g$=n(u4e,"A",{href:!0});var tGr=s(g$);pTo=r(tGr,"DebertaV2ForSequenceClassification"),tGr.forEach(t),_To=r(u4e," (DeBERTa-v2 model)"),u4e.forEach(t),uTo=i(L),R2=n(L,"LI",{});var b4e=s(R2);Poe=n(b4e,"STRONG",{});var aGr=s(Poe);bTo=r(aGr,"distilbert"),aGr.forEach(t),vTo=r(b4e," \u2014 "),h$=n(b4e,"A",{href:!0});var nGr=s(h$);TTo=r(nGr,"DistilBertForSequenceClassification"),nGr.forEach(t),FTo=r(b4e," (DistilBERT model)"),b4e.forEach(t),CTo=i(L),S2=n(L,"LI",{});var v4e=s(S2);$oe=n(v4e,"STRONG",{});var sGr=s($oe);MTo=r(sGr,"electra"),sGr.forEach(t),ETo=r(v4e," \u2014 "),p$=n(v4e,"A",{href:!0});var lGr=s(p$);yTo=r(lGr,"ElectraForSequenceClassification"),lGr.forEach(t),wTo=r(v4e," (ELECTRA model)"),v4e.forEach(t),ATo=i(L),P2=n(L,"LI",{});var T4e=s(P2);Ioe=n(T4e,"STRONG",{});var iGr=s(Ioe);LTo=r(iGr,"flaubert"),iGr.forEach(t),BTo=r(T4e," \u2014 "),_$=n(T4e,"A",{href:!0});var dGr=s(_$);kTo=r(dGr,"FlaubertForSequenceClassification"),dGr.forEach(t),xTo=r(T4e," (FlauBERT model)"),T4e.forEach(t),RTo=i(L),$2=n(L,"LI",{});var F4e=s($2);joe=n(F4e,"STRONG",{});var cGr=s(joe);STo=r(cGr,"fnet"),cGr.forEach(t),PTo=r(F4e," \u2014 "),u$=n(F4e,"A",{href:!0});var fGr=s(u$);$To=r(fGr,"FNetForSequenceClassification"),fGr.forEach(t),ITo=r(F4e," (FNet model)"),F4e.forEach(t),jTo=i(L),I2=n(L,"LI",{});var C4e=s(I2);Noe=n(C4e,"STRONG",{});var mGr=s(Noe);NTo=r(mGr,"funnel"),mGr.forEach(t),DTo=r(C4e," \u2014 "),b$=n(C4e,"A",{href:!0});var gGr=s(b$);qTo=r(gGr,"FunnelForSequenceClassification"),gGr.forEach(t),GTo=r(C4e," (Funnel Transformer model)"),C4e.forEach(t),OTo=i(L),j2=n(L,"LI",{});var M4e=s(j2);Doe=n(M4e,"STRONG",{});var hGr=s(Doe);XTo=r(hGr,"gpt2"),hGr.forEach(t),zTo=r(M4e," \u2014 "),v$=n(M4e,"A",{href:!0});var pGr=s(v$);VTo=r(pGr,"GPT2ForSequenceClassification"),pGr.forEach(t),WTo=r(M4e," (OpenAI GPT-2 model)"),M4e.forEach(t),QTo=i(L),N2=n(L,"LI",{});var E4e=s(N2);qoe=n(E4e,"STRONG",{});var _Gr=s(qoe);HTo=r(_Gr,"gpt_neo"),_Gr.forEach(t),UTo=r(E4e," \u2014 "),T$=n(E4e,"A",{href:!0});var uGr=s(T$);JTo=r(uGr,"GPTNeoForSequenceClassification"),uGr.forEach(t),YTo=r(E4e," (GPT Neo model)"),E4e.forEach(t),KTo=i(L),D2=n(L,"LI",{});var y4e=s(D2);Goe=n(y4e,"STRONG",{});var bGr=s(Goe);ZTo=r(bGr,"gptj"),bGr.forEach(t),e7o=r(y4e," \u2014 "),F$=n(y4e,"A",{href:!0});var vGr=s(F$);o7o=r(vGr,"GPTJForSequenceClassification"),vGr.forEach(t),r7o=r(y4e," (GPT-J model)"),y4e.forEach(t),t7o=i(L),q2=n(L,"LI",{});var w4e=s(q2);Ooe=n(w4e,"STRONG",{});var TGr=s(Ooe);a7o=r(TGr,"ibert"),TGr.forEach(t),n7o=r(w4e," \u2014 "),C$=n(w4e,"A",{href:!0});var FGr=s(C$);s7o=r(FGr,"IBertForSequenceClassification"),FGr.forEach(t),l7o=r(w4e," (I-BERT model)"),w4e.forEach(t),i7o=i(L),G2=n(L,"LI",{});var A4e=s(G2);Xoe=n(A4e,"STRONG",{});var CGr=s(Xoe);d7o=r(CGr,"layoutlm"),CGr.forEach(t),c7o=r(A4e," \u2014 "),M$=n(A4e,"A",{href:!0});var MGr=s(M$);f7o=r(MGr,"LayoutLMForSequenceClassification"),MGr.forEach(t),m7o=r(A4e," (LayoutLM model)"),A4e.forEach(t),g7o=i(L),O2=n(L,"LI",{});var L4e=s(O2);zoe=n(L4e,"STRONG",{});var EGr=s(zoe);h7o=r(EGr,"layoutlmv2"),EGr.forEach(t),p7o=r(L4e," \u2014 "),E$=n(L4e,"A",{href:!0});var yGr=s(E$);_7o=r(yGr,"LayoutLMv2ForSequenceClassification"),yGr.forEach(t),u7o=r(L4e," (LayoutLMv2 model)"),L4e.forEach(t),b7o=i(L),X2=n(L,"LI",{});var B4e=s(X2);Voe=n(B4e,"STRONG",{});var wGr=s(Voe);v7o=r(wGr,"led"),wGr.forEach(t),T7o=r(B4e," \u2014 "),y$=n(B4e,"A",{href:!0});var AGr=s(y$);F7o=r(AGr,"LEDForSequenceClassification"),AGr.forEach(t),C7o=r(B4e," (LED model)"),B4e.forEach(t),M7o=i(L),z2=n(L,"LI",{});var k4e=s(z2);Woe=n(k4e,"STRONG",{});var LGr=s(Woe);E7o=r(LGr,"longformer"),LGr.forEach(t),y7o=r(k4e," \u2014 "),w$=n(k4e,"A",{href:!0});var BGr=s(w$);w7o=r(BGr,"LongformerForSequenceClassification"),BGr.forEach(t),A7o=r(k4e," (Longformer model)"),k4e.forEach(t),L7o=i(L),V2=n(L,"LI",{});var x4e=s(V2);Qoe=n(x4e,"STRONG",{});var kGr=s(Qoe);B7o=r(kGr,"mbart"),kGr.forEach(t),k7o=r(x4e," \u2014 "),A$=n(x4e,"A",{href:!0});var xGr=s(A$);x7o=r(xGr,"MBartForSequenceClassification"),xGr.forEach(t),R7o=r(x4e," (mBART model)"),x4e.forEach(t),S7o=i(L),W2=n(L,"LI",{});var R4e=s(W2);Hoe=n(R4e,"STRONG",{});var RGr=s(Hoe);P7o=r(RGr,"megatron-bert"),RGr.forEach(t),$7o=r(R4e," \u2014 "),L$=n(R4e,"A",{href:!0});var SGr=s(L$);I7o=r(SGr,"MegatronBertForSequenceClassification"),SGr.forEach(t),j7o=r(R4e," (MegatronBert model)"),R4e.forEach(t),N7o=i(L),Q2=n(L,"LI",{});var S4e=s(Q2);Uoe=n(S4e,"STRONG",{});var PGr=s(Uoe);D7o=r(PGr,"mobilebert"),PGr.forEach(t),q7o=r(S4e," \u2014 "),B$=n(S4e,"A",{href:!0});var $Gr=s(B$);G7o=r($Gr,"MobileBertForSequenceClassification"),$Gr.forEach(t),O7o=r(S4e," (MobileBERT model)"),S4e.forEach(t),X7o=i(L),H2=n(L,"LI",{});var P4e=s(H2);Joe=n(P4e,"STRONG",{});var IGr=s(Joe);z7o=r(IGr,"mpnet"),IGr.forEach(t),V7o=r(P4e," \u2014 "),k$=n(P4e,"A",{href:!0});var jGr=s(k$);W7o=r(jGr,"MPNetForSequenceClassification"),jGr.forEach(t),Q7o=r(P4e," (MPNet model)"),P4e.forEach(t),H7o=i(L),U2=n(L,"LI",{});var $4e=s(U2);Yoe=n($4e,"STRONG",{});var NGr=s(Yoe);U7o=r(NGr,"nystromformer"),NGr.forEach(t),J7o=r($4e," \u2014 "),x$=n($4e,"A",{href:!0});var DGr=s(x$);Y7o=r(DGr,"NystromformerForSequenceClassification"),DGr.forEach(t),K7o=r($4e," (Nystromformer model)"),$4e.forEach(t),Z7o=i(L),J2=n(L,"LI",{});var I4e=s(J2);Koe=n(I4e,"STRONG",{});var qGr=s(Koe);eFo=r(qGr,"openai-gpt"),qGr.forEach(t),oFo=r(I4e," \u2014 "),R$=n(I4e,"A",{href:!0});var GGr=s(R$);rFo=r(GGr,"OpenAIGPTForSequenceClassification"),GGr.forEach(t),tFo=r(I4e," (OpenAI GPT model)"),I4e.forEach(t),aFo=i(L),Y2=n(L,"LI",{});var j4e=s(Y2);Zoe=n(j4e,"STRONG",{});var OGr=s(Zoe);nFo=r(OGr,"perceiver"),OGr.forEach(t),sFo=r(j4e," \u2014 "),S$=n(j4e,"A",{href:!0});var XGr=s(S$);lFo=r(XGr,"PerceiverForSequenceClassification"),XGr.forEach(t),iFo=r(j4e," (Perceiver model)"),j4e.forEach(t),dFo=i(L),K2=n(L,"LI",{});var N4e=s(K2);ere=n(N4e,"STRONG",{});var zGr=s(ere);cFo=r(zGr,"plbart"),zGr.forEach(t),fFo=r(N4e," \u2014 "),P$=n(N4e,"A",{href:!0});var VGr=s(P$);mFo=r(VGr,"PLBartForSequenceClassification"),VGr.forEach(t),gFo=r(N4e," (PLBart model)"),N4e.forEach(t),hFo=i(L),Z2=n(L,"LI",{});var D4e=s(Z2);ore=n(D4e,"STRONG",{});var WGr=s(ore);pFo=r(WGr,"qdqbert"),WGr.forEach(t),_Fo=r(D4e," \u2014 "),$$=n(D4e,"A",{href:!0});var QGr=s($$);uFo=r(QGr,"QDQBertForSequenceClassification"),QGr.forEach(t),bFo=r(D4e," (QDQBert model)"),D4e.forEach(t),vFo=i(L),e1=n(L,"LI",{});var q4e=s(e1);rre=n(q4e,"STRONG",{});var HGr=s(rre);TFo=r(HGr,"reformer"),HGr.forEach(t),FFo=r(q4e," \u2014 "),I$=n(q4e,"A",{href:!0});var UGr=s(I$);CFo=r(UGr,"ReformerForSequenceClassification"),UGr.forEach(t),MFo=r(q4e," (Reformer model)"),q4e.forEach(t),EFo=i(L),o1=n(L,"LI",{});var G4e=s(o1);tre=n(G4e,"STRONG",{});var JGr=s(tre);yFo=r(JGr,"rembert"),JGr.forEach(t),wFo=r(G4e," \u2014 "),j$=n(G4e,"A",{href:!0});var YGr=s(j$);AFo=r(YGr,"RemBertForSequenceClassification"),YGr.forEach(t),LFo=r(G4e," (RemBERT model)"),G4e.forEach(t),BFo=i(L),r1=n(L,"LI",{});var O4e=s(r1);are=n(O4e,"STRONG",{});var KGr=s(are);kFo=r(KGr,"roberta"),KGr.forEach(t),xFo=r(O4e," \u2014 "),N$=n(O4e,"A",{href:!0});var ZGr=s(N$);RFo=r(ZGr,"RobertaForSequenceClassification"),ZGr.forEach(t),SFo=r(O4e," (RoBERTa model)"),O4e.forEach(t),PFo=i(L),t1=n(L,"LI",{});var X4e=s(t1);nre=n(X4e,"STRONG",{});var eOr=s(nre);$Fo=r(eOr,"roformer"),eOr.forEach(t),IFo=r(X4e," \u2014 "),D$=n(X4e,"A",{href:!0});var oOr=s(D$);jFo=r(oOr,"RoFormerForSequenceClassification"),oOr.forEach(t),NFo=r(X4e," (RoFormer model)"),X4e.forEach(t),DFo=i(L),a1=n(L,"LI",{});var z4e=s(a1);sre=n(z4e,"STRONG",{});var rOr=s(sre);qFo=r(rOr,"squeezebert"),rOr.forEach(t),GFo=r(z4e," \u2014 "),q$=n(z4e,"A",{href:!0});var tOr=s(q$);OFo=r(tOr,"SqueezeBertForSequenceClassification"),tOr.forEach(t),XFo=r(z4e," (SqueezeBERT model)"),z4e.forEach(t),zFo=i(L),n1=n(L,"LI",{});var V4e=s(n1);lre=n(V4e,"STRONG",{});var aOr=s(lre);VFo=r(aOr,"tapas"),aOr.forEach(t),WFo=r(V4e," \u2014 "),G$=n(V4e,"A",{href:!0});var nOr=s(G$);QFo=r(nOr,"TapasForSequenceClassification"),nOr.forEach(t),HFo=r(V4e," (TAPAS model)"),V4e.forEach(t),UFo=i(L),s1=n(L,"LI",{});var W4e=s(s1);ire=n(W4e,"STRONG",{});var sOr=s(ire);JFo=r(sOr,"transfo-xl"),sOr.forEach(t),YFo=r(W4e," \u2014 "),O$=n(W4e,"A",{href:!0});var lOr=s(O$);KFo=r(lOr,"TransfoXLForSequenceClassification"),lOr.forEach(t),ZFo=r(W4e," (Transformer-XL model)"),W4e.forEach(t),e9o=i(L),l1=n(L,"LI",{});var Q4e=s(l1);dre=n(Q4e,"STRONG",{});var iOr=s(dre);o9o=r(iOr,"xlm"),iOr.forEach(t),r9o=r(Q4e," \u2014 "),X$=n(Q4e,"A",{href:!0});var dOr=s(X$);t9o=r(dOr,"XLMForSequenceClassification"),dOr.forEach(t),a9o=r(Q4e," (XLM model)"),Q4e.forEach(t),n9o=i(L),i1=n(L,"LI",{});var H4e=s(i1);cre=n(H4e,"STRONG",{});var cOr=s(cre);s9o=r(cOr,"xlm-roberta"),cOr.forEach(t),l9o=r(H4e," \u2014 "),z$=n(H4e,"A",{href:!0});var fOr=s(z$);i9o=r(fOr,"XLMRobertaForSequenceClassification"),fOr.forEach(t),d9o=r(H4e," (XLM-RoBERTa model)"),H4e.forEach(t),c9o=i(L),d1=n(L,"LI",{});var U4e=s(d1);fre=n(U4e,"STRONG",{});var mOr=s(fre);f9o=r(mOr,"xlm-roberta-xl"),mOr.forEach(t),m9o=r(U4e," \u2014 "),V$=n(U4e,"A",{href:!0});var gOr=s(V$);g9o=r(gOr,"XLMRobertaXLForSequenceClassification"),gOr.forEach(t),h9o=r(U4e," (XLM-RoBERTa-XL model)"),U4e.forEach(t),p9o=i(L),c1=n(L,"LI",{});var J4e=s(c1);mre=n(J4e,"STRONG",{});var hOr=s(mre);_9o=r(hOr,"xlnet"),hOr.forEach(t),u9o=r(J4e," \u2014 "),W$=n(J4e,"A",{href:!0});var pOr=s(W$);b9o=r(pOr,"XLNetForSequenceClassification"),pOr.forEach(t),v9o=r(J4e," (XLNet model)"),J4e.forEach(t),T9o=i(L),f1=n(L,"LI",{});var Y4e=s(f1);gre=n(Y4e,"STRONG",{});var _Or=s(gre);F9o=r(_Or,"yoso"),_Or.forEach(t),C9o=r(Y4e," \u2014 "),Q$=n(Y4e,"A",{href:!0});var uOr=s(Q$);M9o=r(uOr,"YosoForSequenceClassification"),uOr.forEach(t),E9o=r(Y4e," (YOSO model)"),Y4e.forEach(t),L.forEach(t),y9o=i(Nt),m1=n(Nt,"P",{});var K4e=s(m1);w9o=r(K4e,"The model is set in evaluation mode by default using "),hre=n(K4e,"CODE",{});var bOr=s(hre);A9o=r(bOr,"model.eval()"),bOr.forEach(t),L9o=r(K4e,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),pre=n(K4e,"CODE",{});var vOr=s(pre);B9o=r(vOr,"model.train()"),vOr.forEach(t),K4e.forEach(t),k9o=i(Nt),_re=n(Nt,"P",{});var TOr=s(_re);x9o=r(TOr,"Examples:"),TOr.forEach(t),R9o=i(Nt),m(CE.$$.fragment,Nt),Nt.forEach(t),Hs.forEach(t),LLe=i(d),ad=n(d,"H2",{class:!0});var $Be=s(ad);g1=n($Be,"A",{id:!0,class:!0,href:!0});var FOr=s(g1);ure=n(FOr,"SPAN",{});var COr=s(ure);m(ME.$$.fragment,COr),COr.forEach(t),FOr.forEach(t),S9o=i($Be),bre=n($Be,"SPAN",{});var MOr=s(bre);P9o=r(MOr,"AutoModelForMultipleChoice"),MOr.forEach(t),$Be.forEach(t),BLe=i(d),Yo=n(d,"DIV",{class:!0});var Js=s(Yo);m(EE.$$.fragment,Js),$9o=i(Js),nd=n(Js,"P",{});var az=s(nd);I9o=r(az,`This is a generic model class that will be instantiated as one of the model classes of the library (with a multiple choice head) when created
with the `),vre=n(az,"CODE",{});var EOr=s(vre);j9o=r(EOr,"from_pretrained()"),EOr.forEach(t),N9o=r(az,"class method or the "),Tre=n(az,"CODE",{});var yOr=s(Tre);D9o=r(yOr,"from_config()"),yOr.forEach(t),q9o=r(az,`class
method.`),az.forEach(t),G9o=i(Js),yE=n(Js,"P",{});var IBe=s(yE);O9o=r(IBe,"This class cannot be instantiated directly using "),Fre=n(IBe,"CODE",{});var wOr=s(Fre);X9o=r(wOr,"__init__()"),wOr.forEach(t),z9o=r(IBe," (throws an error)."),IBe.forEach(t),V9o=i(Js),zr=n(Js,"DIV",{class:!0});var Ys=s(zr);m(wE.$$.fragment,Ys),W9o=i(Ys),Cre=n(Ys,"P",{});var AOr=s(Cre);Q9o=r(AOr,"Instantiates one of the model classes of the library (with a multiple choice head) from a configuration."),AOr.forEach(t),H9o=i(Ys),sd=n(Ys,"P",{});var nz=s(sd);U9o=r(nz,`Note:
Loading a model from its configuration file does `),Mre=n(nz,"STRONG",{});var LOr=s(Mre);J9o=r(LOr,"not"),LOr.forEach(t),Y9o=r(nz,` load the model weights. It only affects the
model\u2019s configuration. Use `),Ere=n(nz,"CODE",{});var BOr=s(Ere);K9o=r(BOr,"from_pretrained()"),BOr.forEach(t),Z9o=r(nz,"to load the model weights."),nz.forEach(t),eCo=i(Ys),yre=n(Ys,"P",{});var kOr=s(yre);oCo=r(kOr,"Examples:"),kOr.forEach(t),rCo=i(Ys),m(AE.$$.fragment,Ys),Ys.forEach(t),tCo=i(Js),Ie=n(Js,"DIV",{class:!0});var Dt=s(Ie);m(LE.$$.fragment,Dt),aCo=i(Dt),wre=n(Dt,"P",{});var xOr=s(wre);nCo=r(xOr,"Instantiate one of the model classes of the library (with a multiple choice head) from a pretrained model."),xOr.forEach(t),sCo=i(Dt),Va=n(Dt,"P",{});var g4=s(Va);lCo=r(g4,"The model class to instantiate is selected based on the "),Are=n(g4,"CODE",{});var ROr=s(Are);iCo=r(ROr,"model_type"),ROr.forEach(t),dCo=r(g4,` property of the config object (either
passed as an argument or loaded from `),Lre=n(g4,"CODE",{});var SOr=s(Lre);cCo=r(SOr,"pretrained_model_name_or_path"),SOr.forEach(t),fCo=r(g4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Bre=n(g4,"CODE",{});var POr=s(Bre);mCo=r(POr,"pretrained_model_name_or_path"),POr.forEach(t),gCo=r(g4,":"),g4.forEach(t),hCo=i(Dt),G=n(Dt,"UL",{});var O=s(G);h1=n(O,"LI",{});var Z4e=s(h1);kre=n(Z4e,"STRONG",{});var $Or=s(kre);pCo=r($Or,"albert"),$Or.forEach(t),_Co=r(Z4e," \u2014 "),H$=n(Z4e,"A",{href:!0});var IOr=s(H$);uCo=r(IOr,"AlbertForMultipleChoice"),IOr.forEach(t),bCo=r(Z4e," (ALBERT model)"),Z4e.forEach(t),vCo=i(O),p1=n(O,"LI",{});var eMe=s(p1);xre=n(eMe,"STRONG",{});var jOr=s(xre);TCo=r(jOr,"bert"),jOr.forEach(t),FCo=r(eMe," \u2014 "),U$=n(eMe,"A",{href:!0});var NOr=s(U$);CCo=r(NOr,"BertForMultipleChoice"),NOr.forEach(t),MCo=r(eMe," (BERT model)"),eMe.forEach(t),ECo=i(O),_1=n(O,"LI",{});var oMe=s(_1);Rre=n(oMe,"STRONG",{});var DOr=s(Rre);yCo=r(DOr,"big_bird"),DOr.forEach(t),wCo=r(oMe," \u2014 "),J$=n(oMe,"A",{href:!0});var qOr=s(J$);ACo=r(qOr,"BigBirdForMultipleChoice"),qOr.forEach(t),LCo=r(oMe," (BigBird model)"),oMe.forEach(t),BCo=i(O),u1=n(O,"LI",{});var rMe=s(u1);Sre=n(rMe,"STRONG",{});var GOr=s(Sre);kCo=r(GOr,"camembert"),GOr.forEach(t),xCo=r(rMe," \u2014 "),Y$=n(rMe,"A",{href:!0});var OOr=s(Y$);RCo=r(OOr,"CamembertForMultipleChoice"),OOr.forEach(t),SCo=r(rMe," (CamemBERT model)"),rMe.forEach(t),PCo=i(O),b1=n(O,"LI",{});var tMe=s(b1);Pre=n(tMe,"STRONG",{});var XOr=s(Pre);$Co=r(XOr,"canine"),XOr.forEach(t),ICo=r(tMe," \u2014 "),K$=n(tMe,"A",{href:!0});var zOr=s(K$);jCo=r(zOr,"CanineForMultipleChoice"),zOr.forEach(t),NCo=r(tMe," (Canine model)"),tMe.forEach(t),DCo=i(O),v1=n(O,"LI",{});var aMe=s(v1);$re=n(aMe,"STRONG",{});var VOr=s($re);qCo=r(VOr,"convbert"),VOr.forEach(t),GCo=r(aMe," \u2014 "),Z$=n(aMe,"A",{href:!0});var WOr=s(Z$);OCo=r(WOr,"ConvBertForMultipleChoice"),WOr.forEach(t),XCo=r(aMe," (ConvBERT model)"),aMe.forEach(t),zCo=i(O),T1=n(O,"LI",{});var nMe=s(T1);Ire=n(nMe,"STRONG",{});var QOr=s(Ire);VCo=r(QOr,"distilbert"),QOr.forEach(t),WCo=r(nMe," \u2014 "),eI=n(nMe,"A",{href:!0});var HOr=s(eI);QCo=r(HOr,"DistilBertForMultipleChoice"),HOr.forEach(t),HCo=r(nMe," (DistilBERT model)"),nMe.forEach(t),UCo=i(O),F1=n(O,"LI",{});var sMe=s(F1);jre=n(sMe,"STRONG",{});var UOr=s(jre);JCo=r(UOr,"electra"),UOr.forEach(t),YCo=r(sMe," \u2014 "),oI=n(sMe,"A",{href:!0});var JOr=s(oI);KCo=r(JOr,"ElectraForMultipleChoice"),JOr.forEach(t),ZCo=r(sMe," (ELECTRA model)"),sMe.forEach(t),e4o=i(O),C1=n(O,"LI",{});var lMe=s(C1);Nre=n(lMe,"STRONG",{});var YOr=s(Nre);o4o=r(YOr,"flaubert"),YOr.forEach(t),r4o=r(lMe," \u2014 "),rI=n(lMe,"A",{href:!0});var KOr=s(rI);t4o=r(KOr,"FlaubertForMultipleChoice"),KOr.forEach(t),a4o=r(lMe," (FlauBERT model)"),lMe.forEach(t),n4o=i(O),M1=n(O,"LI",{});var iMe=s(M1);Dre=n(iMe,"STRONG",{});var ZOr=s(Dre);s4o=r(ZOr,"fnet"),ZOr.forEach(t),l4o=r(iMe," \u2014 "),tI=n(iMe,"A",{href:!0});var eXr=s(tI);i4o=r(eXr,"FNetForMultipleChoice"),eXr.forEach(t),d4o=r(iMe," (FNet model)"),iMe.forEach(t),c4o=i(O),E1=n(O,"LI",{});var dMe=s(E1);qre=n(dMe,"STRONG",{});var oXr=s(qre);f4o=r(oXr,"funnel"),oXr.forEach(t),m4o=r(dMe," \u2014 "),aI=n(dMe,"A",{href:!0});var rXr=s(aI);g4o=r(rXr,"FunnelForMultipleChoice"),rXr.forEach(t),h4o=r(dMe," (Funnel Transformer model)"),dMe.forEach(t),p4o=i(O),y1=n(O,"LI",{});var cMe=s(y1);Gre=n(cMe,"STRONG",{});var tXr=s(Gre);_4o=r(tXr,"ibert"),tXr.forEach(t),u4o=r(cMe," \u2014 "),nI=n(cMe,"A",{href:!0});var aXr=s(nI);b4o=r(aXr,"IBertForMultipleChoice"),aXr.forEach(t),v4o=r(cMe," (I-BERT model)"),cMe.forEach(t),T4o=i(O),w1=n(O,"LI",{});var fMe=s(w1);Ore=n(fMe,"STRONG",{});var nXr=s(Ore);F4o=r(nXr,"longformer"),nXr.forEach(t),C4o=r(fMe," \u2014 "),sI=n(fMe,"A",{href:!0});var sXr=s(sI);M4o=r(sXr,"LongformerForMultipleChoice"),sXr.forEach(t),E4o=r(fMe," (Longformer model)"),fMe.forEach(t),y4o=i(O),A1=n(O,"LI",{});var mMe=s(A1);Xre=n(mMe,"STRONG",{});var lXr=s(Xre);w4o=r(lXr,"megatron-bert"),lXr.forEach(t),A4o=r(mMe," \u2014 "),lI=n(mMe,"A",{href:!0});var iXr=s(lI);L4o=r(iXr,"MegatronBertForMultipleChoice"),iXr.forEach(t),B4o=r(mMe," (MegatronBert model)"),mMe.forEach(t),k4o=i(O),L1=n(O,"LI",{});var gMe=s(L1);zre=n(gMe,"STRONG",{});var dXr=s(zre);x4o=r(dXr,"mobilebert"),dXr.forEach(t),R4o=r(gMe," \u2014 "),iI=n(gMe,"A",{href:!0});var cXr=s(iI);S4o=r(cXr,"MobileBertForMultipleChoice"),cXr.forEach(t),P4o=r(gMe," (MobileBERT model)"),gMe.forEach(t),$4o=i(O),B1=n(O,"LI",{});var hMe=s(B1);Vre=n(hMe,"STRONG",{});var fXr=s(Vre);I4o=r(fXr,"mpnet"),fXr.forEach(t),j4o=r(hMe," \u2014 "),dI=n(hMe,"A",{href:!0});var mXr=s(dI);N4o=r(mXr,"MPNetForMultipleChoice"),mXr.forEach(t),D4o=r(hMe," (MPNet model)"),hMe.forEach(t),q4o=i(O),k1=n(O,"LI",{});var pMe=s(k1);Wre=n(pMe,"STRONG",{});var gXr=s(Wre);G4o=r(gXr,"nystromformer"),gXr.forEach(t),O4o=r(pMe," \u2014 "),cI=n(pMe,"A",{href:!0});var hXr=s(cI);X4o=r(hXr,"NystromformerForMultipleChoice"),hXr.forEach(t),z4o=r(pMe," (Nystromformer model)"),pMe.forEach(t),V4o=i(O),x1=n(O,"LI",{});var _Me=s(x1);Qre=n(_Me,"STRONG",{});var pXr=s(Qre);W4o=r(pXr,"qdqbert"),pXr.forEach(t),Q4o=r(_Me," \u2014 "),fI=n(_Me,"A",{href:!0});var _Xr=s(fI);H4o=r(_Xr,"QDQBertForMultipleChoice"),_Xr.forEach(t),U4o=r(_Me," (QDQBert model)"),_Me.forEach(t),J4o=i(O),R1=n(O,"LI",{});var uMe=s(R1);Hre=n(uMe,"STRONG",{});var uXr=s(Hre);Y4o=r(uXr,"rembert"),uXr.forEach(t),K4o=r(uMe," \u2014 "),mI=n(uMe,"A",{href:!0});var bXr=s(mI);Z4o=r(bXr,"RemBertForMultipleChoice"),bXr.forEach(t),eMo=r(uMe," (RemBERT model)"),uMe.forEach(t),oMo=i(O),S1=n(O,"LI",{});var bMe=s(S1);Ure=n(bMe,"STRONG",{});var vXr=s(Ure);rMo=r(vXr,"roberta"),vXr.forEach(t),tMo=r(bMe," \u2014 "),gI=n(bMe,"A",{href:!0});var TXr=s(gI);aMo=r(TXr,"RobertaForMultipleChoice"),TXr.forEach(t),nMo=r(bMe," (RoBERTa model)"),bMe.forEach(t),sMo=i(O),P1=n(O,"LI",{});var vMe=s(P1);Jre=n(vMe,"STRONG",{});var FXr=s(Jre);lMo=r(FXr,"roformer"),FXr.forEach(t),iMo=r(vMe," \u2014 "),hI=n(vMe,"A",{href:!0});var CXr=s(hI);dMo=r(CXr,"RoFormerForMultipleChoice"),CXr.forEach(t),cMo=r(vMe," (RoFormer model)"),vMe.forEach(t),fMo=i(O),$1=n(O,"LI",{});var TMe=s($1);Yre=n(TMe,"STRONG",{});var MXr=s(Yre);mMo=r(MXr,"squeezebert"),MXr.forEach(t),gMo=r(TMe," \u2014 "),pI=n(TMe,"A",{href:!0});var EXr=s(pI);hMo=r(EXr,"SqueezeBertForMultipleChoice"),EXr.forEach(t),pMo=r(TMe," (SqueezeBERT model)"),TMe.forEach(t),_Mo=i(O),I1=n(O,"LI",{});var FMe=s(I1);Kre=n(FMe,"STRONG",{});var yXr=s(Kre);uMo=r(yXr,"xlm"),yXr.forEach(t),bMo=r(FMe," \u2014 "),_I=n(FMe,"A",{href:!0});var wXr=s(_I);vMo=r(wXr,"XLMForMultipleChoice"),wXr.forEach(t),TMo=r(FMe," (XLM model)"),FMe.forEach(t),FMo=i(O),j1=n(O,"LI",{});var CMe=s(j1);Zre=n(CMe,"STRONG",{});var AXr=s(Zre);CMo=r(AXr,"xlm-roberta"),AXr.forEach(t),MMo=r(CMe," \u2014 "),uI=n(CMe,"A",{href:!0});var LXr=s(uI);EMo=r(LXr,"XLMRobertaForMultipleChoice"),LXr.forEach(t),yMo=r(CMe," (XLM-RoBERTa model)"),CMe.forEach(t),wMo=i(O),N1=n(O,"LI",{});var MMe=s(N1);ete=n(MMe,"STRONG",{});var BXr=s(ete);AMo=r(BXr,"xlm-roberta-xl"),BXr.forEach(t),LMo=r(MMe," \u2014 "),bI=n(MMe,"A",{href:!0});var kXr=s(bI);BMo=r(kXr,"XLMRobertaXLForMultipleChoice"),kXr.forEach(t),kMo=r(MMe," (XLM-RoBERTa-XL model)"),MMe.forEach(t),xMo=i(O),D1=n(O,"LI",{});var EMe=s(D1);ote=n(EMe,"STRONG",{});var xXr=s(ote);RMo=r(xXr,"xlnet"),xXr.forEach(t),SMo=r(EMe," \u2014 "),vI=n(EMe,"A",{href:!0});var RXr=s(vI);PMo=r(RXr,"XLNetForMultipleChoice"),RXr.forEach(t),$Mo=r(EMe," (XLNet model)"),EMe.forEach(t),IMo=i(O),q1=n(O,"LI",{});var yMe=s(q1);rte=n(yMe,"STRONG",{});var SXr=s(rte);jMo=r(SXr,"yoso"),SXr.forEach(t),NMo=r(yMe," \u2014 "),TI=n(yMe,"A",{href:!0});var PXr=s(TI);DMo=r(PXr,"YosoForMultipleChoice"),PXr.forEach(t),qMo=r(yMe," (YOSO model)"),yMe.forEach(t),O.forEach(t),GMo=i(Dt),G1=n(Dt,"P",{});var wMe=s(G1);OMo=r(wMe,"The model is set in evaluation mode by default using "),tte=n(wMe,"CODE",{});var $Xr=s(tte);XMo=r($Xr,"model.eval()"),$Xr.forEach(t),zMo=r(wMe,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),ate=n(wMe,"CODE",{});var IXr=s(ate);VMo=r(IXr,"model.train()"),IXr.forEach(t),wMe.forEach(t),WMo=i(Dt),nte=n(Dt,"P",{});var jXr=s(nte);QMo=r(jXr,"Examples:"),jXr.forEach(t),HMo=i(Dt),m(BE.$$.fragment,Dt),Dt.forEach(t),Js.forEach(t),kLe=i(d),ld=n(d,"H2",{class:!0});var jBe=s(ld);O1=n(jBe,"A",{id:!0,class:!0,href:!0});var NXr=s(O1);ste=n(NXr,"SPAN",{});var DXr=s(ste);m(kE.$$.fragment,DXr),DXr.forEach(t),NXr.forEach(t),UMo=i(jBe),lte=n(jBe,"SPAN",{});var qXr=s(lte);JMo=r(qXr,"AutoModelForNextSentencePrediction"),qXr.forEach(t),jBe.forEach(t),xLe=i(d),Ko=n(d,"DIV",{class:!0});var Ks=s(Ko);m(xE.$$.fragment,Ks),YMo=i(Ks),id=n(Ks,"P",{});var sz=s(id);KMo=r(sz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a next sentence prediction head) when created
with the `),ite=n(sz,"CODE",{});var GXr=s(ite);ZMo=r(GXr,"from_pretrained()"),GXr.forEach(t),eEo=r(sz,"class method or the "),dte=n(sz,"CODE",{});var OXr=s(dte);oEo=r(OXr,"from_config()"),OXr.forEach(t),rEo=r(sz,`class
method.`),sz.forEach(t),tEo=i(Ks),RE=n(Ks,"P",{});var NBe=s(RE);aEo=r(NBe,"This class cannot be instantiated directly using "),cte=n(NBe,"CODE",{});var XXr=s(cte);nEo=r(XXr,"__init__()"),XXr.forEach(t),sEo=r(NBe," (throws an error)."),NBe.forEach(t),lEo=i(Ks),Vr=n(Ks,"DIV",{class:!0});var Zs=s(Vr);m(SE.$$.fragment,Zs),iEo=i(Zs),fte=n(Zs,"P",{});var zXr=s(fte);dEo=r(zXr,"Instantiates one of the model classes of the library (with a next sentence prediction head) from a configuration."),zXr.forEach(t),cEo=i(Zs),dd=n(Zs,"P",{});var lz=s(dd);fEo=r(lz,`Note:
Loading a model from its configuration file does `),mte=n(lz,"STRONG",{});var VXr=s(mte);mEo=r(VXr,"not"),VXr.forEach(t),gEo=r(lz,` load the model weights. It only affects the
model\u2019s configuration. Use `),gte=n(lz,"CODE",{});var WXr=s(gte);hEo=r(WXr,"from_pretrained()"),WXr.forEach(t),pEo=r(lz,"to load the model weights."),lz.forEach(t),_Eo=i(Zs),hte=n(Zs,"P",{});var QXr=s(hte);uEo=r(QXr,"Examples:"),QXr.forEach(t),bEo=i(Zs),m(PE.$$.fragment,Zs),Zs.forEach(t),vEo=i(Ks),je=n(Ks,"DIV",{class:!0});var qt=s(je);m($E.$$.fragment,qt),TEo=i(qt),pte=n(qt,"P",{});var HXr=s(pte);FEo=r(HXr,"Instantiate one of the model classes of the library (with a next sentence prediction head) from a pretrained model."),HXr.forEach(t),CEo=i(qt),Wa=n(qt,"P",{});var h4=s(Wa);MEo=r(h4,"The model class to instantiate is selected based on the "),_te=n(h4,"CODE",{});var UXr=s(_te);EEo=r(UXr,"model_type"),UXr.forEach(t),yEo=r(h4,` property of the config object (either
passed as an argument or loaded from `),ute=n(h4,"CODE",{});var JXr=s(ute);wEo=r(JXr,"pretrained_model_name_or_path"),JXr.forEach(t),AEo=r(h4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),bte=n(h4,"CODE",{});var YXr=s(bte);LEo=r(YXr,"pretrained_model_name_or_path"),YXr.forEach(t),BEo=r(h4,":"),h4.forEach(t),kEo=i(qt),na=n(qt,"UL",{});var el=s(na);X1=n(el,"LI",{});var AMe=s(X1);vte=n(AMe,"STRONG",{});var KXr=s(vte);xEo=r(KXr,"bert"),KXr.forEach(t),REo=r(AMe," \u2014 "),FI=n(AMe,"A",{href:!0});var ZXr=s(FI);SEo=r(ZXr,"BertForNextSentencePrediction"),ZXr.forEach(t),PEo=r(AMe," (BERT model)"),AMe.forEach(t),$Eo=i(el),z1=n(el,"LI",{});var LMe=s(z1);Tte=n(LMe,"STRONG",{});var ezr=s(Tte);IEo=r(ezr,"fnet"),ezr.forEach(t),jEo=r(LMe," \u2014 "),CI=n(LMe,"A",{href:!0});var ozr=s(CI);NEo=r(ozr,"FNetForNextSentencePrediction"),ozr.forEach(t),DEo=r(LMe," (FNet model)"),LMe.forEach(t),qEo=i(el),V1=n(el,"LI",{});var BMe=s(V1);Fte=n(BMe,"STRONG",{});var rzr=s(Fte);GEo=r(rzr,"megatron-bert"),rzr.forEach(t),OEo=r(BMe," \u2014 "),MI=n(BMe,"A",{href:!0});var tzr=s(MI);XEo=r(tzr,"MegatronBertForNextSentencePrediction"),tzr.forEach(t),zEo=r(BMe," (MegatronBert model)"),BMe.forEach(t),VEo=i(el),W1=n(el,"LI",{});var kMe=s(W1);Cte=n(kMe,"STRONG",{});var azr=s(Cte);WEo=r(azr,"mobilebert"),azr.forEach(t),QEo=r(kMe," \u2014 "),EI=n(kMe,"A",{href:!0});var nzr=s(EI);HEo=r(nzr,"MobileBertForNextSentencePrediction"),nzr.forEach(t),UEo=r(kMe," (MobileBERT model)"),kMe.forEach(t),JEo=i(el),Q1=n(el,"LI",{});var xMe=s(Q1);Mte=n(xMe,"STRONG",{});var szr=s(Mte);YEo=r(szr,"qdqbert"),szr.forEach(t),KEo=r(xMe," \u2014 "),yI=n(xMe,"A",{href:!0});var lzr=s(yI);ZEo=r(lzr,"QDQBertForNextSentencePrediction"),lzr.forEach(t),e3o=r(xMe," (QDQBert model)"),xMe.forEach(t),el.forEach(t),o3o=i(qt),H1=n(qt,"P",{});var RMe=s(H1);r3o=r(RMe,"The model is set in evaluation mode by default using "),Ete=n(RMe,"CODE",{});var izr=s(Ete);t3o=r(izr,"model.eval()"),izr.forEach(t),a3o=r(RMe,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),yte=n(RMe,"CODE",{});var dzr=s(yte);n3o=r(dzr,"model.train()"),dzr.forEach(t),RMe.forEach(t),s3o=i(qt),wte=n(qt,"P",{});var czr=s(wte);l3o=r(czr,"Examples:"),czr.forEach(t),i3o=i(qt),m(IE.$$.fragment,qt),qt.forEach(t),Ks.forEach(t),RLe=i(d),cd=n(d,"H2",{class:!0});var DBe=s(cd);U1=n(DBe,"A",{id:!0,class:!0,href:!0});var fzr=s(U1);Ate=n(fzr,"SPAN",{});var mzr=s(Ate);m(jE.$$.fragment,mzr),mzr.forEach(t),fzr.forEach(t),d3o=i(DBe),Lte=n(DBe,"SPAN",{});var gzr=s(Lte);c3o=r(gzr,"AutoModelForTokenClassification"),gzr.forEach(t),DBe.forEach(t),SLe=i(d),Zo=n(d,"DIV",{class:!0});var ol=s(Zo);m(NE.$$.fragment,ol),f3o=i(ol),fd=n(ol,"P",{});var iz=s(fd);m3o=r(iz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a token classification head) when created
with the `),Bte=n(iz,"CODE",{});var hzr=s(Bte);g3o=r(hzr,"from_pretrained()"),hzr.forEach(t),h3o=r(iz,"class method or the "),kte=n(iz,"CODE",{});var pzr=s(kte);p3o=r(pzr,"from_config()"),pzr.forEach(t),_3o=r(iz,`class
method.`),iz.forEach(t),u3o=i(ol),DE=n(ol,"P",{});var qBe=s(DE);b3o=r(qBe,"This class cannot be instantiated directly using "),xte=n(qBe,"CODE",{});var _zr=s(xte);v3o=r(_zr,"__init__()"),_zr.forEach(t),T3o=r(qBe," (throws an error)."),qBe.forEach(t),F3o=i(ol),Wr=n(ol,"DIV",{class:!0});var rl=s(Wr);m(qE.$$.fragment,rl),C3o=i(rl),Rte=n(rl,"P",{});var uzr=s(Rte);M3o=r(uzr,"Instantiates one of the model classes of the library (with a token classification head) from a configuration."),uzr.forEach(t),E3o=i(rl),md=n(rl,"P",{});var dz=s(md);y3o=r(dz,`Note:
Loading a model from its configuration file does `),Ste=n(dz,"STRONG",{});var bzr=s(Ste);w3o=r(bzr,"not"),bzr.forEach(t),A3o=r(dz,` load the model weights. It only affects the
model\u2019s configuration. Use `),Pte=n(dz,"CODE",{});var vzr=s(Pte);L3o=r(vzr,"from_pretrained()"),vzr.forEach(t),B3o=r(dz,"to load the model weights."),dz.forEach(t),k3o=i(rl),$te=n(rl,"P",{});var Tzr=s($te);x3o=r(Tzr,"Examples:"),Tzr.forEach(t),R3o=i(rl),m(GE.$$.fragment,rl),rl.forEach(t),S3o=i(ol),Ne=n(ol,"DIV",{class:!0});var Gt=s(Ne);m(OE.$$.fragment,Gt),P3o=i(Gt),Ite=n(Gt,"P",{});var Fzr=s(Ite);$3o=r(Fzr,"Instantiate one of the model classes of the library (with a token classification head) from a pretrained model."),Fzr.forEach(t),I3o=i(Gt),Qa=n(Gt,"P",{});var p4=s(Qa);j3o=r(p4,"The model class to instantiate is selected based on the "),jte=n(p4,"CODE",{});var Czr=s(jte);N3o=r(Czr,"model_type"),Czr.forEach(t),D3o=r(p4,` property of the config object (either
passed as an argument or loaded from `),Nte=n(p4,"CODE",{});var Mzr=s(Nte);q3o=r(Mzr,"pretrained_model_name_or_path"),Mzr.forEach(t),G3o=r(p4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Dte=n(p4,"CODE",{});var Ezr=s(Dte);O3o=r(Ezr,"pretrained_model_name_or_path"),Ezr.forEach(t),X3o=r(p4,":"),p4.forEach(t),z3o=i(Gt),D=n(Gt,"UL",{});var q=s(D);J1=n(q,"LI",{});var SMe=s(J1);qte=n(SMe,"STRONG",{});var yzr=s(qte);V3o=r(yzr,"albert"),yzr.forEach(t),W3o=r(SMe," \u2014 "),wI=n(SMe,"A",{href:!0});var wzr=s(wI);Q3o=r(wzr,"AlbertForTokenClassification"),wzr.forEach(t),H3o=r(SMe," (ALBERT model)"),SMe.forEach(t),U3o=i(q),Y1=n(q,"LI",{});var PMe=s(Y1);Gte=n(PMe,"STRONG",{});var Azr=s(Gte);J3o=r(Azr,"bert"),Azr.forEach(t),Y3o=r(PMe," \u2014 "),AI=n(PMe,"A",{href:!0});var Lzr=s(AI);K3o=r(Lzr,"BertForTokenClassification"),Lzr.forEach(t),Z3o=r(PMe," (BERT model)"),PMe.forEach(t),eyo=i(q),K1=n(q,"LI",{});var $Me=s(K1);Ote=n($Me,"STRONG",{});var Bzr=s(Ote);oyo=r(Bzr,"big_bird"),Bzr.forEach(t),ryo=r($Me," \u2014 "),LI=n($Me,"A",{href:!0});var kzr=s(LI);tyo=r(kzr,"BigBirdForTokenClassification"),kzr.forEach(t),ayo=r($Me," (BigBird model)"),$Me.forEach(t),nyo=i(q),Z1=n(q,"LI",{});var IMe=s(Z1);Xte=n(IMe,"STRONG",{});var xzr=s(Xte);syo=r(xzr,"camembert"),xzr.forEach(t),lyo=r(IMe," \u2014 "),BI=n(IMe,"A",{href:!0});var Rzr=s(BI);iyo=r(Rzr,"CamembertForTokenClassification"),Rzr.forEach(t),dyo=r(IMe," (CamemBERT model)"),IMe.forEach(t),cyo=i(q),eb=n(q,"LI",{});var jMe=s(eb);zte=n(jMe,"STRONG",{});var Szr=s(zte);fyo=r(Szr,"canine"),Szr.forEach(t),myo=r(jMe," \u2014 "),kI=n(jMe,"A",{href:!0});var Pzr=s(kI);gyo=r(Pzr,"CanineForTokenClassification"),Pzr.forEach(t),hyo=r(jMe," (Canine model)"),jMe.forEach(t),pyo=i(q),ob=n(q,"LI",{});var NMe=s(ob);Vte=n(NMe,"STRONG",{});var $zr=s(Vte);_yo=r($zr,"convbert"),$zr.forEach(t),uyo=r(NMe," \u2014 "),xI=n(NMe,"A",{href:!0});var Izr=s(xI);byo=r(Izr,"ConvBertForTokenClassification"),Izr.forEach(t),vyo=r(NMe," (ConvBERT model)"),NMe.forEach(t),Tyo=i(q),rb=n(q,"LI",{});var DMe=s(rb);Wte=n(DMe,"STRONG",{});var jzr=s(Wte);Fyo=r(jzr,"deberta"),jzr.forEach(t),Cyo=r(DMe," \u2014 "),RI=n(DMe,"A",{href:!0});var Nzr=s(RI);Myo=r(Nzr,"DebertaForTokenClassification"),Nzr.forEach(t),Eyo=r(DMe," (DeBERTa model)"),DMe.forEach(t),yyo=i(q),tb=n(q,"LI",{});var qMe=s(tb);Qte=n(qMe,"STRONG",{});var Dzr=s(Qte);wyo=r(Dzr,"deberta-v2"),Dzr.forEach(t),Ayo=r(qMe," \u2014 "),SI=n(qMe,"A",{href:!0});var qzr=s(SI);Lyo=r(qzr,"DebertaV2ForTokenClassification"),qzr.forEach(t),Byo=r(qMe," (DeBERTa-v2 model)"),qMe.forEach(t),kyo=i(q),ab=n(q,"LI",{});var GMe=s(ab);Hte=n(GMe,"STRONG",{});var Gzr=s(Hte);xyo=r(Gzr,"distilbert"),Gzr.forEach(t),Ryo=r(GMe," \u2014 "),PI=n(GMe,"A",{href:!0});var Ozr=s(PI);Syo=r(Ozr,"DistilBertForTokenClassification"),Ozr.forEach(t),Pyo=r(GMe," (DistilBERT model)"),GMe.forEach(t),$yo=i(q),nb=n(q,"LI",{});var OMe=s(nb);Ute=n(OMe,"STRONG",{});var Xzr=s(Ute);Iyo=r(Xzr,"electra"),Xzr.forEach(t),jyo=r(OMe," \u2014 "),$I=n(OMe,"A",{href:!0});var zzr=s($I);Nyo=r(zzr,"ElectraForTokenClassification"),zzr.forEach(t),Dyo=r(OMe," (ELECTRA model)"),OMe.forEach(t),qyo=i(q),sb=n(q,"LI",{});var XMe=s(sb);Jte=n(XMe,"STRONG",{});var Vzr=s(Jte);Gyo=r(Vzr,"flaubert"),Vzr.forEach(t),Oyo=r(XMe," \u2014 "),II=n(XMe,"A",{href:!0});var Wzr=s(II);Xyo=r(Wzr,"FlaubertForTokenClassification"),Wzr.forEach(t),zyo=r(XMe," (FlauBERT model)"),XMe.forEach(t),Vyo=i(q),lb=n(q,"LI",{});var zMe=s(lb);Yte=n(zMe,"STRONG",{});var Qzr=s(Yte);Wyo=r(Qzr,"fnet"),Qzr.forEach(t),Qyo=r(zMe," \u2014 "),jI=n(zMe,"A",{href:!0});var Hzr=s(jI);Hyo=r(Hzr,"FNetForTokenClassification"),Hzr.forEach(t),Uyo=r(zMe," (FNet model)"),zMe.forEach(t),Jyo=i(q),ib=n(q,"LI",{});var VMe=s(ib);Kte=n(VMe,"STRONG",{});var Uzr=s(Kte);Yyo=r(Uzr,"funnel"),Uzr.forEach(t),Kyo=r(VMe," \u2014 "),NI=n(VMe,"A",{href:!0});var Jzr=s(NI);Zyo=r(Jzr,"FunnelForTokenClassification"),Jzr.forEach(t),ewo=r(VMe," (Funnel Transformer model)"),VMe.forEach(t),owo=i(q),db=n(q,"LI",{});var WMe=s(db);Zte=n(WMe,"STRONG",{});var Yzr=s(Zte);rwo=r(Yzr,"gpt2"),Yzr.forEach(t),two=r(WMe," \u2014 "),DI=n(WMe,"A",{href:!0});var Kzr=s(DI);awo=r(Kzr,"GPT2ForTokenClassification"),Kzr.forEach(t),nwo=r(WMe," (OpenAI GPT-2 model)"),WMe.forEach(t),swo=i(q),cb=n(q,"LI",{});var QMe=s(cb);eae=n(QMe,"STRONG",{});var Zzr=s(eae);lwo=r(Zzr,"ibert"),Zzr.forEach(t),iwo=r(QMe," \u2014 "),qI=n(QMe,"A",{href:!0});var eVr=s(qI);dwo=r(eVr,"IBertForTokenClassification"),eVr.forEach(t),cwo=r(QMe," (I-BERT model)"),QMe.forEach(t),fwo=i(q),fb=n(q,"LI",{});var HMe=s(fb);oae=n(HMe,"STRONG",{});var oVr=s(oae);mwo=r(oVr,"layoutlm"),oVr.forEach(t),gwo=r(HMe," \u2014 "),GI=n(HMe,"A",{href:!0});var rVr=s(GI);hwo=r(rVr,"LayoutLMForTokenClassification"),rVr.forEach(t),pwo=r(HMe," (LayoutLM model)"),HMe.forEach(t),_wo=i(q),mb=n(q,"LI",{});var UMe=s(mb);rae=n(UMe,"STRONG",{});var tVr=s(rae);uwo=r(tVr,"layoutlmv2"),tVr.forEach(t),bwo=r(UMe," \u2014 "),OI=n(UMe,"A",{href:!0});var aVr=s(OI);vwo=r(aVr,"LayoutLMv2ForTokenClassification"),aVr.forEach(t),Two=r(UMe," (LayoutLMv2 model)"),UMe.forEach(t),Fwo=i(q),gb=n(q,"LI",{});var JMe=s(gb);tae=n(JMe,"STRONG",{});var nVr=s(tae);Cwo=r(nVr,"longformer"),nVr.forEach(t),Mwo=r(JMe," \u2014 "),XI=n(JMe,"A",{href:!0});var sVr=s(XI);Ewo=r(sVr,"LongformerForTokenClassification"),sVr.forEach(t),ywo=r(JMe," (Longformer model)"),JMe.forEach(t),wwo=i(q),hb=n(q,"LI",{});var YMe=s(hb);aae=n(YMe,"STRONG",{});var lVr=s(aae);Awo=r(lVr,"megatron-bert"),lVr.forEach(t),Lwo=r(YMe," \u2014 "),zI=n(YMe,"A",{href:!0});var iVr=s(zI);Bwo=r(iVr,"MegatronBertForTokenClassification"),iVr.forEach(t),kwo=r(YMe," (MegatronBert model)"),YMe.forEach(t),xwo=i(q),pb=n(q,"LI",{});var KMe=s(pb);nae=n(KMe,"STRONG",{});var dVr=s(nae);Rwo=r(dVr,"mobilebert"),dVr.forEach(t),Swo=r(KMe," \u2014 "),VI=n(KMe,"A",{href:!0});var cVr=s(VI);Pwo=r(cVr,"MobileBertForTokenClassification"),cVr.forEach(t),$wo=r(KMe," (MobileBERT model)"),KMe.forEach(t),Iwo=i(q),_b=n(q,"LI",{});var ZMe=s(_b);sae=n(ZMe,"STRONG",{});var fVr=s(sae);jwo=r(fVr,"mpnet"),fVr.forEach(t),Nwo=r(ZMe," \u2014 "),WI=n(ZMe,"A",{href:!0});var mVr=s(WI);Dwo=r(mVr,"MPNetForTokenClassification"),mVr.forEach(t),qwo=r(ZMe," (MPNet model)"),ZMe.forEach(t),Gwo=i(q),ub=n(q,"LI",{});var eEe=s(ub);lae=n(eEe,"STRONG",{});var gVr=s(lae);Owo=r(gVr,"nystromformer"),gVr.forEach(t),Xwo=r(eEe," \u2014 "),QI=n(eEe,"A",{href:!0});var hVr=s(QI);zwo=r(hVr,"NystromformerForTokenClassification"),hVr.forEach(t),Vwo=r(eEe," (Nystromformer model)"),eEe.forEach(t),Wwo=i(q),bb=n(q,"LI",{});var oEe=s(bb);iae=n(oEe,"STRONG",{});var pVr=s(iae);Qwo=r(pVr,"qdqbert"),pVr.forEach(t),Hwo=r(oEe," \u2014 "),HI=n(oEe,"A",{href:!0});var _Vr=s(HI);Uwo=r(_Vr,"QDQBertForTokenClassification"),_Vr.forEach(t),Jwo=r(oEe," (QDQBert model)"),oEe.forEach(t),Ywo=i(q),vb=n(q,"LI",{});var rEe=s(vb);dae=n(rEe,"STRONG",{});var uVr=s(dae);Kwo=r(uVr,"rembert"),uVr.forEach(t),Zwo=r(rEe," \u2014 "),UI=n(rEe,"A",{href:!0});var bVr=s(UI);eAo=r(bVr,"RemBertForTokenClassification"),bVr.forEach(t),oAo=r(rEe," (RemBERT model)"),rEe.forEach(t),rAo=i(q),Tb=n(q,"LI",{});var tEe=s(Tb);cae=n(tEe,"STRONG",{});var vVr=s(cae);tAo=r(vVr,"roberta"),vVr.forEach(t),aAo=r(tEe," \u2014 "),JI=n(tEe,"A",{href:!0});var TVr=s(JI);nAo=r(TVr,"RobertaForTokenClassification"),TVr.forEach(t),sAo=r(tEe," (RoBERTa model)"),tEe.forEach(t),lAo=i(q),Fb=n(q,"LI",{});var aEe=s(Fb);fae=n(aEe,"STRONG",{});var FVr=s(fae);iAo=r(FVr,"roformer"),FVr.forEach(t),dAo=r(aEe," \u2014 "),YI=n(aEe,"A",{href:!0});var CVr=s(YI);cAo=r(CVr,"RoFormerForTokenClassification"),CVr.forEach(t),fAo=r(aEe," (RoFormer model)"),aEe.forEach(t),mAo=i(q),Cb=n(q,"LI",{});var nEe=s(Cb);mae=n(nEe,"STRONG",{});var MVr=s(mae);gAo=r(MVr,"squeezebert"),MVr.forEach(t),hAo=r(nEe," \u2014 "),KI=n(nEe,"A",{href:!0});var EVr=s(KI);pAo=r(EVr,"SqueezeBertForTokenClassification"),EVr.forEach(t),_Ao=r(nEe," (SqueezeBERT model)"),nEe.forEach(t),uAo=i(q),Mb=n(q,"LI",{});var sEe=s(Mb);gae=n(sEe,"STRONG",{});var yVr=s(gae);bAo=r(yVr,"xlm"),yVr.forEach(t),vAo=r(sEe," \u2014 "),ZI=n(sEe,"A",{href:!0});var wVr=s(ZI);TAo=r(wVr,"XLMForTokenClassification"),wVr.forEach(t),FAo=r(sEe," (XLM model)"),sEe.forEach(t),CAo=i(q),Eb=n(q,"LI",{});var lEe=s(Eb);hae=n(lEe,"STRONG",{});var AVr=s(hae);MAo=r(AVr,"xlm-roberta"),AVr.forEach(t),EAo=r(lEe," \u2014 "),ej=n(lEe,"A",{href:!0});var LVr=s(ej);yAo=r(LVr,"XLMRobertaForTokenClassification"),LVr.forEach(t),wAo=r(lEe," (XLM-RoBERTa model)"),lEe.forEach(t),AAo=i(q),yb=n(q,"LI",{});var iEe=s(yb);pae=n(iEe,"STRONG",{});var BVr=s(pae);LAo=r(BVr,"xlm-roberta-xl"),BVr.forEach(t),BAo=r(iEe," \u2014 "),oj=n(iEe,"A",{href:!0});var kVr=s(oj);kAo=r(kVr,"XLMRobertaXLForTokenClassification"),kVr.forEach(t),xAo=r(iEe," (XLM-RoBERTa-XL model)"),iEe.forEach(t),RAo=i(q),wb=n(q,"LI",{});var dEe=s(wb);_ae=n(dEe,"STRONG",{});var xVr=s(_ae);SAo=r(xVr,"xlnet"),xVr.forEach(t),PAo=r(dEe," \u2014 "),rj=n(dEe,"A",{href:!0});var RVr=s(rj);$Ao=r(RVr,"XLNetForTokenClassification"),RVr.forEach(t),IAo=r(dEe," (XLNet model)"),dEe.forEach(t),jAo=i(q),Ab=n(q,"LI",{});var cEe=s(Ab);uae=n(cEe,"STRONG",{});var SVr=s(uae);NAo=r(SVr,"yoso"),SVr.forEach(t),DAo=r(cEe," \u2014 "),tj=n(cEe,"A",{href:!0});var PVr=s(tj);qAo=r(PVr,"YosoForTokenClassification"),PVr.forEach(t),GAo=r(cEe," (YOSO model)"),cEe.forEach(t),q.forEach(t),OAo=i(Gt),Lb=n(Gt,"P",{});var fEe=s(Lb);XAo=r(fEe,"The model is set in evaluation mode by default using "),bae=n(fEe,"CODE",{});var $Vr=s(bae);zAo=r($Vr,"model.eval()"),$Vr.forEach(t),VAo=r(fEe,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),vae=n(fEe,"CODE",{});var IVr=s(vae);WAo=r(IVr,"model.train()"),IVr.forEach(t),fEe.forEach(t),QAo=i(Gt),Tae=n(Gt,"P",{});var jVr=s(Tae);HAo=r(jVr,"Examples:"),jVr.forEach(t),UAo=i(Gt),m(XE.$$.fragment,Gt),Gt.forEach(t),ol.forEach(t),PLe=i(d),gd=n(d,"H2",{class:!0});var GBe=s(gd);Bb=n(GBe,"A",{id:!0,class:!0,href:!0});var NVr=s(Bb);Fae=n(NVr,"SPAN",{});var DVr=s(Fae);m(zE.$$.fragment,DVr),DVr.forEach(t),NVr.forEach(t),JAo=i(GBe),Cae=n(GBe,"SPAN",{});var qVr=s(Cae);YAo=r(qVr,"AutoModelForQuestionAnswering"),qVr.forEach(t),GBe.forEach(t),$Le=i(d),er=n(d,"DIV",{class:!0});var tl=s(er);m(VE.$$.fragment,tl),KAo=i(tl),hd=n(tl,"P",{});var cz=s(hd);ZAo=r(cz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a question answering head) when created
with the `),Mae=n(cz,"CODE",{});var GVr=s(Mae);e6o=r(GVr,"from_pretrained()"),GVr.forEach(t),o6o=r(cz,"class method or the "),Eae=n(cz,"CODE",{});var OVr=s(Eae);r6o=r(OVr,"from_config()"),OVr.forEach(t),t6o=r(cz,`class
method.`),cz.forEach(t),a6o=i(tl),WE=n(tl,"P",{});var OBe=s(WE);n6o=r(OBe,"This class cannot be instantiated directly using "),yae=n(OBe,"CODE",{});var XVr=s(yae);s6o=r(XVr,"__init__()"),XVr.forEach(t),l6o=r(OBe," (throws an error)."),OBe.forEach(t),i6o=i(tl),Qr=n(tl,"DIV",{class:!0});var al=s(Qr);m(QE.$$.fragment,al),d6o=i(al),wae=n(al,"P",{});var zVr=s(wae);c6o=r(zVr,"Instantiates one of the model classes of the library (with a question answering head) from a configuration."),zVr.forEach(t),f6o=i(al),pd=n(al,"P",{});var fz=s(pd);m6o=r(fz,`Note:
Loading a model from its configuration file does `),Aae=n(fz,"STRONG",{});var VVr=s(Aae);g6o=r(VVr,"not"),VVr.forEach(t),h6o=r(fz,` load the model weights. It only affects the
model\u2019s configuration. Use `),Lae=n(fz,"CODE",{});var WVr=s(Lae);p6o=r(WVr,"from_pretrained()"),WVr.forEach(t),_6o=r(fz,"to load the model weights."),fz.forEach(t),u6o=i(al),Bae=n(al,"P",{});var QVr=s(Bae);b6o=r(QVr,"Examples:"),QVr.forEach(t),v6o=i(al),m(HE.$$.fragment,al),al.forEach(t),T6o=i(tl),De=n(tl,"DIV",{class:!0});var Ot=s(De);m(UE.$$.fragment,Ot),F6o=i(Ot),kae=n(Ot,"P",{});var HVr=s(kae);C6o=r(HVr,"Instantiate one of the model classes of the library (with a question answering head) from a pretrained model."),HVr.forEach(t),M6o=i(Ot),Ha=n(Ot,"P",{});var _4=s(Ha);E6o=r(_4,"The model class to instantiate is selected based on the "),xae=n(_4,"CODE",{});var UVr=s(xae);y6o=r(UVr,"model_type"),UVr.forEach(t),w6o=r(_4,` property of the config object (either
passed as an argument or loaded from `),Rae=n(_4,"CODE",{});var JVr=s(Rae);A6o=r(JVr,"pretrained_model_name_or_path"),JVr.forEach(t),L6o=r(_4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Sae=n(_4,"CODE",{});var YVr=s(Sae);B6o=r(YVr,"pretrained_model_name_or_path"),YVr.forEach(t),k6o=r(_4,":"),_4.forEach(t),x6o=i(Ot),R=n(Ot,"UL",{});var P=s(R);kb=n(P,"LI",{});var mEe=s(kb);Pae=n(mEe,"STRONG",{});var KVr=s(Pae);R6o=r(KVr,"albert"),KVr.forEach(t),S6o=r(mEe," \u2014 "),aj=n(mEe,"A",{href:!0});var ZVr=s(aj);P6o=r(ZVr,"AlbertForQuestionAnswering"),ZVr.forEach(t),$6o=r(mEe," (ALBERT model)"),mEe.forEach(t),I6o=i(P),xb=n(P,"LI",{});var gEe=s(xb);$ae=n(gEe,"STRONG",{});var eWr=s($ae);j6o=r(eWr,"bart"),eWr.forEach(t),N6o=r(gEe," \u2014 "),nj=n(gEe,"A",{href:!0});var oWr=s(nj);D6o=r(oWr,"BartForQuestionAnswering"),oWr.forEach(t),q6o=r(gEe," (BART model)"),gEe.forEach(t),G6o=i(P),Rb=n(P,"LI",{});var hEe=s(Rb);Iae=n(hEe,"STRONG",{});var rWr=s(Iae);O6o=r(rWr,"bert"),rWr.forEach(t),X6o=r(hEe," \u2014 "),sj=n(hEe,"A",{href:!0});var tWr=s(sj);z6o=r(tWr,"BertForQuestionAnswering"),tWr.forEach(t),V6o=r(hEe," (BERT model)"),hEe.forEach(t),W6o=i(P),Sb=n(P,"LI",{});var pEe=s(Sb);jae=n(pEe,"STRONG",{});var aWr=s(jae);Q6o=r(aWr,"big_bird"),aWr.forEach(t),H6o=r(pEe," \u2014 "),lj=n(pEe,"A",{href:!0});var nWr=s(lj);U6o=r(nWr,"BigBirdForQuestionAnswering"),nWr.forEach(t),J6o=r(pEe," (BigBird model)"),pEe.forEach(t),Y6o=i(P),Pb=n(P,"LI",{});var _Ee=s(Pb);Nae=n(_Ee,"STRONG",{});var sWr=s(Nae);K6o=r(sWr,"bigbird_pegasus"),sWr.forEach(t),Z6o=r(_Ee," \u2014 "),ij=n(_Ee,"A",{href:!0});var lWr=s(ij);e0o=r(lWr,"BigBirdPegasusForQuestionAnswering"),lWr.forEach(t),o0o=r(_Ee," (BigBirdPegasus model)"),_Ee.forEach(t),r0o=i(P),$b=n(P,"LI",{});var uEe=s($b);Dae=n(uEe,"STRONG",{});var iWr=s(Dae);t0o=r(iWr,"camembert"),iWr.forEach(t),a0o=r(uEe," \u2014 "),dj=n(uEe,"A",{href:!0});var dWr=s(dj);n0o=r(dWr,"CamembertForQuestionAnswering"),dWr.forEach(t),s0o=r(uEe," (CamemBERT model)"),uEe.forEach(t),l0o=i(P),Ib=n(P,"LI",{});var bEe=s(Ib);qae=n(bEe,"STRONG",{});var cWr=s(qae);i0o=r(cWr,"canine"),cWr.forEach(t),d0o=r(bEe," \u2014 "),cj=n(bEe,"A",{href:!0});var fWr=s(cj);c0o=r(fWr,"CanineForQuestionAnswering"),fWr.forEach(t),f0o=r(bEe," (Canine model)"),bEe.forEach(t),m0o=i(P),jb=n(P,"LI",{});var vEe=s(jb);Gae=n(vEe,"STRONG",{});var mWr=s(Gae);g0o=r(mWr,"convbert"),mWr.forEach(t),h0o=r(vEe," \u2014 "),fj=n(vEe,"A",{href:!0});var gWr=s(fj);p0o=r(gWr,"ConvBertForQuestionAnswering"),gWr.forEach(t),_0o=r(vEe," (ConvBERT model)"),vEe.forEach(t),u0o=i(P),Nb=n(P,"LI",{});var TEe=s(Nb);Oae=n(TEe,"STRONG",{});var hWr=s(Oae);b0o=r(hWr,"deberta"),hWr.forEach(t),v0o=r(TEe," \u2014 "),mj=n(TEe,"A",{href:!0});var pWr=s(mj);T0o=r(pWr,"DebertaForQuestionAnswering"),pWr.forEach(t),F0o=r(TEe," (DeBERTa model)"),TEe.forEach(t),C0o=i(P),Db=n(P,"LI",{});var FEe=s(Db);Xae=n(FEe,"STRONG",{});var _Wr=s(Xae);M0o=r(_Wr,"deberta-v2"),_Wr.forEach(t),E0o=r(FEe," \u2014 "),gj=n(FEe,"A",{href:!0});var uWr=s(gj);y0o=r(uWr,"DebertaV2ForQuestionAnswering"),uWr.forEach(t),w0o=r(FEe," (DeBERTa-v2 model)"),FEe.forEach(t),A0o=i(P),qb=n(P,"LI",{});var CEe=s(qb);zae=n(CEe,"STRONG",{});var bWr=s(zae);L0o=r(bWr,"distilbert"),bWr.forEach(t),B0o=r(CEe," \u2014 "),hj=n(CEe,"A",{href:!0});var vWr=s(hj);k0o=r(vWr,"DistilBertForQuestionAnswering"),vWr.forEach(t),x0o=r(CEe," (DistilBERT model)"),CEe.forEach(t),R0o=i(P),Gb=n(P,"LI",{});var MEe=s(Gb);Vae=n(MEe,"STRONG",{});var TWr=s(Vae);S0o=r(TWr,"electra"),TWr.forEach(t),P0o=r(MEe," \u2014 "),pj=n(MEe,"A",{href:!0});var FWr=s(pj);$0o=r(FWr,"ElectraForQuestionAnswering"),FWr.forEach(t),I0o=r(MEe," (ELECTRA model)"),MEe.forEach(t),j0o=i(P),Ob=n(P,"LI",{});var EEe=s(Ob);Wae=n(EEe,"STRONG",{});var CWr=s(Wae);N0o=r(CWr,"flaubert"),CWr.forEach(t),D0o=r(EEe," \u2014 "),_j=n(EEe,"A",{href:!0});var MWr=s(_j);q0o=r(MWr,"FlaubertForQuestionAnsweringSimple"),MWr.forEach(t),G0o=r(EEe," (FlauBERT model)"),EEe.forEach(t),O0o=i(P),Xb=n(P,"LI",{});var yEe=s(Xb);Qae=n(yEe,"STRONG",{});var EWr=s(Qae);X0o=r(EWr,"fnet"),EWr.forEach(t),z0o=r(yEe," \u2014 "),uj=n(yEe,"A",{href:!0});var yWr=s(uj);V0o=r(yWr,"FNetForQuestionAnswering"),yWr.forEach(t),W0o=r(yEe," (FNet model)"),yEe.forEach(t),Q0o=i(P),zb=n(P,"LI",{});var wEe=s(zb);Hae=n(wEe,"STRONG",{});var wWr=s(Hae);H0o=r(wWr,"funnel"),wWr.forEach(t),U0o=r(wEe," \u2014 "),bj=n(wEe,"A",{href:!0});var AWr=s(bj);J0o=r(AWr,"FunnelForQuestionAnswering"),AWr.forEach(t),Y0o=r(wEe," (Funnel Transformer model)"),wEe.forEach(t),K0o=i(P),Vb=n(P,"LI",{});var AEe=s(Vb);Uae=n(AEe,"STRONG",{});var LWr=s(Uae);Z0o=r(LWr,"gptj"),LWr.forEach(t),eLo=r(AEe," \u2014 "),vj=n(AEe,"A",{href:!0});var BWr=s(vj);oLo=r(BWr,"GPTJForQuestionAnswering"),BWr.forEach(t),rLo=r(AEe," (GPT-J model)"),AEe.forEach(t),tLo=i(P),Wb=n(P,"LI",{});var LEe=s(Wb);Jae=n(LEe,"STRONG",{});var kWr=s(Jae);aLo=r(kWr,"ibert"),kWr.forEach(t),nLo=r(LEe," \u2014 "),Tj=n(LEe,"A",{href:!0});var xWr=s(Tj);sLo=r(xWr,"IBertForQuestionAnswering"),xWr.forEach(t),lLo=r(LEe," (I-BERT model)"),LEe.forEach(t),iLo=i(P),Qb=n(P,"LI",{});var BEe=s(Qb);Yae=n(BEe,"STRONG",{});var RWr=s(Yae);dLo=r(RWr,"layoutlmv2"),RWr.forEach(t),cLo=r(BEe," \u2014 "),Fj=n(BEe,"A",{href:!0});var SWr=s(Fj);fLo=r(SWr,"LayoutLMv2ForQuestionAnswering"),SWr.forEach(t),mLo=r(BEe," (LayoutLMv2 model)"),BEe.forEach(t),gLo=i(P),Hb=n(P,"LI",{});var kEe=s(Hb);Kae=n(kEe,"STRONG",{});var PWr=s(Kae);hLo=r(PWr,"led"),PWr.forEach(t),pLo=r(kEe," \u2014 "),Cj=n(kEe,"A",{href:!0});var $Wr=s(Cj);_Lo=r($Wr,"LEDForQuestionAnswering"),$Wr.forEach(t),uLo=r(kEe," (LED model)"),kEe.forEach(t),bLo=i(P),Ub=n(P,"LI",{});var xEe=s(Ub);Zae=n(xEe,"STRONG",{});var IWr=s(Zae);vLo=r(IWr,"longformer"),IWr.forEach(t),TLo=r(xEe," \u2014 "),Mj=n(xEe,"A",{href:!0});var jWr=s(Mj);FLo=r(jWr,"LongformerForQuestionAnswering"),jWr.forEach(t),CLo=r(xEe," (Longformer model)"),xEe.forEach(t),MLo=i(P),Jb=n(P,"LI",{});var REe=s(Jb);ene=n(REe,"STRONG",{});var NWr=s(ene);ELo=r(NWr,"lxmert"),NWr.forEach(t),yLo=r(REe," \u2014 "),Ej=n(REe,"A",{href:!0});var DWr=s(Ej);wLo=r(DWr,"LxmertForQuestionAnswering"),DWr.forEach(t),ALo=r(REe," (LXMERT model)"),REe.forEach(t),LLo=i(P),Yb=n(P,"LI",{});var SEe=s(Yb);one=n(SEe,"STRONG",{});var qWr=s(one);BLo=r(qWr,"mbart"),qWr.forEach(t),kLo=r(SEe," \u2014 "),yj=n(SEe,"A",{href:!0});var GWr=s(yj);xLo=r(GWr,"MBartForQuestionAnswering"),GWr.forEach(t),RLo=r(SEe," (mBART model)"),SEe.forEach(t),SLo=i(P),Kb=n(P,"LI",{});var PEe=s(Kb);rne=n(PEe,"STRONG",{});var OWr=s(rne);PLo=r(OWr,"megatron-bert"),OWr.forEach(t),$Lo=r(PEe," \u2014 "),wj=n(PEe,"A",{href:!0});var XWr=s(wj);ILo=r(XWr,"MegatronBertForQuestionAnswering"),XWr.forEach(t),jLo=r(PEe," (MegatronBert model)"),PEe.forEach(t),NLo=i(P),Zb=n(P,"LI",{});var $Ee=s(Zb);tne=n($Ee,"STRONG",{});var zWr=s(tne);DLo=r(zWr,"mobilebert"),zWr.forEach(t),qLo=r($Ee," \u2014 "),Aj=n($Ee,"A",{href:!0});var VWr=s(Aj);GLo=r(VWr,"MobileBertForQuestionAnswering"),VWr.forEach(t),OLo=r($Ee," (MobileBERT model)"),$Ee.forEach(t),XLo=i(P),e5=n(P,"LI",{});var IEe=s(e5);ane=n(IEe,"STRONG",{});var WWr=s(ane);zLo=r(WWr,"mpnet"),WWr.forEach(t),VLo=r(IEe," \u2014 "),Lj=n(IEe,"A",{href:!0});var QWr=s(Lj);WLo=r(QWr,"MPNetForQuestionAnswering"),QWr.forEach(t),QLo=r(IEe," (MPNet model)"),IEe.forEach(t),HLo=i(P),o5=n(P,"LI",{});var jEe=s(o5);nne=n(jEe,"STRONG",{});var HWr=s(nne);ULo=r(HWr,"nystromformer"),HWr.forEach(t),JLo=r(jEe," \u2014 "),Bj=n(jEe,"A",{href:!0});var UWr=s(Bj);YLo=r(UWr,"NystromformerForQuestionAnswering"),UWr.forEach(t),KLo=r(jEe," (Nystromformer model)"),jEe.forEach(t),ZLo=i(P),r5=n(P,"LI",{});var NEe=s(r5);sne=n(NEe,"STRONG",{});var JWr=s(sne);e8o=r(JWr,"qdqbert"),JWr.forEach(t),o8o=r(NEe," \u2014 "),kj=n(NEe,"A",{href:!0});var YWr=s(kj);r8o=r(YWr,"QDQBertForQuestionAnswering"),YWr.forEach(t),t8o=r(NEe," (QDQBert model)"),NEe.forEach(t),a8o=i(P),t5=n(P,"LI",{});var DEe=s(t5);lne=n(DEe,"STRONG",{});var KWr=s(lne);n8o=r(KWr,"reformer"),KWr.forEach(t),s8o=r(DEe," \u2014 "),xj=n(DEe,"A",{href:!0});var ZWr=s(xj);l8o=r(ZWr,"ReformerForQuestionAnswering"),ZWr.forEach(t),i8o=r(DEe," (Reformer model)"),DEe.forEach(t),d8o=i(P),a5=n(P,"LI",{});var qEe=s(a5);ine=n(qEe,"STRONG",{});var eQr=s(ine);c8o=r(eQr,"rembert"),eQr.forEach(t),f8o=r(qEe," \u2014 "),Rj=n(qEe,"A",{href:!0});var oQr=s(Rj);m8o=r(oQr,"RemBertForQuestionAnswering"),oQr.forEach(t),g8o=r(qEe," (RemBERT model)"),qEe.forEach(t),h8o=i(P),n5=n(P,"LI",{});var GEe=s(n5);dne=n(GEe,"STRONG",{});var rQr=s(dne);p8o=r(rQr,"roberta"),rQr.forEach(t),_8o=r(GEe," \u2014 "),Sj=n(GEe,"A",{href:!0});var tQr=s(Sj);u8o=r(tQr,"RobertaForQuestionAnswering"),tQr.forEach(t),b8o=r(GEe," (RoBERTa model)"),GEe.forEach(t),v8o=i(P),s5=n(P,"LI",{});var OEe=s(s5);cne=n(OEe,"STRONG",{});var aQr=s(cne);T8o=r(aQr,"roformer"),aQr.forEach(t),F8o=r(OEe," \u2014 "),Pj=n(OEe,"A",{href:!0});var nQr=s(Pj);C8o=r(nQr,"RoFormerForQuestionAnswering"),nQr.forEach(t),M8o=r(OEe," (RoFormer model)"),OEe.forEach(t),E8o=i(P),l5=n(P,"LI",{});var XEe=s(l5);fne=n(XEe,"STRONG",{});var sQr=s(fne);y8o=r(sQr,"splinter"),sQr.forEach(t),w8o=r(XEe," \u2014 "),$j=n(XEe,"A",{href:!0});var lQr=s($j);A8o=r(lQr,"SplinterForQuestionAnswering"),lQr.forEach(t),L8o=r(XEe," (Splinter model)"),XEe.forEach(t),B8o=i(P),i5=n(P,"LI",{});var zEe=s(i5);mne=n(zEe,"STRONG",{});var iQr=s(mne);k8o=r(iQr,"squeezebert"),iQr.forEach(t),x8o=r(zEe," \u2014 "),Ij=n(zEe,"A",{href:!0});var dQr=s(Ij);R8o=r(dQr,"SqueezeBertForQuestionAnswering"),dQr.forEach(t),S8o=r(zEe," (SqueezeBERT model)"),zEe.forEach(t),P8o=i(P),d5=n(P,"LI",{});var VEe=s(d5);gne=n(VEe,"STRONG",{});var cQr=s(gne);$8o=r(cQr,"xlm"),cQr.forEach(t),I8o=r(VEe," \u2014 "),jj=n(VEe,"A",{href:!0});var fQr=s(jj);j8o=r(fQr,"XLMForQuestionAnsweringSimple"),fQr.forEach(t),N8o=r(VEe," (XLM model)"),VEe.forEach(t),D8o=i(P),c5=n(P,"LI",{});var WEe=s(c5);hne=n(WEe,"STRONG",{});var mQr=s(hne);q8o=r(mQr,"xlm-roberta"),mQr.forEach(t),G8o=r(WEe," \u2014 "),Nj=n(WEe,"A",{href:!0});var gQr=s(Nj);O8o=r(gQr,"XLMRobertaForQuestionAnswering"),gQr.forEach(t),X8o=r(WEe," (XLM-RoBERTa model)"),WEe.forEach(t),z8o=i(P),f5=n(P,"LI",{});var QEe=s(f5);pne=n(QEe,"STRONG",{});var hQr=s(pne);V8o=r(hQr,"xlm-roberta-xl"),hQr.forEach(t),W8o=r(QEe," \u2014 "),Dj=n(QEe,"A",{href:!0});var pQr=s(Dj);Q8o=r(pQr,"XLMRobertaXLForQuestionAnswering"),pQr.forEach(t),H8o=r(QEe," (XLM-RoBERTa-XL model)"),QEe.forEach(t),U8o=i(P),m5=n(P,"LI",{});var HEe=s(m5);_ne=n(HEe,"STRONG",{});var _Qr=s(_ne);J8o=r(_Qr,"xlnet"),_Qr.forEach(t),Y8o=r(HEe," \u2014 "),qj=n(HEe,"A",{href:!0});var uQr=s(qj);K8o=r(uQr,"XLNetForQuestionAnsweringSimple"),uQr.forEach(t),Z8o=r(HEe," (XLNet model)"),HEe.forEach(t),eBo=i(P),g5=n(P,"LI",{});var UEe=s(g5);une=n(UEe,"STRONG",{});var bQr=s(une);oBo=r(bQr,"yoso"),bQr.forEach(t),rBo=r(UEe," \u2014 "),Gj=n(UEe,"A",{href:!0});var vQr=s(Gj);tBo=r(vQr,"YosoForQuestionAnswering"),vQr.forEach(t),aBo=r(UEe," (YOSO model)"),UEe.forEach(t),P.forEach(t),nBo=i(Ot),h5=n(Ot,"P",{});var JEe=s(h5);sBo=r(JEe,"The model is set in evaluation mode by default using "),bne=n(JEe,"CODE",{});var TQr=s(bne);lBo=r(TQr,"model.eval()"),TQr.forEach(t),iBo=r(JEe,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),vne=n(JEe,"CODE",{});var FQr=s(vne);dBo=r(FQr,"model.train()"),FQr.forEach(t),JEe.forEach(t),cBo=i(Ot),Tne=n(Ot,"P",{});var CQr=s(Tne);fBo=r(CQr,"Examples:"),CQr.forEach(t),mBo=i(Ot),m(JE.$$.fragment,Ot),Ot.forEach(t),tl.forEach(t),ILe=i(d),_d=n(d,"H2",{class:!0});var XBe=s(_d);p5=n(XBe,"A",{id:!0,class:!0,href:!0});var MQr=s(p5);Fne=n(MQr,"SPAN",{});var EQr=s(Fne);m(YE.$$.fragment,EQr),EQr.forEach(t),MQr.forEach(t),gBo=i(XBe),Cne=n(XBe,"SPAN",{});var yQr=s(Cne);hBo=r(yQr,"AutoModelForTableQuestionAnswering"),yQr.forEach(t),XBe.forEach(t),jLe=i(d),or=n(d,"DIV",{class:!0});var nl=s(or);m(KE.$$.fragment,nl),pBo=i(nl),ud=n(nl,"P",{});var mz=s(ud);_Bo=r(mz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a table question answering head) when created
with the `),Mne=n(mz,"CODE",{});var wQr=s(Mne);uBo=r(wQr,"from_pretrained()"),wQr.forEach(t),bBo=r(mz,"class method or the "),Ene=n(mz,"CODE",{});var AQr=s(Ene);vBo=r(AQr,"from_config()"),AQr.forEach(t),TBo=r(mz,`class
method.`),mz.forEach(t),FBo=i(nl),ZE=n(nl,"P",{});var zBe=s(ZE);CBo=r(zBe,"This class cannot be instantiated directly using "),yne=n(zBe,"CODE",{});var LQr=s(yne);MBo=r(LQr,"__init__()"),LQr.forEach(t),EBo=r(zBe," (throws an error)."),zBe.forEach(t),yBo=i(nl),Hr=n(nl,"DIV",{class:!0});var sl=s(Hr);m(e3.$$.fragment,sl),wBo=i(sl),wne=n(sl,"P",{});var BQr=s(wne);ABo=r(BQr,"Instantiates one of the model classes of the library (with a table question answering head) from a configuration."),BQr.forEach(t),LBo=i(sl),bd=n(sl,"P",{});var gz=s(bd);BBo=r(gz,`Note:
Loading a model from its configuration file does `),Ane=n(gz,"STRONG",{});var kQr=s(Ane);kBo=r(kQr,"not"),kQr.forEach(t),xBo=r(gz,` load the model weights. It only affects the
model\u2019s configuration. Use `),Lne=n(gz,"CODE",{});var xQr=s(Lne);RBo=r(xQr,"from_pretrained()"),xQr.forEach(t),SBo=r(gz,"to load the model weights."),gz.forEach(t),PBo=i(sl),Bne=n(sl,"P",{});var RQr=s(Bne);$Bo=r(RQr,"Examples:"),RQr.forEach(t),IBo=i(sl),m(o3.$$.fragment,sl),sl.forEach(t),jBo=i(nl),qe=n(nl,"DIV",{class:!0});var Xt=s(qe);m(r3.$$.fragment,Xt),NBo=i(Xt),kne=n(Xt,"P",{});var SQr=s(kne);DBo=r(SQr,"Instantiate one of the model classes of the library (with a table question answering head) from a pretrained model."),SQr.forEach(t),qBo=i(Xt),Ua=n(Xt,"P",{});var u4=s(Ua);GBo=r(u4,"The model class to instantiate is selected based on the "),xne=n(u4,"CODE",{});var PQr=s(xne);OBo=r(PQr,"model_type"),PQr.forEach(t),XBo=r(u4,` property of the config object (either
passed as an argument or loaded from `),Rne=n(u4,"CODE",{});var $Qr=s(Rne);zBo=r($Qr,"pretrained_model_name_or_path"),$Qr.forEach(t),VBo=r(u4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Sne=n(u4,"CODE",{});var IQr=s(Sne);WBo=r(IQr,"pretrained_model_name_or_path"),IQr.forEach(t),QBo=r(u4,":"),u4.forEach(t),HBo=i(Xt),Pne=n(Xt,"UL",{});var jQr=s(Pne);_5=n(jQr,"LI",{});var YEe=s(_5);$ne=n(YEe,"STRONG",{});var NQr=s($ne);UBo=r(NQr,"tapas"),NQr.forEach(t),JBo=r(YEe," \u2014 "),Oj=n(YEe,"A",{href:!0});var DQr=s(Oj);YBo=r(DQr,"TapasForQuestionAnswering"),DQr.forEach(t),KBo=r(YEe," (TAPAS model)"),YEe.forEach(t),jQr.forEach(t),ZBo=i(Xt),u5=n(Xt,"P",{});var KEe=s(u5);eko=r(KEe,"The model is set in evaluation mode by default using "),Ine=n(KEe,"CODE",{});var qQr=s(Ine);oko=r(qQr,"model.eval()"),qQr.forEach(t),rko=r(KEe,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),jne=n(KEe,"CODE",{});var GQr=s(jne);tko=r(GQr,"model.train()"),GQr.forEach(t),KEe.forEach(t),ako=i(Xt),Nne=n(Xt,"P",{});var OQr=s(Nne);nko=r(OQr,"Examples:"),OQr.forEach(t),sko=i(Xt),m(t3.$$.fragment,Xt),Xt.forEach(t),nl.forEach(t),NLe=i(d),vd=n(d,"H2",{class:!0});var VBe=s(vd);b5=n(VBe,"A",{id:!0,class:!0,href:!0});var XQr=s(b5);Dne=n(XQr,"SPAN",{});var zQr=s(Dne);m(a3.$$.fragment,zQr),zQr.forEach(t),XQr.forEach(t),lko=i(VBe),qne=n(VBe,"SPAN",{});var VQr=s(qne);iko=r(VQr,"AutoModelForImageClassification"),VQr.forEach(t),VBe.forEach(t),DLe=i(d),rr=n(d,"DIV",{class:!0});var ll=s(rr);m(n3.$$.fragment,ll),dko=i(ll),Td=n(ll,"P",{});var hz=s(Td);cko=r(hz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a image classification head) when created
with the `),Gne=n(hz,"CODE",{});var WQr=s(Gne);fko=r(WQr,"from_pretrained()"),WQr.forEach(t),mko=r(hz,"class method or the "),One=n(hz,"CODE",{});var QQr=s(One);gko=r(QQr,"from_config()"),QQr.forEach(t),hko=r(hz,`class
method.`),hz.forEach(t),pko=i(ll),s3=n(ll,"P",{});var WBe=s(s3);_ko=r(WBe,"This class cannot be instantiated directly using "),Xne=n(WBe,"CODE",{});var HQr=s(Xne);uko=r(HQr,"__init__()"),HQr.forEach(t),bko=r(WBe," (throws an error)."),WBe.forEach(t),vko=i(ll),Ur=n(ll,"DIV",{class:!0});var il=s(Ur);m(l3.$$.fragment,il),Tko=i(il),zne=n(il,"P",{});var UQr=s(zne);Fko=r(UQr,"Instantiates one of the model classes of the library (with a image classification head) from a configuration."),UQr.forEach(t),Cko=i(il),Fd=n(il,"P",{});var pz=s(Fd);Mko=r(pz,`Note:
Loading a model from its configuration file does `),Vne=n(pz,"STRONG",{});var JQr=s(Vne);Eko=r(JQr,"not"),JQr.forEach(t),yko=r(pz,` load the model weights. It only affects the
model\u2019s configuration. Use `),Wne=n(pz,"CODE",{});var YQr=s(Wne);wko=r(YQr,"from_pretrained()"),YQr.forEach(t),Ako=r(pz,"to load the model weights."),pz.forEach(t),Lko=i(il),Qne=n(il,"P",{});var KQr=s(Qne);Bko=r(KQr,"Examples:"),KQr.forEach(t),kko=i(il),m(i3.$$.fragment,il),il.forEach(t),xko=i(ll),Ge=n(ll,"DIV",{class:!0});var zt=s(Ge);m(d3.$$.fragment,zt),Rko=i(zt),Hne=n(zt,"P",{});var ZQr=s(Hne);Sko=r(ZQr,"Instantiate one of the model classes of the library (with a image classification head) from a pretrained model."),ZQr.forEach(t),Pko=i(zt),Ja=n(zt,"P",{});var b4=s(Ja);$ko=r(b4,"The model class to instantiate is selected based on the "),Une=n(b4,"CODE",{});var eHr=s(Une);Iko=r(eHr,"model_type"),eHr.forEach(t),jko=r(b4,` property of the config object (either
passed as an argument or loaded from `),Jne=n(b4,"CODE",{});var oHr=s(Jne);Nko=r(oHr,"pretrained_model_name_or_path"),oHr.forEach(t),Dko=r(b4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Yne=n(b4,"CODE",{});var rHr=s(Yne);qko=r(rHr,"pretrained_model_name_or_path"),rHr.forEach(t),Gko=r(b4,":"),b4.forEach(t),Oko=i(zt),be=n(zt,"UL",{});var Ke=s(be);v5=n(Ke,"LI",{});var ZEe=s(v5);Kne=n(ZEe,"STRONG",{});var tHr=s(Kne);Xko=r(tHr,"beit"),tHr.forEach(t),zko=r(ZEe," \u2014 "),Xj=n(ZEe,"A",{href:!0});var aHr=s(Xj);Vko=r(aHr,"BeitForImageClassification"),aHr.forEach(t),Wko=r(ZEe," (BEiT model)"),ZEe.forEach(t),Qko=i(Ke),T5=n(Ke,"LI",{});var e3e=s(T5);Zne=n(e3e,"STRONG",{});var nHr=s(Zne);Hko=r(nHr,"convnext"),nHr.forEach(t),Uko=r(e3e," \u2014 "),zj=n(e3e,"A",{href:!0});var sHr=s(zj);Jko=r(sHr,"ConvNextForImageClassification"),sHr.forEach(t),Yko=r(e3e," (ConvNext model)"),e3e.forEach(t),Kko=i(Ke),Rs=n(Ke,"LI",{});var j0=s(Rs);ese=n(j0,"STRONG",{});var lHr=s(ese);Zko=r(lHr,"deit"),lHr.forEach(t),exo=r(j0," \u2014 "),Vj=n(j0,"A",{href:!0});var iHr=s(Vj);oxo=r(iHr,"DeiTForImageClassification"),iHr.forEach(t),rxo=r(j0," or "),Wj=n(j0,"A",{href:!0});var dHr=s(Wj);txo=r(dHr,"DeiTForImageClassificationWithTeacher"),dHr.forEach(t),axo=r(j0," (DeiT model)"),j0.forEach(t),nxo=i(Ke),F5=n(Ke,"LI",{});var o3e=s(F5);ose=n(o3e,"STRONG",{});var cHr=s(ose);sxo=r(cHr,"imagegpt"),cHr.forEach(t),lxo=r(o3e," \u2014 "),Qj=n(o3e,"A",{href:!0});var fHr=s(Qj);ixo=r(fHr,"ImageGPTForImageClassification"),fHr.forEach(t),dxo=r(o3e," (ImageGPT model)"),o3e.forEach(t),cxo=i(Ke),la=n(Ke,"LI",{});var Mf=s(la);rse=n(Mf,"STRONG",{});var mHr=s(rse);fxo=r(mHr,"perceiver"),mHr.forEach(t),mxo=r(Mf," \u2014 "),Hj=n(Mf,"A",{href:!0});var gHr=s(Hj);gxo=r(gHr,"PerceiverForImageClassificationLearned"),gHr.forEach(t),hxo=r(Mf," or "),Uj=n(Mf,"A",{href:!0});var hHr=s(Uj);pxo=r(hHr,"PerceiverForImageClassificationFourier"),hHr.forEach(t),_xo=r(Mf," or "),Jj=n(Mf,"A",{href:!0});var pHr=s(Jj);uxo=r(pHr,"PerceiverForImageClassificationConvProcessing"),pHr.forEach(t),bxo=r(Mf," (Perceiver model)"),Mf.forEach(t),vxo=i(Ke),C5=n(Ke,"LI",{});var r3e=s(C5);tse=n(r3e,"STRONG",{});var _Hr=s(tse);Txo=r(_Hr,"poolformer"),_Hr.forEach(t),Fxo=r(r3e," \u2014 "),Yj=n(r3e,"A",{href:!0});var uHr=s(Yj);Cxo=r(uHr,"PoolFormerForImageClassification"),uHr.forEach(t),Mxo=r(r3e," (PoolFormer model)"),r3e.forEach(t),Exo=i(Ke),M5=n(Ke,"LI",{});var t3e=s(M5);ase=n(t3e,"STRONG",{});var bHr=s(ase);yxo=r(bHr,"segformer"),bHr.forEach(t),wxo=r(t3e," \u2014 "),Kj=n(t3e,"A",{href:!0});var vHr=s(Kj);Axo=r(vHr,"SegformerForImageClassification"),vHr.forEach(t),Lxo=r(t3e," (SegFormer model)"),t3e.forEach(t),Bxo=i(Ke),E5=n(Ke,"LI",{});var a3e=s(E5);nse=n(a3e,"STRONG",{});var THr=s(nse);kxo=r(THr,"swin"),THr.forEach(t),xxo=r(a3e," \u2014 "),Zj=n(a3e,"A",{href:!0});var FHr=s(Zj);Rxo=r(FHr,"SwinForImageClassification"),FHr.forEach(t),Sxo=r(a3e," (Swin model)"),a3e.forEach(t),Pxo=i(Ke),y5=n(Ke,"LI",{});var n3e=s(y5);sse=n(n3e,"STRONG",{});var CHr=s(sse);$xo=r(CHr,"vit"),CHr.forEach(t),Ixo=r(n3e," \u2014 "),eN=n(n3e,"A",{href:!0});var MHr=s(eN);jxo=r(MHr,"ViTForImageClassification"),MHr.forEach(t),Nxo=r(n3e," (ViT model)"),n3e.forEach(t),Ke.forEach(t),Dxo=i(zt),w5=n(zt,"P",{});var s3e=s(w5);qxo=r(s3e,"The model is set in evaluation mode by default using "),lse=n(s3e,"CODE",{});var EHr=s(lse);Gxo=r(EHr,"model.eval()"),EHr.forEach(t),Oxo=r(s3e,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),ise=n(s3e,"CODE",{});var yHr=s(ise);Xxo=r(yHr,"model.train()"),yHr.forEach(t),s3e.forEach(t),zxo=i(zt),dse=n(zt,"P",{});var wHr=s(dse);Vxo=r(wHr,"Examples:"),wHr.forEach(t),Wxo=i(zt),m(c3.$$.fragment,zt),zt.forEach(t),ll.forEach(t),qLe=i(d),Cd=n(d,"H2",{class:!0});var QBe=s(Cd);A5=n(QBe,"A",{id:!0,class:!0,href:!0});var AHr=s(A5);cse=n(AHr,"SPAN",{});var LHr=s(cse);m(f3.$$.fragment,LHr),LHr.forEach(t),AHr.forEach(t),Qxo=i(QBe),fse=n(QBe,"SPAN",{});var BHr=s(fse);Hxo=r(BHr,"AutoModelForVision2Seq"),BHr.forEach(t),QBe.forEach(t),GLe=i(d),tr=n(d,"DIV",{class:!0});var dl=s(tr);m(m3.$$.fragment,dl),Uxo=i(dl),Md=n(dl,"P",{});var _z=s(Md);Jxo=r(_z,`This is a generic model class that will be instantiated as one of the model classes of the library (with a vision-to-text modeling head) when created
with the `),mse=n(_z,"CODE",{});var kHr=s(mse);Yxo=r(kHr,"from_pretrained()"),kHr.forEach(t),Kxo=r(_z,"class method or the "),gse=n(_z,"CODE",{});var xHr=s(gse);Zxo=r(xHr,"from_config()"),xHr.forEach(t),eRo=r(_z,`class
method.`),_z.forEach(t),oRo=i(dl),g3=n(dl,"P",{});var HBe=s(g3);rRo=r(HBe,"This class cannot be instantiated directly using "),hse=n(HBe,"CODE",{});var RHr=s(hse);tRo=r(RHr,"__init__()"),RHr.forEach(t),aRo=r(HBe," (throws an error)."),HBe.forEach(t),nRo=i(dl),Jr=n(dl,"DIV",{class:!0});var cl=s(Jr);m(h3.$$.fragment,cl),sRo=i(cl),pse=n(cl,"P",{});var SHr=s(pse);lRo=r(SHr,"Instantiates one of the model classes of the library (with a vision-to-text modeling head) from a configuration."),SHr.forEach(t),iRo=i(cl),Ed=n(cl,"P",{});var uz=s(Ed);dRo=r(uz,`Note:
Loading a model from its configuration file does `),_se=n(uz,"STRONG",{});var PHr=s(_se);cRo=r(PHr,"not"),PHr.forEach(t),fRo=r(uz,` load the model weights. It only affects the
model\u2019s configuration. Use `),use=n(uz,"CODE",{});var $Hr=s(use);mRo=r($Hr,"from_pretrained()"),$Hr.forEach(t),gRo=r(uz,"to load the model weights."),uz.forEach(t),hRo=i(cl),bse=n(cl,"P",{});var IHr=s(bse);pRo=r(IHr,"Examples:"),IHr.forEach(t),_Ro=i(cl),m(p3.$$.fragment,cl),cl.forEach(t),uRo=i(dl),Oe=n(dl,"DIV",{class:!0});var Vt=s(Oe);m(_3.$$.fragment,Vt),bRo=i(Vt),vse=n(Vt,"P",{});var jHr=s(vse);vRo=r(jHr,"Instantiate one of the model classes of the library (with a vision-to-text modeling head) from a pretrained model."),jHr.forEach(t),TRo=i(Vt),Ya=n(Vt,"P",{});var v4=s(Ya);FRo=r(v4,"The model class to instantiate is selected based on the "),Tse=n(v4,"CODE",{});var NHr=s(Tse);CRo=r(NHr,"model_type"),NHr.forEach(t),MRo=r(v4,` property of the config object (either
passed as an argument or loaded from `),Fse=n(v4,"CODE",{});var DHr=s(Fse);ERo=r(DHr,"pretrained_model_name_or_path"),DHr.forEach(t),yRo=r(v4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Cse=n(v4,"CODE",{});var qHr=s(Cse);wRo=r(qHr,"pretrained_model_name_or_path"),qHr.forEach(t),ARo=r(v4,":"),v4.forEach(t),LRo=i(Vt),Mse=n(Vt,"UL",{});var GHr=s(Mse);L5=n(GHr,"LI",{});var l3e=s(L5);Ese=n(l3e,"STRONG",{});var OHr=s(Ese);BRo=r(OHr,"vision-encoder-decoder"),OHr.forEach(t),kRo=r(l3e," \u2014 "),oN=n(l3e,"A",{href:!0});var XHr=s(oN);xRo=r(XHr,"VisionEncoderDecoderModel"),XHr.forEach(t),RRo=r(l3e," (Vision Encoder decoder model)"),l3e.forEach(t),GHr.forEach(t),SRo=i(Vt),B5=n(Vt,"P",{});var i3e=s(B5);PRo=r(i3e,"The model is set in evaluation mode by default using "),yse=n(i3e,"CODE",{});var zHr=s(yse);$Ro=r(zHr,"model.eval()"),zHr.forEach(t),IRo=r(i3e,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),wse=n(i3e,"CODE",{});var VHr=s(wse);jRo=r(VHr,"model.train()"),VHr.forEach(t),i3e.forEach(t),NRo=i(Vt),Ase=n(Vt,"P",{});var WHr=s(Ase);DRo=r(WHr,"Examples:"),WHr.forEach(t),qRo=i(Vt),m(u3.$$.fragment,Vt),Vt.forEach(t),dl.forEach(t),OLe=i(d),yd=n(d,"H2",{class:!0});var UBe=s(yd);k5=n(UBe,"A",{id:!0,class:!0,href:!0});var QHr=s(k5);Lse=n(QHr,"SPAN",{});var HHr=s(Lse);m(b3.$$.fragment,HHr),HHr.forEach(t),QHr.forEach(t),GRo=i(UBe),Bse=n(UBe,"SPAN",{});var UHr=s(Bse);ORo=r(UHr,"AutoModelForAudioClassification"),UHr.forEach(t),UBe.forEach(t),XLe=i(d),ar=n(d,"DIV",{class:!0});var fl=s(ar);m(v3.$$.fragment,fl),XRo=i(fl),wd=n(fl,"P",{});var bz=s(wd);zRo=r(bz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a audio classification head) when created
with the `),kse=n(bz,"CODE",{});var JHr=s(kse);VRo=r(JHr,"from_pretrained()"),JHr.forEach(t),WRo=r(bz,"class method or the "),xse=n(bz,"CODE",{});var YHr=s(xse);QRo=r(YHr,"from_config()"),YHr.forEach(t),HRo=r(bz,`class
method.`),bz.forEach(t),URo=i(fl),T3=n(fl,"P",{});var JBe=s(T3);JRo=r(JBe,"This class cannot be instantiated directly using "),Rse=n(JBe,"CODE",{});var KHr=s(Rse);YRo=r(KHr,"__init__()"),KHr.forEach(t),KRo=r(JBe," (throws an error)."),JBe.forEach(t),ZRo=i(fl),Yr=n(fl,"DIV",{class:!0});var ml=s(Yr);m(F3.$$.fragment,ml),eSo=i(ml),Sse=n(ml,"P",{});var ZHr=s(Sse);oSo=r(ZHr,"Instantiates one of the model classes of the library (with a audio classification head) from a configuration."),ZHr.forEach(t),rSo=i(ml),Ad=n(ml,"P",{});var vz=s(Ad);tSo=r(vz,`Note:
Loading a model from its configuration file does `),Pse=n(vz,"STRONG",{});var eUr=s(Pse);aSo=r(eUr,"not"),eUr.forEach(t),nSo=r(vz,` load the model weights. It only affects the
model\u2019s configuration. Use `),$se=n(vz,"CODE",{});var oUr=s($se);sSo=r(oUr,"from_pretrained()"),oUr.forEach(t),lSo=r(vz,"to load the model weights."),vz.forEach(t),iSo=i(ml),Ise=n(ml,"P",{});var rUr=s(Ise);dSo=r(rUr,"Examples:"),rUr.forEach(t),cSo=i(ml),m(C3.$$.fragment,ml),ml.forEach(t),fSo=i(fl),Xe=n(fl,"DIV",{class:!0});var Wt=s(Xe);m(M3.$$.fragment,Wt),mSo=i(Wt),jse=n(Wt,"P",{});var tUr=s(jse);gSo=r(tUr,"Instantiate one of the model classes of the library (with a audio classification head) from a pretrained model."),tUr.forEach(t),hSo=i(Wt),Ka=n(Wt,"P",{});var T4=s(Ka);pSo=r(T4,"The model class to instantiate is selected based on the "),Nse=n(T4,"CODE",{});var aUr=s(Nse);_So=r(aUr,"model_type"),aUr.forEach(t),uSo=r(T4,` property of the config object (either
passed as an argument or loaded from `),Dse=n(T4,"CODE",{});var nUr=s(Dse);bSo=r(nUr,"pretrained_model_name_or_path"),nUr.forEach(t),vSo=r(T4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),qse=n(T4,"CODE",{});var sUr=s(qse);TSo=r(sUr,"pretrained_model_name_or_path"),sUr.forEach(t),FSo=r(T4,":"),T4.forEach(t),CSo=i(Wt),ao=n(Wt,"UL",{});var Qt=s(ao);x5=n(Qt,"LI",{});var d3e=s(x5);Gse=n(d3e,"STRONG",{});var lUr=s(Gse);MSo=r(lUr,"hubert"),lUr.forEach(t),ESo=r(d3e," \u2014 "),rN=n(d3e,"A",{href:!0});var iUr=s(rN);ySo=r(iUr,"HubertForSequenceClassification"),iUr.forEach(t),wSo=r(d3e," (Hubert model)"),d3e.forEach(t),ASo=i(Qt),R5=n(Qt,"LI",{});var c3e=s(R5);Ose=n(c3e,"STRONG",{});var dUr=s(Ose);LSo=r(dUr,"sew"),dUr.forEach(t),BSo=r(c3e," \u2014 "),tN=n(c3e,"A",{href:!0});var cUr=s(tN);kSo=r(cUr,"SEWForSequenceClassification"),cUr.forEach(t),xSo=r(c3e," (SEW model)"),c3e.forEach(t),RSo=i(Qt),S5=n(Qt,"LI",{});var f3e=s(S5);Xse=n(f3e,"STRONG",{});var fUr=s(Xse);SSo=r(fUr,"sew-d"),fUr.forEach(t),PSo=r(f3e," \u2014 "),aN=n(f3e,"A",{href:!0});var mUr=s(aN);$So=r(mUr,"SEWDForSequenceClassification"),mUr.forEach(t),ISo=r(f3e," (SEW-D model)"),f3e.forEach(t),jSo=i(Qt),P5=n(Qt,"LI",{});var m3e=s(P5);zse=n(m3e,"STRONG",{});var gUr=s(zse);NSo=r(gUr,"unispeech"),gUr.forEach(t),DSo=r(m3e," \u2014 "),nN=n(m3e,"A",{href:!0});var hUr=s(nN);qSo=r(hUr,"UniSpeechForSequenceClassification"),hUr.forEach(t),GSo=r(m3e," (UniSpeech model)"),m3e.forEach(t),OSo=i(Qt),$5=n(Qt,"LI",{});var g3e=s($5);Vse=n(g3e,"STRONG",{});var pUr=s(Vse);XSo=r(pUr,"unispeech-sat"),pUr.forEach(t),zSo=r(g3e," \u2014 "),sN=n(g3e,"A",{href:!0});var _Ur=s(sN);VSo=r(_Ur,"UniSpeechSatForSequenceClassification"),_Ur.forEach(t),WSo=r(g3e," (UniSpeechSat model)"),g3e.forEach(t),QSo=i(Qt),I5=n(Qt,"LI",{});var h3e=s(I5);Wse=n(h3e,"STRONG",{});var uUr=s(Wse);HSo=r(uUr,"wav2vec2"),uUr.forEach(t),USo=r(h3e," \u2014 "),lN=n(h3e,"A",{href:!0});var bUr=s(lN);JSo=r(bUr,"Wav2Vec2ForSequenceClassification"),bUr.forEach(t),YSo=r(h3e," (Wav2Vec2 model)"),h3e.forEach(t),KSo=i(Qt),j5=n(Qt,"LI",{});var p3e=s(j5);Qse=n(p3e,"STRONG",{});var vUr=s(Qse);ZSo=r(vUr,"wavlm"),vUr.forEach(t),ePo=r(p3e," \u2014 "),iN=n(p3e,"A",{href:!0});var TUr=s(iN);oPo=r(TUr,"WavLMForSequenceClassification"),TUr.forEach(t),rPo=r(p3e," (WavLM model)"),p3e.forEach(t),Qt.forEach(t),tPo=i(Wt),N5=n(Wt,"P",{});var _3e=s(N5);aPo=r(_3e,"The model is set in evaluation mode by default using "),Hse=n(_3e,"CODE",{});var FUr=s(Hse);nPo=r(FUr,"model.eval()"),FUr.forEach(t),sPo=r(_3e,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Use=n(_3e,"CODE",{});var CUr=s(Use);lPo=r(CUr,"model.train()"),CUr.forEach(t),_3e.forEach(t),iPo=i(Wt),Jse=n(Wt,"P",{});var MUr=s(Jse);dPo=r(MUr,"Examples:"),MUr.forEach(t),cPo=i(Wt),m(E3.$$.fragment,Wt),Wt.forEach(t),fl.forEach(t),zLe=i(d),Ld=n(d,"H2",{class:!0});var YBe=s(Ld);D5=n(YBe,"A",{id:!0,class:!0,href:!0});var EUr=s(D5);Yse=n(EUr,"SPAN",{});var yUr=s(Yse);m(y3.$$.fragment,yUr),yUr.forEach(t),EUr.forEach(t),fPo=i(YBe),Kse=n(YBe,"SPAN",{});var wUr=s(Kse);mPo=r(wUr,"AutoModelForAudioFrameClassification"),wUr.forEach(t),YBe.forEach(t),VLe=i(d),nr=n(d,"DIV",{class:!0});var gl=s(nr);m(w3.$$.fragment,gl),gPo=i(gl),Bd=n(gl,"P",{});var Tz=s(Bd);hPo=r(Tz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a audio frame (token) classification head) when created
with the `),Zse=n(Tz,"CODE",{});var AUr=s(Zse);pPo=r(AUr,"from_pretrained()"),AUr.forEach(t),_Po=r(Tz,"class method or the "),ele=n(Tz,"CODE",{});var LUr=s(ele);uPo=r(LUr,"from_config()"),LUr.forEach(t),bPo=r(Tz,`class
method.`),Tz.forEach(t),vPo=i(gl),A3=n(gl,"P",{});var KBe=s(A3);TPo=r(KBe,"This class cannot be instantiated directly using "),ole=n(KBe,"CODE",{});var BUr=s(ole);FPo=r(BUr,"__init__()"),BUr.forEach(t),CPo=r(KBe," (throws an error)."),KBe.forEach(t),MPo=i(gl),Kr=n(gl,"DIV",{class:!0});var hl=s(Kr);m(L3.$$.fragment,hl),EPo=i(hl),rle=n(hl,"P",{});var kUr=s(rle);yPo=r(kUr,"Instantiates one of the model classes of the library (with a audio frame (token) classification head) from a configuration."),kUr.forEach(t),wPo=i(hl),kd=n(hl,"P",{});var Fz=s(kd);APo=r(Fz,`Note:
Loading a model from its configuration file does `),tle=n(Fz,"STRONG",{});var xUr=s(tle);LPo=r(xUr,"not"),xUr.forEach(t),BPo=r(Fz,` load the model weights. It only affects the
model\u2019s configuration. Use `),ale=n(Fz,"CODE",{});var RUr=s(ale);kPo=r(RUr,"from_pretrained()"),RUr.forEach(t),xPo=r(Fz,"to load the model weights."),Fz.forEach(t),RPo=i(hl),nle=n(hl,"P",{});var SUr=s(nle);SPo=r(SUr,"Examples:"),SUr.forEach(t),PPo=i(hl),m(B3.$$.fragment,hl),hl.forEach(t),$Po=i(gl),ze=n(gl,"DIV",{class:!0});var Ht=s(ze);m(k3.$$.fragment,Ht),IPo=i(Ht),sle=n(Ht,"P",{});var PUr=s(sle);jPo=r(PUr,"Instantiate one of the model classes of the library (with a audio frame (token) classification head) from a pretrained model."),PUr.forEach(t),NPo=i(Ht),Za=n(Ht,"P",{});var F4=s(Za);DPo=r(F4,"The model class to instantiate is selected based on the "),lle=n(F4,"CODE",{});var $Ur=s(lle);qPo=r($Ur,"model_type"),$Ur.forEach(t),GPo=r(F4,` property of the config object (either
passed as an argument or loaded from `),ile=n(F4,"CODE",{});var IUr=s(ile);OPo=r(IUr,"pretrained_model_name_or_path"),IUr.forEach(t),XPo=r(F4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),dle=n(F4,"CODE",{});var jUr=s(dle);zPo=r(jUr,"pretrained_model_name_or_path"),jUr.forEach(t),VPo=r(F4,":"),F4.forEach(t),WPo=i(Ht),xd=n(Ht,"UL",{});var Cz=s(xd);q5=n(Cz,"LI",{});var u3e=s(q5);cle=n(u3e,"STRONG",{});var NUr=s(cle);QPo=r(NUr,"unispeech-sat"),NUr.forEach(t),HPo=r(u3e," \u2014 "),dN=n(u3e,"A",{href:!0});var DUr=s(dN);UPo=r(DUr,"UniSpeechSatForAudioFrameClassification"),DUr.forEach(t),JPo=r(u3e," (UniSpeechSat model)"),u3e.forEach(t),YPo=i(Cz),G5=n(Cz,"LI",{});var b3e=s(G5);fle=n(b3e,"STRONG",{});var qUr=s(fle);KPo=r(qUr,"wav2vec2"),qUr.forEach(t),ZPo=r(b3e," \u2014 "),cN=n(b3e,"A",{href:!0});var GUr=s(cN);e$o=r(GUr,"Wav2Vec2ForAudioFrameClassification"),GUr.forEach(t),o$o=r(b3e," (Wav2Vec2 model)"),b3e.forEach(t),r$o=i(Cz),O5=n(Cz,"LI",{});var v3e=s(O5);mle=n(v3e,"STRONG",{});var OUr=s(mle);t$o=r(OUr,"wavlm"),OUr.forEach(t),a$o=r(v3e," \u2014 "),fN=n(v3e,"A",{href:!0});var XUr=s(fN);n$o=r(XUr,"WavLMForAudioFrameClassification"),XUr.forEach(t),s$o=r(v3e," (WavLM model)"),v3e.forEach(t),Cz.forEach(t),l$o=i(Ht),X5=n(Ht,"P",{});var T3e=s(X5);i$o=r(T3e,"The model is set in evaluation mode by default using "),gle=n(T3e,"CODE",{});var zUr=s(gle);d$o=r(zUr,"model.eval()"),zUr.forEach(t),c$o=r(T3e,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),hle=n(T3e,"CODE",{});var VUr=s(hle);f$o=r(VUr,"model.train()"),VUr.forEach(t),T3e.forEach(t),m$o=i(Ht),ple=n(Ht,"P",{});var WUr=s(ple);g$o=r(WUr,"Examples:"),WUr.forEach(t),h$o=i(Ht),m(x3.$$.fragment,Ht),Ht.forEach(t),gl.forEach(t),WLe=i(d),Rd=n(d,"H2",{class:!0});var ZBe=s(Rd);z5=n(ZBe,"A",{id:!0,class:!0,href:!0});var QUr=s(z5);_le=n(QUr,"SPAN",{});var HUr=s(_le);m(R3.$$.fragment,HUr),HUr.forEach(t),QUr.forEach(t),p$o=i(ZBe),ule=n(ZBe,"SPAN",{});var UUr=s(ule);_$o=r(UUr,"AutoModelForCTC"),UUr.forEach(t),ZBe.forEach(t),QLe=i(d),sr=n(d,"DIV",{class:!0});var pl=s(sr);m(S3.$$.fragment,pl),u$o=i(pl),Sd=n(pl,"P",{});var Mz=s(Sd);b$o=r(Mz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a connectionist temporal classification head) when created
with the `),ble=n(Mz,"CODE",{});var JUr=s(ble);v$o=r(JUr,"from_pretrained()"),JUr.forEach(t),T$o=r(Mz,"class method or the "),vle=n(Mz,"CODE",{});var YUr=s(vle);F$o=r(YUr,"from_config()"),YUr.forEach(t),C$o=r(Mz,`class
method.`),Mz.forEach(t),M$o=i(pl),P3=n(pl,"P",{});var eke=s(P3);E$o=r(eke,"This class cannot be instantiated directly using "),Tle=n(eke,"CODE",{});var KUr=s(Tle);y$o=r(KUr,"__init__()"),KUr.forEach(t),w$o=r(eke," (throws an error)."),eke.forEach(t),A$o=i(pl),Zr=n(pl,"DIV",{class:!0});var _l=s(Zr);m($3.$$.fragment,_l),L$o=i(_l),Fle=n(_l,"P",{});var ZUr=s(Fle);B$o=r(ZUr,"Instantiates one of the model classes of the library (with a connectionist temporal classification head) from a configuration."),ZUr.forEach(t),k$o=i(_l),Pd=n(_l,"P",{});var Ez=s(Pd);x$o=r(Ez,`Note:
Loading a model from its configuration file does `),Cle=n(Ez,"STRONG",{});var eJr=s(Cle);R$o=r(eJr,"not"),eJr.forEach(t),S$o=r(Ez,` load the model weights. It only affects the
model\u2019s configuration. Use `),Mle=n(Ez,"CODE",{});var oJr=s(Mle);P$o=r(oJr,"from_pretrained()"),oJr.forEach(t),$$o=r(Ez,"to load the model weights."),Ez.forEach(t),I$o=i(_l),Ele=n(_l,"P",{});var rJr=s(Ele);j$o=r(rJr,"Examples:"),rJr.forEach(t),N$o=i(_l),m(I3.$$.fragment,_l),_l.forEach(t),D$o=i(pl),Ve=n(pl,"DIV",{class:!0});var Ut=s(Ve);m(j3.$$.fragment,Ut),q$o=i(Ut),yle=n(Ut,"P",{});var tJr=s(yle);G$o=r(tJr,"Instantiate one of the model classes of the library (with a connectionist temporal classification head) from a pretrained model."),tJr.forEach(t),O$o=i(Ut),en=n(Ut,"P",{});var C4=s(en);X$o=r(C4,"The model class to instantiate is selected based on the "),wle=n(C4,"CODE",{});var aJr=s(wle);z$o=r(aJr,"model_type"),aJr.forEach(t),V$o=r(C4,` property of the config object (either
passed as an argument or loaded from `),Ale=n(C4,"CODE",{});var nJr=s(Ale);W$o=r(nJr,"pretrained_model_name_or_path"),nJr.forEach(t),Q$o=r(C4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Lle=n(C4,"CODE",{});var sJr=s(Lle);H$o=r(sJr,"pretrained_model_name_or_path"),sJr.forEach(t),U$o=r(C4,":"),C4.forEach(t),J$o=i(Ut),no=n(Ut,"UL",{});var Jt=s(no);V5=n(Jt,"LI",{});var F3e=s(V5);Ble=n(F3e,"STRONG",{});var lJr=s(Ble);Y$o=r(lJr,"hubert"),lJr.forEach(t),K$o=r(F3e," \u2014 "),mN=n(F3e,"A",{href:!0});var iJr=s(mN);Z$o=r(iJr,"HubertForCTC"),iJr.forEach(t),eIo=r(F3e," (Hubert model)"),F3e.forEach(t),oIo=i(Jt),W5=n(Jt,"LI",{});var C3e=s(W5);kle=n(C3e,"STRONG",{});var dJr=s(kle);rIo=r(dJr,"sew"),dJr.forEach(t),tIo=r(C3e," \u2014 "),gN=n(C3e,"A",{href:!0});var cJr=s(gN);aIo=r(cJr,"SEWForCTC"),cJr.forEach(t),nIo=r(C3e," (SEW model)"),C3e.forEach(t),sIo=i(Jt),Q5=n(Jt,"LI",{});var M3e=s(Q5);xle=n(M3e,"STRONG",{});var fJr=s(xle);lIo=r(fJr,"sew-d"),fJr.forEach(t),iIo=r(M3e," \u2014 "),hN=n(M3e,"A",{href:!0});var mJr=s(hN);dIo=r(mJr,"SEWDForCTC"),mJr.forEach(t),cIo=r(M3e," (SEW-D model)"),M3e.forEach(t),fIo=i(Jt),H5=n(Jt,"LI",{});var E3e=s(H5);Rle=n(E3e,"STRONG",{});var gJr=s(Rle);mIo=r(gJr,"unispeech"),gJr.forEach(t),gIo=r(E3e," \u2014 "),pN=n(E3e,"A",{href:!0});var hJr=s(pN);hIo=r(hJr,"UniSpeechForCTC"),hJr.forEach(t),pIo=r(E3e," (UniSpeech model)"),E3e.forEach(t),_Io=i(Jt),U5=n(Jt,"LI",{});var y3e=s(U5);Sle=n(y3e,"STRONG",{});var pJr=s(Sle);uIo=r(pJr,"unispeech-sat"),pJr.forEach(t),bIo=r(y3e," \u2014 "),_N=n(y3e,"A",{href:!0});var _Jr=s(_N);vIo=r(_Jr,"UniSpeechSatForCTC"),_Jr.forEach(t),TIo=r(y3e," (UniSpeechSat model)"),y3e.forEach(t),FIo=i(Jt),J5=n(Jt,"LI",{});var w3e=s(J5);Ple=n(w3e,"STRONG",{});var uJr=s(Ple);CIo=r(uJr,"wav2vec2"),uJr.forEach(t),MIo=r(w3e," \u2014 "),uN=n(w3e,"A",{href:!0});var bJr=s(uN);EIo=r(bJr,"Wav2Vec2ForCTC"),bJr.forEach(t),yIo=r(w3e," (Wav2Vec2 model)"),w3e.forEach(t),wIo=i(Jt),Y5=n(Jt,"LI",{});var A3e=s(Y5);$le=n(A3e,"STRONG",{});var vJr=s($le);AIo=r(vJr,"wavlm"),vJr.forEach(t),LIo=r(A3e," \u2014 "),bN=n(A3e,"A",{href:!0});var TJr=s(bN);BIo=r(TJr,"WavLMForCTC"),TJr.forEach(t),kIo=r(A3e," (WavLM model)"),A3e.forEach(t),Jt.forEach(t),xIo=i(Ut),K5=n(Ut,"P",{});var L3e=s(K5);RIo=r(L3e,"The model is set in evaluation mode by default using "),Ile=n(L3e,"CODE",{});var FJr=s(Ile);SIo=r(FJr,"model.eval()"),FJr.forEach(t),PIo=r(L3e,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),jle=n(L3e,"CODE",{});var CJr=s(jle);$Io=r(CJr,"model.train()"),CJr.forEach(t),L3e.forEach(t),IIo=i(Ut),Nle=n(Ut,"P",{});var MJr=s(Nle);jIo=r(MJr,"Examples:"),MJr.forEach(t),NIo=i(Ut),m(N3.$$.fragment,Ut),Ut.forEach(t),pl.forEach(t),HLe=i(d),$d=n(d,"H2",{class:!0});var oke=s($d);Z5=n(oke,"A",{id:!0,class:!0,href:!0});var EJr=s(Z5);Dle=n(EJr,"SPAN",{});var yJr=s(Dle);m(D3.$$.fragment,yJr),yJr.forEach(t),EJr.forEach(t),DIo=i(oke),qle=n(oke,"SPAN",{});var wJr=s(qle);qIo=r(wJr,"AutoModelForSpeechSeq2Seq"),wJr.forEach(t),oke.forEach(t),ULe=i(d),lr=n(d,"DIV",{class:!0});var ul=s(lr);m(q3.$$.fragment,ul),GIo=i(ul),Id=n(ul,"P",{});var yz=s(Id);OIo=r(yz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) when created
with the `),Gle=n(yz,"CODE",{});var AJr=s(Gle);XIo=r(AJr,"from_pretrained()"),AJr.forEach(t),zIo=r(yz,"class method or the "),Ole=n(yz,"CODE",{});var LJr=s(Ole);VIo=r(LJr,"from_config()"),LJr.forEach(t),WIo=r(yz,`class
method.`),yz.forEach(t),QIo=i(ul),G3=n(ul,"P",{});var rke=s(G3);HIo=r(rke,"This class cannot be instantiated directly using "),Xle=n(rke,"CODE",{});var BJr=s(Xle);UIo=r(BJr,"__init__()"),BJr.forEach(t),JIo=r(rke," (throws an error)."),rke.forEach(t),YIo=i(ul),et=n(ul,"DIV",{class:!0});var bl=s(et);m(O3.$$.fragment,bl),KIo=i(bl),zle=n(bl,"P",{});var kJr=s(zle);ZIo=r(kJr,"Instantiates one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) from a configuration."),kJr.forEach(t),ejo=i(bl),jd=n(bl,"P",{});var wz=s(jd);ojo=r(wz,`Note:
Loading a model from its configuration file does `),Vle=n(wz,"STRONG",{});var xJr=s(Vle);rjo=r(xJr,"not"),xJr.forEach(t),tjo=r(wz,` load the model weights. It only affects the
model\u2019s configuration. Use `),Wle=n(wz,"CODE",{});var RJr=s(Wle);ajo=r(RJr,"from_pretrained()"),RJr.forEach(t),njo=r(wz,"to load the model weights."),wz.forEach(t),sjo=i(bl),Qle=n(bl,"P",{});var SJr=s(Qle);ljo=r(SJr,"Examples:"),SJr.forEach(t),ijo=i(bl),m(X3.$$.fragment,bl),bl.forEach(t),djo=i(ul),We=n(ul,"DIV",{class:!0});var Yt=s(We);m(z3.$$.fragment,Yt),cjo=i(Yt),Hle=n(Yt,"P",{});var PJr=s(Hle);fjo=r(PJr,"Instantiate one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) from a pretrained model."),PJr.forEach(t),mjo=i(Yt),on=n(Yt,"P",{});var M4=s(on);gjo=r(M4,"The model class to instantiate is selected based on the "),Ule=n(M4,"CODE",{});var $Jr=s(Ule);hjo=r($Jr,"model_type"),$Jr.forEach(t),pjo=r(M4,` property of the config object (either
passed as an argument or loaded from `),Jle=n(M4,"CODE",{});var IJr=s(Jle);_jo=r(IJr,"pretrained_model_name_or_path"),IJr.forEach(t),ujo=r(M4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Yle=n(M4,"CODE",{});var jJr=s(Yle);bjo=r(jJr,"pretrained_model_name_or_path"),jJr.forEach(t),vjo=r(M4,":"),M4.forEach(t),Tjo=i(Yt),V3=n(Yt,"UL",{});var tke=s(V3);ev=n(tke,"LI",{});var B3e=s(ev);Kle=n(B3e,"STRONG",{});var NJr=s(Kle);Fjo=r(NJr,"speech-encoder-decoder"),NJr.forEach(t),Cjo=r(B3e," \u2014 "),vN=n(B3e,"A",{href:!0});var DJr=s(vN);Mjo=r(DJr,"SpeechEncoderDecoderModel"),DJr.forEach(t),Ejo=r(B3e," (Speech Encoder decoder model)"),B3e.forEach(t),yjo=i(tke),ov=n(tke,"LI",{});var k3e=s(ov);Zle=n(k3e,"STRONG",{});var qJr=s(Zle);wjo=r(qJr,"speech_to_text"),qJr.forEach(t),Ajo=r(k3e," \u2014 "),TN=n(k3e,"A",{href:!0});var GJr=s(TN);Ljo=r(GJr,"Speech2TextForConditionalGeneration"),GJr.forEach(t),Bjo=r(k3e," (Speech2Text model)"),k3e.forEach(t),tke.forEach(t),kjo=i(Yt),rv=n(Yt,"P",{});var x3e=s(rv);xjo=r(x3e,"The model is set in evaluation mode by default using "),eie=n(x3e,"CODE",{});var OJr=s(eie);Rjo=r(OJr,"model.eval()"),OJr.forEach(t),Sjo=r(x3e,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),oie=n(x3e,"CODE",{});var XJr=s(oie);Pjo=r(XJr,"model.train()"),XJr.forEach(t),x3e.forEach(t),$jo=i(Yt),rie=n(Yt,"P",{});var zJr=s(rie);Ijo=r(zJr,"Examples:"),zJr.forEach(t),jjo=i(Yt),m(W3.$$.fragment,Yt),Yt.forEach(t),ul.forEach(t),JLe=i(d),Nd=n(d,"H2",{class:!0});var ake=s(Nd);tv=n(ake,"A",{id:!0,class:!0,href:!0});var VJr=s(tv);tie=n(VJr,"SPAN",{});var WJr=s(tie);m(Q3.$$.fragment,WJr),WJr.forEach(t),VJr.forEach(t),Njo=i(ake),aie=n(ake,"SPAN",{});var QJr=s(aie);Djo=r(QJr,"AutoModelForAudioXVector"),QJr.forEach(t),ake.forEach(t),YLe=i(d),ir=n(d,"DIV",{class:!0});var vl=s(ir);m(H3.$$.fragment,vl),qjo=i(vl),Dd=n(vl,"P",{});var Az=s(Dd);Gjo=r(Az,`This is a generic model class that will be instantiated as one of the model classes of the library (with a audio retrieval via x-vector head) when created
with the `),nie=n(Az,"CODE",{});var HJr=s(nie);Ojo=r(HJr,"from_pretrained()"),HJr.forEach(t),Xjo=r(Az,"class method or the "),sie=n(Az,"CODE",{});var UJr=s(sie);zjo=r(UJr,"from_config()"),UJr.forEach(t),Vjo=r(Az,`class
method.`),Az.forEach(t),Wjo=i(vl),U3=n(vl,"P",{});var nke=s(U3);Qjo=r(nke,"This class cannot be instantiated directly using "),lie=n(nke,"CODE",{});var JJr=s(lie);Hjo=r(JJr,"__init__()"),JJr.forEach(t),Ujo=r(nke," (throws an error)."),nke.forEach(t),Jjo=i(vl),ot=n(vl,"DIV",{class:!0});var Tl=s(ot);m(J3.$$.fragment,Tl),Yjo=i(Tl),iie=n(Tl,"P",{});var YJr=s(iie);Kjo=r(YJr,"Instantiates one of the model classes of the library (with a audio retrieval via x-vector head) from a configuration."),YJr.forEach(t),Zjo=i(Tl),qd=n(Tl,"P",{});var Lz=s(qd);eNo=r(Lz,`Note:
Loading a model from its configuration file does `),die=n(Lz,"STRONG",{});var KJr=s(die);oNo=r(KJr,"not"),KJr.forEach(t),rNo=r(Lz,` load the model weights. It only affects the
model\u2019s configuration. Use `),cie=n(Lz,"CODE",{});var ZJr=s(cie);tNo=r(ZJr,"from_pretrained()"),ZJr.forEach(t),aNo=r(Lz,"to load the model weights."),Lz.forEach(t),nNo=i(Tl),fie=n(Tl,"P",{});var eYr=s(fie);sNo=r(eYr,"Examples:"),eYr.forEach(t),lNo=i(Tl),m(Y3.$$.fragment,Tl),Tl.forEach(t),iNo=i(vl),Qe=n(vl,"DIV",{class:!0});var Kt=s(Qe);m(K3.$$.fragment,Kt),dNo=i(Kt),mie=n(Kt,"P",{});var oYr=s(mie);cNo=r(oYr,"Instantiate one of the model classes of the library (with a audio retrieval via x-vector head) from a pretrained model."),oYr.forEach(t),fNo=i(Kt),rn=n(Kt,"P",{});var E4=s(rn);mNo=r(E4,"The model class to instantiate is selected based on the "),gie=n(E4,"CODE",{});var rYr=s(gie);gNo=r(rYr,"model_type"),rYr.forEach(t),hNo=r(E4,` property of the config object (either
passed as an argument or loaded from `),hie=n(E4,"CODE",{});var tYr=s(hie);pNo=r(tYr,"pretrained_model_name_or_path"),tYr.forEach(t),_No=r(E4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),pie=n(E4,"CODE",{});var aYr=s(pie);uNo=r(aYr,"pretrained_model_name_or_path"),aYr.forEach(t),bNo=r(E4,":"),E4.forEach(t),vNo=i(Kt),Gd=n(Kt,"UL",{});var Bz=s(Gd);av=n(Bz,"LI",{});var R3e=s(av);_ie=n(R3e,"STRONG",{});var nYr=s(_ie);TNo=r(nYr,"unispeech-sat"),nYr.forEach(t),FNo=r(R3e," \u2014 "),FN=n(R3e,"A",{href:!0});var sYr=s(FN);CNo=r(sYr,"UniSpeechSatForXVector"),sYr.forEach(t),MNo=r(R3e," (UniSpeechSat model)"),R3e.forEach(t),ENo=i(Bz),nv=n(Bz,"LI",{});var S3e=s(nv);uie=n(S3e,"STRONG",{});var lYr=s(uie);yNo=r(lYr,"wav2vec2"),lYr.forEach(t),wNo=r(S3e," \u2014 "),CN=n(S3e,"A",{href:!0});var iYr=s(CN);ANo=r(iYr,"Wav2Vec2ForXVector"),iYr.forEach(t),LNo=r(S3e," (Wav2Vec2 model)"),S3e.forEach(t),BNo=i(Bz),sv=n(Bz,"LI",{});var P3e=s(sv);bie=n(P3e,"STRONG",{});var dYr=s(bie);kNo=r(dYr,"wavlm"),dYr.forEach(t),xNo=r(P3e," \u2014 "),MN=n(P3e,"A",{href:!0});var cYr=s(MN);RNo=r(cYr,"WavLMForXVector"),cYr.forEach(t),SNo=r(P3e," (WavLM model)"),P3e.forEach(t),Bz.forEach(t),PNo=i(Kt),lv=n(Kt,"P",{});var $3e=s(lv);$No=r($3e,"The model is set in evaluation mode by default using "),vie=n($3e,"CODE",{});var fYr=s(vie);INo=r(fYr,"model.eval()"),fYr.forEach(t),jNo=r($3e,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Tie=n($3e,"CODE",{});var mYr=s(Tie);NNo=r(mYr,"model.train()"),mYr.forEach(t),$3e.forEach(t),DNo=i(Kt),Fie=n(Kt,"P",{});var gYr=s(Fie);qNo=r(gYr,"Examples:"),gYr.forEach(t),GNo=i(Kt),m(Z3.$$.fragment,Kt),Kt.forEach(t),vl.forEach(t),KLe=i(d),Od=n(d,"H2",{class:!0});var ske=s(Od);iv=n(ske,"A",{id:!0,class:!0,href:!0});var hYr=s(iv);Cie=n(hYr,"SPAN",{});var pYr=s(Cie);m(ey.$$.fragment,pYr),pYr.forEach(t),hYr.forEach(t),ONo=i(ske),Mie=n(ske,"SPAN",{});var _Yr=s(Mie);XNo=r(_Yr,"AutoModelForMaskedImageModeling"),_Yr.forEach(t),ske.forEach(t),ZLe=i(d),dr=n(d,"DIV",{class:!0});var Fl=s(dr);m(oy.$$.fragment,Fl),zNo=i(Fl),Xd=n(Fl,"P",{});var kz=s(Xd);VNo=r(kz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a masked image modeling head) when created
with the `),Eie=n(kz,"CODE",{});var uYr=s(Eie);WNo=r(uYr,"from_pretrained()"),uYr.forEach(t),QNo=r(kz,"class method or the "),yie=n(kz,"CODE",{});var bYr=s(yie);HNo=r(bYr,"from_config()"),bYr.forEach(t),UNo=r(kz,`class
method.`),kz.forEach(t),JNo=i(Fl),ry=n(Fl,"P",{});var lke=s(ry);YNo=r(lke,"This class cannot be instantiated directly using "),wie=n(lke,"CODE",{});var vYr=s(wie);KNo=r(vYr,"__init__()"),vYr.forEach(t),ZNo=r(lke," (throws an error)."),lke.forEach(t),eDo=i(Fl),rt=n(Fl,"DIV",{class:!0});var Cl=s(rt);m(ty.$$.fragment,Cl),oDo=i(Cl),Aie=n(Cl,"P",{});var TYr=s(Aie);rDo=r(TYr,"Instantiates one of the model classes of the library (with a masked image modeling head) from a configuration."),TYr.forEach(t),tDo=i(Cl),zd=n(Cl,"P",{});var xz=s(zd);aDo=r(xz,`Note:
Loading a model from its configuration file does `),Lie=n(xz,"STRONG",{});var FYr=s(Lie);nDo=r(FYr,"not"),FYr.forEach(t),sDo=r(xz,` load the model weights. It only affects the
model\u2019s configuration. Use `),Bie=n(xz,"CODE",{});var CYr=s(Bie);lDo=r(CYr,"from_pretrained()"),CYr.forEach(t),iDo=r(xz,"to load the model weights."),xz.forEach(t),dDo=i(Cl),kie=n(Cl,"P",{});var MYr=s(kie);cDo=r(MYr,"Examples:"),MYr.forEach(t),fDo=i(Cl),m(ay.$$.fragment,Cl),Cl.forEach(t),mDo=i(Fl),He=n(Fl,"DIV",{class:!0});var Zt=s(He);m(ny.$$.fragment,Zt),gDo=i(Zt),xie=n(Zt,"P",{});var EYr=s(xie);hDo=r(EYr,"Instantiate one of the model classes of the library (with a masked image modeling head) from a pretrained model."),EYr.forEach(t),pDo=i(Zt),tn=n(Zt,"P",{});var y4=s(tn);_Do=r(y4,"The model class to instantiate is selected based on the "),Rie=n(y4,"CODE",{});var yYr=s(Rie);uDo=r(yYr,"model_type"),yYr.forEach(t),bDo=r(y4,` property of the config object (either
passed as an argument or loaded from `),Sie=n(y4,"CODE",{});var wYr=s(Sie);vDo=r(wYr,"pretrained_model_name_or_path"),wYr.forEach(t),TDo=r(y4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Pie=n(y4,"CODE",{});var AYr=s(Pie);FDo=r(AYr,"pretrained_model_name_or_path"),AYr.forEach(t),CDo=r(y4,":"),y4.forEach(t),MDo=i(Zt),Vd=n(Zt,"UL",{});var Rz=s(Vd);dv=n(Rz,"LI",{});var I3e=s(dv);$ie=n(I3e,"STRONG",{});var LYr=s($ie);EDo=r(LYr,"deit"),LYr.forEach(t),yDo=r(I3e," \u2014 "),EN=n(I3e,"A",{href:!0});var BYr=s(EN);wDo=r(BYr,"DeiTForMaskedImageModeling"),BYr.forEach(t),ADo=r(I3e," (DeiT model)"),I3e.forEach(t),LDo=i(Rz),cv=n(Rz,"LI",{});var j3e=s(cv);Iie=n(j3e,"STRONG",{});var kYr=s(Iie);BDo=r(kYr,"swin"),kYr.forEach(t),kDo=r(j3e," \u2014 "),yN=n(j3e,"A",{href:!0});var xYr=s(yN);xDo=r(xYr,"SwinForMaskedImageModeling"),xYr.forEach(t),RDo=r(j3e," (Swin model)"),j3e.forEach(t),SDo=i(Rz),fv=n(Rz,"LI",{});var N3e=s(fv);jie=n(N3e,"STRONG",{});var RYr=s(jie);PDo=r(RYr,"vit"),RYr.forEach(t),$Do=r(N3e," \u2014 "),wN=n(N3e,"A",{href:!0});var SYr=s(wN);IDo=r(SYr,"ViTForMaskedImageModeling"),SYr.forEach(t),jDo=r(N3e," (ViT model)"),N3e.forEach(t),Rz.forEach(t),NDo=i(Zt),mv=n(Zt,"P",{});var D3e=s(mv);DDo=r(D3e,"The model is set in evaluation mode by default using "),Nie=n(D3e,"CODE",{});var PYr=s(Nie);qDo=r(PYr,"model.eval()"),PYr.forEach(t),GDo=r(D3e,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Die=n(D3e,"CODE",{});var $Yr=s(Die);ODo=r($Yr,"model.train()"),$Yr.forEach(t),D3e.forEach(t),XDo=i(Zt),qie=n(Zt,"P",{});var IYr=s(qie);zDo=r(IYr,"Examples:"),IYr.forEach(t),VDo=i(Zt),m(sy.$$.fragment,Zt),Zt.forEach(t),Fl.forEach(t),e8e=i(d),Wd=n(d,"H2",{class:!0});var ike=s(Wd);gv=n(ike,"A",{id:!0,class:!0,href:!0});var jYr=s(gv);Gie=n(jYr,"SPAN",{});var NYr=s(Gie);m(ly.$$.fragment,NYr),NYr.forEach(t),jYr.forEach(t),WDo=i(ike),Oie=n(ike,"SPAN",{});var DYr=s(Oie);QDo=r(DYr,"AutoModelForObjectDetection"),DYr.forEach(t),ike.forEach(t),o8e=i(d),cr=n(d,"DIV",{class:!0});var Ml=s(cr);m(iy.$$.fragment,Ml),HDo=i(Ml),Qd=n(Ml,"P",{});var Sz=s(Qd);UDo=r(Sz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a object detection head) when created
with the `),Xie=n(Sz,"CODE",{});var qYr=s(Xie);JDo=r(qYr,"from_pretrained()"),qYr.forEach(t),YDo=r(Sz,"class method or the "),zie=n(Sz,"CODE",{});var GYr=s(zie);KDo=r(GYr,"from_config()"),GYr.forEach(t),ZDo=r(Sz,`class
method.`),Sz.forEach(t),eqo=i(Ml),dy=n(Ml,"P",{});var dke=s(dy);oqo=r(dke,"This class cannot be instantiated directly using "),Vie=n(dke,"CODE",{});var OYr=s(Vie);rqo=r(OYr,"__init__()"),OYr.forEach(t),tqo=r(dke," (throws an error)."),dke.forEach(t),aqo=i(Ml),tt=n(Ml,"DIV",{class:!0});var El=s(tt);m(cy.$$.fragment,El),nqo=i(El),Wie=n(El,"P",{});var XYr=s(Wie);sqo=r(XYr,"Instantiates one of the model classes of the library (with a object detection head) from a configuration."),XYr.forEach(t),lqo=i(El),Hd=n(El,"P",{});var Pz=s(Hd);iqo=r(Pz,`Note:
Loading a model from its configuration file does `),Qie=n(Pz,"STRONG",{});var zYr=s(Qie);dqo=r(zYr,"not"),zYr.forEach(t),cqo=r(Pz,` load the model weights. It only affects the
model\u2019s configuration. Use `),Hie=n(Pz,"CODE",{});var VYr=s(Hie);fqo=r(VYr,"from_pretrained()"),VYr.forEach(t),mqo=r(Pz,"to load the model weights."),Pz.forEach(t),gqo=i(El),Uie=n(El,"P",{});var WYr=s(Uie);hqo=r(WYr,"Examples:"),WYr.forEach(t),pqo=i(El),m(fy.$$.fragment,El),El.forEach(t),_qo=i(Ml),Ue=n(Ml,"DIV",{class:!0});var ea=s(Ue);m(my.$$.fragment,ea),uqo=i(ea),Jie=n(ea,"P",{});var QYr=s(Jie);bqo=r(QYr,"Instantiate one of the model classes of the library (with a object detection head) from a pretrained model."),QYr.forEach(t),vqo=i(ea),an=n(ea,"P",{});var w4=s(an);Tqo=r(w4,"The model class to instantiate is selected based on the "),Yie=n(w4,"CODE",{});var HYr=s(Yie);Fqo=r(HYr,"model_type"),HYr.forEach(t),Cqo=r(w4,` property of the config object (either
passed as an argument or loaded from `),Kie=n(w4,"CODE",{});var UYr=s(Kie);Mqo=r(UYr,"pretrained_model_name_or_path"),UYr.forEach(t),Eqo=r(w4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Zie=n(w4,"CODE",{});var JYr=s(Zie);yqo=r(JYr,"pretrained_model_name_or_path"),JYr.forEach(t),wqo=r(w4,":"),w4.forEach(t),Aqo=i(ea),ede=n(ea,"UL",{});var YYr=s(ede);hv=n(YYr,"LI",{});var q3e=s(hv);ode=n(q3e,"STRONG",{});var KYr=s(ode);Lqo=r(KYr,"detr"),KYr.forEach(t),Bqo=r(q3e," \u2014 "),AN=n(q3e,"A",{href:!0});var ZYr=s(AN);kqo=r(ZYr,"DetrForObjectDetection"),ZYr.forEach(t),xqo=r(q3e," (DETR model)"),q3e.forEach(t),YYr.forEach(t),Rqo=i(ea),pv=n(ea,"P",{});var G3e=s(pv);Sqo=r(G3e,"The model is set in evaluation mode by default using "),rde=n(G3e,"CODE",{});var eKr=s(rde);Pqo=r(eKr,"model.eval()"),eKr.forEach(t),$qo=r(G3e,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),tde=n(G3e,"CODE",{});var oKr=s(tde);Iqo=r(oKr,"model.train()"),oKr.forEach(t),G3e.forEach(t),jqo=i(ea),ade=n(ea,"P",{});var rKr=s(ade);Nqo=r(rKr,"Examples:"),rKr.forEach(t),Dqo=i(ea),m(gy.$$.fragment,ea),ea.forEach(t),Ml.forEach(t),r8e=i(d),Ud=n(d,"H2",{class:!0});var cke=s(Ud);_v=n(cke,"A",{id:!0,class:!0,href:!0});var tKr=s(_v);nde=n(tKr,"SPAN",{});var aKr=s(nde);m(hy.$$.fragment,aKr),aKr.forEach(t),tKr.forEach(t),qqo=i(cke),sde=n(cke,"SPAN",{});var nKr=s(sde);Gqo=r(nKr,"AutoModelForImageSegmentation"),nKr.forEach(t),cke.forEach(t),t8e=i(d),fr=n(d,"DIV",{class:!0});var yl=s(fr);m(py.$$.fragment,yl),Oqo=i(yl),Jd=n(yl,"P",{});var $z=s(Jd);Xqo=r($z,`This is a generic model class that will be instantiated as one of the model classes of the library (with a image segmentation head) when created
with the `),lde=n($z,"CODE",{});var sKr=s(lde);zqo=r(sKr,"from_pretrained()"),sKr.forEach(t),Vqo=r($z,"class method or the "),ide=n($z,"CODE",{});var lKr=s(ide);Wqo=r(lKr,"from_config()"),lKr.forEach(t),Qqo=r($z,`class
method.`),$z.forEach(t),Hqo=i(yl),_y=n(yl,"P",{});var fke=s(_y);Uqo=r(fke,"This class cannot be instantiated directly using "),dde=n(fke,"CODE",{});var iKr=s(dde);Jqo=r(iKr,"__init__()"),iKr.forEach(t),Yqo=r(fke," (throws an error)."),fke.forEach(t),Kqo=i(yl),at=n(yl,"DIV",{class:!0});var wl=s(at);m(uy.$$.fragment,wl),Zqo=i(wl),cde=n(wl,"P",{});var dKr=s(cde);eGo=r(dKr,"Instantiates one of the model classes of the library (with a image segmentation head) from a configuration."),dKr.forEach(t),oGo=i(wl),Yd=n(wl,"P",{});var Iz=s(Yd);rGo=r(Iz,`Note:
Loading a model from its configuration file does `),fde=n(Iz,"STRONG",{});var cKr=s(fde);tGo=r(cKr,"not"),cKr.forEach(t),aGo=r(Iz,` load the model weights. It only affects the
model\u2019s configuration. Use `),mde=n(Iz,"CODE",{});var fKr=s(mde);nGo=r(fKr,"from_pretrained()"),fKr.forEach(t),sGo=r(Iz,"to load the model weights."),Iz.forEach(t),lGo=i(wl),gde=n(wl,"P",{});var mKr=s(gde);iGo=r(mKr,"Examples:"),mKr.forEach(t),dGo=i(wl),m(by.$$.fragment,wl),wl.forEach(t),cGo=i(yl),Je=n(yl,"DIV",{class:!0});var oa=s(Je);m(vy.$$.fragment,oa),fGo=i(oa),hde=n(oa,"P",{});var gKr=s(hde);mGo=r(gKr,"Instantiate one of the model classes of the library (with a image segmentation head) from a pretrained model."),gKr.forEach(t),gGo=i(oa),nn=n(oa,"P",{});var A4=s(nn);hGo=r(A4,"The model class to instantiate is selected based on the "),pde=n(A4,"CODE",{});var hKr=s(pde);pGo=r(hKr,"model_type"),hKr.forEach(t),_Go=r(A4,` property of the config object (either
passed as an argument or loaded from `),_de=n(A4,"CODE",{});var pKr=s(_de);uGo=r(pKr,"pretrained_model_name_or_path"),pKr.forEach(t),bGo=r(A4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),ude=n(A4,"CODE",{});var _Kr=s(ude);vGo=r(_Kr,"pretrained_model_name_or_path"),_Kr.forEach(t),TGo=r(A4,":"),A4.forEach(t),FGo=i(oa),bde=n(oa,"UL",{});var uKr=s(bde);uv=n(uKr,"LI",{});var O3e=s(uv);vde=n(O3e,"STRONG",{});var bKr=s(vde);CGo=r(bKr,"detr"),bKr.forEach(t),MGo=r(O3e," \u2014 "),LN=n(O3e,"A",{href:!0});var vKr=s(LN);EGo=r(vKr,"DetrForSegmentation"),vKr.forEach(t),yGo=r(O3e," (DETR model)"),O3e.forEach(t),uKr.forEach(t),wGo=i(oa),bv=n(oa,"P",{});var X3e=s(bv);AGo=r(X3e,"The model is set in evaluation mode by default using "),Tde=n(X3e,"CODE",{});var TKr=s(Tde);LGo=r(TKr,"model.eval()"),TKr.forEach(t),BGo=r(X3e,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Fde=n(X3e,"CODE",{});var FKr=s(Fde);kGo=r(FKr,"model.train()"),FKr.forEach(t),X3e.forEach(t),xGo=i(oa),Cde=n(oa,"P",{});var CKr=s(Cde);RGo=r(CKr,"Examples:"),CKr.forEach(t),SGo=i(oa),m(Ty.$$.fragment,oa),oa.forEach(t),yl.forEach(t),a8e=i(d),Kd=n(d,"H2",{class:!0});var mke=s(Kd);vv=n(mke,"A",{id:!0,class:!0,href:!0});var MKr=s(vv);Mde=n(MKr,"SPAN",{});var EKr=s(Mde);m(Fy.$$.fragment,EKr),EKr.forEach(t),MKr.forEach(t),PGo=i(mke),Ede=n(mke,"SPAN",{});var yKr=s(Ede);$Go=r(yKr,"AutoModelForSemanticSegmentation"),yKr.forEach(t),mke.forEach(t),n8e=i(d),mr=n(d,"DIV",{class:!0});var Al=s(mr);m(Cy.$$.fragment,Al),IGo=i(Al),Zd=n(Al,"P",{});var jz=s(Zd);jGo=r(jz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a semantic segmentation head) when created
with the `),yde=n(jz,"CODE",{});var wKr=s(yde);NGo=r(wKr,"from_pretrained()"),wKr.forEach(t),DGo=r(jz,"class method or the "),wde=n(jz,"CODE",{});var AKr=s(wde);qGo=r(AKr,"from_config()"),AKr.forEach(t),GGo=r(jz,`class
method.`),jz.forEach(t),OGo=i(Al),My=n(Al,"P",{});var gke=s(My);XGo=r(gke,"This class cannot be instantiated directly using "),Ade=n(gke,"CODE",{});var LKr=s(Ade);zGo=r(LKr,"__init__()"),LKr.forEach(t),VGo=r(gke," (throws an error)."),gke.forEach(t),WGo=i(Al),nt=n(Al,"DIV",{class:!0});var Ll=s(nt);m(Ey.$$.fragment,Ll),QGo=i(Ll),Lde=n(Ll,"P",{});var BKr=s(Lde);HGo=r(BKr,"Instantiates one of the model classes of the library (with a semantic segmentation head) from a configuration."),BKr.forEach(t),UGo=i(Ll),ec=n(Ll,"P",{});var Nz=s(ec);JGo=r(Nz,`Note:
Loading a model from its configuration file does `),Bde=n(Nz,"STRONG",{});var kKr=s(Bde);YGo=r(kKr,"not"),kKr.forEach(t),KGo=r(Nz,` load the model weights. It only affects the
model\u2019s configuration. Use `),kde=n(Nz,"CODE",{});var xKr=s(kde);ZGo=r(xKr,"from_pretrained()"),xKr.forEach(t),eOo=r(Nz,"to load the model weights."),Nz.forEach(t),oOo=i(Ll),xde=n(Ll,"P",{});var RKr=s(xde);rOo=r(RKr,"Examples:"),RKr.forEach(t),tOo=i(Ll),m(yy.$$.fragment,Ll),Ll.forEach(t),aOo=i(Al),Ye=n(Al,"DIV",{class:!0});var ra=s(Ye);m(wy.$$.fragment,ra),nOo=i(ra),Rde=n(ra,"P",{});var SKr=s(Rde);sOo=r(SKr,"Instantiate one of the model classes of the library (with a semantic segmentation head) from a pretrained model."),SKr.forEach(t),lOo=i(ra),sn=n(ra,"P",{});var L4=s(sn);iOo=r(L4,"The model class to instantiate is selected based on the "),Sde=n(L4,"CODE",{});var PKr=s(Sde);dOo=r(PKr,"model_type"),PKr.forEach(t),cOo=r(L4,` property of the config object (either
passed as an argument or loaded from `),Pde=n(L4,"CODE",{});var $Kr=s(Pde);fOo=r($Kr,"pretrained_model_name_or_path"),$Kr.forEach(t),mOo=r(L4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),$de=n(L4,"CODE",{});var IKr=s($de);gOo=r(IKr,"pretrained_model_name_or_path"),IKr.forEach(t),hOo=r(L4,":"),L4.forEach(t),pOo=i(ra),Ay=n(ra,"UL",{});var hke=s(Ay);Tv=n(hke,"LI",{});var z3e=s(Tv);Ide=n(z3e,"STRONG",{});var jKr=s(Ide);_Oo=r(jKr,"beit"),jKr.forEach(t),uOo=r(z3e," \u2014 "),BN=n(z3e,"A",{href:!0});var NKr=s(BN);bOo=r(NKr,"BeitForSemanticSegmentation"),NKr.forEach(t),vOo=r(z3e," (BEiT model)"),z3e.forEach(t),TOo=i(hke),Fv=n(hke,"LI",{});var V3e=s(Fv);jde=n(V3e,"STRONG",{});var DKr=s(jde);FOo=r(DKr,"segformer"),DKr.forEach(t),COo=r(V3e," \u2014 "),kN=n(V3e,"A",{href:!0});var qKr=s(kN);MOo=r(qKr,"SegformerForSemanticSegmentation"),qKr.forEach(t),EOo=r(V3e," (SegFormer model)"),V3e.forEach(t),hke.forEach(t),yOo=i(ra),Cv=n(ra,"P",{});var W3e=s(Cv);wOo=r(W3e,"The model is set in evaluation mode by default using "),Nde=n(W3e,"CODE",{});var GKr=s(Nde);AOo=r(GKr,"model.eval()"),GKr.forEach(t),LOo=r(W3e,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Dde=n(W3e,"CODE",{});var OKr=s(Dde);BOo=r(OKr,"model.train()"),OKr.forEach(t),W3e.forEach(t),kOo=i(ra),qde=n(ra,"P",{});var XKr=s(qde);xOo=r(XKr,"Examples:"),XKr.forEach(t),ROo=i(ra),m(Ly.$$.fragment,ra),ra.forEach(t),Al.forEach(t),s8e=i(d),oc=n(d,"H2",{class:!0});var pke=s(oc);Mv=n(pke,"A",{id:!0,class:!0,href:!0});var zKr=s(Mv);Gde=n(zKr,"SPAN",{});var VKr=s(Gde);m(By.$$.fragment,VKr),VKr.forEach(t),zKr.forEach(t),SOo=i(pke),Ode=n(pke,"SPAN",{});var WKr=s(Ode);POo=r(WKr,"TFAutoModel"),WKr.forEach(t),pke.forEach(t),l8e=i(d),gr=n(d,"DIV",{class:!0});var Bl=s(gr);m(ky.$$.fragment,Bl),$Oo=i(Bl),rc=n(Bl,"P",{});var Dz=s(rc);IOo=r(Dz,`This is a generic model class that will be instantiated as one of the base model classes of the library when created
with the `),Xde=n(Dz,"CODE",{});var QKr=s(Xde);jOo=r(QKr,"from_pretrained()"),QKr.forEach(t),NOo=r(Dz,"class method or the "),zde=n(Dz,"CODE",{});var HKr=s(zde);DOo=r(HKr,"from_config()"),HKr.forEach(t),qOo=r(Dz,`class
method.`),Dz.forEach(t),GOo=i(Bl),xy=n(Bl,"P",{});var _ke=s(xy);OOo=r(_ke,"This class cannot be instantiated directly using "),Vde=n(_ke,"CODE",{});var UKr=s(Vde);XOo=r(UKr,"__init__()"),UKr.forEach(t),zOo=r(_ke," (throws an error)."),_ke.forEach(t),VOo=i(Bl),st=n(Bl,"DIV",{class:!0});var kl=s(st);m(Ry.$$.fragment,kl),WOo=i(kl),Wde=n(kl,"P",{});var JKr=s(Wde);QOo=r(JKr,"Instantiates one of the base model classes of the library from a configuration."),JKr.forEach(t),HOo=i(kl),tc=n(kl,"P",{});var qz=s(tc);UOo=r(qz,`Note:
Loading a model from its configuration file does `),Qde=n(qz,"STRONG",{});var YKr=s(Qde);JOo=r(YKr,"not"),YKr.forEach(t),YOo=r(qz,` load the model weights. It only affects the
model\u2019s configuration. Use `),Hde=n(qz,"CODE",{});var KKr=s(Hde);KOo=r(KKr,"from_pretrained()"),KKr.forEach(t),ZOo=r(qz,"to load the model weights."),qz.forEach(t),eXo=i(kl),Ude=n(kl,"P",{});var ZKr=s(Ude);oXo=r(ZKr,"Examples:"),ZKr.forEach(t),rXo=i(kl),m(Sy.$$.fragment,kl),kl.forEach(t),tXo=i(Bl),go=n(Bl,"DIV",{class:!0});var ca=s(go);m(Py.$$.fragment,ca),aXo=i(ca),Jde=n(ca,"P",{});var eZr=s(Jde);nXo=r(eZr,"Instantiate one of the base model classes of the library from a pretrained model."),eZr.forEach(t),sXo=i(ca),ln=n(ca,"P",{});var B4=s(ln);lXo=r(B4,"The model class to instantiate is selected based on the "),Yde=n(B4,"CODE",{});var oZr=s(Yde);iXo=r(oZr,"model_type"),oZr.forEach(t),dXo=r(B4,` property of the config object (either
passed as an argument or loaded from `),Kde=n(B4,"CODE",{});var rZr=s(Kde);cXo=r(rZr,"pretrained_model_name_or_path"),rZr.forEach(t),fXo=r(B4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Zde=n(B4,"CODE",{});var tZr=s(Zde);mXo=r(tZr,"pretrained_model_name_or_path"),tZr.forEach(t),gXo=r(B4,":"),B4.forEach(t),hXo=i(ca),B=n(ca,"UL",{});var k=s(B);Ev=n(k,"LI",{});var Q3e=s(Ev);ece=n(Q3e,"STRONG",{});var aZr=s(ece);pXo=r(aZr,"albert"),aZr.forEach(t),_Xo=r(Q3e," \u2014 "),xN=n(Q3e,"A",{href:!0});var nZr=s(xN);uXo=r(nZr,"TFAlbertModel"),nZr.forEach(t),bXo=r(Q3e," (ALBERT model)"),Q3e.forEach(t),vXo=i(k),yv=n(k,"LI",{});var H3e=s(yv);oce=n(H3e,"STRONG",{});var sZr=s(oce);TXo=r(sZr,"bart"),sZr.forEach(t),FXo=r(H3e," \u2014 "),RN=n(H3e,"A",{href:!0});var lZr=s(RN);CXo=r(lZr,"TFBartModel"),lZr.forEach(t),MXo=r(H3e," (BART model)"),H3e.forEach(t),EXo=i(k),wv=n(k,"LI",{});var U3e=s(wv);rce=n(U3e,"STRONG",{});var iZr=s(rce);yXo=r(iZr,"bert"),iZr.forEach(t),wXo=r(U3e," \u2014 "),SN=n(U3e,"A",{href:!0});var dZr=s(SN);AXo=r(dZr,"TFBertModel"),dZr.forEach(t),LXo=r(U3e," (BERT model)"),U3e.forEach(t),BXo=i(k),Av=n(k,"LI",{});var J3e=s(Av);tce=n(J3e,"STRONG",{});var cZr=s(tce);kXo=r(cZr,"blenderbot"),cZr.forEach(t),xXo=r(J3e," \u2014 "),PN=n(J3e,"A",{href:!0});var fZr=s(PN);RXo=r(fZr,"TFBlenderbotModel"),fZr.forEach(t),SXo=r(J3e," (Blenderbot model)"),J3e.forEach(t),PXo=i(k),Lv=n(k,"LI",{});var Y3e=s(Lv);ace=n(Y3e,"STRONG",{});var mZr=s(ace);$Xo=r(mZr,"blenderbot-small"),mZr.forEach(t),IXo=r(Y3e," \u2014 "),$N=n(Y3e,"A",{href:!0});var gZr=s($N);jXo=r(gZr,"TFBlenderbotSmallModel"),gZr.forEach(t),NXo=r(Y3e," (BlenderbotSmall model)"),Y3e.forEach(t),DXo=i(k),Bv=n(k,"LI",{});var K3e=s(Bv);nce=n(K3e,"STRONG",{});var hZr=s(nce);qXo=r(hZr,"camembert"),hZr.forEach(t),GXo=r(K3e," \u2014 "),IN=n(K3e,"A",{href:!0});var pZr=s(IN);OXo=r(pZr,"TFCamembertModel"),pZr.forEach(t),XXo=r(K3e," (CamemBERT model)"),K3e.forEach(t),zXo=i(k),kv=n(k,"LI",{});var Z3e=s(kv);sce=n(Z3e,"STRONG",{});var _Zr=s(sce);VXo=r(_Zr,"clip"),_Zr.forEach(t),WXo=r(Z3e," \u2014 "),jN=n(Z3e,"A",{href:!0});var uZr=s(jN);QXo=r(uZr,"TFCLIPModel"),uZr.forEach(t),HXo=r(Z3e," (CLIP model)"),Z3e.forEach(t),UXo=i(k),xv=n(k,"LI",{});var eye=s(xv);lce=n(eye,"STRONG",{});var bZr=s(lce);JXo=r(bZr,"convbert"),bZr.forEach(t),YXo=r(eye," \u2014 "),NN=n(eye,"A",{href:!0});var vZr=s(NN);KXo=r(vZr,"TFConvBertModel"),vZr.forEach(t),ZXo=r(eye," (ConvBERT model)"),eye.forEach(t),ezo=i(k),Rv=n(k,"LI",{});var oye=s(Rv);ice=n(oye,"STRONG",{});var TZr=s(ice);ozo=r(TZr,"ctrl"),TZr.forEach(t),rzo=r(oye," \u2014 "),DN=n(oye,"A",{href:!0});var FZr=s(DN);tzo=r(FZr,"TFCTRLModel"),FZr.forEach(t),azo=r(oye," (CTRL model)"),oye.forEach(t),nzo=i(k),Sv=n(k,"LI",{});var rye=s(Sv);dce=n(rye,"STRONG",{});var CZr=s(dce);szo=r(CZr,"deberta"),CZr.forEach(t),lzo=r(rye," \u2014 "),qN=n(rye,"A",{href:!0});var MZr=s(qN);izo=r(MZr,"TFDebertaModel"),MZr.forEach(t),dzo=r(rye," (DeBERTa model)"),rye.forEach(t),czo=i(k),Pv=n(k,"LI",{});var tye=s(Pv);cce=n(tye,"STRONG",{});var EZr=s(cce);fzo=r(EZr,"deberta-v2"),EZr.forEach(t),mzo=r(tye," \u2014 "),GN=n(tye,"A",{href:!0});var yZr=s(GN);gzo=r(yZr,"TFDebertaV2Model"),yZr.forEach(t),hzo=r(tye," (DeBERTa-v2 model)"),tye.forEach(t),pzo=i(k),$v=n(k,"LI",{});var aye=s($v);fce=n(aye,"STRONG",{});var wZr=s(fce);_zo=r(wZr,"distilbert"),wZr.forEach(t),uzo=r(aye," \u2014 "),ON=n(aye,"A",{href:!0});var AZr=s(ON);bzo=r(AZr,"TFDistilBertModel"),AZr.forEach(t),vzo=r(aye," (DistilBERT model)"),aye.forEach(t),Tzo=i(k),Iv=n(k,"LI",{});var nye=s(Iv);mce=n(nye,"STRONG",{});var LZr=s(mce);Fzo=r(LZr,"dpr"),LZr.forEach(t),Czo=r(nye," \u2014 "),XN=n(nye,"A",{href:!0});var BZr=s(XN);Mzo=r(BZr,"TFDPRQuestionEncoder"),BZr.forEach(t),Ezo=r(nye," (DPR model)"),nye.forEach(t),yzo=i(k),jv=n(k,"LI",{});var sye=s(jv);gce=n(sye,"STRONG",{});var kZr=s(gce);wzo=r(kZr,"electra"),kZr.forEach(t),Azo=r(sye," \u2014 "),zN=n(sye,"A",{href:!0});var xZr=s(zN);Lzo=r(xZr,"TFElectraModel"),xZr.forEach(t),Bzo=r(sye," (ELECTRA model)"),sye.forEach(t),kzo=i(k),Nv=n(k,"LI",{});var lye=s(Nv);hce=n(lye,"STRONG",{});var RZr=s(hce);xzo=r(RZr,"flaubert"),RZr.forEach(t),Rzo=r(lye," \u2014 "),VN=n(lye,"A",{href:!0});var SZr=s(VN);Szo=r(SZr,"TFFlaubertModel"),SZr.forEach(t),Pzo=r(lye," (FlauBERT model)"),lye.forEach(t),$zo=i(k),Ss=n(k,"LI",{});var N0=s(Ss);pce=n(N0,"STRONG",{});var PZr=s(pce);Izo=r(PZr,"funnel"),PZr.forEach(t),jzo=r(N0," \u2014 "),WN=n(N0,"A",{href:!0});var $Zr=s(WN);Nzo=r($Zr,"TFFunnelModel"),$Zr.forEach(t),Dzo=r(N0," or "),QN=n(N0,"A",{href:!0});var IZr=s(QN);qzo=r(IZr,"TFFunnelBaseModel"),IZr.forEach(t),Gzo=r(N0," (Funnel Transformer model)"),N0.forEach(t),Ozo=i(k),Dv=n(k,"LI",{});var iye=s(Dv);_ce=n(iye,"STRONG",{});var jZr=s(_ce);Xzo=r(jZr,"gpt2"),jZr.forEach(t),zzo=r(iye," \u2014 "),HN=n(iye,"A",{href:!0});var NZr=s(HN);Vzo=r(NZr,"TFGPT2Model"),NZr.forEach(t),Wzo=r(iye," (OpenAI GPT-2 model)"),iye.forEach(t),Qzo=i(k),qv=n(k,"LI",{});var dye=s(qv);uce=n(dye,"STRONG",{});var DZr=s(uce);Hzo=r(DZr,"hubert"),DZr.forEach(t),Uzo=r(dye," \u2014 "),UN=n(dye,"A",{href:!0});var qZr=s(UN);Jzo=r(qZr,"TFHubertModel"),qZr.forEach(t),Yzo=r(dye," (Hubert model)"),dye.forEach(t),Kzo=i(k),Gv=n(k,"LI",{});var cye=s(Gv);bce=n(cye,"STRONG",{});var GZr=s(bce);Zzo=r(GZr,"layoutlm"),GZr.forEach(t),eVo=r(cye," \u2014 "),JN=n(cye,"A",{href:!0});var OZr=s(JN);oVo=r(OZr,"TFLayoutLMModel"),OZr.forEach(t),rVo=r(cye," (LayoutLM model)"),cye.forEach(t),tVo=i(k),Ov=n(k,"LI",{});var fye=s(Ov);vce=n(fye,"STRONG",{});var XZr=s(vce);aVo=r(XZr,"led"),XZr.forEach(t),nVo=r(fye," \u2014 "),YN=n(fye,"A",{href:!0});var zZr=s(YN);sVo=r(zZr,"TFLEDModel"),zZr.forEach(t),lVo=r(fye," (LED model)"),fye.forEach(t),iVo=i(k),Xv=n(k,"LI",{});var mye=s(Xv);Tce=n(mye,"STRONG",{});var VZr=s(Tce);dVo=r(VZr,"longformer"),VZr.forEach(t),cVo=r(mye," \u2014 "),KN=n(mye,"A",{href:!0});var WZr=s(KN);fVo=r(WZr,"TFLongformerModel"),WZr.forEach(t),mVo=r(mye," (Longformer model)"),mye.forEach(t),gVo=i(k),zv=n(k,"LI",{});var gye=s(zv);Fce=n(gye,"STRONG",{});var QZr=s(Fce);hVo=r(QZr,"lxmert"),QZr.forEach(t),pVo=r(gye," \u2014 "),ZN=n(gye,"A",{href:!0});var HZr=s(ZN);_Vo=r(HZr,"TFLxmertModel"),HZr.forEach(t),uVo=r(gye," (LXMERT model)"),gye.forEach(t),bVo=i(k),Vv=n(k,"LI",{});var hye=s(Vv);Cce=n(hye,"STRONG",{});var UZr=s(Cce);vVo=r(UZr,"marian"),UZr.forEach(t),TVo=r(hye," \u2014 "),eD=n(hye,"A",{href:!0});var JZr=s(eD);FVo=r(JZr,"TFMarianModel"),JZr.forEach(t),CVo=r(hye," (Marian model)"),hye.forEach(t),MVo=i(k),Wv=n(k,"LI",{});var pye=s(Wv);Mce=n(pye,"STRONG",{});var YZr=s(Mce);EVo=r(YZr,"mbart"),YZr.forEach(t),yVo=r(pye," \u2014 "),oD=n(pye,"A",{href:!0});var KZr=s(oD);wVo=r(KZr,"TFMBartModel"),KZr.forEach(t),AVo=r(pye," (mBART model)"),pye.forEach(t),LVo=i(k),Qv=n(k,"LI",{});var _ye=s(Qv);Ece=n(_ye,"STRONG",{});var ZZr=s(Ece);BVo=r(ZZr,"mobilebert"),ZZr.forEach(t),kVo=r(_ye," \u2014 "),rD=n(_ye,"A",{href:!0});var eet=s(rD);xVo=r(eet,"TFMobileBertModel"),eet.forEach(t),RVo=r(_ye," (MobileBERT model)"),_ye.forEach(t),SVo=i(k),Hv=n(k,"LI",{});var uye=s(Hv);yce=n(uye,"STRONG",{});var oet=s(yce);PVo=r(oet,"mpnet"),oet.forEach(t),$Vo=r(uye," \u2014 "),tD=n(uye,"A",{href:!0});var ret=s(tD);IVo=r(ret,"TFMPNetModel"),ret.forEach(t),jVo=r(uye," (MPNet model)"),uye.forEach(t),NVo=i(k),Uv=n(k,"LI",{});var bye=s(Uv);wce=n(bye,"STRONG",{});var tet=s(wce);DVo=r(tet,"mt5"),tet.forEach(t),qVo=r(bye," \u2014 "),aD=n(bye,"A",{href:!0});var aet=s(aD);GVo=r(aet,"TFMT5Model"),aet.forEach(t),OVo=r(bye," (mT5 model)"),bye.forEach(t),XVo=i(k),Jv=n(k,"LI",{});var vye=s(Jv);Ace=n(vye,"STRONG",{});var net=s(Ace);zVo=r(net,"openai-gpt"),net.forEach(t),VVo=r(vye," \u2014 "),nD=n(vye,"A",{href:!0});var set=s(nD);WVo=r(set,"TFOpenAIGPTModel"),set.forEach(t),QVo=r(vye," (OpenAI GPT model)"),vye.forEach(t),HVo=i(k),Yv=n(k,"LI",{});var Tye=s(Yv);Lce=n(Tye,"STRONG",{});var iet=s(Lce);UVo=r(iet,"pegasus"),iet.forEach(t),JVo=r(Tye," \u2014 "),sD=n(Tye,"A",{href:!0});var det=s(sD);YVo=r(det,"TFPegasusModel"),det.forEach(t),KVo=r(Tye," (Pegasus model)"),Tye.forEach(t),ZVo=i(k),Kv=n(k,"LI",{});var Fye=s(Kv);Bce=n(Fye,"STRONG",{});var cet=s(Bce);eWo=r(cet,"rembert"),cet.forEach(t),oWo=r(Fye," \u2014 "),lD=n(Fye,"A",{href:!0});var fet=s(lD);rWo=r(fet,"TFRemBertModel"),fet.forEach(t),tWo=r(Fye," (RemBERT model)"),Fye.forEach(t),aWo=i(k),Zv=n(k,"LI",{});var Cye=s(Zv);kce=n(Cye,"STRONG",{});var met=s(kce);nWo=r(met,"roberta"),met.forEach(t),sWo=r(Cye," \u2014 "),iD=n(Cye,"A",{href:!0});var get=s(iD);lWo=r(get,"TFRobertaModel"),get.forEach(t),iWo=r(Cye," (RoBERTa model)"),Cye.forEach(t),dWo=i(k),eT=n(k,"LI",{});var Mye=s(eT);xce=n(Mye,"STRONG",{});var het=s(xce);cWo=r(het,"roformer"),het.forEach(t),fWo=r(Mye," \u2014 "),dD=n(Mye,"A",{href:!0});var pet=s(dD);mWo=r(pet,"TFRoFormerModel"),pet.forEach(t),gWo=r(Mye," (RoFormer model)"),Mye.forEach(t),hWo=i(k),oT=n(k,"LI",{});var Eye=s(oT);Rce=n(Eye,"STRONG",{});var _et=s(Rce);pWo=r(_et,"speech_to_text"),_et.forEach(t),_Wo=r(Eye," \u2014 "),cD=n(Eye,"A",{href:!0});var uet=s(cD);uWo=r(uet,"TFSpeech2TextModel"),uet.forEach(t),bWo=r(Eye," (Speech2Text model)"),Eye.forEach(t),vWo=i(k),rT=n(k,"LI",{});var yye=s(rT);Sce=n(yye,"STRONG",{});var bet=s(Sce);TWo=r(bet,"t5"),bet.forEach(t),FWo=r(yye," \u2014 "),fD=n(yye,"A",{href:!0});var vet=s(fD);CWo=r(vet,"TFT5Model"),vet.forEach(t),MWo=r(yye," (T5 model)"),yye.forEach(t),EWo=i(k),tT=n(k,"LI",{});var wye=s(tT);Pce=n(wye,"STRONG",{});var Tet=s(Pce);yWo=r(Tet,"tapas"),Tet.forEach(t),wWo=r(wye," \u2014 "),mD=n(wye,"A",{href:!0});var Fet=s(mD);AWo=r(Fet,"TFTapasModel"),Fet.forEach(t),LWo=r(wye," (TAPAS model)"),wye.forEach(t),BWo=i(k),aT=n(k,"LI",{});var Aye=s(aT);$ce=n(Aye,"STRONG",{});var Cet=s($ce);kWo=r(Cet,"transfo-xl"),Cet.forEach(t),xWo=r(Aye," \u2014 "),gD=n(Aye,"A",{href:!0});var Met=s(gD);RWo=r(Met,"TFTransfoXLModel"),Met.forEach(t),SWo=r(Aye," (Transformer-XL model)"),Aye.forEach(t),PWo=i(k),nT=n(k,"LI",{});var Lye=s(nT);Ice=n(Lye,"STRONG",{});var Eet=s(Ice);$Wo=r(Eet,"vit"),Eet.forEach(t),IWo=r(Lye," \u2014 "),hD=n(Lye,"A",{href:!0});var yet=s(hD);jWo=r(yet,"TFViTModel"),yet.forEach(t),NWo=r(Lye," (ViT model)"),Lye.forEach(t),DWo=i(k),sT=n(k,"LI",{});var Bye=s(sT);jce=n(Bye,"STRONG",{});var wet=s(jce);qWo=r(wet,"wav2vec2"),wet.forEach(t),GWo=r(Bye," \u2014 "),pD=n(Bye,"A",{href:!0});var Aet=s(pD);OWo=r(Aet,"TFWav2Vec2Model"),Aet.forEach(t),XWo=r(Bye," (Wav2Vec2 model)"),Bye.forEach(t),zWo=i(k),lT=n(k,"LI",{});var kye=s(lT);Nce=n(kye,"STRONG",{});var Let=s(Nce);VWo=r(Let,"xlm"),Let.forEach(t),WWo=r(kye," \u2014 "),_D=n(kye,"A",{href:!0});var Bet=s(_D);QWo=r(Bet,"TFXLMModel"),Bet.forEach(t),HWo=r(kye," (XLM model)"),kye.forEach(t),UWo=i(k),iT=n(k,"LI",{});var xye=s(iT);Dce=n(xye,"STRONG",{});var ket=s(Dce);JWo=r(ket,"xlm-roberta"),ket.forEach(t),YWo=r(xye," \u2014 "),uD=n(xye,"A",{href:!0});var xet=s(uD);KWo=r(xet,"TFXLMRobertaModel"),xet.forEach(t),ZWo=r(xye," (XLM-RoBERTa model)"),xye.forEach(t),eQo=i(k),dT=n(k,"LI",{});var Rye=s(dT);qce=n(Rye,"STRONG",{});var Ret=s(qce);oQo=r(Ret,"xlnet"),Ret.forEach(t),rQo=r(Rye," \u2014 "),bD=n(Rye,"A",{href:!0});var Set=s(bD);tQo=r(Set,"TFXLNetModel"),Set.forEach(t),aQo=r(Rye," (XLNet model)"),Rye.forEach(t),k.forEach(t),nQo=i(ca),Gce=n(ca,"P",{});var Pet=s(Gce);sQo=r(Pet,"Examples:"),Pet.forEach(t),lQo=i(ca),m($y.$$.fragment,ca),ca.forEach(t),Bl.forEach(t),i8e=i(d),ac=n(d,"H2",{class:!0});var uke=s(ac);cT=n(uke,"A",{id:!0,class:!0,href:!0});var $et=s(cT);Oce=n($et,"SPAN",{});var Iet=s(Oce);m(Iy.$$.fragment,Iet),Iet.forEach(t),$et.forEach(t),iQo=i(uke),Xce=n(uke,"SPAN",{});var jet=s(Xce);dQo=r(jet,"TFAutoModelForPreTraining"),jet.forEach(t),uke.forEach(t),d8e=i(d),hr=n(d,"DIV",{class:!0});var xl=s(hr);m(jy.$$.fragment,xl),cQo=i(xl),nc=n(xl,"P",{});var Gz=s(nc);fQo=r(Gz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a pretraining head) when created
with the `),zce=n(Gz,"CODE",{});var Net=s(zce);mQo=r(Net,"from_pretrained()"),Net.forEach(t),gQo=r(Gz,"class method or the "),Vce=n(Gz,"CODE",{});var Det=s(Vce);hQo=r(Det,"from_config()"),Det.forEach(t),pQo=r(Gz,`class
method.`),Gz.forEach(t),_Qo=i(xl),Ny=n(xl,"P",{});var bke=s(Ny);uQo=r(bke,"This class cannot be instantiated directly using "),Wce=n(bke,"CODE",{});var qet=s(Wce);bQo=r(qet,"__init__()"),qet.forEach(t),vQo=r(bke," (throws an error)."),bke.forEach(t),TQo=i(xl),lt=n(xl,"DIV",{class:!0});var Rl=s(lt);m(Dy.$$.fragment,Rl),FQo=i(Rl),Qce=n(Rl,"P",{});var Get=s(Qce);CQo=r(Get,"Instantiates one of the model classes of the library (with a pretraining head) from a configuration."),Get.forEach(t),MQo=i(Rl),sc=n(Rl,"P",{});var Oz=s(sc);EQo=r(Oz,`Note:
Loading a model from its configuration file does `),Hce=n(Oz,"STRONG",{});var Oet=s(Hce);yQo=r(Oet,"not"),Oet.forEach(t),wQo=r(Oz,` load the model weights. It only affects the
model\u2019s configuration. Use `),Uce=n(Oz,"CODE",{});var Xet=s(Uce);AQo=r(Xet,"from_pretrained()"),Xet.forEach(t),LQo=r(Oz,"to load the model weights."),Oz.forEach(t),BQo=i(Rl),Jce=n(Rl,"P",{});var zet=s(Jce);kQo=r(zet,"Examples:"),zet.forEach(t),xQo=i(Rl),m(qy.$$.fragment,Rl),Rl.forEach(t),RQo=i(xl),ho=n(xl,"DIV",{class:!0});var fa=s(ho);m(Gy.$$.fragment,fa),SQo=i(fa),Yce=n(fa,"P",{});var Vet=s(Yce);PQo=r(Vet,"Instantiate one of the model classes of the library (with a pretraining head) from a pretrained model."),Vet.forEach(t),$Qo=i(fa),dn=n(fa,"P",{});var k4=s(dn);IQo=r(k4,"The model class to instantiate is selected based on the "),Kce=n(k4,"CODE",{});var Wet=s(Kce);jQo=r(Wet,"model_type"),Wet.forEach(t),NQo=r(k4,` property of the config object (either
passed as an argument or loaded from `),Zce=n(k4,"CODE",{});var Qet=s(Zce);DQo=r(Qet,"pretrained_model_name_or_path"),Qet.forEach(t),qQo=r(k4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),efe=n(k4,"CODE",{});var Het=s(efe);GQo=r(Het,"pretrained_model_name_or_path"),Het.forEach(t),OQo=r(k4,":"),k4.forEach(t),XQo=i(fa),H=n(fa,"UL",{});var U=s(H);fT=n(U,"LI",{});var Sye=s(fT);ofe=n(Sye,"STRONG",{});var Uet=s(ofe);zQo=r(Uet,"albert"),Uet.forEach(t),VQo=r(Sye," \u2014 "),vD=n(Sye,"A",{href:!0});var Jet=s(vD);WQo=r(Jet,"TFAlbertForPreTraining"),Jet.forEach(t),QQo=r(Sye," (ALBERT model)"),Sye.forEach(t),HQo=i(U),mT=n(U,"LI",{});var Pye=s(mT);rfe=n(Pye,"STRONG",{});var Yet=s(rfe);UQo=r(Yet,"bart"),Yet.forEach(t),JQo=r(Pye," \u2014 "),TD=n(Pye,"A",{href:!0});var Ket=s(TD);YQo=r(Ket,"TFBartForConditionalGeneration"),Ket.forEach(t),KQo=r(Pye," (BART model)"),Pye.forEach(t),ZQo=i(U),gT=n(U,"LI",{});var $ye=s(gT);tfe=n($ye,"STRONG",{});var Zet=s(tfe);eHo=r(Zet,"bert"),Zet.forEach(t),oHo=r($ye," \u2014 "),FD=n($ye,"A",{href:!0});var eot=s(FD);rHo=r(eot,"TFBertForPreTraining"),eot.forEach(t),tHo=r($ye," (BERT model)"),$ye.forEach(t),aHo=i(U),hT=n(U,"LI",{});var Iye=s(hT);afe=n(Iye,"STRONG",{});var oot=s(afe);nHo=r(oot,"camembert"),oot.forEach(t),sHo=r(Iye," \u2014 "),CD=n(Iye,"A",{href:!0});var rot=s(CD);lHo=r(rot,"TFCamembertForMaskedLM"),rot.forEach(t),iHo=r(Iye," (CamemBERT model)"),Iye.forEach(t),dHo=i(U),pT=n(U,"LI",{});var jye=s(pT);nfe=n(jye,"STRONG",{});var tot=s(nfe);cHo=r(tot,"ctrl"),tot.forEach(t),fHo=r(jye," \u2014 "),MD=n(jye,"A",{href:!0});var aot=s(MD);mHo=r(aot,"TFCTRLLMHeadModel"),aot.forEach(t),gHo=r(jye," (CTRL model)"),jye.forEach(t),hHo=i(U),_T=n(U,"LI",{});var Nye=s(_T);sfe=n(Nye,"STRONG",{});var not=s(sfe);pHo=r(not,"distilbert"),not.forEach(t),_Ho=r(Nye," \u2014 "),ED=n(Nye,"A",{href:!0});var sot=s(ED);uHo=r(sot,"TFDistilBertForMaskedLM"),sot.forEach(t),bHo=r(Nye," (DistilBERT model)"),Nye.forEach(t),vHo=i(U),uT=n(U,"LI",{});var Dye=s(uT);lfe=n(Dye,"STRONG",{});var lot=s(lfe);THo=r(lot,"electra"),lot.forEach(t),FHo=r(Dye," \u2014 "),yD=n(Dye,"A",{href:!0});var iot=s(yD);CHo=r(iot,"TFElectraForPreTraining"),iot.forEach(t),MHo=r(Dye," (ELECTRA model)"),Dye.forEach(t),EHo=i(U),bT=n(U,"LI",{});var qye=s(bT);ife=n(qye,"STRONG",{});var dot=s(ife);yHo=r(dot,"flaubert"),dot.forEach(t),wHo=r(qye," \u2014 "),wD=n(qye,"A",{href:!0});var cot=s(wD);AHo=r(cot,"TFFlaubertWithLMHeadModel"),cot.forEach(t),LHo=r(qye," (FlauBERT model)"),qye.forEach(t),BHo=i(U),vT=n(U,"LI",{});var Gye=s(vT);dfe=n(Gye,"STRONG",{});var fot=s(dfe);kHo=r(fot,"funnel"),fot.forEach(t),xHo=r(Gye," \u2014 "),AD=n(Gye,"A",{href:!0});var mot=s(AD);RHo=r(mot,"TFFunnelForPreTraining"),mot.forEach(t),SHo=r(Gye," (Funnel Transformer model)"),Gye.forEach(t),PHo=i(U),TT=n(U,"LI",{});var Oye=s(TT);cfe=n(Oye,"STRONG",{});var got=s(cfe);$Ho=r(got,"gpt2"),got.forEach(t),IHo=r(Oye," \u2014 "),LD=n(Oye,"A",{href:!0});var hot=s(LD);jHo=r(hot,"TFGPT2LMHeadModel"),hot.forEach(t),NHo=r(Oye," (OpenAI GPT-2 model)"),Oye.forEach(t),DHo=i(U),FT=n(U,"LI",{});var Xye=s(FT);ffe=n(Xye,"STRONG",{});var pot=s(ffe);qHo=r(pot,"layoutlm"),pot.forEach(t),GHo=r(Xye," \u2014 "),BD=n(Xye,"A",{href:!0});var _ot=s(BD);OHo=r(_ot,"TFLayoutLMForMaskedLM"),_ot.forEach(t),XHo=r(Xye," (LayoutLM model)"),Xye.forEach(t),zHo=i(U),CT=n(U,"LI",{});var zye=s(CT);mfe=n(zye,"STRONG",{});var uot=s(mfe);VHo=r(uot,"lxmert"),uot.forEach(t),WHo=r(zye," \u2014 "),kD=n(zye,"A",{href:!0});var bot=s(kD);QHo=r(bot,"TFLxmertForPreTraining"),bot.forEach(t),HHo=r(zye," (LXMERT model)"),zye.forEach(t),UHo=i(U),MT=n(U,"LI",{});var Vye=s(MT);gfe=n(Vye,"STRONG",{});var vot=s(gfe);JHo=r(vot,"mobilebert"),vot.forEach(t),YHo=r(Vye," \u2014 "),xD=n(Vye,"A",{href:!0});var Tot=s(xD);KHo=r(Tot,"TFMobileBertForPreTraining"),Tot.forEach(t),ZHo=r(Vye," (MobileBERT model)"),Vye.forEach(t),eUo=i(U),ET=n(U,"LI",{});var Wye=s(ET);hfe=n(Wye,"STRONG",{});var Fot=s(hfe);oUo=r(Fot,"mpnet"),Fot.forEach(t),rUo=r(Wye," \u2014 "),RD=n(Wye,"A",{href:!0});var Cot=s(RD);tUo=r(Cot,"TFMPNetForMaskedLM"),Cot.forEach(t),aUo=r(Wye," (MPNet model)"),Wye.forEach(t),nUo=i(U),yT=n(U,"LI",{});var Qye=s(yT);pfe=n(Qye,"STRONG",{});var Mot=s(pfe);sUo=r(Mot,"openai-gpt"),Mot.forEach(t),lUo=r(Qye," \u2014 "),SD=n(Qye,"A",{href:!0});var Eot=s(SD);iUo=r(Eot,"TFOpenAIGPTLMHeadModel"),Eot.forEach(t),dUo=r(Qye," (OpenAI GPT model)"),Qye.forEach(t),cUo=i(U),wT=n(U,"LI",{});var Hye=s(wT);_fe=n(Hye,"STRONG",{});var yot=s(_fe);fUo=r(yot,"roberta"),yot.forEach(t),mUo=r(Hye," \u2014 "),PD=n(Hye,"A",{href:!0});var wot=s(PD);gUo=r(wot,"TFRobertaForMaskedLM"),wot.forEach(t),hUo=r(Hye," (RoBERTa model)"),Hye.forEach(t),pUo=i(U),AT=n(U,"LI",{});var Uye=s(AT);ufe=n(Uye,"STRONG",{});var Aot=s(ufe);_Uo=r(Aot,"t5"),Aot.forEach(t),uUo=r(Uye," \u2014 "),$D=n(Uye,"A",{href:!0});var Lot=s($D);bUo=r(Lot,"TFT5ForConditionalGeneration"),Lot.forEach(t),vUo=r(Uye," (T5 model)"),Uye.forEach(t),TUo=i(U),LT=n(U,"LI",{});var Jye=s(LT);bfe=n(Jye,"STRONG",{});var Bot=s(bfe);FUo=r(Bot,"tapas"),Bot.forEach(t),CUo=r(Jye," \u2014 "),ID=n(Jye,"A",{href:!0});var kot=s(ID);MUo=r(kot,"TFTapasForMaskedLM"),kot.forEach(t),EUo=r(Jye," (TAPAS model)"),Jye.forEach(t),yUo=i(U),BT=n(U,"LI",{});var Yye=s(BT);vfe=n(Yye,"STRONG",{});var xot=s(vfe);wUo=r(xot,"transfo-xl"),xot.forEach(t),AUo=r(Yye," \u2014 "),jD=n(Yye,"A",{href:!0});var Rot=s(jD);LUo=r(Rot,"TFTransfoXLLMHeadModel"),Rot.forEach(t),BUo=r(Yye," (Transformer-XL model)"),Yye.forEach(t),kUo=i(U),kT=n(U,"LI",{});var Kye=s(kT);Tfe=n(Kye,"STRONG",{});var Sot=s(Tfe);xUo=r(Sot,"xlm"),Sot.forEach(t),RUo=r(Kye," \u2014 "),ND=n(Kye,"A",{href:!0});var Pot=s(ND);SUo=r(Pot,"TFXLMWithLMHeadModel"),Pot.forEach(t),PUo=r(Kye," (XLM model)"),Kye.forEach(t),$Uo=i(U),xT=n(U,"LI",{});var Zye=s(xT);Ffe=n(Zye,"STRONG",{});var $ot=s(Ffe);IUo=r($ot,"xlm-roberta"),$ot.forEach(t),jUo=r(Zye," \u2014 "),DD=n(Zye,"A",{href:!0});var Iot=s(DD);NUo=r(Iot,"TFXLMRobertaForMaskedLM"),Iot.forEach(t),DUo=r(Zye," (XLM-RoBERTa model)"),Zye.forEach(t),qUo=i(U),RT=n(U,"LI",{});var ewe=s(RT);Cfe=n(ewe,"STRONG",{});var jot=s(Cfe);GUo=r(jot,"xlnet"),jot.forEach(t),OUo=r(ewe," \u2014 "),qD=n(ewe,"A",{href:!0});var Not=s(qD);XUo=r(Not,"TFXLNetLMHeadModel"),Not.forEach(t),zUo=r(ewe," (XLNet model)"),ewe.forEach(t),U.forEach(t),VUo=i(fa),Mfe=n(fa,"P",{});var Dot=s(Mfe);WUo=r(Dot,"Examples:"),Dot.forEach(t),QUo=i(fa),m(Oy.$$.fragment,fa),fa.forEach(t),xl.forEach(t),c8e=i(d),lc=n(d,"H2",{class:!0});var vke=s(lc);ST=n(vke,"A",{id:!0,class:!0,href:!0});var qot=s(ST);Efe=n(qot,"SPAN",{});var Got=s(Efe);m(Xy.$$.fragment,Got),Got.forEach(t),qot.forEach(t),HUo=i(vke),yfe=n(vke,"SPAN",{});var Oot=s(yfe);UUo=r(Oot,"TFAutoModelForCausalLM"),Oot.forEach(t),vke.forEach(t),f8e=i(d),pr=n(d,"DIV",{class:!0});var Sl=s(pr);m(zy.$$.fragment,Sl),JUo=i(Sl),ic=n(Sl,"P",{});var Xz=s(ic);YUo=r(Xz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a causal language modeling head) when created
with the `),wfe=n(Xz,"CODE",{});var Xot=s(wfe);KUo=r(Xot,"from_pretrained()"),Xot.forEach(t),ZUo=r(Xz,"class method or the "),Afe=n(Xz,"CODE",{});var zot=s(Afe);eJo=r(zot,"from_config()"),zot.forEach(t),oJo=r(Xz,`class
method.`),Xz.forEach(t),rJo=i(Sl),Vy=n(Sl,"P",{});var Tke=s(Vy);tJo=r(Tke,"This class cannot be instantiated directly using "),Lfe=n(Tke,"CODE",{});var Vot=s(Lfe);aJo=r(Vot,"__init__()"),Vot.forEach(t),nJo=r(Tke," (throws an error)."),Tke.forEach(t),sJo=i(Sl),it=n(Sl,"DIV",{class:!0});var Pl=s(it);m(Wy.$$.fragment,Pl),lJo=i(Pl),Bfe=n(Pl,"P",{});var Wot=s(Bfe);iJo=r(Wot,"Instantiates one of the model classes of the library (with a causal language modeling head) from a configuration."),Wot.forEach(t),dJo=i(Pl),dc=n(Pl,"P",{});var zz=s(dc);cJo=r(zz,`Note:
Loading a model from its configuration file does `),kfe=n(zz,"STRONG",{});var Qot=s(kfe);fJo=r(Qot,"not"),Qot.forEach(t),mJo=r(zz,` load the model weights. It only affects the
model\u2019s configuration. Use `),xfe=n(zz,"CODE",{});var Hot=s(xfe);gJo=r(Hot,"from_pretrained()"),Hot.forEach(t),hJo=r(zz,"to load the model weights."),zz.forEach(t),pJo=i(Pl),Rfe=n(Pl,"P",{});var Uot=s(Rfe);_Jo=r(Uot,"Examples:"),Uot.forEach(t),uJo=i(Pl),m(Qy.$$.fragment,Pl),Pl.forEach(t),bJo=i(Sl),po=n(Sl,"DIV",{class:!0});var ma=s(po);m(Hy.$$.fragment,ma),vJo=i(ma),Sfe=n(ma,"P",{});var Jot=s(Sfe);TJo=r(Jot,"Instantiate one of the model classes of the library (with a causal language modeling head) from a pretrained model."),Jot.forEach(t),FJo=i(ma),cn=n(ma,"P",{});var x4=s(cn);CJo=r(x4,"The model class to instantiate is selected based on the "),Pfe=n(x4,"CODE",{});var Yot=s(Pfe);MJo=r(Yot,"model_type"),Yot.forEach(t),EJo=r(x4,` property of the config object (either
passed as an argument or loaded from `),$fe=n(x4,"CODE",{});var Kot=s($fe);yJo=r(Kot,"pretrained_model_name_or_path"),Kot.forEach(t),wJo=r(x4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Ife=n(x4,"CODE",{});var Zot=s(Ife);AJo=r(Zot,"pretrained_model_name_or_path"),Zot.forEach(t),LJo=r(x4,":"),x4.forEach(t),BJo=i(ma),he=n(ma,"UL",{});var Me=s(he);PT=n(Me,"LI",{});var owe=s(PT);jfe=n(owe,"STRONG",{});var ert=s(jfe);kJo=r(ert,"bert"),ert.forEach(t),xJo=r(owe," \u2014 "),GD=n(owe,"A",{href:!0});var ort=s(GD);RJo=r(ort,"TFBertLMHeadModel"),ort.forEach(t),SJo=r(owe," (BERT model)"),owe.forEach(t),PJo=i(Me),$T=n(Me,"LI",{});var rwe=s($T);Nfe=n(rwe,"STRONG",{});var rrt=s(Nfe);$Jo=r(rrt,"ctrl"),rrt.forEach(t),IJo=r(rwe," \u2014 "),OD=n(rwe,"A",{href:!0});var trt=s(OD);jJo=r(trt,"TFCTRLLMHeadModel"),trt.forEach(t),NJo=r(rwe," (CTRL model)"),rwe.forEach(t),DJo=i(Me),IT=n(Me,"LI",{});var twe=s(IT);Dfe=n(twe,"STRONG",{});var art=s(Dfe);qJo=r(art,"gpt2"),art.forEach(t),GJo=r(twe," \u2014 "),XD=n(twe,"A",{href:!0});var nrt=s(XD);OJo=r(nrt,"TFGPT2LMHeadModel"),nrt.forEach(t),XJo=r(twe," (OpenAI GPT-2 model)"),twe.forEach(t),zJo=i(Me),jT=n(Me,"LI",{});var awe=s(jT);qfe=n(awe,"STRONG",{});var srt=s(qfe);VJo=r(srt,"openai-gpt"),srt.forEach(t),WJo=r(awe," \u2014 "),zD=n(awe,"A",{href:!0});var lrt=s(zD);QJo=r(lrt,"TFOpenAIGPTLMHeadModel"),lrt.forEach(t),HJo=r(awe," (OpenAI GPT model)"),awe.forEach(t),UJo=i(Me),NT=n(Me,"LI",{});var nwe=s(NT);Gfe=n(nwe,"STRONG",{});var irt=s(Gfe);JJo=r(irt,"rembert"),irt.forEach(t),YJo=r(nwe," \u2014 "),VD=n(nwe,"A",{href:!0});var drt=s(VD);KJo=r(drt,"TFRemBertForCausalLM"),drt.forEach(t),ZJo=r(nwe," (RemBERT model)"),nwe.forEach(t),eYo=i(Me),DT=n(Me,"LI",{});var swe=s(DT);Ofe=n(swe,"STRONG",{});var crt=s(Ofe);oYo=r(crt,"roberta"),crt.forEach(t),rYo=r(swe," \u2014 "),WD=n(swe,"A",{href:!0});var frt=s(WD);tYo=r(frt,"TFRobertaForCausalLM"),frt.forEach(t),aYo=r(swe," (RoBERTa model)"),swe.forEach(t),nYo=i(Me),qT=n(Me,"LI",{});var lwe=s(qT);Xfe=n(lwe,"STRONG",{});var mrt=s(Xfe);sYo=r(mrt,"roformer"),mrt.forEach(t),lYo=r(lwe," \u2014 "),QD=n(lwe,"A",{href:!0});var grt=s(QD);iYo=r(grt,"TFRoFormerForCausalLM"),grt.forEach(t),dYo=r(lwe," (RoFormer model)"),lwe.forEach(t),cYo=i(Me),GT=n(Me,"LI",{});var iwe=s(GT);zfe=n(iwe,"STRONG",{});var hrt=s(zfe);fYo=r(hrt,"transfo-xl"),hrt.forEach(t),mYo=r(iwe," \u2014 "),HD=n(iwe,"A",{href:!0});var prt=s(HD);gYo=r(prt,"TFTransfoXLLMHeadModel"),prt.forEach(t),hYo=r(iwe," (Transformer-XL model)"),iwe.forEach(t),pYo=i(Me),OT=n(Me,"LI",{});var dwe=s(OT);Vfe=n(dwe,"STRONG",{});var _rt=s(Vfe);_Yo=r(_rt,"xlm"),_rt.forEach(t),uYo=r(dwe," \u2014 "),UD=n(dwe,"A",{href:!0});var urt=s(UD);bYo=r(urt,"TFXLMWithLMHeadModel"),urt.forEach(t),vYo=r(dwe," (XLM model)"),dwe.forEach(t),TYo=i(Me),XT=n(Me,"LI",{});var cwe=s(XT);Wfe=n(cwe,"STRONG",{});var brt=s(Wfe);FYo=r(brt,"xlnet"),brt.forEach(t),CYo=r(cwe," \u2014 "),JD=n(cwe,"A",{href:!0});var vrt=s(JD);MYo=r(vrt,"TFXLNetLMHeadModel"),vrt.forEach(t),EYo=r(cwe," (XLNet model)"),cwe.forEach(t),Me.forEach(t),yYo=i(ma),Qfe=n(ma,"P",{});var Trt=s(Qfe);wYo=r(Trt,"Examples:"),Trt.forEach(t),AYo=i(ma),m(Uy.$$.fragment,ma),ma.forEach(t),Sl.forEach(t),m8e=i(d),cc=n(d,"H2",{class:!0});var Fke=s(cc);zT=n(Fke,"A",{id:!0,class:!0,href:!0});var Frt=s(zT);Hfe=n(Frt,"SPAN",{});var Crt=s(Hfe);m(Jy.$$.fragment,Crt),Crt.forEach(t),Frt.forEach(t),LYo=i(Fke),Ufe=n(Fke,"SPAN",{});var Mrt=s(Ufe);BYo=r(Mrt,"TFAutoModelForImageClassification"),Mrt.forEach(t),Fke.forEach(t),g8e=i(d),_r=n(d,"DIV",{class:!0});var $l=s(_r);m(Yy.$$.fragment,$l),kYo=i($l),fc=n($l,"P",{});var Vz=s(fc);xYo=r(Vz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a image classification head) when created
with the `),Jfe=n(Vz,"CODE",{});var Ert=s(Jfe);RYo=r(Ert,"from_pretrained()"),Ert.forEach(t),SYo=r(Vz,"class method or the "),Yfe=n(Vz,"CODE",{});var yrt=s(Yfe);PYo=r(yrt,"from_config()"),yrt.forEach(t),$Yo=r(Vz,`class
method.`),Vz.forEach(t),IYo=i($l),Ky=n($l,"P",{});var Cke=s(Ky);jYo=r(Cke,"This class cannot be instantiated directly using "),Kfe=n(Cke,"CODE",{});var wrt=s(Kfe);NYo=r(wrt,"__init__()"),wrt.forEach(t),DYo=r(Cke," (throws an error)."),Cke.forEach(t),qYo=i($l),dt=n($l,"DIV",{class:!0});var Il=s(dt);m(Zy.$$.fragment,Il),GYo=i(Il),Zfe=n(Il,"P",{});var Art=s(Zfe);OYo=r(Art,"Instantiates one of the model classes of the library (with a image classification head) from a configuration."),Art.forEach(t),XYo=i(Il),mc=n(Il,"P",{});var Wz=s(mc);zYo=r(Wz,`Note:
Loading a model from its configuration file does `),eme=n(Wz,"STRONG",{});var Lrt=s(eme);VYo=r(Lrt,"not"),Lrt.forEach(t),WYo=r(Wz,` load the model weights. It only affects the
model\u2019s configuration. Use `),ome=n(Wz,"CODE",{});var Brt=s(ome);QYo=r(Brt,"from_pretrained()"),Brt.forEach(t),HYo=r(Wz,"to load the model weights."),Wz.forEach(t),UYo=i(Il),rme=n(Il,"P",{});var krt=s(rme);JYo=r(krt,"Examples:"),krt.forEach(t),YYo=i(Il),m(ew.$$.fragment,Il),Il.forEach(t),KYo=i($l),_o=n($l,"DIV",{class:!0});var ga=s(_o);m(ow.$$.fragment,ga),ZYo=i(ga),tme=n(ga,"P",{});var xrt=s(tme);eKo=r(xrt,"Instantiate one of the model classes of the library (with a image classification head) from a pretrained model."),xrt.forEach(t),oKo=i(ga),fn=n(ga,"P",{});var R4=s(fn);rKo=r(R4,"The model class to instantiate is selected based on the "),ame=n(R4,"CODE",{});var Rrt=s(ame);tKo=r(Rrt,"model_type"),Rrt.forEach(t),aKo=r(R4,` property of the config object (either
passed as an argument or loaded from `),nme=n(R4,"CODE",{});var Srt=s(nme);nKo=r(Srt,"pretrained_model_name_or_path"),Srt.forEach(t),sKo=r(R4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),sme=n(R4,"CODE",{});var Prt=s(sme);lKo=r(Prt,"pretrained_model_name_or_path"),Prt.forEach(t),iKo=r(R4,":"),R4.forEach(t),dKo=i(ga),lme=n(ga,"UL",{});var $rt=s(lme);VT=n($rt,"LI",{});var fwe=s(VT);ime=n(fwe,"STRONG",{});var Irt=s(ime);cKo=r(Irt,"vit"),Irt.forEach(t),fKo=r(fwe," \u2014 "),YD=n(fwe,"A",{href:!0});var jrt=s(YD);mKo=r(jrt,"TFViTForImageClassification"),jrt.forEach(t),gKo=r(fwe," (ViT model)"),fwe.forEach(t),$rt.forEach(t),hKo=i(ga),dme=n(ga,"P",{});var Nrt=s(dme);pKo=r(Nrt,"Examples:"),Nrt.forEach(t),_Ko=i(ga),m(rw.$$.fragment,ga),ga.forEach(t),$l.forEach(t),h8e=i(d),gc=n(d,"H2",{class:!0});var Mke=s(gc);WT=n(Mke,"A",{id:!0,class:!0,href:!0});var Drt=s(WT);cme=n(Drt,"SPAN",{});var qrt=s(cme);m(tw.$$.fragment,qrt),qrt.forEach(t),Drt.forEach(t),uKo=i(Mke),fme=n(Mke,"SPAN",{});var Grt=s(fme);bKo=r(Grt,"TFAutoModelForMaskedLM"),Grt.forEach(t),Mke.forEach(t),p8e=i(d),ur=n(d,"DIV",{class:!0});var jl=s(ur);m(aw.$$.fragment,jl),vKo=i(jl),hc=n(jl,"P",{});var Qz=s(hc);TKo=r(Qz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a masked language modeling head) when created
with the `),mme=n(Qz,"CODE",{});var Ort=s(mme);FKo=r(Ort,"from_pretrained()"),Ort.forEach(t),CKo=r(Qz,"class method or the "),gme=n(Qz,"CODE",{});var Xrt=s(gme);MKo=r(Xrt,"from_config()"),Xrt.forEach(t),EKo=r(Qz,`class
method.`),Qz.forEach(t),yKo=i(jl),nw=n(jl,"P",{});var Eke=s(nw);wKo=r(Eke,"This class cannot be instantiated directly using "),hme=n(Eke,"CODE",{});var zrt=s(hme);AKo=r(zrt,"__init__()"),zrt.forEach(t),LKo=r(Eke," (throws an error)."),Eke.forEach(t),BKo=i(jl),ct=n(jl,"DIV",{class:!0});var Nl=s(ct);m(sw.$$.fragment,Nl),kKo=i(Nl),pme=n(Nl,"P",{});var Vrt=s(pme);xKo=r(Vrt,"Instantiates one of the model classes of the library (with a masked language modeling head) from a configuration."),Vrt.forEach(t),RKo=i(Nl),pc=n(Nl,"P",{});var Hz=s(pc);SKo=r(Hz,`Note:
Loading a model from its configuration file does `),_me=n(Hz,"STRONG",{});var Wrt=s(_me);PKo=r(Wrt,"not"),Wrt.forEach(t),$Ko=r(Hz,` load the model weights. It only affects the
model\u2019s configuration. Use `),ume=n(Hz,"CODE",{});var Qrt=s(ume);IKo=r(Qrt,"from_pretrained()"),Qrt.forEach(t),jKo=r(Hz,"to load the model weights."),Hz.forEach(t),NKo=i(Nl),bme=n(Nl,"P",{});var Hrt=s(bme);DKo=r(Hrt,"Examples:"),Hrt.forEach(t),qKo=i(Nl),m(lw.$$.fragment,Nl),Nl.forEach(t),GKo=i(jl),uo=n(jl,"DIV",{class:!0});var ha=s(uo);m(iw.$$.fragment,ha),OKo=i(ha),vme=n(ha,"P",{});var Urt=s(vme);XKo=r(Urt,"Instantiate one of the model classes of the library (with a masked language modeling head) from a pretrained model."),Urt.forEach(t),zKo=i(ha),mn=n(ha,"P",{});var S4=s(mn);VKo=r(S4,"The model class to instantiate is selected based on the "),Tme=n(S4,"CODE",{});var Jrt=s(Tme);WKo=r(Jrt,"model_type"),Jrt.forEach(t),QKo=r(S4,` property of the config object (either
passed as an argument or loaded from `),Fme=n(S4,"CODE",{});var Yrt=s(Fme);HKo=r(Yrt,"pretrained_model_name_or_path"),Yrt.forEach(t),UKo=r(S4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Cme=n(S4,"CODE",{});var Krt=s(Cme);JKo=r(Krt,"pretrained_model_name_or_path"),Krt.forEach(t),YKo=r(S4,":"),S4.forEach(t),KKo=i(ha),Y=n(ha,"UL",{});var ee=s(Y);QT=n(ee,"LI",{});var mwe=s(QT);Mme=n(mwe,"STRONG",{});var Zrt=s(Mme);ZKo=r(Zrt,"albert"),Zrt.forEach(t),eZo=r(mwe," \u2014 "),KD=n(mwe,"A",{href:!0});var ett=s(KD);oZo=r(ett,"TFAlbertForMaskedLM"),ett.forEach(t),rZo=r(mwe," (ALBERT model)"),mwe.forEach(t),tZo=i(ee),HT=n(ee,"LI",{});var gwe=s(HT);Eme=n(gwe,"STRONG",{});var ott=s(Eme);aZo=r(ott,"bert"),ott.forEach(t),nZo=r(gwe," \u2014 "),ZD=n(gwe,"A",{href:!0});var rtt=s(ZD);sZo=r(rtt,"TFBertForMaskedLM"),rtt.forEach(t),lZo=r(gwe," (BERT model)"),gwe.forEach(t),iZo=i(ee),UT=n(ee,"LI",{});var hwe=s(UT);yme=n(hwe,"STRONG",{});var ttt=s(yme);dZo=r(ttt,"camembert"),ttt.forEach(t),cZo=r(hwe," \u2014 "),eq=n(hwe,"A",{href:!0});var att=s(eq);fZo=r(att,"TFCamembertForMaskedLM"),att.forEach(t),mZo=r(hwe," (CamemBERT model)"),hwe.forEach(t),gZo=i(ee),JT=n(ee,"LI",{});var pwe=s(JT);wme=n(pwe,"STRONG",{});var ntt=s(wme);hZo=r(ntt,"convbert"),ntt.forEach(t),pZo=r(pwe," \u2014 "),oq=n(pwe,"A",{href:!0});var stt=s(oq);_Zo=r(stt,"TFConvBertForMaskedLM"),stt.forEach(t),uZo=r(pwe," (ConvBERT model)"),pwe.forEach(t),bZo=i(ee),YT=n(ee,"LI",{});var _we=s(YT);Ame=n(_we,"STRONG",{});var ltt=s(Ame);vZo=r(ltt,"deberta"),ltt.forEach(t),TZo=r(_we," \u2014 "),rq=n(_we,"A",{href:!0});var itt=s(rq);FZo=r(itt,"TFDebertaForMaskedLM"),itt.forEach(t),CZo=r(_we," (DeBERTa model)"),_we.forEach(t),MZo=i(ee),KT=n(ee,"LI",{});var uwe=s(KT);Lme=n(uwe,"STRONG",{});var dtt=s(Lme);EZo=r(dtt,"deberta-v2"),dtt.forEach(t),yZo=r(uwe," \u2014 "),tq=n(uwe,"A",{href:!0});var ctt=s(tq);wZo=r(ctt,"TFDebertaV2ForMaskedLM"),ctt.forEach(t),AZo=r(uwe," (DeBERTa-v2 model)"),uwe.forEach(t),LZo=i(ee),ZT=n(ee,"LI",{});var bwe=s(ZT);Bme=n(bwe,"STRONG",{});var ftt=s(Bme);BZo=r(ftt,"distilbert"),ftt.forEach(t),kZo=r(bwe," \u2014 "),aq=n(bwe,"A",{href:!0});var mtt=s(aq);xZo=r(mtt,"TFDistilBertForMaskedLM"),mtt.forEach(t),RZo=r(bwe," (DistilBERT model)"),bwe.forEach(t),SZo=i(ee),e7=n(ee,"LI",{});var vwe=s(e7);kme=n(vwe,"STRONG",{});var gtt=s(kme);PZo=r(gtt,"electra"),gtt.forEach(t),$Zo=r(vwe," \u2014 "),nq=n(vwe,"A",{href:!0});var htt=s(nq);IZo=r(htt,"TFElectraForMaskedLM"),htt.forEach(t),jZo=r(vwe," (ELECTRA model)"),vwe.forEach(t),NZo=i(ee),o7=n(ee,"LI",{});var Twe=s(o7);xme=n(Twe,"STRONG",{});var ptt=s(xme);DZo=r(ptt,"flaubert"),ptt.forEach(t),qZo=r(Twe," \u2014 "),sq=n(Twe,"A",{href:!0});var _tt=s(sq);GZo=r(_tt,"TFFlaubertWithLMHeadModel"),_tt.forEach(t),OZo=r(Twe," (FlauBERT model)"),Twe.forEach(t),XZo=i(ee),r7=n(ee,"LI",{});var Fwe=s(r7);Rme=n(Fwe,"STRONG",{});var utt=s(Rme);zZo=r(utt,"funnel"),utt.forEach(t),VZo=r(Fwe," \u2014 "),lq=n(Fwe,"A",{href:!0});var btt=s(lq);WZo=r(btt,"TFFunnelForMaskedLM"),btt.forEach(t),QZo=r(Fwe," (Funnel Transformer model)"),Fwe.forEach(t),HZo=i(ee),t7=n(ee,"LI",{});var Cwe=s(t7);Sme=n(Cwe,"STRONG",{});var vtt=s(Sme);UZo=r(vtt,"layoutlm"),vtt.forEach(t),JZo=r(Cwe," \u2014 "),iq=n(Cwe,"A",{href:!0});var Ttt=s(iq);YZo=r(Ttt,"TFLayoutLMForMaskedLM"),Ttt.forEach(t),KZo=r(Cwe," (LayoutLM model)"),Cwe.forEach(t),ZZo=i(ee),a7=n(ee,"LI",{});var Mwe=s(a7);Pme=n(Mwe,"STRONG",{});var Ftt=s(Pme);eer=r(Ftt,"longformer"),Ftt.forEach(t),oer=r(Mwe," \u2014 "),dq=n(Mwe,"A",{href:!0});var Ctt=s(dq);rer=r(Ctt,"TFLongformerForMaskedLM"),Ctt.forEach(t),ter=r(Mwe," (Longformer model)"),Mwe.forEach(t),aer=i(ee),n7=n(ee,"LI",{});var Ewe=s(n7);$me=n(Ewe,"STRONG",{});var Mtt=s($me);ner=r(Mtt,"mobilebert"),Mtt.forEach(t),ser=r(Ewe," \u2014 "),cq=n(Ewe,"A",{href:!0});var Ett=s(cq);ler=r(Ett,"TFMobileBertForMaskedLM"),Ett.forEach(t),ier=r(Ewe," (MobileBERT model)"),Ewe.forEach(t),der=i(ee),s7=n(ee,"LI",{});var ywe=s(s7);Ime=n(ywe,"STRONG",{});var ytt=s(Ime);cer=r(ytt,"mpnet"),ytt.forEach(t),fer=r(ywe," \u2014 "),fq=n(ywe,"A",{href:!0});var wtt=s(fq);mer=r(wtt,"TFMPNetForMaskedLM"),wtt.forEach(t),ger=r(ywe," (MPNet model)"),ywe.forEach(t),her=i(ee),l7=n(ee,"LI",{});var wwe=s(l7);jme=n(wwe,"STRONG",{});var Att=s(jme);per=r(Att,"rembert"),Att.forEach(t),_er=r(wwe," \u2014 "),mq=n(wwe,"A",{href:!0});var Ltt=s(mq);uer=r(Ltt,"TFRemBertForMaskedLM"),Ltt.forEach(t),ber=r(wwe," (RemBERT model)"),wwe.forEach(t),ver=i(ee),i7=n(ee,"LI",{});var Awe=s(i7);Nme=n(Awe,"STRONG",{});var Btt=s(Nme);Ter=r(Btt,"roberta"),Btt.forEach(t),Fer=r(Awe," \u2014 "),gq=n(Awe,"A",{href:!0});var ktt=s(gq);Cer=r(ktt,"TFRobertaForMaskedLM"),ktt.forEach(t),Mer=r(Awe," (RoBERTa model)"),Awe.forEach(t),Eer=i(ee),d7=n(ee,"LI",{});var Lwe=s(d7);Dme=n(Lwe,"STRONG",{});var xtt=s(Dme);yer=r(xtt,"roformer"),xtt.forEach(t),wer=r(Lwe," \u2014 "),hq=n(Lwe,"A",{href:!0});var Rtt=s(hq);Aer=r(Rtt,"TFRoFormerForMaskedLM"),Rtt.forEach(t),Ler=r(Lwe," (RoFormer model)"),Lwe.forEach(t),Ber=i(ee),c7=n(ee,"LI",{});var Bwe=s(c7);qme=n(Bwe,"STRONG",{});var Stt=s(qme);ker=r(Stt,"tapas"),Stt.forEach(t),xer=r(Bwe," \u2014 "),pq=n(Bwe,"A",{href:!0});var Ptt=s(pq);Rer=r(Ptt,"TFTapasForMaskedLM"),Ptt.forEach(t),Ser=r(Bwe," (TAPAS model)"),Bwe.forEach(t),Per=i(ee),f7=n(ee,"LI",{});var kwe=s(f7);Gme=n(kwe,"STRONG",{});var $tt=s(Gme);$er=r($tt,"xlm"),$tt.forEach(t),Ier=r(kwe," \u2014 "),_q=n(kwe,"A",{href:!0});var Itt=s(_q);jer=r(Itt,"TFXLMWithLMHeadModel"),Itt.forEach(t),Ner=r(kwe," (XLM model)"),kwe.forEach(t),Der=i(ee),m7=n(ee,"LI",{});var xwe=s(m7);Ome=n(xwe,"STRONG",{});var jtt=s(Ome);qer=r(jtt,"xlm-roberta"),jtt.forEach(t),Ger=r(xwe," \u2014 "),uq=n(xwe,"A",{href:!0});var Ntt=s(uq);Oer=r(Ntt,"TFXLMRobertaForMaskedLM"),Ntt.forEach(t),Xer=r(xwe," (XLM-RoBERTa model)"),xwe.forEach(t),ee.forEach(t),zer=i(ha),Xme=n(ha,"P",{});var Dtt=s(Xme);Ver=r(Dtt,"Examples:"),Dtt.forEach(t),Wer=i(ha),m(dw.$$.fragment,ha),ha.forEach(t),jl.forEach(t),_8e=i(d),_c=n(d,"H2",{class:!0});var yke=s(_c);g7=n(yke,"A",{id:!0,class:!0,href:!0});var qtt=s(g7);zme=n(qtt,"SPAN",{});var Gtt=s(zme);m(cw.$$.fragment,Gtt),Gtt.forEach(t),qtt.forEach(t),Qer=i(yke),Vme=n(yke,"SPAN",{});var Ott=s(Vme);Her=r(Ott,"TFAutoModelForSeq2SeqLM"),Ott.forEach(t),yke.forEach(t),u8e=i(d),br=n(d,"DIV",{class:!0});var Dl=s(br);m(fw.$$.fragment,Dl),Uer=i(Dl),uc=n(Dl,"P",{});var Uz=s(uc);Jer=r(Uz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence language modeling head) when created
with the `),Wme=n(Uz,"CODE",{});var Xtt=s(Wme);Yer=r(Xtt,"from_pretrained()"),Xtt.forEach(t),Ker=r(Uz,"class method or the "),Qme=n(Uz,"CODE",{});var ztt=s(Qme);Zer=r(ztt,"from_config()"),ztt.forEach(t),eor=r(Uz,`class
method.`),Uz.forEach(t),oor=i(Dl),mw=n(Dl,"P",{});var wke=s(mw);ror=r(wke,"This class cannot be instantiated directly using "),Hme=n(wke,"CODE",{});var Vtt=s(Hme);tor=r(Vtt,"__init__()"),Vtt.forEach(t),aor=r(wke," (throws an error)."),wke.forEach(t),nor=i(Dl),ft=n(Dl,"DIV",{class:!0});var ql=s(ft);m(gw.$$.fragment,ql),sor=i(ql),Ume=n(ql,"P",{});var Wtt=s(Ume);lor=r(Wtt,"Instantiates one of the model classes of the library (with a sequence-to-sequence language modeling head) from a configuration."),Wtt.forEach(t),ior=i(ql),bc=n(ql,"P",{});var Jz=s(bc);dor=r(Jz,`Note:
Loading a model from its configuration file does `),Jme=n(Jz,"STRONG",{});var Qtt=s(Jme);cor=r(Qtt,"not"),Qtt.forEach(t),mor=r(Jz,` load the model weights. It only affects the
model\u2019s configuration. Use `),Yme=n(Jz,"CODE",{});var Htt=s(Yme);gor=r(Htt,"from_pretrained()"),Htt.forEach(t),hor=r(Jz,"to load the model weights."),Jz.forEach(t),por=i(ql),Kme=n(ql,"P",{});var Utt=s(Kme);_or=r(Utt,"Examples:"),Utt.forEach(t),uor=i(ql),m(hw.$$.fragment,ql),ql.forEach(t),bor=i(Dl),bo=n(Dl,"DIV",{class:!0});var pa=s(bo);m(pw.$$.fragment,pa),vor=i(pa),Zme=n(pa,"P",{});var Jtt=s(Zme);Tor=r(Jtt,"Instantiate one of the model classes of the library (with a sequence-to-sequence language modeling head) from a pretrained model."),Jtt.forEach(t),For=i(pa),gn=n(pa,"P",{});var P4=s(gn);Cor=r(P4,"The model class to instantiate is selected based on the "),ege=n(P4,"CODE",{});var Ytt=s(ege);Mor=r(Ytt,"model_type"),Ytt.forEach(t),Eor=r(P4,` property of the config object (either
passed as an argument or loaded from `),oge=n(P4,"CODE",{});var Ktt=s(oge);yor=r(Ktt,"pretrained_model_name_or_path"),Ktt.forEach(t),wor=r(P4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),rge=n(P4,"CODE",{});var Ztt=s(rge);Aor=r(Ztt,"pretrained_model_name_or_path"),Ztt.forEach(t),Lor=r(P4,":"),P4.forEach(t),Bor=i(pa),pe=n(pa,"UL",{});var Ee=s(pe);h7=n(Ee,"LI",{});var Rwe=s(h7);tge=n(Rwe,"STRONG",{});var eat=s(tge);kor=r(eat,"bart"),eat.forEach(t),xor=r(Rwe," \u2014 "),bq=n(Rwe,"A",{href:!0});var oat=s(bq);Ror=r(oat,"TFBartForConditionalGeneration"),oat.forEach(t),Sor=r(Rwe," (BART model)"),Rwe.forEach(t),Por=i(Ee),p7=n(Ee,"LI",{});var Swe=s(p7);age=n(Swe,"STRONG",{});var rat=s(age);$or=r(rat,"blenderbot"),rat.forEach(t),Ior=r(Swe," \u2014 "),vq=n(Swe,"A",{href:!0});var tat=s(vq);jor=r(tat,"TFBlenderbotForConditionalGeneration"),tat.forEach(t),Nor=r(Swe," (Blenderbot model)"),Swe.forEach(t),Dor=i(Ee),_7=n(Ee,"LI",{});var Pwe=s(_7);nge=n(Pwe,"STRONG",{});var aat=s(nge);qor=r(aat,"blenderbot-small"),aat.forEach(t),Gor=r(Pwe," \u2014 "),Tq=n(Pwe,"A",{href:!0});var nat=s(Tq);Oor=r(nat,"TFBlenderbotSmallForConditionalGeneration"),nat.forEach(t),Xor=r(Pwe," (BlenderbotSmall model)"),Pwe.forEach(t),zor=i(Ee),u7=n(Ee,"LI",{});var $we=s(u7);sge=n($we,"STRONG",{});var sat=s(sge);Vor=r(sat,"encoder-decoder"),sat.forEach(t),Wor=r($we," \u2014 "),Fq=n($we,"A",{href:!0});var lat=s(Fq);Qor=r(lat,"TFEncoderDecoderModel"),lat.forEach(t),Hor=r($we," (Encoder decoder model)"),$we.forEach(t),Uor=i(Ee),b7=n(Ee,"LI",{});var Iwe=s(b7);lge=n(Iwe,"STRONG",{});var iat=s(lge);Jor=r(iat,"led"),iat.forEach(t),Yor=r(Iwe," \u2014 "),Cq=n(Iwe,"A",{href:!0});var dat=s(Cq);Kor=r(dat,"TFLEDForConditionalGeneration"),dat.forEach(t),Zor=r(Iwe," (LED model)"),Iwe.forEach(t),err=i(Ee),v7=n(Ee,"LI",{});var jwe=s(v7);ige=n(jwe,"STRONG",{});var cat=s(ige);orr=r(cat,"marian"),cat.forEach(t),rrr=r(jwe," \u2014 "),Mq=n(jwe,"A",{href:!0});var fat=s(Mq);trr=r(fat,"TFMarianMTModel"),fat.forEach(t),arr=r(jwe," (Marian model)"),jwe.forEach(t),nrr=i(Ee),T7=n(Ee,"LI",{});var Nwe=s(T7);dge=n(Nwe,"STRONG",{});var mat=s(dge);srr=r(mat,"mbart"),mat.forEach(t),lrr=r(Nwe," \u2014 "),Eq=n(Nwe,"A",{href:!0});var gat=s(Eq);irr=r(gat,"TFMBartForConditionalGeneration"),gat.forEach(t),drr=r(Nwe," (mBART model)"),Nwe.forEach(t),crr=i(Ee),F7=n(Ee,"LI",{});var Dwe=s(F7);cge=n(Dwe,"STRONG",{});var hat=s(cge);frr=r(hat,"mt5"),hat.forEach(t),mrr=r(Dwe," \u2014 "),yq=n(Dwe,"A",{href:!0});var pat=s(yq);grr=r(pat,"TFMT5ForConditionalGeneration"),pat.forEach(t),hrr=r(Dwe," (mT5 model)"),Dwe.forEach(t),prr=i(Ee),C7=n(Ee,"LI",{});var qwe=s(C7);fge=n(qwe,"STRONG",{});var _at=s(fge);_rr=r(_at,"pegasus"),_at.forEach(t),urr=r(qwe," \u2014 "),wq=n(qwe,"A",{href:!0});var uat=s(wq);brr=r(uat,"TFPegasusForConditionalGeneration"),uat.forEach(t),vrr=r(qwe," (Pegasus model)"),qwe.forEach(t),Trr=i(Ee),M7=n(Ee,"LI",{});var Gwe=s(M7);mge=n(Gwe,"STRONG",{});var bat=s(mge);Frr=r(bat,"t5"),bat.forEach(t),Crr=r(Gwe," \u2014 "),Aq=n(Gwe,"A",{href:!0});var vat=s(Aq);Mrr=r(vat,"TFT5ForConditionalGeneration"),vat.forEach(t),Err=r(Gwe," (T5 model)"),Gwe.forEach(t),Ee.forEach(t),yrr=i(pa),gge=n(pa,"P",{});var Tat=s(gge);wrr=r(Tat,"Examples:"),Tat.forEach(t),Arr=i(pa),m(_w.$$.fragment,pa),pa.forEach(t),Dl.forEach(t),b8e=i(d),vc=n(d,"H2",{class:!0});var Ake=s(vc);E7=n(Ake,"A",{id:!0,class:!0,href:!0});var Fat=s(E7);hge=n(Fat,"SPAN",{});var Cat=s(hge);m(uw.$$.fragment,Cat),Cat.forEach(t),Fat.forEach(t),Lrr=i(Ake),pge=n(Ake,"SPAN",{});var Mat=s(pge);Brr=r(Mat,"TFAutoModelForSequenceClassification"),Mat.forEach(t),Ake.forEach(t),v8e=i(d),vr=n(d,"DIV",{class:!0});var Gl=s(vr);m(bw.$$.fragment,Gl),krr=i(Gl),Tc=n(Gl,"P",{});var Yz=s(Tc);xrr=r(Yz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence classification head) when created
with the `),_ge=n(Yz,"CODE",{});var Eat=s(_ge);Rrr=r(Eat,"from_pretrained()"),Eat.forEach(t),Srr=r(Yz,"class method or the "),uge=n(Yz,"CODE",{});var yat=s(uge);Prr=r(yat,"from_config()"),yat.forEach(t),$rr=r(Yz,`class
method.`),Yz.forEach(t),Irr=i(Gl),vw=n(Gl,"P",{});var Lke=s(vw);jrr=r(Lke,"This class cannot be instantiated directly using "),bge=n(Lke,"CODE",{});var wat=s(bge);Nrr=r(wat,"__init__()"),wat.forEach(t),Drr=r(Lke," (throws an error)."),Lke.forEach(t),qrr=i(Gl),mt=n(Gl,"DIV",{class:!0});var Ol=s(mt);m(Tw.$$.fragment,Ol),Grr=i(Ol),vge=n(Ol,"P",{});var Aat=s(vge);Orr=r(Aat,"Instantiates one of the model classes of the library (with a sequence classification head) from a configuration."),Aat.forEach(t),Xrr=i(Ol),Fc=n(Ol,"P",{});var Kz=s(Fc);zrr=r(Kz,`Note:
Loading a model from its configuration file does `),Tge=n(Kz,"STRONG",{});var Lat=s(Tge);Vrr=r(Lat,"not"),Lat.forEach(t),Wrr=r(Kz,` load the model weights. It only affects the
model\u2019s configuration. Use `),Fge=n(Kz,"CODE",{});var Bat=s(Fge);Qrr=r(Bat,"from_pretrained()"),Bat.forEach(t),Hrr=r(Kz,"to load the model weights."),Kz.forEach(t),Urr=i(Ol),Cge=n(Ol,"P",{});var kat=s(Cge);Jrr=r(kat,"Examples:"),kat.forEach(t),Yrr=i(Ol),m(Fw.$$.fragment,Ol),Ol.forEach(t),Krr=i(Gl),vo=n(Gl,"DIV",{class:!0});var _a=s(vo);m(Cw.$$.fragment,_a),Zrr=i(_a),Mge=n(_a,"P",{});var xat=s(Mge);etr=r(xat,"Instantiate one of the model classes of the library (with a sequence classification head) from a pretrained model."),xat.forEach(t),otr=i(_a),hn=n(_a,"P",{});var $4=s(hn);rtr=r($4,"The model class to instantiate is selected based on the "),Ege=n($4,"CODE",{});var Rat=s(Ege);ttr=r(Rat,"model_type"),Rat.forEach(t),atr=r($4,` property of the config object (either
passed as an argument or loaded from `),yge=n($4,"CODE",{});var Sat=s(yge);ntr=r(Sat,"pretrained_model_name_or_path"),Sat.forEach(t),str=r($4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),wge=n($4,"CODE",{});var Pat=s(wge);ltr=r(Pat,"pretrained_model_name_or_path"),Pat.forEach(t),itr=r($4,":"),$4.forEach(t),dtr=i(_a),X=n(_a,"UL",{});var W=s(X);y7=n(W,"LI",{});var Owe=s(y7);Age=n(Owe,"STRONG",{});var $at=s(Age);ctr=r($at,"albert"),$at.forEach(t),ftr=r(Owe," \u2014 "),Lq=n(Owe,"A",{href:!0});var Iat=s(Lq);mtr=r(Iat,"TFAlbertForSequenceClassification"),Iat.forEach(t),gtr=r(Owe," (ALBERT model)"),Owe.forEach(t),htr=i(W),w7=n(W,"LI",{});var Xwe=s(w7);Lge=n(Xwe,"STRONG",{});var jat=s(Lge);ptr=r(jat,"bert"),jat.forEach(t),_tr=r(Xwe," \u2014 "),Bq=n(Xwe,"A",{href:!0});var Nat=s(Bq);utr=r(Nat,"TFBertForSequenceClassification"),Nat.forEach(t),btr=r(Xwe," (BERT model)"),Xwe.forEach(t),vtr=i(W),A7=n(W,"LI",{});var zwe=s(A7);Bge=n(zwe,"STRONG",{});var Dat=s(Bge);Ttr=r(Dat,"camembert"),Dat.forEach(t),Ftr=r(zwe," \u2014 "),kq=n(zwe,"A",{href:!0});var qat=s(kq);Ctr=r(qat,"TFCamembertForSequenceClassification"),qat.forEach(t),Mtr=r(zwe," (CamemBERT model)"),zwe.forEach(t),Etr=i(W),L7=n(W,"LI",{});var Vwe=s(L7);kge=n(Vwe,"STRONG",{});var Gat=s(kge);ytr=r(Gat,"convbert"),Gat.forEach(t),wtr=r(Vwe," \u2014 "),xq=n(Vwe,"A",{href:!0});var Oat=s(xq);Atr=r(Oat,"TFConvBertForSequenceClassification"),Oat.forEach(t),Ltr=r(Vwe," (ConvBERT model)"),Vwe.forEach(t),Btr=i(W),B7=n(W,"LI",{});var Wwe=s(B7);xge=n(Wwe,"STRONG",{});var Xat=s(xge);ktr=r(Xat,"ctrl"),Xat.forEach(t),xtr=r(Wwe," \u2014 "),Rq=n(Wwe,"A",{href:!0});var zat=s(Rq);Rtr=r(zat,"TFCTRLForSequenceClassification"),zat.forEach(t),Str=r(Wwe," (CTRL model)"),Wwe.forEach(t),Ptr=i(W),k7=n(W,"LI",{});var Qwe=s(k7);Rge=n(Qwe,"STRONG",{});var Vat=s(Rge);$tr=r(Vat,"deberta"),Vat.forEach(t),Itr=r(Qwe," \u2014 "),Sq=n(Qwe,"A",{href:!0});var Wat=s(Sq);jtr=r(Wat,"TFDebertaForSequenceClassification"),Wat.forEach(t),Ntr=r(Qwe," (DeBERTa model)"),Qwe.forEach(t),Dtr=i(W),x7=n(W,"LI",{});var Hwe=s(x7);Sge=n(Hwe,"STRONG",{});var Qat=s(Sge);qtr=r(Qat,"deberta-v2"),Qat.forEach(t),Gtr=r(Hwe," \u2014 "),Pq=n(Hwe,"A",{href:!0});var Hat=s(Pq);Otr=r(Hat,"TFDebertaV2ForSequenceClassification"),Hat.forEach(t),Xtr=r(Hwe," (DeBERTa-v2 model)"),Hwe.forEach(t),ztr=i(W),R7=n(W,"LI",{});var Uwe=s(R7);Pge=n(Uwe,"STRONG",{});var Uat=s(Pge);Vtr=r(Uat,"distilbert"),Uat.forEach(t),Wtr=r(Uwe," \u2014 "),$q=n(Uwe,"A",{href:!0});var Jat=s($q);Qtr=r(Jat,"TFDistilBertForSequenceClassification"),Jat.forEach(t),Htr=r(Uwe," (DistilBERT model)"),Uwe.forEach(t),Utr=i(W),S7=n(W,"LI",{});var Jwe=s(S7);$ge=n(Jwe,"STRONG",{});var Yat=s($ge);Jtr=r(Yat,"electra"),Yat.forEach(t),Ytr=r(Jwe," \u2014 "),Iq=n(Jwe,"A",{href:!0});var Kat=s(Iq);Ktr=r(Kat,"TFElectraForSequenceClassification"),Kat.forEach(t),Ztr=r(Jwe," (ELECTRA model)"),Jwe.forEach(t),ear=i(W),P7=n(W,"LI",{});var Ywe=s(P7);Ige=n(Ywe,"STRONG",{});var Zat=s(Ige);oar=r(Zat,"flaubert"),Zat.forEach(t),rar=r(Ywe," \u2014 "),jq=n(Ywe,"A",{href:!0});var ent=s(jq);tar=r(ent,"TFFlaubertForSequenceClassification"),ent.forEach(t),aar=r(Ywe," (FlauBERT model)"),Ywe.forEach(t),nar=i(W),$7=n(W,"LI",{});var Kwe=s($7);jge=n(Kwe,"STRONG",{});var ont=s(jge);sar=r(ont,"funnel"),ont.forEach(t),lar=r(Kwe," \u2014 "),Nq=n(Kwe,"A",{href:!0});var rnt=s(Nq);iar=r(rnt,"TFFunnelForSequenceClassification"),rnt.forEach(t),dar=r(Kwe," (Funnel Transformer model)"),Kwe.forEach(t),car=i(W),I7=n(W,"LI",{});var Zwe=s(I7);Nge=n(Zwe,"STRONG",{});var tnt=s(Nge);far=r(tnt,"gpt2"),tnt.forEach(t),mar=r(Zwe," \u2014 "),Dq=n(Zwe,"A",{href:!0});var ant=s(Dq);gar=r(ant,"TFGPT2ForSequenceClassification"),ant.forEach(t),har=r(Zwe," (OpenAI GPT-2 model)"),Zwe.forEach(t),par=i(W),j7=n(W,"LI",{});var eAe=s(j7);Dge=n(eAe,"STRONG",{});var nnt=s(Dge);_ar=r(nnt,"layoutlm"),nnt.forEach(t),uar=r(eAe," \u2014 "),qq=n(eAe,"A",{href:!0});var snt=s(qq);bar=r(snt,"TFLayoutLMForSequenceClassification"),snt.forEach(t),Tar=r(eAe," (LayoutLM model)"),eAe.forEach(t),Far=i(W),N7=n(W,"LI",{});var oAe=s(N7);qge=n(oAe,"STRONG",{});var lnt=s(qge);Car=r(lnt,"longformer"),lnt.forEach(t),Mar=r(oAe," \u2014 "),Gq=n(oAe,"A",{href:!0});var int=s(Gq);Ear=r(int,"TFLongformerForSequenceClassification"),int.forEach(t),yar=r(oAe," (Longformer model)"),oAe.forEach(t),war=i(W),D7=n(W,"LI",{});var rAe=s(D7);Gge=n(rAe,"STRONG",{});var dnt=s(Gge);Aar=r(dnt,"mobilebert"),dnt.forEach(t),Lar=r(rAe," \u2014 "),Oq=n(rAe,"A",{href:!0});var cnt=s(Oq);Bar=r(cnt,"TFMobileBertForSequenceClassification"),cnt.forEach(t),kar=r(rAe," (MobileBERT model)"),rAe.forEach(t),xar=i(W),q7=n(W,"LI",{});var tAe=s(q7);Oge=n(tAe,"STRONG",{});var fnt=s(Oge);Rar=r(fnt,"mpnet"),fnt.forEach(t),Sar=r(tAe," \u2014 "),Xq=n(tAe,"A",{href:!0});var mnt=s(Xq);Par=r(mnt,"TFMPNetForSequenceClassification"),mnt.forEach(t),$ar=r(tAe," (MPNet model)"),tAe.forEach(t),Iar=i(W),G7=n(W,"LI",{});var aAe=s(G7);Xge=n(aAe,"STRONG",{});var gnt=s(Xge);jar=r(gnt,"openai-gpt"),gnt.forEach(t),Nar=r(aAe," \u2014 "),zq=n(aAe,"A",{href:!0});var hnt=s(zq);Dar=r(hnt,"TFOpenAIGPTForSequenceClassification"),hnt.forEach(t),qar=r(aAe," (OpenAI GPT model)"),aAe.forEach(t),Gar=i(W),O7=n(W,"LI",{});var nAe=s(O7);zge=n(nAe,"STRONG",{});var pnt=s(zge);Oar=r(pnt,"rembert"),pnt.forEach(t),Xar=r(nAe," \u2014 "),Vq=n(nAe,"A",{href:!0});var _nt=s(Vq);zar=r(_nt,"TFRemBertForSequenceClassification"),_nt.forEach(t),Var=r(nAe," (RemBERT model)"),nAe.forEach(t),War=i(W),X7=n(W,"LI",{});var sAe=s(X7);Vge=n(sAe,"STRONG",{});var unt=s(Vge);Qar=r(unt,"roberta"),unt.forEach(t),Har=r(sAe," \u2014 "),Wq=n(sAe,"A",{href:!0});var bnt=s(Wq);Uar=r(bnt,"TFRobertaForSequenceClassification"),bnt.forEach(t),Jar=r(sAe," (RoBERTa model)"),sAe.forEach(t),Yar=i(W),z7=n(W,"LI",{});var lAe=s(z7);Wge=n(lAe,"STRONG",{});var vnt=s(Wge);Kar=r(vnt,"roformer"),vnt.forEach(t),Zar=r(lAe," \u2014 "),Qq=n(lAe,"A",{href:!0});var Tnt=s(Qq);enr=r(Tnt,"TFRoFormerForSequenceClassification"),Tnt.forEach(t),onr=r(lAe," (RoFormer model)"),lAe.forEach(t),rnr=i(W),V7=n(W,"LI",{});var iAe=s(V7);Qge=n(iAe,"STRONG",{});var Fnt=s(Qge);tnr=r(Fnt,"tapas"),Fnt.forEach(t),anr=r(iAe," \u2014 "),Hq=n(iAe,"A",{href:!0});var Cnt=s(Hq);nnr=r(Cnt,"TFTapasForSequenceClassification"),Cnt.forEach(t),snr=r(iAe," (TAPAS model)"),iAe.forEach(t),lnr=i(W),W7=n(W,"LI",{});var dAe=s(W7);Hge=n(dAe,"STRONG",{});var Mnt=s(Hge);inr=r(Mnt,"transfo-xl"),Mnt.forEach(t),dnr=r(dAe," \u2014 "),Uq=n(dAe,"A",{href:!0});var Ent=s(Uq);cnr=r(Ent,"TFTransfoXLForSequenceClassification"),Ent.forEach(t),fnr=r(dAe," (Transformer-XL model)"),dAe.forEach(t),mnr=i(W),Q7=n(W,"LI",{});var cAe=s(Q7);Uge=n(cAe,"STRONG",{});var ynt=s(Uge);gnr=r(ynt,"xlm"),ynt.forEach(t),hnr=r(cAe," \u2014 "),Jq=n(cAe,"A",{href:!0});var wnt=s(Jq);pnr=r(wnt,"TFXLMForSequenceClassification"),wnt.forEach(t),_nr=r(cAe," (XLM model)"),cAe.forEach(t),unr=i(W),H7=n(W,"LI",{});var fAe=s(H7);Jge=n(fAe,"STRONG",{});var Ant=s(Jge);bnr=r(Ant,"xlm-roberta"),Ant.forEach(t),vnr=r(fAe," \u2014 "),Yq=n(fAe,"A",{href:!0});var Lnt=s(Yq);Tnr=r(Lnt,"TFXLMRobertaForSequenceClassification"),Lnt.forEach(t),Fnr=r(fAe," (XLM-RoBERTa model)"),fAe.forEach(t),Cnr=i(W),U7=n(W,"LI",{});var mAe=s(U7);Yge=n(mAe,"STRONG",{});var Bnt=s(Yge);Mnr=r(Bnt,"xlnet"),Bnt.forEach(t),Enr=r(mAe," \u2014 "),Kq=n(mAe,"A",{href:!0});var knt=s(Kq);ynr=r(knt,"TFXLNetForSequenceClassification"),knt.forEach(t),wnr=r(mAe," (XLNet model)"),mAe.forEach(t),W.forEach(t),Anr=i(_a),Kge=n(_a,"P",{});var xnt=s(Kge);Lnr=r(xnt,"Examples:"),xnt.forEach(t),Bnr=i(_a),m(Mw.$$.fragment,_a),_a.forEach(t),Gl.forEach(t),T8e=i(d),Cc=n(d,"H2",{class:!0});var Bke=s(Cc);J7=n(Bke,"A",{id:!0,class:!0,href:!0});var Rnt=s(J7);Zge=n(Rnt,"SPAN",{});var Snt=s(Zge);m(Ew.$$.fragment,Snt),Snt.forEach(t),Rnt.forEach(t),knr=i(Bke),ehe=n(Bke,"SPAN",{});var Pnt=s(ehe);xnr=r(Pnt,"TFAutoModelForMultipleChoice"),Pnt.forEach(t),Bke.forEach(t),F8e=i(d),Tr=n(d,"DIV",{class:!0});var Xl=s(Tr);m(yw.$$.fragment,Xl),Rnr=i(Xl),Mc=n(Xl,"P",{});var Zz=s(Mc);Snr=r(Zz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a multiple choice head) when created
with the `),ohe=n(Zz,"CODE",{});var $nt=s(ohe);Pnr=r($nt,"from_pretrained()"),$nt.forEach(t),$nr=r(Zz,"class method or the "),rhe=n(Zz,"CODE",{});var Int=s(rhe);Inr=r(Int,"from_config()"),Int.forEach(t),jnr=r(Zz,`class
method.`),Zz.forEach(t),Nnr=i(Xl),ww=n(Xl,"P",{});var kke=s(ww);Dnr=r(kke,"This class cannot be instantiated directly using "),the=n(kke,"CODE",{});var jnt=s(the);qnr=r(jnt,"__init__()"),jnt.forEach(t),Gnr=r(kke," (throws an error)."),kke.forEach(t),Onr=i(Xl),gt=n(Xl,"DIV",{class:!0});var zl=s(gt);m(Aw.$$.fragment,zl),Xnr=i(zl),ahe=n(zl,"P",{});var Nnt=s(ahe);znr=r(Nnt,"Instantiates one of the model classes of the library (with a multiple choice head) from a configuration."),Nnt.forEach(t),Vnr=i(zl),Ec=n(zl,"P",{});var eV=s(Ec);Wnr=r(eV,`Note:
Loading a model from its configuration file does `),nhe=n(eV,"STRONG",{});var Dnt=s(nhe);Qnr=r(Dnt,"not"),Dnt.forEach(t),Hnr=r(eV,` load the model weights. It only affects the
model\u2019s configuration. Use `),she=n(eV,"CODE",{});var qnt=s(she);Unr=r(qnt,"from_pretrained()"),qnt.forEach(t),Jnr=r(eV,"to load the model weights."),eV.forEach(t),Ynr=i(zl),lhe=n(zl,"P",{});var Gnt=s(lhe);Knr=r(Gnt,"Examples:"),Gnt.forEach(t),Znr=i(zl),m(Lw.$$.fragment,zl),zl.forEach(t),esr=i(Xl),To=n(Xl,"DIV",{class:!0});var ua=s(To);m(Bw.$$.fragment,ua),osr=i(ua),ihe=n(ua,"P",{});var Ont=s(ihe);rsr=r(Ont,"Instantiate one of the model classes of the library (with a multiple choice head) from a pretrained model."),Ont.forEach(t),tsr=i(ua),pn=n(ua,"P",{});var I4=s(pn);asr=r(I4,"The model class to instantiate is selected based on the "),dhe=n(I4,"CODE",{});var Xnt=s(dhe);nsr=r(Xnt,"model_type"),Xnt.forEach(t),ssr=r(I4,` property of the config object (either
passed as an argument or loaded from `),che=n(I4,"CODE",{});var znt=s(che);lsr=r(znt,"pretrained_model_name_or_path"),znt.forEach(t),isr=r(I4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),fhe=n(I4,"CODE",{});var Vnt=s(fhe);dsr=r(Vnt,"pretrained_model_name_or_path"),Vnt.forEach(t),csr=r(I4,":"),I4.forEach(t),fsr=i(ua),te=n(ua,"UL",{});var ne=s(te);Y7=n(ne,"LI",{});var gAe=s(Y7);mhe=n(gAe,"STRONG",{});var Wnt=s(mhe);msr=r(Wnt,"albert"),Wnt.forEach(t),gsr=r(gAe," \u2014 "),Zq=n(gAe,"A",{href:!0});var Qnt=s(Zq);hsr=r(Qnt,"TFAlbertForMultipleChoice"),Qnt.forEach(t),psr=r(gAe," (ALBERT model)"),gAe.forEach(t),_sr=i(ne),K7=n(ne,"LI",{});var hAe=s(K7);ghe=n(hAe,"STRONG",{});var Hnt=s(ghe);usr=r(Hnt,"bert"),Hnt.forEach(t),bsr=r(hAe," \u2014 "),eG=n(hAe,"A",{href:!0});var Unt=s(eG);vsr=r(Unt,"TFBertForMultipleChoice"),Unt.forEach(t),Tsr=r(hAe," (BERT model)"),hAe.forEach(t),Fsr=i(ne),Z7=n(ne,"LI",{});var pAe=s(Z7);hhe=n(pAe,"STRONG",{});var Jnt=s(hhe);Csr=r(Jnt,"camembert"),Jnt.forEach(t),Msr=r(pAe," \u2014 "),oG=n(pAe,"A",{href:!0});var Ynt=s(oG);Esr=r(Ynt,"TFCamembertForMultipleChoice"),Ynt.forEach(t),ysr=r(pAe," (CamemBERT model)"),pAe.forEach(t),wsr=i(ne),eF=n(ne,"LI",{});var _Ae=s(eF);phe=n(_Ae,"STRONG",{});var Knt=s(phe);Asr=r(Knt,"convbert"),Knt.forEach(t),Lsr=r(_Ae," \u2014 "),rG=n(_Ae,"A",{href:!0});var Znt=s(rG);Bsr=r(Znt,"TFConvBertForMultipleChoice"),Znt.forEach(t),ksr=r(_Ae," (ConvBERT model)"),_Ae.forEach(t),xsr=i(ne),oF=n(ne,"LI",{});var uAe=s(oF);_he=n(uAe,"STRONG",{});var est=s(_he);Rsr=r(est,"distilbert"),est.forEach(t),Ssr=r(uAe," \u2014 "),tG=n(uAe,"A",{href:!0});var ost=s(tG);Psr=r(ost,"TFDistilBertForMultipleChoice"),ost.forEach(t),$sr=r(uAe," (DistilBERT model)"),uAe.forEach(t),Isr=i(ne),rF=n(ne,"LI",{});var bAe=s(rF);uhe=n(bAe,"STRONG",{});var rst=s(uhe);jsr=r(rst,"electra"),rst.forEach(t),Nsr=r(bAe," \u2014 "),aG=n(bAe,"A",{href:!0});var tst=s(aG);Dsr=r(tst,"TFElectraForMultipleChoice"),tst.forEach(t),qsr=r(bAe," (ELECTRA model)"),bAe.forEach(t),Gsr=i(ne),tF=n(ne,"LI",{});var vAe=s(tF);bhe=n(vAe,"STRONG",{});var ast=s(bhe);Osr=r(ast,"flaubert"),ast.forEach(t),Xsr=r(vAe," \u2014 "),nG=n(vAe,"A",{href:!0});var nst=s(nG);zsr=r(nst,"TFFlaubertForMultipleChoice"),nst.forEach(t),Vsr=r(vAe," (FlauBERT model)"),vAe.forEach(t),Wsr=i(ne),aF=n(ne,"LI",{});var TAe=s(aF);vhe=n(TAe,"STRONG",{});var sst=s(vhe);Qsr=r(sst,"funnel"),sst.forEach(t),Hsr=r(TAe," \u2014 "),sG=n(TAe,"A",{href:!0});var lst=s(sG);Usr=r(lst,"TFFunnelForMultipleChoice"),lst.forEach(t),Jsr=r(TAe," (Funnel Transformer model)"),TAe.forEach(t),Ysr=i(ne),nF=n(ne,"LI",{});var FAe=s(nF);The=n(FAe,"STRONG",{});var ist=s(The);Ksr=r(ist,"longformer"),ist.forEach(t),Zsr=r(FAe," \u2014 "),lG=n(FAe,"A",{href:!0});var dst=s(lG);elr=r(dst,"TFLongformerForMultipleChoice"),dst.forEach(t),olr=r(FAe," (Longformer model)"),FAe.forEach(t),rlr=i(ne),sF=n(ne,"LI",{});var CAe=s(sF);Fhe=n(CAe,"STRONG",{});var cst=s(Fhe);tlr=r(cst,"mobilebert"),cst.forEach(t),alr=r(CAe," \u2014 "),iG=n(CAe,"A",{href:!0});var fst=s(iG);nlr=r(fst,"TFMobileBertForMultipleChoice"),fst.forEach(t),slr=r(CAe," (MobileBERT model)"),CAe.forEach(t),llr=i(ne),lF=n(ne,"LI",{});var MAe=s(lF);Che=n(MAe,"STRONG",{});var mst=s(Che);ilr=r(mst,"mpnet"),mst.forEach(t),dlr=r(MAe," \u2014 "),dG=n(MAe,"A",{href:!0});var gst=s(dG);clr=r(gst,"TFMPNetForMultipleChoice"),gst.forEach(t),flr=r(MAe," (MPNet model)"),MAe.forEach(t),mlr=i(ne),iF=n(ne,"LI",{});var EAe=s(iF);Mhe=n(EAe,"STRONG",{});var hst=s(Mhe);glr=r(hst,"rembert"),hst.forEach(t),hlr=r(EAe," \u2014 "),cG=n(EAe,"A",{href:!0});var pst=s(cG);plr=r(pst,"TFRemBertForMultipleChoice"),pst.forEach(t),_lr=r(EAe," (RemBERT model)"),EAe.forEach(t),ulr=i(ne),dF=n(ne,"LI",{});var yAe=s(dF);Ehe=n(yAe,"STRONG",{});var _st=s(Ehe);blr=r(_st,"roberta"),_st.forEach(t),vlr=r(yAe," \u2014 "),fG=n(yAe,"A",{href:!0});var ust=s(fG);Tlr=r(ust,"TFRobertaForMultipleChoice"),ust.forEach(t),Flr=r(yAe," (RoBERTa model)"),yAe.forEach(t),Clr=i(ne),cF=n(ne,"LI",{});var wAe=s(cF);yhe=n(wAe,"STRONG",{});var bst=s(yhe);Mlr=r(bst,"roformer"),bst.forEach(t),Elr=r(wAe," \u2014 "),mG=n(wAe,"A",{href:!0});var vst=s(mG);ylr=r(vst,"TFRoFormerForMultipleChoice"),vst.forEach(t),wlr=r(wAe," (RoFormer model)"),wAe.forEach(t),Alr=i(ne),fF=n(ne,"LI",{});var AAe=s(fF);whe=n(AAe,"STRONG",{});var Tst=s(whe);Llr=r(Tst,"xlm"),Tst.forEach(t),Blr=r(AAe," \u2014 "),gG=n(AAe,"A",{href:!0});var Fst=s(gG);klr=r(Fst,"TFXLMForMultipleChoice"),Fst.forEach(t),xlr=r(AAe," (XLM model)"),AAe.forEach(t),Rlr=i(ne),mF=n(ne,"LI",{});var LAe=s(mF);Ahe=n(LAe,"STRONG",{});var Cst=s(Ahe);Slr=r(Cst,"xlm-roberta"),Cst.forEach(t),Plr=r(LAe," \u2014 "),hG=n(LAe,"A",{href:!0});var Mst=s(hG);$lr=r(Mst,"TFXLMRobertaForMultipleChoice"),Mst.forEach(t),Ilr=r(LAe," (XLM-RoBERTa model)"),LAe.forEach(t),jlr=i(ne),gF=n(ne,"LI",{});var BAe=s(gF);Lhe=n(BAe,"STRONG",{});var Est=s(Lhe);Nlr=r(Est,"xlnet"),Est.forEach(t),Dlr=r(BAe," \u2014 "),pG=n(BAe,"A",{href:!0});var yst=s(pG);qlr=r(yst,"TFXLNetForMultipleChoice"),yst.forEach(t),Glr=r(BAe," (XLNet model)"),BAe.forEach(t),ne.forEach(t),Olr=i(ua),Bhe=n(ua,"P",{});var wst=s(Bhe);Xlr=r(wst,"Examples:"),wst.forEach(t),zlr=i(ua),m(kw.$$.fragment,ua),ua.forEach(t),Xl.forEach(t),C8e=i(d),yc=n(d,"H2",{class:!0});var xke=s(yc);hF=n(xke,"A",{id:!0,class:!0,href:!0});var Ast=s(hF);khe=n(Ast,"SPAN",{});var Lst=s(khe);m(xw.$$.fragment,Lst),Lst.forEach(t),Ast.forEach(t),Vlr=i(xke),xhe=n(xke,"SPAN",{});var Bst=s(xhe);Wlr=r(Bst,"TFAutoModelForTableQuestionAnswering"),Bst.forEach(t),xke.forEach(t),M8e=i(d),Fr=n(d,"DIV",{class:!0});var Vl=s(Fr);m(Rw.$$.fragment,Vl),Qlr=i(Vl),wc=n(Vl,"P",{});var oV=s(wc);Hlr=r(oV,`This is a generic model class that will be instantiated as one of the model classes of the library (with a table question answering head) when created
with the `),Rhe=n(oV,"CODE",{});var kst=s(Rhe);Ulr=r(kst,"from_pretrained()"),kst.forEach(t),Jlr=r(oV,"class method or the "),She=n(oV,"CODE",{});var xst=s(She);Ylr=r(xst,"from_config()"),xst.forEach(t),Klr=r(oV,`class
method.`),oV.forEach(t),Zlr=i(Vl),Sw=n(Vl,"P",{});var Rke=s(Sw);eir=r(Rke,"This class cannot be instantiated directly using "),Phe=n(Rke,"CODE",{});var Rst=s(Phe);oir=r(Rst,"__init__()"),Rst.forEach(t),rir=r(Rke," (throws an error)."),Rke.forEach(t),tir=i(Vl),ht=n(Vl,"DIV",{class:!0});var Wl=s(ht);m(Pw.$$.fragment,Wl),air=i(Wl),$he=n(Wl,"P",{});var Sst=s($he);nir=r(Sst,"Instantiates one of the model classes of the library (with a table question answering head) from a configuration."),Sst.forEach(t),sir=i(Wl),Ac=n(Wl,"P",{});var rV=s(Ac);lir=r(rV,`Note:
Loading a model from its configuration file does `),Ihe=n(rV,"STRONG",{});var Pst=s(Ihe);iir=r(Pst,"not"),Pst.forEach(t),dir=r(rV,` load the model weights. It only affects the
model\u2019s configuration. Use `),jhe=n(rV,"CODE",{});var $st=s(jhe);cir=r($st,"from_pretrained()"),$st.forEach(t),fir=r(rV,"to load the model weights."),rV.forEach(t),mir=i(Wl),Nhe=n(Wl,"P",{});var Ist=s(Nhe);gir=r(Ist,"Examples:"),Ist.forEach(t),hir=i(Wl),m($w.$$.fragment,Wl),Wl.forEach(t),pir=i(Vl),Fo=n(Vl,"DIV",{class:!0});var ba=s(Fo);m(Iw.$$.fragment,ba),_ir=i(ba),Dhe=n(ba,"P",{});var jst=s(Dhe);uir=r(jst,"Instantiate one of the model classes of the library (with a table question answering head) from a pretrained model."),jst.forEach(t),bir=i(ba),_n=n(ba,"P",{});var j4=s(_n);vir=r(j4,"The model class to instantiate is selected based on the "),qhe=n(j4,"CODE",{});var Nst=s(qhe);Tir=r(Nst,"model_type"),Nst.forEach(t),Fir=r(j4,` property of the config object (either
passed as an argument or loaded from `),Ghe=n(j4,"CODE",{});var Dst=s(Ghe);Cir=r(Dst,"pretrained_model_name_or_path"),Dst.forEach(t),Mir=r(j4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Ohe=n(j4,"CODE",{});var qst=s(Ohe);Eir=r(qst,"pretrained_model_name_or_path"),qst.forEach(t),yir=r(j4,":"),j4.forEach(t),wir=i(ba),Xhe=n(ba,"UL",{});var Gst=s(Xhe);pF=n(Gst,"LI",{});var kAe=s(pF);zhe=n(kAe,"STRONG",{});var Ost=s(zhe);Air=r(Ost,"tapas"),Ost.forEach(t),Lir=r(kAe," \u2014 "),_G=n(kAe,"A",{href:!0});var Xst=s(_G);Bir=r(Xst,"TFTapasForQuestionAnswering"),Xst.forEach(t),kir=r(kAe," (TAPAS model)"),kAe.forEach(t),Gst.forEach(t),xir=i(ba),Vhe=n(ba,"P",{});var zst=s(Vhe);Rir=r(zst,"Examples:"),zst.forEach(t),Sir=i(ba),m(jw.$$.fragment,ba),ba.forEach(t),Vl.forEach(t),E8e=i(d),Lc=n(d,"H2",{class:!0});var Ske=s(Lc);_F=n(Ske,"A",{id:!0,class:!0,href:!0});var Vst=s(_F);Whe=n(Vst,"SPAN",{});var Wst=s(Whe);m(Nw.$$.fragment,Wst),Wst.forEach(t),Vst.forEach(t),Pir=i(Ske),Qhe=n(Ske,"SPAN",{});var Qst=s(Qhe);$ir=r(Qst,"TFAutoModelForTokenClassification"),Qst.forEach(t),Ske.forEach(t),y8e=i(d),Cr=n(d,"DIV",{class:!0});var Ql=s(Cr);m(Dw.$$.fragment,Ql),Iir=i(Ql),Bc=n(Ql,"P",{});var tV=s(Bc);jir=r(tV,`This is a generic model class that will be instantiated as one of the model classes of the library (with a token classification head) when created
with the `),Hhe=n(tV,"CODE",{});var Hst=s(Hhe);Nir=r(Hst,"from_pretrained()"),Hst.forEach(t),Dir=r(tV,"class method or the "),Uhe=n(tV,"CODE",{});var Ust=s(Uhe);qir=r(Ust,"from_config()"),Ust.forEach(t),Gir=r(tV,`class
method.`),tV.forEach(t),Oir=i(Ql),qw=n(Ql,"P",{});var Pke=s(qw);Xir=r(Pke,"This class cannot be instantiated directly using "),Jhe=n(Pke,"CODE",{});var Jst=s(Jhe);zir=r(Jst,"__init__()"),Jst.forEach(t),Vir=r(Pke," (throws an error)."),Pke.forEach(t),Wir=i(Ql),pt=n(Ql,"DIV",{class:!0});var Hl=s(pt);m(Gw.$$.fragment,Hl),Qir=i(Hl),Yhe=n(Hl,"P",{});var Yst=s(Yhe);Hir=r(Yst,"Instantiates one of the model classes of the library (with a token classification head) from a configuration."),Yst.forEach(t),Uir=i(Hl),kc=n(Hl,"P",{});var aV=s(kc);Jir=r(aV,`Note:
Loading a model from its configuration file does `),Khe=n(aV,"STRONG",{});var Kst=s(Khe);Yir=r(Kst,"not"),Kst.forEach(t),Kir=r(aV,` load the model weights. It only affects the
model\u2019s configuration. Use `),Zhe=n(aV,"CODE",{});var Zst=s(Zhe);Zir=r(Zst,"from_pretrained()"),Zst.forEach(t),edr=r(aV,"to load the model weights."),aV.forEach(t),odr=i(Hl),epe=n(Hl,"P",{});var elt=s(epe);rdr=r(elt,"Examples:"),elt.forEach(t),tdr=i(Hl),m(Ow.$$.fragment,Hl),Hl.forEach(t),adr=i(Ql),Co=n(Ql,"DIV",{class:!0});var va=s(Co);m(Xw.$$.fragment,va),ndr=i(va),ope=n(va,"P",{});var olt=s(ope);sdr=r(olt,"Instantiate one of the model classes of the library (with a token classification head) from a pretrained model."),olt.forEach(t),ldr=i(va),un=n(va,"P",{});var N4=s(un);idr=r(N4,"The model class to instantiate is selected based on the "),rpe=n(N4,"CODE",{});var rlt=s(rpe);ddr=r(rlt,"model_type"),rlt.forEach(t),cdr=r(N4,` property of the config object (either
passed as an argument or loaded from `),tpe=n(N4,"CODE",{});var tlt=s(tpe);fdr=r(tlt,"pretrained_model_name_or_path"),tlt.forEach(t),mdr=r(N4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),ape=n(N4,"CODE",{});var alt=s(ape);gdr=r(alt,"pretrained_model_name_or_path"),alt.forEach(t),hdr=r(N4,":"),N4.forEach(t),pdr=i(va),K=n(va,"UL",{});var oe=s(K);uF=n(oe,"LI",{});var xAe=s(uF);npe=n(xAe,"STRONG",{});var nlt=s(npe);_dr=r(nlt,"albert"),nlt.forEach(t),udr=r(xAe," \u2014 "),uG=n(xAe,"A",{href:!0});var slt=s(uG);bdr=r(slt,"TFAlbertForTokenClassification"),slt.forEach(t),vdr=r(xAe," (ALBERT model)"),xAe.forEach(t),Tdr=i(oe),bF=n(oe,"LI",{});var RAe=s(bF);spe=n(RAe,"STRONG",{});var llt=s(spe);Fdr=r(llt,"bert"),llt.forEach(t),Cdr=r(RAe," \u2014 "),bG=n(RAe,"A",{href:!0});var ilt=s(bG);Mdr=r(ilt,"TFBertForTokenClassification"),ilt.forEach(t),Edr=r(RAe," (BERT model)"),RAe.forEach(t),ydr=i(oe),vF=n(oe,"LI",{});var SAe=s(vF);lpe=n(SAe,"STRONG",{});var dlt=s(lpe);wdr=r(dlt,"camembert"),dlt.forEach(t),Adr=r(SAe," \u2014 "),vG=n(SAe,"A",{href:!0});var clt=s(vG);Ldr=r(clt,"TFCamembertForTokenClassification"),clt.forEach(t),Bdr=r(SAe," (CamemBERT model)"),SAe.forEach(t),kdr=i(oe),TF=n(oe,"LI",{});var PAe=s(TF);ipe=n(PAe,"STRONG",{});var flt=s(ipe);xdr=r(flt,"convbert"),flt.forEach(t),Rdr=r(PAe," \u2014 "),TG=n(PAe,"A",{href:!0});var mlt=s(TG);Sdr=r(mlt,"TFConvBertForTokenClassification"),mlt.forEach(t),Pdr=r(PAe," (ConvBERT model)"),PAe.forEach(t),$dr=i(oe),FF=n(oe,"LI",{});var $Ae=s(FF);dpe=n($Ae,"STRONG",{});var glt=s(dpe);Idr=r(glt,"deberta"),glt.forEach(t),jdr=r($Ae," \u2014 "),FG=n($Ae,"A",{href:!0});var hlt=s(FG);Ndr=r(hlt,"TFDebertaForTokenClassification"),hlt.forEach(t),Ddr=r($Ae," (DeBERTa model)"),$Ae.forEach(t),qdr=i(oe),CF=n(oe,"LI",{});var IAe=s(CF);cpe=n(IAe,"STRONG",{});var plt=s(cpe);Gdr=r(plt,"deberta-v2"),plt.forEach(t),Odr=r(IAe," \u2014 "),CG=n(IAe,"A",{href:!0});var _lt=s(CG);Xdr=r(_lt,"TFDebertaV2ForTokenClassification"),_lt.forEach(t),zdr=r(IAe," (DeBERTa-v2 model)"),IAe.forEach(t),Vdr=i(oe),MF=n(oe,"LI",{});var jAe=s(MF);fpe=n(jAe,"STRONG",{});var ult=s(fpe);Wdr=r(ult,"distilbert"),ult.forEach(t),Qdr=r(jAe," \u2014 "),MG=n(jAe,"A",{href:!0});var blt=s(MG);Hdr=r(blt,"TFDistilBertForTokenClassification"),blt.forEach(t),Udr=r(jAe," (DistilBERT model)"),jAe.forEach(t),Jdr=i(oe),EF=n(oe,"LI",{});var NAe=s(EF);mpe=n(NAe,"STRONG",{});var vlt=s(mpe);Ydr=r(vlt,"electra"),vlt.forEach(t),Kdr=r(NAe," \u2014 "),EG=n(NAe,"A",{href:!0});var Tlt=s(EG);Zdr=r(Tlt,"TFElectraForTokenClassification"),Tlt.forEach(t),ecr=r(NAe," (ELECTRA model)"),NAe.forEach(t),ocr=i(oe),yF=n(oe,"LI",{});var DAe=s(yF);gpe=n(DAe,"STRONG",{});var Flt=s(gpe);rcr=r(Flt,"flaubert"),Flt.forEach(t),tcr=r(DAe," \u2014 "),yG=n(DAe,"A",{href:!0});var Clt=s(yG);acr=r(Clt,"TFFlaubertForTokenClassification"),Clt.forEach(t),ncr=r(DAe," (FlauBERT model)"),DAe.forEach(t),scr=i(oe),wF=n(oe,"LI",{});var qAe=s(wF);hpe=n(qAe,"STRONG",{});var Mlt=s(hpe);lcr=r(Mlt,"funnel"),Mlt.forEach(t),icr=r(qAe," \u2014 "),wG=n(qAe,"A",{href:!0});var Elt=s(wG);dcr=r(Elt,"TFFunnelForTokenClassification"),Elt.forEach(t),ccr=r(qAe," (Funnel Transformer model)"),qAe.forEach(t),fcr=i(oe),AF=n(oe,"LI",{});var GAe=s(AF);ppe=n(GAe,"STRONG",{});var ylt=s(ppe);mcr=r(ylt,"layoutlm"),ylt.forEach(t),gcr=r(GAe," \u2014 "),AG=n(GAe,"A",{href:!0});var wlt=s(AG);hcr=r(wlt,"TFLayoutLMForTokenClassification"),wlt.forEach(t),pcr=r(GAe," (LayoutLM model)"),GAe.forEach(t),_cr=i(oe),LF=n(oe,"LI",{});var OAe=s(LF);_pe=n(OAe,"STRONG",{});var Alt=s(_pe);ucr=r(Alt,"longformer"),Alt.forEach(t),bcr=r(OAe," \u2014 "),LG=n(OAe,"A",{href:!0});var Llt=s(LG);vcr=r(Llt,"TFLongformerForTokenClassification"),Llt.forEach(t),Tcr=r(OAe," (Longformer model)"),OAe.forEach(t),Fcr=i(oe),BF=n(oe,"LI",{});var XAe=s(BF);upe=n(XAe,"STRONG",{});var Blt=s(upe);Ccr=r(Blt,"mobilebert"),Blt.forEach(t),Mcr=r(XAe," \u2014 "),BG=n(XAe,"A",{href:!0});var klt=s(BG);Ecr=r(klt,"TFMobileBertForTokenClassification"),klt.forEach(t),ycr=r(XAe," (MobileBERT model)"),XAe.forEach(t),wcr=i(oe),kF=n(oe,"LI",{});var zAe=s(kF);bpe=n(zAe,"STRONG",{});var xlt=s(bpe);Acr=r(xlt,"mpnet"),xlt.forEach(t),Lcr=r(zAe," \u2014 "),kG=n(zAe,"A",{href:!0});var Rlt=s(kG);Bcr=r(Rlt,"TFMPNetForTokenClassification"),Rlt.forEach(t),kcr=r(zAe," (MPNet model)"),zAe.forEach(t),xcr=i(oe),xF=n(oe,"LI",{});var VAe=s(xF);vpe=n(VAe,"STRONG",{});var Slt=s(vpe);Rcr=r(Slt,"rembert"),Slt.forEach(t),Scr=r(VAe," \u2014 "),xG=n(VAe,"A",{href:!0});var Plt=s(xG);Pcr=r(Plt,"TFRemBertForTokenClassification"),Plt.forEach(t),$cr=r(VAe," (RemBERT model)"),VAe.forEach(t),Icr=i(oe),RF=n(oe,"LI",{});var WAe=s(RF);Tpe=n(WAe,"STRONG",{});var $lt=s(Tpe);jcr=r($lt,"roberta"),$lt.forEach(t),Ncr=r(WAe," \u2014 "),RG=n(WAe,"A",{href:!0});var Ilt=s(RG);Dcr=r(Ilt,"TFRobertaForTokenClassification"),Ilt.forEach(t),qcr=r(WAe," (RoBERTa model)"),WAe.forEach(t),Gcr=i(oe),SF=n(oe,"LI",{});var QAe=s(SF);Fpe=n(QAe,"STRONG",{});var jlt=s(Fpe);Ocr=r(jlt,"roformer"),jlt.forEach(t),Xcr=r(QAe," \u2014 "),SG=n(QAe,"A",{href:!0});var Nlt=s(SG);zcr=r(Nlt,"TFRoFormerForTokenClassification"),Nlt.forEach(t),Vcr=r(QAe," (RoFormer model)"),QAe.forEach(t),Wcr=i(oe),PF=n(oe,"LI",{});var HAe=s(PF);Cpe=n(HAe,"STRONG",{});var Dlt=s(Cpe);Qcr=r(Dlt,"xlm"),Dlt.forEach(t),Hcr=r(HAe," \u2014 "),PG=n(HAe,"A",{href:!0});var qlt=s(PG);Ucr=r(qlt,"TFXLMForTokenClassification"),qlt.forEach(t),Jcr=r(HAe," (XLM model)"),HAe.forEach(t),Ycr=i(oe),$F=n(oe,"LI",{});var UAe=s($F);Mpe=n(UAe,"STRONG",{});var Glt=s(Mpe);Kcr=r(Glt,"xlm-roberta"),Glt.forEach(t),Zcr=r(UAe," \u2014 "),$G=n(UAe,"A",{href:!0});var Olt=s($G);efr=r(Olt,"TFXLMRobertaForTokenClassification"),Olt.forEach(t),ofr=r(UAe," (XLM-RoBERTa model)"),UAe.forEach(t),rfr=i(oe),IF=n(oe,"LI",{});var JAe=s(IF);Epe=n(JAe,"STRONG",{});var Xlt=s(Epe);tfr=r(Xlt,"xlnet"),Xlt.forEach(t),afr=r(JAe," \u2014 "),IG=n(JAe,"A",{href:!0});var zlt=s(IG);nfr=r(zlt,"TFXLNetForTokenClassification"),zlt.forEach(t),sfr=r(JAe," (XLNet model)"),JAe.forEach(t),oe.forEach(t),lfr=i(va),ype=n(va,"P",{});var Vlt=s(ype);ifr=r(Vlt,"Examples:"),Vlt.forEach(t),dfr=i(va),m(zw.$$.fragment,va),va.forEach(t),Ql.forEach(t),w8e=i(d),xc=n(d,"H2",{class:!0});var $ke=s(xc);jF=n($ke,"A",{id:!0,class:!0,href:!0});var Wlt=s(jF);wpe=n(Wlt,"SPAN",{});var Qlt=s(wpe);m(Vw.$$.fragment,Qlt),Qlt.forEach(t),Wlt.forEach(t),cfr=i($ke),Ape=n($ke,"SPAN",{});var Hlt=s(Ape);ffr=r(Hlt,"TFAutoModelForQuestionAnswering"),Hlt.forEach(t),$ke.forEach(t),A8e=i(d),Mr=n(d,"DIV",{class:!0});var Ul=s(Mr);m(Ww.$$.fragment,Ul),mfr=i(Ul),Rc=n(Ul,"P",{});var nV=s(Rc);gfr=r(nV,`This is a generic model class that will be instantiated as one of the model classes of the library (with a question answering head) when created
with the `),Lpe=n(nV,"CODE",{});var Ult=s(Lpe);hfr=r(Ult,"from_pretrained()"),Ult.forEach(t),pfr=r(nV,"class method or the "),Bpe=n(nV,"CODE",{});var Jlt=s(Bpe);_fr=r(Jlt,"from_config()"),Jlt.forEach(t),ufr=r(nV,`class
method.`),nV.forEach(t),bfr=i(Ul),Qw=n(Ul,"P",{});var Ike=s(Qw);vfr=r(Ike,"This class cannot be instantiated directly using "),kpe=n(Ike,"CODE",{});var Ylt=s(kpe);Tfr=r(Ylt,"__init__()"),Ylt.forEach(t),Ffr=r(Ike," (throws an error)."),Ike.forEach(t),Cfr=i(Ul),_t=n(Ul,"DIV",{class:!0});var Jl=s(_t);m(Hw.$$.fragment,Jl),Mfr=i(Jl),xpe=n(Jl,"P",{});var Klt=s(xpe);Efr=r(Klt,"Instantiates one of the model classes of the library (with a question answering head) from a configuration."),Klt.forEach(t),yfr=i(Jl),Sc=n(Jl,"P",{});var sV=s(Sc);wfr=r(sV,`Note:
Loading a model from its configuration file does `),Rpe=n(sV,"STRONG",{});var Zlt=s(Rpe);Afr=r(Zlt,"not"),Zlt.forEach(t),Lfr=r(sV,` load the model weights. It only affects the
model\u2019s configuration. Use `),Spe=n(sV,"CODE",{});var eit=s(Spe);Bfr=r(eit,"from_pretrained()"),eit.forEach(t),kfr=r(sV,"to load the model weights."),sV.forEach(t),xfr=i(Jl),Ppe=n(Jl,"P",{});var oit=s(Ppe);Rfr=r(oit,"Examples:"),oit.forEach(t),Sfr=i(Jl),m(Uw.$$.fragment,Jl),Jl.forEach(t),Pfr=i(Ul),Mo=n(Ul,"DIV",{class:!0});var Ta=s(Mo);m(Jw.$$.fragment,Ta),$fr=i(Ta),$pe=n(Ta,"P",{});var rit=s($pe);Ifr=r(rit,"Instantiate one of the model classes of the library (with a question answering head) from a pretrained model."),rit.forEach(t),jfr=i(Ta),bn=n(Ta,"P",{});var D4=s(bn);Nfr=r(D4,"The model class to instantiate is selected based on the "),Ipe=n(D4,"CODE",{});var tit=s(Ipe);Dfr=r(tit,"model_type"),tit.forEach(t),qfr=r(D4,` property of the config object (either
passed as an argument or loaded from `),jpe=n(D4,"CODE",{});var ait=s(jpe);Gfr=r(ait,"pretrained_model_name_or_path"),ait.forEach(t),Ofr=r(D4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Npe=n(D4,"CODE",{});var nit=s(Npe);Xfr=r(nit,"pretrained_model_name_or_path"),nit.forEach(t),zfr=r(D4,":"),D4.forEach(t),Vfr=i(Ta),Z=n(Ta,"UL",{});var re=s(Z);NF=n(re,"LI",{});var YAe=s(NF);Dpe=n(YAe,"STRONG",{});var sit=s(Dpe);Wfr=r(sit,"albert"),sit.forEach(t),Qfr=r(YAe," \u2014 "),jG=n(YAe,"A",{href:!0});var lit=s(jG);Hfr=r(lit,"TFAlbertForQuestionAnswering"),lit.forEach(t),Ufr=r(YAe," (ALBERT model)"),YAe.forEach(t),Jfr=i(re),DF=n(re,"LI",{});var KAe=s(DF);qpe=n(KAe,"STRONG",{});var iit=s(qpe);Yfr=r(iit,"bert"),iit.forEach(t),Kfr=r(KAe," \u2014 "),NG=n(KAe,"A",{href:!0});var dit=s(NG);Zfr=r(dit,"TFBertForQuestionAnswering"),dit.forEach(t),emr=r(KAe," (BERT model)"),KAe.forEach(t),omr=i(re),qF=n(re,"LI",{});var ZAe=s(qF);Gpe=n(ZAe,"STRONG",{});var cit=s(Gpe);rmr=r(cit,"camembert"),cit.forEach(t),tmr=r(ZAe," \u2014 "),DG=n(ZAe,"A",{href:!0});var fit=s(DG);amr=r(fit,"TFCamembertForQuestionAnswering"),fit.forEach(t),nmr=r(ZAe," (CamemBERT model)"),ZAe.forEach(t),smr=i(re),GF=n(re,"LI",{});var e6e=s(GF);Ope=n(e6e,"STRONG",{});var mit=s(Ope);lmr=r(mit,"convbert"),mit.forEach(t),imr=r(e6e," \u2014 "),qG=n(e6e,"A",{href:!0});var git=s(qG);dmr=r(git,"TFConvBertForQuestionAnswering"),git.forEach(t),cmr=r(e6e," (ConvBERT model)"),e6e.forEach(t),fmr=i(re),OF=n(re,"LI",{});var o6e=s(OF);Xpe=n(o6e,"STRONG",{});var hit=s(Xpe);mmr=r(hit,"deberta"),hit.forEach(t),gmr=r(o6e," \u2014 "),GG=n(o6e,"A",{href:!0});var pit=s(GG);hmr=r(pit,"TFDebertaForQuestionAnswering"),pit.forEach(t),pmr=r(o6e," (DeBERTa model)"),o6e.forEach(t),_mr=i(re),XF=n(re,"LI",{});var r6e=s(XF);zpe=n(r6e,"STRONG",{});var _it=s(zpe);umr=r(_it,"deberta-v2"),_it.forEach(t),bmr=r(r6e," \u2014 "),OG=n(r6e,"A",{href:!0});var uit=s(OG);vmr=r(uit,"TFDebertaV2ForQuestionAnswering"),uit.forEach(t),Tmr=r(r6e," (DeBERTa-v2 model)"),r6e.forEach(t),Fmr=i(re),zF=n(re,"LI",{});var t6e=s(zF);Vpe=n(t6e,"STRONG",{});var bit=s(Vpe);Cmr=r(bit,"distilbert"),bit.forEach(t),Mmr=r(t6e," \u2014 "),XG=n(t6e,"A",{href:!0});var vit=s(XG);Emr=r(vit,"TFDistilBertForQuestionAnswering"),vit.forEach(t),ymr=r(t6e," (DistilBERT model)"),t6e.forEach(t),wmr=i(re),VF=n(re,"LI",{});var a6e=s(VF);Wpe=n(a6e,"STRONG",{});var Tit=s(Wpe);Amr=r(Tit,"electra"),Tit.forEach(t),Lmr=r(a6e," \u2014 "),zG=n(a6e,"A",{href:!0});var Fit=s(zG);Bmr=r(Fit,"TFElectraForQuestionAnswering"),Fit.forEach(t),kmr=r(a6e," (ELECTRA model)"),a6e.forEach(t),xmr=i(re),WF=n(re,"LI",{});var n6e=s(WF);Qpe=n(n6e,"STRONG",{});var Cit=s(Qpe);Rmr=r(Cit,"flaubert"),Cit.forEach(t),Smr=r(n6e," \u2014 "),VG=n(n6e,"A",{href:!0});var Mit=s(VG);Pmr=r(Mit,"TFFlaubertForQuestionAnsweringSimple"),Mit.forEach(t),$mr=r(n6e," (FlauBERT model)"),n6e.forEach(t),Imr=i(re),QF=n(re,"LI",{});var s6e=s(QF);Hpe=n(s6e,"STRONG",{});var Eit=s(Hpe);jmr=r(Eit,"funnel"),Eit.forEach(t),Nmr=r(s6e," \u2014 "),WG=n(s6e,"A",{href:!0});var yit=s(WG);Dmr=r(yit,"TFFunnelForQuestionAnswering"),yit.forEach(t),qmr=r(s6e," (Funnel Transformer model)"),s6e.forEach(t),Gmr=i(re),HF=n(re,"LI",{});var l6e=s(HF);Upe=n(l6e,"STRONG",{});var wit=s(Upe);Omr=r(wit,"longformer"),wit.forEach(t),Xmr=r(l6e," \u2014 "),QG=n(l6e,"A",{href:!0});var Ait=s(QG);zmr=r(Ait,"TFLongformerForQuestionAnswering"),Ait.forEach(t),Vmr=r(l6e," (Longformer model)"),l6e.forEach(t),Wmr=i(re),UF=n(re,"LI",{});var i6e=s(UF);Jpe=n(i6e,"STRONG",{});var Lit=s(Jpe);Qmr=r(Lit,"mobilebert"),Lit.forEach(t),Hmr=r(i6e," \u2014 "),HG=n(i6e,"A",{href:!0});var Bit=s(HG);Umr=r(Bit,"TFMobileBertForQuestionAnswering"),Bit.forEach(t),Jmr=r(i6e," (MobileBERT model)"),i6e.forEach(t),Ymr=i(re),JF=n(re,"LI",{});var d6e=s(JF);Ype=n(d6e,"STRONG",{});var kit=s(Ype);Kmr=r(kit,"mpnet"),kit.forEach(t),Zmr=r(d6e," \u2014 "),UG=n(d6e,"A",{href:!0});var xit=s(UG);egr=r(xit,"TFMPNetForQuestionAnswering"),xit.forEach(t),ogr=r(d6e," (MPNet model)"),d6e.forEach(t),rgr=i(re),YF=n(re,"LI",{});var c6e=s(YF);Kpe=n(c6e,"STRONG",{});var Rit=s(Kpe);tgr=r(Rit,"rembert"),Rit.forEach(t),agr=r(c6e," \u2014 "),JG=n(c6e,"A",{href:!0});var Sit=s(JG);ngr=r(Sit,"TFRemBertForQuestionAnswering"),Sit.forEach(t),sgr=r(c6e," (RemBERT model)"),c6e.forEach(t),lgr=i(re),KF=n(re,"LI",{});var f6e=s(KF);Zpe=n(f6e,"STRONG",{});var Pit=s(Zpe);igr=r(Pit,"roberta"),Pit.forEach(t),dgr=r(f6e," \u2014 "),YG=n(f6e,"A",{href:!0});var $it=s(YG);cgr=r($it,"TFRobertaForQuestionAnswering"),$it.forEach(t),fgr=r(f6e," (RoBERTa model)"),f6e.forEach(t),mgr=i(re),ZF=n(re,"LI",{});var m6e=s(ZF);e_e=n(m6e,"STRONG",{});var Iit=s(e_e);ggr=r(Iit,"roformer"),Iit.forEach(t),hgr=r(m6e," \u2014 "),KG=n(m6e,"A",{href:!0});var jit=s(KG);pgr=r(jit,"TFRoFormerForQuestionAnswering"),jit.forEach(t),_gr=r(m6e," (RoFormer model)"),m6e.forEach(t),ugr=i(re),e9=n(re,"LI",{});var g6e=s(e9);o_e=n(g6e,"STRONG",{});var Nit=s(o_e);bgr=r(Nit,"xlm"),Nit.forEach(t),vgr=r(g6e," \u2014 "),ZG=n(g6e,"A",{href:!0});var Dit=s(ZG);Tgr=r(Dit,"TFXLMForQuestionAnsweringSimple"),Dit.forEach(t),Fgr=r(g6e," (XLM model)"),g6e.forEach(t),Cgr=i(re),o9=n(re,"LI",{});var h6e=s(o9);r_e=n(h6e,"STRONG",{});var qit=s(r_e);Mgr=r(qit,"xlm-roberta"),qit.forEach(t),Egr=r(h6e," \u2014 "),eO=n(h6e,"A",{href:!0});var Git=s(eO);ygr=r(Git,"TFXLMRobertaForQuestionAnswering"),Git.forEach(t),wgr=r(h6e," (XLM-RoBERTa model)"),h6e.forEach(t),Agr=i(re),r9=n(re,"LI",{});var p6e=s(r9);t_e=n(p6e,"STRONG",{});var Oit=s(t_e);Lgr=r(Oit,"xlnet"),Oit.forEach(t),Bgr=r(p6e," \u2014 "),oO=n(p6e,"A",{href:!0});var Xit=s(oO);kgr=r(Xit,"TFXLNetForQuestionAnsweringSimple"),Xit.forEach(t),xgr=r(p6e," (XLNet model)"),p6e.forEach(t),re.forEach(t),Rgr=i(Ta),a_e=n(Ta,"P",{});var zit=s(a_e);Sgr=r(zit,"Examples:"),zit.forEach(t),Pgr=i(Ta),m(Yw.$$.fragment,Ta),Ta.forEach(t),Ul.forEach(t),L8e=i(d),Pc=n(d,"H2",{class:!0});var jke=s(Pc);t9=n(jke,"A",{id:!0,class:!0,href:!0});var Vit=s(t9);n_e=n(Vit,"SPAN",{});var Wit=s(n_e);m(Kw.$$.fragment,Wit),Wit.forEach(t),Vit.forEach(t),$gr=i(jke),s_e=n(jke,"SPAN",{});var Qit=s(s_e);Igr=r(Qit,"TFAutoModelForVision2Seq"),Qit.forEach(t),jke.forEach(t),B8e=i(d),Er=n(d,"DIV",{class:!0});var Yl=s(Er);m(Zw.$$.fragment,Yl),jgr=i(Yl),$c=n(Yl,"P",{});var lV=s($c);Ngr=r(lV,`This is a generic model class that will be instantiated as one of the model classes of the library (with a vision-to-text modeling head) when created
with the `),l_e=n(lV,"CODE",{});var Hit=s(l_e);Dgr=r(Hit,"from_pretrained()"),Hit.forEach(t),qgr=r(lV,"class method or the "),i_e=n(lV,"CODE",{});var Uit=s(i_e);Ggr=r(Uit,"from_config()"),Uit.forEach(t),Ogr=r(lV,`class
method.`),lV.forEach(t),Xgr=i(Yl),eA=n(Yl,"P",{});var Nke=s(eA);zgr=r(Nke,"This class cannot be instantiated directly using "),d_e=n(Nke,"CODE",{});var Jit=s(d_e);Vgr=r(Jit,"__init__()"),Jit.forEach(t),Wgr=r(Nke," (throws an error)."),Nke.forEach(t),Qgr=i(Yl),ut=n(Yl,"DIV",{class:!0});var Kl=s(ut);m(oA.$$.fragment,Kl),Hgr=i(Kl),c_e=n(Kl,"P",{});var Yit=s(c_e);Ugr=r(Yit,"Instantiates one of the model classes of the library (with a vision-to-text modeling head) from a configuration."),Yit.forEach(t),Jgr=i(Kl),Ic=n(Kl,"P",{});var iV=s(Ic);Ygr=r(iV,`Note:
Loading a model from its configuration file does `),f_e=n(iV,"STRONG",{});var Kit=s(f_e);Kgr=r(Kit,"not"),Kit.forEach(t),Zgr=r(iV,` load the model weights. It only affects the
model\u2019s configuration. Use `),m_e=n(iV,"CODE",{});var Zit=s(m_e);ehr=r(Zit,"from_pretrained()"),Zit.forEach(t),ohr=r(iV,"to load the model weights."),iV.forEach(t),rhr=i(Kl),g_e=n(Kl,"P",{});var edt=s(g_e);thr=r(edt,"Examples:"),edt.forEach(t),ahr=i(Kl),m(rA.$$.fragment,Kl),Kl.forEach(t),nhr=i(Yl),Eo=n(Yl,"DIV",{class:!0});var Fa=s(Eo);m(tA.$$.fragment,Fa),shr=i(Fa),h_e=n(Fa,"P",{});var odt=s(h_e);lhr=r(odt,"Instantiate one of the model classes of the library (with a vision-to-text modeling head) from a pretrained model."),odt.forEach(t),ihr=i(Fa),vn=n(Fa,"P",{});var q4=s(vn);dhr=r(q4,"The model class to instantiate is selected based on the "),p_e=n(q4,"CODE",{});var rdt=s(p_e);chr=r(rdt,"model_type"),rdt.forEach(t),fhr=r(q4,` property of the config object (either
passed as an argument or loaded from `),__e=n(q4,"CODE",{});var tdt=s(__e);mhr=r(tdt,"pretrained_model_name_or_path"),tdt.forEach(t),ghr=r(q4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),u_e=n(q4,"CODE",{});var adt=s(u_e);hhr=r(adt,"pretrained_model_name_or_path"),adt.forEach(t),phr=r(q4,":"),q4.forEach(t),_hr=i(Fa),b_e=n(Fa,"UL",{});var ndt=s(b_e);a9=n(ndt,"LI",{});var _6e=s(a9);v_e=n(_6e,"STRONG",{});var sdt=s(v_e);uhr=r(sdt,"vision-encoder-decoder"),sdt.forEach(t),bhr=r(_6e," \u2014 "),rO=n(_6e,"A",{href:!0});var ldt=s(rO);vhr=r(ldt,"TFVisionEncoderDecoderModel"),ldt.forEach(t),Thr=r(_6e," (Vision Encoder decoder model)"),_6e.forEach(t),ndt.forEach(t),Fhr=i(Fa),T_e=n(Fa,"P",{});var idt=s(T_e);Chr=r(idt,"Examples:"),idt.forEach(t),Mhr=i(Fa),m(aA.$$.fragment,Fa),Fa.forEach(t),Yl.forEach(t),k8e=i(d),jc=n(d,"H2",{class:!0});var Dke=s(jc);n9=n(Dke,"A",{id:!0,class:!0,href:!0});var ddt=s(n9);F_e=n(ddt,"SPAN",{});var cdt=s(F_e);m(nA.$$.fragment,cdt),cdt.forEach(t),ddt.forEach(t),Ehr=i(Dke),C_e=n(Dke,"SPAN",{});var fdt=s(C_e);yhr=r(fdt,"TFAutoModelForSpeechSeq2Seq"),fdt.forEach(t),Dke.forEach(t),x8e=i(d),yr=n(d,"DIV",{class:!0});var Zl=s(yr);m(sA.$$.fragment,Zl),whr=i(Zl),Nc=n(Zl,"P",{});var dV=s(Nc);Ahr=r(dV,`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) when created
with the `),M_e=n(dV,"CODE",{});var mdt=s(M_e);Lhr=r(mdt,"from_pretrained()"),mdt.forEach(t),Bhr=r(dV,"class method or the "),E_e=n(dV,"CODE",{});var gdt=s(E_e);khr=r(gdt,"from_config()"),gdt.forEach(t),xhr=r(dV,`class
method.`),dV.forEach(t),Rhr=i(Zl),lA=n(Zl,"P",{});var qke=s(lA);Shr=r(qke,"This class cannot be instantiated directly using "),y_e=n(qke,"CODE",{});var hdt=s(y_e);Phr=r(hdt,"__init__()"),hdt.forEach(t),$hr=r(qke," (throws an error)."),qke.forEach(t),Ihr=i(Zl),bt=n(Zl,"DIV",{class:!0});var ei=s(bt);m(iA.$$.fragment,ei),jhr=i(ei),w_e=n(ei,"P",{});var pdt=s(w_e);Nhr=r(pdt,"Instantiates one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) from a configuration."),pdt.forEach(t),Dhr=i(ei),Dc=n(ei,"P",{});var cV=s(Dc);qhr=r(cV,`Note:
Loading a model from its configuration file does `),A_e=n(cV,"STRONG",{});var _dt=s(A_e);Ghr=r(_dt,"not"),_dt.forEach(t),Ohr=r(cV,` load the model weights. It only affects the
model\u2019s configuration. Use `),L_e=n(cV,"CODE",{});var udt=s(L_e);Xhr=r(udt,"from_pretrained()"),udt.forEach(t),zhr=r(cV,"to load the model weights."),cV.forEach(t),Vhr=i(ei),B_e=n(ei,"P",{});var bdt=s(B_e);Whr=r(bdt,"Examples:"),bdt.forEach(t),Qhr=i(ei),m(dA.$$.fragment,ei),ei.forEach(t),Hhr=i(Zl),yo=n(Zl,"DIV",{class:!0});var Ca=s(yo);m(cA.$$.fragment,Ca),Uhr=i(Ca),k_e=n(Ca,"P",{});var vdt=s(k_e);Jhr=r(vdt,"Instantiate one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) from a pretrained model."),vdt.forEach(t),Yhr=i(Ca),Tn=n(Ca,"P",{});var G4=s(Tn);Khr=r(G4,"The model class to instantiate is selected based on the "),x_e=n(G4,"CODE",{});var Tdt=s(x_e);Zhr=r(Tdt,"model_type"),Tdt.forEach(t),epr=r(G4,` property of the config object (either
passed as an argument or loaded from `),R_e=n(G4,"CODE",{});var Fdt=s(R_e);opr=r(Fdt,"pretrained_model_name_or_path"),Fdt.forEach(t),rpr=r(G4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),S_e=n(G4,"CODE",{});var Cdt=s(S_e);tpr=r(Cdt,"pretrained_model_name_or_path"),Cdt.forEach(t),apr=r(G4,":"),G4.forEach(t),npr=i(Ca),P_e=n(Ca,"UL",{});var Mdt=s(P_e);s9=n(Mdt,"LI",{});var u6e=s(s9);$_e=n(u6e,"STRONG",{});var Edt=s($_e);spr=r(Edt,"speech_to_text"),Edt.forEach(t),lpr=r(u6e," \u2014 "),tO=n(u6e,"A",{href:!0});var ydt=s(tO);ipr=r(ydt,"TFSpeech2TextForConditionalGeneration"),ydt.forEach(t),dpr=r(u6e," (Speech2Text model)"),u6e.forEach(t),Mdt.forEach(t),cpr=i(Ca),I_e=n(Ca,"P",{});var wdt=s(I_e);fpr=r(wdt,"Examples:"),wdt.forEach(t),mpr=i(Ca),m(fA.$$.fragment,Ca),Ca.forEach(t),Zl.forEach(t),R8e=i(d),qc=n(d,"H2",{class:!0});var Gke=s(qc);l9=n(Gke,"A",{id:!0,class:!0,href:!0});var Adt=s(l9);j_e=n(Adt,"SPAN",{});var Ldt=s(j_e);m(mA.$$.fragment,Ldt),Ldt.forEach(t),Adt.forEach(t),gpr=i(Gke),N_e=n(Gke,"SPAN",{});var Bdt=s(N_e);hpr=r(Bdt,"FlaxAutoModel"),Bdt.forEach(t),Gke.forEach(t),S8e=i(d),wr=n(d,"DIV",{class:!0});var oi=s(wr);m(gA.$$.fragment,oi),ppr=i(oi),Gc=n(oi,"P",{});var fV=s(Gc);_pr=r(fV,`This is a generic model class that will be instantiated as one of the base model classes of the library when created
with the `),D_e=n(fV,"CODE",{});var kdt=s(D_e);upr=r(kdt,"from_pretrained()"),kdt.forEach(t),bpr=r(fV,"class method or the "),q_e=n(fV,"CODE",{});var xdt=s(q_e);vpr=r(xdt,"from_config()"),xdt.forEach(t),Tpr=r(fV,`class
method.`),fV.forEach(t),Fpr=i(oi),hA=n(oi,"P",{});var Oke=s(hA);Cpr=r(Oke,"This class cannot be instantiated directly using "),G_e=n(Oke,"CODE",{});var Rdt=s(G_e);Mpr=r(Rdt,"__init__()"),Rdt.forEach(t),Epr=r(Oke," (throws an error)."),Oke.forEach(t),ypr=i(oi),vt=n(oi,"DIV",{class:!0});var ri=s(vt);m(pA.$$.fragment,ri),wpr=i(ri),O_e=n(ri,"P",{});var Sdt=s(O_e);Apr=r(Sdt,"Instantiates one of the base model classes of the library from a configuration."),Sdt.forEach(t),Lpr=i(ri),Oc=n(ri,"P",{});var mV=s(Oc);Bpr=r(mV,`Note:
Loading a model from its configuration file does `),X_e=n(mV,"STRONG",{});var Pdt=s(X_e);kpr=r(Pdt,"not"),Pdt.forEach(t),xpr=r(mV,` load the model weights. It only affects the
model\u2019s configuration. Use `),z_e=n(mV,"CODE",{});var $dt=s(z_e);Rpr=r($dt,"from_pretrained()"),$dt.forEach(t),Spr=r(mV,"to load the model weights."),mV.forEach(t),Ppr=i(ri),V_e=n(ri,"P",{});var Idt=s(V_e);$pr=r(Idt,"Examples:"),Idt.forEach(t),Ipr=i(ri),m(_A.$$.fragment,ri),ri.forEach(t),jpr=i(oi),wo=n(oi,"DIV",{class:!0});var Ma=s(wo);m(uA.$$.fragment,Ma),Npr=i(Ma),W_e=n(Ma,"P",{});var jdt=s(W_e);Dpr=r(jdt,"Instantiate one of the base model classes of the library from a pretrained model."),jdt.forEach(t),qpr=i(Ma),Fn=n(Ma,"P",{});var O4=s(Fn);Gpr=r(O4,"The model class to instantiate is selected based on the "),Q_e=n(O4,"CODE",{});var Ndt=s(Q_e);Opr=r(Ndt,"model_type"),Ndt.forEach(t),Xpr=r(O4,` property of the config object (either
passed as an argument or loaded from `),H_e=n(O4,"CODE",{});var Ddt=s(H_e);zpr=r(Ddt,"pretrained_model_name_or_path"),Ddt.forEach(t),Vpr=r(O4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),U_e=n(O4,"CODE",{});var qdt=s(U_e);Wpr=r(qdt,"pretrained_model_name_or_path"),qdt.forEach(t),Qpr=r(O4,":"),O4.forEach(t),Hpr=i(Ma),V=n(Ma,"UL",{});var Q=s(V);i9=n(Q,"LI",{});var b6e=s(i9);J_e=n(b6e,"STRONG",{});var Gdt=s(J_e);Upr=r(Gdt,"albert"),Gdt.forEach(t),Jpr=r(b6e," \u2014 "),aO=n(b6e,"A",{href:!0});var Odt=s(aO);Ypr=r(Odt,"FlaxAlbertModel"),Odt.forEach(t),Kpr=r(b6e," (ALBERT model)"),b6e.forEach(t),Zpr=i(Q),d9=n(Q,"LI",{});var v6e=s(d9);Y_e=n(v6e,"STRONG",{});var Xdt=s(Y_e);e_r=r(Xdt,"bart"),Xdt.forEach(t),o_r=r(v6e," \u2014 "),nO=n(v6e,"A",{href:!0});var zdt=s(nO);r_r=r(zdt,"FlaxBartModel"),zdt.forEach(t),t_r=r(v6e," (BART model)"),v6e.forEach(t),a_r=i(Q),c9=n(Q,"LI",{});var T6e=s(c9);K_e=n(T6e,"STRONG",{});var Vdt=s(K_e);n_r=r(Vdt,"beit"),Vdt.forEach(t),s_r=r(T6e," \u2014 "),sO=n(T6e,"A",{href:!0});var Wdt=s(sO);l_r=r(Wdt,"FlaxBeitModel"),Wdt.forEach(t),i_r=r(T6e," (BEiT model)"),T6e.forEach(t),d_r=i(Q),f9=n(Q,"LI",{});var F6e=s(f9);Z_e=n(F6e,"STRONG",{});var Qdt=s(Z_e);c_r=r(Qdt,"bert"),Qdt.forEach(t),f_r=r(F6e," \u2014 "),lO=n(F6e,"A",{href:!0});var Hdt=s(lO);m_r=r(Hdt,"FlaxBertModel"),Hdt.forEach(t),g_r=r(F6e," (BERT model)"),F6e.forEach(t),h_r=i(Q),m9=n(Q,"LI",{});var C6e=s(m9);eue=n(C6e,"STRONG",{});var Udt=s(eue);p_r=r(Udt,"big_bird"),Udt.forEach(t),__r=r(C6e," \u2014 "),iO=n(C6e,"A",{href:!0});var Jdt=s(iO);u_r=r(Jdt,"FlaxBigBirdModel"),Jdt.forEach(t),b_r=r(C6e," (BigBird model)"),C6e.forEach(t),v_r=i(Q),g9=n(Q,"LI",{});var M6e=s(g9);oue=n(M6e,"STRONG",{});var Ydt=s(oue);T_r=r(Ydt,"blenderbot"),Ydt.forEach(t),F_r=r(M6e," \u2014 "),dO=n(M6e,"A",{href:!0});var Kdt=s(dO);C_r=r(Kdt,"FlaxBlenderbotModel"),Kdt.forEach(t),M_r=r(M6e," (Blenderbot model)"),M6e.forEach(t),E_r=i(Q),h9=n(Q,"LI",{});var E6e=s(h9);rue=n(E6e,"STRONG",{});var Zdt=s(rue);y_r=r(Zdt,"blenderbot-small"),Zdt.forEach(t),w_r=r(E6e," \u2014 "),cO=n(E6e,"A",{href:!0});var ect=s(cO);A_r=r(ect,"FlaxBlenderbotSmallModel"),ect.forEach(t),L_r=r(E6e," (BlenderbotSmall model)"),E6e.forEach(t),B_r=i(Q),p9=n(Q,"LI",{});var y6e=s(p9);tue=n(y6e,"STRONG",{});var oct=s(tue);k_r=r(oct,"clip"),oct.forEach(t),x_r=r(y6e," \u2014 "),fO=n(y6e,"A",{href:!0});var rct=s(fO);R_r=r(rct,"FlaxCLIPModel"),rct.forEach(t),S_r=r(y6e," (CLIP model)"),y6e.forEach(t),P_r=i(Q),_9=n(Q,"LI",{});var w6e=s(_9);aue=n(w6e,"STRONG",{});var tct=s(aue);$_r=r(tct,"distilbert"),tct.forEach(t),I_r=r(w6e," \u2014 "),mO=n(w6e,"A",{href:!0});var act=s(mO);j_r=r(act,"FlaxDistilBertModel"),act.forEach(t),N_r=r(w6e," (DistilBERT model)"),w6e.forEach(t),D_r=i(Q),u9=n(Q,"LI",{});var A6e=s(u9);nue=n(A6e,"STRONG",{});var nct=s(nue);q_r=r(nct,"electra"),nct.forEach(t),G_r=r(A6e," \u2014 "),gO=n(A6e,"A",{href:!0});var sct=s(gO);O_r=r(sct,"FlaxElectraModel"),sct.forEach(t),X_r=r(A6e," (ELECTRA model)"),A6e.forEach(t),z_r=i(Q),b9=n(Q,"LI",{});var L6e=s(b9);sue=n(L6e,"STRONG",{});var lct=s(sue);V_r=r(lct,"gpt2"),lct.forEach(t),W_r=r(L6e," \u2014 "),hO=n(L6e,"A",{href:!0});var ict=s(hO);Q_r=r(ict,"FlaxGPT2Model"),ict.forEach(t),H_r=r(L6e," (OpenAI GPT-2 model)"),L6e.forEach(t),U_r=i(Q),v9=n(Q,"LI",{});var B6e=s(v9);lue=n(B6e,"STRONG",{});var dct=s(lue);J_r=r(dct,"gpt_neo"),dct.forEach(t),Y_r=r(B6e," \u2014 "),pO=n(B6e,"A",{href:!0});var cct=s(pO);K_r=r(cct,"FlaxGPTNeoModel"),cct.forEach(t),Z_r=r(B6e," (GPT Neo model)"),B6e.forEach(t),eur=i(Q),T9=n(Q,"LI",{});var k6e=s(T9);iue=n(k6e,"STRONG",{});var fct=s(iue);our=r(fct,"gptj"),fct.forEach(t),rur=r(k6e," \u2014 "),_O=n(k6e,"A",{href:!0});var mct=s(_O);tur=r(mct,"FlaxGPTJModel"),mct.forEach(t),aur=r(k6e," (GPT-J model)"),k6e.forEach(t),nur=i(Q),F9=n(Q,"LI",{});var x6e=s(F9);due=n(x6e,"STRONG",{});var gct=s(due);sur=r(gct,"marian"),gct.forEach(t),lur=r(x6e," \u2014 "),uO=n(x6e,"A",{href:!0});var hct=s(uO);iur=r(hct,"FlaxMarianModel"),hct.forEach(t),dur=r(x6e," (Marian model)"),x6e.forEach(t),cur=i(Q),C9=n(Q,"LI",{});var R6e=s(C9);cue=n(R6e,"STRONG",{});var pct=s(cue);fur=r(pct,"mbart"),pct.forEach(t),mur=r(R6e," \u2014 "),bO=n(R6e,"A",{href:!0});var _ct=s(bO);gur=r(_ct,"FlaxMBartModel"),_ct.forEach(t),hur=r(R6e," (mBART model)"),R6e.forEach(t),pur=i(Q),M9=n(Q,"LI",{});var S6e=s(M9);fue=n(S6e,"STRONG",{});var uct=s(fue);_ur=r(uct,"mt5"),uct.forEach(t),uur=r(S6e," \u2014 "),vO=n(S6e,"A",{href:!0});var bct=s(vO);bur=r(bct,"FlaxMT5Model"),bct.forEach(t),vur=r(S6e," (mT5 model)"),S6e.forEach(t),Tur=i(Q),E9=n(Q,"LI",{});var P6e=s(E9);mue=n(P6e,"STRONG",{});var vct=s(mue);Fur=r(vct,"pegasus"),vct.forEach(t),Cur=r(P6e," \u2014 "),TO=n(P6e,"A",{href:!0});var Tct=s(TO);Mur=r(Tct,"FlaxPegasusModel"),Tct.forEach(t),Eur=r(P6e," (Pegasus model)"),P6e.forEach(t),yur=i(Q),y9=n(Q,"LI",{});var $6e=s(y9);gue=n($6e,"STRONG",{});var Fct=s(gue);wur=r(Fct,"roberta"),Fct.forEach(t),Aur=r($6e," \u2014 "),FO=n($6e,"A",{href:!0});var Cct=s(FO);Lur=r(Cct,"FlaxRobertaModel"),Cct.forEach(t),Bur=r($6e," (RoBERTa model)"),$6e.forEach(t),kur=i(Q),w9=n(Q,"LI",{});var I6e=s(w9);hue=n(I6e,"STRONG",{});var Mct=s(hue);xur=r(Mct,"roformer"),Mct.forEach(t),Rur=r(I6e," \u2014 "),CO=n(I6e,"A",{href:!0});var Ect=s(CO);Sur=r(Ect,"FlaxRoFormerModel"),Ect.forEach(t),Pur=r(I6e," (RoFormer model)"),I6e.forEach(t),$ur=i(Q),A9=n(Q,"LI",{});var j6e=s(A9);pue=n(j6e,"STRONG",{});var yct=s(pue);Iur=r(yct,"t5"),yct.forEach(t),jur=r(j6e," \u2014 "),MO=n(j6e,"A",{href:!0});var wct=s(MO);Nur=r(wct,"FlaxT5Model"),wct.forEach(t),Dur=r(j6e," (T5 model)"),j6e.forEach(t),qur=i(Q),L9=n(Q,"LI",{});var N6e=s(L9);_ue=n(N6e,"STRONG",{});var Act=s(_ue);Gur=r(Act,"vision-text-dual-encoder"),Act.forEach(t),Our=r(N6e," \u2014 "),EO=n(N6e,"A",{href:!0});var Lct=s(EO);Xur=r(Lct,"FlaxVisionTextDualEncoderModel"),Lct.forEach(t),zur=r(N6e," (VisionTextDualEncoder model)"),N6e.forEach(t),Vur=i(Q),B9=n(Q,"LI",{});var D6e=s(B9);uue=n(D6e,"STRONG",{});var Bct=s(uue);Wur=r(Bct,"vit"),Bct.forEach(t),Qur=r(D6e," \u2014 "),yO=n(D6e,"A",{href:!0});var kct=s(yO);Hur=r(kct,"FlaxViTModel"),kct.forEach(t),Uur=r(D6e," (ViT model)"),D6e.forEach(t),Jur=i(Q),k9=n(Q,"LI",{});var q6e=s(k9);bue=n(q6e,"STRONG",{});var xct=s(bue);Yur=r(xct,"wav2vec2"),xct.forEach(t),Kur=r(q6e," \u2014 "),wO=n(q6e,"A",{href:!0});var Rct=s(wO);Zur=r(Rct,"FlaxWav2Vec2Model"),Rct.forEach(t),e2r=r(q6e," (Wav2Vec2 model)"),q6e.forEach(t),o2r=i(Q),x9=n(Q,"LI",{});var G6e=s(x9);vue=n(G6e,"STRONG",{});var Sct=s(vue);r2r=r(Sct,"xglm"),Sct.forEach(t),t2r=r(G6e," \u2014 "),AO=n(G6e,"A",{href:!0});var Pct=s(AO);a2r=r(Pct,"FlaxXGLMModel"),Pct.forEach(t),n2r=r(G6e," (XGLM model)"),G6e.forEach(t),Q.forEach(t),s2r=i(Ma),Tue=n(Ma,"P",{});var $ct=s(Tue);l2r=r($ct,"Examples:"),$ct.forEach(t),i2r=i(Ma),m(bA.$$.fragment,Ma),Ma.forEach(t),oi.forEach(t),P8e=i(d),Xc=n(d,"H2",{class:!0});var Xke=s(Xc);R9=n(Xke,"A",{id:!0,class:!0,href:!0});var Ict=s(R9);Fue=n(Ict,"SPAN",{});var jct=s(Fue);m(vA.$$.fragment,jct),jct.forEach(t),Ict.forEach(t),d2r=i(Xke),Cue=n(Xke,"SPAN",{});var Nct=s(Cue);c2r=r(Nct,"FlaxAutoModelForCausalLM"),Nct.forEach(t),Xke.forEach(t),$8e=i(d),Ar=n(d,"DIV",{class:!0});var ti=s(Ar);m(TA.$$.fragment,ti),f2r=i(ti),zc=n(ti,"P",{});var gV=s(zc);m2r=r(gV,`This is a generic model class that will be instantiated as one of the model classes of the library (with a causal language modeling head) when created
with the `),Mue=n(gV,"CODE",{});var Dct=s(Mue);g2r=r(Dct,"from_pretrained()"),Dct.forEach(t),h2r=r(gV,"class method or the "),Eue=n(gV,"CODE",{});var qct=s(Eue);p2r=r(qct,"from_config()"),qct.forEach(t),_2r=r(gV,`class
method.`),gV.forEach(t),u2r=i(ti),FA=n(ti,"P",{});var zke=s(FA);b2r=r(zke,"This class cannot be instantiated directly using "),yue=n(zke,"CODE",{});var Gct=s(yue);v2r=r(Gct,"__init__()"),Gct.forEach(t),T2r=r(zke," (throws an error)."),zke.forEach(t),F2r=i(ti),Tt=n(ti,"DIV",{class:!0});var ai=s(Tt);m(CA.$$.fragment,ai),C2r=i(ai),wue=n(ai,"P",{});var Oct=s(wue);M2r=r(Oct,"Instantiates one of the model classes of the library (with a causal language modeling head) from a configuration."),Oct.forEach(t),E2r=i(ai),Vc=n(ai,"P",{});var hV=s(Vc);y2r=r(hV,`Note:
Loading a model from its configuration file does `),Aue=n(hV,"STRONG",{});var Xct=s(Aue);w2r=r(Xct,"not"),Xct.forEach(t),A2r=r(hV,` load the model weights. It only affects the
model\u2019s configuration. Use `),Lue=n(hV,"CODE",{});var zct=s(Lue);L2r=r(zct,"from_pretrained()"),zct.forEach(t),B2r=r(hV,"to load the model weights."),hV.forEach(t),k2r=i(ai),Bue=n(ai,"P",{});var Vct=s(Bue);x2r=r(Vct,"Examples:"),Vct.forEach(t),R2r=i(ai),m(MA.$$.fragment,ai),ai.forEach(t),S2r=i(ti),Ao=n(ti,"DIV",{class:!0});var Ea=s(Ao);m(EA.$$.fragment,Ea),P2r=i(Ea),kue=n(Ea,"P",{});var Wct=s(kue);$2r=r(Wct,"Instantiate one of the model classes of the library (with a causal language modeling head) from a pretrained model."),Wct.forEach(t),I2r=i(Ea),Cn=n(Ea,"P",{});var X4=s(Cn);j2r=r(X4,"The model class to instantiate is selected based on the "),xue=n(X4,"CODE",{});var Qct=s(xue);N2r=r(Qct,"model_type"),Qct.forEach(t),D2r=r(X4,` property of the config object (either
passed as an argument or loaded from `),Rue=n(X4,"CODE",{});var Hct=s(Rue);q2r=r(Hct,"pretrained_model_name_or_path"),Hct.forEach(t),G2r=r(X4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Sue=n(X4,"CODE",{});var Uct=s(Sue);O2r=r(Uct,"pretrained_model_name_or_path"),Uct.forEach(t),X2r=r(X4,":"),X4.forEach(t),z2r=i(Ea),Mn=n(Ea,"UL",{});var z4=s(Mn);S9=n(z4,"LI",{});var O6e=s(S9);Pue=n(O6e,"STRONG",{});var Jct=s(Pue);V2r=r(Jct,"gpt2"),Jct.forEach(t),W2r=r(O6e," \u2014 "),LO=n(O6e,"A",{href:!0});var Yct=s(LO);Q2r=r(Yct,"FlaxGPT2LMHeadModel"),Yct.forEach(t),H2r=r(O6e," (OpenAI GPT-2 model)"),O6e.forEach(t),U2r=i(z4),P9=n(z4,"LI",{});var X6e=s(P9);$ue=n(X6e,"STRONG",{});var Kct=s($ue);J2r=r(Kct,"gpt_neo"),Kct.forEach(t),Y2r=r(X6e," \u2014 "),BO=n(X6e,"A",{href:!0});var Zct=s(BO);K2r=r(Zct,"FlaxGPTNeoForCausalLM"),Zct.forEach(t),Z2r=r(X6e," (GPT Neo model)"),X6e.forEach(t),e1r=i(z4),$9=n(z4,"LI",{});var z6e=s($9);Iue=n(z6e,"STRONG",{});var eft=s(Iue);o1r=r(eft,"gptj"),eft.forEach(t),r1r=r(z6e," \u2014 "),kO=n(z6e,"A",{href:!0});var oft=s(kO);t1r=r(oft,"FlaxGPTJForCausalLM"),oft.forEach(t),a1r=r(z6e," (GPT-J model)"),z6e.forEach(t),n1r=i(z4),I9=n(z4,"LI",{});var V6e=s(I9);jue=n(V6e,"STRONG",{});var rft=s(jue);s1r=r(rft,"xglm"),rft.forEach(t),l1r=r(V6e," \u2014 "),xO=n(V6e,"A",{href:!0});var tft=s(xO);i1r=r(tft,"FlaxXGLMForCausalLM"),tft.forEach(t),d1r=r(V6e," (XGLM model)"),V6e.forEach(t),z4.forEach(t),c1r=i(Ea),Nue=n(Ea,"P",{});var aft=s(Nue);f1r=r(aft,"Examples:"),aft.forEach(t),m1r=i(Ea),m(yA.$$.fragment,Ea),Ea.forEach(t),ti.forEach(t),I8e=i(d),Wc=n(d,"H2",{class:!0});var Vke=s(Wc);j9=n(Vke,"A",{id:!0,class:!0,href:!0});var nft=s(j9);Due=n(nft,"SPAN",{});var sft=s(Due);m(wA.$$.fragment,sft),sft.forEach(t),nft.forEach(t),g1r=i(Vke),que=n(Vke,"SPAN",{});var lft=s(que);h1r=r(lft,"FlaxAutoModelForPreTraining"),lft.forEach(t),Vke.forEach(t),j8e=i(d),Lr=n(d,"DIV",{class:!0});var ni=s(Lr);m(AA.$$.fragment,ni),p1r=i(ni),Qc=n(ni,"P",{});var pV=s(Qc);_1r=r(pV,`This is a generic model class that will be instantiated as one of the model classes of the library (with a pretraining head) when created
with the `),Gue=n(pV,"CODE",{});var ift=s(Gue);u1r=r(ift,"from_pretrained()"),ift.forEach(t),b1r=r(pV,"class method or the "),Oue=n(pV,"CODE",{});var dft=s(Oue);v1r=r(dft,"from_config()"),dft.forEach(t),T1r=r(pV,`class
method.`),pV.forEach(t),F1r=i(ni),LA=n(ni,"P",{});var Wke=s(LA);C1r=r(Wke,"This class cannot be instantiated directly using "),Xue=n(Wke,"CODE",{});var cft=s(Xue);M1r=r(cft,"__init__()"),cft.forEach(t),E1r=r(Wke," (throws an error)."),Wke.forEach(t),y1r=i(ni),Ft=n(ni,"DIV",{class:!0});var si=s(Ft);m(BA.$$.fragment,si),w1r=i(si),zue=n(si,"P",{});var fft=s(zue);A1r=r(fft,"Instantiates one of the model classes of the library (with a pretraining head) from a configuration."),fft.forEach(t),L1r=i(si),Hc=n(si,"P",{});var _V=s(Hc);B1r=r(_V,`Note:
Loading a model from its configuration file does `),Vue=n(_V,"STRONG",{});var mft=s(Vue);k1r=r(mft,"not"),mft.forEach(t),x1r=r(_V,` load the model weights. It only affects the
model\u2019s configuration. Use `),Wue=n(_V,"CODE",{});var gft=s(Wue);R1r=r(gft,"from_pretrained()"),gft.forEach(t),S1r=r(_V,"to load the model weights."),_V.forEach(t),P1r=i(si),Que=n(si,"P",{});var hft=s(Que);$1r=r(hft,"Examples:"),hft.forEach(t),I1r=i(si),m(kA.$$.fragment,si),si.forEach(t),j1r=i(ni),Lo=n(ni,"DIV",{class:!0});var ya=s(Lo);m(xA.$$.fragment,ya),N1r=i(ya),Hue=n(ya,"P",{});var pft=s(Hue);D1r=r(pft,"Instantiate one of the model classes of the library (with a pretraining head) from a pretrained model."),pft.forEach(t),q1r=i(ya),En=n(ya,"P",{});var V4=s(En);G1r=r(V4,"The model class to instantiate is selected based on the "),Uue=n(V4,"CODE",{});var _ft=s(Uue);O1r=r(_ft,"model_type"),_ft.forEach(t),X1r=r(V4,` property of the config object (either
passed as an argument or loaded from `),Jue=n(V4,"CODE",{});var uft=s(Jue);z1r=r(uft,"pretrained_model_name_or_path"),uft.forEach(t),V1r=r(V4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Yue=n(V4,"CODE",{});var bft=s(Yue);W1r=r(bft,"pretrained_model_name_or_path"),bft.forEach(t),Q1r=r(V4,":"),V4.forEach(t),H1r=i(ya),fe=n(ya,"UL",{});var _e=s(fe);N9=n(_e,"LI",{});var W6e=s(N9);Kue=n(W6e,"STRONG",{});var vft=s(Kue);U1r=r(vft,"albert"),vft.forEach(t),J1r=r(W6e," \u2014 "),RO=n(W6e,"A",{href:!0});var Tft=s(RO);Y1r=r(Tft,"FlaxAlbertForPreTraining"),Tft.forEach(t),K1r=r(W6e," (ALBERT model)"),W6e.forEach(t),Z1r=i(_e),D9=n(_e,"LI",{});var Q6e=s(D9);Zue=n(Q6e,"STRONG",{});var Fft=s(Zue);ebr=r(Fft,"bart"),Fft.forEach(t),obr=r(Q6e," \u2014 "),SO=n(Q6e,"A",{href:!0});var Cft=s(SO);rbr=r(Cft,"FlaxBartForConditionalGeneration"),Cft.forEach(t),tbr=r(Q6e," (BART model)"),Q6e.forEach(t),abr=i(_e),q9=n(_e,"LI",{});var H6e=s(q9);e2e=n(H6e,"STRONG",{});var Mft=s(e2e);nbr=r(Mft,"bert"),Mft.forEach(t),sbr=r(H6e," \u2014 "),PO=n(H6e,"A",{href:!0});var Eft=s(PO);lbr=r(Eft,"FlaxBertForPreTraining"),Eft.forEach(t),ibr=r(H6e," (BERT model)"),H6e.forEach(t),dbr=i(_e),G9=n(_e,"LI",{});var U6e=s(G9);o2e=n(U6e,"STRONG",{});var yft=s(o2e);cbr=r(yft,"big_bird"),yft.forEach(t),fbr=r(U6e," \u2014 "),$O=n(U6e,"A",{href:!0});var wft=s($O);mbr=r(wft,"FlaxBigBirdForPreTraining"),wft.forEach(t),gbr=r(U6e," (BigBird model)"),U6e.forEach(t),hbr=i(_e),O9=n(_e,"LI",{});var J6e=s(O9);r2e=n(J6e,"STRONG",{});var Aft=s(r2e);pbr=r(Aft,"electra"),Aft.forEach(t),_br=r(J6e," \u2014 "),IO=n(J6e,"A",{href:!0});var Lft=s(IO);ubr=r(Lft,"FlaxElectraForPreTraining"),Lft.forEach(t),bbr=r(J6e," (ELECTRA model)"),J6e.forEach(t),vbr=i(_e),X9=n(_e,"LI",{});var Y6e=s(X9);t2e=n(Y6e,"STRONG",{});var Bft=s(t2e);Tbr=r(Bft,"mbart"),Bft.forEach(t),Fbr=r(Y6e," \u2014 "),jO=n(Y6e,"A",{href:!0});var kft=s(jO);Cbr=r(kft,"FlaxMBartForConditionalGeneration"),kft.forEach(t),Mbr=r(Y6e," (mBART model)"),Y6e.forEach(t),Ebr=i(_e),z9=n(_e,"LI",{});var K6e=s(z9);a2e=n(K6e,"STRONG",{});var xft=s(a2e);ybr=r(xft,"mt5"),xft.forEach(t),wbr=r(K6e," \u2014 "),NO=n(K6e,"A",{href:!0});var Rft=s(NO);Abr=r(Rft,"FlaxMT5ForConditionalGeneration"),Rft.forEach(t),Lbr=r(K6e," (mT5 model)"),K6e.forEach(t),Bbr=i(_e),V9=n(_e,"LI",{});var Z6e=s(V9);n2e=n(Z6e,"STRONG",{});var Sft=s(n2e);kbr=r(Sft,"roberta"),Sft.forEach(t),xbr=r(Z6e," \u2014 "),DO=n(Z6e,"A",{href:!0});var Pft=s(DO);Rbr=r(Pft,"FlaxRobertaForMaskedLM"),Pft.forEach(t),Sbr=r(Z6e," (RoBERTa model)"),Z6e.forEach(t),Pbr=i(_e),W9=n(_e,"LI",{});var e0e=s(W9);s2e=n(e0e,"STRONG",{});var $ft=s(s2e);$br=r($ft,"roformer"),$ft.forEach(t),Ibr=r(e0e," \u2014 "),qO=n(e0e,"A",{href:!0});var Ift=s(qO);jbr=r(Ift,"FlaxRoFormerForMaskedLM"),Ift.forEach(t),Nbr=r(e0e," (RoFormer model)"),e0e.forEach(t),Dbr=i(_e),Q9=n(_e,"LI",{});var o0e=s(Q9);l2e=n(o0e,"STRONG",{});var jft=s(l2e);qbr=r(jft,"t5"),jft.forEach(t),Gbr=r(o0e," \u2014 "),GO=n(o0e,"A",{href:!0});var Nft=s(GO);Obr=r(Nft,"FlaxT5ForConditionalGeneration"),Nft.forEach(t),Xbr=r(o0e," (T5 model)"),o0e.forEach(t),zbr=i(_e),H9=n(_e,"LI",{});var r0e=s(H9);i2e=n(r0e,"STRONG",{});var Dft=s(i2e);Vbr=r(Dft,"wav2vec2"),Dft.forEach(t),Wbr=r(r0e," \u2014 "),OO=n(r0e,"A",{href:!0});var qft=s(OO);Qbr=r(qft,"FlaxWav2Vec2ForPreTraining"),qft.forEach(t),Hbr=r(r0e," (Wav2Vec2 model)"),r0e.forEach(t),_e.forEach(t),Ubr=i(ya),d2e=n(ya,"P",{});var Gft=s(d2e);Jbr=r(Gft,"Examples:"),Gft.forEach(t),Ybr=i(ya),m(RA.$$.fragment,ya),ya.forEach(t),ni.forEach(t),N8e=i(d),Uc=n(d,"H2",{class:!0});var Qke=s(Uc);U9=n(Qke,"A",{id:!0,class:!0,href:!0});var Oft=s(U9);c2e=n(Oft,"SPAN",{});var Xft=s(c2e);m(SA.$$.fragment,Xft),Xft.forEach(t),Oft.forEach(t),Kbr=i(Qke),f2e=n(Qke,"SPAN",{});var zft=s(f2e);Zbr=r(zft,"FlaxAutoModelForMaskedLM"),zft.forEach(t),Qke.forEach(t),D8e=i(d),Br=n(d,"DIV",{class:!0});var li=s(Br);m(PA.$$.fragment,li),e5r=i(li),Jc=n(li,"P",{});var uV=s(Jc);o5r=r(uV,`This is a generic model class that will be instantiated as one of the model classes of the library (with a masked language modeling head) when created
with the `),m2e=n(uV,"CODE",{});var Vft=s(m2e);r5r=r(Vft,"from_pretrained()"),Vft.forEach(t),t5r=r(uV,"class method or the "),g2e=n(uV,"CODE",{});var Wft=s(g2e);a5r=r(Wft,"from_config()"),Wft.forEach(t),n5r=r(uV,`class
method.`),uV.forEach(t),s5r=i(li),$A=n(li,"P",{});var Hke=s($A);l5r=r(Hke,"This class cannot be instantiated directly using "),h2e=n(Hke,"CODE",{});var Qft=s(h2e);i5r=r(Qft,"__init__()"),Qft.forEach(t),d5r=r(Hke," (throws an error)."),Hke.forEach(t),c5r=i(li),Ct=n(li,"DIV",{class:!0});var ii=s(Ct);m(IA.$$.fragment,ii),f5r=i(ii),p2e=n(ii,"P",{});var Hft=s(p2e);m5r=r(Hft,"Instantiates one of the model classes of the library (with a masked language modeling head) from a configuration."),Hft.forEach(t),g5r=i(ii),Yc=n(ii,"P",{});var bV=s(Yc);h5r=r(bV,`Note:
Loading a model from its configuration file does `),_2e=n(bV,"STRONG",{});var Uft=s(_2e);p5r=r(Uft,"not"),Uft.forEach(t),_5r=r(bV,` load the model weights. It only affects the
model\u2019s configuration. Use `),u2e=n(bV,"CODE",{});var Jft=s(u2e);u5r=r(Jft,"from_pretrained()"),Jft.forEach(t),b5r=r(bV,"to load the model weights."),bV.forEach(t),v5r=i(ii),b2e=n(ii,"P",{});var Yft=s(b2e);T5r=r(Yft,"Examples:"),Yft.forEach(t),F5r=i(ii),m(jA.$$.fragment,ii),ii.forEach(t),C5r=i(li),Bo=n(li,"DIV",{class:!0});var wa=s(Bo);m(NA.$$.fragment,wa),M5r=i(wa),v2e=n(wa,"P",{});var Kft=s(v2e);E5r=r(Kft,"Instantiate one of the model classes of the library (with a masked language modeling head) from a pretrained model."),Kft.forEach(t),y5r=i(wa),yn=n(wa,"P",{});var W4=s(yn);w5r=r(W4,"The model class to instantiate is selected based on the "),T2e=n(W4,"CODE",{});var Zft=s(T2e);A5r=r(Zft,"model_type"),Zft.forEach(t),L5r=r(W4,` property of the config object (either
passed as an argument or loaded from `),F2e=n(W4,"CODE",{});var emt=s(F2e);B5r=r(emt,"pretrained_model_name_or_path"),emt.forEach(t),k5r=r(W4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),C2e=n(W4,"CODE",{});var omt=s(C2e);x5r=r(omt,"pretrained_model_name_or_path"),omt.forEach(t),R5r=r(W4,":"),W4.forEach(t),S5r=i(wa),ve=n(wa,"UL",{});var Ze=s(ve);J9=n(Ze,"LI",{});var t0e=s(J9);M2e=n(t0e,"STRONG",{});var rmt=s(M2e);P5r=r(rmt,"albert"),rmt.forEach(t),$5r=r(t0e," \u2014 "),XO=n(t0e,"A",{href:!0});var tmt=s(XO);I5r=r(tmt,"FlaxAlbertForMaskedLM"),tmt.forEach(t),j5r=r(t0e," (ALBERT model)"),t0e.forEach(t),N5r=i(Ze),Y9=n(Ze,"LI",{});var a0e=s(Y9);E2e=n(a0e,"STRONG",{});var amt=s(E2e);D5r=r(amt,"bart"),amt.forEach(t),q5r=r(a0e," \u2014 "),zO=n(a0e,"A",{href:!0});var nmt=s(zO);G5r=r(nmt,"FlaxBartForConditionalGeneration"),nmt.forEach(t),O5r=r(a0e," (BART model)"),a0e.forEach(t),X5r=i(Ze),K9=n(Ze,"LI",{});var n0e=s(K9);y2e=n(n0e,"STRONG",{});var smt=s(y2e);z5r=r(smt,"bert"),smt.forEach(t),V5r=r(n0e," \u2014 "),VO=n(n0e,"A",{href:!0});var lmt=s(VO);W5r=r(lmt,"FlaxBertForMaskedLM"),lmt.forEach(t),Q5r=r(n0e," (BERT model)"),n0e.forEach(t),H5r=i(Ze),Z9=n(Ze,"LI",{});var s0e=s(Z9);w2e=n(s0e,"STRONG",{});var imt=s(w2e);U5r=r(imt,"big_bird"),imt.forEach(t),J5r=r(s0e," \u2014 "),WO=n(s0e,"A",{href:!0});var dmt=s(WO);Y5r=r(dmt,"FlaxBigBirdForMaskedLM"),dmt.forEach(t),K5r=r(s0e," (BigBird model)"),s0e.forEach(t),Z5r=i(Ze),eC=n(Ze,"LI",{});var l0e=s(eC);A2e=n(l0e,"STRONG",{});var cmt=s(A2e);evr=r(cmt,"distilbert"),cmt.forEach(t),ovr=r(l0e," \u2014 "),QO=n(l0e,"A",{href:!0});var fmt=s(QO);rvr=r(fmt,"FlaxDistilBertForMaskedLM"),fmt.forEach(t),tvr=r(l0e," (DistilBERT model)"),l0e.forEach(t),avr=i(Ze),oC=n(Ze,"LI",{});var i0e=s(oC);L2e=n(i0e,"STRONG",{});var mmt=s(L2e);nvr=r(mmt,"electra"),mmt.forEach(t),svr=r(i0e," \u2014 "),HO=n(i0e,"A",{href:!0});var gmt=s(HO);lvr=r(gmt,"FlaxElectraForMaskedLM"),gmt.forEach(t),ivr=r(i0e," (ELECTRA model)"),i0e.forEach(t),dvr=i(Ze),rC=n(Ze,"LI",{});var d0e=s(rC);B2e=n(d0e,"STRONG",{});var hmt=s(B2e);cvr=r(hmt,"mbart"),hmt.forEach(t),fvr=r(d0e," \u2014 "),UO=n(d0e,"A",{href:!0});var pmt=s(UO);mvr=r(pmt,"FlaxMBartForConditionalGeneration"),pmt.forEach(t),gvr=r(d0e," (mBART model)"),d0e.forEach(t),hvr=i(Ze),tC=n(Ze,"LI",{});var c0e=s(tC);k2e=n(c0e,"STRONG",{});var _mt=s(k2e);pvr=r(_mt,"roberta"),_mt.forEach(t),_vr=r(c0e," \u2014 "),JO=n(c0e,"A",{href:!0});var umt=s(JO);uvr=r(umt,"FlaxRobertaForMaskedLM"),umt.forEach(t),bvr=r(c0e," (RoBERTa model)"),c0e.forEach(t),vvr=i(Ze),aC=n(Ze,"LI",{});var f0e=s(aC);x2e=n(f0e,"STRONG",{});var bmt=s(x2e);Tvr=r(bmt,"roformer"),bmt.forEach(t),Fvr=r(f0e," \u2014 "),YO=n(f0e,"A",{href:!0});var vmt=s(YO);Cvr=r(vmt,"FlaxRoFormerForMaskedLM"),vmt.forEach(t),Mvr=r(f0e," (RoFormer model)"),f0e.forEach(t),Ze.forEach(t),Evr=i(wa),R2e=n(wa,"P",{});var Tmt=s(R2e);yvr=r(Tmt,"Examples:"),Tmt.forEach(t),wvr=i(wa),m(DA.$$.fragment,wa),wa.forEach(t),li.forEach(t),q8e=i(d),Kc=n(d,"H2",{class:!0});var Uke=s(Kc);nC=n(Uke,"A",{id:!0,class:!0,href:!0});var Fmt=s(nC);S2e=n(Fmt,"SPAN",{});var Cmt=s(S2e);m(qA.$$.fragment,Cmt),Cmt.forEach(t),Fmt.forEach(t),Avr=i(Uke),P2e=n(Uke,"SPAN",{});var Mmt=s(P2e);Lvr=r(Mmt,"FlaxAutoModelForSeq2SeqLM"),Mmt.forEach(t),Uke.forEach(t),G8e=i(d),kr=n(d,"DIV",{class:!0});var di=s(kr);m(GA.$$.fragment,di),Bvr=i(di),Zc=n(di,"P",{});var vV=s(Zc);kvr=r(vV,`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence language modeling head) when created
with the `),$2e=n(vV,"CODE",{});var Emt=s($2e);xvr=r(Emt,"from_pretrained()"),Emt.forEach(t),Rvr=r(vV,"class method or the "),I2e=n(vV,"CODE",{});var ymt=s(I2e);Svr=r(ymt,"from_config()"),ymt.forEach(t),Pvr=r(vV,`class
method.`),vV.forEach(t),$vr=i(di),OA=n(di,"P",{});var Jke=s(OA);Ivr=r(Jke,"This class cannot be instantiated directly using "),j2e=n(Jke,"CODE",{});var wmt=s(j2e);jvr=r(wmt,"__init__()"),wmt.forEach(t),Nvr=r(Jke," (throws an error)."),Jke.forEach(t),Dvr=i(di),Mt=n(di,"DIV",{class:!0});var ci=s(Mt);m(XA.$$.fragment,ci),qvr=i(ci),N2e=n(ci,"P",{});var Amt=s(N2e);Gvr=r(Amt,"Instantiates one of the model classes of the library (with a sequence-to-sequence language modeling head) from a configuration."),Amt.forEach(t),Ovr=i(ci),ef=n(ci,"P",{});var TV=s(ef);Xvr=r(TV,`Note:
Loading a model from its configuration file does `),D2e=n(TV,"STRONG",{});var Lmt=s(D2e);zvr=r(Lmt,"not"),Lmt.forEach(t),Vvr=r(TV,` load the model weights. It only affects the
model\u2019s configuration. Use `),q2e=n(TV,"CODE",{});var Bmt=s(q2e);Wvr=r(Bmt,"from_pretrained()"),Bmt.forEach(t),Qvr=r(TV,"to load the model weights."),TV.forEach(t),Hvr=i(ci),G2e=n(ci,"P",{});var kmt=s(G2e);Uvr=r(kmt,"Examples:"),kmt.forEach(t),Jvr=i(ci),m(zA.$$.fragment,ci),ci.forEach(t),Yvr=i(di),ko=n(di,"DIV",{class:!0});var Aa=s(ko);m(VA.$$.fragment,Aa),Kvr=i(Aa),O2e=n(Aa,"P",{});var xmt=s(O2e);Zvr=r(xmt,"Instantiate one of the model classes of the library (with a sequence-to-sequence language modeling head) from a pretrained model."),xmt.forEach(t),eTr=i(Aa),wn=n(Aa,"P",{});var Q4=s(wn);oTr=r(Q4,"The model class to instantiate is selected based on the "),X2e=n(Q4,"CODE",{});var Rmt=s(X2e);rTr=r(Rmt,"model_type"),Rmt.forEach(t),tTr=r(Q4,` property of the config object (either
passed as an argument or loaded from `),z2e=n(Q4,"CODE",{});var Smt=s(z2e);aTr=r(Smt,"pretrained_model_name_or_path"),Smt.forEach(t),nTr=r(Q4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),V2e=n(Q4,"CODE",{});var Pmt=s(V2e);sTr=r(Pmt,"pretrained_model_name_or_path"),Pmt.forEach(t),lTr=r(Q4,":"),Q4.forEach(t),iTr=i(Aa),Te=n(Aa,"UL",{});var eo=s(Te);sC=n(eo,"LI",{});var m0e=s(sC);W2e=n(m0e,"STRONG",{});var $mt=s(W2e);dTr=r($mt,"bart"),$mt.forEach(t),cTr=r(m0e," \u2014 "),KO=n(m0e,"A",{href:!0});var Imt=s(KO);fTr=r(Imt,"FlaxBartForConditionalGeneration"),Imt.forEach(t),mTr=r(m0e," (BART model)"),m0e.forEach(t),gTr=i(eo),lC=n(eo,"LI",{});var g0e=s(lC);Q2e=n(g0e,"STRONG",{});var jmt=s(Q2e);hTr=r(jmt,"blenderbot"),jmt.forEach(t),pTr=r(g0e," \u2014 "),ZO=n(g0e,"A",{href:!0});var Nmt=s(ZO);_Tr=r(Nmt,"FlaxBlenderbotForConditionalGeneration"),Nmt.forEach(t),uTr=r(g0e," (Blenderbot model)"),g0e.forEach(t),bTr=i(eo),iC=n(eo,"LI",{});var h0e=s(iC);H2e=n(h0e,"STRONG",{});var Dmt=s(H2e);vTr=r(Dmt,"blenderbot-small"),Dmt.forEach(t),TTr=r(h0e," \u2014 "),eX=n(h0e,"A",{href:!0});var qmt=s(eX);FTr=r(qmt,"FlaxBlenderbotSmallForConditionalGeneration"),qmt.forEach(t),CTr=r(h0e," (BlenderbotSmall model)"),h0e.forEach(t),MTr=i(eo),dC=n(eo,"LI",{});var p0e=s(dC);U2e=n(p0e,"STRONG",{});var Gmt=s(U2e);ETr=r(Gmt,"encoder-decoder"),Gmt.forEach(t),yTr=r(p0e," \u2014 "),oX=n(p0e,"A",{href:!0});var Omt=s(oX);wTr=r(Omt,"FlaxEncoderDecoderModel"),Omt.forEach(t),ATr=r(p0e," (Encoder decoder model)"),p0e.forEach(t),LTr=i(eo),cC=n(eo,"LI",{});var _0e=s(cC);J2e=n(_0e,"STRONG",{});var Xmt=s(J2e);BTr=r(Xmt,"marian"),Xmt.forEach(t),kTr=r(_0e," \u2014 "),rX=n(_0e,"A",{href:!0});var zmt=s(rX);xTr=r(zmt,"FlaxMarianMTModel"),zmt.forEach(t),RTr=r(_0e," (Marian model)"),_0e.forEach(t),STr=i(eo),fC=n(eo,"LI",{});var u0e=s(fC);Y2e=n(u0e,"STRONG",{});var Vmt=s(Y2e);PTr=r(Vmt,"mbart"),Vmt.forEach(t),$Tr=r(u0e," \u2014 "),tX=n(u0e,"A",{href:!0});var Wmt=s(tX);ITr=r(Wmt,"FlaxMBartForConditionalGeneration"),Wmt.forEach(t),jTr=r(u0e," (mBART model)"),u0e.forEach(t),NTr=i(eo),mC=n(eo,"LI",{});var b0e=s(mC);K2e=n(b0e,"STRONG",{});var Qmt=s(K2e);DTr=r(Qmt,"mt5"),Qmt.forEach(t),qTr=r(b0e," \u2014 "),aX=n(b0e,"A",{href:!0});var Hmt=s(aX);GTr=r(Hmt,"FlaxMT5ForConditionalGeneration"),Hmt.forEach(t),OTr=r(b0e," (mT5 model)"),b0e.forEach(t),XTr=i(eo),gC=n(eo,"LI",{});var v0e=s(gC);Z2e=n(v0e,"STRONG",{});var Umt=s(Z2e);zTr=r(Umt,"pegasus"),Umt.forEach(t),VTr=r(v0e," \u2014 "),nX=n(v0e,"A",{href:!0});var Jmt=s(nX);WTr=r(Jmt,"FlaxPegasusForConditionalGeneration"),Jmt.forEach(t),QTr=r(v0e," (Pegasus model)"),v0e.forEach(t),HTr=i(eo),hC=n(eo,"LI",{});var T0e=s(hC);e1e=n(T0e,"STRONG",{});var Ymt=s(e1e);UTr=r(Ymt,"t5"),Ymt.forEach(t),JTr=r(T0e," \u2014 "),sX=n(T0e,"A",{href:!0});var Kmt=s(sX);YTr=r(Kmt,"FlaxT5ForConditionalGeneration"),Kmt.forEach(t),KTr=r(T0e," (T5 model)"),T0e.forEach(t),eo.forEach(t),ZTr=i(Aa),o1e=n(Aa,"P",{});var Zmt=s(o1e);e7r=r(Zmt,"Examples:"),Zmt.forEach(t),o7r=i(Aa),m(WA.$$.fragment,Aa),Aa.forEach(t),di.forEach(t),O8e=i(d),of=n(d,"H2",{class:!0});var Yke=s(of);pC=n(Yke,"A",{id:!0,class:!0,href:!0});var egt=s(pC);r1e=n(egt,"SPAN",{});var ogt=s(r1e);m(QA.$$.fragment,ogt),ogt.forEach(t),egt.forEach(t),r7r=i(Yke),t1e=n(Yke,"SPAN",{});var rgt=s(t1e);t7r=r(rgt,"FlaxAutoModelForSequenceClassification"),rgt.forEach(t),Yke.forEach(t),X8e=i(d),xr=n(d,"DIV",{class:!0});var fi=s(xr);m(HA.$$.fragment,fi),a7r=i(fi),rf=n(fi,"P",{});var FV=s(rf);n7r=r(FV,`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence classification head) when created
with the `),a1e=n(FV,"CODE",{});var tgt=s(a1e);s7r=r(tgt,"from_pretrained()"),tgt.forEach(t),l7r=r(FV,"class method or the "),n1e=n(FV,"CODE",{});var agt=s(n1e);i7r=r(agt,"from_config()"),agt.forEach(t),d7r=r(FV,`class
method.`),FV.forEach(t),c7r=i(fi),UA=n(fi,"P",{});var Kke=s(UA);f7r=r(Kke,"This class cannot be instantiated directly using "),s1e=n(Kke,"CODE",{});var ngt=s(s1e);m7r=r(ngt,"__init__()"),ngt.forEach(t),g7r=r(Kke," (throws an error)."),Kke.forEach(t),h7r=i(fi),Et=n(fi,"DIV",{class:!0});var mi=s(Et);m(JA.$$.fragment,mi),p7r=i(mi),l1e=n(mi,"P",{});var sgt=s(l1e);_7r=r(sgt,"Instantiates one of the model classes of the library (with a sequence classification head) from a configuration."),sgt.forEach(t),u7r=i(mi),tf=n(mi,"P",{});var CV=s(tf);b7r=r(CV,`Note:
Loading a model from its configuration file does `),i1e=n(CV,"STRONG",{});var lgt=s(i1e);v7r=r(lgt,"not"),lgt.forEach(t),T7r=r(CV,` load the model weights. It only affects the
model\u2019s configuration. Use `),d1e=n(CV,"CODE",{});var igt=s(d1e);F7r=r(igt,"from_pretrained()"),igt.forEach(t),C7r=r(CV,"to load the model weights."),CV.forEach(t),M7r=i(mi),c1e=n(mi,"P",{});var dgt=s(c1e);E7r=r(dgt,"Examples:"),dgt.forEach(t),y7r=i(mi),m(YA.$$.fragment,mi),mi.forEach(t),w7r=i(fi),xo=n(fi,"DIV",{class:!0});var La=s(xo);m(KA.$$.fragment,La),A7r=i(La),f1e=n(La,"P",{});var cgt=s(f1e);L7r=r(cgt,"Instantiate one of the model classes of the library (with a sequence classification head) from a pretrained model."),cgt.forEach(t),B7r=i(La),An=n(La,"P",{});var H4=s(An);k7r=r(H4,"The model class to instantiate is selected based on the "),m1e=n(H4,"CODE",{});var fgt=s(m1e);x7r=r(fgt,"model_type"),fgt.forEach(t),R7r=r(H4,` property of the config object (either
passed as an argument or loaded from `),g1e=n(H4,"CODE",{});var mgt=s(g1e);S7r=r(mgt,"pretrained_model_name_or_path"),mgt.forEach(t),P7r=r(H4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),h1e=n(H4,"CODE",{});var ggt=s(h1e);$7r=r(ggt,"pretrained_model_name_or_path"),ggt.forEach(t),I7r=r(H4,":"),H4.forEach(t),j7r=i(La),Fe=n(La,"UL",{});var oo=s(Fe);_C=n(oo,"LI",{});var F0e=s(_C);p1e=n(F0e,"STRONG",{});var hgt=s(p1e);N7r=r(hgt,"albert"),hgt.forEach(t),D7r=r(F0e," \u2014 "),lX=n(F0e,"A",{href:!0});var pgt=s(lX);q7r=r(pgt,"FlaxAlbertForSequenceClassification"),pgt.forEach(t),G7r=r(F0e," (ALBERT model)"),F0e.forEach(t),O7r=i(oo),uC=n(oo,"LI",{});var C0e=s(uC);_1e=n(C0e,"STRONG",{});var _gt=s(_1e);X7r=r(_gt,"bart"),_gt.forEach(t),z7r=r(C0e," \u2014 "),iX=n(C0e,"A",{href:!0});var ugt=s(iX);V7r=r(ugt,"FlaxBartForSequenceClassification"),ugt.forEach(t),W7r=r(C0e," (BART model)"),C0e.forEach(t),Q7r=i(oo),bC=n(oo,"LI",{});var M0e=s(bC);u1e=n(M0e,"STRONG",{});var bgt=s(u1e);H7r=r(bgt,"bert"),bgt.forEach(t),U7r=r(M0e," \u2014 "),dX=n(M0e,"A",{href:!0});var vgt=s(dX);J7r=r(vgt,"FlaxBertForSequenceClassification"),vgt.forEach(t),Y7r=r(M0e," (BERT model)"),M0e.forEach(t),K7r=i(oo),vC=n(oo,"LI",{});var E0e=s(vC);b1e=n(E0e,"STRONG",{});var Tgt=s(b1e);Z7r=r(Tgt,"big_bird"),Tgt.forEach(t),eFr=r(E0e," \u2014 "),cX=n(E0e,"A",{href:!0});var Fgt=s(cX);oFr=r(Fgt,"FlaxBigBirdForSequenceClassification"),Fgt.forEach(t),rFr=r(E0e," (BigBird model)"),E0e.forEach(t),tFr=i(oo),TC=n(oo,"LI",{});var y0e=s(TC);v1e=n(y0e,"STRONG",{});var Cgt=s(v1e);aFr=r(Cgt,"distilbert"),Cgt.forEach(t),nFr=r(y0e," \u2014 "),fX=n(y0e,"A",{href:!0});var Mgt=s(fX);sFr=r(Mgt,"FlaxDistilBertForSequenceClassification"),Mgt.forEach(t),lFr=r(y0e," (DistilBERT model)"),y0e.forEach(t),iFr=i(oo),FC=n(oo,"LI",{});var w0e=s(FC);T1e=n(w0e,"STRONG",{});var Egt=s(T1e);dFr=r(Egt,"electra"),Egt.forEach(t),cFr=r(w0e," \u2014 "),mX=n(w0e,"A",{href:!0});var ygt=s(mX);fFr=r(ygt,"FlaxElectraForSequenceClassification"),ygt.forEach(t),mFr=r(w0e," (ELECTRA model)"),w0e.forEach(t),gFr=i(oo),CC=n(oo,"LI",{});var A0e=s(CC);F1e=n(A0e,"STRONG",{});var wgt=s(F1e);hFr=r(wgt,"mbart"),wgt.forEach(t),pFr=r(A0e," \u2014 "),gX=n(A0e,"A",{href:!0});var Agt=s(gX);_Fr=r(Agt,"FlaxMBartForSequenceClassification"),Agt.forEach(t),uFr=r(A0e," (mBART model)"),A0e.forEach(t),bFr=i(oo),MC=n(oo,"LI",{});var L0e=s(MC);C1e=n(L0e,"STRONG",{});var Lgt=s(C1e);vFr=r(Lgt,"roberta"),Lgt.forEach(t),TFr=r(L0e," \u2014 "),hX=n(L0e,"A",{href:!0});var Bgt=s(hX);FFr=r(Bgt,"FlaxRobertaForSequenceClassification"),Bgt.forEach(t),CFr=r(L0e," (RoBERTa model)"),L0e.forEach(t),MFr=i(oo),EC=n(oo,"LI",{});var B0e=s(EC);M1e=n(B0e,"STRONG",{});var kgt=s(M1e);EFr=r(kgt,"roformer"),kgt.forEach(t),yFr=r(B0e," \u2014 "),pX=n(B0e,"A",{href:!0});var xgt=s(pX);wFr=r(xgt,"FlaxRoFormerForSequenceClassification"),xgt.forEach(t),AFr=r(B0e," (RoFormer model)"),B0e.forEach(t),oo.forEach(t),LFr=i(La),E1e=n(La,"P",{});var Rgt=s(E1e);BFr=r(Rgt,"Examples:"),Rgt.forEach(t),kFr=i(La),m(ZA.$$.fragment,La),La.forEach(t),fi.forEach(t),z8e=i(d),af=n(d,"H2",{class:!0});var Zke=s(af);yC=n(Zke,"A",{id:!0,class:!0,href:!0});var Sgt=s(yC);y1e=n(Sgt,"SPAN",{});var Pgt=s(y1e);m(e6.$$.fragment,Pgt),Pgt.forEach(t),Sgt.forEach(t),xFr=i(Zke),w1e=n(Zke,"SPAN",{});var $gt=s(w1e);RFr=r($gt,"FlaxAutoModelForQuestionAnswering"),$gt.forEach(t),Zke.forEach(t),V8e=i(d),Rr=n(d,"DIV",{class:!0});var gi=s(Rr);m(o6.$$.fragment,gi),SFr=i(gi),nf=n(gi,"P",{});var MV=s(nf);PFr=r(MV,`This is a generic model class that will be instantiated as one of the model classes of the library (with a question answering head) when created
with the `),A1e=n(MV,"CODE",{});var Igt=s(A1e);$Fr=r(Igt,"from_pretrained()"),Igt.forEach(t),IFr=r(MV,"class method or the "),L1e=n(MV,"CODE",{});var jgt=s(L1e);jFr=r(jgt,"from_config()"),jgt.forEach(t),NFr=r(MV,`class
method.`),MV.forEach(t),DFr=i(gi),r6=n(gi,"P",{});var exe=s(r6);qFr=r(exe,"This class cannot be instantiated directly using "),B1e=n(exe,"CODE",{});var Ngt=s(B1e);GFr=r(Ngt,"__init__()"),Ngt.forEach(t),OFr=r(exe," (throws an error)."),exe.forEach(t),XFr=i(gi),yt=n(gi,"DIV",{class:!0});var hi=s(yt);m(t6.$$.fragment,hi),zFr=i(hi),k1e=n(hi,"P",{});var Dgt=s(k1e);VFr=r(Dgt,"Instantiates one of the model classes of the library (with a question answering head) from a configuration."),Dgt.forEach(t),WFr=i(hi),sf=n(hi,"P",{});var EV=s(sf);QFr=r(EV,`Note:
Loading a model from its configuration file does `),x1e=n(EV,"STRONG",{});var qgt=s(x1e);HFr=r(qgt,"not"),qgt.forEach(t),UFr=r(EV,` load the model weights. It only affects the
model\u2019s configuration. Use `),R1e=n(EV,"CODE",{});var Ggt=s(R1e);JFr=r(Ggt,"from_pretrained()"),Ggt.forEach(t),YFr=r(EV,"to load the model weights."),EV.forEach(t),KFr=i(hi),S1e=n(hi,"P",{});var Ogt=s(S1e);ZFr=r(Ogt,"Examples:"),Ogt.forEach(t),e9r=i(hi),m(a6.$$.fragment,hi),hi.forEach(t),o9r=i(gi),Ro=n(gi,"DIV",{class:!0});var Ba=s(Ro);m(n6.$$.fragment,Ba),r9r=i(Ba),P1e=n(Ba,"P",{});var Xgt=s(P1e);t9r=r(Xgt,"Instantiate one of the model classes of the library (with a question answering head) from a pretrained model."),Xgt.forEach(t),a9r=i(Ba),Ln=n(Ba,"P",{});var U4=s(Ln);n9r=r(U4,"The model class to instantiate is selected based on the "),$1e=n(U4,"CODE",{});var zgt=s($1e);s9r=r(zgt,"model_type"),zgt.forEach(t),l9r=r(U4,` property of the config object (either
passed as an argument or loaded from `),I1e=n(U4,"CODE",{});var Vgt=s(I1e);i9r=r(Vgt,"pretrained_model_name_or_path"),Vgt.forEach(t),d9r=r(U4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),j1e=n(U4,"CODE",{});var Wgt=s(j1e);c9r=r(Wgt,"pretrained_model_name_or_path"),Wgt.forEach(t),f9r=r(U4,":"),U4.forEach(t),m9r=i(Ba),Ce=n(Ba,"UL",{});var ro=s(Ce);wC=n(ro,"LI",{});var k0e=s(wC);N1e=n(k0e,"STRONG",{});var Qgt=s(N1e);g9r=r(Qgt,"albert"),Qgt.forEach(t),h9r=r(k0e," \u2014 "),_X=n(k0e,"A",{href:!0});var Hgt=s(_X);p9r=r(Hgt,"FlaxAlbertForQuestionAnswering"),Hgt.forEach(t),_9r=r(k0e," (ALBERT model)"),k0e.forEach(t),u9r=i(ro),AC=n(ro,"LI",{});var x0e=s(AC);D1e=n(x0e,"STRONG",{});var Ugt=s(D1e);b9r=r(Ugt,"bart"),Ugt.forEach(t),v9r=r(x0e," \u2014 "),uX=n(x0e,"A",{href:!0});var Jgt=s(uX);T9r=r(Jgt,"FlaxBartForQuestionAnswering"),Jgt.forEach(t),F9r=r(x0e," (BART model)"),x0e.forEach(t),C9r=i(ro),LC=n(ro,"LI",{});var R0e=s(LC);q1e=n(R0e,"STRONG",{});var Ygt=s(q1e);M9r=r(Ygt,"bert"),Ygt.forEach(t),E9r=r(R0e," \u2014 "),bX=n(R0e,"A",{href:!0});var Kgt=s(bX);y9r=r(Kgt,"FlaxBertForQuestionAnswering"),Kgt.forEach(t),w9r=r(R0e," (BERT model)"),R0e.forEach(t),A9r=i(ro),BC=n(ro,"LI",{});var S0e=s(BC);G1e=n(S0e,"STRONG",{});var Zgt=s(G1e);L9r=r(Zgt,"big_bird"),Zgt.forEach(t),B9r=r(S0e," \u2014 "),vX=n(S0e,"A",{href:!0});var eht=s(vX);k9r=r(eht,"FlaxBigBirdForQuestionAnswering"),eht.forEach(t),x9r=r(S0e," (BigBird model)"),S0e.forEach(t),R9r=i(ro),kC=n(ro,"LI",{});var P0e=s(kC);O1e=n(P0e,"STRONG",{});var oht=s(O1e);S9r=r(oht,"distilbert"),oht.forEach(t),P9r=r(P0e," \u2014 "),TX=n(P0e,"A",{href:!0});var rht=s(TX);$9r=r(rht,"FlaxDistilBertForQuestionAnswering"),rht.forEach(t),I9r=r(P0e," (DistilBERT model)"),P0e.forEach(t),j9r=i(ro),xC=n(ro,"LI",{});var $0e=s(xC);X1e=n($0e,"STRONG",{});var tht=s(X1e);N9r=r(tht,"electra"),tht.forEach(t),D9r=r($0e," \u2014 "),FX=n($0e,"A",{href:!0});var aht=s(FX);q9r=r(aht,"FlaxElectraForQuestionAnswering"),aht.forEach(t),G9r=r($0e," (ELECTRA model)"),$0e.forEach(t),O9r=i(ro),RC=n(ro,"LI",{});var I0e=s(RC);z1e=n(I0e,"STRONG",{});var nht=s(z1e);X9r=r(nht,"mbart"),nht.forEach(t),z9r=r(I0e," \u2014 "),CX=n(I0e,"A",{href:!0});var sht=s(CX);V9r=r(sht,"FlaxMBartForQuestionAnswering"),sht.forEach(t),W9r=r(I0e," (mBART model)"),I0e.forEach(t),Q9r=i(ro),SC=n(ro,"LI",{});var j0e=s(SC);V1e=n(j0e,"STRONG",{});var lht=s(V1e);H9r=r(lht,"roberta"),lht.forEach(t),U9r=r(j0e," \u2014 "),MX=n(j0e,"A",{href:!0});var iht=s(MX);J9r=r(iht,"FlaxRobertaForQuestionAnswering"),iht.forEach(t),Y9r=r(j0e," (RoBERTa model)"),j0e.forEach(t),K9r=i(ro),PC=n(ro,"LI",{});var N0e=s(PC);W1e=n(N0e,"STRONG",{});var dht=s(W1e);Z9r=r(dht,"roformer"),dht.forEach(t),eCr=r(N0e," \u2014 "),EX=n(N0e,"A",{href:!0});var cht=s(EX);oCr=r(cht,"FlaxRoFormerForQuestionAnswering"),cht.forEach(t),rCr=r(N0e," (RoFormer model)"),N0e.forEach(t),ro.forEach(t),tCr=i(Ba),Q1e=n(Ba,"P",{});var fht=s(Q1e);aCr=r(fht,"Examples:"),fht.forEach(t),nCr=i(Ba),m(s6.$$.fragment,Ba),Ba.forEach(t),gi.forEach(t),W8e=i(d),lf=n(d,"H2",{class:!0});var oxe=s(lf);$C=n(oxe,"A",{id:!0,class:!0,href:!0});var mht=s($C);H1e=n(mht,"SPAN",{});var ght=s(H1e);m(l6.$$.fragment,ght),ght.forEach(t),mht.forEach(t),sCr=i(oxe),U1e=n(oxe,"SPAN",{});var hht=s(U1e);lCr=r(hht,"FlaxAutoModelForTokenClassification"),hht.forEach(t),oxe.forEach(t),Q8e=i(d),Sr=n(d,"DIV",{class:!0});var pi=s(Sr);m(i6.$$.fragment,pi),iCr=i(pi),df=n(pi,"P",{});var yV=s(df);dCr=r(yV,`This is a generic model class that will be instantiated as one of the model classes of the library (with a token classification head) when created
with the `),J1e=n(yV,"CODE",{});var pht=s(J1e);cCr=r(pht,"from_pretrained()"),pht.forEach(t),fCr=r(yV,"class method or the "),Y1e=n(yV,"CODE",{});var _ht=s(Y1e);mCr=r(_ht,"from_config()"),_ht.forEach(t),gCr=r(yV,`class
method.`),yV.forEach(t),hCr=i(pi),d6=n(pi,"P",{});var rxe=s(d6);pCr=r(rxe,"This class cannot be instantiated directly using "),K1e=n(rxe,"CODE",{});var uht=s(K1e);_Cr=r(uht,"__init__()"),uht.forEach(t),uCr=r(rxe," (throws an error)."),rxe.forEach(t),bCr=i(pi),wt=n(pi,"DIV",{class:!0});var _i=s(wt);m(c6.$$.fragment,_i),vCr=i(_i),Z1e=n(_i,"P",{});var bht=s(Z1e);TCr=r(bht,"Instantiates one of the model classes of the library (with a token classification head) from a configuration."),bht.forEach(t),FCr=i(_i),cf=n(_i,"P",{});var wV=s(cf);CCr=r(wV,`Note:
Loading a model from its configuration file does `),ebe=n(wV,"STRONG",{});var vht=s(ebe);MCr=r(vht,"not"),vht.forEach(t),ECr=r(wV,` load the model weights. It only affects the
model\u2019s configuration. Use `),obe=n(wV,"CODE",{});var Tht=s(obe);yCr=r(Tht,"from_pretrained()"),Tht.forEach(t),wCr=r(wV,"to load the model weights."),wV.forEach(t),ACr=i(_i),rbe=n(_i,"P",{});var Fht=s(rbe);LCr=r(Fht,"Examples:"),Fht.forEach(t),BCr=i(_i),m(f6.$$.fragment,_i),_i.forEach(t),kCr=i(pi),So=n(pi,"DIV",{class:!0});var ka=s(So);m(m6.$$.fragment,ka),xCr=i(ka),tbe=n(ka,"P",{});var Cht=s(tbe);RCr=r(Cht,"Instantiate one of the model classes of the library (with a token classification head) from a pretrained model."),Cht.forEach(t),SCr=i(ka),Bn=n(ka,"P",{});var J4=s(Bn);PCr=r(J4,"The model class to instantiate is selected based on the "),abe=n(J4,"CODE",{});var Mht=s(abe);$Cr=r(Mht,"model_type"),Mht.forEach(t),ICr=r(J4,` property of the config object (either
passed as an argument or loaded from `),nbe=n(J4,"CODE",{});var Eht=s(nbe);jCr=r(Eht,"pretrained_model_name_or_path"),Eht.forEach(t),NCr=r(J4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),sbe=n(J4,"CODE",{});var yht=s(sbe);DCr=r(yht,"pretrained_model_name_or_path"),yht.forEach(t),qCr=r(J4,":"),J4.forEach(t),GCr=i(ka),so=n(ka,"UL",{});var ta=s(so);IC=n(ta,"LI",{});var D0e=s(IC);lbe=n(D0e,"STRONG",{});var wht=s(lbe);OCr=r(wht,"albert"),wht.forEach(t),XCr=r(D0e," \u2014 "),yX=n(D0e,"A",{href:!0});var Aht=s(yX);zCr=r(Aht,"FlaxAlbertForTokenClassification"),Aht.forEach(t),VCr=r(D0e," (ALBERT model)"),D0e.forEach(t),WCr=i(ta),jC=n(ta,"LI",{});var q0e=s(jC);ibe=n(q0e,"STRONG",{});var Lht=s(ibe);QCr=r(Lht,"bert"),Lht.forEach(t),HCr=r(q0e," \u2014 "),wX=n(q0e,"A",{href:!0});var Bht=s(wX);UCr=r(Bht,"FlaxBertForTokenClassification"),Bht.forEach(t),JCr=r(q0e," (BERT model)"),q0e.forEach(t),YCr=i(ta),NC=n(ta,"LI",{});var G0e=s(NC);dbe=n(G0e,"STRONG",{});var kht=s(dbe);KCr=r(kht,"big_bird"),kht.forEach(t),ZCr=r(G0e," \u2014 "),AX=n(G0e,"A",{href:!0});var xht=s(AX);e4r=r(xht,"FlaxBigBirdForTokenClassification"),xht.forEach(t),o4r=r(G0e," (BigBird model)"),G0e.forEach(t),r4r=i(ta),DC=n(ta,"LI",{});var O0e=s(DC);cbe=n(O0e,"STRONG",{});var Rht=s(cbe);t4r=r(Rht,"distilbert"),Rht.forEach(t),a4r=r(O0e," \u2014 "),LX=n(O0e,"A",{href:!0});var Sht=s(LX);n4r=r(Sht,"FlaxDistilBertForTokenClassification"),Sht.forEach(t),s4r=r(O0e," (DistilBERT model)"),O0e.forEach(t),l4r=i(ta),qC=n(ta,"LI",{});var X0e=s(qC);fbe=n(X0e,"STRONG",{});var Pht=s(fbe);i4r=r(Pht,"electra"),Pht.forEach(t),d4r=r(X0e," \u2014 "),BX=n(X0e,"A",{href:!0});var $ht=s(BX);c4r=r($ht,"FlaxElectraForTokenClassification"),$ht.forEach(t),f4r=r(X0e," (ELECTRA model)"),X0e.forEach(t),m4r=i(ta),GC=n(ta,"LI",{});var z0e=s(GC);mbe=n(z0e,"STRONG",{});var Iht=s(mbe);g4r=r(Iht,"roberta"),Iht.forEach(t),h4r=r(z0e," \u2014 "),kX=n(z0e,"A",{href:!0});var jht=s(kX);p4r=r(jht,"FlaxRobertaForTokenClassification"),jht.forEach(t),_4r=r(z0e," (RoBERTa model)"),z0e.forEach(t),u4r=i(ta),OC=n(ta,"LI",{});var V0e=s(OC);gbe=n(V0e,"STRONG",{});var Nht=s(gbe);b4r=r(Nht,"roformer"),Nht.forEach(t),v4r=r(V0e," \u2014 "),xX=n(V0e,"A",{href:!0});var Dht=s(xX);T4r=r(Dht,"FlaxRoFormerForTokenClassification"),Dht.forEach(t),F4r=r(V0e," (RoFormer model)"),V0e.forEach(t),ta.forEach(t),C4r=i(ka),hbe=n(ka,"P",{});var qht=s(hbe);M4r=r(qht,"Examples:"),qht.forEach(t),E4r=i(ka),m(g6.$$.fragment,ka),ka.forEach(t),pi.forEach(t),H8e=i(d),ff=n(d,"H2",{class:!0});var txe=s(ff);XC=n(txe,"A",{id:!0,class:!0,href:!0});var Ght=s(XC);pbe=n(Ght,"SPAN",{});var Oht=s(pbe);m(h6.$$.fragment,Oht),Oht.forEach(t),Ght.forEach(t),y4r=i(txe),_be=n(txe,"SPAN",{});var Xht=s(_be);w4r=r(Xht,"FlaxAutoModelForMultipleChoice"),Xht.forEach(t),txe.forEach(t),U8e=i(d),Pr=n(d,"DIV",{class:!0});var ui=s(Pr);m(p6.$$.fragment,ui),A4r=i(ui),mf=n(ui,"P",{});var AV=s(mf);L4r=r(AV,`This is a generic model class that will be instantiated as one of the model classes of the library (with a multiple choice head) when created
with the `),ube=n(AV,"CODE",{});var zht=s(ube);B4r=r(zht,"from_pretrained()"),zht.forEach(t),k4r=r(AV,"class method or the "),bbe=n(AV,"CODE",{});var Vht=s(bbe);x4r=r(Vht,"from_config()"),Vht.forEach(t),R4r=r(AV,`class
method.`),AV.forEach(t),S4r=i(ui),_6=n(ui,"P",{});var axe=s(_6);P4r=r(axe,"This class cannot be instantiated directly using "),vbe=n(axe,"CODE",{});var Wht=s(vbe);$4r=r(Wht,"__init__()"),Wht.forEach(t),I4r=r(axe," (throws an error)."),axe.forEach(t),j4r=i(ui),At=n(ui,"DIV",{class:!0});var bi=s(At);m(u6.$$.fragment,bi),N4r=i(bi),Tbe=n(bi,"P",{});var Qht=s(Tbe);D4r=r(Qht,"Instantiates one of the model classes of the library (with a multiple choice head) from a configuration."),Qht.forEach(t),q4r=i(bi),gf=n(bi,"P",{});var LV=s(gf);G4r=r(LV,`Note:
Loading a model from its configuration file does `),Fbe=n(LV,"STRONG",{});var Hht=s(Fbe);O4r=r(Hht,"not"),Hht.forEach(t),X4r=r(LV,` load the model weights. It only affects the
model\u2019s configuration. Use `),Cbe=n(LV,"CODE",{});var Uht=s(Cbe);z4r=r(Uht,"from_pretrained()"),Uht.forEach(t),V4r=r(LV,"to load the model weights."),LV.forEach(t),W4r=i(bi),Mbe=n(bi,"P",{});var Jht=s(Mbe);Q4r=r(Jht,"Examples:"),Jht.forEach(t),H4r=i(bi),m(b6.$$.fragment,bi),bi.forEach(t),U4r=i(ui),Po=n(ui,"DIV",{class:!0});var xa=s(Po);m(v6.$$.fragment,xa),J4r=i(xa),Ebe=n(xa,"P",{});var Yht=s(Ebe);Y4r=r(Yht,"Instantiate one of the model classes of the library (with a multiple choice head) from a pretrained model."),Yht.forEach(t),K4r=i(xa),kn=n(xa,"P",{});var Y4=s(kn);Z4r=r(Y4,"The model class to instantiate is selected based on the "),ybe=n(Y4,"CODE",{});var Kht=s(ybe);eMr=r(Kht,"model_type"),Kht.forEach(t),oMr=r(Y4,` property of the config object (either
passed as an argument or loaded from `),wbe=n(Y4,"CODE",{});var Zht=s(wbe);rMr=r(Zht,"pretrained_model_name_or_path"),Zht.forEach(t),tMr=r(Y4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Abe=n(Y4,"CODE",{});var ept=s(Abe);aMr=r(ept,"pretrained_model_name_or_path"),ept.forEach(t),nMr=r(Y4,":"),Y4.forEach(t),sMr=i(xa),lo=n(xa,"UL",{});var aa=s(lo);zC=n(aa,"LI",{});var W0e=s(zC);Lbe=n(W0e,"STRONG",{});var opt=s(Lbe);lMr=r(opt,"albert"),opt.forEach(t),iMr=r(W0e," \u2014 "),RX=n(W0e,"A",{href:!0});var rpt=s(RX);dMr=r(rpt,"FlaxAlbertForMultipleChoice"),rpt.forEach(t),cMr=r(W0e," (ALBERT model)"),W0e.forEach(t),fMr=i(aa),VC=n(aa,"LI",{});var Q0e=s(VC);Bbe=n(Q0e,"STRONG",{});var tpt=s(Bbe);mMr=r(tpt,"bert"),tpt.forEach(t),gMr=r(Q0e," \u2014 "),SX=n(Q0e,"A",{href:!0});var apt=s(SX);hMr=r(apt,"FlaxBertForMultipleChoice"),apt.forEach(t),pMr=r(Q0e," (BERT model)"),Q0e.forEach(t),_Mr=i(aa),WC=n(aa,"LI",{});var H0e=s(WC);kbe=n(H0e,"STRONG",{});var npt=s(kbe);uMr=r(npt,"big_bird"),npt.forEach(t),bMr=r(H0e," \u2014 "),PX=n(H0e,"A",{href:!0});var spt=s(PX);vMr=r(spt,"FlaxBigBirdForMultipleChoice"),spt.forEach(t),TMr=r(H0e," (BigBird model)"),H0e.forEach(t),FMr=i(aa),QC=n(aa,"LI",{});var U0e=s(QC);xbe=n(U0e,"STRONG",{});var lpt=s(xbe);CMr=r(lpt,"distilbert"),lpt.forEach(t),MMr=r(U0e," \u2014 "),$X=n(U0e,"A",{href:!0});var ipt=s($X);EMr=r(ipt,"FlaxDistilBertForMultipleChoice"),ipt.forEach(t),yMr=r(U0e," (DistilBERT model)"),U0e.forEach(t),wMr=i(aa),HC=n(aa,"LI",{});var J0e=s(HC);Rbe=n(J0e,"STRONG",{});var dpt=s(Rbe);AMr=r(dpt,"electra"),dpt.forEach(t),LMr=r(J0e," \u2014 "),IX=n(J0e,"A",{href:!0});var cpt=s(IX);BMr=r(cpt,"FlaxElectraForMultipleChoice"),cpt.forEach(t),kMr=r(J0e," (ELECTRA model)"),J0e.forEach(t),xMr=i(aa),UC=n(aa,"LI",{});var Y0e=s(UC);Sbe=n(Y0e,"STRONG",{});var fpt=s(Sbe);RMr=r(fpt,"roberta"),fpt.forEach(t),SMr=r(Y0e," \u2014 "),jX=n(Y0e,"A",{href:!0});var mpt=s(jX);PMr=r(mpt,"FlaxRobertaForMultipleChoice"),mpt.forEach(t),$Mr=r(Y0e," (RoBERTa model)"),Y0e.forEach(t),IMr=i(aa),JC=n(aa,"LI",{});var K0e=s(JC);Pbe=n(K0e,"STRONG",{});var gpt=s(Pbe);jMr=r(gpt,"roformer"),gpt.forEach(t),NMr=r(K0e," \u2014 "),NX=n(K0e,"A",{href:!0});var hpt=s(NX);DMr=r(hpt,"FlaxRoFormerForMultipleChoice"),hpt.forEach(t),qMr=r(K0e," (RoFormer model)"),K0e.forEach(t),aa.forEach(t),GMr=i(xa),$be=n(xa,"P",{});var ppt=s($be);OMr=r(ppt,"Examples:"),ppt.forEach(t),XMr=i(xa),m(T6.$$.fragment,xa),xa.forEach(t),ui.forEach(t),J8e=i(d),hf=n(d,"H2",{class:!0});var nxe=s(hf);YC=n(nxe,"A",{id:!0,class:!0,href:!0});var _pt=s(YC);Ibe=n(_pt,"SPAN",{});var upt=s(Ibe);m(F6.$$.fragment,upt),upt.forEach(t),_pt.forEach(t),zMr=i(nxe),jbe=n(nxe,"SPAN",{});var bpt=s(jbe);VMr=r(bpt,"FlaxAutoModelForNextSentencePrediction"),bpt.forEach(t),nxe.forEach(t),Y8e=i(d),$r=n(d,"DIV",{class:!0});var vi=s($r);m(C6.$$.fragment,vi),WMr=i(vi),pf=n(vi,"P",{});var BV=s(pf);QMr=r(BV,`This is a generic model class that will be instantiated as one of the model classes of the library (with a next sentence prediction head) when created
with the `),Nbe=n(BV,"CODE",{});var vpt=s(Nbe);HMr=r(vpt,"from_pretrained()"),vpt.forEach(t),UMr=r(BV,"class method or the "),Dbe=n(BV,"CODE",{});var Tpt=s(Dbe);JMr=r(Tpt,"from_config()"),Tpt.forEach(t),YMr=r(BV,`class
method.`),BV.forEach(t),KMr=i(vi),M6=n(vi,"P",{});var sxe=s(M6);ZMr=r(sxe,"This class cannot be instantiated directly using "),qbe=n(sxe,"CODE",{});var Fpt=s(qbe);eEr=r(Fpt,"__init__()"),Fpt.forEach(t),oEr=r(sxe," (throws an error)."),sxe.forEach(t),rEr=i(vi),Lt=n(vi,"DIV",{class:!0});var Ti=s(Lt);m(E6.$$.fragment,Ti),tEr=i(Ti),Gbe=n(Ti,"P",{});var Cpt=s(Gbe);aEr=r(Cpt,"Instantiates one of the model classes of the library (with a next sentence prediction head) from a configuration."),Cpt.forEach(t),nEr=i(Ti),_f=n(Ti,"P",{});var kV=s(_f);sEr=r(kV,`Note:
Loading a model from its configuration file does `),Obe=n(kV,"STRONG",{});var Mpt=s(Obe);lEr=r(Mpt,"not"),Mpt.forEach(t),iEr=r(kV,` load the model weights. It only affects the
model\u2019s configuration. Use `),Xbe=n(kV,"CODE",{});var Ept=s(Xbe);dEr=r(Ept,"from_pretrained()"),Ept.forEach(t),cEr=r(kV,"to load the model weights."),kV.forEach(t),fEr=i(Ti),zbe=n(Ti,"P",{});var ypt=s(zbe);mEr=r(ypt,"Examples:"),ypt.forEach(t),gEr=i(Ti),m(y6.$$.fragment,Ti),Ti.forEach(t),hEr=i(vi),$o=n(vi,"DIV",{class:!0});var Ra=s($o);m(w6.$$.fragment,Ra),pEr=i(Ra),Vbe=n(Ra,"P",{});var wpt=s(Vbe);_Er=r(wpt,"Instantiate one of the model classes of the library (with a next sentence prediction head) from a pretrained model."),wpt.forEach(t),uEr=i(Ra),xn=n(Ra,"P",{});var K4=s(xn);bEr=r(K4,"The model class to instantiate is selected based on the "),Wbe=n(K4,"CODE",{});var Apt=s(Wbe);vEr=r(Apt,"model_type"),Apt.forEach(t),TEr=r(K4,` property of the config object (either
passed as an argument or loaded from `),Qbe=n(K4,"CODE",{});var Lpt=s(Qbe);FEr=r(Lpt,"pretrained_model_name_or_path"),Lpt.forEach(t),CEr=r(K4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Hbe=n(K4,"CODE",{});var Bpt=s(Hbe);MEr=r(Bpt,"pretrained_model_name_or_path"),Bpt.forEach(t),EEr=r(K4,":"),K4.forEach(t),yEr=i(Ra),Ube=n(Ra,"UL",{});var kpt=s(Ube);KC=n(kpt,"LI",{});var Z0e=s(KC);Jbe=n(Z0e,"STRONG",{});var xpt=s(Jbe);wEr=r(xpt,"bert"),xpt.forEach(t),AEr=r(Z0e," \u2014 "),DX=n(Z0e,"A",{href:!0});var Rpt=s(DX);LEr=r(Rpt,"FlaxBertForNextSentencePrediction"),Rpt.forEach(t),BEr=r(Z0e," (BERT model)"),Z0e.forEach(t),kpt.forEach(t),kEr=i(Ra),Ybe=n(Ra,"P",{});var Spt=s(Ybe);xEr=r(Spt,"Examples:"),Spt.forEach(t),REr=i(Ra),m(A6.$$.fragment,Ra),Ra.forEach(t),vi.forEach(t),K8e=i(d),uf=n(d,"H2",{class:!0});var lxe=s(uf);ZC=n(lxe,"A",{id:!0,class:!0,href:!0});var Ppt=s(ZC);Kbe=n(Ppt,"SPAN",{});var $pt=s(Kbe);m(L6.$$.fragment,$pt),$pt.forEach(t),Ppt.forEach(t),SEr=i(lxe),Zbe=n(lxe,"SPAN",{});var Ipt=s(Zbe);PEr=r(Ipt,"FlaxAutoModelForImageClassification"),Ipt.forEach(t),lxe.forEach(t),Z8e=i(d),Ir=n(d,"DIV",{class:!0});var Fi=s(Ir);m(B6.$$.fragment,Fi),$Er=i(Fi),bf=n(Fi,"P",{});var xV=s(bf);IEr=r(xV,`This is a generic model class that will be instantiated as one of the model classes of the library (with a image classification head) when created
with the `),e5e=n(xV,"CODE",{});var jpt=s(e5e);jEr=r(jpt,"from_pretrained()"),jpt.forEach(t),NEr=r(xV,"class method or the "),o5e=n(xV,"CODE",{});var Npt=s(o5e);DEr=r(Npt,"from_config()"),Npt.forEach(t),qEr=r(xV,`class
method.`),xV.forEach(t),GEr=i(Fi),k6=n(Fi,"P",{});var ixe=s(k6);OEr=r(ixe,"This class cannot be instantiated directly using "),r5e=n(ixe,"CODE",{});var Dpt=s(r5e);XEr=r(Dpt,"__init__()"),Dpt.forEach(t),zEr=r(ixe," (throws an error)."),ixe.forEach(t),VEr=i(Fi),Bt=n(Fi,"DIV",{class:!0});var Ci=s(Bt);m(x6.$$.fragment,Ci),WEr=i(Ci),t5e=n(Ci,"P",{});var qpt=s(t5e);QEr=r(qpt,"Instantiates one of the model classes of the library (with a image classification head) from a configuration."),qpt.forEach(t),HEr=i(Ci),vf=n(Ci,"P",{});var RV=s(vf);UEr=r(RV,`Note:
Loading a model from its configuration file does `),a5e=n(RV,"STRONG",{});var Gpt=s(a5e);JEr=r(Gpt,"not"),Gpt.forEach(t),YEr=r(RV,` load the model weights. It only affects the
model\u2019s configuration. Use `),n5e=n(RV,"CODE",{});var Opt=s(n5e);KEr=r(Opt,"from_pretrained()"),Opt.forEach(t),ZEr=r(RV,"to load the model weights."),RV.forEach(t),e3r=i(Ci),s5e=n(Ci,"P",{});var Xpt=s(s5e);o3r=r(Xpt,"Examples:"),Xpt.forEach(t),r3r=i(Ci),m(R6.$$.fragment,Ci),Ci.forEach(t),t3r=i(Fi),Io=n(Fi,"DIV",{class:!0});var Sa=s(Io);m(S6.$$.fragment,Sa),a3r=i(Sa),l5e=n(Sa,"P",{});var zpt=s(l5e);n3r=r(zpt,"Instantiate one of the model classes of the library (with a image classification head) from a pretrained model."),zpt.forEach(t),s3r=i(Sa),Rn=n(Sa,"P",{});var Z4=s(Rn);l3r=r(Z4,"The model class to instantiate is selected based on the "),i5e=n(Z4,"CODE",{});var Vpt=s(i5e);i3r=r(Vpt,"model_type"),Vpt.forEach(t),d3r=r(Z4,` property of the config object (either
passed as an argument or loaded from `),d5e=n(Z4,"CODE",{});var Wpt=s(d5e);c3r=r(Wpt,"pretrained_model_name_or_path"),Wpt.forEach(t),f3r=r(Z4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),c5e=n(Z4,"CODE",{});var Qpt=s(c5e);m3r=r(Qpt,"pretrained_model_name_or_path"),Qpt.forEach(t),g3r=r(Z4,":"),Z4.forEach(t),h3r=i(Sa),P6=n(Sa,"UL",{});var dxe=s(P6);e4=n(dxe,"LI",{});var eLe=s(e4);f5e=n(eLe,"STRONG",{});var Hpt=s(f5e);p3r=r(Hpt,"beit"),Hpt.forEach(t),_3r=r(eLe," \u2014 "),qX=n(eLe,"A",{href:!0});var Upt=s(qX);u3r=r(Upt,"FlaxBeitForImageClassification"),Upt.forEach(t),b3r=r(eLe," (BEiT model)"),eLe.forEach(t),v3r=i(dxe),o4=n(dxe,"LI",{});var oLe=s(o4);m5e=n(oLe,"STRONG",{});var Jpt=s(m5e);T3r=r(Jpt,"vit"),Jpt.forEach(t),F3r=r(oLe," \u2014 "),GX=n(oLe,"A",{href:!0});var Ypt=s(GX);C3r=r(Ypt,"FlaxViTForImageClassification"),Ypt.forEach(t),M3r=r(oLe," (ViT model)"),oLe.forEach(t),dxe.forEach(t),E3r=i(Sa),g5e=n(Sa,"P",{});var Kpt=s(g5e);y3r=r(Kpt,"Examples:"),Kpt.forEach(t),w3r=i(Sa),m($6.$$.fragment,Sa),Sa.forEach(t),Fi.forEach(t),eBe=i(d),Tf=n(d,"H2",{class:!0});var cxe=s(Tf);r4=n(cxe,"A",{id:!0,class:!0,href:!0});var Zpt=s(r4);h5e=n(Zpt,"SPAN",{});var e_t=s(h5e);m(I6.$$.fragment,e_t),e_t.forEach(t),Zpt.forEach(t),A3r=i(cxe),p5e=n(cxe,"SPAN",{});var o_t=s(p5e);L3r=r(o_t,"FlaxAutoModelForVision2Seq"),o_t.forEach(t),cxe.forEach(t),oBe=i(d),jr=n(d,"DIV",{class:!0});var Mi=s(jr);m(j6.$$.fragment,Mi),B3r=i(Mi),Ff=n(Mi,"P",{});var SV=s(Ff);k3r=r(SV,`This is a generic model class that will be instantiated as one of the model classes of the library (with a vision-to-text modeling head) when created
with the `),_5e=n(SV,"CODE",{});var r_t=s(_5e);x3r=r(r_t,"from_pretrained()"),r_t.forEach(t),R3r=r(SV,"class method or the "),u5e=n(SV,"CODE",{});var t_t=s(u5e);S3r=r(t_t,"from_config()"),t_t.forEach(t),P3r=r(SV,`class
method.`),SV.forEach(t),$3r=i(Mi),N6=n(Mi,"P",{});var fxe=s(N6);I3r=r(fxe,"This class cannot be instantiated directly using "),b5e=n(fxe,"CODE",{});var a_t=s(b5e);j3r=r(a_t,"__init__()"),a_t.forEach(t),N3r=r(fxe," (throws an error)."),fxe.forEach(t),D3r=i(Mi),kt=n(Mi,"DIV",{class:!0});var Ei=s(kt);m(D6.$$.fragment,Ei),q3r=i(Ei),v5e=n(Ei,"P",{});var n_t=s(v5e);G3r=r(n_t,"Instantiates one of the model classes of the library (with a vision-to-text modeling head) from a configuration."),n_t.forEach(t),O3r=i(Ei),Cf=n(Ei,"P",{});var PV=s(Cf);X3r=r(PV,`Note:
Loading a model from its configuration file does `),T5e=n(PV,"STRONG",{});var s_t=s(T5e);z3r=r(s_t,"not"),s_t.forEach(t),V3r=r(PV,` load the model weights. It only affects the
model\u2019s configuration. Use `),F5e=n(PV,"CODE",{});var l_t=s(F5e);W3r=r(l_t,"from_pretrained()"),l_t.forEach(t),Q3r=r(PV,"to load the model weights."),PV.forEach(t),H3r=i(Ei),C5e=n(Ei,"P",{});var i_t=s(C5e);U3r=r(i_t,"Examples:"),i_t.forEach(t),J3r=i(Ei),m(q6.$$.fragment,Ei),Ei.forEach(t),Y3r=i(Mi),jo=n(Mi,"DIV",{class:!0});var Pa=s(jo);m(G6.$$.fragment,Pa),K3r=i(Pa),M5e=n(Pa,"P",{});var d_t=s(M5e);Z3r=r(d_t,"Instantiate one of the model classes of the library (with a vision-to-text modeling head) from a pretrained model."),d_t.forEach(t),eyr=i(Pa),Sn=n(Pa,"P",{});var eM=s(Sn);oyr=r(eM,"The model class to instantiate is selected based on the "),E5e=n(eM,"CODE",{});var c_t=s(E5e);ryr=r(c_t,"model_type"),c_t.forEach(t),tyr=r(eM,` property of the config object (either
passed as an argument or loaded from `),y5e=n(eM,"CODE",{});var f_t=s(y5e);ayr=r(f_t,"pretrained_model_name_or_path"),f_t.forEach(t),nyr=r(eM,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),w5e=n(eM,"CODE",{});var m_t=s(w5e);syr=r(m_t,"pretrained_model_name_or_path"),m_t.forEach(t),lyr=r(eM,":"),eM.forEach(t),iyr=i(Pa),A5e=n(Pa,"UL",{});var g_t=s(A5e);t4=n(g_t,"LI",{});var rLe=s(t4);L5e=n(rLe,"STRONG",{});var h_t=s(L5e);dyr=r(h_t,"vision-encoder-decoder"),h_t.forEach(t),cyr=r(rLe," \u2014 "),OX=n(rLe,"A",{href:!0});var p_t=s(OX);fyr=r(p_t,"FlaxVisionEncoderDecoderModel"),p_t.forEach(t),myr=r(rLe," (Vision Encoder decoder model)"),rLe.forEach(t),g_t.forEach(t),gyr=i(Pa),B5e=n(Pa,"P",{});var __t=s(B5e);hyr=r(__t,"Examples:"),__t.forEach(t),pyr=i(Pa),m(O6.$$.fragment,Pa),Pa.forEach(t),Mi.forEach(t),this.h()},h(){c(J,"name","hf:doc:metadata"),c(J,"content",JSON.stringify(y_t)),c(me,"id","auto-classes"),c(me,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(me,"href","#auto-classes"),c(ie,"class","relative group"),c(Pn,"href","/docs/transformers/pr_15792/en/model_doc/auto#transformers.AutoConfig"),c(In,"href","/docs/transformers/pr_15792/en/model_doc/auto#transformers.AutoModel"),c(jn,"href","/docs/transformers/pr_15792/en/model_doc/auto#transformers.AutoTokenizer"),c(Ri,"href","/docs/transformers/pr_15792/en/model_doc/bert#transformers.BertModel"),c(Lf,"id","extending-the-auto-classes"),c(Lf,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Lf,"href","#extending-the-auto-classes"),c(Si,"class","relative group"),c(kf,"id","transformers.AutoConfig"),c(kf,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(kf,"href","#transformers.AutoConfig"),c(Pi,"class","relative group"),c(V0,"href","/docs/transformers/pr_15792/en/model_doc/auto#transformers.AutoConfig.from_pretrained"),c(W0,"href","/docs/transformers/pr_15792/en/model_doc/albert#transformers.AlbertConfig"),c(Q0,"href","/docs/transformers/pr_15792/en/model_doc/bart#transformers.BartConfig"),c(H0,"href","/docs/transformers/pr_15792/en/model_doc/beit#transformers.BeitConfig"),c(U0,"href","/docs/transformers/pr_15792/en/model_doc/bert#transformers.BertConfig"),c(J0,"href","/docs/transformers/pr_15792/en/model_doc/bert-generation#transformers.BertGenerationConfig"),c(Y0,"href","/docs/transformers/pr_15792/en/model_doc/big_bird#transformers.BigBirdConfig"),c(K0,"href","/docs/transformers/pr_15792/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusConfig"),c(Z0,"href","/docs/transformers/pr_15792/en/model_doc/blenderbot#transformers.BlenderbotConfig"),c(eL,"href","/docs/transformers/pr_15792/en/model_doc/blenderbot-small#transformers.BlenderbotSmallConfig"),c(oL,"href","/docs/transformers/pr_15792/en/model_doc/camembert#transformers.CamembertConfig"),c(rL,"href","/docs/transformers/pr_15792/en/model_doc/canine#transformers.CanineConfig"),c(tL,"href","/docs/transformers/pr_15792/en/model_doc/clip#transformers.CLIPConfig"),c(aL,"href","/docs/transformers/pr_15792/en/model_doc/convbert#transformers.ConvBertConfig"),c(nL,"href","/docs/transformers/pr_15792/en/model_doc/convnext#transformers.ConvNextConfig"),c(sL,"href","/docs/transformers/pr_15792/en/model_doc/ctrl#transformers.CTRLConfig"),c(lL,"href","/docs/transformers/pr_15792/en/model_doc/deberta#transformers.DebertaConfig"),c(iL,"href","/docs/transformers/pr_15792/en/model_doc/deberta-v2#transformers.DebertaV2Config"),c(dL,"href","/docs/transformers/pr_15792/en/model_doc/deit#transformers.DeiTConfig"),c(cL,"href","/docs/transformers/pr_15792/en/model_doc/detr#transformers.DetrConfig"),c(fL,"href","/docs/transformers/pr_15792/en/model_doc/distilbert#transformers.DistilBertConfig"),c(mL,"href","/docs/transformers/pr_15792/en/model_doc/dpr#transformers.DPRConfig"),c(gL,"href","/docs/transformers/pr_15792/en/model_doc/electra#transformers.ElectraConfig"),c(hL,"href","/docs/transformers/pr_15792/en/model_doc/encoder-decoder#transformers.EncoderDecoderConfig"),c(pL,"href","/docs/transformers/pr_15792/en/model_doc/flaubert#transformers.FlaubertConfig"),c(_L,"href","/docs/transformers/pr_15792/en/model_doc/fnet#transformers.FNetConfig"),c(uL,"href","/docs/transformers/pr_15792/en/model_doc/fsmt#transformers.FSMTConfig"),c(bL,"href","/docs/transformers/pr_15792/en/model_doc/funnel#transformers.FunnelConfig"),c(vL,"href","/docs/transformers/pr_15792/en/model_doc/gpt2#transformers.GPT2Config"),c(TL,"href","/docs/transformers/pr_15792/en/model_doc/gpt_neo#transformers.GPTNeoConfig"),c(FL,"href","/docs/transformers/pr_15792/en/model_doc/gptj#transformers.GPTJConfig"),c(CL,"href","/docs/transformers/pr_15792/en/model_doc/hubert#transformers.HubertConfig"),c(ML,"href","/docs/transformers/pr_15792/en/model_doc/ibert#transformers.IBertConfig"),c(EL,"href","/docs/transformers/pr_15792/en/model_doc/imagegpt#transformers.ImageGPTConfig"),c(yL,"href","/docs/transformers/pr_15792/en/model_doc/layoutlm#transformers.LayoutLMConfig"),c(wL,"href","/docs/transformers/pr_15792/en/model_doc/layoutlmv2#transformers.LayoutLMv2Config"),c(AL,"href","/docs/transformers/pr_15792/en/model_doc/led#transformers.LEDConfig"),c(LL,"href","/docs/transformers/pr_15792/en/model_doc/longformer#transformers.LongformerConfig"),c(BL,"href","/docs/transformers/pr_15792/en/model_doc/luke#transformers.LukeConfig"),c(kL,"href","/docs/transformers/pr_15792/en/model_doc/lxmert#transformers.LxmertConfig"),c(xL,"href","/docs/transformers/pr_15792/en/model_doc/m2m_100#transformers.M2M100Config"),c(RL,"href","/docs/transformers/pr_15792/en/model_doc/marian#transformers.MarianConfig"),c(SL,"href","/docs/transformers/pr_15792/en/model_doc/mbart#transformers.MBartConfig"),c(PL,"href","/docs/transformers/pr_15792/en/model_doc/megatron-bert#transformers.MegatronBertConfig"),c($L,"href","/docs/transformers/pr_15792/en/model_doc/mobilebert#transformers.MobileBertConfig"),c(IL,"href","/docs/transformers/pr_15792/en/model_doc/mpnet#transformers.MPNetConfig"),c(jL,"href","/docs/transformers/pr_15792/en/model_doc/mt5#transformers.MT5Config"),c(NL,"href","/docs/transformers/pr_15792/en/model_doc/nystromformer#transformers.NystromformerConfig"),c(DL,"href","/docs/transformers/pr_15792/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig"),c(qL,"href","/docs/transformers/pr_15792/en/model_doc/pegasus#transformers.PegasusConfig"),c(GL,"href","/docs/transformers/pr_15792/en/model_doc/perceiver#transformers.PerceiverConfig"),c(OL,"href","/docs/transformers/pr_15792/en/model_doc/plbart#transformers.PLBartConfig"),c(XL,"href","/docs/transformers/pr_15792/en/model_doc/poolformer#transformers.PoolFormerConfig"),c(zL,"href","/docs/transformers/pr_15792/en/model_doc/prophetnet#transformers.ProphetNetConfig"),c(VL,"href","/docs/transformers/pr_15792/en/model_doc/qdqbert#transformers.QDQBertConfig"),c(WL,"href","/docs/transformers/pr_15792/en/model_doc/rag#transformers.RagConfig"),c(QL,"href","/docs/transformers/pr_15792/en/model_doc/realm#transformers.RealmConfig"),c(HL,"href","/docs/transformers/pr_15792/en/model_doc/reformer#transformers.ReformerConfig"),c(UL,"href","/docs/transformers/pr_15792/en/model_doc/rembert#transformers.RemBertConfig"),c(JL,"href","/docs/transformers/pr_15792/en/model_doc/retribert#transformers.RetriBertConfig"),c(YL,"href","/docs/transformers/pr_15792/en/model_doc/roberta#transformers.RobertaConfig"),c(KL,"href","/docs/transformers/pr_15792/en/model_doc/roformer#transformers.RoFormerConfig"),c(ZL,"href","/docs/transformers/pr_15792/en/model_doc/segformer#transformers.SegformerConfig"),c(e8,"href","/docs/transformers/pr_15792/en/model_doc/sew#transformers.SEWConfig"),c(o8,"href","/docs/transformers/pr_15792/en/model_doc/sew-d#transformers.SEWDConfig"),c(r8,"href","/docs/transformers/pr_15792/en/model_doc/speech-encoder-decoder#transformers.SpeechEncoderDecoderConfig"),c(t8,"href","/docs/transformers/pr_15792/en/model_doc/speech_to_text#transformers.Speech2TextConfig"),c(a8,"href","/docs/transformers/pr_15792/en/model_doc/speech_to_text_2#transformers.Speech2Text2Config"),c(n8,"href","/docs/transformers/pr_15792/en/model_doc/splinter#transformers.SplinterConfig"),c(s8,"href","/docs/transformers/pr_15792/en/model_doc/squeezebert#transformers.SqueezeBertConfig"),c(l8,"href","/docs/transformers/pr_15792/en/model_doc/swin#transformers.SwinConfig"),c(i8,"href","/docs/transformers/pr_15792/en/model_doc/t5#transformers.T5Config"),c(d8,"href","/docs/transformers/pr_15792/en/model_doc/tapas#transformers.TapasConfig"),c(c8,"href","/docs/transformers/pr_15792/en/model_doc/transfo-xl#transformers.TransfoXLConfig"),c(f8,"href","/docs/transformers/pr_15792/en/model_doc/trocr#transformers.TrOCRConfig"),c(m8,"href","/docs/transformers/pr_15792/en/model_doc/unispeech#transformers.UniSpeechConfig"),c(g8,"href","/docs/transformers/pr_15792/en/model_doc/unispeech-sat#transformers.UniSpeechSatConfig"),c(h8,"href","/docs/transformers/pr_15792/en/model_doc/vilt#transformers.ViltConfig"),c(p8,"href","/docs/transformers/pr_15792/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderConfig"),c(_8,"href","/docs/transformers/pr_15792/en/model_doc/vision-text-dual-encoder#transformers.VisionTextDualEncoderConfig"),c(u8,"href","/docs/transformers/pr_15792/en/model_doc/visual_bert#transformers.VisualBertConfig"),c(b8,"href","/docs/transformers/pr_15792/en/model_doc/vit#transformers.ViTConfig"),c(v8,"href","/docs/transformers/pr_15792/en/model_doc/vit_mae#transformers.ViTMAEConfig"),c(T8,"href","/docs/transformers/pr_15792/en/model_doc/wav2vec2#transformers.Wav2Vec2Config"),c(F8,"href","/docs/transformers/pr_15792/en/model_doc/wavlm#transformers.WavLMConfig"),c(C8,"href","/docs/transformers/pr_15792/en/model_doc/xglm#transformers.XGLMConfig"),c(M8,"href","/docs/transformers/pr_15792/en/model_doc/xlm#transformers.XLMConfig"),c(E8,"href","/docs/transformers/pr_15792/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetConfig"),c(y8,"href","/docs/transformers/pr_15792/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig"),c(w8,"href","/docs/transformers/pr_15792/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLConfig"),c(A8,"href","/docs/transformers/pr_15792/en/model_doc/xlnet#transformers.XLNetConfig"),c(L8,"href","/docs/transformers/pr_15792/en/model_doc/yoso#transformers.YosoConfig"),c(fo,"class","docstring"),c(hg,"class","docstring"),c(Go,"class","docstring"),c(pg,"id","transformers.AutoTokenizer"),c(pg,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(pg,"href","#transformers.AutoTokenizer"),c(Ii,"class","relative group"),c(B8,"href","/docs/transformers/pr_15792/en/model_doc/auto#transformers.AutoTokenizer.from_pretrained"),c(k8,"href","/docs/transformers/pr_15792/en/model_doc/albert#transformers.AlbertTokenizer"),c(x8,"href","/docs/transformers/pr_15792/en/model_doc/albert#transformers.AlbertTokenizerFast"),c(R8,"href","/docs/transformers/pr_15792/en/model_doc/bart#transformers.BartTokenizer"),c(S8,"href","/docs/transformers/pr_15792/en/model_doc/bart#transformers.BartTokenizerFast"),c(P8,"href","/docs/transformers/pr_15792/en/model_doc/barthez#transformers.BarthezTokenizer"),c($8,"href","/docs/transformers/pr_15792/en/model_doc/barthez#transformers.BarthezTokenizerFast"),c(I8,"href","/docs/transformers/pr_15792/en/model_doc/bartpho#transformers.BartphoTokenizer"),c(j8,"href","/docs/transformers/pr_15792/en/model_doc/bert#transformers.BertTokenizer"),c(N8,"href","/docs/transformers/pr_15792/en/model_doc/bert#transformers.BertTokenizerFast"),c(D8,"href","/docs/transformers/pr_15792/en/model_doc/bert-generation#transformers.BertGenerationTokenizer"),c(q8,"href","/docs/transformers/pr_15792/en/model_doc/bert-japanese#transformers.BertJapaneseTokenizer"),c(G8,"href","/docs/transformers/pr_15792/en/model_doc/bertweet#transformers.BertweetTokenizer"),c(O8,"href","/docs/transformers/pr_15792/en/model_doc/big_bird#transformers.BigBirdTokenizer"),c(X8,"href","/docs/transformers/pr_15792/en/model_doc/big_bird#transformers.BigBirdTokenizerFast"),c(z8,"href","/docs/transformers/pr_15792/en/model_doc/pegasus#transformers.PegasusTokenizer"),c(V8,"href","/docs/transformers/pr_15792/en/model_doc/pegasus#transformers.PegasusTokenizerFast"),c(W8,"href","/docs/transformers/pr_15792/en/model_doc/blenderbot#transformers.BlenderbotTokenizer"),c(Q8,"href","/docs/transformers/pr_15792/en/model_doc/blenderbot#transformers.BlenderbotTokenizerFast"),c(H8,"href","/docs/transformers/pr_15792/en/model_doc/blenderbot-small#transformers.BlenderbotSmallTokenizer"),c(U8,"href","/docs/transformers/pr_15792/en/model_doc/byt5#transformers.ByT5Tokenizer"),c(J8,"href","/docs/transformers/pr_15792/en/model_doc/camembert#transformers.CamembertTokenizer"),c(Y8,"href","/docs/transformers/pr_15792/en/model_doc/camembert#transformers.CamembertTokenizerFast"),c(K8,"href","/docs/transformers/pr_15792/en/model_doc/canine#transformers.CanineTokenizer"),c(Z8,"href","/docs/transformers/pr_15792/en/model_doc/clip#transformers.CLIPTokenizer"),c(eB,"href","/docs/transformers/pr_15792/en/model_doc/clip#transformers.CLIPTokenizerFast"),c(oB,"href","/docs/transformers/pr_15792/en/model_doc/convbert#transformers.ConvBertTokenizer"),c(rB,"href","/docs/transformers/pr_15792/en/model_doc/convbert#transformers.ConvBertTokenizerFast"),c(tB,"href","/docs/transformers/pr_15792/en/model_doc/cpm#transformers.CpmTokenizer"),c(aB,"href","/docs/transformers/pr_15792/en/model_doc/ctrl#transformers.CTRLTokenizer"),c(nB,"href","/docs/transformers/pr_15792/en/model_doc/deberta#transformers.DebertaTokenizer"),c(sB,"href","/docs/transformers/pr_15792/en/model_doc/deberta#transformers.DebertaTokenizerFast"),c(lB,"href","/docs/transformers/pr_15792/en/model_doc/deberta-v2#transformers.DebertaV2Tokenizer"),c(iB,"href","/docs/transformers/pr_15792/en/model_doc/distilbert#transformers.DistilBertTokenizer"),c(dB,"href","/docs/transformers/pr_15792/en/model_doc/distilbert#transformers.DistilBertTokenizerFast"),c(cB,"href","/docs/transformers/pr_15792/en/model_doc/dpr#transformers.DPRQuestionEncoderTokenizer"),c(fB,"href","/docs/transformers/pr_15792/en/model_doc/dpr#transformers.DPRQuestionEncoderTokenizerFast"),c(mB,"href","/docs/transformers/pr_15792/en/model_doc/electra#transformers.ElectraTokenizer"),c(gB,"href","/docs/transformers/pr_15792/en/model_doc/electra#transformers.ElectraTokenizerFast"),c(hB,"href","/docs/transformers/pr_15792/en/model_doc/flaubert#transformers.FlaubertTokenizer"),c(pB,"href","/docs/transformers/pr_15792/en/model_doc/fnet#transformers.FNetTokenizer"),c(_B,"href","/docs/transformers/pr_15792/en/model_doc/fnet#transformers.FNetTokenizerFast"),c(uB,"href","/docs/transformers/pr_15792/en/model_doc/fsmt#transformers.FSMTTokenizer"),c(bB,"href","/docs/transformers/pr_15792/en/model_doc/funnel#transformers.FunnelTokenizer"),c(vB,"href","/docs/transformers/pr_15792/en/model_doc/funnel#transformers.FunnelTokenizerFast"),c(TB,"href","/docs/transformers/pr_15792/en/model_doc/gpt2#transformers.GPT2Tokenizer"),c(FB,"href","/docs/transformers/pr_15792/en/model_doc/gpt2#transformers.GPT2TokenizerFast"),c(CB,"href","/docs/transformers/pr_15792/en/model_doc/gpt2#transformers.GPT2Tokenizer"),c(MB,"href","/docs/transformers/pr_15792/en/model_doc/gpt2#transformers.GPT2TokenizerFast"),c(EB,"href","/docs/transformers/pr_15792/en/model_doc/herbert#transformers.HerbertTokenizer"),c(yB,"href","/docs/transformers/pr_15792/en/model_doc/herbert#transformers.HerbertTokenizerFast"),c(wB,"href","/docs/transformers/pr_15792/en/model_doc/wav2vec2#transformers.Wav2Vec2CTCTokenizer"),c(AB,"href","/docs/transformers/pr_15792/en/model_doc/roberta#transformers.RobertaTokenizer"),c(LB,"href","/docs/transformers/pr_15792/en/model_doc/roberta#transformers.RobertaTokenizerFast"),c(BB,"href","/docs/transformers/pr_15792/en/model_doc/layoutlm#transformers.LayoutLMTokenizer"),c(kB,"href","/docs/transformers/pr_15792/en/model_doc/layoutlm#transformers.LayoutLMTokenizerFast"),c(xB,"href","/docs/transformers/pr_15792/en/model_doc/layoutlmv2#transformers.LayoutLMv2Tokenizer"),c(RB,"href","/docs/transformers/pr_15792/en/model_doc/layoutlmv2#transformers.LayoutLMv2TokenizerFast"),c(SB,"href","/docs/transformers/pr_15792/en/model_doc/layoutxlm#transformers.LayoutXLMTokenizer"),c(PB,"href","/docs/transformers/pr_15792/en/model_doc/layoutxlm#transformers.LayoutXLMTokenizerFast"),c($B,"href","/docs/transformers/pr_15792/en/model_doc/led#transformers.LEDTokenizer"),c(IB,"href","/docs/transformers/pr_15792/en/model_doc/led#transformers.LEDTokenizerFast"),c(jB,"href","/docs/transformers/pr_15792/en/model_doc/longformer#transformers.LongformerTokenizer"),c(NB,"href","/docs/transformers/pr_15792/en/model_doc/longformer#transformers.LongformerTokenizerFast"),c(DB,"href","/docs/transformers/pr_15792/en/model_doc/luke#transformers.LukeTokenizer"),c(qB,"href","/docs/transformers/pr_15792/en/model_doc/lxmert#transformers.LxmertTokenizer"),c(GB,"href","/docs/transformers/pr_15792/en/model_doc/lxmert#transformers.LxmertTokenizerFast"),c(OB,"href","/docs/transformers/pr_15792/en/model_doc/m2m_100#transformers.M2M100Tokenizer"),c(XB,"href","/docs/transformers/pr_15792/en/model_doc/marian#transformers.MarianTokenizer"),c(zB,"href","/docs/transformers/pr_15792/en/model_doc/mbart#transformers.MBartTokenizer"),c(VB,"href","/docs/transformers/pr_15792/en/model_doc/mbart#transformers.MBartTokenizerFast"),c(WB,"href","/docs/transformers/pr_15792/en/model_doc/mbart#transformers.MBart50Tokenizer"),c(QB,"href","/docs/transformers/pr_15792/en/model_doc/mbart#transformers.MBart50TokenizerFast"),c(HB,"href","/docs/transformers/pr_15792/en/model_doc/mluke#transformers.MLukeTokenizer"),c(UB,"href","/docs/transformers/pr_15792/en/model_doc/mobilebert#transformers.MobileBertTokenizer"),c(JB,"href","/docs/transformers/pr_15792/en/model_doc/mobilebert#transformers.MobileBertTokenizerFast"),c(YB,"href","/docs/transformers/pr_15792/en/model_doc/mpnet#transformers.MPNetTokenizer"),c(KB,"href","/docs/transformers/pr_15792/en/model_doc/mpnet#transformers.MPNetTokenizerFast"),c(ZB,"href","/docs/transformers/pr_15792/en/model_doc/mt5#transformers.T5Tokenizer"),c(ek,"href","/docs/transformers/pr_15792/en/model_doc/mt5#transformers.T5TokenizerFast"),c(ok,"href","/docs/transformers/pr_15792/en/model_doc/openai-gpt#transformers.OpenAIGPTTokenizer"),c(rk,"href","/docs/transformers/pr_15792/en/model_doc/openai-gpt#transformers.OpenAIGPTTokenizerFast"),c(tk,"href","/docs/transformers/pr_15792/en/model_doc/pegasus#transformers.PegasusTokenizer"),c(ak,"href","/docs/transformers/pr_15792/en/model_doc/pegasus#transformers.PegasusTokenizerFast"),c(nk,"href","/docs/transformers/pr_15792/en/model_doc/perceiver#transformers.PerceiverTokenizer"),c(sk,"href","/docs/transformers/pr_15792/en/model_doc/phobert#transformers.PhobertTokenizer"),c(lk,"href","/docs/transformers/pr_15792/en/model_doc/plbart#transformers.PLBartTokenizer"),c(ik,"href","/docs/transformers/pr_15792/en/model_doc/prophetnet#transformers.ProphetNetTokenizer"),c(dk,"href","/docs/transformers/pr_15792/en/model_doc/bert#transformers.BertTokenizer"),c(ck,"href","/docs/transformers/pr_15792/en/model_doc/bert#transformers.BertTokenizerFast"),c(fk,"href","/docs/transformers/pr_15792/en/model_doc/rag#transformers.RagTokenizer"),c(mk,"href","/docs/transformers/pr_15792/en/model_doc/reformer#transformers.ReformerTokenizer"),c(gk,"href","/docs/transformers/pr_15792/en/model_doc/reformer#transformers.ReformerTokenizerFast"),c(hk,"href","/docs/transformers/pr_15792/en/model_doc/rembert#transformers.RemBertTokenizer"),c(pk,"href","/docs/transformers/pr_15792/en/model_doc/rembert#transformers.RemBertTokenizerFast"),c(_k,"href","/docs/transformers/pr_15792/en/model_doc/retribert#transformers.RetriBertTokenizer"),c(uk,"href","/docs/transformers/pr_15792/en/model_doc/retribert#transformers.RetriBertTokenizerFast"),c(bk,"href","/docs/transformers/pr_15792/en/model_doc/roberta#transformers.RobertaTokenizer"),c(vk,"href","/docs/transformers/pr_15792/en/model_doc/roberta#transformers.RobertaTokenizerFast"),c(Tk,"href","/docs/transformers/pr_15792/en/model_doc/roformer#transformers.RoFormerTokenizer"),c(Fk,"href","/docs/transformers/pr_15792/en/model_doc/roformer#transformers.RoFormerTokenizerFast"),c(Ck,"href","/docs/transformers/pr_15792/en/model_doc/speech_to_text#transformers.Speech2TextTokenizer"),c(Mk,"href","/docs/transformers/pr_15792/en/model_doc/speech_to_text_2#transformers.Speech2Text2Tokenizer"),c(Ek,"href","/docs/transformers/pr_15792/en/model_doc/splinter#transformers.SplinterTokenizer"),c(yk,"href","/docs/transformers/pr_15792/en/model_doc/splinter#transformers.SplinterTokenizerFast"),c(wk,"href","/docs/transformers/pr_15792/en/model_doc/squeezebert#transformers.SqueezeBertTokenizer"),c(Ak,"href","/docs/transformers/pr_15792/en/model_doc/squeezebert#transformers.SqueezeBertTokenizerFast"),c(Lk,"href","/docs/transformers/pr_15792/en/model_doc/mt5#transformers.T5Tokenizer"),c(Bk,"href","/docs/transformers/pr_15792/en/model_doc/mt5#transformers.T5TokenizerFast"),c(kk,"href","/docs/transformers/pr_15792/en/model_doc/tapas#transformers.TapasTokenizer"),c(xk,"href","/docs/transformers/pr_15792/en/model_doc/transfo-xl#transformers.TransfoXLTokenizer"),c(Rk,"href","/docs/transformers/pr_15792/en/model_doc/wav2vec2#transformers.Wav2Vec2CTCTokenizer"),c(Sk,"href","/docs/transformers/pr_15792/en/model_doc/wav2vec2_phoneme#transformers.Wav2Vec2PhonemeCTCTokenizer"),c(Pk,"href","/docs/transformers/pr_15792/en/model_doc/xglm#transformers.XGLMTokenizer"),c($k,"href","/docs/transformers/pr_15792/en/model_doc/xglm#transformers.XGLMTokenizerFast"),c(Ik,"href","/docs/transformers/pr_15792/en/model_doc/xlm#transformers.XLMTokenizer"),c(jk,"href","/docs/transformers/pr_15792/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetTokenizer"),c(Nk,"href","/docs/transformers/pr_15792/en/model_doc/xlm-roberta#transformers.XLMRobertaTokenizer"),c(Dk,"href","/docs/transformers/pr_15792/en/model_doc/xlm-roberta#transformers.XLMRobertaTokenizerFast"),c(qk,"href","/docs/transformers/pr_15792/en/model_doc/xlnet#transformers.XLNetTokenizer"),c(Gk,"href","/docs/transformers/pr_15792/en/model_doc/xlnet#transformers.XLNetTokenizerFast"),c(mo,"class","docstring"),c(Vg,"class","docstring"),c(Oo,"class","docstring"),c(Wg,"id","transformers.AutoFeatureExtractor"),c(Wg,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Wg,"href","#transformers.AutoFeatureExtractor"),c(ji,"class","relative group"),c(Ok,"href","/docs/transformers/pr_15792/en/model_doc/auto#transformers.AutoFeatureExtractor.from_pretrained"),c(Xk,"href","/docs/transformers/pr_15792/en/model_doc/beit#transformers.BeitFeatureExtractor"),c(zk,"href","/docs/transformers/pr_15792/en/model_doc/clip#transformers.CLIPFeatureExtractor"),c(Vk,"href","/docs/transformers/pr_15792/en/model_doc/convnext#transformers.ConvNextFeatureExtractor"),c(Wk,"href","/docs/transformers/pr_15792/en/model_doc/deit#transformers.DeiTFeatureExtractor"),c(Qk,"href","/docs/transformers/pr_15792/en/model_doc/detr#transformers.DetrFeatureExtractor"),c(Hk,"href","/docs/transformers/pr_15792/en/model_doc/wav2vec2#transformers.Wav2Vec2FeatureExtractor"),c(Uk,"href","/docs/transformers/pr_15792/en/model_doc/layoutlmv2#transformers.LayoutLMv2FeatureExtractor"),c(Jk,"href","/docs/transformers/pr_15792/en/model_doc/perceiver#transformers.PerceiverFeatureExtractor"),c(Yk,"href","/docs/transformers/pr_15792/en/model_doc/poolformer#transformers.PoolFormerFeatureExtractor"),c(Kk,"href","/docs/transformers/pr_15792/en/model_doc/segformer#transformers.SegformerFeatureExtractor"),c(Zk,"href","/docs/transformers/pr_15792/en/model_doc/speech_to_text#transformers.Speech2TextFeatureExtractor"),c(ex,"href","/docs/transformers/pr_15792/en/model_doc/vit#transformers.ViTFeatureExtractor"),c(ox,"href","/docs/transformers/pr_15792/en/model_doc/vit#transformers.ViTFeatureExtractor"),c(rx,"href","/docs/transformers/pr_15792/en/model_doc/vit#transformers.ViTFeatureExtractor"),c(tx,"href","/docs/transformers/pr_15792/en/model_doc/wav2vec2#transformers.Wav2Vec2FeatureExtractor"),c(Le,"class","docstring"),c(dh,"class","docstring"),c(Xo,"class","docstring"),c(ch,"id","transformers.AutoProcessor"),c(ch,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(ch,"href","#transformers.AutoProcessor"),c(Ni,"class","relative group"),c(ax,"href","/docs/transformers/pr_15792/en/model_doc/auto#transformers.AutoProcessor.from_pretrained"),c(nx,"href","/docs/transformers/pr_15792/en/model_doc/clip#transformers.CLIPProcessor"),c(sx,"href","/docs/transformers/pr_15792/en/model_doc/layoutlmv2#transformers.LayoutLMv2Processor"),c(lx,"href","/docs/transformers/pr_15792/en/model_doc/layoutxlm#transformers.LayoutXLMProcessor"),c(ix,"href","/docs/transformers/pr_15792/en/model_doc/speech_to_text#transformers.Speech2TextProcessor"),c(dx,"href","/docs/transformers/pr_15792/en/model_doc/speech_to_text_2#transformers.Speech2Text2Processor"),c(cx,"href","/docs/transformers/pr_15792/en/model_doc/trocr#transformers.TrOCRProcessor"),c(fx,"href","/docs/transformers/pr_15792/en/model_doc/vision-text-dual-encoder#transformers.VisionTextDualEncoderProcessor"),c(mx,"href","/docs/transformers/pr_15792/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor"),c(Be,"class","docstring"),c(Th,"class","docstring"),c(zo,"class","docstring"),c(Fh,"id","transformers.AutoModel"),c(Fh,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Fh,"href","#transformers.AutoModel"),c(qi,"class","relative group"),c(Nr,"class","docstring"),c(gx,"href","/docs/transformers/pr_15792/en/model_doc/albert#transformers.AlbertModel"),c(hx,"href","/docs/transformers/pr_15792/en/model_doc/bart#transformers.BartModel"),c(px,"href","/docs/transformers/pr_15792/en/model_doc/beit#transformers.BeitModel"),c(_x,"href","/docs/transformers/pr_15792/en/model_doc/bert#transformers.BertModel"),c(ux,"href","/docs/transformers/pr_15792/en/model_doc/bert-generation#transformers.BertGenerationEncoder"),c(bx,"href","/docs/transformers/pr_15792/en/model_doc/big_bird#transformers.BigBirdModel"),c(vx,"href","/docs/transformers/pr_15792/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusModel"),c(Tx,"href","/docs/transformers/pr_15792/en/model_doc/blenderbot#transformers.BlenderbotModel"),c(Fx,"href","/docs/transformers/pr_15792/en/model_doc/blenderbot-small#transformers.BlenderbotSmallModel"),c(Cx,"href","/docs/transformers/pr_15792/en/model_doc/camembert#transformers.CamembertModel"),c(Mx,"href","/docs/transformers/pr_15792/en/model_doc/canine#transformers.CanineModel"),c(Ex,"href","/docs/transformers/pr_15792/en/model_doc/clip#transformers.CLIPModel"),c(yx,"href","/docs/transformers/pr_15792/en/model_doc/convbert#transformers.ConvBertModel"),c(wx,"href","/docs/transformers/pr_15792/en/model_doc/convnext#transformers.ConvNextModel"),c(Ax,"href","/docs/transformers/pr_15792/en/model_doc/ctrl#transformers.CTRLModel"),c(Lx,"href","/docs/transformers/pr_15792/en/model_doc/deberta#transformers.DebertaModel"),c(Bx,"href","/docs/transformers/pr_15792/en/model_doc/deberta-v2#transformers.DebertaV2Model"),c(kx,"href","/docs/transformers/pr_15792/en/model_doc/deit#transformers.DeiTModel"),c(xx,"href","/docs/transformers/pr_15792/en/model_doc/detr#transformers.DetrModel"),c(Rx,"href","/docs/transformers/pr_15792/en/model_doc/distilbert#transformers.DistilBertModel"),c(Sx,"href","/docs/transformers/pr_15792/en/model_doc/dpr#transformers.DPRQuestionEncoder"),c(Px,"href","/docs/transformers/pr_15792/en/model_doc/electra#transformers.ElectraModel"),c($x,"href","/docs/transformers/pr_15792/en/model_doc/flaubert#transformers.FlaubertModel"),c(Ix,"href","/docs/transformers/pr_15792/en/model_doc/fnet#transformers.FNetModel"),c(jx,"href","/docs/transformers/pr_15792/en/model_doc/fsmt#transformers.FSMTModel"),c(Nx,"href","/docs/transformers/pr_15792/en/model_doc/funnel#transformers.FunnelModel"),c(Dx,"href","/docs/transformers/pr_15792/en/model_doc/funnel#transformers.FunnelBaseModel"),c(qx,"href","/docs/transformers/pr_15792/en/model_doc/gpt2#transformers.GPT2Model"),c(Gx,"href","/docs/transformers/pr_15792/en/model_doc/gpt_neo#transformers.GPTNeoModel"),c(Ox,"href","/docs/transformers/pr_15792/en/model_doc/gptj#transformers.GPTJModel"),c(Xx,"href","/docs/transformers/pr_15792/en/model_doc/hubert#transformers.HubertModel"),c(zx,"href","/docs/transformers/pr_15792/en/model_doc/ibert#transformers.IBertModel"),c(Vx,"href","/docs/transformers/pr_15792/en/model_doc/imagegpt#transformers.ImageGPTModel"),c(Wx,"href","/docs/transformers/pr_15792/en/model_doc/layoutlm#transformers.LayoutLMModel"),c(Qx,"href","/docs/transformers/pr_15792/en/model_doc/layoutlmv2#transformers.LayoutLMv2Model"),c(Hx,"href","/docs/transformers/pr_15792/en/model_doc/led#transformers.LEDModel"),c(Ux,"href","/docs/transformers/pr_15792/en/model_doc/longformer#transformers.LongformerModel"),c(Jx,"href","/docs/transformers/pr_15792/en/model_doc/luke#transformers.LukeModel"),c(Yx,"href","/docs/transformers/pr_15792/en/model_doc/lxmert#transformers.LxmertModel"),c(Kx,"href","/docs/transformers/pr_15792/en/model_doc/m2m_100#transformers.M2M100Model"),c(Zx,"href","/docs/transformers/pr_15792/en/model_doc/marian#transformers.MarianModel"),c(eR,"href","/docs/transformers/pr_15792/en/model_doc/mbart#transformers.MBartModel"),c(oR,"href","/docs/transformers/pr_15792/en/model_doc/megatron-bert#transformers.MegatronBertModel"),c(rR,"href","/docs/transformers/pr_15792/en/model_doc/mobilebert#transformers.MobileBertModel"),c(tR,"href","/docs/transformers/pr_15792/en/model_doc/mpnet#transformers.MPNetModel"),c(aR,"href","/docs/transformers/pr_15792/en/model_doc/mt5#transformers.MT5Model"),c(nR,"href","/docs/transformers/pr_15792/en/model_doc/nystromformer#transformers.NystromformerModel"),c(sR,"href","/docs/transformers/pr_15792/en/model_doc/openai-gpt#transformers.OpenAIGPTModel"),c(lR,"href","/docs/transformers/pr_15792/en/model_doc/pegasus#transformers.PegasusModel"),c(iR,"href","/docs/transformers/pr_15792/en/model_doc/perceiver#transformers.PerceiverModel"),c(dR,"href","/docs/transformers/pr_15792/en/model_doc/plbart#transformers.PLBartModel"),c(cR,"href","/docs/transformers/pr_15792/en/model_doc/poolformer#transformers.PoolFormerModel"),c(fR,"href","/docs/transformers/pr_15792/en/model_doc/prophetnet#transformers.ProphetNetModel"),c(mR,"href","/docs/transformers/pr_15792/en/model_doc/qdqbert#transformers.QDQBertModel"),c(gR,"href","/docs/transformers/pr_15792/en/model_doc/reformer#transformers.ReformerModel"),c(hR,"href","/docs/transformers/pr_15792/en/model_doc/rembert#transformers.RemBertModel"),c(pR,"href","/docs/transformers/pr_15792/en/model_doc/retribert#transformers.RetriBertModel"),c(_R,"href","/docs/transformers/pr_15792/en/model_doc/roberta#transformers.RobertaModel"),c(uR,"href","/docs/transformers/pr_15792/en/model_doc/roformer#transformers.RoFormerModel"),c(bR,"href","/docs/transformers/pr_15792/en/model_doc/segformer#transformers.SegformerModel"),c(vR,"href","/docs/transformers/pr_15792/en/model_doc/sew#transformers.SEWModel"),c(TR,"href","/docs/transformers/pr_15792/en/model_doc/sew-d#transformers.SEWDModel"),c(FR,"href","/docs/transformers/pr_15792/en/model_doc/speech_to_text#transformers.Speech2TextModel"),c(CR,"href","/docs/transformers/pr_15792/en/model_doc/splinter#transformers.SplinterModel"),c(MR,"href","/docs/transformers/pr_15792/en/model_doc/squeezebert#transformers.SqueezeBertModel"),c(ER,"href","/docs/transformers/pr_15792/en/model_doc/swin#transformers.SwinModel"),c(yR,"href","/docs/transformers/pr_15792/en/model_doc/t5#transformers.T5Model"),c(wR,"href","/docs/transformers/pr_15792/en/model_doc/tapas#transformers.TapasModel"),c(AR,"href","/docs/transformers/pr_15792/en/model_doc/transfo-xl#transformers.TransfoXLModel"),c(LR,"href","/docs/transformers/pr_15792/en/model_doc/unispeech#transformers.UniSpeechModel"),c(BR,"href","/docs/transformers/pr_15792/en/model_doc/unispeech-sat#transformers.UniSpeechSatModel"),c(kR,"href","/docs/transformers/pr_15792/en/model_doc/vilt#transformers.ViltModel"),c(xR,"href","/docs/transformers/pr_15792/en/model_doc/vision-text-dual-encoder#transformers.VisionTextDualEncoderModel"),c(RR,"href","/docs/transformers/pr_15792/en/model_doc/visual_bert#transformers.VisualBertModel"),c(SR,"href","/docs/transformers/pr_15792/en/model_doc/vit#transformers.ViTModel"),c(PR,"href","/docs/transformers/pr_15792/en/model_doc/vit_mae#transformers.ViTMAEModel"),c($R,"href","/docs/transformers/pr_15792/en/model_doc/wav2vec2#transformers.Wav2Vec2Model"),c(IR,"href","/docs/transformers/pr_15792/en/model_doc/wavlm#transformers.WavLMModel"),c(jR,"href","/docs/transformers/pr_15792/en/model_doc/xglm#transformers.XGLMModel"),c(NR,"href","/docs/transformers/pr_15792/en/model_doc/xlm#transformers.XLMModel"),c(DR,"href","/docs/transformers/pr_15792/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetModel"),c(qR,"href","/docs/transformers/pr_15792/en/model_doc/xlm-roberta#transformers.XLMRobertaModel"),c(GR,"href","/docs/transformers/pr_15792/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLModel"),c(OR,"href","/docs/transformers/pr_15792/en/model_doc/xlnet#transformers.XLNetModel"),c(XR,"href","/docs/transformers/pr_15792/en/model_doc/yoso#transformers.YosoModel"),c(ke,"class","docstring"),c(Vo,"class","docstring"),c(Kp,"id","transformers.AutoModelForPreTraining"),c(Kp,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Kp,"href","#transformers.AutoModelForPreTraining"),c(Xi,"class","relative group"),c(Dr,"class","docstring"),c(zR,"href","/docs/transformers/pr_15792/en/model_doc/albert#transformers.AlbertForPreTraining"),c(VR,"href","/docs/transformers/pr_15792/en/model_doc/bart#transformers.BartForConditionalGeneration"),c(WR,"href","/docs/transformers/pr_15792/en/model_doc/bert#transformers.BertForPreTraining"),c(QR,"href","/docs/transformers/pr_15792/en/model_doc/big_bird#transformers.BigBirdForPreTraining"),c(HR,"href","/docs/transformers/pr_15792/en/model_doc/camembert#transformers.CamembertForMaskedLM"),c(UR,"href","/docs/transformers/pr_15792/en/model_doc/ctrl#transformers.CTRLLMHeadModel"),c(JR,"href","/docs/transformers/pr_15792/en/model_doc/deberta#transformers.DebertaForMaskedLM"),c(YR,"href","/docs/transformers/pr_15792/en/model_doc/deberta-v2#transformers.DebertaV2ForMaskedLM"),c(KR,"href","/docs/transformers/pr_15792/en/model_doc/distilbert#transformers.DistilBertForMaskedLM"),c(ZR,"href","/docs/transformers/pr_15792/en/model_doc/electra#transformers.ElectraForPreTraining"),c(eS,"href","/docs/transformers/pr_15792/en/model_doc/flaubert#transformers.FlaubertWithLMHeadModel"),c(oS,"href","/docs/transformers/pr_15792/en/model_doc/fnet#transformers.FNetForPreTraining"),c(rS,"href","/docs/transformers/pr_15792/en/model_doc/fsmt#transformers.FSMTForConditionalGeneration"),c(tS,"href","/docs/transformers/pr_15792/en/model_doc/funnel#transformers.FunnelForPreTraining"),c(aS,"href","/docs/transformers/pr_15792/en/model_doc/gpt2#transformers.GPT2LMHeadModel"),c(nS,"href","/docs/transformers/pr_15792/en/model_doc/ibert#transformers.IBertForMaskedLM"),c(sS,"href","/docs/transformers/pr_15792/en/model_doc/layoutlm#transformers.LayoutLMForMaskedLM"),c(lS,"href","/docs/transformers/pr_15792/en/model_doc/longformer#transformers.LongformerForMaskedLM"),c(iS,"href","/docs/transformers/pr_15792/en/model_doc/lxmert#transformers.LxmertForPreTraining"),c(dS,"href","/docs/transformers/pr_15792/en/model_doc/megatron-bert#transformers.MegatronBertForPreTraining"),c(cS,"href","/docs/transformers/pr_15792/en/model_doc/mobilebert#transformers.MobileBertForPreTraining"),c(fS,"href","/docs/transformers/pr_15792/en/model_doc/mpnet#transformers.MPNetForMaskedLM"),c(mS,"href","/docs/transformers/pr_15792/en/model_doc/openai-gpt#transformers.OpenAIGPTLMHeadModel"),c(gS,"href","/docs/transformers/pr_15792/en/model_doc/retribert#transformers.RetriBertModel"),c(hS,"href","/docs/transformers/pr_15792/en/model_doc/roberta#transformers.RobertaForMaskedLM"),c(pS,"href","/docs/transformers/pr_15792/en/model_doc/squeezebert#transformers.SqueezeBertForMaskedLM"),c(_S,"href","/docs/transformers/pr_15792/en/model_doc/t5#transformers.T5ForConditionalGeneration"),c(uS,"href","/docs/transformers/pr_15792/en/model_doc/tapas#transformers.TapasForMaskedLM"),c(bS,"href","/docs/transformers/pr_15792/en/model_doc/transfo-xl#transformers.TransfoXLLMHeadModel"),c(vS,"href","/docs/transformers/pr_15792/en/model_doc/unispeech#transformers.UniSpeechForPreTraining"),c(TS,"href","/docs/transformers/pr_15792/en/model_doc/unispeech-sat#transformers.UniSpeechSatForPreTraining"),c(FS,"href","/docs/transformers/pr_15792/en/model_doc/visual_bert#transformers.VisualBertForPreTraining"),c(CS,"href","/docs/transformers/pr_15792/en/model_doc/vit_mae#transformers.ViTMAEForPreTraining"),c(MS,"href","/docs/transformers/pr_15792/en/model_doc/wav2vec2#transformers.Wav2Vec2ForPreTraining"),c(ES,"href","/docs/transformers/pr_15792/en/model_doc/xlm#transformers.XLMWithLMHeadModel"),c(yS,"href","/docs/transformers/pr_15792/en/model_doc/xlm-roberta#transformers.XLMRobertaForMaskedLM"),c(wS,"href","/docs/transformers/pr_15792/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForMaskedLM"),c(AS,"href","/docs/transformers/pr_15792/en/model_doc/xlnet#transformers.XLNetLMHeadModel"),c(xe,"class","docstring"),c(Wo,"class","docstring"),c(N_,"id","transformers.AutoModelForCausalLM"),c(N_,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(N_,"href","#transformers.AutoModelForCausalLM"),c(Wi,"class","relative group"),c(qr,"class","docstring"),c(LS,"href","/docs/transformers/pr_15792/en/model_doc/bart#transformers.BartForCausalLM"),c(BS,"href","/docs/transformers/pr_15792/en/model_doc/bert#transformers.BertLMHeadModel"),c(kS,"href","/docs/transformers/pr_15792/en/model_doc/bert-generation#transformers.BertGenerationDecoder"),c(xS,"href","/docs/transformers/pr_15792/en/model_doc/big_bird#transformers.BigBirdForCausalLM"),c(RS,"href","/docs/transformers/pr_15792/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusForCausalLM"),c(SS,"href","/docs/transformers/pr_15792/en/model_doc/blenderbot#transformers.BlenderbotForCausalLM"),c(PS,"href","/docs/transformers/pr_15792/en/model_doc/blenderbot-small#transformers.BlenderbotSmallForCausalLM"),c($S,"href","/docs/transformers/pr_15792/en/model_doc/camembert#transformers.CamembertForCausalLM"),c(IS,"href","/docs/transformers/pr_15792/en/model_doc/ctrl#transformers.CTRLLMHeadModel"),c(jS,"href","/docs/transformers/pr_15792/en/model_doc/electra#transformers.ElectraForCausalLM"),c(NS,"href","/docs/transformers/pr_15792/en/model_doc/gpt2#transformers.GPT2LMHeadModel"),c(DS,"href","/docs/transformers/pr_15792/en/model_doc/gpt_neo#transformers.GPTNeoForCausalLM"),c(qS,"href","/docs/transformers/pr_15792/en/model_doc/gptj#transformers.GPTJForCausalLM"),c(GS,"href","/docs/transformers/pr_15792/en/model_doc/marian#transformers.MarianForCausalLM"),c(OS,"href","/docs/transformers/pr_15792/en/model_doc/mbart#transformers.MBartForCausalLM"),c(XS,"href","/docs/transformers/pr_15792/en/model_doc/megatron-bert#transformers.MegatronBertForCausalLM"),c(zS,"href","/docs/transformers/pr_15792/en/model_doc/openai-gpt#transformers.OpenAIGPTLMHeadModel"),c(VS,"href","/docs/transformers/pr_15792/en/model_doc/pegasus#transformers.PegasusForCausalLM"),c(WS,"href","/docs/transformers/pr_15792/en/model_doc/plbart#transformers.PLBartForCausalLM"),c(QS,"href","/docs/transformers/pr_15792/en/model_doc/prophetnet#transformers.ProphetNetForCausalLM"),c(HS,"href","/docs/transformers/pr_15792/en/model_doc/qdqbert#transformers.QDQBertLMHeadModel"),c(US,"href","/docs/transformers/pr_15792/en/model_doc/reformer#transformers.ReformerModelWithLMHead"),c(JS,"href","/docs/transformers/pr_15792/en/model_doc/rembert#transformers.RemBertForCausalLM"),c(YS,"href","/docs/transformers/pr_15792/en/model_doc/roberta#transformers.RobertaForCausalLM"),c(KS,"href","/docs/transformers/pr_15792/en/model_doc/roformer#transformers.RoFormerForCausalLM"),c(ZS,"href","/docs/transformers/pr_15792/en/model_doc/speech_to_text_2#transformers.Speech2Text2ForCausalLM"),c(eP,"href","/docs/transformers/pr_15792/en/model_doc/transfo-xl#transformers.TransfoXLLMHeadModel"),c(oP,"href","/docs/transformers/pr_15792/en/model_doc/trocr#transformers.TrOCRForCausalLM"),c(rP,"href","/docs/transformers/pr_15792/en/model_doc/xglm#transformers.XGLMForCausalLM"),c(tP,"href","/docs/transformers/pr_15792/en/model_doc/xlm#transformers.XLMWithLMHeadModel"),c(aP,"href","/docs/transformers/pr_15792/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetForCausalLM"),c(nP,"href","/docs/transformers/pr_15792/en/model_doc/xlm-roberta#transformers.XLMRobertaForCausalLM"),c(sP,"href","/docs/transformers/pr_15792/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForCausalLM"),c(lP,"href","/docs/transformers/pr_15792/en/model_doc/xlnet#transformers.XLNetLMHeadModel"),c(Re,"class","docstring"),c(Qo,"class","docstring"),c(Tu,"id","transformers.AutoModelForMaskedLM"),c(Tu,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Tu,"href","#transformers.AutoModelForMaskedLM"),c(Ui,"class","relative group"),c(Gr,"class","docstring"),c(iP,"href","/docs/transformers/pr_15792/en/model_doc/albert#transformers.AlbertForMaskedLM"),c(dP,"href","/docs/transformers/pr_15792/en/model_doc/bart#transformers.BartForConditionalGeneration"),c(cP,"href","/docs/transformers/pr_15792/en/model_doc/bert#transformers.BertForMaskedLM"),c(fP,"href","/docs/transformers/pr_15792/en/model_doc/big_bird#transformers.BigBirdForMaskedLM"),c(mP,"href","/docs/transformers/pr_15792/en/model_doc/camembert#transformers.CamembertForMaskedLM"),c(gP,"href","/docs/transformers/pr_15792/en/model_doc/convbert#transformers.ConvBertForMaskedLM"),c(hP,"href","/docs/transformers/pr_15792/en/model_doc/deberta#transformers.DebertaForMaskedLM"),c(pP,"href","/docs/transformers/pr_15792/en/model_doc/deberta-v2#transformers.DebertaV2ForMaskedLM"),c(_P,"href","/docs/transformers/pr_15792/en/model_doc/distilbert#transformers.DistilBertForMaskedLM"),c(uP,"href","/docs/transformers/pr_15792/en/model_doc/electra#transformers.ElectraForMaskedLM"),c(bP,"href","/docs/transformers/pr_15792/en/model_doc/flaubert#transformers.FlaubertWithLMHeadModel"),c(vP,"href","/docs/transformers/pr_15792/en/model_doc/fnet#transformers.FNetForMaskedLM"),c(TP,"href","/docs/transformers/pr_15792/en/model_doc/funnel#transformers.FunnelForMaskedLM"),c(FP,"href","/docs/transformers/pr_15792/en/model_doc/ibert#transformers.IBertForMaskedLM"),c(CP,"href","/docs/transformers/pr_15792/en/model_doc/layoutlm#transformers.LayoutLMForMaskedLM"),c(MP,"href","/docs/transformers/pr_15792/en/model_doc/longformer#transformers.LongformerForMaskedLM"),c(EP,"href","/docs/transformers/pr_15792/en/model_doc/mbart#transformers.MBartForConditionalGeneration"),c(yP,"href","/docs/transformers/pr_15792/en/model_doc/megatron-bert#transformers.MegatronBertForMaskedLM"),c(wP,"href","/docs/transformers/pr_15792/en/model_doc/mobilebert#transformers.MobileBertForMaskedLM"),c(AP,"href","/docs/transformers/pr_15792/en/model_doc/mpnet#transformers.MPNetForMaskedLM"),c(LP,"href","/docs/transformers/pr_15792/en/model_doc/nystromformer#transformers.NystromformerForMaskedLM"),c(BP,"href","/docs/transformers/pr_15792/en/model_doc/perceiver#transformers.PerceiverForMaskedLM"),c(kP,"href","/docs/transformers/pr_15792/en/model_doc/qdqbert#transformers.QDQBertForMaskedLM"),c(xP,"href","/docs/transformers/pr_15792/en/model_doc/reformer#transformers.ReformerForMaskedLM"),c(RP,"href","/docs/transformers/pr_15792/en/model_doc/rembert#transformers.RemBertForMaskedLM"),c(SP,"href","/docs/transformers/pr_15792/en/model_doc/roberta#transformers.RobertaForMaskedLM"),c(PP,"href","/docs/transformers/pr_15792/en/model_doc/roformer#transformers.RoFormerForMaskedLM"),c($P,"href","/docs/transformers/pr_15792/en/model_doc/squeezebert#transformers.SqueezeBertForMaskedLM"),c(IP,"href","/docs/transformers/pr_15792/en/model_doc/tapas#transformers.TapasForMaskedLM"),c(jP,"href","/docs/transformers/pr_15792/en/model_doc/xlm#transformers.XLMWithLMHeadModel"),c(NP,"href","/docs/transformers/pr_15792/en/model_doc/xlm-roberta#transformers.XLMRobertaForMaskedLM"),c(DP,"href","/docs/transformers/pr_15792/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForMaskedLM"),c(qP,"href","/docs/transformers/pr_15792/en/model_doc/yoso#transformers.YosoForMaskedLM"),c(Se,"class","docstring"),c(Ho,"class","docstring"),c(r2,"id","transformers.AutoModelForSeq2SeqLM"),c(r2,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(r2,"href","#transformers.AutoModelForSeq2SeqLM"),c(Ki,"class","relative group"),c(Or,"class","docstring"),c(GP,"href","/docs/transformers/pr_15792/en/model_doc/bart#transformers.BartForConditionalGeneration"),c(OP,"href","/docs/transformers/pr_15792/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusForConditionalGeneration"),c(XP,"href","/docs/transformers/pr_15792/en/model_doc/blenderbot#transformers.BlenderbotForConditionalGeneration"),c(zP,"href","/docs/transformers/pr_15792/en/model_doc/blenderbot-small#transformers.BlenderbotSmallForConditionalGeneration"),c(VP,"href","/docs/transformers/pr_15792/en/model_doc/encoder-decoder#transformers.EncoderDecoderModel"),c(WP,"href","/docs/transformers/pr_15792/en/model_doc/fsmt#transformers.FSMTForConditionalGeneration"),c(QP,"href","/docs/transformers/pr_15792/en/model_doc/led#transformers.LEDForConditionalGeneration"),c(HP,"href","/docs/transformers/pr_15792/en/model_doc/m2m_100#transformers.M2M100ForConditionalGeneration"),c(UP,"href","/docs/transformers/pr_15792/en/model_doc/marian#transformers.MarianMTModel"),c(JP,"href","/docs/transformers/pr_15792/en/model_doc/mbart#transformers.MBartForConditionalGeneration"),c(YP,"href","/docs/transformers/pr_15792/en/model_doc/mt5#transformers.MT5ForConditionalGeneration"),c(KP,"href","/docs/transformers/pr_15792/en/model_doc/pegasus#transformers.PegasusForConditionalGeneration"),c(ZP,"href","/docs/transformers/pr_15792/en/model_doc/plbart#transformers.PLBartForConditionalGeneration"),c(e$,"href","/docs/transformers/pr_15792/en/model_doc/prophetnet#transformers.ProphetNetForConditionalGeneration"),c(o$,"href","/docs/transformers/pr_15792/en/model_doc/t5#transformers.T5ForConditionalGeneration"),c(r$,"href","/docs/transformers/pr_15792/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetForConditionalGeneration"),c(Pe,"class","docstring"),c(Uo,"class","docstring"),c(T2,"id","transformers.AutoModelForSequenceClassification"),c(T2,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(T2,"href","#transformers.AutoModelForSequenceClassification"),c(od,"class","relative group"),c(Xr,"class","docstring"),c(t$,"href","/docs/transformers/pr_15792/en/model_doc/albert#transformers.AlbertForSequenceClassification"),c(a$,"href","/docs/transformers/pr_15792/en/model_doc/bart#transformers.BartForSequenceClassification"),c(n$,"href","/docs/transformers/pr_15792/en/model_doc/bert#transformers.BertForSequenceClassification"),c(s$,"href","/docs/transformers/pr_15792/en/model_doc/big_bird#transformers.BigBirdForSequenceClassification"),c(l$,"href","/docs/transformers/pr_15792/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusForSequenceClassification"),c(i$,"href","/docs/transformers/pr_15792/en/model_doc/camembert#transformers.CamembertForSequenceClassification"),c(d$,"href","/docs/transformers/pr_15792/en/model_doc/canine#transformers.CanineForSequenceClassification"),c(c$,"href","/docs/transformers/pr_15792/en/model_doc/convbert#transformers.ConvBertForSequenceClassification"),c(f$,"href","/docs/transformers/pr_15792/en/model_doc/ctrl#transformers.CTRLForSequenceClassification"),c(m$,"href","/docs/transformers/pr_15792/en/model_doc/deberta#transformers.DebertaForSequenceClassification"),c(g$,"href","/docs/transformers/pr_15792/en/model_doc/deberta-v2#transformers.DebertaV2ForSequenceClassification"),c(h$,"href","/docs/transformers/pr_15792/en/model_doc/distilbert#transformers.DistilBertForSequenceClassification"),c(p$,"href","/docs/transformers/pr_15792/en/model_doc/electra#transformers.ElectraForSequenceClassification"),c(_$,"href","/docs/transformers/pr_15792/en/model_doc/flaubert#transformers.FlaubertForSequenceClassification"),c(u$,"href","/docs/transformers/pr_15792/en/model_doc/fnet#transformers.FNetForSequenceClassification"),c(b$,"href","/docs/transformers/pr_15792/en/model_doc/funnel#transformers.FunnelForSequenceClassification"),c(v$,"href","/docs/transformers/pr_15792/en/model_doc/gpt2#transformers.GPT2ForSequenceClassification"),c(T$,"href","/docs/transformers/pr_15792/en/model_doc/gpt_neo#transformers.GPTNeoForSequenceClassification"),c(F$,"href","/docs/transformers/pr_15792/en/model_doc/gptj#transformers.GPTJForSequenceClassification"),c(C$,"href","/docs/transformers/pr_15792/en/model_doc/ibert#transformers.IBertForSequenceClassification"),c(M$,"href","/docs/transformers/pr_15792/en/model_doc/layoutlm#transformers.LayoutLMForSequenceClassification"),c(E$,"href","/docs/transformers/pr_15792/en/model_doc/layoutlmv2#transformers.LayoutLMv2ForSequenceClassification"),c(y$,"href","/docs/transformers/pr_15792/en/model_doc/led#transformers.LEDForSequenceClassification"),c(w$,"href","/docs/transformers/pr_15792/en/model_doc/longformer#transformers.LongformerForSequenceClassification"),c(A$,"href","/docs/transformers/pr_15792/en/model_doc/mbart#transformers.MBartForSequenceClassification"),c(L$,"href","/docs/transformers/pr_15792/en/model_doc/megatron-bert#transformers.MegatronBertForSequenceClassification"),c(B$,"href","/docs/transformers/pr_15792/en/model_doc/mobilebert#transformers.MobileBertForSequenceClassification"),c(k$,"href","/docs/transformers/pr_15792/en/model_doc/mpnet#transformers.MPNetForSequenceClassification"),c(x$,"href","/docs/transformers/pr_15792/en/model_doc/nystromformer#transformers.NystromformerForSequenceClassification"),c(R$,"href","/docs/transformers/pr_15792/en/model_doc/openai-gpt#transformers.OpenAIGPTForSequenceClassification"),c(S$,"href","/docs/transformers/pr_15792/en/model_doc/perceiver#transformers.PerceiverForSequenceClassification"),c(P$,"href","/docs/transformers/pr_15792/en/model_doc/plbart#transformers.PLBartForSequenceClassification"),c($$,"href","/docs/transformers/pr_15792/en/model_doc/qdqbert#transformers.QDQBertForSequenceClassification"),c(I$,"href","/docs/transformers/pr_15792/en/model_doc/reformer#transformers.ReformerForSequenceClassification"),c(j$,"href","/docs/transformers/pr_15792/en/model_doc/rembert#transformers.RemBertForSequenceClassification"),c(N$,"href","/docs/transformers/pr_15792/en/model_doc/roberta#transformers.RobertaForSequenceClassification"),c(D$,"href","/docs/transformers/pr_15792/en/model_doc/roformer#transformers.RoFormerForSequenceClassification"),c(q$,"href","/docs/transformers/pr_15792/en/model_doc/squeezebert#transformers.SqueezeBertForSequenceClassification"),c(G$,"href","/docs/transformers/pr_15792/en/model_doc/tapas#transformers.TapasForSequenceClassification"),c(O$,"href","/docs/transformers/pr_15792/en/model_doc/transfo-xl#transformers.TransfoXLForSequenceClassification"),c(X$,"href","/docs/transformers/pr_15792/en/model_doc/xlm#transformers.XLMForSequenceClassification"),c(z$,"href","/docs/transformers/pr_15792/en/model_doc/xlm-roberta#transformers.XLMRobertaForSequenceClassification"),c(V$,"href","/docs/transformers/pr_15792/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForSequenceClassification"),c(W$,"href","/docs/transformers/pr_15792/en/model_doc/xlnet#transformers.XLNetForSequenceClassification"),c(Q$,"href","/docs/transformers/pr_15792/en/model_doc/yoso#transformers.YosoForSequenceClassification"),c($e,"class","docstring"),c(Jo,"class","docstring"),c(g1,"id","transformers.AutoModelForMultipleChoice"),c(g1,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(g1,"href","#transformers.AutoModelForMultipleChoice"),c(ad,"class","relative group"),c(zr,"class","docstring"),c(H$,"href","/docs/transformers/pr_15792/en/model_doc/albert#transformers.AlbertForMultipleChoice"),c(U$,"href","/docs/transformers/pr_15792/en/model_doc/bert#transformers.BertForMultipleChoice"),c(J$,"href","/docs/transformers/pr_15792/en/model_doc/big_bird#transformers.BigBirdForMultipleChoice"),c(Y$,"href","/docs/transformers/pr_15792/en/model_doc/camembert#transformers.CamembertForMultipleChoice"),c(K$,"href","/docs/transformers/pr_15792/en/model_doc/canine#transformers.CanineForMultipleChoice"),c(Z$,"href","/docs/transformers/pr_15792/en/model_doc/convbert#transformers.ConvBertForMultipleChoice"),c(eI,"href","/docs/transformers/pr_15792/en/model_doc/distilbert#transformers.DistilBertForMultipleChoice"),c(oI,"href","/docs/transformers/pr_15792/en/model_doc/electra#transformers.ElectraForMultipleChoice"),c(rI,"href","/docs/transformers/pr_15792/en/model_doc/flaubert#transformers.FlaubertForMultipleChoice"),c(tI,"href","/docs/transformers/pr_15792/en/model_doc/fnet#transformers.FNetForMultipleChoice"),c(aI,"href","/docs/transformers/pr_15792/en/model_doc/funnel#transformers.FunnelForMultipleChoice"),c(nI,"href","/docs/transformers/pr_15792/en/model_doc/ibert#transformers.IBertForMultipleChoice"),c(sI,"href","/docs/transformers/pr_15792/en/model_doc/longformer#transformers.LongformerForMultipleChoice"),c(lI,"href","/docs/transformers/pr_15792/en/model_doc/megatron-bert#transformers.MegatronBertForMultipleChoice"),c(iI,"href","/docs/transformers/pr_15792/en/model_doc/mobilebert#transformers.MobileBertForMultipleChoice"),c(dI,"href","/docs/transformers/pr_15792/en/model_doc/mpnet#transformers.MPNetForMultipleChoice"),c(cI,"href","/docs/transformers/pr_15792/en/model_doc/nystromformer#transformers.NystromformerForMultipleChoice"),c(fI,"href","/docs/transformers/pr_15792/en/model_doc/qdqbert#transformers.QDQBertForMultipleChoice"),c(mI,"href","/docs/transformers/pr_15792/en/model_doc/rembert#transformers.RemBertForMultipleChoice"),c(gI,"href","/docs/transformers/pr_15792/en/model_doc/roberta#transformers.RobertaForMultipleChoice"),c(hI,"href","/docs/transformers/pr_15792/en/model_doc/roformer#transformers.RoFormerForMultipleChoice"),c(pI,"href","/docs/transformers/pr_15792/en/model_doc/squeezebert#transformers.SqueezeBertForMultipleChoice"),c(_I,"href","/docs/transformers/pr_15792/en/model_doc/xlm#transformers.XLMForMultipleChoice"),c(uI,"href","/docs/transformers/pr_15792/en/model_doc/xlm-roberta#transformers.XLMRobertaForMultipleChoice"),c(bI,"href","/docs/transformers/pr_15792/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForMultipleChoice"),c(vI,"href","/docs/transformers/pr_15792/en/model_doc/xlnet#transformers.XLNetForMultipleChoice"),c(TI,"href","/docs/transformers/pr_15792/en/model_doc/yoso#transformers.YosoForMultipleChoice"),c(Ie,"class","docstring"),c(Yo,"class","docstring"),c(O1,"id","transformers.AutoModelForNextSentencePrediction"),c(O1,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(O1,"href","#transformers.AutoModelForNextSentencePrediction"),c(ld,"class","relative group"),c(Vr,"class","docstring"),c(FI,"href","/docs/transformers/pr_15792/en/model_doc/bert#transformers.BertForNextSentencePrediction"),c(CI,"href","/docs/transformers/pr_15792/en/model_doc/fnet#transformers.FNetForNextSentencePrediction"),c(MI,"href","/docs/transformers/pr_15792/en/model_doc/megatron-bert#transformers.MegatronBertForNextSentencePrediction"),c(EI,"href","/docs/transformers/pr_15792/en/model_doc/mobilebert#transformers.MobileBertForNextSentencePrediction"),c(yI,"href","/docs/transformers/pr_15792/en/model_doc/qdqbert#transformers.QDQBertForNextSentencePrediction"),c(je,"class","docstring"),c(Ko,"class","docstring"),c(U1,"id","transformers.AutoModelForTokenClassification"),c(U1,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(U1,"href","#transformers.AutoModelForTokenClassification"),c(cd,"class","relative group"),c(Wr,"class","docstring"),c(wI,"href","/docs/transformers/pr_15792/en/model_doc/albert#transformers.AlbertForTokenClassification"),c(AI,"href","/docs/transformers/pr_15792/en/model_doc/bert#transformers.BertForTokenClassification"),c(LI,"href","/docs/transformers/pr_15792/en/model_doc/big_bird#transformers.BigBirdForTokenClassification"),c(BI,"href","/docs/transformers/pr_15792/en/model_doc/camembert#transformers.CamembertForTokenClassification"),c(kI,"href","/docs/transformers/pr_15792/en/model_doc/canine#transformers.CanineForTokenClassification"),c(xI,"href","/docs/transformers/pr_15792/en/model_doc/convbert#transformers.ConvBertForTokenClassification"),c(RI,"href","/docs/transformers/pr_15792/en/model_doc/deberta#transformers.DebertaForTokenClassification"),c(SI,"href","/docs/transformers/pr_15792/en/model_doc/deberta-v2#transformers.DebertaV2ForTokenClassification"),c(PI,"href","/docs/transformers/pr_15792/en/model_doc/distilbert#transformers.DistilBertForTokenClassification"),c($I,"href","/docs/transformers/pr_15792/en/model_doc/electra#transformers.ElectraForTokenClassification"),c(II,"href","/docs/transformers/pr_15792/en/model_doc/flaubert#transformers.FlaubertForTokenClassification"),c(jI,"href","/docs/transformers/pr_15792/en/model_doc/fnet#transformers.FNetForTokenClassification"),c(NI,"href","/docs/transformers/pr_15792/en/model_doc/funnel#transformers.FunnelForTokenClassification"),c(DI,"href","/docs/transformers/pr_15792/en/model_doc/gpt2#transformers.GPT2ForTokenClassification"),c(qI,"href","/docs/transformers/pr_15792/en/model_doc/ibert#transformers.IBertForTokenClassification"),c(GI,"href","/docs/transformers/pr_15792/en/model_doc/layoutlm#transformers.LayoutLMForTokenClassification"),c(OI,"href","/docs/transformers/pr_15792/en/model_doc/layoutlmv2#transformers.LayoutLMv2ForTokenClassification"),c(XI,"href","/docs/transformers/pr_15792/en/model_doc/longformer#transformers.LongformerForTokenClassification"),c(zI,"href","/docs/transformers/pr_15792/en/model_doc/megatron-bert#transformers.MegatronBertForTokenClassification"),c(VI,"href","/docs/transformers/pr_15792/en/model_doc/mobilebert#transformers.MobileBertForTokenClassification"),c(WI,"href","/docs/transformers/pr_15792/en/model_doc/mpnet#transformers.MPNetForTokenClassification"),c(QI,"href","/docs/transformers/pr_15792/en/model_doc/nystromformer#transformers.NystromformerForTokenClassification"),c(HI,"href","/docs/transformers/pr_15792/en/model_doc/qdqbert#transformers.QDQBertForTokenClassification"),c(UI,"href","/docs/transformers/pr_15792/en/model_doc/rembert#transformers.RemBertForTokenClassification"),c(JI,"href","/docs/transformers/pr_15792/en/model_doc/roberta#transformers.RobertaForTokenClassification"),c(YI,"href","/docs/transformers/pr_15792/en/model_doc/roformer#transformers.RoFormerForTokenClassification"),c(KI,"href","/docs/transformers/pr_15792/en/model_doc/squeezebert#transformers.SqueezeBertForTokenClassification"),c(ZI,"href","/docs/transformers/pr_15792/en/model_doc/xlm#transformers.XLMForTokenClassification"),c(ej,"href","/docs/transformers/pr_15792/en/model_doc/xlm-roberta#transformers.XLMRobertaForTokenClassification"),c(oj,"href","/docs/transformers/pr_15792/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForTokenClassification"),c(rj,"href","/docs/transformers/pr_15792/en/model_doc/xlnet#transformers.XLNetForTokenClassification"),c(tj,"href","/docs/transformers/pr_15792/en/model_doc/yoso#transformers.YosoForTokenClassification"),c(Ne,"class","docstring"),c(Zo,"class","docstring"),c(Bb,"id","transformers.AutoModelForQuestionAnswering"),c(Bb,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Bb,"href","#transformers.AutoModelForQuestionAnswering"),c(gd,"class","relative group"),c(Qr,"class","docstring"),c(aj,"href","/docs/transformers/pr_15792/en/model_doc/albert#transformers.AlbertForQuestionAnswering"),c(nj,"href","/docs/transformers/pr_15792/en/model_doc/bart#transformers.BartForQuestionAnswering"),c(sj,"href","/docs/transformers/pr_15792/en/model_doc/bert#transformers.BertForQuestionAnswering"),c(lj,"href","/docs/transformers/pr_15792/en/model_doc/big_bird#transformers.BigBirdForQuestionAnswering"),c(ij,"href","/docs/transformers/pr_15792/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusForQuestionAnswering"),c(dj,"href","/docs/transformers/pr_15792/en/model_doc/camembert#transformers.CamembertForQuestionAnswering"),c(cj,"href","/docs/transformers/pr_15792/en/model_doc/canine#transformers.CanineForQuestionAnswering"),c(fj,"href","/docs/transformers/pr_15792/en/model_doc/convbert#transformers.ConvBertForQuestionAnswering"),c(mj,"href","/docs/transformers/pr_15792/en/model_doc/deberta#transformers.DebertaForQuestionAnswering"),c(gj,"href","/docs/transformers/pr_15792/en/model_doc/deberta-v2#transformers.DebertaV2ForQuestionAnswering"),c(hj,"href","/docs/transformers/pr_15792/en/model_doc/distilbert#transformers.DistilBertForQuestionAnswering"),c(pj,"href","/docs/transformers/pr_15792/en/model_doc/electra#transformers.ElectraForQuestionAnswering"),c(_j,"href","/docs/transformers/pr_15792/en/model_doc/flaubert#transformers.FlaubertForQuestionAnsweringSimple"),c(uj,"href","/docs/transformers/pr_15792/en/model_doc/fnet#transformers.FNetForQuestionAnswering"),c(bj,"href","/docs/transformers/pr_15792/en/model_doc/funnel#transformers.FunnelForQuestionAnswering"),c(vj,"href","/docs/transformers/pr_15792/en/model_doc/gptj#transformers.GPTJForQuestionAnswering"),c(Tj,"href","/docs/transformers/pr_15792/en/model_doc/ibert#transformers.IBertForQuestionAnswering"),c(Fj,"href","/docs/transformers/pr_15792/en/model_doc/layoutlmv2#transformers.LayoutLMv2ForQuestionAnswering"),c(Cj,"href","/docs/transformers/pr_15792/en/model_doc/led#transformers.LEDForQuestionAnswering"),c(Mj,"href","/docs/transformers/pr_15792/en/model_doc/longformer#transformers.LongformerForQuestionAnswering"),c(Ej,"href","/docs/transformers/pr_15792/en/model_doc/lxmert#transformers.LxmertForQuestionAnswering"),c(yj,"href","/docs/transformers/pr_15792/en/model_doc/mbart#transformers.MBartForQuestionAnswering"),c(wj,"href","/docs/transformers/pr_15792/en/model_doc/megatron-bert#transformers.MegatronBertForQuestionAnswering"),c(Aj,"href","/docs/transformers/pr_15792/en/model_doc/mobilebert#transformers.MobileBertForQuestionAnswering"),c(Lj,"href","/docs/transformers/pr_15792/en/model_doc/mpnet#transformers.MPNetForQuestionAnswering"),c(Bj,"href","/docs/transformers/pr_15792/en/model_doc/nystromformer#transformers.NystromformerForQuestionAnswering"),c(kj,"href","/docs/transformers/pr_15792/en/model_doc/qdqbert#transformers.QDQBertForQuestionAnswering"),c(xj,"href","/docs/transformers/pr_15792/en/model_doc/reformer#transformers.ReformerForQuestionAnswering"),c(Rj,"href","/docs/transformers/pr_15792/en/model_doc/rembert#transformers.RemBertForQuestionAnswering"),c(Sj,"href","/docs/transformers/pr_15792/en/model_doc/roberta#transformers.RobertaForQuestionAnswering"),c(Pj,"href","/docs/transformers/pr_15792/en/model_doc/roformer#transformers.RoFormerForQuestionAnswering"),c($j,"href","/docs/transformers/pr_15792/en/model_doc/splinter#transformers.SplinterForQuestionAnswering"),c(Ij,"href","/docs/transformers/pr_15792/en/model_doc/squeezebert#transformers.SqueezeBertForQuestionAnswering"),c(jj,"href","/docs/transformers/pr_15792/en/model_doc/xlm#transformers.XLMForQuestionAnsweringSimple"),c(Nj,"href","/docs/transformers/pr_15792/en/model_doc/xlm-roberta#transformers.XLMRobertaForQuestionAnswering"),c(Dj,"href","/docs/transformers/pr_15792/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForQuestionAnswering"),c(qj,"href","/docs/transformers/pr_15792/en/model_doc/xlnet#transformers.XLNetForQuestionAnsweringSimple"),c(Gj,"href","/docs/transformers/pr_15792/en/model_doc/yoso#transformers.YosoForQuestionAnswering"),c(De,"class","docstring"),c(er,"class","docstring"),c(p5,"id","transformers.AutoModelForTableQuestionAnswering"),c(p5,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(p5,"href","#transformers.AutoModelForTableQuestionAnswering"),c(_d,"class","relative group"),c(Hr,"class","docstring"),c(Oj,"href","/docs/transformers/pr_15792/en/model_doc/tapas#transformers.TapasForQuestionAnswering"),c(qe,"class","docstring"),c(or,"class","docstring"),c(b5,"id","transformers.AutoModelForImageClassification"),c(b5,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(b5,"href","#transformers.AutoModelForImageClassification"),c(vd,"class","relative group"),c(Ur,"class","docstring"),c(Xj,"href","/docs/transformers/pr_15792/en/model_doc/beit#transformers.BeitForImageClassification"),c(zj,"href","/docs/transformers/pr_15792/en/model_doc/convnext#transformers.ConvNextForImageClassification"),c(Vj,"href","/docs/transformers/pr_15792/en/model_doc/deit#transformers.DeiTForImageClassification"),c(Wj,"href","/docs/transformers/pr_15792/en/model_doc/deit#transformers.DeiTForImageClassificationWithTeacher"),c(Qj,"href","/docs/transformers/pr_15792/en/model_doc/imagegpt#transformers.ImageGPTForImageClassification"),c(Hj,"href","/docs/transformers/pr_15792/en/model_doc/perceiver#transformers.PerceiverForImageClassificationLearned"),c(Uj,"href","/docs/transformers/pr_15792/en/model_doc/perceiver#transformers.PerceiverForImageClassificationFourier"),c(Jj,"href","/docs/transformers/pr_15792/en/model_doc/perceiver#transformers.PerceiverForImageClassificationConvProcessing"),c(Yj,"href","/docs/transformers/pr_15792/en/model_doc/poolformer#transformers.PoolFormerForImageClassification"),c(Kj,"href","/docs/transformers/pr_15792/en/model_doc/segformer#transformers.SegformerForImageClassification"),c(Zj,"href","/docs/transformers/pr_15792/en/model_doc/swin#transformers.SwinForImageClassification"),c(eN,"href","/docs/transformers/pr_15792/en/model_doc/vit#transformers.ViTForImageClassification"),c(Ge,"class","docstring"),c(rr,"class","docstring"),c(A5,"id","transformers.AutoModelForVision2Seq"),c(A5,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(A5,"href","#transformers.AutoModelForVision2Seq"),c(Cd,"class","relative group"),c(Jr,"class","docstring"),c(oN,"href","/docs/transformers/pr_15792/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderModel"),c(Oe,"class","docstring"),c(tr,"class","docstring"),c(k5,"id","transformers.AutoModelForAudioClassification"),c(k5,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(k5,"href","#transformers.AutoModelForAudioClassification"),c(yd,"class","relative group"),c(Yr,"class","docstring"),c(rN,"href","/docs/transformers/pr_15792/en/model_doc/hubert#transformers.HubertForSequenceClassification"),c(tN,"href","/docs/transformers/pr_15792/en/model_doc/sew#transformers.SEWForSequenceClassification"),c(aN,"href","/docs/transformers/pr_15792/en/model_doc/sew-d#transformers.SEWDForSequenceClassification"),c(nN,"href","/docs/transformers/pr_15792/en/model_doc/unispeech#transformers.UniSpeechForSequenceClassification"),c(sN,"href","/docs/transformers/pr_15792/en/model_doc/unispeech-sat#transformers.UniSpeechSatForSequenceClassification"),c(lN,"href","/docs/transformers/pr_15792/en/model_doc/wav2vec2#transformers.Wav2Vec2ForSequenceClassification"),c(iN,"href","/docs/transformers/pr_15792/en/model_doc/wavlm#transformers.WavLMForSequenceClassification"),c(Xe,"class","docstring"),c(ar,"class","docstring"),c(D5,"id","transformers.AutoModelForAudioFrameClassification"),c(D5,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(D5,"href","#transformers.AutoModelForAudioFrameClassification"),c(Ld,"class","relative group"),c(Kr,"class","docstring"),c(dN,"href","/docs/transformers/pr_15792/en/model_doc/unispeech-sat#transformers.UniSpeechSatForAudioFrameClassification"),c(cN,"href","/docs/transformers/pr_15792/en/model_doc/wav2vec2#transformers.Wav2Vec2ForAudioFrameClassification"),c(fN,"href","/docs/transformers/pr_15792/en/model_doc/wavlm#transformers.WavLMForAudioFrameClassification"),c(ze,"class","docstring"),c(nr,"class","docstring"),c(z5,"id","transformers.AutoModelForCTC"),c(z5,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(z5,"href","#transformers.AutoModelForCTC"),c(Rd,"class","relative group"),c(Zr,"class","docstring"),c(mN,"href","/docs/transformers/pr_15792/en/model_doc/hubert#transformers.HubertForCTC"),c(gN,"href","/docs/transformers/pr_15792/en/model_doc/sew#transformers.SEWForCTC"),c(hN,"href","/docs/transformers/pr_15792/en/model_doc/sew-d#transformers.SEWDForCTC"),c(pN,"href","/docs/transformers/pr_15792/en/model_doc/unispeech#transformers.UniSpeechForCTC"),c(_N,"href","/docs/transformers/pr_15792/en/model_doc/unispeech-sat#transformers.UniSpeechSatForCTC"),c(uN,"href","/docs/transformers/pr_15792/en/model_doc/wav2vec2#transformers.Wav2Vec2ForCTC"),c(bN,"href","/docs/transformers/pr_15792/en/model_doc/wavlm#transformers.WavLMForCTC"),c(Ve,"class","docstring"),c(sr,"class","docstring"),c(Z5,"id","transformers.AutoModelForSpeechSeq2Seq"),c(Z5,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Z5,"href","#transformers.AutoModelForSpeechSeq2Seq"),c($d,"class","relative group"),c(et,"class","docstring"),c(vN,"href","/docs/transformers/pr_15792/en/model_doc/speech-encoder-decoder#transformers.SpeechEncoderDecoderModel"),c(TN,"href","/docs/transformers/pr_15792/en/model_doc/speech_to_text#transformers.Speech2TextForConditionalGeneration"),c(We,"class","docstring"),c(lr,"class","docstring"),c(tv,"id","transformers.AutoModelForAudioXVector"),c(tv,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(tv,"href","#transformers.AutoModelForAudioXVector"),c(Nd,"class","relative group"),c(ot,"class","docstring"),c(FN,"href","/docs/transformers/pr_15792/en/model_doc/unispeech-sat#transformers.UniSpeechSatForXVector"),c(CN,"href","/docs/transformers/pr_15792/en/model_doc/wav2vec2#transformers.Wav2Vec2ForXVector"),c(MN,"href","/docs/transformers/pr_15792/en/model_doc/wavlm#transformers.WavLMForXVector"),c(Qe,"class","docstring"),c(ir,"class","docstring"),c(iv,"id","transformers.AutoModelForMaskedImageModeling"),c(iv,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(iv,"href","#transformers.AutoModelForMaskedImageModeling"),c(Od,"class","relative group"),c(rt,"class","docstring"),c(EN,"href","/docs/transformers/pr_15792/en/model_doc/deit#transformers.DeiTForMaskedImageModeling"),c(yN,"href","/docs/transformers/pr_15792/en/model_doc/swin#transformers.SwinForMaskedImageModeling"),c(wN,"href","/docs/transformers/pr_15792/en/model_doc/vit#transformers.ViTForMaskedImageModeling"),c(He,"class","docstring"),c(dr,"class","docstring"),c(gv,"id","transformers.AutoModelForObjectDetection"),c(gv,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(gv,"href","#transformers.AutoModelForObjectDetection"),c(Wd,"class","relative group"),c(tt,"class","docstring"),c(AN,"href","/docs/transformers/pr_15792/en/model_doc/detr#transformers.DetrForObjectDetection"),c(Ue,"class","docstring"),c(cr,"class","docstring"),c(_v,"id","transformers.AutoModelForImageSegmentation"),c(_v,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(_v,"href","#transformers.AutoModelForImageSegmentation"),c(Ud,"class","relative group"),c(at,"class","docstring"),c(LN,"href","/docs/transformers/pr_15792/en/model_doc/detr#transformers.DetrForSegmentation"),c(Je,"class","docstring"),c(fr,"class","docstring"),c(vv,"id","transformers.AutoModelForSemanticSegmentation"),c(vv,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(vv,"href","#transformers.AutoModelForSemanticSegmentation"),c(Kd,"class","relative group"),c(nt,"class","docstring"),c(BN,"href","/docs/transformers/pr_15792/en/model_doc/beit#transformers.BeitForSemanticSegmentation"),c(kN,"href","/docs/transformers/pr_15792/en/model_doc/segformer#transformers.SegformerForSemanticSegmentation"),c(Ye,"class","docstring"),c(mr,"class","docstring"),c(Mv,"id","transformers.TFAutoModel"),c(Mv,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Mv,"href","#transformers.TFAutoModel"),c(oc,"class","relative group"),c(st,"class","docstring"),c(xN,"href","/docs/transformers/pr_15792/en/model_doc/albert#transformers.TFAlbertModel"),c(RN,"href","/docs/transformers/pr_15792/en/model_doc/bart#transformers.TFBartModel"),c(SN,"href","/docs/transformers/pr_15792/en/model_doc/bert#transformers.TFBertModel"),c(PN,"href","/docs/transformers/pr_15792/en/model_doc/blenderbot#transformers.TFBlenderbotModel"),c($N,"href","/docs/transformers/pr_15792/en/model_doc/blenderbot-small#transformers.TFBlenderbotSmallModel"),c(IN,"href","/docs/transformers/pr_15792/en/model_doc/camembert#transformers.TFCamembertModel"),c(jN,"href","/docs/transformers/pr_15792/en/model_doc/clip#transformers.TFCLIPModel"),c(NN,"href","/docs/transformers/pr_15792/en/model_doc/convbert#transformers.TFConvBertModel"),c(DN,"href","/docs/transformers/pr_15792/en/model_doc/ctrl#transformers.TFCTRLModel"),c(qN,"href","/docs/transformers/pr_15792/en/model_doc/deberta#transformers.TFDebertaModel"),c(GN,"href","/docs/transformers/pr_15792/en/model_doc/deberta-v2#transformers.TFDebertaV2Model"),c(ON,"href","/docs/transformers/pr_15792/en/model_doc/distilbert#transformers.TFDistilBertModel"),c(XN,"href","/docs/transformers/pr_15792/en/model_doc/dpr#transformers.TFDPRQuestionEncoder"),c(zN,"href","/docs/transformers/pr_15792/en/model_doc/electra#transformers.TFElectraModel"),c(VN,"href","/docs/transformers/pr_15792/en/model_doc/flaubert#transformers.TFFlaubertModel"),c(WN,"href","/docs/transformers/pr_15792/en/model_doc/funnel#transformers.TFFunnelModel"),c(QN,"href","/docs/transformers/pr_15792/en/model_doc/funnel#transformers.TFFunnelBaseModel"),c(HN,"href","/docs/transformers/pr_15792/en/model_doc/gpt2#transformers.TFGPT2Model"),c(UN,"href","/docs/transformers/pr_15792/en/model_doc/hubert#transformers.TFHubertModel"),c(JN,"href","/docs/transformers/pr_15792/en/model_doc/layoutlm#transformers.TFLayoutLMModel"),c(YN,"href","/docs/transformers/pr_15792/en/model_doc/led#transformers.TFLEDModel"),c(KN,"href","/docs/transformers/pr_15792/en/model_doc/longformer#transformers.TFLongformerModel"),c(ZN,"href","/docs/transformers/pr_15792/en/model_doc/lxmert#transformers.TFLxmertModel"),c(eD,"href","/docs/transformers/pr_15792/en/model_doc/marian#transformers.TFMarianModel"),c(oD,"href","/docs/transformers/pr_15792/en/model_doc/mbart#transformers.TFMBartModel"),c(rD,"href","/docs/transformers/pr_15792/en/model_doc/mobilebert#transformers.TFMobileBertModel"),c(tD,"href","/docs/transformers/pr_15792/en/model_doc/mpnet#transformers.TFMPNetModel"),c(aD,"href","/docs/transformers/pr_15792/en/model_doc/mt5#transformers.TFMT5Model"),c(nD,"href","/docs/transformers/pr_15792/en/model_doc/openai-gpt#transformers.TFOpenAIGPTModel"),c(sD,"href","/docs/transformers/pr_15792/en/model_doc/pegasus#transformers.TFPegasusModel"),c(lD,"href","/docs/transformers/pr_15792/en/model_doc/rembert#transformers.TFRemBertModel"),c(iD,"href","/docs/transformers/pr_15792/en/model_doc/roberta#transformers.TFRobertaModel"),c(dD,"href","/docs/transformers/pr_15792/en/model_doc/roformer#transformers.TFRoFormerModel"),c(cD,"href","/docs/transformers/pr_15792/en/model_doc/speech_to_text#transformers.TFSpeech2TextModel"),c(fD,"href","/docs/transformers/pr_15792/en/model_doc/t5#transformers.TFT5Model"),c(mD,"href","/docs/transformers/pr_15792/en/model_doc/tapas#transformers.TFTapasModel"),c(gD,"href","/docs/transformers/pr_15792/en/model_doc/transfo-xl#transformers.TFTransfoXLModel"),c(hD,"href","/docs/transformers/pr_15792/en/model_doc/vit#transformers.TFViTModel"),c(pD,"href","/docs/transformers/pr_15792/en/model_doc/wav2vec2#transformers.TFWav2Vec2Model"),c(_D,"href","/docs/transformers/pr_15792/en/model_doc/xlm#transformers.TFXLMModel"),c(uD,"href","/docs/transformers/pr_15792/en/model_doc/xlm-roberta#transformers.TFXLMRobertaModel"),c(bD,"href","/docs/transformers/pr_15792/en/model_doc/xlnet#transformers.TFXLNetModel"),c(go,"class","docstring"),c(gr,"class","docstring"),c(cT,"id","transformers.TFAutoModelForPreTraining"),c(cT,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(cT,"href","#transformers.TFAutoModelForPreTraining"),c(ac,"class","relative group"),c(lt,"class","docstring"),c(vD,"href","/docs/transformers/pr_15792/en/model_doc/albert#transformers.TFAlbertForPreTraining"),c(TD,"href","/docs/transformers/pr_15792/en/model_doc/bart#transformers.TFBartForConditionalGeneration"),c(FD,"href","/docs/transformers/pr_15792/en/model_doc/bert#transformers.TFBertForPreTraining"),c(CD,"href","/docs/transformers/pr_15792/en/model_doc/camembert#transformers.TFCamembertForMaskedLM"),c(MD,"href","/docs/transformers/pr_15792/en/model_doc/ctrl#transformers.TFCTRLLMHeadModel"),c(ED,"href","/docs/transformers/pr_15792/en/model_doc/distilbert#transformers.TFDistilBertForMaskedLM"),c(yD,"href","/docs/transformers/pr_15792/en/model_doc/electra#transformers.TFElectraForPreTraining"),c(wD,"href","/docs/transformers/pr_15792/en/model_doc/flaubert#transformers.TFFlaubertWithLMHeadModel"),c(AD,"href","/docs/transformers/pr_15792/en/model_doc/funnel#transformers.TFFunnelForPreTraining"),c(LD,"href","/docs/transformers/pr_15792/en/model_doc/gpt2#transformers.TFGPT2LMHeadModel"),c(BD,"href","/docs/transformers/pr_15792/en/model_doc/layoutlm#transformers.TFLayoutLMForMaskedLM"),c(kD,"href","/docs/transformers/pr_15792/en/model_doc/lxmert#transformers.TFLxmertForPreTraining"),c(xD,"href","/docs/transformers/pr_15792/en/model_doc/mobilebert#transformers.TFMobileBertForPreTraining"),c(RD,"href","/docs/transformers/pr_15792/en/model_doc/mpnet#transformers.TFMPNetForMaskedLM"),c(SD,"href","/docs/transformers/pr_15792/en/model_doc/openai-gpt#transformers.TFOpenAIGPTLMHeadModel"),c(PD,"href","/docs/transformers/pr_15792/en/model_doc/roberta#transformers.TFRobertaForMaskedLM"),c($D,"href","/docs/transformers/pr_15792/en/model_doc/t5#transformers.TFT5ForConditionalGeneration"),c(ID,"href","/docs/transformers/pr_15792/en/model_doc/tapas#transformers.TFTapasForMaskedLM"),c(jD,"href","/docs/transformers/pr_15792/en/model_doc/transfo-xl#transformers.TFTransfoXLLMHeadModel"),c(ND,"href","/docs/transformers/pr_15792/en/model_doc/xlm#transformers.TFXLMWithLMHeadModel"),c(DD,"href","/docs/transformers/pr_15792/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForMaskedLM"),c(qD,"href","/docs/transformers/pr_15792/en/model_doc/xlnet#transformers.TFXLNetLMHeadModel"),c(ho,"class","docstring"),c(hr,"class","docstring"),c(ST,"id","transformers.TFAutoModelForCausalLM"),c(ST,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(ST,"href","#transformers.TFAutoModelForCausalLM"),c(lc,"class","relative group"),c(it,"class","docstring"),c(GD,"href","/docs/transformers/pr_15792/en/model_doc/bert#transformers.TFBertLMHeadModel"),c(OD,"href","/docs/transformers/pr_15792/en/model_doc/ctrl#transformers.TFCTRLLMHeadModel"),c(XD,"href","/docs/transformers/pr_15792/en/model_doc/gpt2#transformers.TFGPT2LMHeadModel"),c(zD,"href","/docs/transformers/pr_15792/en/model_doc/openai-gpt#transformers.TFOpenAIGPTLMHeadModel"),c(VD,"href","/docs/transformers/pr_15792/en/model_doc/rembert#transformers.TFRemBertForCausalLM"),c(WD,"href","/docs/transformers/pr_15792/en/model_doc/roberta#transformers.TFRobertaForCausalLM"),c(QD,"href","/docs/transformers/pr_15792/en/model_doc/roformer#transformers.TFRoFormerForCausalLM"),c(HD,"href","/docs/transformers/pr_15792/en/model_doc/transfo-xl#transformers.TFTransfoXLLMHeadModel"),c(UD,"href","/docs/transformers/pr_15792/en/model_doc/xlm#transformers.TFXLMWithLMHeadModel"),c(JD,"href","/docs/transformers/pr_15792/en/model_doc/xlnet#transformers.TFXLNetLMHeadModel"),c(po,"class","docstring"),c(pr,"class","docstring"),c(zT,"id","transformers.TFAutoModelForImageClassification"),c(zT,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(zT,"href","#transformers.TFAutoModelForImageClassification"),c(cc,"class","relative group"),c(dt,"class","docstring"),c(YD,"href","/docs/transformers/pr_15792/en/model_doc/vit#transformers.TFViTForImageClassification"),c(_o,"class","docstring"),c(_r,"class","docstring"),c(WT,"id","transformers.TFAutoModelForMaskedLM"),c(WT,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(WT,"href","#transformers.TFAutoModelForMaskedLM"),c(gc,"class","relative group"),c(ct,"class","docstring"),c(KD,"href","/docs/transformers/pr_15792/en/model_doc/albert#transformers.TFAlbertForMaskedLM"),c(ZD,"href","/docs/transformers/pr_15792/en/model_doc/bert#transformers.TFBertForMaskedLM"),c(eq,"href","/docs/transformers/pr_15792/en/model_doc/camembert#transformers.TFCamembertForMaskedLM"),c(oq,"href","/docs/transformers/pr_15792/en/model_doc/convbert#transformers.TFConvBertForMaskedLM"),c(rq,"href","/docs/transformers/pr_15792/en/model_doc/deberta#transformers.TFDebertaForMaskedLM"),c(tq,"href","/docs/transformers/pr_15792/en/model_doc/deberta-v2#transformers.TFDebertaV2ForMaskedLM"),c(aq,"href","/docs/transformers/pr_15792/en/model_doc/distilbert#transformers.TFDistilBertForMaskedLM"),c(nq,"href","/docs/transformers/pr_15792/en/model_doc/electra#transformers.TFElectraForMaskedLM"),c(sq,"href","/docs/transformers/pr_15792/en/model_doc/flaubert#transformers.TFFlaubertWithLMHeadModel"),c(lq,"href","/docs/transformers/pr_15792/en/model_doc/funnel#transformers.TFFunnelForMaskedLM"),c(iq,"href","/docs/transformers/pr_15792/en/model_doc/layoutlm#transformers.TFLayoutLMForMaskedLM"),c(dq,"href","/docs/transformers/pr_15792/en/model_doc/longformer#transformers.TFLongformerForMaskedLM"),c(cq,"href","/docs/transformers/pr_15792/en/model_doc/mobilebert#transformers.TFMobileBertForMaskedLM"),c(fq,"href","/docs/transformers/pr_15792/en/model_doc/mpnet#transformers.TFMPNetForMaskedLM"),c(mq,"href","/docs/transformers/pr_15792/en/model_doc/rembert#transformers.TFRemBertForMaskedLM"),c(gq,"href","/docs/transformers/pr_15792/en/model_doc/roberta#transformers.TFRobertaForMaskedLM"),c(hq,"href","/docs/transformers/pr_15792/en/model_doc/roformer#transformers.TFRoFormerForMaskedLM"),c(pq,"href","/docs/transformers/pr_15792/en/model_doc/tapas#transformers.TFTapasForMaskedLM"),c(_q,"href","/docs/transformers/pr_15792/en/model_doc/xlm#transformers.TFXLMWithLMHeadModel"),c(uq,"href","/docs/transformers/pr_15792/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForMaskedLM"),c(uo,"class","docstring"),c(ur,"class","docstring"),c(g7,"id","transformers.TFAutoModelForSeq2SeqLM"),c(g7,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(g7,"href","#transformers.TFAutoModelForSeq2SeqLM"),c(_c,"class","relative group"),c(ft,"class","docstring"),c(bq,"href","/docs/transformers/pr_15792/en/model_doc/bart#transformers.TFBartForConditionalGeneration"),c(vq,"href","/docs/transformers/pr_15792/en/model_doc/blenderbot#transformers.TFBlenderbotForConditionalGeneration"),c(Tq,"href","/docs/transformers/pr_15792/en/model_doc/blenderbot-small#transformers.TFBlenderbotSmallForConditionalGeneration"),c(Fq,"href","/docs/transformers/pr_15792/en/model_doc/encoder-decoder#transformers.TFEncoderDecoderModel"),c(Cq,"href","/docs/transformers/pr_15792/en/model_doc/led#transformers.TFLEDForConditionalGeneration"),c(Mq,"href","/docs/transformers/pr_15792/en/model_doc/marian#transformers.TFMarianMTModel"),c(Eq,"href","/docs/transformers/pr_15792/en/model_doc/mbart#transformers.TFMBartForConditionalGeneration"),c(yq,"href","/docs/transformers/pr_15792/en/model_doc/mt5#transformers.TFMT5ForConditionalGeneration"),c(wq,"href","/docs/transformers/pr_15792/en/model_doc/pegasus#transformers.TFPegasusForConditionalGeneration"),c(Aq,"href","/docs/transformers/pr_15792/en/model_doc/t5#transformers.TFT5ForConditionalGeneration"),c(bo,"class","docstring"),c(br,"class","docstring"),c(E7,"id","transformers.TFAutoModelForSequenceClassification"),c(E7,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(E7,"href","#transformers.TFAutoModelForSequenceClassification"),c(vc,"class","relative group"),c(mt,"class","docstring"),c(Lq,"href","/docs/transformers/pr_15792/en/model_doc/albert#transformers.TFAlbertForSequenceClassification"),c(Bq,"href","/docs/transformers/pr_15792/en/model_doc/bert#transformers.TFBertForSequenceClassification"),c(kq,"href","/docs/transformers/pr_15792/en/model_doc/camembert#transformers.TFCamembertForSequenceClassification"),c(xq,"href","/docs/transformers/pr_15792/en/model_doc/convbert#transformers.TFConvBertForSequenceClassification"),c(Rq,"href","/docs/transformers/pr_15792/en/model_doc/ctrl#transformers.TFCTRLForSequenceClassification"),c(Sq,"href","/docs/transformers/pr_15792/en/model_doc/deberta#transformers.TFDebertaForSequenceClassification"),c(Pq,"href","/docs/transformers/pr_15792/en/model_doc/deberta-v2#transformers.TFDebertaV2ForSequenceClassification"),c($q,"href","/docs/transformers/pr_15792/en/model_doc/distilbert#transformers.TFDistilBertForSequenceClassification"),c(Iq,"href","/docs/transformers/pr_15792/en/model_doc/electra#transformers.TFElectraForSequenceClassification"),c(jq,"href","/docs/transformers/pr_15792/en/model_doc/flaubert#transformers.TFFlaubertForSequenceClassification"),c(Nq,"href","/docs/transformers/pr_15792/en/model_doc/funnel#transformers.TFFunnelForSequenceClassification"),c(Dq,"href","/docs/transformers/pr_15792/en/model_doc/gpt2#transformers.TFGPT2ForSequenceClassification"),c(qq,"href","/docs/transformers/pr_15792/en/model_doc/layoutlm#transformers.TFLayoutLMForSequenceClassification"),c(Gq,"href","/docs/transformers/pr_15792/en/model_doc/longformer#transformers.TFLongformerForSequenceClassification"),c(Oq,"href","/docs/transformers/pr_15792/en/model_doc/mobilebert#transformers.TFMobileBertForSequenceClassification"),c(Xq,"href","/docs/transformers/pr_15792/en/model_doc/mpnet#transformers.TFMPNetForSequenceClassification"),c(zq,"href","/docs/transformers/pr_15792/en/model_doc/openai-gpt#transformers.TFOpenAIGPTForSequenceClassification"),c(Vq,"href","/docs/transformers/pr_15792/en/model_doc/rembert#transformers.TFRemBertForSequenceClassification"),c(Wq,"href","/docs/transformers/pr_15792/en/model_doc/roberta#transformers.TFRobertaForSequenceClassification"),c(Qq,"href","/docs/transformers/pr_15792/en/model_doc/roformer#transformers.TFRoFormerForSequenceClassification"),c(Hq,"href","/docs/transformers/pr_15792/en/model_doc/tapas#transformers.TFTapasForSequenceClassification"),c(Uq,"href","/docs/transformers/pr_15792/en/model_doc/transfo-xl#transformers.TFTransfoXLForSequenceClassification"),c(Jq,"href","/docs/transformers/pr_15792/en/model_doc/xlm#transformers.TFXLMForSequenceClassification"),c(Yq,"href","/docs/transformers/pr_15792/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForSequenceClassification"),c(Kq,"href","/docs/transformers/pr_15792/en/model_doc/xlnet#transformers.TFXLNetForSequenceClassification"),c(vo,"class","docstring"),c(vr,"class","docstring"),c(J7,"id","transformers.TFAutoModelForMultipleChoice"),c(J7,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(J7,"href","#transformers.TFAutoModelForMultipleChoice"),c(Cc,"class","relative group"),c(gt,"class","docstring"),c(Zq,"href","/docs/transformers/pr_15792/en/model_doc/albert#transformers.TFAlbertForMultipleChoice"),c(eG,"href","/docs/transformers/pr_15792/en/model_doc/bert#transformers.TFBertForMultipleChoice"),c(oG,"href","/docs/transformers/pr_15792/en/model_doc/camembert#transformers.TFCamembertForMultipleChoice"),c(rG,"href","/docs/transformers/pr_15792/en/model_doc/convbert#transformers.TFConvBertForMultipleChoice"),c(tG,"href","/docs/transformers/pr_15792/en/model_doc/distilbert#transformers.TFDistilBertForMultipleChoice"),c(aG,"href","/docs/transformers/pr_15792/en/model_doc/electra#transformers.TFElectraForMultipleChoice"),c(nG,"href","/docs/transformers/pr_15792/en/model_doc/flaubert#transformers.TFFlaubertForMultipleChoice"),c(sG,"href","/docs/transformers/pr_15792/en/model_doc/funnel#transformers.TFFunnelForMultipleChoice"),c(lG,"href","/docs/transformers/pr_15792/en/model_doc/longformer#transformers.TFLongformerForMultipleChoice"),c(iG,"href","/docs/transformers/pr_15792/en/model_doc/mobilebert#transformers.TFMobileBertForMultipleChoice"),c(dG,"href","/docs/transformers/pr_15792/en/model_doc/mpnet#transformers.TFMPNetForMultipleChoice"),c(cG,"href","/docs/transformers/pr_15792/en/model_doc/rembert#transformers.TFRemBertForMultipleChoice"),c(fG,"href","/docs/transformers/pr_15792/en/model_doc/roberta#transformers.TFRobertaForMultipleChoice"),c(mG,"href","/docs/transformers/pr_15792/en/model_doc/roformer#transformers.TFRoFormerForMultipleChoice"),c(gG,"href","/docs/transformers/pr_15792/en/model_doc/xlm#transformers.TFXLMForMultipleChoice"),c(hG,"href","/docs/transformers/pr_15792/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForMultipleChoice"),c(pG,"href","/docs/transformers/pr_15792/en/model_doc/xlnet#transformers.TFXLNetForMultipleChoice"),c(To,"class","docstring"),c(Tr,"class","docstring"),c(hF,"id","transformers.TFAutoModelForTableQuestionAnswering"),c(hF,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(hF,"href","#transformers.TFAutoModelForTableQuestionAnswering"),c(yc,"class","relative group"),c(ht,"class","docstring"),c(_G,"href","/docs/transformers/pr_15792/en/model_doc/tapas#transformers.TFTapasForQuestionAnswering"),c(Fo,"class","docstring"),c(Fr,"class","docstring"),c(_F,"id","transformers.TFAutoModelForTokenClassification"),c(_F,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(_F,"href","#transformers.TFAutoModelForTokenClassification"),c(Lc,"class","relative group"),c(pt,"class","docstring"),c(uG,"href","/docs/transformers/pr_15792/en/model_doc/albert#transformers.TFAlbertForTokenClassification"),c(bG,"href","/docs/transformers/pr_15792/en/model_doc/bert#transformers.TFBertForTokenClassification"),c(vG,"href","/docs/transformers/pr_15792/en/model_doc/camembert#transformers.TFCamembertForTokenClassification"),c(TG,"href","/docs/transformers/pr_15792/en/model_doc/convbert#transformers.TFConvBertForTokenClassification"),c(FG,"href","/docs/transformers/pr_15792/en/model_doc/deberta#transformers.TFDebertaForTokenClassification"),c(CG,"href","/docs/transformers/pr_15792/en/model_doc/deberta-v2#transformers.TFDebertaV2ForTokenClassification"),c(MG,"href","/docs/transformers/pr_15792/en/model_doc/distilbert#transformers.TFDistilBertForTokenClassification"),c(EG,"href","/docs/transformers/pr_15792/en/model_doc/electra#transformers.TFElectraForTokenClassification"),c(yG,"href","/docs/transformers/pr_15792/en/model_doc/flaubert#transformers.TFFlaubertForTokenClassification"),c(wG,"href","/docs/transformers/pr_15792/en/model_doc/funnel#transformers.TFFunnelForTokenClassification"),c(AG,"href","/docs/transformers/pr_15792/en/model_doc/layoutlm#transformers.TFLayoutLMForTokenClassification"),c(LG,"href","/docs/transformers/pr_15792/en/model_doc/longformer#transformers.TFLongformerForTokenClassification"),c(BG,"href","/docs/transformers/pr_15792/en/model_doc/mobilebert#transformers.TFMobileBertForTokenClassification"),c(kG,"href","/docs/transformers/pr_15792/en/model_doc/mpnet#transformers.TFMPNetForTokenClassification"),c(xG,"href","/docs/transformers/pr_15792/en/model_doc/rembert#transformers.TFRemBertForTokenClassification"),c(RG,"href","/docs/transformers/pr_15792/en/model_doc/roberta#transformers.TFRobertaForTokenClassification"),c(SG,"href","/docs/transformers/pr_15792/en/model_doc/roformer#transformers.TFRoFormerForTokenClassification"),c(PG,"href","/docs/transformers/pr_15792/en/model_doc/xlm#transformers.TFXLMForTokenClassification"),c($G,"href","/docs/transformers/pr_15792/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForTokenClassification"),c(IG,"href","/docs/transformers/pr_15792/en/model_doc/xlnet#transformers.TFXLNetForTokenClassification"),c(Co,"class","docstring"),c(Cr,"class","docstring"),c(jF,"id","transformers.TFAutoModelForQuestionAnswering"),c(jF,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(jF,"href","#transformers.TFAutoModelForQuestionAnswering"),c(xc,"class","relative group"),c(_t,"class","docstring"),c(jG,"href","/docs/transformers/pr_15792/en/model_doc/albert#transformers.TFAlbertForQuestionAnswering"),c(NG,"href","/docs/transformers/pr_15792/en/model_doc/bert#transformers.TFBertForQuestionAnswering"),c(DG,"href","/docs/transformers/pr_15792/en/model_doc/camembert#transformers.TFCamembertForQuestionAnswering"),c(qG,"href","/docs/transformers/pr_15792/en/model_doc/convbert#transformers.TFConvBertForQuestionAnswering"),c(GG,"href","/docs/transformers/pr_15792/en/model_doc/deberta#transformers.TFDebertaForQuestionAnswering"),c(OG,"href","/docs/transformers/pr_15792/en/model_doc/deberta-v2#transformers.TFDebertaV2ForQuestionAnswering"),c(XG,"href","/docs/transformers/pr_15792/en/model_doc/distilbert#transformers.TFDistilBertForQuestionAnswering"),c(zG,"href","/docs/transformers/pr_15792/en/model_doc/electra#transformers.TFElectraForQuestionAnswering"),c(VG,"href","/docs/transformers/pr_15792/en/model_doc/flaubert#transformers.TFFlaubertForQuestionAnsweringSimple"),c(WG,"href","/docs/transformers/pr_15792/en/model_doc/funnel#transformers.TFFunnelForQuestionAnswering"),c(QG,"href","/docs/transformers/pr_15792/en/model_doc/longformer#transformers.TFLongformerForQuestionAnswering"),c(HG,"href","/docs/transformers/pr_15792/en/model_doc/mobilebert#transformers.TFMobileBertForQuestionAnswering"),c(UG,"href","/docs/transformers/pr_15792/en/model_doc/mpnet#transformers.TFMPNetForQuestionAnswering"),c(JG,"href","/docs/transformers/pr_15792/en/model_doc/rembert#transformers.TFRemBertForQuestionAnswering"),c(YG,"href","/docs/transformers/pr_15792/en/model_doc/roberta#transformers.TFRobertaForQuestionAnswering"),c(KG,"href","/docs/transformers/pr_15792/en/model_doc/roformer#transformers.TFRoFormerForQuestionAnswering"),c(ZG,"href","/docs/transformers/pr_15792/en/model_doc/xlm#transformers.TFXLMForQuestionAnsweringSimple"),c(eO,"href","/docs/transformers/pr_15792/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForQuestionAnswering"),c(oO,"href","/docs/transformers/pr_15792/en/model_doc/xlnet#transformers.TFXLNetForQuestionAnsweringSimple"),c(Mo,"class","docstring"),c(Mr,"class","docstring"),c(t9,"id","transformers.TFAutoModelForVision2Seq"),c(t9,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(t9,"href","#transformers.TFAutoModelForVision2Seq"),c(Pc,"class","relative group"),c(ut,"class","docstring"),c(rO,"href","/docs/transformers/pr_15792/en/model_doc/vision-encoder-decoder#transformers.TFVisionEncoderDecoderModel"),c(Eo,"class","docstring"),c(Er,"class","docstring"),c(n9,"id","transformers.TFAutoModelForSpeechSeq2Seq"),c(n9,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(n9,"href","#transformers.TFAutoModelForSpeechSeq2Seq"),c(jc,"class","relative group"),c(bt,"class","docstring"),c(tO,"href","/docs/transformers/pr_15792/en/model_doc/speech_to_text#transformers.TFSpeech2TextForConditionalGeneration"),c(yo,"class","docstring"),c(yr,"class","docstring"),c(l9,"id","transformers.FlaxAutoModel"),c(l9,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(l9,"href","#transformers.FlaxAutoModel"),c(qc,"class","relative group"),c(vt,"class","docstring"),c(aO,"href","/docs/transformers/pr_15792/en/model_doc/albert#transformers.FlaxAlbertModel"),c(nO,"href","/docs/transformers/pr_15792/en/model_doc/bart#transformers.FlaxBartModel"),c(sO,"href","/docs/transformers/pr_15792/en/model_doc/beit#transformers.FlaxBeitModel"),c(lO,"href","/docs/transformers/pr_15792/en/model_doc/bert#transformers.FlaxBertModel"),c(iO,"href","/docs/transformers/pr_15792/en/model_doc/big_bird#transformers.FlaxBigBirdModel"),c(dO,"href","/docs/transformers/pr_15792/en/model_doc/blenderbot#transformers.FlaxBlenderbotModel"),c(cO,"href","/docs/transformers/pr_15792/en/model_doc/blenderbot-small#transformers.FlaxBlenderbotSmallModel"),c(fO,"href","/docs/transformers/pr_15792/en/model_doc/clip#transformers.FlaxCLIPModel"),c(mO,"href","/docs/transformers/pr_15792/en/model_doc/distilbert#transformers.FlaxDistilBertModel"),c(gO,"href","/docs/transformers/pr_15792/en/model_doc/electra#transformers.FlaxElectraModel"),c(hO,"href","/docs/transformers/pr_15792/en/model_doc/gpt2#transformers.FlaxGPT2Model"),c(pO,"href","/docs/transformers/pr_15792/en/model_doc/gpt_neo#transformers.FlaxGPTNeoModel"),c(_O,"href","/docs/transformers/pr_15792/en/model_doc/gptj#transformers.FlaxGPTJModel"),c(uO,"href","/docs/transformers/pr_15792/en/model_doc/marian#transformers.FlaxMarianModel"),c(bO,"href","/docs/transformers/pr_15792/en/model_doc/mbart#transformers.FlaxMBartModel"),c(vO,"href","/docs/transformers/pr_15792/en/model_doc/mt5#transformers.FlaxMT5Model"),c(TO,"href","/docs/transformers/pr_15792/en/model_doc/pegasus#transformers.FlaxPegasusModel"),c(FO,"href","/docs/transformers/pr_15792/en/model_doc/roberta#transformers.FlaxRobertaModel"),c(CO,"href","/docs/transformers/pr_15792/en/model_doc/roformer#transformers.FlaxRoFormerModel"),c(MO,"href","/docs/transformers/pr_15792/en/model_doc/t5#transformers.FlaxT5Model"),c(EO,"href","/docs/transformers/pr_15792/en/model_doc/vision-text-dual-encoder#transformers.FlaxVisionTextDualEncoderModel"),c(yO,"href","/docs/transformers/pr_15792/en/model_doc/vit#transformers.FlaxViTModel"),c(wO,"href","/docs/transformers/pr_15792/en/model_doc/wav2vec2#transformers.FlaxWav2Vec2Model"),c(AO,"href","/docs/transformers/pr_15792/en/model_doc/xglm#transformers.FlaxXGLMModel"),c(wo,"class","docstring"),c(wr,"class","docstring"),c(R9,"id","transformers.FlaxAutoModelForCausalLM"),c(R9,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(R9,"href","#transformers.FlaxAutoModelForCausalLM"),c(Xc,"class","relative group"),c(Tt,"class","docstring"),c(LO,"href","/docs/transformers/pr_15792/en/model_doc/gpt2#transformers.FlaxGPT2LMHeadModel"),c(BO,"href","/docs/transformers/pr_15792/en/model_doc/gpt_neo#transformers.FlaxGPTNeoForCausalLM"),c(kO,"href","/docs/transformers/pr_15792/en/model_doc/gptj#transformers.FlaxGPTJForCausalLM"),c(xO,"href","/docs/transformers/pr_15792/en/model_doc/xglm#transformers.FlaxXGLMForCausalLM"),c(Ao,"class","docstring"),c(Ar,"class","docstring"),c(j9,"id","transformers.FlaxAutoModelForPreTraining"),c(j9,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(j9,"href","#transformers.FlaxAutoModelForPreTraining"),c(Wc,"class","relative group"),c(Ft,"class","docstring"),c(RO,"href","/docs/transformers/pr_15792/en/model_doc/albert#transformers.FlaxAlbertForPreTraining"),c(SO,"href","/docs/transformers/pr_15792/en/model_doc/bart#transformers.FlaxBartForConditionalGeneration"),c(PO,"href","/docs/transformers/pr_15792/en/model_doc/bert#transformers.FlaxBertForPreTraining"),c($O,"href","/docs/transformers/pr_15792/en/model_doc/big_bird#transformers.FlaxBigBirdForPreTraining"),c(IO,"href","/docs/transformers/pr_15792/en/model_doc/electra#transformers.FlaxElectraForPreTraining"),c(jO,"href","/docs/transformers/pr_15792/en/model_doc/mbart#transformers.FlaxMBartForConditionalGeneration"),c(NO,"href","/docs/transformers/pr_15792/en/model_doc/mt5#transformers.FlaxMT5ForConditionalGeneration"),c(DO,"href","/docs/transformers/pr_15792/en/model_doc/roberta#transformers.FlaxRobertaForMaskedLM"),c(qO,"href","/docs/transformers/pr_15792/en/model_doc/roformer#transformers.FlaxRoFormerForMaskedLM"),c(GO,"href","/docs/transformers/pr_15792/en/model_doc/t5#transformers.FlaxT5ForConditionalGeneration"),c(OO,"href","/docs/transformers/pr_15792/en/model_doc/wav2vec2#transformers.FlaxWav2Vec2ForPreTraining"),c(Lo,"class","docstring"),c(Lr,"class","docstring"),c(U9,"id","transformers.FlaxAutoModelForMaskedLM"),c(U9,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(U9,"href","#transformers.FlaxAutoModelForMaskedLM"),c(Uc,"class","relative group"),c(Ct,"class","docstring"),c(XO,"href","/docs/transformers/pr_15792/en/model_doc/albert#transformers.FlaxAlbertForMaskedLM"),c(zO,"href","/docs/transformers/pr_15792/en/model_doc/bart#transformers.FlaxBartForConditionalGeneration"),c(VO,"href","/docs/transformers/pr_15792/en/model_doc/bert#transformers.FlaxBertForMaskedLM"),c(WO,"href","/docs/transformers/pr_15792/en/model_doc/big_bird#transformers.FlaxBigBirdForMaskedLM"),c(QO,"href","/docs/transformers/pr_15792/en/model_doc/distilbert#transformers.FlaxDistilBertForMaskedLM"),c(HO,"href","/docs/transformers/pr_15792/en/model_doc/electra#transformers.FlaxElectraForMaskedLM"),c(UO,"href","/docs/transformers/pr_15792/en/model_doc/mbart#transformers.FlaxMBartForConditionalGeneration"),c(JO,"href","/docs/transformers/pr_15792/en/model_doc/roberta#transformers.FlaxRobertaForMaskedLM"),c(YO,"href","/docs/transformers/pr_15792/en/model_doc/roformer#transformers.FlaxRoFormerForMaskedLM"),c(Bo,"class","docstring"),c(Br,"class","docstring"),c(nC,"id","transformers.FlaxAutoModelForSeq2SeqLM"),c(nC,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(nC,"href","#transformers.FlaxAutoModelForSeq2SeqLM"),c(Kc,"class","relative group"),c(Mt,"class","docstring"),c(KO,"href","/docs/transformers/pr_15792/en/model_doc/bart#transformers.FlaxBartForConditionalGeneration"),c(ZO,"href","/docs/transformers/pr_15792/en/model_doc/blenderbot#transformers.FlaxBlenderbotForConditionalGeneration"),c(eX,"href","/docs/transformers/pr_15792/en/model_doc/blenderbot-small#transformers.FlaxBlenderbotSmallForConditionalGeneration"),c(oX,"href","/docs/transformers/pr_15792/en/model_doc/encoder-decoder#transformers.FlaxEncoderDecoderModel"),c(rX,"href","/docs/transformers/pr_15792/en/model_doc/marian#transformers.FlaxMarianMTModel"),c(tX,"href","/docs/transformers/pr_15792/en/model_doc/mbart#transformers.FlaxMBartForConditionalGeneration"),c(aX,"href","/docs/transformers/pr_15792/en/model_doc/mt5#transformers.FlaxMT5ForConditionalGeneration"),c(nX,"href","/docs/transformers/pr_15792/en/model_doc/pegasus#transformers.FlaxPegasusForConditionalGeneration"),c(sX,"href","/docs/transformers/pr_15792/en/model_doc/t5#transformers.FlaxT5ForConditionalGeneration"),c(ko,"class","docstring"),c(kr,"class","docstring"),c(pC,"id","transformers.FlaxAutoModelForSequenceClassification"),c(pC,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(pC,"href","#transformers.FlaxAutoModelForSequenceClassification"),c(of,"class","relative group"),c(Et,"class","docstring"),c(lX,"href","/docs/transformers/pr_15792/en/model_doc/albert#transformers.FlaxAlbertForSequenceClassification"),c(iX,"href","/docs/transformers/pr_15792/en/model_doc/bart#transformers.FlaxBartForSequenceClassification"),c(dX,"href","/docs/transformers/pr_15792/en/model_doc/bert#transformers.FlaxBertForSequenceClassification"),c(cX,"href","/docs/transformers/pr_15792/en/model_doc/big_bird#transformers.FlaxBigBirdForSequenceClassification"),c(fX,"href","/docs/transformers/pr_15792/en/model_doc/distilbert#transformers.FlaxDistilBertForSequenceClassification"),c(mX,"href","/docs/transformers/pr_15792/en/model_doc/electra#transformers.FlaxElectraForSequenceClassification"),c(gX,"href","/docs/transformers/pr_15792/en/model_doc/mbart#transformers.FlaxMBartForSequenceClassification"),c(hX,"href","/docs/transformers/pr_15792/en/model_doc/roberta#transformers.FlaxRobertaForSequenceClassification"),c(pX,"href","/docs/transformers/pr_15792/en/model_doc/roformer#transformers.FlaxRoFormerForSequenceClassification"),c(xo,"class","docstring"),c(xr,"class","docstring"),c(yC,"id","transformers.FlaxAutoModelForQuestionAnswering"),c(yC,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(yC,"href","#transformers.FlaxAutoModelForQuestionAnswering"),c(af,"class","relative group"),c(yt,"class","docstring"),c(_X,"href","/docs/transformers/pr_15792/en/model_doc/albert#transformers.FlaxAlbertForQuestionAnswering"),c(uX,"href","/docs/transformers/pr_15792/en/model_doc/bart#transformers.FlaxBartForQuestionAnswering"),c(bX,"href","/docs/transformers/pr_15792/en/model_doc/bert#transformers.FlaxBertForQuestionAnswering"),c(vX,"href","/docs/transformers/pr_15792/en/model_doc/big_bird#transformers.FlaxBigBirdForQuestionAnswering"),c(TX,"href","/docs/transformers/pr_15792/en/model_doc/distilbert#transformers.FlaxDistilBertForQuestionAnswering"),c(FX,"href","/docs/transformers/pr_15792/en/model_doc/electra#transformers.FlaxElectraForQuestionAnswering"),c(CX,"href","/docs/transformers/pr_15792/en/model_doc/mbart#transformers.FlaxMBartForQuestionAnswering"),c(MX,"href","/docs/transformers/pr_15792/en/model_doc/roberta#transformers.FlaxRobertaForQuestionAnswering"),c(EX,"href","/docs/transformers/pr_15792/en/model_doc/roformer#transformers.FlaxRoFormerForQuestionAnswering"),c(Ro,"class","docstring"),c(Rr,"class","docstring"),c($C,"id","transformers.FlaxAutoModelForTokenClassification"),c($C,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c($C,"href","#transformers.FlaxAutoModelForTokenClassification"),c(lf,"class","relative group"),c(wt,"class","docstring"),c(yX,"href","/docs/transformers/pr_15792/en/model_doc/albert#transformers.FlaxAlbertForTokenClassification"),c(wX,"href","/docs/transformers/pr_15792/en/model_doc/bert#transformers.FlaxBertForTokenClassification"),c(AX,"href","/docs/transformers/pr_15792/en/model_doc/big_bird#transformers.FlaxBigBirdForTokenClassification"),c(LX,"href","/docs/transformers/pr_15792/en/model_doc/distilbert#transformers.FlaxDistilBertForTokenClassification"),c(BX,"href","/docs/transformers/pr_15792/en/model_doc/electra#transformers.FlaxElectraForTokenClassification"),c(kX,"href","/docs/transformers/pr_15792/en/model_doc/roberta#transformers.FlaxRobertaForTokenClassification"),c(xX,"href","/docs/transformers/pr_15792/en/model_doc/roformer#transformers.FlaxRoFormerForTokenClassification"),c(So,"class","docstring"),c(Sr,"class","docstring"),c(XC,"id","transformers.FlaxAutoModelForMultipleChoice"),c(XC,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(XC,"href","#transformers.FlaxAutoModelForMultipleChoice"),c(ff,"class","relative group"),c(At,"class","docstring"),c(RX,"href","/docs/transformers/pr_15792/en/model_doc/albert#transformers.FlaxAlbertForMultipleChoice"),c(SX,"href","/docs/transformers/pr_15792/en/model_doc/bert#transformers.FlaxBertForMultipleChoice"),c(PX,"href","/docs/transformers/pr_15792/en/model_doc/big_bird#transformers.FlaxBigBirdForMultipleChoice"),c($X,"href","/docs/transformers/pr_15792/en/model_doc/distilbert#transformers.FlaxDistilBertForMultipleChoice"),c(IX,"href","/docs/transformers/pr_15792/en/model_doc/electra#transformers.FlaxElectraForMultipleChoice"),c(jX,"href","/docs/transformers/pr_15792/en/model_doc/roberta#transformers.FlaxRobertaForMultipleChoice"),c(NX,"href","/docs/transformers/pr_15792/en/model_doc/roformer#transformers.FlaxRoFormerForMultipleChoice"),c(Po,"class","docstring"),c(Pr,"class","docstring"),c(YC,"id","transformers.FlaxAutoModelForNextSentencePrediction"),c(YC,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(YC,"href","#transformers.FlaxAutoModelForNextSentencePrediction"),c(hf,"class","relative group"),c(Lt,"class","docstring"),c(DX,"href","/docs/transformers/pr_15792/en/model_doc/bert#transformers.FlaxBertForNextSentencePrediction"),c($o,"class","docstring"),c($r,"class","docstring"),c(ZC,"id","transformers.FlaxAutoModelForImageClassification"),c(ZC,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(ZC,"href","#transformers.FlaxAutoModelForImageClassification"),c(uf,"class","relative group"),c(Bt,"class","docstring"),c(qX,"href","/docs/transformers/pr_15792/en/model_doc/beit#transformers.FlaxBeitForImageClassification"),c(GX,"href","/docs/transformers/pr_15792/en/model_doc/vit#transformers.FlaxViTForImageClassification"),c(Io,"class","docstring"),c(Ir,"class","docstring"),c(r4,"id","transformers.FlaxAutoModelForVision2Seq"),c(r4,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(r4,"href","#transformers.FlaxAutoModelForVision2Seq"),c(Tf,"class","relative group"),c(kt,"class","docstring"),c(OX,"href","/docs/transformers/pr_15792/en/model_doc/vision-encoder-decoder#transformers.FlaxVisionEncoderDecoderModel"),c(jo,"class","docstring"),c(jr,"class","docstring")},m(d,u){e(document.head,J),b(d,Ae,u),b(d,ie,u),e(ie,me),e(me,to),g(ce,to,null),e(ie,ue),e(ie,Do),e(Do,wi),b(d,Ef,u),b(d,sa,u),e(sa,Ai),e(sa,Li),e(Li,oM),e(sa,yf),b(d,ye,u),b(d,io,u),e(io,Bi),e(io,Pn),e(Pn,rM),e(io,$n),e(io,In),e(In,tM),e(io,ki),e(io,jn),e(jn,aM),e(io,xi),b(d,wf,u),g($a,d,u),b(d,co,u),b(d,ge,u),e(ge,D0),e(ge,Ri),e(Ri,q0),e(ge,G0),b(d,qo,u),b(d,Ia,u),e(Ia,O0),e(Ia,Af),e(Af,X0),e(Ia,mxe),b(d,tLe,u),b(d,Si,u),e(Si,Lf),e(Lf,$V),g(nM,$V,null),e(Si,gxe),e(Si,IV),e(IV,hxe),b(d,aLe,u),b(d,Nn,u),e(Nn,pxe),e(Nn,jV),e(jV,_xe),e(Nn,uxe),e(Nn,NV),e(NV,bxe),e(Nn,vxe),b(d,nLe,u),g(sM,d,u),b(d,sLe,u),b(d,z0,u),e(z0,Txe),b(d,lLe,u),g(Bf,d,u),b(d,iLe,u),b(d,Pi,u),e(Pi,kf),e(kf,DV),g(lM,DV,null),e(Pi,Fxe),e(Pi,qV),e(qV,Cxe),b(d,dLe,u),b(d,Go,u),g(iM,Go,null),e(Go,Mxe),e(Go,dM),e(dM,Exe),e(dM,V0),e(V0,yxe),e(dM,wxe),e(Go,Axe),e(Go,cM),e(cM,Lxe),e(cM,GV),e(GV,Bxe),e(cM,kxe),e(Go,xxe),e(Go,fo),g(fM,fo,null),e(fo,Rxe),e(fo,OV),e(OV,Sxe),e(fo,Pxe),e(fo,$i),e($i,$xe),e($i,XV),e(XV,Ixe),e($i,jxe),e($i,zV),e(zV,Nxe),e($i,Dxe),e(fo,qxe),e(fo,v),e(v,xf),e(xf,VV),e(VV,Gxe),e(xf,Oxe),e(xf,W0),e(W0,Xxe),e(xf,zxe),e(v,Vxe),e(v,Rf),e(Rf,WV),e(WV,Wxe),e(Rf,Qxe),e(Rf,Q0),e(Q0,Hxe),e(Rf,Uxe),e(v,Jxe),e(v,Sf),e(Sf,QV),e(QV,Yxe),e(Sf,Kxe),e(Sf,H0),e(H0,Zxe),e(Sf,eRe),e(v,oRe),e(v,Pf),e(Pf,HV),e(HV,rRe),e(Pf,tRe),e(Pf,U0),e(U0,aRe),e(Pf,nRe),e(v,sRe),e(v,$f),e($f,UV),e(UV,lRe),e($f,iRe),e($f,J0),e(J0,dRe),e($f,cRe),e(v,fRe),e(v,If),e(If,JV),e(JV,mRe),e(If,gRe),e(If,Y0),e(Y0,hRe),e(If,pRe),e(v,_Re),e(v,jf),e(jf,YV),e(YV,uRe),e(jf,bRe),e(jf,K0),e(K0,vRe),e(jf,TRe),e(v,FRe),e(v,Nf),e(Nf,KV),e(KV,CRe),e(Nf,MRe),e(Nf,Z0),e(Z0,ERe),e(Nf,yRe),e(v,wRe),e(v,Df),e(Df,ZV),e(ZV,ARe),e(Df,LRe),e(Df,eL),e(eL,BRe),e(Df,kRe),e(v,xRe),e(v,qf),e(qf,eW),e(eW,RRe),e(qf,SRe),e(qf,oL),e(oL,PRe),e(qf,$Re),e(v,IRe),e(v,Gf),e(Gf,oW),e(oW,jRe),e(Gf,NRe),e(Gf,rL),e(rL,DRe),e(Gf,qRe),e(v,GRe),e(v,Of),e(Of,rW),e(rW,ORe),e(Of,XRe),e(Of,tL),e(tL,zRe),e(Of,VRe),e(v,WRe),e(v,Xf),e(Xf,tW),e(tW,QRe),e(Xf,HRe),e(Xf,aL),e(aL,URe),e(Xf,JRe),e(v,YRe),e(v,zf),e(zf,aW),e(aW,KRe),e(zf,ZRe),e(zf,nL),e(nL,eSe),e(zf,oSe),e(v,rSe),e(v,Vf),e(Vf,nW),e(nW,tSe),e(Vf,aSe),e(Vf,sL),e(sL,nSe),e(Vf,sSe),e(v,lSe),e(v,Wf),e(Wf,sW),e(sW,iSe),e(Wf,dSe),e(Wf,lL),e(lL,cSe),e(Wf,fSe),e(v,mSe),e(v,Qf),e(Qf,lW),e(lW,gSe),e(Qf,hSe),e(Qf,iL),e(iL,pSe),e(Qf,_Se),e(v,uSe),e(v,Hf),e(Hf,iW),e(iW,bSe),e(Hf,vSe),e(Hf,dL),e(dL,TSe),e(Hf,FSe),e(v,CSe),e(v,Uf),e(Uf,dW),e(dW,MSe),e(Uf,ESe),e(Uf,cL),e(cL,ySe),e(Uf,wSe),e(v,ASe),e(v,Jf),e(Jf,cW),e(cW,LSe),e(Jf,BSe),e(Jf,fL),e(fL,kSe),e(Jf,xSe),e(v,RSe),e(v,Yf),e(Yf,fW),e(fW,SSe),e(Yf,PSe),e(Yf,mL),e(mL,$Se),e(Yf,ISe),e(v,jSe),e(v,Kf),e(Kf,mW),e(mW,NSe),e(Kf,DSe),e(Kf,gL),e(gL,qSe),e(Kf,GSe),e(v,OSe),e(v,Zf),e(Zf,gW),e(gW,XSe),e(Zf,zSe),e(Zf,hL),e(hL,VSe),e(Zf,WSe),e(v,QSe),e(v,em),e(em,hW),e(hW,HSe),e(em,USe),e(em,pL),e(pL,JSe),e(em,YSe),e(v,KSe),e(v,om),e(om,pW),e(pW,ZSe),e(om,ePe),e(om,_L),e(_L,oPe),e(om,rPe),e(v,tPe),e(v,rm),e(rm,_W),e(_W,aPe),e(rm,nPe),e(rm,uL),e(uL,sPe),e(rm,lPe),e(v,iPe),e(v,tm),e(tm,uW),e(uW,dPe),e(tm,cPe),e(tm,bL),e(bL,fPe),e(tm,mPe),e(v,gPe),e(v,am),e(am,bW),e(bW,hPe),e(am,pPe),e(am,vL),e(vL,_Pe),e(am,uPe),e(v,bPe),e(v,nm),e(nm,vW),e(vW,vPe),e(nm,TPe),e(nm,TL),e(TL,FPe),e(nm,CPe),e(v,MPe),e(v,sm),e(sm,TW),e(TW,EPe),e(sm,yPe),e(sm,FL),e(FL,wPe),e(sm,APe),e(v,LPe),e(v,lm),e(lm,FW),e(FW,BPe),e(lm,kPe),e(lm,CL),e(CL,xPe),e(lm,RPe),e(v,SPe),e(v,im),e(im,CW),e(CW,PPe),e(im,$Pe),e(im,ML),e(ML,IPe),e(im,jPe),e(v,NPe),e(v,dm),e(dm,MW),e(MW,DPe),e(dm,qPe),e(dm,EL),e(EL,GPe),e(dm,OPe),e(v,XPe),e(v,cm),e(cm,EW),e(EW,zPe),e(cm,VPe),e(cm,yL),e(yL,WPe),e(cm,QPe),e(v,HPe),e(v,fm),e(fm,yW),e(yW,UPe),e(fm,JPe),e(fm,wL),e(wL,YPe),e(fm,KPe),e(v,ZPe),e(v,mm),e(mm,wW),e(wW,e$e),e(mm,o$e),e(mm,AL),e(AL,r$e),e(mm,t$e),e(v,a$e),e(v,gm),e(gm,AW),e(AW,n$e),e(gm,s$e),e(gm,LL),e(LL,l$e),e(gm,i$e),e(v,d$e),e(v,hm),e(hm,LW),e(LW,c$e),e(hm,f$e),e(hm,BL),e(BL,m$e),e(hm,g$e),e(v,h$e),e(v,pm),e(pm,BW),e(BW,p$e),e(pm,_$e),e(pm,kL),e(kL,u$e),e(pm,b$e),e(v,v$e),e(v,_m),e(_m,kW),e(kW,T$e),e(_m,F$e),e(_m,xL),e(xL,C$e),e(_m,M$e),e(v,E$e),e(v,um),e(um,xW),e(xW,y$e),e(um,w$e),e(um,RL),e(RL,A$e),e(um,L$e),e(v,B$e),e(v,bm),e(bm,RW),e(RW,k$e),e(bm,x$e),e(bm,SL),e(SL,R$e),e(bm,S$e),e(v,P$e),e(v,vm),e(vm,SW),e(SW,$$e),e(vm,I$e),e(vm,PL),e(PL,j$e),e(vm,N$e),e(v,D$e),e(v,Tm),e(Tm,PW),e(PW,q$e),e(Tm,G$e),e(Tm,$L),e($L,O$e),e(Tm,X$e),e(v,z$e),e(v,Fm),e(Fm,$W),e($W,V$e),e(Fm,W$e),e(Fm,IL),e(IL,Q$e),e(Fm,H$e),e(v,U$e),e(v,Cm),e(Cm,IW),e(IW,J$e),e(Cm,Y$e),e(Cm,jL),e(jL,K$e),e(Cm,Z$e),e(v,eIe),e(v,Mm),e(Mm,jW),e(jW,oIe),e(Mm,rIe),e(Mm,NL),e(NL,tIe),e(Mm,aIe),e(v,nIe),e(v,Em),e(Em,NW),e(NW,sIe),e(Em,lIe),e(Em,DL),e(DL,iIe),e(Em,dIe),e(v,cIe),e(v,ym),e(ym,DW),e(DW,fIe),e(ym,mIe),e(ym,qL),e(qL,gIe),e(ym,hIe),e(v,pIe),e(v,wm),e(wm,qW),e(qW,_Ie),e(wm,uIe),e(wm,GL),e(GL,bIe),e(wm,vIe),e(v,TIe),e(v,Am),e(Am,GW),e(GW,FIe),e(Am,CIe),e(Am,OL),e(OL,MIe),e(Am,EIe),e(v,yIe),e(v,Lm),e(Lm,OW),e(OW,wIe),e(Lm,AIe),e(Lm,XL),e(XL,LIe),e(Lm,BIe),e(v,kIe),e(v,Bm),e(Bm,XW),e(XW,xIe),e(Bm,RIe),e(Bm,zL),e(zL,SIe),e(Bm,PIe),e(v,$Ie),e(v,km),e(km,zW),e(zW,IIe),e(km,jIe),e(km,VL),e(VL,NIe),e(km,DIe),e(v,qIe),e(v,xm),e(xm,VW),e(VW,GIe),e(xm,OIe),e(xm,WL),e(WL,XIe),e(xm,zIe),e(v,VIe),e(v,Rm),e(Rm,WW),e(WW,WIe),e(Rm,QIe),e(Rm,QL),e(QL,HIe),e(Rm,UIe),e(v,JIe),e(v,Sm),e(Sm,QW),e(QW,YIe),e(Sm,KIe),e(Sm,HL),e(HL,ZIe),e(Sm,eje),e(v,oje),e(v,Pm),e(Pm,HW),e(HW,rje),e(Pm,tje),e(Pm,UL),e(UL,aje),e(Pm,nje),e(v,sje),e(v,$m),e($m,UW),e(UW,lje),e($m,ije),e($m,JL),e(JL,dje),e($m,cje),e(v,fje),e(v,Im),e(Im,JW),e(JW,mje),e(Im,gje),e(Im,YL),e(YL,hje),e(Im,pje),e(v,_je),e(v,jm),e(jm,YW),e(YW,uje),e(jm,bje),e(jm,KL),e(KL,vje),e(jm,Tje),e(v,Fje),e(v,Nm),e(Nm,KW),e(KW,Cje),e(Nm,Mje),e(Nm,ZL),e(ZL,Eje),e(Nm,yje),e(v,wje),e(v,Dm),e(Dm,ZW),e(ZW,Aje),e(Dm,Lje),e(Dm,e8),e(e8,Bje),e(Dm,kje),e(v,xje),e(v,qm),e(qm,eQ),e(eQ,Rje),e(qm,Sje),e(qm,o8),e(o8,Pje),e(qm,$je),e(v,Ije),e(v,Gm),e(Gm,oQ),e(oQ,jje),e(Gm,Nje),e(Gm,r8),e(r8,Dje),e(Gm,qje),e(v,Gje),e(v,Om),e(Om,rQ),e(rQ,Oje),e(Om,Xje),e(Om,t8),e(t8,zje),e(Om,Vje),e(v,Wje),e(v,Xm),e(Xm,tQ),e(tQ,Qje),e(Xm,Hje),e(Xm,a8),e(a8,Uje),e(Xm,Jje),e(v,Yje),e(v,zm),e(zm,aQ),e(aQ,Kje),e(zm,Zje),e(zm,n8),e(n8,eNe),e(zm,oNe),e(v,rNe),e(v,Vm),e(Vm,nQ),e(nQ,tNe),e(Vm,aNe),e(Vm,s8),e(s8,nNe),e(Vm,sNe),e(v,lNe),e(v,Wm),e(Wm,sQ),e(sQ,iNe),e(Wm,dNe),e(Wm,l8),e(l8,cNe),e(Wm,fNe),e(v,mNe),e(v,Qm),e(Qm,lQ),e(lQ,gNe),e(Qm,hNe),e(Qm,i8),e(i8,pNe),e(Qm,_Ne),e(v,uNe),e(v,Hm),e(Hm,iQ),e(iQ,bNe),e(Hm,vNe),e(Hm,d8),e(d8,TNe),e(Hm,FNe),e(v,CNe),e(v,Um),e(Um,dQ),e(dQ,MNe),e(Um,ENe),e(Um,c8),e(c8,yNe),e(Um,wNe),e(v,ANe),e(v,Jm),e(Jm,cQ),e(cQ,LNe),e(Jm,BNe),e(Jm,f8),e(f8,kNe),e(Jm,xNe),e(v,RNe),e(v,Ym),e(Ym,fQ),e(fQ,SNe),e(Ym,PNe),e(Ym,m8),e(m8,$Ne),e(Ym,INe),e(v,jNe),e(v,Km),e(Km,mQ),e(mQ,NNe),e(Km,DNe),e(Km,g8),e(g8,qNe),e(Km,GNe),e(v,ONe),e(v,Zm),e(Zm,gQ),e(gQ,XNe),e(Zm,zNe),e(Zm,h8),e(h8,VNe),e(Zm,WNe),e(v,QNe),e(v,eg),e(eg,hQ),e(hQ,HNe),e(eg,UNe),e(eg,p8),e(p8,JNe),e(eg,YNe),e(v,KNe),e(v,og),e(og,pQ),e(pQ,ZNe),e(og,eDe),e(og,_8),e(_8,oDe),e(og,rDe),e(v,tDe),e(v,rg),e(rg,_Q),e(_Q,aDe),e(rg,nDe),e(rg,u8),e(u8,sDe),e(rg,lDe),e(v,iDe),e(v,tg),e(tg,uQ),e(uQ,dDe),e(tg,cDe),e(tg,b8),e(b8,fDe),e(tg,mDe),e(v,gDe),e(v,ag),e(ag,bQ),e(bQ,hDe),e(ag,pDe),e(ag,v8),e(v8,_De),e(ag,uDe),e(v,bDe),e(v,ng),e(ng,vQ),e(vQ,vDe),e(ng,TDe),e(ng,T8),e(T8,FDe),e(ng,CDe),e(v,MDe),e(v,sg),e(sg,TQ),e(TQ,EDe),e(sg,yDe),e(sg,F8),e(F8,wDe),e(sg,ADe),e(v,LDe),e(v,lg),e(lg,FQ),e(FQ,BDe),e(lg,kDe),e(lg,C8),e(C8,xDe),e(lg,RDe),e(v,SDe),e(v,ig),e(ig,CQ),e(CQ,PDe),e(ig,$De),e(ig,M8),e(M8,IDe),e(ig,jDe),e(v,NDe),e(v,dg),e(dg,MQ),e(MQ,DDe),e(dg,qDe),e(dg,E8),e(E8,GDe),e(dg,ODe),e(v,XDe),e(v,cg),e(cg,EQ),e(EQ,zDe),e(cg,VDe),e(cg,y8),e(y8,WDe),e(cg,QDe),e(v,HDe),e(v,fg),e(fg,yQ),e(yQ,UDe),e(fg,JDe),e(fg,w8),e(w8,YDe),e(fg,KDe),e(v,ZDe),e(v,mg),e(mg,wQ),e(wQ,eqe),e(mg,oqe),e(mg,A8),e(A8,rqe),e(mg,tqe),e(v,aqe),e(v,gg),e(gg,AQ),e(AQ,nqe),e(gg,sqe),e(gg,L8),e(L8,lqe),e(gg,iqe),e(fo,dqe),e(fo,LQ),e(LQ,cqe),e(fo,fqe),g(mM,fo,null),e(Go,mqe),e(Go,hg),g(gM,hg,null),e(hg,gqe),e(hg,BQ),e(BQ,hqe),b(d,cLe,u),b(d,Ii,u),e(Ii,pg),e(pg,kQ),g(hM,kQ,null),e(Ii,pqe),e(Ii,xQ),e(xQ,_qe),b(d,fLe,u),b(d,Oo,u),g(pM,Oo,null),e(Oo,uqe),e(Oo,_M),e(_M,bqe),e(_M,B8),e(B8,vqe),e(_M,Tqe),e(Oo,Fqe),e(Oo,uM),e(uM,Cqe),e(uM,RQ),e(RQ,Mqe),e(uM,Eqe),e(Oo,yqe),e(Oo,mo),g(bM,mo,null),e(mo,wqe),e(mo,SQ),e(SQ,Aqe),e(mo,Lqe),e(mo,ja),e(ja,Bqe),e(ja,PQ),e(PQ,kqe),e(ja,xqe),e(ja,$Q),e($Q,Rqe),e(ja,Sqe),e(ja,IQ),e(IQ,Pqe),e(ja,$qe),e(mo,Iqe),e(mo,M),e(M,Dn),e(Dn,jQ),e(jQ,jqe),e(Dn,Nqe),e(Dn,k8),e(k8,Dqe),e(Dn,qqe),e(Dn,x8),e(x8,Gqe),e(Dn,Oqe),e(M,Xqe),e(M,qn),e(qn,NQ),e(NQ,zqe),e(qn,Vqe),e(qn,R8),e(R8,Wqe),e(qn,Qqe),e(qn,S8),e(S8,Hqe),e(qn,Uqe),e(M,Jqe),e(M,Gn),e(Gn,DQ),e(DQ,Yqe),e(Gn,Kqe),e(Gn,P8),e(P8,Zqe),e(Gn,eGe),e(Gn,$8),e($8,oGe),e(Gn,rGe),e(M,tGe),e(M,_g),e(_g,qQ),e(qQ,aGe),e(_g,nGe),e(_g,I8),e(I8,sGe),e(_g,lGe),e(M,iGe),e(M,On),e(On,GQ),e(GQ,dGe),e(On,cGe),e(On,j8),e(j8,fGe),e(On,mGe),e(On,N8),e(N8,gGe),e(On,hGe),e(M,pGe),e(M,ug),e(ug,OQ),e(OQ,_Ge),e(ug,uGe),e(ug,D8),e(D8,bGe),e(ug,vGe),e(M,TGe),e(M,bg),e(bg,XQ),e(XQ,FGe),e(bg,CGe),e(bg,q8),e(q8,MGe),e(bg,EGe),e(M,yGe),e(M,vg),e(vg,zQ),e(zQ,wGe),e(vg,AGe),e(vg,G8),e(G8,LGe),e(vg,BGe),e(M,kGe),e(M,Xn),e(Xn,VQ),e(VQ,xGe),e(Xn,RGe),e(Xn,O8),e(O8,SGe),e(Xn,PGe),e(Xn,X8),e(X8,$Ge),e(Xn,IGe),e(M,jGe),e(M,zn),e(zn,WQ),e(WQ,NGe),e(zn,DGe),e(zn,z8),e(z8,qGe),e(zn,GGe),e(zn,V8),e(V8,OGe),e(zn,XGe),e(M,zGe),e(M,Vn),e(Vn,QQ),e(QQ,VGe),e(Vn,WGe),e(Vn,W8),e(W8,QGe),e(Vn,HGe),e(Vn,Q8),e(Q8,UGe),e(Vn,JGe),e(M,YGe),e(M,Tg),e(Tg,HQ),e(HQ,KGe),e(Tg,ZGe),e(Tg,H8),e(H8,eOe),e(Tg,oOe),e(M,rOe),e(M,Fg),e(Fg,UQ),e(UQ,tOe),e(Fg,aOe),e(Fg,U8),e(U8,nOe),e(Fg,sOe),e(M,lOe),e(M,Wn),e(Wn,JQ),e(JQ,iOe),e(Wn,dOe),e(Wn,J8),e(J8,cOe),e(Wn,fOe),e(Wn,Y8),e(Y8,mOe),e(Wn,gOe),e(M,hOe),e(M,Cg),e(Cg,YQ),e(YQ,pOe),e(Cg,_Oe),e(Cg,K8),e(K8,uOe),e(Cg,bOe),e(M,vOe),e(M,Qn),e(Qn,KQ),e(KQ,TOe),e(Qn,FOe),e(Qn,Z8),e(Z8,COe),e(Qn,MOe),e(Qn,eB),e(eB,EOe),e(Qn,yOe),e(M,wOe),e(M,Hn),e(Hn,ZQ),e(ZQ,AOe),e(Hn,LOe),e(Hn,oB),e(oB,BOe),e(Hn,kOe),e(Hn,rB),e(rB,xOe),e(Hn,ROe),e(M,SOe),e(M,Un),e(Un,eH),e(eH,POe),e(Un,$Oe),e(Un,tB),e(tB,IOe),e(Un,jOe),e(Un,oH),e(oH,NOe),e(Un,DOe),e(M,qOe),e(M,Mg),e(Mg,rH),e(rH,GOe),e(Mg,OOe),e(Mg,aB),e(aB,XOe),e(Mg,zOe),e(M,VOe),e(M,Jn),e(Jn,tH),e(tH,WOe),e(Jn,QOe),e(Jn,nB),e(nB,HOe),e(Jn,UOe),e(Jn,sB),e(sB,JOe),e(Jn,YOe),e(M,KOe),e(M,Eg),e(Eg,aH),e(aH,ZOe),e(Eg,eXe),e(Eg,lB),e(lB,oXe),e(Eg,rXe),e(M,tXe),e(M,Yn),e(Yn,nH),e(nH,aXe),e(Yn,nXe),e(Yn,iB),e(iB,sXe),e(Yn,lXe),e(Yn,dB),e(dB,iXe),e(Yn,dXe),e(M,cXe),e(M,Kn),e(Kn,sH),e(sH,fXe),e(Kn,mXe),e(Kn,cB),e(cB,gXe),e(Kn,hXe),e(Kn,fB),e(fB,pXe),e(Kn,_Xe),e(M,uXe),e(M,Zn),e(Zn,lH),e(lH,bXe),e(Zn,vXe),e(Zn,mB),e(mB,TXe),e(Zn,FXe),e(Zn,gB),e(gB,CXe),e(Zn,MXe),e(M,EXe),e(M,yg),e(yg,iH),e(iH,yXe),e(yg,wXe),e(yg,hB),e(hB,AXe),e(yg,LXe),e(M,BXe),e(M,es),e(es,dH),e(dH,kXe),e(es,xXe),e(es,pB),e(pB,RXe),e(es,SXe),e(es,_B),e(_B,PXe),e(es,$Xe),e(M,IXe),e(M,wg),e(wg,cH),e(cH,jXe),e(wg,NXe),e(wg,uB),e(uB,DXe),e(wg,qXe),e(M,GXe),e(M,os),e(os,fH),e(fH,OXe),e(os,XXe),e(os,bB),e(bB,zXe),e(os,VXe),e(os,vB),e(vB,WXe),e(os,QXe),e(M,HXe),e(M,rs),e(rs,mH),e(mH,UXe),e(rs,JXe),e(rs,TB),e(TB,YXe),e(rs,KXe),e(rs,FB),e(FB,ZXe),e(rs,eze),e(M,oze),e(M,ts),e(ts,gH),e(gH,rze),e(ts,tze),e(ts,CB),e(CB,aze),e(ts,nze),e(ts,MB),e(MB,sze),e(ts,lze),e(M,ize),e(M,as),e(as,hH),e(hH,dze),e(as,cze),e(as,EB),e(EB,fze),e(as,mze),e(as,yB),e(yB,gze),e(as,hze),e(M,pze),e(M,Ag),e(Ag,pH),e(pH,_ze),e(Ag,uze),e(Ag,wB),e(wB,bze),e(Ag,vze),e(M,Tze),e(M,ns),e(ns,_H),e(_H,Fze),e(ns,Cze),e(ns,AB),e(AB,Mze),e(ns,Eze),e(ns,LB),e(LB,yze),e(ns,wze),e(M,Aze),e(M,ss),e(ss,uH),e(uH,Lze),e(ss,Bze),e(ss,BB),e(BB,kze),e(ss,xze),e(ss,kB),e(kB,Rze),e(ss,Sze),e(M,Pze),e(M,ls),e(ls,bH),e(bH,$ze),e(ls,Ize),e(ls,xB),e(xB,jze),e(ls,Nze),e(ls,RB),e(RB,Dze),e(ls,qze),e(M,Gze),e(M,is),e(is,vH),e(vH,Oze),e(is,Xze),e(is,SB),e(SB,zze),e(is,Vze),e(is,PB),e(PB,Wze),e(is,Qze),e(M,Hze),e(M,ds),e(ds,TH),e(TH,Uze),e(ds,Jze),e(ds,$B),e($B,Yze),e(ds,Kze),e(ds,IB),e(IB,Zze),e(ds,eVe),e(M,oVe),e(M,cs),e(cs,FH),e(FH,rVe),e(cs,tVe),e(cs,jB),e(jB,aVe),e(cs,nVe),e(cs,NB),e(NB,sVe),e(cs,lVe),e(M,iVe),e(M,Lg),e(Lg,CH),e(CH,dVe),e(Lg,cVe),e(Lg,DB),e(DB,fVe),e(Lg,mVe),e(M,gVe),e(M,fs),e(fs,MH),e(MH,hVe),e(fs,pVe),e(fs,qB),e(qB,_Ve),e(fs,uVe),e(fs,GB),e(GB,bVe),e(fs,vVe),e(M,TVe),e(M,Bg),e(Bg,EH),e(EH,FVe),e(Bg,CVe),e(Bg,OB),e(OB,MVe),e(Bg,EVe),e(M,yVe),e(M,kg),e(kg,yH),e(yH,wVe),e(kg,AVe),e(kg,XB),e(XB,LVe),e(kg,BVe),e(M,kVe),e(M,ms),e(ms,wH),e(wH,xVe),e(ms,RVe),e(ms,zB),e(zB,SVe),e(ms,PVe),e(ms,VB),e(VB,$Ve),e(ms,IVe),e(M,jVe),e(M,gs),e(gs,AH),e(AH,NVe),e(gs,DVe),e(gs,WB),e(WB,qVe),e(gs,GVe),e(gs,QB),e(QB,OVe),e(gs,XVe),e(M,zVe),e(M,xg),e(xg,LH),e(LH,VVe),e(xg,WVe),e(xg,HB),e(HB,QVe),e(xg,HVe),e(M,UVe),e(M,hs),e(hs,BH),e(BH,JVe),e(hs,YVe),e(hs,UB),e(UB,KVe),e(hs,ZVe),e(hs,JB),e(JB,eWe),e(hs,oWe),e(M,rWe),e(M,ps),e(ps,kH),e(kH,tWe),e(ps,aWe),e(ps,YB),e(YB,nWe),e(ps,sWe),e(ps,KB),e(KB,lWe),e(ps,iWe),e(M,dWe),e(M,_s),e(_s,xH),e(xH,cWe),e(_s,fWe),e(_s,ZB),e(ZB,mWe),e(_s,gWe),e(_s,ek),e(ek,hWe),e(_s,pWe),e(M,_We),e(M,us),e(us,RH),e(RH,uWe),e(us,bWe),e(us,ok),e(ok,vWe),e(us,TWe),e(us,rk),e(rk,FWe),e(us,CWe),e(M,MWe),e(M,bs),e(bs,SH),e(SH,EWe),e(bs,yWe),e(bs,tk),e(tk,wWe),e(bs,AWe),e(bs,ak),e(ak,LWe),e(bs,BWe),e(M,kWe),e(M,Rg),e(Rg,PH),e(PH,xWe),e(Rg,RWe),e(Rg,nk),e(nk,SWe),e(Rg,PWe),e(M,$We),e(M,Sg),e(Sg,$H),e($H,IWe),e(Sg,jWe),e(Sg,sk),e(sk,NWe),e(Sg,DWe),e(M,qWe),e(M,Pg),e(Pg,IH),e(IH,GWe),e(Pg,OWe),e(Pg,lk),e(lk,XWe),e(Pg,zWe),e(M,VWe),e(M,$g),e($g,jH),e(jH,WWe),e($g,QWe),e($g,ik),e(ik,HWe),e($g,UWe),e(M,JWe),e(M,vs),e(vs,NH),e(NH,YWe),e(vs,KWe),e(vs,dk),e(dk,ZWe),e(vs,eQe),e(vs,ck),e(ck,oQe),e(vs,rQe),e(M,tQe),e(M,Ig),e(Ig,DH),e(DH,aQe),e(Ig,nQe),e(Ig,fk),e(fk,sQe),e(Ig,lQe),e(M,iQe),e(M,Ts),e(Ts,qH),e(qH,dQe),e(Ts,cQe),e(Ts,mk),e(mk,fQe),e(Ts,mQe),e(Ts,gk),e(gk,gQe),e(Ts,hQe),e(M,pQe),e(M,Fs),e(Fs,GH),e(GH,_Qe),e(Fs,uQe),e(Fs,hk),e(hk,bQe),e(Fs,vQe),e(Fs,pk),e(pk,TQe),e(Fs,FQe),e(M,CQe),e(M,Cs),e(Cs,OH),e(OH,MQe),e(Cs,EQe),e(Cs,_k),e(_k,yQe),e(Cs,wQe),e(Cs,uk),e(uk,AQe),e(Cs,LQe),e(M,BQe),e(M,Ms),e(Ms,XH),e(XH,kQe),e(Ms,xQe),e(Ms,bk),e(bk,RQe),e(Ms,SQe),e(Ms,vk),e(vk,PQe),e(Ms,$Qe),e(M,IQe),e(M,Es),e(Es,zH),e(zH,jQe),e(Es,NQe),e(Es,Tk),e(Tk,DQe),e(Es,qQe),e(Es,Fk),e(Fk,GQe),e(Es,OQe),e(M,XQe),e(M,jg),e(jg,VH),e(VH,zQe),e(jg,VQe),e(jg,Ck),e(Ck,WQe),e(jg,QQe),e(M,HQe),e(M,Ng),e(Ng,WH),e(WH,UQe),e(Ng,JQe),e(Ng,Mk),e(Mk,YQe),e(Ng,KQe),e(M,ZQe),e(M,ys),e(ys,QH),e(QH,eHe),e(ys,oHe),e(ys,Ek),e(Ek,rHe),e(ys,tHe),e(ys,yk),e(yk,aHe),e(ys,nHe),e(M,sHe),e(M,ws),e(ws,HH),e(HH,lHe),e(ws,iHe),e(ws,wk),e(wk,dHe),e(ws,cHe),e(ws,Ak),e(Ak,fHe),e(ws,mHe),e(M,gHe),e(M,As),e(As,UH),e(UH,hHe),e(As,pHe),e(As,Lk),e(Lk,_He),e(As,uHe),e(As,Bk),e(Bk,bHe),e(As,vHe),e(M,THe),e(M,Dg),e(Dg,JH),e(JH,FHe),e(Dg,CHe),e(Dg,kk),e(kk,MHe),e(Dg,EHe),e(M,yHe),e(M,qg),e(qg,YH),e(YH,wHe),e(qg,AHe),e(qg,xk),e(xk,LHe),e(qg,BHe),e(M,kHe),e(M,Gg),e(Gg,KH),e(KH,xHe),e(Gg,RHe),e(Gg,Rk),e(Rk,SHe),e(Gg,PHe),e(M,$He),e(M,Og),e(Og,ZH),e(ZH,IHe),e(Og,jHe),e(Og,Sk),e(Sk,NHe),e(Og,DHe),e(M,qHe),e(M,Ls),e(Ls,eU),e(eU,GHe),e(Ls,OHe),e(Ls,Pk),e(Pk,XHe),e(Ls,zHe),e(Ls,$k),e($k,VHe),e(Ls,WHe),e(M,QHe),e(M,Xg),e(Xg,oU),e(oU,HHe),e(Xg,UHe),e(Xg,Ik),e(Ik,JHe),e(Xg,YHe),e(M,KHe),e(M,zg),e(zg,rU),e(rU,ZHe),e(zg,eUe),e(zg,jk),e(jk,oUe),e(zg,rUe),e(M,tUe),e(M,Bs),e(Bs,tU),e(tU,aUe),e(Bs,nUe),e(Bs,Nk),e(Nk,sUe),e(Bs,lUe),e(Bs,Dk),e(Dk,iUe),e(Bs,dUe),e(M,cUe),e(M,ks),e(ks,aU),e(aU,fUe),e(ks,mUe),e(ks,qk),e(qk,gUe),e(ks,hUe),e(ks,Gk),e(Gk,pUe),e(ks,_Ue),e(mo,uUe),e(mo,nU),e(nU,bUe),e(mo,vUe),g(vM,mo,null),e(Oo,TUe),e(Oo,Vg),g(TM,Vg,null),e(Vg,FUe),e(Vg,sU),e(sU,CUe),b(d,mLe,u),b(d,ji,u),e(ji,Wg),e(Wg,lU),g(FM,lU,null),e(ji,MUe),e(ji,iU),e(iU,EUe),b(d,gLe,u),b(d,Xo,u),g(CM,Xo,null),e(Xo,yUe),e(Xo,MM),e(MM,wUe),e(MM,Ok),e(Ok,AUe),e(MM,LUe),e(Xo,BUe),e(Xo,EM),e(EM,kUe),e(EM,dU),e(dU,xUe),e(EM,RUe),e(Xo,SUe),e(Xo,Le),g(yM,Le,null),e(Le,PUe),e(Le,cU),e(cU,$Ue),e(Le,IUe),e(Le,Na),e(Na,jUe),e(Na,fU),e(fU,NUe),e(Na,DUe),e(Na,mU),e(mU,qUe),e(Na,GUe),e(Na,gU),e(gU,OUe),e(Na,XUe),e(Le,zUe),e(Le,se),e(se,Qg),e(Qg,hU),e(hU,VUe),e(Qg,WUe),e(Qg,Xk),e(Xk,QUe),e(Qg,HUe),e(se,UUe),e(se,Hg),e(Hg,pU),e(pU,JUe),e(Hg,YUe),e(Hg,zk),e(zk,KUe),e(Hg,ZUe),e(se,eJe),e(se,Ug),e(Ug,_U),e(_U,oJe),e(Ug,rJe),e(Ug,Vk),e(Vk,tJe),e(Ug,aJe),e(se,nJe),e(se,Jg),e(Jg,uU),e(uU,sJe),e(Jg,lJe),e(Jg,Wk),e(Wk,iJe),e(Jg,dJe),e(se,cJe),e(se,Yg),e(Yg,bU),e(bU,fJe),e(Yg,mJe),e(Yg,Qk),e(Qk,gJe),e(Yg,hJe),e(se,pJe),e(se,Kg),e(Kg,vU),e(vU,_Je),e(Kg,uJe),e(Kg,Hk),e(Hk,bJe),e(Kg,vJe),e(se,TJe),e(se,Zg),e(Zg,TU),e(TU,FJe),e(Zg,CJe),e(Zg,Uk),e(Uk,MJe),e(Zg,EJe),e(se,yJe),e(se,eh),e(eh,FU),e(FU,wJe),e(eh,AJe),e(eh,Jk),e(Jk,LJe),e(eh,BJe),e(se,kJe),e(se,oh),e(oh,CU),e(CU,xJe),e(oh,RJe),e(oh,Yk),e(Yk,SJe),e(oh,PJe),e(se,$Je),e(se,rh),e(rh,MU),e(MU,IJe),e(rh,jJe),e(rh,Kk),e(Kk,NJe),e(rh,DJe),e(se,qJe),e(se,th),e(th,EU),e(EU,GJe),e(th,OJe),e(th,Zk),e(Zk,XJe),e(th,zJe),e(se,VJe),e(se,ah),e(ah,yU),e(yU,WJe),e(ah,QJe),e(ah,ex),e(ex,HJe),e(ah,UJe),e(se,JJe),e(se,nh),e(nh,wU),e(wU,YJe),e(nh,KJe),e(nh,ox),e(ox,ZJe),e(nh,eYe),e(se,oYe),e(se,sh),e(sh,AU),e(AU,rYe),e(sh,tYe),e(sh,rx),e(rx,aYe),e(sh,nYe),e(se,sYe),e(se,lh),e(lh,LU),e(LU,lYe),e(lh,iYe),e(lh,tx),e(tx,dYe),e(lh,cYe),e(Le,fYe),g(ih,Le,null),e(Le,mYe),e(Le,BU),e(BU,gYe),e(Le,hYe),g(wM,Le,null),e(Xo,pYe),e(Xo,dh),g(AM,dh,null),e(dh,_Ye),e(dh,kU),e(kU,uYe),b(d,hLe,u),b(d,Ni,u),e(Ni,ch),e(ch,xU),g(LM,xU,null),e(Ni,bYe),e(Ni,RU),e(RU,vYe),b(d,pLe,u),b(d,zo,u),g(BM,zo,null),e(zo,TYe),e(zo,kM),e(kM,FYe),e(kM,ax),e(ax,CYe),e(kM,MYe),e(zo,EYe),e(zo,xM),e(xM,yYe),e(xM,SU),e(SU,wYe),e(xM,AYe),e(zo,LYe),e(zo,Be),g(RM,Be,null),e(Be,BYe),e(Be,PU),e(PU,kYe),e(Be,xYe),e(Be,Di),e(Di,RYe),e(Di,$U),e($U,SYe),e(Di,PYe),e(Di,IU),e(IU,$Ye),e(Di,IYe),e(Be,jYe),e(Be,we),e(we,fh),e(fh,jU),e(jU,NYe),e(fh,DYe),e(fh,nx),e(nx,qYe),e(fh,GYe),e(we,OYe),e(we,mh),e(mh,NU),e(NU,XYe),e(mh,zYe),e(mh,sx),e(sx,VYe),e(mh,WYe),e(we,QYe),e(we,gh),e(gh,DU),e(DU,HYe),e(gh,UYe),e(gh,lx),e(lx,JYe),e(gh,YYe),e(we,KYe),e(we,hh),e(hh,qU),e(qU,ZYe),e(hh,eKe),e(hh,ix),e(ix,oKe),e(hh,rKe),e(we,tKe),e(we,ph),e(ph,GU),e(GU,aKe),e(ph,nKe),e(ph,dx),e(dx,sKe),e(ph,lKe),e(we,iKe),e(we,_h),e(_h,OU),e(OU,dKe),e(_h,cKe),e(_h,cx),e(cx,fKe),e(_h,mKe),e(we,gKe),e(we,uh),e(uh,XU),e(XU,hKe),e(uh,pKe),e(uh,fx),e(fx,_Ke),e(uh,uKe),e(we,bKe),e(we,bh),e(bh,zU),e(zU,vKe),e(bh,TKe),e(bh,mx),e(mx,FKe),e(bh,CKe),e(Be,MKe),g(vh,Be,null),e(Be,EKe),e(Be,VU),e(VU,yKe),e(Be,wKe),g(SM,Be,null),e(zo,AKe),e(zo,Th),g(PM,Th,null),e(Th,LKe),e(Th,WU),e(WU,BKe),b(d,_Le,u),b(d,qi,u),e(qi,Fh),e(Fh,QU),g($M,QU,null),e(qi,kKe),e(qi,HU),e(HU,xKe),b(d,uLe,u),b(d,Vo,u),g(IM,Vo,null),e(Vo,RKe),e(Vo,Gi),e(Gi,SKe),e(Gi,UU),e(UU,PKe),e(Gi,$Ke),e(Gi,JU),e(JU,IKe),e(Gi,jKe),e(Vo,NKe),e(Vo,jM),e(jM,DKe),e(jM,YU),e(YU,qKe),e(jM,GKe),e(Vo,OKe),e(Vo,Nr),g(NM,Nr,null),e(Nr,XKe),e(Nr,KU),e(KU,zKe),e(Nr,VKe),e(Nr,Oi),e(Oi,WKe),e(Oi,ZU),e(ZU,QKe),e(Oi,HKe),e(Oi,eJ),e(eJ,UKe),e(Oi,JKe),e(Nr,YKe),e(Nr,oJ),e(oJ,KKe),e(Nr,ZKe),g(DM,Nr,null),e(Vo,eZe),e(Vo,ke),g(qM,ke,null),e(ke,oZe),e(ke,rJ),e(rJ,rZe),e(ke,tZe),e(ke,Da),e(Da,aZe),e(Da,tJ),e(tJ,nZe),e(Da,sZe),e(Da,aJ),e(aJ,lZe),e(Da,iZe),e(Da,nJ),e(nJ,dZe),e(Da,cZe),e(ke,fZe),e(ke,F),e(F,Ch),e(Ch,sJ),e(sJ,mZe),e(Ch,gZe),e(Ch,gx),e(gx,hZe),e(Ch,pZe),e(F,_Ze),e(F,Mh),e(Mh,lJ),e(lJ,uZe),e(Mh,bZe),e(Mh,hx),e(hx,vZe),e(Mh,TZe),e(F,FZe),e(F,Eh),e(Eh,iJ),e(iJ,CZe),e(Eh,MZe),e(Eh,px),e(px,EZe),e(Eh,yZe),e(F,wZe),e(F,yh),e(yh,dJ),e(dJ,AZe),e(yh,LZe),e(yh,_x),e(_x,BZe),e(yh,kZe),e(F,xZe),e(F,wh),e(wh,cJ),e(cJ,RZe),e(wh,SZe),e(wh,ux),e(ux,PZe),e(wh,$Ze),e(F,IZe),e(F,Ah),e(Ah,fJ),e(fJ,jZe),e(Ah,NZe),e(Ah,bx),e(bx,DZe),e(Ah,qZe),e(F,GZe),e(F,Lh),e(Lh,mJ),e(mJ,OZe),e(Lh,XZe),e(Lh,vx),e(vx,zZe),e(Lh,VZe),e(F,WZe),e(F,Bh),e(Bh,gJ),e(gJ,QZe),e(Bh,HZe),e(Bh,Tx),e(Tx,UZe),e(Bh,JZe),e(F,YZe),e(F,kh),e(kh,hJ),e(hJ,KZe),e(kh,ZZe),e(kh,Fx),e(Fx,eeo),e(kh,oeo),e(F,reo),e(F,xh),e(xh,pJ),e(pJ,teo),e(xh,aeo),e(xh,Cx),e(Cx,neo),e(xh,seo),e(F,leo),e(F,Rh),e(Rh,_J),e(_J,ieo),e(Rh,deo),e(Rh,Mx),e(Mx,ceo),e(Rh,feo),e(F,meo),e(F,Sh),e(Sh,uJ),e(uJ,geo),e(Sh,heo),e(Sh,Ex),e(Ex,peo),e(Sh,_eo),e(F,ueo),e(F,Ph),e(Ph,bJ),e(bJ,beo),e(Ph,veo),e(Ph,yx),e(yx,Teo),e(Ph,Feo),e(F,Ceo),e(F,$h),e($h,vJ),e(vJ,Meo),e($h,Eeo),e($h,wx),e(wx,yeo),e($h,weo),e(F,Aeo),e(F,Ih),e(Ih,TJ),e(TJ,Leo),e(Ih,Beo),e(Ih,Ax),e(Ax,keo),e(Ih,xeo),e(F,Reo),e(F,jh),e(jh,FJ),e(FJ,Seo),e(jh,Peo),e(jh,Lx),e(Lx,$eo),e(jh,Ieo),e(F,jeo),e(F,Nh),e(Nh,CJ),e(CJ,Neo),e(Nh,Deo),e(Nh,Bx),e(Bx,qeo),e(Nh,Geo),e(F,Oeo),e(F,Dh),e(Dh,MJ),e(MJ,Xeo),e(Dh,zeo),e(Dh,kx),e(kx,Veo),e(Dh,Weo),e(F,Qeo),e(F,qh),e(qh,EJ),e(EJ,Heo),e(qh,Ueo),e(qh,xx),e(xx,Jeo),e(qh,Yeo),e(F,Keo),e(F,Gh),e(Gh,yJ),e(yJ,Zeo),e(Gh,eoo),e(Gh,Rx),e(Rx,ooo),e(Gh,roo),e(F,too),e(F,Oh),e(Oh,wJ),e(wJ,aoo),e(Oh,noo),e(Oh,Sx),e(Sx,soo),e(Oh,loo),e(F,ioo),e(F,Xh),e(Xh,AJ),e(AJ,doo),e(Xh,coo),e(Xh,Px),e(Px,foo),e(Xh,moo),e(F,goo),e(F,zh),e(zh,LJ),e(LJ,hoo),e(zh,poo),e(zh,$x),e($x,_oo),e(zh,uoo),e(F,boo),e(F,Vh),e(Vh,BJ),e(BJ,voo),e(Vh,Too),e(Vh,Ix),e(Ix,Foo),e(Vh,Coo),e(F,Moo),e(F,Wh),e(Wh,kJ),e(kJ,Eoo),e(Wh,yoo),e(Wh,jx),e(jx,woo),e(Wh,Aoo),e(F,Loo),e(F,xs),e(xs,xJ),e(xJ,Boo),e(xs,koo),e(xs,Nx),e(Nx,xoo),e(xs,Roo),e(xs,Dx),e(Dx,Soo),e(xs,Poo),e(F,$oo),e(F,Qh),e(Qh,RJ),e(RJ,Ioo),e(Qh,joo),e(Qh,qx),e(qx,Noo),e(Qh,Doo),e(F,qoo),e(F,Hh),e(Hh,SJ),e(SJ,Goo),e(Hh,Ooo),e(Hh,Gx),e(Gx,Xoo),e(Hh,zoo),e(F,Voo),e(F,Uh),e(Uh,PJ),e(PJ,Woo),e(Uh,Qoo),e(Uh,Ox),e(Ox,Hoo),e(Uh,Uoo),e(F,Joo),e(F,Jh),e(Jh,$J),e($J,Yoo),e(Jh,Koo),e(Jh,Xx),e(Xx,Zoo),e(Jh,ero),e(F,oro),e(F,Yh),e(Yh,IJ),e(IJ,rro),e(Yh,tro),e(Yh,zx),e(zx,aro),e(Yh,nro),e(F,sro),e(F,Kh),e(Kh,jJ),e(jJ,lro),e(Kh,iro),e(Kh,Vx),e(Vx,dro),e(Kh,cro),e(F,fro),e(F,Zh),e(Zh,NJ),e(NJ,mro),e(Zh,gro),e(Zh,Wx),e(Wx,hro),e(Zh,pro),e(F,_ro),e(F,ep),e(ep,DJ),e(DJ,uro),e(ep,bro),e(ep,Qx),e(Qx,vro),e(ep,Tro),e(F,Fro),e(F,op),e(op,qJ),e(qJ,Cro),e(op,Mro),e(op,Hx),e(Hx,Ero),e(op,yro),e(F,wro),e(F,rp),e(rp,GJ),e(GJ,Aro),e(rp,Lro),e(rp,Ux),e(Ux,Bro),e(rp,kro),e(F,xro),e(F,tp),e(tp,OJ),e(OJ,Rro),e(tp,Sro),e(tp,Jx),e(Jx,Pro),e(tp,$ro),e(F,Iro),e(F,ap),e(ap,XJ),e(XJ,jro),e(ap,Nro),e(ap,Yx),e(Yx,Dro),e(ap,qro),e(F,Gro),e(F,np),e(np,zJ),e(zJ,Oro),e(np,Xro),e(np,Kx),e(Kx,zro),e(np,Vro),e(F,Wro),e(F,sp),e(sp,VJ),e(VJ,Qro),e(sp,Hro),e(sp,Zx),e(Zx,Uro),e(sp,Jro),e(F,Yro),e(F,lp),e(lp,WJ),e(WJ,Kro),e(lp,Zro),e(lp,eR),e(eR,eto),e(lp,oto),e(F,rto),e(F,ip),e(ip,QJ),e(QJ,tto),e(ip,ato),e(ip,oR),e(oR,nto),e(ip,sto),e(F,lto),e(F,dp),e(dp,HJ),e(HJ,ito),e(dp,dto),e(dp,rR),e(rR,cto),e(dp,fto),e(F,mto),e(F,cp),e(cp,UJ),e(UJ,gto),e(cp,hto),e(cp,tR),e(tR,pto),e(cp,_to),e(F,uto),e(F,fp),e(fp,JJ),e(JJ,bto),e(fp,vto),e(fp,aR),e(aR,Tto),e(fp,Fto),e(F,Cto),e(F,mp),e(mp,YJ),e(YJ,Mto),e(mp,Eto),e(mp,nR),e(nR,yto),e(mp,wto),e(F,Ato),e(F,gp),e(gp,KJ),e(KJ,Lto),e(gp,Bto),e(gp,sR),e(sR,kto),e(gp,xto),e(F,Rto),e(F,hp),e(hp,ZJ),e(ZJ,Sto),e(hp,Pto),e(hp,lR),e(lR,$to),e(hp,Ito),e(F,jto),e(F,pp),e(pp,eY),e(eY,Nto),e(pp,Dto),e(pp,iR),e(iR,qto),e(pp,Gto),e(F,Oto),e(F,_p),e(_p,oY),e(oY,Xto),e(_p,zto),e(_p,dR),e(dR,Vto),e(_p,Wto),e(F,Qto),e(F,up),e(up,rY),e(rY,Hto),e(up,Uto),e(up,cR),e(cR,Jto),e(up,Yto),e(F,Kto),e(F,bp),e(bp,tY),e(tY,Zto),e(bp,eao),e(bp,fR),e(fR,oao),e(bp,rao),e(F,tao),e(F,vp),e(vp,aY),e(aY,aao),e(vp,nao),e(vp,mR),e(mR,sao),e(vp,lao),e(F,iao),e(F,Tp),e(Tp,nY),e(nY,dao),e(Tp,cao),e(Tp,gR),e(gR,fao),e(Tp,mao),e(F,gao),e(F,Fp),e(Fp,sY),e(sY,hao),e(Fp,pao),e(Fp,hR),e(hR,_ao),e(Fp,uao),e(F,bao),e(F,Cp),e(Cp,lY),e(lY,vao),e(Cp,Tao),e(Cp,pR),e(pR,Fao),e(Cp,Cao),e(F,Mao),e(F,Mp),e(Mp,iY),e(iY,Eao),e(Mp,yao),e(Mp,_R),e(_R,wao),e(Mp,Aao),e(F,Lao),e(F,Ep),e(Ep,dY),e(dY,Bao),e(Ep,kao),e(Ep,uR),e(uR,xao),e(Ep,Rao),e(F,Sao),e(F,yp),e(yp,cY),e(cY,Pao),e(yp,$ao),e(yp,bR),e(bR,Iao),e(yp,jao),e(F,Nao),e(F,wp),e(wp,fY),e(fY,Dao),e(wp,qao),e(wp,vR),e(vR,Gao),e(wp,Oao),e(F,Xao),e(F,Ap),e(Ap,mY),e(mY,zao),e(Ap,Vao),e(Ap,TR),e(TR,Wao),e(Ap,Qao),e(F,Hao),e(F,Lp),e(Lp,gY),e(gY,Uao),e(Lp,Jao),e(Lp,FR),e(FR,Yao),e(Lp,Kao),e(F,Zao),e(F,Bp),e(Bp,hY),e(hY,eno),e(Bp,ono),e(Bp,CR),e(CR,rno),e(Bp,tno),e(F,ano),e(F,kp),e(kp,pY),e(pY,nno),e(kp,sno),e(kp,MR),e(MR,lno),e(kp,ino),e(F,dno),e(F,xp),e(xp,_Y),e(_Y,cno),e(xp,fno),e(xp,ER),e(ER,mno),e(xp,gno),e(F,hno),e(F,Rp),e(Rp,uY),e(uY,pno),e(Rp,_no),e(Rp,yR),e(yR,uno),e(Rp,bno),e(F,vno),e(F,Sp),e(Sp,bY),e(bY,Tno),e(Sp,Fno),e(Sp,wR),e(wR,Cno),e(Sp,Mno),e(F,Eno),e(F,Pp),e(Pp,vY),e(vY,yno),e(Pp,wno),e(Pp,AR),e(AR,Ano),e(Pp,Lno),e(F,Bno),e(F,$p),e($p,TY),e(TY,kno),e($p,xno),e($p,LR),e(LR,Rno),e($p,Sno),e(F,Pno),e(F,Ip),e(Ip,FY),e(FY,$no),e(Ip,Ino),e(Ip,BR),e(BR,jno),e(Ip,Nno),e(F,Dno),e(F,jp),e(jp,CY),e(CY,qno),e(jp,Gno),e(jp,kR),e(kR,Ono),e(jp,Xno),e(F,zno),e(F,Np),e(Np,MY),e(MY,Vno),e(Np,Wno),e(Np,xR),e(xR,Qno),e(Np,Hno),e(F,Uno),e(F,Dp),e(Dp,EY),e(EY,Jno),e(Dp,Yno),e(Dp,RR),e(RR,Kno),e(Dp,Zno),e(F,eso),e(F,qp),e(qp,yY),e(yY,oso),e(qp,rso),e(qp,SR),e(SR,tso),e(qp,aso),e(F,nso),e(F,Gp),e(Gp,wY),e(wY,sso),e(Gp,lso),e(Gp,PR),e(PR,iso),e(Gp,dso),e(F,cso),e(F,Op),e(Op,AY),e(AY,fso),e(Op,mso),e(Op,$R),e($R,gso),e(Op,hso),e(F,pso),e(F,Xp),e(Xp,LY),e(LY,_so),e(Xp,uso),e(Xp,IR),e(IR,bso),e(Xp,vso),e(F,Tso),e(F,zp),e(zp,BY),e(BY,Fso),e(zp,Cso),e(zp,jR),e(jR,Mso),e(zp,Eso),e(F,yso),e(F,Vp),e(Vp,kY),e(kY,wso),e(Vp,Aso),e(Vp,NR),e(NR,Lso),e(Vp,Bso),e(F,kso),e(F,Wp),e(Wp,xY),e(xY,xso),e(Wp,Rso),e(Wp,DR),e(DR,Sso),e(Wp,Pso),e(F,$so),e(F,Qp),e(Qp,RY),e(RY,Iso),e(Qp,jso),e(Qp,qR),e(qR,Nso),e(Qp,Dso),e(F,qso),e(F,Hp),e(Hp,SY),e(SY,Gso),e(Hp,Oso),e(Hp,GR),e(GR,Xso),e(Hp,zso),e(F,Vso),e(F,Up),e(Up,PY),e(PY,Wso),e(Up,Qso),e(Up,OR),e(OR,Hso),e(Up,Uso),e(F,Jso),e(F,Jp),e(Jp,$Y),e($Y,Yso),e(Jp,Kso),e(Jp,XR),e(XR,Zso),e(Jp,elo),e(ke,olo),e(ke,Yp),e(Yp,rlo),e(Yp,IY),e(IY,tlo),e(Yp,alo),e(Yp,jY),e(jY,nlo),e(ke,slo),e(ke,NY),e(NY,llo),e(ke,ilo),g(GM,ke,null),b(d,bLe,u),b(d,Xi,u),e(Xi,Kp),e(Kp,DY),g(OM,DY,null),e(Xi,dlo),e(Xi,qY),e(qY,clo),b(d,vLe,u),b(d,Wo,u),g(XM,Wo,null),e(Wo,flo),e(Wo,zi),e(zi,mlo),e(zi,GY),e(GY,glo),e(zi,hlo),e(zi,OY),e(OY,plo),e(zi,_lo),e(Wo,ulo),e(Wo,zM),e(zM,blo),e(zM,XY),e(XY,vlo),e(zM,Tlo),e(Wo,Flo),e(Wo,Dr),g(VM,Dr,null),e(Dr,Clo),e(Dr,zY),e(zY,Mlo),e(Dr,Elo),e(Dr,Vi),e(Vi,ylo),e(Vi,VY),e(VY,wlo),e(Vi,Alo),e(Vi,WY),e(WY,Llo),e(Vi,Blo),e(Dr,klo),e(Dr,QY),e(QY,xlo),e(Dr,Rlo),g(WM,Dr,null),e(Wo,Slo),e(Wo,xe),g(QM,xe,null),e(xe,Plo),e(xe,HY),e(HY,$lo),e(xe,Ilo),e(xe,qa),e(qa,jlo),e(qa,UY),e(UY,Nlo),e(qa,Dlo),e(qa,JY),e(JY,qlo),e(qa,Glo),e(qa,YY),e(YY,Olo),e(qa,Xlo),e(xe,zlo),e(xe,x),e(x,Zp),e(Zp,KY),e(KY,Vlo),e(Zp,Wlo),e(Zp,zR),e(zR,Qlo),e(Zp,Hlo),e(x,Ulo),e(x,e_),e(e_,ZY),e(ZY,Jlo),e(e_,Ylo),e(e_,VR),e(VR,Klo),e(e_,Zlo),e(x,eio),e(x,o_),e(o_,eK),e(eK,oio),e(o_,rio),e(o_,WR),e(WR,tio),e(o_,aio),e(x,nio),e(x,r_),e(r_,oK),e(oK,sio),e(r_,lio),e(r_,QR),e(QR,iio),e(r_,dio),e(x,cio),e(x,t_),e(t_,rK),e(rK,fio),e(t_,mio),e(t_,HR),e(HR,gio),e(t_,hio),e(x,pio),e(x,a_),e(a_,tK),e(tK,_io),e(a_,uio),e(a_,UR),e(UR,bio),e(a_,vio),e(x,Tio),e(x,n_),e(n_,aK),e(aK,Fio),e(n_,Cio),e(n_,JR),e(JR,Mio),e(n_,Eio),e(x,yio),e(x,s_),e(s_,nK),e(nK,wio),e(s_,Aio),e(s_,YR),e(YR,Lio),e(s_,Bio),e(x,kio),e(x,l_),e(l_,sK),e(sK,xio),e(l_,Rio),e(l_,KR),e(KR,Sio),e(l_,Pio),e(x,$io),e(x,i_),e(i_,lK),e(lK,Iio),e(i_,jio),e(i_,ZR),e(ZR,Nio),e(i_,Dio),e(x,qio),e(x,d_),e(d_,iK),e(iK,Gio),e(d_,Oio),e(d_,eS),e(eS,Xio),e(d_,zio),e(x,Vio),e(x,c_),e(c_,dK),e(dK,Wio),e(c_,Qio),e(c_,oS),e(oS,Hio),e(c_,Uio),e(x,Jio),e(x,f_),e(f_,cK),e(cK,Yio),e(f_,Kio),e(f_,rS),e(rS,Zio),e(f_,edo),e(x,odo),e(x,m_),e(m_,fK),e(fK,rdo),e(m_,tdo),e(m_,tS),e(tS,ado),e(m_,ndo),e(x,sdo),e(x,g_),e(g_,mK),e(mK,ldo),e(g_,ido),e(g_,aS),e(aS,ddo),e(g_,cdo),e(x,fdo),e(x,h_),e(h_,gK),e(gK,mdo),e(h_,gdo),e(h_,nS),e(nS,hdo),e(h_,pdo),e(x,_do),e(x,p_),e(p_,hK),e(hK,udo),e(p_,bdo),e(p_,sS),e(sS,vdo),e(p_,Tdo),e(x,Fdo),e(x,__),e(__,pK),e(pK,Cdo),e(__,Mdo),e(__,lS),e(lS,Edo),e(__,ydo),e(x,wdo),e(x,u_),e(u_,_K),e(_K,Ado),e(u_,Ldo),e(u_,iS),e(iS,Bdo),e(u_,kdo),e(x,xdo),e(x,b_),e(b_,uK),e(uK,Rdo),e(b_,Sdo),e(b_,dS),e(dS,Pdo),e(b_,$do),e(x,Ido),e(x,v_),e(v_,bK),e(bK,jdo),e(v_,Ndo),e(v_,cS),e(cS,Ddo),e(v_,qdo),e(x,Gdo),e(x,T_),e(T_,vK),e(vK,Odo),e(T_,Xdo),e(T_,fS),e(fS,zdo),e(T_,Vdo),e(x,Wdo),e(x,F_),e(F_,TK),e(TK,Qdo),e(F_,Hdo),e(F_,mS),e(mS,Udo),e(F_,Jdo),e(x,Ydo),e(x,C_),e(C_,FK),e(FK,Kdo),e(C_,Zdo),e(C_,gS),e(gS,eco),e(C_,oco),e(x,rco),e(x,M_),e(M_,CK),e(CK,tco),e(M_,aco),e(M_,hS),e(hS,nco),e(M_,sco),e(x,lco),e(x,E_),e(E_,MK),e(MK,ico),e(E_,dco),e(E_,pS),e(pS,cco),e(E_,fco),e(x,mco),e(x,y_),e(y_,EK),e(EK,gco),e(y_,hco),e(y_,_S),e(_S,pco),e(y_,_co),e(x,uco),e(x,w_),e(w_,yK),e(yK,bco),e(w_,vco),e(w_,uS),e(uS,Tco),e(w_,Fco),e(x,Cco),e(x,A_),e(A_,wK),e(wK,Mco),e(A_,Eco),e(A_,bS),e(bS,yco),e(A_,wco),e(x,Aco),e(x,L_),e(L_,AK),e(AK,Lco),e(L_,Bco),e(L_,vS),e(vS,kco),e(L_,xco),e(x,Rco),e(x,B_),e(B_,LK),e(LK,Sco),e(B_,Pco),e(B_,TS),e(TS,$co),e(B_,Ico),e(x,jco),e(x,k_),e(k_,BK),e(BK,Nco),e(k_,Dco),e(k_,FS),e(FS,qco),e(k_,Gco),e(x,Oco),e(x,x_),e(x_,kK),e(kK,Xco),e(x_,zco),e(x_,CS),e(CS,Vco),e(x_,Wco),e(x,Qco),e(x,R_),e(R_,xK),e(xK,Hco),e(R_,Uco),e(R_,MS),e(MS,Jco),e(R_,Yco),e(x,Kco),e(x,S_),e(S_,RK),e(RK,Zco),e(S_,efo),e(S_,ES),e(ES,ofo),e(S_,rfo),e(x,tfo),e(x,P_),e(P_,SK),e(SK,afo),e(P_,nfo),e(P_,yS),e(yS,sfo),e(P_,lfo),e(x,ifo),e(x,$_),e($_,PK),e(PK,dfo),e($_,cfo),e($_,wS),e(wS,ffo),e($_,mfo),e(x,gfo),e(x,I_),e(I_,$K),e($K,hfo),e(I_,pfo),e(I_,AS),e(AS,_fo),e(I_,ufo),e(xe,bfo),e(xe,j_),e(j_,vfo),e(j_,IK),e(IK,Tfo),e(j_,Ffo),e(j_,jK),e(jK,Cfo),e(xe,Mfo),e(xe,NK),e(NK,Efo),e(xe,yfo),g(HM,xe,null),b(d,TLe,u),b(d,Wi,u),e(Wi,N_),e(N_,DK),g(UM,DK,null),e(Wi,wfo),e(Wi,qK),e(qK,Afo),b(d,FLe,u),b(d,Qo,u),g(JM,Qo,null),e(Qo,Lfo),e(Qo,Qi),e(Qi,Bfo),e(Qi,GK),e(GK,kfo),e(Qi,xfo),e(Qi,OK),e(OK,Rfo),e(Qi,Sfo),e(Qo,Pfo),e(Qo,YM),e(YM,$fo),e(YM,XK),e(XK,Ifo),e(YM,jfo),e(Qo,Nfo),e(Qo,qr),g(KM,qr,null),e(qr,Dfo),e(qr,zK),e(zK,qfo),e(qr,Gfo),e(qr,Hi),e(Hi,Ofo),e(Hi,VK),e(VK,Xfo),e(Hi,zfo),e(Hi,WK),e(WK,Vfo),e(Hi,Wfo),e(qr,Qfo),e(qr,QK),e(QK,Hfo),e(qr,Ufo),g(ZM,qr,null),e(Qo,Jfo),e(Qo,Re),g(eE,Re,null),e(Re,Yfo),e(Re,HK),e(HK,Kfo),e(Re,Zfo),e(Re,Ga),e(Ga,emo),e(Ga,UK),e(UK,omo),e(Ga,rmo),e(Ga,JK),e(JK,tmo),e(Ga,amo),e(Ga,YK),e(YK,nmo),e(Ga,smo),e(Re,lmo),e(Re,$),e($,D_),e(D_,KK),e(KK,imo),e(D_,dmo),e(D_,LS),e(LS,cmo),e(D_,fmo),e($,mmo),e($,q_),e(q_,ZK),e(ZK,gmo),e(q_,hmo),e(q_,BS),e(BS,pmo),e(q_,_mo),e($,umo),e($,G_),e(G_,eZ),e(eZ,bmo),e(G_,vmo),e(G_,kS),e(kS,Tmo),e(G_,Fmo),e($,Cmo),e($,O_),e(O_,oZ),e(oZ,Mmo),e(O_,Emo),e(O_,xS),e(xS,ymo),e(O_,wmo),e($,Amo),e($,X_),e(X_,rZ),e(rZ,Lmo),e(X_,Bmo),e(X_,RS),e(RS,kmo),e(X_,xmo),e($,Rmo),e($,z_),e(z_,tZ),e(tZ,Smo),e(z_,Pmo),e(z_,SS),e(SS,$mo),e(z_,Imo),e($,jmo),e($,V_),e(V_,aZ),e(aZ,Nmo),e(V_,Dmo),e(V_,PS),e(PS,qmo),e(V_,Gmo),e($,Omo),e($,W_),e(W_,nZ),e(nZ,Xmo),e(W_,zmo),e(W_,$S),e($S,Vmo),e(W_,Wmo),e($,Qmo),e($,Q_),e(Q_,sZ),e(sZ,Hmo),e(Q_,Umo),e(Q_,IS),e(IS,Jmo),e(Q_,Ymo),e($,Kmo),e($,H_),e(H_,lZ),e(lZ,Zmo),e(H_,ego),e(H_,jS),e(jS,ogo),e(H_,rgo),e($,tgo),e($,U_),e(U_,iZ),e(iZ,ago),e(U_,ngo),e(U_,NS),e(NS,sgo),e(U_,lgo),e($,igo),e($,J_),e(J_,dZ),e(dZ,dgo),e(J_,cgo),e(J_,DS),e(DS,fgo),e(J_,mgo),e($,ggo),e($,Y_),e(Y_,cZ),e(cZ,hgo),e(Y_,pgo),e(Y_,qS),e(qS,_go),e(Y_,ugo),e($,bgo),e($,K_),e(K_,fZ),e(fZ,vgo),e(K_,Tgo),e(K_,GS),e(GS,Fgo),e(K_,Cgo),e($,Mgo),e($,Z_),e(Z_,mZ),e(mZ,Ego),e(Z_,ygo),e(Z_,OS),e(OS,wgo),e(Z_,Ago),e($,Lgo),e($,eu),e(eu,gZ),e(gZ,Bgo),e(eu,kgo),e(eu,XS),e(XS,xgo),e(eu,Rgo),e($,Sgo),e($,ou),e(ou,hZ),e(hZ,Pgo),e(ou,$go),e(ou,zS),e(zS,Igo),e(ou,jgo),e($,Ngo),e($,ru),e(ru,pZ),e(pZ,Dgo),e(ru,qgo),e(ru,VS),e(VS,Ggo),e(ru,Ogo),e($,Xgo),e($,tu),e(tu,_Z),e(_Z,zgo),e(tu,Vgo),e(tu,WS),e(WS,Wgo),e(tu,Qgo),e($,Hgo),e($,au),e(au,uZ),e(uZ,Ugo),e(au,Jgo),e(au,QS),e(QS,Ygo),e(au,Kgo),e($,Zgo),e($,nu),e(nu,bZ),e(bZ,eho),e(nu,oho),e(nu,HS),e(HS,rho),e(nu,tho),e($,aho),e($,su),e(su,vZ),e(vZ,nho),e(su,sho),e(su,US),e(US,lho),e(su,iho),e($,dho),e($,lu),e(lu,TZ),e(TZ,cho),e(lu,fho),e(lu,JS),e(JS,mho),e(lu,gho),e($,hho),e($,iu),e(iu,FZ),e(FZ,pho),e(iu,_ho),e(iu,YS),e(YS,uho),e(iu,bho),e($,vho),e($,du),e(du,CZ),e(CZ,Tho),e(du,Fho),e(du,KS),e(KS,Cho),e(du,Mho),e($,Eho),e($,cu),e(cu,MZ),e(MZ,yho),e(cu,who),e(cu,ZS),e(ZS,Aho),e(cu,Lho),e($,Bho),e($,fu),e(fu,EZ),e(EZ,kho),e(fu,xho),e(fu,eP),e(eP,Rho),e(fu,Sho),e($,Pho),e($,mu),e(mu,yZ),e(yZ,$ho),e(mu,Iho),e(mu,oP),e(oP,jho),e(mu,Nho),e($,Dho),e($,gu),e(gu,wZ),e(wZ,qho),e(gu,Gho),e(gu,rP),e(rP,Oho),e(gu,Xho),e($,zho),e($,hu),e(hu,AZ),e(AZ,Vho),e(hu,Who),e(hu,tP),e(tP,Qho),e(hu,Hho),e($,Uho),e($,pu),e(pu,LZ),e(LZ,Jho),e(pu,Yho),e(pu,aP),e(aP,Kho),e(pu,Zho),e($,epo),e($,_u),e(_u,BZ),e(BZ,opo),e(_u,rpo),e(_u,nP),e(nP,tpo),e(_u,apo),e($,npo),e($,uu),e(uu,kZ),e(kZ,spo),e(uu,lpo),e(uu,sP),e(sP,ipo),e(uu,dpo),e($,cpo),e($,bu),e(bu,xZ),e(xZ,fpo),e(bu,mpo),e(bu,lP),e(lP,gpo),e(bu,hpo),e(Re,ppo),e(Re,vu),e(vu,_po),e(vu,RZ),e(RZ,upo),e(vu,bpo),e(vu,SZ),e(SZ,vpo),e(Re,Tpo),e(Re,PZ),e(PZ,Fpo),e(Re,Cpo),g(oE,Re,null),b(d,CLe,u),b(d,Ui,u),e(Ui,Tu),e(Tu,$Z),g(rE,$Z,null),e(Ui,Mpo),e(Ui,IZ),e(IZ,Epo),b(d,MLe,u),b(d,Ho,u),g(tE,Ho,null),e(Ho,ypo),e(Ho,Ji),e(Ji,wpo),e(Ji,jZ),e(jZ,Apo),e(Ji,Lpo),e(Ji,NZ),e(NZ,Bpo),e(Ji,kpo),e(Ho,xpo),e(Ho,aE),e(aE,Rpo),e(aE,DZ),e(DZ,Spo),e(aE,Ppo),e(Ho,$po),e(Ho,Gr),g(nE,Gr,null),e(Gr,Ipo),e(Gr,qZ),e(qZ,jpo),e(Gr,Npo),e(Gr,Yi),e(Yi,Dpo),e(Yi,GZ),e(GZ,qpo),e(Yi,Gpo),e(Yi,OZ),e(OZ,Opo),e(Yi,Xpo),e(Gr,zpo),e(Gr,XZ),e(XZ,Vpo),e(Gr,Wpo),g(sE,Gr,null),e(Ho,Qpo),e(Ho,Se),g(lE,Se,null),e(Se,Hpo),e(Se,zZ),e(zZ,Upo),e(Se,Jpo),e(Se,Oa),e(Oa,Ypo),e(Oa,VZ),e(VZ,Kpo),e(Oa,Zpo),e(Oa,WZ),e(WZ,e_o),e(Oa,o_o),e(Oa,QZ),e(QZ,r_o),e(Oa,t_o),e(Se,a_o),e(Se,I),e(I,Fu),e(Fu,HZ),e(HZ,n_o),e(Fu,s_o),e(Fu,iP),e(iP,l_o),e(Fu,i_o),e(I,d_o),e(I,Cu),e(Cu,UZ),e(UZ,c_o),e(Cu,f_o),e(Cu,dP),e(dP,m_o),e(Cu,g_o),e(I,h_o),e(I,Mu),e(Mu,JZ),e(JZ,p_o),e(Mu,__o),e(Mu,cP),e(cP,u_o),e(Mu,b_o),e(I,v_o),e(I,Eu),e(Eu,YZ),e(YZ,T_o),e(Eu,F_o),e(Eu,fP),e(fP,C_o),e(Eu,M_o),e(I,E_o),e(I,yu),e(yu,KZ),e(KZ,y_o),e(yu,w_o),e(yu,mP),e(mP,A_o),e(yu,L_o),e(I,B_o),e(I,wu),e(wu,ZZ),e(ZZ,k_o),e(wu,x_o),e(wu,gP),e(gP,R_o),e(wu,S_o),e(I,P_o),e(I,Au),e(Au,eee),e(eee,$_o),e(Au,I_o),e(Au,hP),e(hP,j_o),e(Au,N_o),e(I,D_o),e(I,Lu),e(Lu,oee),e(oee,q_o),e(Lu,G_o),e(Lu,pP),e(pP,O_o),e(Lu,X_o),e(I,z_o),e(I,Bu),e(Bu,ree),e(ree,V_o),e(Bu,W_o),e(Bu,_P),e(_P,Q_o),e(Bu,H_o),e(I,U_o),e(I,ku),e(ku,tee),e(tee,J_o),e(ku,Y_o),e(ku,uP),e(uP,K_o),e(ku,Z_o),e(I,euo),e(I,xu),e(xu,aee),e(aee,ouo),e(xu,ruo),e(xu,bP),e(bP,tuo),e(xu,auo),e(I,nuo),e(I,Ru),e(Ru,nee),e(nee,suo),e(Ru,luo),e(Ru,vP),e(vP,iuo),e(Ru,duo),e(I,cuo),e(I,Su),e(Su,see),e(see,fuo),e(Su,muo),e(Su,TP),e(TP,guo),e(Su,huo),e(I,puo),e(I,Pu),e(Pu,lee),e(lee,_uo),e(Pu,uuo),e(Pu,FP),e(FP,buo),e(Pu,vuo),e(I,Tuo),e(I,$u),e($u,iee),e(iee,Fuo),e($u,Cuo),e($u,CP),e(CP,Muo),e($u,Euo),e(I,yuo),e(I,Iu),e(Iu,dee),e(dee,wuo),e(Iu,Auo),e(Iu,MP),e(MP,Luo),e(Iu,Buo),e(I,kuo),e(I,ju),e(ju,cee),e(cee,xuo),e(ju,Ruo),e(ju,EP),e(EP,Suo),e(ju,Puo),e(I,$uo),e(I,Nu),e(Nu,fee),e(fee,Iuo),e(Nu,juo),e(Nu,yP),e(yP,Nuo),e(Nu,Duo),e(I,quo),e(I,Du),e(Du,mee),e(mee,Guo),e(Du,Ouo),e(Du,wP),e(wP,Xuo),e(Du,zuo),e(I,Vuo),e(I,qu),e(qu,gee),e(gee,Wuo),e(qu,Quo),e(qu,AP),e(AP,Huo),e(qu,Uuo),e(I,Juo),e(I,Gu),e(Gu,hee),e(hee,Yuo),e(Gu,Kuo),e(Gu,LP),e(LP,Zuo),e(Gu,e2o),e(I,o2o),e(I,Ou),e(Ou,pee),e(pee,r2o),e(Ou,t2o),e(Ou,BP),e(BP,a2o),e(Ou,n2o),e(I,s2o),e(I,Xu),e(Xu,_ee),e(_ee,l2o),e(Xu,i2o),e(Xu,kP),e(kP,d2o),e(Xu,c2o),e(I,f2o),e(I,zu),e(zu,uee),e(uee,m2o),e(zu,g2o),e(zu,xP),e(xP,h2o),e(zu,p2o),e(I,_2o),e(I,Vu),e(Vu,bee),e(bee,u2o),e(Vu,b2o),e(Vu,RP),e(RP,v2o),e(Vu,T2o),e(I,F2o),e(I,Wu),e(Wu,vee),e(vee,C2o),e(Wu,M2o),e(Wu,SP),e(SP,E2o),e(Wu,y2o),e(I,w2o),e(I,Qu),e(Qu,Tee),e(Tee,A2o),e(Qu,L2o),e(Qu,PP),e(PP,B2o),e(Qu,k2o),e(I,x2o),e(I,Hu),e(Hu,Fee),e(Fee,R2o),e(Hu,S2o),e(Hu,$P),e($P,P2o),e(Hu,$2o),e(I,I2o),e(I,Uu),e(Uu,Cee),e(Cee,j2o),e(Uu,N2o),e(Uu,IP),e(IP,D2o),e(Uu,q2o),e(I,G2o),e(I,Ju),e(Ju,Mee),e(Mee,O2o),e(Ju,X2o),e(Ju,Eee),e(Eee,z2o),e(Ju,V2o),e(I,W2o),e(I,Yu),e(Yu,yee),e(yee,Q2o),e(Yu,H2o),e(Yu,jP),e(jP,U2o),e(Yu,J2o),e(I,Y2o),e(I,Ku),e(Ku,wee),e(wee,K2o),e(Ku,Z2o),e(Ku,NP),e(NP,e1o),e(Ku,o1o),e(I,r1o),e(I,Zu),e(Zu,Aee),e(Aee,t1o),e(Zu,a1o),e(Zu,DP),e(DP,n1o),e(Zu,s1o),e(I,l1o),e(I,e2),e(e2,Lee),e(Lee,i1o),e(e2,d1o),e(e2,qP),e(qP,c1o),e(e2,f1o),e(Se,m1o),e(Se,o2),e(o2,g1o),e(o2,Bee),e(Bee,h1o),e(o2,p1o),e(o2,kee),e(kee,_1o),e(Se,u1o),e(Se,xee),e(xee,b1o),e(Se,v1o),g(iE,Se,null),b(d,ELe,u),b(d,Ki,u),e(Ki,r2),e(r2,Ree),g(dE,Ree,null),e(Ki,T1o),e(Ki,See),e(See,F1o),b(d,yLe,u),b(d,Uo,u),g(cE,Uo,null),e(Uo,C1o),e(Uo,Zi),e(Zi,M1o),e(Zi,Pee),e(Pee,E1o),e(Zi,y1o),e(Zi,$ee),e($ee,w1o),e(Zi,A1o),e(Uo,L1o),e(Uo,fE),e(fE,B1o),e(fE,Iee),e(Iee,k1o),e(fE,x1o),e(Uo,R1o),e(Uo,Or),g(mE,Or,null),e(Or,S1o),e(Or,jee),e(jee,P1o),e(Or,$1o),e(Or,ed),e(ed,I1o),e(ed,Nee),e(Nee,j1o),e(ed,N1o),e(ed,Dee),e(Dee,D1o),e(ed,q1o),e(Or,G1o),e(Or,qee),e(qee,O1o),e(Or,X1o),g(gE,Or,null),e(Uo,z1o),e(Uo,Pe),g(hE,Pe,null),e(Pe,V1o),e(Pe,Gee),e(Gee,W1o),e(Pe,Q1o),e(Pe,Xa),e(Xa,H1o),e(Xa,Oee),e(Oee,U1o),e(Xa,J1o),e(Xa,Xee),e(Xee,Y1o),e(Xa,K1o),e(Xa,zee),e(zee,Z1o),e(Xa,ebo),e(Pe,obo),e(Pe,ae),e(ae,t2),e(t2,Vee),e(Vee,rbo),e(t2,tbo),e(t2,GP),e(GP,abo),e(t2,nbo),e(ae,sbo),e(ae,a2),e(a2,Wee),e(Wee,lbo),e(a2,ibo),e(a2,OP),e(OP,dbo),e(a2,cbo),e(ae,fbo),e(ae,n2),e(n2,Qee),e(Qee,mbo),e(n2,gbo),e(n2,XP),e(XP,hbo),e(n2,pbo),e(ae,_bo),e(ae,s2),e(s2,Hee),e(Hee,ubo),e(s2,bbo),e(s2,zP),e(zP,vbo),e(s2,Tbo),e(ae,Fbo),e(ae,l2),e(l2,Uee),e(Uee,Cbo),e(l2,Mbo),e(l2,VP),e(VP,Ebo),e(l2,ybo),e(ae,wbo),e(ae,i2),e(i2,Jee),e(Jee,Abo),e(i2,Lbo),e(i2,WP),e(WP,Bbo),e(i2,kbo),e(ae,xbo),e(ae,d2),e(d2,Yee),e(Yee,Rbo),e(d2,Sbo),e(d2,QP),e(QP,Pbo),e(d2,$bo),e(ae,Ibo),e(ae,c2),e(c2,Kee),e(Kee,jbo),e(c2,Nbo),e(c2,HP),e(HP,Dbo),e(c2,qbo),e(ae,Gbo),e(ae,f2),e(f2,Zee),e(Zee,Obo),e(f2,Xbo),e(f2,UP),e(UP,zbo),e(f2,Vbo),e(ae,Wbo),e(ae,m2),e(m2,eoe),e(eoe,Qbo),e(m2,Hbo),e(m2,JP),e(JP,Ubo),e(m2,Jbo),e(ae,Ybo),e(ae,g2),e(g2,ooe),e(ooe,Kbo),e(g2,Zbo),e(g2,YP),e(YP,e5o),e(g2,o5o),e(ae,r5o),e(ae,h2),e(h2,roe),e(roe,t5o),e(h2,a5o),e(h2,KP),e(KP,n5o),e(h2,s5o),e(ae,l5o),e(ae,p2),e(p2,toe),e(toe,i5o),e(p2,d5o),e(p2,ZP),e(ZP,c5o),e(p2,f5o),e(ae,m5o),e(ae,_2),e(_2,aoe),e(aoe,g5o),e(_2,h5o),e(_2,e$),e(e$,p5o),e(_2,_5o),e(ae,u5o),e(ae,u2),e(u2,noe),e(noe,b5o),e(u2,v5o),e(u2,o$),e(o$,T5o),e(u2,F5o),e(ae,C5o),e(ae,b2),e(b2,soe),e(soe,M5o),e(b2,E5o),e(b2,r$),e(r$,y5o),e(b2,w5o),e(Pe,A5o),e(Pe,v2),e(v2,L5o),e(v2,loe),e(loe,B5o),e(v2,k5o),e(v2,ioe),e(ioe,x5o),e(Pe,R5o),e(Pe,doe),e(doe,S5o),e(Pe,P5o),g(pE,Pe,null),b(d,wLe,u),b(d,od,u),e(od,T2),e(T2,coe),g(_E,coe,null),e(od,$5o),e(od,foe),e(foe,I5o),b(d,ALe,u),b(d,Jo,u),g(uE,Jo,null),e(Jo,j5o),e(Jo,rd),e(rd,N5o),e(rd,moe),e(moe,D5o),e(rd,q5o),e(rd,goe),e(goe,G5o),e(rd,O5o),e(Jo,X5o),e(Jo,bE),e(bE,z5o),e(bE,hoe),e(hoe,V5o),e(bE,W5o),e(Jo,Q5o),e(Jo,Xr),g(vE,Xr,null),e(Xr,H5o),e(Xr,poe),e(poe,U5o),e(Xr,J5o),e(Xr,td),e(td,Y5o),e(td,_oe),e(_oe,K5o),e(td,Z5o),e(td,uoe),e(uoe,evo),e(td,ovo),e(Xr,rvo),e(Xr,boe),e(boe,tvo),e(Xr,avo),g(TE,Xr,null),e(Jo,nvo),e(Jo,$e),g(FE,$e,null),e($e,svo),e($e,voe),e(voe,lvo),e($e,ivo),e($e,za),e(za,dvo),e(za,Toe),e(Toe,cvo),e(za,fvo),e(za,Foe),e(Foe,mvo),e(za,gvo),e(za,Coe),e(Coe,hvo),e(za,pvo),e($e,_vo),e($e,A),e(A,F2),e(F2,Moe),e(Moe,uvo),e(F2,bvo),e(F2,t$),e(t$,vvo),e(F2,Tvo),e(A,Fvo),e(A,C2),e(C2,Eoe),e(Eoe,Cvo),e(C2,Mvo),e(C2,a$),e(a$,Evo),e(C2,yvo),e(A,wvo),e(A,M2),e(M2,yoe),e(yoe,Avo),e(M2,Lvo),e(M2,n$),e(n$,Bvo),e(M2,kvo),e(A,xvo),e(A,E2),e(E2,woe),e(woe,Rvo),e(E2,Svo),e(E2,s$),e(s$,Pvo),e(E2,$vo),e(A,Ivo),e(A,y2),e(y2,Aoe),e(Aoe,jvo),e(y2,Nvo),e(y2,l$),e(l$,Dvo),e(y2,qvo),e(A,Gvo),e(A,w2),e(w2,Loe),e(Loe,Ovo),e(w2,Xvo),e(w2,i$),e(i$,zvo),e(w2,Vvo),e(A,Wvo),e(A,A2),e(A2,Boe),e(Boe,Qvo),e(A2,Hvo),e(A2,d$),e(d$,Uvo),e(A2,Jvo),e(A,Yvo),e(A,L2),e(L2,koe),e(koe,Kvo),e(L2,Zvo),e(L2,c$),e(c$,eTo),e(L2,oTo),e(A,rTo),e(A,B2),e(B2,xoe),e(xoe,tTo),e(B2,aTo),e(B2,f$),e(f$,nTo),e(B2,sTo),e(A,lTo),e(A,k2),e(k2,Roe),e(Roe,iTo),e(k2,dTo),e(k2,m$),e(m$,cTo),e(k2,fTo),e(A,mTo),e(A,x2),e(x2,Soe),e(Soe,gTo),e(x2,hTo),e(x2,g$),e(g$,pTo),e(x2,_To),e(A,uTo),e(A,R2),e(R2,Poe),e(Poe,bTo),e(R2,vTo),e(R2,h$),e(h$,TTo),e(R2,FTo),e(A,CTo),e(A,S2),e(S2,$oe),e($oe,MTo),e(S2,ETo),e(S2,p$),e(p$,yTo),e(S2,wTo),e(A,ATo),e(A,P2),e(P2,Ioe),e(Ioe,LTo),e(P2,BTo),e(P2,_$),e(_$,kTo),e(P2,xTo),e(A,RTo),e(A,$2),e($2,joe),e(joe,STo),e($2,PTo),e($2,u$),e(u$,$To),e($2,ITo),e(A,jTo),e(A,I2),e(I2,Noe),e(Noe,NTo),e(I2,DTo),e(I2,b$),e(b$,qTo),e(I2,GTo),e(A,OTo),e(A,j2),e(j2,Doe),e(Doe,XTo),e(j2,zTo),e(j2,v$),e(v$,VTo),e(j2,WTo),e(A,QTo),e(A,N2),e(N2,qoe),e(qoe,HTo),e(N2,UTo),e(N2,T$),e(T$,JTo),e(N2,YTo),e(A,KTo),e(A,D2),e(D2,Goe),e(Goe,ZTo),e(D2,e7o),e(D2,F$),e(F$,o7o),e(D2,r7o),e(A,t7o),e(A,q2),e(q2,Ooe),e(Ooe,a7o),e(q2,n7o),e(q2,C$),e(C$,s7o),e(q2,l7o),e(A,i7o),e(A,G2),e(G2,Xoe),e(Xoe,d7o),e(G2,c7o),e(G2,M$),e(M$,f7o),e(G2,m7o),e(A,g7o),e(A,O2),e(O2,zoe),e(zoe,h7o),e(O2,p7o),e(O2,E$),e(E$,_7o),e(O2,u7o),e(A,b7o),e(A,X2),e(X2,Voe),e(Voe,v7o),e(X2,T7o),e(X2,y$),e(y$,F7o),e(X2,C7o),e(A,M7o),e(A,z2),e(z2,Woe),e(Woe,E7o),e(z2,y7o),e(z2,w$),e(w$,w7o),e(z2,A7o),e(A,L7o),e(A,V2),e(V2,Qoe),e(Qoe,B7o),e(V2,k7o),e(V2,A$),e(A$,x7o),e(V2,R7o),e(A,S7o),e(A,W2),e(W2,Hoe),e(Hoe,P7o),e(W2,$7o),e(W2,L$),e(L$,I7o),e(W2,j7o),e(A,N7o),e(A,Q2),e(Q2,Uoe),e(Uoe,D7o),e(Q2,q7o),e(Q2,B$),e(B$,G7o),e(Q2,O7o),e(A,X7o),e(A,H2),e(H2,Joe),e(Joe,z7o),e(H2,V7o),e(H2,k$),e(k$,W7o),e(H2,Q7o),e(A,H7o),e(A,U2),e(U2,Yoe),e(Yoe,U7o),e(U2,J7o),e(U2,x$),e(x$,Y7o),e(U2,K7o),e(A,Z7o),e(A,J2),e(J2,Koe),e(Koe,eFo),e(J2,oFo),e(J2,R$),e(R$,rFo),e(J2,tFo),e(A,aFo),e(A,Y2),e(Y2,Zoe),e(Zoe,nFo),e(Y2,sFo),e(Y2,S$),e(S$,lFo),e(Y2,iFo),e(A,dFo),e(A,K2),e(K2,ere),e(ere,cFo),e(K2,fFo),e(K2,P$),e(P$,mFo),e(K2,gFo),e(A,hFo),e(A,Z2),e(Z2,ore),e(ore,pFo),e(Z2,_Fo),e(Z2,$$),e($$,uFo),e(Z2,bFo),e(A,vFo),e(A,e1),e(e1,rre),e(rre,TFo),e(e1,FFo),e(e1,I$),e(I$,CFo),e(e1,MFo),e(A,EFo),e(A,o1),e(o1,tre),e(tre,yFo),e(o1,wFo),e(o1,j$),e(j$,AFo),e(o1,LFo),e(A,BFo),e(A,r1),e(r1,are),e(are,kFo),e(r1,xFo),e(r1,N$),e(N$,RFo),e(r1,SFo),e(A,PFo),e(A,t1),e(t1,nre),e(nre,$Fo),e(t1,IFo),e(t1,D$),e(D$,jFo),e(t1,NFo),e(A,DFo),e(A,a1),e(a1,sre),e(sre,qFo),e(a1,GFo),e(a1,q$),e(q$,OFo),e(a1,XFo),e(A,zFo),e(A,n1),e(n1,lre),e(lre,VFo),e(n1,WFo),e(n1,G$),e(G$,QFo),e(n1,HFo),e(A,UFo),e(A,s1),e(s1,ire),e(ire,JFo),e(s1,YFo),e(s1,O$),e(O$,KFo),e(s1,ZFo),e(A,e9o),e(A,l1),e(l1,dre),e(dre,o9o),e(l1,r9o),e(l1,X$),e(X$,t9o),e(l1,a9o),e(A,n9o),e(A,i1),e(i1,cre),e(cre,s9o),e(i1,l9o),e(i1,z$),e(z$,i9o),e(i1,d9o),e(A,c9o),e(A,d1),e(d1,fre),e(fre,f9o),e(d1,m9o),e(d1,V$),e(V$,g9o),e(d1,h9o),e(A,p9o),e(A,c1),e(c1,mre),e(mre,_9o),e(c1,u9o),e(c1,W$),e(W$,b9o),e(c1,v9o),e(A,T9o),e(A,f1),e(f1,gre),e(gre,F9o),e(f1,C9o),e(f1,Q$),e(Q$,M9o),e(f1,E9o),e($e,y9o),e($e,m1),e(m1,w9o),e(m1,hre),e(hre,A9o),e(m1,L9o),e(m1,pre),e(pre,B9o),e($e,k9o),e($e,_re),e(_re,x9o),e($e,R9o),g(CE,$e,null),b(d,LLe,u),b(d,ad,u),e(ad,g1),e(g1,ure),g(ME,ure,null),e(ad,S9o),e(ad,bre),e(bre,P9o),b(d,BLe,u),b(d,Yo,u),g(EE,Yo,null),e(Yo,$9o),e(Yo,nd),e(nd,I9o),e(nd,vre),e(vre,j9o),e(nd,N9o),e(nd,Tre),e(Tre,D9o),e(nd,q9o),e(Yo,G9o),e(Yo,yE),e(yE,O9o),e(yE,Fre),e(Fre,X9o),e(yE,z9o),e(Yo,V9o),e(Yo,zr),g(wE,zr,null),e(zr,W9o),e(zr,Cre),e(Cre,Q9o),e(zr,H9o),e(zr,sd),e(sd,U9o),e(sd,Mre),e(Mre,J9o),e(sd,Y9o),e(sd,Ere),e(Ere,K9o),e(sd,Z9o),e(zr,eCo),e(zr,yre),e(yre,oCo),e(zr,rCo),g(AE,zr,null),e(Yo,tCo),e(Yo,Ie),g(LE,Ie,null),e(Ie,aCo),e(Ie,wre),e(wre,nCo),e(Ie,sCo),e(Ie,Va),e(Va,lCo),e(Va,Are),e(Are,iCo),e(Va,dCo),e(Va,Lre),e(Lre,cCo),e(Va,fCo),e(Va,Bre),e(Bre,mCo),e(Va,gCo),e(Ie,hCo),e(Ie,G),e(G,h1),e(h1,kre),e(kre,pCo),e(h1,_Co),e(h1,H$),e(H$,uCo),e(h1,bCo),e(G,vCo),e(G,p1),e(p1,xre),e(xre,TCo),e(p1,FCo),e(p1,U$),e(U$,CCo),e(p1,MCo),e(G,ECo),e(G,_1),e(_1,Rre),e(Rre,yCo),e(_1,wCo),e(_1,J$),e(J$,ACo),e(_1,LCo),e(G,BCo),e(G,u1),e(u1,Sre),e(Sre,kCo),e(u1,xCo),e(u1,Y$),e(Y$,RCo),e(u1,SCo),e(G,PCo),e(G,b1),e(b1,Pre),e(Pre,$Co),e(b1,ICo),e(b1,K$),e(K$,jCo),e(b1,NCo),e(G,DCo),e(G,v1),e(v1,$re),e($re,qCo),e(v1,GCo),e(v1,Z$),e(Z$,OCo),e(v1,XCo),e(G,zCo),e(G,T1),e(T1,Ire),e(Ire,VCo),e(T1,WCo),e(T1,eI),e(eI,QCo),e(T1,HCo),e(G,UCo),e(G,F1),e(F1,jre),e(jre,JCo),e(F1,YCo),e(F1,oI),e(oI,KCo),e(F1,ZCo),e(G,e4o),e(G,C1),e(C1,Nre),e(Nre,o4o),e(C1,r4o),e(C1,rI),e(rI,t4o),e(C1,a4o),e(G,n4o),e(G,M1),e(M1,Dre),e(Dre,s4o),e(M1,l4o),e(M1,tI),e(tI,i4o),e(M1,d4o),e(G,c4o),e(G,E1),e(E1,qre),e(qre,f4o),e(E1,m4o),e(E1,aI),e(aI,g4o),e(E1,h4o),e(G,p4o),e(G,y1),e(y1,Gre),e(Gre,_4o),e(y1,u4o),e(y1,nI),e(nI,b4o),e(y1,v4o),e(G,T4o),e(G,w1),e(w1,Ore),e(Ore,F4o),e(w1,C4o),e(w1,sI),e(sI,M4o),e(w1,E4o),e(G,y4o),e(G,A1),e(A1,Xre),e(Xre,w4o),e(A1,A4o),e(A1,lI),e(lI,L4o),e(A1,B4o),e(G,k4o),e(G,L1),e(L1,zre),e(zre,x4o),e(L1,R4o),e(L1,iI),e(iI,S4o),e(L1,P4o),e(G,$4o),e(G,B1),e(B1,Vre),e(Vre,I4o),e(B1,j4o),e(B1,dI),e(dI,N4o),e(B1,D4o),e(G,q4o),e(G,k1),e(k1,Wre),e(Wre,G4o),e(k1,O4o),e(k1,cI),e(cI,X4o),e(k1,z4o),e(G,V4o),e(G,x1),e(x1,Qre),e(Qre,W4o),e(x1,Q4o),e(x1,fI),e(fI,H4o),e(x1,U4o),e(G,J4o),e(G,R1),e(R1,Hre),e(Hre,Y4o),e(R1,K4o),e(R1,mI),e(mI,Z4o),e(R1,eMo),e(G,oMo),e(G,S1),e(S1,Ure),e(Ure,rMo),e(S1,tMo),e(S1,gI),e(gI,aMo),e(S1,nMo),e(G,sMo),e(G,P1),e(P1,Jre),e(Jre,lMo),e(P1,iMo),e(P1,hI),e(hI,dMo),e(P1,cMo),e(G,fMo),e(G,$1),e($1,Yre),e(Yre,mMo),e($1,gMo),e($1,pI),e(pI,hMo),e($1,pMo),e(G,_Mo),e(G,I1),e(I1,Kre),e(Kre,uMo),e(I1,bMo),e(I1,_I),e(_I,vMo),e(I1,TMo),e(G,FMo),e(G,j1),e(j1,Zre),e(Zre,CMo),e(j1,MMo),e(j1,uI),e(uI,EMo),e(j1,yMo),e(G,wMo),e(G,N1),e(N1,ete),e(ete,AMo),e(N1,LMo),e(N1,bI),e(bI,BMo),e(N1,kMo),e(G,xMo),e(G,D1),e(D1,ote),e(ote,RMo),e(D1,SMo),e(D1,vI),e(vI,PMo),e(D1,$Mo),e(G,IMo),e(G,q1),e(q1,rte),e(rte,jMo),e(q1,NMo),e(q1,TI),e(TI,DMo),e(q1,qMo),e(Ie,GMo),e(Ie,G1),e(G1,OMo),e(G1,tte),e(tte,XMo),e(G1,zMo),e(G1,ate),e(ate,VMo),e(Ie,WMo),e(Ie,nte),e(nte,QMo),e(Ie,HMo),g(BE,Ie,null),b(d,kLe,u),b(d,ld,u),e(ld,O1),e(O1,ste),g(kE,ste,null),e(ld,UMo),e(ld,lte),e(lte,JMo),b(d,xLe,u),b(d,Ko,u),g(xE,Ko,null),e(Ko,YMo),e(Ko,id),e(id,KMo),e(id,ite),e(ite,ZMo),e(id,eEo),e(id,dte),e(dte,oEo),e(id,rEo),e(Ko,tEo),e(Ko,RE),e(RE,aEo),e(RE,cte),e(cte,nEo),e(RE,sEo),e(Ko,lEo),e(Ko,Vr),g(SE,Vr,null),e(Vr,iEo),e(Vr,fte),e(fte,dEo),e(Vr,cEo),e(Vr,dd),e(dd,fEo),e(dd,mte),e(mte,mEo),e(dd,gEo),e(dd,gte),e(gte,hEo),e(dd,pEo),e(Vr,_Eo),e(Vr,hte),e(hte,uEo),e(Vr,bEo),g(PE,Vr,null),e(Ko,vEo),e(Ko,je),g($E,je,null),e(je,TEo),e(je,pte),e(pte,FEo),e(je,CEo),e(je,Wa),e(Wa,MEo),e(Wa,_te),e(_te,EEo),e(Wa,yEo),e(Wa,ute),e(ute,wEo),e(Wa,AEo),e(Wa,bte),e(bte,LEo),e(Wa,BEo),e(je,kEo),e(je,na),e(na,X1),e(X1,vte),e(vte,xEo),e(X1,REo),e(X1,FI),e(FI,SEo),e(X1,PEo),e(na,$Eo),e(na,z1),e(z1,Tte),e(Tte,IEo),e(z1,jEo),e(z1,CI),e(CI,NEo),e(z1,DEo),e(na,qEo),e(na,V1),e(V1,Fte),e(Fte,GEo),e(V1,OEo),e(V1,MI),e(MI,XEo),e(V1,zEo),e(na,VEo),e(na,W1),e(W1,Cte),e(Cte,WEo),e(W1,QEo),e(W1,EI),e(EI,HEo),e(W1,UEo),e(na,JEo),e(na,Q1),e(Q1,Mte),e(Mte,YEo),e(Q1,KEo),e(Q1,yI),e(yI,ZEo),e(Q1,e3o),e(je,o3o),e(je,H1),e(H1,r3o),e(H1,Ete),e(Ete,t3o),e(H1,a3o),e(H1,yte),e(yte,n3o),e(je,s3o),e(je,wte),e(wte,l3o),e(je,i3o),g(IE,je,null),b(d,RLe,u),b(d,cd,u),e(cd,U1),e(U1,Ate),g(jE,Ate,null),e(cd,d3o),e(cd,Lte),e(Lte,c3o),b(d,SLe,u),b(d,Zo,u),g(NE,Zo,null),e(Zo,f3o),e(Zo,fd),e(fd,m3o),e(fd,Bte),e(Bte,g3o),e(fd,h3o),e(fd,kte),e(kte,p3o),e(fd,_3o),e(Zo,u3o),e(Zo,DE),e(DE,b3o),e(DE,xte),e(xte,v3o),e(DE,T3o),e(Zo,F3o),e(Zo,Wr),g(qE,Wr,null),e(Wr,C3o),e(Wr,Rte),e(Rte,M3o),e(Wr,E3o),e(Wr,md),e(md,y3o),e(md,Ste),e(Ste,w3o),e(md,A3o),e(md,Pte),e(Pte,L3o),e(md,B3o),e(Wr,k3o),e(Wr,$te),e($te,x3o),e(Wr,R3o),g(GE,Wr,null),e(Zo,S3o),e(Zo,Ne),g(OE,Ne,null),e(Ne,P3o),e(Ne,Ite),e(Ite,$3o),e(Ne,I3o),e(Ne,Qa),e(Qa,j3o),e(Qa,jte),e(jte,N3o),e(Qa,D3o),e(Qa,Nte),e(Nte,q3o),e(Qa,G3o),e(Qa,Dte),e(Dte,O3o),e(Qa,X3o),e(Ne,z3o),e(Ne,D),e(D,J1),e(J1,qte),e(qte,V3o),e(J1,W3o),e(J1,wI),e(wI,Q3o),e(J1,H3o),e(D,U3o),e(D,Y1),e(Y1,Gte),e(Gte,J3o),e(Y1,Y3o),e(Y1,AI),e(AI,K3o),e(Y1,Z3o),e(D,eyo),e(D,K1),e(K1,Ote),e(Ote,oyo),e(K1,ryo),e(K1,LI),e(LI,tyo),e(K1,ayo),e(D,nyo),e(D,Z1),e(Z1,Xte),e(Xte,syo),e(Z1,lyo),e(Z1,BI),e(BI,iyo),e(Z1,dyo),e(D,cyo),e(D,eb),e(eb,zte),e(zte,fyo),e(eb,myo),e(eb,kI),e(kI,gyo),e(eb,hyo),e(D,pyo),e(D,ob),e(ob,Vte),e(Vte,_yo),e(ob,uyo),e(ob,xI),e(xI,byo),e(ob,vyo),e(D,Tyo),e(D,rb),e(rb,Wte),e(Wte,Fyo),e(rb,Cyo),e(rb,RI),e(RI,Myo),e(rb,Eyo),e(D,yyo),e(D,tb),e(tb,Qte),e(Qte,wyo),e(tb,Ayo),e(tb,SI),e(SI,Lyo),e(tb,Byo),e(D,kyo),e(D,ab),e(ab,Hte),e(Hte,xyo),e(ab,Ryo),e(ab,PI),e(PI,Syo),e(ab,Pyo),e(D,$yo),e(D,nb),e(nb,Ute),e(Ute,Iyo),e(nb,jyo),e(nb,$I),e($I,Nyo),e(nb,Dyo),e(D,qyo),e(D,sb),e(sb,Jte),e(Jte,Gyo),e(sb,Oyo),e(sb,II),e(II,Xyo),e(sb,zyo),e(D,Vyo),e(D,lb),e(lb,Yte),e(Yte,Wyo),e(lb,Qyo),e(lb,jI),e(jI,Hyo),e(lb,Uyo),e(D,Jyo),e(D,ib),e(ib,Kte),e(Kte,Yyo),e(ib,Kyo),e(ib,NI),e(NI,Zyo),e(ib,ewo),e(D,owo),e(D,db),e(db,Zte),e(Zte,rwo),e(db,two),e(db,DI),e(DI,awo),e(db,nwo),e(D,swo),e(D,cb),e(cb,eae),e(eae,lwo),e(cb,iwo),e(cb,qI),e(qI,dwo),e(cb,cwo),e(D,fwo),e(D,fb),e(fb,oae),e(oae,mwo),e(fb,gwo),e(fb,GI),e(GI,hwo),e(fb,pwo),e(D,_wo),e(D,mb),e(mb,rae),e(rae,uwo),e(mb,bwo),e(mb,OI),e(OI,vwo),e(mb,Two),e(D,Fwo),e(D,gb),e(gb,tae),e(tae,Cwo),e(gb,Mwo),e(gb,XI),e(XI,Ewo),e(gb,ywo),e(D,wwo),e(D,hb),e(hb,aae),e(aae,Awo),e(hb,Lwo),e(hb,zI),e(zI,Bwo),e(hb,kwo),e(D,xwo),e(D,pb),e(pb,nae),e(nae,Rwo),e(pb,Swo),e(pb,VI),e(VI,Pwo),e(pb,$wo),e(D,Iwo),e(D,_b),e(_b,sae),e(sae,jwo),e(_b,Nwo),e(_b,WI),e(WI,Dwo),e(_b,qwo),e(D,Gwo),e(D,ub),e(ub,lae),e(lae,Owo),e(ub,Xwo),e(ub,QI),e(QI,zwo),e(ub,Vwo),e(D,Wwo),e(D,bb),e(bb,iae),e(iae,Qwo),e(bb,Hwo),e(bb,HI),e(HI,Uwo),e(bb,Jwo),e(D,Ywo),e(D,vb),e(vb,dae),e(dae,Kwo),e(vb,Zwo),e(vb,UI),e(UI,eAo),e(vb,oAo),e(D,rAo),e(D,Tb),e(Tb,cae),e(cae,tAo),e(Tb,aAo),e(Tb,JI),e(JI,nAo),e(Tb,sAo),e(D,lAo),e(D,Fb),e(Fb,fae),e(fae,iAo),e(Fb,dAo),e(Fb,YI),e(YI,cAo),e(Fb,fAo),e(D,mAo),e(D,Cb),e(Cb,mae),e(mae,gAo),e(Cb,hAo),e(Cb,KI),e(KI,pAo),e(Cb,_Ao),e(D,uAo),e(D,Mb),e(Mb,gae),e(gae,bAo),e(Mb,vAo),e(Mb,ZI),e(ZI,TAo),e(Mb,FAo),e(D,CAo),e(D,Eb),e(Eb,hae),e(hae,MAo),e(Eb,EAo),e(Eb,ej),e(ej,yAo),e(Eb,wAo),e(D,AAo),e(D,yb),e(yb,pae),e(pae,LAo),e(yb,BAo),e(yb,oj),e(oj,kAo),e(yb,xAo),e(D,RAo),e(D,wb),e(wb,_ae),e(_ae,SAo),e(wb,PAo),e(wb,rj),e(rj,$Ao),e(wb,IAo),e(D,jAo),e(D,Ab),e(Ab,uae),e(uae,NAo),e(Ab,DAo),e(Ab,tj),e(tj,qAo),e(Ab,GAo),e(Ne,OAo),e(Ne,Lb),e(Lb,XAo),e(Lb,bae),e(bae,zAo),e(Lb,VAo),e(Lb,vae),e(vae,WAo),e(Ne,QAo),e(Ne,Tae),e(Tae,HAo),e(Ne,UAo),g(XE,Ne,null),b(d,PLe,u),b(d,gd,u),e(gd,Bb),e(Bb,Fae),g(zE,Fae,null),e(gd,JAo),e(gd,Cae),e(Cae,YAo),b(d,$Le,u),b(d,er,u),g(VE,er,null),e(er,KAo),e(er,hd),e(hd,ZAo),e(hd,Mae),e(Mae,e6o),e(hd,o6o),e(hd,Eae),e(Eae,r6o),e(hd,t6o),e(er,a6o),e(er,WE),e(WE,n6o),e(WE,yae),e(yae,s6o),e(WE,l6o),e(er,i6o),e(er,Qr),g(QE,Qr,null),e(Qr,d6o),e(Qr,wae),e(wae,c6o),e(Qr,f6o),e(Qr,pd),e(pd,m6o),e(pd,Aae),e(Aae,g6o),e(pd,h6o),e(pd,Lae),e(Lae,p6o),e(pd,_6o),e(Qr,u6o),e(Qr,Bae),e(Bae,b6o),e(Qr,v6o),g(HE,Qr,null),e(er,T6o),e(er,De),g(UE,De,null),e(De,F6o),e(De,kae),e(kae,C6o),e(De,M6o),e(De,Ha),e(Ha,E6o),e(Ha,xae),e(xae,y6o),e(Ha,w6o),e(Ha,Rae),e(Rae,A6o),e(Ha,L6o),e(Ha,Sae),e(Sae,B6o),e(Ha,k6o),e(De,x6o),e(De,R),e(R,kb),e(kb,Pae),e(Pae,R6o),e(kb,S6o),e(kb,aj),e(aj,P6o),e(kb,$6o),e(R,I6o),e(R,xb),e(xb,$ae),e($ae,j6o),e(xb,N6o),e(xb,nj),e(nj,D6o),e(xb,q6o),e(R,G6o),e(R,Rb),e(Rb,Iae),e(Iae,O6o),e(Rb,X6o),e(Rb,sj),e(sj,z6o),e(Rb,V6o),e(R,W6o),e(R,Sb),e(Sb,jae),e(jae,Q6o),e(Sb,H6o),e(Sb,lj),e(lj,U6o),e(Sb,J6o),e(R,Y6o),e(R,Pb),e(Pb,Nae),e(Nae,K6o),e(Pb,Z6o),e(Pb,ij),e(ij,e0o),e(Pb,o0o),e(R,r0o),e(R,$b),e($b,Dae),e(Dae,t0o),e($b,a0o),e($b,dj),e(dj,n0o),e($b,s0o),e(R,l0o),e(R,Ib),e(Ib,qae),e(qae,i0o),e(Ib,d0o),e(Ib,cj),e(cj,c0o),e(Ib,f0o),e(R,m0o),e(R,jb),e(jb,Gae),e(Gae,g0o),e(jb,h0o),e(jb,fj),e(fj,p0o),e(jb,_0o),e(R,u0o),e(R,Nb),e(Nb,Oae),e(Oae,b0o),e(Nb,v0o),e(Nb,mj),e(mj,T0o),e(Nb,F0o),e(R,C0o),e(R,Db),e(Db,Xae),e(Xae,M0o),e(Db,E0o),e(Db,gj),e(gj,y0o),e(Db,w0o),e(R,A0o),e(R,qb),e(qb,zae),e(zae,L0o),e(qb,B0o),e(qb,hj),e(hj,k0o),e(qb,x0o),e(R,R0o),e(R,Gb),e(Gb,Vae),e(Vae,S0o),e(Gb,P0o),e(Gb,pj),e(pj,$0o),e(Gb,I0o),e(R,j0o),e(R,Ob),e(Ob,Wae),e(Wae,N0o),e(Ob,D0o),e(Ob,_j),e(_j,q0o),e(Ob,G0o),e(R,O0o),e(R,Xb),e(Xb,Qae),e(Qae,X0o),e(Xb,z0o),e(Xb,uj),e(uj,V0o),e(Xb,W0o),e(R,Q0o),e(R,zb),e(zb,Hae),e(Hae,H0o),e(zb,U0o),e(zb,bj),e(bj,J0o),e(zb,Y0o),e(R,K0o),e(R,Vb),e(Vb,Uae),e(Uae,Z0o),e(Vb,eLo),e(Vb,vj),e(vj,oLo),e(Vb,rLo),e(R,tLo),e(R,Wb),e(Wb,Jae),e(Jae,aLo),e(Wb,nLo),e(Wb,Tj),e(Tj,sLo),e(Wb,lLo),e(R,iLo),e(R,Qb),e(Qb,Yae),e(Yae,dLo),e(Qb,cLo),e(Qb,Fj),e(Fj,fLo),e(Qb,mLo),e(R,gLo),e(R,Hb),e(Hb,Kae),e(Kae,hLo),e(Hb,pLo),e(Hb,Cj),e(Cj,_Lo),e(Hb,uLo),e(R,bLo),e(R,Ub),e(Ub,Zae),e(Zae,vLo),e(Ub,TLo),e(Ub,Mj),e(Mj,FLo),e(Ub,CLo),e(R,MLo),e(R,Jb),e(Jb,ene),e(ene,ELo),e(Jb,yLo),e(Jb,Ej),e(Ej,wLo),e(Jb,ALo),e(R,LLo),e(R,Yb),e(Yb,one),e(one,BLo),e(Yb,kLo),e(Yb,yj),e(yj,xLo),e(Yb,RLo),e(R,SLo),e(R,Kb),e(Kb,rne),e(rne,PLo),e(Kb,$Lo),e(Kb,wj),e(wj,ILo),e(Kb,jLo),e(R,NLo),e(R,Zb),e(Zb,tne),e(tne,DLo),e(Zb,qLo),e(Zb,Aj),e(Aj,GLo),e(Zb,OLo),e(R,XLo),e(R,e5),e(e5,ane),e(ane,zLo),e(e5,VLo),e(e5,Lj),e(Lj,WLo),e(e5,QLo),e(R,HLo),e(R,o5),e(o5,nne),e(nne,ULo),e(o5,JLo),e(o5,Bj),e(Bj,YLo),e(o5,KLo),e(R,ZLo),e(R,r5),e(r5,sne),e(sne,e8o),e(r5,o8o),e(r5,kj),e(kj,r8o),e(r5,t8o),e(R,a8o),e(R,t5),e(t5,lne),e(lne,n8o),e(t5,s8o),e(t5,xj),e(xj,l8o),e(t5,i8o),e(R,d8o),e(R,a5),e(a5,ine),e(ine,c8o),e(a5,f8o),e(a5,Rj),e(Rj,m8o),e(a5,g8o),e(R,h8o),e(R,n5),e(n5,dne),e(dne,p8o),e(n5,_8o),e(n5,Sj),e(Sj,u8o),e(n5,b8o),e(R,v8o),e(R,s5),e(s5,cne),e(cne,T8o),e(s5,F8o),e(s5,Pj),e(Pj,C8o),e(s5,M8o),e(R,E8o),e(R,l5),e(l5,fne),e(fne,y8o),e(l5,w8o),e(l5,$j),e($j,A8o),e(l5,L8o),e(R,B8o),e(R,i5),e(i5,mne),e(mne,k8o),e(i5,x8o),e(i5,Ij),e(Ij,R8o),e(i5,S8o),e(R,P8o),e(R,d5),e(d5,gne),e(gne,$8o),e(d5,I8o),e(d5,jj),e(jj,j8o),e(d5,N8o),e(R,D8o),e(R,c5),e(c5,hne),e(hne,q8o),e(c5,G8o),e(c5,Nj),e(Nj,O8o),e(c5,X8o),e(R,z8o),e(R,f5),e(f5,pne),e(pne,V8o),e(f5,W8o),e(f5,Dj),e(Dj,Q8o),e(f5,H8o),e(R,U8o),e(R,m5),e(m5,_ne),e(_ne,J8o),e(m5,Y8o),e(m5,qj),e(qj,K8o),e(m5,Z8o),e(R,eBo),e(R,g5),e(g5,une),e(une,oBo),e(g5,rBo),e(g5,Gj),e(Gj,tBo),e(g5,aBo),e(De,nBo),e(De,h5),e(h5,sBo),e(h5,bne),e(bne,lBo),e(h5,iBo),e(h5,vne),e(vne,dBo),e(De,cBo),e(De,Tne),e(Tne,fBo),e(De,mBo),g(JE,De,null),b(d,ILe,u),b(d,_d,u),e(_d,p5),e(p5,Fne),g(YE,Fne,null),e(_d,gBo),e(_d,Cne),e(Cne,hBo),b(d,jLe,u),b(d,or,u),g(KE,or,null),e(or,pBo),e(or,ud),e(ud,_Bo),e(ud,Mne),e(Mne,uBo),e(ud,bBo),e(ud,Ene),e(Ene,vBo),e(ud,TBo),e(or,FBo),e(or,ZE),e(ZE,CBo),e(ZE,yne),e(yne,MBo),e(ZE,EBo),e(or,yBo),e(or,Hr),g(e3,Hr,null),e(Hr,wBo),e(Hr,wne),e(wne,ABo),e(Hr,LBo),e(Hr,bd),e(bd,BBo),e(bd,Ane),e(Ane,kBo),e(bd,xBo),e(bd,Lne),e(Lne,RBo),e(bd,SBo),e(Hr,PBo),e(Hr,Bne),e(Bne,$Bo),e(Hr,IBo),g(o3,Hr,null),e(or,jBo),e(or,qe),g(r3,qe,null),e(qe,NBo),e(qe,kne),e(kne,DBo),e(qe,qBo),e(qe,Ua),e(Ua,GBo),e(Ua,xne),e(xne,OBo),e(Ua,XBo),e(Ua,Rne),e(Rne,zBo),e(Ua,VBo),e(Ua,Sne),e(Sne,WBo),e(Ua,QBo),e(qe,HBo),e(qe,Pne),e(Pne,_5),e(_5,$ne),e($ne,UBo),e(_5,JBo),e(_5,Oj),e(Oj,YBo),e(_5,KBo),e(qe,ZBo),e(qe,u5),e(u5,eko),e(u5,Ine),e(Ine,oko),e(u5,rko),e(u5,jne),e(jne,tko),e(qe,ako),e(qe,Nne),e(Nne,nko),e(qe,sko),g(t3,qe,null),b(d,NLe,u),b(d,vd,u),e(vd,b5),e(b5,Dne),g(a3,Dne,null),e(vd,lko),e(vd,qne),e(qne,iko),b(d,DLe,u),b(d,rr,u),g(n3,rr,null),e(rr,dko),e(rr,Td),e(Td,cko),e(Td,Gne),e(Gne,fko),e(Td,mko),e(Td,One),e(One,gko),e(Td,hko),e(rr,pko),e(rr,s3),e(s3,_ko),e(s3,Xne),e(Xne,uko),e(s3,bko),e(rr,vko),e(rr,Ur),g(l3,Ur,null),e(Ur,Tko),e(Ur,zne),e(zne,Fko),e(Ur,Cko),e(Ur,Fd),e(Fd,Mko),e(Fd,Vne),e(Vne,Eko),e(Fd,yko),e(Fd,Wne),e(Wne,wko),e(Fd,Ako),e(Ur,Lko),e(Ur,Qne),e(Qne,Bko),e(Ur,kko),g(i3,Ur,null),e(rr,xko),e(rr,Ge),g(d3,Ge,null),e(Ge,Rko),e(Ge,Hne),e(Hne,Sko),e(Ge,Pko),e(Ge,Ja),e(Ja,$ko),e(Ja,Une),e(Une,Iko),e(Ja,jko),e(Ja,Jne),e(Jne,Nko),e(Ja,Dko),e(Ja,Yne),e(Yne,qko),e(Ja,Gko),e(Ge,Oko),e(Ge,be),e(be,v5),e(v5,Kne),e(Kne,Xko),e(v5,zko),e(v5,Xj),e(Xj,Vko),e(v5,Wko),e(be,Qko),e(be,T5),e(T5,Zne),e(Zne,Hko),e(T5,Uko),e(T5,zj),e(zj,Jko),e(T5,Yko),e(be,Kko),e(be,Rs),e(Rs,ese),e(ese,Zko),e(Rs,exo),e(Rs,Vj),e(Vj,oxo),e(Rs,rxo),e(Rs,Wj),e(Wj,txo),e(Rs,axo),e(be,nxo),e(be,F5),e(F5,ose),e(ose,sxo),e(F5,lxo),e(F5,Qj),e(Qj,ixo),e(F5,dxo),e(be,cxo),e(be,la),e(la,rse),e(rse,fxo),e(la,mxo),e(la,Hj),e(Hj,gxo),e(la,hxo),e(la,Uj),e(Uj,pxo),e(la,_xo),e(la,Jj),e(Jj,uxo),e(la,bxo),e(be,vxo),e(be,C5),e(C5,tse),e(tse,Txo),e(C5,Fxo),e(C5,Yj),e(Yj,Cxo),e(C5,Mxo),e(be,Exo),e(be,M5),e(M5,ase),e(ase,yxo),e(M5,wxo),e(M5,Kj),e(Kj,Axo),e(M5,Lxo),e(be,Bxo),e(be,E5),e(E5,nse),e(nse,kxo),e(E5,xxo),e(E5,Zj),e(Zj,Rxo),e(E5,Sxo),e(be,Pxo),e(be,y5),e(y5,sse),e(sse,$xo),e(y5,Ixo),e(y5,eN),e(eN,jxo),e(y5,Nxo),e(Ge,Dxo),e(Ge,w5),e(w5,qxo),e(w5,lse),e(lse,Gxo),e(w5,Oxo),e(w5,ise),e(ise,Xxo),e(Ge,zxo),e(Ge,dse),e(dse,Vxo),e(Ge,Wxo),g(c3,Ge,null),b(d,qLe,u),b(d,Cd,u),e(Cd,A5),e(A5,cse),g(f3,cse,null),e(Cd,Qxo),e(Cd,fse),e(fse,Hxo),b(d,GLe,u),b(d,tr,u),g(m3,tr,null),e(tr,Uxo),e(tr,Md),e(Md,Jxo),e(Md,mse),e(mse,Yxo),e(Md,Kxo),e(Md,gse),e(gse,Zxo),e(Md,eRo),e(tr,oRo),e(tr,g3),e(g3,rRo),e(g3,hse),e(hse,tRo),e(g3,aRo),e(tr,nRo),e(tr,Jr),g(h3,Jr,null),e(Jr,sRo),e(Jr,pse),e(pse,lRo),e(Jr,iRo),e(Jr,Ed),e(Ed,dRo),e(Ed,_se),e(_se,cRo),e(Ed,fRo),e(Ed,use),e(use,mRo),e(Ed,gRo),e(Jr,hRo),e(Jr,bse),e(bse,pRo),e(Jr,_Ro),g(p3,Jr,null),e(tr,uRo),e(tr,Oe),g(_3,Oe,null),e(Oe,bRo),e(Oe,vse),e(vse,vRo),e(Oe,TRo),e(Oe,Ya),e(Ya,FRo),e(Ya,Tse),e(Tse,CRo),e(Ya,MRo),e(Ya,Fse),e(Fse,ERo),e(Ya,yRo),e(Ya,Cse),e(Cse,wRo),e(Ya,ARo),e(Oe,LRo),e(Oe,Mse),e(Mse,L5),e(L5,Ese),e(Ese,BRo),e(L5,kRo),e(L5,oN),e(oN,xRo),e(L5,RRo),e(Oe,SRo),e(Oe,B5),e(B5,PRo),e(B5,yse),e(yse,$Ro),e(B5,IRo),e(B5,wse),e(wse,jRo),e(Oe,NRo),e(Oe,Ase),e(Ase,DRo),e(Oe,qRo),g(u3,Oe,null),b(d,OLe,u),b(d,yd,u),e(yd,k5),e(k5,Lse),g(b3,Lse,null),e(yd,GRo),e(yd,Bse),e(Bse,ORo),b(d,XLe,u),b(d,ar,u),g(v3,ar,null),e(ar,XRo),e(ar,wd),e(wd,zRo),e(wd,kse),e(kse,VRo),e(wd,WRo),e(wd,xse),e(xse,QRo),e(wd,HRo),e(ar,URo),e(ar,T3),e(T3,JRo),e(T3,Rse),e(Rse,YRo),e(T3,KRo),e(ar,ZRo),e(ar,Yr),g(F3,Yr,null),e(Yr,eSo),e(Yr,Sse),e(Sse,oSo),e(Yr,rSo),e(Yr,Ad),e(Ad,tSo),e(Ad,Pse),e(Pse,aSo),e(Ad,nSo),e(Ad,$se),e($se,sSo),e(Ad,lSo),e(Yr,iSo),e(Yr,Ise),e(Ise,dSo),e(Yr,cSo),g(C3,Yr,null),e(ar,fSo),e(ar,Xe),g(M3,Xe,null),e(Xe,mSo),e(Xe,jse),e(jse,gSo),e(Xe,hSo),e(Xe,Ka),e(Ka,pSo),e(Ka,Nse),e(Nse,_So),e(Ka,uSo),e(Ka,Dse),e(Dse,bSo),e(Ka,vSo),e(Ka,qse),e(qse,TSo),e(Ka,FSo),e(Xe,CSo),e(Xe,ao),e(ao,x5),e(x5,Gse),e(Gse,MSo),e(x5,ESo),e(x5,rN),e(rN,ySo),e(x5,wSo),e(ao,ASo),e(ao,R5),e(R5,Ose),e(Ose,LSo),e(R5,BSo),e(R5,tN),e(tN,kSo),e(R5,xSo),e(ao,RSo),e(ao,S5),e(S5,Xse),e(Xse,SSo),e(S5,PSo),e(S5,aN),e(aN,$So),e(S5,ISo),e(ao,jSo),e(ao,P5),e(P5,zse),e(zse,NSo),e(P5,DSo),e(P5,nN),e(nN,qSo),e(P5,GSo),e(ao,OSo),e(ao,$5),e($5,Vse),e(Vse,XSo),e($5,zSo),e($5,sN),e(sN,VSo),e($5,WSo),e(ao,QSo),e(ao,I5),e(I5,Wse),e(Wse,HSo),e(I5,USo),e(I5,lN),e(lN,JSo),e(I5,YSo),e(ao,KSo),e(ao,j5),e(j5,Qse),e(Qse,ZSo),e(j5,ePo),e(j5,iN),e(iN,oPo),e(j5,rPo),e(Xe,tPo),e(Xe,N5),e(N5,aPo),e(N5,Hse),e(Hse,nPo),e(N5,sPo),e(N5,Use),e(Use,lPo),e(Xe,iPo),e(Xe,Jse),e(Jse,dPo),e(Xe,cPo),g(E3,Xe,null),b(d,zLe,u),b(d,Ld,u),e(Ld,D5),e(D5,Yse),g(y3,Yse,null),e(Ld,fPo),e(Ld,Kse),e(Kse,mPo),b(d,VLe,u),b(d,nr,u),g(w3,nr,null),e(nr,gPo),e(nr,Bd),e(Bd,hPo),e(Bd,Zse),e(Zse,pPo),e(Bd,_Po),e(Bd,ele),e(ele,uPo),e(Bd,bPo),e(nr,vPo),e(nr,A3),e(A3,TPo),e(A3,ole),e(ole,FPo),e(A3,CPo),e(nr,MPo),e(nr,Kr),g(L3,Kr,null),e(Kr,EPo),e(Kr,rle),e(rle,yPo),e(Kr,wPo),e(Kr,kd),e(kd,APo),e(kd,tle),e(tle,LPo),e(kd,BPo),e(kd,ale),e(ale,kPo),e(kd,xPo),e(Kr,RPo),e(Kr,nle),e(nle,SPo),e(Kr,PPo),g(B3,Kr,null),e(nr,$Po),e(nr,ze),g(k3,ze,null),e(ze,IPo),e(ze,sle),e(sle,jPo),e(ze,NPo),e(ze,Za),e(Za,DPo),e(Za,lle),e(lle,qPo),e(Za,GPo),e(Za,ile),e(ile,OPo),e(Za,XPo),e(Za,dle),e(dle,zPo),e(Za,VPo),e(ze,WPo),e(ze,xd),e(xd,q5),e(q5,cle),e(cle,QPo),e(q5,HPo),e(q5,dN),e(dN,UPo),e(q5,JPo),e(xd,YPo),e(xd,G5),e(G5,fle),e(fle,KPo),e(G5,ZPo),e(G5,cN),e(cN,e$o),e(G5,o$o),e(xd,r$o),e(xd,O5),e(O5,mle),e(mle,t$o),e(O5,a$o),e(O5,fN),e(fN,n$o),e(O5,s$o),e(ze,l$o),e(ze,X5),e(X5,i$o),e(X5,gle),e(gle,d$o),e(X5,c$o),e(X5,hle),e(hle,f$o),e(ze,m$o),e(ze,ple),e(ple,g$o),e(ze,h$o),g(x3,ze,null),b(d,WLe,u),b(d,Rd,u),e(Rd,z5),e(z5,_le),g(R3,_le,null),e(Rd,p$o),e(Rd,ule),e(ule,_$o),b(d,QLe,u),b(d,sr,u),g(S3,sr,null),e(sr,u$o),e(sr,Sd),e(Sd,b$o),e(Sd,ble),e(ble,v$o),e(Sd,T$o),e(Sd,vle),e(vle,F$o),e(Sd,C$o),e(sr,M$o),e(sr,P3),e(P3,E$o),e(P3,Tle),e(Tle,y$o),e(P3,w$o),e(sr,A$o),e(sr,Zr),g($3,Zr,null),e(Zr,L$o),e(Zr,Fle),e(Fle,B$o),e(Zr,k$o),e(Zr,Pd),e(Pd,x$o),e(Pd,Cle),e(Cle,R$o),e(Pd,S$o),e(Pd,Mle),e(Mle,P$o),e(Pd,$$o),e(Zr,I$o),e(Zr,Ele),e(Ele,j$o),e(Zr,N$o),g(I3,Zr,null),e(sr,D$o),e(sr,Ve),g(j3,Ve,null),e(Ve,q$o),e(Ve,yle),e(yle,G$o),e(Ve,O$o),e(Ve,en),e(en,X$o),e(en,wle),e(wle,z$o),e(en,V$o),e(en,Ale),e(Ale,W$o),e(en,Q$o),e(en,Lle),e(Lle,H$o),e(en,U$o),e(Ve,J$o),e(Ve,no),e(no,V5),e(V5,Ble),e(Ble,Y$o),e(V5,K$o),e(V5,mN),e(mN,Z$o),e(V5,eIo),e(no,oIo),e(no,W5),e(W5,kle),e(kle,rIo),e(W5,tIo),e(W5,gN),e(gN,aIo),e(W5,nIo),e(no,sIo),e(no,Q5),e(Q5,xle),e(xle,lIo),e(Q5,iIo),e(Q5,hN),e(hN,dIo),e(Q5,cIo),e(no,fIo),e(no,H5),e(H5,Rle),e(Rle,mIo),e(H5,gIo),e(H5,pN),e(pN,hIo),e(H5,pIo),e(no,_Io),e(no,U5),e(U5,Sle),e(Sle,uIo),e(U5,bIo),e(U5,_N),e(_N,vIo),e(U5,TIo),e(no,FIo),e(no,J5),e(J5,Ple),e(Ple,CIo),e(J5,MIo),e(J5,uN),e(uN,EIo),e(J5,yIo),e(no,wIo),e(no,Y5),e(Y5,$le),e($le,AIo),e(Y5,LIo),e(Y5,bN),e(bN,BIo),e(Y5,kIo),e(Ve,xIo),e(Ve,K5),e(K5,RIo),e(K5,Ile),e(Ile,SIo),e(K5,PIo),e(K5,jle),e(jle,$Io),e(Ve,IIo),e(Ve,Nle),e(Nle,jIo),e(Ve,NIo),g(N3,Ve,null),b(d,HLe,u),b(d,$d,u),e($d,Z5),e(Z5,Dle),g(D3,Dle,null),e($d,DIo),e($d,qle),e(qle,qIo),b(d,ULe,u),b(d,lr,u),g(q3,lr,null),e(lr,GIo),e(lr,Id),e(Id,OIo),e(Id,Gle),e(Gle,XIo),e(Id,zIo),e(Id,Ole),e(Ole,VIo),e(Id,WIo),e(lr,QIo),e(lr,G3),e(G3,HIo),e(G3,Xle),e(Xle,UIo),e(G3,JIo),e(lr,YIo),e(lr,et),g(O3,et,null),e(et,KIo),e(et,zle),e(zle,ZIo),e(et,ejo),e(et,jd),e(jd,ojo),e(jd,Vle),e(Vle,rjo),e(jd,tjo),e(jd,Wle),e(Wle,ajo),e(jd,njo),e(et,sjo),e(et,Qle),e(Qle,ljo),e(et,ijo),g(X3,et,null),e(lr,djo),e(lr,We),g(z3,We,null),e(We,cjo),e(We,Hle),e(Hle,fjo),e(We,mjo),e(We,on),e(on,gjo),e(on,Ule),e(Ule,hjo),e(on,pjo),e(on,Jle),e(Jle,_jo),e(on,ujo),e(on,Yle),e(Yle,bjo),e(on,vjo),e(We,Tjo),e(We,V3),e(V3,ev),e(ev,Kle),e(Kle,Fjo),e(ev,Cjo),e(ev,vN),e(vN,Mjo),e(ev,Ejo),e(V3,yjo),e(V3,ov),e(ov,Zle),e(Zle,wjo),e(ov,Ajo),e(ov,TN),e(TN,Ljo),e(ov,Bjo),e(We,kjo),e(We,rv),e(rv,xjo),e(rv,eie),e(eie,Rjo),e(rv,Sjo),e(rv,oie),e(oie,Pjo),e(We,$jo),e(We,rie),e(rie,Ijo),e(We,jjo),g(W3,We,null),b(d,JLe,u),b(d,Nd,u),e(Nd,tv),e(tv,tie),g(Q3,tie,null),e(Nd,Njo),e(Nd,aie),e(aie,Djo),b(d,YLe,u),b(d,ir,u),g(H3,ir,null),e(ir,qjo),e(ir,Dd),e(Dd,Gjo),e(Dd,nie),e(nie,Ojo),e(Dd,Xjo),e(Dd,sie),e(sie,zjo),e(Dd,Vjo),e(ir,Wjo),e(ir,U3),e(U3,Qjo),e(U3,lie),e(lie,Hjo),e(U3,Ujo),e(ir,Jjo),e(ir,ot),g(J3,ot,null),e(ot,Yjo),e(ot,iie),e(iie,Kjo),e(ot,Zjo),e(ot,qd),e(qd,eNo),e(qd,die),e(die,oNo),e(qd,rNo),e(qd,cie),e(cie,tNo),e(qd,aNo),e(ot,nNo),e(ot,fie),e(fie,sNo),e(ot,lNo),g(Y3,ot,null),e(ir,iNo),e(ir,Qe),g(K3,Qe,null),e(Qe,dNo),e(Qe,mie),e(mie,cNo),e(Qe,fNo),e(Qe,rn),e(rn,mNo),e(rn,gie),e(gie,gNo),e(rn,hNo),e(rn,hie),e(hie,pNo),e(rn,_No),e(rn,pie),e(pie,uNo),e(rn,bNo),e(Qe,vNo),e(Qe,Gd),e(Gd,av),e(av,_ie),e(_ie,TNo),e(av,FNo),e(av,FN),e(FN,CNo),e(av,MNo),e(Gd,ENo),e(Gd,nv),e(nv,uie),e(uie,yNo),e(nv,wNo),e(nv,CN),e(CN,ANo),e(nv,LNo),e(Gd,BNo),e(Gd,sv),e(sv,bie),e(bie,kNo),e(sv,xNo),e(sv,MN),e(MN,RNo),e(sv,SNo),e(Qe,PNo),e(Qe,lv),e(lv,$No),e(lv,vie),e(vie,INo),e(lv,jNo),e(lv,Tie),e(Tie,NNo),e(Qe,DNo),e(Qe,Fie),e(Fie,qNo),e(Qe,GNo),g(Z3,Qe,null),b(d,KLe,u),b(d,Od,u),e(Od,iv),e(iv,Cie),g(ey,Cie,null),e(Od,ONo),e(Od,Mie),e(Mie,XNo),b(d,ZLe,u),b(d,dr,u),g(oy,dr,null),e(dr,zNo),e(dr,Xd),e(Xd,VNo),e(Xd,Eie),e(Eie,WNo),e(Xd,QNo),e(Xd,yie),e(yie,HNo),e(Xd,UNo),e(dr,JNo),e(dr,ry),e(ry,YNo),e(ry,wie),e(wie,KNo),e(ry,ZNo),e(dr,eDo),e(dr,rt),g(ty,rt,null),e(rt,oDo),e(rt,Aie),e(Aie,rDo),e(rt,tDo),e(rt,zd),e(zd,aDo),e(zd,Lie),e(Lie,nDo),e(zd,sDo),e(zd,Bie),e(Bie,lDo),e(zd,iDo),e(rt,dDo),e(rt,kie),e(kie,cDo),e(rt,fDo),g(ay,rt,null),e(dr,mDo),e(dr,He),g(ny,He,null),e(He,gDo),e(He,xie),e(xie,hDo),e(He,pDo),e(He,tn),e(tn,_Do),e(tn,Rie),e(Rie,uDo),e(tn,bDo),e(tn,Sie),e(Sie,vDo),e(tn,TDo),e(tn,Pie),e(Pie,FDo),e(tn,CDo),e(He,MDo),e(He,Vd),e(Vd,dv),e(dv,$ie),e($ie,EDo),e(dv,yDo),e(dv,EN),e(EN,wDo),e(dv,ADo),e(Vd,LDo),e(Vd,cv),e(cv,Iie),e(Iie,BDo),e(cv,kDo),e(cv,yN),e(yN,xDo),e(cv,RDo),e(Vd,SDo),e(Vd,fv),e(fv,jie),e(jie,PDo),e(fv,$Do),e(fv,wN),e(wN,IDo),e(fv,jDo),e(He,NDo),e(He,mv),e(mv,DDo),e(mv,Nie),e(Nie,qDo),e(mv,GDo),e(mv,Die),e(Die,ODo),e(He,XDo),e(He,qie),e(qie,zDo),e(He,VDo),g(sy,He,null),b(d,e8e,u),b(d,Wd,u),e(Wd,gv),e(gv,Gie),g(ly,Gie,null),e(Wd,WDo),e(Wd,Oie),e(Oie,QDo),b(d,o8e,u),b(d,cr,u),g(iy,cr,null),e(cr,HDo),e(cr,Qd),e(Qd,UDo),e(Qd,Xie),e(Xie,JDo),e(Qd,YDo),e(Qd,zie),e(zie,KDo),e(Qd,ZDo),e(cr,eqo),e(cr,dy),e(dy,oqo),e(dy,Vie),e(Vie,rqo),e(dy,tqo),e(cr,aqo),e(cr,tt),g(cy,tt,null),e(tt,nqo),e(tt,Wie),e(Wie,sqo),e(tt,lqo),e(tt,Hd),e(Hd,iqo),e(Hd,Qie),e(Qie,dqo),e(Hd,cqo),e(Hd,Hie),e(Hie,fqo),e(Hd,mqo),e(tt,gqo),e(tt,Uie),e(Uie,hqo),e(tt,pqo),g(fy,tt,null),e(cr,_qo),e(cr,Ue),g(my,Ue,null),e(Ue,uqo),e(Ue,Jie),e(Jie,bqo),e(Ue,vqo),e(Ue,an),e(an,Tqo),e(an,Yie),e(Yie,Fqo),e(an,Cqo),e(an,Kie),e(Kie,Mqo),e(an,Eqo),e(an,Zie),e(Zie,yqo),e(an,wqo),e(Ue,Aqo),e(Ue,ede),e(ede,hv),e(hv,ode),e(ode,Lqo),e(hv,Bqo),e(hv,AN),e(AN,kqo),e(hv,xqo),e(Ue,Rqo),e(Ue,pv),e(pv,Sqo),e(pv,rde),e(rde,Pqo),e(pv,$qo),e(pv,tde),e(tde,Iqo),e(Ue,jqo),e(Ue,ade),e(ade,Nqo),e(Ue,Dqo),g(gy,Ue,null),b(d,r8e,u),b(d,Ud,u),e(Ud,_v),e(_v,nde),g(hy,nde,null),e(Ud,qqo),e(Ud,sde),e(sde,Gqo),b(d,t8e,u),b(d,fr,u),g(py,fr,null),e(fr,Oqo),e(fr,Jd),e(Jd,Xqo),e(Jd,lde),e(lde,zqo),e(Jd,Vqo),e(Jd,ide),e(ide,Wqo),e(Jd,Qqo),e(fr,Hqo),e(fr,_y),e(_y,Uqo),e(_y,dde),e(dde,Jqo),e(_y,Yqo),e(fr,Kqo),e(fr,at),g(uy,at,null),e(at,Zqo),e(at,cde),e(cde,eGo),e(at,oGo),e(at,Yd),e(Yd,rGo),e(Yd,fde),e(fde,tGo),e(Yd,aGo),e(Yd,mde),e(mde,nGo),e(Yd,sGo),e(at,lGo),e(at,gde),e(gde,iGo),e(at,dGo),g(by,at,null),e(fr,cGo),e(fr,Je),g(vy,Je,null),e(Je,fGo),e(Je,hde),e(hde,mGo),e(Je,gGo),e(Je,nn),e(nn,hGo),e(nn,pde),e(pde,pGo),e(nn,_Go),e(nn,_de),e(_de,uGo),e(nn,bGo),e(nn,ude),e(ude,vGo),e(nn,TGo),e(Je,FGo),e(Je,bde),e(bde,uv),e(uv,vde),e(vde,CGo),e(uv,MGo),e(uv,LN),e(LN,EGo),e(uv,yGo),e(Je,wGo),e(Je,bv),e(bv,AGo),e(bv,Tde),e(Tde,LGo),e(bv,BGo),e(bv,Fde),e(Fde,kGo),e(Je,xGo),e(Je,Cde),e(Cde,RGo),e(Je,SGo),g(Ty,Je,null),b(d,a8e,u),b(d,Kd,u),e(Kd,vv),e(vv,Mde),g(Fy,Mde,null),e(Kd,PGo),e(Kd,Ede),e(Ede,$Go),b(d,n8e,u),b(d,mr,u),g(Cy,mr,null),e(mr,IGo),e(mr,Zd),e(Zd,jGo),e(Zd,yde),e(yde,NGo),e(Zd,DGo),e(Zd,wde),e(wde,qGo),e(Zd,GGo),e(mr,OGo),e(mr,My),e(My,XGo),e(My,Ade),e(Ade,zGo),e(My,VGo),e(mr,WGo),e(mr,nt),g(Ey,nt,null),e(nt,QGo),e(nt,Lde),e(Lde,HGo),e(nt,UGo),e(nt,ec),e(ec,JGo),e(ec,Bde),e(Bde,YGo),e(ec,KGo),e(ec,kde),e(kde,ZGo),e(ec,eOo),e(nt,oOo),e(nt,xde),e(xde,rOo),e(nt,tOo),g(yy,nt,null),e(mr,aOo),e(mr,Ye),g(wy,Ye,null),e(Ye,nOo),e(Ye,Rde),e(Rde,sOo),e(Ye,lOo),e(Ye,sn),e(sn,iOo),e(sn,Sde),e(Sde,dOo),e(sn,cOo),e(sn,Pde),e(Pde,fOo),e(sn,mOo),e(sn,$de),e($de,gOo),e(sn,hOo),e(Ye,pOo),e(Ye,Ay),e(Ay,Tv),e(Tv,Ide),e(Ide,_Oo),e(Tv,uOo),e(Tv,BN),e(BN,bOo),e(Tv,vOo),e(Ay,TOo),e(Ay,Fv),e(Fv,jde),e(jde,FOo),e(Fv,COo),e(Fv,kN),e(kN,MOo),e(Fv,EOo),e(Ye,yOo),e(Ye,Cv),e(Cv,wOo),e(Cv,Nde),e(Nde,AOo),e(Cv,LOo),e(Cv,Dde),e(Dde,BOo),e(Ye,kOo),e(Ye,qde),e(qde,xOo),e(Ye,ROo),g(Ly,Ye,null),b(d,s8e,u),b(d,oc,u),e(oc,Mv),e(Mv,Gde),g(By,Gde,null),e(oc,SOo),e(oc,Ode),e(Ode,POo),b(d,l8e,u),b(d,gr,u),g(ky,gr,null),e(gr,$Oo),e(gr,rc),e(rc,IOo),e(rc,Xde),e(Xde,jOo),e(rc,NOo),e(rc,zde),e(zde,DOo),e(rc,qOo),e(gr,GOo),e(gr,xy),e(xy,OOo),e(xy,Vde),e(Vde,XOo),e(xy,zOo),e(gr,VOo),e(gr,st),g(Ry,st,null),e(st,WOo),e(st,Wde),e(Wde,QOo),e(st,HOo),e(st,tc),e(tc,UOo),e(tc,Qde),e(Qde,JOo),e(tc,YOo),e(tc,Hde),e(Hde,KOo),e(tc,ZOo),e(st,eXo),e(st,Ude),e(Ude,oXo),e(st,rXo),g(Sy,st,null),e(gr,tXo),e(gr,go),g(Py,go,null),e(go,aXo),e(go,Jde),e(Jde,nXo),e(go,sXo),e(go,ln),e(ln,lXo),e(ln,Yde),e(Yde,iXo),e(ln,dXo),e(ln,Kde),e(Kde,cXo),e(ln,fXo),e(ln,Zde),e(Zde,mXo),e(ln,gXo),e(go,hXo),e(go,B),e(B,Ev),e(Ev,ece),e(ece,pXo),e(Ev,_Xo),e(Ev,xN),e(xN,uXo),e(Ev,bXo),e(B,vXo),e(B,yv),e(yv,oce),e(oce,TXo),e(yv,FXo),e(yv,RN),e(RN,CXo),e(yv,MXo),e(B,EXo),e(B,wv),e(wv,rce),e(rce,yXo),e(wv,wXo),e(wv,SN),e(SN,AXo),e(wv,LXo),e(B,BXo),e(B,Av),e(Av,tce),e(tce,kXo),e(Av,xXo),e(Av,PN),e(PN,RXo),e(Av,SXo),e(B,PXo),e(B,Lv),e(Lv,ace),e(ace,$Xo),e(Lv,IXo),e(Lv,$N),e($N,jXo),e(Lv,NXo),e(B,DXo),e(B,Bv),e(Bv,nce),e(nce,qXo),e(Bv,GXo),e(Bv,IN),e(IN,OXo),e(Bv,XXo),e(B,zXo),e(B,kv),e(kv,sce),e(sce,VXo),e(kv,WXo),e(kv,jN),e(jN,QXo),e(kv,HXo),e(B,UXo),e(B,xv),e(xv,lce),e(lce,JXo),e(xv,YXo),e(xv,NN),e(NN,KXo),e(xv,ZXo),e(B,ezo),e(B,Rv),e(Rv,ice),e(ice,ozo),e(Rv,rzo),e(Rv,DN),e(DN,tzo),e(Rv,azo),e(B,nzo),e(B,Sv),e(Sv,dce),e(dce,szo),e(Sv,lzo),e(Sv,qN),e(qN,izo),e(Sv,dzo),e(B,czo),e(B,Pv),e(Pv,cce),e(cce,fzo),e(Pv,mzo),e(Pv,GN),e(GN,gzo),e(Pv,hzo),e(B,pzo),e(B,$v),e($v,fce),e(fce,_zo),e($v,uzo),e($v,ON),e(ON,bzo),e($v,vzo),e(B,Tzo),e(B,Iv),e(Iv,mce),e(mce,Fzo),e(Iv,Czo),e(Iv,XN),e(XN,Mzo),e(Iv,Ezo),e(B,yzo),e(B,jv),e(jv,gce),e(gce,wzo),e(jv,Azo),e(jv,zN),e(zN,Lzo),e(jv,Bzo),e(B,kzo),e(B,Nv),e(Nv,hce),e(hce,xzo),e(Nv,Rzo),e(Nv,VN),e(VN,Szo),e(Nv,Pzo),e(B,$zo),e(B,Ss),e(Ss,pce),e(pce,Izo),e(Ss,jzo),e(Ss,WN),e(WN,Nzo),e(Ss,Dzo),e(Ss,QN),e(QN,qzo),e(Ss,Gzo),e(B,Ozo),e(B,Dv),e(Dv,_ce),e(_ce,Xzo),e(Dv,zzo),e(Dv,HN),e(HN,Vzo),e(Dv,Wzo),e(B,Qzo),e(B,qv),e(qv,uce),e(uce,Hzo),e(qv,Uzo),e(qv,UN),e(UN,Jzo),e(qv,Yzo),e(B,Kzo),e(B,Gv),e(Gv,bce),e(bce,Zzo),e(Gv,eVo),e(Gv,JN),e(JN,oVo),e(Gv,rVo),e(B,tVo),e(B,Ov),e(Ov,vce),e(vce,aVo),e(Ov,nVo),e(Ov,YN),e(YN,sVo),e(Ov,lVo),e(B,iVo),e(B,Xv),e(Xv,Tce),e(Tce,dVo),e(Xv,cVo),e(Xv,KN),e(KN,fVo),e(Xv,mVo),e(B,gVo),e(B,zv),e(zv,Fce),e(Fce,hVo),e(zv,pVo),e(zv,ZN),e(ZN,_Vo),e(zv,uVo),e(B,bVo),e(B,Vv),e(Vv,Cce),e(Cce,vVo),e(Vv,TVo),e(Vv,eD),e(eD,FVo),e(Vv,CVo),e(B,MVo),e(B,Wv),e(Wv,Mce),e(Mce,EVo),e(Wv,yVo),e(Wv,oD),e(oD,wVo),e(Wv,AVo),e(B,LVo),e(B,Qv),e(Qv,Ece),e(Ece,BVo),e(Qv,kVo),e(Qv,rD),e(rD,xVo),e(Qv,RVo),e(B,SVo),e(B,Hv),e(Hv,yce),e(yce,PVo),e(Hv,$Vo),e(Hv,tD),e(tD,IVo),e(Hv,jVo),e(B,NVo),e(B,Uv),e(Uv,wce),e(wce,DVo),e(Uv,qVo),e(Uv,aD),e(aD,GVo),e(Uv,OVo),e(B,XVo),e(B,Jv),e(Jv,Ace),e(Ace,zVo),e(Jv,VVo),e(Jv,nD),e(nD,WVo),e(Jv,QVo),e(B,HVo),e(B,Yv),e(Yv,Lce),e(Lce,UVo),e(Yv,JVo),e(Yv,sD),e(sD,YVo),e(Yv,KVo),e(B,ZVo),e(B,Kv),e(Kv,Bce),e(Bce,eWo),e(Kv,oWo),e(Kv,lD),e(lD,rWo),e(Kv,tWo),e(B,aWo),e(B,Zv),e(Zv,kce),e(kce,nWo),e(Zv,sWo),e(Zv,iD),e(iD,lWo),e(Zv,iWo),e(B,dWo),e(B,eT),e(eT,xce),e(xce,cWo),e(eT,fWo),e(eT,dD),e(dD,mWo),e(eT,gWo),e(B,hWo),e(B,oT),e(oT,Rce),e(Rce,pWo),e(oT,_Wo),e(oT,cD),e(cD,uWo),e(oT,bWo),e(B,vWo),e(B,rT),e(rT,Sce),e(Sce,TWo),e(rT,FWo),e(rT,fD),e(fD,CWo),e(rT,MWo),e(B,EWo),e(B,tT),e(tT,Pce),e(Pce,yWo),e(tT,wWo),e(tT,mD),e(mD,AWo),e(tT,LWo),e(B,BWo),e(B,aT),e(aT,$ce),e($ce,kWo),e(aT,xWo),e(aT,gD),e(gD,RWo),e(aT,SWo),e(B,PWo),e(B,nT),e(nT,Ice),e(Ice,$Wo),e(nT,IWo),e(nT,hD),e(hD,jWo),e(nT,NWo),e(B,DWo),e(B,sT),e(sT,jce),e(jce,qWo),e(sT,GWo),e(sT,pD),e(pD,OWo),e(sT,XWo),e(B,zWo),e(B,lT),e(lT,Nce),e(Nce,VWo),e(lT,WWo),e(lT,_D),e(_D,QWo),e(lT,HWo),e(B,UWo),e(B,iT),e(iT,Dce),e(Dce,JWo),e(iT,YWo),e(iT,uD),e(uD,KWo),e(iT,ZWo),e(B,eQo),e(B,dT),e(dT,qce),e(qce,oQo),e(dT,rQo),e(dT,bD),e(bD,tQo),e(dT,aQo),e(go,nQo),e(go,Gce),e(Gce,sQo),e(go,lQo),g($y,go,null),b(d,i8e,u),b(d,ac,u),e(ac,cT),e(cT,Oce),g(Iy,Oce,null),e(ac,iQo),e(ac,Xce),e(Xce,dQo),b(d,d8e,u),b(d,hr,u),g(jy,hr,null),e(hr,cQo),e(hr,nc),e(nc,fQo),e(nc,zce),e(zce,mQo),e(nc,gQo),e(nc,Vce),e(Vce,hQo),e(nc,pQo),e(hr,_Qo),e(hr,Ny),e(Ny,uQo),e(Ny,Wce),e(Wce,bQo),e(Ny,vQo),e(hr,TQo),e(hr,lt),g(Dy,lt,null),e(lt,FQo),e(lt,Qce),e(Qce,CQo),e(lt,MQo),e(lt,sc),e(sc,EQo),e(sc,Hce),e(Hce,yQo),e(sc,wQo),e(sc,Uce),e(Uce,AQo),e(sc,LQo),e(lt,BQo),e(lt,Jce),e(Jce,kQo),e(lt,xQo),g(qy,lt,null),e(hr,RQo),e(hr,ho),g(Gy,ho,null),e(ho,SQo),e(ho,Yce),e(Yce,PQo),e(ho,$Qo),e(ho,dn),e(dn,IQo),e(dn,Kce),e(Kce,jQo),e(dn,NQo),e(dn,Zce),e(Zce,DQo),e(dn,qQo),e(dn,efe),e(efe,GQo),e(dn,OQo),e(ho,XQo),e(ho,H),e(H,fT),e(fT,ofe),e(ofe,zQo),e(fT,VQo),e(fT,vD),e(vD,WQo),e(fT,QQo),e(H,HQo),e(H,mT),e(mT,rfe),e(rfe,UQo),e(mT,JQo),e(mT,TD),e(TD,YQo),e(mT,KQo),e(H,ZQo),e(H,gT),e(gT,tfe),e(tfe,eHo),e(gT,oHo),e(gT,FD),e(FD,rHo),e(gT,tHo),e(H,aHo),e(H,hT),e(hT,afe),e(afe,nHo),e(hT,sHo),e(hT,CD),e(CD,lHo),e(hT,iHo),e(H,dHo),e(H,pT),e(pT,nfe),e(nfe,cHo),e(pT,fHo),e(pT,MD),e(MD,mHo),e(pT,gHo),e(H,hHo),e(H,_T),e(_T,sfe),e(sfe,pHo),e(_T,_Ho),e(_T,ED),e(ED,uHo),e(_T,bHo),e(H,vHo),e(H,uT),e(uT,lfe),e(lfe,THo),e(uT,FHo),e(uT,yD),e(yD,CHo),e(uT,MHo),e(H,EHo),e(H,bT),e(bT,ife),e(ife,yHo),e(bT,wHo),e(bT,wD),e(wD,AHo),e(bT,LHo),e(H,BHo),e(H,vT),e(vT,dfe),e(dfe,kHo),e(vT,xHo),e(vT,AD),e(AD,RHo),e(vT,SHo),e(H,PHo),e(H,TT),e(TT,cfe),e(cfe,$Ho),e(TT,IHo),e(TT,LD),e(LD,jHo),e(TT,NHo),e(H,DHo),e(H,FT),e(FT,ffe),e(ffe,qHo),e(FT,GHo),e(FT,BD),e(BD,OHo),e(FT,XHo),e(H,zHo),e(H,CT),e(CT,mfe),e(mfe,VHo),e(CT,WHo),e(CT,kD),e(kD,QHo),e(CT,HHo),e(H,UHo),e(H,MT),e(MT,gfe),e(gfe,JHo),e(MT,YHo),e(MT,xD),e(xD,KHo),e(MT,ZHo),e(H,eUo),e(H,ET),e(ET,hfe),e(hfe,oUo),e(ET,rUo),e(ET,RD),e(RD,tUo),e(ET,aUo),e(H,nUo),e(H,yT),e(yT,pfe),e(pfe,sUo),e(yT,lUo),e(yT,SD),e(SD,iUo),e(yT,dUo),e(H,cUo),e(H,wT),e(wT,_fe),e(_fe,fUo),e(wT,mUo),e(wT,PD),e(PD,gUo),e(wT,hUo),e(H,pUo),e(H,AT),e(AT,ufe),e(ufe,_Uo),e(AT,uUo),e(AT,$D),e($D,bUo),e(AT,vUo),e(H,TUo),e(H,LT),e(LT,bfe),e(bfe,FUo),e(LT,CUo),e(LT,ID),e(ID,MUo),e(LT,EUo),e(H,yUo),e(H,BT),e(BT,vfe),e(vfe,wUo),e(BT,AUo),e(BT,jD),e(jD,LUo),e(BT,BUo),e(H,kUo),e(H,kT),e(kT,Tfe),e(Tfe,xUo),e(kT,RUo),e(kT,ND),e(ND,SUo),e(kT,PUo),e(H,$Uo),e(H,xT),e(xT,Ffe),e(Ffe,IUo),e(xT,jUo),e(xT,DD),e(DD,NUo),e(xT,DUo),e(H,qUo),e(H,RT),e(RT,Cfe),e(Cfe,GUo),e(RT,OUo),e(RT,qD),e(qD,XUo),e(RT,zUo),e(ho,VUo),e(ho,Mfe),e(Mfe,WUo),e(ho,QUo),g(Oy,ho,null),b(d,c8e,u),b(d,lc,u),e(lc,ST),e(ST,Efe),g(Xy,Efe,null),e(lc,HUo),e(lc,yfe),e(yfe,UUo),b(d,f8e,u),b(d,pr,u),g(zy,pr,null),e(pr,JUo),e(pr,ic),e(ic,YUo),e(ic,wfe),e(wfe,KUo),e(ic,ZUo),e(ic,Afe),e(Afe,eJo),e(ic,oJo),e(pr,rJo),e(pr,Vy),e(Vy,tJo),e(Vy,Lfe),e(Lfe,aJo),e(Vy,nJo),e(pr,sJo),e(pr,it),g(Wy,it,null),e(it,lJo),e(it,Bfe),e(Bfe,iJo),e(it,dJo),e(it,dc),e(dc,cJo),e(dc,kfe),e(kfe,fJo),e(dc,mJo),e(dc,xfe),e(xfe,gJo),e(dc,hJo),e(it,pJo),e(it,Rfe),e(Rfe,_Jo),e(it,uJo),g(Qy,it,null),e(pr,bJo),e(pr,po),g(Hy,po,null),e(po,vJo),e(po,Sfe),e(Sfe,TJo),e(po,FJo),e(po,cn),e(cn,CJo),e(cn,Pfe),e(Pfe,MJo),e(cn,EJo),e(cn,$fe),e($fe,yJo),e(cn,wJo),e(cn,Ife),e(Ife,AJo),e(cn,LJo),e(po,BJo),e(po,he),e(he,PT),e(PT,jfe),e(jfe,kJo),e(PT,xJo),e(PT,GD),e(GD,RJo),e(PT,SJo),e(he,PJo),e(he,$T),e($T,Nfe),e(Nfe,$Jo),e($T,IJo),e($T,OD),e(OD,jJo),e($T,NJo),e(he,DJo),e(he,IT),e(IT,Dfe),e(Dfe,qJo),e(IT,GJo),e(IT,XD),e(XD,OJo),e(IT,XJo),e(he,zJo),e(he,jT),e(jT,qfe),e(qfe,VJo),e(jT,WJo),e(jT,zD),e(zD,QJo),e(jT,HJo),e(he,UJo),e(he,NT),e(NT,Gfe),e(Gfe,JJo),e(NT,YJo),e(NT,VD),e(VD,KJo),e(NT,ZJo),e(he,eYo),e(he,DT),e(DT,Ofe),e(Ofe,oYo),e(DT,rYo),e(DT,WD),e(WD,tYo),e(DT,aYo),e(he,nYo),e(he,qT),e(qT,Xfe),e(Xfe,sYo),e(qT,lYo),e(qT,QD),e(QD,iYo),e(qT,dYo),e(he,cYo),e(he,GT),e(GT,zfe),e(zfe,fYo),e(GT,mYo),e(GT,HD),e(HD,gYo),e(GT,hYo),e(he,pYo),e(he,OT),e(OT,Vfe),e(Vfe,_Yo),e(OT,uYo),e(OT,UD),e(UD,bYo),e(OT,vYo),e(he,TYo),e(he,XT),e(XT,Wfe),e(Wfe,FYo),e(XT,CYo),e(XT,JD),e(JD,MYo),e(XT,EYo),e(po,yYo),e(po,Qfe),e(Qfe,wYo),e(po,AYo),g(Uy,po,null),b(d,m8e,u),b(d,cc,u),e(cc,zT),e(zT,Hfe),g(Jy,Hfe,null),e(cc,LYo),e(cc,Ufe),e(Ufe,BYo),b(d,g8e,u),b(d,_r,u),g(Yy,_r,null),e(_r,kYo),e(_r,fc),e(fc,xYo),e(fc,Jfe),e(Jfe,RYo),e(fc,SYo),e(fc,Yfe),e(Yfe,PYo),e(fc,$Yo),e(_r,IYo),e(_r,Ky),e(Ky,jYo),e(Ky,Kfe),e(Kfe,NYo),e(Ky,DYo),e(_r,qYo),e(_r,dt),g(Zy,dt,null),e(dt,GYo),e(dt,Zfe),e(Zfe,OYo),e(dt,XYo),e(dt,mc),e(mc,zYo),e(mc,eme),e(eme,VYo),e(mc,WYo),e(mc,ome),e(ome,QYo),e(mc,HYo),e(dt,UYo),e(dt,rme),e(rme,JYo),e(dt,YYo),g(ew,dt,null),e(_r,KYo),e(_r,_o),g(ow,_o,null),e(_o,ZYo),e(_o,tme),e(tme,eKo),e(_o,oKo),e(_o,fn),e(fn,rKo),e(fn,ame),e(ame,tKo),e(fn,aKo),e(fn,nme),e(nme,nKo),e(fn,sKo),e(fn,sme),e(sme,lKo),e(fn,iKo),e(_o,dKo),e(_o,lme),e(lme,VT),e(VT,ime),e(ime,cKo),e(VT,fKo),e(VT,YD),e(YD,mKo),e(VT,gKo),e(_o,hKo),e(_o,dme),e(dme,pKo),e(_o,_Ko),g(rw,_o,null),b(d,h8e,u),b(d,gc,u),e(gc,WT),e(WT,cme),g(tw,cme,null),e(gc,uKo),e(gc,fme),e(fme,bKo),b(d,p8e,u),b(d,ur,u),g(aw,ur,null),e(ur,vKo),e(ur,hc),e(hc,TKo),e(hc,mme),e(mme,FKo),e(hc,CKo),e(hc,gme),e(gme,MKo),e(hc,EKo),e(ur,yKo),e(ur,nw),e(nw,wKo),e(nw,hme),e(hme,AKo),e(nw,LKo),e(ur,BKo),e(ur,ct),g(sw,ct,null),e(ct,kKo),e(ct,pme),e(pme,xKo),e(ct,RKo),e(ct,pc),e(pc,SKo),e(pc,_me),e(_me,PKo),e(pc,$Ko),e(pc,ume),e(ume,IKo),e(pc,jKo),e(ct,NKo),e(ct,bme),e(bme,DKo),e(ct,qKo),g(lw,ct,null),e(ur,GKo),e(ur,uo),g(iw,uo,null),e(uo,OKo),e(uo,vme),e(vme,XKo),e(uo,zKo),e(uo,mn),e(mn,VKo),e(mn,Tme),e(Tme,WKo),e(mn,QKo),e(mn,Fme),e(Fme,HKo),e(mn,UKo),e(mn,Cme),e(Cme,JKo),e(mn,YKo),e(uo,KKo),e(uo,Y),e(Y,QT),e(QT,Mme),e(Mme,ZKo),e(QT,eZo),e(QT,KD),e(KD,oZo),e(QT,rZo),e(Y,tZo),e(Y,HT),e(HT,Eme),e(Eme,aZo),e(HT,nZo),e(HT,ZD),e(ZD,sZo),e(HT,lZo),e(Y,iZo),e(Y,UT),e(UT,yme),e(yme,dZo),e(UT,cZo),e(UT,eq),e(eq,fZo),e(UT,mZo),e(Y,gZo),e(Y,JT),e(JT,wme),e(wme,hZo),e(JT,pZo),e(JT,oq),e(oq,_Zo),e(JT,uZo),e(Y,bZo),e(Y,YT),e(YT,Ame),e(Ame,vZo),e(YT,TZo),e(YT,rq),e(rq,FZo),e(YT,CZo),e(Y,MZo),e(Y,KT),e(KT,Lme),e(Lme,EZo),e(KT,yZo),e(KT,tq),e(tq,wZo),e(KT,AZo),e(Y,LZo),e(Y,ZT),e(ZT,Bme),e(Bme,BZo),e(ZT,kZo),e(ZT,aq),e(aq,xZo),e(ZT,RZo),e(Y,SZo),e(Y,e7),e(e7,kme),e(kme,PZo),e(e7,$Zo),e(e7,nq),e(nq,IZo),e(e7,jZo),e(Y,NZo),e(Y,o7),e(o7,xme),e(xme,DZo),e(o7,qZo),e(o7,sq),e(sq,GZo),e(o7,OZo),e(Y,XZo),e(Y,r7),e(r7,Rme),e(Rme,zZo),e(r7,VZo),e(r7,lq),e(lq,WZo),e(r7,QZo),e(Y,HZo),e(Y,t7),e(t7,Sme),e(Sme,UZo),e(t7,JZo),e(t7,iq),e(iq,YZo),e(t7,KZo),e(Y,ZZo),e(Y,a7),e(a7,Pme),e(Pme,eer),e(a7,oer),e(a7,dq),e(dq,rer),e(a7,ter),e(Y,aer),e(Y,n7),e(n7,$me),e($me,ner),e(n7,ser),e(n7,cq),e(cq,ler),e(n7,ier),e(Y,der),e(Y,s7),e(s7,Ime),e(Ime,cer),e(s7,fer),e(s7,fq),e(fq,mer),e(s7,ger),e(Y,her),e(Y,l7),e(l7,jme),e(jme,per),e(l7,_er),e(l7,mq),e(mq,uer),e(l7,ber),e(Y,ver),e(Y,i7),e(i7,Nme),e(Nme,Ter),e(i7,Fer),e(i7,gq),e(gq,Cer),e(i7,Mer),e(Y,Eer),e(Y,d7),e(d7,Dme),e(Dme,yer),e(d7,wer),e(d7,hq),e(hq,Aer),e(d7,Ler),e(Y,Ber),e(Y,c7),e(c7,qme),e(qme,ker),e(c7,xer),e(c7,pq),e(pq,Rer),e(c7,Ser),e(Y,Per),e(Y,f7),e(f7,Gme),e(Gme,$er),e(f7,Ier),e(f7,_q),e(_q,jer),e(f7,Ner),e(Y,Der),e(Y,m7),e(m7,Ome),e(Ome,qer),e(m7,Ger),e(m7,uq),e(uq,Oer),e(m7,Xer),e(uo,zer),e(uo,Xme),e(Xme,Ver),e(uo,Wer),g(dw,uo,null),b(d,_8e,u),b(d,_c,u),e(_c,g7),e(g7,zme),g(cw,zme,null),e(_c,Qer),e(_c,Vme),e(Vme,Her),b(d,u8e,u),b(d,br,u),g(fw,br,null),e(br,Uer),e(br,uc),e(uc,Jer),e(uc,Wme),e(Wme,Yer),e(uc,Ker),e(uc,Qme),e(Qme,Zer),e(uc,eor),e(br,oor),e(br,mw),e(mw,ror),e(mw,Hme),e(Hme,tor),e(mw,aor),e(br,nor),e(br,ft),g(gw,ft,null),e(ft,sor),e(ft,Ume),e(Ume,lor),e(ft,ior),e(ft,bc),e(bc,dor),e(bc,Jme),e(Jme,cor),e(bc,mor),e(bc,Yme),e(Yme,gor),e(bc,hor),e(ft,por),e(ft,Kme),e(Kme,_or),e(ft,uor),g(hw,ft,null),e(br,bor),e(br,bo),g(pw,bo,null),e(bo,vor),e(bo,Zme),e(Zme,Tor),e(bo,For),e(bo,gn),e(gn,Cor),e(gn,ege),e(ege,Mor),e(gn,Eor),e(gn,oge),e(oge,yor),e(gn,wor),e(gn,rge),e(rge,Aor),e(gn,Lor),e(bo,Bor),e(bo,pe),e(pe,h7),e(h7,tge),e(tge,kor),e(h7,xor),e(h7,bq),e(bq,Ror),e(h7,Sor),e(pe,Por),e(pe,p7),e(p7,age),e(age,$or),e(p7,Ior),e(p7,vq),e(vq,jor),e(p7,Nor),e(pe,Dor),e(pe,_7),e(_7,nge),e(nge,qor),e(_7,Gor),e(_7,Tq),e(Tq,Oor),e(_7,Xor),e(pe,zor),e(pe,u7),e(u7,sge),e(sge,Vor),e(u7,Wor),e(u7,Fq),e(Fq,Qor),e(u7,Hor),e(pe,Uor),e(pe,b7),e(b7,lge),e(lge,Jor),e(b7,Yor),e(b7,Cq),e(Cq,Kor),e(b7,Zor),e(pe,err),e(pe,v7),e(v7,ige),e(ige,orr),e(v7,rrr),e(v7,Mq),e(Mq,trr),e(v7,arr),e(pe,nrr),e(pe,T7),e(T7,dge),e(dge,srr),e(T7,lrr),e(T7,Eq),e(Eq,irr),e(T7,drr),e(pe,crr),e(pe,F7),e(F7,cge),e(cge,frr),e(F7,mrr),e(F7,yq),e(yq,grr),e(F7,hrr),e(pe,prr),e(pe,C7),e(C7,fge),e(fge,_rr),e(C7,urr),e(C7,wq),e(wq,brr),e(C7,vrr),e(pe,Trr),e(pe,M7),e(M7,mge),e(mge,Frr),e(M7,Crr),e(M7,Aq),e(Aq,Mrr),e(M7,Err),e(bo,yrr),e(bo,gge),e(gge,wrr),e(bo,Arr),g(_w,bo,null),b(d,b8e,u),b(d,vc,u),e(vc,E7),e(E7,hge),g(uw,hge,null),e(vc,Lrr),e(vc,pge),e(pge,Brr),b(d,v8e,u),b(d,vr,u),g(bw,vr,null),e(vr,krr),e(vr,Tc),e(Tc,xrr),e(Tc,_ge),e(_ge,Rrr),e(Tc,Srr),e(Tc,uge),e(uge,Prr),e(Tc,$rr),e(vr,Irr),e(vr,vw),e(vw,jrr),e(vw,bge),e(bge,Nrr),e(vw,Drr),e(vr,qrr),e(vr,mt),g(Tw,mt,null),e(mt,Grr),e(mt,vge),e(vge,Orr),e(mt,Xrr),e(mt,Fc),e(Fc,zrr),e(Fc,Tge),e(Tge,Vrr),e(Fc,Wrr),e(Fc,Fge),e(Fge,Qrr),e(Fc,Hrr),e(mt,Urr),e(mt,Cge),e(Cge,Jrr),e(mt,Yrr),g(Fw,mt,null),e(vr,Krr),e(vr,vo),g(Cw,vo,null),e(vo,Zrr),e(vo,Mge),e(Mge,etr),e(vo,otr),e(vo,hn),e(hn,rtr),e(hn,Ege),e(Ege,ttr),e(hn,atr),e(hn,yge),e(yge,ntr),e(hn,str),e(hn,wge),e(wge,ltr),e(hn,itr),e(vo,dtr),e(vo,X),e(X,y7),e(y7,Age),e(Age,ctr),e(y7,ftr),e(y7,Lq),e(Lq,mtr),e(y7,gtr),e(X,htr),e(X,w7),e(w7,Lge),e(Lge,ptr),e(w7,_tr),e(w7,Bq),e(Bq,utr),e(w7,btr),e(X,vtr),e(X,A7),e(A7,Bge),e(Bge,Ttr),e(A7,Ftr),e(A7,kq),e(kq,Ctr),e(A7,Mtr),e(X,Etr),e(X,L7),e(L7,kge),e(kge,ytr),e(L7,wtr),e(L7,xq),e(xq,Atr),e(L7,Ltr),e(X,Btr),e(X,B7),e(B7,xge),e(xge,ktr),e(B7,xtr),e(B7,Rq),e(Rq,Rtr),e(B7,Str),e(X,Ptr),e(X,k7),e(k7,Rge),e(Rge,$tr),e(k7,Itr),e(k7,Sq),e(Sq,jtr),e(k7,Ntr),e(X,Dtr),e(X,x7),e(x7,Sge),e(Sge,qtr),e(x7,Gtr),e(x7,Pq),e(Pq,Otr),e(x7,Xtr),e(X,ztr),e(X,R7),e(R7,Pge),e(Pge,Vtr),e(R7,Wtr),e(R7,$q),e($q,Qtr),e(R7,Htr),e(X,Utr),e(X,S7),e(S7,$ge),e($ge,Jtr),e(S7,Ytr),e(S7,Iq),e(Iq,Ktr),e(S7,Ztr),e(X,ear),e(X,P7),e(P7,Ige),e(Ige,oar),e(P7,rar),e(P7,jq),e(jq,tar),e(P7,aar),e(X,nar),e(X,$7),e($7,jge),e(jge,sar),e($7,lar),e($7,Nq),e(Nq,iar),e($7,dar),e(X,car),e(X,I7),e(I7,Nge),e(Nge,far),e(I7,mar),e(I7,Dq),e(Dq,gar),e(I7,har),e(X,par),e(X,j7),e(j7,Dge),e(Dge,_ar),e(j7,uar),e(j7,qq),e(qq,bar),e(j7,Tar),e(X,Far),e(X,N7),e(N7,qge),e(qge,Car),e(N7,Mar),e(N7,Gq),e(Gq,Ear),e(N7,yar),e(X,war),e(X,D7),e(D7,Gge),e(Gge,Aar),e(D7,Lar),e(D7,Oq),e(Oq,Bar),e(D7,kar),e(X,xar),e(X,q7),e(q7,Oge),e(Oge,Rar),e(q7,Sar),e(q7,Xq),e(Xq,Par),e(q7,$ar),e(X,Iar),e(X,G7),e(G7,Xge),e(Xge,jar),e(G7,Nar),e(G7,zq),e(zq,Dar),e(G7,qar),e(X,Gar),e(X,O7),e(O7,zge),e(zge,Oar),e(O7,Xar),e(O7,Vq),e(Vq,zar),e(O7,Var),e(X,War),e(X,X7),e(X7,Vge),e(Vge,Qar),e(X7,Har),e(X7,Wq),e(Wq,Uar),e(X7,Jar),e(X,Yar),e(X,z7),e(z7,Wge),e(Wge,Kar),e(z7,Zar),e(z7,Qq),e(Qq,enr),e(z7,onr),e(X,rnr),e(X,V7),e(V7,Qge),e(Qge,tnr),e(V7,anr),e(V7,Hq),e(Hq,nnr),e(V7,snr),e(X,lnr),e(X,W7),e(W7,Hge),e(Hge,inr),e(W7,dnr),e(W7,Uq),e(Uq,cnr),e(W7,fnr),e(X,mnr),e(X,Q7),e(Q7,Uge),e(Uge,gnr),e(Q7,hnr),e(Q7,Jq),e(Jq,pnr),e(Q7,_nr),e(X,unr),e(X,H7),e(H7,Jge),e(Jge,bnr),e(H7,vnr),e(H7,Yq),e(Yq,Tnr),e(H7,Fnr),e(X,Cnr),e(X,U7),e(U7,Yge),e(Yge,Mnr),e(U7,Enr),e(U7,Kq),e(Kq,ynr),e(U7,wnr),e(vo,Anr),e(vo,Kge),e(Kge,Lnr),e(vo,Bnr),g(Mw,vo,null),b(d,T8e,u),b(d,Cc,u),e(Cc,J7),e(J7,Zge),g(Ew,Zge,null),e(Cc,knr),e(Cc,ehe),e(ehe,xnr),b(d,F8e,u),b(d,Tr,u),g(yw,Tr,null),e(Tr,Rnr),e(Tr,Mc),e(Mc,Snr),e(Mc,ohe),e(ohe,Pnr),e(Mc,$nr),e(Mc,rhe),e(rhe,Inr),e(Mc,jnr),e(Tr,Nnr),e(Tr,ww),e(ww,Dnr),e(ww,the),e(the,qnr),e(ww,Gnr),e(Tr,Onr),e(Tr,gt),g(Aw,gt,null),e(gt,Xnr),e(gt,ahe),e(ahe,znr),e(gt,Vnr),e(gt,Ec),e(Ec,Wnr),e(Ec,nhe),e(nhe,Qnr),e(Ec,Hnr),e(Ec,she),e(she,Unr),e(Ec,Jnr),e(gt,Ynr),e(gt,lhe),e(lhe,Knr),e(gt,Znr),g(Lw,gt,null),e(Tr,esr),e(Tr,To),g(Bw,To,null),e(To,osr),e(To,ihe),e(ihe,rsr),e(To,tsr),e(To,pn),e(pn,asr),e(pn,dhe),e(dhe,nsr),e(pn,ssr),e(pn,che),e(che,lsr),e(pn,isr),e(pn,fhe),e(fhe,dsr),e(pn,csr),e(To,fsr),e(To,te),e(te,Y7),e(Y7,mhe),e(mhe,msr),e(Y7,gsr),e(Y7,Zq),e(Zq,hsr),e(Y7,psr),e(te,_sr),e(te,K7),e(K7,ghe),e(ghe,usr),e(K7,bsr),e(K7,eG),e(eG,vsr),e(K7,Tsr),e(te,Fsr),e(te,Z7),e(Z7,hhe),e(hhe,Csr),e(Z7,Msr),e(Z7,oG),e(oG,Esr),e(Z7,ysr),e(te,wsr),e(te,eF),e(eF,phe),e(phe,Asr),e(eF,Lsr),e(eF,rG),e(rG,Bsr),e(eF,ksr),e(te,xsr),e(te,oF),e(oF,_he),e(_he,Rsr),e(oF,Ssr),e(oF,tG),e(tG,Psr),e(oF,$sr),e(te,Isr),e(te,rF),e(rF,uhe),e(uhe,jsr),e(rF,Nsr),e(rF,aG),e(aG,Dsr),e(rF,qsr),e(te,Gsr),e(te,tF),e(tF,bhe),e(bhe,Osr),e(tF,Xsr),e(tF,nG),e(nG,zsr),e(tF,Vsr),e(te,Wsr),e(te,aF),e(aF,vhe),e(vhe,Qsr),e(aF,Hsr),e(aF,sG),e(sG,Usr),e(aF,Jsr),e(te,Ysr),e(te,nF),e(nF,The),e(The,Ksr),e(nF,Zsr),e(nF,lG),e(lG,elr),e(nF,olr),e(te,rlr),e(te,sF),e(sF,Fhe),e(Fhe,tlr),e(sF,alr),e(sF,iG),e(iG,nlr),e(sF,slr),e(te,llr),e(te,lF),e(lF,Che),e(Che,ilr),e(lF,dlr),e(lF,dG),e(dG,clr),e(lF,flr),e(te,mlr),e(te,iF),e(iF,Mhe),e(Mhe,glr),e(iF,hlr),e(iF,cG),e(cG,plr),e(iF,_lr),e(te,ulr),e(te,dF),e(dF,Ehe),e(Ehe,blr),e(dF,vlr),e(dF,fG),e(fG,Tlr),e(dF,Flr),e(te,Clr),e(te,cF),e(cF,yhe),e(yhe,Mlr),e(cF,Elr),e(cF,mG),e(mG,ylr),e(cF,wlr),e(te,Alr),e(te,fF),e(fF,whe),e(whe,Llr),e(fF,Blr),e(fF,gG),e(gG,klr),e(fF,xlr),e(te,Rlr),e(te,mF),e(mF,Ahe),e(Ahe,Slr),e(mF,Plr),e(mF,hG),e(hG,$lr),e(mF,Ilr),e(te,jlr),e(te,gF),e(gF,Lhe),e(Lhe,Nlr),e(gF,Dlr),e(gF,pG),e(pG,qlr),e(gF,Glr),e(To,Olr),e(To,Bhe),e(Bhe,Xlr),e(To,zlr),g(kw,To,null),b(d,C8e,u),b(d,yc,u),e(yc,hF),e(hF,khe),g(xw,khe,null),e(yc,Vlr),e(yc,xhe),e(xhe,Wlr),b(d,M8e,u),b(d,Fr,u),g(Rw,Fr,null),e(Fr,Qlr),e(Fr,wc),e(wc,Hlr),e(wc,Rhe),e(Rhe,Ulr),e(wc,Jlr),e(wc,She),e(She,Ylr),e(wc,Klr),e(Fr,Zlr),e(Fr,Sw),e(Sw,eir),e(Sw,Phe),e(Phe,oir),e(Sw,rir),e(Fr,tir),e(Fr,ht),g(Pw,ht,null),e(ht,air),e(ht,$he),e($he,nir),e(ht,sir),e(ht,Ac),e(Ac,lir),e(Ac,Ihe),e(Ihe,iir),e(Ac,dir),e(Ac,jhe),e(jhe,cir),e(Ac,fir),e(ht,mir),e(ht,Nhe),e(Nhe,gir),e(ht,hir),g($w,ht,null),e(Fr,pir),e(Fr,Fo),g(Iw,Fo,null),e(Fo,_ir),e(Fo,Dhe),e(Dhe,uir),e(Fo,bir),e(Fo,_n),e(_n,vir),e(_n,qhe),e(qhe,Tir),e(_n,Fir),e(_n,Ghe),e(Ghe,Cir),e(_n,Mir),e(_n,Ohe),e(Ohe,Eir),e(_n,yir),e(Fo,wir),e(Fo,Xhe),e(Xhe,pF),e(pF,zhe),e(zhe,Air),e(pF,Lir),e(pF,_G),e(_G,Bir),e(pF,kir),e(Fo,xir),e(Fo,Vhe),e(Vhe,Rir),e(Fo,Sir),g(jw,Fo,null),b(d,E8e,u),b(d,Lc,u),e(Lc,_F),e(_F,Whe),g(Nw,Whe,null),e(Lc,Pir),e(Lc,Qhe),e(Qhe,$ir),b(d,y8e,u),b(d,Cr,u),g(Dw,Cr,null),e(Cr,Iir),e(Cr,Bc),e(Bc,jir),e(Bc,Hhe),e(Hhe,Nir),e(Bc,Dir),e(Bc,Uhe),e(Uhe,qir),e(Bc,Gir),e(Cr,Oir),e(Cr,qw),e(qw,Xir),e(qw,Jhe),e(Jhe,zir),e(qw,Vir),e(Cr,Wir),e(Cr,pt),g(Gw,pt,null),e(pt,Qir),e(pt,Yhe),e(Yhe,Hir),e(pt,Uir),e(pt,kc),e(kc,Jir),e(kc,Khe),e(Khe,Yir),e(kc,Kir),e(kc,Zhe),e(Zhe,Zir),e(kc,edr),e(pt,odr),e(pt,epe),e(epe,rdr),e(pt,tdr),g(Ow,pt,null),e(Cr,adr),e(Cr,Co),g(Xw,Co,null),e(Co,ndr),e(Co,ope),e(ope,sdr),e(Co,ldr),e(Co,un),e(un,idr),e(un,rpe),e(rpe,ddr),e(un,cdr),e(un,tpe),e(tpe,fdr),e(un,mdr),e(un,ape),e(ape,gdr),e(un,hdr),e(Co,pdr),e(Co,K),e(K,uF),e(uF,npe),e(npe,_dr),e(uF,udr),e(uF,uG),e(uG,bdr),e(uF,vdr),e(K,Tdr),e(K,bF),e(bF,spe),e(spe,Fdr),e(bF,Cdr),e(bF,bG),e(bG,Mdr),e(bF,Edr),e(K,ydr),e(K,vF),e(vF,lpe),e(lpe,wdr),e(vF,Adr),e(vF,vG),e(vG,Ldr),e(vF,Bdr),e(K,kdr),e(K,TF),e(TF,ipe),e(ipe,xdr),e(TF,Rdr),e(TF,TG),e(TG,Sdr),e(TF,Pdr),e(K,$dr),e(K,FF),e(FF,dpe),e(dpe,Idr),e(FF,jdr),e(FF,FG),e(FG,Ndr),e(FF,Ddr),e(K,qdr),e(K,CF),e(CF,cpe),e(cpe,Gdr),e(CF,Odr),e(CF,CG),e(CG,Xdr),e(CF,zdr),e(K,Vdr),e(K,MF),e(MF,fpe),e(fpe,Wdr),e(MF,Qdr),e(MF,MG),e(MG,Hdr),e(MF,Udr),e(K,Jdr),e(K,EF),e(EF,mpe),e(mpe,Ydr),e(EF,Kdr),e(EF,EG),e(EG,Zdr),e(EF,ecr),e(K,ocr),e(K,yF),e(yF,gpe),e(gpe,rcr),e(yF,tcr),e(yF,yG),e(yG,acr),e(yF,ncr),e(K,scr),e(K,wF),e(wF,hpe),e(hpe,lcr),e(wF,icr),e(wF,wG),e(wG,dcr),e(wF,ccr),e(K,fcr),e(K,AF),e(AF,ppe),e(ppe,mcr),e(AF,gcr),e(AF,AG),e(AG,hcr),e(AF,pcr),e(K,_cr),e(K,LF),e(LF,_pe),e(_pe,ucr),e(LF,bcr),e(LF,LG),e(LG,vcr),e(LF,Tcr),e(K,Fcr),e(K,BF),e(BF,upe),e(upe,Ccr),e(BF,Mcr),e(BF,BG),e(BG,Ecr),e(BF,ycr),e(K,wcr),e(K,kF),e(kF,bpe),e(bpe,Acr),e(kF,Lcr),e(kF,kG),e(kG,Bcr),e(kF,kcr),e(K,xcr),e(K,xF),e(xF,vpe),e(vpe,Rcr),e(xF,Scr),e(xF,xG),e(xG,Pcr),e(xF,$cr),e(K,Icr),e(K,RF),e(RF,Tpe),e(Tpe,jcr),e(RF,Ncr),e(RF,RG),e(RG,Dcr),e(RF,qcr),e(K,Gcr),e(K,SF),e(SF,Fpe),e(Fpe,Ocr),e(SF,Xcr),e(SF,SG),e(SG,zcr),e(SF,Vcr),e(K,Wcr),e(K,PF),e(PF,Cpe),e(Cpe,Qcr),e(PF,Hcr),e(PF,PG),e(PG,Ucr),e(PF,Jcr),e(K,Ycr),e(K,$F),e($F,Mpe),e(Mpe,Kcr),e($F,Zcr),e($F,$G),e($G,efr),e($F,ofr),e(K,rfr),e(K,IF),e(IF,Epe),e(Epe,tfr),e(IF,afr),e(IF,IG),e(IG,nfr),e(IF,sfr),e(Co,lfr),e(Co,ype),e(ype,ifr),e(Co,dfr),g(zw,Co,null),b(d,w8e,u),b(d,xc,u),e(xc,jF),e(jF,wpe),g(Vw,wpe,null),e(xc,cfr),e(xc,Ape),e(Ape,ffr),b(d,A8e,u),b(d,Mr,u),g(Ww,Mr,null),e(Mr,mfr),e(Mr,Rc),e(Rc,gfr),e(Rc,Lpe),e(Lpe,hfr),e(Rc,pfr),e(Rc,Bpe),e(Bpe,_fr),e(Rc,ufr),e(Mr,bfr),e(Mr,Qw),e(Qw,vfr),e(Qw,kpe),e(kpe,Tfr),e(Qw,Ffr),e(Mr,Cfr),e(Mr,_t),g(Hw,_t,null),e(_t,Mfr),e(_t,xpe),e(xpe,Efr),e(_t,yfr),e(_t,Sc),e(Sc,wfr),e(Sc,Rpe),e(Rpe,Afr),e(Sc,Lfr),e(Sc,Spe),e(Spe,Bfr),e(Sc,kfr),e(_t,xfr),e(_t,Ppe),e(Ppe,Rfr),e(_t,Sfr),g(Uw,_t,null),e(Mr,Pfr),e(Mr,Mo),g(Jw,Mo,null),e(Mo,$fr),e(Mo,$pe),e($pe,Ifr),e(Mo,jfr),e(Mo,bn),e(bn,Nfr),e(bn,Ipe),e(Ipe,Dfr),e(bn,qfr),e(bn,jpe),e(jpe,Gfr),e(bn,Ofr),e(bn,Npe),e(Npe,Xfr),e(bn,zfr),e(Mo,Vfr),e(Mo,Z),e(Z,NF),e(NF,Dpe),e(Dpe,Wfr),e(NF,Qfr),e(NF,jG),e(jG,Hfr),e(NF,Ufr),e(Z,Jfr),e(Z,DF),e(DF,qpe),e(qpe,Yfr),e(DF,Kfr),e(DF,NG),e(NG,Zfr),e(DF,emr),e(Z,omr),e(Z,qF),e(qF,Gpe),e(Gpe,rmr),e(qF,tmr),e(qF,DG),e(DG,amr),e(qF,nmr),e(Z,smr),e(Z,GF),e(GF,Ope),e(Ope,lmr),e(GF,imr),e(GF,qG),e(qG,dmr),e(GF,cmr),e(Z,fmr),e(Z,OF),e(OF,Xpe),e(Xpe,mmr),e(OF,gmr),e(OF,GG),e(GG,hmr),e(OF,pmr),e(Z,_mr),e(Z,XF),e(XF,zpe),e(zpe,umr),e(XF,bmr),e(XF,OG),e(OG,vmr),e(XF,Tmr),e(Z,Fmr),e(Z,zF),e(zF,Vpe),e(Vpe,Cmr),e(zF,Mmr),e(zF,XG),e(XG,Emr),e(zF,ymr),e(Z,wmr),e(Z,VF),e(VF,Wpe),e(Wpe,Amr),e(VF,Lmr),e(VF,zG),e(zG,Bmr),e(VF,kmr),e(Z,xmr),e(Z,WF),e(WF,Qpe),e(Qpe,Rmr),e(WF,Smr),e(WF,VG),e(VG,Pmr),e(WF,$mr),e(Z,Imr),e(Z,QF),e(QF,Hpe),e(Hpe,jmr),e(QF,Nmr),e(QF,WG),e(WG,Dmr),e(QF,qmr),e(Z,Gmr),e(Z,HF),e(HF,Upe),e(Upe,Omr),e(HF,Xmr),e(HF,QG),e(QG,zmr),e(HF,Vmr),e(Z,Wmr),e(Z,UF),e(UF,Jpe),e(Jpe,Qmr),e(UF,Hmr),e(UF,HG),e(HG,Umr),e(UF,Jmr),e(Z,Ymr),e(Z,JF),e(JF,Ype),e(Ype,Kmr),e(JF,Zmr),e(JF,UG),e(UG,egr),e(JF,ogr),e(Z,rgr),e(Z,YF),e(YF,Kpe),e(Kpe,tgr),e(YF,agr),e(YF,JG),e(JG,ngr),e(YF,sgr),e(Z,lgr),e(Z,KF),e(KF,Zpe),e(Zpe,igr),e(KF,dgr),e(KF,YG),e(YG,cgr),e(KF,fgr),e(Z,mgr),e(Z,ZF),e(ZF,e_e),e(e_e,ggr),e(ZF,hgr),e(ZF,KG),e(KG,pgr),e(ZF,_gr),e(Z,ugr),e(Z,e9),e(e9,o_e),e(o_e,bgr),e(e9,vgr),e(e9,ZG),e(ZG,Tgr),e(e9,Fgr),e(Z,Cgr),e(Z,o9),e(o9,r_e),e(r_e,Mgr),e(o9,Egr),e(o9,eO),e(eO,ygr),e(o9,wgr),e(Z,Agr),e(Z,r9),e(r9,t_e),e(t_e,Lgr),e(r9,Bgr),e(r9,oO),e(oO,kgr),e(r9,xgr),e(Mo,Rgr),e(Mo,a_e),e(a_e,Sgr),e(Mo,Pgr),g(Yw,Mo,null),b(d,L8e,u),b(d,Pc,u),e(Pc,t9),e(t9,n_e),g(Kw,n_e,null),e(Pc,$gr),e(Pc,s_e),e(s_e,Igr),b(d,B8e,u),b(d,Er,u),g(Zw,Er,null),e(Er,jgr),e(Er,$c),e($c,Ngr),e($c,l_e),e(l_e,Dgr),e($c,qgr),e($c,i_e),e(i_e,Ggr),e($c,Ogr),e(Er,Xgr),e(Er,eA),e(eA,zgr),e(eA,d_e),e(d_e,Vgr),e(eA,Wgr),e(Er,Qgr),e(Er,ut),g(oA,ut,null),e(ut,Hgr),e(ut,c_e),e(c_e,Ugr),e(ut,Jgr),e(ut,Ic),e(Ic,Ygr),e(Ic,f_e),e(f_e,Kgr),e(Ic,Zgr),e(Ic,m_e),e(m_e,ehr),e(Ic,ohr),e(ut,rhr),e(ut,g_e),e(g_e,thr),e(ut,ahr),g(rA,ut,null),e(Er,nhr),e(Er,Eo),g(tA,Eo,null),e(Eo,shr),e(Eo,h_e),e(h_e,lhr),e(Eo,ihr),e(Eo,vn),e(vn,dhr),e(vn,p_e),e(p_e,chr),e(vn,fhr),e(vn,__e),e(__e,mhr),e(vn,ghr),e(vn,u_e),e(u_e,hhr),e(vn,phr),e(Eo,_hr),e(Eo,b_e),e(b_e,a9),e(a9,v_e),e(v_e,uhr),e(a9,bhr),e(a9,rO),e(rO,vhr),e(a9,Thr),e(Eo,Fhr),e(Eo,T_e),e(T_e,Chr),e(Eo,Mhr),g(aA,Eo,null),b(d,k8e,u),b(d,jc,u),e(jc,n9),e(n9,F_e),g(nA,F_e,null),e(jc,Ehr),e(jc,C_e),e(C_e,yhr),b(d,x8e,u),b(d,yr,u),g(sA,yr,null),e(yr,whr),e(yr,Nc),e(Nc,Ahr),e(Nc,M_e),e(M_e,Lhr),e(Nc,Bhr),e(Nc,E_e),e(E_e,khr),e(Nc,xhr),e(yr,Rhr),e(yr,lA),e(lA,Shr),e(lA,y_e),e(y_e,Phr),e(lA,$hr),e(yr,Ihr),e(yr,bt),g(iA,bt,null),e(bt,jhr),e(bt,w_e),e(w_e,Nhr),e(bt,Dhr),e(bt,Dc),e(Dc,qhr),e(Dc,A_e),e(A_e,Ghr),e(Dc,Ohr),e(Dc,L_e),e(L_e,Xhr),e(Dc,zhr),e(bt,Vhr),e(bt,B_e),e(B_e,Whr),e(bt,Qhr),g(dA,bt,null),e(yr,Hhr),e(yr,yo),g(cA,yo,null),e(yo,Uhr),e(yo,k_e),e(k_e,Jhr),e(yo,Yhr),e(yo,Tn),e(Tn,Khr),e(Tn,x_e),e(x_e,Zhr),e(Tn,epr),e(Tn,R_e),e(R_e,opr),e(Tn,rpr),e(Tn,S_e),e(S_e,tpr),e(Tn,apr),e(yo,npr),e(yo,P_e),e(P_e,s9),e(s9,$_e),e($_e,spr),e(s9,lpr),e(s9,tO),e(tO,ipr),e(s9,dpr),e(yo,cpr),e(yo,I_e),e(I_e,fpr),e(yo,mpr),g(fA,yo,null),b(d,R8e,u),b(d,qc,u),e(qc,l9),e(l9,j_e),g(mA,j_e,null),e(qc,gpr),e(qc,N_e),e(N_e,hpr),b(d,S8e,u),b(d,wr,u),g(gA,wr,null),e(wr,ppr),e(wr,Gc),e(Gc,_pr),e(Gc,D_e),e(D_e,upr),e(Gc,bpr),e(Gc,q_e),e(q_e,vpr),e(Gc,Tpr),e(wr,Fpr),e(wr,hA),e(hA,Cpr),e(hA,G_e),e(G_e,Mpr),e(hA,Epr),e(wr,ypr),e(wr,vt),g(pA,vt,null),e(vt,wpr),e(vt,O_e),e(O_e,Apr),e(vt,Lpr),e(vt,Oc),e(Oc,Bpr),e(Oc,X_e),e(X_e,kpr),e(Oc,xpr),e(Oc,z_e),e(z_e,Rpr),e(Oc,Spr),e(vt,Ppr),e(vt,V_e),e(V_e,$pr),e(vt,Ipr),g(_A,vt,null),e(wr,jpr),e(wr,wo),g(uA,wo,null),e(wo,Npr),e(wo,W_e),e(W_e,Dpr),e(wo,qpr),e(wo,Fn),e(Fn,Gpr),e(Fn,Q_e),e(Q_e,Opr),e(Fn,Xpr),e(Fn,H_e),e(H_e,zpr),e(Fn,Vpr),e(Fn,U_e),e(U_e,Wpr),e(Fn,Qpr),e(wo,Hpr),e(wo,V),e(V,i9),e(i9,J_e),e(J_e,Upr),e(i9,Jpr),e(i9,aO),e(aO,Ypr),e(i9,Kpr),e(V,Zpr),e(V,d9),e(d9,Y_e),e(Y_e,e_r),e(d9,o_r),e(d9,nO),e(nO,r_r),e(d9,t_r),e(V,a_r),e(V,c9),e(c9,K_e),e(K_e,n_r),e(c9,s_r),e(c9,sO),e(sO,l_r),e(c9,i_r),e(V,d_r),e(V,f9),e(f9,Z_e),e(Z_e,c_r),e(f9,f_r),e(f9,lO),e(lO,m_r),e(f9,g_r),e(V,h_r),e(V,m9),e(m9,eue),e(eue,p_r),e(m9,__r),e(m9,iO),e(iO,u_r),e(m9,b_r),e(V,v_r),e(V,g9),e(g9,oue),e(oue,T_r),e(g9,F_r),e(g9,dO),e(dO,C_r),e(g9,M_r),e(V,E_r),e(V,h9),e(h9,rue),e(rue,y_r),e(h9,w_r),e(h9,cO),e(cO,A_r),e(h9,L_r),e(V,B_r),e(V,p9),e(p9,tue),e(tue,k_r),e(p9,x_r),e(p9,fO),e(fO,R_r),e(p9,S_r),e(V,P_r),e(V,_9),e(_9,aue),e(aue,$_r),e(_9,I_r),e(_9,mO),e(mO,j_r),e(_9,N_r),e(V,D_r),e(V,u9),e(u9,nue),e(nue,q_r),e(u9,G_r),e(u9,gO),e(gO,O_r),e(u9,X_r),e(V,z_r),e(V,b9),e(b9,sue),e(sue,V_r),e(b9,W_r),e(b9,hO),e(hO,Q_r),e(b9,H_r),e(V,U_r),e(V,v9),e(v9,lue),e(lue,J_r),e(v9,Y_r),e(v9,pO),e(pO,K_r),e(v9,Z_r),e(V,eur),e(V,T9),e(T9,iue),e(iue,our),e(T9,rur),e(T9,_O),e(_O,tur),e(T9,aur),e(V,nur),e(V,F9),e(F9,due),e(due,sur),e(F9,lur),e(F9,uO),e(uO,iur),e(F9,dur),e(V,cur),e(V,C9),e(C9,cue),e(cue,fur),e(C9,mur),e(C9,bO),e(bO,gur),e(C9,hur),e(V,pur),e(V,M9),e(M9,fue),e(fue,_ur),e(M9,uur),e(M9,vO),e(vO,bur),e(M9,vur),e(V,Tur),e(V,E9),e(E9,mue),e(mue,Fur),e(E9,Cur),e(E9,TO),e(TO,Mur),e(E9,Eur),e(V,yur),e(V,y9),e(y9,gue),e(gue,wur),e(y9,Aur),e(y9,FO),e(FO,Lur),e(y9,Bur),e(V,kur),e(V,w9),e(w9,hue),e(hue,xur),e(w9,Rur),e(w9,CO),e(CO,Sur),e(w9,Pur),e(V,$ur),e(V,A9),e(A9,pue),e(pue,Iur),e(A9,jur),e(A9,MO),e(MO,Nur),e(A9,Dur),e(V,qur),e(V,L9),e(L9,_ue),e(_ue,Gur),e(L9,Our),e(L9,EO),e(EO,Xur),e(L9,zur),e(V,Vur),e(V,B9),e(B9,uue),e(uue,Wur),e(B9,Qur),e(B9,yO),e(yO,Hur),e(B9,Uur),e(V,Jur),e(V,k9),e(k9,bue),e(bue,Yur),e(k9,Kur),e(k9,wO),e(wO,Zur),e(k9,e2r),e(V,o2r),e(V,x9),e(x9,vue),e(vue,r2r),e(x9,t2r),e(x9,AO),e(AO,a2r),e(x9,n2r),e(wo,s2r),e(wo,Tue),e(Tue,l2r),e(wo,i2r),g(bA,wo,null),b(d,P8e,u),b(d,Xc,u),e(Xc,R9),e(R9,Fue),g(vA,Fue,null),e(Xc,d2r),e(Xc,Cue),e(Cue,c2r),b(d,$8e,u),b(d,Ar,u),g(TA,Ar,null),e(Ar,f2r),e(Ar,zc),e(zc,m2r),e(zc,Mue),e(Mue,g2r),e(zc,h2r),e(zc,Eue),e(Eue,p2r),e(zc,_2r),e(Ar,u2r),e(Ar,FA),e(FA,b2r),e(FA,yue),e(yue,v2r),e(FA,T2r),e(Ar,F2r),e(Ar,Tt),g(CA,Tt,null),e(Tt,C2r),e(Tt,wue),e(wue,M2r),e(Tt,E2r),e(Tt,Vc),e(Vc,y2r),e(Vc,Aue),e(Aue,w2r),e(Vc,A2r),e(Vc,Lue),e(Lue,L2r),e(Vc,B2r),e(Tt,k2r),e(Tt,Bue),e(Bue,x2r),e(Tt,R2r),g(MA,Tt,null),e(Ar,S2r),e(Ar,Ao),g(EA,Ao,null),e(Ao,P2r),e(Ao,kue),e(kue,$2r),e(Ao,I2r),e(Ao,Cn),e(Cn,j2r),e(Cn,xue),e(xue,N2r),e(Cn,D2r),e(Cn,Rue),e(Rue,q2r),e(Cn,G2r),e(Cn,Sue),e(Sue,O2r),e(Cn,X2r),e(Ao,z2r),e(Ao,Mn),e(Mn,S9),e(S9,Pue),e(Pue,V2r),e(S9,W2r),e(S9,LO),e(LO,Q2r),e(S9,H2r),e(Mn,U2r),e(Mn,P9),e(P9,$ue),e($ue,J2r),e(P9,Y2r),e(P9,BO),e(BO,K2r),e(P9,Z2r),e(Mn,e1r),e(Mn,$9),e($9,Iue),e(Iue,o1r),e($9,r1r),e($9,kO),e(kO,t1r),e($9,a1r),e(Mn,n1r),e(Mn,I9),e(I9,jue),e(jue,s1r),e(I9,l1r),e(I9,xO),e(xO,i1r),e(I9,d1r),e(Ao,c1r),e(Ao,Nue),e(Nue,f1r),e(Ao,m1r),g(yA,Ao,null),b(d,I8e,u),b(d,Wc,u),e(Wc,j9),e(j9,Due),g(wA,Due,null),e(Wc,g1r),e(Wc,que),e(que,h1r),b(d,j8e,u),b(d,Lr,u),g(AA,Lr,null),e(Lr,p1r),e(Lr,Qc),e(Qc,_1r),e(Qc,Gue),e(Gue,u1r),e(Qc,b1r),e(Qc,Oue),e(Oue,v1r),e(Qc,T1r),e(Lr,F1r),e(Lr,LA),e(LA,C1r),e(LA,Xue),e(Xue,M1r),e(LA,E1r),e(Lr,y1r),e(Lr,Ft),g(BA,Ft,null),e(Ft,w1r),e(Ft,zue),e(zue,A1r),e(Ft,L1r),e(Ft,Hc),e(Hc,B1r),e(Hc,Vue),e(Vue,k1r),e(Hc,x1r),e(Hc,Wue),e(Wue,R1r),e(Hc,S1r),e(Ft,P1r),e(Ft,Que),e(Que,$1r),e(Ft,I1r),g(kA,Ft,null),e(Lr,j1r),e(Lr,Lo),g(xA,Lo,null),e(Lo,N1r),e(Lo,Hue),e(Hue,D1r),e(Lo,q1r),e(Lo,En),e(En,G1r),e(En,Uue),e(Uue,O1r),e(En,X1r),e(En,Jue),e(Jue,z1r),e(En,V1r),e(En,Yue),e(Yue,W1r),e(En,Q1r),e(Lo,H1r),e(Lo,fe),e(fe,N9),e(N9,Kue),e(Kue,U1r),e(N9,J1r),e(N9,RO),e(RO,Y1r),e(N9,K1r),e(fe,Z1r),e(fe,D9),e(D9,Zue),e(Zue,ebr),e(D9,obr),e(D9,SO),e(SO,rbr),e(D9,tbr),e(fe,abr),e(fe,q9),e(q9,e2e),e(e2e,nbr),e(q9,sbr),e(q9,PO),e(PO,lbr),e(q9,ibr),e(fe,dbr),e(fe,G9),e(G9,o2e),e(o2e,cbr),e(G9,fbr),e(G9,$O),e($O,mbr),e(G9,gbr),e(fe,hbr),e(fe,O9),e(O9,r2e),e(r2e,pbr),e(O9,_br),e(O9,IO),e(IO,ubr),e(O9,bbr),e(fe,vbr),e(fe,X9),e(X9,t2e),e(t2e,Tbr),e(X9,Fbr),e(X9,jO),e(jO,Cbr),e(X9,Mbr),e(fe,Ebr),e(fe,z9),e(z9,a2e),e(a2e,ybr),e(z9,wbr),e(z9,NO),e(NO,Abr),e(z9,Lbr),e(fe,Bbr),e(fe,V9),e(V9,n2e),e(n2e,kbr),e(V9,xbr),e(V9,DO),e(DO,Rbr),e(V9,Sbr),e(fe,Pbr),e(fe,W9),e(W9,s2e),e(s2e,$br),e(W9,Ibr),e(W9,qO),e(qO,jbr),e(W9,Nbr),e(fe,Dbr),e(fe,Q9),e(Q9,l2e),e(l2e,qbr),e(Q9,Gbr),e(Q9,GO),e(GO,Obr),e(Q9,Xbr),e(fe,zbr),e(fe,H9),e(H9,i2e),e(i2e,Vbr),e(H9,Wbr),e(H9,OO),e(OO,Qbr),e(H9,Hbr),e(Lo,Ubr),e(Lo,d2e),e(d2e,Jbr),e(Lo,Ybr),g(RA,Lo,null),b(d,N8e,u),b(d,Uc,u),e(Uc,U9),e(U9,c2e),g(SA,c2e,null),e(Uc,Kbr),e(Uc,f2e),e(f2e,Zbr),b(d,D8e,u),b(d,Br,u),g(PA,Br,null),e(Br,e5r),e(Br,Jc),e(Jc,o5r),e(Jc,m2e),e(m2e,r5r),e(Jc,t5r),e(Jc,g2e),e(g2e,a5r),e(Jc,n5r),e(Br,s5r),e(Br,$A),e($A,l5r),e($A,h2e),e(h2e,i5r),e($A,d5r),e(Br,c5r),e(Br,Ct),g(IA,Ct,null),e(Ct,f5r),e(Ct,p2e),e(p2e,m5r),e(Ct,g5r),e(Ct,Yc),e(Yc,h5r),e(Yc,_2e),e(_2e,p5r),e(Yc,_5r),e(Yc,u2e),e(u2e,u5r),e(Yc,b5r),e(Ct,v5r),e(Ct,b2e),e(b2e,T5r),e(Ct,F5r),g(jA,Ct,null),e(Br,C5r),e(Br,Bo),g(NA,Bo,null),e(Bo,M5r),e(Bo,v2e),e(v2e,E5r),e(Bo,y5r),e(Bo,yn),e(yn,w5r),e(yn,T2e),e(T2e,A5r),e(yn,L5r),e(yn,F2e),e(F2e,B5r),e(yn,k5r),e(yn,C2e),e(C2e,x5r),e(yn,R5r),e(Bo,S5r),e(Bo,ve),e(ve,J9),e(J9,M2e),e(M2e,P5r),e(J9,$5r),e(J9,XO),e(XO,I5r),e(J9,j5r),e(ve,N5r),e(ve,Y9),e(Y9,E2e),e(E2e,D5r),e(Y9,q5r),e(Y9,zO),e(zO,G5r),e(Y9,O5r),e(ve,X5r),e(ve,K9),e(K9,y2e),e(y2e,z5r),e(K9,V5r),e(K9,VO),e(VO,W5r),e(K9,Q5r),e(ve,H5r),e(ve,Z9),e(Z9,w2e),e(w2e,U5r),e(Z9,J5r),e(Z9,WO),e(WO,Y5r),e(Z9,K5r),e(ve,Z5r),e(ve,eC),e(eC,A2e),e(A2e,evr),e(eC,ovr),e(eC,QO),e(QO,rvr),e(eC,tvr),e(ve,avr),e(ve,oC),e(oC,L2e),e(L2e,nvr),e(oC,svr),e(oC,HO),e(HO,lvr),e(oC,ivr),e(ve,dvr),e(ve,rC),e(rC,B2e),e(B2e,cvr),e(rC,fvr),e(rC,UO),e(UO,mvr),e(rC,gvr),e(ve,hvr),e(ve,tC),e(tC,k2e),e(k2e,pvr),e(tC,_vr),e(tC,JO),e(JO,uvr),e(tC,bvr),e(ve,vvr),e(ve,aC),e(aC,x2e),e(x2e,Tvr),e(aC,Fvr),e(aC,YO),e(YO,Cvr),e(aC,Mvr),e(Bo,Evr),e(Bo,R2e),e(R2e,yvr),e(Bo,wvr),g(DA,Bo,null),b(d,q8e,u),b(d,Kc,u),e(Kc,nC),e(nC,S2e),g(qA,S2e,null),e(Kc,Avr),e(Kc,P2e),e(P2e,Lvr),b(d,G8e,u),b(d,kr,u),g(GA,kr,null),e(kr,Bvr),e(kr,Zc),e(Zc,kvr),e(Zc,$2e),e($2e,xvr),e(Zc,Rvr),e(Zc,I2e),e(I2e,Svr),e(Zc,Pvr),e(kr,$vr),e(kr,OA),e(OA,Ivr),e(OA,j2e),e(j2e,jvr),e(OA,Nvr),e(kr,Dvr),e(kr,Mt),g(XA,Mt,null),e(Mt,qvr),e(Mt,N2e),e(N2e,Gvr),e(Mt,Ovr),e(Mt,ef),e(ef,Xvr),e(ef,D2e),e(D2e,zvr),e(ef,Vvr),e(ef,q2e),e(q2e,Wvr),e(ef,Qvr),e(Mt,Hvr),e(Mt,G2e),e(G2e,Uvr),e(Mt,Jvr),g(zA,Mt,null),e(kr,Yvr),e(kr,ko),g(VA,ko,null),e(ko,Kvr),e(ko,O2e),e(O2e,Zvr),e(ko,eTr),e(ko,wn),e(wn,oTr),e(wn,X2e),e(X2e,rTr),e(wn,tTr),e(wn,z2e),e(z2e,aTr),e(wn,nTr),e(wn,V2e),e(V2e,sTr),e(wn,lTr),e(ko,iTr),e(ko,Te),e(Te,sC),e(sC,W2e),e(W2e,dTr),e(sC,cTr),e(sC,KO),e(KO,fTr),e(sC,mTr),e(Te,gTr),e(Te,lC),e(lC,Q2e),e(Q2e,hTr),e(lC,pTr),e(lC,ZO),e(ZO,_Tr),e(lC,uTr),e(Te,bTr),e(Te,iC),e(iC,H2e),e(H2e,vTr),e(iC,TTr),e(iC,eX),e(eX,FTr),e(iC,CTr),e(Te,MTr),e(Te,dC),e(dC,U2e),e(U2e,ETr),e(dC,yTr),e(dC,oX),e(oX,wTr),e(dC,ATr),e(Te,LTr),e(Te,cC),e(cC,J2e),e(J2e,BTr),e(cC,kTr),e(cC,rX),e(rX,xTr),e(cC,RTr),e(Te,STr),e(Te,fC),e(fC,Y2e),e(Y2e,PTr),e(fC,$Tr),e(fC,tX),e(tX,ITr),e(fC,jTr),e(Te,NTr),e(Te,mC),e(mC,K2e),e(K2e,DTr),e(mC,qTr),e(mC,aX),e(aX,GTr),e(mC,OTr),e(Te,XTr),e(Te,gC),e(gC,Z2e),e(Z2e,zTr),e(gC,VTr),e(gC,nX),e(nX,WTr),e(gC,QTr),e(Te,HTr),e(Te,hC),e(hC,e1e),e(e1e,UTr),e(hC,JTr),e(hC,sX),e(sX,YTr),e(hC,KTr),e(ko,ZTr),e(ko,o1e),e(o1e,e7r),e(ko,o7r),g(WA,ko,null),b(d,O8e,u),b(d,of,u),e(of,pC),e(pC,r1e),g(QA,r1e,null),e(of,r7r),e(of,t1e),e(t1e,t7r),b(d,X8e,u),b(d,xr,u),g(HA,xr,null),e(xr,a7r),e(xr,rf),e(rf,n7r),e(rf,a1e),e(a1e,s7r),e(rf,l7r),e(rf,n1e),e(n1e,i7r),e(rf,d7r),e(xr,c7r),e(xr,UA),e(UA,f7r),e(UA,s1e),e(s1e,m7r),e(UA,g7r),e(xr,h7r),e(xr,Et),g(JA,Et,null),e(Et,p7r),e(Et,l1e),e(l1e,_7r),e(Et,u7r),e(Et,tf),e(tf,b7r),e(tf,i1e),e(i1e,v7r),e(tf,T7r),e(tf,d1e),e(d1e,F7r),e(tf,C7r),e(Et,M7r),e(Et,c1e),e(c1e,E7r),e(Et,y7r),g(YA,Et,null),e(xr,w7r),e(xr,xo),g(KA,xo,null),e(xo,A7r),e(xo,f1e),e(f1e,L7r),e(xo,B7r),e(xo,An),e(An,k7r),e(An,m1e),e(m1e,x7r),e(An,R7r),e(An,g1e),e(g1e,S7r),e(An,P7r),e(An,h1e),e(h1e,$7r),e(An,I7r),e(xo,j7r),e(xo,Fe),e(Fe,_C),e(_C,p1e),e(p1e,N7r),e(_C,D7r),e(_C,lX),e(lX,q7r),e(_C,G7r),e(Fe,O7r),e(Fe,uC),e(uC,_1e),e(_1e,X7r),e(uC,z7r),e(uC,iX),e(iX,V7r),e(uC,W7r),e(Fe,Q7r),e(Fe,bC),e(bC,u1e),e(u1e,H7r),e(bC,U7r),e(bC,dX),e(dX,J7r),e(bC,Y7r),e(Fe,K7r),e(Fe,vC),e(vC,b1e),e(b1e,Z7r),e(vC,eFr),e(vC,cX),e(cX,oFr),e(vC,rFr),e(Fe,tFr),e(Fe,TC),e(TC,v1e),e(v1e,aFr),e(TC,nFr),e(TC,fX),e(fX,sFr),e(TC,lFr),e(Fe,iFr),e(Fe,FC),e(FC,T1e),e(T1e,dFr),e(FC,cFr),e(FC,mX),e(mX,fFr),e(FC,mFr),e(Fe,gFr),e(Fe,CC),e(CC,F1e),e(F1e,hFr),e(CC,pFr),e(CC,gX),e(gX,_Fr),e(CC,uFr),e(Fe,bFr),e(Fe,MC),e(MC,C1e),e(C1e,vFr),e(MC,TFr),e(MC,hX),e(hX,FFr),e(MC,CFr),e(Fe,MFr),e(Fe,EC),e(EC,M1e),e(M1e,EFr),e(EC,yFr),e(EC,pX),e(pX,wFr),e(EC,AFr),e(xo,LFr),e(xo,E1e),e(E1e,BFr),e(xo,kFr),g(ZA,xo,null),b(d,z8e,u),b(d,af,u),e(af,yC),e(yC,y1e),g(e6,y1e,null),e(af,xFr),e(af,w1e),e(w1e,RFr),b(d,V8e,u),b(d,Rr,u),g(o6,Rr,null),e(Rr,SFr),e(Rr,nf),e(nf,PFr),e(nf,A1e),e(A1e,$Fr),e(nf,IFr),e(nf,L1e),e(L1e,jFr),e(nf,NFr),e(Rr,DFr),e(Rr,r6),e(r6,qFr),e(r6,B1e),e(B1e,GFr),e(r6,OFr),e(Rr,XFr),e(Rr,yt),g(t6,yt,null),e(yt,zFr),e(yt,k1e),e(k1e,VFr),e(yt,WFr),e(yt,sf),e(sf,QFr),e(sf,x1e),e(x1e,HFr),e(sf,UFr),e(sf,R1e),e(R1e,JFr),e(sf,YFr),e(yt,KFr),e(yt,S1e),e(S1e,ZFr),e(yt,e9r),g(a6,yt,null),e(Rr,o9r),e(Rr,Ro),g(n6,Ro,null),e(Ro,r9r),e(Ro,P1e),e(P1e,t9r),e(Ro,a9r),e(Ro,Ln),e(Ln,n9r),e(Ln,$1e),e($1e,s9r),e(Ln,l9r),e(Ln,I1e),e(I1e,i9r),e(Ln,d9r),e(Ln,j1e),e(j1e,c9r),e(Ln,f9r),e(Ro,m9r),e(Ro,Ce),e(Ce,wC),e(wC,N1e),e(N1e,g9r),e(wC,h9r),e(wC,_X),e(_X,p9r),e(wC,_9r),e(Ce,u9r),e(Ce,AC),e(AC,D1e),e(D1e,b9r),e(AC,v9r),e(AC,uX),e(uX,T9r),e(AC,F9r),e(Ce,C9r),e(Ce,LC),e(LC,q1e),e(q1e,M9r),e(LC,E9r),e(LC,bX),e(bX,y9r),e(LC,w9r),e(Ce,A9r),e(Ce,BC),e(BC,G1e),e(G1e,L9r),e(BC,B9r),e(BC,vX),e(vX,k9r),e(BC,x9r),e(Ce,R9r),e(Ce,kC),e(kC,O1e),e(O1e,S9r),e(kC,P9r),e(kC,TX),e(TX,$9r),e(kC,I9r),e(Ce,j9r),e(Ce,xC),e(xC,X1e),e(X1e,N9r),e(xC,D9r),e(xC,FX),e(FX,q9r),e(xC,G9r),e(Ce,O9r),e(Ce,RC),e(RC,z1e),e(z1e,X9r),e(RC,z9r),e(RC,CX),e(CX,V9r),e(RC,W9r),e(Ce,Q9r),e(Ce,SC),e(SC,V1e),e(V1e,H9r),e(SC,U9r),e(SC,MX),e(MX,J9r),e(SC,Y9r),e(Ce,K9r),e(Ce,PC),e(PC,W1e),e(W1e,Z9r),e(PC,eCr),e(PC,EX),e(EX,oCr),e(PC,rCr),e(Ro,tCr),e(Ro,Q1e),e(Q1e,aCr),e(Ro,nCr),g(s6,Ro,null),b(d,W8e,u),b(d,lf,u),e(lf,$C),e($C,H1e),g(l6,H1e,null),e(lf,sCr),e(lf,U1e),e(U1e,lCr),b(d,Q8e,u),b(d,Sr,u),g(i6,Sr,null),e(Sr,iCr),e(Sr,df),e(df,dCr),e(df,J1e),e(J1e,cCr),e(df,fCr),e(df,Y1e),e(Y1e,mCr),e(df,gCr),e(Sr,hCr),e(Sr,d6),e(d6,pCr),e(d6,K1e),e(K1e,_Cr),e(d6,uCr),e(Sr,bCr),e(Sr,wt),g(c6,wt,null),e(wt,vCr),e(wt,Z1e),e(Z1e,TCr),e(wt,FCr),e(wt,cf),e(cf,CCr),e(cf,ebe),e(ebe,MCr),e(cf,ECr),e(cf,obe),e(obe,yCr),e(cf,wCr),e(wt,ACr),e(wt,rbe),e(rbe,LCr),e(wt,BCr),g(f6,wt,null),e(Sr,kCr),e(Sr,So),g(m6,So,null),e(So,xCr),e(So,tbe),e(tbe,RCr),e(So,SCr),e(So,Bn),e(Bn,PCr),e(Bn,abe),e(abe,$Cr),e(Bn,ICr),e(Bn,nbe),e(nbe,jCr),e(Bn,NCr),e(Bn,sbe),e(sbe,DCr),e(Bn,qCr),e(So,GCr),e(So,so),e(so,IC),e(IC,lbe),e(lbe,OCr),e(IC,XCr),e(IC,yX),e(yX,zCr),e(IC,VCr),e(so,WCr),e(so,jC),e(jC,ibe),e(ibe,QCr),e(jC,HCr),e(jC,wX),e(wX,UCr),e(jC,JCr),e(so,YCr),e(so,NC),e(NC,dbe),e(dbe,KCr),e(NC,ZCr),e(NC,AX),e(AX,e4r),e(NC,o4r),e(so,r4r),e(so,DC),e(DC,cbe),e(cbe,t4r),e(DC,a4r),e(DC,LX),e(LX,n4r),e(DC,s4r),e(so,l4r),e(so,qC),e(qC,fbe),e(fbe,i4r),e(qC,d4r),e(qC,BX),e(BX,c4r),e(qC,f4r),e(so,m4r),e(so,GC),e(GC,mbe),e(mbe,g4r),e(GC,h4r),e(GC,kX),e(kX,p4r),e(GC,_4r),e(so,u4r),e(so,OC),e(OC,gbe),e(gbe,b4r),e(OC,v4r),e(OC,xX),e(xX,T4r),e(OC,F4r),e(So,C4r),e(So,hbe),e(hbe,M4r),e(So,E4r),g(g6,So,null),b(d,H8e,u),b(d,ff,u),e(ff,XC),e(XC,pbe),g(h6,pbe,null),e(ff,y4r),e(ff,_be),e(_be,w4r),b(d,U8e,u),b(d,Pr,u),g(p6,Pr,null),e(Pr,A4r),e(Pr,mf),e(mf,L4r),e(mf,ube),e(ube,B4r),e(mf,k4r),e(mf,bbe),e(bbe,x4r),e(mf,R4r),e(Pr,S4r),e(Pr,_6),e(_6,P4r),e(_6,vbe),e(vbe,$4r),e(_6,I4r),e(Pr,j4r),e(Pr,At),g(u6,At,null),e(At,N4r),e(At,Tbe),e(Tbe,D4r),e(At,q4r),e(At,gf),e(gf,G4r),e(gf,Fbe),e(Fbe,O4r),e(gf,X4r),e(gf,Cbe),e(Cbe,z4r),e(gf,V4r),e(At,W4r),e(At,Mbe),e(Mbe,Q4r),e(At,H4r),g(b6,At,null),e(Pr,U4r),e(Pr,Po),g(v6,Po,null),e(Po,J4r),e(Po,Ebe),e(Ebe,Y4r),e(Po,K4r),e(Po,kn),e(kn,Z4r),e(kn,ybe),e(ybe,eMr),e(kn,oMr),e(kn,wbe),e(wbe,rMr),e(kn,tMr),e(kn,Abe),e(Abe,aMr),e(kn,nMr),e(Po,sMr),e(Po,lo),e(lo,zC),e(zC,Lbe),e(Lbe,lMr),e(zC,iMr),e(zC,RX),e(RX,dMr),e(zC,cMr),e(lo,fMr),e(lo,VC),e(VC,Bbe),e(Bbe,mMr),e(VC,gMr),e(VC,SX),e(SX,hMr),e(VC,pMr),e(lo,_Mr),e(lo,WC),e(WC,kbe),e(kbe,uMr),e(WC,bMr),e(WC,PX),e(PX,vMr),e(WC,TMr),e(lo,FMr),e(lo,QC),e(QC,xbe),e(xbe,CMr),e(QC,MMr),e(QC,$X),e($X,EMr),e(QC,yMr),e(lo,wMr),e(lo,HC),e(HC,Rbe),e(Rbe,AMr),e(HC,LMr),e(HC,IX),e(IX,BMr),e(HC,kMr),e(lo,xMr),e(lo,UC),e(UC,Sbe),e(Sbe,RMr),e(UC,SMr),e(UC,jX),e(jX,PMr),e(UC,$Mr),e(lo,IMr),e(lo,JC),e(JC,Pbe),e(Pbe,jMr),e(JC,NMr),e(JC,NX),e(NX,DMr),e(JC,qMr),e(Po,GMr),e(Po,$be),e($be,OMr),e(Po,XMr),g(T6,Po,null),b(d,J8e,u),b(d,hf,u),e(hf,YC),e(YC,Ibe),g(F6,Ibe,null),e(hf,zMr),e(hf,jbe),e(jbe,VMr),b(d,Y8e,u),b(d,$r,u),g(C6,$r,null),e($r,WMr),e($r,pf),e(pf,QMr),e(pf,Nbe),e(Nbe,HMr),e(pf,UMr),e(pf,Dbe),e(Dbe,JMr),e(pf,YMr),e($r,KMr),e($r,M6),e(M6,ZMr),e(M6,qbe),e(qbe,eEr),e(M6,oEr),e($r,rEr),e($r,Lt),g(E6,Lt,null),e(Lt,tEr),e(Lt,Gbe),e(Gbe,aEr),e(Lt,nEr),e(Lt,_f),e(_f,sEr),e(_f,Obe),e(Obe,lEr),e(_f,iEr),e(_f,Xbe),e(Xbe,dEr),e(_f,cEr),e(Lt,fEr),e(Lt,zbe),e(zbe,mEr),e(Lt,gEr),g(y6,Lt,null),e($r,hEr),e($r,$o),g(w6,$o,null),e($o,pEr),e($o,Vbe),e(Vbe,_Er),e($o,uEr),e($o,xn),e(xn,bEr),e(xn,Wbe),e(Wbe,vEr),e(xn,TEr),e(xn,Qbe),e(Qbe,FEr),e(xn,CEr),e(xn,Hbe),e(Hbe,MEr),e(xn,EEr),e($o,yEr),e($o,Ube),e(Ube,KC),e(KC,Jbe),e(Jbe,wEr),e(KC,AEr),e(KC,DX),e(DX,LEr),e(KC,BEr),e($o,kEr),e($o,Ybe),e(Ybe,xEr),e($o,REr),g(A6,$o,null),b(d,K8e,u),b(d,uf,u),e(uf,ZC),e(ZC,Kbe),g(L6,Kbe,null),e(uf,SEr),e(uf,Zbe),e(Zbe,PEr),b(d,Z8e,u),b(d,Ir,u),g(B6,Ir,null),e(Ir,$Er),e(Ir,bf),e(bf,IEr),e(bf,e5e),e(e5e,jEr),e(bf,NEr),e(bf,o5e),e(o5e,DEr),e(bf,qEr),e(Ir,GEr),e(Ir,k6),e(k6,OEr),e(k6,r5e),e(r5e,XEr),e(k6,zEr),e(Ir,VEr),e(Ir,Bt),g(x6,Bt,null),e(Bt,WEr),e(Bt,t5e),e(t5e,QEr),e(Bt,HEr),e(Bt,vf),e(vf,UEr),e(vf,a5e),e(a5e,JEr),e(vf,YEr),e(vf,n5e),e(n5e,KEr),e(vf,ZEr),e(Bt,e3r),e(Bt,s5e),e(s5e,o3r),e(Bt,r3r),g(R6,Bt,null),e(Ir,t3r),e(Ir,Io),g(S6,Io,null),e(Io,a3r),e(Io,l5e),e(l5e,n3r),e(Io,s3r),e(Io,Rn),e(Rn,l3r),e(Rn,i5e),e(i5e,i3r),e(Rn,d3r),e(Rn,d5e),e(d5e,c3r),e(Rn,f3r),e(Rn,c5e),e(c5e,m3r),e(Rn,g3r),e(Io,h3r),e(Io,P6),e(P6,e4),e(e4,f5e),e(f5e,p3r),e(e4,_3r),e(e4,qX),e(qX,u3r),e(e4,b3r),e(P6,v3r),e(P6,o4),e(o4,m5e),e(m5e,T3r),e(o4,F3r),e(o4,GX),e(GX,C3r),e(o4,M3r),e(Io,E3r),e(Io,g5e),e(g5e,y3r),e(Io,w3r),g($6,Io,null),b(d,eBe,u),b(d,Tf,u),e(Tf,r4),e(r4,h5e),g(I6,h5e,null),e(Tf,A3r),e(Tf,p5e),e(p5e,L3r),b(d,oBe,u),b(d,jr,u),g(j6,jr,null),e(jr,B3r),e(jr,Ff),e(Ff,k3r),e(Ff,_5e),e(_5e,x3r),e(Ff,R3r),e(Ff,u5e),e(u5e,S3r),e(Ff,P3r),e(jr,$3r),e(jr,N6),e(N6,I3r),e(N6,b5e),e(b5e,j3r),e(N6,N3r),e(jr,D3r),e(jr,kt),g(D6,kt,null),e(kt,q3r),e(kt,v5e),e(v5e,G3r),e(kt,O3r),e(kt,Cf),e(Cf,X3r),e(Cf,T5e),e(T5e,z3r),e(Cf,V3r),e(Cf,F5e),e(F5e,W3r),e(Cf,Q3r),e(kt,H3r),e(kt,C5e),e(C5e,U3r),e(kt,J3r),g(q6,kt,null),e(jr,Y3r),e(jr,jo),g(G6,jo,null),e(jo,K3r),e(jo,M5e),e(M5e,Z3r),e(jo,eyr),e(jo,Sn),e(Sn,oyr),e(Sn,E5e),e(E5e,ryr),e(Sn,tyr),e(Sn,y5e),e(y5e,ayr),e(Sn,nyr),e(Sn,w5e),e(w5e,syr),e(Sn,lyr),e(jo,iyr),e(jo,A5e),e(A5e,t4),e(t4,L5e),e(L5e,dyr),e(t4,cyr),e(t4,OX),e(OX,fyr),e(t4,myr),e(jo,gyr),e(jo,B5e),e(B5e,hyr),e(jo,pyr),g(O6,jo,null),rBe=!0},p(d,[u]){const X6={};u&2&&(X6.$$scope={dirty:u,ctx:d}),Bf.$set(X6);const k5e={};u&2&&(k5e.$$scope={dirty:u,ctx:d}),ih.$set(k5e);const x5e={};u&2&&(x5e.$$scope={dirty:u,ctx:d}),vh.$set(x5e)},i(d){rBe||(h(ce.$$.fragment,d),h($a.$$.fragment,d),h(nM.$$.fragment,d),h(sM.$$.fragment,d),h(Bf.$$.fragment,d),h(lM.$$.fragment,d),h(iM.$$.fragment,d),h(fM.$$.fragment,d),h(mM.$$.fragment,d),h(gM.$$.fragment,d),h(hM.$$.fragment,d),h(pM.$$.fragment,d),h(bM.$$.fragment,d),h(vM.$$.fragment,d),h(TM.$$.fragment,d),h(FM.$$.fragment,d),h(CM.$$.fragment,d),h(yM.$$.fragment,d),h(ih.$$.fragment,d),h(wM.$$.fragment,d),h(AM.$$.fragment,d),h(LM.$$.fragment,d),h(BM.$$.fragment,d),h(RM.$$.fragment,d),h(vh.$$.fragment,d),h(SM.$$.fragment,d),h(PM.$$.fragment,d),h($M.$$.fragment,d),h(IM.$$.fragment,d),h(NM.$$.fragment,d),h(DM.$$.fragment,d),h(qM.$$.fragment,d),h(GM.$$.fragment,d),h(OM.$$.fragment,d),h(XM.$$.fragment,d),h(VM.$$.fragment,d),h(WM.$$.fragment,d),h(QM.$$.fragment,d),h(HM.$$.fragment,d),h(UM.$$.fragment,d),h(JM.$$.fragment,d),h(KM.$$.fragment,d),h(ZM.$$.fragment,d),h(eE.$$.fragment,d),h(oE.$$.fragment,d),h(rE.$$.fragment,d),h(tE.$$.fragment,d),h(nE.$$.fragment,d),h(sE.$$.fragment,d),h(lE.$$.fragment,d),h(iE.$$.fragment,d),h(dE.$$.fragment,d),h(cE.$$.fragment,d),h(mE.$$.fragment,d),h(gE.$$.fragment,d),h(hE.$$.fragment,d),h(pE.$$.fragment,d),h(_E.$$.fragment,d),h(uE.$$.fragment,d),h(vE.$$.fragment,d),h(TE.$$.fragment,d),h(FE.$$.fragment,d),h(CE.$$.fragment,d),h(ME.$$.fragment,d),h(EE.$$.fragment,d),h(wE.$$.fragment,d),h(AE.$$.fragment,d),h(LE.$$.fragment,d),h(BE.$$.fragment,d),h(kE.$$.fragment,d),h(xE.$$.fragment,d),h(SE.$$.fragment,d),h(PE.$$.fragment,d),h($E.$$.fragment,d),h(IE.$$.fragment,d),h(jE.$$.fragment,d),h(NE.$$.fragment,d),h(qE.$$.fragment,d),h(GE.$$.fragment,d),h(OE.$$.fragment,d),h(XE.$$.fragment,d),h(zE.$$.fragment,d),h(VE.$$.fragment,d),h(QE.$$.fragment,d),h(HE.$$.fragment,d),h(UE.$$.fragment,d),h(JE.$$.fragment,d),h(YE.$$.fragment,d),h(KE.$$.fragment,d),h(e3.$$.fragment,d),h(o3.$$.fragment,d),h(r3.$$.fragment,d),h(t3.$$.fragment,d),h(a3.$$.fragment,d),h(n3.$$.fragment,d),h(l3.$$.fragment,d),h(i3.$$.fragment,d),h(d3.$$.fragment,d),h(c3.$$.fragment,d),h(f3.$$.fragment,d),h(m3.$$.fragment,d),h(h3.$$.fragment,d),h(p3.$$.fragment,d),h(_3.$$.fragment,d),h(u3.$$.fragment,d),h(b3.$$.fragment,d),h(v3.$$.fragment,d),h(F3.$$.fragment,d),h(C3.$$.fragment,d),h(M3.$$.fragment,d),h(E3.$$.fragment,d),h(y3.$$.fragment,d),h(w3.$$.fragment,d),h(L3.$$.fragment,d),h(B3.$$.fragment,d),h(k3.$$.fragment,d),h(x3.$$.fragment,d),h(R3.$$.fragment,d),h(S3.$$.fragment,d),h($3.$$.fragment,d),h(I3.$$.fragment,d),h(j3.$$.fragment,d),h(N3.$$.fragment,d),h(D3.$$.fragment,d),h(q3.$$.fragment,d),h(O3.$$.fragment,d),h(X3.$$.fragment,d),h(z3.$$.fragment,d),h(W3.$$.fragment,d),h(Q3.$$.fragment,d),h(H3.$$.fragment,d),h(J3.$$.fragment,d),h(Y3.$$.fragment,d),h(K3.$$.fragment,d),h(Z3.$$.fragment,d),h(ey.$$.fragment,d),h(oy.$$.fragment,d),h(ty.$$.fragment,d),h(ay.$$.fragment,d),h(ny.$$.fragment,d),h(sy.$$.fragment,d),h(ly.$$.fragment,d),h(iy.$$.fragment,d),h(cy.$$.fragment,d),h(fy.$$.fragment,d),h(my.$$.fragment,d),h(gy.$$.fragment,d),h(hy.$$.fragment,d),h(py.$$.fragment,d),h(uy.$$.fragment,d),h(by.$$.fragment,d),h(vy.$$.fragment,d),h(Ty.$$.fragment,d),h(Fy.$$.fragment,d),h(Cy.$$.fragment,d),h(Ey.$$.fragment,d),h(yy.$$.fragment,d),h(wy.$$.fragment,d),h(Ly.$$.fragment,d),h(By.$$.fragment,d),h(ky.$$.fragment,d),h(Ry.$$.fragment,d),h(Sy.$$.fragment,d),h(Py.$$.fragment,d),h($y.$$.fragment,d),h(Iy.$$.fragment,d),h(jy.$$.fragment,d),h(Dy.$$.fragment,d),h(qy.$$.fragment,d),h(Gy.$$.fragment,d),h(Oy.$$.fragment,d),h(Xy.$$.fragment,d),h(zy.$$.fragment,d),h(Wy.$$.fragment,d),h(Qy.$$.fragment,d),h(Hy.$$.fragment,d),h(Uy.$$.fragment,d),h(Jy.$$.fragment,d),h(Yy.$$.fragment,d),h(Zy.$$.fragment,d),h(ew.$$.fragment,d),h(ow.$$.fragment,d),h(rw.$$.fragment,d),h(tw.$$.fragment,d),h(aw.$$.fragment,d),h(sw.$$.fragment,d),h(lw.$$.fragment,d),h(iw.$$.fragment,d),h(dw.$$.fragment,d),h(cw.$$.fragment,d),h(fw.$$.fragment,d),h(gw.$$.fragment,d),h(hw.$$.fragment,d),h(pw.$$.fragment,d),h(_w.$$.fragment,d),h(uw.$$.fragment,d),h(bw.$$.fragment,d),h(Tw.$$.fragment,d),h(Fw.$$.fragment,d),h(Cw.$$.fragment,d),h(Mw.$$.fragment,d),h(Ew.$$.fragment,d),h(yw.$$.fragment,d),h(Aw.$$.fragment,d),h(Lw.$$.fragment,d),h(Bw.$$.fragment,d),h(kw.$$.fragment,d),h(xw.$$.fragment,d),h(Rw.$$.fragment,d),h(Pw.$$.fragment,d),h($w.$$.fragment,d),h(Iw.$$.fragment,d),h(jw.$$.fragment,d),h(Nw.$$.fragment,d),h(Dw.$$.fragment,d),h(Gw.$$.fragment,d),h(Ow.$$.fragment,d),h(Xw.$$.fragment,d),h(zw.$$.fragment,d),h(Vw.$$.fragment,d),h(Ww.$$.fragment,d),h(Hw.$$.fragment,d),h(Uw.$$.fragment,d),h(Jw.$$.fragment,d),h(Yw.$$.fragment,d),h(Kw.$$.fragment,d),h(Zw.$$.fragment,d),h(oA.$$.fragment,d),h(rA.$$.fragment,d),h(tA.$$.fragment,d),h(aA.$$.fragment,d),h(nA.$$.fragment,d),h(sA.$$.fragment,d),h(iA.$$.fragment,d),h(dA.$$.fragment,d),h(cA.$$.fragment,d),h(fA.$$.fragment,d),h(mA.$$.fragment,d),h(gA.$$.fragment,d),h(pA.$$.fragment,d),h(_A.$$.fragment,d),h(uA.$$.fragment,d),h(bA.$$.fragment,d),h(vA.$$.fragment,d),h(TA.$$.fragment,d),h(CA.$$.fragment,d),h(MA.$$.fragment,d),h(EA.$$.fragment,d),h(yA.$$.fragment,d),h(wA.$$.fragment,d),h(AA.$$.fragment,d),h(BA.$$.fragment,d),h(kA.$$.fragment,d),h(xA.$$.fragment,d),h(RA.$$.fragment,d),h(SA.$$.fragment,d),h(PA.$$.fragment,d),h(IA.$$.fragment,d),h(jA.$$.fragment,d),h(NA.$$.fragment,d),h(DA.$$.fragment,d),h(qA.$$.fragment,d),h(GA.$$.fragment,d),h(XA.$$.fragment,d),h(zA.$$.fragment,d),h(VA.$$.fragment,d),h(WA.$$.fragment,d),h(QA.$$.fragment,d),h(HA.$$.fragment,d),h(JA.$$.fragment,d),h(YA.$$.fragment,d),h(KA.$$.fragment,d),h(ZA.$$.fragment,d),h(e6.$$.fragment,d),h(o6.$$.fragment,d),h(t6.$$.fragment,d),h(a6.$$.fragment,d),h(n6.$$.fragment,d),h(s6.$$.fragment,d),h(l6.$$.fragment,d),h(i6.$$.fragment,d),h(c6.$$.fragment,d),h(f6.$$.fragment,d),h(m6.$$.fragment,d),h(g6.$$.fragment,d),h(h6.$$.fragment,d),h(p6.$$.fragment,d),h(u6.$$.fragment,d),h(b6.$$.fragment,d),h(v6.$$.fragment,d),h(T6.$$.fragment,d),h(F6.$$.fragment,d),h(C6.$$.fragment,d),h(E6.$$.fragment,d),h(y6.$$.fragment,d),h(w6.$$.fragment,d),h(A6.$$.fragment,d),h(L6.$$.fragment,d),h(B6.$$.fragment,d),h(x6.$$.fragment,d),h(R6.$$.fragment,d),h(S6.$$.fragment,d),h($6.$$.fragment,d),h(I6.$$.fragment,d),h(j6.$$.fragment,d),h(D6.$$.fragment,d),h(q6.$$.fragment,d),h(G6.$$.fragment,d),h(O6.$$.fragment,d),rBe=!0)},o(d){p(ce.$$.fragment,d),p($a.$$.fragment,d),p(nM.$$.fragment,d),p(sM.$$.fragment,d),p(Bf.$$.fragment,d),p(lM.$$.fragment,d),p(iM.$$.fragment,d),p(fM.$$.fragment,d),p(mM.$$.fragment,d),p(gM.$$.fragment,d),p(hM.$$.fragment,d),p(pM.$$.fragment,d),p(bM.$$.fragment,d),p(vM.$$.fragment,d),p(TM.$$.fragment,d),p(FM.$$.fragment,d),p(CM.$$.fragment,d),p(yM.$$.fragment,d),p(ih.$$.fragment,d),p(wM.$$.fragment,d),p(AM.$$.fragment,d),p(LM.$$.fragment,d),p(BM.$$.fragment,d),p(RM.$$.fragment,d),p(vh.$$.fragment,d),p(SM.$$.fragment,d),p(PM.$$.fragment,d),p($M.$$.fragment,d),p(IM.$$.fragment,d),p(NM.$$.fragment,d),p(DM.$$.fragment,d),p(qM.$$.fragment,d),p(GM.$$.fragment,d),p(OM.$$.fragment,d),p(XM.$$.fragment,d),p(VM.$$.fragment,d),p(WM.$$.fragment,d),p(QM.$$.fragment,d),p(HM.$$.fragment,d),p(UM.$$.fragment,d),p(JM.$$.fragment,d),p(KM.$$.fragment,d),p(ZM.$$.fragment,d),p(eE.$$.fragment,d),p(oE.$$.fragment,d),p(rE.$$.fragment,d),p(tE.$$.fragment,d),p(nE.$$.fragment,d),p(sE.$$.fragment,d),p(lE.$$.fragment,d),p(iE.$$.fragment,d),p(dE.$$.fragment,d),p(cE.$$.fragment,d),p(mE.$$.fragment,d),p(gE.$$.fragment,d),p(hE.$$.fragment,d),p(pE.$$.fragment,d),p(_E.$$.fragment,d),p(uE.$$.fragment,d),p(vE.$$.fragment,d),p(TE.$$.fragment,d),p(FE.$$.fragment,d),p(CE.$$.fragment,d),p(ME.$$.fragment,d),p(EE.$$.fragment,d),p(wE.$$.fragment,d),p(AE.$$.fragment,d),p(LE.$$.fragment,d),p(BE.$$.fragment,d),p(kE.$$.fragment,d),p(xE.$$.fragment,d),p(SE.$$.fragment,d),p(PE.$$.fragment,d),p($E.$$.fragment,d),p(IE.$$.fragment,d),p(jE.$$.fragment,d),p(NE.$$.fragment,d),p(qE.$$.fragment,d),p(GE.$$.fragment,d),p(OE.$$.fragment,d),p(XE.$$.fragment,d),p(zE.$$.fragment,d),p(VE.$$.fragment,d),p(QE.$$.fragment,d),p(HE.$$.fragment,d),p(UE.$$.fragment,d),p(JE.$$.fragment,d),p(YE.$$.fragment,d),p(KE.$$.fragment,d),p(e3.$$.fragment,d),p(o3.$$.fragment,d),p(r3.$$.fragment,d),p(t3.$$.fragment,d),p(a3.$$.fragment,d),p(n3.$$.fragment,d),p(l3.$$.fragment,d),p(i3.$$.fragment,d),p(d3.$$.fragment,d),p(c3.$$.fragment,d),p(f3.$$.fragment,d),p(m3.$$.fragment,d),p(h3.$$.fragment,d),p(p3.$$.fragment,d),p(_3.$$.fragment,d),p(u3.$$.fragment,d),p(b3.$$.fragment,d),p(v3.$$.fragment,d),p(F3.$$.fragment,d),p(C3.$$.fragment,d),p(M3.$$.fragment,d),p(E3.$$.fragment,d),p(y3.$$.fragment,d),p(w3.$$.fragment,d),p(L3.$$.fragment,d),p(B3.$$.fragment,d),p(k3.$$.fragment,d),p(x3.$$.fragment,d),p(R3.$$.fragment,d),p(S3.$$.fragment,d),p($3.$$.fragment,d),p(I3.$$.fragment,d),p(j3.$$.fragment,d),p(N3.$$.fragment,d),p(D3.$$.fragment,d),p(q3.$$.fragment,d),p(O3.$$.fragment,d),p(X3.$$.fragment,d),p(z3.$$.fragment,d),p(W3.$$.fragment,d),p(Q3.$$.fragment,d),p(H3.$$.fragment,d),p(J3.$$.fragment,d),p(Y3.$$.fragment,d),p(K3.$$.fragment,d),p(Z3.$$.fragment,d),p(ey.$$.fragment,d),p(oy.$$.fragment,d),p(ty.$$.fragment,d),p(ay.$$.fragment,d),p(ny.$$.fragment,d),p(sy.$$.fragment,d),p(ly.$$.fragment,d),p(iy.$$.fragment,d),p(cy.$$.fragment,d),p(fy.$$.fragment,d),p(my.$$.fragment,d),p(gy.$$.fragment,d),p(hy.$$.fragment,d),p(py.$$.fragment,d),p(uy.$$.fragment,d),p(by.$$.fragment,d),p(vy.$$.fragment,d),p(Ty.$$.fragment,d),p(Fy.$$.fragment,d),p(Cy.$$.fragment,d),p(Ey.$$.fragment,d),p(yy.$$.fragment,d),p(wy.$$.fragment,d),p(Ly.$$.fragment,d),p(By.$$.fragment,d),p(ky.$$.fragment,d),p(Ry.$$.fragment,d),p(Sy.$$.fragment,d),p(Py.$$.fragment,d),p($y.$$.fragment,d),p(Iy.$$.fragment,d),p(jy.$$.fragment,d),p(Dy.$$.fragment,d),p(qy.$$.fragment,d),p(Gy.$$.fragment,d),p(Oy.$$.fragment,d),p(Xy.$$.fragment,d),p(zy.$$.fragment,d),p(Wy.$$.fragment,d),p(Qy.$$.fragment,d),p(Hy.$$.fragment,d),p(Uy.$$.fragment,d),p(Jy.$$.fragment,d),p(Yy.$$.fragment,d),p(Zy.$$.fragment,d),p(ew.$$.fragment,d),p(ow.$$.fragment,d),p(rw.$$.fragment,d),p(tw.$$.fragment,d),p(aw.$$.fragment,d),p(sw.$$.fragment,d),p(lw.$$.fragment,d),p(iw.$$.fragment,d),p(dw.$$.fragment,d),p(cw.$$.fragment,d),p(fw.$$.fragment,d),p(gw.$$.fragment,d),p(hw.$$.fragment,d),p(pw.$$.fragment,d),p(_w.$$.fragment,d),p(uw.$$.fragment,d),p(bw.$$.fragment,d),p(Tw.$$.fragment,d),p(Fw.$$.fragment,d),p(Cw.$$.fragment,d),p(Mw.$$.fragment,d),p(Ew.$$.fragment,d),p(yw.$$.fragment,d),p(Aw.$$.fragment,d),p(Lw.$$.fragment,d),p(Bw.$$.fragment,d),p(kw.$$.fragment,d),p(xw.$$.fragment,d),p(Rw.$$.fragment,d),p(Pw.$$.fragment,d),p($w.$$.fragment,d),p(Iw.$$.fragment,d),p(jw.$$.fragment,d),p(Nw.$$.fragment,d),p(Dw.$$.fragment,d),p(Gw.$$.fragment,d),p(Ow.$$.fragment,d),p(Xw.$$.fragment,d),p(zw.$$.fragment,d),p(Vw.$$.fragment,d),p(Ww.$$.fragment,d),p(Hw.$$.fragment,d),p(Uw.$$.fragment,d),p(Jw.$$.fragment,d),p(Yw.$$.fragment,d),p(Kw.$$.fragment,d),p(Zw.$$.fragment,d),p(oA.$$.fragment,d),p(rA.$$.fragment,d),p(tA.$$.fragment,d),p(aA.$$.fragment,d),p(nA.$$.fragment,d),p(sA.$$.fragment,d),p(iA.$$.fragment,d),p(dA.$$.fragment,d),p(cA.$$.fragment,d),p(fA.$$.fragment,d),p(mA.$$.fragment,d),p(gA.$$.fragment,d),p(pA.$$.fragment,d),p(_A.$$.fragment,d),p(uA.$$.fragment,d),p(bA.$$.fragment,d),p(vA.$$.fragment,d),p(TA.$$.fragment,d),p(CA.$$.fragment,d),p(MA.$$.fragment,d),p(EA.$$.fragment,d),p(yA.$$.fragment,d),p(wA.$$.fragment,d),p(AA.$$.fragment,d),p(BA.$$.fragment,d),p(kA.$$.fragment,d),p(xA.$$.fragment,d),p(RA.$$.fragment,d),p(SA.$$.fragment,d),p(PA.$$.fragment,d),p(IA.$$.fragment,d),p(jA.$$.fragment,d),p(NA.$$.fragment,d),p(DA.$$.fragment,d),p(qA.$$.fragment,d),p(GA.$$.fragment,d),p(XA.$$.fragment,d),p(zA.$$.fragment,d),p(VA.$$.fragment,d),p(WA.$$.fragment,d),p(QA.$$.fragment,d),p(HA.$$.fragment,d),p(JA.$$.fragment,d),p(YA.$$.fragment,d),p(KA.$$.fragment,d),p(ZA.$$.fragment,d),p(e6.$$.fragment,d),p(o6.$$.fragment,d),p(t6.$$.fragment,d),p(a6.$$.fragment,d),p(n6.$$.fragment,d),p(s6.$$.fragment,d),p(l6.$$.fragment,d),p(i6.$$.fragment,d),p(c6.$$.fragment,d),p(f6.$$.fragment,d),p(m6.$$.fragment,d),p(g6.$$.fragment,d),p(h6.$$.fragment,d),p(p6.$$.fragment,d),p(u6.$$.fragment,d),p(b6.$$.fragment,d),p(v6.$$.fragment,d),p(T6.$$.fragment,d),p(F6.$$.fragment,d),p(C6.$$.fragment,d),p(E6.$$.fragment,d),p(y6.$$.fragment,d),p(w6.$$.fragment,d),p(A6.$$.fragment,d),p(L6.$$.fragment,d),p(B6.$$.fragment,d),p(x6.$$.fragment,d),p(R6.$$.fragment,d),p(S6.$$.fragment,d),p($6.$$.fragment,d),p(I6.$$.fragment,d),p(j6.$$.fragment,d),p(D6.$$.fragment,d),p(q6.$$.fragment,d),p(G6.$$.fragment,d),p(O6.$$.fragment,d),rBe=!1},d(d){t(J),d&&t(Ae),d&&t(ie),_(ce),d&&t(Ef),d&&t(sa),d&&t(ye),d&&t(io),d&&t(wf),_($a,d),d&&t(co),d&&t(ge),d&&t(qo),d&&t(Ia),d&&t(tLe),d&&t(Si),_(nM),d&&t(aLe),d&&t(Nn),d&&t(nLe),_(sM,d),d&&t(sLe),d&&t(z0),d&&t(lLe),_(Bf,d),d&&t(iLe),d&&t(Pi),_(lM),d&&t(dLe),d&&t(Go),_(iM),_(fM),_(mM),_(gM),d&&t(cLe),d&&t(Ii),_(hM),d&&t(fLe),d&&t(Oo),_(pM),_(bM),_(vM),_(TM),d&&t(mLe),d&&t(ji),_(FM),d&&t(gLe),d&&t(Xo),_(CM),_(yM),_(ih),_(wM),_(AM),d&&t(hLe),d&&t(Ni),_(LM),d&&t(pLe),d&&t(zo),_(BM),_(RM),_(vh),_(SM),_(PM),d&&t(_Le),d&&t(qi),_($M),d&&t(uLe),d&&t(Vo),_(IM),_(NM),_(DM),_(qM),_(GM),d&&t(bLe),d&&t(Xi),_(OM),d&&t(vLe),d&&t(Wo),_(XM),_(VM),_(WM),_(QM),_(HM),d&&t(TLe),d&&t(Wi),_(UM),d&&t(FLe),d&&t(Qo),_(JM),_(KM),_(ZM),_(eE),_(oE),d&&t(CLe),d&&t(Ui),_(rE),d&&t(MLe),d&&t(Ho),_(tE),_(nE),_(sE),_(lE),_(iE),d&&t(ELe),d&&t(Ki),_(dE),d&&t(yLe),d&&t(Uo),_(cE),_(mE),_(gE),_(hE),_(pE),d&&t(wLe),d&&t(od),_(_E),d&&t(ALe),d&&t(Jo),_(uE),_(vE),_(TE),_(FE),_(CE),d&&t(LLe),d&&t(ad),_(ME),d&&t(BLe),d&&t(Yo),_(EE),_(wE),_(AE),_(LE),_(BE),d&&t(kLe),d&&t(ld),_(kE),d&&t(xLe),d&&t(Ko),_(xE),_(SE),_(PE),_($E),_(IE),d&&t(RLe),d&&t(cd),_(jE),d&&t(SLe),d&&t(Zo),_(NE),_(qE),_(GE),_(OE),_(XE),d&&t(PLe),d&&t(gd),_(zE),d&&t($Le),d&&t(er),_(VE),_(QE),_(HE),_(UE),_(JE),d&&t(ILe),d&&t(_d),_(YE),d&&t(jLe),d&&t(or),_(KE),_(e3),_(o3),_(r3),_(t3),d&&t(NLe),d&&t(vd),_(a3),d&&t(DLe),d&&t(rr),_(n3),_(l3),_(i3),_(d3),_(c3),d&&t(qLe),d&&t(Cd),_(f3),d&&t(GLe),d&&t(tr),_(m3),_(h3),_(p3),_(_3),_(u3),d&&t(OLe),d&&t(yd),_(b3),d&&t(XLe),d&&t(ar),_(v3),_(F3),_(C3),_(M3),_(E3),d&&t(zLe),d&&t(Ld),_(y3),d&&t(VLe),d&&t(nr),_(w3),_(L3),_(B3),_(k3),_(x3),d&&t(WLe),d&&t(Rd),_(R3),d&&t(QLe),d&&t(sr),_(S3),_($3),_(I3),_(j3),_(N3),d&&t(HLe),d&&t($d),_(D3),d&&t(ULe),d&&t(lr),_(q3),_(O3),_(X3),_(z3),_(W3),d&&t(JLe),d&&t(Nd),_(Q3),d&&t(YLe),d&&t(ir),_(H3),_(J3),_(Y3),_(K3),_(Z3),d&&t(KLe),d&&t(Od),_(ey),d&&t(ZLe),d&&t(dr),_(oy),_(ty),_(ay),_(ny),_(sy),d&&t(e8e),d&&t(Wd),_(ly),d&&t(o8e),d&&t(cr),_(iy),_(cy),_(fy),_(my),_(gy),d&&t(r8e),d&&t(Ud),_(hy),d&&t(t8e),d&&t(fr),_(py),_(uy),_(by),_(vy),_(Ty),d&&t(a8e),d&&t(Kd),_(Fy),d&&t(n8e),d&&t(mr),_(Cy),_(Ey),_(yy),_(wy),_(Ly),d&&t(s8e),d&&t(oc),_(By),d&&t(l8e),d&&t(gr),_(ky),_(Ry),_(Sy),_(Py),_($y),d&&t(i8e),d&&t(ac),_(Iy),d&&t(d8e),d&&t(hr),_(jy),_(Dy),_(qy),_(Gy),_(Oy),d&&t(c8e),d&&t(lc),_(Xy),d&&t(f8e),d&&t(pr),_(zy),_(Wy),_(Qy),_(Hy),_(Uy),d&&t(m8e),d&&t(cc),_(Jy),d&&t(g8e),d&&t(_r),_(Yy),_(Zy),_(ew),_(ow),_(rw),d&&t(h8e),d&&t(gc),_(tw),d&&t(p8e),d&&t(ur),_(aw),_(sw),_(lw),_(iw),_(dw),d&&t(_8e),d&&t(_c),_(cw),d&&t(u8e),d&&t(br),_(fw),_(gw),_(hw),_(pw),_(_w),d&&t(b8e),d&&t(vc),_(uw),d&&t(v8e),d&&t(vr),_(bw),_(Tw),_(Fw),_(Cw),_(Mw),d&&t(T8e),d&&t(Cc),_(Ew),d&&t(F8e),d&&t(Tr),_(yw),_(Aw),_(Lw),_(Bw),_(kw),d&&t(C8e),d&&t(yc),_(xw),d&&t(M8e),d&&t(Fr),_(Rw),_(Pw),_($w),_(Iw),_(jw),d&&t(E8e),d&&t(Lc),_(Nw),d&&t(y8e),d&&t(Cr),_(Dw),_(Gw),_(Ow),_(Xw),_(zw),d&&t(w8e),d&&t(xc),_(Vw),d&&t(A8e),d&&t(Mr),_(Ww),_(Hw),_(Uw),_(Jw),_(Yw),d&&t(L8e),d&&t(Pc),_(Kw),d&&t(B8e),d&&t(Er),_(Zw),_(oA),_(rA),_(tA),_(aA),d&&t(k8e),d&&t(jc),_(nA),d&&t(x8e),d&&t(yr),_(sA),_(iA),_(dA),_(cA),_(fA),d&&t(R8e),d&&t(qc),_(mA),d&&t(S8e),d&&t(wr),_(gA),_(pA),_(_A),_(uA),_(bA),d&&t(P8e),d&&t(Xc),_(vA),d&&t($8e),d&&t(Ar),_(TA),_(CA),_(MA),_(EA),_(yA),d&&t(I8e),d&&t(Wc),_(wA),d&&t(j8e),d&&t(Lr),_(AA),_(BA),_(kA),_(xA),_(RA),d&&t(N8e),d&&t(Uc),_(SA),d&&t(D8e),d&&t(Br),_(PA),_(IA),_(jA),_(NA),_(DA),d&&t(q8e),d&&t(Kc),_(qA),d&&t(G8e),d&&t(kr),_(GA),_(XA),_(zA),_(VA),_(WA),d&&t(O8e),d&&t(of),_(QA),d&&t(X8e),d&&t(xr),_(HA),_(JA),_(YA),_(KA),_(ZA),d&&t(z8e),d&&t(af),_(e6),d&&t(V8e),d&&t(Rr),_(o6),_(t6),_(a6),_(n6),_(s6),d&&t(W8e),d&&t(lf),_(l6),d&&t(Q8e),d&&t(Sr),_(i6),_(c6),_(f6),_(m6),_(g6),d&&t(H8e),d&&t(ff),_(h6),d&&t(U8e),d&&t(Pr),_(p6),_(u6),_(b6),_(v6),_(T6),d&&t(J8e),d&&t(hf),_(F6),d&&t(Y8e),d&&t($r),_(C6),_(E6),_(y6),_(w6),_(A6),d&&t(K8e),d&&t(uf),_(L6),d&&t(Z8e),d&&t(Ir),_(B6),_(x6),_(R6),_(S6),_($6),d&&t(eBe),d&&t(Tf),_(I6),d&&t(oBe),d&&t(jr),_(j6),_(D6),_(q6),_(G6),_(O6)}}}const y_t={local:"auto-classes",sections:[{local:"extending-the-auto-classes",title:"Extending the Auto Classes"},{local:"transformers.AutoConfig",title:"AutoConfig"},{local:"transformers.AutoTokenizer",title:"AutoTokenizer"},{local:"transformers.AutoFeatureExtractor",title:"AutoFeatureExtractor"},{local:"transformers.AutoProcessor",title:"AutoProcessor"},{local:"transformers.AutoModel",title:"AutoModel"},{local:"transformers.AutoModelForPreTraining",title:"AutoModelForPreTraining"},{local:"transformers.AutoModelForCausalLM",title:"AutoModelForCausalLM"},{local:"transformers.AutoModelForMaskedLM",title:"AutoModelForMaskedLM"},{local:"transformers.AutoModelForSeq2SeqLM",title:"AutoModelForSeq2SeqLM"},{local:"transformers.AutoModelForSequenceClassification",title:"AutoModelForSequenceClassification"},{local:"transformers.AutoModelForMultipleChoice",title:"AutoModelForMultipleChoice"},{local:"transformers.AutoModelForNextSentencePrediction",title:"AutoModelForNextSentencePrediction"},{local:"transformers.AutoModelForTokenClassification",title:"AutoModelForTokenClassification"},{local:"transformers.AutoModelForQuestionAnswering",title:"AutoModelForQuestionAnswering"},{local:"transformers.AutoModelForTableQuestionAnswering",title:"AutoModelForTableQuestionAnswering"},{local:"transformers.AutoModelForImageClassification",title:"AutoModelForImageClassification"},{local:"transformers.AutoModelForVision2Seq",title:"AutoModelForVision2Seq"},{local:"transformers.AutoModelForAudioClassification",title:"AutoModelForAudioClassification"},{local:"transformers.AutoModelForAudioFrameClassification",title:"AutoModelForAudioFrameClassification"},{local:"transformers.AutoModelForCTC",title:"AutoModelForCTC"},{local:"transformers.AutoModelForSpeechSeq2Seq",title:"AutoModelForSpeechSeq2Seq"},{local:"transformers.AutoModelForAudioXVector",title:"AutoModelForAudioXVector"},{local:"transformers.AutoModelForMaskedImageModeling",title:"AutoModelForMaskedImageModeling"},{local:"transformers.AutoModelForObjectDetection",title:"AutoModelForObjectDetection"},{local:"transformers.AutoModelForImageSegmentation",title:"AutoModelForImageSegmentation"},{local:"transformers.AutoModelForSemanticSegmentation",title:"AutoModelForSemanticSegmentation"},{local:"transformers.TFAutoModel",title:"TFAutoModel"},{local:"transformers.TFAutoModelForPreTraining",title:"TFAutoModelForPreTraining"},{local:"transformers.TFAutoModelForCausalLM",title:"TFAutoModelForCausalLM"},{local:"transformers.TFAutoModelForImageClassification",title:"TFAutoModelForImageClassification"},{local:"transformers.TFAutoModelForMaskedLM",title:"TFAutoModelForMaskedLM"},{local:"transformers.TFAutoModelForSeq2SeqLM",title:"TFAutoModelForSeq2SeqLM"},{local:"transformers.TFAutoModelForSequenceClassification",title:"TFAutoModelForSequenceClassification"},{local:"transformers.TFAutoModelForMultipleChoice",title:"TFAutoModelForMultipleChoice"},{local:"transformers.TFAutoModelForTableQuestionAnswering",title:"TFAutoModelForTableQuestionAnswering"},{local:"transformers.TFAutoModelForTokenClassification",title:"TFAutoModelForTokenClassification"},{local:"transformers.TFAutoModelForQuestionAnswering",title:"TFAutoModelForQuestionAnswering"},{local:"transformers.TFAutoModelForVision2Seq",title:"TFAutoModelForVision2Seq"},{local:"transformers.TFAutoModelForSpeechSeq2Seq",title:"TFAutoModelForSpeechSeq2Seq"},{local:"transformers.FlaxAutoModel",title:"FlaxAutoModel"},{local:"transformers.FlaxAutoModelForCausalLM",title:"FlaxAutoModelForCausalLM"},{local:"transformers.FlaxAutoModelForPreTraining",title:"FlaxAutoModelForPreTraining"},{local:"transformers.FlaxAutoModelForMaskedLM",title:"FlaxAutoModelForMaskedLM"},{local:"transformers.FlaxAutoModelForSeq2SeqLM",title:"FlaxAutoModelForSeq2SeqLM"},{local:"transformers.FlaxAutoModelForSequenceClassification",title:"FlaxAutoModelForSequenceClassification"},{local:"transformers.FlaxAutoModelForQuestionAnswering",title:"FlaxAutoModelForQuestionAnswering"},{local:"transformers.FlaxAutoModelForTokenClassification",title:"FlaxAutoModelForTokenClassification"},{local:"transformers.FlaxAutoModelForMultipleChoice",title:"FlaxAutoModelForMultipleChoice"},{local:"transformers.FlaxAutoModelForNextSentencePrediction",title:"FlaxAutoModelForNextSentencePrediction"},{local:"transformers.FlaxAutoModelForImageClassification",title:"FlaxAutoModelForImageClassification"},{local:"transformers.FlaxAutoModelForVision2Seq",title:"FlaxAutoModelForVision2Seq"}],title:"Auto Classes"};function w_t(yi,J,Ae){let{fw:ie}=J;return yi.$$set=me=>{"fw"in me&&Ae(0,ie=me.fw)},[ie]}class S_t extends u_t{constructor(J){super();b_t(this,J,w_t,E_t,v_t,{fw:0})}}export{S_t as default,y_t as metadata};
