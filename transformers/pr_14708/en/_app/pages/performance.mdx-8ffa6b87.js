import{S as UC,i as zC,s as SC,e as s,k as h,w as m,t as l,M as NC,c as r,d as t,m as f,a as i,x as d,h as n,b as c,N as MT,F as a,g as p,y as u,q as v,o as w,B as g,v as CC}from"../chunks/vendor-6b77c823.js";import{T as GC}from"../chunks/Tip-39098574.js";import{I as _}from"../chunks/IconCopyLink-7a11ce68.js";import{C as b}from"../chunks/CodeBlock-3a8b25a8.js";import{D as LC}from"../chunks/DocNotebookDropdown-f2b55cd8.js";function MC(Wo){let y,x;return{c(){y=s("p"),x=l("Note: In order to properly clear the memory after experiments we need restart the Python kernel between experiments. Run all steps above and then just one of the experiments below.")},l($){y=r($,"P",{});var S=i(y);x=n(S,"Note: In order to properly clear the memory after experiments we need restart the Python kernel between experiments. Run all steps above and then just one of the experiments below."),S.forEach(t)},m($,S){p($,y,S),a(y,x)},d($){$&&t(y)}}}function OC(Wo){let y;return{c(){y=l(`Note that in order to use the 8-bit optimizer with an existing pretrained model a change to the embedding layer is needed.
Read [this issue](https://github.com/huggingface/transformers/issues/14819) for more information.`)},l(x){y=n(x,`Note that in order to use the 8-bit optimizer with an existing pretrained model a change to the embedding layer is needed.
Read [this issue](https://github.com/huggingface/transformers/issues/14819) for more information.`)},m(x,$){p(x,y,$)},d(x){x&&t(y)}}}function BC(Wo){let y,x,$,S,Xh,xs,x2,Yh,D2,Iu,Ro,Qh,Jh,I2,Gu,Ds,Uu,te,G2,Xo,U2,z2,Is,S2,N2,zu,Gs,Su,ae,C2,Zh,L2,M2,Kh,O2,B2,Nu,At,q2,Us,ef,V2,H2,Cu,zs,Lu,Tt,F2,Yo,W2,R2,Mu,Ss,Ou,Qo,X2,Bu,Ns,qu,Jo,Y2,Vu,Cs,Hu,Zo,Q2,Fu,Pe,jt,tf,Ls,J2,af,Z2,Wu,xt,K2,sf,e0,t0,Ru,Ms,Xu,Dt,a0,rf,s0,r0,Yu,Os,Qu,Bs,Ju,Ko,i0,Zu,qs,Ku,It,ev,$e,Gt,of,Vs,o0,lf,l0,tv,Ut,n0,el,p0,h0,av,Hs,sv,Fs,rv,tl,f0,iv,ke,zt,nf,Ws,c0,pf,m0,ov,al,d0,lv,N,u0,sl,v0,w0,hf,g0,_0,rl,b0,y0,nv,Rs,pv,Xs,hv,D,E0,ff,P0,$0,cf,k0,A0,mf,T0,j0,df,x0,D0,fv,il,I0,cv,Ae,St,uf,Ys,G0,vf,U0,mv,ol,z0,dv,Nt,S0,Qs,N0,C0,uv,se,L0,ll,M0,O0,nl,B0,q0,vv,Js,wv,Zs,gv,pl,V0,_v,Te,Ct,wf,Ks,H0,gf,F0,bv,re,W0,_f,R0,X0,bf,Y0,Q0,yv,er,Ev,tr,Pv,hl,J0,$v,ar,kv,sr,Av,fl,Z0,Tv,je,Lt,yf,rr,K0,Ef,eE,jv,cl,tE,xv,xe,Mt,Pf,ir,aE,$f,sE,Dv,Ot,rE,kf,iE,oE,Iv,or,Gv,lr,Uv,ml,lE,zv,nr,Sv,pr,Nv,dl,nE,Cv,De,Bt,Af,hr,pE,Tf,hE,Lv,ul,fE,Mv,I,cE,vl,mE,dE,wl,uE,vE,fr,wE,gE,jf,_E,bE,Ov,gl,yE,Bv,qt,qv,cr,Vv,Vt,EE,xf,PE,$E,Hv,mr,Fv,dr,Wv,_l,kE,Rv,ur,Xv,vr,Yv,bl,AE,Qv,yl,El,OT,Jv,Ie,Ht,Df,wr,TE,If,jE,Zv,ie,xE,Pl,DE,IE,$l,GE,UE,Kv,gr,ew,kl,zE,tw,_r,aw,k,SE,br,Gf,NE,CE,Al,LE,ME,yr,Uf,OE,BE,zf,qE,VE,Er,Sf,HE,FE,sw,Ft,WE,Nf,RE,XE,rw,Pr,iw,Tl,YE,ow,jl,QE,lw,Ge,Wt,Cf,$r,JE,Lf,ZE,nw,xl,KE,pw,Rt,Mf,e3,t3,Of,a3,hw,Dl,s3,fw,Il,r3,cw,Gl,i3,mw,Ue,Xt,Bf,kr,o3,qf,l3,dw,C,n3,Vf,p3,h3,Hf,f3,c3,Ff,m3,d3,uw,L,u3,Ul,v3,w3,zl,g3,_3,Wf,b3,y3,vw,Ar,ww,Sl,E3,gw,ze,Yt,Rf,Tr,P3,Xf,$3,_w,Qt,k3,Nl,A3,T3,bw,Cl,j3,yw,Se,Jt,Yf,jr,x3,Qf,D3,Ew,Ll,I3,Pw,Ne,Zt,Jf,xr,G3,Zf,U3,$w,Ml,z3,kw,Ol,Bl,S3,Dr,Kf,N3,C3,ec,L3,Aw,ql,M3,Tw,Kt,tc,O3,B3,ac,q3,jw,Ce,ea,sc,Ir,V3,rc,H3,xw,Vl,F3,Dw,oe,ic,W3,R3,oc,X3,Y3,Gr,Q3,Hl,J3,Z3,Iw,Fl,K3,Gw,E,lc,e6,t6,nc,a6,s6,pc,r6,i6,hc,o6,l6,fc,n6,p6,cc,h6,f6,mc,c6,Uw,Le,ta,dc,Ur,m6,uc,d6,zw,Me,aa,vc,zr,u6,wc,v6,Sw,Wl,w6,Nw,Sr,gc,g6,_6,Cw,Rl,b6,Lw,Xl,y6,Mw,Yl,E6,Ow,Ql,P6,Bw,Jl,$6,qw,Zl,k6,Vw,Nr,_c,A6,T6,Hw,Kl,j6,Fw,en,x6,Ww,Oe,sa,bc,Cr,D6,yc,I6,Rw,tn,G6,Xw,an,U6,Yw,Lr,Qw,sn,z6,Jw,rn,S6,Zw,Mr,Kw,on,N6,e1,Or,t1,ln,C6,a1,Br,s1,le,L6,Ec,M6,O6,Pc,B6,q6,r1,nn,V6,i1,pn,H6,o1,Be,ra,$c,qr,F6,kc,W6,l1,Vr,Hr,R6,X6,n1,ia,Y6,Fr,Q6,J6,p1,hn,Ac,Z6,h1,M,K6,Tc,eP,tP,jc,aP,sP,xc,rP,iP,f1,fn,oP,c1,cn,lP,m1,oa,Dc,Wr,Ic,nP,pP,mn,hP,fP,Rr,Xr,Gc,cP,mP,dn,dP,uP,Yr,Uc,vP,wP,un,gP,d1,vn,_P,u1,la,bP,zc,yP,EP,v1,wn,PP,w1,Qr,g1,T,$P,Sc,kP,AP,Nc,TP,jP,Cc,xP,DP,Lc,IP,GP,Mc,UP,_1,qe,na,Oc,Jr,zP,Bc,SP,b1,Ve,pa,qc,Zr,NP,Vc,CP,y1,gn,LP,E1,Kr,MP,_n,OP,P1,He,ha,Hc,ei,BP,Fc,qP,$1,bn,VP,k1,ne,ti,Wc,Rc,HP,FP,ai,WP,Xc,RP,XP,YP,si,Yc,Qc,QP,JP,ri,ZP,Jc,KP,e4,t4,ii,Zc,Kc,a4,s4,oi,r4,em,i4,o4,A1,yn,l4,T1,li,n4,ni,p4,j1,Fe,fa,tm,pi,h4,am,f4,x1,En,c4,D1,A,sm,m4,d4,rm,u4,v4,im,w4,g4,om,_4,b4,lm,y4,E4,nm,P4,I1,Pn,$4,G1,$n,k4,U1,kn,A4,z1,We,ca,pm,hi,T4,hm,j4,S1,ma,fm,x4,D4,cm,I4,N1,Re,da,mm,fi,G4,dm,U4,C1,pe,um,z4,S4,An,N4,ci,C4,L4,vm,M4,L1,Xe,ua,wm,mi,O4,gm,B4,M1,Tn,_m,q4,O1,Ye,va,bm,di,V4,ym,H4,B1,jn,Em,F4,q1,xn,W4,V1,Qe,wa,Pm,ui,R4,$m,X4,H1,Dn,Y4,F1,Je,ga,km,vi,Q4,Am,J4,W1,In,Z4,R1,Ze,_a,Tm,wi,K4,ba,jm,e5,t5,xm,a5,s5,X1,Gn,r5,Y1,Ke,ya,Dm,gi,i5,Im,o5,Q1,Un,l5,J1,O,_i,n5,Gm,p5,h5,f5,bi,c5,Um,m5,d5,u5,yi,v5,zm,w5,g5,_5,Sm,b5,Z1,zn,y5,K1,Sn,Nn,BT,eg,Ea,E5,Ei,P5,$5,tg,Cn,k5,ag,et,Pa,Nm,Pi,A5,Cm,T5,sg,Ln,j5,rg,Mn,x5,ig,B,Lm,D5,I5,Mm,G5,U5,$i,z5,Om,S5,N5,C5,Bm,L5,og,On,M5,lg,$a,O5,qm,B5,q5,ng,Bn,V5,pg,ki,hg,ka,H5,Vm,F5,W5,fg,Aa,Hm,R,qn,R5,X5,Vn,Y5,Q5,Hn,J5,Z5,Fn,K5,e$,X,Y,Wn,t$,a$,Rn,s$,r$,Xn,i$,o$,Yn,l$,n$,Q,Qn,p$,h$,Jn,f$,c$,Zn,m$,d$,Kn,u$,v$,J,ep,w$,g$,tp,_$,b$,ap,y$,E$,sp,P$,$$,Z,rp,k$,A$,ip,T$,j$,op,x$,D$,lp,I$,cg,Ta,G$,Fm,U$,z$,mg,np,S$,dg,pp,N$,ug,ja,C$,Ai,L$,M$,vg,hp,O$,wg,xa,fp,B$,Ti,q$,V$,cp,H$,ji,F$,gg,he,W$,xi,R$,X$,Di,Y$,Q$,_g,tt,Da,Wm,Ii,J$,Rm,Z$,bg,fe,K$,Xm,ek,tk,Gi,ak,sk,yg,mp,rk,Eg,at,Ia,Ym,Ui,ik,Qm,ok,Pg,dp,lk,$g,up,nk,kg,vp,pk,Ag,Ga,hk,Jm,fk,ck,Tg,st,Ua,Zm,zi,mk,Km,dk,jg,ce,uk,ed,vk,wk,td,gk,_k,xg,wp,bk,Dg,gp,yk,Ig,_p,Ek,Gg,bp,Pk,Ug,za,$k,ad,kk,Ak,zg,yp,Tk,Sg,Si,Ng,Ni,jk,sd,xk,Cg,Ep,Dk,Lg,Ci,Mg,Pp,Ik,Og,me,Gk,Li,Uk,zk,Mi,Sk,Nk,Bg,rt,Sa,rd,Oi,Ck,id,Lk,qg,Na,Mk,$p,Ok,Bk,Vg,Ca,qk,od,Vk,Hk,Hg,it,La,ld,Bi,Fk,nd,Wk,Fg,kp,Rk,Wg,Ap,Xk,Rg,qi,Xg,Tp,Yk,Yg,Ma,Qk,Vi,Jk,Zk,Qg,jp,Kk,Jg,q,e7,pd,t7,a7,hd,s7,r7,fd,i7,o7,Zg,de,l7,cd,n7,p7,md,h7,f7,Kg,Oa,c7,dd,m7,d7,e_,ue,u7,Hi,v7,w7,Fi,g7,_7,t_,ot,Ba,ud,Wi,b7,vd,y7,a_,ve,E7,Ri,P7,$7,Xi,k7,A7,s_,qa,T7,wd,j7,x7,r_,xp,D7,i_,lt,Va,gd,Yi,I7,_d,G7,o_,Dp,U7,l_,V,z7,Qi,S7,N7,bd,C7,L7,yd,M7,O7,n_,Ip,B7,p_,Ji,h_,Ha,q7,Ed,V7,H7,f_,nt,Fa,Pd,Zi,F7,$d,W7,c_,Gp,R7,m_,we,X7,Ki,Y7,Q7,eo,J7,Z7,d_,to,ao,K7,e8,u_,Wa,t8,so,a8,s8,v_,H,r8,Up,i8,o8,ro,l8,n8,io,p8,h8,w_,pt,Ra,kd,oo,f8,Ad,c8,g_,ht,Td,m8,d8,jd,u8,v8,__,Xa,xd,w8,g8,Dd,_8,b_,zp,b8,y_,lo,no,y8,E8,E_,Ya,Id,P8,$8,Sp,k8,po,Gd,A8,T8,ho,j8,Ud,x8,D8,P_,fo,co,I8,G8,$_,Np,U8,k_,G,zd,z8,S8,Sd,N8,C8,mo,L8,Nd,M8,O8,B8,Cp,q8,Cd,V8,H8,Ld,F8,A_,Lp,W8,T_,Qa,R8,uo,X8,Y8,j_,Mp,Q8,x_,Op,J8,D_,Bp,Z8,I_,Ja,K8,vo,e9,t9,G_,qp,a9,U_,Za,Md,ft,Vp,s9,r9,Od,i9,o9,Hp,l9,n9,ct,mt,Fp,p9,h9,Bd,f9,c9,Wp,m9,d9,dt,Rp,u9,v9,qd,w9,g9,Xp,_9,b9,ut,Yp,y9,E9,Vd,P9,$9,Qp,k9,z_,Jp,A9,S_,Zp,T9,N_,Kp,j9,C_,eh,x9,L_,wo,Hd,D9,I9,M_,go,O_,j,G9,Fd,U9,z9,Wd,S9,N9,Rd,C9,L9,Xd,M9,O9,Yd,B9,B_,vt,Ka,Qd,_o,q9,Jd,V9,q_,th,H9,V_,es,ah,Zd,F9,W9,R9,sh,Kd,X9,Y9,H_,wt,ts,eu,bo,Q9,tu,J9,F_,gt,as,au,yo,Z9,su,K9,W_,rh,ru,iu,eA,R_,ss,ou,tA,aA,lu,sA,X_,ih,oh,nu,rA,iA,Y_,lh,Eo,oA,Q_,rs,lA,Po,nA,pA,J_,_t,is,pu,$o,hA,hu,fA,Z_,nh,ko,ph,Ao,cA,mA,dA,fu,uA,K_,bt,os,cu,To,vA,mu,wA,eb,yt,gA,du,_A,bA,jo,yA,tb,Et,ls,uu,xo,EA,vu,PA,ab,Pt,ns,wu,Do,$A,gu,kA,sb,hh,AA,rb,fh,TA,ib,ch,jA,ob,mh,dh,qT,lb,ps,xA,Io,DA,IA,nb,uh,GA,pb,vh,UA,hb,wh,zA,fb,gh,SA,cb,ge,_u,Go,NA,CA,bu,Uo,LA,MA,yu,zo,OA,mb,P,BA,So,qA,VA,No,HA,FA,Co,WA,RA,Lo,XA,YA,Mo,QA,JA,_h,ZA,KA,db,$t,hs,Eu,Oo,eT,Pu,tT,ub,fs,aT,Bo,sT,rT,vb,cs,iT,$u,oT,lT,wb,bh,nT,gb,_e,pT,qo,hT,fT,Vo,cT,mT,_b,yh,dT,bb,kt,ms,ku,Ho,uT,Au,vT,yb,Eh,wT,Eb,Ph,gT,Pb;return xs=new _({}),Ds=new LC({props:{classNames:"absolute z-10 right-0 top-0",options:[{label:"Mixed",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/main/transformers_doc/en/performance.ipynb"},{label:"PyTorch",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/main/transformers_doc/en/pytorch/performance.ipynb"},{label:"TensorFlow",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/main/transformers_doc/en/tensorflow/performance.ipynb"},{label:"Mixed",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/transformers_doc/en/performance.ipynb"},{label:"PyTorch",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/transformers_doc/en/pytorch/performance.ipynb"},{label:"TensorFlow",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/transformers_doc/en/tensorflow/performance.ipynb"}]}}),Gs=new b({props:{code:"pip install transformers datasets accelerate nvidia-ml-py3",highlighted:"pip install transformers datasets accelerate nvidia-ml-py3"}}),zs=new b({props:{code:`import numpy as np
from datasets import Dataset


seq_len, dataset_size = 512, 512
dummy_data = {
    "input_ids": np.random.randint(100, 30000, (dataset_size, seq_len)),
    "labels": np.random.randint(0, 1, (dataset_size)),
}
ds = Dataset.from_dict(dummy_data)
ds.set_format("pt")`,highlighted:`<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> Dataset


seq_len, dataset_size = <span class="hljs-number">512</span>, <span class="hljs-number">512</span>
dummy_data = {
    <span class="hljs-string">&quot;input_ids&quot;</span>: np.random.randint(<span class="hljs-number">100</span>, <span class="hljs-number">30000</span>, (dataset_size, seq_len)),
    <span class="hljs-string">&quot;labels&quot;</span>: np.random.randint(<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, (dataset_size)),
}
ds = Dataset.from_dict(dummy_data)
ds.set_format(<span class="hljs-string">&quot;pt&quot;</span>)`}}),Ss=new b({props:{code:`from pynvml import *


def print_gpu_utilization():
    nvmlInit()
    handle = nvmlDeviceGetHandleByIndex(0)
    info = nvmlDeviceGetMemoryInfo(handle)
    print(f"GPU memory occupied: {info.used//1024**2} MB.")


def print_summary(result):
    print(f"Time: {result.metrics['train_runtime']:.2f}")
    print(f"Samples/second: {result.metrics['train_samples_per_second']:.2f}")
    print_gpu_utilization()`,highlighted:`<span class="hljs-keyword">from</span> pynvml <span class="hljs-keyword">import</span> *


<span class="hljs-keyword">def</span> <span class="hljs-title function_">print_gpu_utilization</span>():
    nvmlInit()
    handle = nvmlDeviceGetHandleByIndex(<span class="hljs-number">0</span>)
    info = nvmlDeviceGetMemoryInfo(handle)
    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;GPU memory occupied: <span class="hljs-subst">{info.used//<span class="hljs-number">1024</span>**<span class="hljs-number">2</span>}</span> MB.&quot;</span>)


<span class="hljs-keyword">def</span> <span class="hljs-title function_">print_summary</span>(<span class="hljs-params">result</span>):
    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Time: <span class="hljs-subst">{result.metrics[<span class="hljs-string">&#x27;train_runtime&#x27;</span>]:<span class="hljs-number">.2</span>f}</span>&quot;</span>)
    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Samples/second: <span class="hljs-subst">{result.metrics[<span class="hljs-string">&#x27;train_samples_per_second&#x27;</span>]:<span class="hljs-number">.2</span>f}</span>&quot;</span>)
    print_gpu_utilization()`}}),Ns=new b({props:{code:"print_gpu_utilization()",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>print_gpu_utilization()
GPU memory occupied: <span class="hljs-number">0</span> MB.`}}),Cs=new b({props:{code:`import torch


torch.ones((1, 1)).to("cuda")
print_gpu_utilization()`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch


<span class="hljs-meta">&gt;&gt;&gt; </span>torch.ones((<span class="hljs-number">1</span>, <span class="hljs-number">1</span>)).to(<span class="hljs-string">&quot;cuda&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>print_gpu_utilization()
GPU memory occupied: <span class="hljs-number">1343</span> MB.`}}),Ls=new _({}),Ms=new b({props:{code:`from transformers import AutoModelForSequenceClassification


model = AutoModelForSequenceClassification.from_pretrained("bert-large-uncased").to("cuda")
print_gpu_utilization()`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForSequenceClassification


<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;bert-large-uncased&quot;</span>).to(<span class="hljs-string">&quot;cuda&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>print_gpu_utilization()
GPU memory occupied: <span class="hljs-number">2631</span> MB.`}}),Os=new b({props:{code:"nvidia-smi",highlighted:"nvidia-smi"}}),Bs=new b({props:{code:`Tue Jan 11 08:58:05 2022
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 460.91.03    Driver Version: 460.91.03    CUDA Version: 11.2     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  Tesla V100-SXM2...  On   | 00000000:00:04.0 Off |                    0 |
| N/A   37C    P0    39W / 300W |   2631MiB / 16160MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|    0   N/A  N/A      3721      C   ...nvs/codeparrot/bin/python     2629MiB |
+-----------------------------------------------------------------------------+`,highlighted:`Tue Jan 11 08:58:05 2022
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 460.91.03    Driver Version: 460.91.03    CUDA Version: 11.2     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  Tesla V100-SXM2...  On   | 00000000:00:04.0 Off |                    0 |
| N/A   37C    P0    39W / 300W |   2631MiB / 16160MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|    0   N/A  N/A      3721      C   ...nvs/codeparrot/bin/python     2629MiB |
+-----------------------------------------------------------------------------+`}}),qs=new b({props:{code:`default_args = {
    "output_dir": "tmp",
    "evaluation_strategy": "steps",
    "num_train_epochs": 1,
    "log_level": "error",
    "report_to": "none",
}`,highlighted:`default_args = {
    <span class="hljs-string">&quot;output_dir&quot;</span>: <span class="hljs-string">&quot;tmp&quot;</span>,
    <span class="hljs-string">&quot;evaluation_strategy&quot;</span>: <span class="hljs-string">&quot;steps&quot;</span>,
    <span class="hljs-string">&quot;num_train_epochs&quot;</span>: <span class="hljs-number">1</span>,
    <span class="hljs-string">&quot;log_level&quot;</span>: <span class="hljs-string">&quot;error&quot;</span>,
    <span class="hljs-string">&quot;report_to&quot;</span>: <span class="hljs-string">&quot;none&quot;</span>,
}`}}),It=new GC({props:{$$slots:{default:[MC]},$$scope:{ctx:Wo}}}),Vs=new _({}),Hs=new b({props:{code:`from transformers import TrainingArguments, Trainer, logging

logging.set_verbosity_error()


training_args = TrainingArguments(per_device_train_batch_size=4, **default_args)
trainer = Trainer(model=model, args=training_args, train_dataset=ds)
result = trainer.train()
print_summary(result)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TrainingArguments, Trainer, logging

logging.set_verbosity_error()


training_args = TrainingArguments(per_device_train_batch_size=<span class="hljs-number">4</span>, **default_args)
trainer = Trainer(model=model, args=training_args, train_dataset=ds)
result = trainer.train()
print_summary(result)`}}),Fs=new b({props:{code:`Time: 57.82
Samples/second: 8.86
GPU memory occupied: 14949 MB.`,highlighted:`<span class="hljs-keyword">Time:</span> 57.82
Samples/second: 8.86
GPU memory occupied: 14949 MB.`}}),Ws=new _({}),Rs=new b({props:{code:`training_args = TrainingArguments(per_device_train_batch_size=1, gradient_accumulation_steps=4, **default_args)

trainer = Trainer(model=model, args=training_args, train_dataset=ds)
result = trainer.train()
print_summary(result)`,highlighted:`training_args = TrainingArguments(per_device_train_batch_size=<span class="hljs-number">1</span>, gradient_accumulation_steps=<span class="hljs-number">4</span>, **default_args)

trainer = Trainer(model=model, args=training_args, train_dataset=ds)
result = trainer.train()
print_summary(result)`}}),Xs=new b({props:{code:`Time: 66.03
Samples/second: 7.75
GPU memory occupied: 8681 MB.`,highlighted:`<span class="hljs-keyword">Time:</span> 66.03
Samples/second: 7.75
GPU memory occupied: 8681 MB.`}}),Ys=new _({}),Js=new b({props:{code:`training_args = TrainingArguments(
    per_device_train_batch_size=1, gradient_accumulation_steps=4, gradient_checkpointing=True, **default_args
)

trainer = Trainer(model=model, args=training_args, train_dataset=ds)
result = trainer.train()
print_summary(result)`,highlighted:`training_args = TrainingArguments(
    per_device_train_batch_size=<span class="hljs-number">1</span>, gradient_accumulation_steps=<span class="hljs-number">4</span>, gradient_checkpointing=<span class="hljs-literal">True</span>, **default_args
)

trainer = Trainer(model=model, args=training_args, train_dataset=ds)
result = trainer.train()
print_summary(result)`}}),Zs=new b({props:{code:`Time: 85.47
Samples/second: 5.99
GPU memory occupied: 6775 MB.`,highlighted:`<span class="hljs-keyword">Time:</span> 85.47
Samples/second: 5.99
GPU memory occupied: 6775 MB.`}}),Ks=new _({}),er=new b({props:{code:`training_args = TrainingArguments(per_device_train_batch_size=4, fp16=True, **default_args)

trainer = Trainer(model=model, args=training_args, train_dataset=ds)
result = trainer.train()
print_summary(result)`,highlighted:`training_args = TrainingArguments(per_device_train_batch_size=<span class="hljs-number">4</span>, fp16=<span class="hljs-literal">True</span>, **default_args)

trainer = Trainer(model=model, args=training_args, train_dataset=ds)
result = trainer.train()
print_summary(result)`}}),tr=new b({props:{code:`Time: 27.46
Samples/second: 18.64
GPU memory occupied: 13939 MB.`,highlighted:`<span class="hljs-keyword">Time:</span> 27.46
Samples/second: 18.64
GPU memory occupied: 13939 MB.`}}),ar=new b({props:{code:`training_args = TrainingArguments(
    per_device_train_batch_size=1,
    gradient_accumulation_steps=4,
    gradient_checkpointing=True,
    fp16=True,
    **default_args,
)

trainer = Trainer(model=model, args=training_args, train_dataset=ds)
result = trainer.train()
print_summary(result)`,highlighted:`training_args = TrainingArguments(
    per_device_train_batch_size=<span class="hljs-number">1</span>,
    gradient_accumulation_steps=<span class="hljs-number">4</span>,
    gradient_checkpointing=<span class="hljs-literal">True</span>,
    fp16=<span class="hljs-literal">True</span>,
    **default_args,
)

trainer = Trainer(model=model, args=training_args, train_dataset=ds)
result = trainer.train()
print_summary(result)`}}),sr=new b({props:{code:`Time: 50.76
Samples/second: 10.09
GPU memory occupied: 7275 MB.`,highlighted:`<span class="hljs-keyword">Time:</span> 50.76
Samples/second: 10.09
GPU memory occupied: 7275 MB.`}}),rr=new _({}),ir=new _({}),or=new b({props:{code:`training_args = TrainingArguments(per_device_train_batch_size=4, optim="adafactor", **default_args)

trainer = Trainer(model=model, args=training_args, train_dataset=ds)
result = trainer.train()
print_summary(result)`,highlighted:`training_args = TrainingArguments(per_device_train_batch_size=<span class="hljs-number">4</span>, optim=<span class="hljs-string">&quot;adafactor&quot;</span>, **default_args)

trainer = Trainer(model=model, args=training_args, train_dataset=ds)
result = trainer.train()
print_summary(result)`}}),lr=new b({props:{code:`Time: 64.31
Samples/second: 7.96
GPU memory occupied: 12295 MB.`,highlighted:`<span class="hljs-keyword">Time:</span> 64.31
Samples/second: 7.96
GPU memory occupied: 12295 MB.`}}),nr=new b({props:{code:`training_args = TrainingArguments(
    per_device_train_batch_size=1,
    gradient_accumulation_steps=4,
    gradient_checkpointing=True,
    fp16=True,
    optim="adafactor",
    **default_args,
)

trainer = Trainer(model=model, args=training_args, train_dataset=ds)
result = trainer.train()
print_summary(result)`,highlighted:`training_args = TrainingArguments(
    per_device_train_batch_size=<span class="hljs-number">1</span>,
    gradient_accumulation_steps=<span class="hljs-number">4</span>,
    gradient_checkpointing=<span class="hljs-literal">True</span>,
    fp16=<span class="hljs-literal">True</span>,
    optim=<span class="hljs-string">&quot;adafactor&quot;</span>,
    **default_args,
)

trainer = Trainer(model=model, args=training_args, train_dataset=ds)
result = trainer.train()
print_summary(result)`}}),pr=new b({props:{code:`Time: 56.54
Samples/second: 9.06
GPU memory occupied: 4847 MB.`,highlighted:`<span class="hljs-keyword">Time:</span> 56.54
Samples/second: 9.06
GPU memory occupied: 4847 MB.`}}),hr=new _({}),qt=new GC({props:{$$slots:{default:[OC]},$$scope:{ctx:Wo}}}),cr=new b({props:{code:`import bitsandbytes as bnb
from torch import nn
from transformers.trainer_pt_utils import get_parameter_names

training_args = TrainingArguments(per_device_train_batch_size=4, **default_args)

decay_parameters = get_parameter_names(model, [nn.LayerNorm])
decay_parameters = [name for name in decay_parameters if "bias" not in name]
optimizer_grouped_parameters = [
    {
        "params": [p for n, p in model.named_parameters() if n in decay_parameters],
        "weight_decay": training_args.weight_decay,
    },
    {
        "params": [p for n, p in model.named_parameters() if n not in decay_parameters],
        "weight_decay": 0.0,
    },
]

optimizer_kwargs = {
    "betas": (training_args.adam_beta1, training_args.adam_beta2),
    "eps": training_args.adam_epsilon,
}
optimizer_kwargs["lr"] = training_args.learning_rate
adam_bnb_optim = bnb.optim.Adam8bit(
    optimizer_grouped_parameters,
    betas=(training_args.adam_beta1, training_args.adam_beta2),
    eps=training_args.adam_epsilon,
    lr=training_args.learning_rate,
)`,highlighted:`<span class="hljs-keyword">import</span> bitsandbytes <span class="hljs-keyword">as</span> bnb
<span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn
<span class="hljs-keyword">from</span> transformers.trainer_pt_utils <span class="hljs-keyword">import</span> get_parameter_names

training_args = TrainingArguments(per_device_train_batch_size=<span class="hljs-number">4</span>, **default_args)

decay_parameters = get_parameter_names(model, [nn.LayerNorm])
decay_parameters = [name <span class="hljs-keyword">for</span> name <span class="hljs-keyword">in</span> decay_parameters <span class="hljs-keyword">if</span> <span class="hljs-string">&quot;bias&quot;</span> <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> name]
optimizer_grouped_parameters = [
    {
        <span class="hljs-string">&quot;params&quot;</span>: [p <span class="hljs-keyword">for</span> n, p <span class="hljs-keyword">in</span> model.named_parameters() <span class="hljs-keyword">if</span> n <span class="hljs-keyword">in</span> decay_parameters],
        <span class="hljs-string">&quot;weight_decay&quot;</span>: training_args.weight_decay,
    },
    {
        <span class="hljs-string">&quot;params&quot;</span>: [p <span class="hljs-keyword">for</span> n, p <span class="hljs-keyword">in</span> model.named_parameters() <span class="hljs-keyword">if</span> n <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> decay_parameters],
        <span class="hljs-string">&quot;weight_decay&quot;</span>: <span class="hljs-number">0.0</span>,
    },
]

optimizer_kwargs = {
    <span class="hljs-string">&quot;betas&quot;</span>: (training_args.adam_beta1, training_args.adam_beta2),
    <span class="hljs-string">&quot;eps&quot;</span>: training_args.adam_epsilon,
}
optimizer_kwargs[<span class="hljs-string">&quot;lr&quot;</span>] = training_args.learning_rate
adam_bnb_optim = bnb.optim.Adam8bit(
    optimizer_grouped_parameters,
    betas=(training_args.adam_beta1, training_args.adam_beta2),
    eps=training_args.adam_epsilon,
    lr=training_args.learning_rate,
)`}}),mr=new b({props:{code:`trainer = Trainer(model=model, args=training_args, train_dataset=ds, optimizers=(adam_bnb_optim, None))
result = trainer.train()
print_summary(result)`,highlighted:`trainer = Trainer(model=model, args=training_args, train_dataset=ds, optimizers=(adam_bnb_optim, <span class="hljs-literal">None</span>))
result = trainer.train()
print_summary(result)`}}),dr=new b({props:{code:`Time: 55.95
Samples/second: 9.15
GPU memory occupied: 13085 MB.`,highlighted:`<span class="hljs-keyword">Time:</span> 55.95
Samples/second: 9.15
GPU memory occupied: 13085 MB.`}}),ur=new b({props:{code:`training_args = TrainingArguments(
    per_device_train_batch_size=1,
    gradient_accumulation_steps=4,
    gradient_checkpointing=True,
    fp16=True,
    **default_args,
)

trainer = Trainer(model=model, args=training_args, train_dataset=ds, optimizers=(adam_bnb_optim, None))
result = trainer.train()
print_summary(result)`,highlighted:`training_args = TrainingArguments(
    per_device_train_batch_size=<span class="hljs-number">1</span>,
    gradient_accumulation_steps=<span class="hljs-number">4</span>,
    gradient_checkpointing=<span class="hljs-literal">True</span>,
    fp16=<span class="hljs-literal">True</span>,
    **default_args,
)

trainer = Trainer(model=model, args=training_args, train_dataset=ds, optimizers=(adam_bnb_optim, <span class="hljs-literal">None</span>))
result = trainer.train()
print_summary(result)`}}),vr=new b({props:{code:`Time: 49.46
Samples/second: 10.35
GPU memory occupied: 5363 MB.`,highlighted:`<span class="hljs-keyword">Time:</span> 49.46
Samples/second: 10.35
GPU memory occupied: 5363 MB.`}}),wr=new _({}),gr=new b({props:{code:`training_args = TrainingArguments(
    per_device_train_batch_size=1,
    gradient_accumulation_steps=4,
    gradient_checkpointing=True,
    fp16=True,
    **default_args,
)`,highlighted:`training_args = TrainingArguments(
    per_device_train_batch_size=<span class="hljs-number">1</span>,
    gradient_accumulation_steps=<span class="hljs-number">4</span>,
    gradient_checkpointing=<span class="hljs-literal">True</span>,
    fp16=<span class="hljs-literal">True</span>,
    **default_args,
)`}}),_r=new b({props:{code:`from accelerate import Accelerator
from torch.utils.data.dataloader import DataLoader

dataloader = DataLoader(ds, batch_size=training_args.per_device_train_batch_size)

if training_args.gradient_checkpointing:
    model.gradient_checkpointing_enable()

accelerator = Accelerator(fp16=training_args.fp16)
model, optimizer, dataloader = accelerator.prepare(model, adam_bnb_optim, dataloader)

model.train()
for step, batch in enumerate(dataloader, start=1):
    loss = model(**batch).loss
    loss = loss / training_args.gradient_accumulation_steps
    accelerator.backward(loss)
    if step % training_args.gradient_accumulation_steps == 0:
        optimizer.step()
        optimizer.zero_grad()`,highlighted:`<span class="hljs-keyword">from</span> accelerate <span class="hljs-keyword">import</span> Accelerator
<span class="hljs-keyword">from</span> torch.utils.data.dataloader <span class="hljs-keyword">import</span> DataLoader

dataloader = DataLoader(ds, batch_size=training_args.per_device_train_batch_size)

<span class="hljs-keyword">if</span> training_args.gradient_checkpointing:
    model.gradient_checkpointing_enable()

accelerator = Accelerator(fp16=training_args.fp16)
model, optimizer, dataloader = accelerator.prepare(model, adam_bnb_optim, dataloader)

model.train()
<span class="hljs-keyword">for</span> step, batch <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(dataloader, start=<span class="hljs-number">1</span>):
    loss = model(**batch).loss
    loss = loss / training_args.gradient_accumulation_steps
    accelerator.backward(loss)
    <span class="hljs-keyword">if</span> step % training_args.gradient_accumulation_steps == <span class="hljs-number">0</span>:
        optimizer.step()
        optimizer.zero_grad()`}}),Pr=new b({props:{code:"print_gpu_utilization()",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>print_gpu_utilization()
GPU memory occupied: <span class="hljs-number">5363</span> MB.`}}),$r=new _({}),kr=new _({}),Ar=new b({props:{code:"accelerate config",highlighted:"accelerate config"}}),Tr=new _({}),jr=new _({}),xr=new _({}),Ir=new _({}),Ur=new _({}),zr=new _({}),Cr=new _({}),Lr=new b({props:{code:"nvidia-smi topo -m",highlighted:'<span class="hljs-symbol">nvidia</span>-<span class="hljs-keyword">smi</span> topo -m'}}),Mr=new b({props:{code:`        GPU0    GPU1    CPU Affinity    NUMA Affinity
GPU0     X      NV2     0-23            N/A
GPU1    NV2      X      0-23            N/A`,highlighted:`        <span class="hljs-attribute">GPU0</span>    GPU1    CPU Affinity    NUMA Affinity
<span class="hljs-attribute">GPU0</span>     X      NV2     <span class="hljs-number">0</span>-<span class="hljs-number">23</span>            N/A
<span class="hljs-attribute">GPU1</span>    NV2      X      <span class="hljs-number">0</span>-<span class="hljs-number">23</span>            N/A`}}),Or=new b({props:{code:`        GPU0    GPU1    CPU Affinity    NUMA Affinity
GPU0     X      PHB     0-11            N/A
GPU1    PHB      X      0-11            N/A`,highlighted:`        <span class="hljs-attribute">GPU0</span>    GPU1    CPU Affinity    NUMA Affinity
<span class="hljs-attribute">GPU0</span>     X      PHB     <span class="hljs-number">0</span>-<span class="hljs-number">11</span>            N/A
<span class="hljs-attribute">GPU1</span>    PHB      X      <span class="hljs-number">0</span>-<span class="hljs-number">11</span>            N/A`}}),Br=new b({props:{code:`  X    = Self
  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)
  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node
  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)
  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)
  PIX  = Connection traversing at most a single PCIe bridge
  NV#  = Connection traversing a bonded set of # NVLinks`,highlighted:`  X    = Self
  SYS  = Connection traversing PCIe <span class="hljs-keyword">as</span> well <span class="hljs-keyword">as</span> <span class="hljs-keyword">the</span> SMP interconnect between NUMA nodes (e.g., QPI/UPI)
  NODE = Connection traversing PCIe <span class="hljs-keyword">as</span> well <span class="hljs-keyword">as</span> <span class="hljs-keyword">the</span> interconnect between PCIe Host Bridges <span class="hljs-keyword">within</span> <span class="hljs-keyword">a</span> NUMA node
  PHB  = Connection traversing PCIe <span class="hljs-keyword">as</span> well <span class="hljs-keyword">as</span> <span class="hljs-keyword">a</span> PCIe Host Bridge (typically <span class="hljs-keyword">the</span> CPU)
  PXB  = Connection traversing multiple PCIe bridges (<span class="hljs-keyword">without</span> traversing <span class="hljs-keyword">the</span> PCIe Host Bridge)
  PIX  = Connection traversing <span class="hljs-keyword">at</span> most <span class="hljs-keyword">a</span> single PCIe bridge
  NV<span class="hljs-comment">#  = Connection traversing a bonded set of # NVLinks</span>`}}),qr=new _({}),Qr=new b({props:{code:`# DDP w/ NVLink

rm -r /tmp/test-clm; CUDA_VISIBLE_DEVICES=0,1 python -m torch.distributed.launch \\
--nproc_per_node 2 examples/pytorch/language-modeling/run_clm.py --model_name_or_path gpt2 \\
--dataset_name wikitext --dataset_config_name wikitext-2-raw-v1 --do_train \\
--output_dir /tmp/test-clm --per_device_train_batch_size 4 --max_steps 200

{'train_runtime': 101.9003, 'train_samples_per_second': 1.963, 'epoch': 0.69}

# DDP w/o NVLink

rm -r /tmp/test-clm; CUDA_VISIBLE_DEVICES=0,1 NCCL_P2P_DISABLE=1 python -m torch.distributed.launch \\
--nproc_per_node 2 examples/pytorch/language-modeling/run_clm.py --model_name_or_path gpt2 \\
--dataset_name wikitext --dataset_config_name wikitext-2-raw-v1 --do_train
--output_dir /tmp/test-clm --per_device_train_batch_size 4 --max_steps 200

{'train_runtime': 131.4367, 'train_samples_per_second': 1.522, 'epoch': 0.69}`,highlighted:`<span class="hljs-comment"># DDP w/ NVLink</span>

<span class="hljs-string">rm</span> -<span class="hljs-string">r</span> /<span class="hljs-string">tmp</span>/<span class="hljs-string">test-clm</span>; <span class="hljs-string">CUDA_VISIBLE_DEVICES</span>=<span class="hljs-string">0</span>,<span class="hljs-string">1</span> <span class="hljs-string">python</span> -<span class="hljs-string">m</span> <span class="hljs-string">torch</span>.<span class="hljs-string">distributed</span>.<span class="hljs-string">launch</span> \\
<span class="hljs-built_in">--nproc_per_node</span> <span class="hljs-string">2</span> <span class="hljs-string">examples</span>/<span class="hljs-string">pytorch</span>/<span class="hljs-string">language-modeling</span>/<span class="hljs-string">run_clm</span>.<span class="hljs-string">py</span> <span class="hljs-built_in">--model_name_or_path</span> <span class="hljs-string">gpt2</span> \\
<span class="hljs-built_in">--dataset_name</span> <span class="hljs-string">wikitext</span> <span class="hljs-built_in">--dataset_config_name</span> <span class="hljs-string">wikitext-2-raw-v1</span> <span class="hljs-built_in">--do_train</span> \\
<span class="hljs-built_in">--output_dir</span> /<span class="hljs-string">tmp</span>/<span class="hljs-string">test-clm</span> <span class="hljs-built_in">--per_device_train_batch_size</span> <span class="hljs-string">4</span> <span class="hljs-built_in">--max_steps</span> <span class="hljs-string">200</span>

{<span class="hljs-string">&#x27;train_runtime&#x27;</span>: <span class="hljs-string">101</span>.<span class="hljs-string">9003</span>, <span class="hljs-string">&#x27;train_samples_per_second&#x27;</span>: <span class="hljs-string">1</span>.<span class="hljs-string">963</span>, <span class="hljs-string">&#x27;epoch&#x27;</span>: <span class="hljs-string">0</span>.<span class="hljs-string">69</span>}

<span class="hljs-comment"># DDP w/o NVLink</span>

<span class="hljs-string">rm</span> -<span class="hljs-string">r</span> /<span class="hljs-string">tmp</span>/<span class="hljs-string">test-clm</span>; <span class="hljs-string">CUDA_VISIBLE_DEVICES</span>=<span class="hljs-string">0</span>,<span class="hljs-string">1</span> <span class="hljs-string">NCCL_P2P_DISABLE</span>=<span class="hljs-string">1</span> <span class="hljs-string">python</span> -<span class="hljs-string">m</span> <span class="hljs-string">torch</span>.<span class="hljs-string">distributed</span>.<span class="hljs-string">launch</span> \\
<span class="hljs-built_in">--nproc_per_node</span> <span class="hljs-string">2</span> <span class="hljs-string">examples</span>/<span class="hljs-string">pytorch</span>/<span class="hljs-string">language-modeling</span>/<span class="hljs-string">run_clm</span>.<span class="hljs-string">py</span> <span class="hljs-built_in">--model_name_or_path</span> <span class="hljs-string">gpt2</span> \\
<span class="hljs-built_in">--dataset_name</span> <span class="hljs-string">wikitext</span> <span class="hljs-built_in">--dataset_config_name</span> <span class="hljs-string">wikitext-2-raw-v1</span> <span class="hljs-built_in">--do_train</span>
<span class="hljs-built_in">--output_dir</span> /<span class="hljs-string">tmp</span>/<span class="hljs-string">test-clm</span> <span class="hljs-built_in">--per_device_train_batch_size</span> <span class="hljs-string">4</span> <span class="hljs-built_in">--max_steps</span> <span class="hljs-string">200</span>

{<span class="hljs-string">&#x27;train_runtime&#x27;</span>: <span class="hljs-string">131</span>.<span class="hljs-string">4367</span>, <span class="hljs-string">&#x27;train_samples_per_second&#x27;</span>: <span class="hljs-string">1</span>.<span class="hljs-string">522</span>, <span class="hljs-string">&#x27;epoch&#x27;</span>: <span class="hljs-string">0</span>.<span class="hljs-string">69</span>}`}}),Jr=new _({}),Zr=new _({}),ei=new _({}),pi=new _({}),hi=new _({}),fi=new _({}),mi=new _({}),di=new _({}),ui=new _({}),vi=new _({}),wi=new _({}),gi=new _({}),Pi=new _({}),ki=new b({props:{code:`export BS=16
python -m torch.distributed.launch \\
    --nproc_per_node 2 examples/pytorch/text-classification/run_glue.py \\
    --model_name_or_path bert-base-cased \\
    --task_name mrpc \\
    --do_train \\
    --do_eval \\
    --max_seq_length 128 \\
    --per_device_train_batch_size $BS \\
    --learning_rate 2e-5 \\
    --num_train_epochs 3.0 \\
    --output_dir /tmp/mrpc \\
    --overwrite_output_dir \\
    --fp16`,highlighted:`export BS=<span class="hljs-number">16</span>
python -m torch<span class="hljs-selector-class">.distributed</span><span class="hljs-selector-class">.launch</span> \\
    <span class="hljs-attr">--nproc_per_node</span> <span class="hljs-number">2</span> examples/pytorch/text-classification/run_glue<span class="hljs-selector-class">.py</span> \\
    <span class="hljs-attr">--model_name_or_path</span> bert-base-cased \\
    <span class="hljs-attr">--task_name</span> mrpc \\
    <span class="hljs-attr">--do_train</span> \\
    <span class="hljs-attr">--do_eval</span> \\
    <span class="hljs-attr">--max_seq_length</span> <span class="hljs-number">128</span> \\
    <span class="hljs-attr">--per_device_train_batch_size</span> <span class="hljs-variable">$BS</span> \\
    <span class="hljs-attr">--learning_rate</span> <span class="hljs-number">2</span>e-<span class="hljs-number">5</span> \\
    <span class="hljs-attr">--num_train_epochs</span> <span class="hljs-number">3.0</span> \\
    <span class="hljs-attr">--output_dir</span> /tmp/mrpc \\
    <span class="hljs-attr">--overwrite_output_dir</span> \\
    <span class="hljs-attr">--fp16</span>`}}),Ii=new _({}),Ui=new _({}),zi=new _({}),Si=new b({props:{code:`from torch.cuda.amp import autocast
with autocast(dtype=torch.bfloat16):
    loss, outputs = ...`,highlighted:`<span class="hljs-keyword">from</span> torch.cuda.amp <span class="hljs-keyword">import</span> <span class="hljs-built_in">auto</span><span class="hljs-keyword">cast</span>
with <span class="hljs-built_in">auto</span><span class="hljs-keyword">cast</span>(dtype=torch.bfloat16):
    loss, outputs = ...`}}),Ci=new b({props:{code:`python -c 'import transformers; print(f"BF16 support is {transformers.utils.is_torch_bf16_available()}")'`,highlighted:'python -c &#x27;<span class="hljs-keyword">import</span> transformers; <span class="hljs-keyword">print</span>(f<span class="hljs-string">&quot;BF16 support is {transformers.utils.is_torch_bf16_available()}&quot;</span>)&#x27;'}}),Oi=new _({}),Bi=new _({}),qi=new b({props:{code:`import torch
torch.backends.cuda.matmul.allow_tf32 = True`,highlighted:`import torch
torch<span class="hljs-selector-class">.backends</span><span class="hljs-selector-class">.cuda</span><span class="hljs-selector-class">.matmul</span><span class="hljs-selector-class">.allow_tf32</span> = True`}}),Wi=new _({}),Yi=new _({}),Ji=new b({props:{code:"model.gradient_checkpointing_enable()",highlighted:"model.gradient_checkpointing_enable()"}}),Zi=new _({}),oo=new _({}),go=new b({props:{code:`
# DP
rm -r /tmp/test-clm; CUDA_VISIBLE_DEVICES=0,1 \\
python examples/pytorch/language-modeling/run_clm.py \\
--model_name_or_path gpt2 --dataset_name wikitext --dataset_config_name wikitext-2-raw-v1 \\
--do_train --output_dir /tmp/test-clm --per_device_train_batch_size 4 --max_steps 200

{'train_runtime': 110.5948, 'train_samples_per_second': 1.808, 'epoch': 0.69}

# DDP w/ NVlink
rm -r /tmp/test-clm; CUDA_VISIBLE_DEVICES=0,1 \\
python -m torch.distributed.launch --nproc_per_node 2 examples/pytorch/language-modeling/run_clm.py \\
--model_name_or_path gpt2 --dataset_name wikitext --dataset_config_name wikitext-2-raw-v1 \\
--do_train --output_dir /tmp/test-clm --per_device_train_batch_size 4 --max_steps 200

{'train_runtime': 101.9003, 'train_samples_per_second': 1.963, 'epoch': 0.69}

# DDP w/o NVlink
rm -r /tmp/test-clm; NCCL_P2P_DISABLE=1 CUDA_VISIBLE_DEVICES=0,1 \\
python -m torch.distributed.launch --nproc_per_node 2 examples/pytorch/language-modeling/run_clm.py \\
--model_name_or_path gpt2 --dataset_name wikitext --dataset_config_name wikitext-2-raw-v1 \\
--do_train --output_dir /tmp/test-clm --per_device_train_batch_size 4 --max_steps 200

{'train_runtime': 131.4367, 'train_samples_per_second': 1.522, 'epoch': 0.69}`,highlighted:`
<span class="hljs-comment"># DP</span>
<span class="hljs-string">rm</span> -<span class="hljs-string">r</span> /<span class="hljs-string">tmp</span>/<span class="hljs-string">test-clm</span>; <span class="hljs-string">CUDA_VISIBLE_DEVICES</span>=<span class="hljs-string">0</span>,<span class="hljs-string">1</span> \\
<span class="hljs-string">python</span> <span class="hljs-string">examples</span>/<span class="hljs-string">pytorch</span>/<span class="hljs-string">language-modeling</span>/<span class="hljs-string">run_clm</span>.<span class="hljs-string">py</span> \\
<span class="hljs-built_in">--model_name_or_path</span> <span class="hljs-string">gpt2</span> <span class="hljs-built_in">--dataset_name</span> <span class="hljs-string">wikitext</span> <span class="hljs-built_in">--dataset_config_name</span> <span class="hljs-string">wikitext-2-raw-v1</span> \\
<span class="hljs-built_in">--do_train</span> <span class="hljs-built_in">--output_dir</span> /<span class="hljs-string">tmp</span>/<span class="hljs-string">test-clm</span> <span class="hljs-built_in">--per_device_train_batch_size</span> <span class="hljs-string">4</span> <span class="hljs-built_in">--max_steps</span> <span class="hljs-string">200</span>

{<span class="hljs-string">&#x27;train_runtime&#x27;</span>: <span class="hljs-string">110</span>.<span class="hljs-string">5948</span>, <span class="hljs-string">&#x27;train_samples_per_second&#x27;</span>: <span class="hljs-string">1</span>.<span class="hljs-string">808</span>, <span class="hljs-string">&#x27;epoch&#x27;</span>: <span class="hljs-string">0</span>.<span class="hljs-string">69</span>}

<span class="hljs-comment"># DDP w/ NVlink</span>
<span class="hljs-string">rm</span> -<span class="hljs-string">r</span> /<span class="hljs-string">tmp</span>/<span class="hljs-string">test-clm</span>; <span class="hljs-string">CUDA_VISIBLE_DEVICES</span>=<span class="hljs-string">0</span>,<span class="hljs-string">1</span> \\
<span class="hljs-string">python</span> -<span class="hljs-string">m</span> <span class="hljs-string">torch</span>.<span class="hljs-string">distributed</span>.<span class="hljs-string">launch</span> <span class="hljs-built_in">--nproc_per_node</span> <span class="hljs-string">2</span> <span class="hljs-string">examples</span>/<span class="hljs-string">pytorch</span>/<span class="hljs-string">language-modeling</span>/<span class="hljs-string">run_clm</span>.<span class="hljs-string">py</span> \\
<span class="hljs-built_in">--model_name_or_path</span> <span class="hljs-string">gpt2</span> <span class="hljs-built_in">--dataset_name</span> <span class="hljs-string">wikitext</span> <span class="hljs-built_in">--dataset_config_name</span> <span class="hljs-string">wikitext-2-raw-v1</span> \\
<span class="hljs-built_in">--do_train</span> <span class="hljs-built_in">--output_dir</span> /<span class="hljs-string">tmp</span>/<span class="hljs-string">test-clm</span> <span class="hljs-built_in">--per_device_train_batch_size</span> <span class="hljs-string">4</span> <span class="hljs-built_in">--max_steps</span> <span class="hljs-string">200</span>

{<span class="hljs-string">&#x27;train_runtime&#x27;</span>: <span class="hljs-string">101</span>.<span class="hljs-string">9003</span>, <span class="hljs-string">&#x27;train_samples_per_second&#x27;</span>: <span class="hljs-string">1</span>.<span class="hljs-string">963</span>, <span class="hljs-string">&#x27;epoch&#x27;</span>: <span class="hljs-string">0</span>.<span class="hljs-string">69</span>}

<span class="hljs-comment"># DDP w/o NVlink</span>
<span class="hljs-string">rm</span> -<span class="hljs-string">r</span> /<span class="hljs-string">tmp</span>/<span class="hljs-string">test-clm</span>; <span class="hljs-string">NCCL_P2P_DISABLE</span>=<span class="hljs-string">1</span> <span class="hljs-string">CUDA_VISIBLE_DEVICES</span>=<span class="hljs-string">0</span>,<span class="hljs-string">1</span> \\
<span class="hljs-string">python</span> -<span class="hljs-string">m</span> <span class="hljs-string">torch</span>.<span class="hljs-string">distributed</span>.<span class="hljs-string">launch</span> <span class="hljs-built_in">--nproc_per_node</span> <span class="hljs-string">2</span> <span class="hljs-string">examples</span>/<span class="hljs-string">pytorch</span>/<span class="hljs-string">language-modeling</span>/<span class="hljs-string">run_clm</span>.<span class="hljs-string">py</span> \\
<span class="hljs-built_in">--model_name_or_path</span> <span class="hljs-string">gpt2</span> <span class="hljs-built_in">--dataset_name</span> <span class="hljs-string">wikitext</span> <span class="hljs-built_in">--dataset_config_name</span> <span class="hljs-string">wikitext-2-raw-v1</span> \\
<span class="hljs-built_in">--do_train</span> <span class="hljs-built_in">--output_dir</span> /<span class="hljs-string">tmp</span>/<span class="hljs-string">test-clm</span> <span class="hljs-built_in">--per_device_train_batch_size</span> <span class="hljs-string">4</span> <span class="hljs-built_in">--max_steps</span> <span class="hljs-string">200</span>

{<span class="hljs-string">&#x27;train_runtime&#x27;</span>: <span class="hljs-string">131</span>.<span class="hljs-string">4367</span>, <span class="hljs-string">&#x27;train_samples_per_second&#x27;</span>: <span class="hljs-string">1</span>.<span class="hljs-string">522</span>, <span class="hljs-string">&#x27;epoch&#x27;</span>: <span class="hljs-string">0</span>.<span class="hljs-string">69</span>}`}}),_o=new _({}),bo=new _({}),yo=new _({}),$o=new _({}),To=new _({}),xo=new _({}),Do=new _({}),Oo=new _({}),Ho=new _({}),{c(){y=s("meta"),x=h(),$=s("h1"),S=s("a"),Xh=s("span"),m(xs.$$.fragment),x2=h(),Yh=s("span"),D2=l("Performance and Scalability: How To Fit a Bigger Model and Train It Faster"),Iu=h(),Ro=s("blockquote"),Qh=s("p"),Jh=s("em"),I2=l("Or how to escape the dreaded \u201CRuntimeError: CUDA error: out of memory\u201D error."),Gu=h(),m(Ds.$$.fragment),Uu=h(),te=s("p"),G2=l("Training ever larger models can become challenging even on modern GPUs. Due to their immense size we often run out of GPU memory and training can take very long. In this section we have a look at a few tricks to reduce the memory footprint and speed up training for large models and how they are integrated in the "),Xo=s("a"),U2=l("Trainer"),z2=l(" and "),Is=s("a"),S2=l("\u{1F917} Accelerate"),N2=l(". Before we start make sure you have installed the following libraries:"),zu=h(),m(Gs.$$.fragment),Su=h(),ae=s("p"),C2=l("The "),Zh=s("code"),L2=l("nvidia-ml-py3"),M2=l(" library allows us to monitor the memory usage of the models from within Python. You might be familiar with the "),Kh=s("code"),O2=l("nvidia-smi"),B2=l(" command in the terminal - this library allows to access the same information in Python directly."),Nu=h(),At=s("p"),q2=l("Then we create some dummy data. We create random token IDs between 100 and 30000 and binary labels for a classifier. In total we get 512 sequences each with length 512 and store them in a "),Us=s("a"),ef=s("code"),V2=l("Dataset"),H2=l(" with PyTorch format."),Cu=h(),m(zs.$$.fragment),Lu=h(),Tt=s("p"),F2=l("We want to print some summary statistics for the GPU utilization and the training run with the "),Yo=s("a"),W2=l("Trainer"),R2=l(". We setup a two helper functions to do just that:"),Mu=h(),m(Ss.$$.fragment),Ou=h(),Qo=s("p"),X2=l("Let\u2019s verify that we start with a free GPU memory:"),Bu=h(),m(Ns.$$.fragment),qu=h(),Jo=s("p"),Y2=l("That looks good: the GPU memory is not occupied as we would expect before we load any models. If that\u2019s not the case on your machine make sure to stop all processes that are using GPU memory. However, not all free GPU memory can be used by the user. When a model is loaded to the GPU also the kernels are loaded which can take up 1-2GB of memory. To see how much it is we load a tiny tensor into the GPU which triggers the kernels to be loaded as well."),Vu=h(),m(Cs.$$.fragment),Hu=h(),Zo=s("p"),Q2=l("We see that the kernels alone take up 1.3GB of GPU memory. Now let\u2019s see how much space the model uses."),Fu=h(),Pe=s("h2"),jt=s("a"),tf=s("span"),m(Ls.$$.fragment),J2=h(),af=s("span"),Z2=l("Load Model"),Wu=h(),xt=s("p"),K2=l("First, we load the "),sf=s("code"),e0=l("bert-large-uncased"),t0=l(" model. We load the model weights directly to the GPU so that we can check how much space just weights use."),Ru=h(),m(Ms.$$.fragment),Xu=h(),Dt=s("p"),a0=l("We can see that the model weights alone take up 1.3 GB of the GPU memory. The exact number depends on the specific GPU you are using. Note that on newer GPUs a model can sometimes take up more space since the weights are loaded in an optimized fashion that speeds up the usage of the model. Now we can also quickly check if we get the same result as with "),rf=s("code"),s0=l("nvidia-smi"),r0=l(" CLI:"),Yu=h(),m(Os.$$.fragment),Qu=h(),m(Bs.$$.fragment),Ju=h(),Ko=s("p"),i0=l("We get the same number as before and you can also see that we are using a V100 GPU with 16GB of memory. So now we can start training the model and see how the GPU memory consumption changes. First, we set up a few standard training arguments that we will use across all our experiments:"),Zu=h(),m(qs.$$.fragment),Ku=h(),m(It.$$.fragment),ev=h(),$e=s("h2"),Gt=s("a"),of=s("span"),m(Vs.$$.fragment),o0=h(),lf=s("span"),l0=l("Vanilla Training"),tv=h(),Ut=s("p"),n0=l("As a first experiment we will use the "),el=s("a"),p0=l("Trainer"),h0=l(" and train the model without any further modifications and a batch size of 4:"),av=h(),m(Hs.$$.fragment),sv=h(),m(Fs.$$.fragment),rv=h(),tl=s("p"),f0=l("We see that already a relatively small batch size almost fills up our GPU\u2019s entire memory. However, a larger batch size can often result in faster model convergence or better end performance. So ideally we want to tune the batch size to our model\u2019s needs and not to the GPU limitations. A simple trick to effectively train larger batch size is gradient accumulation."),iv=h(),ke=s("h2"),zt=s("a"),nf=s("span"),m(Ws.$$.fragment),c0=h(),pf=s("span"),m0=l("Gradient Accumulation"),ov=h(),al=s("p"),d0=l("The idea behind gradient accumulation is to instead of calculating the gradients for the whole batch at once to do it in smaller steps. The way we do that is to calculate the gradients iteratively in smaller batches by doing a forward and backward pass through the model and accumulating the gradients in the process. When enough gradients are accumulated we run the model\u2019s optimization step. This way we can easily increase the overall batch size to numbers that would never fit into the GPU\u2019s memory. In turn, however, the added forward and backward passes can slow down the training a bit."),lv=h(),N=s("p"),u0=l("We can use gradient accumulation in the "),sl=s("a"),v0=l("Trainer"),w0=l(" by simply adding the "),hf=s("code"),g0=l("gradient_accumulation_steps"),_0=l(" argument to "),rl=s("a"),b0=l("TrainingArguments"),y0=l(". Let\u2019s see how it impacts the models memory footprint:"),nv=h(),m(Rs.$$.fragment),pv=h(),m(Xs.$$.fragment),hv=h(),D=s("p"),E0=l("We can see that the memory footprint was dramatically reduced at the cost of being only slightly slower than the vanilla run. Of course, this would change as you increase the number of accumulation steps. In general you would want to max out the GPU usage as much as possible. So in our case, the batch_size of 4 was already pretty close to the GPU\u2019s limit. If we wanted to train with a batch size of 64 we should not use "),ff=s("code"),P0=l("per_device_train_batch_size=1"),$0=l(" and "),cf=s("code"),k0=l("gradient_accumulation_steps=64"),A0=l(" but instead "),mf=s("code"),T0=l("per_device_train_batch_size=4"),j0=l(" and "),df=s("code"),x0=l("gradient_accumulation_steps=16"),D0=l(" which has the same effective batch size while making better use of the available GPU resources."),fv=h(),il=s("p"),I0=l("Next we have a look at another trick to save a little bit more GPU memory called gradient checkpointing."),cv=h(),Ae=s("h2"),St=s("a"),uf=s("span"),m(Ys.$$.fragment),G0=h(),vf=s("span"),U0=l("Gradient Checkpointing"),mv=h(),ol=s("p"),z0=l("Even when we set the batch size to 1 and use gradient accumulation we can still run out of memory when working with large models. In order to compute the gradients during the backward pass all activations from the forward pass are normally saved. This can create a big memory overhead. Alternatively, one could forget all activations during the forward pass and recompute them on demand during the backward pass. This would however add a significant computational overhead and slow down training."),dv=h(),Nt=s("p"),S0=l("Gradient checkpointing strikes a compromise between the two approaches and saves strategically selected activations throughout the computational graph so only a fraction of the activations need to be re-computed for the gradients. See "),Qs=s("a"),N0=l("this great article"),C0=l(" explaining the ideas behind gradient checkpointing."),uv=h(),se=s("p"),L0=l("To enable gradient checkpointing in the "),ll=s("a"),M0=l("Trainer"),O0=l(" we only need ot pass it as a flag to the "),nl=s("a"),B0=l("TrainingArguments"),q0=l(". Everything else is handled under the hood:"),vv=h(),m(Js.$$.fragment),wv=h(),m(Zs.$$.fragment),gv=h(),pl=s("p"),V0=l("We can see that this saved some more memory but at the same time training became a bit slower. A general rule of thumb is that gradient checkpointing slows down training by about 20%. Let\u2019s have a look at another method with which we can regain some speed: mixed precision training."),_v=h(),Te=s("h2"),Ct=s("a"),wf=s("span"),m(Ks.$$.fragment),H0=h(),gf=s("span"),F0=l("FP16 Training"),bv=h(),re=s("p"),W0=l("The idea of mixed precision training is that no all variables need to be stored in full (32-bit) floating point precision. If we can reduce the precision the variales and their computations are faster. The main advantage comes from saving the activations in half (16-bit) precision. Although the gradients are also computed in half precision they are converted back to full precision for the optimization step so no memory is saved here. Since the model is present on the GPU in both 16-bit and 32-bit precision this can use more GPU memory (1.5x the original model is on the GPU), especially for small batch sizes. Since some computations are performed in full and some in half precision this approach is also called mixed precision training. Enabling mixed precision training is also just a matter of setting the "),_f=s("code"),R0=l("fp16"),X0=l(" flag to "),bf=s("code"),Y0=l("True"),Q0=l(":"),yv=h(),m(er.$$.fragment),Ev=h(),m(tr.$$.fragment),Pv=h(),hl=s("p"),J0=l("We can see that this is almost twice as fast as the vanilla training. Let\u2019s add it to the mix of the previous methods:"),$v=h(),m(ar.$$.fragment),kv=h(),m(sr.$$.fragment),Av=h(),fl=s("p"),Z0=l("We can see that with these tweaks we use about half the GPU memory as at the beginning while also being slightly faster. But we are not done, yet! There is another area where we can save GPU memory: the optimizer."),Tv=h(),je=s("h2"),Lt=s("a"),yf=s("span"),m(rr.$$.fragment),K0=h(),Ef=s("span"),eE=l("Optimizer"),jv=h(),cl=s("p"),tE=l("The most common optimizer used to train transformer model is Adam or AdamW (Adam with weight decay). Adam achieves good convergence by storing the rolling average of the previous gradients which, however, adds an additional memory footprint of the order of the number of model parameters. One remedy to this is to use an alternative optimizer such as Adafactor."),xv=h(),xe=s("h3"),Mt=s("a"),Pf=s("span"),m(ir.$$.fragment),aE=h(),$f=s("span"),sE=l("Adafactor"),Dv=h(),Ot=s("p"),rE=l("Instead of keeping the rolling average for each element in the weight matrices Adafactor only stores aggregated information (row- and column-wise sums of the rolling averages) which reduces the footprint considerably. One downside of Adafactor is that in some instances convergence can be slower than Adam\u2019s so some experimentation is advised here. We can use Adafactor simply by setting "),kf=s("code"),iE=l('optim="adafactor"'),oE=l(":"),Iv=h(),m(or.$$.fragment),Gv=h(),m(lr.$$.fragment),Uv=h(),ml=s("p"),lE=l("We can see that this saves a few more GB on the GPU. Let\u2019s see how it looks when we add it to the other methods we introduced earlier:"),zv=h(),m(nr.$$.fragment),Sv=h(),m(pr.$$.fragment),Nv=h(),dl=s("p"),nE=l("We went from 15 GB memory usage to 5 GB - a 3x improvement while maintaining the throughput! However, as mentioned before, the convergence of Adafactor can be worse than Adam. There is an alternative to Adafactor called 8-bit Adam that takes a slightly different approach."),Cv=h(),De=s("h3"),Bt=s("a"),Af=s("span"),m(hr.$$.fragment),pE=h(),Tf=s("span"),hE=l("8-bit Adam"),Lv=h(),ul=s("p"),fE=l("Instead of aggregating optimizer states like Adafactor, 8-bit Adam keeps the full state and quantizes it. Quantization means that it stores the state with lower precision and dequantizes it only for the optimization. This is similar to the idea behind FP16 training where using variables with lower precision saves memory."),Mv=h(),I=s("p"),cE=l("In contrast to the previous approaches is this one not integrated into the "),vl=s("a"),mE=l("Trainer"),dE=l(" as a simple flag. We need to install the 8-bit optimizer and then pass it as a custom optimizer to the "),wl=s("a"),uE=l("Trainer"),vE=l(". Follow the installation guide in the Github "),fr=s("a"),wE=l("repo"),gE=l(" to install the "),jf=s("code"),_E=l("bitsandbytes"),bE=l(" library that implements the 8-bit Adam optimizer."),Ov=h(),gl=s("p"),yE=l("Once installed, we just need to initialize the the optimizer. Although this looks like a considerable amount of work it actually just involves two steps: first we need to group the model\u2019s parameters into two groups where to one group we apply weight decay and to the other we don\u2019t. Usually, biases and layer norm parameters are not weight decayed. Then in a second step we just do some argument housekeeping to use the same parameters as the previously used AdamW optimizer."),Bv=h(),m(qt.$$.fragment),qv=h(),m(cr.$$.fragment),Vv=h(),Vt=s("p"),EE=l("We can now pass the custom optimizer as an argument to the "),xf=s("code"),PE=l("Trainer"),$E=l(":"),Hv=h(),m(mr.$$.fragment),Fv=h(),m(dr.$$.fragment),Wv=h(),_l=s("p"),kE=l("We can see that we get a similar memory improvement as with Adafactor while keeping the full rolling average of the gradients. Let\u2019s repeat the experiment with the full settings:"),Rv=h(),m(ur.$$.fragment),Xv=h(),m(vr.$$.fragment),Yv=h(),bl=s("p"),AE=l("Again, we get about a 3x memory improvement and even slightly higher throughput as using Adafactor. So we have seen how we can optimize the memory footprint of large models. The following plot summarizes all our experiments:"),Qv=h(),yl=s("p"),El=s("img"),Jv=h(),Ie=s("h2"),Ht=s("a"),Df=s("span"),m(wr.$$.fragment),TE=h(),If=s("span"),jE=l("Using \u{1F917} Accelerate"),Zv=h(),ie=s("p"),xE=l("So far we have used the "),Pl=s("a"),DE=l("Trainer"),IE=l(" to run the experiments but a more flexible alternative to that approach is to use \u{1F917} Accelerate. With \u{1F917} Accelerate you have full control over the training loop and can essentially write the loop in pure PyTorch with some minor modifications. In turn it allows you to easily scale across different infrastructures such as CPUs, GPUs, TPUs, or distributed multi-GPU setups without changing any code. Let\u2019s see what it takes to implement all of the above tweaks in \u{1F917} Accelerate. We can still use the "),$l=s("a"),GE=l("TrainingArguments"),UE=l(" to wrap the training settings:"),Kv=h(),m(gr.$$.fragment),ew=h(),kl=s("p"),zE=l("The full example training loop with \u{1F917} Accelerate is only a handful of lines of code long:"),tw=h(),m(_r.$$.fragment),aw=h(),k=s("p"),SE=l("First we wrap the dataset in a "),br=s("a"),Gf=s("code"),NE=l("DataLoader"),CE=l(". Then we can enable gradient checkpointing by calling the model\u2019s "),Al=s("a"),LE=l("gradient_checkpointing_enable()"),ME=l(" method. When we initialize the "),yr=s("a"),Uf=s("code"),OE=l("Accelerator"),BE=l(" we can specifiy if we want to use mixed precision training and it will take care of it for us in the "),zf=s("code"),qE=l("prepare"),VE=l(" call. During the "),Er=s("a"),Sf=s("code"),HE=l("prepare"),FE=l(" call the dataloader will also be distributed across workers should we use multiple GPUs. We use the same 8-bit optimizer from the earlier experiments."),sw=h(),Ft=s("p"),WE=l("Finally, we can write the main training loop. Note that the "),Nf=s("code"),RE=l("backward"),XE=l(" call is handled by \u{1F917} Accelerate. We can also see how gradient accumulation works: we normalize the loss so we get the average at the end of accumulation and once we have enough steps we run the optimization. Now the question is: does this use the same amount of memory as the previous steps? Let\u2019s check:"),rw=h(),m(Pr.$$.fragment),iw=h(),Tl=s("p"),YE=l("Indeed it does. Implementing these optimization techniques with \u{1F917} Accelerate only takes a handful of lines of code and comes with the benefit of more flexiblity in the training loop."),ow=h(),jl=s("p"),QE=l("Now, let\u2019s take a step back and discuss what we should optimize for when scaling the training of large models."),lw=h(),Ge=s("h2"),Wt=s("a"),Cf=s("span"),m($r.$$.fragment),JE=h(),Lf=s("span"),ZE=l("How to scale"),nw=h(),xl=s("p"),KE=l("When we train models there are a two aspects we want to optimize at the same time:"),pw=h(),Rt=s("ul"),Mf=s("li"),e3=l("Data throughput/training time"),t3=h(),Of=s("li"),a3=l("Model performance"),hw=h(),Dl=s("p"),s3=l("We have seen that each method changes the memory usage and throughput. In general we want to maximize the throughput (samples/second) to minimize the training cost. This is generally achieved by utilizing the GPU as much as possible and thus filling GPU memory to its limit. For example, as mentioned earlier, we only employ gradient accumulation when we want to use a batch size beyond the size of the GPU memory. If the desired batch size fits into memory then there is no reason to apply gradient accumulation which will only slow down training."),fw=h(),Il=s("p"),r3=l("The second objective is model performance. Just because we can does not mean we should use a large batch size. As part of hyperparameter tuning you should determine which batch size yields the best result and then optimize the throughput accordingly."),cw=h(),Gl=s("p"),i3=l("Sometimes, even when applying all the above tweaks the throughput on a given GPU might still not be good enough. One easy solution is to change the type of GPU. For example switching from let\u2019s say a K80 (which you typically get on Google Colab) to a fancier GPU such as the V100 or A100. Although they are more expensive they are usually more cost effective than cheaper GPUs due to their larger memory and faster architecture. For some applications, such as pretraining, this might still not be fast enough. In this case you want to scale your experiment to several GPUs."),mw=h(),Ue=s("h2"),Xt=s("a"),Bf=s("span"),m(kr.$$.fragment),o3=h(),qf=s("span"),l3=l("Multi-GPU Training"),dw=h(),C=s("p"),n3=l("If your model fits on a single GPU scaling to many GPUs can be achieved fairly easily with data parallelism. The idea is very similar to gradient accumulation with the distinction that instead of running the forward and backward passes during the accumulation in sequence on a single machine they are performed in parallel on multiple machines. So each GPU gets a small batch, runs the forward and backward passes and then the gradients from all machines are aggregated and the model is optimized. You can combine this with all the methods we described before. For example, if you have 4 GPUs and use "),Vf=s("code"),p3=l("per_device_train_batch_size=12"),h3=l(" and "),Hf=s("code"),f3=l("gradient_accumulation_steps=3"),c3=l(" you will have an effective batch size of "),Ff=s("code"),m3=l("4*12*3=144"),d3=l("."),uw=h(),L=s("p"),u3=l("The "),Ul=s("a"),v3=l("Trainer"),w3=l(" allows for distributed training and if you execute your "),zl=s("a"),g3=l("Trainer"),_3=l(" training script on a machine with multiple GPUs it will automatically utilize all of them, hence the name "),Wf=s("code"),b3=l("per_device_train_batch_size"),y3=l(". In \u{1F917} Accelerate you can configure the infrastructure setup with the following command:"),vw=h(),m(Ar.$$.fragment),ww=h(),Sl=s("p"),E3=l("Until now we have opperated under the assumption that we can fit the model onto a single GPU without or with the introduced tricks . But what if this is not possible? We still have a few tricks up our sleeves!"),gw=h(),ze=s("h2"),Yt=s("a"),Rf=s("span"),m(Tr.$$.fragment),P3=h(),Xf=s("span"),$3=l("What if my model still does not fit?"),_w=h(),Qt=s("p"),k3=l("If the model does not fit on a single GPU with all the mentioned tricks there are still more methods we can apply although life starts to get a bit more complicated. This usually involves some form of pipeline or tensor parallelism where the model itself is distributed across several GPUs. One can also make use of DeepSpeed which implements some of these parallelism strategies along with some more optimization to reduce the memory footprint such as partitioning the optimizer states. You can read more about this in the "),Nl=s("a"),A3=l("\u201CModel Parallelism\u201D section"),T3=l("."),bw=h(),Cl=s("p"),j3=l("This concludes the practical part of this guide for scaling the training of large models. The following section goes into more details on some of the aspects discussed above."),yw=h(),Se=s("h2"),Jt=s("a"),Yf=s("span"),m(jr.$$.fragment),x3=h(),Qf=s("span"),D3=l("Further discussions"),Ew=h(),Ll=s("p"),I3=l("This section gives brief ideas on how to make training faster and support bigger models. Later sections will expand, demonstrate and elucidate each of these."),Pw=h(),Ne=s("h2"),Zt=s("a"),Jf=s("span"),m(xr.$$.fragment),G3=h(),Zf=s("span"),U3=l("Faster Training"),$w=h(),Ml=s("p"),z3=l("Hardware:"),kw=h(),Ol=s("ul"),Bl=s("li"),S3=l("fast connectivity between GPUs"),Dr=s("ul"),Kf=s("li"),N3=l("intra-node: NVLink"),C3=h(),ec=s("li"),L3=l("inter-node: Infiniband / Intel OPA"),Aw=h(),ql=s("p"),M3=l("Software:"),Tw=h(),Kt=s("ul"),tc=s("li"),O3=l("Data Parallel / Distributed Data Parallel"),B3=h(),ac=s("li"),q3=l("fp16 (autocast caching)"),jw=h(),Ce=s("h2"),ea=s("a"),sc=s("span"),m(Ir.$$.fragment),V3=h(),rc=s("span"),H3=l("Bigger Models"),xw=h(),Vl=s("p"),F3=l("Hardware:"),Dw=h(),oe=s("ul"),ic=s("li"),W3=l("bigger GPUs"),R3=h(),oc=s("li"),X3=l("more GPUs"),Y3=h(),Gr=s("li"),Q3=l("more CPU and NVMe (offloaded to by "),Hl=s("a"),J3=l("DeepSpeed-Infinity"),Z3=l(")"),Iw=h(),Fl=s("p"),K3=l("Software:"),Gw=h(),E=s("ul"),lc=s("li"),e6=l("Model Scalability (ZeRO and 3D Parallelism)"),t6=h(),nc=s("li"),a6=l("Low-memory Optimizers"),s6=h(),pc=s("li"),r6=l("fp16/bf16 (smaller data/faster throughput)"),i6=h(),hc=s("li"),o6=l("tf32 (faster throughput)"),l6=h(),fc=s("li"),n6=l("Gradient accumulation"),p6=h(),cc=s("li"),h6=l("Gradient checkpointing"),f6=h(),mc=s("li"),c6=l("Sparsity"),Uw=h(),Le=s("h2"),ta=s("a"),dc=s("span"),m(Ur.$$.fragment),m6=h(),uc=s("span"),d6=l("Hardware"),zw=h(),Me=s("h3"),aa=s("a"),vc=s("span"),m(zr.$$.fragment),u6=h(),wc=s("span"),v6=l("Power and Cooling"),Sw=h(),Wl=s("p"),w6=l("If you bought an expensive high end GPU make sure you give it the correct power and sufficient cooling."),Nw=h(),Sr=s("p"),gc=s("strong"),g6=l("Power"),_6=l(":"),Cw=h(),Rl=s("p"),b6=l("Some high end consumer GPU cards have 2 and sometimes 3 PCI-E 8-Pin power sockets. Make sure you have as many independent 12V PCI-E 8-Pin cables plugged into the card as there are sockets. Do not use the 2 splits at one end of the same cable (also known as pigtail cable). That is if you have 2 sockets on the GPU, you want 2 PCI-E 8-Pin cables going from your PSU to the card and not one that has 2 PCI-E 8-Pin connectors at the end! You won\u2019t get the full performance out of your card otherwise."),Lw=h(),Xl=s("p"),y6=l("Each PCI-E 8-Pin power cable needs to be plugged into a 12V rail on the PSU side and can supply up to 150W of power."),Mw=h(),Yl=s("p"),E6=l("Some other cards may use a PCI-E 12-Pin connectors, and these can deliver up to 500-600W of power."),Ow=h(),Ql=s("p"),P6=l("Low end cards may use 6-Pin connectors, which supply up to 75W of power."),Bw=h(),Jl=s("p"),$6=l("Additionally you want the high-end PSU that has stable voltage. Some lower quality ones may not give the card the stable voltage it needs to function at its peak."),qw=h(),Zl=s("p"),k6=l("And of course the PSU needs to have enough unused Watts to power the card."),Vw=h(),Nr=s("p"),_c=s("strong"),A6=l("Cooling"),T6=l(":"),Hw=h(),Kl=s("p"),j6=l("When a GPU gets overheated it would start throttling down and will not deliver full performance. And it will shutdown if it gets too hot."),Fw=h(),en=s("p"),x6=l("It\u2019s hard to tell the exact best temperature to strive for when a GPU is heavily loaded, but probably anything under +80C is good, but lower is better - perhaps 70-75C is an excellent range to be in. The throttling down is likely to start at around 84-90C. But other than throttling performance a prolonged very higher temperature is likely to reduce the lifespan of a GPU."),Ww=h(),Oe=s("h3"),sa=s("a"),bc=s("span"),m(Cr.$$.fragment),D6=h(),yc=s("span"),I6=l("Multi-GPU Connectivity"),Rw=h(),tn=s("p"),G6=l("If you use multiple GPUs the way cards are inter-connected can have a huge impact on the total training time."),Xw=h(),an=s("p"),U6=l("If the GPUs are on the same physical node, you can run:"),Yw=h(),m(Lr.$$.fragment),Qw=h(),sn=s("p"),z6=l("and it will tell you how the GPUs are inter-connected."),Jw=h(),rn=s("p"),S6=l("On a machine with dual-GPU and which are connected with NVLink, you will most likely see something like:"),Zw=h(),m(Mr.$$.fragment),Kw=h(),on=s("p"),N6=l("on a different machine w/o NVLink we may see:"),e1=h(),m(Or.$$.fragment),t1=h(),ln=s("p"),C6=l("The report includes this legend:"),a1=h(),m(Br.$$.fragment),s1=h(),le=s("p"),L6=l("So the first report "),Ec=s("code"),M6=l("NV2"),O6=l(" tells us the GPUs are interconnected with 2 NVLinks, and the second report "),Pc=s("code"),B6=l("PHB"),q6=l(" we have a typical consumer-level PCIe+Bridge setup."),r1=h(),nn=s("p"),V6=l("Check what type of connectivity you have on your setup. Some of these will make the communication between cards faster (e.g. NVLink), others slower (e.g. PHB)."),i1=h(),pn=s("p"),H6=l("Depending on the type of scalability solution used, the connectivity speed could have a major or a minor impact. If the GPUs need to sync rarely, as in DDP, the impact of a slower connection will be less significant. If the GPUs need to send messages to each other often, as in ZeRO-DP, then faster connectivity becomes super important to achieve faster training."),o1=h(),Be=s("h3"),ra=s("a"),$c=s("span"),m(qr.$$.fragment),F6=h(),kc=s("span"),W6=l("NVlink"),l1=h(),Vr=s("p"),Hr=s("a"),R6=l("NVLink"),X6=l(" is a wire-based serial multi-lane near-range communications link developed by Nvidia."),n1=h(),ia=s("p"),Y6=l("Each new generation provides a faster bandwidth, e.g. here is a quote from "),Fr=s("a"),Q6=l("Nvidia Ampere GA102 GPU Architecture"),J6=l(":"),p1=h(),hn=s("blockquote"),Ac=s("p"),Z6=l(`Third-Generation NVLink\xAE
GA102 GPUs utilize NVIDIA\u2019s third-generation NVLink interface, which includes four x4 links,
with each link providing 14.0625 GB/sec bandwidth in each direction between two GPUs. Four
links provide 56.25 GB/sec bandwidth in each direction, and 112.5 GB/sec total bandwidth
between two GPUs. Two RTX 3090 GPUs can be connected together for SLI using NVLink.
(Note that 3-Way and 4-Way SLI configurations are not supported.)`),h1=h(),M=s("p"),K6=l("So the higher "),Tc=s("code"),eP=l("X"),tP=l(" you get in the report of "),jc=s("code"),aP=l("NVX"),sP=l(" in the output of "),xc=s("code"),rP=l("nvidia-smi topo -m"),iP=l(" the better. The generation will depend on your GPU architecture."),f1=h(),fn=s("p"),oP=l("Let\u2019s compare the execution of a gpt2 language model training over a small sample of wikitext."),c1=h(),cn=s("p"),lP=l("The results are:"),m1=h(),oa=s("table"),Dc=s("thead"),Wr=s("tr"),Ic=s("th"),nP=l("NVlink"),pP=h(),mn=s("th"),hP=l("Time"),fP=h(),Rr=s("tbody"),Xr=s("tr"),Gc=s("td"),cP=l("Y"),mP=h(),dn=s("td"),dP=l("101s"),uP=h(),Yr=s("tr"),Uc=s("td"),vP=l("N"),wP=h(),un=s("td"),gP=l("131s"),d1=h(),vn=s("p"),_P=l("You can see that NVLink completes the training ~23% faster."),u1=h(),la=s("p"),bP=l("In the second benchmark we use "),zc=s("code"),yP=l("NCCL_P2P_DISABLE=1"),EP=l(" to tell the GPUs not to use NVLink."),v1=h(),wn=s("p"),PP=l("Here is the full benchmark code and outputs:"),w1=h(),m(Qr.$$.fragment),g1=h(),T=s("p"),$P=l("Hardware: 2x TITAN RTX 24GB each + NVlink with 2 NVLinks ("),Sc=s("code"),kP=l("NV2"),AP=l(" in "),Nc=s("code"),TP=l("nvidia-smi topo -m"),jP=l(`)
Software: `),Cc=s("code"),xP=l("pytorch-1.8-to-be"),DP=l(" + "),Lc=s("code"),IP=l("cuda-11.0"),GP=l(" / "),Mc=s("code"),UP=l("transformers==4.3.0.dev0"),_1=h(),qe=s("h2"),na=s("a"),Oc=s("span"),m(Jr.$$.fragment),zP=h(),Bc=s("span"),SP=l("Software"),b1=h(),Ve=s("h3"),pa=s("a"),qc=s("span"),m(Zr.$$.fragment),NP=h(),Vc=s("span"),CP=l("Model Scalability"),y1=h(),gn=s("p"),LP=l("When you can\u2019t fit a model into the available GPU memory, you need to start using a solution that allows you to scale a large model to use multiple GPUs in parallel."),E1=h(),Kr=s("p"),MP=l("For indepth details on ZeRO and various other model parallelism protocols please see: "),_n=s("a"),OP=l("Model Parallelism"),P1=h(),He=s("h3"),ha=s("a"),Hc=s("span"),m(ei.$$.fragment),BP=h(),Fc=s("span"),qP=l("Anatomy of Model's Operations"),$1=h(),bn=s("p"),VP=l("Transformers architecture includes 3 main groups of operations grouped below by compute-intensity."),k1=h(),ne=s("ol"),ti=s("li"),Wc=s("p"),Rc=s("strong"),HP=l("Tensor Contractions"),FP=h(),ai=s("p"),WP=l("Linear layers and components of Multi-Head Attention all do batched "),Xc=s("strong"),RP=l("matrix-matrix multiplications"),XP=l(". These operations are the most compute-intensive part of training a transformer."),YP=h(),si=s("li"),Yc=s("p"),Qc=s("strong"),QP=l("Statistical Normalizations"),JP=h(),ri=s("p"),ZP=l("Softmax and layer normalization are less compute-intensive than tensor contractions, and involve one or more "),Jc=s("strong"),KP=l("reduction operations"),e4=l(", the result of which is then applied via a map."),t4=h(),ii=s("li"),Zc=s("p"),Kc=s("strong"),a4=l("Element-wise Operators"),s4=h(),oi=s("p"),r4=l("These are the remaining operators: "),em=s("strong"),i4=l("biases, dropout, activations, and residual connections"),o4=l(". These are the least compute-intensive operations."),A1=h(),yn=s("p"),l4=l("This knowledge can be helpful to know when analyzing performance bottlenecks."),T1=h(),li=s("p"),n4=l("This summary is derived from "),ni=s("a"),p4=l("Data Movement Is All You Need: A Case Study on Optimizing Transformers 2020"),j1=h(),Fe=s("h3"),fa=s("a"),tm=s("span"),m(pi.$$.fragment),h4=h(),am=s("span"),f4=l("Anatomy of Model's Memory"),x1=h(),En=s("p"),c4=l("The components on GPU memory are the following:"),D1=h(),A=s("ol"),sm=s("li"),m4=l("model weights"),d4=h(),rm=s("li"),u4=l("optimizer states"),v4=h(),im=s("li"),w4=l("gradients"),g4=h(),om=s("li"),_4=l("forward activations saved for gradient computation"),b4=h(),lm=s("li"),y4=l("temporary buffers"),E4=h(),nm=s("li"),P4=l("functionality-specific memory"),I1=h(),Pn=s("p"),$4=l("A typical model trained in mixed precision with AdamW requires 18 bytes per model parameter plus activation memory."),G1=h(),$n=s("p"),k4=l("For inference there are no optimizer states and gradients, so we can subtract those. And thus we end up with 6 bytes per model parameter for mixed precision inference, plus activation memory."),U1=h(),kn=s("p"),A4=l("Let\u2019s look at the details."),z1=h(),We=s("h4"),ca=s("a"),pm=s("span"),m(hi.$$.fragment),T4=h(),hm=s("span"),j4=l("Model Weights"),S1=h(),ma=s("ul"),fm=s("li"),x4=l("4 bytes * number of parameters for fp32 training"),D4=h(),cm=s("li"),I4=l("6 bytes * number of parameters for mixed precision training"),N1=h(),Re=s("h4"),da=s("a"),mm=s("span"),m(fi.$$.fragment),G4=h(),dm=s("span"),U4=l("Optimizer States"),C1=h(),pe=s("ul"),um=s("li"),z4=l("8 bytes * number of parameters for normal AdamW (maintains 2 states)"),S4=h(),An=s("li"),N4=l("2 bytes * number of parameters for 8-bit AdamW optimizers like "),ci=s("a"),C4=l("bitsandbytes"),L4=h(),vm=s("li"),M4=l("4 bytes * number of parameters for optimizers like SGD (maintains only 1 state)"),L1=h(),Xe=s("h4"),ua=s("a"),wm=s("span"),m(mi.$$.fragment),O4=h(),gm=s("span"),B4=l("Gradients"),M1=h(),Tn=s("ul"),_m=s("li"),q4=l("4 bytes * number of parameters for either fp32 or mixed precision training"),O1=h(),Ye=s("h4"),va=s("a"),bm=s("span"),m(di.$$.fragment),V4=h(),ym=s("span"),H4=l("Forward Activations"),B1=h(),jn=s("ul"),Em=s("li"),F4=l("size depends on many factors, the key ones being sequence length, hidden size and batch size."),q1=h(),xn=s("p"),W4=l("There are the input and output that are being passed and returned by the forward and the backward functions and the forward activations saved for gradient computation."),V1=h(),Qe=s("h4"),wa=s("a"),Pm=s("span"),m(ui.$$.fragment),R4=h(),$m=s("span"),X4=l("Temporary Memory"),H1=h(),Dn=s("p"),Y4=l("Additionally there are all kinds of temporary variables which get released once the calculation is done, but in the moment these could require additional memory and could push to OOM. Therefore when coding it\u2019s crucial to think strategically about such temporary variables and sometimes to explicitly free those as soon as they are no longer needed."),F1=h(),Je=s("h4"),ga=s("a"),km=s("span"),m(vi.$$.fragment),Q4=h(),Am=s("span"),J4=l("Functionality-specific memory"),W1=h(),In=s("p"),Z4=l("Then your software could have special memory needs. For example, when generating text using beam search, the software needs to maintain multiple copies of inputs and outputs."),R1=h(),Ze=s("h3"),_a=s("a"),Tm=s("span"),m(wi.$$.fragment),K4=h(),ba=s("span"),jm=s("code"),e5=l("forward"),t5=l(" vs "),xm=s("code"),a5=l("backward"),s5=l(" Execution Speed"),X1=h(),Gn=s("p"),r5=l("For convolutions and linear layers there are 2x flops in the backward compared to the forward, which generally translates into ~2x slower (sometimes more, because sizes in the backward tend to be more awkward). Activations are usually bandwidth-limited, and it\u2019s typical for an activation to have to read more data in the backward than in the forward (e.g. activation forward reads once, writes once, activation backward reads twice, gradOutput and output of the forward, and writes once, gradInput)."),Y1=h(),Ke=s("h3"),ya=s("a"),Dm=s("span"),m(gi.$$.fragment),i5=h(),Im=s("span"),o5=l("Floating Data Types"),Q1=h(),Un=s("p"),l5=l("Here are the commonly used floating point data types choice of which impacts both memory usage and throughput:"),J1=h(),O=s("ul"),_i=s("li"),n5=l("fp32 ("),Gm=s("code"),p5=l("float32"),h5=l(")"),f5=h(),bi=s("li"),c5=l("fp16 ("),Um=s("code"),m5=l("float16"),d5=l(")"),u5=h(),yi=s("li"),v5=l("bf16 ("),zm=s("code"),w5=l("bfloat16"),g5=l(")"),_5=h(),Sm=s("li"),b5=l("tf32 (CUDA internal data type)"),Z1=h(),zn=s("p"),y5=l("Here is a diagram that shows how these data types correlate to each other."),K1=h(),Sn=s("p"),Nn=s("img"),eg=h(),Ea=s("p"),E5=l("(source: "),Ei=s("a"),P5=l("NVIDIA Blog"),$5=l(")"),tg=h(),Cn=s("p"),k5=l("While fp16 and fp32 have been around for quite some time, bf16 and tf32 are only available on the Ampere architecture GPUS. TPUs support bf16 as well."),ag=h(),et=s("h4"),Pa=s("a"),Nm=s("span"),m(Pi.$$.fragment),A5=h(),Cm=s("span"),T5=l("fp16"),sg=h(),Ln=s("p"),j5=l("AMP = Automatic Mixed Precision"),rg=h(),Mn=s("p"),x5=l("If we look at what\u2019s happening with FP16 training (mixed precision) we have:"),ig=h(),B=s("ul"),Lm=s("li"),D5=l("the model has two copies in memory: one in half-precision for the forward/backward computations and one in full precision - no memory saved here"),I5=h(),Mm=s("li"),G5=l("the forward activations saved for gradient computation are in half-precision - memory is saved here"),U5=h(),$i=s("li"),z5=l("the gradients are computed in half-precision "),Om=s("em"),S5=l("but"),N5=l(" converted to full-precision for the update, no saving there"),C5=h(),Bm=s("li"),L5=l("the optimizer states are in full precision as all the updates are done in full-precision"),og=h(),On=s("p"),M5=l("So the savings only happen for the forward activations saved for the backward computation, and there is a slight overhead because the model weights are stored both in half- and full-precision."),lg=h(),$a=s("p"),O5=l("In \u{1F917} Transformers fp16 mixed precision is enabled by passing "),qm=s("code"),B5=l("--fp16"),q5=l(" to the \u{1F917} Trainer."),ng=h(),Bn=s("p"),V5=l("Now let\u2019s look at a simple text-classification fine-tuning on 2 GPUs (I\u2019m giving the command for reference):"),pg=h(),m(ki.$$.fragment),hg=h(),ka=s("p"),H5=l("Since the only savings we get are in the model activations saved for the backward passed, it\u2019s logical that the bigger those activations are, the bigger the saving will be. If we try different batch sizes, I indeed get (this is with "),Vm=s("code"),F5=l("nvidia-smi"),W5=l(" so not completely reliable as said above but it will be a fair comparison):"),fg=h(),Aa=s("table"),Hm=s("thead"),R=s("tr"),qn=s("th"),R5=l("batch size"),X5=h(),Vn=s("th"),Y5=l("w/o \u2014fp16"),Q5=h(),Hn=s("th"),J5=l("w/ \u2014fp16"),Z5=h(),Fn=s("th"),K5=l("savings"),e$=h(),X=s("tbody"),Y=s("tr"),Wn=s("td"),t$=l("8"),a$=h(),Rn=s("td"),s$=l("4247"),r$=h(),Xn=s("td"),i$=l("4163"),o$=h(),Yn=s("td"),l$=l("84"),n$=h(),Q=s("tr"),Qn=s("td"),p$=l("16"),h$=h(),Jn=s("td"),f$=l("4971"),c$=h(),Zn=s("td"),m$=l("4793"),d$=h(),Kn=s("td"),u$=l("178"),v$=h(),J=s("tr"),ep=s("td"),w$=l("32"),g$=h(),tp=s("td"),_$=l("6827"),b$=h(),ap=s("td"),y$=l("6207"),E$=h(),sp=s("td"),P$=l("620"),$$=h(),Z=s("tr"),rp=s("td"),k$=l("64"),A$=h(),ip=s("td"),T$=l("10037"),j$=h(),op=s("td"),x$=l("8061"),D$=h(),lp=s("td"),I$=l("1976"),cg=h(),Ta=s("p"),G$=l("So there is only a real memory saving if we train at a high batch size (and it\u2019s not half) and at batch sizes lower than 8, you actually get a bigger memory footprint (because of the overhead mentioned above). The gain for FP16 training is that in each of those cases, the training with the flag "),Fm=s("code"),U$=l("--fp16"),z$=l(" is twice as fast, which does require every tensor to have every dimension be a multiple of 8 (examples pad the tensors to a sequence length that is a multiple of 8)."),mg=h(),np=s("p"),S$=l("Summary: FP16 with apex or AMP will only give you some memory savings with a reasonably high batch size."),dg=h(),pp=s("p"),N$=l("Additionally, under mixed precision when possible, it\u2019s important that the batch size is a multiple of 8 to efficiently use tensor cores."),ug=h(),ja=s("p"),C$=l("Note that in some situations the speed up can be as big as 5x when using mixed precision. e.g. we have observed that while using "),Ai=s("a"),L$=l("Megatron-Deepspeed"),M$=l("."),vg=h(),hp=s("p"),O$=l("Some amazing tutorials to read on mixed precision:"),wg=h(),xa=s("ul"),fp=s("li"),B$=l("@sgugger wrote a great explanation of mixed precision "),Ti=s("a"),q$=l("here"),V$=h(),cp=s("li"),H$=l("Aleksey Bilogur\u2019s "),ji=s("a"),F$=l("A developer-friendly guide to mixed precision training with PyTorch"),gg=h(),he=s("p"),W$=l(`You can also see a variety of benchmarks on fp16 vs other precisions:
`),xi=s("a"),R$=l("RTX-3090"),X$=l(` and
`),Di=s("a"),Y$=l("A100"),Q$=l("."),_g=h(),tt=s("h5"),Da=s("a"),Wm=s("span"),m(Ii.$$.fragment),J$=h(),Rm=s("span"),Z$=l("fp16 caching"),bg=h(),fe=s("p"),K$=l("pytorch "),Xm=s("code"),ek=l("autocast"),tk=l(" which performs AMP include a caching feature, which speed things up by caching fp16-converted values. Here is the full description from this "),Gi=s("a"),ak=l("comment"),sk=l(":"),yg=h(),mp=s("p"),rk=l("Autocast maintains a cache of the FP16 casts of model parameters (leaves). This helps streamline parameter reuse: if the same FP32 param is used in several different FP16list ops, like several matmuls, instead of re-casting the param to FP16 on entering each matmul, the cast will occur on the first matmul, the casted FP16 copy will be cached, and for all later matmuls the FP16 copy will be reused. The cache is maintained only within a particular outermost autocast context. When you exit the autocast context the cache is dropped. For recommended usage, in which autocast wraps the forward pass, and then you exit the context before calling backward(), this means the cache only lasts the duration of the forward pass each iteration, and will be rebuilt next iteration. (The cache of FP16-casted copies MUST be rebuilt each iteration. The FP32 parameters get updated by the optimizer, so the FP16 copies must be recreated, otherwise the FP16 values will be stale.)"),Eg=h(),at=s("h5"),Ia=s("a"),Ym=s("span"),m(Ui.$$.fragment),ik=h(),Qm=s("span"),ok=l("fp16 Inference"),Pg=h(),dp=s("p"),lk=l("While normally inference is done with fp16/amp as with training, it\u2019s also possible to use the full fp16 mode without using mixed precision. This is especially a good fit if the pretrained model weights are already in fp16. So a lot less memory is used: 2 bytes per parameter vs 6 bytes with mixed precision!"),$g=h(),up=s("p"),nk=l("How good the results this will deliver will depend on the model. If it can handle fp16 without overflows and accuracy issues, then it\u2019ll definitely better to use the full fp16 mode."),kg=h(),vp=s("p"),pk=l("For example, LayerNorm has to be done in fp32 and recent pytorch (1.10+) has been fixed to do that regardless of the input types, but earlier pytorch versions accumulate in the input type which can be an issue."),Ag=h(),Ga=s("p"),hk=l("In \u{1F917} Transformers the full fp16 inference is enabled by passing "),Jm=s("code"),fk=l("--fp16_full_eval"),ck=l(" to the \u{1F917} Trainer."),Tg=h(),st=s("h4"),Ua=s("a"),Zm=s("span"),m(zi.$$.fragment),mk=h(),Km=s("span"),dk=l("bf16"),jg=h(),ce=s("p"),uk=l("If you own Ampere or newer hardware you can start using bf16 for your training and evaluation. While bf16 has a worse precision than fp16, it has a much much bigger dynamic range. Therefore, if in the past you were experiencing overflow issues while training the model, bf16 will prevent this from happening most of the time. Remember that in fp16 the biggest number you can have is "),ed=s("code"),vk=l("65535"),wk=l(" and any number above that will overflow. A bf16 number can be as large as "),td=s("code"),gk=l("3.39e+38"),_k=l(" (!) which is about the same as fp32 - because both have 8-bits used for the numerical range."),xg=h(),wp=s("p"),bk=l("Automatic Mixed Precision (AMP) is the same as with fp16, except it\u2019ll use bf16."),Dg=h(),gp=s("p"),yk=l("Thanks to the fp32-like dynamic range with bf16 mixed precision loss scaling is no longer needed."),Ig=h(),_p=s("p"),Ek=l("If you have tried to finetune models pre-trained under bf16 mixed precision (e.g. T5) it\u2019s very likely that you have encountered overflow issues. Now you should be able to finetune those models without any issues."),Gg=h(),bp=s("p"),Pk=l("That said, also be aware that if you pre-trained a model in bf16, it\u2019s likely to have overflow issues if someone tries to finetune it in fp16 down the road. So once started on the bf16-mode path it\u2019s best to remain on it and not switch to fp16."),Ug=h(),za=s("p"),$k=l("In \u{1F917} Transformers bf16 mixed precision is enabled by passing "),ad=s("code"),kk=l("--bf16"),Ak=l(" to the \u{1F917} Trainer."),zg=h(),yp=s("p"),Tk=l("If you use your own trainer, this is just:"),Sg=h(),m(Si.$$.fragment),Ng=h(),Ni=s("p"),jk=l("If you need to switch a tensor to bf16, it\u2019s just: "),sd=s("code"),xk=l("t.to(dtype=torch.bfloat16)"),Cg=h(),Ep=s("p"),Dk=l("Here is how you can check if your setup supports bf16:"),Lg=h(),m(Ci.$$.fragment),Mg=h(),Pp=s("p"),Ik=l("On the other hand bf16 has a much worse precision than fp16, so there are certain situations where you\u2019d still want to use fp16 and not bf16."),Og=h(),me=s("p"),Gk=l(`You can also see a variety of benchmarks on bf16 vs other precisions:
`),Li=s("a"),Uk=l("RTX-3090"),zk=l(` and
`),Mi=s("a"),Sk=l("A100"),Nk=l("."),Bg=h(),rt=s("h5"),Sa=s("a"),rd=s("span"),m(Oi.$$.fragment),Ck=h(),id=s("span"),Lk=l("bf16 Inference"),qg=h(),Na=s("p"),Mk=l("Same as with fp16, you can do inference in either the mixed precision bf16 or using the full bf16 mode. The same caveats apply. For details see "),$p=s("a"),Ok=l("fp16 Inference"),Bk=l("."),Vg=h(),Ca=s("p"),qk=l("In \u{1F917} Transformers the full bf16 inference is enabled by passing "),od=s("code"),Vk=l("--bf16_full_eval"),Hk=l(" to the \u{1F917} Trainer."),Hg=h(),it=s("h4"),La=s("a"),ld=s("span"),m(Bi.$$.fragment),Fk=h(),nd=s("span"),Wk=l("tf32"),Fg=h(),kp=s("p"),Rk=l("The Ampere hardware uses a magical data type called tf32. It has the same numerical range as fp32 (8-bits), but instead of 23 bits precision it has only 10 bits (same as fp16). In total it uses only 19 bits."),Wg=h(),Ap=s("p"),Xk=l("It\u2019s magical in the sense that you can use the normal fp32 training and/or inference code and by enabling tf32 support you can get up to 3x throughput improvement. All you need to do is to add this to your code:"),Rg=h(),m(qi.$$.fragment),Xg=h(),Tp=s("p"),Yk=l("When this is done CUDA will automatically switch to using tf32 instead of fp32 where it\u2019s possible. This, of course, assumes that the used GPU is from the Ampere series."),Yg=h(),Ma=s("p"),Qk=l("Like all cases with reduced precision this may or may not be satisfactory for your needs, so you have to experiment and see. According to "),Vi=s("a"),Jk=l("NVIDIA research"),Zk=l(" the majority of machine learning training shouldn\u2019t be impacted and showed the same perplexity and convergence as the fp32 training."),Qg=h(),jp=s("p"),Kk=l("If you\u2019re already using fp16 or bf16 mixed precision it may help with the throughput as well."),Jg=h(),q=s("p"),e7=l("You can enable this mode in the \u{1F917} Trainer with "),pd=s("code"),t7=l("--tf32"),a7=l(", or disable it with "),hd=s("code"),s7=l("--tf32 0"),r7=l(" or "),fd=s("code"),i7=l("--no_tf32"),o7=l(`.
By default the PyTorch default is used.`),Zg=h(),de=s("p"),l7=l("Note: tf32 mode is internal to CUDA and can\u2019t be accessed directly via "),cd=s("code"),n7=l("tensor.to(dtype=torch.tf32)"),p7=l(" as "),md=s("code"),h7=l("torch.tf32"),f7=l(" doesn\u2019t exit."),Kg=h(),Oa=s("p"),c7=l("Note: you need "),dd=s("code"),m7=l("torch>=1.7"),d7=l(" to enjoy this feature."),e_=h(),ue=s("p"),u7=l(`You can also see a variety of benchmarks on tf32 vs other precisions:
`),Hi=s("a"),v7=l("RTX-3090"),w7=l(` and
`),Fi=s("a"),g7=l("A100"),_7=l("."),t_=h(),ot=s("h3"),Ba=s("a"),ud=s("span"),m(Wi.$$.fragment),b7=h(),vd=s("span"),y7=l("Gradient Accumulation"),a_=h(),ve=s("p"),E7=l("Since gradient accumulation essentially is identical to having a larger batch size, just as with the larger batch size here you are likely to see a 20-30% speedup due to the optimizer running less often. For example, see benchmarks for "),Ri=s("a"),P7=l("RTX-3090"),$7=l(`
and `),Xi=s("a"),k7=l("A100"),A7=l("."),s_=h(),qa=s("p"),T7=l("To activate this feature in \u{1F917} Trainer add "),wd=s("code"),j7=l("--gradient_accumulation_steps 4"),x7=l(" to its arguments (experiment with the value to get the best performance)."),r_=h(),xp=s("p"),D7=l("It\u2019s important to remember that using gradient accumulation you may end up with a much larger effective batch size, so you may need to adjust the learning rate, its warm up and for very short datasets it\u2019ll impact the loss as the training will end up doing less steps than normal."),i_=h(),lt=s("h3"),Va=s("a"),gd=s("span"),m(Yi.$$.fragment),I7=h(),_d=s("span"),G7=l("Gradient Checkpointing"),o_=h(),Dp=s("p"),U7=l("One way to use significantly less GPU memory is to enabled \u201CGradient Checkpointing\u201D (also known as \u201Cactivation checkpointing\u201D). When enabled, a lot of memory can be freed at the cost of small decrease in the training speed due to recomputing parts of the graph during back-propagation. The slowdown will depend on the model but quite often it is around 20-30%."),l_=h(),V=s("p"),z7=l("This technique was first shared in the paper: "),Qi=s("a"),S7=l("Training Deep Nets with Sublinear Memory Cost"),N7=l(". The paper will also give you the exact details on the savings, but it\u2019s in the ballpark of "),bd=s("code"),C7=l("O(sqrt(n))"),L7=l(", where "),yd=s("code"),M7=l("n"),O7=l(" is the number of feed-forward layers."),n_=h(),Ip=s("p"),B7=l("To activate this feature in \u{1F917} Transformers for models that support it, use:"),p_=h(),m(Ji.$$.fragment),h_=h(),Ha=s("p"),q7=l("or add "),Ed=s("code"),V7=l("--gradient_checkpointing"),H7=l(" to the Trainer arguments."),f_=h(),nt=s("h3"),Fa=s("a"),Pd=s("span"),m(Zi.$$.fragment),F7=h(),$d=s("span"),W7=l("Batch sizes"),c_=h(),Gp=s("p"),R7=l("One gets the most efficient performance when batch sizes and input/output neuron counts are divisible by a certain number, which typically starts at 8, but can be much higher as well. That number varies a lot depending on the specific hardware being used and the dtype of the model."),m_=h(),we=s("p"),X7=l("For example for fully connected layers (which correspond to GEMMs), NVIDIA provides recommendations for "),Ki=s("a"),Y7=l("input/output neuron counts"),Q7=l(" and "),eo=s("a"),J7=l("batch size"),Z7=l("."),d_=h(),to=s("p"),ao=s("a"),K7=l("Tensor Core Requirements"),e8=l(" define the multiplier based on the dtype and the hardware. For example, for fp16 a multiple of 8 is recommended, but on A100 it\u2019s 64!"),u_=h(),Wa=s("p"),t8=l("For parameters that are small, there is also "),so=s("a"),a8=l("Dimension Quantization Effects"),s8=l(" to consider, this is where tiling happens and the right multiplier can have a significant speedup."),v_=h(),H=s("p"),r8=l("Additionally, as explained in the "),Up=s("a"),i8=l("Gradient Accumulation"),o8=l(` section, the bigger the batch size the less often the optimizer is run, the faster the training is (considering the same dataset length). See benchmarks
for `),ro=s("a"),l8=l("RTX-3090"),n8=l(`
and `),io=s("a"),p8=l("A100"),h8=l("."),w_=h(),pt=s("h3"),Ra=s("a"),kd=s("span"),m(oo.$$.fragment),f8=h(),Ad=s("span"),c8=l("DP vs DDP"),g_=h(),ht=s("p"),Td=s("code"),m8=l("DistributedDataParallel"),d8=l(" (DDP) is typically faster than "),jd=s("code"),u8=l("DataParallel"),v8=l(" (DP), but it is not always the case:"),__=h(),Xa=s("ul"),xd=s("li"),w8=l("while DP is python threads-based, DDP is multiprocess-based - and as such it has no python threads limitations, such as GIL"),g8=h(),Dd=s("li"),_8=l("on the other hand a slow inter-connectivity between the GPU cards could lead to an actual slower outcome with DDP"),b_=h(),zp=s("p"),b8=l("Here are the main differences in the inter-GPU communication overhead between the two modes:"),y_=h(),lo=s("p"),no=s("a"),y8=l("DDP"),E8=l(":"),E_=h(),Ya=s("ul"),Id=s("li"),P8=l("At the start time the main process replicates the model once from gpu 0 to the rest of gpus"),$8=h(),Sp=s("li"),k8=l("Then for each batch:"),po=s("ol"),Gd=s("li"),A8=l("each gpu consumes each own mini-batch of data directly"),T8=h(),ho=s("li"),j8=l("during "),Ud=s("code"),x8=l("backward"),D8=l(", once the local gradients are ready, they are then averaged across all processes"),P_=h(),fo=s("p"),co=s("a"),I8=l("DP"),G8=l(":"),$_=h(),Np=s("p"),U8=l("For each batch:"),k_=h(),G=s("ol"),zd=s("li"),z8=l("gpu 0 reads the batch of data and then sends a mini-batch to each gpu"),S8=h(),Sd=s("li"),N8=l("replicates the up-to-date model from gpu 0 to each gpu"),C8=h(),mo=s("li"),L8=l("runs "),Nd=s("code"),M8=l("forward"),O8=l(" and sends output from each gpu to gpu 0, computes loss"),B8=h(),Cp=s("li"),q8=l("scatters loss from gpu 0 to all gpus, runs "),Cd=s("code"),V8=l("backward"),H8=h(),Ld=s("li"),F8=l("sends gradients from each gpu to gpu 0 and averages those"),A_=h(),Lp=s("p"),W8=l("The only communication DDP performs per batch is sending gradients, whereas DP does 5 different data exchanges per batch."),T_=h(),Qa=s("p"),R8=l("DP copies data within the process via python threads, whereas DDP copies data via "),uo=s("a"),X8=l("torch.distributed"),Y8=l("."),j_=h(),Mp=s("p"),Q8=l("Under DP gpu 0 performs a lot more work than the rest of the gpus, thus resulting in under-utilization of gpus."),x_=h(),Op=s("p"),J8=l("You can use DDP across multiple machines, but this is not the case with DP."),D_=h(),Bp=s("p"),Z8=l("There are other differences between DP and DDP but they aren\u2019t relevant to this discussion."),I_=h(),Ja=s("p"),K8=l("If you want to go really deep into understanding these 2 modes, this "),vo=s("a"),e9=l("article"),t9=l(" is highly recommended, as it has great diagrams, includes multiple benchmarks and profiler outputs on various hardware, explains all the nuances that you may need to know."),G_=h(),qp=s("p"),a9=l("Let\u2019s look at an actual benchmark:"),U_=h(),Za=s("table"),Md=s("thead"),ft=s("tr"),Vp=s("th"),s9=l("Type"),r9=h(),Od=s("th"),i9=l("NVlink"),o9=h(),Hp=s("th"),l9=l("Time"),n9=h(),ct=s("tbody"),mt=s("tr"),Fp=s("td"),p9=l("2:DP"),h9=h(),Bd=s("td"),f9=l("Y"),c9=h(),Wp=s("td"),m9=l("110s"),d9=h(),dt=s("tr"),Rp=s("td"),u9=l("2:DDP"),v9=h(),qd=s("td"),w9=l("Y"),g9=h(),Xp=s("td"),_9=l("101s"),b9=h(),ut=s("tr"),Yp=s("td"),y9=l("2:DDP"),E9=h(),Vd=s("td"),P9=l("N"),$9=h(),Qp=s("td"),k9=l("131s"),z_=h(),Jp=s("p"),A9=l("Analysis:"),S_=h(),Zp=s("p"),T9=l("Here DP is ~10% slower than DDP w/ NVlink, but ~15% faster than DDP w/o NVlink"),N_=h(),Kp=s("p"),j9=l("The real difference will depend on how much data each GPU needs to sync with the others - the more there is to sync, the more a slow link will slow down the total runtime."),C_=h(),eh=s("p"),x9=l("Here is the full benchmark code and outputs:"),L_=h(),wo=s("p"),Hd=s("code"),D9=l("NCCL_P2P_DISABLE=1"),I9=l(" was used to disable the NVLink feature on the corresponding benchmark."),M_=h(),m(go.$$.fragment),O_=h(),j=s("p"),G9=l("Hardware: 2x TITAN RTX 24GB each + NVlink with 2 NVLinks ("),Fd=s("code"),U9=l("NV2"),z9=l(" in "),Wd=s("code"),S9=l("nvidia-smi topo -m"),N9=l(`)
Software: `),Rd=s("code"),C9=l("pytorch-1.8-to-be"),L9=l(" + "),Xd=s("code"),M9=l("cuda-11.0"),O9=l(" / "),Yd=s("code"),B9=l("transformers==4.3.0.dev0"),B_=h(),vt=s("h3"),Ka=s("a"),Qd=s("span"),m(_o.$$.fragment),q9=h(),Jd=s("span"),V9=l("DataLoader"),q_=h(),th=s("p"),H9=l("One of the important requirements to reach great training speed is the ability to feed the GPU at the maximum speed it can handle. By default everything happens in the main process and it might not be able to read the data from disk fast enough, and thus create a bottleneck, leading to GPU under-utilization."),V_=h(),es=s("ul"),ah=s("li"),Zd=s("code"),F9=l("DataLoader(pin_memory=True, ...)"),W9=l(" which ensures that the data gets preloaded into the pinned memory on CPU and typically leads to much faster transfers from CPU to GPU memory."),R9=h(),sh=s("li"),Kd=s("code"),X9=l("DataLoader(num_workers=4, ...)"),Y9=l(" - spawn several workers to pre-load data faster - during training watch the GPU utilization stats and if it\u2019s far from 100% experiment with raising the number of workers. Of course, the problem could be elsewhere so a very big number of workers won\u2019t necessarily lead to a better performance."),H_=h(),wt=s("h3"),ts=s("a"),eu=s("span"),m(bo.$$.fragment),Q9=h(),tu=s("span"),J9=l("Optimizers"),F_=h(),gt=s("h4"),as=s("a"),au=s("span"),m(yo.$$.fragment),Z9=h(),su=s("span"),K9=l("Faster optimizers"),W_=h(),rh=s("ul"),ru=s("li"),iu=s("code"),eA=l("torch.optim.AdamW"),R_=h(),ss=s("p"),ou=s("code"),tA=l("torch.optim.AdamW"),aA=l(" is faster than Transformers\u2019 "),lu=s("code"),sA=l("AdamW"),X_=h(),ih=s("ul"),oh=s("li"),nu=s("code"),rA=l("apex.optimizers.FusedAdam"),iA=l(" is supposed to be even faster"),Y_=h(),lh=s("p"),Eo=s("a"),oA=l("https://nvidia.github.io/apex/optimizers.html"),Q_=h(),rs=s("p"),lA=l("XXX: benchmark once "),Po=s("a"),nA=l("https://github.com/huggingface/transformers/issues/14539"),pA=l(" is resolved"),J_=h(),_t=s("h4"),is=s("a"),pu=s("span"),m($o.$$.fragment),hA=h(),hu=s("span"),fA=l("Leaner Optimizers"),Z_=h(),nh=s("ul"),ko=s("li"),ph=s("p"),Ao=s("a"),cA=l("bitsandbytes"),mA=l(" uses 1/4 of normal AdamW optimizer\u2019s memory and otherwise performs on par with the normal optimizer quality-wise."),dA=h(),fu=s("p"),uA=l("It\u2019s a bit tricky to integrate into Transformers since it requires an Embedding layer that includes a layer norm, which currently our models don\u2019t have. But other than that you just configure to use that optimizer instead of the normal one."),K_=h(),bt=s("h4"),os=s("a"),cu=s("span"),m(To.$$.fragment),vA=h(),mu=s("span"),wA=l("Faster optimizer"),eb=h(),yt=s("p"),gA=l("pytorch-nightly introduced "),du=s("code"),_A=l("torch.optim._multi_tensor"),bA=l(" which should significantly speed up the optimizers for situations with lots of small feature tensors. It should eventually become the default, but if you want to experiment with it sooner and don\u2019t mind using the bleed-edge, see: "),jo=s("a"),yA=l("https://github.com/huggingface/transformers/issues/9965"),tb=h(),Et=s("h3"),ls=s("a"),uu=s("span"),m(xo.$$.fragment),EA=h(),vu=s("span"),PA=l("Sparsity"),ab=h(),Pt=s("h4"),ns=s("a"),wu=s("span"),m(Do.$$.fragment),$A=h(),gu=s("span"),kA=l("Mixture of Experts"),sb=h(),hh=s("p"),AA=l(`Quite a few of the recent papers reported a 4-5x training speedup and a faster inference by integrating
Mixture of Experts (MoE) into the Transformer models.`),rb=h(),fh=s("p"),TA=l("Since it has been discovered that more parameters lead to better performance, this technique allows to increase the number of parameters by an order of magnitude without increasing training costs."),ib=h(),ch=s("p"),jA=l("In this approach every other FFN layer is replaced with a MoE Layer which consists of many experts, with a gated function that trains each expert in a balanced way depending on the input token\u2019s position in a sequence."),ob=h(),mh=s("p"),dh=s("img"),lb=h(),ps=s("p"),xA=l("(source: "),Io=s("a"),DA=l("GLAM"),IA=l(")"),nb=h(),uh=s("p"),GA=l("You can find exhaustive details and comparison tables in the papers listed at the end of this section."),pb=h(),vh=s("p"),UA=l("The main drawback of this approach is that it requires staggering amounts of GPU memory - almost an order of magnitude larger than its dense equivalent. Various distillation and approaches are proposed to how to overcome the much higher memory requirements."),hb=h(),wh=s("p"),zA=l("There is direct trade-off though, you can use just a few experts with a 2-3x smaller base model instead of dozens or hundreds experts leading to a 5x smaller model and thus increase the training speed moderately while increasing the memory requirements moderately as well."),fb=h(),gh=s("p"),SA=l("Most related papers and implementations are built around Tensorflow/TPUs:"),cb=h(),ge=s("ul"),_u=s("li"),Go=s("a"),NA=l("GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding"),CA=h(),bu=s("li"),Uo=s("a"),LA=l("Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity"),MA=h(),yu=s("li"),zo=s("a"),OA=l("GLaM: Generalist Language Model (GLaM)"),mb=h(),P=s("p"),BA=l("And for Pytorch DeepSpeed has built one as well: "),So=s("a"),qA=l("DeepSpeed-MoE: Advancing Mixture-of-Experts Inference and Training to Power Next-Generation AI Scale"),VA=l(", "),No=s("a"),HA=l("Mixture of Experts"),FA=l(" - blog posts:  "),Co=s("a"),WA=l("1"),RA=l(", "),Lo=s("a"),XA=l("2"),YA=l(" and specific deployment with large transformer-based natural language generation models: "),Mo=s("a"),QA=l("blog post"),JA=l(", "),_h=s("a"),ZA=l("Megatron-Deepspeed branch"),KA=l("."),db=h(),$t=s("h3"),hs=s("a"),Eu=s("span"),m(Oo.$$.fragment),eT=h(),Pu=s("span"),tT=l("Efficient Software Prebuilds"),ub=h(),fs=s("p"),aT=l("PyTorch\u2019s "),Bo=s("a"),sT=l("pip and conda builds"),rT=l(" come prebuit with the cuda toolkit which is enough to run PyTorch, but it is insufficient if you need to build cuda extensions."),vb=h(),cs=s("p"),iT=l("At times it may take an additional effort to pre-build some components, e.g., if you\u2019re using libraries like "),$u=s("code"),oT=l("apex"),lT=l(" that don\u2019t come pre-compiled. In other situations figuring out how to install the right cuda toolkit system-wide can be complicated. To address these users\u2019 needs PyTorch and NVIDIA release a new version of NGC docker container which already comes with everything prebuilt and you just need to install your programs on it and it will run out of the box."),wb=h(),bh=s("p"),nT=l("This approach is also useful if you want to tweak the pytorch source and/or make a new customized build."),gb=h(),_e=s("p"),pT=l("To find the docker image version you want start "),qo=s("a"),hT=l("here"),fT=l(", choose one of the latest monthly releases. Go into the release\u2019s notes for the desired release, check that the environment\u2019s components are matching your needs (including NVIDIA Driver requirements!) and then at the very top of that document go to the corresponding NGC page. If for some reason you get lost, here is "),Vo=s("a"),cT=l("the index of all PyTorch NGC images"),mT=l("."),_b=h(),yh=s("p"),dT=l("Next follow the instructions to download and deploy the docker image."),bb=h(),kt=s("h2"),ms=s("a"),ku=s("span"),m(Ho.$$.fragment),uT=h(),Au=s("span"),vT=l("Contribute"),yb=h(),Eh=s("p"),wT=l("This document is far from being complete and a lot more needs to be added, so if you have additions or corrections to make please don\u2019t hesitate to open a PR or if you aren\u2019t sure start an Issue and we can discuss the details there."),Eb=h(),Ph=s("p"),gT=l("When making contributions that A is better than B, please try to include a reproducible benchmark and/or a link to the source of that information (unless it comes directly from you)."),this.h()},l(e){const o=NC('[data-svelte="svelte-1phssyn"]',document.head);y=r(o,"META",{name:!0,content:!0}),o.forEach(t),x=f(e),$=r(e,"H1",{class:!0});var Fo=i($);S=r(Fo,"A",{id:!0,class:!0,href:!0});var Tu=i(S);Xh=r(Tu,"SPAN",{});var VT=i(Xh);d(xs.$$.fragment,VT),VT.forEach(t),Tu.forEach(t),x2=f(Fo),Yh=r(Fo,"SPAN",{});var HT=i(Yh);D2=n(HT,"Performance and Scalability: How To Fit a Bigger Model and Train It Faster"),HT.forEach(t),Fo.forEach(t),Iu=f(e),Ro=r(e,"BLOCKQUOTE",{});var FT=i(Ro);Qh=r(FT,"P",{});var WT=i(Qh);Jh=r(WT,"EM",{});var RT=i(Jh);I2=n(RT,"Or how to escape the dreaded \u201CRuntimeError: CUDA error: out of memory\u201D error."),RT.forEach(t),WT.forEach(t),FT.forEach(t),Gu=f(e),d(Ds.$$.fragment,e),Uu=f(e),te=r(e,"P",{});var $h=i(te);G2=n($h,"Training ever larger models can become challenging even on modern GPUs. Due to their immense size we often run out of GPU memory and training can take very long. In this section we have a look at a few tricks to reduce the memory footprint and speed up training for large models and how they are integrated in the "),Xo=r($h,"A",{href:!0});var XT=i(Xo);U2=n(XT,"Trainer"),XT.forEach(t),z2=n($h," and "),Is=r($h,"A",{href:!0,rel:!0});var YT=i(Is);S2=n(YT,"\u{1F917} Accelerate"),YT.forEach(t),N2=n($h,". Before we start make sure you have installed the following libraries:"),$h.forEach(t),zu=f(e),d(Gs.$$.fragment,e),Su=f(e),ae=r(e,"P",{});var kh=i(ae);C2=n(kh,"The "),Zh=r(kh,"CODE",{});var QT=i(Zh);L2=n(QT,"nvidia-ml-py3"),QT.forEach(t),M2=n(kh," library allows us to monitor the memory usage of the models from within Python. You might be familiar with the "),Kh=r(kh,"CODE",{});var JT=i(Kh);O2=n(JT,"nvidia-smi"),JT.forEach(t),B2=n(kh," command in the terminal - this library allows to access the same information in Python directly."),kh.forEach(t),Nu=f(e),At=r(e,"P",{});var $b=i(At);q2=n($b,"Then we create some dummy data. We create random token IDs between 100 and 30000 and binary labels for a classifier. In total we get 512 sequences each with length 512 and store them in a "),Us=r($b,"A",{href:!0,rel:!0});var ZT=i(Us);ef=r(ZT,"CODE",{});var KT=i(ef);V2=n(KT,"Dataset"),KT.forEach(t),ZT.forEach(t),H2=n($b," with PyTorch format."),$b.forEach(t),Cu=f(e),d(zs.$$.fragment,e),Lu=f(e),Tt=r(e,"P",{});var kb=i(Tt);F2=n(kb,"We want to print some summary statistics for the GPU utilization and the training run with the "),Yo=r(kb,"A",{href:!0});var ej=i(Yo);W2=n(ej,"Trainer"),ej.forEach(t),R2=n(kb,". We setup a two helper functions to do just that:"),kb.forEach(t),Mu=f(e),d(Ss.$$.fragment,e),Ou=f(e),Qo=r(e,"P",{});var tj=i(Qo);X2=n(tj,"Let\u2019s verify that we start with a free GPU memory:"),tj.forEach(t),Bu=f(e),d(Ns.$$.fragment,e),qu=f(e),Jo=r(e,"P",{});var aj=i(Jo);Y2=n(aj,"That looks good: the GPU memory is not occupied as we would expect before we load any models. If that\u2019s not the case on your machine make sure to stop all processes that are using GPU memory. However, not all free GPU memory can be used by the user. When a model is loaded to the GPU also the kernels are loaded which can take up 1-2GB of memory. To see how much it is we load a tiny tensor into the GPU which triggers the kernels to be loaded as well."),aj.forEach(t),Vu=f(e),d(Cs.$$.fragment,e),Hu=f(e),Zo=r(e,"P",{});var sj=i(Zo);Q2=n(sj,"We see that the kernels alone take up 1.3GB of GPU memory. Now let\u2019s see how much space the model uses."),sj.forEach(t),Fu=f(e),Pe=r(e,"H2",{class:!0});var Ab=i(Pe);jt=r(Ab,"A",{id:!0,class:!0,href:!0});var rj=i(jt);tf=r(rj,"SPAN",{});var ij=i(tf);d(Ls.$$.fragment,ij),ij.forEach(t),rj.forEach(t),J2=f(Ab),af=r(Ab,"SPAN",{});var oj=i(af);Z2=n(oj,"Load Model"),oj.forEach(t),Ab.forEach(t),Wu=f(e),xt=r(e,"P",{});var Tb=i(xt);K2=n(Tb,"First, we load the "),sf=r(Tb,"CODE",{});var lj=i(sf);e0=n(lj,"bert-large-uncased"),lj.forEach(t),t0=n(Tb," model. We load the model weights directly to the GPU so that we can check how much space just weights use."),Tb.forEach(t),Ru=f(e),d(Ms.$$.fragment,e),Xu=f(e),Dt=r(e,"P",{});var jb=i(Dt);a0=n(jb,"We can see that the model weights alone take up 1.3 GB of the GPU memory. The exact number depends on the specific GPU you are using. Note that on newer GPUs a model can sometimes take up more space since the weights are loaded in an optimized fashion that speeds up the usage of the model. Now we can also quickly check if we get the same result as with "),rf=r(jb,"CODE",{});var nj=i(rf);s0=n(nj,"nvidia-smi"),nj.forEach(t),r0=n(jb," CLI:"),jb.forEach(t),Yu=f(e),d(Os.$$.fragment,e),Qu=f(e),d(Bs.$$.fragment,e),Ju=f(e),Ko=r(e,"P",{});var pj=i(Ko);i0=n(pj,"We get the same number as before and you can also see that we are using a V100 GPU with 16GB of memory. So now we can start training the model and see how the GPU memory consumption changes. First, we set up a few standard training arguments that we will use across all our experiments:"),pj.forEach(t),Zu=f(e),d(qs.$$.fragment,e),Ku=f(e),d(It.$$.fragment,e),ev=f(e),$e=r(e,"H2",{class:!0});var xb=i($e);Gt=r(xb,"A",{id:!0,class:!0,href:!0});var hj=i(Gt);of=r(hj,"SPAN",{});var fj=i(of);d(Vs.$$.fragment,fj),fj.forEach(t),hj.forEach(t),o0=f(xb),lf=r(xb,"SPAN",{});var cj=i(lf);l0=n(cj,"Vanilla Training"),cj.forEach(t),xb.forEach(t),tv=f(e),Ut=r(e,"P",{});var Db=i(Ut);n0=n(Db,"As a first experiment we will use the "),el=r(Db,"A",{href:!0});var mj=i(el);p0=n(mj,"Trainer"),mj.forEach(t),h0=n(Db," and train the model without any further modifications and a batch size of 4:"),Db.forEach(t),av=f(e),d(Hs.$$.fragment,e),sv=f(e),d(Fs.$$.fragment,e),rv=f(e),tl=r(e,"P",{});var dj=i(tl);f0=n(dj,"We see that already a relatively small batch size almost fills up our GPU\u2019s entire memory. However, a larger batch size can often result in faster model convergence or better end performance. So ideally we want to tune the batch size to our model\u2019s needs and not to the GPU limitations. A simple trick to effectively train larger batch size is gradient accumulation."),dj.forEach(t),iv=f(e),ke=r(e,"H2",{class:!0});var Ib=i(ke);zt=r(Ib,"A",{id:!0,class:!0,href:!0});var uj=i(zt);nf=r(uj,"SPAN",{});var vj=i(nf);d(Ws.$$.fragment,vj),vj.forEach(t),uj.forEach(t),c0=f(Ib),pf=r(Ib,"SPAN",{});var wj=i(pf);m0=n(wj,"Gradient Accumulation"),wj.forEach(t),Ib.forEach(t),ov=f(e),al=r(e,"P",{});var gj=i(al);d0=n(gj,"The idea behind gradient accumulation is to instead of calculating the gradients for the whole batch at once to do it in smaller steps. The way we do that is to calculate the gradients iteratively in smaller batches by doing a forward and backward pass through the model and accumulating the gradients in the process. When enough gradients are accumulated we run the model\u2019s optimization step. This way we can easily increase the overall batch size to numbers that would never fit into the GPU\u2019s memory. In turn, however, the added forward and backward passes can slow down the training a bit."),gj.forEach(t),lv=f(e),N=r(e,"P",{});var ds=i(N);u0=n(ds,"We can use gradient accumulation in the "),sl=r(ds,"A",{href:!0});var _j=i(sl);v0=n(_j,"Trainer"),_j.forEach(t),w0=n(ds," by simply adding the "),hf=r(ds,"CODE",{});var bj=i(hf);g0=n(bj,"gradient_accumulation_steps"),bj.forEach(t),_0=n(ds," argument to "),rl=r(ds,"A",{href:!0});var yj=i(rl);b0=n(yj,"TrainingArguments"),yj.forEach(t),y0=n(ds,". Let\u2019s see how it impacts the models memory footprint:"),ds.forEach(t),nv=f(e),d(Rs.$$.fragment,e),pv=f(e),d(Xs.$$.fragment,e),hv=f(e),D=r(e,"P",{});var be=i(D);E0=n(be,"We can see that the memory footprint was dramatically reduced at the cost of being only slightly slower than the vanilla run. Of course, this would change as you increase the number of accumulation steps. In general you would want to max out the GPU usage as much as possible. So in our case, the batch_size of 4 was already pretty close to the GPU\u2019s limit. If we wanted to train with a batch size of 64 we should not use "),ff=r(be,"CODE",{});var Ej=i(ff);P0=n(Ej,"per_device_train_batch_size=1"),Ej.forEach(t),$0=n(be," and "),cf=r(be,"CODE",{});var Pj=i(cf);k0=n(Pj,"gradient_accumulation_steps=64"),Pj.forEach(t),A0=n(be," but instead "),mf=r(be,"CODE",{});var $j=i(mf);T0=n($j,"per_device_train_batch_size=4"),$j.forEach(t),j0=n(be," and "),df=r(be,"CODE",{});var kj=i(df);x0=n(kj,"gradient_accumulation_steps=16"),kj.forEach(t),D0=n(be," which has the same effective batch size while making better use of the available GPU resources."),be.forEach(t),fv=f(e),il=r(e,"P",{});var Aj=i(il);I0=n(Aj,"Next we have a look at another trick to save a little bit more GPU memory called gradient checkpointing."),Aj.forEach(t),cv=f(e),Ae=r(e,"H2",{class:!0});var Gb=i(Ae);St=r(Gb,"A",{id:!0,class:!0,href:!0});var Tj=i(St);uf=r(Tj,"SPAN",{});var jj=i(uf);d(Ys.$$.fragment,jj),jj.forEach(t),Tj.forEach(t),G0=f(Gb),vf=r(Gb,"SPAN",{});var xj=i(vf);U0=n(xj,"Gradient Checkpointing"),xj.forEach(t),Gb.forEach(t),mv=f(e),ol=r(e,"P",{});var Dj=i(ol);z0=n(Dj,"Even when we set the batch size to 1 and use gradient accumulation we can still run out of memory when working with large models. In order to compute the gradients during the backward pass all activations from the forward pass are normally saved. This can create a big memory overhead. Alternatively, one could forget all activations during the forward pass and recompute them on demand during the backward pass. This would however add a significant computational overhead and slow down training."),Dj.forEach(t),dv=f(e),Nt=r(e,"P",{});var Ub=i(Nt);S0=n(Ub,"Gradient checkpointing strikes a compromise between the two approaches and saves strategically selected activations throughout the computational graph so only a fraction of the activations need to be re-computed for the gradients. See "),Qs=r(Ub,"A",{href:!0,rel:!0});var Ij=i(Qs);N0=n(Ij,"this great article"),Ij.forEach(t),C0=n(Ub," explaining the ideas behind gradient checkpointing."),Ub.forEach(t),uv=f(e),se=r(e,"P",{});var Ah=i(se);L0=n(Ah,"To enable gradient checkpointing in the "),ll=r(Ah,"A",{href:!0});var Gj=i(ll);M0=n(Gj,"Trainer"),Gj.forEach(t),O0=n(Ah," we only need ot pass it as a flag to the "),nl=r(Ah,"A",{href:!0});var Uj=i(nl);B0=n(Uj,"TrainingArguments"),Uj.forEach(t),q0=n(Ah,". Everything else is handled under the hood:"),Ah.forEach(t),vv=f(e),d(Js.$$.fragment,e),wv=f(e),d(Zs.$$.fragment,e),gv=f(e),pl=r(e,"P",{});var zj=i(pl);V0=n(zj,"We can see that this saved some more memory but at the same time training became a bit slower. A general rule of thumb is that gradient checkpointing slows down training by about 20%. Let\u2019s have a look at another method with which we can regain some speed: mixed precision training."),zj.forEach(t),_v=f(e),Te=r(e,"H2",{class:!0});var zb=i(Te);Ct=r(zb,"A",{id:!0,class:!0,href:!0});var Sj=i(Ct);wf=r(Sj,"SPAN",{});var Nj=i(wf);d(Ks.$$.fragment,Nj),Nj.forEach(t),Sj.forEach(t),H0=f(zb),gf=r(zb,"SPAN",{});var Cj=i(gf);F0=n(Cj,"FP16 Training"),Cj.forEach(t),zb.forEach(t),bv=f(e),re=r(e,"P",{});var Th=i(re);W0=n(Th,"The idea of mixed precision training is that no all variables need to be stored in full (32-bit) floating point precision. If we can reduce the precision the variales and their computations are faster. The main advantage comes from saving the activations in half (16-bit) precision. Although the gradients are also computed in half precision they are converted back to full precision for the optimization step so no memory is saved here. Since the model is present on the GPU in both 16-bit and 32-bit precision this can use more GPU memory (1.5x the original model is on the GPU), especially for small batch sizes. Since some computations are performed in full and some in half precision this approach is also called mixed precision training. Enabling mixed precision training is also just a matter of setting the "),_f=r(Th,"CODE",{});var Lj=i(_f);R0=n(Lj,"fp16"),Lj.forEach(t),X0=n(Th," flag to "),bf=r(Th,"CODE",{});var Mj=i(bf);Y0=n(Mj,"True"),Mj.forEach(t),Q0=n(Th,":"),Th.forEach(t),yv=f(e),d(er.$$.fragment,e),Ev=f(e),d(tr.$$.fragment,e),Pv=f(e),hl=r(e,"P",{});var Oj=i(hl);J0=n(Oj,"We can see that this is almost twice as fast as the vanilla training. Let\u2019s add it to the mix of the previous methods:"),Oj.forEach(t),$v=f(e),d(ar.$$.fragment,e),kv=f(e),d(sr.$$.fragment,e),Av=f(e),fl=r(e,"P",{});var Bj=i(fl);Z0=n(Bj,"We can see that with these tweaks we use about half the GPU memory as at the beginning while also being slightly faster. But we are not done, yet! There is another area where we can save GPU memory: the optimizer."),Bj.forEach(t),Tv=f(e),je=r(e,"H2",{class:!0});var Sb=i(je);Lt=r(Sb,"A",{id:!0,class:!0,href:!0});var qj=i(Lt);yf=r(qj,"SPAN",{});var Vj=i(yf);d(rr.$$.fragment,Vj),Vj.forEach(t),qj.forEach(t),K0=f(Sb),Ef=r(Sb,"SPAN",{});var Hj=i(Ef);eE=n(Hj,"Optimizer"),Hj.forEach(t),Sb.forEach(t),jv=f(e),cl=r(e,"P",{});var Fj=i(cl);tE=n(Fj,"The most common optimizer used to train transformer model is Adam or AdamW (Adam with weight decay). Adam achieves good convergence by storing the rolling average of the previous gradients which, however, adds an additional memory footprint of the order of the number of model parameters. One remedy to this is to use an alternative optimizer such as Adafactor."),Fj.forEach(t),xv=f(e),xe=r(e,"H3",{class:!0});var Nb=i(xe);Mt=r(Nb,"A",{id:!0,class:!0,href:!0});var Wj=i(Mt);Pf=r(Wj,"SPAN",{});var Rj=i(Pf);d(ir.$$.fragment,Rj),Rj.forEach(t),Wj.forEach(t),aE=f(Nb),$f=r(Nb,"SPAN",{});var Xj=i($f);sE=n(Xj,"Adafactor"),Xj.forEach(t),Nb.forEach(t),Dv=f(e),Ot=r(e,"P",{});var Cb=i(Ot);rE=n(Cb,"Instead of keeping the rolling average for each element in the weight matrices Adafactor only stores aggregated information (row- and column-wise sums of the rolling averages) which reduces the footprint considerably. One downside of Adafactor is that in some instances convergence can be slower than Adam\u2019s so some experimentation is advised here. We can use Adafactor simply by setting "),kf=r(Cb,"CODE",{});var Yj=i(kf);iE=n(Yj,'optim="adafactor"'),Yj.forEach(t),oE=n(Cb,":"),Cb.forEach(t),Iv=f(e),d(or.$$.fragment,e),Gv=f(e),d(lr.$$.fragment,e),Uv=f(e),ml=r(e,"P",{});var Qj=i(ml);lE=n(Qj,"We can see that this saves a few more GB on the GPU. Let\u2019s see how it looks when we add it to the other methods we introduced earlier:"),Qj.forEach(t),zv=f(e),d(nr.$$.fragment,e),Sv=f(e),d(pr.$$.fragment,e),Nv=f(e),dl=r(e,"P",{});var Jj=i(dl);nE=n(Jj,"We went from 15 GB memory usage to 5 GB - a 3x improvement while maintaining the throughput! However, as mentioned before, the convergence of Adafactor can be worse than Adam. There is an alternative to Adafactor called 8-bit Adam that takes a slightly different approach."),Jj.forEach(t),Cv=f(e),De=r(e,"H3",{class:!0});var Lb=i(De);Bt=r(Lb,"A",{id:!0,class:!0,href:!0});var Zj=i(Bt);Af=r(Zj,"SPAN",{});var Kj=i(Af);d(hr.$$.fragment,Kj),Kj.forEach(t),Zj.forEach(t),pE=f(Lb),Tf=r(Lb,"SPAN",{});var ex=i(Tf);hE=n(ex,"8-bit Adam"),ex.forEach(t),Lb.forEach(t),Lv=f(e),ul=r(e,"P",{});var tx=i(ul);fE=n(tx,"Instead of aggregating optimizer states like Adafactor, 8-bit Adam keeps the full state and quantizes it. Quantization means that it stores the state with lower precision and dequantizes it only for the optimization. This is similar to the idea behind FP16 training where using variables with lower precision saves memory."),tx.forEach(t),Mv=f(e),I=r(e,"P",{});var ye=i(I);cE=n(ye,"In contrast to the previous approaches is this one not integrated into the "),vl=r(ye,"A",{href:!0});var ax=i(vl);mE=n(ax,"Trainer"),ax.forEach(t),dE=n(ye," as a simple flag. We need to install the 8-bit optimizer and then pass it as a custom optimizer to the "),wl=r(ye,"A",{href:!0});var sx=i(wl);uE=n(sx,"Trainer"),sx.forEach(t),vE=n(ye,". Follow the installation guide in the Github "),fr=r(ye,"A",{href:!0,rel:!0});var rx=i(fr);wE=n(rx,"repo"),rx.forEach(t),gE=n(ye," to install the "),jf=r(ye,"CODE",{});var ix=i(jf);_E=n(ix,"bitsandbytes"),ix.forEach(t),bE=n(ye," library that implements the 8-bit Adam optimizer."),ye.forEach(t),Ov=f(e),gl=r(e,"P",{});var ox=i(gl);yE=n(ox,"Once installed, we just need to initialize the the optimizer. Although this looks like a considerable amount of work it actually just involves two steps: first we need to group the model\u2019s parameters into two groups where to one group we apply weight decay and to the other we don\u2019t. Usually, biases and layer norm parameters are not weight decayed. Then in a second step we just do some argument housekeeping to use the same parameters as the previously used AdamW optimizer."),ox.forEach(t),Bv=f(e),d(qt.$$.fragment,e),qv=f(e),d(cr.$$.fragment,e),Vv=f(e),Vt=r(e,"P",{});var Mb=i(Vt);EE=n(Mb,"We can now pass the custom optimizer as an argument to the "),xf=r(Mb,"CODE",{});var lx=i(xf);PE=n(lx,"Trainer"),lx.forEach(t),$E=n(Mb,":"),Mb.forEach(t),Hv=f(e),d(mr.$$.fragment,e),Fv=f(e),d(dr.$$.fragment,e),Wv=f(e),_l=r(e,"P",{});var nx=i(_l);kE=n(nx,"We can see that we get a similar memory improvement as with Adafactor while keeping the full rolling average of the gradients. Let\u2019s repeat the experiment with the full settings:"),nx.forEach(t),Rv=f(e),d(ur.$$.fragment,e),Xv=f(e),d(vr.$$.fragment,e),Yv=f(e),bl=r(e,"P",{});var px=i(bl);AE=n(px,"Again, we get about a 3x memory improvement and even slightly higher throughput as using Adafactor. So we have seen how we can optimize the memory footprint of large models. The following plot summarizes all our experiments:"),px.forEach(t),Qv=f(e),yl=r(e,"P",{});var hx=i(yl);El=r(hx,"IMG",{src:!0,alt:!0}),hx.forEach(t),Jv=f(e),Ie=r(e,"H2",{class:!0});var Ob=i(Ie);Ht=r(Ob,"A",{id:!0,class:!0,href:!0});var fx=i(Ht);Df=r(fx,"SPAN",{});var cx=i(Df);d(wr.$$.fragment,cx),cx.forEach(t),fx.forEach(t),TE=f(Ob),If=r(Ob,"SPAN",{});var mx=i(If);jE=n(mx,"Using \u{1F917} Accelerate"),mx.forEach(t),Ob.forEach(t),Zv=f(e),ie=r(e,"P",{});var jh=i(ie);xE=n(jh,"So far we have used the "),Pl=r(jh,"A",{href:!0});var dx=i(Pl);DE=n(dx,"Trainer"),dx.forEach(t),IE=n(jh," to run the experiments but a more flexible alternative to that approach is to use \u{1F917} Accelerate. With \u{1F917} Accelerate you have full control over the training loop and can essentially write the loop in pure PyTorch with some minor modifications. In turn it allows you to easily scale across different infrastructures such as CPUs, GPUs, TPUs, or distributed multi-GPU setups without changing any code. Let\u2019s see what it takes to implement all of the above tweaks in \u{1F917} Accelerate. We can still use the "),$l=r(jh,"A",{href:!0});var ux=i($l);GE=n(ux,"TrainingArguments"),ux.forEach(t),UE=n(jh," to wrap the training settings:"),jh.forEach(t),Kv=f(e),d(gr.$$.fragment,e),ew=f(e),kl=r(e,"P",{});var vx=i(kl);zE=n(vx,"The full example training loop with \u{1F917} Accelerate is only a handful of lines of code long:"),vx.forEach(t),tw=f(e),d(_r.$$.fragment,e),aw=f(e),k=r(e,"P",{});var F=i(k);SE=n(F,"First we wrap the dataset in a "),br=r(F,"A",{href:!0,rel:!0});var wx=i(br);Gf=r(wx,"CODE",{});var gx=i(Gf);NE=n(gx,"DataLoader"),gx.forEach(t),wx.forEach(t),CE=n(F,". Then we can enable gradient checkpointing by calling the model\u2019s "),Al=r(F,"A",{href:!0});var _x=i(Al);LE=n(_x,"gradient_checkpointing_enable()"),_x.forEach(t),ME=n(F," method. When we initialize the "),yr=r(F,"A",{href:!0,rel:!0});var bx=i(yr);Uf=r(bx,"CODE",{});var yx=i(Uf);OE=n(yx,"Accelerator"),yx.forEach(t),bx.forEach(t),BE=n(F," we can specifiy if we want to use mixed precision training and it will take care of it for us in the "),zf=r(F,"CODE",{});var Ex=i(zf);qE=n(Ex,"prepare"),Ex.forEach(t),VE=n(F," call. During the "),Er=r(F,"A",{href:!0,rel:!0});var Px=i(Er);Sf=r(Px,"CODE",{});var $x=i(Sf);HE=n($x,"prepare"),$x.forEach(t),Px.forEach(t),FE=n(F," call the dataloader will also be distributed across workers should we use multiple GPUs. We use the same 8-bit optimizer from the earlier experiments."),F.forEach(t),sw=f(e),Ft=r(e,"P",{});var Bb=i(Ft);WE=n(Bb,"Finally, we can write the main training loop. Note that the "),Nf=r(Bb,"CODE",{});var kx=i(Nf);RE=n(kx,"backward"),kx.forEach(t),XE=n(Bb," call is handled by \u{1F917} Accelerate. We can also see how gradient accumulation works: we normalize the loss so we get the average at the end of accumulation and once we have enough steps we run the optimization. Now the question is: does this use the same amount of memory as the previous steps? Let\u2019s check:"),Bb.forEach(t),rw=f(e),d(Pr.$$.fragment,e),iw=f(e),Tl=r(e,"P",{});var Ax=i(Tl);YE=n(Ax,"Indeed it does. Implementing these optimization techniques with \u{1F917} Accelerate only takes a handful of lines of code and comes with the benefit of more flexiblity in the training loop."),Ax.forEach(t),ow=f(e),jl=r(e,"P",{});var Tx=i(jl);QE=n(Tx,"Now, let\u2019s take a step back and discuss what we should optimize for when scaling the training of large models."),Tx.forEach(t),lw=f(e),Ge=r(e,"H2",{class:!0});var qb=i(Ge);Wt=r(qb,"A",{id:!0,class:!0,href:!0});var jx=i(Wt);Cf=r(jx,"SPAN",{});var xx=i(Cf);d($r.$$.fragment,xx),xx.forEach(t),jx.forEach(t),JE=f(qb),Lf=r(qb,"SPAN",{});var Dx=i(Lf);ZE=n(Dx,"How to scale"),Dx.forEach(t),qb.forEach(t),nw=f(e),xl=r(e,"P",{});var Ix=i(xl);KE=n(Ix,"When we train models there are a two aspects we want to optimize at the same time:"),Ix.forEach(t),pw=f(e),Rt=r(e,"UL",{});var Vb=i(Rt);Mf=r(Vb,"LI",{});var Gx=i(Mf);e3=n(Gx,"Data throughput/training time"),Gx.forEach(t),t3=f(Vb),Of=r(Vb,"LI",{});var Ux=i(Of);a3=n(Ux,"Model performance"),Ux.forEach(t),Vb.forEach(t),hw=f(e),Dl=r(e,"P",{});var zx=i(Dl);s3=n(zx,"We have seen that each method changes the memory usage and throughput. In general we want to maximize the throughput (samples/second) to minimize the training cost. This is generally achieved by utilizing the GPU as much as possible and thus filling GPU memory to its limit. For example, as mentioned earlier, we only employ gradient accumulation when we want to use a batch size beyond the size of the GPU memory. If the desired batch size fits into memory then there is no reason to apply gradient accumulation which will only slow down training."),zx.forEach(t),fw=f(e),Il=r(e,"P",{});var Sx=i(Il);r3=n(Sx,"The second objective is model performance. Just because we can does not mean we should use a large batch size. As part of hyperparameter tuning you should determine which batch size yields the best result and then optimize the throughput accordingly."),Sx.forEach(t),cw=f(e),Gl=r(e,"P",{});var Nx=i(Gl);i3=n(Nx,"Sometimes, even when applying all the above tweaks the throughput on a given GPU might still not be good enough. One easy solution is to change the type of GPU. For example switching from let\u2019s say a K80 (which you typically get on Google Colab) to a fancier GPU such as the V100 or A100. Although they are more expensive they are usually more cost effective than cheaper GPUs due to their larger memory and faster architecture. For some applications, such as pretraining, this might still not be fast enough. In this case you want to scale your experiment to several GPUs."),Nx.forEach(t),mw=f(e),Ue=r(e,"H2",{class:!0});var Hb=i(Ue);Xt=r(Hb,"A",{id:!0,class:!0,href:!0});var Cx=i(Xt);Bf=r(Cx,"SPAN",{});var Lx=i(Bf);d(kr.$$.fragment,Lx),Lx.forEach(t),Cx.forEach(t),o3=f(Hb),qf=r(Hb,"SPAN",{});var Mx=i(qf);l3=n(Mx,"Multi-GPU Training"),Mx.forEach(t),Hb.forEach(t),dw=f(e),C=r(e,"P",{});var us=i(C);n3=n(us,"If your model fits on a single GPU scaling to many GPUs can be achieved fairly easily with data parallelism. The idea is very similar to gradient accumulation with the distinction that instead of running the forward and backward passes during the accumulation in sequence on a single machine they are performed in parallel on multiple machines. So each GPU gets a small batch, runs the forward and backward passes and then the gradients from all machines are aggregated and the model is optimized. You can combine this with all the methods we described before. For example, if you have 4 GPUs and use "),Vf=r(us,"CODE",{});var Ox=i(Vf);p3=n(Ox,"per_device_train_batch_size=12"),Ox.forEach(t),h3=n(us," and "),Hf=r(us,"CODE",{});var Bx=i(Hf);f3=n(Bx,"gradient_accumulation_steps=3"),Bx.forEach(t),c3=n(us," you will have an effective batch size of "),Ff=r(us,"CODE",{});var qx=i(Ff);m3=n(qx,"4*12*3=144"),qx.forEach(t),d3=n(us,"."),us.forEach(t),uw=f(e),L=r(e,"P",{});var vs=i(L);u3=n(vs,"The "),Ul=r(vs,"A",{href:!0});var Vx=i(Ul);v3=n(Vx,"Trainer"),Vx.forEach(t),w3=n(vs," allows for distributed training and if you execute your "),zl=r(vs,"A",{href:!0});var Hx=i(zl);g3=n(Hx,"Trainer"),Hx.forEach(t),_3=n(vs," training script on a machine with multiple GPUs it will automatically utilize all of them, hence the name "),Wf=r(vs,"CODE",{});var Fx=i(Wf);b3=n(Fx,"per_device_train_batch_size"),Fx.forEach(t),y3=n(vs,". In \u{1F917} Accelerate you can configure the infrastructure setup with the following command:"),vs.forEach(t),vw=f(e),d(Ar.$$.fragment,e),ww=f(e),Sl=r(e,"P",{});var Wx=i(Sl);E3=n(Wx,"Until now we have opperated under the assumption that we can fit the model onto a single GPU without or with the introduced tricks . But what if this is not possible? We still have a few tricks up our sleeves!"),Wx.forEach(t),gw=f(e),ze=r(e,"H2",{class:!0});var Fb=i(ze);Yt=r(Fb,"A",{id:!0,class:!0,href:!0});var Rx=i(Yt);Rf=r(Rx,"SPAN",{});var Xx=i(Rf);d(Tr.$$.fragment,Xx),Xx.forEach(t),Rx.forEach(t),P3=f(Fb),Xf=r(Fb,"SPAN",{});var Yx=i(Xf);$3=n(Yx,"What if my model still does not fit?"),Yx.forEach(t),Fb.forEach(t),_w=f(e),Qt=r(e,"P",{});var Wb=i(Qt);k3=n(Wb,"If the model does not fit on a single GPU with all the mentioned tricks there are still more methods we can apply although life starts to get a bit more complicated. This usually involves some form of pipeline or tensor parallelism where the model itself is distributed across several GPUs. One can also make use of DeepSpeed which implements some of these parallelism strategies along with some more optimization to reduce the memory footprint such as partitioning the optimizer states. You can read more about this in the "),Nl=r(Wb,"A",{href:!0});var Qx=i(Nl);A3=n(Qx,"\u201CModel Parallelism\u201D section"),Qx.forEach(t),T3=n(Wb,"."),Wb.forEach(t),bw=f(e),Cl=r(e,"P",{});var Jx=i(Cl);j3=n(Jx,"This concludes the practical part of this guide for scaling the training of large models. The following section goes into more details on some of the aspects discussed above."),Jx.forEach(t),yw=f(e),Se=r(e,"H2",{class:!0});var Rb=i(Se);Jt=r(Rb,"A",{id:!0,class:!0,href:!0});var Zx=i(Jt);Yf=r(Zx,"SPAN",{});var Kx=i(Yf);d(jr.$$.fragment,Kx),Kx.forEach(t),Zx.forEach(t),x3=f(Rb),Qf=r(Rb,"SPAN",{});var eD=i(Qf);D3=n(eD,"Further discussions"),eD.forEach(t),Rb.forEach(t),Ew=f(e),Ll=r(e,"P",{});var tD=i(Ll);I3=n(tD,"This section gives brief ideas on how to make training faster and support bigger models. Later sections will expand, demonstrate and elucidate each of these."),tD.forEach(t),Pw=f(e),Ne=r(e,"H2",{class:!0});var Xb=i(Ne);Zt=r(Xb,"A",{id:!0,class:!0,href:!0});var aD=i(Zt);Jf=r(aD,"SPAN",{});var sD=i(Jf);d(xr.$$.fragment,sD),sD.forEach(t),aD.forEach(t),G3=f(Xb),Zf=r(Xb,"SPAN",{});var rD=i(Zf);U3=n(rD,"Faster Training"),rD.forEach(t),Xb.forEach(t),$w=f(e),Ml=r(e,"P",{});var iD=i(Ml);z3=n(iD,"Hardware:"),iD.forEach(t),kw=f(e),Ol=r(e,"UL",{});var oD=i(Ol);Bl=r(oD,"LI",{});var _T=i(Bl);S3=n(_T,"fast connectivity between GPUs"),Dr=r(_T,"UL",{});var Yb=i(Dr);Kf=r(Yb,"LI",{});var lD=i(Kf);N3=n(lD,"intra-node: NVLink"),lD.forEach(t),C3=f(Yb),ec=r(Yb,"LI",{});var nD=i(ec);L3=n(nD,"inter-node: Infiniband / Intel OPA"),nD.forEach(t),Yb.forEach(t),_T.forEach(t),oD.forEach(t),Aw=f(e),ql=r(e,"P",{});var pD=i(ql);M3=n(pD,"Software:"),pD.forEach(t),Tw=f(e),Kt=r(e,"UL",{});var Qb=i(Kt);tc=r(Qb,"LI",{});var hD=i(tc);O3=n(hD,"Data Parallel / Distributed Data Parallel"),hD.forEach(t),B3=f(Qb),ac=r(Qb,"LI",{});var fD=i(ac);q3=n(fD,"fp16 (autocast caching)"),fD.forEach(t),Qb.forEach(t),jw=f(e),Ce=r(e,"H2",{class:!0});var Jb=i(Ce);ea=r(Jb,"A",{id:!0,class:!0,href:!0});var cD=i(ea);sc=r(cD,"SPAN",{});var mD=i(sc);d(Ir.$$.fragment,mD),mD.forEach(t),cD.forEach(t),V3=f(Jb),rc=r(Jb,"SPAN",{});var dD=i(rc);H3=n(dD,"Bigger Models"),dD.forEach(t),Jb.forEach(t),xw=f(e),Vl=r(e,"P",{});var uD=i(Vl);F3=n(uD,"Hardware:"),uD.forEach(t),Dw=f(e),oe=r(e,"UL",{});var xh=i(oe);ic=r(xh,"LI",{});var vD=i(ic);W3=n(vD,"bigger GPUs"),vD.forEach(t),R3=f(xh),oc=r(xh,"LI",{});var wD=i(oc);X3=n(wD,"more GPUs"),wD.forEach(t),Y3=f(xh),Gr=r(xh,"LI",{});var Zb=i(Gr);Q3=n(Zb,"more CPU and NVMe (offloaded to by "),Hl=r(Zb,"A",{href:!0});var gD=i(Hl);J3=n(gD,"DeepSpeed-Infinity"),gD.forEach(t),Z3=n(Zb,")"),Zb.forEach(t),xh.forEach(t),Iw=f(e),Fl=r(e,"P",{});var _D=i(Fl);K3=n(_D,"Software:"),_D.forEach(t),Gw=f(e),E=r(e,"UL",{});var U=i(E);lc=r(U,"LI",{});var bD=i(lc);e6=n(bD,"Model Scalability (ZeRO and 3D Parallelism)"),bD.forEach(t),t6=f(U),nc=r(U,"LI",{});var yD=i(nc);a6=n(yD,"Low-memory Optimizers"),yD.forEach(t),s6=f(U),pc=r(U,"LI",{});var ED=i(pc);r6=n(ED,"fp16/bf16 (smaller data/faster throughput)"),ED.forEach(t),i6=f(U),hc=r(U,"LI",{});var PD=i(hc);o6=n(PD,"tf32 (faster throughput)"),PD.forEach(t),l6=f(U),fc=r(U,"LI",{});var $D=i(fc);n6=n($D,"Gradient accumulation"),$D.forEach(t),p6=f(U),cc=r(U,"LI",{});var kD=i(cc);h6=n(kD,"Gradient checkpointing"),kD.forEach(t),f6=f(U),mc=r(U,"LI",{});var AD=i(mc);c6=n(AD,"Sparsity"),AD.forEach(t),U.forEach(t),Uw=f(e),Le=r(e,"H2",{class:!0});var Kb=i(Le);ta=r(Kb,"A",{id:!0,class:!0,href:!0});var TD=i(ta);dc=r(TD,"SPAN",{});var jD=i(dc);d(Ur.$$.fragment,jD),jD.forEach(t),TD.forEach(t),m6=f(Kb),uc=r(Kb,"SPAN",{});var xD=i(uc);d6=n(xD,"Hardware"),xD.forEach(t),Kb.forEach(t),zw=f(e),Me=r(e,"H3",{class:!0});var ey=i(Me);aa=r(ey,"A",{id:!0,class:!0,href:!0});var DD=i(aa);vc=r(DD,"SPAN",{});var ID=i(vc);d(zr.$$.fragment,ID),ID.forEach(t),DD.forEach(t),u6=f(ey),wc=r(ey,"SPAN",{});var GD=i(wc);v6=n(GD,"Power and Cooling"),GD.forEach(t),ey.forEach(t),Sw=f(e),Wl=r(e,"P",{});var UD=i(Wl);w6=n(UD,"If you bought an expensive high end GPU make sure you give it the correct power and sufficient cooling."),UD.forEach(t),Nw=f(e),Sr=r(e,"P",{});var bT=i(Sr);gc=r(bT,"STRONG",{});var zD=i(gc);g6=n(zD,"Power"),zD.forEach(t),_6=n(bT,":"),bT.forEach(t),Cw=f(e),Rl=r(e,"P",{});var SD=i(Rl);b6=n(SD,"Some high end consumer GPU cards have 2 and sometimes 3 PCI-E 8-Pin power sockets. Make sure you have as many independent 12V PCI-E 8-Pin cables plugged into the card as there are sockets. Do not use the 2 splits at one end of the same cable (also known as pigtail cable). That is if you have 2 sockets on the GPU, you want 2 PCI-E 8-Pin cables going from your PSU to the card and not one that has 2 PCI-E 8-Pin connectors at the end! You won\u2019t get the full performance out of your card otherwise."),SD.forEach(t),Lw=f(e),Xl=r(e,"P",{});var ND=i(Xl);y6=n(ND,"Each PCI-E 8-Pin power cable needs to be plugged into a 12V rail on the PSU side and can supply up to 150W of power."),ND.forEach(t),Mw=f(e),Yl=r(e,"P",{});var CD=i(Yl);E6=n(CD,"Some other cards may use a PCI-E 12-Pin connectors, and these can deliver up to 500-600W of power."),CD.forEach(t),Ow=f(e),Ql=r(e,"P",{});var LD=i(Ql);P6=n(LD,"Low end cards may use 6-Pin connectors, which supply up to 75W of power."),LD.forEach(t),Bw=f(e),Jl=r(e,"P",{});var MD=i(Jl);$6=n(MD,"Additionally you want the high-end PSU that has stable voltage. Some lower quality ones may not give the card the stable voltage it needs to function at its peak."),MD.forEach(t),qw=f(e),Zl=r(e,"P",{});var OD=i(Zl);k6=n(OD,"And of course the PSU needs to have enough unused Watts to power the card."),OD.forEach(t),Vw=f(e),Nr=r(e,"P",{});var yT=i(Nr);_c=r(yT,"STRONG",{});var BD=i(_c);A6=n(BD,"Cooling"),BD.forEach(t),T6=n(yT,":"),yT.forEach(t),Hw=f(e),Kl=r(e,"P",{});var qD=i(Kl);j6=n(qD,"When a GPU gets overheated it would start throttling down and will not deliver full performance. And it will shutdown if it gets too hot."),qD.forEach(t),Fw=f(e),en=r(e,"P",{});var VD=i(en);x6=n(VD,"It\u2019s hard to tell the exact best temperature to strive for when a GPU is heavily loaded, but probably anything under +80C is good, but lower is better - perhaps 70-75C is an excellent range to be in. The throttling down is likely to start at around 84-90C. But other than throttling performance a prolonged very higher temperature is likely to reduce the lifespan of a GPU."),VD.forEach(t),Ww=f(e),Oe=r(e,"H3",{class:!0});var ty=i(Oe);sa=r(ty,"A",{id:!0,class:!0,href:!0});var HD=i(sa);bc=r(HD,"SPAN",{});var FD=i(bc);d(Cr.$$.fragment,FD),FD.forEach(t),HD.forEach(t),D6=f(ty),yc=r(ty,"SPAN",{});var WD=i(yc);I6=n(WD,"Multi-GPU Connectivity"),WD.forEach(t),ty.forEach(t),Rw=f(e),tn=r(e,"P",{});var RD=i(tn);G6=n(RD,"If you use multiple GPUs the way cards are inter-connected can have a huge impact on the total training time."),RD.forEach(t),Xw=f(e),an=r(e,"P",{});var XD=i(an);U6=n(XD,"If the GPUs are on the same physical node, you can run:"),XD.forEach(t),Yw=f(e),d(Lr.$$.fragment,e),Qw=f(e),sn=r(e,"P",{});var YD=i(sn);z6=n(YD,"and it will tell you how the GPUs are inter-connected."),YD.forEach(t),Jw=f(e),rn=r(e,"P",{});var QD=i(rn);S6=n(QD,"On a machine with dual-GPU and which are connected with NVLink, you will most likely see something like:"),QD.forEach(t),Zw=f(e),d(Mr.$$.fragment,e),Kw=f(e),on=r(e,"P",{});var JD=i(on);N6=n(JD,"on a different machine w/o NVLink we may see:"),JD.forEach(t),e1=f(e),d(Or.$$.fragment,e),t1=f(e),ln=r(e,"P",{});var ZD=i(ln);C6=n(ZD,"The report includes this legend:"),ZD.forEach(t),a1=f(e),d(Br.$$.fragment,e),s1=f(e),le=r(e,"P",{});var Dh=i(le);L6=n(Dh,"So the first report "),Ec=r(Dh,"CODE",{});var KD=i(Ec);M6=n(KD,"NV2"),KD.forEach(t),O6=n(Dh," tells us the GPUs are interconnected with 2 NVLinks, and the second report "),Pc=r(Dh,"CODE",{});var eI=i(Pc);B6=n(eI,"PHB"),eI.forEach(t),q6=n(Dh," we have a typical consumer-level PCIe+Bridge setup."),Dh.forEach(t),r1=f(e),nn=r(e,"P",{});var tI=i(nn);V6=n(tI,"Check what type of connectivity you have on your setup. Some of these will make the communication between cards faster (e.g. NVLink), others slower (e.g. PHB)."),tI.forEach(t),i1=f(e),pn=r(e,"P",{});var aI=i(pn);H6=n(aI,"Depending on the type of scalability solution used, the connectivity speed could have a major or a minor impact. If the GPUs need to sync rarely, as in DDP, the impact of a slower connection will be less significant. If the GPUs need to send messages to each other often, as in ZeRO-DP, then faster connectivity becomes super important to achieve faster training."),aI.forEach(t),o1=f(e),Be=r(e,"H3",{class:!0});var ay=i(Be);ra=r(ay,"A",{id:!0,class:!0,href:!0});var sI=i(ra);$c=r(sI,"SPAN",{});var rI=i($c);d(qr.$$.fragment,rI),rI.forEach(t),sI.forEach(t),F6=f(ay),kc=r(ay,"SPAN",{});var iI=i(kc);W6=n(iI,"NVlink"),iI.forEach(t),ay.forEach(t),l1=f(e),Vr=r(e,"P",{});var ET=i(Vr);Hr=r(ET,"A",{href:!0,rel:!0});var oI=i(Hr);R6=n(oI,"NVLink"),oI.forEach(t),X6=n(ET," is a wire-based serial multi-lane near-range communications link developed by Nvidia."),ET.forEach(t),n1=f(e),ia=r(e,"P",{});var sy=i(ia);Y6=n(sy,"Each new generation provides a faster bandwidth, e.g. here is a quote from "),Fr=r(sy,"A",{href:!0,rel:!0});var lI=i(Fr);Q6=n(lI,"Nvidia Ampere GA102 GPU Architecture"),lI.forEach(t),J6=n(sy,":"),sy.forEach(t),p1=f(e),hn=r(e,"BLOCKQUOTE",{});var nI=i(hn);Ac=r(nI,"P",{});var pI=i(Ac);Z6=n(pI,`Third-Generation NVLink\xAE
GA102 GPUs utilize NVIDIA\u2019s third-generation NVLink interface, which includes four x4 links,
with each link providing 14.0625 GB/sec bandwidth in each direction between two GPUs. Four
links provide 56.25 GB/sec bandwidth in each direction, and 112.5 GB/sec total bandwidth
between two GPUs. Two RTX 3090 GPUs can be connected together for SLI using NVLink.
(Note that 3-Way and 4-Way SLI configurations are not supported.)`),pI.forEach(t),nI.forEach(t),h1=f(e),M=r(e,"P",{});var ws=i(M);K6=n(ws,"So the higher "),Tc=r(ws,"CODE",{});var hI=i(Tc);eP=n(hI,"X"),hI.forEach(t),tP=n(ws," you get in the report of "),jc=r(ws,"CODE",{});var fI=i(jc);aP=n(fI,"NVX"),fI.forEach(t),sP=n(ws," in the output of "),xc=r(ws,"CODE",{});var cI=i(xc);rP=n(cI,"nvidia-smi topo -m"),cI.forEach(t),iP=n(ws," the better. The generation will depend on your GPU architecture."),ws.forEach(t),f1=f(e),fn=r(e,"P",{});var mI=i(fn);oP=n(mI,"Let\u2019s compare the execution of a gpt2 language model training over a small sample of wikitext."),mI.forEach(t),c1=f(e),cn=r(e,"P",{});var dI=i(cn);lP=n(dI,"The results are:"),dI.forEach(t),m1=f(e),oa=r(e,"TABLE",{});var ry=i(oa);Dc=r(ry,"THEAD",{});var uI=i(Dc);Wr=r(uI,"TR",{});var iy=i(Wr);Ic=r(iy,"TH",{});var vI=i(Ic);nP=n(vI,"NVlink"),vI.forEach(t),pP=f(iy),mn=r(iy,"TH",{align:!0});var wI=i(mn);hP=n(wI,"Time"),wI.forEach(t),iy.forEach(t),uI.forEach(t),fP=f(ry),Rr=r(ry,"TBODY",{});var oy=i(Rr);Xr=r(oy,"TR",{});var ly=i(Xr);Gc=r(ly,"TD",{});var gI=i(Gc);cP=n(gI,"Y"),gI.forEach(t),mP=f(ly),dn=r(ly,"TD",{align:!0});var _I=i(dn);dP=n(_I,"101s"),_I.forEach(t),ly.forEach(t),uP=f(oy),Yr=r(oy,"TR",{});var ny=i(Yr);Uc=r(ny,"TD",{});var bI=i(Uc);vP=n(bI,"N"),bI.forEach(t),wP=f(ny),un=r(ny,"TD",{align:!0});var yI=i(un);gP=n(yI,"131s"),yI.forEach(t),ny.forEach(t),oy.forEach(t),ry.forEach(t),d1=f(e),vn=r(e,"P",{});var EI=i(vn);_P=n(EI,"You can see that NVLink completes the training ~23% faster."),EI.forEach(t),u1=f(e),la=r(e,"P",{});var py=i(la);bP=n(py,"In the second benchmark we use "),zc=r(py,"CODE",{});var PI=i(zc);yP=n(PI,"NCCL_P2P_DISABLE=1"),PI.forEach(t),EP=n(py," to tell the GPUs not to use NVLink."),py.forEach(t),v1=f(e),wn=r(e,"P",{});var $I=i(wn);PP=n($I,"Here is the full benchmark code and outputs:"),$I.forEach(t),w1=f(e),d(Qr.$$.fragment,e),g1=f(e),T=r(e,"P",{});var K=i(T);$P=n(K,"Hardware: 2x TITAN RTX 24GB each + NVlink with 2 NVLinks ("),Sc=r(K,"CODE",{});var kI=i(Sc);kP=n(kI,"NV2"),kI.forEach(t),AP=n(K," in "),Nc=r(K,"CODE",{});var AI=i(Nc);TP=n(AI,"nvidia-smi topo -m"),AI.forEach(t),jP=n(K,`)
Software: `),Cc=r(K,"CODE",{});var TI=i(Cc);xP=n(TI,"pytorch-1.8-to-be"),TI.forEach(t),DP=n(K," + "),Lc=r(K,"CODE",{});var jI=i(Lc);IP=n(jI,"cuda-11.0"),jI.forEach(t),GP=n(K," / "),Mc=r(K,"CODE",{});var xI=i(Mc);UP=n(xI,"transformers==4.3.0.dev0"),xI.forEach(t),K.forEach(t),_1=f(e),qe=r(e,"H2",{class:!0});var hy=i(qe);na=r(hy,"A",{id:!0,class:!0,href:!0});var DI=i(na);Oc=r(DI,"SPAN",{});var II=i(Oc);d(Jr.$$.fragment,II),II.forEach(t),DI.forEach(t),zP=f(hy),Bc=r(hy,"SPAN",{});var GI=i(Bc);SP=n(GI,"Software"),GI.forEach(t),hy.forEach(t),b1=f(e),Ve=r(e,"H3",{class:!0});var fy=i(Ve);pa=r(fy,"A",{id:!0,class:!0,href:!0});var UI=i(pa);qc=r(UI,"SPAN",{});var zI=i(qc);d(Zr.$$.fragment,zI),zI.forEach(t),UI.forEach(t),NP=f(fy),Vc=r(fy,"SPAN",{});var SI=i(Vc);CP=n(SI,"Model Scalability"),SI.forEach(t),fy.forEach(t),y1=f(e),gn=r(e,"P",{});var NI=i(gn);LP=n(NI,"When you can\u2019t fit a model into the available GPU memory, you need to start using a solution that allows you to scale a large model to use multiple GPUs in parallel."),NI.forEach(t),E1=f(e),Kr=r(e,"P",{});var PT=i(Kr);MP=n(PT,"For indepth details on ZeRO and various other model parallelism protocols please see: "),_n=r(PT,"A",{href:!0});var CI=i(_n);OP=n(CI,"Model Parallelism"),CI.forEach(t),PT.forEach(t),P1=f(e),He=r(e,"H3",{class:!0});var cy=i(He);ha=r(cy,"A",{id:!0,class:!0,href:!0});var LI=i(ha);Hc=r(LI,"SPAN",{});var MI=i(Hc);d(ei.$$.fragment,MI),MI.forEach(t),LI.forEach(t),BP=f(cy),Fc=r(cy,"SPAN",{});var OI=i(Fc);qP=n(OI,"Anatomy of Model's Operations"),OI.forEach(t),cy.forEach(t),$1=f(e),bn=r(e,"P",{});var BI=i(bn);VP=n(BI,"Transformers architecture includes 3 main groups of operations grouped below by compute-intensity."),BI.forEach(t),k1=f(e),ne=r(e,"OL",{});var Ih=i(ne);ti=r(Ih,"LI",{});var my=i(ti);Wc=r(my,"P",{});var qI=i(Wc);Rc=r(qI,"STRONG",{});var VI=i(Rc);HP=n(VI,"Tensor Contractions"),VI.forEach(t),qI.forEach(t),FP=f(my),ai=r(my,"P",{});var dy=i(ai);WP=n(dy,"Linear layers and components of Multi-Head Attention all do batched "),Xc=r(dy,"STRONG",{});var HI=i(Xc);RP=n(HI,"matrix-matrix multiplications"),HI.forEach(t),XP=n(dy,". These operations are the most compute-intensive part of training a transformer."),dy.forEach(t),my.forEach(t),YP=f(Ih),si=r(Ih,"LI",{});var uy=i(si);Yc=r(uy,"P",{});var FI=i(Yc);Qc=r(FI,"STRONG",{});var WI=i(Qc);QP=n(WI,"Statistical Normalizations"),WI.forEach(t),FI.forEach(t),JP=f(uy),ri=r(uy,"P",{});var vy=i(ri);ZP=n(vy,"Softmax and layer normalization are less compute-intensive than tensor contractions, and involve one or more "),Jc=r(vy,"STRONG",{});var RI=i(Jc);KP=n(RI,"reduction operations"),RI.forEach(t),e4=n(vy,", the result of which is then applied via a map."),vy.forEach(t),uy.forEach(t),t4=f(Ih),ii=r(Ih,"LI",{});var wy=i(ii);Zc=r(wy,"P",{});var XI=i(Zc);Kc=r(XI,"STRONG",{});var YI=i(Kc);a4=n(YI,"Element-wise Operators"),YI.forEach(t),XI.forEach(t),s4=f(wy),oi=r(wy,"P",{});var gy=i(oi);r4=n(gy,"These are the remaining operators: "),em=r(gy,"STRONG",{});var QI=i(em);i4=n(QI,"biases, dropout, activations, and residual connections"),QI.forEach(t),o4=n(gy,". These are the least compute-intensive operations."),gy.forEach(t),wy.forEach(t),Ih.forEach(t),A1=f(e),yn=r(e,"P",{});var JI=i(yn);l4=n(JI,"This knowledge can be helpful to know when analyzing performance bottlenecks."),JI.forEach(t),T1=f(e),li=r(e,"P",{});var $T=i(li);n4=n($T,"This summary is derived from "),ni=r($T,"A",{href:!0,rel:!0});var ZI=i(ni);p4=n(ZI,"Data Movement Is All You Need: A Case Study on Optimizing Transformers 2020"),ZI.forEach(t),$T.forEach(t),j1=f(e),Fe=r(e,"H3",{class:!0});var _y=i(Fe);fa=r(_y,"A",{id:!0,class:!0,href:!0});var KI=i(fa);tm=r(KI,"SPAN",{});var eG=i(tm);d(pi.$$.fragment,eG),eG.forEach(t),KI.forEach(t),h4=f(_y),am=r(_y,"SPAN",{});var tG=i(am);f4=n(tG,"Anatomy of Model's Memory"),tG.forEach(t),_y.forEach(t),x1=f(e),En=r(e,"P",{});var aG=i(En);c4=n(aG,"The components on GPU memory are the following:"),aG.forEach(t),D1=f(e),A=r(e,"OL",{});var W=i(A);sm=r(W,"LI",{});var sG=i(sm);m4=n(sG,"model weights"),sG.forEach(t),d4=f(W),rm=r(W,"LI",{});var rG=i(rm);u4=n(rG,"optimizer states"),rG.forEach(t),v4=f(W),im=r(W,"LI",{});var iG=i(im);w4=n(iG,"gradients"),iG.forEach(t),g4=f(W),om=r(W,"LI",{});var oG=i(om);_4=n(oG,"forward activations saved for gradient computation"),oG.forEach(t),b4=f(W),lm=r(W,"LI",{});var lG=i(lm);y4=n(lG,"temporary buffers"),lG.forEach(t),E4=f(W),nm=r(W,"LI",{});var nG=i(nm);P4=n(nG,"functionality-specific memory"),nG.forEach(t),W.forEach(t),I1=f(e),Pn=r(e,"P",{});var pG=i(Pn);$4=n(pG,"A typical model trained in mixed precision with AdamW requires 18 bytes per model parameter plus activation memory."),pG.forEach(t),G1=f(e),$n=r(e,"P",{});var hG=i($n);k4=n(hG,"For inference there are no optimizer states and gradients, so we can subtract those. And thus we end up with 6 bytes per model parameter for mixed precision inference, plus activation memory."),hG.forEach(t),U1=f(e),kn=r(e,"P",{});var fG=i(kn);A4=n(fG,"Let\u2019s look at the details."),fG.forEach(t),z1=f(e),We=r(e,"H4",{class:!0});var by=i(We);ca=r(by,"A",{id:!0,class:!0,href:!0});var cG=i(ca);pm=r(cG,"SPAN",{});var mG=i(pm);d(hi.$$.fragment,mG),mG.forEach(t),cG.forEach(t),T4=f(by),hm=r(by,"SPAN",{});var dG=i(hm);j4=n(dG,"Model Weights"),dG.forEach(t),by.forEach(t),S1=f(e),ma=r(e,"UL",{});var yy=i(ma);fm=r(yy,"LI",{});var uG=i(fm);x4=n(uG,"4 bytes * number of parameters for fp32 training"),uG.forEach(t),D4=f(yy),cm=r(yy,"LI",{});var vG=i(cm);I4=n(vG,"6 bytes * number of parameters for mixed precision training"),vG.forEach(t),yy.forEach(t),N1=f(e),Re=r(e,"H4",{class:!0});var Ey=i(Re);da=r(Ey,"A",{id:!0,class:!0,href:!0});var wG=i(da);mm=r(wG,"SPAN",{});var gG=i(mm);d(fi.$$.fragment,gG),gG.forEach(t),wG.forEach(t),G4=f(Ey),dm=r(Ey,"SPAN",{});var _G=i(dm);U4=n(_G,"Optimizer States"),_G.forEach(t),Ey.forEach(t),C1=f(e),pe=r(e,"UL",{});var Gh=i(pe);um=r(Gh,"LI",{});var bG=i(um);z4=n(bG,"8 bytes * number of parameters for normal AdamW (maintains 2 states)"),bG.forEach(t),S4=f(Gh),An=r(Gh,"LI",{});var kT=i(An);N4=n(kT,"2 bytes * number of parameters for 8-bit AdamW optimizers like "),ci=r(kT,"A",{href:!0,rel:!0});var yG=i(ci);C4=n(yG,"bitsandbytes"),yG.forEach(t),kT.forEach(t),L4=f(Gh),vm=r(Gh,"LI",{});var EG=i(vm);M4=n(EG,"4 bytes * number of parameters for optimizers like SGD (maintains only 1 state)"),EG.forEach(t),Gh.forEach(t),L1=f(e),Xe=r(e,"H4",{class:!0});var Py=i(Xe);ua=r(Py,"A",{id:!0,class:!0,href:!0});var PG=i(ua);wm=r(PG,"SPAN",{});var $G=i(wm);d(mi.$$.fragment,$G),$G.forEach(t),PG.forEach(t),O4=f(Py),gm=r(Py,"SPAN",{});var kG=i(gm);B4=n(kG,"Gradients"),kG.forEach(t),Py.forEach(t),M1=f(e),Tn=r(e,"UL",{});var AG=i(Tn);_m=r(AG,"LI",{});var TG=i(_m);q4=n(TG,"4 bytes * number of parameters for either fp32 or mixed precision training"),TG.forEach(t),AG.forEach(t),O1=f(e),Ye=r(e,"H4",{class:!0});var $y=i(Ye);va=r($y,"A",{id:!0,class:!0,href:!0});var jG=i(va);bm=r(jG,"SPAN",{});var xG=i(bm);d(di.$$.fragment,xG),xG.forEach(t),jG.forEach(t),V4=f($y),ym=r($y,"SPAN",{});var DG=i(ym);H4=n(DG,"Forward Activations"),DG.forEach(t),$y.forEach(t),B1=f(e),jn=r(e,"UL",{});var IG=i(jn);Em=r(IG,"LI",{});var GG=i(Em);F4=n(GG,"size depends on many factors, the key ones being sequence length, hidden size and batch size."),GG.forEach(t),IG.forEach(t),q1=f(e),xn=r(e,"P",{});var UG=i(xn);W4=n(UG,"There are the input and output that are being passed and returned by the forward and the backward functions and the forward activations saved for gradient computation."),UG.forEach(t),V1=f(e),Qe=r(e,"H4",{class:!0});var ky=i(Qe);wa=r(ky,"A",{id:!0,class:!0,href:!0});var zG=i(wa);Pm=r(zG,"SPAN",{});var SG=i(Pm);d(ui.$$.fragment,SG),SG.forEach(t),zG.forEach(t),R4=f(ky),$m=r(ky,"SPAN",{});var NG=i($m);X4=n(NG,"Temporary Memory"),NG.forEach(t),ky.forEach(t),H1=f(e),Dn=r(e,"P",{});var CG=i(Dn);Y4=n(CG,"Additionally there are all kinds of temporary variables which get released once the calculation is done, but in the moment these could require additional memory and could push to OOM. Therefore when coding it\u2019s crucial to think strategically about such temporary variables and sometimes to explicitly free those as soon as they are no longer needed."),CG.forEach(t),F1=f(e),Je=r(e,"H4",{class:!0});var Ay=i(Je);ga=r(Ay,"A",{id:!0,class:!0,href:!0});var LG=i(ga);km=r(LG,"SPAN",{});var MG=i(km);d(vi.$$.fragment,MG),MG.forEach(t),LG.forEach(t),Q4=f(Ay),Am=r(Ay,"SPAN",{});var OG=i(Am);J4=n(OG,"Functionality-specific memory"),OG.forEach(t),Ay.forEach(t),W1=f(e),In=r(e,"P",{});var BG=i(In);Z4=n(BG,"Then your software could have special memory needs. For example, when generating text using beam search, the software needs to maintain multiple copies of inputs and outputs."),BG.forEach(t),R1=f(e),Ze=r(e,"H3",{class:!0});var Ty=i(Ze);_a=r(Ty,"A",{id:!0,class:!0,href:!0});var qG=i(_a);Tm=r(qG,"SPAN",{});var VG=i(Tm);d(wi.$$.fragment,VG),VG.forEach(t),qG.forEach(t),K4=f(Ty),ba=r(Ty,"SPAN",{});var ju=i(ba);jm=r(ju,"CODE",{});var HG=i(jm);e5=n(HG,"forward"),HG.forEach(t),t5=n(ju," vs "),xm=r(ju,"CODE",{});var FG=i(xm);a5=n(FG,"backward"),FG.forEach(t),s5=n(ju," Execution Speed"),ju.forEach(t),Ty.forEach(t),X1=f(e),Gn=r(e,"P",{});var WG=i(Gn);r5=n(WG,"For convolutions and linear layers there are 2x flops in the backward compared to the forward, which generally translates into ~2x slower (sometimes more, because sizes in the backward tend to be more awkward). Activations are usually bandwidth-limited, and it\u2019s typical for an activation to have to read more data in the backward than in the forward (e.g. activation forward reads once, writes once, activation backward reads twice, gradOutput and output of the forward, and writes once, gradInput)."),WG.forEach(t),Y1=f(e),Ke=r(e,"H3",{class:!0});var jy=i(Ke);ya=r(jy,"A",{id:!0,class:!0,href:!0});var RG=i(ya);Dm=r(RG,"SPAN",{});var XG=i(Dm);d(gi.$$.fragment,XG),XG.forEach(t),RG.forEach(t),i5=f(jy),Im=r(jy,"SPAN",{});var YG=i(Im);o5=n(YG,"Floating Data Types"),YG.forEach(t),jy.forEach(t),Q1=f(e),Un=r(e,"P",{});var QG=i(Un);l5=n(QG,"Here are the commonly used floating point data types choice of which impacts both memory usage and throughput:"),QG.forEach(t),J1=f(e),O=r(e,"UL",{});var gs=i(O);_i=r(gs,"LI",{});var xy=i(_i);n5=n(xy,"fp32 ("),Gm=r(xy,"CODE",{});var JG=i(Gm);p5=n(JG,"float32"),JG.forEach(t),h5=n(xy,")"),xy.forEach(t),f5=f(gs),bi=r(gs,"LI",{});var Dy=i(bi);c5=n(Dy,"fp16 ("),Um=r(Dy,"CODE",{});var ZG=i(Um);m5=n(ZG,"float16"),ZG.forEach(t),d5=n(Dy,")"),Dy.forEach(t),u5=f(gs),yi=r(gs,"LI",{});var Iy=i(yi);v5=n(Iy,"bf16 ("),zm=r(Iy,"CODE",{});var KG=i(zm);w5=n(KG,"bfloat16"),KG.forEach(t),g5=n(Iy,")"),Iy.forEach(t),_5=f(gs),Sm=r(gs,"LI",{});var eU=i(Sm);b5=n(eU,"tf32 (CUDA internal data type)"),eU.forEach(t),gs.forEach(t),Z1=f(e),zn=r(e,"P",{});var tU=i(zn);y5=n(tU,"Here is a diagram that shows how these data types correlate to each other."),tU.forEach(t),K1=f(e),Sn=r(e,"P",{});var aU=i(Sn);Nn=r(aU,"IMG",{src:!0,alt:!0}),aU.forEach(t),eg=f(e),Ea=r(e,"P",{});var Gy=i(Ea);E5=n(Gy,"(source: "),Ei=r(Gy,"A",{href:!0,rel:!0});var sU=i(Ei);P5=n(sU,"NVIDIA Blog"),sU.forEach(t),$5=n(Gy,")"),Gy.forEach(t),tg=f(e),Cn=r(e,"P",{});var rU=i(Cn);k5=n(rU,"While fp16 and fp32 have been around for quite some time, bf16 and tf32 are only available on the Ampere architecture GPUS. TPUs support bf16 as well."),rU.forEach(t),ag=f(e),et=r(e,"H4",{class:!0});var Uy=i(et);Pa=r(Uy,"A",{id:!0,class:!0,href:!0});var iU=i(Pa);Nm=r(iU,"SPAN",{});var oU=i(Nm);d(Pi.$$.fragment,oU),oU.forEach(t),iU.forEach(t),A5=f(Uy),Cm=r(Uy,"SPAN",{});var lU=i(Cm);T5=n(lU,"fp16"),lU.forEach(t),Uy.forEach(t),sg=f(e),Ln=r(e,"P",{});var nU=i(Ln);j5=n(nU,"AMP = Automatic Mixed Precision"),nU.forEach(t),rg=f(e),Mn=r(e,"P",{});var pU=i(Mn);x5=n(pU,"If we look at what\u2019s happening with FP16 training (mixed precision) we have:"),pU.forEach(t),ig=f(e),B=r(e,"UL",{});var _s=i(B);Lm=r(_s,"LI",{});var hU=i(Lm);D5=n(hU,"the model has two copies in memory: one in half-precision for the forward/backward computations and one in full precision - no memory saved here"),hU.forEach(t),I5=f(_s),Mm=r(_s,"LI",{});var fU=i(Mm);G5=n(fU,"the forward activations saved for gradient computation are in half-precision - memory is saved here"),fU.forEach(t),U5=f(_s),$i=r(_s,"LI",{});var zy=i($i);z5=n(zy,"the gradients are computed in half-precision "),Om=r(zy,"EM",{});var cU=i(Om);S5=n(cU,"but"),cU.forEach(t),N5=n(zy," converted to full-precision for the update, no saving there"),zy.forEach(t),C5=f(_s),Bm=r(_s,"LI",{});var mU=i(Bm);L5=n(mU,"the optimizer states are in full precision as all the updates are done in full-precision"),mU.forEach(t),_s.forEach(t),og=f(e),On=r(e,"P",{});var dU=i(On);M5=n(dU,"So the savings only happen for the forward activations saved for the backward computation, and there is a slight overhead because the model weights are stored both in half- and full-precision."),dU.forEach(t),lg=f(e),$a=r(e,"P",{});var Sy=i($a);O5=n(Sy,"In \u{1F917} Transformers fp16 mixed precision is enabled by passing "),qm=r(Sy,"CODE",{});var uU=i(qm);B5=n(uU,"--fp16"),uU.forEach(t),q5=n(Sy," to the \u{1F917} Trainer."),Sy.forEach(t),ng=f(e),Bn=r(e,"P",{});var vU=i(Bn);V5=n(vU,"Now let\u2019s look at a simple text-classification fine-tuning on 2 GPUs (I\u2019m giving the command for reference):"),vU.forEach(t),pg=f(e),d(ki.$$.fragment,e),hg=f(e),ka=r(e,"P",{});var Ny=i(ka);H5=n(Ny,"Since the only savings we get are in the model activations saved for the backward passed, it\u2019s logical that the bigger those activations are, the bigger the saving will be. If we try different batch sizes, I indeed get (this is with "),Vm=r(Ny,"CODE",{});var wU=i(Vm);F5=n(wU,"nvidia-smi"),wU.forEach(t),W5=n(Ny," so not completely reliable as said above but it will be a fair comparison):"),Ny.forEach(t),fg=f(e),Aa=r(e,"TABLE",{});var Cy=i(Aa);Hm=r(Cy,"THEAD",{});var gU=i(Hm);R=r(gU,"TR",{});var bs=i(R);qn=r(bs,"TH",{align:!0});var _U=i(qn);R5=n(_U,"batch size"),_U.forEach(t),X5=f(bs),Vn=r(bs,"TH",{align:!0});var bU=i(Vn);Y5=n(bU,"w/o \u2014fp16"),bU.forEach(t),Q5=f(bs),Hn=r(bs,"TH",{align:!0});var yU=i(Hn);J5=n(yU,"w/ \u2014fp16"),yU.forEach(t),Z5=f(bs),Fn=r(bs,"TH",{align:!0});var EU=i(Fn);K5=n(EU,"savings"),EU.forEach(t),bs.forEach(t),gU.forEach(t),e$=f(Cy),X=r(Cy,"TBODY",{});var ys=i(X);Y=r(ys,"TR",{});var Es=i(Y);Wn=r(Es,"TD",{align:!0});var PU=i(Wn);t$=n(PU,"8"),PU.forEach(t),a$=f(Es),Rn=r(Es,"TD",{align:!0});var $U=i(Rn);s$=n($U,"4247"),$U.forEach(t),r$=f(Es),Xn=r(Es,"TD",{align:!0});var kU=i(Xn);i$=n(kU,"4163"),kU.forEach(t),o$=f(Es),Yn=r(Es,"TD",{align:!0});var AU=i(Yn);l$=n(AU,"84"),AU.forEach(t),Es.forEach(t),n$=f(ys),Q=r(ys,"TR",{});var Ps=i(Q);Qn=r(Ps,"TD",{align:!0});var TU=i(Qn);p$=n(TU,"16"),TU.forEach(t),h$=f(Ps),Jn=r(Ps,"TD",{align:!0});var jU=i(Jn);f$=n(jU,"4971"),jU.forEach(t),c$=f(Ps),Zn=r(Ps,"TD",{align:!0});var xU=i(Zn);m$=n(xU,"4793"),xU.forEach(t),d$=f(Ps),Kn=r(Ps,"TD",{align:!0});var DU=i(Kn);u$=n(DU,"178"),DU.forEach(t),Ps.forEach(t),v$=f(ys),J=r(ys,"TR",{});var $s=i(J);ep=r($s,"TD",{align:!0});var IU=i(ep);w$=n(IU,"32"),IU.forEach(t),g$=f($s),tp=r($s,"TD",{align:!0});var GU=i(tp);_$=n(GU,"6827"),GU.forEach(t),b$=f($s),ap=r($s,"TD",{align:!0});var UU=i(ap);y$=n(UU,"6207"),UU.forEach(t),E$=f($s),sp=r($s,"TD",{align:!0});var zU=i(sp);P$=n(zU,"620"),zU.forEach(t),$s.forEach(t),$$=f(ys),Z=r(ys,"TR",{});var ks=i(Z);rp=r(ks,"TD",{align:!0});var SU=i(rp);k$=n(SU,"64"),SU.forEach(t),A$=f(ks),ip=r(ks,"TD",{align:!0});var NU=i(ip);T$=n(NU,"10037"),NU.forEach(t),j$=f(ks),op=r(ks,"TD",{align:!0});var CU=i(op);x$=n(CU,"8061"),CU.forEach(t),D$=f(ks),lp=r(ks,"TD",{align:!0});var LU=i(lp);I$=n(LU,"1976"),LU.forEach(t),ks.forEach(t),ys.forEach(t),Cy.forEach(t),cg=f(e),Ta=r(e,"P",{});var Ly=i(Ta);G$=n(Ly,"So there is only a real memory saving if we train at a high batch size (and it\u2019s not half) and at batch sizes lower than 8, you actually get a bigger memory footprint (because of the overhead mentioned above). The gain for FP16 training is that in each of those cases, the training with the flag "),Fm=r(Ly,"CODE",{});var MU=i(Fm);U$=n(MU,"--fp16"),MU.forEach(t),z$=n(Ly," is twice as fast, which does require every tensor to have every dimension be a multiple of 8 (examples pad the tensors to a sequence length that is a multiple of 8)."),Ly.forEach(t),mg=f(e),np=r(e,"P",{});var OU=i(np);S$=n(OU,"Summary: FP16 with apex or AMP will only give you some memory savings with a reasonably high batch size."),OU.forEach(t),dg=f(e),pp=r(e,"P",{});var BU=i(pp);N$=n(BU,"Additionally, under mixed precision when possible, it\u2019s important that the batch size is a multiple of 8 to efficiently use tensor cores."),BU.forEach(t),ug=f(e),ja=r(e,"P",{});var My=i(ja);C$=n(My,"Note that in some situations the speed up can be as big as 5x when using mixed precision. e.g. we have observed that while using "),Ai=r(My,"A",{href:!0,rel:!0});var qU=i(Ai);L$=n(qU,"Megatron-Deepspeed"),qU.forEach(t),M$=n(My,"."),My.forEach(t),vg=f(e),hp=r(e,"P",{});var VU=i(hp);O$=n(VU,"Some amazing tutorials to read on mixed precision:"),VU.forEach(t),wg=f(e),xa=r(e,"UL",{});var Oy=i(xa);fp=r(Oy,"LI",{});var AT=i(fp);B$=n(AT,"@sgugger wrote a great explanation of mixed precision "),Ti=r(AT,"A",{href:!0,rel:!0});var HU=i(Ti);q$=n(HU,"here"),HU.forEach(t),AT.forEach(t),V$=f(Oy),cp=r(Oy,"LI",{});var TT=i(cp);H$=n(TT,"Aleksey Bilogur\u2019s "),ji=r(TT,"A",{href:!0,rel:!0});var FU=i(ji);F$=n(FU,"A developer-friendly guide to mixed precision training with PyTorch"),FU.forEach(t),TT.forEach(t),Oy.forEach(t),gg=f(e),he=r(e,"P",{});var Uh=i(he);W$=n(Uh,`You can also see a variety of benchmarks on fp16 vs other precisions:
`),xi=r(Uh,"A",{href:!0,rel:!0});var WU=i(xi);R$=n(WU,"RTX-3090"),WU.forEach(t),X$=n(Uh,` and
`),Di=r(Uh,"A",{href:!0,rel:!0});var RU=i(Di);Y$=n(RU,"A100"),RU.forEach(t),Q$=n(Uh,"."),Uh.forEach(t),_g=f(e),tt=r(e,"H5",{class:!0});var By=i(tt);Da=r(By,"A",{id:!0,class:!0,href:!0});var XU=i(Da);Wm=r(XU,"SPAN",{});var YU=i(Wm);d(Ii.$$.fragment,YU),YU.forEach(t),XU.forEach(t),J$=f(By),Rm=r(By,"SPAN",{});var QU=i(Rm);Z$=n(QU,"fp16 caching"),QU.forEach(t),By.forEach(t),bg=f(e),fe=r(e,"P",{});var zh=i(fe);K$=n(zh,"pytorch "),Xm=r(zh,"CODE",{});var JU=i(Xm);ek=n(JU,"autocast"),JU.forEach(t),tk=n(zh," which performs AMP include a caching feature, which speed things up by caching fp16-converted values. Here is the full description from this "),Gi=r(zh,"A",{href:!0,rel:!0});var ZU=i(Gi);ak=n(ZU,"comment"),ZU.forEach(t),sk=n(zh,":"),zh.forEach(t),yg=f(e),mp=r(e,"P",{});var KU=i(mp);rk=n(KU,"Autocast maintains a cache of the FP16 casts of model parameters (leaves). This helps streamline parameter reuse: if the same FP32 param is used in several different FP16list ops, like several matmuls, instead of re-casting the param to FP16 on entering each matmul, the cast will occur on the first matmul, the casted FP16 copy will be cached, and for all later matmuls the FP16 copy will be reused. The cache is maintained only within a particular outermost autocast context. When you exit the autocast context the cache is dropped. For recommended usage, in which autocast wraps the forward pass, and then you exit the context before calling backward(), this means the cache only lasts the duration of the forward pass each iteration, and will be rebuilt next iteration. (The cache of FP16-casted copies MUST be rebuilt each iteration. The FP32 parameters get updated by the optimizer, so the FP16 copies must be recreated, otherwise the FP16 values will be stale.)"),KU.forEach(t),Eg=f(e),at=r(e,"H5",{class:!0});var qy=i(at);Ia=r(qy,"A",{id:!0,class:!0,href:!0});var ez=i(Ia);Ym=r(ez,"SPAN",{});var tz=i(Ym);d(Ui.$$.fragment,tz),tz.forEach(t),ez.forEach(t),ik=f(qy),Qm=r(qy,"SPAN",{});var az=i(Qm);ok=n(az,"fp16 Inference"),az.forEach(t),qy.forEach(t),Pg=f(e),dp=r(e,"P",{});var sz=i(dp);lk=n(sz,"While normally inference is done with fp16/amp as with training, it\u2019s also possible to use the full fp16 mode without using mixed precision. This is especially a good fit if the pretrained model weights are already in fp16. So a lot less memory is used: 2 bytes per parameter vs 6 bytes with mixed precision!"),sz.forEach(t),$g=f(e),up=r(e,"P",{});var rz=i(up);nk=n(rz,"How good the results this will deliver will depend on the model. If it can handle fp16 without overflows and accuracy issues, then it\u2019ll definitely better to use the full fp16 mode."),rz.forEach(t),kg=f(e),vp=r(e,"P",{});var iz=i(vp);pk=n(iz,"For example, LayerNorm has to be done in fp32 and recent pytorch (1.10+) has been fixed to do that regardless of the input types, but earlier pytorch versions accumulate in the input type which can be an issue."),iz.forEach(t),Ag=f(e),Ga=r(e,"P",{});var Vy=i(Ga);hk=n(Vy,"In \u{1F917} Transformers the full fp16 inference is enabled by passing "),Jm=r(Vy,"CODE",{});var oz=i(Jm);fk=n(oz,"--fp16_full_eval"),oz.forEach(t),ck=n(Vy," to the \u{1F917} Trainer."),Vy.forEach(t),Tg=f(e),st=r(e,"H4",{class:!0});var Hy=i(st);Ua=r(Hy,"A",{id:!0,class:!0,href:!0});var lz=i(Ua);Zm=r(lz,"SPAN",{});var nz=i(Zm);d(zi.$$.fragment,nz),nz.forEach(t),lz.forEach(t),mk=f(Hy),Km=r(Hy,"SPAN",{});var pz=i(Km);dk=n(pz,"bf16"),pz.forEach(t),Hy.forEach(t),jg=f(e),ce=r(e,"P",{});var Sh=i(ce);uk=n(Sh,"If you own Ampere or newer hardware you can start using bf16 for your training and evaluation. While bf16 has a worse precision than fp16, it has a much much bigger dynamic range. Therefore, if in the past you were experiencing overflow issues while training the model, bf16 will prevent this from happening most of the time. Remember that in fp16 the biggest number you can have is "),ed=r(Sh,"CODE",{});var hz=i(ed);vk=n(hz,"65535"),hz.forEach(t),wk=n(Sh," and any number above that will overflow. A bf16 number can be as large as "),td=r(Sh,"CODE",{});var fz=i(td);gk=n(fz,"3.39e+38"),fz.forEach(t),_k=n(Sh," (!) which is about the same as fp32 - because both have 8-bits used for the numerical range."),Sh.forEach(t),xg=f(e),wp=r(e,"P",{});var cz=i(wp);bk=n(cz,"Automatic Mixed Precision (AMP) is the same as with fp16, except it\u2019ll use bf16."),cz.forEach(t),Dg=f(e),gp=r(e,"P",{});var mz=i(gp);yk=n(mz,"Thanks to the fp32-like dynamic range with bf16 mixed precision loss scaling is no longer needed."),mz.forEach(t),Ig=f(e),_p=r(e,"P",{});var dz=i(_p);Ek=n(dz,"If you have tried to finetune models pre-trained under bf16 mixed precision (e.g. T5) it\u2019s very likely that you have encountered overflow issues. Now you should be able to finetune those models without any issues."),dz.forEach(t),Gg=f(e),bp=r(e,"P",{});var uz=i(bp);Pk=n(uz,"That said, also be aware that if you pre-trained a model in bf16, it\u2019s likely to have overflow issues if someone tries to finetune it in fp16 down the road. So once started on the bf16-mode path it\u2019s best to remain on it and not switch to fp16."),uz.forEach(t),Ug=f(e),za=r(e,"P",{});var Fy=i(za);$k=n(Fy,"In \u{1F917} Transformers bf16 mixed precision is enabled by passing "),ad=r(Fy,"CODE",{});var vz=i(ad);kk=n(vz,"--bf16"),vz.forEach(t),Ak=n(Fy," to the \u{1F917} Trainer."),Fy.forEach(t),zg=f(e),yp=r(e,"P",{});var wz=i(yp);Tk=n(wz,"If you use your own trainer, this is just:"),wz.forEach(t),Sg=f(e),d(Si.$$.fragment,e),Ng=f(e),Ni=r(e,"P",{});var jT=i(Ni);jk=n(jT,"If you need to switch a tensor to bf16, it\u2019s just: "),sd=r(jT,"CODE",{});var gz=i(sd);xk=n(gz,"t.to(dtype=torch.bfloat16)"),gz.forEach(t),jT.forEach(t),Cg=f(e),Ep=r(e,"P",{});var _z=i(Ep);Dk=n(_z,"Here is how you can check if your setup supports bf16:"),_z.forEach(t),Lg=f(e),d(Ci.$$.fragment,e),Mg=f(e),Pp=r(e,"P",{});var bz=i(Pp);Ik=n(bz,"On the other hand bf16 has a much worse precision than fp16, so there are certain situations where you\u2019d still want to use fp16 and not bf16."),bz.forEach(t),Og=f(e),me=r(e,"P",{});var Nh=i(me);Gk=n(Nh,`You can also see a variety of benchmarks on bf16 vs other precisions:
`),Li=r(Nh,"A",{href:!0,rel:!0});var yz=i(Li);Uk=n(yz,"RTX-3090"),yz.forEach(t),zk=n(Nh,` and
`),Mi=r(Nh,"A",{href:!0,rel:!0});var Ez=i(Mi);Sk=n(Ez,"A100"),Ez.forEach(t),Nk=n(Nh,"."),Nh.forEach(t),Bg=f(e),rt=r(e,"H5",{class:!0});var Wy=i(rt);Sa=r(Wy,"A",{id:!0,class:!0,href:!0});var Pz=i(Sa);rd=r(Pz,"SPAN",{});var $z=i(rd);d(Oi.$$.fragment,$z),$z.forEach(t),Pz.forEach(t),Ck=f(Wy),id=r(Wy,"SPAN",{});var kz=i(id);Lk=n(kz,"bf16 Inference"),kz.forEach(t),Wy.forEach(t),qg=f(e),Na=r(e,"P",{});var Ry=i(Na);Mk=n(Ry,"Same as with fp16, you can do inference in either the mixed precision bf16 or using the full bf16 mode. The same caveats apply. For details see "),$p=r(Ry,"A",{href:!0});var Az=i($p);Ok=n(Az,"fp16 Inference"),Az.forEach(t),Bk=n(Ry,"."),Ry.forEach(t),Vg=f(e),Ca=r(e,"P",{});var Xy=i(Ca);qk=n(Xy,"In \u{1F917} Transformers the full bf16 inference is enabled by passing "),od=r(Xy,"CODE",{});var Tz=i(od);Vk=n(Tz,"--bf16_full_eval"),Tz.forEach(t),Hk=n(Xy," to the \u{1F917} Trainer."),Xy.forEach(t),Hg=f(e),it=r(e,"H4",{class:!0});var Yy=i(it);La=r(Yy,"A",{id:!0,class:!0,href:!0});var jz=i(La);ld=r(jz,"SPAN",{});var xz=i(ld);d(Bi.$$.fragment,xz),xz.forEach(t),jz.forEach(t),Fk=f(Yy),nd=r(Yy,"SPAN",{});var Dz=i(nd);Wk=n(Dz,"tf32"),Dz.forEach(t),Yy.forEach(t),Fg=f(e),kp=r(e,"P",{});var Iz=i(kp);Rk=n(Iz,"The Ampere hardware uses a magical data type called tf32. It has the same numerical range as fp32 (8-bits), but instead of 23 bits precision it has only 10 bits (same as fp16). In total it uses only 19 bits."),Iz.forEach(t),Wg=f(e),Ap=r(e,"P",{});var Gz=i(Ap);Xk=n(Gz,"It\u2019s magical in the sense that you can use the normal fp32 training and/or inference code and by enabling tf32 support you can get up to 3x throughput improvement. All you need to do is to add this to your code:"),Gz.forEach(t),Rg=f(e),d(qi.$$.fragment,e),Xg=f(e),Tp=r(e,"P",{});var Uz=i(Tp);Yk=n(Uz,"When this is done CUDA will automatically switch to using tf32 instead of fp32 where it\u2019s possible. This, of course, assumes that the used GPU is from the Ampere series."),Uz.forEach(t),Yg=f(e),Ma=r(e,"P",{});var Qy=i(Ma);Qk=n(Qy,"Like all cases with reduced precision this may or may not be satisfactory for your needs, so you have to experiment and see. According to "),Vi=r(Qy,"A",{href:!0,rel:!0});var zz=i(Vi);Jk=n(zz,"NVIDIA research"),zz.forEach(t),Zk=n(Qy," the majority of machine learning training shouldn\u2019t be impacted and showed the same perplexity and convergence as the fp32 training."),Qy.forEach(t),Qg=f(e),jp=r(e,"P",{});var Sz=i(jp);Kk=n(Sz,"If you\u2019re already using fp16 or bf16 mixed precision it may help with the throughput as well."),Sz.forEach(t),Jg=f(e),q=r(e,"P",{});var As=i(q);e7=n(As,"You can enable this mode in the \u{1F917} Trainer with "),pd=r(As,"CODE",{});var Nz=i(pd);t7=n(Nz,"--tf32"),Nz.forEach(t),a7=n(As,", or disable it with "),hd=r(As,"CODE",{});var Cz=i(hd);s7=n(Cz,"--tf32 0"),Cz.forEach(t),r7=n(As," or "),fd=r(As,"CODE",{});var Lz=i(fd);i7=n(Lz,"--no_tf32"),Lz.forEach(t),o7=n(As,`.
By default the PyTorch default is used.`),As.forEach(t),Zg=f(e),de=r(e,"P",{});var Ch=i(de);l7=n(Ch,"Note: tf32 mode is internal to CUDA and can\u2019t be accessed directly via "),cd=r(Ch,"CODE",{});var Mz=i(cd);n7=n(Mz,"tensor.to(dtype=torch.tf32)"),Mz.forEach(t),p7=n(Ch," as "),md=r(Ch,"CODE",{});var Oz=i(md);h7=n(Oz,"torch.tf32"),Oz.forEach(t),f7=n(Ch," doesn\u2019t exit."),Ch.forEach(t),Kg=f(e),Oa=r(e,"P",{});var Jy=i(Oa);c7=n(Jy,"Note: you need "),dd=r(Jy,"CODE",{});var Bz=i(dd);m7=n(Bz,"torch>=1.7"),Bz.forEach(t),d7=n(Jy," to enjoy this feature."),Jy.forEach(t),e_=f(e),ue=r(e,"P",{});var Lh=i(ue);u7=n(Lh,`You can also see a variety of benchmarks on tf32 vs other precisions:
`),Hi=r(Lh,"A",{href:!0,rel:!0});var qz=i(Hi);v7=n(qz,"RTX-3090"),qz.forEach(t),w7=n(Lh,` and
`),Fi=r(Lh,"A",{href:!0,rel:!0});var Vz=i(Fi);g7=n(Vz,"A100"),Vz.forEach(t),_7=n(Lh,"."),Lh.forEach(t),t_=f(e),ot=r(e,"H3",{class:!0});var Zy=i(ot);Ba=r(Zy,"A",{id:!0,class:!0,href:!0});var Hz=i(Ba);ud=r(Hz,"SPAN",{});var Fz=i(ud);d(Wi.$$.fragment,Fz),Fz.forEach(t),Hz.forEach(t),b7=f(Zy),vd=r(Zy,"SPAN",{});var Wz=i(vd);y7=n(Wz,"Gradient Accumulation"),Wz.forEach(t),Zy.forEach(t),a_=f(e),ve=r(e,"P",{});var Mh=i(ve);E7=n(Mh,"Since gradient accumulation essentially is identical to having a larger batch size, just as with the larger batch size here you are likely to see a 20-30% speedup due to the optimizer running less often. For example, see benchmarks for "),Ri=r(Mh,"A",{href:!0,rel:!0});var Rz=i(Ri);P7=n(Rz,"RTX-3090"),Rz.forEach(t),$7=n(Mh,`
and `),Xi=r(Mh,"A",{href:!0,rel:!0});var Xz=i(Xi);k7=n(Xz,"A100"),Xz.forEach(t),A7=n(Mh,"."),Mh.forEach(t),s_=f(e),qa=r(e,"P",{});var Ky=i(qa);T7=n(Ky,"To activate this feature in \u{1F917} Trainer add "),wd=r(Ky,"CODE",{});var Yz=i(wd);j7=n(Yz,"--gradient_accumulation_steps 4"),Yz.forEach(t),x7=n(Ky," to its arguments (experiment with the value to get the best performance)."),Ky.forEach(t),r_=f(e),xp=r(e,"P",{});var Qz=i(xp);D7=n(Qz,"It\u2019s important to remember that using gradient accumulation you may end up with a much larger effective batch size, so you may need to adjust the learning rate, its warm up and for very short datasets it\u2019ll impact the loss as the training will end up doing less steps than normal."),Qz.forEach(t),i_=f(e),lt=r(e,"H3",{class:!0});var e2=i(lt);Va=r(e2,"A",{id:!0,class:!0,href:!0});var Jz=i(Va);gd=r(Jz,"SPAN",{});var Zz=i(gd);d(Yi.$$.fragment,Zz),Zz.forEach(t),Jz.forEach(t),I7=f(e2),_d=r(e2,"SPAN",{});var Kz=i(_d);G7=n(Kz,"Gradient Checkpointing"),Kz.forEach(t),e2.forEach(t),o_=f(e),Dp=r(e,"P",{});var eS=i(Dp);U7=n(eS,"One way to use significantly less GPU memory is to enabled \u201CGradient Checkpointing\u201D (also known as \u201Cactivation checkpointing\u201D). When enabled, a lot of memory can be freed at the cost of small decrease in the training speed due to recomputing parts of the graph during back-propagation. The slowdown will depend on the model but quite often it is around 20-30%."),eS.forEach(t),l_=f(e),V=r(e,"P",{});var Ts=i(V);z7=n(Ts,"This technique was first shared in the paper: "),Qi=r(Ts,"A",{href:!0,rel:!0});var tS=i(Qi);S7=n(tS,"Training Deep Nets with Sublinear Memory Cost"),tS.forEach(t),N7=n(Ts,". The paper will also give you the exact details on the savings, but it\u2019s in the ballpark of "),bd=r(Ts,"CODE",{});var aS=i(bd);C7=n(aS,"O(sqrt(n))"),aS.forEach(t),L7=n(Ts,", where "),yd=r(Ts,"CODE",{});var sS=i(yd);M7=n(sS,"n"),sS.forEach(t),O7=n(Ts," is the number of feed-forward layers."),Ts.forEach(t),n_=f(e),Ip=r(e,"P",{});var rS=i(Ip);B7=n(rS,"To activate this feature in \u{1F917} Transformers for models that support it, use:"),rS.forEach(t),p_=f(e),d(Ji.$$.fragment,e),h_=f(e),Ha=r(e,"P",{});var t2=i(Ha);q7=n(t2,"or add "),Ed=r(t2,"CODE",{});var iS=i(Ed);V7=n(iS,"--gradient_checkpointing"),iS.forEach(t),H7=n(t2," to the Trainer arguments."),t2.forEach(t),f_=f(e),nt=r(e,"H3",{class:!0});var a2=i(nt);Fa=r(a2,"A",{id:!0,class:!0,href:!0});var oS=i(Fa);Pd=r(oS,"SPAN",{});var lS=i(Pd);d(Zi.$$.fragment,lS),lS.forEach(t),oS.forEach(t),F7=f(a2),$d=r(a2,"SPAN",{});var nS=i($d);W7=n(nS,"Batch sizes"),nS.forEach(t),a2.forEach(t),c_=f(e),Gp=r(e,"P",{});var pS=i(Gp);R7=n(pS,"One gets the most efficient performance when batch sizes and input/output neuron counts are divisible by a certain number, which typically starts at 8, but can be much higher as well. That number varies a lot depending on the specific hardware being used and the dtype of the model."),pS.forEach(t),m_=f(e),we=r(e,"P",{});var Oh=i(we);X7=n(Oh,"For example for fully connected layers (which correspond to GEMMs), NVIDIA provides recommendations for "),Ki=r(Oh,"A",{href:!0,rel:!0});var hS=i(Ki);Y7=n(hS,"input/output neuron counts"),hS.forEach(t),Q7=n(Oh," and "),eo=r(Oh,"A",{href:!0,rel:!0});var fS=i(eo);J7=n(fS,"batch size"),fS.forEach(t),Z7=n(Oh,"."),Oh.forEach(t),d_=f(e),to=r(e,"P",{});var xT=i(to);ao=r(xT,"A",{href:!0,rel:!0});var cS=i(ao);K7=n(cS,"Tensor Core Requirements"),cS.forEach(t),e8=n(xT," define the multiplier based on the dtype and the hardware. For example, for fp16 a multiple of 8 is recommended, but on A100 it\u2019s 64!"),xT.forEach(t),u_=f(e),Wa=r(e,"P",{});var s2=i(Wa);t8=n(s2,"For parameters that are small, there is also "),so=r(s2,"A",{href:!0,rel:!0});var mS=i(so);a8=n(mS,"Dimension Quantization Effects"),mS.forEach(t),s8=n(s2," to consider, this is where tiling happens and the right multiplier can have a significant speedup."),s2.forEach(t),v_=f(e),H=r(e,"P",{});var js=i(H);r8=n(js,"Additionally, as explained in the "),Up=r(js,"A",{href:!0});var dS=i(Up);i8=n(dS,"Gradient Accumulation"),dS.forEach(t),o8=n(js,` section, the bigger the batch size the less often the optimizer is run, the faster the training is (considering the same dataset length). See benchmarks
for `),ro=r(js,"A",{href:!0,rel:!0});var uS=i(ro);l8=n(uS,"RTX-3090"),uS.forEach(t),n8=n(js,`
and `),io=r(js,"A",{href:!0,rel:!0});var vS=i(io);p8=n(vS,"A100"),vS.forEach(t),h8=n(js,"."),js.forEach(t),w_=f(e),pt=r(e,"H3",{class:!0});var r2=i(pt);Ra=r(r2,"A",{id:!0,class:!0,href:!0});var wS=i(Ra);kd=r(wS,"SPAN",{});var gS=i(kd);d(oo.$$.fragment,gS),gS.forEach(t),wS.forEach(t),f8=f(r2),Ad=r(r2,"SPAN",{});var _S=i(Ad);c8=n(_S,"DP vs DDP"),_S.forEach(t),r2.forEach(t),g_=f(e),ht=r(e,"P",{});var xu=i(ht);Td=r(xu,"CODE",{});var bS=i(Td);m8=n(bS,"DistributedDataParallel"),bS.forEach(t),d8=n(xu," (DDP) is typically faster than "),jd=r(xu,"CODE",{});var yS=i(jd);u8=n(yS,"DataParallel"),yS.forEach(t),v8=n(xu," (DP), but it is not always the case:"),xu.forEach(t),__=f(e),Xa=r(e,"UL",{});var i2=i(Xa);xd=r(i2,"LI",{});var ES=i(xd);w8=n(ES,"while DP is python threads-based, DDP is multiprocess-based - and as such it has no python threads limitations, such as GIL"),ES.forEach(t),g8=f(i2),Dd=r(i2,"LI",{});var PS=i(Dd);_8=n(PS,"on the other hand a slow inter-connectivity between the GPU cards could lead to an actual slower outcome with DDP"),PS.forEach(t),i2.forEach(t),b_=f(e),zp=r(e,"P",{});var $S=i(zp);b8=n($S,"Here are the main differences in the inter-GPU communication overhead between the two modes:"),$S.forEach(t),y_=f(e),lo=r(e,"P",{});var DT=i(lo);no=r(DT,"A",{href:!0,rel:!0});var kS=i(no);y8=n(kS,"DDP"),kS.forEach(t),E8=n(DT,":"),DT.forEach(t),E_=f(e),Ya=r(e,"UL",{});var o2=i(Ya);Id=r(o2,"LI",{});var AS=i(Id);P8=n(AS,"At the start time the main process replicates the model once from gpu 0 to the rest of gpus"),AS.forEach(t),$8=f(o2),Sp=r(o2,"LI",{});var IT=i(Sp);k8=n(IT,"Then for each batch:"),po=r(IT,"OL",{});var l2=i(po);Gd=r(l2,"LI",{});var TS=i(Gd);A8=n(TS,"each gpu consumes each own mini-batch of data directly"),TS.forEach(t),T8=f(l2),ho=r(l2,"LI",{});var n2=i(ho);j8=n(n2,"during "),Ud=r(n2,"CODE",{});var jS=i(Ud);x8=n(jS,"backward"),jS.forEach(t),D8=n(n2,", once the local gradients are ready, they are then averaged across all processes"),n2.forEach(t),l2.forEach(t),IT.forEach(t),o2.forEach(t),P_=f(e),fo=r(e,"P",{});var GT=i(fo);co=r(GT,"A",{href:!0,rel:!0});var xS=i(co);I8=n(xS,"DP"),xS.forEach(t),G8=n(GT,":"),GT.forEach(t),$_=f(e),Np=r(e,"P",{});var DS=i(Np);U8=n(DS,"For each batch:"),DS.forEach(t),k_=f(e),G=r(e,"OL",{});var Ee=i(G);zd=r(Ee,"LI",{});var IS=i(zd);z8=n(IS,"gpu 0 reads the batch of data and then sends a mini-batch to each gpu"),IS.forEach(t),S8=f(Ee),Sd=r(Ee,"LI",{});var GS=i(Sd);N8=n(GS,"replicates the up-to-date model from gpu 0 to each gpu"),GS.forEach(t),C8=f(Ee),mo=r(Ee,"LI",{});var p2=i(mo);L8=n(p2,"runs "),Nd=r(p2,"CODE",{});var US=i(Nd);M8=n(US,"forward"),US.forEach(t),O8=n(p2," and sends output from each gpu to gpu 0, computes loss"),p2.forEach(t),B8=f(Ee),Cp=r(Ee,"LI",{});var UT=i(Cp);q8=n(UT,"scatters loss from gpu 0 to all gpus, runs "),Cd=r(UT,"CODE",{});var zS=i(Cd);V8=n(zS,"backward"),zS.forEach(t),UT.forEach(t),H8=f(Ee),Ld=r(Ee,"LI",{});var SS=i(Ld);F8=n(SS,"sends gradients from each gpu to gpu 0 and averages those"),SS.forEach(t),Ee.forEach(t),A_=f(e),Lp=r(e,"P",{});var NS=i(Lp);W8=n(NS,"The only communication DDP performs per batch is sending gradients, whereas DP does 5 different data exchanges per batch."),NS.forEach(t),T_=f(e),Qa=r(e,"P",{});var h2=i(Qa);R8=n(h2,"DP copies data within the process via python threads, whereas DDP copies data via "),uo=r(h2,"A",{href:!0,rel:!0});var CS=i(uo);X8=n(CS,"torch.distributed"),CS.forEach(t),Y8=n(h2,"."),h2.forEach(t),j_=f(e),Mp=r(e,"P",{});var LS=i(Mp);Q8=n(LS,"Under DP gpu 0 performs a lot more work than the rest of the gpus, thus resulting in under-utilization of gpus."),LS.forEach(t),x_=f(e),Op=r(e,"P",{});var MS=i(Op);J8=n(MS,"You can use DDP across multiple machines, but this is not the case with DP."),MS.forEach(t),D_=f(e),Bp=r(e,"P",{});var OS=i(Bp);Z8=n(OS,"There are other differences between DP and DDP but they aren\u2019t relevant to this discussion."),OS.forEach(t),I_=f(e),Ja=r(e,"P",{});var f2=i(Ja);K8=n(f2,"If you want to go really deep into understanding these 2 modes, this "),vo=r(f2,"A",{href:!0,rel:!0});var BS=i(vo);e9=n(BS,"article"),BS.forEach(t),t9=n(f2," is highly recommended, as it has great diagrams, includes multiple benchmarks and profiler outputs on various hardware, explains all the nuances that you may need to know."),f2.forEach(t),G_=f(e),qp=r(e,"P",{});var qS=i(qp);a9=n(qS,"Let\u2019s look at an actual benchmark:"),qS.forEach(t),U_=f(e),Za=r(e,"TABLE",{});var c2=i(Za);Md=r(c2,"THEAD",{});var VS=i(Md);ft=r(VS,"TR",{});var Bh=i(ft);Vp=r(Bh,"TH",{align:!0});var HS=i(Vp);s9=n(HS,"Type"),HS.forEach(t),r9=f(Bh),Od=r(Bh,"TH",{});var FS=i(Od);i9=n(FS,"NVlink"),FS.forEach(t),o9=f(Bh),Hp=r(Bh,"TH",{align:!0});var WS=i(Hp);l9=n(WS,"Time"),WS.forEach(t),Bh.forEach(t),VS.forEach(t),n9=f(c2),ct=r(c2,"TBODY",{});var qh=i(ct);mt=r(qh,"TR",{});var Vh=i(mt);Fp=r(Vh,"TD",{align:!0});var RS=i(Fp);p9=n(RS,"2:DP"),RS.forEach(t),h9=f(Vh),Bd=r(Vh,"TD",{});var XS=i(Bd);f9=n(XS,"Y"),XS.forEach(t),c9=f(Vh),Wp=r(Vh,"TD",{align:!0});var YS=i(Wp);m9=n(YS,"110s"),YS.forEach(t),Vh.forEach(t),d9=f(qh),dt=r(qh,"TR",{});var Hh=i(dt);Rp=r(Hh,"TD",{align:!0});var QS=i(Rp);u9=n(QS,"2:DDP"),QS.forEach(t),v9=f(Hh),qd=r(Hh,"TD",{});var JS=i(qd);w9=n(JS,"Y"),JS.forEach(t),g9=f(Hh),Xp=r(Hh,"TD",{align:!0});var ZS=i(Xp);_9=n(ZS,"101s"),ZS.forEach(t),Hh.forEach(t),b9=f(qh),ut=r(qh,"TR",{});var Fh=i(ut);Yp=r(Fh,"TD",{align:!0});var KS=i(Yp);y9=n(KS,"2:DDP"),KS.forEach(t),E9=f(Fh),Vd=r(Fh,"TD",{});var eN=i(Vd);P9=n(eN,"N"),eN.forEach(t),$9=f(Fh),Qp=r(Fh,"TD",{align:!0});var tN=i(Qp);k9=n(tN,"131s"),tN.forEach(t),Fh.forEach(t),qh.forEach(t),c2.forEach(t),z_=f(e),Jp=r(e,"P",{});var aN=i(Jp);A9=n(aN,"Analysis:"),aN.forEach(t),S_=f(e),Zp=r(e,"P",{});var sN=i(Zp);T9=n(sN,"Here DP is ~10% slower than DDP w/ NVlink, but ~15% faster than DDP w/o NVlink"),sN.forEach(t),N_=f(e),Kp=r(e,"P",{});var rN=i(Kp);j9=n(rN,"The real difference will depend on how much data each GPU needs to sync with the others - the more there is to sync, the more a slow link will slow down the total runtime."),rN.forEach(t),C_=f(e),eh=r(e,"P",{});var iN=i(eh);x9=n(iN,"Here is the full benchmark code and outputs:"),iN.forEach(t),L_=f(e),wo=r(e,"P",{});var zT=i(wo);Hd=r(zT,"CODE",{});var oN=i(Hd);D9=n(oN,"NCCL_P2P_DISABLE=1"),oN.forEach(t),I9=n(zT," was used to disable the NVLink feature on the corresponding benchmark."),zT.forEach(t),M_=f(e),d(go.$$.fragment,e),O_=f(e),j=r(e,"P",{});var ee=i(j);G9=n(ee,"Hardware: 2x TITAN RTX 24GB each + NVlink with 2 NVLinks ("),Fd=r(ee,"CODE",{});var lN=i(Fd);U9=n(lN,"NV2"),lN.forEach(t),z9=n(ee," in "),Wd=r(ee,"CODE",{});var nN=i(Wd);S9=n(nN,"nvidia-smi topo -m"),nN.forEach(t),N9=n(ee,`)
Software: `),Rd=r(ee,"CODE",{});var pN=i(Rd);C9=n(pN,"pytorch-1.8-to-be"),pN.forEach(t),L9=n(ee," + "),Xd=r(ee,"CODE",{});var hN=i(Xd);M9=n(hN,"cuda-11.0"),hN.forEach(t),O9=n(ee," / "),Yd=r(ee,"CODE",{});var fN=i(Yd);B9=n(fN,"transformers==4.3.0.dev0"),fN.forEach(t),ee.forEach(t),B_=f(e),vt=r(e,"H3",{class:!0});var m2=i(vt);Ka=r(m2,"A",{id:!0,class:!0,href:!0});var cN=i(Ka);Qd=r(cN,"SPAN",{});var mN=i(Qd);d(_o.$$.fragment,mN),mN.forEach(t),cN.forEach(t),q9=f(m2),Jd=r(m2,"SPAN",{});var dN=i(Jd);V9=n(dN,"DataLoader"),dN.forEach(t),m2.forEach(t),q_=f(e),th=r(e,"P",{});var uN=i(th);H9=n(uN,"One of the important requirements to reach great training speed is the ability to feed the GPU at the maximum speed it can handle. By default everything happens in the main process and it might not be able to read the data from disk fast enough, and thus create a bottleneck, leading to GPU under-utilization."),uN.forEach(t),V_=f(e),es=r(e,"UL",{});var d2=i(es);ah=r(d2,"LI",{});var ST=i(ah);Zd=r(ST,"CODE",{});var vN=i(Zd);F9=n(vN,"DataLoader(pin_memory=True, ...)"),vN.forEach(t),W9=n(ST," which ensures that the data gets preloaded into the pinned memory on CPU and typically leads to much faster transfers from CPU to GPU memory."),ST.forEach(t),R9=f(d2),sh=r(d2,"LI",{});var NT=i(sh);Kd=r(NT,"CODE",{});var wN=i(Kd);X9=n(wN,"DataLoader(num_workers=4, ...)"),wN.forEach(t),Y9=n(NT," - spawn several workers to pre-load data faster - during training watch the GPU utilization stats and if it\u2019s far from 100% experiment with raising the number of workers. Of course, the problem could be elsewhere so a very big number of workers won\u2019t necessarily lead to a better performance."),NT.forEach(t),d2.forEach(t),H_=f(e),wt=r(e,"H3",{class:!0});var u2=i(wt);ts=r(u2,"A",{id:!0,class:!0,href:!0});var gN=i(ts);eu=r(gN,"SPAN",{});var _N=i(eu);d(bo.$$.fragment,_N),_N.forEach(t),gN.forEach(t),Q9=f(u2),tu=r(u2,"SPAN",{});var bN=i(tu);J9=n(bN,"Optimizers"),bN.forEach(t),u2.forEach(t),F_=f(e),gt=r(e,"H4",{class:!0});var v2=i(gt);as=r(v2,"A",{id:!0,class:!0,href:!0});var yN=i(as);au=r(yN,"SPAN",{});var EN=i(au);d(yo.$$.fragment,EN),EN.forEach(t),yN.forEach(t),Z9=f(v2),su=r(v2,"SPAN",{});var PN=i(su);K9=n(PN,"Faster optimizers"),PN.forEach(t),v2.forEach(t),W_=f(e),rh=r(e,"UL",{});var $N=i(rh);ru=r($N,"LI",{});var kN=i(ru);iu=r(kN,"CODE",{});var AN=i(iu);eA=n(AN,"torch.optim.AdamW"),AN.forEach(t),kN.forEach(t),$N.forEach(t),R_=f(e),ss=r(e,"P",{});var w2=i(ss);ou=r(w2,"CODE",{});var TN=i(ou);tA=n(TN,"torch.optim.AdamW"),TN.forEach(t),aA=n(w2," is faster than Transformers\u2019 "),lu=r(w2,"CODE",{});var jN=i(lu);sA=n(jN,"AdamW"),jN.forEach(t),w2.forEach(t),X_=f(e),ih=r(e,"UL",{});var xN=i(ih);oh=r(xN,"LI",{});var CT=i(oh);nu=r(CT,"CODE",{});var DN=i(nu);rA=n(DN,"apex.optimizers.FusedAdam"),DN.forEach(t),iA=n(CT," is supposed to be even faster"),CT.forEach(t),xN.forEach(t),Y_=f(e),lh=r(e,"P",{});var IN=i(lh);Eo=r(IN,"A",{href:!0,rel:!0});var GN=i(Eo);oA=n(GN,"https://nvidia.github.io/apex/optimizers.html"),GN.forEach(t),IN.forEach(t),Q_=f(e),rs=r(e,"P",{});var g2=i(rs);lA=n(g2,"XXX: benchmark once "),Po=r(g2,"A",{href:!0,rel:!0});var UN=i(Po);nA=n(UN,"https://github.com/huggingface/transformers/issues/14539"),UN.forEach(t),pA=n(g2," is resolved"),g2.forEach(t),J_=f(e),_t=r(e,"H4",{class:!0});var _2=i(_t);is=r(_2,"A",{id:!0,class:!0,href:!0});var zN=i(is);pu=r(zN,"SPAN",{});var SN=i(pu);d($o.$$.fragment,SN),SN.forEach(t),zN.forEach(t),hA=f(_2),hu=r(_2,"SPAN",{});var NN=i(hu);fA=n(NN,"Leaner Optimizers"),NN.forEach(t),_2.forEach(t),Z_=f(e),nh=r(e,"UL",{});var CN=i(nh);ko=r(CN,"LI",{});var b2=i(ko);ph=r(b2,"P",{});var LT=i(ph);Ao=r(LT,"A",{href:!0,rel:!0});var LN=i(Ao);cA=n(LN,"bitsandbytes"),LN.forEach(t),mA=n(LT," uses 1/4 of normal AdamW optimizer\u2019s memory and otherwise performs on par with the normal optimizer quality-wise."),LT.forEach(t),dA=f(b2),fu=r(b2,"P",{});var MN=i(fu);uA=n(MN,"It\u2019s a bit tricky to integrate into Transformers since it requires an Embedding layer that includes a layer norm, which currently our models don\u2019t have. But other than that you just configure to use that optimizer instead of the normal one."),MN.forEach(t),b2.forEach(t),CN.forEach(t),K_=f(e),bt=r(e,"H4",{class:!0});var y2=i(bt);os=r(y2,"A",{id:!0,class:!0,href:!0});var ON=i(os);cu=r(ON,"SPAN",{});var BN=i(cu);d(To.$$.fragment,BN),BN.forEach(t),ON.forEach(t),vA=f(y2),mu=r(y2,"SPAN",{});var qN=i(mu);wA=n(qN,"Faster optimizer"),qN.forEach(t),y2.forEach(t),eb=f(e),yt=r(e,"P",{});var Du=i(yt);gA=n(Du,"pytorch-nightly introduced "),du=r(Du,"CODE",{});var VN=i(du);_A=n(VN,"torch.optim._multi_tensor"),VN.forEach(t),bA=n(Du," which should significantly speed up the optimizers for situations with lots of small feature tensors. It should eventually become the default, but if you want to experiment with it sooner and don\u2019t mind using the bleed-edge, see: "),jo=r(Du,"A",{href:!0,rel:!0});var HN=i(jo);yA=n(HN,"https://github.com/huggingface/transformers/issues/9965"),HN.forEach(t),Du.forEach(t),tb=f(e),Et=r(e,"H3",{class:!0});var E2=i(Et);ls=r(E2,"A",{id:!0,class:!0,href:!0});var FN=i(ls);uu=r(FN,"SPAN",{});var WN=i(uu);d(xo.$$.fragment,WN),WN.forEach(t),FN.forEach(t),EA=f(E2),vu=r(E2,"SPAN",{});var RN=i(vu);PA=n(RN,"Sparsity"),RN.forEach(t),E2.forEach(t),ab=f(e),Pt=r(e,"H4",{class:!0});var P2=i(Pt);ns=r(P2,"A",{id:!0,class:!0,href:!0});var XN=i(ns);wu=r(XN,"SPAN",{});var YN=i(wu);d(Do.$$.fragment,YN),YN.forEach(t),XN.forEach(t),$A=f(P2),gu=r(P2,"SPAN",{});var QN=i(gu);kA=n(QN,"Mixture of Experts"),QN.forEach(t),P2.forEach(t),sb=f(e),hh=r(e,"P",{});var JN=i(hh);AA=n(JN,`Quite a few of the recent papers reported a 4-5x training speedup and a faster inference by integrating
Mixture of Experts (MoE) into the Transformer models.`),JN.forEach(t),rb=f(e),fh=r(e,"P",{});var ZN=i(fh);TA=n(ZN,"Since it has been discovered that more parameters lead to better performance, this technique allows to increase the number of parameters by an order of magnitude without increasing training costs."),ZN.forEach(t),ib=f(e),ch=r(e,"P",{});var KN=i(ch);jA=n(KN,"In this approach every other FFN layer is replaced with a MoE Layer which consists of many experts, with a gated function that trains each expert in a balanced way depending on the input token\u2019s position in a sequence."),KN.forEach(t),ob=f(e),mh=r(e,"P",{});var eC=i(mh);dh=r(eC,"IMG",{src:!0,alt:!0}),eC.forEach(t),lb=f(e),ps=r(e,"P",{});var $2=i(ps);xA=n($2,"(source: "),Io=r($2,"A",{href:!0,rel:!0});var tC=i(Io);DA=n(tC,"GLAM"),tC.forEach(t),IA=n($2,")"),$2.forEach(t),nb=f(e),uh=r(e,"P",{});var aC=i(uh);GA=n(aC,"You can find exhaustive details and comparison tables in the papers listed at the end of this section."),aC.forEach(t),pb=f(e),vh=r(e,"P",{});var sC=i(vh);UA=n(sC,"The main drawback of this approach is that it requires staggering amounts of GPU memory - almost an order of magnitude larger than its dense equivalent. Various distillation and approaches are proposed to how to overcome the much higher memory requirements."),sC.forEach(t),hb=f(e),wh=r(e,"P",{});var rC=i(wh);zA=n(rC,"There is direct trade-off though, you can use just a few experts with a 2-3x smaller base model instead of dozens or hundreds experts leading to a 5x smaller model and thus increase the training speed moderately while increasing the memory requirements moderately as well."),rC.forEach(t),fb=f(e),gh=r(e,"P",{});var iC=i(gh);SA=n(iC,"Most related papers and implementations are built around Tensorflow/TPUs:"),iC.forEach(t),cb=f(e),ge=r(e,"UL",{});var Wh=i(ge);_u=r(Wh,"LI",{});var oC=i(_u);Go=r(oC,"A",{href:!0,rel:!0});var lC=i(Go);NA=n(lC,"GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding"),lC.forEach(t),oC.forEach(t),CA=f(Wh),bu=r(Wh,"LI",{});var nC=i(bu);Uo=r(nC,"A",{href:!0,rel:!0});var pC=i(Uo);LA=n(pC,"Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity"),pC.forEach(t),nC.forEach(t),MA=f(Wh),yu=r(Wh,"LI",{});var hC=i(yu);zo=r(hC,"A",{href:!0,rel:!0});var fC=i(zo);OA=n(fC,"GLaM: Generalist Language Model (GLaM)"),fC.forEach(t),hC.forEach(t),Wh.forEach(t),mb=f(e),P=r(e,"P",{});var z=i(P);BA=n(z,"And for Pytorch DeepSpeed has built one as well: "),So=r(z,"A",{href:!0,rel:!0});var cC=i(So);qA=n(cC,"DeepSpeed-MoE: Advancing Mixture-of-Experts Inference and Training to Power Next-Generation AI Scale"),cC.forEach(t),VA=n(z,", "),No=r(z,"A",{href:!0,rel:!0});var mC=i(No);HA=n(mC,"Mixture of Experts"),mC.forEach(t),FA=n(z," - blog posts:  "),Co=r(z,"A",{href:!0,rel:!0});var dC=i(Co);WA=n(dC,"1"),dC.forEach(t),RA=n(z,", "),Lo=r(z,"A",{href:!0,rel:!0});var uC=i(Lo);XA=n(uC,"2"),uC.forEach(t),YA=n(z," and specific deployment with large transformer-based natural language generation models: "),Mo=r(z,"A",{href:!0,rel:!0});var vC=i(Mo);QA=n(vC,"blog post"),vC.forEach(t),JA=n(z,", "),_h=r(z,"A",{href:!0});var wC=i(_h);ZA=n(wC,"Megatron-Deepspeed branch"),wC.forEach(t),KA=n(z,"."),z.forEach(t),db=f(e),$t=r(e,"H3",{class:!0});var k2=i($t);hs=r(k2,"A",{id:!0,class:!0,href:!0});var gC=i(hs);Eu=r(gC,"SPAN",{});var _C=i(Eu);d(Oo.$$.fragment,_C),_C.forEach(t),gC.forEach(t),eT=f(k2),Pu=r(k2,"SPAN",{});var bC=i(Pu);tT=n(bC,"Efficient Software Prebuilds"),bC.forEach(t),k2.forEach(t),ub=f(e),fs=r(e,"P",{});var A2=i(fs);aT=n(A2,"PyTorch\u2019s "),Bo=r(A2,"A",{href:!0,rel:!0});var yC=i(Bo);sT=n(yC,"pip and conda builds"),yC.forEach(t),rT=n(A2," come prebuit with the cuda toolkit which is enough to run PyTorch, but it is insufficient if you need to build cuda extensions."),A2.forEach(t),vb=f(e),cs=r(e,"P",{});var T2=i(cs);iT=n(T2,"At times it may take an additional effort to pre-build some components, e.g., if you\u2019re using libraries like "),$u=r(T2,"CODE",{});var EC=i($u);oT=n(EC,"apex"),EC.forEach(t),lT=n(T2," that don\u2019t come pre-compiled. In other situations figuring out how to install the right cuda toolkit system-wide can be complicated. To address these users\u2019 needs PyTorch and NVIDIA release a new version of NGC docker container which already comes with everything prebuilt and you just need to install your programs on it and it will run out of the box."),T2.forEach(t),wb=f(e),bh=r(e,"P",{});var PC=i(bh);nT=n(PC,"This approach is also useful if you want to tweak the pytorch source and/or make a new customized build."),PC.forEach(t),gb=f(e),_e=r(e,"P",{});var Rh=i(_e);pT=n(Rh,"To find the docker image version you want start "),qo=r(Rh,"A",{href:!0,rel:!0});var $C=i(qo);hT=n($C,"here"),$C.forEach(t),fT=n(Rh,", choose one of the latest monthly releases. Go into the release\u2019s notes for the desired release, check that the environment\u2019s components are matching your needs (including NVIDIA Driver requirements!) and then at the very top of that document go to the corresponding NGC page. If for some reason you get lost, here is "),Vo=r(Rh,"A",{href:!0,rel:!0});var kC=i(Vo);cT=n(kC,"the index of all PyTorch NGC images"),kC.forEach(t),mT=n(Rh,"."),Rh.forEach(t),_b=f(e),yh=r(e,"P",{});var AC=i(yh);dT=n(AC,"Next follow the instructions to download and deploy the docker image."),AC.forEach(t),bb=f(e),kt=r(e,"H2",{class:!0});var j2=i(kt);ms=r(j2,"A",{id:!0,class:!0,href:!0});var TC=i(ms);ku=r(TC,"SPAN",{});var jC=i(ku);d(Ho.$$.fragment,jC),jC.forEach(t),TC.forEach(t),uT=f(j2),Au=r(j2,"SPAN",{});var xC=i(Au);vT=n(xC,"Contribute"),xC.forEach(t),j2.forEach(t),yb=f(e),Eh=r(e,"P",{});var DC=i(Eh);wT=n(DC,"This document is far from being complete and a lot more needs to be added, so if you have additions or corrections to make please don\u2019t hesitate to open a PR or if you aren\u2019t sure start an Issue and we can discuss the details there."),DC.forEach(t),Eb=f(e),Ph=r(e,"P",{});var IC=i(Ph);gT=n(IC,"When making contributions that A is better than B, please try to include a reproducible benchmark and/or a link to the source of that information (unless it comes directly from you)."),IC.forEach(t),this.h()},h(){c(y,"name","hf:doc:metadata"),c(y,"content",JSON.stringify(qC)),c(S,"id","performance-and-scalability-how-to-fit-a-bigger-model-and-train-it-faster"),c(S,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(S,"href","#performance-and-scalability-how-to-fit-a-bigger-model-and-train-it-faster"),c($,"class","relative group"),c(Xo,"href","/docs/transformers/pr_14708/en/main_classes/trainer#transformers.Trainer"),c(Is,"href","https://huggingface.co/docs/accelerate/"),c(Is,"rel","nofollow"),c(Us,"href","https://huggingface.co/docs/datasets/package_reference/main_classes.html?highlight=dataset#datasets.Dataset"),c(Us,"rel","nofollow"),c(Yo,"href","/docs/transformers/pr_14708/en/main_classes/trainer#transformers.Trainer"),c(jt,"id","load-model"),c(jt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(jt,"href","#load-model"),c(Pe,"class","relative group"),c(Gt,"id","vanilla-training"),c(Gt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Gt,"href","#vanilla-training"),c($e,"class","relative group"),c(el,"href","/docs/transformers/pr_14708/en/main_classes/trainer#transformers.Trainer"),c(zt,"id","gradient-accumulation"),c(zt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(zt,"href","#gradient-accumulation"),c(ke,"class","relative group"),c(sl,"href","/docs/transformers/pr_14708/en/main_classes/trainer#transformers.Trainer"),c(rl,"href","/docs/transformers/pr_14708/en/main_classes/trainer#transformers.TrainingArguments"),c(St,"id","gradient-checkpointing"),c(St,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(St,"href","#gradient-checkpointing"),c(Ae,"class","relative group"),c(Qs,"href","https://medium.com/tensorflow/fitting-larger-networks-into-memory-583e3c758ff9"),c(Qs,"rel","nofollow"),c(ll,"href","/docs/transformers/pr_14708/en/main_classes/trainer#transformers.Trainer"),c(nl,"href","/docs/transformers/pr_14708/en/main_classes/trainer#transformers.TrainingArguments"),c(Ct,"id","fp16-training"),c(Ct,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Ct,"href","#fp16-training"),c(Te,"class","relative group"),c(Lt,"id","optimizer"),c(Lt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Lt,"href","#optimizer"),c(je,"class","relative group"),c(Mt,"id","adafactor"),c(Mt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Mt,"href","#adafactor"),c(xe,"class","relative group"),c(Bt,"id","8bit-adam"),c(Bt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Bt,"href","#8bit-adam"),c(De,"class","relative group"),c(vl,"href","/docs/transformers/pr_14708/en/main_classes/trainer#transformers.Trainer"),c(wl,"href","/docs/transformers/pr_14708/en/main_classes/trainer#transformers.Trainer"),c(fr,"href","https://github.com/facebookresearch/bitsandbytes"),c(fr,"rel","nofollow"),MT(El.src,OT="https://huggingface.co/datasets/lvwerra/repo-images/raw/main/gpu-memory-savings.png")||c(El,"src",OT),c(El,"alt","png"),c(Ht,"id","using-accelerate"),c(Ht,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Ht,"href","#using-accelerate"),c(Ie,"class","relative group"),c(Pl,"href","/docs/transformers/pr_14708/en/main_classes/trainer#transformers.Trainer"),c($l,"href","/docs/transformers/pr_14708/en/main_classes/trainer#transformers.TrainingArguments"),c(br,"href","https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader"),c(br,"rel","nofollow"),c(Al,"href","/docs/transformers/pr_14708/en/main_classes/model#transformers.PreTrainedModel.gradient_checkpointing_enable"),c(yr,"href","https://huggingface.co/docs/accelerate/accelerator.html#accelerate.Accelerator"),c(yr,"rel","nofollow"),c(Er,"href","https://huggingface.co/docs/accelerate/accelerator.html#accelerate.Accelerator.prepare"),c(Er,"rel","nofollow"),c(Wt,"id","how-to-scale"),c(Wt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Wt,"href","#how-to-scale"),c(Ge,"class","relative group"),c(Xt,"id","multigpu-training"),c(Xt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Xt,"href","#multigpu-training"),c(Ue,"class","relative group"),c(Ul,"href","/docs/transformers/pr_14708/en/main_classes/trainer#transformers.Trainer"),c(zl,"href","/docs/transformers/pr_14708/en/main_classes/trainer#transformers.Trainer"),c(Yt,"id","what-if-my-model-still-does-not-fit"),c(Yt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Yt,"href","#what-if-my-model-still-does-not-fit"),c(ze,"class","relative group"),c(Nl,"href","parallelism"),c(Jt,"id","further-discussions"),c(Jt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Jt,"href","#further-discussions"),c(Se,"class","relative group"),c(Zt,"id","faster-training"),c(Zt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Zt,"href","#faster-training"),c(Ne,"class","relative group"),c(ea,"id","bigger-models"),c(ea,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(ea,"href","#bigger-models"),c(Ce,"class","relative group"),c(Hl,"href","deepspeed#nvme-support"),c(ta,"id","hardware"),c(ta,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(ta,"href","#hardware"),c(Le,"class","relative group"),c(aa,"id","power-and-cooling"),c(aa,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(aa,"href","#power-and-cooling"),c(Me,"class","relative group"),c(sa,"id","multigpu-connectivity"),c(sa,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(sa,"href","#multigpu-connectivity"),c(Oe,"class","relative group"),c(ra,"id","nvlink"),c(ra,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(ra,"href","#nvlink"),c(Be,"class","relative group"),c(Hr,"href","https://en.wikipedia.org/wiki/NVLink"),c(Hr,"rel","nofollow"),c(Fr,"href","https://www.nvidia.com/content/dam/en-zz/Solutions/geforce/ampere/pdf/NVIDIA-ampere-GA102-GPU-Architecture-Whitepaper-V1.pdf"),c(Fr,"rel","nofollow"),c(mn,"align","right"),c(dn,"align","right"),c(un,"align","right"),c(na,"id","software"),c(na,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(na,"href","#software"),c(qe,"class","relative group"),c(pa,"id","model-scalability"),c(pa,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(pa,"href","#model-scalability"),c(Ve,"class","relative group"),c(_n,"href","parallelism"),c(ha,"id","anatomy-of-models-operations"),c(ha,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(ha,"href","#anatomy-of-models-operations"),c(He,"class","relative group"),c(ni,"href","https://arxiv.org/abs/2007.00072"),c(ni,"rel","nofollow"),c(fa,"id","anatomy-of-models-memory"),c(fa,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(fa,"href","#anatomy-of-models-memory"),c(Fe,"class","relative group"),c(ca,"id","model-weights"),c(ca,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(ca,"href","#model-weights"),c(We,"class","relative group"),c(da,"id","optimizer-states"),c(da,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(da,"href","#optimizer-states"),c(Re,"class","relative group"),c(ci,"href","https://github.com/facebookresearch/bitsandbytes"),c(ci,"rel","nofollow"),c(ua,"id","gradients"),c(ua,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(ua,"href","#gradients"),c(Xe,"class","relative group"),c(va,"id","forward-activations"),c(va,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(va,"href","#forward-activations"),c(Ye,"class","relative group"),c(wa,"id","temporary-memory"),c(wa,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(wa,"href","#temporary-memory"),c(Qe,"class","relative group"),c(ga,"id","functionalityspecific-memory"),c(ga,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(ga,"href","#functionalityspecific-memory"),c(Je,"class","relative group"),c(_a,"id","forward-vs-backward-execution-speed"),c(_a,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(_a,"href","#forward-vs-backward-execution-speed"),c(Ze,"class","relative group"),c(ya,"id","floating-data-types"),c(ya,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(ya,"href","#floating-data-types"),c(Ke,"class","relative group"),MT(Nn.src,BT="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/tf32-bf16-fp16-fp32.png")||c(Nn,"src",BT),c(Nn,"alt","data types"),c(Ei,"href","https://developer.nvidia.com/blog/accelerating-ai-training-with-tf32-tensor-cores/"),c(Ei,"rel","nofollow"),c(Pa,"id","fp16"),c(Pa,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Pa,"href","#fp16"),c(et,"class","relative group"),c(qn,"align","right"),c(Vn,"align","right"),c(Hn,"align","right"),c(Fn,"align","right"),c(Wn,"align","right"),c(Rn,"align","right"),c(Xn,"align","right"),c(Yn,"align","right"),c(Qn,"align","right"),c(Jn,"align","right"),c(Zn,"align","right"),c(Kn,"align","right"),c(ep,"align","right"),c(tp,"align","right"),c(ap,"align","right"),c(sp,"align","right"),c(rp,"align","right"),c(ip,"align","right"),c(op,"align","right"),c(lp,"align","right"),c(Ai,"href","https://github.com/bigscience-workshop/Megatron-DeepSpeed"),c(Ai,"rel","nofollow"),c(Ti,"href","https://docs.fast.ai/callback.fp16.html#A-little-bit-of-theory"),c(Ti,"rel","nofollow"),c(ji,"href","https://spell.ml/blog/mixed-precision-training-with-pytorch-Xuk7YBEAACAASJam"),c(ji,"rel","nofollow"),c(xi,"href","https://github.com/huggingface/transformers/issues/14608#issuecomment-1004390803"),c(xi,"rel","nofollow"),c(Di,"href","https://github.com/huggingface/transformers/issues/15026#issuecomment-1004543189"),c(Di,"rel","nofollow"),c(Da,"id","fp16-caching"),c(Da,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Da,"href","#fp16-caching"),c(tt,"class","relative group"),c(Gi,"href","https://discuss.pytorch.org/t/autocast-and-torch-no-grad-unexpected-behaviour/93475/3"),c(Gi,"rel","nofollow"),c(Ia,"id","fp16-inference"),c(Ia,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Ia,"href","#fp16-inference"),c(at,"class","relative group"),c(Ua,"id","bf16"),c(Ua,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Ua,"href","#bf16"),c(st,"class","relative group"),c(Li,"href","https://github.com/huggingface/transformers/issues/14608#issuecomment-1004390803"),c(Li,"rel","nofollow"),c(Mi,"href","https://github.com/huggingface/transformers/issues/15026#issuecomment-1004543189"),c(Mi,"rel","nofollow"),c(Sa,"id","bf16-inference"),c(Sa,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Sa,"href","#bf16-inference"),c(rt,"class","relative group"),c($p,"href","#fp16-inference"),c(La,"id","tf32"),c(La,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(La,"href","#tf32"),c(it,"class","relative group"),c(Vi,"href","https://developer.nvidia.com/blog/accelerating-ai-training-with-tf32-tensor-cores/"),c(Vi,"rel","nofollow"),c(Hi,"href","https://github.com/huggingface/transformers/issues/14608#issuecomment-1004390803"),c(Hi,"rel","nofollow"),c(Fi,"href","https://github.com/huggingface/transformers/issues/15026#issuecomment-1004543189"),c(Fi,"rel","nofollow"),c(Ba,"id","gradient-accumulation"),c(Ba,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Ba,"href","#gradient-accumulation"),c(ot,"class","relative group"),c(Ri,"href","https://github.com/huggingface/transformers/issues/14608#issuecomment-1004392537"),c(Ri,"rel","nofollow"),c(Xi,"href","https://github.com/huggingface/transformers/issues/15026#issuecomment-1004592231"),c(Xi,"rel","nofollow"),c(Va,"id","gradient-checkpointing"),c(Va,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Va,"href","#gradient-checkpointing"),c(lt,"class","relative group"),c(Qi,"href","https://arxiv.org/abs/1604.06174"),c(Qi,"rel","nofollow"),c(Fa,"id","batch-sizes"),c(Fa,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Fa,"href","#batch-sizes"),c(nt,"class","relative group"),c(Ki,"href","https://docs.nvidia.com/deeplearning/performance/dl-performance-fully-connected/index.html#input-features"),c(Ki,"rel","nofollow"),c(eo,"href","https://docs.nvidia.com/deeplearning/performance/dl-performance-fully-connected/index.html#batch-size"),c(eo,"rel","nofollow"),c(ao,"href","https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc"),c(ao,"rel","nofollow"),c(so,"href","https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#dim-quantization"),c(so,"rel","nofollow"),c(Up,"href","#gradient-accumulation"),c(ro,"href","https://github.com/huggingface/transformers/issues/14608#issuecomment-1004392537"),c(ro,"rel","nofollow"),c(io,"href","https://github.com/huggingface/transformers/issues/15026#issuecomment-1005033957"),c(io,"rel","nofollow"),c(Ra,"id","dp-vs-ddp"),c(Ra,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Ra,"href","#dp-vs-ddp"),c(pt,"class","relative group"),c(no,"href","https://pytorch.org/docs/master/notes/ddp.html"),c(no,"rel","nofollow"),c(co,"href","https://pytorch.org/docs/master/generated/torch.nn.DataParallel.html"),c(co,"rel","nofollow"),c(uo,"href","https://pytorch.org/docs/master/distributed.html"),c(uo,"rel","nofollow"),c(vo,"href","https://www.telesens.co/2019/04/04/distributed-data-parallel-training-using-pytorch-on-aws/"),c(vo,"rel","nofollow"),c(Vp,"align","left"),c(Hp,"align","right"),c(Fp,"align","left"),c(Wp,"align","right"),c(Rp,"align","left"),c(Xp,"align","right"),c(Yp,"align","left"),c(Qp,"align","right"),c(Ka,"id","dataloader"),c(Ka,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Ka,"href","#dataloader"),c(vt,"class","relative group"),c(ts,"id","optimizers"),c(ts,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(ts,"href","#optimizers"),c(wt,"class","relative group"),c(as,"id","faster-optimizers"),c(as,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(as,"href","#faster-optimizers"),c(gt,"class","relative group"),c(Eo,"href","https://nvidia.github.io/apex/optimizers.html"),c(Eo,"rel","nofollow"),c(Po,"href","https://github.com/huggingface/transformers/issues/14539"),c(Po,"rel","nofollow"),c(is,"id","leaner-optimizers"),c(is,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(is,"href","#leaner-optimizers"),c(_t,"class","relative group"),c(Ao,"href","https://github.com/facebookresearch/bitsandbytes"),c(Ao,"rel","nofollow"),c(os,"id","faster-optimizer"),c(os,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(os,"href","#faster-optimizer"),c(bt,"class","relative group"),c(jo,"href","https://github.com/huggingface/transformers/issues/9965"),c(jo,"rel","nofollow"),c(ls,"id","sparsity"),c(ls,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(ls,"href","#sparsity"),c(Et,"class","relative group"),c(ns,"id","mixture-of-experts"),c(ns,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(ns,"href","#mixture-of-experts"),c(Pt,"class","relative group"),MT(dh.src,qT="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/perf-moe-transformer.png")||c(dh,"src",qT),c(dh,"alt","MoE Transformer 2x block"),c(Io,"href","https://ai.googleblog.com/2021/12/more-efficient-in-context-learning-with.html"),c(Io,"rel","nofollow"),c(Go,"href","https://arxiv.org/abs/2006.16668"),c(Go,"rel","nofollow"),c(Uo,"href","https://arxiv.org/abs/2101.03961"),c(Uo,"rel","nofollow"),c(zo,"href","https://ai.googleblog.com/2021/12/more-efficient-in-context-learning-with.html"),c(zo,"rel","nofollow"),c(So,"href","https://arxiv.org/abs/2201.05596"),c(So,"rel","nofollow"),c(No,"href","https://www.deepspeed.ai/tutorials/mixture-of-experts/"),c(No,"rel","nofollow"),c(Co,"href","https://www.microsoft.com/en-us/research/blog/deepspeed-powers-8x-larger-moe-model-training-with-high-performance/"),c(Co,"rel","nofollow"),c(Lo,"href","https://www.microsoft.com/en-us/research/publication/scalable-and-efficient-moe-training-for-multitask-multilingual-models/"),c(Lo,"rel","nofollow"),c(Mo,"href","https://www.deepspeed.ai/news/2021/12/09/deepspeed-moe-nlg.html"),c(Mo,"rel","nofollow"),c(_h,"href","Thttps://github.com/microsoft/Megatron-DeepSpeed/tree/moe-training"),c(hs,"id","efficient-software-prebuilds"),c(hs,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(hs,"href","#efficient-software-prebuilds"),c($t,"class","relative group"),c(Bo,"href","https://pytorch.org/get-started/locally/#start-locally"),c(Bo,"rel","nofollow"),c(qo,"href","https://docs.nvidia.com/deeplearning/frameworks/pytorch-release-notes/"),c(qo,"rel","nofollow"),c(Vo,"href","https://ngc.nvidia.com/catalog/containers/nvidia:pytorch"),c(Vo,"rel","nofollow"),c(ms,"id","contribute"),c(ms,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(ms,"href","#contribute"),c(kt,"class","relative group")},m(e,o){a(document.head,y),p(e,x,o),p(e,$,o),a($,S),a(S,Xh),u(xs,Xh,null),a($,x2),a($,Yh),a(Yh,D2),p(e,Iu,o),p(e,Ro,o),a(Ro,Qh),a(Qh,Jh),a(Jh,I2),p(e,Gu,o),u(Ds,e,o),p(e,Uu,o),p(e,te,o),a(te,G2),a(te,Xo),a(Xo,U2),a(te,z2),a(te,Is),a(Is,S2),a(te,N2),p(e,zu,o),u(Gs,e,o),p(e,Su,o),p(e,ae,o),a(ae,C2),a(ae,Zh),a(Zh,L2),a(ae,M2),a(ae,Kh),a(Kh,O2),a(ae,B2),p(e,Nu,o),p(e,At,o),a(At,q2),a(At,Us),a(Us,ef),a(ef,V2),a(At,H2),p(e,Cu,o),u(zs,e,o),p(e,Lu,o),p(e,Tt,o),a(Tt,F2),a(Tt,Yo),a(Yo,W2),a(Tt,R2),p(e,Mu,o),u(Ss,e,o),p(e,Ou,o),p(e,Qo,o),a(Qo,X2),p(e,Bu,o),u(Ns,e,o),p(e,qu,o),p(e,Jo,o),a(Jo,Y2),p(e,Vu,o),u(Cs,e,o),p(e,Hu,o),p(e,Zo,o),a(Zo,Q2),p(e,Fu,o),p(e,Pe,o),a(Pe,jt),a(jt,tf),u(Ls,tf,null),a(Pe,J2),a(Pe,af),a(af,Z2),p(e,Wu,o),p(e,xt,o),a(xt,K2),a(xt,sf),a(sf,e0),a(xt,t0),p(e,Ru,o),u(Ms,e,o),p(e,Xu,o),p(e,Dt,o),a(Dt,a0),a(Dt,rf),a(rf,s0),a(Dt,r0),p(e,Yu,o),u(Os,e,o),p(e,Qu,o),u(Bs,e,o),p(e,Ju,o),p(e,Ko,o),a(Ko,i0),p(e,Zu,o),u(qs,e,o),p(e,Ku,o),u(It,e,o),p(e,ev,o),p(e,$e,o),a($e,Gt),a(Gt,of),u(Vs,of,null),a($e,o0),a($e,lf),a(lf,l0),p(e,tv,o),p(e,Ut,o),a(Ut,n0),a(Ut,el),a(el,p0),a(Ut,h0),p(e,av,o),u(Hs,e,o),p(e,sv,o),u(Fs,e,o),p(e,rv,o),p(e,tl,o),a(tl,f0),p(e,iv,o),p(e,ke,o),a(ke,zt),a(zt,nf),u(Ws,nf,null),a(ke,c0),a(ke,pf),a(pf,m0),p(e,ov,o),p(e,al,o),a(al,d0),p(e,lv,o),p(e,N,o),a(N,u0),a(N,sl),a(sl,v0),a(N,w0),a(N,hf),a(hf,g0),a(N,_0),a(N,rl),a(rl,b0),a(N,y0),p(e,nv,o),u(Rs,e,o),p(e,pv,o),u(Xs,e,o),p(e,hv,o),p(e,D,o),a(D,E0),a(D,ff),a(ff,P0),a(D,$0),a(D,cf),a(cf,k0),a(D,A0),a(D,mf),a(mf,T0),a(D,j0),a(D,df),a(df,x0),a(D,D0),p(e,fv,o),p(e,il,o),a(il,I0),p(e,cv,o),p(e,Ae,o),a(Ae,St),a(St,uf),u(Ys,uf,null),a(Ae,G0),a(Ae,vf),a(vf,U0),p(e,mv,o),p(e,ol,o),a(ol,z0),p(e,dv,o),p(e,Nt,o),a(Nt,S0),a(Nt,Qs),a(Qs,N0),a(Nt,C0),p(e,uv,o),p(e,se,o),a(se,L0),a(se,ll),a(ll,M0),a(se,O0),a(se,nl),a(nl,B0),a(se,q0),p(e,vv,o),u(Js,e,o),p(e,wv,o),u(Zs,e,o),p(e,gv,o),p(e,pl,o),a(pl,V0),p(e,_v,o),p(e,Te,o),a(Te,Ct),a(Ct,wf),u(Ks,wf,null),a(Te,H0),a(Te,gf),a(gf,F0),p(e,bv,o),p(e,re,o),a(re,W0),a(re,_f),a(_f,R0),a(re,X0),a(re,bf),a(bf,Y0),a(re,Q0),p(e,yv,o),u(er,e,o),p(e,Ev,o),u(tr,e,o),p(e,Pv,o),p(e,hl,o),a(hl,J0),p(e,$v,o),u(ar,e,o),p(e,kv,o),u(sr,e,o),p(e,Av,o),p(e,fl,o),a(fl,Z0),p(e,Tv,o),p(e,je,o),a(je,Lt),a(Lt,yf),u(rr,yf,null),a(je,K0),a(je,Ef),a(Ef,eE),p(e,jv,o),p(e,cl,o),a(cl,tE),p(e,xv,o),p(e,xe,o),a(xe,Mt),a(Mt,Pf),u(ir,Pf,null),a(xe,aE),a(xe,$f),a($f,sE),p(e,Dv,o),p(e,Ot,o),a(Ot,rE),a(Ot,kf),a(kf,iE),a(Ot,oE),p(e,Iv,o),u(or,e,o),p(e,Gv,o),u(lr,e,o),p(e,Uv,o),p(e,ml,o),a(ml,lE),p(e,zv,o),u(nr,e,o),p(e,Sv,o),u(pr,e,o),p(e,Nv,o),p(e,dl,o),a(dl,nE),p(e,Cv,o),p(e,De,o),a(De,Bt),a(Bt,Af),u(hr,Af,null),a(De,pE),a(De,Tf),a(Tf,hE),p(e,Lv,o),p(e,ul,o),a(ul,fE),p(e,Mv,o),p(e,I,o),a(I,cE),a(I,vl),a(vl,mE),a(I,dE),a(I,wl),a(wl,uE),a(I,vE),a(I,fr),a(fr,wE),a(I,gE),a(I,jf),a(jf,_E),a(I,bE),p(e,Ov,o),p(e,gl,o),a(gl,yE),p(e,Bv,o),u(qt,e,o),p(e,qv,o),u(cr,e,o),p(e,Vv,o),p(e,Vt,o),a(Vt,EE),a(Vt,xf),a(xf,PE),a(Vt,$E),p(e,Hv,o),u(mr,e,o),p(e,Fv,o),u(dr,e,o),p(e,Wv,o),p(e,_l,o),a(_l,kE),p(e,Rv,o),u(ur,e,o),p(e,Xv,o),u(vr,e,o),p(e,Yv,o),p(e,bl,o),a(bl,AE),p(e,Qv,o),p(e,yl,o),a(yl,El),p(e,Jv,o),p(e,Ie,o),a(Ie,Ht),a(Ht,Df),u(wr,Df,null),a(Ie,TE),a(Ie,If),a(If,jE),p(e,Zv,o),p(e,ie,o),a(ie,xE),a(ie,Pl),a(Pl,DE),a(ie,IE),a(ie,$l),a($l,GE),a(ie,UE),p(e,Kv,o),u(gr,e,o),p(e,ew,o),p(e,kl,o),a(kl,zE),p(e,tw,o),u(_r,e,o),p(e,aw,o),p(e,k,o),a(k,SE),a(k,br),a(br,Gf),a(Gf,NE),a(k,CE),a(k,Al),a(Al,LE),a(k,ME),a(k,yr),a(yr,Uf),a(Uf,OE),a(k,BE),a(k,zf),a(zf,qE),a(k,VE),a(k,Er),a(Er,Sf),a(Sf,HE),a(k,FE),p(e,sw,o),p(e,Ft,o),a(Ft,WE),a(Ft,Nf),a(Nf,RE),a(Ft,XE),p(e,rw,o),u(Pr,e,o),p(e,iw,o),p(e,Tl,o),a(Tl,YE),p(e,ow,o),p(e,jl,o),a(jl,QE),p(e,lw,o),p(e,Ge,o),a(Ge,Wt),a(Wt,Cf),u($r,Cf,null),a(Ge,JE),a(Ge,Lf),a(Lf,ZE),p(e,nw,o),p(e,xl,o),a(xl,KE),p(e,pw,o),p(e,Rt,o),a(Rt,Mf),a(Mf,e3),a(Rt,t3),a(Rt,Of),a(Of,a3),p(e,hw,o),p(e,Dl,o),a(Dl,s3),p(e,fw,o),p(e,Il,o),a(Il,r3),p(e,cw,o),p(e,Gl,o),a(Gl,i3),p(e,mw,o),p(e,Ue,o),a(Ue,Xt),a(Xt,Bf),u(kr,Bf,null),a(Ue,o3),a(Ue,qf),a(qf,l3),p(e,dw,o),p(e,C,o),a(C,n3),a(C,Vf),a(Vf,p3),a(C,h3),a(C,Hf),a(Hf,f3),a(C,c3),a(C,Ff),a(Ff,m3),a(C,d3),p(e,uw,o),p(e,L,o),a(L,u3),a(L,Ul),a(Ul,v3),a(L,w3),a(L,zl),a(zl,g3),a(L,_3),a(L,Wf),a(Wf,b3),a(L,y3),p(e,vw,o),u(Ar,e,o),p(e,ww,o),p(e,Sl,o),a(Sl,E3),p(e,gw,o),p(e,ze,o),a(ze,Yt),a(Yt,Rf),u(Tr,Rf,null),a(ze,P3),a(ze,Xf),a(Xf,$3),p(e,_w,o),p(e,Qt,o),a(Qt,k3),a(Qt,Nl),a(Nl,A3),a(Qt,T3),p(e,bw,o),p(e,Cl,o),a(Cl,j3),p(e,yw,o),p(e,Se,o),a(Se,Jt),a(Jt,Yf),u(jr,Yf,null),a(Se,x3),a(Se,Qf),a(Qf,D3),p(e,Ew,o),p(e,Ll,o),a(Ll,I3),p(e,Pw,o),p(e,Ne,o),a(Ne,Zt),a(Zt,Jf),u(xr,Jf,null),a(Ne,G3),a(Ne,Zf),a(Zf,U3),p(e,$w,o),p(e,Ml,o),a(Ml,z3),p(e,kw,o),p(e,Ol,o),a(Ol,Bl),a(Bl,S3),a(Bl,Dr),a(Dr,Kf),a(Kf,N3),a(Dr,C3),a(Dr,ec),a(ec,L3),p(e,Aw,o),p(e,ql,o),a(ql,M3),p(e,Tw,o),p(e,Kt,o),a(Kt,tc),a(tc,O3),a(Kt,B3),a(Kt,ac),a(ac,q3),p(e,jw,o),p(e,Ce,o),a(Ce,ea),a(ea,sc),u(Ir,sc,null),a(Ce,V3),a(Ce,rc),a(rc,H3),p(e,xw,o),p(e,Vl,o),a(Vl,F3),p(e,Dw,o),p(e,oe,o),a(oe,ic),a(ic,W3),a(oe,R3),a(oe,oc),a(oc,X3),a(oe,Y3),a(oe,Gr),a(Gr,Q3),a(Gr,Hl),a(Hl,J3),a(Gr,Z3),p(e,Iw,o),p(e,Fl,o),a(Fl,K3),p(e,Gw,o),p(e,E,o),a(E,lc),a(lc,e6),a(E,t6),a(E,nc),a(nc,a6),a(E,s6),a(E,pc),a(pc,r6),a(E,i6),a(E,hc),a(hc,o6),a(E,l6),a(E,fc),a(fc,n6),a(E,p6),a(E,cc),a(cc,h6),a(E,f6),a(E,mc),a(mc,c6),p(e,Uw,o),p(e,Le,o),a(Le,ta),a(ta,dc),u(Ur,dc,null),a(Le,m6),a(Le,uc),a(uc,d6),p(e,zw,o),p(e,Me,o),a(Me,aa),a(aa,vc),u(zr,vc,null),a(Me,u6),a(Me,wc),a(wc,v6),p(e,Sw,o),p(e,Wl,o),a(Wl,w6),p(e,Nw,o),p(e,Sr,o),a(Sr,gc),a(gc,g6),a(Sr,_6),p(e,Cw,o),p(e,Rl,o),a(Rl,b6),p(e,Lw,o),p(e,Xl,o),a(Xl,y6),p(e,Mw,o),p(e,Yl,o),a(Yl,E6),p(e,Ow,o),p(e,Ql,o),a(Ql,P6),p(e,Bw,o),p(e,Jl,o),a(Jl,$6),p(e,qw,o),p(e,Zl,o),a(Zl,k6),p(e,Vw,o),p(e,Nr,o),a(Nr,_c),a(_c,A6),a(Nr,T6),p(e,Hw,o),p(e,Kl,o),a(Kl,j6),p(e,Fw,o),p(e,en,o),a(en,x6),p(e,Ww,o),p(e,Oe,o),a(Oe,sa),a(sa,bc),u(Cr,bc,null),a(Oe,D6),a(Oe,yc),a(yc,I6),p(e,Rw,o),p(e,tn,o),a(tn,G6),p(e,Xw,o),p(e,an,o),a(an,U6),p(e,Yw,o),u(Lr,e,o),p(e,Qw,o),p(e,sn,o),a(sn,z6),p(e,Jw,o),p(e,rn,o),a(rn,S6),p(e,Zw,o),u(Mr,e,o),p(e,Kw,o),p(e,on,o),a(on,N6),p(e,e1,o),u(Or,e,o),p(e,t1,o),p(e,ln,o),a(ln,C6),p(e,a1,o),u(Br,e,o),p(e,s1,o),p(e,le,o),a(le,L6),a(le,Ec),a(Ec,M6),a(le,O6),a(le,Pc),a(Pc,B6),a(le,q6),p(e,r1,o),p(e,nn,o),a(nn,V6),p(e,i1,o),p(e,pn,o),a(pn,H6),p(e,o1,o),p(e,Be,o),a(Be,ra),a(ra,$c),u(qr,$c,null),a(Be,F6),a(Be,kc),a(kc,W6),p(e,l1,o),p(e,Vr,o),a(Vr,Hr),a(Hr,R6),a(Vr,X6),p(e,n1,o),p(e,ia,o),a(ia,Y6),a(ia,Fr),a(Fr,Q6),a(ia,J6),p(e,p1,o),p(e,hn,o),a(hn,Ac),a(Ac,Z6),p(e,h1,o),p(e,M,o),a(M,K6),a(M,Tc),a(Tc,eP),a(M,tP),a(M,jc),a(jc,aP),a(M,sP),a(M,xc),a(xc,rP),a(M,iP),p(e,f1,o),p(e,fn,o),a(fn,oP),p(e,c1,o),p(e,cn,o),a(cn,lP),p(e,m1,o),p(e,oa,o),a(oa,Dc),a(Dc,Wr),a(Wr,Ic),a(Ic,nP),a(Wr,pP),a(Wr,mn),a(mn,hP),a(oa,fP),a(oa,Rr),a(Rr,Xr),a(Xr,Gc),a(Gc,cP),a(Xr,mP),a(Xr,dn),a(dn,dP),a(Rr,uP),a(Rr,Yr),a(Yr,Uc),a(Uc,vP),a(Yr,wP),a(Yr,un),a(un,gP),p(e,d1,o),p(e,vn,o),a(vn,_P),p(e,u1,o),p(e,la,o),a(la,bP),a(la,zc),a(zc,yP),a(la,EP),p(e,v1,o),p(e,wn,o),a(wn,PP),p(e,w1,o),u(Qr,e,o),p(e,g1,o),p(e,T,o),a(T,$P),a(T,Sc),a(Sc,kP),a(T,AP),a(T,Nc),a(Nc,TP),a(T,jP),a(T,Cc),a(Cc,xP),a(T,DP),a(T,Lc),a(Lc,IP),a(T,GP),a(T,Mc),a(Mc,UP),p(e,_1,o),p(e,qe,o),a(qe,na),a(na,Oc),u(Jr,Oc,null),a(qe,zP),a(qe,Bc),a(Bc,SP),p(e,b1,o),p(e,Ve,o),a(Ve,pa),a(pa,qc),u(Zr,qc,null),a(Ve,NP),a(Ve,Vc),a(Vc,CP),p(e,y1,o),p(e,gn,o),a(gn,LP),p(e,E1,o),p(e,Kr,o),a(Kr,MP),a(Kr,_n),a(_n,OP),p(e,P1,o),p(e,He,o),a(He,ha),a(ha,Hc),u(ei,Hc,null),a(He,BP),a(He,Fc),a(Fc,qP),p(e,$1,o),p(e,bn,o),a(bn,VP),p(e,k1,o),p(e,ne,o),a(ne,ti),a(ti,Wc),a(Wc,Rc),a(Rc,HP),a(ti,FP),a(ti,ai),a(ai,WP),a(ai,Xc),a(Xc,RP),a(ai,XP),a(ne,YP),a(ne,si),a(si,Yc),a(Yc,Qc),a(Qc,QP),a(si,JP),a(si,ri),a(ri,ZP),a(ri,Jc),a(Jc,KP),a(ri,e4),a(ne,t4),a(ne,ii),a(ii,Zc),a(Zc,Kc),a(Kc,a4),a(ii,s4),a(ii,oi),a(oi,r4),a(oi,em),a(em,i4),a(oi,o4),p(e,A1,o),p(e,yn,o),a(yn,l4),p(e,T1,o),p(e,li,o),a(li,n4),a(li,ni),a(ni,p4),p(e,j1,o),p(e,Fe,o),a(Fe,fa),a(fa,tm),u(pi,tm,null),a(Fe,h4),a(Fe,am),a(am,f4),p(e,x1,o),p(e,En,o),a(En,c4),p(e,D1,o),p(e,A,o),a(A,sm),a(sm,m4),a(A,d4),a(A,rm),a(rm,u4),a(A,v4),a(A,im),a(im,w4),a(A,g4),a(A,om),a(om,_4),a(A,b4),a(A,lm),a(lm,y4),a(A,E4),a(A,nm),a(nm,P4),p(e,I1,o),p(e,Pn,o),a(Pn,$4),p(e,G1,o),p(e,$n,o),a($n,k4),p(e,U1,o),p(e,kn,o),a(kn,A4),p(e,z1,o),p(e,We,o),a(We,ca),a(ca,pm),u(hi,pm,null),a(We,T4),a(We,hm),a(hm,j4),p(e,S1,o),p(e,ma,o),a(ma,fm),a(fm,x4),a(ma,D4),a(ma,cm),a(cm,I4),p(e,N1,o),p(e,Re,o),a(Re,da),a(da,mm),u(fi,mm,null),a(Re,G4),a(Re,dm),a(dm,U4),p(e,C1,o),p(e,pe,o),a(pe,um),a(um,z4),a(pe,S4),a(pe,An),a(An,N4),a(An,ci),a(ci,C4),a(pe,L4),a(pe,vm),a(vm,M4),p(e,L1,o),p(e,Xe,o),a(Xe,ua),a(ua,wm),u(mi,wm,null),a(Xe,O4),a(Xe,gm),a(gm,B4),p(e,M1,o),p(e,Tn,o),a(Tn,_m),a(_m,q4),p(e,O1,o),p(e,Ye,o),a(Ye,va),a(va,bm),u(di,bm,null),a(Ye,V4),a(Ye,ym),a(ym,H4),p(e,B1,o),p(e,jn,o),a(jn,Em),a(Em,F4),p(e,q1,o),p(e,xn,o),a(xn,W4),p(e,V1,o),p(e,Qe,o),a(Qe,wa),a(wa,Pm),u(ui,Pm,null),a(Qe,R4),a(Qe,$m),a($m,X4),p(e,H1,o),p(e,Dn,o),a(Dn,Y4),p(e,F1,o),p(e,Je,o),a(Je,ga),a(ga,km),u(vi,km,null),a(Je,Q4),a(Je,Am),a(Am,J4),p(e,W1,o),p(e,In,o),a(In,Z4),p(e,R1,o),p(e,Ze,o),a(Ze,_a),a(_a,Tm),u(wi,Tm,null),a(Ze,K4),a(Ze,ba),a(ba,jm),a(jm,e5),a(ba,t5),a(ba,xm),a(xm,a5),a(ba,s5),p(e,X1,o),p(e,Gn,o),a(Gn,r5),p(e,Y1,o),p(e,Ke,o),a(Ke,ya),a(ya,Dm),u(gi,Dm,null),a(Ke,i5),a(Ke,Im),a(Im,o5),p(e,Q1,o),p(e,Un,o),a(Un,l5),p(e,J1,o),p(e,O,o),a(O,_i),a(_i,n5),a(_i,Gm),a(Gm,p5),a(_i,h5),a(O,f5),a(O,bi),a(bi,c5),a(bi,Um),a(Um,m5),a(bi,d5),a(O,u5),a(O,yi),a(yi,v5),a(yi,zm),a(zm,w5),a(yi,g5),a(O,_5),a(O,Sm),a(Sm,b5),p(e,Z1,o),p(e,zn,o),a(zn,y5),p(e,K1,o),p(e,Sn,o),a(Sn,Nn),p(e,eg,o),p(e,Ea,o),a(Ea,E5),a(Ea,Ei),a(Ei,P5),a(Ea,$5),p(e,tg,o),p(e,Cn,o),a(Cn,k5),p(e,ag,o),p(e,et,o),a(et,Pa),a(Pa,Nm),u(Pi,Nm,null),a(et,A5),a(et,Cm),a(Cm,T5),p(e,sg,o),p(e,Ln,o),a(Ln,j5),p(e,rg,o),p(e,Mn,o),a(Mn,x5),p(e,ig,o),p(e,B,o),a(B,Lm),a(Lm,D5),a(B,I5),a(B,Mm),a(Mm,G5),a(B,U5),a(B,$i),a($i,z5),a($i,Om),a(Om,S5),a($i,N5),a(B,C5),a(B,Bm),a(Bm,L5),p(e,og,o),p(e,On,o),a(On,M5),p(e,lg,o),p(e,$a,o),a($a,O5),a($a,qm),a(qm,B5),a($a,q5),p(e,ng,o),p(e,Bn,o),a(Bn,V5),p(e,pg,o),u(ki,e,o),p(e,hg,o),p(e,ka,o),a(ka,H5),a(ka,Vm),a(Vm,F5),a(ka,W5),p(e,fg,o),p(e,Aa,o),a(Aa,Hm),a(Hm,R),a(R,qn),a(qn,R5),a(R,X5),a(R,Vn),a(Vn,Y5),a(R,Q5),a(R,Hn),a(Hn,J5),a(R,Z5),a(R,Fn),a(Fn,K5),a(Aa,e$),a(Aa,X),a(X,Y),a(Y,Wn),a(Wn,t$),a(Y,a$),a(Y,Rn),a(Rn,s$),a(Y,r$),a(Y,Xn),a(Xn,i$),a(Y,o$),a(Y,Yn),a(Yn,l$),a(X,n$),a(X,Q),a(Q,Qn),a(Qn,p$),a(Q,h$),a(Q,Jn),a(Jn,f$),a(Q,c$),a(Q,Zn),a(Zn,m$),a(Q,d$),a(Q,Kn),a(Kn,u$),a(X,v$),a(X,J),a(J,ep),a(ep,w$),a(J,g$),a(J,tp),a(tp,_$),a(J,b$),a(J,ap),a(ap,y$),a(J,E$),a(J,sp),a(sp,P$),a(X,$$),a(X,Z),a(Z,rp),a(rp,k$),a(Z,A$),a(Z,ip),a(ip,T$),a(Z,j$),a(Z,op),a(op,x$),a(Z,D$),a(Z,lp),a(lp,I$),p(e,cg,o),p(e,Ta,o),a(Ta,G$),a(Ta,Fm),a(Fm,U$),a(Ta,z$),p(e,mg,o),p(e,np,o),a(np,S$),p(e,dg,o),p(e,pp,o),a(pp,N$),p(e,ug,o),p(e,ja,o),a(ja,C$),a(ja,Ai),a(Ai,L$),a(ja,M$),p(e,vg,o),p(e,hp,o),a(hp,O$),p(e,wg,o),p(e,xa,o),a(xa,fp),a(fp,B$),a(fp,Ti),a(Ti,q$),a(xa,V$),a(xa,cp),a(cp,H$),a(cp,ji),a(ji,F$),p(e,gg,o),p(e,he,o),a(he,W$),a(he,xi),a(xi,R$),a(he,X$),a(he,Di),a(Di,Y$),a(he,Q$),p(e,_g,o),p(e,tt,o),a(tt,Da),a(Da,Wm),u(Ii,Wm,null),a(tt,J$),a(tt,Rm),a(Rm,Z$),p(e,bg,o),p(e,fe,o),a(fe,K$),a(fe,Xm),a(Xm,ek),a(fe,tk),a(fe,Gi),a(Gi,ak),a(fe,sk),p(e,yg,o),p(e,mp,o),a(mp,rk),p(e,Eg,o),p(e,at,o),a(at,Ia),a(Ia,Ym),u(Ui,Ym,null),a(at,ik),a(at,Qm),a(Qm,ok),p(e,Pg,o),p(e,dp,o),a(dp,lk),p(e,$g,o),p(e,up,o),a(up,nk),p(e,kg,o),p(e,vp,o),a(vp,pk),p(e,Ag,o),p(e,Ga,o),a(Ga,hk),a(Ga,Jm),a(Jm,fk),a(Ga,ck),p(e,Tg,o),p(e,st,o),a(st,Ua),a(Ua,Zm),u(zi,Zm,null),a(st,mk),a(st,Km),a(Km,dk),p(e,jg,o),p(e,ce,o),a(ce,uk),a(ce,ed),a(ed,vk),a(ce,wk),a(ce,td),a(td,gk),a(ce,_k),p(e,xg,o),p(e,wp,o),a(wp,bk),p(e,Dg,o),p(e,gp,o),a(gp,yk),p(e,Ig,o),p(e,_p,o),a(_p,Ek),p(e,Gg,o),p(e,bp,o),a(bp,Pk),p(e,Ug,o),p(e,za,o),a(za,$k),a(za,ad),a(ad,kk),a(za,Ak),p(e,zg,o),p(e,yp,o),a(yp,Tk),p(e,Sg,o),u(Si,e,o),p(e,Ng,o),p(e,Ni,o),a(Ni,jk),a(Ni,sd),a(sd,xk),p(e,Cg,o),p(e,Ep,o),a(Ep,Dk),p(e,Lg,o),u(Ci,e,o),p(e,Mg,o),p(e,Pp,o),a(Pp,Ik),p(e,Og,o),p(e,me,o),a(me,Gk),a(me,Li),a(Li,Uk),a(me,zk),a(me,Mi),a(Mi,Sk),a(me,Nk),p(e,Bg,o),p(e,rt,o),a(rt,Sa),a(Sa,rd),u(Oi,rd,null),a(rt,Ck),a(rt,id),a(id,Lk),p(e,qg,o),p(e,Na,o),a(Na,Mk),a(Na,$p),a($p,Ok),a(Na,Bk),p(e,Vg,o),p(e,Ca,o),a(Ca,qk),a(Ca,od),a(od,Vk),a(Ca,Hk),p(e,Hg,o),p(e,it,o),a(it,La),a(La,ld),u(Bi,ld,null),a(it,Fk),a(it,nd),a(nd,Wk),p(e,Fg,o),p(e,kp,o),a(kp,Rk),p(e,Wg,o),p(e,Ap,o),a(Ap,Xk),p(e,Rg,o),u(qi,e,o),p(e,Xg,o),p(e,Tp,o),a(Tp,Yk),p(e,Yg,o),p(e,Ma,o),a(Ma,Qk),a(Ma,Vi),a(Vi,Jk),a(Ma,Zk),p(e,Qg,o),p(e,jp,o),a(jp,Kk),p(e,Jg,o),p(e,q,o),a(q,e7),a(q,pd),a(pd,t7),a(q,a7),a(q,hd),a(hd,s7),a(q,r7),a(q,fd),a(fd,i7),a(q,o7),p(e,Zg,o),p(e,de,o),a(de,l7),a(de,cd),a(cd,n7),a(de,p7),a(de,md),a(md,h7),a(de,f7),p(e,Kg,o),p(e,Oa,o),a(Oa,c7),a(Oa,dd),a(dd,m7),a(Oa,d7),p(e,e_,o),p(e,ue,o),a(ue,u7),a(ue,Hi),a(Hi,v7),a(ue,w7),a(ue,Fi),a(Fi,g7),a(ue,_7),p(e,t_,o),p(e,ot,o),a(ot,Ba),a(Ba,ud),u(Wi,ud,null),a(ot,b7),a(ot,vd),a(vd,y7),p(e,a_,o),p(e,ve,o),a(ve,E7),a(ve,Ri),a(Ri,P7),a(ve,$7),a(ve,Xi),a(Xi,k7),a(ve,A7),p(e,s_,o),p(e,qa,o),a(qa,T7),a(qa,wd),a(wd,j7),a(qa,x7),p(e,r_,o),p(e,xp,o),a(xp,D7),p(e,i_,o),p(e,lt,o),a(lt,Va),a(Va,gd),u(Yi,gd,null),a(lt,I7),a(lt,_d),a(_d,G7),p(e,o_,o),p(e,Dp,o),a(Dp,U7),p(e,l_,o),p(e,V,o),a(V,z7),a(V,Qi),a(Qi,S7),a(V,N7),a(V,bd),a(bd,C7),a(V,L7),a(V,yd),a(yd,M7),a(V,O7),p(e,n_,o),p(e,Ip,o),a(Ip,B7),p(e,p_,o),u(Ji,e,o),p(e,h_,o),p(e,Ha,o),a(Ha,q7),a(Ha,Ed),a(Ed,V7),a(Ha,H7),p(e,f_,o),p(e,nt,o),a(nt,Fa),a(Fa,Pd),u(Zi,Pd,null),a(nt,F7),a(nt,$d),a($d,W7),p(e,c_,o),p(e,Gp,o),a(Gp,R7),p(e,m_,o),p(e,we,o),a(we,X7),a(we,Ki),a(Ki,Y7),a(we,Q7),a(we,eo),a(eo,J7),a(we,Z7),p(e,d_,o),p(e,to,o),a(to,ao),a(ao,K7),a(to,e8),p(e,u_,o),p(e,Wa,o),a(Wa,t8),a(Wa,so),a(so,a8),a(Wa,s8),p(e,v_,o),p(e,H,o),a(H,r8),a(H,Up),a(Up,i8),a(H,o8),a(H,ro),a(ro,l8),a(H,n8),a(H,io),a(io,p8),a(H,h8),p(e,w_,o),p(e,pt,o),a(pt,Ra),a(Ra,kd),u(oo,kd,null),a(pt,f8),a(pt,Ad),a(Ad,c8),p(e,g_,o),p(e,ht,o),a(ht,Td),a(Td,m8),a(ht,d8),a(ht,jd),a(jd,u8),a(ht,v8),p(e,__,o),p(e,Xa,o),a(Xa,xd),a(xd,w8),a(Xa,g8),a(Xa,Dd),a(Dd,_8),p(e,b_,o),p(e,zp,o),a(zp,b8),p(e,y_,o),p(e,lo,o),a(lo,no),a(no,y8),a(lo,E8),p(e,E_,o),p(e,Ya,o),a(Ya,Id),a(Id,P8),a(Ya,$8),a(Ya,Sp),a(Sp,k8),a(Sp,po),a(po,Gd),a(Gd,A8),a(po,T8),a(po,ho),a(ho,j8),a(ho,Ud),a(Ud,x8),a(ho,D8),p(e,P_,o),p(e,fo,o),a(fo,co),a(co,I8),a(fo,G8),p(e,$_,o),p(e,Np,o),a(Np,U8),p(e,k_,o),p(e,G,o),a(G,zd),a(zd,z8),a(G,S8),a(G,Sd),a(Sd,N8),a(G,C8),a(G,mo),a(mo,L8),a(mo,Nd),a(Nd,M8),a(mo,O8),a(G,B8),a(G,Cp),a(Cp,q8),a(Cp,Cd),a(Cd,V8),a(G,H8),a(G,Ld),a(Ld,F8),p(e,A_,o),p(e,Lp,o),a(Lp,W8),p(e,T_,o),p(e,Qa,o),a(Qa,R8),a(Qa,uo),a(uo,X8),a(Qa,Y8),p(e,j_,o),p(e,Mp,o),a(Mp,Q8),p(e,x_,o),p(e,Op,o),a(Op,J8),p(e,D_,o),p(e,Bp,o),a(Bp,Z8),p(e,I_,o),p(e,Ja,o),a(Ja,K8),a(Ja,vo),a(vo,e9),a(Ja,t9),p(e,G_,o),p(e,qp,o),a(qp,a9),p(e,U_,o),p(e,Za,o),a(Za,Md),a(Md,ft),a(ft,Vp),a(Vp,s9),a(ft,r9),a(ft,Od),a(Od,i9),a(ft,o9),a(ft,Hp),a(Hp,l9),a(Za,n9),a(Za,ct),a(ct,mt),a(mt,Fp),a(Fp,p9),a(mt,h9),a(mt,Bd),a(Bd,f9),a(mt,c9),a(mt,Wp),a(Wp,m9),a(ct,d9),a(ct,dt),a(dt,Rp),a(Rp,u9),a(dt,v9),a(dt,qd),a(qd,w9),a(dt,g9),a(dt,Xp),a(Xp,_9),a(ct,b9),a(ct,ut),a(ut,Yp),a(Yp,y9),a(ut,E9),a(ut,Vd),a(Vd,P9),a(ut,$9),a(ut,Qp),a(Qp,k9),p(e,z_,o),p(e,Jp,o),a(Jp,A9),p(e,S_,o),p(e,Zp,o),a(Zp,T9),p(e,N_,o),p(e,Kp,o),a(Kp,j9),p(e,C_,o),p(e,eh,o),a(eh,x9),p(e,L_,o),p(e,wo,o),a(wo,Hd),a(Hd,D9),a(wo,I9),p(e,M_,o),u(go,e,o),p(e,O_,o),p(e,j,o),a(j,G9),a(j,Fd),a(Fd,U9),a(j,z9),a(j,Wd),a(Wd,S9),a(j,N9),a(j,Rd),a(Rd,C9),a(j,L9),a(j,Xd),a(Xd,M9),a(j,O9),a(j,Yd),a(Yd,B9),p(e,B_,o),p(e,vt,o),a(vt,Ka),a(Ka,Qd),u(_o,Qd,null),a(vt,q9),a(vt,Jd),a(Jd,V9),p(e,q_,o),p(e,th,o),a(th,H9),p(e,V_,o),p(e,es,o),a(es,ah),a(ah,Zd),a(Zd,F9),a(ah,W9),a(es,R9),a(es,sh),a(sh,Kd),a(Kd,X9),a(sh,Y9),p(e,H_,o),p(e,wt,o),a(wt,ts),a(ts,eu),u(bo,eu,null),a(wt,Q9),a(wt,tu),a(tu,J9),p(e,F_,o),p(e,gt,o),a(gt,as),a(as,au),u(yo,au,null),a(gt,Z9),a(gt,su),a(su,K9),p(e,W_,o),p(e,rh,o),a(rh,ru),a(ru,iu),a(iu,eA),p(e,R_,o),p(e,ss,o),a(ss,ou),a(ou,tA),a(ss,aA),a(ss,lu),a(lu,sA),p(e,X_,o),p(e,ih,o),a(ih,oh),a(oh,nu),a(nu,rA),a(oh,iA),p(e,Y_,o),p(e,lh,o),a(lh,Eo),a(Eo,oA),p(e,Q_,o),p(e,rs,o),a(rs,lA),a(rs,Po),a(Po,nA),a(rs,pA),p(e,J_,o),p(e,_t,o),a(_t,is),a(is,pu),u($o,pu,null),a(_t,hA),a(_t,hu),a(hu,fA),p(e,Z_,o),p(e,nh,o),a(nh,ko),a(ko,ph),a(ph,Ao),a(Ao,cA),a(ph,mA),a(ko,dA),a(ko,fu),a(fu,uA),p(e,K_,o),p(e,bt,o),a(bt,os),a(os,cu),u(To,cu,null),a(bt,vA),a(bt,mu),a(mu,wA),p(e,eb,o),p(e,yt,o),a(yt,gA),a(yt,du),a(du,_A),a(yt,bA),a(yt,jo),a(jo,yA),p(e,tb,o),p(e,Et,o),a(Et,ls),a(ls,uu),u(xo,uu,null),a(Et,EA),a(Et,vu),a(vu,PA),p(e,ab,o),p(e,Pt,o),a(Pt,ns),a(ns,wu),u(Do,wu,null),a(Pt,$A),a(Pt,gu),a(gu,kA),p(e,sb,o),p(e,hh,o),a(hh,AA),p(e,rb,o),p(e,fh,o),a(fh,TA),p(e,ib,o),p(e,ch,o),a(ch,jA),p(e,ob,o),p(e,mh,o),a(mh,dh),p(e,lb,o),p(e,ps,o),a(ps,xA),a(ps,Io),a(Io,DA),a(ps,IA),p(e,nb,o),p(e,uh,o),a(uh,GA),p(e,pb,o),p(e,vh,o),a(vh,UA),p(e,hb,o),p(e,wh,o),a(wh,zA),p(e,fb,o),p(e,gh,o),a(gh,SA),p(e,cb,o),p(e,ge,o),a(ge,_u),a(_u,Go),a(Go,NA),a(ge,CA),a(ge,bu),a(bu,Uo),a(Uo,LA),a(ge,MA),a(ge,yu),a(yu,zo),a(zo,OA),p(e,mb,o),p(e,P,o),a(P,BA),a(P,So),a(So,qA),a(P,VA),a(P,No),a(No,HA),a(P,FA),a(P,Co),a(Co,WA),a(P,RA),a(P,Lo),a(Lo,XA),a(P,YA),a(P,Mo),a(Mo,QA),a(P,JA),a(P,_h),a(_h,ZA),a(P,KA),p(e,db,o),p(e,$t,o),a($t,hs),a(hs,Eu),u(Oo,Eu,null),a($t,eT),a($t,Pu),a(Pu,tT),p(e,ub,o),p(e,fs,o),a(fs,aT),a(fs,Bo),a(Bo,sT),a(fs,rT),p(e,vb,o),p(e,cs,o),a(cs,iT),a(cs,$u),a($u,oT),a(cs,lT),p(e,wb,o),p(e,bh,o),a(bh,nT),p(e,gb,o),p(e,_e,o),a(_e,pT),a(_e,qo),a(qo,hT),a(_e,fT),a(_e,Vo),a(Vo,cT),a(_e,mT),p(e,_b,o),p(e,yh,o),a(yh,dT),p(e,bb,o),p(e,kt,o),a(kt,ms),a(ms,ku),u(Ho,ku,null),a(kt,uT),a(kt,Au),a(Au,vT),p(e,yb,o),p(e,Eh,o),a(Eh,wT),p(e,Eb,o),p(e,Ph,o),a(Ph,gT),Pb=!0},p(e,[o]){const Fo={};o&2&&(Fo.$$scope={dirty:o,ctx:e}),It.$set(Fo);const Tu={};o&2&&(Tu.$$scope={dirty:o,ctx:e}),qt.$set(Tu)},i(e){Pb||(v(xs.$$.fragment,e),v(Ds.$$.fragment,e),v(Gs.$$.fragment,e),v(zs.$$.fragment,e),v(Ss.$$.fragment,e),v(Ns.$$.fragment,e),v(Cs.$$.fragment,e),v(Ls.$$.fragment,e),v(Ms.$$.fragment,e),v(Os.$$.fragment,e),v(Bs.$$.fragment,e),v(qs.$$.fragment,e),v(It.$$.fragment,e),v(Vs.$$.fragment,e),v(Hs.$$.fragment,e),v(Fs.$$.fragment,e),v(Ws.$$.fragment,e),v(Rs.$$.fragment,e),v(Xs.$$.fragment,e),v(Ys.$$.fragment,e),v(Js.$$.fragment,e),v(Zs.$$.fragment,e),v(Ks.$$.fragment,e),v(er.$$.fragment,e),v(tr.$$.fragment,e),v(ar.$$.fragment,e),v(sr.$$.fragment,e),v(rr.$$.fragment,e),v(ir.$$.fragment,e),v(or.$$.fragment,e),v(lr.$$.fragment,e),v(nr.$$.fragment,e),v(pr.$$.fragment,e),v(hr.$$.fragment,e),v(qt.$$.fragment,e),v(cr.$$.fragment,e),v(mr.$$.fragment,e),v(dr.$$.fragment,e),v(ur.$$.fragment,e),v(vr.$$.fragment,e),v(wr.$$.fragment,e),v(gr.$$.fragment,e),v(_r.$$.fragment,e),v(Pr.$$.fragment,e),v($r.$$.fragment,e),v(kr.$$.fragment,e),v(Ar.$$.fragment,e),v(Tr.$$.fragment,e),v(jr.$$.fragment,e),v(xr.$$.fragment,e),v(Ir.$$.fragment,e),v(Ur.$$.fragment,e),v(zr.$$.fragment,e),v(Cr.$$.fragment,e),v(Lr.$$.fragment,e),v(Mr.$$.fragment,e),v(Or.$$.fragment,e),v(Br.$$.fragment,e),v(qr.$$.fragment,e),v(Qr.$$.fragment,e),v(Jr.$$.fragment,e),v(Zr.$$.fragment,e),v(ei.$$.fragment,e),v(pi.$$.fragment,e),v(hi.$$.fragment,e),v(fi.$$.fragment,e),v(mi.$$.fragment,e),v(di.$$.fragment,e),v(ui.$$.fragment,e),v(vi.$$.fragment,e),v(wi.$$.fragment,e),v(gi.$$.fragment,e),v(Pi.$$.fragment,e),v(ki.$$.fragment,e),v(Ii.$$.fragment,e),v(Ui.$$.fragment,e),v(zi.$$.fragment,e),v(Si.$$.fragment,e),v(Ci.$$.fragment,e),v(Oi.$$.fragment,e),v(Bi.$$.fragment,e),v(qi.$$.fragment,e),v(Wi.$$.fragment,e),v(Yi.$$.fragment,e),v(Ji.$$.fragment,e),v(Zi.$$.fragment,e),v(oo.$$.fragment,e),v(go.$$.fragment,e),v(_o.$$.fragment,e),v(bo.$$.fragment,e),v(yo.$$.fragment,e),v($o.$$.fragment,e),v(To.$$.fragment,e),v(xo.$$.fragment,e),v(Do.$$.fragment,e),v(Oo.$$.fragment,e),v(Ho.$$.fragment,e),Pb=!0)},o(e){w(xs.$$.fragment,e),w(Ds.$$.fragment,e),w(Gs.$$.fragment,e),w(zs.$$.fragment,e),w(Ss.$$.fragment,e),w(Ns.$$.fragment,e),w(Cs.$$.fragment,e),w(Ls.$$.fragment,e),w(Ms.$$.fragment,e),w(Os.$$.fragment,e),w(Bs.$$.fragment,e),w(qs.$$.fragment,e),w(It.$$.fragment,e),w(Vs.$$.fragment,e),w(Hs.$$.fragment,e),w(Fs.$$.fragment,e),w(Ws.$$.fragment,e),w(Rs.$$.fragment,e),w(Xs.$$.fragment,e),w(Ys.$$.fragment,e),w(Js.$$.fragment,e),w(Zs.$$.fragment,e),w(Ks.$$.fragment,e),w(er.$$.fragment,e),w(tr.$$.fragment,e),w(ar.$$.fragment,e),w(sr.$$.fragment,e),w(rr.$$.fragment,e),w(ir.$$.fragment,e),w(or.$$.fragment,e),w(lr.$$.fragment,e),w(nr.$$.fragment,e),w(pr.$$.fragment,e),w(hr.$$.fragment,e),w(qt.$$.fragment,e),w(cr.$$.fragment,e),w(mr.$$.fragment,e),w(dr.$$.fragment,e),w(ur.$$.fragment,e),w(vr.$$.fragment,e),w(wr.$$.fragment,e),w(gr.$$.fragment,e),w(_r.$$.fragment,e),w(Pr.$$.fragment,e),w($r.$$.fragment,e),w(kr.$$.fragment,e),w(Ar.$$.fragment,e),w(Tr.$$.fragment,e),w(jr.$$.fragment,e),w(xr.$$.fragment,e),w(Ir.$$.fragment,e),w(Ur.$$.fragment,e),w(zr.$$.fragment,e),w(Cr.$$.fragment,e),w(Lr.$$.fragment,e),w(Mr.$$.fragment,e),w(Or.$$.fragment,e),w(Br.$$.fragment,e),w(qr.$$.fragment,e),w(Qr.$$.fragment,e),w(Jr.$$.fragment,e),w(Zr.$$.fragment,e),w(ei.$$.fragment,e),w(pi.$$.fragment,e),w(hi.$$.fragment,e),w(fi.$$.fragment,e),w(mi.$$.fragment,e),w(di.$$.fragment,e),w(ui.$$.fragment,e),w(vi.$$.fragment,e),w(wi.$$.fragment,e),w(gi.$$.fragment,e),w(Pi.$$.fragment,e),w(ki.$$.fragment,e),w(Ii.$$.fragment,e),w(Ui.$$.fragment,e),w(zi.$$.fragment,e),w(Si.$$.fragment,e),w(Ci.$$.fragment,e),w(Oi.$$.fragment,e),w(Bi.$$.fragment,e),w(qi.$$.fragment,e),w(Wi.$$.fragment,e),w(Yi.$$.fragment,e),w(Ji.$$.fragment,e),w(Zi.$$.fragment,e),w(oo.$$.fragment,e),w(go.$$.fragment,e),w(_o.$$.fragment,e),w(bo.$$.fragment,e),w(yo.$$.fragment,e),w($o.$$.fragment,e),w(To.$$.fragment,e),w(xo.$$.fragment,e),w(Do.$$.fragment,e),w(Oo.$$.fragment,e),w(Ho.$$.fragment,e),Pb=!1},d(e){t(y),e&&t(x),e&&t($),g(xs),e&&t(Iu),e&&t(Ro),e&&t(Gu),g(Ds,e),e&&t(Uu),e&&t(te),e&&t(zu),g(Gs,e),e&&t(Su),e&&t(ae),e&&t(Nu),e&&t(At),e&&t(Cu),g(zs,e),e&&t(Lu),e&&t(Tt),e&&t(Mu),g(Ss,e),e&&t(Ou),e&&t(Qo),e&&t(Bu),g(Ns,e),e&&t(qu),e&&t(Jo),e&&t(Vu),g(Cs,e),e&&t(Hu),e&&t(Zo),e&&t(Fu),e&&t(Pe),g(Ls),e&&t(Wu),e&&t(xt),e&&t(Ru),g(Ms,e),e&&t(Xu),e&&t(Dt),e&&t(Yu),g(Os,e),e&&t(Qu),g(Bs,e),e&&t(Ju),e&&t(Ko),e&&t(Zu),g(qs,e),e&&t(Ku),g(It,e),e&&t(ev),e&&t($e),g(Vs),e&&t(tv),e&&t(Ut),e&&t(av),g(Hs,e),e&&t(sv),g(Fs,e),e&&t(rv),e&&t(tl),e&&t(iv),e&&t(ke),g(Ws),e&&t(ov),e&&t(al),e&&t(lv),e&&t(N),e&&t(nv),g(Rs,e),e&&t(pv),g(Xs,e),e&&t(hv),e&&t(D),e&&t(fv),e&&t(il),e&&t(cv),e&&t(Ae),g(Ys),e&&t(mv),e&&t(ol),e&&t(dv),e&&t(Nt),e&&t(uv),e&&t(se),e&&t(vv),g(Js,e),e&&t(wv),g(Zs,e),e&&t(gv),e&&t(pl),e&&t(_v),e&&t(Te),g(Ks),e&&t(bv),e&&t(re),e&&t(yv),g(er,e),e&&t(Ev),g(tr,e),e&&t(Pv),e&&t(hl),e&&t($v),g(ar,e),e&&t(kv),g(sr,e),e&&t(Av),e&&t(fl),e&&t(Tv),e&&t(je),g(rr),e&&t(jv),e&&t(cl),e&&t(xv),e&&t(xe),g(ir),e&&t(Dv),e&&t(Ot),e&&t(Iv),g(or,e),e&&t(Gv),g(lr,e),e&&t(Uv),e&&t(ml),e&&t(zv),g(nr,e),e&&t(Sv),g(pr,e),e&&t(Nv),e&&t(dl),e&&t(Cv),e&&t(De),g(hr),e&&t(Lv),e&&t(ul),e&&t(Mv),e&&t(I),e&&t(Ov),e&&t(gl),e&&t(Bv),g(qt,e),e&&t(qv),g(cr,e),e&&t(Vv),e&&t(Vt),e&&t(Hv),g(mr,e),e&&t(Fv),g(dr,e),e&&t(Wv),e&&t(_l),e&&t(Rv),g(ur,e),e&&t(Xv),g(vr,e),e&&t(Yv),e&&t(bl),e&&t(Qv),e&&t(yl),e&&t(Jv),e&&t(Ie),g(wr),e&&t(Zv),e&&t(ie),e&&t(Kv),g(gr,e),e&&t(ew),e&&t(kl),e&&t(tw),g(_r,e),e&&t(aw),e&&t(k),e&&t(sw),e&&t(Ft),e&&t(rw),g(Pr,e),e&&t(iw),e&&t(Tl),e&&t(ow),e&&t(jl),e&&t(lw),e&&t(Ge),g($r),e&&t(nw),e&&t(xl),e&&t(pw),e&&t(Rt),e&&t(hw),e&&t(Dl),e&&t(fw),e&&t(Il),e&&t(cw),e&&t(Gl),e&&t(mw),e&&t(Ue),g(kr),e&&t(dw),e&&t(C),e&&t(uw),e&&t(L),e&&t(vw),g(Ar,e),e&&t(ww),e&&t(Sl),e&&t(gw),e&&t(ze),g(Tr),e&&t(_w),e&&t(Qt),e&&t(bw),e&&t(Cl),e&&t(yw),e&&t(Se),g(jr),e&&t(Ew),e&&t(Ll),e&&t(Pw),e&&t(Ne),g(xr),e&&t($w),e&&t(Ml),e&&t(kw),e&&t(Ol),e&&t(Aw),e&&t(ql),e&&t(Tw),e&&t(Kt),e&&t(jw),e&&t(Ce),g(Ir),e&&t(xw),e&&t(Vl),e&&t(Dw),e&&t(oe),e&&t(Iw),e&&t(Fl),e&&t(Gw),e&&t(E),e&&t(Uw),e&&t(Le),g(Ur),e&&t(zw),e&&t(Me),g(zr),e&&t(Sw),e&&t(Wl),e&&t(Nw),e&&t(Sr),e&&t(Cw),e&&t(Rl),e&&t(Lw),e&&t(Xl),e&&t(Mw),e&&t(Yl),e&&t(Ow),e&&t(Ql),e&&t(Bw),e&&t(Jl),e&&t(qw),e&&t(Zl),e&&t(Vw),e&&t(Nr),e&&t(Hw),e&&t(Kl),e&&t(Fw),e&&t(en),e&&t(Ww),e&&t(Oe),g(Cr),e&&t(Rw),e&&t(tn),e&&t(Xw),e&&t(an),e&&t(Yw),g(Lr,e),e&&t(Qw),e&&t(sn),e&&t(Jw),e&&t(rn),e&&t(Zw),g(Mr,e),e&&t(Kw),e&&t(on),e&&t(e1),g(Or,e),e&&t(t1),e&&t(ln),e&&t(a1),g(Br,e),e&&t(s1),e&&t(le),e&&t(r1),e&&t(nn),e&&t(i1),e&&t(pn),e&&t(o1),e&&t(Be),g(qr),e&&t(l1),e&&t(Vr),e&&t(n1),e&&t(ia),e&&t(p1),e&&t(hn),e&&t(h1),e&&t(M),e&&t(f1),e&&t(fn),e&&t(c1),e&&t(cn),e&&t(m1),e&&t(oa),e&&t(d1),e&&t(vn),e&&t(u1),e&&t(la),e&&t(v1),e&&t(wn),e&&t(w1),g(Qr,e),e&&t(g1),e&&t(T),e&&t(_1),e&&t(qe),g(Jr),e&&t(b1),e&&t(Ve),g(Zr),e&&t(y1),e&&t(gn),e&&t(E1),e&&t(Kr),e&&t(P1),e&&t(He),g(ei),e&&t($1),e&&t(bn),e&&t(k1),e&&t(ne),e&&t(A1),e&&t(yn),e&&t(T1),e&&t(li),e&&t(j1),e&&t(Fe),g(pi),e&&t(x1),e&&t(En),e&&t(D1),e&&t(A),e&&t(I1),e&&t(Pn),e&&t(G1),e&&t($n),e&&t(U1),e&&t(kn),e&&t(z1),e&&t(We),g(hi),e&&t(S1),e&&t(ma),e&&t(N1),e&&t(Re),g(fi),e&&t(C1),e&&t(pe),e&&t(L1),e&&t(Xe),g(mi),e&&t(M1),e&&t(Tn),e&&t(O1),e&&t(Ye),g(di),e&&t(B1),e&&t(jn),e&&t(q1),e&&t(xn),e&&t(V1),e&&t(Qe),g(ui),e&&t(H1),e&&t(Dn),e&&t(F1),e&&t(Je),g(vi),e&&t(W1),e&&t(In),e&&t(R1),e&&t(Ze),g(wi),e&&t(X1),e&&t(Gn),e&&t(Y1),e&&t(Ke),g(gi),e&&t(Q1),e&&t(Un),e&&t(J1),e&&t(O),e&&t(Z1),e&&t(zn),e&&t(K1),e&&t(Sn),e&&t(eg),e&&t(Ea),e&&t(tg),e&&t(Cn),e&&t(ag),e&&t(et),g(Pi),e&&t(sg),e&&t(Ln),e&&t(rg),e&&t(Mn),e&&t(ig),e&&t(B),e&&t(og),e&&t(On),e&&t(lg),e&&t($a),e&&t(ng),e&&t(Bn),e&&t(pg),g(ki,e),e&&t(hg),e&&t(ka),e&&t(fg),e&&t(Aa),e&&t(cg),e&&t(Ta),e&&t(mg),e&&t(np),e&&t(dg),e&&t(pp),e&&t(ug),e&&t(ja),e&&t(vg),e&&t(hp),e&&t(wg),e&&t(xa),e&&t(gg),e&&t(he),e&&t(_g),e&&t(tt),g(Ii),e&&t(bg),e&&t(fe),e&&t(yg),e&&t(mp),e&&t(Eg),e&&t(at),g(Ui),e&&t(Pg),e&&t(dp),e&&t($g),e&&t(up),e&&t(kg),e&&t(vp),e&&t(Ag),e&&t(Ga),e&&t(Tg),e&&t(st),g(zi),e&&t(jg),e&&t(ce),e&&t(xg),e&&t(wp),e&&t(Dg),e&&t(gp),e&&t(Ig),e&&t(_p),e&&t(Gg),e&&t(bp),e&&t(Ug),e&&t(za),e&&t(zg),e&&t(yp),e&&t(Sg),g(Si,e),e&&t(Ng),e&&t(Ni),e&&t(Cg),e&&t(Ep),e&&t(Lg),g(Ci,e),e&&t(Mg),e&&t(Pp),e&&t(Og),e&&t(me),e&&t(Bg),e&&t(rt),g(Oi),e&&t(qg),e&&t(Na),e&&t(Vg),e&&t(Ca),e&&t(Hg),e&&t(it),g(Bi),e&&t(Fg),e&&t(kp),e&&t(Wg),e&&t(Ap),e&&t(Rg),g(qi,e),e&&t(Xg),e&&t(Tp),e&&t(Yg),e&&t(Ma),e&&t(Qg),e&&t(jp),e&&t(Jg),e&&t(q),e&&t(Zg),e&&t(de),e&&t(Kg),e&&t(Oa),e&&t(e_),e&&t(ue),e&&t(t_),e&&t(ot),g(Wi),e&&t(a_),e&&t(ve),e&&t(s_),e&&t(qa),e&&t(r_),e&&t(xp),e&&t(i_),e&&t(lt),g(Yi),e&&t(o_),e&&t(Dp),e&&t(l_),e&&t(V),e&&t(n_),e&&t(Ip),e&&t(p_),g(Ji,e),e&&t(h_),e&&t(Ha),e&&t(f_),e&&t(nt),g(Zi),e&&t(c_),e&&t(Gp),e&&t(m_),e&&t(we),e&&t(d_),e&&t(to),e&&t(u_),e&&t(Wa),e&&t(v_),e&&t(H),e&&t(w_),e&&t(pt),g(oo),e&&t(g_),e&&t(ht),e&&t(__),e&&t(Xa),e&&t(b_),e&&t(zp),e&&t(y_),e&&t(lo),e&&t(E_),e&&t(Ya),e&&t(P_),e&&t(fo),e&&t($_),e&&t(Np),e&&t(k_),e&&t(G),e&&t(A_),e&&t(Lp),e&&t(T_),e&&t(Qa),e&&t(j_),e&&t(Mp),e&&t(x_),e&&t(Op),e&&t(D_),e&&t(Bp),e&&t(I_),e&&t(Ja),e&&t(G_),e&&t(qp),e&&t(U_),e&&t(Za),e&&t(z_),e&&t(Jp),e&&t(S_),e&&t(Zp),e&&t(N_),e&&t(Kp),e&&t(C_),e&&t(eh),e&&t(L_),e&&t(wo),e&&t(M_),g(go,e),e&&t(O_),e&&t(j),e&&t(B_),e&&t(vt),g(_o),e&&t(q_),e&&t(th),e&&t(V_),e&&t(es),e&&t(H_),e&&t(wt),g(bo),e&&t(F_),e&&t(gt),g(yo),e&&t(W_),e&&t(rh),e&&t(R_),e&&t(ss),e&&t(X_),e&&t(ih),e&&t(Y_),e&&t(lh),e&&t(Q_),e&&t(rs),e&&t(J_),e&&t(_t),g($o),e&&t(Z_),e&&t(nh),e&&t(K_),e&&t(bt),g(To),e&&t(eb),e&&t(yt),e&&t(tb),e&&t(Et),g(xo),e&&t(ab),e&&t(Pt),g(Do),e&&t(sb),e&&t(hh),e&&t(rb),e&&t(fh),e&&t(ib),e&&t(ch),e&&t(ob),e&&t(mh),e&&t(lb),e&&t(ps),e&&t(nb),e&&t(uh),e&&t(pb),e&&t(vh),e&&t(hb),e&&t(wh),e&&t(fb),e&&t(gh),e&&t(cb),e&&t(ge),e&&t(mb),e&&t(P),e&&t(db),e&&t($t),g(Oo),e&&t(ub),e&&t(fs),e&&t(vb),e&&t(cs),e&&t(wb),e&&t(bh),e&&t(gb),e&&t(_e),e&&t(_b),e&&t(yh),e&&t(bb),e&&t(kt),g(Ho),e&&t(yb),e&&t(Eh),e&&t(Eb),e&&t(Ph)}}}const qC={local:"performance-and-scalability-how-to-fit-a-bigger-model-and-train-it-faster",sections:[{local:"load-model",title:"Load Model"},{local:"vanilla-training",title:"Vanilla Training"},{local:"gradient-accumulation",title:"Gradient Accumulation"},{local:"gradient-checkpointing",title:"Gradient Checkpointing"},{local:"fp16-training",title:"FP16 Training"},{local:"optimizer",sections:[{local:"adafactor",title:"Adafactor"},{local:"8bit-adam",title:"8-bit Adam"}],title:"Optimizer"},{local:"using-accelerate",title:"Using \u{1F917} Accelerate"},{local:"how-to-scale",title:"How to scale"},{local:"multigpu-training",title:"Multi-GPU Training"},{local:"what-if-my-model-still-does-not-fit",title:"What if my model still does not fit?"},{local:"further-discussions",title:"Further discussions"},{local:"faster-training",title:"Faster Training"},{local:"bigger-models",title:"Bigger Models"},{local:"hardware",sections:[{local:"power-and-cooling",title:"Power and Cooling"},{local:"multigpu-connectivity",title:"Multi-GPU Connectivity"},{local:"nvlink",title:"NVlink"}],title:"Hardware"},{local:"software",sections:[{local:"model-scalability",title:"Model Scalability"},{local:"anatomy-of-models-operations",title:"Anatomy of Model's Operations"},{local:"anatomy-of-models-memory",sections:[{local:"model-weights",title:"Model Weights"},{local:"optimizer-states",title:"Optimizer States"},{local:"gradients",title:"Gradients"},{local:"forward-activations",title:"Forward Activations"},{local:"temporary-memory",title:"Temporary Memory"},{local:"functionalityspecific-memory",title:"Functionality-specific memory"}],title:"Anatomy of Model's Memory"},{local:"forward-vs-backward-execution-speed",title:"`forward` vs `backward` Execution Speed"},{local:"floating-data-types",sections:[{local:"fp16",sections:[{local:"fp16-caching",title:"fp16 caching"},{local:"fp16-inference",title:"fp16 Inference"}],title:"fp16"},{local:"bf16",sections:[{local:"bf16-inference",title:"bf16 Inference"}],title:"bf16"},{local:"tf32",title:"tf32"}],title:"Floating Data Types"},{local:"gradient-accumulation",title:"Gradient Accumulation"},{local:"gradient-checkpointing",title:"Gradient Checkpointing"},{local:"batch-sizes",title:"Batch sizes"},{local:"dp-vs-ddp",title:"DP vs DDP"},{local:"dataloader",title:"DataLoader"},{local:"optimizers",sections:[{local:"faster-optimizers",title:"Faster optimizers"},{local:"leaner-optimizers",title:"Leaner Optimizers"},{local:"faster-optimizer",title:"Faster optimizer"}],title:"Optimizers"},{local:"sparsity",sections:[{local:"mixture-of-experts",title:"Mixture of Experts"}],title:"Sparsity"},{local:"efficient-software-prebuilds",title:"Efficient Software Prebuilds"}],title:"Software"},{local:"contribute",title:"Contribute"}],title:"Performance and Scalability: How To Fit a Bigger Model and Train It Faster"};function VC(Wo){return CC(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class YC extends UC{constructor(y){super();zC(this,y,VC,BC,SC,{})}}export{YC as default,qC as metadata};
