import{S as Sf,i as Hf,s as Bf,e as s,k as l,w as b,t as n,M as Wf,c as a,d as t,m as c,a as i,x as T,h as r,b as d,F as e,g as p,y as k,q as P,o as w,B as E,v as Uf}from"../../chunks/vendor-6b77c823.js";import{T as At}from"../../chunks/Tip-39098574.js";import{D as W}from"../../chunks/Docstring-1088f2fb.js";import{C as Sn}from"../../chunks/CodeBlock-3a8b25a8.js";import{I as ge}from"../../chunks/IconCopyLink-7a11ce68.js";function Vf(X){let f,x,m,g,R;return{c(){f=s("p"),x=n("Although the recipe for forward pass needs to be defined within this function, one should call the "),m=s("code"),g=n("Module"),R=n(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(v){f=a(v,"P",{});var _=i(f);x=r(_,"Although the recipe for forward pass needs to be defined within this function, one should call the "),m=a(_,"CODE",{});var $=i(m);g=r($,"Module"),$.forEach(t),R=r(_,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),_.forEach(t)},m(v,_){p(v,f,_),e(f,x),e(f,m),e(m,g),e(f,R)},d(v){v&&t(f)}}}function Kf(X){let f,x,m,g,R;return{c(){f=s("p"),x=n("Although the recipe for forward pass needs to be defined within this function, one should call the "),m=s("code"),g=n("Module"),R=n(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(v){f=a(v,"P",{});var _=i(f);x=r(_,"Although the recipe for forward pass needs to be defined within this function, one should call the "),m=a(_,"CODE",{});var $=i(m);g=r($,"Module"),$.forEach(t),R=r(_,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),_.forEach(t)},m(v,_){p(v,f,_),e(f,x),e(f,m),e(m,g),e(f,R)},d(v){v&&t(f)}}}function Yf(X){let f,x,m,g,R;return{c(){f=s("p"),x=n("Although the recipe for forward pass needs to be defined within this function, one should call the "),m=s("code"),g=n("Module"),R=n(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(v){f=a(v,"P",{});var _=i(f);x=r(_,"Although the recipe for forward pass needs to be defined within this function, one should call the "),m=a(_,"CODE",{});var $=i(m);g=r($,"Module"),$.forEach(t),R=r(_,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),_.forEach(t)},m(v,_){p(v,f,_),e(f,x),e(f,m),e(m,g),e(f,R)},d(v){v&&t(f)}}}function Xf(X){let f,x,m,g,R,v,_,$,_e,Z,D,J,I,ee,ve,S,be,he,H,L,te,oe,q,C,ne,U,ue,re,B,Te,fe,z,ke,j,Pe,we,M,Ee,Re,Q,V,N,se;return{c(){f=s("p"),x=n("TF 2.0 models accepts two formats as inputs:"),m=l(),g=s("ul"),R=s("li"),v=n("having all inputs as keyword arguments (like PyTorch models), or"),_=l(),$=s("li"),_e=n("having all inputs as a list, tuple or dict in the first positional arguments."),Z=l(),D=s("p"),J=n("This second option is useful when using "),I=s("code"),ee=n("tf.keras.Model.fit"),ve=n(` method which currently requires having all the
tensors in the first argument of the model call function: `),S=s("code"),be=n("model(inputs)"),he=n("."),H=l(),L=s("p"),te=n(`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),oe=l(),q=s("ul"),C=s("li"),ne=n("a single Tensor with "),U=s("code"),ue=n("input_ids"),re=n(" only and nothing else: "),B=s("code"),Te=n("model(inputs_ids)"),fe=l(),z=s("li"),ke=n(`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),j=s("code"),Pe=n("model([input_ids, attention_mask])"),we=n(" or "),M=s("code"),Ee=n("model([input_ids, attention_mask, token_type_ids])"),Re=l(),Q=s("li"),V=n(`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),N=s("code"),se=n('model({"input_ids": input_ids, "token_type_ids": token_type_ids})')},l(h){f=a(h,"P",{});var y=i(f);x=r(y,"TF 2.0 models accepts two formats as inputs:"),y.forEach(t),m=c(h),g=a(h,"UL",{});var G=i(g);R=a(G,"LI",{});var Se=i(R);v=r(Se,"having all inputs as keyword arguments (like PyTorch models), or"),Se.forEach(t),_=c(G),$=a(G,"LI",{});var je=i($);_e=r(je,"having all inputs as a list, tuple or dict in the first positional arguments."),je.forEach(t),G.forEach(t),Z=c(h),D=a(h,"P",{});var F=i(D);J=r(F,"This second option is useful when using "),I=a(F,"CODE",{});var me=i(I);ee=r(me,"tf.keras.Model.fit"),me.forEach(t),ve=r(F,` method which currently requires having all the
tensors in the first argument of the model call function: `),S=a(F,"CODE",{});var He=i(S);be=r(He,"model(inputs)"),He.forEach(t),he=r(F,"."),F.forEach(t),H=c(h),L=a(h,"P",{});var ae=i(L);te=r(ae,`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),ae.forEach(t),oe=c(h),q=a(h,"UL",{});var O=i(q);C=a(O,"LI",{});var K=i(C);ne=r(K,"a single Tensor with "),U=a(K,"CODE",{});var Be=i(U);ue=r(Be,"input_ids"),Be.forEach(t),re=r(K," only and nothing else: "),B=a(K,"CODE",{});var xe=i(B);Te=r(xe,"model(inputs_ids)"),xe.forEach(t),K.forEach(t),fe=c(O),z=a(O,"LI",{});var Y=i(z);ke=r(Y,`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),j=a(Y,"CODE",{});var We=i(j);Pe=r(We,"model([input_ids, attention_mask])"),We.forEach(t),we=r(Y," or "),M=a(Y,"CODE",{});var Ue=i(M);Ee=r(Ue,"model([input_ids, attention_mask, token_type_ids])"),Ue.forEach(t),Y.forEach(t),Re=c(O),Q=a(O,"LI",{});var A=i(Q);V=r(A,`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),N=a(A,"CODE",{});var Ve=i(N);se=r(Ve,'model({"input_ids": input_ids, "token_type_ids": token_type_ids})'),Ve.forEach(t),A.forEach(t),O.forEach(t)},m(h,y){p(h,f,y),e(f,x),p(h,m,y),p(h,g,y),e(g,R),e(R,v),e(g,_),e(g,$),e($,_e),p(h,Z,y),p(h,D,y),e(D,J),e(D,I),e(I,ee),e(D,ve),e(D,S),e(S,be),e(D,he),p(h,H,y),p(h,L,y),e(L,te),p(h,oe,y),p(h,q,y),e(q,C),e(C,ne),e(C,U),e(U,ue),e(C,re),e(C,B),e(B,Te),e(q,fe),e(q,z),e(z,ke),e(z,j),e(j,Pe),e(z,we),e(z,M),e(M,Ee),e(q,Re),e(q,Q),e(Q,V),e(Q,N),e(N,se)},d(h){h&&t(f),h&&t(m),h&&t(g),h&&t(Z),h&&t(D),h&&t(H),h&&t(L),h&&t(oe),h&&t(q)}}}function Jf(X){let f,x,m,g,R;return{c(){f=s("p"),x=n("Although the recipe for forward pass needs to be defined within this function, one should call the "),m=s("code"),g=n("Module"),R=n(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(v){f=a(v,"P",{});var _=i(f);x=r(_,"Although the recipe for forward pass needs to be defined within this function, one should call the "),m=a(_,"CODE",{});var $=i(m);g=r($,"Module"),$.forEach(t),R=r(_,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),_.forEach(t)},m(v,_){p(v,f,_),e(f,x),e(f,m),e(m,g),e(f,R)},d(v){v&&t(f)}}}function Gf(X){let f,x,m,g,R,v,_,$,_e,Z,D,J,I,ee,ve,S,be,he,H,L,te,oe,q,C,ne,U,ue,re,B,Te,fe,z,ke,j,Pe,we,M,Ee,Re,Q,V,N,se;return{c(){f=s("p"),x=n("TF 2.0 models accepts two formats as inputs:"),m=l(),g=s("ul"),R=s("li"),v=n("having all inputs as keyword arguments (like PyTorch models), or"),_=l(),$=s("li"),_e=n("having all inputs as a list, tuple or dict in the first positional arguments."),Z=l(),D=s("p"),J=n("This second option is useful when using "),I=s("code"),ee=n("tf.keras.Model.fit"),ve=n(` method which currently requires having all the
tensors in the first argument of the model call function: `),S=s("code"),be=n("model(inputs)"),he=n("."),H=l(),L=s("p"),te=n(`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),oe=l(),q=s("ul"),C=s("li"),ne=n("a single Tensor with "),U=s("code"),ue=n("input_ids"),re=n(" only and nothing else: "),B=s("code"),Te=n("model(inputs_ids)"),fe=l(),z=s("li"),ke=n(`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),j=s("code"),Pe=n("model([input_ids, attention_mask])"),we=n(" or "),M=s("code"),Ee=n("model([input_ids, attention_mask, token_type_ids])"),Re=l(),Q=s("li"),V=n(`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),N=s("code"),se=n('model({"input_ids": input_ids, "token_type_ids": token_type_ids})')},l(h){f=a(h,"P",{});var y=i(f);x=r(y,"TF 2.0 models accepts two formats as inputs:"),y.forEach(t),m=c(h),g=a(h,"UL",{});var G=i(g);R=a(G,"LI",{});var Se=i(R);v=r(Se,"having all inputs as keyword arguments (like PyTorch models), or"),Se.forEach(t),_=c(G),$=a(G,"LI",{});var je=i($);_e=r(je,"having all inputs as a list, tuple or dict in the first positional arguments."),je.forEach(t),G.forEach(t),Z=c(h),D=a(h,"P",{});var F=i(D);J=r(F,"This second option is useful when using "),I=a(F,"CODE",{});var me=i(I);ee=r(me,"tf.keras.Model.fit"),me.forEach(t),ve=r(F,` method which currently requires having all the
tensors in the first argument of the model call function: `),S=a(F,"CODE",{});var He=i(S);be=r(He,"model(inputs)"),He.forEach(t),he=r(F,"."),F.forEach(t),H=c(h),L=a(h,"P",{});var ae=i(L);te=r(ae,`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),ae.forEach(t),oe=c(h),q=a(h,"UL",{});var O=i(q);C=a(O,"LI",{});var K=i(C);ne=r(K,"a single Tensor with "),U=a(K,"CODE",{});var Be=i(U);ue=r(Be,"input_ids"),Be.forEach(t),re=r(K," only and nothing else: "),B=a(K,"CODE",{});var xe=i(B);Te=r(xe,"model(inputs_ids)"),xe.forEach(t),K.forEach(t),fe=c(O),z=a(O,"LI",{});var Y=i(z);ke=r(Y,`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),j=a(Y,"CODE",{});var We=i(j);Pe=r(We,"model([input_ids, attention_mask])"),We.forEach(t),we=r(Y," or "),M=a(Y,"CODE",{});var Ue=i(M);Ee=r(Ue,"model([input_ids, attention_mask, token_type_ids])"),Ue.forEach(t),Y.forEach(t),Re=c(O),Q=a(O,"LI",{});var A=i(Q);V=r(A,`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),N=a(A,"CODE",{});var Ve=i(N);se=r(Ve,'model({"input_ids": input_ids, "token_type_ids": token_type_ids})'),Ve.forEach(t),A.forEach(t),O.forEach(t)},m(h,y){p(h,f,y),e(f,x),p(h,m,y),p(h,g,y),e(g,R),e(R,v),e(g,_),e(g,$),e($,_e),p(h,Z,y),p(h,D,y),e(D,J),e(D,I),e(I,ee),e(D,ve),e(D,S),e(S,be),e(D,he),p(h,H,y),p(h,L,y),e(L,te),p(h,oe,y),p(h,q,y),e(q,C),e(C,ne),e(C,U),e(U,ue),e(C,re),e(C,B),e(B,Te),e(q,fe),e(q,z),e(z,ke),e(z,j),e(j,Pe),e(z,we),e(z,M),e(M,Ee),e(q,Re),e(q,Q),e(Q,V),e(Q,N),e(N,se)},d(h){h&&t(f),h&&t(m),h&&t(g),h&&t(Z),h&&t(D),h&&t(H),h&&t(L),h&&t(oe),h&&t(q)}}}function Zf(X){let f,x,m,g,R;return{c(){f=s("p"),x=n("Although the recipe for forward pass needs to be defined within this function, one should call the "),m=s("code"),g=n("Module"),R=n(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(v){f=a(v,"P",{});var _=i(f);x=r(_,"Although the recipe for forward pass needs to be defined within this function, one should call the "),m=a(_,"CODE",{});var $=i(m);g=r($,"Module"),$.forEach(t),R=r(_,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),_.forEach(t)},m(v,_){p(v,f,_),e(f,x),e(f,m),e(m,g),e(f,R)},d(v){v&&t(f)}}}function em(X){let f,x,m,g,R,v,_,$,_e,Z,D,J,I,ee,ve,S,be,he,H,L,te,oe,q,C,ne,U,ue,re,B,Te,fe,z,ke,j,Pe,we,M,Ee,Re,Q,V,N,se;return{c(){f=s("p"),x=n("TF 2.0 models accepts two formats as inputs:"),m=l(),g=s("ul"),R=s("li"),v=n("having all inputs as keyword arguments (like PyTorch models), or"),_=l(),$=s("li"),_e=n("having all inputs as a list, tuple or dict in the first positional arguments."),Z=l(),D=s("p"),J=n("This second option is useful when using "),I=s("code"),ee=n("tf.keras.Model.fit"),ve=n(` method which currently requires having all the
tensors in the first argument of the model call function: `),S=s("code"),be=n("model(inputs)"),he=n("."),H=l(),L=s("p"),te=n(`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),oe=l(),q=s("ul"),C=s("li"),ne=n("a single Tensor with "),U=s("code"),ue=n("input_ids"),re=n(" only and nothing else: "),B=s("code"),Te=n("model(inputs_ids)"),fe=l(),z=s("li"),ke=n(`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),j=s("code"),Pe=n("model([input_ids, attention_mask])"),we=n(" or "),M=s("code"),Ee=n("model([input_ids, attention_mask, token_type_ids])"),Re=l(),Q=s("li"),V=n(`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),N=s("code"),se=n('model({"input_ids": input_ids, "token_type_ids": token_type_ids})')},l(h){f=a(h,"P",{});var y=i(f);x=r(y,"TF 2.0 models accepts two formats as inputs:"),y.forEach(t),m=c(h),g=a(h,"UL",{});var G=i(g);R=a(G,"LI",{});var Se=i(R);v=r(Se,"having all inputs as keyword arguments (like PyTorch models), or"),Se.forEach(t),_=c(G),$=a(G,"LI",{});var je=i($);_e=r(je,"having all inputs as a list, tuple or dict in the first positional arguments."),je.forEach(t),G.forEach(t),Z=c(h),D=a(h,"P",{});var F=i(D);J=r(F,"This second option is useful when using "),I=a(F,"CODE",{});var me=i(I);ee=r(me,"tf.keras.Model.fit"),me.forEach(t),ve=r(F,` method which currently requires having all the
tensors in the first argument of the model call function: `),S=a(F,"CODE",{});var He=i(S);be=r(He,"model(inputs)"),He.forEach(t),he=r(F,"."),F.forEach(t),H=c(h),L=a(h,"P",{});var ae=i(L);te=r(ae,`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),ae.forEach(t),oe=c(h),q=a(h,"UL",{});var O=i(q);C=a(O,"LI",{});var K=i(C);ne=r(K,"a single Tensor with "),U=a(K,"CODE",{});var Be=i(U);ue=r(Be,"input_ids"),Be.forEach(t),re=r(K," only and nothing else: "),B=a(K,"CODE",{});var xe=i(B);Te=r(xe,"model(inputs_ids)"),xe.forEach(t),K.forEach(t),fe=c(O),z=a(O,"LI",{});var Y=i(z);ke=r(Y,`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),j=a(Y,"CODE",{});var We=i(j);Pe=r(We,"model([input_ids, attention_mask])"),We.forEach(t),we=r(Y," or "),M=a(Y,"CODE",{});var Ue=i(M);Ee=r(Ue,"model([input_ids, attention_mask, token_type_ids])"),Ue.forEach(t),Y.forEach(t),Re=c(O),Q=a(O,"LI",{});var A=i(Q);V=r(A,`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),N=a(A,"CODE",{});var Ve=i(N);se=r(Ve,'model({"input_ids": input_ids, "token_type_ids": token_type_ids})'),Ve.forEach(t),A.forEach(t),O.forEach(t)},m(h,y){p(h,f,y),e(f,x),p(h,m,y),p(h,g,y),e(g,R),e(R,v),e(g,_),e(g,$),e($,_e),p(h,Z,y),p(h,D,y),e(D,J),e(D,I),e(I,ee),e(D,ve),e(D,S),e(S,be),e(D,he),p(h,H,y),p(h,L,y),e(L,te),p(h,oe,y),p(h,q,y),e(q,C),e(C,ne),e(C,U),e(U,ue),e(C,re),e(C,B),e(B,Te),e(q,fe),e(q,z),e(z,ke),e(z,j),e(j,Pe),e(z,we),e(z,M),e(M,Ee),e(q,Re),e(q,Q),e(Q,V),e(Q,N),e(N,se)},d(h){h&&t(f),h&&t(m),h&&t(g),h&&t(Z),h&&t(D),h&&t(H),h&&t(L),h&&t(oe),h&&t(q)}}}function tm(X){let f,x,m,g,R;return{c(){f=s("p"),x=n("Although the recipe for forward pass needs to be defined within this function, one should call the "),m=s("code"),g=n("Module"),R=n(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(v){f=a(v,"P",{});var _=i(f);x=r(_,"Although the recipe for forward pass needs to be defined within this function, one should call the "),m=a(_,"CODE",{});var $=i(m);g=r($,"Module"),$.forEach(t),R=r(_,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),_.forEach(t)},m(v,_){p(v,f,_),e(f,x),e(f,m),e(m,g),e(f,R)},d(v){v&&t(f)}}}function om(X){let f,x,m,g,R,v,_,$,_e,Z,D,J,I,ee,ve,S,be,he,H,L,te,oe,q,C,ne,U,ue,re,B,Te,fe,z,ke,j,Pe,we,M,Ee,Re,Q,V,N,se,h,y,G,Se,je,F,me,He,ae,O,K,Be,xe,Y,We,Ue,A,Ve,Hn,hi,ui,Bn,fi,mi,Wn,gi,_i,mo,vi,bi,go,Ti,ki,_o,Pi,wi,Ei,vo,Ri,Un,Di,yi,Ys,ht,Ot,Or,bo,$i,Nr,xi,Xs,Me,To,zi,jr,qi,Fi,Nt,Vn,Ci,Ai,Kn,Oi,Ni,ji,ko,Mi,Yn,Qi,Li,Js,ut,jt,Mr,Po,Ii,Qr,Si,Gs,Qe,wo,Hi,Eo,Bi,Lr,Wi,Ui,Vi,Mt,Xn,Ki,Yi,Jn,Xi,Ji,Gi,Ro,Zi,Gn,ed,td,Zs,ft,Qt,Ir,Do,od,Sr,nd,ea,Le,yo,rd,Hr,sd,ad,Lt,Zn,id,dd,er,ld,cd,pd,$o,hd,tr,ud,fd,ta,mt,It,Br,xo,md,Wr,gd,oa,Ie,zo,_d,qo,vd,Ur,bd,Td,kd,St,or,Pd,wd,nr,Ed,Rd,Dd,Fo,yd,rr,$d,xd,na,gt,Ht,Vr,Co,zd,Kr,qd,ra,ie,Ao,Fd,Yr,Cd,Ad,tt,sr,Od,Nd,ar,jd,Md,ir,Qd,Ld,Id,Oo,Sd,dr,Hd,Bd,Wd,Ze,Ud,Xr,Vd,Kd,Jr,Yd,Xd,Gr,Jd,Gd,Zd,No,sa,_t,Bt,Zr,jo,el,es,tl,aa,de,Mo,ol,Qo,nl,ts,rl,sl,al,ot,lr,il,dl,cr,ll,cl,pr,pl,hl,ul,Lo,fl,hr,ml,gl,_l,et,vl,os,bl,Tl,ns,kl,Pl,rs,wl,El,Rl,ss,Dl,ia,vt,Wt,as,Io,yl,is,$l,da,bt,So,xl,Ho,zl,ur,ql,Fl,la,Tt,Bo,Cl,Wo,Al,fr,Ol,Nl,ca,kt,Uo,jl,Vo,Ml,mr,Ql,Ll,pa,Pt,Ut,ds,Ko,Il,ls,Sl,ha,De,Yo,Hl,cs,Bl,Wl,Xo,Ul,gr,Vl,Kl,Yl,Jo,Xl,Go,Jl,Gl,Zl,ze,Zo,ec,wt,tc,_r,oc,nc,ps,rc,sc,ac,Vt,ic,hs,dc,lc,en,ua,Et,Kt,us,tn,cc,fs,pc,fa,ye,on,hc,ms,uc,fc,nn,mc,vr,gc,_c,vc,rn,bc,sn,Tc,kc,Pc,qe,an,wc,Rt,Ec,br,Rc,Dc,gs,yc,$c,xc,Yt,zc,_s,qc,Fc,dn,ma,Dt,Xt,vs,ln,Cc,bs,Ac,ga,$e,cn,Oc,Ts,Nc,jc,pn,Mc,Tr,Qc,Lc,Ic,hn,Sc,un,Hc,Bc,Wc,Fe,fn,Uc,yt,Vc,kr,Kc,Yc,ks,Xc,Jc,Gc,Jt,Zc,Ps,ep,tp,mn,_a,$t,Gt,ws,gn,op,Es,np,va,le,_n,rp,Rs,sp,ap,vn,ip,Pr,dp,lp,cp,bn,pp,Tn,hp,up,fp,Zt,mp,Ce,kn,gp,xt,_p,wr,vp,bp,Ds,Tp,kp,Pp,eo,wp,ys,Ep,Rp,Pn,ba,zt,to,$s,wn,Dp,xs,yp,Ta,ce,En,$p,zs,xp,zp,Rn,qp,Er,Fp,Cp,Ap,Dn,Op,yn,Np,jp,Mp,oo,Qp,Ae,$n,Lp,qt,Ip,Rr,Sp,Hp,qs,Bp,Wp,Up,no,Vp,Fs,Kp,Yp,xn,ka,Ft,ro,Cs,zn,Xp,As,Jp,Pa,pe,qn,Gp,Os,Zp,eh,Fn,th,Dr,oh,nh,rh,Cn,sh,An,ah,ih,dh,so,lh,Oe,On,ch,Ct,ph,yr,hh,uh,Ns,fh,mh,gh,ao,_h,js,vh,bh,Nn,wa;return v=new ge({}),ee=new ge({}),h=new ge({}),me=new W({props:{name:"class transformers.DPRConfig",anchor:"transformers.DPRConfig",parameters:[{name:"vocab_size",val:" = 30522"},{name:"hidden_size",val:" = 768"},{name:"num_hidden_layers",val:" = 12"},{name:"num_attention_heads",val:" = 12"},{name:"intermediate_size",val:" = 3072"},{name:"hidden_act",val:" = 'gelu'"},{name:"hidden_dropout_prob",val:" = 0.1"},{name:"attention_probs_dropout_prob",val:" = 0.1"},{name:"max_position_embeddings",val:" = 512"},{name:"type_vocab_size",val:" = 2"},{name:"initializer_range",val:" = 0.02"},{name:"layer_norm_eps",val:" = 1e-12"},{name:"pad_token_id",val:" = 0"},{name:"position_embedding_type",val:" = 'absolute'"},{name:"projection_dim",val:": int = 0"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.DPRConfig.vocab_size",description:`<strong>vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 30522) &#x2014;
Vocabulary size of the DPR model. Defines the different tokens that can be represented by the <em>inputs_ids</em>
passed to the forward method of <a href="/docs/transformers/pr_16900/en/model_doc/bert#transformers.BertModel">BertModel</a>.`,name:"vocab_size"},{anchor:"transformers.DPRConfig.hidden_size",description:`<strong>hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 768) &#x2014;
Dimensionality of the encoder layers and the pooler layer.`,name:"hidden_size"},{anchor:"transformers.DPRConfig.num_hidden_layers",description:`<strong>num_hidden_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of hidden layers in the Transformer encoder.`,name:"num_hidden_layers"},{anchor:"transformers.DPRConfig.num_attention_heads",description:`<strong>num_attention_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of attention heads for each attention layer in the Transformer encoder.`,name:"num_attention_heads"},{anchor:"transformers.DPRConfig.intermediate_size",description:`<strong>intermediate_size</strong> (<code>int</code>, <em>optional</em>, defaults to 3072) &#x2014;
Dimensionality of the &#x201C;intermediate&#x201D; (i.e., feed-forward) layer in the Transformer encoder.`,name:"intermediate_size"},{anchor:"transformers.DPRConfig.hidden_act",description:`<strong>hidden_act</strong> (<code>str</code> or <code>function</code>, <em>optional</em>, defaults to <code>&quot;gelu&quot;</code>) &#x2014;
The non-linear activation function (function or string) in the encoder and pooler. If string, <code>&quot;gelu&quot;</code>,
<code>&quot;relu&quot;</code>, <code>&quot;silu&quot;</code> and <code>&quot;gelu_new&quot;</code> are supported.`,name:"hidden_act"},{anchor:"transformers.DPRConfig.hidden_dropout_prob",description:`<strong>hidden_dropout_prob</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout probability for all fully connected layers in the embeddings, encoder, and pooler.`,name:"hidden_dropout_prob"},{anchor:"transformers.DPRConfig.attention_probs_dropout_prob",description:`<strong>attention_probs_dropout_prob</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout ratio for the attention probabilities.`,name:"attention_probs_dropout_prob"},{anchor:"transformers.DPRConfig.max_position_embeddings",description:`<strong>max_position_embeddings</strong> (<code>int</code>, <em>optional</em>, defaults to 512) &#x2014;
The maximum sequence length that this model might ever be used with. Typically set this to something large
just in case (e.g., 512 or 1024 or 2048).`,name:"max_position_embeddings"},{anchor:"transformers.DPRConfig.type_vocab_size",description:`<strong>type_vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 2) &#x2014;
The vocabulary size of the <em>token_type_ids</em> passed into <a href="/docs/transformers/pr_16900/en/model_doc/bert#transformers.BertModel">BertModel</a>.`,name:"type_vocab_size"},{anchor:"transformers.DPRConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.DPRConfig.layer_norm_eps",description:`<strong>layer_norm_eps</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-12) &#x2014;
The epsilon used by the layer normalization layers.`,name:"layer_norm_eps"},{anchor:"transformers.DPRConfig.position_embedding_type",description:`<strong>position_embedding_type</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;absolute&quot;</code>) &#x2014;
Type of position embedding. Choose one of <code>&quot;absolute&quot;</code>, <code>&quot;relative_key&quot;</code>, <code>&quot;relative_key_query&quot;</code>. For
positional embeddings use <code>&quot;absolute&quot;</code>. For more information on <code>&quot;relative_key&quot;</code>, please refer to
<a href="https://arxiv.org/abs/1803.02155" rel="nofollow">Self-Attention with Relative Position Representations (Shaw et al.)</a>.
For more information on <code>&quot;relative_key_query&quot;</code>, please refer to <em>Method 4</em> in <a href="https://arxiv.org/abs/2009.13658" rel="nofollow">Improve Transformer Models
with Better Relative Position Embeddings (Huang et al.)</a>.`,name:"position_embedding_type"},{anchor:"transformers.DPRConfig.projection_dim",description:`<strong>projection_dim</strong> (<code>int</code>, <em>optional</em>, defaults to 0) &#x2014;
Dimension of the projection for the context and question encoders. If it is set to zero (default), then no
projection is done.`,name:"projection_dim"}],source:"https://github.com/huggingface/transformers/blob/pr_16900/src/transformers/models/dpr/configuration_dpr.py#L33"}}),bo=new ge({}),To=new W({props:{name:"class transformers.DPRContextEncoderTokenizer",anchor:"transformers.DPRContextEncoderTokenizer",parameters:[{name:"vocab_file",val:""},{name:"do_lower_case",val:" = True"},{name:"do_basic_tokenize",val:" = True"},{name:"never_split",val:" = None"},{name:"unk_token",val:" = '[UNK]'"},{name:"sep_token",val:" = '[SEP]'"},{name:"pad_token",val:" = '[PAD]'"},{name:"cls_token",val:" = '[CLS]'"},{name:"mask_token",val:" = '[MASK]'"},{name:"tokenize_chinese_chars",val:" = True"},{name:"strip_accents",val:" = None"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_16900/src/transformers/models/dpr/tokenization_dpr.py#L89"}}),Po=new ge({}),wo=new W({props:{name:"class transformers.DPRContextEncoderTokenizerFast",anchor:"transformers.DPRContextEncoderTokenizerFast",parameters:[{name:"vocab_file",val:" = None"},{name:"tokenizer_file",val:" = None"},{name:"do_lower_case",val:" = True"},{name:"unk_token",val:" = '[UNK]'"},{name:"sep_token",val:" = '[SEP]'"},{name:"pad_token",val:" = '[PAD]'"},{name:"cls_token",val:" = '[CLS]'"},{name:"mask_token",val:" = '[MASK]'"},{name:"tokenize_chinese_chars",val:" = True"},{name:"strip_accents",val:" = None"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_16900/src/transformers/models/dpr/tokenization_dpr_fast.py#L90"}}),Do=new ge({}),yo=new W({props:{name:"class transformers.DPRQuestionEncoderTokenizer",anchor:"transformers.DPRQuestionEncoderTokenizer",parameters:[{name:"vocab_file",val:""},{name:"do_lower_case",val:" = True"},{name:"do_basic_tokenize",val:" = True"},{name:"never_split",val:" = None"},{name:"unk_token",val:" = '[UNK]'"},{name:"sep_token",val:" = '[SEP]'"},{name:"pad_token",val:" = '[PAD]'"},{name:"cls_token",val:" = '[CLS]'"},{name:"mask_token",val:" = '[MASK]'"},{name:"tokenize_chinese_chars",val:" = True"},{name:"strip_accents",val:" = None"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_16900/src/transformers/models/dpr/tokenization_dpr.py#L105"}}),xo=new ge({}),zo=new W({props:{name:"class transformers.DPRQuestionEncoderTokenizerFast",anchor:"transformers.DPRQuestionEncoderTokenizerFast",parameters:[{name:"vocab_file",val:" = None"},{name:"tokenizer_file",val:" = None"},{name:"do_lower_case",val:" = True"},{name:"unk_token",val:" = '[UNK]'"},{name:"sep_token",val:" = '[SEP]'"},{name:"pad_token",val:" = '[PAD]'"},{name:"cls_token",val:" = '[CLS]'"},{name:"mask_token",val:" = '[MASK]'"},{name:"tokenize_chinese_chars",val:" = True"},{name:"strip_accents",val:" = None"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_16900/src/transformers/models/dpr/tokenization_dpr_fast.py#L107"}}),Co=new ge({}),Ao=new W({props:{name:"class transformers.DPRReaderTokenizer",anchor:"transformers.DPRReaderTokenizer",parameters:[{name:"vocab_file",val:""},{name:"do_lower_case",val:" = True"},{name:"do_basic_tokenize",val:" = True"},{name:"never_split",val:" = None"},{name:"unk_token",val:" = '[UNK]'"},{name:"sep_token",val:" = '[SEP]'"},{name:"pad_token",val:" = '[PAD]'"},{name:"cls_token",val:" = '[CLS]'"},{name:"mask_token",val:" = '[MASK]'"},{name:"tokenize_chinese_chars",val:" = True"},{name:"strip_accents",val:" = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.DPRReaderTokenizer.questions",description:`<strong>questions</strong> (<code>str</code> or <code>List[str]</code>) &#x2014;
The questions to be encoded. You can specify one question for many passages. In this case, the question
will be duplicated like <code>[questions] * n_passages</code>. Otherwise you have to specify as many questions as in
<code>titles</code> or <code>texts</code>.`,name:"questions"},{anchor:"transformers.DPRReaderTokenizer.titles",description:`<strong>titles</strong> (<code>str</code> or <code>List[str]</code>) &#x2014;
The passages titles to be encoded. This can be a string or a list of strings if there are several passages.`,name:"titles"},{anchor:"transformers.DPRReaderTokenizer.texts",description:`<strong>texts</strong> (<code>str</code> or <code>List[str]</code>) &#x2014;
The passages texts to be encoded. This can be a string or a list of strings if there are several passages.`,name:"texts"},{anchor:"transformers.DPRReaderTokenizer.padding",description:`<strong>padding</strong> (<code>bool</code>, <code>str</code> or <a href="/docs/transformers/pr_16900/en/internal/file_utils#transformers.utils.PaddingStrategy">PaddingStrategy</a>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Activates and controls padding. Accepts the following values:</p>
<ul>
<li><code>True</code> or <code>&apos;longest&apos;</code>: Pad to the longest sequence in the batch (or no padding if only a single sequence
if provided).</li>
<li><code>&apos;max_length&apos;</code>: Pad to a maximum length specified with the argument <code>max_length</code> or to the maximum
acceptable input length for the model if that argument is not provided.</li>
<li><code>False</code> or <code>&apos;do_not_pad&apos;</code> (default): No padding (i.e., can output a batch with sequences of different
lengths).</li>
</ul>`,name:"padding"},{anchor:"transformers.DPRReaderTokenizer.truncation",description:`<strong>truncation</strong> (<code>bool</code>, <code>str</code> or <a href="/docs/transformers/pr_16900/en/internal/tokenization_utils#transformers.tokenization_utils_base.TruncationStrategy">TruncationStrategy</a>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Activates and controls truncation. Accepts the following values:</p>
<ul>
<li><code>True</code> or <code>&apos;longest_first&apos;</code>: Truncate to a maximum length specified with the argument <code>max_length</code> or to
the maximum acceptable input length for the model if that argument is not provided. This will truncate
token by token, removing a token from the longest sequence in the pair if a pair of sequences (or a batch
of pairs) is provided.</li>
<li><code>&apos;only_first&apos;</code>: Truncate to a maximum length specified with the argument <code>max_length</code> or to the maximum
acceptable input length for the model if that argument is not provided. This will only truncate the first
sequence of a pair if a pair of sequences (or a batch of pairs) is provided.</li>
<li><code>&apos;only_second&apos;</code>: Truncate to a maximum length specified with the argument <code>max_length</code> or to the maximum
acceptable input length for the model if that argument is not provided. This will only truncate the
second sequence of a pair if a pair of sequences (or a batch of pairs) is provided.</li>
<li><code>False</code> or <code>&apos;do_not_truncate&apos;</code> (default): No truncation (i.e., can output batch with sequence lengths
greater than the model maximum admissible input size).</li>
</ul>`,name:"truncation"},{anchor:"transformers.DPRReaderTokenizer.max_length",description:`<strong>max_length</strong> (<code>int</code>, <em>optional</em>) &#x2014;
Controls the maximum length to use by one of the truncation/padding parameters.</p>
<p>If left unset or set to <code>None</code>, this will use the predefined model maximum length if a maximum length
is required by one of the truncation/padding parameters. If the model has no specific maximum input
length (like XLNet) truncation/padding to a maximum length will be deactivated.`,name:"max_length"},{anchor:"transformers.DPRReaderTokenizer.return_tensors",description:`<strong>return_tensors</strong> (<code>str</code> or <a href="/docs/transformers/pr_16900/en/internal/file_utils#transformers.TensorType">TensorType</a>, <em>optional</em>) &#x2014;
If set, will return tensors instead of list of python integers. Acceptable values are:</p>
<ul>
<li><code>&apos;tf&apos;</code>: Return TensorFlow <code>tf.constant</code> objects.</li>
<li><code>&apos;pt&apos;</code>: Return PyTorch <code>torch.Tensor</code> objects.</li>
<li><code>&apos;np&apos;</code>: Return Numpy <code>np.ndarray</code> objects.</li>
</ul>`,name:"return_tensors"},{anchor:"transformers.DPRReaderTokenizer.return_attention_mask",description:`<strong>return_attention_mask</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attention mask. If not set, will return the attention mask according to the
specific tokenizer&#x2019;s default, defined by the <code>return_outputs</code> attribute.</p>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"return_attention_mask"}],source:"https://github.com/huggingface/transformers/blob/pr_16900/src/transformers/models/dpr/tokenization_dpr.py#L372",returnDescription:`
<p>A dictionary with the following keys:</p>
<ul>
<li><code>input_ids</code>: List of token ids to be fed to a model.</li>
<li><code>attention_mask</code>: List of indices specifying which tokens should be attended to by the model.</li>
</ul>
`,returnType:`
<p><code>Dict[str, List[List[int]]]</code></p>
`}}),No=new Sn({props:{code:"[CLS] <question token ids> [SEP] <titles ids> [SEP] <texts ids>",highlighted:'[CLS] <span class="hljs-tag">&lt;<span class="hljs-name">question</span> <span class="hljs-attr">token</span> <span class="hljs-attr">ids</span>&gt;</span> [SEP] <span class="hljs-tag">&lt;<span class="hljs-name">titles</span> <span class="hljs-attr">ids</span>&gt;</span> [SEP] <span class="hljs-tag">&lt;<span class="hljs-name">texts</span> <span class="hljs-attr">ids</span>&gt;</span>'}}),jo=new ge({}),Mo=new W({props:{name:"class transformers.DPRReaderTokenizerFast",anchor:"transformers.DPRReaderTokenizerFast",parameters:[{name:"vocab_file",val:" = None"},{name:"tokenizer_file",val:" = None"},{name:"do_lower_case",val:" = True"},{name:"unk_token",val:" = '[UNK]'"},{name:"sep_token",val:" = '[SEP]'"},{name:"pad_token",val:" = '[PAD]'"},{name:"cls_token",val:" = '[CLS]'"},{name:"mask_token",val:" = '[MASK]'"},{name:"tokenize_chinese_chars",val:" = True"},{name:"strip_accents",val:" = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.DPRReaderTokenizerFast.questions",description:`<strong>questions</strong> (<code>str</code> or <code>List[str]</code>) &#x2014;
The questions to be encoded. You can specify one question for many passages. In this case, the question
will be duplicated like <code>[questions] * n_passages</code>. Otherwise you have to specify as many questions as in
<code>titles</code> or <code>texts</code>.`,name:"questions"},{anchor:"transformers.DPRReaderTokenizerFast.titles",description:`<strong>titles</strong> (<code>str</code> or <code>List[str]</code>) &#x2014;
The passages titles to be encoded. This can be a string or a list of strings if there are several passages.`,name:"titles"},{anchor:"transformers.DPRReaderTokenizerFast.texts",description:`<strong>texts</strong> (<code>str</code> or <code>List[str]</code>) &#x2014;
The passages texts to be encoded. This can be a string or a list of strings if there are several passages.`,name:"texts"},{anchor:"transformers.DPRReaderTokenizerFast.padding",description:`<strong>padding</strong> (<code>bool</code>, <code>str</code> or <a href="/docs/transformers/pr_16900/en/internal/file_utils#transformers.utils.PaddingStrategy">PaddingStrategy</a>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Activates and controls padding. Accepts the following values:</p>
<ul>
<li><code>True</code> or <code>&apos;longest&apos;</code>: Pad to the longest sequence in the batch (or no padding if only a single sequence
if provided).</li>
<li><code>&apos;max_length&apos;</code>: Pad to a maximum length specified with the argument <code>max_length</code> or to the maximum
acceptable input length for the model if that argument is not provided.</li>
<li><code>False</code> or <code>&apos;do_not_pad&apos;</code> (default): No padding (i.e., can output a batch with sequences of different
lengths).</li>
</ul>`,name:"padding"},{anchor:"transformers.DPRReaderTokenizerFast.truncation",description:`<strong>truncation</strong> (<code>bool</code>, <code>str</code> or <a href="/docs/transformers/pr_16900/en/internal/tokenization_utils#transformers.tokenization_utils_base.TruncationStrategy">TruncationStrategy</a>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Activates and controls truncation. Accepts the following values:</p>
<ul>
<li><code>True</code> or <code>&apos;longest_first&apos;</code>: Truncate to a maximum length specified with the argument <code>max_length</code> or to
the maximum acceptable input length for the model if that argument is not provided. This will truncate
token by token, removing a token from the longest sequence in the pair if a pair of sequences (or a batch
of pairs) is provided.</li>
<li><code>&apos;only_first&apos;</code>: Truncate to a maximum length specified with the argument <code>max_length</code> or to the maximum
acceptable input length for the model if that argument is not provided. This will only truncate the first
sequence of a pair if a pair of sequences (or a batch of pairs) is provided.</li>
<li><code>&apos;only_second&apos;</code>: Truncate to a maximum length specified with the argument <code>max_length</code> or to the maximum
acceptable input length for the model if that argument is not provided. This will only truncate the
second sequence of a pair if a pair of sequences (or a batch of pairs) is provided.</li>
<li><code>False</code> or <code>&apos;do_not_truncate&apos;</code> (default): No truncation (i.e., can output batch with sequence lengths
greater than the model maximum admissible input size).</li>
</ul>`,name:"truncation"},{anchor:"transformers.DPRReaderTokenizerFast.max_length",description:`<strong>max_length</strong> (<code>int</code>, <em>optional</em>) &#x2014;
Controls the maximum length to use by one of the truncation/padding parameters.</p>
<p>If left unset or set to <code>None</code>, this will use the predefined model maximum length if a maximum length
is required by one of the truncation/padding parameters. If the model has no specific maximum input
length (like XLNet) truncation/padding to a maximum length will be deactivated.`,name:"max_length"},{anchor:"transformers.DPRReaderTokenizerFast.return_tensors",description:`<strong>return_tensors</strong> (<code>str</code> or <a href="/docs/transformers/pr_16900/en/internal/file_utils#transformers.TensorType">TensorType</a>, <em>optional</em>) &#x2014;
If set, will return tensors instead of list of python integers. Acceptable values are:</p>
<ul>
<li><code>&apos;tf&apos;</code>: Return TensorFlow <code>tf.constant</code> objects.</li>
<li><code>&apos;pt&apos;</code>: Return PyTorch <code>torch.Tensor</code> objects.</li>
<li><code>&apos;np&apos;</code>: Return Numpy <code>np.ndarray</code> objects.</li>
</ul>`,name:"return_tensors"},{anchor:"transformers.DPRReaderTokenizerFast.return_attention_mask",description:`<strong>return_attention_mask</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attention mask. If not set, will return the attention mask according to the
specific tokenizer&#x2019;s default, defined by the <code>return_outputs</code> attribute.</p>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"return_attention_mask"}],source:"https://github.com/huggingface/transformers/blob/pr_16900/src/transformers/models/dpr/tokenization_dpr_fast.py#L370",returnDescription:`
<p>A dictionary with the following keys:</p>
<ul>
<li><code>input_ids</code>: List of token ids to be fed to a model.</li>
<li><code>attention_mask</code>: List of indices specifying which tokens should be attended to by the model.</li>
</ul>
`,returnType:`
<p><code>Dict[str, List[List[int]]]</code></p>
`}}),Io=new ge({}),So=new W({props:{name:"class transformers.models.dpr.modeling_dpr.DPRContextEncoderOutput",anchor:"transformers.models.dpr.modeling_dpr.DPRContextEncoderOutput",parameters:[{name:"pooler_output",val:": FloatTensor"},{name:"hidden_states",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"}],parametersDescription:[{anchor:"transformers.models.dpr.modeling_dpr.DPRContextEncoderOutput.pooler_output",description:`<strong>pooler_output</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, embeddings_size)</code>) &#x2014;
The DPR encoder outputs the <em>pooler_output</em> that corresponds to the context representation. Last layer
hidden-state of the first token of the sequence (classification token) further processed by a Linear layer.
This output is to be used to embed contexts for nearest neighbors queries with questions embeddings.`,name:"pooler_output"},{anchor:"transformers.models.dpr.modeling_dpr.DPRContextEncoderOutput.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.models.dpr.modeling_dpr.DPRContextEncoderOutput.attentions",description:`<strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"}],source:"https://github.com/huggingface/transformers/blob/pr_16900/src/transformers/models/dpr/modeling_dpr.py#L62"}}),Bo=new W({props:{name:"class transformers.models.dpr.modeling_dpr.DPRQuestionEncoderOutput",anchor:"transformers.models.dpr.modeling_dpr.DPRQuestionEncoderOutput",parameters:[{name:"pooler_output",val:": FloatTensor"},{name:"hidden_states",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"}],parametersDescription:[{anchor:"transformers.models.dpr.modeling_dpr.DPRQuestionEncoderOutput.pooler_output",description:`<strong>pooler_output</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, embeddings_size)</code>) &#x2014;
The DPR encoder outputs the <em>pooler_output</em> that corresponds to the question representation. Last layer
hidden-state of the first token of the sequence (classification token) further processed by a Linear layer.
This output is to be used to embed questions for nearest neighbors queries with context embeddings.`,name:"pooler_output"},{anchor:"transformers.models.dpr.modeling_dpr.DPRQuestionEncoderOutput.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.models.dpr.modeling_dpr.DPRQuestionEncoderOutput.attentions",description:`<strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"}],source:"https://github.com/huggingface/transformers/blob/pr_16900/src/transformers/models/dpr/modeling_dpr.py#L90"}}),Uo=new W({props:{name:"class transformers.DPRReaderOutput",anchor:"transformers.DPRReaderOutput",parameters:[{name:"start_logits",val:": FloatTensor"},{name:"end_logits",val:": FloatTensor = None"},{name:"relevance_logits",val:": FloatTensor = None"},{name:"hidden_states",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"}],parametersDescription:[{anchor:"transformers.DPRReaderOutput.start_logits",description:`<strong>start_logits</strong> (<code>torch.FloatTensor</code> of shape <code>(n_passages, sequence_length)</code>) &#x2014;
Logits of the start index of the span for each passage.`,name:"start_logits"},{anchor:"transformers.DPRReaderOutput.end_logits",description:`<strong>end_logits</strong> (<code>torch.FloatTensor</code> of shape <code>(n_passages, sequence_length)</code>) &#x2014;
Logits of the end index of the span for each passage.`,name:"end_logits"},{anchor:"transformers.DPRReaderOutput.relevance_logits",description:`<strong>relevance_logits</strong> (<code>torch.FloatTensor</code> of shape <code>(n_passages, )</code>) &#x2014;
Outputs of the QA classifier of the DPRReader that corresponds to the scores of each passage to answer the
question, compared to all the other passages.`,name:"relevance_logits"},{anchor:"transformers.DPRReaderOutput.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.DPRReaderOutput.attentions",description:`<strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"}],source:"https://github.com/huggingface/transformers/blob/pr_16900/src/transformers/models/dpr/modeling_dpr.py#L118"}}),Ko=new ge({}),Yo=new W({props:{name:"class transformers.DPRContextEncoder",anchor:"transformers.DPRContextEncoder",parameters:[{name:"config",val:": DPRConfig"}],parametersDescription:[{anchor:"transformers.DPRContextEncoder.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_16900/en/model_doc/dpr#transformers.DPRConfig">DPRConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_16900/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/pr_16900/src/transformers/models/dpr/modeling_dpr.py#L446"}}),Zo=new W({props:{name:"forward",anchor:"transformers.DPRContextEncoder.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"token_type_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"inputs_embeds",val:": typing.Optional[torch.Tensor] = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"}],parametersDescription:[{anchor:"transformers.DPRContextEncoder.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. To match pretraining, DPR input sequence should be
formatted with [CLS] and [SEP] tokens as follows:</p>
<p>(a) For sequence pairs (for a pair title+text for example):`,name:"input_ids"}],source:"https://github.com/huggingface/transformers/blob/pr_16900/src/transformers/models/dpr/modeling_dpr.py#L454",returnDescription:`
<p>A <a
  href="/docs/transformers/pr_16900/en/model_doc/dpr#transformers.models.dpr.modeling_dpr.DPRContextEncoderOutput"
>transformers.models.dpr.modeling_dpr.DPRContextEncoderOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/pr_16900/en/model_doc/dpr#transformers.DPRConfig"
>DPRConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>pooler_output</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, embeddings_size)</code>) \u2014 The DPR encoder outputs the <em>pooler_output</em> that corresponds to the context representation. Last layer
hidden-state of the first token of the sequence (classification token) further processed by a Linear layer.
This output is to be used to embed contexts for nearest neighbors queries with questions embeddings.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/pr_16900/en/model_doc/dpr#transformers.models.dpr.modeling_dpr.DPRContextEncoderOutput"
>transformers.models.dpr.modeling_dpr.DPRContextEncoderOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),Vt=new At({props:{$$slots:{default:[Vf]},$$scope:{ctx:X}}}),en=new Sn({props:{code:`from transformers import DPRContextEncoder, DPRContextEncoderTokenizer

tokenizer = DPRContextEncoderTokenizer.from_pretrained("facebook/dpr-ctx_encoder-single-nq-base")
model = DPRContextEncoder.from_pretrained("facebook/dpr-ctx_encoder-single-nq-base")
input_ids = tokenizer("Hello, is my dog cute ?", return_tensors="pt")["input_ids"]
embeddings = model(input_ids).pooler_output`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DPRContextEncoder, DPRContextEncoderTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = DPRContextEncoderTokenizer.from_pretrained(<span class="hljs-string">&quot;facebook/dpr-ctx_encoder-single-nq-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = DPRContextEncoder.from_pretrained(<span class="hljs-string">&quot;facebook/dpr-ctx_encoder-single-nq-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = tokenizer(<span class="hljs-string">&quot;Hello, is my dog cute ?&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)[<span class="hljs-string">&quot;input_ids&quot;</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>embeddings = model(input_ids).pooler_output`}}),tn=new ge({}),on=new W({props:{name:"class transformers.DPRQuestionEncoder",anchor:"transformers.DPRQuestionEncoder",parameters:[{name:"config",val:": DPRConfig"}],parametersDescription:[{anchor:"transformers.DPRQuestionEncoder.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_16900/en/model_doc/dpr#transformers.DPRConfig">DPRConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_16900/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/pr_16900/src/transformers/models/dpr/modeling_dpr.py#L527"}}),an=new W({props:{name:"forward",anchor:"transformers.DPRQuestionEncoder.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"token_type_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"inputs_embeds",val:": typing.Optional[torch.Tensor] = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"}],parametersDescription:[{anchor:"transformers.DPRQuestionEncoder.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. To match pretraining, DPR input sequence should be
formatted with [CLS] and [SEP] tokens as follows:</p>
<p>(a) For sequence pairs (for a pair title+text for example):`,name:"input_ids"}],source:"https://github.com/huggingface/transformers/blob/pr_16900/src/transformers/models/dpr/modeling_dpr.py#L535",returnDescription:`
<p>A <a
  href="/docs/transformers/pr_16900/en/model_doc/dpr#transformers.models.dpr.modeling_dpr.DPRQuestionEncoderOutput"
>transformers.models.dpr.modeling_dpr.DPRQuestionEncoderOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/pr_16900/en/model_doc/dpr#transformers.DPRConfig"
>DPRConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>pooler_output</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, embeddings_size)</code>) \u2014 The DPR encoder outputs the <em>pooler_output</em> that corresponds to the question representation. Last layer
hidden-state of the first token of the sequence (classification token) further processed by a Linear layer.
This output is to be used to embed questions for nearest neighbors queries with context embeddings.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/pr_16900/en/model_doc/dpr#transformers.models.dpr.modeling_dpr.DPRQuestionEncoderOutput"
>transformers.models.dpr.modeling_dpr.DPRQuestionEncoderOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),Yt=new At({props:{$$slots:{default:[Kf]},$$scope:{ctx:X}}}),dn=new Sn({props:{code:`from transformers import DPRQuestionEncoder, DPRQuestionEncoderTokenizer

tokenizer = DPRQuestionEncoderTokenizer.from_pretrained("facebook/dpr-question_encoder-single-nq-base")
model = DPRQuestionEncoder.from_pretrained("facebook/dpr-question_encoder-single-nq-base")
input_ids = tokenizer("Hello, is my dog cute ?", return_tensors="pt")["input_ids"]
embeddings = model(input_ids).pooler_output`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DPRQuestionEncoder, DPRQuestionEncoderTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = DPRQuestionEncoderTokenizer.from_pretrained(<span class="hljs-string">&quot;facebook/dpr-question_encoder-single-nq-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = DPRQuestionEncoder.from_pretrained(<span class="hljs-string">&quot;facebook/dpr-question_encoder-single-nq-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = tokenizer(<span class="hljs-string">&quot;Hello, is my dog cute ?&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)[<span class="hljs-string">&quot;input_ids&quot;</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>embeddings = model(input_ids).pooler_output`}}),ln=new ge({}),cn=new W({props:{name:"class transformers.DPRReader",anchor:"transformers.DPRReader",parameters:[{name:"config",val:": DPRConfig"}],parametersDescription:[{anchor:"transformers.DPRReader.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_16900/en/model_doc/dpr#transformers.DPRConfig">DPRConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_16900/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/pr_16900/src/transformers/models/dpr/modeling_dpr.py#L608"}}),fn=new W({props:{name:"forward",anchor:"transformers.DPRReader.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"inputs_embeds",val:": typing.Optional[torch.Tensor] = None"},{name:"output_attentions",val:": bool = None"},{name:"output_hidden_states",val:": bool = None"},{name:"return_dict",val:" = None"}],parametersDescription:[{anchor:"transformers.DPRReader.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(n_passages, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.DPRReader.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(n_passages, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.DPRReader.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.DPRReader.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.DPRReader.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_16900/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/pr_16900/src/transformers/models/dpr/modeling_dpr.py#L616",returnDescription:`
<p>A <a
  href="/docs/transformers/pr_16900/en/model_doc/dpr#transformers.DPRReaderOutput"
>transformers.models.dpr.modeling_dpr.DPRReaderOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/pr_16900/en/model_doc/dpr#transformers.DPRConfig"
>DPRConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>start_logits</strong> (<code>torch.FloatTensor</code> of shape <code>(n_passages, sequence_length)</code>) \u2014 Logits of the start index of the span for each passage.</p>
</li>
<li>
<p><strong>end_logits</strong> (<code>torch.FloatTensor</code> of shape <code>(n_passages, sequence_length)</code>) \u2014 Logits of the end index of the span for each passage.</p>
</li>
<li>
<p><strong>relevance_logits</strong> (<code>torch.FloatTensor</code> of shape <code>(n_passages, )</code>) \u2014 Outputs of the QA classifier of the DPRReader that corresponds to the scores of each passage to answer the
question, compared to all the other passages.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/pr_16900/en/model_doc/dpr#transformers.DPRReaderOutput"
>transformers.models.dpr.modeling_dpr.DPRReaderOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),Jt=new At({props:{$$slots:{default:[Yf]},$$scope:{ctx:X}}}),mn=new Sn({props:{code:`from transformers import DPRReader, DPRReaderTokenizer

tokenizer = DPRReaderTokenizer.from_pretrained("facebook/dpr-reader-single-nq-base")
model = DPRReader.from_pretrained("facebook/dpr-reader-single-nq-base")
encoded_inputs = tokenizer(
    questions=["What is love ?"],
    titles=["Haddaway"],
    texts=["'What Is Love' is a song recorded by the artist Haddaway"],
    return_tensors="pt",
)
outputs = model(**encoded_inputs)
start_logits = outputs.start_logits
end_logits = outputs.end_logits
relevance_logits = outputs.relevance_logits`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DPRReader, DPRReaderTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = DPRReaderTokenizer.from_pretrained(<span class="hljs-string">&quot;facebook/dpr-reader-single-nq-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = DPRReader.from_pretrained(<span class="hljs-string">&quot;facebook/dpr-reader-single-nq-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>encoded_inputs = tokenizer(
<span class="hljs-meta">... </span>    questions=[<span class="hljs-string">&quot;What is love ?&quot;</span>],
<span class="hljs-meta">... </span>    titles=[<span class="hljs-string">&quot;Haddaway&quot;</span>],
<span class="hljs-meta">... </span>    texts=[<span class="hljs-string">&quot;&#x27;What Is Love&#x27; is a song recorded by the artist Haddaway&quot;</span>],
<span class="hljs-meta">... </span>    return_tensors=<span class="hljs-string">&quot;pt&quot;</span>,
<span class="hljs-meta">... </span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**encoded_inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>start_logits = outputs.start_logits
<span class="hljs-meta">&gt;&gt;&gt; </span>end_logits = outputs.end_logits
<span class="hljs-meta">&gt;&gt;&gt; </span>relevance_logits = outputs.relevance_logits`}}),gn=new ge({}),_n=new W({props:{name:"class transformers.TFDPRContextEncoder",anchor:"transformers.TFDPRContextEncoder",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.TFDPRContextEncoder.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_16900/en/model_doc/dpr#transformers.DPRConfig">DPRConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_16900/en/main_classes/model#transformers.TFPreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/pr_16900/src/transformers/models/dpr/modeling_tf_dpr.py#L535"}}),Zt=new At({props:{$$slots:{default:[Xf]},$$scope:{ctx:X}}}),kn=new W({props:{name:"call",anchor:"transformers.TFDPRContextEncoder.call",parameters:[{name:"input_ids",val:" = None"},{name:"attention_mask",val:": typing.Optional[tensorflow.python.framework.ops.Tensor] = None"},{name:"token_type_ids",val:": typing.Optional[tensorflow.python.framework.ops.Tensor] = None"},{name:"inputs_embeds",val:": typing.Optional[tensorflow.python.framework.ops.Tensor] = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"},{name:"training",val:": bool = False"}],parametersDescription:[{anchor:"transformers.TFDPRContextEncoder.call.input_ids",description:`<strong>input_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. To match pretraining, DPR input sequence should be
formatted with [CLS] and [SEP] tokens as follows:</p>
<p>(a) For sequence pairs (for a pair title+text for example):`,name:"input_ids"}],source:"https://github.com/huggingface/transformers/blob/pr_16900/src/transformers/models/dpr/modeling_tf_dpr.py#L547",returnDescription:`
<p>A <code>transformers.models.dpr.modeling_tf_dpr.TFDPRContextEncoderOutput</code> or a tuple of <code>tf.Tensor</code> (if
<code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various elements depending on the
configuration (<a
  href="/docs/transformers/pr_16900/en/model_doc/dpr#transformers.DPRConfig"
>DPRConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>pooler_output</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, embeddings_size)</code>) \u2014 The DPR encoder outputs the <em>pooler_output</em> that corresponds to the context representation. Last layer
hidden-state of the first token of the sequence (classification token) further processed by a Linear layer.
This output is to be used to embed contexts for nearest neighbors queries with questions embeddings.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><code>transformers.models.dpr.modeling_tf_dpr.TFDPRContextEncoderOutput</code> or <code>tuple(tf.Tensor)</code></p>
`}}),eo=new At({props:{$$slots:{default:[Jf]},$$scope:{ctx:X}}}),Pn=new Sn({props:{code:`from transformers import TFDPRContextEncoder, DPRContextEncoderTokenizer

tokenizer = DPRContextEncoderTokenizer.from_pretrained("facebook/dpr-ctx_encoder-single-nq-base")
model = TFDPRContextEncoder.from_pretrained("facebook/dpr-ctx_encoder-single-nq-base", from_pt=True)
input_ids = tokenizer("Hello, is my dog cute ?", return_tensors="tf")["input_ids"]
embeddings = model(input_ids).pooler_output`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFDPRContextEncoder, DPRContextEncoderTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = DPRContextEncoderTokenizer.from_pretrained(<span class="hljs-string">&quot;facebook/dpr-ctx_encoder-single-nq-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFDPRContextEncoder.from_pretrained(<span class="hljs-string">&quot;facebook/dpr-ctx_encoder-single-nq-base&quot;</span>, from_pt=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = tokenizer(<span class="hljs-string">&quot;Hello, is my dog cute ?&quot;</span>, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)[<span class="hljs-string">&quot;input_ids&quot;</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>embeddings = model(input_ids).pooler_output`}}),wn=new ge({}),En=new W({props:{name:"class transformers.TFDPRQuestionEncoder",anchor:"transformers.TFDPRQuestionEncoder",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.TFDPRQuestionEncoder.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_16900/en/model_doc/dpr#transformers.DPRConfig">DPRConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_16900/en/main_classes/model#transformers.TFPreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/pr_16900/src/transformers/models/dpr/modeling_tf_dpr.py#L622"}}),oo=new At({props:{$$slots:{default:[Gf]},$$scope:{ctx:X}}}),$n=new W({props:{name:"call",anchor:"transformers.TFDPRQuestionEncoder.call",parameters:[{name:"input_ids",val:" = None"},{name:"attention_mask",val:": typing.Optional[tensorflow.python.framework.ops.Tensor] = None"},{name:"token_type_ids",val:": typing.Optional[tensorflow.python.framework.ops.Tensor] = None"},{name:"inputs_embeds",val:": typing.Optional[tensorflow.python.framework.ops.Tensor] = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"},{name:"training",val:": bool = False"}],parametersDescription:[{anchor:"transformers.TFDPRQuestionEncoder.call.input_ids",description:`<strong>input_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. To match pretraining, DPR input sequence should be
formatted with [CLS] and [SEP] tokens as follows:</p>
<p>(a) For sequence pairs (for a pair title+text for example):`,name:"input_ids"}],source:"https://github.com/huggingface/transformers/blob/pr_16900/src/transformers/models/dpr/modeling_tf_dpr.py#L634",returnDescription:`
<p>A <code>transformers.models.dpr.modeling_tf_dpr.TFDPRQuestionEncoderOutput</code> or a tuple of <code>tf.Tensor</code> (if
<code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various elements depending on the
configuration (<a
  href="/docs/transformers/pr_16900/en/model_doc/dpr#transformers.DPRConfig"
>DPRConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>pooler_output</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, embeddings_size)</code>) \u2014 The DPR encoder outputs the <em>pooler_output</em> that corresponds to the question representation. Last layer
hidden-state of the first token of the sequence (classification token) further processed by a Linear layer.
This output is to be used to embed questions for nearest neighbors queries with context embeddings.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><code>transformers.models.dpr.modeling_tf_dpr.TFDPRQuestionEncoderOutput</code> or <code>tuple(tf.Tensor)</code></p>
`}}),no=new At({props:{$$slots:{default:[Zf]},$$scope:{ctx:X}}}),xn=new Sn({props:{code:`from transformers import TFDPRQuestionEncoder, DPRQuestionEncoderTokenizer

tokenizer = DPRQuestionEncoderTokenizer.from_pretrained("facebook/dpr-question_encoder-single-nq-base")
model = TFDPRQuestionEncoder.from_pretrained("facebook/dpr-question_encoder-single-nq-base", from_pt=True)
input_ids = tokenizer("Hello, is my dog cute ?", return_tensors="tf")["input_ids"]
embeddings = model(input_ids).pooler_output`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFDPRQuestionEncoder, DPRQuestionEncoderTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = DPRQuestionEncoderTokenizer.from_pretrained(<span class="hljs-string">&quot;facebook/dpr-question_encoder-single-nq-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFDPRQuestionEncoder.from_pretrained(<span class="hljs-string">&quot;facebook/dpr-question_encoder-single-nq-base&quot;</span>, from_pt=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = tokenizer(<span class="hljs-string">&quot;Hello, is my dog cute ?&quot;</span>, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)[<span class="hljs-string">&quot;input_ids&quot;</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>embeddings = model(input_ids).pooler_output`}}),zn=new ge({}),qn=new W({props:{name:"class transformers.TFDPRReader",anchor:"transformers.TFDPRReader",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.TFDPRReader.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_16900/en/model_doc/dpr#transformers.DPRConfig">DPRConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_16900/en/main_classes/model#transformers.TFPreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/pr_16900/src/transformers/models/dpr/modeling_tf_dpr.py#L708"}}),so=new At({props:{$$slots:{default:[em]},$$scope:{ctx:X}}}),On=new W({props:{name:"call",anchor:"transformers.TFDPRReader.call",parameters:[{name:"input_ids",val:" = None"},{name:"attention_mask",val:": typing.Optional[tensorflow.python.framework.ops.Tensor] = None"},{name:"inputs_embeds",val:": typing.Optional[tensorflow.python.framework.ops.Tensor] = None"},{name:"output_attentions",val:": bool = None"},{name:"output_hidden_states",val:": bool = None"},{name:"return_dict",val:" = None"},{name:"training",val:": bool = False"}],parametersDescription:[{anchor:"transformers.TFDPRReader.call.attention_mask",description:`<strong>attention_mask</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(n_passages, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.TFDPRReader.call.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(n_passages, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.TFDPRReader.call.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail. This argument can be used only in eager mode, in graph mode the value in the config will be
used instead.`,name:"output_hidden_states"},{anchor:"transformers.TFDPRReader.call.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_16900/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple. This argument can be used in
eager mode, in graph mode the value will always be set to True.`,name:"return_dict"},{anchor:"transformers.TFDPRReader.call.training",description:`<strong>training</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to use the model in training mode (some modules like dropout modules have different
behaviors between training and evaluation).`,name:"training"}],source:"https://github.com/huggingface/transformers/blob/pr_16900/src/transformers/models/dpr/modeling_tf_dpr.py#L720",returnDescription:`
<p>A <code>transformers.models.dpr.modeling_tf_dpr.TFDPRReaderOutput</code> or a tuple of <code>tf.Tensor</code> (if
<code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various elements depending on the
configuration (<a
  href="/docs/transformers/pr_16900/en/model_doc/dpr#transformers.DPRConfig"
>DPRConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>start_logits</strong> (<code>tf.Tensor</code> of shape <code>(n_passages, sequence_length)</code>) \u2014 Logits of the start index of the span for each passage.</p>
</li>
<li>
<p><strong>end_logits</strong> (<code>tf.Tensor</code> of shape <code>(n_passages, sequence_length)</code>) \u2014 Logits of the end index of the span for each passage.</p>
</li>
<li>
<p><strong>relevance_logits</strong> (<code>tf.Tensor</code> of shape <code>(n_passages, )</code>) \u2014 Outputs of the QA classifier of the DPRReader that corresponds to the scores of each passage to answer the
question, compared to all the other passages.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><code>transformers.models.dpr.modeling_tf_dpr.TFDPRReaderOutput</code> or <code>tuple(tf.Tensor)</code></p>
`}}),ao=new At({props:{$$slots:{default:[tm]},$$scope:{ctx:X}}}),Nn=new Sn({props:{code:`from transformers import TFDPRReader, DPRReaderTokenizer

tokenizer = DPRReaderTokenizer.from_pretrained("facebook/dpr-reader-single-nq-base")
model = TFDPRReader.from_pretrained("facebook/dpr-reader-single-nq-base", from_pt=True)
encoded_inputs = tokenizer(
    questions=["What is love ?"],
    titles=["Haddaway"],
    texts=["'What Is Love' is a song recorded by the artist Haddaway"],
    return_tensors="tf",
)
outputs = model(encoded_inputs)
start_logits = outputs.start_logits
end_logits = outputs.end_logits
relevance_logits = outputs.relevance_logits`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFDPRReader, DPRReaderTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = DPRReaderTokenizer.from_pretrained(<span class="hljs-string">&quot;facebook/dpr-reader-single-nq-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFDPRReader.from_pretrained(<span class="hljs-string">&quot;facebook/dpr-reader-single-nq-base&quot;</span>, from_pt=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>encoded_inputs = tokenizer(
<span class="hljs-meta">... </span>    questions=[<span class="hljs-string">&quot;What is love ?&quot;</span>],
<span class="hljs-meta">... </span>    titles=[<span class="hljs-string">&quot;Haddaway&quot;</span>],
<span class="hljs-meta">... </span>    texts=[<span class="hljs-string">&quot;&#x27;What Is Love&#x27; is a song recorded by the artist Haddaway&quot;</span>],
<span class="hljs-meta">... </span>    return_tensors=<span class="hljs-string">&quot;tf&quot;</span>,
<span class="hljs-meta">... </span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(encoded_inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>start_logits = outputs.start_logits
<span class="hljs-meta">&gt;&gt;&gt; </span>end_logits = outputs.end_logits
<span class="hljs-meta">&gt;&gt;&gt; </span>relevance_logits = outputs.relevance_logits`}}),{c(){f=s("meta"),x=l(),m=s("h1"),g=s("a"),R=s("span"),b(v.$$.fragment),_=l(),$=s("span"),_e=n("DPR"),Z=l(),D=s("h2"),J=s("a"),I=s("span"),b(ee.$$.fragment),ve=l(),S=s("span"),be=n("Overview"),he=l(),H=s("p"),L=n(`Dense Passage Retrieval (DPR) is a set of tools and models for state-of-the-art open-domain Q&A research. It was
introduced in `),te=s("a"),oe=n("Dense Passage Retrieval for Open-Domain Question Answering"),q=n(` by
Vladimir Karpukhin, Barlas O\u011Fuz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, Wen-tau Yih.`),C=l(),ne=s("p"),U=n("The abstract from the paper is the following:"),ue=l(),re=s("p"),B=s("em"),Te=n(`Open-domain question answering relies on efficient passage retrieval to select candidate contexts, where traditional
sparse vector space models, such as TF-IDF or BM25, are the de facto method. In this work, we show that retrieval can
be practically implemented using dense representations alone, where embeddings are learned from a small number of
questions and passages by a simple dual-encoder framework. When evaluated on a wide range of open-domain QA datasets,
our dense retriever outperforms a strong Lucene-BM25 system largely by 9%-19% absolute in terms of top-20 passage
retrieval accuracy, and helps our end-to-end QA system establish new state-of-the-art on multiple open-domain QA
benchmarks.`),fe=l(),z=s("p"),ke=n("This model was contributed by "),j=s("a"),Pe=n("lhoestq"),we=n(". The original code can be found "),M=s("a"),Ee=n("here"),Re=n("."),Q=l(),V=s("h2"),N=s("a"),se=s("span"),b(h.$$.fragment),y=l(),G=s("span"),Se=n("DPRConfig"),je=l(),F=s("div"),b(me.$$.fragment),He=l(),ae=s("p"),O=s("a"),K=n("DPRConfig"),Be=n(" is the configuration class to store the configuration of a "),xe=s("em"),Y=n("DPRModel"),We=n("."),Ue=l(),A=s("p"),Ve=n("This is the configuration class to store the configuration of a "),Hn=s("a"),hi=n("DPRContextEncoder"),ui=n(", "),Bn=s("a"),fi=n("DPRQuestionEncoder"),mi=n(`, or a
`),Wn=s("a"),gi=n("DPRReader"),_i=n(`. It is used to instantiate the components of the DPR model according to the specified arguments,
defining the model component architectures. Instantiating a configuration with the defaults will yield a similar
configuration to that of the DPRContextEncoder
`),mo=s("a"),vi=n("facebook/dpr-ctx_encoder-single-nq-base"),bi=n(`,
DPRQuestionEncoder
`),go=s("a"),Ti=n("facebook/dpr-question_encoder-single-nq-base"),ki=n(`,
or DPRReader `),_o=s("a"),Pi=n("facebook/dpr-reader-single-nq-base"),wi=n(`
architecture.`),Ei=l(),vo=s("p"),Ri=n("This class is a subclass of "),Un=s("a"),Di=n("BertConfig"),yi=n(". Please check the superclass for the documentation of all kwargs."),Ys=l(),ht=s("h2"),Ot=s("a"),Or=s("span"),b(bo.$$.fragment),$i=l(),Nr=s("span"),xi=n("DPRContextEncoderTokenizer"),Xs=l(),Me=s("div"),b(To.$$.fragment),zi=l(),jr=s("p"),qi=n("Construct a DPRContextEncoder tokenizer."),Fi=l(),Nt=s("p"),Vn=s("a"),Ci=n("DPRContextEncoderTokenizer"),Ai=n(" is identical to "),Kn=s("a"),Oi=n("BertTokenizer"),Ni=n(` and runs end-to-end tokenization: punctuation
splitting and wordpiece.`),ji=l(),ko=s("p"),Mi=n("Refer to superclass "),Yn=s("a"),Qi=n("BertTokenizer"),Li=n(" for usage examples and documentation concerning parameters."),Js=l(),ut=s("h2"),jt=s("a"),Mr=s("span"),b(Po.$$.fragment),Ii=l(),Qr=s("span"),Si=n("DPRContextEncoderTokenizerFast"),Gs=l(),Qe=s("div"),b(wo.$$.fragment),Hi=l(),Eo=s("p"),Bi=n("Construct a \u201Cfast\u201D DPRContextEncoder tokenizer (backed by HuggingFace\u2019s "),Lr=s("em"),Wi=n("tokenizers"),Ui=n(" library)."),Vi=l(),Mt=s("p"),Xn=s("a"),Ki=n("DPRContextEncoderTokenizerFast"),Yi=n(" is identical to "),Jn=s("a"),Xi=n("BertTokenizerFast"),Ji=n(` and runs end-to-end tokenization:
punctuation splitting and wordpiece.`),Gi=l(),Ro=s("p"),Zi=n("Refer to superclass "),Gn=s("a"),ed=n("BertTokenizerFast"),td=n(" for usage examples and documentation concerning parameters."),Zs=l(),ft=s("h2"),Qt=s("a"),Ir=s("span"),b(Do.$$.fragment),od=l(),Sr=s("span"),nd=n("DPRQuestionEncoderTokenizer"),ea=l(),Le=s("div"),b(yo.$$.fragment),rd=l(),Hr=s("p"),sd=n("Constructs a DPRQuestionEncoder tokenizer."),ad=l(),Lt=s("p"),Zn=s("a"),id=n("DPRQuestionEncoderTokenizer"),dd=n(" is identical to "),er=s("a"),ld=n("BertTokenizer"),cd=n(` and runs end-to-end tokenization: punctuation
splitting and wordpiece.`),pd=l(),$o=s("p"),hd=n("Refer to superclass "),tr=s("a"),ud=n("BertTokenizer"),fd=n(" for usage examples and documentation concerning parameters."),ta=l(),mt=s("h2"),It=s("a"),Br=s("span"),b(xo.$$.fragment),md=l(),Wr=s("span"),gd=n("DPRQuestionEncoderTokenizerFast"),oa=l(),Ie=s("div"),b(zo.$$.fragment),_d=l(),qo=s("p"),vd=n("Constructs a \u201Cfast\u201D DPRQuestionEncoder tokenizer (backed by HuggingFace\u2019s "),Ur=s("em"),bd=n("tokenizers"),Td=n(" library)."),kd=l(),St=s("p"),or=s("a"),Pd=n("DPRQuestionEncoderTokenizerFast"),wd=n(" is identical to "),nr=s("a"),Ed=n("BertTokenizerFast"),Rd=n(` and runs end-to-end tokenization:
punctuation splitting and wordpiece.`),Dd=l(),Fo=s("p"),yd=n("Refer to superclass "),rr=s("a"),$d=n("BertTokenizerFast"),xd=n(" for usage examples and documentation concerning parameters."),na=l(),gt=s("h2"),Ht=s("a"),Vr=s("span"),b(Co.$$.fragment),zd=l(),Kr=s("span"),qd=n("DPRReaderTokenizer"),ra=l(),ie=s("div"),b(Ao.$$.fragment),Fd=l(),Yr=s("p"),Cd=n("Construct a DPRReader tokenizer."),Ad=l(),tt=s("p"),sr=s("a"),Od=n("DPRReaderTokenizer"),Nd=n(" is almost identical to "),ar=s("a"),jd=n("BertTokenizer"),Md=n(` and runs end-to-end tokenization: punctuation
splitting and wordpiece. The difference is that is has three inputs strings: question, titles and texts that are
combined to be fed to the `),ir=s("a"),Qd=n("DPRReader"),Ld=n(" model."),Id=l(),Oo=s("p"),Sd=n("Refer to superclass "),dr=s("a"),Hd=n("BertTokenizer"),Bd=n(" for usage examples and documentation concerning parameters."),Wd=l(),Ze=s("p"),Ud=n("Return a dictionary with the token ids of the input strings and other information to give to "),Xr=s("code"),Vd=n(".decode_best_spans"),Kd=n(`.
It converts the strings of a question and different passages (title and text) in a sequence of IDs (integers),
using the tokenizer and vocabulary. The resulting `),Jr=s("code"),Yd=n("input_ids"),Xd=n(" is a matrix of size "),Gr=s("code"),Jd=n("(n_passages, sequence_length)"),Gd=n(`
with the format:`),Zd=l(),b(No.$$.fragment),sa=l(),_t=s("h2"),Bt=s("a"),Zr=s("span"),b(jo.$$.fragment),el=l(),es=s("span"),tl=n("DPRReaderTokenizerFast"),aa=l(),de=s("div"),b(Mo.$$.fragment),ol=l(),Qo=s("p"),nl=n("Constructs a \u201Cfast\u201D DPRReader tokenizer (backed by HuggingFace\u2019s "),ts=s("em"),rl=n("tokenizers"),sl=n(" library)."),al=l(),ot=s("p"),lr=s("a"),il=n("DPRReaderTokenizerFast"),dl=n(" is almost identical to "),cr=s("a"),ll=n("BertTokenizerFast"),cl=n(` and runs end-to-end tokenization:
punctuation splitting and wordpiece. The difference is that is has three inputs strings: question, titles and texts
that are combined to be fed to the `),pr=s("a"),pl=n("DPRReader"),hl=n(" model."),ul=l(),Lo=s("p"),fl=n("Refer to superclass "),hr=s("a"),ml=n("BertTokenizerFast"),gl=n(" for usage examples and documentation concerning parameters."),_l=l(),et=s("p"),vl=n("Return a dictionary with the token ids of the input strings and other information to give to "),os=s("code"),bl=n(".decode_best_spans"),Tl=n(`.
It converts the strings of a question and different passages (title and text) in a sequence of IDs (integers),
using the tokenizer and vocabulary. The resulting `),ns=s("code"),kl=n("input_ids"),Pl=n(" is a matrix of size "),rs=s("code"),wl=n("(n_passages, sequence_length)"),El=n(`
with the format:`),Rl=l(),ss=s("p"),Dl=n("[CLS] <question token ids> [SEP] <titles ids> [SEP] <texts ids>"),ia=l(),vt=s("h2"),Wt=s("a"),as=s("span"),b(Io.$$.fragment),yl=l(),is=s("span"),$l=n("DPR specific outputs"),da=l(),bt=s("div"),b(So.$$.fragment),xl=l(),Ho=s("p"),zl=n("Class for outputs of "),ur=s("a"),ql=n("DPRQuestionEncoder"),Fl=n("."),la=l(),Tt=s("div"),b(Bo.$$.fragment),Cl=l(),Wo=s("p"),Al=n("Class for outputs of "),fr=s("a"),Ol=n("DPRQuestionEncoder"),Nl=n("."),ca=l(),kt=s("div"),b(Uo.$$.fragment),jl=l(),Vo=s("p"),Ml=n("Class for outputs of "),mr=s("a"),Ql=n("DPRQuestionEncoder"),Ll=n("."),pa=l(),Pt=s("h2"),Ut=s("a"),ds=s("span"),b(Ko.$$.fragment),Il=l(),ls=s("span"),Sl=n("DPRContextEncoder"),ha=l(),De=s("div"),b(Yo.$$.fragment),Hl=l(),cs=s("p"),Bl=n("The bare DPRContextEncoder transformer outputting pooler outputs as context representations."),Wl=l(),Xo=s("p"),Ul=n("This model inherits from "),gr=s("a"),Vl=n("PreTrainedModel"),Kl=n(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Yl=l(),Jo=s("p"),Xl=n("This model is also a PyTorch "),Go=s("a"),Jl=n("torch.nn.Module"),Gl=n(` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),Zl=l(),ze=s("div"),b(Zo.$$.fragment),ec=l(),wt=s("p"),tc=n("The "),_r=s("a"),oc=n("DPRContextEncoder"),nc=n(" forward method, overrides the "),ps=s("code"),rc=n("__call__"),sc=n(" special method."),ac=l(),b(Vt.$$.fragment),ic=l(),hs=s("p"),dc=n("Examples:"),lc=l(),b(en.$$.fragment),ua=l(),Et=s("h2"),Kt=s("a"),us=s("span"),b(tn.$$.fragment),cc=l(),fs=s("span"),pc=n("DPRQuestionEncoder"),fa=l(),ye=s("div"),b(on.$$.fragment),hc=l(),ms=s("p"),uc=n("The bare DPRQuestionEncoder transformer outputting pooler outputs as question representations."),fc=l(),nn=s("p"),mc=n("This model inherits from "),vr=s("a"),gc=n("PreTrainedModel"),_c=n(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),vc=l(),rn=s("p"),bc=n("This model is also a PyTorch "),sn=s("a"),Tc=n("torch.nn.Module"),kc=n(` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),Pc=l(),qe=s("div"),b(an.$$.fragment),wc=l(),Rt=s("p"),Ec=n("The "),br=s("a"),Rc=n("DPRQuestionEncoder"),Dc=n(" forward method, overrides the "),gs=s("code"),yc=n("__call__"),$c=n(" special method."),xc=l(),b(Yt.$$.fragment),zc=l(),_s=s("p"),qc=n("Examples:"),Fc=l(),b(dn.$$.fragment),ma=l(),Dt=s("h2"),Xt=s("a"),vs=s("span"),b(ln.$$.fragment),Cc=l(),bs=s("span"),Ac=n("DPRReader"),ga=l(),$e=s("div"),b(cn.$$.fragment),Oc=l(),Ts=s("p"),Nc=n("The bare DPRReader transformer outputting span predictions."),jc=l(),pn=s("p"),Mc=n("This model inherits from "),Tr=s("a"),Qc=n("PreTrainedModel"),Lc=n(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Ic=l(),hn=s("p"),Sc=n("This model is also a PyTorch "),un=s("a"),Hc=n("torch.nn.Module"),Bc=n(` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),Wc=l(),Fe=s("div"),b(fn.$$.fragment),Uc=l(),yt=s("p"),Vc=n("The "),kr=s("a"),Kc=n("DPRReader"),Yc=n(" forward method, overrides the "),ks=s("code"),Xc=n("__call__"),Jc=n(" special method."),Gc=l(),b(Jt.$$.fragment),Zc=l(),Ps=s("p"),ep=n("Examples:"),tp=l(),b(mn.$$.fragment),_a=l(),$t=s("h2"),Gt=s("a"),ws=s("span"),b(gn.$$.fragment),op=l(),Es=s("span"),np=n("TFDPRContextEncoder"),va=l(),le=s("div"),b(_n.$$.fragment),rp=l(),Rs=s("p"),sp=n("The bare DPRContextEncoder transformer outputting pooler outputs as context representations."),ap=l(),vn=s("p"),ip=n("This model inherits from "),Pr=s("a"),dp=n("TFPreTrainedModel"),lp=n(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),cp=l(),bn=s("p"),pp=n("This model is also a Tensorflow "),Tn=s("a"),hp=n("tf.keras.Model"),up=n(`
subclass. Use it as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to
general usage and behavior.`),fp=l(),b(Zt.$$.fragment),mp=l(),Ce=s("div"),b(kn.$$.fragment),gp=l(),xt=s("p"),_p=n("The "),wr=s("a"),vp=n("TFDPRContextEncoder"),bp=n(" forward method, overrides the "),Ds=s("code"),Tp=n("__call__"),kp=n(" special method."),Pp=l(),b(eo.$$.fragment),wp=l(),ys=s("p"),Ep=n("Examples:"),Rp=l(),b(Pn.$$.fragment),ba=l(),zt=s("h2"),to=s("a"),$s=s("span"),b(wn.$$.fragment),Dp=l(),xs=s("span"),yp=n("TFDPRQuestionEncoder"),Ta=l(),ce=s("div"),b(En.$$.fragment),$p=l(),zs=s("p"),xp=n("The bare DPRQuestionEncoder transformer outputting pooler outputs as question representations."),zp=l(),Rn=s("p"),qp=n("This model inherits from "),Er=s("a"),Fp=n("TFPreTrainedModel"),Cp=n(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Ap=l(),Dn=s("p"),Op=n("This model is also a Tensorflow "),yn=s("a"),Np=n("tf.keras.Model"),jp=n(`
subclass. Use it as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to
general usage and behavior.`),Mp=l(),b(oo.$$.fragment),Qp=l(),Ae=s("div"),b($n.$$.fragment),Lp=l(),qt=s("p"),Ip=n("The "),Rr=s("a"),Sp=n("TFDPRQuestionEncoder"),Hp=n(" forward method, overrides the "),qs=s("code"),Bp=n("__call__"),Wp=n(" special method."),Up=l(),b(no.$$.fragment),Vp=l(),Fs=s("p"),Kp=n("Examples:"),Yp=l(),b(xn.$$.fragment),ka=l(),Ft=s("h2"),ro=s("a"),Cs=s("span"),b(zn.$$.fragment),Xp=l(),As=s("span"),Jp=n("TFDPRReader"),Pa=l(),pe=s("div"),b(qn.$$.fragment),Gp=l(),Os=s("p"),Zp=n("The bare DPRReader transformer outputting span predictions."),eh=l(),Fn=s("p"),th=n("This model inherits from "),Dr=s("a"),oh=n("TFPreTrainedModel"),nh=n(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),rh=l(),Cn=s("p"),sh=n("This model is also a Tensorflow "),An=s("a"),ah=n("tf.keras.Model"),ih=n(`
subclass. Use it as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to
general usage and behavior.`),dh=l(),b(so.$$.fragment),lh=l(),Oe=s("div"),b(On.$$.fragment),ch=l(),Ct=s("p"),ph=n("The "),yr=s("a"),hh=n("TFDPRReader"),uh=n(" forward method, overrides the "),Ns=s("code"),fh=n("__call__"),mh=n(" special method."),gh=l(),b(ao.$$.fragment),_h=l(),js=s("p"),vh=n("Examples:"),bh=l(),b(Nn.$$.fragment),this.h()},l(o){const u=Wf('[data-svelte="svelte-1phssyn"]',document.head);f=a(u,"META",{name:!0,content:!0}),u.forEach(t),x=c(o),m=a(o,"H1",{class:!0});var jn=i(m);g=a(jn,"A",{id:!0,class:!0,href:!0});var Ms=i(g);R=a(Ms,"SPAN",{});var Qs=i(R);T(v.$$.fragment,Qs),Qs.forEach(t),Ms.forEach(t),_=c(jn),$=a(jn,"SPAN",{});var Ls=i($);_e=r(Ls,"DPR"),Ls.forEach(t),jn.forEach(t),Z=c(o),D=a(o,"H2",{class:!0});var Mn=i(D);J=a(Mn,"A",{id:!0,class:!0,href:!0});var Is=i(J);I=a(Is,"SPAN",{});var Ss=i(I);T(ee.$$.fragment,Ss),Ss.forEach(t),Is.forEach(t),ve=c(Mn),S=a(Mn,"SPAN",{});var Hs=i(S);be=r(Hs,"Overview"),Hs.forEach(t),Mn.forEach(t),he=c(o),H=a(o,"P",{});var Qn=i(H);L=r(Qn,`Dense Passage Retrieval (DPR) is a set of tools and models for state-of-the-art open-domain Q&A research. It was
introduced in `),te=a(Qn,"A",{href:!0,rel:!0});var Th=i(te);oe=r(Th,"Dense Passage Retrieval for Open-Domain Question Answering"),Th.forEach(t),q=r(Qn,` by
Vladimir Karpukhin, Barlas O\u011Fuz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, Wen-tau Yih.`),Qn.forEach(t),C=c(o),ne=a(o,"P",{});var kh=i(ne);U=r(kh,"The abstract from the paper is the following:"),kh.forEach(t),ue=c(o),re=a(o,"P",{});var Ph=i(re);B=a(Ph,"EM",{});var wh=i(B);Te=r(wh,`Open-domain question answering relies on efficient passage retrieval to select candidate contexts, where traditional
sparse vector space models, such as TF-IDF or BM25, are the de facto method. In this work, we show that retrieval can
be practically implemented using dense representations alone, where embeddings are learned from a small number of
questions and passages by a simple dual-encoder framework. When evaluated on a wide range of open-domain QA datasets,
our dense retriever outperforms a strong Lucene-BM25 system largely by 9%-19% absolute in terms of top-20 passage
retrieval accuracy, and helps our end-to-end QA system establish new state-of-the-art on multiple open-domain QA
benchmarks.`),wh.forEach(t),Ph.forEach(t),fe=c(o),z=a(o,"P",{});var $r=i(z);ke=r($r,"This model was contributed by "),j=a($r,"A",{href:!0,rel:!0});var Eh=i(j);Pe=r(Eh,"lhoestq"),Eh.forEach(t),we=r($r,". The original code can be found "),M=a($r,"A",{href:!0,rel:!0});var Rh=i(M);Ee=r(Rh,"here"),Rh.forEach(t),Re=r($r,"."),$r.forEach(t),Q=c(o),V=a(o,"H2",{class:!0});var Ea=i(V);N=a(Ea,"A",{id:!0,class:!0,href:!0});var Dh=i(N);se=a(Dh,"SPAN",{});var yh=i(se);T(h.$$.fragment,yh),yh.forEach(t),Dh.forEach(t),y=c(Ea),G=a(Ea,"SPAN",{});var $h=i(G);Se=r($h,"DPRConfig"),$h.forEach(t),Ea.forEach(t),je=c(o),F=a(o,"DIV",{class:!0});var io=i(F);T(me.$$.fragment,io),He=c(io),ae=a(io,"P",{});var Bs=i(ae);O=a(Bs,"A",{href:!0});var xh=i(O);K=r(xh,"DPRConfig"),xh.forEach(t),Be=r(Bs," is the configuration class to store the configuration of a "),xe=a(Bs,"EM",{});var zh=i(xe);Y=r(zh,"DPRModel"),zh.forEach(t),We=r(Bs,"."),Bs.forEach(t),Ue=c(io),A=a(io,"P",{});var Ne=i(A);Ve=r(Ne,"This is the configuration class to store the configuration of a "),Hn=a(Ne,"A",{href:!0});var qh=i(Hn);hi=r(qh,"DPRContextEncoder"),qh.forEach(t),ui=r(Ne,", "),Bn=a(Ne,"A",{href:!0});var Fh=i(Bn);fi=r(Fh,"DPRQuestionEncoder"),Fh.forEach(t),mi=r(Ne,`, or a
`),Wn=a(Ne,"A",{href:!0});var Ch=i(Wn);gi=r(Ch,"DPRReader"),Ch.forEach(t),_i=r(Ne,`. It is used to instantiate the components of the DPR model according to the specified arguments,
defining the model component architectures. Instantiating a configuration with the defaults will yield a similar
configuration to that of the DPRContextEncoder
`),mo=a(Ne,"A",{href:!0,rel:!0});var Ah=i(mo);vi=r(Ah,"facebook/dpr-ctx_encoder-single-nq-base"),Ah.forEach(t),bi=r(Ne,`,
DPRQuestionEncoder
`),go=a(Ne,"A",{href:!0,rel:!0});var Oh=i(go);Ti=r(Oh,"facebook/dpr-question_encoder-single-nq-base"),Oh.forEach(t),ki=r(Ne,`,
or DPRReader `),_o=a(Ne,"A",{href:!0,rel:!0});var Nh=i(_o);Pi=r(Nh,"facebook/dpr-reader-single-nq-base"),Nh.forEach(t),wi=r(Ne,`
architecture.`),Ne.forEach(t),Ei=c(io),vo=a(io,"P",{});var Ra=i(vo);Ri=r(Ra,"This class is a subclass of "),Un=a(Ra,"A",{href:!0});var jh=i(Un);Di=r(jh,"BertConfig"),jh.forEach(t),yi=r(Ra,". Please check the superclass for the documentation of all kwargs."),Ra.forEach(t),io.forEach(t),Ys=c(o),ht=a(o,"H2",{class:!0});var Da=i(ht);Ot=a(Da,"A",{id:!0,class:!0,href:!0});var Mh=i(Ot);Or=a(Mh,"SPAN",{});var Qh=i(Or);T(bo.$$.fragment,Qh),Qh.forEach(t),Mh.forEach(t),$i=c(Da),Nr=a(Da,"SPAN",{});var Lh=i(Nr);xi=r(Lh,"DPRContextEncoderTokenizer"),Lh.forEach(t),Da.forEach(t),Xs=c(o),Me=a(o,"DIV",{class:!0});var lo=i(Me);T(To.$$.fragment,lo),zi=c(lo),jr=a(lo,"P",{});var Ih=i(jr);qi=r(Ih,"Construct a DPRContextEncoder tokenizer."),Ih.forEach(t),Fi=c(lo),Nt=a(lo,"P",{});var Ws=i(Nt);Vn=a(Ws,"A",{href:!0});var Sh=i(Vn);Ci=r(Sh,"DPRContextEncoderTokenizer"),Sh.forEach(t),Ai=r(Ws," is identical to "),Kn=a(Ws,"A",{href:!0});var Hh=i(Kn);Oi=r(Hh,"BertTokenizer"),Hh.forEach(t),Ni=r(Ws,` and runs end-to-end tokenization: punctuation
splitting and wordpiece.`),Ws.forEach(t),ji=c(lo),ko=a(lo,"P",{});var ya=i(ko);Mi=r(ya,"Refer to superclass "),Yn=a(ya,"A",{href:!0});var Bh=i(Yn);Qi=r(Bh,"BertTokenizer"),Bh.forEach(t),Li=r(ya," for usage examples and documentation concerning parameters."),ya.forEach(t),lo.forEach(t),Js=c(o),ut=a(o,"H2",{class:!0});var $a=i(ut);jt=a($a,"A",{id:!0,class:!0,href:!0});var Wh=i(jt);Mr=a(Wh,"SPAN",{});var Uh=i(Mr);T(Po.$$.fragment,Uh),Uh.forEach(t),Wh.forEach(t),Ii=c($a),Qr=a($a,"SPAN",{});var Vh=i(Qr);Si=r(Vh,"DPRContextEncoderTokenizerFast"),Vh.forEach(t),$a.forEach(t),Gs=c(o),Qe=a(o,"DIV",{class:!0});var co=i(Qe);T(wo.$$.fragment,co),Hi=c(co),Eo=a(co,"P",{});var xa=i(Eo);Bi=r(xa,"Construct a \u201Cfast\u201D DPRContextEncoder tokenizer (backed by HuggingFace\u2019s "),Lr=a(xa,"EM",{});var Kh=i(Lr);Wi=r(Kh,"tokenizers"),Kh.forEach(t),Ui=r(xa," library)."),xa.forEach(t),Vi=c(co),Mt=a(co,"P",{});var Us=i(Mt);Xn=a(Us,"A",{href:!0});var Yh=i(Xn);Ki=r(Yh,"DPRContextEncoderTokenizerFast"),Yh.forEach(t),Yi=r(Us," is identical to "),Jn=a(Us,"A",{href:!0});var Xh=i(Jn);Xi=r(Xh,"BertTokenizerFast"),Xh.forEach(t),Ji=r(Us,` and runs end-to-end tokenization:
punctuation splitting and wordpiece.`),Us.forEach(t),Gi=c(co),Ro=a(co,"P",{});var za=i(Ro);Zi=r(za,"Refer to superclass "),Gn=a(za,"A",{href:!0});var Jh=i(Gn);ed=r(Jh,"BertTokenizerFast"),Jh.forEach(t),td=r(za," for usage examples and documentation concerning parameters."),za.forEach(t),co.forEach(t),Zs=c(o),ft=a(o,"H2",{class:!0});var qa=i(ft);Qt=a(qa,"A",{id:!0,class:!0,href:!0});var Gh=i(Qt);Ir=a(Gh,"SPAN",{});var Zh=i(Ir);T(Do.$$.fragment,Zh),Zh.forEach(t),Gh.forEach(t),od=c(qa),Sr=a(qa,"SPAN",{});var eu=i(Sr);nd=r(eu,"DPRQuestionEncoderTokenizer"),eu.forEach(t),qa.forEach(t),ea=c(o),Le=a(o,"DIV",{class:!0});var po=i(Le);T(yo.$$.fragment,po),rd=c(po),Hr=a(po,"P",{});var tu=i(Hr);sd=r(tu,"Constructs a DPRQuestionEncoder tokenizer."),tu.forEach(t),ad=c(po),Lt=a(po,"P",{});var Vs=i(Lt);Zn=a(Vs,"A",{href:!0});var ou=i(Zn);id=r(ou,"DPRQuestionEncoderTokenizer"),ou.forEach(t),dd=r(Vs," is identical to "),er=a(Vs,"A",{href:!0});var nu=i(er);ld=r(nu,"BertTokenizer"),nu.forEach(t),cd=r(Vs,` and runs end-to-end tokenization: punctuation
splitting and wordpiece.`),Vs.forEach(t),pd=c(po),$o=a(po,"P",{});var Fa=i($o);hd=r(Fa,"Refer to superclass "),tr=a(Fa,"A",{href:!0});var ru=i(tr);ud=r(ru,"BertTokenizer"),ru.forEach(t),fd=r(Fa," for usage examples and documentation concerning parameters."),Fa.forEach(t),po.forEach(t),ta=c(o),mt=a(o,"H2",{class:!0});var Ca=i(mt);It=a(Ca,"A",{id:!0,class:!0,href:!0});var su=i(It);Br=a(su,"SPAN",{});var au=i(Br);T(xo.$$.fragment,au),au.forEach(t),su.forEach(t),md=c(Ca),Wr=a(Ca,"SPAN",{});var iu=i(Wr);gd=r(iu,"DPRQuestionEncoderTokenizerFast"),iu.forEach(t),Ca.forEach(t),oa=c(o),Ie=a(o,"DIV",{class:!0});var ho=i(Ie);T(zo.$$.fragment,ho),_d=c(ho),qo=a(ho,"P",{});var Aa=i(qo);vd=r(Aa,"Constructs a \u201Cfast\u201D DPRQuestionEncoder tokenizer (backed by HuggingFace\u2019s "),Ur=a(Aa,"EM",{});var du=i(Ur);bd=r(du,"tokenizers"),du.forEach(t),Td=r(Aa," library)."),Aa.forEach(t),kd=c(ho),St=a(ho,"P",{});var Ks=i(St);or=a(Ks,"A",{href:!0});var lu=i(or);Pd=r(lu,"DPRQuestionEncoderTokenizerFast"),lu.forEach(t),wd=r(Ks," is identical to "),nr=a(Ks,"A",{href:!0});var cu=i(nr);Ed=r(cu,"BertTokenizerFast"),cu.forEach(t),Rd=r(Ks,` and runs end-to-end tokenization:
punctuation splitting and wordpiece.`),Ks.forEach(t),Dd=c(ho),Fo=a(ho,"P",{});var Oa=i(Fo);yd=r(Oa,"Refer to superclass "),rr=a(Oa,"A",{href:!0});var pu=i(rr);$d=r(pu,"BertTokenizerFast"),pu.forEach(t),xd=r(Oa," for usage examples and documentation concerning parameters."),Oa.forEach(t),ho.forEach(t),na=c(o),gt=a(o,"H2",{class:!0});var Na=i(gt);Ht=a(Na,"A",{id:!0,class:!0,href:!0});var hu=i(Ht);Vr=a(hu,"SPAN",{});var uu=i(Vr);T(Co.$$.fragment,uu),uu.forEach(t),hu.forEach(t),zd=c(Na),Kr=a(Na,"SPAN",{});var fu=i(Kr);qd=r(fu,"DPRReaderTokenizer"),fu.forEach(t),Na.forEach(t),ra=c(o),ie=a(o,"DIV",{class:!0});var Ke=i(ie);T(Ao.$$.fragment,Ke),Fd=c(Ke),Yr=a(Ke,"P",{});var mu=i(Yr);Cd=r(mu,"Construct a DPRReader tokenizer."),mu.forEach(t),Ad=c(Ke),tt=a(Ke,"P",{});var Ln=i(tt);sr=a(Ln,"A",{href:!0});var gu=i(sr);Od=r(gu,"DPRReaderTokenizer"),gu.forEach(t),Nd=r(Ln," is almost identical to "),ar=a(Ln,"A",{href:!0});var _u=i(ar);jd=r(_u,"BertTokenizer"),_u.forEach(t),Md=r(Ln,` and runs end-to-end tokenization: punctuation
splitting and wordpiece. The difference is that is has three inputs strings: question, titles and texts that are
combined to be fed to the `),ir=a(Ln,"A",{href:!0});var vu=i(ir);Qd=r(vu,"DPRReader"),vu.forEach(t),Ld=r(Ln," model."),Ln.forEach(t),Id=c(Ke),Oo=a(Ke,"P",{});var ja=i(Oo);Sd=r(ja,"Refer to superclass "),dr=a(ja,"A",{href:!0});var bu=i(dr);Hd=r(bu,"BertTokenizer"),bu.forEach(t),Bd=r(ja," for usage examples and documentation concerning parameters."),ja.forEach(t),Wd=c(Ke),Ze=a(Ke,"P",{});var uo=i(Ze);Ud=r(uo,"Return a dictionary with the token ids of the input strings and other information to give to "),Xr=a(uo,"CODE",{});var Tu=i(Xr);Vd=r(Tu,".decode_best_spans"),Tu.forEach(t),Kd=r(uo,`.
It converts the strings of a question and different passages (title and text) in a sequence of IDs (integers),
using the tokenizer and vocabulary. The resulting `),Jr=a(uo,"CODE",{});var ku=i(Jr);Yd=r(ku,"input_ids"),ku.forEach(t),Xd=r(uo," is a matrix of size "),Gr=a(uo,"CODE",{});var Pu=i(Gr);Jd=r(Pu,"(n_passages, sequence_length)"),Pu.forEach(t),Gd=r(uo,`
with the format:`),uo.forEach(t),Zd=c(Ke),T(No.$$.fragment,Ke),Ke.forEach(t),sa=c(o),_t=a(o,"H2",{class:!0});var Ma=i(_t);Bt=a(Ma,"A",{id:!0,class:!0,href:!0});var wu=i(Bt);Zr=a(wu,"SPAN",{});var Eu=i(Zr);T(jo.$$.fragment,Eu),Eu.forEach(t),wu.forEach(t),el=c(Ma),es=a(Ma,"SPAN",{});var Ru=i(es);tl=r(Ru,"DPRReaderTokenizerFast"),Ru.forEach(t),Ma.forEach(t),aa=c(o),de=a(o,"DIV",{class:!0});var Ye=i(de);T(Mo.$$.fragment,Ye),ol=c(Ye),Qo=a(Ye,"P",{});var Qa=i(Qo);nl=r(Qa,"Constructs a \u201Cfast\u201D DPRReader tokenizer (backed by HuggingFace\u2019s "),ts=a(Qa,"EM",{});var Du=i(ts);rl=r(Du,"tokenizers"),Du.forEach(t),sl=r(Qa," library)."),Qa.forEach(t),al=c(Ye),ot=a(Ye,"P",{});var In=i(ot);lr=a(In,"A",{href:!0});var yu=i(lr);il=r(yu,"DPRReaderTokenizerFast"),yu.forEach(t),dl=r(In," is almost identical to "),cr=a(In,"A",{href:!0});var $u=i(cr);ll=r($u,"BertTokenizerFast"),$u.forEach(t),cl=r(In,` and runs end-to-end tokenization:
punctuation splitting and wordpiece. The difference is that is has three inputs strings: question, titles and texts
that are combined to be fed to the `),pr=a(In,"A",{href:!0});var xu=i(pr);pl=r(xu,"DPRReader"),xu.forEach(t),hl=r(In," model."),In.forEach(t),ul=c(Ye),Lo=a(Ye,"P",{});var La=i(Lo);fl=r(La,"Refer to superclass "),hr=a(La,"A",{href:!0});var zu=i(hr);ml=r(zu,"BertTokenizerFast"),zu.forEach(t),gl=r(La," for usage examples and documentation concerning parameters."),La.forEach(t),_l=c(Ye),et=a(Ye,"P",{});var fo=i(et);vl=r(fo,"Return a dictionary with the token ids of the input strings and other information to give to "),os=a(fo,"CODE",{});var qu=i(os);bl=r(qu,".decode_best_spans"),qu.forEach(t),Tl=r(fo,`.
It converts the strings of a question and different passages (title and text) in a sequence of IDs (integers),
using the tokenizer and vocabulary. The resulting `),ns=a(fo,"CODE",{});var Fu=i(ns);kl=r(Fu,"input_ids"),Fu.forEach(t),Pl=r(fo," is a matrix of size "),rs=a(fo,"CODE",{});var Cu=i(rs);wl=r(Cu,"(n_passages, sequence_length)"),Cu.forEach(t),El=r(fo,`
with the format:`),fo.forEach(t),Rl=c(Ye),ss=a(Ye,"P",{});var Au=i(ss);Dl=r(Au,"[CLS] <question token ids> [SEP] <titles ids> [SEP] <texts ids>"),Au.forEach(t),Ye.forEach(t),ia=c(o),vt=a(o,"H2",{class:!0});var Ia=i(vt);Wt=a(Ia,"A",{id:!0,class:!0,href:!0});var Ou=i(Wt);as=a(Ou,"SPAN",{});var Nu=i(as);T(Io.$$.fragment,Nu),Nu.forEach(t),Ou.forEach(t),yl=c(Ia),is=a(Ia,"SPAN",{});var ju=i(is);$l=r(ju,"DPR specific outputs"),ju.forEach(t),Ia.forEach(t),da=c(o),bt=a(o,"DIV",{class:!0});var Sa=i(bt);T(So.$$.fragment,Sa),xl=c(Sa),Ho=a(Sa,"P",{});var Ha=i(Ho);zl=r(Ha,"Class for outputs of "),ur=a(Ha,"A",{href:!0});var Mu=i(ur);ql=r(Mu,"DPRQuestionEncoder"),Mu.forEach(t),Fl=r(Ha,"."),Ha.forEach(t),Sa.forEach(t),la=c(o),Tt=a(o,"DIV",{class:!0});var Ba=i(Tt);T(Bo.$$.fragment,Ba),Cl=c(Ba),Wo=a(Ba,"P",{});var Wa=i(Wo);Al=r(Wa,"Class for outputs of "),fr=a(Wa,"A",{href:!0});var Qu=i(fr);Ol=r(Qu,"DPRQuestionEncoder"),Qu.forEach(t),Nl=r(Wa,"."),Wa.forEach(t),Ba.forEach(t),ca=c(o),kt=a(o,"DIV",{class:!0});var Ua=i(kt);T(Uo.$$.fragment,Ua),jl=c(Ua),Vo=a(Ua,"P",{});var Va=i(Vo);Ml=r(Va,"Class for outputs of "),mr=a(Va,"A",{href:!0});var Lu=i(mr);Ql=r(Lu,"DPRQuestionEncoder"),Lu.forEach(t),Ll=r(Va,"."),Va.forEach(t),Ua.forEach(t),pa=c(o),Pt=a(o,"H2",{class:!0});var Ka=i(Pt);Ut=a(Ka,"A",{id:!0,class:!0,href:!0});var Iu=i(Ut);ds=a(Iu,"SPAN",{});var Su=i(ds);T(Ko.$$.fragment,Su),Su.forEach(t),Iu.forEach(t),Il=c(Ka),ls=a(Ka,"SPAN",{});var Hu=i(ls);Sl=r(Hu,"DPRContextEncoder"),Hu.forEach(t),Ka.forEach(t),ha=c(o),De=a(o,"DIV",{class:!0});var nt=i(De);T(Yo.$$.fragment,nt),Hl=c(nt),cs=a(nt,"P",{});var Bu=i(cs);Bl=r(Bu,"The bare DPRContextEncoder transformer outputting pooler outputs as context representations."),Bu.forEach(t),Wl=c(nt),Xo=a(nt,"P",{});var Ya=i(Xo);Ul=r(Ya,"This model inherits from "),gr=a(Ya,"A",{href:!0});var Wu=i(gr);Vl=r(Wu,"PreTrainedModel"),Wu.forEach(t),Kl=r(Ya,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Ya.forEach(t),Yl=c(nt),Jo=a(nt,"P",{});var Xa=i(Jo);Xl=r(Xa,"This model is also a PyTorch "),Go=a(Xa,"A",{href:!0,rel:!0});var Uu=i(Go);Jl=r(Uu,"torch.nn.Module"),Uu.forEach(t),Gl=r(Xa,` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),Xa.forEach(t),Zl=c(nt),ze=a(nt,"DIV",{class:!0});var rt=i(ze);T(Zo.$$.fragment,rt),ec=c(rt),wt=a(rt,"P",{});var xr=i(wt);tc=r(xr,"The "),_r=a(xr,"A",{href:!0});var Vu=i(_r);oc=r(Vu,"DPRContextEncoder"),Vu.forEach(t),nc=r(xr," forward method, overrides the "),ps=a(xr,"CODE",{});var Ku=i(ps);rc=r(Ku,"__call__"),Ku.forEach(t),sc=r(xr," special method."),xr.forEach(t),ac=c(rt),T(Vt.$$.fragment,rt),ic=c(rt),hs=a(rt,"P",{});var Yu=i(hs);dc=r(Yu,"Examples:"),Yu.forEach(t),lc=c(rt),T(en.$$.fragment,rt),rt.forEach(t),nt.forEach(t),ua=c(o),Et=a(o,"H2",{class:!0});var Ja=i(Et);Kt=a(Ja,"A",{id:!0,class:!0,href:!0});var Xu=i(Kt);us=a(Xu,"SPAN",{});var Ju=i(us);T(tn.$$.fragment,Ju),Ju.forEach(t),Xu.forEach(t),cc=c(Ja),fs=a(Ja,"SPAN",{});var Gu=i(fs);pc=r(Gu,"DPRQuestionEncoder"),Gu.forEach(t),Ja.forEach(t),fa=c(o),ye=a(o,"DIV",{class:!0});var st=i(ye);T(on.$$.fragment,st),hc=c(st),ms=a(st,"P",{});var Zu=i(ms);uc=r(Zu,"The bare DPRQuestionEncoder transformer outputting pooler outputs as question representations."),Zu.forEach(t),fc=c(st),nn=a(st,"P",{});var Ga=i(nn);mc=r(Ga,"This model inherits from "),vr=a(Ga,"A",{href:!0});var ef=i(vr);gc=r(ef,"PreTrainedModel"),ef.forEach(t),_c=r(Ga,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Ga.forEach(t),vc=c(st),rn=a(st,"P",{});var Za=i(rn);bc=r(Za,"This model is also a PyTorch "),sn=a(Za,"A",{href:!0,rel:!0});var tf=i(sn);Tc=r(tf,"torch.nn.Module"),tf.forEach(t),kc=r(Za,` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),Za.forEach(t),Pc=c(st),qe=a(st,"DIV",{class:!0});var at=i(qe);T(an.$$.fragment,at),wc=c(at),Rt=a(at,"P",{});var zr=i(Rt);Ec=r(zr,"The "),br=a(zr,"A",{href:!0});var of=i(br);Rc=r(of,"DPRQuestionEncoder"),of.forEach(t),Dc=r(zr," forward method, overrides the "),gs=a(zr,"CODE",{});var nf=i(gs);yc=r(nf,"__call__"),nf.forEach(t),$c=r(zr," special method."),zr.forEach(t),xc=c(at),T(Yt.$$.fragment,at),zc=c(at),_s=a(at,"P",{});var rf=i(_s);qc=r(rf,"Examples:"),rf.forEach(t),Fc=c(at),T(dn.$$.fragment,at),at.forEach(t),st.forEach(t),ma=c(o),Dt=a(o,"H2",{class:!0});var ei=i(Dt);Xt=a(ei,"A",{id:!0,class:!0,href:!0});var sf=i(Xt);vs=a(sf,"SPAN",{});var af=i(vs);T(ln.$$.fragment,af),af.forEach(t),sf.forEach(t),Cc=c(ei),bs=a(ei,"SPAN",{});var df=i(bs);Ac=r(df,"DPRReader"),df.forEach(t),ei.forEach(t),ga=c(o),$e=a(o,"DIV",{class:!0});var it=i($e);T(cn.$$.fragment,it),Oc=c(it),Ts=a(it,"P",{});var lf=i(Ts);Nc=r(lf,"The bare DPRReader transformer outputting span predictions."),lf.forEach(t),jc=c(it),pn=a(it,"P",{});var ti=i(pn);Mc=r(ti,"This model inherits from "),Tr=a(ti,"A",{href:!0});var cf=i(Tr);Qc=r(cf,"PreTrainedModel"),cf.forEach(t),Lc=r(ti,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),ti.forEach(t),Ic=c(it),hn=a(it,"P",{});var oi=i(hn);Sc=r(oi,"This model is also a PyTorch "),un=a(oi,"A",{href:!0,rel:!0});var pf=i(un);Hc=r(pf,"torch.nn.Module"),pf.forEach(t),Bc=r(oi,` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),oi.forEach(t),Wc=c(it),Fe=a(it,"DIV",{class:!0});var dt=i(Fe);T(fn.$$.fragment,dt),Uc=c(dt),yt=a(dt,"P",{});var qr=i(yt);Vc=r(qr,"The "),kr=a(qr,"A",{href:!0});var hf=i(kr);Kc=r(hf,"DPRReader"),hf.forEach(t),Yc=r(qr," forward method, overrides the "),ks=a(qr,"CODE",{});var uf=i(ks);Xc=r(uf,"__call__"),uf.forEach(t),Jc=r(qr," special method."),qr.forEach(t),Gc=c(dt),T(Jt.$$.fragment,dt),Zc=c(dt),Ps=a(dt,"P",{});var ff=i(Ps);ep=r(ff,"Examples:"),ff.forEach(t),tp=c(dt),T(mn.$$.fragment,dt),dt.forEach(t),it.forEach(t),_a=c(o),$t=a(o,"H2",{class:!0});var ni=i($t);Gt=a(ni,"A",{id:!0,class:!0,href:!0});var mf=i(Gt);ws=a(mf,"SPAN",{});var gf=i(ws);T(gn.$$.fragment,gf),gf.forEach(t),mf.forEach(t),op=c(ni),Es=a(ni,"SPAN",{});var _f=i(Es);np=r(_f,"TFDPRContextEncoder"),_f.forEach(t),ni.forEach(t),va=c(o),le=a(o,"DIV",{class:!0});var Xe=i(le);T(_n.$$.fragment,Xe),rp=c(Xe),Rs=a(Xe,"P",{});var vf=i(Rs);sp=r(vf,"The bare DPRContextEncoder transformer outputting pooler outputs as context representations."),vf.forEach(t),ap=c(Xe),vn=a(Xe,"P",{});var ri=i(vn);ip=r(ri,"This model inherits from "),Pr=a(ri,"A",{href:!0});var bf=i(Pr);dp=r(bf,"TFPreTrainedModel"),bf.forEach(t),lp=r(ri,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),ri.forEach(t),cp=c(Xe),bn=a(Xe,"P",{});var si=i(bn);pp=r(si,"This model is also a Tensorflow "),Tn=a(si,"A",{href:!0,rel:!0});var Tf=i(Tn);hp=r(Tf,"tf.keras.Model"),Tf.forEach(t),up=r(si,`
subclass. Use it as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to
general usage and behavior.`),si.forEach(t),fp=c(Xe),T(Zt.$$.fragment,Xe),mp=c(Xe),Ce=a(Xe,"DIV",{class:!0});var lt=i(Ce);T(kn.$$.fragment,lt),gp=c(lt),xt=a(lt,"P",{});var Fr=i(xt);_p=r(Fr,"The "),wr=a(Fr,"A",{href:!0});var kf=i(wr);vp=r(kf,"TFDPRContextEncoder"),kf.forEach(t),bp=r(Fr," forward method, overrides the "),Ds=a(Fr,"CODE",{});var Pf=i(Ds);Tp=r(Pf,"__call__"),Pf.forEach(t),kp=r(Fr," special method."),Fr.forEach(t),Pp=c(lt),T(eo.$$.fragment,lt),wp=c(lt),ys=a(lt,"P",{});var wf=i(ys);Ep=r(wf,"Examples:"),wf.forEach(t),Rp=c(lt),T(Pn.$$.fragment,lt),lt.forEach(t),Xe.forEach(t),ba=c(o),zt=a(o,"H2",{class:!0});var ai=i(zt);to=a(ai,"A",{id:!0,class:!0,href:!0});var Ef=i(to);$s=a(Ef,"SPAN",{});var Rf=i($s);T(wn.$$.fragment,Rf),Rf.forEach(t),Ef.forEach(t),Dp=c(ai),xs=a(ai,"SPAN",{});var Df=i(xs);yp=r(Df,"TFDPRQuestionEncoder"),Df.forEach(t),ai.forEach(t),Ta=c(o),ce=a(o,"DIV",{class:!0});var Je=i(ce);T(En.$$.fragment,Je),$p=c(Je),zs=a(Je,"P",{});var yf=i(zs);xp=r(yf,"The bare DPRQuestionEncoder transformer outputting pooler outputs as question representations."),yf.forEach(t),zp=c(Je),Rn=a(Je,"P",{});var ii=i(Rn);qp=r(ii,"This model inherits from "),Er=a(ii,"A",{href:!0});var $f=i(Er);Fp=r($f,"TFPreTrainedModel"),$f.forEach(t),Cp=r(ii,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),ii.forEach(t),Ap=c(Je),Dn=a(Je,"P",{});var di=i(Dn);Op=r(di,"This model is also a Tensorflow "),yn=a(di,"A",{href:!0,rel:!0});var xf=i(yn);Np=r(xf,"tf.keras.Model"),xf.forEach(t),jp=r(di,`
subclass. Use it as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to
general usage and behavior.`),di.forEach(t),Mp=c(Je),T(oo.$$.fragment,Je),Qp=c(Je),Ae=a(Je,"DIV",{class:!0});var ct=i(Ae);T($n.$$.fragment,ct),Lp=c(ct),qt=a(ct,"P",{});var Cr=i(qt);Ip=r(Cr,"The "),Rr=a(Cr,"A",{href:!0});var zf=i(Rr);Sp=r(zf,"TFDPRQuestionEncoder"),zf.forEach(t),Hp=r(Cr," forward method, overrides the "),qs=a(Cr,"CODE",{});var qf=i(qs);Bp=r(qf,"__call__"),qf.forEach(t),Wp=r(Cr," special method."),Cr.forEach(t),Up=c(ct),T(no.$$.fragment,ct),Vp=c(ct),Fs=a(ct,"P",{});var Ff=i(Fs);Kp=r(Ff,"Examples:"),Ff.forEach(t),Yp=c(ct),T(xn.$$.fragment,ct),ct.forEach(t),Je.forEach(t),ka=c(o),Ft=a(o,"H2",{class:!0});var li=i(Ft);ro=a(li,"A",{id:!0,class:!0,href:!0});var Cf=i(ro);Cs=a(Cf,"SPAN",{});var Af=i(Cs);T(zn.$$.fragment,Af),Af.forEach(t),Cf.forEach(t),Xp=c(li),As=a(li,"SPAN",{});var Of=i(As);Jp=r(Of,"TFDPRReader"),Of.forEach(t),li.forEach(t),Pa=c(o),pe=a(o,"DIV",{class:!0});var Ge=i(pe);T(qn.$$.fragment,Ge),Gp=c(Ge),Os=a(Ge,"P",{});var Nf=i(Os);Zp=r(Nf,"The bare DPRReader transformer outputting span predictions."),Nf.forEach(t),eh=c(Ge),Fn=a(Ge,"P",{});var ci=i(Fn);th=r(ci,"This model inherits from "),Dr=a(ci,"A",{href:!0});var jf=i(Dr);oh=r(jf,"TFPreTrainedModel"),jf.forEach(t),nh=r(ci,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),ci.forEach(t),rh=c(Ge),Cn=a(Ge,"P",{});var pi=i(Cn);sh=r(pi,"This model is also a Tensorflow "),An=a(pi,"A",{href:!0,rel:!0});var Mf=i(An);ah=r(Mf,"tf.keras.Model"),Mf.forEach(t),ih=r(pi,`
subclass. Use it as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to
general usage and behavior.`),pi.forEach(t),dh=c(Ge),T(so.$$.fragment,Ge),lh=c(Ge),Oe=a(Ge,"DIV",{class:!0});var pt=i(Oe);T(On.$$.fragment,pt),ch=c(pt),Ct=a(pt,"P",{});var Ar=i(Ct);ph=r(Ar,"The "),yr=a(Ar,"A",{href:!0});var Qf=i(yr);hh=r(Qf,"TFDPRReader"),Qf.forEach(t),uh=r(Ar," forward method, overrides the "),Ns=a(Ar,"CODE",{});var Lf=i(Ns);fh=r(Lf,"__call__"),Lf.forEach(t),mh=r(Ar," special method."),Ar.forEach(t),gh=c(pt),T(ao.$$.fragment,pt),_h=c(pt),js=a(pt,"P",{});var If=i(js);vh=r(If,"Examples:"),If.forEach(t),bh=c(pt),T(Nn.$$.fragment,pt),pt.forEach(t),Ge.forEach(t),this.h()},h(){d(f,"name","hf:doc:metadata"),d(f,"content",JSON.stringify(nm)),d(g,"id","dpr"),d(g,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(g,"href","#dpr"),d(m,"class","relative group"),d(J,"id","overview"),d(J,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(J,"href","#overview"),d(D,"class","relative group"),d(te,"href","https://arxiv.org/abs/2004.04906"),d(te,"rel","nofollow"),d(j,"href","https://huggingface.co/lhoestq"),d(j,"rel","nofollow"),d(M,"href","https://github.com/facebookresearch/DPR"),d(M,"rel","nofollow"),d(N,"id","transformers.DPRConfig"),d(N,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(N,"href","#transformers.DPRConfig"),d(V,"class","relative group"),d(O,"href","/docs/transformers/pr_16900/en/model_doc/dpr#transformers.DPRConfig"),d(Hn,"href","/docs/transformers/pr_16900/en/model_doc/dpr#transformers.DPRContextEncoder"),d(Bn,"href","/docs/transformers/pr_16900/en/model_doc/dpr#transformers.DPRQuestionEncoder"),d(Wn,"href","/docs/transformers/pr_16900/en/model_doc/dpr#transformers.DPRReader"),d(mo,"href","https://huggingface.co/facebook/dpr-ctx_encoder-single-nq-base"),d(mo,"rel","nofollow"),d(go,"href","https://huggingface.co/facebook/dpr-question_encoder-single-nq-base"),d(go,"rel","nofollow"),d(_o,"href","https://huggingface.co/facebook/dpr-reader-single-nq-base"),d(_o,"rel","nofollow"),d(Un,"href","/docs/transformers/pr_16900/en/model_doc/bert#transformers.BertConfig"),d(F,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Ot,"id","transformers.DPRContextEncoderTokenizer"),d(Ot,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Ot,"href","#transformers.DPRContextEncoderTokenizer"),d(ht,"class","relative group"),d(Vn,"href","/docs/transformers/pr_16900/en/model_doc/dpr#transformers.DPRContextEncoderTokenizer"),d(Kn,"href","/docs/transformers/pr_16900/en/model_doc/bert#transformers.BertTokenizer"),d(Yn,"href","/docs/transformers/pr_16900/en/model_doc/bert#transformers.BertTokenizer"),d(Me,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(jt,"id","transformers.DPRContextEncoderTokenizerFast"),d(jt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(jt,"href","#transformers.DPRContextEncoderTokenizerFast"),d(ut,"class","relative group"),d(Xn,"href","/docs/transformers/pr_16900/en/model_doc/dpr#transformers.DPRContextEncoderTokenizerFast"),d(Jn,"href","/docs/transformers/pr_16900/en/model_doc/bert#transformers.BertTokenizerFast"),d(Gn,"href","/docs/transformers/pr_16900/en/model_doc/bert#transformers.BertTokenizerFast"),d(Qe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Qt,"id","transformers.DPRQuestionEncoderTokenizer"),d(Qt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Qt,"href","#transformers.DPRQuestionEncoderTokenizer"),d(ft,"class","relative group"),d(Zn,"href","/docs/transformers/pr_16900/en/model_doc/dpr#transformers.DPRQuestionEncoderTokenizer"),d(er,"href","/docs/transformers/pr_16900/en/model_doc/bert#transformers.BertTokenizer"),d(tr,"href","/docs/transformers/pr_16900/en/model_doc/bert#transformers.BertTokenizer"),d(Le,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(It,"id","transformers.DPRQuestionEncoderTokenizerFast"),d(It,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(It,"href","#transformers.DPRQuestionEncoderTokenizerFast"),d(mt,"class","relative group"),d(or,"href","/docs/transformers/pr_16900/en/model_doc/dpr#transformers.DPRQuestionEncoderTokenizerFast"),d(nr,"href","/docs/transformers/pr_16900/en/model_doc/bert#transformers.BertTokenizerFast"),d(rr,"href","/docs/transformers/pr_16900/en/model_doc/bert#transformers.BertTokenizerFast"),d(Ie,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Ht,"id","transformers.DPRReaderTokenizer"),d(Ht,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Ht,"href","#transformers.DPRReaderTokenizer"),d(gt,"class","relative group"),d(sr,"href","/docs/transformers/pr_16900/en/model_doc/dpr#transformers.DPRReaderTokenizer"),d(ar,"href","/docs/transformers/pr_16900/en/model_doc/bert#transformers.BertTokenizer"),d(ir,"href","/docs/transformers/pr_16900/en/model_doc/dpr#transformers.DPRReader"),d(dr,"href","/docs/transformers/pr_16900/en/model_doc/bert#transformers.BertTokenizer"),d(ie,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Bt,"id","transformers.DPRReaderTokenizerFast"),d(Bt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Bt,"href","#transformers.DPRReaderTokenizerFast"),d(_t,"class","relative group"),d(lr,"href","/docs/transformers/pr_16900/en/model_doc/dpr#transformers.DPRReaderTokenizerFast"),d(cr,"href","/docs/transformers/pr_16900/en/model_doc/bert#transformers.BertTokenizerFast"),d(pr,"href","/docs/transformers/pr_16900/en/model_doc/dpr#transformers.DPRReader"),d(hr,"href","/docs/transformers/pr_16900/en/model_doc/bert#transformers.BertTokenizerFast"),d(de,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Wt,"id","transformers.models.dpr.modeling_dpr.DPRContextEncoderOutput"),d(Wt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Wt,"href","#transformers.models.dpr.modeling_dpr.DPRContextEncoderOutput"),d(vt,"class","relative group"),d(ur,"href","/docs/transformers/pr_16900/en/model_doc/dpr#transformers.DPRQuestionEncoder"),d(bt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(fr,"href","/docs/transformers/pr_16900/en/model_doc/dpr#transformers.DPRQuestionEncoder"),d(Tt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(mr,"href","/docs/transformers/pr_16900/en/model_doc/dpr#transformers.DPRQuestionEncoder"),d(kt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Ut,"id","transformers.DPRContextEncoder"),d(Ut,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Ut,"href","#transformers.DPRContextEncoder"),d(Pt,"class","relative group"),d(gr,"href","/docs/transformers/pr_16900/en/main_classes/model#transformers.PreTrainedModel"),d(Go,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),d(Go,"rel","nofollow"),d(_r,"href","/docs/transformers/pr_16900/en/model_doc/dpr#transformers.DPRContextEncoder"),d(ze,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(De,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Kt,"id","transformers.DPRQuestionEncoder"),d(Kt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Kt,"href","#transformers.DPRQuestionEncoder"),d(Et,"class","relative group"),d(vr,"href","/docs/transformers/pr_16900/en/main_classes/model#transformers.PreTrainedModel"),d(sn,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),d(sn,"rel","nofollow"),d(br,"href","/docs/transformers/pr_16900/en/model_doc/dpr#transformers.DPRQuestionEncoder"),d(qe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(ye,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Xt,"id","transformers.DPRReader"),d(Xt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Xt,"href","#transformers.DPRReader"),d(Dt,"class","relative group"),d(Tr,"href","/docs/transformers/pr_16900/en/main_classes/model#transformers.PreTrainedModel"),d(un,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),d(un,"rel","nofollow"),d(kr,"href","/docs/transformers/pr_16900/en/model_doc/dpr#transformers.DPRReader"),d(Fe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d($e,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Gt,"id","transformers.TFDPRContextEncoder"),d(Gt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Gt,"href","#transformers.TFDPRContextEncoder"),d($t,"class","relative group"),d(Pr,"href","/docs/transformers/pr_16900/en/main_classes/model#transformers.TFPreTrainedModel"),d(Tn,"href","https://www.tensorflow.org/api_docs/python/tf/keras/Model"),d(Tn,"rel","nofollow"),d(wr,"href","/docs/transformers/pr_16900/en/model_doc/dpr#transformers.TFDPRContextEncoder"),d(Ce,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(le,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(to,"id","transformers.TFDPRQuestionEncoder"),d(to,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(to,"href","#transformers.TFDPRQuestionEncoder"),d(zt,"class","relative group"),d(Er,"href","/docs/transformers/pr_16900/en/main_classes/model#transformers.TFPreTrainedModel"),d(yn,"href","https://www.tensorflow.org/api_docs/python/tf/keras/Model"),d(yn,"rel","nofollow"),d(Rr,"href","/docs/transformers/pr_16900/en/model_doc/dpr#transformers.TFDPRQuestionEncoder"),d(Ae,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(ce,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(ro,"id","transformers.TFDPRReader"),d(ro,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(ro,"href","#transformers.TFDPRReader"),d(Ft,"class","relative group"),d(Dr,"href","/docs/transformers/pr_16900/en/main_classes/model#transformers.TFPreTrainedModel"),d(An,"href","https://www.tensorflow.org/api_docs/python/tf/keras/Model"),d(An,"rel","nofollow"),d(yr,"href","/docs/transformers/pr_16900/en/model_doc/dpr#transformers.TFDPRReader"),d(Oe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(pe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(o,u){e(document.head,f),p(o,x,u),p(o,m,u),e(m,g),e(g,R),k(v,R,null),e(m,_),e(m,$),e($,_e),p(o,Z,u),p(o,D,u),e(D,J),e(J,I),k(ee,I,null),e(D,ve),e(D,S),e(S,be),p(o,he,u),p(o,H,u),e(H,L),e(H,te),e(te,oe),e(H,q),p(o,C,u),p(o,ne,u),e(ne,U),p(o,ue,u),p(o,re,u),e(re,B),e(B,Te),p(o,fe,u),p(o,z,u),e(z,ke),e(z,j),e(j,Pe),e(z,we),e(z,M),e(M,Ee),e(z,Re),p(o,Q,u),p(o,V,u),e(V,N),e(N,se),k(h,se,null),e(V,y),e(V,G),e(G,Se),p(o,je,u),p(o,F,u),k(me,F,null),e(F,He),e(F,ae),e(ae,O),e(O,K),e(ae,Be),e(ae,xe),e(xe,Y),e(ae,We),e(F,Ue),e(F,A),e(A,Ve),e(A,Hn),e(Hn,hi),e(A,ui),e(A,Bn),e(Bn,fi),e(A,mi),e(A,Wn),e(Wn,gi),e(A,_i),e(A,mo),e(mo,vi),e(A,bi),e(A,go),e(go,Ti),e(A,ki),e(A,_o),e(_o,Pi),e(A,wi),e(F,Ei),e(F,vo),e(vo,Ri),e(vo,Un),e(Un,Di),e(vo,yi),p(o,Ys,u),p(o,ht,u),e(ht,Ot),e(Ot,Or),k(bo,Or,null),e(ht,$i),e(ht,Nr),e(Nr,xi),p(o,Xs,u),p(o,Me,u),k(To,Me,null),e(Me,zi),e(Me,jr),e(jr,qi),e(Me,Fi),e(Me,Nt),e(Nt,Vn),e(Vn,Ci),e(Nt,Ai),e(Nt,Kn),e(Kn,Oi),e(Nt,Ni),e(Me,ji),e(Me,ko),e(ko,Mi),e(ko,Yn),e(Yn,Qi),e(ko,Li),p(o,Js,u),p(o,ut,u),e(ut,jt),e(jt,Mr),k(Po,Mr,null),e(ut,Ii),e(ut,Qr),e(Qr,Si),p(o,Gs,u),p(o,Qe,u),k(wo,Qe,null),e(Qe,Hi),e(Qe,Eo),e(Eo,Bi),e(Eo,Lr),e(Lr,Wi),e(Eo,Ui),e(Qe,Vi),e(Qe,Mt),e(Mt,Xn),e(Xn,Ki),e(Mt,Yi),e(Mt,Jn),e(Jn,Xi),e(Mt,Ji),e(Qe,Gi),e(Qe,Ro),e(Ro,Zi),e(Ro,Gn),e(Gn,ed),e(Ro,td),p(o,Zs,u),p(o,ft,u),e(ft,Qt),e(Qt,Ir),k(Do,Ir,null),e(ft,od),e(ft,Sr),e(Sr,nd),p(o,ea,u),p(o,Le,u),k(yo,Le,null),e(Le,rd),e(Le,Hr),e(Hr,sd),e(Le,ad),e(Le,Lt),e(Lt,Zn),e(Zn,id),e(Lt,dd),e(Lt,er),e(er,ld),e(Lt,cd),e(Le,pd),e(Le,$o),e($o,hd),e($o,tr),e(tr,ud),e($o,fd),p(o,ta,u),p(o,mt,u),e(mt,It),e(It,Br),k(xo,Br,null),e(mt,md),e(mt,Wr),e(Wr,gd),p(o,oa,u),p(o,Ie,u),k(zo,Ie,null),e(Ie,_d),e(Ie,qo),e(qo,vd),e(qo,Ur),e(Ur,bd),e(qo,Td),e(Ie,kd),e(Ie,St),e(St,or),e(or,Pd),e(St,wd),e(St,nr),e(nr,Ed),e(St,Rd),e(Ie,Dd),e(Ie,Fo),e(Fo,yd),e(Fo,rr),e(rr,$d),e(Fo,xd),p(o,na,u),p(o,gt,u),e(gt,Ht),e(Ht,Vr),k(Co,Vr,null),e(gt,zd),e(gt,Kr),e(Kr,qd),p(o,ra,u),p(o,ie,u),k(Ao,ie,null),e(ie,Fd),e(ie,Yr),e(Yr,Cd),e(ie,Ad),e(ie,tt),e(tt,sr),e(sr,Od),e(tt,Nd),e(tt,ar),e(ar,jd),e(tt,Md),e(tt,ir),e(ir,Qd),e(tt,Ld),e(ie,Id),e(ie,Oo),e(Oo,Sd),e(Oo,dr),e(dr,Hd),e(Oo,Bd),e(ie,Wd),e(ie,Ze),e(Ze,Ud),e(Ze,Xr),e(Xr,Vd),e(Ze,Kd),e(Ze,Jr),e(Jr,Yd),e(Ze,Xd),e(Ze,Gr),e(Gr,Jd),e(Ze,Gd),e(ie,Zd),k(No,ie,null),p(o,sa,u),p(o,_t,u),e(_t,Bt),e(Bt,Zr),k(jo,Zr,null),e(_t,el),e(_t,es),e(es,tl),p(o,aa,u),p(o,de,u),k(Mo,de,null),e(de,ol),e(de,Qo),e(Qo,nl),e(Qo,ts),e(ts,rl),e(Qo,sl),e(de,al),e(de,ot),e(ot,lr),e(lr,il),e(ot,dl),e(ot,cr),e(cr,ll),e(ot,cl),e(ot,pr),e(pr,pl),e(ot,hl),e(de,ul),e(de,Lo),e(Lo,fl),e(Lo,hr),e(hr,ml),e(Lo,gl),e(de,_l),e(de,et),e(et,vl),e(et,os),e(os,bl),e(et,Tl),e(et,ns),e(ns,kl),e(et,Pl),e(et,rs),e(rs,wl),e(et,El),e(de,Rl),e(de,ss),e(ss,Dl),p(o,ia,u),p(o,vt,u),e(vt,Wt),e(Wt,as),k(Io,as,null),e(vt,yl),e(vt,is),e(is,$l),p(o,da,u),p(o,bt,u),k(So,bt,null),e(bt,xl),e(bt,Ho),e(Ho,zl),e(Ho,ur),e(ur,ql),e(Ho,Fl),p(o,la,u),p(o,Tt,u),k(Bo,Tt,null),e(Tt,Cl),e(Tt,Wo),e(Wo,Al),e(Wo,fr),e(fr,Ol),e(Wo,Nl),p(o,ca,u),p(o,kt,u),k(Uo,kt,null),e(kt,jl),e(kt,Vo),e(Vo,Ml),e(Vo,mr),e(mr,Ql),e(Vo,Ll),p(o,pa,u),p(o,Pt,u),e(Pt,Ut),e(Ut,ds),k(Ko,ds,null),e(Pt,Il),e(Pt,ls),e(ls,Sl),p(o,ha,u),p(o,De,u),k(Yo,De,null),e(De,Hl),e(De,cs),e(cs,Bl),e(De,Wl),e(De,Xo),e(Xo,Ul),e(Xo,gr),e(gr,Vl),e(Xo,Kl),e(De,Yl),e(De,Jo),e(Jo,Xl),e(Jo,Go),e(Go,Jl),e(Jo,Gl),e(De,Zl),e(De,ze),k(Zo,ze,null),e(ze,ec),e(ze,wt),e(wt,tc),e(wt,_r),e(_r,oc),e(wt,nc),e(wt,ps),e(ps,rc),e(wt,sc),e(ze,ac),k(Vt,ze,null),e(ze,ic),e(ze,hs),e(hs,dc),e(ze,lc),k(en,ze,null),p(o,ua,u),p(o,Et,u),e(Et,Kt),e(Kt,us),k(tn,us,null),e(Et,cc),e(Et,fs),e(fs,pc),p(o,fa,u),p(o,ye,u),k(on,ye,null),e(ye,hc),e(ye,ms),e(ms,uc),e(ye,fc),e(ye,nn),e(nn,mc),e(nn,vr),e(vr,gc),e(nn,_c),e(ye,vc),e(ye,rn),e(rn,bc),e(rn,sn),e(sn,Tc),e(rn,kc),e(ye,Pc),e(ye,qe),k(an,qe,null),e(qe,wc),e(qe,Rt),e(Rt,Ec),e(Rt,br),e(br,Rc),e(Rt,Dc),e(Rt,gs),e(gs,yc),e(Rt,$c),e(qe,xc),k(Yt,qe,null),e(qe,zc),e(qe,_s),e(_s,qc),e(qe,Fc),k(dn,qe,null),p(o,ma,u),p(o,Dt,u),e(Dt,Xt),e(Xt,vs),k(ln,vs,null),e(Dt,Cc),e(Dt,bs),e(bs,Ac),p(o,ga,u),p(o,$e,u),k(cn,$e,null),e($e,Oc),e($e,Ts),e(Ts,Nc),e($e,jc),e($e,pn),e(pn,Mc),e(pn,Tr),e(Tr,Qc),e(pn,Lc),e($e,Ic),e($e,hn),e(hn,Sc),e(hn,un),e(un,Hc),e(hn,Bc),e($e,Wc),e($e,Fe),k(fn,Fe,null),e(Fe,Uc),e(Fe,yt),e(yt,Vc),e(yt,kr),e(kr,Kc),e(yt,Yc),e(yt,ks),e(ks,Xc),e(yt,Jc),e(Fe,Gc),k(Jt,Fe,null),e(Fe,Zc),e(Fe,Ps),e(Ps,ep),e(Fe,tp),k(mn,Fe,null),p(o,_a,u),p(o,$t,u),e($t,Gt),e(Gt,ws),k(gn,ws,null),e($t,op),e($t,Es),e(Es,np),p(o,va,u),p(o,le,u),k(_n,le,null),e(le,rp),e(le,Rs),e(Rs,sp),e(le,ap),e(le,vn),e(vn,ip),e(vn,Pr),e(Pr,dp),e(vn,lp),e(le,cp),e(le,bn),e(bn,pp),e(bn,Tn),e(Tn,hp),e(bn,up),e(le,fp),k(Zt,le,null),e(le,mp),e(le,Ce),k(kn,Ce,null),e(Ce,gp),e(Ce,xt),e(xt,_p),e(xt,wr),e(wr,vp),e(xt,bp),e(xt,Ds),e(Ds,Tp),e(xt,kp),e(Ce,Pp),k(eo,Ce,null),e(Ce,wp),e(Ce,ys),e(ys,Ep),e(Ce,Rp),k(Pn,Ce,null),p(o,ba,u),p(o,zt,u),e(zt,to),e(to,$s),k(wn,$s,null),e(zt,Dp),e(zt,xs),e(xs,yp),p(o,Ta,u),p(o,ce,u),k(En,ce,null),e(ce,$p),e(ce,zs),e(zs,xp),e(ce,zp),e(ce,Rn),e(Rn,qp),e(Rn,Er),e(Er,Fp),e(Rn,Cp),e(ce,Ap),e(ce,Dn),e(Dn,Op),e(Dn,yn),e(yn,Np),e(Dn,jp),e(ce,Mp),k(oo,ce,null),e(ce,Qp),e(ce,Ae),k($n,Ae,null),e(Ae,Lp),e(Ae,qt),e(qt,Ip),e(qt,Rr),e(Rr,Sp),e(qt,Hp),e(qt,qs),e(qs,Bp),e(qt,Wp),e(Ae,Up),k(no,Ae,null),e(Ae,Vp),e(Ae,Fs),e(Fs,Kp),e(Ae,Yp),k(xn,Ae,null),p(o,ka,u),p(o,Ft,u),e(Ft,ro),e(ro,Cs),k(zn,Cs,null),e(Ft,Xp),e(Ft,As),e(As,Jp),p(o,Pa,u),p(o,pe,u),k(qn,pe,null),e(pe,Gp),e(pe,Os),e(Os,Zp),e(pe,eh),e(pe,Fn),e(Fn,th),e(Fn,Dr),e(Dr,oh),e(Fn,nh),e(pe,rh),e(pe,Cn),e(Cn,sh),e(Cn,An),e(An,ah),e(Cn,ih),e(pe,dh),k(so,pe,null),e(pe,lh),e(pe,Oe),k(On,Oe,null),e(Oe,ch),e(Oe,Ct),e(Ct,ph),e(Ct,yr),e(yr,hh),e(Ct,uh),e(Ct,Ns),e(Ns,fh),e(Ct,mh),e(Oe,gh),k(ao,Oe,null),e(Oe,_h),e(Oe,js),e(js,vh),e(Oe,bh),k(Nn,Oe,null),wa=!0},p(o,[u]){const jn={};u&2&&(jn.$$scope={dirty:u,ctx:o}),Vt.$set(jn);const Ms={};u&2&&(Ms.$$scope={dirty:u,ctx:o}),Yt.$set(Ms);const Qs={};u&2&&(Qs.$$scope={dirty:u,ctx:o}),Jt.$set(Qs);const Ls={};u&2&&(Ls.$$scope={dirty:u,ctx:o}),Zt.$set(Ls);const Mn={};u&2&&(Mn.$$scope={dirty:u,ctx:o}),eo.$set(Mn);const Is={};u&2&&(Is.$$scope={dirty:u,ctx:o}),oo.$set(Is);const Ss={};u&2&&(Ss.$$scope={dirty:u,ctx:o}),no.$set(Ss);const Hs={};u&2&&(Hs.$$scope={dirty:u,ctx:o}),so.$set(Hs);const Qn={};u&2&&(Qn.$$scope={dirty:u,ctx:o}),ao.$set(Qn)},i(o){wa||(P(v.$$.fragment,o),P(ee.$$.fragment,o),P(h.$$.fragment,o),P(me.$$.fragment,o),P(bo.$$.fragment,o),P(To.$$.fragment,o),P(Po.$$.fragment,o),P(wo.$$.fragment,o),P(Do.$$.fragment,o),P(yo.$$.fragment,o),P(xo.$$.fragment,o),P(zo.$$.fragment,o),P(Co.$$.fragment,o),P(Ao.$$.fragment,o),P(No.$$.fragment,o),P(jo.$$.fragment,o),P(Mo.$$.fragment,o),P(Io.$$.fragment,o),P(So.$$.fragment,o),P(Bo.$$.fragment,o),P(Uo.$$.fragment,o),P(Ko.$$.fragment,o),P(Yo.$$.fragment,o),P(Zo.$$.fragment,o),P(Vt.$$.fragment,o),P(en.$$.fragment,o),P(tn.$$.fragment,o),P(on.$$.fragment,o),P(an.$$.fragment,o),P(Yt.$$.fragment,o),P(dn.$$.fragment,o),P(ln.$$.fragment,o),P(cn.$$.fragment,o),P(fn.$$.fragment,o),P(Jt.$$.fragment,o),P(mn.$$.fragment,o),P(gn.$$.fragment,o),P(_n.$$.fragment,o),P(Zt.$$.fragment,o),P(kn.$$.fragment,o),P(eo.$$.fragment,o),P(Pn.$$.fragment,o),P(wn.$$.fragment,o),P(En.$$.fragment,o),P(oo.$$.fragment,o),P($n.$$.fragment,o),P(no.$$.fragment,o),P(xn.$$.fragment,o),P(zn.$$.fragment,o),P(qn.$$.fragment,o),P(so.$$.fragment,o),P(On.$$.fragment,o),P(ao.$$.fragment,o),P(Nn.$$.fragment,o),wa=!0)},o(o){w(v.$$.fragment,o),w(ee.$$.fragment,o),w(h.$$.fragment,o),w(me.$$.fragment,o),w(bo.$$.fragment,o),w(To.$$.fragment,o),w(Po.$$.fragment,o),w(wo.$$.fragment,o),w(Do.$$.fragment,o),w(yo.$$.fragment,o),w(xo.$$.fragment,o),w(zo.$$.fragment,o),w(Co.$$.fragment,o),w(Ao.$$.fragment,o),w(No.$$.fragment,o),w(jo.$$.fragment,o),w(Mo.$$.fragment,o),w(Io.$$.fragment,o),w(So.$$.fragment,o),w(Bo.$$.fragment,o),w(Uo.$$.fragment,o),w(Ko.$$.fragment,o),w(Yo.$$.fragment,o),w(Zo.$$.fragment,o),w(Vt.$$.fragment,o),w(en.$$.fragment,o),w(tn.$$.fragment,o),w(on.$$.fragment,o),w(an.$$.fragment,o),w(Yt.$$.fragment,o),w(dn.$$.fragment,o),w(ln.$$.fragment,o),w(cn.$$.fragment,o),w(fn.$$.fragment,o),w(Jt.$$.fragment,o),w(mn.$$.fragment,o),w(gn.$$.fragment,o),w(_n.$$.fragment,o),w(Zt.$$.fragment,o),w(kn.$$.fragment,o),w(eo.$$.fragment,o),w(Pn.$$.fragment,o),w(wn.$$.fragment,o),w(En.$$.fragment,o),w(oo.$$.fragment,o),w($n.$$.fragment,o),w(no.$$.fragment,o),w(xn.$$.fragment,o),w(zn.$$.fragment,o),w(qn.$$.fragment,o),w(so.$$.fragment,o),w(On.$$.fragment,o),w(ao.$$.fragment,o),w(Nn.$$.fragment,o),wa=!1},d(o){t(f),o&&t(x),o&&t(m),E(v),o&&t(Z),o&&t(D),E(ee),o&&t(he),o&&t(H),o&&t(C),o&&t(ne),o&&t(ue),o&&t(re),o&&t(fe),o&&t(z),o&&t(Q),o&&t(V),E(h),o&&t(je),o&&t(F),E(me),o&&t(Ys),o&&t(ht),E(bo),o&&t(Xs),o&&t(Me),E(To),o&&t(Js),o&&t(ut),E(Po),o&&t(Gs),o&&t(Qe),E(wo),o&&t(Zs),o&&t(ft),E(Do),o&&t(ea),o&&t(Le),E(yo),o&&t(ta),o&&t(mt),E(xo),o&&t(oa),o&&t(Ie),E(zo),o&&t(na),o&&t(gt),E(Co),o&&t(ra),o&&t(ie),E(Ao),E(No),o&&t(sa),o&&t(_t),E(jo),o&&t(aa),o&&t(de),E(Mo),o&&t(ia),o&&t(vt),E(Io),o&&t(da),o&&t(bt),E(So),o&&t(la),o&&t(Tt),E(Bo),o&&t(ca),o&&t(kt),E(Uo),o&&t(pa),o&&t(Pt),E(Ko),o&&t(ha),o&&t(De),E(Yo),E(Zo),E(Vt),E(en),o&&t(ua),o&&t(Et),E(tn),o&&t(fa),o&&t(ye),E(on),E(an),E(Yt),E(dn),o&&t(ma),o&&t(Dt),E(ln),o&&t(ga),o&&t($e),E(cn),E(fn),E(Jt),E(mn),o&&t(_a),o&&t($t),E(gn),o&&t(va),o&&t(le),E(_n),E(Zt),E(kn),E(eo),E(Pn),o&&t(ba),o&&t(zt),E(wn),o&&t(Ta),o&&t(ce),E(En),E(oo),E($n),E(no),E(xn),o&&t(ka),o&&t(Ft),E(zn),o&&t(Pa),o&&t(pe),E(qn),E(so),E(On),E(ao),E(Nn)}}}const nm={local:"dpr",sections:[{local:"overview",title:"Overview"},{local:"transformers.DPRConfig",title:"DPRConfig"},{local:"transformers.DPRContextEncoderTokenizer",title:"DPRContextEncoderTokenizer"},{local:"transformers.DPRContextEncoderTokenizerFast",title:"DPRContextEncoderTokenizerFast"},{local:"transformers.DPRQuestionEncoderTokenizer",title:"DPRQuestionEncoderTokenizer"},{local:"transformers.DPRQuestionEncoderTokenizerFast",title:"DPRQuestionEncoderTokenizerFast"},{local:"transformers.DPRReaderTokenizer",title:"DPRReaderTokenizer"},{local:"transformers.DPRReaderTokenizerFast",title:"DPRReaderTokenizerFast"},{local:"transformers.models.dpr.modeling_dpr.DPRContextEncoderOutput",title:"DPR specific outputs"},{local:"transformers.DPRContextEncoder",title:"DPRContextEncoder"},{local:"transformers.DPRQuestionEncoder",title:"DPRQuestionEncoder"},{local:"transformers.DPRReader",title:"DPRReader"},{local:"transformers.TFDPRContextEncoder",title:"TFDPRContextEncoder"},{local:"transformers.TFDPRQuestionEncoder",title:"TFDPRQuestionEncoder"},{local:"transformers.TFDPRReader",title:"TFDPRReader"}],title:"DPR"};function rm(X){return Uf(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class cm extends Sf{constructor(f){super();Hf(this,f,rm,om,Bf,{})}}export{cm as default,nm as metadata};
