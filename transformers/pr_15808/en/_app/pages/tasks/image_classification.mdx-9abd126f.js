import{S as Br,i as Wr,s as Yr,e as o,k as f,w as _,t as l,M as Gr,c as n,d as a,m,a as i,x as g,h as r,b as h,F as t,g as p,y as $,q as b,o as v,B as j}from"../../chunks/vendor-4833417e.js";import{T as ss}from"../../chunks/Tip-fffd6df1.js";import{Y as Jr}from"../../chunks/Youtube-27813aed.js";import{I as Ua}from"../../chunks/IconCopyLink-4b81c553.js";import{C as x}from"../../chunks/CodeBlock-90ffda97.js";import"../../chunks/CopyButton-04a16537.js";function Kr(P){let c,E,u,y,k;return{c(){c=o("p"),E=l("See the image classification "),u=o("a"),y=l("task page"),k=l(" for more information about its associated models, datasets, and metrics."),this.h()},l(d){c=n(d,"P",{});var w=i(c);E=r(w,"See the image classification "),u=n(w,"A",{href:!0,rel:!0});var T=i(u);y=r(T,"task page"),T.forEach(a),k=r(w," for more information about its associated models, datasets, and metrics."),w.forEach(a),this.h()},h(){h(u,"href","https://huggingface.co/tasks/audio-classification"),h(u,"rel","nofollow")},m(d,w){p(d,c,w),t(c,E),t(c,u),t(u,y),t(c,k)},d(d){d&&a(c)}}}function Qr(P){let c,E,u,y,k,d,w,T;return{c(){c=o("p"),E=l("If you aren\u2019t familiar with fine-tuning a model with the "),u=o("a"),y=l("Trainer"),k=l(", take a look at the basic tutorial "),d=o("a"),w=l("here"),T=l("!"),this.h()},l(M){c=n(M,"P",{});var C=i(c);E=r(C,"If you aren\u2019t familiar with fine-tuning a model with the "),u=n(C,"A",{href:!0});var z=i(u);y=r(z,"Trainer"),z.forEach(a),k=r(C,", take a look at the basic tutorial "),d=n(C,"A",{href:!0});var pe=i(d);w=r(pe,"here"),pe.forEach(a),T=r(C,"!"),C.forEach(a),this.h()},h(){h(u,"href","/docs/transformers/pr_15808/en/main_classes/trainer#transformers.Trainer"),h(d,"href","training#finetune-with-trainer")},m(M,C){p(M,c,C),t(c,E),t(c,u),t(u,y),t(c,k),t(c,d),t(d,w),t(c,T)},d(M){M&&a(c)}}}function Xr(P){let c,E,u,y,k;return{c(){c=o("p"),E=l("If you aren\u2019t familiar with fine-tuning a model with Keras, take a look at the basic tutorial "),u=o("a"),y=l("here"),k=l("!"),this.h()},l(d){c=n(d,"P",{});var w=i(c);E=r(w,"If you aren\u2019t familiar with fine-tuning a model with Keras, take a look at the basic tutorial "),u=n(w,"A",{href:!0});var T=i(u);y=r(T,"here"),T.forEach(a),k=r(w,"!"),w.forEach(a),this.h()},h(){h(u,"href","training#finetune-with-keras")},m(d,w){p(d,c,w),t(c,E),t(c,u),t(u,y),t(c,k)},d(d){d&&a(c)}}}function Zr(P){let c,E,u,y,k;return{c(){c=o("p"),E=l("For a more in-depth example of how to fine-tune a model for image classification, take a look at the corresponding "),u=o("a"),y=l("PyTorch notebook"),k=l("."),this.h()},l(d){c=n(d,"P",{});var w=i(c);E=r(w,"For a more in-depth example of how to fine-tune a model for image classification, take a look at the corresponding "),u=n(w,"A",{href:!0,rel:!0});var T=i(u);y=r(T,"PyTorch notebook"),T.forEach(a),k=r(w,"."),w.forEach(a),this.h()},h(){h(u,"href","https://colab.research.google.com/github/huggingface/notebooks/blob/master/examples/image_classification.ipynb"),h(u,"rel","nofollow")},m(d,w){p(d,c,w),t(c,E),t(c,u),t(u,y),t(c,k)},d(d){d&&a(c)}}}function eo(P){let c,E,u,y,k,d,w,T,M,C,z,pe,Be,ls,Va,I,rs,fe,os,ns,me,is,ps,Ha,H,Ba,N,B,ma,he,fs,ha,ms,Wa,We,hs,Ya,ce,Ga,Ye,cs,Ja,ue,Ka,Ge,us,Qa,de,Xa,S,ds,ca,_s,gs,ua,$s,bs,Za,_e,et,Je,vs,at,ge,tt,W,js,da,ws,ys,st,R,Y,_a,$e,Es,ga,ks,lt,Ke,xs,rt,be,ot,G,Ts,ve,$a,As,Cs,nt,je,it,J,qs,ba,Fs,Ds,pt,we,ft,K,Ps,ye,va,zs,Is,mt,Ee,ht,Qe,Ss,ct,ke,ut,U,Q,ja,xe,Os,wa,Ls,dt,X,Ms,Xe,Ns,Rs,_t,Te,gt,Z,$t,Ze,Us,bt,O,A,Vs,ea,Hs,Bs,ya,Ws,Ys,Ea,Gs,Js,ka,Ks,Qs,xa,Xs,Zs,el,Ae,al,aa,tl,sl,ll,Ce,rl,ta,ol,nl,vt,qe,jt,V,ee,Ta,Fe,il,Aa,pl,wt,sa,fl,yt,ae,Et,te,ml,Ca,hl,cl,kt,De,xt,q,ul,Pe,qa,dl,_l,Fa,gl,$l,Da,bl,vl,Tt,ze,At,L,jl,la,wl,yl,Pa,El,kl,Ct,Ie,qt,F,xl,za,Tl,Al,Se,Ia,Cl,ql,Sa,Fl,Dl,Ft,Oe,Dt,ra,Pl,Pt,Le,zt,se,zl,oa,Il,Sl,It,Me,St,le,Ol,Ne,Oa,Ll,Ml,Ot,Re,Lt,re,Nl,Ue,La,Rl,Ul,Mt,Ve,Nt,oe,Rt;return d=new Ua({}),z=new Jr({props:{id:"tjAIM7BOYhw"}}),H=new ss({props:{$$slots:{default:[Kr]},$$scope:{ctx:P}}}),he=new Ua({}),ce=new x({props:{code:`from datasets import load_dataset

food = load_dataset("food101", split="train[:5000]"),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

<span class="hljs-meta">&gt;&gt;&gt; </span>food = load_dataset(<span class="hljs-string">&quot;food101&quot;</span>, split=<span class="hljs-string">&quot;train[:5000]&quot;</span>)`}}),ue=new x({props:{code:"food = food.train_test_split(test_size=0.2),",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>food = food.train_test_split(test_size=<span class="hljs-number">0.2</span>)'}}),de=new x({props:{code:'food["train"][0],',highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>food[<span class="hljs-string">&quot;train&quot;</span>][<span class="hljs-number">0</span>]
{<span class="hljs-string">&#x27;image&#x27;</span>: &lt;PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=512x512 at <span class="hljs-number">0x7F52AFC8AC50</span>&gt;,
 <span class="hljs-string">&#x27;label&#x27;</span>: <span class="hljs-number">79</span>}`}}),_e=new x({props:{code:`labels = food["train"].features["label"].names
label2id, id2label = dict(), dict()
for i, label in enumerate(labels):
    label2id[label] = str(i)
    id2label[str(i)] = label,`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>labels = food[<span class="hljs-string">&quot;train&quot;</span>].features[<span class="hljs-string">&quot;label&quot;</span>].names
<span class="hljs-meta">&gt;&gt;&gt; </span>label2id, id2label = <span class="hljs-built_in">dict</span>(), <span class="hljs-built_in">dict</span>()
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">for</span> i, label <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(labels):
<span class="hljs-meta">... </span>    label2id[label] = <span class="hljs-built_in">str</span>(i)
<span class="hljs-meta">... </span>    id2label[<span class="hljs-built_in">str</span>(i)] = label`}}),ge=new x({props:{code:"id2label[str(79)],",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>id2label[<span class="hljs-built_in">str</span>(<span class="hljs-number">79</span>)]
<span class="hljs-string">&#x27;prime_rib&#x27;</span>`}}),$e=new Ua({}),be=new x({props:{code:`from transformers import AutoFeatureExtractor

feature_extractor = AutoFeatureExtractor.from_pretrained("google/vit-base-patch16-224-in21k"),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoFeatureExtractor

<span class="hljs-meta">&gt;&gt;&gt; </span>feature_extractor = AutoFeatureExtractor.from_pretrained(<span class="hljs-string">&quot;google/vit-base-patch16-224-in21k&quot;</span>)`}}),je=new x({props:{code:`from torchvision.transforms import RandomResizedCrop, Compose, Normalize, ToTensor

normalize = Normalize(mean=feature_extractor.image_mean, std=feature_extractor.image_std)
_transforms = Compose([RandomResizedCrop(feature_extractor.size), ToTensor(), normalize]),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> torchvision.transforms <span class="hljs-keyword">import</span> RandomResizedCrop, Compose, Normalize, ToTensor

<span class="hljs-meta">&gt;&gt;&gt; </span>normalize = Normalize(mean=feature_extractor.image_mean, std=feature_extractor.image_std)
<span class="hljs-meta">&gt;&gt;&gt; </span>_transforms = Compose([RandomResizedCrop(feature_extractor.size), ToTensor(), normalize])`}}),we=new x({props:{code:`def transforms(examples):
    examples["pixel_values"] = [_transforms(img.convert("RGB")) for img in examples["image"]]
    return examples,`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">def</span> <span class="hljs-title function_">transforms</span>(<span class="hljs-params">examples</span>):
<span class="hljs-meta">... </span>    examples[<span class="hljs-string">&quot;pixel_values&quot;</span>] = [_transforms(img.convert(<span class="hljs-string">&quot;RGB&quot;</span>)) <span class="hljs-keyword">for</span> img <span class="hljs-keyword">in</span> examples[<span class="hljs-string">&quot;image&quot;</span>]]
<span class="hljs-meta">... </span>    <span class="hljs-keyword">return</span> examples`}}),Ee=new x({props:{code:"food = food.with_transform(transforms),",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>food = food.with_transform(transforms)'}}),ke=new x({props:{code:`import torch


def data_collator(examples):
    pixel_values = torch.stack([example["pixel_values"] for example in examples])
    labels = torch.tensor([example["label"] for example in examples])
    return {"pixel_values": pixel_values, "labels": labels},`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch


<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">def</span> <span class="hljs-title function_">data_collator</span>(<span class="hljs-params">examples</span>):
<span class="hljs-meta">... </span>    pixel_values = torch.stack([example[<span class="hljs-string">&quot;pixel_values&quot;</span>] <span class="hljs-keyword">for</span> example <span class="hljs-keyword">in</span> examples])
<span class="hljs-meta">... </span>    labels = torch.tensor([example[<span class="hljs-string">&quot;label&quot;</span>] <span class="hljs-keyword">for</span> example <span class="hljs-keyword">in</span> examples])
<span class="hljs-meta">... </span>    <span class="hljs-keyword">return</span> {<span class="hljs-string">&quot;pixel_values&quot;</span>: pixel_values, <span class="hljs-string">&quot;labels&quot;</span>: labels}`}}),xe=new Ua({}),Te=new x({props:{code:`from transformers import AutoModelForImageClassification, TrainingArguments, Trainer

model = AutoModelForImageClassification.from_pretrained(
    "google/vit-base-patch16-224-in21k",
    num_labels=len(labels),
    id2label=id2label,
    label2id=label2id,
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForImageClassification, TrainingArguments, Trainer

<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForImageClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;google/vit-base-patch16-224-in21k&quot;</span>,
<span class="hljs-meta">... </span>    num_labels=<span class="hljs-built_in">len</span>(labels),
<span class="hljs-meta">... </span>    id2label=id2label,
<span class="hljs-meta">... </span>    label2id=label2id,
<span class="hljs-meta">... </span>)`}}),Z=new ss({props:{$$slots:{default:[Qr]},$$scope:{ctx:P}}}),qe=new x({props:{code:`training_args = TrainingArguments(
    output_dir="./results",
    per_device_train_batch_size=16,
    evaluation_strategy="steps",
    num_train_epochs=4,
    fp16=True,
    save_steps=100,
    eval_steps=100,
    logging_steps=10,
    learning_rate=2e-4,
    save_total_limit=2,
    remove_unused_columns=False,
)

trainer = Trainer(
    model=model,
    args=training_args,
    data_collator=data_collator,
    train_dataset=food["train"],
    eval_dataset=food["test"],
    tokenizer=feature_extractor,
)

trainer.train(),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>training_args = TrainingArguments(
<span class="hljs-meta">... </span>    output_dir=<span class="hljs-string">&quot;./results&quot;</span>,
<span class="hljs-meta">... </span>    per_device_train_batch_size=<span class="hljs-number">16</span>,
<span class="hljs-meta">... </span>    evaluation_strategy=<span class="hljs-string">&quot;steps&quot;</span>,
<span class="hljs-meta">... </span>    num_train_epochs=<span class="hljs-number">4</span>,
<span class="hljs-meta">... </span>    fp16=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>    save_steps=<span class="hljs-number">100</span>,
<span class="hljs-meta">... </span>    eval_steps=<span class="hljs-number">100</span>,
<span class="hljs-meta">... </span>    logging_steps=<span class="hljs-number">10</span>,
<span class="hljs-meta">... </span>    learning_rate=<span class="hljs-number">2e-4</span>,
<span class="hljs-meta">... </span>    save_total_limit=<span class="hljs-number">2</span>,
<span class="hljs-meta">... </span>    remove_unused_columns=<span class="hljs-literal">False</span>,
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>trainer = Trainer(
<span class="hljs-meta">... </span>    model=model,
<span class="hljs-meta">... </span>    args=training_args,
<span class="hljs-meta">... </span>    data_collator=data_collator,
<span class="hljs-meta">... </span>    train_dataset=food[<span class="hljs-string">&quot;train&quot;</span>],
<span class="hljs-meta">... </span>    eval_dataset=food[<span class="hljs-string">&quot;test&quot;</span>],
<span class="hljs-meta">... </span>    tokenizer=feature_extractor,
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>trainer.train()`}}),Fe=new Ua({}),ae=new ss({props:{$$slots:{default:[Xr]},$$scope:{ctx:P}}}),De=new x({props:{code:`def process(examples):
    examples.update(
        feature_extractor(
            examples["image"],
        )
    )
    return examples,`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">def</span> <span class="hljs-title function_">process</span>(<span class="hljs-params">examples</span>):
<span class="hljs-meta">... </span>    examples.update(
<span class="hljs-meta">... </span>        feature_extractor(
<span class="hljs-meta">... </span>            examples[<span class="hljs-string">&quot;image&quot;</span>],
<span class="hljs-meta">... </span>        )
<span class="hljs-meta">... </span>    )
<span class="hljs-meta">... </span>    <span class="hljs-keyword">return</span> examples`}}),ze=new x({props:{code:"processed_food = food.map(process, batched=True),",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>processed_food = food.<span class="hljs-built_in">map</span>(process, batched=<span class="hljs-literal">True</span>)'}}),Ie=new x({props:{code:`from transformers import DefaultDataCollator

data_collator = DefaultDataCollator(return_tensors="tf"),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DefaultDataCollator

<span class="hljs-meta">&gt;&gt;&gt; </span>data_collator = DefaultDataCollator(return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)`}}),Oe=new x({props:{code:`tf_train_set = processed_food["train"].to_tf_dataset(
    columns=["pixel_values", "labels"],
    shuffle=True,
    batch_size=16,
    collate_fn=data_collator,
)

tf_test_set = processed_food["test"].to_tf_dataset(
    columns=["pixel_values", "labels"],
    shuffle=False,
    batch_size=16,
    collate_fn=data_collator,
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>tf_train_set = processed_food[<span class="hljs-string">&quot;train&quot;</span>].to_tf_dataset(
<span class="hljs-meta">... </span>    columns=[<span class="hljs-string">&quot;pixel_values&quot;</span>, <span class="hljs-string">&quot;labels&quot;</span>],
<span class="hljs-meta">... </span>    shuffle=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>    batch_size=<span class="hljs-number">16</span>,
<span class="hljs-meta">... </span>    collate_fn=data_collator,
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>tf_test_set = processed_food[<span class="hljs-string">&quot;test&quot;</span>].to_tf_dataset(
<span class="hljs-meta">... </span>    columns=[<span class="hljs-string">&quot;pixel_values&quot;</span>, <span class="hljs-string">&quot;labels&quot;</span>],
<span class="hljs-meta">... </span>    shuffle=<span class="hljs-literal">False</span>,
<span class="hljs-meta">... </span>    batch_size=<span class="hljs-number">16</span>,
<span class="hljs-meta">... </span>    collate_fn=data_collator,
<span class="hljs-meta">... </span>)`}}),Le=new x({props:{code:`from transformers import create_optimizer, AdamWeightDecay

optimizer = AdamWeightDecay(learning_rate=2e-5, weight_decay_rate=0.01),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> create_optimizer, AdamWeightDecay

<span class="hljs-meta">&gt;&gt;&gt; </span>optimizer = AdamWeightDecay(learning_rate=<span class="hljs-number">2e-5</span>, weight_decay_rate=<span class="hljs-number">0.01</span>)`}}),Me=new x({props:{code:`from transformers import TFAutoModelForImageClassification

model = TFAutoModelForImageClassification.from_pretrained(
    "google/vit-base-patch16-224-in21k",
    num_labels=len(labels),
    id2label=id2label,
    label2id=label2id,
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFAutoModelForImageClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForImageClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;google/vit-base-patch16-224-in21k&quot;</span>,
<span class="hljs-meta">... </span>    num_labels=<span class="hljs-built_in">len</span>(labels),
<span class="hljs-meta">... </span>    id2label=id2label,
<span class="hljs-meta">... </span>    label2id=label2id,
<span class="hljs-meta">... </span>)`}}),Re=new x({props:{code:"model.compile(optimizer=optimizer),",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>model.<span class="hljs-built_in">compile</span>(optimizer=optimizer)'}}),Ve=new x({props:{code:"model.fit(x=tf_train_set, validation_data=tf_test_set, epochs=2),",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>model.fit(x=tf_train_set, validation_data=tf_test_set, epochs=<span class="hljs-number">2</span>)'}}),oe=new ss({props:{$$slots:{default:[Zr]},$$scope:{ctx:P}}}),{c(){c=o("meta"),E=f(),u=o("h1"),y=o("a"),k=o("span"),_(d.$$.fragment),w=f(),T=o("span"),M=l("Image classification"),C=f(),_(z.$$.fragment),pe=f(),Be=o("p"),ls=l("Image classification assigns a label or class to an image. Unlike text or audio classification, the inputs are the pixel values that represent an image. There are many uses for image classification, like detecting damage after a disaster, monitoring crop health, or helping screen medical images for signs of disease."),Va=f(),I=o("p"),rs=l("This guide will show you how to fine-tune "),fe=o("a"),os=l("ViT"),ns=l(" on the "),me=o("a"),is=l("Food-101"),ps=l(" dataset to classify a food item in an image."),Ha=f(),_(H.$$.fragment),Ba=f(),N=o("h2"),B=o("a"),ma=o("span"),_(he.$$.fragment),fs=f(),ha=o("span"),ms=l("Load Food-101 dataset"),Wa=f(),We=o("p"),hs=l("Load only the first 5000 images of the Food-101 dataset from the \u{1F917} Datasets library since it is pretty large:"),Ya=f(),_(ce.$$.fragment),Ga=f(),Ye=o("p"),cs=l("Split this dataset into a train and test set:"),Ja=f(),_(ue.$$.fragment),Ka=f(),Ge=o("p"),us=l("Then take a look at an example:"),Qa=f(),_(de.$$.fragment),Xa=f(),S=o("p"),ds=l("The "),ca=o("code"),_s=l("image"),gs=l(" field contains a PIL image, and each "),ua=o("code"),$s=l("label"),bs=l(" is an integer that represents a class. Create a dictionary that maps a label name to an integer and vice versa. The mapping will help the model recover the label name from the label number:"),Za=f(),_(_e.$$.fragment),et=f(),Je=o("p"),vs=l("Now you can convert the label number to a label name for more information:"),at=f(),_(ge.$$.fragment),tt=f(),W=o("p"),js=l("Each food class - or label - corresponds to a number; "),da=o("code"),ws=l("79"),ys=l(" indicates a prime rib in the example above."),st=f(),R=o("h2"),Y=o("a"),_a=o("span"),_($e.$$.fragment),Es=f(),ga=o("span"),ks=l("Preprocess"),lt=f(),Ke=o("p"),xs=l("Load the ViT feature extractor to process the image into a tensor:"),rt=f(),_(be.$$.fragment),ot=f(),G=o("p"),Ts=l("Apply several image transformations to the dataset to make the model more robust against overfitting. Here you\u2019ll use torchvision\u2019s "),ve=o("a"),$a=o("code"),As=l("transforms"),Cs=l(" module. Crop a random part of the image, resize it, and normalize it with the image mean and standard deviation:"),nt=f(),_(je.$$.fragment),it=f(),J=o("p"),qs=l("Create a preprocessing function that will apply the transforms and return the "),ba=o("code"),Fs=l("pixel_values"),Ds=l(" - the inputs to the model - of the image:"),pt=f(),_(we.$$.fragment),ft=f(),K=o("p"),Ps=l("Use \u{1F917} Dataset\u2019s "),ye=o("a"),va=o("code"),zs=l("with_transform"),Is=l(" method to apply the transforms over the entire dataset. The transforms are applied on-the-fly when you load an element of the dataset:"),mt=f(),_(Ee.$$.fragment),ht=f(),Qe=o("p"),Ss=l("\u{1F917} Transformers doesn\u2019t have a data collator for image classification, so you will need to create one to build a batch of examples:"),ct=f(),_(ke.$$.fragment),ut=f(),U=o("h2"),Q=o("a"),ja=o("span"),_(xe.$$.fragment),Os=f(),wa=o("span"),Ls=l("Fine-tune with Trainer"),dt=f(),X=o("p"),Ms=l("Load ViT with "),Xe=o("a"),Ns=l("AutoModelForImageClassification"),Rs=l(". Specify the number of labels, and pass the model the mapping between label number and label class:"),_t=f(),_(Te.$$.fragment),gt=f(),_(Z.$$.fragment),$t=f(),Ze=o("p"),Us=l("At this point, only three steps remain:"),bt=f(),O=o("ol"),A=o("li"),Vs=l("Define your training hyperparameters in "),ea=o("a"),Hs=l("TrainingArguments"),Bs=l(". It is important you don\u2019t remove unused columns because this will drop the "),ya=o("code"),Ws=l("image"),Ys=l(" column. Without the "),Ea=o("code"),Gs=l("image"),Js=l(" column, you can\u2019t create "),ka=o("code"),Ks=l("pixel_values"),Qs=l(". Set "),xa=o("code"),Xs=l("remove_unused_columns=False"),Zs=l(" to prevent this behavior!"),el=f(),Ae=o("li"),al=l("Pass the training arguments to "),aa=o("a"),tl=l("Trainer"),sl=l(" along with the model, datasets, tokenizer, and data collator."),ll=f(),Ce=o("li"),rl=l("Call "),ta=o("a"),ol=l("train()"),nl=l(" to fine-tune your model."),vt=f(),_(qe.$$.fragment),jt=f(),V=o("h2"),ee=o("a"),Ta=o("span"),_(Fe.$$.fragment),il=f(),Aa=o("span"),pl=l("Fine-tune with TensorFlow"),wt=f(),sa=o("p"),fl=l("To fine-tune a model in TensorFlow requires a few minor changes to how you preprocess an image."),yt=f(),_(ae.$$.fragment),Et=f(),te=o("p"),ml=l("Create a simple preprocessing function to create "),Ca=o("code"),hl=l("pixel_values"),cl=l(" from an image. The feature extractor will resize the image:"),kt=f(),_(De.$$.fragment),xt=f(),q=o("p"),ul=l("Use \u{1F917} Datasets "),Pe=o("a"),qa=o("code"),dl=l("map"),_l=l(" function to apply the preprocessing function to the entire dataset. You can speed up the "),Fa=o("code"),gl=l("map"),$l=l(" function by setting "),Da=o("code"),bl=l("batched=True"),vl=l(" to process multiple elements of the dataset at once:"),Tt=f(),_(ze.$$.fragment),At=f(),L=o("p"),jl=l("Use "),la=o("a"),wl=l("DefaultDataCollator"),yl=l(" to create a batch of examples and instruct the data collator to return "),Pa=o("code"),El=l("tf.Tensor's"),kl=l(":"),Ct=f(),_(Ie.$$.fragment),qt=f(),F=o("p"),xl=l("Convert your datasets to the "),za=o("code"),Tl=l("tf.data.Dataset"),Al=l(" format with "),Se=o("a"),Ia=o("code"),Cl=l("to_tf_dataset"),ql=l(". Specify inputs and labels in "),Sa=o("code"),Fl=l("columns"),Dl=l(", whether to shuffle the dataset order, batch size and the data collator:"),Ft=f(),_(Oe.$$.fragment),Dt=f(),ra=o("p"),Pl=l("Set up an optimizer function, learning rate schedule, and some training hyperparameters:"),Pt=f(),_(Le.$$.fragment),zt=f(),se=o("p"),zl=l("Load ViT with "),oa=o("a"),Il=l("TFAutoModelForImageClassification"),Sl=l(". Specify the number of labels, and pass the model the mapping between label number and label class:"),It=f(),_(Me.$$.fragment),St=f(),le=o("p"),Ol=l("Configure the model for training with "),Ne=o("a"),Oa=o("code"),Ll=l("compile"),Ml=l("::"),Ot=f(),_(Re.$$.fragment),Lt=f(),re=o("p"),Nl=l("Call "),Ue=o("a"),La=o("code"),Rl=l("fit"),Ul=l(" to fine-tune the model:"),Mt=f(),_(Ve.$$.fragment),Nt=f(),_(oe.$$.fragment),this.h()},l(e){const s=Gr('[data-svelte="svelte-1phssyn"]',document.head);c=n(s,"META",{name:!0,content:!0}),s.forEach(a),E=m(e),u=n(e,"H1",{class:!0});var He=i(u);y=n(He,"A",{id:!0,class:!0,href:!0});var Ma=i(y);k=n(Ma,"SPAN",{});var Na=i(k);g(d.$$.fragment,Na),Na.forEach(a),Ma.forEach(a),w=m(He),T=n(He,"SPAN",{});var Ra=i(T);M=r(Ra,"Image classification"),Ra.forEach(a),He.forEach(a),C=m(e),g(z.$$.fragment,e),pe=m(e),Be=n(e,"P",{});var Vl=i(Be);ls=r(Vl,"Image classification assigns a label or class to an image. Unlike text or audio classification, the inputs are the pixel values that represent an image. There are many uses for image classification, like detecting damage after a disaster, monitoring crop health, or helping screen medical images for signs of disease."),Vl.forEach(a),Va=m(e),I=n(e,"P",{});var na=i(I);rs=r(na,"This guide will show you how to fine-tune "),fe=n(na,"A",{href:!0,rel:!0});var Hl=i(fe);os=r(Hl,"ViT"),Hl.forEach(a),ns=r(na," on the "),me=n(na,"A",{href:!0,rel:!0});var Bl=i(me);is=r(Bl,"Food-101"),Bl.forEach(a),ps=r(na," dataset to classify a food item in an image."),na.forEach(a),Ha=m(e),g(H.$$.fragment,e),Ba=m(e),N=n(e,"H2",{class:!0});var Ut=i(N);B=n(Ut,"A",{id:!0,class:!0,href:!0});var Wl=i(B);ma=n(Wl,"SPAN",{});var Yl=i(ma);g(he.$$.fragment,Yl),Yl.forEach(a),Wl.forEach(a),fs=m(Ut),ha=n(Ut,"SPAN",{});var Gl=i(ha);ms=r(Gl,"Load Food-101 dataset"),Gl.forEach(a),Ut.forEach(a),Wa=m(e),We=n(e,"P",{});var Jl=i(We);hs=r(Jl,"Load only the first 5000 images of the Food-101 dataset from the \u{1F917} Datasets library since it is pretty large:"),Jl.forEach(a),Ya=m(e),g(ce.$$.fragment,e),Ga=m(e),Ye=n(e,"P",{});var Kl=i(Ye);cs=r(Kl,"Split this dataset into a train and test set:"),Kl.forEach(a),Ja=m(e),g(ue.$$.fragment,e),Ka=m(e),Ge=n(e,"P",{});var Ql=i(Ge);us=r(Ql,"Then take a look at an example:"),Ql.forEach(a),Qa=m(e),g(de.$$.fragment,e),Xa=m(e),S=n(e,"P",{});var ia=i(S);ds=r(ia,"The "),ca=n(ia,"CODE",{});var Xl=i(ca);_s=r(Xl,"image"),Xl.forEach(a),gs=r(ia," field contains a PIL image, and each "),ua=n(ia,"CODE",{});var Zl=i(ua);$s=r(Zl,"label"),Zl.forEach(a),bs=r(ia," is an integer that represents a class. Create a dictionary that maps a label name to an integer and vice versa. The mapping will help the model recover the label name from the label number:"),ia.forEach(a),Za=m(e),g(_e.$$.fragment,e),et=m(e),Je=n(e,"P",{});var er=i(Je);vs=r(er,"Now you can convert the label number to a label name for more information:"),er.forEach(a),at=m(e),g(ge.$$.fragment,e),tt=m(e),W=n(e,"P",{});var Vt=i(W);js=r(Vt,"Each food class - or label - corresponds to a number; "),da=n(Vt,"CODE",{});var ar=i(da);ws=r(ar,"79"),ar.forEach(a),ys=r(Vt," indicates a prime rib in the example above."),Vt.forEach(a),st=m(e),R=n(e,"H2",{class:!0});var Ht=i(R);Y=n(Ht,"A",{id:!0,class:!0,href:!0});var tr=i(Y);_a=n(tr,"SPAN",{});var sr=i(_a);g($e.$$.fragment,sr),sr.forEach(a),tr.forEach(a),Es=m(Ht),ga=n(Ht,"SPAN",{});var lr=i(ga);ks=r(lr,"Preprocess"),lr.forEach(a),Ht.forEach(a),lt=m(e),Ke=n(e,"P",{});var rr=i(Ke);xs=r(rr,"Load the ViT feature extractor to process the image into a tensor:"),rr.forEach(a),rt=m(e),g(be.$$.fragment,e),ot=m(e),G=n(e,"P",{});var Bt=i(G);Ts=r(Bt,"Apply several image transformations to the dataset to make the model more robust against overfitting. Here you\u2019ll use torchvision\u2019s "),ve=n(Bt,"A",{href:!0,rel:!0});var or=i(ve);$a=n(or,"CODE",{});var nr=i($a);As=r(nr,"transforms"),nr.forEach(a),or.forEach(a),Cs=r(Bt," module. Crop a random part of the image, resize it, and normalize it with the image mean and standard deviation:"),Bt.forEach(a),nt=m(e),g(je.$$.fragment,e),it=m(e),J=n(e,"P",{});var Wt=i(J);qs=r(Wt,"Create a preprocessing function that will apply the transforms and return the "),ba=n(Wt,"CODE",{});var ir=i(ba);Fs=r(ir,"pixel_values"),ir.forEach(a),Ds=r(Wt," - the inputs to the model - of the image:"),Wt.forEach(a),pt=m(e),g(we.$$.fragment,e),ft=m(e),K=n(e,"P",{});var Yt=i(K);Ps=r(Yt,"Use \u{1F917} Dataset\u2019s "),ye=n(Yt,"A",{href:!0,rel:!0});var pr=i(ye);va=n(pr,"CODE",{});var fr=i(va);zs=r(fr,"with_transform"),fr.forEach(a),pr.forEach(a),Is=r(Yt," method to apply the transforms over the entire dataset. The transforms are applied on-the-fly when you load an element of the dataset:"),Yt.forEach(a),mt=m(e),g(Ee.$$.fragment,e),ht=m(e),Qe=n(e,"P",{});var mr=i(Qe);Ss=r(mr,"\u{1F917} Transformers doesn\u2019t have a data collator for image classification, so you will need to create one to build a batch of examples:"),mr.forEach(a),ct=m(e),g(ke.$$.fragment,e),ut=m(e),U=n(e,"H2",{class:!0});var Gt=i(U);Q=n(Gt,"A",{id:!0,class:!0,href:!0});var hr=i(Q);ja=n(hr,"SPAN",{});var cr=i(ja);g(xe.$$.fragment,cr),cr.forEach(a),hr.forEach(a),Os=m(Gt),wa=n(Gt,"SPAN",{});var ur=i(wa);Ls=r(ur,"Fine-tune with Trainer"),ur.forEach(a),Gt.forEach(a),dt=m(e),X=n(e,"P",{});var Jt=i(X);Ms=r(Jt,"Load ViT with "),Xe=n(Jt,"A",{href:!0});var dr=i(Xe);Ns=r(dr,"AutoModelForImageClassification"),dr.forEach(a),Rs=r(Jt,". Specify the number of labels, and pass the model the mapping between label number and label class:"),Jt.forEach(a),_t=m(e),g(Te.$$.fragment,e),gt=m(e),g(Z.$$.fragment,e),$t=m(e),Ze=n(e,"P",{});var _r=i(Ze);Us=r(_r,"At this point, only three steps remain:"),_r.forEach(a),bt=m(e),O=n(e,"OL",{});var pa=i(O);A=n(pa,"LI",{});var D=i(A);Vs=r(D,"Define your training hyperparameters in "),ea=n(D,"A",{href:!0});var gr=i(ea);Hs=r(gr,"TrainingArguments"),gr.forEach(a),Bs=r(D,". It is important you don\u2019t remove unused columns because this will drop the "),ya=n(D,"CODE",{});var $r=i(ya);Ws=r($r,"image"),$r.forEach(a),Ys=r(D," column. Without the "),Ea=n(D,"CODE",{});var br=i(Ea);Gs=r(br,"image"),br.forEach(a),Js=r(D," column, you can\u2019t create "),ka=n(D,"CODE",{});var vr=i(ka);Ks=r(vr,"pixel_values"),vr.forEach(a),Qs=r(D,". Set "),xa=n(D,"CODE",{});var jr=i(xa);Xs=r(jr,"remove_unused_columns=False"),jr.forEach(a),Zs=r(D," to prevent this behavior!"),D.forEach(a),el=m(pa),Ae=n(pa,"LI",{});var Kt=i(Ae);al=r(Kt,"Pass the training arguments to "),aa=n(Kt,"A",{href:!0});var wr=i(aa);tl=r(wr,"Trainer"),wr.forEach(a),sl=r(Kt," along with the model, datasets, tokenizer, and data collator."),Kt.forEach(a),ll=m(pa),Ce=n(pa,"LI",{});var Qt=i(Ce);rl=r(Qt,"Call "),ta=n(Qt,"A",{href:!0});var yr=i(ta);ol=r(yr,"train()"),yr.forEach(a),nl=r(Qt," to fine-tune your model."),Qt.forEach(a),pa.forEach(a),vt=m(e),g(qe.$$.fragment,e),jt=m(e),V=n(e,"H2",{class:!0});var Xt=i(V);ee=n(Xt,"A",{id:!0,class:!0,href:!0});var Er=i(ee);Ta=n(Er,"SPAN",{});var kr=i(Ta);g(Fe.$$.fragment,kr),kr.forEach(a),Er.forEach(a),il=m(Xt),Aa=n(Xt,"SPAN",{});var xr=i(Aa);pl=r(xr,"Fine-tune with TensorFlow"),xr.forEach(a),Xt.forEach(a),wt=m(e),sa=n(e,"P",{});var Tr=i(sa);fl=r(Tr,"To fine-tune a model in TensorFlow requires a few minor changes to how you preprocess an image."),Tr.forEach(a),yt=m(e),g(ae.$$.fragment,e),Et=m(e),te=n(e,"P",{});var Zt=i(te);ml=r(Zt,"Create a simple preprocessing function to create "),Ca=n(Zt,"CODE",{});var Ar=i(Ca);hl=r(Ar,"pixel_values"),Ar.forEach(a),cl=r(Zt," from an image. The feature extractor will resize the image:"),Zt.forEach(a),kt=m(e),g(De.$$.fragment,e),xt=m(e),q=n(e,"P",{});var ne=i(q);ul=r(ne,"Use \u{1F917} Datasets "),Pe=n(ne,"A",{href:!0,rel:!0});var Cr=i(Pe);qa=n(Cr,"CODE",{});var qr=i(qa);dl=r(qr,"map"),qr.forEach(a),Cr.forEach(a),_l=r(ne," function to apply the preprocessing function to the entire dataset. You can speed up the "),Fa=n(ne,"CODE",{});var Fr=i(Fa);gl=r(Fr,"map"),Fr.forEach(a),$l=r(ne," function by setting "),Da=n(ne,"CODE",{});var Dr=i(Da);bl=r(Dr,"batched=True"),Dr.forEach(a),vl=r(ne," to process multiple elements of the dataset at once:"),ne.forEach(a),Tt=m(e),g(ze.$$.fragment,e),At=m(e),L=n(e,"P",{});var fa=i(L);jl=r(fa,"Use "),la=n(fa,"A",{href:!0});var Pr=i(la);wl=r(Pr,"DefaultDataCollator"),Pr.forEach(a),yl=r(fa," to create a batch of examples and instruct the data collator to return "),Pa=n(fa,"CODE",{});var zr=i(Pa);El=r(zr,"tf.Tensor's"),zr.forEach(a),kl=r(fa,":"),fa.forEach(a),Ct=m(e),g(Ie.$$.fragment,e),qt=m(e),F=n(e,"P",{});var ie=i(F);xl=r(ie,"Convert your datasets to the "),za=n(ie,"CODE",{});var Ir=i(za);Tl=r(Ir,"tf.data.Dataset"),Ir.forEach(a),Al=r(ie," format with "),Se=n(ie,"A",{href:!0,rel:!0});var Sr=i(Se);Ia=n(Sr,"CODE",{});var Or=i(Ia);Cl=r(Or,"to_tf_dataset"),Or.forEach(a),Sr.forEach(a),ql=r(ie,". Specify inputs and labels in "),Sa=n(ie,"CODE",{});var Lr=i(Sa);Fl=r(Lr,"columns"),Lr.forEach(a),Dl=r(ie,", whether to shuffle the dataset order, batch size and the data collator:"),ie.forEach(a),Ft=m(e),g(Oe.$$.fragment,e),Dt=m(e),ra=n(e,"P",{});var Mr=i(ra);Pl=r(Mr,"Set up an optimizer function, learning rate schedule, and some training hyperparameters:"),Mr.forEach(a),Pt=m(e),g(Le.$$.fragment,e),zt=m(e),se=n(e,"P",{});var es=i(se);zl=r(es,"Load ViT with "),oa=n(es,"A",{href:!0});var Nr=i(oa);Il=r(Nr,"TFAutoModelForImageClassification"),Nr.forEach(a),Sl=r(es,". Specify the number of labels, and pass the model the mapping between label number and label class:"),es.forEach(a),It=m(e),g(Me.$$.fragment,e),St=m(e),le=n(e,"P",{});var as=i(le);Ol=r(as,"Configure the model for training with "),Ne=n(as,"A",{href:!0,rel:!0});var Rr=i(Ne);Oa=n(Rr,"CODE",{});var Ur=i(Oa);Ll=r(Ur,"compile"),Ur.forEach(a),Rr.forEach(a),Ml=r(as,"::"),as.forEach(a),Ot=m(e),g(Re.$$.fragment,e),Lt=m(e),re=n(e,"P",{});var ts=i(re);Nl=r(ts,"Call "),Ue=n(ts,"A",{href:!0,rel:!0});var Vr=i(Ue);La=n(Vr,"CODE",{});var Hr=i(La);Rl=r(Hr,"fit"),Hr.forEach(a),Vr.forEach(a),Ul=r(ts," to fine-tune the model:"),ts.forEach(a),Mt=m(e),g(Ve.$$.fragment,e),Nt=m(e),g(oe.$$.fragment,e),this.h()},h(){h(c,"name","hf:doc:metadata"),h(c,"content",JSON.stringify(ao)),h(y,"id","image-classification"),h(y,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(y,"href","#image-classification"),h(u,"class","relative group"),h(fe,"href","https://huggingface.co/docs/transformers/v4.16.2/en/model_doc/vit"),h(fe,"rel","nofollow"),h(me,"href","https://huggingface.co/datasets/food101"),h(me,"rel","nofollow"),h(B,"id","load-food101-dataset"),h(B,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(B,"href","#load-food101-dataset"),h(N,"class","relative group"),h(Y,"id","preprocess"),h(Y,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(Y,"href","#preprocess"),h(R,"class","relative group"),h(ve,"href","https://pytorch.org/vision/stable/transforms.html"),h(ve,"rel","nofollow"),h(ye,"href","https://huggingface.co/docs/datasets/package_reference/main_classes.html?#datasets.Dataset.with_transform"),h(ye,"rel","nofollow"),h(Q,"id","finetune-with-trainer"),h(Q,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(Q,"href","#finetune-with-trainer"),h(U,"class","relative group"),h(Xe,"href","/docs/transformers/pr_15808/en/model_doc/auto#transformers.AutoModelForImageClassification"),h(ea,"href","/docs/transformers/pr_15808/en/main_classes/trainer#transformers.TrainingArguments"),h(aa,"href","/docs/transformers/pr_15808/en/main_classes/trainer#transformers.Trainer"),h(ta,"href","/docs/transformers/pr_15808/en/main_classes/trainer#transformers.Trainer.train"),h(ee,"id","finetune-with-tensorflow"),h(ee,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(ee,"href","#finetune-with-tensorflow"),h(V,"class","relative group"),h(Pe,"href","https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Dataset.map"),h(Pe,"rel","nofollow"),h(la,"href","/docs/transformers/pr_15808/en/main_classes/data_collator#transformers.DefaultDataCollator"),h(Se,"href","https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Dataset.to_tf_dataset"),h(Se,"rel","nofollow"),h(oa,"href","/docs/transformers/pr_15808/en/model_doc/auto#transformers.TFAutoModelForImageClassification"),h(Ne,"href","https://keras.io/api/models/model_training_apis/#compile-method"),h(Ne,"rel","nofollow"),h(Ue,"href","https://keras.io/api/models/model_training_apis/#fit-method"),h(Ue,"rel","nofollow")},m(e,s){t(document.head,c),p(e,E,s),p(e,u,s),t(u,y),t(y,k),$(d,k,null),t(u,w),t(u,T),t(T,M),p(e,C,s),$(z,e,s),p(e,pe,s),p(e,Be,s),t(Be,ls),p(e,Va,s),p(e,I,s),t(I,rs),t(I,fe),t(fe,os),t(I,ns),t(I,me),t(me,is),t(I,ps),p(e,Ha,s),$(H,e,s),p(e,Ba,s),p(e,N,s),t(N,B),t(B,ma),$(he,ma,null),t(N,fs),t(N,ha),t(ha,ms),p(e,Wa,s),p(e,We,s),t(We,hs),p(e,Ya,s),$(ce,e,s),p(e,Ga,s),p(e,Ye,s),t(Ye,cs),p(e,Ja,s),$(ue,e,s),p(e,Ka,s),p(e,Ge,s),t(Ge,us),p(e,Qa,s),$(de,e,s),p(e,Xa,s),p(e,S,s),t(S,ds),t(S,ca),t(ca,_s),t(S,gs),t(S,ua),t(ua,$s),t(S,bs),p(e,Za,s),$(_e,e,s),p(e,et,s),p(e,Je,s),t(Je,vs),p(e,at,s),$(ge,e,s),p(e,tt,s),p(e,W,s),t(W,js),t(W,da),t(da,ws),t(W,ys),p(e,st,s),p(e,R,s),t(R,Y),t(Y,_a),$($e,_a,null),t(R,Es),t(R,ga),t(ga,ks),p(e,lt,s),p(e,Ke,s),t(Ke,xs),p(e,rt,s),$(be,e,s),p(e,ot,s),p(e,G,s),t(G,Ts),t(G,ve),t(ve,$a),t($a,As),t(G,Cs),p(e,nt,s),$(je,e,s),p(e,it,s),p(e,J,s),t(J,qs),t(J,ba),t(ba,Fs),t(J,Ds),p(e,pt,s),$(we,e,s),p(e,ft,s),p(e,K,s),t(K,Ps),t(K,ye),t(ye,va),t(va,zs),t(K,Is),p(e,mt,s),$(Ee,e,s),p(e,ht,s),p(e,Qe,s),t(Qe,Ss),p(e,ct,s),$(ke,e,s),p(e,ut,s),p(e,U,s),t(U,Q),t(Q,ja),$(xe,ja,null),t(U,Os),t(U,wa),t(wa,Ls),p(e,dt,s),p(e,X,s),t(X,Ms),t(X,Xe),t(Xe,Ns),t(X,Rs),p(e,_t,s),$(Te,e,s),p(e,gt,s),$(Z,e,s),p(e,$t,s),p(e,Ze,s),t(Ze,Us),p(e,bt,s),p(e,O,s),t(O,A),t(A,Vs),t(A,ea),t(ea,Hs),t(A,Bs),t(A,ya),t(ya,Ws),t(A,Ys),t(A,Ea),t(Ea,Gs),t(A,Js),t(A,ka),t(ka,Ks),t(A,Qs),t(A,xa),t(xa,Xs),t(A,Zs),t(O,el),t(O,Ae),t(Ae,al),t(Ae,aa),t(aa,tl),t(Ae,sl),t(O,ll),t(O,Ce),t(Ce,rl),t(Ce,ta),t(ta,ol),t(Ce,nl),p(e,vt,s),$(qe,e,s),p(e,jt,s),p(e,V,s),t(V,ee),t(ee,Ta),$(Fe,Ta,null),t(V,il),t(V,Aa),t(Aa,pl),p(e,wt,s),p(e,sa,s),t(sa,fl),p(e,yt,s),$(ae,e,s),p(e,Et,s),p(e,te,s),t(te,ml),t(te,Ca),t(Ca,hl),t(te,cl),p(e,kt,s),$(De,e,s),p(e,xt,s),p(e,q,s),t(q,ul),t(q,Pe),t(Pe,qa),t(qa,dl),t(q,_l),t(q,Fa),t(Fa,gl),t(q,$l),t(q,Da),t(Da,bl),t(q,vl),p(e,Tt,s),$(ze,e,s),p(e,At,s),p(e,L,s),t(L,jl),t(L,la),t(la,wl),t(L,yl),t(L,Pa),t(Pa,El),t(L,kl),p(e,Ct,s),$(Ie,e,s),p(e,qt,s),p(e,F,s),t(F,xl),t(F,za),t(za,Tl),t(F,Al),t(F,Se),t(Se,Ia),t(Ia,Cl),t(F,ql),t(F,Sa),t(Sa,Fl),t(F,Dl),p(e,Ft,s),$(Oe,e,s),p(e,Dt,s),p(e,ra,s),t(ra,Pl),p(e,Pt,s),$(Le,e,s),p(e,zt,s),p(e,se,s),t(se,zl),t(se,oa),t(oa,Il),t(se,Sl),p(e,It,s),$(Me,e,s),p(e,St,s),p(e,le,s),t(le,Ol),t(le,Ne),t(Ne,Oa),t(Oa,Ll),t(le,Ml),p(e,Ot,s),$(Re,e,s),p(e,Lt,s),p(e,re,s),t(re,Nl),t(re,Ue),t(Ue,La),t(La,Rl),t(re,Ul),p(e,Mt,s),$(Ve,e,s),p(e,Nt,s),$(oe,e,s),Rt=!0},p(e,[s]){const He={};s&2&&(He.$$scope={dirty:s,ctx:e}),H.$set(He);const Ma={};s&2&&(Ma.$$scope={dirty:s,ctx:e}),Z.$set(Ma);const Na={};s&2&&(Na.$$scope={dirty:s,ctx:e}),ae.$set(Na);const Ra={};s&2&&(Ra.$$scope={dirty:s,ctx:e}),oe.$set(Ra)},i(e){Rt||(b(d.$$.fragment,e),b(z.$$.fragment,e),b(H.$$.fragment,e),b(he.$$.fragment,e),b(ce.$$.fragment,e),b(ue.$$.fragment,e),b(de.$$.fragment,e),b(_e.$$.fragment,e),b(ge.$$.fragment,e),b($e.$$.fragment,e),b(be.$$.fragment,e),b(je.$$.fragment,e),b(we.$$.fragment,e),b(Ee.$$.fragment,e),b(ke.$$.fragment,e),b(xe.$$.fragment,e),b(Te.$$.fragment,e),b(Z.$$.fragment,e),b(qe.$$.fragment,e),b(Fe.$$.fragment,e),b(ae.$$.fragment,e),b(De.$$.fragment,e),b(ze.$$.fragment,e),b(Ie.$$.fragment,e),b(Oe.$$.fragment,e),b(Le.$$.fragment,e),b(Me.$$.fragment,e),b(Re.$$.fragment,e),b(Ve.$$.fragment,e),b(oe.$$.fragment,e),Rt=!0)},o(e){v(d.$$.fragment,e),v(z.$$.fragment,e),v(H.$$.fragment,e),v(he.$$.fragment,e),v(ce.$$.fragment,e),v(ue.$$.fragment,e),v(de.$$.fragment,e),v(_e.$$.fragment,e),v(ge.$$.fragment,e),v($e.$$.fragment,e),v(be.$$.fragment,e),v(je.$$.fragment,e),v(we.$$.fragment,e),v(Ee.$$.fragment,e),v(ke.$$.fragment,e),v(xe.$$.fragment,e),v(Te.$$.fragment,e),v(Z.$$.fragment,e),v(qe.$$.fragment,e),v(Fe.$$.fragment,e),v(ae.$$.fragment,e),v(De.$$.fragment,e),v(ze.$$.fragment,e),v(Ie.$$.fragment,e),v(Oe.$$.fragment,e),v(Le.$$.fragment,e),v(Me.$$.fragment,e),v(Re.$$.fragment,e),v(Ve.$$.fragment,e),v(oe.$$.fragment,e),Rt=!1},d(e){a(c),e&&a(E),e&&a(u),j(d),e&&a(C),j(z,e),e&&a(pe),e&&a(Be),e&&a(Va),e&&a(I),e&&a(Ha),j(H,e),e&&a(Ba),e&&a(N),j(he),e&&a(Wa),e&&a(We),e&&a(Ya),j(ce,e),e&&a(Ga),e&&a(Ye),e&&a(Ja),j(ue,e),e&&a(Ka),e&&a(Ge),e&&a(Qa),j(de,e),e&&a(Xa),e&&a(S),e&&a(Za),j(_e,e),e&&a(et),e&&a(Je),e&&a(at),j(ge,e),e&&a(tt),e&&a(W),e&&a(st),e&&a(R),j($e),e&&a(lt),e&&a(Ke),e&&a(rt),j(be,e),e&&a(ot),e&&a(G),e&&a(nt),j(je,e),e&&a(it),e&&a(J),e&&a(pt),j(we,e),e&&a(ft),e&&a(K),e&&a(mt),j(Ee,e),e&&a(ht),e&&a(Qe),e&&a(ct),j(ke,e),e&&a(ut),e&&a(U),j(xe),e&&a(dt),e&&a(X),e&&a(_t),j(Te,e),e&&a(gt),j(Z,e),e&&a($t),e&&a(Ze),e&&a(bt),e&&a(O),e&&a(vt),j(qe,e),e&&a(jt),e&&a(V),j(Fe),e&&a(wt),e&&a(sa),e&&a(yt),j(ae,e),e&&a(Et),e&&a(te),e&&a(kt),j(De,e),e&&a(xt),e&&a(q),e&&a(Tt),j(ze,e),e&&a(At),e&&a(L),e&&a(Ct),j(Ie,e),e&&a(qt),e&&a(F),e&&a(Ft),j(Oe,e),e&&a(Dt),e&&a(ra),e&&a(Pt),j(Le,e),e&&a(zt),e&&a(se),e&&a(It),j(Me,e),e&&a(St),e&&a(le),e&&a(Ot),j(Re,e),e&&a(Lt),e&&a(re),e&&a(Mt),j(Ve,e),e&&a(Nt),j(oe,e)}}}const ao={local:"image-classification",sections:[{local:"load-food101-dataset",title:"Load Food-101 dataset"},{local:"preprocess",title:"Preprocess"},{local:"finetune-with-trainer",title:"Fine-tune with Trainer"},{local:"finetune-with-tensorflow",title:"Fine-tune with TensorFlow"}],title:"Image classification"};function to(P,c,E){let{fw:u}=c;return P.$$set=y=>{"fw"in y&&E(0,u=y.fw)},[u]}class po extends Br{constructor(c){super();Wr(this,c,to,eo,Yr,{fw:0})}}export{po as default,ao as metadata};
