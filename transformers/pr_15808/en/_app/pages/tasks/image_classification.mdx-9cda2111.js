import{S as Pt,i as Ft,s as It,e as o,k as m,w as $,t as l,M as zt,c as n,d as a,m as f,a as i,x as v,h as r,b as c,F as s,g as p,y as b,q as j,o as w,B as y}from"../../chunks/vendor-4833417e.js";import{T as Ys}from"../../chunks/Tip-fffd6df1.js";import{Y as St}from"../../chunks/Youtube-27813aed.js";import{I as Ha}from"../../chunks/IconCopyLink-4b81c553.js";import{C}from"../../chunks/CodeBlock-6a3d1b46.js";import"../../chunks/CopyButton-dacfbfaf.js";function Dt(S){let h,k,u,g,x;return{c(){h=o("p"),k=l("See the image classification "),u=o("a"),g=l("task page"),x=l(" for more information about its associated models, datasets, and metrics."),this.h()},l(d){h=n(d,"P",{});var _=i(h);k=r(_,"See the image classification "),u=n(_,"A",{href:!0,rel:!0});var E=i(u);g=r(E,"task page"),E.forEach(a),x=r(_," for more information about its associated models, datasets, and metrics."),_.forEach(a),this.h()},h(){c(u,"href","https://huggingface.co/tasks/audio-classification"),c(u,"rel","nofollow")},m(d,_){p(d,h,_),s(h,k),s(h,u),s(u,g),s(h,x)},d(d){d&&a(h)}}}function Lt(S){let h,k,u,g,x,d,_,E;return{c(){h=o("p"),k=l("If you aren\u2019t familiar with fine-tuning a model with the "),u=o("a"),g=l("Trainer"),x=l(", take a look at the basic tutorial "),d=o("a"),_=l("here"),E=l("!"),this.h()},l(D){h=n(D,"P",{});var A=i(h);k=r(A,"If you aren\u2019t familiar with fine-tuning a model with the "),u=n(A,"A",{href:!0});var P=i(u);g=r(P,"Trainer"),P.forEach(a),x=r(A,", take a look at the basic tutorial "),d=n(A,"A",{href:!0});var Q=i(d);_=r(Q,"here"),Q.forEach(a),E=r(A,"!"),A.forEach(a),this.h()},h(){c(u,"href","/docs/transformers/pr_15808/en/main_classes/trainer#transformers.Trainer"),c(d,"href","training#finetune-with-trainer")},m(D,A){p(D,h,A),s(h,k),s(h,u),s(u,g),s(h,x),s(h,d),s(d,_),s(h,E)},d(D){D&&a(h)}}}function Nt(S){let h,k,u,g,x;return{c(){h=o("p"),k=l("For a more in-depth example of how to fine-tune a model for image classification, take a look at the corresponding "),u=o("a"),g=l("PyTorch notebook"),x=l("."),this.h()},l(d){h=n(d,"P",{});var _=i(h);k=r(_,"For a more in-depth example of how to fine-tune a model for image classification, take a look at the corresponding "),u=n(_,"A",{href:!0,rel:!0});var E=i(u);g=r(E,"PyTorch notebook"),E.forEach(a),x=r(_,"."),_.forEach(a),this.h()},h(){c(u,"href","https://colab.research.google.com/github/huggingface/notebooks/blob/master/examples/image_classification.ipynb"),c(u,"rel","nofollow")},m(d,_){p(d,h,_),s(h,k),s(h,u),s(u,g),s(h,x)},d(d){d&&a(h)}}}function Ot(S){let h,k,u,g,x,d,_,E,D,A,P,Q,be,Va,Ze,F,Ua,X,Ga,Ja,Z,Ya,Wa,ea,R,aa,L,M,Se,ee,Ka,De,Qa,sa,je,Xa,ta,ae,la,we,Za,ra,se,oa,ye,es,na,te,ia,I,as,Le,ss,ts,Ne,ls,rs,pa,le,ma,ke,os,fa,re,ha,B,ns,Oe,is,ps,ca,N,H,Re,oe,ms,Me,fs,ua,xe,hs,da,ne,ga,V,cs,ie,Be,us,ds,_a,pe,$a,U,gs,He,_s,$s,va,me,ba,G,vs,fe,Ve,bs,js,ja,he,wa,Ee,ws,ya,ce,ka,O,J,Ue,ue,ys,Ge,ks,xa,Y,xs,Te,Es,Ts,Ea,de,Ta,W,Aa,Ae,As,qa,z,T,qs,qe,Cs,Ps,Je,Fs,Is,Ye,zs,Ss,We,Ds,Ls,Ke,Ns,Os,Rs,ge,Ms,Ce,Bs,Hs,Vs,_e,Us,Pe,Gs,Js,Ca,$e,Pa,K,Fa;return d=new Ha({}),P=new St({props:{id:"tjAIM7BOYhw"}}),R=new Ys({props:{$$slots:{default:[Dt]},$$scope:{ctx:S}}}),ee=new Ha({}),ae=new C({props:{code:`from datasets import load_dataset

food = load_dataset("food101", split="train[:5000]")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

<span class="hljs-meta">&gt;&gt;&gt; </span>food = load_dataset(<span class="hljs-string">&quot;food101&quot;</span>, split=<span class="hljs-string">&quot;train[:5000]&quot;</span>)`}}),se=new C({props:{code:"food = food.train_test_split(test_size=0.2)",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>food = food.train_test_split(test_size=<span class="hljs-number">0.2</span>)'}}),te=new C({props:{code:'food["train"][0]',highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>food[<span class="hljs-string">&quot;train&quot;</span>][<span class="hljs-number">0</span>]
{<span class="hljs-string">&#x27;image&#x27;</span>: &lt;PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=512x512 at <span class="hljs-number">0x7F52AFC8AC50</span>&gt;,
 <span class="hljs-string">&#x27;label&#x27;</span>: <span class="hljs-number">79</span>}`}}),le=new C({props:{code:`labels = food["train"].features["label"].names
label2id, id2label = dict(), dict()
for i, label in enumerate(labels):
    label2id[label] = str(i)
    id2label[str(i)] = label`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>labels = food[<span class="hljs-string">&quot;train&quot;</span>].features[<span class="hljs-string">&quot;label&quot;</span>].names
<span class="hljs-meta">&gt;&gt;&gt; </span>label2id, id2label = <span class="hljs-built_in">dict</span>(), <span class="hljs-built_in">dict</span>()
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">for</span> i, label <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(labels):
<span class="hljs-meta">... </span>    label2id[label] = <span class="hljs-built_in">str</span>(i)
<span class="hljs-meta">... </span>    id2label[<span class="hljs-built_in">str</span>(i)] = label`}}),re=new C({props:{code:"id2label[str(79)]",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>id2label[<span class="hljs-built_in">str</span>(<span class="hljs-number">79</span>)]
<span class="hljs-string">&#x27;prime_rib&#x27;</span>`}}),oe=new Ha({}),ne=new C({props:{code:`from transformers import AutoFeatureExtractor

feature_extractor = AutoFeatureExtractor.from_pretrained("google/vit-base-patch16-224-in21k")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoFeatureExtractor

<span class="hljs-meta">&gt;&gt;&gt; </span>feature_extractor = AutoFeatureExtractor.from_pretrained(<span class="hljs-string">&quot;google/vit-base-patch16-224-in21k&quot;</span>)`}}),pe=new C({props:{code:`from torchvision.transforms import RandomResizedCrop, Compose, Normalize, ToTensor

normalize = Normalize(mean=feature_extractor.image_mean, std=feature_extractor.image_std)
_transforms = Compose([RandomResizedCrop(feature_extractor.size), ToTensor(), normalize])`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> torchvision.transforms <span class="hljs-keyword">import</span> RandomResizedCrop, Compose, Normalize, ToTensor

<span class="hljs-meta">&gt;&gt;&gt; </span>normalize = Normalize(mean=feature_extractor.image_mean, std=feature_extractor.image_std)
<span class="hljs-meta">&gt;&gt;&gt; </span>_transforms = Compose([RandomResizedCrop(feature_extractor.size), ToTensor(), normalize])`}}),me=new C({props:{code:`def transforms(examples):
    examples["pixel_values"] = [_transforms(img.convert("RGB")) for img in examples["image"]]
    return examples`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">def</span> <span class="hljs-title function_">transforms</span>(<span class="hljs-params">examples</span>):
<span class="hljs-meta">... </span>    examples[<span class="hljs-string">&quot;pixel_values&quot;</span>] = [_transforms(img.convert(<span class="hljs-string">&quot;RGB&quot;</span>)) <span class="hljs-keyword">for</span> img <span class="hljs-keyword">in</span> examples[<span class="hljs-string">&quot;image&quot;</span>]]
<span class="hljs-meta">... </span>    <span class="hljs-keyword">return</span> examples`}}),he=new C({props:{code:"food = food.with_transform(transforms)",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>food = food.with_transform(transforms)'}}),ce=new C({props:{code:`import torch


def data_collator(examples):
    pixel_values = torch.stack([example["pixel_values"] for example in examples])
    labels = torch.tensor([example["label"] for example in examples])
    return {"pixel_values": pixel_values, "labels": labels}`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch


<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">def</span> <span class="hljs-title function_">data_collator</span>(<span class="hljs-params">examples</span>):
<span class="hljs-meta">... </span>    pixel_values = torch.stack([example[<span class="hljs-string">&quot;pixel_values&quot;</span>] <span class="hljs-keyword">for</span> example <span class="hljs-keyword">in</span> examples])
<span class="hljs-meta">... </span>    labels = torch.tensor([example[<span class="hljs-string">&quot;label&quot;</span>] <span class="hljs-keyword">for</span> example <span class="hljs-keyword">in</span> examples])
<span class="hljs-meta">... </span>    <span class="hljs-keyword">return</span> {<span class="hljs-string">&quot;pixel_values&quot;</span>: pixel_values, <span class="hljs-string">&quot;labels&quot;</span>: labels}`}}),ue=new Ha({}),de=new C({props:{code:`from transformers import AutoModelForImageClassification, TrainingArguments, Trainer

model = AutoModelForImageClassification.from_pretrained(
    "google/vit-base-patch16-224-in21k",
    num_labels=len(labels),
    id2label=id2label,
    label2id=label2id,
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForImageClassification, TrainingArguments, Trainer

<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForImageClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;google/vit-base-patch16-224-in21k&quot;</span>,
<span class="hljs-meta">... </span>    num_labels=<span class="hljs-built_in">len</span>(labels),
<span class="hljs-meta">... </span>    id2label=id2label,
<span class="hljs-meta">... </span>    label2id=label2id,
<span class="hljs-meta">... </span>)`}}),W=new Ys({props:{$$slots:{default:[Lt]},$$scope:{ctx:S}}}),$e=new C({props:{code:`training_args = TrainingArguments(
    output_dir="./results",
    per_device_train_batch_size=16,
    evaluation_strategy="steps",
    num_train_epochs=4,
    fp16=True,
    save_steps=100,
    eval_steps=100,
    logging_steps=10,
    learning_rate=2e-4,
    save_total_limit=2,
    remove_unused_columns=False,
)

trainer = Trainer(
    model=model,
    args=training_args,
    data_collator=data_collator,
    train_dataset=food["train"],
    eval_dataset=food["test"],
    tokenizer=feature_extractor,
)

trainer.train()`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>training_args = TrainingArguments(
<span class="hljs-meta">... </span>    output_dir=<span class="hljs-string">&quot;./results&quot;</span>,
<span class="hljs-meta">... </span>    per_device_train_batch_size=<span class="hljs-number">16</span>,
<span class="hljs-meta">... </span>    evaluation_strategy=<span class="hljs-string">&quot;steps&quot;</span>,
<span class="hljs-meta">... </span>    num_train_epochs=<span class="hljs-number">4</span>,
<span class="hljs-meta">... </span>    fp16=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>    save_steps=<span class="hljs-number">100</span>,
<span class="hljs-meta">... </span>    eval_steps=<span class="hljs-number">100</span>,
<span class="hljs-meta">... </span>    logging_steps=<span class="hljs-number">10</span>,
<span class="hljs-meta">... </span>    learning_rate=<span class="hljs-number">2e-4</span>,
<span class="hljs-meta">... </span>    save_total_limit=<span class="hljs-number">2</span>,
<span class="hljs-meta">... </span>    remove_unused_columns=<span class="hljs-literal">False</span>,
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>trainer = Trainer(
<span class="hljs-meta">... </span>    model=model,
<span class="hljs-meta">... </span>    args=training_args,
<span class="hljs-meta">... </span>    data_collator=data_collator,
<span class="hljs-meta">... </span>    train_dataset=food[<span class="hljs-string">&quot;train&quot;</span>],
<span class="hljs-meta">... </span>    eval_dataset=food[<span class="hljs-string">&quot;test&quot;</span>],
<span class="hljs-meta">... </span>    tokenizer=feature_extractor,
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>trainer.train()`}}),K=new Ys({props:{$$slots:{default:[Nt]},$$scope:{ctx:S}}}),{c(){h=o("meta"),k=m(),u=o("h1"),g=o("a"),x=o("span"),$(d.$$.fragment),_=m(),E=o("span"),D=l("Image classification"),A=m(),$(P.$$.fragment),Q=m(),be=o("p"),Va=l("Image classification assigns a label or class to an image. Unlike text or audio classification, the inputs are the pixel values that represent an image. There are many uses for image classification, like detecting damage after a disaster, monitoring crop health, or helping screen medical images for signs of disease."),Ze=m(),F=o("p"),Ua=l("This guide will show you how to fine-tune "),X=o("a"),Ga=l("ViT"),Ja=l(" on the "),Z=o("a"),Ya=l("Food-101"),Wa=l(" dataset to classify a food item in an image."),ea=m(),$(R.$$.fragment),aa=m(),L=o("h2"),M=o("a"),Se=o("span"),$(ee.$$.fragment),Ka=m(),De=o("span"),Qa=l("Load Food-101 dataset"),sa=m(),je=o("p"),Xa=l("Load only the first 5000 images of the Food-101 dataset from the \u{1F917} Datasets library since it is pretty large:"),ta=m(),$(ae.$$.fragment),la=m(),we=o("p"),Za=l("Split this dataset into a train and test set:"),ra=m(),$(se.$$.fragment),oa=m(),ye=o("p"),es=l("Then take a look at an example:"),na=m(),$(te.$$.fragment),ia=m(),I=o("p"),as=l("The "),Le=o("code"),ss=l("image"),ts=l(" field contains a PIL image, and each "),Ne=o("code"),ls=l("label"),rs=l(" is an integer that represents a class. Create a dictionary that maps a label name to an integer and vice versa. The mapping will help the model recover the label name from the label number:"),pa=m(),$(le.$$.fragment),ma=m(),ke=o("p"),os=l("Now you can convert the label number to a label name for more information:"),fa=m(),$(re.$$.fragment),ha=m(),B=o("p"),ns=l("Each food class - or label - corresponds to a number; "),Oe=o("code"),is=l("79"),ps=l(" indicates a prime rib in the example above."),ca=m(),N=o("h2"),H=o("a"),Re=o("span"),$(oe.$$.fragment),ms=m(),Me=o("span"),fs=l("Preprocess"),ua=m(),xe=o("p"),hs=l("Load the ViT feature extractor to process the image into a tensor:"),da=m(),$(ne.$$.fragment),ga=m(),V=o("p"),cs=l("Apply several image transformations to the dataset to make the model more robust against overfitting. Here you\u2019ll use torchvision\u2019s "),ie=o("a"),Be=o("code"),us=l("transforms"),ds=l(" module. Crop a random part of the image, resize it, and normalize it with the image mean and standard deviation:"),_a=m(),$(pe.$$.fragment),$a=m(),U=o("p"),gs=l("Create a preprocessing function that will apply the transforms and return the "),He=o("code"),_s=l("pixel_values"),$s=l(" - the inputs to the model - of the image:"),va=m(),$(me.$$.fragment),ba=m(),G=o("p"),vs=l("Use \u{1F917} Dataset\u2019s "),fe=o("a"),Ve=o("code"),bs=l("with_transform"),js=l(" method to apply the transforms over the entire dataset. The transforms are applied on-the-fly when you load an element of the dataset:"),ja=m(),$(he.$$.fragment),wa=m(),Ee=o("p"),ws=l("\u{1F917} Transformers doesn\u2019t have a data collator for image classification, so you will need to create one to build a batch of examples:"),ya=m(),$(ce.$$.fragment),ka=m(),O=o("h2"),J=o("a"),Ue=o("span"),$(ue.$$.fragment),ys=m(),Ge=o("span"),ks=l("Fine-tune with Trainer"),xa=m(),Y=o("p"),xs=l("Load ViT with "),Te=o("a"),Es=l("AutoModelForImageClassification"),Ts=l(". Specify the number of labels, and pass the model the mapping between label number and label class:"),Ea=m(),$(de.$$.fragment),Ta=m(),$(W.$$.fragment),Aa=m(),Ae=o("p"),As=l("At this point, only three steps remain:"),qa=m(),z=o("ol"),T=o("li"),qs=l("Define your training hyperparameters in "),qe=o("a"),Cs=l("TrainingArguments"),Ps=l(". It is important you don\u2019t remove unused columns because this will drop the "),Je=o("code"),Fs=l("image"),Is=l(" column. Without the "),Ye=o("code"),zs=l("image"),Ss=l(" column, you can\u2019t create "),We=o("code"),Ds=l("pixel_values"),Ls=l(". Set "),Ke=o("code"),Ns=l("remove_unused_columns=False"),Os=l(" to prevent this behavior!"),Rs=m(),ge=o("li"),Ms=l("Pass the training arguments to "),Ce=o("a"),Bs=l("Trainer"),Hs=l(" along with the model, datasets, tokenizer, and data collator."),Vs=m(),_e=o("li"),Us=l("Call "),Pe=o("a"),Gs=l("train()"),Js=l(" to fine-tune your model."),Ca=m(),$($e.$$.fragment),Pa=m(),$(K.$$.fragment),this.h()},l(e){const t=zt('[data-svelte="svelte-1phssyn"]',document.head);h=n(t,"META",{name:!0,content:!0}),t.forEach(a),k=f(e),u=n(e,"H1",{class:!0});var ve=i(u);g=n(ve,"A",{id:!0,class:!0,href:!0});var Qe=i(g);x=n(Qe,"SPAN",{});var Xe=i(x);v(d.$$.fragment,Xe),Xe.forEach(a),Qe.forEach(a),_=f(ve),E=n(ve,"SPAN",{});var Ws=i(E);D=r(Ws,"Image classification"),Ws.forEach(a),ve.forEach(a),A=f(e),v(P.$$.fragment,e),Q=f(e),be=n(e,"P",{});var Ks=i(be);Va=r(Ks,"Image classification assigns a label or class to an image. Unlike text or audio classification, the inputs are the pixel values that represent an image. There are many uses for image classification, like detecting damage after a disaster, monitoring crop health, or helping screen medical images for signs of disease."),Ks.forEach(a),Ze=f(e),F=n(e,"P",{});var Fe=i(F);Ua=r(Fe,"This guide will show you how to fine-tune "),X=n(Fe,"A",{href:!0,rel:!0});var Qs=i(X);Ga=r(Qs,"ViT"),Qs.forEach(a),Ja=r(Fe," on the "),Z=n(Fe,"A",{href:!0,rel:!0});var Xs=i(Z);Ya=r(Xs,"Food-101"),Xs.forEach(a),Wa=r(Fe," dataset to classify a food item in an image."),Fe.forEach(a),ea=f(e),v(R.$$.fragment,e),aa=f(e),L=n(e,"H2",{class:!0});var Ia=i(L);M=n(Ia,"A",{id:!0,class:!0,href:!0});var Zs=i(M);Se=n(Zs,"SPAN",{});var et=i(Se);v(ee.$$.fragment,et),et.forEach(a),Zs.forEach(a),Ka=f(Ia),De=n(Ia,"SPAN",{});var at=i(De);Qa=r(at,"Load Food-101 dataset"),at.forEach(a),Ia.forEach(a),sa=f(e),je=n(e,"P",{});var st=i(je);Xa=r(st,"Load only the first 5000 images of the Food-101 dataset from the \u{1F917} Datasets library since it is pretty large:"),st.forEach(a),ta=f(e),v(ae.$$.fragment,e),la=f(e),we=n(e,"P",{});var tt=i(we);Za=r(tt,"Split this dataset into a train and test set:"),tt.forEach(a),ra=f(e),v(se.$$.fragment,e),oa=f(e),ye=n(e,"P",{});var lt=i(ye);es=r(lt,"Then take a look at an example:"),lt.forEach(a),na=f(e),v(te.$$.fragment,e),ia=f(e),I=n(e,"P",{});var Ie=i(I);as=r(Ie,"The "),Le=n(Ie,"CODE",{});var rt=i(Le);ss=r(rt,"image"),rt.forEach(a),ts=r(Ie," field contains a PIL image, and each "),Ne=n(Ie,"CODE",{});var ot=i(Ne);ls=r(ot,"label"),ot.forEach(a),rs=r(Ie," is an integer that represents a class. Create a dictionary that maps a label name to an integer and vice versa. The mapping will help the model recover the label name from the label number:"),Ie.forEach(a),pa=f(e),v(le.$$.fragment,e),ma=f(e),ke=n(e,"P",{});var nt=i(ke);os=r(nt,"Now you can convert the label number to a label name for more information:"),nt.forEach(a),fa=f(e),v(re.$$.fragment,e),ha=f(e),B=n(e,"P",{});var za=i(B);ns=r(za,"Each food class - or label - corresponds to a number; "),Oe=n(za,"CODE",{});var it=i(Oe);is=r(it,"79"),it.forEach(a),ps=r(za," indicates a prime rib in the example above."),za.forEach(a),ca=f(e),N=n(e,"H2",{class:!0});var Sa=i(N);H=n(Sa,"A",{id:!0,class:!0,href:!0});var pt=i(H);Re=n(pt,"SPAN",{});var mt=i(Re);v(oe.$$.fragment,mt),mt.forEach(a),pt.forEach(a),ms=f(Sa),Me=n(Sa,"SPAN",{});var ft=i(Me);fs=r(ft,"Preprocess"),ft.forEach(a),Sa.forEach(a),ua=f(e),xe=n(e,"P",{});var ht=i(xe);hs=r(ht,"Load the ViT feature extractor to process the image into a tensor:"),ht.forEach(a),da=f(e),v(ne.$$.fragment,e),ga=f(e),V=n(e,"P",{});var Da=i(V);cs=r(Da,"Apply several image transformations to the dataset to make the model more robust against overfitting. Here you\u2019ll use torchvision\u2019s "),ie=n(Da,"A",{href:!0,rel:!0});var ct=i(ie);Be=n(ct,"CODE",{});var ut=i(Be);us=r(ut,"transforms"),ut.forEach(a),ct.forEach(a),ds=r(Da," module. Crop a random part of the image, resize it, and normalize it with the image mean and standard deviation:"),Da.forEach(a),_a=f(e),v(pe.$$.fragment,e),$a=f(e),U=n(e,"P",{});var La=i(U);gs=r(La,"Create a preprocessing function that will apply the transforms and return the "),He=n(La,"CODE",{});var dt=i(He);_s=r(dt,"pixel_values"),dt.forEach(a),$s=r(La," - the inputs to the model - of the image:"),La.forEach(a),va=f(e),v(me.$$.fragment,e),ba=f(e),G=n(e,"P",{});var Na=i(G);vs=r(Na,"Use \u{1F917} Dataset\u2019s "),fe=n(Na,"A",{href:!0,rel:!0});var gt=i(fe);Ve=n(gt,"CODE",{});var _t=i(Ve);bs=r(_t,"with_transform"),_t.forEach(a),gt.forEach(a),js=r(Na," method to apply the transforms over the entire dataset. The transforms are applied on-the-fly when you load an element of the dataset:"),Na.forEach(a),ja=f(e),v(he.$$.fragment,e),wa=f(e),Ee=n(e,"P",{});var $t=i(Ee);ws=r($t,"\u{1F917} Transformers doesn\u2019t have a data collator for image classification, so you will need to create one to build a batch of examples:"),$t.forEach(a),ya=f(e),v(ce.$$.fragment,e),ka=f(e),O=n(e,"H2",{class:!0});var Oa=i(O);J=n(Oa,"A",{id:!0,class:!0,href:!0});var vt=i(J);Ue=n(vt,"SPAN",{});var bt=i(Ue);v(ue.$$.fragment,bt),bt.forEach(a),vt.forEach(a),ys=f(Oa),Ge=n(Oa,"SPAN",{});var jt=i(Ge);ks=r(jt,"Fine-tune with Trainer"),jt.forEach(a),Oa.forEach(a),xa=f(e),Y=n(e,"P",{});var Ra=i(Y);xs=r(Ra,"Load ViT with "),Te=n(Ra,"A",{href:!0});var wt=i(Te);Es=r(wt,"AutoModelForImageClassification"),wt.forEach(a),Ts=r(Ra,". Specify the number of labels, and pass the model the mapping between label number and label class:"),Ra.forEach(a),Ea=f(e),v(de.$$.fragment,e),Ta=f(e),v(W.$$.fragment,e),Aa=f(e),Ae=n(e,"P",{});var yt=i(Ae);As=r(yt,"At this point, only three steps remain:"),yt.forEach(a),qa=f(e),z=n(e,"OL",{});var ze=i(z);T=n(ze,"LI",{});var q=i(T);qs=r(q,"Define your training hyperparameters in "),qe=n(q,"A",{href:!0});var kt=i(qe);Cs=r(kt,"TrainingArguments"),kt.forEach(a),Ps=r(q,". It is important you don\u2019t remove unused columns because this will drop the "),Je=n(q,"CODE",{});var xt=i(Je);Fs=r(xt,"image"),xt.forEach(a),Is=r(q," column. Without the "),Ye=n(q,"CODE",{});var Et=i(Ye);zs=r(Et,"image"),Et.forEach(a),Ss=r(q," column, you can\u2019t create "),We=n(q,"CODE",{});var Tt=i(We);Ds=r(Tt,"pixel_values"),Tt.forEach(a),Ls=r(q,". Set "),Ke=n(q,"CODE",{});var At=i(Ke);Ns=r(At,"remove_unused_columns=False"),At.forEach(a),Os=r(q," to prevent this behavior!"),q.forEach(a),Rs=f(ze),ge=n(ze,"LI",{});var Ma=i(ge);Ms=r(Ma,"Pass the training arguments to "),Ce=n(Ma,"A",{href:!0});var qt=i(Ce);Bs=r(qt,"Trainer"),qt.forEach(a),Hs=r(Ma," along with the model, datasets, tokenizer, and data collator."),Ma.forEach(a),Vs=f(ze),_e=n(ze,"LI",{});var Ba=i(_e);Us=r(Ba,"Call "),Pe=n(Ba,"A",{href:!0});var Ct=i(Pe);Gs=r(Ct,"train()"),Ct.forEach(a),Js=r(Ba," to fine-tune your model."),Ba.forEach(a),ze.forEach(a),Ca=f(e),v($e.$$.fragment,e),Pa=f(e),v(K.$$.fragment,e),this.h()},h(){c(h,"name","hf:doc:metadata"),c(h,"content",JSON.stringify(Rt)),c(g,"id","image-classification"),c(g,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(g,"href","#image-classification"),c(u,"class","relative group"),c(X,"href","https://huggingface.co/docs/transformers/v4.16.2/en/model_doc/vit"),c(X,"rel","nofollow"),c(Z,"href","https://huggingface.co/datasets/food101"),c(Z,"rel","nofollow"),c(M,"id","load-food101-dataset"),c(M,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(M,"href","#load-food101-dataset"),c(L,"class","relative group"),c(H,"id","preprocess"),c(H,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(H,"href","#preprocess"),c(N,"class","relative group"),c(ie,"href","https://pytorch.org/vision/stable/transforms.html"),c(ie,"rel","nofollow"),c(fe,"href","https://huggingface.co/docs/datasets/package_reference/main_classes.html?#datasets.Dataset.with_transform"),c(fe,"rel","nofollow"),c(J,"id","finetune-with-trainer"),c(J,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(J,"href","#finetune-with-trainer"),c(O,"class","relative group"),c(Te,"href","/docs/transformers/pr_15808/en/model_doc/auto#transformers.AutoModelForImageClassification"),c(qe,"href","/docs/transformers/pr_15808/en/main_classes/trainer#transformers.TrainingArguments"),c(Ce,"href","/docs/transformers/pr_15808/en/main_classes/trainer#transformers.Trainer"),c(Pe,"href","/docs/transformers/pr_15808/en/main_classes/trainer#transformers.Trainer.train")},m(e,t){s(document.head,h),p(e,k,t),p(e,u,t),s(u,g),s(g,x),b(d,x,null),s(u,_),s(u,E),s(E,D),p(e,A,t),b(P,e,t),p(e,Q,t),p(e,be,t),s(be,Va),p(e,Ze,t),p(e,F,t),s(F,Ua),s(F,X),s(X,Ga),s(F,Ja),s(F,Z),s(Z,Ya),s(F,Wa),p(e,ea,t),b(R,e,t),p(e,aa,t),p(e,L,t),s(L,M),s(M,Se),b(ee,Se,null),s(L,Ka),s(L,De),s(De,Qa),p(e,sa,t),p(e,je,t),s(je,Xa),p(e,ta,t),b(ae,e,t),p(e,la,t),p(e,we,t),s(we,Za),p(e,ra,t),b(se,e,t),p(e,oa,t),p(e,ye,t),s(ye,es),p(e,na,t),b(te,e,t),p(e,ia,t),p(e,I,t),s(I,as),s(I,Le),s(Le,ss),s(I,ts),s(I,Ne),s(Ne,ls),s(I,rs),p(e,pa,t),b(le,e,t),p(e,ma,t),p(e,ke,t),s(ke,os),p(e,fa,t),b(re,e,t),p(e,ha,t),p(e,B,t),s(B,ns),s(B,Oe),s(Oe,is),s(B,ps),p(e,ca,t),p(e,N,t),s(N,H),s(H,Re),b(oe,Re,null),s(N,ms),s(N,Me),s(Me,fs),p(e,ua,t),p(e,xe,t),s(xe,hs),p(e,da,t),b(ne,e,t),p(e,ga,t),p(e,V,t),s(V,cs),s(V,ie),s(ie,Be),s(Be,us),s(V,ds),p(e,_a,t),b(pe,e,t),p(e,$a,t),p(e,U,t),s(U,gs),s(U,He),s(He,_s),s(U,$s),p(e,va,t),b(me,e,t),p(e,ba,t),p(e,G,t),s(G,vs),s(G,fe),s(fe,Ve),s(Ve,bs),s(G,js),p(e,ja,t),b(he,e,t),p(e,wa,t),p(e,Ee,t),s(Ee,ws),p(e,ya,t),b(ce,e,t),p(e,ka,t),p(e,O,t),s(O,J),s(J,Ue),b(ue,Ue,null),s(O,ys),s(O,Ge),s(Ge,ks),p(e,xa,t),p(e,Y,t),s(Y,xs),s(Y,Te),s(Te,Es),s(Y,Ts),p(e,Ea,t),b(de,e,t),p(e,Ta,t),b(W,e,t),p(e,Aa,t),p(e,Ae,t),s(Ae,As),p(e,qa,t),p(e,z,t),s(z,T),s(T,qs),s(T,qe),s(qe,Cs),s(T,Ps),s(T,Je),s(Je,Fs),s(T,Is),s(T,Ye),s(Ye,zs),s(T,Ss),s(T,We),s(We,Ds),s(T,Ls),s(T,Ke),s(Ke,Ns),s(T,Os),s(z,Rs),s(z,ge),s(ge,Ms),s(ge,Ce),s(Ce,Bs),s(ge,Hs),s(z,Vs),s(z,_e),s(_e,Us),s(_e,Pe),s(Pe,Gs),s(_e,Js),p(e,Ca,t),b($e,e,t),p(e,Pa,t),b(K,e,t),Fa=!0},p(e,[t]){const ve={};t&2&&(ve.$$scope={dirty:t,ctx:e}),R.$set(ve);const Qe={};t&2&&(Qe.$$scope={dirty:t,ctx:e}),W.$set(Qe);const Xe={};t&2&&(Xe.$$scope={dirty:t,ctx:e}),K.$set(Xe)},i(e){Fa||(j(d.$$.fragment,e),j(P.$$.fragment,e),j(R.$$.fragment,e),j(ee.$$.fragment,e),j(ae.$$.fragment,e),j(se.$$.fragment,e),j(te.$$.fragment,e),j(le.$$.fragment,e),j(re.$$.fragment,e),j(oe.$$.fragment,e),j(ne.$$.fragment,e),j(pe.$$.fragment,e),j(me.$$.fragment,e),j(he.$$.fragment,e),j(ce.$$.fragment,e),j(ue.$$.fragment,e),j(de.$$.fragment,e),j(W.$$.fragment,e),j($e.$$.fragment,e),j(K.$$.fragment,e),Fa=!0)},o(e){w(d.$$.fragment,e),w(P.$$.fragment,e),w(R.$$.fragment,e),w(ee.$$.fragment,e),w(ae.$$.fragment,e),w(se.$$.fragment,e),w(te.$$.fragment,e),w(le.$$.fragment,e),w(re.$$.fragment,e),w(oe.$$.fragment,e),w(ne.$$.fragment,e),w(pe.$$.fragment,e),w(me.$$.fragment,e),w(he.$$.fragment,e),w(ce.$$.fragment,e),w(ue.$$.fragment,e),w(de.$$.fragment,e),w(W.$$.fragment,e),w($e.$$.fragment,e),w(K.$$.fragment,e),Fa=!1},d(e){a(h),e&&a(k),e&&a(u),y(d),e&&a(A),y(P,e),e&&a(Q),e&&a(be),e&&a(Ze),e&&a(F),e&&a(ea),y(R,e),e&&a(aa),e&&a(L),y(ee),e&&a(sa),e&&a(je),e&&a(ta),y(ae,e),e&&a(la),e&&a(we),e&&a(ra),y(se,e),e&&a(oa),e&&a(ye),e&&a(na),y(te,e),e&&a(ia),e&&a(I),e&&a(pa),y(le,e),e&&a(ma),e&&a(ke),e&&a(fa),y(re,e),e&&a(ha),e&&a(B),e&&a(ca),e&&a(N),y(oe),e&&a(ua),e&&a(xe),e&&a(da),y(ne,e),e&&a(ga),e&&a(V),e&&a(_a),y(pe,e),e&&a($a),e&&a(U),e&&a(va),y(me,e),e&&a(ba),e&&a(G),e&&a(ja),y(he,e),e&&a(wa),e&&a(Ee),e&&a(ya),y(ce,e),e&&a(ka),e&&a(O),y(ue),e&&a(xa),e&&a(Y),e&&a(Ea),y(de,e),e&&a(Ta),y(W,e),e&&a(Aa),e&&a(Ae),e&&a(qa),e&&a(z),e&&a(Ca),y($e,e),e&&a(Pa),y(K,e)}}}const Rt={local:"image-classification",sections:[{local:"load-food101-dataset",title:"Load Food-101 dataset"},{local:"preprocess",title:"Preprocess"},{local:"finetune-with-trainer",title:"Fine-tune with Trainer"}],title:"Image classification"};function Mt(S,h,k){let{fw:u}=h;return S.$$set=g=>{"fw"in g&&k(0,u=g.fw)},[u]}class Yt extends Pt{constructor(h){super();Ft(this,h,Mt,Ot,It,{fw:0})}}export{Yt as default,Rt as metadata};
