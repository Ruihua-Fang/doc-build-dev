import{S as $n,i as vn,s as yn,e as o,k as c,w as j,t as e,L as wn,c as r,d as t,m as h,a as p,x as b,h as n,b as m,J as a,g as i,y as $,q as v,o as y,B as w}from"../../chunks/vendor-9e2b328e.js";import{T as Ae}from"../../chunks/Tip-76f97a76.js";import{Y as xn}from"../../chunks/Youtube-cb4469d8.js";import{I as et}from"../../chunks/IconCopyLink-fd0e58fd.js";import{C as D}from"../../chunks/CodeBlock-88e23343.js";import"../../chunks/CopyButton-4ae140ab.js";function kn(N){let f,x,u,_,k;return{c(){f=o("p"),x=e("See the automatic speech recognition "),u=o("a"),_=e("task page"),k=e(" for more information about its associated models, datasets, and metrics."),this.h()},l(d){f=r(d,"P",{});var g=p(f);x=n(g,"See the automatic speech recognition "),u=r(g,"A",{href:!0,rel:!0});var E=p(u);_=n(E,"task page"),E.forEach(t),k=n(g," for more information about its associated models, datasets, and metrics."),g.forEach(t),this.h()},h(){m(u,"href","https://huggingface.co/tasks/automatic-speech-recognition"),m(u,"rel","nofollow")},m(d,g){i(d,f,g),a(f,x),a(f,u),a(u,_),a(f,k)},d(d){d&&t(f)}}}function En(N){let f,x,u,_,k,d,g,E;return{c(){f=o("p"),x=e("If you aren\u2019t familiar with fine-tuning a model with the "),u=o("a"),_=e("Trainer"),k=e(", take a look at the basic tutorial "),d=o("a"),g=e("here"),E=e("!"),this.h()},l(z){f=r(z,"P",{});var q=p(f);x=n(q,"If you aren\u2019t familiar with fine-tuning a model with the "),u=r(q,"A",{href:!0});var I=p(u);_=n(I,"Trainer"),I.forEach(t),k=n(q,", take a look at the basic tutorial "),d=r(q,"A",{href:!0});var as=p(d);g=n(as,"here"),as.forEach(t),E=n(q,"!"),q.forEach(t),this.h()},h(){m(u,"href","/docs/transformers/pr_15808/en/main_classes/trainer#transformers.Trainer"),m(d,"href","training#finetune-with-trainer")},m(z,q){i(z,f,q),a(f,x),a(f,u),a(u,_),a(f,k),a(f,d),a(d,g),a(f,E)},d(z){z&&t(f)}}}function Tn(N){let f,x,u,_,k;return{c(){f=o("p"),x=e("For a more in-depth example of how to fine-tune a model for automatic speech recognition, take a look at the corresponding "),u=o("a"),_=e("PyTorch notebook"),k=e("."),this.h()},l(d){f=r(d,"P",{});var g=p(f);x=n(g,"For a more in-depth example of how to fine-tune a model for automatic speech recognition, take a look at the corresponding "),u=r(g,"A",{href:!0,rel:!0});var E=p(u);_=n(E,"PyTorch notebook"),E.forEach(t),k=n(g,"."),g.forEach(t),this.h()},h(){m(u,"href","https://colab.research.google.com/github/huggingface/notebooks/blob/master/examples/speech_recognition.ipynb"),m(u,"rel","nofollow")},m(d,g){i(d,f,g),a(f,x),a(f,u),a(u,_),a(f,k)},d(d){d&&t(f)}}}function qn(N){let f,x,u,_,k,d,g,E,z,q,I,as,ks,nt,ua,O,lt,ts,ot,rt,es,pt,it,ma,V,da,F,Y,Fs,ns,ct,Rs,ht,_a,Es,ft,ga,ls,ja,Ts,ut,ba,os,$a,T,mt,Us,dt,_t,Vs,gt,jt,Ys,bt,$t,Hs,vt,yt,va,rs,ya,qs,wt,wa,ps,xa,S,xt,Bs,kt,Et,Gs,Tt,qt,ka,R,H,Js,is,At,Ks,Ct,Ea,As,Pt,Ta,cs,qa,Cs,Dt,Aa,L,hs,It,Qs,Ot,St,Lt,fs,Wt,Xs,Mt,Nt,zt,Zs,Ft,Ca,us,Pa,A,Rt,ms,sa,Ut,Vt,aa,Yt,Ht,ta,Bt,Gt,Da,ds,Ia,C,Jt,Ps,Kt,Qt,ea,Xt,Zt,na,se,ae,Oa,P,te,la,ee,ne,oa,le,oe,ra,re,pe,Sa,_s,La,B,ie,pa,ce,he,Wa,gs,Ma,U,G,ia,js,fe,ca,ue,Na,J,me,Ds,de,_e,za,bs,Fa,K,Ra,Is,ge,Ua,W,$s,je,Os,be,$e,ve,vs,ye,Ss,we,xe,ke,ys,Ee,Ls,Te,qe,Va,ws,Ya,Q,Ha;return d=new et({}),I=new xn({props:{id:"TksaY_FDgnk"}}),V=new Ae({props:{$$slots:{default:[kn]},$$scope:{ctx:N}}}),ns=new et({}),ls=new D({props:{code:`from datasets import load_dataset

timit = load_dataset("timit_asr"),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

<span class="hljs-meta">&gt;&gt;&gt; </span>timit = load_dataset(<span class="hljs-string">&quot;timit_asr&quot;</span>)`}}),os=new D({props:{code:"timit,",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>timit
DatasetDict({
    train: Dataset({
        features: [<span class="hljs-string">&#x27;file&#x27;</span>, <span class="hljs-string">&#x27;audio&#x27;</span>, <span class="hljs-string">&#x27;text&#x27;</span>, <span class="hljs-string">&#x27;phonetic_detail&#x27;</span>, <span class="hljs-string">&#x27;word_detail&#x27;</span>, <span class="hljs-string">&#x27;dialect_region&#x27;</span>, <span class="hljs-string">&#x27;sentence_type&#x27;</span>, <span class="hljs-string">&#x27;speaker_id&#x27;</span>, <span class="hljs-string">&#x27;id&#x27;</span>],
        num_rows: <span class="hljs-number">4620</span>
    })
    test: Dataset({
        features: [<span class="hljs-string">&#x27;file&#x27;</span>, <span class="hljs-string">&#x27;audio&#x27;</span>, <span class="hljs-string">&#x27;text&#x27;</span>, <span class="hljs-string">&#x27;phonetic_detail&#x27;</span>, <span class="hljs-string">&#x27;word_detail&#x27;</span>, <span class="hljs-string">&#x27;dialect_region&#x27;</span>, <span class="hljs-string">&#x27;sentence_type&#x27;</span>, <span class="hljs-string">&#x27;speaker_id&#x27;</span>, <span class="hljs-string">&#x27;id&#x27;</span>],
        num_rows: <span class="hljs-number">1680</span>
    })
})`}}),rs=new D({props:{code:`timit = timit.remove_columns(
    ["phonetic_detail", "word_detail", "dialect_region", "id", "sentence_type", "speaker_id"]
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>timit = timit.remove_columns(
<span class="hljs-meta">... </span>    [<span class="hljs-string">&quot;phonetic_detail&quot;</span>, <span class="hljs-string">&quot;word_detail&quot;</span>, <span class="hljs-string">&quot;dialect_region&quot;</span>, <span class="hljs-string">&quot;id&quot;</span>, <span class="hljs-string">&quot;sentence_type&quot;</span>, <span class="hljs-string">&quot;speaker_id&quot;</span>]
<span class="hljs-meta">... </span>)`}}),ps=new D({props:{code:'timit["train"][0],',highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>timit[<span class="hljs-string">&quot;train&quot;</span>][<span class="hljs-number">0</span>]
{<span class="hljs-string">&#x27;audio&#x27;</span>: {<span class="hljs-string">&#x27;array&#x27;</span>: array([-<span class="hljs-number">2.1362305e-04</span>,  <span class="hljs-number">6.1035156e-05</span>,  <span class="hljs-number">3.0517578e-05</span>, ...,
         -<span class="hljs-number">3.0517578e-05</span>, -<span class="hljs-number">9.1552734e-05</span>, -<span class="hljs-number">6.1035156e-05</span>], dtype=float32),
  <span class="hljs-string">&#x27;path&#x27;</span>: <span class="hljs-string">&#x27;/root/.cache/huggingface/datasets/downloads/extracted/404950a46da14eac65eb4e2a8317b1372fb3971d980d91d5d5b221275b1fd7e0/data/TRAIN/DR4/MMDM0/SI681.WAV&#x27;</span>,
  <span class="hljs-string">&#x27;sampling_rate&#x27;</span>: <span class="hljs-number">16000</span>},
 <span class="hljs-string">&#x27;file&#x27;</span>: <span class="hljs-string">&#x27;/root/.cache/huggingface/datasets/downloads/extracted/404950a46da14eac65eb4e2a8317b1372fb3971d980d91d5d5b221275b1fd7e0/data/TRAIN/DR4/MMDM0/SI681.WAV&#x27;</span>,
 <span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;Would such an act of refusal be useful?&#x27;</span>}`}}),is=new et({}),cs=new D({props:{code:`from transformers import AutoProcessor

processor = AutoProcessor.from_pretrained("facebook/wav2vec2-base"),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoProcessor

<span class="hljs-meta">&gt;&gt;&gt; </span>processor = AutoProcessor.from_pretrained(<span class="hljs-string">&quot;facebook/wav2vec2-base&quot;</span>)`}}),us=new D({props:{code:`def prepare_dataset(batch):
    audio = batch["audio"]
    batch["input_values"] = processor(audio["array"], sampling_rate=16000).input_values[0]
    batch["input_length"] = len(batch["input_values"])

    with processor.as_target_processor():
         batch["labels"] = processor(batch["text"]).input_ids
    return batch,`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">def</span> <span class="hljs-title function_">prepare_dataset</span>(<span class="hljs-params">batch</span>):
<span class="hljs-meta">... </span>    audio = batch[<span class="hljs-string">&quot;audio&quot;</span>]
    
<span class="hljs-meta">... </span>    batch[<span class="hljs-string">&quot;input_values&quot;</span>] = processor(audio[<span class="hljs-string">&quot;array&quot;</span>], sampling_rate=<span class="hljs-number">16000</span>).input_values[<span class="hljs-number">0</span>]
<span class="hljs-meta">... </span>    batch[<span class="hljs-string">&quot;input_length&quot;</span>] = <span class="hljs-built_in">len</span>(batch[<span class="hljs-string">&quot;input_values&quot;</span>])

<span class="hljs-meta">... </span>    <span class="hljs-keyword">with</span> processor.as_target_processor():
<span class="hljs-meta">... </span>         batch[<span class="hljs-string">&quot;labels&quot;</span>] = processor(batch[<span class="hljs-string">&quot;text&quot;</span>]).input_ids
<span class="hljs-meta">... </span>    <span class="hljs-keyword">return</span> batch`}}),ds=new D({props:{code:'timit = timit.map(prepare_dataset, remove_columns=timit.column_names["train"], num_proc=4),',highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>timit = timit.<span class="hljs-built_in">map</span>(prepare_dataset, remove_columns=timit.column_names[<span class="hljs-string">&quot;train&quot;</span>], num_proc=<span class="hljs-number">4</span>)'}}),_s=new D({props:{code:`import torch

from dataclasses import dataclass, field
from typing import Any, Dict, List, Optional, Union


@dataclass
class DataCollatorCTCWithPadding:

    processor: AutoProcessor
    padding: Union[bool, str] = True
    max_length: Optional[int] = None
    max_length_labels: Optional[int] = None
    pad_to_multiple_of: Optional[int] = None
    pad_to_multiple_of_labels: Optional[int] = None

    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:
        # split inputs and labels since they have to be of different lengths and need
        # different padding methods
        input_features = [{"input_values": feature["input_values"]} for feature in features]
        label_features = [{"input_ids": feature["labels"]} for feature in features]

        batch = self.processor.pad(
            input_features,
            padding=self.padding,
            max_length=self.max_length,
            pad_to_multiple_of=self.pad_to_multiple_of,
            return_tensors="pt",
        )
        with self.processor.as_target_processor():
            labels_batch = self.processor.pad(
                label_features,
                padding=self.padding,
                max_length=self.max_length_labels,
                pad_to_multiple_of=self.pad_to_multiple_of_labels,
                return_tensors="pt",
            )

        # replace padding with -100 to ignore loss correctly
        labels = labels_batch["input_ids"].masked_fill(labels_batch.attention_mask.ne(1), -100)

        batch["labels"] = labels

        return batch,`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> dataclasses <span class="hljs-keyword">import</span> dataclass, field
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> typing <span class="hljs-keyword">import</span> <span class="hljs-type">Any</span>, <span class="hljs-type">Dict</span>, <span class="hljs-type">List</span>, <span class="hljs-type">Optional</span>, <span class="hljs-type">Union</span>


<span class="hljs-meta">&gt;&gt;&gt; </span>@dataclass
<span class="hljs-meta">... </span><span class="hljs-keyword">class</span> <span class="hljs-title class_">DataCollatorCTCWithPadding</span>:

<span class="hljs-meta">... </span>    processor: AutoProcessor
<span class="hljs-meta">... </span>    padding: <span class="hljs-type">Union</span>[<span class="hljs-built_in">bool</span>, <span class="hljs-built_in">str</span>] = <span class="hljs-literal">True</span>
<span class="hljs-meta">... </span>    max_length: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">int</span>] = <span class="hljs-literal">None</span>
<span class="hljs-meta">... </span>    max_length_labels: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">int</span>] = <span class="hljs-literal">None</span>
<span class="hljs-meta">... </span>    pad_to_multiple_of: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">int</span>] = <span class="hljs-literal">None</span>
<span class="hljs-meta">... </span>    pad_to_multiple_of_labels: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">int</span>] = <span class="hljs-literal">None</span>

<span class="hljs-meta">... </span>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__call__</span>(<span class="hljs-params">self, features: <span class="hljs-type">List</span>[<span class="hljs-type">Dict</span>[<span class="hljs-built_in">str</span>, <span class="hljs-type">Union</span>[<span class="hljs-type">List</span>[<span class="hljs-built_in">int</span>], torch.Tensor]]]</span>) -&gt; <span class="hljs-type">Dict</span>[<span class="hljs-built_in">str</span>, torch.Tensor]:
<span class="hljs-meta">... </span>        <span class="hljs-comment"># split inputs and labels since they have to be of different lengths and need</span>
<span class="hljs-meta">... </span>        <span class="hljs-comment"># different padding methods</span>
<span class="hljs-meta">... </span>        input_features = [{<span class="hljs-string">&quot;input_values&quot;</span>: feature[<span class="hljs-string">&quot;input_values&quot;</span>]} <span class="hljs-keyword">for</span> feature <span class="hljs-keyword">in</span> features]
<span class="hljs-meta">... </span>        label_features = [{<span class="hljs-string">&quot;input_ids&quot;</span>: feature[<span class="hljs-string">&quot;labels&quot;</span>]} <span class="hljs-keyword">for</span> feature <span class="hljs-keyword">in</span> features]

<span class="hljs-meta">... </span>        batch = self.processor.pad(
<span class="hljs-meta">... </span>            input_features,
<span class="hljs-meta">... </span>            padding=self.padding,
<span class="hljs-meta">... </span>            max_length=self.max_length,
<span class="hljs-meta">... </span>            pad_to_multiple_of=self.pad_to_multiple_of,
<span class="hljs-meta">... </span>            return_tensors=<span class="hljs-string">&quot;pt&quot;</span>,
<span class="hljs-meta">... </span>        )
<span class="hljs-meta">... </span>        <span class="hljs-keyword">with</span> self.processor.as_target_processor():
<span class="hljs-meta">... </span>            labels_batch = self.processor.pad(
<span class="hljs-meta">... </span>                label_features,
<span class="hljs-meta">... </span>                padding=self.padding,
<span class="hljs-meta">... </span>                max_length=self.max_length_labels,
<span class="hljs-meta">... </span>                pad_to_multiple_of=self.pad_to_multiple_of_labels,
<span class="hljs-meta">... </span>                return_tensors=<span class="hljs-string">&quot;pt&quot;</span>,
<span class="hljs-meta">... </span>            )

<span class="hljs-meta">... </span>        <span class="hljs-comment"># replace padding with -100 to ignore loss correctly</span>
<span class="hljs-meta">... </span>        labels = labels_batch[<span class="hljs-string">&quot;input_ids&quot;</span>].masked_fill(labels_batch.attention_mask.ne(<span class="hljs-number">1</span>), -<span class="hljs-number">100</span>)

<span class="hljs-meta">... </span>        batch[<span class="hljs-string">&quot;labels&quot;</span>] = labels

<span class="hljs-meta">... </span>        <span class="hljs-keyword">return</span> batch`}}),gs=new D({props:{code:"data_collator = DataCollatorCTCWithPadding(processor=processor, padding=True),",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>data_collator = DataCollatorCTCWithPadding(processor=processor, padding=<span class="hljs-literal">True</span>)'}}),js=new et({}),bs=new D({props:{code:`from transformers import AutoModelForCTC, TrainingArguments, Trainer

model = AutoModelForCTC.from_pretrained(
    "facebook/wav2vec-base",
    ctc_loss_reduction="mean",
    pad_token_id=processor.tokenizer.pad_token_id,
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForCTC, TrainingArguments, Trainer

<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCTC.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;facebook/wav2vec-base&quot;</span>,
<span class="hljs-meta">... </span>    ctc_loss_reduction=<span class="hljs-string">&quot;mean&quot;</span>,
<span class="hljs-meta">... </span>    pad_token_id=processor.tokenizer.pad_token_id,
<span class="hljs-meta">... </span>)`}}),K=new Ae({props:{$$slots:{default:[En]},$$scope:{ctx:N}}}),ws=new D({props:{code:`training_args = TrainingArguments(
    output_dir="./results",
    group_by_length=True,
    per_device_train_batch_size=16,
    evaluation_strategy="steps",
    num_train_epochs=3,
    fp16=True,
    gradient_checkpointing=True,
    learning_rate=1e-4,
    weight_decay=0.005,
    save_total_limit=2,
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=timit["train"],
    eval_dataset=timit["test"],
    tokenizer=processor.feature_extractor,
    data_collator=data_collator,
)

trainer.train(),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>training_args = TrainingArguments(
<span class="hljs-meta">... </span>    output_dir=<span class="hljs-string">&quot;./results&quot;</span>,
<span class="hljs-meta">... </span>    group_by_length=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>    per_device_train_batch_size=<span class="hljs-number">16</span>,
<span class="hljs-meta">... </span>    evaluation_strategy=<span class="hljs-string">&quot;steps&quot;</span>,
<span class="hljs-meta">... </span>    num_train_epochs=<span class="hljs-number">3</span>,
<span class="hljs-meta">... </span>    fp16=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>    gradient_checkpointing=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>    learning_rate=<span class="hljs-number">1e-4</span>,
<span class="hljs-meta">... </span>    weight_decay=<span class="hljs-number">0.005</span>,
<span class="hljs-meta">... </span>    save_total_limit=<span class="hljs-number">2</span>,
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>trainer = Trainer(
<span class="hljs-meta">... </span>    model=model,
<span class="hljs-meta">... </span>    args=training_args,
<span class="hljs-meta">... </span>    train_dataset=timit[<span class="hljs-string">&quot;train&quot;</span>],
<span class="hljs-meta">... </span>    eval_dataset=timit[<span class="hljs-string">&quot;test&quot;</span>],
<span class="hljs-meta">... </span>    tokenizer=processor.feature_extractor,
<span class="hljs-meta">... </span>    data_collator=data_collator,
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>trainer.train()`}}),Q=new Ae({props:{$$slots:{default:[Tn]},$$scope:{ctx:N}}}),{c(){f=o("meta"),x=c(),u=o("h1"),_=o("a"),k=o("span"),j(d.$$.fragment),g=c(),E=o("span"),z=e("Automatic speech recognition"),q=c(),j(I.$$.fragment),as=c(),ks=o("p"),nt=e("Automatic speech recognition (ASR) converts a speech signal to text. It is an example of a sequence-to-sequence task, going from a sequence of audio inputs to textual outputs. Voice assistants like Siri and Alexa utilize ASR models to assist users."),ua=c(),O=o("p"),lt=e("This guide will show you how to fine-tune "),ts=o("a"),ot=e("Wav2Vec2"),rt=e(" on the "),es=o("a"),pt=e("TIMIT"),it=e(" dataset to transcribe audio to text."),ma=c(),j(V.$$.fragment),da=c(),F=o("h2"),Y=o("a"),Fs=o("span"),j(ns.$$.fragment),ct=c(),Rs=o("span"),ht=e("Load TIMIT dataset"),_a=c(),Es=o("p"),ft=e("Load the TIMIT dataset from the \u{1F917} Datasets library:"),ga=c(),j(ls.$$.fragment),ja=c(),Ts=o("p"),ut=e("Then take a look at an example:"),ba=c(),j(os.$$.fragment),$a=c(),T=o("p"),mt=e("While the dataset contains a lot of helpful information, like "),Us=o("code"),dt=e("dialect_region"),_t=e(" and "),Vs=o("code"),gt=e("sentence_type"),jt=e(", you will focus on the "),Ys=o("code"),bt=e("audio"),$t=e(" and "),Hs=o("code"),vt=e("text"),yt=e(" fields in this guide. Remove the other columns:"),va=c(),j(rs.$$.fragment),ya=c(),qs=o("p"),wt=e("Take a look at the example again:"),wa=c(),j(ps.$$.fragment),xa=c(),S=o("p"),xt=e("The "),Bs=o("code"),kt=e("audio"),Et=e(" column contains a 1-dimensional "),Gs=o("code"),Tt=e("array"),qt=e(" of the speech signal that must be called to load and resample the audio file."),ka=c(),R=o("h2"),H=o("a"),Js=o("span"),j(is.$$.fragment),At=c(),Ks=o("span"),Ct=e("Preprocess"),Ea=c(),As=o("p"),Pt=e("Load the Wav2Vec2 processor to process the audio signal and transcribed text:"),Ta=c(),j(cs.$$.fragment),qa=c(),Cs=o("p"),Dt=e("The preprocessing function needs to:"),Aa=c(),L=o("ol"),hs=o("li"),It=e("Call the "),Qs=o("code"),Ot=e("audio"),St=e(" column to load and resample the audio file."),Lt=c(),fs=o("li"),Wt=e("Extract the "),Xs=o("code"),Mt=e("input_values"),Nt=e(" from the audio file."),zt=c(),Zs=o("li"),Ft=e("Typically, when you call the processor, you call the feature extractor. Since you also want to tokenize text, instruct the processor to call the tokenizer instead with a context manager."),Ca=c(),j(us.$$.fragment),Pa=c(),A=o("p"),Rt=e("Use \u{1F917} Datasets "),ms=o("a"),sa=o("code"),Ut=e("map"),Vt=e(" function to apply the preprocessing function over the entire dataset. You can speed up the map function by setting "),aa=o("code"),Yt=e("batched=True"),Ht=e(" to process multiple elements of the dataset at once and increasing the number of processes with "),ta=o("code"),Bt=e("num_proc"),Gt=e(". Remove the columns you don\u2019t need:"),Da=c(),j(ds.$$.fragment),Ia=c(),C=o("p"),Jt=e("\u{1F917} Transformers doesn\u2019t have a data collator for automatic speech recognition, so you will need to create one. You can adapt the "),Ps=o("a"),Kt=e("DataCollatorWithPadding"),Qt=e(" to create a batch of examples for automatic speech recognition. It will also dynamically pad your text and labels to the length of the longest element in its batch, so they are a uniform length. While it is possible to pad your text in the "),ea=o("code"),Xt=e("tokenizer"),Zt=e(" function by setting "),na=o("code"),se=e("padding=True"),ae=e(", dynamic padding is more efficient."),Oa=c(),P=o("p"),te=e("Unlike other data collators, "),la=o("code"),ee=e("DataCollatorWithPadding"),ne=e(" needs to apply a different padding method to "),oa=o("code"),le=e("input_values"),oe=e(" and "),ra=o("code"),re=e("labels"),pe=e(". You can apply a different padding method with a context manager:"),Sa=c(),j(_s.$$.fragment),La=c(),B=o("p"),ie=e("Create a batch of examples and dynamically pad them with "),pa=o("code"),ce=e("DataCollatorForCTCWithPadding"),he=e(":"),Wa=c(),j(gs.$$.fragment),Ma=c(),U=o("h1"),G=o("a"),ia=o("span"),j(js.$$.fragment),fe=c(),ca=o("span"),ue=e("Fine-tune with Trainer"),Na=c(),J=o("p"),me=e("Load Wav2Vec2 with "),Ds=o("a"),de=e("AutoModelForCTC"),_e=e(". Save GPU memory by calculating the mean of the loss reduction instead of the sum, and specify the padding token:"),za=c(),j(bs.$$.fragment),Fa=c(),j(K.$$.fragment),Ra=c(),Is=o("p"),ge=e("At this point, only three steps remain:"),Ua=c(),W=o("ol"),$s=o("li"),je=e("Define your training hyperparameters in "),Os=o("a"),be=e("TrainingArguments"),$e=e("."),ve=c(),vs=o("li"),ye=e("Pass the training arguments to "),Ss=o("a"),we=e("Trainer"),xe=e(" along with the model, datasets, tokenizer, and data collator."),ke=c(),ys=o("li"),Ee=e("Call "),Ls=o("a"),Te=e("train()"),qe=e(" to fine-tune your model."),Va=c(),j(ws.$$.fragment),Ya=c(),j(Q.$$.fragment),this.h()},l(s){const l=wn('[data-svelte="svelte-1phssyn"]',document.head);f=r(l,"META",{name:!0,content:!0}),l.forEach(t),x=h(s),u=r(s,"H1",{class:!0});var xs=p(u);_=r(xs,"A",{id:!0,class:!0,href:!0});var ha=p(_);k=r(ha,"SPAN",{});var fa=p(k);b(d.$$.fragment,fa),fa.forEach(t),ha.forEach(t),g=h(xs),E=r(xs,"SPAN",{});var Ce=p(E);z=n(Ce,"Automatic speech recognition"),Ce.forEach(t),xs.forEach(t),q=h(s),b(I.$$.fragment,s),as=h(s),ks=r(s,"P",{});var Pe=p(ks);nt=n(Pe,"Automatic speech recognition (ASR) converts a speech signal to text. It is an example of a sequence-to-sequence task, going from a sequence of audio inputs to textual outputs. Voice assistants like Siri and Alexa utilize ASR models to assist users."),Pe.forEach(t),ua=h(s),O=r(s,"P",{});var Ws=p(O);lt=n(Ws,"This guide will show you how to fine-tune "),ts=r(Ws,"A",{href:!0,rel:!0});var De=p(ts);ot=n(De,"Wav2Vec2"),De.forEach(t),rt=n(Ws," on the "),es=r(Ws,"A",{href:!0,rel:!0});var Ie=p(es);pt=n(Ie,"TIMIT"),Ie.forEach(t),it=n(Ws," dataset to transcribe audio to text."),Ws.forEach(t),ma=h(s),b(V.$$.fragment,s),da=h(s),F=r(s,"H2",{class:!0});var Ba=p(F);Y=r(Ba,"A",{id:!0,class:!0,href:!0});var Oe=p(Y);Fs=r(Oe,"SPAN",{});var Se=p(Fs);b(ns.$$.fragment,Se),Se.forEach(t),Oe.forEach(t),ct=h(Ba),Rs=r(Ba,"SPAN",{});var Le=p(Rs);ht=n(Le,"Load TIMIT dataset"),Le.forEach(t),Ba.forEach(t),_a=h(s),Es=r(s,"P",{});var We=p(Es);ft=n(We,"Load the TIMIT dataset from the \u{1F917} Datasets library:"),We.forEach(t),ga=h(s),b(ls.$$.fragment,s),ja=h(s),Ts=r(s,"P",{});var Me=p(Ts);ut=n(Me,"Then take a look at an example:"),Me.forEach(t),ba=h(s),b(os.$$.fragment,s),$a=h(s),T=r(s,"P",{});var M=p(T);mt=n(M,"While the dataset contains a lot of helpful information, like "),Us=r(M,"CODE",{});var Ne=p(Us);dt=n(Ne,"dialect_region"),Ne.forEach(t),_t=n(M," and "),Vs=r(M,"CODE",{});var ze=p(Vs);gt=n(ze,"sentence_type"),ze.forEach(t),jt=n(M,", you will focus on the "),Ys=r(M,"CODE",{});var Fe=p(Ys);bt=n(Fe,"audio"),Fe.forEach(t),$t=n(M," and "),Hs=r(M,"CODE",{});var Re=p(Hs);vt=n(Re,"text"),Re.forEach(t),yt=n(M," fields in this guide. Remove the other columns:"),M.forEach(t),va=h(s),b(rs.$$.fragment,s),ya=h(s),qs=r(s,"P",{});var Ue=p(qs);wt=n(Ue,"Take a look at the example again:"),Ue.forEach(t),wa=h(s),b(ps.$$.fragment,s),xa=h(s),S=r(s,"P",{});var Ms=p(S);xt=n(Ms,"The "),Bs=r(Ms,"CODE",{});var Ve=p(Bs);kt=n(Ve,"audio"),Ve.forEach(t),Et=n(Ms," column contains a 1-dimensional "),Gs=r(Ms,"CODE",{});var Ye=p(Gs);Tt=n(Ye,"array"),Ye.forEach(t),qt=n(Ms," of the speech signal that must be called to load and resample the audio file."),Ms.forEach(t),ka=h(s),R=r(s,"H2",{class:!0});var Ga=p(R);H=r(Ga,"A",{id:!0,class:!0,href:!0});var He=p(H);Js=r(He,"SPAN",{});var Be=p(Js);b(is.$$.fragment,Be),Be.forEach(t),He.forEach(t),At=h(Ga),Ks=r(Ga,"SPAN",{});var Ge=p(Ks);Ct=n(Ge,"Preprocess"),Ge.forEach(t),Ga.forEach(t),Ea=h(s),As=r(s,"P",{});var Je=p(As);Pt=n(Je,"Load the Wav2Vec2 processor to process the audio signal and transcribed text:"),Je.forEach(t),Ta=h(s),b(cs.$$.fragment,s),qa=h(s),Cs=r(s,"P",{});var Ke=p(Cs);Dt=n(Ke,"The preprocessing function needs to:"),Ke.forEach(t),Aa=h(s),L=r(s,"OL",{});var Ns=p(L);hs=r(Ns,"LI",{});var Ja=p(hs);It=n(Ja,"Call the "),Qs=r(Ja,"CODE",{});var Qe=p(Qs);Ot=n(Qe,"audio"),Qe.forEach(t),St=n(Ja," column to load and resample the audio file."),Ja.forEach(t),Lt=h(Ns),fs=r(Ns,"LI",{});var Ka=p(fs);Wt=n(Ka,"Extract the "),Xs=r(Ka,"CODE",{});var Xe=p(Xs);Mt=n(Xe,"input_values"),Xe.forEach(t),Nt=n(Ka," from the audio file."),Ka.forEach(t),zt=h(Ns),Zs=r(Ns,"LI",{});var Ze=p(Zs);Ft=n(Ze,"Typically, when you call the processor, you call the feature extractor. Since you also want to tokenize text, instruct the processor to call the tokenizer instead with a context manager."),Ze.forEach(t),Ns.forEach(t),Ca=h(s),b(us.$$.fragment,s),Pa=h(s),A=r(s,"P",{});var X=p(A);Rt=n(X,"Use \u{1F917} Datasets "),ms=r(X,"A",{href:!0,rel:!0});var sn=p(ms);sa=r(sn,"CODE",{});var an=p(sa);Ut=n(an,"map"),an.forEach(t),sn.forEach(t),Vt=n(X," function to apply the preprocessing function over the entire dataset. You can speed up the map function by setting "),aa=r(X,"CODE",{});var tn=p(aa);Yt=n(tn,"batched=True"),tn.forEach(t),Ht=n(X," to process multiple elements of the dataset at once and increasing the number of processes with "),ta=r(X,"CODE",{});var en=p(ta);Bt=n(en,"num_proc"),en.forEach(t),Gt=n(X,". Remove the columns you don\u2019t need:"),X.forEach(t),Da=h(s),b(ds.$$.fragment,s),Ia=h(s),C=r(s,"P",{});var Z=p(C);Jt=n(Z,"\u{1F917} Transformers doesn\u2019t have a data collator for automatic speech recognition, so you will need to create one. You can adapt the "),Ps=r(Z,"A",{href:!0});var nn=p(Ps);Kt=n(nn,"DataCollatorWithPadding"),nn.forEach(t),Qt=n(Z," to create a batch of examples for automatic speech recognition. It will also dynamically pad your text and labels to the length of the longest element in its batch, so they are a uniform length. While it is possible to pad your text in the "),ea=r(Z,"CODE",{});var ln=p(ea);Xt=n(ln,"tokenizer"),ln.forEach(t),Zt=n(Z," function by setting "),na=r(Z,"CODE",{});var on=p(na);se=n(on,"padding=True"),on.forEach(t),ae=n(Z,", dynamic padding is more efficient."),Z.forEach(t),Oa=h(s),P=r(s,"P",{});var ss=p(P);te=n(ss,"Unlike other data collators, "),la=r(ss,"CODE",{});var rn=p(la);ee=n(rn,"DataCollatorWithPadding"),rn.forEach(t),ne=n(ss," needs to apply a different padding method to "),oa=r(ss,"CODE",{});var pn=p(oa);le=n(pn,"input_values"),pn.forEach(t),oe=n(ss," and "),ra=r(ss,"CODE",{});var cn=p(ra);re=n(cn,"labels"),cn.forEach(t),pe=n(ss,". You can apply a different padding method with a context manager:"),ss.forEach(t),Sa=h(s),b(_s.$$.fragment,s),La=h(s),B=r(s,"P",{});var Qa=p(B);ie=n(Qa,"Create a batch of examples and dynamically pad them with "),pa=r(Qa,"CODE",{});var hn=p(pa);ce=n(hn,"DataCollatorForCTCWithPadding"),hn.forEach(t),he=n(Qa,":"),Qa.forEach(t),Wa=h(s),b(gs.$$.fragment,s),Ma=h(s),U=r(s,"H1",{class:!0});var Xa=p(U);G=r(Xa,"A",{id:!0,class:!0,href:!0});var fn=p(G);ia=r(fn,"SPAN",{});var un=p(ia);b(js.$$.fragment,un),un.forEach(t),fn.forEach(t),fe=h(Xa),ca=r(Xa,"SPAN",{});var mn=p(ca);ue=n(mn,"Fine-tune with Trainer"),mn.forEach(t),Xa.forEach(t),Na=h(s),J=r(s,"P",{});var Za=p(J);me=n(Za,"Load Wav2Vec2 with "),Ds=r(Za,"A",{href:!0});var dn=p(Ds);de=n(dn,"AutoModelForCTC"),dn.forEach(t),_e=n(Za,". Save GPU memory by calculating the mean of the loss reduction instead of the sum, and specify the padding token:"),Za.forEach(t),za=h(s),b(bs.$$.fragment,s),Fa=h(s),b(K.$$.fragment,s),Ra=h(s),Is=r(s,"P",{});var _n=p(Is);ge=n(_n,"At this point, only three steps remain:"),_n.forEach(t),Ua=h(s),W=r(s,"OL",{});var zs=p(W);$s=r(zs,"LI",{});var st=p($s);je=n(st,"Define your training hyperparameters in "),Os=r(st,"A",{href:!0});var gn=p(Os);be=n(gn,"TrainingArguments"),gn.forEach(t),$e=n(st,"."),st.forEach(t),ve=h(zs),vs=r(zs,"LI",{});var at=p(vs);ye=n(at,"Pass the training arguments to "),Ss=r(at,"A",{href:!0});var jn=p(Ss);we=n(jn,"Trainer"),jn.forEach(t),xe=n(at," along with the model, datasets, tokenizer, and data collator."),at.forEach(t),ke=h(zs),ys=r(zs,"LI",{});var tt=p(ys);Ee=n(tt,"Call "),Ls=r(tt,"A",{href:!0});var bn=p(Ls);Te=n(bn,"train()"),bn.forEach(t),qe=n(tt," to fine-tune your model."),tt.forEach(t),zs.forEach(t),Va=h(s),b(ws.$$.fragment,s),Ya=h(s),b(Q.$$.fragment,s),this.h()},h(){m(f,"name","hf:doc:metadata"),m(f,"content",JSON.stringify(An)),m(_,"id","automatic-speech-recognition"),m(_,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(_,"href","#automatic-speech-recognition"),m(u,"class","relative group"),m(ts,"href","https://huggingface.co/facebook/wav2vec2-base"),m(ts,"rel","nofollow"),m(es,"href","https://huggingface.co/datasets/timit_asr"),m(es,"rel","nofollow"),m(Y,"id","load-timit-dataset"),m(Y,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(Y,"href","#load-timit-dataset"),m(F,"class","relative group"),m(H,"id","preprocess"),m(H,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(H,"href","#preprocess"),m(R,"class","relative group"),m(ms,"href","https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Dataset.map"),m(ms,"rel","nofollow"),m(Ps,"href","/docs/transformers/pr_15808/en/main_classes/data_collator#transformers.DataCollatorWithPadding"),m(G,"id","finetune-with-trainer"),m(G,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(G,"href","#finetune-with-trainer"),m(U,"class","relative group"),m(Ds,"href","/docs/transformers/pr_15808/en/model_doc/auto#transformers.AutoModelForCTC"),m(Os,"href","/docs/transformers/pr_15808/en/main_classes/trainer#transformers.TrainingArguments"),m(Ss,"href","/docs/transformers/pr_15808/en/main_classes/trainer#transformers.Trainer"),m(Ls,"href","/docs/transformers/pr_15808/en/main_classes/trainer#transformers.Trainer.train")},m(s,l){a(document.head,f),i(s,x,l),i(s,u,l),a(u,_),a(_,k),$(d,k,null),a(u,g),a(u,E),a(E,z),i(s,q,l),$(I,s,l),i(s,as,l),i(s,ks,l),a(ks,nt),i(s,ua,l),i(s,O,l),a(O,lt),a(O,ts),a(ts,ot),a(O,rt),a(O,es),a(es,pt),a(O,it),i(s,ma,l),$(V,s,l),i(s,da,l),i(s,F,l),a(F,Y),a(Y,Fs),$(ns,Fs,null),a(F,ct),a(F,Rs),a(Rs,ht),i(s,_a,l),i(s,Es,l),a(Es,ft),i(s,ga,l),$(ls,s,l),i(s,ja,l),i(s,Ts,l),a(Ts,ut),i(s,ba,l),$(os,s,l),i(s,$a,l),i(s,T,l),a(T,mt),a(T,Us),a(Us,dt),a(T,_t),a(T,Vs),a(Vs,gt),a(T,jt),a(T,Ys),a(Ys,bt),a(T,$t),a(T,Hs),a(Hs,vt),a(T,yt),i(s,va,l),$(rs,s,l),i(s,ya,l),i(s,qs,l),a(qs,wt),i(s,wa,l),$(ps,s,l),i(s,xa,l),i(s,S,l),a(S,xt),a(S,Bs),a(Bs,kt),a(S,Et),a(S,Gs),a(Gs,Tt),a(S,qt),i(s,ka,l),i(s,R,l),a(R,H),a(H,Js),$(is,Js,null),a(R,At),a(R,Ks),a(Ks,Ct),i(s,Ea,l),i(s,As,l),a(As,Pt),i(s,Ta,l),$(cs,s,l),i(s,qa,l),i(s,Cs,l),a(Cs,Dt),i(s,Aa,l),i(s,L,l),a(L,hs),a(hs,It),a(hs,Qs),a(Qs,Ot),a(hs,St),a(L,Lt),a(L,fs),a(fs,Wt),a(fs,Xs),a(Xs,Mt),a(fs,Nt),a(L,zt),a(L,Zs),a(Zs,Ft),i(s,Ca,l),$(us,s,l),i(s,Pa,l),i(s,A,l),a(A,Rt),a(A,ms),a(ms,sa),a(sa,Ut),a(A,Vt),a(A,aa),a(aa,Yt),a(A,Ht),a(A,ta),a(ta,Bt),a(A,Gt),i(s,Da,l),$(ds,s,l),i(s,Ia,l),i(s,C,l),a(C,Jt),a(C,Ps),a(Ps,Kt),a(C,Qt),a(C,ea),a(ea,Xt),a(C,Zt),a(C,na),a(na,se),a(C,ae),i(s,Oa,l),i(s,P,l),a(P,te),a(P,la),a(la,ee),a(P,ne),a(P,oa),a(oa,le),a(P,oe),a(P,ra),a(ra,re),a(P,pe),i(s,Sa,l),$(_s,s,l),i(s,La,l),i(s,B,l),a(B,ie),a(B,pa),a(pa,ce),a(B,he),i(s,Wa,l),$(gs,s,l),i(s,Ma,l),i(s,U,l),a(U,G),a(G,ia),$(js,ia,null),a(U,fe),a(U,ca),a(ca,ue),i(s,Na,l),i(s,J,l),a(J,me),a(J,Ds),a(Ds,de),a(J,_e),i(s,za,l),$(bs,s,l),i(s,Fa,l),$(K,s,l),i(s,Ra,l),i(s,Is,l),a(Is,ge),i(s,Ua,l),i(s,W,l),a(W,$s),a($s,je),a($s,Os),a(Os,be),a($s,$e),a(W,ve),a(W,vs),a(vs,ye),a(vs,Ss),a(Ss,we),a(vs,xe),a(W,ke),a(W,ys),a(ys,Ee),a(ys,Ls),a(Ls,Te),a(ys,qe),i(s,Va,l),$(ws,s,l),i(s,Ya,l),$(Q,s,l),Ha=!0},p(s,[l]){const xs={};l&2&&(xs.$$scope={dirty:l,ctx:s}),V.$set(xs);const ha={};l&2&&(ha.$$scope={dirty:l,ctx:s}),K.$set(ha);const fa={};l&2&&(fa.$$scope={dirty:l,ctx:s}),Q.$set(fa)},i(s){Ha||(v(d.$$.fragment,s),v(I.$$.fragment,s),v(V.$$.fragment,s),v(ns.$$.fragment,s),v(ls.$$.fragment,s),v(os.$$.fragment,s),v(rs.$$.fragment,s),v(ps.$$.fragment,s),v(is.$$.fragment,s),v(cs.$$.fragment,s),v(us.$$.fragment,s),v(ds.$$.fragment,s),v(_s.$$.fragment,s),v(gs.$$.fragment,s),v(js.$$.fragment,s),v(bs.$$.fragment,s),v(K.$$.fragment,s),v(ws.$$.fragment,s),v(Q.$$.fragment,s),Ha=!0)},o(s){y(d.$$.fragment,s),y(I.$$.fragment,s),y(V.$$.fragment,s),y(ns.$$.fragment,s),y(ls.$$.fragment,s),y(os.$$.fragment,s),y(rs.$$.fragment,s),y(ps.$$.fragment,s),y(is.$$.fragment,s),y(cs.$$.fragment,s),y(us.$$.fragment,s),y(ds.$$.fragment,s),y(_s.$$.fragment,s),y(gs.$$.fragment,s),y(js.$$.fragment,s),y(bs.$$.fragment,s),y(K.$$.fragment,s),y(ws.$$.fragment,s),y(Q.$$.fragment,s),Ha=!1},d(s){t(f),s&&t(x),s&&t(u),w(d),s&&t(q),w(I,s),s&&t(as),s&&t(ks),s&&t(ua),s&&t(O),s&&t(ma),w(V,s),s&&t(da),s&&t(F),w(ns),s&&t(_a),s&&t(Es),s&&t(ga),w(ls,s),s&&t(ja),s&&t(Ts),s&&t(ba),w(os,s),s&&t($a),s&&t(T),s&&t(va),w(rs,s),s&&t(ya),s&&t(qs),s&&t(wa),w(ps,s),s&&t(xa),s&&t(S),s&&t(ka),s&&t(R),w(is),s&&t(Ea),s&&t(As),s&&t(Ta),w(cs,s),s&&t(qa),s&&t(Cs),s&&t(Aa),s&&t(L),s&&t(Ca),w(us,s),s&&t(Pa),s&&t(A),s&&t(Da),w(ds,s),s&&t(Ia),s&&t(C),s&&t(Oa),s&&t(P),s&&t(Sa),w(_s,s),s&&t(La),s&&t(B),s&&t(Wa),w(gs,s),s&&t(Ma),s&&t(U),w(js),s&&t(Na),s&&t(J),s&&t(za),w(bs,s),s&&t(Fa),w(K,s),s&&t(Ra),s&&t(Is),s&&t(Ua),s&&t(W),s&&t(Va),w(ws,s),s&&t(Ya),w(Q,s)}}}const An={local:"finetune-with-trainer",title:"Fine-tune with Trainer"};function Cn(N,f,x){let{fw:u}=f;return N.$$set=_=>{"fw"in _&&x(0,u=_.fw)},[u]}class Wn extends $n{constructor(f){super();vn(this,f,Cn,qn,yn,{fw:0})}}export{Wn as default,An as metadata};
