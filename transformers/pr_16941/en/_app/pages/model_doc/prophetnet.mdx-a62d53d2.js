import{S as nh,i as rh,s as sh,e as s,k as c,w as m,t as n,M as ah,c as a,d as o,m as l,a as d,x as f,h as r,b as i,F as e,g as p,y as _,q as g,o as v,B as k,v as dh}from"../../chunks/vendor-6b77c823.js";import{T as Yn}from"../../chunks/Tip-39098574.js";import{D as z}from"../../chunks/Docstring-1088f2fb.js";import{C as Ho}from"../../chunks/CodeBlock-3a8b25a8.js";import{I as re}from"../../chunks/IconCopyLink-7a11ce68.js";function ih(B){let u,N,T,w,P;return{c(){u=s("p"),N=n("Although the recipe for forward pass needs to be defined within this function, one should call the "),T=s("code"),w=n("Module"),P=n(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(y){u=a(y,"P",{});var b=d(u);N=r(b,"Although the recipe for forward pass needs to be defined within this function, one should call the "),T=a(b,"CODE",{});var q=d(T);w=r(q,"Module"),q.forEach(o),P=r(b,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),b.forEach(o)},m(y,b){p(y,u,b),e(u,N),e(u,T),e(T,w),e(u,P)},d(y){y&&o(u)}}}function ch(B){let u,N,T,w,P;return{c(){u=s("p"),N=n("Although the recipe for forward pass needs to be defined within this function, one should call the "),T=s("code"),w=n("Module"),P=n(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(y){u=a(y,"P",{});var b=d(u);N=r(b,"Although the recipe for forward pass needs to be defined within this function, one should call the "),T=a(b,"CODE",{});var q=d(T);w=r(q,"Module"),q.forEach(o),P=r(b,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),b.forEach(o)},m(y,b){p(y,u,b),e(u,N),e(u,T),e(T,w),e(u,P)},d(y){y&&o(u)}}}function lh(B){let u,N,T,w,P;return{c(){u=s("p"),N=n("Although the recipe for forward pass needs to be defined within this function, one should call the "),T=s("code"),w=n("Module"),P=n(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(y){u=a(y,"P",{});var b=d(u);N=r(b,"Although the recipe for forward pass needs to be defined within this function, one should call the "),T=a(b,"CODE",{});var q=d(T);w=r(q,"Module"),q.forEach(o),P=r(b,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),b.forEach(o)},m(y,b){p(y,u,b),e(u,N),e(u,T),e(T,w),e(u,P)},d(y){y&&o(u)}}}function hh(B){let u,N,T,w,P;return{c(){u=s("p"),N=n("Although the recipe for forward pass needs to be defined within this function, one should call the "),T=s("code"),w=n("Module"),P=n(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(y){u=a(y,"P",{});var b=d(u);N=r(b,"Although the recipe for forward pass needs to be defined within this function, one should call the "),T=a(b,"CODE",{});var q=d(T);w=r(q,"Module"),q.forEach(o),P=r(b,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),b.forEach(o)},m(y,b){p(y,u,b),e(u,N),e(u,T),e(T,w),e(u,P)},d(y){y&&o(u)}}}function ph(B){let u,N,T,w,P;return{c(){u=s("p"),N=n("Although the recipe for forward pass needs to be defined within this function, one should call the "),T=s("code"),w=n("Module"),P=n(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(y){u=a(y,"P",{});var b=d(u);N=r(b,"Although the recipe for forward pass needs to be defined within this function, one should call the "),T=a(b,"CODE",{});var q=d(T);w=r(q,"Module"),q.forEach(o),P=r(b,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),b.forEach(o)},m(y,b){p(y,u,b),e(u,N),e(u,T),e(T,w),e(u,P)},d(y){y&&o(u)}}}function uh(B){let u,N,T,w,P,y,b,q,Zr,Kn,se,Vo,es,ts,Xe,os,ns,Qn,ae,Ee,Uo,Ze,rs,Ro,ss,Xn,Ce,as,et,ds,is,Zn,io,cs,er,co,ls,tr,lo,Jo,hs,or,Oe,ps,tt,us,ms,nr,de,Se,Yo,ot,fs,Ko,_s,rr,U,nt,gs,ie,vs,ho,ks,Ts,rt,bs,ys,ws,ce,Ps,po,Ns,qs,uo,zs,$s,sr,le,De,Qo,st,Fs,Xo,Ms,ar,$,at,xs,Zo,Es,Cs,dt,Os,mo,Ss,Ds,Ls,R,it,js,en,As,Is,ct,fo,Ws,tn,Bs,Gs,_o,Hs,on,Vs,Us,Le,lt,Rs,nn,Js,Ys,G,ht,Ks,rn,Qs,Xs,pt,Zs,he,ea,sn,ta,oa,an,na,ra,sa,je,ut,aa,mt,da,dn,ia,ca,dr,pe,Ae,cn,ft,la,ln,ha,ir,ue,_t,pa,hn,ua,cr,me,gt,ma,pn,fa,lr,fe,vt,_a,un,ga,hr,_e,kt,va,mn,ka,pr,ge,Ie,fn,Tt,Ta,_n,ba,ur,x,bt,ya,yt,wa,go,Pa,Na,qa,ve,za,wt,$a,Fa,gn,Ma,xa,Ea,Pt,Ca,Nt,Oa,Sa,Da,O,qt,La,ke,ja,vo,Aa,Ia,vn,Wa,Ba,Ga,We,Ha,kn,Va,Ua,zt,mr,Te,Be,Tn,$t,Ra,bn,Ja,fr,F,Ft,Ya,Mt,Ka,ko,Qa,Xa,Za,be,ed,xt,td,od,yn,nd,rd,sd,Et,ad,Ct,dd,id,cd,I,ld,wn,hd,pd,Pn,ud,md,Nn,fd,_d,To,gd,vd,kd,S,Ot,Td,ye,bd,bo,yd,wd,qn,Pd,Nd,qd,Ge,zd,zn,$d,Fd,St,_r,we,He,$n,Dt,Md,Fn,xd,gr,M,Lt,Ed,jt,Cd,yo,Od,Sd,Dd,Pe,Ld,At,jd,Ad,Mn,Id,Wd,Bd,It,Gd,Wt,Hd,Vd,Ud,W,Rd,xn,Jd,Yd,En,Kd,Qd,Cn,Xd,Zd,wo,ei,ti,oi,D,Bt,ni,Ne,ri,Po,si,ai,On,di,ii,ci,Ve,li,Sn,hi,pi,Gt,vr,qe,Ue,Dn,Ht,ui,Ln,mi,kr,E,Vt,fi,Ut,_i,No,gi,vi,ki,ze,Ti,Rt,bi,yi,jn,wi,Pi,Ni,Jt,qi,Yt,zi,$i,Fi,L,Kt,Mi,$e,xi,qo,Ei,Ci,An,Oi,Si,Di,Re,Li,In,ji,Ai,Qt,Tr,Fe,Je,Wn,Xt,Ii,Bn,Wi,br,C,Zt,Bi,eo,Gi,zo,Hi,Vi,Ui,Me,Ri,to,Ji,Yi,Gn,Ki,Qi,Xi,oo,Zi,no,ec,tc,oc,j,ro,nc,xe,rc,$o,sc,ac,Hn,dc,ic,cc,Ye,lc,Vn,hc,pc,so,yr;return y=new re({}),Ze=new re({}),ot=new re({}),nt=new z({props:{name:"class transformers.ProphetNetConfig",anchor:"transformers.ProphetNetConfig",parameters:[{name:"activation_dropout",val:" = 0.1"},{name:"activation_function",val:" = 'gelu'"},{name:"vocab_size",val:" = 30522"},{name:"hidden_size",val:" = 1024"},{name:"encoder_ffn_dim",val:" = 4096"},{name:"num_encoder_layers",val:" = 12"},{name:"num_encoder_attention_heads",val:" = 16"},{name:"decoder_ffn_dim",val:" = 4096"},{name:"num_decoder_layers",val:" = 12"},{name:"num_decoder_attention_heads",val:" = 16"},{name:"attention_dropout",val:" = 0.1"},{name:"dropout",val:" = 0.1"},{name:"max_position_embeddings",val:" = 512"},{name:"init_std",val:" = 0.02"},{name:"is_encoder_decoder",val:" = True"},{name:"add_cross_attention",val:" = True"},{name:"decoder_start_token_id",val:" = 0"},{name:"ngram",val:" = 2"},{name:"num_buckets",val:" = 32"},{name:"relative_max_distance",val:" = 128"},{name:"disable_ngram_loss",val:" = False"},{name:"eps",val:" = 0.0"},{name:"use_cache",val:" = True"},{name:"pad_token_id",val:" = 0"},{name:"bos_token_id",val:" = 1"},{name:"eos_token_id",val:" = 2"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.ProphetNetConfig.activation_dropout",description:`<strong>activation_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout ratio for activations inside the fully connected layer.`,name:"activation_dropout"},{anchor:"transformers.ProphetNetConfig.activation_function",description:`<strong>activation_function</strong> (<code>str</code> or <code>function</code>, <em>optional</em>, defaults to <code>&quot;gelu&quot;</code>) &#x2014;
The non-linear activation function (function or string) in the encoder and pooler. If string, <code>&quot;gelu&quot;</code>,
<code>&quot;relu&quot;</code>, <code>&quot;silu&quot;</code> and <code>&quot;gelu_new&quot;</code> are supported.`,name:"activation_function"},{anchor:"transformers.ProphetNetConfig.vocab_size",description:`<strong>vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 30522) &#x2014;
Vocabulary size of the ProphetNET model. Defines the number of different tokens that can be represented by
the <code>inputs_ids</code> passed when calling <a href="/docs/transformers/pr_16941/en/model_doc/prophetnet#transformers.ProphetNetModel">ProphetNetModel</a>.`,name:"vocab_size"},{anchor:"transformers.ProphetNetConfig.hidden_size",description:`<strong>hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 1024) &#x2014;
Dimensionality of the layers and the pooler layer.`,name:"hidden_size"},{anchor:"transformers.ProphetNetConfig.encoder_ffn_dim",description:`<strong>encoder_ffn_dim</strong> (<code>int</code>, <em>optional</em>, defaults to 4096) &#x2014;
Dimensionality of the &#x201C;intermediate&#x201D; (often named feed-forward) layer in decoder.`,name:"encoder_ffn_dim"},{anchor:"transformers.ProphetNetConfig.num_encoder_layers",description:`<strong>num_encoder_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of encoder layers.`,name:"num_encoder_layers"},{anchor:"transformers.ProphetNetConfig.num_encoder_attention_heads",description:`<strong>num_encoder_attention_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 16) &#x2014;
Number of attention heads for each attention layer in the Transformer encoder.`,name:"num_encoder_attention_heads"},{anchor:"transformers.ProphetNetConfig.decoder_ffn_dim",description:`<strong>decoder_ffn_dim</strong> (<code>int</code>, <em>optional</em>, defaults to 4096) &#x2014;
Dimensionality of the <code>intermediate</code> (often named feed-forward) layer in decoder.`,name:"decoder_ffn_dim"},{anchor:"transformers.ProphetNetConfig.num_decoder_layers",description:`<strong>num_decoder_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of decoder layers.`,name:"num_decoder_layers"},{anchor:"transformers.ProphetNetConfig.num_decoder_attention_heads",description:`<strong>num_decoder_attention_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 16) &#x2014;
Number of attention heads for each attention layer in the Transformer decoder.`,name:"num_decoder_attention_heads"},{anchor:"transformers.ProphetNetConfig.attention_dropout",description:`<strong>attention_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout ratio for the attention probabilities.`,name:"attention_dropout"},{anchor:"transformers.ProphetNetConfig.dropout",description:`<strong>dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout probability for all fully connected layers in the embeddings, encoder, and pooler.`,name:"dropout"},{anchor:"transformers.ProphetNetConfig.max_position_embeddings",description:`<strong>max_position_embeddings</strong> (<code>int</code>, <em>optional</em>, defaults to 512) &#x2014;
The maximum sequence length that this model might ever be used with. Typically set this to something large
just in case (e.g., 512 or 1024 or 2048).`,name:"max_position_embeddings"},{anchor:"transformers.ProphetNetConfig.init_std",description:`<strong>init_std</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"init_std"},{anchor:"transformers.ProphetNetConfig.add_cross_attention",description:`<strong>add_cross_attention</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether cross-attention layers should be added to the model.`,name:"add_cross_attention"},{anchor:"transformers.ProphetNetConfig.is_encoder_decoder",description:`<strong>is_encoder_decoder</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether this is an encoder/decoder model.`,name:"is_encoder_decoder"},{anchor:"transformers.ProphetNetConfig.pad_token_id",description:`<strong>pad_token_id</strong> (<code>int</code>, <em>optional</em>, defaults to 1) &#x2014;
Padding token id.`,name:"pad_token_id"},{anchor:"transformers.ProphetNetConfig.bos_token_id",description:`<strong>bos_token_id</strong> (<code>int</code>, <em>optional</em>, defaults to 0) &#x2014;
Beginning of stream token id.`,name:"bos_token_id"},{anchor:"transformers.ProphetNetConfig.eos_token_id",description:`<strong>eos_token_id</strong> (<code>int</code>, <em>optional</em>, defaults to 2) &#x2014;
End of stream token id.`,name:"eos_token_id"},{anchor:"transformers.ProphetNetConfig.ngram",description:`<strong>ngram</strong> (<code>int</code>, <em>optional</em>, defaults to 2) &#x2014;
Number of future tokens to predict. Set to 1 to be same as traditional Language model to predict next first
token.`,name:"ngram"},{anchor:"transformers.ProphetNetConfig.num_buckets",description:`<strong>num_buckets</strong> (<code>int</code>, <em>optional</em>, defaults to 32) &#x2014;
The number of buckets to use for each attention layer. This is for relative position calculation. See the
[T5 paper](see <a href="https://arxiv.org/abs/1910.10683" rel="nofollow">https://arxiv.org/abs/1910.10683</a>) for more details.`,name:"num_buckets"},{anchor:"transformers.ProphetNetConfig.relative_max_distance",description:`<strong>relative_max_distance</strong> (<code>int</code>, <em>optional</em>, defaults to 128) &#x2014;
Relative distances greater than this number will be put into the last same bucket. This is for relative
position calculation. See the [T5 paper](see <a href="https://arxiv.org/abs/1910.10683" rel="nofollow">https://arxiv.org/abs/1910.10683</a>) for more details.`,name:"relative_max_distance"},{anchor:"transformers.ProphetNetConfig.disable_ngram_loss",description:`<strong>disable_ngram_loss</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether be trained predicting only the next first token.`,name:"disable_ngram_loss"},{anchor:"transformers.ProphetNetConfig.eps",description:`<strong>eps</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
Controls the <code>epsilon</code> parameter value for label smoothing in the loss calculation. If set to 0, no label
smoothing is performed.`,name:"eps"},{anchor:"transformers.ProphetNetConfig.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not the model should return the last key/values attentions (not used by all models).`,name:"use_cache"}],source:"https://github.com/huggingface/transformers/blob/pr_16941/src/transformers/models/prophetnet/configuration_prophetnet.py#L29"}}),st=new re({}),at=new z({props:{name:"class transformers.ProphetNetTokenizer",anchor:"transformers.ProphetNetTokenizer",parameters:[{name:"vocab_file",val:""},{name:"do_lower_case",val:" = True"},{name:"do_basic_tokenize",val:" = True"},{name:"never_split",val:" = None"},{name:"unk_token",val:" = '[UNK]'"},{name:"sep_token",val:" = '[SEP]'"},{name:"x_sep_token",val:" = '[X_SEP]'"},{name:"pad_token",val:" = '[PAD]'"},{name:"mask_token",val:" = '[MASK]'"},{name:"tokenize_chinese_chars",val:" = True"},{name:"strip_accents",val:" = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.ProphetNetTokenizer.vocab_file",description:`<strong>vocab_file</strong> (<code>str</code>) &#x2014;
File containing the vocabulary.`,name:"vocab_file"},{anchor:"transformers.ProphetNetTokenizer.do_lower_case",description:`<strong>do_lower_case</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to lowercase the input when tokenizing.`,name:"do_lower_case"},{anchor:"transformers.ProphetNetTokenizer.do_basic_tokenize",description:`<strong>do_basic_tokenize</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to do basic tokenization before WordPiece.`,name:"do_basic_tokenize"},{anchor:"transformers.ProphetNetTokenizer.never_split",description:`<strong>never_split</strong> (<code>Iterable</code>, <em>optional</em>) &#x2014;
Collection of tokens which will never be split during tokenization. Only has an effect when
<code>do_basic_tokenize=True</code>`,name:"never_split"},{anchor:"transformers.ProphetNetTokenizer.unk_token",description:`<strong>unk_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;[UNK]&quot;</code>) &#x2014;
The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this
token instead.`,name:"unk_token"},{anchor:"transformers.ProphetNetTokenizer.sep_token",description:`<strong>sep_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;[SEP]&quot;</code>) &#x2014;
The separator token, which is used when building a sequence from multiple sequences, e.g. two sequences for
sequence classification or for a text and a question for question answering. It is also used as the last
token of a sequence built with special tokens.`,name:"sep_token"},{anchor:"transformers.ProphetNetTokenizer.x_sep_token",description:`<strong>x_sep_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;[X_SEP]&quot;</code>) &#x2014;
Special second separator token, which can be generated by <a href="/docs/transformers/pr_16941/en/model_doc/prophetnet#transformers.ProphetNetForConditionalGeneration">ProphetNetForConditionalGeneration</a>. It is
used to separate bullet-point like sentences in summarization, <em>e.g.</em>.`,name:"x_sep_token"},{anchor:"transformers.ProphetNetTokenizer.pad_token",description:`<strong>pad_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;[PAD]&quot;</code>) &#x2014;
The token used for padding, for example when batching sequences of different lengths.`,name:"pad_token"},{anchor:"transformers.ProphetNetTokenizer.cls_token",description:`<strong>cls_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;[CLS]&quot;</code>) &#x2014;
The classifier token which is used when doing sequence classification (classification of the whole sequence
instead of per-token classification). It is the first token of the sequence when built with special tokens.`,name:"cls_token"},{anchor:"transformers.ProphetNetTokenizer.mask_token",description:`<strong>mask_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;[MASK]&quot;</code>) &#x2014;
The token used for masking values. This is the token used when training this model with masked language
modeling. This is the token which the model will try to predict.`,name:"mask_token"},{anchor:"transformers.ProphetNetTokenizer.tokenize_chinese_chars",description:`<strong>tokenize_chinese_chars</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to tokenize Chinese characters.</p>
<p>This should likely be deactivated for Japanese (see this
<a href="https://github.com/huggingface/transformers/issues/328" rel="nofollow">issue</a>).
strip_accents &#x2014; (<code>bool</code>, <em>optional</em>):
Whether or not to strip all accents. If this option is not specified, then it will be determined by the
value for <code>lowercase</code> (as in the original BERT).`,name:"tokenize_chinese_chars"}],source:"https://github.com/huggingface/transformers/blob/pr_16941/src/transformers/models/prophetnet/tokenization_prophetnet.py#L55"}}),it=new z({props:{name:"build_inputs_with_special_tokens",anchor:"transformers.ProphetNetTokenizer.build_inputs_with_special_tokens",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"}],parametersDescription:[{anchor:"transformers.ProphetNetTokenizer.build_inputs_with_special_tokens.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs to which the special tokens will be added.`,name:"token_ids_0"},{anchor:"transformers.ProphetNetTokenizer.build_inputs_with_special_tokens.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],source:"https://github.com/huggingface/transformers/blob/pr_16941/src/transformers/models/prophetnet/tokenization_prophetnet.py#L266",returnDescription:`
<p>List of <a href="../glossary#input-ids">input IDs</a> with the appropriate special tokens.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),lt=new z({props:{name:"convert_tokens_to_string",anchor:"transformers.ProphetNetTokenizer.convert_tokens_to_string",parameters:[{name:"tokens",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_16941/src/transformers/models/prophetnet/tokenization_prophetnet.py#L186"}}),ht=new z({props:{name:"create_token_type_ids_from_sequences",anchor:"transformers.ProphetNetTokenizer.create_token_type_ids_from_sequences",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"}],parametersDescription:[{anchor:"transformers.ProphetNetTokenizer.create_token_type_ids_from_sequences.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs.`,name:"token_ids_0"},{anchor:"transformers.ProphetNetTokenizer.create_token_type_ids_from_sequences.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],source:"https://github.com/huggingface/transformers/blob/pr_16941/src/transformers/models/prophetnet/tokenization_prophetnet.py#L218",returnDescription:`
<p>List of <a href="../glossary#token-type-ids">token type IDs</a> according to the given sequence(s).</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),pt=new Ho({props:{code:`0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1
| first sequence    | second sequence |`,highlighted:`0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 1 </span>1<span class="hljs-number"> 1 </span>1<span class="hljs-number"> 1 </span>1<span class="hljs-number"> 1 </span>1 1
| first sequence    | second sequence |`}}),ut=new z({props:{name:"get_special_tokens_mask",anchor:"transformers.ProphetNetTokenizer.get_special_tokens_mask",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"},{name:"already_has_special_tokens",val:": bool = False"}],parametersDescription:[{anchor:"transformers.ProphetNetTokenizer.get_special_tokens_mask.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs.`,name:"token_ids_0"},{anchor:"transformers.ProphetNetTokenizer.get_special_tokens_mask.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"},{anchor:"transformers.ProphetNetTokenizer.get_special_tokens_mask.already_has_special_tokens",description:`<strong>already_has_special_tokens</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not the token list is already formatted with special tokens for the model.`,name:"already_has_special_tokens"}],source:"https://github.com/huggingface/transformers/blob/pr_16941/src/transformers/models/prophetnet/tokenization_prophetnet.py#L191",returnDescription:`
<p>A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),ft=new re({}),_t=new z({props:{name:"class transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqLMOutput",anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqLMOutput",parameters:[{name:"loss",val:": typing.Optional[torch.FloatTensor] = None"},{name:"logits",val:": FloatTensor = None"},{name:"logits_ngram",val:": typing.Optional[torch.FloatTensor] = None"},{name:"past_key_values",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"decoder_hidden_states",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"decoder_ngram_hidden_states",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"decoder_attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"decoder_ngram_attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"cross_attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"encoder_last_hidden_state",val:": typing.Optional[torch.FloatTensor] = None"},{name:"encoder_hidden_states",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"encoder_attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"}],parametersDescription:[{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqLMOutput.loss",description:`<strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) &#x2014;
Language modeling loss.`,name:"loss"},{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqLMOutput.logits",description:`<strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, decoder_sequence_length, config.vocab_size)</code>) &#x2014;
Prediction scores of the main stream language modeling head (scores for each vocabulary token before
SoftMax).`,name:"logits"},{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqLMOutput.logits_ngram",description:`<strong>logits_ngram</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, ngram * decoder_sequence_length, config.vocab_size)</code>) &#x2014;
Prediction scores of the predict stream language modeling head (scores for each vocabulary token before
SoftMax).`,name:"logits_ngram"},{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqLMOutput.past_key_values",description:`<strong>past_key_values</strong> (<code>List[torch.FloatTensor]</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) &#x2014;
List of <code>torch.FloatTensor</code> of length <code>config.n_layers</code>, with each tensor of shape <code>(2, batch_size, num_attn_heads, decoder_sequence_length, embed_size_per_head)</code>).</p>
<p>Contains pre-computed hidden-states (key and values in the attention blocks) of the decoder that can be
used (see <code>past_key_values</code> input) to speed up sequential decoding.`,name:"past_key_values"},{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqLMOutput.decoder_hidden_states",description:`<strong>decoder_hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, decoder_sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of main stream of the decoder at the output of each layer plus the initial embedding outputs.`,name:"decoder_hidden_states"},{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqLMOutput.decoder_ngram_hidden_states",description:`<strong>decoder_ngram_hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, ngram * decoder_sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the predict stream of the decoder at the output of each layer plus the initial embedding
outputs.`,name:"decoder_ngram_hidden_states"},{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqLMOutput.decoder_attentions",description:`<strong>decoder_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_attn_heads, decoder_sequence_length, decoder_sequence_length)</code>.</p>
<p>Attentions weights of the decoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.`,name:"decoder_attentions"},{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqLMOutput.decoder_ngram_attentions",description:`<strong>decoder_ngram_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_attn_heads, decoder_sequence_length, decoder_sequence_length)</code>.</p>
<p>Attentions weights of the predict stream of the decoder, after the attention softmax, used to compute the
weighted average in the self-attention heads.`,name:"decoder_ngram_attentions"},{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqLMOutput.cross_attentions",description:`<strong>cross_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_attn_heads, encoder_sequence_length, decoder_sequence_length)</code>.</p>
<p>Attentions weights of the cross-attention layer of the decoder, after the attention softmax, used to
compute the weighted average in the`,name:"cross_attentions"},{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqLMOutput.encoder_last_hidden_state",description:`<strong>encoder_last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, encoder_sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Sequence of hidden-states at the output of the last layer of the encoder of the model.`,name:"encoder_last_hidden_state"},{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqLMOutput.encoder_hidden_states",description:`<strong>encoder_hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, encoder_sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the encoder at the output of each layer plus the initial embedding outputs.`,name:"encoder_hidden_states"},{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqLMOutput.encoder_attentions",description:`<strong>encoder_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_attn_heads, encoder_sequence_length, encoder_sequence_length)</code>. Attentions weights of the encoder, after the attention
softmax, used to compute the weighted average in the self-attention heads.`,name:"encoder_attentions"}],source:"https://github.com/huggingface/transformers/blob/pr_16941/src/transformers/models/prophetnet/modeling_prophetnet.py#L252"}}),gt=new z({props:{name:"class transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqModelOutput",anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqModelOutput",parameters:[{name:"last_hidden_state",val:": FloatTensor"},{name:"last_hidden_state_ngram",val:": typing.Optional[torch.FloatTensor] = None"},{name:"past_key_values",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"decoder_hidden_states",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"decoder_ngram_hidden_states",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"decoder_attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"decoder_ngram_attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"cross_attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"encoder_last_hidden_state",val:": typing.Optional[torch.FloatTensor] = None"},{name:"encoder_hidden_states",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"encoder_attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"}],parametersDescription:[{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqModelOutput.last_hidden_state",description:`<strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, decoder_sequence_length, hidden_size)</code>) &#x2014;
Sequence of main stream hidden-states at the output of the last layer of the decoder of the model.</p>
<p>If <code>past_key_values</code> is used only the last hidden-state of the sequences of shape <code>(batch_size, 1, hidden_size)</code> is output.`,name:"last_hidden_state"},{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqModelOutput.last_hidden_state_ngram",description:`<strong>last_hidden_state_ngram</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size,ngram * decoder_sequence_length, config.vocab_size)</code>) &#x2014;
Sequence of predict stream hidden-states at the output of the last layer of the decoder of the model.`,name:"last_hidden_state_ngram"},{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqModelOutput.past_key_values",description:`<strong>past_key_values</strong> (<code>List[torch.FloatTensor]</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) &#x2014;
List of <code>torch.FloatTensor</code> of length <code>config.n_layers</code>, with each tensor of shape <code>(2, batch_size, num_attn_heads, decoder_sequence_length, embed_size_per_head)</code>).</p>
<p>Contains pre-computed hidden-states (key and values in the attention blocks) of the decoder that can be
used (see <code>past_key_values</code> input) to speed up sequential decoding.`,name:"past_key_values"},{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqModelOutput.decoder_hidden_states",description:`<strong>decoder_hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, decoder_sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of main stream of the decoder at the output of each layer plus the initial embedding outputs.`,name:"decoder_hidden_states"},{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqModelOutput.decoder_ngram_hidden_states",description:`<strong>decoder_ngram_hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, ngram * decoder_sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the predict stream of the decoder at the output of each layer plus the initial embedding
outputs.`,name:"decoder_ngram_hidden_states"},{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqModelOutput.decoder_attentions",description:`<strong>decoder_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_attn_heads, decoder_sequence_length, decoder_sequence_length)</code>.</p>
<p>Attentions weights of the decoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.`,name:"decoder_attentions"},{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqModelOutput.decoder_ngram_attentions",description:`<strong>decoder_ngram_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_attn_heads, decoder_sequence_length, decoder_sequence_length)</code>.</p>
<p>Attentions weights of the predict stream of the decoder, after the attention softmax, used to compute the
weighted average in the`,name:"decoder_ngram_attentions"},{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqModelOutput.cross_attentions",description:`<strong>cross_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_attn_heads, encoder_sequence_length, decoder_sequence_length)</code>.</p>
<p>Attentions weights of the cross-attention layer of the decoder, after the attention softmax, used to
compute the weighted average in the`,name:"cross_attentions"},{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqModelOutput.encoder_last_hidden_state",description:`<strong>encoder_last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, encoder_sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Sequence of hidden-states at the output of the last layer of the encoder of the model.`,name:"encoder_last_hidden_state"},{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqModelOutput.encoder_hidden_states",description:`<strong>encoder_hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, encoder_sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the encoder at the output of each layer plus the initial embedding outputs.`,name:"encoder_hidden_states"},{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqModelOutput.encoder_attentions",description:`<strong>encoder_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_attn_heads, encoder_sequence_length, encoder_sequence_length)</code>.</p>
<p>Attentions weights of the encoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.`,name:"encoder_attentions"}],source:"https://github.com/huggingface/transformers/blob/pr_16941/src/transformers/models/prophetnet/modeling_prophetnet.py#L336"}}),vt=new z({props:{name:"class transformers.models.prophetnet.modeling_prophetnet.ProphetNetDecoderModelOutput",anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetDecoderModelOutput",parameters:[{name:"last_hidden_state",val:": FloatTensor"},{name:"last_hidden_state_ngram",val:": typing.Optional[torch.FloatTensor] = None"},{name:"past_key_values",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"hidden_states",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"hidden_states_ngram",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"ngram_attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"cross_attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"}],parametersDescription:[{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetDecoderModelOutput.last_hidden_state",description:`<strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, decoder_sequence_length, hidden_size)</code>) &#x2014;
Sequence of main stream hidden-states at the output of the last layer of the decoder of the model.</p>
<p>If <code>past_key_values</code> is used only the last hidden-state of the sequences of shape <code>(batch_size, 1, hidden_size)</code> is output.`,name:"last_hidden_state"},{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetDecoderModelOutput.last_hidden_state_ngram",description:`<strong>last_hidden_state_ngram</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, ngram * decoder_sequence_length, config.vocab_size)</code>) &#x2014;
Sequence of predict stream hidden-states at the output of the last layer of the decoder of the model.`,name:"last_hidden_state_ngram"},{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetDecoderModelOutput.past_key_values",description:`<strong>past_key_values</strong> (<code>List[torch.FloatTensor]</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) &#x2014;
List of <code>torch.FloatTensor</code> of length <code>config.n_layers</code>, with each tensor of shape <code>(2, batch_size, num_attn_heads, decoder_sequence_length, embed_size_per_head)</code>).</p>
<p>Contains pre-computed hidden-states (key and values in the attention blocks) of the decoder that can be
used (see <code>past_key_values</code> input) to speed up sequential decoding.`,name:"past_key_values"},{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetDecoderModelOutput.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, decoder_sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of main stream of the decoder at the output of each layer plus the initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetDecoderModelOutput.ngram_hidden_states",description:`<strong>ngram_hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, ngram * decoder_sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the predict stream of the decoder at the output of each layer plus the initial embedding
outputs.`,name:"ngram_hidden_states"},{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetDecoderModelOutput.attentions",description:`<strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_attn_heads, decoder_sequence_length, decoder_sequence_length)</code>.</p>
<p>Attentions weights of the decoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.`,name:"attentions"},{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetDecoderModelOutput.ngram_attentions",description:`<strong>ngram_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_attn_heads, decoder_sequence_length, decoder_sequence_length)</code>.</p>
<p>Attentions weights of the predict stream of the decoder, after the attention softmax, used to compute the
weighted average in the`,name:"ngram_attentions"},{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetDecoderModelOutput.cross_attentions",description:`<strong>cross_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_attn_heads, encoder_sequence_length, decoder_sequence_length)</code>.</p>
<p>Attentions weights of the cross-attention layer of the decoder, after the attention softmax, used to
compute the weighted average in the`,name:"cross_attentions"}],source:"https://github.com/huggingface/transformers/blob/pr_16941/src/transformers/models/prophetnet/modeling_prophetnet.py#L421"}}),kt=new z({props:{name:"class transformers.models.prophetnet.modeling_prophetnet.ProphetNetDecoderLMOutput",anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetDecoderLMOutput",parameters:[{name:"loss",val:": typing.Optional[torch.FloatTensor] = None"},{name:"logits",val:": FloatTensor = None"},{name:"logits_ngram",val:": typing.Optional[torch.FloatTensor] = None"},{name:"past_key_values",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"hidden_states",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"hidden_states_ngram",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"ngram_attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"cross_attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"}],parametersDescription:[{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetDecoderLMOutput.loss",description:`<strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) &#x2014;
Language modeling loss.`,name:"loss"},{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetDecoderLMOutput.logits",description:`<strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, decoder_sequence_length, config.vocab_size)</code>) &#x2014;
Prediction scores of the main stream language modeling head (scores for each vocabulary token before
SoftMax).`,name:"logits"},{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetDecoderLMOutput.logits_ngram",description:`<strong>logits_ngram</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, ngram * decoder_sequence_length, config.vocab_size)</code>) &#x2014;
Prediction scores of the predict stream language modeling head (scores for each vocabulary token before
SoftMax).`,name:"logits_ngram"},{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetDecoderLMOutput.past_key_values",description:`<strong>past_key_values</strong> (<code>List[torch.FloatTensor]</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) &#x2014;
List of <code>torch.FloatTensor</code> of length <code>config.n_layers</code>, with each tensor of shape <code>(2, batch_size, num_attn_heads, decoder_sequence_length, embed_size_per_head)</code>).</p>
<p>Contains pre-computed hidden-states (key and values in the attention blocks) of the decoder that can be
used (see <code>past_key_values</code> input) to speed up sequential decoding.`,name:"past_key_values"},{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetDecoderLMOutput.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, decoder_sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of main stream of the decoder at the output of each layer plus the initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetDecoderLMOutput.ngram_hidden_states",description:`<strong>ngram_hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, ngram * decoder_sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the predict stream of the decoder at the output of each layer plus the initial embedding
outputs.`,name:"ngram_hidden_states"},{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetDecoderLMOutput.attentions",description:`<strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_attn_heads, decoder_sequence_length, decoder_sequence_length)</code>.</p>
<p>Attentions weights of the decoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.`,name:"attentions"},{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetDecoderLMOutput.ngram_attentions",description:`<strong>ngram_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_attn_heads, decoder_sequence_length, decoder_sequence_length)</code>.</p>
<p>Attentions weights of the predict stream of the decoder, after the attention softmax, used to compute the
weighted average in the`,name:"ngram_attentions"},{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetDecoderLMOutput.cross_attentions",description:`<strong>cross_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_attn_heads, encoder_sequence_length, decoder_sequence_length)</code>.</p>
<p>Attentions weights of the cross-attention layer of the decoder, after the attention softmax, used to
compute the weighted average in the`,name:"cross_attentions"}],source:"https://github.com/huggingface/transformers/blob/pr_16941/src/transformers/models/prophetnet/modeling_prophetnet.py#L481"}}),Tt=new re({}),bt=new z({props:{name:"class transformers.ProphetNetModel",anchor:"transformers.ProphetNetModel",parameters:[{name:"config",val:""}],parametersDescription:[{anchor:"transformers.ProphetNetModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_16941/en/model_doc/prophetnet#transformers.ProphetNetConfig">ProphetNetConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_16941/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/pr_16941/src/transformers/models/prophetnet/modeling_prophetnet.py#L1751"}}),qt=new z({props:{name:"forward",anchor:"transformers.ProphetNetModel.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"decoder_input_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"decoder_attention_mask",val:": typing.Optional[torch.BoolTensor] = None"},{name:"head_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"decoder_head_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"cross_attn_head_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"encoder_outputs",val:": typing.Optional[typing.Tuple] = None"},{name:"past_key_values",val:": typing.Optional[typing.Tuple[typing.Tuple[torch.Tensor]]] = None"},{name:"inputs_embeds",val:": typing.Optional[torch.Tensor] = None"},{name:"decoder_inputs_embeds",val:": typing.Optional[torch.Tensor] = None"},{name:"use_cache",val:": typing.Optional[bool] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.ProphetNetModel.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
it.</p>
<p>Indices can be obtained using <a href="/docs/transformers/pr_16941/en/model_doc/prophetnet#transformers.ProphetNetTokenizer">ProphetNetTokenizer</a>. See <a href="/docs/transformers/pr_16941/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/pr_16941/en/model_doc/wav2vec2_phoneme#transformers.Wav2Vec2PhonemeCTCTokenizer.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.ProphetNetModel.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.ProphetNetModel.forward.decoder_input_ids",description:`<strong>decoder_input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, target_sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of decoder input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/pr_16941/en/model_doc/prophetnet#transformers.ProphetNetTokenizer">ProphetNetTokenizer</a>. See <a href="/docs/transformers/pr_16941/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/pr_16941/en/model_doc/wav2vec2_phoneme#transformers.Wav2Vec2PhonemeCTCTokenizer.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#decoder-input-ids">What are decoder input IDs?</a></p>
<p>ProphetNet uses the <code>eos_token_id</code> as the starting token for <code>decoder_input_ids</code> generation. If
<code>past_key_values</code> is used, optionally only the last <code>decoder_input_ids</code> have to be input (see
<code>past_key_values</code>).`,name:"decoder_input_ids"},{anchor:"transformers.ProphetNetModel.forward.decoder_attention_mask",description:`<strong>decoder_attention_mask</strong> (<code>torch.BoolTensor</code> of shape <code>(batch_size, target_sequence_length)</code>, <em>optional</em>) &#x2014;
Default behavior: generate a tensor that ignores pad tokens in <code>decoder_input_ids</code>. Causal mask will also
be used by default.`,name:"decoder_attention_mask"},{anchor:"transformers.ProphetNetModel.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.Tensor</code> of shape <code>(encoder_layers, encoder_attention_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the attention modules in the encoder. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.ProphetNetModel.forward.decoder_head_mask",description:`<strong>decoder_head_mask</strong> (<code>torch.Tensor</code> of shape <code>(decoder_layers, decoder_attention_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the attention modules in the decoder. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"decoder_head_mask"},{anchor:"transformers.ProphetNetModel.forward.cross_attn_head_mask",description:`<strong>cross_attn_head_mask</strong> (<code>torch.Tensor</code> of shape <code>(decoder_layers, decoder_attention_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the cross-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"cross_attn_head_mask"},{anchor:"transformers.ProphetNetModel.forward.encoder_outputs",description:`<strong>encoder_outputs</strong> (<code>tuple(tuple(torch.FloatTensor)</code>, <em>optional</em>) &#x2014;
Tuple consists of (<code>last_hidden_state</code>, <em>optional</em>: <code>hidden_states</code>, <em>optional</em>: <code>attentions</code>)
<code>last_hidden_state</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) is a sequence of
hidden-states at the output of the last layer of the encoder. Used in the cross-attention of the decoder.`,name:"encoder_outputs"},{anchor:"transformers.ProphetNetModel.forward.past_key_values",description:`<strong>past_key_values</strong> (<code>tuple(tuple(torch.FloatTensor))</code> of length <code>config.n_layers</code> with each tuple having 4 tensors of shape <code>(batch_size, num_heads, sequence_length - 1, embed_size_per_head)</code>) &#x2014;
Contains precomputed key and value hidden-states of the attention blocks. Can be used to speed up decoding.</p>
<p>If <code>past_key_values</code> are used, the user can optionally input only the last <code>decoder_input_ids</code> (those that
don&#x2019;t have their past key value states given to this model) of shape <code>(batch_size, 1)</code> instead of all
<code>decoder_input_ids</code> of shape <code>(batch_size, sequence_length)</code>.`,name:"past_key_values"},{anchor:"transformers.ProphetNetModel.forward.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
If set to <code>True</code>, <code>past_key_values</code> key value states are returned and can be used to speed up decoding (see
<code>past_key_values</code>).`,name:"use_cache"},{anchor:"transformers.ProphetNetModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.ProphetNetModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.ProphetNetModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_16941/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/pr_16941/src/transformers/models/prophetnet/modeling_prophetnet.py#L1783",returnDescription:`
<p>A <a
  href="/docs/transformers/pr_16941/en/model_doc/prophetnet#transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqModelOutput"
>transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqModelOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<code>ProphenetConfig</code>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, decoder_sequence_length, hidden_size)</code>) \u2014 Sequence of main stream hidden-states at the output of the last layer of the decoder of the model.</p>
<p>If <code>past_key_values</code> is used only the last hidden-state of the sequences of shape <code>(batch_size, 1, hidden_size)</code> is output.</p>
</li>
<li>
<p><strong>last_hidden_state_ngram</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size,ngram * decoder_sequence_length, config.vocab_size)</code>) \u2014 Sequence of predict stream hidden-states at the output of the last layer of the decoder of the model.</p>
</li>
<li>
<p><strong>past_key_values</strong> (<code>List[torch.FloatTensor]</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) \u2014 List of <code>torch.FloatTensor</code> of length <code>config.n_layers</code>, with each tensor of shape <code>(2, batch_size, num_attn_heads, decoder_sequence_length, embed_size_per_head)</code>).</p>
<p>Contains pre-computed hidden-states (key and values in the attention blocks) of the decoder that can be
used (see <code>past_key_values</code> input) to speed up sequential decoding.</p>
</li>
<li>
<p><strong>decoder_hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, decoder_sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of main stream of the decoder at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>decoder_ngram_hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, ngram * decoder_sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the predict stream of the decoder at the output of each layer plus the initial embedding
outputs.</p>
</li>
<li>
<p><strong>decoder_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_attn_heads, decoder_sequence_length, decoder_sequence_length)</code>.</p>
<p>Attentions weights of the decoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.</p>
</li>
<li>
<p><strong>decoder_ngram_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_attn_heads, decoder_sequence_length, decoder_sequence_length)</code>.</p>
<p>Attentions weights of the predict stream of the decoder, after the attention softmax, used to compute the
weighted average in the</p>
</li>
<li>
<p><strong>cross_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_attn_heads, encoder_sequence_length, decoder_sequence_length)</code>.</p>
<p>Attentions weights of the cross-attention layer of the decoder, after the attention softmax, used to
compute the weighted average in the</p>
</li>
<li>
<p><strong>encoder_last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, encoder_sequence_length, hidden_size)</code>, <em>optional</em>) \u2014 Sequence of hidden-states at the output of the last layer of the encoder of the model.</p>
</li>
<li>
<p><strong>encoder_hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, encoder_sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the encoder at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>encoder_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_attn_heads, encoder_sequence_length, encoder_sequence_length)</code>.</p>
<p>Attentions weights of the encoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/pr_16941/en/model_doc/prophetnet#transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqModelOutput"
>transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqModelOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),We=new Yn({props:{$$slots:{default:[ih]},$$scope:{ctx:B}}}),zt=new Ho({props:{code:`from transformers import ProphetNetTokenizer, ProphetNetModel

tokenizer = ProphetNetTokenizer.from_pretrained("microsoft/prophetnet-large-uncased")
model = ProphetNetModel.from_pretrained("microsoft/prophetnet-large-uncased")

input_ids = tokenizer(
    "Studies have been shown that owning a dog is good for you", return_tensors="pt"
).input_ids  # Batch size 1
decoder_input_ids = tokenizer("Studies show that", return_tensors="pt").input_ids  # Batch size 1
outputs = model(input_ids=input_ids, decoder_input_ids=decoder_input_ids)

last_hidden_states = outputs.last_hidden_state  # main stream hidden states
last_hidden_states_ngram = outputs.last_hidden_state_ngram  # predict hidden states`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> ProphetNetTokenizer, ProphetNetModel

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = ProphetNetTokenizer.from_pretrained(<span class="hljs-string">&quot;microsoft/prophetnet-large-uncased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = ProphetNetModel.from_pretrained(<span class="hljs-string">&quot;microsoft/prophetnet-large-uncased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = tokenizer(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;Studies have been shown that owning a dog is good for you&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>).input_ids  <span class="hljs-comment"># Batch size 1</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>decoder_input_ids = tokenizer(<span class="hljs-string">&quot;Studies show that&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).input_ids  <span class="hljs-comment"># Batch size 1</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(input_ids=input_ids, decoder_input_ids=decoder_input_ids)

<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_states = outputs.last_hidden_state  <span class="hljs-comment"># main stream hidden states</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_states_ngram = outputs.last_hidden_state_ngram  <span class="hljs-comment"># predict hidden states</span>`}}),$t=new re({}),Ft=new z({props:{name:"class transformers.ProphetNetEncoder",anchor:"transformers.ProphetNetEncoder",parameters:[{name:"config",val:": ProphetNetConfig"},{name:"word_embeddings",val:": Embedding = None"}],parametersDescription:[{anchor:"transformers.ProphetNetEncoder.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_16941/en/model_doc/prophetnet#transformers.ProphetNetConfig">ProphetNetConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_16941/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/pr_16941/src/transformers/models/prophetnet/modeling_prophetnet.py#L1244"}}),Ot=new z({props:{name:"forward",anchor:"transformers.ProphetNetEncoder.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"head_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"inputs_embeds",val:": typing.Optional[torch.Tensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.ProphetNetEncoder.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
it.</p>
<p>Indices can be obtained using <a href="/docs/transformers/pr_16941/en/model_doc/prophetnet#transformers.ProphetNetTokenizer">ProphetNetTokenizer</a>. See <a href="/docs/transformers/pr_16941/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/pr_16941/en/model_doc/wav2vec2_phoneme#transformers.Wav2Vec2PhonemeCTCTokenizer.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.ProphetNetEncoder.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.ProphetNetEncoder.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.Tensor</code> of shape <code>(encoder_layers, encoder_attention_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the attention modules in the encoder. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.ProphetNetEncoder.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.ProphetNetEncoder.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.ProphetNetEncoder.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_16941/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/pr_16941/src/transformers/models/prophetnet/modeling_prophetnet.py#L1274",returnDescription:`
<p>A <a
  href="/docs/transformers/pr_16941/en/main_classes/output#transformers.modeling_outputs.BaseModelOutput"
>transformers.modeling_outputs.BaseModelOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<code>ProphenetConfig</code>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) \u2014 Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/pr_16941/en/main_classes/output#transformers.modeling_outputs.BaseModelOutput"
>transformers.modeling_outputs.BaseModelOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),Ge=new Yn({props:{$$slots:{default:[ch]},$$scope:{ctx:B}}}),St=new Ho({props:{code:`from transformers import ProphetNetTokenizer, ProphetNetEncoder
import torch

tokenizer = ProphetNetTokenizer.from_pretrained("microsoft/prophetnet-large-uncased")
model = ProphetNetEncoder.from_pretrained("patrickvonplaten/prophetnet-large-uncased-standalone")
inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
outputs = model(**inputs)

last_hidden_states = outputs.last_hidden_state`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> ProphetNetTokenizer, ProphetNetEncoder
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = ProphetNetTokenizer.from_pretrained(<span class="hljs-string">&quot;microsoft/prophetnet-large-uncased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = ProphetNetEncoder.from_pretrained(<span class="hljs-string">&quot;patrickvonplaten/prophetnet-large-uncased-standalone&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_states = outputs.last_hidden_state`}}),Dt=new re({}),Lt=new z({props:{name:"class transformers.ProphetNetDecoder",anchor:"transformers.ProphetNetDecoder",parameters:[{name:"config",val:": ProphetNetConfig"},{name:"word_embeddings",val:": Embedding = None"}],parametersDescription:[{anchor:"transformers.ProphetNetDecoder.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_16941/en/model_doc/prophetnet#transformers.ProphetNetConfig">ProphetNetConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_16941/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/pr_16941/src/transformers/models/prophetnet/modeling_prophetnet.py#L1384"}}),Bt=new z({props:{name:"forward",anchor:"transformers.ProphetNetDecoder.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"encoder_hidden_states",val:": typing.Optional[torch.Tensor] = None"},{name:"encoder_attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"head_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"cross_attn_head_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"past_key_values",val:": typing.Optional[typing.Tuple[typing.Tuple[torch.Tensor]]] = None"},{name:"inputs_embeds",val:": typing.Optional[torch.Tensor] = None"},{name:"use_cache",val:": typing.Optional[bool] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.ProphetNetDecoder.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
it.</p>
<p>Indices can be obtained using <a href="/docs/transformers/pr_16941/en/model_doc/prophetnet#transformers.ProphetNetTokenizer">ProphetNetTokenizer</a>. See <a href="/docs/transformers/pr_16941/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/pr_16941/en/model_doc/wav2vec2_phoneme#transformers.Wav2Vec2PhonemeCTCTokenizer.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.ProphetNetDecoder.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.ProphetNetDecoder.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.Tensor</code> of shape <code>(encoder_layers, encoder_attention_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the attention modules in the encoder. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.ProphetNetDecoder.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.ProphetNetDecoder.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.ProphetNetDecoder.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_16941/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.ProphetNetDecoder.forward.encoder_hidden_states",description:`<strong>encoder_hidden_states</strong>  (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if
the model is configured as a decoder.`,name:"encoder_hidden_states"},{anchor:"transformers.ProphetNetDecoder.forward.encoder_attention_mask",description:`<strong>encoder_attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used in
the cross-attention if the model is configured as a decoder. Mask values selected in <code>[0, 1]</code>:`,name:"encoder_attention_mask"},{anchor:"transformers.ProphetNetDecoder.forward.cross_attn_head_mask",description:`<strong>cross_attn_head_mask</strong> (<code>torch.Tensor</code> of shape <code>(decoder_layers, decoder_attention_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the cross-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"cross_attn_head_mask"},{anchor:"transformers.ProphetNetDecoder.forward.past_key_values",description:`<strong>past_key_values</strong> (<code>tuple(tuple(torch.FloatTensor))</code> of length <code>config.n_layers</code> with each tuple having 4 tensors of shape <code>(batch_size, num_heads, sequence_length - 1, embed_size_per_head)</code>) &#x2014;
Contains precomputed key and value hidden-states of the attention blocks. Can be used to speed up decoding.</p>
<p>If <code>past_key_values</code> are used, the user can optionally input only the last <code>decoder_input_ids</code> (those that
don&#x2019;t have their past key value states given to this model) of shape <code>(batch_size, 1)</code> instead of all
<code>decoder_input_ids</code> of shape <code>(batch_size, sequence_length)</code>.`,name:"past_key_values"},{anchor:"transformers.ProphetNetDecoder.forward.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
If set to <code>True</code>, <code>past_key_values</code> key value states are returned and can be used to speed up decoding (see
<code>past_key_values</code>).</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>`,name:"use_cache"}],source:"https://github.com/huggingface/transformers/blob/pr_16941/src/transformers/models/prophetnet/modeling_prophetnet.py#L1421",returnDescription:`
<p>A <a
  href="/docs/transformers/pr_16941/en/model_doc/prophetnet#transformers.models.prophetnet.modeling_prophetnet.ProphetNetDecoderModelOutput"
>transformers.models.prophetnet.modeling_prophetnet.ProphetNetDecoderModelOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<code>ProphenetConfig</code>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, decoder_sequence_length, hidden_size)</code>) \u2014 Sequence of main stream hidden-states at the output of the last layer of the decoder of the model.</p>
<p>If <code>past_key_values</code> is used only the last hidden-state of the sequences of shape <code>(batch_size, 1, hidden_size)</code> is output.</p>
</li>
<li>
<p><strong>last_hidden_state_ngram</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, ngram * decoder_sequence_length, config.vocab_size)</code>) \u2014 Sequence of predict stream hidden-states at the output of the last layer of the decoder of the model.</p>
</li>
<li>
<p><strong>past_key_values</strong> (<code>List[torch.FloatTensor]</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) \u2014 List of <code>torch.FloatTensor</code> of length <code>config.n_layers</code>, with each tensor of shape <code>(2, batch_size, num_attn_heads, decoder_sequence_length, embed_size_per_head)</code>).</p>
<p>Contains pre-computed hidden-states (key and values in the attention blocks) of the decoder that can be
used (see <code>past_key_values</code> input) to speed up sequential decoding.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, decoder_sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of main stream of the decoder at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>ngram_hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, ngram * decoder_sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the predict stream of the decoder at the output of each layer plus the initial embedding
outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_attn_heads, decoder_sequence_length, decoder_sequence_length)</code>.</p>
<p>Attentions weights of the decoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.</p>
</li>
<li>
<p><strong>ngram_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_attn_heads, decoder_sequence_length, decoder_sequence_length)</code>.</p>
<p>Attentions weights of the predict stream of the decoder, after the attention softmax, used to compute the
weighted average in the</p>
</li>
<li>
<p><strong>cross_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_attn_heads, encoder_sequence_length, decoder_sequence_length)</code>.</p>
<p>Attentions weights of the cross-attention layer of the decoder, after the attention softmax, used to
compute the weighted average in the</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/pr_16941/en/model_doc/prophetnet#transformers.models.prophetnet.modeling_prophetnet.ProphetNetDecoderModelOutput"
>transformers.models.prophetnet.modeling_prophetnet.ProphetNetDecoderModelOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),Ve=new Yn({props:{$$slots:{default:[lh]},$$scope:{ctx:B}}}),Gt=new Ho({props:{code:`from transformers import ProphetNetTokenizer, ProphetNetDecoder
import torch

tokenizer = ProphetNetTokenizer.from_pretrained("microsoft/prophetnet-large-uncased")
model = ProphetNetDecoder.from_pretrained("microsoft/prophetnet-large-uncased", add_cross_attention=False)
inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
outputs = model(**inputs)

last_hidden_states = outputs.last_hidden_state`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> ProphetNetTokenizer, ProphetNetDecoder
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = ProphetNetTokenizer.from_pretrained(<span class="hljs-string">&quot;microsoft/prophetnet-large-uncased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = ProphetNetDecoder.from_pretrained(<span class="hljs-string">&quot;microsoft/prophetnet-large-uncased&quot;</span>, add_cross_attention=<span class="hljs-literal">False</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_states = outputs.last_hidden_state`}}),Ht=new re({}),Vt=new z({props:{name:"class transformers.ProphetNetForConditionalGeneration",anchor:"transformers.ProphetNetForConditionalGeneration",parameters:[{name:"config",val:": ProphetNetConfig"}],parametersDescription:[{anchor:"transformers.ProphetNetForConditionalGeneration.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_16941/en/model_doc/prophetnet#transformers.ProphetNetConfig">ProphetNetConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_16941/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/pr_16941/src/transformers/models/prophetnet/modeling_prophetnet.py#L1878"}}),Kt=new z({props:{name:"forward",anchor:"transformers.ProphetNetForConditionalGeneration.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"decoder_input_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"decoder_attention_mask",val:": typing.Optional[torch.BoolTensor] = None"},{name:"head_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"decoder_head_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"cross_attn_head_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"encoder_outputs",val:": typing.Optional[torch.Tensor] = None"},{name:"past_key_values",val:": typing.Optional[typing.Tuple[typing.Tuple[torch.Tensor]]] = None"},{name:"inputs_embeds",val:": typing.Optional[torch.Tensor] = None"},{name:"decoder_inputs_embeds",val:": typing.Optional[torch.Tensor] = None"},{name:"labels",val:": typing.Optional[torch.Tensor] = None"},{name:"use_cache",val:": typing.Optional[bool] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.ProphetNetForConditionalGeneration.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
it.</p>
<p>Indices can be obtained using <a href="/docs/transformers/pr_16941/en/model_doc/prophetnet#transformers.ProphetNetTokenizer">ProphetNetTokenizer</a>. See <a href="/docs/transformers/pr_16941/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/pr_16941/en/model_doc/wav2vec2_phoneme#transformers.Wav2Vec2PhonemeCTCTokenizer.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.ProphetNetForConditionalGeneration.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.ProphetNetForConditionalGeneration.forward.decoder_input_ids",description:`<strong>decoder_input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, target_sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of decoder input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/pr_16941/en/model_doc/prophetnet#transformers.ProphetNetTokenizer">ProphetNetTokenizer</a>. See <a href="/docs/transformers/pr_16941/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/pr_16941/en/model_doc/wav2vec2_phoneme#transformers.Wav2Vec2PhonemeCTCTokenizer.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#decoder-input-ids">What are decoder input IDs?</a></p>
<p>ProphetNet uses the <code>eos_token_id</code> as the starting token for <code>decoder_input_ids</code> generation. If
<code>past_key_values</code> is used, optionally only the last <code>decoder_input_ids</code> have to be input (see
<code>past_key_values</code>).`,name:"decoder_input_ids"},{anchor:"transformers.ProphetNetForConditionalGeneration.forward.decoder_attention_mask",description:`<strong>decoder_attention_mask</strong> (<code>torch.BoolTensor</code> of shape <code>(batch_size, target_sequence_length)</code>, <em>optional</em>) &#x2014;
Default behavior: generate a tensor that ignores pad tokens in <code>decoder_input_ids</code>. Causal mask will also
be used by default.`,name:"decoder_attention_mask"},{anchor:"transformers.ProphetNetForConditionalGeneration.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.Tensor</code> of shape <code>(encoder_layers, encoder_attention_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the attention modules in the encoder. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.ProphetNetForConditionalGeneration.forward.decoder_head_mask",description:`<strong>decoder_head_mask</strong> (<code>torch.Tensor</code> of shape <code>(decoder_layers, decoder_attention_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the attention modules in the decoder. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"decoder_head_mask"},{anchor:"transformers.ProphetNetForConditionalGeneration.forward.cross_attn_head_mask",description:`<strong>cross_attn_head_mask</strong> (<code>torch.Tensor</code> of shape <code>(decoder_layers, decoder_attention_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the cross-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"cross_attn_head_mask"},{anchor:"transformers.ProphetNetForConditionalGeneration.forward.encoder_outputs",description:`<strong>encoder_outputs</strong> (<code>tuple(tuple(torch.FloatTensor)</code>, <em>optional</em>) &#x2014;
Tuple consists of (<code>last_hidden_state</code>, <em>optional</em>: <code>hidden_states</code>, <em>optional</em>: <code>attentions</code>)
<code>last_hidden_state</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) is a sequence of
hidden-states at the output of the last layer of the encoder. Used in the cross-attention of the decoder.`,name:"encoder_outputs"},{anchor:"transformers.ProphetNetForConditionalGeneration.forward.past_key_values",description:`<strong>past_key_values</strong> (<code>tuple(tuple(torch.FloatTensor))</code> of length <code>config.n_layers</code> with each tuple having 4 tensors of shape <code>(batch_size, num_heads, sequence_length - 1, embed_size_per_head)</code>) &#x2014;
Contains precomputed key and value hidden-states of the attention blocks. Can be used to speed up decoding.</p>
<p>If <code>past_key_values</code> are used, the user can optionally input only the last <code>decoder_input_ids</code> (those that
don&#x2019;t have their past key value states given to this model) of shape <code>(batch_size, 1)</code> instead of all
<code>decoder_input_ids</code> of shape <code>(batch_size, sequence_length)</code>.`,name:"past_key_values"},{anchor:"transformers.ProphetNetForConditionalGeneration.forward.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
If set to <code>True</code>, <code>past_key_values</code> key value states are returned and can be used to speed up decoding (see
<code>past_key_values</code>).`,name:"use_cache"},{anchor:"transformers.ProphetNetForConditionalGeneration.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.ProphetNetForConditionalGeneration.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.ProphetNetForConditionalGeneration.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_16941/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.ProphetNetForConditionalGeneration.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for computing the sequence classification/regression loss. Indices should be in <code>[-100, 0, ..., config.vocab_size - 1]</code>. All labels set to <code>-100</code> are ignored (masked), the loss is only computed for
labels in <code>[0, ..., config.vocab_size]</code>`,name:"labels"}],source:"https://github.com/huggingface/transformers/blob/pr_16941/src/transformers/models/prophetnet/modeling_prophetnet.py#L1899",returnDescription:`
<p>A <a
  href="/docs/transformers/pr_16941/en/model_doc/prophetnet#transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqLMOutput"
>transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqLMOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<code>ProphenetConfig</code>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) \u2014 Language modeling loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, decoder_sequence_length, config.vocab_size)</code>) \u2014 Prediction scores of the main stream language modeling head (scores for each vocabulary token before
SoftMax).</p>
</li>
<li>
<p><strong>logits_ngram</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, ngram * decoder_sequence_length, config.vocab_size)</code>) \u2014 Prediction scores of the predict stream language modeling head (scores for each vocabulary token before
SoftMax).</p>
</li>
<li>
<p><strong>past_key_values</strong> (<code>List[torch.FloatTensor]</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) \u2014 List of <code>torch.FloatTensor</code> of length <code>config.n_layers</code>, with each tensor of shape <code>(2, batch_size, num_attn_heads, decoder_sequence_length, embed_size_per_head)</code>).</p>
<p>Contains pre-computed hidden-states (key and values in the attention blocks) of the decoder that can be
used (see <code>past_key_values</code> input) to speed up sequential decoding.</p>
</li>
<li>
<p><strong>decoder_hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, decoder_sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of main stream of the decoder at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>decoder_ngram_hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, ngram * decoder_sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the predict stream of the decoder at the output of each layer plus the initial embedding
outputs.</p>
</li>
<li>
<p><strong>decoder_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_attn_heads, decoder_sequence_length, decoder_sequence_length)</code>.</p>
<p>Attentions weights of the decoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.</p>
</li>
<li>
<p><strong>decoder_ngram_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_attn_heads, decoder_sequence_length, decoder_sequence_length)</code>.</p>
<p>Attentions weights of the predict stream of the decoder, after the attention softmax, used to compute the
weighted average in the self-attention heads.</p>
</li>
<li>
<p><strong>cross_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_attn_heads, encoder_sequence_length, decoder_sequence_length)</code>.</p>
<p>Attentions weights of the cross-attention layer of the decoder, after the attention softmax, used to
compute the weighted average in the</p>
</li>
<li>
<p><strong>encoder_last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, encoder_sequence_length, hidden_size)</code>, <em>optional</em>) \u2014 Sequence of hidden-states at the output of the last layer of the encoder of the model.</p>
</li>
<li>
<p><strong>encoder_hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, encoder_sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the encoder at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>encoder_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_attn_heads, encoder_sequence_length, encoder_sequence_length)</code>. Attentions weights of the encoder, after the attention
softmax, used to compute the weighted average in the self-attention heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/pr_16941/en/model_doc/prophetnet#transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqLMOutput"
>transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqLMOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),Re=new Yn({props:{$$slots:{default:[hh]},$$scope:{ctx:B}}}),Qt=new Ho({props:{code:`from transformers import ProphetNetTokenizer, ProphetNetForConditionalGeneration

tokenizer = ProphetNetTokenizer.from_pretrained("microsoft/prophetnet-large-uncased")
model = ProphetNetForConditionalGeneration.from_pretrained("microsoft/prophetnet-large-uncased")

input_ids = tokenizer(
    "Studies have been shown that owning a dog is good for you", return_tensors="pt"
).input_ids  # Batch size 1
decoder_input_ids = tokenizer("Studies show that", return_tensors="pt").input_ids  # Batch size 1
outputs = model(input_ids=input_ids, decoder_input_ids=decoder_input_ids)

logits_next_token = outputs.logits  # logits to predict next token as usual
logits_ngram_next_tokens = outputs.logits_ngram  # logits to predict 2nd, 3rd, ... next tokens`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> ProphetNetTokenizer, ProphetNetForConditionalGeneration

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = ProphetNetTokenizer.from_pretrained(<span class="hljs-string">&quot;microsoft/prophetnet-large-uncased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = ProphetNetForConditionalGeneration.from_pretrained(<span class="hljs-string">&quot;microsoft/prophetnet-large-uncased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = tokenizer(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;Studies have been shown that owning a dog is good for you&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>).input_ids  <span class="hljs-comment"># Batch size 1</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>decoder_input_ids = tokenizer(<span class="hljs-string">&quot;Studies show that&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).input_ids  <span class="hljs-comment"># Batch size 1</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(input_ids=input_ids, decoder_input_ids=decoder_input_ids)

<span class="hljs-meta">&gt;&gt;&gt; </span>logits_next_token = outputs.logits  <span class="hljs-comment"># logits to predict next token as usual</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>logits_ngram_next_tokens = outputs.logits_ngram  <span class="hljs-comment"># logits to predict 2nd, 3rd, ... next tokens</span>`}}),Xt=new re({}),Zt=new z({props:{name:"class transformers.ProphetNetForCausalLM",anchor:"transformers.ProphetNetForCausalLM",parameters:[{name:"config",val:""}],parametersDescription:[{anchor:"transformers.ProphetNetForCausalLM.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_16941/en/model_doc/prophetnet#transformers.ProphetNetConfig">ProphetNetConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_16941/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/pr_16941/src/transformers/models/prophetnet/modeling_prophetnet.py#L2087"}}),ro=new z({props:{name:"forward",anchor:"transformers.ProphetNetForCausalLM.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"encoder_hidden_states",val:": typing.Optional[torch.Tensor] = None"},{name:"encoder_attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"head_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"cross_attn_head_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"past_key_values",val:": typing.Optional[typing.Tuple[typing.Tuple[torch.Tensor]]] = None"},{name:"inputs_embeds",val:": typing.Optional[torch.Tensor] = None"},{name:"labels",val:": typing.Optional[torch.Tensor] = None"},{name:"use_cache",val:": typing.Optional[bool] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.ProphetNetForCausalLM.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
it.</p>
<p>Indices can be obtained using <a href="/docs/transformers/pr_16941/en/model_doc/prophetnet#transformers.ProphetNetTokenizer">ProphetNetTokenizer</a>. See <a href="/docs/transformers/pr_16941/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/pr_16941/en/model_doc/wav2vec2_phoneme#transformers.Wav2Vec2PhonemeCTCTokenizer.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.ProphetNetForCausalLM.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.ProphetNetForCausalLM.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.Tensor</code> of shape <code>(encoder_layers, encoder_attention_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the attention modules in the encoder. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.ProphetNetForCausalLM.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.ProphetNetForCausalLM.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.ProphetNetForCausalLM.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_16941/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.ProphetNetForCausalLM.forward.encoder_hidden_states",description:`<strong>encoder_hidden_states</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if
the model is configured as a decoder.`,name:"encoder_hidden_states"},{anchor:"transformers.ProphetNetForCausalLM.forward.encoder_attention_mask",description:`<strong>encoder_attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used in
the cross-attention if the model is configured as a decoder. Mask values selected in <code>[0, 1]</code>:`,name:"encoder_attention_mask"},{anchor:"transformers.ProphetNetForCausalLM.forward.cross_attn_head_mask",description:`<strong>cross_attn_head_mask</strong> (<code>torch.Tensor</code> of shape <code>(decoder_layers, decoder_attention_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the cross-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"cross_attn_head_mask"},{anchor:"transformers.ProphetNetForCausalLM.forward.past_key_values",description:`<strong>past_key_values</strong> (<code>tuple(tuple(torch.FloatTensor))</code> of length <code>config.n_layers</code> with each tuple having 4 tensors of shape <code>(batch_size, num_heads, sequence_length - 1, embed_size_per_head)</code>) &#x2014;
Contains precomputed key and value hidden-states of the attention blocks. Can be used to speed up decoding.</p>
<p>If <code>past_key_values</code> are used, the user can optionally input only the last <code>decoder_input_ids</code> (those that
don&#x2019;t have their past key value states given to this model) of shape <code>(batch_size, 1)</code> instead of all
<code>decoder_input_ids</code> of shape <code>(batch_size, sequence_length)</code>.`,name:"past_key_values"},{anchor:"transformers.ProphetNetForCausalLM.forward.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
If set to <code>True</code>, <code>past_key_values</code> key value states are returned and can be used to speed up decoding (see
<code>past_key_values</code>).</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>`,name:"use_cache"},{anchor:"transformers.ProphetNetForCausalLM.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Labels for computing the left-to-right language modeling loss (next word prediction). Indices should be in
<code>[-100, 0, ..., config.vocab_size]</code> (see <code>input_ids</code> docstring) Tokens with indices set to <code>-100</code> are
ignored (masked), the loss is only computed for the tokens with labels n <code>[0, ..., config.vocab_size]</code>`,name:"labels"}],source:"https://github.com/huggingface/transformers/blob/pr_16941/src/transformers/models/prophetnet/modeling_prophetnet.py#L2122",returnDescription:`
<p>A <a
  href="/docs/transformers/pr_16941/en/model_doc/prophetnet#transformers.models.prophetnet.modeling_prophetnet.ProphetNetDecoderLMOutput"
>transformers.models.prophetnet.modeling_prophetnet.ProphetNetDecoderLMOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<code>ProphenetConfig</code>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) \u2014 Language modeling loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, decoder_sequence_length, config.vocab_size)</code>) \u2014 Prediction scores of the main stream language modeling head (scores for each vocabulary token before
SoftMax).</p>
</li>
<li>
<p><strong>logits_ngram</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, ngram * decoder_sequence_length, config.vocab_size)</code>) \u2014 Prediction scores of the predict stream language modeling head (scores for each vocabulary token before
SoftMax).</p>
</li>
<li>
<p><strong>past_key_values</strong> (<code>List[torch.FloatTensor]</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) \u2014 List of <code>torch.FloatTensor</code> of length <code>config.n_layers</code>, with each tensor of shape <code>(2, batch_size, num_attn_heads, decoder_sequence_length, embed_size_per_head)</code>).</p>
<p>Contains pre-computed hidden-states (key and values in the attention blocks) of the decoder that can be
used (see <code>past_key_values</code> input) to speed up sequential decoding.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, decoder_sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of main stream of the decoder at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>ngram_hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, ngram * decoder_sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the predict stream of the decoder at the output of each layer plus the initial embedding
outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_attn_heads, decoder_sequence_length, decoder_sequence_length)</code>.</p>
<p>Attentions weights of the decoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.</p>
</li>
<li>
<p><strong>ngram_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_attn_heads, decoder_sequence_length, decoder_sequence_length)</code>.</p>
<p>Attentions weights of the predict stream of the decoder, after the attention softmax, used to compute the
weighted average in the</p>
</li>
<li>
<p><strong>cross_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_attn_heads, encoder_sequence_length, decoder_sequence_length)</code>.</p>
<p>Attentions weights of the cross-attention layer of the decoder, after the attention softmax, used to
compute the weighted average in the</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/pr_16941/en/model_doc/prophetnet#transformers.models.prophetnet.modeling_prophetnet.ProphetNetDecoderLMOutput"
>transformers.models.prophetnet.modeling_prophetnet.ProphetNetDecoderLMOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),Ye=new Yn({props:{$$slots:{default:[ph]},$$scope:{ctx:B}}}),so=new Ho({props:{code:`from transformers import ProphetNetTokenizer, ProphetNetForCausalLM
import torch

tokenizer = ProphetNetTokenizer.from_pretrained("microsoft/prophetnet-large-uncased")
model = ProphetNetForCausalLM.from_pretrained("microsoft/prophetnet-large-uncased")
assert model.config.is_decoder, f"{model.__class__} has to be configured as a decoder."
inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
outputs = model(**inputs)

logits = outputs.logits

# Model can also be used with EncoderDecoder framework
from transformers import BertTokenizer, EncoderDecoderModel, ProphetNetTokenizer
import torch

tokenizer_enc = BertTokenizer.from_pretrained("bert-large-uncased")
tokenizer_dec = ProphetNetTokenizer.from_pretrained("microsoft/prophetnet-large-uncased")
model = EncoderDecoderModel.from_encoder_decoder_pretrained(
    "bert-large-uncased", "microsoft/prophetnet-large-uncased"
)

ARTICLE = (
    "the us state department said wednesday it had received no "
    "formal word from bolivia that it was expelling the us ambassador there "
    "but said the charges made against him are \`\` baseless ."
)
input_ids = tokenizer_enc(ARTICLE, return_tensors="pt").input_ids
labels = tokenizer_dec(
    "us rejects charges against its ambassador in bolivia", return_tensors="pt"
).input_ids
outputs = model(input_ids=input_ids, decoder_input_ids=labels[:, :-1], labels=labels[:, 1:])

loss = outputs.loss`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> ProphetNetTokenizer, ProphetNetForCausalLM
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = ProphetNetTokenizer.from_pretrained(<span class="hljs-string">&quot;microsoft/prophetnet-large-uncased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = ProphetNetForCausalLM.from_pretrained(<span class="hljs-string">&quot;microsoft/prophetnet-large-uncased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">assert</span> model.config.is_decoder, <span class="hljs-string">f&quot;<span class="hljs-subst">{model.__class__}</span> has to be configured as a decoder.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Model can also be used with EncoderDecoder framework</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> BertTokenizer, EncoderDecoderModel, ProphetNetTokenizer
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer_enc = BertTokenizer.from_pretrained(<span class="hljs-string">&quot;bert-large-uncased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer_dec = ProphetNetTokenizer.from_pretrained(<span class="hljs-string">&quot;microsoft/prophetnet-large-uncased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = EncoderDecoderModel.from_encoder_decoder_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;bert-large-uncased&quot;</span>, <span class="hljs-string">&quot;microsoft/prophetnet-large-uncased&quot;</span>
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>ARTICLE = (
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;the us state department said wednesday it had received no &quot;</span>
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;formal word from bolivia that it was expelling the us ambassador there &quot;</span>
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;but said the charges made against him are \`\` baseless .&quot;</span>
<span class="hljs-meta">... </span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = tokenizer_enc(ARTICLE, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).input_ids
<span class="hljs-meta">&gt;&gt;&gt; </span>labels = tokenizer_dec(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;us rejects charges against its ambassador in bolivia&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>).input_ids
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(input_ids=input_ids, decoder_input_ids=labels[:, :-<span class="hljs-number">1</span>], labels=labels[:, <span class="hljs-number">1</span>:])

<span class="hljs-meta">&gt;&gt;&gt; </span>loss = outputs.loss`}}),{c(){u=s("meta"),N=c(),T=s("h1"),w=s("a"),P=s("span"),m(y.$$.fragment),b=c(),q=s("span"),Zr=n("ProphetNet"),Kn=c(),se=s("p"),Vo=s("strong"),es=n("DISCLAIMER:"),ts=n(" If you see something strange, file a "),Xe=s("a"),os=n("Github Issue"),ns=n(` and assign
@patrickvonplaten`),Qn=c(),ae=s("h2"),Ee=s("a"),Uo=s("span"),m(Ze.$$.fragment),rs=c(),Ro=s("span"),ss=n("Overview"),Xn=c(),Ce=s("p"),as=n("The ProphetNet model was proposed in "),et=s("a"),ds=n("ProphetNet: Predicting Future N-gram for Sequence-to-Sequence Pre-training,"),is=n(` by Yu Yan, Weizhen Qi, Yeyun Gong, Dayiheng Liu, Nan Duan, Jiusheng Chen, Ruofei
Zhang, Ming Zhou on 13 Jan, 2020.`),Zn=c(),io=s("p"),cs=n(`ProphetNet is an encoder-decoder model and can predict n-future tokens for \u201Cngram\u201D language modeling instead of just
the next token.`),er=c(),co=s("p"),ls=n("The abstract from the paper is the following:"),tr=c(),lo=s("p"),Jo=s("em"),hs=n(`In this paper, we present a new sequence-to-sequence pretraining model called ProphetNet, which introduces a novel
self-supervised objective named future n-gram prediction and the proposed n-stream self-attention mechanism. Instead of
the optimization of one-step ahead prediction in traditional sequence-to-sequence model, the ProphetNet is optimized by
n-step ahead prediction which predicts the next n tokens simultaneously based on previous context tokens at each time
step. The future n-gram prediction explicitly encourages the model to plan for the future tokens and prevent
overfitting on strong local correlations. We pre-train ProphetNet using a base scale dataset (16GB) and a large scale
dataset (160GB) respectively. Then we conduct experiments on CNN/DailyMail, Gigaword, and SQuAD 1.1 benchmarks for
abstractive summarization and question generation tasks. Experimental results show that ProphetNet achieves new
state-of-the-art results on all these datasets compared to the models using the same scale pretraining corpus.`),or=c(),Oe=s("p"),ps=n("The Authors\u2019 code can be found "),tt=s("a"),us=n("here"),ms=n("."),nr=c(),de=s("h2"),Se=s("a"),Yo=s("span"),m(ot.$$.fragment),fs=c(),Ko=s("span"),_s=n("ProphetNetConfig"),rr=c(),U=s("div"),m(nt.$$.fragment),gs=c(),ie=s("p"),vs=n("This is the configuration class to store the configuration of a "),ho=s("a"),ks=n("ProphetNetModel"),Ts=n(`. It is used to instantiate a
ProphetNet model according to the specified arguments, defining the model architecture. Instantiating a
configuration with the defaults will yield a similar configuration to that of the ProphetNet
`),rt=s("a"),bs=n("microsoft/prophetnet-large-uncased"),ys=n(" architecture."),ws=c(),ce=s("p"),Ps=n("Configuration objects inherit from "),po=s("a"),Ns=n("PretrainedConfig"),qs=n(` and can be used to control the model outputs. Read the
documentation from `),uo=s("a"),zs=n("PretrainedConfig"),$s=n(" for more information."),sr=c(),le=s("h2"),De=s("a"),Qo=s("span"),m(st.$$.fragment),Fs=c(),Xo=s("span"),Ms=n("ProphetNetTokenizer"),ar=c(),$=s("div"),m(at.$$.fragment),xs=c(),Zo=s("p"),Es=n("Construct a ProphetNetTokenizer. Based on WordPiece."),Cs=c(),dt=s("p"),Os=n("This tokenizer inherits from "),mo=s("a"),Ss=n("PreTrainedTokenizer"),Ds=n(` which contains most of the main methods. Users should refer to
this superclass for more information regarding those methods.`),Ls=c(),R=s("div"),m(it.$$.fragment),js=c(),en=s("p"),As=n(`Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens. A BERT sequence has the following format:`),Is=c(),ct=s("ul"),fo=s("li"),Ws=n("single sequence: "),tn=s("code"),Bs=n("[CLS] X [SEP]"),Gs=c(),_o=s("li"),Hs=n("pair of sequences: "),on=s("code"),Vs=n("[CLS] A [SEP] B [SEP]"),Us=c(),Le=s("div"),m(lt.$$.fragment),Rs=c(),nn=s("p"),Js=n("Converts a sequence of tokens (string) in a single string."),Ys=c(),G=s("div"),m(ht.$$.fragment),Ks=c(),rn=s("p"),Qs=n(`Create a mask from the two sequences passed to be used in a sequence-pair classification task. A ProphetNet
sequence pair mask has the following format:`),Xs=c(),m(pt.$$.fragment),Zs=c(),he=s("p"),ea=n("If "),sn=s("code"),ta=n("token_ids_1"),oa=n(" is "),an=s("code"),na=n("None"),ra=n(", this method only returns the first portion of the mask (0s)."),sa=c(),je=s("div"),m(ut.$$.fragment),aa=c(),mt=s("p"),da=n(`Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding
special tokens using the tokenizer `),dn=s("code"),ia=n("prepare_for_model"),ca=n(" method."),dr=c(),pe=s("h2"),Ae=s("a"),cn=s("span"),m(ft.$$.fragment),la=c(),ln=s("span"),ha=n("ProphetNet specific outputs"),ir=c(),ue=s("div"),m(_t.$$.fragment),pa=c(),hn=s("p"),ua=n("Base class for sequence-to-sequence language models outputs."),cr=c(),me=s("div"),m(gt.$$.fragment),ma=c(),pn=s("p"),fa=n(`Base class for model encoder\u2019s outputs that also contains : pre-computed hidden states that can speed up sequential
decoding.`),lr=c(),fe=s("div"),m(vt.$$.fragment),_a=c(),un=s("p"),ga=n("Base class for model\u2019s outputs that may also contain a past key/values (to speed up sequential decoding)."),hr=c(),_e=s("div"),m(kt.$$.fragment),va=c(),mn=s("p"),ka=n("Base class for model\u2019s outputs that may also contain a past key/values (to speed up sequential decoding)."),pr=c(),ge=s("h2"),Ie=s("a"),fn=s("span"),m(Tt.$$.fragment),Ta=c(),_n=s("span"),ba=n("ProphetNetModel"),ur=c(),x=s("div"),m(bt.$$.fragment),ya=c(),yt=s("p"),wa=n(`The bare ProphetNet Model outputting raw hidden-states without any specific head on top.
This model inherits from `),go=s("a"),Pa=n("PreTrainedModel"),Na=n(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),qa=c(),ve=s("p"),za=n("Original ProphetNet code can be found "),wt=s("a"),$a=n("here"),Fa=n(`. Checkpoints were converted
from original Fairseq checkpoints. For more information on the checkpoint conversion, please take a look at the
file `),gn=s("code"),Ma=n("convert_prophetnet_original_pytorch_checkpoint_to_pytorch.py"),xa=n("."),Ea=c(),Pt=s("p"),Ca=n("This model is a PyTorch "),Nt=s("a"),Oa=n("torch.nn.Module"),Sa=n(` sub-class. Use
it as a regular PyTorch Module and refer to the PyTorch documentation for all matters related to general usage and
behavior.`),Da=c(),O=s("div"),m(qt.$$.fragment),La=c(),ke=s("p"),ja=n("The "),vo=s("a"),Aa=n("ProphetNetModel"),Ia=n(" forward method, overrides the "),vn=s("code"),Wa=n("__call__"),Ba=n(" special method."),Ga=c(),m(We.$$.fragment),Ha=c(),kn=s("p"),Va=n("Example:"),Ua=c(),m(zt.$$.fragment),mr=c(),Te=s("h2"),Be=s("a"),Tn=s("span"),m($t.$$.fragment),Ra=c(),bn=s("span"),Ja=n("ProphetNetEncoder"),fr=c(),F=s("div"),m(Ft.$$.fragment),Ya=c(),Mt=s("p"),Ka=n(`The standalone encoder part of the ProphetNetModel.
This model inherits from `),ko=s("a"),Qa=n("PreTrainedModel"),Xa=n(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Za=c(),be=s("p"),ed=n("Original ProphetNet code can be found "),xt=s("a"),td=n("here"),od=n(`. Checkpoints were converted
from original Fairseq checkpoints. For more information on the checkpoint conversion, please take a look at the
file `),yn=s("code"),nd=n("convert_prophetnet_original_pytorch_checkpoint_to_pytorch.py"),rd=n("."),sd=c(),Et=s("p"),ad=n("This model is a PyTorch "),Ct=s("a"),dd=n("torch.nn.Module"),id=n(` sub-class. Use
it as a regular PyTorch Module and refer to the PyTorch documentation for all matters related to general usage and
behavior.`),cd=c(),I=s("p"),ld=n("word_embeddings  ("),wn=s("code"),hd=n("torch.nn.Embeddings"),pd=n(" of shape "),Pn=s("code"),ud=n("(config.vocab_size, config.hidden_size)"),md=n(", "),Nn=s("em"),fd=n("optional"),_d=n(`):
The word embedding parameters. This can be used to initialize `),To=s("a"),gd=n("ProphetNetEncoder"),vd=n(` with pre-defined word
embeddings instead of randomly initialized word embeddings.`),kd=c(),S=s("div"),m(Ot.$$.fragment),Td=c(),ye=s("p"),bd=n("The "),bo=s("a"),yd=n("ProphetNetEncoder"),wd=n(" forward method, overrides the "),qn=s("code"),Pd=n("__call__"),Nd=n(" special method."),qd=c(),m(Ge.$$.fragment),zd=c(),zn=s("p"),$d=n("Example:"),Fd=c(),m(St.$$.fragment),_r=c(),we=s("h2"),He=s("a"),$n=s("span"),m(Dt.$$.fragment),Md=c(),Fn=s("span"),xd=n("ProphetNetDecoder"),gr=c(),M=s("div"),m(Lt.$$.fragment),Ed=c(),jt=s("p"),Cd=n(`The standalone decoder part of the ProphetNetModel.
This model inherits from `),yo=s("a"),Od=n("PreTrainedModel"),Sd=n(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Dd=c(),Pe=s("p"),Ld=n("Original ProphetNet code can be found "),At=s("a"),jd=n("here"),Ad=n(`. Checkpoints were converted
from original Fairseq checkpoints. For more information on the checkpoint conversion, please take a look at the
file `),Mn=s("code"),Id=n("convert_prophetnet_original_pytorch_checkpoint_to_pytorch.py"),Wd=n("."),Bd=c(),It=s("p"),Gd=n("This model is a PyTorch "),Wt=s("a"),Hd=n("torch.nn.Module"),Vd=n(` sub-class. Use
it as a regular PyTorch Module and refer to the PyTorch documentation for all matters related to general usage and
behavior.`),Ud=c(),W=s("p"),Rd=n("word_embeddings  ("),xn=s("code"),Jd=n("torch.nn.Embeddings"),Yd=n(" of shape "),En=s("code"),Kd=n("(config.vocab_size, config.hidden_size)"),Qd=n(", "),Cn=s("em"),Xd=n("optional"),Zd=n(`):
The word embedding parameters. This can be used to initialize `),wo=s("a"),ei=n("ProphetNetEncoder"),ti=n(` with pre-defined word
embeddings instead of randomly initialized word embeddings.`),oi=c(),D=s("div"),m(Bt.$$.fragment),ni=c(),Ne=s("p"),ri=n("The "),Po=s("a"),si=n("ProphetNetDecoder"),ai=n(" forward method, overrides the "),On=s("code"),di=n("__call__"),ii=n(" special method."),ci=c(),m(Ve.$$.fragment),li=c(),Sn=s("p"),hi=n("Example:"),pi=c(),m(Gt.$$.fragment),vr=c(),qe=s("h2"),Ue=s("a"),Dn=s("span"),m(Ht.$$.fragment),ui=c(),Ln=s("span"),mi=n("ProphetNetForConditionalGeneration"),kr=c(),E=s("div"),m(Vt.$$.fragment),fi=c(),Ut=s("p"),_i=n(`The ProphetNet Model with a language modeling head. Can be used for sequence generation tasks.
This model inherits from `),No=s("a"),gi=n("PreTrainedModel"),vi=n(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),ki=c(),ze=s("p"),Ti=n("Original ProphetNet code can be found "),Rt=s("a"),bi=n("here"),yi=n(`. Checkpoints were converted
from original Fairseq checkpoints. For more information on the checkpoint conversion, please take a look at the
file `),jn=s("code"),wi=n("convert_prophetnet_original_pytorch_checkpoint_to_pytorch.py"),Pi=n("."),Ni=c(),Jt=s("p"),qi=n("This model is a PyTorch "),Yt=s("a"),zi=n("torch.nn.Module"),$i=n(` sub-class. Use
it as a regular PyTorch Module and refer to the PyTorch documentation for all matters related to general usage and
behavior.`),Fi=c(),L=s("div"),m(Kt.$$.fragment),Mi=c(),$e=s("p"),xi=n("The "),qo=s("a"),Ei=n("ProphetNetForConditionalGeneration"),Ci=n(" forward method, overrides the "),An=s("code"),Oi=n("__call__"),Si=n(" special method."),Di=c(),m(Re.$$.fragment),Li=c(),In=s("p"),ji=n("Example:"),Ai=c(),m(Qt.$$.fragment),Tr=c(),Fe=s("h2"),Je=s("a"),Wn=s("span"),m(Xt.$$.fragment),Ii=c(),Bn=s("span"),Wi=n("ProphetNetForCausalLM"),br=c(),C=s("div"),m(Zt.$$.fragment),Bi=c(),eo=s("p"),Gi=n(`The standalone decoder part of the ProphetNetModel with a lm head on top. The model can be used for causal language modeling.
This model inherits from `),zo=s("a"),Hi=n("PreTrainedModel"),Vi=n(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Ui=c(),Me=s("p"),Ri=n("Original ProphetNet code can be found "),to=s("a"),Ji=n("here"),Yi=n(`. Checkpoints were converted
from original Fairseq checkpoints. For more information on the checkpoint conversion, please take a look at the
file `),Gn=s("code"),Ki=n("convert_prophetnet_original_pytorch_checkpoint_to_pytorch.py"),Qi=n("."),Xi=c(),oo=s("p"),Zi=n("This model is a PyTorch "),no=s("a"),ec=n("torch.nn.Module"),tc=n(` sub-class. Use
it as a regular PyTorch Module and refer to the PyTorch documentation for all matters related to general usage and
behavior.`),oc=c(),j=s("div"),m(ro.$$.fragment),nc=c(),xe=s("p"),rc=n("The "),$o=s("a"),sc=n("ProphetNetForCausalLM"),ac=n(" forward method, overrides the "),Hn=s("code"),dc=n("__call__"),ic=n(" special method."),cc=c(),m(Ye.$$.fragment),lc=c(),Vn=s("p"),hc=n("Example:"),pc=c(),m(so.$$.fragment),this.h()},l(t){const h=ah('[data-svelte="svelte-1phssyn"]',document.head);u=a(h,"META",{name:!0,content:!0}),h.forEach(o),N=l(t),T=a(t,"H1",{class:!0});var ao=d(T);w=a(ao,"A",{id:!0,class:!0,href:!0});var Un=d(w);P=a(Un,"SPAN",{});var Rn=d(P);f(y.$$.fragment,Rn),Rn.forEach(o),Un.forEach(o),b=l(ao),q=a(ao,"SPAN",{});var Jn=d(q);Zr=r(Jn,"ProphetNet"),Jn.forEach(o),ao.forEach(o),Kn=l(t),se=a(t,"P",{});var Ke=d(se);Vo=a(Ke,"STRONG",{});var fc=d(Vo);es=r(fc,"DISCLAIMER:"),fc.forEach(o),ts=r(Ke," If you see something strange, file a "),Xe=a(Ke,"A",{href:!0,rel:!0});var _c=d(Xe);os=r(_c,"Github Issue"),_c.forEach(o),ns=r(Ke,` and assign
@patrickvonplaten`),Ke.forEach(o),Qn=l(t),ae=a(t,"H2",{class:!0});var wr=d(ae);Ee=a(wr,"A",{id:!0,class:!0,href:!0});var gc=d(Ee);Uo=a(gc,"SPAN",{});var vc=d(Uo);f(Ze.$$.fragment,vc),vc.forEach(o),gc.forEach(o),rs=l(wr),Ro=a(wr,"SPAN",{});var kc=d(Ro);ss=r(kc,"Overview"),kc.forEach(o),wr.forEach(o),Xn=l(t),Ce=a(t,"P",{});var Pr=d(Ce);as=r(Pr,"The ProphetNet model was proposed in "),et=a(Pr,"A",{href:!0,rel:!0});var Tc=d(et);ds=r(Tc,"ProphetNet: Predicting Future N-gram for Sequence-to-Sequence Pre-training,"),Tc.forEach(o),is=r(Pr,` by Yu Yan, Weizhen Qi, Yeyun Gong, Dayiheng Liu, Nan Duan, Jiusheng Chen, Ruofei
Zhang, Ming Zhou on 13 Jan, 2020.`),Pr.forEach(o),Zn=l(t),io=a(t,"P",{});var bc=d(io);cs=r(bc,`ProphetNet is an encoder-decoder model and can predict n-future tokens for \u201Cngram\u201D language modeling instead of just
the next token.`),bc.forEach(o),er=l(t),co=a(t,"P",{});var yc=d(co);ls=r(yc,"The abstract from the paper is the following:"),yc.forEach(o),tr=l(t),lo=a(t,"P",{});var wc=d(lo);Jo=a(wc,"EM",{});var Pc=d(Jo);hs=r(Pc,`In this paper, we present a new sequence-to-sequence pretraining model called ProphetNet, which introduces a novel
self-supervised objective named future n-gram prediction and the proposed n-stream self-attention mechanism. Instead of
the optimization of one-step ahead prediction in traditional sequence-to-sequence model, the ProphetNet is optimized by
n-step ahead prediction which predicts the next n tokens simultaneously based on previous context tokens at each time
step. The future n-gram prediction explicitly encourages the model to plan for the future tokens and prevent
overfitting on strong local correlations. We pre-train ProphetNet using a base scale dataset (16GB) and a large scale
dataset (160GB) respectively. Then we conduct experiments on CNN/DailyMail, Gigaword, and SQuAD 1.1 benchmarks for
abstractive summarization and question generation tasks. Experimental results show that ProphetNet achieves new
state-of-the-art results on all these datasets compared to the models using the same scale pretraining corpus.`),Pc.forEach(o),wc.forEach(o),or=l(t),Oe=a(t,"P",{});var Nr=d(Oe);ps=r(Nr,"The Authors\u2019 code can be found "),tt=a(Nr,"A",{href:!0,rel:!0});var Nc=d(tt);us=r(Nc,"here"),Nc.forEach(o),ms=r(Nr,"."),Nr.forEach(o),nr=l(t),de=a(t,"H2",{class:!0});var qr=d(de);Se=a(qr,"A",{id:!0,class:!0,href:!0});var qc=d(Se);Yo=a(qc,"SPAN",{});var zc=d(Yo);f(ot.$$.fragment,zc),zc.forEach(o),qc.forEach(o),fs=l(qr),Ko=a(qr,"SPAN",{});var $c=d(Ko);_s=r($c,"ProphetNetConfig"),$c.forEach(o),qr.forEach(o),rr=l(t),U=a(t,"DIV",{class:!0});var Fo=d(U);f(nt.$$.fragment,Fo),gs=l(Fo),ie=a(Fo,"P",{});var Mo=d(ie);vs=r(Mo,"This is the configuration class to store the configuration of a "),ho=a(Mo,"A",{href:!0});var Fc=d(ho);ks=r(Fc,"ProphetNetModel"),Fc.forEach(o),Ts=r(Mo,`. It is used to instantiate a
ProphetNet model according to the specified arguments, defining the model architecture. Instantiating a
configuration with the defaults will yield a similar configuration to that of the ProphetNet
`),rt=a(Mo,"A",{href:!0,rel:!0});var Mc=d(rt);bs=r(Mc,"microsoft/prophetnet-large-uncased"),Mc.forEach(o),ys=r(Mo," architecture."),Mo.forEach(o),ws=l(Fo),ce=a(Fo,"P",{});var xo=d(ce);Ps=r(xo,"Configuration objects inherit from "),po=a(xo,"A",{href:!0});var xc=d(po);Ns=r(xc,"PretrainedConfig"),xc.forEach(o),qs=r(xo,` and can be used to control the model outputs. Read the
documentation from `),uo=a(xo,"A",{href:!0});var Ec=d(uo);zs=r(Ec,"PretrainedConfig"),Ec.forEach(o),$s=r(xo," for more information."),xo.forEach(o),Fo.forEach(o),sr=l(t),le=a(t,"H2",{class:!0});var zr=d(le);De=a(zr,"A",{id:!0,class:!0,href:!0});var Cc=d(De);Qo=a(Cc,"SPAN",{});var Oc=d(Qo);f(st.$$.fragment,Oc),Oc.forEach(o),Cc.forEach(o),Fs=l(zr),Xo=a(zr,"SPAN",{});var Sc=d(Xo);Ms=r(Sc,"ProphetNetTokenizer"),Sc.forEach(o),zr.forEach(o),ar=l(t),$=a(t,"DIV",{class:!0});var A=d($);f(at.$$.fragment,A),xs=l(A),Zo=a(A,"P",{});var Dc=d(Zo);Es=r(Dc,"Construct a ProphetNetTokenizer. Based on WordPiece."),Dc.forEach(o),Cs=l(A),dt=a(A,"P",{});var $r=d(dt);Os=r($r,"This tokenizer inherits from "),mo=a($r,"A",{href:!0});var Lc=d(mo);Ss=r(Lc,"PreTrainedTokenizer"),Lc.forEach(o),Ds=r($r,` which contains most of the main methods. Users should refer to
this superclass for more information regarding those methods.`),$r.forEach(o),Ls=l(A),R=a(A,"DIV",{class:!0});var Eo=d(R);f(it.$$.fragment,Eo),js=l(Eo),en=a(Eo,"P",{});var jc=d(en);As=r(jc,`Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens. A BERT sequence has the following format:`),jc.forEach(o),Is=l(Eo),ct=a(Eo,"UL",{});var Fr=d(ct);fo=a(Fr,"LI",{});var uc=d(fo);Ws=r(uc,"single sequence: "),tn=a(uc,"CODE",{});var Ac=d(tn);Bs=r(Ac,"[CLS] X [SEP]"),Ac.forEach(o),uc.forEach(o),Gs=l(Fr),_o=a(Fr,"LI",{});var mc=d(_o);Hs=r(mc,"pair of sequences: "),on=a(mc,"CODE",{});var Ic=d(on);Vs=r(Ic,"[CLS] A [SEP] B [SEP]"),Ic.forEach(o),mc.forEach(o),Fr.forEach(o),Eo.forEach(o),Us=l(A),Le=a(A,"DIV",{class:!0});var Mr=d(Le);f(lt.$$.fragment,Mr),Rs=l(Mr),nn=a(Mr,"P",{});var Wc=d(nn);Js=r(Wc,"Converts a sequence of tokens (string) in a single string."),Wc.forEach(o),Mr.forEach(o),Ys=l(A),G=a(A,"DIV",{class:!0});var Qe=d(G);f(ht.$$.fragment,Qe),Ks=l(Qe),rn=a(Qe,"P",{});var Bc=d(rn);Qs=r(Bc,`Create a mask from the two sequences passed to be used in a sequence-pair classification task. A ProphetNet
sequence pair mask has the following format:`),Bc.forEach(o),Xs=l(Qe),f(pt.$$.fragment,Qe),Zs=l(Qe),he=a(Qe,"P",{});var Co=d(he);ea=r(Co,"If "),sn=a(Co,"CODE",{});var Gc=d(sn);ta=r(Gc,"token_ids_1"),Gc.forEach(o),oa=r(Co," is "),an=a(Co,"CODE",{});var Hc=d(an);na=r(Hc,"None"),Hc.forEach(o),ra=r(Co,", this method only returns the first portion of the mask (0s)."),Co.forEach(o),Qe.forEach(o),sa=l(A),je=a(A,"DIV",{class:!0});var xr=d(je);f(ut.$$.fragment,xr),aa=l(xr),mt=a(xr,"P",{});var Er=d(mt);da=r(Er,`Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding
special tokens using the tokenizer `),dn=a(Er,"CODE",{});var Vc=d(dn);ia=r(Vc,"prepare_for_model"),Vc.forEach(o),ca=r(Er," method."),Er.forEach(o),xr.forEach(o),A.forEach(o),dr=l(t),pe=a(t,"H2",{class:!0});var Cr=d(pe);Ae=a(Cr,"A",{id:!0,class:!0,href:!0});var Uc=d(Ae);cn=a(Uc,"SPAN",{});var Rc=d(cn);f(ft.$$.fragment,Rc),Rc.forEach(o),Uc.forEach(o),la=l(Cr),ln=a(Cr,"SPAN",{});var Jc=d(ln);ha=r(Jc,"ProphetNet specific outputs"),Jc.forEach(o),Cr.forEach(o),ir=l(t),ue=a(t,"DIV",{class:!0});var Or=d(ue);f(_t.$$.fragment,Or),pa=l(Or),hn=a(Or,"P",{});var Yc=d(hn);ua=r(Yc,"Base class for sequence-to-sequence language models outputs."),Yc.forEach(o),Or.forEach(o),cr=l(t),me=a(t,"DIV",{class:!0});var Sr=d(me);f(gt.$$.fragment,Sr),ma=l(Sr),pn=a(Sr,"P",{});var Kc=d(pn);fa=r(Kc,`Base class for model encoder\u2019s outputs that also contains : pre-computed hidden states that can speed up sequential
decoding.`),Kc.forEach(o),Sr.forEach(o),lr=l(t),fe=a(t,"DIV",{class:!0});var Dr=d(fe);f(vt.$$.fragment,Dr),_a=l(Dr),un=a(Dr,"P",{});var Qc=d(un);ga=r(Qc,"Base class for model\u2019s outputs that may also contain a past key/values (to speed up sequential decoding)."),Qc.forEach(o),Dr.forEach(o),hr=l(t),_e=a(t,"DIV",{class:!0});var Lr=d(_e);f(kt.$$.fragment,Lr),va=l(Lr),mn=a(Lr,"P",{});var Xc=d(mn);ka=r(Xc,"Base class for model\u2019s outputs that may also contain a past key/values (to speed up sequential decoding)."),Xc.forEach(o),Lr.forEach(o),pr=l(t),ge=a(t,"H2",{class:!0});var jr=d(ge);Ie=a(jr,"A",{id:!0,class:!0,href:!0});var Zc=d(Ie);fn=a(Zc,"SPAN",{});var el=d(fn);f(Tt.$$.fragment,el),el.forEach(o),Zc.forEach(o),Ta=l(jr),_n=a(jr,"SPAN",{});var tl=d(_n);ba=r(tl,"ProphetNetModel"),tl.forEach(o),jr.forEach(o),ur=l(t),x=a(t,"DIV",{class:!0});var J=d(x);f(bt.$$.fragment,J),ya=l(J),yt=a(J,"P",{});var Ar=d(yt);wa=r(Ar,`The bare ProphetNet Model outputting raw hidden-states without any specific head on top.
This model inherits from `),go=a(Ar,"A",{href:!0});var ol=d(go);Pa=r(ol,"PreTrainedModel"),ol.forEach(o),Na=r(Ar,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Ar.forEach(o),qa=l(J),ve=a(J,"P",{});var Oo=d(ve);za=r(Oo,"Original ProphetNet code can be found "),wt=a(Oo,"A",{href:!0,rel:!0});var nl=d(wt);$a=r(nl,"here"),nl.forEach(o),Fa=r(Oo,`. Checkpoints were converted
from original Fairseq checkpoints. For more information on the checkpoint conversion, please take a look at the
file `),gn=a(Oo,"CODE",{});var rl=d(gn);Ma=r(rl,"convert_prophetnet_original_pytorch_checkpoint_to_pytorch.py"),rl.forEach(o),xa=r(Oo,"."),Oo.forEach(o),Ea=l(J),Pt=a(J,"P",{});var Ir=d(Pt);Ca=r(Ir,"This model is a PyTorch "),Nt=a(Ir,"A",{href:!0,rel:!0});var sl=d(Nt);Oa=r(sl,"torch.nn.Module"),sl.forEach(o),Sa=r(Ir,` sub-class. Use
it as a regular PyTorch Module and refer to the PyTorch documentation for all matters related to general usage and
behavior.`),Ir.forEach(o),Da=l(J),O=a(J,"DIV",{class:!0});var Y=d(O);f(qt.$$.fragment,Y),La=l(Y),ke=a(Y,"P",{});var So=d(ke);ja=r(So,"The "),vo=a(So,"A",{href:!0});var al=d(vo);Aa=r(al,"ProphetNetModel"),al.forEach(o),Ia=r(So," forward method, overrides the "),vn=a(So,"CODE",{});var dl=d(vn);Wa=r(dl,"__call__"),dl.forEach(o),Ba=r(So," special method."),So.forEach(o),Ga=l(Y),f(We.$$.fragment,Y),Ha=l(Y),kn=a(Y,"P",{});var il=d(kn);Va=r(il,"Example:"),il.forEach(o),Ua=l(Y),f(zt.$$.fragment,Y),Y.forEach(o),J.forEach(o),mr=l(t),Te=a(t,"H2",{class:!0});var Wr=d(Te);Be=a(Wr,"A",{id:!0,class:!0,href:!0});var cl=d(Be);Tn=a(cl,"SPAN",{});var ll=d(Tn);f($t.$$.fragment,ll),ll.forEach(o),cl.forEach(o),Ra=l(Wr),bn=a(Wr,"SPAN",{});var hl=d(bn);Ja=r(hl,"ProphetNetEncoder"),hl.forEach(o),Wr.forEach(o),fr=l(t),F=a(t,"DIV",{class:!0});var H=d(F);f(Ft.$$.fragment,H),Ya=l(H),Mt=a(H,"P",{});var Br=d(Mt);Ka=r(Br,`The standalone encoder part of the ProphetNetModel.
This model inherits from `),ko=a(Br,"A",{href:!0});var pl=d(ko);Qa=r(pl,"PreTrainedModel"),pl.forEach(o),Xa=r(Br,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Br.forEach(o),Za=l(H),be=a(H,"P",{});var Do=d(be);ed=r(Do,"Original ProphetNet code can be found "),xt=a(Do,"A",{href:!0,rel:!0});var ul=d(xt);td=r(ul,"here"),ul.forEach(o),od=r(Do,`. Checkpoints were converted
from original Fairseq checkpoints. For more information on the checkpoint conversion, please take a look at the
file `),yn=a(Do,"CODE",{});var ml=d(yn);nd=r(ml,"convert_prophetnet_original_pytorch_checkpoint_to_pytorch.py"),ml.forEach(o),rd=r(Do,"."),Do.forEach(o),sd=l(H),Et=a(H,"P",{});var Gr=d(Et);ad=r(Gr,"This model is a PyTorch "),Ct=a(Gr,"A",{href:!0,rel:!0});var fl=d(Ct);dd=r(fl,"torch.nn.Module"),fl.forEach(o),id=r(Gr,` sub-class. Use
it as a regular PyTorch Module and refer to the PyTorch documentation for all matters related to general usage and
behavior.`),Gr.forEach(o),cd=l(H),I=a(H,"P",{});var K=d(I);ld=r(K,"word_embeddings  ("),wn=a(K,"CODE",{});var _l=d(wn);hd=r(_l,"torch.nn.Embeddings"),_l.forEach(o),pd=r(K," of shape "),Pn=a(K,"CODE",{});var gl=d(Pn);ud=r(gl,"(config.vocab_size, config.hidden_size)"),gl.forEach(o),md=r(K,", "),Nn=a(K,"EM",{});var vl=d(Nn);fd=r(vl,"optional"),vl.forEach(o),_d=r(K,`):
The word embedding parameters. This can be used to initialize `),To=a(K,"A",{href:!0});var kl=d(To);gd=r(kl,"ProphetNetEncoder"),kl.forEach(o),vd=r(K,` with pre-defined word
embeddings instead of randomly initialized word embeddings.`),K.forEach(o),kd=l(H),S=a(H,"DIV",{class:!0});var Q=d(S);f(Ot.$$.fragment,Q),Td=l(Q),ye=a(Q,"P",{});var Lo=d(ye);bd=r(Lo,"The "),bo=a(Lo,"A",{href:!0});var Tl=d(bo);yd=r(Tl,"ProphetNetEncoder"),Tl.forEach(o),wd=r(Lo," forward method, overrides the "),qn=a(Lo,"CODE",{});var bl=d(qn);Pd=r(bl,"__call__"),bl.forEach(o),Nd=r(Lo," special method."),Lo.forEach(o),qd=l(Q),f(Ge.$$.fragment,Q),zd=l(Q),zn=a(Q,"P",{});var yl=d(zn);$d=r(yl,"Example:"),yl.forEach(o),Fd=l(Q),f(St.$$.fragment,Q),Q.forEach(o),H.forEach(o),_r=l(t),we=a(t,"H2",{class:!0});var Hr=d(we);He=a(Hr,"A",{id:!0,class:!0,href:!0});var wl=d(He);$n=a(wl,"SPAN",{});var Pl=d($n);f(Dt.$$.fragment,Pl),Pl.forEach(o),wl.forEach(o),Md=l(Hr),Fn=a(Hr,"SPAN",{});var Nl=d(Fn);xd=r(Nl,"ProphetNetDecoder"),Nl.forEach(o),Hr.forEach(o),gr=l(t),M=a(t,"DIV",{class:!0});var V=d(M);f(Lt.$$.fragment,V),Ed=l(V),jt=a(V,"P",{});var Vr=d(jt);Cd=r(Vr,`The standalone decoder part of the ProphetNetModel.
This model inherits from `),yo=a(Vr,"A",{href:!0});var ql=d(yo);Od=r(ql,"PreTrainedModel"),ql.forEach(o),Sd=r(Vr,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Vr.forEach(o),Dd=l(V),Pe=a(V,"P",{});var jo=d(Pe);Ld=r(jo,"Original ProphetNet code can be found "),At=a(jo,"A",{href:!0,rel:!0});var zl=d(At);jd=r(zl,"here"),zl.forEach(o),Ad=r(jo,`. Checkpoints were converted
from original Fairseq checkpoints. For more information on the checkpoint conversion, please take a look at the
file `),Mn=a(jo,"CODE",{});var $l=d(Mn);Id=r($l,"convert_prophetnet_original_pytorch_checkpoint_to_pytorch.py"),$l.forEach(o),Wd=r(jo,"."),jo.forEach(o),Bd=l(V),It=a(V,"P",{});var Ur=d(It);Gd=r(Ur,"This model is a PyTorch "),Wt=a(Ur,"A",{href:!0,rel:!0});var Fl=d(Wt);Hd=r(Fl,"torch.nn.Module"),Fl.forEach(o),Vd=r(Ur,` sub-class. Use
it as a regular PyTorch Module and refer to the PyTorch documentation for all matters related to general usage and
behavior.`),Ur.forEach(o),Ud=l(V),W=a(V,"P",{});var X=d(W);Rd=r(X,"word_embeddings  ("),xn=a(X,"CODE",{});var Ml=d(xn);Jd=r(Ml,"torch.nn.Embeddings"),Ml.forEach(o),Yd=r(X," of shape "),En=a(X,"CODE",{});var xl=d(En);Kd=r(xl,"(config.vocab_size, config.hidden_size)"),xl.forEach(o),Qd=r(X,", "),Cn=a(X,"EM",{});var El=d(Cn);Xd=r(El,"optional"),El.forEach(o),Zd=r(X,`):
The word embedding parameters. This can be used to initialize `),wo=a(X,"A",{href:!0});var Cl=d(wo);ei=r(Cl,"ProphetNetEncoder"),Cl.forEach(o),ti=r(X,` with pre-defined word
embeddings instead of randomly initialized word embeddings.`),X.forEach(o),oi=l(V),D=a(V,"DIV",{class:!0});var Z=d(D);f(Bt.$$.fragment,Z),ni=l(Z),Ne=a(Z,"P",{});var Ao=d(Ne);ri=r(Ao,"The "),Po=a(Ao,"A",{href:!0});var Ol=d(Po);si=r(Ol,"ProphetNetDecoder"),Ol.forEach(o),ai=r(Ao," forward method, overrides the "),On=a(Ao,"CODE",{});var Sl=d(On);di=r(Sl,"__call__"),Sl.forEach(o),ii=r(Ao," special method."),Ao.forEach(o),ci=l(Z),f(Ve.$$.fragment,Z),li=l(Z),Sn=a(Z,"P",{});var Dl=d(Sn);hi=r(Dl,"Example:"),Dl.forEach(o),pi=l(Z),f(Gt.$$.fragment,Z),Z.forEach(o),V.forEach(o),vr=l(t),qe=a(t,"H2",{class:!0});var Rr=d(qe);Ue=a(Rr,"A",{id:!0,class:!0,href:!0});var Ll=d(Ue);Dn=a(Ll,"SPAN",{});var jl=d(Dn);f(Ht.$$.fragment,jl),jl.forEach(o),Ll.forEach(o),ui=l(Rr),Ln=a(Rr,"SPAN",{});var Al=d(Ln);mi=r(Al,"ProphetNetForConditionalGeneration"),Al.forEach(o),Rr.forEach(o),kr=l(t),E=a(t,"DIV",{class:!0});var ee=d(E);f(Vt.$$.fragment,ee),fi=l(ee),Ut=a(ee,"P",{});var Jr=d(Ut);_i=r(Jr,`The ProphetNet Model with a language modeling head. Can be used for sequence generation tasks.
This model inherits from `),No=a(Jr,"A",{href:!0});var Il=d(No);gi=r(Il,"PreTrainedModel"),Il.forEach(o),vi=r(Jr,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Jr.forEach(o),ki=l(ee),ze=a(ee,"P",{});var Io=d(ze);Ti=r(Io,"Original ProphetNet code can be found "),Rt=a(Io,"A",{href:!0,rel:!0});var Wl=d(Rt);bi=r(Wl,"here"),Wl.forEach(o),yi=r(Io,`. Checkpoints were converted
from original Fairseq checkpoints. For more information on the checkpoint conversion, please take a look at the
file `),jn=a(Io,"CODE",{});var Bl=d(jn);wi=r(Bl,"convert_prophetnet_original_pytorch_checkpoint_to_pytorch.py"),Bl.forEach(o),Pi=r(Io,"."),Io.forEach(o),Ni=l(ee),Jt=a(ee,"P",{});var Yr=d(Jt);qi=r(Yr,"This model is a PyTorch "),Yt=a(Yr,"A",{href:!0,rel:!0});var Gl=d(Yt);zi=r(Gl,"torch.nn.Module"),Gl.forEach(o),$i=r(Yr,` sub-class. Use
it as a regular PyTorch Module and refer to the PyTorch documentation for all matters related to general usage and
behavior.`),Yr.forEach(o),Fi=l(ee),L=a(ee,"DIV",{class:!0});var te=d(L);f(Kt.$$.fragment,te),Mi=l(te),$e=a(te,"P",{});var Wo=d($e);xi=r(Wo,"The "),qo=a(Wo,"A",{href:!0});var Hl=d(qo);Ei=r(Hl,"ProphetNetForConditionalGeneration"),Hl.forEach(o),Ci=r(Wo," forward method, overrides the "),An=a(Wo,"CODE",{});var Vl=d(An);Oi=r(Vl,"__call__"),Vl.forEach(o),Si=r(Wo," special method."),Wo.forEach(o),Di=l(te),f(Re.$$.fragment,te),Li=l(te),In=a(te,"P",{});var Ul=d(In);ji=r(Ul,"Example:"),Ul.forEach(o),Ai=l(te),f(Qt.$$.fragment,te),te.forEach(o),ee.forEach(o),Tr=l(t),Fe=a(t,"H2",{class:!0});var Kr=d(Fe);Je=a(Kr,"A",{id:!0,class:!0,href:!0});var Rl=d(Je);Wn=a(Rl,"SPAN",{});var Jl=d(Wn);f(Xt.$$.fragment,Jl),Jl.forEach(o),Rl.forEach(o),Ii=l(Kr),Bn=a(Kr,"SPAN",{});var Yl=d(Bn);Wi=r(Yl,"ProphetNetForCausalLM"),Yl.forEach(o),Kr.forEach(o),br=l(t),C=a(t,"DIV",{class:!0});var oe=d(C);f(Zt.$$.fragment,oe),Bi=l(oe),eo=a(oe,"P",{});var Qr=d(eo);Gi=r(Qr,`The standalone decoder part of the ProphetNetModel with a lm head on top. The model can be used for causal language modeling.
This model inherits from `),zo=a(Qr,"A",{href:!0});var Kl=d(zo);Hi=r(Kl,"PreTrainedModel"),Kl.forEach(o),Vi=r(Qr,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Qr.forEach(o),Ui=l(oe),Me=a(oe,"P",{});var Bo=d(Me);Ri=r(Bo,"Original ProphetNet code can be found "),to=a(Bo,"A",{href:!0,rel:!0});var Ql=d(to);Ji=r(Ql,"here"),Ql.forEach(o),Yi=r(Bo,`. Checkpoints were converted
from original Fairseq checkpoints. For more information on the checkpoint conversion, please take a look at the
file `),Gn=a(Bo,"CODE",{});var Xl=d(Gn);Ki=r(Xl,"convert_prophetnet_original_pytorch_checkpoint_to_pytorch.py"),Xl.forEach(o),Qi=r(Bo,"."),Bo.forEach(o),Xi=l(oe),oo=a(oe,"P",{});var Xr=d(oo);Zi=r(Xr,"This model is a PyTorch "),no=a(Xr,"A",{href:!0,rel:!0});var Zl=d(no);ec=r(Zl,"torch.nn.Module"),Zl.forEach(o),tc=r(Xr,` sub-class. Use
it as a regular PyTorch Module and refer to the PyTorch documentation for all matters related to general usage and
behavior.`),Xr.forEach(o),oc=l(oe),j=a(oe,"DIV",{class:!0});var ne=d(j);f(ro.$$.fragment,ne),nc=l(ne),xe=a(ne,"P",{});var Go=d(xe);rc=r(Go,"The "),$o=a(Go,"A",{href:!0});var eh=d($o);sc=r(eh,"ProphetNetForCausalLM"),eh.forEach(o),ac=r(Go," forward method, overrides the "),Hn=a(Go,"CODE",{});var th=d(Hn);dc=r(th,"__call__"),th.forEach(o),ic=r(Go," special method."),Go.forEach(o),cc=l(ne),f(Ye.$$.fragment,ne),lc=l(ne),Vn=a(ne,"P",{});var oh=d(Vn);hc=r(oh,"Example:"),oh.forEach(o),pc=l(ne),f(so.$$.fragment,ne),ne.forEach(o),oe.forEach(o),this.h()},h(){i(u,"name","hf:doc:metadata"),i(u,"content",JSON.stringify(mh)),i(w,"id","prophetnet"),i(w,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(w,"href","#prophetnet"),i(T,"class","relative group"),i(Xe,"href","https://github.com/huggingface/transformers/issues/new?assignees=&labels=&template=bug-report.md&title"),i(Xe,"rel","nofollow"),i(Ee,"id","overview"),i(Ee,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(Ee,"href","#overview"),i(ae,"class","relative group"),i(et,"href","https://arxiv.org/abs/2001.04063"),i(et,"rel","nofollow"),i(tt,"href","https://github.com/microsoft/ProphetNet"),i(tt,"rel","nofollow"),i(Se,"id","transformers.ProphetNetConfig"),i(Se,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(Se,"href","#transformers.ProphetNetConfig"),i(de,"class","relative group"),i(ho,"href","/docs/transformers/pr_16941/en/model_doc/prophetnet#transformers.ProphetNetModel"),i(rt,"href","https://huggingface.co/microsoft/prophetnet-large-uncased"),i(rt,"rel","nofollow"),i(po,"href","/docs/transformers/pr_16941/en/main_classes/configuration#transformers.PretrainedConfig"),i(uo,"href","/docs/transformers/pr_16941/en/main_classes/configuration#transformers.PretrainedConfig"),i(U,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),i(De,"id","transformers.ProphetNetTokenizer"),i(De,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(De,"href","#transformers.ProphetNetTokenizer"),i(le,"class","relative group"),i(mo,"href","/docs/transformers/pr_16941/en/main_classes/tokenizer#transformers.PreTrainedTokenizer"),i(R,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),i(Le,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),i(G,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),i(je,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),i($,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),i(Ae,"id","transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqLMOutput"),i(Ae,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(Ae,"href","#transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqLMOutput"),i(pe,"class","relative group"),i(ue,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),i(me,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),i(fe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),i(_e,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),i(Ie,"id","transformers.ProphetNetModel"),i(Ie,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(Ie,"href","#transformers.ProphetNetModel"),i(ge,"class","relative group"),i(go,"href","/docs/transformers/pr_16941/en/main_classes/model#transformers.PreTrainedModel"),i(wt,"href","https://github.com/microsoft/ProphetNet"),i(wt,"rel","nofollow"),i(Nt,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),i(Nt,"rel","nofollow"),i(vo,"href","/docs/transformers/pr_16941/en/model_doc/prophetnet#transformers.ProphetNetModel"),i(O,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),i(x,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),i(Be,"id","transformers.ProphetNetEncoder"),i(Be,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(Be,"href","#transformers.ProphetNetEncoder"),i(Te,"class","relative group"),i(ko,"href","/docs/transformers/pr_16941/en/main_classes/model#transformers.PreTrainedModel"),i(xt,"href","https://github.com/microsoft/ProphetNet"),i(xt,"rel","nofollow"),i(Ct,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),i(Ct,"rel","nofollow"),i(To,"href","/docs/transformers/pr_16941/en/model_doc/prophetnet#transformers.ProphetNetEncoder"),i(bo,"href","/docs/transformers/pr_16941/en/model_doc/prophetnet#transformers.ProphetNetEncoder"),i(S,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),i(F,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),i(He,"id","transformers.ProphetNetDecoder"),i(He,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(He,"href","#transformers.ProphetNetDecoder"),i(we,"class","relative group"),i(yo,"href","/docs/transformers/pr_16941/en/main_classes/model#transformers.PreTrainedModel"),i(At,"href","https://github.com/microsoft/ProphetNet"),i(At,"rel","nofollow"),i(Wt,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),i(Wt,"rel","nofollow"),i(wo,"href","/docs/transformers/pr_16941/en/model_doc/prophetnet#transformers.ProphetNetEncoder"),i(Po,"href","/docs/transformers/pr_16941/en/model_doc/prophetnet#transformers.ProphetNetDecoder"),i(D,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),i(M,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),i(Ue,"id","transformers.ProphetNetForConditionalGeneration"),i(Ue,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(Ue,"href","#transformers.ProphetNetForConditionalGeneration"),i(qe,"class","relative group"),i(No,"href","/docs/transformers/pr_16941/en/main_classes/model#transformers.PreTrainedModel"),i(Rt,"href","https://github.com/microsoft/ProphetNet"),i(Rt,"rel","nofollow"),i(Yt,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),i(Yt,"rel","nofollow"),i(qo,"href","/docs/transformers/pr_16941/en/model_doc/prophetnet#transformers.ProphetNetForConditionalGeneration"),i(L,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),i(E,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),i(Je,"id","transformers.ProphetNetForCausalLM"),i(Je,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(Je,"href","#transformers.ProphetNetForCausalLM"),i(Fe,"class","relative group"),i(zo,"href","/docs/transformers/pr_16941/en/main_classes/model#transformers.PreTrainedModel"),i(to,"href","https://github.com/microsoft/ProphetNet"),i(to,"rel","nofollow"),i(no,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),i(no,"rel","nofollow"),i($o,"href","/docs/transformers/pr_16941/en/model_doc/prophetnet#transformers.ProphetNetForCausalLM"),i(j,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),i(C,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(t,h){e(document.head,u),p(t,N,h),p(t,T,h),e(T,w),e(w,P),_(y,P,null),e(T,b),e(T,q),e(q,Zr),p(t,Kn,h),p(t,se,h),e(se,Vo),e(Vo,es),e(se,ts),e(se,Xe),e(Xe,os),e(se,ns),p(t,Qn,h),p(t,ae,h),e(ae,Ee),e(Ee,Uo),_(Ze,Uo,null),e(ae,rs),e(ae,Ro),e(Ro,ss),p(t,Xn,h),p(t,Ce,h),e(Ce,as),e(Ce,et),e(et,ds),e(Ce,is),p(t,Zn,h),p(t,io,h),e(io,cs),p(t,er,h),p(t,co,h),e(co,ls),p(t,tr,h),p(t,lo,h),e(lo,Jo),e(Jo,hs),p(t,or,h),p(t,Oe,h),e(Oe,ps),e(Oe,tt),e(tt,us),e(Oe,ms),p(t,nr,h),p(t,de,h),e(de,Se),e(Se,Yo),_(ot,Yo,null),e(de,fs),e(de,Ko),e(Ko,_s),p(t,rr,h),p(t,U,h),_(nt,U,null),e(U,gs),e(U,ie),e(ie,vs),e(ie,ho),e(ho,ks),e(ie,Ts),e(ie,rt),e(rt,bs),e(ie,ys),e(U,ws),e(U,ce),e(ce,Ps),e(ce,po),e(po,Ns),e(ce,qs),e(ce,uo),e(uo,zs),e(ce,$s),p(t,sr,h),p(t,le,h),e(le,De),e(De,Qo),_(st,Qo,null),e(le,Fs),e(le,Xo),e(Xo,Ms),p(t,ar,h),p(t,$,h),_(at,$,null),e($,xs),e($,Zo),e(Zo,Es),e($,Cs),e($,dt),e(dt,Os),e(dt,mo),e(mo,Ss),e(dt,Ds),e($,Ls),e($,R),_(it,R,null),e(R,js),e(R,en),e(en,As),e(R,Is),e(R,ct),e(ct,fo),e(fo,Ws),e(fo,tn),e(tn,Bs),e(ct,Gs),e(ct,_o),e(_o,Hs),e(_o,on),e(on,Vs),e($,Us),e($,Le),_(lt,Le,null),e(Le,Rs),e(Le,nn),e(nn,Js),e($,Ys),e($,G),_(ht,G,null),e(G,Ks),e(G,rn),e(rn,Qs),e(G,Xs),_(pt,G,null),e(G,Zs),e(G,he),e(he,ea),e(he,sn),e(sn,ta),e(he,oa),e(he,an),e(an,na),e(he,ra),e($,sa),e($,je),_(ut,je,null),e(je,aa),e(je,mt),e(mt,da),e(mt,dn),e(dn,ia),e(mt,ca),p(t,dr,h),p(t,pe,h),e(pe,Ae),e(Ae,cn),_(ft,cn,null),e(pe,la),e(pe,ln),e(ln,ha),p(t,ir,h),p(t,ue,h),_(_t,ue,null),e(ue,pa),e(ue,hn),e(hn,ua),p(t,cr,h),p(t,me,h),_(gt,me,null),e(me,ma),e(me,pn),e(pn,fa),p(t,lr,h),p(t,fe,h),_(vt,fe,null),e(fe,_a),e(fe,un),e(un,ga),p(t,hr,h),p(t,_e,h),_(kt,_e,null),e(_e,va),e(_e,mn),e(mn,ka),p(t,pr,h),p(t,ge,h),e(ge,Ie),e(Ie,fn),_(Tt,fn,null),e(ge,Ta),e(ge,_n),e(_n,ba),p(t,ur,h),p(t,x,h),_(bt,x,null),e(x,ya),e(x,yt),e(yt,wa),e(yt,go),e(go,Pa),e(yt,Na),e(x,qa),e(x,ve),e(ve,za),e(ve,wt),e(wt,$a),e(ve,Fa),e(ve,gn),e(gn,Ma),e(ve,xa),e(x,Ea),e(x,Pt),e(Pt,Ca),e(Pt,Nt),e(Nt,Oa),e(Pt,Sa),e(x,Da),e(x,O),_(qt,O,null),e(O,La),e(O,ke),e(ke,ja),e(ke,vo),e(vo,Aa),e(ke,Ia),e(ke,vn),e(vn,Wa),e(ke,Ba),e(O,Ga),_(We,O,null),e(O,Ha),e(O,kn),e(kn,Va),e(O,Ua),_(zt,O,null),p(t,mr,h),p(t,Te,h),e(Te,Be),e(Be,Tn),_($t,Tn,null),e(Te,Ra),e(Te,bn),e(bn,Ja),p(t,fr,h),p(t,F,h),_(Ft,F,null),e(F,Ya),e(F,Mt),e(Mt,Ka),e(Mt,ko),e(ko,Qa),e(Mt,Xa),e(F,Za),e(F,be),e(be,ed),e(be,xt),e(xt,td),e(be,od),e(be,yn),e(yn,nd),e(be,rd),e(F,sd),e(F,Et),e(Et,ad),e(Et,Ct),e(Ct,dd),e(Et,id),e(F,cd),e(F,I),e(I,ld),e(I,wn),e(wn,hd),e(I,pd),e(I,Pn),e(Pn,ud),e(I,md),e(I,Nn),e(Nn,fd),e(I,_d),e(I,To),e(To,gd),e(I,vd),e(F,kd),e(F,S),_(Ot,S,null),e(S,Td),e(S,ye),e(ye,bd),e(ye,bo),e(bo,yd),e(ye,wd),e(ye,qn),e(qn,Pd),e(ye,Nd),e(S,qd),_(Ge,S,null),e(S,zd),e(S,zn),e(zn,$d),e(S,Fd),_(St,S,null),p(t,_r,h),p(t,we,h),e(we,He),e(He,$n),_(Dt,$n,null),e(we,Md),e(we,Fn),e(Fn,xd),p(t,gr,h),p(t,M,h),_(Lt,M,null),e(M,Ed),e(M,jt),e(jt,Cd),e(jt,yo),e(yo,Od),e(jt,Sd),e(M,Dd),e(M,Pe),e(Pe,Ld),e(Pe,At),e(At,jd),e(Pe,Ad),e(Pe,Mn),e(Mn,Id),e(Pe,Wd),e(M,Bd),e(M,It),e(It,Gd),e(It,Wt),e(Wt,Hd),e(It,Vd),e(M,Ud),e(M,W),e(W,Rd),e(W,xn),e(xn,Jd),e(W,Yd),e(W,En),e(En,Kd),e(W,Qd),e(W,Cn),e(Cn,Xd),e(W,Zd),e(W,wo),e(wo,ei),e(W,ti),e(M,oi),e(M,D),_(Bt,D,null),e(D,ni),e(D,Ne),e(Ne,ri),e(Ne,Po),e(Po,si),e(Ne,ai),e(Ne,On),e(On,di),e(Ne,ii),e(D,ci),_(Ve,D,null),e(D,li),e(D,Sn),e(Sn,hi),e(D,pi),_(Gt,D,null),p(t,vr,h),p(t,qe,h),e(qe,Ue),e(Ue,Dn),_(Ht,Dn,null),e(qe,ui),e(qe,Ln),e(Ln,mi),p(t,kr,h),p(t,E,h),_(Vt,E,null),e(E,fi),e(E,Ut),e(Ut,_i),e(Ut,No),e(No,gi),e(Ut,vi),e(E,ki),e(E,ze),e(ze,Ti),e(ze,Rt),e(Rt,bi),e(ze,yi),e(ze,jn),e(jn,wi),e(ze,Pi),e(E,Ni),e(E,Jt),e(Jt,qi),e(Jt,Yt),e(Yt,zi),e(Jt,$i),e(E,Fi),e(E,L),_(Kt,L,null),e(L,Mi),e(L,$e),e($e,xi),e($e,qo),e(qo,Ei),e($e,Ci),e($e,An),e(An,Oi),e($e,Si),e(L,Di),_(Re,L,null),e(L,Li),e(L,In),e(In,ji),e(L,Ai),_(Qt,L,null),p(t,Tr,h),p(t,Fe,h),e(Fe,Je),e(Je,Wn),_(Xt,Wn,null),e(Fe,Ii),e(Fe,Bn),e(Bn,Wi),p(t,br,h),p(t,C,h),_(Zt,C,null),e(C,Bi),e(C,eo),e(eo,Gi),e(eo,zo),e(zo,Hi),e(eo,Vi),e(C,Ui),e(C,Me),e(Me,Ri),e(Me,to),e(to,Ji),e(Me,Yi),e(Me,Gn),e(Gn,Ki),e(Me,Qi),e(C,Xi),e(C,oo),e(oo,Zi),e(oo,no),e(no,ec),e(oo,tc),e(C,oc),e(C,j),_(ro,j,null),e(j,nc),e(j,xe),e(xe,rc),e(xe,$o),e($o,sc),e(xe,ac),e(xe,Hn),e(Hn,dc),e(xe,ic),e(j,cc),_(Ye,j,null),e(j,lc),e(j,Vn),e(Vn,hc),e(j,pc),_(so,j,null),yr=!0},p(t,[h]){const ao={};h&2&&(ao.$$scope={dirty:h,ctx:t}),We.$set(ao);const Un={};h&2&&(Un.$$scope={dirty:h,ctx:t}),Ge.$set(Un);const Rn={};h&2&&(Rn.$$scope={dirty:h,ctx:t}),Ve.$set(Rn);const Jn={};h&2&&(Jn.$$scope={dirty:h,ctx:t}),Re.$set(Jn);const Ke={};h&2&&(Ke.$$scope={dirty:h,ctx:t}),Ye.$set(Ke)},i(t){yr||(g(y.$$.fragment,t),g(Ze.$$.fragment,t),g(ot.$$.fragment,t),g(nt.$$.fragment,t),g(st.$$.fragment,t),g(at.$$.fragment,t),g(it.$$.fragment,t),g(lt.$$.fragment,t),g(ht.$$.fragment,t),g(pt.$$.fragment,t),g(ut.$$.fragment,t),g(ft.$$.fragment,t),g(_t.$$.fragment,t),g(gt.$$.fragment,t),g(vt.$$.fragment,t),g(kt.$$.fragment,t),g(Tt.$$.fragment,t),g(bt.$$.fragment,t),g(qt.$$.fragment,t),g(We.$$.fragment,t),g(zt.$$.fragment,t),g($t.$$.fragment,t),g(Ft.$$.fragment,t),g(Ot.$$.fragment,t),g(Ge.$$.fragment,t),g(St.$$.fragment,t),g(Dt.$$.fragment,t),g(Lt.$$.fragment,t),g(Bt.$$.fragment,t),g(Ve.$$.fragment,t),g(Gt.$$.fragment,t),g(Ht.$$.fragment,t),g(Vt.$$.fragment,t),g(Kt.$$.fragment,t),g(Re.$$.fragment,t),g(Qt.$$.fragment,t),g(Xt.$$.fragment,t),g(Zt.$$.fragment,t),g(ro.$$.fragment,t),g(Ye.$$.fragment,t),g(so.$$.fragment,t),yr=!0)},o(t){v(y.$$.fragment,t),v(Ze.$$.fragment,t),v(ot.$$.fragment,t),v(nt.$$.fragment,t),v(st.$$.fragment,t),v(at.$$.fragment,t),v(it.$$.fragment,t),v(lt.$$.fragment,t),v(ht.$$.fragment,t),v(pt.$$.fragment,t),v(ut.$$.fragment,t),v(ft.$$.fragment,t),v(_t.$$.fragment,t),v(gt.$$.fragment,t),v(vt.$$.fragment,t),v(kt.$$.fragment,t),v(Tt.$$.fragment,t),v(bt.$$.fragment,t),v(qt.$$.fragment,t),v(We.$$.fragment,t),v(zt.$$.fragment,t),v($t.$$.fragment,t),v(Ft.$$.fragment,t),v(Ot.$$.fragment,t),v(Ge.$$.fragment,t),v(St.$$.fragment,t),v(Dt.$$.fragment,t),v(Lt.$$.fragment,t),v(Bt.$$.fragment,t),v(Ve.$$.fragment,t),v(Gt.$$.fragment,t),v(Ht.$$.fragment,t),v(Vt.$$.fragment,t),v(Kt.$$.fragment,t),v(Re.$$.fragment,t),v(Qt.$$.fragment,t),v(Xt.$$.fragment,t),v(Zt.$$.fragment,t),v(ro.$$.fragment,t),v(Ye.$$.fragment,t),v(so.$$.fragment,t),yr=!1},d(t){o(u),t&&o(N),t&&o(T),k(y),t&&o(Kn),t&&o(se),t&&o(Qn),t&&o(ae),k(Ze),t&&o(Xn),t&&o(Ce),t&&o(Zn),t&&o(io),t&&o(er),t&&o(co),t&&o(tr),t&&o(lo),t&&o(or),t&&o(Oe),t&&o(nr),t&&o(de),k(ot),t&&o(rr),t&&o(U),k(nt),t&&o(sr),t&&o(le),k(st),t&&o(ar),t&&o($),k(at),k(it),k(lt),k(ht),k(pt),k(ut),t&&o(dr),t&&o(pe),k(ft),t&&o(ir),t&&o(ue),k(_t),t&&o(cr),t&&o(me),k(gt),t&&o(lr),t&&o(fe),k(vt),t&&o(hr),t&&o(_e),k(kt),t&&o(pr),t&&o(ge),k(Tt),t&&o(ur),t&&o(x),k(bt),k(qt),k(We),k(zt),t&&o(mr),t&&o(Te),k($t),t&&o(fr),t&&o(F),k(Ft),k(Ot),k(Ge),k(St),t&&o(_r),t&&o(we),k(Dt),t&&o(gr),t&&o(M),k(Lt),k(Bt),k(Ve),k(Gt),t&&o(vr),t&&o(qe),k(Ht),t&&o(kr),t&&o(E),k(Vt),k(Kt),k(Re),k(Qt),t&&o(Tr),t&&o(Fe),k(Xt),t&&o(br),t&&o(C),k(Zt),k(ro),k(Ye),k(so)}}}const mh={local:"prophetnet",sections:[{local:"overview",title:"Overview"},{local:"transformers.ProphetNetConfig",title:"ProphetNetConfig"},{local:"transformers.ProphetNetTokenizer",title:"ProphetNetTokenizer"},{local:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqLMOutput",title:"ProphetNet specific outputs"},{local:"transformers.ProphetNetModel",title:"ProphetNetModel"},{local:"transformers.ProphetNetEncoder",title:"ProphetNetEncoder"},{local:"transformers.ProphetNetDecoder",title:"ProphetNetDecoder"},{local:"transformers.ProphetNetForConditionalGeneration",title:"ProphetNetForConditionalGeneration"},{local:"transformers.ProphetNetForCausalLM",title:"ProphetNetForCausalLM"}],title:"ProphetNet"};function fh(B){return dh(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class bh extends nh{constructor(u){super();rh(this,u,fh,uh,sh,{})}}export{bh as default,mh as metadata};
