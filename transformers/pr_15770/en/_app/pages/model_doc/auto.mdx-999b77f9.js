import{S as lut,i as iut,s as dut,e as a,k as l,w as f,t as o,L as cut,c as n,d as t,m as i,a as s,x as m,h as r,b as c,J as e,g as b,y as g,q as h,o as p,B as _}from"../../chunks/vendor-9e2b328e.js";import{T as Kyr}from"../../chunks/Tip-76f97a76.js";import{D as E}from"../../chunks/Docstring-50fd6873.js";import{C as w}from"../../chunks/CodeBlock-88e23343.js";import{I as z}from"../../chunks/IconCopyLink-fd0e58fd.js";import"../../chunks/CopyButton-4ae140ab.js";function fut(yi){let J,Le,de,me,to,ce,be,Do,wi,Ef,sa,Ai,Li,nM,yf,we,io,Bi,Pn,sM,$n,In,lM,ki,jn,iM,xi,wf,$a;return{c(){J=a("p"),Le=o("If your "),de=a("code"),me=o("NewModelConfig"),to=o(" is a subclass of "),ce=a("code"),be=o("PretrainedConfig"),Do=o(`, make sure its
`),wi=a("code"),Ef=o("model_type"),sa=o(" attribute is set to the same key you use when registering the config (here "),Ai=a("code"),Li=o('"new-model"'),nM=o(")."),yf=l(),we=a("p"),io=o("Likewise, if your "),Bi=a("code"),Pn=o("NewModel"),sM=o(" is a subclass of "),$n=a("a"),In=o("PreTrainedModel"),lM=o(`, make sure its
`),ki=a("code"),jn=o("config_class"),iM=o(` attribute is set to the same class you use when registering the model (here
`),xi=a("code"),wf=o("NewModelConfig"),$a=o(")."),this.h()},l(co){J=n(co,"P",{});var ge=s(J);Le=r(ge,"If your "),de=n(ge,"CODE",{});var XL=s(de);me=r(XL,"NewModelConfig"),XL.forEach(t),to=r(ge," is a subclass of "),ce=n(ge,"CODE",{});var Ri=s(ce);be=r(Ri,"PretrainedConfig"),Ri.forEach(t),Do=r(ge,`, make sure its
`),wi=n(ge,"CODE",{});var zL=s(wi);Ef=r(zL,"model_type"),zL.forEach(t),sa=r(ge," attribute is set to the same key you use when registering the config (here "),Ai=n(ge,"CODE",{});var VL=s(Ai);Li=r(VL,'"new-model"'),VL.forEach(t),nM=r(ge,")."),ge.forEach(t),yf=i(co),we=n(co,"P",{});var qo=s(we);io=r(qo,"Likewise, if your "),Bi=n(qo,"CODE",{});var Ia=s(Bi);Pn=r(Ia,"NewModel"),Ia.forEach(t),sM=r(qo," is a subclass of "),$n=n(qo,"A",{href:!0});var WL=s($n);In=r(WL,"PreTrainedModel"),WL.forEach(t),lM=r(qo,`, make sure its
`),ki=n(qo,"CODE",{});var Af=s(ki);jn=r(Af,"config_class"),Af.forEach(t),iM=r(qo,` attribute is set to the same class you use when registering the model (here
`),xi=n(qo,"CODE",{});var QL=s(xi);wf=r(QL,"NewModelConfig"),QL.forEach(t),$a=r(qo,")."),qo.forEach(t),this.h()},h(){c($n,"href","/docs/transformers/pr_15770/en/main_classes/model#transformers.PreTrainedModel")},m(co,ge){b(co,J,ge),e(J,Le),e(J,de),e(de,me),e(J,to),e(J,ce),e(ce,be),e(J,Do),e(J,wi),e(wi,Ef),e(J,sa),e(J,Ai),e(Ai,Li),e(J,nM),b(co,yf,ge),b(co,we,ge),e(we,io),e(we,Bi),e(Bi,Pn),e(we,sM),e(we,$n),e($n,In),e(we,lM),e(we,ki),e(ki,jn),e(we,iM),e(we,xi),e(xi,wf),e(we,$a)},d(co){co&&t(J),co&&t(yf),co&&t(we)}}}function mut(yi){let J,Le,de,me,to;return{c(){J=a("p"),Le=o("Passing "),de=a("code"),me=o("use_auth_token=True"),to=o(" is required when you want to use a private model.")},l(ce){J=n(ce,"P",{});var be=s(J);Le=r(be,"Passing "),de=n(be,"CODE",{});var Do=s(de);me=r(Do,"use_auth_token=True"),Do.forEach(t),to=r(be," is required when you want to use a private model."),be.forEach(t)},m(ce,be){b(ce,J,be),e(J,Le),e(J,de),e(de,me),e(J,to)},d(ce){ce&&t(J)}}}function gut(yi){let J,Le,de,me,to;return{c(){J=a("p"),Le=o("Passing "),de=a("code"),me=o("use_auth_token=True"),to=o(" is required when you want to use a private model.")},l(ce){J=n(ce,"P",{});var be=s(J);Le=r(be,"Passing "),de=n(be,"CODE",{});var Do=s(de);me=r(Do,"use_auth_token=True"),Do.forEach(t),to=r(be," is required when you want to use a private model."),be.forEach(t)},m(ce,be){b(ce,J,be),e(J,Le),e(J,de),e(de,me),e(J,to)},d(ce){ce&&t(J)}}}function hut(yi){let J,Le,de,me,to,ce,be,Do,wi,Ef,sa,Ai,Li,nM,yf,we,io,Bi,Pn,sM,$n,In,lM,ki,jn,iM,xi,wf,$a,co,ge,XL,Ri,zL,VL,qo,Ia,WL,Af,QL,Lxe,v8e,Si,Lf,XV,dM,Bxe,zV,kxe,T8e,Nn,xxe,VV,Rxe,Sxe,WV,Pxe,$xe,F8e,cM,C8e,HL,Ixe,M8e,Bf,E8e,Pi,kf,QV,fM,jxe,HV,Nxe,y8e,Go,mM,Dxe,gM,qxe,UL,Gxe,Oxe,Xxe,hM,zxe,UV,Vxe,Wxe,Qxe,fo,pM,Hxe,JV,Uxe,Jxe,$i,Yxe,YV,Kxe,Zxe,KV,eRe,oRe,rRe,v,xf,ZV,tRe,aRe,JL,nRe,sRe,lRe,Rf,eW,iRe,dRe,YL,cRe,fRe,mRe,Sf,oW,gRe,hRe,KL,pRe,_Re,uRe,Pf,rW,bRe,vRe,ZL,TRe,FRe,CRe,$f,tW,MRe,ERe,e8,yRe,wRe,ARe,If,aW,LRe,BRe,o8,kRe,xRe,RRe,jf,nW,SRe,PRe,r8,$Re,IRe,jRe,Nf,sW,NRe,DRe,t8,qRe,GRe,ORe,Df,lW,XRe,zRe,a8,VRe,WRe,QRe,qf,iW,HRe,URe,n8,JRe,YRe,KRe,Gf,dW,ZRe,eSe,s8,oSe,rSe,tSe,Of,cW,aSe,nSe,l8,sSe,lSe,iSe,Xf,fW,dSe,cSe,i8,fSe,mSe,gSe,zf,mW,hSe,pSe,d8,_Se,uSe,bSe,Vf,gW,vSe,TSe,c8,FSe,CSe,MSe,Wf,hW,ESe,ySe,f8,wSe,ASe,LSe,Qf,pW,BSe,kSe,m8,xSe,RSe,SSe,Hf,_W,PSe,$Se,g8,ISe,jSe,NSe,Uf,uW,DSe,qSe,h8,GSe,OSe,XSe,Jf,bW,zSe,VSe,p8,WSe,QSe,HSe,Yf,vW,USe,JSe,_8,YSe,KSe,ZSe,Kf,TW,ePe,oPe,u8,rPe,tPe,aPe,Zf,FW,nPe,sPe,b8,lPe,iPe,dPe,em,CW,cPe,fPe,v8,mPe,gPe,hPe,om,MW,pPe,_Pe,T8,uPe,bPe,vPe,rm,EW,TPe,FPe,F8,CPe,MPe,EPe,tm,yW,yPe,wPe,C8,APe,LPe,BPe,am,wW,kPe,xPe,M8,RPe,SPe,PPe,nm,AW,$Pe,IPe,E8,jPe,NPe,DPe,sm,LW,qPe,GPe,y8,OPe,XPe,zPe,lm,BW,VPe,WPe,w8,QPe,HPe,UPe,im,kW,JPe,YPe,A8,KPe,ZPe,e$e,dm,xW,o$e,r$e,L8,t$e,a$e,n$e,cm,RW,s$e,l$e,B8,i$e,d$e,c$e,fm,SW,f$e,m$e,k8,g$e,h$e,p$e,mm,PW,_$e,u$e,x8,b$e,v$e,T$e,gm,$W,F$e,C$e,R8,M$e,E$e,y$e,hm,IW,w$e,A$e,S8,L$e,B$e,k$e,pm,jW,x$e,R$e,P8,S$e,P$e,$$e,_m,NW,I$e,j$e,$8,N$e,D$e,q$e,um,DW,G$e,O$e,I8,X$e,z$e,V$e,bm,qW,W$e,Q$e,j8,H$e,U$e,J$e,vm,GW,Y$e,K$e,N8,Z$e,eIe,oIe,Tm,OW,rIe,tIe,D8,aIe,nIe,sIe,Fm,XW,lIe,iIe,q8,dIe,cIe,fIe,Cm,zW,mIe,gIe,G8,hIe,pIe,_Ie,Mm,VW,uIe,bIe,O8,vIe,TIe,FIe,Em,WW,CIe,MIe,X8,EIe,yIe,wIe,ym,QW,AIe,LIe,z8,BIe,kIe,xIe,wm,HW,RIe,SIe,V8,PIe,$Ie,IIe,Am,UW,jIe,NIe,W8,DIe,qIe,GIe,Lm,JW,OIe,XIe,Q8,zIe,VIe,WIe,Bm,YW,QIe,HIe,H8,UIe,JIe,YIe,km,KW,KIe,ZIe,U8,eje,oje,rje,xm,ZW,tje,aje,J8,nje,sje,lje,Rm,eQ,ije,dje,Y8,cje,fje,mje,Sm,oQ,gje,hje,K8,pje,_je,uje,Pm,rQ,bje,vje,Z8,Tje,Fje,Cje,$m,tQ,Mje,Eje,e9,yje,wje,Aje,Im,aQ,Lje,Bje,o9,kje,xje,Rje,jm,nQ,Sje,Pje,r9,$je,Ije,jje,Nm,sQ,Nje,Dje,t9,qje,Gje,Oje,Dm,lQ,Xje,zje,a9,Vje,Wje,Qje,qm,iQ,Hje,Uje,n9,Jje,Yje,Kje,Gm,dQ,Zje,eNe,s9,oNe,rNe,tNe,Om,cQ,aNe,nNe,l9,sNe,lNe,iNe,Xm,fQ,dNe,cNe,i9,fNe,mNe,gNe,zm,mQ,hNe,pNe,d9,_Ne,uNe,bNe,Vm,gQ,vNe,TNe,c9,FNe,CNe,MNe,Wm,hQ,ENe,yNe,f9,wNe,ANe,LNe,Qm,pQ,BNe,kNe,m9,xNe,RNe,SNe,Hm,_Q,PNe,$Ne,g9,INe,jNe,NNe,Um,uQ,DNe,qNe,h9,GNe,ONe,XNe,Jm,bQ,zNe,VNe,p9,WNe,QNe,HNe,Ym,vQ,UNe,JNe,_9,YNe,KNe,ZNe,Km,TQ,eDe,oDe,u9,rDe,tDe,aDe,Zm,FQ,nDe,sDe,b9,lDe,iDe,dDe,eg,CQ,cDe,fDe,v9,mDe,gDe,hDe,og,MQ,pDe,_De,T9,uDe,bDe,vDe,rg,EQ,TDe,FDe,F9,CDe,MDe,EDe,tg,yQ,yDe,wDe,C9,ADe,LDe,BDe,ag,wQ,kDe,xDe,M9,RDe,SDe,PDe,ng,AQ,$De,IDe,E9,jDe,NDe,DDe,sg,LQ,qDe,GDe,y9,ODe,XDe,zDe,lg,BQ,VDe,WDe,w9,QDe,HDe,UDe,ig,kQ,JDe,YDe,A9,KDe,ZDe,eqe,dg,xQ,oqe,rqe,L9,tqe,aqe,nqe,cg,RQ,sqe,lqe,B9,iqe,dqe,cqe,fg,SQ,fqe,mqe,k9,gqe,hqe,pqe,mg,PQ,_qe,uqe,x9,bqe,vqe,Tqe,gg,$Q,Fqe,Cqe,R9,Mqe,Eqe,yqe,hg,IQ,wqe,Aqe,S9,Lqe,Bqe,kqe,jQ,xqe,Rqe,_M,Sqe,pg,uM,Pqe,NQ,$qe,w8e,Ii,_g,DQ,bM,Iqe,qQ,jqe,A8e,Oo,vM,Nqe,TM,Dqe,P9,qqe,Gqe,Oqe,FM,Xqe,GQ,zqe,Vqe,Wqe,mo,CM,Qqe,OQ,Hqe,Uqe,ja,Jqe,XQ,Yqe,Kqe,zQ,Zqe,eGe,VQ,oGe,rGe,tGe,M,Dn,WQ,aGe,nGe,$9,sGe,lGe,I9,iGe,dGe,cGe,qn,QQ,fGe,mGe,j9,gGe,hGe,N9,pGe,_Ge,uGe,Gn,HQ,bGe,vGe,D9,TGe,FGe,q9,CGe,MGe,EGe,ug,UQ,yGe,wGe,G9,AGe,LGe,BGe,On,JQ,kGe,xGe,O9,RGe,SGe,X9,PGe,$Ge,IGe,bg,YQ,jGe,NGe,z9,DGe,qGe,GGe,vg,KQ,OGe,XGe,V9,zGe,VGe,WGe,Tg,ZQ,QGe,HGe,W9,UGe,JGe,YGe,Xn,eH,KGe,ZGe,Q9,eOe,oOe,H9,rOe,tOe,aOe,zn,oH,nOe,sOe,U9,lOe,iOe,J9,dOe,cOe,fOe,Vn,rH,mOe,gOe,Y9,hOe,pOe,K9,_Oe,uOe,bOe,Fg,tH,vOe,TOe,Z9,FOe,COe,MOe,Cg,aH,EOe,yOe,eB,wOe,AOe,LOe,Wn,nH,BOe,kOe,oB,xOe,ROe,rB,SOe,POe,$Oe,Mg,sH,IOe,jOe,tB,NOe,DOe,qOe,Qn,lH,GOe,OOe,aB,XOe,zOe,nB,VOe,WOe,QOe,Hn,iH,HOe,UOe,sB,JOe,YOe,lB,KOe,ZOe,eXe,Un,dH,oXe,rXe,iB,tXe,aXe,cH,nXe,sXe,lXe,Eg,fH,iXe,dXe,dB,cXe,fXe,mXe,Jn,mH,gXe,hXe,cB,pXe,_Xe,fB,uXe,bXe,vXe,yg,gH,TXe,FXe,mB,CXe,MXe,EXe,Yn,hH,yXe,wXe,gB,AXe,LXe,hB,BXe,kXe,xXe,Kn,pH,RXe,SXe,pB,PXe,$Xe,_B,IXe,jXe,NXe,Zn,_H,DXe,qXe,uB,GXe,OXe,bB,XXe,zXe,VXe,wg,uH,WXe,QXe,vB,HXe,UXe,JXe,es,bH,YXe,KXe,TB,ZXe,eze,FB,oze,rze,tze,Ag,vH,aze,nze,CB,sze,lze,ize,os,TH,dze,cze,MB,fze,mze,EB,gze,hze,pze,rs,FH,_ze,uze,yB,bze,vze,wB,Tze,Fze,Cze,ts,CH,Mze,Eze,AB,yze,wze,LB,Aze,Lze,Bze,as,MH,kze,xze,BB,Rze,Sze,kB,Pze,$ze,Ize,Lg,EH,jze,Nze,xB,Dze,qze,Gze,ns,yH,Oze,Xze,RB,zze,Vze,SB,Wze,Qze,Hze,ss,wH,Uze,Jze,PB,Yze,Kze,$B,Zze,eVe,oVe,ls,AH,rVe,tVe,IB,aVe,nVe,jB,sVe,lVe,iVe,is,LH,dVe,cVe,NB,fVe,mVe,DB,gVe,hVe,pVe,ds,BH,_Ve,uVe,qB,bVe,vVe,GB,TVe,FVe,CVe,cs,kH,MVe,EVe,OB,yVe,wVe,XB,AVe,LVe,BVe,Bg,xH,kVe,xVe,zB,RVe,SVe,PVe,fs,RH,$Ve,IVe,VB,jVe,NVe,WB,DVe,qVe,GVe,kg,SH,OVe,XVe,QB,zVe,VVe,WVe,xg,PH,QVe,HVe,HB,UVe,JVe,YVe,ms,$H,KVe,ZVe,UB,eWe,oWe,JB,rWe,tWe,aWe,gs,IH,nWe,sWe,YB,lWe,iWe,KB,dWe,cWe,fWe,Rg,jH,mWe,gWe,ZB,hWe,pWe,_We,hs,NH,uWe,bWe,ek,vWe,TWe,ok,FWe,CWe,MWe,ps,DH,EWe,yWe,rk,wWe,AWe,tk,LWe,BWe,kWe,_s,qH,xWe,RWe,ak,SWe,PWe,nk,$We,IWe,jWe,us,GH,NWe,DWe,sk,qWe,GWe,lk,OWe,XWe,zWe,bs,OH,VWe,WWe,ik,QWe,HWe,dk,UWe,JWe,YWe,Sg,XH,KWe,ZWe,ck,eQe,oQe,rQe,Pg,zH,tQe,aQe,fk,nQe,sQe,lQe,$g,VH,iQe,dQe,mk,cQe,fQe,mQe,Ig,WH,gQe,hQe,gk,pQe,_Qe,uQe,vs,QH,bQe,vQe,hk,TQe,FQe,pk,CQe,MQe,EQe,jg,HH,yQe,wQe,_k,AQe,LQe,BQe,Ts,UH,kQe,xQe,uk,RQe,SQe,bk,PQe,$Qe,IQe,Fs,JH,jQe,NQe,vk,DQe,qQe,Tk,GQe,OQe,XQe,Cs,YH,zQe,VQe,Fk,WQe,QQe,Ck,HQe,UQe,JQe,Ms,KH,YQe,KQe,Mk,ZQe,eHe,Ek,oHe,rHe,tHe,Es,ZH,aHe,nHe,yk,sHe,lHe,wk,iHe,dHe,cHe,Ng,eU,fHe,mHe,Ak,gHe,hHe,pHe,Dg,oU,_He,uHe,Lk,bHe,vHe,THe,ys,rU,FHe,CHe,Bk,MHe,EHe,kk,yHe,wHe,AHe,ws,tU,LHe,BHe,xk,kHe,xHe,Rk,RHe,SHe,PHe,As,aU,$He,IHe,Sk,jHe,NHe,Pk,DHe,qHe,GHe,qg,nU,OHe,XHe,$k,zHe,VHe,WHe,Gg,sU,QHe,HHe,Ik,UHe,JHe,YHe,Og,lU,KHe,ZHe,jk,eUe,oUe,rUe,Xg,iU,tUe,aUe,Nk,nUe,sUe,lUe,Ls,dU,iUe,dUe,Dk,cUe,fUe,qk,mUe,gUe,hUe,zg,cU,pUe,_Ue,Gk,uUe,bUe,vUe,Vg,fU,TUe,FUe,Ok,CUe,MUe,EUe,Bs,mU,yUe,wUe,Xk,AUe,LUe,zk,BUe,kUe,xUe,ks,gU,RUe,SUe,Vk,PUe,$Ue,Wk,IUe,jUe,NUe,hU,DUe,qUe,MM,GUe,Wg,EM,OUe,pU,XUe,L8e,ji,Qg,_U,yM,zUe,uU,VUe,B8e,Xo,wM,WUe,AM,QUe,Qk,HUe,UUe,JUe,LM,YUe,bU,KUe,ZUe,eJe,Be,BM,oJe,vU,rJe,tJe,Na,aJe,TU,nJe,sJe,FU,lJe,iJe,CU,dJe,cJe,fJe,ae,Hg,MU,mJe,gJe,Hk,hJe,pJe,_Je,Ug,EU,uJe,bJe,Uk,vJe,TJe,FJe,Jg,yU,CJe,MJe,Jk,EJe,yJe,wJe,Yg,wU,AJe,LJe,Yk,BJe,kJe,xJe,Kg,AU,RJe,SJe,Kk,PJe,$Je,IJe,Zg,LU,jJe,NJe,Zk,DJe,qJe,GJe,eh,BU,OJe,XJe,ex,zJe,VJe,WJe,oh,kU,QJe,HJe,ox,UJe,JJe,YJe,rh,xU,KJe,ZJe,rx,eYe,oYe,rYe,th,RU,tYe,aYe,tx,nYe,sYe,lYe,ah,SU,iYe,dYe,ax,cYe,fYe,mYe,nh,PU,gYe,hYe,nx,pYe,_Ye,uYe,sh,$U,bYe,vYe,sx,TYe,FYe,CYe,lh,IU,MYe,EYe,lx,yYe,wYe,AYe,ih,jU,LYe,BYe,ix,kYe,xYe,RYe,dh,NU,SYe,PYe,dx,$Ye,IYe,jYe,ch,NYe,DU,DYe,qYe,kM,GYe,fh,xM,OYe,qU,XYe,k8e,Ni,mh,GU,RM,zYe,OU,VYe,x8e,zo,SM,WYe,PM,QYe,cx,HYe,UYe,JYe,$M,YYe,XU,KYe,ZYe,eKe,ke,IM,oKe,zU,rKe,tKe,Di,aKe,VU,nKe,sKe,WU,lKe,iKe,dKe,Ae,gh,QU,cKe,fKe,fx,mKe,gKe,hKe,hh,HU,pKe,_Ke,mx,uKe,bKe,vKe,ph,UU,TKe,FKe,gx,CKe,MKe,EKe,_h,JU,yKe,wKe,hx,AKe,LKe,BKe,uh,YU,kKe,xKe,px,RKe,SKe,PKe,bh,KU,$Ke,IKe,_x,jKe,NKe,DKe,vh,ZU,qKe,GKe,ux,OKe,XKe,zKe,Th,eJ,VKe,WKe,bx,QKe,HKe,UKe,Fh,JKe,oJ,YKe,KKe,jM,ZKe,Ch,NM,eZe,rJ,oZe,R8e,qi,Mh,tJ,DM,rZe,aJ,tZe,S8e,Vo,qM,aZe,Gi,nZe,nJ,sZe,lZe,sJ,iZe,dZe,cZe,GM,fZe,lJ,mZe,gZe,hZe,Nr,OM,pZe,iJ,_Ze,uZe,Oi,bZe,dJ,vZe,TZe,cJ,FZe,CZe,MZe,fJ,EZe,yZe,XM,wZe,xe,zM,AZe,mJ,LZe,BZe,Da,kZe,gJ,xZe,RZe,hJ,SZe,PZe,pJ,$Ze,IZe,jZe,F,Eh,_J,NZe,DZe,vx,qZe,GZe,OZe,yh,uJ,XZe,zZe,Tx,VZe,WZe,QZe,wh,bJ,HZe,UZe,Fx,JZe,YZe,KZe,Ah,vJ,ZZe,eeo,Cx,oeo,reo,teo,Lh,TJ,aeo,neo,Mx,seo,leo,ieo,Bh,FJ,deo,ceo,Ex,feo,meo,geo,kh,CJ,heo,peo,yx,_eo,ueo,beo,xh,MJ,veo,Teo,wx,Feo,Ceo,Meo,Rh,EJ,Eeo,yeo,Ax,weo,Aeo,Leo,Sh,yJ,Beo,keo,Lx,xeo,Reo,Seo,Ph,wJ,Peo,$eo,Bx,Ieo,jeo,Neo,$h,AJ,Deo,qeo,kx,Geo,Oeo,Xeo,Ih,LJ,zeo,Veo,xx,Weo,Qeo,Heo,jh,BJ,Ueo,Jeo,Rx,Yeo,Keo,Zeo,Nh,kJ,eoo,ooo,Sx,roo,too,aoo,Dh,xJ,noo,soo,Px,loo,ioo,doo,qh,RJ,coo,foo,$x,moo,goo,hoo,Gh,SJ,poo,_oo,Ix,uoo,boo,voo,Oh,PJ,Too,Foo,jx,Coo,Moo,Eoo,Xh,$J,yoo,woo,Nx,Aoo,Loo,Boo,zh,IJ,koo,xoo,Dx,Roo,Soo,Poo,Vh,jJ,$oo,Ioo,qx,joo,Noo,Doo,Wh,NJ,qoo,Goo,Gx,Ooo,Xoo,zoo,Qh,DJ,Voo,Woo,Ox,Qoo,Hoo,Uoo,Hh,qJ,Joo,Yoo,Xx,Koo,Zoo,ero,xs,GJ,oro,rro,zx,tro,aro,Vx,nro,sro,lro,Uh,OJ,iro,dro,Wx,cro,fro,mro,Jh,XJ,gro,hro,Qx,pro,_ro,uro,Yh,zJ,bro,vro,Hx,Tro,Fro,Cro,Kh,VJ,Mro,Ero,Ux,yro,wro,Aro,Zh,WJ,Lro,Bro,Jx,kro,xro,Rro,ep,QJ,Sro,Pro,Yx,$ro,Iro,jro,op,HJ,Nro,Dro,Kx,qro,Gro,Oro,rp,UJ,Xro,zro,Zx,Vro,Wro,Qro,tp,JJ,Hro,Uro,eR,Jro,Yro,Kro,ap,YJ,Zro,eto,oR,oto,rto,tto,np,KJ,ato,nto,rR,sto,lto,ito,sp,ZJ,dto,cto,tR,fto,mto,gto,lp,eY,hto,pto,aR,_to,uto,bto,ip,oY,vto,Tto,nR,Fto,Cto,Mto,dp,rY,Eto,yto,sR,wto,Ato,Lto,cp,tY,Bto,kto,lR,xto,Rto,Sto,fp,aY,Pto,$to,iR,Ito,jto,Nto,mp,nY,Dto,qto,dR,Gto,Oto,Xto,gp,sY,zto,Vto,cR,Wto,Qto,Hto,hp,lY,Uto,Jto,fR,Yto,Kto,Zto,pp,iY,eao,oao,mR,rao,tao,aao,_p,dY,nao,sao,gR,lao,iao,dao,up,cY,cao,fao,hR,mao,gao,hao,bp,fY,pao,_ao,pR,uao,bao,vao,vp,mY,Tao,Fao,_R,Cao,Mao,Eao,Tp,gY,yao,wao,uR,Aao,Lao,Bao,Fp,hY,kao,xao,bR,Rao,Sao,Pao,Cp,pY,$ao,Iao,vR,jao,Nao,Dao,Mp,_Y,qao,Gao,TR,Oao,Xao,zao,Ep,uY,Vao,Wao,FR,Qao,Hao,Uao,yp,bY,Jao,Yao,CR,Kao,Zao,eno,wp,vY,ono,rno,MR,tno,ano,nno,Ap,TY,sno,lno,ER,ino,dno,cno,Lp,FY,fno,mno,yR,gno,hno,pno,Bp,CY,_no,uno,wR,bno,vno,Tno,kp,MY,Fno,Cno,AR,Mno,Eno,yno,xp,EY,wno,Ano,LR,Lno,Bno,kno,Rp,yY,xno,Rno,BR,Sno,Pno,$no,Sp,wY,Ino,jno,kR,Nno,Dno,qno,Pp,AY,Gno,Ono,xR,Xno,zno,Vno,$p,LY,Wno,Qno,RR,Hno,Uno,Jno,Ip,BY,Yno,Kno,SR,Zno,eso,oso,jp,kY,rso,tso,PR,aso,nso,sso,Np,xY,lso,iso,$R,dso,cso,fso,Dp,RY,mso,gso,IR,hso,pso,_so,qp,SY,uso,bso,jR,vso,Tso,Fso,Gp,PY,Cso,Mso,NR,Eso,yso,wso,Op,$Y,Aso,Lso,DR,Bso,kso,xso,Xp,IY,Rso,Sso,qR,Pso,$so,Iso,zp,jY,jso,Nso,GR,Dso,qso,Gso,Vp,NY,Oso,Xso,OR,zso,Vso,Wso,Wp,DY,Qso,Hso,XR,Uso,Jso,Yso,Qp,qY,Kso,Zso,zR,elo,olo,rlo,Hp,GY,tlo,alo,VR,nlo,slo,llo,Up,OY,ilo,dlo,WR,clo,flo,mlo,Jp,XY,glo,hlo,QR,plo,_lo,ulo,Yp,zY,blo,vlo,HR,Tlo,Flo,Clo,Kp,VY,Mlo,Elo,UR,ylo,wlo,Alo,Zp,WY,Llo,Blo,JR,klo,xlo,Rlo,e_,Slo,QY,Plo,$lo,HY,Ilo,jlo,UY,Nlo,Dlo,VM,P8e,Xi,o_,JY,WM,qlo,YY,Glo,$8e,Wo,QM,Olo,zi,Xlo,KY,zlo,Vlo,ZY,Wlo,Qlo,Hlo,HM,Ulo,eK,Jlo,Ylo,Klo,Dr,UM,Zlo,oK,eio,oio,Vi,rio,rK,tio,aio,tK,nio,sio,lio,aK,iio,dio,JM,cio,Re,YM,fio,nK,mio,gio,qa,hio,sK,pio,_io,lK,uio,bio,iK,vio,Tio,Fio,x,r_,dK,Cio,Mio,YR,Eio,yio,wio,t_,cK,Aio,Lio,KR,Bio,kio,xio,a_,fK,Rio,Sio,ZR,Pio,$io,Iio,n_,mK,jio,Nio,eS,Dio,qio,Gio,s_,gK,Oio,Xio,oS,zio,Vio,Wio,l_,hK,Qio,Hio,rS,Uio,Jio,Yio,i_,pK,Kio,Zio,tS,edo,odo,rdo,d_,_K,tdo,ado,aS,ndo,sdo,ldo,c_,uK,ido,ddo,nS,cdo,fdo,mdo,f_,bK,gdo,hdo,sS,pdo,_do,udo,m_,vK,bdo,vdo,lS,Tdo,Fdo,Cdo,g_,TK,Mdo,Edo,iS,ydo,wdo,Ado,h_,FK,Ldo,Bdo,dS,kdo,xdo,Rdo,p_,CK,Sdo,Pdo,cS,$do,Ido,jdo,__,MK,Ndo,Ddo,fS,qdo,Gdo,Odo,u_,EK,Xdo,zdo,mS,Vdo,Wdo,Qdo,b_,yK,Hdo,Udo,gS,Jdo,Ydo,Kdo,v_,wK,Zdo,eco,hS,oco,rco,tco,T_,AK,aco,nco,pS,sco,lco,ico,F_,LK,dco,cco,_S,fco,mco,gco,C_,BK,hco,pco,uS,_co,uco,bco,M_,kK,vco,Tco,bS,Fco,Cco,Mco,E_,xK,Eco,yco,vS,wco,Aco,Lco,y_,RK,Bco,kco,TS,xco,Rco,Sco,w_,SK,Pco,$co,FS,Ico,jco,Nco,A_,PK,Dco,qco,CS,Gco,Oco,Xco,L_,$K,zco,Vco,MS,Wco,Qco,Hco,B_,IK,Uco,Jco,ES,Yco,Kco,Zco,k_,jK,efo,ofo,yS,rfo,tfo,afo,x_,NK,nfo,sfo,wS,lfo,ifo,dfo,R_,DK,cfo,ffo,AS,mfo,gfo,hfo,S_,qK,pfo,_fo,LS,ufo,bfo,vfo,P_,GK,Tfo,Ffo,BS,Cfo,Mfo,Efo,$_,OK,yfo,wfo,kS,Afo,Lfo,Bfo,I_,XK,kfo,xfo,xS,Rfo,Sfo,Pfo,j_,zK,$fo,Ifo,RS,jfo,Nfo,Dfo,N_,VK,qfo,Gfo,SS,Ofo,Xfo,zfo,D_,WK,Vfo,Wfo,PS,Qfo,Hfo,Ufo,q_,Jfo,QK,Yfo,Kfo,HK,Zfo,emo,UK,omo,rmo,KM,I8e,Wi,G_,JK,ZM,tmo,YK,amo,j8e,Qo,eE,nmo,Qi,smo,KK,lmo,imo,ZK,dmo,cmo,fmo,oE,mmo,eZ,gmo,hmo,pmo,qr,rE,_mo,oZ,umo,bmo,Hi,vmo,rZ,Tmo,Fmo,tZ,Cmo,Mmo,Emo,aZ,ymo,wmo,tE,Amo,Se,aE,Lmo,nZ,Bmo,kmo,Ga,xmo,sZ,Rmo,Smo,lZ,Pmo,$mo,iZ,Imo,jmo,Nmo,$,O_,dZ,Dmo,qmo,$S,Gmo,Omo,Xmo,X_,cZ,zmo,Vmo,IS,Wmo,Qmo,Hmo,z_,fZ,Umo,Jmo,jS,Ymo,Kmo,Zmo,V_,mZ,ego,ogo,NS,rgo,tgo,ago,W_,gZ,ngo,sgo,DS,lgo,igo,dgo,Q_,hZ,cgo,fgo,qS,mgo,ggo,hgo,H_,pZ,pgo,_go,GS,ugo,bgo,vgo,U_,_Z,Tgo,Fgo,OS,Cgo,Mgo,Ego,J_,uZ,ygo,wgo,XS,Ago,Lgo,Bgo,Y_,bZ,kgo,xgo,zS,Rgo,Sgo,Pgo,K_,vZ,$go,Igo,VS,jgo,Ngo,Dgo,Z_,TZ,qgo,Ggo,WS,Ogo,Xgo,zgo,eu,FZ,Vgo,Wgo,QS,Qgo,Hgo,Ugo,ou,CZ,Jgo,Ygo,HS,Kgo,Zgo,eho,ru,MZ,oho,rho,US,tho,aho,nho,tu,EZ,sho,lho,JS,iho,dho,cho,au,yZ,fho,mho,YS,gho,hho,pho,nu,wZ,_ho,uho,KS,bho,vho,Tho,su,AZ,Fho,Cho,ZS,Mho,Eho,yho,lu,LZ,who,Aho,eP,Lho,Bho,kho,iu,BZ,xho,Rho,oP,Sho,Pho,$ho,du,kZ,Iho,jho,rP,Nho,Dho,qho,cu,xZ,Gho,Oho,tP,Xho,zho,Vho,fu,RZ,Who,Qho,aP,Hho,Uho,Jho,mu,SZ,Yho,Kho,nP,Zho,epo,opo,gu,PZ,rpo,tpo,sP,apo,npo,spo,hu,$Z,lpo,ipo,lP,dpo,cpo,fpo,pu,IZ,mpo,gpo,iP,hpo,ppo,_po,_u,jZ,upo,bpo,dP,vpo,Tpo,Fpo,uu,NZ,Cpo,Mpo,cP,Epo,ypo,wpo,bu,DZ,Apo,Lpo,fP,Bpo,kpo,xpo,vu,qZ,Rpo,Spo,mP,Ppo,$po,Ipo,Tu,GZ,jpo,Npo,gP,Dpo,qpo,Gpo,Fu,OZ,Opo,Xpo,hP,zpo,Vpo,Wpo,Cu,Qpo,XZ,Hpo,Upo,zZ,Jpo,Ypo,VZ,Kpo,Zpo,nE,N8e,Ui,Mu,WZ,sE,e_o,QZ,o_o,D8e,Ho,lE,r_o,Ji,t_o,HZ,a_o,n_o,UZ,s_o,l_o,i_o,iE,d_o,JZ,c_o,f_o,m_o,Gr,dE,g_o,YZ,h_o,p_o,Yi,__o,KZ,u_o,b_o,ZZ,v_o,T_o,F_o,eee,C_o,M_o,cE,E_o,Pe,fE,y_o,oee,w_o,A_o,Oa,L_o,ree,B_o,k_o,tee,x_o,R_o,aee,S_o,P_o,$_o,I,Eu,nee,I_o,j_o,pP,N_o,D_o,q_o,yu,see,G_o,O_o,_P,X_o,z_o,V_o,wu,lee,W_o,Q_o,uP,H_o,U_o,J_o,Au,iee,Y_o,K_o,bP,Z_o,euo,ouo,Lu,dee,ruo,tuo,vP,auo,nuo,suo,Bu,cee,luo,iuo,TP,duo,cuo,fuo,ku,fee,muo,guo,FP,huo,puo,_uo,xu,mee,uuo,buo,CP,vuo,Tuo,Fuo,Ru,gee,Cuo,Muo,MP,Euo,yuo,wuo,Su,hee,Auo,Luo,EP,Buo,kuo,xuo,Pu,pee,Ruo,Suo,yP,Puo,$uo,Iuo,$u,_ee,juo,Nuo,wP,Duo,quo,Guo,Iu,uee,Ouo,Xuo,AP,zuo,Vuo,Wuo,ju,bee,Quo,Huo,LP,Uuo,Juo,Yuo,Nu,vee,Kuo,Zuo,BP,e1o,o1o,r1o,Du,Tee,t1o,a1o,kP,n1o,s1o,l1o,qu,Fee,i1o,d1o,xP,c1o,f1o,m1o,Gu,Cee,g1o,h1o,RP,p1o,_1o,u1o,Ou,Mee,b1o,v1o,SP,T1o,F1o,C1o,Xu,Eee,M1o,E1o,PP,y1o,w1o,A1o,zu,yee,L1o,B1o,$P,k1o,x1o,R1o,Vu,wee,S1o,P1o,IP,$1o,I1o,j1o,Wu,Aee,N1o,D1o,jP,q1o,G1o,O1o,Qu,Lee,X1o,z1o,NP,V1o,W1o,Q1o,Hu,Bee,H1o,U1o,DP,J1o,Y1o,K1o,Uu,kee,Z1o,e7o,qP,o7o,r7o,t7o,Ju,xee,a7o,n7o,GP,s7o,l7o,i7o,Yu,Ree,d7o,c7o,OP,f7o,m7o,g7o,Ku,See,h7o,p7o,XP,_7o,u7o,b7o,Zu,Pee,v7o,T7o,$ee,F7o,C7o,M7o,e1,Iee,E7o,y7o,zP,w7o,A7o,L7o,o1,jee,B7o,k7o,VP,x7o,R7o,S7o,r1,Nee,P7o,$7o,WP,I7o,j7o,N7o,t1,Dee,D7o,q7o,QP,G7o,O7o,X7o,a1,z7o,qee,V7o,W7o,Gee,Q7o,H7o,Oee,U7o,J7o,mE,q8e,Ki,n1,Xee,gE,Y7o,zee,K7o,G8e,Uo,hE,Z7o,Zi,ebo,Vee,obo,rbo,Wee,tbo,abo,nbo,pE,sbo,Qee,lbo,ibo,dbo,Or,_E,cbo,Hee,fbo,mbo,ed,gbo,Uee,hbo,pbo,Jee,_bo,ubo,bbo,Yee,vbo,Tbo,uE,Fbo,$e,bE,Cbo,Kee,Mbo,Ebo,Xa,ybo,Zee,wbo,Abo,eoe,Lbo,Bbo,ooe,kbo,xbo,Rbo,ne,s1,roe,Sbo,Pbo,HP,$bo,Ibo,jbo,l1,toe,Nbo,Dbo,UP,qbo,Gbo,Obo,i1,aoe,Xbo,zbo,JP,Vbo,Wbo,Qbo,d1,noe,Hbo,Ubo,YP,Jbo,Ybo,Kbo,c1,soe,Zbo,e5o,KP,o5o,r5o,t5o,f1,loe,a5o,n5o,ZP,s5o,l5o,i5o,m1,ioe,d5o,c5o,e$,f5o,m5o,g5o,g1,doe,h5o,p5o,o$,_5o,u5o,b5o,h1,coe,v5o,T5o,r$,F5o,C5o,M5o,p1,foe,E5o,y5o,t$,w5o,A5o,L5o,_1,moe,B5o,k5o,a$,x5o,R5o,S5o,u1,goe,P5o,$5o,n$,I5o,j5o,N5o,b1,hoe,D5o,q5o,s$,G5o,O5o,X5o,v1,poe,z5o,V5o,l$,W5o,Q5o,H5o,T1,_oe,U5o,J5o,i$,Y5o,K5o,Z5o,F1,uoe,e2o,o2o,d$,r2o,t2o,a2o,C1,n2o,boe,s2o,l2o,voe,i2o,d2o,Toe,c2o,f2o,vE,O8e,od,M1,Foe,TE,m2o,Coe,g2o,X8e,Jo,FE,h2o,rd,p2o,Moe,_2o,u2o,Eoe,b2o,v2o,T2o,CE,F2o,yoe,C2o,M2o,E2o,Xr,ME,y2o,woe,w2o,A2o,td,L2o,Aoe,B2o,k2o,Loe,x2o,R2o,S2o,Boe,P2o,$2o,EE,I2o,Ie,yE,j2o,koe,N2o,D2o,za,q2o,xoe,G2o,O2o,Roe,X2o,z2o,Soe,V2o,W2o,Q2o,A,E1,Poe,H2o,U2o,c$,J2o,Y2o,K2o,y1,$oe,Z2o,evo,f$,ovo,rvo,tvo,w1,Ioe,avo,nvo,m$,svo,lvo,ivo,A1,joe,dvo,cvo,g$,fvo,mvo,gvo,L1,Noe,hvo,pvo,h$,_vo,uvo,bvo,B1,Doe,vvo,Tvo,p$,Fvo,Cvo,Mvo,k1,qoe,Evo,yvo,_$,wvo,Avo,Lvo,x1,Goe,Bvo,kvo,u$,xvo,Rvo,Svo,R1,Ooe,Pvo,$vo,b$,Ivo,jvo,Nvo,S1,Xoe,Dvo,qvo,v$,Gvo,Ovo,Xvo,P1,zoe,zvo,Vvo,T$,Wvo,Qvo,Hvo,$1,Voe,Uvo,Jvo,F$,Yvo,Kvo,Zvo,I1,Woe,e0o,o0o,C$,r0o,t0o,a0o,j1,Qoe,n0o,s0o,M$,l0o,i0o,d0o,N1,Hoe,c0o,f0o,E$,m0o,g0o,h0o,D1,Uoe,p0o,_0o,y$,u0o,b0o,v0o,q1,Joe,T0o,F0o,w$,C0o,M0o,E0o,G1,Yoe,y0o,w0o,A$,A0o,L0o,B0o,O1,Koe,k0o,x0o,L$,R0o,S0o,P0o,X1,Zoe,$0o,I0o,B$,j0o,N0o,D0o,z1,ere,q0o,G0o,k$,O0o,X0o,z0o,V1,ore,V0o,W0o,x$,Q0o,H0o,U0o,W1,rre,J0o,Y0o,R$,K0o,Z0o,eTo,Q1,tre,oTo,rTo,S$,tTo,aTo,nTo,H1,are,sTo,lTo,P$,iTo,dTo,cTo,U1,nre,fTo,mTo,$$,gTo,hTo,pTo,J1,sre,_To,uTo,I$,bTo,vTo,TTo,Y1,lre,FTo,CTo,j$,MTo,ETo,yTo,K1,ire,wTo,ATo,N$,LTo,BTo,kTo,Z1,dre,xTo,RTo,D$,STo,PTo,$To,e7,cre,ITo,jTo,q$,NTo,DTo,qTo,o7,fre,GTo,OTo,G$,XTo,zTo,VTo,r7,mre,WTo,QTo,O$,HTo,UTo,JTo,t7,gre,YTo,KTo,X$,ZTo,eFo,oFo,a7,hre,rFo,tFo,z$,aFo,nFo,sFo,n7,pre,lFo,iFo,V$,dFo,cFo,fFo,s7,_re,mFo,gFo,W$,hFo,pFo,_Fo,l7,ure,uFo,bFo,Q$,vFo,TFo,FFo,i7,bre,CFo,MFo,H$,EFo,yFo,wFo,d7,vre,AFo,LFo,U$,BFo,kFo,xFo,c7,Tre,RFo,SFo,J$,PFo,$Fo,IFo,f7,Fre,jFo,NFo,Y$,DFo,qFo,GFo,m7,Cre,OFo,XFo,K$,zFo,VFo,WFo,g7,Mre,QFo,HFo,Z$,UFo,JFo,YFo,h7,Ere,KFo,ZFo,eI,eCo,oCo,rCo,p7,tCo,yre,aCo,nCo,wre,sCo,lCo,Are,iCo,dCo,wE,z8e,ad,_7,Lre,AE,cCo,Bre,fCo,V8e,Yo,LE,mCo,nd,gCo,kre,hCo,pCo,xre,_Co,uCo,bCo,BE,vCo,Rre,TCo,FCo,CCo,zr,kE,MCo,Sre,ECo,yCo,sd,wCo,Pre,ACo,LCo,$re,BCo,kCo,xCo,Ire,RCo,SCo,xE,PCo,je,RE,$Co,jre,ICo,jCo,Va,NCo,Nre,DCo,qCo,Dre,GCo,OCo,qre,XCo,zCo,VCo,G,u7,Gre,WCo,QCo,oI,HCo,UCo,JCo,b7,Ore,YCo,KCo,rI,ZCo,e4o,o4o,v7,Xre,r4o,t4o,tI,a4o,n4o,s4o,T7,zre,l4o,i4o,aI,d4o,c4o,f4o,F7,Vre,m4o,g4o,nI,h4o,p4o,_4o,C7,Wre,u4o,b4o,sI,v4o,T4o,F4o,M7,Qre,C4o,M4o,lI,E4o,y4o,w4o,E7,Hre,A4o,L4o,iI,B4o,k4o,x4o,y7,Ure,R4o,S4o,dI,P4o,$4o,I4o,w7,Jre,j4o,N4o,cI,D4o,q4o,G4o,A7,Yre,O4o,X4o,fI,z4o,V4o,W4o,L7,Kre,Q4o,H4o,mI,U4o,J4o,Y4o,B7,Zre,K4o,Z4o,gI,eMo,oMo,rMo,k7,ete,tMo,aMo,hI,nMo,sMo,lMo,x7,ote,iMo,dMo,pI,cMo,fMo,mMo,R7,rte,gMo,hMo,_I,pMo,_Mo,uMo,S7,tte,bMo,vMo,uI,TMo,FMo,CMo,P7,ate,MMo,EMo,bI,yMo,wMo,AMo,$7,nte,LMo,BMo,vI,kMo,xMo,RMo,I7,ste,SMo,PMo,TI,$Mo,IMo,jMo,j7,lte,NMo,DMo,FI,qMo,GMo,OMo,N7,ite,XMo,zMo,CI,VMo,WMo,QMo,D7,dte,HMo,UMo,MI,JMo,YMo,KMo,q7,cte,ZMo,eEo,EI,oEo,rEo,tEo,G7,fte,aEo,nEo,yI,sEo,lEo,iEo,O7,mte,dEo,cEo,wI,fEo,mEo,gEo,X7,gte,hEo,pEo,AI,_Eo,uEo,bEo,z7,vEo,hte,TEo,FEo,pte,CEo,MEo,_te,EEo,yEo,SE,W8e,ld,V7,ute,PE,wEo,bte,AEo,Q8e,Ko,$E,LEo,id,BEo,vte,kEo,xEo,Tte,REo,SEo,PEo,IE,$Eo,Fte,IEo,jEo,NEo,Vr,jE,DEo,Cte,qEo,GEo,dd,OEo,Mte,XEo,zEo,Ete,VEo,WEo,QEo,yte,HEo,UEo,NE,JEo,Ne,DE,YEo,wte,KEo,ZEo,Wa,e3o,Ate,o3o,r3o,Lte,t3o,a3o,Bte,n3o,s3o,l3o,na,W7,kte,i3o,d3o,LI,c3o,f3o,m3o,Q7,xte,g3o,h3o,BI,p3o,_3o,u3o,H7,Rte,b3o,v3o,kI,T3o,F3o,C3o,U7,Ste,M3o,E3o,xI,y3o,w3o,A3o,J7,Pte,L3o,B3o,RI,k3o,x3o,R3o,Y7,S3o,$te,P3o,$3o,Ite,I3o,j3o,jte,N3o,D3o,qE,H8e,cd,K7,Nte,GE,q3o,Dte,G3o,U8e,Zo,OE,O3o,fd,X3o,qte,z3o,V3o,Gte,W3o,Q3o,H3o,XE,U3o,Ote,J3o,Y3o,K3o,Wr,zE,Z3o,Xte,eyo,oyo,md,ryo,zte,tyo,ayo,Vte,nyo,syo,lyo,Wte,iyo,dyo,VE,cyo,De,WE,fyo,Qte,myo,gyo,Qa,hyo,Hte,pyo,_yo,Ute,uyo,byo,Jte,vyo,Tyo,Fyo,D,Z7,Yte,Cyo,Myo,SI,Eyo,yyo,wyo,eb,Kte,Ayo,Lyo,PI,Byo,kyo,xyo,ob,Zte,Ryo,Syo,$I,Pyo,$yo,Iyo,rb,eae,jyo,Nyo,II,Dyo,qyo,Gyo,tb,oae,Oyo,Xyo,jI,zyo,Vyo,Wyo,ab,rae,Qyo,Hyo,NI,Uyo,Jyo,Yyo,nb,tae,Kyo,Zyo,DI,ewo,owo,rwo,sb,aae,two,awo,qI,nwo,swo,lwo,lb,nae,iwo,dwo,GI,cwo,fwo,mwo,ib,sae,gwo,hwo,OI,pwo,_wo,uwo,db,lae,bwo,vwo,XI,Two,Fwo,Cwo,cb,iae,Mwo,Ewo,zI,ywo,wwo,Awo,fb,dae,Lwo,Bwo,VI,kwo,xwo,Rwo,mb,cae,Swo,Pwo,WI,$wo,Iwo,jwo,gb,fae,Nwo,Dwo,QI,qwo,Gwo,Owo,hb,mae,Xwo,zwo,HI,Vwo,Wwo,Qwo,pb,gae,Hwo,Uwo,UI,Jwo,Ywo,Kwo,_b,hae,Zwo,e6o,JI,o6o,r6o,t6o,ub,pae,a6o,n6o,YI,s6o,l6o,i6o,bb,_ae,d6o,c6o,KI,f6o,m6o,g6o,vb,uae,h6o,p6o,ZI,_6o,u6o,b6o,Tb,bae,v6o,T6o,ej,F6o,C6o,M6o,Fb,vae,E6o,y6o,oj,w6o,A6o,L6o,Cb,Tae,B6o,k6o,rj,x6o,R6o,S6o,Mb,Fae,P6o,$6o,tj,I6o,j6o,N6o,Eb,Cae,D6o,q6o,aj,G6o,O6o,X6o,yb,Mae,z6o,V6o,nj,W6o,Q6o,H6o,wb,Eae,U6o,J6o,sj,Y6o,K6o,Z6o,Ab,yae,eAo,oAo,lj,rAo,tAo,aAo,Lb,wae,nAo,sAo,ij,lAo,iAo,dAo,Bb,Aae,cAo,fAo,dj,mAo,gAo,hAo,kb,Lae,pAo,_Ao,cj,uAo,bAo,vAo,xb,TAo,Bae,FAo,CAo,kae,MAo,EAo,xae,yAo,wAo,QE,J8e,gd,Rb,Rae,HE,AAo,Sae,LAo,Y8e,er,UE,BAo,hd,kAo,Pae,xAo,RAo,$ae,SAo,PAo,$Ao,JE,IAo,Iae,jAo,NAo,DAo,Qr,YE,qAo,jae,GAo,OAo,pd,XAo,Nae,zAo,VAo,Dae,WAo,QAo,HAo,qae,UAo,JAo,KE,YAo,qe,ZE,KAo,Gae,ZAo,eLo,Ha,oLo,Oae,rLo,tLo,Xae,aLo,nLo,zae,sLo,lLo,iLo,R,Sb,Vae,dLo,cLo,fj,fLo,mLo,gLo,Pb,Wae,hLo,pLo,mj,_Lo,uLo,bLo,$b,Qae,vLo,TLo,gj,FLo,CLo,MLo,Ib,Hae,ELo,yLo,hj,wLo,ALo,LLo,jb,Uae,BLo,kLo,pj,xLo,RLo,SLo,Nb,Jae,PLo,$Lo,_j,ILo,jLo,NLo,Db,Yae,DLo,qLo,uj,GLo,OLo,XLo,qb,Kae,zLo,VLo,bj,WLo,QLo,HLo,Gb,Zae,ULo,JLo,vj,YLo,KLo,ZLo,Ob,ene,e8o,o8o,Tj,r8o,t8o,a8o,Xb,one,n8o,s8o,Fj,l8o,i8o,d8o,zb,rne,c8o,f8o,Cj,m8o,g8o,h8o,Vb,tne,p8o,_8o,Mj,u8o,b8o,v8o,Wb,ane,T8o,F8o,Ej,C8o,M8o,E8o,Qb,nne,y8o,w8o,yj,A8o,L8o,B8o,Hb,sne,k8o,x8o,wj,R8o,S8o,P8o,Ub,lne,$8o,I8o,Aj,j8o,N8o,D8o,Jb,ine,q8o,G8o,Lj,O8o,X8o,z8o,Yb,dne,V8o,W8o,Bj,Q8o,H8o,U8o,Kb,cne,J8o,Y8o,kj,K8o,Z8o,e9o,Zb,fne,o9o,r9o,xj,t9o,a9o,n9o,e5,mne,s9o,l9o,Rj,i9o,d9o,c9o,o5,gne,f9o,m9o,Sj,g9o,h9o,p9o,r5,hne,_9o,u9o,Pj,b9o,v9o,T9o,t5,pne,F9o,C9o,$j,M9o,E9o,y9o,a5,_ne,w9o,A9o,Ij,L9o,B9o,k9o,n5,une,x9o,R9o,jj,S9o,P9o,$9o,s5,bne,I9o,j9o,Nj,N9o,D9o,q9o,l5,vne,G9o,O9o,Dj,X9o,z9o,V9o,i5,Tne,W9o,Q9o,qj,H9o,U9o,J9o,d5,Fne,Y9o,K9o,Gj,Z9o,eBo,oBo,c5,Cne,rBo,tBo,Oj,aBo,nBo,sBo,f5,Mne,lBo,iBo,Xj,dBo,cBo,fBo,m5,Ene,mBo,gBo,zj,hBo,pBo,_Bo,g5,yne,uBo,bBo,Vj,vBo,TBo,FBo,h5,wne,CBo,MBo,Wj,EBo,yBo,wBo,p5,Ane,ABo,LBo,Qj,BBo,kBo,xBo,_5,Lne,RBo,SBo,Hj,PBo,$Bo,IBo,u5,jBo,Bne,NBo,DBo,kne,qBo,GBo,xne,OBo,XBo,e3,K8e,_d,b5,Rne,o3,zBo,Sne,VBo,Z8e,or,r3,WBo,ud,QBo,Pne,HBo,UBo,$ne,JBo,YBo,KBo,t3,ZBo,Ine,eko,oko,rko,Hr,a3,tko,jne,ako,nko,bd,sko,Nne,lko,iko,Dne,dko,cko,fko,qne,mko,gko,n3,hko,Ge,s3,pko,Gne,_ko,uko,Ua,bko,One,vko,Tko,Xne,Fko,Cko,zne,Mko,Eko,yko,Vne,v5,Wne,wko,Ako,Uj,Lko,Bko,kko,T5,xko,Qne,Rko,Sko,Hne,Pko,$ko,Une,Iko,jko,l3,e9e,vd,F5,Jne,i3,Nko,Yne,Dko,o9e,rr,d3,qko,Td,Gko,Kne,Oko,Xko,Zne,zko,Vko,Wko,c3,Qko,ese,Hko,Uko,Jko,Ur,f3,Yko,ose,Kko,Zko,Fd,exo,rse,oxo,rxo,tse,txo,axo,nxo,ase,sxo,lxo,m3,ixo,Oe,g3,dxo,nse,cxo,fxo,Ja,mxo,sse,gxo,hxo,lse,pxo,_xo,ise,uxo,bxo,vxo,he,C5,dse,Txo,Fxo,Jj,Cxo,Mxo,Exo,M5,cse,yxo,wxo,Yj,Axo,Lxo,Bxo,Rs,fse,kxo,xxo,Kj,Rxo,Sxo,Zj,Pxo,$xo,Ixo,E5,mse,jxo,Nxo,eN,Dxo,qxo,Gxo,la,gse,Oxo,Xxo,oN,zxo,Vxo,rN,Wxo,Qxo,tN,Hxo,Uxo,Jxo,y5,hse,Yxo,Kxo,aN,Zxo,eRo,oRo,w5,pse,rRo,tRo,nN,aRo,nRo,sRo,A5,_se,lRo,iRo,sN,dRo,cRo,fRo,L5,use,mRo,gRo,lN,hRo,pRo,_Ro,B5,bse,uRo,bRo,iN,vRo,TRo,FRo,k5,CRo,vse,MRo,ERo,Tse,yRo,wRo,Fse,ARo,LRo,h3,r9e,Cd,x5,Cse,p3,BRo,Mse,kRo,t9e,tr,_3,xRo,Md,RRo,Ese,SRo,PRo,yse,$Ro,IRo,jRo,u3,NRo,wse,DRo,qRo,GRo,Jr,b3,ORo,Ase,XRo,zRo,Ed,VRo,Lse,WRo,QRo,Bse,HRo,URo,JRo,kse,YRo,KRo,v3,ZRo,Xe,T3,eSo,xse,oSo,rSo,Ya,tSo,Rse,aSo,nSo,Sse,sSo,lSo,Pse,iSo,dSo,cSo,$se,R5,Ise,fSo,mSo,dN,gSo,hSo,pSo,S5,_So,jse,uSo,bSo,Nse,vSo,TSo,Dse,FSo,CSo,F3,a9e,yd,P5,qse,C3,MSo,Gse,ESo,n9e,ar,M3,ySo,wd,wSo,Ose,ASo,LSo,Xse,BSo,kSo,xSo,E3,RSo,zse,SSo,PSo,$So,Yr,y3,ISo,Vse,jSo,NSo,Ad,DSo,Wse,qSo,GSo,Qse,OSo,XSo,zSo,Hse,VSo,WSo,w3,QSo,ze,A3,HSo,Use,USo,JSo,Ka,YSo,Jse,KSo,ZSo,Yse,ePo,oPo,Kse,rPo,tPo,aPo,ao,$5,Zse,nPo,sPo,cN,lPo,iPo,dPo,I5,ele,cPo,fPo,fN,mPo,gPo,hPo,j5,ole,pPo,_Po,mN,uPo,bPo,vPo,N5,rle,TPo,FPo,gN,CPo,MPo,EPo,D5,tle,yPo,wPo,hN,APo,LPo,BPo,q5,ale,kPo,xPo,pN,RPo,SPo,PPo,G5,nle,$Po,IPo,_N,jPo,NPo,DPo,O5,qPo,sle,GPo,OPo,lle,XPo,zPo,ile,VPo,WPo,L3,s9e,Ld,X5,dle,B3,QPo,cle,HPo,l9e,nr,k3,UPo,Bd,JPo,fle,YPo,KPo,mle,ZPo,e$o,o$o,x3,r$o,gle,t$o,a$o,n$o,Kr,R3,s$o,hle,l$o,i$o,kd,d$o,ple,c$o,f$o,_le,m$o,g$o,h$o,ule,p$o,_$o,S3,u$o,Ve,P3,b$o,ble,v$o,T$o,Za,F$o,vle,C$o,M$o,Tle,E$o,y$o,Fle,w$o,A$o,L$o,xd,z5,Cle,B$o,k$o,uN,x$o,R$o,S$o,V5,Mle,P$o,$$o,bN,I$o,j$o,N$o,W5,Ele,D$o,q$o,vN,G$o,O$o,X$o,Q5,z$o,yle,V$o,W$o,wle,Q$o,H$o,Ale,U$o,J$o,$3,i9e,Rd,H5,Lle,I3,Y$o,Ble,K$o,d9e,sr,j3,Z$o,Sd,eIo,kle,oIo,rIo,xle,tIo,aIo,nIo,N3,sIo,Rle,lIo,iIo,dIo,Zr,D3,cIo,Sle,fIo,mIo,Pd,gIo,Ple,hIo,pIo,$le,_Io,uIo,bIo,Ile,vIo,TIo,q3,FIo,We,G3,CIo,jle,MIo,EIo,en,yIo,Nle,wIo,AIo,Dle,LIo,BIo,qle,kIo,xIo,RIo,no,U5,Gle,SIo,PIo,TN,$Io,IIo,jIo,J5,Ole,NIo,DIo,FN,qIo,GIo,OIo,Y5,Xle,XIo,zIo,CN,VIo,WIo,QIo,K5,zle,HIo,UIo,MN,JIo,YIo,KIo,Z5,Vle,ZIo,ejo,EN,ojo,rjo,tjo,e2,Wle,ajo,njo,yN,sjo,ljo,ijo,o2,Qle,djo,cjo,wN,fjo,mjo,gjo,r2,hjo,Hle,pjo,_jo,Ule,ujo,bjo,Jle,vjo,Tjo,O3,c9e,$d,t2,Yle,X3,Fjo,Kle,Cjo,f9e,lr,z3,Mjo,Id,Ejo,Zle,yjo,wjo,eie,Ajo,Ljo,Bjo,V3,kjo,oie,xjo,Rjo,Sjo,et,W3,Pjo,rie,$jo,Ijo,jd,jjo,tie,Njo,Djo,aie,qjo,Gjo,Ojo,nie,Xjo,zjo,Q3,Vjo,Qe,H3,Wjo,sie,Qjo,Hjo,on,Ujo,lie,Jjo,Yjo,iie,Kjo,Zjo,die,eNo,oNo,rNo,U3,a2,cie,tNo,aNo,AN,nNo,sNo,lNo,n2,fie,iNo,dNo,LN,cNo,fNo,mNo,s2,gNo,mie,hNo,pNo,gie,_No,uNo,hie,bNo,vNo,J3,m9e,Nd,l2,pie,Y3,TNo,_ie,FNo,g9e,ir,K3,CNo,Dd,MNo,uie,ENo,yNo,bie,wNo,ANo,LNo,Z3,BNo,vie,kNo,xNo,RNo,ot,ey,SNo,Tie,PNo,$No,qd,INo,Fie,jNo,NNo,Cie,DNo,qNo,GNo,Mie,ONo,XNo,oy,zNo,He,ry,VNo,Eie,WNo,QNo,rn,HNo,yie,UNo,JNo,wie,YNo,KNo,Aie,ZNo,eDo,oDo,Gd,i2,Lie,rDo,tDo,BN,aDo,nDo,sDo,d2,Bie,lDo,iDo,kN,dDo,cDo,fDo,c2,kie,mDo,gDo,xN,hDo,pDo,_Do,f2,uDo,xie,bDo,vDo,Rie,TDo,FDo,Sie,CDo,MDo,ty,h9e,Od,m2,Pie,ay,EDo,$ie,yDo,p9e,dr,ny,wDo,Xd,ADo,Iie,LDo,BDo,jie,kDo,xDo,RDo,sy,SDo,Nie,PDo,$Do,IDo,rt,ly,jDo,Die,NDo,DDo,zd,qDo,qie,GDo,ODo,Gie,XDo,zDo,VDo,Oie,WDo,QDo,iy,HDo,Ue,dy,UDo,Xie,JDo,YDo,tn,KDo,zie,ZDo,eqo,Vie,oqo,rqo,Wie,tqo,aqo,nqo,Vd,g2,Qie,sqo,lqo,RN,iqo,dqo,cqo,h2,Hie,fqo,mqo,SN,gqo,hqo,pqo,p2,Uie,_qo,uqo,PN,bqo,vqo,Tqo,_2,Fqo,Jie,Cqo,Mqo,Yie,Eqo,yqo,Kie,wqo,Aqo,cy,_9e,Wd,u2,Zie,fy,Lqo,ede,Bqo,u9e,cr,my,kqo,Qd,xqo,ode,Rqo,Sqo,rde,Pqo,$qo,Iqo,gy,jqo,tde,Nqo,Dqo,qqo,tt,hy,Gqo,ade,Oqo,Xqo,Hd,zqo,nde,Vqo,Wqo,sde,Qqo,Hqo,Uqo,lde,Jqo,Yqo,py,Kqo,Je,_y,Zqo,ide,eGo,oGo,an,rGo,dde,tGo,aGo,cde,nGo,sGo,fde,lGo,iGo,dGo,mde,b2,gde,cGo,fGo,$N,mGo,gGo,hGo,v2,pGo,hde,_Go,uGo,pde,bGo,vGo,_de,TGo,FGo,uy,b9e,Ud,T2,ude,by,CGo,bde,MGo,v9e,fr,vy,EGo,Jd,yGo,vde,wGo,AGo,Tde,LGo,BGo,kGo,Ty,xGo,Fde,RGo,SGo,PGo,at,Fy,$Go,Cde,IGo,jGo,Yd,NGo,Mde,DGo,qGo,Ede,GGo,OGo,XGo,yde,zGo,VGo,Cy,WGo,Ye,My,QGo,wde,HGo,UGo,nn,JGo,Ade,YGo,KGo,Lde,ZGo,eOo,Bde,oOo,rOo,tOo,kde,F2,xde,aOo,nOo,IN,sOo,lOo,iOo,C2,dOo,Rde,cOo,fOo,Sde,mOo,gOo,Pde,hOo,pOo,Ey,T9e,Kd,M2,$de,yy,_Oo,Ide,uOo,F9e,mr,wy,bOo,Zd,vOo,jde,TOo,FOo,Nde,COo,MOo,EOo,Ay,yOo,Dde,wOo,AOo,LOo,nt,Ly,BOo,qde,kOo,xOo,ec,ROo,Gde,SOo,POo,Ode,$Oo,IOo,jOo,Xde,NOo,DOo,By,qOo,Ke,ky,GOo,zde,OOo,XOo,sn,zOo,Vde,VOo,WOo,Wde,QOo,HOo,Qde,UOo,JOo,YOo,xy,E2,Hde,KOo,ZOo,jN,eXo,oXo,rXo,y2,Ude,tXo,aXo,NN,nXo,sXo,lXo,w2,iXo,Jde,dXo,cXo,Yde,fXo,mXo,Kde,gXo,hXo,Ry,C9e,oc,A2,Zde,Sy,pXo,ece,_Xo,M9e,gr,Py,uXo,rc,bXo,oce,vXo,TXo,rce,FXo,CXo,MXo,$y,EXo,tce,yXo,wXo,AXo,st,Iy,LXo,ace,BXo,kXo,tc,xXo,nce,RXo,SXo,sce,PXo,$Xo,IXo,lce,jXo,NXo,jy,DXo,go,Ny,qXo,ice,GXo,OXo,ln,XXo,dce,zXo,VXo,cce,WXo,QXo,fce,HXo,UXo,JXo,B,L2,mce,YXo,KXo,DN,ZXo,ezo,ozo,B2,gce,rzo,tzo,qN,azo,nzo,szo,k2,hce,lzo,izo,GN,dzo,czo,fzo,x2,pce,mzo,gzo,ON,hzo,pzo,_zo,R2,_ce,uzo,bzo,XN,vzo,Tzo,Fzo,S2,uce,Czo,Mzo,zN,Ezo,yzo,wzo,P2,bce,Azo,Lzo,VN,Bzo,kzo,xzo,$2,vce,Rzo,Szo,WN,Pzo,$zo,Izo,I2,Tce,jzo,Nzo,QN,Dzo,qzo,Gzo,j2,Fce,Ozo,Xzo,HN,zzo,Vzo,Wzo,N2,Cce,Qzo,Hzo,UN,Uzo,Jzo,Yzo,D2,Mce,Kzo,Zzo,JN,eVo,oVo,rVo,q2,Ece,tVo,aVo,YN,nVo,sVo,lVo,G2,yce,iVo,dVo,KN,cVo,fVo,mVo,O2,wce,gVo,hVo,ZN,pVo,_Vo,uVo,Ss,Ace,bVo,vVo,eD,TVo,FVo,oD,CVo,MVo,EVo,X2,Lce,yVo,wVo,rD,AVo,LVo,BVo,z2,Bce,kVo,xVo,tD,RVo,SVo,PVo,V2,kce,$Vo,IVo,aD,jVo,NVo,DVo,W2,xce,qVo,GVo,nD,OVo,XVo,zVo,Q2,Rce,VVo,WVo,sD,QVo,HVo,UVo,H2,Sce,JVo,YVo,lD,KVo,ZVo,eWo,U2,Pce,oWo,rWo,iD,tWo,aWo,nWo,J2,$ce,sWo,lWo,dD,iWo,dWo,cWo,Y2,Ice,fWo,mWo,cD,gWo,hWo,pWo,K2,jce,_Wo,uWo,fD,bWo,vWo,TWo,Z2,Nce,FWo,CWo,mD,MWo,EWo,yWo,ev,Dce,wWo,AWo,gD,LWo,BWo,kWo,ov,qce,xWo,RWo,hD,SWo,PWo,$Wo,rv,Gce,IWo,jWo,pD,NWo,DWo,qWo,tv,Oce,GWo,OWo,_D,XWo,zWo,VWo,av,Xce,WWo,QWo,uD,HWo,UWo,JWo,nv,zce,YWo,KWo,bD,ZWo,eQo,oQo,sv,Vce,rQo,tQo,vD,aQo,nQo,sQo,lv,Wce,lQo,iQo,TD,dQo,cQo,fQo,iv,Qce,mQo,gQo,FD,hQo,pQo,_Qo,dv,Hce,uQo,bQo,CD,vQo,TQo,FQo,cv,Uce,CQo,MQo,MD,EQo,yQo,wQo,fv,Jce,AQo,LQo,ED,BQo,kQo,xQo,mv,Yce,RQo,SQo,yD,PQo,$Qo,IQo,gv,Kce,jQo,NQo,wD,DQo,qQo,GQo,Zce,OQo,XQo,Dy,E9e,ac,hv,efe,qy,zQo,ofe,VQo,y9e,hr,Gy,WQo,nc,QQo,rfe,HQo,UQo,tfe,JQo,YQo,KQo,Oy,ZQo,afe,eHo,oHo,rHo,lt,Xy,tHo,nfe,aHo,nHo,sc,sHo,sfe,lHo,iHo,lfe,dHo,cHo,fHo,ife,mHo,gHo,zy,hHo,ho,Vy,pHo,dfe,_Ho,uHo,dn,bHo,cfe,vHo,THo,ffe,FHo,CHo,mfe,MHo,EHo,yHo,H,pv,gfe,wHo,AHo,AD,LHo,BHo,kHo,_v,hfe,xHo,RHo,LD,SHo,PHo,$Ho,uv,pfe,IHo,jHo,BD,NHo,DHo,qHo,bv,_fe,GHo,OHo,kD,XHo,zHo,VHo,vv,ufe,WHo,QHo,xD,HHo,UHo,JHo,Tv,bfe,YHo,KHo,RD,ZHo,eUo,oUo,Fv,vfe,rUo,tUo,SD,aUo,nUo,sUo,Cv,Tfe,lUo,iUo,PD,dUo,cUo,fUo,Mv,Ffe,mUo,gUo,$D,hUo,pUo,_Uo,Ev,Cfe,uUo,bUo,ID,vUo,TUo,FUo,yv,Mfe,CUo,MUo,jD,EUo,yUo,wUo,wv,Efe,AUo,LUo,ND,BUo,kUo,xUo,Av,yfe,RUo,SUo,DD,PUo,$Uo,IUo,Lv,wfe,jUo,NUo,qD,DUo,qUo,GUo,Bv,Afe,OUo,XUo,GD,zUo,VUo,WUo,kv,Lfe,QUo,HUo,OD,UUo,JUo,YUo,xv,Bfe,KUo,ZUo,XD,eJo,oJo,rJo,Rv,kfe,tJo,aJo,zD,nJo,sJo,lJo,Sv,xfe,iJo,dJo,VD,cJo,fJo,mJo,Pv,Rfe,gJo,hJo,WD,pJo,_Jo,uJo,$v,Sfe,bJo,vJo,QD,TJo,FJo,CJo,Iv,Pfe,MJo,EJo,HD,yJo,wJo,AJo,$fe,LJo,BJo,Wy,w9e,lc,jv,Ife,Qy,kJo,jfe,xJo,A9e,pr,Hy,RJo,ic,SJo,Nfe,PJo,$Jo,Dfe,IJo,jJo,NJo,Uy,DJo,qfe,qJo,GJo,OJo,it,Jy,XJo,Gfe,zJo,VJo,dc,WJo,Ofe,QJo,HJo,Xfe,UJo,JJo,YJo,zfe,KJo,ZJo,Yy,eYo,po,Ky,oYo,Vfe,rYo,tYo,cn,aYo,Wfe,nYo,sYo,Qfe,lYo,iYo,Hfe,dYo,cYo,fYo,pe,Nv,Ufe,mYo,gYo,UD,hYo,pYo,_Yo,Dv,Jfe,uYo,bYo,JD,vYo,TYo,FYo,qv,Yfe,CYo,MYo,YD,EYo,yYo,wYo,Gv,Kfe,AYo,LYo,KD,BYo,kYo,xYo,Ov,Zfe,RYo,SYo,ZD,PYo,$Yo,IYo,Xv,eme,jYo,NYo,eq,DYo,qYo,GYo,zv,ome,OYo,XYo,oq,zYo,VYo,WYo,Vv,rme,QYo,HYo,rq,UYo,JYo,YYo,Wv,tme,KYo,ZYo,tq,eKo,oKo,rKo,Qv,ame,tKo,aKo,aq,nKo,sKo,lKo,nme,iKo,dKo,Zy,L9e,cc,Hv,sme,ew,cKo,lme,fKo,B9e,_r,ow,mKo,fc,gKo,ime,hKo,pKo,dme,_Ko,uKo,bKo,rw,vKo,cme,TKo,FKo,CKo,dt,tw,MKo,fme,EKo,yKo,mc,wKo,mme,AKo,LKo,gme,BKo,kKo,xKo,hme,RKo,SKo,aw,PKo,_o,nw,$Ko,pme,IKo,jKo,fn,NKo,_me,DKo,qKo,ume,GKo,OKo,bme,XKo,zKo,VKo,vme,Uv,Tme,WKo,QKo,nq,HKo,UKo,JKo,Fme,YKo,KKo,sw,k9e,gc,Jv,Cme,lw,ZKo,Mme,eZo,x9e,ur,iw,oZo,hc,rZo,Eme,tZo,aZo,yme,nZo,sZo,lZo,dw,iZo,wme,dZo,cZo,fZo,ct,cw,mZo,Ame,gZo,hZo,pc,pZo,Lme,_Zo,uZo,Bme,bZo,vZo,TZo,kme,FZo,CZo,fw,MZo,uo,mw,EZo,xme,yZo,wZo,mn,AZo,Rme,LZo,BZo,Sme,kZo,xZo,Pme,RZo,SZo,PZo,Y,Yv,$me,$Zo,IZo,sq,jZo,NZo,DZo,Kv,Ime,qZo,GZo,lq,OZo,XZo,zZo,Zv,jme,VZo,WZo,iq,QZo,HZo,UZo,e0,Nme,JZo,YZo,dq,KZo,ZZo,eer,o0,Dme,oer,rer,cq,ter,aer,ner,r0,qme,ser,ler,fq,ier,der,cer,t0,Gme,fer,mer,mq,ger,her,per,a0,Ome,_er,uer,gq,ber,ver,Ter,n0,Xme,Fer,Cer,hq,Mer,Eer,yer,s0,zme,wer,Aer,pq,Ler,Ber,ker,l0,Vme,xer,Rer,_q,Ser,Per,$er,i0,Wme,Ier,jer,uq,Ner,Der,qer,d0,Qme,Ger,Oer,bq,Xer,zer,Ver,c0,Hme,Wer,Qer,vq,Her,Uer,Jer,f0,Ume,Yer,Ker,Tq,Zer,eor,oor,m0,Jme,ror,tor,Fq,aor,nor,sor,g0,Yme,lor,ior,Cq,dor,cor,mor,h0,Kme,gor,hor,Mq,por,_or,uor,p0,Zme,bor,vor,Eq,Tor,For,Cor,_0,ege,Mor,Eor,yq,yor,wor,Aor,oge,Lor,Bor,gw,R9e,_c,u0,rge,hw,kor,tge,xor,S9e,br,pw,Ror,uc,Sor,age,Por,$or,nge,Ior,jor,Nor,_w,Dor,sge,qor,Gor,Oor,ft,uw,Xor,lge,zor,Vor,bc,Wor,ige,Qor,Hor,dge,Uor,Jor,Yor,cge,Kor,Zor,bw,err,bo,vw,orr,fge,rrr,trr,gn,arr,mge,nrr,srr,gge,lrr,irr,hge,drr,crr,frr,_e,b0,pge,mrr,grr,wq,hrr,prr,_rr,v0,_ge,urr,brr,Aq,vrr,Trr,Frr,T0,uge,Crr,Mrr,Lq,Err,yrr,wrr,F0,bge,Arr,Lrr,Bq,Brr,krr,xrr,C0,vge,Rrr,Srr,kq,Prr,$rr,Irr,M0,Tge,jrr,Nrr,xq,Drr,qrr,Grr,E0,Fge,Orr,Xrr,Rq,zrr,Vrr,Wrr,y0,Cge,Qrr,Hrr,Sq,Urr,Jrr,Yrr,w0,Mge,Krr,Zrr,Pq,etr,otr,rtr,A0,Ege,ttr,atr,$q,ntr,str,ltr,yge,itr,dtr,Tw,P9e,vc,L0,wge,Fw,ctr,Age,ftr,$9e,vr,Cw,mtr,Tc,gtr,Lge,htr,ptr,Bge,_tr,utr,btr,Mw,vtr,kge,Ttr,Ftr,Ctr,mt,Ew,Mtr,xge,Etr,ytr,Fc,wtr,Rge,Atr,Ltr,Sge,Btr,ktr,xtr,Pge,Rtr,Str,yw,Ptr,vo,ww,$tr,$ge,Itr,jtr,hn,Ntr,Ige,Dtr,qtr,jge,Gtr,Otr,Nge,Xtr,ztr,Vtr,X,B0,Dge,Wtr,Qtr,Iq,Htr,Utr,Jtr,k0,qge,Ytr,Ktr,jq,Ztr,ear,oar,x0,Gge,rar,tar,Nq,aar,nar,sar,R0,Oge,lar,iar,Dq,dar,car,far,S0,Xge,mar,gar,qq,har,par,_ar,P0,zge,uar,bar,Gq,Tar,Far,Car,$0,Vge,Mar,Ear,Oq,yar,war,Aar,I0,Wge,Lar,Bar,Xq,kar,xar,Rar,j0,Qge,Sar,Par,zq,$ar,Iar,jar,N0,Hge,Nar,Dar,Vq,qar,Gar,Oar,D0,Uge,Xar,zar,Wq,Var,War,Qar,q0,Jge,Har,Uar,Qq,Jar,Yar,Kar,G0,Yge,Zar,enr,Hq,onr,rnr,tnr,O0,Kge,anr,nnr,Uq,snr,lnr,inr,X0,Zge,dnr,cnr,Jq,fnr,mnr,gnr,z0,ehe,hnr,pnr,Yq,_nr,unr,bnr,V0,ohe,vnr,Tnr,Kq,Fnr,Cnr,Mnr,W0,rhe,Enr,ynr,Zq,wnr,Anr,Lnr,Q0,the,Bnr,knr,eG,xnr,Rnr,Snr,H0,ahe,Pnr,$nr,oG,Inr,jnr,Nnr,U0,nhe,Dnr,qnr,rG,Gnr,Onr,Xnr,J0,she,znr,Vnr,tG,Wnr,Qnr,Hnr,Y0,lhe,Unr,Jnr,aG,Ynr,Knr,Znr,K0,ihe,esr,osr,nG,rsr,tsr,asr,Z0,dhe,nsr,ssr,sG,lsr,isr,dsr,che,csr,fsr,Aw,I9e,Cc,eT,fhe,Lw,msr,mhe,gsr,j9e,Tr,Bw,hsr,Mc,psr,ghe,_sr,usr,hhe,bsr,vsr,Tsr,kw,Fsr,phe,Csr,Msr,Esr,gt,xw,ysr,_he,wsr,Asr,Ec,Lsr,uhe,Bsr,ksr,bhe,xsr,Rsr,Ssr,vhe,Psr,$sr,Rw,Isr,To,Sw,jsr,The,Nsr,Dsr,pn,qsr,Fhe,Gsr,Osr,Che,Xsr,zsr,Mhe,Vsr,Wsr,Qsr,te,oT,Ehe,Hsr,Usr,lG,Jsr,Ysr,Ksr,rT,yhe,Zsr,elr,iG,olr,rlr,tlr,tT,whe,alr,nlr,dG,slr,llr,ilr,aT,Ahe,dlr,clr,cG,flr,mlr,glr,nT,Lhe,hlr,plr,fG,_lr,ulr,blr,sT,Bhe,vlr,Tlr,mG,Flr,Clr,Mlr,lT,khe,Elr,ylr,gG,wlr,Alr,Llr,iT,xhe,Blr,klr,hG,xlr,Rlr,Slr,dT,Rhe,Plr,$lr,pG,Ilr,jlr,Nlr,cT,She,Dlr,qlr,_G,Glr,Olr,Xlr,fT,Phe,zlr,Vlr,uG,Wlr,Qlr,Hlr,mT,$he,Ulr,Jlr,bG,Ylr,Klr,Zlr,gT,Ihe,eir,oir,vG,rir,tir,air,hT,jhe,nir,sir,TG,lir,iir,dir,pT,Nhe,cir,fir,FG,mir,gir,hir,_T,Dhe,pir,_ir,CG,uir,bir,vir,uT,qhe,Tir,Fir,MG,Cir,Mir,Eir,Ghe,yir,wir,Pw,N9e,yc,bT,Ohe,$w,Air,Xhe,Lir,D9e,Fr,Iw,Bir,wc,kir,zhe,xir,Rir,Vhe,Sir,Pir,$ir,jw,Iir,Whe,jir,Nir,Dir,ht,Nw,qir,Qhe,Gir,Oir,Ac,Xir,Hhe,zir,Vir,Uhe,Wir,Qir,Hir,Jhe,Uir,Jir,Dw,Yir,Fo,qw,Kir,Yhe,Zir,edr,_n,odr,Khe,rdr,tdr,Zhe,adr,ndr,epe,sdr,ldr,idr,ope,vT,rpe,ddr,cdr,EG,fdr,mdr,gdr,tpe,hdr,pdr,Gw,q9e,Lc,TT,ape,Ow,_dr,npe,udr,G9e,Cr,Xw,bdr,Bc,vdr,spe,Tdr,Fdr,lpe,Cdr,Mdr,Edr,zw,ydr,ipe,wdr,Adr,Ldr,pt,Vw,Bdr,dpe,kdr,xdr,kc,Rdr,cpe,Sdr,Pdr,fpe,$dr,Idr,jdr,mpe,Ndr,Ddr,Ww,qdr,Co,Qw,Gdr,gpe,Odr,Xdr,un,zdr,hpe,Vdr,Wdr,ppe,Qdr,Hdr,_pe,Udr,Jdr,Ydr,K,FT,upe,Kdr,Zdr,yG,ecr,ocr,rcr,CT,bpe,tcr,acr,wG,ncr,scr,lcr,MT,vpe,icr,dcr,AG,ccr,fcr,mcr,ET,Tpe,gcr,hcr,LG,pcr,_cr,ucr,yT,Fpe,bcr,vcr,BG,Tcr,Fcr,Ccr,wT,Cpe,Mcr,Ecr,kG,ycr,wcr,Acr,AT,Mpe,Lcr,Bcr,xG,kcr,xcr,Rcr,LT,Epe,Scr,Pcr,RG,$cr,Icr,jcr,BT,ype,Ncr,Dcr,SG,qcr,Gcr,Ocr,kT,wpe,Xcr,zcr,PG,Vcr,Wcr,Qcr,xT,Ape,Hcr,Ucr,$G,Jcr,Ycr,Kcr,RT,Lpe,Zcr,efr,IG,ofr,rfr,tfr,ST,Bpe,afr,nfr,jG,sfr,lfr,ifr,PT,kpe,dfr,cfr,NG,ffr,mfr,gfr,$T,xpe,hfr,pfr,DG,_fr,ufr,bfr,IT,Rpe,vfr,Tfr,qG,Ffr,Cfr,Mfr,jT,Spe,Efr,yfr,GG,wfr,Afr,Lfr,NT,Ppe,Bfr,kfr,OG,xfr,Rfr,Sfr,DT,$pe,Pfr,$fr,XG,Ifr,jfr,Nfr,qT,Ipe,Dfr,qfr,zG,Gfr,Ofr,Xfr,jpe,zfr,Vfr,Hw,O9e,xc,GT,Npe,Uw,Wfr,Dpe,Qfr,X9e,Mr,Jw,Hfr,Rc,Ufr,qpe,Jfr,Yfr,Gpe,Kfr,Zfr,emr,Yw,omr,Ope,rmr,tmr,amr,_t,Kw,nmr,Xpe,smr,lmr,Sc,imr,zpe,dmr,cmr,Vpe,fmr,mmr,gmr,Wpe,hmr,pmr,Zw,_mr,Mo,e6,umr,Qpe,bmr,vmr,bn,Tmr,Hpe,Fmr,Cmr,Upe,Mmr,Emr,Jpe,ymr,wmr,Amr,Z,OT,Ype,Lmr,Bmr,VG,kmr,xmr,Rmr,XT,Kpe,Smr,Pmr,WG,$mr,Imr,jmr,zT,Zpe,Nmr,Dmr,QG,qmr,Gmr,Omr,VT,e_e,Xmr,zmr,HG,Vmr,Wmr,Qmr,WT,o_e,Hmr,Umr,UG,Jmr,Ymr,Kmr,QT,r_e,Zmr,egr,JG,ogr,rgr,tgr,HT,t_e,agr,ngr,YG,sgr,lgr,igr,UT,a_e,dgr,cgr,KG,fgr,mgr,ggr,JT,n_e,hgr,pgr,ZG,_gr,ugr,bgr,YT,s_e,vgr,Tgr,eO,Fgr,Cgr,Mgr,KT,l_e,Egr,ygr,oO,wgr,Agr,Lgr,ZT,i_e,Bgr,kgr,rO,xgr,Rgr,Sgr,eF,d_e,Pgr,$gr,tO,Igr,jgr,Ngr,oF,c_e,Dgr,qgr,aO,Ggr,Ogr,Xgr,rF,f_e,zgr,Vgr,nO,Wgr,Qgr,Hgr,tF,m_e,Ugr,Jgr,sO,Ygr,Kgr,Zgr,aF,g_e,ehr,ohr,lO,rhr,thr,ahr,nF,h_e,nhr,shr,iO,lhr,ihr,dhr,sF,p_e,chr,fhr,dO,mhr,ghr,hhr,__e,phr,_hr,o6,z9e,Pc,lF,u_e,r6,uhr,b_e,bhr,V9e,Er,t6,vhr,$c,Thr,v_e,Fhr,Chr,T_e,Mhr,Ehr,yhr,a6,whr,F_e,Ahr,Lhr,Bhr,ut,n6,khr,C_e,xhr,Rhr,Ic,Shr,M_e,Phr,$hr,E_e,Ihr,jhr,Nhr,y_e,Dhr,qhr,s6,Ghr,Eo,l6,Ohr,w_e,Xhr,zhr,vn,Vhr,A_e,Whr,Qhr,L_e,Hhr,Uhr,B_e,Jhr,Yhr,Khr,k_e,iF,x_e,Zhr,epr,cO,opr,rpr,tpr,R_e,apr,npr,i6,W9e,jc,dF,S_e,d6,spr,P_e,lpr,Q9e,yr,c6,ipr,Nc,dpr,$_e,cpr,fpr,I_e,mpr,gpr,hpr,f6,ppr,j_e,_pr,upr,bpr,bt,m6,vpr,N_e,Tpr,Fpr,Dc,Cpr,D_e,Mpr,Epr,q_e,ypr,wpr,Apr,G_e,Lpr,Bpr,g6,kpr,yo,h6,xpr,O_e,Rpr,Spr,Tn,Ppr,X_e,$pr,Ipr,z_e,jpr,Npr,V_e,Dpr,qpr,Gpr,W_e,cF,Q_e,Opr,Xpr,fO,zpr,Vpr,Wpr,H_e,Qpr,Hpr,p6,H9e,qc,fF,U_e,_6,Upr,J_e,Jpr,U9e,wr,u6,Ypr,Gc,Kpr,Y_e,Zpr,e_r,K_e,o_r,r_r,t_r,b6,a_r,Z_e,n_r,s_r,l_r,vt,v6,i_r,eue,d_r,c_r,Oc,f_r,oue,m_r,g_r,rue,h_r,p_r,__r,tue,u_r,b_r,T6,v_r,wo,F6,T_r,aue,F_r,C_r,Fn,M_r,nue,E_r,y_r,sue,w_r,A_r,lue,L_r,B_r,k_r,V,mF,iue,x_r,R_r,mO,S_r,P_r,$_r,gF,due,I_r,j_r,gO,N_r,D_r,q_r,hF,cue,G_r,O_r,hO,X_r,z_r,V_r,pF,fue,W_r,Q_r,pO,H_r,U_r,J_r,_F,mue,Y_r,K_r,_O,Z_r,eur,our,uF,gue,rur,tur,uO,aur,nur,sur,bF,hue,lur,iur,bO,dur,cur,fur,vF,pue,mur,gur,vO,hur,pur,_ur,TF,_ue,uur,bur,TO,vur,Tur,Fur,FF,uue,Cur,Mur,FO,Eur,yur,wur,CF,bue,Aur,Lur,CO,Bur,kur,xur,MF,vue,Rur,Sur,MO,Pur,$ur,Iur,EF,Tue,jur,Nur,EO,Dur,qur,Gur,yF,Fue,Our,Xur,yO,zur,Vur,Wur,wF,Cue,Qur,Hur,wO,Uur,Jur,Yur,AF,Mue,Kur,Zur,AO,e1r,o1r,r1r,LF,Eue,t1r,a1r,LO,n1r,s1r,l1r,BF,yue,i1r,d1r,BO,c1r,f1r,m1r,kF,wue,g1r,h1r,kO,p1r,_1r,u1r,xF,Aue,b1r,v1r,xO,T1r,F1r,C1r,RF,Lue,M1r,E1r,RO,y1r,w1r,A1r,SF,Bue,L1r,B1r,SO,k1r,x1r,R1r,PF,kue,S1r,P1r,PO,$1r,I1r,j1r,$F,xue,N1r,D1r,$O,q1r,G1r,O1r,Rue,X1r,z1r,C6,J9e,Xc,IF,Sue,M6,V1r,Pue,W1r,Y9e,Ar,E6,Q1r,zc,H1r,$ue,U1r,J1r,Iue,Y1r,K1r,Z1r,y6,e7r,jue,o7r,r7r,t7r,Tt,w6,a7r,Nue,n7r,s7r,Vc,l7r,Due,i7r,d7r,que,c7r,f7r,m7r,Gue,g7r,h7r,A6,p7r,Ao,L6,_7r,Oue,u7r,b7r,Cn,v7r,Xue,T7r,F7r,zue,C7r,M7r,Vue,E7r,y7r,w7r,Mn,jF,Wue,A7r,L7r,IO,B7r,k7r,x7r,NF,Que,R7r,S7r,jO,P7r,$7r,I7r,DF,Hue,j7r,N7r,NO,D7r,q7r,G7r,qF,Uue,O7r,X7r,DO,z7r,V7r,W7r,Jue,Q7r,H7r,B6,K9e,Wc,GF,Yue,k6,U7r,Kue,J7r,Z9e,Lr,x6,Y7r,Qc,K7r,Zue,Z7r,ebr,e1e,obr,rbr,tbr,R6,abr,o1e,nbr,sbr,lbr,Ft,S6,ibr,r1e,dbr,cbr,Hc,fbr,t1e,mbr,gbr,a1e,hbr,pbr,_br,n1e,ubr,bbr,P6,vbr,Lo,$6,Tbr,s1e,Fbr,Cbr,En,Mbr,l1e,Ebr,ybr,i1e,wbr,Abr,d1e,Lbr,Bbr,kbr,fe,OF,c1e,xbr,Rbr,qO,Sbr,Pbr,$br,XF,f1e,Ibr,jbr,GO,Nbr,Dbr,qbr,zF,m1e,Gbr,Obr,OO,Xbr,zbr,Vbr,VF,g1e,Wbr,Qbr,XO,Hbr,Ubr,Jbr,WF,h1e,Ybr,Kbr,zO,Zbr,e5r,o5r,QF,p1e,r5r,t5r,VO,a5r,n5r,s5r,HF,_1e,l5r,i5r,WO,d5r,c5r,f5r,UF,u1e,m5r,g5r,QO,h5r,p5r,_5r,JF,b1e,u5r,b5r,HO,v5r,T5r,F5r,YF,v1e,C5r,M5r,UO,E5r,y5r,w5r,KF,T1e,A5r,L5r,JO,B5r,k5r,x5r,F1e,R5r,S5r,I6,eBe,Uc,ZF,C1e,j6,P5r,M1e,$5r,oBe,Br,N6,I5r,Jc,j5r,E1e,N5r,D5r,y1e,q5r,G5r,O5r,D6,X5r,w1e,z5r,V5r,W5r,Ct,q6,Q5r,A1e,H5r,U5r,Yc,J5r,L1e,Y5r,K5r,B1e,Z5r,e2r,o2r,k1e,r2r,t2r,G6,a2r,Bo,O6,n2r,x1e,s2r,l2r,yn,i2r,R1e,d2r,c2r,S1e,f2r,m2r,P1e,g2r,h2r,p2r,ve,eC,$1e,_2r,u2r,YO,b2r,v2r,T2r,oC,I1e,F2r,C2r,KO,M2r,E2r,y2r,rC,j1e,w2r,A2r,ZO,L2r,B2r,k2r,tC,N1e,x2r,R2r,eX,S2r,P2r,$2r,aC,D1e,I2r,j2r,oX,N2r,D2r,q2r,nC,q1e,G2r,O2r,rX,X2r,z2r,V2r,sC,G1e,W2r,Q2r,tX,H2r,U2r,J2r,lC,O1e,Y2r,K2r,aX,Z2r,evr,ovr,iC,X1e,rvr,tvr,nX,avr,nvr,svr,z1e,lvr,ivr,X6,rBe,Kc,dC,V1e,z6,dvr,W1e,cvr,tBe,kr,V6,fvr,Zc,mvr,Q1e,gvr,hvr,H1e,pvr,_vr,uvr,W6,bvr,U1e,vvr,Tvr,Fvr,Mt,Q6,Cvr,J1e,Mvr,Evr,ef,yvr,Y1e,wvr,Avr,K1e,Lvr,Bvr,kvr,Z1e,xvr,Rvr,H6,Svr,ko,U6,Pvr,e7e,$vr,Ivr,wn,jvr,o7e,Nvr,Dvr,r7e,qvr,Gvr,t7e,Ovr,Xvr,zvr,Te,cC,a7e,Vvr,Wvr,sX,Qvr,Hvr,Uvr,fC,n7e,Jvr,Yvr,lX,Kvr,Zvr,e0r,mC,s7e,o0r,r0r,iX,t0r,a0r,n0r,gC,l7e,s0r,l0r,dX,i0r,d0r,c0r,hC,i7e,f0r,m0r,cX,g0r,h0r,p0r,pC,d7e,_0r,u0r,fX,b0r,v0r,T0r,_C,c7e,F0r,C0r,mX,M0r,E0r,y0r,uC,f7e,w0r,A0r,gX,L0r,B0r,k0r,bC,m7e,x0r,R0r,hX,S0r,P0r,$0r,g7e,I0r,j0r,J6,aBe,of,vC,h7e,Y6,N0r,p7e,D0r,nBe,xr,K6,q0r,rf,G0r,_7e,O0r,X0r,u7e,z0r,V0r,W0r,Z6,Q0r,b7e,H0r,U0r,J0r,Et,eA,Y0r,v7e,K0r,Z0r,tf,eTr,T7e,oTr,rTr,F7e,tTr,aTr,nTr,C7e,sTr,lTr,oA,iTr,xo,rA,dTr,M7e,cTr,fTr,An,mTr,E7e,gTr,hTr,y7e,pTr,_Tr,w7e,uTr,bTr,vTr,Fe,TC,A7e,TTr,FTr,pX,CTr,MTr,ETr,FC,L7e,yTr,wTr,_X,ATr,LTr,BTr,CC,B7e,kTr,xTr,uX,RTr,STr,PTr,MC,k7e,$Tr,ITr,bX,jTr,NTr,DTr,EC,x7e,qTr,GTr,vX,OTr,XTr,zTr,yC,R7e,VTr,WTr,TX,QTr,HTr,UTr,wC,S7e,JTr,YTr,FX,KTr,ZTr,eFr,AC,P7e,oFr,rFr,CX,tFr,aFr,nFr,LC,$7e,sFr,lFr,MX,iFr,dFr,cFr,I7e,fFr,mFr,tA,sBe,af,BC,j7e,aA,gFr,N7e,hFr,lBe,Rr,nA,pFr,nf,_Fr,D7e,uFr,bFr,q7e,vFr,TFr,FFr,sA,CFr,G7e,MFr,EFr,yFr,yt,lA,wFr,O7e,AFr,LFr,sf,BFr,X7e,kFr,xFr,z7e,RFr,SFr,PFr,V7e,$Fr,IFr,iA,jFr,Ro,dA,NFr,W7e,DFr,qFr,Ln,GFr,Q7e,OFr,XFr,H7e,zFr,VFr,U7e,WFr,QFr,HFr,Ce,kC,J7e,UFr,JFr,EX,YFr,KFr,ZFr,xC,Y7e,eCr,oCr,yX,rCr,tCr,aCr,RC,K7e,nCr,sCr,wX,lCr,iCr,dCr,SC,Z7e,cCr,fCr,AX,mCr,gCr,hCr,PC,ebe,pCr,_Cr,LX,uCr,bCr,vCr,$C,obe,TCr,FCr,BX,CCr,MCr,ECr,IC,rbe,yCr,wCr,kX,ACr,LCr,BCr,jC,tbe,kCr,xCr,xX,RCr,SCr,PCr,NC,abe,$Cr,ICr,RX,jCr,NCr,DCr,nbe,qCr,GCr,cA,iBe,lf,DC,sbe,fA,OCr,lbe,XCr,dBe,Sr,mA,zCr,df,VCr,ibe,WCr,QCr,dbe,HCr,UCr,JCr,gA,YCr,cbe,KCr,ZCr,e4r,wt,hA,o4r,fbe,r4r,t4r,cf,a4r,mbe,n4r,s4r,gbe,l4r,i4r,d4r,hbe,c4r,f4r,pA,m4r,So,_A,g4r,pbe,h4r,p4r,Bn,_4r,_be,u4r,b4r,ube,v4r,T4r,bbe,F4r,C4r,M4r,so,qC,vbe,E4r,y4r,SX,w4r,A4r,L4r,GC,Tbe,B4r,k4r,PX,x4r,R4r,S4r,OC,Fbe,P4r,$4r,$X,I4r,j4r,N4r,XC,Cbe,D4r,q4r,IX,G4r,O4r,X4r,zC,Mbe,z4r,V4r,jX,W4r,Q4r,H4r,VC,Ebe,U4r,J4r,NX,Y4r,K4r,Z4r,WC,ybe,eMr,oMr,DX,rMr,tMr,aMr,wbe,nMr,sMr,uA,cBe,ff,QC,Abe,bA,lMr,Lbe,iMr,fBe,Pr,vA,dMr,mf,cMr,Bbe,fMr,mMr,kbe,gMr,hMr,pMr,TA,_Mr,xbe,uMr,bMr,vMr,At,FA,TMr,Rbe,FMr,CMr,gf,MMr,Sbe,EMr,yMr,Pbe,wMr,AMr,LMr,$be,BMr,kMr,CA,xMr,Po,MA,RMr,Ibe,SMr,PMr,kn,$Mr,jbe,IMr,jMr,Nbe,NMr,DMr,Dbe,qMr,GMr,OMr,lo,HC,qbe,XMr,zMr,qX,VMr,WMr,QMr,UC,Gbe,HMr,UMr,GX,JMr,YMr,KMr,JC,Obe,ZMr,eEr,OX,oEr,rEr,tEr,YC,Xbe,aEr,nEr,XX,sEr,lEr,iEr,KC,zbe,dEr,cEr,zX,fEr,mEr,gEr,ZC,Vbe,hEr,pEr,VX,_Er,uEr,bEr,e4,Wbe,vEr,TEr,WX,FEr,CEr,MEr,Qbe,EEr,yEr,EA,mBe,hf,o4,Hbe,yA,wEr,Ube,AEr,gBe,$r,wA,LEr,pf,BEr,Jbe,kEr,xEr,Ybe,REr,SEr,PEr,AA,$Er,Kbe,IEr,jEr,NEr,Lt,LA,DEr,Zbe,qEr,GEr,_f,OEr,e5e,XEr,zEr,o5e,VEr,WEr,QEr,r5e,HEr,UEr,BA,JEr,$o,kA,YEr,t5e,KEr,ZEr,xn,e3r,a5e,o3r,r3r,n5e,t3r,a3r,s5e,n3r,s3r,l3r,l5e,r4,i5e,i3r,d3r,QX,c3r,f3r,m3r,d5e,g3r,h3r,xA,hBe,uf,t4,c5e,RA,p3r,f5e,_3r,pBe,Ir,SA,u3r,bf,b3r,m5e,v3r,T3r,g5e,F3r,C3r,M3r,PA,E3r,h5e,y3r,w3r,A3r,Bt,$A,L3r,p5e,B3r,k3r,vf,x3r,_5e,R3r,S3r,u5e,P3r,$3r,I3r,b5e,j3r,N3r,IA,D3r,Io,jA,q3r,v5e,G3r,O3r,Rn,X3r,T5e,z3r,V3r,F5e,W3r,Q3r,C5e,H3r,U3r,J3r,NA,a4,M5e,Y3r,K3r,HX,Z3r,eyr,oyr,n4,E5e,ryr,tyr,UX,ayr,nyr,syr,y5e,lyr,iyr,DA,_Be,Tf,s4,w5e,qA,dyr,A5e,cyr,uBe,jr,GA,fyr,Ff,myr,L5e,gyr,hyr,B5e,pyr,_yr,uyr,OA,byr,k5e,vyr,Tyr,Fyr,kt,XA,Cyr,x5e,Myr,Eyr,Cf,yyr,R5e,wyr,Ayr,S5e,Lyr,Byr,kyr,P5e,xyr,Ryr,zA,Syr,jo,VA,Pyr,$5e,$yr,Iyr,Sn,jyr,I5e,Nyr,Dyr,j5e,qyr,Gyr,N5e,Oyr,Xyr,zyr,D5e,l4,q5e,Vyr,Wyr,JX,Qyr,Hyr,Uyr,G5e,Jyr,Yyr,WA,bBe;return ce=new z({}),$a=new w({props:{code:'model = AutoModel.from_pretrained("bert-base-cased"),',highlighted:'model = AutoModel.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)'}}),dM=new z({}),cM=new w({props:{code:`from transformers import AutoConfig, AutoModel

AutoConfig.register("new-model", NewModelConfig)
AutoModel.register(NewModelConfig, NewModel),`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModel

AutoConfig.register(<span class="hljs-string">&quot;new-model&quot;</span>, NewModelConfig)
AutoModel.register(NewModelConfig, NewModel)`}}),Bf=new Kyr({props:{warning:"&lcub;true}",$$slots:{default:[fut]},$$scope:{ctx:yi}}}),fM=new z({}),mM=new E({props:{name:"class transformers.AutoConfig",anchor:"transformers.AutoConfig",parameters:[],source:"https://github.com/huggingface/transformers/blob/pr_15770/src/transformers/models/auto/configuration_auto.py#L518"}}),pM=new E({props:{name:"from_pretrained",anchor:"transformers.AutoConfig.from_pretrained",parameters:[{name:"pretrained_model_name_or_path",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15770/src/transformers/models/auto/configuration_auto.py#L541",parametersDescription:[{anchor:"transformers.AutoConfig.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model configuration hosted inside a model repo on
huggingface.co. Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or
namespaced under a user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing a configuration file saved using the
<a href="/docs/transformers/pr_15770/en/main_classes/configuration#transformers.PretrainedConfig.save_pretrained">save_pretrained()</a> method, or the <a href="/docs/transformers/pr_15770/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> method,
e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a saved configuration JSON <em>file</em>, e.g.,
<code>./my_model_directory/configuration.json</code>.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.AutoConfig.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.AutoConfig.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download the model weights and configuration files and override the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.AutoConfig.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.AutoConfig.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.AutoConfig.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.AutoConfig.from_pretrained.return_unused_kwargs",description:`<strong>return_unused_kwargs</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
If <code>False</code>, then this function returns just the final configuration object.</p>
<p>If <code>True</code>, then this functions returns a <code>Tuple(config, unused_kwargs)</code> where <em>unused_kwargs</em> is a
dictionary consisting of the key/value pairs whose keys are not configuration attributes: i.e., the
part of <code>kwargs</code> which has not been used to update <code>config</code> and is otherwise ignored.`,name:"return_unused_kwargs"},{anchor:"transformers.AutoConfig.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.AutoConfig.from_pretrained.kwargs(additional",description:`<strong>kwargs(additional</strong> keyword arguments, <em>optional</em>) &#x2014;
The values in kwargs of any keys which are configuration attributes will be used to override the loaded
values. Behavior concerning key/value pairs whose keys are <em>not</em> configuration attributes is controlled
by the <code>return_unused_kwargs</code> keyword parameter.`,name:"kwargs(additional"}]}}),_M=new w({props:{code:`from transformers import AutoConfig

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-uncased")

# Download configuration from huggingface.co (user-uploaded) and cache.
config = AutoConfig.from_pretrained("dbmdz/bert-base-german-cased")

# If configuration file is in a directory (e.g., was saved using *save_pretrained('./test/saved_model/')*).
config = AutoConfig.from_pretrained("./test/bert_saved_model/")

# Load a specific configuration file.
config = AutoConfig.from_pretrained("./test/bert_saved_model/my_configuration.json")

# Change some config attributes when loading a pretrained config.
config = AutoConfig.from_pretrained("bert-base-uncased", output_attentions=True, foo=False)
config.output_attentions

config, unused_kwargs = AutoConfig.from_pretrained(
    "bert-base-uncased", output_attentions=True, foo=False, return_unused_kwargs=True
)
config.output_attentions

config.unused_kwargs,`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-uncased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co (user-uploaded) and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;dbmdz/bert-base-german-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># If configuration file is in a directory (e.g., was saved using *save_pretrained(&#x27;./test/saved_model/&#x27;)*).</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./test/bert_saved_model/&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Load a specific configuration file.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./test/bert_saved_model/my_configuration.json&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Change some config attributes when loading a pretrained config.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-uncased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>, foo=<span class="hljs-literal">False</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>config, unused_kwargs = AutoConfig.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;bert-base-uncased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>, foo=<span class="hljs-literal">False</span>, return_unused_kwargs=<span class="hljs-literal">True</span>
<span class="hljs-meta">... </span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>config.unused_kwargs
{<span class="hljs-string">&#x27;foo&#x27;</span>: <span class="hljs-literal">False</span>}`}}),uM=new E({props:{name:"register",anchor:"transformers.AutoConfig.register",parameters:[{name:"model_type",val:""},{name:"config",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15770/src/transformers/models/auto/configuration_auto.py#L663",parametersDescription:[{anchor:"transformers.AutoConfig.register.model_type",description:"<strong>model_type</strong> (<code>str</code>) &#x2014; The model type like &#x201C;bert&#x201D; or &#x201C;gpt&#x201D;.",name:"model_type"},{anchor:"transformers.AutoConfig.register.config",description:'<strong>config</strong> (<a href="/docs/transformers/pr_15770/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014; The config to register.',name:"config"}]}}),bM=new z({}),vM=new E({props:{name:"class transformers.AutoTokenizer",anchor:"transformers.AutoTokenizer",parameters:[],source:"https://github.com/huggingface/transformers/blob/pr_15770/src/transformers/models/auto/tokenization_auto.py#L351"}}),CM=new E({props:{name:"from_pretrained",anchor:"transformers.AutoTokenizer.from_pretrained",parameters:[{name:"pretrained_model_name_or_path",val:""},{name:"*inputs",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15770/src/transformers/models/auto/tokenization_auto.py#L365",parametersDescription:[{anchor:"transformers.AutoTokenizer.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a predefined tokenizer hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing vocabulary files required by the tokenizer, for instance saved
using the <a href="/docs/transformers/pr_15770/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.save_pretrained">save_pretrained()</a> method, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a single saved vocabulary file if and only if the tokenizer only requires a
single vocabulary file (like Bert or XLNet), e.g.: <code>./my_model_directory/vocab.txt</code>. (Not
applicable to all derived classes)</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.AutoTokenizer.from_pretrained.inputs",description:`<strong>inputs</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the Tokenizer <code>__init__()</code> method.`,name:"inputs"},{anchor:"transformers.AutoTokenizer.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15770/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
The configuration object used to dertermine the tokenizer class to instantiate.`,name:"config"},{anchor:"transformers.AutoTokenizer.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.AutoTokenizer.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download the model weights and configuration files and override the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.AutoTokenizer.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.AutoTokenizer.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.AutoTokenizer.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.AutoTokenizer.from_pretrained.subfolder",description:`<strong>subfolder</strong> (<code>str</code>, <em>optional</em>) &#x2014;
In case the relevant files are located inside a subfolder of the model repo on huggingface.co (e.g. for
facebook/rag-token-base), specify it here.`,name:"subfolder"},{anchor:"transformers.AutoTokenizer.from_pretrained.use_fast",description:`<strong>use_fast</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to try to load the fast version of the tokenizer.`,name:"use_fast"},{anchor:"transformers.AutoTokenizer.from_pretrained.tokenizer_type",description:`<strong>tokenizer_type</strong> (<code>str</code>, <em>optional</em>) &#x2014;
Tokenizer type to be loaded.`,name:"tokenizer_type"},{anchor:"transformers.AutoTokenizer.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.AutoTokenizer.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Will be passed to the Tokenizer <code>__init__()</code> method. Can be used to set special tokens like
<code>bos_token</code>, <code>eos_token</code>, <code>unk_token</code>, <code>sep_token</code>, <code>pad_token</code>, <code>cls_token</code>, <code>mask_token</code>,
<code>additional_special_tokens</code>. See parameters in the <code>__init__()</code> for more details.`,name:"kwargs"}]}}),MM=new w({props:{code:`from transformers import AutoTokenizer

# Download vocabulary from huggingface.co and cache.
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

# Download vocabulary from huggingface.co (user-uploaded) and cache.
tokenizer = AutoTokenizer.from_pretrained("dbmdz/bert-base-german-cased")

# If vocabulary files are in a directory (e.g. tokenizer was saved using *save_pretrained('./test/saved_model/')*)
tokenizer = AutoTokenizer.from_pretrained("./test/bert_saved_model/"),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download vocabulary from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;bert-base-uncased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download vocabulary from huggingface.co (user-uploaded) and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;dbmdz/bert-base-german-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># If vocabulary files are in a directory (e.g. tokenizer was saved using *save_pretrained(&#x27;./test/saved_model/&#x27;)*)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;./test/bert_saved_model/&quot;</span>)`}}),EM=new E({props:{name:"register",anchor:"transformers.AutoTokenizer.register",parameters:[{name:"config_class",val:""},{name:"slow_tokenizer_class",val:" = None"},{name:"fast_tokenizer_class",val:" = None"}],source:"https://github.com/huggingface/transformers/blob/pr_15770/src/transformers/models/auto/tokenization_auto.py#L561",parametersDescription:[{anchor:"transformers.AutoTokenizer.register.config_class",description:`<strong>config_class</strong> (<a href="/docs/transformers/pr_15770/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The configuration corresponding to the model to register.`,name:"config_class"},{anchor:"transformers.AutoTokenizer.register.slow_tokenizer_class",description:`<strong>slow_tokenizer_class</strong> (<code>PretrainedTokenizer</code>, <em>optional</em>) &#x2014;
The slow tokenizer to register.`,name:"slow_tokenizer_class"},{anchor:"transformers.AutoTokenizer.register.slow_tokenizer_class",description:`<strong>slow_tokenizer_class</strong> (<code>PretrainedTokenizerFast</code>, <em>optional</em>) &#x2014;
The fast tokenizer to register.`,name:"slow_tokenizer_class"}]}}),yM=new z({}),wM=new E({props:{name:"class transformers.AutoFeatureExtractor",anchor:"transformers.AutoFeatureExtractor",parameters:[],source:"https://github.com/huggingface/transformers/blob/pr_15770/src/transformers/models/auto/feature_extraction_auto.py#L170"}}),BM=new E({props:{name:"from_pretrained",anchor:"transformers.AutoFeatureExtractor.from_pretrained",parameters:[{name:"pretrained_model_name_or_path",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15770/src/transformers/models/auto/feature_extraction_auto.py#L184",parametersDescription:[{anchor:"transformers.AutoFeatureExtractor.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
This can be either:</p>
<ul>
<li>a string, the <em>model id</em> of a pretrained feature_extractor hosted inside a model repo on
huggingface.co. Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or
namespaced under a user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>a path to a <em>directory</em> containing a feature extractor file saved using the
<a href="/docs/transformers/pr_15770/en/main_classes/feature_extractor#transformers.FeatureExtractionMixin.save_pretrained">save_pretrained()</a> method, e.g.,
<code>./my_model_directory/</code>.</li>
<li>a path or url to a saved feature extractor JSON <em>file</em>, e.g.,
<code>./my_model_directory/preprocessor_config.json</code>.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.AutoFeatureExtractor.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model feature extractor should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.AutoFeatureExtractor.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force to (re-)download the feature extractor files and override the cached versions
if they exist.`,name:"force_download"},{anchor:"transformers.AutoFeatureExtractor.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received file. Attempts to resume the download if such a file
exists.`,name:"resume_download"},{anchor:"transformers.AutoFeatureExtractor.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}.</code> The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.AutoFeatureExtractor.from_pretrained.use_auth_token",description:`<strong>use_auth_token</strong> (<code>str</code> or <em>bool</em>, <em>optional</em>) &#x2014;
The token to use as HTTP bearer authorization for remote files. If <code>True</code>, will use the token generated
when running <code>transformers-cli login</code> (stored in <code>~/.huggingface</code>).`,name:"use_auth_token"},{anchor:"transformers.AutoFeatureExtractor.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.AutoFeatureExtractor.from_pretrained.return_unused_kwargs",description:`<strong>return_unused_kwargs</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
If <code>False</code>, then this function returns just the final feature extractor object. If <code>True</code>, then this
functions returns a <code>Tuple(feature_extractor, unused_kwargs)</code> where <em>unused_kwargs</em> is a dictionary
consisting of the key/value pairs whose keys are not feature extractor attributes: i.e., the part of
<code>kwargs</code> which has not been used to update <code>feature_extractor</code> and is otherwise ignored.`,name:"return_unused_kwargs"},{anchor:"transformers.AutoFeatureExtractor.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.AutoFeatureExtractor.from_pretrained.kwargs",description:`<strong>kwargs</strong> (<code>Dict[str, Any]</code>, <em>optional</em>) &#x2014;
The values in kwargs of any keys which are feature extractor attributes will be used to override the
loaded values. Behavior concerning key/value pairs whose keys are <em>not</em> feature extractor attributes is
controlled by the <code>return_unused_kwargs</code> keyword parameter.`,name:"kwargs"}]}}),ch=new Kyr({props:{$$slots:{default:[mut]},$$scope:{ctx:yi}}}),kM=new w({props:{code:`from transformers import AutoFeatureExtractor

# Download feature extractor from huggingface.co and cache.
feature_extractor = AutoFeatureExtractor.from_pretrained("facebook/wav2vec2-base-960h")

# If feature extractor files are in a directory (e.g. feature extractor was saved using *save_pretrained('./test/saved_model/')*)
feature_extractor = AutoFeatureExtractor.from_pretrained("./test/saved_model/"),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoFeatureExtractor

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download feature extractor from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>feature_extractor = AutoFeatureExtractor.from_pretrained(<span class="hljs-string">&quot;facebook/wav2vec2-base-960h&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># If feature extractor files are in a directory (e.g. feature extractor was saved using *save_pretrained(&#x27;./test/saved_model/&#x27;)*)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>feature_extractor = AutoFeatureExtractor.from_pretrained(<span class="hljs-string">&quot;./test/saved_model/&quot;</span>)`}}),xM=new E({props:{name:"register",anchor:"transformers.AutoFeatureExtractor.register",parameters:[{name:"config_class",val:""},{name:"feature_extractor_class",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15770/src/transformers/models/auto/feature_extraction_auto.py#L311",parametersDescription:[{anchor:"transformers.AutoFeatureExtractor.register.config_class",description:`<strong>config_class</strong> (<a href="/docs/transformers/pr_15770/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The configuration corresponding to the model to register.`,name:"config_class"},{anchor:"transformers.AutoFeatureExtractor.register.feature_extractor_class",description:"<strong>feature_extractor_class</strong> (<code>FeatureExtractorMixin</code>) &#x2014; The feature extractor to register.",name:"feature_extractor_class"}]}}),RM=new z({}),SM=new E({props:{name:"class transformers.AutoProcessor",anchor:"transformers.AutoProcessor",parameters:[],source:"https://github.com/huggingface/transformers/blob/pr_15770/src/transformers/models/auto/processing_auto.py#L71"}}),IM=new E({props:{name:"from_pretrained",anchor:"transformers.AutoProcessor.from_pretrained",parameters:[{name:"pretrained_model_name_or_path",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15770/src/transformers/models/auto/processing_auto.py#L85",parametersDescription:[{anchor:"transformers.AutoProcessor.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
This can be either:</p>
<ul>
<li>a string, the <em>model id</em> of a pretrained feature_extractor hosted inside a model repo on
huggingface.co. Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or
namespaced under a user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>a path to a <em>directory</em> containing a processor files saved using the <code>save_pretrained()</code> method,
e.g., <code>./my_model_directory/</code>.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.AutoProcessor.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model feature extractor should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.AutoProcessor.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force to (re-)download the feature extractor files and override the cached versions
if they exist.`,name:"force_download"},{anchor:"transformers.AutoProcessor.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received file. Attempts to resume the download if such a file
exists.`,name:"resume_download"},{anchor:"transformers.AutoProcessor.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}.</code> The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.AutoProcessor.from_pretrained.use_auth_token",description:`<strong>use_auth_token</strong> (<code>str</code> or <em>bool</em>, <em>optional</em>) &#x2014;
The token to use as HTTP bearer authorization for remote files. If <code>True</code>, will use the token generated
when running <code>transformers-cli login</code> (stored in <code>~/.huggingface</code>).`,name:"use_auth_token"},{anchor:"transformers.AutoProcessor.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.AutoProcessor.from_pretrained.return_unused_kwargs",description:`<strong>return_unused_kwargs</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
If <code>False</code>, then this function returns just the final feature extractor object. If <code>True</code>, then this
functions returns a <code>Tuple(feature_extractor, unused_kwargs)</code> where <em>unused_kwargs</em> is a dictionary
consisting of the key/value pairs whose keys are not feature extractor attributes: i.e., the part of
<code>kwargs</code> which has not been used to update <code>feature_extractor</code> and is otherwise ignored.`,name:"return_unused_kwargs"},{anchor:"transformers.AutoProcessor.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.AutoProcessor.from_pretrained.kwargs",description:`<strong>kwargs</strong> (<code>Dict[str, Any]</code>, <em>optional</em>) &#x2014;
The values in kwargs of any keys which are feature extractor attributes will be used to override the
loaded values. Behavior concerning key/value pairs whose keys are <em>not</em> feature extractor attributes is
controlled by the <code>return_unused_kwargs</code> keyword parameter.`,name:"kwargs"}]}}),Fh=new Kyr({props:{$$slots:{default:[gut]},$$scope:{ctx:yi}}}),jM=new w({props:{code:`from transformers import AutoProcessor

# Download processor from huggingface.co and cache.
processor = AutoProcessor.from_pretrained("facebook/wav2vec2-base-960h")

# If processor files are in a directory (e.g. processor was saved using *save_pretrained('./test/saved_model/')*)
processor = AutoProcessor.from_pretrained("./test/saved_model/"),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoProcessor

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download processor from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>processor = AutoProcessor.from_pretrained(<span class="hljs-string">&quot;facebook/wav2vec2-base-960h&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># If processor files are in a directory (e.g. processor was saved using *save_pretrained(&#x27;./test/saved_model/&#x27;)*)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>processor = AutoProcessor.from_pretrained(<span class="hljs-string">&quot;./test/saved_model/&quot;</span>)`}}),NM=new E({props:{name:"register",anchor:"transformers.AutoProcessor.register",parameters:[{name:"config_class",val:""},{name:"processor_class",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15770/src/transformers/models/auto/processing_auto.py#L238",parametersDescription:[{anchor:"transformers.AutoProcessor.register.config_class",description:`<strong>config_class</strong> (<a href="/docs/transformers/pr_15770/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The configuration corresponding to the model to register.`,name:"config_class"},{anchor:"transformers.AutoProcessor.register.processor_class",description:"<strong>processor_class</strong> (<code>FeatureExtractorMixin</code>) &#x2014; The processor to register.",name:"processor_class"}]}}),DM=new z({}),qM=new E({props:{name:"class transformers.AutoModel",anchor:"transformers.AutoModel",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15770/src/transformers/models/auto/modeling_auto.py#L674"}}),OM=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15770/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15770/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15770/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/albert#transformers.AlbertModel">AlbertModel</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/bart#transformers.BartModel">BartModel</a> (BART model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/beit#transformers.BeitConfig">BeitConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/beit#transformers.BeitModel">BeitModel</a> (BEiT model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/bert#transformers.BertModel">BertModel</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/bert-generation#transformers.BertGenerationConfig">BertGenerationConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/bert-generation#transformers.BertGenerationEncoder">BertGenerationEncoder</a> (Bert Generation model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/big_bird#transformers.BigBirdModel">BigBirdModel</a> (BigBird model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusConfig">BigBirdPegasusConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusModel">BigBirdPegasusModel</a> (BigBirdPegasus model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/blenderbot#transformers.BlenderbotConfig">BlenderbotConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/blenderbot#transformers.BlenderbotModel">BlenderbotModel</a> (Blenderbot model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/blenderbot-small#transformers.BlenderbotSmallConfig">BlenderbotSmallConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/blenderbot-small#transformers.BlenderbotSmallModel">BlenderbotSmallModel</a> (BlenderbotSmall model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/clip#transformers.CLIPConfig">CLIPConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/clip#transformers.CLIPModel">CLIPModel</a> (CLIP model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/ctrl#transformers.CTRLConfig">CTRLConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/ctrl#transformers.CTRLModel">CTRLModel</a> (CTRL model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/camembert#transformers.CamembertModel">CamembertModel</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/canine#transformers.CanineConfig">CanineConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/canine#transformers.CanineModel">CanineModel</a> (Canine model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/convbert#transformers.ConvBertModel">ConvBertModel</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/convnext#transformers.ConvNextConfig">ConvNextConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/convnext#transformers.ConvNextModel">ConvNextModel</a> (ConvNext model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/dpr#transformers.DPRConfig">DPRConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/dpr#transformers.DPRQuestionEncoder">DPRQuestionEncoder</a> (DPR model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/deberta#transformers.DebertaModel">DebertaModel</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/deberta-v2#transformers.DebertaV2Model">DebertaV2Model</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/deit#transformers.DeiTConfig">DeiTConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/deit#transformers.DeiTModel">DeiTModel</a> (DeiT model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/detr#transformers.DetrConfig">DetrConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/detr#transformers.DetrModel">DetrModel</a> (DETR model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/distilbert#transformers.DistilBertModel">DistilBertModel</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/electra#transformers.ElectraModel">ElectraModel</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/fnet#transformers.FNetConfig">FNetConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/fnet#transformers.FNetModel">FNetModel</a> (FNet model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/fsmt#transformers.FSMTConfig">FSMTConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/fsmt#transformers.FSMTModel">FSMTModel</a> (FairSeq Machine-Translation model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/flaubert#transformers.FlaubertModel">FlaubertModel</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/funnel#transformers.FunnelModel">FunnelModel</a> or <a href="/docs/transformers/pr_15770/en/model_doc/funnel#transformers.FunnelBaseModel">FunnelBaseModel</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/gpt2#transformers.GPT2Model">GPT2Model</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/gptj#transformers.GPTJConfig">GPTJConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/gptj#transformers.GPTJModel">GPTJModel</a> (GPT-J model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/gpt_neo#transformers.GPTNeoConfig">GPTNeoConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/gpt_neo#transformers.GPTNeoModel">GPTNeoModel</a> (GPT Neo model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/hubert#transformers.HubertConfig">HubertConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/hubert#transformers.HubertModel">HubertModel</a> (Hubert model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/ibert#transformers.IBertConfig">IBertConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/ibert#transformers.IBertModel">IBertModel</a> (I-BERT model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/imagegpt#transformers.ImageGPTConfig">ImageGPTConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/imagegpt#transformers.ImageGPTModel">ImageGPTModel</a> (ImageGPT model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/led#transformers.LEDConfig">LEDConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/led#transformers.LEDModel">LEDModel</a> (LED model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/layoutlm#transformers.LayoutLMModel">LayoutLMModel</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/layoutlmv2#transformers.LayoutLMv2Config">LayoutLMv2Config</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/layoutlmv2#transformers.LayoutLMv2Model">LayoutLMv2Model</a> (LayoutLMv2 model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/longformer#transformers.LongformerModel">LongformerModel</a> (Longformer model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/luke#transformers.LukeConfig">LukeConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/luke#transformers.LukeModel">LukeModel</a> (LUKE model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/lxmert#transformers.LxmertConfig">LxmertConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/lxmert#transformers.LxmertModel">LxmertModel</a> (LXMERT model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/m2m_100#transformers.M2M100Config">M2M100Config</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/m2m_100#transformers.M2M100Model">M2M100Model</a> (M2M100 model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/mbart#transformers.MBartModel">MBartModel</a> (mBART model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/mpnet#transformers.MPNetModel">MPNetModel</a> (MPNet model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/mt5#transformers.MT5Config">MT5Config</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/mt5#transformers.MT5Model">MT5Model</a> (mT5 model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/marian#transformers.MarianConfig">MarianConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/marian#transformers.MarianModel">MarianModel</a> (Marian model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/megatron-bert#transformers.MegatronBertConfig">MegatronBertConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/megatron-bert#transformers.MegatronBertModel">MegatronBertModel</a> (MegatronBert model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/mobilebert#transformers.MobileBertModel">MobileBertModel</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/nystromformer#transformers.NystromformerConfig">NystromformerConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/nystromformer#transformers.NystromformerModel">NystromformerModel</a> (Nystromformer model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig">OpenAIGPTConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/openai-gpt#transformers.OpenAIGPTModel">OpenAIGPTModel</a> (OpenAI GPT model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/plbart#transformers.PLBartConfig">PLBartConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/plbart#transformers.PLBartModel">PLBartModel</a> (PLBart model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/pegasus#transformers.PegasusConfig">PegasusConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/pegasus#transformers.PegasusModel">PegasusModel</a> (Pegasus model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/perceiver#transformers.PerceiverConfig">PerceiverConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/perceiver#transformers.PerceiverModel">PerceiverModel</a> (Perceiver model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/poolformer#transformers.PoolFormerConfig">PoolFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/poolformer#transformers.PoolFormerModel">PoolFormerModel</a> (PoolFormer model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/prophetnet#transformers.ProphetNetConfig">ProphetNetConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/prophetnet#transformers.ProphetNetModel">ProphetNetModel</a> (ProphetNet model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/qdqbert#transformers.QDQBertConfig">QDQBertConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/qdqbert#transformers.QDQBertModel">QDQBertModel</a> (QDQBert model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/reformer#transformers.ReformerConfig">ReformerConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/reformer#transformers.ReformerModel">ReformerModel</a> (Reformer model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/rembert#transformers.RemBertModel">RemBertModel</a> (RemBERT model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/resnet#transformers.ResNetConfig">ResNetConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/resnet#transformers.ResNetModel">ResNetModel</a> (resnet model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/retribert#transformers.RetriBertConfig">RetriBertConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/retribert#transformers.RetriBertModel">RetriBertModel</a> (RetriBERT model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/roformer#transformers.RoFormerModel">RoFormerModel</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/roberta#transformers.RobertaModel">RobertaModel</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/sew#transformers.SEWConfig">SEWConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/sew#transformers.SEWModel">SEWModel</a> (SEW model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/sew-d#transformers.SEWDConfig">SEWDConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/sew-d#transformers.SEWDModel">SEWDModel</a> (SEW-D model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/segformer#transformers.SegformerConfig">SegformerConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/segformer#transformers.SegformerModel">SegformerModel</a> (SegFormer model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/speech_to_text#transformers.Speech2TextConfig">Speech2TextConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/speech_to_text#transformers.Speech2TextModel">Speech2TextModel</a> (Speech2Text model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/splinter#transformers.SplinterConfig">SplinterConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/splinter#transformers.SplinterModel">SplinterModel</a> (Splinter model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/squeezebert#transformers.SqueezeBertConfig">SqueezeBertConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/squeezebert#transformers.SqueezeBertModel">SqueezeBertModel</a> (SqueezeBERT model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/swin#transformers.SwinConfig">SwinConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/swin#transformers.SwinModel">SwinModel</a> (Swin model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/t5#transformers.T5Config">T5Config</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/t5#transformers.T5Model">T5Model</a> (T5 model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/tapas#transformers.TapasModel">TapasModel</a> (TAPAS model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/transfo-xl#transformers.TransfoXLConfig">TransfoXLConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/transfo-xl#transformers.TransfoXLModel">TransfoXLModel</a> (Transformer-XL model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/unispeech#transformers.UniSpeechConfig">UniSpeechConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/unispeech#transformers.UniSpeechModel">UniSpeechModel</a> (UniSpeech model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/unispeech-sat#transformers.UniSpeechSatConfig">UniSpeechSatConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/unispeech-sat#transformers.UniSpeechSatModel">UniSpeechSatModel</a> (UniSpeechSat model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/vit#transformers.ViTConfig">ViTConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/vit#transformers.ViTModel">ViTModel</a> (ViT model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/vit_mae#transformers.ViTMAEConfig">ViTMAEConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/vit_mae#transformers.ViTMAEModel">ViTMAEModel</a> (ViTMAE model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/vilt#transformers.ViltConfig">ViltConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/vilt#transformers.ViltModel">ViltModel</a> (ViLT model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/vision-text-dual-encoder#transformers.VisionTextDualEncoderConfig">VisionTextDualEncoderConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/vision-text-dual-encoder#transformers.VisionTextDualEncoderModel">VisionTextDualEncoderModel</a> (VisionTextDualEncoder model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/visual_bert#transformers.VisualBertConfig">VisualBertConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/visual_bert#transformers.VisualBertModel">VisualBertModel</a> (VisualBert model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/wav2vec2#transformers.Wav2Vec2Model">Wav2Vec2Model</a> (Wav2Vec2 model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/wavlm#transformers.WavLMConfig">WavLMConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/wavlm#transformers.WavLMModel">WavLMModel</a> (WavLM model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/xglm#transformers.XGLMConfig">XGLMConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/xglm#transformers.XGLMModel">XGLMModel</a> (XGLM model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/xlm#transformers.XLMModel">XLMModel</a> (XLM model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetConfig">XLMProphetNetConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetModel">XLMProphetNetModel</a> (XLMProphetNet model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/xlm-roberta#transformers.XLMRobertaModel">XLMRobertaModel</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLConfig">XLMRobertaXLConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLModel">XLMRobertaXLModel</a> (XLM-RoBERTa-XL model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/xlnet#transformers.XLNetModel">XLNetModel</a> (XLNet model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/yoso#transformers.YosoConfig">YosoConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/yoso#transformers.YosoModel">YosoModel</a> (YOSO model)</li>
</ul>`,name:"config"}]}}),XM=new w({props:{code:`from transformers import AutoConfig, AutoModel

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModel.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModel.from_config(config)`}}),zM=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15770/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15770/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15770/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15770/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15770/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15770/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15770/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),VM=new w({props:{code:`from transformers import AutoConfig, AutoModel

# Download model and configuration from huggingface.co and cache.
model = AutoModel.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModel.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModel.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModel.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModel.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModel.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),WM=new z({}),QM=new E({props:{name:"class transformers.AutoModelForPreTraining",anchor:"transformers.AutoModelForPreTraining",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15770/src/transformers/models/auto/modeling_auto.py#L681"}}),UM=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15770/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15770/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15770/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/albert#transformers.AlbertForPreTraining">AlbertForPreTraining</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/bart#transformers.BartForConditionalGeneration">BartForConditionalGeneration</a> (BART model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/bert#transformers.BertForPreTraining">BertForPreTraining</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/big_bird#transformers.BigBirdForPreTraining">BigBirdForPreTraining</a> (BigBird model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/ctrl#transformers.CTRLConfig">CTRLConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/ctrl#transformers.CTRLLMHeadModel">CTRLLMHeadModel</a> (CTRL model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/camembert#transformers.CamembertForMaskedLM">CamembertForMaskedLM</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/deberta#transformers.DebertaForMaskedLM">DebertaForMaskedLM</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/deberta-v2#transformers.DebertaV2ForMaskedLM">DebertaV2ForMaskedLM</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/distilbert#transformers.DistilBertForMaskedLM">DistilBertForMaskedLM</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/electra#transformers.ElectraForPreTraining">ElectraForPreTraining</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/fnet#transformers.FNetConfig">FNetConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/fnet#transformers.FNetForPreTraining">FNetForPreTraining</a> (FNet model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/fsmt#transformers.FSMTConfig">FSMTConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/fsmt#transformers.FSMTForConditionalGeneration">FSMTForConditionalGeneration</a> (FairSeq Machine-Translation model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/flaubert#transformers.FlaubertWithLMHeadModel">FlaubertWithLMHeadModel</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/funnel#transformers.FunnelForPreTraining">FunnelForPreTraining</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/gpt2#transformers.GPT2LMHeadModel">GPT2LMHeadModel</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/ibert#transformers.IBertConfig">IBertConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/ibert#transformers.IBertForMaskedLM">IBertForMaskedLM</a> (I-BERT model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/layoutlm#transformers.LayoutLMForMaskedLM">LayoutLMForMaskedLM</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/longformer#transformers.LongformerForMaskedLM">LongformerForMaskedLM</a> (Longformer model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/lxmert#transformers.LxmertConfig">LxmertConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/lxmert#transformers.LxmertForPreTraining">LxmertForPreTraining</a> (LXMERT model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/mpnet#transformers.MPNetForMaskedLM">MPNetForMaskedLM</a> (MPNet model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/megatron-bert#transformers.MegatronBertConfig">MegatronBertConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/megatron-bert#transformers.MegatronBertForPreTraining">MegatronBertForPreTraining</a> (MegatronBert model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/mobilebert#transformers.MobileBertForPreTraining">MobileBertForPreTraining</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig">OpenAIGPTConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/openai-gpt#transformers.OpenAIGPTLMHeadModel">OpenAIGPTLMHeadModel</a> (OpenAI GPT model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/retribert#transformers.RetriBertConfig">RetriBertConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/retribert#transformers.RetriBertModel">RetriBertModel</a> (RetriBERT model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/roberta#transformers.RobertaForMaskedLM">RobertaForMaskedLM</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/squeezebert#transformers.SqueezeBertConfig">SqueezeBertConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/squeezebert#transformers.SqueezeBertForMaskedLM">SqueezeBertForMaskedLM</a> (SqueezeBERT model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/t5#transformers.T5Config">T5Config</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/t5#transformers.T5ForConditionalGeneration">T5ForConditionalGeneration</a> (T5 model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/tapas#transformers.TapasForMaskedLM">TapasForMaskedLM</a> (TAPAS model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/transfo-xl#transformers.TransfoXLConfig">TransfoXLConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/transfo-xl#transformers.TransfoXLLMHeadModel">TransfoXLLMHeadModel</a> (Transformer-XL model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/unispeech#transformers.UniSpeechConfig">UniSpeechConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/unispeech#transformers.UniSpeechForPreTraining">UniSpeechForPreTraining</a> (UniSpeech model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/unispeech-sat#transformers.UniSpeechSatConfig">UniSpeechSatConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/unispeech-sat#transformers.UniSpeechSatForPreTraining">UniSpeechSatForPreTraining</a> (UniSpeechSat model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/vit_mae#transformers.ViTMAEConfig">ViTMAEConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/vit_mae#transformers.ViTMAEForPreTraining">ViTMAEForPreTraining</a> (ViTMAE model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/visual_bert#transformers.VisualBertConfig">VisualBertConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/visual_bert#transformers.VisualBertForPreTraining">VisualBertForPreTraining</a> (VisualBert model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/wav2vec2#transformers.Wav2Vec2ForPreTraining">Wav2Vec2ForPreTraining</a> (Wav2Vec2 model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/xlm#transformers.XLMWithLMHeadModel">XLMWithLMHeadModel</a> (XLM model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/xlm-roberta#transformers.XLMRobertaForMaskedLM">XLMRobertaForMaskedLM</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLConfig">XLMRobertaXLConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForMaskedLM">XLMRobertaXLForMaskedLM</a> (XLM-RoBERTa-XL model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/xlnet#transformers.XLNetLMHeadModel">XLNetLMHeadModel</a> (XLNet model)</li>
</ul>`,name:"config"}]}}),JM=new w({props:{code:`from transformers import AutoConfig, AutoModelForPreTraining

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForPreTraining.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForPreTraining

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForPreTraining.from_config(config)`}}),YM=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15770/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15770/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15770/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15770/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15770/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15770/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15770/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),KM=new w({props:{code:`from transformers import AutoConfig, AutoModelForPreTraining

# Download model and configuration from huggingface.co and cache.
model = AutoModelForPreTraining.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForPreTraining.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForPreTraining.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForPreTraining

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForPreTraining.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForPreTraining.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForPreTraining.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),ZM=new z({}),eE=new E({props:{name:"class transformers.AutoModelForCausalLM",anchor:"transformers.AutoModelForCausalLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15770/src/transformers/models/auto/modeling_auto.py#L696"}}),rE=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15770/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15770/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15770/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/bart#transformers.BartForCausalLM">BartForCausalLM</a> (BART model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/bert#transformers.BertLMHeadModel">BertLMHeadModel</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/bert-generation#transformers.BertGenerationConfig">BertGenerationConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/bert-generation#transformers.BertGenerationDecoder">BertGenerationDecoder</a> (Bert Generation model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/big_bird#transformers.BigBirdForCausalLM">BigBirdForCausalLM</a> (BigBird model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusConfig">BigBirdPegasusConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusForCausalLM">BigBirdPegasusForCausalLM</a> (BigBirdPegasus model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/blenderbot#transformers.BlenderbotConfig">BlenderbotConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/blenderbot#transformers.BlenderbotForCausalLM">BlenderbotForCausalLM</a> (Blenderbot model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/blenderbot-small#transformers.BlenderbotSmallConfig">BlenderbotSmallConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/blenderbot-small#transformers.BlenderbotSmallForCausalLM">BlenderbotSmallForCausalLM</a> (BlenderbotSmall model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/ctrl#transformers.CTRLConfig">CTRLConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/ctrl#transformers.CTRLLMHeadModel">CTRLLMHeadModel</a> (CTRL model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/camembert#transformers.CamembertForCausalLM">CamembertForCausalLM</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/electra#transformers.ElectraForCausalLM">ElectraForCausalLM</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/gpt2#transformers.GPT2LMHeadModel">GPT2LMHeadModel</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/gptj#transformers.GPTJConfig">GPTJConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/gptj#transformers.GPTJForCausalLM">GPTJForCausalLM</a> (GPT-J model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/gpt_neo#transformers.GPTNeoConfig">GPTNeoConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/gpt_neo#transformers.GPTNeoForCausalLM">GPTNeoForCausalLM</a> (GPT Neo model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/mbart#transformers.MBartForCausalLM">MBartForCausalLM</a> (mBART model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/marian#transformers.MarianConfig">MarianConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/marian#transformers.MarianForCausalLM">MarianForCausalLM</a> (Marian model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/megatron-bert#transformers.MegatronBertConfig">MegatronBertConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/megatron-bert#transformers.MegatronBertForCausalLM">MegatronBertForCausalLM</a> (MegatronBert model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig">OpenAIGPTConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/openai-gpt#transformers.OpenAIGPTLMHeadModel">OpenAIGPTLMHeadModel</a> (OpenAI GPT model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/plbart#transformers.PLBartConfig">PLBartConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/plbart#transformers.PLBartForCausalLM">PLBartForCausalLM</a> (PLBart model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/pegasus#transformers.PegasusConfig">PegasusConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/pegasus#transformers.PegasusForCausalLM">PegasusForCausalLM</a> (Pegasus model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/prophetnet#transformers.ProphetNetConfig">ProphetNetConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/prophetnet#transformers.ProphetNetForCausalLM">ProphetNetForCausalLM</a> (ProphetNet model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/qdqbert#transformers.QDQBertConfig">QDQBertConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/qdqbert#transformers.QDQBertLMHeadModel">QDQBertLMHeadModel</a> (QDQBert model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/reformer#transformers.ReformerConfig">ReformerConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/reformer#transformers.ReformerModelWithLMHead">ReformerModelWithLMHead</a> (Reformer model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/rembert#transformers.RemBertForCausalLM">RemBertForCausalLM</a> (RemBERT model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/roformer#transformers.RoFormerForCausalLM">RoFormerForCausalLM</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/roberta#transformers.RobertaForCausalLM">RobertaForCausalLM</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/speech_to_text_2#transformers.Speech2Text2Config">Speech2Text2Config</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/speech_to_text_2#transformers.Speech2Text2ForCausalLM">Speech2Text2ForCausalLM</a> (Speech2Text2 model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/trocr#transformers.TrOCRConfig">TrOCRConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/trocr#transformers.TrOCRForCausalLM">TrOCRForCausalLM</a> (TrOCR model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/transfo-xl#transformers.TransfoXLConfig">TransfoXLConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/transfo-xl#transformers.TransfoXLLMHeadModel">TransfoXLLMHeadModel</a> (Transformer-XL model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/xglm#transformers.XGLMConfig">XGLMConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/xglm#transformers.XGLMForCausalLM">XGLMForCausalLM</a> (XGLM model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/xlm#transformers.XLMWithLMHeadModel">XLMWithLMHeadModel</a> (XLM model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetConfig">XLMProphetNetConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetForCausalLM">XLMProphetNetForCausalLM</a> (XLMProphetNet model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/xlm-roberta#transformers.XLMRobertaForCausalLM">XLMRobertaForCausalLM</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLConfig">XLMRobertaXLConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForCausalLM">XLMRobertaXLForCausalLM</a> (XLM-RoBERTa-XL model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/xlnet#transformers.XLNetLMHeadModel">XLNetLMHeadModel</a> (XLNet model)</li>
</ul>`,name:"config"}]}}),tE=new w({props:{code:`from transformers import AutoConfig, AutoModelForCausalLM

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForCausalLM.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForCausalLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCausalLM.from_config(config)`}}),aE=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15770/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15770/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15770/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15770/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15770/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15770/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15770/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),nE=new w({props:{code:`from transformers import AutoConfig, AutoModelForCausalLM

# Download model and configuration from huggingface.co and cache.
model = AutoModelForCausalLM.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForCausalLM.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForCausalLM.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForCausalLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCausalLM.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),sE=new z({}),lE=new E({props:{name:"class transformers.AutoModelForMaskedLM",anchor:"transformers.AutoModelForMaskedLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15770/src/transformers/models/auto/modeling_auto.py#L703"}}),dE=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15770/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15770/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15770/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/albert#transformers.AlbertForMaskedLM">AlbertForMaskedLM</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/bart#transformers.BartForConditionalGeneration">BartForConditionalGeneration</a> (BART model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/bert#transformers.BertForMaskedLM">BertForMaskedLM</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/big_bird#transformers.BigBirdForMaskedLM">BigBirdForMaskedLM</a> (BigBird model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/camembert#transformers.CamembertForMaskedLM">CamembertForMaskedLM</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/convbert#transformers.ConvBertForMaskedLM">ConvBertForMaskedLM</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/deberta#transformers.DebertaForMaskedLM">DebertaForMaskedLM</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/deberta-v2#transformers.DebertaV2ForMaskedLM">DebertaV2ForMaskedLM</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/distilbert#transformers.DistilBertForMaskedLM">DistilBertForMaskedLM</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/electra#transformers.ElectraForMaskedLM">ElectraForMaskedLM</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/fnet#transformers.FNetConfig">FNetConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/fnet#transformers.FNetForMaskedLM">FNetForMaskedLM</a> (FNet model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/flaubert#transformers.FlaubertWithLMHeadModel">FlaubertWithLMHeadModel</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/funnel#transformers.FunnelForMaskedLM">FunnelForMaskedLM</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/ibert#transformers.IBertConfig">IBertConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/ibert#transformers.IBertForMaskedLM">IBertForMaskedLM</a> (I-BERT model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/layoutlm#transformers.LayoutLMForMaskedLM">LayoutLMForMaskedLM</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/longformer#transformers.LongformerForMaskedLM">LongformerForMaskedLM</a> (Longformer model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/mbart#transformers.MBartForConditionalGeneration">MBartForConditionalGeneration</a> (mBART model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/mpnet#transformers.MPNetForMaskedLM">MPNetForMaskedLM</a> (MPNet model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/megatron-bert#transformers.MegatronBertConfig">MegatronBertConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/megatron-bert#transformers.MegatronBertForMaskedLM">MegatronBertForMaskedLM</a> (MegatronBert model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/mobilebert#transformers.MobileBertForMaskedLM">MobileBertForMaskedLM</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/nystromformer#transformers.NystromformerConfig">NystromformerConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/nystromformer#transformers.NystromformerForMaskedLM">NystromformerForMaskedLM</a> (Nystromformer model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/perceiver#transformers.PerceiverConfig">PerceiverConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/perceiver#transformers.PerceiverForMaskedLM">PerceiverForMaskedLM</a> (Perceiver model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/qdqbert#transformers.QDQBertConfig">QDQBertConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/qdqbert#transformers.QDQBertForMaskedLM">QDQBertForMaskedLM</a> (QDQBert model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/reformer#transformers.ReformerConfig">ReformerConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/reformer#transformers.ReformerForMaskedLM">ReformerForMaskedLM</a> (Reformer model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/rembert#transformers.RemBertForMaskedLM">RemBertForMaskedLM</a> (RemBERT model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/roformer#transformers.RoFormerForMaskedLM">RoFormerForMaskedLM</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/roberta#transformers.RobertaForMaskedLM">RobertaForMaskedLM</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/squeezebert#transformers.SqueezeBertConfig">SqueezeBertConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/squeezebert#transformers.SqueezeBertForMaskedLM">SqueezeBertForMaskedLM</a> (SqueezeBERT model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/tapas#transformers.TapasForMaskedLM">TapasForMaskedLM</a> (TAPAS model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <code>Wav2Vec2ForMaskedLM</code>(Wav2Vec2 model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/xlm#transformers.XLMWithLMHeadModel">XLMWithLMHeadModel</a> (XLM model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/xlm-roberta#transformers.XLMRobertaForMaskedLM">XLMRobertaForMaskedLM</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLConfig">XLMRobertaXLConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForMaskedLM">XLMRobertaXLForMaskedLM</a> (XLM-RoBERTa-XL model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/yoso#transformers.YosoConfig">YosoConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/yoso#transformers.YosoForMaskedLM">YosoForMaskedLM</a> (YOSO model)</li>
</ul>`,name:"config"}]}}),cE=new w({props:{code:`from transformers import AutoConfig, AutoModelForMaskedLM

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForMaskedLM.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForMaskedLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMaskedLM.from_config(config)`}}),fE=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15770/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15770/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15770/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15770/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15770/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15770/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15770/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),mE=new w({props:{code:`from transformers import AutoConfig, AutoModelForMaskedLM

# Download model and configuration from huggingface.co and cache.
model = AutoModelForMaskedLM.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForMaskedLM.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForMaskedLM.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForMaskedLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMaskedLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMaskedLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMaskedLM.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),gE=new z({}),hE=new E({props:{name:"class transformers.AutoModelForSeq2SeqLM",anchor:"transformers.AutoModelForSeq2SeqLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15770/src/transformers/models/auto/modeling_auto.py#L710"}}),_E=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15770/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15770/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15770/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/bart#transformers.BartForConditionalGeneration">BartForConditionalGeneration</a> (BART model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusConfig">BigBirdPegasusConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusForConditionalGeneration">BigBirdPegasusForConditionalGeneration</a> (BigBirdPegasus model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/blenderbot#transformers.BlenderbotConfig">BlenderbotConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/blenderbot#transformers.BlenderbotForConditionalGeneration">BlenderbotForConditionalGeneration</a> (Blenderbot model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/blenderbot-small#transformers.BlenderbotSmallConfig">BlenderbotSmallConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/blenderbot-small#transformers.BlenderbotSmallForConditionalGeneration">BlenderbotSmallForConditionalGeneration</a> (BlenderbotSmall model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/encoder-decoder#transformers.EncoderDecoderConfig">EncoderDecoderConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/encoder-decoder#transformers.EncoderDecoderModel">EncoderDecoderModel</a> (Encoder decoder model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/fsmt#transformers.FSMTConfig">FSMTConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/fsmt#transformers.FSMTForConditionalGeneration">FSMTForConditionalGeneration</a> (FairSeq Machine-Translation model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/led#transformers.LEDConfig">LEDConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/led#transformers.LEDForConditionalGeneration">LEDForConditionalGeneration</a> (LED model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/m2m_100#transformers.M2M100Config">M2M100Config</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/m2m_100#transformers.M2M100ForConditionalGeneration">M2M100ForConditionalGeneration</a> (M2M100 model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/mbart#transformers.MBartForConditionalGeneration">MBartForConditionalGeneration</a> (mBART model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/mt5#transformers.MT5Config">MT5Config</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/mt5#transformers.MT5ForConditionalGeneration">MT5ForConditionalGeneration</a> (mT5 model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/marian#transformers.MarianConfig">MarianConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/marian#transformers.MarianMTModel">MarianMTModel</a> (Marian model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/plbart#transformers.PLBartConfig">PLBartConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/plbart#transformers.PLBartForConditionalGeneration">PLBartForConditionalGeneration</a> (PLBart model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/pegasus#transformers.PegasusConfig">PegasusConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/pegasus#transformers.PegasusForConditionalGeneration">PegasusForConditionalGeneration</a> (Pegasus model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/prophetnet#transformers.ProphetNetConfig">ProphetNetConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/prophetnet#transformers.ProphetNetForConditionalGeneration">ProphetNetForConditionalGeneration</a> (ProphetNet model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/t5#transformers.T5Config">T5Config</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/t5#transformers.T5ForConditionalGeneration">T5ForConditionalGeneration</a> (T5 model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetConfig">XLMProphetNetConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetForConditionalGeneration">XLMProphetNetForConditionalGeneration</a> (XLMProphetNet model)</li>
</ul>`,name:"config"}]}}),uE=new w({props:{code:`from transformers import AutoConfig, AutoModelForSeq2SeqLM

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("t5-base")
model = AutoModelForSeq2SeqLM.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForSeq2SeqLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;t5-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSeq2SeqLM.from_config(config)`}}),bE=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15770/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15770/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15770/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15770/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15770/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15770/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15770/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),vE=new w({props:{code:`from transformers import AutoConfig, AutoModelForSeq2SeqLM

# Download model and configuration from huggingface.co and cache.
model = AutoModelForSeq2SeqLM.from_pretrained("t5-base")

# Update configuration during loading
model = AutoModelForSeq2SeqLM.from_pretrained("t5-base", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/t5_tf_model_config.json")
model = AutoModelForSeq2SeqLM.from_pretrained(
    "./tf_model/t5_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForSeq2SeqLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSeq2SeqLM.from_pretrained(<span class="hljs-string">&quot;t5-base&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSeq2SeqLM.from_pretrained(<span class="hljs-string">&quot;t5-base&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/t5_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSeq2SeqLM.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/t5_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),TE=new z({}),FE=new E({props:{name:"class transformers.AutoModelForSequenceClassification",anchor:"transformers.AutoModelForSequenceClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15770/src/transformers/models/auto/modeling_auto.py#L719"}}),ME=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15770/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15770/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15770/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/albert#transformers.AlbertForSequenceClassification">AlbertForSequenceClassification</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/bart#transformers.BartForSequenceClassification">BartForSequenceClassification</a> (BART model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/bert#transformers.BertForSequenceClassification">BertForSequenceClassification</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/big_bird#transformers.BigBirdForSequenceClassification">BigBirdForSequenceClassification</a> (BigBird model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusConfig">BigBirdPegasusConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusForSequenceClassification">BigBirdPegasusForSequenceClassification</a> (BigBirdPegasus model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/ctrl#transformers.CTRLConfig">CTRLConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/ctrl#transformers.CTRLForSequenceClassification">CTRLForSequenceClassification</a> (CTRL model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/camembert#transformers.CamembertForSequenceClassification">CamembertForSequenceClassification</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/canine#transformers.CanineConfig">CanineConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/canine#transformers.CanineForSequenceClassification">CanineForSequenceClassification</a> (Canine model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/convbert#transformers.ConvBertForSequenceClassification">ConvBertForSequenceClassification</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/deberta#transformers.DebertaForSequenceClassification">DebertaForSequenceClassification</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/deberta-v2#transformers.DebertaV2ForSequenceClassification">DebertaV2ForSequenceClassification</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/distilbert#transformers.DistilBertForSequenceClassification">DistilBertForSequenceClassification</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/electra#transformers.ElectraForSequenceClassification">ElectraForSequenceClassification</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/fnet#transformers.FNetConfig">FNetConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/fnet#transformers.FNetForSequenceClassification">FNetForSequenceClassification</a> (FNet model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/flaubert#transformers.FlaubertForSequenceClassification">FlaubertForSequenceClassification</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/funnel#transformers.FunnelForSequenceClassification">FunnelForSequenceClassification</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/gpt2#transformers.GPT2ForSequenceClassification">GPT2ForSequenceClassification</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/gptj#transformers.GPTJConfig">GPTJConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/gptj#transformers.GPTJForSequenceClassification">GPTJForSequenceClassification</a> (GPT-J model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/gpt_neo#transformers.GPTNeoConfig">GPTNeoConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/gpt_neo#transformers.GPTNeoForSequenceClassification">GPTNeoForSequenceClassification</a> (GPT Neo model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/ibert#transformers.IBertConfig">IBertConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/ibert#transformers.IBertForSequenceClassification">IBertForSequenceClassification</a> (I-BERT model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/led#transformers.LEDConfig">LEDConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/led#transformers.LEDForSequenceClassification">LEDForSequenceClassification</a> (LED model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/layoutlm#transformers.LayoutLMForSequenceClassification">LayoutLMForSequenceClassification</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/layoutlmv2#transformers.LayoutLMv2Config">LayoutLMv2Config</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/layoutlmv2#transformers.LayoutLMv2ForSequenceClassification">LayoutLMv2ForSequenceClassification</a> (LayoutLMv2 model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/longformer#transformers.LongformerForSequenceClassification">LongformerForSequenceClassification</a> (Longformer model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/mbart#transformers.MBartForSequenceClassification">MBartForSequenceClassification</a> (mBART model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/mpnet#transformers.MPNetForSequenceClassification">MPNetForSequenceClassification</a> (MPNet model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/megatron-bert#transformers.MegatronBertConfig">MegatronBertConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/megatron-bert#transformers.MegatronBertForSequenceClassification">MegatronBertForSequenceClassification</a> (MegatronBert model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/mobilebert#transformers.MobileBertForSequenceClassification">MobileBertForSequenceClassification</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/nystromformer#transformers.NystromformerConfig">NystromformerConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/nystromformer#transformers.NystromformerForSequenceClassification">NystromformerForSequenceClassification</a> (Nystromformer model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig">OpenAIGPTConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/openai-gpt#transformers.OpenAIGPTForSequenceClassification">OpenAIGPTForSequenceClassification</a> (OpenAI GPT model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/plbart#transformers.PLBartConfig">PLBartConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/plbart#transformers.PLBartForSequenceClassification">PLBartForSequenceClassification</a> (PLBart model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/perceiver#transformers.PerceiverConfig">PerceiverConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/perceiver#transformers.PerceiverForSequenceClassification">PerceiverForSequenceClassification</a> (Perceiver model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/qdqbert#transformers.QDQBertConfig">QDQBertConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/qdqbert#transformers.QDQBertForSequenceClassification">QDQBertForSequenceClassification</a> (QDQBert model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/reformer#transformers.ReformerConfig">ReformerConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/reformer#transformers.ReformerForSequenceClassification">ReformerForSequenceClassification</a> (Reformer model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/rembert#transformers.RemBertForSequenceClassification">RemBertForSequenceClassification</a> (RemBERT model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/roformer#transformers.RoFormerForSequenceClassification">RoFormerForSequenceClassification</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/roberta#transformers.RobertaForSequenceClassification">RobertaForSequenceClassification</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/squeezebert#transformers.SqueezeBertConfig">SqueezeBertConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/squeezebert#transformers.SqueezeBertForSequenceClassification">SqueezeBertForSequenceClassification</a> (SqueezeBERT model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/tapas#transformers.TapasForSequenceClassification">TapasForSequenceClassification</a> (TAPAS model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/transfo-xl#transformers.TransfoXLConfig">TransfoXLConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/transfo-xl#transformers.TransfoXLForSequenceClassification">TransfoXLForSequenceClassification</a> (Transformer-XL model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/xlm#transformers.XLMForSequenceClassification">XLMForSequenceClassification</a> (XLM model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/xlm-roberta#transformers.XLMRobertaForSequenceClassification">XLMRobertaForSequenceClassification</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLConfig">XLMRobertaXLConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForSequenceClassification">XLMRobertaXLForSequenceClassification</a> (XLM-RoBERTa-XL model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/xlnet#transformers.XLNetForSequenceClassification">XLNetForSequenceClassification</a> (XLNet model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/yoso#transformers.YosoConfig">YosoConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/yoso#transformers.YosoForSequenceClassification">YosoForSequenceClassification</a> (YOSO model)</li>
</ul>`,name:"config"}]}}),EE=new w({props:{code:`from transformers import AutoConfig, AutoModelForSequenceClassification

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForSequenceClassification.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSequenceClassification.from_config(config)`}}),yE=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15770/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15770/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15770/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15770/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15770/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15770/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15770/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),wE=new w({props:{code:`from transformers import AutoConfig, AutoModelForSequenceClassification

# Download model and configuration from huggingface.co and cache.
model = AutoModelForSequenceClassification.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForSequenceClassification.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForSequenceClassification.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSequenceClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),AE=new z({}),LE=new E({props:{name:"class transformers.AutoModelForMultipleChoice",anchor:"transformers.AutoModelForMultipleChoice",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15770/src/transformers/models/auto/modeling_auto.py#L753"}}),kE=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15770/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15770/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15770/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/albert#transformers.AlbertForMultipleChoice">AlbertForMultipleChoice</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/bert#transformers.BertForMultipleChoice">BertForMultipleChoice</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/big_bird#transformers.BigBirdForMultipleChoice">BigBirdForMultipleChoice</a> (BigBird model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/camembert#transformers.CamembertForMultipleChoice">CamembertForMultipleChoice</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/canine#transformers.CanineConfig">CanineConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/canine#transformers.CanineForMultipleChoice">CanineForMultipleChoice</a> (Canine model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/convbert#transformers.ConvBertForMultipleChoice">ConvBertForMultipleChoice</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/distilbert#transformers.DistilBertForMultipleChoice">DistilBertForMultipleChoice</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/electra#transformers.ElectraForMultipleChoice">ElectraForMultipleChoice</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/fnet#transformers.FNetConfig">FNetConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/fnet#transformers.FNetForMultipleChoice">FNetForMultipleChoice</a> (FNet model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/flaubert#transformers.FlaubertForMultipleChoice">FlaubertForMultipleChoice</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/funnel#transformers.FunnelForMultipleChoice">FunnelForMultipleChoice</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/ibert#transformers.IBertConfig">IBertConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/ibert#transformers.IBertForMultipleChoice">IBertForMultipleChoice</a> (I-BERT model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/longformer#transformers.LongformerForMultipleChoice">LongformerForMultipleChoice</a> (Longformer model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/mpnet#transformers.MPNetForMultipleChoice">MPNetForMultipleChoice</a> (MPNet model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/megatron-bert#transformers.MegatronBertConfig">MegatronBertConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/megatron-bert#transformers.MegatronBertForMultipleChoice">MegatronBertForMultipleChoice</a> (MegatronBert model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/mobilebert#transformers.MobileBertForMultipleChoice">MobileBertForMultipleChoice</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/nystromformer#transformers.NystromformerConfig">NystromformerConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/nystromformer#transformers.NystromformerForMultipleChoice">NystromformerForMultipleChoice</a> (Nystromformer model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/qdqbert#transformers.QDQBertConfig">QDQBertConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/qdqbert#transformers.QDQBertForMultipleChoice">QDQBertForMultipleChoice</a> (QDQBert model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/rembert#transformers.RemBertForMultipleChoice">RemBertForMultipleChoice</a> (RemBERT model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/roformer#transformers.RoFormerForMultipleChoice">RoFormerForMultipleChoice</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/roberta#transformers.RobertaForMultipleChoice">RobertaForMultipleChoice</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/squeezebert#transformers.SqueezeBertConfig">SqueezeBertConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/squeezebert#transformers.SqueezeBertForMultipleChoice">SqueezeBertForMultipleChoice</a> (SqueezeBERT model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/xlm#transformers.XLMForMultipleChoice">XLMForMultipleChoice</a> (XLM model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/xlm-roberta#transformers.XLMRobertaForMultipleChoice">XLMRobertaForMultipleChoice</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLConfig">XLMRobertaXLConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForMultipleChoice">XLMRobertaXLForMultipleChoice</a> (XLM-RoBERTa-XL model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/xlnet#transformers.XLNetForMultipleChoice">XLNetForMultipleChoice</a> (XLNet model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/yoso#transformers.YosoConfig">YosoConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/yoso#transformers.YosoForMultipleChoice">YosoForMultipleChoice</a> (YOSO model)</li>
</ul>`,name:"config"}]}}),xE=new w({props:{code:`from transformers import AutoConfig, AutoModelForMultipleChoice

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForMultipleChoice.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForMultipleChoice

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMultipleChoice.from_config(config)`}}),RE=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15770/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15770/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15770/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15770/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15770/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15770/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15770/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),SE=new w({props:{code:`from transformers import AutoConfig, AutoModelForMultipleChoice

# Download model and configuration from huggingface.co and cache.
model = AutoModelForMultipleChoice.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForMultipleChoice.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForMultipleChoice.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForMultipleChoice

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMultipleChoice.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMultipleChoice.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMultipleChoice.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),PE=new z({}),$E=new E({props:{name:"class transformers.AutoModelForNextSentencePrediction",anchor:"transformers.AutoModelForNextSentencePrediction",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15770/src/transformers/models/auto/modeling_auto.py#L760"}}),jE=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15770/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15770/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15770/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/bert#transformers.BertForNextSentencePrediction">BertForNextSentencePrediction</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/fnet#transformers.FNetConfig">FNetConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/fnet#transformers.FNetForNextSentencePrediction">FNetForNextSentencePrediction</a> (FNet model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/megatron-bert#transformers.MegatronBertConfig">MegatronBertConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/megatron-bert#transformers.MegatronBertForNextSentencePrediction">MegatronBertForNextSentencePrediction</a> (MegatronBert model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/mobilebert#transformers.MobileBertForNextSentencePrediction">MobileBertForNextSentencePrediction</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/qdqbert#transformers.QDQBertConfig">QDQBertConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/qdqbert#transformers.QDQBertForNextSentencePrediction">QDQBertForNextSentencePrediction</a> (QDQBert model)</li>
</ul>`,name:"config"}]}}),NE=new w({props:{code:`from transformers import AutoConfig, AutoModelForNextSentencePrediction

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForNextSentencePrediction.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForNextSentencePrediction

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForNextSentencePrediction.from_config(config)`}}),DE=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15770/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15770/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15770/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15770/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15770/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15770/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15770/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),qE=new w({props:{code:`from transformers import AutoConfig, AutoModelForNextSentencePrediction

# Download model and configuration from huggingface.co and cache.
model = AutoModelForNextSentencePrediction.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForNextSentencePrediction.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForNextSentencePrediction.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForNextSentencePrediction

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForNextSentencePrediction.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForNextSentencePrediction.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForNextSentencePrediction.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),GE=new z({}),OE=new E({props:{name:"class transformers.AutoModelForTokenClassification",anchor:"transformers.AutoModelForTokenClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15770/src/transformers/models/auto/modeling_auto.py#L746"}}),zE=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15770/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15770/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15770/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/albert#transformers.AlbertForTokenClassification">AlbertForTokenClassification</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/bert#transformers.BertForTokenClassification">BertForTokenClassification</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/big_bird#transformers.BigBirdForTokenClassification">BigBirdForTokenClassification</a> (BigBird model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/camembert#transformers.CamembertForTokenClassification">CamembertForTokenClassification</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/canine#transformers.CanineConfig">CanineConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/canine#transformers.CanineForTokenClassification">CanineForTokenClassification</a> (Canine model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/convbert#transformers.ConvBertForTokenClassification">ConvBertForTokenClassification</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/deberta#transformers.DebertaForTokenClassification">DebertaForTokenClassification</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/deberta-v2#transformers.DebertaV2ForTokenClassification">DebertaV2ForTokenClassification</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/distilbert#transformers.DistilBertForTokenClassification">DistilBertForTokenClassification</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/electra#transformers.ElectraForTokenClassification">ElectraForTokenClassification</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/fnet#transformers.FNetConfig">FNetConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/fnet#transformers.FNetForTokenClassification">FNetForTokenClassification</a> (FNet model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/flaubert#transformers.FlaubertForTokenClassification">FlaubertForTokenClassification</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/funnel#transformers.FunnelForTokenClassification">FunnelForTokenClassification</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/gpt2#transformers.GPT2ForTokenClassification">GPT2ForTokenClassification</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/ibert#transformers.IBertConfig">IBertConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/ibert#transformers.IBertForTokenClassification">IBertForTokenClassification</a> (I-BERT model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/layoutlm#transformers.LayoutLMForTokenClassification">LayoutLMForTokenClassification</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/layoutlmv2#transformers.LayoutLMv2Config">LayoutLMv2Config</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/layoutlmv2#transformers.LayoutLMv2ForTokenClassification">LayoutLMv2ForTokenClassification</a> (LayoutLMv2 model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/longformer#transformers.LongformerForTokenClassification">LongformerForTokenClassification</a> (Longformer model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/mpnet#transformers.MPNetForTokenClassification">MPNetForTokenClassification</a> (MPNet model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/megatron-bert#transformers.MegatronBertConfig">MegatronBertConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/megatron-bert#transformers.MegatronBertForTokenClassification">MegatronBertForTokenClassification</a> (MegatronBert model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/mobilebert#transformers.MobileBertForTokenClassification">MobileBertForTokenClassification</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/nystromformer#transformers.NystromformerConfig">NystromformerConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/nystromformer#transformers.NystromformerForTokenClassification">NystromformerForTokenClassification</a> (Nystromformer model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/qdqbert#transformers.QDQBertConfig">QDQBertConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/qdqbert#transformers.QDQBertForTokenClassification">QDQBertForTokenClassification</a> (QDQBert model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/rembert#transformers.RemBertForTokenClassification">RemBertForTokenClassification</a> (RemBERT model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/roformer#transformers.RoFormerForTokenClassification">RoFormerForTokenClassification</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/roberta#transformers.RobertaForTokenClassification">RobertaForTokenClassification</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/squeezebert#transformers.SqueezeBertConfig">SqueezeBertConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/squeezebert#transformers.SqueezeBertForTokenClassification">SqueezeBertForTokenClassification</a> (SqueezeBERT model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/xlm#transformers.XLMForTokenClassification">XLMForTokenClassification</a> (XLM model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/xlm-roberta#transformers.XLMRobertaForTokenClassification">XLMRobertaForTokenClassification</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLConfig">XLMRobertaXLConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForTokenClassification">XLMRobertaXLForTokenClassification</a> (XLM-RoBERTa-XL model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/xlnet#transformers.XLNetForTokenClassification">XLNetForTokenClassification</a> (XLNet model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/yoso#transformers.YosoConfig">YosoConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/yoso#transformers.YosoForTokenClassification">YosoForTokenClassification</a> (YOSO model)</li>
</ul>`,name:"config"}]}}),VE=new w({props:{code:`from transformers import AutoConfig, AutoModelForTokenClassification

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForTokenClassification.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForTokenClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForTokenClassification.from_config(config)`}}),WE=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15770/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15770/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15770/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15770/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15770/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15770/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15770/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),QE=new w({props:{code:`from transformers import AutoConfig, AutoModelForTokenClassification

# Download model and configuration from huggingface.co and cache.
model = AutoModelForTokenClassification.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForTokenClassification.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForTokenClassification.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForTokenClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForTokenClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForTokenClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForTokenClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),HE=new z({}),UE=new E({props:{name:"class transformers.AutoModelForQuestionAnswering",anchor:"transformers.AutoModelForQuestionAnswering",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15770/src/transformers/models/auto/modeling_auto.py#L728"}}),YE=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15770/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15770/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15770/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/albert#transformers.AlbertForQuestionAnswering">AlbertForQuestionAnswering</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/bart#transformers.BartForQuestionAnswering">BartForQuestionAnswering</a> (BART model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/bert#transformers.BertForQuestionAnswering">BertForQuestionAnswering</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/big_bird#transformers.BigBirdForQuestionAnswering">BigBirdForQuestionAnswering</a> (BigBird model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusConfig">BigBirdPegasusConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusForQuestionAnswering">BigBirdPegasusForQuestionAnswering</a> (BigBirdPegasus model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/camembert#transformers.CamembertForQuestionAnswering">CamembertForQuestionAnswering</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/canine#transformers.CanineConfig">CanineConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/canine#transformers.CanineForQuestionAnswering">CanineForQuestionAnswering</a> (Canine model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/convbert#transformers.ConvBertForQuestionAnswering">ConvBertForQuestionAnswering</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/deberta#transformers.DebertaForQuestionAnswering">DebertaForQuestionAnswering</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/deberta-v2#transformers.DebertaV2ForQuestionAnswering">DebertaV2ForQuestionAnswering</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/distilbert#transformers.DistilBertForQuestionAnswering">DistilBertForQuestionAnswering</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/electra#transformers.ElectraForQuestionAnswering">ElectraForQuestionAnswering</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/fnet#transformers.FNetConfig">FNetConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/fnet#transformers.FNetForQuestionAnswering">FNetForQuestionAnswering</a> (FNet model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/flaubert#transformers.FlaubertForQuestionAnsweringSimple">FlaubertForQuestionAnsweringSimple</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/funnel#transformers.FunnelForQuestionAnswering">FunnelForQuestionAnswering</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/gptj#transformers.GPTJConfig">GPTJConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/gptj#transformers.GPTJForQuestionAnswering">GPTJForQuestionAnswering</a> (GPT-J model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/ibert#transformers.IBertConfig">IBertConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/ibert#transformers.IBertForQuestionAnswering">IBertForQuestionAnswering</a> (I-BERT model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/led#transformers.LEDConfig">LEDConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/led#transformers.LEDForQuestionAnswering">LEDForQuestionAnswering</a> (LED model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/layoutlmv2#transformers.LayoutLMv2Config">LayoutLMv2Config</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/layoutlmv2#transformers.LayoutLMv2ForQuestionAnswering">LayoutLMv2ForQuestionAnswering</a> (LayoutLMv2 model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/longformer#transformers.LongformerForQuestionAnswering">LongformerForQuestionAnswering</a> (Longformer model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/lxmert#transformers.LxmertConfig">LxmertConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/lxmert#transformers.LxmertForQuestionAnswering">LxmertForQuestionAnswering</a> (LXMERT model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/mbart#transformers.MBartForQuestionAnswering">MBartForQuestionAnswering</a> (mBART model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/mpnet#transformers.MPNetForQuestionAnswering">MPNetForQuestionAnswering</a> (MPNet model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/megatron-bert#transformers.MegatronBertConfig">MegatronBertConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/megatron-bert#transformers.MegatronBertForQuestionAnswering">MegatronBertForQuestionAnswering</a> (MegatronBert model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/mobilebert#transformers.MobileBertForQuestionAnswering">MobileBertForQuestionAnswering</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/nystromformer#transformers.NystromformerConfig">NystromformerConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/nystromformer#transformers.NystromformerForQuestionAnswering">NystromformerForQuestionAnswering</a> (Nystromformer model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/qdqbert#transformers.QDQBertConfig">QDQBertConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/qdqbert#transformers.QDQBertForQuestionAnswering">QDQBertForQuestionAnswering</a> (QDQBert model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/reformer#transformers.ReformerConfig">ReformerConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/reformer#transformers.ReformerForQuestionAnswering">ReformerForQuestionAnswering</a> (Reformer model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/rembert#transformers.RemBertForQuestionAnswering">RemBertForQuestionAnswering</a> (RemBERT model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/roformer#transformers.RoFormerForQuestionAnswering">RoFormerForQuestionAnswering</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/roberta#transformers.RobertaForQuestionAnswering">RobertaForQuestionAnswering</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/splinter#transformers.SplinterConfig">SplinterConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/splinter#transformers.SplinterForQuestionAnswering">SplinterForQuestionAnswering</a> (Splinter model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/squeezebert#transformers.SqueezeBertConfig">SqueezeBertConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/squeezebert#transformers.SqueezeBertForQuestionAnswering">SqueezeBertForQuestionAnswering</a> (SqueezeBERT model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/xlm#transformers.XLMForQuestionAnsweringSimple">XLMForQuestionAnsweringSimple</a> (XLM model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/xlm-roberta#transformers.XLMRobertaForQuestionAnswering">XLMRobertaForQuestionAnswering</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLConfig">XLMRobertaXLConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForQuestionAnswering">XLMRobertaXLForQuestionAnswering</a> (XLM-RoBERTa-XL model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/xlnet#transformers.XLNetForQuestionAnsweringSimple">XLNetForQuestionAnsweringSimple</a> (XLNet model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/yoso#transformers.YosoConfig">YosoConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/yoso#transformers.YosoForQuestionAnswering">YosoForQuestionAnswering</a> (YOSO model)</li>
</ul>`,name:"config"}]}}),KE=new w({props:{code:`from transformers import AutoConfig, AutoModelForQuestionAnswering

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForQuestionAnswering.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForQuestionAnswering.from_config(config)`}}),ZE=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15770/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15770/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15770/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15770/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15770/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15770/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15770/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),e3=new w({props:{code:`from transformers import AutoConfig, AutoModelForQuestionAnswering

# Download model and configuration from huggingface.co and cache.
model = AutoModelForQuestionAnswering.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForQuestionAnswering.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForQuestionAnswering.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForQuestionAnswering.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),o3=new z({}),r3=new E({props:{name:"class transformers.AutoModelForTableQuestionAnswering",anchor:"transformers.AutoModelForTableQuestionAnswering",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15770/src/transformers/models/auto/modeling_auto.py#L735"}}),a3=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15770/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15770/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15770/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/tapas#transformers.TapasForQuestionAnswering">TapasForQuestionAnswering</a> (TAPAS model)</li>
</ul>`,name:"config"}]}}),n3=new w({props:{code:`from transformers import AutoConfig, AutoModelForTableQuestionAnswering

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("google/tapas-base-finetuned-wtq")
model = AutoModelForTableQuestionAnswering.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForTableQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;google/tapas-base-finetuned-wtq&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForTableQuestionAnswering.from_config(config)`}}),s3=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15770/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15770/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15770/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15770/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15770/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15770/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15770/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),l3=new w({props:{code:`from transformers import AutoConfig, AutoModelForTableQuestionAnswering

# Download model and configuration from huggingface.co and cache.
model = AutoModelForTableQuestionAnswering.from_pretrained("google/tapas-base-finetuned-wtq")

# Update configuration during loading
model = AutoModelForTableQuestionAnswering.from_pretrained("google/tapas-base-finetuned-wtq", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/tapas_tf_model_config.json")
model = AutoModelForTableQuestionAnswering.from_pretrained(
    "./tf_model/tapas_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForTableQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForTableQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;google/tapas-base-finetuned-wtq&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForTableQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;google/tapas-base-finetuned-wtq&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/tapas_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForTableQuestionAnswering.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/tapas_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),i3=new z({}),d3=new E({props:{name:"class transformers.AutoModelForImageClassification",anchor:"transformers.AutoModelForImageClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15770/src/transformers/models/auto/modeling_auto.py#L769"}}),f3=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15770/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15770/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15770/en/model_doc/beit#transformers.BeitConfig">BeitConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/beit#transformers.BeitForImageClassification">BeitForImageClassification</a> (BEiT model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/convnext#transformers.ConvNextConfig">ConvNextConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/convnext#transformers.ConvNextForImageClassification">ConvNextForImageClassification</a> (ConvNext model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/deit#transformers.DeiTConfig">DeiTConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/deit#transformers.DeiTForImageClassification">DeiTForImageClassification</a> or <a href="/docs/transformers/pr_15770/en/model_doc/deit#transformers.DeiTForImageClassificationWithTeacher">DeiTForImageClassificationWithTeacher</a> (DeiT model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/imagegpt#transformers.ImageGPTConfig">ImageGPTConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/imagegpt#transformers.ImageGPTForImageClassification">ImageGPTForImageClassification</a> (ImageGPT model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/perceiver#transformers.PerceiverConfig">PerceiverConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/perceiver#transformers.PerceiverForImageClassificationLearned">PerceiverForImageClassificationLearned</a> or <a href="/docs/transformers/pr_15770/en/model_doc/perceiver#transformers.PerceiverForImageClassificationFourier">PerceiverForImageClassificationFourier</a> or <a href="/docs/transformers/pr_15770/en/model_doc/perceiver#transformers.PerceiverForImageClassificationConvProcessing">PerceiverForImageClassificationConvProcessing</a> (Perceiver model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/poolformer#transformers.PoolFormerConfig">PoolFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/poolformer#transformers.PoolFormerForImageClassification">PoolFormerForImageClassification</a> (PoolFormer model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/resnet#transformers.ResNetConfig">ResNetConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/resnet#transformers.ResNetForImageClassification">ResNetForImageClassification</a> (resnet model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/segformer#transformers.SegformerConfig">SegformerConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/segformer#transformers.SegformerForImageClassification">SegformerForImageClassification</a> (SegFormer model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/swin#transformers.SwinConfig">SwinConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/swin#transformers.SwinForImageClassification">SwinForImageClassification</a> (Swin model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/vit#transformers.ViTConfig">ViTConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/vit#transformers.ViTForImageClassification">ViTForImageClassification</a> (ViT model)</li>
</ul>`,name:"config"}]}}),m3=new w({props:{code:`from transformers import AutoConfig, AutoModelForImageClassification

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForImageClassification.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForImageClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForImageClassification.from_config(config)`}}),g3=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15770/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15770/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15770/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15770/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15770/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15770/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15770/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),h3=new w({props:{code:`from transformers import AutoConfig, AutoModelForImageClassification

# Download model and configuration from huggingface.co and cache.
model = AutoModelForImageClassification.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForImageClassification.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForImageClassification.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForImageClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForImageClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForImageClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForImageClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),p3=new z({}),_3=new E({props:{name:"class transformers.AutoModelForVision2Seq",anchor:"transformers.AutoModelForVision2Seq",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15770/src/transformers/models/auto/modeling_auto.py#L799"}}),b3=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15770/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15770/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15770/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderConfig">VisionEncoderDecoderConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderModel">VisionEncoderDecoderModel</a> (Vision Encoder decoder model)</li>
</ul>`,name:"config"}]}}),v3=new w({props:{code:`from transformers import AutoConfig, AutoModelForVision2Seq

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForVision2Seq.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForVision2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForVision2Seq.from_config(config)`}}),T3=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15770/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15770/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15770/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15770/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15770/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15770/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15770/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),F3=new w({props:{code:`from transformers import AutoConfig, AutoModelForVision2Seq

# Download model and configuration from huggingface.co and cache.
model = AutoModelForVision2Seq.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForVision2Seq.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForVision2Seq.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForVision2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForVision2Seq.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForVision2Seq.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForVision2Seq.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),C3=new z({}),M3=new E({props:{name:"class transformers.AutoModelForAudioClassification",anchor:"transformers.AutoModelForAudioClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15770/src/transformers/models/auto/modeling_auto.py#L806"}}),y3=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15770/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15770/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15770/en/model_doc/hubert#transformers.HubertConfig">HubertConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/hubert#transformers.HubertForSequenceClassification">HubertForSequenceClassification</a> (Hubert model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/sew#transformers.SEWConfig">SEWConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/sew#transformers.SEWForSequenceClassification">SEWForSequenceClassification</a> (SEW model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/sew-d#transformers.SEWDConfig">SEWDConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/sew-d#transformers.SEWDForSequenceClassification">SEWDForSequenceClassification</a> (SEW-D model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/unispeech#transformers.UniSpeechConfig">UniSpeechConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/unispeech#transformers.UniSpeechForSequenceClassification">UniSpeechForSequenceClassification</a> (UniSpeech model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/unispeech-sat#transformers.UniSpeechSatConfig">UniSpeechSatConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/unispeech-sat#transformers.UniSpeechSatForSequenceClassification">UniSpeechSatForSequenceClassification</a> (UniSpeechSat model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/wav2vec2#transformers.Wav2Vec2ForSequenceClassification">Wav2Vec2ForSequenceClassification</a> (Wav2Vec2 model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/wavlm#transformers.WavLMConfig">WavLMConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/wavlm#transformers.WavLMForSequenceClassification">WavLMForSequenceClassification</a> (WavLM model)</li>
</ul>`,name:"config"}]}}),w3=new w({props:{code:`from transformers import AutoConfig, AutoModelForAudioClassification

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForAudioClassification.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForAudioClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioClassification.from_config(config)`}}),A3=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15770/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15770/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15770/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15770/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15770/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15770/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15770/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),L3=new w({props:{code:`from transformers import AutoConfig, AutoModelForAudioClassification

# Download model and configuration from huggingface.co and cache.
model = AutoModelForAudioClassification.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForAudioClassification.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForAudioClassification.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForAudioClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),B3=new z({}),k3=new E({props:{name:"class transformers.AutoModelForAudioFrameClassification",anchor:"transformers.AutoModelForAudioFrameClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15770/src/transformers/models/auto/modeling_auto.py#L829"}}),R3=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15770/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15770/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15770/en/model_doc/unispeech-sat#transformers.UniSpeechSatConfig">UniSpeechSatConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/unispeech-sat#transformers.UniSpeechSatForAudioFrameClassification">UniSpeechSatForAudioFrameClassification</a> (UniSpeechSat model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/wav2vec2#transformers.Wav2Vec2ForAudioFrameClassification">Wav2Vec2ForAudioFrameClassification</a> (Wav2Vec2 model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/wavlm#transformers.WavLMConfig">WavLMConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/wavlm#transformers.WavLMForAudioFrameClassification">WavLMForAudioFrameClassification</a> (WavLM model)</li>
</ul>`,name:"config"}]}}),S3=new w({props:{code:`from transformers import AutoConfig, AutoModelForAudioFrameClassification

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForAudioFrameClassification.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForAudioFrameClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioFrameClassification.from_config(config)`}}),P3=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15770/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15770/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15770/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15770/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15770/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15770/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15770/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),$3=new w({props:{code:`from transformers import AutoConfig, AutoModelForAudioFrameClassification

# Download model and configuration from huggingface.co and cache.
model = AutoModelForAudioFrameClassification.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForAudioFrameClassification.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForAudioFrameClassification.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForAudioFrameClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioFrameClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioFrameClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioFrameClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),I3=new z({}),j3=new E({props:{name:"class transformers.AutoModelForCTC",anchor:"transformers.AutoModelForCTC",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15770/src/transformers/models/auto/modeling_auto.py#L813"}}),D3=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15770/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15770/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15770/en/model_doc/hubert#transformers.HubertConfig">HubertConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/hubert#transformers.HubertForCTC">HubertForCTC</a> (Hubert model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/sew#transformers.SEWConfig">SEWConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/sew#transformers.SEWForCTC">SEWForCTC</a> (SEW model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/sew-d#transformers.SEWDConfig">SEWDConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/sew-d#transformers.SEWDForCTC">SEWDForCTC</a> (SEW-D model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/unispeech#transformers.UniSpeechConfig">UniSpeechConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/unispeech#transformers.UniSpeechForCTC">UniSpeechForCTC</a> (UniSpeech model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/unispeech-sat#transformers.UniSpeechSatConfig">UniSpeechSatConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/unispeech-sat#transformers.UniSpeechSatForCTC">UniSpeechSatForCTC</a> (UniSpeechSat model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/wav2vec2#transformers.Wav2Vec2ForCTC">Wav2Vec2ForCTC</a> (Wav2Vec2 model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/wavlm#transformers.WavLMConfig">WavLMConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/wavlm#transformers.WavLMForCTC">WavLMForCTC</a> (WavLM model)</li>
</ul>`,name:"config"}]}}),q3=new w({props:{code:`from transformers import AutoConfig, AutoModelForCTC

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForCTC.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForCTC

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCTC.from_config(config)`}}),G3=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15770/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15770/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15770/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15770/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15770/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15770/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15770/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),O3=new w({props:{code:`from transformers import AutoConfig, AutoModelForCTC

# Download model and configuration from huggingface.co and cache.
model = AutoModelForCTC.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForCTC.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForCTC.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForCTC

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCTC.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCTC.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCTC.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),X3=new z({}),z3=new E({props:{name:"class transformers.AutoModelForSpeechSeq2Seq",anchor:"transformers.AutoModelForSpeechSeq2Seq",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15770/src/transformers/models/auto/modeling_auto.py#L820"}}),W3=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15770/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15770/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15770/en/model_doc/speech_to_text#transformers.Speech2TextConfig">Speech2TextConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/speech_to_text#transformers.Speech2TextForConditionalGeneration">Speech2TextForConditionalGeneration</a> (Speech2Text model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/speech-encoder-decoder#transformers.SpeechEncoderDecoderConfig">SpeechEncoderDecoderConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/speech-encoder-decoder#transformers.SpeechEncoderDecoderModel">SpeechEncoderDecoderModel</a> (Speech Encoder decoder model)</li>
</ul>`,name:"config"}]}}),Q3=new w({props:{code:`from transformers import AutoConfig, AutoModelForSpeechSeq2Seq

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForSpeechSeq2Seq.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForSpeechSeq2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSpeechSeq2Seq.from_config(config)`}}),H3=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15770/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15770/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15770/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15770/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15770/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15770/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15770/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),J3=new w({props:{code:`from transformers import AutoConfig, AutoModelForSpeechSeq2Seq

# Download model and configuration from huggingface.co and cache.
model = AutoModelForSpeechSeq2Seq.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForSpeechSeq2Seq.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForSpeechSeq2Seq.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForSpeechSeq2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSpeechSeq2Seq.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSpeechSeq2Seq.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSpeechSeq2Seq.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),Y3=new z({}),K3=new E({props:{name:"class transformers.AutoModelForAudioXVector",anchor:"transformers.AutoModelForAudioXVector",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15770/src/transformers/models/auto/modeling_auto.py#L838"}}),ey=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15770/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15770/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15770/en/model_doc/unispeech-sat#transformers.UniSpeechSatConfig">UniSpeechSatConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/unispeech-sat#transformers.UniSpeechSatForXVector">UniSpeechSatForXVector</a> (UniSpeechSat model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/wav2vec2#transformers.Wav2Vec2ForXVector">Wav2Vec2ForXVector</a> (Wav2Vec2 model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/wavlm#transformers.WavLMConfig">WavLMConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/wavlm#transformers.WavLMForXVector">WavLMForXVector</a> (WavLM model)</li>
</ul>`,name:"config"}]}}),oy=new w({props:{code:`from transformers import AutoConfig, AutoModelForAudioXVector

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForAudioXVector.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForAudioXVector

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioXVector.from_config(config)`}}),ry=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15770/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15770/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15770/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15770/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15770/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15770/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15770/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),ty=new w({props:{code:`from transformers import AutoConfig, AutoModelForAudioXVector

# Download model and configuration from huggingface.co and cache.
model = AutoModelForAudioXVector.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForAudioXVector.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForAudioXVector.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForAudioXVector

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioXVector.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioXVector.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioXVector.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),ay=new z({}),ny=new E({props:{name:"class transformers.AutoModelForMaskedImageModeling",anchor:"transformers.AutoModelForMaskedImageModeling",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15770/src/transformers/models/auto/modeling_auto.py#L845"}}),ly=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15770/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15770/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15770/en/model_doc/deit#transformers.DeiTConfig">DeiTConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/deit#transformers.DeiTForMaskedImageModeling">DeiTForMaskedImageModeling</a> (DeiT model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/swin#transformers.SwinConfig">SwinConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/swin#transformers.SwinForMaskedImageModeling">SwinForMaskedImageModeling</a> (Swin model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/vit#transformers.ViTConfig">ViTConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/vit#transformers.ViTForMaskedImageModeling">ViTForMaskedImageModeling</a> (ViT model)</li>
</ul>`,name:"config"}]}}),iy=new w({props:{code:`from transformers import AutoConfig, AutoModelForMaskedImageModeling

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForMaskedImageModeling.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForMaskedImageModeling

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMaskedImageModeling.from_config(config)`}}),dy=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15770/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15770/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15770/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15770/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15770/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15770/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15770/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),cy=new w({props:{code:`from transformers import AutoConfig, AutoModelForMaskedImageModeling

# Download model and configuration from huggingface.co and cache.
model = AutoModelForMaskedImageModeling.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForMaskedImageModeling.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForMaskedImageModeling.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForMaskedImageModeling

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMaskedImageModeling.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMaskedImageModeling.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMaskedImageModeling.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),fy=new z({}),my=new E({props:{name:"class transformers.AutoModelForObjectDetection",anchor:"transformers.AutoModelForObjectDetection",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15770/src/transformers/models/auto/modeling_auto.py#L792"}}),hy=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15770/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15770/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15770/en/model_doc/detr#transformers.DetrConfig">DetrConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/detr#transformers.DetrForObjectDetection">DetrForObjectDetection</a> (DETR model)</li>
</ul>`,name:"config"}]}}),py=new w({props:{code:`from transformers import AutoConfig, AutoModelForObjectDetection

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForObjectDetection.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForObjectDetection

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForObjectDetection.from_config(config)`}}),_y=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15770/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15770/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15770/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15770/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15770/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15770/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15770/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),uy=new w({props:{code:`from transformers import AutoConfig, AutoModelForObjectDetection

# Download model and configuration from huggingface.co and cache.
model = AutoModelForObjectDetection.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForObjectDetection.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForObjectDetection.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForObjectDetection

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForObjectDetection.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForObjectDetection.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForObjectDetection.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),by=new z({}),vy=new E({props:{name:"class transformers.AutoModelForImageSegmentation",anchor:"transformers.AutoModelForImageSegmentation",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15770/src/transformers/models/auto/modeling_auto.py#L776"}}),Fy=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15770/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15770/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15770/en/model_doc/detr#transformers.DetrConfig">DetrConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/detr#transformers.DetrForSegmentation">DetrForSegmentation</a> (DETR model)</li>
</ul>`,name:"config"}]}}),Cy=new w({props:{code:`from transformers import AutoConfig, AutoModelForImageSegmentation

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForImageSegmentation.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForImageSegmentation

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForImageSegmentation.from_config(config)`}}),My=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15770/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15770/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15770/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15770/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15770/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15770/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15770/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),Ey=new w({props:{code:`from transformers import AutoConfig, AutoModelForImageSegmentation

# Download model and configuration from huggingface.co and cache.
model = AutoModelForImageSegmentation.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForImageSegmentation.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForImageSegmentation.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForImageSegmentation

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForImageSegmentation.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForImageSegmentation.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForImageSegmentation.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),yy=new z({}),wy=new E({props:{name:"class transformers.AutoModelForSemanticSegmentation",anchor:"transformers.AutoModelForSemanticSegmentation",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15770/src/transformers/models/auto/modeling_auto.py#L783"}}),Ly=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15770/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15770/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15770/en/model_doc/beit#transformers.BeitConfig">BeitConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/beit#transformers.BeitForSemanticSegmentation">BeitForSemanticSegmentation</a> (BEiT model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/segformer#transformers.SegformerConfig">SegformerConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/segformer#transformers.SegformerForSemanticSegmentation">SegformerForSemanticSegmentation</a> (SegFormer model)</li>
</ul>`,name:"config"}]}}),By=new w({props:{code:`from transformers import AutoConfig, AutoModelForSemanticSegmentation

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForSemanticSegmentation.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForSemanticSegmentation

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSemanticSegmentation.from_config(config)`}}),ky=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15770/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15770/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15770/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15770/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15770/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15770/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15770/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),Ry=new w({props:{code:`from transformers import AutoConfig, AutoModelForSemanticSegmentation

# Download model and configuration from huggingface.co and cache.
model = AutoModelForSemanticSegmentation.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForSemanticSegmentation.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForSemanticSegmentation.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForSemanticSegmentation

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSemanticSegmentation.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSemanticSegmentation.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSemanticSegmentation.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),Sy=new z({}),Py=new E({props:{name:"class transformers.TFAutoModel",anchor:"transformers.TFAutoModel",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15770/src/transformers/models/auto/modeling_tf_auto.py#L371"}}),Iy=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15770/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15770/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15770/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/albert#transformers.TFAlbertModel">TFAlbertModel</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/bart#transformers.TFBartModel">TFBartModel</a> (BART model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/bert#transformers.TFBertModel">TFBertModel</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/blenderbot#transformers.BlenderbotConfig">BlenderbotConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/blenderbot#transformers.TFBlenderbotModel">TFBlenderbotModel</a> (Blenderbot model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/blenderbot-small#transformers.BlenderbotSmallConfig">BlenderbotSmallConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/blenderbot-small#transformers.TFBlenderbotSmallModel">TFBlenderbotSmallModel</a> (BlenderbotSmall model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/clip#transformers.CLIPConfig">CLIPConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/clip#transformers.TFCLIPModel">TFCLIPModel</a> (CLIP model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/ctrl#transformers.CTRLConfig">CTRLConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/ctrl#transformers.TFCTRLModel">TFCTRLModel</a> (CTRL model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/camembert#transformers.TFCamembertModel">TFCamembertModel</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/convbert#transformers.TFConvBertModel">TFConvBertModel</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/dpr#transformers.DPRConfig">DPRConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/dpr#transformers.TFDPRQuestionEncoder">TFDPRQuestionEncoder</a> (DPR model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/deberta#transformers.TFDebertaModel">TFDebertaModel</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/deberta-v2#transformers.TFDebertaV2Model">TFDebertaV2Model</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/distilbert#transformers.TFDistilBertModel">TFDistilBertModel</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/electra#transformers.TFElectraModel">TFElectraModel</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/flaubert#transformers.TFFlaubertModel">TFFlaubertModel</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/funnel#transformers.TFFunnelModel">TFFunnelModel</a> or <a href="/docs/transformers/pr_15770/en/model_doc/funnel#transformers.TFFunnelBaseModel">TFFunnelBaseModel</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/gpt2#transformers.TFGPT2Model">TFGPT2Model</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/hubert#transformers.HubertConfig">HubertConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/hubert#transformers.TFHubertModel">TFHubertModel</a> (Hubert model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/led#transformers.LEDConfig">LEDConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/led#transformers.TFLEDModel">TFLEDModel</a> (LED model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/layoutlm#transformers.TFLayoutLMModel">TFLayoutLMModel</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/longformer#transformers.TFLongformerModel">TFLongformerModel</a> (Longformer model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/lxmert#transformers.LxmertConfig">LxmertConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/lxmert#transformers.TFLxmertModel">TFLxmertModel</a> (LXMERT model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/mbart#transformers.TFMBartModel">TFMBartModel</a> (mBART model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/mpnet#transformers.TFMPNetModel">TFMPNetModel</a> (MPNet model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/mt5#transformers.MT5Config">MT5Config</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/mt5#transformers.TFMT5Model">TFMT5Model</a> (mT5 model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/marian#transformers.MarianConfig">MarianConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/marian#transformers.TFMarianModel">TFMarianModel</a> (Marian model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/mobilebert#transformers.TFMobileBertModel">TFMobileBertModel</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig">OpenAIGPTConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/openai-gpt#transformers.TFOpenAIGPTModel">TFOpenAIGPTModel</a> (OpenAI GPT model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/pegasus#transformers.PegasusConfig">PegasusConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/pegasus#transformers.TFPegasusModel">TFPegasusModel</a> (Pegasus model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/rembert#transformers.TFRemBertModel">TFRemBertModel</a> (RemBERT model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/roformer#transformers.TFRoFormerModel">TFRoFormerModel</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/roberta#transformers.TFRobertaModel">TFRobertaModel</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/speech_to_text#transformers.Speech2TextConfig">Speech2TextConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/speech_to_text#transformers.TFSpeech2TextModel">TFSpeech2TextModel</a> (Speech2Text model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/t5#transformers.T5Config">T5Config</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/t5#transformers.TFT5Model">TFT5Model</a> (T5 model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/tapas#transformers.TFTapasModel">TFTapasModel</a> (TAPAS model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/transfo-xl#transformers.TransfoXLConfig">TransfoXLConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/transfo-xl#transformers.TFTransfoXLModel">TFTransfoXLModel</a> (Transformer-XL model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/vit#transformers.ViTConfig">ViTConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/vit#transformers.TFViTModel">TFViTModel</a> (ViT model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/wav2vec2#transformers.TFWav2Vec2Model">TFWav2Vec2Model</a> (Wav2Vec2 model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/xlm#transformers.TFXLMModel">TFXLMModel</a> (XLM model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/xlm-roberta#transformers.TFXLMRobertaModel">TFXLMRobertaModel</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/xlnet#transformers.TFXLNetModel">TFXLNetModel</a> (XLNet model)</li>
</ul>`,name:"config"}]}}),jy=new w({props:{code:`from transformers import AutoConfig, TFAutoModel

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = TFAutoModel.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModel.from_config(config)`}}),Ny=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15770/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15770/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15770/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15770/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15770/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),Dy=new w({props:{code:`from transformers import AutoConfig, TFAutoModel

# Download model and configuration from huggingface.co and cache.
model = TFAutoModel.from_pretrained("bert-base-cased")

# Update configuration during loading
model = TFAutoModel.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = TFAutoModel.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModel.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModel.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModel.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),qy=new z({}),Gy=new E({props:{name:"class transformers.TFAutoModelForPreTraining",anchor:"transformers.TFAutoModelForPreTraining",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15770/src/transformers/models/auto/modeling_tf_auto.py#L378"}}),Xy=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15770/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15770/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15770/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/albert#transformers.TFAlbertForPreTraining">TFAlbertForPreTraining</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/bart#transformers.TFBartForConditionalGeneration">TFBartForConditionalGeneration</a> (BART model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/bert#transformers.TFBertForPreTraining">TFBertForPreTraining</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/ctrl#transformers.CTRLConfig">CTRLConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/ctrl#transformers.TFCTRLLMHeadModel">TFCTRLLMHeadModel</a> (CTRL model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/camembert#transformers.TFCamembertForMaskedLM">TFCamembertForMaskedLM</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/distilbert#transformers.TFDistilBertForMaskedLM">TFDistilBertForMaskedLM</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/electra#transformers.TFElectraForPreTraining">TFElectraForPreTraining</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/flaubert#transformers.TFFlaubertWithLMHeadModel">TFFlaubertWithLMHeadModel</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/funnel#transformers.TFFunnelForPreTraining">TFFunnelForPreTraining</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/gpt2#transformers.TFGPT2LMHeadModel">TFGPT2LMHeadModel</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/layoutlm#transformers.TFLayoutLMForMaskedLM">TFLayoutLMForMaskedLM</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/lxmert#transformers.LxmertConfig">LxmertConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/lxmert#transformers.TFLxmertForPreTraining">TFLxmertForPreTraining</a> (LXMERT model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/mpnet#transformers.TFMPNetForMaskedLM">TFMPNetForMaskedLM</a> (MPNet model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/mobilebert#transformers.TFMobileBertForPreTraining">TFMobileBertForPreTraining</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig">OpenAIGPTConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/openai-gpt#transformers.TFOpenAIGPTLMHeadModel">TFOpenAIGPTLMHeadModel</a> (OpenAI GPT model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/roberta#transformers.TFRobertaForMaskedLM">TFRobertaForMaskedLM</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/t5#transformers.T5Config">T5Config</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/t5#transformers.TFT5ForConditionalGeneration">TFT5ForConditionalGeneration</a> (T5 model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/tapas#transformers.TFTapasForMaskedLM">TFTapasForMaskedLM</a> (TAPAS model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/transfo-xl#transformers.TransfoXLConfig">TransfoXLConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/transfo-xl#transformers.TFTransfoXLLMHeadModel">TFTransfoXLLMHeadModel</a> (Transformer-XL model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/xlm#transformers.TFXLMWithLMHeadModel">TFXLMWithLMHeadModel</a> (XLM model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForMaskedLM">TFXLMRobertaForMaskedLM</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/xlnet#transformers.TFXLNetLMHeadModel">TFXLNetLMHeadModel</a> (XLNet model)</li>
</ul>`,name:"config"}]}}),zy=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForPreTraining

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = TFAutoModelForPreTraining.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForPreTraining

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForPreTraining.from_config(config)`}}),Vy=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15770/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15770/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15770/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15770/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15770/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),Wy=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForPreTraining

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForPreTraining.from_pretrained("bert-base-cased")

# Update configuration during loading
model = TFAutoModelForPreTraining.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = TFAutoModelForPreTraining.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForPreTraining

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForPreTraining.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForPreTraining.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForPreTraining.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),Qy=new z({}),Hy=new E({props:{name:"class transformers.TFAutoModelForCausalLM",anchor:"transformers.TFAutoModelForCausalLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15770/src/transformers/models/auto/modeling_tf_auto.py#L393"}}),Jy=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15770/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15770/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15770/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/bert#transformers.TFBertLMHeadModel">TFBertLMHeadModel</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/ctrl#transformers.CTRLConfig">CTRLConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/ctrl#transformers.TFCTRLLMHeadModel">TFCTRLLMHeadModel</a> (CTRL model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/gpt2#transformers.TFGPT2LMHeadModel">TFGPT2LMHeadModel</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig">OpenAIGPTConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/openai-gpt#transformers.TFOpenAIGPTLMHeadModel">TFOpenAIGPTLMHeadModel</a> (OpenAI GPT model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/rembert#transformers.TFRemBertForCausalLM">TFRemBertForCausalLM</a> (RemBERT model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/roformer#transformers.TFRoFormerForCausalLM">TFRoFormerForCausalLM</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/roberta#transformers.TFRobertaForCausalLM">TFRobertaForCausalLM</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/transfo-xl#transformers.TransfoXLConfig">TransfoXLConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/transfo-xl#transformers.TFTransfoXLLMHeadModel">TFTransfoXLLMHeadModel</a> (Transformer-XL model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/xlm#transformers.TFXLMWithLMHeadModel">TFXLMWithLMHeadModel</a> (XLM model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/xlnet#transformers.TFXLNetLMHeadModel">TFXLNetLMHeadModel</a> (XLNet model)</li>
</ul>`,name:"config"}]}}),Yy=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForCausalLM

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = TFAutoModelForCausalLM.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForCausalLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForCausalLM.from_config(config)`}}),Ky=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15770/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15770/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15770/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15770/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15770/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),Zy=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForCausalLM

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForCausalLM.from_pretrained("bert-base-cased")

# Update configuration during loading
model = TFAutoModelForCausalLM.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = TFAutoModelForCausalLM.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForCausalLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForCausalLM.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),ew=new z({}),ow=new E({props:{name:"class transformers.TFAutoModelForImageClassification",anchor:"transformers.TFAutoModelForImageClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15770/src/transformers/models/auto/modeling_tf_auto.py#L400"}}),tw=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15770/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15770/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15770/en/model_doc/vit#transformers.ViTConfig">ViTConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/vit#transformers.TFViTForImageClassification">TFViTForImageClassification</a> (ViT model)</li>
</ul>`,name:"config"}]}}),aw=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForImageClassification

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = TFAutoModelForImageClassification.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForImageClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForImageClassification.from_config(config)`}}),nw=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15770/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15770/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15770/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15770/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15770/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),sw=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForImageClassification

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForImageClassification.from_pretrained("bert-base-cased")

# Update configuration during loading
model = TFAutoModelForImageClassification.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = TFAutoModelForImageClassification.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForImageClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForImageClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForImageClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForImageClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),lw=new z({}),iw=new E({props:{name:"class transformers.TFAutoModelForMaskedLM",anchor:"transformers.TFAutoModelForMaskedLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15770/src/transformers/models/auto/modeling_tf_auto.py#L414"}}),cw=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15770/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15770/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15770/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/albert#transformers.TFAlbertForMaskedLM">TFAlbertForMaskedLM</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/bert#transformers.TFBertForMaskedLM">TFBertForMaskedLM</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/camembert#transformers.TFCamembertForMaskedLM">TFCamembertForMaskedLM</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/convbert#transformers.TFConvBertForMaskedLM">TFConvBertForMaskedLM</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/deberta#transformers.TFDebertaForMaskedLM">TFDebertaForMaskedLM</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/deberta-v2#transformers.TFDebertaV2ForMaskedLM">TFDebertaV2ForMaskedLM</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/distilbert#transformers.TFDistilBertForMaskedLM">TFDistilBertForMaskedLM</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/electra#transformers.TFElectraForMaskedLM">TFElectraForMaskedLM</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/flaubert#transformers.TFFlaubertWithLMHeadModel">TFFlaubertWithLMHeadModel</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/funnel#transformers.TFFunnelForMaskedLM">TFFunnelForMaskedLM</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/layoutlm#transformers.TFLayoutLMForMaskedLM">TFLayoutLMForMaskedLM</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/longformer#transformers.TFLongformerForMaskedLM">TFLongformerForMaskedLM</a> (Longformer model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/mpnet#transformers.TFMPNetForMaskedLM">TFMPNetForMaskedLM</a> (MPNet model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/mobilebert#transformers.TFMobileBertForMaskedLM">TFMobileBertForMaskedLM</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/rembert#transformers.TFRemBertForMaskedLM">TFRemBertForMaskedLM</a> (RemBERT model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/roformer#transformers.TFRoFormerForMaskedLM">TFRoFormerForMaskedLM</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/roberta#transformers.TFRobertaForMaskedLM">TFRobertaForMaskedLM</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/tapas#transformers.TFTapasForMaskedLM">TFTapasForMaskedLM</a> (TAPAS model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/xlm#transformers.TFXLMWithLMHeadModel">TFXLMWithLMHeadModel</a> (XLM model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForMaskedLM">TFXLMRobertaForMaskedLM</a> (XLM-RoBERTa model)</li>
</ul>`,name:"config"}]}}),fw=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForMaskedLM

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = TFAutoModelForMaskedLM.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForMaskedLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForMaskedLM.from_config(config)`}}),mw=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15770/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15770/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15770/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15770/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15770/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),gw=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForMaskedLM

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForMaskedLM.from_pretrained("bert-base-cased")

# Update configuration during loading
model = TFAutoModelForMaskedLM.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = TFAutoModelForMaskedLM.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForMaskedLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForMaskedLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForMaskedLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForMaskedLM.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),hw=new z({}),pw=new E({props:{name:"class transformers.TFAutoModelForSeq2SeqLM",anchor:"transformers.TFAutoModelForSeq2SeqLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15770/src/transformers/models/auto/modeling_tf_auto.py#L421"}}),uw=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15770/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15770/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15770/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/bart#transformers.TFBartForConditionalGeneration">TFBartForConditionalGeneration</a> (BART model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/blenderbot#transformers.BlenderbotConfig">BlenderbotConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/blenderbot#transformers.TFBlenderbotForConditionalGeneration">TFBlenderbotForConditionalGeneration</a> (Blenderbot model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/blenderbot-small#transformers.BlenderbotSmallConfig">BlenderbotSmallConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/blenderbot-small#transformers.TFBlenderbotSmallForConditionalGeneration">TFBlenderbotSmallForConditionalGeneration</a> (BlenderbotSmall model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/encoder-decoder#transformers.EncoderDecoderConfig">EncoderDecoderConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/encoder-decoder#transformers.TFEncoderDecoderModel">TFEncoderDecoderModel</a> (Encoder decoder model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/led#transformers.LEDConfig">LEDConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/led#transformers.TFLEDForConditionalGeneration">TFLEDForConditionalGeneration</a> (LED model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/mbart#transformers.TFMBartForConditionalGeneration">TFMBartForConditionalGeneration</a> (mBART model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/mt5#transformers.MT5Config">MT5Config</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/mt5#transformers.TFMT5ForConditionalGeneration">TFMT5ForConditionalGeneration</a> (mT5 model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/marian#transformers.MarianConfig">MarianConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/marian#transformers.TFMarianMTModel">TFMarianMTModel</a> (Marian model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/pegasus#transformers.PegasusConfig">PegasusConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/pegasus#transformers.TFPegasusForConditionalGeneration">TFPegasusForConditionalGeneration</a> (Pegasus model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/t5#transformers.T5Config">T5Config</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/t5#transformers.TFT5ForConditionalGeneration">TFT5ForConditionalGeneration</a> (T5 model)</li>
</ul>`,name:"config"}]}}),bw=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForSeq2SeqLM

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("t5-base")
model = TFAutoModelForSeq2SeqLM.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForSeq2SeqLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;t5-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSeq2SeqLM.from_config(config)`}}),vw=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15770/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15770/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15770/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15770/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15770/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),Tw=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForSeq2SeqLM

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForSeq2SeqLM.from_pretrained("t5-base")

# Update configuration during loading
model = TFAutoModelForSeq2SeqLM.from_pretrained("t5-base", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/t5_pt_model_config.json")
model = TFAutoModelForSeq2SeqLM.from_pretrained(
    "./pt_model/t5_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForSeq2SeqLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSeq2SeqLM.from_pretrained(<span class="hljs-string">&quot;t5-base&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSeq2SeqLM.from_pretrained(<span class="hljs-string">&quot;t5-base&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/t5_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSeq2SeqLM.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/t5_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),Fw=new z({}),Cw=new E({props:{name:"class transformers.TFAutoModelForSequenceClassification",anchor:"transformers.TFAutoModelForSequenceClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15770/src/transformers/models/auto/modeling_tf_auto.py#L430"}}),Ew=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15770/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15770/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15770/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/albert#transformers.TFAlbertForSequenceClassification">TFAlbertForSequenceClassification</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/bert#transformers.TFBertForSequenceClassification">TFBertForSequenceClassification</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/ctrl#transformers.CTRLConfig">CTRLConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/ctrl#transformers.TFCTRLForSequenceClassification">TFCTRLForSequenceClassification</a> (CTRL model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/camembert#transformers.TFCamembertForSequenceClassification">TFCamembertForSequenceClassification</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/convbert#transformers.TFConvBertForSequenceClassification">TFConvBertForSequenceClassification</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/deberta#transformers.TFDebertaForSequenceClassification">TFDebertaForSequenceClassification</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/deberta-v2#transformers.TFDebertaV2ForSequenceClassification">TFDebertaV2ForSequenceClassification</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/distilbert#transformers.TFDistilBertForSequenceClassification">TFDistilBertForSequenceClassification</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/electra#transformers.TFElectraForSequenceClassification">TFElectraForSequenceClassification</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/flaubert#transformers.TFFlaubertForSequenceClassification">TFFlaubertForSequenceClassification</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/funnel#transformers.TFFunnelForSequenceClassification">TFFunnelForSequenceClassification</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/gpt2#transformers.TFGPT2ForSequenceClassification">TFGPT2ForSequenceClassification</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/layoutlm#transformers.TFLayoutLMForSequenceClassification">TFLayoutLMForSequenceClassification</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/longformer#transformers.TFLongformerForSequenceClassification">TFLongformerForSequenceClassification</a> (Longformer model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/mpnet#transformers.TFMPNetForSequenceClassification">TFMPNetForSequenceClassification</a> (MPNet model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/mobilebert#transformers.TFMobileBertForSequenceClassification">TFMobileBertForSequenceClassification</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig">OpenAIGPTConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/openai-gpt#transformers.TFOpenAIGPTForSequenceClassification">TFOpenAIGPTForSequenceClassification</a> (OpenAI GPT model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/rembert#transformers.TFRemBertForSequenceClassification">TFRemBertForSequenceClassification</a> (RemBERT model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/roformer#transformers.TFRoFormerForSequenceClassification">TFRoFormerForSequenceClassification</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/roberta#transformers.TFRobertaForSequenceClassification">TFRobertaForSequenceClassification</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/tapas#transformers.TFTapasForSequenceClassification">TFTapasForSequenceClassification</a> (TAPAS model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/transfo-xl#transformers.TransfoXLConfig">TransfoXLConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/transfo-xl#transformers.TFTransfoXLForSequenceClassification">TFTransfoXLForSequenceClassification</a> (Transformer-XL model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/xlm#transformers.TFXLMForSequenceClassification">TFXLMForSequenceClassification</a> (XLM model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForSequenceClassification">TFXLMRobertaForSequenceClassification</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/xlnet#transformers.TFXLNetForSequenceClassification">TFXLNetForSequenceClassification</a> (XLNet model)</li>
</ul>`,name:"config"}]}}),yw=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForSequenceClassification

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = TFAutoModelForSequenceClassification.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSequenceClassification.from_config(config)`}}),ww=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15770/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15770/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15770/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15770/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15770/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),Aw=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForSequenceClassification

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForSequenceClassification.from_pretrained("bert-base-cased")

# Update configuration during loading
model = TFAutoModelForSequenceClassification.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = TFAutoModelForSequenceClassification.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSequenceClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),Lw=new z({}),Bw=new E({props:{name:"class transformers.TFAutoModelForMultipleChoice",anchor:"transformers.TFAutoModelForMultipleChoice",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15770/src/transformers/models/auto/modeling_tf_auto.py#L466"}}),xw=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15770/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15770/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15770/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/albert#transformers.TFAlbertForMultipleChoice">TFAlbertForMultipleChoice</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/bert#transformers.TFBertForMultipleChoice">TFBertForMultipleChoice</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/camembert#transformers.TFCamembertForMultipleChoice">TFCamembertForMultipleChoice</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/convbert#transformers.TFConvBertForMultipleChoice">TFConvBertForMultipleChoice</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/distilbert#transformers.TFDistilBertForMultipleChoice">TFDistilBertForMultipleChoice</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/electra#transformers.TFElectraForMultipleChoice">TFElectraForMultipleChoice</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/flaubert#transformers.TFFlaubertForMultipleChoice">TFFlaubertForMultipleChoice</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/funnel#transformers.TFFunnelForMultipleChoice">TFFunnelForMultipleChoice</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/longformer#transformers.TFLongformerForMultipleChoice">TFLongformerForMultipleChoice</a> (Longformer model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/mpnet#transformers.TFMPNetForMultipleChoice">TFMPNetForMultipleChoice</a> (MPNet model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/mobilebert#transformers.TFMobileBertForMultipleChoice">TFMobileBertForMultipleChoice</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/rembert#transformers.TFRemBertForMultipleChoice">TFRemBertForMultipleChoice</a> (RemBERT model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/roformer#transformers.TFRoFormerForMultipleChoice">TFRoFormerForMultipleChoice</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/roberta#transformers.TFRobertaForMultipleChoice">TFRobertaForMultipleChoice</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/xlm#transformers.TFXLMForMultipleChoice">TFXLMForMultipleChoice</a> (XLM model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForMultipleChoice">TFXLMRobertaForMultipleChoice</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/xlnet#transformers.TFXLNetForMultipleChoice">TFXLNetForMultipleChoice</a> (XLNet model)</li>
</ul>`,name:"config"}]}}),Rw=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForMultipleChoice

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = TFAutoModelForMultipleChoice.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForMultipleChoice

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForMultipleChoice.from_config(config)`}}),Sw=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15770/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15770/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15770/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15770/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15770/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),Pw=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForMultipleChoice

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForMultipleChoice.from_pretrained("bert-base-cased")

# Update configuration during loading
model = TFAutoModelForMultipleChoice.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = TFAutoModelForMultipleChoice.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForMultipleChoice

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForMultipleChoice.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForMultipleChoice.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForMultipleChoice.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),$w=new z({}),Iw=new E({props:{name:"class transformers.TFAutoModelForTableQuestionAnswering",anchor:"transformers.TFAutoModelForTableQuestionAnswering",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15770/src/transformers/models/auto/modeling_tf_auto.py#L446"}}),Nw=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15770/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15770/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15770/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/tapas#transformers.TFTapasForQuestionAnswering">TFTapasForQuestionAnswering</a> (TAPAS model)</li>
</ul>`,name:"config"}]}}),Dw=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForTableQuestionAnswering

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("google/tapas-base-finetuned-wtq")
model = TFAutoModelForTableQuestionAnswering.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForTableQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;google/tapas-base-finetuned-wtq&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForTableQuestionAnswering.from_config(config)`}}),qw=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15770/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15770/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15770/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15770/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15770/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),Gw=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForTableQuestionAnswering

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForTableQuestionAnswering.from_pretrained("google/tapas-base-finetuned-wtq")

# Update configuration during loading
model = TFAutoModelForTableQuestionAnswering.from_pretrained("google/tapas-base-finetuned-wtq", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/tapas_pt_model_config.json")
model = TFAutoModelForTableQuestionAnswering.from_pretrained(
    "./pt_model/tapas_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForTableQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForTableQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;google/tapas-base-finetuned-wtq&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForTableQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;google/tapas-base-finetuned-wtq&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/tapas_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForTableQuestionAnswering.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/tapas_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),Ow=new z({}),Xw=new E({props:{name:"class transformers.TFAutoModelForTokenClassification",anchor:"transformers.TFAutoModelForTokenClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15770/src/transformers/models/auto/modeling_tf_auto.py#L457"}}),Vw=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15770/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15770/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15770/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/albert#transformers.TFAlbertForTokenClassification">TFAlbertForTokenClassification</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/bert#transformers.TFBertForTokenClassification">TFBertForTokenClassification</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/camembert#transformers.TFCamembertForTokenClassification">TFCamembertForTokenClassification</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/convbert#transformers.TFConvBertForTokenClassification">TFConvBertForTokenClassification</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/deberta#transformers.TFDebertaForTokenClassification">TFDebertaForTokenClassification</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/deberta-v2#transformers.TFDebertaV2ForTokenClassification">TFDebertaV2ForTokenClassification</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/distilbert#transformers.TFDistilBertForTokenClassification">TFDistilBertForTokenClassification</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/electra#transformers.TFElectraForTokenClassification">TFElectraForTokenClassification</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/flaubert#transformers.TFFlaubertForTokenClassification">TFFlaubertForTokenClassification</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/funnel#transformers.TFFunnelForTokenClassification">TFFunnelForTokenClassification</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/layoutlm#transformers.TFLayoutLMForTokenClassification">TFLayoutLMForTokenClassification</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/longformer#transformers.TFLongformerForTokenClassification">TFLongformerForTokenClassification</a> (Longformer model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/mpnet#transformers.TFMPNetForTokenClassification">TFMPNetForTokenClassification</a> (MPNet model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/mobilebert#transformers.TFMobileBertForTokenClassification">TFMobileBertForTokenClassification</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/rembert#transformers.TFRemBertForTokenClassification">TFRemBertForTokenClassification</a> (RemBERT model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/roformer#transformers.TFRoFormerForTokenClassification">TFRoFormerForTokenClassification</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/roberta#transformers.TFRobertaForTokenClassification">TFRobertaForTokenClassification</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/xlm#transformers.TFXLMForTokenClassification">TFXLMForTokenClassification</a> (XLM model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForTokenClassification">TFXLMRobertaForTokenClassification</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/xlnet#transformers.TFXLNetForTokenClassification">TFXLNetForTokenClassification</a> (XLNet model)</li>
</ul>`,name:"config"}]}}),Ww=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForTokenClassification

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = TFAutoModelForTokenClassification.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForTokenClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForTokenClassification.from_config(config)`}}),Qw=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15770/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15770/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15770/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15770/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15770/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),Hw=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForTokenClassification

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForTokenClassification.from_pretrained("bert-base-cased")

# Update configuration during loading
model = TFAutoModelForTokenClassification.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = TFAutoModelForTokenClassification.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForTokenClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForTokenClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForTokenClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForTokenClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),Uw=new z({}),Jw=new E({props:{name:"class transformers.TFAutoModelForQuestionAnswering",anchor:"transformers.TFAutoModelForQuestionAnswering",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15770/src/transformers/models/auto/modeling_tf_auto.py#L439"}}),Kw=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15770/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15770/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15770/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/albert#transformers.TFAlbertForQuestionAnswering">TFAlbertForQuestionAnswering</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/bert#transformers.TFBertForQuestionAnswering">TFBertForQuestionAnswering</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/camembert#transformers.TFCamembertForQuestionAnswering">TFCamembertForQuestionAnswering</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/convbert#transformers.TFConvBertForQuestionAnswering">TFConvBertForQuestionAnswering</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/deberta#transformers.TFDebertaForQuestionAnswering">TFDebertaForQuestionAnswering</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/deberta-v2#transformers.TFDebertaV2ForQuestionAnswering">TFDebertaV2ForQuestionAnswering</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/distilbert#transformers.TFDistilBertForQuestionAnswering">TFDistilBertForQuestionAnswering</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/electra#transformers.TFElectraForQuestionAnswering">TFElectraForQuestionAnswering</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/flaubert#transformers.TFFlaubertForQuestionAnsweringSimple">TFFlaubertForQuestionAnsweringSimple</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/funnel#transformers.TFFunnelForQuestionAnswering">TFFunnelForQuestionAnswering</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/longformer#transformers.TFLongformerForQuestionAnswering">TFLongformerForQuestionAnswering</a> (Longformer model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/mpnet#transformers.TFMPNetForQuestionAnswering">TFMPNetForQuestionAnswering</a> (MPNet model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/mobilebert#transformers.TFMobileBertForQuestionAnswering">TFMobileBertForQuestionAnswering</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/rembert#transformers.TFRemBertForQuestionAnswering">TFRemBertForQuestionAnswering</a> (RemBERT model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/roformer#transformers.TFRoFormerForQuestionAnswering">TFRoFormerForQuestionAnswering</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/roberta#transformers.TFRobertaForQuestionAnswering">TFRobertaForQuestionAnswering</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/xlm#transformers.TFXLMForQuestionAnsweringSimple">TFXLMForQuestionAnsweringSimple</a> (XLM model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForQuestionAnswering">TFXLMRobertaForQuestionAnswering</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/xlnet#transformers.TFXLNetForQuestionAnsweringSimple">TFXLNetForQuestionAnsweringSimple</a> (XLNet model)</li>
</ul>`,name:"config"}]}}),Zw=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForQuestionAnswering

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = TFAutoModelForQuestionAnswering.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForQuestionAnswering.from_config(config)`}}),e6=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15770/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15770/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15770/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15770/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15770/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),o6=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForQuestionAnswering

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForQuestionAnswering.from_pretrained("bert-base-cased")

# Update configuration during loading
model = TFAutoModelForQuestionAnswering.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = TFAutoModelForQuestionAnswering.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForQuestionAnswering.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),r6=new z({}),t6=new E({props:{name:"class transformers.TFAutoModelForVision2Seq",anchor:"transformers.TFAutoModelForVision2Seq",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15770/src/transformers/models/auto/modeling_tf_auto.py#L407"}}),n6=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15770/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15770/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15770/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderConfig">VisionEncoderDecoderConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/vision-encoder-decoder#transformers.TFVisionEncoderDecoderModel">TFVisionEncoderDecoderModel</a> (Vision Encoder decoder model)</li>
</ul>`,name:"config"}]}}),s6=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForVision2Seq

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = TFAutoModelForVision2Seq.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForVision2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForVision2Seq.from_config(config)`}}),l6=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15770/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15770/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15770/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15770/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15770/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),i6=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForVision2Seq

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForVision2Seq.from_pretrained("bert-base-cased")

# Update configuration during loading
model = TFAutoModelForVision2Seq.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = TFAutoModelForVision2Seq.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForVision2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForVision2Seq.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForVision2Seq.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForVision2Seq.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),d6=new z({}),c6=new E({props:{name:"class transformers.TFAutoModelForSpeechSeq2Seq",anchor:"transformers.TFAutoModelForSpeechSeq2Seq",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15770/src/transformers/models/auto/modeling_tf_auto.py#L482"}}),m6=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15770/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15770/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15770/en/model_doc/speech_to_text#transformers.Speech2TextConfig">Speech2TextConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/speech_to_text#transformers.TFSpeech2TextForConditionalGeneration">TFSpeech2TextForConditionalGeneration</a> (Speech2Text model)</li>
</ul>`,name:"config"}]}}),g6=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForSpeechSeq2Seq

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = TFAutoModelForSpeechSeq2Seq.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForSpeechSeq2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSpeechSeq2Seq.from_config(config)`}}),h6=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15770/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15770/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15770/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15770/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15770/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),p6=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForSpeechSeq2Seq

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForSpeechSeq2Seq.from_pretrained("bert-base-cased")

# Update configuration during loading
model = TFAutoModelForSpeechSeq2Seq.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = TFAutoModelForSpeechSeq2Seq.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForSpeechSeq2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSpeechSeq2Seq.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSpeechSeq2Seq.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSpeechSeq2Seq.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),_6=new z({}),u6=new E({props:{name:"class transformers.FlaxAutoModel",anchor:"transformers.FlaxAutoModel",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15770/src/transformers/models/auto/modeling_flax_auto.py#L220"}}),v6=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15770/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15770/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15770/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/albert#transformers.FlaxAlbertModel">FlaxAlbertModel</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/bart#transformers.FlaxBartModel">FlaxBartModel</a> (BART model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/beit#transformers.BeitConfig">BeitConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/beit#transformers.FlaxBeitModel">FlaxBeitModel</a> (BEiT model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/bert#transformers.FlaxBertModel">FlaxBertModel</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/big_bird#transformers.FlaxBigBirdModel">FlaxBigBirdModel</a> (BigBird model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/blenderbot#transformers.BlenderbotConfig">BlenderbotConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/blenderbot#transformers.FlaxBlenderbotModel">FlaxBlenderbotModel</a> (Blenderbot model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/blenderbot-small#transformers.BlenderbotSmallConfig">BlenderbotSmallConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/blenderbot-small#transformers.FlaxBlenderbotSmallModel">FlaxBlenderbotSmallModel</a> (BlenderbotSmall model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/clip#transformers.CLIPConfig">CLIPConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/clip#transformers.FlaxCLIPModel">FlaxCLIPModel</a> (CLIP model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/distilbert#transformers.FlaxDistilBertModel">FlaxDistilBertModel</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/electra#transformers.FlaxElectraModel">FlaxElectraModel</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/gpt2#transformers.FlaxGPT2Model">FlaxGPT2Model</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/gptj#transformers.GPTJConfig">GPTJConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/gptj#transformers.FlaxGPTJModel">FlaxGPTJModel</a> (GPT-J model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/gpt_neo#transformers.GPTNeoConfig">GPTNeoConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/gpt_neo#transformers.FlaxGPTNeoModel">FlaxGPTNeoModel</a> (GPT Neo model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/mbart#transformers.FlaxMBartModel">FlaxMBartModel</a> (mBART model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/mt5#transformers.MT5Config">MT5Config</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/mt5#transformers.FlaxMT5Model">FlaxMT5Model</a> (mT5 model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/marian#transformers.MarianConfig">MarianConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/marian#transformers.FlaxMarianModel">FlaxMarianModel</a> (Marian model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/pegasus#transformers.PegasusConfig">PegasusConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/pegasus#transformers.FlaxPegasusModel">FlaxPegasusModel</a> (Pegasus model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/roformer#transformers.FlaxRoFormerModel">FlaxRoFormerModel</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/roberta#transformers.FlaxRobertaModel">FlaxRobertaModel</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/t5#transformers.T5Config">T5Config</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/t5#transformers.FlaxT5Model">FlaxT5Model</a> (T5 model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/vit#transformers.ViTConfig">ViTConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/vit#transformers.FlaxViTModel">FlaxViTModel</a> (ViT model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/vision-text-dual-encoder#transformers.VisionTextDualEncoderConfig">VisionTextDualEncoderConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/vision-text-dual-encoder#transformers.FlaxVisionTextDualEncoderModel">FlaxVisionTextDualEncoderModel</a> (VisionTextDualEncoder model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/wav2vec2#transformers.FlaxWav2Vec2Model">FlaxWav2Vec2Model</a> (Wav2Vec2 model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/xglm#transformers.XGLMConfig">XGLMConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/xglm#transformers.FlaxXGLMModel">FlaxXGLMModel</a> (XGLM model)</li>
</ul>`,name:"config"}]}}),T6=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModel

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = FlaxAutoModel.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModel.from_config(config)`}}),F6=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15770/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15770/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15770/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15770/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15770/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),C6=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModel

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModel.from_pretrained("bert-base-cased")

# Update configuration during loading
model = FlaxAutoModel.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = FlaxAutoModel.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModel.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModel.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModel.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),M6=new z({}),E6=new E({props:{name:"class transformers.FlaxAutoModelForCausalLM",anchor:"transformers.FlaxAutoModelForCausalLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15770/src/transformers/models/auto/modeling_flax_auto.py#L234"}}),w6=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15770/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15770/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15770/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/gpt2#transformers.FlaxGPT2LMHeadModel">FlaxGPT2LMHeadModel</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/gptj#transformers.GPTJConfig">GPTJConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/gptj#transformers.FlaxGPTJForCausalLM">FlaxGPTJForCausalLM</a> (GPT-J model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/gpt_neo#transformers.GPTNeoConfig">GPTNeoConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/gpt_neo#transformers.FlaxGPTNeoForCausalLM">FlaxGPTNeoForCausalLM</a> (GPT Neo model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/xglm#transformers.XGLMConfig">XGLMConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/xglm#transformers.FlaxXGLMForCausalLM">FlaxXGLMForCausalLM</a> (XGLM model)</li>
</ul>`,name:"config"}]}}),A6=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForCausalLM

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = FlaxAutoModelForCausalLM.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForCausalLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForCausalLM.from_config(config)`}}),L6=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15770/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15770/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15770/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15770/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15770/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),B6=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForCausalLM

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForCausalLM.from_pretrained("bert-base-cased")

# Update configuration during loading
model = FlaxAutoModelForCausalLM.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = FlaxAutoModelForCausalLM.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForCausalLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForCausalLM.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),k6=new z({}),x6=new E({props:{name:"class transformers.FlaxAutoModelForPreTraining",anchor:"transformers.FlaxAutoModelForPreTraining",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15770/src/transformers/models/auto/modeling_flax_auto.py#L227"}}),S6=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15770/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15770/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15770/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/albert#transformers.FlaxAlbertForPreTraining">FlaxAlbertForPreTraining</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/bart#transformers.FlaxBartForConditionalGeneration">FlaxBartForConditionalGeneration</a> (BART model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/bert#transformers.FlaxBertForPreTraining">FlaxBertForPreTraining</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/big_bird#transformers.FlaxBigBirdForPreTraining">FlaxBigBirdForPreTraining</a> (BigBird model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/electra#transformers.FlaxElectraForPreTraining">FlaxElectraForPreTraining</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/mbart#transformers.FlaxMBartForConditionalGeneration">FlaxMBartForConditionalGeneration</a> (mBART model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/mt5#transformers.MT5Config">MT5Config</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/mt5#transformers.FlaxMT5ForConditionalGeneration">FlaxMT5ForConditionalGeneration</a> (mT5 model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/roformer#transformers.FlaxRoFormerForMaskedLM">FlaxRoFormerForMaskedLM</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/roberta#transformers.FlaxRobertaForMaskedLM">FlaxRobertaForMaskedLM</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/t5#transformers.T5Config">T5Config</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/t5#transformers.FlaxT5ForConditionalGeneration">FlaxT5ForConditionalGeneration</a> (T5 model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/wav2vec2#transformers.FlaxWav2Vec2ForPreTraining">FlaxWav2Vec2ForPreTraining</a> (Wav2Vec2 model)</li>
</ul>`,name:"config"}]}}),P6=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForPreTraining

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = FlaxAutoModelForPreTraining.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForPreTraining

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForPreTraining.from_config(config)`}}),$6=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15770/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15770/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15770/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15770/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15770/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),I6=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForPreTraining

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForPreTraining.from_pretrained("bert-base-cased")

# Update configuration during loading
model = FlaxAutoModelForPreTraining.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = FlaxAutoModelForPreTraining.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForPreTraining

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForPreTraining.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForPreTraining.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForPreTraining.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),j6=new z({}),N6=new E({props:{name:"class transformers.FlaxAutoModelForMaskedLM",anchor:"transformers.FlaxAutoModelForMaskedLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15770/src/transformers/models/auto/modeling_flax_auto.py#L241"}}),q6=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15770/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15770/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15770/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/albert#transformers.FlaxAlbertForMaskedLM">FlaxAlbertForMaskedLM</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/bart#transformers.FlaxBartForConditionalGeneration">FlaxBartForConditionalGeneration</a> (BART model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/bert#transformers.FlaxBertForMaskedLM">FlaxBertForMaskedLM</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/big_bird#transformers.FlaxBigBirdForMaskedLM">FlaxBigBirdForMaskedLM</a> (BigBird model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/distilbert#transformers.FlaxDistilBertForMaskedLM">FlaxDistilBertForMaskedLM</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/electra#transformers.FlaxElectraForMaskedLM">FlaxElectraForMaskedLM</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/mbart#transformers.FlaxMBartForConditionalGeneration">FlaxMBartForConditionalGeneration</a> (mBART model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/roformer#transformers.FlaxRoFormerForMaskedLM">FlaxRoFormerForMaskedLM</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/roberta#transformers.FlaxRobertaForMaskedLM">FlaxRobertaForMaskedLM</a> (RoBERTa model)</li>
</ul>`,name:"config"}]}}),G6=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForMaskedLM

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = FlaxAutoModelForMaskedLM.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForMaskedLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForMaskedLM.from_config(config)`}}),O6=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15770/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15770/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15770/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15770/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15770/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),X6=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForMaskedLM

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForMaskedLM.from_pretrained("bert-base-cased")

# Update configuration during loading
model = FlaxAutoModelForMaskedLM.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = FlaxAutoModelForMaskedLM.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForMaskedLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForMaskedLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForMaskedLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForMaskedLM.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),z6=new z({}),V6=new E({props:{name:"class transformers.FlaxAutoModelForSeq2SeqLM",anchor:"transformers.FlaxAutoModelForSeq2SeqLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15770/src/transformers/models/auto/modeling_flax_auto.py#L248"}}),Q6=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15770/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15770/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15770/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/bart#transformers.FlaxBartForConditionalGeneration">FlaxBartForConditionalGeneration</a> (BART model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/blenderbot#transformers.BlenderbotConfig">BlenderbotConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/blenderbot#transformers.FlaxBlenderbotForConditionalGeneration">FlaxBlenderbotForConditionalGeneration</a> (Blenderbot model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/blenderbot-small#transformers.BlenderbotSmallConfig">BlenderbotSmallConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/blenderbot-small#transformers.FlaxBlenderbotSmallForConditionalGeneration">FlaxBlenderbotSmallForConditionalGeneration</a> (BlenderbotSmall model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/encoder-decoder#transformers.EncoderDecoderConfig">EncoderDecoderConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/encoder-decoder#transformers.FlaxEncoderDecoderModel">FlaxEncoderDecoderModel</a> (Encoder decoder model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/mbart#transformers.FlaxMBartForConditionalGeneration">FlaxMBartForConditionalGeneration</a> (mBART model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/mt5#transformers.MT5Config">MT5Config</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/mt5#transformers.FlaxMT5ForConditionalGeneration">FlaxMT5ForConditionalGeneration</a> (mT5 model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/marian#transformers.MarianConfig">MarianConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/marian#transformers.FlaxMarianMTModel">FlaxMarianMTModel</a> (Marian model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/pegasus#transformers.PegasusConfig">PegasusConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/pegasus#transformers.FlaxPegasusForConditionalGeneration">FlaxPegasusForConditionalGeneration</a> (Pegasus model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/t5#transformers.T5Config">T5Config</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/t5#transformers.FlaxT5ForConditionalGeneration">FlaxT5ForConditionalGeneration</a> (T5 model)</li>
</ul>`,name:"config"}]}}),H6=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForSeq2SeqLM

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("t5-base")
model = FlaxAutoModelForSeq2SeqLM.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForSeq2SeqLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;t5-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForSeq2SeqLM.from_config(config)`}}),U6=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15770/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15770/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15770/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15770/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15770/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),J6=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForSeq2SeqLM

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForSeq2SeqLM.from_pretrained("t5-base")

# Update configuration during loading
model = FlaxAutoModelForSeq2SeqLM.from_pretrained("t5-base", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/t5_pt_model_config.json")
model = FlaxAutoModelForSeq2SeqLM.from_pretrained(
    "./pt_model/t5_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForSeq2SeqLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForSeq2SeqLM.from_pretrained(<span class="hljs-string">&quot;t5-base&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForSeq2SeqLM.from_pretrained(<span class="hljs-string">&quot;t5-base&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/t5_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForSeq2SeqLM.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/t5_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),Y6=new z({}),K6=new E({props:{name:"class transformers.FlaxAutoModelForSequenceClassification",anchor:"transformers.FlaxAutoModelForSequenceClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15770/src/transformers/models/auto/modeling_flax_auto.py#L257"}}),eA=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15770/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15770/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15770/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/albert#transformers.FlaxAlbertForSequenceClassification">FlaxAlbertForSequenceClassification</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/bart#transformers.FlaxBartForSequenceClassification">FlaxBartForSequenceClassification</a> (BART model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/bert#transformers.FlaxBertForSequenceClassification">FlaxBertForSequenceClassification</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/big_bird#transformers.FlaxBigBirdForSequenceClassification">FlaxBigBirdForSequenceClassification</a> (BigBird model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/distilbert#transformers.FlaxDistilBertForSequenceClassification">FlaxDistilBertForSequenceClassification</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/electra#transformers.FlaxElectraForSequenceClassification">FlaxElectraForSequenceClassification</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/mbart#transformers.FlaxMBartForSequenceClassification">FlaxMBartForSequenceClassification</a> (mBART model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/roformer#transformers.FlaxRoFormerForSequenceClassification">FlaxRoFormerForSequenceClassification</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/roberta#transformers.FlaxRobertaForSequenceClassification">FlaxRobertaForSequenceClassification</a> (RoBERTa model)</li>
</ul>`,name:"config"}]}}),oA=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForSequenceClassification

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = FlaxAutoModelForSequenceClassification.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForSequenceClassification.from_config(config)`}}),rA=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15770/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15770/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15770/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15770/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15770/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),tA=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForSequenceClassification

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForSequenceClassification.from_pretrained("bert-base-cased")

# Update configuration during loading
model = FlaxAutoModelForSequenceClassification.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = FlaxAutoModelForSequenceClassification.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForSequenceClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),aA=new z({}),nA=new E({props:{name:"class transformers.FlaxAutoModelForQuestionAnswering",anchor:"transformers.FlaxAutoModelForQuestionAnswering",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15770/src/transformers/models/auto/modeling_flax_auto.py#L266"}}),lA=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15770/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15770/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15770/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/albert#transformers.FlaxAlbertForQuestionAnswering">FlaxAlbertForQuestionAnswering</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/bart#transformers.FlaxBartForQuestionAnswering">FlaxBartForQuestionAnswering</a> (BART model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/bert#transformers.FlaxBertForQuestionAnswering">FlaxBertForQuestionAnswering</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/big_bird#transformers.FlaxBigBirdForQuestionAnswering">FlaxBigBirdForQuestionAnswering</a> (BigBird model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/distilbert#transformers.FlaxDistilBertForQuestionAnswering">FlaxDistilBertForQuestionAnswering</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/electra#transformers.FlaxElectraForQuestionAnswering">FlaxElectraForQuestionAnswering</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/mbart#transformers.FlaxMBartForQuestionAnswering">FlaxMBartForQuestionAnswering</a> (mBART model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/roformer#transformers.FlaxRoFormerForQuestionAnswering">FlaxRoFormerForQuestionAnswering</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/roberta#transformers.FlaxRobertaForQuestionAnswering">FlaxRobertaForQuestionAnswering</a> (RoBERTa model)</li>
</ul>`,name:"config"}]}}),iA=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForQuestionAnswering

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = FlaxAutoModelForQuestionAnswering.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForQuestionAnswering.from_config(config)`}}),dA=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15770/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15770/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15770/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15770/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15770/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),cA=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForQuestionAnswering

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForQuestionAnswering.from_pretrained("bert-base-cased")

# Update configuration during loading
model = FlaxAutoModelForQuestionAnswering.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = FlaxAutoModelForQuestionAnswering.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForQuestionAnswering.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),fA=new z({}),mA=new E({props:{name:"class transformers.FlaxAutoModelForTokenClassification",anchor:"transformers.FlaxAutoModelForTokenClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15770/src/transformers/models/auto/modeling_flax_auto.py#L273"}}),hA=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15770/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15770/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15770/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/albert#transformers.FlaxAlbertForTokenClassification">FlaxAlbertForTokenClassification</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/bert#transformers.FlaxBertForTokenClassification">FlaxBertForTokenClassification</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/big_bird#transformers.FlaxBigBirdForTokenClassification">FlaxBigBirdForTokenClassification</a> (BigBird model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/distilbert#transformers.FlaxDistilBertForTokenClassification">FlaxDistilBertForTokenClassification</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/electra#transformers.FlaxElectraForTokenClassification">FlaxElectraForTokenClassification</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/roformer#transformers.FlaxRoFormerForTokenClassification">FlaxRoFormerForTokenClassification</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/roberta#transformers.FlaxRobertaForTokenClassification">FlaxRobertaForTokenClassification</a> (RoBERTa model)</li>
</ul>`,name:"config"}]}}),pA=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForTokenClassification

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = FlaxAutoModelForTokenClassification.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForTokenClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForTokenClassification.from_config(config)`}}),_A=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15770/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15770/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15770/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15770/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15770/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),uA=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForTokenClassification

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForTokenClassification.from_pretrained("bert-base-cased")

# Update configuration during loading
model = FlaxAutoModelForTokenClassification.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = FlaxAutoModelForTokenClassification.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForTokenClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForTokenClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForTokenClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForTokenClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),bA=new z({}),vA=new E({props:{name:"class transformers.FlaxAutoModelForMultipleChoice",anchor:"transformers.FlaxAutoModelForMultipleChoice",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15770/src/transformers/models/auto/modeling_flax_auto.py#L282"}}),FA=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15770/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15770/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15770/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/albert#transformers.FlaxAlbertForMultipleChoice">FlaxAlbertForMultipleChoice</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/bert#transformers.FlaxBertForMultipleChoice">FlaxBertForMultipleChoice</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/big_bird#transformers.FlaxBigBirdForMultipleChoice">FlaxBigBirdForMultipleChoice</a> (BigBird model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/distilbert#transformers.FlaxDistilBertForMultipleChoice">FlaxDistilBertForMultipleChoice</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/electra#transformers.FlaxElectraForMultipleChoice">FlaxElectraForMultipleChoice</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/roformer#transformers.FlaxRoFormerForMultipleChoice">FlaxRoFormerForMultipleChoice</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/roberta#transformers.FlaxRobertaForMultipleChoice">FlaxRobertaForMultipleChoice</a> (RoBERTa model)</li>
</ul>`,name:"config"}]}}),CA=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForMultipleChoice

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = FlaxAutoModelForMultipleChoice.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForMultipleChoice

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForMultipleChoice.from_config(config)`}}),MA=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15770/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15770/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15770/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15770/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15770/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),EA=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForMultipleChoice

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForMultipleChoice.from_pretrained("bert-base-cased")

# Update configuration during loading
model = FlaxAutoModelForMultipleChoice.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = FlaxAutoModelForMultipleChoice.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForMultipleChoice

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForMultipleChoice.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForMultipleChoice.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForMultipleChoice.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),yA=new z({}),wA=new E({props:{name:"class transformers.FlaxAutoModelForNextSentencePrediction",anchor:"transformers.FlaxAutoModelForNextSentencePrediction",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15770/src/transformers/models/auto/modeling_flax_auto.py#L289"}}),LA=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15770/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15770/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15770/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/bert#transformers.FlaxBertForNextSentencePrediction">FlaxBertForNextSentencePrediction</a> (BERT model)</li>
</ul>`,name:"config"}]}}),BA=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForNextSentencePrediction

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = FlaxAutoModelForNextSentencePrediction.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForNextSentencePrediction

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForNextSentencePrediction.from_config(config)`}}),kA=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15770/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15770/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15770/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15770/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15770/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),xA=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForNextSentencePrediction

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForNextSentencePrediction.from_pretrained("bert-base-cased")

# Update configuration during loading
model = FlaxAutoModelForNextSentencePrediction.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = FlaxAutoModelForNextSentencePrediction.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForNextSentencePrediction

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForNextSentencePrediction.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForNextSentencePrediction.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForNextSentencePrediction.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),RA=new z({}),SA=new E({props:{name:"class transformers.FlaxAutoModelForImageClassification",anchor:"transformers.FlaxAutoModelForImageClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15770/src/transformers/models/auto/modeling_flax_auto.py#L298"}}),$A=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15770/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15770/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15770/en/model_doc/beit#transformers.BeitConfig">BeitConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/beit#transformers.FlaxBeitForImageClassification">FlaxBeitForImageClassification</a> (BEiT model)</li>
<li><a href="/docs/transformers/pr_15770/en/model_doc/vit#transformers.ViTConfig">ViTConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/vit#transformers.FlaxViTForImageClassification">FlaxViTForImageClassification</a> (ViT model)</li>
</ul>`,name:"config"}]}}),IA=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForImageClassification

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = FlaxAutoModelForImageClassification.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForImageClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForImageClassification.from_config(config)`}}),jA=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15770/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15770/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15770/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15770/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15770/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),DA=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForImageClassification

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForImageClassification.from_pretrained("bert-base-cased")

# Update configuration during loading
model = FlaxAutoModelForImageClassification.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = FlaxAutoModelForImageClassification.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForImageClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForImageClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForImageClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForImageClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),qA=new z({}),GA=new E({props:{name:"class transformers.FlaxAutoModelForVision2Seq",anchor:"transformers.FlaxAutoModelForVision2Seq",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15770/src/transformers/models/auto/modeling_flax_auto.py#L307"}}),XA=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15770/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15770/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15770/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderConfig">VisionEncoderDecoderConfig</a> configuration class: <a href="/docs/transformers/pr_15770/en/model_doc/vision-encoder-decoder#transformers.FlaxVisionEncoderDecoderModel">FlaxVisionEncoderDecoderModel</a> (Vision Encoder decoder model)</li>
</ul>`,name:"config"}]}}),zA=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForVision2Seq

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = FlaxAutoModelForVision2Seq.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForVision2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForVision2Seq.from_config(config)`}}),VA=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15770/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15770/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15770/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15770/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15770/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),WA=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForVision2Seq

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForVision2Seq.from_pretrained("bert-base-cased")

# Update configuration during loading
model = FlaxAutoModelForVision2Seq.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = FlaxAutoModelForVision2Seq.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForVision2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForVision2Seq.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForVision2Seq.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForVision2Seq.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),{c(){J=a("meta"),Le=l(),de=a("h1"),me=a("a"),to=a("span"),f(ce.$$.fragment),be=l(),Do=a("span"),wi=o("Auto Classes"),Ef=l(),sa=a("p"),Ai=o(`In many cases, the architecture you want to use can be guessed from the name or the path of the pretrained model you
are supplying to the `),Li=a("code"),nM=o("from_pretrained()"),yf=o(` method. AutoClasses are here to do this job for you so that you
automatically retrieve the relevant model given the name/path to the pretrained weights/config/vocabulary.`),we=l(),io=a("p"),Bi=o("Instantiating one of "),Pn=a("a"),sM=o("AutoConfig"),$n=o(", "),In=a("a"),lM=o("AutoModel"),ki=o(`, and
`),jn=a("a"),iM=o("AutoTokenizer"),xi=o(" will directly create a class of the relevant architecture. For instance"),wf=l(),f($a.$$.fragment),co=l(),ge=a("p"),XL=o("will create a model that is an instance of "),Ri=a("a"),zL=o("BertModel"),VL=o("."),qo=l(),Ia=a("p"),WL=o("There is one class of "),Af=a("code"),QL=o("AutoModel"),Lxe=o(" for each task, and for each backend (PyTorch, TensorFlow, or Flax)."),v8e=l(),Si=a("h2"),Lf=a("a"),XV=a("span"),f(dM.$$.fragment),Bxe=l(),zV=a("span"),kxe=o("Extending the Auto Classes"),T8e=l(),Nn=a("p"),xxe=o(`Each of the auto classes has a method to be extended with your custom classes. For instance, if you have defined a
custom class of model `),VV=a("code"),Rxe=o("NewModel"),Sxe=o(", make sure you have a "),WV=a("code"),Pxe=o("NewModelConfig"),$xe=o(` then you can add those to the auto
classes like this:`),F8e=l(),f(cM.$$.fragment),C8e=l(),HL=a("p"),Ixe=o("You will then be able to use the auto classes like you would usually do!"),M8e=l(),f(Bf.$$.fragment),E8e=l(),Pi=a("h2"),kf=a("a"),QV=a("span"),f(fM.$$.fragment),jxe=l(),HV=a("span"),Nxe=o("AutoConfig"),y8e=l(),Go=a("div"),f(mM.$$.fragment),Dxe=l(),gM=a("p"),qxe=o(`This is a generic configuration class that will be instantiated as one of the configuration classes of the library
when created with the `),UL=a("a"),Gxe=o("from_pretrained()"),Oxe=o(" class method."),Xxe=l(),hM=a("p"),zxe=o("This class cannot be instantiated directly using "),UV=a("code"),Vxe=o("__init__()"),Wxe=o(" (throws an error)."),Qxe=l(),fo=a("div"),f(pM.$$.fragment),Hxe=l(),JV=a("p"),Uxe=o("Instantiate one of the configuration classes of the library from a pretrained model configuration."),Jxe=l(),$i=a("p"),Yxe=o("The configuration class to instantiate is selected based on the "),YV=a("code"),Kxe=o("model_type"),Zxe=o(` property of the config object that
is loaded, or when it\u2019s missing, by falling back to using pattern matching on `),KV=a("code"),eRe=o("pretrained_model_name_or_path"),oRe=o(":"),rRe=l(),v=a("ul"),xf=a("li"),ZV=a("strong"),tRe=o("albert"),aRe=o(" \u2014 "),JL=a("a"),nRe=o("AlbertConfig"),sRe=o(" (ALBERT model)"),lRe=l(),Rf=a("li"),eW=a("strong"),iRe=o("bart"),dRe=o(" \u2014 "),YL=a("a"),cRe=o("BartConfig"),fRe=o(" (BART model)"),mRe=l(),Sf=a("li"),oW=a("strong"),gRe=o("beit"),hRe=o(" \u2014 "),KL=a("a"),pRe=o("BeitConfig"),_Re=o(" (BEiT model)"),uRe=l(),Pf=a("li"),rW=a("strong"),bRe=o("bert"),vRe=o(" \u2014 "),ZL=a("a"),TRe=o("BertConfig"),FRe=o(" (BERT model)"),CRe=l(),$f=a("li"),tW=a("strong"),MRe=o("bert-generation"),ERe=o(" \u2014 "),e8=a("a"),yRe=o("BertGenerationConfig"),wRe=o(" (Bert Generation model)"),ARe=l(),If=a("li"),aW=a("strong"),LRe=o("big_bird"),BRe=o(" \u2014 "),o8=a("a"),kRe=o("BigBirdConfig"),xRe=o(" (BigBird model)"),RRe=l(),jf=a("li"),nW=a("strong"),SRe=o("bigbird_pegasus"),PRe=o(" \u2014 "),r8=a("a"),$Re=o("BigBirdPegasusConfig"),IRe=o(" (BigBirdPegasus model)"),jRe=l(),Nf=a("li"),sW=a("strong"),NRe=o("blenderbot"),DRe=o(" \u2014 "),t8=a("a"),qRe=o("BlenderbotConfig"),GRe=o(" (Blenderbot model)"),ORe=l(),Df=a("li"),lW=a("strong"),XRe=o("blenderbot-small"),zRe=o(" \u2014 "),a8=a("a"),VRe=o("BlenderbotSmallConfig"),WRe=o(" (BlenderbotSmall model)"),QRe=l(),qf=a("li"),iW=a("strong"),HRe=o("camembert"),URe=o(" \u2014 "),n8=a("a"),JRe=o("CamembertConfig"),YRe=o(" (CamemBERT model)"),KRe=l(),Gf=a("li"),dW=a("strong"),ZRe=o("canine"),eSe=o(" \u2014 "),s8=a("a"),oSe=o("CanineConfig"),rSe=o(" (Canine model)"),tSe=l(),Of=a("li"),cW=a("strong"),aSe=o("clip"),nSe=o(" \u2014 "),l8=a("a"),sSe=o("CLIPConfig"),lSe=o(" (CLIP model)"),iSe=l(),Xf=a("li"),fW=a("strong"),dSe=o("convbert"),cSe=o(" \u2014 "),i8=a("a"),fSe=o("ConvBertConfig"),mSe=o(" (ConvBERT model)"),gSe=l(),zf=a("li"),mW=a("strong"),hSe=o("convnext"),pSe=o(" \u2014 "),d8=a("a"),_Se=o("ConvNextConfig"),uSe=o(" (ConvNext model)"),bSe=l(),Vf=a("li"),gW=a("strong"),vSe=o("ctrl"),TSe=o(" \u2014 "),c8=a("a"),FSe=o("CTRLConfig"),CSe=o(" (CTRL model)"),MSe=l(),Wf=a("li"),hW=a("strong"),ESe=o("deberta"),ySe=o(" \u2014 "),f8=a("a"),wSe=o("DebertaConfig"),ASe=o(" (DeBERTa model)"),LSe=l(),Qf=a("li"),pW=a("strong"),BSe=o("deberta-v2"),kSe=o(" \u2014 "),m8=a("a"),xSe=o("DebertaV2Config"),RSe=o(" (DeBERTa-v2 model)"),SSe=l(),Hf=a("li"),_W=a("strong"),PSe=o("deit"),$Se=o(" \u2014 "),g8=a("a"),ISe=o("DeiTConfig"),jSe=o(" (DeiT model)"),NSe=l(),Uf=a("li"),uW=a("strong"),DSe=o("detr"),qSe=o(" \u2014 "),h8=a("a"),GSe=o("DetrConfig"),OSe=o(" (DETR model)"),XSe=l(),Jf=a("li"),bW=a("strong"),zSe=o("distilbert"),VSe=o(" \u2014 "),p8=a("a"),WSe=o("DistilBertConfig"),QSe=o(" (DistilBERT model)"),HSe=l(),Yf=a("li"),vW=a("strong"),USe=o("dpr"),JSe=o(" \u2014 "),_8=a("a"),YSe=o("DPRConfig"),KSe=o(" (DPR model)"),ZSe=l(),Kf=a("li"),TW=a("strong"),ePe=o("electra"),oPe=o(" \u2014 "),u8=a("a"),rPe=o("ElectraConfig"),tPe=o(" (ELECTRA model)"),aPe=l(),Zf=a("li"),FW=a("strong"),nPe=o("encoder-decoder"),sPe=o(" \u2014 "),b8=a("a"),lPe=o("EncoderDecoderConfig"),iPe=o(" (Encoder decoder model)"),dPe=l(),em=a("li"),CW=a("strong"),cPe=o("flaubert"),fPe=o(" \u2014 "),v8=a("a"),mPe=o("FlaubertConfig"),gPe=o(" (FlauBERT model)"),hPe=l(),om=a("li"),MW=a("strong"),pPe=o("fnet"),_Pe=o(" \u2014 "),T8=a("a"),uPe=o("FNetConfig"),bPe=o(" (FNet model)"),vPe=l(),rm=a("li"),EW=a("strong"),TPe=o("fsmt"),FPe=o(" \u2014 "),F8=a("a"),CPe=o("FSMTConfig"),MPe=o(" (FairSeq Machine-Translation model)"),EPe=l(),tm=a("li"),yW=a("strong"),yPe=o("funnel"),wPe=o(" \u2014 "),C8=a("a"),APe=o("FunnelConfig"),LPe=o(" (Funnel Transformer model)"),BPe=l(),am=a("li"),wW=a("strong"),kPe=o("gpt2"),xPe=o(" \u2014 "),M8=a("a"),RPe=o("GPT2Config"),SPe=o(" (OpenAI GPT-2 model)"),PPe=l(),nm=a("li"),AW=a("strong"),$Pe=o("gpt_neo"),IPe=o(" \u2014 "),E8=a("a"),jPe=o("GPTNeoConfig"),NPe=o(" (GPT Neo model)"),DPe=l(),sm=a("li"),LW=a("strong"),qPe=o("gptj"),GPe=o(" \u2014 "),y8=a("a"),OPe=o("GPTJConfig"),XPe=o(" (GPT-J model)"),zPe=l(),lm=a("li"),BW=a("strong"),VPe=o("hubert"),WPe=o(" \u2014 "),w8=a("a"),QPe=o("HubertConfig"),HPe=o(" (Hubert model)"),UPe=l(),im=a("li"),kW=a("strong"),JPe=o("ibert"),YPe=o(" \u2014 "),A8=a("a"),KPe=o("IBertConfig"),ZPe=o(" (I-BERT model)"),e$e=l(),dm=a("li"),xW=a("strong"),o$e=o("imagegpt"),r$e=o(" \u2014 "),L8=a("a"),t$e=o("ImageGPTConfig"),a$e=o(" (ImageGPT model)"),n$e=l(),cm=a("li"),RW=a("strong"),s$e=o("layoutlm"),l$e=o(" \u2014 "),B8=a("a"),i$e=o("LayoutLMConfig"),d$e=o(" (LayoutLM model)"),c$e=l(),fm=a("li"),SW=a("strong"),f$e=o("layoutlmv2"),m$e=o(" \u2014 "),k8=a("a"),g$e=o("LayoutLMv2Config"),h$e=o(" (LayoutLMv2 model)"),p$e=l(),mm=a("li"),PW=a("strong"),_$e=o("led"),u$e=o(" \u2014 "),x8=a("a"),b$e=o("LEDConfig"),v$e=o(" (LED model)"),T$e=l(),gm=a("li"),$W=a("strong"),F$e=o("longformer"),C$e=o(" \u2014 "),R8=a("a"),M$e=o("LongformerConfig"),E$e=o(" (Longformer model)"),y$e=l(),hm=a("li"),IW=a("strong"),w$e=o("luke"),A$e=o(" \u2014 "),S8=a("a"),L$e=o("LukeConfig"),B$e=o(" (LUKE model)"),k$e=l(),pm=a("li"),jW=a("strong"),x$e=o("lxmert"),R$e=o(" \u2014 "),P8=a("a"),S$e=o("LxmertConfig"),P$e=o(" (LXMERT model)"),$$e=l(),_m=a("li"),NW=a("strong"),I$e=o("m2m_100"),j$e=o(" \u2014 "),$8=a("a"),N$e=o("M2M100Config"),D$e=o(" (M2M100 model)"),q$e=l(),um=a("li"),DW=a("strong"),G$e=o("marian"),O$e=o(" \u2014 "),I8=a("a"),X$e=o("MarianConfig"),z$e=o(" (Marian model)"),V$e=l(),bm=a("li"),qW=a("strong"),W$e=o("mbart"),Q$e=o(" \u2014 "),j8=a("a"),H$e=o("MBartConfig"),U$e=o(" (mBART model)"),J$e=l(),vm=a("li"),GW=a("strong"),Y$e=o("megatron-bert"),K$e=o(" \u2014 "),N8=a("a"),Z$e=o("MegatronBertConfig"),eIe=o(" (MegatronBert model)"),oIe=l(),Tm=a("li"),OW=a("strong"),rIe=o("mobilebert"),tIe=o(" \u2014 "),D8=a("a"),aIe=o("MobileBertConfig"),nIe=o(" (MobileBERT model)"),sIe=l(),Fm=a("li"),XW=a("strong"),lIe=o("mpnet"),iIe=o(" \u2014 "),q8=a("a"),dIe=o("MPNetConfig"),cIe=o(" (MPNet model)"),fIe=l(),Cm=a("li"),zW=a("strong"),mIe=o("mt5"),gIe=o(" \u2014 "),G8=a("a"),hIe=o("MT5Config"),pIe=o(" (mT5 model)"),_Ie=l(),Mm=a("li"),VW=a("strong"),uIe=o("nystromformer"),bIe=o(" \u2014 "),O8=a("a"),vIe=o("NystromformerConfig"),TIe=o(" (Nystromformer model)"),FIe=l(),Em=a("li"),WW=a("strong"),CIe=o("openai-gpt"),MIe=o(" \u2014 "),X8=a("a"),EIe=o("OpenAIGPTConfig"),yIe=o(" (OpenAI GPT model)"),wIe=l(),ym=a("li"),QW=a("strong"),AIe=o("pegasus"),LIe=o(" \u2014 "),z8=a("a"),BIe=o("PegasusConfig"),kIe=o(" (Pegasus model)"),xIe=l(),wm=a("li"),HW=a("strong"),RIe=o("perceiver"),SIe=o(" \u2014 "),V8=a("a"),PIe=o("PerceiverConfig"),$Ie=o(" (Perceiver model)"),IIe=l(),Am=a("li"),UW=a("strong"),jIe=o("plbart"),NIe=o(" \u2014 "),W8=a("a"),DIe=o("PLBartConfig"),qIe=o(" (PLBart model)"),GIe=l(),Lm=a("li"),JW=a("strong"),OIe=o("poolformer"),XIe=o(" \u2014 "),Q8=a("a"),zIe=o("PoolFormerConfig"),VIe=o(" (PoolFormer model)"),WIe=l(),Bm=a("li"),YW=a("strong"),QIe=o("prophetnet"),HIe=o(" \u2014 "),H8=a("a"),UIe=o("ProphetNetConfig"),JIe=o(" (ProphetNet model)"),YIe=l(),km=a("li"),KW=a("strong"),KIe=o("qdqbert"),ZIe=o(" \u2014 "),U8=a("a"),eje=o("QDQBertConfig"),oje=o(" (QDQBert model)"),rje=l(),xm=a("li"),ZW=a("strong"),tje=o("rag"),aje=o(" \u2014 "),J8=a("a"),nje=o("RagConfig"),sje=o(" (RAG model)"),lje=l(),Rm=a("li"),eQ=a("strong"),ije=o("realm"),dje=o(" \u2014 "),Y8=a("a"),cje=o("RealmConfig"),fje=o(" (Realm model)"),mje=l(),Sm=a("li"),oQ=a("strong"),gje=o("reformer"),hje=o(" \u2014 "),K8=a("a"),pje=o("ReformerConfig"),_je=o(" (Reformer model)"),uje=l(),Pm=a("li"),rQ=a("strong"),bje=o("rembert"),vje=o(" \u2014 "),Z8=a("a"),Tje=o("RemBertConfig"),Fje=o(" (RemBERT model)"),Cje=l(),$m=a("li"),tQ=a("strong"),Mje=o("resnet"),Eje=o(" \u2014 "),e9=a("a"),yje=o("ResNetConfig"),wje=o(" (resnet model)"),Aje=l(),Im=a("li"),aQ=a("strong"),Lje=o("retribert"),Bje=o(" \u2014 "),o9=a("a"),kje=o("RetriBertConfig"),xje=o(" (RetriBERT model)"),Rje=l(),jm=a("li"),nQ=a("strong"),Sje=o("roberta"),Pje=o(" \u2014 "),r9=a("a"),$je=o("RobertaConfig"),Ije=o(" (RoBERTa model)"),jje=l(),Nm=a("li"),sQ=a("strong"),Nje=o("roformer"),Dje=o(" \u2014 "),t9=a("a"),qje=o("RoFormerConfig"),Gje=o(" (RoFormer model)"),Oje=l(),Dm=a("li"),lQ=a("strong"),Xje=o("segformer"),zje=o(" \u2014 "),a9=a("a"),Vje=o("SegformerConfig"),Wje=o(" (SegFormer model)"),Qje=l(),qm=a("li"),iQ=a("strong"),Hje=o("sew"),Uje=o(" \u2014 "),n9=a("a"),Jje=o("SEWConfig"),Yje=o(" (SEW model)"),Kje=l(),Gm=a("li"),dQ=a("strong"),Zje=o("sew-d"),eNe=o(" \u2014 "),s9=a("a"),oNe=o("SEWDConfig"),rNe=o(" (SEW-D model)"),tNe=l(),Om=a("li"),cQ=a("strong"),aNe=o("speech-encoder-decoder"),nNe=o(" \u2014 "),l9=a("a"),sNe=o("SpeechEncoderDecoderConfig"),lNe=o(" (Speech Encoder decoder model)"),iNe=l(),Xm=a("li"),fQ=a("strong"),dNe=o("speech_to_text"),cNe=o(" \u2014 "),i9=a("a"),fNe=o("Speech2TextConfig"),mNe=o(" (Speech2Text model)"),gNe=l(),zm=a("li"),mQ=a("strong"),hNe=o("speech_to_text_2"),pNe=o(" \u2014 "),d9=a("a"),_Ne=o("Speech2Text2Config"),uNe=o(" (Speech2Text2 model)"),bNe=l(),Vm=a("li"),gQ=a("strong"),vNe=o("splinter"),TNe=o(" \u2014 "),c9=a("a"),FNe=o("SplinterConfig"),CNe=o(" (Splinter model)"),MNe=l(),Wm=a("li"),hQ=a("strong"),ENe=o("squeezebert"),yNe=o(" \u2014 "),f9=a("a"),wNe=o("SqueezeBertConfig"),ANe=o(" (SqueezeBERT model)"),LNe=l(),Qm=a("li"),pQ=a("strong"),BNe=o("swin"),kNe=o(" \u2014 "),m9=a("a"),xNe=o("SwinConfig"),RNe=o(" (Swin model)"),SNe=l(),Hm=a("li"),_Q=a("strong"),PNe=o("t5"),$Ne=o(" \u2014 "),g9=a("a"),INe=o("T5Config"),jNe=o(" (T5 model)"),NNe=l(),Um=a("li"),uQ=a("strong"),DNe=o("tapas"),qNe=o(" \u2014 "),h9=a("a"),GNe=o("TapasConfig"),ONe=o(" (TAPAS model)"),XNe=l(),Jm=a("li"),bQ=a("strong"),zNe=o("transfo-xl"),VNe=o(" \u2014 "),p9=a("a"),WNe=o("TransfoXLConfig"),QNe=o(" (Transformer-XL model)"),HNe=l(),Ym=a("li"),vQ=a("strong"),UNe=o("trocr"),JNe=o(" \u2014 "),_9=a("a"),YNe=o("TrOCRConfig"),KNe=o(" (TrOCR model)"),ZNe=l(),Km=a("li"),TQ=a("strong"),eDe=o("unispeech"),oDe=o(" \u2014 "),u9=a("a"),rDe=o("UniSpeechConfig"),tDe=o(" (UniSpeech model)"),aDe=l(),Zm=a("li"),FQ=a("strong"),nDe=o("unispeech-sat"),sDe=o(" \u2014 "),b9=a("a"),lDe=o("UniSpeechSatConfig"),iDe=o(" (UniSpeechSat model)"),dDe=l(),eg=a("li"),CQ=a("strong"),cDe=o("vilt"),fDe=o(" \u2014 "),v9=a("a"),mDe=o("ViltConfig"),gDe=o(" (ViLT model)"),hDe=l(),og=a("li"),MQ=a("strong"),pDe=o("vision-encoder-decoder"),_De=o(" \u2014 "),T9=a("a"),uDe=o("VisionEncoderDecoderConfig"),bDe=o(" (Vision Encoder decoder model)"),vDe=l(),rg=a("li"),EQ=a("strong"),TDe=o("vision-text-dual-encoder"),FDe=o(" \u2014 "),F9=a("a"),CDe=o("VisionTextDualEncoderConfig"),MDe=o(" (VisionTextDualEncoder model)"),EDe=l(),tg=a("li"),yQ=a("strong"),yDe=o("visual_bert"),wDe=o(" \u2014 "),C9=a("a"),ADe=o("VisualBertConfig"),LDe=o(" (VisualBert model)"),BDe=l(),ag=a("li"),wQ=a("strong"),kDe=o("vit"),xDe=o(" \u2014 "),M9=a("a"),RDe=o("ViTConfig"),SDe=o(" (ViT model)"),PDe=l(),ng=a("li"),AQ=a("strong"),$De=o("vit_mae"),IDe=o(" \u2014 "),E9=a("a"),jDe=o("ViTMAEConfig"),NDe=o(" (ViTMAE model)"),DDe=l(),sg=a("li"),LQ=a("strong"),qDe=o("wav2vec2"),GDe=o(" \u2014 "),y9=a("a"),ODe=o("Wav2Vec2Config"),XDe=o(" (Wav2Vec2 model)"),zDe=l(),lg=a("li"),BQ=a("strong"),VDe=o("wavlm"),WDe=o(" \u2014 "),w9=a("a"),QDe=o("WavLMConfig"),HDe=o(" (WavLM model)"),UDe=l(),ig=a("li"),kQ=a("strong"),JDe=o("xglm"),YDe=o(" \u2014 "),A9=a("a"),KDe=o("XGLMConfig"),ZDe=o(" (XGLM model)"),eqe=l(),dg=a("li"),xQ=a("strong"),oqe=o("xlm"),rqe=o(" \u2014 "),L9=a("a"),tqe=o("XLMConfig"),aqe=o(" (XLM model)"),nqe=l(),cg=a("li"),RQ=a("strong"),sqe=o("xlm-prophetnet"),lqe=o(" \u2014 "),B9=a("a"),iqe=o("XLMProphetNetConfig"),dqe=o(" (XLMProphetNet model)"),cqe=l(),fg=a("li"),SQ=a("strong"),fqe=o("xlm-roberta"),mqe=o(" \u2014 "),k9=a("a"),gqe=o("XLMRobertaConfig"),hqe=o(" (XLM-RoBERTa model)"),pqe=l(),mg=a("li"),PQ=a("strong"),_qe=o("xlm-roberta-xl"),uqe=o(" \u2014 "),x9=a("a"),bqe=o("XLMRobertaXLConfig"),vqe=o(" (XLM-RoBERTa-XL model)"),Tqe=l(),gg=a("li"),$Q=a("strong"),Fqe=o("xlnet"),Cqe=o(" \u2014 "),R9=a("a"),Mqe=o("XLNetConfig"),Eqe=o(" (XLNet model)"),yqe=l(),hg=a("li"),IQ=a("strong"),wqe=o("yoso"),Aqe=o(" \u2014 "),S9=a("a"),Lqe=o("YosoConfig"),Bqe=o(" (YOSO model)"),kqe=l(),jQ=a("p"),xqe=o("Examples:"),Rqe=l(),f(_M.$$.fragment),Sqe=l(),pg=a("div"),f(uM.$$.fragment),Pqe=l(),NQ=a("p"),$qe=o("Register a new configuration for this class."),w8e=l(),Ii=a("h2"),_g=a("a"),DQ=a("span"),f(bM.$$.fragment),Iqe=l(),qQ=a("span"),jqe=o("AutoTokenizer"),A8e=l(),Oo=a("div"),f(vM.$$.fragment),Nqe=l(),TM=a("p"),Dqe=o(`This is a generic tokenizer class that will be instantiated as one of the tokenizer classes of the library when
created with the `),P9=a("a"),qqe=o("AutoTokenizer.from_pretrained()"),Gqe=o(" class method."),Oqe=l(),FM=a("p"),Xqe=o("This class cannot be instantiated directly using "),GQ=a("code"),zqe=o("__init__()"),Vqe=o(" (throws an error)."),Wqe=l(),mo=a("div"),f(CM.$$.fragment),Qqe=l(),OQ=a("p"),Hqe=o("Instantiate one of the tokenizer classes of the library from a pretrained model vocabulary."),Uqe=l(),ja=a("p"),Jqe=o("The tokenizer class to instantiate is selected based on the "),XQ=a("code"),Yqe=o("model_type"),Kqe=o(` property of the config object (either
passed as an argument or loaded from `),zQ=a("code"),Zqe=o("pretrained_model_name_or_path"),eGe=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),VQ=a("code"),oGe=o("pretrained_model_name_or_path"),rGe=o(":"),tGe=l(),M=a("ul"),Dn=a("li"),WQ=a("strong"),aGe=o("albert"),nGe=o(" \u2014 "),$9=a("a"),sGe=o("AlbertTokenizer"),lGe=o(" or "),I9=a("a"),iGe=o("AlbertTokenizerFast"),dGe=o(" (ALBERT model)"),cGe=l(),qn=a("li"),QQ=a("strong"),fGe=o("bart"),mGe=o(" \u2014 "),j9=a("a"),gGe=o("BartTokenizer"),hGe=o(" or "),N9=a("a"),pGe=o("BartTokenizerFast"),_Ge=o(" (BART model)"),uGe=l(),Gn=a("li"),HQ=a("strong"),bGe=o("barthez"),vGe=o(" \u2014 "),D9=a("a"),TGe=o("BarthezTokenizer"),FGe=o(" or "),q9=a("a"),CGe=o("BarthezTokenizerFast"),MGe=o(" (BARThez model)"),EGe=l(),ug=a("li"),UQ=a("strong"),yGe=o("bartpho"),wGe=o(" \u2014 "),G9=a("a"),AGe=o("BartphoTokenizer"),LGe=o(" (BARTpho model)"),BGe=l(),On=a("li"),JQ=a("strong"),kGe=o("bert"),xGe=o(" \u2014 "),O9=a("a"),RGe=o("BertTokenizer"),SGe=o(" or "),X9=a("a"),PGe=o("BertTokenizerFast"),$Ge=o(" (BERT model)"),IGe=l(),bg=a("li"),YQ=a("strong"),jGe=o("bert-generation"),NGe=o(" \u2014 "),z9=a("a"),DGe=o("BertGenerationTokenizer"),qGe=o(" (Bert Generation model)"),GGe=l(),vg=a("li"),KQ=a("strong"),OGe=o("bert-japanese"),XGe=o(" \u2014 "),V9=a("a"),zGe=o("BertJapaneseTokenizer"),VGe=o(" (BertJapanese model)"),WGe=l(),Tg=a("li"),ZQ=a("strong"),QGe=o("bertweet"),HGe=o(" \u2014 "),W9=a("a"),UGe=o("BertweetTokenizer"),JGe=o(" (Bertweet model)"),YGe=l(),Xn=a("li"),eH=a("strong"),KGe=o("big_bird"),ZGe=o(" \u2014 "),Q9=a("a"),eOe=o("BigBirdTokenizer"),oOe=o(" or "),H9=a("a"),rOe=o("BigBirdTokenizerFast"),tOe=o(" (BigBird model)"),aOe=l(),zn=a("li"),oH=a("strong"),nOe=o("bigbird_pegasus"),sOe=o(" \u2014 "),U9=a("a"),lOe=o("PegasusTokenizer"),iOe=o(" or "),J9=a("a"),dOe=o("PegasusTokenizerFast"),cOe=o(" (BigBirdPegasus model)"),fOe=l(),Vn=a("li"),rH=a("strong"),mOe=o("blenderbot"),gOe=o(" \u2014 "),Y9=a("a"),hOe=o("BlenderbotTokenizer"),pOe=o(" or "),K9=a("a"),_Oe=o("BlenderbotTokenizerFast"),uOe=o(" (Blenderbot model)"),bOe=l(),Fg=a("li"),tH=a("strong"),vOe=o("blenderbot-small"),TOe=o(" \u2014 "),Z9=a("a"),FOe=o("BlenderbotSmallTokenizer"),COe=o(" (BlenderbotSmall model)"),MOe=l(),Cg=a("li"),aH=a("strong"),EOe=o("byt5"),yOe=o(" \u2014 "),eB=a("a"),wOe=o("ByT5Tokenizer"),AOe=o(" (ByT5 model)"),LOe=l(),Wn=a("li"),nH=a("strong"),BOe=o("camembert"),kOe=o(" \u2014 "),oB=a("a"),xOe=o("CamembertTokenizer"),ROe=o(" or "),rB=a("a"),SOe=o("CamembertTokenizerFast"),POe=o(" (CamemBERT model)"),$Oe=l(),Mg=a("li"),sH=a("strong"),IOe=o("canine"),jOe=o(" \u2014 "),tB=a("a"),NOe=o("CanineTokenizer"),DOe=o(" (Canine model)"),qOe=l(),Qn=a("li"),lH=a("strong"),GOe=o("clip"),OOe=o(" \u2014 "),aB=a("a"),XOe=o("CLIPTokenizer"),zOe=o(" or "),nB=a("a"),VOe=o("CLIPTokenizerFast"),WOe=o(" (CLIP model)"),QOe=l(),Hn=a("li"),iH=a("strong"),HOe=o("convbert"),UOe=o(" \u2014 "),sB=a("a"),JOe=o("ConvBertTokenizer"),YOe=o(" or "),lB=a("a"),KOe=o("ConvBertTokenizerFast"),ZOe=o(" (ConvBERT model)"),eXe=l(),Un=a("li"),dH=a("strong"),oXe=o("cpm"),rXe=o(" \u2014 "),iB=a("a"),tXe=o("CpmTokenizer"),aXe=o(" or "),cH=a("code"),nXe=o("CpmTokenizerFast"),sXe=o(" (CPM model)"),lXe=l(),Eg=a("li"),fH=a("strong"),iXe=o("ctrl"),dXe=o(" \u2014 "),dB=a("a"),cXe=o("CTRLTokenizer"),fXe=o(" (CTRL model)"),mXe=l(),Jn=a("li"),mH=a("strong"),gXe=o("deberta"),hXe=o(" \u2014 "),cB=a("a"),pXe=o("DebertaTokenizer"),_Xe=o(" or "),fB=a("a"),uXe=o("DebertaTokenizerFast"),bXe=o(" (DeBERTa model)"),vXe=l(),yg=a("li"),gH=a("strong"),TXe=o("deberta-v2"),FXe=o(" \u2014 "),mB=a("a"),CXe=o("DebertaV2Tokenizer"),MXe=o(" (DeBERTa-v2 model)"),EXe=l(),Yn=a("li"),hH=a("strong"),yXe=o("distilbert"),wXe=o(" \u2014 "),gB=a("a"),AXe=o("DistilBertTokenizer"),LXe=o(" or "),hB=a("a"),BXe=o("DistilBertTokenizerFast"),kXe=o(" (DistilBERT model)"),xXe=l(),Kn=a("li"),pH=a("strong"),RXe=o("dpr"),SXe=o(" \u2014 "),pB=a("a"),PXe=o("DPRQuestionEncoderTokenizer"),$Xe=o(" or "),_B=a("a"),IXe=o("DPRQuestionEncoderTokenizerFast"),jXe=o(" (DPR model)"),NXe=l(),Zn=a("li"),_H=a("strong"),DXe=o("electra"),qXe=o(" \u2014 "),uB=a("a"),GXe=o("ElectraTokenizer"),OXe=o(" or "),bB=a("a"),XXe=o("ElectraTokenizerFast"),zXe=o(" (ELECTRA model)"),VXe=l(),wg=a("li"),uH=a("strong"),WXe=o("flaubert"),QXe=o(" \u2014 "),vB=a("a"),HXe=o("FlaubertTokenizer"),UXe=o(" (FlauBERT model)"),JXe=l(),es=a("li"),bH=a("strong"),YXe=o("fnet"),KXe=o(" \u2014 "),TB=a("a"),ZXe=o("FNetTokenizer"),eze=o(" or "),FB=a("a"),oze=o("FNetTokenizerFast"),rze=o(" (FNet model)"),tze=l(),Ag=a("li"),vH=a("strong"),aze=o("fsmt"),nze=o(" \u2014 "),CB=a("a"),sze=o("FSMTTokenizer"),lze=o(" (FairSeq Machine-Translation model)"),ize=l(),os=a("li"),TH=a("strong"),dze=o("funnel"),cze=o(" \u2014 "),MB=a("a"),fze=o("FunnelTokenizer"),mze=o(" or "),EB=a("a"),gze=o("FunnelTokenizerFast"),hze=o(" (Funnel Transformer model)"),pze=l(),rs=a("li"),FH=a("strong"),_ze=o("gpt2"),uze=o(" \u2014 "),yB=a("a"),bze=o("GPT2Tokenizer"),vze=o(" or "),wB=a("a"),Tze=o("GPT2TokenizerFast"),Fze=o(" (OpenAI GPT-2 model)"),Cze=l(),ts=a("li"),CH=a("strong"),Mze=o("gpt_neo"),Eze=o(" \u2014 "),AB=a("a"),yze=o("GPT2Tokenizer"),wze=o(" or "),LB=a("a"),Aze=o("GPT2TokenizerFast"),Lze=o(" (GPT Neo model)"),Bze=l(),as=a("li"),MH=a("strong"),kze=o("herbert"),xze=o(" \u2014 "),BB=a("a"),Rze=o("HerbertTokenizer"),Sze=o(" or "),kB=a("a"),Pze=o("HerbertTokenizerFast"),$ze=o(" (HerBERT model)"),Ize=l(),Lg=a("li"),EH=a("strong"),jze=o("hubert"),Nze=o(" \u2014 "),xB=a("a"),Dze=o("Wav2Vec2CTCTokenizer"),qze=o(" (Hubert model)"),Gze=l(),ns=a("li"),yH=a("strong"),Oze=o("ibert"),Xze=o(" \u2014 "),RB=a("a"),zze=o("RobertaTokenizer"),Vze=o(" or "),SB=a("a"),Wze=o("RobertaTokenizerFast"),Qze=o(" (I-BERT model)"),Hze=l(),ss=a("li"),wH=a("strong"),Uze=o("layoutlm"),Jze=o(" \u2014 "),PB=a("a"),Yze=o("LayoutLMTokenizer"),Kze=o(" or "),$B=a("a"),Zze=o("LayoutLMTokenizerFast"),eVe=o(" (LayoutLM model)"),oVe=l(),ls=a("li"),AH=a("strong"),rVe=o("layoutlmv2"),tVe=o(" \u2014 "),IB=a("a"),aVe=o("LayoutLMv2Tokenizer"),nVe=o(" or "),jB=a("a"),sVe=o("LayoutLMv2TokenizerFast"),lVe=o(" (LayoutLMv2 model)"),iVe=l(),is=a("li"),LH=a("strong"),dVe=o("layoutxlm"),cVe=o(" \u2014 "),NB=a("a"),fVe=o("LayoutXLMTokenizer"),mVe=o(" or "),DB=a("a"),gVe=o("LayoutXLMTokenizerFast"),hVe=o(" (LayoutXLM model)"),pVe=l(),ds=a("li"),BH=a("strong"),_Ve=o("led"),uVe=o(" \u2014 "),qB=a("a"),bVe=o("LEDTokenizer"),vVe=o(" or "),GB=a("a"),TVe=o("LEDTokenizerFast"),FVe=o(" (LED model)"),CVe=l(),cs=a("li"),kH=a("strong"),MVe=o("longformer"),EVe=o(" \u2014 "),OB=a("a"),yVe=o("LongformerTokenizer"),wVe=o(" or "),XB=a("a"),AVe=o("LongformerTokenizerFast"),LVe=o(" (Longformer model)"),BVe=l(),Bg=a("li"),xH=a("strong"),kVe=o("luke"),xVe=o(" \u2014 "),zB=a("a"),RVe=o("LukeTokenizer"),SVe=o(" (LUKE model)"),PVe=l(),fs=a("li"),RH=a("strong"),$Ve=o("lxmert"),IVe=o(" \u2014 "),VB=a("a"),jVe=o("LxmertTokenizer"),NVe=o(" or "),WB=a("a"),DVe=o("LxmertTokenizerFast"),qVe=o(" (LXMERT model)"),GVe=l(),kg=a("li"),SH=a("strong"),OVe=o("m2m_100"),XVe=o(" \u2014 "),QB=a("a"),zVe=o("M2M100Tokenizer"),VVe=o(" (M2M100 model)"),WVe=l(),xg=a("li"),PH=a("strong"),QVe=o("marian"),HVe=o(" \u2014 "),HB=a("a"),UVe=o("MarianTokenizer"),JVe=o(" (Marian model)"),YVe=l(),ms=a("li"),$H=a("strong"),KVe=o("mbart"),ZVe=o(" \u2014 "),UB=a("a"),eWe=o("MBartTokenizer"),oWe=o(" or "),JB=a("a"),rWe=o("MBartTokenizerFast"),tWe=o(" (mBART model)"),aWe=l(),gs=a("li"),IH=a("strong"),nWe=o("mbart50"),sWe=o(" \u2014 "),YB=a("a"),lWe=o("MBart50Tokenizer"),iWe=o(" or "),KB=a("a"),dWe=o("MBart50TokenizerFast"),cWe=o(" (mBART-50 model)"),fWe=l(),Rg=a("li"),jH=a("strong"),mWe=o("mluke"),gWe=o(" \u2014 "),ZB=a("a"),hWe=o("MLukeTokenizer"),pWe=o(" (mLUKE model)"),_We=l(),hs=a("li"),NH=a("strong"),uWe=o("mobilebert"),bWe=o(" \u2014 "),ek=a("a"),vWe=o("MobileBertTokenizer"),TWe=o(" or "),ok=a("a"),FWe=o("MobileBertTokenizerFast"),CWe=o(" (MobileBERT model)"),MWe=l(),ps=a("li"),DH=a("strong"),EWe=o("mpnet"),yWe=o(" \u2014 "),rk=a("a"),wWe=o("MPNetTokenizer"),AWe=o(" or "),tk=a("a"),LWe=o("MPNetTokenizerFast"),BWe=o(" (MPNet model)"),kWe=l(),_s=a("li"),qH=a("strong"),xWe=o("mt5"),RWe=o(" \u2014 "),ak=a("a"),SWe=o("MT5Tokenizer"),PWe=o(" or "),nk=a("a"),$We=o("MT5TokenizerFast"),IWe=o(" (mT5 model)"),jWe=l(),us=a("li"),GH=a("strong"),NWe=o("openai-gpt"),DWe=o(" \u2014 "),sk=a("a"),qWe=o("OpenAIGPTTokenizer"),GWe=o(" or "),lk=a("a"),OWe=o("OpenAIGPTTokenizerFast"),XWe=o(" (OpenAI GPT model)"),zWe=l(),bs=a("li"),OH=a("strong"),VWe=o("pegasus"),WWe=o(" \u2014 "),ik=a("a"),QWe=o("PegasusTokenizer"),HWe=o(" or "),dk=a("a"),UWe=o("PegasusTokenizerFast"),JWe=o(" (Pegasus model)"),YWe=l(),Sg=a("li"),XH=a("strong"),KWe=o("perceiver"),ZWe=o(" \u2014 "),ck=a("a"),eQe=o("PerceiverTokenizer"),oQe=o(" (Perceiver model)"),rQe=l(),Pg=a("li"),zH=a("strong"),tQe=o("phobert"),aQe=o(" \u2014 "),fk=a("a"),nQe=o("PhobertTokenizer"),sQe=o(" (PhoBERT model)"),lQe=l(),$g=a("li"),VH=a("strong"),iQe=o("plbart"),dQe=o(" \u2014 "),mk=a("a"),cQe=o("PLBartTokenizer"),fQe=o(" (PLBart model)"),mQe=l(),Ig=a("li"),WH=a("strong"),gQe=o("prophetnet"),hQe=o(" \u2014 "),gk=a("a"),pQe=o("ProphetNetTokenizer"),_Qe=o(" (ProphetNet model)"),uQe=l(),vs=a("li"),QH=a("strong"),bQe=o("qdqbert"),vQe=o(" \u2014 "),hk=a("a"),TQe=o("BertTokenizer"),FQe=o(" or "),pk=a("a"),CQe=o("BertTokenizerFast"),MQe=o(" (QDQBert model)"),EQe=l(),jg=a("li"),HH=a("strong"),yQe=o("rag"),wQe=o(" \u2014 "),_k=a("a"),AQe=o("RagTokenizer"),LQe=o(" (RAG model)"),BQe=l(),Ts=a("li"),UH=a("strong"),kQe=o("reformer"),xQe=o(" \u2014 "),uk=a("a"),RQe=o("ReformerTokenizer"),SQe=o(" or "),bk=a("a"),PQe=o("ReformerTokenizerFast"),$Qe=o(" (Reformer model)"),IQe=l(),Fs=a("li"),JH=a("strong"),jQe=o("rembert"),NQe=o(" \u2014 "),vk=a("a"),DQe=o("RemBertTokenizer"),qQe=o(" or "),Tk=a("a"),GQe=o("RemBertTokenizerFast"),OQe=o(" (RemBERT model)"),XQe=l(),Cs=a("li"),YH=a("strong"),zQe=o("retribert"),VQe=o(" \u2014 "),Fk=a("a"),WQe=o("RetriBertTokenizer"),QQe=o(" or "),Ck=a("a"),HQe=o("RetriBertTokenizerFast"),UQe=o(" (RetriBERT model)"),JQe=l(),Ms=a("li"),KH=a("strong"),YQe=o("roberta"),KQe=o(" \u2014 "),Mk=a("a"),ZQe=o("RobertaTokenizer"),eHe=o(" or "),Ek=a("a"),oHe=o("RobertaTokenizerFast"),rHe=o(" (RoBERTa model)"),tHe=l(),Es=a("li"),ZH=a("strong"),aHe=o("roformer"),nHe=o(" \u2014 "),yk=a("a"),sHe=o("RoFormerTokenizer"),lHe=o(" or "),wk=a("a"),iHe=o("RoFormerTokenizerFast"),dHe=o(" (RoFormer model)"),cHe=l(),Ng=a("li"),eU=a("strong"),fHe=o("speech_to_text"),mHe=o(" \u2014 "),Ak=a("a"),gHe=o("Speech2TextTokenizer"),hHe=o(" (Speech2Text model)"),pHe=l(),Dg=a("li"),oU=a("strong"),_He=o("speech_to_text_2"),uHe=o(" \u2014 "),Lk=a("a"),bHe=o("Speech2Text2Tokenizer"),vHe=o(" (Speech2Text2 model)"),THe=l(),ys=a("li"),rU=a("strong"),FHe=o("splinter"),CHe=o(" \u2014 "),Bk=a("a"),MHe=o("SplinterTokenizer"),EHe=o(" or "),kk=a("a"),yHe=o("SplinterTokenizerFast"),wHe=o(" (Splinter model)"),AHe=l(),ws=a("li"),tU=a("strong"),LHe=o("squeezebert"),BHe=o(" \u2014 "),xk=a("a"),kHe=o("SqueezeBertTokenizer"),xHe=o(" or "),Rk=a("a"),RHe=o("SqueezeBertTokenizerFast"),SHe=o(" (SqueezeBERT model)"),PHe=l(),As=a("li"),aU=a("strong"),$He=o("t5"),IHe=o(" \u2014 "),Sk=a("a"),jHe=o("T5Tokenizer"),NHe=o(" or "),Pk=a("a"),DHe=o("T5TokenizerFast"),qHe=o(" (T5 model)"),GHe=l(),qg=a("li"),nU=a("strong"),OHe=o("tapas"),XHe=o(" \u2014 "),$k=a("a"),zHe=o("TapasTokenizer"),VHe=o(" (TAPAS model)"),WHe=l(),Gg=a("li"),sU=a("strong"),QHe=o("transfo-xl"),HHe=o(" \u2014 "),Ik=a("a"),UHe=o("TransfoXLTokenizer"),JHe=o(" (Transformer-XL model)"),YHe=l(),Og=a("li"),lU=a("strong"),KHe=o("wav2vec2"),ZHe=o(" \u2014 "),jk=a("a"),eUe=o("Wav2Vec2CTCTokenizer"),oUe=o(" (Wav2Vec2 model)"),rUe=l(),Xg=a("li"),iU=a("strong"),tUe=o("wav2vec2_phoneme"),aUe=o(" \u2014 "),Nk=a("a"),nUe=o("Wav2Vec2PhonemeCTCTokenizer"),sUe=o(" (Wav2Vec2Phoneme model)"),lUe=l(),Ls=a("li"),dU=a("strong"),iUe=o("xglm"),dUe=o(" \u2014 "),Dk=a("a"),cUe=o("XGLMTokenizer"),fUe=o(" or "),qk=a("a"),mUe=o("XGLMTokenizerFast"),gUe=o(" (XGLM model)"),hUe=l(),zg=a("li"),cU=a("strong"),pUe=o("xlm"),_Ue=o(" \u2014 "),Gk=a("a"),uUe=o("XLMTokenizer"),bUe=o(" (XLM model)"),vUe=l(),Vg=a("li"),fU=a("strong"),TUe=o("xlm-prophetnet"),FUe=o(" \u2014 "),Ok=a("a"),CUe=o("XLMProphetNetTokenizer"),MUe=o(" (XLMProphetNet model)"),EUe=l(),Bs=a("li"),mU=a("strong"),yUe=o("xlm-roberta"),wUe=o(" \u2014 "),Xk=a("a"),AUe=o("XLMRobertaTokenizer"),LUe=o(" or "),zk=a("a"),BUe=o("XLMRobertaTokenizerFast"),kUe=o(" (XLM-RoBERTa model)"),xUe=l(),ks=a("li"),gU=a("strong"),RUe=o("xlnet"),SUe=o(" \u2014 "),Vk=a("a"),PUe=o("XLNetTokenizer"),$Ue=o(" or "),Wk=a("a"),IUe=o("XLNetTokenizerFast"),jUe=o(" (XLNet model)"),NUe=l(),hU=a("p"),DUe=o("Examples:"),qUe=l(),f(MM.$$.fragment),GUe=l(),Wg=a("div"),f(EM.$$.fragment),OUe=l(),pU=a("p"),XUe=o("Register a new tokenizer in this mapping."),L8e=l(),ji=a("h2"),Qg=a("a"),_U=a("span"),f(yM.$$.fragment),zUe=l(),uU=a("span"),VUe=o("AutoFeatureExtractor"),B8e=l(),Xo=a("div"),f(wM.$$.fragment),WUe=l(),AM=a("p"),QUe=o(`This is a generic feature extractor class that will be instantiated as one of the feature extractor classes of the
library when created with the `),Qk=a("a"),HUe=o("AutoFeatureExtractor.from_pretrained()"),UUe=o(" class method."),JUe=l(),LM=a("p"),YUe=o("This class cannot be instantiated directly using "),bU=a("code"),KUe=o("__init__()"),ZUe=o(" (throws an error)."),eJe=l(),Be=a("div"),f(BM.$$.fragment),oJe=l(),vU=a("p"),rJe=o("Instantiate one of the feature extractor classes of the library from a pretrained model vocabulary."),tJe=l(),Na=a("p"),aJe=o("The feature extractor class to instantiate is selected based on the "),TU=a("code"),nJe=o("model_type"),sJe=o(` property of the config object
(either passed as an argument or loaded from `),FU=a("code"),lJe=o("pretrained_model_name_or_path"),iJe=o(` if possible), or when it\u2019s
missing, by falling back to using pattern matching on `),CU=a("code"),dJe=o("pretrained_model_name_or_path"),cJe=o(":"),fJe=l(),ae=a("ul"),Hg=a("li"),MU=a("strong"),mJe=o("beit"),gJe=o(" \u2014 "),Hk=a("a"),hJe=o("BeitFeatureExtractor"),pJe=o(" (BEiT model)"),_Je=l(),Ug=a("li"),EU=a("strong"),uJe=o("clip"),bJe=o(" \u2014 "),Uk=a("a"),vJe=o("CLIPFeatureExtractor"),TJe=o(" (CLIP model)"),FJe=l(),Jg=a("li"),yU=a("strong"),CJe=o("convnext"),MJe=o(" \u2014 "),Jk=a("a"),EJe=o("ConvNextFeatureExtractor"),yJe=o(" (ConvNext model)"),wJe=l(),Yg=a("li"),wU=a("strong"),AJe=o("deit"),LJe=o(" \u2014 "),Yk=a("a"),BJe=o("DeiTFeatureExtractor"),kJe=o(" (DeiT model)"),xJe=l(),Kg=a("li"),AU=a("strong"),RJe=o("detr"),SJe=o(" \u2014 "),Kk=a("a"),PJe=o("DetrFeatureExtractor"),$Je=o(" (DETR model)"),IJe=l(),Zg=a("li"),LU=a("strong"),jJe=o("hubert"),NJe=o(" \u2014 "),Zk=a("a"),DJe=o("Wav2Vec2FeatureExtractor"),qJe=o(" (Hubert model)"),GJe=l(),eh=a("li"),BU=a("strong"),OJe=o("layoutlmv2"),XJe=o(" \u2014 "),ex=a("a"),zJe=o("LayoutLMv2FeatureExtractor"),VJe=o(" (LayoutLMv2 model)"),WJe=l(),oh=a("li"),kU=a("strong"),QJe=o("perceiver"),HJe=o(" \u2014 "),ox=a("a"),UJe=o("PerceiverFeatureExtractor"),JJe=o(" (Perceiver model)"),YJe=l(),rh=a("li"),xU=a("strong"),KJe=o("poolformer"),ZJe=o(" \u2014 "),rx=a("a"),eYe=o("PoolFormerFeatureExtractor"),oYe=o(" (PoolFormer model)"),rYe=l(),th=a("li"),RU=a("strong"),tYe=o("resnet"),aYe=o(" \u2014 "),tx=a("a"),nYe=o("ConvNextFeatureExtractor"),sYe=o(" (resnet model)"),lYe=l(),ah=a("li"),SU=a("strong"),iYe=o("segformer"),dYe=o(" \u2014 "),ax=a("a"),cYe=o("SegformerFeatureExtractor"),fYe=o(" (SegFormer model)"),mYe=l(),nh=a("li"),PU=a("strong"),gYe=o("speech_to_text"),hYe=o(" \u2014 "),nx=a("a"),pYe=o("Speech2TextFeatureExtractor"),_Ye=o(" (Speech2Text model)"),uYe=l(),sh=a("li"),$U=a("strong"),bYe=o("swin"),vYe=o(" \u2014 "),sx=a("a"),TYe=o("ViTFeatureExtractor"),FYe=o(" (Swin model)"),CYe=l(),lh=a("li"),IU=a("strong"),MYe=o("vit"),EYe=o(" \u2014 "),lx=a("a"),yYe=o("ViTFeatureExtractor"),wYe=o(" (ViT model)"),AYe=l(),ih=a("li"),jU=a("strong"),LYe=o("vit_mae"),BYe=o(" \u2014 "),ix=a("a"),kYe=o("ViTFeatureExtractor"),xYe=o(" (ViTMAE model)"),RYe=l(),dh=a("li"),NU=a("strong"),SYe=o("wav2vec2"),PYe=o(" \u2014 "),dx=a("a"),$Ye=o("Wav2Vec2FeatureExtractor"),IYe=o(" (Wav2Vec2 model)"),jYe=l(),f(ch.$$.fragment),NYe=l(),DU=a("p"),DYe=o("Examples:"),qYe=l(),f(kM.$$.fragment),GYe=l(),fh=a("div"),f(xM.$$.fragment),OYe=l(),qU=a("p"),XYe=o("Register a new feature extractor for this class."),k8e=l(),Ni=a("h2"),mh=a("a"),GU=a("span"),f(RM.$$.fragment),zYe=l(),OU=a("span"),VYe=o("AutoProcessor"),x8e=l(),zo=a("div"),f(SM.$$.fragment),WYe=l(),PM=a("p"),QYe=o(`This is a generic processor class that will be instantiated as one of the processor classes of the library when
created with the `),cx=a("a"),HYe=o("AutoProcessor.from_pretrained()"),UYe=o(" class method."),JYe=l(),$M=a("p"),YYe=o("This class cannot be instantiated directly using "),XU=a("code"),KYe=o("__init__()"),ZYe=o(" (throws an error)."),eKe=l(),ke=a("div"),f(IM.$$.fragment),oKe=l(),zU=a("p"),rKe=o("Instantiate one of the processor classes of the library from a pretrained model vocabulary."),tKe=l(),Di=a("p"),aKe=o("The processor class to instantiate is selected based on the "),VU=a("code"),nKe=o("model_type"),sKe=o(` property of the config object (either
passed as an argument or loaded from `),WU=a("code"),lKe=o("pretrained_model_name_or_path"),iKe=o(" if possible):"),dKe=l(),Ae=a("ul"),gh=a("li"),QU=a("strong"),cKe=o("clip"),fKe=o(" \u2014 "),fx=a("a"),mKe=o("CLIPProcessor"),gKe=o(" (CLIP model)"),hKe=l(),hh=a("li"),HU=a("strong"),pKe=o("layoutlmv2"),_Ke=o(" \u2014 "),mx=a("a"),uKe=o("LayoutLMv2Processor"),bKe=o(" (LayoutLMv2 model)"),vKe=l(),ph=a("li"),UU=a("strong"),TKe=o("layoutxlm"),FKe=o(" \u2014 "),gx=a("a"),CKe=o("LayoutXLMProcessor"),MKe=o(" (LayoutXLM model)"),EKe=l(),_h=a("li"),JU=a("strong"),yKe=o("speech_to_text"),wKe=o(" \u2014 "),hx=a("a"),AKe=o("Speech2TextProcessor"),LKe=o(" (Speech2Text model)"),BKe=l(),uh=a("li"),YU=a("strong"),kKe=o("speech_to_text_2"),xKe=o(" \u2014 "),px=a("a"),RKe=o("Speech2Text2Processor"),SKe=o(" (Speech2Text2 model)"),PKe=l(),bh=a("li"),KU=a("strong"),$Ke=o("trocr"),IKe=o(" \u2014 "),_x=a("a"),jKe=o("TrOCRProcessor"),NKe=o(" (TrOCR model)"),DKe=l(),vh=a("li"),ZU=a("strong"),qKe=o("vision-text-dual-encoder"),GKe=o(" \u2014 "),ux=a("a"),OKe=o("VisionTextDualEncoderProcessor"),XKe=o(" (VisionTextDualEncoder model)"),zKe=l(),Th=a("li"),eJ=a("strong"),VKe=o("wav2vec2"),WKe=o(" \u2014 "),bx=a("a"),QKe=o("Wav2Vec2Processor"),HKe=o(" (Wav2Vec2 model)"),UKe=l(),f(Fh.$$.fragment),JKe=l(),oJ=a("p"),YKe=o("Examples:"),KKe=l(),f(jM.$$.fragment),ZKe=l(),Ch=a("div"),f(NM.$$.fragment),eZe=l(),rJ=a("p"),oZe=o("Register a new processor for this class."),R8e=l(),qi=a("h2"),Mh=a("a"),tJ=a("span"),f(DM.$$.fragment),rZe=l(),aJ=a("span"),tZe=o("AutoModel"),S8e=l(),Vo=a("div"),f(qM.$$.fragment),aZe=l(),Gi=a("p"),nZe=o(`This is a generic model class that will be instantiated as one of the base model classes of the library when created
with the `),nJ=a("code"),sZe=o("from_pretrained()"),lZe=o("class method or the "),sJ=a("code"),iZe=o("from_config()"),dZe=o(`class
method.`),cZe=l(),GM=a("p"),fZe=o("This class cannot be instantiated directly using "),lJ=a("code"),mZe=o("__init__()"),gZe=o(" (throws an error)."),hZe=l(),Nr=a("div"),f(OM.$$.fragment),pZe=l(),iJ=a("p"),_Ze=o("Instantiates one of the base model classes of the library from a configuration."),uZe=l(),Oi=a("p"),bZe=o(`Note:
Loading a model from its configuration file does `),dJ=a("strong"),vZe=o("not"),TZe=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),cJ=a("code"),FZe=o("from_pretrained()"),CZe=o("to load the model weights."),MZe=l(),fJ=a("p"),EZe=o("Examples:"),yZe=l(),f(XM.$$.fragment),wZe=l(),xe=a("div"),f(zM.$$.fragment),AZe=l(),mJ=a("p"),LZe=o("Instantiate one of the base model classes of the library from a pretrained model."),BZe=l(),Da=a("p"),kZe=o("The model class to instantiate is selected based on the "),gJ=a("code"),xZe=o("model_type"),RZe=o(` property of the config object (either
passed as an argument or loaded from `),hJ=a("code"),SZe=o("pretrained_model_name_or_path"),PZe=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),pJ=a("code"),$Ze=o("pretrained_model_name_or_path"),IZe=o(":"),jZe=l(),F=a("ul"),Eh=a("li"),_J=a("strong"),NZe=o("albert"),DZe=o(" \u2014 "),vx=a("a"),qZe=o("AlbertModel"),GZe=o(" (ALBERT model)"),OZe=l(),yh=a("li"),uJ=a("strong"),XZe=o("bart"),zZe=o(" \u2014 "),Tx=a("a"),VZe=o("BartModel"),WZe=o(" (BART model)"),QZe=l(),wh=a("li"),bJ=a("strong"),HZe=o("beit"),UZe=o(" \u2014 "),Fx=a("a"),JZe=o("BeitModel"),YZe=o(" (BEiT model)"),KZe=l(),Ah=a("li"),vJ=a("strong"),ZZe=o("bert"),eeo=o(" \u2014 "),Cx=a("a"),oeo=o("BertModel"),reo=o(" (BERT model)"),teo=l(),Lh=a("li"),TJ=a("strong"),aeo=o("bert-generation"),neo=o(" \u2014 "),Mx=a("a"),seo=o("BertGenerationEncoder"),leo=o(" (Bert Generation model)"),ieo=l(),Bh=a("li"),FJ=a("strong"),deo=o("big_bird"),ceo=o(" \u2014 "),Ex=a("a"),feo=o("BigBirdModel"),meo=o(" (BigBird model)"),geo=l(),kh=a("li"),CJ=a("strong"),heo=o("bigbird_pegasus"),peo=o(" \u2014 "),yx=a("a"),_eo=o("BigBirdPegasusModel"),ueo=o(" (BigBirdPegasus model)"),beo=l(),xh=a("li"),MJ=a("strong"),veo=o("blenderbot"),Teo=o(" \u2014 "),wx=a("a"),Feo=o("BlenderbotModel"),Ceo=o(" (Blenderbot model)"),Meo=l(),Rh=a("li"),EJ=a("strong"),Eeo=o("blenderbot-small"),yeo=o(" \u2014 "),Ax=a("a"),weo=o("BlenderbotSmallModel"),Aeo=o(" (BlenderbotSmall model)"),Leo=l(),Sh=a("li"),yJ=a("strong"),Beo=o("camembert"),keo=o(" \u2014 "),Lx=a("a"),xeo=o("CamembertModel"),Reo=o(" (CamemBERT model)"),Seo=l(),Ph=a("li"),wJ=a("strong"),Peo=o("canine"),$eo=o(" \u2014 "),Bx=a("a"),Ieo=o("CanineModel"),jeo=o(" (Canine model)"),Neo=l(),$h=a("li"),AJ=a("strong"),Deo=o("clip"),qeo=o(" \u2014 "),kx=a("a"),Geo=o("CLIPModel"),Oeo=o(" (CLIP model)"),Xeo=l(),Ih=a("li"),LJ=a("strong"),zeo=o("convbert"),Veo=o(" \u2014 "),xx=a("a"),Weo=o("ConvBertModel"),Qeo=o(" (ConvBERT model)"),Heo=l(),jh=a("li"),BJ=a("strong"),Ueo=o("convnext"),Jeo=o(" \u2014 "),Rx=a("a"),Yeo=o("ConvNextModel"),Keo=o(" (ConvNext model)"),Zeo=l(),Nh=a("li"),kJ=a("strong"),eoo=o("ctrl"),ooo=o(" \u2014 "),Sx=a("a"),roo=o("CTRLModel"),too=o(" (CTRL model)"),aoo=l(),Dh=a("li"),xJ=a("strong"),noo=o("deberta"),soo=o(" \u2014 "),Px=a("a"),loo=o("DebertaModel"),ioo=o(" (DeBERTa model)"),doo=l(),qh=a("li"),RJ=a("strong"),coo=o("deberta-v2"),foo=o(" \u2014 "),$x=a("a"),moo=o("DebertaV2Model"),goo=o(" (DeBERTa-v2 model)"),hoo=l(),Gh=a("li"),SJ=a("strong"),poo=o("deit"),_oo=o(" \u2014 "),Ix=a("a"),uoo=o("DeiTModel"),boo=o(" (DeiT model)"),voo=l(),Oh=a("li"),PJ=a("strong"),Too=o("detr"),Foo=o(" \u2014 "),jx=a("a"),Coo=o("DetrModel"),Moo=o(" (DETR model)"),Eoo=l(),Xh=a("li"),$J=a("strong"),yoo=o("distilbert"),woo=o(" \u2014 "),Nx=a("a"),Aoo=o("DistilBertModel"),Loo=o(" (DistilBERT model)"),Boo=l(),zh=a("li"),IJ=a("strong"),koo=o("dpr"),xoo=o(" \u2014 "),Dx=a("a"),Roo=o("DPRQuestionEncoder"),Soo=o(" (DPR model)"),Poo=l(),Vh=a("li"),jJ=a("strong"),$oo=o("electra"),Ioo=o(" \u2014 "),qx=a("a"),joo=o("ElectraModel"),Noo=o(" (ELECTRA model)"),Doo=l(),Wh=a("li"),NJ=a("strong"),qoo=o("flaubert"),Goo=o(" \u2014 "),Gx=a("a"),Ooo=o("FlaubertModel"),Xoo=o(" (FlauBERT model)"),zoo=l(),Qh=a("li"),DJ=a("strong"),Voo=o("fnet"),Woo=o(" \u2014 "),Ox=a("a"),Qoo=o("FNetModel"),Hoo=o(" (FNet model)"),Uoo=l(),Hh=a("li"),qJ=a("strong"),Joo=o("fsmt"),Yoo=o(" \u2014 "),Xx=a("a"),Koo=o("FSMTModel"),Zoo=o(" (FairSeq Machine-Translation model)"),ero=l(),xs=a("li"),GJ=a("strong"),oro=o("funnel"),rro=o(" \u2014 "),zx=a("a"),tro=o("FunnelModel"),aro=o(" or "),Vx=a("a"),nro=o("FunnelBaseModel"),sro=o(" (Funnel Transformer model)"),lro=l(),Uh=a("li"),OJ=a("strong"),iro=o("gpt2"),dro=o(" \u2014 "),Wx=a("a"),cro=o("GPT2Model"),fro=o(" (OpenAI GPT-2 model)"),mro=l(),Jh=a("li"),XJ=a("strong"),gro=o("gpt_neo"),hro=o(" \u2014 "),Qx=a("a"),pro=o("GPTNeoModel"),_ro=o(" (GPT Neo model)"),uro=l(),Yh=a("li"),zJ=a("strong"),bro=o("gptj"),vro=o(" \u2014 "),Hx=a("a"),Tro=o("GPTJModel"),Fro=o(" (GPT-J model)"),Cro=l(),Kh=a("li"),VJ=a("strong"),Mro=o("hubert"),Ero=o(" \u2014 "),Ux=a("a"),yro=o("HubertModel"),wro=o(" (Hubert model)"),Aro=l(),Zh=a("li"),WJ=a("strong"),Lro=o("ibert"),Bro=o(" \u2014 "),Jx=a("a"),kro=o("IBertModel"),xro=o(" (I-BERT model)"),Rro=l(),ep=a("li"),QJ=a("strong"),Sro=o("imagegpt"),Pro=o(" \u2014 "),Yx=a("a"),$ro=o("ImageGPTModel"),Iro=o(" (ImageGPT model)"),jro=l(),op=a("li"),HJ=a("strong"),Nro=o("layoutlm"),Dro=o(" \u2014 "),Kx=a("a"),qro=o("LayoutLMModel"),Gro=o(" (LayoutLM model)"),Oro=l(),rp=a("li"),UJ=a("strong"),Xro=o("layoutlmv2"),zro=o(" \u2014 "),Zx=a("a"),Vro=o("LayoutLMv2Model"),Wro=o(" (LayoutLMv2 model)"),Qro=l(),tp=a("li"),JJ=a("strong"),Hro=o("led"),Uro=o(" \u2014 "),eR=a("a"),Jro=o("LEDModel"),Yro=o(" (LED model)"),Kro=l(),ap=a("li"),YJ=a("strong"),Zro=o("longformer"),eto=o(" \u2014 "),oR=a("a"),oto=o("LongformerModel"),rto=o(" (Longformer model)"),tto=l(),np=a("li"),KJ=a("strong"),ato=o("luke"),nto=o(" \u2014 "),rR=a("a"),sto=o("LukeModel"),lto=o(" (LUKE model)"),ito=l(),sp=a("li"),ZJ=a("strong"),dto=o("lxmert"),cto=o(" \u2014 "),tR=a("a"),fto=o("LxmertModel"),mto=o(" (LXMERT model)"),gto=l(),lp=a("li"),eY=a("strong"),hto=o("m2m_100"),pto=o(" \u2014 "),aR=a("a"),_to=o("M2M100Model"),uto=o(" (M2M100 model)"),bto=l(),ip=a("li"),oY=a("strong"),vto=o("marian"),Tto=o(" \u2014 "),nR=a("a"),Fto=o("MarianModel"),Cto=o(" (Marian model)"),Mto=l(),dp=a("li"),rY=a("strong"),Eto=o("mbart"),yto=o(" \u2014 "),sR=a("a"),wto=o("MBartModel"),Ato=o(" (mBART model)"),Lto=l(),cp=a("li"),tY=a("strong"),Bto=o("megatron-bert"),kto=o(" \u2014 "),lR=a("a"),xto=o("MegatronBertModel"),Rto=o(" (MegatronBert model)"),Sto=l(),fp=a("li"),aY=a("strong"),Pto=o("mobilebert"),$to=o(" \u2014 "),iR=a("a"),Ito=o("MobileBertModel"),jto=o(" (MobileBERT model)"),Nto=l(),mp=a("li"),nY=a("strong"),Dto=o("mpnet"),qto=o(" \u2014 "),dR=a("a"),Gto=o("MPNetModel"),Oto=o(" (MPNet model)"),Xto=l(),gp=a("li"),sY=a("strong"),zto=o("mt5"),Vto=o(" \u2014 "),cR=a("a"),Wto=o("MT5Model"),Qto=o(" (mT5 model)"),Hto=l(),hp=a("li"),lY=a("strong"),Uto=o("nystromformer"),Jto=o(" \u2014 "),fR=a("a"),Yto=o("NystromformerModel"),Kto=o(" (Nystromformer model)"),Zto=l(),pp=a("li"),iY=a("strong"),eao=o("openai-gpt"),oao=o(" \u2014 "),mR=a("a"),rao=o("OpenAIGPTModel"),tao=o(" (OpenAI GPT model)"),aao=l(),_p=a("li"),dY=a("strong"),nao=o("pegasus"),sao=o(" \u2014 "),gR=a("a"),lao=o("PegasusModel"),iao=o(" (Pegasus model)"),dao=l(),up=a("li"),cY=a("strong"),cao=o("perceiver"),fao=o(" \u2014 "),hR=a("a"),mao=o("PerceiverModel"),gao=o(" (Perceiver model)"),hao=l(),bp=a("li"),fY=a("strong"),pao=o("plbart"),_ao=o(" \u2014 "),pR=a("a"),uao=o("PLBartModel"),bao=o(" (PLBart model)"),vao=l(),vp=a("li"),mY=a("strong"),Tao=o("poolformer"),Fao=o(" \u2014 "),_R=a("a"),Cao=o("PoolFormerModel"),Mao=o(" (PoolFormer model)"),Eao=l(),Tp=a("li"),gY=a("strong"),yao=o("prophetnet"),wao=o(" \u2014 "),uR=a("a"),Aao=o("ProphetNetModel"),Lao=o(" (ProphetNet model)"),Bao=l(),Fp=a("li"),hY=a("strong"),kao=o("qdqbert"),xao=o(" \u2014 "),bR=a("a"),Rao=o("QDQBertModel"),Sao=o(" (QDQBert model)"),Pao=l(),Cp=a("li"),pY=a("strong"),$ao=o("reformer"),Iao=o(" \u2014 "),vR=a("a"),jao=o("ReformerModel"),Nao=o(" (Reformer model)"),Dao=l(),Mp=a("li"),_Y=a("strong"),qao=o("rembert"),Gao=o(" \u2014 "),TR=a("a"),Oao=o("RemBertModel"),Xao=o(" (RemBERT model)"),zao=l(),Ep=a("li"),uY=a("strong"),Vao=o("resnet"),Wao=o(" \u2014 "),FR=a("a"),Qao=o("ResNetModel"),Hao=o(" (resnet model)"),Uao=l(),yp=a("li"),bY=a("strong"),Jao=o("retribert"),Yao=o(" \u2014 "),CR=a("a"),Kao=o("RetriBertModel"),Zao=o(" (RetriBERT model)"),eno=l(),wp=a("li"),vY=a("strong"),ono=o("roberta"),rno=o(" \u2014 "),MR=a("a"),tno=o("RobertaModel"),ano=o(" (RoBERTa model)"),nno=l(),Ap=a("li"),TY=a("strong"),sno=o("roformer"),lno=o(" \u2014 "),ER=a("a"),ino=o("RoFormerModel"),dno=o(" (RoFormer model)"),cno=l(),Lp=a("li"),FY=a("strong"),fno=o("segformer"),mno=o(" \u2014 "),yR=a("a"),gno=o("SegformerModel"),hno=o(" (SegFormer model)"),pno=l(),Bp=a("li"),CY=a("strong"),_no=o("sew"),uno=o(" \u2014 "),wR=a("a"),bno=o("SEWModel"),vno=o(" (SEW model)"),Tno=l(),kp=a("li"),MY=a("strong"),Fno=o("sew-d"),Cno=o(" \u2014 "),AR=a("a"),Mno=o("SEWDModel"),Eno=o(" (SEW-D model)"),yno=l(),xp=a("li"),EY=a("strong"),wno=o("speech_to_text"),Ano=o(" \u2014 "),LR=a("a"),Lno=o("Speech2TextModel"),Bno=o(" (Speech2Text model)"),kno=l(),Rp=a("li"),yY=a("strong"),xno=o("splinter"),Rno=o(" \u2014 "),BR=a("a"),Sno=o("SplinterModel"),Pno=o(" (Splinter model)"),$no=l(),Sp=a("li"),wY=a("strong"),Ino=o("squeezebert"),jno=o(" \u2014 "),kR=a("a"),Nno=o("SqueezeBertModel"),Dno=o(" (SqueezeBERT model)"),qno=l(),Pp=a("li"),AY=a("strong"),Gno=o("swin"),Ono=o(" \u2014 "),xR=a("a"),Xno=o("SwinModel"),zno=o(" (Swin model)"),Vno=l(),$p=a("li"),LY=a("strong"),Wno=o("t5"),Qno=o(" \u2014 "),RR=a("a"),Hno=o("T5Model"),Uno=o(" (T5 model)"),Jno=l(),Ip=a("li"),BY=a("strong"),Yno=o("tapas"),Kno=o(" \u2014 "),SR=a("a"),Zno=o("TapasModel"),eso=o(" (TAPAS model)"),oso=l(),jp=a("li"),kY=a("strong"),rso=o("transfo-xl"),tso=o(" \u2014 "),PR=a("a"),aso=o("TransfoXLModel"),nso=o(" (Transformer-XL model)"),sso=l(),Np=a("li"),xY=a("strong"),lso=o("unispeech"),iso=o(" \u2014 "),$R=a("a"),dso=o("UniSpeechModel"),cso=o(" (UniSpeech model)"),fso=l(),Dp=a("li"),RY=a("strong"),mso=o("unispeech-sat"),gso=o(" \u2014 "),IR=a("a"),hso=o("UniSpeechSatModel"),pso=o(" (UniSpeechSat model)"),_so=l(),qp=a("li"),SY=a("strong"),uso=o("vilt"),bso=o(" \u2014 "),jR=a("a"),vso=o("ViltModel"),Tso=o(" (ViLT model)"),Fso=l(),Gp=a("li"),PY=a("strong"),Cso=o("vision-text-dual-encoder"),Mso=o(" \u2014 "),NR=a("a"),Eso=o("VisionTextDualEncoderModel"),yso=o(" (VisionTextDualEncoder model)"),wso=l(),Op=a("li"),$Y=a("strong"),Aso=o("visual_bert"),Lso=o(" \u2014 "),DR=a("a"),Bso=o("VisualBertModel"),kso=o(" (VisualBert model)"),xso=l(),Xp=a("li"),IY=a("strong"),Rso=o("vit"),Sso=o(" \u2014 "),qR=a("a"),Pso=o("ViTModel"),$so=o(" (ViT model)"),Iso=l(),zp=a("li"),jY=a("strong"),jso=o("vit_mae"),Nso=o(" \u2014 "),GR=a("a"),Dso=o("ViTMAEModel"),qso=o(" (ViTMAE model)"),Gso=l(),Vp=a("li"),NY=a("strong"),Oso=o("wav2vec2"),Xso=o(" \u2014 "),OR=a("a"),zso=o("Wav2Vec2Model"),Vso=o(" (Wav2Vec2 model)"),Wso=l(),Wp=a("li"),DY=a("strong"),Qso=o("wavlm"),Hso=o(" \u2014 "),XR=a("a"),Uso=o("WavLMModel"),Jso=o(" (WavLM model)"),Yso=l(),Qp=a("li"),qY=a("strong"),Kso=o("xglm"),Zso=o(" \u2014 "),zR=a("a"),elo=o("XGLMModel"),olo=o(" (XGLM model)"),rlo=l(),Hp=a("li"),GY=a("strong"),tlo=o("xlm"),alo=o(" \u2014 "),VR=a("a"),nlo=o("XLMModel"),slo=o(" (XLM model)"),llo=l(),Up=a("li"),OY=a("strong"),ilo=o("xlm-prophetnet"),dlo=o(" \u2014 "),WR=a("a"),clo=o("XLMProphetNetModel"),flo=o(" (XLMProphetNet model)"),mlo=l(),Jp=a("li"),XY=a("strong"),glo=o("xlm-roberta"),hlo=o(" \u2014 "),QR=a("a"),plo=o("XLMRobertaModel"),_lo=o(" (XLM-RoBERTa model)"),ulo=l(),Yp=a("li"),zY=a("strong"),blo=o("xlm-roberta-xl"),vlo=o(" \u2014 "),HR=a("a"),Tlo=o("XLMRobertaXLModel"),Flo=o(" (XLM-RoBERTa-XL model)"),Clo=l(),Kp=a("li"),VY=a("strong"),Mlo=o("xlnet"),Elo=o(" \u2014 "),UR=a("a"),ylo=o("XLNetModel"),wlo=o(" (XLNet model)"),Alo=l(),Zp=a("li"),WY=a("strong"),Llo=o("yoso"),Blo=o(" \u2014 "),JR=a("a"),klo=o("YosoModel"),xlo=o(" (YOSO model)"),Rlo=l(),e_=a("p"),Slo=o("The model is set in evaluation mode by default using "),QY=a("code"),Plo=o("model.eval()"),$lo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),HY=a("code"),Ilo=o("model.train()"),jlo=l(),UY=a("p"),Nlo=o("Examples:"),Dlo=l(),f(VM.$$.fragment),P8e=l(),Xi=a("h2"),o_=a("a"),JY=a("span"),f(WM.$$.fragment),qlo=l(),YY=a("span"),Glo=o("AutoModelForPreTraining"),$8e=l(),Wo=a("div"),f(QM.$$.fragment),Olo=l(),zi=a("p"),Xlo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a pretraining head) when created
with the `),KY=a("code"),zlo=o("from_pretrained()"),Vlo=o("class method or the "),ZY=a("code"),Wlo=o("from_config()"),Qlo=o(`class
method.`),Hlo=l(),HM=a("p"),Ulo=o("This class cannot be instantiated directly using "),eK=a("code"),Jlo=o("__init__()"),Ylo=o(" (throws an error)."),Klo=l(),Dr=a("div"),f(UM.$$.fragment),Zlo=l(),oK=a("p"),eio=o("Instantiates one of the model classes of the library (with a pretraining head) from a configuration."),oio=l(),Vi=a("p"),rio=o(`Note:
Loading a model from its configuration file does `),rK=a("strong"),tio=o("not"),aio=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),tK=a("code"),nio=o("from_pretrained()"),sio=o("to load the model weights."),lio=l(),aK=a("p"),iio=o("Examples:"),dio=l(),f(JM.$$.fragment),cio=l(),Re=a("div"),f(YM.$$.fragment),fio=l(),nK=a("p"),mio=o("Instantiate one of the model classes of the library (with a pretraining head) from a pretrained model."),gio=l(),qa=a("p"),hio=o("The model class to instantiate is selected based on the "),sK=a("code"),pio=o("model_type"),_io=o(` property of the config object (either
passed as an argument or loaded from `),lK=a("code"),uio=o("pretrained_model_name_or_path"),bio=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),iK=a("code"),vio=o("pretrained_model_name_or_path"),Tio=o(":"),Fio=l(),x=a("ul"),r_=a("li"),dK=a("strong"),Cio=o("albert"),Mio=o(" \u2014 "),YR=a("a"),Eio=o("AlbertForPreTraining"),yio=o(" (ALBERT model)"),wio=l(),t_=a("li"),cK=a("strong"),Aio=o("bart"),Lio=o(" \u2014 "),KR=a("a"),Bio=o("BartForConditionalGeneration"),kio=o(" (BART model)"),xio=l(),a_=a("li"),fK=a("strong"),Rio=o("bert"),Sio=o(" \u2014 "),ZR=a("a"),Pio=o("BertForPreTraining"),$io=o(" (BERT model)"),Iio=l(),n_=a("li"),mK=a("strong"),jio=o("big_bird"),Nio=o(" \u2014 "),eS=a("a"),Dio=o("BigBirdForPreTraining"),qio=o(" (BigBird model)"),Gio=l(),s_=a("li"),gK=a("strong"),Oio=o("camembert"),Xio=o(" \u2014 "),oS=a("a"),zio=o("CamembertForMaskedLM"),Vio=o(" (CamemBERT model)"),Wio=l(),l_=a("li"),hK=a("strong"),Qio=o("ctrl"),Hio=o(" \u2014 "),rS=a("a"),Uio=o("CTRLLMHeadModel"),Jio=o(" (CTRL model)"),Yio=l(),i_=a("li"),pK=a("strong"),Kio=o("deberta"),Zio=o(" \u2014 "),tS=a("a"),edo=o("DebertaForMaskedLM"),odo=o(" (DeBERTa model)"),rdo=l(),d_=a("li"),_K=a("strong"),tdo=o("deberta-v2"),ado=o(" \u2014 "),aS=a("a"),ndo=o("DebertaV2ForMaskedLM"),sdo=o(" (DeBERTa-v2 model)"),ldo=l(),c_=a("li"),uK=a("strong"),ido=o("distilbert"),ddo=o(" \u2014 "),nS=a("a"),cdo=o("DistilBertForMaskedLM"),fdo=o(" (DistilBERT model)"),mdo=l(),f_=a("li"),bK=a("strong"),gdo=o("electra"),hdo=o(" \u2014 "),sS=a("a"),pdo=o("ElectraForPreTraining"),_do=o(" (ELECTRA model)"),udo=l(),m_=a("li"),vK=a("strong"),bdo=o("flaubert"),vdo=o(" \u2014 "),lS=a("a"),Tdo=o("FlaubertWithLMHeadModel"),Fdo=o(" (FlauBERT model)"),Cdo=l(),g_=a("li"),TK=a("strong"),Mdo=o("fnet"),Edo=o(" \u2014 "),iS=a("a"),ydo=o("FNetForPreTraining"),wdo=o(" (FNet model)"),Ado=l(),h_=a("li"),FK=a("strong"),Ldo=o("fsmt"),Bdo=o(" \u2014 "),dS=a("a"),kdo=o("FSMTForConditionalGeneration"),xdo=o(" (FairSeq Machine-Translation model)"),Rdo=l(),p_=a("li"),CK=a("strong"),Sdo=o("funnel"),Pdo=o(" \u2014 "),cS=a("a"),$do=o("FunnelForPreTraining"),Ido=o(" (Funnel Transformer model)"),jdo=l(),__=a("li"),MK=a("strong"),Ndo=o("gpt2"),Ddo=o(" \u2014 "),fS=a("a"),qdo=o("GPT2LMHeadModel"),Gdo=o(" (OpenAI GPT-2 model)"),Odo=l(),u_=a("li"),EK=a("strong"),Xdo=o("ibert"),zdo=o(" \u2014 "),mS=a("a"),Vdo=o("IBertForMaskedLM"),Wdo=o(" (I-BERT model)"),Qdo=l(),b_=a("li"),yK=a("strong"),Hdo=o("layoutlm"),Udo=o(" \u2014 "),gS=a("a"),Jdo=o("LayoutLMForMaskedLM"),Ydo=o(" (LayoutLM model)"),Kdo=l(),v_=a("li"),wK=a("strong"),Zdo=o("longformer"),eco=o(" \u2014 "),hS=a("a"),oco=o("LongformerForMaskedLM"),rco=o(" (Longformer model)"),tco=l(),T_=a("li"),AK=a("strong"),aco=o("lxmert"),nco=o(" \u2014 "),pS=a("a"),sco=o("LxmertForPreTraining"),lco=o(" (LXMERT model)"),ico=l(),F_=a("li"),LK=a("strong"),dco=o("megatron-bert"),cco=o(" \u2014 "),_S=a("a"),fco=o("MegatronBertForPreTraining"),mco=o(" (MegatronBert model)"),gco=l(),C_=a("li"),BK=a("strong"),hco=o("mobilebert"),pco=o(" \u2014 "),uS=a("a"),_co=o("MobileBertForPreTraining"),uco=o(" (MobileBERT model)"),bco=l(),M_=a("li"),kK=a("strong"),vco=o("mpnet"),Tco=o(" \u2014 "),bS=a("a"),Fco=o("MPNetForMaskedLM"),Cco=o(" (MPNet model)"),Mco=l(),E_=a("li"),xK=a("strong"),Eco=o("openai-gpt"),yco=o(" \u2014 "),vS=a("a"),wco=o("OpenAIGPTLMHeadModel"),Aco=o(" (OpenAI GPT model)"),Lco=l(),y_=a("li"),RK=a("strong"),Bco=o("retribert"),kco=o(" \u2014 "),TS=a("a"),xco=o("RetriBertModel"),Rco=o(" (RetriBERT model)"),Sco=l(),w_=a("li"),SK=a("strong"),Pco=o("roberta"),$co=o(" \u2014 "),FS=a("a"),Ico=o("RobertaForMaskedLM"),jco=o(" (RoBERTa model)"),Nco=l(),A_=a("li"),PK=a("strong"),Dco=o("squeezebert"),qco=o(" \u2014 "),CS=a("a"),Gco=o("SqueezeBertForMaskedLM"),Oco=o(" (SqueezeBERT model)"),Xco=l(),L_=a("li"),$K=a("strong"),zco=o("t5"),Vco=o(" \u2014 "),MS=a("a"),Wco=o("T5ForConditionalGeneration"),Qco=o(" (T5 model)"),Hco=l(),B_=a("li"),IK=a("strong"),Uco=o("tapas"),Jco=o(" \u2014 "),ES=a("a"),Yco=o("TapasForMaskedLM"),Kco=o(" (TAPAS model)"),Zco=l(),k_=a("li"),jK=a("strong"),efo=o("transfo-xl"),ofo=o(" \u2014 "),yS=a("a"),rfo=o("TransfoXLLMHeadModel"),tfo=o(" (Transformer-XL model)"),afo=l(),x_=a("li"),NK=a("strong"),nfo=o("unispeech"),sfo=o(" \u2014 "),wS=a("a"),lfo=o("UniSpeechForPreTraining"),ifo=o(" (UniSpeech model)"),dfo=l(),R_=a("li"),DK=a("strong"),cfo=o("unispeech-sat"),ffo=o(" \u2014 "),AS=a("a"),mfo=o("UniSpeechSatForPreTraining"),gfo=o(" (UniSpeechSat model)"),hfo=l(),S_=a("li"),qK=a("strong"),pfo=o("visual_bert"),_fo=o(" \u2014 "),LS=a("a"),ufo=o("VisualBertForPreTraining"),bfo=o(" (VisualBert model)"),vfo=l(),P_=a("li"),GK=a("strong"),Tfo=o("vit_mae"),Ffo=o(" \u2014 "),BS=a("a"),Cfo=o("ViTMAEForPreTraining"),Mfo=o(" (ViTMAE model)"),Efo=l(),$_=a("li"),OK=a("strong"),yfo=o("wav2vec2"),wfo=o(" \u2014 "),kS=a("a"),Afo=o("Wav2Vec2ForPreTraining"),Lfo=o(" (Wav2Vec2 model)"),Bfo=l(),I_=a("li"),XK=a("strong"),kfo=o("xlm"),xfo=o(" \u2014 "),xS=a("a"),Rfo=o("XLMWithLMHeadModel"),Sfo=o(" (XLM model)"),Pfo=l(),j_=a("li"),zK=a("strong"),$fo=o("xlm-roberta"),Ifo=o(" \u2014 "),RS=a("a"),jfo=o("XLMRobertaForMaskedLM"),Nfo=o(" (XLM-RoBERTa model)"),Dfo=l(),N_=a("li"),VK=a("strong"),qfo=o("xlm-roberta-xl"),Gfo=o(" \u2014 "),SS=a("a"),Ofo=o("XLMRobertaXLForMaskedLM"),Xfo=o(" (XLM-RoBERTa-XL model)"),zfo=l(),D_=a("li"),WK=a("strong"),Vfo=o("xlnet"),Wfo=o(" \u2014 "),PS=a("a"),Qfo=o("XLNetLMHeadModel"),Hfo=o(" (XLNet model)"),Ufo=l(),q_=a("p"),Jfo=o("The model is set in evaluation mode by default using "),QK=a("code"),Yfo=o("model.eval()"),Kfo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),HK=a("code"),Zfo=o("model.train()"),emo=l(),UK=a("p"),omo=o("Examples:"),rmo=l(),f(KM.$$.fragment),I8e=l(),Wi=a("h2"),G_=a("a"),JK=a("span"),f(ZM.$$.fragment),tmo=l(),YK=a("span"),amo=o("AutoModelForCausalLM"),j8e=l(),Qo=a("div"),f(eE.$$.fragment),nmo=l(),Qi=a("p"),smo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a causal language modeling head) when created
with the `),KK=a("code"),lmo=o("from_pretrained()"),imo=o("class method or the "),ZK=a("code"),dmo=o("from_config()"),cmo=o(`class
method.`),fmo=l(),oE=a("p"),mmo=o("This class cannot be instantiated directly using "),eZ=a("code"),gmo=o("__init__()"),hmo=o(" (throws an error)."),pmo=l(),qr=a("div"),f(rE.$$.fragment),_mo=l(),oZ=a("p"),umo=o("Instantiates one of the model classes of the library (with a causal language modeling head) from a configuration."),bmo=l(),Hi=a("p"),vmo=o(`Note:
Loading a model from its configuration file does `),rZ=a("strong"),Tmo=o("not"),Fmo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),tZ=a("code"),Cmo=o("from_pretrained()"),Mmo=o("to load the model weights."),Emo=l(),aZ=a("p"),ymo=o("Examples:"),wmo=l(),f(tE.$$.fragment),Amo=l(),Se=a("div"),f(aE.$$.fragment),Lmo=l(),nZ=a("p"),Bmo=o("Instantiate one of the model classes of the library (with a causal language modeling head) from a pretrained model."),kmo=l(),Ga=a("p"),xmo=o("The model class to instantiate is selected based on the "),sZ=a("code"),Rmo=o("model_type"),Smo=o(` property of the config object (either
passed as an argument or loaded from `),lZ=a("code"),Pmo=o("pretrained_model_name_or_path"),$mo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),iZ=a("code"),Imo=o("pretrained_model_name_or_path"),jmo=o(":"),Nmo=l(),$=a("ul"),O_=a("li"),dZ=a("strong"),Dmo=o("bart"),qmo=o(" \u2014 "),$S=a("a"),Gmo=o("BartForCausalLM"),Omo=o(" (BART model)"),Xmo=l(),X_=a("li"),cZ=a("strong"),zmo=o("bert"),Vmo=o(" \u2014 "),IS=a("a"),Wmo=o("BertLMHeadModel"),Qmo=o(" (BERT model)"),Hmo=l(),z_=a("li"),fZ=a("strong"),Umo=o("bert-generation"),Jmo=o(" \u2014 "),jS=a("a"),Ymo=o("BertGenerationDecoder"),Kmo=o(" (Bert Generation model)"),Zmo=l(),V_=a("li"),mZ=a("strong"),ego=o("big_bird"),ogo=o(" \u2014 "),NS=a("a"),rgo=o("BigBirdForCausalLM"),tgo=o(" (BigBird model)"),ago=l(),W_=a("li"),gZ=a("strong"),ngo=o("bigbird_pegasus"),sgo=o(" \u2014 "),DS=a("a"),lgo=o("BigBirdPegasusForCausalLM"),igo=o(" (BigBirdPegasus model)"),dgo=l(),Q_=a("li"),hZ=a("strong"),cgo=o("blenderbot"),fgo=o(" \u2014 "),qS=a("a"),mgo=o("BlenderbotForCausalLM"),ggo=o(" (Blenderbot model)"),hgo=l(),H_=a("li"),pZ=a("strong"),pgo=o("blenderbot-small"),_go=o(" \u2014 "),GS=a("a"),ugo=o("BlenderbotSmallForCausalLM"),bgo=o(" (BlenderbotSmall model)"),vgo=l(),U_=a("li"),_Z=a("strong"),Tgo=o("camembert"),Fgo=o(" \u2014 "),OS=a("a"),Cgo=o("CamembertForCausalLM"),Mgo=o(" (CamemBERT model)"),Ego=l(),J_=a("li"),uZ=a("strong"),ygo=o("ctrl"),wgo=o(" \u2014 "),XS=a("a"),Ago=o("CTRLLMHeadModel"),Lgo=o(" (CTRL model)"),Bgo=l(),Y_=a("li"),bZ=a("strong"),kgo=o("electra"),xgo=o(" \u2014 "),zS=a("a"),Rgo=o("ElectraForCausalLM"),Sgo=o(" (ELECTRA model)"),Pgo=l(),K_=a("li"),vZ=a("strong"),$go=o("gpt2"),Igo=o(" \u2014 "),VS=a("a"),jgo=o("GPT2LMHeadModel"),Ngo=o(" (OpenAI GPT-2 model)"),Dgo=l(),Z_=a("li"),TZ=a("strong"),qgo=o("gpt_neo"),Ggo=o(" \u2014 "),WS=a("a"),Ogo=o("GPTNeoForCausalLM"),Xgo=o(" (GPT Neo model)"),zgo=l(),eu=a("li"),FZ=a("strong"),Vgo=o("gptj"),Wgo=o(" \u2014 "),QS=a("a"),Qgo=o("GPTJForCausalLM"),Hgo=o(" (GPT-J model)"),Ugo=l(),ou=a("li"),CZ=a("strong"),Jgo=o("marian"),Ygo=o(" \u2014 "),HS=a("a"),Kgo=o("MarianForCausalLM"),Zgo=o(" (Marian model)"),eho=l(),ru=a("li"),MZ=a("strong"),oho=o("mbart"),rho=o(" \u2014 "),US=a("a"),tho=o("MBartForCausalLM"),aho=o(" (mBART model)"),nho=l(),tu=a("li"),EZ=a("strong"),sho=o("megatron-bert"),lho=o(" \u2014 "),JS=a("a"),iho=o("MegatronBertForCausalLM"),dho=o(" (MegatronBert model)"),cho=l(),au=a("li"),yZ=a("strong"),fho=o("openai-gpt"),mho=o(" \u2014 "),YS=a("a"),gho=o("OpenAIGPTLMHeadModel"),hho=o(" (OpenAI GPT model)"),pho=l(),nu=a("li"),wZ=a("strong"),_ho=o("pegasus"),uho=o(" \u2014 "),KS=a("a"),bho=o("PegasusForCausalLM"),vho=o(" (Pegasus model)"),Tho=l(),su=a("li"),AZ=a("strong"),Fho=o("plbart"),Cho=o(" \u2014 "),ZS=a("a"),Mho=o("PLBartForCausalLM"),Eho=o(" (PLBart model)"),yho=l(),lu=a("li"),LZ=a("strong"),who=o("prophetnet"),Aho=o(" \u2014 "),eP=a("a"),Lho=o("ProphetNetForCausalLM"),Bho=o(" (ProphetNet model)"),kho=l(),iu=a("li"),BZ=a("strong"),xho=o("qdqbert"),Rho=o(" \u2014 "),oP=a("a"),Sho=o("QDQBertLMHeadModel"),Pho=o(" (QDQBert model)"),$ho=l(),du=a("li"),kZ=a("strong"),Iho=o("reformer"),jho=o(" \u2014 "),rP=a("a"),Nho=o("ReformerModelWithLMHead"),Dho=o(" (Reformer model)"),qho=l(),cu=a("li"),xZ=a("strong"),Gho=o("rembert"),Oho=o(" \u2014 "),tP=a("a"),Xho=o("RemBertForCausalLM"),zho=o(" (RemBERT model)"),Vho=l(),fu=a("li"),RZ=a("strong"),Who=o("roberta"),Qho=o(" \u2014 "),aP=a("a"),Hho=o("RobertaForCausalLM"),Uho=o(" (RoBERTa model)"),Jho=l(),mu=a("li"),SZ=a("strong"),Yho=o("roformer"),Kho=o(" \u2014 "),nP=a("a"),Zho=o("RoFormerForCausalLM"),epo=o(" (RoFormer model)"),opo=l(),gu=a("li"),PZ=a("strong"),rpo=o("speech_to_text_2"),tpo=o(" \u2014 "),sP=a("a"),apo=o("Speech2Text2ForCausalLM"),npo=o(" (Speech2Text2 model)"),spo=l(),hu=a("li"),$Z=a("strong"),lpo=o("transfo-xl"),ipo=o(" \u2014 "),lP=a("a"),dpo=o("TransfoXLLMHeadModel"),cpo=o(" (Transformer-XL model)"),fpo=l(),pu=a("li"),IZ=a("strong"),mpo=o("trocr"),gpo=o(" \u2014 "),iP=a("a"),hpo=o("TrOCRForCausalLM"),ppo=o(" (TrOCR model)"),_po=l(),_u=a("li"),jZ=a("strong"),upo=o("xglm"),bpo=o(" \u2014 "),dP=a("a"),vpo=o("XGLMForCausalLM"),Tpo=o(" (XGLM model)"),Fpo=l(),uu=a("li"),NZ=a("strong"),Cpo=o("xlm"),Mpo=o(" \u2014 "),cP=a("a"),Epo=o("XLMWithLMHeadModel"),ypo=o(" (XLM model)"),wpo=l(),bu=a("li"),DZ=a("strong"),Apo=o("xlm-prophetnet"),Lpo=o(" \u2014 "),fP=a("a"),Bpo=o("XLMProphetNetForCausalLM"),kpo=o(" (XLMProphetNet model)"),xpo=l(),vu=a("li"),qZ=a("strong"),Rpo=o("xlm-roberta"),Spo=o(" \u2014 "),mP=a("a"),Ppo=o("XLMRobertaForCausalLM"),$po=o(" (XLM-RoBERTa model)"),Ipo=l(),Tu=a("li"),GZ=a("strong"),jpo=o("xlm-roberta-xl"),Npo=o(" \u2014 "),gP=a("a"),Dpo=o("XLMRobertaXLForCausalLM"),qpo=o(" (XLM-RoBERTa-XL model)"),Gpo=l(),Fu=a("li"),OZ=a("strong"),Opo=o("xlnet"),Xpo=o(" \u2014 "),hP=a("a"),zpo=o("XLNetLMHeadModel"),Vpo=o(" (XLNet model)"),Wpo=l(),Cu=a("p"),Qpo=o("The model is set in evaluation mode by default using "),XZ=a("code"),Hpo=o("model.eval()"),Upo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),zZ=a("code"),Jpo=o("model.train()"),Ypo=l(),VZ=a("p"),Kpo=o("Examples:"),Zpo=l(),f(nE.$$.fragment),N8e=l(),Ui=a("h2"),Mu=a("a"),WZ=a("span"),f(sE.$$.fragment),e_o=l(),QZ=a("span"),o_o=o("AutoModelForMaskedLM"),D8e=l(),Ho=a("div"),f(lE.$$.fragment),r_o=l(),Ji=a("p"),t_o=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a masked language modeling head) when created
with the `),HZ=a("code"),a_o=o("from_pretrained()"),n_o=o("class method or the "),UZ=a("code"),s_o=o("from_config()"),l_o=o(`class
method.`),i_o=l(),iE=a("p"),d_o=o("This class cannot be instantiated directly using "),JZ=a("code"),c_o=o("__init__()"),f_o=o(" (throws an error)."),m_o=l(),Gr=a("div"),f(dE.$$.fragment),g_o=l(),YZ=a("p"),h_o=o("Instantiates one of the model classes of the library (with a masked language modeling head) from a configuration."),p_o=l(),Yi=a("p"),__o=o(`Note:
Loading a model from its configuration file does `),KZ=a("strong"),u_o=o("not"),b_o=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),ZZ=a("code"),v_o=o("from_pretrained()"),T_o=o("to load the model weights."),F_o=l(),eee=a("p"),C_o=o("Examples:"),M_o=l(),f(cE.$$.fragment),E_o=l(),Pe=a("div"),f(fE.$$.fragment),y_o=l(),oee=a("p"),w_o=o("Instantiate one of the model classes of the library (with a masked language modeling head) from a pretrained model."),A_o=l(),Oa=a("p"),L_o=o("The model class to instantiate is selected based on the "),ree=a("code"),B_o=o("model_type"),k_o=o(` property of the config object (either
passed as an argument or loaded from `),tee=a("code"),x_o=o("pretrained_model_name_or_path"),R_o=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),aee=a("code"),S_o=o("pretrained_model_name_or_path"),P_o=o(":"),$_o=l(),I=a("ul"),Eu=a("li"),nee=a("strong"),I_o=o("albert"),j_o=o(" \u2014 "),pP=a("a"),N_o=o("AlbertForMaskedLM"),D_o=o(" (ALBERT model)"),q_o=l(),yu=a("li"),see=a("strong"),G_o=o("bart"),O_o=o(" \u2014 "),_P=a("a"),X_o=o("BartForConditionalGeneration"),z_o=o(" (BART model)"),V_o=l(),wu=a("li"),lee=a("strong"),W_o=o("bert"),Q_o=o(" \u2014 "),uP=a("a"),H_o=o("BertForMaskedLM"),U_o=o(" (BERT model)"),J_o=l(),Au=a("li"),iee=a("strong"),Y_o=o("big_bird"),K_o=o(" \u2014 "),bP=a("a"),Z_o=o("BigBirdForMaskedLM"),euo=o(" (BigBird model)"),ouo=l(),Lu=a("li"),dee=a("strong"),ruo=o("camembert"),tuo=o(" \u2014 "),vP=a("a"),auo=o("CamembertForMaskedLM"),nuo=o(" (CamemBERT model)"),suo=l(),Bu=a("li"),cee=a("strong"),luo=o("convbert"),iuo=o(" \u2014 "),TP=a("a"),duo=o("ConvBertForMaskedLM"),cuo=o(" (ConvBERT model)"),fuo=l(),ku=a("li"),fee=a("strong"),muo=o("deberta"),guo=o(" \u2014 "),FP=a("a"),huo=o("DebertaForMaskedLM"),puo=o(" (DeBERTa model)"),_uo=l(),xu=a("li"),mee=a("strong"),uuo=o("deberta-v2"),buo=o(" \u2014 "),CP=a("a"),vuo=o("DebertaV2ForMaskedLM"),Tuo=o(" (DeBERTa-v2 model)"),Fuo=l(),Ru=a("li"),gee=a("strong"),Cuo=o("distilbert"),Muo=o(" \u2014 "),MP=a("a"),Euo=o("DistilBertForMaskedLM"),yuo=o(" (DistilBERT model)"),wuo=l(),Su=a("li"),hee=a("strong"),Auo=o("electra"),Luo=o(" \u2014 "),EP=a("a"),Buo=o("ElectraForMaskedLM"),kuo=o(" (ELECTRA model)"),xuo=l(),Pu=a("li"),pee=a("strong"),Ruo=o("flaubert"),Suo=o(" \u2014 "),yP=a("a"),Puo=o("FlaubertWithLMHeadModel"),$uo=o(" (FlauBERT model)"),Iuo=l(),$u=a("li"),_ee=a("strong"),juo=o("fnet"),Nuo=o(" \u2014 "),wP=a("a"),Duo=o("FNetForMaskedLM"),quo=o(" (FNet model)"),Guo=l(),Iu=a("li"),uee=a("strong"),Ouo=o("funnel"),Xuo=o(" \u2014 "),AP=a("a"),zuo=o("FunnelForMaskedLM"),Vuo=o(" (Funnel Transformer model)"),Wuo=l(),ju=a("li"),bee=a("strong"),Quo=o("ibert"),Huo=o(" \u2014 "),LP=a("a"),Uuo=o("IBertForMaskedLM"),Juo=o(" (I-BERT model)"),Yuo=l(),Nu=a("li"),vee=a("strong"),Kuo=o("layoutlm"),Zuo=o(" \u2014 "),BP=a("a"),e1o=o("LayoutLMForMaskedLM"),o1o=o(" (LayoutLM model)"),r1o=l(),Du=a("li"),Tee=a("strong"),t1o=o("longformer"),a1o=o(" \u2014 "),kP=a("a"),n1o=o("LongformerForMaskedLM"),s1o=o(" (Longformer model)"),l1o=l(),qu=a("li"),Fee=a("strong"),i1o=o("mbart"),d1o=o(" \u2014 "),xP=a("a"),c1o=o("MBartForConditionalGeneration"),f1o=o(" (mBART model)"),m1o=l(),Gu=a("li"),Cee=a("strong"),g1o=o("megatron-bert"),h1o=o(" \u2014 "),RP=a("a"),p1o=o("MegatronBertForMaskedLM"),_1o=o(" (MegatronBert model)"),u1o=l(),Ou=a("li"),Mee=a("strong"),b1o=o("mobilebert"),v1o=o(" \u2014 "),SP=a("a"),T1o=o("MobileBertForMaskedLM"),F1o=o(" (MobileBERT model)"),C1o=l(),Xu=a("li"),Eee=a("strong"),M1o=o("mpnet"),E1o=o(" \u2014 "),PP=a("a"),y1o=o("MPNetForMaskedLM"),w1o=o(" (MPNet model)"),A1o=l(),zu=a("li"),yee=a("strong"),L1o=o("nystromformer"),B1o=o(" \u2014 "),$P=a("a"),k1o=o("NystromformerForMaskedLM"),x1o=o(" (Nystromformer model)"),R1o=l(),Vu=a("li"),wee=a("strong"),S1o=o("perceiver"),P1o=o(" \u2014 "),IP=a("a"),$1o=o("PerceiverForMaskedLM"),I1o=o(" (Perceiver model)"),j1o=l(),Wu=a("li"),Aee=a("strong"),N1o=o("qdqbert"),D1o=o(" \u2014 "),jP=a("a"),q1o=o("QDQBertForMaskedLM"),G1o=o(" (QDQBert model)"),O1o=l(),Qu=a("li"),Lee=a("strong"),X1o=o("reformer"),z1o=o(" \u2014 "),NP=a("a"),V1o=o("ReformerForMaskedLM"),W1o=o(" (Reformer model)"),Q1o=l(),Hu=a("li"),Bee=a("strong"),H1o=o("rembert"),U1o=o(" \u2014 "),DP=a("a"),J1o=o("RemBertForMaskedLM"),Y1o=o(" (RemBERT model)"),K1o=l(),Uu=a("li"),kee=a("strong"),Z1o=o("roberta"),e7o=o(" \u2014 "),qP=a("a"),o7o=o("RobertaForMaskedLM"),r7o=o(" (RoBERTa model)"),t7o=l(),Ju=a("li"),xee=a("strong"),a7o=o("roformer"),n7o=o(" \u2014 "),GP=a("a"),s7o=o("RoFormerForMaskedLM"),l7o=o(" (RoFormer model)"),i7o=l(),Yu=a("li"),Ree=a("strong"),d7o=o("squeezebert"),c7o=o(" \u2014 "),OP=a("a"),f7o=o("SqueezeBertForMaskedLM"),m7o=o(" (SqueezeBERT model)"),g7o=l(),Ku=a("li"),See=a("strong"),h7o=o("tapas"),p7o=o(" \u2014 "),XP=a("a"),_7o=o("TapasForMaskedLM"),u7o=o(" (TAPAS model)"),b7o=l(),Zu=a("li"),Pee=a("strong"),v7o=o("wav2vec2"),T7o=o(" \u2014 "),$ee=a("code"),F7o=o("Wav2Vec2ForMaskedLM"),C7o=o("(Wav2Vec2 model)"),M7o=l(),e1=a("li"),Iee=a("strong"),E7o=o("xlm"),y7o=o(" \u2014 "),zP=a("a"),w7o=o("XLMWithLMHeadModel"),A7o=o(" (XLM model)"),L7o=l(),o1=a("li"),jee=a("strong"),B7o=o("xlm-roberta"),k7o=o(" \u2014 "),VP=a("a"),x7o=o("XLMRobertaForMaskedLM"),R7o=o(" (XLM-RoBERTa model)"),S7o=l(),r1=a("li"),Nee=a("strong"),P7o=o("xlm-roberta-xl"),$7o=o(" \u2014 "),WP=a("a"),I7o=o("XLMRobertaXLForMaskedLM"),j7o=o(" (XLM-RoBERTa-XL model)"),N7o=l(),t1=a("li"),Dee=a("strong"),D7o=o("yoso"),q7o=o(" \u2014 "),QP=a("a"),G7o=o("YosoForMaskedLM"),O7o=o(" (YOSO model)"),X7o=l(),a1=a("p"),z7o=o("The model is set in evaluation mode by default using "),qee=a("code"),V7o=o("model.eval()"),W7o=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Gee=a("code"),Q7o=o("model.train()"),H7o=l(),Oee=a("p"),U7o=o("Examples:"),J7o=l(),f(mE.$$.fragment),q8e=l(),Ki=a("h2"),n1=a("a"),Xee=a("span"),f(gE.$$.fragment),Y7o=l(),zee=a("span"),K7o=o("AutoModelForSeq2SeqLM"),G8e=l(),Uo=a("div"),f(hE.$$.fragment),Z7o=l(),Zi=a("p"),ebo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence language modeling head) when created
with the `),Vee=a("code"),obo=o("from_pretrained()"),rbo=o("class method or the "),Wee=a("code"),tbo=o("from_config()"),abo=o(`class
method.`),nbo=l(),pE=a("p"),sbo=o("This class cannot be instantiated directly using "),Qee=a("code"),lbo=o("__init__()"),ibo=o(" (throws an error)."),dbo=l(),Or=a("div"),f(_E.$$.fragment),cbo=l(),Hee=a("p"),fbo=o("Instantiates one of the model classes of the library (with a sequence-to-sequence language modeling head) from a configuration."),mbo=l(),ed=a("p"),gbo=o(`Note:
Loading a model from its configuration file does `),Uee=a("strong"),hbo=o("not"),pbo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Jee=a("code"),_bo=o("from_pretrained()"),ubo=o("to load the model weights."),bbo=l(),Yee=a("p"),vbo=o("Examples:"),Tbo=l(),f(uE.$$.fragment),Fbo=l(),$e=a("div"),f(bE.$$.fragment),Cbo=l(),Kee=a("p"),Mbo=o("Instantiate one of the model classes of the library (with a sequence-to-sequence language modeling head) from a pretrained model."),Ebo=l(),Xa=a("p"),ybo=o("The model class to instantiate is selected based on the "),Zee=a("code"),wbo=o("model_type"),Abo=o(` property of the config object (either
passed as an argument or loaded from `),eoe=a("code"),Lbo=o("pretrained_model_name_or_path"),Bbo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),ooe=a("code"),kbo=o("pretrained_model_name_or_path"),xbo=o(":"),Rbo=l(),ne=a("ul"),s1=a("li"),roe=a("strong"),Sbo=o("bart"),Pbo=o(" \u2014 "),HP=a("a"),$bo=o("BartForConditionalGeneration"),Ibo=o(" (BART model)"),jbo=l(),l1=a("li"),toe=a("strong"),Nbo=o("bigbird_pegasus"),Dbo=o(" \u2014 "),UP=a("a"),qbo=o("BigBirdPegasusForConditionalGeneration"),Gbo=o(" (BigBirdPegasus model)"),Obo=l(),i1=a("li"),aoe=a("strong"),Xbo=o("blenderbot"),zbo=o(" \u2014 "),JP=a("a"),Vbo=o("BlenderbotForConditionalGeneration"),Wbo=o(" (Blenderbot model)"),Qbo=l(),d1=a("li"),noe=a("strong"),Hbo=o("blenderbot-small"),Ubo=o(" \u2014 "),YP=a("a"),Jbo=o("BlenderbotSmallForConditionalGeneration"),Ybo=o(" (BlenderbotSmall model)"),Kbo=l(),c1=a("li"),soe=a("strong"),Zbo=o("encoder-decoder"),e5o=o(" \u2014 "),KP=a("a"),o5o=o("EncoderDecoderModel"),r5o=o(" (Encoder decoder model)"),t5o=l(),f1=a("li"),loe=a("strong"),a5o=o("fsmt"),n5o=o(" \u2014 "),ZP=a("a"),s5o=o("FSMTForConditionalGeneration"),l5o=o(" (FairSeq Machine-Translation model)"),i5o=l(),m1=a("li"),ioe=a("strong"),d5o=o("led"),c5o=o(" \u2014 "),e$=a("a"),f5o=o("LEDForConditionalGeneration"),m5o=o(" (LED model)"),g5o=l(),g1=a("li"),doe=a("strong"),h5o=o("m2m_100"),p5o=o(" \u2014 "),o$=a("a"),_5o=o("M2M100ForConditionalGeneration"),u5o=o(" (M2M100 model)"),b5o=l(),h1=a("li"),coe=a("strong"),v5o=o("marian"),T5o=o(" \u2014 "),r$=a("a"),F5o=o("MarianMTModel"),C5o=o(" (Marian model)"),M5o=l(),p1=a("li"),foe=a("strong"),E5o=o("mbart"),y5o=o(" \u2014 "),t$=a("a"),w5o=o("MBartForConditionalGeneration"),A5o=o(" (mBART model)"),L5o=l(),_1=a("li"),moe=a("strong"),B5o=o("mt5"),k5o=o(" \u2014 "),a$=a("a"),x5o=o("MT5ForConditionalGeneration"),R5o=o(" (mT5 model)"),S5o=l(),u1=a("li"),goe=a("strong"),P5o=o("pegasus"),$5o=o(" \u2014 "),n$=a("a"),I5o=o("PegasusForConditionalGeneration"),j5o=o(" (Pegasus model)"),N5o=l(),b1=a("li"),hoe=a("strong"),D5o=o("plbart"),q5o=o(" \u2014 "),s$=a("a"),G5o=o("PLBartForConditionalGeneration"),O5o=o(" (PLBart model)"),X5o=l(),v1=a("li"),poe=a("strong"),z5o=o("prophetnet"),V5o=o(" \u2014 "),l$=a("a"),W5o=o("ProphetNetForConditionalGeneration"),Q5o=o(" (ProphetNet model)"),H5o=l(),T1=a("li"),_oe=a("strong"),U5o=o("t5"),J5o=o(" \u2014 "),i$=a("a"),Y5o=o("T5ForConditionalGeneration"),K5o=o(" (T5 model)"),Z5o=l(),F1=a("li"),uoe=a("strong"),e2o=o("xlm-prophetnet"),o2o=o(" \u2014 "),d$=a("a"),r2o=o("XLMProphetNetForConditionalGeneration"),t2o=o(" (XLMProphetNet model)"),a2o=l(),C1=a("p"),n2o=o("The model is set in evaluation mode by default using "),boe=a("code"),s2o=o("model.eval()"),l2o=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),voe=a("code"),i2o=o("model.train()"),d2o=l(),Toe=a("p"),c2o=o("Examples:"),f2o=l(),f(vE.$$.fragment),O8e=l(),od=a("h2"),M1=a("a"),Foe=a("span"),f(TE.$$.fragment),m2o=l(),Coe=a("span"),g2o=o("AutoModelForSequenceClassification"),X8e=l(),Jo=a("div"),f(FE.$$.fragment),h2o=l(),rd=a("p"),p2o=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence classification head) when created
with the `),Moe=a("code"),_2o=o("from_pretrained()"),u2o=o("class method or the "),Eoe=a("code"),b2o=o("from_config()"),v2o=o(`class
method.`),T2o=l(),CE=a("p"),F2o=o("This class cannot be instantiated directly using "),yoe=a("code"),C2o=o("__init__()"),M2o=o(" (throws an error)."),E2o=l(),Xr=a("div"),f(ME.$$.fragment),y2o=l(),woe=a("p"),w2o=o("Instantiates one of the model classes of the library (with a sequence classification head) from a configuration."),A2o=l(),td=a("p"),L2o=o(`Note:
Loading a model from its configuration file does `),Aoe=a("strong"),B2o=o("not"),k2o=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Loe=a("code"),x2o=o("from_pretrained()"),R2o=o("to load the model weights."),S2o=l(),Boe=a("p"),P2o=o("Examples:"),$2o=l(),f(EE.$$.fragment),I2o=l(),Ie=a("div"),f(yE.$$.fragment),j2o=l(),koe=a("p"),N2o=o("Instantiate one of the model classes of the library (with a sequence classification head) from a pretrained model."),D2o=l(),za=a("p"),q2o=o("The model class to instantiate is selected based on the "),xoe=a("code"),G2o=o("model_type"),O2o=o(` property of the config object (either
passed as an argument or loaded from `),Roe=a("code"),X2o=o("pretrained_model_name_or_path"),z2o=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Soe=a("code"),V2o=o("pretrained_model_name_or_path"),W2o=o(":"),Q2o=l(),A=a("ul"),E1=a("li"),Poe=a("strong"),H2o=o("albert"),U2o=o(" \u2014 "),c$=a("a"),J2o=o("AlbertForSequenceClassification"),Y2o=o(" (ALBERT model)"),K2o=l(),y1=a("li"),$oe=a("strong"),Z2o=o("bart"),evo=o(" \u2014 "),f$=a("a"),ovo=o("BartForSequenceClassification"),rvo=o(" (BART model)"),tvo=l(),w1=a("li"),Ioe=a("strong"),avo=o("bert"),nvo=o(" \u2014 "),m$=a("a"),svo=o("BertForSequenceClassification"),lvo=o(" (BERT model)"),ivo=l(),A1=a("li"),joe=a("strong"),dvo=o("big_bird"),cvo=o(" \u2014 "),g$=a("a"),fvo=o("BigBirdForSequenceClassification"),mvo=o(" (BigBird model)"),gvo=l(),L1=a("li"),Noe=a("strong"),hvo=o("bigbird_pegasus"),pvo=o(" \u2014 "),h$=a("a"),_vo=o("BigBirdPegasusForSequenceClassification"),uvo=o(" (BigBirdPegasus model)"),bvo=l(),B1=a("li"),Doe=a("strong"),vvo=o("camembert"),Tvo=o(" \u2014 "),p$=a("a"),Fvo=o("CamembertForSequenceClassification"),Cvo=o(" (CamemBERT model)"),Mvo=l(),k1=a("li"),qoe=a("strong"),Evo=o("canine"),yvo=o(" \u2014 "),_$=a("a"),wvo=o("CanineForSequenceClassification"),Avo=o(" (Canine model)"),Lvo=l(),x1=a("li"),Goe=a("strong"),Bvo=o("convbert"),kvo=o(" \u2014 "),u$=a("a"),xvo=o("ConvBertForSequenceClassification"),Rvo=o(" (ConvBERT model)"),Svo=l(),R1=a("li"),Ooe=a("strong"),Pvo=o("ctrl"),$vo=o(" \u2014 "),b$=a("a"),Ivo=o("CTRLForSequenceClassification"),jvo=o(" (CTRL model)"),Nvo=l(),S1=a("li"),Xoe=a("strong"),Dvo=o("deberta"),qvo=o(" \u2014 "),v$=a("a"),Gvo=o("DebertaForSequenceClassification"),Ovo=o(" (DeBERTa model)"),Xvo=l(),P1=a("li"),zoe=a("strong"),zvo=o("deberta-v2"),Vvo=o(" \u2014 "),T$=a("a"),Wvo=o("DebertaV2ForSequenceClassification"),Qvo=o(" (DeBERTa-v2 model)"),Hvo=l(),$1=a("li"),Voe=a("strong"),Uvo=o("distilbert"),Jvo=o(" \u2014 "),F$=a("a"),Yvo=o("DistilBertForSequenceClassification"),Kvo=o(" (DistilBERT model)"),Zvo=l(),I1=a("li"),Woe=a("strong"),e0o=o("electra"),o0o=o(" \u2014 "),C$=a("a"),r0o=o("ElectraForSequenceClassification"),t0o=o(" (ELECTRA model)"),a0o=l(),j1=a("li"),Qoe=a("strong"),n0o=o("flaubert"),s0o=o(" \u2014 "),M$=a("a"),l0o=o("FlaubertForSequenceClassification"),i0o=o(" (FlauBERT model)"),d0o=l(),N1=a("li"),Hoe=a("strong"),c0o=o("fnet"),f0o=o(" \u2014 "),E$=a("a"),m0o=o("FNetForSequenceClassification"),g0o=o(" (FNet model)"),h0o=l(),D1=a("li"),Uoe=a("strong"),p0o=o("funnel"),_0o=o(" \u2014 "),y$=a("a"),u0o=o("FunnelForSequenceClassification"),b0o=o(" (Funnel Transformer model)"),v0o=l(),q1=a("li"),Joe=a("strong"),T0o=o("gpt2"),F0o=o(" \u2014 "),w$=a("a"),C0o=o("GPT2ForSequenceClassification"),M0o=o(" (OpenAI GPT-2 model)"),E0o=l(),G1=a("li"),Yoe=a("strong"),y0o=o("gpt_neo"),w0o=o(" \u2014 "),A$=a("a"),A0o=o("GPTNeoForSequenceClassification"),L0o=o(" (GPT Neo model)"),B0o=l(),O1=a("li"),Koe=a("strong"),k0o=o("gptj"),x0o=o(" \u2014 "),L$=a("a"),R0o=o("GPTJForSequenceClassification"),S0o=o(" (GPT-J model)"),P0o=l(),X1=a("li"),Zoe=a("strong"),$0o=o("ibert"),I0o=o(" \u2014 "),B$=a("a"),j0o=o("IBertForSequenceClassification"),N0o=o(" (I-BERT model)"),D0o=l(),z1=a("li"),ere=a("strong"),q0o=o("layoutlm"),G0o=o(" \u2014 "),k$=a("a"),O0o=o("LayoutLMForSequenceClassification"),X0o=o(" (LayoutLM model)"),z0o=l(),V1=a("li"),ore=a("strong"),V0o=o("layoutlmv2"),W0o=o(" \u2014 "),x$=a("a"),Q0o=o("LayoutLMv2ForSequenceClassification"),H0o=o(" (LayoutLMv2 model)"),U0o=l(),W1=a("li"),rre=a("strong"),J0o=o("led"),Y0o=o(" \u2014 "),R$=a("a"),K0o=o("LEDForSequenceClassification"),Z0o=o(" (LED model)"),eTo=l(),Q1=a("li"),tre=a("strong"),oTo=o("longformer"),rTo=o(" \u2014 "),S$=a("a"),tTo=o("LongformerForSequenceClassification"),aTo=o(" (Longformer model)"),nTo=l(),H1=a("li"),are=a("strong"),sTo=o("mbart"),lTo=o(" \u2014 "),P$=a("a"),iTo=o("MBartForSequenceClassification"),dTo=o(" (mBART model)"),cTo=l(),U1=a("li"),nre=a("strong"),fTo=o("megatron-bert"),mTo=o(" \u2014 "),$$=a("a"),gTo=o("MegatronBertForSequenceClassification"),hTo=o(" (MegatronBert model)"),pTo=l(),J1=a("li"),sre=a("strong"),_To=o("mobilebert"),uTo=o(" \u2014 "),I$=a("a"),bTo=o("MobileBertForSequenceClassification"),vTo=o(" (MobileBERT model)"),TTo=l(),Y1=a("li"),lre=a("strong"),FTo=o("mpnet"),CTo=o(" \u2014 "),j$=a("a"),MTo=o("MPNetForSequenceClassification"),ETo=o(" (MPNet model)"),yTo=l(),K1=a("li"),ire=a("strong"),wTo=o("nystromformer"),ATo=o(" \u2014 "),N$=a("a"),LTo=o("NystromformerForSequenceClassification"),BTo=o(" (Nystromformer model)"),kTo=l(),Z1=a("li"),dre=a("strong"),xTo=o("openai-gpt"),RTo=o(" \u2014 "),D$=a("a"),STo=o("OpenAIGPTForSequenceClassification"),PTo=o(" (OpenAI GPT model)"),$To=l(),e7=a("li"),cre=a("strong"),ITo=o("perceiver"),jTo=o(" \u2014 "),q$=a("a"),NTo=o("PerceiverForSequenceClassification"),DTo=o(" (Perceiver model)"),qTo=l(),o7=a("li"),fre=a("strong"),GTo=o("plbart"),OTo=o(" \u2014 "),G$=a("a"),XTo=o("PLBartForSequenceClassification"),zTo=o(" (PLBart model)"),VTo=l(),r7=a("li"),mre=a("strong"),WTo=o("qdqbert"),QTo=o(" \u2014 "),O$=a("a"),HTo=o("QDQBertForSequenceClassification"),UTo=o(" (QDQBert model)"),JTo=l(),t7=a("li"),gre=a("strong"),YTo=o("reformer"),KTo=o(" \u2014 "),X$=a("a"),ZTo=o("ReformerForSequenceClassification"),eFo=o(" (Reformer model)"),oFo=l(),a7=a("li"),hre=a("strong"),rFo=o("rembert"),tFo=o(" \u2014 "),z$=a("a"),aFo=o("RemBertForSequenceClassification"),nFo=o(" (RemBERT model)"),sFo=l(),n7=a("li"),pre=a("strong"),lFo=o("roberta"),iFo=o(" \u2014 "),V$=a("a"),dFo=o("RobertaForSequenceClassification"),cFo=o(" (RoBERTa model)"),fFo=l(),s7=a("li"),_re=a("strong"),mFo=o("roformer"),gFo=o(" \u2014 "),W$=a("a"),hFo=o("RoFormerForSequenceClassification"),pFo=o(" (RoFormer model)"),_Fo=l(),l7=a("li"),ure=a("strong"),uFo=o("squeezebert"),bFo=o(" \u2014 "),Q$=a("a"),vFo=o("SqueezeBertForSequenceClassification"),TFo=o(" (SqueezeBERT model)"),FFo=l(),i7=a("li"),bre=a("strong"),CFo=o("tapas"),MFo=o(" \u2014 "),H$=a("a"),EFo=o("TapasForSequenceClassification"),yFo=o(" (TAPAS model)"),wFo=l(),d7=a("li"),vre=a("strong"),AFo=o("transfo-xl"),LFo=o(" \u2014 "),U$=a("a"),BFo=o("TransfoXLForSequenceClassification"),kFo=o(" (Transformer-XL model)"),xFo=l(),c7=a("li"),Tre=a("strong"),RFo=o("xlm"),SFo=o(" \u2014 "),J$=a("a"),PFo=o("XLMForSequenceClassification"),$Fo=o(" (XLM model)"),IFo=l(),f7=a("li"),Fre=a("strong"),jFo=o("xlm-roberta"),NFo=o(" \u2014 "),Y$=a("a"),DFo=o("XLMRobertaForSequenceClassification"),qFo=o(" (XLM-RoBERTa model)"),GFo=l(),m7=a("li"),Cre=a("strong"),OFo=o("xlm-roberta-xl"),XFo=o(" \u2014 "),K$=a("a"),zFo=o("XLMRobertaXLForSequenceClassification"),VFo=o(" (XLM-RoBERTa-XL model)"),WFo=l(),g7=a("li"),Mre=a("strong"),QFo=o("xlnet"),HFo=o(" \u2014 "),Z$=a("a"),UFo=o("XLNetForSequenceClassification"),JFo=o(" (XLNet model)"),YFo=l(),h7=a("li"),Ere=a("strong"),KFo=o("yoso"),ZFo=o(" \u2014 "),eI=a("a"),eCo=o("YosoForSequenceClassification"),oCo=o(" (YOSO model)"),rCo=l(),p7=a("p"),tCo=o("The model is set in evaluation mode by default using "),yre=a("code"),aCo=o("model.eval()"),nCo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),wre=a("code"),sCo=o("model.train()"),lCo=l(),Are=a("p"),iCo=o("Examples:"),dCo=l(),f(wE.$$.fragment),z8e=l(),ad=a("h2"),_7=a("a"),Lre=a("span"),f(AE.$$.fragment),cCo=l(),Bre=a("span"),fCo=o("AutoModelForMultipleChoice"),V8e=l(),Yo=a("div"),f(LE.$$.fragment),mCo=l(),nd=a("p"),gCo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a multiple choice head) when created
with the `),kre=a("code"),hCo=o("from_pretrained()"),pCo=o("class method or the "),xre=a("code"),_Co=o("from_config()"),uCo=o(`class
method.`),bCo=l(),BE=a("p"),vCo=o("This class cannot be instantiated directly using "),Rre=a("code"),TCo=o("__init__()"),FCo=o(" (throws an error)."),CCo=l(),zr=a("div"),f(kE.$$.fragment),MCo=l(),Sre=a("p"),ECo=o("Instantiates one of the model classes of the library (with a multiple choice head) from a configuration."),yCo=l(),sd=a("p"),wCo=o(`Note:
Loading a model from its configuration file does `),Pre=a("strong"),ACo=o("not"),LCo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),$re=a("code"),BCo=o("from_pretrained()"),kCo=o("to load the model weights."),xCo=l(),Ire=a("p"),RCo=o("Examples:"),SCo=l(),f(xE.$$.fragment),PCo=l(),je=a("div"),f(RE.$$.fragment),$Co=l(),jre=a("p"),ICo=o("Instantiate one of the model classes of the library (with a multiple choice head) from a pretrained model."),jCo=l(),Va=a("p"),NCo=o("The model class to instantiate is selected based on the "),Nre=a("code"),DCo=o("model_type"),qCo=o(` property of the config object (either
passed as an argument or loaded from `),Dre=a("code"),GCo=o("pretrained_model_name_or_path"),OCo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),qre=a("code"),XCo=o("pretrained_model_name_or_path"),zCo=o(":"),VCo=l(),G=a("ul"),u7=a("li"),Gre=a("strong"),WCo=o("albert"),QCo=o(" \u2014 "),oI=a("a"),HCo=o("AlbertForMultipleChoice"),UCo=o(" (ALBERT model)"),JCo=l(),b7=a("li"),Ore=a("strong"),YCo=o("bert"),KCo=o(" \u2014 "),rI=a("a"),ZCo=o("BertForMultipleChoice"),e4o=o(" (BERT model)"),o4o=l(),v7=a("li"),Xre=a("strong"),r4o=o("big_bird"),t4o=o(" \u2014 "),tI=a("a"),a4o=o("BigBirdForMultipleChoice"),n4o=o(" (BigBird model)"),s4o=l(),T7=a("li"),zre=a("strong"),l4o=o("camembert"),i4o=o(" \u2014 "),aI=a("a"),d4o=o("CamembertForMultipleChoice"),c4o=o(" (CamemBERT model)"),f4o=l(),F7=a("li"),Vre=a("strong"),m4o=o("canine"),g4o=o(" \u2014 "),nI=a("a"),h4o=o("CanineForMultipleChoice"),p4o=o(" (Canine model)"),_4o=l(),C7=a("li"),Wre=a("strong"),u4o=o("convbert"),b4o=o(" \u2014 "),sI=a("a"),v4o=o("ConvBertForMultipleChoice"),T4o=o(" (ConvBERT model)"),F4o=l(),M7=a("li"),Qre=a("strong"),C4o=o("distilbert"),M4o=o(" \u2014 "),lI=a("a"),E4o=o("DistilBertForMultipleChoice"),y4o=o(" (DistilBERT model)"),w4o=l(),E7=a("li"),Hre=a("strong"),A4o=o("electra"),L4o=o(" \u2014 "),iI=a("a"),B4o=o("ElectraForMultipleChoice"),k4o=o(" (ELECTRA model)"),x4o=l(),y7=a("li"),Ure=a("strong"),R4o=o("flaubert"),S4o=o(" \u2014 "),dI=a("a"),P4o=o("FlaubertForMultipleChoice"),$4o=o(" (FlauBERT model)"),I4o=l(),w7=a("li"),Jre=a("strong"),j4o=o("fnet"),N4o=o(" \u2014 "),cI=a("a"),D4o=o("FNetForMultipleChoice"),q4o=o(" (FNet model)"),G4o=l(),A7=a("li"),Yre=a("strong"),O4o=o("funnel"),X4o=o(" \u2014 "),fI=a("a"),z4o=o("FunnelForMultipleChoice"),V4o=o(" (Funnel Transformer model)"),W4o=l(),L7=a("li"),Kre=a("strong"),Q4o=o("ibert"),H4o=o(" \u2014 "),mI=a("a"),U4o=o("IBertForMultipleChoice"),J4o=o(" (I-BERT model)"),Y4o=l(),B7=a("li"),Zre=a("strong"),K4o=o("longformer"),Z4o=o(" \u2014 "),gI=a("a"),eMo=o("LongformerForMultipleChoice"),oMo=o(" (Longformer model)"),rMo=l(),k7=a("li"),ete=a("strong"),tMo=o("megatron-bert"),aMo=o(" \u2014 "),hI=a("a"),nMo=o("MegatronBertForMultipleChoice"),sMo=o(" (MegatronBert model)"),lMo=l(),x7=a("li"),ote=a("strong"),iMo=o("mobilebert"),dMo=o(" \u2014 "),pI=a("a"),cMo=o("MobileBertForMultipleChoice"),fMo=o(" (MobileBERT model)"),mMo=l(),R7=a("li"),rte=a("strong"),gMo=o("mpnet"),hMo=o(" \u2014 "),_I=a("a"),pMo=o("MPNetForMultipleChoice"),_Mo=o(" (MPNet model)"),uMo=l(),S7=a("li"),tte=a("strong"),bMo=o("nystromformer"),vMo=o(" \u2014 "),uI=a("a"),TMo=o("NystromformerForMultipleChoice"),FMo=o(" (Nystromformer model)"),CMo=l(),P7=a("li"),ate=a("strong"),MMo=o("qdqbert"),EMo=o(" \u2014 "),bI=a("a"),yMo=o("QDQBertForMultipleChoice"),wMo=o(" (QDQBert model)"),AMo=l(),$7=a("li"),nte=a("strong"),LMo=o("rembert"),BMo=o(" \u2014 "),vI=a("a"),kMo=o("RemBertForMultipleChoice"),xMo=o(" (RemBERT model)"),RMo=l(),I7=a("li"),ste=a("strong"),SMo=o("roberta"),PMo=o(" \u2014 "),TI=a("a"),$Mo=o("RobertaForMultipleChoice"),IMo=o(" (RoBERTa model)"),jMo=l(),j7=a("li"),lte=a("strong"),NMo=o("roformer"),DMo=o(" \u2014 "),FI=a("a"),qMo=o("RoFormerForMultipleChoice"),GMo=o(" (RoFormer model)"),OMo=l(),N7=a("li"),ite=a("strong"),XMo=o("squeezebert"),zMo=o(" \u2014 "),CI=a("a"),VMo=o("SqueezeBertForMultipleChoice"),WMo=o(" (SqueezeBERT model)"),QMo=l(),D7=a("li"),dte=a("strong"),HMo=o("xlm"),UMo=o(" \u2014 "),MI=a("a"),JMo=o("XLMForMultipleChoice"),YMo=o(" (XLM model)"),KMo=l(),q7=a("li"),cte=a("strong"),ZMo=o("xlm-roberta"),eEo=o(" \u2014 "),EI=a("a"),oEo=o("XLMRobertaForMultipleChoice"),rEo=o(" (XLM-RoBERTa model)"),tEo=l(),G7=a("li"),fte=a("strong"),aEo=o("xlm-roberta-xl"),nEo=o(" \u2014 "),yI=a("a"),sEo=o("XLMRobertaXLForMultipleChoice"),lEo=o(" (XLM-RoBERTa-XL model)"),iEo=l(),O7=a("li"),mte=a("strong"),dEo=o("xlnet"),cEo=o(" \u2014 "),wI=a("a"),fEo=o("XLNetForMultipleChoice"),mEo=o(" (XLNet model)"),gEo=l(),X7=a("li"),gte=a("strong"),hEo=o("yoso"),pEo=o(" \u2014 "),AI=a("a"),_Eo=o("YosoForMultipleChoice"),uEo=o(" (YOSO model)"),bEo=l(),z7=a("p"),vEo=o("The model is set in evaluation mode by default using "),hte=a("code"),TEo=o("model.eval()"),FEo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),pte=a("code"),CEo=o("model.train()"),MEo=l(),_te=a("p"),EEo=o("Examples:"),yEo=l(),f(SE.$$.fragment),W8e=l(),ld=a("h2"),V7=a("a"),ute=a("span"),f(PE.$$.fragment),wEo=l(),bte=a("span"),AEo=o("AutoModelForNextSentencePrediction"),Q8e=l(),Ko=a("div"),f($E.$$.fragment),LEo=l(),id=a("p"),BEo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a next sentence prediction head) when created
with the `),vte=a("code"),kEo=o("from_pretrained()"),xEo=o("class method or the "),Tte=a("code"),REo=o("from_config()"),SEo=o(`class
method.`),PEo=l(),IE=a("p"),$Eo=o("This class cannot be instantiated directly using "),Fte=a("code"),IEo=o("__init__()"),jEo=o(" (throws an error)."),NEo=l(),Vr=a("div"),f(jE.$$.fragment),DEo=l(),Cte=a("p"),qEo=o("Instantiates one of the model classes of the library (with a next sentence prediction head) from a configuration."),GEo=l(),dd=a("p"),OEo=o(`Note:
Loading a model from its configuration file does `),Mte=a("strong"),XEo=o("not"),zEo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Ete=a("code"),VEo=o("from_pretrained()"),WEo=o("to load the model weights."),QEo=l(),yte=a("p"),HEo=o("Examples:"),UEo=l(),f(NE.$$.fragment),JEo=l(),Ne=a("div"),f(DE.$$.fragment),YEo=l(),wte=a("p"),KEo=o("Instantiate one of the model classes of the library (with a next sentence prediction head) from a pretrained model."),ZEo=l(),Wa=a("p"),e3o=o("The model class to instantiate is selected based on the "),Ate=a("code"),o3o=o("model_type"),r3o=o(` property of the config object (either
passed as an argument or loaded from `),Lte=a("code"),t3o=o("pretrained_model_name_or_path"),a3o=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Bte=a("code"),n3o=o("pretrained_model_name_or_path"),s3o=o(":"),l3o=l(),na=a("ul"),W7=a("li"),kte=a("strong"),i3o=o("bert"),d3o=o(" \u2014 "),LI=a("a"),c3o=o("BertForNextSentencePrediction"),f3o=o(" (BERT model)"),m3o=l(),Q7=a("li"),xte=a("strong"),g3o=o("fnet"),h3o=o(" \u2014 "),BI=a("a"),p3o=o("FNetForNextSentencePrediction"),_3o=o(" (FNet model)"),u3o=l(),H7=a("li"),Rte=a("strong"),b3o=o("megatron-bert"),v3o=o(" \u2014 "),kI=a("a"),T3o=o("MegatronBertForNextSentencePrediction"),F3o=o(" (MegatronBert model)"),C3o=l(),U7=a("li"),Ste=a("strong"),M3o=o("mobilebert"),E3o=o(" \u2014 "),xI=a("a"),y3o=o("MobileBertForNextSentencePrediction"),w3o=o(" (MobileBERT model)"),A3o=l(),J7=a("li"),Pte=a("strong"),L3o=o("qdqbert"),B3o=o(" \u2014 "),RI=a("a"),k3o=o("QDQBertForNextSentencePrediction"),x3o=o(" (QDQBert model)"),R3o=l(),Y7=a("p"),S3o=o("The model is set in evaluation mode by default using "),$te=a("code"),P3o=o("model.eval()"),$3o=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Ite=a("code"),I3o=o("model.train()"),j3o=l(),jte=a("p"),N3o=o("Examples:"),D3o=l(),f(qE.$$.fragment),H8e=l(),cd=a("h2"),K7=a("a"),Nte=a("span"),f(GE.$$.fragment),q3o=l(),Dte=a("span"),G3o=o("AutoModelForTokenClassification"),U8e=l(),Zo=a("div"),f(OE.$$.fragment),O3o=l(),fd=a("p"),X3o=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a token classification head) when created
with the `),qte=a("code"),z3o=o("from_pretrained()"),V3o=o("class method or the "),Gte=a("code"),W3o=o("from_config()"),Q3o=o(`class
method.`),H3o=l(),XE=a("p"),U3o=o("This class cannot be instantiated directly using "),Ote=a("code"),J3o=o("__init__()"),Y3o=o(" (throws an error)."),K3o=l(),Wr=a("div"),f(zE.$$.fragment),Z3o=l(),Xte=a("p"),eyo=o("Instantiates one of the model classes of the library (with a token classification head) from a configuration."),oyo=l(),md=a("p"),ryo=o(`Note:
Loading a model from its configuration file does `),zte=a("strong"),tyo=o("not"),ayo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Vte=a("code"),nyo=o("from_pretrained()"),syo=o("to load the model weights."),lyo=l(),Wte=a("p"),iyo=o("Examples:"),dyo=l(),f(VE.$$.fragment),cyo=l(),De=a("div"),f(WE.$$.fragment),fyo=l(),Qte=a("p"),myo=o("Instantiate one of the model classes of the library (with a token classification head) from a pretrained model."),gyo=l(),Qa=a("p"),hyo=o("The model class to instantiate is selected based on the "),Hte=a("code"),pyo=o("model_type"),_yo=o(` property of the config object (either
passed as an argument or loaded from `),Ute=a("code"),uyo=o("pretrained_model_name_or_path"),byo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Jte=a("code"),vyo=o("pretrained_model_name_or_path"),Tyo=o(":"),Fyo=l(),D=a("ul"),Z7=a("li"),Yte=a("strong"),Cyo=o("albert"),Myo=o(" \u2014 "),SI=a("a"),Eyo=o("AlbertForTokenClassification"),yyo=o(" (ALBERT model)"),wyo=l(),eb=a("li"),Kte=a("strong"),Ayo=o("bert"),Lyo=o(" \u2014 "),PI=a("a"),Byo=o("BertForTokenClassification"),kyo=o(" (BERT model)"),xyo=l(),ob=a("li"),Zte=a("strong"),Ryo=o("big_bird"),Syo=o(" \u2014 "),$I=a("a"),Pyo=o("BigBirdForTokenClassification"),$yo=o(" (BigBird model)"),Iyo=l(),rb=a("li"),eae=a("strong"),jyo=o("camembert"),Nyo=o(" \u2014 "),II=a("a"),Dyo=o("CamembertForTokenClassification"),qyo=o(" (CamemBERT model)"),Gyo=l(),tb=a("li"),oae=a("strong"),Oyo=o("canine"),Xyo=o(" \u2014 "),jI=a("a"),zyo=o("CanineForTokenClassification"),Vyo=o(" (Canine model)"),Wyo=l(),ab=a("li"),rae=a("strong"),Qyo=o("convbert"),Hyo=o(" \u2014 "),NI=a("a"),Uyo=o("ConvBertForTokenClassification"),Jyo=o(" (ConvBERT model)"),Yyo=l(),nb=a("li"),tae=a("strong"),Kyo=o("deberta"),Zyo=o(" \u2014 "),DI=a("a"),ewo=o("DebertaForTokenClassification"),owo=o(" (DeBERTa model)"),rwo=l(),sb=a("li"),aae=a("strong"),two=o("deberta-v2"),awo=o(" \u2014 "),qI=a("a"),nwo=o("DebertaV2ForTokenClassification"),swo=o(" (DeBERTa-v2 model)"),lwo=l(),lb=a("li"),nae=a("strong"),iwo=o("distilbert"),dwo=o(" \u2014 "),GI=a("a"),cwo=o("DistilBertForTokenClassification"),fwo=o(" (DistilBERT model)"),mwo=l(),ib=a("li"),sae=a("strong"),gwo=o("electra"),hwo=o(" \u2014 "),OI=a("a"),pwo=o("ElectraForTokenClassification"),_wo=o(" (ELECTRA model)"),uwo=l(),db=a("li"),lae=a("strong"),bwo=o("flaubert"),vwo=o(" \u2014 "),XI=a("a"),Two=o("FlaubertForTokenClassification"),Fwo=o(" (FlauBERT model)"),Cwo=l(),cb=a("li"),iae=a("strong"),Mwo=o("fnet"),Ewo=o(" \u2014 "),zI=a("a"),ywo=o("FNetForTokenClassification"),wwo=o(" (FNet model)"),Awo=l(),fb=a("li"),dae=a("strong"),Lwo=o("funnel"),Bwo=o(" \u2014 "),VI=a("a"),kwo=o("FunnelForTokenClassification"),xwo=o(" (Funnel Transformer model)"),Rwo=l(),mb=a("li"),cae=a("strong"),Swo=o("gpt2"),Pwo=o(" \u2014 "),WI=a("a"),$wo=o("GPT2ForTokenClassification"),Iwo=o(" (OpenAI GPT-2 model)"),jwo=l(),gb=a("li"),fae=a("strong"),Nwo=o("ibert"),Dwo=o(" \u2014 "),QI=a("a"),qwo=o("IBertForTokenClassification"),Gwo=o(" (I-BERT model)"),Owo=l(),hb=a("li"),mae=a("strong"),Xwo=o("layoutlm"),zwo=o(" \u2014 "),HI=a("a"),Vwo=o("LayoutLMForTokenClassification"),Wwo=o(" (LayoutLM model)"),Qwo=l(),pb=a("li"),gae=a("strong"),Hwo=o("layoutlmv2"),Uwo=o(" \u2014 "),UI=a("a"),Jwo=o("LayoutLMv2ForTokenClassification"),Ywo=o(" (LayoutLMv2 model)"),Kwo=l(),_b=a("li"),hae=a("strong"),Zwo=o("longformer"),e6o=o(" \u2014 "),JI=a("a"),o6o=o("LongformerForTokenClassification"),r6o=o(" (Longformer model)"),t6o=l(),ub=a("li"),pae=a("strong"),a6o=o("megatron-bert"),n6o=o(" \u2014 "),YI=a("a"),s6o=o("MegatronBertForTokenClassification"),l6o=o(" (MegatronBert model)"),i6o=l(),bb=a("li"),_ae=a("strong"),d6o=o("mobilebert"),c6o=o(" \u2014 "),KI=a("a"),f6o=o("MobileBertForTokenClassification"),m6o=o(" (MobileBERT model)"),g6o=l(),vb=a("li"),uae=a("strong"),h6o=o("mpnet"),p6o=o(" \u2014 "),ZI=a("a"),_6o=o("MPNetForTokenClassification"),u6o=o(" (MPNet model)"),b6o=l(),Tb=a("li"),bae=a("strong"),v6o=o("nystromformer"),T6o=o(" \u2014 "),ej=a("a"),F6o=o("NystromformerForTokenClassification"),C6o=o(" (Nystromformer model)"),M6o=l(),Fb=a("li"),vae=a("strong"),E6o=o("qdqbert"),y6o=o(" \u2014 "),oj=a("a"),w6o=o("QDQBertForTokenClassification"),A6o=o(" (QDQBert model)"),L6o=l(),Cb=a("li"),Tae=a("strong"),B6o=o("rembert"),k6o=o(" \u2014 "),rj=a("a"),x6o=o("RemBertForTokenClassification"),R6o=o(" (RemBERT model)"),S6o=l(),Mb=a("li"),Fae=a("strong"),P6o=o("roberta"),$6o=o(" \u2014 "),tj=a("a"),I6o=o("RobertaForTokenClassification"),j6o=o(" (RoBERTa model)"),N6o=l(),Eb=a("li"),Cae=a("strong"),D6o=o("roformer"),q6o=o(" \u2014 "),aj=a("a"),G6o=o("RoFormerForTokenClassification"),O6o=o(" (RoFormer model)"),X6o=l(),yb=a("li"),Mae=a("strong"),z6o=o("squeezebert"),V6o=o(" \u2014 "),nj=a("a"),W6o=o("SqueezeBertForTokenClassification"),Q6o=o(" (SqueezeBERT model)"),H6o=l(),wb=a("li"),Eae=a("strong"),U6o=o("xlm"),J6o=o(" \u2014 "),sj=a("a"),Y6o=o("XLMForTokenClassification"),K6o=o(" (XLM model)"),Z6o=l(),Ab=a("li"),yae=a("strong"),eAo=o("xlm-roberta"),oAo=o(" \u2014 "),lj=a("a"),rAo=o("XLMRobertaForTokenClassification"),tAo=o(" (XLM-RoBERTa model)"),aAo=l(),Lb=a("li"),wae=a("strong"),nAo=o("xlm-roberta-xl"),sAo=o(" \u2014 "),ij=a("a"),lAo=o("XLMRobertaXLForTokenClassification"),iAo=o(" (XLM-RoBERTa-XL model)"),dAo=l(),Bb=a("li"),Aae=a("strong"),cAo=o("xlnet"),fAo=o(" \u2014 "),dj=a("a"),mAo=o("XLNetForTokenClassification"),gAo=o(" (XLNet model)"),hAo=l(),kb=a("li"),Lae=a("strong"),pAo=o("yoso"),_Ao=o(" \u2014 "),cj=a("a"),uAo=o("YosoForTokenClassification"),bAo=o(" (YOSO model)"),vAo=l(),xb=a("p"),TAo=o("The model is set in evaluation mode by default using "),Bae=a("code"),FAo=o("model.eval()"),CAo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),kae=a("code"),MAo=o("model.train()"),EAo=l(),xae=a("p"),yAo=o("Examples:"),wAo=l(),f(QE.$$.fragment),J8e=l(),gd=a("h2"),Rb=a("a"),Rae=a("span"),f(HE.$$.fragment),AAo=l(),Sae=a("span"),LAo=o("AutoModelForQuestionAnswering"),Y8e=l(),er=a("div"),f(UE.$$.fragment),BAo=l(),hd=a("p"),kAo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a question answering head) when created
with the `),Pae=a("code"),xAo=o("from_pretrained()"),RAo=o("class method or the "),$ae=a("code"),SAo=o("from_config()"),PAo=o(`class
method.`),$Ao=l(),JE=a("p"),IAo=o("This class cannot be instantiated directly using "),Iae=a("code"),jAo=o("__init__()"),NAo=o(" (throws an error)."),DAo=l(),Qr=a("div"),f(YE.$$.fragment),qAo=l(),jae=a("p"),GAo=o("Instantiates one of the model classes of the library (with a question answering head) from a configuration."),OAo=l(),pd=a("p"),XAo=o(`Note:
Loading a model from its configuration file does `),Nae=a("strong"),zAo=o("not"),VAo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Dae=a("code"),WAo=o("from_pretrained()"),QAo=o("to load the model weights."),HAo=l(),qae=a("p"),UAo=o("Examples:"),JAo=l(),f(KE.$$.fragment),YAo=l(),qe=a("div"),f(ZE.$$.fragment),KAo=l(),Gae=a("p"),ZAo=o("Instantiate one of the model classes of the library (with a question answering head) from a pretrained model."),eLo=l(),Ha=a("p"),oLo=o("The model class to instantiate is selected based on the "),Oae=a("code"),rLo=o("model_type"),tLo=o(` property of the config object (either
passed as an argument or loaded from `),Xae=a("code"),aLo=o("pretrained_model_name_or_path"),nLo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),zae=a("code"),sLo=o("pretrained_model_name_or_path"),lLo=o(":"),iLo=l(),R=a("ul"),Sb=a("li"),Vae=a("strong"),dLo=o("albert"),cLo=o(" \u2014 "),fj=a("a"),fLo=o("AlbertForQuestionAnswering"),mLo=o(" (ALBERT model)"),gLo=l(),Pb=a("li"),Wae=a("strong"),hLo=o("bart"),pLo=o(" \u2014 "),mj=a("a"),_Lo=o("BartForQuestionAnswering"),uLo=o(" (BART model)"),bLo=l(),$b=a("li"),Qae=a("strong"),vLo=o("bert"),TLo=o(" \u2014 "),gj=a("a"),FLo=o("BertForQuestionAnswering"),CLo=o(" (BERT model)"),MLo=l(),Ib=a("li"),Hae=a("strong"),ELo=o("big_bird"),yLo=o(" \u2014 "),hj=a("a"),wLo=o("BigBirdForQuestionAnswering"),ALo=o(" (BigBird model)"),LLo=l(),jb=a("li"),Uae=a("strong"),BLo=o("bigbird_pegasus"),kLo=o(" \u2014 "),pj=a("a"),xLo=o("BigBirdPegasusForQuestionAnswering"),RLo=o(" (BigBirdPegasus model)"),SLo=l(),Nb=a("li"),Jae=a("strong"),PLo=o("camembert"),$Lo=o(" \u2014 "),_j=a("a"),ILo=o("CamembertForQuestionAnswering"),jLo=o(" (CamemBERT model)"),NLo=l(),Db=a("li"),Yae=a("strong"),DLo=o("canine"),qLo=o(" \u2014 "),uj=a("a"),GLo=o("CanineForQuestionAnswering"),OLo=o(" (Canine model)"),XLo=l(),qb=a("li"),Kae=a("strong"),zLo=o("convbert"),VLo=o(" \u2014 "),bj=a("a"),WLo=o("ConvBertForQuestionAnswering"),QLo=o(" (ConvBERT model)"),HLo=l(),Gb=a("li"),Zae=a("strong"),ULo=o("deberta"),JLo=o(" \u2014 "),vj=a("a"),YLo=o("DebertaForQuestionAnswering"),KLo=o(" (DeBERTa model)"),ZLo=l(),Ob=a("li"),ene=a("strong"),e8o=o("deberta-v2"),o8o=o(" \u2014 "),Tj=a("a"),r8o=o("DebertaV2ForQuestionAnswering"),t8o=o(" (DeBERTa-v2 model)"),a8o=l(),Xb=a("li"),one=a("strong"),n8o=o("distilbert"),s8o=o(" \u2014 "),Fj=a("a"),l8o=o("DistilBertForQuestionAnswering"),i8o=o(" (DistilBERT model)"),d8o=l(),zb=a("li"),rne=a("strong"),c8o=o("electra"),f8o=o(" \u2014 "),Cj=a("a"),m8o=o("ElectraForQuestionAnswering"),g8o=o(" (ELECTRA model)"),h8o=l(),Vb=a("li"),tne=a("strong"),p8o=o("flaubert"),_8o=o(" \u2014 "),Mj=a("a"),u8o=o("FlaubertForQuestionAnsweringSimple"),b8o=o(" (FlauBERT model)"),v8o=l(),Wb=a("li"),ane=a("strong"),T8o=o("fnet"),F8o=o(" \u2014 "),Ej=a("a"),C8o=o("FNetForQuestionAnswering"),M8o=o(" (FNet model)"),E8o=l(),Qb=a("li"),nne=a("strong"),y8o=o("funnel"),w8o=o(" \u2014 "),yj=a("a"),A8o=o("FunnelForQuestionAnswering"),L8o=o(" (Funnel Transformer model)"),B8o=l(),Hb=a("li"),sne=a("strong"),k8o=o("gptj"),x8o=o(" \u2014 "),wj=a("a"),R8o=o("GPTJForQuestionAnswering"),S8o=o(" (GPT-J model)"),P8o=l(),Ub=a("li"),lne=a("strong"),$8o=o("ibert"),I8o=o(" \u2014 "),Aj=a("a"),j8o=o("IBertForQuestionAnswering"),N8o=o(" (I-BERT model)"),D8o=l(),Jb=a("li"),ine=a("strong"),q8o=o("layoutlmv2"),G8o=o(" \u2014 "),Lj=a("a"),O8o=o("LayoutLMv2ForQuestionAnswering"),X8o=o(" (LayoutLMv2 model)"),z8o=l(),Yb=a("li"),dne=a("strong"),V8o=o("led"),W8o=o(" \u2014 "),Bj=a("a"),Q8o=o("LEDForQuestionAnswering"),H8o=o(" (LED model)"),U8o=l(),Kb=a("li"),cne=a("strong"),J8o=o("longformer"),Y8o=o(" \u2014 "),kj=a("a"),K8o=o("LongformerForQuestionAnswering"),Z8o=o(" (Longformer model)"),e9o=l(),Zb=a("li"),fne=a("strong"),o9o=o("lxmert"),r9o=o(" \u2014 "),xj=a("a"),t9o=o("LxmertForQuestionAnswering"),a9o=o(" (LXMERT model)"),n9o=l(),e5=a("li"),mne=a("strong"),s9o=o("mbart"),l9o=o(" \u2014 "),Rj=a("a"),i9o=o("MBartForQuestionAnswering"),d9o=o(" (mBART model)"),c9o=l(),o5=a("li"),gne=a("strong"),f9o=o("megatron-bert"),m9o=o(" \u2014 "),Sj=a("a"),g9o=o("MegatronBertForQuestionAnswering"),h9o=o(" (MegatronBert model)"),p9o=l(),r5=a("li"),hne=a("strong"),_9o=o("mobilebert"),u9o=o(" \u2014 "),Pj=a("a"),b9o=o("MobileBertForQuestionAnswering"),v9o=o(" (MobileBERT model)"),T9o=l(),t5=a("li"),pne=a("strong"),F9o=o("mpnet"),C9o=o(" \u2014 "),$j=a("a"),M9o=o("MPNetForQuestionAnswering"),E9o=o(" (MPNet model)"),y9o=l(),a5=a("li"),_ne=a("strong"),w9o=o("nystromformer"),A9o=o(" \u2014 "),Ij=a("a"),L9o=o("NystromformerForQuestionAnswering"),B9o=o(" (Nystromformer model)"),k9o=l(),n5=a("li"),une=a("strong"),x9o=o("qdqbert"),R9o=o(" \u2014 "),jj=a("a"),S9o=o("QDQBertForQuestionAnswering"),P9o=o(" (QDQBert model)"),$9o=l(),s5=a("li"),bne=a("strong"),I9o=o("reformer"),j9o=o(" \u2014 "),Nj=a("a"),N9o=o("ReformerForQuestionAnswering"),D9o=o(" (Reformer model)"),q9o=l(),l5=a("li"),vne=a("strong"),G9o=o("rembert"),O9o=o(" \u2014 "),Dj=a("a"),X9o=o("RemBertForQuestionAnswering"),z9o=o(" (RemBERT model)"),V9o=l(),i5=a("li"),Tne=a("strong"),W9o=o("roberta"),Q9o=o(" \u2014 "),qj=a("a"),H9o=o("RobertaForQuestionAnswering"),U9o=o(" (RoBERTa model)"),J9o=l(),d5=a("li"),Fne=a("strong"),Y9o=o("roformer"),K9o=o(" \u2014 "),Gj=a("a"),Z9o=o("RoFormerForQuestionAnswering"),eBo=o(" (RoFormer model)"),oBo=l(),c5=a("li"),Cne=a("strong"),rBo=o("splinter"),tBo=o(" \u2014 "),Oj=a("a"),aBo=o("SplinterForQuestionAnswering"),nBo=o(" (Splinter model)"),sBo=l(),f5=a("li"),Mne=a("strong"),lBo=o("squeezebert"),iBo=o(" \u2014 "),Xj=a("a"),dBo=o("SqueezeBertForQuestionAnswering"),cBo=o(" (SqueezeBERT model)"),fBo=l(),m5=a("li"),Ene=a("strong"),mBo=o("xlm"),gBo=o(" \u2014 "),zj=a("a"),hBo=o("XLMForQuestionAnsweringSimple"),pBo=o(" (XLM model)"),_Bo=l(),g5=a("li"),yne=a("strong"),uBo=o("xlm-roberta"),bBo=o(" \u2014 "),Vj=a("a"),vBo=o("XLMRobertaForQuestionAnswering"),TBo=o(" (XLM-RoBERTa model)"),FBo=l(),h5=a("li"),wne=a("strong"),CBo=o("xlm-roberta-xl"),MBo=o(" \u2014 "),Wj=a("a"),EBo=o("XLMRobertaXLForQuestionAnswering"),yBo=o(" (XLM-RoBERTa-XL model)"),wBo=l(),p5=a("li"),Ane=a("strong"),ABo=o("xlnet"),LBo=o(" \u2014 "),Qj=a("a"),BBo=o("XLNetForQuestionAnsweringSimple"),kBo=o(" (XLNet model)"),xBo=l(),_5=a("li"),Lne=a("strong"),RBo=o("yoso"),SBo=o(" \u2014 "),Hj=a("a"),PBo=o("YosoForQuestionAnswering"),$Bo=o(" (YOSO model)"),IBo=l(),u5=a("p"),jBo=o("The model is set in evaluation mode by default using "),Bne=a("code"),NBo=o("model.eval()"),DBo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),kne=a("code"),qBo=o("model.train()"),GBo=l(),xne=a("p"),OBo=o("Examples:"),XBo=l(),f(e3.$$.fragment),K8e=l(),_d=a("h2"),b5=a("a"),Rne=a("span"),f(o3.$$.fragment),zBo=l(),Sne=a("span"),VBo=o("AutoModelForTableQuestionAnswering"),Z8e=l(),or=a("div"),f(r3.$$.fragment),WBo=l(),ud=a("p"),QBo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a table question answering head) when created
with the `),Pne=a("code"),HBo=o("from_pretrained()"),UBo=o("class method or the "),$ne=a("code"),JBo=o("from_config()"),YBo=o(`class
method.`),KBo=l(),t3=a("p"),ZBo=o("This class cannot be instantiated directly using "),Ine=a("code"),eko=o("__init__()"),oko=o(" (throws an error)."),rko=l(),Hr=a("div"),f(a3.$$.fragment),tko=l(),jne=a("p"),ako=o("Instantiates one of the model classes of the library (with a table question answering head) from a configuration."),nko=l(),bd=a("p"),sko=o(`Note:
Loading a model from its configuration file does `),Nne=a("strong"),lko=o("not"),iko=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Dne=a("code"),dko=o("from_pretrained()"),cko=o("to load the model weights."),fko=l(),qne=a("p"),mko=o("Examples:"),gko=l(),f(n3.$$.fragment),hko=l(),Ge=a("div"),f(s3.$$.fragment),pko=l(),Gne=a("p"),_ko=o("Instantiate one of the model classes of the library (with a table question answering head) from a pretrained model."),uko=l(),Ua=a("p"),bko=o("The model class to instantiate is selected based on the "),One=a("code"),vko=o("model_type"),Tko=o(` property of the config object (either
passed as an argument or loaded from `),Xne=a("code"),Fko=o("pretrained_model_name_or_path"),Cko=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),zne=a("code"),Mko=o("pretrained_model_name_or_path"),Eko=o(":"),yko=l(),Vne=a("ul"),v5=a("li"),Wne=a("strong"),wko=o("tapas"),Ako=o(" \u2014 "),Uj=a("a"),Lko=o("TapasForQuestionAnswering"),Bko=o(" (TAPAS model)"),kko=l(),T5=a("p"),xko=o("The model is set in evaluation mode by default using "),Qne=a("code"),Rko=o("model.eval()"),Sko=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Hne=a("code"),Pko=o("model.train()"),$ko=l(),Une=a("p"),Iko=o("Examples:"),jko=l(),f(l3.$$.fragment),e9e=l(),vd=a("h2"),F5=a("a"),Jne=a("span"),f(i3.$$.fragment),Nko=l(),Yne=a("span"),Dko=o("AutoModelForImageClassification"),o9e=l(),rr=a("div"),f(d3.$$.fragment),qko=l(),Td=a("p"),Gko=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a image classification head) when created
with the `),Kne=a("code"),Oko=o("from_pretrained()"),Xko=o("class method or the "),Zne=a("code"),zko=o("from_config()"),Vko=o(`class
method.`),Wko=l(),c3=a("p"),Qko=o("This class cannot be instantiated directly using "),ese=a("code"),Hko=o("__init__()"),Uko=o(" (throws an error)."),Jko=l(),Ur=a("div"),f(f3.$$.fragment),Yko=l(),ose=a("p"),Kko=o("Instantiates one of the model classes of the library (with a image classification head) from a configuration."),Zko=l(),Fd=a("p"),exo=o(`Note:
Loading a model from its configuration file does `),rse=a("strong"),oxo=o("not"),rxo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),tse=a("code"),txo=o("from_pretrained()"),axo=o("to load the model weights."),nxo=l(),ase=a("p"),sxo=o("Examples:"),lxo=l(),f(m3.$$.fragment),ixo=l(),Oe=a("div"),f(g3.$$.fragment),dxo=l(),nse=a("p"),cxo=o("Instantiate one of the model classes of the library (with a image classification head) from a pretrained model."),fxo=l(),Ja=a("p"),mxo=o("The model class to instantiate is selected based on the "),sse=a("code"),gxo=o("model_type"),hxo=o(` property of the config object (either
passed as an argument or loaded from `),lse=a("code"),pxo=o("pretrained_model_name_or_path"),_xo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),ise=a("code"),uxo=o("pretrained_model_name_or_path"),bxo=o(":"),vxo=l(),he=a("ul"),C5=a("li"),dse=a("strong"),Txo=o("beit"),Fxo=o(" \u2014 "),Jj=a("a"),Cxo=o("BeitForImageClassification"),Mxo=o(" (BEiT model)"),Exo=l(),M5=a("li"),cse=a("strong"),yxo=o("convnext"),wxo=o(" \u2014 "),Yj=a("a"),Axo=o("ConvNextForImageClassification"),Lxo=o(" (ConvNext model)"),Bxo=l(),Rs=a("li"),fse=a("strong"),kxo=o("deit"),xxo=o(" \u2014 "),Kj=a("a"),Rxo=o("DeiTForImageClassification"),Sxo=o(" or "),Zj=a("a"),Pxo=o("DeiTForImageClassificationWithTeacher"),$xo=o(" (DeiT model)"),Ixo=l(),E5=a("li"),mse=a("strong"),jxo=o("imagegpt"),Nxo=o(" \u2014 "),eN=a("a"),Dxo=o("ImageGPTForImageClassification"),qxo=o(" (ImageGPT model)"),Gxo=l(),la=a("li"),gse=a("strong"),Oxo=o("perceiver"),Xxo=o(" \u2014 "),oN=a("a"),zxo=o("PerceiverForImageClassificationLearned"),Vxo=o(" or "),rN=a("a"),Wxo=o("PerceiverForImageClassificationFourier"),Qxo=o(" or "),tN=a("a"),Hxo=o("PerceiverForImageClassificationConvProcessing"),Uxo=o(" (Perceiver model)"),Jxo=l(),y5=a("li"),hse=a("strong"),Yxo=o("poolformer"),Kxo=o(" \u2014 "),aN=a("a"),Zxo=o("PoolFormerForImageClassification"),eRo=o(" (PoolFormer model)"),oRo=l(),w5=a("li"),pse=a("strong"),rRo=o("resnet"),tRo=o(" \u2014 "),nN=a("a"),aRo=o("ResNetForImageClassification"),nRo=o(" (resnet model)"),sRo=l(),A5=a("li"),_se=a("strong"),lRo=o("segformer"),iRo=o(" \u2014 "),sN=a("a"),dRo=o("SegformerForImageClassification"),cRo=o(" (SegFormer model)"),fRo=l(),L5=a("li"),use=a("strong"),mRo=o("swin"),gRo=o(" \u2014 "),lN=a("a"),hRo=o("SwinForImageClassification"),pRo=o(" (Swin model)"),_Ro=l(),B5=a("li"),bse=a("strong"),uRo=o("vit"),bRo=o(" \u2014 "),iN=a("a"),vRo=o("ViTForImageClassification"),TRo=o(" (ViT model)"),FRo=l(),k5=a("p"),CRo=o("The model is set in evaluation mode by default using "),vse=a("code"),MRo=o("model.eval()"),ERo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Tse=a("code"),yRo=o("model.train()"),wRo=l(),Fse=a("p"),ARo=o("Examples:"),LRo=l(),f(h3.$$.fragment),r9e=l(),Cd=a("h2"),x5=a("a"),Cse=a("span"),f(p3.$$.fragment),BRo=l(),Mse=a("span"),kRo=o("AutoModelForVision2Seq"),t9e=l(),tr=a("div"),f(_3.$$.fragment),xRo=l(),Md=a("p"),RRo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a vision-to-text modeling head) when created
with the `),Ese=a("code"),SRo=o("from_pretrained()"),PRo=o("class method or the "),yse=a("code"),$Ro=o("from_config()"),IRo=o(`class
method.`),jRo=l(),u3=a("p"),NRo=o("This class cannot be instantiated directly using "),wse=a("code"),DRo=o("__init__()"),qRo=o(" (throws an error)."),GRo=l(),Jr=a("div"),f(b3.$$.fragment),ORo=l(),Ase=a("p"),XRo=o("Instantiates one of the model classes of the library (with a vision-to-text modeling head) from a configuration."),zRo=l(),Ed=a("p"),VRo=o(`Note:
Loading a model from its configuration file does `),Lse=a("strong"),WRo=o("not"),QRo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Bse=a("code"),HRo=o("from_pretrained()"),URo=o("to load the model weights."),JRo=l(),kse=a("p"),YRo=o("Examples:"),KRo=l(),f(v3.$$.fragment),ZRo=l(),Xe=a("div"),f(T3.$$.fragment),eSo=l(),xse=a("p"),oSo=o("Instantiate one of the model classes of the library (with a vision-to-text modeling head) from a pretrained model."),rSo=l(),Ya=a("p"),tSo=o("The model class to instantiate is selected based on the "),Rse=a("code"),aSo=o("model_type"),nSo=o(` property of the config object (either
passed as an argument or loaded from `),Sse=a("code"),sSo=o("pretrained_model_name_or_path"),lSo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Pse=a("code"),iSo=o("pretrained_model_name_or_path"),dSo=o(":"),cSo=l(),$se=a("ul"),R5=a("li"),Ise=a("strong"),fSo=o("vision-encoder-decoder"),mSo=o(" \u2014 "),dN=a("a"),gSo=o("VisionEncoderDecoderModel"),hSo=o(" (Vision Encoder decoder model)"),pSo=l(),S5=a("p"),_So=o("The model is set in evaluation mode by default using "),jse=a("code"),uSo=o("model.eval()"),bSo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Nse=a("code"),vSo=o("model.train()"),TSo=l(),Dse=a("p"),FSo=o("Examples:"),CSo=l(),f(F3.$$.fragment),a9e=l(),yd=a("h2"),P5=a("a"),qse=a("span"),f(C3.$$.fragment),MSo=l(),Gse=a("span"),ESo=o("AutoModelForAudioClassification"),n9e=l(),ar=a("div"),f(M3.$$.fragment),ySo=l(),wd=a("p"),wSo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a audio classification head) when created
with the `),Ose=a("code"),ASo=o("from_pretrained()"),LSo=o("class method or the "),Xse=a("code"),BSo=o("from_config()"),kSo=o(`class
method.`),xSo=l(),E3=a("p"),RSo=o("This class cannot be instantiated directly using "),zse=a("code"),SSo=o("__init__()"),PSo=o(" (throws an error)."),$So=l(),Yr=a("div"),f(y3.$$.fragment),ISo=l(),Vse=a("p"),jSo=o("Instantiates one of the model classes of the library (with a audio classification head) from a configuration."),NSo=l(),Ad=a("p"),DSo=o(`Note:
Loading a model from its configuration file does `),Wse=a("strong"),qSo=o("not"),GSo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Qse=a("code"),OSo=o("from_pretrained()"),XSo=o("to load the model weights."),zSo=l(),Hse=a("p"),VSo=o("Examples:"),WSo=l(),f(w3.$$.fragment),QSo=l(),ze=a("div"),f(A3.$$.fragment),HSo=l(),Use=a("p"),USo=o("Instantiate one of the model classes of the library (with a audio classification head) from a pretrained model."),JSo=l(),Ka=a("p"),YSo=o("The model class to instantiate is selected based on the "),Jse=a("code"),KSo=o("model_type"),ZSo=o(` property of the config object (either
passed as an argument or loaded from `),Yse=a("code"),ePo=o("pretrained_model_name_or_path"),oPo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Kse=a("code"),rPo=o("pretrained_model_name_or_path"),tPo=o(":"),aPo=l(),ao=a("ul"),$5=a("li"),Zse=a("strong"),nPo=o("hubert"),sPo=o(" \u2014 "),cN=a("a"),lPo=o("HubertForSequenceClassification"),iPo=o(" (Hubert model)"),dPo=l(),I5=a("li"),ele=a("strong"),cPo=o("sew"),fPo=o(" \u2014 "),fN=a("a"),mPo=o("SEWForSequenceClassification"),gPo=o(" (SEW model)"),hPo=l(),j5=a("li"),ole=a("strong"),pPo=o("sew-d"),_Po=o(" \u2014 "),mN=a("a"),uPo=o("SEWDForSequenceClassification"),bPo=o(" (SEW-D model)"),vPo=l(),N5=a("li"),rle=a("strong"),TPo=o("unispeech"),FPo=o(" \u2014 "),gN=a("a"),CPo=o("UniSpeechForSequenceClassification"),MPo=o(" (UniSpeech model)"),EPo=l(),D5=a("li"),tle=a("strong"),yPo=o("unispeech-sat"),wPo=o(" \u2014 "),hN=a("a"),APo=o("UniSpeechSatForSequenceClassification"),LPo=o(" (UniSpeechSat model)"),BPo=l(),q5=a("li"),ale=a("strong"),kPo=o("wav2vec2"),xPo=o(" \u2014 "),pN=a("a"),RPo=o("Wav2Vec2ForSequenceClassification"),SPo=o(" (Wav2Vec2 model)"),PPo=l(),G5=a("li"),nle=a("strong"),$Po=o("wavlm"),IPo=o(" \u2014 "),_N=a("a"),jPo=o("WavLMForSequenceClassification"),NPo=o(" (WavLM model)"),DPo=l(),O5=a("p"),qPo=o("The model is set in evaluation mode by default using "),sle=a("code"),GPo=o("model.eval()"),OPo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),lle=a("code"),XPo=o("model.train()"),zPo=l(),ile=a("p"),VPo=o("Examples:"),WPo=l(),f(L3.$$.fragment),s9e=l(),Ld=a("h2"),X5=a("a"),dle=a("span"),f(B3.$$.fragment),QPo=l(),cle=a("span"),HPo=o("AutoModelForAudioFrameClassification"),l9e=l(),nr=a("div"),f(k3.$$.fragment),UPo=l(),Bd=a("p"),JPo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a audio frame (token) classification head) when created
with the `),fle=a("code"),YPo=o("from_pretrained()"),KPo=o("class method or the "),mle=a("code"),ZPo=o("from_config()"),e$o=o(`class
method.`),o$o=l(),x3=a("p"),r$o=o("This class cannot be instantiated directly using "),gle=a("code"),t$o=o("__init__()"),a$o=o(" (throws an error)."),n$o=l(),Kr=a("div"),f(R3.$$.fragment),s$o=l(),hle=a("p"),l$o=o("Instantiates one of the model classes of the library (with a audio frame (token) classification head) from a configuration."),i$o=l(),kd=a("p"),d$o=o(`Note:
Loading a model from its configuration file does `),ple=a("strong"),c$o=o("not"),f$o=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),_le=a("code"),m$o=o("from_pretrained()"),g$o=o("to load the model weights."),h$o=l(),ule=a("p"),p$o=o("Examples:"),_$o=l(),f(S3.$$.fragment),u$o=l(),Ve=a("div"),f(P3.$$.fragment),b$o=l(),ble=a("p"),v$o=o("Instantiate one of the model classes of the library (with a audio frame (token) classification head) from a pretrained model."),T$o=l(),Za=a("p"),F$o=o("The model class to instantiate is selected based on the "),vle=a("code"),C$o=o("model_type"),M$o=o(` property of the config object (either
passed as an argument or loaded from `),Tle=a("code"),E$o=o("pretrained_model_name_or_path"),y$o=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Fle=a("code"),w$o=o("pretrained_model_name_or_path"),A$o=o(":"),L$o=l(),xd=a("ul"),z5=a("li"),Cle=a("strong"),B$o=o("unispeech-sat"),k$o=o(" \u2014 "),uN=a("a"),x$o=o("UniSpeechSatForAudioFrameClassification"),R$o=o(" (UniSpeechSat model)"),S$o=l(),V5=a("li"),Mle=a("strong"),P$o=o("wav2vec2"),$$o=o(" \u2014 "),bN=a("a"),I$o=o("Wav2Vec2ForAudioFrameClassification"),j$o=o(" (Wav2Vec2 model)"),N$o=l(),W5=a("li"),Ele=a("strong"),D$o=o("wavlm"),q$o=o(" \u2014 "),vN=a("a"),G$o=o("WavLMForAudioFrameClassification"),O$o=o(" (WavLM model)"),X$o=l(),Q5=a("p"),z$o=o("The model is set in evaluation mode by default using "),yle=a("code"),V$o=o("model.eval()"),W$o=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),wle=a("code"),Q$o=o("model.train()"),H$o=l(),Ale=a("p"),U$o=o("Examples:"),J$o=l(),f($3.$$.fragment),i9e=l(),Rd=a("h2"),H5=a("a"),Lle=a("span"),f(I3.$$.fragment),Y$o=l(),Ble=a("span"),K$o=o("AutoModelForCTC"),d9e=l(),sr=a("div"),f(j3.$$.fragment),Z$o=l(),Sd=a("p"),eIo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a connectionist temporal classification head) when created
with the `),kle=a("code"),oIo=o("from_pretrained()"),rIo=o("class method or the "),xle=a("code"),tIo=o("from_config()"),aIo=o(`class
method.`),nIo=l(),N3=a("p"),sIo=o("This class cannot be instantiated directly using "),Rle=a("code"),lIo=o("__init__()"),iIo=o(" (throws an error)."),dIo=l(),Zr=a("div"),f(D3.$$.fragment),cIo=l(),Sle=a("p"),fIo=o("Instantiates one of the model classes of the library (with a connectionist temporal classification head) from a configuration."),mIo=l(),Pd=a("p"),gIo=o(`Note:
Loading a model from its configuration file does `),Ple=a("strong"),hIo=o("not"),pIo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),$le=a("code"),_Io=o("from_pretrained()"),uIo=o("to load the model weights."),bIo=l(),Ile=a("p"),vIo=o("Examples:"),TIo=l(),f(q3.$$.fragment),FIo=l(),We=a("div"),f(G3.$$.fragment),CIo=l(),jle=a("p"),MIo=o("Instantiate one of the model classes of the library (with a connectionist temporal classification head) from a pretrained model."),EIo=l(),en=a("p"),yIo=o("The model class to instantiate is selected based on the "),Nle=a("code"),wIo=o("model_type"),AIo=o(` property of the config object (either
passed as an argument or loaded from `),Dle=a("code"),LIo=o("pretrained_model_name_or_path"),BIo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),qle=a("code"),kIo=o("pretrained_model_name_or_path"),xIo=o(":"),RIo=l(),no=a("ul"),U5=a("li"),Gle=a("strong"),SIo=o("hubert"),PIo=o(" \u2014 "),TN=a("a"),$Io=o("HubertForCTC"),IIo=o(" (Hubert model)"),jIo=l(),J5=a("li"),Ole=a("strong"),NIo=o("sew"),DIo=o(" \u2014 "),FN=a("a"),qIo=o("SEWForCTC"),GIo=o(" (SEW model)"),OIo=l(),Y5=a("li"),Xle=a("strong"),XIo=o("sew-d"),zIo=o(" \u2014 "),CN=a("a"),VIo=o("SEWDForCTC"),WIo=o(" (SEW-D model)"),QIo=l(),K5=a("li"),zle=a("strong"),HIo=o("unispeech"),UIo=o(" \u2014 "),MN=a("a"),JIo=o("UniSpeechForCTC"),YIo=o(" (UniSpeech model)"),KIo=l(),Z5=a("li"),Vle=a("strong"),ZIo=o("unispeech-sat"),ejo=o(" \u2014 "),EN=a("a"),ojo=o("UniSpeechSatForCTC"),rjo=o(" (UniSpeechSat model)"),tjo=l(),e2=a("li"),Wle=a("strong"),ajo=o("wav2vec2"),njo=o(" \u2014 "),yN=a("a"),sjo=o("Wav2Vec2ForCTC"),ljo=o(" (Wav2Vec2 model)"),ijo=l(),o2=a("li"),Qle=a("strong"),djo=o("wavlm"),cjo=o(" \u2014 "),wN=a("a"),fjo=o("WavLMForCTC"),mjo=o(" (WavLM model)"),gjo=l(),r2=a("p"),hjo=o("The model is set in evaluation mode by default using "),Hle=a("code"),pjo=o("model.eval()"),_jo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Ule=a("code"),ujo=o("model.train()"),bjo=l(),Jle=a("p"),vjo=o("Examples:"),Tjo=l(),f(O3.$$.fragment),c9e=l(),$d=a("h2"),t2=a("a"),Yle=a("span"),f(X3.$$.fragment),Fjo=l(),Kle=a("span"),Cjo=o("AutoModelForSpeechSeq2Seq"),f9e=l(),lr=a("div"),f(z3.$$.fragment),Mjo=l(),Id=a("p"),Ejo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) when created
with the `),Zle=a("code"),yjo=o("from_pretrained()"),wjo=o("class method or the "),eie=a("code"),Ajo=o("from_config()"),Ljo=o(`class
method.`),Bjo=l(),V3=a("p"),kjo=o("This class cannot be instantiated directly using "),oie=a("code"),xjo=o("__init__()"),Rjo=o(" (throws an error)."),Sjo=l(),et=a("div"),f(W3.$$.fragment),Pjo=l(),rie=a("p"),$jo=o("Instantiates one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) from a configuration."),Ijo=l(),jd=a("p"),jjo=o(`Note:
Loading a model from its configuration file does `),tie=a("strong"),Njo=o("not"),Djo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),aie=a("code"),qjo=o("from_pretrained()"),Gjo=o("to load the model weights."),Ojo=l(),nie=a("p"),Xjo=o("Examples:"),zjo=l(),f(Q3.$$.fragment),Vjo=l(),Qe=a("div"),f(H3.$$.fragment),Wjo=l(),sie=a("p"),Qjo=o("Instantiate one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) from a pretrained model."),Hjo=l(),on=a("p"),Ujo=o("The model class to instantiate is selected based on the "),lie=a("code"),Jjo=o("model_type"),Yjo=o(` property of the config object (either
passed as an argument or loaded from `),iie=a("code"),Kjo=o("pretrained_model_name_or_path"),Zjo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),die=a("code"),eNo=o("pretrained_model_name_or_path"),oNo=o(":"),rNo=l(),U3=a("ul"),a2=a("li"),cie=a("strong"),tNo=o("speech-encoder-decoder"),aNo=o(" \u2014 "),AN=a("a"),nNo=o("SpeechEncoderDecoderModel"),sNo=o(" (Speech Encoder decoder model)"),lNo=l(),n2=a("li"),fie=a("strong"),iNo=o("speech_to_text"),dNo=o(" \u2014 "),LN=a("a"),cNo=o("Speech2TextForConditionalGeneration"),fNo=o(" (Speech2Text model)"),mNo=l(),s2=a("p"),gNo=o("The model is set in evaluation mode by default using "),mie=a("code"),hNo=o("model.eval()"),pNo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),gie=a("code"),_No=o("model.train()"),uNo=l(),hie=a("p"),bNo=o("Examples:"),vNo=l(),f(J3.$$.fragment),m9e=l(),Nd=a("h2"),l2=a("a"),pie=a("span"),f(Y3.$$.fragment),TNo=l(),_ie=a("span"),FNo=o("AutoModelForAudioXVector"),g9e=l(),ir=a("div"),f(K3.$$.fragment),CNo=l(),Dd=a("p"),MNo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a audio retrieval via x-vector head) when created
with the `),uie=a("code"),ENo=o("from_pretrained()"),yNo=o("class method or the "),bie=a("code"),wNo=o("from_config()"),ANo=o(`class
method.`),LNo=l(),Z3=a("p"),BNo=o("This class cannot be instantiated directly using "),vie=a("code"),kNo=o("__init__()"),xNo=o(" (throws an error)."),RNo=l(),ot=a("div"),f(ey.$$.fragment),SNo=l(),Tie=a("p"),PNo=o("Instantiates one of the model classes of the library (with a audio retrieval via x-vector head) from a configuration."),$No=l(),qd=a("p"),INo=o(`Note:
Loading a model from its configuration file does `),Fie=a("strong"),jNo=o("not"),NNo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Cie=a("code"),DNo=o("from_pretrained()"),qNo=o("to load the model weights."),GNo=l(),Mie=a("p"),ONo=o("Examples:"),XNo=l(),f(oy.$$.fragment),zNo=l(),He=a("div"),f(ry.$$.fragment),VNo=l(),Eie=a("p"),WNo=o("Instantiate one of the model classes of the library (with a audio retrieval via x-vector head) from a pretrained model."),QNo=l(),rn=a("p"),HNo=o("The model class to instantiate is selected based on the "),yie=a("code"),UNo=o("model_type"),JNo=o(` property of the config object (either
passed as an argument or loaded from `),wie=a("code"),YNo=o("pretrained_model_name_or_path"),KNo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Aie=a("code"),ZNo=o("pretrained_model_name_or_path"),eDo=o(":"),oDo=l(),Gd=a("ul"),i2=a("li"),Lie=a("strong"),rDo=o("unispeech-sat"),tDo=o(" \u2014 "),BN=a("a"),aDo=o("UniSpeechSatForXVector"),nDo=o(" (UniSpeechSat model)"),sDo=l(),d2=a("li"),Bie=a("strong"),lDo=o("wav2vec2"),iDo=o(" \u2014 "),kN=a("a"),dDo=o("Wav2Vec2ForXVector"),cDo=o(" (Wav2Vec2 model)"),fDo=l(),c2=a("li"),kie=a("strong"),mDo=o("wavlm"),gDo=o(" \u2014 "),xN=a("a"),hDo=o("WavLMForXVector"),pDo=o(" (WavLM model)"),_Do=l(),f2=a("p"),uDo=o("The model is set in evaluation mode by default using "),xie=a("code"),bDo=o("model.eval()"),vDo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Rie=a("code"),TDo=o("model.train()"),FDo=l(),Sie=a("p"),CDo=o("Examples:"),MDo=l(),f(ty.$$.fragment),h9e=l(),Od=a("h2"),m2=a("a"),Pie=a("span"),f(ay.$$.fragment),EDo=l(),$ie=a("span"),yDo=o("AutoModelForMaskedImageModeling"),p9e=l(),dr=a("div"),f(ny.$$.fragment),wDo=l(),Xd=a("p"),ADo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a masked image modeling head) when created
with the `),Iie=a("code"),LDo=o("from_pretrained()"),BDo=o("class method or the "),jie=a("code"),kDo=o("from_config()"),xDo=o(`class
method.`),RDo=l(),sy=a("p"),SDo=o("This class cannot be instantiated directly using "),Nie=a("code"),PDo=o("__init__()"),$Do=o(" (throws an error)."),IDo=l(),rt=a("div"),f(ly.$$.fragment),jDo=l(),Die=a("p"),NDo=o("Instantiates one of the model classes of the library (with a masked image modeling head) from a configuration."),DDo=l(),zd=a("p"),qDo=o(`Note:
Loading a model from its configuration file does `),qie=a("strong"),GDo=o("not"),ODo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Gie=a("code"),XDo=o("from_pretrained()"),zDo=o("to load the model weights."),VDo=l(),Oie=a("p"),WDo=o("Examples:"),QDo=l(),f(iy.$$.fragment),HDo=l(),Ue=a("div"),f(dy.$$.fragment),UDo=l(),Xie=a("p"),JDo=o("Instantiate one of the model classes of the library (with a masked image modeling head) from a pretrained model."),YDo=l(),tn=a("p"),KDo=o("The model class to instantiate is selected based on the "),zie=a("code"),ZDo=o("model_type"),eqo=o(` property of the config object (either
passed as an argument or loaded from `),Vie=a("code"),oqo=o("pretrained_model_name_or_path"),rqo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Wie=a("code"),tqo=o("pretrained_model_name_or_path"),aqo=o(":"),nqo=l(),Vd=a("ul"),g2=a("li"),Qie=a("strong"),sqo=o("deit"),lqo=o(" \u2014 "),RN=a("a"),iqo=o("DeiTForMaskedImageModeling"),dqo=o(" (DeiT model)"),cqo=l(),h2=a("li"),Hie=a("strong"),fqo=o("swin"),mqo=o(" \u2014 "),SN=a("a"),gqo=o("SwinForMaskedImageModeling"),hqo=o(" (Swin model)"),pqo=l(),p2=a("li"),Uie=a("strong"),_qo=o("vit"),uqo=o(" \u2014 "),PN=a("a"),bqo=o("ViTForMaskedImageModeling"),vqo=o(" (ViT model)"),Tqo=l(),_2=a("p"),Fqo=o("The model is set in evaluation mode by default using "),Jie=a("code"),Cqo=o("model.eval()"),Mqo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Yie=a("code"),Eqo=o("model.train()"),yqo=l(),Kie=a("p"),wqo=o("Examples:"),Aqo=l(),f(cy.$$.fragment),_9e=l(),Wd=a("h2"),u2=a("a"),Zie=a("span"),f(fy.$$.fragment),Lqo=l(),ede=a("span"),Bqo=o("AutoModelForObjectDetection"),u9e=l(),cr=a("div"),f(my.$$.fragment),kqo=l(),Qd=a("p"),xqo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a object detection head) when created
with the `),ode=a("code"),Rqo=o("from_pretrained()"),Sqo=o("class method or the "),rde=a("code"),Pqo=o("from_config()"),$qo=o(`class
method.`),Iqo=l(),gy=a("p"),jqo=o("This class cannot be instantiated directly using "),tde=a("code"),Nqo=o("__init__()"),Dqo=o(" (throws an error)."),qqo=l(),tt=a("div"),f(hy.$$.fragment),Gqo=l(),ade=a("p"),Oqo=o("Instantiates one of the model classes of the library (with a object detection head) from a configuration."),Xqo=l(),Hd=a("p"),zqo=o(`Note:
Loading a model from its configuration file does `),nde=a("strong"),Vqo=o("not"),Wqo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),sde=a("code"),Qqo=o("from_pretrained()"),Hqo=o("to load the model weights."),Uqo=l(),lde=a("p"),Jqo=o("Examples:"),Yqo=l(),f(py.$$.fragment),Kqo=l(),Je=a("div"),f(_y.$$.fragment),Zqo=l(),ide=a("p"),eGo=o("Instantiate one of the model classes of the library (with a object detection head) from a pretrained model."),oGo=l(),an=a("p"),rGo=o("The model class to instantiate is selected based on the "),dde=a("code"),tGo=o("model_type"),aGo=o(` property of the config object (either
passed as an argument or loaded from `),cde=a("code"),nGo=o("pretrained_model_name_or_path"),sGo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),fde=a("code"),lGo=o("pretrained_model_name_or_path"),iGo=o(":"),dGo=l(),mde=a("ul"),b2=a("li"),gde=a("strong"),cGo=o("detr"),fGo=o(" \u2014 "),$N=a("a"),mGo=o("DetrForObjectDetection"),gGo=o(" (DETR model)"),hGo=l(),v2=a("p"),pGo=o("The model is set in evaluation mode by default using "),hde=a("code"),_Go=o("model.eval()"),uGo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),pde=a("code"),bGo=o("model.train()"),vGo=l(),_de=a("p"),TGo=o("Examples:"),FGo=l(),f(uy.$$.fragment),b9e=l(),Ud=a("h2"),T2=a("a"),ude=a("span"),f(by.$$.fragment),CGo=l(),bde=a("span"),MGo=o("AutoModelForImageSegmentation"),v9e=l(),fr=a("div"),f(vy.$$.fragment),EGo=l(),Jd=a("p"),yGo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a image segmentation head) when created
with the `),vde=a("code"),wGo=o("from_pretrained()"),AGo=o("class method or the "),Tde=a("code"),LGo=o("from_config()"),BGo=o(`class
method.`),kGo=l(),Ty=a("p"),xGo=o("This class cannot be instantiated directly using "),Fde=a("code"),RGo=o("__init__()"),SGo=o(" (throws an error)."),PGo=l(),at=a("div"),f(Fy.$$.fragment),$Go=l(),Cde=a("p"),IGo=o("Instantiates one of the model classes of the library (with a image segmentation head) from a configuration."),jGo=l(),Yd=a("p"),NGo=o(`Note:
Loading a model from its configuration file does `),Mde=a("strong"),DGo=o("not"),qGo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Ede=a("code"),GGo=o("from_pretrained()"),OGo=o("to load the model weights."),XGo=l(),yde=a("p"),zGo=o("Examples:"),VGo=l(),f(Cy.$$.fragment),WGo=l(),Ye=a("div"),f(My.$$.fragment),QGo=l(),wde=a("p"),HGo=o("Instantiate one of the model classes of the library (with a image segmentation head) from a pretrained model."),UGo=l(),nn=a("p"),JGo=o("The model class to instantiate is selected based on the "),Ade=a("code"),YGo=o("model_type"),KGo=o(` property of the config object (either
passed as an argument or loaded from `),Lde=a("code"),ZGo=o("pretrained_model_name_or_path"),eOo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Bde=a("code"),oOo=o("pretrained_model_name_or_path"),rOo=o(":"),tOo=l(),kde=a("ul"),F2=a("li"),xde=a("strong"),aOo=o("detr"),nOo=o(" \u2014 "),IN=a("a"),sOo=o("DetrForSegmentation"),lOo=o(" (DETR model)"),iOo=l(),C2=a("p"),dOo=o("The model is set in evaluation mode by default using "),Rde=a("code"),cOo=o("model.eval()"),fOo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Sde=a("code"),mOo=o("model.train()"),gOo=l(),Pde=a("p"),hOo=o("Examples:"),pOo=l(),f(Ey.$$.fragment),T9e=l(),Kd=a("h2"),M2=a("a"),$de=a("span"),f(yy.$$.fragment),_Oo=l(),Ide=a("span"),uOo=o("AutoModelForSemanticSegmentation"),F9e=l(),mr=a("div"),f(wy.$$.fragment),bOo=l(),Zd=a("p"),vOo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a semantic segmentation head) when created
with the `),jde=a("code"),TOo=o("from_pretrained()"),FOo=o("class method or the "),Nde=a("code"),COo=o("from_config()"),MOo=o(`class
method.`),EOo=l(),Ay=a("p"),yOo=o("This class cannot be instantiated directly using "),Dde=a("code"),wOo=o("__init__()"),AOo=o(" (throws an error)."),LOo=l(),nt=a("div"),f(Ly.$$.fragment),BOo=l(),qde=a("p"),kOo=o("Instantiates one of the model classes of the library (with a semantic segmentation head) from a configuration."),xOo=l(),ec=a("p"),ROo=o(`Note:
Loading a model from its configuration file does `),Gde=a("strong"),SOo=o("not"),POo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Ode=a("code"),$Oo=o("from_pretrained()"),IOo=o("to load the model weights."),jOo=l(),Xde=a("p"),NOo=o("Examples:"),DOo=l(),f(By.$$.fragment),qOo=l(),Ke=a("div"),f(ky.$$.fragment),GOo=l(),zde=a("p"),OOo=o("Instantiate one of the model classes of the library (with a semantic segmentation head) from a pretrained model."),XOo=l(),sn=a("p"),zOo=o("The model class to instantiate is selected based on the "),Vde=a("code"),VOo=o("model_type"),WOo=o(` property of the config object (either
passed as an argument or loaded from `),Wde=a("code"),QOo=o("pretrained_model_name_or_path"),HOo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Qde=a("code"),UOo=o("pretrained_model_name_or_path"),JOo=o(":"),YOo=l(),xy=a("ul"),E2=a("li"),Hde=a("strong"),KOo=o("beit"),ZOo=o(" \u2014 "),jN=a("a"),eXo=o("BeitForSemanticSegmentation"),oXo=o(" (BEiT model)"),rXo=l(),y2=a("li"),Ude=a("strong"),tXo=o("segformer"),aXo=o(" \u2014 "),NN=a("a"),nXo=o("SegformerForSemanticSegmentation"),sXo=o(" (SegFormer model)"),lXo=l(),w2=a("p"),iXo=o("The model is set in evaluation mode by default using "),Jde=a("code"),dXo=o("model.eval()"),cXo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Yde=a("code"),fXo=o("model.train()"),mXo=l(),Kde=a("p"),gXo=o("Examples:"),hXo=l(),f(Ry.$$.fragment),C9e=l(),oc=a("h2"),A2=a("a"),Zde=a("span"),f(Sy.$$.fragment),pXo=l(),ece=a("span"),_Xo=o("TFAutoModel"),M9e=l(),gr=a("div"),f(Py.$$.fragment),uXo=l(),rc=a("p"),bXo=o(`This is a generic model class that will be instantiated as one of the base model classes of the library when created
with the `),oce=a("code"),vXo=o("from_pretrained()"),TXo=o("class method or the "),rce=a("code"),FXo=o("from_config()"),CXo=o(`class
method.`),MXo=l(),$y=a("p"),EXo=o("This class cannot be instantiated directly using "),tce=a("code"),yXo=o("__init__()"),wXo=o(" (throws an error)."),AXo=l(),st=a("div"),f(Iy.$$.fragment),LXo=l(),ace=a("p"),BXo=o("Instantiates one of the base model classes of the library from a configuration."),kXo=l(),tc=a("p"),xXo=o(`Note:
Loading a model from its configuration file does `),nce=a("strong"),RXo=o("not"),SXo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),sce=a("code"),PXo=o("from_pretrained()"),$Xo=o("to load the model weights."),IXo=l(),lce=a("p"),jXo=o("Examples:"),NXo=l(),f(jy.$$.fragment),DXo=l(),go=a("div"),f(Ny.$$.fragment),qXo=l(),ice=a("p"),GXo=o("Instantiate one of the base model classes of the library from a pretrained model."),OXo=l(),ln=a("p"),XXo=o("The model class to instantiate is selected based on the "),dce=a("code"),zXo=o("model_type"),VXo=o(` property of the config object (either
passed as an argument or loaded from `),cce=a("code"),WXo=o("pretrained_model_name_or_path"),QXo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),fce=a("code"),HXo=o("pretrained_model_name_or_path"),UXo=o(":"),JXo=l(),B=a("ul"),L2=a("li"),mce=a("strong"),YXo=o("albert"),KXo=o(" \u2014 "),DN=a("a"),ZXo=o("TFAlbertModel"),ezo=o(" (ALBERT model)"),ozo=l(),B2=a("li"),gce=a("strong"),rzo=o("bart"),tzo=o(" \u2014 "),qN=a("a"),azo=o("TFBartModel"),nzo=o(" (BART model)"),szo=l(),k2=a("li"),hce=a("strong"),lzo=o("bert"),izo=o(" \u2014 "),GN=a("a"),dzo=o("TFBertModel"),czo=o(" (BERT model)"),fzo=l(),x2=a("li"),pce=a("strong"),mzo=o("blenderbot"),gzo=o(" \u2014 "),ON=a("a"),hzo=o("TFBlenderbotModel"),pzo=o(" (Blenderbot model)"),_zo=l(),R2=a("li"),_ce=a("strong"),uzo=o("blenderbot-small"),bzo=o(" \u2014 "),XN=a("a"),vzo=o("TFBlenderbotSmallModel"),Tzo=o(" (BlenderbotSmall model)"),Fzo=l(),S2=a("li"),uce=a("strong"),Czo=o("camembert"),Mzo=o(" \u2014 "),zN=a("a"),Ezo=o("TFCamembertModel"),yzo=o(" (CamemBERT model)"),wzo=l(),P2=a("li"),bce=a("strong"),Azo=o("clip"),Lzo=o(" \u2014 "),VN=a("a"),Bzo=o("TFCLIPModel"),kzo=o(" (CLIP model)"),xzo=l(),$2=a("li"),vce=a("strong"),Rzo=o("convbert"),Szo=o(" \u2014 "),WN=a("a"),Pzo=o("TFConvBertModel"),$zo=o(" (ConvBERT model)"),Izo=l(),I2=a("li"),Tce=a("strong"),jzo=o("ctrl"),Nzo=o(" \u2014 "),QN=a("a"),Dzo=o("TFCTRLModel"),qzo=o(" (CTRL model)"),Gzo=l(),j2=a("li"),Fce=a("strong"),Ozo=o("deberta"),Xzo=o(" \u2014 "),HN=a("a"),zzo=o("TFDebertaModel"),Vzo=o(" (DeBERTa model)"),Wzo=l(),N2=a("li"),Cce=a("strong"),Qzo=o("deberta-v2"),Hzo=o(" \u2014 "),UN=a("a"),Uzo=o("TFDebertaV2Model"),Jzo=o(" (DeBERTa-v2 model)"),Yzo=l(),D2=a("li"),Mce=a("strong"),Kzo=o("distilbert"),Zzo=o(" \u2014 "),JN=a("a"),eVo=o("TFDistilBertModel"),oVo=o(" (DistilBERT model)"),rVo=l(),q2=a("li"),Ece=a("strong"),tVo=o("dpr"),aVo=o(" \u2014 "),YN=a("a"),nVo=o("TFDPRQuestionEncoder"),sVo=o(" (DPR model)"),lVo=l(),G2=a("li"),yce=a("strong"),iVo=o("electra"),dVo=o(" \u2014 "),KN=a("a"),cVo=o("TFElectraModel"),fVo=o(" (ELECTRA model)"),mVo=l(),O2=a("li"),wce=a("strong"),gVo=o("flaubert"),hVo=o(" \u2014 "),ZN=a("a"),pVo=o("TFFlaubertModel"),_Vo=o(" (FlauBERT model)"),uVo=l(),Ss=a("li"),Ace=a("strong"),bVo=o("funnel"),vVo=o(" \u2014 "),eD=a("a"),TVo=o("TFFunnelModel"),FVo=o(" or "),oD=a("a"),CVo=o("TFFunnelBaseModel"),MVo=o(" (Funnel Transformer model)"),EVo=l(),X2=a("li"),Lce=a("strong"),yVo=o("gpt2"),wVo=o(" \u2014 "),rD=a("a"),AVo=o("TFGPT2Model"),LVo=o(" (OpenAI GPT-2 model)"),BVo=l(),z2=a("li"),Bce=a("strong"),kVo=o("hubert"),xVo=o(" \u2014 "),tD=a("a"),RVo=o("TFHubertModel"),SVo=o(" (Hubert model)"),PVo=l(),V2=a("li"),kce=a("strong"),$Vo=o("layoutlm"),IVo=o(" \u2014 "),aD=a("a"),jVo=o("TFLayoutLMModel"),NVo=o(" (LayoutLM model)"),DVo=l(),W2=a("li"),xce=a("strong"),qVo=o("led"),GVo=o(" \u2014 "),nD=a("a"),OVo=o("TFLEDModel"),XVo=o(" (LED model)"),zVo=l(),Q2=a("li"),Rce=a("strong"),VVo=o("longformer"),WVo=o(" \u2014 "),sD=a("a"),QVo=o("TFLongformerModel"),HVo=o(" (Longformer model)"),UVo=l(),H2=a("li"),Sce=a("strong"),JVo=o("lxmert"),YVo=o(" \u2014 "),lD=a("a"),KVo=o("TFLxmertModel"),ZVo=o(" (LXMERT model)"),eWo=l(),U2=a("li"),Pce=a("strong"),oWo=o("marian"),rWo=o(" \u2014 "),iD=a("a"),tWo=o("TFMarianModel"),aWo=o(" (Marian model)"),nWo=l(),J2=a("li"),$ce=a("strong"),sWo=o("mbart"),lWo=o(" \u2014 "),dD=a("a"),iWo=o("TFMBartModel"),dWo=o(" (mBART model)"),cWo=l(),Y2=a("li"),Ice=a("strong"),fWo=o("mobilebert"),mWo=o(" \u2014 "),cD=a("a"),gWo=o("TFMobileBertModel"),hWo=o(" (MobileBERT model)"),pWo=l(),K2=a("li"),jce=a("strong"),_Wo=o("mpnet"),uWo=o(" \u2014 "),fD=a("a"),bWo=o("TFMPNetModel"),vWo=o(" (MPNet model)"),TWo=l(),Z2=a("li"),Nce=a("strong"),FWo=o("mt5"),CWo=o(" \u2014 "),mD=a("a"),MWo=o("TFMT5Model"),EWo=o(" (mT5 model)"),yWo=l(),ev=a("li"),Dce=a("strong"),wWo=o("openai-gpt"),AWo=o(" \u2014 "),gD=a("a"),LWo=o("TFOpenAIGPTModel"),BWo=o(" (OpenAI GPT model)"),kWo=l(),ov=a("li"),qce=a("strong"),xWo=o("pegasus"),RWo=o(" \u2014 "),hD=a("a"),SWo=o("TFPegasusModel"),PWo=o(" (Pegasus model)"),$Wo=l(),rv=a("li"),Gce=a("strong"),IWo=o("rembert"),jWo=o(" \u2014 "),pD=a("a"),NWo=o("TFRemBertModel"),DWo=o(" (RemBERT model)"),qWo=l(),tv=a("li"),Oce=a("strong"),GWo=o("roberta"),OWo=o(" \u2014 "),_D=a("a"),XWo=o("TFRobertaModel"),zWo=o(" (RoBERTa model)"),VWo=l(),av=a("li"),Xce=a("strong"),WWo=o("roformer"),QWo=o(" \u2014 "),uD=a("a"),HWo=o("TFRoFormerModel"),UWo=o(" (RoFormer model)"),JWo=l(),nv=a("li"),zce=a("strong"),YWo=o("speech_to_text"),KWo=o(" \u2014 "),bD=a("a"),ZWo=o("TFSpeech2TextModel"),eQo=o(" (Speech2Text model)"),oQo=l(),sv=a("li"),Vce=a("strong"),rQo=o("t5"),tQo=o(" \u2014 "),vD=a("a"),aQo=o("TFT5Model"),nQo=o(" (T5 model)"),sQo=l(),lv=a("li"),Wce=a("strong"),lQo=o("tapas"),iQo=o(" \u2014 "),TD=a("a"),dQo=o("TFTapasModel"),cQo=o(" (TAPAS model)"),fQo=l(),iv=a("li"),Qce=a("strong"),mQo=o("transfo-xl"),gQo=o(" \u2014 "),FD=a("a"),hQo=o("TFTransfoXLModel"),pQo=o(" (Transformer-XL model)"),_Qo=l(),dv=a("li"),Hce=a("strong"),uQo=o("vit"),bQo=o(" \u2014 "),CD=a("a"),vQo=o("TFViTModel"),TQo=o(" (ViT model)"),FQo=l(),cv=a("li"),Uce=a("strong"),CQo=o("wav2vec2"),MQo=o(" \u2014 "),MD=a("a"),EQo=o("TFWav2Vec2Model"),yQo=o(" (Wav2Vec2 model)"),wQo=l(),fv=a("li"),Jce=a("strong"),AQo=o("xlm"),LQo=o(" \u2014 "),ED=a("a"),BQo=o("TFXLMModel"),kQo=o(" (XLM model)"),xQo=l(),mv=a("li"),Yce=a("strong"),RQo=o("xlm-roberta"),SQo=o(" \u2014 "),yD=a("a"),PQo=o("TFXLMRobertaModel"),$Qo=o(" (XLM-RoBERTa model)"),IQo=l(),gv=a("li"),Kce=a("strong"),jQo=o("xlnet"),NQo=o(" \u2014 "),wD=a("a"),DQo=o("TFXLNetModel"),qQo=o(" (XLNet model)"),GQo=l(),Zce=a("p"),OQo=o("Examples:"),XQo=l(),f(Dy.$$.fragment),E9e=l(),ac=a("h2"),hv=a("a"),efe=a("span"),f(qy.$$.fragment),zQo=l(),ofe=a("span"),VQo=o("TFAutoModelForPreTraining"),y9e=l(),hr=a("div"),f(Gy.$$.fragment),WQo=l(),nc=a("p"),QQo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a pretraining head) when created
with the `),rfe=a("code"),HQo=o("from_pretrained()"),UQo=o("class method or the "),tfe=a("code"),JQo=o("from_config()"),YQo=o(`class
method.`),KQo=l(),Oy=a("p"),ZQo=o("This class cannot be instantiated directly using "),afe=a("code"),eHo=o("__init__()"),oHo=o(" (throws an error)."),rHo=l(),lt=a("div"),f(Xy.$$.fragment),tHo=l(),nfe=a("p"),aHo=o("Instantiates one of the model classes of the library (with a pretraining head) from a configuration."),nHo=l(),sc=a("p"),sHo=o(`Note:
Loading a model from its configuration file does `),sfe=a("strong"),lHo=o("not"),iHo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),lfe=a("code"),dHo=o("from_pretrained()"),cHo=o("to load the model weights."),fHo=l(),ife=a("p"),mHo=o("Examples:"),gHo=l(),f(zy.$$.fragment),hHo=l(),ho=a("div"),f(Vy.$$.fragment),pHo=l(),dfe=a("p"),_Ho=o("Instantiate one of the model classes of the library (with a pretraining head) from a pretrained model."),uHo=l(),dn=a("p"),bHo=o("The model class to instantiate is selected based on the "),cfe=a("code"),vHo=o("model_type"),THo=o(` property of the config object (either
passed as an argument or loaded from `),ffe=a("code"),FHo=o("pretrained_model_name_or_path"),CHo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),mfe=a("code"),MHo=o("pretrained_model_name_or_path"),EHo=o(":"),yHo=l(),H=a("ul"),pv=a("li"),gfe=a("strong"),wHo=o("albert"),AHo=o(" \u2014 "),AD=a("a"),LHo=o("TFAlbertForPreTraining"),BHo=o(" (ALBERT model)"),kHo=l(),_v=a("li"),hfe=a("strong"),xHo=o("bart"),RHo=o(" \u2014 "),LD=a("a"),SHo=o("TFBartForConditionalGeneration"),PHo=o(" (BART model)"),$Ho=l(),uv=a("li"),pfe=a("strong"),IHo=o("bert"),jHo=o(" \u2014 "),BD=a("a"),NHo=o("TFBertForPreTraining"),DHo=o(" (BERT model)"),qHo=l(),bv=a("li"),_fe=a("strong"),GHo=o("camembert"),OHo=o(" \u2014 "),kD=a("a"),XHo=o("TFCamembertForMaskedLM"),zHo=o(" (CamemBERT model)"),VHo=l(),vv=a("li"),ufe=a("strong"),WHo=o("ctrl"),QHo=o(" \u2014 "),xD=a("a"),HHo=o("TFCTRLLMHeadModel"),UHo=o(" (CTRL model)"),JHo=l(),Tv=a("li"),bfe=a("strong"),YHo=o("distilbert"),KHo=o(" \u2014 "),RD=a("a"),ZHo=o("TFDistilBertForMaskedLM"),eUo=o(" (DistilBERT model)"),oUo=l(),Fv=a("li"),vfe=a("strong"),rUo=o("electra"),tUo=o(" \u2014 "),SD=a("a"),aUo=o("TFElectraForPreTraining"),nUo=o(" (ELECTRA model)"),sUo=l(),Cv=a("li"),Tfe=a("strong"),lUo=o("flaubert"),iUo=o(" \u2014 "),PD=a("a"),dUo=o("TFFlaubertWithLMHeadModel"),cUo=o(" (FlauBERT model)"),fUo=l(),Mv=a("li"),Ffe=a("strong"),mUo=o("funnel"),gUo=o(" \u2014 "),$D=a("a"),hUo=o("TFFunnelForPreTraining"),pUo=o(" (Funnel Transformer model)"),_Uo=l(),Ev=a("li"),Cfe=a("strong"),uUo=o("gpt2"),bUo=o(" \u2014 "),ID=a("a"),vUo=o("TFGPT2LMHeadModel"),TUo=o(" (OpenAI GPT-2 model)"),FUo=l(),yv=a("li"),Mfe=a("strong"),CUo=o("layoutlm"),MUo=o(" \u2014 "),jD=a("a"),EUo=o("TFLayoutLMForMaskedLM"),yUo=o(" (LayoutLM model)"),wUo=l(),wv=a("li"),Efe=a("strong"),AUo=o("lxmert"),LUo=o(" \u2014 "),ND=a("a"),BUo=o("TFLxmertForPreTraining"),kUo=o(" (LXMERT model)"),xUo=l(),Av=a("li"),yfe=a("strong"),RUo=o("mobilebert"),SUo=o(" \u2014 "),DD=a("a"),PUo=o("TFMobileBertForPreTraining"),$Uo=o(" (MobileBERT model)"),IUo=l(),Lv=a("li"),wfe=a("strong"),jUo=o("mpnet"),NUo=o(" \u2014 "),qD=a("a"),DUo=o("TFMPNetForMaskedLM"),qUo=o(" (MPNet model)"),GUo=l(),Bv=a("li"),Afe=a("strong"),OUo=o("openai-gpt"),XUo=o(" \u2014 "),GD=a("a"),zUo=o("TFOpenAIGPTLMHeadModel"),VUo=o(" (OpenAI GPT model)"),WUo=l(),kv=a("li"),Lfe=a("strong"),QUo=o("roberta"),HUo=o(" \u2014 "),OD=a("a"),UUo=o("TFRobertaForMaskedLM"),JUo=o(" (RoBERTa model)"),YUo=l(),xv=a("li"),Bfe=a("strong"),KUo=o("t5"),ZUo=o(" \u2014 "),XD=a("a"),eJo=o("TFT5ForConditionalGeneration"),oJo=o(" (T5 model)"),rJo=l(),Rv=a("li"),kfe=a("strong"),tJo=o("tapas"),aJo=o(" \u2014 "),zD=a("a"),nJo=o("TFTapasForMaskedLM"),sJo=o(" (TAPAS model)"),lJo=l(),Sv=a("li"),xfe=a("strong"),iJo=o("transfo-xl"),dJo=o(" \u2014 "),VD=a("a"),cJo=o("TFTransfoXLLMHeadModel"),fJo=o(" (Transformer-XL model)"),mJo=l(),Pv=a("li"),Rfe=a("strong"),gJo=o("xlm"),hJo=o(" \u2014 "),WD=a("a"),pJo=o("TFXLMWithLMHeadModel"),_Jo=o(" (XLM model)"),uJo=l(),$v=a("li"),Sfe=a("strong"),bJo=o("xlm-roberta"),vJo=o(" \u2014 "),QD=a("a"),TJo=o("TFXLMRobertaForMaskedLM"),FJo=o(" (XLM-RoBERTa model)"),CJo=l(),Iv=a("li"),Pfe=a("strong"),MJo=o("xlnet"),EJo=o(" \u2014 "),HD=a("a"),yJo=o("TFXLNetLMHeadModel"),wJo=o(" (XLNet model)"),AJo=l(),$fe=a("p"),LJo=o("Examples:"),BJo=l(),f(Wy.$$.fragment),w9e=l(),lc=a("h2"),jv=a("a"),Ife=a("span"),f(Qy.$$.fragment),kJo=l(),jfe=a("span"),xJo=o("TFAutoModelForCausalLM"),A9e=l(),pr=a("div"),f(Hy.$$.fragment),RJo=l(),ic=a("p"),SJo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a causal language modeling head) when created
with the `),Nfe=a("code"),PJo=o("from_pretrained()"),$Jo=o("class method or the "),Dfe=a("code"),IJo=o("from_config()"),jJo=o(`class
method.`),NJo=l(),Uy=a("p"),DJo=o("This class cannot be instantiated directly using "),qfe=a("code"),qJo=o("__init__()"),GJo=o(" (throws an error)."),OJo=l(),it=a("div"),f(Jy.$$.fragment),XJo=l(),Gfe=a("p"),zJo=o("Instantiates one of the model classes of the library (with a causal language modeling head) from a configuration."),VJo=l(),dc=a("p"),WJo=o(`Note:
Loading a model from its configuration file does `),Ofe=a("strong"),QJo=o("not"),HJo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Xfe=a("code"),UJo=o("from_pretrained()"),JJo=o("to load the model weights."),YJo=l(),zfe=a("p"),KJo=o("Examples:"),ZJo=l(),f(Yy.$$.fragment),eYo=l(),po=a("div"),f(Ky.$$.fragment),oYo=l(),Vfe=a("p"),rYo=o("Instantiate one of the model classes of the library (with a causal language modeling head) from a pretrained model."),tYo=l(),cn=a("p"),aYo=o("The model class to instantiate is selected based on the "),Wfe=a("code"),nYo=o("model_type"),sYo=o(` property of the config object (either
passed as an argument or loaded from `),Qfe=a("code"),lYo=o("pretrained_model_name_or_path"),iYo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Hfe=a("code"),dYo=o("pretrained_model_name_or_path"),cYo=o(":"),fYo=l(),pe=a("ul"),Nv=a("li"),Ufe=a("strong"),mYo=o("bert"),gYo=o(" \u2014 "),UD=a("a"),hYo=o("TFBertLMHeadModel"),pYo=o(" (BERT model)"),_Yo=l(),Dv=a("li"),Jfe=a("strong"),uYo=o("ctrl"),bYo=o(" \u2014 "),JD=a("a"),vYo=o("TFCTRLLMHeadModel"),TYo=o(" (CTRL model)"),FYo=l(),qv=a("li"),Yfe=a("strong"),CYo=o("gpt2"),MYo=o(" \u2014 "),YD=a("a"),EYo=o("TFGPT2LMHeadModel"),yYo=o(" (OpenAI GPT-2 model)"),wYo=l(),Gv=a("li"),Kfe=a("strong"),AYo=o("openai-gpt"),LYo=o(" \u2014 "),KD=a("a"),BYo=o("TFOpenAIGPTLMHeadModel"),kYo=o(" (OpenAI GPT model)"),xYo=l(),Ov=a("li"),Zfe=a("strong"),RYo=o("rembert"),SYo=o(" \u2014 "),ZD=a("a"),PYo=o("TFRemBertForCausalLM"),$Yo=o(" (RemBERT model)"),IYo=l(),Xv=a("li"),eme=a("strong"),jYo=o("roberta"),NYo=o(" \u2014 "),eq=a("a"),DYo=o("TFRobertaForCausalLM"),qYo=o(" (RoBERTa model)"),GYo=l(),zv=a("li"),ome=a("strong"),OYo=o("roformer"),XYo=o(" \u2014 "),oq=a("a"),zYo=o("TFRoFormerForCausalLM"),VYo=o(" (RoFormer model)"),WYo=l(),Vv=a("li"),rme=a("strong"),QYo=o("transfo-xl"),HYo=o(" \u2014 "),rq=a("a"),UYo=o("TFTransfoXLLMHeadModel"),JYo=o(" (Transformer-XL model)"),YYo=l(),Wv=a("li"),tme=a("strong"),KYo=o("xlm"),ZYo=o(" \u2014 "),tq=a("a"),eKo=o("TFXLMWithLMHeadModel"),oKo=o(" (XLM model)"),rKo=l(),Qv=a("li"),ame=a("strong"),tKo=o("xlnet"),aKo=o(" \u2014 "),aq=a("a"),nKo=o("TFXLNetLMHeadModel"),sKo=o(" (XLNet model)"),lKo=l(),nme=a("p"),iKo=o("Examples:"),dKo=l(),f(Zy.$$.fragment),L9e=l(),cc=a("h2"),Hv=a("a"),sme=a("span"),f(ew.$$.fragment),cKo=l(),lme=a("span"),fKo=o("TFAutoModelForImageClassification"),B9e=l(),_r=a("div"),f(ow.$$.fragment),mKo=l(),fc=a("p"),gKo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a image classification head) when created
with the `),ime=a("code"),hKo=o("from_pretrained()"),pKo=o("class method or the "),dme=a("code"),_Ko=o("from_config()"),uKo=o(`class
method.`),bKo=l(),rw=a("p"),vKo=o("This class cannot be instantiated directly using "),cme=a("code"),TKo=o("__init__()"),FKo=o(" (throws an error)."),CKo=l(),dt=a("div"),f(tw.$$.fragment),MKo=l(),fme=a("p"),EKo=o("Instantiates one of the model classes of the library (with a image classification head) from a configuration."),yKo=l(),mc=a("p"),wKo=o(`Note:
Loading a model from its configuration file does `),mme=a("strong"),AKo=o("not"),LKo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),gme=a("code"),BKo=o("from_pretrained()"),kKo=o("to load the model weights."),xKo=l(),hme=a("p"),RKo=o("Examples:"),SKo=l(),f(aw.$$.fragment),PKo=l(),_o=a("div"),f(nw.$$.fragment),$Ko=l(),pme=a("p"),IKo=o("Instantiate one of the model classes of the library (with a image classification head) from a pretrained model."),jKo=l(),fn=a("p"),NKo=o("The model class to instantiate is selected based on the "),_me=a("code"),DKo=o("model_type"),qKo=o(` property of the config object (either
passed as an argument or loaded from `),ume=a("code"),GKo=o("pretrained_model_name_or_path"),OKo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),bme=a("code"),XKo=o("pretrained_model_name_or_path"),zKo=o(":"),VKo=l(),vme=a("ul"),Uv=a("li"),Tme=a("strong"),WKo=o("vit"),QKo=o(" \u2014 "),nq=a("a"),HKo=o("TFViTForImageClassification"),UKo=o(" (ViT model)"),JKo=l(),Fme=a("p"),YKo=o("Examples:"),KKo=l(),f(sw.$$.fragment),k9e=l(),gc=a("h2"),Jv=a("a"),Cme=a("span"),f(lw.$$.fragment),ZKo=l(),Mme=a("span"),eZo=o("TFAutoModelForMaskedLM"),x9e=l(),ur=a("div"),f(iw.$$.fragment),oZo=l(),hc=a("p"),rZo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a masked language modeling head) when created
with the `),Eme=a("code"),tZo=o("from_pretrained()"),aZo=o("class method or the "),yme=a("code"),nZo=o("from_config()"),sZo=o(`class
method.`),lZo=l(),dw=a("p"),iZo=o("This class cannot be instantiated directly using "),wme=a("code"),dZo=o("__init__()"),cZo=o(" (throws an error)."),fZo=l(),ct=a("div"),f(cw.$$.fragment),mZo=l(),Ame=a("p"),gZo=o("Instantiates one of the model classes of the library (with a masked language modeling head) from a configuration."),hZo=l(),pc=a("p"),pZo=o(`Note:
Loading a model from its configuration file does `),Lme=a("strong"),_Zo=o("not"),uZo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Bme=a("code"),bZo=o("from_pretrained()"),vZo=o("to load the model weights."),TZo=l(),kme=a("p"),FZo=o("Examples:"),CZo=l(),f(fw.$$.fragment),MZo=l(),uo=a("div"),f(mw.$$.fragment),EZo=l(),xme=a("p"),yZo=o("Instantiate one of the model classes of the library (with a masked language modeling head) from a pretrained model."),wZo=l(),mn=a("p"),AZo=o("The model class to instantiate is selected based on the "),Rme=a("code"),LZo=o("model_type"),BZo=o(` property of the config object (either
passed as an argument or loaded from `),Sme=a("code"),kZo=o("pretrained_model_name_or_path"),xZo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Pme=a("code"),RZo=o("pretrained_model_name_or_path"),SZo=o(":"),PZo=l(),Y=a("ul"),Yv=a("li"),$me=a("strong"),$Zo=o("albert"),IZo=o(" \u2014 "),sq=a("a"),jZo=o("TFAlbertForMaskedLM"),NZo=o(" (ALBERT model)"),DZo=l(),Kv=a("li"),Ime=a("strong"),qZo=o("bert"),GZo=o(" \u2014 "),lq=a("a"),OZo=o("TFBertForMaskedLM"),XZo=o(" (BERT model)"),zZo=l(),Zv=a("li"),jme=a("strong"),VZo=o("camembert"),WZo=o(" \u2014 "),iq=a("a"),QZo=o("TFCamembertForMaskedLM"),HZo=o(" (CamemBERT model)"),UZo=l(),e0=a("li"),Nme=a("strong"),JZo=o("convbert"),YZo=o(" \u2014 "),dq=a("a"),KZo=o("TFConvBertForMaskedLM"),ZZo=o(" (ConvBERT model)"),eer=l(),o0=a("li"),Dme=a("strong"),oer=o("deberta"),rer=o(" \u2014 "),cq=a("a"),ter=o("TFDebertaForMaskedLM"),aer=o(" (DeBERTa model)"),ner=l(),r0=a("li"),qme=a("strong"),ser=o("deberta-v2"),ler=o(" \u2014 "),fq=a("a"),ier=o("TFDebertaV2ForMaskedLM"),der=o(" (DeBERTa-v2 model)"),cer=l(),t0=a("li"),Gme=a("strong"),fer=o("distilbert"),mer=o(" \u2014 "),mq=a("a"),ger=o("TFDistilBertForMaskedLM"),her=o(" (DistilBERT model)"),per=l(),a0=a("li"),Ome=a("strong"),_er=o("electra"),uer=o(" \u2014 "),gq=a("a"),ber=o("TFElectraForMaskedLM"),ver=o(" (ELECTRA model)"),Ter=l(),n0=a("li"),Xme=a("strong"),Fer=o("flaubert"),Cer=o(" \u2014 "),hq=a("a"),Mer=o("TFFlaubertWithLMHeadModel"),Eer=o(" (FlauBERT model)"),yer=l(),s0=a("li"),zme=a("strong"),wer=o("funnel"),Aer=o(" \u2014 "),pq=a("a"),Ler=o("TFFunnelForMaskedLM"),Ber=o(" (Funnel Transformer model)"),ker=l(),l0=a("li"),Vme=a("strong"),xer=o("layoutlm"),Rer=o(" \u2014 "),_q=a("a"),Ser=o("TFLayoutLMForMaskedLM"),Per=o(" (LayoutLM model)"),$er=l(),i0=a("li"),Wme=a("strong"),Ier=o("longformer"),jer=o(" \u2014 "),uq=a("a"),Ner=o("TFLongformerForMaskedLM"),Der=o(" (Longformer model)"),qer=l(),d0=a("li"),Qme=a("strong"),Ger=o("mobilebert"),Oer=o(" \u2014 "),bq=a("a"),Xer=o("TFMobileBertForMaskedLM"),zer=o(" (MobileBERT model)"),Ver=l(),c0=a("li"),Hme=a("strong"),Wer=o("mpnet"),Qer=o(" \u2014 "),vq=a("a"),Her=o("TFMPNetForMaskedLM"),Uer=o(" (MPNet model)"),Jer=l(),f0=a("li"),Ume=a("strong"),Yer=o("rembert"),Ker=o(" \u2014 "),Tq=a("a"),Zer=o("TFRemBertForMaskedLM"),eor=o(" (RemBERT model)"),oor=l(),m0=a("li"),Jme=a("strong"),ror=o("roberta"),tor=o(" \u2014 "),Fq=a("a"),aor=o("TFRobertaForMaskedLM"),nor=o(" (RoBERTa model)"),sor=l(),g0=a("li"),Yme=a("strong"),lor=o("roformer"),ior=o(" \u2014 "),Cq=a("a"),dor=o("TFRoFormerForMaskedLM"),cor=o(" (RoFormer model)"),mor=l(),h0=a("li"),Kme=a("strong"),gor=o("tapas"),hor=o(" \u2014 "),Mq=a("a"),por=o("TFTapasForMaskedLM"),_or=o(" (TAPAS model)"),uor=l(),p0=a("li"),Zme=a("strong"),bor=o("xlm"),vor=o(" \u2014 "),Eq=a("a"),Tor=o("TFXLMWithLMHeadModel"),For=o(" (XLM model)"),Cor=l(),_0=a("li"),ege=a("strong"),Mor=o("xlm-roberta"),Eor=o(" \u2014 "),yq=a("a"),yor=o("TFXLMRobertaForMaskedLM"),wor=o(" (XLM-RoBERTa model)"),Aor=l(),oge=a("p"),Lor=o("Examples:"),Bor=l(),f(gw.$$.fragment),R9e=l(),_c=a("h2"),u0=a("a"),rge=a("span"),f(hw.$$.fragment),kor=l(),tge=a("span"),xor=o("TFAutoModelForSeq2SeqLM"),S9e=l(),br=a("div"),f(pw.$$.fragment),Ror=l(),uc=a("p"),Sor=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence language modeling head) when created
with the `),age=a("code"),Por=o("from_pretrained()"),$or=o("class method or the "),nge=a("code"),Ior=o("from_config()"),jor=o(`class
method.`),Nor=l(),_w=a("p"),Dor=o("This class cannot be instantiated directly using "),sge=a("code"),qor=o("__init__()"),Gor=o(" (throws an error)."),Oor=l(),ft=a("div"),f(uw.$$.fragment),Xor=l(),lge=a("p"),zor=o("Instantiates one of the model classes of the library (with a sequence-to-sequence language modeling head) from a configuration."),Vor=l(),bc=a("p"),Wor=o(`Note:
Loading a model from its configuration file does `),ige=a("strong"),Qor=o("not"),Hor=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),dge=a("code"),Uor=o("from_pretrained()"),Jor=o("to load the model weights."),Yor=l(),cge=a("p"),Kor=o("Examples:"),Zor=l(),f(bw.$$.fragment),err=l(),bo=a("div"),f(vw.$$.fragment),orr=l(),fge=a("p"),rrr=o("Instantiate one of the model classes of the library (with a sequence-to-sequence language modeling head) from a pretrained model."),trr=l(),gn=a("p"),arr=o("The model class to instantiate is selected based on the "),mge=a("code"),nrr=o("model_type"),srr=o(` property of the config object (either
passed as an argument or loaded from `),gge=a("code"),lrr=o("pretrained_model_name_or_path"),irr=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),hge=a("code"),drr=o("pretrained_model_name_or_path"),crr=o(":"),frr=l(),_e=a("ul"),b0=a("li"),pge=a("strong"),mrr=o("bart"),grr=o(" \u2014 "),wq=a("a"),hrr=o("TFBartForConditionalGeneration"),prr=o(" (BART model)"),_rr=l(),v0=a("li"),_ge=a("strong"),urr=o("blenderbot"),brr=o(" \u2014 "),Aq=a("a"),vrr=o("TFBlenderbotForConditionalGeneration"),Trr=o(" (Blenderbot model)"),Frr=l(),T0=a("li"),uge=a("strong"),Crr=o("blenderbot-small"),Mrr=o(" \u2014 "),Lq=a("a"),Err=o("TFBlenderbotSmallForConditionalGeneration"),yrr=o(" (BlenderbotSmall model)"),wrr=l(),F0=a("li"),bge=a("strong"),Arr=o("encoder-decoder"),Lrr=o(" \u2014 "),Bq=a("a"),Brr=o("TFEncoderDecoderModel"),krr=o(" (Encoder decoder model)"),xrr=l(),C0=a("li"),vge=a("strong"),Rrr=o("led"),Srr=o(" \u2014 "),kq=a("a"),Prr=o("TFLEDForConditionalGeneration"),$rr=o(" (LED model)"),Irr=l(),M0=a("li"),Tge=a("strong"),jrr=o("marian"),Nrr=o(" \u2014 "),xq=a("a"),Drr=o("TFMarianMTModel"),qrr=o(" (Marian model)"),Grr=l(),E0=a("li"),Fge=a("strong"),Orr=o("mbart"),Xrr=o(" \u2014 "),Rq=a("a"),zrr=o("TFMBartForConditionalGeneration"),Vrr=o(" (mBART model)"),Wrr=l(),y0=a("li"),Cge=a("strong"),Qrr=o("mt5"),Hrr=o(" \u2014 "),Sq=a("a"),Urr=o("TFMT5ForConditionalGeneration"),Jrr=o(" (mT5 model)"),Yrr=l(),w0=a("li"),Mge=a("strong"),Krr=o("pegasus"),Zrr=o(" \u2014 "),Pq=a("a"),etr=o("TFPegasusForConditionalGeneration"),otr=o(" (Pegasus model)"),rtr=l(),A0=a("li"),Ege=a("strong"),ttr=o("t5"),atr=o(" \u2014 "),$q=a("a"),ntr=o("TFT5ForConditionalGeneration"),str=o(" (T5 model)"),ltr=l(),yge=a("p"),itr=o("Examples:"),dtr=l(),f(Tw.$$.fragment),P9e=l(),vc=a("h2"),L0=a("a"),wge=a("span"),f(Fw.$$.fragment),ctr=l(),Age=a("span"),ftr=o("TFAutoModelForSequenceClassification"),$9e=l(),vr=a("div"),f(Cw.$$.fragment),mtr=l(),Tc=a("p"),gtr=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence classification head) when created
with the `),Lge=a("code"),htr=o("from_pretrained()"),ptr=o("class method or the "),Bge=a("code"),_tr=o("from_config()"),utr=o(`class
method.`),btr=l(),Mw=a("p"),vtr=o("This class cannot be instantiated directly using "),kge=a("code"),Ttr=o("__init__()"),Ftr=o(" (throws an error)."),Ctr=l(),mt=a("div"),f(Ew.$$.fragment),Mtr=l(),xge=a("p"),Etr=o("Instantiates one of the model classes of the library (with a sequence classification head) from a configuration."),ytr=l(),Fc=a("p"),wtr=o(`Note:
Loading a model from its configuration file does `),Rge=a("strong"),Atr=o("not"),Ltr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Sge=a("code"),Btr=o("from_pretrained()"),ktr=o("to load the model weights."),xtr=l(),Pge=a("p"),Rtr=o("Examples:"),Str=l(),f(yw.$$.fragment),Ptr=l(),vo=a("div"),f(ww.$$.fragment),$tr=l(),$ge=a("p"),Itr=o("Instantiate one of the model classes of the library (with a sequence classification head) from a pretrained model."),jtr=l(),hn=a("p"),Ntr=o("The model class to instantiate is selected based on the "),Ige=a("code"),Dtr=o("model_type"),qtr=o(` property of the config object (either
passed as an argument or loaded from `),jge=a("code"),Gtr=o("pretrained_model_name_or_path"),Otr=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Nge=a("code"),Xtr=o("pretrained_model_name_or_path"),ztr=o(":"),Vtr=l(),X=a("ul"),B0=a("li"),Dge=a("strong"),Wtr=o("albert"),Qtr=o(" \u2014 "),Iq=a("a"),Htr=o("TFAlbertForSequenceClassification"),Utr=o(" (ALBERT model)"),Jtr=l(),k0=a("li"),qge=a("strong"),Ytr=o("bert"),Ktr=o(" \u2014 "),jq=a("a"),Ztr=o("TFBertForSequenceClassification"),ear=o(" (BERT model)"),oar=l(),x0=a("li"),Gge=a("strong"),rar=o("camembert"),tar=o(" \u2014 "),Nq=a("a"),aar=o("TFCamembertForSequenceClassification"),nar=o(" (CamemBERT model)"),sar=l(),R0=a("li"),Oge=a("strong"),lar=o("convbert"),iar=o(" \u2014 "),Dq=a("a"),dar=o("TFConvBertForSequenceClassification"),car=o(" (ConvBERT model)"),far=l(),S0=a("li"),Xge=a("strong"),mar=o("ctrl"),gar=o(" \u2014 "),qq=a("a"),har=o("TFCTRLForSequenceClassification"),par=o(" (CTRL model)"),_ar=l(),P0=a("li"),zge=a("strong"),uar=o("deberta"),bar=o(" \u2014 "),Gq=a("a"),Tar=o("TFDebertaForSequenceClassification"),Far=o(" (DeBERTa model)"),Car=l(),$0=a("li"),Vge=a("strong"),Mar=o("deberta-v2"),Ear=o(" \u2014 "),Oq=a("a"),yar=o("TFDebertaV2ForSequenceClassification"),war=o(" (DeBERTa-v2 model)"),Aar=l(),I0=a("li"),Wge=a("strong"),Lar=o("distilbert"),Bar=o(" \u2014 "),Xq=a("a"),kar=o("TFDistilBertForSequenceClassification"),xar=o(" (DistilBERT model)"),Rar=l(),j0=a("li"),Qge=a("strong"),Sar=o("electra"),Par=o(" \u2014 "),zq=a("a"),$ar=o("TFElectraForSequenceClassification"),Iar=o(" (ELECTRA model)"),jar=l(),N0=a("li"),Hge=a("strong"),Nar=o("flaubert"),Dar=o(" \u2014 "),Vq=a("a"),qar=o("TFFlaubertForSequenceClassification"),Gar=o(" (FlauBERT model)"),Oar=l(),D0=a("li"),Uge=a("strong"),Xar=o("funnel"),zar=o(" \u2014 "),Wq=a("a"),Var=o("TFFunnelForSequenceClassification"),War=o(" (Funnel Transformer model)"),Qar=l(),q0=a("li"),Jge=a("strong"),Har=o("gpt2"),Uar=o(" \u2014 "),Qq=a("a"),Jar=o("TFGPT2ForSequenceClassification"),Yar=o(" (OpenAI GPT-2 model)"),Kar=l(),G0=a("li"),Yge=a("strong"),Zar=o("layoutlm"),enr=o(" \u2014 "),Hq=a("a"),onr=o("TFLayoutLMForSequenceClassification"),rnr=o(" (LayoutLM model)"),tnr=l(),O0=a("li"),Kge=a("strong"),anr=o("longformer"),nnr=o(" \u2014 "),Uq=a("a"),snr=o("TFLongformerForSequenceClassification"),lnr=o(" (Longformer model)"),inr=l(),X0=a("li"),Zge=a("strong"),dnr=o("mobilebert"),cnr=o(" \u2014 "),Jq=a("a"),fnr=o("TFMobileBertForSequenceClassification"),mnr=o(" (MobileBERT model)"),gnr=l(),z0=a("li"),ehe=a("strong"),hnr=o("mpnet"),pnr=o(" \u2014 "),Yq=a("a"),_nr=o("TFMPNetForSequenceClassification"),unr=o(" (MPNet model)"),bnr=l(),V0=a("li"),ohe=a("strong"),vnr=o("openai-gpt"),Tnr=o(" \u2014 "),Kq=a("a"),Fnr=o("TFOpenAIGPTForSequenceClassification"),Cnr=o(" (OpenAI GPT model)"),Mnr=l(),W0=a("li"),rhe=a("strong"),Enr=o("rembert"),ynr=o(" \u2014 "),Zq=a("a"),wnr=o("TFRemBertForSequenceClassification"),Anr=o(" (RemBERT model)"),Lnr=l(),Q0=a("li"),the=a("strong"),Bnr=o("roberta"),knr=o(" \u2014 "),eG=a("a"),xnr=o("TFRobertaForSequenceClassification"),Rnr=o(" (RoBERTa model)"),Snr=l(),H0=a("li"),ahe=a("strong"),Pnr=o("roformer"),$nr=o(" \u2014 "),oG=a("a"),Inr=o("TFRoFormerForSequenceClassification"),jnr=o(" (RoFormer model)"),Nnr=l(),U0=a("li"),nhe=a("strong"),Dnr=o("tapas"),qnr=o(" \u2014 "),rG=a("a"),Gnr=o("TFTapasForSequenceClassification"),Onr=o(" (TAPAS model)"),Xnr=l(),J0=a("li"),she=a("strong"),znr=o("transfo-xl"),Vnr=o(" \u2014 "),tG=a("a"),Wnr=o("TFTransfoXLForSequenceClassification"),Qnr=o(" (Transformer-XL model)"),Hnr=l(),Y0=a("li"),lhe=a("strong"),Unr=o("xlm"),Jnr=o(" \u2014 "),aG=a("a"),Ynr=o("TFXLMForSequenceClassification"),Knr=o(" (XLM model)"),Znr=l(),K0=a("li"),ihe=a("strong"),esr=o("xlm-roberta"),osr=o(" \u2014 "),nG=a("a"),rsr=o("TFXLMRobertaForSequenceClassification"),tsr=o(" (XLM-RoBERTa model)"),asr=l(),Z0=a("li"),dhe=a("strong"),nsr=o("xlnet"),ssr=o(" \u2014 "),sG=a("a"),lsr=o("TFXLNetForSequenceClassification"),isr=o(" (XLNet model)"),dsr=l(),che=a("p"),csr=o("Examples:"),fsr=l(),f(Aw.$$.fragment),I9e=l(),Cc=a("h2"),eT=a("a"),fhe=a("span"),f(Lw.$$.fragment),msr=l(),mhe=a("span"),gsr=o("TFAutoModelForMultipleChoice"),j9e=l(),Tr=a("div"),f(Bw.$$.fragment),hsr=l(),Mc=a("p"),psr=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a multiple choice head) when created
with the `),ghe=a("code"),_sr=o("from_pretrained()"),usr=o("class method or the "),hhe=a("code"),bsr=o("from_config()"),vsr=o(`class
method.`),Tsr=l(),kw=a("p"),Fsr=o("This class cannot be instantiated directly using "),phe=a("code"),Csr=o("__init__()"),Msr=o(" (throws an error)."),Esr=l(),gt=a("div"),f(xw.$$.fragment),ysr=l(),_he=a("p"),wsr=o("Instantiates one of the model classes of the library (with a multiple choice head) from a configuration."),Asr=l(),Ec=a("p"),Lsr=o(`Note:
Loading a model from its configuration file does `),uhe=a("strong"),Bsr=o("not"),ksr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),bhe=a("code"),xsr=o("from_pretrained()"),Rsr=o("to load the model weights."),Ssr=l(),vhe=a("p"),Psr=o("Examples:"),$sr=l(),f(Rw.$$.fragment),Isr=l(),To=a("div"),f(Sw.$$.fragment),jsr=l(),The=a("p"),Nsr=o("Instantiate one of the model classes of the library (with a multiple choice head) from a pretrained model."),Dsr=l(),pn=a("p"),qsr=o("The model class to instantiate is selected based on the "),Fhe=a("code"),Gsr=o("model_type"),Osr=o(` property of the config object (either
passed as an argument or loaded from `),Che=a("code"),Xsr=o("pretrained_model_name_or_path"),zsr=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Mhe=a("code"),Vsr=o("pretrained_model_name_or_path"),Wsr=o(":"),Qsr=l(),te=a("ul"),oT=a("li"),Ehe=a("strong"),Hsr=o("albert"),Usr=o(" \u2014 "),lG=a("a"),Jsr=o("TFAlbertForMultipleChoice"),Ysr=o(" (ALBERT model)"),Ksr=l(),rT=a("li"),yhe=a("strong"),Zsr=o("bert"),elr=o(" \u2014 "),iG=a("a"),olr=o("TFBertForMultipleChoice"),rlr=o(" (BERT model)"),tlr=l(),tT=a("li"),whe=a("strong"),alr=o("camembert"),nlr=o(" \u2014 "),dG=a("a"),slr=o("TFCamembertForMultipleChoice"),llr=o(" (CamemBERT model)"),ilr=l(),aT=a("li"),Ahe=a("strong"),dlr=o("convbert"),clr=o(" \u2014 "),cG=a("a"),flr=o("TFConvBertForMultipleChoice"),mlr=o(" (ConvBERT model)"),glr=l(),nT=a("li"),Lhe=a("strong"),hlr=o("distilbert"),plr=o(" \u2014 "),fG=a("a"),_lr=o("TFDistilBertForMultipleChoice"),ulr=o(" (DistilBERT model)"),blr=l(),sT=a("li"),Bhe=a("strong"),vlr=o("electra"),Tlr=o(" \u2014 "),mG=a("a"),Flr=o("TFElectraForMultipleChoice"),Clr=o(" (ELECTRA model)"),Mlr=l(),lT=a("li"),khe=a("strong"),Elr=o("flaubert"),ylr=o(" \u2014 "),gG=a("a"),wlr=o("TFFlaubertForMultipleChoice"),Alr=o(" (FlauBERT model)"),Llr=l(),iT=a("li"),xhe=a("strong"),Blr=o("funnel"),klr=o(" \u2014 "),hG=a("a"),xlr=o("TFFunnelForMultipleChoice"),Rlr=o(" (Funnel Transformer model)"),Slr=l(),dT=a("li"),Rhe=a("strong"),Plr=o("longformer"),$lr=o(" \u2014 "),pG=a("a"),Ilr=o("TFLongformerForMultipleChoice"),jlr=o(" (Longformer model)"),Nlr=l(),cT=a("li"),She=a("strong"),Dlr=o("mobilebert"),qlr=o(" \u2014 "),_G=a("a"),Glr=o("TFMobileBertForMultipleChoice"),Olr=o(" (MobileBERT model)"),Xlr=l(),fT=a("li"),Phe=a("strong"),zlr=o("mpnet"),Vlr=o(" \u2014 "),uG=a("a"),Wlr=o("TFMPNetForMultipleChoice"),Qlr=o(" (MPNet model)"),Hlr=l(),mT=a("li"),$he=a("strong"),Ulr=o("rembert"),Jlr=o(" \u2014 "),bG=a("a"),Ylr=o("TFRemBertForMultipleChoice"),Klr=o(" (RemBERT model)"),Zlr=l(),gT=a("li"),Ihe=a("strong"),eir=o("roberta"),oir=o(" \u2014 "),vG=a("a"),rir=o("TFRobertaForMultipleChoice"),tir=o(" (RoBERTa model)"),air=l(),hT=a("li"),jhe=a("strong"),nir=o("roformer"),sir=o(" \u2014 "),TG=a("a"),lir=o("TFRoFormerForMultipleChoice"),iir=o(" (RoFormer model)"),dir=l(),pT=a("li"),Nhe=a("strong"),cir=o("xlm"),fir=o(" \u2014 "),FG=a("a"),mir=o("TFXLMForMultipleChoice"),gir=o(" (XLM model)"),hir=l(),_T=a("li"),Dhe=a("strong"),pir=o("xlm-roberta"),_ir=o(" \u2014 "),CG=a("a"),uir=o("TFXLMRobertaForMultipleChoice"),bir=o(" (XLM-RoBERTa model)"),vir=l(),uT=a("li"),qhe=a("strong"),Tir=o("xlnet"),Fir=o(" \u2014 "),MG=a("a"),Cir=o("TFXLNetForMultipleChoice"),Mir=o(" (XLNet model)"),Eir=l(),Ghe=a("p"),yir=o("Examples:"),wir=l(),f(Pw.$$.fragment),N9e=l(),yc=a("h2"),bT=a("a"),Ohe=a("span"),f($w.$$.fragment),Air=l(),Xhe=a("span"),Lir=o("TFAutoModelForTableQuestionAnswering"),D9e=l(),Fr=a("div"),f(Iw.$$.fragment),Bir=l(),wc=a("p"),kir=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a table question answering head) when created
with the `),zhe=a("code"),xir=o("from_pretrained()"),Rir=o("class method or the "),Vhe=a("code"),Sir=o("from_config()"),Pir=o(`class
method.`),$ir=l(),jw=a("p"),Iir=o("This class cannot be instantiated directly using "),Whe=a("code"),jir=o("__init__()"),Nir=o(" (throws an error)."),Dir=l(),ht=a("div"),f(Nw.$$.fragment),qir=l(),Qhe=a("p"),Gir=o("Instantiates one of the model classes of the library (with a table question answering head) from a configuration."),Oir=l(),Ac=a("p"),Xir=o(`Note:
Loading a model from its configuration file does `),Hhe=a("strong"),zir=o("not"),Vir=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Uhe=a("code"),Wir=o("from_pretrained()"),Qir=o("to load the model weights."),Hir=l(),Jhe=a("p"),Uir=o("Examples:"),Jir=l(),f(Dw.$$.fragment),Yir=l(),Fo=a("div"),f(qw.$$.fragment),Kir=l(),Yhe=a("p"),Zir=o("Instantiate one of the model classes of the library (with a table question answering head) from a pretrained model."),edr=l(),_n=a("p"),odr=o("The model class to instantiate is selected based on the "),Khe=a("code"),rdr=o("model_type"),tdr=o(` property of the config object (either
passed as an argument or loaded from `),Zhe=a("code"),adr=o("pretrained_model_name_or_path"),ndr=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),epe=a("code"),sdr=o("pretrained_model_name_or_path"),ldr=o(":"),idr=l(),ope=a("ul"),vT=a("li"),rpe=a("strong"),ddr=o("tapas"),cdr=o(" \u2014 "),EG=a("a"),fdr=o("TFTapasForQuestionAnswering"),mdr=o(" (TAPAS model)"),gdr=l(),tpe=a("p"),hdr=o("Examples:"),pdr=l(),f(Gw.$$.fragment),q9e=l(),Lc=a("h2"),TT=a("a"),ape=a("span"),f(Ow.$$.fragment),_dr=l(),npe=a("span"),udr=o("TFAutoModelForTokenClassification"),G9e=l(),Cr=a("div"),f(Xw.$$.fragment),bdr=l(),Bc=a("p"),vdr=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a token classification head) when created
with the `),spe=a("code"),Tdr=o("from_pretrained()"),Fdr=o("class method or the "),lpe=a("code"),Cdr=o("from_config()"),Mdr=o(`class
method.`),Edr=l(),zw=a("p"),ydr=o("This class cannot be instantiated directly using "),ipe=a("code"),wdr=o("__init__()"),Adr=o(" (throws an error)."),Ldr=l(),pt=a("div"),f(Vw.$$.fragment),Bdr=l(),dpe=a("p"),kdr=o("Instantiates one of the model classes of the library (with a token classification head) from a configuration."),xdr=l(),kc=a("p"),Rdr=o(`Note:
Loading a model from its configuration file does `),cpe=a("strong"),Sdr=o("not"),Pdr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),fpe=a("code"),$dr=o("from_pretrained()"),Idr=o("to load the model weights."),jdr=l(),mpe=a("p"),Ndr=o("Examples:"),Ddr=l(),f(Ww.$$.fragment),qdr=l(),Co=a("div"),f(Qw.$$.fragment),Gdr=l(),gpe=a("p"),Odr=o("Instantiate one of the model classes of the library (with a token classification head) from a pretrained model."),Xdr=l(),un=a("p"),zdr=o("The model class to instantiate is selected based on the "),hpe=a("code"),Vdr=o("model_type"),Wdr=o(` property of the config object (either
passed as an argument or loaded from `),ppe=a("code"),Qdr=o("pretrained_model_name_or_path"),Hdr=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),_pe=a("code"),Udr=o("pretrained_model_name_or_path"),Jdr=o(":"),Ydr=l(),K=a("ul"),FT=a("li"),upe=a("strong"),Kdr=o("albert"),Zdr=o(" \u2014 "),yG=a("a"),ecr=o("TFAlbertForTokenClassification"),ocr=o(" (ALBERT model)"),rcr=l(),CT=a("li"),bpe=a("strong"),tcr=o("bert"),acr=o(" \u2014 "),wG=a("a"),ncr=o("TFBertForTokenClassification"),scr=o(" (BERT model)"),lcr=l(),MT=a("li"),vpe=a("strong"),icr=o("camembert"),dcr=o(" \u2014 "),AG=a("a"),ccr=o("TFCamembertForTokenClassification"),fcr=o(" (CamemBERT model)"),mcr=l(),ET=a("li"),Tpe=a("strong"),gcr=o("convbert"),hcr=o(" \u2014 "),LG=a("a"),pcr=o("TFConvBertForTokenClassification"),_cr=o(" (ConvBERT model)"),ucr=l(),yT=a("li"),Fpe=a("strong"),bcr=o("deberta"),vcr=o(" \u2014 "),BG=a("a"),Tcr=o("TFDebertaForTokenClassification"),Fcr=o(" (DeBERTa model)"),Ccr=l(),wT=a("li"),Cpe=a("strong"),Mcr=o("deberta-v2"),Ecr=o(" \u2014 "),kG=a("a"),ycr=o("TFDebertaV2ForTokenClassification"),wcr=o(" (DeBERTa-v2 model)"),Acr=l(),AT=a("li"),Mpe=a("strong"),Lcr=o("distilbert"),Bcr=o(" \u2014 "),xG=a("a"),kcr=o("TFDistilBertForTokenClassification"),xcr=o(" (DistilBERT model)"),Rcr=l(),LT=a("li"),Epe=a("strong"),Scr=o("electra"),Pcr=o(" \u2014 "),RG=a("a"),$cr=o("TFElectraForTokenClassification"),Icr=o(" (ELECTRA model)"),jcr=l(),BT=a("li"),ype=a("strong"),Ncr=o("flaubert"),Dcr=o(" \u2014 "),SG=a("a"),qcr=o("TFFlaubertForTokenClassification"),Gcr=o(" (FlauBERT model)"),Ocr=l(),kT=a("li"),wpe=a("strong"),Xcr=o("funnel"),zcr=o(" \u2014 "),PG=a("a"),Vcr=o("TFFunnelForTokenClassification"),Wcr=o(" (Funnel Transformer model)"),Qcr=l(),xT=a("li"),Ape=a("strong"),Hcr=o("layoutlm"),Ucr=o(" \u2014 "),$G=a("a"),Jcr=o("TFLayoutLMForTokenClassification"),Ycr=o(" (LayoutLM model)"),Kcr=l(),RT=a("li"),Lpe=a("strong"),Zcr=o("longformer"),efr=o(" \u2014 "),IG=a("a"),ofr=o("TFLongformerForTokenClassification"),rfr=o(" (Longformer model)"),tfr=l(),ST=a("li"),Bpe=a("strong"),afr=o("mobilebert"),nfr=o(" \u2014 "),jG=a("a"),sfr=o("TFMobileBertForTokenClassification"),lfr=o(" (MobileBERT model)"),ifr=l(),PT=a("li"),kpe=a("strong"),dfr=o("mpnet"),cfr=o(" \u2014 "),NG=a("a"),ffr=o("TFMPNetForTokenClassification"),mfr=o(" (MPNet model)"),gfr=l(),$T=a("li"),xpe=a("strong"),hfr=o("rembert"),pfr=o(" \u2014 "),DG=a("a"),_fr=o("TFRemBertForTokenClassification"),ufr=o(" (RemBERT model)"),bfr=l(),IT=a("li"),Rpe=a("strong"),vfr=o("roberta"),Tfr=o(" \u2014 "),qG=a("a"),Ffr=o("TFRobertaForTokenClassification"),Cfr=o(" (RoBERTa model)"),Mfr=l(),jT=a("li"),Spe=a("strong"),Efr=o("roformer"),yfr=o(" \u2014 "),GG=a("a"),wfr=o("TFRoFormerForTokenClassification"),Afr=o(" (RoFormer model)"),Lfr=l(),NT=a("li"),Ppe=a("strong"),Bfr=o("xlm"),kfr=o(" \u2014 "),OG=a("a"),xfr=o("TFXLMForTokenClassification"),Rfr=o(" (XLM model)"),Sfr=l(),DT=a("li"),$pe=a("strong"),Pfr=o("xlm-roberta"),$fr=o(" \u2014 "),XG=a("a"),Ifr=o("TFXLMRobertaForTokenClassification"),jfr=o(" (XLM-RoBERTa model)"),Nfr=l(),qT=a("li"),Ipe=a("strong"),Dfr=o("xlnet"),qfr=o(" \u2014 "),zG=a("a"),Gfr=o("TFXLNetForTokenClassification"),Ofr=o(" (XLNet model)"),Xfr=l(),jpe=a("p"),zfr=o("Examples:"),Vfr=l(),f(Hw.$$.fragment),O9e=l(),xc=a("h2"),GT=a("a"),Npe=a("span"),f(Uw.$$.fragment),Wfr=l(),Dpe=a("span"),Qfr=o("TFAutoModelForQuestionAnswering"),X9e=l(),Mr=a("div"),f(Jw.$$.fragment),Hfr=l(),Rc=a("p"),Ufr=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a question answering head) when created
with the `),qpe=a("code"),Jfr=o("from_pretrained()"),Yfr=o("class method or the "),Gpe=a("code"),Kfr=o("from_config()"),Zfr=o(`class
method.`),emr=l(),Yw=a("p"),omr=o("This class cannot be instantiated directly using "),Ope=a("code"),rmr=o("__init__()"),tmr=o(" (throws an error)."),amr=l(),_t=a("div"),f(Kw.$$.fragment),nmr=l(),Xpe=a("p"),smr=o("Instantiates one of the model classes of the library (with a question answering head) from a configuration."),lmr=l(),Sc=a("p"),imr=o(`Note:
Loading a model from its configuration file does `),zpe=a("strong"),dmr=o("not"),cmr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Vpe=a("code"),fmr=o("from_pretrained()"),mmr=o("to load the model weights."),gmr=l(),Wpe=a("p"),hmr=o("Examples:"),pmr=l(),f(Zw.$$.fragment),_mr=l(),Mo=a("div"),f(e6.$$.fragment),umr=l(),Qpe=a("p"),bmr=o("Instantiate one of the model classes of the library (with a question answering head) from a pretrained model."),vmr=l(),bn=a("p"),Tmr=o("The model class to instantiate is selected based on the "),Hpe=a("code"),Fmr=o("model_type"),Cmr=o(` property of the config object (either
passed as an argument or loaded from `),Upe=a("code"),Mmr=o("pretrained_model_name_or_path"),Emr=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Jpe=a("code"),ymr=o("pretrained_model_name_or_path"),wmr=o(":"),Amr=l(),Z=a("ul"),OT=a("li"),Ype=a("strong"),Lmr=o("albert"),Bmr=o(" \u2014 "),VG=a("a"),kmr=o("TFAlbertForQuestionAnswering"),xmr=o(" (ALBERT model)"),Rmr=l(),XT=a("li"),Kpe=a("strong"),Smr=o("bert"),Pmr=o(" \u2014 "),WG=a("a"),$mr=o("TFBertForQuestionAnswering"),Imr=o(" (BERT model)"),jmr=l(),zT=a("li"),Zpe=a("strong"),Nmr=o("camembert"),Dmr=o(" \u2014 "),QG=a("a"),qmr=o("TFCamembertForQuestionAnswering"),Gmr=o(" (CamemBERT model)"),Omr=l(),VT=a("li"),e_e=a("strong"),Xmr=o("convbert"),zmr=o(" \u2014 "),HG=a("a"),Vmr=o("TFConvBertForQuestionAnswering"),Wmr=o(" (ConvBERT model)"),Qmr=l(),WT=a("li"),o_e=a("strong"),Hmr=o("deberta"),Umr=o(" \u2014 "),UG=a("a"),Jmr=o("TFDebertaForQuestionAnswering"),Ymr=o(" (DeBERTa model)"),Kmr=l(),QT=a("li"),r_e=a("strong"),Zmr=o("deberta-v2"),egr=o(" \u2014 "),JG=a("a"),ogr=o("TFDebertaV2ForQuestionAnswering"),rgr=o(" (DeBERTa-v2 model)"),tgr=l(),HT=a("li"),t_e=a("strong"),agr=o("distilbert"),ngr=o(" \u2014 "),YG=a("a"),sgr=o("TFDistilBertForQuestionAnswering"),lgr=o(" (DistilBERT model)"),igr=l(),UT=a("li"),a_e=a("strong"),dgr=o("electra"),cgr=o(" \u2014 "),KG=a("a"),fgr=o("TFElectraForQuestionAnswering"),mgr=o(" (ELECTRA model)"),ggr=l(),JT=a("li"),n_e=a("strong"),hgr=o("flaubert"),pgr=o(" \u2014 "),ZG=a("a"),_gr=o("TFFlaubertForQuestionAnsweringSimple"),ugr=o(" (FlauBERT model)"),bgr=l(),YT=a("li"),s_e=a("strong"),vgr=o("funnel"),Tgr=o(" \u2014 "),eO=a("a"),Fgr=o("TFFunnelForQuestionAnswering"),Cgr=o(" (Funnel Transformer model)"),Mgr=l(),KT=a("li"),l_e=a("strong"),Egr=o("longformer"),ygr=o(" \u2014 "),oO=a("a"),wgr=o("TFLongformerForQuestionAnswering"),Agr=o(" (Longformer model)"),Lgr=l(),ZT=a("li"),i_e=a("strong"),Bgr=o("mobilebert"),kgr=o(" \u2014 "),rO=a("a"),xgr=o("TFMobileBertForQuestionAnswering"),Rgr=o(" (MobileBERT model)"),Sgr=l(),eF=a("li"),d_e=a("strong"),Pgr=o("mpnet"),$gr=o(" \u2014 "),tO=a("a"),Igr=o("TFMPNetForQuestionAnswering"),jgr=o(" (MPNet model)"),Ngr=l(),oF=a("li"),c_e=a("strong"),Dgr=o("rembert"),qgr=o(" \u2014 "),aO=a("a"),Ggr=o("TFRemBertForQuestionAnswering"),Ogr=o(" (RemBERT model)"),Xgr=l(),rF=a("li"),f_e=a("strong"),zgr=o("roberta"),Vgr=o(" \u2014 "),nO=a("a"),Wgr=o("TFRobertaForQuestionAnswering"),Qgr=o(" (RoBERTa model)"),Hgr=l(),tF=a("li"),m_e=a("strong"),Ugr=o("roformer"),Jgr=o(" \u2014 "),sO=a("a"),Ygr=o("TFRoFormerForQuestionAnswering"),Kgr=o(" (RoFormer model)"),Zgr=l(),aF=a("li"),g_e=a("strong"),ehr=o("xlm"),ohr=o(" \u2014 "),lO=a("a"),rhr=o("TFXLMForQuestionAnsweringSimple"),thr=o(" (XLM model)"),ahr=l(),nF=a("li"),h_e=a("strong"),nhr=o("xlm-roberta"),shr=o(" \u2014 "),iO=a("a"),lhr=o("TFXLMRobertaForQuestionAnswering"),ihr=o(" (XLM-RoBERTa model)"),dhr=l(),sF=a("li"),p_e=a("strong"),chr=o("xlnet"),fhr=o(" \u2014 "),dO=a("a"),mhr=o("TFXLNetForQuestionAnsweringSimple"),ghr=o(" (XLNet model)"),hhr=l(),__e=a("p"),phr=o("Examples:"),_hr=l(),f(o6.$$.fragment),z9e=l(),Pc=a("h2"),lF=a("a"),u_e=a("span"),f(r6.$$.fragment),uhr=l(),b_e=a("span"),bhr=o("TFAutoModelForVision2Seq"),V9e=l(),Er=a("div"),f(t6.$$.fragment),vhr=l(),$c=a("p"),Thr=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a vision-to-text modeling head) when created
with the `),v_e=a("code"),Fhr=o("from_pretrained()"),Chr=o("class method or the "),T_e=a("code"),Mhr=o("from_config()"),Ehr=o(`class
method.`),yhr=l(),a6=a("p"),whr=o("This class cannot be instantiated directly using "),F_e=a("code"),Ahr=o("__init__()"),Lhr=o(" (throws an error)."),Bhr=l(),ut=a("div"),f(n6.$$.fragment),khr=l(),C_e=a("p"),xhr=o("Instantiates one of the model classes of the library (with a vision-to-text modeling head) from a configuration."),Rhr=l(),Ic=a("p"),Shr=o(`Note:
Loading a model from its configuration file does `),M_e=a("strong"),Phr=o("not"),$hr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),E_e=a("code"),Ihr=o("from_pretrained()"),jhr=o("to load the model weights."),Nhr=l(),y_e=a("p"),Dhr=o("Examples:"),qhr=l(),f(s6.$$.fragment),Ghr=l(),Eo=a("div"),f(l6.$$.fragment),Ohr=l(),w_e=a("p"),Xhr=o("Instantiate one of the model classes of the library (with a vision-to-text modeling head) from a pretrained model."),zhr=l(),vn=a("p"),Vhr=o("The model class to instantiate is selected based on the "),A_e=a("code"),Whr=o("model_type"),Qhr=o(` property of the config object (either
passed as an argument or loaded from `),L_e=a("code"),Hhr=o("pretrained_model_name_or_path"),Uhr=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),B_e=a("code"),Jhr=o("pretrained_model_name_or_path"),Yhr=o(":"),Khr=l(),k_e=a("ul"),iF=a("li"),x_e=a("strong"),Zhr=o("vision-encoder-decoder"),epr=o(" \u2014 "),cO=a("a"),opr=o("TFVisionEncoderDecoderModel"),rpr=o(" (Vision Encoder decoder model)"),tpr=l(),R_e=a("p"),apr=o("Examples:"),npr=l(),f(i6.$$.fragment),W9e=l(),jc=a("h2"),dF=a("a"),S_e=a("span"),f(d6.$$.fragment),spr=l(),P_e=a("span"),lpr=o("TFAutoModelForSpeechSeq2Seq"),Q9e=l(),yr=a("div"),f(c6.$$.fragment),ipr=l(),Nc=a("p"),dpr=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) when created
with the `),$_e=a("code"),cpr=o("from_pretrained()"),fpr=o("class method or the "),I_e=a("code"),mpr=o("from_config()"),gpr=o(`class
method.`),hpr=l(),f6=a("p"),ppr=o("This class cannot be instantiated directly using "),j_e=a("code"),_pr=o("__init__()"),upr=o(" (throws an error)."),bpr=l(),bt=a("div"),f(m6.$$.fragment),vpr=l(),N_e=a("p"),Tpr=o("Instantiates one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) from a configuration."),Fpr=l(),Dc=a("p"),Cpr=o(`Note:
Loading a model from its configuration file does `),D_e=a("strong"),Mpr=o("not"),Epr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),q_e=a("code"),ypr=o("from_pretrained()"),wpr=o("to load the model weights."),Apr=l(),G_e=a("p"),Lpr=o("Examples:"),Bpr=l(),f(g6.$$.fragment),kpr=l(),yo=a("div"),f(h6.$$.fragment),xpr=l(),O_e=a("p"),Rpr=o("Instantiate one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) from a pretrained model."),Spr=l(),Tn=a("p"),Ppr=o("The model class to instantiate is selected based on the "),X_e=a("code"),$pr=o("model_type"),Ipr=o(` property of the config object (either
passed as an argument or loaded from `),z_e=a("code"),jpr=o("pretrained_model_name_or_path"),Npr=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),V_e=a("code"),Dpr=o("pretrained_model_name_or_path"),qpr=o(":"),Gpr=l(),W_e=a("ul"),cF=a("li"),Q_e=a("strong"),Opr=o("speech_to_text"),Xpr=o(" \u2014 "),fO=a("a"),zpr=o("TFSpeech2TextForConditionalGeneration"),Vpr=o(" (Speech2Text model)"),Wpr=l(),H_e=a("p"),Qpr=o("Examples:"),Hpr=l(),f(p6.$$.fragment),H9e=l(),qc=a("h2"),fF=a("a"),U_e=a("span"),f(_6.$$.fragment),Upr=l(),J_e=a("span"),Jpr=o("FlaxAutoModel"),U9e=l(),wr=a("div"),f(u6.$$.fragment),Ypr=l(),Gc=a("p"),Kpr=o(`This is a generic model class that will be instantiated as one of the base model classes of the library when created
with the `),Y_e=a("code"),Zpr=o("from_pretrained()"),e_r=o("class method or the "),K_e=a("code"),o_r=o("from_config()"),r_r=o(`class
method.`),t_r=l(),b6=a("p"),a_r=o("This class cannot be instantiated directly using "),Z_e=a("code"),n_r=o("__init__()"),s_r=o(" (throws an error)."),l_r=l(),vt=a("div"),f(v6.$$.fragment),i_r=l(),eue=a("p"),d_r=o("Instantiates one of the base model classes of the library from a configuration."),c_r=l(),Oc=a("p"),f_r=o(`Note:
Loading a model from its configuration file does `),oue=a("strong"),m_r=o("not"),g_r=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),rue=a("code"),h_r=o("from_pretrained()"),p_r=o("to load the model weights."),__r=l(),tue=a("p"),u_r=o("Examples:"),b_r=l(),f(T6.$$.fragment),v_r=l(),wo=a("div"),f(F6.$$.fragment),T_r=l(),aue=a("p"),F_r=o("Instantiate one of the base model classes of the library from a pretrained model."),C_r=l(),Fn=a("p"),M_r=o("The model class to instantiate is selected based on the "),nue=a("code"),E_r=o("model_type"),y_r=o(` property of the config object (either
passed as an argument or loaded from `),sue=a("code"),w_r=o("pretrained_model_name_or_path"),A_r=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),lue=a("code"),L_r=o("pretrained_model_name_or_path"),B_r=o(":"),k_r=l(),V=a("ul"),mF=a("li"),iue=a("strong"),x_r=o("albert"),R_r=o(" \u2014 "),mO=a("a"),S_r=o("FlaxAlbertModel"),P_r=o(" (ALBERT model)"),$_r=l(),gF=a("li"),due=a("strong"),I_r=o("bart"),j_r=o(" \u2014 "),gO=a("a"),N_r=o("FlaxBartModel"),D_r=o(" (BART model)"),q_r=l(),hF=a("li"),cue=a("strong"),G_r=o("beit"),O_r=o(" \u2014 "),hO=a("a"),X_r=o("FlaxBeitModel"),z_r=o(" (BEiT model)"),V_r=l(),pF=a("li"),fue=a("strong"),W_r=o("bert"),Q_r=o(" \u2014 "),pO=a("a"),H_r=o("FlaxBertModel"),U_r=o(" (BERT model)"),J_r=l(),_F=a("li"),mue=a("strong"),Y_r=o("big_bird"),K_r=o(" \u2014 "),_O=a("a"),Z_r=o("FlaxBigBirdModel"),eur=o(" (BigBird model)"),our=l(),uF=a("li"),gue=a("strong"),rur=o("blenderbot"),tur=o(" \u2014 "),uO=a("a"),aur=o("FlaxBlenderbotModel"),nur=o(" (Blenderbot model)"),sur=l(),bF=a("li"),hue=a("strong"),lur=o("blenderbot-small"),iur=o(" \u2014 "),bO=a("a"),dur=o("FlaxBlenderbotSmallModel"),cur=o(" (BlenderbotSmall model)"),fur=l(),vF=a("li"),pue=a("strong"),mur=o("clip"),gur=o(" \u2014 "),vO=a("a"),hur=o("FlaxCLIPModel"),pur=o(" (CLIP model)"),_ur=l(),TF=a("li"),_ue=a("strong"),uur=o("distilbert"),bur=o(" \u2014 "),TO=a("a"),vur=o("FlaxDistilBertModel"),Tur=o(" (DistilBERT model)"),Fur=l(),FF=a("li"),uue=a("strong"),Cur=o("electra"),Mur=o(" \u2014 "),FO=a("a"),Eur=o("FlaxElectraModel"),yur=o(" (ELECTRA model)"),wur=l(),CF=a("li"),bue=a("strong"),Aur=o("gpt2"),Lur=o(" \u2014 "),CO=a("a"),Bur=o("FlaxGPT2Model"),kur=o(" (OpenAI GPT-2 model)"),xur=l(),MF=a("li"),vue=a("strong"),Rur=o("gpt_neo"),Sur=o(" \u2014 "),MO=a("a"),Pur=o("FlaxGPTNeoModel"),$ur=o(" (GPT Neo model)"),Iur=l(),EF=a("li"),Tue=a("strong"),jur=o("gptj"),Nur=o(" \u2014 "),EO=a("a"),Dur=o("FlaxGPTJModel"),qur=o(" (GPT-J model)"),Gur=l(),yF=a("li"),Fue=a("strong"),Our=o("marian"),Xur=o(" \u2014 "),yO=a("a"),zur=o("FlaxMarianModel"),Vur=o(" (Marian model)"),Wur=l(),wF=a("li"),Cue=a("strong"),Qur=o("mbart"),Hur=o(" \u2014 "),wO=a("a"),Uur=o("FlaxMBartModel"),Jur=o(" (mBART model)"),Yur=l(),AF=a("li"),Mue=a("strong"),Kur=o("mt5"),Zur=o(" \u2014 "),AO=a("a"),e1r=o("FlaxMT5Model"),o1r=o(" (mT5 model)"),r1r=l(),LF=a("li"),Eue=a("strong"),t1r=o("pegasus"),a1r=o(" \u2014 "),LO=a("a"),n1r=o("FlaxPegasusModel"),s1r=o(" (Pegasus model)"),l1r=l(),BF=a("li"),yue=a("strong"),i1r=o("roberta"),d1r=o(" \u2014 "),BO=a("a"),c1r=o("FlaxRobertaModel"),f1r=o(" (RoBERTa model)"),m1r=l(),kF=a("li"),wue=a("strong"),g1r=o("roformer"),h1r=o(" \u2014 "),kO=a("a"),p1r=o("FlaxRoFormerModel"),_1r=o(" (RoFormer model)"),u1r=l(),xF=a("li"),Aue=a("strong"),b1r=o("t5"),v1r=o(" \u2014 "),xO=a("a"),T1r=o("FlaxT5Model"),F1r=o(" (T5 model)"),C1r=l(),RF=a("li"),Lue=a("strong"),M1r=o("vision-text-dual-encoder"),E1r=o(" \u2014 "),RO=a("a"),y1r=o("FlaxVisionTextDualEncoderModel"),w1r=o(" (VisionTextDualEncoder model)"),A1r=l(),SF=a("li"),Bue=a("strong"),L1r=o("vit"),B1r=o(" \u2014 "),SO=a("a"),k1r=o("FlaxViTModel"),x1r=o(" (ViT model)"),R1r=l(),PF=a("li"),kue=a("strong"),S1r=o("wav2vec2"),P1r=o(" \u2014 "),PO=a("a"),$1r=o("FlaxWav2Vec2Model"),I1r=o(" (Wav2Vec2 model)"),j1r=l(),$F=a("li"),xue=a("strong"),N1r=o("xglm"),D1r=o(" \u2014 "),$O=a("a"),q1r=o("FlaxXGLMModel"),G1r=o(" (XGLM model)"),O1r=l(),Rue=a("p"),X1r=o("Examples:"),z1r=l(),f(C6.$$.fragment),J9e=l(),Xc=a("h2"),IF=a("a"),Sue=a("span"),f(M6.$$.fragment),V1r=l(),Pue=a("span"),W1r=o("FlaxAutoModelForCausalLM"),Y9e=l(),Ar=a("div"),f(E6.$$.fragment),Q1r=l(),zc=a("p"),H1r=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a causal language modeling head) when created
with the `),$ue=a("code"),U1r=o("from_pretrained()"),J1r=o("class method or the "),Iue=a("code"),Y1r=o("from_config()"),K1r=o(`class
method.`),Z1r=l(),y6=a("p"),e7r=o("This class cannot be instantiated directly using "),jue=a("code"),o7r=o("__init__()"),r7r=o(" (throws an error)."),t7r=l(),Tt=a("div"),f(w6.$$.fragment),a7r=l(),Nue=a("p"),n7r=o("Instantiates one of the model classes of the library (with a causal language modeling head) from a configuration."),s7r=l(),Vc=a("p"),l7r=o(`Note:
Loading a model from its configuration file does `),Due=a("strong"),i7r=o("not"),d7r=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),que=a("code"),c7r=o("from_pretrained()"),f7r=o("to load the model weights."),m7r=l(),Gue=a("p"),g7r=o("Examples:"),h7r=l(),f(A6.$$.fragment),p7r=l(),Ao=a("div"),f(L6.$$.fragment),_7r=l(),Oue=a("p"),u7r=o("Instantiate one of the model classes of the library (with a causal language modeling head) from a pretrained model."),b7r=l(),Cn=a("p"),v7r=o("The model class to instantiate is selected based on the "),Xue=a("code"),T7r=o("model_type"),F7r=o(` property of the config object (either
passed as an argument or loaded from `),zue=a("code"),C7r=o("pretrained_model_name_or_path"),M7r=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Vue=a("code"),E7r=o("pretrained_model_name_or_path"),y7r=o(":"),w7r=l(),Mn=a("ul"),jF=a("li"),Wue=a("strong"),A7r=o("gpt2"),L7r=o(" \u2014 "),IO=a("a"),B7r=o("FlaxGPT2LMHeadModel"),k7r=o(" (OpenAI GPT-2 model)"),x7r=l(),NF=a("li"),Que=a("strong"),R7r=o("gpt_neo"),S7r=o(" \u2014 "),jO=a("a"),P7r=o("FlaxGPTNeoForCausalLM"),$7r=o(" (GPT Neo model)"),I7r=l(),DF=a("li"),Hue=a("strong"),j7r=o("gptj"),N7r=o(" \u2014 "),NO=a("a"),D7r=o("FlaxGPTJForCausalLM"),q7r=o(" (GPT-J model)"),G7r=l(),qF=a("li"),Uue=a("strong"),O7r=o("xglm"),X7r=o(" \u2014 "),DO=a("a"),z7r=o("FlaxXGLMForCausalLM"),V7r=o(" (XGLM model)"),W7r=l(),Jue=a("p"),Q7r=o("Examples:"),H7r=l(),f(B6.$$.fragment),K9e=l(),Wc=a("h2"),GF=a("a"),Yue=a("span"),f(k6.$$.fragment),U7r=l(),Kue=a("span"),J7r=o("FlaxAutoModelForPreTraining"),Z9e=l(),Lr=a("div"),f(x6.$$.fragment),Y7r=l(),Qc=a("p"),K7r=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a pretraining head) when created
with the `),Zue=a("code"),Z7r=o("from_pretrained()"),ebr=o("class method or the "),e1e=a("code"),obr=o("from_config()"),rbr=o(`class
method.`),tbr=l(),R6=a("p"),abr=o("This class cannot be instantiated directly using "),o1e=a("code"),nbr=o("__init__()"),sbr=o(" (throws an error)."),lbr=l(),Ft=a("div"),f(S6.$$.fragment),ibr=l(),r1e=a("p"),dbr=o("Instantiates one of the model classes of the library (with a pretraining head) from a configuration."),cbr=l(),Hc=a("p"),fbr=o(`Note:
Loading a model from its configuration file does `),t1e=a("strong"),mbr=o("not"),gbr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),a1e=a("code"),hbr=o("from_pretrained()"),pbr=o("to load the model weights."),_br=l(),n1e=a("p"),ubr=o("Examples:"),bbr=l(),f(P6.$$.fragment),vbr=l(),Lo=a("div"),f($6.$$.fragment),Tbr=l(),s1e=a("p"),Fbr=o("Instantiate one of the model classes of the library (with a pretraining head) from a pretrained model."),Cbr=l(),En=a("p"),Mbr=o("The model class to instantiate is selected based on the "),l1e=a("code"),Ebr=o("model_type"),ybr=o(` property of the config object (either
passed as an argument or loaded from `),i1e=a("code"),wbr=o("pretrained_model_name_or_path"),Abr=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),d1e=a("code"),Lbr=o("pretrained_model_name_or_path"),Bbr=o(":"),kbr=l(),fe=a("ul"),OF=a("li"),c1e=a("strong"),xbr=o("albert"),Rbr=o(" \u2014 "),qO=a("a"),Sbr=o("FlaxAlbertForPreTraining"),Pbr=o(" (ALBERT model)"),$br=l(),XF=a("li"),f1e=a("strong"),Ibr=o("bart"),jbr=o(" \u2014 "),GO=a("a"),Nbr=o("FlaxBartForConditionalGeneration"),Dbr=o(" (BART model)"),qbr=l(),zF=a("li"),m1e=a("strong"),Gbr=o("bert"),Obr=o(" \u2014 "),OO=a("a"),Xbr=o("FlaxBertForPreTraining"),zbr=o(" (BERT model)"),Vbr=l(),VF=a("li"),g1e=a("strong"),Wbr=o("big_bird"),Qbr=o(" \u2014 "),XO=a("a"),Hbr=o("FlaxBigBirdForPreTraining"),Ubr=o(" (BigBird model)"),Jbr=l(),WF=a("li"),h1e=a("strong"),Ybr=o("electra"),Kbr=o(" \u2014 "),zO=a("a"),Zbr=o("FlaxElectraForPreTraining"),e5r=o(" (ELECTRA model)"),o5r=l(),QF=a("li"),p1e=a("strong"),r5r=o("mbart"),t5r=o(" \u2014 "),VO=a("a"),a5r=o("FlaxMBartForConditionalGeneration"),n5r=o(" (mBART model)"),s5r=l(),HF=a("li"),_1e=a("strong"),l5r=o("mt5"),i5r=o(" \u2014 "),WO=a("a"),d5r=o("FlaxMT5ForConditionalGeneration"),c5r=o(" (mT5 model)"),f5r=l(),UF=a("li"),u1e=a("strong"),m5r=o("roberta"),g5r=o(" \u2014 "),QO=a("a"),h5r=o("FlaxRobertaForMaskedLM"),p5r=o(" (RoBERTa model)"),_5r=l(),JF=a("li"),b1e=a("strong"),u5r=o("roformer"),b5r=o(" \u2014 "),HO=a("a"),v5r=o("FlaxRoFormerForMaskedLM"),T5r=o(" (RoFormer model)"),F5r=l(),YF=a("li"),v1e=a("strong"),C5r=o("t5"),M5r=o(" \u2014 "),UO=a("a"),E5r=o("FlaxT5ForConditionalGeneration"),y5r=o(" (T5 model)"),w5r=l(),KF=a("li"),T1e=a("strong"),A5r=o("wav2vec2"),L5r=o(" \u2014 "),JO=a("a"),B5r=o("FlaxWav2Vec2ForPreTraining"),k5r=o(" (Wav2Vec2 model)"),x5r=l(),F1e=a("p"),R5r=o("Examples:"),S5r=l(),f(I6.$$.fragment),eBe=l(),Uc=a("h2"),ZF=a("a"),C1e=a("span"),f(j6.$$.fragment),P5r=l(),M1e=a("span"),$5r=o("FlaxAutoModelForMaskedLM"),oBe=l(),Br=a("div"),f(N6.$$.fragment),I5r=l(),Jc=a("p"),j5r=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a masked language modeling head) when created
with the `),E1e=a("code"),N5r=o("from_pretrained()"),D5r=o("class method or the "),y1e=a("code"),q5r=o("from_config()"),G5r=o(`class
method.`),O5r=l(),D6=a("p"),X5r=o("This class cannot be instantiated directly using "),w1e=a("code"),z5r=o("__init__()"),V5r=o(" (throws an error)."),W5r=l(),Ct=a("div"),f(q6.$$.fragment),Q5r=l(),A1e=a("p"),H5r=o("Instantiates one of the model classes of the library (with a masked language modeling head) from a configuration."),U5r=l(),Yc=a("p"),J5r=o(`Note:
Loading a model from its configuration file does `),L1e=a("strong"),Y5r=o("not"),K5r=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),B1e=a("code"),Z5r=o("from_pretrained()"),e2r=o("to load the model weights."),o2r=l(),k1e=a("p"),r2r=o("Examples:"),t2r=l(),f(G6.$$.fragment),a2r=l(),Bo=a("div"),f(O6.$$.fragment),n2r=l(),x1e=a("p"),s2r=o("Instantiate one of the model classes of the library (with a masked language modeling head) from a pretrained model."),l2r=l(),yn=a("p"),i2r=o("The model class to instantiate is selected based on the "),R1e=a("code"),d2r=o("model_type"),c2r=o(` property of the config object (either
passed as an argument or loaded from `),S1e=a("code"),f2r=o("pretrained_model_name_or_path"),m2r=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),P1e=a("code"),g2r=o("pretrained_model_name_or_path"),h2r=o(":"),p2r=l(),ve=a("ul"),eC=a("li"),$1e=a("strong"),_2r=o("albert"),u2r=o(" \u2014 "),YO=a("a"),b2r=o("FlaxAlbertForMaskedLM"),v2r=o(" (ALBERT model)"),T2r=l(),oC=a("li"),I1e=a("strong"),F2r=o("bart"),C2r=o(" \u2014 "),KO=a("a"),M2r=o("FlaxBartForConditionalGeneration"),E2r=o(" (BART model)"),y2r=l(),rC=a("li"),j1e=a("strong"),w2r=o("bert"),A2r=o(" \u2014 "),ZO=a("a"),L2r=o("FlaxBertForMaskedLM"),B2r=o(" (BERT model)"),k2r=l(),tC=a("li"),N1e=a("strong"),x2r=o("big_bird"),R2r=o(" \u2014 "),eX=a("a"),S2r=o("FlaxBigBirdForMaskedLM"),P2r=o(" (BigBird model)"),$2r=l(),aC=a("li"),D1e=a("strong"),I2r=o("distilbert"),j2r=o(" \u2014 "),oX=a("a"),N2r=o("FlaxDistilBertForMaskedLM"),D2r=o(" (DistilBERT model)"),q2r=l(),nC=a("li"),q1e=a("strong"),G2r=o("electra"),O2r=o(" \u2014 "),rX=a("a"),X2r=o("FlaxElectraForMaskedLM"),z2r=o(" (ELECTRA model)"),V2r=l(),sC=a("li"),G1e=a("strong"),W2r=o("mbart"),Q2r=o(" \u2014 "),tX=a("a"),H2r=o("FlaxMBartForConditionalGeneration"),U2r=o(" (mBART model)"),J2r=l(),lC=a("li"),O1e=a("strong"),Y2r=o("roberta"),K2r=o(" \u2014 "),aX=a("a"),Z2r=o("FlaxRobertaForMaskedLM"),evr=o(" (RoBERTa model)"),ovr=l(),iC=a("li"),X1e=a("strong"),rvr=o("roformer"),tvr=o(" \u2014 "),nX=a("a"),avr=o("FlaxRoFormerForMaskedLM"),nvr=o(" (RoFormer model)"),svr=l(),z1e=a("p"),lvr=o("Examples:"),ivr=l(),f(X6.$$.fragment),rBe=l(),Kc=a("h2"),dC=a("a"),V1e=a("span"),f(z6.$$.fragment),dvr=l(),W1e=a("span"),cvr=o("FlaxAutoModelForSeq2SeqLM"),tBe=l(),kr=a("div"),f(V6.$$.fragment),fvr=l(),Zc=a("p"),mvr=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence language modeling head) when created
with the `),Q1e=a("code"),gvr=o("from_pretrained()"),hvr=o("class method or the "),H1e=a("code"),pvr=o("from_config()"),_vr=o(`class
method.`),uvr=l(),W6=a("p"),bvr=o("This class cannot be instantiated directly using "),U1e=a("code"),vvr=o("__init__()"),Tvr=o(" (throws an error)."),Fvr=l(),Mt=a("div"),f(Q6.$$.fragment),Cvr=l(),J1e=a("p"),Mvr=o("Instantiates one of the model classes of the library (with a sequence-to-sequence language modeling head) from a configuration."),Evr=l(),ef=a("p"),yvr=o(`Note:
Loading a model from its configuration file does `),Y1e=a("strong"),wvr=o("not"),Avr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),K1e=a("code"),Lvr=o("from_pretrained()"),Bvr=o("to load the model weights."),kvr=l(),Z1e=a("p"),xvr=o("Examples:"),Rvr=l(),f(H6.$$.fragment),Svr=l(),ko=a("div"),f(U6.$$.fragment),Pvr=l(),e7e=a("p"),$vr=o("Instantiate one of the model classes of the library (with a sequence-to-sequence language modeling head) from a pretrained model."),Ivr=l(),wn=a("p"),jvr=o("The model class to instantiate is selected based on the "),o7e=a("code"),Nvr=o("model_type"),Dvr=o(` property of the config object (either
passed as an argument or loaded from `),r7e=a("code"),qvr=o("pretrained_model_name_or_path"),Gvr=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),t7e=a("code"),Ovr=o("pretrained_model_name_or_path"),Xvr=o(":"),zvr=l(),Te=a("ul"),cC=a("li"),a7e=a("strong"),Vvr=o("bart"),Wvr=o(" \u2014 "),sX=a("a"),Qvr=o("FlaxBartForConditionalGeneration"),Hvr=o(" (BART model)"),Uvr=l(),fC=a("li"),n7e=a("strong"),Jvr=o("blenderbot"),Yvr=o(" \u2014 "),lX=a("a"),Kvr=o("FlaxBlenderbotForConditionalGeneration"),Zvr=o(" (Blenderbot model)"),e0r=l(),mC=a("li"),s7e=a("strong"),o0r=o("blenderbot-small"),r0r=o(" \u2014 "),iX=a("a"),t0r=o("FlaxBlenderbotSmallForConditionalGeneration"),a0r=o(" (BlenderbotSmall model)"),n0r=l(),gC=a("li"),l7e=a("strong"),s0r=o("encoder-decoder"),l0r=o(" \u2014 "),dX=a("a"),i0r=o("FlaxEncoderDecoderModel"),d0r=o(" (Encoder decoder model)"),c0r=l(),hC=a("li"),i7e=a("strong"),f0r=o("marian"),m0r=o(" \u2014 "),cX=a("a"),g0r=o("FlaxMarianMTModel"),h0r=o(" (Marian model)"),p0r=l(),pC=a("li"),d7e=a("strong"),_0r=o("mbart"),u0r=o(" \u2014 "),fX=a("a"),b0r=o("FlaxMBartForConditionalGeneration"),v0r=o(" (mBART model)"),T0r=l(),_C=a("li"),c7e=a("strong"),F0r=o("mt5"),C0r=o(" \u2014 "),mX=a("a"),M0r=o("FlaxMT5ForConditionalGeneration"),E0r=o(" (mT5 model)"),y0r=l(),uC=a("li"),f7e=a("strong"),w0r=o("pegasus"),A0r=o(" \u2014 "),gX=a("a"),L0r=o("FlaxPegasusForConditionalGeneration"),B0r=o(" (Pegasus model)"),k0r=l(),bC=a("li"),m7e=a("strong"),x0r=o("t5"),R0r=o(" \u2014 "),hX=a("a"),S0r=o("FlaxT5ForConditionalGeneration"),P0r=o(" (T5 model)"),$0r=l(),g7e=a("p"),I0r=o("Examples:"),j0r=l(),f(J6.$$.fragment),aBe=l(),of=a("h2"),vC=a("a"),h7e=a("span"),f(Y6.$$.fragment),N0r=l(),p7e=a("span"),D0r=o("FlaxAutoModelForSequenceClassification"),nBe=l(),xr=a("div"),f(K6.$$.fragment),q0r=l(),rf=a("p"),G0r=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence classification head) when created
with the `),_7e=a("code"),O0r=o("from_pretrained()"),X0r=o("class method or the "),u7e=a("code"),z0r=o("from_config()"),V0r=o(`class
method.`),W0r=l(),Z6=a("p"),Q0r=o("This class cannot be instantiated directly using "),b7e=a("code"),H0r=o("__init__()"),U0r=o(" (throws an error)."),J0r=l(),Et=a("div"),f(eA.$$.fragment),Y0r=l(),v7e=a("p"),K0r=o("Instantiates one of the model classes of the library (with a sequence classification head) from a configuration."),Z0r=l(),tf=a("p"),eTr=o(`Note:
Loading a model from its configuration file does `),T7e=a("strong"),oTr=o("not"),rTr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),F7e=a("code"),tTr=o("from_pretrained()"),aTr=o("to load the model weights."),nTr=l(),C7e=a("p"),sTr=o("Examples:"),lTr=l(),f(oA.$$.fragment),iTr=l(),xo=a("div"),f(rA.$$.fragment),dTr=l(),M7e=a("p"),cTr=o("Instantiate one of the model classes of the library (with a sequence classification head) from a pretrained model."),fTr=l(),An=a("p"),mTr=o("The model class to instantiate is selected based on the "),E7e=a("code"),gTr=o("model_type"),hTr=o(` property of the config object (either
passed as an argument or loaded from `),y7e=a("code"),pTr=o("pretrained_model_name_or_path"),_Tr=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),w7e=a("code"),uTr=o("pretrained_model_name_or_path"),bTr=o(":"),vTr=l(),Fe=a("ul"),TC=a("li"),A7e=a("strong"),TTr=o("albert"),FTr=o(" \u2014 "),pX=a("a"),CTr=o("FlaxAlbertForSequenceClassification"),MTr=o(" (ALBERT model)"),ETr=l(),FC=a("li"),L7e=a("strong"),yTr=o("bart"),wTr=o(" \u2014 "),_X=a("a"),ATr=o("FlaxBartForSequenceClassification"),LTr=o(" (BART model)"),BTr=l(),CC=a("li"),B7e=a("strong"),kTr=o("bert"),xTr=o(" \u2014 "),uX=a("a"),RTr=o("FlaxBertForSequenceClassification"),STr=o(" (BERT model)"),PTr=l(),MC=a("li"),k7e=a("strong"),$Tr=o("big_bird"),ITr=o(" \u2014 "),bX=a("a"),jTr=o("FlaxBigBirdForSequenceClassification"),NTr=o(" (BigBird model)"),DTr=l(),EC=a("li"),x7e=a("strong"),qTr=o("distilbert"),GTr=o(" \u2014 "),vX=a("a"),OTr=o("FlaxDistilBertForSequenceClassification"),XTr=o(" (DistilBERT model)"),zTr=l(),yC=a("li"),R7e=a("strong"),VTr=o("electra"),WTr=o(" \u2014 "),TX=a("a"),QTr=o("FlaxElectraForSequenceClassification"),HTr=o(" (ELECTRA model)"),UTr=l(),wC=a("li"),S7e=a("strong"),JTr=o("mbart"),YTr=o(" \u2014 "),FX=a("a"),KTr=o("FlaxMBartForSequenceClassification"),ZTr=o(" (mBART model)"),eFr=l(),AC=a("li"),P7e=a("strong"),oFr=o("roberta"),rFr=o(" \u2014 "),CX=a("a"),tFr=o("FlaxRobertaForSequenceClassification"),aFr=o(" (RoBERTa model)"),nFr=l(),LC=a("li"),$7e=a("strong"),sFr=o("roformer"),lFr=o(" \u2014 "),MX=a("a"),iFr=o("FlaxRoFormerForSequenceClassification"),dFr=o(" (RoFormer model)"),cFr=l(),I7e=a("p"),fFr=o("Examples:"),mFr=l(),f(tA.$$.fragment),sBe=l(),af=a("h2"),BC=a("a"),j7e=a("span"),f(aA.$$.fragment),gFr=l(),N7e=a("span"),hFr=o("FlaxAutoModelForQuestionAnswering"),lBe=l(),Rr=a("div"),f(nA.$$.fragment),pFr=l(),nf=a("p"),_Fr=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a question answering head) when created
with the `),D7e=a("code"),uFr=o("from_pretrained()"),bFr=o("class method or the "),q7e=a("code"),vFr=o("from_config()"),TFr=o(`class
method.`),FFr=l(),sA=a("p"),CFr=o("This class cannot be instantiated directly using "),G7e=a("code"),MFr=o("__init__()"),EFr=o(" (throws an error)."),yFr=l(),yt=a("div"),f(lA.$$.fragment),wFr=l(),O7e=a("p"),AFr=o("Instantiates one of the model classes of the library (with a question answering head) from a configuration."),LFr=l(),sf=a("p"),BFr=o(`Note:
Loading a model from its configuration file does `),X7e=a("strong"),kFr=o("not"),xFr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),z7e=a("code"),RFr=o("from_pretrained()"),SFr=o("to load the model weights."),PFr=l(),V7e=a("p"),$Fr=o("Examples:"),IFr=l(),f(iA.$$.fragment),jFr=l(),Ro=a("div"),f(dA.$$.fragment),NFr=l(),W7e=a("p"),DFr=o("Instantiate one of the model classes of the library (with a question answering head) from a pretrained model."),qFr=l(),Ln=a("p"),GFr=o("The model class to instantiate is selected based on the "),Q7e=a("code"),OFr=o("model_type"),XFr=o(` property of the config object (either
passed as an argument or loaded from `),H7e=a("code"),zFr=o("pretrained_model_name_or_path"),VFr=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),U7e=a("code"),WFr=o("pretrained_model_name_or_path"),QFr=o(":"),HFr=l(),Ce=a("ul"),kC=a("li"),J7e=a("strong"),UFr=o("albert"),JFr=o(" \u2014 "),EX=a("a"),YFr=o("FlaxAlbertForQuestionAnswering"),KFr=o(" (ALBERT model)"),ZFr=l(),xC=a("li"),Y7e=a("strong"),eCr=o("bart"),oCr=o(" \u2014 "),yX=a("a"),rCr=o("FlaxBartForQuestionAnswering"),tCr=o(" (BART model)"),aCr=l(),RC=a("li"),K7e=a("strong"),nCr=o("bert"),sCr=o(" \u2014 "),wX=a("a"),lCr=o("FlaxBertForQuestionAnswering"),iCr=o(" (BERT model)"),dCr=l(),SC=a("li"),Z7e=a("strong"),cCr=o("big_bird"),fCr=o(" \u2014 "),AX=a("a"),mCr=o("FlaxBigBirdForQuestionAnswering"),gCr=o(" (BigBird model)"),hCr=l(),PC=a("li"),ebe=a("strong"),pCr=o("distilbert"),_Cr=o(" \u2014 "),LX=a("a"),uCr=o("FlaxDistilBertForQuestionAnswering"),bCr=o(" (DistilBERT model)"),vCr=l(),$C=a("li"),obe=a("strong"),TCr=o("electra"),FCr=o(" \u2014 "),BX=a("a"),CCr=o("FlaxElectraForQuestionAnswering"),MCr=o(" (ELECTRA model)"),ECr=l(),IC=a("li"),rbe=a("strong"),yCr=o("mbart"),wCr=o(" \u2014 "),kX=a("a"),ACr=o("FlaxMBartForQuestionAnswering"),LCr=o(" (mBART model)"),BCr=l(),jC=a("li"),tbe=a("strong"),kCr=o("roberta"),xCr=o(" \u2014 "),xX=a("a"),RCr=o("FlaxRobertaForQuestionAnswering"),SCr=o(" (RoBERTa model)"),PCr=l(),NC=a("li"),abe=a("strong"),$Cr=o("roformer"),ICr=o(" \u2014 "),RX=a("a"),jCr=o("FlaxRoFormerForQuestionAnswering"),NCr=o(" (RoFormer model)"),DCr=l(),nbe=a("p"),qCr=o("Examples:"),GCr=l(),f(cA.$$.fragment),iBe=l(),lf=a("h2"),DC=a("a"),sbe=a("span"),f(fA.$$.fragment),OCr=l(),lbe=a("span"),XCr=o("FlaxAutoModelForTokenClassification"),dBe=l(),Sr=a("div"),f(mA.$$.fragment),zCr=l(),df=a("p"),VCr=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a token classification head) when created
with the `),ibe=a("code"),WCr=o("from_pretrained()"),QCr=o("class method or the "),dbe=a("code"),HCr=o("from_config()"),UCr=o(`class
method.`),JCr=l(),gA=a("p"),YCr=o("This class cannot be instantiated directly using "),cbe=a("code"),KCr=o("__init__()"),ZCr=o(" (throws an error)."),e4r=l(),wt=a("div"),f(hA.$$.fragment),o4r=l(),fbe=a("p"),r4r=o("Instantiates one of the model classes of the library (with a token classification head) from a configuration."),t4r=l(),cf=a("p"),a4r=o(`Note:
Loading a model from its configuration file does `),mbe=a("strong"),n4r=o("not"),s4r=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),gbe=a("code"),l4r=o("from_pretrained()"),i4r=o("to load the model weights."),d4r=l(),hbe=a("p"),c4r=o("Examples:"),f4r=l(),f(pA.$$.fragment),m4r=l(),So=a("div"),f(_A.$$.fragment),g4r=l(),pbe=a("p"),h4r=o("Instantiate one of the model classes of the library (with a token classification head) from a pretrained model."),p4r=l(),Bn=a("p"),_4r=o("The model class to instantiate is selected based on the "),_be=a("code"),u4r=o("model_type"),b4r=o(` property of the config object (either
passed as an argument or loaded from `),ube=a("code"),v4r=o("pretrained_model_name_or_path"),T4r=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),bbe=a("code"),F4r=o("pretrained_model_name_or_path"),C4r=o(":"),M4r=l(),so=a("ul"),qC=a("li"),vbe=a("strong"),E4r=o("albert"),y4r=o(" \u2014 "),SX=a("a"),w4r=o("FlaxAlbertForTokenClassification"),A4r=o(" (ALBERT model)"),L4r=l(),GC=a("li"),Tbe=a("strong"),B4r=o("bert"),k4r=o(" \u2014 "),PX=a("a"),x4r=o("FlaxBertForTokenClassification"),R4r=o(" (BERT model)"),S4r=l(),OC=a("li"),Fbe=a("strong"),P4r=o("big_bird"),$4r=o(" \u2014 "),$X=a("a"),I4r=o("FlaxBigBirdForTokenClassification"),j4r=o(" (BigBird model)"),N4r=l(),XC=a("li"),Cbe=a("strong"),D4r=o("distilbert"),q4r=o(" \u2014 "),IX=a("a"),G4r=o("FlaxDistilBertForTokenClassification"),O4r=o(" (DistilBERT model)"),X4r=l(),zC=a("li"),Mbe=a("strong"),z4r=o("electra"),V4r=o(" \u2014 "),jX=a("a"),W4r=o("FlaxElectraForTokenClassification"),Q4r=o(" (ELECTRA model)"),H4r=l(),VC=a("li"),Ebe=a("strong"),U4r=o("roberta"),J4r=o(" \u2014 "),NX=a("a"),Y4r=o("FlaxRobertaForTokenClassification"),K4r=o(" (RoBERTa model)"),Z4r=l(),WC=a("li"),ybe=a("strong"),eMr=o("roformer"),oMr=o(" \u2014 "),DX=a("a"),rMr=o("FlaxRoFormerForTokenClassification"),tMr=o(" (RoFormer model)"),aMr=l(),wbe=a("p"),nMr=o("Examples:"),sMr=l(),f(uA.$$.fragment),cBe=l(),ff=a("h2"),QC=a("a"),Abe=a("span"),f(bA.$$.fragment),lMr=l(),Lbe=a("span"),iMr=o("FlaxAutoModelForMultipleChoice"),fBe=l(),Pr=a("div"),f(vA.$$.fragment),dMr=l(),mf=a("p"),cMr=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a multiple choice head) when created
with the `),Bbe=a("code"),fMr=o("from_pretrained()"),mMr=o("class method or the "),kbe=a("code"),gMr=o("from_config()"),hMr=o(`class
method.`),pMr=l(),TA=a("p"),_Mr=o("This class cannot be instantiated directly using "),xbe=a("code"),uMr=o("__init__()"),bMr=o(" (throws an error)."),vMr=l(),At=a("div"),f(FA.$$.fragment),TMr=l(),Rbe=a("p"),FMr=o("Instantiates one of the model classes of the library (with a multiple choice head) from a configuration."),CMr=l(),gf=a("p"),MMr=o(`Note:
Loading a model from its configuration file does `),Sbe=a("strong"),EMr=o("not"),yMr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Pbe=a("code"),wMr=o("from_pretrained()"),AMr=o("to load the model weights."),LMr=l(),$be=a("p"),BMr=o("Examples:"),kMr=l(),f(CA.$$.fragment),xMr=l(),Po=a("div"),f(MA.$$.fragment),RMr=l(),Ibe=a("p"),SMr=o("Instantiate one of the model classes of the library (with a multiple choice head) from a pretrained model."),PMr=l(),kn=a("p"),$Mr=o("The model class to instantiate is selected based on the "),jbe=a("code"),IMr=o("model_type"),jMr=o(` property of the config object (either
passed as an argument or loaded from `),Nbe=a("code"),NMr=o("pretrained_model_name_or_path"),DMr=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Dbe=a("code"),qMr=o("pretrained_model_name_or_path"),GMr=o(":"),OMr=l(),lo=a("ul"),HC=a("li"),qbe=a("strong"),XMr=o("albert"),zMr=o(" \u2014 "),qX=a("a"),VMr=o("FlaxAlbertForMultipleChoice"),WMr=o(" (ALBERT model)"),QMr=l(),UC=a("li"),Gbe=a("strong"),HMr=o("bert"),UMr=o(" \u2014 "),GX=a("a"),JMr=o("FlaxBertForMultipleChoice"),YMr=o(" (BERT model)"),KMr=l(),JC=a("li"),Obe=a("strong"),ZMr=o("big_bird"),eEr=o(" \u2014 "),OX=a("a"),oEr=o("FlaxBigBirdForMultipleChoice"),rEr=o(" (BigBird model)"),tEr=l(),YC=a("li"),Xbe=a("strong"),aEr=o("distilbert"),nEr=o(" \u2014 "),XX=a("a"),sEr=o("FlaxDistilBertForMultipleChoice"),lEr=o(" (DistilBERT model)"),iEr=l(),KC=a("li"),zbe=a("strong"),dEr=o("electra"),cEr=o(" \u2014 "),zX=a("a"),fEr=o("FlaxElectraForMultipleChoice"),mEr=o(" (ELECTRA model)"),gEr=l(),ZC=a("li"),Vbe=a("strong"),hEr=o("roberta"),pEr=o(" \u2014 "),VX=a("a"),_Er=o("FlaxRobertaForMultipleChoice"),uEr=o(" (RoBERTa model)"),bEr=l(),e4=a("li"),Wbe=a("strong"),vEr=o("roformer"),TEr=o(" \u2014 "),WX=a("a"),FEr=o("FlaxRoFormerForMultipleChoice"),CEr=o(" (RoFormer model)"),MEr=l(),Qbe=a("p"),EEr=o("Examples:"),yEr=l(),f(EA.$$.fragment),mBe=l(),hf=a("h2"),o4=a("a"),Hbe=a("span"),f(yA.$$.fragment),wEr=l(),Ube=a("span"),AEr=o("FlaxAutoModelForNextSentencePrediction"),gBe=l(),$r=a("div"),f(wA.$$.fragment),LEr=l(),pf=a("p"),BEr=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a next sentence prediction head) when created
with the `),Jbe=a("code"),kEr=o("from_pretrained()"),xEr=o("class method or the "),Ybe=a("code"),REr=o("from_config()"),SEr=o(`class
method.`),PEr=l(),AA=a("p"),$Er=o("This class cannot be instantiated directly using "),Kbe=a("code"),IEr=o("__init__()"),jEr=o(" (throws an error)."),NEr=l(),Lt=a("div"),f(LA.$$.fragment),DEr=l(),Zbe=a("p"),qEr=o("Instantiates one of the model classes of the library (with a next sentence prediction head) from a configuration."),GEr=l(),_f=a("p"),OEr=o(`Note:
Loading a model from its configuration file does `),e5e=a("strong"),XEr=o("not"),zEr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),o5e=a("code"),VEr=o("from_pretrained()"),WEr=o("to load the model weights."),QEr=l(),r5e=a("p"),HEr=o("Examples:"),UEr=l(),f(BA.$$.fragment),JEr=l(),$o=a("div"),f(kA.$$.fragment),YEr=l(),t5e=a("p"),KEr=o("Instantiate one of the model classes of the library (with a next sentence prediction head) from a pretrained model."),ZEr=l(),xn=a("p"),e3r=o("The model class to instantiate is selected based on the "),a5e=a("code"),o3r=o("model_type"),r3r=o(` property of the config object (either
passed as an argument or loaded from `),n5e=a("code"),t3r=o("pretrained_model_name_or_path"),a3r=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),s5e=a("code"),n3r=o("pretrained_model_name_or_path"),s3r=o(":"),l3r=l(),l5e=a("ul"),r4=a("li"),i5e=a("strong"),i3r=o("bert"),d3r=o(" \u2014 "),QX=a("a"),c3r=o("FlaxBertForNextSentencePrediction"),f3r=o(" (BERT model)"),m3r=l(),d5e=a("p"),g3r=o("Examples:"),h3r=l(),f(xA.$$.fragment),hBe=l(),uf=a("h2"),t4=a("a"),c5e=a("span"),f(RA.$$.fragment),p3r=l(),f5e=a("span"),_3r=o("FlaxAutoModelForImageClassification"),pBe=l(),Ir=a("div"),f(SA.$$.fragment),u3r=l(),bf=a("p"),b3r=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a image classification head) when created
with the `),m5e=a("code"),v3r=o("from_pretrained()"),T3r=o("class method or the "),g5e=a("code"),F3r=o("from_config()"),C3r=o(`class
method.`),M3r=l(),PA=a("p"),E3r=o("This class cannot be instantiated directly using "),h5e=a("code"),y3r=o("__init__()"),w3r=o(" (throws an error)."),A3r=l(),Bt=a("div"),f($A.$$.fragment),L3r=l(),p5e=a("p"),B3r=o("Instantiates one of the model classes of the library (with a image classification head) from a configuration."),k3r=l(),vf=a("p"),x3r=o(`Note:
Loading a model from its configuration file does `),_5e=a("strong"),R3r=o("not"),S3r=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),u5e=a("code"),P3r=o("from_pretrained()"),$3r=o("to load the model weights."),I3r=l(),b5e=a("p"),j3r=o("Examples:"),N3r=l(),f(IA.$$.fragment),D3r=l(),Io=a("div"),f(jA.$$.fragment),q3r=l(),v5e=a("p"),G3r=o("Instantiate one of the model classes of the library (with a image classification head) from a pretrained model."),O3r=l(),Rn=a("p"),X3r=o("The model class to instantiate is selected based on the "),T5e=a("code"),z3r=o("model_type"),V3r=o(` property of the config object (either
passed as an argument or loaded from `),F5e=a("code"),W3r=o("pretrained_model_name_or_path"),Q3r=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),C5e=a("code"),H3r=o("pretrained_model_name_or_path"),U3r=o(":"),J3r=l(),NA=a("ul"),a4=a("li"),M5e=a("strong"),Y3r=o("beit"),K3r=o(" \u2014 "),HX=a("a"),Z3r=o("FlaxBeitForImageClassification"),eyr=o(" (BEiT model)"),oyr=l(),n4=a("li"),E5e=a("strong"),ryr=o("vit"),tyr=o(" \u2014 "),UX=a("a"),ayr=o("FlaxViTForImageClassification"),nyr=o(" (ViT model)"),syr=l(),y5e=a("p"),lyr=o("Examples:"),iyr=l(),f(DA.$$.fragment),_Be=l(),Tf=a("h2"),s4=a("a"),w5e=a("span"),f(qA.$$.fragment),dyr=l(),A5e=a("span"),cyr=o("FlaxAutoModelForVision2Seq"),uBe=l(),jr=a("div"),f(GA.$$.fragment),fyr=l(),Ff=a("p"),myr=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a vision-to-text modeling head) when created
with the `),L5e=a("code"),gyr=o("from_pretrained()"),hyr=o("class method or the "),B5e=a("code"),pyr=o("from_config()"),_yr=o(`class
method.`),uyr=l(),OA=a("p"),byr=o("This class cannot be instantiated directly using "),k5e=a("code"),vyr=o("__init__()"),Tyr=o(" (throws an error)."),Fyr=l(),kt=a("div"),f(XA.$$.fragment),Cyr=l(),x5e=a("p"),Myr=o("Instantiates one of the model classes of the library (with a vision-to-text modeling head) from a configuration."),Eyr=l(),Cf=a("p"),yyr=o(`Note:
Loading a model from its configuration file does `),R5e=a("strong"),wyr=o("not"),Ayr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),S5e=a("code"),Lyr=o("from_pretrained()"),Byr=o("to load the model weights."),kyr=l(),P5e=a("p"),xyr=o("Examples:"),Ryr=l(),f(zA.$$.fragment),Syr=l(),jo=a("div"),f(VA.$$.fragment),Pyr=l(),$5e=a("p"),$yr=o("Instantiate one of the model classes of the library (with a vision-to-text modeling head) from a pretrained model."),Iyr=l(),Sn=a("p"),jyr=o("The model class to instantiate is selected based on the "),I5e=a("code"),Nyr=o("model_type"),Dyr=o(` property of the config object (either
passed as an argument or loaded from `),j5e=a("code"),qyr=o("pretrained_model_name_or_path"),Gyr=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),N5e=a("code"),Oyr=o("pretrained_model_name_or_path"),Xyr=o(":"),zyr=l(),D5e=a("ul"),l4=a("li"),q5e=a("strong"),Vyr=o("vision-encoder-decoder"),Wyr=o(" \u2014 "),JX=a("a"),Qyr=o("FlaxVisionEncoderDecoderModel"),Hyr=o(" (Vision Encoder decoder model)"),Uyr=l(),G5e=a("p"),Jyr=o("Examples:"),Yyr=l(),f(WA.$$.fragment),this.h()},l(d){const u=cut('[data-svelte="svelte-1phssyn"]',document.head);J=n(u,"META",{name:!0,content:!0}),u.forEach(t),Le=i(d),de=n(d,"H1",{class:!0});var QA=s(de);me=n(QA,"A",{id:!0,class:!0,href:!0});var O5e=s(me);to=n(O5e,"SPAN",{});var X5e=s(to);m(ce.$$.fragment,X5e),X5e.forEach(t),O5e.forEach(t),be=i(QA),Do=n(QA,"SPAN",{});var Zyr=s(Do);wi=r(Zyr,"Auto Classes"),Zyr.forEach(t),QA.forEach(t),Ef=i(d),sa=n(d,"P",{});var vBe=s(sa);Ai=r(vBe,`In many cases, the architecture you want to use can be guessed from the name or the path of the pretrained model you
are supplying to the `),Li=n(vBe,"CODE",{});var ewr=s(Li);nM=r(ewr,"from_pretrained()"),ewr.forEach(t),yf=r(vBe,` method. AutoClasses are here to do this job for you so that you
automatically retrieve the relevant model given the name/path to the pretrained weights/config/vocabulary.`),vBe.forEach(t),we=i(d),io=n(d,"P",{});var i4=s(io);Bi=r(i4,"Instantiating one of "),Pn=n(i4,"A",{href:!0});var owr=s(Pn);sM=r(owr,"AutoConfig"),owr.forEach(t),$n=r(i4,", "),In=n(i4,"A",{href:!0});var rwr=s(In);lM=r(rwr,"AutoModel"),rwr.forEach(t),ki=r(i4,`, and
`),jn=n(i4,"A",{href:!0});var twr=s(jn);iM=r(twr,"AutoTokenizer"),twr.forEach(t),xi=r(i4," will directly create a class of the relevant architecture. For instance"),i4.forEach(t),wf=i(d),m($a.$$.fragment,d),co=i(d),ge=n(d,"P",{});var TBe=s(ge);XL=r(TBe,"will create a model that is an instance of "),Ri=n(TBe,"A",{href:!0});var awr=s(Ri);zL=r(awr,"BertModel"),awr.forEach(t),VL=r(TBe,"."),TBe.forEach(t),qo=i(d),Ia=n(d,"P",{});var FBe=s(Ia);WL=r(FBe,"There is one class of "),Af=n(FBe,"CODE",{});var nwr=s(Af);QL=r(nwr,"AutoModel"),nwr.forEach(t),Lxe=r(FBe," for each task, and for each backend (PyTorch, TensorFlow, or Flax)."),FBe.forEach(t),v8e=i(d),Si=n(d,"H2",{class:!0});var CBe=s(Si);Lf=n(CBe,"A",{id:!0,class:!0,href:!0});var swr=s(Lf);XV=n(swr,"SPAN",{});var lwr=s(XV);m(dM.$$.fragment,lwr),lwr.forEach(t),swr.forEach(t),Bxe=i(CBe),zV=n(CBe,"SPAN",{});var iwr=s(zV);kxe=r(iwr,"Extending the Auto Classes"),iwr.forEach(t),CBe.forEach(t),T8e=i(d),Nn=n(d,"P",{});var YX=s(Nn);xxe=r(YX,`Each of the auto classes has a method to be extended with your custom classes. For instance, if you have defined a
custom class of model `),VV=n(YX,"CODE",{});var dwr=s(VV);Rxe=r(dwr,"NewModel"),dwr.forEach(t),Sxe=r(YX,", make sure you have a "),WV=n(YX,"CODE",{});var cwr=s(WV);Pxe=r(cwr,"NewModelConfig"),cwr.forEach(t),$xe=r(YX,` then you can add those to the auto
classes like this:`),YX.forEach(t),F8e=i(d),m(cM.$$.fragment,d),C8e=i(d),HL=n(d,"P",{});var fwr=s(HL);Ixe=r(fwr,"You will then be able to use the auto classes like you would usually do!"),fwr.forEach(t),M8e=i(d),m(Bf.$$.fragment,d),E8e=i(d),Pi=n(d,"H2",{class:!0});var MBe=s(Pi);kf=n(MBe,"A",{id:!0,class:!0,href:!0});var mwr=s(kf);QV=n(mwr,"SPAN",{});var gwr=s(QV);m(fM.$$.fragment,gwr),gwr.forEach(t),mwr.forEach(t),jxe=i(MBe),HV=n(MBe,"SPAN",{});var hwr=s(HV);Nxe=r(hwr,"AutoConfig"),hwr.forEach(t),MBe.forEach(t),y8e=i(d),Go=n(d,"DIV",{class:!0});var Ps=s(Go);m(mM.$$.fragment,Ps),Dxe=i(Ps),gM=n(Ps,"P",{});var EBe=s(gM);qxe=r(EBe,`This is a generic configuration class that will be instantiated as one of the configuration classes of the library
when created with the `),UL=n(EBe,"A",{href:!0});var pwr=s(UL);Gxe=r(pwr,"from_pretrained()"),pwr.forEach(t),Oxe=r(EBe," class method."),EBe.forEach(t),Xxe=i(Ps),hM=n(Ps,"P",{});var yBe=s(hM);zxe=r(yBe,"This class cannot be instantiated directly using "),UV=n(yBe,"CODE",{});var _wr=s(UV);Vxe=r(_wr,"__init__()"),_wr.forEach(t),Wxe=r(yBe," (throws an error)."),yBe.forEach(t),Qxe=i(Ps),fo=n(Ps,"DIV",{class:!0});var ia=s(fo);m(pM.$$.fragment,ia),Hxe=i(ia),JV=n(ia,"P",{});var uwr=s(JV);Uxe=r(uwr,"Instantiate one of the configuration classes of the library from a pretrained model configuration."),uwr.forEach(t),Jxe=i(ia),$i=n(ia,"P",{});var KX=s($i);Yxe=r(KX,"The configuration class to instantiate is selected based on the "),YV=n(KX,"CODE",{});var bwr=s(YV);Kxe=r(bwr,"model_type"),bwr.forEach(t),Zxe=r(KX,` property of the config object that
is loaded, or when it\u2019s missing, by falling back to using pattern matching on `),KV=n(KX,"CODE",{});var vwr=s(KV);eRe=r(vwr,"pretrained_model_name_or_path"),vwr.forEach(t),oRe=r(KX,":"),KX.forEach(t),rRe=i(ia),v=n(ia,"UL",{});var T=s(v);xf=n(T,"LI",{});var z5e=s(xf);ZV=n(z5e,"STRONG",{});var Twr=s(ZV);tRe=r(Twr,"albert"),Twr.forEach(t),aRe=r(z5e," \u2014 "),JL=n(z5e,"A",{href:!0});var Fwr=s(JL);nRe=r(Fwr,"AlbertConfig"),Fwr.forEach(t),sRe=r(z5e," (ALBERT model)"),z5e.forEach(t),lRe=i(T),Rf=n(T,"LI",{});var V5e=s(Rf);eW=n(V5e,"STRONG",{});var Cwr=s(eW);iRe=r(Cwr,"bart"),Cwr.forEach(t),dRe=r(V5e," \u2014 "),YL=n(V5e,"A",{href:!0});var Mwr=s(YL);cRe=r(Mwr,"BartConfig"),Mwr.forEach(t),fRe=r(V5e," (BART model)"),V5e.forEach(t),mRe=i(T),Sf=n(T,"LI",{});var W5e=s(Sf);oW=n(W5e,"STRONG",{});var Ewr=s(oW);gRe=r(Ewr,"beit"),Ewr.forEach(t),hRe=r(W5e," \u2014 "),KL=n(W5e,"A",{href:!0});var ywr=s(KL);pRe=r(ywr,"BeitConfig"),ywr.forEach(t),_Re=r(W5e," (BEiT model)"),W5e.forEach(t),uRe=i(T),Pf=n(T,"LI",{});var Q5e=s(Pf);rW=n(Q5e,"STRONG",{});var wwr=s(rW);bRe=r(wwr,"bert"),wwr.forEach(t),vRe=r(Q5e," \u2014 "),ZL=n(Q5e,"A",{href:!0});var Awr=s(ZL);TRe=r(Awr,"BertConfig"),Awr.forEach(t),FRe=r(Q5e," (BERT model)"),Q5e.forEach(t),CRe=i(T),$f=n(T,"LI",{});var H5e=s($f);tW=n(H5e,"STRONG",{});var Lwr=s(tW);MRe=r(Lwr,"bert-generation"),Lwr.forEach(t),ERe=r(H5e," \u2014 "),e8=n(H5e,"A",{href:!0});var Bwr=s(e8);yRe=r(Bwr,"BertGenerationConfig"),Bwr.forEach(t),wRe=r(H5e," (Bert Generation model)"),H5e.forEach(t),ARe=i(T),If=n(T,"LI",{});var U5e=s(If);aW=n(U5e,"STRONG",{});var kwr=s(aW);LRe=r(kwr,"big_bird"),kwr.forEach(t),BRe=r(U5e," \u2014 "),o8=n(U5e,"A",{href:!0});var xwr=s(o8);kRe=r(xwr,"BigBirdConfig"),xwr.forEach(t),xRe=r(U5e," (BigBird model)"),U5e.forEach(t),RRe=i(T),jf=n(T,"LI",{});var J5e=s(jf);nW=n(J5e,"STRONG",{});var Rwr=s(nW);SRe=r(Rwr,"bigbird_pegasus"),Rwr.forEach(t),PRe=r(J5e," \u2014 "),r8=n(J5e,"A",{href:!0});var Swr=s(r8);$Re=r(Swr,"BigBirdPegasusConfig"),Swr.forEach(t),IRe=r(J5e," (BigBirdPegasus model)"),J5e.forEach(t),jRe=i(T),Nf=n(T,"LI",{});var Y5e=s(Nf);sW=n(Y5e,"STRONG",{});var Pwr=s(sW);NRe=r(Pwr,"blenderbot"),Pwr.forEach(t),DRe=r(Y5e," \u2014 "),t8=n(Y5e,"A",{href:!0});var $wr=s(t8);qRe=r($wr,"BlenderbotConfig"),$wr.forEach(t),GRe=r(Y5e," (Blenderbot model)"),Y5e.forEach(t),ORe=i(T),Df=n(T,"LI",{});var K5e=s(Df);lW=n(K5e,"STRONG",{});var Iwr=s(lW);XRe=r(Iwr,"blenderbot-small"),Iwr.forEach(t),zRe=r(K5e," \u2014 "),a8=n(K5e,"A",{href:!0});var jwr=s(a8);VRe=r(jwr,"BlenderbotSmallConfig"),jwr.forEach(t),WRe=r(K5e," (BlenderbotSmall model)"),K5e.forEach(t),QRe=i(T),qf=n(T,"LI",{});var Z5e=s(qf);iW=n(Z5e,"STRONG",{});var Nwr=s(iW);HRe=r(Nwr,"camembert"),Nwr.forEach(t),URe=r(Z5e," \u2014 "),n8=n(Z5e,"A",{href:!0});var Dwr=s(n8);JRe=r(Dwr,"CamembertConfig"),Dwr.forEach(t),YRe=r(Z5e," (CamemBERT model)"),Z5e.forEach(t),KRe=i(T),Gf=n(T,"LI",{});var e2e=s(Gf);dW=n(e2e,"STRONG",{});var qwr=s(dW);ZRe=r(qwr,"canine"),qwr.forEach(t),eSe=r(e2e," \u2014 "),s8=n(e2e,"A",{href:!0});var Gwr=s(s8);oSe=r(Gwr,"CanineConfig"),Gwr.forEach(t),rSe=r(e2e," (Canine model)"),e2e.forEach(t),tSe=i(T),Of=n(T,"LI",{});var o2e=s(Of);cW=n(o2e,"STRONG",{});var Owr=s(cW);aSe=r(Owr,"clip"),Owr.forEach(t),nSe=r(o2e," \u2014 "),l8=n(o2e,"A",{href:!0});var Xwr=s(l8);sSe=r(Xwr,"CLIPConfig"),Xwr.forEach(t),lSe=r(o2e," (CLIP model)"),o2e.forEach(t),iSe=i(T),Xf=n(T,"LI",{});var r2e=s(Xf);fW=n(r2e,"STRONG",{});var zwr=s(fW);dSe=r(zwr,"convbert"),zwr.forEach(t),cSe=r(r2e," \u2014 "),i8=n(r2e,"A",{href:!0});var Vwr=s(i8);fSe=r(Vwr,"ConvBertConfig"),Vwr.forEach(t),mSe=r(r2e," (ConvBERT model)"),r2e.forEach(t),gSe=i(T),zf=n(T,"LI",{});var t2e=s(zf);mW=n(t2e,"STRONG",{});var Wwr=s(mW);hSe=r(Wwr,"convnext"),Wwr.forEach(t),pSe=r(t2e," \u2014 "),d8=n(t2e,"A",{href:!0});var Qwr=s(d8);_Se=r(Qwr,"ConvNextConfig"),Qwr.forEach(t),uSe=r(t2e," (ConvNext model)"),t2e.forEach(t),bSe=i(T),Vf=n(T,"LI",{});var a2e=s(Vf);gW=n(a2e,"STRONG",{});var Hwr=s(gW);vSe=r(Hwr,"ctrl"),Hwr.forEach(t),TSe=r(a2e," \u2014 "),c8=n(a2e,"A",{href:!0});var Uwr=s(c8);FSe=r(Uwr,"CTRLConfig"),Uwr.forEach(t),CSe=r(a2e," (CTRL model)"),a2e.forEach(t),MSe=i(T),Wf=n(T,"LI",{});var n2e=s(Wf);hW=n(n2e,"STRONG",{});var Jwr=s(hW);ESe=r(Jwr,"deberta"),Jwr.forEach(t),ySe=r(n2e," \u2014 "),f8=n(n2e,"A",{href:!0});var Ywr=s(f8);wSe=r(Ywr,"DebertaConfig"),Ywr.forEach(t),ASe=r(n2e," (DeBERTa model)"),n2e.forEach(t),LSe=i(T),Qf=n(T,"LI",{});var s2e=s(Qf);pW=n(s2e,"STRONG",{});var Kwr=s(pW);BSe=r(Kwr,"deberta-v2"),Kwr.forEach(t),kSe=r(s2e," \u2014 "),m8=n(s2e,"A",{href:!0});var Zwr=s(m8);xSe=r(Zwr,"DebertaV2Config"),Zwr.forEach(t),RSe=r(s2e," (DeBERTa-v2 model)"),s2e.forEach(t),SSe=i(T),Hf=n(T,"LI",{});var l2e=s(Hf);_W=n(l2e,"STRONG",{});var e6r=s(_W);PSe=r(e6r,"deit"),e6r.forEach(t),$Se=r(l2e," \u2014 "),g8=n(l2e,"A",{href:!0});var o6r=s(g8);ISe=r(o6r,"DeiTConfig"),o6r.forEach(t),jSe=r(l2e," (DeiT model)"),l2e.forEach(t),NSe=i(T),Uf=n(T,"LI",{});var i2e=s(Uf);uW=n(i2e,"STRONG",{});var r6r=s(uW);DSe=r(r6r,"detr"),r6r.forEach(t),qSe=r(i2e," \u2014 "),h8=n(i2e,"A",{href:!0});var t6r=s(h8);GSe=r(t6r,"DetrConfig"),t6r.forEach(t),OSe=r(i2e," (DETR model)"),i2e.forEach(t),XSe=i(T),Jf=n(T,"LI",{});var d2e=s(Jf);bW=n(d2e,"STRONG",{});var a6r=s(bW);zSe=r(a6r,"distilbert"),a6r.forEach(t),VSe=r(d2e," \u2014 "),p8=n(d2e,"A",{href:!0});var n6r=s(p8);WSe=r(n6r,"DistilBertConfig"),n6r.forEach(t),QSe=r(d2e," (DistilBERT model)"),d2e.forEach(t),HSe=i(T),Yf=n(T,"LI",{});var c2e=s(Yf);vW=n(c2e,"STRONG",{});var s6r=s(vW);USe=r(s6r,"dpr"),s6r.forEach(t),JSe=r(c2e," \u2014 "),_8=n(c2e,"A",{href:!0});var l6r=s(_8);YSe=r(l6r,"DPRConfig"),l6r.forEach(t),KSe=r(c2e," (DPR model)"),c2e.forEach(t),ZSe=i(T),Kf=n(T,"LI",{});var f2e=s(Kf);TW=n(f2e,"STRONG",{});var i6r=s(TW);ePe=r(i6r,"electra"),i6r.forEach(t),oPe=r(f2e," \u2014 "),u8=n(f2e,"A",{href:!0});var d6r=s(u8);rPe=r(d6r,"ElectraConfig"),d6r.forEach(t),tPe=r(f2e," (ELECTRA model)"),f2e.forEach(t),aPe=i(T),Zf=n(T,"LI",{});var m2e=s(Zf);FW=n(m2e,"STRONG",{});var c6r=s(FW);nPe=r(c6r,"encoder-decoder"),c6r.forEach(t),sPe=r(m2e," \u2014 "),b8=n(m2e,"A",{href:!0});var f6r=s(b8);lPe=r(f6r,"EncoderDecoderConfig"),f6r.forEach(t),iPe=r(m2e," (Encoder decoder model)"),m2e.forEach(t),dPe=i(T),em=n(T,"LI",{});var g2e=s(em);CW=n(g2e,"STRONG",{});var m6r=s(CW);cPe=r(m6r,"flaubert"),m6r.forEach(t),fPe=r(g2e," \u2014 "),v8=n(g2e,"A",{href:!0});var g6r=s(v8);mPe=r(g6r,"FlaubertConfig"),g6r.forEach(t),gPe=r(g2e," (FlauBERT model)"),g2e.forEach(t),hPe=i(T),om=n(T,"LI",{});var h2e=s(om);MW=n(h2e,"STRONG",{});var h6r=s(MW);pPe=r(h6r,"fnet"),h6r.forEach(t),_Pe=r(h2e," \u2014 "),T8=n(h2e,"A",{href:!0});var p6r=s(T8);uPe=r(p6r,"FNetConfig"),p6r.forEach(t),bPe=r(h2e," (FNet model)"),h2e.forEach(t),vPe=i(T),rm=n(T,"LI",{});var p2e=s(rm);EW=n(p2e,"STRONG",{});var _6r=s(EW);TPe=r(_6r,"fsmt"),_6r.forEach(t),FPe=r(p2e," \u2014 "),F8=n(p2e,"A",{href:!0});var u6r=s(F8);CPe=r(u6r,"FSMTConfig"),u6r.forEach(t),MPe=r(p2e," (FairSeq Machine-Translation model)"),p2e.forEach(t),EPe=i(T),tm=n(T,"LI",{});var _2e=s(tm);yW=n(_2e,"STRONG",{});var b6r=s(yW);yPe=r(b6r,"funnel"),b6r.forEach(t),wPe=r(_2e," \u2014 "),C8=n(_2e,"A",{href:!0});var v6r=s(C8);APe=r(v6r,"FunnelConfig"),v6r.forEach(t),LPe=r(_2e," (Funnel Transformer model)"),_2e.forEach(t),BPe=i(T),am=n(T,"LI",{});var u2e=s(am);wW=n(u2e,"STRONG",{});var T6r=s(wW);kPe=r(T6r,"gpt2"),T6r.forEach(t),xPe=r(u2e," \u2014 "),M8=n(u2e,"A",{href:!0});var F6r=s(M8);RPe=r(F6r,"GPT2Config"),F6r.forEach(t),SPe=r(u2e," (OpenAI GPT-2 model)"),u2e.forEach(t),PPe=i(T),nm=n(T,"LI",{});var b2e=s(nm);AW=n(b2e,"STRONG",{});var C6r=s(AW);$Pe=r(C6r,"gpt_neo"),C6r.forEach(t),IPe=r(b2e," \u2014 "),E8=n(b2e,"A",{href:!0});var M6r=s(E8);jPe=r(M6r,"GPTNeoConfig"),M6r.forEach(t),NPe=r(b2e," (GPT Neo model)"),b2e.forEach(t),DPe=i(T),sm=n(T,"LI",{});var v2e=s(sm);LW=n(v2e,"STRONG",{});var E6r=s(LW);qPe=r(E6r,"gptj"),E6r.forEach(t),GPe=r(v2e," \u2014 "),y8=n(v2e,"A",{href:!0});var y6r=s(y8);OPe=r(y6r,"GPTJConfig"),y6r.forEach(t),XPe=r(v2e," (GPT-J model)"),v2e.forEach(t),zPe=i(T),lm=n(T,"LI",{});var T2e=s(lm);BW=n(T2e,"STRONG",{});var w6r=s(BW);VPe=r(w6r,"hubert"),w6r.forEach(t),WPe=r(T2e," \u2014 "),w8=n(T2e,"A",{href:!0});var A6r=s(w8);QPe=r(A6r,"HubertConfig"),A6r.forEach(t),HPe=r(T2e," (Hubert model)"),T2e.forEach(t),UPe=i(T),im=n(T,"LI",{});var F2e=s(im);kW=n(F2e,"STRONG",{});var L6r=s(kW);JPe=r(L6r,"ibert"),L6r.forEach(t),YPe=r(F2e," \u2014 "),A8=n(F2e,"A",{href:!0});var B6r=s(A8);KPe=r(B6r,"IBertConfig"),B6r.forEach(t),ZPe=r(F2e," (I-BERT model)"),F2e.forEach(t),e$e=i(T),dm=n(T,"LI",{});var C2e=s(dm);xW=n(C2e,"STRONG",{});var k6r=s(xW);o$e=r(k6r,"imagegpt"),k6r.forEach(t),r$e=r(C2e," \u2014 "),L8=n(C2e,"A",{href:!0});var x6r=s(L8);t$e=r(x6r,"ImageGPTConfig"),x6r.forEach(t),a$e=r(C2e," (ImageGPT model)"),C2e.forEach(t),n$e=i(T),cm=n(T,"LI",{});var M2e=s(cm);RW=n(M2e,"STRONG",{});var R6r=s(RW);s$e=r(R6r,"layoutlm"),R6r.forEach(t),l$e=r(M2e," \u2014 "),B8=n(M2e,"A",{href:!0});var S6r=s(B8);i$e=r(S6r,"LayoutLMConfig"),S6r.forEach(t),d$e=r(M2e," (LayoutLM model)"),M2e.forEach(t),c$e=i(T),fm=n(T,"LI",{});var E2e=s(fm);SW=n(E2e,"STRONG",{});var P6r=s(SW);f$e=r(P6r,"layoutlmv2"),P6r.forEach(t),m$e=r(E2e," \u2014 "),k8=n(E2e,"A",{href:!0});var $6r=s(k8);g$e=r($6r,"LayoutLMv2Config"),$6r.forEach(t),h$e=r(E2e," (LayoutLMv2 model)"),E2e.forEach(t),p$e=i(T),mm=n(T,"LI",{});var y2e=s(mm);PW=n(y2e,"STRONG",{});var I6r=s(PW);_$e=r(I6r,"led"),I6r.forEach(t),u$e=r(y2e," \u2014 "),x8=n(y2e,"A",{href:!0});var j6r=s(x8);b$e=r(j6r,"LEDConfig"),j6r.forEach(t),v$e=r(y2e," (LED model)"),y2e.forEach(t),T$e=i(T),gm=n(T,"LI",{});var w2e=s(gm);$W=n(w2e,"STRONG",{});var N6r=s($W);F$e=r(N6r,"longformer"),N6r.forEach(t),C$e=r(w2e," \u2014 "),R8=n(w2e,"A",{href:!0});var D6r=s(R8);M$e=r(D6r,"LongformerConfig"),D6r.forEach(t),E$e=r(w2e," (Longformer model)"),w2e.forEach(t),y$e=i(T),hm=n(T,"LI",{});var A2e=s(hm);IW=n(A2e,"STRONG",{});var q6r=s(IW);w$e=r(q6r,"luke"),q6r.forEach(t),A$e=r(A2e," \u2014 "),S8=n(A2e,"A",{href:!0});var G6r=s(S8);L$e=r(G6r,"LukeConfig"),G6r.forEach(t),B$e=r(A2e," (LUKE model)"),A2e.forEach(t),k$e=i(T),pm=n(T,"LI",{});var L2e=s(pm);jW=n(L2e,"STRONG",{});var O6r=s(jW);x$e=r(O6r,"lxmert"),O6r.forEach(t),R$e=r(L2e," \u2014 "),P8=n(L2e,"A",{href:!0});var X6r=s(P8);S$e=r(X6r,"LxmertConfig"),X6r.forEach(t),P$e=r(L2e," (LXMERT model)"),L2e.forEach(t),$$e=i(T),_m=n(T,"LI",{});var B2e=s(_m);NW=n(B2e,"STRONG",{});var z6r=s(NW);I$e=r(z6r,"m2m_100"),z6r.forEach(t),j$e=r(B2e," \u2014 "),$8=n(B2e,"A",{href:!0});var V6r=s($8);N$e=r(V6r,"M2M100Config"),V6r.forEach(t),D$e=r(B2e," (M2M100 model)"),B2e.forEach(t),q$e=i(T),um=n(T,"LI",{});var k2e=s(um);DW=n(k2e,"STRONG",{});var W6r=s(DW);G$e=r(W6r,"marian"),W6r.forEach(t),O$e=r(k2e," \u2014 "),I8=n(k2e,"A",{href:!0});var Q6r=s(I8);X$e=r(Q6r,"MarianConfig"),Q6r.forEach(t),z$e=r(k2e," (Marian model)"),k2e.forEach(t),V$e=i(T),bm=n(T,"LI",{});var x2e=s(bm);qW=n(x2e,"STRONG",{});var H6r=s(qW);W$e=r(H6r,"mbart"),H6r.forEach(t),Q$e=r(x2e," \u2014 "),j8=n(x2e,"A",{href:!0});var U6r=s(j8);H$e=r(U6r,"MBartConfig"),U6r.forEach(t),U$e=r(x2e," (mBART model)"),x2e.forEach(t),J$e=i(T),vm=n(T,"LI",{});var R2e=s(vm);GW=n(R2e,"STRONG",{});var J6r=s(GW);Y$e=r(J6r,"megatron-bert"),J6r.forEach(t),K$e=r(R2e," \u2014 "),N8=n(R2e,"A",{href:!0});var Y6r=s(N8);Z$e=r(Y6r,"MegatronBertConfig"),Y6r.forEach(t),eIe=r(R2e," (MegatronBert model)"),R2e.forEach(t),oIe=i(T),Tm=n(T,"LI",{});var S2e=s(Tm);OW=n(S2e,"STRONG",{});var K6r=s(OW);rIe=r(K6r,"mobilebert"),K6r.forEach(t),tIe=r(S2e," \u2014 "),D8=n(S2e,"A",{href:!0});var Z6r=s(D8);aIe=r(Z6r,"MobileBertConfig"),Z6r.forEach(t),nIe=r(S2e," (MobileBERT model)"),S2e.forEach(t),sIe=i(T),Fm=n(T,"LI",{});var P2e=s(Fm);XW=n(P2e,"STRONG",{});var eAr=s(XW);lIe=r(eAr,"mpnet"),eAr.forEach(t),iIe=r(P2e," \u2014 "),q8=n(P2e,"A",{href:!0});var oAr=s(q8);dIe=r(oAr,"MPNetConfig"),oAr.forEach(t),cIe=r(P2e," (MPNet model)"),P2e.forEach(t),fIe=i(T),Cm=n(T,"LI",{});var $2e=s(Cm);zW=n($2e,"STRONG",{});var rAr=s(zW);mIe=r(rAr,"mt5"),rAr.forEach(t),gIe=r($2e," \u2014 "),G8=n($2e,"A",{href:!0});var tAr=s(G8);hIe=r(tAr,"MT5Config"),tAr.forEach(t),pIe=r($2e," (mT5 model)"),$2e.forEach(t),_Ie=i(T),Mm=n(T,"LI",{});var I2e=s(Mm);VW=n(I2e,"STRONG",{});var aAr=s(VW);uIe=r(aAr,"nystromformer"),aAr.forEach(t),bIe=r(I2e," \u2014 "),O8=n(I2e,"A",{href:!0});var nAr=s(O8);vIe=r(nAr,"NystromformerConfig"),nAr.forEach(t),TIe=r(I2e," (Nystromformer model)"),I2e.forEach(t),FIe=i(T),Em=n(T,"LI",{});var j2e=s(Em);WW=n(j2e,"STRONG",{});var sAr=s(WW);CIe=r(sAr,"openai-gpt"),sAr.forEach(t),MIe=r(j2e," \u2014 "),X8=n(j2e,"A",{href:!0});var lAr=s(X8);EIe=r(lAr,"OpenAIGPTConfig"),lAr.forEach(t),yIe=r(j2e," (OpenAI GPT model)"),j2e.forEach(t),wIe=i(T),ym=n(T,"LI",{});var N2e=s(ym);QW=n(N2e,"STRONG",{});var iAr=s(QW);AIe=r(iAr,"pegasus"),iAr.forEach(t),LIe=r(N2e," \u2014 "),z8=n(N2e,"A",{href:!0});var dAr=s(z8);BIe=r(dAr,"PegasusConfig"),dAr.forEach(t),kIe=r(N2e," (Pegasus model)"),N2e.forEach(t),xIe=i(T),wm=n(T,"LI",{});var D2e=s(wm);HW=n(D2e,"STRONG",{});var cAr=s(HW);RIe=r(cAr,"perceiver"),cAr.forEach(t),SIe=r(D2e," \u2014 "),V8=n(D2e,"A",{href:!0});var fAr=s(V8);PIe=r(fAr,"PerceiverConfig"),fAr.forEach(t),$Ie=r(D2e," (Perceiver model)"),D2e.forEach(t),IIe=i(T),Am=n(T,"LI",{});var q2e=s(Am);UW=n(q2e,"STRONG",{});var mAr=s(UW);jIe=r(mAr,"plbart"),mAr.forEach(t),NIe=r(q2e," \u2014 "),W8=n(q2e,"A",{href:!0});var gAr=s(W8);DIe=r(gAr,"PLBartConfig"),gAr.forEach(t),qIe=r(q2e," (PLBart model)"),q2e.forEach(t),GIe=i(T),Lm=n(T,"LI",{});var G2e=s(Lm);JW=n(G2e,"STRONG",{});var hAr=s(JW);OIe=r(hAr,"poolformer"),hAr.forEach(t),XIe=r(G2e," \u2014 "),Q8=n(G2e,"A",{href:!0});var pAr=s(Q8);zIe=r(pAr,"PoolFormerConfig"),pAr.forEach(t),VIe=r(G2e," (PoolFormer model)"),G2e.forEach(t),WIe=i(T),Bm=n(T,"LI",{});var O2e=s(Bm);YW=n(O2e,"STRONG",{});var _Ar=s(YW);QIe=r(_Ar,"prophetnet"),_Ar.forEach(t),HIe=r(O2e," \u2014 "),H8=n(O2e,"A",{href:!0});var uAr=s(H8);UIe=r(uAr,"ProphetNetConfig"),uAr.forEach(t),JIe=r(O2e," (ProphetNet model)"),O2e.forEach(t),YIe=i(T),km=n(T,"LI",{});var X2e=s(km);KW=n(X2e,"STRONG",{});var bAr=s(KW);KIe=r(bAr,"qdqbert"),bAr.forEach(t),ZIe=r(X2e," \u2014 "),U8=n(X2e,"A",{href:!0});var vAr=s(U8);eje=r(vAr,"QDQBertConfig"),vAr.forEach(t),oje=r(X2e," (QDQBert model)"),X2e.forEach(t),rje=i(T),xm=n(T,"LI",{});var z2e=s(xm);ZW=n(z2e,"STRONG",{});var TAr=s(ZW);tje=r(TAr,"rag"),TAr.forEach(t),aje=r(z2e," \u2014 "),J8=n(z2e,"A",{href:!0});var FAr=s(J8);nje=r(FAr,"RagConfig"),FAr.forEach(t),sje=r(z2e," (RAG model)"),z2e.forEach(t),lje=i(T),Rm=n(T,"LI",{});var V2e=s(Rm);eQ=n(V2e,"STRONG",{});var CAr=s(eQ);ije=r(CAr,"realm"),CAr.forEach(t),dje=r(V2e," \u2014 "),Y8=n(V2e,"A",{href:!0});var MAr=s(Y8);cje=r(MAr,"RealmConfig"),MAr.forEach(t),fje=r(V2e," (Realm model)"),V2e.forEach(t),mje=i(T),Sm=n(T,"LI",{});var W2e=s(Sm);oQ=n(W2e,"STRONG",{});var EAr=s(oQ);gje=r(EAr,"reformer"),EAr.forEach(t),hje=r(W2e," \u2014 "),K8=n(W2e,"A",{href:!0});var yAr=s(K8);pje=r(yAr,"ReformerConfig"),yAr.forEach(t),_je=r(W2e," (Reformer model)"),W2e.forEach(t),uje=i(T),Pm=n(T,"LI",{});var Q2e=s(Pm);rQ=n(Q2e,"STRONG",{});var wAr=s(rQ);bje=r(wAr,"rembert"),wAr.forEach(t),vje=r(Q2e," \u2014 "),Z8=n(Q2e,"A",{href:!0});var AAr=s(Z8);Tje=r(AAr,"RemBertConfig"),AAr.forEach(t),Fje=r(Q2e," (RemBERT model)"),Q2e.forEach(t),Cje=i(T),$m=n(T,"LI",{});var H2e=s($m);tQ=n(H2e,"STRONG",{});var LAr=s(tQ);Mje=r(LAr,"resnet"),LAr.forEach(t),Eje=r(H2e," \u2014 "),e9=n(H2e,"A",{href:!0});var BAr=s(e9);yje=r(BAr,"ResNetConfig"),BAr.forEach(t),wje=r(H2e," (resnet model)"),H2e.forEach(t),Aje=i(T),Im=n(T,"LI",{});var U2e=s(Im);aQ=n(U2e,"STRONG",{});var kAr=s(aQ);Lje=r(kAr,"retribert"),kAr.forEach(t),Bje=r(U2e," \u2014 "),o9=n(U2e,"A",{href:!0});var xAr=s(o9);kje=r(xAr,"RetriBertConfig"),xAr.forEach(t),xje=r(U2e," (RetriBERT model)"),U2e.forEach(t),Rje=i(T),jm=n(T,"LI",{});var J2e=s(jm);nQ=n(J2e,"STRONG",{});var RAr=s(nQ);Sje=r(RAr,"roberta"),RAr.forEach(t),Pje=r(J2e," \u2014 "),r9=n(J2e,"A",{href:!0});var SAr=s(r9);$je=r(SAr,"RobertaConfig"),SAr.forEach(t),Ije=r(J2e," (RoBERTa model)"),J2e.forEach(t),jje=i(T),Nm=n(T,"LI",{});var Y2e=s(Nm);sQ=n(Y2e,"STRONG",{});var PAr=s(sQ);Nje=r(PAr,"roformer"),PAr.forEach(t),Dje=r(Y2e," \u2014 "),t9=n(Y2e,"A",{href:!0});var $Ar=s(t9);qje=r($Ar,"RoFormerConfig"),$Ar.forEach(t),Gje=r(Y2e," (RoFormer model)"),Y2e.forEach(t),Oje=i(T),Dm=n(T,"LI",{});var K2e=s(Dm);lQ=n(K2e,"STRONG",{});var IAr=s(lQ);Xje=r(IAr,"segformer"),IAr.forEach(t),zje=r(K2e," \u2014 "),a9=n(K2e,"A",{href:!0});var jAr=s(a9);Vje=r(jAr,"SegformerConfig"),jAr.forEach(t),Wje=r(K2e," (SegFormer model)"),K2e.forEach(t),Qje=i(T),qm=n(T,"LI",{});var Z2e=s(qm);iQ=n(Z2e,"STRONG",{});var NAr=s(iQ);Hje=r(NAr,"sew"),NAr.forEach(t),Uje=r(Z2e," \u2014 "),n9=n(Z2e,"A",{href:!0});var DAr=s(n9);Jje=r(DAr,"SEWConfig"),DAr.forEach(t),Yje=r(Z2e," (SEW model)"),Z2e.forEach(t),Kje=i(T),Gm=n(T,"LI",{});var eve=s(Gm);dQ=n(eve,"STRONG",{});var qAr=s(dQ);Zje=r(qAr,"sew-d"),qAr.forEach(t),eNe=r(eve," \u2014 "),s9=n(eve,"A",{href:!0});var GAr=s(s9);oNe=r(GAr,"SEWDConfig"),GAr.forEach(t),rNe=r(eve," (SEW-D model)"),eve.forEach(t),tNe=i(T),Om=n(T,"LI",{});var ove=s(Om);cQ=n(ove,"STRONG",{});var OAr=s(cQ);aNe=r(OAr,"speech-encoder-decoder"),OAr.forEach(t),nNe=r(ove," \u2014 "),l9=n(ove,"A",{href:!0});var XAr=s(l9);sNe=r(XAr,"SpeechEncoderDecoderConfig"),XAr.forEach(t),lNe=r(ove," (Speech Encoder decoder model)"),ove.forEach(t),iNe=i(T),Xm=n(T,"LI",{});var rve=s(Xm);fQ=n(rve,"STRONG",{});var zAr=s(fQ);dNe=r(zAr,"speech_to_text"),zAr.forEach(t),cNe=r(rve," \u2014 "),i9=n(rve,"A",{href:!0});var VAr=s(i9);fNe=r(VAr,"Speech2TextConfig"),VAr.forEach(t),mNe=r(rve," (Speech2Text model)"),rve.forEach(t),gNe=i(T),zm=n(T,"LI",{});var tve=s(zm);mQ=n(tve,"STRONG",{});var WAr=s(mQ);hNe=r(WAr,"speech_to_text_2"),WAr.forEach(t),pNe=r(tve," \u2014 "),d9=n(tve,"A",{href:!0});var QAr=s(d9);_Ne=r(QAr,"Speech2Text2Config"),QAr.forEach(t),uNe=r(tve," (Speech2Text2 model)"),tve.forEach(t),bNe=i(T),Vm=n(T,"LI",{});var ave=s(Vm);gQ=n(ave,"STRONG",{});var HAr=s(gQ);vNe=r(HAr,"splinter"),HAr.forEach(t),TNe=r(ave," \u2014 "),c9=n(ave,"A",{href:!0});var UAr=s(c9);FNe=r(UAr,"SplinterConfig"),UAr.forEach(t),CNe=r(ave," (Splinter model)"),ave.forEach(t),MNe=i(T),Wm=n(T,"LI",{});var nve=s(Wm);hQ=n(nve,"STRONG",{});var JAr=s(hQ);ENe=r(JAr,"squeezebert"),JAr.forEach(t),yNe=r(nve," \u2014 "),f9=n(nve,"A",{href:!0});var YAr=s(f9);wNe=r(YAr,"SqueezeBertConfig"),YAr.forEach(t),ANe=r(nve," (SqueezeBERT model)"),nve.forEach(t),LNe=i(T),Qm=n(T,"LI",{});var sve=s(Qm);pQ=n(sve,"STRONG",{});var KAr=s(pQ);BNe=r(KAr,"swin"),KAr.forEach(t),kNe=r(sve," \u2014 "),m9=n(sve,"A",{href:!0});var ZAr=s(m9);xNe=r(ZAr,"SwinConfig"),ZAr.forEach(t),RNe=r(sve," (Swin model)"),sve.forEach(t),SNe=i(T),Hm=n(T,"LI",{});var lve=s(Hm);_Q=n(lve,"STRONG",{});var eLr=s(_Q);PNe=r(eLr,"t5"),eLr.forEach(t),$Ne=r(lve," \u2014 "),g9=n(lve,"A",{href:!0});var oLr=s(g9);INe=r(oLr,"T5Config"),oLr.forEach(t),jNe=r(lve," (T5 model)"),lve.forEach(t),NNe=i(T),Um=n(T,"LI",{});var ive=s(Um);uQ=n(ive,"STRONG",{});var rLr=s(uQ);DNe=r(rLr,"tapas"),rLr.forEach(t),qNe=r(ive," \u2014 "),h9=n(ive,"A",{href:!0});var tLr=s(h9);GNe=r(tLr,"TapasConfig"),tLr.forEach(t),ONe=r(ive," (TAPAS model)"),ive.forEach(t),XNe=i(T),Jm=n(T,"LI",{});var dve=s(Jm);bQ=n(dve,"STRONG",{});var aLr=s(bQ);zNe=r(aLr,"transfo-xl"),aLr.forEach(t),VNe=r(dve," \u2014 "),p9=n(dve,"A",{href:!0});var nLr=s(p9);WNe=r(nLr,"TransfoXLConfig"),nLr.forEach(t),QNe=r(dve," (Transformer-XL model)"),dve.forEach(t),HNe=i(T),Ym=n(T,"LI",{});var cve=s(Ym);vQ=n(cve,"STRONG",{});var sLr=s(vQ);UNe=r(sLr,"trocr"),sLr.forEach(t),JNe=r(cve," \u2014 "),_9=n(cve,"A",{href:!0});var lLr=s(_9);YNe=r(lLr,"TrOCRConfig"),lLr.forEach(t),KNe=r(cve," (TrOCR model)"),cve.forEach(t),ZNe=i(T),Km=n(T,"LI",{});var fve=s(Km);TQ=n(fve,"STRONG",{});var iLr=s(TQ);eDe=r(iLr,"unispeech"),iLr.forEach(t),oDe=r(fve," \u2014 "),u9=n(fve,"A",{href:!0});var dLr=s(u9);rDe=r(dLr,"UniSpeechConfig"),dLr.forEach(t),tDe=r(fve," (UniSpeech model)"),fve.forEach(t),aDe=i(T),Zm=n(T,"LI",{});var mve=s(Zm);FQ=n(mve,"STRONG",{});var cLr=s(FQ);nDe=r(cLr,"unispeech-sat"),cLr.forEach(t),sDe=r(mve," \u2014 "),b9=n(mve,"A",{href:!0});var fLr=s(b9);lDe=r(fLr,"UniSpeechSatConfig"),fLr.forEach(t),iDe=r(mve," (UniSpeechSat model)"),mve.forEach(t),dDe=i(T),eg=n(T,"LI",{});var gve=s(eg);CQ=n(gve,"STRONG",{});var mLr=s(CQ);cDe=r(mLr,"vilt"),mLr.forEach(t),fDe=r(gve," \u2014 "),v9=n(gve,"A",{href:!0});var gLr=s(v9);mDe=r(gLr,"ViltConfig"),gLr.forEach(t),gDe=r(gve," (ViLT model)"),gve.forEach(t),hDe=i(T),og=n(T,"LI",{});var hve=s(og);MQ=n(hve,"STRONG",{});var hLr=s(MQ);pDe=r(hLr,"vision-encoder-decoder"),hLr.forEach(t),_De=r(hve," \u2014 "),T9=n(hve,"A",{href:!0});var pLr=s(T9);uDe=r(pLr,"VisionEncoderDecoderConfig"),pLr.forEach(t),bDe=r(hve," (Vision Encoder decoder model)"),hve.forEach(t),vDe=i(T),rg=n(T,"LI",{});var pve=s(rg);EQ=n(pve,"STRONG",{});var _Lr=s(EQ);TDe=r(_Lr,"vision-text-dual-encoder"),_Lr.forEach(t),FDe=r(pve," \u2014 "),F9=n(pve,"A",{href:!0});var uLr=s(F9);CDe=r(uLr,"VisionTextDualEncoderConfig"),uLr.forEach(t),MDe=r(pve," (VisionTextDualEncoder model)"),pve.forEach(t),EDe=i(T),tg=n(T,"LI",{});var _ve=s(tg);yQ=n(_ve,"STRONG",{});var bLr=s(yQ);yDe=r(bLr,"visual_bert"),bLr.forEach(t),wDe=r(_ve," \u2014 "),C9=n(_ve,"A",{href:!0});var vLr=s(C9);ADe=r(vLr,"VisualBertConfig"),vLr.forEach(t),LDe=r(_ve," (VisualBert model)"),_ve.forEach(t),BDe=i(T),ag=n(T,"LI",{});var uve=s(ag);wQ=n(uve,"STRONG",{});var TLr=s(wQ);kDe=r(TLr,"vit"),TLr.forEach(t),xDe=r(uve," \u2014 "),M9=n(uve,"A",{href:!0});var FLr=s(M9);RDe=r(FLr,"ViTConfig"),FLr.forEach(t),SDe=r(uve," (ViT model)"),uve.forEach(t),PDe=i(T),ng=n(T,"LI",{});var bve=s(ng);AQ=n(bve,"STRONG",{});var CLr=s(AQ);$De=r(CLr,"vit_mae"),CLr.forEach(t),IDe=r(bve," \u2014 "),E9=n(bve,"A",{href:!0});var MLr=s(E9);jDe=r(MLr,"ViTMAEConfig"),MLr.forEach(t),NDe=r(bve," (ViTMAE model)"),bve.forEach(t),DDe=i(T),sg=n(T,"LI",{});var vve=s(sg);LQ=n(vve,"STRONG",{});var ELr=s(LQ);qDe=r(ELr,"wav2vec2"),ELr.forEach(t),GDe=r(vve," \u2014 "),y9=n(vve,"A",{href:!0});var yLr=s(y9);ODe=r(yLr,"Wav2Vec2Config"),yLr.forEach(t),XDe=r(vve," (Wav2Vec2 model)"),vve.forEach(t),zDe=i(T),lg=n(T,"LI",{});var Tve=s(lg);BQ=n(Tve,"STRONG",{});var wLr=s(BQ);VDe=r(wLr,"wavlm"),wLr.forEach(t),WDe=r(Tve," \u2014 "),w9=n(Tve,"A",{href:!0});var ALr=s(w9);QDe=r(ALr,"WavLMConfig"),ALr.forEach(t),HDe=r(Tve," (WavLM model)"),Tve.forEach(t),UDe=i(T),ig=n(T,"LI",{});var Fve=s(ig);kQ=n(Fve,"STRONG",{});var LLr=s(kQ);JDe=r(LLr,"xglm"),LLr.forEach(t),YDe=r(Fve," \u2014 "),A9=n(Fve,"A",{href:!0});var BLr=s(A9);KDe=r(BLr,"XGLMConfig"),BLr.forEach(t),ZDe=r(Fve," (XGLM model)"),Fve.forEach(t),eqe=i(T),dg=n(T,"LI",{});var Cve=s(dg);xQ=n(Cve,"STRONG",{});var kLr=s(xQ);oqe=r(kLr,"xlm"),kLr.forEach(t),rqe=r(Cve," \u2014 "),L9=n(Cve,"A",{href:!0});var xLr=s(L9);tqe=r(xLr,"XLMConfig"),xLr.forEach(t),aqe=r(Cve," (XLM model)"),Cve.forEach(t),nqe=i(T),cg=n(T,"LI",{});var Mve=s(cg);RQ=n(Mve,"STRONG",{});var RLr=s(RQ);sqe=r(RLr,"xlm-prophetnet"),RLr.forEach(t),lqe=r(Mve," \u2014 "),B9=n(Mve,"A",{href:!0});var SLr=s(B9);iqe=r(SLr,"XLMProphetNetConfig"),SLr.forEach(t),dqe=r(Mve," (XLMProphetNet model)"),Mve.forEach(t),cqe=i(T),fg=n(T,"LI",{});var Eve=s(fg);SQ=n(Eve,"STRONG",{});var PLr=s(SQ);fqe=r(PLr,"xlm-roberta"),PLr.forEach(t),mqe=r(Eve," \u2014 "),k9=n(Eve,"A",{href:!0});var $Lr=s(k9);gqe=r($Lr,"XLMRobertaConfig"),$Lr.forEach(t),hqe=r(Eve," (XLM-RoBERTa model)"),Eve.forEach(t),pqe=i(T),mg=n(T,"LI",{});var yve=s(mg);PQ=n(yve,"STRONG",{});var ILr=s(PQ);_qe=r(ILr,"xlm-roberta-xl"),ILr.forEach(t),uqe=r(yve," \u2014 "),x9=n(yve,"A",{href:!0});var jLr=s(x9);bqe=r(jLr,"XLMRobertaXLConfig"),jLr.forEach(t),vqe=r(yve," (XLM-RoBERTa-XL model)"),yve.forEach(t),Tqe=i(T),gg=n(T,"LI",{});var wve=s(gg);$Q=n(wve,"STRONG",{});var NLr=s($Q);Fqe=r(NLr,"xlnet"),NLr.forEach(t),Cqe=r(wve," \u2014 "),R9=n(wve,"A",{href:!0});var DLr=s(R9);Mqe=r(DLr,"XLNetConfig"),DLr.forEach(t),Eqe=r(wve," (XLNet model)"),wve.forEach(t),yqe=i(T),hg=n(T,"LI",{});var Ave=s(hg);IQ=n(Ave,"STRONG",{});var qLr=s(IQ);wqe=r(qLr,"yoso"),qLr.forEach(t),Aqe=r(Ave," \u2014 "),S9=n(Ave,"A",{href:!0});var GLr=s(S9);Lqe=r(GLr,"YosoConfig"),GLr.forEach(t),Bqe=r(Ave," (YOSO model)"),Ave.forEach(t),T.forEach(t),kqe=i(ia),jQ=n(ia,"P",{});var OLr=s(jQ);xqe=r(OLr,"Examples:"),OLr.forEach(t),Rqe=i(ia),m(_M.$$.fragment,ia),ia.forEach(t),Sqe=i(Ps),pg=n(Ps,"DIV",{class:!0});var wBe=s(pg);m(uM.$$.fragment,wBe),Pqe=i(wBe),NQ=n(wBe,"P",{});var XLr=s(NQ);$qe=r(XLr,"Register a new configuration for this class."),XLr.forEach(t),wBe.forEach(t),Ps.forEach(t),w8e=i(d),Ii=n(d,"H2",{class:!0});var ABe=s(Ii);_g=n(ABe,"A",{id:!0,class:!0,href:!0});var zLr=s(_g);DQ=n(zLr,"SPAN",{});var VLr=s(DQ);m(bM.$$.fragment,VLr),VLr.forEach(t),zLr.forEach(t),Iqe=i(ABe),qQ=n(ABe,"SPAN",{});var WLr=s(qQ);jqe=r(WLr,"AutoTokenizer"),WLr.forEach(t),ABe.forEach(t),A8e=i(d),Oo=n(d,"DIV",{class:!0});var $s=s(Oo);m(vM.$$.fragment,$s),Nqe=i($s),TM=n($s,"P",{});var LBe=s(TM);Dqe=r(LBe,`This is a generic tokenizer class that will be instantiated as one of the tokenizer classes of the library when
created with the `),P9=n(LBe,"A",{href:!0});var QLr=s(P9);qqe=r(QLr,"AutoTokenizer.from_pretrained()"),QLr.forEach(t),Gqe=r(LBe," class method."),LBe.forEach(t),Oqe=i($s),FM=n($s,"P",{});var BBe=s(FM);Xqe=r(BBe,"This class cannot be instantiated directly using "),GQ=n(BBe,"CODE",{});var HLr=s(GQ);zqe=r(HLr,"__init__()"),HLr.forEach(t),Vqe=r(BBe," (throws an error)."),BBe.forEach(t),Wqe=i($s),mo=n($s,"DIV",{class:!0});var da=s(mo);m(CM.$$.fragment,da),Qqe=i(da),OQ=n(da,"P",{});var ULr=s(OQ);Hqe=r(ULr,"Instantiate one of the tokenizer classes of the library from a pretrained model vocabulary."),ULr.forEach(t),Uqe=i(da),ja=n(da,"P",{});var d4=s(ja);Jqe=r(d4,"The tokenizer class to instantiate is selected based on the "),XQ=n(d4,"CODE",{});var JLr=s(XQ);Yqe=r(JLr,"model_type"),JLr.forEach(t),Kqe=r(d4,` property of the config object (either
passed as an argument or loaded from `),zQ=n(d4,"CODE",{});var YLr=s(zQ);Zqe=r(YLr,"pretrained_model_name_or_path"),YLr.forEach(t),eGe=r(d4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),VQ=n(d4,"CODE",{});var KLr=s(VQ);oGe=r(KLr,"pretrained_model_name_or_path"),KLr.forEach(t),rGe=r(d4,":"),d4.forEach(t),tGe=i(da),M=n(da,"UL",{});var y=s(M);Dn=n(y,"LI",{});var HA=s(Dn);WQ=n(HA,"STRONG",{});var ZLr=s(WQ);aGe=r(ZLr,"albert"),ZLr.forEach(t),nGe=r(HA," \u2014 "),$9=n(HA,"A",{href:!0});var e8r=s($9);sGe=r(e8r,"AlbertTokenizer"),e8r.forEach(t),lGe=r(HA," or "),I9=n(HA,"A",{href:!0});var o8r=s(I9);iGe=r(o8r,"AlbertTokenizerFast"),o8r.forEach(t),dGe=r(HA," (ALBERT model)"),HA.forEach(t),cGe=i(y),qn=n(y,"LI",{});var UA=s(qn);QQ=n(UA,"STRONG",{});var r8r=s(QQ);fGe=r(r8r,"bart"),r8r.forEach(t),mGe=r(UA," \u2014 "),j9=n(UA,"A",{href:!0});var t8r=s(j9);gGe=r(t8r,"BartTokenizer"),t8r.forEach(t),hGe=r(UA," or "),N9=n(UA,"A",{href:!0});var a8r=s(N9);pGe=r(a8r,"BartTokenizerFast"),a8r.forEach(t),_Ge=r(UA," (BART model)"),UA.forEach(t),uGe=i(y),Gn=n(y,"LI",{});var JA=s(Gn);HQ=n(JA,"STRONG",{});var n8r=s(HQ);bGe=r(n8r,"barthez"),n8r.forEach(t),vGe=r(JA," \u2014 "),D9=n(JA,"A",{href:!0});var s8r=s(D9);TGe=r(s8r,"BarthezTokenizer"),s8r.forEach(t),FGe=r(JA," or "),q9=n(JA,"A",{href:!0});var l8r=s(q9);CGe=r(l8r,"BarthezTokenizerFast"),l8r.forEach(t),MGe=r(JA," (BARThez model)"),JA.forEach(t),EGe=i(y),ug=n(y,"LI",{});var Lve=s(ug);UQ=n(Lve,"STRONG",{});var i8r=s(UQ);yGe=r(i8r,"bartpho"),i8r.forEach(t),wGe=r(Lve," \u2014 "),G9=n(Lve,"A",{href:!0});var d8r=s(G9);AGe=r(d8r,"BartphoTokenizer"),d8r.forEach(t),LGe=r(Lve," (BARTpho model)"),Lve.forEach(t),BGe=i(y),On=n(y,"LI",{});var YA=s(On);JQ=n(YA,"STRONG",{});var c8r=s(JQ);kGe=r(c8r,"bert"),c8r.forEach(t),xGe=r(YA," \u2014 "),O9=n(YA,"A",{href:!0});var f8r=s(O9);RGe=r(f8r,"BertTokenizer"),f8r.forEach(t),SGe=r(YA," or "),X9=n(YA,"A",{href:!0});var m8r=s(X9);PGe=r(m8r,"BertTokenizerFast"),m8r.forEach(t),$Ge=r(YA," (BERT model)"),YA.forEach(t),IGe=i(y),bg=n(y,"LI",{});var Bve=s(bg);YQ=n(Bve,"STRONG",{});var g8r=s(YQ);jGe=r(g8r,"bert-generation"),g8r.forEach(t),NGe=r(Bve," \u2014 "),z9=n(Bve,"A",{href:!0});var h8r=s(z9);DGe=r(h8r,"BertGenerationTokenizer"),h8r.forEach(t),qGe=r(Bve," (Bert Generation model)"),Bve.forEach(t),GGe=i(y),vg=n(y,"LI",{});var kve=s(vg);KQ=n(kve,"STRONG",{});var p8r=s(KQ);OGe=r(p8r,"bert-japanese"),p8r.forEach(t),XGe=r(kve," \u2014 "),V9=n(kve,"A",{href:!0});var _8r=s(V9);zGe=r(_8r,"BertJapaneseTokenizer"),_8r.forEach(t),VGe=r(kve," (BertJapanese model)"),kve.forEach(t),WGe=i(y),Tg=n(y,"LI",{});var xve=s(Tg);ZQ=n(xve,"STRONG",{});var u8r=s(ZQ);QGe=r(u8r,"bertweet"),u8r.forEach(t),HGe=r(xve," \u2014 "),W9=n(xve,"A",{href:!0});var b8r=s(W9);UGe=r(b8r,"BertweetTokenizer"),b8r.forEach(t),JGe=r(xve," (Bertweet model)"),xve.forEach(t),YGe=i(y),Xn=n(y,"LI",{});var KA=s(Xn);eH=n(KA,"STRONG",{});var v8r=s(eH);KGe=r(v8r,"big_bird"),v8r.forEach(t),ZGe=r(KA," \u2014 "),Q9=n(KA,"A",{href:!0});var T8r=s(Q9);eOe=r(T8r,"BigBirdTokenizer"),T8r.forEach(t),oOe=r(KA," or "),H9=n(KA,"A",{href:!0});var F8r=s(H9);rOe=r(F8r,"BigBirdTokenizerFast"),F8r.forEach(t),tOe=r(KA," (BigBird model)"),KA.forEach(t),aOe=i(y),zn=n(y,"LI",{});var ZA=s(zn);oH=n(ZA,"STRONG",{});var C8r=s(oH);nOe=r(C8r,"bigbird_pegasus"),C8r.forEach(t),sOe=r(ZA," \u2014 "),U9=n(ZA,"A",{href:!0});var M8r=s(U9);lOe=r(M8r,"PegasusTokenizer"),M8r.forEach(t),iOe=r(ZA," or "),J9=n(ZA,"A",{href:!0});var E8r=s(J9);dOe=r(E8r,"PegasusTokenizerFast"),E8r.forEach(t),cOe=r(ZA," (BigBirdPegasus model)"),ZA.forEach(t),fOe=i(y),Vn=n(y,"LI",{});var eL=s(Vn);rH=n(eL,"STRONG",{});var y8r=s(rH);mOe=r(y8r,"blenderbot"),y8r.forEach(t),gOe=r(eL," \u2014 "),Y9=n(eL,"A",{href:!0});var w8r=s(Y9);hOe=r(w8r,"BlenderbotTokenizer"),w8r.forEach(t),pOe=r(eL," or "),K9=n(eL,"A",{href:!0});var A8r=s(K9);_Oe=r(A8r,"BlenderbotTokenizerFast"),A8r.forEach(t),uOe=r(eL," (Blenderbot model)"),eL.forEach(t),bOe=i(y),Fg=n(y,"LI",{});var Rve=s(Fg);tH=n(Rve,"STRONG",{});var L8r=s(tH);vOe=r(L8r,"blenderbot-small"),L8r.forEach(t),TOe=r(Rve," \u2014 "),Z9=n(Rve,"A",{href:!0});var B8r=s(Z9);FOe=r(B8r,"BlenderbotSmallTokenizer"),B8r.forEach(t),COe=r(Rve," (BlenderbotSmall model)"),Rve.forEach(t),MOe=i(y),Cg=n(y,"LI",{});var Sve=s(Cg);aH=n(Sve,"STRONG",{});var k8r=s(aH);EOe=r(k8r,"byt5"),k8r.forEach(t),yOe=r(Sve," \u2014 "),eB=n(Sve,"A",{href:!0});var x8r=s(eB);wOe=r(x8r,"ByT5Tokenizer"),x8r.forEach(t),AOe=r(Sve," (ByT5 model)"),Sve.forEach(t),LOe=i(y),Wn=n(y,"LI",{});var oL=s(Wn);nH=n(oL,"STRONG",{});var R8r=s(nH);BOe=r(R8r,"camembert"),R8r.forEach(t),kOe=r(oL," \u2014 "),oB=n(oL,"A",{href:!0});var S8r=s(oB);xOe=r(S8r,"CamembertTokenizer"),S8r.forEach(t),ROe=r(oL," or "),rB=n(oL,"A",{href:!0});var P8r=s(rB);SOe=r(P8r,"CamembertTokenizerFast"),P8r.forEach(t),POe=r(oL," (CamemBERT model)"),oL.forEach(t),$Oe=i(y),Mg=n(y,"LI",{});var Pve=s(Mg);sH=n(Pve,"STRONG",{});var $8r=s(sH);IOe=r($8r,"canine"),$8r.forEach(t),jOe=r(Pve," \u2014 "),tB=n(Pve,"A",{href:!0});var I8r=s(tB);NOe=r(I8r,"CanineTokenizer"),I8r.forEach(t),DOe=r(Pve," (Canine model)"),Pve.forEach(t),qOe=i(y),Qn=n(y,"LI",{});var rL=s(Qn);lH=n(rL,"STRONG",{});var j8r=s(lH);GOe=r(j8r,"clip"),j8r.forEach(t),OOe=r(rL," \u2014 "),aB=n(rL,"A",{href:!0});var N8r=s(aB);XOe=r(N8r,"CLIPTokenizer"),N8r.forEach(t),zOe=r(rL," or "),nB=n(rL,"A",{href:!0});var D8r=s(nB);VOe=r(D8r,"CLIPTokenizerFast"),D8r.forEach(t),WOe=r(rL," (CLIP model)"),rL.forEach(t),QOe=i(y),Hn=n(y,"LI",{});var tL=s(Hn);iH=n(tL,"STRONG",{});var q8r=s(iH);HOe=r(q8r,"convbert"),q8r.forEach(t),UOe=r(tL," \u2014 "),sB=n(tL,"A",{href:!0});var G8r=s(sB);JOe=r(G8r,"ConvBertTokenizer"),G8r.forEach(t),YOe=r(tL," or "),lB=n(tL,"A",{href:!0});var O8r=s(lB);KOe=r(O8r,"ConvBertTokenizerFast"),O8r.forEach(t),ZOe=r(tL," (ConvBERT model)"),tL.forEach(t),eXe=i(y),Un=n(y,"LI",{});var aL=s(Un);dH=n(aL,"STRONG",{});var X8r=s(dH);oXe=r(X8r,"cpm"),X8r.forEach(t),rXe=r(aL," \u2014 "),iB=n(aL,"A",{href:!0});var z8r=s(iB);tXe=r(z8r,"CpmTokenizer"),z8r.forEach(t),aXe=r(aL," or "),cH=n(aL,"CODE",{});var V8r=s(cH);nXe=r(V8r,"CpmTokenizerFast"),V8r.forEach(t),sXe=r(aL," (CPM model)"),aL.forEach(t),lXe=i(y),Eg=n(y,"LI",{});var $ve=s(Eg);fH=n($ve,"STRONG",{});var W8r=s(fH);iXe=r(W8r,"ctrl"),W8r.forEach(t),dXe=r($ve," \u2014 "),dB=n($ve,"A",{href:!0});var Q8r=s(dB);cXe=r(Q8r,"CTRLTokenizer"),Q8r.forEach(t),fXe=r($ve," (CTRL model)"),$ve.forEach(t),mXe=i(y),Jn=n(y,"LI",{});var nL=s(Jn);mH=n(nL,"STRONG",{});var H8r=s(mH);gXe=r(H8r,"deberta"),H8r.forEach(t),hXe=r(nL," \u2014 "),cB=n(nL,"A",{href:!0});var U8r=s(cB);pXe=r(U8r,"DebertaTokenizer"),U8r.forEach(t),_Xe=r(nL," or "),fB=n(nL,"A",{href:!0});var J8r=s(fB);uXe=r(J8r,"DebertaTokenizerFast"),J8r.forEach(t),bXe=r(nL," (DeBERTa model)"),nL.forEach(t),vXe=i(y),yg=n(y,"LI",{});var Ive=s(yg);gH=n(Ive,"STRONG",{});var Y8r=s(gH);TXe=r(Y8r,"deberta-v2"),Y8r.forEach(t),FXe=r(Ive," \u2014 "),mB=n(Ive,"A",{href:!0});var K8r=s(mB);CXe=r(K8r,"DebertaV2Tokenizer"),K8r.forEach(t),MXe=r(Ive," (DeBERTa-v2 model)"),Ive.forEach(t),EXe=i(y),Yn=n(y,"LI",{});var sL=s(Yn);hH=n(sL,"STRONG",{});var Z8r=s(hH);yXe=r(Z8r,"distilbert"),Z8r.forEach(t),wXe=r(sL," \u2014 "),gB=n(sL,"A",{href:!0});var e9r=s(gB);AXe=r(e9r,"DistilBertTokenizer"),e9r.forEach(t),LXe=r(sL," or "),hB=n(sL,"A",{href:!0});var o9r=s(hB);BXe=r(o9r,"DistilBertTokenizerFast"),o9r.forEach(t),kXe=r(sL," (DistilBERT model)"),sL.forEach(t),xXe=i(y),Kn=n(y,"LI",{});var lL=s(Kn);pH=n(lL,"STRONG",{});var r9r=s(pH);RXe=r(r9r,"dpr"),r9r.forEach(t),SXe=r(lL," \u2014 "),pB=n(lL,"A",{href:!0});var t9r=s(pB);PXe=r(t9r,"DPRQuestionEncoderTokenizer"),t9r.forEach(t),$Xe=r(lL," or "),_B=n(lL,"A",{href:!0});var a9r=s(_B);IXe=r(a9r,"DPRQuestionEncoderTokenizerFast"),a9r.forEach(t),jXe=r(lL," (DPR model)"),lL.forEach(t),NXe=i(y),Zn=n(y,"LI",{});var iL=s(Zn);_H=n(iL,"STRONG",{});var n9r=s(_H);DXe=r(n9r,"electra"),n9r.forEach(t),qXe=r(iL," \u2014 "),uB=n(iL,"A",{href:!0});var s9r=s(uB);GXe=r(s9r,"ElectraTokenizer"),s9r.forEach(t),OXe=r(iL," or "),bB=n(iL,"A",{href:!0});var l9r=s(bB);XXe=r(l9r,"ElectraTokenizerFast"),l9r.forEach(t),zXe=r(iL," (ELECTRA model)"),iL.forEach(t),VXe=i(y),wg=n(y,"LI",{});var jve=s(wg);uH=n(jve,"STRONG",{});var i9r=s(uH);WXe=r(i9r,"flaubert"),i9r.forEach(t),QXe=r(jve," \u2014 "),vB=n(jve,"A",{href:!0});var d9r=s(vB);HXe=r(d9r,"FlaubertTokenizer"),d9r.forEach(t),UXe=r(jve," (FlauBERT model)"),jve.forEach(t),JXe=i(y),es=n(y,"LI",{});var dL=s(es);bH=n(dL,"STRONG",{});var c9r=s(bH);YXe=r(c9r,"fnet"),c9r.forEach(t),KXe=r(dL," \u2014 "),TB=n(dL,"A",{href:!0});var f9r=s(TB);ZXe=r(f9r,"FNetTokenizer"),f9r.forEach(t),eze=r(dL," or "),FB=n(dL,"A",{href:!0});var m9r=s(FB);oze=r(m9r,"FNetTokenizerFast"),m9r.forEach(t),rze=r(dL," (FNet model)"),dL.forEach(t),tze=i(y),Ag=n(y,"LI",{});var Nve=s(Ag);vH=n(Nve,"STRONG",{});var g9r=s(vH);aze=r(g9r,"fsmt"),g9r.forEach(t),nze=r(Nve," \u2014 "),CB=n(Nve,"A",{href:!0});var h9r=s(CB);sze=r(h9r,"FSMTTokenizer"),h9r.forEach(t),lze=r(Nve," (FairSeq Machine-Translation model)"),Nve.forEach(t),ize=i(y),os=n(y,"LI",{});var cL=s(os);TH=n(cL,"STRONG",{});var p9r=s(TH);dze=r(p9r,"funnel"),p9r.forEach(t),cze=r(cL," \u2014 "),MB=n(cL,"A",{href:!0});var _9r=s(MB);fze=r(_9r,"FunnelTokenizer"),_9r.forEach(t),mze=r(cL," or "),EB=n(cL,"A",{href:!0});var u9r=s(EB);gze=r(u9r,"FunnelTokenizerFast"),u9r.forEach(t),hze=r(cL," (Funnel Transformer model)"),cL.forEach(t),pze=i(y),rs=n(y,"LI",{});var fL=s(rs);FH=n(fL,"STRONG",{});var b9r=s(FH);_ze=r(b9r,"gpt2"),b9r.forEach(t),uze=r(fL," \u2014 "),yB=n(fL,"A",{href:!0});var v9r=s(yB);bze=r(v9r,"GPT2Tokenizer"),v9r.forEach(t),vze=r(fL," or "),wB=n(fL,"A",{href:!0});var T9r=s(wB);Tze=r(T9r,"GPT2TokenizerFast"),T9r.forEach(t),Fze=r(fL," (OpenAI GPT-2 model)"),fL.forEach(t),Cze=i(y),ts=n(y,"LI",{});var mL=s(ts);CH=n(mL,"STRONG",{});var F9r=s(CH);Mze=r(F9r,"gpt_neo"),F9r.forEach(t),Eze=r(mL," \u2014 "),AB=n(mL,"A",{href:!0});var C9r=s(AB);yze=r(C9r,"GPT2Tokenizer"),C9r.forEach(t),wze=r(mL," or "),LB=n(mL,"A",{href:!0});var M9r=s(LB);Aze=r(M9r,"GPT2TokenizerFast"),M9r.forEach(t),Lze=r(mL," (GPT Neo model)"),mL.forEach(t),Bze=i(y),as=n(y,"LI",{});var gL=s(as);MH=n(gL,"STRONG",{});var E9r=s(MH);kze=r(E9r,"herbert"),E9r.forEach(t),xze=r(gL," \u2014 "),BB=n(gL,"A",{href:!0});var y9r=s(BB);Rze=r(y9r,"HerbertTokenizer"),y9r.forEach(t),Sze=r(gL," or "),kB=n(gL,"A",{href:!0});var w9r=s(kB);Pze=r(w9r,"HerbertTokenizerFast"),w9r.forEach(t),$ze=r(gL," (HerBERT model)"),gL.forEach(t),Ize=i(y),Lg=n(y,"LI",{});var Dve=s(Lg);EH=n(Dve,"STRONG",{});var A9r=s(EH);jze=r(A9r,"hubert"),A9r.forEach(t),Nze=r(Dve," \u2014 "),xB=n(Dve,"A",{href:!0});var L9r=s(xB);Dze=r(L9r,"Wav2Vec2CTCTokenizer"),L9r.forEach(t),qze=r(Dve," (Hubert model)"),Dve.forEach(t),Gze=i(y),ns=n(y,"LI",{});var hL=s(ns);yH=n(hL,"STRONG",{});var B9r=s(yH);Oze=r(B9r,"ibert"),B9r.forEach(t),Xze=r(hL," \u2014 "),RB=n(hL,"A",{href:!0});var k9r=s(RB);zze=r(k9r,"RobertaTokenizer"),k9r.forEach(t),Vze=r(hL," or "),SB=n(hL,"A",{href:!0});var x9r=s(SB);Wze=r(x9r,"RobertaTokenizerFast"),x9r.forEach(t),Qze=r(hL," (I-BERT model)"),hL.forEach(t),Hze=i(y),ss=n(y,"LI",{});var pL=s(ss);wH=n(pL,"STRONG",{});var R9r=s(wH);Uze=r(R9r,"layoutlm"),R9r.forEach(t),Jze=r(pL," \u2014 "),PB=n(pL,"A",{href:!0});var S9r=s(PB);Yze=r(S9r,"LayoutLMTokenizer"),S9r.forEach(t),Kze=r(pL," or "),$B=n(pL,"A",{href:!0});var P9r=s($B);Zze=r(P9r,"LayoutLMTokenizerFast"),P9r.forEach(t),eVe=r(pL," (LayoutLM model)"),pL.forEach(t),oVe=i(y),ls=n(y,"LI",{});var _L=s(ls);AH=n(_L,"STRONG",{});var $9r=s(AH);rVe=r($9r,"layoutlmv2"),$9r.forEach(t),tVe=r(_L," \u2014 "),IB=n(_L,"A",{href:!0});var I9r=s(IB);aVe=r(I9r,"LayoutLMv2Tokenizer"),I9r.forEach(t),nVe=r(_L," or "),jB=n(_L,"A",{href:!0});var j9r=s(jB);sVe=r(j9r,"LayoutLMv2TokenizerFast"),j9r.forEach(t),lVe=r(_L," (LayoutLMv2 model)"),_L.forEach(t),iVe=i(y),is=n(y,"LI",{});var uL=s(is);LH=n(uL,"STRONG",{});var N9r=s(LH);dVe=r(N9r,"layoutxlm"),N9r.forEach(t),cVe=r(uL," \u2014 "),NB=n(uL,"A",{href:!0});var D9r=s(NB);fVe=r(D9r,"LayoutXLMTokenizer"),D9r.forEach(t),mVe=r(uL," or "),DB=n(uL,"A",{href:!0});var q9r=s(DB);gVe=r(q9r,"LayoutXLMTokenizerFast"),q9r.forEach(t),hVe=r(uL," (LayoutXLM model)"),uL.forEach(t),pVe=i(y),ds=n(y,"LI",{});var bL=s(ds);BH=n(bL,"STRONG",{});var G9r=s(BH);_Ve=r(G9r,"led"),G9r.forEach(t),uVe=r(bL," \u2014 "),qB=n(bL,"A",{href:!0});var O9r=s(qB);bVe=r(O9r,"LEDTokenizer"),O9r.forEach(t),vVe=r(bL," or "),GB=n(bL,"A",{href:!0});var X9r=s(GB);TVe=r(X9r,"LEDTokenizerFast"),X9r.forEach(t),FVe=r(bL," (LED model)"),bL.forEach(t),CVe=i(y),cs=n(y,"LI",{});var vL=s(cs);kH=n(vL,"STRONG",{});var z9r=s(kH);MVe=r(z9r,"longformer"),z9r.forEach(t),EVe=r(vL," \u2014 "),OB=n(vL,"A",{href:!0});var V9r=s(OB);yVe=r(V9r,"LongformerTokenizer"),V9r.forEach(t),wVe=r(vL," or "),XB=n(vL,"A",{href:!0});var W9r=s(XB);AVe=r(W9r,"LongformerTokenizerFast"),W9r.forEach(t),LVe=r(vL," (Longformer model)"),vL.forEach(t),BVe=i(y),Bg=n(y,"LI",{});var qve=s(Bg);xH=n(qve,"STRONG",{});var Q9r=s(xH);kVe=r(Q9r,"luke"),Q9r.forEach(t),xVe=r(qve," \u2014 "),zB=n(qve,"A",{href:!0});var H9r=s(zB);RVe=r(H9r,"LukeTokenizer"),H9r.forEach(t),SVe=r(qve," (LUKE model)"),qve.forEach(t),PVe=i(y),fs=n(y,"LI",{});var TL=s(fs);RH=n(TL,"STRONG",{});var U9r=s(RH);$Ve=r(U9r,"lxmert"),U9r.forEach(t),IVe=r(TL," \u2014 "),VB=n(TL,"A",{href:!0});var J9r=s(VB);jVe=r(J9r,"LxmertTokenizer"),J9r.forEach(t),NVe=r(TL," or "),WB=n(TL,"A",{href:!0});var Y9r=s(WB);DVe=r(Y9r,"LxmertTokenizerFast"),Y9r.forEach(t),qVe=r(TL," (LXMERT model)"),TL.forEach(t),GVe=i(y),kg=n(y,"LI",{});var Gve=s(kg);SH=n(Gve,"STRONG",{});var K9r=s(SH);OVe=r(K9r,"m2m_100"),K9r.forEach(t),XVe=r(Gve," \u2014 "),QB=n(Gve,"A",{href:!0});var Z9r=s(QB);zVe=r(Z9r,"M2M100Tokenizer"),Z9r.forEach(t),VVe=r(Gve," (M2M100 model)"),Gve.forEach(t),WVe=i(y),xg=n(y,"LI",{});var Ove=s(xg);PH=n(Ove,"STRONG",{});var eBr=s(PH);QVe=r(eBr,"marian"),eBr.forEach(t),HVe=r(Ove," \u2014 "),HB=n(Ove,"A",{href:!0});var oBr=s(HB);UVe=r(oBr,"MarianTokenizer"),oBr.forEach(t),JVe=r(Ove," (Marian model)"),Ove.forEach(t),YVe=i(y),ms=n(y,"LI",{});var FL=s(ms);$H=n(FL,"STRONG",{});var rBr=s($H);KVe=r(rBr,"mbart"),rBr.forEach(t),ZVe=r(FL," \u2014 "),UB=n(FL,"A",{href:!0});var tBr=s(UB);eWe=r(tBr,"MBartTokenizer"),tBr.forEach(t),oWe=r(FL," or "),JB=n(FL,"A",{href:!0});var aBr=s(JB);rWe=r(aBr,"MBartTokenizerFast"),aBr.forEach(t),tWe=r(FL," (mBART model)"),FL.forEach(t),aWe=i(y),gs=n(y,"LI",{});var CL=s(gs);IH=n(CL,"STRONG",{});var nBr=s(IH);nWe=r(nBr,"mbart50"),nBr.forEach(t),sWe=r(CL," \u2014 "),YB=n(CL,"A",{href:!0});var sBr=s(YB);lWe=r(sBr,"MBart50Tokenizer"),sBr.forEach(t),iWe=r(CL," or "),KB=n(CL,"A",{href:!0});var lBr=s(KB);dWe=r(lBr,"MBart50TokenizerFast"),lBr.forEach(t),cWe=r(CL," (mBART-50 model)"),CL.forEach(t),fWe=i(y),Rg=n(y,"LI",{});var Xve=s(Rg);jH=n(Xve,"STRONG",{});var iBr=s(jH);mWe=r(iBr,"mluke"),iBr.forEach(t),gWe=r(Xve," \u2014 "),ZB=n(Xve,"A",{href:!0});var dBr=s(ZB);hWe=r(dBr,"MLukeTokenizer"),dBr.forEach(t),pWe=r(Xve," (mLUKE model)"),Xve.forEach(t),_We=i(y),hs=n(y,"LI",{});var ML=s(hs);NH=n(ML,"STRONG",{});var cBr=s(NH);uWe=r(cBr,"mobilebert"),cBr.forEach(t),bWe=r(ML," \u2014 "),ek=n(ML,"A",{href:!0});var fBr=s(ek);vWe=r(fBr,"MobileBertTokenizer"),fBr.forEach(t),TWe=r(ML," or "),ok=n(ML,"A",{href:!0});var mBr=s(ok);FWe=r(mBr,"MobileBertTokenizerFast"),mBr.forEach(t),CWe=r(ML," (MobileBERT model)"),ML.forEach(t),MWe=i(y),ps=n(y,"LI",{});var EL=s(ps);DH=n(EL,"STRONG",{});var gBr=s(DH);EWe=r(gBr,"mpnet"),gBr.forEach(t),yWe=r(EL," \u2014 "),rk=n(EL,"A",{href:!0});var hBr=s(rk);wWe=r(hBr,"MPNetTokenizer"),hBr.forEach(t),AWe=r(EL," or "),tk=n(EL,"A",{href:!0});var pBr=s(tk);LWe=r(pBr,"MPNetTokenizerFast"),pBr.forEach(t),BWe=r(EL," (MPNet model)"),EL.forEach(t),kWe=i(y),_s=n(y,"LI",{});var yL=s(_s);qH=n(yL,"STRONG",{});var _Br=s(qH);xWe=r(_Br,"mt5"),_Br.forEach(t),RWe=r(yL," \u2014 "),ak=n(yL,"A",{href:!0});var uBr=s(ak);SWe=r(uBr,"MT5Tokenizer"),uBr.forEach(t),PWe=r(yL," or "),nk=n(yL,"A",{href:!0});var bBr=s(nk);$We=r(bBr,"MT5TokenizerFast"),bBr.forEach(t),IWe=r(yL," (mT5 model)"),yL.forEach(t),jWe=i(y),us=n(y,"LI",{});var wL=s(us);GH=n(wL,"STRONG",{});var vBr=s(GH);NWe=r(vBr,"openai-gpt"),vBr.forEach(t),DWe=r(wL," \u2014 "),sk=n(wL,"A",{href:!0});var TBr=s(sk);qWe=r(TBr,"OpenAIGPTTokenizer"),TBr.forEach(t),GWe=r(wL," or "),lk=n(wL,"A",{href:!0});var FBr=s(lk);OWe=r(FBr,"OpenAIGPTTokenizerFast"),FBr.forEach(t),XWe=r(wL," (OpenAI GPT model)"),wL.forEach(t),zWe=i(y),bs=n(y,"LI",{});var AL=s(bs);OH=n(AL,"STRONG",{});var CBr=s(OH);VWe=r(CBr,"pegasus"),CBr.forEach(t),WWe=r(AL," \u2014 "),ik=n(AL,"A",{href:!0});var MBr=s(ik);QWe=r(MBr,"PegasusTokenizer"),MBr.forEach(t),HWe=r(AL," or "),dk=n(AL,"A",{href:!0});var EBr=s(dk);UWe=r(EBr,"PegasusTokenizerFast"),EBr.forEach(t),JWe=r(AL," (Pegasus model)"),AL.forEach(t),YWe=i(y),Sg=n(y,"LI",{});var zve=s(Sg);XH=n(zve,"STRONG",{});var yBr=s(XH);KWe=r(yBr,"perceiver"),yBr.forEach(t),ZWe=r(zve," \u2014 "),ck=n(zve,"A",{href:!0});var wBr=s(ck);eQe=r(wBr,"PerceiverTokenizer"),wBr.forEach(t),oQe=r(zve," (Perceiver model)"),zve.forEach(t),rQe=i(y),Pg=n(y,"LI",{});var Vve=s(Pg);zH=n(Vve,"STRONG",{});var ABr=s(zH);tQe=r(ABr,"phobert"),ABr.forEach(t),aQe=r(Vve," \u2014 "),fk=n(Vve,"A",{href:!0});var LBr=s(fk);nQe=r(LBr,"PhobertTokenizer"),LBr.forEach(t),sQe=r(Vve," (PhoBERT model)"),Vve.forEach(t),lQe=i(y),$g=n(y,"LI",{});var Wve=s($g);VH=n(Wve,"STRONG",{});var BBr=s(VH);iQe=r(BBr,"plbart"),BBr.forEach(t),dQe=r(Wve," \u2014 "),mk=n(Wve,"A",{href:!0});var kBr=s(mk);cQe=r(kBr,"PLBartTokenizer"),kBr.forEach(t),fQe=r(Wve," (PLBart model)"),Wve.forEach(t),mQe=i(y),Ig=n(y,"LI",{});var Qve=s(Ig);WH=n(Qve,"STRONG",{});var xBr=s(WH);gQe=r(xBr,"prophetnet"),xBr.forEach(t),hQe=r(Qve," \u2014 "),gk=n(Qve,"A",{href:!0});var RBr=s(gk);pQe=r(RBr,"ProphetNetTokenizer"),RBr.forEach(t),_Qe=r(Qve," (ProphetNet model)"),Qve.forEach(t),uQe=i(y),vs=n(y,"LI",{});var LL=s(vs);QH=n(LL,"STRONG",{});var SBr=s(QH);bQe=r(SBr,"qdqbert"),SBr.forEach(t),vQe=r(LL," \u2014 "),hk=n(LL,"A",{href:!0});var PBr=s(hk);TQe=r(PBr,"BertTokenizer"),PBr.forEach(t),FQe=r(LL," or "),pk=n(LL,"A",{href:!0});var $Br=s(pk);CQe=r($Br,"BertTokenizerFast"),$Br.forEach(t),MQe=r(LL," (QDQBert model)"),LL.forEach(t),EQe=i(y),jg=n(y,"LI",{});var Hve=s(jg);HH=n(Hve,"STRONG",{});var IBr=s(HH);yQe=r(IBr,"rag"),IBr.forEach(t),wQe=r(Hve," \u2014 "),_k=n(Hve,"A",{href:!0});var jBr=s(_k);AQe=r(jBr,"RagTokenizer"),jBr.forEach(t),LQe=r(Hve," (RAG model)"),Hve.forEach(t),BQe=i(y),Ts=n(y,"LI",{});var BL=s(Ts);UH=n(BL,"STRONG",{});var NBr=s(UH);kQe=r(NBr,"reformer"),NBr.forEach(t),xQe=r(BL," \u2014 "),uk=n(BL,"A",{href:!0});var DBr=s(uk);RQe=r(DBr,"ReformerTokenizer"),DBr.forEach(t),SQe=r(BL," or "),bk=n(BL,"A",{href:!0});var qBr=s(bk);PQe=r(qBr,"ReformerTokenizerFast"),qBr.forEach(t),$Qe=r(BL," (Reformer model)"),BL.forEach(t),IQe=i(y),Fs=n(y,"LI",{});var kL=s(Fs);JH=n(kL,"STRONG",{});var GBr=s(JH);jQe=r(GBr,"rembert"),GBr.forEach(t),NQe=r(kL," \u2014 "),vk=n(kL,"A",{href:!0});var OBr=s(vk);DQe=r(OBr,"RemBertTokenizer"),OBr.forEach(t),qQe=r(kL," or "),Tk=n(kL,"A",{href:!0});var XBr=s(Tk);GQe=r(XBr,"RemBertTokenizerFast"),XBr.forEach(t),OQe=r(kL," (RemBERT model)"),kL.forEach(t),XQe=i(y),Cs=n(y,"LI",{});var xL=s(Cs);YH=n(xL,"STRONG",{});var zBr=s(YH);zQe=r(zBr,"retribert"),zBr.forEach(t),VQe=r(xL," \u2014 "),Fk=n(xL,"A",{href:!0});var VBr=s(Fk);WQe=r(VBr,"RetriBertTokenizer"),VBr.forEach(t),QQe=r(xL," or "),Ck=n(xL,"A",{href:!0});var WBr=s(Ck);HQe=r(WBr,"RetriBertTokenizerFast"),WBr.forEach(t),UQe=r(xL," (RetriBERT model)"),xL.forEach(t),JQe=i(y),Ms=n(y,"LI",{});var RL=s(Ms);KH=n(RL,"STRONG",{});var QBr=s(KH);YQe=r(QBr,"roberta"),QBr.forEach(t),KQe=r(RL," \u2014 "),Mk=n(RL,"A",{href:!0});var HBr=s(Mk);ZQe=r(HBr,"RobertaTokenizer"),HBr.forEach(t),eHe=r(RL," or "),Ek=n(RL,"A",{href:!0});var UBr=s(Ek);oHe=r(UBr,"RobertaTokenizerFast"),UBr.forEach(t),rHe=r(RL," (RoBERTa model)"),RL.forEach(t),tHe=i(y),Es=n(y,"LI",{});var SL=s(Es);ZH=n(SL,"STRONG",{});var JBr=s(ZH);aHe=r(JBr,"roformer"),JBr.forEach(t),nHe=r(SL," \u2014 "),yk=n(SL,"A",{href:!0});var YBr=s(yk);sHe=r(YBr,"RoFormerTokenizer"),YBr.forEach(t),lHe=r(SL," or "),wk=n(SL,"A",{href:!0});var KBr=s(wk);iHe=r(KBr,"RoFormerTokenizerFast"),KBr.forEach(t),dHe=r(SL," (RoFormer model)"),SL.forEach(t),cHe=i(y),Ng=n(y,"LI",{});var Uve=s(Ng);eU=n(Uve,"STRONG",{});var ZBr=s(eU);fHe=r(ZBr,"speech_to_text"),ZBr.forEach(t),mHe=r(Uve," \u2014 "),Ak=n(Uve,"A",{href:!0});var ekr=s(Ak);gHe=r(ekr,"Speech2TextTokenizer"),ekr.forEach(t),hHe=r(Uve," (Speech2Text model)"),Uve.forEach(t),pHe=i(y),Dg=n(y,"LI",{});var Jve=s(Dg);oU=n(Jve,"STRONG",{});var okr=s(oU);_He=r(okr,"speech_to_text_2"),okr.forEach(t),uHe=r(Jve," \u2014 "),Lk=n(Jve,"A",{href:!0});var rkr=s(Lk);bHe=r(rkr,"Speech2Text2Tokenizer"),rkr.forEach(t),vHe=r(Jve," (Speech2Text2 model)"),Jve.forEach(t),THe=i(y),ys=n(y,"LI",{});var PL=s(ys);rU=n(PL,"STRONG",{});var tkr=s(rU);FHe=r(tkr,"splinter"),tkr.forEach(t),CHe=r(PL," \u2014 "),Bk=n(PL,"A",{href:!0});var akr=s(Bk);MHe=r(akr,"SplinterTokenizer"),akr.forEach(t),EHe=r(PL," or "),kk=n(PL,"A",{href:!0});var nkr=s(kk);yHe=r(nkr,"SplinterTokenizerFast"),nkr.forEach(t),wHe=r(PL," (Splinter model)"),PL.forEach(t),AHe=i(y),ws=n(y,"LI",{});var $L=s(ws);tU=n($L,"STRONG",{});var skr=s(tU);LHe=r(skr,"squeezebert"),skr.forEach(t),BHe=r($L," \u2014 "),xk=n($L,"A",{href:!0});var lkr=s(xk);kHe=r(lkr,"SqueezeBertTokenizer"),lkr.forEach(t),xHe=r($L," or "),Rk=n($L,"A",{href:!0});var ikr=s(Rk);RHe=r(ikr,"SqueezeBertTokenizerFast"),ikr.forEach(t),SHe=r($L," (SqueezeBERT model)"),$L.forEach(t),PHe=i(y),As=n(y,"LI",{});var IL=s(As);aU=n(IL,"STRONG",{});var dkr=s(aU);$He=r(dkr,"t5"),dkr.forEach(t),IHe=r(IL," \u2014 "),Sk=n(IL,"A",{href:!0});var ckr=s(Sk);jHe=r(ckr,"T5Tokenizer"),ckr.forEach(t),NHe=r(IL," or "),Pk=n(IL,"A",{href:!0});var fkr=s(Pk);DHe=r(fkr,"T5TokenizerFast"),fkr.forEach(t),qHe=r(IL," (T5 model)"),IL.forEach(t),GHe=i(y),qg=n(y,"LI",{});var Yve=s(qg);nU=n(Yve,"STRONG",{});var mkr=s(nU);OHe=r(mkr,"tapas"),mkr.forEach(t),XHe=r(Yve," \u2014 "),$k=n(Yve,"A",{href:!0});var gkr=s($k);zHe=r(gkr,"TapasTokenizer"),gkr.forEach(t),VHe=r(Yve," (TAPAS model)"),Yve.forEach(t),WHe=i(y),Gg=n(y,"LI",{});var Kve=s(Gg);sU=n(Kve,"STRONG",{});var hkr=s(sU);QHe=r(hkr,"transfo-xl"),hkr.forEach(t),HHe=r(Kve," \u2014 "),Ik=n(Kve,"A",{href:!0});var pkr=s(Ik);UHe=r(pkr,"TransfoXLTokenizer"),pkr.forEach(t),JHe=r(Kve," (Transformer-XL model)"),Kve.forEach(t),YHe=i(y),Og=n(y,"LI",{});var Zve=s(Og);lU=n(Zve,"STRONG",{});var _kr=s(lU);KHe=r(_kr,"wav2vec2"),_kr.forEach(t),ZHe=r(Zve," \u2014 "),jk=n(Zve,"A",{href:!0});var ukr=s(jk);eUe=r(ukr,"Wav2Vec2CTCTokenizer"),ukr.forEach(t),oUe=r(Zve," (Wav2Vec2 model)"),Zve.forEach(t),rUe=i(y),Xg=n(y,"LI",{});var e0e=s(Xg);iU=n(e0e,"STRONG",{});var bkr=s(iU);tUe=r(bkr,"wav2vec2_phoneme"),bkr.forEach(t),aUe=r(e0e," \u2014 "),Nk=n(e0e,"A",{href:!0});var vkr=s(Nk);nUe=r(vkr,"Wav2Vec2PhonemeCTCTokenizer"),vkr.forEach(t),sUe=r(e0e," (Wav2Vec2Phoneme model)"),e0e.forEach(t),lUe=i(y),Ls=n(y,"LI",{});var jL=s(Ls);dU=n(jL,"STRONG",{});var Tkr=s(dU);iUe=r(Tkr,"xglm"),Tkr.forEach(t),dUe=r(jL," \u2014 "),Dk=n(jL,"A",{href:!0});var Fkr=s(Dk);cUe=r(Fkr,"XGLMTokenizer"),Fkr.forEach(t),fUe=r(jL," or "),qk=n(jL,"A",{href:!0});var Ckr=s(qk);mUe=r(Ckr,"XGLMTokenizerFast"),Ckr.forEach(t),gUe=r(jL," (XGLM model)"),jL.forEach(t),hUe=i(y),zg=n(y,"LI",{});var o0e=s(zg);cU=n(o0e,"STRONG",{});var Mkr=s(cU);pUe=r(Mkr,"xlm"),Mkr.forEach(t),_Ue=r(o0e," \u2014 "),Gk=n(o0e,"A",{href:!0});var Ekr=s(Gk);uUe=r(Ekr,"XLMTokenizer"),Ekr.forEach(t),bUe=r(o0e," (XLM model)"),o0e.forEach(t),vUe=i(y),Vg=n(y,"LI",{});var r0e=s(Vg);fU=n(r0e,"STRONG",{});var ykr=s(fU);TUe=r(ykr,"xlm-prophetnet"),ykr.forEach(t),FUe=r(r0e," \u2014 "),Ok=n(r0e,"A",{href:!0});var wkr=s(Ok);CUe=r(wkr,"XLMProphetNetTokenizer"),wkr.forEach(t),MUe=r(r0e," (XLMProphetNet model)"),r0e.forEach(t),EUe=i(y),Bs=n(y,"LI",{});var NL=s(Bs);mU=n(NL,"STRONG",{});var Akr=s(mU);yUe=r(Akr,"xlm-roberta"),Akr.forEach(t),wUe=r(NL," \u2014 "),Xk=n(NL,"A",{href:!0});var Lkr=s(Xk);AUe=r(Lkr,"XLMRobertaTokenizer"),Lkr.forEach(t),LUe=r(NL," or "),zk=n(NL,"A",{href:!0});var Bkr=s(zk);BUe=r(Bkr,"XLMRobertaTokenizerFast"),Bkr.forEach(t),kUe=r(NL," (XLM-RoBERTa model)"),NL.forEach(t),xUe=i(y),ks=n(y,"LI",{});var DL=s(ks);gU=n(DL,"STRONG",{});var kkr=s(gU);RUe=r(kkr,"xlnet"),kkr.forEach(t),SUe=r(DL," \u2014 "),Vk=n(DL,"A",{href:!0});var xkr=s(Vk);PUe=r(xkr,"XLNetTokenizer"),xkr.forEach(t),$Ue=r(DL," or "),Wk=n(DL,"A",{href:!0});var Rkr=s(Wk);IUe=r(Rkr,"XLNetTokenizerFast"),Rkr.forEach(t),jUe=r(DL," (XLNet model)"),DL.forEach(t),y.forEach(t),NUe=i(da),hU=n(da,"P",{});var Skr=s(hU);DUe=r(Skr,"Examples:"),Skr.forEach(t),qUe=i(da),m(MM.$$.fragment,da),da.forEach(t),GUe=i($s),Wg=n($s,"DIV",{class:!0});var kBe=s(Wg);m(EM.$$.fragment,kBe),OUe=i(kBe),pU=n(kBe,"P",{});var Pkr=s(pU);XUe=r(Pkr,"Register a new tokenizer in this mapping."),Pkr.forEach(t),kBe.forEach(t),$s.forEach(t),L8e=i(d),ji=n(d,"H2",{class:!0});var xBe=s(ji);Qg=n(xBe,"A",{id:!0,class:!0,href:!0});var $kr=s(Qg);_U=n($kr,"SPAN",{});var Ikr=s(_U);m(yM.$$.fragment,Ikr),Ikr.forEach(t),$kr.forEach(t),zUe=i(xBe),uU=n(xBe,"SPAN",{});var jkr=s(uU);VUe=r(jkr,"AutoFeatureExtractor"),jkr.forEach(t),xBe.forEach(t),B8e=i(d),Xo=n(d,"DIV",{class:!0});var Is=s(Xo);m(wM.$$.fragment,Is),WUe=i(Is),AM=n(Is,"P",{});var RBe=s(AM);QUe=r(RBe,`This is a generic feature extractor class that will be instantiated as one of the feature extractor classes of the
library when created with the `),Qk=n(RBe,"A",{href:!0});var Nkr=s(Qk);HUe=r(Nkr,"AutoFeatureExtractor.from_pretrained()"),Nkr.forEach(t),UUe=r(RBe," class method."),RBe.forEach(t),JUe=i(Is),LM=n(Is,"P",{});var SBe=s(LM);YUe=r(SBe,"This class cannot be instantiated directly using "),bU=n(SBe,"CODE",{});var Dkr=s(bU);KUe=r(Dkr,"__init__()"),Dkr.forEach(t),ZUe=r(SBe," (throws an error)."),SBe.forEach(t),eJe=i(Is),Be=n(Is,"DIV",{class:!0});var xt=s(Be);m(BM.$$.fragment,xt),oJe=i(xt),vU=n(xt,"P",{});var qkr=s(vU);rJe=r(qkr,"Instantiate one of the feature extractor classes of the library from a pretrained model vocabulary."),qkr.forEach(t),tJe=i(xt),Na=n(xt,"P",{});var c4=s(Na);aJe=r(c4,"The feature extractor class to instantiate is selected based on the "),TU=n(c4,"CODE",{});var Gkr=s(TU);nJe=r(Gkr,"model_type"),Gkr.forEach(t),sJe=r(c4,` property of the config object
(either passed as an argument or loaded from `),FU=n(c4,"CODE",{});var Okr=s(FU);lJe=r(Okr,"pretrained_model_name_or_path"),Okr.forEach(t),iJe=r(c4,` if possible), or when it\u2019s
missing, by falling back to using pattern matching on `),CU=n(c4,"CODE",{});var Xkr=s(CU);dJe=r(Xkr,"pretrained_model_name_or_path"),Xkr.forEach(t),cJe=r(c4,":"),c4.forEach(t),fJe=i(xt),ae=n(xt,"UL",{});var le=s(ae);Hg=n(le,"LI",{});var t0e=s(Hg);MU=n(t0e,"STRONG",{});var zkr=s(MU);mJe=r(zkr,"beit"),zkr.forEach(t),gJe=r(t0e," \u2014 "),Hk=n(t0e,"A",{href:!0});var Vkr=s(Hk);hJe=r(Vkr,"BeitFeatureExtractor"),Vkr.forEach(t),pJe=r(t0e," (BEiT model)"),t0e.forEach(t),_Je=i(le),Ug=n(le,"LI",{});var a0e=s(Ug);EU=n(a0e,"STRONG",{});var Wkr=s(EU);uJe=r(Wkr,"clip"),Wkr.forEach(t),bJe=r(a0e," \u2014 "),Uk=n(a0e,"A",{href:!0});var Qkr=s(Uk);vJe=r(Qkr,"CLIPFeatureExtractor"),Qkr.forEach(t),TJe=r(a0e," (CLIP model)"),a0e.forEach(t),FJe=i(le),Jg=n(le,"LI",{});var n0e=s(Jg);yU=n(n0e,"STRONG",{});var Hkr=s(yU);CJe=r(Hkr,"convnext"),Hkr.forEach(t),MJe=r(n0e," \u2014 "),Jk=n(n0e,"A",{href:!0});var Ukr=s(Jk);EJe=r(Ukr,"ConvNextFeatureExtractor"),Ukr.forEach(t),yJe=r(n0e," (ConvNext model)"),n0e.forEach(t),wJe=i(le),Yg=n(le,"LI",{});var s0e=s(Yg);wU=n(s0e,"STRONG",{});var Jkr=s(wU);AJe=r(Jkr,"deit"),Jkr.forEach(t),LJe=r(s0e," \u2014 "),Yk=n(s0e,"A",{href:!0});var Ykr=s(Yk);BJe=r(Ykr,"DeiTFeatureExtractor"),Ykr.forEach(t),kJe=r(s0e," (DeiT model)"),s0e.forEach(t),xJe=i(le),Kg=n(le,"LI",{});var l0e=s(Kg);AU=n(l0e,"STRONG",{});var Kkr=s(AU);RJe=r(Kkr,"detr"),Kkr.forEach(t),SJe=r(l0e," \u2014 "),Kk=n(l0e,"A",{href:!0});var Zkr=s(Kk);PJe=r(Zkr,"DetrFeatureExtractor"),Zkr.forEach(t),$Je=r(l0e," (DETR model)"),l0e.forEach(t),IJe=i(le),Zg=n(le,"LI",{});var i0e=s(Zg);LU=n(i0e,"STRONG",{});var exr=s(LU);jJe=r(exr,"hubert"),exr.forEach(t),NJe=r(i0e," \u2014 "),Zk=n(i0e,"A",{href:!0});var oxr=s(Zk);DJe=r(oxr,"Wav2Vec2FeatureExtractor"),oxr.forEach(t),qJe=r(i0e," (Hubert model)"),i0e.forEach(t),GJe=i(le),eh=n(le,"LI",{});var d0e=s(eh);BU=n(d0e,"STRONG",{});var rxr=s(BU);OJe=r(rxr,"layoutlmv2"),rxr.forEach(t),XJe=r(d0e," \u2014 "),ex=n(d0e,"A",{href:!0});var txr=s(ex);zJe=r(txr,"LayoutLMv2FeatureExtractor"),txr.forEach(t),VJe=r(d0e," (LayoutLMv2 model)"),d0e.forEach(t),WJe=i(le),oh=n(le,"LI",{});var c0e=s(oh);kU=n(c0e,"STRONG",{});var axr=s(kU);QJe=r(axr,"perceiver"),axr.forEach(t),HJe=r(c0e," \u2014 "),ox=n(c0e,"A",{href:!0});var nxr=s(ox);UJe=r(nxr,"PerceiverFeatureExtractor"),nxr.forEach(t),JJe=r(c0e," (Perceiver model)"),c0e.forEach(t),YJe=i(le),rh=n(le,"LI",{});var f0e=s(rh);xU=n(f0e,"STRONG",{});var sxr=s(xU);KJe=r(sxr,"poolformer"),sxr.forEach(t),ZJe=r(f0e," \u2014 "),rx=n(f0e,"A",{href:!0});var lxr=s(rx);eYe=r(lxr,"PoolFormerFeatureExtractor"),lxr.forEach(t),oYe=r(f0e," (PoolFormer model)"),f0e.forEach(t),rYe=i(le),th=n(le,"LI",{});var m0e=s(th);RU=n(m0e,"STRONG",{});var ixr=s(RU);tYe=r(ixr,"resnet"),ixr.forEach(t),aYe=r(m0e," \u2014 "),tx=n(m0e,"A",{href:!0});var dxr=s(tx);nYe=r(dxr,"ConvNextFeatureExtractor"),dxr.forEach(t),sYe=r(m0e," (resnet model)"),m0e.forEach(t),lYe=i(le),ah=n(le,"LI",{});var g0e=s(ah);SU=n(g0e,"STRONG",{});var cxr=s(SU);iYe=r(cxr,"segformer"),cxr.forEach(t),dYe=r(g0e," \u2014 "),ax=n(g0e,"A",{href:!0});var fxr=s(ax);cYe=r(fxr,"SegformerFeatureExtractor"),fxr.forEach(t),fYe=r(g0e," (SegFormer model)"),g0e.forEach(t),mYe=i(le),nh=n(le,"LI",{});var h0e=s(nh);PU=n(h0e,"STRONG",{});var mxr=s(PU);gYe=r(mxr,"speech_to_text"),mxr.forEach(t),hYe=r(h0e," \u2014 "),nx=n(h0e,"A",{href:!0});var gxr=s(nx);pYe=r(gxr,"Speech2TextFeatureExtractor"),gxr.forEach(t),_Ye=r(h0e," (Speech2Text model)"),h0e.forEach(t),uYe=i(le),sh=n(le,"LI",{});var p0e=s(sh);$U=n(p0e,"STRONG",{});var hxr=s($U);bYe=r(hxr,"swin"),hxr.forEach(t),vYe=r(p0e," \u2014 "),sx=n(p0e,"A",{href:!0});var pxr=s(sx);TYe=r(pxr,"ViTFeatureExtractor"),pxr.forEach(t),FYe=r(p0e," (Swin model)"),p0e.forEach(t),CYe=i(le),lh=n(le,"LI",{});var _0e=s(lh);IU=n(_0e,"STRONG",{});var _xr=s(IU);MYe=r(_xr,"vit"),_xr.forEach(t),EYe=r(_0e," \u2014 "),lx=n(_0e,"A",{href:!0});var uxr=s(lx);yYe=r(uxr,"ViTFeatureExtractor"),uxr.forEach(t),wYe=r(_0e," (ViT model)"),_0e.forEach(t),AYe=i(le),ih=n(le,"LI",{});var u0e=s(ih);jU=n(u0e,"STRONG",{});var bxr=s(jU);LYe=r(bxr,"vit_mae"),bxr.forEach(t),BYe=r(u0e," \u2014 "),ix=n(u0e,"A",{href:!0});var vxr=s(ix);kYe=r(vxr,"ViTFeatureExtractor"),vxr.forEach(t),xYe=r(u0e," (ViTMAE model)"),u0e.forEach(t),RYe=i(le),dh=n(le,"LI",{});var b0e=s(dh);NU=n(b0e,"STRONG",{});var Txr=s(NU);SYe=r(Txr,"wav2vec2"),Txr.forEach(t),PYe=r(b0e," \u2014 "),dx=n(b0e,"A",{href:!0});var Fxr=s(dx);$Ye=r(Fxr,"Wav2Vec2FeatureExtractor"),Fxr.forEach(t),IYe=r(b0e," (Wav2Vec2 model)"),b0e.forEach(t),le.forEach(t),jYe=i(xt),m(ch.$$.fragment,xt),NYe=i(xt),DU=n(xt,"P",{});var Cxr=s(DU);DYe=r(Cxr,"Examples:"),Cxr.forEach(t),qYe=i(xt),m(kM.$$.fragment,xt),xt.forEach(t),GYe=i(Is),fh=n(Is,"DIV",{class:!0});var PBe=s(fh);m(xM.$$.fragment,PBe),OYe=i(PBe),qU=n(PBe,"P",{});var Mxr=s(qU);XYe=r(Mxr,"Register a new feature extractor for this class."),Mxr.forEach(t),PBe.forEach(t),Is.forEach(t),k8e=i(d),Ni=n(d,"H2",{class:!0});var $Be=s(Ni);mh=n($Be,"A",{id:!0,class:!0,href:!0});var Exr=s(mh);GU=n(Exr,"SPAN",{});var yxr=s(GU);m(RM.$$.fragment,yxr),yxr.forEach(t),Exr.forEach(t),zYe=i($Be),OU=n($Be,"SPAN",{});var wxr=s(OU);VYe=r(wxr,"AutoProcessor"),wxr.forEach(t),$Be.forEach(t),x8e=i(d),zo=n(d,"DIV",{class:!0});var js=s(zo);m(SM.$$.fragment,js),WYe=i(js),PM=n(js,"P",{});var IBe=s(PM);QYe=r(IBe,`This is a generic processor class that will be instantiated as one of the processor classes of the library when
created with the `),cx=n(IBe,"A",{href:!0});var Axr=s(cx);HYe=r(Axr,"AutoProcessor.from_pretrained()"),Axr.forEach(t),UYe=r(IBe," class method."),IBe.forEach(t),JYe=i(js),$M=n(js,"P",{});var jBe=s($M);YYe=r(jBe,"This class cannot be instantiated directly using "),XU=n(jBe,"CODE",{});var Lxr=s(XU);KYe=r(Lxr,"__init__()"),Lxr.forEach(t),ZYe=r(jBe," (throws an error)."),jBe.forEach(t),eKe=i(js),ke=n(js,"DIV",{class:!0});var Rt=s(ke);m(IM.$$.fragment,Rt),oKe=i(Rt),zU=n(Rt,"P",{});var Bxr=s(zU);rKe=r(Bxr,"Instantiate one of the processor classes of the library from a pretrained model vocabulary."),Bxr.forEach(t),tKe=i(Rt),Di=n(Rt,"P",{});var ZX=s(Di);aKe=r(ZX,"The processor class to instantiate is selected based on the "),VU=n(ZX,"CODE",{});var kxr=s(VU);nKe=r(kxr,"model_type"),kxr.forEach(t),sKe=r(ZX,` property of the config object (either
passed as an argument or loaded from `),WU=n(ZX,"CODE",{});var xxr=s(WU);lKe=r(xxr,"pretrained_model_name_or_path"),xxr.forEach(t),iKe=r(ZX," if possible):"),ZX.forEach(t),dKe=i(Rt),Ae=n(Rt,"UL",{});var No=s(Ae);gh=n(No,"LI",{});var v0e=s(gh);QU=n(v0e,"STRONG",{});var Rxr=s(QU);cKe=r(Rxr,"clip"),Rxr.forEach(t),fKe=r(v0e," \u2014 "),fx=n(v0e,"A",{href:!0});var Sxr=s(fx);mKe=r(Sxr,"CLIPProcessor"),Sxr.forEach(t),gKe=r(v0e," (CLIP model)"),v0e.forEach(t),hKe=i(No),hh=n(No,"LI",{});var T0e=s(hh);HU=n(T0e,"STRONG",{});var Pxr=s(HU);pKe=r(Pxr,"layoutlmv2"),Pxr.forEach(t),_Ke=r(T0e," \u2014 "),mx=n(T0e,"A",{href:!0});var $xr=s(mx);uKe=r($xr,"LayoutLMv2Processor"),$xr.forEach(t),bKe=r(T0e," (LayoutLMv2 model)"),T0e.forEach(t),vKe=i(No),ph=n(No,"LI",{});var F0e=s(ph);UU=n(F0e,"STRONG",{});var Ixr=s(UU);TKe=r(Ixr,"layoutxlm"),Ixr.forEach(t),FKe=r(F0e," \u2014 "),gx=n(F0e,"A",{href:!0});var jxr=s(gx);CKe=r(jxr,"LayoutXLMProcessor"),jxr.forEach(t),MKe=r(F0e," (LayoutXLM model)"),F0e.forEach(t),EKe=i(No),_h=n(No,"LI",{});var C0e=s(_h);JU=n(C0e,"STRONG",{});var Nxr=s(JU);yKe=r(Nxr,"speech_to_text"),Nxr.forEach(t),wKe=r(C0e," \u2014 "),hx=n(C0e,"A",{href:!0});var Dxr=s(hx);AKe=r(Dxr,"Speech2TextProcessor"),Dxr.forEach(t),LKe=r(C0e," (Speech2Text model)"),C0e.forEach(t),BKe=i(No),uh=n(No,"LI",{});var M0e=s(uh);YU=n(M0e,"STRONG",{});var qxr=s(YU);kKe=r(qxr,"speech_to_text_2"),qxr.forEach(t),xKe=r(M0e," \u2014 "),px=n(M0e,"A",{href:!0});var Gxr=s(px);RKe=r(Gxr,"Speech2Text2Processor"),Gxr.forEach(t),SKe=r(M0e," (Speech2Text2 model)"),M0e.forEach(t),PKe=i(No),bh=n(No,"LI",{});var E0e=s(bh);KU=n(E0e,"STRONG",{});var Oxr=s(KU);$Ke=r(Oxr,"trocr"),Oxr.forEach(t),IKe=r(E0e," \u2014 "),_x=n(E0e,"A",{href:!0});var Xxr=s(_x);jKe=r(Xxr,"TrOCRProcessor"),Xxr.forEach(t),NKe=r(E0e," (TrOCR model)"),E0e.forEach(t),DKe=i(No),vh=n(No,"LI",{});var y0e=s(vh);ZU=n(y0e,"STRONG",{});var zxr=s(ZU);qKe=r(zxr,"vision-text-dual-encoder"),zxr.forEach(t),GKe=r(y0e," \u2014 "),ux=n(y0e,"A",{href:!0});var Vxr=s(ux);OKe=r(Vxr,"VisionTextDualEncoderProcessor"),Vxr.forEach(t),XKe=r(y0e," (VisionTextDualEncoder model)"),y0e.forEach(t),zKe=i(No),Th=n(No,"LI",{});var w0e=s(Th);eJ=n(w0e,"STRONG",{});var Wxr=s(eJ);VKe=r(Wxr,"wav2vec2"),Wxr.forEach(t),WKe=r(w0e," \u2014 "),bx=n(w0e,"A",{href:!0});var Qxr=s(bx);QKe=r(Qxr,"Wav2Vec2Processor"),Qxr.forEach(t),HKe=r(w0e," (Wav2Vec2 model)"),w0e.forEach(t),No.forEach(t),UKe=i(Rt),m(Fh.$$.fragment,Rt),JKe=i(Rt),oJ=n(Rt,"P",{});var Hxr=s(oJ);YKe=r(Hxr,"Examples:"),Hxr.forEach(t),KKe=i(Rt),m(jM.$$.fragment,Rt),Rt.forEach(t),ZKe=i(js),Ch=n(js,"DIV",{class:!0});var NBe=s(Ch);m(NM.$$.fragment,NBe),eZe=i(NBe),rJ=n(NBe,"P",{});var Uxr=s(rJ);oZe=r(Uxr,"Register a new processor for this class."),Uxr.forEach(t),NBe.forEach(t),js.forEach(t),R8e=i(d),qi=n(d,"H2",{class:!0});var DBe=s(qi);Mh=n(DBe,"A",{id:!0,class:!0,href:!0});var Jxr=s(Mh);tJ=n(Jxr,"SPAN",{});var Yxr=s(tJ);m(DM.$$.fragment,Yxr),Yxr.forEach(t),Jxr.forEach(t),rZe=i(DBe),aJ=n(DBe,"SPAN",{});var Kxr=s(aJ);tZe=r(Kxr,"AutoModel"),Kxr.forEach(t),DBe.forEach(t),S8e=i(d),Vo=n(d,"DIV",{class:!0});var Ns=s(Vo);m(qM.$$.fragment,Ns),aZe=i(Ns),Gi=n(Ns,"P",{});var ez=s(Gi);nZe=r(ez,`This is a generic model class that will be instantiated as one of the base model classes of the library when created
with the `),nJ=n(ez,"CODE",{});var Zxr=s(nJ);sZe=r(Zxr,"from_pretrained()"),Zxr.forEach(t),lZe=r(ez,"class method or the "),sJ=n(ez,"CODE",{});var eRr=s(sJ);iZe=r(eRr,"from_config()"),eRr.forEach(t),dZe=r(ez,`class
method.`),ez.forEach(t),cZe=i(Ns),GM=n(Ns,"P",{});var qBe=s(GM);fZe=r(qBe,"This class cannot be instantiated directly using "),lJ=n(qBe,"CODE",{});var oRr=s(lJ);mZe=r(oRr,"__init__()"),oRr.forEach(t),gZe=r(qBe," (throws an error)."),qBe.forEach(t),hZe=i(Ns),Nr=n(Ns,"DIV",{class:!0});var Ds=s(Nr);m(OM.$$.fragment,Ds),pZe=i(Ds),iJ=n(Ds,"P",{});var rRr=s(iJ);_Ze=r(rRr,"Instantiates one of the base model classes of the library from a configuration."),rRr.forEach(t),uZe=i(Ds),Oi=n(Ds,"P",{});var oz=s(Oi);bZe=r(oz,`Note:
Loading a model from its configuration file does `),dJ=n(oz,"STRONG",{});var tRr=s(dJ);vZe=r(tRr,"not"),tRr.forEach(t),TZe=r(oz,` load the model weights. It only affects the
model\u2019s configuration. Use `),cJ=n(oz,"CODE",{});var aRr=s(cJ);FZe=r(aRr,"from_pretrained()"),aRr.forEach(t),CZe=r(oz,"to load the model weights."),oz.forEach(t),MZe=i(Ds),fJ=n(Ds,"P",{});var nRr=s(fJ);EZe=r(nRr,"Examples:"),nRr.forEach(t),yZe=i(Ds),m(XM.$$.fragment,Ds),Ds.forEach(t),wZe=i(Ns),xe=n(Ns,"DIV",{class:!0});var St=s(xe);m(zM.$$.fragment,St),AZe=i(St),mJ=n(St,"P",{});var sRr=s(mJ);LZe=r(sRr,"Instantiate one of the base model classes of the library from a pretrained model."),sRr.forEach(t),BZe=i(St),Da=n(St,"P",{});var f4=s(Da);kZe=r(f4,"The model class to instantiate is selected based on the "),gJ=n(f4,"CODE",{});var lRr=s(gJ);xZe=r(lRr,"model_type"),lRr.forEach(t),RZe=r(f4,` property of the config object (either
passed as an argument or loaded from `),hJ=n(f4,"CODE",{});var iRr=s(hJ);SZe=r(iRr,"pretrained_model_name_or_path"),iRr.forEach(t),PZe=r(f4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),pJ=n(f4,"CODE",{});var dRr=s(pJ);$Ze=r(dRr,"pretrained_model_name_or_path"),dRr.forEach(t),IZe=r(f4,":"),f4.forEach(t),jZe=i(St),F=n(St,"UL",{});var C=s(F);Eh=n(C,"LI",{});var A0e=s(Eh);_J=n(A0e,"STRONG",{});var cRr=s(_J);NZe=r(cRr,"albert"),cRr.forEach(t),DZe=r(A0e," \u2014 "),vx=n(A0e,"A",{href:!0});var fRr=s(vx);qZe=r(fRr,"AlbertModel"),fRr.forEach(t),GZe=r(A0e," (ALBERT model)"),A0e.forEach(t),OZe=i(C),yh=n(C,"LI",{});var L0e=s(yh);uJ=n(L0e,"STRONG",{});var mRr=s(uJ);XZe=r(mRr,"bart"),mRr.forEach(t),zZe=r(L0e," \u2014 "),Tx=n(L0e,"A",{href:!0});var gRr=s(Tx);VZe=r(gRr,"BartModel"),gRr.forEach(t),WZe=r(L0e," (BART model)"),L0e.forEach(t),QZe=i(C),wh=n(C,"LI",{});var B0e=s(wh);bJ=n(B0e,"STRONG",{});var hRr=s(bJ);HZe=r(hRr,"beit"),hRr.forEach(t),UZe=r(B0e," \u2014 "),Fx=n(B0e,"A",{href:!0});var pRr=s(Fx);JZe=r(pRr,"BeitModel"),pRr.forEach(t),YZe=r(B0e," (BEiT model)"),B0e.forEach(t),KZe=i(C),Ah=n(C,"LI",{});var k0e=s(Ah);vJ=n(k0e,"STRONG",{});var _Rr=s(vJ);ZZe=r(_Rr,"bert"),_Rr.forEach(t),eeo=r(k0e," \u2014 "),Cx=n(k0e,"A",{href:!0});var uRr=s(Cx);oeo=r(uRr,"BertModel"),uRr.forEach(t),reo=r(k0e," (BERT model)"),k0e.forEach(t),teo=i(C),Lh=n(C,"LI",{});var x0e=s(Lh);TJ=n(x0e,"STRONG",{});var bRr=s(TJ);aeo=r(bRr,"bert-generation"),bRr.forEach(t),neo=r(x0e," \u2014 "),Mx=n(x0e,"A",{href:!0});var vRr=s(Mx);seo=r(vRr,"BertGenerationEncoder"),vRr.forEach(t),leo=r(x0e," (Bert Generation model)"),x0e.forEach(t),ieo=i(C),Bh=n(C,"LI",{});var R0e=s(Bh);FJ=n(R0e,"STRONG",{});var TRr=s(FJ);deo=r(TRr,"big_bird"),TRr.forEach(t),ceo=r(R0e," \u2014 "),Ex=n(R0e,"A",{href:!0});var FRr=s(Ex);feo=r(FRr,"BigBirdModel"),FRr.forEach(t),meo=r(R0e," (BigBird model)"),R0e.forEach(t),geo=i(C),kh=n(C,"LI",{});var S0e=s(kh);CJ=n(S0e,"STRONG",{});var CRr=s(CJ);heo=r(CRr,"bigbird_pegasus"),CRr.forEach(t),peo=r(S0e," \u2014 "),yx=n(S0e,"A",{href:!0});var MRr=s(yx);_eo=r(MRr,"BigBirdPegasusModel"),MRr.forEach(t),ueo=r(S0e," (BigBirdPegasus model)"),S0e.forEach(t),beo=i(C),xh=n(C,"LI",{});var P0e=s(xh);MJ=n(P0e,"STRONG",{});var ERr=s(MJ);veo=r(ERr,"blenderbot"),ERr.forEach(t),Teo=r(P0e," \u2014 "),wx=n(P0e,"A",{href:!0});var yRr=s(wx);Feo=r(yRr,"BlenderbotModel"),yRr.forEach(t),Ceo=r(P0e," (Blenderbot model)"),P0e.forEach(t),Meo=i(C),Rh=n(C,"LI",{});var $0e=s(Rh);EJ=n($0e,"STRONG",{});var wRr=s(EJ);Eeo=r(wRr,"blenderbot-small"),wRr.forEach(t),yeo=r($0e," \u2014 "),Ax=n($0e,"A",{href:!0});var ARr=s(Ax);weo=r(ARr,"BlenderbotSmallModel"),ARr.forEach(t),Aeo=r($0e," (BlenderbotSmall model)"),$0e.forEach(t),Leo=i(C),Sh=n(C,"LI",{});var I0e=s(Sh);yJ=n(I0e,"STRONG",{});var LRr=s(yJ);Beo=r(LRr,"camembert"),LRr.forEach(t),keo=r(I0e," \u2014 "),Lx=n(I0e,"A",{href:!0});var BRr=s(Lx);xeo=r(BRr,"CamembertModel"),BRr.forEach(t),Reo=r(I0e," (CamemBERT model)"),I0e.forEach(t),Seo=i(C),Ph=n(C,"LI",{});var j0e=s(Ph);wJ=n(j0e,"STRONG",{});var kRr=s(wJ);Peo=r(kRr,"canine"),kRr.forEach(t),$eo=r(j0e," \u2014 "),Bx=n(j0e,"A",{href:!0});var xRr=s(Bx);Ieo=r(xRr,"CanineModel"),xRr.forEach(t),jeo=r(j0e," (Canine model)"),j0e.forEach(t),Neo=i(C),$h=n(C,"LI",{});var N0e=s($h);AJ=n(N0e,"STRONG",{});var RRr=s(AJ);Deo=r(RRr,"clip"),RRr.forEach(t),qeo=r(N0e," \u2014 "),kx=n(N0e,"A",{href:!0});var SRr=s(kx);Geo=r(SRr,"CLIPModel"),SRr.forEach(t),Oeo=r(N0e," (CLIP model)"),N0e.forEach(t),Xeo=i(C),Ih=n(C,"LI",{});var D0e=s(Ih);LJ=n(D0e,"STRONG",{});var PRr=s(LJ);zeo=r(PRr,"convbert"),PRr.forEach(t),Veo=r(D0e," \u2014 "),xx=n(D0e,"A",{href:!0});var $Rr=s(xx);Weo=r($Rr,"ConvBertModel"),$Rr.forEach(t),Qeo=r(D0e," (ConvBERT model)"),D0e.forEach(t),Heo=i(C),jh=n(C,"LI",{});var q0e=s(jh);BJ=n(q0e,"STRONG",{});var IRr=s(BJ);Ueo=r(IRr,"convnext"),IRr.forEach(t),Jeo=r(q0e," \u2014 "),Rx=n(q0e,"A",{href:!0});var jRr=s(Rx);Yeo=r(jRr,"ConvNextModel"),jRr.forEach(t),Keo=r(q0e," (ConvNext model)"),q0e.forEach(t),Zeo=i(C),Nh=n(C,"LI",{});var G0e=s(Nh);kJ=n(G0e,"STRONG",{});var NRr=s(kJ);eoo=r(NRr,"ctrl"),NRr.forEach(t),ooo=r(G0e," \u2014 "),Sx=n(G0e,"A",{href:!0});var DRr=s(Sx);roo=r(DRr,"CTRLModel"),DRr.forEach(t),too=r(G0e," (CTRL model)"),G0e.forEach(t),aoo=i(C),Dh=n(C,"LI",{});var O0e=s(Dh);xJ=n(O0e,"STRONG",{});var qRr=s(xJ);noo=r(qRr,"deberta"),qRr.forEach(t),soo=r(O0e," \u2014 "),Px=n(O0e,"A",{href:!0});var GRr=s(Px);loo=r(GRr,"DebertaModel"),GRr.forEach(t),ioo=r(O0e," (DeBERTa model)"),O0e.forEach(t),doo=i(C),qh=n(C,"LI",{});var X0e=s(qh);RJ=n(X0e,"STRONG",{});var ORr=s(RJ);coo=r(ORr,"deberta-v2"),ORr.forEach(t),foo=r(X0e," \u2014 "),$x=n(X0e,"A",{href:!0});var XRr=s($x);moo=r(XRr,"DebertaV2Model"),XRr.forEach(t),goo=r(X0e," (DeBERTa-v2 model)"),X0e.forEach(t),hoo=i(C),Gh=n(C,"LI",{});var z0e=s(Gh);SJ=n(z0e,"STRONG",{});var zRr=s(SJ);poo=r(zRr,"deit"),zRr.forEach(t),_oo=r(z0e," \u2014 "),Ix=n(z0e,"A",{href:!0});var VRr=s(Ix);uoo=r(VRr,"DeiTModel"),VRr.forEach(t),boo=r(z0e," (DeiT model)"),z0e.forEach(t),voo=i(C),Oh=n(C,"LI",{});var V0e=s(Oh);PJ=n(V0e,"STRONG",{});var WRr=s(PJ);Too=r(WRr,"detr"),WRr.forEach(t),Foo=r(V0e," \u2014 "),jx=n(V0e,"A",{href:!0});var QRr=s(jx);Coo=r(QRr,"DetrModel"),QRr.forEach(t),Moo=r(V0e," (DETR model)"),V0e.forEach(t),Eoo=i(C),Xh=n(C,"LI",{});var W0e=s(Xh);$J=n(W0e,"STRONG",{});var HRr=s($J);yoo=r(HRr,"distilbert"),HRr.forEach(t),woo=r(W0e," \u2014 "),Nx=n(W0e,"A",{href:!0});var URr=s(Nx);Aoo=r(URr,"DistilBertModel"),URr.forEach(t),Loo=r(W0e," (DistilBERT model)"),W0e.forEach(t),Boo=i(C),zh=n(C,"LI",{});var Q0e=s(zh);IJ=n(Q0e,"STRONG",{});var JRr=s(IJ);koo=r(JRr,"dpr"),JRr.forEach(t),xoo=r(Q0e," \u2014 "),Dx=n(Q0e,"A",{href:!0});var YRr=s(Dx);Roo=r(YRr,"DPRQuestionEncoder"),YRr.forEach(t),Soo=r(Q0e," (DPR model)"),Q0e.forEach(t),Poo=i(C),Vh=n(C,"LI",{});var H0e=s(Vh);jJ=n(H0e,"STRONG",{});var KRr=s(jJ);$oo=r(KRr,"electra"),KRr.forEach(t),Ioo=r(H0e," \u2014 "),qx=n(H0e,"A",{href:!0});var ZRr=s(qx);joo=r(ZRr,"ElectraModel"),ZRr.forEach(t),Noo=r(H0e," (ELECTRA model)"),H0e.forEach(t),Doo=i(C),Wh=n(C,"LI",{});var U0e=s(Wh);NJ=n(U0e,"STRONG",{});var eSr=s(NJ);qoo=r(eSr,"flaubert"),eSr.forEach(t),Goo=r(U0e," \u2014 "),Gx=n(U0e,"A",{href:!0});var oSr=s(Gx);Ooo=r(oSr,"FlaubertModel"),oSr.forEach(t),Xoo=r(U0e," (FlauBERT model)"),U0e.forEach(t),zoo=i(C),Qh=n(C,"LI",{});var J0e=s(Qh);DJ=n(J0e,"STRONG",{});var rSr=s(DJ);Voo=r(rSr,"fnet"),rSr.forEach(t),Woo=r(J0e," \u2014 "),Ox=n(J0e,"A",{href:!0});var tSr=s(Ox);Qoo=r(tSr,"FNetModel"),tSr.forEach(t),Hoo=r(J0e," (FNet model)"),J0e.forEach(t),Uoo=i(C),Hh=n(C,"LI",{});var Y0e=s(Hh);qJ=n(Y0e,"STRONG",{});var aSr=s(qJ);Joo=r(aSr,"fsmt"),aSr.forEach(t),Yoo=r(Y0e," \u2014 "),Xx=n(Y0e,"A",{href:!0});var nSr=s(Xx);Koo=r(nSr,"FSMTModel"),nSr.forEach(t),Zoo=r(Y0e," (FairSeq Machine-Translation model)"),Y0e.forEach(t),ero=i(C),xs=n(C,"LI",{});var qL=s(xs);GJ=n(qL,"STRONG",{});var sSr=s(GJ);oro=r(sSr,"funnel"),sSr.forEach(t),rro=r(qL," \u2014 "),zx=n(qL,"A",{href:!0});var lSr=s(zx);tro=r(lSr,"FunnelModel"),lSr.forEach(t),aro=r(qL," or "),Vx=n(qL,"A",{href:!0});var iSr=s(Vx);nro=r(iSr,"FunnelBaseModel"),iSr.forEach(t),sro=r(qL," (Funnel Transformer model)"),qL.forEach(t),lro=i(C),Uh=n(C,"LI",{});var K0e=s(Uh);OJ=n(K0e,"STRONG",{});var dSr=s(OJ);iro=r(dSr,"gpt2"),dSr.forEach(t),dro=r(K0e," \u2014 "),Wx=n(K0e,"A",{href:!0});var cSr=s(Wx);cro=r(cSr,"GPT2Model"),cSr.forEach(t),fro=r(K0e," (OpenAI GPT-2 model)"),K0e.forEach(t),mro=i(C),Jh=n(C,"LI",{});var Z0e=s(Jh);XJ=n(Z0e,"STRONG",{});var fSr=s(XJ);gro=r(fSr,"gpt_neo"),fSr.forEach(t),hro=r(Z0e," \u2014 "),Qx=n(Z0e,"A",{href:!0});var mSr=s(Qx);pro=r(mSr,"GPTNeoModel"),mSr.forEach(t),_ro=r(Z0e," (GPT Neo model)"),Z0e.forEach(t),uro=i(C),Yh=n(C,"LI",{});var eTe=s(Yh);zJ=n(eTe,"STRONG",{});var gSr=s(zJ);bro=r(gSr,"gptj"),gSr.forEach(t),vro=r(eTe," \u2014 "),Hx=n(eTe,"A",{href:!0});var hSr=s(Hx);Tro=r(hSr,"GPTJModel"),hSr.forEach(t),Fro=r(eTe," (GPT-J model)"),eTe.forEach(t),Cro=i(C),Kh=n(C,"LI",{});var oTe=s(Kh);VJ=n(oTe,"STRONG",{});var pSr=s(VJ);Mro=r(pSr,"hubert"),pSr.forEach(t),Ero=r(oTe," \u2014 "),Ux=n(oTe,"A",{href:!0});var _Sr=s(Ux);yro=r(_Sr,"HubertModel"),_Sr.forEach(t),wro=r(oTe," (Hubert model)"),oTe.forEach(t),Aro=i(C),Zh=n(C,"LI",{});var rTe=s(Zh);WJ=n(rTe,"STRONG",{});var uSr=s(WJ);Lro=r(uSr,"ibert"),uSr.forEach(t),Bro=r(rTe," \u2014 "),Jx=n(rTe,"A",{href:!0});var bSr=s(Jx);kro=r(bSr,"IBertModel"),bSr.forEach(t),xro=r(rTe," (I-BERT model)"),rTe.forEach(t),Rro=i(C),ep=n(C,"LI",{});var tTe=s(ep);QJ=n(tTe,"STRONG",{});var vSr=s(QJ);Sro=r(vSr,"imagegpt"),vSr.forEach(t),Pro=r(tTe," \u2014 "),Yx=n(tTe,"A",{href:!0});var TSr=s(Yx);$ro=r(TSr,"ImageGPTModel"),TSr.forEach(t),Iro=r(tTe," (ImageGPT model)"),tTe.forEach(t),jro=i(C),op=n(C,"LI",{});var aTe=s(op);HJ=n(aTe,"STRONG",{});var FSr=s(HJ);Nro=r(FSr,"layoutlm"),FSr.forEach(t),Dro=r(aTe," \u2014 "),Kx=n(aTe,"A",{href:!0});var CSr=s(Kx);qro=r(CSr,"LayoutLMModel"),CSr.forEach(t),Gro=r(aTe," (LayoutLM model)"),aTe.forEach(t),Oro=i(C),rp=n(C,"LI",{});var nTe=s(rp);UJ=n(nTe,"STRONG",{});var MSr=s(UJ);Xro=r(MSr,"layoutlmv2"),MSr.forEach(t),zro=r(nTe," \u2014 "),Zx=n(nTe,"A",{href:!0});var ESr=s(Zx);Vro=r(ESr,"LayoutLMv2Model"),ESr.forEach(t),Wro=r(nTe," (LayoutLMv2 model)"),nTe.forEach(t),Qro=i(C),tp=n(C,"LI",{});var sTe=s(tp);JJ=n(sTe,"STRONG",{});var ySr=s(JJ);Hro=r(ySr,"led"),ySr.forEach(t),Uro=r(sTe," \u2014 "),eR=n(sTe,"A",{href:!0});var wSr=s(eR);Jro=r(wSr,"LEDModel"),wSr.forEach(t),Yro=r(sTe," (LED model)"),sTe.forEach(t),Kro=i(C),ap=n(C,"LI",{});var lTe=s(ap);YJ=n(lTe,"STRONG",{});var ASr=s(YJ);Zro=r(ASr,"longformer"),ASr.forEach(t),eto=r(lTe," \u2014 "),oR=n(lTe,"A",{href:!0});var LSr=s(oR);oto=r(LSr,"LongformerModel"),LSr.forEach(t),rto=r(lTe," (Longformer model)"),lTe.forEach(t),tto=i(C),np=n(C,"LI",{});var iTe=s(np);KJ=n(iTe,"STRONG",{});var BSr=s(KJ);ato=r(BSr,"luke"),BSr.forEach(t),nto=r(iTe," \u2014 "),rR=n(iTe,"A",{href:!0});var kSr=s(rR);sto=r(kSr,"LukeModel"),kSr.forEach(t),lto=r(iTe," (LUKE model)"),iTe.forEach(t),ito=i(C),sp=n(C,"LI",{});var dTe=s(sp);ZJ=n(dTe,"STRONG",{});var xSr=s(ZJ);dto=r(xSr,"lxmert"),xSr.forEach(t),cto=r(dTe," \u2014 "),tR=n(dTe,"A",{href:!0});var RSr=s(tR);fto=r(RSr,"LxmertModel"),RSr.forEach(t),mto=r(dTe," (LXMERT model)"),dTe.forEach(t),gto=i(C),lp=n(C,"LI",{});var cTe=s(lp);eY=n(cTe,"STRONG",{});var SSr=s(eY);hto=r(SSr,"m2m_100"),SSr.forEach(t),pto=r(cTe," \u2014 "),aR=n(cTe,"A",{href:!0});var PSr=s(aR);_to=r(PSr,"M2M100Model"),PSr.forEach(t),uto=r(cTe," (M2M100 model)"),cTe.forEach(t),bto=i(C),ip=n(C,"LI",{});var fTe=s(ip);oY=n(fTe,"STRONG",{});var $Sr=s(oY);vto=r($Sr,"marian"),$Sr.forEach(t),Tto=r(fTe," \u2014 "),nR=n(fTe,"A",{href:!0});var ISr=s(nR);Fto=r(ISr,"MarianModel"),ISr.forEach(t),Cto=r(fTe," (Marian model)"),fTe.forEach(t),Mto=i(C),dp=n(C,"LI",{});var mTe=s(dp);rY=n(mTe,"STRONG",{});var jSr=s(rY);Eto=r(jSr,"mbart"),jSr.forEach(t),yto=r(mTe," \u2014 "),sR=n(mTe,"A",{href:!0});var NSr=s(sR);wto=r(NSr,"MBartModel"),NSr.forEach(t),Ato=r(mTe," (mBART model)"),mTe.forEach(t),Lto=i(C),cp=n(C,"LI",{});var gTe=s(cp);tY=n(gTe,"STRONG",{});var DSr=s(tY);Bto=r(DSr,"megatron-bert"),DSr.forEach(t),kto=r(gTe," \u2014 "),lR=n(gTe,"A",{href:!0});var qSr=s(lR);xto=r(qSr,"MegatronBertModel"),qSr.forEach(t),Rto=r(gTe," (MegatronBert model)"),gTe.forEach(t),Sto=i(C),fp=n(C,"LI",{});var hTe=s(fp);aY=n(hTe,"STRONG",{});var GSr=s(aY);Pto=r(GSr,"mobilebert"),GSr.forEach(t),$to=r(hTe," \u2014 "),iR=n(hTe,"A",{href:!0});var OSr=s(iR);Ito=r(OSr,"MobileBertModel"),OSr.forEach(t),jto=r(hTe," (MobileBERT model)"),hTe.forEach(t),Nto=i(C),mp=n(C,"LI",{});var pTe=s(mp);nY=n(pTe,"STRONG",{});var XSr=s(nY);Dto=r(XSr,"mpnet"),XSr.forEach(t),qto=r(pTe," \u2014 "),dR=n(pTe,"A",{href:!0});var zSr=s(dR);Gto=r(zSr,"MPNetModel"),zSr.forEach(t),Oto=r(pTe," (MPNet model)"),pTe.forEach(t),Xto=i(C),gp=n(C,"LI",{});var _Te=s(gp);sY=n(_Te,"STRONG",{});var VSr=s(sY);zto=r(VSr,"mt5"),VSr.forEach(t),Vto=r(_Te," \u2014 "),cR=n(_Te,"A",{href:!0});var WSr=s(cR);Wto=r(WSr,"MT5Model"),WSr.forEach(t),Qto=r(_Te," (mT5 model)"),_Te.forEach(t),Hto=i(C),hp=n(C,"LI",{});var uTe=s(hp);lY=n(uTe,"STRONG",{});var QSr=s(lY);Uto=r(QSr,"nystromformer"),QSr.forEach(t),Jto=r(uTe," \u2014 "),fR=n(uTe,"A",{href:!0});var HSr=s(fR);Yto=r(HSr,"NystromformerModel"),HSr.forEach(t),Kto=r(uTe," (Nystromformer model)"),uTe.forEach(t),Zto=i(C),pp=n(C,"LI",{});var bTe=s(pp);iY=n(bTe,"STRONG",{});var USr=s(iY);eao=r(USr,"openai-gpt"),USr.forEach(t),oao=r(bTe," \u2014 "),mR=n(bTe,"A",{href:!0});var JSr=s(mR);rao=r(JSr,"OpenAIGPTModel"),JSr.forEach(t),tao=r(bTe," (OpenAI GPT model)"),bTe.forEach(t),aao=i(C),_p=n(C,"LI",{});var vTe=s(_p);dY=n(vTe,"STRONG",{});var YSr=s(dY);nao=r(YSr,"pegasus"),YSr.forEach(t),sao=r(vTe," \u2014 "),gR=n(vTe,"A",{href:!0});var KSr=s(gR);lao=r(KSr,"PegasusModel"),KSr.forEach(t),iao=r(vTe," (Pegasus model)"),vTe.forEach(t),dao=i(C),up=n(C,"LI",{});var TTe=s(up);cY=n(TTe,"STRONG",{});var ZSr=s(cY);cao=r(ZSr,"perceiver"),ZSr.forEach(t),fao=r(TTe," \u2014 "),hR=n(TTe,"A",{href:!0});var ePr=s(hR);mao=r(ePr,"PerceiverModel"),ePr.forEach(t),gao=r(TTe," (Perceiver model)"),TTe.forEach(t),hao=i(C),bp=n(C,"LI",{});var FTe=s(bp);fY=n(FTe,"STRONG",{});var oPr=s(fY);pao=r(oPr,"plbart"),oPr.forEach(t),_ao=r(FTe," \u2014 "),pR=n(FTe,"A",{href:!0});var rPr=s(pR);uao=r(rPr,"PLBartModel"),rPr.forEach(t),bao=r(FTe," (PLBart model)"),FTe.forEach(t),vao=i(C),vp=n(C,"LI",{});var CTe=s(vp);mY=n(CTe,"STRONG",{});var tPr=s(mY);Tao=r(tPr,"poolformer"),tPr.forEach(t),Fao=r(CTe," \u2014 "),_R=n(CTe,"A",{href:!0});var aPr=s(_R);Cao=r(aPr,"PoolFormerModel"),aPr.forEach(t),Mao=r(CTe," (PoolFormer model)"),CTe.forEach(t),Eao=i(C),Tp=n(C,"LI",{});var MTe=s(Tp);gY=n(MTe,"STRONG",{});var nPr=s(gY);yao=r(nPr,"prophetnet"),nPr.forEach(t),wao=r(MTe," \u2014 "),uR=n(MTe,"A",{href:!0});var sPr=s(uR);Aao=r(sPr,"ProphetNetModel"),sPr.forEach(t),Lao=r(MTe," (ProphetNet model)"),MTe.forEach(t),Bao=i(C),Fp=n(C,"LI",{});var ETe=s(Fp);hY=n(ETe,"STRONG",{});var lPr=s(hY);kao=r(lPr,"qdqbert"),lPr.forEach(t),xao=r(ETe," \u2014 "),bR=n(ETe,"A",{href:!0});var iPr=s(bR);Rao=r(iPr,"QDQBertModel"),iPr.forEach(t),Sao=r(ETe," (QDQBert model)"),ETe.forEach(t),Pao=i(C),Cp=n(C,"LI",{});var yTe=s(Cp);pY=n(yTe,"STRONG",{});var dPr=s(pY);$ao=r(dPr,"reformer"),dPr.forEach(t),Iao=r(yTe," \u2014 "),vR=n(yTe,"A",{href:!0});var cPr=s(vR);jao=r(cPr,"ReformerModel"),cPr.forEach(t),Nao=r(yTe," (Reformer model)"),yTe.forEach(t),Dao=i(C),Mp=n(C,"LI",{});var wTe=s(Mp);_Y=n(wTe,"STRONG",{});var fPr=s(_Y);qao=r(fPr,"rembert"),fPr.forEach(t),Gao=r(wTe," \u2014 "),TR=n(wTe,"A",{href:!0});var mPr=s(TR);Oao=r(mPr,"RemBertModel"),mPr.forEach(t),Xao=r(wTe," (RemBERT model)"),wTe.forEach(t),zao=i(C),Ep=n(C,"LI",{});var ATe=s(Ep);uY=n(ATe,"STRONG",{});var gPr=s(uY);Vao=r(gPr,"resnet"),gPr.forEach(t),Wao=r(ATe," \u2014 "),FR=n(ATe,"A",{href:!0});var hPr=s(FR);Qao=r(hPr,"ResNetModel"),hPr.forEach(t),Hao=r(ATe," (resnet model)"),ATe.forEach(t),Uao=i(C),yp=n(C,"LI",{});var LTe=s(yp);bY=n(LTe,"STRONG",{});var pPr=s(bY);Jao=r(pPr,"retribert"),pPr.forEach(t),Yao=r(LTe," \u2014 "),CR=n(LTe,"A",{href:!0});var _Pr=s(CR);Kao=r(_Pr,"RetriBertModel"),_Pr.forEach(t),Zao=r(LTe," (RetriBERT model)"),LTe.forEach(t),eno=i(C),wp=n(C,"LI",{});var BTe=s(wp);vY=n(BTe,"STRONG",{});var uPr=s(vY);ono=r(uPr,"roberta"),uPr.forEach(t),rno=r(BTe," \u2014 "),MR=n(BTe,"A",{href:!0});var bPr=s(MR);tno=r(bPr,"RobertaModel"),bPr.forEach(t),ano=r(BTe," (RoBERTa model)"),BTe.forEach(t),nno=i(C),Ap=n(C,"LI",{});var kTe=s(Ap);TY=n(kTe,"STRONG",{});var vPr=s(TY);sno=r(vPr,"roformer"),vPr.forEach(t),lno=r(kTe," \u2014 "),ER=n(kTe,"A",{href:!0});var TPr=s(ER);ino=r(TPr,"RoFormerModel"),TPr.forEach(t),dno=r(kTe," (RoFormer model)"),kTe.forEach(t),cno=i(C),Lp=n(C,"LI",{});var xTe=s(Lp);FY=n(xTe,"STRONG",{});var FPr=s(FY);fno=r(FPr,"segformer"),FPr.forEach(t),mno=r(xTe," \u2014 "),yR=n(xTe,"A",{href:!0});var CPr=s(yR);gno=r(CPr,"SegformerModel"),CPr.forEach(t),hno=r(xTe," (SegFormer model)"),xTe.forEach(t),pno=i(C),Bp=n(C,"LI",{});var RTe=s(Bp);CY=n(RTe,"STRONG",{});var MPr=s(CY);_no=r(MPr,"sew"),MPr.forEach(t),uno=r(RTe," \u2014 "),wR=n(RTe,"A",{href:!0});var EPr=s(wR);bno=r(EPr,"SEWModel"),EPr.forEach(t),vno=r(RTe," (SEW model)"),RTe.forEach(t),Tno=i(C),kp=n(C,"LI",{});var STe=s(kp);MY=n(STe,"STRONG",{});var yPr=s(MY);Fno=r(yPr,"sew-d"),yPr.forEach(t),Cno=r(STe," \u2014 "),AR=n(STe,"A",{href:!0});var wPr=s(AR);Mno=r(wPr,"SEWDModel"),wPr.forEach(t),Eno=r(STe," (SEW-D model)"),STe.forEach(t),yno=i(C),xp=n(C,"LI",{});var PTe=s(xp);EY=n(PTe,"STRONG",{});var APr=s(EY);wno=r(APr,"speech_to_text"),APr.forEach(t),Ano=r(PTe," \u2014 "),LR=n(PTe,"A",{href:!0});var LPr=s(LR);Lno=r(LPr,"Speech2TextModel"),LPr.forEach(t),Bno=r(PTe," (Speech2Text model)"),PTe.forEach(t),kno=i(C),Rp=n(C,"LI",{});var $Te=s(Rp);yY=n($Te,"STRONG",{});var BPr=s(yY);xno=r(BPr,"splinter"),BPr.forEach(t),Rno=r($Te," \u2014 "),BR=n($Te,"A",{href:!0});var kPr=s(BR);Sno=r(kPr,"SplinterModel"),kPr.forEach(t),Pno=r($Te," (Splinter model)"),$Te.forEach(t),$no=i(C),Sp=n(C,"LI",{});var ITe=s(Sp);wY=n(ITe,"STRONG",{});var xPr=s(wY);Ino=r(xPr,"squeezebert"),xPr.forEach(t),jno=r(ITe," \u2014 "),kR=n(ITe,"A",{href:!0});var RPr=s(kR);Nno=r(RPr,"SqueezeBertModel"),RPr.forEach(t),Dno=r(ITe," (SqueezeBERT model)"),ITe.forEach(t),qno=i(C),Pp=n(C,"LI",{});var jTe=s(Pp);AY=n(jTe,"STRONG",{});var SPr=s(AY);Gno=r(SPr,"swin"),SPr.forEach(t),Ono=r(jTe," \u2014 "),xR=n(jTe,"A",{href:!0});var PPr=s(xR);Xno=r(PPr,"SwinModel"),PPr.forEach(t),zno=r(jTe," (Swin model)"),jTe.forEach(t),Vno=i(C),$p=n(C,"LI",{});var NTe=s($p);LY=n(NTe,"STRONG",{});var $Pr=s(LY);Wno=r($Pr,"t5"),$Pr.forEach(t),Qno=r(NTe," \u2014 "),RR=n(NTe,"A",{href:!0});var IPr=s(RR);Hno=r(IPr,"T5Model"),IPr.forEach(t),Uno=r(NTe," (T5 model)"),NTe.forEach(t),Jno=i(C),Ip=n(C,"LI",{});var DTe=s(Ip);BY=n(DTe,"STRONG",{});var jPr=s(BY);Yno=r(jPr,"tapas"),jPr.forEach(t),Kno=r(DTe," \u2014 "),SR=n(DTe,"A",{href:!0});var NPr=s(SR);Zno=r(NPr,"TapasModel"),NPr.forEach(t),eso=r(DTe," (TAPAS model)"),DTe.forEach(t),oso=i(C),jp=n(C,"LI",{});var qTe=s(jp);kY=n(qTe,"STRONG",{});var DPr=s(kY);rso=r(DPr,"transfo-xl"),DPr.forEach(t),tso=r(qTe," \u2014 "),PR=n(qTe,"A",{href:!0});var qPr=s(PR);aso=r(qPr,"TransfoXLModel"),qPr.forEach(t),nso=r(qTe," (Transformer-XL model)"),qTe.forEach(t),sso=i(C),Np=n(C,"LI",{});var GTe=s(Np);xY=n(GTe,"STRONG",{});var GPr=s(xY);lso=r(GPr,"unispeech"),GPr.forEach(t),iso=r(GTe," \u2014 "),$R=n(GTe,"A",{href:!0});var OPr=s($R);dso=r(OPr,"UniSpeechModel"),OPr.forEach(t),cso=r(GTe," (UniSpeech model)"),GTe.forEach(t),fso=i(C),Dp=n(C,"LI",{});var OTe=s(Dp);RY=n(OTe,"STRONG",{});var XPr=s(RY);mso=r(XPr,"unispeech-sat"),XPr.forEach(t),gso=r(OTe," \u2014 "),IR=n(OTe,"A",{href:!0});var zPr=s(IR);hso=r(zPr,"UniSpeechSatModel"),zPr.forEach(t),pso=r(OTe," (UniSpeechSat model)"),OTe.forEach(t),_so=i(C),qp=n(C,"LI",{});var XTe=s(qp);SY=n(XTe,"STRONG",{});var VPr=s(SY);uso=r(VPr,"vilt"),VPr.forEach(t),bso=r(XTe," \u2014 "),jR=n(XTe,"A",{href:!0});var WPr=s(jR);vso=r(WPr,"ViltModel"),WPr.forEach(t),Tso=r(XTe," (ViLT model)"),XTe.forEach(t),Fso=i(C),Gp=n(C,"LI",{});var zTe=s(Gp);PY=n(zTe,"STRONG",{});var QPr=s(PY);Cso=r(QPr,"vision-text-dual-encoder"),QPr.forEach(t),Mso=r(zTe," \u2014 "),NR=n(zTe,"A",{href:!0});var HPr=s(NR);Eso=r(HPr,"VisionTextDualEncoderModel"),HPr.forEach(t),yso=r(zTe," (VisionTextDualEncoder model)"),zTe.forEach(t),wso=i(C),Op=n(C,"LI",{});var VTe=s(Op);$Y=n(VTe,"STRONG",{});var UPr=s($Y);Aso=r(UPr,"visual_bert"),UPr.forEach(t),Lso=r(VTe," \u2014 "),DR=n(VTe,"A",{href:!0});var JPr=s(DR);Bso=r(JPr,"VisualBertModel"),JPr.forEach(t),kso=r(VTe," (VisualBert model)"),VTe.forEach(t),xso=i(C),Xp=n(C,"LI",{});var WTe=s(Xp);IY=n(WTe,"STRONG",{});var YPr=s(IY);Rso=r(YPr,"vit"),YPr.forEach(t),Sso=r(WTe," \u2014 "),qR=n(WTe,"A",{href:!0});var KPr=s(qR);Pso=r(KPr,"ViTModel"),KPr.forEach(t),$so=r(WTe," (ViT model)"),WTe.forEach(t),Iso=i(C),zp=n(C,"LI",{});var QTe=s(zp);jY=n(QTe,"STRONG",{});var ZPr=s(jY);jso=r(ZPr,"vit_mae"),ZPr.forEach(t),Nso=r(QTe," \u2014 "),GR=n(QTe,"A",{href:!0});var e$r=s(GR);Dso=r(e$r,"ViTMAEModel"),e$r.forEach(t),qso=r(QTe," (ViTMAE model)"),QTe.forEach(t),Gso=i(C),Vp=n(C,"LI",{});var HTe=s(Vp);NY=n(HTe,"STRONG",{});var o$r=s(NY);Oso=r(o$r,"wav2vec2"),o$r.forEach(t),Xso=r(HTe," \u2014 "),OR=n(HTe,"A",{href:!0});var r$r=s(OR);zso=r(r$r,"Wav2Vec2Model"),r$r.forEach(t),Vso=r(HTe," (Wav2Vec2 model)"),HTe.forEach(t),Wso=i(C),Wp=n(C,"LI",{});var UTe=s(Wp);DY=n(UTe,"STRONG",{});var t$r=s(DY);Qso=r(t$r,"wavlm"),t$r.forEach(t),Hso=r(UTe," \u2014 "),XR=n(UTe,"A",{href:!0});var a$r=s(XR);Uso=r(a$r,"WavLMModel"),a$r.forEach(t),Jso=r(UTe," (WavLM model)"),UTe.forEach(t),Yso=i(C),Qp=n(C,"LI",{});var JTe=s(Qp);qY=n(JTe,"STRONG",{});var n$r=s(qY);Kso=r(n$r,"xglm"),n$r.forEach(t),Zso=r(JTe," \u2014 "),zR=n(JTe,"A",{href:!0});var s$r=s(zR);elo=r(s$r,"XGLMModel"),s$r.forEach(t),olo=r(JTe," (XGLM model)"),JTe.forEach(t),rlo=i(C),Hp=n(C,"LI",{});var YTe=s(Hp);GY=n(YTe,"STRONG",{});var l$r=s(GY);tlo=r(l$r,"xlm"),l$r.forEach(t),alo=r(YTe," \u2014 "),VR=n(YTe,"A",{href:!0});var i$r=s(VR);nlo=r(i$r,"XLMModel"),i$r.forEach(t),slo=r(YTe," (XLM model)"),YTe.forEach(t),llo=i(C),Up=n(C,"LI",{});var KTe=s(Up);OY=n(KTe,"STRONG",{});var d$r=s(OY);ilo=r(d$r,"xlm-prophetnet"),d$r.forEach(t),dlo=r(KTe," \u2014 "),WR=n(KTe,"A",{href:!0});var c$r=s(WR);clo=r(c$r,"XLMProphetNetModel"),c$r.forEach(t),flo=r(KTe," (XLMProphetNet model)"),KTe.forEach(t),mlo=i(C),Jp=n(C,"LI",{});var ZTe=s(Jp);XY=n(ZTe,"STRONG",{});var f$r=s(XY);glo=r(f$r,"xlm-roberta"),f$r.forEach(t),hlo=r(ZTe," \u2014 "),QR=n(ZTe,"A",{href:!0});var m$r=s(QR);plo=r(m$r,"XLMRobertaModel"),m$r.forEach(t),_lo=r(ZTe," (XLM-RoBERTa model)"),ZTe.forEach(t),ulo=i(C),Yp=n(C,"LI",{});var eFe=s(Yp);zY=n(eFe,"STRONG",{});var g$r=s(zY);blo=r(g$r,"xlm-roberta-xl"),g$r.forEach(t),vlo=r(eFe," \u2014 "),HR=n(eFe,"A",{href:!0});var h$r=s(HR);Tlo=r(h$r,"XLMRobertaXLModel"),h$r.forEach(t),Flo=r(eFe," (XLM-RoBERTa-XL model)"),eFe.forEach(t),Clo=i(C),Kp=n(C,"LI",{});var oFe=s(Kp);VY=n(oFe,"STRONG",{});var p$r=s(VY);Mlo=r(p$r,"xlnet"),p$r.forEach(t),Elo=r(oFe," \u2014 "),UR=n(oFe,"A",{href:!0});var _$r=s(UR);ylo=r(_$r,"XLNetModel"),_$r.forEach(t),wlo=r(oFe," (XLNet model)"),oFe.forEach(t),Alo=i(C),Zp=n(C,"LI",{});var rFe=s(Zp);WY=n(rFe,"STRONG",{});var u$r=s(WY);Llo=r(u$r,"yoso"),u$r.forEach(t),Blo=r(rFe," \u2014 "),JR=n(rFe,"A",{href:!0});var b$r=s(JR);klo=r(b$r,"YosoModel"),b$r.forEach(t),xlo=r(rFe," (YOSO model)"),rFe.forEach(t),C.forEach(t),Rlo=i(St),e_=n(St,"P",{});var tFe=s(e_);Slo=r(tFe,"The model is set in evaluation mode by default using "),QY=n(tFe,"CODE",{});var v$r=s(QY);Plo=r(v$r,"model.eval()"),v$r.forEach(t),$lo=r(tFe,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),HY=n(tFe,"CODE",{});var T$r=s(HY);Ilo=r(T$r,"model.train()"),T$r.forEach(t),tFe.forEach(t),jlo=i(St),UY=n(St,"P",{});var F$r=s(UY);Nlo=r(F$r,"Examples:"),F$r.forEach(t),Dlo=i(St),m(VM.$$.fragment,St),St.forEach(t),Ns.forEach(t),P8e=i(d),Xi=n(d,"H2",{class:!0});var GBe=s(Xi);o_=n(GBe,"A",{id:!0,class:!0,href:!0});var C$r=s(o_);JY=n(C$r,"SPAN",{});var M$r=s(JY);m(WM.$$.fragment,M$r),M$r.forEach(t),C$r.forEach(t),qlo=i(GBe),YY=n(GBe,"SPAN",{});var E$r=s(YY);Glo=r(E$r,"AutoModelForPreTraining"),E$r.forEach(t),GBe.forEach(t),$8e=i(d),Wo=n(d,"DIV",{class:!0});var qs=s(Wo);m(QM.$$.fragment,qs),Olo=i(qs),zi=n(qs,"P",{});var rz=s(zi);Xlo=r(rz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a pretraining head) when created
with the `),KY=n(rz,"CODE",{});var y$r=s(KY);zlo=r(y$r,"from_pretrained()"),y$r.forEach(t),Vlo=r(rz,"class method or the "),ZY=n(rz,"CODE",{});var w$r=s(ZY);Wlo=r(w$r,"from_config()"),w$r.forEach(t),Qlo=r(rz,`class
method.`),rz.forEach(t),Hlo=i(qs),HM=n(qs,"P",{});var OBe=s(HM);Ulo=r(OBe,"This class cannot be instantiated directly using "),eK=n(OBe,"CODE",{});var A$r=s(eK);Jlo=r(A$r,"__init__()"),A$r.forEach(t),Ylo=r(OBe," (throws an error)."),OBe.forEach(t),Klo=i(qs),Dr=n(qs,"DIV",{class:!0});var Gs=s(Dr);m(UM.$$.fragment,Gs),Zlo=i(Gs),oK=n(Gs,"P",{});var L$r=s(oK);eio=r(L$r,"Instantiates one of the model classes of the library (with a pretraining head) from a configuration."),L$r.forEach(t),oio=i(Gs),Vi=n(Gs,"P",{});var tz=s(Vi);rio=r(tz,`Note:
Loading a model from its configuration file does `),rK=n(tz,"STRONG",{});var B$r=s(rK);tio=r(B$r,"not"),B$r.forEach(t),aio=r(tz,` load the model weights. It only affects the
model\u2019s configuration. Use `),tK=n(tz,"CODE",{});var k$r=s(tK);nio=r(k$r,"from_pretrained()"),k$r.forEach(t),sio=r(tz,"to load the model weights."),tz.forEach(t),lio=i(Gs),aK=n(Gs,"P",{});var x$r=s(aK);iio=r(x$r,"Examples:"),x$r.forEach(t),dio=i(Gs),m(JM.$$.fragment,Gs),Gs.forEach(t),cio=i(qs),Re=n(qs,"DIV",{class:!0});var Pt=s(Re);m(YM.$$.fragment,Pt),fio=i(Pt),nK=n(Pt,"P",{});var R$r=s(nK);mio=r(R$r,"Instantiate one of the model classes of the library (with a pretraining head) from a pretrained model."),R$r.forEach(t),gio=i(Pt),qa=n(Pt,"P",{});var m4=s(qa);hio=r(m4,"The model class to instantiate is selected based on the "),sK=n(m4,"CODE",{});var S$r=s(sK);pio=r(S$r,"model_type"),S$r.forEach(t),_io=r(m4,` property of the config object (either
passed as an argument or loaded from `),lK=n(m4,"CODE",{});var P$r=s(lK);uio=r(P$r,"pretrained_model_name_or_path"),P$r.forEach(t),bio=r(m4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),iK=n(m4,"CODE",{});var $$r=s(iK);vio=r($$r,"pretrained_model_name_or_path"),$$r.forEach(t),Tio=r(m4,":"),m4.forEach(t),Fio=i(Pt),x=n(Pt,"UL",{});var S=s(x);r_=n(S,"LI",{});var aFe=s(r_);dK=n(aFe,"STRONG",{});var I$r=s(dK);Cio=r(I$r,"albert"),I$r.forEach(t),Mio=r(aFe," \u2014 "),YR=n(aFe,"A",{href:!0});var j$r=s(YR);Eio=r(j$r,"AlbertForPreTraining"),j$r.forEach(t),yio=r(aFe," (ALBERT model)"),aFe.forEach(t),wio=i(S),t_=n(S,"LI",{});var nFe=s(t_);cK=n(nFe,"STRONG",{});var N$r=s(cK);Aio=r(N$r,"bart"),N$r.forEach(t),Lio=r(nFe," \u2014 "),KR=n(nFe,"A",{href:!0});var D$r=s(KR);Bio=r(D$r,"BartForConditionalGeneration"),D$r.forEach(t),kio=r(nFe," (BART model)"),nFe.forEach(t),xio=i(S),a_=n(S,"LI",{});var sFe=s(a_);fK=n(sFe,"STRONG",{});var q$r=s(fK);Rio=r(q$r,"bert"),q$r.forEach(t),Sio=r(sFe," \u2014 "),ZR=n(sFe,"A",{href:!0});var G$r=s(ZR);Pio=r(G$r,"BertForPreTraining"),G$r.forEach(t),$io=r(sFe," (BERT model)"),sFe.forEach(t),Iio=i(S),n_=n(S,"LI",{});var lFe=s(n_);mK=n(lFe,"STRONG",{});var O$r=s(mK);jio=r(O$r,"big_bird"),O$r.forEach(t),Nio=r(lFe," \u2014 "),eS=n(lFe,"A",{href:!0});var X$r=s(eS);Dio=r(X$r,"BigBirdForPreTraining"),X$r.forEach(t),qio=r(lFe," (BigBird model)"),lFe.forEach(t),Gio=i(S),s_=n(S,"LI",{});var iFe=s(s_);gK=n(iFe,"STRONG",{});var z$r=s(gK);Oio=r(z$r,"camembert"),z$r.forEach(t),Xio=r(iFe," \u2014 "),oS=n(iFe,"A",{href:!0});var V$r=s(oS);zio=r(V$r,"CamembertForMaskedLM"),V$r.forEach(t),Vio=r(iFe," (CamemBERT model)"),iFe.forEach(t),Wio=i(S),l_=n(S,"LI",{});var dFe=s(l_);hK=n(dFe,"STRONG",{});var W$r=s(hK);Qio=r(W$r,"ctrl"),W$r.forEach(t),Hio=r(dFe," \u2014 "),rS=n(dFe,"A",{href:!0});var Q$r=s(rS);Uio=r(Q$r,"CTRLLMHeadModel"),Q$r.forEach(t),Jio=r(dFe," (CTRL model)"),dFe.forEach(t),Yio=i(S),i_=n(S,"LI",{});var cFe=s(i_);pK=n(cFe,"STRONG",{});var H$r=s(pK);Kio=r(H$r,"deberta"),H$r.forEach(t),Zio=r(cFe," \u2014 "),tS=n(cFe,"A",{href:!0});var U$r=s(tS);edo=r(U$r,"DebertaForMaskedLM"),U$r.forEach(t),odo=r(cFe," (DeBERTa model)"),cFe.forEach(t),rdo=i(S),d_=n(S,"LI",{});var fFe=s(d_);_K=n(fFe,"STRONG",{});var J$r=s(_K);tdo=r(J$r,"deberta-v2"),J$r.forEach(t),ado=r(fFe," \u2014 "),aS=n(fFe,"A",{href:!0});var Y$r=s(aS);ndo=r(Y$r,"DebertaV2ForMaskedLM"),Y$r.forEach(t),sdo=r(fFe," (DeBERTa-v2 model)"),fFe.forEach(t),ldo=i(S),c_=n(S,"LI",{});var mFe=s(c_);uK=n(mFe,"STRONG",{});var K$r=s(uK);ido=r(K$r,"distilbert"),K$r.forEach(t),ddo=r(mFe," \u2014 "),nS=n(mFe,"A",{href:!0});var Z$r=s(nS);cdo=r(Z$r,"DistilBertForMaskedLM"),Z$r.forEach(t),fdo=r(mFe," (DistilBERT model)"),mFe.forEach(t),mdo=i(S),f_=n(S,"LI",{});var gFe=s(f_);bK=n(gFe,"STRONG",{});var eIr=s(bK);gdo=r(eIr,"electra"),eIr.forEach(t),hdo=r(gFe," \u2014 "),sS=n(gFe,"A",{href:!0});var oIr=s(sS);pdo=r(oIr,"ElectraForPreTraining"),oIr.forEach(t),_do=r(gFe," (ELECTRA model)"),gFe.forEach(t),udo=i(S),m_=n(S,"LI",{});var hFe=s(m_);vK=n(hFe,"STRONG",{});var rIr=s(vK);bdo=r(rIr,"flaubert"),rIr.forEach(t),vdo=r(hFe," \u2014 "),lS=n(hFe,"A",{href:!0});var tIr=s(lS);Tdo=r(tIr,"FlaubertWithLMHeadModel"),tIr.forEach(t),Fdo=r(hFe," (FlauBERT model)"),hFe.forEach(t),Cdo=i(S),g_=n(S,"LI",{});var pFe=s(g_);TK=n(pFe,"STRONG",{});var aIr=s(TK);Mdo=r(aIr,"fnet"),aIr.forEach(t),Edo=r(pFe," \u2014 "),iS=n(pFe,"A",{href:!0});var nIr=s(iS);ydo=r(nIr,"FNetForPreTraining"),nIr.forEach(t),wdo=r(pFe," (FNet model)"),pFe.forEach(t),Ado=i(S),h_=n(S,"LI",{});var _Fe=s(h_);FK=n(_Fe,"STRONG",{});var sIr=s(FK);Ldo=r(sIr,"fsmt"),sIr.forEach(t),Bdo=r(_Fe," \u2014 "),dS=n(_Fe,"A",{href:!0});var lIr=s(dS);kdo=r(lIr,"FSMTForConditionalGeneration"),lIr.forEach(t),xdo=r(_Fe," (FairSeq Machine-Translation model)"),_Fe.forEach(t),Rdo=i(S),p_=n(S,"LI",{});var uFe=s(p_);CK=n(uFe,"STRONG",{});var iIr=s(CK);Sdo=r(iIr,"funnel"),iIr.forEach(t),Pdo=r(uFe," \u2014 "),cS=n(uFe,"A",{href:!0});var dIr=s(cS);$do=r(dIr,"FunnelForPreTraining"),dIr.forEach(t),Ido=r(uFe," (Funnel Transformer model)"),uFe.forEach(t),jdo=i(S),__=n(S,"LI",{});var bFe=s(__);MK=n(bFe,"STRONG",{});var cIr=s(MK);Ndo=r(cIr,"gpt2"),cIr.forEach(t),Ddo=r(bFe," \u2014 "),fS=n(bFe,"A",{href:!0});var fIr=s(fS);qdo=r(fIr,"GPT2LMHeadModel"),fIr.forEach(t),Gdo=r(bFe," (OpenAI GPT-2 model)"),bFe.forEach(t),Odo=i(S),u_=n(S,"LI",{});var vFe=s(u_);EK=n(vFe,"STRONG",{});var mIr=s(EK);Xdo=r(mIr,"ibert"),mIr.forEach(t),zdo=r(vFe," \u2014 "),mS=n(vFe,"A",{href:!0});var gIr=s(mS);Vdo=r(gIr,"IBertForMaskedLM"),gIr.forEach(t),Wdo=r(vFe," (I-BERT model)"),vFe.forEach(t),Qdo=i(S),b_=n(S,"LI",{});var TFe=s(b_);yK=n(TFe,"STRONG",{});var hIr=s(yK);Hdo=r(hIr,"layoutlm"),hIr.forEach(t),Udo=r(TFe," \u2014 "),gS=n(TFe,"A",{href:!0});var pIr=s(gS);Jdo=r(pIr,"LayoutLMForMaskedLM"),pIr.forEach(t),Ydo=r(TFe," (LayoutLM model)"),TFe.forEach(t),Kdo=i(S),v_=n(S,"LI",{});var FFe=s(v_);wK=n(FFe,"STRONG",{});var _Ir=s(wK);Zdo=r(_Ir,"longformer"),_Ir.forEach(t),eco=r(FFe," \u2014 "),hS=n(FFe,"A",{href:!0});var uIr=s(hS);oco=r(uIr,"LongformerForMaskedLM"),uIr.forEach(t),rco=r(FFe," (Longformer model)"),FFe.forEach(t),tco=i(S),T_=n(S,"LI",{});var CFe=s(T_);AK=n(CFe,"STRONG",{});var bIr=s(AK);aco=r(bIr,"lxmert"),bIr.forEach(t),nco=r(CFe," \u2014 "),pS=n(CFe,"A",{href:!0});var vIr=s(pS);sco=r(vIr,"LxmertForPreTraining"),vIr.forEach(t),lco=r(CFe," (LXMERT model)"),CFe.forEach(t),ico=i(S),F_=n(S,"LI",{});var MFe=s(F_);LK=n(MFe,"STRONG",{});var TIr=s(LK);dco=r(TIr,"megatron-bert"),TIr.forEach(t),cco=r(MFe," \u2014 "),_S=n(MFe,"A",{href:!0});var FIr=s(_S);fco=r(FIr,"MegatronBertForPreTraining"),FIr.forEach(t),mco=r(MFe," (MegatronBert model)"),MFe.forEach(t),gco=i(S),C_=n(S,"LI",{});var EFe=s(C_);BK=n(EFe,"STRONG",{});var CIr=s(BK);hco=r(CIr,"mobilebert"),CIr.forEach(t),pco=r(EFe," \u2014 "),uS=n(EFe,"A",{href:!0});var MIr=s(uS);_co=r(MIr,"MobileBertForPreTraining"),MIr.forEach(t),uco=r(EFe," (MobileBERT model)"),EFe.forEach(t),bco=i(S),M_=n(S,"LI",{});var yFe=s(M_);kK=n(yFe,"STRONG",{});var EIr=s(kK);vco=r(EIr,"mpnet"),EIr.forEach(t),Tco=r(yFe," \u2014 "),bS=n(yFe,"A",{href:!0});var yIr=s(bS);Fco=r(yIr,"MPNetForMaskedLM"),yIr.forEach(t),Cco=r(yFe," (MPNet model)"),yFe.forEach(t),Mco=i(S),E_=n(S,"LI",{});var wFe=s(E_);xK=n(wFe,"STRONG",{});var wIr=s(xK);Eco=r(wIr,"openai-gpt"),wIr.forEach(t),yco=r(wFe," \u2014 "),vS=n(wFe,"A",{href:!0});var AIr=s(vS);wco=r(AIr,"OpenAIGPTLMHeadModel"),AIr.forEach(t),Aco=r(wFe," (OpenAI GPT model)"),wFe.forEach(t),Lco=i(S),y_=n(S,"LI",{});var AFe=s(y_);RK=n(AFe,"STRONG",{});var LIr=s(RK);Bco=r(LIr,"retribert"),LIr.forEach(t),kco=r(AFe," \u2014 "),TS=n(AFe,"A",{href:!0});var BIr=s(TS);xco=r(BIr,"RetriBertModel"),BIr.forEach(t),Rco=r(AFe," (RetriBERT model)"),AFe.forEach(t),Sco=i(S),w_=n(S,"LI",{});var LFe=s(w_);SK=n(LFe,"STRONG",{});var kIr=s(SK);Pco=r(kIr,"roberta"),kIr.forEach(t),$co=r(LFe," \u2014 "),FS=n(LFe,"A",{href:!0});var xIr=s(FS);Ico=r(xIr,"RobertaForMaskedLM"),xIr.forEach(t),jco=r(LFe," (RoBERTa model)"),LFe.forEach(t),Nco=i(S),A_=n(S,"LI",{});var BFe=s(A_);PK=n(BFe,"STRONG",{});var RIr=s(PK);Dco=r(RIr,"squeezebert"),RIr.forEach(t),qco=r(BFe," \u2014 "),CS=n(BFe,"A",{href:!0});var SIr=s(CS);Gco=r(SIr,"SqueezeBertForMaskedLM"),SIr.forEach(t),Oco=r(BFe," (SqueezeBERT model)"),BFe.forEach(t),Xco=i(S),L_=n(S,"LI",{});var kFe=s(L_);$K=n(kFe,"STRONG",{});var PIr=s($K);zco=r(PIr,"t5"),PIr.forEach(t),Vco=r(kFe," \u2014 "),MS=n(kFe,"A",{href:!0});var $Ir=s(MS);Wco=r($Ir,"T5ForConditionalGeneration"),$Ir.forEach(t),Qco=r(kFe," (T5 model)"),kFe.forEach(t),Hco=i(S),B_=n(S,"LI",{});var xFe=s(B_);IK=n(xFe,"STRONG",{});var IIr=s(IK);Uco=r(IIr,"tapas"),IIr.forEach(t),Jco=r(xFe," \u2014 "),ES=n(xFe,"A",{href:!0});var jIr=s(ES);Yco=r(jIr,"TapasForMaskedLM"),jIr.forEach(t),Kco=r(xFe," (TAPAS model)"),xFe.forEach(t),Zco=i(S),k_=n(S,"LI",{});var RFe=s(k_);jK=n(RFe,"STRONG",{});var NIr=s(jK);efo=r(NIr,"transfo-xl"),NIr.forEach(t),ofo=r(RFe," \u2014 "),yS=n(RFe,"A",{href:!0});var DIr=s(yS);rfo=r(DIr,"TransfoXLLMHeadModel"),DIr.forEach(t),tfo=r(RFe," (Transformer-XL model)"),RFe.forEach(t),afo=i(S),x_=n(S,"LI",{});var SFe=s(x_);NK=n(SFe,"STRONG",{});var qIr=s(NK);nfo=r(qIr,"unispeech"),qIr.forEach(t),sfo=r(SFe," \u2014 "),wS=n(SFe,"A",{href:!0});var GIr=s(wS);lfo=r(GIr,"UniSpeechForPreTraining"),GIr.forEach(t),ifo=r(SFe," (UniSpeech model)"),SFe.forEach(t),dfo=i(S),R_=n(S,"LI",{});var PFe=s(R_);DK=n(PFe,"STRONG",{});var OIr=s(DK);cfo=r(OIr,"unispeech-sat"),OIr.forEach(t),ffo=r(PFe," \u2014 "),AS=n(PFe,"A",{href:!0});var XIr=s(AS);mfo=r(XIr,"UniSpeechSatForPreTraining"),XIr.forEach(t),gfo=r(PFe," (UniSpeechSat model)"),PFe.forEach(t),hfo=i(S),S_=n(S,"LI",{});var $Fe=s(S_);qK=n($Fe,"STRONG",{});var zIr=s(qK);pfo=r(zIr,"visual_bert"),zIr.forEach(t),_fo=r($Fe," \u2014 "),LS=n($Fe,"A",{href:!0});var VIr=s(LS);ufo=r(VIr,"VisualBertForPreTraining"),VIr.forEach(t),bfo=r($Fe," (VisualBert model)"),$Fe.forEach(t),vfo=i(S),P_=n(S,"LI",{});var IFe=s(P_);GK=n(IFe,"STRONG",{});var WIr=s(GK);Tfo=r(WIr,"vit_mae"),WIr.forEach(t),Ffo=r(IFe," \u2014 "),BS=n(IFe,"A",{href:!0});var QIr=s(BS);Cfo=r(QIr,"ViTMAEForPreTraining"),QIr.forEach(t),Mfo=r(IFe," (ViTMAE model)"),IFe.forEach(t),Efo=i(S),$_=n(S,"LI",{});var jFe=s($_);OK=n(jFe,"STRONG",{});var HIr=s(OK);yfo=r(HIr,"wav2vec2"),HIr.forEach(t),wfo=r(jFe," \u2014 "),kS=n(jFe,"A",{href:!0});var UIr=s(kS);Afo=r(UIr,"Wav2Vec2ForPreTraining"),UIr.forEach(t),Lfo=r(jFe," (Wav2Vec2 model)"),jFe.forEach(t),Bfo=i(S),I_=n(S,"LI",{});var NFe=s(I_);XK=n(NFe,"STRONG",{});var JIr=s(XK);kfo=r(JIr,"xlm"),JIr.forEach(t),xfo=r(NFe," \u2014 "),xS=n(NFe,"A",{href:!0});var YIr=s(xS);Rfo=r(YIr,"XLMWithLMHeadModel"),YIr.forEach(t),Sfo=r(NFe," (XLM model)"),NFe.forEach(t),Pfo=i(S),j_=n(S,"LI",{});var DFe=s(j_);zK=n(DFe,"STRONG",{});var KIr=s(zK);$fo=r(KIr,"xlm-roberta"),KIr.forEach(t),Ifo=r(DFe," \u2014 "),RS=n(DFe,"A",{href:!0});var ZIr=s(RS);jfo=r(ZIr,"XLMRobertaForMaskedLM"),ZIr.forEach(t),Nfo=r(DFe," (XLM-RoBERTa model)"),DFe.forEach(t),Dfo=i(S),N_=n(S,"LI",{});var qFe=s(N_);VK=n(qFe,"STRONG",{});var ejr=s(VK);qfo=r(ejr,"xlm-roberta-xl"),ejr.forEach(t),Gfo=r(qFe," \u2014 "),SS=n(qFe,"A",{href:!0});var ojr=s(SS);Ofo=r(ojr,"XLMRobertaXLForMaskedLM"),ojr.forEach(t),Xfo=r(qFe," (XLM-RoBERTa-XL model)"),qFe.forEach(t),zfo=i(S),D_=n(S,"LI",{});var GFe=s(D_);WK=n(GFe,"STRONG",{});var rjr=s(WK);Vfo=r(rjr,"xlnet"),rjr.forEach(t),Wfo=r(GFe," \u2014 "),PS=n(GFe,"A",{href:!0});var tjr=s(PS);Qfo=r(tjr,"XLNetLMHeadModel"),tjr.forEach(t),Hfo=r(GFe," (XLNet model)"),GFe.forEach(t),S.forEach(t),Ufo=i(Pt),q_=n(Pt,"P",{});var OFe=s(q_);Jfo=r(OFe,"The model is set in evaluation mode by default using "),QK=n(OFe,"CODE",{});var ajr=s(QK);Yfo=r(ajr,"model.eval()"),ajr.forEach(t),Kfo=r(OFe,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),HK=n(OFe,"CODE",{});var njr=s(HK);Zfo=r(njr,"model.train()"),njr.forEach(t),OFe.forEach(t),emo=i(Pt),UK=n(Pt,"P",{});var sjr=s(UK);omo=r(sjr,"Examples:"),sjr.forEach(t),rmo=i(Pt),m(KM.$$.fragment,Pt),Pt.forEach(t),qs.forEach(t),I8e=i(d),Wi=n(d,"H2",{class:!0});var XBe=s(Wi);G_=n(XBe,"A",{id:!0,class:!0,href:!0});var ljr=s(G_);JK=n(ljr,"SPAN",{});var ijr=s(JK);m(ZM.$$.fragment,ijr),ijr.forEach(t),ljr.forEach(t),tmo=i(XBe),YK=n(XBe,"SPAN",{});var djr=s(YK);amo=r(djr,"AutoModelForCausalLM"),djr.forEach(t),XBe.forEach(t),j8e=i(d),Qo=n(d,"DIV",{class:!0});var Os=s(Qo);m(eE.$$.fragment,Os),nmo=i(Os),Qi=n(Os,"P",{});var az=s(Qi);smo=r(az,`This is a generic model class that will be instantiated as one of the model classes of the library (with a causal language modeling head) when created
with the `),KK=n(az,"CODE",{});var cjr=s(KK);lmo=r(cjr,"from_pretrained()"),cjr.forEach(t),imo=r(az,"class method or the "),ZK=n(az,"CODE",{});var fjr=s(ZK);dmo=r(fjr,"from_config()"),fjr.forEach(t),cmo=r(az,`class
method.`),az.forEach(t),fmo=i(Os),oE=n(Os,"P",{});var zBe=s(oE);mmo=r(zBe,"This class cannot be instantiated directly using "),eZ=n(zBe,"CODE",{});var mjr=s(eZ);gmo=r(mjr,"__init__()"),mjr.forEach(t),hmo=r(zBe," (throws an error)."),zBe.forEach(t),pmo=i(Os),qr=n(Os,"DIV",{class:!0});var Xs=s(qr);m(rE.$$.fragment,Xs),_mo=i(Xs),oZ=n(Xs,"P",{});var gjr=s(oZ);umo=r(gjr,"Instantiates one of the model classes of the library (with a causal language modeling head) from a configuration."),gjr.forEach(t),bmo=i(Xs),Hi=n(Xs,"P",{});var nz=s(Hi);vmo=r(nz,`Note:
Loading a model from its configuration file does `),rZ=n(nz,"STRONG",{});var hjr=s(rZ);Tmo=r(hjr,"not"),hjr.forEach(t),Fmo=r(nz,` load the model weights. It only affects the
model\u2019s configuration. Use `),tZ=n(nz,"CODE",{});var pjr=s(tZ);Cmo=r(pjr,"from_pretrained()"),pjr.forEach(t),Mmo=r(nz,"to load the model weights."),nz.forEach(t),Emo=i(Xs),aZ=n(Xs,"P",{});var _jr=s(aZ);ymo=r(_jr,"Examples:"),_jr.forEach(t),wmo=i(Xs),m(tE.$$.fragment,Xs),Xs.forEach(t),Amo=i(Os),Se=n(Os,"DIV",{class:!0});var $t=s(Se);m(aE.$$.fragment,$t),Lmo=i($t),nZ=n($t,"P",{});var ujr=s(nZ);Bmo=r(ujr,"Instantiate one of the model classes of the library (with a causal language modeling head) from a pretrained model."),ujr.forEach(t),kmo=i($t),Ga=n($t,"P",{});var g4=s(Ga);xmo=r(g4,"The model class to instantiate is selected based on the "),sZ=n(g4,"CODE",{});var bjr=s(sZ);Rmo=r(bjr,"model_type"),bjr.forEach(t),Smo=r(g4,` property of the config object (either
passed as an argument or loaded from `),lZ=n(g4,"CODE",{});var vjr=s(lZ);Pmo=r(vjr,"pretrained_model_name_or_path"),vjr.forEach(t),$mo=r(g4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),iZ=n(g4,"CODE",{});var Tjr=s(iZ);Imo=r(Tjr,"pretrained_model_name_or_path"),Tjr.forEach(t),jmo=r(g4,":"),g4.forEach(t),Nmo=i($t),$=n($t,"UL",{});var j=s($);O_=n(j,"LI",{});var XFe=s(O_);dZ=n(XFe,"STRONG",{});var Fjr=s(dZ);Dmo=r(Fjr,"bart"),Fjr.forEach(t),qmo=r(XFe," \u2014 "),$S=n(XFe,"A",{href:!0});var Cjr=s($S);Gmo=r(Cjr,"BartForCausalLM"),Cjr.forEach(t),Omo=r(XFe," (BART model)"),XFe.forEach(t),Xmo=i(j),X_=n(j,"LI",{});var zFe=s(X_);cZ=n(zFe,"STRONG",{});var Mjr=s(cZ);zmo=r(Mjr,"bert"),Mjr.forEach(t),Vmo=r(zFe," \u2014 "),IS=n(zFe,"A",{href:!0});var Ejr=s(IS);Wmo=r(Ejr,"BertLMHeadModel"),Ejr.forEach(t),Qmo=r(zFe," (BERT model)"),zFe.forEach(t),Hmo=i(j),z_=n(j,"LI",{});var VFe=s(z_);fZ=n(VFe,"STRONG",{});var yjr=s(fZ);Umo=r(yjr,"bert-generation"),yjr.forEach(t),Jmo=r(VFe," \u2014 "),jS=n(VFe,"A",{href:!0});var wjr=s(jS);Ymo=r(wjr,"BertGenerationDecoder"),wjr.forEach(t),Kmo=r(VFe," (Bert Generation model)"),VFe.forEach(t),Zmo=i(j),V_=n(j,"LI",{});var WFe=s(V_);mZ=n(WFe,"STRONG",{});var Ajr=s(mZ);ego=r(Ajr,"big_bird"),Ajr.forEach(t),ogo=r(WFe," \u2014 "),NS=n(WFe,"A",{href:!0});var Ljr=s(NS);rgo=r(Ljr,"BigBirdForCausalLM"),Ljr.forEach(t),tgo=r(WFe," (BigBird model)"),WFe.forEach(t),ago=i(j),W_=n(j,"LI",{});var QFe=s(W_);gZ=n(QFe,"STRONG",{});var Bjr=s(gZ);ngo=r(Bjr,"bigbird_pegasus"),Bjr.forEach(t),sgo=r(QFe," \u2014 "),DS=n(QFe,"A",{href:!0});var kjr=s(DS);lgo=r(kjr,"BigBirdPegasusForCausalLM"),kjr.forEach(t),igo=r(QFe," (BigBirdPegasus model)"),QFe.forEach(t),dgo=i(j),Q_=n(j,"LI",{});var HFe=s(Q_);hZ=n(HFe,"STRONG",{});var xjr=s(hZ);cgo=r(xjr,"blenderbot"),xjr.forEach(t),fgo=r(HFe," \u2014 "),qS=n(HFe,"A",{href:!0});var Rjr=s(qS);mgo=r(Rjr,"BlenderbotForCausalLM"),Rjr.forEach(t),ggo=r(HFe," (Blenderbot model)"),HFe.forEach(t),hgo=i(j),H_=n(j,"LI",{});var UFe=s(H_);pZ=n(UFe,"STRONG",{});var Sjr=s(pZ);pgo=r(Sjr,"blenderbot-small"),Sjr.forEach(t),_go=r(UFe," \u2014 "),GS=n(UFe,"A",{href:!0});var Pjr=s(GS);ugo=r(Pjr,"BlenderbotSmallForCausalLM"),Pjr.forEach(t),bgo=r(UFe," (BlenderbotSmall model)"),UFe.forEach(t),vgo=i(j),U_=n(j,"LI",{});var JFe=s(U_);_Z=n(JFe,"STRONG",{});var $jr=s(_Z);Tgo=r($jr,"camembert"),$jr.forEach(t),Fgo=r(JFe," \u2014 "),OS=n(JFe,"A",{href:!0});var Ijr=s(OS);Cgo=r(Ijr,"CamembertForCausalLM"),Ijr.forEach(t),Mgo=r(JFe," (CamemBERT model)"),JFe.forEach(t),Ego=i(j),J_=n(j,"LI",{});var YFe=s(J_);uZ=n(YFe,"STRONG",{});var jjr=s(uZ);ygo=r(jjr,"ctrl"),jjr.forEach(t),wgo=r(YFe," \u2014 "),XS=n(YFe,"A",{href:!0});var Njr=s(XS);Ago=r(Njr,"CTRLLMHeadModel"),Njr.forEach(t),Lgo=r(YFe," (CTRL model)"),YFe.forEach(t),Bgo=i(j),Y_=n(j,"LI",{});var KFe=s(Y_);bZ=n(KFe,"STRONG",{});var Djr=s(bZ);kgo=r(Djr,"electra"),Djr.forEach(t),xgo=r(KFe," \u2014 "),zS=n(KFe,"A",{href:!0});var qjr=s(zS);Rgo=r(qjr,"ElectraForCausalLM"),qjr.forEach(t),Sgo=r(KFe," (ELECTRA model)"),KFe.forEach(t),Pgo=i(j),K_=n(j,"LI",{});var ZFe=s(K_);vZ=n(ZFe,"STRONG",{});var Gjr=s(vZ);$go=r(Gjr,"gpt2"),Gjr.forEach(t),Igo=r(ZFe," \u2014 "),VS=n(ZFe,"A",{href:!0});var Ojr=s(VS);jgo=r(Ojr,"GPT2LMHeadModel"),Ojr.forEach(t),Ngo=r(ZFe," (OpenAI GPT-2 model)"),ZFe.forEach(t),Dgo=i(j),Z_=n(j,"LI",{});var eCe=s(Z_);TZ=n(eCe,"STRONG",{});var Xjr=s(TZ);qgo=r(Xjr,"gpt_neo"),Xjr.forEach(t),Ggo=r(eCe," \u2014 "),WS=n(eCe,"A",{href:!0});var zjr=s(WS);Ogo=r(zjr,"GPTNeoForCausalLM"),zjr.forEach(t),Xgo=r(eCe," (GPT Neo model)"),eCe.forEach(t),zgo=i(j),eu=n(j,"LI",{});var oCe=s(eu);FZ=n(oCe,"STRONG",{});var Vjr=s(FZ);Vgo=r(Vjr,"gptj"),Vjr.forEach(t),Wgo=r(oCe," \u2014 "),QS=n(oCe,"A",{href:!0});var Wjr=s(QS);Qgo=r(Wjr,"GPTJForCausalLM"),Wjr.forEach(t),Hgo=r(oCe," (GPT-J model)"),oCe.forEach(t),Ugo=i(j),ou=n(j,"LI",{});var rCe=s(ou);CZ=n(rCe,"STRONG",{});var Qjr=s(CZ);Jgo=r(Qjr,"marian"),Qjr.forEach(t),Ygo=r(rCe," \u2014 "),HS=n(rCe,"A",{href:!0});var Hjr=s(HS);Kgo=r(Hjr,"MarianForCausalLM"),Hjr.forEach(t),Zgo=r(rCe," (Marian model)"),rCe.forEach(t),eho=i(j),ru=n(j,"LI",{});var tCe=s(ru);MZ=n(tCe,"STRONG",{});var Ujr=s(MZ);oho=r(Ujr,"mbart"),Ujr.forEach(t),rho=r(tCe," \u2014 "),US=n(tCe,"A",{href:!0});var Jjr=s(US);tho=r(Jjr,"MBartForCausalLM"),Jjr.forEach(t),aho=r(tCe," (mBART model)"),tCe.forEach(t),nho=i(j),tu=n(j,"LI",{});var aCe=s(tu);EZ=n(aCe,"STRONG",{});var Yjr=s(EZ);sho=r(Yjr,"megatron-bert"),Yjr.forEach(t),lho=r(aCe," \u2014 "),JS=n(aCe,"A",{href:!0});var Kjr=s(JS);iho=r(Kjr,"MegatronBertForCausalLM"),Kjr.forEach(t),dho=r(aCe," (MegatronBert model)"),aCe.forEach(t),cho=i(j),au=n(j,"LI",{});var nCe=s(au);yZ=n(nCe,"STRONG",{});var Zjr=s(yZ);fho=r(Zjr,"openai-gpt"),Zjr.forEach(t),mho=r(nCe," \u2014 "),YS=n(nCe,"A",{href:!0});var eNr=s(YS);gho=r(eNr,"OpenAIGPTLMHeadModel"),eNr.forEach(t),hho=r(nCe," (OpenAI GPT model)"),nCe.forEach(t),pho=i(j),nu=n(j,"LI",{});var sCe=s(nu);wZ=n(sCe,"STRONG",{});var oNr=s(wZ);_ho=r(oNr,"pegasus"),oNr.forEach(t),uho=r(sCe," \u2014 "),KS=n(sCe,"A",{href:!0});var rNr=s(KS);bho=r(rNr,"PegasusForCausalLM"),rNr.forEach(t),vho=r(sCe," (Pegasus model)"),sCe.forEach(t),Tho=i(j),su=n(j,"LI",{});var lCe=s(su);AZ=n(lCe,"STRONG",{});var tNr=s(AZ);Fho=r(tNr,"plbart"),tNr.forEach(t),Cho=r(lCe," \u2014 "),ZS=n(lCe,"A",{href:!0});var aNr=s(ZS);Mho=r(aNr,"PLBartForCausalLM"),aNr.forEach(t),Eho=r(lCe," (PLBart model)"),lCe.forEach(t),yho=i(j),lu=n(j,"LI",{});var iCe=s(lu);LZ=n(iCe,"STRONG",{});var nNr=s(LZ);who=r(nNr,"prophetnet"),nNr.forEach(t),Aho=r(iCe," \u2014 "),eP=n(iCe,"A",{href:!0});var sNr=s(eP);Lho=r(sNr,"ProphetNetForCausalLM"),sNr.forEach(t),Bho=r(iCe," (ProphetNet model)"),iCe.forEach(t),kho=i(j),iu=n(j,"LI",{});var dCe=s(iu);BZ=n(dCe,"STRONG",{});var lNr=s(BZ);xho=r(lNr,"qdqbert"),lNr.forEach(t),Rho=r(dCe," \u2014 "),oP=n(dCe,"A",{href:!0});var iNr=s(oP);Sho=r(iNr,"QDQBertLMHeadModel"),iNr.forEach(t),Pho=r(dCe," (QDQBert model)"),dCe.forEach(t),$ho=i(j),du=n(j,"LI",{});var cCe=s(du);kZ=n(cCe,"STRONG",{});var dNr=s(kZ);Iho=r(dNr,"reformer"),dNr.forEach(t),jho=r(cCe," \u2014 "),rP=n(cCe,"A",{href:!0});var cNr=s(rP);Nho=r(cNr,"ReformerModelWithLMHead"),cNr.forEach(t),Dho=r(cCe," (Reformer model)"),cCe.forEach(t),qho=i(j),cu=n(j,"LI",{});var fCe=s(cu);xZ=n(fCe,"STRONG",{});var fNr=s(xZ);Gho=r(fNr,"rembert"),fNr.forEach(t),Oho=r(fCe," \u2014 "),tP=n(fCe,"A",{href:!0});var mNr=s(tP);Xho=r(mNr,"RemBertForCausalLM"),mNr.forEach(t),zho=r(fCe," (RemBERT model)"),fCe.forEach(t),Vho=i(j),fu=n(j,"LI",{});var mCe=s(fu);RZ=n(mCe,"STRONG",{});var gNr=s(RZ);Who=r(gNr,"roberta"),gNr.forEach(t),Qho=r(mCe," \u2014 "),aP=n(mCe,"A",{href:!0});var hNr=s(aP);Hho=r(hNr,"RobertaForCausalLM"),hNr.forEach(t),Uho=r(mCe," (RoBERTa model)"),mCe.forEach(t),Jho=i(j),mu=n(j,"LI",{});var gCe=s(mu);SZ=n(gCe,"STRONG",{});var pNr=s(SZ);Yho=r(pNr,"roformer"),pNr.forEach(t),Kho=r(gCe," \u2014 "),nP=n(gCe,"A",{href:!0});var _Nr=s(nP);Zho=r(_Nr,"RoFormerForCausalLM"),_Nr.forEach(t),epo=r(gCe," (RoFormer model)"),gCe.forEach(t),opo=i(j),gu=n(j,"LI",{});var hCe=s(gu);PZ=n(hCe,"STRONG",{});var uNr=s(PZ);rpo=r(uNr,"speech_to_text_2"),uNr.forEach(t),tpo=r(hCe," \u2014 "),sP=n(hCe,"A",{href:!0});var bNr=s(sP);apo=r(bNr,"Speech2Text2ForCausalLM"),bNr.forEach(t),npo=r(hCe," (Speech2Text2 model)"),hCe.forEach(t),spo=i(j),hu=n(j,"LI",{});var pCe=s(hu);$Z=n(pCe,"STRONG",{});var vNr=s($Z);lpo=r(vNr,"transfo-xl"),vNr.forEach(t),ipo=r(pCe," \u2014 "),lP=n(pCe,"A",{href:!0});var TNr=s(lP);dpo=r(TNr,"TransfoXLLMHeadModel"),TNr.forEach(t),cpo=r(pCe," (Transformer-XL model)"),pCe.forEach(t),fpo=i(j),pu=n(j,"LI",{});var _Ce=s(pu);IZ=n(_Ce,"STRONG",{});var FNr=s(IZ);mpo=r(FNr,"trocr"),FNr.forEach(t),gpo=r(_Ce," \u2014 "),iP=n(_Ce,"A",{href:!0});var CNr=s(iP);hpo=r(CNr,"TrOCRForCausalLM"),CNr.forEach(t),ppo=r(_Ce," (TrOCR model)"),_Ce.forEach(t),_po=i(j),_u=n(j,"LI",{});var uCe=s(_u);jZ=n(uCe,"STRONG",{});var MNr=s(jZ);upo=r(MNr,"xglm"),MNr.forEach(t),bpo=r(uCe," \u2014 "),dP=n(uCe,"A",{href:!0});var ENr=s(dP);vpo=r(ENr,"XGLMForCausalLM"),ENr.forEach(t),Tpo=r(uCe," (XGLM model)"),uCe.forEach(t),Fpo=i(j),uu=n(j,"LI",{});var bCe=s(uu);NZ=n(bCe,"STRONG",{});var yNr=s(NZ);Cpo=r(yNr,"xlm"),yNr.forEach(t),Mpo=r(bCe," \u2014 "),cP=n(bCe,"A",{href:!0});var wNr=s(cP);Epo=r(wNr,"XLMWithLMHeadModel"),wNr.forEach(t),ypo=r(bCe," (XLM model)"),bCe.forEach(t),wpo=i(j),bu=n(j,"LI",{});var vCe=s(bu);DZ=n(vCe,"STRONG",{});var ANr=s(DZ);Apo=r(ANr,"xlm-prophetnet"),ANr.forEach(t),Lpo=r(vCe," \u2014 "),fP=n(vCe,"A",{href:!0});var LNr=s(fP);Bpo=r(LNr,"XLMProphetNetForCausalLM"),LNr.forEach(t),kpo=r(vCe," (XLMProphetNet model)"),vCe.forEach(t),xpo=i(j),vu=n(j,"LI",{});var TCe=s(vu);qZ=n(TCe,"STRONG",{});var BNr=s(qZ);Rpo=r(BNr,"xlm-roberta"),BNr.forEach(t),Spo=r(TCe," \u2014 "),mP=n(TCe,"A",{href:!0});var kNr=s(mP);Ppo=r(kNr,"XLMRobertaForCausalLM"),kNr.forEach(t),$po=r(TCe," (XLM-RoBERTa model)"),TCe.forEach(t),Ipo=i(j),Tu=n(j,"LI",{});var FCe=s(Tu);GZ=n(FCe,"STRONG",{});var xNr=s(GZ);jpo=r(xNr,"xlm-roberta-xl"),xNr.forEach(t),Npo=r(FCe," \u2014 "),gP=n(FCe,"A",{href:!0});var RNr=s(gP);Dpo=r(RNr,"XLMRobertaXLForCausalLM"),RNr.forEach(t),qpo=r(FCe," (XLM-RoBERTa-XL model)"),FCe.forEach(t),Gpo=i(j),Fu=n(j,"LI",{});var CCe=s(Fu);OZ=n(CCe,"STRONG",{});var SNr=s(OZ);Opo=r(SNr,"xlnet"),SNr.forEach(t),Xpo=r(CCe," \u2014 "),hP=n(CCe,"A",{href:!0});var PNr=s(hP);zpo=r(PNr,"XLNetLMHeadModel"),PNr.forEach(t),Vpo=r(CCe," (XLNet model)"),CCe.forEach(t),j.forEach(t),Wpo=i($t),Cu=n($t,"P",{});var MCe=s(Cu);Qpo=r(MCe,"The model is set in evaluation mode by default using "),XZ=n(MCe,"CODE",{});var $Nr=s(XZ);Hpo=r($Nr,"model.eval()"),$Nr.forEach(t),Upo=r(MCe,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),zZ=n(MCe,"CODE",{});var INr=s(zZ);Jpo=r(INr,"model.train()"),INr.forEach(t),MCe.forEach(t),Ypo=i($t),VZ=n($t,"P",{});var jNr=s(VZ);Kpo=r(jNr,"Examples:"),jNr.forEach(t),Zpo=i($t),m(nE.$$.fragment,$t),$t.forEach(t),Os.forEach(t),N8e=i(d),Ui=n(d,"H2",{class:!0});var VBe=s(Ui);Mu=n(VBe,"A",{id:!0,class:!0,href:!0});var NNr=s(Mu);WZ=n(NNr,"SPAN",{});var DNr=s(WZ);m(sE.$$.fragment,DNr),DNr.forEach(t),NNr.forEach(t),e_o=i(VBe),QZ=n(VBe,"SPAN",{});var qNr=s(QZ);o_o=r(qNr,"AutoModelForMaskedLM"),qNr.forEach(t),VBe.forEach(t),D8e=i(d),Ho=n(d,"DIV",{class:!0});var zs=s(Ho);m(lE.$$.fragment,zs),r_o=i(zs),Ji=n(zs,"P",{});var sz=s(Ji);t_o=r(sz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a masked language modeling head) when created
with the `),HZ=n(sz,"CODE",{});var GNr=s(HZ);a_o=r(GNr,"from_pretrained()"),GNr.forEach(t),n_o=r(sz,"class method or the "),UZ=n(sz,"CODE",{});var ONr=s(UZ);s_o=r(ONr,"from_config()"),ONr.forEach(t),l_o=r(sz,`class
method.`),sz.forEach(t),i_o=i(zs),iE=n(zs,"P",{});var WBe=s(iE);d_o=r(WBe,"This class cannot be instantiated directly using "),JZ=n(WBe,"CODE",{});var XNr=s(JZ);c_o=r(XNr,"__init__()"),XNr.forEach(t),f_o=r(WBe," (throws an error)."),WBe.forEach(t),m_o=i(zs),Gr=n(zs,"DIV",{class:!0});var Vs=s(Gr);m(dE.$$.fragment,Vs),g_o=i(Vs),YZ=n(Vs,"P",{});var zNr=s(YZ);h_o=r(zNr,"Instantiates one of the model classes of the library (with a masked language modeling head) from a configuration."),zNr.forEach(t),p_o=i(Vs),Yi=n(Vs,"P",{});var lz=s(Yi);__o=r(lz,`Note:
Loading a model from its configuration file does `),KZ=n(lz,"STRONG",{});var VNr=s(KZ);u_o=r(VNr,"not"),VNr.forEach(t),b_o=r(lz,` load the model weights. It only affects the
model\u2019s configuration. Use `),ZZ=n(lz,"CODE",{});var WNr=s(ZZ);v_o=r(WNr,"from_pretrained()"),WNr.forEach(t),T_o=r(lz,"to load the model weights."),lz.forEach(t),F_o=i(Vs),eee=n(Vs,"P",{});var QNr=s(eee);C_o=r(QNr,"Examples:"),QNr.forEach(t),M_o=i(Vs),m(cE.$$.fragment,Vs),Vs.forEach(t),E_o=i(zs),Pe=n(zs,"DIV",{class:!0});var It=s(Pe);m(fE.$$.fragment,It),y_o=i(It),oee=n(It,"P",{});var HNr=s(oee);w_o=r(HNr,"Instantiate one of the model classes of the library (with a masked language modeling head) from a pretrained model."),HNr.forEach(t),A_o=i(It),Oa=n(It,"P",{});var h4=s(Oa);L_o=r(h4,"The model class to instantiate is selected based on the "),ree=n(h4,"CODE",{});var UNr=s(ree);B_o=r(UNr,"model_type"),UNr.forEach(t),k_o=r(h4,` property of the config object (either
passed as an argument or loaded from `),tee=n(h4,"CODE",{});var JNr=s(tee);x_o=r(JNr,"pretrained_model_name_or_path"),JNr.forEach(t),R_o=r(h4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),aee=n(h4,"CODE",{});var YNr=s(aee);S_o=r(YNr,"pretrained_model_name_or_path"),YNr.forEach(t),P_o=r(h4,":"),h4.forEach(t),$_o=i(It),I=n(It,"UL",{});var N=s(I);Eu=n(N,"LI",{});var ECe=s(Eu);nee=n(ECe,"STRONG",{});var KNr=s(nee);I_o=r(KNr,"albert"),KNr.forEach(t),j_o=r(ECe," \u2014 "),pP=n(ECe,"A",{href:!0});var ZNr=s(pP);N_o=r(ZNr,"AlbertForMaskedLM"),ZNr.forEach(t),D_o=r(ECe," (ALBERT model)"),ECe.forEach(t),q_o=i(N),yu=n(N,"LI",{});var yCe=s(yu);see=n(yCe,"STRONG",{});var eDr=s(see);G_o=r(eDr,"bart"),eDr.forEach(t),O_o=r(yCe," \u2014 "),_P=n(yCe,"A",{href:!0});var oDr=s(_P);X_o=r(oDr,"BartForConditionalGeneration"),oDr.forEach(t),z_o=r(yCe," (BART model)"),yCe.forEach(t),V_o=i(N),wu=n(N,"LI",{});var wCe=s(wu);lee=n(wCe,"STRONG",{});var rDr=s(lee);W_o=r(rDr,"bert"),rDr.forEach(t),Q_o=r(wCe," \u2014 "),uP=n(wCe,"A",{href:!0});var tDr=s(uP);H_o=r(tDr,"BertForMaskedLM"),tDr.forEach(t),U_o=r(wCe," (BERT model)"),wCe.forEach(t),J_o=i(N),Au=n(N,"LI",{});var ACe=s(Au);iee=n(ACe,"STRONG",{});var aDr=s(iee);Y_o=r(aDr,"big_bird"),aDr.forEach(t),K_o=r(ACe," \u2014 "),bP=n(ACe,"A",{href:!0});var nDr=s(bP);Z_o=r(nDr,"BigBirdForMaskedLM"),nDr.forEach(t),euo=r(ACe," (BigBird model)"),ACe.forEach(t),ouo=i(N),Lu=n(N,"LI",{});var LCe=s(Lu);dee=n(LCe,"STRONG",{});var sDr=s(dee);ruo=r(sDr,"camembert"),sDr.forEach(t),tuo=r(LCe," \u2014 "),vP=n(LCe,"A",{href:!0});var lDr=s(vP);auo=r(lDr,"CamembertForMaskedLM"),lDr.forEach(t),nuo=r(LCe," (CamemBERT model)"),LCe.forEach(t),suo=i(N),Bu=n(N,"LI",{});var BCe=s(Bu);cee=n(BCe,"STRONG",{});var iDr=s(cee);luo=r(iDr,"convbert"),iDr.forEach(t),iuo=r(BCe," \u2014 "),TP=n(BCe,"A",{href:!0});var dDr=s(TP);duo=r(dDr,"ConvBertForMaskedLM"),dDr.forEach(t),cuo=r(BCe," (ConvBERT model)"),BCe.forEach(t),fuo=i(N),ku=n(N,"LI",{});var kCe=s(ku);fee=n(kCe,"STRONG",{});var cDr=s(fee);muo=r(cDr,"deberta"),cDr.forEach(t),guo=r(kCe," \u2014 "),FP=n(kCe,"A",{href:!0});var fDr=s(FP);huo=r(fDr,"DebertaForMaskedLM"),fDr.forEach(t),puo=r(kCe," (DeBERTa model)"),kCe.forEach(t),_uo=i(N),xu=n(N,"LI",{});var xCe=s(xu);mee=n(xCe,"STRONG",{});var mDr=s(mee);uuo=r(mDr,"deberta-v2"),mDr.forEach(t),buo=r(xCe," \u2014 "),CP=n(xCe,"A",{href:!0});var gDr=s(CP);vuo=r(gDr,"DebertaV2ForMaskedLM"),gDr.forEach(t),Tuo=r(xCe," (DeBERTa-v2 model)"),xCe.forEach(t),Fuo=i(N),Ru=n(N,"LI",{});var RCe=s(Ru);gee=n(RCe,"STRONG",{});var hDr=s(gee);Cuo=r(hDr,"distilbert"),hDr.forEach(t),Muo=r(RCe," \u2014 "),MP=n(RCe,"A",{href:!0});var pDr=s(MP);Euo=r(pDr,"DistilBertForMaskedLM"),pDr.forEach(t),yuo=r(RCe," (DistilBERT model)"),RCe.forEach(t),wuo=i(N),Su=n(N,"LI",{});var SCe=s(Su);hee=n(SCe,"STRONG",{});var _Dr=s(hee);Auo=r(_Dr,"electra"),_Dr.forEach(t),Luo=r(SCe," \u2014 "),EP=n(SCe,"A",{href:!0});var uDr=s(EP);Buo=r(uDr,"ElectraForMaskedLM"),uDr.forEach(t),kuo=r(SCe," (ELECTRA model)"),SCe.forEach(t),xuo=i(N),Pu=n(N,"LI",{});var PCe=s(Pu);pee=n(PCe,"STRONG",{});var bDr=s(pee);Ruo=r(bDr,"flaubert"),bDr.forEach(t),Suo=r(PCe," \u2014 "),yP=n(PCe,"A",{href:!0});var vDr=s(yP);Puo=r(vDr,"FlaubertWithLMHeadModel"),vDr.forEach(t),$uo=r(PCe," (FlauBERT model)"),PCe.forEach(t),Iuo=i(N),$u=n(N,"LI",{});var $Ce=s($u);_ee=n($Ce,"STRONG",{});var TDr=s(_ee);juo=r(TDr,"fnet"),TDr.forEach(t),Nuo=r($Ce," \u2014 "),wP=n($Ce,"A",{href:!0});var FDr=s(wP);Duo=r(FDr,"FNetForMaskedLM"),FDr.forEach(t),quo=r($Ce," (FNet model)"),$Ce.forEach(t),Guo=i(N),Iu=n(N,"LI",{});var ICe=s(Iu);uee=n(ICe,"STRONG",{});var CDr=s(uee);Ouo=r(CDr,"funnel"),CDr.forEach(t),Xuo=r(ICe," \u2014 "),AP=n(ICe,"A",{href:!0});var MDr=s(AP);zuo=r(MDr,"FunnelForMaskedLM"),MDr.forEach(t),Vuo=r(ICe," (Funnel Transformer model)"),ICe.forEach(t),Wuo=i(N),ju=n(N,"LI",{});var jCe=s(ju);bee=n(jCe,"STRONG",{});var EDr=s(bee);Quo=r(EDr,"ibert"),EDr.forEach(t),Huo=r(jCe," \u2014 "),LP=n(jCe,"A",{href:!0});var yDr=s(LP);Uuo=r(yDr,"IBertForMaskedLM"),yDr.forEach(t),Juo=r(jCe," (I-BERT model)"),jCe.forEach(t),Yuo=i(N),Nu=n(N,"LI",{});var NCe=s(Nu);vee=n(NCe,"STRONG",{});var wDr=s(vee);Kuo=r(wDr,"layoutlm"),wDr.forEach(t),Zuo=r(NCe," \u2014 "),BP=n(NCe,"A",{href:!0});var ADr=s(BP);e1o=r(ADr,"LayoutLMForMaskedLM"),ADr.forEach(t),o1o=r(NCe," (LayoutLM model)"),NCe.forEach(t),r1o=i(N),Du=n(N,"LI",{});var DCe=s(Du);Tee=n(DCe,"STRONG",{});var LDr=s(Tee);t1o=r(LDr,"longformer"),LDr.forEach(t),a1o=r(DCe," \u2014 "),kP=n(DCe,"A",{href:!0});var BDr=s(kP);n1o=r(BDr,"LongformerForMaskedLM"),BDr.forEach(t),s1o=r(DCe," (Longformer model)"),DCe.forEach(t),l1o=i(N),qu=n(N,"LI",{});var qCe=s(qu);Fee=n(qCe,"STRONG",{});var kDr=s(Fee);i1o=r(kDr,"mbart"),kDr.forEach(t),d1o=r(qCe," \u2014 "),xP=n(qCe,"A",{href:!0});var xDr=s(xP);c1o=r(xDr,"MBartForConditionalGeneration"),xDr.forEach(t),f1o=r(qCe," (mBART model)"),qCe.forEach(t),m1o=i(N),Gu=n(N,"LI",{});var GCe=s(Gu);Cee=n(GCe,"STRONG",{});var RDr=s(Cee);g1o=r(RDr,"megatron-bert"),RDr.forEach(t),h1o=r(GCe," \u2014 "),RP=n(GCe,"A",{href:!0});var SDr=s(RP);p1o=r(SDr,"MegatronBertForMaskedLM"),SDr.forEach(t),_1o=r(GCe," (MegatronBert model)"),GCe.forEach(t),u1o=i(N),Ou=n(N,"LI",{});var OCe=s(Ou);Mee=n(OCe,"STRONG",{});var PDr=s(Mee);b1o=r(PDr,"mobilebert"),PDr.forEach(t),v1o=r(OCe," \u2014 "),SP=n(OCe,"A",{href:!0});var $Dr=s(SP);T1o=r($Dr,"MobileBertForMaskedLM"),$Dr.forEach(t),F1o=r(OCe," (MobileBERT model)"),OCe.forEach(t),C1o=i(N),Xu=n(N,"LI",{});var XCe=s(Xu);Eee=n(XCe,"STRONG",{});var IDr=s(Eee);M1o=r(IDr,"mpnet"),IDr.forEach(t),E1o=r(XCe," \u2014 "),PP=n(XCe,"A",{href:!0});var jDr=s(PP);y1o=r(jDr,"MPNetForMaskedLM"),jDr.forEach(t),w1o=r(XCe," (MPNet model)"),XCe.forEach(t),A1o=i(N),zu=n(N,"LI",{});var zCe=s(zu);yee=n(zCe,"STRONG",{});var NDr=s(yee);L1o=r(NDr,"nystromformer"),NDr.forEach(t),B1o=r(zCe," \u2014 "),$P=n(zCe,"A",{href:!0});var DDr=s($P);k1o=r(DDr,"NystromformerForMaskedLM"),DDr.forEach(t),x1o=r(zCe," (Nystromformer model)"),zCe.forEach(t),R1o=i(N),Vu=n(N,"LI",{});var VCe=s(Vu);wee=n(VCe,"STRONG",{});var qDr=s(wee);S1o=r(qDr,"perceiver"),qDr.forEach(t),P1o=r(VCe," \u2014 "),IP=n(VCe,"A",{href:!0});var GDr=s(IP);$1o=r(GDr,"PerceiverForMaskedLM"),GDr.forEach(t),I1o=r(VCe," (Perceiver model)"),VCe.forEach(t),j1o=i(N),Wu=n(N,"LI",{});var WCe=s(Wu);Aee=n(WCe,"STRONG",{});var ODr=s(Aee);N1o=r(ODr,"qdqbert"),ODr.forEach(t),D1o=r(WCe," \u2014 "),jP=n(WCe,"A",{href:!0});var XDr=s(jP);q1o=r(XDr,"QDQBertForMaskedLM"),XDr.forEach(t),G1o=r(WCe," (QDQBert model)"),WCe.forEach(t),O1o=i(N),Qu=n(N,"LI",{});var QCe=s(Qu);Lee=n(QCe,"STRONG",{});var zDr=s(Lee);X1o=r(zDr,"reformer"),zDr.forEach(t),z1o=r(QCe," \u2014 "),NP=n(QCe,"A",{href:!0});var VDr=s(NP);V1o=r(VDr,"ReformerForMaskedLM"),VDr.forEach(t),W1o=r(QCe," (Reformer model)"),QCe.forEach(t),Q1o=i(N),Hu=n(N,"LI",{});var HCe=s(Hu);Bee=n(HCe,"STRONG",{});var WDr=s(Bee);H1o=r(WDr,"rembert"),WDr.forEach(t),U1o=r(HCe," \u2014 "),DP=n(HCe,"A",{href:!0});var QDr=s(DP);J1o=r(QDr,"RemBertForMaskedLM"),QDr.forEach(t),Y1o=r(HCe," (RemBERT model)"),HCe.forEach(t),K1o=i(N),Uu=n(N,"LI",{});var UCe=s(Uu);kee=n(UCe,"STRONG",{});var HDr=s(kee);Z1o=r(HDr,"roberta"),HDr.forEach(t),e7o=r(UCe," \u2014 "),qP=n(UCe,"A",{href:!0});var UDr=s(qP);o7o=r(UDr,"RobertaForMaskedLM"),UDr.forEach(t),r7o=r(UCe," (RoBERTa model)"),UCe.forEach(t),t7o=i(N),Ju=n(N,"LI",{});var JCe=s(Ju);xee=n(JCe,"STRONG",{});var JDr=s(xee);a7o=r(JDr,"roformer"),JDr.forEach(t),n7o=r(JCe," \u2014 "),GP=n(JCe,"A",{href:!0});var YDr=s(GP);s7o=r(YDr,"RoFormerForMaskedLM"),YDr.forEach(t),l7o=r(JCe," (RoFormer model)"),JCe.forEach(t),i7o=i(N),Yu=n(N,"LI",{});var YCe=s(Yu);Ree=n(YCe,"STRONG",{});var KDr=s(Ree);d7o=r(KDr,"squeezebert"),KDr.forEach(t),c7o=r(YCe," \u2014 "),OP=n(YCe,"A",{href:!0});var ZDr=s(OP);f7o=r(ZDr,"SqueezeBertForMaskedLM"),ZDr.forEach(t),m7o=r(YCe," (SqueezeBERT model)"),YCe.forEach(t),g7o=i(N),Ku=n(N,"LI",{});var KCe=s(Ku);See=n(KCe,"STRONG",{});var eqr=s(See);h7o=r(eqr,"tapas"),eqr.forEach(t),p7o=r(KCe," \u2014 "),XP=n(KCe,"A",{href:!0});var oqr=s(XP);_7o=r(oqr,"TapasForMaskedLM"),oqr.forEach(t),u7o=r(KCe," (TAPAS model)"),KCe.forEach(t),b7o=i(N),Zu=n(N,"LI",{});var ZCe=s(Zu);Pee=n(ZCe,"STRONG",{});var rqr=s(Pee);v7o=r(rqr,"wav2vec2"),rqr.forEach(t),T7o=r(ZCe," \u2014 "),$ee=n(ZCe,"CODE",{});var tqr=s($ee);F7o=r(tqr,"Wav2Vec2ForMaskedLM"),tqr.forEach(t),C7o=r(ZCe,"(Wav2Vec2 model)"),ZCe.forEach(t),M7o=i(N),e1=n(N,"LI",{});var e4e=s(e1);Iee=n(e4e,"STRONG",{});var aqr=s(Iee);E7o=r(aqr,"xlm"),aqr.forEach(t),y7o=r(e4e," \u2014 "),zP=n(e4e,"A",{href:!0});var nqr=s(zP);w7o=r(nqr,"XLMWithLMHeadModel"),nqr.forEach(t),A7o=r(e4e," (XLM model)"),e4e.forEach(t),L7o=i(N),o1=n(N,"LI",{});var o4e=s(o1);jee=n(o4e,"STRONG",{});var sqr=s(jee);B7o=r(sqr,"xlm-roberta"),sqr.forEach(t),k7o=r(o4e," \u2014 "),VP=n(o4e,"A",{href:!0});var lqr=s(VP);x7o=r(lqr,"XLMRobertaForMaskedLM"),lqr.forEach(t),R7o=r(o4e," (XLM-RoBERTa model)"),o4e.forEach(t),S7o=i(N),r1=n(N,"LI",{});var r4e=s(r1);Nee=n(r4e,"STRONG",{});var iqr=s(Nee);P7o=r(iqr,"xlm-roberta-xl"),iqr.forEach(t),$7o=r(r4e," \u2014 "),WP=n(r4e,"A",{href:!0});var dqr=s(WP);I7o=r(dqr,"XLMRobertaXLForMaskedLM"),dqr.forEach(t),j7o=r(r4e," (XLM-RoBERTa-XL model)"),r4e.forEach(t),N7o=i(N),t1=n(N,"LI",{});var t4e=s(t1);Dee=n(t4e,"STRONG",{});var cqr=s(Dee);D7o=r(cqr,"yoso"),cqr.forEach(t),q7o=r(t4e," \u2014 "),QP=n(t4e,"A",{href:!0});var fqr=s(QP);G7o=r(fqr,"YosoForMaskedLM"),fqr.forEach(t),O7o=r(t4e," (YOSO model)"),t4e.forEach(t),N.forEach(t),X7o=i(It),a1=n(It,"P",{});var a4e=s(a1);z7o=r(a4e,"The model is set in evaluation mode by default using "),qee=n(a4e,"CODE",{});var mqr=s(qee);V7o=r(mqr,"model.eval()"),mqr.forEach(t),W7o=r(a4e,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Gee=n(a4e,"CODE",{});var gqr=s(Gee);Q7o=r(gqr,"model.train()"),gqr.forEach(t),a4e.forEach(t),H7o=i(It),Oee=n(It,"P",{});var hqr=s(Oee);U7o=r(hqr,"Examples:"),hqr.forEach(t),J7o=i(It),m(mE.$$.fragment,It),It.forEach(t),zs.forEach(t),q8e=i(d),Ki=n(d,"H2",{class:!0});var QBe=s(Ki);n1=n(QBe,"A",{id:!0,class:!0,href:!0});var pqr=s(n1);Xee=n(pqr,"SPAN",{});var _qr=s(Xee);m(gE.$$.fragment,_qr),_qr.forEach(t),pqr.forEach(t),Y7o=i(QBe),zee=n(QBe,"SPAN",{});var uqr=s(zee);K7o=r(uqr,"AutoModelForSeq2SeqLM"),uqr.forEach(t),QBe.forEach(t),G8e=i(d),Uo=n(d,"DIV",{class:!0});var Ws=s(Uo);m(hE.$$.fragment,Ws),Z7o=i(Ws),Zi=n(Ws,"P",{});var iz=s(Zi);ebo=r(iz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence language modeling head) when created
with the `),Vee=n(iz,"CODE",{});var bqr=s(Vee);obo=r(bqr,"from_pretrained()"),bqr.forEach(t),rbo=r(iz,"class method or the "),Wee=n(iz,"CODE",{});var vqr=s(Wee);tbo=r(vqr,"from_config()"),vqr.forEach(t),abo=r(iz,`class
method.`),iz.forEach(t),nbo=i(Ws),pE=n(Ws,"P",{});var HBe=s(pE);sbo=r(HBe,"This class cannot be instantiated directly using "),Qee=n(HBe,"CODE",{});var Tqr=s(Qee);lbo=r(Tqr,"__init__()"),Tqr.forEach(t),ibo=r(HBe," (throws an error)."),HBe.forEach(t),dbo=i(Ws),Or=n(Ws,"DIV",{class:!0});var Qs=s(Or);m(_E.$$.fragment,Qs),cbo=i(Qs),Hee=n(Qs,"P",{});var Fqr=s(Hee);fbo=r(Fqr,"Instantiates one of the model classes of the library (with a sequence-to-sequence language modeling head) from a configuration."),Fqr.forEach(t),mbo=i(Qs),ed=n(Qs,"P",{});var dz=s(ed);gbo=r(dz,`Note:
Loading a model from its configuration file does `),Uee=n(dz,"STRONG",{});var Cqr=s(Uee);hbo=r(Cqr,"not"),Cqr.forEach(t),pbo=r(dz,` load the model weights. It only affects the
model\u2019s configuration. Use `),Jee=n(dz,"CODE",{});var Mqr=s(Jee);_bo=r(Mqr,"from_pretrained()"),Mqr.forEach(t),ubo=r(dz,"to load the model weights."),dz.forEach(t),bbo=i(Qs),Yee=n(Qs,"P",{});var Eqr=s(Yee);vbo=r(Eqr,"Examples:"),Eqr.forEach(t),Tbo=i(Qs),m(uE.$$.fragment,Qs),Qs.forEach(t),Fbo=i(Ws),$e=n(Ws,"DIV",{class:!0});var jt=s($e);m(bE.$$.fragment,jt),Cbo=i(jt),Kee=n(jt,"P",{});var yqr=s(Kee);Mbo=r(yqr,"Instantiate one of the model classes of the library (with a sequence-to-sequence language modeling head) from a pretrained model."),yqr.forEach(t),Ebo=i(jt),Xa=n(jt,"P",{});var p4=s(Xa);ybo=r(p4,"The model class to instantiate is selected based on the "),Zee=n(p4,"CODE",{});var wqr=s(Zee);wbo=r(wqr,"model_type"),wqr.forEach(t),Abo=r(p4,` property of the config object (either
passed as an argument or loaded from `),eoe=n(p4,"CODE",{});var Aqr=s(eoe);Lbo=r(Aqr,"pretrained_model_name_or_path"),Aqr.forEach(t),Bbo=r(p4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),ooe=n(p4,"CODE",{});var Lqr=s(ooe);kbo=r(Lqr,"pretrained_model_name_or_path"),Lqr.forEach(t),xbo=r(p4,":"),p4.forEach(t),Rbo=i(jt),ne=n(jt,"UL",{});var ie=s(ne);s1=n(ie,"LI",{});var n4e=s(s1);roe=n(n4e,"STRONG",{});var Bqr=s(roe);Sbo=r(Bqr,"bart"),Bqr.forEach(t),Pbo=r(n4e," \u2014 "),HP=n(n4e,"A",{href:!0});var kqr=s(HP);$bo=r(kqr,"BartForConditionalGeneration"),kqr.forEach(t),Ibo=r(n4e," (BART model)"),n4e.forEach(t),jbo=i(ie),l1=n(ie,"LI",{});var s4e=s(l1);toe=n(s4e,"STRONG",{});var xqr=s(toe);Nbo=r(xqr,"bigbird_pegasus"),xqr.forEach(t),Dbo=r(s4e," \u2014 "),UP=n(s4e,"A",{href:!0});var Rqr=s(UP);qbo=r(Rqr,"BigBirdPegasusForConditionalGeneration"),Rqr.forEach(t),Gbo=r(s4e," (BigBirdPegasus model)"),s4e.forEach(t),Obo=i(ie),i1=n(ie,"LI",{});var l4e=s(i1);aoe=n(l4e,"STRONG",{});var Sqr=s(aoe);Xbo=r(Sqr,"blenderbot"),Sqr.forEach(t),zbo=r(l4e," \u2014 "),JP=n(l4e,"A",{href:!0});var Pqr=s(JP);Vbo=r(Pqr,"BlenderbotForConditionalGeneration"),Pqr.forEach(t),Wbo=r(l4e," (Blenderbot model)"),l4e.forEach(t),Qbo=i(ie),d1=n(ie,"LI",{});var i4e=s(d1);noe=n(i4e,"STRONG",{});var $qr=s(noe);Hbo=r($qr,"blenderbot-small"),$qr.forEach(t),Ubo=r(i4e," \u2014 "),YP=n(i4e,"A",{href:!0});var Iqr=s(YP);Jbo=r(Iqr,"BlenderbotSmallForConditionalGeneration"),Iqr.forEach(t),Ybo=r(i4e," (BlenderbotSmall model)"),i4e.forEach(t),Kbo=i(ie),c1=n(ie,"LI",{});var d4e=s(c1);soe=n(d4e,"STRONG",{});var jqr=s(soe);Zbo=r(jqr,"encoder-decoder"),jqr.forEach(t),e5o=r(d4e," \u2014 "),KP=n(d4e,"A",{href:!0});var Nqr=s(KP);o5o=r(Nqr,"EncoderDecoderModel"),Nqr.forEach(t),r5o=r(d4e," (Encoder decoder model)"),d4e.forEach(t),t5o=i(ie),f1=n(ie,"LI",{});var c4e=s(f1);loe=n(c4e,"STRONG",{});var Dqr=s(loe);a5o=r(Dqr,"fsmt"),Dqr.forEach(t),n5o=r(c4e," \u2014 "),ZP=n(c4e,"A",{href:!0});var qqr=s(ZP);s5o=r(qqr,"FSMTForConditionalGeneration"),qqr.forEach(t),l5o=r(c4e," (FairSeq Machine-Translation model)"),c4e.forEach(t),i5o=i(ie),m1=n(ie,"LI",{});var f4e=s(m1);ioe=n(f4e,"STRONG",{});var Gqr=s(ioe);d5o=r(Gqr,"led"),Gqr.forEach(t),c5o=r(f4e," \u2014 "),e$=n(f4e,"A",{href:!0});var Oqr=s(e$);f5o=r(Oqr,"LEDForConditionalGeneration"),Oqr.forEach(t),m5o=r(f4e," (LED model)"),f4e.forEach(t),g5o=i(ie),g1=n(ie,"LI",{});var m4e=s(g1);doe=n(m4e,"STRONG",{});var Xqr=s(doe);h5o=r(Xqr,"m2m_100"),Xqr.forEach(t),p5o=r(m4e," \u2014 "),o$=n(m4e,"A",{href:!0});var zqr=s(o$);_5o=r(zqr,"M2M100ForConditionalGeneration"),zqr.forEach(t),u5o=r(m4e," (M2M100 model)"),m4e.forEach(t),b5o=i(ie),h1=n(ie,"LI",{});var g4e=s(h1);coe=n(g4e,"STRONG",{});var Vqr=s(coe);v5o=r(Vqr,"marian"),Vqr.forEach(t),T5o=r(g4e," \u2014 "),r$=n(g4e,"A",{href:!0});var Wqr=s(r$);F5o=r(Wqr,"MarianMTModel"),Wqr.forEach(t),C5o=r(g4e," (Marian model)"),g4e.forEach(t),M5o=i(ie),p1=n(ie,"LI",{});var h4e=s(p1);foe=n(h4e,"STRONG",{});var Qqr=s(foe);E5o=r(Qqr,"mbart"),Qqr.forEach(t),y5o=r(h4e," \u2014 "),t$=n(h4e,"A",{href:!0});var Hqr=s(t$);w5o=r(Hqr,"MBartForConditionalGeneration"),Hqr.forEach(t),A5o=r(h4e," (mBART model)"),h4e.forEach(t),L5o=i(ie),_1=n(ie,"LI",{});var p4e=s(_1);moe=n(p4e,"STRONG",{});var Uqr=s(moe);B5o=r(Uqr,"mt5"),Uqr.forEach(t),k5o=r(p4e," \u2014 "),a$=n(p4e,"A",{href:!0});var Jqr=s(a$);x5o=r(Jqr,"MT5ForConditionalGeneration"),Jqr.forEach(t),R5o=r(p4e," (mT5 model)"),p4e.forEach(t),S5o=i(ie),u1=n(ie,"LI",{});var _4e=s(u1);goe=n(_4e,"STRONG",{});var Yqr=s(goe);P5o=r(Yqr,"pegasus"),Yqr.forEach(t),$5o=r(_4e," \u2014 "),n$=n(_4e,"A",{href:!0});var Kqr=s(n$);I5o=r(Kqr,"PegasusForConditionalGeneration"),Kqr.forEach(t),j5o=r(_4e," (Pegasus model)"),_4e.forEach(t),N5o=i(ie),b1=n(ie,"LI",{});var u4e=s(b1);hoe=n(u4e,"STRONG",{});var Zqr=s(hoe);D5o=r(Zqr,"plbart"),Zqr.forEach(t),q5o=r(u4e," \u2014 "),s$=n(u4e,"A",{href:!0});var eGr=s(s$);G5o=r(eGr,"PLBartForConditionalGeneration"),eGr.forEach(t),O5o=r(u4e," (PLBart model)"),u4e.forEach(t),X5o=i(ie),v1=n(ie,"LI",{});var b4e=s(v1);poe=n(b4e,"STRONG",{});var oGr=s(poe);z5o=r(oGr,"prophetnet"),oGr.forEach(t),V5o=r(b4e," \u2014 "),l$=n(b4e,"A",{href:!0});var rGr=s(l$);W5o=r(rGr,"ProphetNetForConditionalGeneration"),rGr.forEach(t),Q5o=r(b4e," (ProphetNet model)"),b4e.forEach(t),H5o=i(ie),T1=n(ie,"LI",{});var v4e=s(T1);_oe=n(v4e,"STRONG",{});var tGr=s(_oe);U5o=r(tGr,"t5"),tGr.forEach(t),J5o=r(v4e," \u2014 "),i$=n(v4e,"A",{href:!0});var aGr=s(i$);Y5o=r(aGr,"T5ForConditionalGeneration"),aGr.forEach(t),K5o=r(v4e," (T5 model)"),v4e.forEach(t),Z5o=i(ie),F1=n(ie,"LI",{});var T4e=s(F1);uoe=n(T4e,"STRONG",{});var nGr=s(uoe);e2o=r(nGr,"xlm-prophetnet"),nGr.forEach(t),o2o=r(T4e," \u2014 "),d$=n(T4e,"A",{href:!0});var sGr=s(d$);r2o=r(sGr,"XLMProphetNetForConditionalGeneration"),sGr.forEach(t),t2o=r(T4e," (XLMProphetNet model)"),T4e.forEach(t),ie.forEach(t),a2o=i(jt),C1=n(jt,"P",{});var F4e=s(C1);n2o=r(F4e,"The model is set in evaluation mode by default using "),boe=n(F4e,"CODE",{});var lGr=s(boe);s2o=r(lGr,"model.eval()"),lGr.forEach(t),l2o=r(F4e,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),voe=n(F4e,"CODE",{});var iGr=s(voe);i2o=r(iGr,"model.train()"),iGr.forEach(t),F4e.forEach(t),d2o=i(jt),Toe=n(jt,"P",{});var dGr=s(Toe);c2o=r(dGr,"Examples:"),dGr.forEach(t),f2o=i(jt),m(vE.$$.fragment,jt),jt.forEach(t),Ws.forEach(t),O8e=i(d),od=n(d,"H2",{class:!0});var UBe=s(od);M1=n(UBe,"A",{id:!0,class:!0,href:!0});var cGr=s(M1);Foe=n(cGr,"SPAN",{});var fGr=s(Foe);m(TE.$$.fragment,fGr),fGr.forEach(t),cGr.forEach(t),m2o=i(UBe),Coe=n(UBe,"SPAN",{});var mGr=s(Coe);g2o=r(mGr,"AutoModelForSequenceClassification"),mGr.forEach(t),UBe.forEach(t),X8e=i(d),Jo=n(d,"DIV",{class:!0});var Hs=s(Jo);m(FE.$$.fragment,Hs),h2o=i(Hs),rd=n(Hs,"P",{});var cz=s(rd);p2o=r(cz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence classification head) when created
with the `),Moe=n(cz,"CODE",{});var gGr=s(Moe);_2o=r(gGr,"from_pretrained()"),gGr.forEach(t),u2o=r(cz,"class method or the "),Eoe=n(cz,"CODE",{});var hGr=s(Eoe);b2o=r(hGr,"from_config()"),hGr.forEach(t),v2o=r(cz,`class
method.`),cz.forEach(t),T2o=i(Hs),CE=n(Hs,"P",{});var JBe=s(CE);F2o=r(JBe,"This class cannot be instantiated directly using "),yoe=n(JBe,"CODE",{});var pGr=s(yoe);C2o=r(pGr,"__init__()"),pGr.forEach(t),M2o=r(JBe," (throws an error)."),JBe.forEach(t),E2o=i(Hs),Xr=n(Hs,"DIV",{class:!0});var Us=s(Xr);m(ME.$$.fragment,Us),y2o=i(Us),woe=n(Us,"P",{});var _Gr=s(woe);w2o=r(_Gr,"Instantiates one of the model classes of the library (with a sequence classification head) from a configuration."),_Gr.forEach(t),A2o=i(Us),td=n(Us,"P",{});var fz=s(td);L2o=r(fz,`Note:
Loading a model from its configuration file does `),Aoe=n(fz,"STRONG",{});var uGr=s(Aoe);B2o=r(uGr,"not"),uGr.forEach(t),k2o=r(fz,` load the model weights. It only affects the
model\u2019s configuration. Use `),Loe=n(fz,"CODE",{});var bGr=s(Loe);x2o=r(bGr,"from_pretrained()"),bGr.forEach(t),R2o=r(fz,"to load the model weights."),fz.forEach(t),S2o=i(Us),Boe=n(Us,"P",{});var vGr=s(Boe);P2o=r(vGr,"Examples:"),vGr.forEach(t),$2o=i(Us),m(EE.$$.fragment,Us),Us.forEach(t),I2o=i(Hs),Ie=n(Hs,"DIV",{class:!0});var Nt=s(Ie);m(yE.$$.fragment,Nt),j2o=i(Nt),koe=n(Nt,"P",{});var TGr=s(koe);N2o=r(TGr,"Instantiate one of the model classes of the library (with a sequence classification head) from a pretrained model."),TGr.forEach(t),D2o=i(Nt),za=n(Nt,"P",{});var _4=s(za);q2o=r(_4,"The model class to instantiate is selected based on the "),xoe=n(_4,"CODE",{});var FGr=s(xoe);G2o=r(FGr,"model_type"),FGr.forEach(t),O2o=r(_4,` property of the config object (either
passed as an argument or loaded from `),Roe=n(_4,"CODE",{});var CGr=s(Roe);X2o=r(CGr,"pretrained_model_name_or_path"),CGr.forEach(t),z2o=r(_4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Soe=n(_4,"CODE",{});var MGr=s(Soe);V2o=r(MGr,"pretrained_model_name_or_path"),MGr.forEach(t),W2o=r(_4,":"),_4.forEach(t),Q2o=i(Nt),A=n(Nt,"UL",{});var L=s(A);E1=n(L,"LI",{});var C4e=s(E1);Poe=n(C4e,"STRONG",{});var EGr=s(Poe);H2o=r(EGr,"albert"),EGr.forEach(t),U2o=r(C4e," \u2014 "),c$=n(C4e,"A",{href:!0});var yGr=s(c$);J2o=r(yGr,"AlbertForSequenceClassification"),yGr.forEach(t),Y2o=r(C4e," (ALBERT model)"),C4e.forEach(t),K2o=i(L),y1=n(L,"LI",{});var M4e=s(y1);$oe=n(M4e,"STRONG",{});var wGr=s($oe);Z2o=r(wGr,"bart"),wGr.forEach(t),evo=r(M4e," \u2014 "),f$=n(M4e,"A",{href:!0});var AGr=s(f$);ovo=r(AGr,"BartForSequenceClassification"),AGr.forEach(t),rvo=r(M4e," (BART model)"),M4e.forEach(t),tvo=i(L),w1=n(L,"LI",{});var E4e=s(w1);Ioe=n(E4e,"STRONG",{});var LGr=s(Ioe);avo=r(LGr,"bert"),LGr.forEach(t),nvo=r(E4e," \u2014 "),m$=n(E4e,"A",{href:!0});var BGr=s(m$);svo=r(BGr,"BertForSequenceClassification"),BGr.forEach(t),lvo=r(E4e," (BERT model)"),E4e.forEach(t),ivo=i(L),A1=n(L,"LI",{});var y4e=s(A1);joe=n(y4e,"STRONG",{});var kGr=s(joe);dvo=r(kGr,"big_bird"),kGr.forEach(t),cvo=r(y4e," \u2014 "),g$=n(y4e,"A",{href:!0});var xGr=s(g$);fvo=r(xGr,"BigBirdForSequenceClassification"),xGr.forEach(t),mvo=r(y4e," (BigBird model)"),y4e.forEach(t),gvo=i(L),L1=n(L,"LI",{});var w4e=s(L1);Noe=n(w4e,"STRONG",{});var RGr=s(Noe);hvo=r(RGr,"bigbird_pegasus"),RGr.forEach(t),pvo=r(w4e," \u2014 "),h$=n(w4e,"A",{href:!0});var SGr=s(h$);_vo=r(SGr,"BigBirdPegasusForSequenceClassification"),SGr.forEach(t),uvo=r(w4e," (BigBirdPegasus model)"),w4e.forEach(t),bvo=i(L),B1=n(L,"LI",{});var A4e=s(B1);Doe=n(A4e,"STRONG",{});var PGr=s(Doe);vvo=r(PGr,"camembert"),PGr.forEach(t),Tvo=r(A4e," \u2014 "),p$=n(A4e,"A",{href:!0});var $Gr=s(p$);Fvo=r($Gr,"CamembertForSequenceClassification"),$Gr.forEach(t),Cvo=r(A4e," (CamemBERT model)"),A4e.forEach(t),Mvo=i(L),k1=n(L,"LI",{});var L4e=s(k1);qoe=n(L4e,"STRONG",{});var IGr=s(qoe);Evo=r(IGr,"canine"),IGr.forEach(t),yvo=r(L4e," \u2014 "),_$=n(L4e,"A",{href:!0});var jGr=s(_$);wvo=r(jGr,"CanineForSequenceClassification"),jGr.forEach(t),Avo=r(L4e," (Canine model)"),L4e.forEach(t),Lvo=i(L),x1=n(L,"LI",{});var B4e=s(x1);Goe=n(B4e,"STRONG",{});var NGr=s(Goe);Bvo=r(NGr,"convbert"),NGr.forEach(t),kvo=r(B4e," \u2014 "),u$=n(B4e,"A",{href:!0});var DGr=s(u$);xvo=r(DGr,"ConvBertForSequenceClassification"),DGr.forEach(t),Rvo=r(B4e," (ConvBERT model)"),B4e.forEach(t),Svo=i(L),R1=n(L,"LI",{});var k4e=s(R1);Ooe=n(k4e,"STRONG",{});var qGr=s(Ooe);Pvo=r(qGr,"ctrl"),qGr.forEach(t),$vo=r(k4e," \u2014 "),b$=n(k4e,"A",{href:!0});var GGr=s(b$);Ivo=r(GGr,"CTRLForSequenceClassification"),GGr.forEach(t),jvo=r(k4e," (CTRL model)"),k4e.forEach(t),Nvo=i(L),S1=n(L,"LI",{});var x4e=s(S1);Xoe=n(x4e,"STRONG",{});var OGr=s(Xoe);Dvo=r(OGr,"deberta"),OGr.forEach(t),qvo=r(x4e," \u2014 "),v$=n(x4e,"A",{href:!0});var XGr=s(v$);Gvo=r(XGr,"DebertaForSequenceClassification"),XGr.forEach(t),Ovo=r(x4e," (DeBERTa model)"),x4e.forEach(t),Xvo=i(L),P1=n(L,"LI",{});var R4e=s(P1);zoe=n(R4e,"STRONG",{});var zGr=s(zoe);zvo=r(zGr,"deberta-v2"),zGr.forEach(t),Vvo=r(R4e," \u2014 "),T$=n(R4e,"A",{href:!0});var VGr=s(T$);Wvo=r(VGr,"DebertaV2ForSequenceClassification"),VGr.forEach(t),Qvo=r(R4e," (DeBERTa-v2 model)"),R4e.forEach(t),Hvo=i(L),$1=n(L,"LI",{});var S4e=s($1);Voe=n(S4e,"STRONG",{});var WGr=s(Voe);Uvo=r(WGr,"distilbert"),WGr.forEach(t),Jvo=r(S4e," \u2014 "),F$=n(S4e,"A",{href:!0});var QGr=s(F$);Yvo=r(QGr,"DistilBertForSequenceClassification"),QGr.forEach(t),Kvo=r(S4e," (DistilBERT model)"),S4e.forEach(t),Zvo=i(L),I1=n(L,"LI",{});var P4e=s(I1);Woe=n(P4e,"STRONG",{});var HGr=s(Woe);e0o=r(HGr,"electra"),HGr.forEach(t),o0o=r(P4e," \u2014 "),C$=n(P4e,"A",{href:!0});var UGr=s(C$);r0o=r(UGr,"ElectraForSequenceClassification"),UGr.forEach(t),t0o=r(P4e," (ELECTRA model)"),P4e.forEach(t),a0o=i(L),j1=n(L,"LI",{});var $4e=s(j1);Qoe=n($4e,"STRONG",{});var JGr=s(Qoe);n0o=r(JGr,"flaubert"),JGr.forEach(t),s0o=r($4e," \u2014 "),M$=n($4e,"A",{href:!0});var YGr=s(M$);l0o=r(YGr,"FlaubertForSequenceClassification"),YGr.forEach(t),i0o=r($4e," (FlauBERT model)"),$4e.forEach(t),d0o=i(L),N1=n(L,"LI",{});var I4e=s(N1);Hoe=n(I4e,"STRONG",{});var KGr=s(Hoe);c0o=r(KGr,"fnet"),KGr.forEach(t),f0o=r(I4e," \u2014 "),E$=n(I4e,"A",{href:!0});var ZGr=s(E$);m0o=r(ZGr,"FNetForSequenceClassification"),ZGr.forEach(t),g0o=r(I4e," (FNet model)"),I4e.forEach(t),h0o=i(L),D1=n(L,"LI",{});var j4e=s(D1);Uoe=n(j4e,"STRONG",{});var eOr=s(Uoe);p0o=r(eOr,"funnel"),eOr.forEach(t),_0o=r(j4e," \u2014 "),y$=n(j4e,"A",{href:!0});var oOr=s(y$);u0o=r(oOr,"FunnelForSequenceClassification"),oOr.forEach(t),b0o=r(j4e," (Funnel Transformer model)"),j4e.forEach(t),v0o=i(L),q1=n(L,"LI",{});var N4e=s(q1);Joe=n(N4e,"STRONG",{});var rOr=s(Joe);T0o=r(rOr,"gpt2"),rOr.forEach(t),F0o=r(N4e," \u2014 "),w$=n(N4e,"A",{href:!0});var tOr=s(w$);C0o=r(tOr,"GPT2ForSequenceClassification"),tOr.forEach(t),M0o=r(N4e," (OpenAI GPT-2 model)"),N4e.forEach(t),E0o=i(L),G1=n(L,"LI",{});var D4e=s(G1);Yoe=n(D4e,"STRONG",{});var aOr=s(Yoe);y0o=r(aOr,"gpt_neo"),aOr.forEach(t),w0o=r(D4e," \u2014 "),A$=n(D4e,"A",{href:!0});var nOr=s(A$);A0o=r(nOr,"GPTNeoForSequenceClassification"),nOr.forEach(t),L0o=r(D4e," (GPT Neo model)"),D4e.forEach(t),B0o=i(L),O1=n(L,"LI",{});var q4e=s(O1);Koe=n(q4e,"STRONG",{});var sOr=s(Koe);k0o=r(sOr,"gptj"),sOr.forEach(t),x0o=r(q4e," \u2014 "),L$=n(q4e,"A",{href:!0});var lOr=s(L$);R0o=r(lOr,"GPTJForSequenceClassification"),lOr.forEach(t),S0o=r(q4e," (GPT-J model)"),q4e.forEach(t),P0o=i(L),X1=n(L,"LI",{});var G4e=s(X1);Zoe=n(G4e,"STRONG",{});var iOr=s(Zoe);$0o=r(iOr,"ibert"),iOr.forEach(t),I0o=r(G4e," \u2014 "),B$=n(G4e,"A",{href:!0});var dOr=s(B$);j0o=r(dOr,"IBertForSequenceClassification"),dOr.forEach(t),N0o=r(G4e," (I-BERT model)"),G4e.forEach(t),D0o=i(L),z1=n(L,"LI",{});var O4e=s(z1);ere=n(O4e,"STRONG",{});var cOr=s(ere);q0o=r(cOr,"layoutlm"),cOr.forEach(t),G0o=r(O4e," \u2014 "),k$=n(O4e,"A",{href:!0});var fOr=s(k$);O0o=r(fOr,"LayoutLMForSequenceClassification"),fOr.forEach(t),X0o=r(O4e," (LayoutLM model)"),O4e.forEach(t),z0o=i(L),V1=n(L,"LI",{});var X4e=s(V1);ore=n(X4e,"STRONG",{});var mOr=s(ore);V0o=r(mOr,"layoutlmv2"),mOr.forEach(t),W0o=r(X4e," \u2014 "),x$=n(X4e,"A",{href:!0});var gOr=s(x$);Q0o=r(gOr,"LayoutLMv2ForSequenceClassification"),gOr.forEach(t),H0o=r(X4e," (LayoutLMv2 model)"),X4e.forEach(t),U0o=i(L),W1=n(L,"LI",{});var z4e=s(W1);rre=n(z4e,"STRONG",{});var hOr=s(rre);J0o=r(hOr,"led"),hOr.forEach(t),Y0o=r(z4e," \u2014 "),R$=n(z4e,"A",{href:!0});var pOr=s(R$);K0o=r(pOr,"LEDForSequenceClassification"),pOr.forEach(t),Z0o=r(z4e," (LED model)"),z4e.forEach(t),eTo=i(L),Q1=n(L,"LI",{});var V4e=s(Q1);tre=n(V4e,"STRONG",{});var _Or=s(tre);oTo=r(_Or,"longformer"),_Or.forEach(t),rTo=r(V4e," \u2014 "),S$=n(V4e,"A",{href:!0});var uOr=s(S$);tTo=r(uOr,"LongformerForSequenceClassification"),uOr.forEach(t),aTo=r(V4e," (Longformer model)"),V4e.forEach(t),nTo=i(L),H1=n(L,"LI",{});var W4e=s(H1);are=n(W4e,"STRONG",{});var bOr=s(are);sTo=r(bOr,"mbart"),bOr.forEach(t),lTo=r(W4e," \u2014 "),P$=n(W4e,"A",{href:!0});var vOr=s(P$);iTo=r(vOr,"MBartForSequenceClassification"),vOr.forEach(t),dTo=r(W4e," (mBART model)"),W4e.forEach(t),cTo=i(L),U1=n(L,"LI",{});var Q4e=s(U1);nre=n(Q4e,"STRONG",{});var TOr=s(nre);fTo=r(TOr,"megatron-bert"),TOr.forEach(t),mTo=r(Q4e," \u2014 "),$$=n(Q4e,"A",{href:!0});var FOr=s($$);gTo=r(FOr,"MegatronBertForSequenceClassification"),FOr.forEach(t),hTo=r(Q4e," (MegatronBert model)"),Q4e.forEach(t),pTo=i(L),J1=n(L,"LI",{});var H4e=s(J1);sre=n(H4e,"STRONG",{});var COr=s(sre);_To=r(COr,"mobilebert"),COr.forEach(t),uTo=r(H4e," \u2014 "),I$=n(H4e,"A",{href:!0});var MOr=s(I$);bTo=r(MOr,"MobileBertForSequenceClassification"),MOr.forEach(t),vTo=r(H4e," (MobileBERT model)"),H4e.forEach(t),TTo=i(L),Y1=n(L,"LI",{});var U4e=s(Y1);lre=n(U4e,"STRONG",{});var EOr=s(lre);FTo=r(EOr,"mpnet"),EOr.forEach(t),CTo=r(U4e," \u2014 "),j$=n(U4e,"A",{href:!0});var yOr=s(j$);MTo=r(yOr,"MPNetForSequenceClassification"),yOr.forEach(t),ETo=r(U4e," (MPNet model)"),U4e.forEach(t),yTo=i(L),K1=n(L,"LI",{});var J4e=s(K1);ire=n(J4e,"STRONG",{});var wOr=s(ire);wTo=r(wOr,"nystromformer"),wOr.forEach(t),ATo=r(J4e," \u2014 "),N$=n(J4e,"A",{href:!0});var AOr=s(N$);LTo=r(AOr,"NystromformerForSequenceClassification"),AOr.forEach(t),BTo=r(J4e," (Nystromformer model)"),J4e.forEach(t),kTo=i(L),Z1=n(L,"LI",{});var Y4e=s(Z1);dre=n(Y4e,"STRONG",{});var LOr=s(dre);xTo=r(LOr,"openai-gpt"),LOr.forEach(t),RTo=r(Y4e," \u2014 "),D$=n(Y4e,"A",{href:!0});var BOr=s(D$);STo=r(BOr,"OpenAIGPTForSequenceClassification"),BOr.forEach(t),PTo=r(Y4e," (OpenAI GPT model)"),Y4e.forEach(t),$To=i(L),e7=n(L,"LI",{});var K4e=s(e7);cre=n(K4e,"STRONG",{});var kOr=s(cre);ITo=r(kOr,"perceiver"),kOr.forEach(t),jTo=r(K4e," \u2014 "),q$=n(K4e,"A",{href:!0});var xOr=s(q$);NTo=r(xOr,"PerceiverForSequenceClassification"),xOr.forEach(t),DTo=r(K4e," (Perceiver model)"),K4e.forEach(t),qTo=i(L),o7=n(L,"LI",{});var Z4e=s(o7);fre=n(Z4e,"STRONG",{});var ROr=s(fre);GTo=r(ROr,"plbart"),ROr.forEach(t),OTo=r(Z4e," \u2014 "),G$=n(Z4e,"A",{href:!0});var SOr=s(G$);XTo=r(SOr,"PLBartForSequenceClassification"),SOr.forEach(t),zTo=r(Z4e," (PLBart model)"),Z4e.forEach(t),VTo=i(L),r7=n(L,"LI",{});var eMe=s(r7);mre=n(eMe,"STRONG",{});var POr=s(mre);WTo=r(POr,"qdqbert"),POr.forEach(t),QTo=r(eMe," \u2014 "),O$=n(eMe,"A",{href:!0});var $Or=s(O$);HTo=r($Or,"QDQBertForSequenceClassification"),$Or.forEach(t),UTo=r(eMe," (QDQBert model)"),eMe.forEach(t),JTo=i(L),t7=n(L,"LI",{});var oMe=s(t7);gre=n(oMe,"STRONG",{});var IOr=s(gre);YTo=r(IOr,"reformer"),IOr.forEach(t),KTo=r(oMe," \u2014 "),X$=n(oMe,"A",{href:!0});var jOr=s(X$);ZTo=r(jOr,"ReformerForSequenceClassification"),jOr.forEach(t),eFo=r(oMe," (Reformer model)"),oMe.forEach(t),oFo=i(L),a7=n(L,"LI",{});var rMe=s(a7);hre=n(rMe,"STRONG",{});var NOr=s(hre);rFo=r(NOr,"rembert"),NOr.forEach(t),tFo=r(rMe," \u2014 "),z$=n(rMe,"A",{href:!0});var DOr=s(z$);aFo=r(DOr,"RemBertForSequenceClassification"),DOr.forEach(t),nFo=r(rMe," (RemBERT model)"),rMe.forEach(t),sFo=i(L),n7=n(L,"LI",{});var tMe=s(n7);pre=n(tMe,"STRONG",{});var qOr=s(pre);lFo=r(qOr,"roberta"),qOr.forEach(t),iFo=r(tMe," \u2014 "),V$=n(tMe,"A",{href:!0});var GOr=s(V$);dFo=r(GOr,"RobertaForSequenceClassification"),GOr.forEach(t),cFo=r(tMe," (RoBERTa model)"),tMe.forEach(t),fFo=i(L),s7=n(L,"LI",{});var aMe=s(s7);_re=n(aMe,"STRONG",{});var OOr=s(_re);mFo=r(OOr,"roformer"),OOr.forEach(t),gFo=r(aMe," \u2014 "),W$=n(aMe,"A",{href:!0});var XOr=s(W$);hFo=r(XOr,"RoFormerForSequenceClassification"),XOr.forEach(t),pFo=r(aMe," (RoFormer model)"),aMe.forEach(t),_Fo=i(L),l7=n(L,"LI",{});var nMe=s(l7);ure=n(nMe,"STRONG",{});var zOr=s(ure);uFo=r(zOr,"squeezebert"),zOr.forEach(t),bFo=r(nMe," \u2014 "),Q$=n(nMe,"A",{href:!0});var VOr=s(Q$);vFo=r(VOr,"SqueezeBertForSequenceClassification"),VOr.forEach(t),TFo=r(nMe," (SqueezeBERT model)"),nMe.forEach(t),FFo=i(L),i7=n(L,"LI",{});var sMe=s(i7);bre=n(sMe,"STRONG",{});var WOr=s(bre);CFo=r(WOr,"tapas"),WOr.forEach(t),MFo=r(sMe," \u2014 "),H$=n(sMe,"A",{href:!0});var QOr=s(H$);EFo=r(QOr,"TapasForSequenceClassification"),QOr.forEach(t),yFo=r(sMe," (TAPAS model)"),sMe.forEach(t),wFo=i(L),d7=n(L,"LI",{});var lMe=s(d7);vre=n(lMe,"STRONG",{});var HOr=s(vre);AFo=r(HOr,"transfo-xl"),HOr.forEach(t),LFo=r(lMe," \u2014 "),U$=n(lMe,"A",{href:!0});var UOr=s(U$);BFo=r(UOr,"TransfoXLForSequenceClassification"),UOr.forEach(t),kFo=r(lMe," (Transformer-XL model)"),lMe.forEach(t),xFo=i(L),c7=n(L,"LI",{});var iMe=s(c7);Tre=n(iMe,"STRONG",{});var JOr=s(Tre);RFo=r(JOr,"xlm"),JOr.forEach(t),SFo=r(iMe," \u2014 "),J$=n(iMe,"A",{href:!0});var YOr=s(J$);PFo=r(YOr,"XLMForSequenceClassification"),YOr.forEach(t),$Fo=r(iMe," (XLM model)"),iMe.forEach(t),IFo=i(L),f7=n(L,"LI",{});var dMe=s(f7);Fre=n(dMe,"STRONG",{});var KOr=s(Fre);jFo=r(KOr,"xlm-roberta"),KOr.forEach(t),NFo=r(dMe," \u2014 "),Y$=n(dMe,"A",{href:!0});var ZOr=s(Y$);DFo=r(ZOr,"XLMRobertaForSequenceClassification"),ZOr.forEach(t),qFo=r(dMe," (XLM-RoBERTa model)"),dMe.forEach(t),GFo=i(L),m7=n(L,"LI",{});var cMe=s(m7);Cre=n(cMe,"STRONG",{});var eXr=s(Cre);OFo=r(eXr,"xlm-roberta-xl"),eXr.forEach(t),XFo=r(cMe," \u2014 "),K$=n(cMe,"A",{href:!0});var oXr=s(K$);zFo=r(oXr,"XLMRobertaXLForSequenceClassification"),oXr.forEach(t),VFo=r(cMe," (XLM-RoBERTa-XL model)"),cMe.forEach(t),WFo=i(L),g7=n(L,"LI",{});var fMe=s(g7);Mre=n(fMe,"STRONG",{});var rXr=s(Mre);QFo=r(rXr,"xlnet"),rXr.forEach(t),HFo=r(fMe," \u2014 "),Z$=n(fMe,"A",{href:!0});var tXr=s(Z$);UFo=r(tXr,"XLNetForSequenceClassification"),tXr.forEach(t),JFo=r(fMe," (XLNet model)"),fMe.forEach(t),YFo=i(L),h7=n(L,"LI",{});var mMe=s(h7);Ere=n(mMe,"STRONG",{});var aXr=s(Ere);KFo=r(aXr,"yoso"),aXr.forEach(t),ZFo=r(mMe," \u2014 "),eI=n(mMe,"A",{href:!0});var nXr=s(eI);eCo=r(nXr,"YosoForSequenceClassification"),nXr.forEach(t),oCo=r(mMe," (YOSO model)"),mMe.forEach(t),L.forEach(t),rCo=i(Nt),p7=n(Nt,"P",{});var gMe=s(p7);tCo=r(gMe,"The model is set in evaluation mode by default using "),yre=n(gMe,"CODE",{});var sXr=s(yre);aCo=r(sXr,"model.eval()"),sXr.forEach(t),nCo=r(gMe,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),wre=n(gMe,"CODE",{});var lXr=s(wre);sCo=r(lXr,"model.train()"),lXr.forEach(t),gMe.forEach(t),lCo=i(Nt),Are=n(Nt,"P",{});var iXr=s(Are);iCo=r(iXr,"Examples:"),iXr.forEach(t),dCo=i(Nt),m(wE.$$.fragment,Nt),Nt.forEach(t),Hs.forEach(t),z8e=i(d),ad=n(d,"H2",{class:!0});var YBe=s(ad);_7=n(YBe,"A",{id:!0,class:!0,href:!0});var dXr=s(_7);Lre=n(dXr,"SPAN",{});var cXr=s(Lre);m(AE.$$.fragment,cXr),cXr.forEach(t),dXr.forEach(t),cCo=i(YBe),Bre=n(YBe,"SPAN",{});var fXr=s(Bre);fCo=r(fXr,"AutoModelForMultipleChoice"),fXr.forEach(t),YBe.forEach(t),V8e=i(d),Yo=n(d,"DIV",{class:!0});var Js=s(Yo);m(LE.$$.fragment,Js),mCo=i(Js),nd=n(Js,"P",{});var mz=s(nd);gCo=r(mz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a multiple choice head) when created
with the `),kre=n(mz,"CODE",{});var mXr=s(kre);hCo=r(mXr,"from_pretrained()"),mXr.forEach(t),pCo=r(mz,"class method or the "),xre=n(mz,"CODE",{});var gXr=s(xre);_Co=r(gXr,"from_config()"),gXr.forEach(t),uCo=r(mz,`class
method.`),mz.forEach(t),bCo=i(Js),BE=n(Js,"P",{});var KBe=s(BE);vCo=r(KBe,"This class cannot be instantiated directly using "),Rre=n(KBe,"CODE",{});var hXr=s(Rre);TCo=r(hXr,"__init__()"),hXr.forEach(t),FCo=r(KBe," (throws an error)."),KBe.forEach(t),CCo=i(Js),zr=n(Js,"DIV",{class:!0});var Ys=s(zr);m(kE.$$.fragment,Ys),MCo=i(Ys),Sre=n(Ys,"P",{});var pXr=s(Sre);ECo=r(pXr,"Instantiates one of the model classes of the library (with a multiple choice head) from a configuration."),pXr.forEach(t),yCo=i(Ys),sd=n(Ys,"P",{});var gz=s(sd);wCo=r(gz,`Note:
Loading a model from its configuration file does `),Pre=n(gz,"STRONG",{});var _Xr=s(Pre);ACo=r(_Xr,"not"),_Xr.forEach(t),LCo=r(gz,` load the model weights. It only affects the
model\u2019s configuration. Use `),$re=n(gz,"CODE",{});var uXr=s($re);BCo=r(uXr,"from_pretrained()"),uXr.forEach(t),kCo=r(gz,"to load the model weights."),gz.forEach(t),xCo=i(Ys),Ire=n(Ys,"P",{});var bXr=s(Ire);RCo=r(bXr,"Examples:"),bXr.forEach(t),SCo=i(Ys),m(xE.$$.fragment,Ys),Ys.forEach(t),PCo=i(Js),je=n(Js,"DIV",{class:!0});var Dt=s(je);m(RE.$$.fragment,Dt),$Co=i(Dt),jre=n(Dt,"P",{});var vXr=s(jre);ICo=r(vXr,"Instantiate one of the model classes of the library (with a multiple choice head) from a pretrained model."),vXr.forEach(t),jCo=i(Dt),Va=n(Dt,"P",{});var u4=s(Va);NCo=r(u4,"The model class to instantiate is selected based on the "),Nre=n(u4,"CODE",{});var TXr=s(Nre);DCo=r(TXr,"model_type"),TXr.forEach(t),qCo=r(u4,` property of the config object (either
passed as an argument or loaded from `),Dre=n(u4,"CODE",{});var FXr=s(Dre);GCo=r(FXr,"pretrained_model_name_or_path"),FXr.forEach(t),OCo=r(u4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),qre=n(u4,"CODE",{});var CXr=s(qre);XCo=r(CXr,"pretrained_model_name_or_path"),CXr.forEach(t),zCo=r(u4,":"),u4.forEach(t),VCo=i(Dt),G=n(Dt,"UL",{});var O=s(G);u7=n(O,"LI",{});var hMe=s(u7);Gre=n(hMe,"STRONG",{});var MXr=s(Gre);WCo=r(MXr,"albert"),MXr.forEach(t),QCo=r(hMe," \u2014 "),oI=n(hMe,"A",{href:!0});var EXr=s(oI);HCo=r(EXr,"AlbertForMultipleChoice"),EXr.forEach(t),UCo=r(hMe," (ALBERT model)"),hMe.forEach(t),JCo=i(O),b7=n(O,"LI",{});var pMe=s(b7);Ore=n(pMe,"STRONG",{});var yXr=s(Ore);YCo=r(yXr,"bert"),yXr.forEach(t),KCo=r(pMe," \u2014 "),rI=n(pMe,"A",{href:!0});var wXr=s(rI);ZCo=r(wXr,"BertForMultipleChoice"),wXr.forEach(t),e4o=r(pMe," (BERT model)"),pMe.forEach(t),o4o=i(O),v7=n(O,"LI",{});var _Me=s(v7);Xre=n(_Me,"STRONG",{});var AXr=s(Xre);r4o=r(AXr,"big_bird"),AXr.forEach(t),t4o=r(_Me," \u2014 "),tI=n(_Me,"A",{href:!0});var LXr=s(tI);a4o=r(LXr,"BigBirdForMultipleChoice"),LXr.forEach(t),n4o=r(_Me," (BigBird model)"),_Me.forEach(t),s4o=i(O),T7=n(O,"LI",{});var uMe=s(T7);zre=n(uMe,"STRONG",{});var BXr=s(zre);l4o=r(BXr,"camembert"),BXr.forEach(t),i4o=r(uMe," \u2014 "),aI=n(uMe,"A",{href:!0});var kXr=s(aI);d4o=r(kXr,"CamembertForMultipleChoice"),kXr.forEach(t),c4o=r(uMe," (CamemBERT model)"),uMe.forEach(t),f4o=i(O),F7=n(O,"LI",{});var bMe=s(F7);Vre=n(bMe,"STRONG",{});var xXr=s(Vre);m4o=r(xXr,"canine"),xXr.forEach(t),g4o=r(bMe," \u2014 "),nI=n(bMe,"A",{href:!0});var RXr=s(nI);h4o=r(RXr,"CanineForMultipleChoice"),RXr.forEach(t),p4o=r(bMe," (Canine model)"),bMe.forEach(t),_4o=i(O),C7=n(O,"LI",{});var vMe=s(C7);Wre=n(vMe,"STRONG",{});var SXr=s(Wre);u4o=r(SXr,"convbert"),SXr.forEach(t),b4o=r(vMe," \u2014 "),sI=n(vMe,"A",{href:!0});var PXr=s(sI);v4o=r(PXr,"ConvBertForMultipleChoice"),PXr.forEach(t),T4o=r(vMe," (ConvBERT model)"),vMe.forEach(t),F4o=i(O),M7=n(O,"LI",{});var TMe=s(M7);Qre=n(TMe,"STRONG",{});var $Xr=s(Qre);C4o=r($Xr,"distilbert"),$Xr.forEach(t),M4o=r(TMe," \u2014 "),lI=n(TMe,"A",{href:!0});var IXr=s(lI);E4o=r(IXr,"DistilBertForMultipleChoice"),IXr.forEach(t),y4o=r(TMe," (DistilBERT model)"),TMe.forEach(t),w4o=i(O),E7=n(O,"LI",{});var FMe=s(E7);Hre=n(FMe,"STRONG",{});var jXr=s(Hre);A4o=r(jXr,"electra"),jXr.forEach(t),L4o=r(FMe," \u2014 "),iI=n(FMe,"A",{href:!0});var NXr=s(iI);B4o=r(NXr,"ElectraForMultipleChoice"),NXr.forEach(t),k4o=r(FMe," (ELECTRA model)"),FMe.forEach(t),x4o=i(O),y7=n(O,"LI",{});var CMe=s(y7);Ure=n(CMe,"STRONG",{});var DXr=s(Ure);R4o=r(DXr,"flaubert"),DXr.forEach(t),S4o=r(CMe," \u2014 "),dI=n(CMe,"A",{href:!0});var qXr=s(dI);P4o=r(qXr,"FlaubertForMultipleChoice"),qXr.forEach(t),$4o=r(CMe," (FlauBERT model)"),CMe.forEach(t),I4o=i(O),w7=n(O,"LI",{});var MMe=s(w7);Jre=n(MMe,"STRONG",{});var GXr=s(Jre);j4o=r(GXr,"fnet"),GXr.forEach(t),N4o=r(MMe," \u2014 "),cI=n(MMe,"A",{href:!0});var OXr=s(cI);D4o=r(OXr,"FNetForMultipleChoice"),OXr.forEach(t),q4o=r(MMe," (FNet model)"),MMe.forEach(t),G4o=i(O),A7=n(O,"LI",{});var EMe=s(A7);Yre=n(EMe,"STRONG",{});var XXr=s(Yre);O4o=r(XXr,"funnel"),XXr.forEach(t),X4o=r(EMe," \u2014 "),fI=n(EMe,"A",{href:!0});var zXr=s(fI);z4o=r(zXr,"FunnelForMultipleChoice"),zXr.forEach(t),V4o=r(EMe," (Funnel Transformer model)"),EMe.forEach(t),W4o=i(O),L7=n(O,"LI",{});var yMe=s(L7);Kre=n(yMe,"STRONG",{});var VXr=s(Kre);Q4o=r(VXr,"ibert"),VXr.forEach(t),H4o=r(yMe," \u2014 "),mI=n(yMe,"A",{href:!0});var WXr=s(mI);U4o=r(WXr,"IBertForMultipleChoice"),WXr.forEach(t),J4o=r(yMe," (I-BERT model)"),yMe.forEach(t),Y4o=i(O),B7=n(O,"LI",{});var wMe=s(B7);Zre=n(wMe,"STRONG",{});var QXr=s(Zre);K4o=r(QXr,"longformer"),QXr.forEach(t),Z4o=r(wMe," \u2014 "),gI=n(wMe,"A",{href:!0});var HXr=s(gI);eMo=r(HXr,"LongformerForMultipleChoice"),HXr.forEach(t),oMo=r(wMe," (Longformer model)"),wMe.forEach(t),rMo=i(O),k7=n(O,"LI",{});var AMe=s(k7);ete=n(AMe,"STRONG",{});var UXr=s(ete);tMo=r(UXr,"megatron-bert"),UXr.forEach(t),aMo=r(AMe," \u2014 "),hI=n(AMe,"A",{href:!0});var JXr=s(hI);nMo=r(JXr,"MegatronBertForMultipleChoice"),JXr.forEach(t),sMo=r(AMe," (MegatronBert model)"),AMe.forEach(t),lMo=i(O),x7=n(O,"LI",{});var LMe=s(x7);ote=n(LMe,"STRONG",{});var YXr=s(ote);iMo=r(YXr,"mobilebert"),YXr.forEach(t),dMo=r(LMe," \u2014 "),pI=n(LMe,"A",{href:!0});var KXr=s(pI);cMo=r(KXr,"MobileBertForMultipleChoice"),KXr.forEach(t),fMo=r(LMe," (MobileBERT model)"),LMe.forEach(t),mMo=i(O),R7=n(O,"LI",{});var BMe=s(R7);rte=n(BMe,"STRONG",{});var ZXr=s(rte);gMo=r(ZXr,"mpnet"),ZXr.forEach(t),hMo=r(BMe," \u2014 "),_I=n(BMe,"A",{href:!0});var ezr=s(_I);pMo=r(ezr,"MPNetForMultipleChoice"),ezr.forEach(t),_Mo=r(BMe," (MPNet model)"),BMe.forEach(t),uMo=i(O),S7=n(O,"LI",{});var kMe=s(S7);tte=n(kMe,"STRONG",{});var ozr=s(tte);bMo=r(ozr,"nystromformer"),ozr.forEach(t),vMo=r(kMe," \u2014 "),uI=n(kMe,"A",{href:!0});var rzr=s(uI);TMo=r(rzr,"NystromformerForMultipleChoice"),rzr.forEach(t),FMo=r(kMe," (Nystromformer model)"),kMe.forEach(t),CMo=i(O),P7=n(O,"LI",{});var xMe=s(P7);ate=n(xMe,"STRONG",{});var tzr=s(ate);MMo=r(tzr,"qdqbert"),tzr.forEach(t),EMo=r(xMe," \u2014 "),bI=n(xMe,"A",{href:!0});var azr=s(bI);yMo=r(azr,"QDQBertForMultipleChoice"),azr.forEach(t),wMo=r(xMe," (QDQBert model)"),xMe.forEach(t),AMo=i(O),$7=n(O,"LI",{});var RMe=s($7);nte=n(RMe,"STRONG",{});var nzr=s(nte);LMo=r(nzr,"rembert"),nzr.forEach(t),BMo=r(RMe," \u2014 "),vI=n(RMe,"A",{href:!0});var szr=s(vI);kMo=r(szr,"RemBertForMultipleChoice"),szr.forEach(t),xMo=r(RMe," (RemBERT model)"),RMe.forEach(t),RMo=i(O),I7=n(O,"LI",{});var SMe=s(I7);ste=n(SMe,"STRONG",{});var lzr=s(ste);SMo=r(lzr,"roberta"),lzr.forEach(t),PMo=r(SMe," \u2014 "),TI=n(SMe,"A",{href:!0});var izr=s(TI);$Mo=r(izr,"RobertaForMultipleChoice"),izr.forEach(t),IMo=r(SMe," (RoBERTa model)"),SMe.forEach(t),jMo=i(O),j7=n(O,"LI",{});var PMe=s(j7);lte=n(PMe,"STRONG",{});var dzr=s(lte);NMo=r(dzr,"roformer"),dzr.forEach(t),DMo=r(PMe," \u2014 "),FI=n(PMe,"A",{href:!0});var czr=s(FI);qMo=r(czr,"RoFormerForMultipleChoice"),czr.forEach(t),GMo=r(PMe," (RoFormer model)"),PMe.forEach(t),OMo=i(O),N7=n(O,"LI",{});var $Me=s(N7);ite=n($Me,"STRONG",{});var fzr=s(ite);XMo=r(fzr,"squeezebert"),fzr.forEach(t),zMo=r($Me," \u2014 "),CI=n($Me,"A",{href:!0});var mzr=s(CI);VMo=r(mzr,"SqueezeBertForMultipleChoice"),mzr.forEach(t),WMo=r($Me," (SqueezeBERT model)"),$Me.forEach(t),QMo=i(O),D7=n(O,"LI",{});var IMe=s(D7);dte=n(IMe,"STRONG",{});var gzr=s(dte);HMo=r(gzr,"xlm"),gzr.forEach(t),UMo=r(IMe," \u2014 "),MI=n(IMe,"A",{href:!0});var hzr=s(MI);JMo=r(hzr,"XLMForMultipleChoice"),hzr.forEach(t),YMo=r(IMe," (XLM model)"),IMe.forEach(t),KMo=i(O),q7=n(O,"LI",{});var jMe=s(q7);cte=n(jMe,"STRONG",{});var pzr=s(cte);ZMo=r(pzr,"xlm-roberta"),pzr.forEach(t),eEo=r(jMe," \u2014 "),EI=n(jMe,"A",{href:!0});var _zr=s(EI);oEo=r(_zr,"XLMRobertaForMultipleChoice"),_zr.forEach(t),rEo=r(jMe," (XLM-RoBERTa model)"),jMe.forEach(t),tEo=i(O),G7=n(O,"LI",{});var NMe=s(G7);fte=n(NMe,"STRONG",{});var uzr=s(fte);aEo=r(uzr,"xlm-roberta-xl"),uzr.forEach(t),nEo=r(NMe," \u2014 "),yI=n(NMe,"A",{href:!0});var bzr=s(yI);sEo=r(bzr,"XLMRobertaXLForMultipleChoice"),bzr.forEach(t),lEo=r(NMe," (XLM-RoBERTa-XL model)"),NMe.forEach(t),iEo=i(O),O7=n(O,"LI",{});var DMe=s(O7);mte=n(DMe,"STRONG",{});var vzr=s(mte);dEo=r(vzr,"xlnet"),vzr.forEach(t),cEo=r(DMe," \u2014 "),wI=n(DMe,"A",{href:!0});var Tzr=s(wI);fEo=r(Tzr,"XLNetForMultipleChoice"),Tzr.forEach(t),mEo=r(DMe," (XLNet model)"),DMe.forEach(t),gEo=i(O),X7=n(O,"LI",{});var qMe=s(X7);gte=n(qMe,"STRONG",{});var Fzr=s(gte);hEo=r(Fzr,"yoso"),Fzr.forEach(t),pEo=r(qMe," \u2014 "),AI=n(qMe,"A",{href:!0});var Czr=s(AI);_Eo=r(Czr,"YosoForMultipleChoice"),Czr.forEach(t),uEo=r(qMe," (YOSO model)"),qMe.forEach(t),O.forEach(t),bEo=i(Dt),z7=n(Dt,"P",{});var GMe=s(z7);vEo=r(GMe,"The model is set in evaluation mode by default using "),hte=n(GMe,"CODE",{});var Mzr=s(hte);TEo=r(Mzr,"model.eval()"),Mzr.forEach(t),FEo=r(GMe,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),pte=n(GMe,"CODE",{});var Ezr=s(pte);CEo=r(Ezr,"model.train()"),Ezr.forEach(t),GMe.forEach(t),MEo=i(Dt),_te=n(Dt,"P",{});var yzr=s(_te);EEo=r(yzr,"Examples:"),yzr.forEach(t),yEo=i(Dt),m(SE.$$.fragment,Dt),Dt.forEach(t),Js.forEach(t),W8e=i(d),ld=n(d,"H2",{class:!0});var ZBe=s(ld);V7=n(ZBe,"A",{id:!0,class:!0,href:!0});var wzr=s(V7);ute=n(wzr,"SPAN",{});var Azr=s(ute);m(PE.$$.fragment,Azr),Azr.forEach(t),wzr.forEach(t),wEo=i(ZBe),bte=n(ZBe,"SPAN",{});var Lzr=s(bte);AEo=r(Lzr,"AutoModelForNextSentencePrediction"),Lzr.forEach(t),ZBe.forEach(t),Q8e=i(d),Ko=n(d,"DIV",{class:!0});var Ks=s(Ko);m($E.$$.fragment,Ks),LEo=i(Ks),id=n(Ks,"P",{});var hz=s(id);BEo=r(hz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a next sentence prediction head) when created
with the `),vte=n(hz,"CODE",{});var Bzr=s(vte);kEo=r(Bzr,"from_pretrained()"),Bzr.forEach(t),xEo=r(hz,"class method or the "),Tte=n(hz,"CODE",{});var kzr=s(Tte);REo=r(kzr,"from_config()"),kzr.forEach(t),SEo=r(hz,`class
method.`),hz.forEach(t),PEo=i(Ks),IE=n(Ks,"P",{});var eke=s(IE);$Eo=r(eke,"This class cannot be instantiated directly using "),Fte=n(eke,"CODE",{});var xzr=s(Fte);IEo=r(xzr,"__init__()"),xzr.forEach(t),jEo=r(eke," (throws an error)."),eke.forEach(t),NEo=i(Ks),Vr=n(Ks,"DIV",{class:!0});var Zs=s(Vr);m(jE.$$.fragment,Zs),DEo=i(Zs),Cte=n(Zs,"P",{});var Rzr=s(Cte);qEo=r(Rzr,"Instantiates one of the model classes of the library (with a next sentence prediction head) from a configuration."),Rzr.forEach(t),GEo=i(Zs),dd=n(Zs,"P",{});var pz=s(dd);OEo=r(pz,`Note:
Loading a model from its configuration file does `),Mte=n(pz,"STRONG",{});var Szr=s(Mte);XEo=r(Szr,"not"),Szr.forEach(t),zEo=r(pz,` load the model weights. It only affects the
model\u2019s configuration. Use `),Ete=n(pz,"CODE",{});var Pzr=s(Ete);VEo=r(Pzr,"from_pretrained()"),Pzr.forEach(t),WEo=r(pz,"to load the model weights."),pz.forEach(t),QEo=i(Zs),yte=n(Zs,"P",{});var $zr=s(yte);HEo=r($zr,"Examples:"),$zr.forEach(t),UEo=i(Zs),m(NE.$$.fragment,Zs),Zs.forEach(t),JEo=i(Ks),Ne=n(Ks,"DIV",{class:!0});var qt=s(Ne);m(DE.$$.fragment,qt),YEo=i(qt),wte=n(qt,"P",{});var Izr=s(wte);KEo=r(Izr,"Instantiate one of the model classes of the library (with a next sentence prediction head) from a pretrained model."),Izr.forEach(t),ZEo=i(qt),Wa=n(qt,"P",{});var b4=s(Wa);e3o=r(b4,"The model class to instantiate is selected based on the "),Ate=n(b4,"CODE",{});var jzr=s(Ate);o3o=r(jzr,"model_type"),jzr.forEach(t),r3o=r(b4,` property of the config object (either
passed as an argument or loaded from `),Lte=n(b4,"CODE",{});var Nzr=s(Lte);t3o=r(Nzr,"pretrained_model_name_or_path"),Nzr.forEach(t),a3o=r(b4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Bte=n(b4,"CODE",{});var Dzr=s(Bte);n3o=r(Dzr,"pretrained_model_name_or_path"),Dzr.forEach(t),s3o=r(b4,":"),b4.forEach(t),l3o=i(qt),na=n(qt,"UL",{});var el=s(na);W7=n(el,"LI",{});var OMe=s(W7);kte=n(OMe,"STRONG",{});var qzr=s(kte);i3o=r(qzr,"bert"),qzr.forEach(t),d3o=r(OMe," \u2014 "),LI=n(OMe,"A",{href:!0});var Gzr=s(LI);c3o=r(Gzr,"BertForNextSentencePrediction"),Gzr.forEach(t),f3o=r(OMe," (BERT model)"),OMe.forEach(t),m3o=i(el),Q7=n(el,"LI",{});var XMe=s(Q7);xte=n(XMe,"STRONG",{});var Ozr=s(xte);g3o=r(Ozr,"fnet"),Ozr.forEach(t),h3o=r(XMe," \u2014 "),BI=n(XMe,"A",{href:!0});var Xzr=s(BI);p3o=r(Xzr,"FNetForNextSentencePrediction"),Xzr.forEach(t),_3o=r(XMe," (FNet model)"),XMe.forEach(t),u3o=i(el),H7=n(el,"LI",{});var zMe=s(H7);Rte=n(zMe,"STRONG",{});var zzr=s(Rte);b3o=r(zzr,"megatron-bert"),zzr.forEach(t),v3o=r(zMe," \u2014 "),kI=n(zMe,"A",{href:!0});var Vzr=s(kI);T3o=r(Vzr,"MegatronBertForNextSentencePrediction"),Vzr.forEach(t),F3o=r(zMe," (MegatronBert model)"),zMe.forEach(t),C3o=i(el),U7=n(el,"LI",{});var VMe=s(U7);Ste=n(VMe,"STRONG",{});var Wzr=s(Ste);M3o=r(Wzr,"mobilebert"),Wzr.forEach(t),E3o=r(VMe," \u2014 "),xI=n(VMe,"A",{href:!0});var Qzr=s(xI);y3o=r(Qzr,"MobileBertForNextSentencePrediction"),Qzr.forEach(t),w3o=r(VMe," (MobileBERT model)"),VMe.forEach(t),A3o=i(el),J7=n(el,"LI",{});var WMe=s(J7);Pte=n(WMe,"STRONG",{});var Hzr=s(Pte);L3o=r(Hzr,"qdqbert"),Hzr.forEach(t),B3o=r(WMe," \u2014 "),RI=n(WMe,"A",{href:!0});var Uzr=s(RI);k3o=r(Uzr,"QDQBertForNextSentencePrediction"),Uzr.forEach(t),x3o=r(WMe," (QDQBert model)"),WMe.forEach(t),el.forEach(t),R3o=i(qt),Y7=n(qt,"P",{});var QMe=s(Y7);S3o=r(QMe,"The model is set in evaluation mode by default using "),$te=n(QMe,"CODE",{});var Jzr=s($te);P3o=r(Jzr,"model.eval()"),Jzr.forEach(t),$3o=r(QMe,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Ite=n(QMe,"CODE",{});var Yzr=s(Ite);I3o=r(Yzr,"model.train()"),Yzr.forEach(t),QMe.forEach(t),j3o=i(qt),jte=n(qt,"P",{});var Kzr=s(jte);N3o=r(Kzr,"Examples:"),Kzr.forEach(t),D3o=i(qt),m(qE.$$.fragment,qt),qt.forEach(t),Ks.forEach(t),H8e=i(d),cd=n(d,"H2",{class:!0});var oke=s(cd);K7=n(oke,"A",{id:!0,class:!0,href:!0});var Zzr=s(K7);Nte=n(Zzr,"SPAN",{});var eVr=s(Nte);m(GE.$$.fragment,eVr),eVr.forEach(t),Zzr.forEach(t),q3o=i(oke),Dte=n(oke,"SPAN",{});var oVr=s(Dte);G3o=r(oVr,"AutoModelForTokenClassification"),oVr.forEach(t),oke.forEach(t),U8e=i(d),Zo=n(d,"DIV",{class:!0});var ol=s(Zo);m(OE.$$.fragment,ol),O3o=i(ol),fd=n(ol,"P",{});var _z=s(fd);X3o=r(_z,`This is a generic model class that will be instantiated as one of the model classes of the library (with a token classification head) when created
with the `),qte=n(_z,"CODE",{});var rVr=s(qte);z3o=r(rVr,"from_pretrained()"),rVr.forEach(t),V3o=r(_z,"class method or the "),Gte=n(_z,"CODE",{});var tVr=s(Gte);W3o=r(tVr,"from_config()"),tVr.forEach(t),Q3o=r(_z,`class
method.`),_z.forEach(t),H3o=i(ol),XE=n(ol,"P",{});var rke=s(XE);U3o=r(rke,"This class cannot be instantiated directly using "),Ote=n(rke,"CODE",{});var aVr=s(Ote);J3o=r(aVr,"__init__()"),aVr.forEach(t),Y3o=r(rke," (throws an error)."),rke.forEach(t),K3o=i(ol),Wr=n(ol,"DIV",{class:!0});var rl=s(Wr);m(zE.$$.fragment,rl),Z3o=i(rl),Xte=n(rl,"P",{});var nVr=s(Xte);eyo=r(nVr,"Instantiates one of the model classes of the library (with a token classification head) from a configuration."),nVr.forEach(t),oyo=i(rl),md=n(rl,"P",{});var uz=s(md);ryo=r(uz,`Note:
Loading a model from its configuration file does `),zte=n(uz,"STRONG",{});var sVr=s(zte);tyo=r(sVr,"not"),sVr.forEach(t),ayo=r(uz,` load the model weights. It only affects the
model\u2019s configuration. Use `),Vte=n(uz,"CODE",{});var lVr=s(Vte);nyo=r(lVr,"from_pretrained()"),lVr.forEach(t),syo=r(uz,"to load the model weights."),uz.forEach(t),lyo=i(rl),Wte=n(rl,"P",{});var iVr=s(Wte);iyo=r(iVr,"Examples:"),iVr.forEach(t),dyo=i(rl),m(VE.$$.fragment,rl),rl.forEach(t),cyo=i(ol),De=n(ol,"DIV",{class:!0});var Gt=s(De);m(WE.$$.fragment,Gt),fyo=i(Gt),Qte=n(Gt,"P",{});var dVr=s(Qte);myo=r(dVr,"Instantiate one of the model classes of the library (with a token classification head) from a pretrained model."),dVr.forEach(t),gyo=i(Gt),Qa=n(Gt,"P",{});var v4=s(Qa);hyo=r(v4,"The model class to instantiate is selected based on the "),Hte=n(v4,"CODE",{});var cVr=s(Hte);pyo=r(cVr,"model_type"),cVr.forEach(t),_yo=r(v4,` property of the config object (either
passed as an argument or loaded from `),Ute=n(v4,"CODE",{});var fVr=s(Ute);uyo=r(fVr,"pretrained_model_name_or_path"),fVr.forEach(t),byo=r(v4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Jte=n(v4,"CODE",{});var mVr=s(Jte);vyo=r(mVr,"pretrained_model_name_or_path"),mVr.forEach(t),Tyo=r(v4,":"),v4.forEach(t),Fyo=i(Gt),D=n(Gt,"UL",{});var q=s(D);Z7=n(q,"LI",{});var HMe=s(Z7);Yte=n(HMe,"STRONG",{});var gVr=s(Yte);Cyo=r(gVr,"albert"),gVr.forEach(t),Myo=r(HMe," \u2014 "),SI=n(HMe,"A",{href:!0});var hVr=s(SI);Eyo=r(hVr,"AlbertForTokenClassification"),hVr.forEach(t),yyo=r(HMe," (ALBERT model)"),HMe.forEach(t),wyo=i(q),eb=n(q,"LI",{});var UMe=s(eb);Kte=n(UMe,"STRONG",{});var pVr=s(Kte);Ayo=r(pVr,"bert"),pVr.forEach(t),Lyo=r(UMe," \u2014 "),PI=n(UMe,"A",{href:!0});var _Vr=s(PI);Byo=r(_Vr,"BertForTokenClassification"),_Vr.forEach(t),kyo=r(UMe," (BERT model)"),UMe.forEach(t),xyo=i(q),ob=n(q,"LI",{});var JMe=s(ob);Zte=n(JMe,"STRONG",{});var uVr=s(Zte);Ryo=r(uVr,"big_bird"),uVr.forEach(t),Syo=r(JMe," \u2014 "),$I=n(JMe,"A",{href:!0});var bVr=s($I);Pyo=r(bVr,"BigBirdForTokenClassification"),bVr.forEach(t),$yo=r(JMe," (BigBird model)"),JMe.forEach(t),Iyo=i(q),rb=n(q,"LI",{});var YMe=s(rb);eae=n(YMe,"STRONG",{});var vVr=s(eae);jyo=r(vVr,"camembert"),vVr.forEach(t),Nyo=r(YMe," \u2014 "),II=n(YMe,"A",{href:!0});var TVr=s(II);Dyo=r(TVr,"CamembertForTokenClassification"),TVr.forEach(t),qyo=r(YMe," (CamemBERT model)"),YMe.forEach(t),Gyo=i(q),tb=n(q,"LI",{});var KMe=s(tb);oae=n(KMe,"STRONG",{});var FVr=s(oae);Oyo=r(FVr,"canine"),FVr.forEach(t),Xyo=r(KMe," \u2014 "),jI=n(KMe,"A",{href:!0});var CVr=s(jI);zyo=r(CVr,"CanineForTokenClassification"),CVr.forEach(t),Vyo=r(KMe," (Canine model)"),KMe.forEach(t),Wyo=i(q),ab=n(q,"LI",{});var ZMe=s(ab);rae=n(ZMe,"STRONG",{});var MVr=s(rae);Qyo=r(MVr,"convbert"),MVr.forEach(t),Hyo=r(ZMe," \u2014 "),NI=n(ZMe,"A",{href:!0});var EVr=s(NI);Uyo=r(EVr,"ConvBertForTokenClassification"),EVr.forEach(t),Jyo=r(ZMe," (ConvBERT model)"),ZMe.forEach(t),Yyo=i(q),nb=n(q,"LI",{});var eEe=s(nb);tae=n(eEe,"STRONG",{});var yVr=s(tae);Kyo=r(yVr,"deberta"),yVr.forEach(t),Zyo=r(eEe," \u2014 "),DI=n(eEe,"A",{href:!0});var wVr=s(DI);ewo=r(wVr,"DebertaForTokenClassification"),wVr.forEach(t),owo=r(eEe," (DeBERTa model)"),eEe.forEach(t),rwo=i(q),sb=n(q,"LI",{});var oEe=s(sb);aae=n(oEe,"STRONG",{});var AVr=s(aae);two=r(AVr,"deberta-v2"),AVr.forEach(t),awo=r(oEe," \u2014 "),qI=n(oEe,"A",{href:!0});var LVr=s(qI);nwo=r(LVr,"DebertaV2ForTokenClassification"),LVr.forEach(t),swo=r(oEe," (DeBERTa-v2 model)"),oEe.forEach(t),lwo=i(q),lb=n(q,"LI",{});var rEe=s(lb);nae=n(rEe,"STRONG",{});var BVr=s(nae);iwo=r(BVr,"distilbert"),BVr.forEach(t),dwo=r(rEe," \u2014 "),GI=n(rEe,"A",{href:!0});var kVr=s(GI);cwo=r(kVr,"DistilBertForTokenClassification"),kVr.forEach(t),fwo=r(rEe," (DistilBERT model)"),rEe.forEach(t),mwo=i(q),ib=n(q,"LI",{});var tEe=s(ib);sae=n(tEe,"STRONG",{});var xVr=s(sae);gwo=r(xVr,"electra"),xVr.forEach(t),hwo=r(tEe," \u2014 "),OI=n(tEe,"A",{href:!0});var RVr=s(OI);pwo=r(RVr,"ElectraForTokenClassification"),RVr.forEach(t),_wo=r(tEe," (ELECTRA model)"),tEe.forEach(t),uwo=i(q),db=n(q,"LI",{});var aEe=s(db);lae=n(aEe,"STRONG",{});var SVr=s(lae);bwo=r(SVr,"flaubert"),SVr.forEach(t),vwo=r(aEe," \u2014 "),XI=n(aEe,"A",{href:!0});var PVr=s(XI);Two=r(PVr,"FlaubertForTokenClassification"),PVr.forEach(t),Fwo=r(aEe," (FlauBERT model)"),aEe.forEach(t),Cwo=i(q),cb=n(q,"LI",{});var nEe=s(cb);iae=n(nEe,"STRONG",{});var $Vr=s(iae);Mwo=r($Vr,"fnet"),$Vr.forEach(t),Ewo=r(nEe," \u2014 "),zI=n(nEe,"A",{href:!0});var IVr=s(zI);ywo=r(IVr,"FNetForTokenClassification"),IVr.forEach(t),wwo=r(nEe," (FNet model)"),nEe.forEach(t),Awo=i(q),fb=n(q,"LI",{});var sEe=s(fb);dae=n(sEe,"STRONG",{});var jVr=s(dae);Lwo=r(jVr,"funnel"),jVr.forEach(t),Bwo=r(sEe," \u2014 "),VI=n(sEe,"A",{href:!0});var NVr=s(VI);kwo=r(NVr,"FunnelForTokenClassification"),NVr.forEach(t),xwo=r(sEe," (Funnel Transformer model)"),sEe.forEach(t),Rwo=i(q),mb=n(q,"LI",{});var lEe=s(mb);cae=n(lEe,"STRONG",{});var DVr=s(cae);Swo=r(DVr,"gpt2"),DVr.forEach(t),Pwo=r(lEe," \u2014 "),WI=n(lEe,"A",{href:!0});var qVr=s(WI);$wo=r(qVr,"GPT2ForTokenClassification"),qVr.forEach(t),Iwo=r(lEe," (OpenAI GPT-2 model)"),lEe.forEach(t),jwo=i(q),gb=n(q,"LI",{});var iEe=s(gb);fae=n(iEe,"STRONG",{});var GVr=s(fae);Nwo=r(GVr,"ibert"),GVr.forEach(t),Dwo=r(iEe," \u2014 "),QI=n(iEe,"A",{href:!0});var OVr=s(QI);qwo=r(OVr,"IBertForTokenClassification"),OVr.forEach(t),Gwo=r(iEe," (I-BERT model)"),iEe.forEach(t),Owo=i(q),hb=n(q,"LI",{});var dEe=s(hb);mae=n(dEe,"STRONG",{});var XVr=s(mae);Xwo=r(XVr,"layoutlm"),XVr.forEach(t),zwo=r(dEe," \u2014 "),HI=n(dEe,"A",{href:!0});var zVr=s(HI);Vwo=r(zVr,"LayoutLMForTokenClassification"),zVr.forEach(t),Wwo=r(dEe," (LayoutLM model)"),dEe.forEach(t),Qwo=i(q),pb=n(q,"LI",{});var cEe=s(pb);gae=n(cEe,"STRONG",{});var VVr=s(gae);Hwo=r(VVr,"layoutlmv2"),VVr.forEach(t),Uwo=r(cEe," \u2014 "),UI=n(cEe,"A",{href:!0});var WVr=s(UI);Jwo=r(WVr,"LayoutLMv2ForTokenClassification"),WVr.forEach(t),Ywo=r(cEe," (LayoutLMv2 model)"),cEe.forEach(t),Kwo=i(q),_b=n(q,"LI",{});var fEe=s(_b);hae=n(fEe,"STRONG",{});var QVr=s(hae);Zwo=r(QVr,"longformer"),QVr.forEach(t),e6o=r(fEe," \u2014 "),JI=n(fEe,"A",{href:!0});var HVr=s(JI);o6o=r(HVr,"LongformerForTokenClassification"),HVr.forEach(t),r6o=r(fEe," (Longformer model)"),fEe.forEach(t),t6o=i(q),ub=n(q,"LI",{});var mEe=s(ub);pae=n(mEe,"STRONG",{});var UVr=s(pae);a6o=r(UVr,"megatron-bert"),UVr.forEach(t),n6o=r(mEe," \u2014 "),YI=n(mEe,"A",{href:!0});var JVr=s(YI);s6o=r(JVr,"MegatronBertForTokenClassification"),JVr.forEach(t),l6o=r(mEe," (MegatronBert model)"),mEe.forEach(t),i6o=i(q),bb=n(q,"LI",{});var gEe=s(bb);_ae=n(gEe,"STRONG",{});var YVr=s(_ae);d6o=r(YVr,"mobilebert"),YVr.forEach(t),c6o=r(gEe," \u2014 "),KI=n(gEe,"A",{href:!0});var KVr=s(KI);f6o=r(KVr,"MobileBertForTokenClassification"),KVr.forEach(t),m6o=r(gEe," (MobileBERT model)"),gEe.forEach(t),g6o=i(q),vb=n(q,"LI",{});var hEe=s(vb);uae=n(hEe,"STRONG",{});var ZVr=s(uae);h6o=r(ZVr,"mpnet"),ZVr.forEach(t),p6o=r(hEe," \u2014 "),ZI=n(hEe,"A",{href:!0});var eWr=s(ZI);_6o=r(eWr,"MPNetForTokenClassification"),eWr.forEach(t),u6o=r(hEe," (MPNet model)"),hEe.forEach(t),b6o=i(q),Tb=n(q,"LI",{});var pEe=s(Tb);bae=n(pEe,"STRONG",{});var oWr=s(bae);v6o=r(oWr,"nystromformer"),oWr.forEach(t),T6o=r(pEe," \u2014 "),ej=n(pEe,"A",{href:!0});var rWr=s(ej);F6o=r(rWr,"NystromformerForTokenClassification"),rWr.forEach(t),C6o=r(pEe," (Nystromformer model)"),pEe.forEach(t),M6o=i(q),Fb=n(q,"LI",{});var _Ee=s(Fb);vae=n(_Ee,"STRONG",{});var tWr=s(vae);E6o=r(tWr,"qdqbert"),tWr.forEach(t),y6o=r(_Ee," \u2014 "),oj=n(_Ee,"A",{href:!0});var aWr=s(oj);w6o=r(aWr,"QDQBertForTokenClassification"),aWr.forEach(t),A6o=r(_Ee," (QDQBert model)"),_Ee.forEach(t),L6o=i(q),Cb=n(q,"LI",{});var uEe=s(Cb);Tae=n(uEe,"STRONG",{});var nWr=s(Tae);B6o=r(nWr,"rembert"),nWr.forEach(t),k6o=r(uEe," \u2014 "),rj=n(uEe,"A",{href:!0});var sWr=s(rj);x6o=r(sWr,"RemBertForTokenClassification"),sWr.forEach(t),R6o=r(uEe," (RemBERT model)"),uEe.forEach(t),S6o=i(q),Mb=n(q,"LI",{});var bEe=s(Mb);Fae=n(bEe,"STRONG",{});var lWr=s(Fae);P6o=r(lWr,"roberta"),lWr.forEach(t),$6o=r(bEe," \u2014 "),tj=n(bEe,"A",{href:!0});var iWr=s(tj);I6o=r(iWr,"RobertaForTokenClassification"),iWr.forEach(t),j6o=r(bEe," (RoBERTa model)"),bEe.forEach(t),N6o=i(q),Eb=n(q,"LI",{});var vEe=s(Eb);Cae=n(vEe,"STRONG",{});var dWr=s(Cae);D6o=r(dWr,"roformer"),dWr.forEach(t),q6o=r(vEe," \u2014 "),aj=n(vEe,"A",{href:!0});var cWr=s(aj);G6o=r(cWr,"RoFormerForTokenClassification"),cWr.forEach(t),O6o=r(vEe," (RoFormer model)"),vEe.forEach(t),X6o=i(q),yb=n(q,"LI",{});var TEe=s(yb);Mae=n(TEe,"STRONG",{});var fWr=s(Mae);z6o=r(fWr,"squeezebert"),fWr.forEach(t),V6o=r(TEe," \u2014 "),nj=n(TEe,"A",{href:!0});var mWr=s(nj);W6o=r(mWr,"SqueezeBertForTokenClassification"),mWr.forEach(t),Q6o=r(TEe," (SqueezeBERT model)"),TEe.forEach(t),H6o=i(q),wb=n(q,"LI",{});var FEe=s(wb);Eae=n(FEe,"STRONG",{});var gWr=s(Eae);U6o=r(gWr,"xlm"),gWr.forEach(t),J6o=r(FEe," \u2014 "),sj=n(FEe,"A",{href:!0});var hWr=s(sj);Y6o=r(hWr,"XLMForTokenClassification"),hWr.forEach(t),K6o=r(FEe," (XLM model)"),FEe.forEach(t),Z6o=i(q),Ab=n(q,"LI",{});var CEe=s(Ab);yae=n(CEe,"STRONG",{});var pWr=s(yae);eAo=r(pWr,"xlm-roberta"),pWr.forEach(t),oAo=r(CEe," \u2014 "),lj=n(CEe,"A",{href:!0});var _Wr=s(lj);rAo=r(_Wr,"XLMRobertaForTokenClassification"),_Wr.forEach(t),tAo=r(CEe," (XLM-RoBERTa model)"),CEe.forEach(t),aAo=i(q),Lb=n(q,"LI",{});var MEe=s(Lb);wae=n(MEe,"STRONG",{});var uWr=s(wae);nAo=r(uWr,"xlm-roberta-xl"),uWr.forEach(t),sAo=r(MEe," \u2014 "),ij=n(MEe,"A",{href:!0});var bWr=s(ij);lAo=r(bWr,"XLMRobertaXLForTokenClassification"),bWr.forEach(t),iAo=r(MEe," (XLM-RoBERTa-XL model)"),MEe.forEach(t),dAo=i(q),Bb=n(q,"LI",{});var EEe=s(Bb);Aae=n(EEe,"STRONG",{});var vWr=s(Aae);cAo=r(vWr,"xlnet"),vWr.forEach(t),fAo=r(EEe," \u2014 "),dj=n(EEe,"A",{href:!0});var TWr=s(dj);mAo=r(TWr,"XLNetForTokenClassification"),TWr.forEach(t),gAo=r(EEe," (XLNet model)"),EEe.forEach(t),hAo=i(q),kb=n(q,"LI",{});var yEe=s(kb);Lae=n(yEe,"STRONG",{});var FWr=s(Lae);pAo=r(FWr,"yoso"),FWr.forEach(t),_Ao=r(yEe," \u2014 "),cj=n(yEe,"A",{href:!0});var CWr=s(cj);uAo=r(CWr,"YosoForTokenClassification"),CWr.forEach(t),bAo=r(yEe," (YOSO model)"),yEe.forEach(t),q.forEach(t),vAo=i(Gt),xb=n(Gt,"P",{});var wEe=s(xb);TAo=r(wEe,"The model is set in evaluation mode by default using "),Bae=n(wEe,"CODE",{});var MWr=s(Bae);FAo=r(MWr,"model.eval()"),MWr.forEach(t),CAo=r(wEe,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),kae=n(wEe,"CODE",{});var EWr=s(kae);MAo=r(EWr,"model.train()"),EWr.forEach(t),wEe.forEach(t),EAo=i(Gt),xae=n(Gt,"P",{});var yWr=s(xae);yAo=r(yWr,"Examples:"),yWr.forEach(t),wAo=i(Gt),m(QE.$$.fragment,Gt),Gt.forEach(t),ol.forEach(t),J8e=i(d),gd=n(d,"H2",{class:!0});var tke=s(gd);Rb=n(tke,"A",{id:!0,class:!0,href:!0});var wWr=s(Rb);Rae=n(wWr,"SPAN",{});var AWr=s(Rae);m(HE.$$.fragment,AWr),AWr.forEach(t),wWr.forEach(t),AAo=i(tke),Sae=n(tke,"SPAN",{});var LWr=s(Sae);LAo=r(LWr,"AutoModelForQuestionAnswering"),LWr.forEach(t),tke.forEach(t),Y8e=i(d),er=n(d,"DIV",{class:!0});var tl=s(er);m(UE.$$.fragment,tl),BAo=i(tl),hd=n(tl,"P",{});var bz=s(hd);kAo=r(bz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a question answering head) when created
with the `),Pae=n(bz,"CODE",{});var BWr=s(Pae);xAo=r(BWr,"from_pretrained()"),BWr.forEach(t),RAo=r(bz,"class method or the "),$ae=n(bz,"CODE",{});var kWr=s($ae);SAo=r(kWr,"from_config()"),kWr.forEach(t),PAo=r(bz,`class
method.`),bz.forEach(t),$Ao=i(tl),JE=n(tl,"P",{});var ake=s(JE);IAo=r(ake,"This class cannot be instantiated directly using "),Iae=n(ake,"CODE",{});var xWr=s(Iae);jAo=r(xWr,"__init__()"),xWr.forEach(t),NAo=r(ake," (throws an error)."),ake.forEach(t),DAo=i(tl),Qr=n(tl,"DIV",{class:!0});var al=s(Qr);m(YE.$$.fragment,al),qAo=i(al),jae=n(al,"P",{});var RWr=s(jae);GAo=r(RWr,"Instantiates one of the model classes of the library (with a question answering head) from a configuration."),RWr.forEach(t),OAo=i(al),pd=n(al,"P",{});var vz=s(pd);XAo=r(vz,`Note:
Loading a model from its configuration file does `),Nae=n(vz,"STRONG",{});var SWr=s(Nae);zAo=r(SWr,"not"),SWr.forEach(t),VAo=r(vz,` load the model weights. It only affects the
model\u2019s configuration. Use `),Dae=n(vz,"CODE",{});var PWr=s(Dae);WAo=r(PWr,"from_pretrained()"),PWr.forEach(t),QAo=r(vz,"to load the model weights."),vz.forEach(t),HAo=i(al),qae=n(al,"P",{});var $Wr=s(qae);UAo=r($Wr,"Examples:"),$Wr.forEach(t),JAo=i(al),m(KE.$$.fragment,al),al.forEach(t),YAo=i(tl),qe=n(tl,"DIV",{class:!0});var Ot=s(qe);m(ZE.$$.fragment,Ot),KAo=i(Ot),Gae=n(Ot,"P",{});var IWr=s(Gae);ZAo=r(IWr,"Instantiate one of the model classes of the library (with a question answering head) from a pretrained model."),IWr.forEach(t),eLo=i(Ot),Ha=n(Ot,"P",{});var T4=s(Ha);oLo=r(T4,"The model class to instantiate is selected based on the "),Oae=n(T4,"CODE",{});var jWr=s(Oae);rLo=r(jWr,"model_type"),jWr.forEach(t),tLo=r(T4,` property of the config object (either
passed as an argument or loaded from `),Xae=n(T4,"CODE",{});var NWr=s(Xae);aLo=r(NWr,"pretrained_model_name_or_path"),NWr.forEach(t),nLo=r(T4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),zae=n(T4,"CODE",{});var DWr=s(zae);sLo=r(DWr,"pretrained_model_name_or_path"),DWr.forEach(t),lLo=r(T4,":"),T4.forEach(t),iLo=i(Ot),R=n(Ot,"UL",{});var P=s(R);Sb=n(P,"LI",{});var AEe=s(Sb);Vae=n(AEe,"STRONG",{});var qWr=s(Vae);dLo=r(qWr,"albert"),qWr.forEach(t),cLo=r(AEe," \u2014 "),fj=n(AEe,"A",{href:!0});var GWr=s(fj);fLo=r(GWr,"AlbertForQuestionAnswering"),GWr.forEach(t),mLo=r(AEe," (ALBERT model)"),AEe.forEach(t),gLo=i(P),Pb=n(P,"LI",{});var LEe=s(Pb);Wae=n(LEe,"STRONG",{});var OWr=s(Wae);hLo=r(OWr,"bart"),OWr.forEach(t),pLo=r(LEe," \u2014 "),mj=n(LEe,"A",{href:!0});var XWr=s(mj);_Lo=r(XWr,"BartForQuestionAnswering"),XWr.forEach(t),uLo=r(LEe," (BART model)"),LEe.forEach(t),bLo=i(P),$b=n(P,"LI",{});var BEe=s($b);Qae=n(BEe,"STRONG",{});var zWr=s(Qae);vLo=r(zWr,"bert"),zWr.forEach(t),TLo=r(BEe," \u2014 "),gj=n(BEe,"A",{href:!0});var VWr=s(gj);FLo=r(VWr,"BertForQuestionAnswering"),VWr.forEach(t),CLo=r(BEe," (BERT model)"),BEe.forEach(t),MLo=i(P),Ib=n(P,"LI",{});var kEe=s(Ib);Hae=n(kEe,"STRONG",{});var WWr=s(Hae);ELo=r(WWr,"big_bird"),WWr.forEach(t),yLo=r(kEe," \u2014 "),hj=n(kEe,"A",{href:!0});var QWr=s(hj);wLo=r(QWr,"BigBirdForQuestionAnswering"),QWr.forEach(t),ALo=r(kEe," (BigBird model)"),kEe.forEach(t),LLo=i(P),jb=n(P,"LI",{});var xEe=s(jb);Uae=n(xEe,"STRONG",{});var HWr=s(Uae);BLo=r(HWr,"bigbird_pegasus"),HWr.forEach(t),kLo=r(xEe," \u2014 "),pj=n(xEe,"A",{href:!0});var UWr=s(pj);xLo=r(UWr,"BigBirdPegasusForQuestionAnswering"),UWr.forEach(t),RLo=r(xEe," (BigBirdPegasus model)"),xEe.forEach(t),SLo=i(P),Nb=n(P,"LI",{});var REe=s(Nb);Jae=n(REe,"STRONG",{});var JWr=s(Jae);PLo=r(JWr,"camembert"),JWr.forEach(t),$Lo=r(REe," \u2014 "),_j=n(REe,"A",{href:!0});var YWr=s(_j);ILo=r(YWr,"CamembertForQuestionAnswering"),YWr.forEach(t),jLo=r(REe," (CamemBERT model)"),REe.forEach(t),NLo=i(P),Db=n(P,"LI",{});var SEe=s(Db);Yae=n(SEe,"STRONG",{});var KWr=s(Yae);DLo=r(KWr,"canine"),KWr.forEach(t),qLo=r(SEe," \u2014 "),uj=n(SEe,"A",{href:!0});var ZWr=s(uj);GLo=r(ZWr,"CanineForQuestionAnswering"),ZWr.forEach(t),OLo=r(SEe," (Canine model)"),SEe.forEach(t),XLo=i(P),qb=n(P,"LI",{});var PEe=s(qb);Kae=n(PEe,"STRONG",{});var eQr=s(Kae);zLo=r(eQr,"convbert"),eQr.forEach(t),VLo=r(PEe," \u2014 "),bj=n(PEe,"A",{href:!0});var oQr=s(bj);WLo=r(oQr,"ConvBertForQuestionAnswering"),oQr.forEach(t),QLo=r(PEe," (ConvBERT model)"),PEe.forEach(t),HLo=i(P),Gb=n(P,"LI",{});var $Ee=s(Gb);Zae=n($Ee,"STRONG",{});var rQr=s(Zae);ULo=r(rQr,"deberta"),rQr.forEach(t),JLo=r($Ee," \u2014 "),vj=n($Ee,"A",{href:!0});var tQr=s(vj);YLo=r(tQr,"DebertaForQuestionAnswering"),tQr.forEach(t),KLo=r($Ee," (DeBERTa model)"),$Ee.forEach(t),ZLo=i(P),Ob=n(P,"LI",{});var IEe=s(Ob);ene=n(IEe,"STRONG",{});var aQr=s(ene);e8o=r(aQr,"deberta-v2"),aQr.forEach(t),o8o=r(IEe," \u2014 "),Tj=n(IEe,"A",{href:!0});var nQr=s(Tj);r8o=r(nQr,"DebertaV2ForQuestionAnswering"),nQr.forEach(t),t8o=r(IEe," (DeBERTa-v2 model)"),IEe.forEach(t),a8o=i(P),Xb=n(P,"LI",{});var jEe=s(Xb);one=n(jEe,"STRONG",{});var sQr=s(one);n8o=r(sQr,"distilbert"),sQr.forEach(t),s8o=r(jEe," \u2014 "),Fj=n(jEe,"A",{href:!0});var lQr=s(Fj);l8o=r(lQr,"DistilBertForQuestionAnswering"),lQr.forEach(t),i8o=r(jEe," (DistilBERT model)"),jEe.forEach(t),d8o=i(P),zb=n(P,"LI",{});var NEe=s(zb);rne=n(NEe,"STRONG",{});var iQr=s(rne);c8o=r(iQr,"electra"),iQr.forEach(t),f8o=r(NEe," \u2014 "),Cj=n(NEe,"A",{href:!0});var dQr=s(Cj);m8o=r(dQr,"ElectraForQuestionAnswering"),dQr.forEach(t),g8o=r(NEe," (ELECTRA model)"),NEe.forEach(t),h8o=i(P),Vb=n(P,"LI",{});var DEe=s(Vb);tne=n(DEe,"STRONG",{});var cQr=s(tne);p8o=r(cQr,"flaubert"),cQr.forEach(t),_8o=r(DEe," \u2014 "),Mj=n(DEe,"A",{href:!0});var fQr=s(Mj);u8o=r(fQr,"FlaubertForQuestionAnsweringSimple"),fQr.forEach(t),b8o=r(DEe," (FlauBERT model)"),DEe.forEach(t),v8o=i(P),Wb=n(P,"LI",{});var qEe=s(Wb);ane=n(qEe,"STRONG",{});var mQr=s(ane);T8o=r(mQr,"fnet"),mQr.forEach(t),F8o=r(qEe," \u2014 "),Ej=n(qEe,"A",{href:!0});var gQr=s(Ej);C8o=r(gQr,"FNetForQuestionAnswering"),gQr.forEach(t),M8o=r(qEe," (FNet model)"),qEe.forEach(t),E8o=i(P),Qb=n(P,"LI",{});var GEe=s(Qb);nne=n(GEe,"STRONG",{});var hQr=s(nne);y8o=r(hQr,"funnel"),hQr.forEach(t),w8o=r(GEe," \u2014 "),yj=n(GEe,"A",{href:!0});var pQr=s(yj);A8o=r(pQr,"FunnelForQuestionAnswering"),pQr.forEach(t),L8o=r(GEe," (Funnel Transformer model)"),GEe.forEach(t),B8o=i(P),Hb=n(P,"LI",{});var OEe=s(Hb);sne=n(OEe,"STRONG",{});var _Qr=s(sne);k8o=r(_Qr,"gptj"),_Qr.forEach(t),x8o=r(OEe," \u2014 "),wj=n(OEe,"A",{href:!0});var uQr=s(wj);R8o=r(uQr,"GPTJForQuestionAnswering"),uQr.forEach(t),S8o=r(OEe," (GPT-J model)"),OEe.forEach(t),P8o=i(P),Ub=n(P,"LI",{});var XEe=s(Ub);lne=n(XEe,"STRONG",{});var bQr=s(lne);$8o=r(bQr,"ibert"),bQr.forEach(t),I8o=r(XEe," \u2014 "),Aj=n(XEe,"A",{href:!0});var vQr=s(Aj);j8o=r(vQr,"IBertForQuestionAnswering"),vQr.forEach(t),N8o=r(XEe," (I-BERT model)"),XEe.forEach(t),D8o=i(P),Jb=n(P,"LI",{});var zEe=s(Jb);ine=n(zEe,"STRONG",{});var TQr=s(ine);q8o=r(TQr,"layoutlmv2"),TQr.forEach(t),G8o=r(zEe," \u2014 "),Lj=n(zEe,"A",{href:!0});var FQr=s(Lj);O8o=r(FQr,"LayoutLMv2ForQuestionAnswering"),FQr.forEach(t),X8o=r(zEe," (LayoutLMv2 model)"),zEe.forEach(t),z8o=i(P),Yb=n(P,"LI",{});var VEe=s(Yb);dne=n(VEe,"STRONG",{});var CQr=s(dne);V8o=r(CQr,"led"),CQr.forEach(t),W8o=r(VEe," \u2014 "),Bj=n(VEe,"A",{href:!0});var MQr=s(Bj);Q8o=r(MQr,"LEDForQuestionAnswering"),MQr.forEach(t),H8o=r(VEe," (LED model)"),VEe.forEach(t),U8o=i(P),Kb=n(P,"LI",{});var WEe=s(Kb);cne=n(WEe,"STRONG",{});var EQr=s(cne);J8o=r(EQr,"longformer"),EQr.forEach(t),Y8o=r(WEe," \u2014 "),kj=n(WEe,"A",{href:!0});var yQr=s(kj);K8o=r(yQr,"LongformerForQuestionAnswering"),yQr.forEach(t),Z8o=r(WEe," (Longformer model)"),WEe.forEach(t),e9o=i(P),Zb=n(P,"LI",{});var QEe=s(Zb);fne=n(QEe,"STRONG",{});var wQr=s(fne);o9o=r(wQr,"lxmert"),wQr.forEach(t),r9o=r(QEe," \u2014 "),xj=n(QEe,"A",{href:!0});var AQr=s(xj);t9o=r(AQr,"LxmertForQuestionAnswering"),AQr.forEach(t),a9o=r(QEe," (LXMERT model)"),QEe.forEach(t),n9o=i(P),e5=n(P,"LI",{});var HEe=s(e5);mne=n(HEe,"STRONG",{});var LQr=s(mne);s9o=r(LQr,"mbart"),LQr.forEach(t),l9o=r(HEe," \u2014 "),Rj=n(HEe,"A",{href:!0});var BQr=s(Rj);i9o=r(BQr,"MBartForQuestionAnswering"),BQr.forEach(t),d9o=r(HEe," (mBART model)"),HEe.forEach(t),c9o=i(P),o5=n(P,"LI",{});var UEe=s(o5);gne=n(UEe,"STRONG",{});var kQr=s(gne);f9o=r(kQr,"megatron-bert"),kQr.forEach(t),m9o=r(UEe," \u2014 "),Sj=n(UEe,"A",{href:!0});var xQr=s(Sj);g9o=r(xQr,"MegatronBertForQuestionAnswering"),xQr.forEach(t),h9o=r(UEe," (MegatronBert model)"),UEe.forEach(t),p9o=i(P),r5=n(P,"LI",{});var JEe=s(r5);hne=n(JEe,"STRONG",{});var RQr=s(hne);_9o=r(RQr,"mobilebert"),RQr.forEach(t),u9o=r(JEe," \u2014 "),Pj=n(JEe,"A",{href:!0});var SQr=s(Pj);b9o=r(SQr,"MobileBertForQuestionAnswering"),SQr.forEach(t),v9o=r(JEe," (MobileBERT model)"),JEe.forEach(t),T9o=i(P),t5=n(P,"LI",{});var YEe=s(t5);pne=n(YEe,"STRONG",{});var PQr=s(pne);F9o=r(PQr,"mpnet"),PQr.forEach(t),C9o=r(YEe," \u2014 "),$j=n(YEe,"A",{href:!0});var $Qr=s($j);M9o=r($Qr,"MPNetForQuestionAnswering"),$Qr.forEach(t),E9o=r(YEe," (MPNet model)"),YEe.forEach(t),y9o=i(P),a5=n(P,"LI",{});var KEe=s(a5);_ne=n(KEe,"STRONG",{});var IQr=s(_ne);w9o=r(IQr,"nystromformer"),IQr.forEach(t),A9o=r(KEe," \u2014 "),Ij=n(KEe,"A",{href:!0});var jQr=s(Ij);L9o=r(jQr,"NystromformerForQuestionAnswering"),jQr.forEach(t),B9o=r(KEe," (Nystromformer model)"),KEe.forEach(t),k9o=i(P),n5=n(P,"LI",{});var ZEe=s(n5);une=n(ZEe,"STRONG",{});var NQr=s(une);x9o=r(NQr,"qdqbert"),NQr.forEach(t),R9o=r(ZEe," \u2014 "),jj=n(ZEe,"A",{href:!0});var DQr=s(jj);S9o=r(DQr,"QDQBertForQuestionAnswering"),DQr.forEach(t),P9o=r(ZEe," (QDQBert model)"),ZEe.forEach(t),$9o=i(P),s5=n(P,"LI",{});var e3e=s(s5);bne=n(e3e,"STRONG",{});var qQr=s(bne);I9o=r(qQr,"reformer"),qQr.forEach(t),j9o=r(e3e," \u2014 "),Nj=n(e3e,"A",{href:!0});var GQr=s(Nj);N9o=r(GQr,"ReformerForQuestionAnswering"),GQr.forEach(t),D9o=r(e3e," (Reformer model)"),e3e.forEach(t),q9o=i(P),l5=n(P,"LI",{});var o3e=s(l5);vne=n(o3e,"STRONG",{});var OQr=s(vne);G9o=r(OQr,"rembert"),OQr.forEach(t),O9o=r(o3e," \u2014 "),Dj=n(o3e,"A",{href:!0});var XQr=s(Dj);X9o=r(XQr,"RemBertForQuestionAnswering"),XQr.forEach(t),z9o=r(o3e," (RemBERT model)"),o3e.forEach(t),V9o=i(P),i5=n(P,"LI",{});var r3e=s(i5);Tne=n(r3e,"STRONG",{});var zQr=s(Tne);W9o=r(zQr,"roberta"),zQr.forEach(t),Q9o=r(r3e," \u2014 "),qj=n(r3e,"A",{href:!0});var VQr=s(qj);H9o=r(VQr,"RobertaForQuestionAnswering"),VQr.forEach(t),U9o=r(r3e," (RoBERTa model)"),r3e.forEach(t),J9o=i(P),d5=n(P,"LI",{});var t3e=s(d5);Fne=n(t3e,"STRONG",{});var WQr=s(Fne);Y9o=r(WQr,"roformer"),WQr.forEach(t),K9o=r(t3e," \u2014 "),Gj=n(t3e,"A",{href:!0});var QQr=s(Gj);Z9o=r(QQr,"RoFormerForQuestionAnswering"),QQr.forEach(t),eBo=r(t3e," (RoFormer model)"),t3e.forEach(t),oBo=i(P),c5=n(P,"LI",{});var a3e=s(c5);Cne=n(a3e,"STRONG",{});var HQr=s(Cne);rBo=r(HQr,"splinter"),HQr.forEach(t),tBo=r(a3e," \u2014 "),Oj=n(a3e,"A",{href:!0});var UQr=s(Oj);aBo=r(UQr,"SplinterForQuestionAnswering"),UQr.forEach(t),nBo=r(a3e," (Splinter model)"),a3e.forEach(t),sBo=i(P),f5=n(P,"LI",{});var n3e=s(f5);Mne=n(n3e,"STRONG",{});var JQr=s(Mne);lBo=r(JQr,"squeezebert"),JQr.forEach(t),iBo=r(n3e," \u2014 "),Xj=n(n3e,"A",{href:!0});var YQr=s(Xj);dBo=r(YQr,"SqueezeBertForQuestionAnswering"),YQr.forEach(t),cBo=r(n3e," (SqueezeBERT model)"),n3e.forEach(t),fBo=i(P),m5=n(P,"LI",{});var s3e=s(m5);Ene=n(s3e,"STRONG",{});var KQr=s(Ene);mBo=r(KQr,"xlm"),KQr.forEach(t),gBo=r(s3e," \u2014 "),zj=n(s3e,"A",{href:!0});var ZQr=s(zj);hBo=r(ZQr,"XLMForQuestionAnsweringSimple"),ZQr.forEach(t),pBo=r(s3e," (XLM model)"),s3e.forEach(t),_Bo=i(P),g5=n(P,"LI",{});var l3e=s(g5);yne=n(l3e,"STRONG",{});var eHr=s(yne);uBo=r(eHr,"xlm-roberta"),eHr.forEach(t),bBo=r(l3e," \u2014 "),Vj=n(l3e,"A",{href:!0});var oHr=s(Vj);vBo=r(oHr,"XLMRobertaForQuestionAnswering"),oHr.forEach(t),TBo=r(l3e," (XLM-RoBERTa model)"),l3e.forEach(t),FBo=i(P),h5=n(P,"LI",{});var i3e=s(h5);wne=n(i3e,"STRONG",{});var rHr=s(wne);CBo=r(rHr,"xlm-roberta-xl"),rHr.forEach(t),MBo=r(i3e," \u2014 "),Wj=n(i3e,"A",{href:!0});var tHr=s(Wj);EBo=r(tHr,"XLMRobertaXLForQuestionAnswering"),tHr.forEach(t),yBo=r(i3e," (XLM-RoBERTa-XL model)"),i3e.forEach(t),wBo=i(P),p5=n(P,"LI",{});var d3e=s(p5);Ane=n(d3e,"STRONG",{});var aHr=s(Ane);ABo=r(aHr,"xlnet"),aHr.forEach(t),LBo=r(d3e," \u2014 "),Qj=n(d3e,"A",{href:!0});var nHr=s(Qj);BBo=r(nHr,"XLNetForQuestionAnsweringSimple"),nHr.forEach(t),kBo=r(d3e," (XLNet model)"),d3e.forEach(t),xBo=i(P),_5=n(P,"LI",{});var c3e=s(_5);Lne=n(c3e,"STRONG",{});var sHr=s(Lne);RBo=r(sHr,"yoso"),sHr.forEach(t),SBo=r(c3e," \u2014 "),Hj=n(c3e,"A",{href:!0});var lHr=s(Hj);PBo=r(lHr,"YosoForQuestionAnswering"),lHr.forEach(t),$Bo=r(c3e," (YOSO model)"),c3e.forEach(t),P.forEach(t),IBo=i(Ot),u5=n(Ot,"P",{});var f3e=s(u5);jBo=r(f3e,"The model is set in evaluation mode by default using "),Bne=n(f3e,"CODE",{});var iHr=s(Bne);NBo=r(iHr,"model.eval()"),iHr.forEach(t),DBo=r(f3e,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),kne=n(f3e,"CODE",{});var dHr=s(kne);qBo=r(dHr,"model.train()"),dHr.forEach(t),f3e.forEach(t),GBo=i(Ot),xne=n(Ot,"P",{});var cHr=s(xne);OBo=r(cHr,"Examples:"),cHr.forEach(t),XBo=i(Ot),m(e3.$$.fragment,Ot),Ot.forEach(t),tl.forEach(t),K8e=i(d),_d=n(d,"H2",{class:!0});var nke=s(_d);b5=n(nke,"A",{id:!0,class:!0,href:!0});var fHr=s(b5);Rne=n(fHr,"SPAN",{});var mHr=s(Rne);m(o3.$$.fragment,mHr),mHr.forEach(t),fHr.forEach(t),zBo=i(nke),Sne=n(nke,"SPAN",{});var gHr=s(Sne);VBo=r(gHr,"AutoModelForTableQuestionAnswering"),gHr.forEach(t),nke.forEach(t),Z8e=i(d),or=n(d,"DIV",{class:!0});var nl=s(or);m(r3.$$.fragment,nl),WBo=i(nl),ud=n(nl,"P",{});var Tz=s(ud);QBo=r(Tz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a table question answering head) when created
with the `),Pne=n(Tz,"CODE",{});var hHr=s(Pne);HBo=r(hHr,"from_pretrained()"),hHr.forEach(t),UBo=r(Tz,"class method or the "),$ne=n(Tz,"CODE",{});var pHr=s($ne);JBo=r(pHr,"from_config()"),pHr.forEach(t),YBo=r(Tz,`class
method.`),Tz.forEach(t),KBo=i(nl),t3=n(nl,"P",{});var ske=s(t3);ZBo=r(ske,"This class cannot be instantiated directly using "),Ine=n(ske,"CODE",{});var _Hr=s(Ine);eko=r(_Hr,"__init__()"),_Hr.forEach(t),oko=r(ske," (throws an error)."),ske.forEach(t),rko=i(nl),Hr=n(nl,"DIV",{class:!0});var sl=s(Hr);m(a3.$$.fragment,sl),tko=i(sl),jne=n(sl,"P",{});var uHr=s(jne);ako=r(uHr,"Instantiates one of the model classes of the library (with a table question answering head) from a configuration."),uHr.forEach(t),nko=i(sl),bd=n(sl,"P",{});var Fz=s(bd);sko=r(Fz,`Note:
Loading a model from its configuration file does `),Nne=n(Fz,"STRONG",{});var bHr=s(Nne);lko=r(bHr,"not"),bHr.forEach(t),iko=r(Fz,` load the model weights. It only affects the
model\u2019s configuration. Use `),Dne=n(Fz,"CODE",{});var vHr=s(Dne);dko=r(vHr,"from_pretrained()"),vHr.forEach(t),cko=r(Fz,"to load the model weights."),Fz.forEach(t),fko=i(sl),qne=n(sl,"P",{});var THr=s(qne);mko=r(THr,"Examples:"),THr.forEach(t),gko=i(sl),m(n3.$$.fragment,sl),sl.forEach(t),hko=i(nl),Ge=n(nl,"DIV",{class:!0});var Xt=s(Ge);m(s3.$$.fragment,Xt),pko=i(Xt),Gne=n(Xt,"P",{});var FHr=s(Gne);_ko=r(FHr,"Instantiate one of the model classes of the library (with a table question answering head) from a pretrained model."),FHr.forEach(t),uko=i(Xt),Ua=n(Xt,"P",{});var F4=s(Ua);bko=r(F4,"The model class to instantiate is selected based on the "),One=n(F4,"CODE",{});var CHr=s(One);vko=r(CHr,"model_type"),CHr.forEach(t),Tko=r(F4,` property of the config object (either
passed as an argument or loaded from `),Xne=n(F4,"CODE",{});var MHr=s(Xne);Fko=r(MHr,"pretrained_model_name_or_path"),MHr.forEach(t),Cko=r(F4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),zne=n(F4,"CODE",{});var EHr=s(zne);Mko=r(EHr,"pretrained_model_name_or_path"),EHr.forEach(t),Eko=r(F4,":"),F4.forEach(t),yko=i(Xt),Vne=n(Xt,"UL",{});var yHr=s(Vne);v5=n(yHr,"LI",{});var m3e=s(v5);Wne=n(m3e,"STRONG",{});var wHr=s(Wne);wko=r(wHr,"tapas"),wHr.forEach(t),Ako=r(m3e," \u2014 "),Uj=n(m3e,"A",{href:!0});var AHr=s(Uj);Lko=r(AHr,"TapasForQuestionAnswering"),AHr.forEach(t),Bko=r(m3e," (TAPAS model)"),m3e.forEach(t),yHr.forEach(t),kko=i(Xt),T5=n(Xt,"P",{});var g3e=s(T5);xko=r(g3e,"The model is set in evaluation mode by default using "),Qne=n(g3e,"CODE",{});var LHr=s(Qne);Rko=r(LHr,"model.eval()"),LHr.forEach(t),Sko=r(g3e,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Hne=n(g3e,"CODE",{});var BHr=s(Hne);Pko=r(BHr,"model.train()"),BHr.forEach(t),g3e.forEach(t),$ko=i(Xt),Une=n(Xt,"P",{});var kHr=s(Une);Iko=r(kHr,"Examples:"),kHr.forEach(t),jko=i(Xt),m(l3.$$.fragment,Xt),Xt.forEach(t),nl.forEach(t),e9e=i(d),vd=n(d,"H2",{class:!0});var lke=s(vd);F5=n(lke,"A",{id:!0,class:!0,href:!0});var xHr=s(F5);Jne=n(xHr,"SPAN",{});var RHr=s(Jne);m(i3.$$.fragment,RHr),RHr.forEach(t),xHr.forEach(t),Nko=i(lke),Yne=n(lke,"SPAN",{});var SHr=s(Yne);Dko=r(SHr,"AutoModelForImageClassification"),SHr.forEach(t),lke.forEach(t),o9e=i(d),rr=n(d,"DIV",{class:!0});var ll=s(rr);m(d3.$$.fragment,ll),qko=i(ll),Td=n(ll,"P",{});var Cz=s(Td);Gko=r(Cz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a image classification head) when created
with the `),Kne=n(Cz,"CODE",{});var PHr=s(Kne);Oko=r(PHr,"from_pretrained()"),PHr.forEach(t),Xko=r(Cz,"class method or the "),Zne=n(Cz,"CODE",{});var $Hr=s(Zne);zko=r($Hr,"from_config()"),$Hr.forEach(t),Vko=r(Cz,`class
method.`),Cz.forEach(t),Wko=i(ll),c3=n(ll,"P",{});var ike=s(c3);Qko=r(ike,"This class cannot be instantiated directly using "),ese=n(ike,"CODE",{});var IHr=s(ese);Hko=r(IHr,"__init__()"),IHr.forEach(t),Uko=r(ike," (throws an error)."),ike.forEach(t),Jko=i(ll),Ur=n(ll,"DIV",{class:!0});var il=s(Ur);m(f3.$$.fragment,il),Yko=i(il),ose=n(il,"P",{});var jHr=s(ose);Kko=r(jHr,"Instantiates one of the model classes of the library (with a image classification head) from a configuration."),jHr.forEach(t),Zko=i(il),Fd=n(il,"P",{});var Mz=s(Fd);exo=r(Mz,`Note:
Loading a model from its configuration file does `),rse=n(Mz,"STRONG",{});var NHr=s(rse);oxo=r(NHr,"not"),NHr.forEach(t),rxo=r(Mz,` load the model weights. It only affects the
model\u2019s configuration. Use `),tse=n(Mz,"CODE",{});var DHr=s(tse);txo=r(DHr,"from_pretrained()"),DHr.forEach(t),axo=r(Mz,"to load the model weights."),Mz.forEach(t),nxo=i(il),ase=n(il,"P",{});var qHr=s(ase);sxo=r(qHr,"Examples:"),qHr.forEach(t),lxo=i(il),m(m3.$$.fragment,il),il.forEach(t),ixo=i(ll),Oe=n(ll,"DIV",{class:!0});var zt=s(Oe);m(g3.$$.fragment,zt),dxo=i(zt),nse=n(zt,"P",{});var GHr=s(nse);cxo=r(GHr,"Instantiate one of the model classes of the library (with a image classification head) from a pretrained model."),GHr.forEach(t),fxo=i(zt),Ja=n(zt,"P",{});var C4=s(Ja);mxo=r(C4,"The model class to instantiate is selected based on the "),sse=n(C4,"CODE",{});var OHr=s(sse);gxo=r(OHr,"model_type"),OHr.forEach(t),hxo=r(C4,` property of the config object (either
passed as an argument or loaded from `),lse=n(C4,"CODE",{});var XHr=s(lse);pxo=r(XHr,"pretrained_model_name_or_path"),XHr.forEach(t),_xo=r(C4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),ise=n(C4,"CODE",{});var zHr=s(ise);uxo=r(zHr,"pretrained_model_name_or_path"),zHr.forEach(t),bxo=r(C4,":"),C4.forEach(t),vxo=i(zt),he=n(zt,"UL",{});var Me=s(he);C5=n(Me,"LI",{});var h3e=s(C5);dse=n(h3e,"STRONG",{});var VHr=s(dse);Txo=r(VHr,"beit"),VHr.forEach(t),Fxo=r(h3e," \u2014 "),Jj=n(h3e,"A",{href:!0});var WHr=s(Jj);Cxo=r(WHr,"BeitForImageClassification"),WHr.forEach(t),Mxo=r(h3e," (BEiT model)"),h3e.forEach(t),Exo=i(Me),M5=n(Me,"LI",{});var p3e=s(M5);cse=n(p3e,"STRONG",{});var QHr=s(cse);yxo=r(QHr,"convnext"),QHr.forEach(t),wxo=r(p3e," \u2014 "),Yj=n(p3e,"A",{href:!0});var HHr=s(Yj);Axo=r(HHr,"ConvNextForImageClassification"),HHr.forEach(t),Lxo=r(p3e," (ConvNext model)"),p3e.forEach(t),Bxo=i(Me),Rs=n(Me,"LI",{});var GL=s(Rs);fse=n(GL,"STRONG",{});var UHr=s(fse);kxo=r(UHr,"deit"),UHr.forEach(t),xxo=r(GL," \u2014 "),Kj=n(GL,"A",{href:!0});var JHr=s(Kj);Rxo=r(JHr,"DeiTForImageClassification"),JHr.forEach(t),Sxo=r(GL," or "),Zj=n(GL,"A",{href:!0});var YHr=s(Zj);Pxo=r(YHr,"DeiTForImageClassificationWithTeacher"),YHr.forEach(t),$xo=r(GL," (DeiT model)"),GL.forEach(t),Ixo=i(Me),E5=n(Me,"LI",{});var _3e=s(E5);mse=n(_3e,"STRONG",{});var KHr=s(mse);jxo=r(KHr,"imagegpt"),KHr.forEach(t),Nxo=r(_3e," \u2014 "),eN=n(_3e,"A",{href:!0});var ZHr=s(eN);Dxo=r(ZHr,"ImageGPTForImageClassification"),ZHr.forEach(t),qxo=r(_3e," (ImageGPT model)"),_3e.forEach(t),Gxo=i(Me),la=n(Me,"LI",{});var Mf=s(la);gse=n(Mf,"STRONG",{});var eUr=s(gse);Oxo=r(eUr,"perceiver"),eUr.forEach(t),Xxo=r(Mf," \u2014 "),oN=n(Mf,"A",{href:!0});var oUr=s(oN);zxo=r(oUr,"PerceiverForImageClassificationLearned"),oUr.forEach(t),Vxo=r(Mf," or "),rN=n(Mf,"A",{href:!0});var rUr=s(rN);Wxo=r(rUr,"PerceiverForImageClassificationFourier"),rUr.forEach(t),Qxo=r(Mf," or "),tN=n(Mf,"A",{href:!0});var tUr=s(tN);Hxo=r(tUr,"PerceiverForImageClassificationConvProcessing"),tUr.forEach(t),Uxo=r(Mf," (Perceiver model)"),Mf.forEach(t),Jxo=i(Me),y5=n(Me,"LI",{});var u3e=s(y5);hse=n(u3e,"STRONG",{});var aUr=s(hse);Yxo=r(aUr,"poolformer"),aUr.forEach(t),Kxo=r(u3e," \u2014 "),aN=n(u3e,"A",{href:!0});var nUr=s(aN);Zxo=r(nUr,"PoolFormerForImageClassification"),nUr.forEach(t),eRo=r(u3e," (PoolFormer model)"),u3e.forEach(t),oRo=i(Me),w5=n(Me,"LI",{});var b3e=s(w5);pse=n(b3e,"STRONG",{});var sUr=s(pse);rRo=r(sUr,"resnet"),sUr.forEach(t),tRo=r(b3e," \u2014 "),nN=n(b3e,"A",{href:!0});var lUr=s(nN);aRo=r(lUr,"ResNetForImageClassification"),lUr.forEach(t),nRo=r(b3e," (resnet model)"),b3e.forEach(t),sRo=i(Me),A5=n(Me,"LI",{});var v3e=s(A5);_se=n(v3e,"STRONG",{});var iUr=s(_se);lRo=r(iUr,"segformer"),iUr.forEach(t),iRo=r(v3e," \u2014 "),sN=n(v3e,"A",{href:!0});var dUr=s(sN);dRo=r(dUr,"SegformerForImageClassification"),dUr.forEach(t),cRo=r(v3e," (SegFormer model)"),v3e.forEach(t),fRo=i(Me),L5=n(Me,"LI",{});var T3e=s(L5);use=n(T3e,"STRONG",{});var cUr=s(use);mRo=r(cUr,"swin"),cUr.forEach(t),gRo=r(T3e," \u2014 "),lN=n(T3e,"A",{href:!0});var fUr=s(lN);hRo=r(fUr,"SwinForImageClassification"),fUr.forEach(t),pRo=r(T3e," (Swin model)"),T3e.forEach(t),_Ro=i(Me),B5=n(Me,"LI",{});var F3e=s(B5);bse=n(F3e,"STRONG",{});var mUr=s(bse);uRo=r(mUr,"vit"),mUr.forEach(t),bRo=r(F3e," \u2014 "),iN=n(F3e,"A",{href:!0});var gUr=s(iN);vRo=r(gUr,"ViTForImageClassification"),gUr.forEach(t),TRo=r(F3e," (ViT model)"),F3e.forEach(t),Me.forEach(t),FRo=i(zt),k5=n(zt,"P",{});var C3e=s(k5);CRo=r(C3e,"The model is set in evaluation mode by default using "),vse=n(C3e,"CODE",{});var hUr=s(vse);MRo=r(hUr,"model.eval()"),hUr.forEach(t),ERo=r(C3e,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Tse=n(C3e,"CODE",{});var pUr=s(Tse);yRo=r(pUr,"model.train()"),pUr.forEach(t),C3e.forEach(t),wRo=i(zt),Fse=n(zt,"P",{});var _Ur=s(Fse);ARo=r(_Ur,"Examples:"),_Ur.forEach(t),LRo=i(zt),m(h3.$$.fragment,zt),zt.forEach(t),ll.forEach(t),r9e=i(d),Cd=n(d,"H2",{class:!0});var dke=s(Cd);x5=n(dke,"A",{id:!0,class:!0,href:!0});var uUr=s(x5);Cse=n(uUr,"SPAN",{});var bUr=s(Cse);m(p3.$$.fragment,bUr),bUr.forEach(t),uUr.forEach(t),BRo=i(dke),Mse=n(dke,"SPAN",{});var vUr=s(Mse);kRo=r(vUr,"AutoModelForVision2Seq"),vUr.forEach(t),dke.forEach(t),t9e=i(d),tr=n(d,"DIV",{class:!0});var dl=s(tr);m(_3.$$.fragment,dl),xRo=i(dl),Md=n(dl,"P",{});var Ez=s(Md);RRo=r(Ez,`This is a generic model class that will be instantiated as one of the model classes of the library (with a vision-to-text modeling head) when created
with the `),Ese=n(Ez,"CODE",{});var TUr=s(Ese);SRo=r(TUr,"from_pretrained()"),TUr.forEach(t),PRo=r(Ez,"class method or the "),yse=n(Ez,"CODE",{});var FUr=s(yse);$Ro=r(FUr,"from_config()"),FUr.forEach(t),IRo=r(Ez,`class
method.`),Ez.forEach(t),jRo=i(dl),u3=n(dl,"P",{});var cke=s(u3);NRo=r(cke,"This class cannot be instantiated directly using "),wse=n(cke,"CODE",{});var CUr=s(wse);DRo=r(CUr,"__init__()"),CUr.forEach(t),qRo=r(cke," (throws an error)."),cke.forEach(t),GRo=i(dl),Jr=n(dl,"DIV",{class:!0});var cl=s(Jr);m(b3.$$.fragment,cl),ORo=i(cl),Ase=n(cl,"P",{});var MUr=s(Ase);XRo=r(MUr,"Instantiates one of the model classes of the library (with a vision-to-text modeling head) from a configuration."),MUr.forEach(t),zRo=i(cl),Ed=n(cl,"P",{});var yz=s(Ed);VRo=r(yz,`Note:
Loading a model from its configuration file does `),Lse=n(yz,"STRONG",{});var EUr=s(Lse);WRo=r(EUr,"not"),EUr.forEach(t),QRo=r(yz,` load the model weights. It only affects the
model\u2019s configuration. Use `),Bse=n(yz,"CODE",{});var yUr=s(Bse);HRo=r(yUr,"from_pretrained()"),yUr.forEach(t),URo=r(yz,"to load the model weights."),yz.forEach(t),JRo=i(cl),kse=n(cl,"P",{});var wUr=s(kse);YRo=r(wUr,"Examples:"),wUr.forEach(t),KRo=i(cl),m(v3.$$.fragment,cl),cl.forEach(t),ZRo=i(dl),Xe=n(dl,"DIV",{class:!0});var Vt=s(Xe);m(T3.$$.fragment,Vt),eSo=i(Vt),xse=n(Vt,"P",{});var AUr=s(xse);oSo=r(AUr,"Instantiate one of the model classes of the library (with a vision-to-text modeling head) from a pretrained model."),AUr.forEach(t),rSo=i(Vt),Ya=n(Vt,"P",{});var M4=s(Ya);tSo=r(M4,"The model class to instantiate is selected based on the "),Rse=n(M4,"CODE",{});var LUr=s(Rse);aSo=r(LUr,"model_type"),LUr.forEach(t),nSo=r(M4,` property of the config object (either
passed as an argument or loaded from `),Sse=n(M4,"CODE",{});var BUr=s(Sse);sSo=r(BUr,"pretrained_model_name_or_path"),BUr.forEach(t),lSo=r(M4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Pse=n(M4,"CODE",{});var kUr=s(Pse);iSo=r(kUr,"pretrained_model_name_or_path"),kUr.forEach(t),dSo=r(M4,":"),M4.forEach(t),cSo=i(Vt),$se=n(Vt,"UL",{});var xUr=s($se);R5=n(xUr,"LI",{});var M3e=s(R5);Ise=n(M3e,"STRONG",{});var RUr=s(Ise);fSo=r(RUr,"vision-encoder-decoder"),RUr.forEach(t),mSo=r(M3e," \u2014 "),dN=n(M3e,"A",{href:!0});var SUr=s(dN);gSo=r(SUr,"VisionEncoderDecoderModel"),SUr.forEach(t),hSo=r(M3e," (Vision Encoder decoder model)"),M3e.forEach(t),xUr.forEach(t),pSo=i(Vt),S5=n(Vt,"P",{});var E3e=s(S5);_So=r(E3e,"The model is set in evaluation mode by default using "),jse=n(E3e,"CODE",{});var PUr=s(jse);uSo=r(PUr,"model.eval()"),PUr.forEach(t),bSo=r(E3e,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Nse=n(E3e,"CODE",{});var $Ur=s(Nse);vSo=r($Ur,"model.train()"),$Ur.forEach(t),E3e.forEach(t),TSo=i(Vt),Dse=n(Vt,"P",{});var IUr=s(Dse);FSo=r(IUr,"Examples:"),IUr.forEach(t),CSo=i(Vt),m(F3.$$.fragment,Vt),Vt.forEach(t),dl.forEach(t),a9e=i(d),yd=n(d,"H2",{class:!0});var fke=s(yd);P5=n(fke,"A",{id:!0,class:!0,href:!0});var jUr=s(P5);qse=n(jUr,"SPAN",{});var NUr=s(qse);m(C3.$$.fragment,NUr),NUr.forEach(t),jUr.forEach(t),MSo=i(fke),Gse=n(fke,"SPAN",{});var DUr=s(Gse);ESo=r(DUr,"AutoModelForAudioClassification"),DUr.forEach(t),fke.forEach(t),n9e=i(d),ar=n(d,"DIV",{class:!0});var fl=s(ar);m(M3.$$.fragment,fl),ySo=i(fl),wd=n(fl,"P",{});var wz=s(wd);wSo=r(wz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a audio classification head) when created
with the `),Ose=n(wz,"CODE",{});var qUr=s(Ose);ASo=r(qUr,"from_pretrained()"),qUr.forEach(t),LSo=r(wz,"class method or the "),Xse=n(wz,"CODE",{});var GUr=s(Xse);BSo=r(GUr,"from_config()"),GUr.forEach(t),kSo=r(wz,`class
method.`),wz.forEach(t),xSo=i(fl),E3=n(fl,"P",{});var mke=s(E3);RSo=r(mke,"This class cannot be instantiated directly using "),zse=n(mke,"CODE",{});var OUr=s(zse);SSo=r(OUr,"__init__()"),OUr.forEach(t),PSo=r(mke," (throws an error)."),mke.forEach(t),$So=i(fl),Yr=n(fl,"DIV",{class:!0});var ml=s(Yr);m(y3.$$.fragment,ml),ISo=i(ml),Vse=n(ml,"P",{});var XUr=s(Vse);jSo=r(XUr,"Instantiates one of the model classes of the library (with a audio classification head) from a configuration."),XUr.forEach(t),NSo=i(ml),Ad=n(ml,"P",{});var Az=s(Ad);DSo=r(Az,`Note:
Loading a model from its configuration file does `),Wse=n(Az,"STRONG",{});var zUr=s(Wse);qSo=r(zUr,"not"),zUr.forEach(t),GSo=r(Az,` load the model weights. It only affects the
model\u2019s configuration. Use `),Qse=n(Az,"CODE",{});var VUr=s(Qse);OSo=r(VUr,"from_pretrained()"),VUr.forEach(t),XSo=r(Az,"to load the model weights."),Az.forEach(t),zSo=i(ml),Hse=n(ml,"P",{});var WUr=s(Hse);VSo=r(WUr,"Examples:"),WUr.forEach(t),WSo=i(ml),m(w3.$$.fragment,ml),ml.forEach(t),QSo=i(fl),ze=n(fl,"DIV",{class:!0});var Wt=s(ze);m(A3.$$.fragment,Wt),HSo=i(Wt),Use=n(Wt,"P",{});var QUr=s(Use);USo=r(QUr,"Instantiate one of the model classes of the library (with a audio classification head) from a pretrained model."),QUr.forEach(t),JSo=i(Wt),Ka=n(Wt,"P",{});var E4=s(Ka);YSo=r(E4,"The model class to instantiate is selected based on the "),Jse=n(E4,"CODE",{});var HUr=s(Jse);KSo=r(HUr,"model_type"),HUr.forEach(t),ZSo=r(E4,` property of the config object (either
passed as an argument or loaded from `),Yse=n(E4,"CODE",{});var UUr=s(Yse);ePo=r(UUr,"pretrained_model_name_or_path"),UUr.forEach(t),oPo=r(E4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Kse=n(E4,"CODE",{});var JUr=s(Kse);rPo=r(JUr,"pretrained_model_name_or_path"),JUr.forEach(t),tPo=r(E4,":"),E4.forEach(t),aPo=i(Wt),ao=n(Wt,"UL",{});var Qt=s(ao);$5=n(Qt,"LI",{});var y3e=s($5);Zse=n(y3e,"STRONG",{});var YUr=s(Zse);nPo=r(YUr,"hubert"),YUr.forEach(t),sPo=r(y3e," \u2014 "),cN=n(y3e,"A",{href:!0});var KUr=s(cN);lPo=r(KUr,"HubertForSequenceClassification"),KUr.forEach(t),iPo=r(y3e," (Hubert model)"),y3e.forEach(t),dPo=i(Qt),I5=n(Qt,"LI",{});var w3e=s(I5);ele=n(w3e,"STRONG",{});var ZUr=s(ele);cPo=r(ZUr,"sew"),ZUr.forEach(t),fPo=r(w3e," \u2014 "),fN=n(w3e,"A",{href:!0});var eJr=s(fN);mPo=r(eJr,"SEWForSequenceClassification"),eJr.forEach(t),gPo=r(w3e," (SEW model)"),w3e.forEach(t),hPo=i(Qt),j5=n(Qt,"LI",{});var A3e=s(j5);ole=n(A3e,"STRONG",{});var oJr=s(ole);pPo=r(oJr,"sew-d"),oJr.forEach(t),_Po=r(A3e," \u2014 "),mN=n(A3e,"A",{href:!0});var rJr=s(mN);uPo=r(rJr,"SEWDForSequenceClassification"),rJr.forEach(t),bPo=r(A3e," (SEW-D model)"),A3e.forEach(t),vPo=i(Qt),N5=n(Qt,"LI",{});var L3e=s(N5);rle=n(L3e,"STRONG",{});var tJr=s(rle);TPo=r(tJr,"unispeech"),tJr.forEach(t),FPo=r(L3e," \u2014 "),gN=n(L3e,"A",{href:!0});var aJr=s(gN);CPo=r(aJr,"UniSpeechForSequenceClassification"),aJr.forEach(t),MPo=r(L3e," (UniSpeech model)"),L3e.forEach(t),EPo=i(Qt),D5=n(Qt,"LI",{});var B3e=s(D5);tle=n(B3e,"STRONG",{});var nJr=s(tle);yPo=r(nJr,"unispeech-sat"),nJr.forEach(t),wPo=r(B3e," \u2014 "),hN=n(B3e,"A",{href:!0});var sJr=s(hN);APo=r(sJr,"UniSpeechSatForSequenceClassification"),sJr.forEach(t),LPo=r(B3e," (UniSpeechSat model)"),B3e.forEach(t),BPo=i(Qt),q5=n(Qt,"LI",{});var k3e=s(q5);ale=n(k3e,"STRONG",{});var lJr=s(ale);kPo=r(lJr,"wav2vec2"),lJr.forEach(t),xPo=r(k3e," \u2014 "),pN=n(k3e,"A",{href:!0});var iJr=s(pN);RPo=r(iJr,"Wav2Vec2ForSequenceClassification"),iJr.forEach(t),SPo=r(k3e," (Wav2Vec2 model)"),k3e.forEach(t),PPo=i(Qt),G5=n(Qt,"LI",{});var x3e=s(G5);nle=n(x3e,"STRONG",{});var dJr=s(nle);$Po=r(dJr,"wavlm"),dJr.forEach(t),IPo=r(x3e," \u2014 "),_N=n(x3e,"A",{href:!0});var cJr=s(_N);jPo=r(cJr,"WavLMForSequenceClassification"),cJr.forEach(t),NPo=r(x3e," (WavLM model)"),x3e.forEach(t),Qt.forEach(t),DPo=i(Wt),O5=n(Wt,"P",{});var R3e=s(O5);qPo=r(R3e,"The model is set in evaluation mode by default using "),sle=n(R3e,"CODE",{});var fJr=s(sle);GPo=r(fJr,"model.eval()"),fJr.forEach(t),OPo=r(R3e,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),lle=n(R3e,"CODE",{});var mJr=s(lle);XPo=r(mJr,"model.train()"),mJr.forEach(t),R3e.forEach(t),zPo=i(Wt),ile=n(Wt,"P",{});var gJr=s(ile);VPo=r(gJr,"Examples:"),gJr.forEach(t),WPo=i(Wt),m(L3.$$.fragment,Wt),Wt.forEach(t),fl.forEach(t),s9e=i(d),Ld=n(d,"H2",{class:!0});var gke=s(Ld);X5=n(gke,"A",{id:!0,class:!0,href:!0});var hJr=s(X5);dle=n(hJr,"SPAN",{});var pJr=s(dle);m(B3.$$.fragment,pJr),pJr.forEach(t),hJr.forEach(t),QPo=i(gke),cle=n(gke,"SPAN",{});var _Jr=s(cle);HPo=r(_Jr,"AutoModelForAudioFrameClassification"),_Jr.forEach(t),gke.forEach(t),l9e=i(d),nr=n(d,"DIV",{class:!0});var gl=s(nr);m(k3.$$.fragment,gl),UPo=i(gl),Bd=n(gl,"P",{});var Lz=s(Bd);JPo=r(Lz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a audio frame (token) classification head) when created
with the `),fle=n(Lz,"CODE",{});var uJr=s(fle);YPo=r(uJr,"from_pretrained()"),uJr.forEach(t),KPo=r(Lz,"class method or the "),mle=n(Lz,"CODE",{});var bJr=s(mle);ZPo=r(bJr,"from_config()"),bJr.forEach(t),e$o=r(Lz,`class
method.`),Lz.forEach(t),o$o=i(gl),x3=n(gl,"P",{});var hke=s(x3);r$o=r(hke,"This class cannot be instantiated directly using "),gle=n(hke,"CODE",{});var vJr=s(gle);t$o=r(vJr,"__init__()"),vJr.forEach(t),a$o=r(hke," (throws an error)."),hke.forEach(t),n$o=i(gl),Kr=n(gl,"DIV",{class:!0});var hl=s(Kr);m(R3.$$.fragment,hl),s$o=i(hl),hle=n(hl,"P",{});var TJr=s(hle);l$o=r(TJr,"Instantiates one of the model classes of the library (with a audio frame (token) classification head) from a configuration."),TJr.forEach(t),i$o=i(hl),kd=n(hl,"P",{});var Bz=s(kd);d$o=r(Bz,`Note:
Loading a model from its configuration file does `),ple=n(Bz,"STRONG",{});var FJr=s(ple);c$o=r(FJr,"not"),FJr.forEach(t),f$o=r(Bz,` load the model weights. It only affects the
model\u2019s configuration. Use `),_le=n(Bz,"CODE",{});var CJr=s(_le);m$o=r(CJr,"from_pretrained()"),CJr.forEach(t),g$o=r(Bz,"to load the model weights."),Bz.forEach(t),h$o=i(hl),ule=n(hl,"P",{});var MJr=s(ule);p$o=r(MJr,"Examples:"),MJr.forEach(t),_$o=i(hl),m(S3.$$.fragment,hl),hl.forEach(t),u$o=i(gl),Ve=n(gl,"DIV",{class:!0});var Ht=s(Ve);m(P3.$$.fragment,Ht),b$o=i(Ht),ble=n(Ht,"P",{});var EJr=s(ble);v$o=r(EJr,"Instantiate one of the model classes of the library (with a audio frame (token) classification head) from a pretrained model."),EJr.forEach(t),T$o=i(Ht),Za=n(Ht,"P",{});var y4=s(Za);F$o=r(y4,"The model class to instantiate is selected based on the "),vle=n(y4,"CODE",{});var yJr=s(vle);C$o=r(yJr,"model_type"),yJr.forEach(t),M$o=r(y4,` property of the config object (either
passed as an argument or loaded from `),Tle=n(y4,"CODE",{});var wJr=s(Tle);E$o=r(wJr,"pretrained_model_name_or_path"),wJr.forEach(t),y$o=r(y4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Fle=n(y4,"CODE",{});var AJr=s(Fle);w$o=r(AJr,"pretrained_model_name_or_path"),AJr.forEach(t),A$o=r(y4,":"),y4.forEach(t),L$o=i(Ht),xd=n(Ht,"UL",{});var kz=s(xd);z5=n(kz,"LI",{});var S3e=s(z5);Cle=n(S3e,"STRONG",{});var LJr=s(Cle);B$o=r(LJr,"unispeech-sat"),LJr.forEach(t),k$o=r(S3e," \u2014 "),uN=n(S3e,"A",{href:!0});var BJr=s(uN);x$o=r(BJr,"UniSpeechSatForAudioFrameClassification"),BJr.forEach(t),R$o=r(S3e," (UniSpeechSat model)"),S3e.forEach(t),S$o=i(kz),V5=n(kz,"LI",{});var P3e=s(V5);Mle=n(P3e,"STRONG",{});var kJr=s(Mle);P$o=r(kJr,"wav2vec2"),kJr.forEach(t),$$o=r(P3e," \u2014 "),bN=n(P3e,"A",{href:!0});var xJr=s(bN);I$o=r(xJr,"Wav2Vec2ForAudioFrameClassification"),xJr.forEach(t),j$o=r(P3e," (Wav2Vec2 model)"),P3e.forEach(t),N$o=i(kz),W5=n(kz,"LI",{});var $3e=s(W5);Ele=n($3e,"STRONG",{});var RJr=s(Ele);D$o=r(RJr,"wavlm"),RJr.forEach(t),q$o=r($3e," \u2014 "),vN=n($3e,"A",{href:!0});var SJr=s(vN);G$o=r(SJr,"WavLMForAudioFrameClassification"),SJr.forEach(t),O$o=r($3e," (WavLM model)"),$3e.forEach(t),kz.forEach(t),X$o=i(Ht),Q5=n(Ht,"P",{});var I3e=s(Q5);z$o=r(I3e,"The model is set in evaluation mode by default using "),yle=n(I3e,"CODE",{});var PJr=s(yle);V$o=r(PJr,"model.eval()"),PJr.forEach(t),W$o=r(I3e,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),wle=n(I3e,"CODE",{});var $Jr=s(wle);Q$o=r($Jr,"model.train()"),$Jr.forEach(t),I3e.forEach(t),H$o=i(Ht),Ale=n(Ht,"P",{});var IJr=s(Ale);U$o=r(IJr,"Examples:"),IJr.forEach(t),J$o=i(Ht),m($3.$$.fragment,Ht),Ht.forEach(t),gl.forEach(t),i9e=i(d),Rd=n(d,"H2",{class:!0});var pke=s(Rd);H5=n(pke,"A",{id:!0,class:!0,href:!0});var jJr=s(H5);Lle=n(jJr,"SPAN",{});var NJr=s(Lle);m(I3.$$.fragment,NJr),NJr.forEach(t),jJr.forEach(t),Y$o=i(pke),Ble=n(pke,"SPAN",{});var DJr=s(Ble);K$o=r(DJr,"AutoModelForCTC"),DJr.forEach(t),pke.forEach(t),d9e=i(d),sr=n(d,"DIV",{class:!0});var pl=s(sr);m(j3.$$.fragment,pl),Z$o=i(pl),Sd=n(pl,"P",{});var xz=s(Sd);eIo=r(xz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a connectionist temporal classification head) when created
with the `),kle=n(xz,"CODE",{});var qJr=s(kle);oIo=r(qJr,"from_pretrained()"),qJr.forEach(t),rIo=r(xz,"class method or the "),xle=n(xz,"CODE",{});var GJr=s(xle);tIo=r(GJr,"from_config()"),GJr.forEach(t),aIo=r(xz,`class
method.`),xz.forEach(t),nIo=i(pl),N3=n(pl,"P",{});var _ke=s(N3);sIo=r(_ke,"This class cannot be instantiated directly using "),Rle=n(_ke,"CODE",{});var OJr=s(Rle);lIo=r(OJr,"__init__()"),OJr.forEach(t),iIo=r(_ke," (throws an error)."),_ke.forEach(t),dIo=i(pl),Zr=n(pl,"DIV",{class:!0});var _l=s(Zr);m(D3.$$.fragment,_l),cIo=i(_l),Sle=n(_l,"P",{});var XJr=s(Sle);fIo=r(XJr,"Instantiates one of the model classes of the library (with a connectionist temporal classification head) from a configuration."),XJr.forEach(t),mIo=i(_l),Pd=n(_l,"P",{});var Rz=s(Pd);gIo=r(Rz,`Note:
Loading a model from its configuration file does `),Ple=n(Rz,"STRONG",{});var zJr=s(Ple);hIo=r(zJr,"not"),zJr.forEach(t),pIo=r(Rz,` load the model weights. It only affects the
model\u2019s configuration. Use `),$le=n(Rz,"CODE",{});var VJr=s($le);_Io=r(VJr,"from_pretrained()"),VJr.forEach(t),uIo=r(Rz,"to load the model weights."),Rz.forEach(t),bIo=i(_l),Ile=n(_l,"P",{});var WJr=s(Ile);vIo=r(WJr,"Examples:"),WJr.forEach(t),TIo=i(_l),m(q3.$$.fragment,_l),_l.forEach(t),FIo=i(pl),We=n(pl,"DIV",{class:!0});var Ut=s(We);m(G3.$$.fragment,Ut),CIo=i(Ut),jle=n(Ut,"P",{});var QJr=s(jle);MIo=r(QJr,"Instantiate one of the model classes of the library (with a connectionist temporal classification head) from a pretrained model."),QJr.forEach(t),EIo=i(Ut),en=n(Ut,"P",{});var w4=s(en);yIo=r(w4,"The model class to instantiate is selected based on the "),Nle=n(w4,"CODE",{});var HJr=s(Nle);wIo=r(HJr,"model_type"),HJr.forEach(t),AIo=r(w4,` property of the config object (either
passed as an argument or loaded from `),Dle=n(w4,"CODE",{});var UJr=s(Dle);LIo=r(UJr,"pretrained_model_name_or_path"),UJr.forEach(t),BIo=r(w4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),qle=n(w4,"CODE",{});var JJr=s(qle);kIo=r(JJr,"pretrained_model_name_or_path"),JJr.forEach(t),xIo=r(w4,":"),w4.forEach(t),RIo=i(Ut),no=n(Ut,"UL",{});var Jt=s(no);U5=n(Jt,"LI",{});var j3e=s(U5);Gle=n(j3e,"STRONG",{});var YJr=s(Gle);SIo=r(YJr,"hubert"),YJr.forEach(t),PIo=r(j3e," \u2014 "),TN=n(j3e,"A",{href:!0});var KJr=s(TN);$Io=r(KJr,"HubertForCTC"),KJr.forEach(t),IIo=r(j3e," (Hubert model)"),j3e.forEach(t),jIo=i(Jt),J5=n(Jt,"LI",{});var N3e=s(J5);Ole=n(N3e,"STRONG",{});var ZJr=s(Ole);NIo=r(ZJr,"sew"),ZJr.forEach(t),DIo=r(N3e," \u2014 "),FN=n(N3e,"A",{href:!0});var eYr=s(FN);qIo=r(eYr,"SEWForCTC"),eYr.forEach(t),GIo=r(N3e," (SEW model)"),N3e.forEach(t),OIo=i(Jt),Y5=n(Jt,"LI",{});var D3e=s(Y5);Xle=n(D3e,"STRONG",{});var oYr=s(Xle);XIo=r(oYr,"sew-d"),oYr.forEach(t),zIo=r(D3e," \u2014 "),CN=n(D3e,"A",{href:!0});var rYr=s(CN);VIo=r(rYr,"SEWDForCTC"),rYr.forEach(t),WIo=r(D3e," (SEW-D model)"),D3e.forEach(t),QIo=i(Jt),K5=n(Jt,"LI",{});var q3e=s(K5);zle=n(q3e,"STRONG",{});var tYr=s(zle);HIo=r(tYr,"unispeech"),tYr.forEach(t),UIo=r(q3e," \u2014 "),MN=n(q3e,"A",{href:!0});var aYr=s(MN);JIo=r(aYr,"UniSpeechForCTC"),aYr.forEach(t),YIo=r(q3e," (UniSpeech model)"),q3e.forEach(t),KIo=i(Jt),Z5=n(Jt,"LI",{});var G3e=s(Z5);Vle=n(G3e,"STRONG",{});var nYr=s(Vle);ZIo=r(nYr,"unispeech-sat"),nYr.forEach(t),ejo=r(G3e," \u2014 "),EN=n(G3e,"A",{href:!0});var sYr=s(EN);ojo=r(sYr,"UniSpeechSatForCTC"),sYr.forEach(t),rjo=r(G3e," (UniSpeechSat model)"),G3e.forEach(t),tjo=i(Jt),e2=n(Jt,"LI",{});var O3e=s(e2);Wle=n(O3e,"STRONG",{});var lYr=s(Wle);ajo=r(lYr,"wav2vec2"),lYr.forEach(t),njo=r(O3e," \u2014 "),yN=n(O3e,"A",{href:!0});var iYr=s(yN);sjo=r(iYr,"Wav2Vec2ForCTC"),iYr.forEach(t),ljo=r(O3e," (Wav2Vec2 model)"),O3e.forEach(t),ijo=i(Jt),o2=n(Jt,"LI",{});var X3e=s(o2);Qle=n(X3e,"STRONG",{});var dYr=s(Qle);djo=r(dYr,"wavlm"),dYr.forEach(t),cjo=r(X3e," \u2014 "),wN=n(X3e,"A",{href:!0});var cYr=s(wN);fjo=r(cYr,"WavLMForCTC"),cYr.forEach(t),mjo=r(X3e," (WavLM model)"),X3e.forEach(t),Jt.forEach(t),gjo=i(Ut),r2=n(Ut,"P",{});var z3e=s(r2);hjo=r(z3e,"The model is set in evaluation mode by default using "),Hle=n(z3e,"CODE",{});var fYr=s(Hle);pjo=r(fYr,"model.eval()"),fYr.forEach(t),_jo=r(z3e,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Ule=n(z3e,"CODE",{});var mYr=s(Ule);ujo=r(mYr,"model.train()"),mYr.forEach(t),z3e.forEach(t),bjo=i(Ut),Jle=n(Ut,"P",{});var gYr=s(Jle);vjo=r(gYr,"Examples:"),gYr.forEach(t),Tjo=i(Ut),m(O3.$$.fragment,Ut),Ut.forEach(t),pl.forEach(t),c9e=i(d),$d=n(d,"H2",{class:!0});var uke=s($d);t2=n(uke,"A",{id:!0,class:!0,href:!0});var hYr=s(t2);Yle=n(hYr,"SPAN",{});var pYr=s(Yle);m(X3.$$.fragment,pYr),pYr.forEach(t),hYr.forEach(t),Fjo=i(uke),Kle=n(uke,"SPAN",{});var _Yr=s(Kle);Cjo=r(_Yr,"AutoModelForSpeechSeq2Seq"),_Yr.forEach(t),uke.forEach(t),f9e=i(d),lr=n(d,"DIV",{class:!0});var ul=s(lr);m(z3.$$.fragment,ul),Mjo=i(ul),Id=n(ul,"P",{});var Sz=s(Id);Ejo=r(Sz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) when created
with the `),Zle=n(Sz,"CODE",{});var uYr=s(Zle);yjo=r(uYr,"from_pretrained()"),uYr.forEach(t),wjo=r(Sz,"class method or the "),eie=n(Sz,"CODE",{});var bYr=s(eie);Ajo=r(bYr,"from_config()"),bYr.forEach(t),Ljo=r(Sz,`class
method.`),Sz.forEach(t),Bjo=i(ul),V3=n(ul,"P",{});var bke=s(V3);kjo=r(bke,"This class cannot be instantiated directly using "),oie=n(bke,"CODE",{});var vYr=s(oie);xjo=r(vYr,"__init__()"),vYr.forEach(t),Rjo=r(bke," (throws an error)."),bke.forEach(t),Sjo=i(ul),et=n(ul,"DIV",{class:!0});var bl=s(et);m(W3.$$.fragment,bl),Pjo=i(bl),rie=n(bl,"P",{});var TYr=s(rie);$jo=r(TYr,"Instantiates one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) from a configuration."),TYr.forEach(t),Ijo=i(bl),jd=n(bl,"P",{});var Pz=s(jd);jjo=r(Pz,`Note:
Loading a model from its configuration file does `),tie=n(Pz,"STRONG",{});var FYr=s(tie);Njo=r(FYr,"not"),FYr.forEach(t),Djo=r(Pz,` load the model weights. It only affects the
model\u2019s configuration. Use `),aie=n(Pz,"CODE",{});var CYr=s(aie);qjo=r(CYr,"from_pretrained()"),CYr.forEach(t),Gjo=r(Pz,"to load the model weights."),Pz.forEach(t),Ojo=i(bl),nie=n(bl,"P",{});var MYr=s(nie);Xjo=r(MYr,"Examples:"),MYr.forEach(t),zjo=i(bl),m(Q3.$$.fragment,bl),bl.forEach(t),Vjo=i(ul),Qe=n(ul,"DIV",{class:!0});var Yt=s(Qe);m(H3.$$.fragment,Yt),Wjo=i(Yt),sie=n(Yt,"P",{});var EYr=s(sie);Qjo=r(EYr,"Instantiate one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) from a pretrained model."),EYr.forEach(t),Hjo=i(Yt),on=n(Yt,"P",{});var A4=s(on);Ujo=r(A4,"The model class to instantiate is selected based on the "),lie=n(A4,"CODE",{});var yYr=s(lie);Jjo=r(yYr,"model_type"),yYr.forEach(t),Yjo=r(A4,` property of the config object (either
passed as an argument or loaded from `),iie=n(A4,"CODE",{});var wYr=s(iie);Kjo=r(wYr,"pretrained_model_name_or_path"),wYr.forEach(t),Zjo=r(A4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),die=n(A4,"CODE",{});var AYr=s(die);eNo=r(AYr,"pretrained_model_name_or_path"),AYr.forEach(t),oNo=r(A4,":"),A4.forEach(t),rNo=i(Yt),U3=n(Yt,"UL",{});var vke=s(U3);a2=n(vke,"LI",{});var V3e=s(a2);cie=n(V3e,"STRONG",{});var LYr=s(cie);tNo=r(LYr,"speech-encoder-decoder"),LYr.forEach(t),aNo=r(V3e," \u2014 "),AN=n(V3e,"A",{href:!0});var BYr=s(AN);nNo=r(BYr,"SpeechEncoderDecoderModel"),BYr.forEach(t),sNo=r(V3e," (Speech Encoder decoder model)"),V3e.forEach(t),lNo=i(vke),n2=n(vke,"LI",{});var W3e=s(n2);fie=n(W3e,"STRONG",{});var kYr=s(fie);iNo=r(kYr,"speech_to_text"),kYr.forEach(t),dNo=r(W3e," \u2014 "),LN=n(W3e,"A",{href:!0});var xYr=s(LN);cNo=r(xYr,"Speech2TextForConditionalGeneration"),xYr.forEach(t),fNo=r(W3e," (Speech2Text model)"),W3e.forEach(t),vke.forEach(t),mNo=i(Yt),s2=n(Yt,"P",{});var Q3e=s(s2);gNo=r(Q3e,"The model is set in evaluation mode by default using "),mie=n(Q3e,"CODE",{});var RYr=s(mie);hNo=r(RYr,"model.eval()"),RYr.forEach(t),pNo=r(Q3e,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),gie=n(Q3e,"CODE",{});var SYr=s(gie);_No=r(SYr,"model.train()"),SYr.forEach(t),Q3e.forEach(t),uNo=i(Yt),hie=n(Yt,"P",{});var PYr=s(hie);bNo=r(PYr,"Examples:"),PYr.forEach(t),vNo=i(Yt),m(J3.$$.fragment,Yt),Yt.forEach(t),ul.forEach(t),m9e=i(d),Nd=n(d,"H2",{class:!0});var Tke=s(Nd);l2=n(Tke,"A",{id:!0,class:!0,href:!0});var $Yr=s(l2);pie=n($Yr,"SPAN",{});var IYr=s(pie);m(Y3.$$.fragment,IYr),IYr.forEach(t),$Yr.forEach(t),TNo=i(Tke),_ie=n(Tke,"SPAN",{});var jYr=s(_ie);FNo=r(jYr,"AutoModelForAudioXVector"),jYr.forEach(t),Tke.forEach(t),g9e=i(d),ir=n(d,"DIV",{class:!0});var vl=s(ir);m(K3.$$.fragment,vl),CNo=i(vl),Dd=n(vl,"P",{});var $z=s(Dd);MNo=r($z,`This is a generic model class that will be instantiated as one of the model classes of the library (with a audio retrieval via x-vector head) when created
with the `),uie=n($z,"CODE",{});var NYr=s(uie);ENo=r(NYr,"from_pretrained()"),NYr.forEach(t),yNo=r($z,"class method or the "),bie=n($z,"CODE",{});var DYr=s(bie);wNo=r(DYr,"from_config()"),DYr.forEach(t),ANo=r($z,`class
method.`),$z.forEach(t),LNo=i(vl),Z3=n(vl,"P",{});var Fke=s(Z3);BNo=r(Fke,"This class cannot be instantiated directly using "),vie=n(Fke,"CODE",{});var qYr=s(vie);kNo=r(qYr,"__init__()"),qYr.forEach(t),xNo=r(Fke," (throws an error)."),Fke.forEach(t),RNo=i(vl),ot=n(vl,"DIV",{class:!0});var Tl=s(ot);m(ey.$$.fragment,Tl),SNo=i(Tl),Tie=n(Tl,"P",{});var GYr=s(Tie);PNo=r(GYr,"Instantiates one of the model classes of the library (with a audio retrieval via x-vector head) from a configuration."),GYr.forEach(t),$No=i(Tl),qd=n(Tl,"P",{});var Iz=s(qd);INo=r(Iz,`Note:
Loading a model from its configuration file does `),Fie=n(Iz,"STRONG",{});var OYr=s(Fie);jNo=r(OYr,"not"),OYr.forEach(t),NNo=r(Iz,` load the model weights. It only affects the
model\u2019s configuration. Use `),Cie=n(Iz,"CODE",{});var XYr=s(Cie);DNo=r(XYr,"from_pretrained()"),XYr.forEach(t),qNo=r(Iz,"to load the model weights."),Iz.forEach(t),GNo=i(Tl),Mie=n(Tl,"P",{});var zYr=s(Mie);ONo=r(zYr,"Examples:"),zYr.forEach(t),XNo=i(Tl),m(oy.$$.fragment,Tl),Tl.forEach(t),zNo=i(vl),He=n(vl,"DIV",{class:!0});var Kt=s(He);m(ry.$$.fragment,Kt),VNo=i(Kt),Eie=n(Kt,"P",{});var VYr=s(Eie);WNo=r(VYr,"Instantiate one of the model classes of the library (with a audio retrieval via x-vector head) from a pretrained model."),VYr.forEach(t),QNo=i(Kt),rn=n(Kt,"P",{});var L4=s(rn);HNo=r(L4,"The model class to instantiate is selected based on the "),yie=n(L4,"CODE",{});var WYr=s(yie);UNo=r(WYr,"model_type"),WYr.forEach(t),JNo=r(L4,` property of the config object (either
passed as an argument or loaded from `),wie=n(L4,"CODE",{});var QYr=s(wie);YNo=r(QYr,"pretrained_model_name_or_path"),QYr.forEach(t),KNo=r(L4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Aie=n(L4,"CODE",{});var HYr=s(Aie);ZNo=r(HYr,"pretrained_model_name_or_path"),HYr.forEach(t),eDo=r(L4,":"),L4.forEach(t),oDo=i(Kt),Gd=n(Kt,"UL",{});var jz=s(Gd);i2=n(jz,"LI",{});var H3e=s(i2);Lie=n(H3e,"STRONG",{});var UYr=s(Lie);rDo=r(UYr,"unispeech-sat"),UYr.forEach(t),tDo=r(H3e," \u2014 "),BN=n(H3e,"A",{href:!0});var JYr=s(BN);aDo=r(JYr,"UniSpeechSatForXVector"),JYr.forEach(t),nDo=r(H3e," (UniSpeechSat model)"),H3e.forEach(t),sDo=i(jz),d2=n(jz,"LI",{});var U3e=s(d2);Bie=n(U3e,"STRONG",{});var YYr=s(Bie);lDo=r(YYr,"wav2vec2"),YYr.forEach(t),iDo=r(U3e," \u2014 "),kN=n(U3e,"A",{href:!0});var KYr=s(kN);dDo=r(KYr,"Wav2Vec2ForXVector"),KYr.forEach(t),cDo=r(U3e," (Wav2Vec2 model)"),U3e.forEach(t),fDo=i(jz),c2=n(jz,"LI",{});var J3e=s(c2);kie=n(J3e,"STRONG",{});var ZYr=s(kie);mDo=r(ZYr,"wavlm"),ZYr.forEach(t),gDo=r(J3e," \u2014 "),xN=n(J3e,"A",{href:!0});var eKr=s(xN);hDo=r(eKr,"WavLMForXVector"),eKr.forEach(t),pDo=r(J3e," (WavLM model)"),J3e.forEach(t),jz.forEach(t),_Do=i(Kt),f2=n(Kt,"P",{});var Y3e=s(f2);uDo=r(Y3e,"The model is set in evaluation mode by default using "),xie=n(Y3e,"CODE",{});var oKr=s(xie);bDo=r(oKr,"model.eval()"),oKr.forEach(t),vDo=r(Y3e,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Rie=n(Y3e,"CODE",{});var rKr=s(Rie);TDo=r(rKr,"model.train()"),rKr.forEach(t),Y3e.forEach(t),FDo=i(Kt),Sie=n(Kt,"P",{});var tKr=s(Sie);CDo=r(tKr,"Examples:"),tKr.forEach(t),MDo=i(Kt),m(ty.$$.fragment,Kt),Kt.forEach(t),vl.forEach(t),h9e=i(d),Od=n(d,"H2",{class:!0});var Cke=s(Od);m2=n(Cke,"A",{id:!0,class:!0,href:!0});var aKr=s(m2);Pie=n(aKr,"SPAN",{});var nKr=s(Pie);m(ay.$$.fragment,nKr),nKr.forEach(t),aKr.forEach(t),EDo=i(Cke),$ie=n(Cke,"SPAN",{});var sKr=s($ie);yDo=r(sKr,"AutoModelForMaskedImageModeling"),sKr.forEach(t),Cke.forEach(t),p9e=i(d),dr=n(d,"DIV",{class:!0});var Fl=s(dr);m(ny.$$.fragment,Fl),wDo=i(Fl),Xd=n(Fl,"P",{});var Nz=s(Xd);ADo=r(Nz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a masked image modeling head) when created
with the `),Iie=n(Nz,"CODE",{});var lKr=s(Iie);LDo=r(lKr,"from_pretrained()"),lKr.forEach(t),BDo=r(Nz,"class method or the "),jie=n(Nz,"CODE",{});var iKr=s(jie);kDo=r(iKr,"from_config()"),iKr.forEach(t),xDo=r(Nz,`class
method.`),Nz.forEach(t),RDo=i(Fl),sy=n(Fl,"P",{});var Mke=s(sy);SDo=r(Mke,"This class cannot be instantiated directly using "),Nie=n(Mke,"CODE",{});var dKr=s(Nie);PDo=r(dKr,"__init__()"),dKr.forEach(t),$Do=r(Mke," (throws an error)."),Mke.forEach(t),IDo=i(Fl),rt=n(Fl,"DIV",{class:!0});var Cl=s(rt);m(ly.$$.fragment,Cl),jDo=i(Cl),Die=n(Cl,"P",{});var cKr=s(Die);NDo=r(cKr,"Instantiates one of the model classes of the library (with a masked image modeling head) from a configuration."),cKr.forEach(t),DDo=i(Cl),zd=n(Cl,"P",{});var Dz=s(zd);qDo=r(Dz,`Note:
Loading a model from its configuration file does `),qie=n(Dz,"STRONG",{});var fKr=s(qie);GDo=r(fKr,"not"),fKr.forEach(t),ODo=r(Dz,` load the model weights. It only affects the
model\u2019s configuration. Use `),Gie=n(Dz,"CODE",{});var mKr=s(Gie);XDo=r(mKr,"from_pretrained()"),mKr.forEach(t),zDo=r(Dz,"to load the model weights."),Dz.forEach(t),VDo=i(Cl),Oie=n(Cl,"P",{});var gKr=s(Oie);WDo=r(gKr,"Examples:"),gKr.forEach(t),QDo=i(Cl),m(iy.$$.fragment,Cl),Cl.forEach(t),HDo=i(Fl),Ue=n(Fl,"DIV",{class:!0});var Zt=s(Ue);m(dy.$$.fragment,Zt),UDo=i(Zt),Xie=n(Zt,"P",{});var hKr=s(Xie);JDo=r(hKr,"Instantiate one of the model classes of the library (with a masked image modeling head) from a pretrained model."),hKr.forEach(t),YDo=i(Zt),tn=n(Zt,"P",{});var B4=s(tn);KDo=r(B4,"The model class to instantiate is selected based on the "),zie=n(B4,"CODE",{});var pKr=s(zie);ZDo=r(pKr,"model_type"),pKr.forEach(t),eqo=r(B4,` property of the config object (either
passed as an argument or loaded from `),Vie=n(B4,"CODE",{});var _Kr=s(Vie);oqo=r(_Kr,"pretrained_model_name_or_path"),_Kr.forEach(t),rqo=r(B4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Wie=n(B4,"CODE",{});var uKr=s(Wie);tqo=r(uKr,"pretrained_model_name_or_path"),uKr.forEach(t),aqo=r(B4,":"),B4.forEach(t),nqo=i(Zt),Vd=n(Zt,"UL",{});var qz=s(Vd);g2=n(qz,"LI",{});var K3e=s(g2);Qie=n(K3e,"STRONG",{});var bKr=s(Qie);sqo=r(bKr,"deit"),bKr.forEach(t),lqo=r(K3e," \u2014 "),RN=n(K3e,"A",{href:!0});var vKr=s(RN);iqo=r(vKr,"DeiTForMaskedImageModeling"),vKr.forEach(t),dqo=r(K3e," (DeiT model)"),K3e.forEach(t),cqo=i(qz),h2=n(qz,"LI",{});var Z3e=s(h2);Hie=n(Z3e,"STRONG",{});var TKr=s(Hie);fqo=r(TKr,"swin"),TKr.forEach(t),mqo=r(Z3e," \u2014 "),SN=n(Z3e,"A",{href:!0});var FKr=s(SN);gqo=r(FKr,"SwinForMaskedImageModeling"),FKr.forEach(t),hqo=r(Z3e," (Swin model)"),Z3e.forEach(t),pqo=i(qz),p2=n(qz,"LI",{});var eye=s(p2);Uie=n(eye,"STRONG",{});var CKr=s(Uie);_qo=r(CKr,"vit"),CKr.forEach(t),uqo=r(eye," \u2014 "),PN=n(eye,"A",{href:!0});var MKr=s(PN);bqo=r(MKr,"ViTForMaskedImageModeling"),MKr.forEach(t),vqo=r(eye," (ViT model)"),eye.forEach(t),qz.forEach(t),Tqo=i(Zt),_2=n(Zt,"P",{});var oye=s(_2);Fqo=r(oye,"The model is set in evaluation mode by default using "),Jie=n(oye,"CODE",{});var EKr=s(Jie);Cqo=r(EKr,"model.eval()"),EKr.forEach(t),Mqo=r(oye,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Yie=n(oye,"CODE",{});var yKr=s(Yie);Eqo=r(yKr,"model.train()"),yKr.forEach(t),oye.forEach(t),yqo=i(Zt),Kie=n(Zt,"P",{});var wKr=s(Kie);wqo=r(wKr,"Examples:"),wKr.forEach(t),Aqo=i(Zt),m(cy.$$.fragment,Zt),Zt.forEach(t),Fl.forEach(t),_9e=i(d),Wd=n(d,"H2",{class:!0});var Eke=s(Wd);u2=n(Eke,"A",{id:!0,class:!0,href:!0});var AKr=s(u2);Zie=n(AKr,"SPAN",{});var LKr=s(Zie);m(fy.$$.fragment,LKr),LKr.forEach(t),AKr.forEach(t),Lqo=i(Eke),ede=n(Eke,"SPAN",{});var BKr=s(ede);Bqo=r(BKr,"AutoModelForObjectDetection"),BKr.forEach(t),Eke.forEach(t),u9e=i(d),cr=n(d,"DIV",{class:!0});var Ml=s(cr);m(my.$$.fragment,Ml),kqo=i(Ml),Qd=n(Ml,"P",{});var Gz=s(Qd);xqo=r(Gz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a object detection head) when created
with the `),ode=n(Gz,"CODE",{});var kKr=s(ode);Rqo=r(kKr,"from_pretrained()"),kKr.forEach(t),Sqo=r(Gz,"class method or the "),rde=n(Gz,"CODE",{});var xKr=s(rde);Pqo=r(xKr,"from_config()"),xKr.forEach(t),$qo=r(Gz,`class
method.`),Gz.forEach(t),Iqo=i(Ml),gy=n(Ml,"P",{});var yke=s(gy);jqo=r(yke,"This class cannot be instantiated directly using "),tde=n(yke,"CODE",{});var RKr=s(tde);Nqo=r(RKr,"__init__()"),RKr.forEach(t),Dqo=r(yke," (throws an error)."),yke.forEach(t),qqo=i(Ml),tt=n(Ml,"DIV",{class:!0});var El=s(tt);m(hy.$$.fragment,El),Gqo=i(El),ade=n(El,"P",{});var SKr=s(ade);Oqo=r(SKr,"Instantiates one of the model classes of the library (with a object detection head) from a configuration."),SKr.forEach(t),Xqo=i(El),Hd=n(El,"P",{});var Oz=s(Hd);zqo=r(Oz,`Note:
Loading a model from its configuration file does `),nde=n(Oz,"STRONG",{});var PKr=s(nde);Vqo=r(PKr,"not"),PKr.forEach(t),Wqo=r(Oz,` load the model weights. It only affects the
model\u2019s configuration. Use `),sde=n(Oz,"CODE",{});var $Kr=s(sde);Qqo=r($Kr,"from_pretrained()"),$Kr.forEach(t),Hqo=r(Oz,"to load the model weights."),Oz.forEach(t),Uqo=i(El),lde=n(El,"P",{});var IKr=s(lde);Jqo=r(IKr,"Examples:"),IKr.forEach(t),Yqo=i(El),m(py.$$.fragment,El),El.forEach(t),Kqo=i(Ml),Je=n(Ml,"DIV",{class:!0});var ea=s(Je);m(_y.$$.fragment,ea),Zqo=i(ea),ide=n(ea,"P",{});var jKr=s(ide);eGo=r(jKr,"Instantiate one of the model classes of the library (with a object detection head) from a pretrained model."),jKr.forEach(t),oGo=i(ea),an=n(ea,"P",{});var k4=s(an);rGo=r(k4,"The model class to instantiate is selected based on the "),dde=n(k4,"CODE",{});var NKr=s(dde);tGo=r(NKr,"model_type"),NKr.forEach(t),aGo=r(k4,` property of the config object (either
passed as an argument or loaded from `),cde=n(k4,"CODE",{});var DKr=s(cde);nGo=r(DKr,"pretrained_model_name_or_path"),DKr.forEach(t),sGo=r(k4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),fde=n(k4,"CODE",{});var qKr=s(fde);lGo=r(qKr,"pretrained_model_name_or_path"),qKr.forEach(t),iGo=r(k4,":"),k4.forEach(t),dGo=i(ea),mde=n(ea,"UL",{});var GKr=s(mde);b2=n(GKr,"LI",{});var rye=s(b2);gde=n(rye,"STRONG",{});var OKr=s(gde);cGo=r(OKr,"detr"),OKr.forEach(t),fGo=r(rye," \u2014 "),$N=n(rye,"A",{href:!0});var XKr=s($N);mGo=r(XKr,"DetrForObjectDetection"),XKr.forEach(t),gGo=r(rye," (DETR model)"),rye.forEach(t),GKr.forEach(t),hGo=i(ea),v2=n(ea,"P",{});var tye=s(v2);pGo=r(tye,"The model is set in evaluation mode by default using "),hde=n(tye,"CODE",{});var zKr=s(hde);_Go=r(zKr,"model.eval()"),zKr.forEach(t),uGo=r(tye,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),pde=n(tye,"CODE",{});var VKr=s(pde);bGo=r(VKr,"model.train()"),VKr.forEach(t),tye.forEach(t),vGo=i(ea),_de=n(ea,"P",{});var WKr=s(_de);TGo=r(WKr,"Examples:"),WKr.forEach(t),FGo=i(ea),m(uy.$$.fragment,ea),ea.forEach(t),Ml.forEach(t),b9e=i(d),Ud=n(d,"H2",{class:!0});var wke=s(Ud);T2=n(wke,"A",{id:!0,class:!0,href:!0});var QKr=s(T2);ude=n(QKr,"SPAN",{});var HKr=s(ude);m(by.$$.fragment,HKr),HKr.forEach(t),QKr.forEach(t),CGo=i(wke),bde=n(wke,"SPAN",{});var UKr=s(bde);MGo=r(UKr,"AutoModelForImageSegmentation"),UKr.forEach(t),wke.forEach(t),v9e=i(d),fr=n(d,"DIV",{class:!0});var yl=s(fr);m(vy.$$.fragment,yl),EGo=i(yl),Jd=n(yl,"P",{});var Xz=s(Jd);yGo=r(Xz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a image segmentation head) when created
with the `),vde=n(Xz,"CODE",{});var JKr=s(vde);wGo=r(JKr,"from_pretrained()"),JKr.forEach(t),AGo=r(Xz,"class method or the "),Tde=n(Xz,"CODE",{});var YKr=s(Tde);LGo=r(YKr,"from_config()"),YKr.forEach(t),BGo=r(Xz,`class
method.`),Xz.forEach(t),kGo=i(yl),Ty=n(yl,"P",{});var Ake=s(Ty);xGo=r(Ake,"This class cannot be instantiated directly using "),Fde=n(Ake,"CODE",{});var KKr=s(Fde);RGo=r(KKr,"__init__()"),KKr.forEach(t),SGo=r(Ake," (throws an error)."),Ake.forEach(t),PGo=i(yl),at=n(yl,"DIV",{class:!0});var wl=s(at);m(Fy.$$.fragment,wl),$Go=i(wl),Cde=n(wl,"P",{});var ZKr=s(Cde);IGo=r(ZKr,"Instantiates one of the model classes of the library (with a image segmentation head) from a configuration."),ZKr.forEach(t),jGo=i(wl),Yd=n(wl,"P",{});var zz=s(Yd);NGo=r(zz,`Note:
Loading a model from its configuration file does `),Mde=n(zz,"STRONG",{});var eZr=s(Mde);DGo=r(eZr,"not"),eZr.forEach(t),qGo=r(zz,` load the model weights. It only affects the
model\u2019s configuration. Use `),Ede=n(zz,"CODE",{});var oZr=s(Ede);GGo=r(oZr,"from_pretrained()"),oZr.forEach(t),OGo=r(zz,"to load the model weights."),zz.forEach(t),XGo=i(wl),yde=n(wl,"P",{});var rZr=s(yde);zGo=r(rZr,"Examples:"),rZr.forEach(t),VGo=i(wl),m(Cy.$$.fragment,wl),wl.forEach(t),WGo=i(yl),Ye=n(yl,"DIV",{class:!0});var oa=s(Ye);m(My.$$.fragment,oa),QGo=i(oa),wde=n(oa,"P",{});var tZr=s(wde);HGo=r(tZr,"Instantiate one of the model classes of the library (with a image segmentation head) from a pretrained model."),tZr.forEach(t),UGo=i(oa),nn=n(oa,"P",{});var x4=s(nn);JGo=r(x4,"The model class to instantiate is selected based on the "),Ade=n(x4,"CODE",{});var aZr=s(Ade);YGo=r(aZr,"model_type"),aZr.forEach(t),KGo=r(x4,` property of the config object (either
passed as an argument or loaded from `),Lde=n(x4,"CODE",{});var nZr=s(Lde);ZGo=r(nZr,"pretrained_model_name_or_path"),nZr.forEach(t),eOo=r(x4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Bde=n(x4,"CODE",{});var sZr=s(Bde);oOo=r(sZr,"pretrained_model_name_or_path"),sZr.forEach(t),rOo=r(x4,":"),x4.forEach(t),tOo=i(oa),kde=n(oa,"UL",{});var lZr=s(kde);F2=n(lZr,"LI",{});var aye=s(F2);xde=n(aye,"STRONG",{});var iZr=s(xde);aOo=r(iZr,"detr"),iZr.forEach(t),nOo=r(aye," \u2014 "),IN=n(aye,"A",{href:!0});var dZr=s(IN);sOo=r(dZr,"DetrForSegmentation"),dZr.forEach(t),lOo=r(aye," (DETR model)"),aye.forEach(t),lZr.forEach(t),iOo=i(oa),C2=n(oa,"P",{});var nye=s(C2);dOo=r(nye,"The model is set in evaluation mode by default using "),Rde=n(nye,"CODE",{});var cZr=s(Rde);cOo=r(cZr,"model.eval()"),cZr.forEach(t),fOo=r(nye,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Sde=n(nye,"CODE",{});var fZr=s(Sde);mOo=r(fZr,"model.train()"),fZr.forEach(t),nye.forEach(t),gOo=i(oa),Pde=n(oa,"P",{});var mZr=s(Pde);hOo=r(mZr,"Examples:"),mZr.forEach(t),pOo=i(oa),m(Ey.$$.fragment,oa),oa.forEach(t),yl.forEach(t),T9e=i(d),Kd=n(d,"H2",{class:!0});var Lke=s(Kd);M2=n(Lke,"A",{id:!0,class:!0,href:!0});var gZr=s(M2);$de=n(gZr,"SPAN",{});var hZr=s($de);m(yy.$$.fragment,hZr),hZr.forEach(t),gZr.forEach(t),_Oo=i(Lke),Ide=n(Lke,"SPAN",{});var pZr=s(Ide);uOo=r(pZr,"AutoModelForSemanticSegmentation"),pZr.forEach(t),Lke.forEach(t),F9e=i(d),mr=n(d,"DIV",{class:!0});var Al=s(mr);m(wy.$$.fragment,Al),bOo=i(Al),Zd=n(Al,"P",{});var Vz=s(Zd);vOo=r(Vz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a semantic segmentation head) when created
with the `),jde=n(Vz,"CODE",{});var _Zr=s(jde);TOo=r(_Zr,"from_pretrained()"),_Zr.forEach(t),FOo=r(Vz,"class method or the "),Nde=n(Vz,"CODE",{});var uZr=s(Nde);COo=r(uZr,"from_config()"),uZr.forEach(t),MOo=r(Vz,`class
method.`),Vz.forEach(t),EOo=i(Al),Ay=n(Al,"P",{});var Bke=s(Ay);yOo=r(Bke,"This class cannot be instantiated directly using "),Dde=n(Bke,"CODE",{});var bZr=s(Dde);wOo=r(bZr,"__init__()"),bZr.forEach(t),AOo=r(Bke," (throws an error)."),Bke.forEach(t),LOo=i(Al),nt=n(Al,"DIV",{class:!0});var Ll=s(nt);m(Ly.$$.fragment,Ll),BOo=i(Ll),qde=n(Ll,"P",{});var vZr=s(qde);kOo=r(vZr,"Instantiates one of the model classes of the library (with a semantic segmentation head) from a configuration."),vZr.forEach(t),xOo=i(Ll),ec=n(Ll,"P",{});var Wz=s(ec);ROo=r(Wz,`Note:
Loading a model from its configuration file does `),Gde=n(Wz,"STRONG",{});var TZr=s(Gde);SOo=r(TZr,"not"),TZr.forEach(t),POo=r(Wz,` load the model weights. It only affects the
model\u2019s configuration. Use `),Ode=n(Wz,"CODE",{});var FZr=s(Ode);$Oo=r(FZr,"from_pretrained()"),FZr.forEach(t),IOo=r(Wz,"to load the model weights."),Wz.forEach(t),jOo=i(Ll),Xde=n(Ll,"P",{});var CZr=s(Xde);NOo=r(CZr,"Examples:"),CZr.forEach(t),DOo=i(Ll),m(By.$$.fragment,Ll),Ll.forEach(t),qOo=i(Al),Ke=n(Al,"DIV",{class:!0});var ra=s(Ke);m(ky.$$.fragment,ra),GOo=i(ra),zde=n(ra,"P",{});var MZr=s(zde);OOo=r(MZr,"Instantiate one of the model classes of the library (with a semantic segmentation head) from a pretrained model."),MZr.forEach(t),XOo=i(ra),sn=n(ra,"P",{});var R4=s(sn);zOo=r(R4,"The model class to instantiate is selected based on the "),Vde=n(R4,"CODE",{});var EZr=s(Vde);VOo=r(EZr,"model_type"),EZr.forEach(t),WOo=r(R4,` property of the config object (either
passed as an argument or loaded from `),Wde=n(R4,"CODE",{});var yZr=s(Wde);QOo=r(yZr,"pretrained_model_name_or_path"),yZr.forEach(t),HOo=r(R4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Qde=n(R4,"CODE",{});var wZr=s(Qde);UOo=r(wZr,"pretrained_model_name_or_path"),wZr.forEach(t),JOo=r(R4,":"),R4.forEach(t),YOo=i(ra),xy=n(ra,"UL",{});var kke=s(xy);E2=n(kke,"LI",{});var sye=s(E2);Hde=n(sye,"STRONG",{});var AZr=s(Hde);KOo=r(AZr,"beit"),AZr.forEach(t),ZOo=r(sye," \u2014 "),jN=n(sye,"A",{href:!0});var LZr=s(jN);eXo=r(LZr,"BeitForSemanticSegmentation"),LZr.forEach(t),oXo=r(sye," (BEiT model)"),sye.forEach(t),rXo=i(kke),y2=n(kke,"LI",{});var lye=s(y2);Ude=n(lye,"STRONG",{});var BZr=s(Ude);tXo=r(BZr,"segformer"),BZr.forEach(t),aXo=r(lye," \u2014 "),NN=n(lye,"A",{href:!0});var kZr=s(NN);nXo=r(kZr,"SegformerForSemanticSegmentation"),kZr.forEach(t),sXo=r(lye," (SegFormer model)"),lye.forEach(t),kke.forEach(t),lXo=i(ra),w2=n(ra,"P",{});var iye=s(w2);iXo=r(iye,"The model is set in evaluation mode by default using "),Jde=n(iye,"CODE",{});var xZr=s(Jde);dXo=r(xZr,"model.eval()"),xZr.forEach(t),cXo=r(iye,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Yde=n(iye,"CODE",{});var RZr=s(Yde);fXo=r(RZr,"model.train()"),RZr.forEach(t),iye.forEach(t),mXo=i(ra),Kde=n(ra,"P",{});var SZr=s(Kde);gXo=r(SZr,"Examples:"),SZr.forEach(t),hXo=i(ra),m(Ry.$$.fragment,ra),ra.forEach(t),Al.forEach(t),C9e=i(d),oc=n(d,"H2",{class:!0});var xke=s(oc);A2=n(xke,"A",{id:!0,class:!0,href:!0});var PZr=s(A2);Zde=n(PZr,"SPAN",{});var $Zr=s(Zde);m(Sy.$$.fragment,$Zr),$Zr.forEach(t),PZr.forEach(t),pXo=i(xke),ece=n(xke,"SPAN",{});var IZr=s(ece);_Xo=r(IZr,"TFAutoModel"),IZr.forEach(t),xke.forEach(t),M9e=i(d),gr=n(d,"DIV",{class:!0});var Bl=s(gr);m(Py.$$.fragment,Bl),uXo=i(Bl),rc=n(Bl,"P",{});var Qz=s(rc);bXo=r(Qz,`This is a generic model class that will be instantiated as one of the base model classes of the library when created
with the `),oce=n(Qz,"CODE",{});var jZr=s(oce);vXo=r(jZr,"from_pretrained()"),jZr.forEach(t),TXo=r(Qz,"class method or the "),rce=n(Qz,"CODE",{});var NZr=s(rce);FXo=r(NZr,"from_config()"),NZr.forEach(t),CXo=r(Qz,`class
method.`),Qz.forEach(t),MXo=i(Bl),$y=n(Bl,"P",{});var Rke=s($y);EXo=r(Rke,"This class cannot be instantiated directly using "),tce=n(Rke,"CODE",{});var DZr=s(tce);yXo=r(DZr,"__init__()"),DZr.forEach(t),wXo=r(Rke," (throws an error)."),Rke.forEach(t),AXo=i(Bl),st=n(Bl,"DIV",{class:!0});var kl=s(st);m(Iy.$$.fragment,kl),LXo=i(kl),ace=n(kl,"P",{});var qZr=s(ace);BXo=r(qZr,"Instantiates one of the base model classes of the library from a configuration."),qZr.forEach(t),kXo=i(kl),tc=n(kl,"P",{});var Hz=s(tc);xXo=r(Hz,`Note:
Loading a model from its configuration file does `),nce=n(Hz,"STRONG",{});var GZr=s(nce);RXo=r(GZr,"not"),GZr.forEach(t),SXo=r(Hz,` load the model weights. It only affects the
model\u2019s configuration. Use `),sce=n(Hz,"CODE",{});var OZr=s(sce);PXo=r(OZr,"from_pretrained()"),OZr.forEach(t),$Xo=r(Hz,"to load the model weights."),Hz.forEach(t),IXo=i(kl),lce=n(kl,"P",{});var XZr=s(lce);jXo=r(XZr,"Examples:"),XZr.forEach(t),NXo=i(kl),m(jy.$$.fragment,kl),kl.forEach(t),DXo=i(Bl),go=n(Bl,"DIV",{class:!0});var ca=s(go);m(Ny.$$.fragment,ca),qXo=i(ca),ice=n(ca,"P",{});var zZr=s(ice);GXo=r(zZr,"Instantiate one of the base model classes of the library from a pretrained model."),zZr.forEach(t),OXo=i(ca),ln=n(ca,"P",{});var S4=s(ln);XXo=r(S4,"The model class to instantiate is selected based on the "),dce=n(S4,"CODE",{});var VZr=s(dce);zXo=r(VZr,"model_type"),VZr.forEach(t),VXo=r(S4,` property of the config object (either
passed as an argument or loaded from `),cce=n(S4,"CODE",{});var WZr=s(cce);WXo=r(WZr,"pretrained_model_name_or_path"),WZr.forEach(t),QXo=r(S4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),fce=n(S4,"CODE",{});var QZr=s(fce);HXo=r(QZr,"pretrained_model_name_or_path"),QZr.forEach(t),UXo=r(S4,":"),S4.forEach(t),JXo=i(ca),B=n(ca,"UL",{});var k=s(B);L2=n(k,"LI",{});var dye=s(L2);mce=n(dye,"STRONG",{});var HZr=s(mce);YXo=r(HZr,"albert"),HZr.forEach(t),KXo=r(dye," \u2014 "),DN=n(dye,"A",{href:!0});var UZr=s(DN);ZXo=r(UZr,"TFAlbertModel"),UZr.forEach(t),ezo=r(dye," (ALBERT model)"),dye.forEach(t),ozo=i(k),B2=n(k,"LI",{});var cye=s(B2);gce=n(cye,"STRONG",{});var JZr=s(gce);rzo=r(JZr,"bart"),JZr.forEach(t),tzo=r(cye," \u2014 "),qN=n(cye,"A",{href:!0});var YZr=s(qN);azo=r(YZr,"TFBartModel"),YZr.forEach(t),nzo=r(cye," (BART model)"),cye.forEach(t),szo=i(k),k2=n(k,"LI",{});var fye=s(k2);hce=n(fye,"STRONG",{});var KZr=s(hce);lzo=r(KZr,"bert"),KZr.forEach(t),izo=r(fye," \u2014 "),GN=n(fye,"A",{href:!0});var ZZr=s(GN);dzo=r(ZZr,"TFBertModel"),ZZr.forEach(t),czo=r(fye," (BERT model)"),fye.forEach(t),fzo=i(k),x2=n(k,"LI",{});var mye=s(x2);pce=n(mye,"STRONG",{});var eet=s(pce);mzo=r(eet,"blenderbot"),eet.forEach(t),gzo=r(mye," \u2014 "),ON=n(mye,"A",{href:!0});var oet=s(ON);hzo=r(oet,"TFBlenderbotModel"),oet.forEach(t),pzo=r(mye," (Blenderbot model)"),mye.forEach(t),_zo=i(k),R2=n(k,"LI",{});var gye=s(R2);_ce=n(gye,"STRONG",{});var ret=s(_ce);uzo=r(ret,"blenderbot-small"),ret.forEach(t),bzo=r(gye," \u2014 "),XN=n(gye,"A",{href:!0});var tet=s(XN);vzo=r(tet,"TFBlenderbotSmallModel"),tet.forEach(t),Tzo=r(gye," (BlenderbotSmall model)"),gye.forEach(t),Fzo=i(k),S2=n(k,"LI",{});var hye=s(S2);uce=n(hye,"STRONG",{});var aet=s(uce);Czo=r(aet,"camembert"),aet.forEach(t),Mzo=r(hye," \u2014 "),zN=n(hye,"A",{href:!0});var net=s(zN);Ezo=r(net,"TFCamembertModel"),net.forEach(t),yzo=r(hye," (CamemBERT model)"),hye.forEach(t),wzo=i(k),P2=n(k,"LI",{});var pye=s(P2);bce=n(pye,"STRONG",{});var set=s(bce);Azo=r(set,"clip"),set.forEach(t),Lzo=r(pye," \u2014 "),VN=n(pye,"A",{href:!0});var iet=s(VN);Bzo=r(iet,"TFCLIPModel"),iet.forEach(t),kzo=r(pye," (CLIP model)"),pye.forEach(t),xzo=i(k),$2=n(k,"LI",{});var _ye=s($2);vce=n(_ye,"STRONG",{});var det=s(vce);Rzo=r(det,"convbert"),det.forEach(t),Szo=r(_ye," \u2014 "),WN=n(_ye,"A",{href:!0});var cet=s(WN);Pzo=r(cet,"TFConvBertModel"),cet.forEach(t),$zo=r(_ye," (ConvBERT model)"),_ye.forEach(t),Izo=i(k),I2=n(k,"LI",{});var uye=s(I2);Tce=n(uye,"STRONG",{});var fet=s(Tce);jzo=r(fet,"ctrl"),fet.forEach(t),Nzo=r(uye," \u2014 "),QN=n(uye,"A",{href:!0});var met=s(QN);Dzo=r(met,"TFCTRLModel"),met.forEach(t),qzo=r(uye," (CTRL model)"),uye.forEach(t),Gzo=i(k),j2=n(k,"LI",{});var bye=s(j2);Fce=n(bye,"STRONG",{});var get=s(Fce);Ozo=r(get,"deberta"),get.forEach(t),Xzo=r(bye," \u2014 "),HN=n(bye,"A",{href:!0});var het=s(HN);zzo=r(het,"TFDebertaModel"),het.forEach(t),Vzo=r(bye," (DeBERTa model)"),bye.forEach(t),Wzo=i(k),N2=n(k,"LI",{});var vye=s(N2);Cce=n(vye,"STRONG",{});var pet=s(Cce);Qzo=r(pet,"deberta-v2"),pet.forEach(t),Hzo=r(vye," \u2014 "),UN=n(vye,"A",{href:!0});var _et=s(UN);Uzo=r(_et,"TFDebertaV2Model"),_et.forEach(t),Jzo=r(vye," (DeBERTa-v2 model)"),vye.forEach(t),Yzo=i(k),D2=n(k,"LI",{});var Tye=s(D2);Mce=n(Tye,"STRONG",{});var uet=s(Mce);Kzo=r(uet,"distilbert"),uet.forEach(t),Zzo=r(Tye," \u2014 "),JN=n(Tye,"A",{href:!0});var bet=s(JN);eVo=r(bet,"TFDistilBertModel"),bet.forEach(t),oVo=r(Tye," (DistilBERT model)"),Tye.forEach(t),rVo=i(k),q2=n(k,"LI",{});var Fye=s(q2);Ece=n(Fye,"STRONG",{});var vet=s(Ece);tVo=r(vet,"dpr"),vet.forEach(t),aVo=r(Fye," \u2014 "),YN=n(Fye,"A",{href:!0});var Tet=s(YN);nVo=r(Tet,"TFDPRQuestionEncoder"),Tet.forEach(t),sVo=r(Fye," (DPR model)"),Fye.forEach(t),lVo=i(k),G2=n(k,"LI",{});var Cye=s(G2);yce=n(Cye,"STRONG",{});var Fet=s(yce);iVo=r(Fet,"electra"),Fet.forEach(t),dVo=r(Cye," \u2014 "),KN=n(Cye,"A",{href:!0});var Cet=s(KN);cVo=r(Cet,"TFElectraModel"),Cet.forEach(t),fVo=r(Cye," (ELECTRA model)"),Cye.forEach(t),mVo=i(k),O2=n(k,"LI",{});var Mye=s(O2);wce=n(Mye,"STRONG",{});var Met=s(wce);gVo=r(Met,"flaubert"),Met.forEach(t),hVo=r(Mye," \u2014 "),ZN=n(Mye,"A",{href:!0});var Eet=s(ZN);pVo=r(Eet,"TFFlaubertModel"),Eet.forEach(t),_Vo=r(Mye," (FlauBERT model)"),Mye.forEach(t),uVo=i(k),Ss=n(k,"LI",{});var OL=s(Ss);Ace=n(OL,"STRONG",{});var yet=s(Ace);bVo=r(yet,"funnel"),yet.forEach(t),vVo=r(OL," \u2014 "),eD=n(OL,"A",{href:!0});var wet=s(eD);TVo=r(wet,"TFFunnelModel"),wet.forEach(t),FVo=r(OL," or "),oD=n(OL,"A",{href:!0});var Aet=s(oD);CVo=r(Aet,"TFFunnelBaseModel"),Aet.forEach(t),MVo=r(OL," (Funnel Transformer model)"),OL.forEach(t),EVo=i(k),X2=n(k,"LI",{});var Eye=s(X2);Lce=n(Eye,"STRONG",{});var Let=s(Lce);yVo=r(Let,"gpt2"),Let.forEach(t),wVo=r(Eye," \u2014 "),rD=n(Eye,"A",{href:!0});var Bet=s(rD);AVo=r(Bet,"TFGPT2Model"),Bet.forEach(t),LVo=r(Eye," (OpenAI GPT-2 model)"),Eye.forEach(t),BVo=i(k),z2=n(k,"LI",{});var yye=s(z2);Bce=n(yye,"STRONG",{});var ket=s(Bce);kVo=r(ket,"hubert"),ket.forEach(t),xVo=r(yye," \u2014 "),tD=n(yye,"A",{href:!0});var xet=s(tD);RVo=r(xet,"TFHubertModel"),xet.forEach(t),SVo=r(yye," (Hubert model)"),yye.forEach(t),PVo=i(k),V2=n(k,"LI",{});var wye=s(V2);kce=n(wye,"STRONG",{});var Ret=s(kce);$Vo=r(Ret,"layoutlm"),Ret.forEach(t),IVo=r(wye," \u2014 "),aD=n(wye,"A",{href:!0});var Set=s(aD);jVo=r(Set,"TFLayoutLMModel"),Set.forEach(t),NVo=r(wye," (LayoutLM model)"),wye.forEach(t),DVo=i(k),W2=n(k,"LI",{});var Aye=s(W2);xce=n(Aye,"STRONG",{});var Pet=s(xce);qVo=r(Pet,"led"),Pet.forEach(t),GVo=r(Aye," \u2014 "),nD=n(Aye,"A",{href:!0});var $et=s(nD);OVo=r($et,"TFLEDModel"),$et.forEach(t),XVo=r(Aye," (LED model)"),Aye.forEach(t),zVo=i(k),Q2=n(k,"LI",{});var Lye=s(Q2);Rce=n(Lye,"STRONG",{});var Iet=s(Rce);VVo=r(Iet,"longformer"),Iet.forEach(t),WVo=r(Lye," \u2014 "),sD=n(Lye,"A",{href:!0});var jet=s(sD);QVo=r(jet,"TFLongformerModel"),jet.forEach(t),HVo=r(Lye," (Longformer model)"),Lye.forEach(t),UVo=i(k),H2=n(k,"LI",{});var Bye=s(H2);Sce=n(Bye,"STRONG",{});var Net=s(Sce);JVo=r(Net,"lxmert"),Net.forEach(t),YVo=r(Bye," \u2014 "),lD=n(Bye,"A",{href:!0});var Det=s(lD);KVo=r(Det,"TFLxmertModel"),Det.forEach(t),ZVo=r(Bye," (LXMERT model)"),Bye.forEach(t),eWo=i(k),U2=n(k,"LI",{});var kye=s(U2);Pce=n(kye,"STRONG",{});var qet=s(Pce);oWo=r(qet,"marian"),qet.forEach(t),rWo=r(kye," \u2014 "),iD=n(kye,"A",{href:!0});var Get=s(iD);tWo=r(Get,"TFMarianModel"),Get.forEach(t),aWo=r(kye," (Marian model)"),kye.forEach(t),nWo=i(k),J2=n(k,"LI",{});var xye=s(J2);$ce=n(xye,"STRONG",{});var Oet=s($ce);sWo=r(Oet,"mbart"),Oet.forEach(t),lWo=r(xye," \u2014 "),dD=n(xye,"A",{href:!0});var Xet=s(dD);iWo=r(Xet,"TFMBartModel"),Xet.forEach(t),dWo=r(xye," (mBART model)"),xye.forEach(t),cWo=i(k),Y2=n(k,"LI",{});var Rye=s(Y2);Ice=n(Rye,"STRONG",{});var zet=s(Ice);fWo=r(zet,"mobilebert"),zet.forEach(t),mWo=r(Rye," \u2014 "),cD=n(Rye,"A",{href:!0});var Vet=s(cD);gWo=r(Vet,"TFMobileBertModel"),Vet.forEach(t),hWo=r(Rye," (MobileBERT model)"),Rye.forEach(t),pWo=i(k),K2=n(k,"LI",{});var Sye=s(K2);jce=n(Sye,"STRONG",{});var Wet=s(jce);_Wo=r(Wet,"mpnet"),Wet.forEach(t),uWo=r(Sye," \u2014 "),fD=n(Sye,"A",{href:!0});var Qet=s(fD);bWo=r(Qet,"TFMPNetModel"),Qet.forEach(t),vWo=r(Sye," (MPNet model)"),Sye.forEach(t),TWo=i(k),Z2=n(k,"LI",{});var Pye=s(Z2);Nce=n(Pye,"STRONG",{});var Het=s(Nce);FWo=r(Het,"mt5"),Het.forEach(t),CWo=r(Pye," \u2014 "),mD=n(Pye,"A",{href:!0});var Uet=s(mD);MWo=r(Uet,"TFMT5Model"),Uet.forEach(t),EWo=r(Pye," (mT5 model)"),Pye.forEach(t),yWo=i(k),ev=n(k,"LI",{});var $ye=s(ev);Dce=n($ye,"STRONG",{});var Jet=s(Dce);wWo=r(Jet,"openai-gpt"),Jet.forEach(t),AWo=r($ye," \u2014 "),gD=n($ye,"A",{href:!0});var Yet=s(gD);LWo=r(Yet,"TFOpenAIGPTModel"),Yet.forEach(t),BWo=r($ye," (OpenAI GPT model)"),$ye.forEach(t),kWo=i(k),ov=n(k,"LI",{});var Iye=s(ov);qce=n(Iye,"STRONG",{});var Ket=s(qce);xWo=r(Ket,"pegasus"),Ket.forEach(t),RWo=r(Iye," \u2014 "),hD=n(Iye,"A",{href:!0});var Zet=s(hD);SWo=r(Zet,"TFPegasusModel"),Zet.forEach(t),PWo=r(Iye," (Pegasus model)"),Iye.forEach(t),$Wo=i(k),rv=n(k,"LI",{});var jye=s(rv);Gce=n(jye,"STRONG",{});var eot=s(Gce);IWo=r(eot,"rembert"),eot.forEach(t),jWo=r(jye," \u2014 "),pD=n(jye,"A",{href:!0});var oot=s(pD);NWo=r(oot,"TFRemBertModel"),oot.forEach(t),DWo=r(jye," (RemBERT model)"),jye.forEach(t),qWo=i(k),tv=n(k,"LI",{});var Nye=s(tv);Oce=n(Nye,"STRONG",{});var rot=s(Oce);GWo=r(rot,"roberta"),rot.forEach(t),OWo=r(Nye," \u2014 "),_D=n(Nye,"A",{href:!0});var tot=s(_D);XWo=r(tot,"TFRobertaModel"),tot.forEach(t),zWo=r(Nye," (RoBERTa model)"),Nye.forEach(t),VWo=i(k),av=n(k,"LI",{});var Dye=s(av);Xce=n(Dye,"STRONG",{});var aot=s(Xce);WWo=r(aot,"roformer"),aot.forEach(t),QWo=r(Dye," \u2014 "),uD=n(Dye,"A",{href:!0});var not=s(uD);HWo=r(not,"TFRoFormerModel"),not.forEach(t),UWo=r(Dye," (RoFormer model)"),Dye.forEach(t),JWo=i(k),nv=n(k,"LI",{});var qye=s(nv);zce=n(qye,"STRONG",{});var sot=s(zce);YWo=r(sot,"speech_to_text"),sot.forEach(t),KWo=r(qye," \u2014 "),bD=n(qye,"A",{href:!0});var lot=s(bD);ZWo=r(lot,"TFSpeech2TextModel"),lot.forEach(t),eQo=r(qye," (Speech2Text model)"),qye.forEach(t),oQo=i(k),sv=n(k,"LI",{});var Gye=s(sv);Vce=n(Gye,"STRONG",{});var iot=s(Vce);rQo=r(iot,"t5"),iot.forEach(t),tQo=r(Gye," \u2014 "),vD=n(Gye,"A",{href:!0});var dot=s(vD);aQo=r(dot,"TFT5Model"),dot.forEach(t),nQo=r(Gye," (T5 model)"),Gye.forEach(t),sQo=i(k),lv=n(k,"LI",{});var Oye=s(lv);Wce=n(Oye,"STRONG",{});var cot=s(Wce);lQo=r(cot,"tapas"),cot.forEach(t),iQo=r(Oye," \u2014 "),TD=n(Oye,"A",{href:!0});var fot=s(TD);dQo=r(fot,"TFTapasModel"),fot.forEach(t),cQo=r(Oye," (TAPAS model)"),Oye.forEach(t),fQo=i(k),iv=n(k,"LI",{});var Xye=s(iv);Qce=n(Xye,"STRONG",{});var mot=s(Qce);mQo=r(mot,"transfo-xl"),mot.forEach(t),gQo=r(Xye," \u2014 "),FD=n(Xye,"A",{href:!0});var got=s(FD);hQo=r(got,"TFTransfoXLModel"),got.forEach(t),pQo=r(Xye," (Transformer-XL model)"),Xye.forEach(t),_Qo=i(k),dv=n(k,"LI",{});var zye=s(dv);Hce=n(zye,"STRONG",{});var hot=s(Hce);uQo=r(hot,"vit"),hot.forEach(t),bQo=r(zye," \u2014 "),CD=n(zye,"A",{href:!0});var pot=s(CD);vQo=r(pot,"TFViTModel"),pot.forEach(t),TQo=r(zye," (ViT model)"),zye.forEach(t),FQo=i(k),cv=n(k,"LI",{});var Vye=s(cv);Uce=n(Vye,"STRONG",{});var _ot=s(Uce);CQo=r(_ot,"wav2vec2"),_ot.forEach(t),MQo=r(Vye," \u2014 "),MD=n(Vye,"A",{href:!0});var uot=s(MD);EQo=r(uot,"TFWav2Vec2Model"),uot.forEach(t),yQo=r(Vye," (Wav2Vec2 model)"),Vye.forEach(t),wQo=i(k),fv=n(k,"LI",{});var Wye=s(fv);Jce=n(Wye,"STRONG",{});var bot=s(Jce);AQo=r(bot,"xlm"),bot.forEach(t),LQo=r(Wye," \u2014 "),ED=n(Wye,"A",{href:!0});var vot=s(ED);BQo=r(vot,"TFXLMModel"),vot.forEach(t),kQo=r(Wye," (XLM model)"),Wye.forEach(t),xQo=i(k),mv=n(k,"LI",{});var Qye=s(mv);Yce=n(Qye,"STRONG",{});var Tot=s(Yce);RQo=r(Tot,"xlm-roberta"),Tot.forEach(t),SQo=r(Qye," \u2014 "),yD=n(Qye,"A",{href:!0});var Fot=s(yD);PQo=r(Fot,"TFXLMRobertaModel"),Fot.forEach(t),$Qo=r(Qye," (XLM-RoBERTa model)"),Qye.forEach(t),IQo=i(k),gv=n(k,"LI",{});var Hye=s(gv);Kce=n(Hye,"STRONG",{});var Cot=s(Kce);jQo=r(Cot,"xlnet"),Cot.forEach(t),NQo=r(Hye," \u2014 "),wD=n(Hye,"A",{href:!0});var Mot=s(wD);DQo=r(Mot,"TFXLNetModel"),Mot.forEach(t),qQo=r(Hye," (XLNet model)"),Hye.forEach(t),k.forEach(t),GQo=i(ca),Zce=n(ca,"P",{});var Eot=s(Zce);OQo=r(Eot,"Examples:"),Eot.forEach(t),XQo=i(ca),m(Dy.$$.fragment,ca),ca.forEach(t),Bl.forEach(t),E9e=i(d),ac=n(d,"H2",{class:!0});var Ske=s(ac);hv=n(Ske,"A",{id:!0,class:!0,href:!0});var yot=s(hv);efe=n(yot,"SPAN",{});var wot=s(efe);m(qy.$$.fragment,wot),wot.forEach(t),yot.forEach(t),zQo=i(Ske),ofe=n(Ske,"SPAN",{});var Aot=s(ofe);VQo=r(Aot,"TFAutoModelForPreTraining"),Aot.forEach(t),Ske.forEach(t),y9e=i(d),hr=n(d,"DIV",{class:!0});var xl=s(hr);m(Gy.$$.fragment,xl),WQo=i(xl),nc=n(xl,"P",{});var Uz=s(nc);QQo=r(Uz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a pretraining head) when created
with the `),rfe=n(Uz,"CODE",{});var Lot=s(rfe);HQo=r(Lot,"from_pretrained()"),Lot.forEach(t),UQo=r(Uz,"class method or the "),tfe=n(Uz,"CODE",{});var Bot=s(tfe);JQo=r(Bot,"from_config()"),Bot.forEach(t),YQo=r(Uz,`class
method.`),Uz.forEach(t),KQo=i(xl),Oy=n(xl,"P",{});var Pke=s(Oy);ZQo=r(Pke,"This class cannot be instantiated directly using "),afe=n(Pke,"CODE",{});var kot=s(afe);eHo=r(kot,"__init__()"),kot.forEach(t),oHo=r(Pke," (throws an error)."),Pke.forEach(t),rHo=i(xl),lt=n(xl,"DIV",{class:!0});var Rl=s(lt);m(Xy.$$.fragment,Rl),tHo=i(Rl),nfe=n(Rl,"P",{});var xot=s(nfe);aHo=r(xot,"Instantiates one of the model classes of the library (with a pretraining head) from a configuration."),xot.forEach(t),nHo=i(Rl),sc=n(Rl,"P",{});var Jz=s(sc);sHo=r(Jz,`Note:
Loading a model from its configuration file does `),sfe=n(Jz,"STRONG",{});var Rot=s(sfe);lHo=r(Rot,"not"),Rot.forEach(t),iHo=r(Jz,` load the model weights. It only affects the
model\u2019s configuration. Use `),lfe=n(Jz,"CODE",{});var Sot=s(lfe);dHo=r(Sot,"from_pretrained()"),Sot.forEach(t),cHo=r(Jz,"to load the model weights."),Jz.forEach(t),fHo=i(Rl),ife=n(Rl,"P",{});var Pot=s(ife);mHo=r(Pot,"Examples:"),Pot.forEach(t),gHo=i(Rl),m(zy.$$.fragment,Rl),Rl.forEach(t),hHo=i(xl),ho=n(xl,"DIV",{class:!0});var fa=s(ho);m(Vy.$$.fragment,fa),pHo=i(fa),dfe=n(fa,"P",{});var $ot=s(dfe);_Ho=r($ot,"Instantiate one of the model classes of the library (with a pretraining head) from a pretrained model."),$ot.forEach(t),uHo=i(fa),dn=n(fa,"P",{});var P4=s(dn);bHo=r(P4,"The model class to instantiate is selected based on the "),cfe=n(P4,"CODE",{});var Iot=s(cfe);vHo=r(Iot,"model_type"),Iot.forEach(t),THo=r(P4,` property of the config object (either
passed as an argument or loaded from `),ffe=n(P4,"CODE",{});var jot=s(ffe);FHo=r(jot,"pretrained_model_name_or_path"),jot.forEach(t),CHo=r(P4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),mfe=n(P4,"CODE",{});var Not=s(mfe);MHo=r(Not,"pretrained_model_name_or_path"),Not.forEach(t),EHo=r(P4,":"),P4.forEach(t),yHo=i(fa),H=n(fa,"UL",{});var U=s(H);pv=n(U,"LI",{});var Uye=s(pv);gfe=n(Uye,"STRONG",{});var Dot=s(gfe);wHo=r(Dot,"albert"),Dot.forEach(t),AHo=r(Uye," \u2014 "),AD=n(Uye,"A",{href:!0});var qot=s(AD);LHo=r(qot,"TFAlbertForPreTraining"),qot.forEach(t),BHo=r(Uye," (ALBERT model)"),Uye.forEach(t),kHo=i(U),_v=n(U,"LI",{});var Jye=s(_v);hfe=n(Jye,"STRONG",{});var Got=s(hfe);xHo=r(Got,"bart"),Got.forEach(t),RHo=r(Jye," \u2014 "),LD=n(Jye,"A",{href:!0});var Oot=s(LD);SHo=r(Oot,"TFBartForConditionalGeneration"),Oot.forEach(t),PHo=r(Jye," (BART model)"),Jye.forEach(t),$Ho=i(U),uv=n(U,"LI",{});var Yye=s(uv);pfe=n(Yye,"STRONG",{});var Xot=s(pfe);IHo=r(Xot,"bert"),Xot.forEach(t),jHo=r(Yye," \u2014 "),BD=n(Yye,"A",{href:!0});var zot=s(BD);NHo=r(zot,"TFBertForPreTraining"),zot.forEach(t),DHo=r(Yye," (BERT model)"),Yye.forEach(t),qHo=i(U),bv=n(U,"LI",{});var Kye=s(bv);_fe=n(Kye,"STRONG",{});var Vot=s(_fe);GHo=r(Vot,"camembert"),Vot.forEach(t),OHo=r(Kye," \u2014 "),kD=n(Kye,"A",{href:!0});var Wot=s(kD);XHo=r(Wot,"TFCamembertForMaskedLM"),Wot.forEach(t),zHo=r(Kye," (CamemBERT model)"),Kye.forEach(t),VHo=i(U),vv=n(U,"LI",{});var Zye=s(vv);ufe=n(Zye,"STRONG",{});var Qot=s(ufe);WHo=r(Qot,"ctrl"),Qot.forEach(t),QHo=r(Zye," \u2014 "),xD=n(Zye,"A",{href:!0});var Hot=s(xD);HHo=r(Hot,"TFCTRLLMHeadModel"),Hot.forEach(t),UHo=r(Zye," (CTRL model)"),Zye.forEach(t),JHo=i(U),Tv=n(U,"LI",{});var ewe=s(Tv);bfe=n(ewe,"STRONG",{});var Uot=s(bfe);YHo=r(Uot,"distilbert"),Uot.forEach(t),KHo=r(ewe," \u2014 "),RD=n(ewe,"A",{href:!0});var Jot=s(RD);ZHo=r(Jot,"TFDistilBertForMaskedLM"),Jot.forEach(t),eUo=r(ewe," (DistilBERT model)"),ewe.forEach(t),oUo=i(U),Fv=n(U,"LI",{});var owe=s(Fv);vfe=n(owe,"STRONG",{});var Yot=s(vfe);rUo=r(Yot,"electra"),Yot.forEach(t),tUo=r(owe," \u2014 "),SD=n(owe,"A",{href:!0});var Kot=s(SD);aUo=r(Kot,"TFElectraForPreTraining"),Kot.forEach(t),nUo=r(owe," (ELECTRA model)"),owe.forEach(t),sUo=i(U),Cv=n(U,"LI",{});var rwe=s(Cv);Tfe=n(rwe,"STRONG",{});var Zot=s(Tfe);lUo=r(Zot,"flaubert"),Zot.forEach(t),iUo=r(rwe," \u2014 "),PD=n(rwe,"A",{href:!0});var ert=s(PD);dUo=r(ert,"TFFlaubertWithLMHeadModel"),ert.forEach(t),cUo=r(rwe," (FlauBERT model)"),rwe.forEach(t),fUo=i(U),Mv=n(U,"LI",{});var twe=s(Mv);Ffe=n(twe,"STRONG",{});var ort=s(Ffe);mUo=r(ort,"funnel"),ort.forEach(t),gUo=r(twe," \u2014 "),$D=n(twe,"A",{href:!0});var rrt=s($D);hUo=r(rrt,"TFFunnelForPreTraining"),rrt.forEach(t),pUo=r(twe," (Funnel Transformer model)"),twe.forEach(t),_Uo=i(U),Ev=n(U,"LI",{});var awe=s(Ev);Cfe=n(awe,"STRONG",{});var trt=s(Cfe);uUo=r(trt,"gpt2"),trt.forEach(t),bUo=r(awe," \u2014 "),ID=n(awe,"A",{href:!0});var art=s(ID);vUo=r(art,"TFGPT2LMHeadModel"),art.forEach(t),TUo=r(awe," (OpenAI GPT-2 model)"),awe.forEach(t),FUo=i(U),yv=n(U,"LI",{});var nwe=s(yv);Mfe=n(nwe,"STRONG",{});var nrt=s(Mfe);CUo=r(nrt,"layoutlm"),nrt.forEach(t),MUo=r(nwe," \u2014 "),jD=n(nwe,"A",{href:!0});var srt=s(jD);EUo=r(srt,"TFLayoutLMForMaskedLM"),srt.forEach(t),yUo=r(nwe," (LayoutLM model)"),nwe.forEach(t),wUo=i(U),wv=n(U,"LI",{});var swe=s(wv);Efe=n(swe,"STRONG",{});var lrt=s(Efe);AUo=r(lrt,"lxmert"),lrt.forEach(t),LUo=r(swe," \u2014 "),ND=n(swe,"A",{href:!0});var irt=s(ND);BUo=r(irt,"TFLxmertForPreTraining"),irt.forEach(t),kUo=r(swe," (LXMERT model)"),swe.forEach(t),xUo=i(U),Av=n(U,"LI",{});var lwe=s(Av);yfe=n(lwe,"STRONG",{});var drt=s(yfe);RUo=r(drt,"mobilebert"),drt.forEach(t),SUo=r(lwe," \u2014 "),DD=n(lwe,"A",{href:!0});var crt=s(DD);PUo=r(crt,"TFMobileBertForPreTraining"),crt.forEach(t),$Uo=r(lwe," (MobileBERT model)"),lwe.forEach(t),IUo=i(U),Lv=n(U,"LI",{});var iwe=s(Lv);wfe=n(iwe,"STRONG",{});var frt=s(wfe);jUo=r(frt,"mpnet"),frt.forEach(t),NUo=r(iwe," \u2014 "),qD=n(iwe,"A",{href:!0});var mrt=s(qD);DUo=r(mrt,"TFMPNetForMaskedLM"),mrt.forEach(t),qUo=r(iwe," (MPNet model)"),iwe.forEach(t),GUo=i(U),Bv=n(U,"LI",{});var dwe=s(Bv);Afe=n(dwe,"STRONG",{});var grt=s(Afe);OUo=r(grt,"openai-gpt"),grt.forEach(t),XUo=r(dwe," \u2014 "),GD=n(dwe,"A",{href:!0});var hrt=s(GD);zUo=r(hrt,"TFOpenAIGPTLMHeadModel"),hrt.forEach(t),VUo=r(dwe," (OpenAI GPT model)"),dwe.forEach(t),WUo=i(U),kv=n(U,"LI",{});var cwe=s(kv);Lfe=n(cwe,"STRONG",{});var prt=s(Lfe);QUo=r(prt,"roberta"),prt.forEach(t),HUo=r(cwe," \u2014 "),OD=n(cwe,"A",{href:!0});var _rt=s(OD);UUo=r(_rt,"TFRobertaForMaskedLM"),_rt.forEach(t),JUo=r(cwe," (RoBERTa model)"),cwe.forEach(t),YUo=i(U),xv=n(U,"LI",{});var fwe=s(xv);Bfe=n(fwe,"STRONG",{});var urt=s(Bfe);KUo=r(urt,"t5"),urt.forEach(t),ZUo=r(fwe," \u2014 "),XD=n(fwe,"A",{href:!0});var brt=s(XD);eJo=r(brt,"TFT5ForConditionalGeneration"),brt.forEach(t),oJo=r(fwe," (T5 model)"),fwe.forEach(t),rJo=i(U),Rv=n(U,"LI",{});var mwe=s(Rv);kfe=n(mwe,"STRONG",{});var vrt=s(kfe);tJo=r(vrt,"tapas"),vrt.forEach(t),aJo=r(mwe," \u2014 "),zD=n(mwe,"A",{href:!0});var Trt=s(zD);nJo=r(Trt,"TFTapasForMaskedLM"),Trt.forEach(t),sJo=r(mwe," (TAPAS model)"),mwe.forEach(t),lJo=i(U),Sv=n(U,"LI",{});var gwe=s(Sv);xfe=n(gwe,"STRONG",{});var Frt=s(xfe);iJo=r(Frt,"transfo-xl"),Frt.forEach(t),dJo=r(gwe," \u2014 "),VD=n(gwe,"A",{href:!0});var Crt=s(VD);cJo=r(Crt,"TFTransfoXLLMHeadModel"),Crt.forEach(t),fJo=r(gwe," (Transformer-XL model)"),gwe.forEach(t),mJo=i(U),Pv=n(U,"LI",{});var hwe=s(Pv);Rfe=n(hwe,"STRONG",{});var Mrt=s(Rfe);gJo=r(Mrt,"xlm"),Mrt.forEach(t),hJo=r(hwe," \u2014 "),WD=n(hwe,"A",{href:!0});var Ert=s(WD);pJo=r(Ert,"TFXLMWithLMHeadModel"),Ert.forEach(t),_Jo=r(hwe," (XLM model)"),hwe.forEach(t),uJo=i(U),$v=n(U,"LI",{});var pwe=s($v);Sfe=n(pwe,"STRONG",{});var yrt=s(Sfe);bJo=r(yrt,"xlm-roberta"),yrt.forEach(t),vJo=r(pwe," \u2014 "),QD=n(pwe,"A",{href:!0});var wrt=s(QD);TJo=r(wrt,"TFXLMRobertaForMaskedLM"),wrt.forEach(t),FJo=r(pwe," (XLM-RoBERTa model)"),pwe.forEach(t),CJo=i(U),Iv=n(U,"LI",{});var _we=s(Iv);Pfe=n(_we,"STRONG",{});var Art=s(Pfe);MJo=r(Art,"xlnet"),Art.forEach(t),EJo=r(_we," \u2014 "),HD=n(_we,"A",{href:!0});var Lrt=s(HD);yJo=r(Lrt,"TFXLNetLMHeadModel"),Lrt.forEach(t),wJo=r(_we," (XLNet model)"),_we.forEach(t),U.forEach(t),AJo=i(fa),$fe=n(fa,"P",{});var Brt=s($fe);LJo=r(Brt,"Examples:"),Brt.forEach(t),BJo=i(fa),m(Wy.$$.fragment,fa),fa.forEach(t),xl.forEach(t),w9e=i(d),lc=n(d,"H2",{class:!0});var $ke=s(lc);jv=n($ke,"A",{id:!0,class:!0,href:!0});var krt=s(jv);Ife=n(krt,"SPAN",{});var xrt=s(Ife);m(Qy.$$.fragment,xrt),xrt.forEach(t),krt.forEach(t),kJo=i($ke),jfe=n($ke,"SPAN",{});var Rrt=s(jfe);xJo=r(Rrt,"TFAutoModelForCausalLM"),Rrt.forEach(t),$ke.forEach(t),A9e=i(d),pr=n(d,"DIV",{class:!0});var Sl=s(pr);m(Hy.$$.fragment,Sl),RJo=i(Sl),ic=n(Sl,"P",{});var Yz=s(ic);SJo=r(Yz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a causal language modeling head) when created
with the `),Nfe=n(Yz,"CODE",{});var Srt=s(Nfe);PJo=r(Srt,"from_pretrained()"),Srt.forEach(t),$Jo=r(Yz,"class method or the "),Dfe=n(Yz,"CODE",{});var Prt=s(Dfe);IJo=r(Prt,"from_config()"),Prt.forEach(t),jJo=r(Yz,`class
method.`),Yz.forEach(t),NJo=i(Sl),Uy=n(Sl,"P",{});var Ike=s(Uy);DJo=r(Ike,"This class cannot be instantiated directly using "),qfe=n(Ike,"CODE",{});var $rt=s(qfe);qJo=r($rt,"__init__()"),$rt.forEach(t),GJo=r(Ike," (throws an error)."),Ike.forEach(t),OJo=i(Sl),it=n(Sl,"DIV",{class:!0});var Pl=s(it);m(Jy.$$.fragment,Pl),XJo=i(Pl),Gfe=n(Pl,"P",{});var Irt=s(Gfe);zJo=r(Irt,"Instantiates one of the model classes of the library (with a causal language modeling head) from a configuration."),Irt.forEach(t),VJo=i(Pl),dc=n(Pl,"P",{});var Kz=s(dc);WJo=r(Kz,`Note:
Loading a model from its configuration file does `),Ofe=n(Kz,"STRONG",{});var jrt=s(Ofe);QJo=r(jrt,"not"),jrt.forEach(t),HJo=r(Kz,` load the model weights. It only affects the
model\u2019s configuration. Use `),Xfe=n(Kz,"CODE",{});var Nrt=s(Xfe);UJo=r(Nrt,"from_pretrained()"),Nrt.forEach(t),JJo=r(Kz,"to load the model weights."),Kz.forEach(t),YJo=i(Pl),zfe=n(Pl,"P",{});var Drt=s(zfe);KJo=r(Drt,"Examples:"),Drt.forEach(t),ZJo=i(Pl),m(Yy.$$.fragment,Pl),Pl.forEach(t),eYo=i(Sl),po=n(Sl,"DIV",{class:!0});var ma=s(po);m(Ky.$$.fragment,ma),oYo=i(ma),Vfe=n(ma,"P",{});var qrt=s(Vfe);rYo=r(qrt,"Instantiate one of the model classes of the library (with a causal language modeling head) from a pretrained model."),qrt.forEach(t),tYo=i(ma),cn=n(ma,"P",{});var $4=s(cn);aYo=r($4,"The model class to instantiate is selected based on the "),Wfe=n($4,"CODE",{});var Grt=s(Wfe);nYo=r(Grt,"model_type"),Grt.forEach(t),sYo=r($4,` property of the config object (either
passed as an argument or loaded from `),Qfe=n($4,"CODE",{});var Ort=s(Qfe);lYo=r(Ort,"pretrained_model_name_or_path"),Ort.forEach(t),iYo=r($4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Hfe=n($4,"CODE",{});var Xrt=s(Hfe);dYo=r(Xrt,"pretrained_model_name_or_path"),Xrt.forEach(t),cYo=r($4,":"),$4.forEach(t),fYo=i(ma),pe=n(ma,"UL",{});var Ee=s(pe);Nv=n(Ee,"LI",{});var uwe=s(Nv);Ufe=n(uwe,"STRONG",{});var zrt=s(Ufe);mYo=r(zrt,"bert"),zrt.forEach(t),gYo=r(uwe," \u2014 "),UD=n(uwe,"A",{href:!0});var Vrt=s(UD);hYo=r(Vrt,"TFBertLMHeadModel"),Vrt.forEach(t),pYo=r(uwe," (BERT model)"),uwe.forEach(t),_Yo=i(Ee),Dv=n(Ee,"LI",{});var bwe=s(Dv);Jfe=n(bwe,"STRONG",{});var Wrt=s(Jfe);uYo=r(Wrt,"ctrl"),Wrt.forEach(t),bYo=r(bwe," \u2014 "),JD=n(bwe,"A",{href:!0});var Qrt=s(JD);vYo=r(Qrt,"TFCTRLLMHeadModel"),Qrt.forEach(t),TYo=r(bwe," (CTRL model)"),bwe.forEach(t),FYo=i(Ee),qv=n(Ee,"LI",{});var vwe=s(qv);Yfe=n(vwe,"STRONG",{});var Hrt=s(Yfe);CYo=r(Hrt,"gpt2"),Hrt.forEach(t),MYo=r(vwe," \u2014 "),YD=n(vwe,"A",{href:!0});var Urt=s(YD);EYo=r(Urt,"TFGPT2LMHeadModel"),Urt.forEach(t),yYo=r(vwe," (OpenAI GPT-2 model)"),vwe.forEach(t),wYo=i(Ee),Gv=n(Ee,"LI",{});var Twe=s(Gv);Kfe=n(Twe,"STRONG",{});var Jrt=s(Kfe);AYo=r(Jrt,"openai-gpt"),Jrt.forEach(t),LYo=r(Twe," \u2014 "),KD=n(Twe,"A",{href:!0});var Yrt=s(KD);BYo=r(Yrt,"TFOpenAIGPTLMHeadModel"),Yrt.forEach(t),kYo=r(Twe," (OpenAI GPT model)"),Twe.forEach(t),xYo=i(Ee),Ov=n(Ee,"LI",{});var Fwe=s(Ov);Zfe=n(Fwe,"STRONG",{});var Krt=s(Zfe);RYo=r(Krt,"rembert"),Krt.forEach(t),SYo=r(Fwe," \u2014 "),ZD=n(Fwe,"A",{href:!0});var Zrt=s(ZD);PYo=r(Zrt,"TFRemBertForCausalLM"),Zrt.forEach(t),$Yo=r(Fwe," (RemBERT model)"),Fwe.forEach(t),IYo=i(Ee),Xv=n(Ee,"LI",{});var Cwe=s(Xv);eme=n(Cwe,"STRONG",{});var ett=s(eme);jYo=r(ett,"roberta"),ett.forEach(t),NYo=r(Cwe," \u2014 "),eq=n(Cwe,"A",{href:!0});var ott=s(eq);DYo=r(ott,"TFRobertaForCausalLM"),ott.forEach(t),qYo=r(Cwe," (RoBERTa model)"),Cwe.forEach(t),GYo=i(Ee),zv=n(Ee,"LI",{});var Mwe=s(zv);ome=n(Mwe,"STRONG",{});var rtt=s(ome);OYo=r(rtt,"roformer"),rtt.forEach(t),XYo=r(Mwe," \u2014 "),oq=n(Mwe,"A",{href:!0});var ttt=s(oq);zYo=r(ttt,"TFRoFormerForCausalLM"),ttt.forEach(t),VYo=r(Mwe," (RoFormer model)"),Mwe.forEach(t),WYo=i(Ee),Vv=n(Ee,"LI",{});var Ewe=s(Vv);rme=n(Ewe,"STRONG",{});var att=s(rme);QYo=r(att,"transfo-xl"),att.forEach(t),HYo=r(Ewe," \u2014 "),rq=n(Ewe,"A",{href:!0});var ntt=s(rq);UYo=r(ntt,"TFTransfoXLLMHeadModel"),ntt.forEach(t),JYo=r(Ewe," (Transformer-XL model)"),Ewe.forEach(t),YYo=i(Ee),Wv=n(Ee,"LI",{});var ywe=s(Wv);tme=n(ywe,"STRONG",{});var stt=s(tme);KYo=r(stt,"xlm"),stt.forEach(t),ZYo=r(ywe," \u2014 "),tq=n(ywe,"A",{href:!0});var ltt=s(tq);eKo=r(ltt,"TFXLMWithLMHeadModel"),ltt.forEach(t),oKo=r(ywe," (XLM model)"),ywe.forEach(t),rKo=i(Ee),Qv=n(Ee,"LI",{});var wwe=s(Qv);ame=n(wwe,"STRONG",{});var itt=s(ame);tKo=r(itt,"xlnet"),itt.forEach(t),aKo=r(wwe," \u2014 "),aq=n(wwe,"A",{href:!0});var dtt=s(aq);nKo=r(dtt,"TFXLNetLMHeadModel"),dtt.forEach(t),sKo=r(wwe," (XLNet model)"),wwe.forEach(t),Ee.forEach(t),lKo=i(ma),nme=n(ma,"P",{});var ctt=s(nme);iKo=r(ctt,"Examples:"),ctt.forEach(t),dKo=i(ma),m(Zy.$$.fragment,ma),ma.forEach(t),Sl.forEach(t),L9e=i(d),cc=n(d,"H2",{class:!0});var jke=s(cc);Hv=n(jke,"A",{id:!0,class:!0,href:!0});var ftt=s(Hv);sme=n(ftt,"SPAN",{});var mtt=s(sme);m(ew.$$.fragment,mtt),mtt.forEach(t),ftt.forEach(t),cKo=i(jke),lme=n(jke,"SPAN",{});var gtt=s(lme);fKo=r(gtt,"TFAutoModelForImageClassification"),gtt.forEach(t),jke.forEach(t),B9e=i(d),_r=n(d,"DIV",{class:!0});var $l=s(_r);m(ow.$$.fragment,$l),mKo=i($l),fc=n($l,"P",{});var Zz=s(fc);gKo=r(Zz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a image classification head) when created
with the `),ime=n(Zz,"CODE",{});var htt=s(ime);hKo=r(htt,"from_pretrained()"),htt.forEach(t),pKo=r(Zz,"class method or the "),dme=n(Zz,"CODE",{});var ptt=s(dme);_Ko=r(ptt,"from_config()"),ptt.forEach(t),uKo=r(Zz,`class
method.`),Zz.forEach(t),bKo=i($l),rw=n($l,"P",{});var Nke=s(rw);vKo=r(Nke,"This class cannot be instantiated directly using "),cme=n(Nke,"CODE",{});var _tt=s(cme);TKo=r(_tt,"__init__()"),_tt.forEach(t),FKo=r(Nke," (throws an error)."),Nke.forEach(t),CKo=i($l),dt=n($l,"DIV",{class:!0});var Il=s(dt);m(tw.$$.fragment,Il),MKo=i(Il),fme=n(Il,"P",{});var utt=s(fme);EKo=r(utt,"Instantiates one of the model classes of the library (with a image classification head) from a configuration."),utt.forEach(t),yKo=i(Il),mc=n(Il,"P",{});var eV=s(mc);wKo=r(eV,`Note:
Loading a model from its configuration file does `),mme=n(eV,"STRONG",{});var btt=s(mme);AKo=r(btt,"not"),btt.forEach(t),LKo=r(eV,` load the model weights. It only affects the
model\u2019s configuration. Use `),gme=n(eV,"CODE",{});var vtt=s(gme);BKo=r(vtt,"from_pretrained()"),vtt.forEach(t),kKo=r(eV,"to load the model weights."),eV.forEach(t),xKo=i(Il),hme=n(Il,"P",{});var Ttt=s(hme);RKo=r(Ttt,"Examples:"),Ttt.forEach(t),SKo=i(Il),m(aw.$$.fragment,Il),Il.forEach(t),PKo=i($l),_o=n($l,"DIV",{class:!0});var ga=s(_o);m(nw.$$.fragment,ga),$Ko=i(ga),pme=n(ga,"P",{});var Ftt=s(pme);IKo=r(Ftt,"Instantiate one of the model classes of the library (with a image classification head) from a pretrained model."),Ftt.forEach(t),jKo=i(ga),fn=n(ga,"P",{});var I4=s(fn);NKo=r(I4,"The model class to instantiate is selected based on the "),_me=n(I4,"CODE",{});var Ctt=s(_me);DKo=r(Ctt,"model_type"),Ctt.forEach(t),qKo=r(I4,` property of the config object (either
passed as an argument or loaded from `),ume=n(I4,"CODE",{});var Mtt=s(ume);GKo=r(Mtt,"pretrained_model_name_or_path"),Mtt.forEach(t),OKo=r(I4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),bme=n(I4,"CODE",{});var Ett=s(bme);XKo=r(Ett,"pretrained_model_name_or_path"),Ett.forEach(t),zKo=r(I4,":"),I4.forEach(t),VKo=i(ga),vme=n(ga,"UL",{});var ytt=s(vme);Uv=n(ytt,"LI",{});var Awe=s(Uv);Tme=n(Awe,"STRONG",{});var wtt=s(Tme);WKo=r(wtt,"vit"),wtt.forEach(t),QKo=r(Awe," \u2014 "),nq=n(Awe,"A",{href:!0});var Att=s(nq);HKo=r(Att,"TFViTForImageClassification"),Att.forEach(t),UKo=r(Awe," (ViT model)"),Awe.forEach(t),ytt.forEach(t),JKo=i(ga),Fme=n(ga,"P",{});var Ltt=s(Fme);YKo=r(Ltt,"Examples:"),Ltt.forEach(t),KKo=i(ga),m(sw.$$.fragment,ga),ga.forEach(t),$l.forEach(t),k9e=i(d),gc=n(d,"H2",{class:!0});var Dke=s(gc);Jv=n(Dke,"A",{id:!0,class:!0,href:!0});var Btt=s(Jv);Cme=n(Btt,"SPAN",{});var ktt=s(Cme);m(lw.$$.fragment,ktt),ktt.forEach(t),Btt.forEach(t),ZKo=i(Dke),Mme=n(Dke,"SPAN",{});var xtt=s(Mme);eZo=r(xtt,"TFAutoModelForMaskedLM"),xtt.forEach(t),Dke.forEach(t),x9e=i(d),ur=n(d,"DIV",{class:!0});var jl=s(ur);m(iw.$$.fragment,jl),oZo=i(jl),hc=n(jl,"P",{});var oV=s(hc);rZo=r(oV,`This is a generic model class that will be instantiated as one of the model classes of the library (with a masked language modeling head) when created
with the `),Eme=n(oV,"CODE",{});var Rtt=s(Eme);tZo=r(Rtt,"from_pretrained()"),Rtt.forEach(t),aZo=r(oV,"class method or the "),yme=n(oV,"CODE",{});var Stt=s(yme);nZo=r(Stt,"from_config()"),Stt.forEach(t),sZo=r(oV,`class
method.`),oV.forEach(t),lZo=i(jl),dw=n(jl,"P",{});var qke=s(dw);iZo=r(qke,"This class cannot be instantiated directly using "),wme=n(qke,"CODE",{});var Ptt=s(wme);dZo=r(Ptt,"__init__()"),Ptt.forEach(t),cZo=r(qke," (throws an error)."),qke.forEach(t),fZo=i(jl),ct=n(jl,"DIV",{class:!0});var Nl=s(ct);m(cw.$$.fragment,Nl),mZo=i(Nl),Ame=n(Nl,"P",{});var $tt=s(Ame);gZo=r($tt,"Instantiates one of the model classes of the library (with a masked language modeling head) from a configuration."),$tt.forEach(t),hZo=i(Nl),pc=n(Nl,"P",{});var rV=s(pc);pZo=r(rV,`Note:
Loading a model from its configuration file does `),Lme=n(rV,"STRONG",{});var Itt=s(Lme);_Zo=r(Itt,"not"),Itt.forEach(t),uZo=r(rV,` load the model weights. It only affects the
model\u2019s configuration. Use `),Bme=n(rV,"CODE",{});var jtt=s(Bme);bZo=r(jtt,"from_pretrained()"),jtt.forEach(t),vZo=r(rV,"to load the model weights."),rV.forEach(t),TZo=i(Nl),kme=n(Nl,"P",{});var Ntt=s(kme);FZo=r(Ntt,"Examples:"),Ntt.forEach(t),CZo=i(Nl),m(fw.$$.fragment,Nl),Nl.forEach(t),MZo=i(jl),uo=n(jl,"DIV",{class:!0});var ha=s(uo);m(mw.$$.fragment,ha),EZo=i(ha),xme=n(ha,"P",{});var Dtt=s(xme);yZo=r(Dtt,"Instantiate one of the model classes of the library (with a masked language modeling head) from a pretrained model."),Dtt.forEach(t),wZo=i(ha),mn=n(ha,"P",{});var j4=s(mn);AZo=r(j4,"The model class to instantiate is selected based on the "),Rme=n(j4,"CODE",{});var qtt=s(Rme);LZo=r(qtt,"model_type"),qtt.forEach(t),BZo=r(j4,` property of the config object (either
passed as an argument or loaded from `),Sme=n(j4,"CODE",{});var Gtt=s(Sme);kZo=r(Gtt,"pretrained_model_name_or_path"),Gtt.forEach(t),xZo=r(j4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Pme=n(j4,"CODE",{});var Ott=s(Pme);RZo=r(Ott,"pretrained_model_name_or_path"),Ott.forEach(t),SZo=r(j4,":"),j4.forEach(t),PZo=i(ha),Y=n(ha,"UL",{});var ee=s(Y);Yv=n(ee,"LI",{});var Lwe=s(Yv);$me=n(Lwe,"STRONG",{});var Xtt=s($me);$Zo=r(Xtt,"albert"),Xtt.forEach(t),IZo=r(Lwe," \u2014 "),sq=n(Lwe,"A",{href:!0});var ztt=s(sq);jZo=r(ztt,"TFAlbertForMaskedLM"),ztt.forEach(t),NZo=r(Lwe," (ALBERT model)"),Lwe.forEach(t),DZo=i(ee),Kv=n(ee,"LI",{});var Bwe=s(Kv);Ime=n(Bwe,"STRONG",{});var Vtt=s(Ime);qZo=r(Vtt,"bert"),Vtt.forEach(t),GZo=r(Bwe," \u2014 "),lq=n(Bwe,"A",{href:!0});var Wtt=s(lq);OZo=r(Wtt,"TFBertForMaskedLM"),Wtt.forEach(t),XZo=r(Bwe," (BERT model)"),Bwe.forEach(t),zZo=i(ee),Zv=n(ee,"LI",{});var kwe=s(Zv);jme=n(kwe,"STRONG",{});var Qtt=s(jme);VZo=r(Qtt,"camembert"),Qtt.forEach(t),WZo=r(kwe," \u2014 "),iq=n(kwe,"A",{href:!0});var Htt=s(iq);QZo=r(Htt,"TFCamembertForMaskedLM"),Htt.forEach(t),HZo=r(kwe," (CamemBERT model)"),kwe.forEach(t),UZo=i(ee),e0=n(ee,"LI",{});var xwe=s(e0);Nme=n(xwe,"STRONG",{});var Utt=s(Nme);JZo=r(Utt,"convbert"),Utt.forEach(t),YZo=r(xwe," \u2014 "),dq=n(xwe,"A",{href:!0});var Jtt=s(dq);KZo=r(Jtt,"TFConvBertForMaskedLM"),Jtt.forEach(t),ZZo=r(xwe," (ConvBERT model)"),xwe.forEach(t),eer=i(ee),o0=n(ee,"LI",{});var Rwe=s(o0);Dme=n(Rwe,"STRONG",{});var Ytt=s(Dme);oer=r(Ytt,"deberta"),Ytt.forEach(t),rer=r(Rwe," \u2014 "),cq=n(Rwe,"A",{href:!0});var Ktt=s(cq);ter=r(Ktt,"TFDebertaForMaskedLM"),Ktt.forEach(t),aer=r(Rwe," (DeBERTa model)"),Rwe.forEach(t),ner=i(ee),r0=n(ee,"LI",{});var Swe=s(r0);qme=n(Swe,"STRONG",{});var Ztt=s(qme);ser=r(Ztt,"deberta-v2"),Ztt.forEach(t),ler=r(Swe," \u2014 "),fq=n(Swe,"A",{href:!0});var eat=s(fq);ier=r(eat,"TFDebertaV2ForMaskedLM"),eat.forEach(t),der=r(Swe," (DeBERTa-v2 model)"),Swe.forEach(t),cer=i(ee),t0=n(ee,"LI",{});var Pwe=s(t0);Gme=n(Pwe,"STRONG",{});var oat=s(Gme);fer=r(oat,"distilbert"),oat.forEach(t),mer=r(Pwe," \u2014 "),mq=n(Pwe,"A",{href:!0});var rat=s(mq);ger=r(rat,"TFDistilBertForMaskedLM"),rat.forEach(t),her=r(Pwe," (DistilBERT model)"),Pwe.forEach(t),per=i(ee),a0=n(ee,"LI",{});var $we=s(a0);Ome=n($we,"STRONG",{});var tat=s(Ome);_er=r(tat,"electra"),tat.forEach(t),uer=r($we," \u2014 "),gq=n($we,"A",{href:!0});var aat=s(gq);ber=r(aat,"TFElectraForMaskedLM"),aat.forEach(t),ver=r($we," (ELECTRA model)"),$we.forEach(t),Ter=i(ee),n0=n(ee,"LI",{});var Iwe=s(n0);Xme=n(Iwe,"STRONG",{});var nat=s(Xme);Fer=r(nat,"flaubert"),nat.forEach(t),Cer=r(Iwe," \u2014 "),hq=n(Iwe,"A",{href:!0});var sat=s(hq);Mer=r(sat,"TFFlaubertWithLMHeadModel"),sat.forEach(t),Eer=r(Iwe," (FlauBERT model)"),Iwe.forEach(t),yer=i(ee),s0=n(ee,"LI",{});var jwe=s(s0);zme=n(jwe,"STRONG",{});var lat=s(zme);wer=r(lat,"funnel"),lat.forEach(t),Aer=r(jwe," \u2014 "),pq=n(jwe,"A",{href:!0});var iat=s(pq);Ler=r(iat,"TFFunnelForMaskedLM"),iat.forEach(t),Ber=r(jwe," (Funnel Transformer model)"),jwe.forEach(t),ker=i(ee),l0=n(ee,"LI",{});var Nwe=s(l0);Vme=n(Nwe,"STRONG",{});var dat=s(Vme);xer=r(dat,"layoutlm"),dat.forEach(t),Rer=r(Nwe," \u2014 "),_q=n(Nwe,"A",{href:!0});var cat=s(_q);Ser=r(cat,"TFLayoutLMForMaskedLM"),cat.forEach(t),Per=r(Nwe," (LayoutLM model)"),Nwe.forEach(t),$er=i(ee),i0=n(ee,"LI",{});var Dwe=s(i0);Wme=n(Dwe,"STRONG",{});var fat=s(Wme);Ier=r(fat,"longformer"),fat.forEach(t),jer=r(Dwe," \u2014 "),uq=n(Dwe,"A",{href:!0});var mat=s(uq);Ner=r(mat,"TFLongformerForMaskedLM"),mat.forEach(t),Der=r(Dwe," (Longformer model)"),Dwe.forEach(t),qer=i(ee),d0=n(ee,"LI",{});var qwe=s(d0);Qme=n(qwe,"STRONG",{});var gat=s(Qme);Ger=r(gat,"mobilebert"),gat.forEach(t),Oer=r(qwe," \u2014 "),bq=n(qwe,"A",{href:!0});var hat=s(bq);Xer=r(hat,"TFMobileBertForMaskedLM"),hat.forEach(t),zer=r(qwe," (MobileBERT model)"),qwe.forEach(t),Ver=i(ee),c0=n(ee,"LI",{});var Gwe=s(c0);Hme=n(Gwe,"STRONG",{});var pat=s(Hme);Wer=r(pat,"mpnet"),pat.forEach(t),Qer=r(Gwe," \u2014 "),vq=n(Gwe,"A",{href:!0});var _at=s(vq);Her=r(_at,"TFMPNetForMaskedLM"),_at.forEach(t),Uer=r(Gwe," (MPNet model)"),Gwe.forEach(t),Jer=i(ee),f0=n(ee,"LI",{});var Owe=s(f0);Ume=n(Owe,"STRONG",{});var uat=s(Ume);Yer=r(uat,"rembert"),uat.forEach(t),Ker=r(Owe," \u2014 "),Tq=n(Owe,"A",{href:!0});var bat=s(Tq);Zer=r(bat,"TFRemBertForMaskedLM"),bat.forEach(t),eor=r(Owe," (RemBERT model)"),Owe.forEach(t),oor=i(ee),m0=n(ee,"LI",{});var Xwe=s(m0);Jme=n(Xwe,"STRONG",{});var vat=s(Jme);ror=r(vat,"roberta"),vat.forEach(t),tor=r(Xwe," \u2014 "),Fq=n(Xwe,"A",{href:!0});var Tat=s(Fq);aor=r(Tat,"TFRobertaForMaskedLM"),Tat.forEach(t),nor=r(Xwe," (RoBERTa model)"),Xwe.forEach(t),sor=i(ee),g0=n(ee,"LI",{});var zwe=s(g0);Yme=n(zwe,"STRONG",{});var Fat=s(Yme);lor=r(Fat,"roformer"),Fat.forEach(t),ior=r(zwe," \u2014 "),Cq=n(zwe,"A",{href:!0});var Cat=s(Cq);dor=r(Cat,"TFRoFormerForMaskedLM"),Cat.forEach(t),cor=r(zwe," (RoFormer model)"),zwe.forEach(t),mor=i(ee),h0=n(ee,"LI",{});var Vwe=s(h0);Kme=n(Vwe,"STRONG",{});var Mat=s(Kme);gor=r(Mat,"tapas"),Mat.forEach(t),hor=r(Vwe," \u2014 "),Mq=n(Vwe,"A",{href:!0});var Eat=s(Mq);por=r(Eat,"TFTapasForMaskedLM"),Eat.forEach(t),_or=r(Vwe," (TAPAS model)"),Vwe.forEach(t),uor=i(ee),p0=n(ee,"LI",{});var Wwe=s(p0);Zme=n(Wwe,"STRONG",{});var yat=s(Zme);bor=r(yat,"xlm"),yat.forEach(t),vor=r(Wwe," \u2014 "),Eq=n(Wwe,"A",{href:!0});var wat=s(Eq);Tor=r(wat,"TFXLMWithLMHeadModel"),wat.forEach(t),For=r(Wwe," (XLM model)"),Wwe.forEach(t),Cor=i(ee),_0=n(ee,"LI",{});var Qwe=s(_0);ege=n(Qwe,"STRONG",{});var Aat=s(ege);Mor=r(Aat,"xlm-roberta"),Aat.forEach(t),Eor=r(Qwe," \u2014 "),yq=n(Qwe,"A",{href:!0});var Lat=s(yq);yor=r(Lat,"TFXLMRobertaForMaskedLM"),Lat.forEach(t),wor=r(Qwe," (XLM-RoBERTa model)"),Qwe.forEach(t),ee.forEach(t),Aor=i(ha),oge=n(ha,"P",{});var Bat=s(oge);Lor=r(Bat,"Examples:"),Bat.forEach(t),Bor=i(ha),m(gw.$$.fragment,ha),ha.forEach(t),jl.forEach(t),R9e=i(d),_c=n(d,"H2",{class:!0});var Gke=s(_c);u0=n(Gke,"A",{id:!0,class:!0,href:!0});var kat=s(u0);rge=n(kat,"SPAN",{});var xat=s(rge);m(hw.$$.fragment,xat),xat.forEach(t),kat.forEach(t),kor=i(Gke),tge=n(Gke,"SPAN",{});var Rat=s(tge);xor=r(Rat,"TFAutoModelForSeq2SeqLM"),Rat.forEach(t),Gke.forEach(t),S9e=i(d),br=n(d,"DIV",{class:!0});var Dl=s(br);m(pw.$$.fragment,Dl),Ror=i(Dl),uc=n(Dl,"P",{});var tV=s(uc);Sor=r(tV,`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence language modeling head) when created
with the `),age=n(tV,"CODE",{});var Sat=s(age);Por=r(Sat,"from_pretrained()"),Sat.forEach(t),$or=r(tV,"class method or the "),nge=n(tV,"CODE",{});var Pat=s(nge);Ior=r(Pat,"from_config()"),Pat.forEach(t),jor=r(tV,`class
method.`),tV.forEach(t),Nor=i(Dl),_w=n(Dl,"P",{});var Oke=s(_w);Dor=r(Oke,"This class cannot be instantiated directly using "),sge=n(Oke,"CODE",{});var $at=s(sge);qor=r($at,"__init__()"),$at.forEach(t),Gor=r(Oke," (throws an error)."),Oke.forEach(t),Oor=i(Dl),ft=n(Dl,"DIV",{class:!0});var ql=s(ft);m(uw.$$.fragment,ql),Xor=i(ql),lge=n(ql,"P",{});var Iat=s(lge);zor=r(Iat,"Instantiates one of the model classes of the library (with a sequence-to-sequence language modeling head) from a configuration."),Iat.forEach(t),Vor=i(ql),bc=n(ql,"P",{});var aV=s(bc);Wor=r(aV,`Note:
Loading a model from its configuration file does `),ige=n(aV,"STRONG",{});var jat=s(ige);Qor=r(jat,"not"),jat.forEach(t),Hor=r(aV,` load the model weights. It only affects the
model\u2019s configuration. Use `),dge=n(aV,"CODE",{});var Nat=s(dge);Uor=r(Nat,"from_pretrained()"),Nat.forEach(t),Jor=r(aV,"to load the model weights."),aV.forEach(t),Yor=i(ql),cge=n(ql,"P",{});var Dat=s(cge);Kor=r(Dat,"Examples:"),Dat.forEach(t),Zor=i(ql),m(bw.$$.fragment,ql),ql.forEach(t),err=i(Dl),bo=n(Dl,"DIV",{class:!0});var pa=s(bo);m(vw.$$.fragment,pa),orr=i(pa),fge=n(pa,"P",{});var qat=s(fge);rrr=r(qat,"Instantiate one of the model classes of the library (with a sequence-to-sequence language modeling head) from a pretrained model."),qat.forEach(t),trr=i(pa),gn=n(pa,"P",{});var N4=s(gn);arr=r(N4,"The model class to instantiate is selected based on the "),mge=n(N4,"CODE",{});var Gat=s(mge);nrr=r(Gat,"model_type"),Gat.forEach(t),srr=r(N4,` property of the config object (either
passed as an argument or loaded from `),gge=n(N4,"CODE",{});var Oat=s(gge);lrr=r(Oat,"pretrained_model_name_or_path"),Oat.forEach(t),irr=r(N4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),hge=n(N4,"CODE",{});var Xat=s(hge);drr=r(Xat,"pretrained_model_name_or_path"),Xat.forEach(t),crr=r(N4,":"),N4.forEach(t),frr=i(pa),_e=n(pa,"UL",{});var ye=s(_e);b0=n(ye,"LI",{});var Hwe=s(b0);pge=n(Hwe,"STRONG",{});var zat=s(pge);mrr=r(zat,"bart"),zat.forEach(t),grr=r(Hwe," \u2014 "),wq=n(Hwe,"A",{href:!0});var Vat=s(wq);hrr=r(Vat,"TFBartForConditionalGeneration"),Vat.forEach(t),prr=r(Hwe," (BART model)"),Hwe.forEach(t),_rr=i(ye),v0=n(ye,"LI",{});var Uwe=s(v0);_ge=n(Uwe,"STRONG",{});var Wat=s(_ge);urr=r(Wat,"blenderbot"),Wat.forEach(t),brr=r(Uwe," \u2014 "),Aq=n(Uwe,"A",{href:!0});var Qat=s(Aq);vrr=r(Qat,"TFBlenderbotForConditionalGeneration"),Qat.forEach(t),Trr=r(Uwe," (Blenderbot model)"),Uwe.forEach(t),Frr=i(ye),T0=n(ye,"LI",{});var Jwe=s(T0);uge=n(Jwe,"STRONG",{});var Hat=s(uge);Crr=r(Hat,"blenderbot-small"),Hat.forEach(t),Mrr=r(Jwe," \u2014 "),Lq=n(Jwe,"A",{href:!0});var Uat=s(Lq);Err=r(Uat,"TFBlenderbotSmallForConditionalGeneration"),Uat.forEach(t),yrr=r(Jwe," (BlenderbotSmall model)"),Jwe.forEach(t),wrr=i(ye),F0=n(ye,"LI",{});var Ywe=s(F0);bge=n(Ywe,"STRONG",{});var Jat=s(bge);Arr=r(Jat,"encoder-decoder"),Jat.forEach(t),Lrr=r(Ywe," \u2014 "),Bq=n(Ywe,"A",{href:!0});var Yat=s(Bq);Brr=r(Yat,"TFEncoderDecoderModel"),Yat.forEach(t),krr=r(Ywe," (Encoder decoder model)"),Ywe.forEach(t),xrr=i(ye),C0=n(ye,"LI",{});var Kwe=s(C0);vge=n(Kwe,"STRONG",{});var Kat=s(vge);Rrr=r(Kat,"led"),Kat.forEach(t),Srr=r(Kwe," \u2014 "),kq=n(Kwe,"A",{href:!0});var Zat=s(kq);Prr=r(Zat,"TFLEDForConditionalGeneration"),Zat.forEach(t),$rr=r(Kwe," (LED model)"),Kwe.forEach(t),Irr=i(ye),M0=n(ye,"LI",{});var Zwe=s(M0);Tge=n(Zwe,"STRONG",{});var ent=s(Tge);jrr=r(ent,"marian"),ent.forEach(t),Nrr=r(Zwe," \u2014 "),xq=n(Zwe,"A",{href:!0});var ont=s(xq);Drr=r(ont,"TFMarianMTModel"),ont.forEach(t),qrr=r(Zwe," (Marian model)"),Zwe.forEach(t),Grr=i(ye),E0=n(ye,"LI",{});var e6e=s(E0);Fge=n(e6e,"STRONG",{});var rnt=s(Fge);Orr=r(rnt,"mbart"),rnt.forEach(t),Xrr=r(e6e," \u2014 "),Rq=n(e6e,"A",{href:!0});var tnt=s(Rq);zrr=r(tnt,"TFMBartForConditionalGeneration"),tnt.forEach(t),Vrr=r(e6e," (mBART model)"),e6e.forEach(t),Wrr=i(ye),y0=n(ye,"LI",{});var o6e=s(y0);Cge=n(o6e,"STRONG",{});var ant=s(Cge);Qrr=r(ant,"mt5"),ant.forEach(t),Hrr=r(o6e," \u2014 "),Sq=n(o6e,"A",{href:!0});var nnt=s(Sq);Urr=r(nnt,"TFMT5ForConditionalGeneration"),nnt.forEach(t),Jrr=r(o6e," (mT5 model)"),o6e.forEach(t),Yrr=i(ye),w0=n(ye,"LI",{});var r6e=s(w0);Mge=n(r6e,"STRONG",{});var snt=s(Mge);Krr=r(snt,"pegasus"),snt.forEach(t),Zrr=r(r6e," \u2014 "),Pq=n(r6e,"A",{href:!0});var lnt=s(Pq);etr=r(lnt,"TFPegasusForConditionalGeneration"),lnt.forEach(t),otr=r(r6e," (Pegasus model)"),r6e.forEach(t),rtr=i(ye),A0=n(ye,"LI",{});var t6e=s(A0);Ege=n(t6e,"STRONG",{});var int=s(Ege);ttr=r(int,"t5"),int.forEach(t),atr=r(t6e," \u2014 "),$q=n(t6e,"A",{href:!0});var dnt=s($q);ntr=r(dnt,"TFT5ForConditionalGeneration"),dnt.forEach(t),str=r(t6e," (T5 model)"),t6e.forEach(t),ye.forEach(t),ltr=i(pa),yge=n(pa,"P",{});var cnt=s(yge);itr=r(cnt,"Examples:"),cnt.forEach(t),dtr=i(pa),m(Tw.$$.fragment,pa),pa.forEach(t),Dl.forEach(t),P9e=i(d),vc=n(d,"H2",{class:!0});var Xke=s(vc);L0=n(Xke,"A",{id:!0,class:!0,href:!0});var fnt=s(L0);wge=n(fnt,"SPAN",{});var mnt=s(wge);m(Fw.$$.fragment,mnt),mnt.forEach(t),fnt.forEach(t),ctr=i(Xke),Age=n(Xke,"SPAN",{});var gnt=s(Age);ftr=r(gnt,"TFAutoModelForSequenceClassification"),gnt.forEach(t),Xke.forEach(t),$9e=i(d),vr=n(d,"DIV",{class:!0});var Gl=s(vr);m(Cw.$$.fragment,Gl),mtr=i(Gl),Tc=n(Gl,"P",{});var nV=s(Tc);gtr=r(nV,`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence classification head) when created
with the `),Lge=n(nV,"CODE",{});var hnt=s(Lge);htr=r(hnt,"from_pretrained()"),hnt.forEach(t),ptr=r(nV,"class method or the "),Bge=n(nV,"CODE",{});var pnt=s(Bge);_tr=r(pnt,"from_config()"),pnt.forEach(t),utr=r(nV,`class
method.`),nV.forEach(t),btr=i(Gl),Mw=n(Gl,"P",{});var zke=s(Mw);vtr=r(zke,"This class cannot be instantiated directly using "),kge=n(zke,"CODE",{});var _nt=s(kge);Ttr=r(_nt,"__init__()"),_nt.forEach(t),Ftr=r(zke," (throws an error)."),zke.forEach(t),Ctr=i(Gl),mt=n(Gl,"DIV",{class:!0});var Ol=s(mt);m(Ew.$$.fragment,Ol),Mtr=i(Ol),xge=n(Ol,"P",{});var unt=s(xge);Etr=r(unt,"Instantiates one of the model classes of the library (with a sequence classification head) from a configuration."),unt.forEach(t),ytr=i(Ol),Fc=n(Ol,"P",{});var sV=s(Fc);wtr=r(sV,`Note:
Loading a model from its configuration file does `),Rge=n(sV,"STRONG",{});var bnt=s(Rge);Atr=r(bnt,"not"),bnt.forEach(t),Ltr=r(sV,` load the model weights. It only affects the
model\u2019s configuration. Use `),Sge=n(sV,"CODE",{});var vnt=s(Sge);Btr=r(vnt,"from_pretrained()"),vnt.forEach(t),ktr=r(sV,"to load the model weights."),sV.forEach(t),xtr=i(Ol),Pge=n(Ol,"P",{});var Tnt=s(Pge);Rtr=r(Tnt,"Examples:"),Tnt.forEach(t),Str=i(Ol),m(yw.$$.fragment,Ol),Ol.forEach(t),Ptr=i(Gl),vo=n(Gl,"DIV",{class:!0});var _a=s(vo);m(ww.$$.fragment,_a),$tr=i(_a),$ge=n(_a,"P",{});var Fnt=s($ge);Itr=r(Fnt,"Instantiate one of the model classes of the library (with a sequence classification head) from a pretrained model."),Fnt.forEach(t),jtr=i(_a),hn=n(_a,"P",{});var D4=s(hn);Ntr=r(D4,"The model class to instantiate is selected based on the "),Ige=n(D4,"CODE",{});var Cnt=s(Ige);Dtr=r(Cnt,"model_type"),Cnt.forEach(t),qtr=r(D4,` property of the config object (either
passed as an argument or loaded from `),jge=n(D4,"CODE",{});var Mnt=s(jge);Gtr=r(Mnt,"pretrained_model_name_or_path"),Mnt.forEach(t),Otr=r(D4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Nge=n(D4,"CODE",{});var Ent=s(Nge);Xtr=r(Ent,"pretrained_model_name_or_path"),Ent.forEach(t),ztr=r(D4,":"),D4.forEach(t),Vtr=i(_a),X=n(_a,"UL",{});var W=s(X);B0=n(W,"LI",{});var a6e=s(B0);Dge=n(a6e,"STRONG",{});var ynt=s(Dge);Wtr=r(ynt,"albert"),ynt.forEach(t),Qtr=r(a6e," \u2014 "),Iq=n(a6e,"A",{href:!0});var wnt=s(Iq);Htr=r(wnt,"TFAlbertForSequenceClassification"),wnt.forEach(t),Utr=r(a6e," (ALBERT model)"),a6e.forEach(t),Jtr=i(W),k0=n(W,"LI",{});var n6e=s(k0);qge=n(n6e,"STRONG",{});var Ant=s(qge);Ytr=r(Ant,"bert"),Ant.forEach(t),Ktr=r(n6e," \u2014 "),jq=n(n6e,"A",{href:!0});var Lnt=s(jq);Ztr=r(Lnt,"TFBertForSequenceClassification"),Lnt.forEach(t),ear=r(n6e," (BERT model)"),n6e.forEach(t),oar=i(W),x0=n(W,"LI",{});var s6e=s(x0);Gge=n(s6e,"STRONG",{});var Bnt=s(Gge);rar=r(Bnt,"camembert"),Bnt.forEach(t),tar=r(s6e," \u2014 "),Nq=n(s6e,"A",{href:!0});var knt=s(Nq);aar=r(knt,"TFCamembertForSequenceClassification"),knt.forEach(t),nar=r(s6e," (CamemBERT model)"),s6e.forEach(t),sar=i(W),R0=n(W,"LI",{});var l6e=s(R0);Oge=n(l6e,"STRONG",{});var xnt=s(Oge);lar=r(xnt,"convbert"),xnt.forEach(t),iar=r(l6e," \u2014 "),Dq=n(l6e,"A",{href:!0});var Rnt=s(Dq);dar=r(Rnt,"TFConvBertForSequenceClassification"),Rnt.forEach(t),car=r(l6e," (ConvBERT model)"),l6e.forEach(t),far=i(W),S0=n(W,"LI",{});var i6e=s(S0);Xge=n(i6e,"STRONG",{});var Snt=s(Xge);mar=r(Snt,"ctrl"),Snt.forEach(t),gar=r(i6e," \u2014 "),qq=n(i6e,"A",{href:!0});var Pnt=s(qq);har=r(Pnt,"TFCTRLForSequenceClassification"),Pnt.forEach(t),par=r(i6e," (CTRL model)"),i6e.forEach(t),_ar=i(W),P0=n(W,"LI",{});var d6e=s(P0);zge=n(d6e,"STRONG",{});var $nt=s(zge);uar=r($nt,"deberta"),$nt.forEach(t),bar=r(d6e," \u2014 "),Gq=n(d6e,"A",{href:!0});var Int=s(Gq);Tar=r(Int,"TFDebertaForSequenceClassification"),Int.forEach(t),Far=r(d6e," (DeBERTa model)"),d6e.forEach(t),Car=i(W),$0=n(W,"LI",{});var c6e=s($0);Vge=n(c6e,"STRONG",{});var jnt=s(Vge);Mar=r(jnt,"deberta-v2"),jnt.forEach(t),Ear=r(c6e," \u2014 "),Oq=n(c6e,"A",{href:!0});var Nnt=s(Oq);yar=r(Nnt,"TFDebertaV2ForSequenceClassification"),Nnt.forEach(t),war=r(c6e," (DeBERTa-v2 model)"),c6e.forEach(t),Aar=i(W),I0=n(W,"LI",{});var f6e=s(I0);Wge=n(f6e,"STRONG",{});var Dnt=s(Wge);Lar=r(Dnt,"distilbert"),Dnt.forEach(t),Bar=r(f6e," \u2014 "),Xq=n(f6e,"A",{href:!0});var qnt=s(Xq);kar=r(qnt,"TFDistilBertForSequenceClassification"),qnt.forEach(t),xar=r(f6e," (DistilBERT model)"),f6e.forEach(t),Rar=i(W),j0=n(W,"LI",{});var m6e=s(j0);Qge=n(m6e,"STRONG",{});var Gnt=s(Qge);Sar=r(Gnt,"electra"),Gnt.forEach(t),Par=r(m6e," \u2014 "),zq=n(m6e,"A",{href:!0});var Ont=s(zq);$ar=r(Ont,"TFElectraForSequenceClassification"),Ont.forEach(t),Iar=r(m6e," (ELECTRA model)"),m6e.forEach(t),jar=i(W),N0=n(W,"LI",{});var g6e=s(N0);Hge=n(g6e,"STRONG",{});var Xnt=s(Hge);Nar=r(Xnt,"flaubert"),Xnt.forEach(t),Dar=r(g6e," \u2014 "),Vq=n(g6e,"A",{href:!0});var znt=s(Vq);qar=r(znt,"TFFlaubertForSequenceClassification"),znt.forEach(t),Gar=r(g6e," (FlauBERT model)"),g6e.forEach(t),Oar=i(W),D0=n(W,"LI",{});var h6e=s(D0);Uge=n(h6e,"STRONG",{});var Vnt=s(Uge);Xar=r(Vnt,"funnel"),Vnt.forEach(t),zar=r(h6e," \u2014 "),Wq=n(h6e,"A",{href:!0});var Wnt=s(Wq);Var=r(Wnt,"TFFunnelForSequenceClassification"),Wnt.forEach(t),War=r(h6e," (Funnel Transformer model)"),h6e.forEach(t),Qar=i(W),q0=n(W,"LI",{});var p6e=s(q0);Jge=n(p6e,"STRONG",{});var Qnt=s(Jge);Har=r(Qnt,"gpt2"),Qnt.forEach(t),Uar=r(p6e," \u2014 "),Qq=n(p6e,"A",{href:!0});var Hnt=s(Qq);Jar=r(Hnt,"TFGPT2ForSequenceClassification"),Hnt.forEach(t),Yar=r(p6e," (OpenAI GPT-2 model)"),p6e.forEach(t),Kar=i(W),G0=n(W,"LI",{});var _6e=s(G0);Yge=n(_6e,"STRONG",{});var Unt=s(Yge);Zar=r(Unt,"layoutlm"),Unt.forEach(t),enr=r(_6e," \u2014 "),Hq=n(_6e,"A",{href:!0});var Jnt=s(Hq);onr=r(Jnt,"TFLayoutLMForSequenceClassification"),Jnt.forEach(t),rnr=r(_6e," (LayoutLM model)"),_6e.forEach(t),tnr=i(W),O0=n(W,"LI",{});var u6e=s(O0);Kge=n(u6e,"STRONG",{});var Ynt=s(Kge);anr=r(Ynt,"longformer"),Ynt.forEach(t),nnr=r(u6e," \u2014 "),Uq=n(u6e,"A",{href:!0});var Knt=s(Uq);snr=r(Knt,"TFLongformerForSequenceClassification"),Knt.forEach(t),lnr=r(u6e," (Longformer model)"),u6e.forEach(t),inr=i(W),X0=n(W,"LI",{});var b6e=s(X0);Zge=n(b6e,"STRONG",{});var Znt=s(Zge);dnr=r(Znt,"mobilebert"),Znt.forEach(t),cnr=r(b6e," \u2014 "),Jq=n(b6e,"A",{href:!0});var est=s(Jq);fnr=r(est,"TFMobileBertForSequenceClassification"),est.forEach(t),mnr=r(b6e," (MobileBERT model)"),b6e.forEach(t),gnr=i(W),z0=n(W,"LI",{});var v6e=s(z0);ehe=n(v6e,"STRONG",{});var ost=s(ehe);hnr=r(ost,"mpnet"),ost.forEach(t),pnr=r(v6e," \u2014 "),Yq=n(v6e,"A",{href:!0});var rst=s(Yq);_nr=r(rst,"TFMPNetForSequenceClassification"),rst.forEach(t),unr=r(v6e," (MPNet model)"),v6e.forEach(t),bnr=i(W),V0=n(W,"LI",{});var T6e=s(V0);ohe=n(T6e,"STRONG",{});var tst=s(ohe);vnr=r(tst,"openai-gpt"),tst.forEach(t),Tnr=r(T6e," \u2014 "),Kq=n(T6e,"A",{href:!0});var ast=s(Kq);Fnr=r(ast,"TFOpenAIGPTForSequenceClassification"),ast.forEach(t),Cnr=r(T6e," (OpenAI GPT model)"),T6e.forEach(t),Mnr=i(W),W0=n(W,"LI",{});var F6e=s(W0);rhe=n(F6e,"STRONG",{});var nst=s(rhe);Enr=r(nst,"rembert"),nst.forEach(t),ynr=r(F6e," \u2014 "),Zq=n(F6e,"A",{href:!0});var sst=s(Zq);wnr=r(sst,"TFRemBertForSequenceClassification"),sst.forEach(t),Anr=r(F6e," (RemBERT model)"),F6e.forEach(t),Lnr=i(W),Q0=n(W,"LI",{});var C6e=s(Q0);the=n(C6e,"STRONG",{});var lst=s(the);Bnr=r(lst,"roberta"),lst.forEach(t),knr=r(C6e," \u2014 "),eG=n(C6e,"A",{href:!0});var ist=s(eG);xnr=r(ist,"TFRobertaForSequenceClassification"),ist.forEach(t),Rnr=r(C6e," (RoBERTa model)"),C6e.forEach(t),Snr=i(W),H0=n(W,"LI",{});var M6e=s(H0);ahe=n(M6e,"STRONG",{});var dst=s(ahe);Pnr=r(dst,"roformer"),dst.forEach(t),$nr=r(M6e," \u2014 "),oG=n(M6e,"A",{href:!0});var cst=s(oG);Inr=r(cst,"TFRoFormerForSequenceClassification"),cst.forEach(t),jnr=r(M6e," (RoFormer model)"),M6e.forEach(t),Nnr=i(W),U0=n(W,"LI",{});var E6e=s(U0);nhe=n(E6e,"STRONG",{});var fst=s(nhe);Dnr=r(fst,"tapas"),fst.forEach(t),qnr=r(E6e," \u2014 "),rG=n(E6e,"A",{href:!0});var mst=s(rG);Gnr=r(mst,"TFTapasForSequenceClassification"),mst.forEach(t),Onr=r(E6e," (TAPAS model)"),E6e.forEach(t),Xnr=i(W),J0=n(W,"LI",{});var y6e=s(J0);she=n(y6e,"STRONG",{});var gst=s(she);znr=r(gst,"transfo-xl"),gst.forEach(t),Vnr=r(y6e," \u2014 "),tG=n(y6e,"A",{href:!0});var hst=s(tG);Wnr=r(hst,"TFTransfoXLForSequenceClassification"),hst.forEach(t),Qnr=r(y6e," (Transformer-XL model)"),y6e.forEach(t),Hnr=i(W),Y0=n(W,"LI",{});var w6e=s(Y0);lhe=n(w6e,"STRONG",{});var pst=s(lhe);Unr=r(pst,"xlm"),pst.forEach(t),Jnr=r(w6e," \u2014 "),aG=n(w6e,"A",{href:!0});var _st=s(aG);Ynr=r(_st,"TFXLMForSequenceClassification"),_st.forEach(t),Knr=r(w6e," (XLM model)"),w6e.forEach(t),Znr=i(W),K0=n(W,"LI",{});var A6e=s(K0);ihe=n(A6e,"STRONG",{});var ust=s(ihe);esr=r(ust,"xlm-roberta"),ust.forEach(t),osr=r(A6e," \u2014 "),nG=n(A6e,"A",{href:!0});var bst=s(nG);rsr=r(bst,"TFXLMRobertaForSequenceClassification"),bst.forEach(t),tsr=r(A6e," (XLM-RoBERTa model)"),A6e.forEach(t),asr=i(W),Z0=n(W,"LI",{});var L6e=s(Z0);dhe=n(L6e,"STRONG",{});var vst=s(dhe);nsr=r(vst,"xlnet"),vst.forEach(t),ssr=r(L6e," \u2014 "),sG=n(L6e,"A",{href:!0});var Tst=s(sG);lsr=r(Tst,"TFXLNetForSequenceClassification"),Tst.forEach(t),isr=r(L6e," (XLNet model)"),L6e.forEach(t),W.forEach(t),dsr=i(_a),che=n(_a,"P",{});var Fst=s(che);csr=r(Fst,"Examples:"),Fst.forEach(t),fsr=i(_a),m(Aw.$$.fragment,_a),_a.forEach(t),Gl.forEach(t),I9e=i(d),Cc=n(d,"H2",{class:!0});var Vke=s(Cc);eT=n(Vke,"A",{id:!0,class:!0,href:!0});var Cst=s(eT);fhe=n(Cst,"SPAN",{});var Mst=s(fhe);m(Lw.$$.fragment,Mst),Mst.forEach(t),Cst.forEach(t),msr=i(Vke),mhe=n(Vke,"SPAN",{});var Est=s(mhe);gsr=r(Est,"TFAutoModelForMultipleChoice"),Est.forEach(t),Vke.forEach(t),j9e=i(d),Tr=n(d,"DIV",{class:!0});var Xl=s(Tr);m(Bw.$$.fragment,Xl),hsr=i(Xl),Mc=n(Xl,"P",{});var lV=s(Mc);psr=r(lV,`This is a generic model class that will be instantiated as one of the model classes of the library (with a multiple choice head) when created
with the `),ghe=n(lV,"CODE",{});var yst=s(ghe);_sr=r(yst,"from_pretrained()"),yst.forEach(t),usr=r(lV,"class method or the "),hhe=n(lV,"CODE",{});var wst=s(hhe);bsr=r(wst,"from_config()"),wst.forEach(t),vsr=r(lV,`class
method.`),lV.forEach(t),Tsr=i(Xl),kw=n(Xl,"P",{});var Wke=s(kw);Fsr=r(Wke,"This class cannot be instantiated directly using "),phe=n(Wke,"CODE",{});var Ast=s(phe);Csr=r(Ast,"__init__()"),Ast.forEach(t),Msr=r(Wke," (throws an error)."),Wke.forEach(t),Esr=i(Xl),gt=n(Xl,"DIV",{class:!0});var zl=s(gt);m(xw.$$.fragment,zl),ysr=i(zl),_he=n(zl,"P",{});var Lst=s(_he);wsr=r(Lst,"Instantiates one of the model classes of the library (with a multiple choice head) from a configuration."),Lst.forEach(t),Asr=i(zl),Ec=n(zl,"P",{});var iV=s(Ec);Lsr=r(iV,`Note:
Loading a model from its configuration file does `),uhe=n(iV,"STRONG",{});var Bst=s(uhe);Bsr=r(Bst,"not"),Bst.forEach(t),ksr=r(iV,` load the model weights. It only affects the
model\u2019s configuration. Use `),bhe=n(iV,"CODE",{});var kst=s(bhe);xsr=r(kst,"from_pretrained()"),kst.forEach(t),Rsr=r(iV,"to load the model weights."),iV.forEach(t),Ssr=i(zl),vhe=n(zl,"P",{});var xst=s(vhe);Psr=r(xst,"Examples:"),xst.forEach(t),$sr=i(zl),m(Rw.$$.fragment,zl),zl.forEach(t),Isr=i(Xl),To=n(Xl,"DIV",{class:!0});var ua=s(To);m(Sw.$$.fragment,ua),jsr=i(ua),The=n(ua,"P",{});var Rst=s(The);Nsr=r(Rst,"Instantiate one of the model classes of the library (with a multiple choice head) from a pretrained model."),Rst.forEach(t),Dsr=i(ua),pn=n(ua,"P",{});var q4=s(pn);qsr=r(q4,"The model class to instantiate is selected based on the "),Fhe=n(q4,"CODE",{});var Sst=s(Fhe);Gsr=r(Sst,"model_type"),Sst.forEach(t),Osr=r(q4,` property of the config object (either
passed as an argument or loaded from `),Che=n(q4,"CODE",{});var Pst=s(Che);Xsr=r(Pst,"pretrained_model_name_or_path"),Pst.forEach(t),zsr=r(q4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Mhe=n(q4,"CODE",{});var $st=s(Mhe);Vsr=r($st,"pretrained_model_name_or_path"),$st.forEach(t),Wsr=r(q4,":"),q4.forEach(t),Qsr=i(ua),te=n(ua,"UL",{});var se=s(te);oT=n(se,"LI",{});var B6e=s(oT);Ehe=n(B6e,"STRONG",{});var Ist=s(Ehe);Hsr=r(Ist,"albert"),Ist.forEach(t),Usr=r(B6e," \u2014 "),lG=n(B6e,"A",{href:!0});var jst=s(lG);Jsr=r(jst,"TFAlbertForMultipleChoice"),jst.forEach(t),Ysr=r(B6e," (ALBERT model)"),B6e.forEach(t),Ksr=i(se),rT=n(se,"LI",{});var k6e=s(rT);yhe=n(k6e,"STRONG",{});var Nst=s(yhe);Zsr=r(Nst,"bert"),Nst.forEach(t),elr=r(k6e," \u2014 "),iG=n(k6e,"A",{href:!0});var Dst=s(iG);olr=r(Dst,"TFBertForMultipleChoice"),Dst.forEach(t),rlr=r(k6e," (BERT model)"),k6e.forEach(t),tlr=i(se),tT=n(se,"LI",{});var x6e=s(tT);whe=n(x6e,"STRONG",{});var qst=s(whe);alr=r(qst,"camembert"),qst.forEach(t),nlr=r(x6e," \u2014 "),dG=n(x6e,"A",{href:!0});var Gst=s(dG);slr=r(Gst,"TFCamembertForMultipleChoice"),Gst.forEach(t),llr=r(x6e," (CamemBERT model)"),x6e.forEach(t),ilr=i(se),aT=n(se,"LI",{});var R6e=s(aT);Ahe=n(R6e,"STRONG",{});var Ost=s(Ahe);dlr=r(Ost,"convbert"),Ost.forEach(t),clr=r(R6e," \u2014 "),cG=n(R6e,"A",{href:!0});var Xst=s(cG);flr=r(Xst,"TFConvBertForMultipleChoice"),Xst.forEach(t),mlr=r(R6e," (ConvBERT model)"),R6e.forEach(t),glr=i(se),nT=n(se,"LI",{});var S6e=s(nT);Lhe=n(S6e,"STRONG",{});var zst=s(Lhe);hlr=r(zst,"distilbert"),zst.forEach(t),plr=r(S6e," \u2014 "),fG=n(S6e,"A",{href:!0});var Vst=s(fG);_lr=r(Vst,"TFDistilBertForMultipleChoice"),Vst.forEach(t),ulr=r(S6e," (DistilBERT model)"),S6e.forEach(t),blr=i(se),sT=n(se,"LI",{});var P6e=s(sT);Bhe=n(P6e,"STRONG",{});var Wst=s(Bhe);vlr=r(Wst,"electra"),Wst.forEach(t),Tlr=r(P6e," \u2014 "),mG=n(P6e,"A",{href:!0});var Qst=s(mG);Flr=r(Qst,"TFElectraForMultipleChoice"),Qst.forEach(t),Clr=r(P6e," (ELECTRA model)"),P6e.forEach(t),Mlr=i(se),lT=n(se,"LI",{});var $6e=s(lT);khe=n($6e,"STRONG",{});var Hst=s(khe);Elr=r(Hst,"flaubert"),Hst.forEach(t),ylr=r($6e," \u2014 "),gG=n($6e,"A",{href:!0});var Ust=s(gG);wlr=r(Ust,"TFFlaubertForMultipleChoice"),Ust.forEach(t),Alr=r($6e," (FlauBERT model)"),$6e.forEach(t),Llr=i(se),iT=n(se,"LI",{});var I6e=s(iT);xhe=n(I6e,"STRONG",{});var Jst=s(xhe);Blr=r(Jst,"funnel"),Jst.forEach(t),klr=r(I6e," \u2014 "),hG=n(I6e,"A",{href:!0});var Yst=s(hG);xlr=r(Yst,"TFFunnelForMultipleChoice"),Yst.forEach(t),Rlr=r(I6e," (Funnel Transformer model)"),I6e.forEach(t),Slr=i(se),dT=n(se,"LI",{});var j6e=s(dT);Rhe=n(j6e,"STRONG",{});var Kst=s(Rhe);Plr=r(Kst,"longformer"),Kst.forEach(t),$lr=r(j6e," \u2014 "),pG=n(j6e,"A",{href:!0});var Zst=s(pG);Ilr=r(Zst,"TFLongformerForMultipleChoice"),Zst.forEach(t),jlr=r(j6e," (Longformer model)"),j6e.forEach(t),Nlr=i(se),cT=n(se,"LI",{});var N6e=s(cT);She=n(N6e,"STRONG",{});var elt=s(She);Dlr=r(elt,"mobilebert"),elt.forEach(t),qlr=r(N6e," \u2014 "),_G=n(N6e,"A",{href:!0});var olt=s(_G);Glr=r(olt,"TFMobileBertForMultipleChoice"),olt.forEach(t),Olr=r(N6e," (MobileBERT model)"),N6e.forEach(t),Xlr=i(se),fT=n(se,"LI",{});var D6e=s(fT);Phe=n(D6e,"STRONG",{});var rlt=s(Phe);zlr=r(rlt,"mpnet"),rlt.forEach(t),Vlr=r(D6e," \u2014 "),uG=n(D6e,"A",{href:!0});var tlt=s(uG);Wlr=r(tlt,"TFMPNetForMultipleChoice"),tlt.forEach(t),Qlr=r(D6e," (MPNet model)"),D6e.forEach(t),Hlr=i(se),mT=n(se,"LI",{});var q6e=s(mT);$he=n(q6e,"STRONG",{});var alt=s($he);Ulr=r(alt,"rembert"),alt.forEach(t),Jlr=r(q6e," \u2014 "),bG=n(q6e,"A",{href:!0});var nlt=s(bG);Ylr=r(nlt,"TFRemBertForMultipleChoice"),nlt.forEach(t),Klr=r(q6e," (RemBERT model)"),q6e.forEach(t),Zlr=i(se),gT=n(se,"LI",{});var G6e=s(gT);Ihe=n(G6e,"STRONG",{});var slt=s(Ihe);eir=r(slt,"roberta"),slt.forEach(t),oir=r(G6e," \u2014 "),vG=n(G6e,"A",{href:!0});var llt=s(vG);rir=r(llt,"TFRobertaForMultipleChoice"),llt.forEach(t),tir=r(G6e," (RoBERTa model)"),G6e.forEach(t),air=i(se),hT=n(se,"LI",{});var O6e=s(hT);jhe=n(O6e,"STRONG",{});var ilt=s(jhe);nir=r(ilt,"roformer"),ilt.forEach(t),sir=r(O6e," \u2014 "),TG=n(O6e,"A",{href:!0});var dlt=s(TG);lir=r(dlt,"TFRoFormerForMultipleChoice"),dlt.forEach(t),iir=r(O6e," (RoFormer model)"),O6e.forEach(t),dir=i(se),pT=n(se,"LI",{});var X6e=s(pT);Nhe=n(X6e,"STRONG",{});var clt=s(Nhe);cir=r(clt,"xlm"),clt.forEach(t),fir=r(X6e," \u2014 "),FG=n(X6e,"A",{href:!0});var flt=s(FG);mir=r(flt,"TFXLMForMultipleChoice"),flt.forEach(t),gir=r(X6e," (XLM model)"),X6e.forEach(t),hir=i(se),_T=n(se,"LI",{});var z6e=s(_T);Dhe=n(z6e,"STRONG",{});var mlt=s(Dhe);pir=r(mlt,"xlm-roberta"),mlt.forEach(t),_ir=r(z6e," \u2014 "),CG=n(z6e,"A",{href:!0});var glt=s(CG);uir=r(glt,"TFXLMRobertaForMultipleChoice"),glt.forEach(t),bir=r(z6e," (XLM-RoBERTa model)"),z6e.forEach(t),vir=i(se),uT=n(se,"LI",{});var V6e=s(uT);qhe=n(V6e,"STRONG",{});var hlt=s(qhe);Tir=r(hlt,"xlnet"),hlt.forEach(t),Fir=r(V6e," \u2014 "),MG=n(V6e,"A",{href:!0});var plt=s(MG);Cir=r(plt,"TFXLNetForMultipleChoice"),plt.forEach(t),Mir=r(V6e," (XLNet model)"),V6e.forEach(t),se.forEach(t),Eir=i(ua),Ghe=n(ua,"P",{});var _lt=s(Ghe);yir=r(_lt,"Examples:"),_lt.forEach(t),wir=i(ua),m(Pw.$$.fragment,ua),ua.forEach(t),Xl.forEach(t),N9e=i(d),yc=n(d,"H2",{class:!0});var Qke=s(yc);bT=n(Qke,"A",{id:!0,class:!0,href:!0});var ult=s(bT);Ohe=n(ult,"SPAN",{});var blt=s(Ohe);m($w.$$.fragment,blt),blt.forEach(t),ult.forEach(t),Air=i(Qke),Xhe=n(Qke,"SPAN",{});var vlt=s(Xhe);Lir=r(vlt,"TFAutoModelForTableQuestionAnswering"),vlt.forEach(t),Qke.forEach(t),D9e=i(d),Fr=n(d,"DIV",{class:!0});var Vl=s(Fr);m(Iw.$$.fragment,Vl),Bir=i(Vl),wc=n(Vl,"P",{});var dV=s(wc);kir=r(dV,`This is a generic model class that will be instantiated as one of the model classes of the library (with a table question answering head) when created
with the `),zhe=n(dV,"CODE",{});var Tlt=s(zhe);xir=r(Tlt,"from_pretrained()"),Tlt.forEach(t),Rir=r(dV,"class method or the "),Vhe=n(dV,"CODE",{});var Flt=s(Vhe);Sir=r(Flt,"from_config()"),Flt.forEach(t),Pir=r(dV,`class
method.`),dV.forEach(t),$ir=i(Vl),jw=n(Vl,"P",{});var Hke=s(jw);Iir=r(Hke,"This class cannot be instantiated directly using "),Whe=n(Hke,"CODE",{});var Clt=s(Whe);jir=r(Clt,"__init__()"),Clt.forEach(t),Nir=r(Hke," (throws an error)."),Hke.forEach(t),Dir=i(Vl),ht=n(Vl,"DIV",{class:!0});var Wl=s(ht);m(Nw.$$.fragment,Wl),qir=i(Wl),Qhe=n(Wl,"P",{});var Mlt=s(Qhe);Gir=r(Mlt,"Instantiates one of the model classes of the library (with a table question answering head) from a configuration."),Mlt.forEach(t),Oir=i(Wl),Ac=n(Wl,"P",{});var cV=s(Ac);Xir=r(cV,`Note:
Loading a model from its configuration file does `),Hhe=n(cV,"STRONG",{});var Elt=s(Hhe);zir=r(Elt,"not"),Elt.forEach(t),Vir=r(cV,` load the model weights. It only affects the
model\u2019s configuration. Use `),Uhe=n(cV,"CODE",{});var ylt=s(Uhe);Wir=r(ylt,"from_pretrained()"),ylt.forEach(t),Qir=r(cV,"to load the model weights."),cV.forEach(t),Hir=i(Wl),Jhe=n(Wl,"P",{});var wlt=s(Jhe);Uir=r(wlt,"Examples:"),wlt.forEach(t),Jir=i(Wl),m(Dw.$$.fragment,Wl),Wl.forEach(t),Yir=i(Vl),Fo=n(Vl,"DIV",{class:!0});var ba=s(Fo);m(qw.$$.fragment,ba),Kir=i(ba),Yhe=n(ba,"P",{});var Alt=s(Yhe);Zir=r(Alt,"Instantiate one of the model classes of the library (with a table question answering head) from a pretrained model."),Alt.forEach(t),edr=i(ba),_n=n(ba,"P",{});var G4=s(_n);odr=r(G4,"The model class to instantiate is selected based on the "),Khe=n(G4,"CODE",{});var Llt=s(Khe);rdr=r(Llt,"model_type"),Llt.forEach(t),tdr=r(G4,` property of the config object (either
passed as an argument or loaded from `),Zhe=n(G4,"CODE",{});var Blt=s(Zhe);adr=r(Blt,"pretrained_model_name_or_path"),Blt.forEach(t),ndr=r(G4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),epe=n(G4,"CODE",{});var klt=s(epe);sdr=r(klt,"pretrained_model_name_or_path"),klt.forEach(t),ldr=r(G4,":"),G4.forEach(t),idr=i(ba),ope=n(ba,"UL",{});var xlt=s(ope);vT=n(xlt,"LI",{});var W6e=s(vT);rpe=n(W6e,"STRONG",{});var Rlt=s(rpe);ddr=r(Rlt,"tapas"),Rlt.forEach(t),cdr=r(W6e," \u2014 "),EG=n(W6e,"A",{href:!0});var Slt=s(EG);fdr=r(Slt,"TFTapasForQuestionAnswering"),Slt.forEach(t),mdr=r(W6e," (TAPAS model)"),W6e.forEach(t),xlt.forEach(t),gdr=i(ba),tpe=n(ba,"P",{});var Plt=s(tpe);hdr=r(Plt,"Examples:"),Plt.forEach(t),pdr=i(ba),m(Gw.$$.fragment,ba),ba.forEach(t),Vl.forEach(t),q9e=i(d),Lc=n(d,"H2",{class:!0});var Uke=s(Lc);TT=n(Uke,"A",{id:!0,class:!0,href:!0});var $lt=s(TT);ape=n($lt,"SPAN",{});var Ilt=s(ape);m(Ow.$$.fragment,Ilt),Ilt.forEach(t),$lt.forEach(t),_dr=i(Uke),npe=n(Uke,"SPAN",{});var jlt=s(npe);udr=r(jlt,"TFAutoModelForTokenClassification"),jlt.forEach(t),Uke.forEach(t),G9e=i(d),Cr=n(d,"DIV",{class:!0});var Ql=s(Cr);m(Xw.$$.fragment,Ql),bdr=i(Ql),Bc=n(Ql,"P",{});var fV=s(Bc);vdr=r(fV,`This is a generic model class that will be instantiated as one of the model classes of the library (with a token classification head) when created
with the `),spe=n(fV,"CODE",{});var Nlt=s(spe);Tdr=r(Nlt,"from_pretrained()"),Nlt.forEach(t),Fdr=r(fV,"class method or the "),lpe=n(fV,"CODE",{});var Dlt=s(lpe);Cdr=r(Dlt,"from_config()"),Dlt.forEach(t),Mdr=r(fV,`class
method.`),fV.forEach(t),Edr=i(Ql),zw=n(Ql,"P",{});var Jke=s(zw);ydr=r(Jke,"This class cannot be instantiated directly using "),ipe=n(Jke,"CODE",{});var qlt=s(ipe);wdr=r(qlt,"__init__()"),qlt.forEach(t),Adr=r(Jke," (throws an error)."),Jke.forEach(t),Ldr=i(Ql),pt=n(Ql,"DIV",{class:!0});var Hl=s(pt);m(Vw.$$.fragment,Hl),Bdr=i(Hl),dpe=n(Hl,"P",{});var Glt=s(dpe);kdr=r(Glt,"Instantiates one of the model classes of the library (with a token classification head) from a configuration."),Glt.forEach(t),xdr=i(Hl),kc=n(Hl,"P",{});var mV=s(kc);Rdr=r(mV,`Note:
Loading a model from its configuration file does `),cpe=n(mV,"STRONG",{});var Olt=s(cpe);Sdr=r(Olt,"not"),Olt.forEach(t),Pdr=r(mV,` load the model weights. It only affects the
model\u2019s configuration. Use `),fpe=n(mV,"CODE",{});var Xlt=s(fpe);$dr=r(Xlt,"from_pretrained()"),Xlt.forEach(t),Idr=r(mV,"to load the model weights."),mV.forEach(t),jdr=i(Hl),mpe=n(Hl,"P",{});var zlt=s(mpe);Ndr=r(zlt,"Examples:"),zlt.forEach(t),Ddr=i(Hl),m(Ww.$$.fragment,Hl),Hl.forEach(t),qdr=i(Ql),Co=n(Ql,"DIV",{class:!0});var va=s(Co);m(Qw.$$.fragment,va),Gdr=i(va),gpe=n(va,"P",{});var Vlt=s(gpe);Odr=r(Vlt,"Instantiate one of the model classes of the library (with a token classification head) from a pretrained model."),Vlt.forEach(t),Xdr=i(va),un=n(va,"P",{});var O4=s(un);zdr=r(O4,"The model class to instantiate is selected based on the "),hpe=n(O4,"CODE",{});var Wlt=s(hpe);Vdr=r(Wlt,"model_type"),Wlt.forEach(t),Wdr=r(O4,` property of the config object (either
passed as an argument or loaded from `),ppe=n(O4,"CODE",{});var Qlt=s(ppe);Qdr=r(Qlt,"pretrained_model_name_or_path"),Qlt.forEach(t),Hdr=r(O4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),_pe=n(O4,"CODE",{});var Hlt=s(_pe);Udr=r(Hlt,"pretrained_model_name_or_path"),Hlt.forEach(t),Jdr=r(O4,":"),O4.forEach(t),Ydr=i(va),K=n(va,"UL",{});var oe=s(K);FT=n(oe,"LI",{});var Q6e=s(FT);upe=n(Q6e,"STRONG",{});var Ult=s(upe);Kdr=r(Ult,"albert"),Ult.forEach(t),Zdr=r(Q6e," \u2014 "),yG=n(Q6e,"A",{href:!0});var Jlt=s(yG);ecr=r(Jlt,"TFAlbertForTokenClassification"),Jlt.forEach(t),ocr=r(Q6e," (ALBERT model)"),Q6e.forEach(t),rcr=i(oe),CT=n(oe,"LI",{});var H6e=s(CT);bpe=n(H6e,"STRONG",{});var Ylt=s(bpe);tcr=r(Ylt,"bert"),Ylt.forEach(t),acr=r(H6e," \u2014 "),wG=n(H6e,"A",{href:!0});var Klt=s(wG);ncr=r(Klt,"TFBertForTokenClassification"),Klt.forEach(t),scr=r(H6e," (BERT model)"),H6e.forEach(t),lcr=i(oe),MT=n(oe,"LI",{});var U6e=s(MT);vpe=n(U6e,"STRONG",{});var Zlt=s(vpe);icr=r(Zlt,"camembert"),Zlt.forEach(t),dcr=r(U6e," \u2014 "),AG=n(U6e,"A",{href:!0});var eit=s(AG);ccr=r(eit,"TFCamembertForTokenClassification"),eit.forEach(t),fcr=r(U6e," (CamemBERT model)"),U6e.forEach(t),mcr=i(oe),ET=n(oe,"LI",{});var J6e=s(ET);Tpe=n(J6e,"STRONG",{});var oit=s(Tpe);gcr=r(oit,"convbert"),oit.forEach(t),hcr=r(J6e," \u2014 "),LG=n(J6e,"A",{href:!0});var rit=s(LG);pcr=r(rit,"TFConvBertForTokenClassification"),rit.forEach(t),_cr=r(J6e," (ConvBERT model)"),J6e.forEach(t),ucr=i(oe),yT=n(oe,"LI",{});var Y6e=s(yT);Fpe=n(Y6e,"STRONG",{});var tit=s(Fpe);bcr=r(tit,"deberta"),tit.forEach(t),vcr=r(Y6e," \u2014 "),BG=n(Y6e,"A",{href:!0});var ait=s(BG);Tcr=r(ait,"TFDebertaForTokenClassification"),ait.forEach(t),Fcr=r(Y6e," (DeBERTa model)"),Y6e.forEach(t),Ccr=i(oe),wT=n(oe,"LI",{});var K6e=s(wT);Cpe=n(K6e,"STRONG",{});var nit=s(Cpe);Mcr=r(nit,"deberta-v2"),nit.forEach(t),Ecr=r(K6e," \u2014 "),kG=n(K6e,"A",{href:!0});var sit=s(kG);ycr=r(sit,"TFDebertaV2ForTokenClassification"),sit.forEach(t),wcr=r(K6e," (DeBERTa-v2 model)"),K6e.forEach(t),Acr=i(oe),AT=n(oe,"LI",{});var Z6e=s(AT);Mpe=n(Z6e,"STRONG",{});var lit=s(Mpe);Lcr=r(lit,"distilbert"),lit.forEach(t),Bcr=r(Z6e," \u2014 "),xG=n(Z6e,"A",{href:!0});var iit=s(xG);kcr=r(iit,"TFDistilBertForTokenClassification"),iit.forEach(t),xcr=r(Z6e," (DistilBERT model)"),Z6e.forEach(t),Rcr=i(oe),LT=n(oe,"LI",{});var eAe=s(LT);Epe=n(eAe,"STRONG",{});var dit=s(Epe);Scr=r(dit,"electra"),dit.forEach(t),Pcr=r(eAe," \u2014 "),RG=n(eAe,"A",{href:!0});var cit=s(RG);$cr=r(cit,"TFElectraForTokenClassification"),cit.forEach(t),Icr=r(eAe," (ELECTRA model)"),eAe.forEach(t),jcr=i(oe),BT=n(oe,"LI",{});var oAe=s(BT);ype=n(oAe,"STRONG",{});var fit=s(ype);Ncr=r(fit,"flaubert"),fit.forEach(t),Dcr=r(oAe," \u2014 "),SG=n(oAe,"A",{href:!0});var mit=s(SG);qcr=r(mit,"TFFlaubertForTokenClassification"),mit.forEach(t),Gcr=r(oAe," (FlauBERT model)"),oAe.forEach(t),Ocr=i(oe),kT=n(oe,"LI",{});var rAe=s(kT);wpe=n(rAe,"STRONG",{});var git=s(wpe);Xcr=r(git,"funnel"),git.forEach(t),zcr=r(rAe," \u2014 "),PG=n(rAe,"A",{href:!0});var hit=s(PG);Vcr=r(hit,"TFFunnelForTokenClassification"),hit.forEach(t),Wcr=r(rAe," (Funnel Transformer model)"),rAe.forEach(t),Qcr=i(oe),xT=n(oe,"LI",{});var tAe=s(xT);Ape=n(tAe,"STRONG",{});var pit=s(Ape);Hcr=r(pit,"layoutlm"),pit.forEach(t),Ucr=r(tAe," \u2014 "),$G=n(tAe,"A",{href:!0});var _it=s($G);Jcr=r(_it,"TFLayoutLMForTokenClassification"),_it.forEach(t),Ycr=r(tAe," (LayoutLM model)"),tAe.forEach(t),Kcr=i(oe),RT=n(oe,"LI",{});var aAe=s(RT);Lpe=n(aAe,"STRONG",{});var uit=s(Lpe);Zcr=r(uit,"longformer"),uit.forEach(t),efr=r(aAe," \u2014 "),IG=n(aAe,"A",{href:!0});var bit=s(IG);ofr=r(bit,"TFLongformerForTokenClassification"),bit.forEach(t),rfr=r(aAe," (Longformer model)"),aAe.forEach(t),tfr=i(oe),ST=n(oe,"LI",{});var nAe=s(ST);Bpe=n(nAe,"STRONG",{});var vit=s(Bpe);afr=r(vit,"mobilebert"),vit.forEach(t),nfr=r(nAe," \u2014 "),jG=n(nAe,"A",{href:!0});var Tit=s(jG);sfr=r(Tit,"TFMobileBertForTokenClassification"),Tit.forEach(t),lfr=r(nAe," (MobileBERT model)"),nAe.forEach(t),ifr=i(oe),PT=n(oe,"LI",{});var sAe=s(PT);kpe=n(sAe,"STRONG",{});var Fit=s(kpe);dfr=r(Fit,"mpnet"),Fit.forEach(t),cfr=r(sAe," \u2014 "),NG=n(sAe,"A",{href:!0});var Cit=s(NG);ffr=r(Cit,"TFMPNetForTokenClassification"),Cit.forEach(t),mfr=r(sAe," (MPNet model)"),sAe.forEach(t),gfr=i(oe),$T=n(oe,"LI",{});var lAe=s($T);xpe=n(lAe,"STRONG",{});var Mit=s(xpe);hfr=r(Mit,"rembert"),Mit.forEach(t),pfr=r(lAe," \u2014 "),DG=n(lAe,"A",{href:!0});var Eit=s(DG);_fr=r(Eit,"TFRemBertForTokenClassification"),Eit.forEach(t),ufr=r(lAe," (RemBERT model)"),lAe.forEach(t),bfr=i(oe),IT=n(oe,"LI",{});var iAe=s(IT);Rpe=n(iAe,"STRONG",{});var yit=s(Rpe);vfr=r(yit,"roberta"),yit.forEach(t),Tfr=r(iAe," \u2014 "),qG=n(iAe,"A",{href:!0});var wit=s(qG);Ffr=r(wit,"TFRobertaForTokenClassification"),wit.forEach(t),Cfr=r(iAe," (RoBERTa model)"),iAe.forEach(t),Mfr=i(oe),jT=n(oe,"LI",{});var dAe=s(jT);Spe=n(dAe,"STRONG",{});var Ait=s(Spe);Efr=r(Ait,"roformer"),Ait.forEach(t),yfr=r(dAe," \u2014 "),GG=n(dAe,"A",{href:!0});var Lit=s(GG);wfr=r(Lit,"TFRoFormerForTokenClassification"),Lit.forEach(t),Afr=r(dAe," (RoFormer model)"),dAe.forEach(t),Lfr=i(oe),NT=n(oe,"LI",{});var cAe=s(NT);Ppe=n(cAe,"STRONG",{});var Bit=s(Ppe);Bfr=r(Bit,"xlm"),Bit.forEach(t),kfr=r(cAe," \u2014 "),OG=n(cAe,"A",{href:!0});var kit=s(OG);xfr=r(kit,"TFXLMForTokenClassification"),kit.forEach(t),Rfr=r(cAe," (XLM model)"),cAe.forEach(t),Sfr=i(oe),DT=n(oe,"LI",{});var fAe=s(DT);$pe=n(fAe,"STRONG",{});var xit=s($pe);Pfr=r(xit,"xlm-roberta"),xit.forEach(t),$fr=r(fAe," \u2014 "),XG=n(fAe,"A",{href:!0});var Rit=s(XG);Ifr=r(Rit,"TFXLMRobertaForTokenClassification"),Rit.forEach(t),jfr=r(fAe," (XLM-RoBERTa model)"),fAe.forEach(t),Nfr=i(oe),qT=n(oe,"LI",{});var mAe=s(qT);Ipe=n(mAe,"STRONG",{});var Sit=s(Ipe);Dfr=r(Sit,"xlnet"),Sit.forEach(t),qfr=r(mAe," \u2014 "),zG=n(mAe,"A",{href:!0});var Pit=s(zG);Gfr=r(Pit,"TFXLNetForTokenClassification"),Pit.forEach(t),Ofr=r(mAe," (XLNet model)"),mAe.forEach(t),oe.forEach(t),Xfr=i(va),jpe=n(va,"P",{});var $it=s(jpe);zfr=r($it,"Examples:"),$it.forEach(t),Vfr=i(va),m(Hw.$$.fragment,va),va.forEach(t),Ql.forEach(t),O9e=i(d),xc=n(d,"H2",{class:!0});var Yke=s(xc);GT=n(Yke,"A",{id:!0,class:!0,href:!0});var Iit=s(GT);Npe=n(Iit,"SPAN",{});var jit=s(Npe);m(Uw.$$.fragment,jit),jit.forEach(t),Iit.forEach(t),Wfr=i(Yke),Dpe=n(Yke,"SPAN",{});var Nit=s(Dpe);Qfr=r(Nit,"TFAutoModelForQuestionAnswering"),Nit.forEach(t),Yke.forEach(t),X9e=i(d),Mr=n(d,"DIV",{class:!0});var Ul=s(Mr);m(Jw.$$.fragment,Ul),Hfr=i(Ul),Rc=n(Ul,"P",{});var gV=s(Rc);Ufr=r(gV,`This is a generic model class that will be instantiated as one of the model classes of the library (with a question answering head) when created
with the `),qpe=n(gV,"CODE",{});var Dit=s(qpe);Jfr=r(Dit,"from_pretrained()"),Dit.forEach(t),Yfr=r(gV,"class method or the "),Gpe=n(gV,"CODE",{});var qit=s(Gpe);Kfr=r(qit,"from_config()"),qit.forEach(t),Zfr=r(gV,`class
method.`),gV.forEach(t),emr=i(Ul),Yw=n(Ul,"P",{});var Kke=s(Yw);omr=r(Kke,"This class cannot be instantiated directly using "),Ope=n(Kke,"CODE",{});var Git=s(Ope);rmr=r(Git,"__init__()"),Git.forEach(t),tmr=r(Kke," (throws an error)."),Kke.forEach(t),amr=i(Ul),_t=n(Ul,"DIV",{class:!0});var Jl=s(_t);m(Kw.$$.fragment,Jl),nmr=i(Jl),Xpe=n(Jl,"P",{});var Oit=s(Xpe);smr=r(Oit,"Instantiates one of the model classes of the library (with a question answering head) from a configuration."),Oit.forEach(t),lmr=i(Jl),Sc=n(Jl,"P",{});var hV=s(Sc);imr=r(hV,`Note:
Loading a model from its configuration file does `),zpe=n(hV,"STRONG",{});var Xit=s(zpe);dmr=r(Xit,"not"),Xit.forEach(t),cmr=r(hV,` load the model weights. It only affects the
model\u2019s configuration. Use `),Vpe=n(hV,"CODE",{});var zit=s(Vpe);fmr=r(zit,"from_pretrained()"),zit.forEach(t),mmr=r(hV,"to load the model weights."),hV.forEach(t),gmr=i(Jl),Wpe=n(Jl,"P",{});var Vit=s(Wpe);hmr=r(Vit,"Examples:"),Vit.forEach(t),pmr=i(Jl),m(Zw.$$.fragment,Jl),Jl.forEach(t),_mr=i(Ul),Mo=n(Ul,"DIV",{class:!0});var Ta=s(Mo);m(e6.$$.fragment,Ta),umr=i(Ta),Qpe=n(Ta,"P",{});var Wit=s(Qpe);bmr=r(Wit,"Instantiate one of the model classes of the library (with a question answering head) from a pretrained model."),Wit.forEach(t),vmr=i(Ta),bn=n(Ta,"P",{});var X4=s(bn);Tmr=r(X4,"The model class to instantiate is selected based on the "),Hpe=n(X4,"CODE",{});var Qit=s(Hpe);Fmr=r(Qit,"model_type"),Qit.forEach(t),Cmr=r(X4,` property of the config object (either
passed as an argument or loaded from `),Upe=n(X4,"CODE",{});var Hit=s(Upe);Mmr=r(Hit,"pretrained_model_name_or_path"),Hit.forEach(t),Emr=r(X4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Jpe=n(X4,"CODE",{});var Uit=s(Jpe);ymr=r(Uit,"pretrained_model_name_or_path"),Uit.forEach(t),wmr=r(X4,":"),X4.forEach(t),Amr=i(Ta),Z=n(Ta,"UL",{});var re=s(Z);OT=n(re,"LI",{});var gAe=s(OT);Ype=n(gAe,"STRONG",{});var Jit=s(Ype);Lmr=r(Jit,"albert"),Jit.forEach(t),Bmr=r(gAe," \u2014 "),VG=n(gAe,"A",{href:!0});var Yit=s(VG);kmr=r(Yit,"TFAlbertForQuestionAnswering"),Yit.forEach(t),xmr=r(gAe," (ALBERT model)"),gAe.forEach(t),Rmr=i(re),XT=n(re,"LI",{});var hAe=s(XT);Kpe=n(hAe,"STRONG",{});var Kit=s(Kpe);Smr=r(Kit,"bert"),Kit.forEach(t),Pmr=r(hAe," \u2014 "),WG=n(hAe,"A",{href:!0});var Zit=s(WG);$mr=r(Zit,"TFBertForQuestionAnswering"),Zit.forEach(t),Imr=r(hAe," (BERT model)"),hAe.forEach(t),jmr=i(re),zT=n(re,"LI",{});var pAe=s(zT);Zpe=n(pAe,"STRONG",{});var edt=s(Zpe);Nmr=r(edt,"camembert"),edt.forEach(t),Dmr=r(pAe," \u2014 "),QG=n(pAe,"A",{href:!0});var odt=s(QG);qmr=r(odt,"TFCamembertForQuestionAnswering"),odt.forEach(t),Gmr=r(pAe," (CamemBERT model)"),pAe.forEach(t),Omr=i(re),VT=n(re,"LI",{});var _Ae=s(VT);e_e=n(_Ae,"STRONG",{});var rdt=s(e_e);Xmr=r(rdt,"convbert"),rdt.forEach(t),zmr=r(_Ae," \u2014 "),HG=n(_Ae,"A",{href:!0});var tdt=s(HG);Vmr=r(tdt,"TFConvBertForQuestionAnswering"),tdt.forEach(t),Wmr=r(_Ae," (ConvBERT model)"),_Ae.forEach(t),Qmr=i(re),WT=n(re,"LI",{});var uAe=s(WT);o_e=n(uAe,"STRONG",{});var adt=s(o_e);Hmr=r(adt,"deberta"),adt.forEach(t),Umr=r(uAe," \u2014 "),UG=n(uAe,"A",{href:!0});var ndt=s(UG);Jmr=r(ndt,"TFDebertaForQuestionAnswering"),ndt.forEach(t),Ymr=r(uAe," (DeBERTa model)"),uAe.forEach(t),Kmr=i(re),QT=n(re,"LI",{});var bAe=s(QT);r_e=n(bAe,"STRONG",{});var sdt=s(r_e);Zmr=r(sdt,"deberta-v2"),sdt.forEach(t),egr=r(bAe," \u2014 "),JG=n(bAe,"A",{href:!0});var ldt=s(JG);ogr=r(ldt,"TFDebertaV2ForQuestionAnswering"),ldt.forEach(t),rgr=r(bAe," (DeBERTa-v2 model)"),bAe.forEach(t),tgr=i(re),HT=n(re,"LI",{});var vAe=s(HT);t_e=n(vAe,"STRONG",{});var idt=s(t_e);agr=r(idt,"distilbert"),idt.forEach(t),ngr=r(vAe," \u2014 "),YG=n(vAe,"A",{href:!0});var ddt=s(YG);sgr=r(ddt,"TFDistilBertForQuestionAnswering"),ddt.forEach(t),lgr=r(vAe," (DistilBERT model)"),vAe.forEach(t),igr=i(re),UT=n(re,"LI",{});var TAe=s(UT);a_e=n(TAe,"STRONG",{});var cdt=s(a_e);dgr=r(cdt,"electra"),cdt.forEach(t),cgr=r(TAe," \u2014 "),KG=n(TAe,"A",{href:!0});var fdt=s(KG);fgr=r(fdt,"TFElectraForQuestionAnswering"),fdt.forEach(t),mgr=r(TAe," (ELECTRA model)"),TAe.forEach(t),ggr=i(re),JT=n(re,"LI",{});var FAe=s(JT);n_e=n(FAe,"STRONG",{});var mdt=s(n_e);hgr=r(mdt,"flaubert"),mdt.forEach(t),pgr=r(FAe," \u2014 "),ZG=n(FAe,"A",{href:!0});var gdt=s(ZG);_gr=r(gdt,"TFFlaubertForQuestionAnsweringSimple"),gdt.forEach(t),ugr=r(FAe," (FlauBERT model)"),FAe.forEach(t),bgr=i(re),YT=n(re,"LI",{});var CAe=s(YT);s_e=n(CAe,"STRONG",{});var hdt=s(s_e);vgr=r(hdt,"funnel"),hdt.forEach(t),Tgr=r(CAe," \u2014 "),eO=n(CAe,"A",{href:!0});var pdt=s(eO);Fgr=r(pdt,"TFFunnelForQuestionAnswering"),pdt.forEach(t),Cgr=r(CAe," (Funnel Transformer model)"),CAe.forEach(t),Mgr=i(re),KT=n(re,"LI",{});var MAe=s(KT);l_e=n(MAe,"STRONG",{});var _dt=s(l_e);Egr=r(_dt,"longformer"),_dt.forEach(t),ygr=r(MAe," \u2014 "),oO=n(MAe,"A",{href:!0});var udt=s(oO);wgr=r(udt,"TFLongformerForQuestionAnswering"),udt.forEach(t),Agr=r(MAe," (Longformer model)"),MAe.forEach(t),Lgr=i(re),ZT=n(re,"LI",{});var EAe=s(ZT);i_e=n(EAe,"STRONG",{});var bdt=s(i_e);Bgr=r(bdt,"mobilebert"),bdt.forEach(t),kgr=r(EAe," \u2014 "),rO=n(EAe,"A",{href:!0});var vdt=s(rO);xgr=r(vdt,"TFMobileBertForQuestionAnswering"),vdt.forEach(t),Rgr=r(EAe," (MobileBERT model)"),EAe.forEach(t),Sgr=i(re),eF=n(re,"LI",{});var yAe=s(eF);d_e=n(yAe,"STRONG",{});var Tdt=s(d_e);Pgr=r(Tdt,"mpnet"),Tdt.forEach(t),$gr=r(yAe," \u2014 "),tO=n(yAe,"A",{href:!0});var Fdt=s(tO);Igr=r(Fdt,"TFMPNetForQuestionAnswering"),Fdt.forEach(t),jgr=r(yAe," (MPNet model)"),yAe.forEach(t),Ngr=i(re),oF=n(re,"LI",{});var wAe=s(oF);c_e=n(wAe,"STRONG",{});var Cdt=s(c_e);Dgr=r(Cdt,"rembert"),Cdt.forEach(t),qgr=r(wAe," \u2014 "),aO=n(wAe,"A",{href:!0});var Mdt=s(aO);Ggr=r(Mdt,"TFRemBertForQuestionAnswering"),Mdt.forEach(t),Ogr=r(wAe," (RemBERT model)"),wAe.forEach(t),Xgr=i(re),rF=n(re,"LI",{});var AAe=s(rF);f_e=n(AAe,"STRONG",{});var Edt=s(f_e);zgr=r(Edt,"roberta"),Edt.forEach(t),Vgr=r(AAe," \u2014 "),nO=n(AAe,"A",{href:!0});var ydt=s(nO);Wgr=r(ydt,"TFRobertaForQuestionAnswering"),ydt.forEach(t),Qgr=r(AAe," (RoBERTa model)"),AAe.forEach(t),Hgr=i(re),tF=n(re,"LI",{});var LAe=s(tF);m_e=n(LAe,"STRONG",{});var wdt=s(m_e);Ugr=r(wdt,"roformer"),wdt.forEach(t),Jgr=r(LAe," \u2014 "),sO=n(LAe,"A",{href:!0});var Adt=s(sO);Ygr=r(Adt,"TFRoFormerForQuestionAnswering"),Adt.forEach(t),Kgr=r(LAe," (RoFormer model)"),LAe.forEach(t),Zgr=i(re),aF=n(re,"LI",{});var BAe=s(aF);g_e=n(BAe,"STRONG",{});var Ldt=s(g_e);ehr=r(Ldt,"xlm"),Ldt.forEach(t),ohr=r(BAe," \u2014 "),lO=n(BAe,"A",{href:!0});var Bdt=s(lO);rhr=r(Bdt,"TFXLMForQuestionAnsweringSimple"),Bdt.forEach(t),thr=r(BAe," (XLM model)"),BAe.forEach(t),ahr=i(re),nF=n(re,"LI",{});var kAe=s(nF);h_e=n(kAe,"STRONG",{});var kdt=s(h_e);nhr=r(kdt,"xlm-roberta"),kdt.forEach(t),shr=r(kAe," \u2014 "),iO=n(kAe,"A",{href:!0});var xdt=s(iO);lhr=r(xdt,"TFXLMRobertaForQuestionAnswering"),xdt.forEach(t),ihr=r(kAe," (XLM-RoBERTa model)"),kAe.forEach(t),dhr=i(re),sF=n(re,"LI",{});var xAe=s(sF);p_e=n(xAe,"STRONG",{});var Rdt=s(p_e);chr=r(Rdt,"xlnet"),Rdt.forEach(t),fhr=r(xAe," \u2014 "),dO=n(xAe,"A",{href:!0});var Sdt=s(dO);mhr=r(Sdt,"TFXLNetForQuestionAnsweringSimple"),Sdt.forEach(t),ghr=r(xAe," (XLNet model)"),xAe.forEach(t),re.forEach(t),hhr=i(Ta),__e=n(Ta,"P",{});var Pdt=s(__e);phr=r(Pdt,"Examples:"),Pdt.forEach(t),_hr=i(Ta),m(o6.$$.fragment,Ta),Ta.forEach(t),Ul.forEach(t),z9e=i(d),Pc=n(d,"H2",{class:!0});var Zke=s(Pc);lF=n(Zke,"A",{id:!0,class:!0,href:!0});var $dt=s(lF);u_e=n($dt,"SPAN",{});var Idt=s(u_e);m(r6.$$.fragment,Idt),Idt.forEach(t),$dt.forEach(t),uhr=i(Zke),b_e=n(Zke,"SPAN",{});var jdt=s(b_e);bhr=r(jdt,"TFAutoModelForVision2Seq"),jdt.forEach(t),Zke.forEach(t),V9e=i(d),Er=n(d,"DIV",{class:!0});var Yl=s(Er);m(t6.$$.fragment,Yl),vhr=i(Yl),$c=n(Yl,"P",{});var pV=s($c);Thr=r(pV,`This is a generic model class that will be instantiated as one of the model classes of the library (with a vision-to-text modeling head) when created
with the `),v_e=n(pV,"CODE",{});var Ndt=s(v_e);Fhr=r(Ndt,"from_pretrained()"),Ndt.forEach(t),Chr=r(pV,"class method or the "),T_e=n(pV,"CODE",{});var Ddt=s(T_e);Mhr=r(Ddt,"from_config()"),Ddt.forEach(t),Ehr=r(pV,`class
method.`),pV.forEach(t),yhr=i(Yl),a6=n(Yl,"P",{});var exe=s(a6);whr=r(exe,"This class cannot be instantiated directly using "),F_e=n(exe,"CODE",{});var qdt=s(F_e);Ahr=r(qdt,"__init__()"),qdt.forEach(t),Lhr=r(exe," (throws an error)."),exe.forEach(t),Bhr=i(Yl),ut=n(Yl,"DIV",{class:!0});var Kl=s(ut);m(n6.$$.fragment,Kl),khr=i(Kl),C_e=n(Kl,"P",{});var Gdt=s(C_e);xhr=r(Gdt,"Instantiates one of the model classes of the library (with a vision-to-text modeling head) from a configuration."),Gdt.forEach(t),Rhr=i(Kl),Ic=n(Kl,"P",{});var _V=s(Ic);Shr=r(_V,`Note:
Loading a model from its configuration file does `),M_e=n(_V,"STRONG",{});var Odt=s(M_e);Phr=r(Odt,"not"),Odt.forEach(t),$hr=r(_V,` load the model weights. It only affects the
model\u2019s configuration. Use `),E_e=n(_V,"CODE",{});var Xdt=s(E_e);Ihr=r(Xdt,"from_pretrained()"),Xdt.forEach(t),jhr=r(_V,"to load the model weights."),_V.forEach(t),Nhr=i(Kl),y_e=n(Kl,"P",{});var zdt=s(y_e);Dhr=r(zdt,"Examples:"),zdt.forEach(t),qhr=i(Kl),m(s6.$$.fragment,Kl),Kl.forEach(t),Ghr=i(Yl),Eo=n(Yl,"DIV",{class:!0});var Fa=s(Eo);m(l6.$$.fragment,Fa),Ohr=i(Fa),w_e=n(Fa,"P",{});var Vdt=s(w_e);Xhr=r(Vdt,"Instantiate one of the model classes of the library (with a vision-to-text modeling head) from a pretrained model."),Vdt.forEach(t),zhr=i(Fa),vn=n(Fa,"P",{});var z4=s(vn);Vhr=r(z4,"The model class to instantiate is selected based on the "),A_e=n(z4,"CODE",{});var Wdt=s(A_e);Whr=r(Wdt,"model_type"),Wdt.forEach(t),Qhr=r(z4,` property of the config object (either
passed as an argument or loaded from `),L_e=n(z4,"CODE",{});var Qdt=s(L_e);Hhr=r(Qdt,"pretrained_model_name_or_path"),Qdt.forEach(t),Uhr=r(z4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),B_e=n(z4,"CODE",{});var Hdt=s(B_e);Jhr=r(Hdt,"pretrained_model_name_or_path"),Hdt.forEach(t),Yhr=r(z4,":"),z4.forEach(t),Khr=i(Fa),k_e=n(Fa,"UL",{});var Udt=s(k_e);iF=n(Udt,"LI",{});var RAe=s(iF);x_e=n(RAe,"STRONG",{});var Jdt=s(x_e);Zhr=r(Jdt,"vision-encoder-decoder"),Jdt.forEach(t),epr=r(RAe," \u2014 "),cO=n(RAe,"A",{href:!0});var Ydt=s(cO);opr=r(Ydt,"TFVisionEncoderDecoderModel"),Ydt.forEach(t),rpr=r(RAe," (Vision Encoder decoder model)"),RAe.forEach(t),Udt.forEach(t),tpr=i(Fa),R_e=n(Fa,"P",{});var Kdt=s(R_e);apr=r(Kdt,"Examples:"),Kdt.forEach(t),npr=i(Fa),m(i6.$$.fragment,Fa),Fa.forEach(t),Yl.forEach(t),W9e=i(d),jc=n(d,"H2",{class:!0});var oxe=s(jc);dF=n(oxe,"A",{id:!0,class:!0,href:!0});var Zdt=s(dF);S_e=n(Zdt,"SPAN",{});var ect=s(S_e);m(d6.$$.fragment,ect),ect.forEach(t),Zdt.forEach(t),spr=i(oxe),P_e=n(oxe,"SPAN",{});var oct=s(P_e);lpr=r(oct,"TFAutoModelForSpeechSeq2Seq"),oct.forEach(t),oxe.forEach(t),Q9e=i(d),yr=n(d,"DIV",{class:!0});var Zl=s(yr);m(c6.$$.fragment,Zl),ipr=i(Zl),Nc=n(Zl,"P",{});var uV=s(Nc);dpr=r(uV,`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) when created
with the `),$_e=n(uV,"CODE",{});var rct=s($_e);cpr=r(rct,"from_pretrained()"),rct.forEach(t),fpr=r(uV,"class method or the "),I_e=n(uV,"CODE",{});var tct=s(I_e);mpr=r(tct,"from_config()"),tct.forEach(t),gpr=r(uV,`class
method.`),uV.forEach(t),hpr=i(Zl),f6=n(Zl,"P",{});var rxe=s(f6);ppr=r(rxe,"This class cannot be instantiated directly using "),j_e=n(rxe,"CODE",{});var act=s(j_e);_pr=r(act,"__init__()"),act.forEach(t),upr=r(rxe," (throws an error)."),rxe.forEach(t),bpr=i(Zl),bt=n(Zl,"DIV",{class:!0});var ei=s(bt);m(m6.$$.fragment,ei),vpr=i(ei),N_e=n(ei,"P",{});var nct=s(N_e);Tpr=r(nct,"Instantiates one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) from a configuration."),nct.forEach(t),Fpr=i(ei),Dc=n(ei,"P",{});var bV=s(Dc);Cpr=r(bV,`Note:
Loading a model from its configuration file does `),D_e=n(bV,"STRONG",{});var sct=s(D_e);Mpr=r(sct,"not"),sct.forEach(t),Epr=r(bV,` load the model weights. It only affects the
model\u2019s configuration. Use `),q_e=n(bV,"CODE",{});var lct=s(q_e);ypr=r(lct,"from_pretrained()"),lct.forEach(t),wpr=r(bV,"to load the model weights."),bV.forEach(t),Apr=i(ei),G_e=n(ei,"P",{});var ict=s(G_e);Lpr=r(ict,"Examples:"),ict.forEach(t),Bpr=i(ei),m(g6.$$.fragment,ei),ei.forEach(t),kpr=i(Zl),yo=n(Zl,"DIV",{class:!0});var Ca=s(yo);m(h6.$$.fragment,Ca),xpr=i(Ca),O_e=n(Ca,"P",{});var dct=s(O_e);Rpr=r(dct,"Instantiate one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) from a pretrained model."),dct.forEach(t),Spr=i(Ca),Tn=n(Ca,"P",{});var V4=s(Tn);Ppr=r(V4,"The model class to instantiate is selected based on the "),X_e=n(V4,"CODE",{});var cct=s(X_e);$pr=r(cct,"model_type"),cct.forEach(t),Ipr=r(V4,` property of the config object (either
passed as an argument or loaded from `),z_e=n(V4,"CODE",{});var fct=s(z_e);jpr=r(fct,"pretrained_model_name_or_path"),fct.forEach(t),Npr=r(V4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),V_e=n(V4,"CODE",{});var mct=s(V_e);Dpr=r(mct,"pretrained_model_name_or_path"),mct.forEach(t),qpr=r(V4,":"),V4.forEach(t),Gpr=i(Ca),W_e=n(Ca,"UL",{});var gct=s(W_e);cF=n(gct,"LI",{});var SAe=s(cF);Q_e=n(SAe,"STRONG",{});var hct=s(Q_e);Opr=r(hct,"speech_to_text"),hct.forEach(t),Xpr=r(SAe," \u2014 "),fO=n(SAe,"A",{href:!0});var pct=s(fO);zpr=r(pct,"TFSpeech2TextForConditionalGeneration"),pct.forEach(t),Vpr=r(SAe," (Speech2Text model)"),SAe.forEach(t),gct.forEach(t),Wpr=i(Ca),H_e=n(Ca,"P",{});var _ct=s(H_e);Qpr=r(_ct,"Examples:"),_ct.forEach(t),Hpr=i(Ca),m(p6.$$.fragment,Ca),Ca.forEach(t),Zl.forEach(t),H9e=i(d),qc=n(d,"H2",{class:!0});var txe=s(qc);fF=n(txe,"A",{id:!0,class:!0,href:!0});var uct=s(fF);U_e=n(uct,"SPAN",{});var bct=s(U_e);m(_6.$$.fragment,bct),bct.forEach(t),uct.forEach(t),Upr=i(txe),J_e=n(txe,"SPAN",{});var vct=s(J_e);Jpr=r(vct,"FlaxAutoModel"),vct.forEach(t),txe.forEach(t),U9e=i(d),wr=n(d,"DIV",{class:!0});var oi=s(wr);m(u6.$$.fragment,oi),Ypr=i(oi),Gc=n(oi,"P",{});var vV=s(Gc);Kpr=r(vV,`This is a generic model class that will be instantiated as one of the base model classes of the library when created
with the `),Y_e=n(vV,"CODE",{});var Tct=s(Y_e);Zpr=r(Tct,"from_pretrained()"),Tct.forEach(t),e_r=r(vV,"class method or the "),K_e=n(vV,"CODE",{});var Fct=s(K_e);o_r=r(Fct,"from_config()"),Fct.forEach(t),r_r=r(vV,`class
method.`),vV.forEach(t),t_r=i(oi),b6=n(oi,"P",{});var axe=s(b6);a_r=r(axe,"This class cannot be instantiated directly using "),Z_e=n(axe,"CODE",{});var Cct=s(Z_e);n_r=r(Cct,"__init__()"),Cct.forEach(t),s_r=r(axe," (throws an error)."),axe.forEach(t),l_r=i(oi),vt=n(oi,"DIV",{class:!0});var ri=s(vt);m(v6.$$.fragment,ri),i_r=i(ri),eue=n(ri,"P",{});var Mct=s(eue);d_r=r(Mct,"Instantiates one of the base model classes of the library from a configuration."),Mct.forEach(t),c_r=i(ri),Oc=n(ri,"P",{});var TV=s(Oc);f_r=r(TV,`Note:
Loading a model from its configuration file does `),oue=n(TV,"STRONG",{});var Ect=s(oue);m_r=r(Ect,"not"),Ect.forEach(t),g_r=r(TV,` load the model weights. It only affects the
model\u2019s configuration. Use `),rue=n(TV,"CODE",{});var yct=s(rue);h_r=r(yct,"from_pretrained()"),yct.forEach(t),p_r=r(TV,"to load the model weights."),TV.forEach(t),__r=i(ri),tue=n(ri,"P",{});var wct=s(tue);u_r=r(wct,"Examples:"),wct.forEach(t),b_r=i(ri),m(T6.$$.fragment,ri),ri.forEach(t),v_r=i(oi),wo=n(oi,"DIV",{class:!0});var Ma=s(wo);m(F6.$$.fragment,Ma),T_r=i(Ma),aue=n(Ma,"P",{});var Act=s(aue);F_r=r(Act,"Instantiate one of the base model classes of the library from a pretrained model."),Act.forEach(t),C_r=i(Ma),Fn=n(Ma,"P",{});var W4=s(Fn);M_r=r(W4,"The model class to instantiate is selected based on the "),nue=n(W4,"CODE",{});var Lct=s(nue);E_r=r(Lct,"model_type"),Lct.forEach(t),y_r=r(W4,` property of the config object (either
passed as an argument or loaded from `),sue=n(W4,"CODE",{});var Bct=s(sue);w_r=r(Bct,"pretrained_model_name_or_path"),Bct.forEach(t),A_r=r(W4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),lue=n(W4,"CODE",{});var kct=s(lue);L_r=r(kct,"pretrained_model_name_or_path"),kct.forEach(t),B_r=r(W4,":"),W4.forEach(t),k_r=i(Ma),V=n(Ma,"UL",{});var Q=s(V);mF=n(Q,"LI",{});var PAe=s(mF);iue=n(PAe,"STRONG",{});var xct=s(iue);x_r=r(xct,"albert"),xct.forEach(t),R_r=r(PAe," \u2014 "),mO=n(PAe,"A",{href:!0});var Rct=s(mO);S_r=r(Rct,"FlaxAlbertModel"),Rct.forEach(t),P_r=r(PAe," (ALBERT model)"),PAe.forEach(t),$_r=i(Q),gF=n(Q,"LI",{});var $Ae=s(gF);due=n($Ae,"STRONG",{});var Sct=s(due);I_r=r(Sct,"bart"),Sct.forEach(t),j_r=r($Ae," \u2014 "),gO=n($Ae,"A",{href:!0});var Pct=s(gO);N_r=r(Pct,"FlaxBartModel"),Pct.forEach(t),D_r=r($Ae," (BART model)"),$Ae.forEach(t),q_r=i(Q),hF=n(Q,"LI",{});var IAe=s(hF);cue=n(IAe,"STRONG",{});var $ct=s(cue);G_r=r($ct,"beit"),$ct.forEach(t),O_r=r(IAe," \u2014 "),hO=n(IAe,"A",{href:!0});var Ict=s(hO);X_r=r(Ict,"FlaxBeitModel"),Ict.forEach(t),z_r=r(IAe," (BEiT model)"),IAe.forEach(t),V_r=i(Q),pF=n(Q,"LI",{});var jAe=s(pF);fue=n(jAe,"STRONG",{});var jct=s(fue);W_r=r(jct,"bert"),jct.forEach(t),Q_r=r(jAe," \u2014 "),pO=n(jAe,"A",{href:!0});var Nct=s(pO);H_r=r(Nct,"FlaxBertModel"),Nct.forEach(t),U_r=r(jAe," (BERT model)"),jAe.forEach(t),J_r=i(Q),_F=n(Q,"LI",{});var NAe=s(_F);mue=n(NAe,"STRONG",{});var Dct=s(mue);Y_r=r(Dct,"big_bird"),Dct.forEach(t),K_r=r(NAe," \u2014 "),_O=n(NAe,"A",{href:!0});var qct=s(_O);Z_r=r(qct,"FlaxBigBirdModel"),qct.forEach(t),eur=r(NAe," (BigBird model)"),NAe.forEach(t),our=i(Q),uF=n(Q,"LI",{});var DAe=s(uF);gue=n(DAe,"STRONG",{});var Gct=s(gue);rur=r(Gct,"blenderbot"),Gct.forEach(t),tur=r(DAe," \u2014 "),uO=n(DAe,"A",{href:!0});var Oct=s(uO);aur=r(Oct,"FlaxBlenderbotModel"),Oct.forEach(t),nur=r(DAe," (Blenderbot model)"),DAe.forEach(t),sur=i(Q),bF=n(Q,"LI",{});var qAe=s(bF);hue=n(qAe,"STRONG",{});var Xct=s(hue);lur=r(Xct,"blenderbot-small"),Xct.forEach(t),iur=r(qAe," \u2014 "),bO=n(qAe,"A",{href:!0});var zct=s(bO);dur=r(zct,"FlaxBlenderbotSmallModel"),zct.forEach(t),cur=r(qAe," (BlenderbotSmall model)"),qAe.forEach(t),fur=i(Q),vF=n(Q,"LI",{});var GAe=s(vF);pue=n(GAe,"STRONG",{});var Vct=s(pue);mur=r(Vct,"clip"),Vct.forEach(t),gur=r(GAe," \u2014 "),vO=n(GAe,"A",{href:!0});var Wct=s(vO);hur=r(Wct,"FlaxCLIPModel"),Wct.forEach(t),pur=r(GAe," (CLIP model)"),GAe.forEach(t),_ur=i(Q),TF=n(Q,"LI",{});var OAe=s(TF);_ue=n(OAe,"STRONG",{});var Qct=s(_ue);uur=r(Qct,"distilbert"),Qct.forEach(t),bur=r(OAe," \u2014 "),TO=n(OAe,"A",{href:!0});var Hct=s(TO);vur=r(Hct,"FlaxDistilBertModel"),Hct.forEach(t),Tur=r(OAe," (DistilBERT model)"),OAe.forEach(t),Fur=i(Q),FF=n(Q,"LI",{});var XAe=s(FF);uue=n(XAe,"STRONG",{});var Uct=s(uue);Cur=r(Uct,"electra"),Uct.forEach(t),Mur=r(XAe," \u2014 "),FO=n(XAe,"A",{href:!0});var Jct=s(FO);Eur=r(Jct,"FlaxElectraModel"),Jct.forEach(t),yur=r(XAe," (ELECTRA model)"),XAe.forEach(t),wur=i(Q),CF=n(Q,"LI",{});var zAe=s(CF);bue=n(zAe,"STRONG",{});var Yct=s(bue);Aur=r(Yct,"gpt2"),Yct.forEach(t),Lur=r(zAe," \u2014 "),CO=n(zAe,"A",{href:!0});var Kct=s(CO);Bur=r(Kct,"FlaxGPT2Model"),Kct.forEach(t),kur=r(zAe," (OpenAI GPT-2 model)"),zAe.forEach(t),xur=i(Q),MF=n(Q,"LI",{});var VAe=s(MF);vue=n(VAe,"STRONG",{});var Zct=s(vue);Rur=r(Zct,"gpt_neo"),Zct.forEach(t),Sur=r(VAe," \u2014 "),MO=n(VAe,"A",{href:!0});var eft=s(MO);Pur=r(eft,"FlaxGPTNeoModel"),eft.forEach(t),$ur=r(VAe," (GPT Neo model)"),VAe.forEach(t),Iur=i(Q),EF=n(Q,"LI",{});var WAe=s(EF);Tue=n(WAe,"STRONG",{});var oft=s(Tue);jur=r(oft,"gptj"),oft.forEach(t),Nur=r(WAe," \u2014 "),EO=n(WAe,"A",{href:!0});var rft=s(EO);Dur=r(rft,"FlaxGPTJModel"),rft.forEach(t),qur=r(WAe," (GPT-J model)"),WAe.forEach(t),Gur=i(Q),yF=n(Q,"LI",{});var QAe=s(yF);Fue=n(QAe,"STRONG",{});var tft=s(Fue);Our=r(tft,"marian"),tft.forEach(t),Xur=r(QAe," \u2014 "),yO=n(QAe,"A",{href:!0});var aft=s(yO);zur=r(aft,"FlaxMarianModel"),aft.forEach(t),Vur=r(QAe," (Marian model)"),QAe.forEach(t),Wur=i(Q),wF=n(Q,"LI",{});var HAe=s(wF);Cue=n(HAe,"STRONG",{});var nft=s(Cue);Qur=r(nft,"mbart"),nft.forEach(t),Hur=r(HAe," \u2014 "),wO=n(HAe,"A",{href:!0});var sft=s(wO);Uur=r(sft,"FlaxMBartModel"),sft.forEach(t),Jur=r(HAe," (mBART model)"),HAe.forEach(t),Yur=i(Q),AF=n(Q,"LI",{});var UAe=s(AF);Mue=n(UAe,"STRONG",{});var lft=s(Mue);Kur=r(lft,"mt5"),lft.forEach(t),Zur=r(UAe," \u2014 "),AO=n(UAe,"A",{href:!0});var ift=s(AO);e1r=r(ift,"FlaxMT5Model"),ift.forEach(t),o1r=r(UAe," (mT5 model)"),UAe.forEach(t),r1r=i(Q),LF=n(Q,"LI",{});var JAe=s(LF);Eue=n(JAe,"STRONG",{});var dft=s(Eue);t1r=r(dft,"pegasus"),dft.forEach(t),a1r=r(JAe," \u2014 "),LO=n(JAe,"A",{href:!0});var cft=s(LO);n1r=r(cft,"FlaxPegasusModel"),cft.forEach(t),s1r=r(JAe," (Pegasus model)"),JAe.forEach(t),l1r=i(Q),BF=n(Q,"LI",{});var YAe=s(BF);yue=n(YAe,"STRONG",{});var fft=s(yue);i1r=r(fft,"roberta"),fft.forEach(t),d1r=r(YAe," \u2014 "),BO=n(YAe,"A",{href:!0});var mft=s(BO);c1r=r(mft,"FlaxRobertaModel"),mft.forEach(t),f1r=r(YAe," (RoBERTa model)"),YAe.forEach(t),m1r=i(Q),kF=n(Q,"LI",{});var KAe=s(kF);wue=n(KAe,"STRONG",{});var gft=s(wue);g1r=r(gft,"roformer"),gft.forEach(t),h1r=r(KAe," \u2014 "),kO=n(KAe,"A",{href:!0});var hft=s(kO);p1r=r(hft,"FlaxRoFormerModel"),hft.forEach(t),_1r=r(KAe," (RoFormer model)"),KAe.forEach(t),u1r=i(Q),xF=n(Q,"LI",{});var ZAe=s(xF);Aue=n(ZAe,"STRONG",{});var pft=s(Aue);b1r=r(pft,"t5"),pft.forEach(t),v1r=r(ZAe," \u2014 "),xO=n(ZAe,"A",{href:!0});var _ft=s(xO);T1r=r(_ft,"FlaxT5Model"),_ft.forEach(t),F1r=r(ZAe," (T5 model)"),ZAe.forEach(t),C1r=i(Q),RF=n(Q,"LI",{});var eLe=s(RF);Lue=n(eLe,"STRONG",{});var uft=s(Lue);M1r=r(uft,"vision-text-dual-encoder"),uft.forEach(t),E1r=r(eLe," \u2014 "),RO=n(eLe,"A",{href:!0});var bft=s(RO);y1r=r(bft,"FlaxVisionTextDualEncoderModel"),bft.forEach(t),w1r=r(eLe," (VisionTextDualEncoder model)"),eLe.forEach(t),A1r=i(Q),SF=n(Q,"LI",{});var oLe=s(SF);Bue=n(oLe,"STRONG",{});var vft=s(Bue);L1r=r(vft,"vit"),vft.forEach(t),B1r=r(oLe," \u2014 "),SO=n(oLe,"A",{href:!0});var Tft=s(SO);k1r=r(Tft,"FlaxViTModel"),Tft.forEach(t),x1r=r(oLe," (ViT model)"),oLe.forEach(t),R1r=i(Q),PF=n(Q,"LI",{});var rLe=s(PF);kue=n(rLe,"STRONG",{});var Fft=s(kue);S1r=r(Fft,"wav2vec2"),Fft.forEach(t),P1r=r(rLe," \u2014 "),PO=n(rLe,"A",{href:!0});var Cft=s(PO);$1r=r(Cft,"FlaxWav2Vec2Model"),Cft.forEach(t),I1r=r(rLe," (Wav2Vec2 model)"),rLe.forEach(t),j1r=i(Q),$F=n(Q,"LI",{});var tLe=s($F);xue=n(tLe,"STRONG",{});var Mft=s(xue);N1r=r(Mft,"xglm"),Mft.forEach(t),D1r=r(tLe," \u2014 "),$O=n(tLe,"A",{href:!0});var Eft=s($O);q1r=r(Eft,"FlaxXGLMModel"),Eft.forEach(t),G1r=r(tLe," (XGLM model)"),tLe.forEach(t),Q.forEach(t),O1r=i(Ma),Rue=n(Ma,"P",{});var yft=s(Rue);X1r=r(yft,"Examples:"),yft.forEach(t),z1r=i(Ma),m(C6.$$.fragment,Ma),Ma.forEach(t),oi.forEach(t),J9e=i(d),Xc=n(d,"H2",{class:!0});var nxe=s(Xc);IF=n(nxe,"A",{id:!0,class:!0,href:!0});var wft=s(IF);Sue=n(wft,"SPAN",{});var Aft=s(Sue);m(M6.$$.fragment,Aft),Aft.forEach(t),wft.forEach(t),V1r=i(nxe),Pue=n(nxe,"SPAN",{});var Lft=s(Pue);W1r=r(Lft,"FlaxAutoModelForCausalLM"),Lft.forEach(t),nxe.forEach(t),Y9e=i(d),Ar=n(d,"DIV",{class:!0});var ti=s(Ar);m(E6.$$.fragment,ti),Q1r=i(ti),zc=n(ti,"P",{});var FV=s(zc);H1r=r(FV,`This is a generic model class that will be instantiated as one of the model classes of the library (with a causal language modeling head) when created
with the `),$ue=n(FV,"CODE",{});var Bft=s($ue);U1r=r(Bft,"from_pretrained()"),Bft.forEach(t),J1r=r(FV,"class method or the "),Iue=n(FV,"CODE",{});var kft=s(Iue);Y1r=r(kft,"from_config()"),kft.forEach(t),K1r=r(FV,`class
method.`),FV.forEach(t),Z1r=i(ti),y6=n(ti,"P",{});var sxe=s(y6);e7r=r(sxe,"This class cannot be instantiated directly using "),jue=n(sxe,"CODE",{});var xft=s(jue);o7r=r(xft,"__init__()"),xft.forEach(t),r7r=r(sxe," (throws an error)."),sxe.forEach(t),t7r=i(ti),Tt=n(ti,"DIV",{class:!0});var ai=s(Tt);m(w6.$$.fragment,ai),a7r=i(ai),Nue=n(ai,"P",{});var Rft=s(Nue);n7r=r(Rft,"Instantiates one of the model classes of the library (with a causal language modeling head) from a configuration."),Rft.forEach(t),s7r=i(ai),Vc=n(ai,"P",{});var CV=s(Vc);l7r=r(CV,`Note:
Loading a model from its configuration file does `),Due=n(CV,"STRONG",{});var Sft=s(Due);i7r=r(Sft,"not"),Sft.forEach(t),d7r=r(CV,` load the model weights. It only affects the
model\u2019s configuration. Use `),que=n(CV,"CODE",{});var Pft=s(que);c7r=r(Pft,"from_pretrained()"),Pft.forEach(t),f7r=r(CV,"to load the model weights."),CV.forEach(t),m7r=i(ai),Gue=n(ai,"P",{});var $ft=s(Gue);g7r=r($ft,"Examples:"),$ft.forEach(t),h7r=i(ai),m(A6.$$.fragment,ai),ai.forEach(t),p7r=i(ti),Ao=n(ti,"DIV",{class:!0});var Ea=s(Ao);m(L6.$$.fragment,Ea),_7r=i(Ea),Oue=n(Ea,"P",{});var Ift=s(Oue);u7r=r(Ift,"Instantiate one of the model classes of the library (with a causal language modeling head) from a pretrained model."),Ift.forEach(t),b7r=i(Ea),Cn=n(Ea,"P",{});var Q4=s(Cn);v7r=r(Q4,"The model class to instantiate is selected based on the "),Xue=n(Q4,"CODE",{});var jft=s(Xue);T7r=r(jft,"model_type"),jft.forEach(t),F7r=r(Q4,` property of the config object (either
passed as an argument or loaded from `),zue=n(Q4,"CODE",{});var Nft=s(zue);C7r=r(Nft,"pretrained_model_name_or_path"),Nft.forEach(t),M7r=r(Q4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Vue=n(Q4,"CODE",{});var Dft=s(Vue);E7r=r(Dft,"pretrained_model_name_or_path"),Dft.forEach(t),y7r=r(Q4,":"),Q4.forEach(t),w7r=i(Ea),Mn=n(Ea,"UL",{});var H4=s(Mn);jF=n(H4,"LI",{});var aLe=s(jF);Wue=n(aLe,"STRONG",{});var qft=s(Wue);A7r=r(qft,"gpt2"),qft.forEach(t),L7r=r(aLe," \u2014 "),IO=n(aLe,"A",{href:!0});var Gft=s(IO);B7r=r(Gft,"FlaxGPT2LMHeadModel"),Gft.forEach(t),k7r=r(aLe," (OpenAI GPT-2 model)"),aLe.forEach(t),x7r=i(H4),NF=n(H4,"LI",{});var nLe=s(NF);Que=n(nLe,"STRONG",{});var Oft=s(Que);R7r=r(Oft,"gpt_neo"),Oft.forEach(t),S7r=r(nLe," \u2014 "),jO=n(nLe,"A",{href:!0});var Xft=s(jO);P7r=r(Xft,"FlaxGPTNeoForCausalLM"),Xft.forEach(t),$7r=r(nLe," (GPT Neo model)"),nLe.forEach(t),I7r=i(H4),DF=n(H4,"LI",{});var sLe=s(DF);Hue=n(sLe,"STRONG",{});var zft=s(Hue);j7r=r(zft,"gptj"),zft.forEach(t),N7r=r(sLe," \u2014 "),NO=n(sLe,"A",{href:!0});var Vft=s(NO);D7r=r(Vft,"FlaxGPTJForCausalLM"),Vft.forEach(t),q7r=r(sLe," (GPT-J model)"),sLe.forEach(t),G7r=i(H4),qF=n(H4,"LI",{});var lLe=s(qF);Uue=n(lLe,"STRONG",{});var Wft=s(Uue);O7r=r(Wft,"xglm"),Wft.forEach(t),X7r=r(lLe," \u2014 "),DO=n(lLe,"A",{href:!0});var Qft=s(DO);z7r=r(Qft,"FlaxXGLMForCausalLM"),Qft.forEach(t),V7r=r(lLe," (XGLM model)"),lLe.forEach(t),H4.forEach(t),W7r=i(Ea),Jue=n(Ea,"P",{});var Hft=s(Jue);Q7r=r(Hft,"Examples:"),Hft.forEach(t),H7r=i(Ea),m(B6.$$.fragment,Ea),Ea.forEach(t),ti.forEach(t),K9e=i(d),Wc=n(d,"H2",{class:!0});var lxe=s(Wc);GF=n(lxe,"A",{id:!0,class:!0,href:!0});var Uft=s(GF);Yue=n(Uft,"SPAN",{});var Jft=s(Yue);m(k6.$$.fragment,Jft),Jft.forEach(t),Uft.forEach(t),U7r=i(lxe),Kue=n(lxe,"SPAN",{});var Yft=s(Kue);J7r=r(Yft,"FlaxAutoModelForPreTraining"),Yft.forEach(t),lxe.forEach(t),Z9e=i(d),Lr=n(d,"DIV",{class:!0});var ni=s(Lr);m(x6.$$.fragment,ni),Y7r=i(ni),Qc=n(ni,"P",{});var MV=s(Qc);K7r=r(MV,`This is a generic model class that will be instantiated as one of the model classes of the library (with a pretraining head) when created
with the `),Zue=n(MV,"CODE",{});var Kft=s(Zue);Z7r=r(Kft,"from_pretrained()"),Kft.forEach(t),ebr=r(MV,"class method or the "),e1e=n(MV,"CODE",{});var Zft=s(e1e);obr=r(Zft,"from_config()"),Zft.forEach(t),rbr=r(MV,`class
method.`),MV.forEach(t),tbr=i(ni),R6=n(ni,"P",{});var ixe=s(R6);abr=r(ixe,"This class cannot be instantiated directly using "),o1e=n(ixe,"CODE",{});var emt=s(o1e);nbr=r(emt,"__init__()"),emt.forEach(t),sbr=r(ixe," (throws an error)."),ixe.forEach(t),lbr=i(ni),Ft=n(ni,"DIV",{class:!0});var si=s(Ft);m(S6.$$.fragment,si),ibr=i(si),r1e=n(si,"P",{});var omt=s(r1e);dbr=r(omt,"Instantiates one of the model classes of the library (with a pretraining head) from a configuration."),omt.forEach(t),cbr=i(si),Hc=n(si,"P",{});var EV=s(Hc);fbr=r(EV,`Note:
Loading a model from its configuration file does `),t1e=n(EV,"STRONG",{});var rmt=s(t1e);mbr=r(rmt,"not"),rmt.forEach(t),gbr=r(EV,` load the model weights. It only affects the
model\u2019s configuration. Use `),a1e=n(EV,"CODE",{});var tmt=s(a1e);hbr=r(tmt,"from_pretrained()"),tmt.forEach(t),pbr=r(EV,"to load the model weights."),EV.forEach(t),_br=i(si),n1e=n(si,"P",{});var amt=s(n1e);ubr=r(amt,"Examples:"),amt.forEach(t),bbr=i(si),m(P6.$$.fragment,si),si.forEach(t),vbr=i(ni),Lo=n(ni,"DIV",{class:!0});var ya=s(Lo);m($6.$$.fragment,ya),Tbr=i(ya),s1e=n(ya,"P",{});var nmt=s(s1e);Fbr=r(nmt,"Instantiate one of the model classes of the library (with a pretraining head) from a pretrained model."),nmt.forEach(t),Cbr=i(ya),En=n(ya,"P",{});var U4=s(En);Mbr=r(U4,"The model class to instantiate is selected based on the "),l1e=n(U4,"CODE",{});var smt=s(l1e);Ebr=r(smt,"model_type"),smt.forEach(t),ybr=r(U4,` property of the config object (either
passed as an argument or loaded from `),i1e=n(U4,"CODE",{});var lmt=s(i1e);wbr=r(lmt,"pretrained_model_name_or_path"),lmt.forEach(t),Abr=r(U4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),d1e=n(U4,"CODE",{});var imt=s(d1e);Lbr=r(imt,"pretrained_model_name_or_path"),imt.forEach(t),Bbr=r(U4,":"),U4.forEach(t),kbr=i(ya),fe=n(ya,"UL",{});var ue=s(fe);OF=n(ue,"LI",{});var iLe=s(OF);c1e=n(iLe,"STRONG",{});var dmt=s(c1e);xbr=r(dmt,"albert"),dmt.forEach(t),Rbr=r(iLe," \u2014 "),qO=n(iLe,"A",{href:!0});var cmt=s(qO);Sbr=r(cmt,"FlaxAlbertForPreTraining"),cmt.forEach(t),Pbr=r(iLe," (ALBERT model)"),iLe.forEach(t),$br=i(ue),XF=n(ue,"LI",{});var dLe=s(XF);f1e=n(dLe,"STRONG",{});var fmt=s(f1e);Ibr=r(fmt,"bart"),fmt.forEach(t),jbr=r(dLe," \u2014 "),GO=n(dLe,"A",{href:!0});var mmt=s(GO);Nbr=r(mmt,"FlaxBartForConditionalGeneration"),mmt.forEach(t),Dbr=r(dLe," (BART model)"),dLe.forEach(t),qbr=i(ue),zF=n(ue,"LI",{});var cLe=s(zF);m1e=n(cLe,"STRONG",{});var gmt=s(m1e);Gbr=r(gmt,"bert"),gmt.forEach(t),Obr=r(cLe," \u2014 "),OO=n(cLe,"A",{href:!0});var hmt=s(OO);Xbr=r(hmt,"FlaxBertForPreTraining"),hmt.forEach(t),zbr=r(cLe," (BERT model)"),cLe.forEach(t),Vbr=i(ue),VF=n(ue,"LI",{});var fLe=s(VF);g1e=n(fLe,"STRONG",{});var pmt=s(g1e);Wbr=r(pmt,"big_bird"),pmt.forEach(t),Qbr=r(fLe," \u2014 "),XO=n(fLe,"A",{href:!0});var _mt=s(XO);Hbr=r(_mt,"FlaxBigBirdForPreTraining"),_mt.forEach(t),Ubr=r(fLe," (BigBird model)"),fLe.forEach(t),Jbr=i(ue),WF=n(ue,"LI",{});var mLe=s(WF);h1e=n(mLe,"STRONG",{});var umt=s(h1e);Ybr=r(umt,"electra"),umt.forEach(t),Kbr=r(mLe," \u2014 "),zO=n(mLe,"A",{href:!0});var bmt=s(zO);Zbr=r(bmt,"FlaxElectraForPreTraining"),bmt.forEach(t),e5r=r(mLe," (ELECTRA model)"),mLe.forEach(t),o5r=i(ue),QF=n(ue,"LI",{});var gLe=s(QF);p1e=n(gLe,"STRONG",{});var vmt=s(p1e);r5r=r(vmt,"mbart"),vmt.forEach(t),t5r=r(gLe," \u2014 "),VO=n(gLe,"A",{href:!0});var Tmt=s(VO);a5r=r(Tmt,"FlaxMBartForConditionalGeneration"),Tmt.forEach(t),n5r=r(gLe," (mBART model)"),gLe.forEach(t),s5r=i(ue),HF=n(ue,"LI",{});var hLe=s(HF);_1e=n(hLe,"STRONG",{});var Fmt=s(_1e);l5r=r(Fmt,"mt5"),Fmt.forEach(t),i5r=r(hLe," \u2014 "),WO=n(hLe,"A",{href:!0});var Cmt=s(WO);d5r=r(Cmt,"FlaxMT5ForConditionalGeneration"),Cmt.forEach(t),c5r=r(hLe," (mT5 model)"),hLe.forEach(t),f5r=i(ue),UF=n(ue,"LI",{});var pLe=s(UF);u1e=n(pLe,"STRONG",{});var Mmt=s(u1e);m5r=r(Mmt,"roberta"),Mmt.forEach(t),g5r=r(pLe," \u2014 "),QO=n(pLe,"A",{href:!0});var Emt=s(QO);h5r=r(Emt,"FlaxRobertaForMaskedLM"),Emt.forEach(t),p5r=r(pLe," (RoBERTa model)"),pLe.forEach(t),_5r=i(ue),JF=n(ue,"LI",{});var _Le=s(JF);b1e=n(_Le,"STRONG",{});var ymt=s(b1e);u5r=r(ymt,"roformer"),ymt.forEach(t),b5r=r(_Le," \u2014 "),HO=n(_Le,"A",{href:!0});var wmt=s(HO);v5r=r(wmt,"FlaxRoFormerForMaskedLM"),wmt.forEach(t),T5r=r(_Le," (RoFormer model)"),_Le.forEach(t),F5r=i(ue),YF=n(ue,"LI",{});var uLe=s(YF);v1e=n(uLe,"STRONG",{});var Amt=s(v1e);C5r=r(Amt,"t5"),Amt.forEach(t),M5r=r(uLe," \u2014 "),UO=n(uLe,"A",{href:!0});var Lmt=s(UO);E5r=r(Lmt,"FlaxT5ForConditionalGeneration"),Lmt.forEach(t),y5r=r(uLe," (T5 model)"),uLe.forEach(t),w5r=i(ue),KF=n(ue,"LI",{});var bLe=s(KF);T1e=n(bLe,"STRONG",{});var Bmt=s(T1e);A5r=r(Bmt,"wav2vec2"),Bmt.forEach(t),L5r=r(bLe," \u2014 "),JO=n(bLe,"A",{href:!0});var kmt=s(JO);B5r=r(kmt,"FlaxWav2Vec2ForPreTraining"),kmt.forEach(t),k5r=r(bLe," (Wav2Vec2 model)"),bLe.forEach(t),ue.forEach(t),x5r=i(ya),F1e=n(ya,"P",{});var xmt=s(F1e);R5r=r(xmt,"Examples:"),xmt.forEach(t),S5r=i(ya),m(I6.$$.fragment,ya),ya.forEach(t),ni.forEach(t),eBe=i(d),Uc=n(d,"H2",{class:!0});var dxe=s(Uc);ZF=n(dxe,"A",{id:!0,class:!0,href:!0});var Rmt=s(ZF);C1e=n(Rmt,"SPAN",{});var Smt=s(C1e);m(j6.$$.fragment,Smt),Smt.forEach(t),Rmt.forEach(t),P5r=i(dxe),M1e=n(dxe,"SPAN",{});var Pmt=s(M1e);$5r=r(Pmt,"FlaxAutoModelForMaskedLM"),Pmt.forEach(t),dxe.forEach(t),oBe=i(d),Br=n(d,"DIV",{class:!0});var li=s(Br);m(N6.$$.fragment,li),I5r=i(li),Jc=n(li,"P",{});var yV=s(Jc);j5r=r(yV,`This is a generic model class that will be instantiated as one of the model classes of the library (with a masked language modeling head) when created
with the `),E1e=n(yV,"CODE",{});var $mt=s(E1e);N5r=r($mt,"from_pretrained()"),$mt.forEach(t),D5r=r(yV,"class method or the "),y1e=n(yV,"CODE",{});var Imt=s(y1e);q5r=r(Imt,"from_config()"),Imt.forEach(t),G5r=r(yV,`class
method.`),yV.forEach(t),O5r=i(li),D6=n(li,"P",{});var cxe=s(D6);X5r=r(cxe,"This class cannot be instantiated directly using "),w1e=n(cxe,"CODE",{});var jmt=s(w1e);z5r=r(jmt,"__init__()"),jmt.forEach(t),V5r=r(cxe," (throws an error)."),cxe.forEach(t),W5r=i(li),Ct=n(li,"DIV",{class:!0});var ii=s(Ct);m(q6.$$.fragment,ii),Q5r=i(ii),A1e=n(ii,"P",{});var Nmt=s(A1e);H5r=r(Nmt,"Instantiates one of the model classes of the library (with a masked language modeling head) from a configuration."),Nmt.forEach(t),U5r=i(ii),Yc=n(ii,"P",{});var wV=s(Yc);J5r=r(wV,`Note:
Loading a model from its configuration file does `),L1e=n(wV,"STRONG",{});var Dmt=s(L1e);Y5r=r(Dmt,"not"),Dmt.forEach(t),K5r=r(wV,` load the model weights. It only affects the
model\u2019s configuration. Use `),B1e=n(wV,"CODE",{});var qmt=s(B1e);Z5r=r(qmt,"from_pretrained()"),qmt.forEach(t),e2r=r(wV,"to load the model weights."),wV.forEach(t),o2r=i(ii),k1e=n(ii,"P",{});var Gmt=s(k1e);r2r=r(Gmt,"Examples:"),Gmt.forEach(t),t2r=i(ii),m(G6.$$.fragment,ii),ii.forEach(t),a2r=i(li),Bo=n(li,"DIV",{class:!0});var wa=s(Bo);m(O6.$$.fragment,wa),n2r=i(wa),x1e=n(wa,"P",{});var Omt=s(x1e);s2r=r(Omt,"Instantiate one of the model classes of the library (with a masked language modeling head) from a pretrained model."),Omt.forEach(t),l2r=i(wa),yn=n(wa,"P",{});var J4=s(yn);i2r=r(J4,"The model class to instantiate is selected based on the "),R1e=n(J4,"CODE",{});var Xmt=s(R1e);d2r=r(Xmt,"model_type"),Xmt.forEach(t),c2r=r(J4,` property of the config object (either
passed as an argument or loaded from `),S1e=n(J4,"CODE",{});var zmt=s(S1e);f2r=r(zmt,"pretrained_model_name_or_path"),zmt.forEach(t),m2r=r(J4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),P1e=n(J4,"CODE",{});var Vmt=s(P1e);g2r=r(Vmt,"pretrained_model_name_or_path"),Vmt.forEach(t),h2r=r(J4,":"),J4.forEach(t),p2r=i(wa),ve=n(wa,"UL",{});var Ze=s(ve);eC=n(Ze,"LI",{});var vLe=s(eC);$1e=n(vLe,"STRONG",{});var Wmt=s($1e);_2r=r(Wmt,"albert"),Wmt.forEach(t),u2r=r(vLe," \u2014 "),YO=n(vLe,"A",{href:!0});var Qmt=s(YO);b2r=r(Qmt,"FlaxAlbertForMaskedLM"),Qmt.forEach(t),v2r=r(vLe," (ALBERT model)"),vLe.forEach(t),T2r=i(Ze),oC=n(Ze,"LI",{});var TLe=s(oC);I1e=n(TLe,"STRONG",{});var Hmt=s(I1e);F2r=r(Hmt,"bart"),Hmt.forEach(t),C2r=r(TLe," \u2014 "),KO=n(TLe,"A",{href:!0});var Umt=s(KO);M2r=r(Umt,"FlaxBartForConditionalGeneration"),Umt.forEach(t),E2r=r(TLe," (BART model)"),TLe.forEach(t),y2r=i(Ze),rC=n(Ze,"LI",{});var FLe=s(rC);j1e=n(FLe,"STRONG",{});var Jmt=s(j1e);w2r=r(Jmt,"bert"),Jmt.forEach(t),A2r=r(FLe," \u2014 "),ZO=n(FLe,"A",{href:!0});var Ymt=s(ZO);L2r=r(Ymt,"FlaxBertForMaskedLM"),Ymt.forEach(t),B2r=r(FLe," (BERT model)"),FLe.forEach(t),k2r=i(Ze),tC=n(Ze,"LI",{});var CLe=s(tC);N1e=n(CLe,"STRONG",{});var Kmt=s(N1e);x2r=r(Kmt,"big_bird"),Kmt.forEach(t),R2r=r(CLe," \u2014 "),eX=n(CLe,"A",{href:!0});var Zmt=s(eX);S2r=r(Zmt,"FlaxBigBirdForMaskedLM"),Zmt.forEach(t),P2r=r(CLe," (BigBird model)"),CLe.forEach(t),$2r=i(Ze),aC=n(Ze,"LI",{});var MLe=s(aC);D1e=n(MLe,"STRONG",{});var egt=s(D1e);I2r=r(egt,"distilbert"),egt.forEach(t),j2r=r(MLe," \u2014 "),oX=n(MLe,"A",{href:!0});var ogt=s(oX);N2r=r(ogt,"FlaxDistilBertForMaskedLM"),ogt.forEach(t),D2r=r(MLe," (DistilBERT model)"),MLe.forEach(t),q2r=i(Ze),nC=n(Ze,"LI",{});var ELe=s(nC);q1e=n(ELe,"STRONG",{});var rgt=s(q1e);G2r=r(rgt,"electra"),rgt.forEach(t),O2r=r(ELe," \u2014 "),rX=n(ELe,"A",{href:!0});var tgt=s(rX);X2r=r(tgt,"FlaxElectraForMaskedLM"),tgt.forEach(t),z2r=r(ELe," (ELECTRA model)"),ELe.forEach(t),V2r=i(Ze),sC=n(Ze,"LI",{});var yLe=s(sC);G1e=n(yLe,"STRONG",{});var agt=s(G1e);W2r=r(agt,"mbart"),agt.forEach(t),Q2r=r(yLe," \u2014 "),tX=n(yLe,"A",{href:!0});var ngt=s(tX);H2r=r(ngt,"FlaxMBartForConditionalGeneration"),ngt.forEach(t),U2r=r(yLe," (mBART model)"),yLe.forEach(t),J2r=i(Ze),lC=n(Ze,"LI",{});var wLe=s(lC);O1e=n(wLe,"STRONG",{});var sgt=s(O1e);Y2r=r(sgt,"roberta"),sgt.forEach(t),K2r=r(wLe," \u2014 "),aX=n(wLe,"A",{href:!0});var lgt=s(aX);Z2r=r(lgt,"FlaxRobertaForMaskedLM"),lgt.forEach(t),evr=r(wLe," (RoBERTa model)"),wLe.forEach(t),ovr=i(Ze),iC=n(Ze,"LI",{});var ALe=s(iC);X1e=n(ALe,"STRONG",{});var igt=s(X1e);rvr=r(igt,"roformer"),igt.forEach(t),tvr=r(ALe," \u2014 "),nX=n(ALe,"A",{href:!0});var dgt=s(nX);avr=r(dgt,"FlaxRoFormerForMaskedLM"),dgt.forEach(t),nvr=r(ALe," (RoFormer model)"),ALe.forEach(t),Ze.forEach(t),svr=i(wa),z1e=n(wa,"P",{});var cgt=s(z1e);lvr=r(cgt,"Examples:"),cgt.forEach(t),ivr=i(wa),m(X6.$$.fragment,wa),wa.forEach(t),li.forEach(t),rBe=i(d),Kc=n(d,"H2",{class:!0});var fxe=s(Kc);dC=n(fxe,"A",{id:!0,class:!0,href:!0});var fgt=s(dC);V1e=n(fgt,"SPAN",{});var mgt=s(V1e);m(z6.$$.fragment,mgt),mgt.forEach(t),fgt.forEach(t),dvr=i(fxe),W1e=n(fxe,"SPAN",{});var ggt=s(W1e);cvr=r(ggt,"FlaxAutoModelForSeq2SeqLM"),ggt.forEach(t),fxe.forEach(t),tBe=i(d),kr=n(d,"DIV",{class:!0});var di=s(kr);m(V6.$$.fragment,di),fvr=i(di),Zc=n(di,"P",{});var AV=s(Zc);mvr=r(AV,`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence language modeling head) when created
with the `),Q1e=n(AV,"CODE",{});var hgt=s(Q1e);gvr=r(hgt,"from_pretrained()"),hgt.forEach(t),hvr=r(AV,"class method or the "),H1e=n(AV,"CODE",{});var pgt=s(H1e);pvr=r(pgt,"from_config()"),pgt.forEach(t),_vr=r(AV,`class
method.`),AV.forEach(t),uvr=i(di),W6=n(di,"P",{});var mxe=s(W6);bvr=r(mxe,"This class cannot be instantiated directly using "),U1e=n(mxe,"CODE",{});var _gt=s(U1e);vvr=r(_gt,"__init__()"),_gt.forEach(t),Tvr=r(mxe," (throws an error)."),mxe.forEach(t),Fvr=i(di),Mt=n(di,"DIV",{class:!0});var ci=s(Mt);m(Q6.$$.fragment,ci),Cvr=i(ci),J1e=n(ci,"P",{});var ugt=s(J1e);Mvr=r(ugt,"Instantiates one of the model classes of the library (with a sequence-to-sequence language modeling head) from a configuration."),ugt.forEach(t),Evr=i(ci),ef=n(ci,"P",{});var LV=s(ef);yvr=r(LV,`Note:
Loading a model from its configuration file does `),Y1e=n(LV,"STRONG",{});var bgt=s(Y1e);wvr=r(bgt,"not"),bgt.forEach(t),Avr=r(LV,` load the model weights. It only affects the
model\u2019s configuration. Use `),K1e=n(LV,"CODE",{});var vgt=s(K1e);Lvr=r(vgt,"from_pretrained()"),vgt.forEach(t),Bvr=r(LV,"to load the model weights."),LV.forEach(t),kvr=i(ci),Z1e=n(ci,"P",{});var Tgt=s(Z1e);xvr=r(Tgt,"Examples:"),Tgt.forEach(t),Rvr=i(ci),m(H6.$$.fragment,ci),ci.forEach(t),Svr=i(di),ko=n(di,"DIV",{class:!0});var Aa=s(ko);m(U6.$$.fragment,Aa),Pvr=i(Aa),e7e=n(Aa,"P",{});var Fgt=s(e7e);$vr=r(Fgt,"Instantiate one of the model classes of the library (with a sequence-to-sequence language modeling head) from a pretrained model."),Fgt.forEach(t),Ivr=i(Aa),wn=n(Aa,"P",{});var Y4=s(wn);jvr=r(Y4,"The model class to instantiate is selected based on the "),o7e=n(Y4,"CODE",{});var Cgt=s(o7e);Nvr=r(Cgt,"model_type"),Cgt.forEach(t),Dvr=r(Y4,` property of the config object (either
passed as an argument or loaded from `),r7e=n(Y4,"CODE",{});var Mgt=s(r7e);qvr=r(Mgt,"pretrained_model_name_or_path"),Mgt.forEach(t),Gvr=r(Y4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),t7e=n(Y4,"CODE",{});var Egt=s(t7e);Ovr=r(Egt,"pretrained_model_name_or_path"),Egt.forEach(t),Xvr=r(Y4,":"),Y4.forEach(t),zvr=i(Aa),Te=n(Aa,"UL",{});var eo=s(Te);cC=n(eo,"LI",{});var LLe=s(cC);a7e=n(LLe,"STRONG",{});var ygt=s(a7e);Vvr=r(ygt,"bart"),ygt.forEach(t),Wvr=r(LLe," \u2014 "),sX=n(LLe,"A",{href:!0});var wgt=s(sX);Qvr=r(wgt,"FlaxBartForConditionalGeneration"),wgt.forEach(t),Hvr=r(LLe," (BART model)"),LLe.forEach(t),Uvr=i(eo),fC=n(eo,"LI",{});var BLe=s(fC);n7e=n(BLe,"STRONG",{});var Agt=s(n7e);Jvr=r(Agt,"blenderbot"),Agt.forEach(t),Yvr=r(BLe," \u2014 "),lX=n(BLe,"A",{href:!0});var Lgt=s(lX);Kvr=r(Lgt,"FlaxBlenderbotForConditionalGeneration"),Lgt.forEach(t),Zvr=r(BLe," (Blenderbot model)"),BLe.forEach(t),e0r=i(eo),mC=n(eo,"LI",{});var kLe=s(mC);s7e=n(kLe,"STRONG",{});var Bgt=s(s7e);o0r=r(Bgt,"blenderbot-small"),Bgt.forEach(t),r0r=r(kLe," \u2014 "),iX=n(kLe,"A",{href:!0});var kgt=s(iX);t0r=r(kgt,"FlaxBlenderbotSmallForConditionalGeneration"),kgt.forEach(t),a0r=r(kLe," (BlenderbotSmall model)"),kLe.forEach(t),n0r=i(eo),gC=n(eo,"LI",{});var xLe=s(gC);l7e=n(xLe,"STRONG",{});var xgt=s(l7e);s0r=r(xgt,"encoder-decoder"),xgt.forEach(t),l0r=r(xLe," \u2014 "),dX=n(xLe,"A",{href:!0});var Rgt=s(dX);i0r=r(Rgt,"FlaxEncoderDecoderModel"),Rgt.forEach(t),d0r=r(xLe," (Encoder decoder model)"),xLe.forEach(t),c0r=i(eo),hC=n(eo,"LI",{});var RLe=s(hC);i7e=n(RLe,"STRONG",{});var Sgt=s(i7e);f0r=r(Sgt,"marian"),Sgt.forEach(t),m0r=r(RLe," \u2014 "),cX=n(RLe,"A",{href:!0});var Pgt=s(cX);g0r=r(Pgt,"FlaxMarianMTModel"),Pgt.forEach(t),h0r=r(RLe," (Marian model)"),RLe.forEach(t),p0r=i(eo),pC=n(eo,"LI",{});var SLe=s(pC);d7e=n(SLe,"STRONG",{});var $gt=s(d7e);_0r=r($gt,"mbart"),$gt.forEach(t),u0r=r(SLe," \u2014 "),fX=n(SLe,"A",{href:!0});var Igt=s(fX);b0r=r(Igt,"FlaxMBartForConditionalGeneration"),Igt.forEach(t),v0r=r(SLe," (mBART model)"),SLe.forEach(t),T0r=i(eo),_C=n(eo,"LI",{});var PLe=s(_C);c7e=n(PLe,"STRONG",{});var jgt=s(c7e);F0r=r(jgt,"mt5"),jgt.forEach(t),C0r=r(PLe," \u2014 "),mX=n(PLe,"A",{href:!0});var Ngt=s(mX);M0r=r(Ngt,"FlaxMT5ForConditionalGeneration"),Ngt.forEach(t),E0r=r(PLe," (mT5 model)"),PLe.forEach(t),y0r=i(eo),uC=n(eo,"LI",{});var $Le=s(uC);f7e=n($Le,"STRONG",{});var Dgt=s(f7e);w0r=r(Dgt,"pegasus"),Dgt.forEach(t),A0r=r($Le," \u2014 "),gX=n($Le,"A",{href:!0});var qgt=s(gX);L0r=r(qgt,"FlaxPegasusForConditionalGeneration"),qgt.forEach(t),B0r=r($Le," (Pegasus model)"),$Le.forEach(t),k0r=i(eo),bC=n(eo,"LI",{});var ILe=s(bC);m7e=n(ILe,"STRONG",{});var Ggt=s(m7e);x0r=r(Ggt,"t5"),Ggt.forEach(t),R0r=r(ILe," \u2014 "),hX=n(ILe,"A",{href:!0});var Ogt=s(hX);S0r=r(Ogt,"FlaxT5ForConditionalGeneration"),Ogt.forEach(t),P0r=r(ILe," (T5 model)"),ILe.forEach(t),eo.forEach(t),$0r=i(Aa),g7e=n(Aa,"P",{});var Xgt=s(g7e);I0r=r(Xgt,"Examples:"),Xgt.forEach(t),j0r=i(Aa),m(J6.$$.fragment,Aa),Aa.forEach(t),di.forEach(t),aBe=i(d),of=n(d,"H2",{class:!0});var gxe=s(of);vC=n(gxe,"A",{id:!0,class:!0,href:!0});var zgt=s(vC);h7e=n(zgt,"SPAN",{});var Vgt=s(h7e);m(Y6.$$.fragment,Vgt),Vgt.forEach(t),zgt.forEach(t),N0r=i(gxe),p7e=n(gxe,"SPAN",{});var Wgt=s(p7e);D0r=r(Wgt,"FlaxAutoModelForSequenceClassification"),Wgt.forEach(t),gxe.forEach(t),nBe=i(d),xr=n(d,"DIV",{class:!0});var fi=s(xr);m(K6.$$.fragment,fi),q0r=i(fi),rf=n(fi,"P",{});var BV=s(rf);G0r=r(BV,`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence classification head) when created
with the `),_7e=n(BV,"CODE",{});var Qgt=s(_7e);O0r=r(Qgt,"from_pretrained()"),Qgt.forEach(t),X0r=r(BV,"class method or the "),u7e=n(BV,"CODE",{});var Hgt=s(u7e);z0r=r(Hgt,"from_config()"),Hgt.forEach(t),V0r=r(BV,`class
method.`),BV.forEach(t),W0r=i(fi),Z6=n(fi,"P",{});var hxe=s(Z6);Q0r=r(hxe,"This class cannot be instantiated directly using "),b7e=n(hxe,"CODE",{});var Ugt=s(b7e);H0r=r(Ugt,"__init__()"),Ugt.forEach(t),U0r=r(hxe," (throws an error)."),hxe.forEach(t),J0r=i(fi),Et=n(fi,"DIV",{class:!0});var mi=s(Et);m(eA.$$.fragment,mi),Y0r=i(mi),v7e=n(mi,"P",{});var Jgt=s(v7e);K0r=r(Jgt,"Instantiates one of the model classes of the library (with a sequence classification head) from a configuration."),Jgt.forEach(t),Z0r=i(mi),tf=n(mi,"P",{});var kV=s(tf);eTr=r(kV,`Note:
Loading a model from its configuration file does `),T7e=n(kV,"STRONG",{});var Ygt=s(T7e);oTr=r(Ygt,"not"),Ygt.forEach(t),rTr=r(kV,` load the model weights. It only affects the
model\u2019s configuration. Use `),F7e=n(kV,"CODE",{});var Kgt=s(F7e);tTr=r(Kgt,"from_pretrained()"),Kgt.forEach(t),aTr=r(kV,"to load the model weights."),kV.forEach(t),nTr=i(mi),C7e=n(mi,"P",{});var Zgt=s(C7e);sTr=r(Zgt,"Examples:"),Zgt.forEach(t),lTr=i(mi),m(oA.$$.fragment,mi),mi.forEach(t),iTr=i(fi),xo=n(fi,"DIV",{class:!0});var La=s(xo);m(rA.$$.fragment,La),dTr=i(La),M7e=n(La,"P",{});var eht=s(M7e);cTr=r(eht,"Instantiate one of the model classes of the library (with a sequence classification head) from a pretrained model."),eht.forEach(t),fTr=i(La),An=n(La,"P",{});var K4=s(An);mTr=r(K4,"The model class to instantiate is selected based on the "),E7e=n(K4,"CODE",{});var oht=s(E7e);gTr=r(oht,"model_type"),oht.forEach(t),hTr=r(K4,` property of the config object (either
passed as an argument or loaded from `),y7e=n(K4,"CODE",{});var rht=s(y7e);pTr=r(rht,"pretrained_model_name_or_path"),rht.forEach(t),_Tr=r(K4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),w7e=n(K4,"CODE",{});var tht=s(w7e);uTr=r(tht,"pretrained_model_name_or_path"),tht.forEach(t),bTr=r(K4,":"),K4.forEach(t),vTr=i(La),Fe=n(La,"UL",{});var oo=s(Fe);TC=n(oo,"LI",{});var jLe=s(TC);A7e=n(jLe,"STRONG",{});var aht=s(A7e);TTr=r(aht,"albert"),aht.forEach(t),FTr=r(jLe," \u2014 "),pX=n(jLe,"A",{href:!0});var nht=s(pX);CTr=r(nht,"FlaxAlbertForSequenceClassification"),nht.forEach(t),MTr=r(jLe," (ALBERT model)"),jLe.forEach(t),ETr=i(oo),FC=n(oo,"LI",{});var NLe=s(FC);L7e=n(NLe,"STRONG",{});var sht=s(L7e);yTr=r(sht,"bart"),sht.forEach(t),wTr=r(NLe," \u2014 "),_X=n(NLe,"A",{href:!0});var lht=s(_X);ATr=r(lht,"FlaxBartForSequenceClassification"),lht.forEach(t),LTr=r(NLe," (BART model)"),NLe.forEach(t),BTr=i(oo),CC=n(oo,"LI",{});var DLe=s(CC);B7e=n(DLe,"STRONG",{});var iht=s(B7e);kTr=r(iht,"bert"),iht.forEach(t),xTr=r(DLe," \u2014 "),uX=n(DLe,"A",{href:!0});var dht=s(uX);RTr=r(dht,"FlaxBertForSequenceClassification"),dht.forEach(t),STr=r(DLe," (BERT model)"),DLe.forEach(t),PTr=i(oo),MC=n(oo,"LI",{});var qLe=s(MC);k7e=n(qLe,"STRONG",{});var cht=s(k7e);$Tr=r(cht,"big_bird"),cht.forEach(t),ITr=r(qLe," \u2014 "),bX=n(qLe,"A",{href:!0});var fht=s(bX);jTr=r(fht,"FlaxBigBirdForSequenceClassification"),fht.forEach(t),NTr=r(qLe," (BigBird model)"),qLe.forEach(t),DTr=i(oo),EC=n(oo,"LI",{});var GLe=s(EC);x7e=n(GLe,"STRONG",{});var mht=s(x7e);qTr=r(mht,"distilbert"),mht.forEach(t),GTr=r(GLe," \u2014 "),vX=n(GLe,"A",{href:!0});var ght=s(vX);OTr=r(ght,"FlaxDistilBertForSequenceClassification"),ght.forEach(t),XTr=r(GLe," (DistilBERT model)"),GLe.forEach(t),zTr=i(oo),yC=n(oo,"LI",{});var OLe=s(yC);R7e=n(OLe,"STRONG",{});var hht=s(R7e);VTr=r(hht,"electra"),hht.forEach(t),WTr=r(OLe," \u2014 "),TX=n(OLe,"A",{href:!0});var pht=s(TX);QTr=r(pht,"FlaxElectraForSequenceClassification"),pht.forEach(t),HTr=r(OLe," (ELECTRA model)"),OLe.forEach(t),UTr=i(oo),wC=n(oo,"LI",{});var XLe=s(wC);S7e=n(XLe,"STRONG",{});var _ht=s(S7e);JTr=r(_ht,"mbart"),_ht.forEach(t),YTr=r(XLe," \u2014 "),FX=n(XLe,"A",{href:!0});var uht=s(FX);KTr=r(uht,"FlaxMBartForSequenceClassification"),uht.forEach(t),ZTr=r(XLe," (mBART model)"),XLe.forEach(t),eFr=i(oo),AC=n(oo,"LI",{});var zLe=s(AC);P7e=n(zLe,"STRONG",{});var bht=s(P7e);oFr=r(bht,"roberta"),bht.forEach(t),rFr=r(zLe," \u2014 "),CX=n(zLe,"A",{href:!0});var vht=s(CX);tFr=r(vht,"FlaxRobertaForSequenceClassification"),vht.forEach(t),aFr=r(zLe," (RoBERTa model)"),zLe.forEach(t),nFr=i(oo),LC=n(oo,"LI",{});var VLe=s(LC);$7e=n(VLe,"STRONG",{});var Tht=s($7e);sFr=r(Tht,"roformer"),Tht.forEach(t),lFr=r(VLe," \u2014 "),MX=n(VLe,"A",{href:!0});var Fht=s(MX);iFr=r(Fht,"FlaxRoFormerForSequenceClassification"),Fht.forEach(t),dFr=r(VLe," (RoFormer model)"),VLe.forEach(t),oo.forEach(t),cFr=i(La),I7e=n(La,"P",{});var Cht=s(I7e);fFr=r(Cht,"Examples:"),Cht.forEach(t),mFr=i(La),m(tA.$$.fragment,La),La.forEach(t),fi.forEach(t),sBe=i(d),af=n(d,"H2",{class:!0});var pxe=s(af);BC=n(pxe,"A",{id:!0,class:!0,href:!0});var Mht=s(BC);j7e=n(Mht,"SPAN",{});var Eht=s(j7e);m(aA.$$.fragment,Eht),Eht.forEach(t),Mht.forEach(t),gFr=i(pxe),N7e=n(pxe,"SPAN",{});var yht=s(N7e);hFr=r(yht,"FlaxAutoModelForQuestionAnswering"),yht.forEach(t),pxe.forEach(t),lBe=i(d),Rr=n(d,"DIV",{class:!0});var gi=s(Rr);m(nA.$$.fragment,gi),pFr=i(gi),nf=n(gi,"P",{});var xV=s(nf);_Fr=r(xV,`This is a generic model class that will be instantiated as one of the model classes of the library (with a question answering head) when created
with the `),D7e=n(xV,"CODE",{});var wht=s(D7e);uFr=r(wht,"from_pretrained()"),wht.forEach(t),bFr=r(xV,"class method or the "),q7e=n(xV,"CODE",{});var Aht=s(q7e);vFr=r(Aht,"from_config()"),Aht.forEach(t),TFr=r(xV,`class
method.`),xV.forEach(t),FFr=i(gi),sA=n(gi,"P",{});var _xe=s(sA);CFr=r(_xe,"This class cannot be instantiated directly using "),G7e=n(_xe,"CODE",{});var Lht=s(G7e);MFr=r(Lht,"__init__()"),Lht.forEach(t),EFr=r(_xe," (throws an error)."),_xe.forEach(t),yFr=i(gi),yt=n(gi,"DIV",{class:!0});var hi=s(yt);m(lA.$$.fragment,hi),wFr=i(hi),O7e=n(hi,"P",{});var Bht=s(O7e);AFr=r(Bht,"Instantiates one of the model classes of the library (with a question answering head) from a configuration."),Bht.forEach(t),LFr=i(hi),sf=n(hi,"P",{});var RV=s(sf);BFr=r(RV,`Note:
Loading a model from its configuration file does `),X7e=n(RV,"STRONG",{});var kht=s(X7e);kFr=r(kht,"not"),kht.forEach(t),xFr=r(RV,` load the model weights. It only affects the
model\u2019s configuration. Use `),z7e=n(RV,"CODE",{});var xht=s(z7e);RFr=r(xht,"from_pretrained()"),xht.forEach(t),SFr=r(RV,"to load the model weights."),RV.forEach(t),PFr=i(hi),V7e=n(hi,"P",{});var Rht=s(V7e);$Fr=r(Rht,"Examples:"),Rht.forEach(t),IFr=i(hi),m(iA.$$.fragment,hi),hi.forEach(t),jFr=i(gi),Ro=n(gi,"DIV",{class:!0});var Ba=s(Ro);m(dA.$$.fragment,Ba),NFr=i(Ba),W7e=n(Ba,"P",{});var Sht=s(W7e);DFr=r(Sht,"Instantiate one of the model classes of the library (with a question answering head) from a pretrained model."),Sht.forEach(t),qFr=i(Ba),Ln=n(Ba,"P",{});var Z4=s(Ln);GFr=r(Z4,"The model class to instantiate is selected based on the "),Q7e=n(Z4,"CODE",{});var Pht=s(Q7e);OFr=r(Pht,"model_type"),Pht.forEach(t),XFr=r(Z4,` property of the config object (either
passed as an argument or loaded from `),H7e=n(Z4,"CODE",{});var $ht=s(H7e);zFr=r($ht,"pretrained_model_name_or_path"),$ht.forEach(t),VFr=r(Z4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),U7e=n(Z4,"CODE",{});var Iht=s(U7e);WFr=r(Iht,"pretrained_model_name_or_path"),Iht.forEach(t),QFr=r(Z4,":"),Z4.forEach(t),HFr=i(Ba),Ce=n(Ba,"UL",{});var ro=s(Ce);kC=n(ro,"LI",{});var WLe=s(kC);J7e=n(WLe,"STRONG",{});var jht=s(J7e);UFr=r(jht,"albert"),jht.forEach(t),JFr=r(WLe," \u2014 "),EX=n(WLe,"A",{href:!0});var Nht=s(EX);YFr=r(Nht,"FlaxAlbertForQuestionAnswering"),Nht.forEach(t),KFr=r(WLe," (ALBERT model)"),WLe.forEach(t),ZFr=i(ro),xC=n(ro,"LI",{});var QLe=s(xC);Y7e=n(QLe,"STRONG",{});var Dht=s(Y7e);eCr=r(Dht,"bart"),Dht.forEach(t),oCr=r(QLe," \u2014 "),yX=n(QLe,"A",{href:!0});var qht=s(yX);rCr=r(qht,"FlaxBartForQuestionAnswering"),qht.forEach(t),tCr=r(QLe," (BART model)"),QLe.forEach(t),aCr=i(ro),RC=n(ro,"LI",{});var HLe=s(RC);K7e=n(HLe,"STRONG",{});var Ght=s(K7e);nCr=r(Ght,"bert"),Ght.forEach(t),sCr=r(HLe," \u2014 "),wX=n(HLe,"A",{href:!0});var Oht=s(wX);lCr=r(Oht,"FlaxBertForQuestionAnswering"),Oht.forEach(t),iCr=r(HLe," (BERT model)"),HLe.forEach(t),dCr=i(ro),SC=n(ro,"LI",{});var ULe=s(SC);Z7e=n(ULe,"STRONG",{});var Xht=s(Z7e);cCr=r(Xht,"big_bird"),Xht.forEach(t),fCr=r(ULe," \u2014 "),AX=n(ULe,"A",{href:!0});var zht=s(AX);mCr=r(zht,"FlaxBigBirdForQuestionAnswering"),zht.forEach(t),gCr=r(ULe," (BigBird model)"),ULe.forEach(t),hCr=i(ro),PC=n(ro,"LI",{});var JLe=s(PC);ebe=n(JLe,"STRONG",{});var Vht=s(ebe);pCr=r(Vht,"distilbert"),Vht.forEach(t),_Cr=r(JLe," \u2014 "),LX=n(JLe,"A",{href:!0});var Wht=s(LX);uCr=r(Wht,"FlaxDistilBertForQuestionAnswering"),Wht.forEach(t),bCr=r(JLe," (DistilBERT model)"),JLe.forEach(t),vCr=i(ro),$C=n(ro,"LI",{});var YLe=s($C);obe=n(YLe,"STRONG",{});var Qht=s(obe);TCr=r(Qht,"electra"),Qht.forEach(t),FCr=r(YLe," \u2014 "),BX=n(YLe,"A",{href:!0});var Hht=s(BX);CCr=r(Hht,"FlaxElectraForQuestionAnswering"),Hht.forEach(t),MCr=r(YLe," (ELECTRA model)"),YLe.forEach(t),ECr=i(ro),IC=n(ro,"LI",{});var KLe=s(IC);rbe=n(KLe,"STRONG",{});var Uht=s(rbe);yCr=r(Uht,"mbart"),Uht.forEach(t),wCr=r(KLe," \u2014 "),kX=n(KLe,"A",{href:!0});var Jht=s(kX);ACr=r(Jht,"FlaxMBartForQuestionAnswering"),Jht.forEach(t),LCr=r(KLe," (mBART model)"),KLe.forEach(t),BCr=i(ro),jC=n(ro,"LI",{});var ZLe=s(jC);tbe=n(ZLe,"STRONG",{});var Yht=s(tbe);kCr=r(Yht,"roberta"),Yht.forEach(t),xCr=r(ZLe," \u2014 "),xX=n(ZLe,"A",{href:!0});var Kht=s(xX);RCr=r(Kht,"FlaxRobertaForQuestionAnswering"),Kht.forEach(t),SCr=r(ZLe," (RoBERTa model)"),ZLe.forEach(t),PCr=i(ro),NC=n(ro,"LI",{});var e8e=s(NC);abe=n(e8e,"STRONG",{});var Zht=s(abe);$Cr=r(Zht,"roformer"),Zht.forEach(t),ICr=r(e8e," \u2014 "),RX=n(e8e,"A",{href:!0});var ept=s(RX);jCr=r(ept,"FlaxRoFormerForQuestionAnswering"),ept.forEach(t),NCr=r(e8e," (RoFormer model)"),e8e.forEach(t),ro.forEach(t),DCr=i(Ba),nbe=n(Ba,"P",{});var opt=s(nbe);qCr=r(opt,"Examples:"),opt.forEach(t),GCr=i(Ba),m(cA.$$.fragment,Ba),Ba.forEach(t),gi.forEach(t),iBe=i(d),lf=n(d,"H2",{class:!0});var uxe=s(lf);DC=n(uxe,"A",{id:!0,class:!0,href:!0});var rpt=s(DC);sbe=n(rpt,"SPAN",{});var tpt=s(sbe);m(fA.$$.fragment,tpt),tpt.forEach(t),rpt.forEach(t),OCr=i(uxe),lbe=n(uxe,"SPAN",{});var apt=s(lbe);XCr=r(apt,"FlaxAutoModelForTokenClassification"),apt.forEach(t),uxe.forEach(t),dBe=i(d),Sr=n(d,"DIV",{class:!0});var pi=s(Sr);m(mA.$$.fragment,pi),zCr=i(pi),df=n(pi,"P",{});var SV=s(df);VCr=r(SV,`This is a generic model class that will be instantiated as one of the model classes of the library (with a token classification head) when created
with the `),ibe=n(SV,"CODE",{});var npt=s(ibe);WCr=r(npt,"from_pretrained()"),npt.forEach(t),QCr=r(SV,"class method or the "),dbe=n(SV,"CODE",{});var spt=s(dbe);HCr=r(spt,"from_config()"),spt.forEach(t),UCr=r(SV,`class
method.`),SV.forEach(t),JCr=i(pi),gA=n(pi,"P",{});var bxe=s(gA);YCr=r(bxe,"This class cannot be instantiated directly using "),cbe=n(bxe,"CODE",{});var lpt=s(cbe);KCr=r(lpt,"__init__()"),lpt.forEach(t),ZCr=r(bxe," (throws an error)."),bxe.forEach(t),e4r=i(pi),wt=n(pi,"DIV",{class:!0});var _i=s(wt);m(hA.$$.fragment,_i),o4r=i(_i),fbe=n(_i,"P",{});var ipt=s(fbe);r4r=r(ipt,"Instantiates one of the model classes of the library (with a token classification head) from a configuration."),ipt.forEach(t),t4r=i(_i),cf=n(_i,"P",{});var PV=s(cf);a4r=r(PV,`Note:
Loading a model from its configuration file does `),mbe=n(PV,"STRONG",{});var dpt=s(mbe);n4r=r(dpt,"not"),dpt.forEach(t),s4r=r(PV,` load the model weights. It only affects the
model\u2019s configuration. Use `),gbe=n(PV,"CODE",{});var cpt=s(gbe);l4r=r(cpt,"from_pretrained()"),cpt.forEach(t),i4r=r(PV,"to load the model weights."),PV.forEach(t),d4r=i(_i),hbe=n(_i,"P",{});var fpt=s(hbe);c4r=r(fpt,"Examples:"),fpt.forEach(t),f4r=i(_i),m(pA.$$.fragment,_i),_i.forEach(t),m4r=i(pi),So=n(pi,"DIV",{class:!0});var ka=s(So);m(_A.$$.fragment,ka),g4r=i(ka),pbe=n(ka,"P",{});var mpt=s(pbe);h4r=r(mpt,"Instantiate one of the model classes of the library (with a token classification head) from a pretrained model."),mpt.forEach(t),p4r=i(ka),Bn=n(ka,"P",{});var eM=s(Bn);_4r=r(eM,"The model class to instantiate is selected based on the "),_be=n(eM,"CODE",{});var gpt=s(_be);u4r=r(gpt,"model_type"),gpt.forEach(t),b4r=r(eM,` property of the config object (either
passed as an argument or loaded from `),ube=n(eM,"CODE",{});var hpt=s(ube);v4r=r(hpt,"pretrained_model_name_or_path"),hpt.forEach(t),T4r=r(eM,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),bbe=n(eM,"CODE",{});var ppt=s(bbe);F4r=r(ppt,"pretrained_model_name_or_path"),ppt.forEach(t),C4r=r(eM,":"),eM.forEach(t),M4r=i(ka),so=n(ka,"UL",{});var ta=s(so);qC=n(ta,"LI",{});var o8e=s(qC);vbe=n(o8e,"STRONG",{});var _pt=s(vbe);E4r=r(_pt,"albert"),_pt.forEach(t),y4r=r(o8e," \u2014 "),SX=n(o8e,"A",{href:!0});var upt=s(SX);w4r=r(upt,"FlaxAlbertForTokenClassification"),upt.forEach(t),A4r=r(o8e," (ALBERT model)"),o8e.forEach(t),L4r=i(ta),GC=n(ta,"LI",{});var r8e=s(GC);Tbe=n(r8e,"STRONG",{});var bpt=s(Tbe);B4r=r(bpt,"bert"),bpt.forEach(t),k4r=r(r8e," \u2014 "),PX=n(r8e,"A",{href:!0});var vpt=s(PX);x4r=r(vpt,"FlaxBertForTokenClassification"),vpt.forEach(t),R4r=r(r8e," (BERT model)"),r8e.forEach(t),S4r=i(ta),OC=n(ta,"LI",{});var t8e=s(OC);Fbe=n(t8e,"STRONG",{});var Tpt=s(Fbe);P4r=r(Tpt,"big_bird"),Tpt.forEach(t),$4r=r(t8e," \u2014 "),$X=n(t8e,"A",{href:!0});var Fpt=s($X);I4r=r(Fpt,"FlaxBigBirdForTokenClassification"),Fpt.forEach(t),j4r=r(t8e," (BigBird model)"),t8e.forEach(t),N4r=i(ta),XC=n(ta,"LI",{});var a8e=s(XC);Cbe=n(a8e,"STRONG",{});var Cpt=s(Cbe);D4r=r(Cpt,"distilbert"),Cpt.forEach(t),q4r=r(a8e," \u2014 "),IX=n(a8e,"A",{href:!0});var Mpt=s(IX);G4r=r(Mpt,"FlaxDistilBertForTokenClassification"),Mpt.forEach(t),O4r=r(a8e," (DistilBERT model)"),a8e.forEach(t),X4r=i(ta),zC=n(ta,"LI",{});var n8e=s(zC);Mbe=n(n8e,"STRONG",{});var Ept=s(Mbe);z4r=r(Ept,"electra"),Ept.forEach(t),V4r=r(n8e," \u2014 "),jX=n(n8e,"A",{href:!0});var ypt=s(jX);W4r=r(ypt,"FlaxElectraForTokenClassification"),ypt.forEach(t),Q4r=r(n8e," (ELECTRA model)"),n8e.forEach(t),H4r=i(ta),VC=n(ta,"LI",{});var s8e=s(VC);Ebe=n(s8e,"STRONG",{});var wpt=s(Ebe);U4r=r(wpt,"roberta"),wpt.forEach(t),J4r=r(s8e," \u2014 "),NX=n(s8e,"A",{href:!0});var Apt=s(NX);Y4r=r(Apt,"FlaxRobertaForTokenClassification"),Apt.forEach(t),K4r=r(s8e," (RoBERTa model)"),s8e.forEach(t),Z4r=i(ta),WC=n(ta,"LI",{});var l8e=s(WC);ybe=n(l8e,"STRONG",{});var Lpt=s(ybe);eMr=r(Lpt,"roformer"),Lpt.forEach(t),oMr=r(l8e," \u2014 "),DX=n(l8e,"A",{href:!0});var Bpt=s(DX);rMr=r(Bpt,"FlaxRoFormerForTokenClassification"),Bpt.forEach(t),tMr=r(l8e," (RoFormer model)"),l8e.forEach(t),ta.forEach(t),aMr=i(ka),wbe=n(ka,"P",{});var kpt=s(wbe);nMr=r(kpt,"Examples:"),kpt.forEach(t),sMr=i(ka),m(uA.$$.fragment,ka),ka.forEach(t),pi.forEach(t),cBe=i(d),ff=n(d,"H2",{class:!0});var vxe=s(ff);QC=n(vxe,"A",{id:!0,class:!0,href:!0});var xpt=s(QC);Abe=n(xpt,"SPAN",{});var Rpt=s(Abe);m(bA.$$.fragment,Rpt),Rpt.forEach(t),xpt.forEach(t),lMr=i(vxe),Lbe=n(vxe,"SPAN",{});var Spt=s(Lbe);iMr=r(Spt,"FlaxAutoModelForMultipleChoice"),Spt.forEach(t),vxe.forEach(t),fBe=i(d),Pr=n(d,"DIV",{class:!0});var ui=s(Pr);m(vA.$$.fragment,ui),dMr=i(ui),mf=n(ui,"P",{});var $V=s(mf);cMr=r($V,`This is a generic model class that will be instantiated as one of the model classes of the library (with a multiple choice head) when created
with the `),Bbe=n($V,"CODE",{});var Ppt=s(Bbe);fMr=r(Ppt,"from_pretrained()"),Ppt.forEach(t),mMr=r($V,"class method or the "),kbe=n($V,"CODE",{});var $pt=s(kbe);gMr=r($pt,"from_config()"),$pt.forEach(t),hMr=r($V,`class
method.`),$V.forEach(t),pMr=i(ui),TA=n(ui,"P",{});var Txe=s(TA);_Mr=r(Txe,"This class cannot be instantiated directly using "),xbe=n(Txe,"CODE",{});var Ipt=s(xbe);uMr=r(Ipt,"__init__()"),Ipt.forEach(t),bMr=r(Txe," (throws an error)."),Txe.forEach(t),vMr=i(ui),At=n(ui,"DIV",{class:!0});var bi=s(At);m(FA.$$.fragment,bi),TMr=i(bi),Rbe=n(bi,"P",{});var jpt=s(Rbe);FMr=r(jpt,"Instantiates one of the model classes of the library (with a multiple choice head) from a configuration."),jpt.forEach(t),CMr=i(bi),gf=n(bi,"P",{});var IV=s(gf);MMr=r(IV,`Note:
Loading a model from its configuration file does `),Sbe=n(IV,"STRONG",{});var Npt=s(Sbe);EMr=r(Npt,"not"),Npt.forEach(t),yMr=r(IV,` load the model weights. It only affects the
model\u2019s configuration. Use `),Pbe=n(IV,"CODE",{});var Dpt=s(Pbe);wMr=r(Dpt,"from_pretrained()"),Dpt.forEach(t),AMr=r(IV,"to load the model weights."),IV.forEach(t),LMr=i(bi),$be=n(bi,"P",{});var qpt=s($be);BMr=r(qpt,"Examples:"),qpt.forEach(t),kMr=i(bi),m(CA.$$.fragment,bi),bi.forEach(t),xMr=i(ui),Po=n(ui,"DIV",{class:!0});var xa=s(Po);m(MA.$$.fragment,xa),RMr=i(xa),Ibe=n(xa,"P",{});var Gpt=s(Ibe);SMr=r(Gpt,"Instantiate one of the model classes of the library (with a multiple choice head) from a pretrained model."),Gpt.forEach(t),PMr=i(xa),kn=n(xa,"P",{});var oM=s(kn);$Mr=r(oM,"The model class to instantiate is selected based on the "),jbe=n(oM,"CODE",{});var Opt=s(jbe);IMr=r(Opt,"model_type"),Opt.forEach(t),jMr=r(oM,` property of the config object (either
passed as an argument or loaded from `),Nbe=n(oM,"CODE",{});var Xpt=s(Nbe);NMr=r(Xpt,"pretrained_model_name_or_path"),Xpt.forEach(t),DMr=r(oM,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Dbe=n(oM,"CODE",{});var zpt=s(Dbe);qMr=r(zpt,"pretrained_model_name_or_path"),zpt.forEach(t),GMr=r(oM,":"),oM.forEach(t),OMr=i(xa),lo=n(xa,"UL",{});var aa=s(lo);HC=n(aa,"LI",{});var i8e=s(HC);qbe=n(i8e,"STRONG",{});var Vpt=s(qbe);XMr=r(Vpt,"albert"),Vpt.forEach(t),zMr=r(i8e," \u2014 "),qX=n(i8e,"A",{href:!0});var Wpt=s(qX);VMr=r(Wpt,"FlaxAlbertForMultipleChoice"),Wpt.forEach(t),WMr=r(i8e," (ALBERT model)"),i8e.forEach(t),QMr=i(aa),UC=n(aa,"LI",{});var d8e=s(UC);Gbe=n(d8e,"STRONG",{});var Qpt=s(Gbe);HMr=r(Qpt,"bert"),Qpt.forEach(t),UMr=r(d8e," \u2014 "),GX=n(d8e,"A",{href:!0});var Hpt=s(GX);JMr=r(Hpt,"FlaxBertForMultipleChoice"),Hpt.forEach(t),YMr=r(d8e," (BERT model)"),d8e.forEach(t),KMr=i(aa),JC=n(aa,"LI",{});var c8e=s(JC);Obe=n(c8e,"STRONG",{});var Upt=s(Obe);ZMr=r(Upt,"big_bird"),Upt.forEach(t),eEr=r(c8e," \u2014 "),OX=n(c8e,"A",{href:!0});var Jpt=s(OX);oEr=r(Jpt,"FlaxBigBirdForMultipleChoice"),Jpt.forEach(t),rEr=r(c8e," (BigBird model)"),c8e.forEach(t),tEr=i(aa),YC=n(aa,"LI",{});var f8e=s(YC);Xbe=n(f8e,"STRONG",{});var Ypt=s(Xbe);aEr=r(Ypt,"distilbert"),Ypt.forEach(t),nEr=r(f8e," \u2014 "),XX=n(f8e,"A",{href:!0});var Kpt=s(XX);sEr=r(Kpt,"FlaxDistilBertForMultipleChoice"),Kpt.forEach(t),lEr=r(f8e," (DistilBERT model)"),f8e.forEach(t),iEr=i(aa),KC=n(aa,"LI",{});var m8e=s(KC);zbe=n(m8e,"STRONG",{});var Zpt=s(zbe);dEr=r(Zpt,"electra"),Zpt.forEach(t),cEr=r(m8e," \u2014 "),zX=n(m8e,"A",{href:!0});var e_t=s(zX);fEr=r(e_t,"FlaxElectraForMultipleChoice"),e_t.forEach(t),mEr=r(m8e," (ELECTRA model)"),m8e.forEach(t),gEr=i(aa),ZC=n(aa,"LI",{});var g8e=s(ZC);Vbe=n(g8e,"STRONG",{});var o_t=s(Vbe);hEr=r(o_t,"roberta"),o_t.forEach(t),pEr=r(g8e," \u2014 "),VX=n(g8e,"A",{href:!0});var r_t=s(VX);_Er=r(r_t,"FlaxRobertaForMultipleChoice"),r_t.forEach(t),uEr=r(g8e," (RoBERTa model)"),g8e.forEach(t),bEr=i(aa),e4=n(aa,"LI",{});var h8e=s(e4);Wbe=n(h8e,"STRONG",{});var t_t=s(Wbe);vEr=r(t_t,"roformer"),t_t.forEach(t),TEr=r(h8e," \u2014 "),WX=n(h8e,"A",{href:!0});var a_t=s(WX);FEr=r(a_t,"FlaxRoFormerForMultipleChoice"),a_t.forEach(t),CEr=r(h8e," (RoFormer model)"),h8e.forEach(t),aa.forEach(t),MEr=i(xa),Qbe=n(xa,"P",{});var n_t=s(Qbe);EEr=r(n_t,"Examples:"),n_t.forEach(t),yEr=i(xa),m(EA.$$.fragment,xa),xa.forEach(t),ui.forEach(t),mBe=i(d),hf=n(d,"H2",{class:!0});var Fxe=s(hf);o4=n(Fxe,"A",{id:!0,class:!0,href:!0});var s_t=s(o4);Hbe=n(s_t,"SPAN",{});var l_t=s(Hbe);m(yA.$$.fragment,l_t),l_t.forEach(t),s_t.forEach(t),wEr=i(Fxe),Ube=n(Fxe,"SPAN",{});var i_t=s(Ube);AEr=r(i_t,"FlaxAutoModelForNextSentencePrediction"),i_t.forEach(t),Fxe.forEach(t),gBe=i(d),$r=n(d,"DIV",{class:!0});var vi=s($r);m(wA.$$.fragment,vi),LEr=i(vi),pf=n(vi,"P",{});var jV=s(pf);BEr=r(jV,`This is a generic model class that will be instantiated as one of the model classes of the library (with a next sentence prediction head) when created
with the `),Jbe=n(jV,"CODE",{});var d_t=s(Jbe);kEr=r(d_t,"from_pretrained()"),d_t.forEach(t),xEr=r(jV,"class method or the "),Ybe=n(jV,"CODE",{});var c_t=s(Ybe);REr=r(c_t,"from_config()"),c_t.forEach(t),SEr=r(jV,`class
method.`),jV.forEach(t),PEr=i(vi),AA=n(vi,"P",{});var Cxe=s(AA);$Er=r(Cxe,"This class cannot be instantiated directly using "),Kbe=n(Cxe,"CODE",{});var f_t=s(Kbe);IEr=r(f_t,"__init__()"),f_t.forEach(t),jEr=r(Cxe," (throws an error)."),Cxe.forEach(t),NEr=i(vi),Lt=n(vi,"DIV",{class:!0});var Ti=s(Lt);m(LA.$$.fragment,Ti),DEr=i(Ti),Zbe=n(Ti,"P",{});var m_t=s(Zbe);qEr=r(m_t,"Instantiates one of the model classes of the library (with a next sentence prediction head) from a configuration."),m_t.forEach(t),GEr=i(Ti),_f=n(Ti,"P",{});var NV=s(_f);OEr=r(NV,`Note:
Loading a model from its configuration file does `),e5e=n(NV,"STRONG",{});var g_t=s(e5e);XEr=r(g_t,"not"),g_t.forEach(t),zEr=r(NV,` load the model weights. It only affects the
model\u2019s configuration. Use `),o5e=n(NV,"CODE",{});var h_t=s(o5e);VEr=r(h_t,"from_pretrained()"),h_t.forEach(t),WEr=r(NV,"to load the model weights."),NV.forEach(t),QEr=i(Ti),r5e=n(Ti,"P",{});var p_t=s(r5e);HEr=r(p_t,"Examples:"),p_t.forEach(t),UEr=i(Ti),m(BA.$$.fragment,Ti),Ti.forEach(t),JEr=i(vi),$o=n(vi,"DIV",{class:!0});var Ra=s($o);m(kA.$$.fragment,Ra),YEr=i(Ra),t5e=n(Ra,"P",{});var __t=s(t5e);KEr=r(__t,"Instantiate one of the model classes of the library (with a next sentence prediction head) from a pretrained model."),__t.forEach(t),ZEr=i(Ra),xn=n(Ra,"P",{});var rM=s(xn);e3r=r(rM,"The model class to instantiate is selected based on the "),a5e=n(rM,"CODE",{});var u_t=s(a5e);o3r=r(u_t,"model_type"),u_t.forEach(t),r3r=r(rM,` property of the config object (either
passed as an argument or loaded from `),n5e=n(rM,"CODE",{});var b_t=s(n5e);t3r=r(b_t,"pretrained_model_name_or_path"),b_t.forEach(t),a3r=r(rM,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),s5e=n(rM,"CODE",{});var v_t=s(s5e);n3r=r(v_t,"pretrained_model_name_or_path"),v_t.forEach(t),s3r=r(rM,":"),rM.forEach(t),l3r=i(Ra),l5e=n(Ra,"UL",{});var T_t=s(l5e);r4=n(T_t,"LI",{});var p8e=s(r4);i5e=n(p8e,"STRONG",{});var F_t=s(i5e);i3r=r(F_t,"bert"),F_t.forEach(t),d3r=r(p8e," \u2014 "),QX=n(p8e,"A",{href:!0});var C_t=s(QX);c3r=r(C_t,"FlaxBertForNextSentencePrediction"),C_t.forEach(t),f3r=r(p8e," (BERT model)"),p8e.forEach(t),T_t.forEach(t),m3r=i(Ra),d5e=n(Ra,"P",{});var M_t=s(d5e);g3r=r(M_t,"Examples:"),M_t.forEach(t),h3r=i(Ra),m(xA.$$.fragment,Ra),Ra.forEach(t),vi.forEach(t),hBe=i(d),uf=n(d,"H2",{class:!0});var Mxe=s(uf);t4=n(Mxe,"A",{id:!0,class:!0,href:!0});var E_t=s(t4);c5e=n(E_t,"SPAN",{});var y_t=s(c5e);m(RA.$$.fragment,y_t),y_t.forEach(t),E_t.forEach(t),p3r=i(Mxe),f5e=n(Mxe,"SPAN",{});var w_t=s(f5e);_3r=r(w_t,"FlaxAutoModelForImageClassification"),w_t.forEach(t),Mxe.forEach(t),pBe=i(d),Ir=n(d,"DIV",{class:!0});var Fi=s(Ir);m(SA.$$.fragment,Fi),u3r=i(Fi),bf=n(Fi,"P",{});var DV=s(bf);b3r=r(DV,`This is a generic model class that will be instantiated as one of the model classes of the library (with a image classification head) when created
with the `),m5e=n(DV,"CODE",{});var A_t=s(m5e);v3r=r(A_t,"from_pretrained()"),A_t.forEach(t),T3r=r(DV,"class method or the "),g5e=n(DV,"CODE",{});var L_t=s(g5e);F3r=r(L_t,"from_config()"),L_t.forEach(t),C3r=r(DV,`class
method.`),DV.forEach(t),M3r=i(Fi),PA=n(Fi,"P",{});var Exe=s(PA);E3r=r(Exe,"This class cannot be instantiated directly using "),h5e=n(Exe,"CODE",{});var B_t=s(h5e);y3r=r(B_t,"__init__()"),B_t.forEach(t),w3r=r(Exe," (throws an error)."),Exe.forEach(t),A3r=i(Fi),Bt=n(Fi,"DIV",{class:!0});var Ci=s(Bt);m($A.$$.fragment,Ci),L3r=i(Ci),p5e=n(Ci,"P",{});var k_t=s(p5e);B3r=r(k_t,"Instantiates one of the model classes of the library (with a image classification head) from a configuration."),k_t.forEach(t),k3r=i(Ci),vf=n(Ci,"P",{});var qV=s(vf);x3r=r(qV,`Note:
Loading a model from its configuration file does `),_5e=n(qV,"STRONG",{});var x_t=s(_5e);R3r=r(x_t,"not"),x_t.forEach(t),S3r=r(qV,` load the model weights. It only affects the
model\u2019s configuration. Use `),u5e=n(qV,"CODE",{});var R_t=s(u5e);P3r=r(R_t,"from_pretrained()"),R_t.forEach(t),$3r=r(qV,"to load the model weights."),qV.forEach(t),I3r=i(Ci),b5e=n(Ci,"P",{});var S_t=s(b5e);j3r=r(S_t,"Examples:"),S_t.forEach(t),N3r=i(Ci),m(IA.$$.fragment,Ci),Ci.forEach(t),D3r=i(Fi),Io=n(Fi,"DIV",{class:!0});var Sa=s(Io);m(jA.$$.fragment,Sa),q3r=i(Sa),v5e=n(Sa,"P",{});var P_t=s(v5e);G3r=r(P_t,"Instantiate one of the model classes of the library (with a image classification head) from a pretrained model."),P_t.forEach(t),O3r=i(Sa),Rn=n(Sa,"P",{});var tM=s(Rn);X3r=r(tM,"The model class to instantiate is selected based on the "),T5e=n(tM,"CODE",{});var $_t=s(T5e);z3r=r($_t,"model_type"),$_t.forEach(t),V3r=r(tM,` property of the config object (either
passed as an argument or loaded from `),F5e=n(tM,"CODE",{});var I_t=s(F5e);W3r=r(I_t,"pretrained_model_name_or_path"),I_t.forEach(t),Q3r=r(tM,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),C5e=n(tM,"CODE",{});var j_t=s(C5e);H3r=r(j_t,"pretrained_model_name_or_path"),j_t.forEach(t),U3r=r(tM,":"),tM.forEach(t),J3r=i(Sa),NA=n(Sa,"UL",{});var yxe=s(NA);a4=n(yxe,"LI",{});var _8e=s(a4);M5e=n(_8e,"STRONG",{});var N_t=s(M5e);Y3r=r(N_t,"beit"),N_t.forEach(t),K3r=r(_8e," \u2014 "),HX=n(_8e,"A",{href:!0});var D_t=s(HX);Z3r=r(D_t,"FlaxBeitForImageClassification"),D_t.forEach(t),eyr=r(_8e," (BEiT model)"),_8e.forEach(t),oyr=i(yxe),n4=n(yxe,"LI",{});var u8e=s(n4);E5e=n(u8e,"STRONG",{});var q_t=s(E5e);ryr=r(q_t,"vit"),q_t.forEach(t),tyr=r(u8e," \u2014 "),UX=n(u8e,"A",{href:!0});var G_t=s(UX);ayr=r(G_t,"FlaxViTForImageClassification"),G_t.forEach(t),nyr=r(u8e," (ViT model)"),u8e.forEach(t),yxe.forEach(t),syr=i(Sa),y5e=n(Sa,"P",{});var O_t=s(y5e);lyr=r(O_t,"Examples:"),O_t.forEach(t),iyr=i(Sa),m(DA.$$.fragment,Sa),Sa.forEach(t),Fi.forEach(t),_Be=i(d),Tf=n(d,"H2",{class:!0});var wxe=s(Tf);s4=n(wxe,"A",{id:!0,class:!0,href:!0});var X_t=s(s4);w5e=n(X_t,"SPAN",{});var z_t=s(w5e);m(qA.$$.fragment,z_t),z_t.forEach(t),X_t.forEach(t),dyr=i(wxe),A5e=n(wxe,"SPAN",{});var V_t=s(A5e);cyr=r(V_t,"FlaxAutoModelForVision2Seq"),V_t.forEach(t),wxe.forEach(t),uBe=i(d),jr=n(d,"DIV",{class:!0});var Mi=s(jr);m(GA.$$.fragment,Mi),fyr=i(Mi),Ff=n(Mi,"P",{});var GV=s(Ff);myr=r(GV,`This is a generic model class that will be instantiated as one of the model classes of the library (with a vision-to-text modeling head) when created
with the `),L5e=n(GV,"CODE",{});var W_t=s(L5e);gyr=r(W_t,"from_pretrained()"),W_t.forEach(t),hyr=r(GV,"class method or the "),B5e=n(GV,"CODE",{});var Q_t=s(B5e);pyr=r(Q_t,"from_config()"),Q_t.forEach(t),_yr=r(GV,`class
method.`),GV.forEach(t),uyr=i(Mi),OA=n(Mi,"P",{});var Axe=s(OA);byr=r(Axe,"This class cannot be instantiated directly using "),k5e=n(Axe,"CODE",{});var H_t=s(k5e);vyr=r(H_t,"__init__()"),H_t.forEach(t),Tyr=r(Axe," (throws an error)."),Axe.forEach(t),Fyr=i(Mi),kt=n(Mi,"DIV",{class:!0});var Ei=s(kt);m(XA.$$.fragment,Ei),Cyr=i(Ei),x5e=n(Ei,"P",{});var U_t=s(x5e);Myr=r(U_t,"Instantiates one of the model classes of the library (with a vision-to-text modeling head) from a configuration."),U_t.forEach(t),Eyr=i(Ei),Cf=n(Ei,"P",{});var OV=s(Cf);yyr=r(OV,`Note:
Loading a model from its configuration file does `),R5e=n(OV,"STRONG",{});var J_t=s(R5e);wyr=r(J_t,"not"),J_t.forEach(t),Ayr=r(OV,` load the model weights. It only affects the
model\u2019s configuration. Use `),S5e=n(OV,"CODE",{});var Y_t=s(S5e);Lyr=r(Y_t,"from_pretrained()"),Y_t.forEach(t),Byr=r(OV,"to load the model weights."),OV.forEach(t),kyr=i(Ei),P5e=n(Ei,"P",{});var K_t=s(P5e);xyr=r(K_t,"Examples:"),K_t.forEach(t),Ryr=i(Ei),m(zA.$$.fragment,Ei),Ei.forEach(t),Syr=i(Mi),jo=n(Mi,"DIV",{class:!0});var Pa=s(jo);m(VA.$$.fragment,Pa),Pyr=i(Pa),$5e=n(Pa,"P",{});var Z_t=s($5e);$yr=r(Z_t,"Instantiate one of the model classes of the library (with a vision-to-text modeling head) from a pretrained model."),Z_t.forEach(t),Iyr=i(Pa),Sn=n(Pa,"P",{});var aM=s(Sn);jyr=r(aM,"The model class to instantiate is selected based on the "),I5e=n(aM,"CODE",{});var eut=s(I5e);Nyr=r(eut,"model_type"),eut.forEach(t),Dyr=r(aM,` property of the config object (either
passed as an argument or loaded from `),j5e=n(aM,"CODE",{});var out=s(j5e);qyr=r(out,"pretrained_model_name_or_path"),out.forEach(t),Gyr=r(aM,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),N5e=n(aM,"CODE",{});var rut=s(N5e);Oyr=r(rut,"pretrained_model_name_or_path"),rut.forEach(t),Xyr=r(aM,":"),aM.forEach(t),zyr=i(Pa),D5e=n(Pa,"UL",{});var tut=s(D5e);l4=n(tut,"LI",{});var b8e=s(l4);q5e=n(b8e,"STRONG",{});var aut=s(q5e);Vyr=r(aut,"vision-encoder-decoder"),aut.forEach(t),Wyr=r(b8e," \u2014 "),JX=n(b8e,"A",{href:!0});var nut=s(JX);Qyr=r(nut,"FlaxVisionEncoderDecoderModel"),nut.forEach(t),Hyr=r(b8e," (Vision Encoder decoder model)"),b8e.forEach(t),tut.forEach(t),Uyr=i(Pa),G5e=n(Pa,"P",{});var sut=s(G5e);Jyr=r(sut,"Examples:"),sut.forEach(t),Yyr=i(Pa),m(WA.$$.fragment,Pa),Pa.forEach(t),Mi.forEach(t),this.h()},h(){c(J,"name","hf:doc:metadata"),c(J,"content",JSON.stringify(put)),c(me,"id","auto-classes"),c(me,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(me,"href","#auto-classes"),c(de,"class","relative group"),c(Pn,"href","/docs/transformers/pr_15770/en/model_doc/auto#transformers.AutoConfig"),c(In,"href","/docs/transformers/pr_15770/en/model_doc/auto#transformers.AutoModel"),c(jn,"href","/docs/transformers/pr_15770/en/model_doc/auto#transformers.AutoTokenizer"),c(Ri,"href","/docs/transformers/pr_15770/en/model_doc/bert#transformers.BertModel"),c(Lf,"id","extending-the-auto-classes"),c(Lf,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Lf,"href","#extending-the-auto-classes"),c(Si,"class","relative group"),c(kf,"id","transformers.AutoConfig"),c(kf,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(kf,"href","#transformers.AutoConfig"),c(Pi,"class","relative group"),c(UL,"href","/docs/transformers/pr_15770/en/model_doc/auto#transformers.AutoConfig.from_pretrained"),c(JL,"href","/docs/transformers/pr_15770/en/model_doc/albert#transformers.AlbertConfig"),c(YL,"href","/docs/transformers/pr_15770/en/model_doc/bart#transformers.BartConfig"),c(KL,"href","/docs/transformers/pr_15770/en/model_doc/beit#transformers.BeitConfig"),c(ZL,"href","/docs/transformers/pr_15770/en/model_doc/bert#transformers.BertConfig"),c(e8,"href","/docs/transformers/pr_15770/en/model_doc/bert-generation#transformers.BertGenerationConfig"),c(o8,"href","/docs/transformers/pr_15770/en/model_doc/big_bird#transformers.BigBirdConfig"),c(r8,"href","/docs/transformers/pr_15770/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusConfig"),c(t8,"href","/docs/transformers/pr_15770/en/model_doc/blenderbot#transformers.BlenderbotConfig"),c(a8,"href","/docs/transformers/pr_15770/en/model_doc/blenderbot-small#transformers.BlenderbotSmallConfig"),c(n8,"href","/docs/transformers/pr_15770/en/model_doc/camembert#transformers.CamembertConfig"),c(s8,"href","/docs/transformers/pr_15770/en/model_doc/canine#transformers.CanineConfig"),c(l8,"href","/docs/transformers/pr_15770/en/model_doc/clip#transformers.CLIPConfig"),c(i8,"href","/docs/transformers/pr_15770/en/model_doc/convbert#transformers.ConvBertConfig"),c(d8,"href","/docs/transformers/pr_15770/en/model_doc/convnext#transformers.ConvNextConfig"),c(c8,"href","/docs/transformers/pr_15770/en/model_doc/ctrl#transformers.CTRLConfig"),c(f8,"href","/docs/transformers/pr_15770/en/model_doc/deberta#transformers.DebertaConfig"),c(m8,"href","/docs/transformers/pr_15770/en/model_doc/deberta-v2#transformers.DebertaV2Config"),c(g8,"href","/docs/transformers/pr_15770/en/model_doc/deit#transformers.DeiTConfig"),c(h8,"href","/docs/transformers/pr_15770/en/model_doc/detr#transformers.DetrConfig"),c(p8,"href","/docs/transformers/pr_15770/en/model_doc/distilbert#transformers.DistilBertConfig"),c(_8,"href","/docs/transformers/pr_15770/en/model_doc/dpr#transformers.DPRConfig"),c(u8,"href","/docs/transformers/pr_15770/en/model_doc/electra#transformers.ElectraConfig"),c(b8,"href","/docs/transformers/pr_15770/en/model_doc/encoder-decoder#transformers.EncoderDecoderConfig"),c(v8,"href","/docs/transformers/pr_15770/en/model_doc/flaubert#transformers.FlaubertConfig"),c(T8,"href","/docs/transformers/pr_15770/en/model_doc/fnet#transformers.FNetConfig"),c(F8,"href","/docs/transformers/pr_15770/en/model_doc/fsmt#transformers.FSMTConfig"),c(C8,"href","/docs/transformers/pr_15770/en/model_doc/funnel#transformers.FunnelConfig"),c(M8,"href","/docs/transformers/pr_15770/en/model_doc/gpt2#transformers.GPT2Config"),c(E8,"href","/docs/transformers/pr_15770/en/model_doc/gpt_neo#transformers.GPTNeoConfig"),c(y8,"href","/docs/transformers/pr_15770/en/model_doc/gptj#transformers.GPTJConfig"),c(w8,"href","/docs/transformers/pr_15770/en/model_doc/hubert#transformers.HubertConfig"),c(A8,"href","/docs/transformers/pr_15770/en/model_doc/ibert#transformers.IBertConfig"),c(L8,"href","/docs/transformers/pr_15770/en/model_doc/imagegpt#transformers.ImageGPTConfig"),c(B8,"href","/docs/transformers/pr_15770/en/model_doc/layoutlm#transformers.LayoutLMConfig"),c(k8,"href","/docs/transformers/pr_15770/en/model_doc/layoutlmv2#transformers.LayoutLMv2Config"),c(x8,"href","/docs/transformers/pr_15770/en/model_doc/led#transformers.LEDConfig"),c(R8,"href","/docs/transformers/pr_15770/en/model_doc/longformer#transformers.LongformerConfig"),c(S8,"href","/docs/transformers/pr_15770/en/model_doc/luke#transformers.LukeConfig"),c(P8,"href","/docs/transformers/pr_15770/en/model_doc/lxmert#transformers.LxmertConfig"),c($8,"href","/docs/transformers/pr_15770/en/model_doc/m2m_100#transformers.M2M100Config"),c(I8,"href","/docs/transformers/pr_15770/en/model_doc/marian#transformers.MarianConfig"),c(j8,"href","/docs/transformers/pr_15770/en/model_doc/mbart#transformers.MBartConfig"),c(N8,"href","/docs/transformers/pr_15770/en/model_doc/megatron-bert#transformers.MegatronBertConfig"),c(D8,"href","/docs/transformers/pr_15770/en/model_doc/mobilebert#transformers.MobileBertConfig"),c(q8,"href","/docs/transformers/pr_15770/en/model_doc/mpnet#transformers.MPNetConfig"),c(G8,"href","/docs/transformers/pr_15770/en/model_doc/mt5#transformers.MT5Config"),c(O8,"href","/docs/transformers/pr_15770/en/model_doc/nystromformer#transformers.NystromformerConfig"),c(X8,"href","/docs/transformers/pr_15770/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig"),c(z8,"href","/docs/transformers/pr_15770/en/model_doc/pegasus#transformers.PegasusConfig"),c(V8,"href","/docs/transformers/pr_15770/en/model_doc/perceiver#transformers.PerceiverConfig"),c(W8,"href","/docs/transformers/pr_15770/en/model_doc/plbart#transformers.PLBartConfig"),c(Q8,"href","/docs/transformers/pr_15770/en/model_doc/poolformer#transformers.PoolFormerConfig"),c(H8,"href","/docs/transformers/pr_15770/en/model_doc/prophetnet#transformers.ProphetNetConfig"),c(U8,"href","/docs/transformers/pr_15770/en/model_doc/qdqbert#transformers.QDQBertConfig"),c(J8,"href","/docs/transformers/pr_15770/en/model_doc/rag#transformers.RagConfig"),c(Y8,"href","/docs/transformers/pr_15770/en/model_doc/realm#transformers.RealmConfig"),c(K8,"href","/docs/transformers/pr_15770/en/model_doc/reformer#transformers.ReformerConfig"),c(Z8,"href","/docs/transformers/pr_15770/en/model_doc/rembert#transformers.RemBertConfig"),c(e9,"href","/docs/transformers/pr_15770/en/model_doc/resnet#transformers.ResNetConfig"),c(o9,"href","/docs/transformers/pr_15770/en/model_doc/retribert#transformers.RetriBertConfig"),c(r9,"href","/docs/transformers/pr_15770/en/model_doc/roberta#transformers.RobertaConfig"),c(t9,"href","/docs/transformers/pr_15770/en/model_doc/roformer#transformers.RoFormerConfig"),c(a9,"href","/docs/transformers/pr_15770/en/model_doc/segformer#transformers.SegformerConfig"),c(n9,"href","/docs/transformers/pr_15770/en/model_doc/sew#transformers.SEWConfig"),c(s9,"href","/docs/transformers/pr_15770/en/model_doc/sew-d#transformers.SEWDConfig"),c(l9,"href","/docs/transformers/pr_15770/en/model_doc/speech-encoder-decoder#transformers.SpeechEncoderDecoderConfig"),c(i9,"href","/docs/transformers/pr_15770/en/model_doc/speech_to_text#transformers.Speech2TextConfig"),c(d9,"href","/docs/transformers/pr_15770/en/model_doc/speech_to_text_2#transformers.Speech2Text2Config"),c(c9,"href","/docs/transformers/pr_15770/en/model_doc/splinter#transformers.SplinterConfig"),c(f9,"href","/docs/transformers/pr_15770/en/model_doc/squeezebert#transformers.SqueezeBertConfig"),c(m9,"href","/docs/transformers/pr_15770/en/model_doc/swin#transformers.SwinConfig"),c(g9,"href","/docs/transformers/pr_15770/en/model_doc/t5#transformers.T5Config"),c(h9,"href","/docs/transformers/pr_15770/en/model_doc/tapas#transformers.TapasConfig"),c(p9,"href","/docs/transformers/pr_15770/en/model_doc/transfo-xl#transformers.TransfoXLConfig"),c(_9,"href","/docs/transformers/pr_15770/en/model_doc/trocr#transformers.TrOCRConfig"),c(u9,"href","/docs/transformers/pr_15770/en/model_doc/unispeech#transformers.UniSpeechConfig"),c(b9,"href","/docs/transformers/pr_15770/en/model_doc/unispeech-sat#transformers.UniSpeechSatConfig"),c(v9,"href","/docs/transformers/pr_15770/en/model_doc/vilt#transformers.ViltConfig"),c(T9,"href","/docs/transformers/pr_15770/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderConfig"),c(F9,"href","/docs/transformers/pr_15770/en/model_doc/vision-text-dual-encoder#transformers.VisionTextDualEncoderConfig"),c(C9,"href","/docs/transformers/pr_15770/en/model_doc/visual_bert#transformers.VisualBertConfig"),c(M9,"href","/docs/transformers/pr_15770/en/model_doc/vit#transformers.ViTConfig"),c(E9,"href","/docs/transformers/pr_15770/en/model_doc/vit_mae#transformers.ViTMAEConfig"),c(y9,"href","/docs/transformers/pr_15770/en/model_doc/wav2vec2#transformers.Wav2Vec2Config"),c(w9,"href","/docs/transformers/pr_15770/en/model_doc/wavlm#transformers.WavLMConfig"),c(A9,"href","/docs/transformers/pr_15770/en/model_doc/xglm#transformers.XGLMConfig"),c(L9,"href","/docs/transformers/pr_15770/en/model_doc/xlm#transformers.XLMConfig"),c(B9,"href","/docs/transformers/pr_15770/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetConfig"),c(k9,"href","/docs/transformers/pr_15770/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig"),c(x9,"href","/docs/transformers/pr_15770/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLConfig"),c(R9,"href","/docs/transformers/pr_15770/en/model_doc/xlnet#transformers.XLNetConfig"),c(S9,"href","/docs/transformers/pr_15770/en/model_doc/yoso#transformers.YosoConfig"),c(fo,"class","docstring"),c(pg,"class","docstring"),c(Go,"class","docstring"),c(_g,"id","transformers.AutoTokenizer"),c(_g,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(_g,"href","#transformers.AutoTokenizer"),c(Ii,"class","relative group"),c(P9,"href","/docs/transformers/pr_15770/en/model_doc/auto#transformers.AutoTokenizer.from_pretrained"),c($9,"href","/docs/transformers/pr_15770/en/model_doc/albert#transformers.AlbertTokenizer"),c(I9,"href","/docs/transformers/pr_15770/en/model_doc/albert#transformers.AlbertTokenizerFast"),c(j9,"href","/docs/transformers/pr_15770/en/model_doc/bart#transformers.BartTokenizer"),c(N9,"href","/docs/transformers/pr_15770/en/model_doc/bart#transformers.BartTokenizerFast"),c(D9,"href","/docs/transformers/pr_15770/en/model_doc/barthez#transformers.BarthezTokenizer"),c(q9,"href","/docs/transformers/pr_15770/en/model_doc/barthez#transformers.BarthezTokenizerFast"),c(G9,"href","/docs/transformers/pr_15770/en/model_doc/bartpho#transformers.BartphoTokenizer"),c(O9,"href","/docs/transformers/pr_15770/en/model_doc/bert#transformers.BertTokenizer"),c(X9,"href","/docs/transformers/pr_15770/en/model_doc/bert#transformers.BertTokenizerFast"),c(z9,"href","/docs/transformers/pr_15770/en/model_doc/bert-generation#transformers.BertGenerationTokenizer"),c(V9,"href","/docs/transformers/pr_15770/en/model_doc/bert-japanese#transformers.BertJapaneseTokenizer"),c(W9,"href","/docs/transformers/pr_15770/en/model_doc/bertweet#transformers.BertweetTokenizer"),c(Q9,"href","/docs/transformers/pr_15770/en/model_doc/big_bird#transformers.BigBirdTokenizer"),c(H9,"href","/docs/transformers/pr_15770/en/model_doc/big_bird#transformers.BigBirdTokenizerFast"),c(U9,"href","/docs/transformers/pr_15770/en/model_doc/pegasus#transformers.PegasusTokenizer"),c(J9,"href","/docs/transformers/pr_15770/en/model_doc/pegasus#transformers.PegasusTokenizerFast"),c(Y9,"href","/docs/transformers/pr_15770/en/model_doc/blenderbot#transformers.BlenderbotTokenizer"),c(K9,"href","/docs/transformers/pr_15770/en/model_doc/blenderbot#transformers.BlenderbotTokenizerFast"),c(Z9,"href","/docs/transformers/pr_15770/en/model_doc/blenderbot-small#transformers.BlenderbotSmallTokenizer"),c(eB,"href","/docs/transformers/pr_15770/en/model_doc/byt5#transformers.ByT5Tokenizer"),c(oB,"href","/docs/transformers/pr_15770/en/model_doc/camembert#transformers.CamembertTokenizer"),c(rB,"href","/docs/transformers/pr_15770/en/model_doc/camembert#transformers.CamembertTokenizerFast"),c(tB,"href","/docs/transformers/pr_15770/en/model_doc/canine#transformers.CanineTokenizer"),c(aB,"href","/docs/transformers/pr_15770/en/model_doc/clip#transformers.CLIPTokenizer"),c(nB,"href","/docs/transformers/pr_15770/en/model_doc/clip#transformers.CLIPTokenizerFast"),c(sB,"href","/docs/transformers/pr_15770/en/model_doc/convbert#transformers.ConvBertTokenizer"),c(lB,"href","/docs/transformers/pr_15770/en/model_doc/convbert#transformers.ConvBertTokenizerFast"),c(iB,"href","/docs/transformers/pr_15770/en/model_doc/cpm#transformers.CpmTokenizer"),c(dB,"href","/docs/transformers/pr_15770/en/model_doc/ctrl#transformers.CTRLTokenizer"),c(cB,"href","/docs/transformers/pr_15770/en/model_doc/deberta#transformers.DebertaTokenizer"),c(fB,"href","/docs/transformers/pr_15770/en/model_doc/deberta#transformers.DebertaTokenizerFast"),c(mB,"href","/docs/transformers/pr_15770/en/model_doc/deberta-v2#transformers.DebertaV2Tokenizer"),c(gB,"href","/docs/transformers/pr_15770/en/model_doc/distilbert#transformers.DistilBertTokenizer"),c(hB,"href","/docs/transformers/pr_15770/en/model_doc/distilbert#transformers.DistilBertTokenizerFast"),c(pB,"href","/docs/transformers/pr_15770/en/model_doc/dpr#transformers.DPRQuestionEncoderTokenizer"),c(_B,"href","/docs/transformers/pr_15770/en/model_doc/dpr#transformers.DPRQuestionEncoderTokenizerFast"),c(uB,"href","/docs/transformers/pr_15770/en/model_doc/electra#transformers.ElectraTokenizer"),c(bB,"href","/docs/transformers/pr_15770/en/model_doc/electra#transformers.ElectraTokenizerFast"),c(vB,"href","/docs/transformers/pr_15770/en/model_doc/flaubert#transformers.FlaubertTokenizer"),c(TB,"href","/docs/transformers/pr_15770/en/model_doc/fnet#transformers.FNetTokenizer"),c(FB,"href","/docs/transformers/pr_15770/en/model_doc/fnet#transformers.FNetTokenizerFast"),c(CB,"href","/docs/transformers/pr_15770/en/model_doc/fsmt#transformers.FSMTTokenizer"),c(MB,"href","/docs/transformers/pr_15770/en/model_doc/funnel#transformers.FunnelTokenizer"),c(EB,"href","/docs/transformers/pr_15770/en/model_doc/funnel#transformers.FunnelTokenizerFast"),c(yB,"href","/docs/transformers/pr_15770/en/model_doc/gpt2#transformers.GPT2Tokenizer"),c(wB,"href","/docs/transformers/pr_15770/en/model_doc/gpt2#transformers.GPT2TokenizerFast"),c(AB,"href","/docs/transformers/pr_15770/en/model_doc/gpt2#transformers.GPT2Tokenizer"),c(LB,"href","/docs/transformers/pr_15770/en/model_doc/gpt2#transformers.GPT2TokenizerFast"),c(BB,"href","/docs/transformers/pr_15770/en/model_doc/herbert#transformers.HerbertTokenizer"),c(kB,"href","/docs/transformers/pr_15770/en/model_doc/herbert#transformers.HerbertTokenizerFast"),c(xB,"href","/docs/transformers/pr_15770/en/model_doc/wav2vec2#transformers.Wav2Vec2CTCTokenizer"),c(RB,"href","/docs/transformers/pr_15770/en/model_doc/roberta#transformers.RobertaTokenizer"),c(SB,"href","/docs/transformers/pr_15770/en/model_doc/roberta#transformers.RobertaTokenizerFast"),c(PB,"href","/docs/transformers/pr_15770/en/model_doc/layoutlm#transformers.LayoutLMTokenizer"),c($B,"href","/docs/transformers/pr_15770/en/model_doc/layoutlm#transformers.LayoutLMTokenizerFast"),c(IB,"href","/docs/transformers/pr_15770/en/model_doc/layoutlmv2#transformers.LayoutLMv2Tokenizer"),c(jB,"href","/docs/transformers/pr_15770/en/model_doc/layoutlmv2#transformers.LayoutLMv2TokenizerFast"),c(NB,"href","/docs/transformers/pr_15770/en/model_doc/layoutxlm#transformers.LayoutXLMTokenizer"),c(DB,"href","/docs/transformers/pr_15770/en/model_doc/layoutxlm#transformers.LayoutXLMTokenizerFast"),c(qB,"href","/docs/transformers/pr_15770/en/model_doc/led#transformers.LEDTokenizer"),c(GB,"href","/docs/transformers/pr_15770/en/model_doc/led#transformers.LEDTokenizerFast"),c(OB,"href","/docs/transformers/pr_15770/en/model_doc/longformer#transformers.LongformerTokenizer"),c(XB,"href","/docs/transformers/pr_15770/en/model_doc/longformer#transformers.LongformerTokenizerFast"),c(zB,"href","/docs/transformers/pr_15770/en/model_doc/luke#transformers.LukeTokenizer"),c(VB,"href","/docs/transformers/pr_15770/en/model_doc/lxmert#transformers.LxmertTokenizer"),c(WB,"href","/docs/transformers/pr_15770/en/model_doc/lxmert#transformers.LxmertTokenizerFast"),c(QB,"href","/docs/transformers/pr_15770/en/model_doc/m2m_100#transformers.M2M100Tokenizer"),c(HB,"href","/docs/transformers/pr_15770/en/model_doc/marian#transformers.MarianTokenizer"),c(UB,"href","/docs/transformers/pr_15770/en/model_doc/mbart#transformers.MBartTokenizer"),c(JB,"href","/docs/transformers/pr_15770/en/model_doc/mbart#transformers.MBartTokenizerFast"),c(YB,"href","/docs/transformers/pr_15770/en/model_doc/mbart#transformers.MBart50Tokenizer"),c(KB,"href","/docs/transformers/pr_15770/en/model_doc/mbart#transformers.MBart50TokenizerFast"),c(ZB,"href","/docs/transformers/pr_15770/en/model_doc/mluke#transformers.MLukeTokenizer"),c(ek,"href","/docs/transformers/pr_15770/en/model_doc/mobilebert#transformers.MobileBertTokenizer"),c(ok,"href","/docs/transformers/pr_15770/en/model_doc/mobilebert#transformers.MobileBertTokenizerFast"),c(rk,"href","/docs/transformers/pr_15770/en/model_doc/mpnet#transformers.MPNetTokenizer"),c(tk,"href","/docs/transformers/pr_15770/en/model_doc/mpnet#transformers.MPNetTokenizerFast"),c(ak,"href","/docs/transformers/pr_15770/en/model_doc/mt5#transformers.T5Tokenizer"),c(nk,"href","/docs/transformers/pr_15770/en/model_doc/mt5#transformers.T5TokenizerFast"),c(sk,"href","/docs/transformers/pr_15770/en/model_doc/openai-gpt#transformers.OpenAIGPTTokenizer"),c(lk,"href","/docs/transformers/pr_15770/en/model_doc/openai-gpt#transformers.OpenAIGPTTokenizerFast"),c(ik,"href","/docs/transformers/pr_15770/en/model_doc/pegasus#transformers.PegasusTokenizer"),c(dk,"href","/docs/transformers/pr_15770/en/model_doc/pegasus#transformers.PegasusTokenizerFast"),c(ck,"href","/docs/transformers/pr_15770/en/model_doc/perceiver#transformers.PerceiverTokenizer"),c(fk,"href","/docs/transformers/pr_15770/en/model_doc/phobert#transformers.PhobertTokenizer"),c(mk,"href","/docs/transformers/pr_15770/en/model_doc/plbart#transformers.PLBartTokenizer"),c(gk,"href","/docs/transformers/pr_15770/en/model_doc/prophetnet#transformers.ProphetNetTokenizer"),c(hk,"href","/docs/transformers/pr_15770/en/model_doc/bert#transformers.BertTokenizer"),c(pk,"href","/docs/transformers/pr_15770/en/model_doc/bert#transformers.BertTokenizerFast"),c(_k,"href","/docs/transformers/pr_15770/en/model_doc/rag#transformers.RagTokenizer"),c(uk,"href","/docs/transformers/pr_15770/en/model_doc/reformer#transformers.ReformerTokenizer"),c(bk,"href","/docs/transformers/pr_15770/en/model_doc/reformer#transformers.ReformerTokenizerFast"),c(vk,"href","/docs/transformers/pr_15770/en/model_doc/rembert#transformers.RemBertTokenizer"),c(Tk,"href","/docs/transformers/pr_15770/en/model_doc/rembert#transformers.RemBertTokenizerFast"),c(Fk,"href","/docs/transformers/pr_15770/en/model_doc/retribert#transformers.RetriBertTokenizer"),c(Ck,"href","/docs/transformers/pr_15770/en/model_doc/retribert#transformers.RetriBertTokenizerFast"),c(Mk,"href","/docs/transformers/pr_15770/en/model_doc/roberta#transformers.RobertaTokenizer"),c(Ek,"href","/docs/transformers/pr_15770/en/model_doc/roberta#transformers.RobertaTokenizerFast"),c(yk,"href","/docs/transformers/pr_15770/en/model_doc/roformer#transformers.RoFormerTokenizer"),c(wk,"href","/docs/transformers/pr_15770/en/model_doc/roformer#transformers.RoFormerTokenizerFast"),c(Ak,"href","/docs/transformers/pr_15770/en/model_doc/speech_to_text#transformers.Speech2TextTokenizer"),c(Lk,"href","/docs/transformers/pr_15770/en/model_doc/speech_to_text_2#transformers.Speech2Text2Tokenizer"),c(Bk,"href","/docs/transformers/pr_15770/en/model_doc/splinter#transformers.SplinterTokenizer"),c(kk,"href","/docs/transformers/pr_15770/en/model_doc/splinter#transformers.SplinterTokenizerFast"),c(xk,"href","/docs/transformers/pr_15770/en/model_doc/squeezebert#transformers.SqueezeBertTokenizer"),c(Rk,"href","/docs/transformers/pr_15770/en/model_doc/squeezebert#transformers.SqueezeBertTokenizerFast"),c(Sk,"href","/docs/transformers/pr_15770/en/model_doc/mt5#transformers.T5Tokenizer"),c(Pk,"href","/docs/transformers/pr_15770/en/model_doc/mt5#transformers.T5TokenizerFast"),c($k,"href","/docs/transformers/pr_15770/en/model_doc/tapas#transformers.TapasTokenizer"),c(Ik,"href","/docs/transformers/pr_15770/en/model_doc/transfo-xl#transformers.TransfoXLTokenizer"),c(jk,"href","/docs/transformers/pr_15770/en/model_doc/wav2vec2#transformers.Wav2Vec2CTCTokenizer"),c(Nk,"href","/docs/transformers/pr_15770/en/model_doc/wav2vec2_phoneme#transformers.Wav2Vec2PhonemeCTCTokenizer"),c(Dk,"href","/docs/transformers/pr_15770/en/model_doc/xglm#transformers.XGLMTokenizer"),c(qk,"href","/docs/transformers/pr_15770/en/model_doc/xglm#transformers.XGLMTokenizerFast"),c(Gk,"href","/docs/transformers/pr_15770/en/model_doc/xlm#transformers.XLMTokenizer"),c(Ok,"href","/docs/transformers/pr_15770/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetTokenizer"),c(Xk,"href","/docs/transformers/pr_15770/en/model_doc/xlm-roberta#transformers.XLMRobertaTokenizer"),c(zk,"href","/docs/transformers/pr_15770/en/model_doc/xlm-roberta#transformers.XLMRobertaTokenizerFast"),c(Vk,"href","/docs/transformers/pr_15770/en/model_doc/xlnet#transformers.XLNetTokenizer"),c(Wk,"href","/docs/transformers/pr_15770/en/model_doc/xlnet#transformers.XLNetTokenizerFast"),c(mo,"class","docstring"),c(Wg,"class","docstring"),c(Oo,"class","docstring"),c(Qg,"id","transformers.AutoFeatureExtractor"),c(Qg,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Qg,"href","#transformers.AutoFeatureExtractor"),c(ji,"class","relative group"),c(Qk,"href","/docs/transformers/pr_15770/en/model_doc/auto#transformers.AutoFeatureExtractor.from_pretrained"),c(Hk,"href","/docs/transformers/pr_15770/en/model_doc/beit#transformers.BeitFeatureExtractor"),c(Uk,"href","/docs/transformers/pr_15770/en/model_doc/clip#transformers.CLIPFeatureExtractor"),c(Jk,"href","/docs/transformers/pr_15770/en/model_doc/convnext#transformers.ConvNextFeatureExtractor"),c(Yk,"href","/docs/transformers/pr_15770/en/model_doc/deit#transformers.DeiTFeatureExtractor"),c(Kk,"href","/docs/transformers/pr_15770/en/model_doc/detr#transformers.DetrFeatureExtractor"),c(Zk,"href","/docs/transformers/pr_15770/en/model_doc/wav2vec2#transformers.Wav2Vec2FeatureExtractor"),c(ex,"href","/docs/transformers/pr_15770/en/model_doc/layoutlmv2#transformers.LayoutLMv2FeatureExtractor"),c(ox,"href","/docs/transformers/pr_15770/en/model_doc/perceiver#transformers.PerceiverFeatureExtractor"),c(rx,"href","/docs/transformers/pr_15770/en/model_doc/poolformer#transformers.PoolFormerFeatureExtractor"),c(tx,"href","/docs/transformers/pr_15770/en/model_doc/convnext#transformers.ConvNextFeatureExtractor"),c(ax,"href","/docs/transformers/pr_15770/en/model_doc/segformer#transformers.SegformerFeatureExtractor"),c(nx,"href","/docs/transformers/pr_15770/en/model_doc/speech_to_text#transformers.Speech2TextFeatureExtractor"),c(sx,"href","/docs/transformers/pr_15770/en/model_doc/vit#transformers.ViTFeatureExtractor"),c(lx,"href","/docs/transformers/pr_15770/en/model_doc/vit#transformers.ViTFeatureExtractor"),c(ix,"href","/docs/transformers/pr_15770/en/model_doc/vit#transformers.ViTFeatureExtractor"),c(dx,"href","/docs/transformers/pr_15770/en/model_doc/wav2vec2#transformers.Wav2Vec2FeatureExtractor"),c(Be,"class","docstring"),c(fh,"class","docstring"),c(Xo,"class","docstring"),c(mh,"id","transformers.AutoProcessor"),c(mh,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(mh,"href","#transformers.AutoProcessor"),c(Ni,"class","relative group"),c(cx,"href","/docs/transformers/pr_15770/en/model_doc/auto#transformers.AutoProcessor.from_pretrained"),c(fx,"href","/docs/transformers/pr_15770/en/model_doc/clip#transformers.CLIPProcessor"),c(mx,"href","/docs/transformers/pr_15770/en/model_doc/layoutlmv2#transformers.LayoutLMv2Processor"),c(gx,"href","/docs/transformers/pr_15770/en/model_doc/layoutxlm#transformers.LayoutXLMProcessor"),c(hx,"href","/docs/transformers/pr_15770/en/model_doc/speech_to_text#transformers.Speech2TextProcessor"),c(px,"href","/docs/transformers/pr_15770/en/model_doc/speech_to_text_2#transformers.Speech2Text2Processor"),c(_x,"href","/docs/transformers/pr_15770/en/model_doc/trocr#transformers.TrOCRProcessor"),c(ux,"href","/docs/transformers/pr_15770/en/model_doc/vision-text-dual-encoder#transformers.VisionTextDualEncoderProcessor"),c(bx,"href","/docs/transformers/pr_15770/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor"),c(ke,"class","docstring"),c(Ch,"class","docstring"),c(zo,"class","docstring"),c(Mh,"id","transformers.AutoModel"),c(Mh,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Mh,"href","#transformers.AutoModel"),c(qi,"class","relative group"),c(Nr,"class","docstring"),c(vx,"href","/docs/transformers/pr_15770/en/model_doc/albert#transformers.AlbertModel"),c(Tx,"href","/docs/transformers/pr_15770/en/model_doc/bart#transformers.BartModel"),c(Fx,"href","/docs/transformers/pr_15770/en/model_doc/beit#transformers.BeitModel"),c(Cx,"href","/docs/transformers/pr_15770/en/model_doc/bert#transformers.BertModel"),c(Mx,"href","/docs/transformers/pr_15770/en/model_doc/bert-generation#transformers.BertGenerationEncoder"),c(Ex,"href","/docs/transformers/pr_15770/en/model_doc/big_bird#transformers.BigBirdModel"),c(yx,"href","/docs/transformers/pr_15770/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusModel"),c(wx,"href","/docs/transformers/pr_15770/en/model_doc/blenderbot#transformers.BlenderbotModel"),c(Ax,"href","/docs/transformers/pr_15770/en/model_doc/blenderbot-small#transformers.BlenderbotSmallModel"),c(Lx,"href","/docs/transformers/pr_15770/en/model_doc/camembert#transformers.CamembertModel"),c(Bx,"href","/docs/transformers/pr_15770/en/model_doc/canine#transformers.CanineModel"),c(kx,"href","/docs/transformers/pr_15770/en/model_doc/clip#transformers.CLIPModel"),c(xx,"href","/docs/transformers/pr_15770/en/model_doc/convbert#transformers.ConvBertModel"),c(Rx,"href","/docs/transformers/pr_15770/en/model_doc/convnext#transformers.ConvNextModel"),c(Sx,"href","/docs/transformers/pr_15770/en/model_doc/ctrl#transformers.CTRLModel"),c(Px,"href","/docs/transformers/pr_15770/en/model_doc/deberta#transformers.DebertaModel"),c($x,"href","/docs/transformers/pr_15770/en/model_doc/deberta-v2#transformers.DebertaV2Model"),c(Ix,"href","/docs/transformers/pr_15770/en/model_doc/deit#transformers.DeiTModel"),c(jx,"href","/docs/transformers/pr_15770/en/model_doc/detr#transformers.DetrModel"),c(Nx,"href","/docs/transformers/pr_15770/en/model_doc/distilbert#transformers.DistilBertModel"),c(Dx,"href","/docs/transformers/pr_15770/en/model_doc/dpr#transformers.DPRQuestionEncoder"),c(qx,"href","/docs/transformers/pr_15770/en/model_doc/electra#transformers.ElectraModel"),c(Gx,"href","/docs/transformers/pr_15770/en/model_doc/flaubert#transformers.FlaubertModel"),c(Ox,"href","/docs/transformers/pr_15770/en/model_doc/fnet#transformers.FNetModel"),c(Xx,"href","/docs/transformers/pr_15770/en/model_doc/fsmt#transformers.FSMTModel"),c(zx,"href","/docs/transformers/pr_15770/en/model_doc/funnel#transformers.FunnelModel"),c(Vx,"href","/docs/transformers/pr_15770/en/model_doc/funnel#transformers.FunnelBaseModel"),c(Wx,"href","/docs/transformers/pr_15770/en/model_doc/gpt2#transformers.GPT2Model"),c(Qx,"href","/docs/transformers/pr_15770/en/model_doc/gpt_neo#transformers.GPTNeoModel"),c(Hx,"href","/docs/transformers/pr_15770/en/model_doc/gptj#transformers.GPTJModel"),c(Ux,"href","/docs/transformers/pr_15770/en/model_doc/hubert#transformers.HubertModel"),c(Jx,"href","/docs/transformers/pr_15770/en/model_doc/ibert#transformers.IBertModel"),c(Yx,"href","/docs/transformers/pr_15770/en/model_doc/imagegpt#transformers.ImageGPTModel"),c(Kx,"href","/docs/transformers/pr_15770/en/model_doc/layoutlm#transformers.LayoutLMModel"),c(Zx,"href","/docs/transformers/pr_15770/en/model_doc/layoutlmv2#transformers.LayoutLMv2Model"),c(eR,"href","/docs/transformers/pr_15770/en/model_doc/led#transformers.LEDModel"),c(oR,"href","/docs/transformers/pr_15770/en/model_doc/longformer#transformers.LongformerModel"),c(rR,"href","/docs/transformers/pr_15770/en/model_doc/luke#transformers.LukeModel"),c(tR,"href","/docs/transformers/pr_15770/en/model_doc/lxmert#transformers.LxmertModel"),c(aR,"href","/docs/transformers/pr_15770/en/model_doc/m2m_100#transformers.M2M100Model"),c(nR,"href","/docs/transformers/pr_15770/en/model_doc/marian#transformers.MarianModel"),c(sR,"href","/docs/transformers/pr_15770/en/model_doc/mbart#transformers.MBartModel"),c(lR,"href","/docs/transformers/pr_15770/en/model_doc/megatron-bert#transformers.MegatronBertModel"),c(iR,"href","/docs/transformers/pr_15770/en/model_doc/mobilebert#transformers.MobileBertModel"),c(dR,"href","/docs/transformers/pr_15770/en/model_doc/mpnet#transformers.MPNetModel"),c(cR,"href","/docs/transformers/pr_15770/en/model_doc/mt5#transformers.MT5Model"),c(fR,"href","/docs/transformers/pr_15770/en/model_doc/nystromformer#transformers.NystromformerModel"),c(mR,"href","/docs/transformers/pr_15770/en/model_doc/openai-gpt#transformers.OpenAIGPTModel"),c(gR,"href","/docs/transformers/pr_15770/en/model_doc/pegasus#transformers.PegasusModel"),c(hR,"href","/docs/transformers/pr_15770/en/model_doc/perceiver#transformers.PerceiverModel"),c(pR,"href","/docs/transformers/pr_15770/en/model_doc/plbart#transformers.PLBartModel"),c(_R,"href","/docs/transformers/pr_15770/en/model_doc/poolformer#transformers.PoolFormerModel"),c(uR,"href","/docs/transformers/pr_15770/en/model_doc/prophetnet#transformers.ProphetNetModel"),c(bR,"href","/docs/transformers/pr_15770/en/model_doc/qdqbert#transformers.QDQBertModel"),c(vR,"href","/docs/transformers/pr_15770/en/model_doc/reformer#transformers.ReformerModel"),c(TR,"href","/docs/transformers/pr_15770/en/model_doc/rembert#transformers.RemBertModel"),c(FR,"href","/docs/transformers/pr_15770/en/model_doc/resnet#transformers.ResNetModel"),c(CR,"href","/docs/transformers/pr_15770/en/model_doc/retribert#transformers.RetriBertModel"),c(MR,"href","/docs/transformers/pr_15770/en/model_doc/roberta#transformers.RobertaModel"),c(ER,"href","/docs/transformers/pr_15770/en/model_doc/roformer#transformers.RoFormerModel"),c(yR,"href","/docs/transformers/pr_15770/en/model_doc/segformer#transformers.SegformerModel"),c(wR,"href","/docs/transformers/pr_15770/en/model_doc/sew#transformers.SEWModel"),c(AR,"href","/docs/transformers/pr_15770/en/model_doc/sew-d#transformers.SEWDModel"),c(LR,"href","/docs/transformers/pr_15770/en/model_doc/speech_to_text#transformers.Speech2TextModel"),c(BR,"href","/docs/transformers/pr_15770/en/model_doc/splinter#transformers.SplinterModel"),c(kR,"href","/docs/transformers/pr_15770/en/model_doc/squeezebert#transformers.SqueezeBertModel"),c(xR,"href","/docs/transformers/pr_15770/en/model_doc/swin#transformers.SwinModel"),c(RR,"href","/docs/transformers/pr_15770/en/model_doc/t5#transformers.T5Model"),c(SR,"href","/docs/transformers/pr_15770/en/model_doc/tapas#transformers.TapasModel"),c(PR,"href","/docs/transformers/pr_15770/en/model_doc/transfo-xl#transformers.TransfoXLModel"),c($R,"href","/docs/transformers/pr_15770/en/model_doc/unispeech#transformers.UniSpeechModel"),c(IR,"href","/docs/transformers/pr_15770/en/model_doc/unispeech-sat#transformers.UniSpeechSatModel"),c(jR,"href","/docs/transformers/pr_15770/en/model_doc/vilt#transformers.ViltModel"),c(NR,"href","/docs/transformers/pr_15770/en/model_doc/vision-text-dual-encoder#transformers.VisionTextDualEncoderModel"),c(DR,"href","/docs/transformers/pr_15770/en/model_doc/visual_bert#transformers.VisualBertModel"),c(qR,"href","/docs/transformers/pr_15770/en/model_doc/vit#transformers.ViTModel"),c(GR,"href","/docs/transformers/pr_15770/en/model_doc/vit_mae#transformers.ViTMAEModel"),c(OR,"href","/docs/transformers/pr_15770/en/model_doc/wav2vec2#transformers.Wav2Vec2Model"),c(XR,"href","/docs/transformers/pr_15770/en/model_doc/wavlm#transformers.WavLMModel"),c(zR,"href","/docs/transformers/pr_15770/en/model_doc/xglm#transformers.XGLMModel"),c(VR,"href","/docs/transformers/pr_15770/en/model_doc/xlm#transformers.XLMModel"),c(WR,"href","/docs/transformers/pr_15770/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetModel"),c(QR,"href","/docs/transformers/pr_15770/en/model_doc/xlm-roberta#transformers.XLMRobertaModel"),c(HR,"href","/docs/transformers/pr_15770/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLModel"),c(UR,"href","/docs/transformers/pr_15770/en/model_doc/xlnet#transformers.XLNetModel"),c(JR,"href","/docs/transformers/pr_15770/en/model_doc/yoso#transformers.YosoModel"),c(xe,"class","docstring"),c(Vo,"class","docstring"),c(o_,"id","transformers.AutoModelForPreTraining"),c(o_,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(o_,"href","#transformers.AutoModelForPreTraining"),c(Xi,"class","relative group"),c(Dr,"class","docstring"),c(YR,"href","/docs/transformers/pr_15770/en/model_doc/albert#transformers.AlbertForPreTraining"),c(KR,"href","/docs/transformers/pr_15770/en/model_doc/bart#transformers.BartForConditionalGeneration"),c(ZR,"href","/docs/transformers/pr_15770/en/model_doc/bert#transformers.BertForPreTraining"),c(eS,"href","/docs/transformers/pr_15770/en/model_doc/big_bird#transformers.BigBirdForPreTraining"),c(oS,"href","/docs/transformers/pr_15770/en/model_doc/camembert#transformers.CamembertForMaskedLM"),c(rS,"href","/docs/transformers/pr_15770/en/model_doc/ctrl#transformers.CTRLLMHeadModel"),c(tS,"href","/docs/transformers/pr_15770/en/model_doc/deberta#transformers.DebertaForMaskedLM"),c(aS,"href","/docs/transformers/pr_15770/en/model_doc/deberta-v2#transformers.DebertaV2ForMaskedLM"),c(nS,"href","/docs/transformers/pr_15770/en/model_doc/distilbert#transformers.DistilBertForMaskedLM"),c(sS,"href","/docs/transformers/pr_15770/en/model_doc/electra#transformers.ElectraForPreTraining"),c(lS,"href","/docs/transformers/pr_15770/en/model_doc/flaubert#transformers.FlaubertWithLMHeadModel"),c(iS,"href","/docs/transformers/pr_15770/en/model_doc/fnet#transformers.FNetForPreTraining"),c(dS,"href","/docs/transformers/pr_15770/en/model_doc/fsmt#transformers.FSMTForConditionalGeneration"),c(cS,"href","/docs/transformers/pr_15770/en/model_doc/funnel#transformers.FunnelForPreTraining"),c(fS,"href","/docs/transformers/pr_15770/en/model_doc/gpt2#transformers.GPT2LMHeadModel"),c(mS,"href","/docs/transformers/pr_15770/en/model_doc/ibert#transformers.IBertForMaskedLM"),c(gS,"href","/docs/transformers/pr_15770/en/model_doc/layoutlm#transformers.LayoutLMForMaskedLM"),c(hS,"href","/docs/transformers/pr_15770/en/model_doc/longformer#transformers.LongformerForMaskedLM"),c(pS,"href","/docs/transformers/pr_15770/en/model_doc/lxmert#transformers.LxmertForPreTraining"),c(_S,"href","/docs/transformers/pr_15770/en/model_doc/megatron-bert#transformers.MegatronBertForPreTraining"),c(uS,"href","/docs/transformers/pr_15770/en/model_doc/mobilebert#transformers.MobileBertForPreTraining"),c(bS,"href","/docs/transformers/pr_15770/en/model_doc/mpnet#transformers.MPNetForMaskedLM"),c(vS,"href","/docs/transformers/pr_15770/en/model_doc/openai-gpt#transformers.OpenAIGPTLMHeadModel"),c(TS,"href","/docs/transformers/pr_15770/en/model_doc/retribert#transformers.RetriBertModel"),c(FS,"href","/docs/transformers/pr_15770/en/model_doc/roberta#transformers.RobertaForMaskedLM"),c(CS,"href","/docs/transformers/pr_15770/en/model_doc/squeezebert#transformers.SqueezeBertForMaskedLM"),c(MS,"href","/docs/transformers/pr_15770/en/model_doc/t5#transformers.T5ForConditionalGeneration"),c(ES,"href","/docs/transformers/pr_15770/en/model_doc/tapas#transformers.TapasForMaskedLM"),c(yS,"href","/docs/transformers/pr_15770/en/model_doc/transfo-xl#transformers.TransfoXLLMHeadModel"),c(wS,"href","/docs/transformers/pr_15770/en/model_doc/unispeech#transformers.UniSpeechForPreTraining"),c(AS,"href","/docs/transformers/pr_15770/en/model_doc/unispeech-sat#transformers.UniSpeechSatForPreTraining"),c(LS,"href","/docs/transformers/pr_15770/en/model_doc/visual_bert#transformers.VisualBertForPreTraining"),c(BS,"href","/docs/transformers/pr_15770/en/model_doc/vit_mae#transformers.ViTMAEForPreTraining"),c(kS,"href","/docs/transformers/pr_15770/en/model_doc/wav2vec2#transformers.Wav2Vec2ForPreTraining"),c(xS,"href","/docs/transformers/pr_15770/en/model_doc/xlm#transformers.XLMWithLMHeadModel"),c(RS,"href","/docs/transformers/pr_15770/en/model_doc/xlm-roberta#transformers.XLMRobertaForMaskedLM"),c(SS,"href","/docs/transformers/pr_15770/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForMaskedLM"),c(PS,"href","/docs/transformers/pr_15770/en/model_doc/xlnet#transformers.XLNetLMHeadModel"),c(Re,"class","docstring"),c(Wo,"class","docstring"),c(G_,"id","transformers.AutoModelForCausalLM"),c(G_,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(G_,"href","#transformers.AutoModelForCausalLM"),c(Wi,"class","relative group"),c(qr,"class","docstring"),c($S,"href","/docs/transformers/pr_15770/en/model_doc/bart#transformers.BartForCausalLM"),c(IS,"href","/docs/transformers/pr_15770/en/model_doc/bert#transformers.BertLMHeadModel"),c(jS,"href","/docs/transformers/pr_15770/en/model_doc/bert-generation#transformers.BertGenerationDecoder"),c(NS,"href","/docs/transformers/pr_15770/en/model_doc/big_bird#transformers.BigBirdForCausalLM"),c(DS,"href","/docs/transformers/pr_15770/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusForCausalLM"),c(qS,"href","/docs/transformers/pr_15770/en/model_doc/blenderbot#transformers.BlenderbotForCausalLM"),c(GS,"href","/docs/transformers/pr_15770/en/model_doc/blenderbot-small#transformers.BlenderbotSmallForCausalLM"),c(OS,"href","/docs/transformers/pr_15770/en/model_doc/camembert#transformers.CamembertForCausalLM"),c(XS,"href","/docs/transformers/pr_15770/en/model_doc/ctrl#transformers.CTRLLMHeadModel"),c(zS,"href","/docs/transformers/pr_15770/en/model_doc/electra#transformers.ElectraForCausalLM"),c(VS,"href","/docs/transformers/pr_15770/en/model_doc/gpt2#transformers.GPT2LMHeadModel"),c(WS,"href","/docs/transformers/pr_15770/en/model_doc/gpt_neo#transformers.GPTNeoForCausalLM"),c(QS,"href","/docs/transformers/pr_15770/en/model_doc/gptj#transformers.GPTJForCausalLM"),c(HS,"href","/docs/transformers/pr_15770/en/model_doc/marian#transformers.MarianForCausalLM"),c(US,"href","/docs/transformers/pr_15770/en/model_doc/mbart#transformers.MBartForCausalLM"),c(JS,"href","/docs/transformers/pr_15770/en/model_doc/megatron-bert#transformers.MegatronBertForCausalLM"),c(YS,"href","/docs/transformers/pr_15770/en/model_doc/openai-gpt#transformers.OpenAIGPTLMHeadModel"),c(KS,"href","/docs/transformers/pr_15770/en/model_doc/pegasus#transformers.PegasusForCausalLM"),c(ZS,"href","/docs/transformers/pr_15770/en/model_doc/plbart#transformers.PLBartForCausalLM"),c(eP,"href","/docs/transformers/pr_15770/en/model_doc/prophetnet#transformers.ProphetNetForCausalLM"),c(oP,"href","/docs/transformers/pr_15770/en/model_doc/qdqbert#transformers.QDQBertLMHeadModel"),c(rP,"href","/docs/transformers/pr_15770/en/model_doc/reformer#transformers.ReformerModelWithLMHead"),c(tP,"href","/docs/transformers/pr_15770/en/model_doc/rembert#transformers.RemBertForCausalLM"),c(aP,"href","/docs/transformers/pr_15770/en/model_doc/roberta#transformers.RobertaForCausalLM"),c(nP,"href","/docs/transformers/pr_15770/en/model_doc/roformer#transformers.RoFormerForCausalLM"),c(sP,"href","/docs/transformers/pr_15770/en/model_doc/speech_to_text_2#transformers.Speech2Text2ForCausalLM"),c(lP,"href","/docs/transformers/pr_15770/en/model_doc/transfo-xl#transformers.TransfoXLLMHeadModel"),c(iP,"href","/docs/transformers/pr_15770/en/model_doc/trocr#transformers.TrOCRForCausalLM"),c(dP,"href","/docs/transformers/pr_15770/en/model_doc/xglm#transformers.XGLMForCausalLM"),c(cP,"href","/docs/transformers/pr_15770/en/model_doc/xlm#transformers.XLMWithLMHeadModel"),c(fP,"href","/docs/transformers/pr_15770/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetForCausalLM"),c(mP,"href","/docs/transformers/pr_15770/en/model_doc/xlm-roberta#transformers.XLMRobertaForCausalLM"),c(gP,"href","/docs/transformers/pr_15770/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForCausalLM"),c(hP,"href","/docs/transformers/pr_15770/en/model_doc/xlnet#transformers.XLNetLMHeadModel"),c(Se,"class","docstring"),c(Qo,"class","docstring"),c(Mu,"id","transformers.AutoModelForMaskedLM"),c(Mu,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Mu,"href","#transformers.AutoModelForMaskedLM"),c(Ui,"class","relative group"),c(Gr,"class","docstring"),c(pP,"href","/docs/transformers/pr_15770/en/model_doc/albert#transformers.AlbertForMaskedLM"),c(_P,"href","/docs/transformers/pr_15770/en/model_doc/bart#transformers.BartForConditionalGeneration"),c(uP,"href","/docs/transformers/pr_15770/en/model_doc/bert#transformers.BertForMaskedLM"),c(bP,"href","/docs/transformers/pr_15770/en/model_doc/big_bird#transformers.BigBirdForMaskedLM"),c(vP,"href","/docs/transformers/pr_15770/en/model_doc/camembert#transformers.CamembertForMaskedLM"),c(TP,"href","/docs/transformers/pr_15770/en/model_doc/convbert#transformers.ConvBertForMaskedLM"),c(FP,"href","/docs/transformers/pr_15770/en/model_doc/deberta#transformers.DebertaForMaskedLM"),c(CP,"href","/docs/transformers/pr_15770/en/model_doc/deberta-v2#transformers.DebertaV2ForMaskedLM"),c(MP,"href","/docs/transformers/pr_15770/en/model_doc/distilbert#transformers.DistilBertForMaskedLM"),c(EP,"href","/docs/transformers/pr_15770/en/model_doc/electra#transformers.ElectraForMaskedLM"),c(yP,"href","/docs/transformers/pr_15770/en/model_doc/flaubert#transformers.FlaubertWithLMHeadModel"),c(wP,"href","/docs/transformers/pr_15770/en/model_doc/fnet#transformers.FNetForMaskedLM"),c(AP,"href","/docs/transformers/pr_15770/en/model_doc/funnel#transformers.FunnelForMaskedLM"),c(LP,"href","/docs/transformers/pr_15770/en/model_doc/ibert#transformers.IBertForMaskedLM"),c(BP,"href","/docs/transformers/pr_15770/en/model_doc/layoutlm#transformers.LayoutLMForMaskedLM"),c(kP,"href","/docs/transformers/pr_15770/en/model_doc/longformer#transformers.LongformerForMaskedLM"),c(xP,"href","/docs/transformers/pr_15770/en/model_doc/mbart#transformers.MBartForConditionalGeneration"),c(RP,"href","/docs/transformers/pr_15770/en/model_doc/megatron-bert#transformers.MegatronBertForMaskedLM"),c(SP,"href","/docs/transformers/pr_15770/en/model_doc/mobilebert#transformers.MobileBertForMaskedLM"),c(PP,"href","/docs/transformers/pr_15770/en/model_doc/mpnet#transformers.MPNetForMaskedLM"),c($P,"href","/docs/transformers/pr_15770/en/model_doc/nystromformer#transformers.NystromformerForMaskedLM"),c(IP,"href","/docs/transformers/pr_15770/en/model_doc/perceiver#transformers.PerceiverForMaskedLM"),c(jP,"href","/docs/transformers/pr_15770/en/model_doc/qdqbert#transformers.QDQBertForMaskedLM"),c(NP,"href","/docs/transformers/pr_15770/en/model_doc/reformer#transformers.ReformerForMaskedLM"),c(DP,"href","/docs/transformers/pr_15770/en/model_doc/rembert#transformers.RemBertForMaskedLM"),c(qP,"href","/docs/transformers/pr_15770/en/model_doc/roberta#transformers.RobertaForMaskedLM"),c(GP,"href","/docs/transformers/pr_15770/en/model_doc/roformer#transformers.RoFormerForMaskedLM"),c(OP,"href","/docs/transformers/pr_15770/en/model_doc/squeezebert#transformers.SqueezeBertForMaskedLM"),c(XP,"href","/docs/transformers/pr_15770/en/model_doc/tapas#transformers.TapasForMaskedLM"),c(zP,"href","/docs/transformers/pr_15770/en/model_doc/xlm#transformers.XLMWithLMHeadModel"),c(VP,"href","/docs/transformers/pr_15770/en/model_doc/xlm-roberta#transformers.XLMRobertaForMaskedLM"),c(WP,"href","/docs/transformers/pr_15770/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForMaskedLM"),c(QP,"href","/docs/transformers/pr_15770/en/model_doc/yoso#transformers.YosoForMaskedLM"),c(Pe,"class","docstring"),c(Ho,"class","docstring"),c(n1,"id","transformers.AutoModelForSeq2SeqLM"),c(n1,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(n1,"href","#transformers.AutoModelForSeq2SeqLM"),c(Ki,"class","relative group"),c(Or,"class","docstring"),c(HP,"href","/docs/transformers/pr_15770/en/model_doc/bart#transformers.BartForConditionalGeneration"),c(UP,"href","/docs/transformers/pr_15770/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusForConditionalGeneration"),c(JP,"href","/docs/transformers/pr_15770/en/model_doc/blenderbot#transformers.BlenderbotForConditionalGeneration"),c(YP,"href","/docs/transformers/pr_15770/en/model_doc/blenderbot-small#transformers.BlenderbotSmallForConditionalGeneration"),c(KP,"href","/docs/transformers/pr_15770/en/model_doc/encoder-decoder#transformers.EncoderDecoderModel"),c(ZP,"href","/docs/transformers/pr_15770/en/model_doc/fsmt#transformers.FSMTForConditionalGeneration"),c(e$,"href","/docs/transformers/pr_15770/en/model_doc/led#transformers.LEDForConditionalGeneration"),c(o$,"href","/docs/transformers/pr_15770/en/model_doc/m2m_100#transformers.M2M100ForConditionalGeneration"),c(r$,"href","/docs/transformers/pr_15770/en/model_doc/marian#transformers.MarianMTModel"),c(t$,"href","/docs/transformers/pr_15770/en/model_doc/mbart#transformers.MBartForConditionalGeneration"),c(a$,"href","/docs/transformers/pr_15770/en/model_doc/mt5#transformers.MT5ForConditionalGeneration"),c(n$,"href","/docs/transformers/pr_15770/en/model_doc/pegasus#transformers.PegasusForConditionalGeneration"),c(s$,"href","/docs/transformers/pr_15770/en/model_doc/plbart#transformers.PLBartForConditionalGeneration"),c(l$,"href","/docs/transformers/pr_15770/en/model_doc/prophetnet#transformers.ProphetNetForConditionalGeneration"),c(i$,"href","/docs/transformers/pr_15770/en/model_doc/t5#transformers.T5ForConditionalGeneration"),c(d$,"href","/docs/transformers/pr_15770/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetForConditionalGeneration"),c($e,"class","docstring"),c(Uo,"class","docstring"),c(M1,"id","transformers.AutoModelForSequenceClassification"),c(M1,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(M1,"href","#transformers.AutoModelForSequenceClassification"),c(od,"class","relative group"),c(Xr,"class","docstring"),c(c$,"href","/docs/transformers/pr_15770/en/model_doc/albert#transformers.AlbertForSequenceClassification"),c(f$,"href","/docs/transformers/pr_15770/en/model_doc/bart#transformers.BartForSequenceClassification"),c(m$,"href","/docs/transformers/pr_15770/en/model_doc/bert#transformers.BertForSequenceClassification"),c(g$,"href","/docs/transformers/pr_15770/en/model_doc/big_bird#transformers.BigBirdForSequenceClassification"),c(h$,"href","/docs/transformers/pr_15770/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusForSequenceClassification"),c(p$,"href","/docs/transformers/pr_15770/en/model_doc/camembert#transformers.CamembertForSequenceClassification"),c(_$,"href","/docs/transformers/pr_15770/en/model_doc/canine#transformers.CanineForSequenceClassification"),c(u$,"href","/docs/transformers/pr_15770/en/model_doc/convbert#transformers.ConvBertForSequenceClassification"),c(b$,"href","/docs/transformers/pr_15770/en/model_doc/ctrl#transformers.CTRLForSequenceClassification"),c(v$,"href","/docs/transformers/pr_15770/en/model_doc/deberta#transformers.DebertaForSequenceClassification"),c(T$,"href","/docs/transformers/pr_15770/en/model_doc/deberta-v2#transformers.DebertaV2ForSequenceClassification"),c(F$,"href","/docs/transformers/pr_15770/en/model_doc/distilbert#transformers.DistilBertForSequenceClassification"),c(C$,"href","/docs/transformers/pr_15770/en/model_doc/electra#transformers.ElectraForSequenceClassification"),c(M$,"href","/docs/transformers/pr_15770/en/model_doc/flaubert#transformers.FlaubertForSequenceClassification"),c(E$,"href","/docs/transformers/pr_15770/en/model_doc/fnet#transformers.FNetForSequenceClassification"),c(y$,"href","/docs/transformers/pr_15770/en/model_doc/funnel#transformers.FunnelForSequenceClassification"),c(w$,"href","/docs/transformers/pr_15770/en/model_doc/gpt2#transformers.GPT2ForSequenceClassification"),c(A$,"href","/docs/transformers/pr_15770/en/model_doc/gpt_neo#transformers.GPTNeoForSequenceClassification"),c(L$,"href","/docs/transformers/pr_15770/en/model_doc/gptj#transformers.GPTJForSequenceClassification"),c(B$,"href","/docs/transformers/pr_15770/en/model_doc/ibert#transformers.IBertForSequenceClassification"),c(k$,"href","/docs/transformers/pr_15770/en/model_doc/layoutlm#transformers.LayoutLMForSequenceClassification"),c(x$,"href","/docs/transformers/pr_15770/en/model_doc/layoutlmv2#transformers.LayoutLMv2ForSequenceClassification"),c(R$,"href","/docs/transformers/pr_15770/en/model_doc/led#transformers.LEDForSequenceClassification"),c(S$,"href","/docs/transformers/pr_15770/en/model_doc/longformer#transformers.LongformerForSequenceClassification"),c(P$,"href","/docs/transformers/pr_15770/en/model_doc/mbart#transformers.MBartForSequenceClassification"),c($$,"href","/docs/transformers/pr_15770/en/model_doc/megatron-bert#transformers.MegatronBertForSequenceClassification"),c(I$,"href","/docs/transformers/pr_15770/en/model_doc/mobilebert#transformers.MobileBertForSequenceClassification"),c(j$,"href","/docs/transformers/pr_15770/en/model_doc/mpnet#transformers.MPNetForSequenceClassification"),c(N$,"href","/docs/transformers/pr_15770/en/model_doc/nystromformer#transformers.NystromformerForSequenceClassification"),c(D$,"href","/docs/transformers/pr_15770/en/model_doc/openai-gpt#transformers.OpenAIGPTForSequenceClassification"),c(q$,"href","/docs/transformers/pr_15770/en/model_doc/perceiver#transformers.PerceiverForSequenceClassification"),c(G$,"href","/docs/transformers/pr_15770/en/model_doc/plbart#transformers.PLBartForSequenceClassification"),c(O$,"href","/docs/transformers/pr_15770/en/model_doc/qdqbert#transformers.QDQBertForSequenceClassification"),c(X$,"href","/docs/transformers/pr_15770/en/model_doc/reformer#transformers.ReformerForSequenceClassification"),c(z$,"href","/docs/transformers/pr_15770/en/model_doc/rembert#transformers.RemBertForSequenceClassification"),c(V$,"href","/docs/transformers/pr_15770/en/model_doc/roberta#transformers.RobertaForSequenceClassification"),c(W$,"href","/docs/transformers/pr_15770/en/model_doc/roformer#transformers.RoFormerForSequenceClassification"),c(Q$,"href","/docs/transformers/pr_15770/en/model_doc/squeezebert#transformers.SqueezeBertForSequenceClassification"),c(H$,"href","/docs/transformers/pr_15770/en/model_doc/tapas#transformers.TapasForSequenceClassification"),c(U$,"href","/docs/transformers/pr_15770/en/model_doc/transfo-xl#transformers.TransfoXLForSequenceClassification"),c(J$,"href","/docs/transformers/pr_15770/en/model_doc/xlm#transformers.XLMForSequenceClassification"),c(Y$,"href","/docs/transformers/pr_15770/en/model_doc/xlm-roberta#transformers.XLMRobertaForSequenceClassification"),c(K$,"href","/docs/transformers/pr_15770/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForSequenceClassification"),c(Z$,"href","/docs/transformers/pr_15770/en/model_doc/xlnet#transformers.XLNetForSequenceClassification"),c(eI,"href","/docs/transformers/pr_15770/en/model_doc/yoso#transformers.YosoForSequenceClassification"),c(Ie,"class","docstring"),c(Jo,"class","docstring"),c(_7,"id","transformers.AutoModelForMultipleChoice"),c(_7,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(_7,"href","#transformers.AutoModelForMultipleChoice"),c(ad,"class","relative group"),c(zr,"class","docstring"),c(oI,"href","/docs/transformers/pr_15770/en/model_doc/albert#transformers.AlbertForMultipleChoice"),c(rI,"href","/docs/transformers/pr_15770/en/model_doc/bert#transformers.BertForMultipleChoice"),c(tI,"href","/docs/transformers/pr_15770/en/model_doc/big_bird#transformers.BigBirdForMultipleChoice"),c(aI,"href","/docs/transformers/pr_15770/en/model_doc/camembert#transformers.CamembertForMultipleChoice"),c(nI,"href","/docs/transformers/pr_15770/en/model_doc/canine#transformers.CanineForMultipleChoice"),c(sI,"href","/docs/transformers/pr_15770/en/model_doc/convbert#transformers.ConvBertForMultipleChoice"),c(lI,"href","/docs/transformers/pr_15770/en/model_doc/distilbert#transformers.DistilBertForMultipleChoice"),c(iI,"href","/docs/transformers/pr_15770/en/model_doc/electra#transformers.ElectraForMultipleChoice"),c(dI,"href","/docs/transformers/pr_15770/en/model_doc/flaubert#transformers.FlaubertForMultipleChoice"),c(cI,"href","/docs/transformers/pr_15770/en/model_doc/fnet#transformers.FNetForMultipleChoice"),c(fI,"href","/docs/transformers/pr_15770/en/model_doc/funnel#transformers.FunnelForMultipleChoice"),c(mI,"href","/docs/transformers/pr_15770/en/model_doc/ibert#transformers.IBertForMultipleChoice"),c(gI,"href","/docs/transformers/pr_15770/en/model_doc/longformer#transformers.LongformerForMultipleChoice"),c(hI,"href","/docs/transformers/pr_15770/en/model_doc/megatron-bert#transformers.MegatronBertForMultipleChoice"),c(pI,"href","/docs/transformers/pr_15770/en/model_doc/mobilebert#transformers.MobileBertForMultipleChoice"),c(_I,"href","/docs/transformers/pr_15770/en/model_doc/mpnet#transformers.MPNetForMultipleChoice"),c(uI,"href","/docs/transformers/pr_15770/en/model_doc/nystromformer#transformers.NystromformerForMultipleChoice"),c(bI,"href","/docs/transformers/pr_15770/en/model_doc/qdqbert#transformers.QDQBertForMultipleChoice"),c(vI,"href","/docs/transformers/pr_15770/en/model_doc/rembert#transformers.RemBertForMultipleChoice"),c(TI,"href","/docs/transformers/pr_15770/en/model_doc/roberta#transformers.RobertaForMultipleChoice"),c(FI,"href","/docs/transformers/pr_15770/en/model_doc/roformer#transformers.RoFormerForMultipleChoice"),c(CI,"href","/docs/transformers/pr_15770/en/model_doc/squeezebert#transformers.SqueezeBertForMultipleChoice"),c(MI,"href","/docs/transformers/pr_15770/en/model_doc/xlm#transformers.XLMForMultipleChoice"),c(EI,"href","/docs/transformers/pr_15770/en/model_doc/xlm-roberta#transformers.XLMRobertaForMultipleChoice"),c(yI,"href","/docs/transformers/pr_15770/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForMultipleChoice"),c(wI,"href","/docs/transformers/pr_15770/en/model_doc/xlnet#transformers.XLNetForMultipleChoice"),c(AI,"href","/docs/transformers/pr_15770/en/model_doc/yoso#transformers.YosoForMultipleChoice"),c(je,"class","docstring"),c(Yo,"class","docstring"),c(V7,"id","transformers.AutoModelForNextSentencePrediction"),c(V7,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(V7,"href","#transformers.AutoModelForNextSentencePrediction"),c(ld,"class","relative group"),c(Vr,"class","docstring"),c(LI,"href","/docs/transformers/pr_15770/en/model_doc/bert#transformers.BertForNextSentencePrediction"),c(BI,"href","/docs/transformers/pr_15770/en/model_doc/fnet#transformers.FNetForNextSentencePrediction"),c(kI,"href","/docs/transformers/pr_15770/en/model_doc/megatron-bert#transformers.MegatronBertForNextSentencePrediction"),c(xI,"href","/docs/transformers/pr_15770/en/model_doc/mobilebert#transformers.MobileBertForNextSentencePrediction"),c(RI,"href","/docs/transformers/pr_15770/en/model_doc/qdqbert#transformers.QDQBertForNextSentencePrediction"),c(Ne,"class","docstring"),c(Ko,"class","docstring"),c(K7,"id","transformers.AutoModelForTokenClassification"),c(K7,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(K7,"href","#transformers.AutoModelForTokenClassification"),c(cd,"class","relative group"),c(Wr,"class","docstring"),c(SI,"href","/docs/transformers/pr_15770/en/model_doc/albert#transformers.AlbertForTokenClassification"),c(PI,"href","/docs/transformers/pr_15770/en/model_doc/bert#transformers.BertForTokenClassification"),c($I,"href","/docs/transformers/pr_15770/en/model_doc/big_bird#transformers.BigBirdForTokenClassification"),c(II,"href","/docs/transformers/pr_15770/en/model_doc/camembert#transformers.CamembertForTokenClassification"),c(jI,"href","/docs/transformers/pr_15770/en/model_doc/canine#transformers.CanineForTokenClassification"),c(NI,"href","/docs/transformers/pr_15770/en/model_doc/convbert#transformers.ConvBertForTokenClassification"),c(DI,"href","/docs/transformers/pr_15770/en/model_doc/deberta#transformers.DebertaForTokenClassification"),c(qI,"href","/docs/transformers/pr_15770/en/model_doc/deberta-v2#transformers.DebertaV2ForTokenClassification"),c(GI,"href","/docs/transformers/pr_15770/en/model_doc/distilbert#transformers.DistilBertForTokenClassification"),c(OI,"href","/docs/transformers/pr_15770/en/model_doc/electra#transformers.ElectraForTokenClassification"),c(XI,"href","/docs/transformers/pr_15770/en/model_doc/flaubert#transformers.FlaubertForTokenClassification"),c(zI,"href","/docs/transformers/pr_15770/en/model_doc/fnet#transformers.FNetForTokenClassification"),c(VI,"href","/docs/transformers/pr_15770/en/model_doc/funnel#transformers.FunnelForTokenClassification"),c(WI,"href","/docs/transformers/pr_15770/en/model_doc/gpt2#transformers.GPT2ForTokenClassification"),c(QI,"href","/docs/transformers/pr_15770/en/model_doc/ibert#transformers.IBertForTokenClassification"),c(HI,"href","/docs/transformers/pr_15770/en/model_doc/layoutlm#transformers.LayoutLMForTokenClassification"),c(UI,"href","/docs/transformers/pr_15770/en/model_doc/layoutlmv2#transformers.LayoutLMv2ForTokenClassification"),c(JI,"href","/docs/transformers/pr_15770/en/model_doc/longformer#transformers.LongformerForTokenClassification"),c(YI,"href","/docs/transformers/pr_15770/en/model_doc/megatron-bert#transformers.MegatronBertForTokenClassification"),c(KI,"href","/docs/transformers/pr_15770/en/model_doc/mobilebert#transformers.MobileBertForTokenClassification"),c(ZI,"href","/docs/transformers/pr_15770/en/model_doc/mpnet#transformers.MPNetForTokenClassification"),c(ej,"href","/docs/transformers/pr_15770/en/model_doc/nystromformer#transformers.NystromformerForTokenClassification"),c(oj,"href","/docs/transformers/pr_15770/en/model_doc/qdqbert#transformers.QDQBertForTokenClassification"),c(rj,"href","/docs/transformers/pr_15770/en/model_doc/rembert#transformers.RemBertForTokenClassification"),c(tj,"href","/docs/transformers/pr_15770/en/model_doc/roberta#transformers.RobertaForTokenClassification"),c(aj,"href","/docs/transformers/pr_15770/en/model_doc/roformer#transformers.RoFormerForTokenClassification"),c(nj,"href","/docs/transformers/pr_15770/en/model_doc/squeezebert#transformers.SqueezeBertForTokenClassification"),c(sj,"href","/docs/transformers/pr_15770/en/model_doc/xlm#transformers.XLMForTokenClassification"),c(lj,"href","/docs/transformers/pr_15770/en/model_doc/xlm-roberta#transformers.XLMRobertaForTokenClassification"),c(ij,"href","/docs/transformers/pr_15770/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForTokenClassification"),c(dj,"href","/docs/transformers/pr_15770/en/model_doc/xlnet#transformers.XLNetForTokenClassification"),c(cj,"href","/docs/transformers/pr_15770/en/model_doc/yoso#transformers.YosoForTokenClassification"),c(De,"class","docstring"),c(Zo,"class","docstring"),c(Rb,"id","transformers.AutoModelForQuestionAnswering"),c(Rb,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Rb,"href","#transformers.AutoModelForQuestionAnswering"),c(gd,"class","relative group"),c(Qr,"class","docstring"),c(fj,"href","/docs/transformers/pr_15770/en/model_doc/albert#transformers.AlbertForQuestionAnswering"),c(mj,"href","/docs/transformers/pr_15770/en/model_doc/bart#transformers.BartForQuestionAnswering"),c(gj,"href","/docs/transformers/pr_15770/en/model_doc/bert#transformers.BertForQuestionAnswering"),c(hj,"href","/docs/transformers/pr_15770/en/model_doc/big_bird#transformers.BigBirdForQuestionAnswering"),c(pj,"href","/docs/transformers/pr_15770/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusForQuestionAnswering"),c(_j,"href","/docs/transformers/pr_15770/en/model_doc/camembert#transformers.CamembertForQuestionAnswering"),c(uj,"href","/docs/transformers/pr_15770/en/model_doc/canine#transformers.CanineForQuestionAnswering"),c(bj,"href","/docs/transformers/pr_15770/en/model_doc/convbert#transformers.ConvBertForQuestionAnswering"),c(vj,"href","/docs/transformers/pr_15770/en/model_doc/deberta#transformers.DebertaForQuestionAnswering"),c(Tj,"href","/docs/transformers/pr_15770/en/model_doc/deberta-v2#transformers.DebertaV2ForQuestionAnswering"),c(Fj,"href","/docs/transformers/pr_15770/en/model_doc/distilbert#transformers.DistilBertForQuestionAnswering"),c(Cj,"href","/docs/transformers/pr_15770/en/model_doc/electra#transformers.ElectraForQuestionAnswering"),c(Mj,"href","/docs/transformers/pr_15770/en/model_doc/flaubert#transformers.FlaubertForQuestionAnsweringSimple"),c(Ej,"href","/docs/transformers/pr_15770/en/model_doc/fnet#transformers.FNetForQuestionAnswering"),c(yj,"href","/docs/transformers/pr_15770/en/model_doc/funnel#transformers.FunnelForQuestionAnswering"),c(wj,"href","/docs/transformers/pr_15770/en/model_doc/gptj#transformers.GPTJForQuestionAnswering"),c(Aj,"href","/docs/transformers/pr_15770/en/model_doc/ibert#transformers.IBertForQuestionAnswering"),c(Lj,"href","/docs/transformers/pr_15770/en/model_doc/layoutlmv2#transformers.LayoutLMv2ForQuestionAnswering"),c(Bj,"href","/docs/transformers/pr_15770/en/model_doc/led#transformers.LEDForQuestionAnswering"),c(kj,"href","/docs/transformers/pr_15770/en/model_doc/longformer#transformers.LongformerForQuestionAnswering"),c(xj,"href","/docs/transformers/pr_15770/en/model_doc/lxmert#transformers.LxmertForQuestionAnswering"),c(Rj,"href","/docs/transformers/pr_15770/en/model_doc/mbart#transformers.MBartForQuestionAnswering"),c(Sj,"href","/docs/transformers/pr_15770/en/model_doc/megatron-bert#transformers.MegatronBertForQuestionAnswering"),c(Pj,"href","/docs/transformers/pr_15770/en/model_doc/mobilebert#transformers.MobileBertForQuestionAnswering"),c($j,"href","/docs/transformers/pr_15770/en/model_doc/mpnet#transformers.MPNetForQuestionAnswering"),c(Ij,"href","/docs/transformers/pr_15770/en/model_doc/nystromformer#transformers.NystromformerForQuestionAnswering"),c(jj,"href","/docs/transformers/pr_15770/en/model_doc/qdqbert#transformers.QDQBertForQuestionAnswering"),c(Nj,"href","/docs/transformers/pr_15770/en/model_doc/reformer#transformers.ReformerForQuestionAnswering"),c(Dj,"href","/docs/transformers/pr_15770/en/model_doc/rembert#transformers.RemBertForQuestionAnswering"),c(qj,"href","/docs/transformers/pr_15770/en/model_doc/roberta#transformers.RobertaForQuestionAnswering"),c(Gj,"href","/docs/transformers/pr_15770/en/model_doc/roformer#transformers.RoFormerForQuestionAnswering"),c(Oj,"href","/docs/transformers/pr_15770/en/model_doc/splinter#transformers.SplinterForQuestionAnswering"),c(Xj,"href","/docs/transformers/pr_15770/en/model_doc/squeezebert#transformers.SqueezeBertForQuestionAnswering"),c(zj,"href","/docs/transformers/pr_15770/en/model_doc/xlm#transformers.XLMForQuestionAnsweringSimple"),c(Vj,"href","/docs/transformers/pr_15770/en/model_doc/xlm-roberta#transformers.XLMRobertaForQuestionAnswering"),c(Wj,"href","/docs/transformers/pr_15770/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForQuestionAnswering"),c(Qj,"href","/docs/transformers/pr_15770/en/model_doc/xlnet#transformers.XLNetForQuestionAnsweringSimple"),c(Hj,"href","/docs/transformers/pr_15770/en/model_doc/yoso#transformers.YosoForQuestionAnswering"),c(qe,"class","docstring"),c(er,"class","docstring"),c(b5,"id","transformers.AutoModelForTableQuestionAnswering"),c(b5,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(b5,"href","#transformers.AutoModelForTableQuestionAnswering"),c(_d,"class","relative group"),c(Hr,"class","docstring"),c(Uj,"href","/docs/transformers/pr_15770/en/model_doc/tapas#transformers.TapasForQuestionAnswering"),c(Ge,"class","docstring"),c(or,"class","docstring"),c(F5,"id","transformers.AutoModelForImageClassification"),c(F5,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(F5,"href","#transformers.AutoModelForImageClassification"),c(vd,"class","relative group"),c(Ur,"class","docstring"),c(Jj,"href","/docs/transformers/pr_15770/en/model_doc/beit#transformers.BeitForImageClassification"),c(Yj,"href","/docs/transformers/pr_15770/en/model_doc/convnext#transformers.ConvNextForImageClassification"),c(Kj,"href","/docs/transformers/pr_15770/en/model_doc/deit#transformers.DeiTForImageClassification"),c(Zj,"href","/docs/transformers/pr_15770/en/model_doc/deit#transformers.DeiTForImageClassificationWithTeacher"),c(eN,"href","/docs/transformers/pr_15770/en/model_doc/imagegpt#transformers.ImageGPTForImageClassification"),c(oN,"href","/docs/transformers/pr_15770/en/model_doc/perceiver#transformers.PerceiverForImageClassificationLearned"),c(rN,"href","/docs/transformers/pr_15770/en/model_doc/perceiver#transformers.PerceiverForImageClassificationFourier"),c(tN,"href","/docs/transformers/pr_15770/en/model_doc/perceiver#transformers.PerceiverForImageClassificationConvProcessing"),c(aN,"href","/docs/transformers/pr_15770/en/model_doc/poolformer#transformers.PoolFormerForImageClassification"),c(nN,"href","/docs/transformers/pr_15770/en/model_doc/resnet#transformers.ResNetForImageClassification"),c(sN,"href","/docs/transformers/pr_15770/en/model_doc/segformer#transformers.SegformerForImageClassification"),c(lN,"href","/docs/transformers/pr_15770/en/model_doc/swin#transformers.SwinForImageClassification"),c(iN,"href","/docs/transformers/pr_15770/en/model_doc/vit#transformers.ViTForImageClassification"),c(Oe,"class","docstring"),c(rr,"class","docstring"),c(x5,"id","transformers.AutoModelForVision2Seq"),c(x5,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(x5,"href","#transformers.AutoModelForVision2Seq"),c(Cd,"class","relative group"),c(Jr,"class","docstring"),c(dN,"href","/docs/transformers/pr_15770/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderModel"),c(Xe,"class","docstring"),c(tr,"class","docstring"),c(P5,"id","transformers.AutoModelForAudioClassification"),c(P5,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(P5,"href","#transformers.AutoModelForAudioClassification"),c(yd,"class","relative group"),c(Yr,"class","docstring"),c(cN,"href","/docs/transformers/pr_15770/en/model_doc/hubert#transformers.HubertForSequenceClassification"),c(fN,"href","/docs/transformers/pr_15770/en/model_doc/sew#transformers.SEWForSequenceClassification"),c(mN,"href","/docs/transformers/pr_15770/en/model_doc/sew-d#transformers.SEWDForSequenceClassification"),c(gN,"href","/docs/transformers/pr_15770/en/model_doc/unispeech#transformers.UniSpeechForSequenceClassification"),c(hN,"href","/docs/transformers/pr_15770/en/model_doc/unispeech-sat#transformers.UniSpeechSatForSequenceClassification"),c(pN,"href","/docs/transformers/pr_15770/en/model_doc/wav2vec2#transformers.Wav2Vec2ForSequenceClassification"),c(_N,"href","/docs/transformers/pr_15770/en/model_doc/wavlm#transformers.WavLMForSequenceClassification"),c(ze,"class","docstring"),c(ar,"class","docstring"),c(X5,"id","transformers.AutoModelForAudioFrameClassification"),c(X5,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(X5,"href","#transformers.AutoModelForAudioFrameClassification"),c(Ld,"class","relative group"),c(Kr,"class","docstring"),c(uN,"href","/docs/transformers/pr_15770/en/model_doc/unispeech-sat#transformers.UniSpeechSatForAudioFrameClassification"),c(bN,"href","/docs/transformers/pr_15770/en/model_doc/wav2vec2#transformers.Wav2Vec2ForAudioFrameClassification"),c(vN,"href","/docs/transformers/pr_15770/en/model_doc/wavlm#transformers.WavLMForAudioFrameClassification"),c(Ve,"class","docstring"),c(nr,"class","docstring"),c(H5,"id","transformers.AutoModelForCTC"),c(H5,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(H5,"href","#transformers.AutoModelForCTC"),c(Rd,"class","relative group"),c(Zr,"class","docstring"),c(TN,"href","/docs/transformers/pr_15770/en/model_doc/hubert#transformers.HubertForCTC"),c(FN,"href","/docs/transformers/pr_15770/en/model_doc/sew#transformers.SEWForCTC"),c(CN,"href","/docs/transformers/pr_15770/en/model_doc/sew-d#transformers.SEWDForCTC"),c(MN,"href","/docs/transformers/pr_15770/en/model_doc/unispeech#transformers.UniSpeechForCTC"),c(EN,"href","/docs/transformers/pr_15770/en/model_doc/unispeech-sat#transformers.UniSpeechSatForCTC"),c(yN,"href","/docs/transformers/pr_15770/en/model_doc/wav2vec2#transformers.Wav2Vec2ForCTC"),c(wN,"href","/docs/transformers/pr_15770/en/model_doc/wavlm#transformers.WavLMForCTC"),c(We,"class","docstring"),c(sr,"class","docstring"),c(t2,"id","transformers.AutoModelForSpeechSeq2Seq"),c(t2,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(t2,"href","#transformers.AutoModelForSpeechSeq2Seq"),c($d,"class","relative group"),c(et,"class","docstring"),c(AN,"href","/docs/transformers/pr_15770/en/model_doc/speech-encoder-decoder#transformers.SpeechEncoderDecoderModel"),c(LN,"href","/docs/transformers/pr_15770/en/model_doc/speech_to_text#transformers.Speech2TextForConditionalGeneration"),c(Qe,"class","docstring"),c(lr,"class","docstring"),c(l2,"id","transformers.AutoModelForAudioXVector"),c(l2,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(l2,"href","#transformers.AutoModelForAudioXVector"),c(Nd,"class","relative group"),c(ot,"class","docstring"),c(BN,"href","/docs/transformers/pr_15770/en/model_doc/unispeech-sat#transformers.UniSpeechSatForXVector"),c(kN,"href","/docs/transformers/pr_15770/en/model_doc/wav2vec2#transformers.Wav2Vec2ForXVector"),c(xN,"href","/docs/transformers/pr_15770/en/model_doc/wavlm#transformers.WavLMForXVector"),c(He,"class","docstring"),c(ir,"class","docstring"),c(m2,"id","transformers.AutoModelForMaskedImageModeling"),c(m2,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(m2,"href","#transformers.AutoModelForMaskedImageModeling"),c(Od,"class","relative group"),c(rt,"class","docstring"),c(RN,"href","/docs/transformers/pr_15770/en/model_doc/deit#transformers.DeiTForMaskedImageModeling"),c(SN,"href","/docs/transformers/pr_15770/en/model_doc/swin#transformers.SwinForMaskedImageModeling"),c(PN,"href","/docs/transformers/pr_15770/en/model_doc/vit#transformers.ViTForMaskedImageModeling"),c(Ue,"class","docstring"),c(dr,"class","docstring"),c(u2,"id","transformers.AutoModelForObjectDetection"),c(u2,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(u2,"href","#transformers.AutoModelForObjectDetection"),c(Wd,"class","relative group"),c(tt,"class","docstring"),c($N,"href","/docs/transformers/pr_15770/en/model_doc/detr#transformers.DetrForObjectDetection"),c(Je,"class","docstring"),c(cr,"class","docstring"),c(T2,"id","transformers.AutoModelForImageSegmentation"),c(T2,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(T2,"href","#transformers.AutoModelForImageSegmentation"),c(Ud,"class","relative group"),c(at,"class","docstring"),c(IN,"href","/docs/transformers/pr_15770/en/model_doc/detr#transformers.DetrForSegmentation"),c(Ye,"class","docstring"),c(fr,"class","docstring"),c(M2,"id","transformers.AutoModelForSemanticSegmentation"),c(M2,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(M2,"href","#transformers.AutoModelForSemanticSegmentation"),c(Kd,"class","relative group"),c(nt,"class","docstring"),c(jN,"href","/docs/transformers/pr_15770/en/model_doc/beit#transformers.BeitForSemanticSegmentation"),c(NN,"href","/docs/transformers/pr_15770/en/model_doc/segformer#transformers.SegformerForSemanticSegmentation"),c(Ke,"class","docstring"),c(mr,"class","docstring"),c(A2,"id","transformers.TFAutoModel"),c(A2,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(A2,"href","#transformers.TFAutoModel"),c(oc,"class","relative group"),c(st,"class","docstring"),c(DN,"href","/docs/transformers/pr_15770/en/model_doc/albert#transformers.TFAlbertModel"),c(qN,"href","/docs/transformers/pr_15770/en/model_doc/bart#transformers.TFBartModel"),c(GN,"href","/docs/transformers/pr_15770/en/model_doc/bert#transformers.TFBertModel"),c(ON,"href","/docs/transformers/pr_15770/en/model_doc/blenderbot#transformers.TFBlenderbotModel"),c(XN,"href","/docs/transformers/pr_15770/en/model_doc/blenderbot-small#transformers.TFBlenderbotSmallModel"),c(zN,"href","/docs/transformers/pr_15770/en/model_doc/camembert#transformers.TFCamembertModel"),c(VN,"href","/docs/transformers/pr_15770/en/model_doc/clip#transformers.TFCLIPModel"),c(WN,"href","/docs/transformers/pr_15770/en/model_doc/convbert#transformers.TFConvBertModel"),c(QN,"href","/docs/transformers/pr_15770/en/model_doc/ctrl#transformers.TFCTRLModel"),c(HN,"href","/docs/transformers/pr_15770/en/model_doc/deberta#transformers.TFDebertaModel"),c(UN,"href","/docs/transformers/pr_15770/en/model_doc/deberta-v2#transformers.TFDebertaV2Model"),c(JN,"href","/docs/transformers/pr_15770/en/model_doc/distilbert#transformers.TFDistilBertModel"),c(YN,"href","/docs/transformers/pr_15770/en/model_doc/dpr#transformers.TFDPRQuestionEncoder"),c(KN,"href","/docs/transformers/pr_15770/en/model_doc/electra#transformers.TFElectraModel"),c(ZN,"href","/docs/transformers/pr_15770/en/model_doc/flaubert#transformers.TFFlaubertModel"),c(eD,"href","/docs/transformers/pr_15770/en/model_doc/funnel#transformers.TFFunnelModel"),c(oD,"href","/docs/transformers/pr_15770/en/model_doc/funnel#transformers.TFFunnelBaseModel"),c(rD,"href","/docs/transformers/pr_15770/en/model_doc/gpt2#transformers.TFGPT2Model"),c(tD,"href","/docs/transformers/pr_15770/en/model_doc/hubert#transformers.TFHubertModel"),c(aD,"href","/docs/transformers/pr_15770/en/model_doc/layoutlm#transformers.TFLayoutLMModel"),c(nD,"href","/docs/transformers/pr_15770/en/model_doc/led#transformers.TFLEDModel"),c(sD,"href","/docs/transformers/pr_15770/en/model_doc/longformer#transformers.TFLongformerModel"),c(lD,"href","/docs/transformers/pr_15770/en/model_doc/lxmert#transformers.TFLxmertModel"),c(iD,"href","/docs/transformers/pr_15770/en/model_doc/marian#transformers.TFMarianModel"),c(dD,"href","/docs/transformers/pr_15770/en/model_doc/mbart#transformers.TFMBartModel"),c(cD,"href","/docs/transformers/pr_15770/en/model_doc/mobilebert#transformers.TFMobileBertModel"),c(fD,"href","/docs/transformers/pr_15770/en/model_doc/mpnet#transformers.TFMPNetModel"),c(mD,"href","/docs/transformers/pr_15770/en/model_doc/mt5#transformers.TFMT5Model"),c(gD,"href","/docs/transformers/pr_15770/en/model_doc/openai-gpt#transformers.TFOpenAIGPTModel"),c(hD,"href","/docs/transformers/pr_15770/en/model_doc/pegasus#transformers.TFPegasusModel"),c(pD,"href","/docs/transformers/pr_15770/en/model_doc/rembert#transformers.TFRemBertModel"),c(_D,"href","/docs/transformers/pr_15770/en/model_doc/roberta#transformers.TFRobertaModel"),c(uD,"href","/docs/transformers/pr_15770/en/model_doc/roformer#transformers.TFRoFormerModel"),c(bD,"href","/docs/transformers/pr_15770/en/model_doc/speech_to_text#transformers.TFSpeech2TextModel"),c(vD,"href","/docs/transformers/pr_15770/en/model_doc/t5#transformers.TFT5Model"),c(TD,"href","/docs/transformers/pr_15770/en/model_doc/tapas#transformers.TFTapasModel"),c(FD,"href","/docs/transformers/pr_15770/en/model_doc/transfo-xl#transformers.TFTransfoXLModel"),c(CD,"href","/docs/transformers/pr_15770/en/model_doc/vit#transformers.TFViTModel"),c(MD,"href","/docs/transformers/pr_15770/en/model_doc/wav2vec2#transformers.TFWav2Vec2Model"),c(ED,"href","/docs/transformers/pr_15770/en/model_doc/xlm#transformers.TFXLMModel"),c(yD,"href","/docs/transformers/pr_15770/en/model_doc/xlm-roberta#transformers.TFXLMRobertaModel"),c(wD,"href","/docs/transformers/pr_15770/en/model_doc/xlnet#transformers.TFXLNetModel"),c(go,"class","docstring"),c(gr,"class","docstring"),c(hv,"id","transformers.TFAutoModelForPreTraining"),c(hv,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(hv,"href","#transformers.TFAutoModelForPreTraining"),c(ac,"class","relative group"),c(lt,"class","docstring"),c(AD,"href","/docs/transformers/pr_15770/en/model_doc/albert#transformers.TFAlbertForPreTraining"),c(LD,"href","/docs/transformers/pr_15770/en/model_doc/bart#transformers.TFBartForConditionalGeneration"),c(BD,"href","/docs/transformers/pr_15770/en/model_doc/bert#transformers.TFBertForPreTraining"),c(kD,"href","/docs/transformers/pr_15770/en/model_doc/camembert#transformers.TFCamembertForMaskedLM"),c(xD,"href","/docs/transformers/pr_15770/en/model_doc/ctrl#transformers.TFCTRLLMHeadModel"),c(RD,"href","/docs/transformers/pr_15770/en/model_doc/distilbert#transformers.TFDistilBertForMaskedLM"),c(SD,"href","/docs/transformers/pr_15770/en/model_doc/electra#transformers.TFElectraForPreTraining"),c(PD,"href","/docs/transformers/pr_15770/en/model_doc/flaubert#transformers.TFFlaubertWithLMHeadModel"),c($D,"href","/docs/transformers/pr_15770/en/model_doc/funnel#transformers.TFFunnelForPreTraining"),c(ID,"href","/docs/transformers/pr_15770/en/model_doc/gpt2#transformers.TFGPT2LMHeadModel"),c(jD,"href","/docs/transformers/pr_15770/en/model_doc/layoutlm#transformers.TFLayoutLMForMaskedLM"),c(ND,"href","/docs/transformers/pr_15770/en/model_doc/lxmert#transformers.TFLxmertForPreTraining"),c(DD,"href","/docs/transformers/pr_15770/en/model_doc/mobilebert#transformers.TFMobileBertForPreTraining"),c(qD,"href","/docs/transformers/pr_15770/en/model_doc/mpnet#transformers.TFMPNetForMaskedLM"),c(GD,"href","/docs/transformers/pr_15770/en/model_doc/openai-gpt#transformers.TFOpenAIGPTLMHeadModel"),c(OD,"href","/docs/transformers/pr_15770/en/model_doc/roberta#transformers.TFRobertaForMaskedLM"),c(XD,"href","/docs/transformers/pr_15770/en/model_doc/t5#transformers.TFT5ForConditionalGeneration"),c(zD,"href","/docs/transformers/pr_15770/en/model_doc/tapas#transformers.TFTapasForMaskedLM"),c(VD,"href","/docs/transformers/pr_15770/en/model_doc/transfo-xl#transformers.TFTransfoXLLMHeadModel"),c(WD,"href","/docs/transformers/pr_15770/en/model_doc/xlm#transformers.TFXLMWithLMHeadModel"),c(QD,"href","/docs/transformers/pr_15770/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForMaskedLM"),c(HD,"href","/docs/transformers/pr_15770/en/model_doc/xlnet#transformers.TFXLNetLMHeadModel"),c(ho,"class","docstring"),c(hr,"class","docstring"),c(jv,"id","transformers.TFAutoModelForCausalLM"),c(jv,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(jv,"href","#transformers.TFAutoModelForCausalLM"),c(lc,"class","relative group"),c(it,"class","docstring"),c(UD,"href","/docs/transformers/pr_15770/en/model_doc/bert#transformers.TFBertLMHeadModel"),c(JD,"href","/docs/transformers/pr_15770/en/model_doc/ctrl#transformers.TFCTRLLMHeadModel"),c(YD,"href","/docs/transformers/pr_15770/en/model_doc/gpt2#transformers.TFGPT2LMHeadModel"),c(KD,"href","/docs/transformers/pr_15770/en/model_doc/openai-gpt#transformers.TFOpenAIGPTLMHeadModel"),c(ZD,"href","/docs/transformers/pr_15770/en/model_doc/rembert#transformers.TFRemBertForCausalLM"),c(eq,"href","/docs/transformers/pr_15770/en/model_doc/roberta#transformers.TFRobertaForCausalLM"),c(oq,"href","/docs/transformers/pr_15770/en/model_doc/roformer#transformers.TFRoFormerForCausalLM"),c(rq,"href","/docs/transformers/pr_15770/en/model_doc/transfo-xl#transformers.TFTransfoXLLMHeadModel"),c(tq,"href","/docs/transformers/pr_15770/en/model_doc/xlm#transformers.TFXLMWithLMHeadModel"),c(aq,"href","/docs/transformers/pr_15770/en/model_doc/xlnet#transformers.TFXLNetLMHeadModel"),c(po,"class","docstring"),c(pr,"class","docstring"),c(Hv,"id","transformers.TFAutoModelForImageClassification"),c(Hv,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Hv,"href","#transformers.TFAutoModelForImageClassification"),c(cc,"class","relative group"),c(dt,"class","docstring"),c(nq,"href","/docs/transformers/pr_15770/en/model_doc/vit#transformers.TFViTForImageClassification"),c(_o,"class","docstring"),c(_r,"class","docstring"),c(Jv,"id","transformers.TFAutoModelForMaskedLM"),c(Jv,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Jv,"href","#transformers.TFAutoModelForMaskedLM"),c(gc,"class","relative group"),c(ct,"class","docstring"),c(sq,"href","/docs/transformers/pr_15770/en/model_doc/albert#transformers.TFAlbertForMaskedLM"),c(lq,"href","/docs/transformers/pr_15770/en/model_doc/bert#transformers.TFBertForMaskedLM"),c(iq,"href","/docs/transformers/pr_15770/en/model_doc/camembert#transformers.TFCamembertForMaskedLM"),c(dq,"href","/docs/transformers/pr_15770/en/model_doc/convbert#transformers.TFConvBertForMaskedLM"),c(cq,"href","/docs/transformers/pr_15770/en/model_doc/deberta#transformers.TFDebertaForMaskedLM"),c(fq,"href","/docs/transformers/pr_15770/en/model_doc/deberta-v2#transformers.TFDebertaV2ForMaskedLM"),c(mq,"href","/docs/transformers/pr_15770/en/model_doc/distilbert#transformers.TFDistilBertForMaskedLM"),c(gq,"href","/docs/transformers/pr_15770/en/model_doc/electra#transformers.TFElectraForMaskedLM"),c(hq,"href","/docs/transformers/pr_15770/en/model_doc/flaubert#transformers.TFFlaubertWithLMHeadModel"),c(pq,"href","/docs/transformers/pr_15770/en/model_doc/funnel#transformers.TFFunnelForMaskedLM"),c(_q,"href","/docs/transformers/pr_15770/en/model_doc/layoutlm#transformers.TFLayoutLMForMaskedLM"),c(uq,"href","/docs/transformers/pr_15770/en/model_doc/longformer#transformers.TFLongformerForMaskedLM"),c(bq,"href","/docs/transformers/pr_15770/en/model_doc/mobilebert#transformers.TFMobileBertForMaskedLM"),c(vq,"href","/docs/transformers/pr_15770/en/model_doc/mpnet#transformers.TFMPNetForMaskedLM"),c(Tq,"href","/docs/transformers/pr_15770/en/model_doc/rembert#transformers.TFRemBertForMaskedLM"),c(Fq,"href","/docs/transformers/pr_15770/en/model_doc/roberta#transformers.TFRobertaForMaskedLM"),c(Cq,"href","/docs/transformers/pr_15770/en/model_doc/roformer#transformers.TFRoFormerForMaskedLM"),c(Mq,"href","/docs/transformers/pr_15770/en/model_doc/tapas#transformers.TFTapasForMaskedLM"),c(Eq,"href","/docs/transformers/pr_15770/en/model_doc/xlm#transformers.TFXLMWithLMHeadModel"),c(yq,"href","/docs/transformers/pr_15770/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForMaskedLM"),c(uo,"class","docstring"),c(ur,"class","docstring"),c(u0,"id","transformers.TFAutoModelForSeq2SeqLM"),c(u0,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(u0,"href","#transformers.TFAutoModelForSeq2SeqLM"),c(_c,"class","relative group"),c(ft,"class","docstring"),c(wq,"href","/docs/transformers/pr_15770/en/model_doc/bart#transformers.TFBartForConditionalGeneration"),c(Aq,"href","/docs/transformers/pr_15770/en/model_doc/blenderbot#transformers.TFBlenderbotForConditionalGeneration"),c(Lq,"href","/docs/transformers/pr_15770/en/model_doc/blenderbot-small#transformers.TFBlenderbotSmallForConditionalGeneration"),c(Bq,"href","/docs/transformers/pr_15770/en/model_doc/encoder-decoder#transformers.TFEncoderDecoderModel"),c(kq,"href","/docs/transformers/pr_15770/en/model_doc/led#transformers.TFLEDForConditionalGeneration"),c(xq,"href","/docs/transformers/pr_15770/en/model_doc/marian#transformers.TFMarianMTModel"),c(Rq,"href","/docs/transformers/pr_15770/en/model_doc/mbart#transformers.TFMBartForConditionalGeneration"),c(Sq,"href","/docs/transformers/pr_15770/en/model_doc/mt5#transformers.TFMT5ForConditionalGeneration"),c(Pq,"href","/docs/transformers/pr_15770/en/model_doc/pegasus#transformers.TFPegasusForConditionalGeneration"),c($q,"href","/docs/transformers/pr_15770/en/model_doc/t5#transformers.TFT5ForConditionalGeneration"),c(bo,"class","docstring"),c(br,"class","docstring"),c(L0,"id","transformers.TFAutoModelForSequenceClassification"),c(L0,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(L0,"href","#transformers.TFAutoModelForSequenceClassification"),c(vc,"class","relative group"),c(mt,"class","docstring"),c(Iq,"href","/docs/transformers/pr_15770/en/model_doc/albert#transformers.TFAlbertForSequenceClassification"),c(jq,"href","/docs/transformers/pr_15770/en/model_doc/bert#transformers.TFBertForSequenceClassification"),c(Nq,"href","/docs/transformers/pr_15770/en/model_doc/camembert#transformers.TFCamembertForSequenceClassification"),c(Dq,"href","/docs/transformers/pr_15770/en/model_doc/convbert#transformers.TFConvBertForSequenceClassification"),c(qq,"href","/docs/transformers/pr_15770/en/model_doc/ctrl#transformers.TFCTRLForSequenceClassification"),c(Gq,"href","/docs/transformers/pr_15770/en/model_doc/deberta#transformers.TFDebertaForSequenceClassification"),c(Oq,"href","/docs/transformers/pr_15770/en/model_doc/deberta-v2#transformers.TFDebertaV2ForSequenceClassification"),c(Xq,"href","/docs/transformers/pr_15770/en/model_doc/distilbert#transformers.TFDistilBertForSequenceClassification"),c(zq,"href","/docs/transformers/pr_15770/en/model_doc/electra#transformers.TFElectraForSequenceClassification"),c(Vq,"href","/docs/transformers/pr_15770/en/model_doc/flaubert#transformers.TFFlaubertForSequenceClassification"),c(Wq,"href","/docs/transformers/pr_15770/en/model_doc/funnel#transformers.TFFunnelForSequenceClassification"),c(Qq,"href","/docs/transformers/pr_15770/en/model_doc/gpt2#transformers.TFGPT2ForSequenceClassification"),c(Hq,"href","/docs/transformers/pr_15770/en/model_doc/layoutlm#transformers.TFLayoutLMForSequenceClassification"),c(Uq,"href","/docs/transformers/pr_15770/en/model_doc/longformer#transformers.TFLongformerForSequenceClassification"),c(Jq,"href","/docs/transformers/pr_15770/en/model_doc/mobilebert#transformers.TFMobileBertForSequenceClassification"),c(Yq,"href","/docs/transformers/pr_15770/en/model_doc/mpnet#transformers.TFMPNetForSequenceClassification"),c(Kq,"href","/docs/transformers/pr_15770/en/model_doc/openai-gpt#transformers.TFOpenAIGPTForSequenceClassification"),c(Zq,"href","/docs/transformers/pr_15770/en/model_doc/rembert#transformers.TFRemBertForSequenceClassification"),c(eG,"href","/docs/transformers/pr_15770/en/model_doc/roberta#transformers.TFRobertaForSequenceClassification"),c(oG,"href","/docs/transformers/pr_15770/en/model_doc/roformer#transformers.TFRoFormerForSequenceClassification"),c(rG,"href","/docs/transformers/pr_15770/en/model_doc/tapas#transformers.TFTapasForSequenceClassification"),c(tG,"href","/docs/transformers/pr_15770/en/model_doc/transfo-xl#transformers.TFTransfoXLForSequenceClassification"),c(aG,"href","/docs/transformers/pr_15770/en/model_doc/xlm#transformers.TFXLMForSequenceClassification"),c(nG,"href","/docs/transformers/pr_15770/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForSequenceClassification"),c(sG,"href","/docs/transformers/pr_15770/en/model_doc/xlnet#transformers.TFXLNetForSequenceClassification"),c(vo,"class","docstring"),c(vr,"class","docstring"),c(eT,"id","transformers.TFAutoModelForMultipleChoice"),c(eT,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(eT,"href","#transformers.TFAutoModelForMultipleChoice"),c(Cc,"class","relative group"),c(gt,"class","docstring"),c(lG,"href","/docs/transformers/pr_15770/en/model_doc/albert#transformers.TFAlbertForMultipleChoice"),c(iG,"href","/docs/transformers/pr_15770/en/model_doc/bert#transformers.TFBertForMultipleChoice"),c(dG,"href","/docs/transformers/pr_15770/en/model_doc/camembert#transformers.TFCamembertForMultipleChoice"),c(cG,"href","/docs/transformers/pr_15770/en/model_doc/convbert#transformers.TFConvBertForMultipleChoice"),c(fG,"href","/docs/transformers/pr_15770/en/model_doc/distilbert#transformers.TFDistilBertForMultipleChoice"),c(mG,"href","/docs/transformers/pr_15770/en/model_doc/electra#transformers.TFElectraForMultipleChoice"),c(gG,"href","/docs/transformers/pr_15770/en/model_doc/flaubert#transformers.TFFlaubertForMultipleChoice"),c(hG,"href","/docs/transformers/pr_15770/en/model_doc/funnel#transformers.TFFunnelForMultipleChoice"),c(pG,"href","/docs/transformers/pr_15770/en/model_doc/longformer#transformers.TFLongformerForMultipleChoice"),c(_G,"href","/docs/transformers/pr_15770/en/model_doc/mobilebert#transformers.TFMobileBertForMultipleChoice"),c(uG,"href","/docs/transformers/pr_15770/en/model_doc/mpnet#transformers.TFMPNetForMultipleChoice"),c(bG,"href","/docs/transformers/pr_15770/en/model_doc/rembert#transformers.TFRemBertForMultipleChoice"),c(vG,"href","/docs/transformers/pr_15770/en/model_doc/roberta#transformers.TFRobertaForMultipleChoice"),c(TG,"href","/docs/transformers/pr_15770/en/model_doc/roformer#transformers.TFRoFormerForMultipleChoice"),c(FG,"href","/docs/transformers/pr_15770/en/model_doc/xlm#transformers.TFXLMForMultipleChoice"),c(CG,"href","/docs/transformers/pr_15770/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForMultipleChoice"),c(MG,"href","/docs/transformers/pr_15770/en/model_doc/xlnet#transformers.TFXLNetForMultipleChoice"),c(To,"class","docstring"),c(Tr,"class","docstring"),c(bT,"id","transformers.TFAutoModelForTableQuestionAnswering"),c(bT,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(bT,"href","#transformers.TFAutoModelForTableQuestionAnswering"),c(yc,"class","relative group"),c(ht,"class","docstring"),c(EG,"href","/docs/transformers/pr_15770/en/model_doc/tapas#transformers.TFTapasForQuestionAnswering"),c(Fo,"class","docstring"),c(Fr,"class","docstring"),c(TT,"id","transformers.TFAutoModelForTokenClassification"),c(TT,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(TT,"href","#transformers.TFAutoModelForTokenClassification"),c(Lc,"class","relative group"),c(pt,"class","docstring"),c(yG,"href","/docs/transformers/pr_15770/en/model_doc/albert#transformers.TFAlbertForTokenClassification"),c(wG,"href","/docs/transformers/pr_15770/en/model_doc/bert#transformers.TFBertForTokenClassification"),c(AG,"href","/docs/transformers/pr_15770/en/model_doc/camembert#transformers.TFCamembertForTokenClassification"),c(LG,"href","/docs/transformers/pr_15770/en/model_doc/convbert#transformers.TFConvBertForTokenClassification"),c(BG,"href","/docs/transformers/pr_15770/en/model_doc/deberta#transformers.TFDebertaForTokenClassification"),c(kG,"href","/docs/transformers/pr_15770/en/model_doc/deberta-v2#transformers.TFDebertaV2ForTokenClassification"),c(xG,"href","/docs/transformers/pr_15770/en/model_doc/distilbert#transformers.TFDistilBertForTokenClassification"),c(RG,"href","/docs/transformers/pr_15770/en/model_doc/electra#transformers.TFElectraForTokenClassification"),c(SG,"href","/docs/transformers/pr_15770/en/model_doc/flaubert#transformers.TFFlaubertForTokenClassification"),c(PG,"href","/docs/transformers/pr_15770/en/model_doc/funnel#transformers.TFFunnelForTokenClassification"),c($G,"href","/docs/transformers/pr_15770/en/model_doc/layoutlm#transformers.TFLayoutLMForTokenClassification"),c(IG,"href","/docs/transformers/pr_15770/en/model_doc/longformer#transformers.TFLongformerForTokenClassification"),c(jG,"href","/docs/transformers/pr_15770/en/model_doc/mobilebert#transformers.TFMobileBertForTokenClassification"),c(NG,"href","/docs/transformers/pr_15770/en/model_doc/mpnet#transformers.TFMPNetForTokenClassification"),c(DG,"href","/docs/transformers/pr_15770/en/model_doc/rembert#transformers.TFRemBertForTokenClassification"),c(qG,"href","/docs/transformers/pr_15770/en/model_doc/roberta#transformers.TFRobertaForTokenClassification"),c(GG,"href","/docs/transformers/pr_15770/en/model_doc/roformer#transformers.TFRoFormerForTokenClassification"),c(OG,"href","/docs/transformers/pr_15770/en/model_doc/xlm#transformers.TFXLMForTokenClassification"),c(XG,"href","/docs/transformers/pr_15770/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForTokenClassification"),c(zG,"href","/docs/transformers/pr_15770/en/model_doc/xlnet#transformers.TFXLNetForTokenClassification"),c(Co,"class","docstring"),c(Cr,"class","docstring"),c(GT,"id","transformers.TFAutoModelForQuestionAnswering"),c(GT,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(GT,"href","#transformers.TFAutoModelForQuestionAnswering"),c(xc,"class","relative group"),c(_t,"class","docstring"),c(VG,"href","/docs/transformers/pr_15770/en/model_doc/albert#transformers.TFAlbertForQuestionAnswering"),c(WG,"href","/docs/transformers/pr_15770/en/model_doc/bert#transformers.TFBertForQuestionAnswering"),c(QG,"href","/docs/transformers/pr_15770/en/model_doc/camembert#transformers.TFCamembertForQuestionAnswering"),c(HG,"href","/docs/transformers/pr_15770/en/model_doc/convbert#transformers.TFConvBertForQuestionAnswering"),c(UG,"href","/docs/transformers/pr_15770/en/model_doc/deberta#transformers.TFDebertaForQuestionAnswering"),c(JG,"href","/docs/transformers/pr_15770/en/model_doc/deberta-v2#transformers.TFDebertaV2ForQuestionAnswering"),c(YG,"href","/docs/transformers/pr_15770/en/model_doc/distilbert#transformers.TFDistilBertForQuestionAnswering"),c(KG,"href","/docs/transformers/pr_15770/en/model_doc/electra#transformers.TFElectraForQuestionAnswering"),c(ZG,"href","/docs/transformers/pr_15770/en/model_doc/flaubert#transformers.TFFlaubertForQuestionAnsweringSimple"),c(eO,"href","/docs/transformers/pr_15770/en/model_doc/funnel#transformers.TFFunnelForQuestionAnswering"),c(oO,"href","/docs/transformers/pr_15770/en/model_doc/longformer#transformers.TFLongformerForQuestionAnswering"),c(rO,"href","/docs/transformers/pr_15770/en/model_doc/mobilebert#transformers.TFMobileBertForQuestionAnswering"),c(tO,"href","/docs/transformers/pr_15770/en/model_doc/mpnet#transformers.TFMPNetForQuestionAnswering"),c(aO,"href","/docs/transformers/pr_15770/en/model_doc/rembert#transformers.TFRemBertForQuestionAnswering"),c(nO,"href","/docs/transformers/pr_15770/en/model_doc/roberta#transformers.TFRobertaForQuestionAnswering"),c(sO,"href","/docs/transformers/pr_15770/en/model_doc/roformer#transformers.TFRoFormerForQuestionAnswering"),c(lO,"href","/docs/transformers/pr_15770/en/model_doc/xlm#transformers.TFXLMForQuestionAnsweringSimple"),c(iO,"href","/docs/transformers/pr_15770/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForQuestionAnswering"),c(dO,"href","/docs/transformers/pr_15770/en/model_doc/xlnet#transformers.TFXLNetForQuestionAnsweringSimple"),c(Mo,"class","docstring"),c(Mr,"class","docstring"),c(lF,"id","transformers.TFAutoModelForVision2Seq"),c(lF,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(lF,"href","#transformers.TFAutoModelForVision2Seq"),c(Pc,"class","relative group"),c(ut,"class","docstring"),c(cO,"href","/docs/transformers/pr_15770/en/model_doc/vision-encoder-decoder#transformers.TFVisionEncoderDecoderModel"),c(Eo,"class","docstring"),c(Er,"class","docstring"),c(dF,"id","transformers.TFAutoModelForSpeechSeq2Seq"),c(dF,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(dF,"href","#transformers.TFAutoModelForSpeechSeq2Seq"),c(jc,"class","relative group"),c(bt,"class","docstring"),c(fO,"href","/docs/transformers/pr_15770/en/model_doc/speech_to_text#transformers.TFSpeech2TextForConditionalGeneration"),c(yo,"class","docstring"),c(yr,"class","docstring"),c(fF,"id","transformers.FlaxAutoModel"),c(fF,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(fF,"href","#transformers.FlaxAutoModel"),c(qc,"class","relative group"),c(vt,"class","docstring"),c(mO,"href","/docs/transformers/pr_15770/en/model_doc/albert#transformers.FlaxAlbertModel"),c(gO,"href","/docs/transformers/pr_15770/en/model_doc/bart#transformers.FlaxBartModel"),c(hO,"href","/docs/transformers/pr_15770/en/model_doc/beit#transformers.FlaxBeitModel"),c(pO,"href","/docs/transformers/pr_15770/en/model_doc/bert#transformers.FlaxBertModel"),c(_O,"href","/docs/transformers/pr_15770/en/model_doc/big_bird#transformers.FlaxBigBirdModel"),c(uO,"href","/docs/transformers/pr_15770/en/model_doc/blenderbot#transformers.FlaxBlenderbotModel"),c(bO,"href","/docs/transformers/pr_15770/en/model_doc/blenderbot-small#transformers.FlaxBlenderbotSmallModel"),c(vO,"href","/docs/transformers/pr_15770/en/model_doc/clip#transformers.FlaxCLIPModel"),c(TO,"href","/docs/transformers/pr_15770/en/model_doc/distilbert#transformers.FlaxDistilBertModel"),c(FO,"href","/docs/transformers/pr_15770/en/model_doc/electra#transformers.FlaxElectraModel"),c(CO,"href","/docs/transformers/pr_15770/en/model_doc/gpt2#transformers.FlaxGPT2Model"),c(MO,"href","/docs/transformers/pr_15770/en/model_doc/gpt_neo#transformers.FlaxGPTNeoModel"),c(EO,"href","/docs/transformers/pr_15770/en/model_doc/gptj#transformers.FlaxGPTJModel"),c(yO,"href","/docs/transformers/pr_15770/en/model_doc/marian#transformers.FlaxMarianModel"),c(wO,"href","/docs/transformers/pr_15770/en/model_doc/mbart#transformers.FlaxMBartModel"),c(AO,"href","/docs/transformers/pr_15770/en/model_doc/mt5#transformers.FlaxMT5Model"),c(LO,"href","/docs/transformers/pr_15770/en/model_doc/pegasus#transformers.FlaxPegasusModel"),c(BO,"href","/docs/transformers/pr_15770/en/model_doc/roberta#transformers.FlaxRobertaModel"),c(kO,"href","/docs/transformers/pr_15770/en/model_doc/roformer#transformers.FlaxRoFormerModel"),c(xO,"href","/docs/transformers/pr_15770/en/model_doc/t5#transformers.FlaxT5Model"),c(RO,"href","/docs/transformers/pr_15770/en/model_doc/vision-text-dual-encoder#transformers.FlaxVisionTextDualEncoderModel"),c(SO,"href","/docs/transformers/pr_15770/en/model_doc/vit#transformers.FlaxViTModel"),c(PO,"href","/docs/transformers/pr_15770/en/model_doc/wav2vec2#transformers.FlaxWav2Vec2Model"),c($O,"href","/docs/transformers/pr_15770/en/model_doc/xglm#transformers.FlaxXGLMModel"),c(wo,"class","docstring"),c(wr,"class","docstring"),c(IF,"id","transformers.FlaxAutoModelForCausalLM"),c(IF,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(IF,"href","#transformers.FlaxAutoModelForCausalLM"),c(Xc,"class","relative group"),c(Tt,"class","docstring"),c(IO,"href","/docs/transformers/pr_15770/en/model_doc/gpt2#transformers.FlaxGPT2LMHeadModel"),c(jO,"href","/docs/transformers/pr_15770/en/model_doc/gpt_neo#transformers.FlaxGPTNeoForCausalLM"),c(NO,"href","/docs/transformers/pr_15770/en/model_doc/gptj#transformers.FlaxGPTJForCausalLM"),c(DO,"href","/docs/transformers/pr_15770/en/model_doc/xglm#transformers.FlaxXGLMForCausalLM"),c(Ao,"class","docstring"),c(Ar,"class","docstring"),c(GF,"id","transformers.FlaxAutoModelForPreTraining"),c(GF,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(GF,"href","#transformers.FlaxAutoModelForPreTraining"),c(Wc,"class","relative group"),c(Ft,"class","docstring"),c(qO,"href","/docs/transformers/pr_15770/en/model_doc/albert#transformers.FlaxAlbertForPreTraining"),c(GO,"href","/docs/transformers/pr_15770/en/model_doc/bart#transformers.FlaxBartForConditionalGeneration"),c(OO,"href","/docs/transformers/pr_15770/en/model_doc/bert#transformers.FlaxBertForPreTraining"),c(XO,"href","/docs/transformers/pr_15770/en/model_doc/big_bird#transformers.FlaxBigBirdForPreTraining"),c(zO,"href","/docs/transformers/pr_15770/en/model_doc/electra#transformers.FlaxElectraForPreTraining"),c(VO,"href","/docs/transformers/pr_15770/en/model_doc/mbart#transformers.FlaxMBartForConditionalGeneration"),c(WO,"href","/docs/transformers/pr_15770/en/model_doc/mt5#transformers.FlaxMT5ForConditionalGeneration"),c(QO,"href","/docs/transformers/pr_15770/en/model_doc/roberta#transformers.FlaxRobertaForMaskedLM"),c(HO,"href","/docs/transformers/pr_15770/en/model_doc/roformer#transformers.FlaxRoFormerForMaskedLM"),c(UO,"href","/docs/transformers/pr_15770/en/model_doc/t5#transformers.FlaxT5ForConditionalGeneration"),c(JO,"href","/docs/transformers/pr_15770/en/model_doc/wav2vec2#transformers.FlaxWav2Vec2ForPreTraining"),c(Lo,"class","docstring"),c(Lr,"class","docstring"),c(ZF,"id","transformers.FlaxAutoModelForMaskedLM"),c(ZF,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(ZF,"href","#transformers.FlaxAutoModelForMaskedLM"),c(Uc,"class","relative group"),c(Ct,"class","docstring"),c(YO,"href","/docs/transformers/pr_15770/en/model_doc/albert#transformers.FlaxAlbertForMaskedLM"),c(KO,"href","/docs/transformers/pr_15770/en/model_doc/bart#transformers.FlaxBartForConditionalGeneration"),c(ZO,"href","/docs/transformers/pr_15770/en/model_doc/bert#transformers.FlaxBertForMaskedLM"),c(eX,"href","/docs/transformers/pr_15770/en/model_doc/big_bird#transformers.FlaxBigBirdForMaskedLM"),c(oX,"href","/docs/transformers/pr_15770/en/model_doc/distilbert#transformers.FlaxDistilBertForMaskedLM"),c(rX,"href","/docs/transformers/pr_15770/en/model_doc/electra#transformers.FlaxElectraForMaskedLM"),c(tX,"href","/docs/transformers/pr_15770/en/model_doc/mbart#transformers.FlaxMBartForConditionalGeneration"),c(aX,"href","/docs/transformers/pr_15770/en/model_doc/roberta#transformers.FlaxRobertaForMaskedLM"),c(nX,"href","/docs/transformers/pr_15770/en/model_doc/roformer#transformers.FlaxRoFormerForMaskedLM"),c(Bo,"class","docstring"),c(Br,"class","docstring"),c(dC,"id","transformers.FlaxAutoModelForSeq2SeqLM"),c(dC,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(dC,"href","#transformers.FlaxAutoModelForSeq2SeqLM"),c(Kc,"class","relative group"),c(Mt,"class","docstring"),c(sX,"href","/docs/transformers/pr_15770/en/model_doc/bart#transformers.FlaxBartForConditionalGeneration"),c(lX,"href","/docs/transformers/pr_15770/en/model_doc/blenderbot#transformers.FlaxBlenderbotForConditionalGeneration"),c(iX,"href","/docs/transformers/pr_15770/en/model_doc/blenderbot-small#transformers.FlaxBlenderbotSmallForConditionalGeneration"),c(dX,"href","/docs/transformers/pr_15770/en/model_doc/encoder-decoder#transformers.FlaxEncoderDecoderModel"),c(cX,"href","/docs/transformers/pr_15770/en/model_doc/marian#transformers.FlaxMarianMTModel"),c(fX,"href","/docs/transformers/pr_15770/en/model_doc/mbart#transformers.FlaxMBartForConditionalGeneration"),c(mX,"href","/docs/transformers/pr_15770/en/model_doc/mt5#transformers.FlaxMT5ForConditionalGeneration"),c(gX,"href","/docs/transformers/pr_15770/en/model_doc/pegasus#transformers.FlaxPegasusForConditionalGeneration"),c(hX,"href","/docs/transformers/pr_15770/en/model_doc/t5#transformers.FlaxT5ForConditionalGeneration"),c(ko,"class","docstring"),c(kr,"class","docstring"),c(vC,"id","transformers.FlaxAutoModelForSequenceClassification"),c(vC,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(vC,"href","#transformers.FlaxAutoModelForSequenceClassification"),c(of,"class","relative group"),c(Et,"class","docstring"),c(pX,"href","/docs/transformers/pr_15770/en/model_doc/albert#transformers.FlaxAlbertForSequenceClassification"),c(_X,"href","/docs/transformers/pr_15770/en/model_doc/bart#transformers.FlaxBartForSequenceClassification"),c(uX,"href","/docs/transformers/pr_15770/en/model_doc/bert#transformers.FlaxBertForSequenceClassification"),c(bX,"href","/docs/transformers/pr_15770/en/model_doc/big_bird#transformers.FlaxBigBirdForSequenceClassification"),c(vX,"href","/docs/transformers/pr_15770/en/model_doc/distilbert#transformers.FlaxDistilBertForSequenceClassification"),c(TX,"href","/docs/transformers/pr_15770/en/model_doc/electra#transformers.FlaxElectraForSequenceClassification"),c(FX,"href","/docs/transformers/pr_15770/en/model_doc/mbart#transformers.FlaxMBartForSequenceClassification"),c(CX,"href","/docs/transformers/pr_15770/en/model_doc/roberta#transformers.FlaxRobertaForSequenceClassification"),c(MX,"href","/docs/transformers/pr_15770/en/model_doc/roformer#transformers.FlaxRoFormerForSequenceClassification"),c(xo,"class","docstring"),c(xr,"class","docstring"),c(BC,"id","transformers.FlaxAutoModelForQuestionAnswering"),c(BC,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(BC,"href","#transformers.FlaxAutoModelForQuestionAnswering"),c(af,"class","relative group"),c(yt,"class","docstring"),c(EX,"href","/docs/transformers/pr_15770/en/model_doc/albert#transformers.FlaxAlbertForQuestionAnswering"),c(yX,"href","/docs/transformers/pr_15770/en/model_doc/bart#transformers.FlaxBartForQuestionAnswering"),c(wX,"href","/docs/transformers/pr_15770/en/model_doc/bert#transformers.FlaxBertForQuestionAnswering"),c(AX,"href","/docs/transformers/pr_15770/en/model_doc/big_bird#transformers.FlaxBigBirdForQuestionAnswering"),c(LX,"href","/docs/transformers/pr_15770/en/model_doc/distilbert#transformers.FlaxDistilBertForQuestionAnswering"),c(BX,"href","/docs/transformers/pr_15770/en/model_doc/electra#transformers.FlaxElectraForQuestionAnswering"),c(kX,"href","/docs/transformers/pr_15770/en/model_doc/mbart#transformers.FlaxMBartForQuestionAnswering"),c(xX,"href","/docs/transformers/pr_15770/en/model_doc/roberta#transformers.FlaxRobertaForQuestionAnswering"),c(RX,"href","/docs/transformers/pr_15770/en/model_doc/roformer#transformers.FlaxRoFormerForQuestionAnswering"),c(Ro,"class","docstring"),c(Rr,"class","docstring"),c(DC,"id","transformers.FlaxAutoModelForTokenClassification"),c(DC,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(DC,"href","#transformers.FlaxAutoModelForTokenClassification"),c(lf,"class","relative group"),c(wt,"class","docstring"),c(SX,"href","/docs/transformers/pr_15770/en/model_doc/albert#transformers.FlaxAlbertForTokenClassification"),c(PX,"href","/docs/transformers/pr_15770/en/model_doc/bert#transformers.FlaxBertForTokenClassification"),c($X,"href","/docs/transformers/pr_15770/en/model_doc/big_bird#transformers.FlaxBigBirdForTokenClassification"),c(IX,"href","/docs/transformers/pr_15770/en/model_doc/distilbert#transformers.FlaxDistilBertForTokenClassification"),c(jX,"href","/docs/transformers/pr_15770/en/model_doc/electra#transformers.FlaxElectraForTokenClassification"),c(NX,"href","/docs/transformers/pr_15770/en/model_doc/roberta#transformers.FlaxRobertaForTokenClassification"),c(DX,"href","/docs/transformers/pr_15770/en/model_doc/roformer#transformers.FlaxRoFormerForTokenClassification"),c(So,"class","docstring"),c(Sr,"class","docstring"),c(QC,"id","transformers.FlaxAutoModelForMultipleChoice"),c(QC,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(QC,"href","#transformers.FlaxAutoModelForMultipleChoice"),c(ff,"class","relative group"),c(At,"class","docstring"),c(qX,"href","/docs/transformers/pr_15770/en/model_doc/albert#transformers.FlaxAlbertForMultipleChoice"),c(GX,"href","/docs/transformers/pr_15770/en/model_doc/bert#transformers.FlaxBertForMultipleChoice"),c(OX,"href","/docs/transformers/pr_15770/en/model_doc/big_bird#transformers.FlaxBigBirdForMultipleChoice"),c(XX,"href","/docs/transformers/pr_15770/en/model_doc/distilbert#transformers.FlaxDistilBertForMultipleChoice"),c(zX,"href","/docs/transformers/pr_15770/en/model_doc/electra#transformers.FlaxElectraForMultipleChoice"),c(VX,"href","/docs/transformers/pr_15770/en/model_doc/roberta#transformers.FlaxRobertaForMultipleChoice"),c(WX,"href","/docs/transformers/pr_15770/en/model_doc/roformer#transformers.FlaxRoFormerForMultipleChoice"),c(Po,"class","docstring"),c(Pr,"class","docstring"),c(o4,"id","transformers.FlaxAutoModelForNextSentencePrediction"),c(o4,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(o4,"href","#transformers.FlaxAutoModelForNextSentencePrediction"),c(hf,"class","relative group"),c(Lt,"class","docstring"),c(QX,"href","/docs/transformers/pr_15770/en/model_doc/bert#transformers.FlaxBertForNextSentencePrediction"),c($o,"class","docstring"),c($r,"class","docstring"),c(t4,"id","transformers.FlaxAutoModelForImageClassification"),c(t4,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(t4,"href","#transformers.FlaxAutoModelForImageClassification"),c(uf,"class","relative group"),c(Bt,"class","docstring"),c(HX,"href","/docs/transformers/pr_15770/en/model_doc/beit#transformers.FlaxBeitForImageClassification"),c(UX,"href","/docs/transformers/pr_15770/en/model_doc/vit#transformers.FlaxViTForImageClassification"),c(Io,"class","docstring"),c(Ir,"class","docstring"),c(s4,"id","transformers.FlaxAutoModelForVision2Seq"),c(s4,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(s4,"href","#transformers.FlaxAutoModelForVision2Seq"),c(Tf,"class","relative group"),c(kt,"class","docstring"),c(JX,"href","/docs/transformers/pr_15770/en/model_doc/vision-encoder-decoder#transformers.FlaxVisionEncoderDecoderModel"),c(jo,"class","docstring"),c(jr,"class","docstring")},m(d,u){e(document.head,J),b(d,Le,u),b(d,de,u),e(de,me),e(me,to),g(ce,to,null),e(de,be),e(de,Do),e(Do,wi),b(d,Ef,u),b(d,sa,u),e(sa,Ai),e(sa,Li),e(Li,nM),e(sa,yf),b(d,we,u),b(d,io,u),e(io,Bi),e(io,Pn),e(Pn,sM),e(io,$n),e(io,In),e(In,lM),e(io,ki),e(io,jn),e(jn,iM),e(io,xi),b(d,wf,u),g($a,d,u),b(d,co,u),b(d,ge,u),e(ge,XL),e(ge,Ri),e(Ri,zL),e(ge,VL),b(d,qo,u),b(d,Ia,u),e(Ia,WL),e(Ia,Af),e(Af,QL),e(Ia,Lxe),b(d,v8e,u),b(d,Si,u),e(Si,Lf),e(Lf,XV),g(dM,XV,null),e(Si,Bxe),e(Si,zV),e(zV,kxe),b(d,T8e,u),b(d,Nn,u),e(Nn,xxe),e(Nn,VV),e(VV,Rxe),e(Nn,Sxe),e(Nn,WV),e(WV,Pxe),e(Nn,$xe),b(d,F8e,u),g(cM,d,u),b(d,C8e,u),b(d,HL,u),e(HL,Ixe),b(d,M8e,u),g(Bf,d,u),b(d,E8e,u),b(d,Pi,u),e(Pi,kf),e(kf,QV),g(fM,QV,null),e(Pi,jxe),e(Pi,HV),e(HV,Nxe),b(d,y8e,u),b(d,Go,u),g(mM,Go,null),e(Go,Dxe),e(Go,gM),e(gM,qxe),e(gM,UL),e(UL,Gxe),e(gM,Oxe),e(Go,Xxe),e(Go,hM),e(hM,zxe),e(hM,UV),e(UV,Vxe),e(hM,Wxe),e(Go,Qxe),e(Go,fo),g(pM,fo,null),e(fo,Hxe),e(fo,JV),e(JV,Uxe),e(fo,Jxe),e(fo,$i),e($i,Yxe),e($i,YV),e(YV,Kxe),e($i,Zxe),e($i,KV),e(KV,eRe),e($i,oRe),e(fo,rRe),e(fo,v),e(v,xf),e(xf,ZV),e(ZV,tRe),e(xf,aRe),e(xf,JL),e(JL,nRe),e(xf,sRe),e(v,lRe),e(v,Rf),e(Rf,eW),e(eW,iRe),e(Rf,dRe),e(Rf,YL),e(YL,cRe),e(Rf,fRe),e(v,mRe),e(v,Sf),e(Sf,oW),e(oW,gRe),e(Sf,hRe),e(Sf,KL),e(KL,pRe),e(Sf,_Re),e(v,uRe),e(v,Pf),e(Pf,rW),e(rW,bRe),e(Pf,vRe),e(Pf,ZL),e(ZL,TRe),e(Pf,FRe),e(v,CRe),e(v,$f),e($f,tW),e(tW,MRe),e($f,ERe),e($f,e8),e(e8,yRe),e($f,wRe),e(v,ARe),e(v,If),e(If,aW),e(aW,LRe),e(If,BRe),e(If,o8),e(o8,kRe),e(If,xRe),e(v,RRe),e(v,jf),e(jf,nW),e(nW,SRe),e(jf,PRe),e(jf,r8),e(r8,$Re),e(jf,IRe),e(v,jRe),e(v,Nf),e(Nf,sW),e(sW,NRe),e(Nf,DRe),e(Nf,t8),e(t8,qRe),e(Nf,GRe),e(v,ORe),e(v,Df),e(Df,lW),e(lW,XRe),e(Df,zRe),e(Df,a8),e(a8,VRe),e(Df,WRe),e(v,QRe),e(v,qf),e(qf,iW),e(iW,HRe),e(qf,URe),e(qf,n8),e(n8,JRe),e(qf,YRe),e(v,KRe),e(v,Gf),e(Gf,dW),e(dW,ZRe),e(Gf,eSe),e(Gf,s8),e(s8,oSe),e(Gf,rSe),e(v,tSe),e(v,Of),e(Of,cW),e(cW,aSe),e(Of,nSe),e(Of,l8),e(l8,sSe),e(Of,lSe),e(v,iSe),e(v,Xf),e(Xf,fW),e(fW,dSe),e(Xf,cSe),e(Xf,i8),e(i8,fSe),e(Xf,mSe),e(v,gSe),e(v,zf),e(zf,mW),e(mW,hSe),e(zf,pSe),e(zf,d8),e(d8,_Se),e(zf,uSe),e(v,bSe),e(v,Vf),e(Vf,gW),e(gW,vSe),e(Vf,TSe),e(Vf,c8),e(c8,FSe),e(Vf,CSe),e(v,MSe),e(v,Wf),e(Wf,hW),e(hW,ESe),e(Wf,ySe),e(Wf,f8),e(f8,wSe),e(Wf,ASe),e(v,LSe),e(v,Qf),e(Qf,pW),e(pW,BSe),e(Qf,kSe),e(Qf,m8),e(m8,xSe),e(Qf,RSe),e(v,SSe),e(v,Hf),e(Hf,_W),e(_W,PSe),e(Hf,$Se),e(Hf,g8),e(g8,ISe),e(Hf,jSe),e(v,NSe),e(v,Uf),e(Uf,uW),e(uW,DSe),e(Uf,qSe),e(Uf,h8),e(h8,GSe),e(Uf,OSe),e(v,XSe),e(v,Jf),e(Jf,bW),e(bW,zSe),e(Jf,VSe),e(Jf,p8),e(p8,WSe),e(Jf,QSe),e(v,HSe),e(v,Yf),e(Yf,vW),e(vW,USe),e(Yf,JSe),e(Yf,_8),e(_8,YSe),e(Yf,KSe),e(v,ZSe),e(v,Kf),e(Kf,TW),e(TW,ePe),e(Kf,oPe),e(Kf,u8),e(u8,rPe),e(Kf,tPe),e(v,aPe),e(v,Zf),e(Zf,FW),e(FW,nPe),e(Zf,sPe),e(Zf,b8),e(b8,lPe),e(Zf,iPe),e(v,dPe),e(v,em),e(em,CW),e(CW,cPe),e(em,fPe),e(em,v8),e(v8,mPe),e(em,gPe),e(v,hPe),e(v,om),e(om,MW),e(MW,pPe),e(om,_Pe),e(om,T8),e(T8,uPe),e(om,bPe),e(v,vPe),e(v,rm),e(rm,EW),e(EW,TPe),e(rm,FPe),e(rm,F8),e(F8,CPe),e(rm,MPe),e(v,EPe),e(v,tm),e(tm,yW),e(yW,yPe),e(tm,wPe),e(tm,C8),e(C8,APe),e(tm,LPe),e(v,BPe),e(v,am),e(am,wW),e(wW,kPe),e(am,xPe),e(am,M8),e(M8,RPe),e(am,SPe),e(v,PPe),e(v,nm),e(nm,AW),e(AW,$Pe),e(nm,IPe),e(nm,E8),e(E8,jPe),e(nm,NPe),e(v,DPe),e(v,sm),e(sm,LW),e(LW,qPe),e(sm,GPe),e(sm,y8),e(y8,OPe),e(sm,XPe),e(v,zPe),e(v,lm),e(lm,BW),e(BW,VPe),e(lm,WPe),e(lm,w8),e(w8,QPe),e(lm,HPe),e(v,UPe),e(v,im),e(im,kW),e(kW,JPe),e(im,YPe),e(im,A8),e(A8,KPe),e(im,ZPe),e(v,e$e),e(v,dm),e(dm,xW),e(xW,o$e),e(dm,r$e),e(dm,L8),e(L8,t$e),e(dm,a$e),e(v,n$e),e(v,cm),e(cm,RW),e(RW,s$e),e(cm,l$e),e(cm,B8),e(B8,i$e),e(cm,d$e),e(v,c$e),e(v,fm),e(fm,SW),e(SW,f$e),e(fm,m$e),e(fm,k8),e(k8,g$e),e(fm,h$e),e(v,p$e),e(v,mm),e(mm,PW),e(PW,_$e),e(mm,u$e),e(mm,x8),e(x8,b$e),e(mm,v$e),e(v,T$e),e(v,gm),e(gm,$W),e($W,F$e),e(gm,C$e),e(gm,R8),e(R8,M$e),e(gm,E$e),e(v,y$e),e(v,hm),e(hm,IW),e(IW,w$e),e(hm,A$e),e(hm,S8),e(S8,L$e),e(hm,B$e),e(v,k$e),e(v,pm),e(pm,jW),e(jW,x$e),e(pm,R$e),e(pm,P8),e(P8,S$e),e(pm,P$e),e(v,$$e),e(v,_m),e(_m,NW),e(NW,I$e),e(_m,j$e),e(_m,$8),e($8,N$e),e(_m,D$e),e(v,q$e),e(v,um),e(um,DW),e(DW,G$e),e(um,O$e),e(um,I8),e(I8,X$e),e(um,z$e),e(v,V$e),e(v,bm),e(bm,qW),e(qW,W$e),e(bm,Q$e),e(bm,j8),e(j8,H$e),e(bm,U$e),e(v,J$e),e(v,vm),e(vm,GW),e(GW,Y$e),e(vm,K$e),e(vm,N8),e(N8,Z$e),e(vm,eIe),e(v,oIe),e(v,Tm),e(Tm,OW),e(OW,rIe),e(Tm,tIe),e(Tm,D8),e(D8,aIe),e(Tm,nIe),e(v,sIe),e(v,Fm),e(Fm,XW),e(XW,lIe),e(Fm,iIe),e(Fm,q8),e(q8,dIe),e(Fm,cIe),e(v,fIe),e(v,Cm),e(Cm,zW),e(zW,mIe),e(Cm,gIe),e(Cm,G8),e(G8,hIe),e(Cm,pIe),e(v,_Ie),e(v,Mm),e(Mm,VW),e(VW,uIe),e(Mm,bIe),e(Mm,O8),e(O8,vIe),e(Mm,TIe),e(v,FIe),e(v,Em),e(Em,WW),e(WW,CIe),e(Em,MIe),e(Em,X8),e(X8,EIe),e(Em,yIe),e(v,wIe),e(v,ym),e(ym,QW),e(QW,AIe),e(ym,LIe),e(ym,z8),e(z8,BIe),e(ym,kIe),e(v,xIe),e(v,wm),e(wm,HW),e(HW,RIe),e(wm,SIe),e(wm,V8),e(V8,PIe),e(wm,$Ie),e(v,IIe),e(v,Am),e(Am,UW),e(UW,jIe),e(Am,NIe),e(Am,W8),e(W8,DIe),e(Am,qIe),e(v,GIe),e(v,Lm),e(Lm,JW),e(JW,OIe),e(Lm,XIe),e(Lm,Q8),e(Q8,zIe),e(Lm,VIe),e(v,WIe),e(v,Bm),e(Bm,YW),e(YW,QIe),e(Bm,HIe),e(Bm,H8),e(H8,UIe),e(Bm,JIe),e(v,YIe),e(v,km),e(km,KW),e(KW,KIe),e(km,ZIe),e(km,U8),e(U8,eje),e(km,oje),e(v,rje),e(v,xm),e(xm,ZW),e(ZW,tje),e(xm,aje),e(xm,J8),e(J8,nje),e(xm,sje),e(v,lje),e(v,Rm),e(Rm,eQ),e(eQ,ije),e(Rm,dje),e(Rm,Y8),e(Y8,cje),e(Rm,fje),e(v,mje),e(v,Sm),e(Sm,oQ),e(oQ,gje),e(Sm,hje),e(Sm,K8),e(K8,pje),e(Sm,_je),e(v,uje),e(v,Pm),e(Pm,rQ),e(rQ,bje),e(Pm,vje),e(Pm,Z8),e(Z8,Tje),e(Pm,Fje),e(v,Cje),e(v,$m),e($m,tQ),e(tQ,Mje),e($m,Eje),e($m,e9),e(e9,yje),e($m,wje),e(v,Aje),e(v,Im),e(Im,aQ),e(aQ,Lje),e(Im,Bje),e(Im,o9),e(o9,kje),e(Im,xje),e(v,Rje),e(v,jm),e(jm,nQ),e(nQ,Sje),e(jm,Pje),e(jm,r9),e(r9,$je),e(jm,Ije),e(v,jje),e(v,Nm),e(Nm,sQ),e(sQ,Nje),e(Nm,Dje),e(Nm,t9),e(t9,qje),e(Nm,Gje),e(v,Oje),e(v,Dm),e(Dm,lQ),e(lQ,Xje),e(Dm,zje),e(Dm,a9),e(a9,Vje),e(Dm,Wje),e(v,Qje),e(v,qm),e(qm,iQ),e(iQ,Hje),e(qm,Uje),e(qm,n9),e(n9,Jje),e(qm,Yje),e(v,Kje),e(v,Gm),e(Gm,dQ),e(dQ,Zje),e(Gm,eNe),e(Gm,s9),e(s9,oNe),e(Gm,rNe),e(v,tNe),e(v,Om),e(Om,cQ),e(cQ,aNe),e(Om,nNe),e(Om,l9),e(l9,sNe),e(Om,lNe),e(v,iNe),e(v,Xm),e(Xm,fQ),e(fQ,dNe),e(Xm,cNe),e(Xm,i9),e(i9,fNe),e(Xm,mNe),e(v,gNe),e(v,zm),e(zm,mQ),e(mQ,hNe),e(zm,pNe),e(zm,d9),e(d9,_Ne),e(zm,uNe),e(v,bNe),e(v,Vm),e(Vm,gQ),e(gQ,vNe),e(Vm,TNe),e(Vm,c9),e(c9,FNe),e(Vm,CNe),e(v,MNe),e(v,Wm),e(Wm,hQ),e(hQ,ENe),e(Wm,yNe),e(Wm,f9),e(f9,wNe),e(Wm,ANe),e(v,LNe),e(v,Qm),e(Qm,pQ),e(pQ,BNe),e(Qm,kNe),e(Qm,m9),e(m9,xNe),e(Qm,RNe),e(v,SNe),e(v,Hm),e(Hm,_Q),e(_Q,PNe),e(Hm,$Ne),e(Hm,g9),e(g9,INe),e(Hm,jNe),e(v,NNe),e(v,Um),e(Um,uQ),e(uQ,DNe),e(Um,qNe),e(Um,h9),e(h9,GNe),e(Um,ONe),e(v,XNe),e(v,Jm),e(Jm,bQ),e(bQ,zNe),e(Jm,VNe),e(Jm,p9),e(p9,WNe),e(Jm,QNe),e(v,HNe),e(v,Ym),e(Ym,vQ),e(vQ,UNe),e(Ym,JNe),e(Ym,_9),e(_9,YNe),e(Ym,KNe),e(v,ZNe),e(v,Km),e(Km,TQ),e(TQ,eDe),e(Km,oDe),e(Km,u9),e(u9,rDe),e(Km,tDe),e(v,aDe),e(v,Zm),e(Zm,FQ),e(FQ,nDe),e(Zm,sDe),e(Zm,b9),e(b9,lDe),e(Zm,iDe),e(v,dDe),e(v,eg),e(eg,CQ),e(CQ,cDe),e(eg,fDe),e(eg,v9),e(v9,mDe),e(eg,gDe),e(v,hDe),e(v,og),e(og,MQ),e(MQ,pDe),e(og,_De),e(og,T9),e(T9,uDe),e(og,bDe),e(v,vDe),e(v,rg),e(rg,EQ),e(EQ,TDe),e(rg,FDe),e(rg,F9),e(F9,CDe),e(rg,MDe),e(v,EDe),e(v,tg),e(tg,yQ),e(yQ,yDe),e(tg,wDe),e(tg,C9),e(C9,ADe),e(tg,LDe),e(v,BDe),e(v,ag),e(ag,wQ),e(wQ,kDe),e(ag,xDe),e(ag,M9),e(M9,RDe),e(ag,SDe),e(v,PDe),e(v,ng),e(ng,AQ),e(AQ,$De),e(ng,IDe),e(ng,E9),e(E9,jDe),e(ng,NDe),e(v,DDe),e(v,sg),e(sg,LQ),e(LQ,qDe),e(sg,GDe),e(sg,y9),e(y9,ODe),e(sg,XDe),e(v,zDe),e(v,lg),e(lg,BQ),e(BQ,VDe),e(lg,WDe),e(lg,w9),e(w9,QDe),e(lg,HDe),e(v,UDe),e(v,ig),e(ig,kQ),e(kQ,JDe),e(ig,YDe),e(ig,A9),e(A9,KDe),e(ig,ZDe),e(v,eqe),e(v,dg),e(dg,xQ),e(xQ,oqe),e(dg,rqe),e(dg,L9),e(L9,tqe),e(dg,aqe),e(v,nqe),e(v,cg),e(cg,RQ),e(RQ,sqe),e(cg,lqe),e(cg,B9),e(B9,iqe),e(cg,dqe),e(v,cqe),e(v,fg),e(fg,SQ),e(SQ,fqe),e(fg,mqe),e(fg,k9),e(k9,gqe),e(fg,hqe),e(v,pqe),e(v,mg),e(mg,PQ),e(PQ,_qe),e(mg,uqe),e(mg,x9),e(x9,bqe),e(mg,vqe),e(v,Tqe),e(v,gg),e(gg,$Q),e($Q,Fqe),e(gg,Cqe),e(gg,R9),e(R9,Mqe),e(gg,Eqe),e(v,yqe),e(v,hg),e(hg,IQ),e(IQ,wqe),e(hg,Aqe),e(hg,S9),e(S9,Lqe),e(hg,Bqe),e(fo,kqe),e(fo,jQ),e(jQ,xqe),e(fo,Rqe),g(_M,fo,null),e(Go,Sqe),e(Go,pg),g(uM,pg,null),e(pg,Pqe),e(pg,NQ),e(NQ,$qe),b(d,w8e,u),b(d,Ii,u),e(Ii,_g),e(_g,DQ),g(bM,DQ,null),e(Ii,Iqe),e(Ii,qQ),e(qQ,jqe),b(d,A8e,u),b(d,Oo,u),g(vM,Oo,null),e(Oo,Nqe),e(Oo,TM),e(TM,Dqe),e(TM,P9),e(P9,qqe),e(TM,Gqe),e(Oo,Oqe),e(Oo,FM),e(FM,Xqe),e(FM,GQ),e(GQ,zqe),e(FM,Vqe),e(Oo,Wqe),e(Oo,mo),g(CM,mo,null),e(mo,Qqe),e(mo,OQ),e(OQ,Hqe),e(mo,Uqe),e(mo,ja),e(ja,Jqe),e(ja,XQ),e(XQ,Yqe),e(ja,Kqe),e(ja,zQ),e(zQ,Zqe),e(ja,eGe),e(ja,VQ),e(VQ,oGe),e(ja,rGe),e(mo,tGe),e(mo,M),e(M,Dn),e(Dn,WQ),e(WQ,aGe),e(Dn,nGe),e(Dn,$9),e($9,sGe),e(Dn,lGe),e(Dn,I9),e(I9,iGe),e(Dn,dGe),e(M,cGe),e(M,qn),e(qn,QQ),e(QQ,fGe),e(qn,mGe),e(qn,j9),e(j9,gGe),e(qn,hGe),e(qn,N9),e(N9,pGe),e(qn,_Ge),e(M,uGe),e(M,Gn),e(Gn,HQ),e(HQ,bGe),e(Gn,vGe),e(Gn,D9),e(D9,TGe),e(Gn,FGe),e(Gn,q9),e(q9,CGe),e(Gn,MGe),e(M,EGe),e(M,ug),e(ug,UQ),e(UQ,yGe),e(ug,wGe),e(ug,G9),e(G9,AGe),e(ug,LGe),e(M,BGe),e(M,On),e(On,JQ),e(JQ,kGe),e(On,xGe),e(On,O9),e(O9,RGe),e(On,SGe),e(On,X9),e(X9,PGe),e(On,$Ge),e(M,IGe),e(M,bg),e(bg,YQ),e(YQ,jGe),e(bg,NGe),e(bg,z9),e(z9,DGe),e(bg,qGe),e(M,GGe),e(M,vg),e(vg,KQ),e(KQ,OGe),e(vg,XGe),e(vg,V9),e(V9,zGe),e(vg,VGe),e(M,WGe),e(M,Tg),e(Tg,ZQ),e(ZQ,QGe),e(Tg,HGe),e(Tg,W9),e(W9,UGe),e(Tg,JGe),e(M,YGe),e(M,Xn),e(Xn,eH),e(eH,KGe),e(Xn,ZGe),e(Xn,Q9),e(Q9,eOe),e(Xn,oOe),e(Xn,H9),e(H9,rOe),e(Xn,tOe),e(M,aOe),e(M,zn),e(zn,oH),e(oH,nOe),e(zn,sOe),e(zn,U9),e(U9,lOe),e(zn,iOe),e(zn,J9),e(J9,dOe),e(zn,cOe),e(M,fOe),e(M,Vn),e(Vn,rH),e(rH,mOe),e(Vn,gOe),e(Vn,Y9),e(Y9,hOe),e(Vn,pOe),e(Vn,K9),e(K9,_Oe),e(Vn,uOe),e(M,bOe),e(M,Fg),e(Fg,tH),e(tH,vOe),e(Fg,TOe),e(Fg,Z9),e(Z9,FOe),e(Fg,COe),e(M,MOe),e(M,Cg),e(Cg,aH),e(aH,EOe),e(Cg,yOe),e(Cg,eB),e(eB,wOe),e(Cg,AOe),e(M,LOe),e(M,Wn),e(Wn,nH),e(nH,BOe),e(Wn,kOe),e(Wn,oB),e(oB,xOe),e(Wn,ROe),e(Wn,rB),e(rB,SOe),e(Wn,POe),e(M,$Oe),e(M,Mg),e(Mg,sH),e(sH,IOe),e(Mg,jOe),e(Mg,tB),e(tB,NOe),e(Mg,DOe),e(M,qOe),e(M,Qn),e(Qn,lH),e(lH,GOe),e(Qn,OOe),e(Qn,aB),e(aB,XOe),e(Qn,zOe),e(Qn,nB),e(nB,VOe),e(Qn,WOe),e(M,QOe),e(M,Hn),e(Hn,iH),e(iH,HOe),e(Hn,UOe),e(Hn,sB),e(sB,JOe),e(Hn,YOe),e(Hn,lB),e(lB,KOe),e(Hn,ZOe),e(M,eXe),e(M,Un),e(Un,dH),e(dH,oXe),e(Un,rXe),e(Un,iB),e(iB,tXe),e(Un,aXe),e(Un,cH),e(cH,nXe),e(Un,sXe),e(M,lXe),e(M,Eg),e(Eg,fH),e(fH,iXe),e(Eg,dXe),e(Eg,dB),e(dB,cXe),e(Eg,fXe),e(M,mXe),e(M,Jn),e(Jn,mH),e(mH,gXe),e(Jn,hXe),e(Jn,cB),e(cB,pXe),e(Jn,_Xe),e(Jn,fB),e(fB,uXe),e(Jn,bXe),e(M,vXe),e(M,yg),e(yg,gH),e(gH,TXe),e(yg,FXe),e(yg,mB),e(mB,CXe),e(yg,MXe),e(M,EXe),e(M,Yn),e(Yn,hH),e(hH,yXe),e(Yn,wXe),e(Yn,gB),e(gB,AXe),e(Yn,LXe),e(Yn,hB),e(hB,BXe),e(Yn,kXe),e(M,xXe),e(M,Kn),e(Kn,pH),e(pH,RXe),e(Kn,SXe),e(Kn,pB),e(pB,PXe),e(Kn,$Xe),e(Kn,_B),e(_B,IXe),e(Kn,jXe),e(M,NXe),e(M,Zn),e(Zn,_H),e(_H,DXe),e(Zn,qXe),e(Zn,uB),e(uB,GXe),e(Zn,OXe),e(Zn,bB),e(bB,XXe),e(Zn,zXe),e(M,VXe),e(M,wg),e(wg,uH),e(uH,WXe),e(wg,QXe),e(wg,vB),e(vB,HXe),e(wg,UXe),e(M,JXe),e(M,es),e(es,bH),e(bH,YXe),e(es,KXe),e(es,TB),e(TB,ZXe),e(es,eze),e(es,FB),e(FB,oze),e(es,rze),e(M,tze),e(M,Ag),e(Ag,vH),e(vH,aze),e(Ag,nze),e(Ag,CB),e(CB,sze),e(Ag,lze),e(M,ize),e(M,os),e(os,TH),e(TH,dze),e(os,cze),e(os,MB),e(MB,fze),e(os,mze),e(os,EB),e(EB,gze),e(os,hze),e(M,pze),e(M,rs),e(rs,FH),e(FH,_ze),e(rs,uze),e(rs,yB),e(yB,bze),e(rs,vze),e(rs,wB),e(wB,Tze),e(rs,Fze),e(M,Cze),e(M,ts),e(ts,CH),e(CH,Mze),e(ts,Eze),e(ts,AB),e(AB,yze),e(ts,wze),e(ts,LB),e(LB,Aze),e(ts,Lze),e(M,Bze),e(M,as),e(as,MH),e(MH,kze),e(as,xze),e(as,BB),e(BB,Rze),e(as,Sze),e(as,kB),e(kB,Pze),e(as,$ze),e(M,Ize),e(M,Lg),e(Lg,EH),e(EH,jze),e(Lg,Nze),e(Lg,xB),e(xB,Dze),e(Lg,qze),e(M,Gze),e(M,ns),e(ns,yH),e(yH,Oze),e(ns,Xze),e(ns,RB),e(RB,zze),e(ns,Vze),e(ns,SB),e(SB,Wze),e(ns,Qze),e(M,Hze),e(M,ss),e(ss,wH),e(wH,Uze),e(ss,Jze),e(ss,PB),e(PB,Yze),e(ss,Kze),e(ss,$B),e($B,Zze),e(ss,eVe),e(M,oVe),e(M,ls),e(ls,AH),e(AH,rVe),e(ls,tVe),e(ls,IB),e(IB,aVe),e(ls,nVe),e(ls,jB),e(jB,sVe),e(ls,lVe),e(M,iVe),e(M,is),e(is,LH),e(LH,dVe),e(is,cVe),e(is,NB),e(NB,fVe),e(is,mVe),e(is,DB),e(DB,gVe),e(is,hVe),e(M,pVe),e(M,ds),e(ds,BH),e(BH,_Ve),e(ds,uVe),e(ds,qB),e(qB,bVe),e(ds,vVe),e(ds,GB),e(GB,TVe),e(ds,FVe),e(M,CVe),e(M,cs),e(cs,kH),e(kH,MVe),e(cs,EVe),e(cs,OB),e(OB,yVe),e(cs,wVe),e(cs,XB),e(XB,AVe),e(cs,LVe),e(M,BVe),e(M,Bg),e(Bg,xH),e(xH,kVe),e(Bg,xVe),e(Bg,zB),e(zB,RVe),e(Bg,SVe),e(M,PVe),e(M,fs),e(fs,RH),e(RH,$Ve),e(fs,IVe),e(fs,VB),e(VB,jVe),e(fs,NVe),e(fs,WB),e(WB,DVe),e(fs,qVe),e(M,GVe),e(M,kg),e(kg,SH),e(SH,OVe),e(kg,XVe),e(kg,QB),e(QB,zVe),e(kg,VVe),e(M,WVe),e(M,xg),e(xg,PH),e(PH,QVe),e(xg,HVe),e(xg,HB),e(HB,UVe),e(xg,JVe),e(M,YVe),e(M,ms),e(ms,$H),e($H,KVe),e(ms,ZVe),e(ms,UB),e(UB,eWe),e(ms,oWe),e(ms,JB),e(JB,rWe),e(ms,tWe),e(M,aWe),e(M,gs),e(gs,IH),e(IH,nWe),e(gs,sWe),e(gs,YB),e(YB,lWe),e(gs,iWe),e(gs,KB),e(KB,dWe),e(gs,cWe),e(M,fWe),e(M,Rg),e(Rg,jH),e(jH,mWe),e(Rg,gWe),e(Rg,ZB),e(ZB,hWe),e(Rg,pWe),e(M,_We),e(M,hs),e(hs,NH),e(NH,uWe),e(hs,bWe),e(hs,ek),e(ek,vWe),e(hs,TWe),e(hs,ok),e(ok,FWe),e(hs,CWe),e(M,MWe),e(M,ps),e(ps,DH),e(DH,EWe),e(ps,yWe),e(ps,rk),e(rk,wWe),e(ps,AWe),e(ps,tk),e(tk,LWe),e(ps,BWe),e(M,kWe),e(M,_s),e(_s,qH),e(qH,xWe),e(_s,RWe),e(_s,ak),e(ak,SWe),e(_s,PWe),e(_s,nk),e(nk,$We),e(_s,IWe),e(M,jWe),e(M,us),e(us,GH),e(GH,NWe),e(us,DWe),e(us,sk),e(sk,qWe),e(us,GWe),e(us,lk),e(lk,OWe),e(us,XWe),e(M,zWe),e(M,bs),e(bs,OH),e(OH,VWe),e(bs,WWe),e(bs,ik),e(ik,QWe),e(bs,HWe),e(bs,dk),e(dk,UWe),e(bs,JWe),e(M,YWe),e(M,Sg),e(Sg,XH),e(XH,KWe),e(Sg,ZWe),e(Sg,ck),e(ck,eQe),e(Sg,oQe),e(M,rQe),e(M,Pg),e(Pg,zH),e(zH,tQe),e(Pg,aQe),e(Pg,fk),e(fk,nQe),e(Pg,sQe),e(M,lQe),e(M,$g),e($g,VH),e(VH,iQe),e($g,dQe),e($g,mk),e(mk,cQe),e($g,fQe),e(M,mQe),e(M,Ig),e(Ig,WH),e(WH,gQe),e(Ig,hQe),e(Ig,gk),e(gk,pQe),e(Ig,_Qe),e(M,uQe),e(M,vs),e(vs,QH),e(QH,bQe),e(vs,vQe),e(vs,hk),e(hk,TQe),e(vs,FQe),e(vs,pk),e(pk,CQe),e(vs,MQe),e(M,EQe),e(M,jg),e(jg,HH),e(HH,yQe),e(jg,wQe),e(jg,_k),e(_k,AQe),e(jg,LQe),e(M,BQe),e(M,Ts),e(Ts,UH),e(UH,kQe),e(Ts,xQe),e(Ts,uk),e(uk,RQe),e(Ts,SQe),e(Ts,bk),e(bk,PQe),e(Ts,$Qe),e(M,IQe),e(M,Fs),e(Fs,JH),e(JH,jQe),e(Fs,NQe),e(Fs,vk),e(vk,DQe),e(Fs,qQe),e(Fs,Tk),e(Tk,GQe),e(Fs,OQe),e(M,XQe),e(M,Cs),e(Cs,YH),e(YH,zQe),e(Cs,VQe),e(Cs,Fk),e(Fk,WQe),e(Cs,QQe),e(Cs,Ck),e(Ck,HQe),e(Cs,UQe),e(M,JQe),e(M,Ms),e(Ms,KH),e(KH,YQe),e(Ms,KQe),e(Ms,Mk),e(Mk,ZQe),e(Ms,eHe),e(Ms,Ek),e(Ek,oHe),e(Ms,rHe),e(M,tHe),e(M,Es),e(Es,ZH),e(ZH,aHe),e(Es,nHe),e(Es,yk),e(yk,sHe),e(Es,lHe),e(Es,wk),e(wk,iHe),e(Es,dHe),e(M,cHe),e(M,Ng),e(Ng,eU),e(eU,fHe),e(Ng,mHe),e(Ng,Ak),e(Ak,gHe),e(Ng,hHe),e(M,pHe),e(M,Dg),e(Dg,oU),e(oU,_He),e(Dg,uHe),e(Dg,Lk),e(Lk,bHe),e(Dg,vHe),e(M,THe),e(M,ys),e(ys,rU),e(rU,FHe),e(ys,CHe),e(ys,Bk),e(Bk,MHe),e(ys,EHe),e(ys,kk),e(kk,yHe),e(ys,wHe),e(M,AHe),e(M,ws),e(ws,tU),e(tU,LHe),e(ws,BHe),e(ws,xk),e(xk,kHe),e(ws,xHe),e(ws,Rk),e(Rk,RHe),e(ws,SHe),e(M,PHe),e(M,As),e(As,aU),e(aU,$He),e(As,IHe),e(As,Sk),e(Sk,jHe),e(As,NHe),e(As,Pk),e(Pk,DHe),e(As,qHe),e(M,GHe),e(M,qg),e(qg,nU),e(nU,OHe),e(qg,XHe),e(qg,$k),e($k,zHe),e(qg,VHe),e(M,WHe),e(M,Gg),e(Gg,sU),e(sU,QHe),e(Gg,HHe),e(Gg,Ik),e(Ik,UHe),e(Gg,JHe),e(M,YHe),e(M,Og),e(Og,lU),e(lU,KHe),e(Og,ZHe),e(Og,jk),e(jk,eUe),e(Og,oUe),e(M,rUe),e(M,Xg),e(Xg,iU),e(iU,tUe),e(Xg,aUe),e(Xg,Nk),e(Nk,nUe),e(Xg,sUe),e(M,lUe),e(M,Ls),e(Ls,dU),e(dU,iUe),e(Ls,dUe),e(Ls,Dk),e(Dk,cUe),e(Ls,fUe),e(Ls,qk),e(qk,mUe),e(Ls,gUe),e(M,hUe),e(M,zg),e(zg,cU),e(cU,pUe),e(zg,_Ue),e(zg,Gk),e(Gk,uUe),e(zg,bUe),e(M,vUe),e(M,Vg),e(Vg,fU),e(fU,TUe),e(Vg,FUe),e(Vg,Ok),e(Ok,CUe),e(Vg,MUe),e(M,EUe),e(M,Bs),e(Bs,mU),e(mU,yUe),e(Bs,wUe),e(Bs,Xk),e(Xk,AUe),e(Bs,LUe),e(Bs,zk),e(zk,BUe),e(Bs,kUe),e(M,xUe),e(M,ks),e(ks,gU),e(gU,RUe),e(ks,SUe),e(ks,Vk),e(Vk,PUe),e(ks,$Ue),e(ks,Wk),e(Wk,IUe),e(ks,jUe),e(mo,NUe),e(mo,hU),e(hU,DUe),e(mo,qUe),g(MM,mo,null),e(Oo,GUe),e(Oo,Wg),g(EM,Wg,null),e(Wg,OUe),e(Wg,pU),e(pU,XUe),b(d,L8e,u),b(d,ji,u),e(ji,Qg),e(Qg,_U),g(yM,_U,null),e(ji,zUe),e(ji,uU),e(uU,VUe),b(d,B8e,u),b(d,Xo,u),g(wM,Xo,null),e(Xo,WUe),e(Xo,AM),e(AM,QUe),e(AM,Qk),e(Qk,HUe),e(AM,UUe),e(Xo,JUe),e(Xo,LM),e(LM,YUe),e(LM,bU),e(bU,KUe),e(LM,ZUe),e(Xo,eJe),e(Xo,Be),g(BM,Be,null),e(Be,oJe),e(Be,vU),e(vU,rJe),e(Be,tJe),e(Be,Na),e(Na,aJe),e(Na,TU),e(TU,nJe),e(Na,sJe),e(Na,FU),e(FU,lJe),e(Na,iJe),e(Na,CU),e(CU,dJe),e(Na,cJe),e(Be,fJe),e(Be,ae),e(ae,Hg),e(Hg,MU),e(MU,mJe),e(Hg,gJe),e(Hg,Hk),e(Hk,hJe),e(Hg,pJe),e(ae,_Je),e(ae,Ug),e(Ug,EU),e(EU,uJe),e(Ug,bJe),e(Ug,Uk),e(Uk,vJe),e(Ug,TJe),e(ae,FJe),e(ae,Jg),e(Jg,yU),e(yU,CJe),e(Jg,MJe),e(Jg,Jk),e(Jk,EJe),e(Jg,yJe),e(ae,wJe),e(ae,Yg),e(Yg,wU),e(wU,AJe),e(Yg,LJe),e(Yg,Yk),e(Yk,BJe),e(Yg,kJe),e(ae,xJe),e(ae,Kg),e(Kg,AU),e(AU,RJe),e(Kg,SJe),e(Kg,Kk),e(Kk,PJe),e(Kg,$Je),e(ae,IJe),e(ae,Zg),e(Zg,LU),e(LU,jJe),e(Zg,NJe),e(Zg,Zk),e(Zk,DJe),e(Zg,qJe),e(ae,GJe),e(ae,eh),e(eh,BU),e(BU,OJe),e(eh,XJe),e(eh,ex),e(ex,zJe),e(eh,VJe),e(ae,WJe),e(ae,oh),e(oh,kU),e(kU,QJe),e(oh,HJe),e(oh,ox),e(ox,UJe),e(oh,JJe),e(ae,YJe),e(ae,rh),e(rh,xU),e(xU,KJe),e(rh,ZJe),e(rh,rx),e(rx,eYe),e(rh,oYe),e(ae,rYe),e(ae,th),e(th,RU),e(RU,tYe),e(th,aYe),e(th,tx),e(tx,nYe),e(th,sYe),e(ae,lYe),e(ae,ah),e(ah,SU),e(SU,iYe),e(ah,dYe),e(ah,ax),e(ax,cYe),e(ah,fYe),e(ae,mYe),e(ae,nh),e(nh,PU),e(PU,gYe),e(nh,hYe),e(nh,nx),e(nx,pYe),e(nh,_Ye),e(ae,uYe),e(ae,sh),e(sh,$U),e($U,bYe),e(sh,vYe),e(sh,sx),e(sx,TYe),e(sh,FYe),e(ae,CYe),e(ae,lh),e(lh,IU),e(IU,MYe),e(lh,EYe),e(lh,lx),e(lx,yYe),e(lh,wYe),e(ae,AYe),e(ae,ih),e(ih,jU),e(jU,LYe),e(ih,BYe),e(ih,ix),e(ix,kYe),e(ih,xYe),e(ae,RYe),e(ae,dh),e(dh,NU),e(NU,SYe),e(dh,PYe),e(dh,dx),e(dx,$Ye),e(dh,IYe),e(Be,jYe),g(ch,Be,null),e(Be,NYe),e(Be,DU),e(DU,DYe),e(Be,qYe),g(kM,Be,null),e(Xo,GYe),e(Xo,fh),g(xM,fh,null),e(fh,OYe),e(fh,qU),e(qU,XYe),b(d,k8e,u),b(d,Ni,u),e(Ni,mh),e(mh,GU),g(RM,GU,null),e(Ni,zYe),e(Ni,OU),e(OU,VYe),b(d,x8e,u),b(d,zo,u),g(SM,zo,null),e(zo,WYe),e(zo,PM),e(PM,QYe),e(PM,cx),e(cx,HYe),e(PM,UYe),e(zo,JYe),e(zo,$M),e($M,YYe),e($M,XU),e(XU,KYe),e($M,ZYe),e(zo,eKe),e(zo,ke),g(IM,ke,null),e(ke,oKe),e(ke,zU),e(zU,rKe),e(ke,tKe),e(ke,Di),e(Di,aKe),e(Di,VU),e(VU,nKe),e(Di,sKe),e(Di,WU),e(WU,lKe),e(Di,iKe),e(ke,dKe),e(ke,Ae),e(Ae,gh),e(gh,QU),e(QU,cKe),e(gh,fKe),e(gh,fx),e(fx,mKe),e(gh,gKe),e(Ae,hKe),e(Ae,hh),e(hh,HU),e(HU,pKe),e(hh,_Ke),e(hh,mx),e(mx,uKe),e(hh,bKe),e(Ae,vKe),e(Ae,ph),e(ph,UU),e(UU,TKe),e(ph,FKe),e(ph,gx),e(gx,CKe),e(ph,MKe),e(Ae,EKe),e(Ae,_h),e(_h,JU),e(JU,yKe),e(_h,wKe),e(_h,hx),e(hx,AKe),e(_h,LKe),e(Ae,BKe),e(Ae,uh),e(uh,YU),e(YU,kKe),e(uh,xKe),e(uh,px),e(px,RKe),e(uh,SKe),e(Ae,PKe),e(Ae,bh),e(bh,KU),e(KU,$Ke),e(bh,IKe),e(bh,_x),e(_x,jKe),e(bh,NKe),e(Ae,DKe),e(Ae,vh),e(vh,ZU),e(ZU,qKe),e(vh,GKe),e(vh,ux),e(ux,OKe),e(vh,XKe),e(Ae,zKe),e(Ae,Th),e(Th,eJ),e(eJ,VKe),e(Th,WKe),e(Th,bx),e(bx,QKe),e(Th,HKe),e(ke,UKe),g(Fh,ke,null),e(ke,JKe),e(ke,oJ),e(oJ,YKe),e(ke,KKe),g(jM,ke,null),e(zo,ZKe),e(zo,Ch),g(NM,Ch,null),e(Ch,eZe),e(Ch,rJ),e(rJ,oZe),b(d,R8e,u),b(d,qi,u),e(qi,Mh),e(Mh,tJ),g(DM,tJ,null),e(qi,rZe),e(qi,aJ),e(aJ,tZe),b(d,S8e,u),b(d,Vo,u),g(qM,Vo,null),e(Vo,aZe),e(Vo,Gi),e(Gi,nZe),e(Gi,nJ),e(nJ,sZe),e(Gi,lZe),e(Gi,sJ),e(sJ,iZe),e(Gi,dZe),e(Vo,cZe),e(Vo,GM),e(GM,fZe),e(GM,lJ),e(lJ,mZe),e(GM,gZe),e(Vo,hZe),e(Vo,Nr),g(OM,Nr,null),e(Nr,pZe),e(Nr,iJ),e(iJ,_Ze),e(Nr,uZe),e(Nr,Oi),e(Oi,bZe),e(Oi,dJ),e(dJ,vZe),e(Oi,TZe),e(Oi,cJ),e(cJ,FZe),e(Oi,CZe),e(Nr,MZe),e(Nr,fJ),e(fJ,EZe),e(Nr,yZe),g(XM,Nr,null),e(Vo,wZe),e(Vo,xe),g(zM,xe,null),e(xe,AZe),e(xe,mJ),e(mJ,LZe),e(xe,BZe),e(xe,Da),e(Da,kZe),e(Da,gJ),e(gJ,xZe),e(Da,RZe),e(Da,hJ),e(hJ,SZe),e(Da,PZe),e(Da,pJ),e(pJ,$Ze),e(Da,IZe),e(xe,jZe),e(xe,F),e(F,Eh),e(Eh,_J),e(_J,NZe),e(Eh,DZe),e(Eh,vx),e(vx,qZe),e(Eh,GZe),e(F,OZe),e(F,yh),e(yh,uJ),e(uJ,XZe),e(yh,zZe),e(yh,Tx),e(Tx,VZe),e(yh,WZe),e(F,QZe),e(F,wh),e(wh,bJ),e(bJ,HZe),e(wh,UZe),e(wh,Fx),e(Fx,JZe),e(wh,YZe),e(F,KZe),e(F,Ah),e(Ah,vJ),e(vJ,ZZe),e(Ah,eeo),e(Ah,Cx),e(Cx,oeo),e(Ah,reo),e(F,teo),e(F,Lh),e(Lh,TJ),e(TJ,aeo),e(Lh,neo),e(Lh,Mx),e(Mx,seo),e(Lh,leo),e(F,ieo),e(F,Bh),e(Bh,FJ),e(FJ,deo),e(Bh,ceo),e(Bh,Ex),e(Ex,feo),e(Bh,meo),e(F,geo),e(F,kh),e(kh,CJ),e(CJ,heo),e(kh,peo),e(kh,yx),e(yx,_eo),e(kh,ueo),e(F,beo),e(F,xh),e(xh,MJ),e(MJ,veo),e(xh,Teo),e(xh,wx),e(wx,Feo),e(xh,Ceo),e(F,Meo),e(F,Rh),e(Rh,EJ),e(EJ,Eeo),e(Rh,yeo),e(Rh,Ax),e(Ax,weo),e(Rh,Aeo),e(F,Leo),e(F,Sh),e(Sh,yJ),e(yJ,Beo),e(Sh,keo),e(Sh,Lx),e(Lx,xeo),e(Sh,Reo),e(F,Seo),e(F,Ph),e(Ph,wJ),e(wJ,Peo),e(Ph,$eo),e(Ph,Bx),e(Bx,Ieo),e(Ph,jeo),e(F,Neo),e(F,$h),e($h,AJ),e(AJ,Deo),e($h,qeo),e($h,kx),e(kx,Geo),e($h,Oeo),e(F,Xeo),e(F,Ih),e(Ih,LJ),e(LJ,zeo),e(Ih,Veo),e(Ih,xx),e(xx,Weo),e(Ih,Qeo),e(F,Heo),e(F,jh),e(jh,BJ),e(BJ,Ueo),e(jh,Jeo),e(jh,Rx),e(Rx,Yeo),e(jh,Keo),e(F,Zeo),e(F,Nh),e(Nh,kJ),e(kJ,eoo),e(Nh,ooo),e(Nh,Sx),e(Sx,roo),e(Nh,too),e(F,aoo),e(F,Dh),e(Dh,xJ),e(xJ,noo),e(Dh,soo),e(Dh,Px),e(Px,loo),e(Dh,ioo),e(F,doo),e(F,qh),e(qh,RJ),e(RJ,coo),e(qh,foo),e(qh,$x),e($x,moo),e(qh,goo),e(F,hoo),e(F,Gh),e(Gh,SJ),e(SJ,poo),e(Gh,_oo),e(Gh,Ix),e(Ix,uoo),e(Gh,boo),e(F,voo),e(F,Oh),e(Oh,PJ),e(PJ,Too),e(Oh,Foo),e(Oh,jx),e(jx,Coo),e(Oh,Moo),e(F,Eoo),e(F,Xh),e(Xh,$J),e($J,yoo),e(Xh,woo),e(Xh,Nx),e(Nx,Aoo),e(Xh,Loo),e(F,Boo),e(F,zh),e(zh,IJ),e(IJ,koo),e(zh,xoo),e(zh,Dx),e(Dx,Roo),e(zh,Soo),e(F,Poo),e(F,Vh),e(Vh,jJ),e(jJ,$oo),e(Vh,Ioo),e(Vh,qx),e(qx,joo),e(Vh,Noo),e(F,Doo),e(F,Wh),e(Wh,NJ),e(NJ,qoo),e(Wh,Goo),e(Wh,Gx),e(Gx,Ooo),e(Wh,Xoo),e(F,zoo),e(F,Qh),e(Qh,DJ),e(DJ,Voo),e(Qh,Woo),e(Qh,Ox),e(Ox,Qoo),e(Qh,Hoo),e(F,Uoo),e(F,Hh),e(Hh,qJ),e(qJ,Joo),e(Hh,Yoo),e(Hh,Xx),e(Xx,Koo),e(Hh,Zoo),e(F,ero),e(F,xs),e(xs,GJ),e(GJ,oro),e(xs,rro),e(xs,zx),e(zx,tro),e(xs,aro),e(xs,Vx),e(Vx,nro),e(xs,sro),e(F,lro),e(F,Uh),e(Uh,OJ),e(OJ,iro),e(Uh,dro),e(Uh,Wx),e(Wx,cro),e(Uh,fro),e(F,mro),e(F,Jh),e(Jh,XJ),e(XJ,gro),e(Jh,hro),e(Jh,Qx),e(Qx,pro),e(Jh,_ro),e(F,uro),e(F,Yh),e(Yh,zJ),e(zJ,bro),e(Yh,vro),e(Yh,Hx),e(Hx,Tro),e(Yh,Fro),e(F,Cro),e(F,Kh),e(Kh,VJ),e(VJ,Mro),e(Kh,Ero),e(Kh,Ux),e(Ux,yro),e(Kh,wro),e(F,Aro),e(F,Zh),e(Zh,WJ),e(WJ,Lro),e(Zh,Bro),e(Zh,Jx),e(Jx,kro),e(Zh,xro),e(F,Rro),e(F,ep),e(ep,QJ),e(QJ,Sro),e(ep,Pro),e(ep,Yx),e(Yx,$ro),e(ep,Iro),e(F,jro),e(F,op),e(op,HJ),e(HJ,Nro),e(op,Dro),e(op,Kx),e(Kx,qro),e(op,Gro),e(F,Oro),e(F,rp),e(rp,UJ),e(UJ,Xro),e(rp,zro),e(rp,Zx),e(Zx,Vro),e(rp,Wro),e(F,Qro),e(F,tp),e(tp,JJ),e(JJ,Hro),e(tp,Uro),e(tp,eR),e(eR,Jro),e(tp,Yro),e(F,Kro),e(F,ap),e(ap,YJ),e(YJ,Zro),e(ap,eto),e(ap,oR),e(oR,oto),e(ap,rto),e(F,tto),e(F,np),e(np,KJ),e(KJ,ato),e(np,nto),e(np,rR),e(rR,sto),e(np,lto),e(F,ito),e(F,sp),e(sp,ZJ),e(ZJ,dto),e(sp,cto),e(sp,tR),e(tR,fto),e(sp,mto),e(F,gto),e(F,lp),e(lp,eY),e(eY,hto),e(lp,pto),e(lp,aR),e(aR,_to),e(lp,uto),e(F,bto),e(F,ip),e(ip,oY),e(oY,vto),e(ip,Tto),e(ip,nR),e(nR,Fto),e(ip,Cto),e(F,Mto),e(F,dp),e(dp,rY),e(rY,Eto),e(dp,yto),e(dp,sR),e(sR,wto),e(dp,Ato),e(F,Lto),e(F,cp),e(cp,tY),e(tY,Bto),e(cp,kto),e(cp,lR),e(lR,xto),e(cp,Rto),e(F,Sto),e(F,fp),e(fp,aY),e(aY,Pto),e(fp,$to),e(fp,iR),e(iR,Ito),e(fp,jto),e(F,Nto),e(F,mp),e(mp,nY),e(nY,Dto),e(mp,qto),e(mp,dR),e(dR,Gto),e(mp,Oto),e(F,Xto),e(F,gp),e(gp,sY),e(sY,zto),e(gp,Vto),e(gp,cR),e(cR,Wto),e(gp,Qto),e(F,Hto),e(F,hp),e(hp,lY),e(lY,Uto),e(hp,Jto),e(hp,fR),e(fR,Yto),e(hp,Kto),e(F,Zto),e(F,pp),e(pp,iY),e(iY,eao),e(pp,oao),e(pp,mR),e(mR,rao),e(pp,tao),e(F,aao),e(F,_p),e(_p,dY),e(dY,nao),e(_p,sao),e(_p,gR),e(gR,lao),e(_p,iao),e(F,dao),e(F,up),e(up,cY),e(cY,cao),e(up,fao),e(up,hR),e(hR,mao),e(up,gao),e(F,hao),e(F,bp),e(bp,fY),e(fY,pao),e(bp,_ao),e(bp,pR),e(pR,uao),e(bp,bao),e(F,vao),e(F,vp),e(vp,mY),e(mY,Tao),e(vp,Fao),e(vp,_R),e(_R,Cao),e(vp,Mao),e(F,Eao),e(F,Tp),e(Tp,gY),e(gY,yao),e(Tp,wao),e(Tp,uR),e(uR,Aao),e(Tp,Lao),e(F,Bao),e(F,Fp),e(Fp,hY),e(hY,kao),e(Fp,xao),e(Fp,bR),e(bR,Rao),e(Fp,Sao),e(F,Pao),e(F,Cp),e(Cp,pY),e(pY,$ao),e(Cp,Iao),e(Cp,vR),e(vR,jao),e(Cp,Nao),e(F,Dao),e(F,Mp),e(Mp,_Y),e(_Y,qao),e(Mp,Gao),e(Mp,TR),e(TR,Oao),e(Mp,Xao),e(F,zao),e(F,Ep),e(Ep,uY),e(uY,Vao),e(Ep,Wao),e(Ep,FR),e(FR,Qao),e(Ep,Hao),e(F,Uao),e(F,yp),e(yp,bY),e(bY,Jao),e(yp,Yao),e(yp,CR),e(CR,Kao),e(yp,Zao),e(F,eno),e(F,wp),e(wp,vY),e(vY,ono),e(wp,rno),e(wp,MR),e(MR,tno),e(wp,ano),e(F,nno),e(F,Ap),e(Ap,TY),e(TY,sno),e(Ap,lno),e(Ap,ER),e(ER,ino),e(Ap,dno),e(F,cno),e(F,Lp),e(Lp,FY),e(FY,fno),e(Lp,mno),e(Lp,yR),e(yR,gno),e(Lp,hno),e(F,pno),e(F,Bp),e(Bp,CY),e(CY,_no),e(Bp,uno),e(Bp,wR),e(wR,bno),e(Bp,vno),e(F,Tno),e(F,kp),e(kp,MY),e(MY,Fno),e(kp,Cno),e(kp,AR),e(AR,Mno),e(kp,Eno),e(F,yno),e(F,xp),e(xp,EY),e(EY,wno),e(xp,Ano),e(xp,LR),e(LR,Lno),e(xp,Bno),e(F,kno),e(F,Rp),e(Rp,yY),e(yY,xno),e(Rp,Rno),e(Rp,BR),e(BR,Sno),e(Rp,Pno),e(F,$no),e(F,Sp),e(Sp,wY),e(wY,Ino),e(Sp,jno),e(Sp,kR),e(kR,Nno),e(Sp,Dno),e(F,qno),e(F,Pp),e(Pp,AY),e(AY,Gno),e(Pp,Ono),e(Pp,xR),e(xR,Xno),e(Pp,zno),e(F,Vno),e(F,$p),e($p,LY),e(LY,Wno),e($p,Qno),e($p,RR),e(RR,Hno),e($p,Uno),e(F,Jno),e(F,Ip),e(Ip,BY),e(BY,Yno),e(Ip,Kno),e(Ip,SR),e(SR,Zno),e(Ip,eso),e(F,oso),e(F,jp),e(jp,kY),e(kY,rso),e(jp,tso),e(jp,PR),e(PR,aso),e(jp,nso),e(F,sso),e(F,Np),e(Np,xY),e(xY,lso),e(Np,iso),e(Np,$R),e($R,dso),e(Np,cso),e(F,fso),e(F,Dp),e(Dp,RY),e(RY,mso),e(Dp,gso),e(Dp,IR),e(IR,hso),e(Dp,pso),e(F,_so),e(F,qp),e(qp,SY),e(SY,uso),e(qp,bso),e(qp,jR),e(jR,vso),e(qp,Tso),e(F,Fso),e(F,Gp),e(Gp,PY),e(PY,Cso),e(Gp,Mso),e(Gp,NR),e(NR,Eso),e(Gp,yso),e(F,wso),e(F,Op),e(Op,$Y),e($Y,Aso),e(Op,Lso),e(Op,DR),e(DR,Bso),e(Op,kso),e(F,xso),e(F,Xp),e(Xp,IY),e(IY,Rso),e(Xp,Sso),e(Xp,qR),e(qR,Pso),e(Xp,$so),e(F,Iso),e(F,zp),e(zp,jY),e(jY,jso),e(zp,Nso),e(zp,GR),e(GR,Dso),e(zp,qso),e(F,Gso),e(F,Vp),e(Vp,NY),e(NY,Oso),e(Vp,Xso),e(Vp,OR),e(OR,zso),e(Vp,Vso),e(F,Wso),e(F,Wp),e(Wp,DY),e(DY,Qso),e(Wp,Hso),e(Wp,XR),e(XR,Uso),e(Wp,Jso),e(F,Yso),e(F,Qp),e(Qp,qY),e(qY,Kso),e(Qp,Zso),e(Qp,zR),e(zR,elo),e(Qp,olo),e(F,rlo),e(F,Hp),e(Hp,GY),e(GY,tlo),e(Hp,alo),e(Hp,VR),e(VR,nlo),e(Hp,slo),e(F,llo),e(F,Up),e(Up,OY),e(OY,ilo),e(Up,dlo),e(Up,WR),e(WR,clo),e(Up,flo),e(F,mlo),e(F,Jp),e(Jp,XY),e(XY,glo),e(Jp,hlo),e(Jp,QR),e(QR,plo),e(Jp,_lo),e(F,ulo),e(F,Yp),e(Yp,zY),e(zY,blo),e(Yp,vlo),e(Yp,HR),e(HR,Tlo),e(Yp,Flo),e(F,Clo),e(F,Kp),e(Kp,VY),e(VY,Mlo),e(Kp,Elo),e(Kp,UR),e(UR,ylo),e(Kp,wlo),e(F,Alo),e(F,Zp),e(Zp,WY),e(WY,Llo),e(Zp,Blo),e(Zp,JR),e(JR,klo),e(Zp,xlo),e(xe,Rlo),e(xe,e_),e(e_,Slo),e(e_,QY),e(QY,Plo),e(e_,$lo),e(e_,HY),e(HY,Ilo),e(xe,jlo),e(xe,UY),e(UY,Nlo),e(xe,Dlo),g(VM,xe,null),b(d,P8e,u),b(d,Xi,u),e(Xi,o_),e(o_,JY),g(WM,JY,null),e(Xi,qlo),e(Xi,YY),e(YY,Glo),b(d,$8e,u),b(d,Wo,u),g(QM,Wo,null),e(Wo,Olo),e(Wo,zi),e(zi,Xlo),e(zi,KY),e(KY,zlo),e(zi,Vlo),e(zi,ZY),e(ZY,Wlo),e(zi,Qlo),e(Wo,Hlo),e(Wo,HM),e(HM,Ulo),e(HM,eK),e(eK,Jlo),e(HM,Ylo),e(Wo,Klo),e(Wo,Dr),g(UM,Dr,null),e(Dr,Zlo),e(Dr,oK),e(oK,eio),e(Dr,oio),e(Dr,Vi),e(Vi,rio),e(Vi,rK),e(rK,tio),e(Vi,aio),e(Vi,tK),e(tK,nio),e(Vi,sio),e(Dr,lio),e(Dr,aK),e(aK,iio),e(Dr,dio),g(JM,Dr,null),e(Wo,cio),e(Wo,Re),g(YM,Re,null),e(Re,fio),e(Re,nK),e(nK,mio),e(Re,gio),e(Re,qa),e(qa,hio),e(qa,sK),e(sK,pio),e(qa,_io),e(qa,lK),e(lK,uio),e(qa,bio),e(qa,iK),e(iK,vio),e(qa,Tio),e(Re,Fio),e(Re,x),e(x,r_),e(r_,dK),e(dK,Cio),e(r_,Mio),e(r_,YR),e(YR,Eio),e(r_,yio),e(x,wio),e(x,t_),e(t_,cK),e(cK,Aio),e(t_,Lio),e(t_,KR),e(KR,Bio),e(t_,kio),e(x,xio),e(x,a_),e(a_,fK),e(fK,Rio),e(a_,Sio),e(a_,ZR),e(ZR,Pio),e(a_,$io),e(x,Iio),e(x,n_),e(n_,mK),e(mK,jio),e(n_,Nio),e(n_,eS),e(eS,Dio),e(n_,qio),e(x,Gio),e(x,s_),e(s_,gK),e(gK,Oio),e(s_,Xio),e(s_,oS),e(oS,zio),e(s_,Vio),e(x,Wio),e(x,l_),e(l_,hK),e(hK,Qio),e(l_,Hio),e(l_,rS),e(rS,Uio),e(l_,Jio),e(x,Yio),e(x,i_),e(i_,pK),e(pK,Kio),e(i_,Zio),e(i_,tS),e(tS,edo),e(i_,odo),e(x,rdo),e(x,d_),e(d_,_K),e(_K,tdo),e(d_,ado),e(d_,aS),e(aS,ndo),e(d_,sdo),e(x,ldo),e(x,c_),e(c_,uK),e(uK,ido),e(c_,ddo),e(c_,nS),e(nS,cdo),e(c_,fdo),e(x,mdo),e(x,f_),e(f_,bK),e(bK,gdo),e(f_,hdo),e(f_,sS),e(sS,pdo),e(f_,_do),e(x,udo),e(x,m_),e(m_,vK),e(vK,bdo),e(m_,vdo),e(m_,lS),e(lS,Tdo),e(m_,Fdo),e(x,Cdo),e(x,g_),e(g_,TK),e(TK,Mdo),e(g_,Edo),e(g_,iS),e(iS,ydo),e(g_,wdo),e(x,Ado),e(x,h_),e(h_,FK),e(FK,Ldo),e(h_,Bdo),e(h_,dS),e(dS,kdo),e(h_,xdo),e(x,Rdo),e(x,p_),e(p_,CK),e(CK,Sdo),e(p_,Pdo),e(p_,cS),e(cS,$do),e(p_,Ido),e(x,jdo),e(x,__),e(__,MK),e(MK,Ndo),e(__,Ddo),e(__,fS),e(fS,qdo),e(__,Gdo),e(x,Odo),e(x,u_),e(u_,EK),e(EK,Xdo),e(u_,zdo),e(u_,mS),e(mS,Vdo),e(u_,Wdo),e(x,Qdo),e(x,b_),e(b_,yK),e(yK,Hdo),e(b_,Udo),e(b_,gS),e(gS,Jdo),e(b_,Ydo),e(x,Kdo),e(x,v_),e(v_,wK),e(wK,Zdo),e(v_,eco),e(v_,hS),e(hS,oco),e(v_,rco),e(x,tco),e(x,T_),e(T_,AK),e(AK,aco),e(T_,nco),e(T_,pS),e(pS,sco),e(T_,lco),e(x,ico),e(x,F_),e(F_,LK),e(LK,dco),e(F_,cco),e(F_,_S),e(_S,fco),e(F_,mco),e(x,gco),e(x,C_),e(C_,BK),e(BK,hco),e(C_,pco),e(C_,uS),e(uS,_co),e(C_,uco),e(x,bco),e(x,M_),e(M_,kK),e(kK,vco),e(M_,Tco),e(M_,bS),e(bS,Fco),e(M_,Cco),e(x,Mco),e(x,E_),e(E_,xK),e(xK,Eco),e(E_,yco),e(E_,vS),e(vS,wco),e(E_,Aco),e(x,Lco),e(x,y_),e(y_,RK),e(RK,Bco),e(y_,kco),e(y_,TS),e(TS,xco),e(y_,Rco),e(x,Sco),e(x,w_),e(w_,SK),e(SK,Pco),e(w_,$co),e(w_,FS),e(FS,Ico),e(w_,jco),e(x,Nco),e(x,A_),e(A_,PK),e(PK,Dco),e(A_,qco),e(A_,CS),e(CS,Gco),e(A_,Oco),e(x,Xco),e(x,L_),e(L_,$K),e($K,zco),e(L_,Vco),e(L_,MS),e(MS,Wco),e(L_,Qco),e(x,Hco),e(x,B_),e(B_,IK),e(IK,Uco),e(B_,Jco),e(B_,ES),e(ES,Yco),e(B_,Kco),e(x,Zco),e(x,k_),e(k_,jK),e(jK,efo),e(k_,ofo),e(k_,yS),e(yS,rfo),e(k_,tfo),e(x,afo),e(x,x_),e(x_,NK),e(NK,nfo),e(x_,sfo),e(x_,wS),e(wS,lfo),e(x_,ifo),e(x,dfo),e(x,R_),e(R_,DK),e(DK,cfo),e(R_,ffo),e(R_,AS),e(AS,mfo),e(R_,gfo),e(x,hfo),e(x,S_),e(S_,qK),e(qK,pfo),e(S_,_fo),e(S_,LS),e(LS,ufo),e(S_,bfo),e(x,vfo),e(x,P_),e(P_,GK),e(GK,Tfo),e(P_,Ffo),e(P_,BS),e(BS,Cfo),e(P_,Mfo),e(x,Efo),e(x,$_),e($_,OK),e(OK,yfo),e($_,wfo),e($_,kS),e(kS,Afo),e($_,Lfo),e(x,Bfo),e(x,I_),e(I_,XK),e(XK,kfo),e(I_,xfo),e(I_,xS),e(xS,Rfo),e(I_,Sfo),e(x,Pfo),e(x,j_),e(j_,zK),e(zK,$fo),e(j_,Ifo),e(j_,RS),e(RS,jfo),e(j_,Nfo),e(x,Dfo),e(x,N_),e(N_,VK),e(VK,qfo),e(N_,Gfo),e(N_,SS),e(SS,Ofo),e(N_,Xfo),e(x,zfo),e(x,D_),e(D_,WK),e(WK,Vfo),e(D_,Wfo),e(D_,PS),e(PS,Qfo),e(D_,Hfo),e(Re,Ufo),e(Re,q_),e(q_,Jfo),e(q_,QK),e(QK,Yfo),e(q_,Kfo),e(q_,HK),e(HK,Zfo),e(Re,emo),e(Re,UK),e(UK,omo),e(Re,rmo),g(KM,Re,null),b(d,I8e,u),b(d,Wi,u),e(Wi,G_),e(G_,JK),g(ZM,JK,null),e(Wi,tmo),e(Wi,YK),e(YK,amo),b(d,j8e,u),b(d,Qo,u),g(eE,Qo,null),e(Qo,nmo),e(Qo,Qi),e(Qi,smo),e(Qi,KK),e(KK,lmo),e(Qi,imo),e(Qi,ZK),e(ZK,dmo),e(Qi,cmo),e(Qo,fmo),e(Qo,oE),e(oE,mmo),e(oE,eZ),e(eZ,gmo),e(oE,hmo),e(Qo,pmo),e(Qo,qr),g(rE,qr,null),e(qr,_mo),e(qr,oZ),e(oZ,umo),e(qr,bmo),e(qr,Hi),e(Hi,vmo),e(Hi,rZ),e(rZ,Tmo),e(Hi,Fmo),e(Hi,tZ),e(tZ,Cmo),e(Hi,Mmo),e(qr,Emo),e(qr,aZ),e(aZ,ymo),e(qr,wmo),g(tE,qr,null),e(Qo,Amo),e(Qo,Se),g(aE,Se,null),e(Se,Lmo),e(Se,nZ),e(nZ,Bmo),e(Se,kmo),e(Se,Ga),e(Ga,xmo),e(Ga,sZ),e(sZ,Rmo),e(Ga,Smo),e(Ga,lZ),e(lZ,Pmo),e(Ga,$mo),e(Ga,iZ),e(iZ,Imo),e(Ga,jmo),e(Se,Nmo),e(Se,$),e($,O_),e(O_,dZ),e(dZ,Dmo),e(O_,qmo),e(O_,$S),e($S,Gmo),e(O_,Omo),e($,Xmo),e($,X_),e(X_,cZ),e(cZ,zmo),e(X_,Vmo),e(X_,IS),e(IS,Wmo),e(X_,Qmo),e($,Hmo),e($,z_),e(z_,fZ),e(fZ,Umo),e(z_,Jmo),e(z_,jS),e(jS,Ymo),e(z_,Kmo),e($,Zmo),e($,V_),e(V_,mZ),e(mZ,ego),e(V_,ogo),e(V_,NS),e(NS,rgo),e(V_,tgo),e($,ago),e($,W_),e(W_,gZ),e(gZ,ngo),e(W_,sgo),e(W_,DS),e(DS,lgo),e(W_,igo),e($,dgo),e($,Q_),e(Q_,hZ),e(hZ,cgo),e(Q_,fgo),e(Q_,qS),e(qS,mgo),e(Q_,ggo),e($,hgo),e($,H_),e(H_,pZ),e(pZ,pgo),e(H_,_go),e(H_,GS),e(GS,ugo),e(H_,bgo),e($,vgo),e($,U_),e(U_,_Z),e(_Z,Tgo),e(U_,Fgo),e(U_,OS),e(OS,Cgo),e(U_,Mgo),e($,Ego),e($,J_),e(J_,uZ),e(uZ,ygo),e(J_,wgo),e(J_,XS),e(XS,Ago),e(J_,Lgo),e($,Bgo),e($,Y_),e(Y_,bZ),e(bZ,kgo),e(Y_,xgo),e(Y_,zS),e(zS,Rgo),e(Y_,Sgo),e($,Pgo),e($,K_),e(K_,vZ),e(vZ,$go),e(K_,Igo),e(K_,VS),e(VS,jgo),e(K_,Ngo),e($,Dgo),e($,Z_),e(Z_,TZ),e(TZ,qgo),e(Z_,Ggo),e(Z_,WS),e(WS,Ogo),e(Z_,Xgo),e($,zgo),e($,eu),e(eu,FZ),e(FZ,Vgo),e(eu,Wgo),e(eu,QS),e(QS,Qgo),e(eu,Hgo),e($,Ugo),e($,ou),e(ou,CZ),e(CZ,Jgo),e(ou,Ygo),e(ou,HS),e(HS,Kgo),e(ou,Zgo),e($,eho),e($,ru),e(ru,MZ),e(MZ,oho),e(ru,rho),e(ru,US),e(US,tho),e(ru,aho),e($,nho),e($,tu),e(tu,EZ),e(EZ,sho),e(tu,lho),e(tu,JS),e(JS,iho),e(tu,dho),e($,cho),e($,au),e(au,yZ),e(yZ,fho),e(au,mho),e(au,YS),e(YS,gho),e(au,hho),e($,pho),e($,nu),e(nu,wZ),e(wZ,_ho),e(nu,uho),e(nu,KS),e(KS,bho),e(nu,vho),e($,Tho),e($,su),e(su,AZ),e(AZ,Fho),e(su,Cho),e(su,ZS),e(ZS,Mho),e(su,Eho),e($,yho),e($,lu),e(lu,LZ),e(LZ,who),e(lu,Aho),e(lu,eP),e(eP,Lho),e(lu,Bho),e($,kho),e($,iu),e(iu,BZ),e(BZ,xho),e(iu,Rho),e(iu,oP),e(oP,Sho),e(iu,Pho),e($,$ho),e($,du),e(du,kZ),e(kZ,Iho),e(du,jho),e(du,rP),e(rP,Nho),e(du,Dho),e($,qho),e($,cu),e(cu,xZ),e(xZ,Gho),e(cu,Oho),e(cu,tP),e(tP,Xho),e(cu,zho),e($,Vho),e($,fu),e(fu,RZ),e(RZ,Who),e(fu,Qho),e(fu,aP),e(aP,Hho),e(fu,Uho),e($,Jho),e($,mu),e(mu,SZ),e(SZ,Yho),e(mu,Kho),e(mu,nP),e(nP,Zho),e(mu,epo),e($,opo),e($,gu),e(gu,PZ),e(PZ,rpo),e(gu,tpo),e(gu,sP),e(sP,apo),e(gu,npo),e($,spo),e($,hu),e(hu,$Z),e($Z,lpo),e(hu,ipo),e(hu,lP),e(lP,dpo),e(hu,cpo),e($,fpo),e($,pu),e(pu,IZ),e(IZ,mpo),e(pu,gpo),e(pu,iP),e(iP,hpo),e(pu,ppo),e($,_po),e($,_u),e(_u,jZ),e(jZ,upo),e(_u,bpo),e(_u,dP),e(dP,vpo),e(_u,Tpo),e($,Fpo),e($,uu),e(uu,NZ),e(NZ,Cpo),e(uu,Mpo),e(uu,cP),e(cP,Epo),e(uu,ypo),e($,wpo),e($,bu),e(bu,DZ),e(DZ,Apo),e(bu,Lpo),e(bu,fP),e(fP,Bpo),e(bu,kpo),e($,xpo),e($,vu),e(vu,qZ),e(qZ,Rpo),e(vu,Spo),e(vu,mP),e(mP,Ppo),e(vu,$po),e($,Ipo),e($,Tu),e(Tu,GZ),e(GZ,jpo),e(Tu,Npo),e(Tu,gP),e(gP,Dpo),e(Tu,qpo),e($,Gpo),e($,Fu),e(Fu,OZ),e(OZ,Opo),e(Fu,Xpo),e(Fu,hP),e(hP,zpo),e(Fu,Vpo),e(Se,Wpo),e(Se,Cu),e(Cu,Qpo),e(Cu,XZ),e(XZ,Hpo),e(Cu,Upo),e(Cu,zZ),e(zZ,Jpo),e(Se,Ypo),e(Se,VZ),e(VZ,Kpo),e(Se,Zpo),g(nE,Se,null),b(d,N8e,u),b(d,Ui,u),e(Ui,Mu),e(Mu,WZ),g(sE,WZ,null),e(Ui,e_o),e(Ui,QZ),e(QZ,o_o),b(d,D8e,u),b(d,Ho,u),g(lE,Ho,null),e(Ho,r_o),e(Ho,Ji),e(Ji,t_o),e(Ji,HZ),e(HZ,a_o),e(Ji,n_o),e(Ji,UZ),e(UZ,s_o),e(Ji,l_o),e(Ho,i_o),e(Ho,iE),e(iE,d_o),e(iE,JZ),e(JZ,c_o),e(iE,f_o),e(Ho,m_o),e(Ho,Gr),g(dE,Gr,null),e(Gr,g_o),e(Gr,YZ),e(YZ,h_o),e(Gr,p_o),e(Gr,Yi),e(Yi,__o),e(Yi,KZ),e(KZ,u_o),e(Yi,b_o),e(Yi,ZZ),e(ZZ,v_o),e(Yi,T_o),e(Gr,F_o),e(Gr,eee),e(eee,C_o),e(Gr,M_o),g(cE,Gr,null),e(Ho,E_o),e(Ho,Pe),g(fE,Pe,null),e(Pe,y_o),e(Pe,oee),e(oee,w_o),e(Pe,A_o),e(Pe,Oa),e(Oa,L_o),e(Oa,ree),e(ree,B_o),e(Oa,k_o),e(Oa,tee),e(tee,x_o),e(Oa,R_o),e(Oa,aee),e(aee,S_o),e(Oa,P_o),e(Pe,$_o),e(Pe,I),e(I,Eu),e(Eu,nee),e(nee,I_o),e(Eu,j_o),e(Eu,pP),e(pP,N_o),e(Eu,D_o),e(I,q_o),e(I,yu),e(yu,see),e(see,G_o),e(yu,O_o),e(yu,_P),e(_P,X_o),e(yu,z_o),e(I,V_o),e(I,wu),e(wu,lee),e(lee,W_o),e(wu,Q_o),e(wu,uP),e(uP,H_o),e(wu,U_o),e(I,J_o),e(I,Au),e(Au,iee),e(iee,Y_o),e(Au,K_o),e(Au,bP),e(bP,Z_o),e(Au,euo),e(I,ouo),e(I,Lu),e(Lu,dee),e(dee,ruo),e(Lu,tuo),e(Lu,vP),e(vP,auo),e(Lu,nuo),e(I,suo),e(I,Bu),e(Bu,cee),e(cee,luo),e(Bu,iuo),e(Bu,TP),e(TP,duo),e(Bu,cuo),e(I,fuo),e(I,ku),e(ku,fee),e(fee,muo),e(ku,guo),e(ku,FP),e(FP,huo),e(ku,puo),e(I,_uo),e(I,xu),e(xu,mee),e(mee,uuo),e(xu,buo),e(xu,CP),e(CP,vuo),e(xu,Tuo),e(I,Fuo),e(I,Ru),e(Ru,gee),e(gee,Cuo),e(Ru,Muo),e(Ru,MP),e(MP,Euo),e(Ru,yuo),e(I,wuo),e(I,Su),e(Su,hee),e(hee,Auo),e(Su,Luo),e(Su,EP),e(EP,Buo),e(Su,kuo),e(I,xuo),e(I,Pu),e(Pu,pee),e(pee,Ruo),e(Pu,Suo),e(Pu,yP),e(yP,Puo),e(Pu,$uo),e(I,Iuo),e(I,$u),e($u,_ee),e(_ee,juo),e($u,Nuo),e($u,wP),e(wP,Duo),e($u,quo),e(I,Guo),e(I,Iu),e(Iu,uee),e(uee,Ouo),e(Iu,Xuo),e(Iu,AP),e(AP,zuo),e(Iu,Vuo),e(I,Wuo),e(I,ju),e(ju,bee),e(bee,Quo),e(ju,Huo),e(ju,LP),e(LP,Uuo),e(ju,Juo),e(I,Yuo),e(I,Nu),e(Nu,vee),e(vee,Kuo),e(Nu,Zuo),e(Nu,BP),e(BP,e1o),e(Nu,o1o),e(I,r1o),e(I,Du),e(Du,Tee),e(Tee,t1o),e(Du,a1o),e(Du,kP),e(kP,n1o),e(Du,s1o),e(I,l1o),e(I,qu),e(qu,Fee),e(Fee,i1o),e(qu,d1o),e(qu,xP),e(xP,c1o),e(qu,f1o),e(I,m1o),e(I,Gu),e(Gu,Cee),e(Cee,g1o),e(Gu,h1o),e(Gu,RP),e(RP,p1o),e(Gu,_1o),e(I,u1o),e(I,Ou),e(Ou,Mee),e(Mee,b1o),e(Ou,v1o),e(Ou,SP),e(SP,T1o),e(Ou,F1o),e(I,C1o),e(I,Xu),e(Xu,Eee),e(Eee,M1o),e(Xu,E1o),e(Xu,PP),e(PP,y1o),e(Xu,w1o),e(I,A1o),e(I,zu),e(zu,yee),e(yee,L1o),e(zu,B1o),e(zu,$P),e($P,k1o),e(zu,x1o),e(I,R1o),e(I,Vu),e(Vu,wee),e(wee,S1o),e(Vu,P1o),e(Vu,IP),e(IP,$1o),e(Vu,I1o),e(I,j1o),e(I,Wu),e(Wu,Aee),e(Aee,N1o),e(Wu,D1o),e(Wu,jP),e(jP,q1o),e(Wu,G1o),e(I,O1o),e(I,Qu),e(Qu,Lee),e(Lee,X1o),e(Qu,z1o),e(Qu,NP),e(NP,V1o),e(Qu,W1o),e(I,Q1o),e(I,Hu),e(Hu,Bee),e(Bee,H1o),e(Hu,U1o),e(Hu,DP),e(DP,J1o),e(Hu,Y1o),e(I,K1o),e(I,Uu),e(Uu,kee),e(kee,Z1o),e(Uu,e7o),e(Uu,qP),e(qP,o7o),e(Uu,r7o),e(I,t7o),e(I,Ju),e(Ju,xee),e(xee,a7o),e(Ju,n7o),e(Ju,GP),e(GP,s7o),e(Ju,l7o),e(I,i7o),e(I,Yu),e(Yu,Ree),e(Ree,d7o),e(Yu,c7o),e(Yu,OP),e(OP,f7o),e(Yu,m7o),e(I,g7o),e(I,Ku),e(Ku,See),e(See,h7o),e(Ku,p7o),e(Ku,XP),e(XP,_7o),e(Ku,u7o),e(I,b7o),e(I,Zu),e(Zu,Pee),e(Pee,v7o),e(Zu,T7o),e(Zu,$ee),e($ee,F7o),e(Zu,C7o),e(I,M7o),e(I,e1),e(e1,Iee),e(Iee,E7o),e(e1,y7o),e(e1,zP),e(zP,w7o),e(e1,A7o),e(I,L7o),e(I,o1),e(o1,jee),e(jee,B7o),e(o1,k7o),e(o1,VP),e(VP,x7o),e(o1,R7o),e(I,S7o),e(I,r1),e(r1,Nee),e(Nee,P7o),e(r1,$7o),e(r1,WP),e(WP,I7o),e(r1,j7o),e(I,N7o),e(I,t1),e(t1,Dee),e(Dee,D7o),e(t1,q7o),e(t1,QP),e(QP,G7o),e(t1,O7o),e(Pe,X7o),e(Pe,a1),e(a1,z7o),e(a1,qee),e(qee,V7o),e(a1,W7o),e(a1,Gee),e(Gee,Q7o),e(Pe,H7o),e(Pe,Oee),e(Oee,U7o),e(Pe,J7o),g(mE,Pe,null),b(d,q8e,u),b(d,Ki,u),e(Ki,n1),e(n1,Xee),g(gE,Xee,null),e(Ki,Y7o),e(Ki,zee),e(zee,K7o),b(d,G8e,u),b(d,Uo,u),g(hE,Uo,null),e(Uo,Z7o),e(Uo,Zi),e(Zi,ebo),e(Zi,Vee),e(Vee,obo),e(Zi,rbo),e(Zi,Wee),e(Wee,tbo),e(Zi,abo),e(Uo,nbo),e(Uo,pE),e(pE,sbo),e(pE,Qee),e(Qee,lbo),e(pE,ibo),e(Uo,dbo),e(Uo,Or),g(_E,Or,null),e(Or,cbo),e(Or,Hee),e(Hee,fbo),e(Or,mbo),e(Or,ed),e(ed,gbo),e(ed,Uee),e(Uee,hbo),e(ed,pbo),e(ed,Jee),e(Jee,_bo),e(ed,ubo),e(Or,bbo),e(Or,Yee),e(Yee,vbo),e(Or,Tbo),g(uE,Or,null),e(Uo,Fbo),e(Uo,$e),g(bE,$e,null),e($e,Cbo),e($e,Kee),e(Kee,Mbo),e($e,Ebo),e($e,Xa),e(Xa,ybo),e(Xa,Zee),e(Zee,wbo),e(Xa,Abo),e(Xa,eoe),e(eoe,Lbo),e(Xa,Bbo),e(Xa,ooe),e(ooe,kbo),e(Xa,xbo),e($e,Rbo),e($e,ne),e(ne,s1),e(s1,roe),e(roe,Sbo),e(s1,Pbo),e(s1,HP),e(HP,$bo),e(s1,Ibo),e(ne,jbo),e(ne,l1),e(l1,toe),e(toe,Nbo),e(l1,Dbo),e(l1,UP),e(UP,qbo),e(l1,Gbo),e(ne,Obo),e(ne,i1),e(i1,aoe),e(aoe,Xbo),e(i1,zbo),e(i1,JP),e(JP,Vbo),e(i1,Wbo),e(ne,Qbo),e(ne,d1),e(d1,noe),e(noe,Hbo),e(d1,Ubo),e(d1,YP),e(YP,Jbo),e(d1,Ybo),e(ne,Kbo),e(ne,c1),e(c1,soe),e(soe,Zbo),e(c1,e5o),e(c1,KP),e(KP,o5o),e(c1,r5o),e(ne,t5o),e(ne,f1),e(f1,loe),e(loe,a5o),e(f1,n5o),e(f1,ZP),e(ZP,s5o),e(f1,l5o),e(ne,i5o),e(ne,m1),e(m1,ioe),e(ioe,d5o),e(m1,c5o),e(m1,e$),e(e$,f5o),e(m1,m5o),e(ne,g5o),e(ne,g1),e(g1,doe),e(doe,h5o),e(g1,p5o),e(g1,o$),e(o$,_5o),e(g1,u5o),e(ne,b5o),e(ne,h1),e(h1,coe),e(coe,v5o),e(h1,T5o),e(h1,r$),e(r$,F5o),e(h1,C5o),e(ne,M5o),e(ne,p1),e(p1,foe),e(foe,E5o),e(p1,y5o),e(p1,t$),e(t$,w5o),e(p1,A5o),e(ne,L5o),e(ne,_1),e(_1,moe),e(moe,B5o),e(_1,k5o),e(_1,a$),e(a$,x5o),e(_1,R5o),e(ne,S5o),e(ne,u1),e(u1,goe),e(goe,P5o),e(u1,$5o),e(u1,n$),e(n$,I5o),e(u1,j5o),e(ne,N5o),e(ne,b1),e(b1,hoe),e(hoe,D5o),e(b1,q5o),e(b1,s$),e(s$,G5o),e(b1,O5o),e(ne,X5o),e(ne,v1),e(v1,poe),e(poe,z5o),e(v1,V5o),e(v1,l$),e(l$,W5o),e(v1,Q5o),e(ne,H5o),e(ne,T1),e(T1,_oe),e(_oe,U5o),e(T1,J5o),e(T1,i$),e(i$,Y5o),e(T1,K5o),e(ne,Z5o),e(ne,F1),e(F1,uoe),e(uoe,e2o),e(F1,o2o),e(F1,d$),e(d$,r2o),e(F1,t2o),e($e,a2o),e($e,C1),e(C1,n2o),e(C1,boe),e(boe,s2o),e(C1,l2o),e(C1,voe),e(voe,i2o),e($e,d2o),e($e,Toe),e(Toe,c2o),e($e,f2o),g(vE,$e,null),b(d,O8e,u),b(d,od,u),e(od,M1),e(M1,Foe),g(TE,Foe,null),e(od,m2o),e(od,Coe),e(Coe,g2o),b(d,X8e,u),b(d,Jo,u),g(FE,Jo,null),e(Jo,h2o),e(Jo,rd),e(rd,p2o),e(rd,Moe),e(Moe,_2o),e(rd,u2o),e(rd,Eoe),e(Eoe,b2o),e(rd,v2o),e(Jo,T2o),e(Jo,CE),e(CE,F2o),e(CE,yoe),e(yoe,C2o),e(CE,M2o),e(Jo,E2o),e(Jo,Xr),g(ME,Xr,null),e(Xr,y2o),e(Xr,woe),e(woe,w2o),e(Xr,A2o),e(Xr,td),e(td,L2o),e(td,Aoe),e(Aoe,B2o),e(td,k2o),e(td,Loe),e(Loe,x2o),e(td,R2o),e(Xr,S2o),e(Xr,Boe),e(Boe,P2o),e(Xr,$2o),g(EE,Xr,null),e(Jo,I2o),e(Jo,Ie),g(yE,Ie,null),e(Ie,j2o),e(Ie,koe),e(koe,N2o),e(Ie,D2o),e(Ie,za),e(za,q2o),e(za,xoe),e(xoe,G2o),e(za,O2o),e(za,Roe),e(Roe,X2o),e(za,z2o),e(za,Soe),e(Soe,V2o),e(za,W2o),e(Ie,Q2o),e(Ie,A),e(A,E1),e(E1,Poe),e(Poe,H2o),e(E1,U2o),e(E1,c$),e(c$,J2o),e(E1,Y2o),e(A,K2o),e(A,y1),e(y1,$oe),e($oe,Z2o),e(y1,evo),e(y1,f$),e(f$,ovo),e(y1,rvo),e(A,tvo),e(A,w1),e(w1,Ioe),e(Ioe,avo),e(w1,nvo),e(w1,m$),e(m$,svo),e(w1,lvo),e(A,ivo),e(A,A1),e(A1,joe),e(joe,dvo),e(A1,cvo),e(A1,g$),e(g$,fvo),e(A1,mvo),e(A,gvo),e(A,L1),e(L1,Noe),e(Noe,hvo),e(L1,pvo),e(L1,h$),e(h$,_vo),e(L1,uvo),e(A,bvo),e(A,B1),e(B1,Doe),e(Doe,vvo),e(B1,Tvo),e(B1,p$),e(p$,Fvo),e(B1,Cvo),e(A,Mvo),e(A,k1),e(k1,qoe),e(qoe,Evo),e(k1,yvo),e(k1,_$),e(_$,wvo),e(k1,Avo),e(A,Lvo),e(A,x1),e(x1,Goe),e(Goe,Bvo),e(x1,kvo),e(x1,u$),e(u$,xvo),e(x1,Rvo),e(A,Svo),e(A,R1),e(R1,Ooe),e(Ooe,Pvo),e(R1,$vo),e(R1,b$),e(b$,Ivo),e(R1,jvo),e(A,Nvo),e(A,S1),e(S1,Xoe),e(Xoe,Dvo),e(S1,qvo),e(S1,v$),e(v$,Gvo),e(S1,Ovo),e(A,Xvo),e(A,P1),e(P1,zoe),e(zoe,zvo),e(P1,Vvo),e(P1,T$),e(T$,Wvo),e(P1,Qvo),e(A,Hvo),e(A,$1),e($1,Voe),e(Voe,Uvo),e($1,Jvo),e($1,F$),e(F$,Yvo),e($1,Kvo),e(A,Zvo),e(A,I1),e(I1,Woe),e(Woe,e0o),e(I1,o0o),e(I1,C$),e(C$,r0o),e(I1,t0o),e(A,a0o),e(A,j1),e(j1,Qoe),e(Qoe,n0o),e(j1,s0o),e(j1,M$),e(M$,l0o),e(j1,i0o),e(A,d0o),e(A,N1),e(N1,Hoe),e(Hoe,c0o),e(N1,f0o),e(N1,E$),e(E$,m0o),e(N1,g0o),e(A,h0o),e(A,D1),e(D1,Uoe),e(Uoe,p0o),e(D1,_0o),e(D1,y$),e(y$,u0o),e(D1,b0o),e(A,v0o),e(A,q1),e(q1,Joe),e(Joe,T0o),e(q1,F0o),e(q1,w$),e(w$,C0o),e(q1,M0o),e(A,E0o),e(A,G1),e(G1,Yoe),e(Yoe,y0o),e(G1,w0o),e(G1,A$),e(A$,A0o),e(G1,L0o),e(A,B0o),e(A,O1),e(O1,Koe),e(Koe,k0o),e(O1,x0o),e(O1,L$),e(L$,R0o),e(O1,S0o),e(A,P0o),e(A,X1),e(X1,Zoe),e(Zoe,$0o),e(X1,I0o),e(X1,B$),e(B$,j0o),e(X1,N0o),e(A,D0o),e(A,z1),e(z1,ere),e(ere,q0o),e(z1,G0o),e(z1,k$),e(k$,O0o),e(z1,X0o),e(A,z0o),e(A,V1),e(V1,ore),e(ore,V0o),e(V1,W0o),e(V1,x$),e(x$,Q0o),e(V1,H0o),e(A,U0o),e(A,W1),e(W1,rre),e(rre,J0o),e(W1,Y0o),e(W1,R$),e(R$,K0o),e(W1,Z0o),e(A,eTo),e(A,Q1),e(Q1,tre),e(tre,oTo),e(Q1,rTo),e(Q1,S$),e(S$,tTo),e(Q1,aTo),e(A,nTo),e(A,H1),e(H1,are),e(are,sTo),e(H1,lTo),e(H1,P$),e(P$,iTo),e(H1,dTo),e(A,cTo),e(A,U1),e(U1,nre),e(nre,fTo),e(U1,mTo),e(U1,$$),e($$,gTo),e(U1,hTo),e(A,pTo),e(A,J1),e(J1,sre),e(sre,_To),e(J1,uTo),e(J1,I$),e(I$,bTo),e(J1,vTo),e(A,TTo),e(A,Y1),e(Y1,lre),e(lre,FTo),e(Y1,CTo),e(Y1,j$),e(j$,MTo),e(Y1,ETo),e(A,yTo),e(A,K1),e(K1,ire),e(ire,wTo),e(K1,ATo),e(K1,N$),e(N$,LTo),e(K1,BTo),e(A,kTo),e(A,Z1),e(Z1,dre),e(dre,xTo),e(Z1,RTo),e(Z1,D$),e(D$,STo),e(Z1,PTo),e(A,$To),e(A,e7),e(e7,cre),e(cre,ITo),e(e7,jTo),e(e7,q$),e(q$,NTo),e(e7,DTo),e(A,qTo),e(A,o7),e(o7,fre),e(fre,GTo),e(o7,OTo),e(o7,G$),e(G$,XTo),e(o7,zTo),e(A,VTo),e(A,r7),e(r7,mre),e(mre,WTo),e(r7,QTo),e(r7,O$),e(O$,HTo),e(r7,UTo),e(A,JTo),e(A,t7),e(t7,gre),e(gre,YTo),e(t7,KTo),e(t7,X$),e(X$,ZTo),e(t7,eFo),e(A,oFo),e(A,a7),e(a7,hre),e(hre,rFo),e(a7,tFo),e(a7,z$),e(z$,aFo),e(a7,nFo),e(A,sFo),e(A,n7),e(n7,pre),e(pre,lFo),e(n7,iFo),e(n7,V$),e(V$,dFo),e(n7,cFo),e(A,fFo),e(A,s7),e(s7,_re),e(_re,mFo),e(s7,gFo),e(s7,W$),e(W$,hFo),e(s7,pFo),e(A,_Fo),e(A,l7),e(l7,ure),e(ure,uFo),e(l7,bFo),e(l7,Q$),e(Q$,vFo),e(l7,TFo),e(A,FFo),e(A,i7),e(i7,bre),e(bre,CFo),e(i7,MFo),e(i7,H$),e(H$,EFo),e(i7,yFo),e(A,wFo),e(A,d7),e(d7,vre),e(vre,AFo),e(d7,LFo),e(d7,U$),e(U$,BFo),e(d7,kFo),e(A,xFo),e(A,c7),e(c7,Tre),e(Tre,RFo),e(c7,SFo),e(c7,J$),e(J$,PFo),e(c7,$Fo),e(A,IFo),e(A,f7),e(f7,Fre),e(Fre,jFo),e(f7,NFo),e(f7,Y$),e(Y$,DFo),e(f7,qFo),e(A,GFo),e(A,m7),e(m7,Cre),e(Cre,OFo),e(m7,XFo),e(m7,K$),e(K$,zFo),e(m7,VFo),e(A,WFo),e(A,g7),e(g7,Mre),e(Mre,QFo),e(g7,HFo),e(g7,Z$),e(Z$,UFo),e(g7,JFo),e(A,YFo),e(A,h7),e(h7,Ere),e(Ere,KFo),e(h7,ZFo),e(h7,eI),e(eI,eCo),e(h7,oCo),e(Ie,rCo),e(Ie,p7),e(p7,tCo),e(p7,yre),e(yre,aCo),e(p7,nCo),e(p7,wre),e(wre,sCo),e(Ie,lCo),e(Ie,Are),e(Are,iCo),e(Ie,dCo),g(wE,Ie,null),b(d,z8e,u),b(d,ad,u),e(ad,_7),e(_7,Lre),g(AE,Lre,null),e(ad,cCo),e(ad,Bre),e(Bre,fCo),b(d,V8e,u),b(d,Yo,u),g(LE,Yo,null),e(Yo,mCo),e(Yo,nd),e(nd,gCo),e(nd,kre),e(kre,hCo),e(nd,pCo),e(nd,xre),e(xre,_Co),e(nd,uCo),e(Yo,bCo),e(Yo,BE),e(BE,vCo),e(BE,Rre),e(Rre,TCo),e(BE,FCo),e(Yo,CCo),e(Yo,zr),g(kE,zr,null),e(zr,MCo),e(zr,Sre),e(Sre,ECo),e(zr,yCo),e(zr,sd),e(sd,wCo),e(sd,Pre),e(Pre,ACo),e(sd,LCo),e(sd,$re),e($re,BCo),e(sd,kCo),e(zr,xCo),e(zr,Ire),e(Ire,RCo),e(zr,SCo),g(xE,zr,null),e(Yo,PCo),e(Yo,je),g(RE,je,null),e(je,$Co),e(je,jre),e(jre,ICo),e(je,jCo),e(je,Va),e(Va,NCo),e(Va,Nre),e(Nre,DCo),e(Va,qCo),e(Va,Dre),e(Dre,GCo),e(Va,OCo),e(Va,qre),e(qre,XCo),e(Va,zCo),e(je,VCo),e(je,G),e(G,u7),e(u7,Gre),e(Gre,WCo),e(u7,QCo),e(u7,oI),e(oI,HCo),e(u7,UCo),e(G,JCo),e(G,b7),e(b7,Ore),e(Ore,YCo),e(b7,KCo),e(b7,rI),e(rI,ZCo),e(b7,e4o),e(G,o4o),e(G,v7),e(v7,Xre),e(Xre,r4o),e(v7,t4o),e(v7,tI),e(tI,a4o),e(v7,n4o),e(G,s4o),e(G,T7),e(T7,zre),e(zre,l4o),e(T7,i4o),e(T7,aI),e(aI,d4o),e(T7,c4o),e(G,f4o),e(G,F7),e(F7,Vre),e(Vre,m4o),e(F7,g4o),e(F7,nI),e(nI,h4o),e(F7,p4o),e(G,_4o),e(G,C7),e(C7,Wre),e(Wre,u4o),e(C7,b4o),e(C7,sI),e(sI,v4o),e(C7,T4o),e(G,F4o),e(G,M7),e(M7,Qre),e(Qre,C4o),e(M7,M4o),e(M7,lI),e(lI,E4o),e(M7,y4o),e(G,w4o),e(G,E7),e(E7,Hre),e(Hre,A4o),e(E7,L4o),e(E7,iI),e(iI,B4o),e(E7,k4o),e(G,x4o),e(G,y7),e(y7,Ure),e(Ure,R4o),e(y7,S4o),e(y7,dI),e(dI,P4o),e(y7,$4o),e(G,I4o),e(G,w7),e(w7,Jre),e(Jre,j4o),e(w7,N4o),e(w7,cI),e(cI,D4o),e(w7,q4o),e(G,G4o),e(G,A7),e(A7,Yre),e(Yre,O4o),e(A7,X4o),e(A7,fI),e(fI,z4o),e(A7,V4o),e(G,W4o),e(G,L7),e(L7,Kre),e(Kre,Q4o),e(L7,H4o),e(L7,mI),e(mI,U4o),e(L7,J4o),e(G,Y4o),e(G,B7),e(B7,Zre),e(Zre,K4o),e(B7,Z4o),e(B7,gI),e(gI,eMo),e(B7,oMo),e(G,rMo),e(G,k7),e(k7,ete),e(ete,tMo),e(k7,aMo),e(k7,hI),e(hI,nMo),e(k7,sMo),e(G,lMo),e(G,x7),e(x7,ote),e(ote,iMo),e(x7,dMo),e(x7,pI),e(pI,cMo),e(x7,fMo),e(G,mMo),e(G,R7),e(R7,rte),e(rte,gMo),e(R7,hMo),e(R7,_I),e(_I,pMo),e(R7,_Mo),e(G,uMo),e(G,S7),e(S7,tte),e(tte,bMo),e(S7,vMo),e(S7,uI),e(uI,TMo),e(S7,FMo),e(G,CMo),e(G,P7),e(P7,ate),e(ate,MMo),e(P7,EMo),e(P7,bI),e(bI,yMo),e(P7,wMo),e(G,AMo),e(G,$7),e($7,nte),e(nte,LMo),e($7,BMo),e($7,vI),e(vI,kMo),e($7,xMo),e(G,RMo),e(G,I7),e(I7,ste),e(ste,SMo),e(I7,PMo),e(I7,TI),e(TI,$Mo),e(I7,IMo),e(G,jMo),e(G,j7),e(j7,lte),e(lte,NMo),e(j7,DMo),e(j7,FI),e(FI,qMo),e(j7,GMo),e(G,OMo),e(G,N7),e(N7,ite),e(ite,XMo),e(N7,zMo),e(N7,CI),e(CI,VMo),e(N7,WMo),e(G,QMo),e(G,D7),e(D7,dte),e(dte,HMo),e(D7,UMo),e(D7,MI),e(MI,JMo),e(D7,YMo),e(G,KMo),e(G,q7),e(q7,cte),e(cte,ZMo),e(q7,eEo),e(q7,EI),e(EI,oEo),e(q7,rEo),e(G,tEo),e(G,G7),e(G7,fte),e(fte,aEo),e(G7,nEo),e(G7,yI),e(yI,sEo),e(G7,lEo),e(G,iEo),e(G,O7),e(O7,mte),e(mte,dEo),e(O7,cEo),e(O7,wI),e(wI,fEo),e(O7,mEo),e(G,gEo),e(G,X7),e(X7,gte),e(gte,hEo),e(X7,pEo),e(X7,AI),e(AI,_Eo),e(X7,uEo),e(je,bEo),e(je,z7),e(z7,vEo),e(z7,hte),e(hte,TEo),e(z7,FEo),e(z7,pte),e(pte,CEo),e(je,MEo),e(je,_te),e(_te,EEo),e(je,yEo),g(SE,je,null),b(d,W8e,u),b(d,ld,u),e(ld,V7),e(V7,ute),g(PE,ute,null),e(ld,wEo),e(ld,bte),e(bte,AEo),b(d,Q8e,u),b(d,Ko,u),g($E,Ko,null),e(Ko,LEo),e(Ko,id),e(id,BEo),e(id,vte),e(vte,kEo),e(id,xEo),e(id,Tte),e(Tte,REo),e(id,SEo),e(Ko,PEo),e(Ko,IE),e(IE,$Eo),e(IE,Fte),e(Fte,IEo),e(IE,jEo),e(Ko,NEo),e(Ko,Vr),g(jE,Vr,null),e(Vr,DEo),e(Vr,Cte),e(Cte,qEo),e(Vr,GEo),e(Vr,dd),e(dd,OEo),e(dd,Mte),e(Mte,XEo),e(dd,zEo),e(dd,Ete),e(Ete,VEo),e(dd,WEo),e(Vr,QEo),e(Vr,yte),e(yte,HEo),e(Vr,UEo),g(NE,Vr,null),e(Ko,JEo),e(Ko,Ne),g(DE,Ne,null),e(Ne,YEo),e(Ne,wte),e(wte,KEo),e(Ne,ZEo),e(Ne,Wa),e(Wa,e3o),e(Wa,Ate),e(Ate,o3o),e(Wa,r3o),e(Wa,Lte),e(Lte,t3o),e(Wa,a3o),e(Wa,Bte),e(Bte,n3o),e(Wa,s3o),e(Ne,l3o),e(Ne,na),e(na,W7),e(W7,kte),e(kte,i3o),e(W7,d3o),e(W7,LI),e(LI,c3o),e(W7,f3o),e(na,m3o),e(na,Q7),e(Q7,xte),e(xte,g3o),e(Q7,h3o),e(Q7,BI),e(BI,p3o),e(Q7,_3o),e(na,u3o),e(na,H7),e(H7,Rte),e(Rte,b3o),e(H7,v3o),e(H7,kI),e(kI,T3o),e(H7,F3o),e(na,C3o),e(na,U7),e(U7,Ste),e(Ste,M3o),e(U7,E3o),e(U7,xI),e(xI,y3o),e(U7,w3o),e(na,A3o),e(na,J7),e(J7,Pte),e(Pte,L3o),e(J7,B3o),e(J7,RI),e(RI,k3o),e(J7,x3o),e(Ne,R3o),e(Ne,Y7),e(Y7,S3o),e(Y7,$te),e($te,P3o),e(Y7,$3o),e(Y7,Ite),e(Ite,I3o),e(Ne,j3o),e(Ne,jte),e(jte,N3o),e(Ne,D3o),g(qE,Ne,null),b(d,H8e,u),b(d,cd,u),e(cd,K7),e(K7,Nte),g(GE,Nte,null),e(cd,q3o),e(cd,Dte),e(Dte,G3o),b(d,U8e,u),b(d,Zo,u),g(OE,Zo,null),e(Zo,O3o),e(Zo,fd),e(fd,X3o),e(fd,qte),e(qte,z3o),e(fd,V3o),e(fd,Gte),e(Gte,W3o),e(fd,Q3o),e(Zo,H3o),e(Zo,XE),e(XE,U3o),e(XE,Ote),e(Ote,J3o),e(XE,Y3o),e(Zo,K3o),e(Zo,Wr),g(zE,Wr,null),e(Wr,Z3o),e(Wr,Xte),e(Xte,eyo),e(Wr,oyo),e(Wr,md),e(md,ryo),e(md,zte),e(zte,tyo),e(md,ayo),e(md,Vte),e(Vte,nyo),e(md,syo),e(Wr,lyo),e(Wr,Wte),e(Wte,iyo),e(Wr,dyo),g(VE,Wr,null),e(Zo,cyo),e(Zo,De),g(WE,De,null),e(De,fyo),e(De,Qte),e(Qte,myo),e(De,gyo),e(De,Qa),e(Qa,hyo),e(Qa,Hte),e(Hte,pyo),e(Qa,_yo),e(Qa,Ute),e(Ute,uyo),e(Qa,byo),e(Qa,Jte),e(Jte,vyo),e(Qa,Tyo),e(De,Fyo),e(De,D),e(D,Z7),e(Z7,Yte),e(Yte,Cyo),e(Z7,Myo),e(Z7,SI),e(SI,Eyo),e(Z7,yyo),e(D,wyo),e(D,eb),e(eb,Kte),e(Kte,Ayo),e(eb,Lyo),e(eb,PI),e(PI,Byo),e(eb,kyo),e(D,xyo),e(D,ob),e(ob,Zte),e(Zte,Ryo),e(ob,Syo),e(ob,$I),e($I,Pyo),e(ob,$yo),e(D,Iyo),e(D,rb),e(rb,eae),e(eae,jyo),e(rb,Nyo),e(rb,II),e(II,Dyo),e(rb,qyo),e(D,Gyo),e(D,tb),e(tb,oae),e(oae,Oyo),e(tb,Xyo),e(tb,jI),e(jI,zyo),e(tb,Vyo),e(D,Wyo),e(D,ab),e(ab,rae),e(rae,Qyo),e(ab,Hyo),e(ab,NI),e(NI,Uyo),e(ab,Jyo),e(D,Yyo),e(D,nb),e(nb,tae),e(tae,Kyo),e(nb,Zyo),e(nb,DI),e(DI,ewo),e(nb,owo),e(D,rwo),e(D,sb),e(sb,aae),e(aae,two),e(sb,awo),e(sb,qI),e(qI,nwo),e(sb,swo),e(D,lwo),e(D,lb),e(lb,nae),e(nae,iwo),e(lb,dwo),e(lb,GI),e(GI,cwo),e(lb,fwo),e(D,mwo),e(D,ib),e(ib,sae),e(sae,gwo),e(ib,hwo),e(ib,OI),e(OI,pwo),e(ib,_wo),e(D,uwo),e(D,db),e(db,lae),e(lae,bwo),e(db,vwo),e(db,XI),e(XI,Two),e(db,Fwo),e(D,Cwo),e(D,cb),e(cb,iae),e(iae,Mwo),e(cb,Ewo),e(cb,zI),e(zI,ywo),e(cb,wwo),e(D,Awo),e(D,fb),e(fb,dae),e(dae,Lwo),e(fb,Bwo),e(fb,VI),e(VI,kwo),e(fb,xwo),e(D,Rwo),e(D,mb),e(mb,cae),e(cae,Swo),e(mb,Pwo),e(mb,WI),e(WI,$wo),e(mb,Iwo),e(D,jwo),e(D,gb),e(gb,fae),e(fae,Nwo),e(gb,Dwo),e(gb,QI),e(QI,qwo),e(gb,Gwo),e(D,Owo),e(D,hb),e(hb,mae),e(mae,Xwo),e(hb,zwo),e(hb,HI),e(HI,Vwo),e(hb,Wwo),e(D,Qwo),e(D,pb),e(pb,gae),e(gae,Hwo),e(pb,Uwo),e(pb,UI),e(UI,Jwo),e(pb,Ywo),e(D,Kwo),e(D,_b),e(_b,hae),e(hae,Zwo),e(_b,e6o),e(_b,JI),e(JI,o6o),e(_b,r6o),e(D,t6o),e(D,ub),e(ub,pae),e(pae,a6o),e(ub,n6o),e(ub,YI),e(YI,s6o),e(ub,l6o),e(D,i6o),e(D,bb),e(bb,_ae),e(_ae,d6o),e(bb,c6o),e(bb,KI),e(KI,f6o),e(bb,m6o),e(D,g6o),e(D,vb),e(vb,uae),e(uae,h6o),e(vb,p6o),e(vb,ZI),e(ZI,_6o),e(vb,u6o),e(D,b6o),e(D,Tb),e(Tb,bae),e(bae,v6o),e(Tb,T6o),e(Tb,ej),e(ej,F6o),e(Tb,C6o),e(D,M6o),e(D,Fb),e(Fb,vae),e(vae,E6o),e(Fb,y6o),e(Fb,oj),e(oj,w6o),e(Fb,A6o),e(D,L6o),e(D,Cb),e(Cb,Tae),e(Tae,B6o),e(Cb,k6o),e(Cb,rj),e(rj,x6o),e(Cb,R6o),e(D,S6o),e(D,Mb),e(Mb,Fae),e(Fae,P6o),e(Mb,$6o),e(Mb,tj),e(tj,I6o),e(Mb,j6o),e(D,N6o),e(D,Eb),e(Eb,Cae),e(Cae,D6o),e(Eb,q6o),e(Eb,aj),e(aj,G6o),e(Eb,O6o),e(D,X6o),e(D,yb),e(yb,Mae),e(Mae,z6o),e(yb,V6o),e(yb,nj),e(nj,W6o),e(yb,Q6o),e(D,H6o),e(D,wb),e(wb,Eae),e(Eae,U6o),e(wb,J6o),e(wb,sj),e(sj,Y6o),e(wb,K6o),e(D,Z6o),e(D,Ab),e(Ab,yae),e(yae,eAo),e(Ab,oAo),e(Ab,lj),e(lj,rAo),e(Ab,tAo),e(D,aAo),e(D,Lb),e(Lb,wae),e(wae,nAo),e(Lb,sAo),e(Lb,ij),e(ij,lAo),e(Lb,iAo),e(D,dAo),e(D,Bb),e(Bb,Aae),e(Aae,cAo),e(Bb,fAo),e(Bb,dj),e(dj,mAo),e(Bb,gAo),e(D,hAo),e(D,kb),e(kb,Lae),e(Lae,pAo),e(kb,_Ao),e(kb,cj),e(cj,uAo),e(kb,bAo),e(De,vAo),e(De,xb),e(xb,TAo),e(xb,Bae),e(Bae,FAo),e(xb,CAo),e(xb,kae),e(kae,MAo),e(De,EAo),e(De,xae),e(xae,yAo),e(De,wAo),g(QE,De,null),b(d,J8e,u),b(d,gd,u),e(gd,Rb),e(Rb,Rae),g(HE,Rae,null),e(gd,AAo),e(gd,Sae),e(Sae,LAo),b(d,Y8e,u),b(d,er,u),g(UE,er,null),e(er,BAo),e(er,hd),e(hd,kAo),e(hd,Pae),e(Pae,xAo),e(hd,RAo),e(hd,$ae),e($ae,SAo),e(hd,PAo),e(er,$Ao),e(er,JE),e(JE,IAo),e(JE,Iae),e(Iae,jAo),e(JE,NAo),e(er,DAo),e(er,Qr),g(YE,Qr,null),e(Qr,qAo),e(Qr,jae),e(jae,GAo),e(Qr,OAo),e(Qr,pd),e(pd,XAo),e(pd,Nae),e(Nae,zAo),e(pd,VAo),e(pd,Dae),e(Dae,WAo),e(pd,QAo),e(Qr,HAo),e(Qr,qae),e(qae,UAo),e(Qr,JAo),g(KE,Qr,null),e(er,YAo),e(er,qe),g(ZE,qe,null),e(qe,KAo),e(qe,Gae),e(Gae,ZAo),e(qe,eLo),e(qe,Ha),e(Ha,oLo),e(Ha,Oae),e(Oae,rLo),e(Ha,tLo),e(Ha,Xae),e(Xae,aLo),e(Ha,nLo),e(Ha,zae),e(zae,sLo),e(Ha,lLo),e(qe,iLo),e(qe,R),e(R,Sb),e(Sb,Vae),e(Vae,dLo),e(Sb,cLo),e(Sb,fj),e(fj,fLo),e(Sb,mLo),e(R,gLo),e(R,Pb),e(Pb,Wae),e(Wae,hLo),e(Pb,pLo),e(Pb,mj),e(mj,_Lo),e(Pb,uLo),e(R,bLo),e(R,$b),e($b,Qae),e(Qae,vLo),e($b,TLo),e($b,gj),e(gj,FLo),e($b,CLo),e(R,MLo),e(R,Ib),e(Ib,Hae),e(Hae,ELo),e(Ib,yLo),e(Ib,hj),e(hj,wLo),e(Ib,ALo),e(R,LLo),e(R,jb),e(jb,Uae),e(Uae,BLo),e(jb,kLo),e(jb,pj),e(pj,xLo),e(jb,RLo),e(R,SLo),e(R,Nb),e(Nb,Jae),e(Jae,PLo),e(Nb,$Lo),e(Nb,_j),e(_j,ILo),e(Nb,jLo),e(R,NLo),e(R,Db),e(Db,Yae),e(Yae,DLo),e(Db,qLo),e(Db,uj),e(uj,GLo),e(Db,OLo),e(R,XLo),e(R,qb),e(qb,Kae),e(Kae,zLo),e(qb,VLo),e(qb,bj),e(bj,WLo),e(qb,QLo),e(R,HLo),e(R,Gb),e(Gb,Zae),e(Zae,ULo),e(Gb,JLo),e(Gb,vj),e(vj,YLo),e(Gb,KLo),e(R,ZLo),e(R,Ob),e(Ob,ene),e(ene,e8o),e(Ob,o8o),e(Ob,Tj),e(Tj,r8o),e(Ob,t8o),e(R,a8o),e(R,Xb),e(Xb,one),e(one,n8o),e(Xb,s8o),e(Xb,Fj),e(Fj,l8o),e(Xb,i8o),e(R,d8o),e(R,zb),e(zb,rne),e(rne,c8o),e(zb,f8o),e(zb,Cj),e(Cj,m8o),e(zb,g8o),e(R,h8o),e(R,Vb),e(Vb,tne),e(tne,p8o),e(Vb,_8o),e(Vb,Mj),e(Mj,u8o),e(Vb,b8o),e(R,v8o),e(R,Wb),e(Wb,ane),e(ane,T8o),e(Wb,F8o),e(Wb,Ej),e(Ej,C8o),e(Wb,M8o),e(R,E8o),e(R,Qb),e(Qb,nne),e(nne,y8o),e(Qb,w8o),e(Qb,yj),e(yj,A8o),e(Qb,L8o),e(R,B8o),e(R,Hb),e(Hb,sne),e(sne,k8o),e(Hb,x8o),e(Hb,wj),e(wj,R8o),e(Hb,S8o),e(R,P8o),e(R,Ub),e(Ub,lne),e(lne,$8o),e(Ub,I8o),e(Ub,Aj),e(Aj,j8o),e(Ub,N8o),e(R,D8o),e(R,Jb),e(Jb,ine),e(ine,q8o),e(Jb,G8o),e(Jb,Lj),e(Lj,O8o),e(Jb,X8o),e(R,z8o),e(R,Yb),e(Yb,dne),e(dne,V8o),e(Yb,W8o),e(Yb,Bj),e(Bj,Q8o),e(Yb,H8o),e(R,U8o),e(R,Kb),e(Kb,cne),e(cne,J8o),e(Kb,Y8o),e(Kb,kj),e(kj,K8o),e(Kb,Z8o),e(R,e9o),e(R,Zb),e(Zb,fne),e(fne,o9o),e(Zb,r9o),e(Zb,xj),e(xj,t9o),e(Zb,a9o),e(R,n9o),e(R,e5),e(e5,mne),e(mne,s9o),e(e5,l9o),e(e5,Rj),e(Rj,i9o),e(e5,d9o),e(R,c9o),e(R,o5),e(o5,gne),e(gne,f9o),e(o5,m9o),e(o5,Sj),e(Sj,g9o),e(o5,h9o),e(R,p9o),e(R,r5),e(r5,hne),e(hne,_9o),e(r5,u9o),e(r5,Pj),e(Pj,b9o),e(r5,v9o),e(R,T9o),e(R,t5),e(t5,pne),e(pne,F9o),e(t5,C9o),e(t5,$j),e($j,M9o),e(t5,E9o),e(R,y9o),e(R,a5),e(a5,_ne),e(_ne,w9o),e(a5,A9o),e(a5,Ij),e(Ij,L9o),e(a5,B9o),e(R,k9o),e(R,n5),e(n5,une),e(une,x9o),e(n5,R9o),e(n5,jj),e(jj,S9o),e(n5,P9o),e(R,$9o),e(R,s5),e(s5,bne),e(bne,I9o),e(s5,j9o),e(s5,Nj),e(Nj,N9o),e(s5,D9o),e(R,q9o),e(R,l5),e(l5,vne),e(vne,G9o),e(l5,O9o),e(l5,Dj),e(Dj,X9o),e(l5,z9o),e(R,V9o),e(R,i5),e(i5,Tne),e(Tne,W9o),e(i5,Q9o),e(i5,qj),e(qj,H9o),e(i5,U9o),e(R,J9o),e(R,d5),e(d5,Fne),e(Fne,Y9o),e(d5,K9o),e(d5,Gj),e(Gj,Z9o),e(d5,eBo),e(R,oBo),e(R,c5),e(c5,Cne),e(Cne,rBo),e(c5,tBo),e(c5,Oj),e(Oj,aBo),e(c5,nBo),e(R,sBo),e(R,f5),e(f5,Mne),e(Mne,lBo),e(f5,iBo),e(f5,Xj),e(Xj,dBo),e(f5,cBo),e(R,fBo),e(R,m5),e(m5,Ene),e(Ene,mBo),e(m5,gBo),e(m5,zj),e(zj,hBo),e(m5,pBo),e(R,_Bo),e(R,g5),e(g5,yne),e(yne,uBo),e(g5,bBo),e(g5,Vj),e(Vj,vBo),e(g5,TBo),e(R,FBo),e(R,h5),e(h5,wne),e(wne,CBo),e(h5,MBo),e(h5,Wj),e(Wj,EBo),e(h5,yBo),e(R,wBo),e(R,p5),e(p5,Ane),e(Ane,ABo),e(p5,LBo),e(p5,Qj),e(Qj,BBo),e(p5,kBo),e(R,xBo),e(R,_5),e(_5,Lne),e(Lne,RBo),e(_5,SBo),e(_5,Hj),e(Hj,PBo),e(_5,$Bo),e(qe,IBo),e(qe,u5),e(u5,jBo),e(u5,Bne),e(Bne,NBo),e(u5,DBo),e(u5,kne),e(kne,qBo),e(qe,GBo),e(qe,xne),e(xne,OBo),e(qe,XBo),g(e3,qe,null),b(d,K8e,u),b(d,_d,u),e(_d,b5),e(b5,Rne),g(o3,Rne,null),e(_d,zBo),e(_d,Sne),e(Sne,VBo),b(d,Z8e,u),b(d,or,u),g(r3,or,null),e(or,WBo),e(or,ud),e(ud,QBo),e(ud,Pne),e(Pne,HBo),e(ud,UBo),e(ud,$ne),e($ne,JBo),e(ud,YBo),e(or,KBo),e(or,t3),e(t3,ZBo),e(t3,Ine),e(Ine,eko),e(t3,oko),e(or,rko),e(or,Hr),g(a3,Hr,null),e(Hr,tko),e(Hr,jne),e(jne,ako),e(Hr,nko),e(Hr,bd),e(bd,sko),e(bd,Nne),e(Nne,lko),e(bd,iko),e(bd,Dne),e(Dne,dko),e(bd,cko),e(Hr,fko),e(Hr,qne),e(qne,mko),e(Hr,gko),g(n3,Hr,null),e(or,hko),e(or,Ge),g(s3,Ge,null),e(Ge,pko),e(Ge,Gne),e(Gne,_ko),e(Ge,uko),e(Ge,Ua),e(Ua,bko),e(Ua,One),e(One,vko),e(Ua,Tko),e(Ua,Xne),e(Xne,Fko),e(Ua,Cko),e(Ua,zne),e(zne,Mko),e(Ua,Eko),e(Ge,yko),e(Ge,Vne),e(Vne,v5),e(v5,Wne),e(Wne,wko),e(v5,Ako),e(v5,Uj),e(Uj,Lko),e(v5,Bko),e(Ge,kko),e(Ge,T5),e(T5,xko),e(T5,Qne),e(Qne,Rko),e(T5,Sko),e(T5,Hne),e(Hne,Pko),e(Ge,$ko),e(Ge,Une),e(Une,Iko),e(Ge,jko),g(l3,Ge,null),b(d,e9e,u),b(d,vd,u),e(vd,F5),e(F5,Jne),g(i3,Jne,null),e(vd,Nko),e(vd,Yne),e(Yne,Dko),b(d,o9e,u),b(d,rr,u),g(d3,rr,null),e(rr,qko),e(rr,Td),e(Td,Gko),e(Td,Kne),e(Kne,Oko),e(Td,Xko),e(Td,Zne),e(Zne,zko),e(Td,Vko),e(rr,Wko),e(rr,c3),e(c3,Qko),e(c3,ese),e(ese,Hko),e(c3,Uko),e(rr,Jko),e(rr,Ur),g(f3,Ur,null),e(Ur,Yko),e(Ur,ose),e(ose,Kko),e(Ur,Zko),e(Ur,Fd),e(Fd,exo),e(Fd,rse),e(rse,oxo),e(Fd,rxo),e(Fd,tse),e(tse,txo),e(Fd,axo),e(Ur,nxo),e(Ur,ase),e(ase,sxo),e(Ur,lxo),g(m3,Ur,null),e(rr,ixo),e(rr,Oe),g(g3,Oe,null),e(Oe,dxo),e(Oe,nse),e(nse,cxo),e(Oe,fxo),e(Oe,Ja),e(Ja,mxo),e(Ja,sse),e(sse,gxo),e(Ja,hxo),e(Ja,lse),e(lse,pxo),e(Ja,_xo),e(Ja,ise),e(ise,uxo),e(Ja,bxo),e(Oe,vxo),e(Oe,he),e(he,C5),e(C5,dse),e(dse,Txo),e(C5,Fxo),e(C5,Jj),e(Jj,Cxo),e(C5,Mxo),e(he,Exo),e(he,M5),e(M5,cse),e(cse,yxo),e(M5,wxo),e(M5,Yj),e(Yj,Axo),e(M5,Lxo),e(he,Bxo),e(he,Rs),e(Rs,fse),e(fse,kxo),e(Rs,xxo),e(Rs,Kj),e(Kj,Rxo),e(Rs,Sxo),e(Rs,Zj),e(Zj,Pxo),e(Rs,$xo),e(he,Ixo),e(he,E5),e(E5,mse),e(mse,jxo),e(E5,Nxo),e(E5,eN),e(eN,Dxo),e(E5,qxo),e(he,Gxo),e(he,la),e(la,gse),e(gse,Oxo),e(la,Xxo),e(la,oN),e(oN,zxo),e(la,Vxo),e(la,rN),e(rN,Wxo),e(la,Qxo),e(la,tN),e(tN,Hxo),e(la,Uxo),e(he,Jxo),e(he,y5),e(y5,hse),e(hse,Yxo),e(y5,Kxo),e(y5,aN),e(aN,Zxo),e(y5,eRo),e(he,oRo),e(he,w5),e(w5,pse),e(pse,rRo),e(w5,tRo),e(w5,nN),e(nN,aRo),e(w5,nRo),e(he,sRo),e(he,A5),e(A5,_se),e(_se,lRo),e(A5,iRo),e(A5,sN),e(sN,dRo),e(A5,cRo),e(he,fRo),e(he,L5),e(L5,use),e(use,mRo),e(L5,gRo),e(L5,lN),e(lN,hRo),e(L5,pRo),e(he,_Ro),e(he,B5),e(B5,bse),e(bse,uRo),e(B5,bRo),e(B5,iN),e(iN,vRo),e(B5,TRo),e(Oe,FRo),e(Oe,k5),e(k5,CRo),e(k5,vse),e(vse,MRo),e(k5,ERo),e(k5,Tse),e(Tse,yRo),e(Oe,wRo),e(Oe,Fse),e(Fse,ARo),e(Oe,LRo),g(h3,Oe,null),b(d,r9e,u),b(d,Cd,u),e(Cd,x5),e(x5,Cse),g(p3,Cse,null),e(Cd,BRo),e(Cd,Mse),e(Mse,kRo),b(d,t9e,u),b(d,tr,u),g(_3,tr,null),e(tr,xRo),e(tr,Md),e(Md,RRo),e(Md,Ese),e(Ese,SRo),e(Md,PRo),e(Md,yse),e(yse,$Ro),e(Md,IRo),e(tr,jRo),e(tr,u3),e(u3,NRo),e(u3,wse),e(wse,DRo),e(u3,qRo),e(tr,GRo),e(tr,Jr),g(b3,Jr,null),e(Jr,ORo),e(Jr,Ase),e(Ase,XRo),e(Jr,zRo),e(Jr,Ed),e(Ed,VRo),e(Ed,Lse),e(Lse,WRo),e(Ed,QRo),e(Ed,Bse),e(Bse,HRo),e(Ed,URo),e(Jr,JRo),e(Jr,kse),e(kse,YRo),e(Jr,KRo),g(v3,Jr,null),e(tr,ZRo),e(tr,Xe),g(T3,Xe,null),e(Xe,eSo),e(Xe,xse),e(xse,oSo),e(Xe,rSo),e(Xe,Ya),e(Ya,tSo),e(Ya,Rse),e(Rse,aSo),e(Ya,nSo),e(Ya,Sse),e(Sse,sSo),e(Ya,lSo),e(Ya,Pse),e(Pse,iSo),e(Ya,dSo),e(Xe,cSo),e(Xe,$se),e($se,R5),e(R5,Ise),e(Ise,fSo),e(R5,mSo),e(R5,dN),e(dN,gSo),e(R5,hSo),e(Xe,pSo),e(Xe,S5),e(S5,_So),e(S5,jse),e(jse,uSo),e(S5,bSo),e(S5,Nse),e(Nse,vSo),e(Xe,TSo),e(Xe,Dse),e(Dse,FSo),e(Xe,CSo),g(F3,Xe,null),b(d,a9e,u),b(d,yd,u),e(yd,P5),e(P5,qse),g(C3,qse,null),e(yd,MSo),e(yd,Gse),e(Gse,ESo),b(d,n9e,u),b(d,ar,u),g(M3,ar,null),e(ar,ySo),e(ar,wd),e(wd,wSo),e(wd,Ose),e(Ose,ASo),e(wd,LSo),e(wd,Xse),e(Xse,BSo),e(wd,kSo),e(ar,xSo),e(ar,E3),e(E3,RSo),e(E3,zse),e(zse,SSo),e(E3,PSo),e(ar,$So),e(ar,Yr),g(y3,Yr,null),e(Yr,ISo),e(Yr,Vse),e(Vse,jSo),e(Yr,NSo),e(Yr,Ad),e(Ad,DSo),e(Ad,Wse),e(Wse,qSo),e(Ad,GSo),e(Ad,Qse),e(Qse,OSo),e(Ad,XSo),e(Yr,zSo),e(Yr,Hse),e(Hse,VSo),e(Yr,WSo),g(w3,Yr,null),e(ar,QSo),e(ar,ze),g(A3,ze,null),e(ze,HSo),e(ze,Use),e(Use,USo),e(ze,JSo),e(ze,Ka),e(Ka,YSo),e(Ka,Jse),e(Jse,KSo),e(Ka,ZSo),e(Ka,Yse),e(Yse,ePo),e(Ka,oPo),e(Ka,Kse),e(Kse,rPo),e(Ka,tPo),e(ze,aPo),e(ze,ao),e(ao,$5),e($5,Zse),e(Zse,nPo),e($5,sPo),e($5,cN),e(cN,lPo),e($5,iPo),e(ao,dPo),e(ao,I5),e(I5,ele),e(ele,cPo),e(I5,fPo),e(I5,fN),e(fN,mPo),e(I5,gPo),e(ao,hPo),e(ao,j5),e(j5,ole),e(ole,pPo),e(j5,_Po),e(j5,mN),e(mN,uPo),e(j5,bPo),e(ao,vPo),e(ao,N5),e(N5,rle),e(rle,TPo),e(N5,FPo),e(N5,gN),e(gN,CPo),e(N5,MPo),e(ao,EPo),e(ao,D5),e(D5,tle),e(tle,yPo),e(D5,wPo),e(D5,hN),e(hN,APo),e(D5,LPo),e(ao,BPo),e(ao,q5),e(q5,ale),e(ale,kPo),e(q5,xPo),e(q5,pN),e(pN,RPo),e(q5,SPo),e(ao,PPo),e(ao,G5),e(G5,nle),e(nle,$Po),e(G5,IPo),e(G5,_N),e(_N,jPo),e(G5,NPo),e(ze,DPo),e(ze,O5),e(O5,qPo),e(O5,sle),e(sle,GPo),e(O5,OPo),e(O5,lle),e(lle,XPo),e(ze,zPo),e(ze,ile),e(ile,VPo),e(ze,WPo),g(L3,ze,null),b(d,s9e,u),b(d,Ld,u),e(Ld,X5),e(X5,dle),g(B3,dle,null),e(Ld,QPo),e(Ld,cle),e(cle,HPo),b(d,l9e,u),b(d,nr,u),g(k3,nr,null),e(nr,UPo),e(nr,Bd),e(Bd,JPo),e(Bd,fle),e(fle,YPo),e(Bd,KPo),e(Bd,mle),e(mle,ZPo),e(Bd,e$o),e(nr,o$o),e(nr,x3),e(x3,r$o),e(x3,gle),e(gle,t$o),e(x3,a$o),e(nr,n$o),e(nr,Kr),g(R3,Kr,null),e(Kr,s$o),e(Kr,hle),e(hle,l$o),e(Kr,i$o),e(Kr,kd),e(kd,d$o),e(kd,ple),e(ple,c$o),e(kd,f$o),e(kd,_le),e(_le,m$o),e(kd,g$o),e(Kr,h$o),e(Kr,ule),e(ule,p$o),e(Kr,_$o),g(S3,Kr,null),e(nr,u$o),e(nr,Ve),g(P3,Ve,null),e(Ve,b$o),e(Ve,ble),e(ble,v$o),e(Ve,T$o),e(Ve,Za),e(Za,F$o),e(Za,vle),e(vle,C$o),e(Za,M$o),e(Za,Tle),e(Tle,E$o),e(Za,y$o),e(Za,Fle),e(Fle,w$o),e(Za,A$o),e(Ve,L$o),e(Ve,xd),e(xd,z5),e(z5,Cle),e(Cle,B$o),e(z5,k$o),e(z5,uN),e(uN,x$o),e(z5,R$o),e(xd,S$o),e(xd,V5),e(V5,Mle),e(Mle,P$o),e(V5,$$o),e(V5,bN),e(bN,I$o),e(V5,j$o),e(xd,N$o),e(xd,W5),e(W5,Ele),e(Ele,D$o),e(W5,q$o),e(W5,vN),e(vN,G$o),e(W5,O$o),e(Ve,X$o),e(Ve,Q5),e(Q5,z$o),e(Q5,yle),e(yle,V$o),e(Q5,W$o),e(Q5,wle),e(wle,Q$o),e(Ve,H$o),e(Ve,Ale),e(Ale,U$o),e(Ve,J$o),g($3,Ve,null),b(d,i9e,u),b(d,Rd,u),e(Rd,H5),e(H5,Lle),g(I3,Lle,null),e(Rd,Y$o),e(Rd,Ble),e(Ble,K$o),b(d,d9e,u),b(d,sr,u),g(j3,sr,null),e(sr,Z$o),e(sr,Sd),e(Sd,eIo),e(Sd,kle),e(kle,oIo),e(Sd,rIo),e(Sd,xle),e(xle,tIo),e(Sd,aIo),e(sr,nIo),e(sr,N3),e(N3,sIo),e(N3,Rle),e(Rle,lIo),e(N3,iIo),e(sr,dIo),e(sr,Zr),g(D3,Zr,null),e(Zr,cIo),e(Zr,Sle),e(Sle,fIo),e(Zr,mIo),e(Zr,Pd),e(Pd,gIo),e(Pd,Ple),e(Ple,hIo),e(Pd,pIo),e(Pd,$le),e($le,_Io),e(Pd,uIo),e(Zr,bIo),e(Zr,Ile),e(Ile,vIo),e(Zr,TIo),g(q3,Zr,null),e(sr,FIo),e(sr,We),g(G3,We,null),e(We,CIo),e(We,jle),e(jle,MIo),e(We,EIo),e(We,en),e(en,yIo),e(en,Nle),e(Nle,wIo),e(en,AIo),e(en,Dle),e(Dle,LIo),e(en,BIo),e(en,qle),e(qle,kIo),e(en,xIo),e(We,RIo),e(We,no),e(no,U5),e(U5,Gle),e(Gle,SIo),e(U5,PIo),e(U5,TN),e(TN,$Io),e(U5,IIo),e(no,jIo),e(no,J5),e(J5,Ole),e(Ole,NIo),e(J5,DIo),e(J5,FN),e(FN,qIo),e(J5,GIo),e(no,OIo),e(no,Y5),e(Y5,Xle),e(Xle,XIo),e(Y5,zIo),e(Y5,CN),e(CN,VIo),e(Y5,WIo),e(no,QIo),e(no,K5),e(K5,zle),e(zle,HIo),e(K5,UIo),e(K5,MN),e(MN,JIo),e(K5,YIo),e(no,KIo),e(no,Z5),e(Z5,Vle),e(Vle,ZIo),e(Z5,ejo),e(Z5,EN),e(EN,ojo),e(Z5,rjo),e(no,tjo),e(no,e2),e(e2,Wle),e(Wle,ajo),e(e2,njo),e(e2,yN),e(yN,sjo),e(e2,ljo),e(no,ijo),e(no,o2),e(o2,Qle),e(Qle,djo),e(o2,cjo),e(o2,wN),e(wN,fjo),e(o2,mjo),e(We,gjo),e(We,r2),e(r2,hjo),e(r2,Hle),e(Hle,pjo),e(r2,_jo),e(r2,Ule),e(Ule,ujo),e(We,bjo),e(We,Jle),e(Jle,vjo),e(We,Tjo),g(O3,We,null),b(d,c9e,u),b(d,$d,u),e($d,t2),e(t2,Yle),g(X3,Yle,null),e($d,Fjo),e($d,Kle),e(Kle,Cjo),b(d,f9e,u),b(d,lr,u),g(z3,lr,null),e(lr,Mjo),e(lr,Id),e(Id,Ejo),e(Id,Zle),e(Zle,yjo),e(Id,wjo),e(Id,eie),e(eie,Ajo),e(Id,Ljo),e(lr,Bjo),e(lr,V3),e(V3,kjo),e(V3,oie),e(oie,xjo),e(V3,Rjo),e(lr,Sjo),e(lr,et),g(W3,et,null),e(et,Pjo),e(et,rie),e(rie,$jo),e(et,Ijo),e(et,jd),e(jd,jjo),e(jd,tie),e(tie,Njo),e(jd,Djo),e(jd,aie),e(aie,qjo),e(jd,Gjo),e(et,Ojo),e(et,nie),e(nie,Xjo),e(et,zjo),g(Q3,et,null),e(lr,Vjo),e(lr,Qe),g(H3,Qe,null),e(Qe,Wjo),e(Qe,sie),e(sie,Qjo),e(Qe,Hjo),e(Qe,on),e(on,Ujo),e(on,lie),e(lie,Jjo),e(on,Yjo),e(on,iie),e(iie,Kjo),e(on,Zjo),e(on,die),e(die,eNo),e(on,oNo),e(Qe,rNo),e(Qe,U3),e(U3,a2),e(a2,cie),e(cie,tNo),e(a2,aNo),e(a2,AN),e(AN,nNo),e(a2,sNo),e(U3,lNo),e(U3,n2),e(n2,fie),e(fie,iNo),e(n2,dNo),e(n2,LN),e(LN,cNo),e(n2,fNo),e(Qe,mNo),e(Qe,s2),e(s2,gNo),e(s2,mie),e(mie,hNo),e(s2,pNo),e(s2,gie),e(gie,_No),e(Qe,uNo),e(Qe,hie),e(hie,bNo),e(Qe,vNo),g(J3,Qe,null),b(d,m9e,u),b(d,Nd,u),e(Nd,l2),e(l2,pie),g(Y3,pie,null),e(Nd,TNo),e(Nd,_ie),e(_ie,FNo),b(d,g9e,u),b(d,ir,u),g(K3,ir,null),e(ir,CNo),e(ir,Dd),e(Dd,MNo),e(Dd,uie),e(uie,ENo),e(Dd,yNo),e(Dd,bie),e(bie,wNo),e(Dd,ANo),e(ir,LNo),e(ir,Z3),e(Z3,BNo),e(Z3,vie),e(vie,kNo),e(Z3,xNo),e(ir,RNo),e(ir,ot),g(ey,ot,null),e(ot,SNo),e(ot,Tie),e(Tie,PNo),e(ot,$No),e(ot,qd),e(qd,INo),e(qd,Fie),e(Fie,jNo),e(qd,NNo),e(qd,Cie),e(Cie,DNo),e(qd,qNo),e(ot,GNo),e(ot,Mie),e(Mie,ONo),e(ot,XNo),g(oy,ot,null),e(ir,zNo),e(ir,He),g(ry,He,null),e(He,VNo),e(He,Eie),e(Eie,WNo),e(He,QNo),e(He,rn),e(rn,HNo),e(rn,yie),e(yie,UNo),e(rn,JNo),e(rn,wie),e(wie,YNo),e(rn,KNo),e(rn,Aie),e(Aie,ZNo),e(rn,eDo),e(He,oDo),e(He,Gd),e(Gd,i2),e(i2,Lie),e(Lie,rDo),e(i2,tDo),e(i2,BN),e(BN,aDo),e(i2,nDo),e(Gd,sDo),e(Gd,d2),e(d2,Bie),e(Bie,lDo),e(d2,iDo),e(d2,kN),e(kN,dDo),e(d2,cDo),e(Gd,fDo),e(Gd,c2),e(c2,kie),e(kie,mDo),e(c2,gDo),e(c2,xN),e(xN,hDo),e(c2,pDo),e(He,_Do),e(He,f2),e(f2,uDo),e(f2,xie),e(xie,bDo),e(f2,vDo),e(f2,Rie),e(Rie,TDo),e(He,FDo),e(He,Sie),e(Sie,CDo),e(He,MDo),g(ty,He,null),b(d,h9e,u),b(d,Od,u),e(Od,m2),e(m2,Pie),g(ay,Pie,null),e(Od,EDo),e(Od,$ie),e($ie,yDo),b(d,p9e,u),b(d,dr,u),g(ny,dr,null),e(dr,wDo),e(dr,Xd),e(Xd,ADo),e(Xd,Iie),e(Iie,LDo),e(Xd,BDo),e(Xd,jie),e(jie,kDo),e(Xd,xDo),e(dr,RDo),e(dr,sy),e(sy,SDo),e(sy,Nie),e(Nie,PDo),e(sy,$Do),e(dr,IDo),e(dr,rt),g(ly,rt,null),e(rt,jDo),e(rt,Die),e(Die,NDo),e(rt,DDo),e(rt,zd),e(zd,qDo),e(zd,qie),e(qie,GDo),e(zd,ODo),e(zd,Gie),e(Gie,XDo),e(zd,zDo),e(rt,VDo),e(rt,Oie),e(Oie,WDo),e(rt,QDo),g(iy,rt,null),e(dr,HDo),e(dr,Ue),g(dy,Ue,null),e(Ue,UDo),e(Ue,Xie),e(Xie,JDo),e(Ue,YDo),e(Ue,tn),e(tn,KDo),e(tn,zie),e(zie,ZDo),e(tn,eqo),e(tn,Vie),e(Vie,oqo),e(tn,rqo),e(tn,Wie),e(Wie,tqo),e(tn,aqo),e(Ue,nqo),e(Ue,Vd),e(Vd,g2),e(g2,Qie),e(Qie,sqo),e(g2,lqo),e(g2,RN),e(RN,iqo),e(g2,dqo),e(Vd,cqo),e(Vd,h2),e(h2,Hie),e(Hie,fqo),e(h2,mqo),e(h2,SN),e(SN,gqo),e(h2,hqo),e(Vd,pqo),e(Vd,p2),e(p2,Uie),e(Uie,_qo),e(p2,uqo),e(p2,PN),e(PN,bqo),e(p2,vqo),e(Ue,Tqo),e(Ue,_2),e(_2,Fqo),e(_2,Jie),e(Jie,Cqo),e(_2,Mqo),e(_2,Yie),e(Yie,Eqo),e(Ue,yqo),e(Ue,Kie),e(Kie,wqo),e(Ue,Aqo),g(cy,Ue,null),b(d,_9e,u),b(d,Wd,u),e(Wd,u2),e(u2,Zie),g(fy,Zie,null),e(Wd,Lqo),e(Wd,ede),e(ede,Bqo),b(d,u9e,u),b(d,cr,u),g(my,cr,null),e(cr,kqo),e(cr,Qd),e(Qd,xqo),e(Qd,ode),e(ode,Rqo),e(Qd,Sqo),e(Qd,rde),e(rde,Pqo),e(Qd,$qo),e(cr,Iqo),e(cr,gy),e(gy,jqo),e(gy,tde),e(tde,Nqo),e(gy,Dqo),e(cr,qqo),e(cr,tt),g(hy,tt,null),e(tt,Gqo),e(tt,ade),e(ade,Oqo),e(tt,Xqo),e(tt,Hd),e(Hd,zqo),e(Hd,nde),e(nde,Vqo),e(Hd,Wqo),e(Hd,sde),e(sde,Qqo),e(Hd,Hqo),e(tt,Uqo),e(tt,lde),e(lde,Jqo),e(tt,Yqo),g(py,tt,null),e(cr,Kqo),e(cr,Je),g(_y,Je,null),e(Je,Zqo),e(Je,ide),e(ide,eGo),e(Je,oGo),e(Je,an),e(an,rGo),e(an,dde),e(dde,tGo),e(an,aGo),e(an,cde),e(cde,nGo),e(an,sGo),e(an,fde),e(fde,lGo),e(an,iGo),e(Je,dGo),e(Je,mde),e(mde,b2),e(b2,gde),e(gde,cGo),e(b2,fGo),e(b2,$N),e($N,mGo),e(b2,gGo),e(Je,hGo),e(Je,v2),e(v2,pGo),e(v2,hde),e(hde,_Go),e(v2,uGo),e(v2,pde),e(pde,bGo),e(Je,vGo),e(Je,_de),e(_de,TGo),e(Je,FGo),g(uy,Je,null),b(d,b9e,u),b(d,Ud,u),e(Ud,T2),e(T2,ude),g(by,ude,null),e(Ud,CGo),e(Ud,bde),e(bde,MGo),b(d,v9e,u),b(d,fr,u),g(vy,fr,null),e(fr,EGo),e(fr,Jd),e(Jd,yGo),e(Jd,vde),e(vde,wGo),e(Jd,AGo),e(Jd,Tde),e(Tde,LGo),e(Jd,BGo),e(fr,kGo),e(fr,Ty),e(Ty,xGo),e(Ty,Fde),e(Fde,RGo),e(Ty,SGo),e(fr,PGo),e(fr,at),g(Fy,at,null),e(at,$Go),e(at,Cde),e(Cde,IGo),e(at,jGo),e(at,Yd),e(Yd,NGo),e(Yd,Mde),e(Mde,DGo),e(Yd,qGo),e(Yd,Ede),e(Ede,GGo),e(Yd,OGo),e(at,XGo),e(at,yde),e(yde,zGo),e(at,VGo),g(Cy,at,null),e(fr,WGo),e(fr,Ye),g(My,Ye,null),e(Ye,QGo),e(Ye,wde),e(wde,HGo),e(Ye,UGo),e(Ye,nn),e(nn,JGo),e(nn,Ade),e(Ade,YGo),e(nn,KGo),e(nn,Lde),e(Lde,ZGo),e(nn,eOo),e(nn,Bde),e(Bde,oOo),e(nn,rOo),e(Ye,tOo),e(Ye,kde),e(kde,F2),e(F2,xde),e(xde,aOo),e(F2,nOo),e(F2,IN),e(IN,sOo),e(F2,lOo),e(Ye,iOo),e(Ye,C2),e(C2,dOo),e(C2,Rde),e(Rde,cOo),e(C2,fOo),e(C2,Sde),e(Sde,mOo),e(Ye,gOo),e(Ye,Pde),e(Pde,hOo),e(Ye,pOo),g(Ey,Ye,null),b(d,T9e,u),b(d,Kd,u),e(Kd,M2),e(M2,$de),g(yy,$de,null),e(Kd,_Oo),e(Kd,Ide),e(Ide,uOo),b(d,F9e,u),b(d,mr,u),g(wy,mr,null),e(mr,bOo),e(mr,Zd),e(Zd,vOo),e(Zd,jde),e(jde,TOo),e(Zd,FOo),e(Zd,Nde),e(Nde,COo),e(Zd,MOo),e(mr,EOo),e(mr,Ay),e(Ay,yOo),e(Ay,Dde),e(Dde,wOo),e(Ay,AOo),e(mr,LOo),e(mr,nt),g(Ly,nt,null),e(nt,BOo),e(nt,qde),e(qde,kOo),e(nt,xOo),e(nt,ec),e(ec,ROo),e(ec,Gde),e(Gde,SOo),e(ec,POo),e(ec,Ode),e(Ode,$Oo),e(ec,IOo),e(nt,jOo),e(nt,Xde),e(Xde,NOo),e(nt,DOo),g(By,nt,null),e(mr,qOo),e(mr,Ke),g(ky,Ke,null),e(Ke,GOo),e(Ke,zde),e(zde,OOo),e(Ke,XOo),e(Ke,sn),e(sn,zOo),e(sn,Vde),e(Vde,VOo),e(sn,WOo),e(sn,Wde),e(Wde,QOo),e(sn,HOo),e(sn,Qde),e(Qde,UOo),e(sn,JOo),e(Ke,YOo),e(Ke,xy),e(xy,E2),e(E2,Hde),e(Hde,KOo),e(E2,ZOo),e(E2,jN),e(jN,eXo),e(E2,oXo),e(xy,rXo),e(xy,y2),e(y2,Ude),e(Ude,tXo),e(y2,aXo),e(y2,NN),e(NN,nXo),e(y2,sXo),e(Ke,lXo),e(Ke,w2),e(w2,iXo),e(w2,Jde),e(Jde,dXo),e(w2,cXo),e(w2,Yde),e(Yde,fXo),e(Ke,mXo),e(Ke,Kde),e(Kde,gXo),e(Ke,hXo),g(Ry,Ke,null),b(d,C9e,u),b(d,oc,u),e(oc,A2),e(A2,Zde),g(Sy,Zde,null),e(oc,pXo),e(oc,ece),e(ece,_Xo),b(d,M9e,u),b(d,gr,u),g(Py,gr,null),e(gr,uXo),e(gr,rc),e(rc,bXo),e(rc,oce),e(oce,vXo),e(rc,TXo),e(rc,rce),e(rce,FXo),e(rc,CXo),e(gr,MXo),e(gr,$y),e($y,EXo),e($y,tce),e(tce,yXo),e($y,wXo),e(gr,AXo),e(gr,st),g(Iy,st,null),e(st,LXo),e(st,ace),e(ace,BXo),e(st,kXo),e(st,tc),e(tc,xXo),e(tc,nce),e(nce,RXo),e(tc,SXo),e(tc,sce),e(sce,PXo),e(tc,$Xo),e(st,IXo),e(st,lce),e(lce,jXo),e(st,NXo),g(jy,st,null),e(gr,DXo),e(gr,go),g(Ny,go,null),e(go,qXo),e(go,ice),e(ice,GXo),e(go,OXo),e(go,ln),e(ln,XXo),e(ln,dce),e(dce,zXo),e(ln,VXo),e(ln,cce),e(cce,WXo),e(ln,QXo),e(ln,fce),e(fce,HXo),e(ln,UXo),e(go,JXo),e(go,B),e(B,L2),e(L2,mce),e(mce,YXo),e(L2,KXo),e(L2,DN),e(DN,ZXo),e(L2,ezo),e(B,ozo),e(B,B2),e(B2,gce),e(gce,rzo),e(B2,tzo),e(B2,qN),e(qN,azo),e(B2,nzo),e(B,szo),e(B,k2),e(k2,hce),e(hce,lzo),e(k2,izo),e(k2,GN),e(GN,dzo),e(k2,czo),e(B,fzo),e(B,x2),e(x2,pce),e(pce,mzo),e(x2,gzo),e(x2,ON),e(ON,hzo),e(x2,pzo),e(B,_zo),e(B,R2),e(R2,_ce),e(_ce,uzo),e(R2,bzo),e(R2,XN),e(XN,vzo),e(R2,Tzo),e(B,Fzo),e(B,S2),e(S2,uce),e(uce,Czo),e(S2,Mzo),e(S2,zN),e(zN,Ezo),e(S2,yzo),e(B,wzo),e(B,P2),e(P2,bce),e(bce,Azo),e(P2,Lzo),e(P2,VN),e(VN,Bzo),e(P2,kzo),e(B,xzo),e(B,$2),e($2,vce),e(vce,Rzo),e($2,Szo),e($2,WN),e(WN,Pzo),e($2,$zo),e(B,Izo),e(B,I2),e(I2,Tce),e(Tce,jzo),e(I2,Nzo),e(I2,QN),e(QN,Dzo),e(I2,qzo),e(B,Gzo),e(B,j2),e(j2,Fce),e(Fce,Ozo),e(j2,Xzo),e(j2,HN),e(HN,zzo),e(j2,Vzo),e(B,Wzo),e(B,N2),e(N2,Cce),e(Cce,Qzo),e(N2,Hzo),e(N2,UN),e(UN,Uzo),e(N2,Jzo),e(B,Yzo),e(B,D2),e(D2,Mce),e(Mce,Kzo),e(D2,Zzo),e(D2,JN),e(JN,eVo),e(D2,oVo),e(B,rVo),e(B,q2),e(q2,Ece),e(Ece,tVo),e(q2,aVo),e(q2,YN),e(YN,nVo),e(q2,sVo),e(B,lVo),e(B,G2),e(G2,yce),e(yce,iVo),e(G2,dVo),e(G2,KN),e(KN,cVo),e(G2,fVo),e(B,mVo),e(B,O2),e(O2,wce),e(wce,gVo),e(O2,hVo),e(O2,ZN),e(ZN,pVo),e(O2,_Vo),e(B,uVo),e(B,Ss),e(Ss,Ace),e(Ace,bVo),e(Ss,vVo),e(Ss,eD),e(eD,TVo),e(Ss,FVo),e(Ss,oD),e(oD,CVo),e(Ss,MVo),e(B,EVo),e(B,X2),e(X2,Lce),e(Lce,yVo),e(X2,wVo),e(X2,rD),e(rD,AVo),e(X2,LVo),e(B,BVo),e(B,z2),e(z2,Bce),e(Bce,kVo),e(z2,xVo),e(z2,tD),e(tD,RVo),e(z2,SVo),e(B,PVo),e(B,V2),e(V2,kce),e(kce,$Vo),e(V2,IVo),e(V2,aD),e(aD,jVo),e(V2,NVo),e(B,DVo),e(B,W2),e(W2,xce),e(xce,qVo),e(W2,GVo),e(W2,nD),e(nD,OVo),e(W2,XVo),e(B,zVo),e(B,Q2),e(Q2,Rce),e(Rce,VVo),e(Q2,WVo),e(Q2,sD),e(sD,QVo),e(Q2,HVo),e(B,UVo),e(B,H2),e(H2,Sce),e(Sce,JVo),e(H2,YVo),e(H2,lD),e(lD,KVo),e(H2,ZVo),e(B,eWo),e(B,U2),e(U2,Pce),e(Pce,oWo),e(U2,rWo),e(U2,iD),e(iD,tWo),e(U2,aWo),e(B,nWo),e(B,J2),e(J2,$ce),e($ce,sWo),e(J2,lWo),e(J2,dD),e(dD,iWo),e(J2,dWo),e(B,cWo),e(B,Y2),e(Y2,Ice),e(Ice,fWo),e(Y2,mWo),e(Y2,cD),e(cD,gWo),e(Y2,hWo),e(B,pWo),e(B,K2),e(K2,jce),e(jce,_Wo),e(K2,uWo),e(K2,fD),e(fD,bWo),e(K2,vWo),e(B,TWo),e(B,Z2),e(Z2,Nce),e(Nce,FWo),e(Z2,CWo),e(Z2,mD),e(mD,MWo),e(Z2,EWo),e(B,yWo),e(B,ev),e(ev,Dce),e(Dce,wWo),e(ev,AWo),e(ev,gD),e(gD,LWo),e(ev,BWo),e(B,kWo),e(B,ov),e(ov,qce),e(qce,xWo),e(ov,RWo),e(ov,hD),e(hD,SWo),e(ov,PWo),e(B,$Wo),e(B,rv),e(rv,Gce),e(Gce,IWo),e(rv,jWo),e(rv,pD),e(pD,NWo),e(rv,DWo),e(B,qWo),e(B,tv),e(tv,Oce),e(Oce,GWo),e(tv,OWo),e(tv,_D),e(_D,XWo),e(tv,zWo),e(B,VWo),e(B,av),e(av,Xce),e(Xce,WWo),e(av,QWo),e(av,uD),e(uD,HWo),e(av,UWo),e(B,JWo),e(B,nv),e(nv,zce),e(zce,YWo),e(nv,KWo),e(nv,bD),e(bD,ZWo),e(nv,eQo),e(B,oQo),e(B,sv),e(sv,Vce),e(Vce,rQo),e(sv,tQo),e(sv,vD),e(vD,aQo),e(sv,nQo),e(B,sQo),e(B,lv),e(lv,Wce),e(Wce,lQo),e(lv,iQo),e(lv,TD),e(TD,dQo),e(lv,cQo),e(B,fQo),e(B,iv),e(iv,Qce),e(Qce,mQo),e(iv,gQo),e(iv,FD),e(FD,hQo),e(iv,pQo),e(B,_Qo),e(B,dv),e(dv,Hce),e(Hce,uQo),e(dv,bQo),e(dv,CD),e(CD,vQo),e(dv,TQo),e(B,FQo),e(B,cv),e(cv,Uce),e(Uce,CQo),e(cv,MQo),e(cv,MD),e(MD,EQo),e(cv,yQo),e(B,wQo),e(B,fv),e(fv,Jce),e(Jce,AQo),e(fv,LQo),e(fv,ED),e(ED,BQo),e(fv,kQo),e(B,xQo),e(B,mv),e(mv,Yce),e(Yce,RQo),e(mv,SQo),e(mv,yD),e(yD,PQo),e(mv,$Qo),e(B,IQo),e(B,gv),e(gv,Kce),e(Kce,jQo),e(gv,NQo),e(gv,wD),e(wD,DQo),e(gv,qQo),e(go,GQo),e(go,Zce),e(Zce,OQo),e(go,XQo),g(Dy,go,null),b(d,E9e,u),b(d,ac,u),e(ac,hv),e(hv,efe),g(qy,efe,null),e(ac,zQo),e(ac,ofe),e(ofe,VQo),b(d,y9e,u),b(d,hr,u),g(Gy,hr,null),e(hr,WQo),e(hr,nc),e(nc,QQo),e(nc,rfe),e(rfe,HQo),e(nc,UQo),e(nc,tfe),e(tfe,JQo),e(nc,YQo),e(hr,KQo),e(hr,Oy),e(Oy,ZQo),e(Oy,afe),e(afe,eHo),e(Oy,oHo),e(hr,rHo),e(hr,lt),g(Xy,lt,null),e(lt,tHo),e(lt,nfe),e(nfe,aHo),e(lt,nHo),e(lt,sc),e(sc,sHo),e(sc,sfe),e(sfe,lHo),e(sc,iHo),e(sc,lfe),e(lfe,dHo),e(sc,cHo),e(lt,fHo),e(lt,ife),e(ife,mHo),e(lt,gHo),g(zy,lt,null),e(hr,hHo),e(hr,ho),g(Vy,ho,null),e(ho,pHo),e(ho,dfe),e(dfe,_Ho),e(ho,uHo),e(ho,dn),e(dn,bHo),e(dn,cfe),e(cfe,vHo),e(dn,THo),e(dn,ffe),e(ffe,FHo),e(dn,CHo),e(dn,mfe),e(mfe,MHo),e(dn,EHo),e(ho,yHo),e(ho,H),e(H,pv),e(pv,gfe),e(gfe,wHo),e(pv,AHo),e(pv,AD),e(AD,LHo),e(pv,BHo),e(H,kHo),e(H,_v),e(_v,hfe),e(hfe,xHo),e(_v,RHo),e(_v,LD),e(LD,SHo),e(_v,PHo),e(H,$Ho),e(H,uv),e(uv,pfe),e(pfe,IHo),e(uv,jHo),e(uv,BD),e(BD,NHo),e(uv,DHo),e(H,qHo),e(H,bv),e(bv,_fe),e(_fe,GHo),e(bv,OHo),e(bv,kD),e(kD,XHo),e(bv,zHo),e(H,VHo),e(H,vv),e(vv,ufe),e(ufe,WHo),e(vv,QHo),e(vv,xD),e(xD,HHo),e(vv,UHo),e(H,JHo),e(H,Tv),e(Tv,bfe),e(bfe,YHo),e(Tv,KHo),e(Tv,RD),e(RD,ZHo),e(Tv,eUo),e(H,oUo),e(H,Fv),e(Fv,vfe),e(vfe,rUo),e(Fv,tUo),e(Fv,SD),e(SD,aUo),e(Fv,nUo),e(H,sUo),e(H,Cv),e(Cv,Tfe),e(Tfe,lUo),e(Cv,iUo),e(Cv,PD),e(PD,dUo),e(Cv,cUo),e(H,fUo),e(H,Mv),e(Mv,Ffe),e(Ffe,mUo),e(Mv,gUo),e(Mv,$D),e($D,hUo),e(Mv,pUo),e(H,_Uo),e(H,Ev),e(Ev,Cfe),e(Cfe,uUo),e(Ev,bUo),e(Ev,ID),e(ID,vUo),e(Ev,TUo),e(H,FUo),e(H,yv),e(yv,Mfe),e(Mfe,CUo),e(yv,MUo),e(yv,jD),e(jD,EUo),e(yv,yUo),e(H,wUo),e(H,wv),e(wv,Efe),e(Efe,AUo),e(wv,LUo),e(wv,ND),e(ND,BUo),e(wv,kUo),e(H,xUo),e(H,Av),e(Av,yfe),e(yfe,RUo),e(Av,SUo),e(Av,DD),e(DD,PUo),e(Av,$Uo),e(H,IUo),e(H,Lv),e(Lv,wfe),e(wfe,jUo),e(Lv,NUo),e(Lv,qD),e(qD,DUo),e(Lv,qUo),e(H,GUo),e(H,Bv),e(Bv,Afe),e(Afe,OUo),e(Bv,XUo),e(Bv,GD),e(GD,zUo),e(Bv,VUo),e(H,WUo),e(H,kv),e(kv,Lfe),e(Lfe,QUo),e(kv,HUo),e(kv,OD),e(OD,UUo),e(kv,JUo),e(H,YUo),e(H,xv),e(xv,Bfe),e(Bfe,KUo),e(xv,ZUo),e(xv,XD),e(XD,eJo),e(xv,oJo),e(H,rJo),e(H,Rv),e(Rv,kfe),e(kfe,tJo),e(Rv,aJo),e(Rv,zD),e(zD,nJo),e(Rv,sJo),e(H,lJo),e(H,Sv),e(Sv,xfe),e(xfe,iJo),e(Sv,dJo),e(Sv,VD),e(VD,cJo),e(Sv,fJo),e(H,mJo),e(H,Pv),e(Pv,Rfe),e(Rfe,gJo),e(Pv,hJo),e(Pv,WD),e(WD,pJo),e(Pv,_Jo),e(H,uJo),e(H,$v),e($v,Sfe),e(Sfe,bJo),e($v,vJo),e($v,QD),e(QD,TJo),e($v,FJo),e(H,CJo),e(H,Iv),e(Iv,Pfe),e(Pfe,MJo),e(Iv,EJo),e(Iv,HD),e(HD,yJo),e(Iv,wJo),e(ho,AJo),e(ho,$fe),e($fe,LJo),e(ho,BJo),g(Wy,ho,null),b(d,w9e,u),b(d,lc,u),e(lc,jv),e(jv,Ife),g(Qy,Ife,null),e(lc,kJo),e(lc,jfe),e(jfe,xJo),b(d,A9e,u),b(d,pr,u),g(Hy,pr,null),e(pr,RJo),e(pr,ic),e(ic,SJo),e(ic,Nfe),e(Nfe,PJo),e(ic,$Jo),e(ic,Dfe),e(Dfe,IJo),e(ic,jJo),e(pr,NJo),e(pr,Uy),e(Uy,DJo),e(Uy,qfe),e(qfe,qJo),e(Uy,GJo),e(pr,OJo),e(pr,it),g(Jy,it,null),e(it,XJo),e(it,Gfe),e(Gfe,zJo),e(it,VJo),e(it,dc),e(dc,WJo),e(dc,Ofe),e(Ofe,QJo),e(dc,HJo),e(dc,Xfe),e(Xfe,UJo),e(dc,JJo),e(it,YJo),e(it,zfe),e(zfe,KJo),e(it,ZJo),g(Yy,it,null),e(pr,eYo),e(pr,po),g(Ky,po,null),e(po,oYo),e(po,Vfe),e(Vfe,rYo),e(po,tYo),e(po,cn),e(cn,aYo),e(cn,Wfe),e(Wfe,nYo),e(cn,sYo),e(cn,Qfe),e(Qfe,lYo),e(cn,iYo),e(cn,Hfe),e(Hfe,dYo),e(cn,cYo),e(po,fYo),e(po,pe),e(pe,Nv),e(Nv,Ufe),e(Ufe,mYo),e(Nv,gYo),e(Nv,UD),e(UD,hYo),e(Nv,pYo),e(pe,_Yo),e(pe,Dv),e(Dv,Jfe),e(Jfe,uYo),e(Dv,bYo),e(Dv,JD),e(JD,vYo),e(Dv,TYo),e(pe,FYo),e(pe,qv),e(qv,Yfe),e(Yfe,CYo),e(qv,MYo),e(qv,YD),e(YD,EYo),e(qv,yYo),e(pe,wYo),e(pe,Gv),e(Gv,Kfe),e(Kfe,AYo),e(Gv,LYo),e(Gv,KD),e(KD,BYo),e(Gv,kYo),e(pe,xYo),e(pe,Ov),e(Ov,Zfe),e(Zfe,RYo),e(Ov,SYo),e(Ov,ZD),e(ZD,PYo),e(Ov,$Yo),e(pe,IYo),e(pe,Xv),e(Xv,eme),e(eme,jYo),e(Xv,NYo),e(Xv,eq),e(eq,DYo),e(Xv,qYo),e(pe,GYo),e(pe,zv),e(zv,ome),e(ome,OYo),e(zv,XYo),e(zv,oq),e(oq,zYo),e(zv,VYo),e(pe,WYo),e(pe,Vv),e(Vv,rme),e(rme,QYo),e(Vv,HYo),e(Vv,rq),e(rq,UYo),e(Vv,JYo),e(pe,YYo),e(pe,Wv),e(Wv,tme),e(tme,KYo),e(Wv,ZYo),e(Wv,tq),e(tq,eKo),e(Wv,oKo),e(pe,rKo),e(pe,Qv),e(Qv,ame),e(ame,tKo),e(Qv,aKo),e(Qv,aq),e(aq,nKo),e(Qv,sKo),e(po,lKo),e(po,nme),e(nme,iKo),e(po,dKo),g(Zy,po,null),b(d,L9e,u),b(d,cc,u),e(cc,Hv),e(Hv,sme),g(ew,sme,null),e(cc,cKo),e(cc,lme),e(lme,fKo),b(d,B9e,u),b(d,_r,u),g(ow,_r,null),e(_r,mKo),e(_r,fc),e(fc,gKo),e(fc,ime),e(ime,hKo),e(fc,pKo),e(fc,dme),e(dme,_Ko),e(fc,uKo),e(_r,bKo),e(_r,rw),e(rw,vKo),e(rw,cme),e(cme,TKo),e(rw,FKo),e(_r,CKo),e(_r,dt),g(tw,dt,null),e(dt,MKo),e(dt,fme),e(fme,EKo),e(dt,yKo),e(dt,mc),e(mc,wKo),e(mc,mme),e(mme,AKo),e(mc,LKo),e(mc,gme),e(gme,BKo),e(mc,kKo),e(dt,xKo),e(dt,hme),e(hme,RKo),e(dt,SKo),g(aw,dt,null),e(_r,PKo),e(_r,_o),g(nw,_o,null),e(_o,$Ko),e(_o,pme),e(pme,IKo),e(_o,jKo),e(_o,fn),e(fn,NKo),e(fn,_me),e(_me,DKo),e(fn,qKo),e(fn,ume),e(ume,GKo),e(fn,OKo),e(fn,bme),e(bme,XKo),e(fn,zKo),e(_o,VKo),e(_o,vme),e(vme,Uv),e(Uv,Tme),e(Tme,WKo),e(Uv,QKo),e(Uv,nq),e(nq,HKo),e(Uv,UKo),e(_o,JKo),e(_o,Fme),e(Fme,YKo),e(_o,KKo),g(sw,_o,null),b(d,k9e,u),b(d,gc,u),e(gc,Jv),e(Jv,Cme),g(lw,Cme,null),e(gc,ZKo),e(gc,Mme),e(Mme,eZo),b(d,x9e,u),b(d,ur,u),g(iw,ur,null),e(ur,oZo),e(ur,hc),e(hc,rZo),e(hc,Eme),e(Eme,tZo),e(hc,aZo),e(hc,yme),e(yme,nZo),e(hc,sZo),e(ur,lZo),e(ur,dw),e(dw,iZo),e(dw,wme),e(wme,dZo),e(dw,cZo),e(ur,fZo),e(ur,ct),g(cw,ct,null),e(ct,mZo),e(ct,Ame),e(Ame,gZo),e(ct,hZo),e(ct,pc),e(pc,pZo),e(pc,Lme),e(Lme,_Zo),e(pc,uZo),e(pc,Bme),e(Bme,bZo),e(pc,vZo),e(ct,TZo),e(ct,kme),e(kme,FZo),e(ct,CZo),g(fw,ct,null),e(ur,MZo),e(ur,uo),g(mw,uo,null),e(uo,EZo),e(uo,xme),e(xme,yZo),e(uo,wZo),e(uo,mn),e(mn,AZo),e(mn,Rme),e(Rme,LZo),e(mn,BZo),e(mn,Sme),e(Sme,kZo),e(mn,xZo),e(mn,Pme),e(Pme,RZo),e(mn,SZo),e(uo,PZo),e(uo,Y),e(Y,Yv),e(Yv,$me),e($me,$Zo),e(Yv,IZo),e(Yv,sq),e(sq,jZo),e(Yv,NZo),e(Y,DZo),e(Y,Kv),e(Kv,Ime),e(Ime,qZo),e(Kv,GZo),e(Kv,lq),e(lq,OZo),e(Kv,XZo),e(Y,zZo),e(Y,Zv),e(Zv,jme),e(jme,VZo),e(Zv,WZo),e(Zv,iq),e(iq,QZo),e(Zv,HZo),e(Y,UZo),e(Y,e0),e(e0,Nme),e(Nme,JZo),e(e0,YZo),e(e0,dq),e(dq,KZo),e(e0,ZZo),e(Y,eer),e(Y,o0),e(o0,Dme),e(Dme,oer),e(o0,rer),e(o0,cq),e(cq,ter),e(o0,aer),e(Y,ner),e(Y,r0),e(r0,qme),e(qme,ser),e(r0,ler),e(r0,fq),e(fq,ier),e(r0,der),e(Y,cer),e(Y,t0),e(t0,Gme),e(Gme,fer),e(t0,mer),e(t0,mq),e(mq,ger),e(t0,her),e(Y,per),e(Y,a0),e(a0,Ome),e(Ome,_er),e(a0,uer),e(a0,gq),e(gq,ber),e(a0,ver),e(Y,Ter),e(Y,n0),e(n0,Xme),e(Xme,Fer),e(n0,Cer),e(n0,hq),e(hq,Mer),e(n0,Eer),e(Y,yer),e(Y,s0),e(s0,zme),e(zme,wer),e(s0,Aer),e(s0,pq),e(pq,Ler),e(s0,Ber),e(Y,ker),e(Y,l0),e(l0,Vme),e(Vme,xer),e(l0,Rer),e(l0,_q),e(_q,Ser),e(l0,Per),e(Y,$er),e(Y,i0),e(i0,Wme),e(Wme,Ier),e(i0,jer),e(i0,uq),e(uq,Ner),e(i0,Der),e(Y,qer),e(Y,d0),e(d0,Qme),e(Qme,Ger),e(d0,Oer),e(d0,bq),e(bq,Xer),e(d0,zer),e(Y,Ver),e(Y,c0),e(c0,Hme),e(Hme,Wer),e(c0,Qer),e(c0,vq),e(vq,Her),e(c0,Uer),e(Y,Jer),e(Y,f0),e(f0,Ume),e(Ume,Yer),e(f0,Ker),e(f0,Tq),e(Tq,Zer),e(f0,eor),e(Y,oor),e(Y,m0),e(m0,Jme),e(Jme,ror),e(m0,tor),e(m0,Fq),e(Fq,aor),e(m0,nor),e(Y,sor),e(Y,g0),e(g0,Yme),e(Yme,lor),e(g0,ior),e(g0,Cq),e(Cq,dor),e(g0,cor),e(Y,mor),e(Y,h0),e(h0,Kme),e(Kme,gor),e(h0,hor),e(h0,Mq),e(Mq,por),e(h0,_or),e(Y,uor),e(Y,p0),e(p0,Zme),e(Zme,bor),e(p0,vor),e(p0,Eq),e(Eq,Tor),e(p0,For),e(Y,Cor),e(Y,_0),e(_0,ege),e(ege,Mor),e(_0,Eor),e(_0,yq),e(yq,yor),e(_0,wor),e(uo,Aor),e(uo,oge),e(oge,Lor),e(uo,Bor),g(gw,uo,null),b(d,R9e,u),b(d,_c,u),e(_c,u0),e(u0,rge),g(hw,rge,null),e(_c,kor),e(_c,tge),e(tge,xor),b(d,S9e,u),b(d,br,u),g(pw,br,null),e(br,Ror),e(br,uc),e(uc,Sor),e(uc,age),e(age,Por),e(uc,$or),e(uc,nge),e(nge,Ior),e(uc,jor),e(br,Nor),e(br,_w),e(_w,Dor),e(_w,sge),e(sge,qor),e(_w,Gor),e(br,Oor),e(br,ft),g(uw,ft,null),e(ft,Xor),e(ft,lge),e(lge,zor),e(ft,Vor),e(ft,bc),e(bc,Wor),e(bc,ige),e(ige,Qor),e(bc,Hor),e(bc,dge),e(dge,Uor),e(bc,Jor),e(ft,Yor),e(ft,cge),e(cge,Kor),e(ft,Zor),g(bw,ft,null),e(br,err),e(br,bo),g(vw,bo,null),e(bo,orr),e(bo,fge),e(fge,rrr),e(bo,trr),e(bo,gn),e(gn,arr),e(gn,mge),e(mge,nrr),e(gn,srr),e(gn,gge),e(gge,lrr),e(gn,irr),e(gn,hge),e(hge,drr),e(gn,crr),e(bo,frr),e(bo,_e),e(_e,b0),e(b0,pge),e(pge,mrr),e(b0,grr),e(b0,wq),e(wq,hrr),e(b0,prr),e(_e,_rr),e(_e,v0),e(v0,_ge),e(_ge,urr),e(v0,brr),e(v0,Aq),e(Aq,vrr),e(v0,Trr),e(_e,Frr),e(_e,T0),e(T0,uge),e(uge,Crr),e(T0,Mrr),e(T0,Lq),e(Lq,Err),e(T0,yrr),e(_e,wrr),e(_e,F0),e(F0,bge),e(bge,Arr),e(F0,Lrr),e(F0,Bq),e(Bq,Brr),e(F0,krr),e(_e,xrr),e(_e,C0),e(C0,vge),e(vge,Rrr),e(C0,Srr),e(C0,kq),e(kq,Prr),e(C0,$rr),e(_e,Irr),e(_e,M0),e(M0,Tge),e(Tge,jrr),e(M0,Nrr),e(M0,xq),e(xq,Drr),e(M0,qrr),e(_e,Grr),e(_e,E0),e(E0,Fge),e(Fge,Orr),e(E0,Xrr),e(E0,Rq),e(Rq,zrr),e(E0,Vrr),e(_e,Wrr),e(_e,y0),e(y0,Cge),e(Cge,Qrr),e(y0,Hrr),e(y0,Sq),e(Sq,Urr),e(y0,Jrr),e(_e,Yrr),e(_e,w0),e(w0,Mge),e(Mge,Krr),e(w0,Zrr),e(w0,Pq),e(Pq,etr),e(w0,otr),e(_e,rtr),e(_e,A0),e(A0,Ege),e(Ege,ttr),e(A0,atr),e(A0,$q),e($q,ntr),e(A0,str),e(bo,ltr),e(bo,yge),e(yge,itr),e(bo,dtr),g(Tw,bo,null),b(d,P9e,u),b(d,vc,u),e(vc,L0),e(L0,wge),g(Fw,wge,null),e(vc,ctr),e(vc,Age),e(Age,ftr),b(d,$9e,u),b(d,vr,u),g(Cw,vr,null),e(vr,mtr),e(vr,Tc),e(Tc,gtr),e(Tc,Lge),e(Lge,htr),e(Tc,ptr),e(Tc,Bge),e(Bge,_tr),e(Tc,utr),e(vr,btr),e(vr,Mw),e(Mw,vtr),e(Mw,kge),e(kge,Ttr),e(Mw,Ftr),e(vr,Ctr),e(vr,mt),g(Ew,mt,null),e(mt,Mtr),e(mt,xge),e(xge,Etr),e(mt,ytr),e(mt,Fc),e(Fc,wtr),e(Fc,Rge),e(Rge,Atr),e(Fc,Ltr),e(Fc,Sge),e(Sge,Btr),e(Fc,ktr),e(mt,xtr),e(mt,Pge),e(Pge,Rtr),e(mt,Str),g(yw,mt,null),e(vr,Ptr),e(vr,vo),g(ww,vo,null),e(vo,$tr),e(vo,$ge),e($ge,Itr),e(vo,jtr),e(vo,hn),e(hn,Ntr),e(hn,Ige),e(Ige,Dtr),e(hn,qtr),e(hn,jge),e(jge,Gtr),e(hn,Otr),e(hn,Nge),e(Nge,Xtr),e(hn,ztr),e(vo,Vtr),e(vo,X),e(X,B0),e(B0,Dge),e(Dge,Wtr),e(B0,Qtr),e(B0,Iq),e(Iq,Htr),e(B0,Utr),e(X,Jtr),e(X,k0),e(k0,qge),e(qge,Ytr),e(k0,Ktr),e(k0,jq),e(jq,Ztr),e(k0,ear),e(X,oar),e(X,x0),e(x0,Gge),e(Gge,rar),e(x0,tar),e(x0,Nq),e(Nq,aar),e(x0,nar),e(X,sar),e(X,R0),e(R0,Oge),e(Oge,lar),e(R0,iar),e(R0,Dq),e(Dq,dar),e(R0,car),e(X,far),e(X,S0),e(S0,Xge),e(Xge,mar),e(S0,gar),e(S0,qq),e(qq,har),e(S0,par),e(X,_ar),e(X,P0),e(P0,zge),e(zge,uar),e(P0,bar),e(P0,Gq),e(Gq,Tar),e(P0,Far),e(X,Car),e(X,$0),e($0,Vge),e(Vge,Mar),e($0,Ear),e($0,Oq),e(Oq,yar),e($0,war),e(X,Aar),e(X,I0),e(I0,Wge),e(Wge,Lar),e(I0,Bar),e(I0,Xq),e(Xq,kar),e(I0,xar),e(X,Rar),e(X,j0),e(j0,Qge),e(Qge,Sar),e(j0,Par),e(j0,zq),e(zq,$ar),e(j0,Iar),e(X,jar),e(X,N0),e(N0,Hge),e(Hge,Nar),e(N0,Dar),e(N0,Vq),e(Vq,qar),e(N0,Gar),e(X,Oar),e(X,D0),e(D0,Uge),e(Uge,Xar),e(D0,zar),e(D0,Wq),e(Wq,Var),e(D0,War),e(X,Qar),e(X,q0),e(q0,Jge),e(Jge,Har),e(q0,Uar),e(q0,Qq),e(Qq,Jar),e(q0,Yar),e(X,Kar),e(X,G0),e(G0,Yge),e(Yge,Zar),e(G0,enr),e(G0,Hq),e(Hq,onr),e(G0,rnr),e(X,tnr),e(X,O0),e(O0,Kge),e(Kge,anr),e(O0,nnr),e(O0,Uq),e(Uq,snr),e(O0,lnr),e(X,inr),e(X,X0),e(X0,Zge),e(Zge,dnr),e(X0,cnr),e(X0,Jq),e(Jq,fnr),e(X0,mnr),e(X,gnr),e(X,z0),e(z0,ehe),e(ehe,hnr),e(z0,pnr),e(z0,Yq),e(Yq,_nr),e(z0,unr),e(X,bnr),e(X,V0),e(V0,ohe),e(ohe,vnr),e(V0,Tnr),e(V0,Kq),e(Kq,Fnr),e(V0,Cnr),e(X,Mnr),e(X,W0),e(W0,rhe),e(rhe,Enr),e(W0,ynr),e(W0,Zq),e(Zq,wnr),e(W0,Anr),e(X,Lnr),e(X,Q0),e(Q0,the),e(the,Bnr),e(Q0,knr),e(Q0,eG),e(eG,xnr),e(Q0,Rnr),e(X,Snr),e(X,H0),e(H0,ahe),e(ahe,Pnr),e(H0,$nr),e(H0,oG),e(oG,Inr),e(H0,jnr),e(X,Nnr),e(X,U0),e(U0,nhe),e(nhe,Dnr),e(U0,qnr),e(U0,rG),e(rG,Gnr),e(U0,Onr),e(X,Xnr),e(X,J0),e(J0,she),e(she,znr),e(J0,Vnr),e(J0,tG),e(tG,Wnr),e(J0,Qnr),e(X,Hnr),e(X,Y0),e(Y0,lhe),e(lhe,Unr),e(Y0,Jnr),e(Y0,aG),e(aG,Ynr),e(Y0,Knr),e(X,Znr),e(X,K0),e(K0,ihe),e(ihe,esr),e(K0,osr),e(K0,nG),e(nG,rsr),e(K0,tsr),e(X,asr),e(X,Z0),e(Z0,dhe),e(dhe,nsr),e(Z0,ssr),e(Z0,sG),e(sG,lsr),e(Z0,isr),e(vo,dsr),e(vo,che),e(che,csr),e(vo,fsr),g(Aw,vo,null),b(d,I9e,u),b(d,Cc,u),e(Cc,eT),e(eT,fhe),g(Lw,fhe,null),e(Cc,msr),e(Cc,mhe),e(mhe,gsr),b(d,j9e,u),b(d,Tr,u),g(Bw,Tr,null),e(Tr,hsr),e(Tr,Mc),e(Mc,psr),e(Mc,ghe),e(ghe,_sr),e(Mc,usr),e(Mc,hhe),e(hhe,bsr),e(Mc,vsr),e(Tr,Tsr),e(Tr,kw),e(kw,Fsr),e(kw,phe),e(phe,Csr),e(kw,Msr),e(Tr,Esr),e(Tr,gt),g(xw,gt,null),e(gt,ysr),e(gt,_he),e(_he,wsr),e(gt,Asr),e(gt,Ec),e(Ec,Lsr),e(Ec,uhe),e(uhe,Bsr),e(Ec,ksr),e(Ec,bhe),e(bhe,xsr),e(Ec,Rsr),e(gt,Ssr),e(gt,vhe),e(vhe,Psr),e(gt,$sr),g(Rw,gt,null),e(Tr,Isr),e(Tr,To),g(Sw,To,null),e(To,jsr),e(To,The),e(The,Nsr),e(To,Dsr),e(To,pn),e(pn,qsr),e(pn,Fhe),e(Fhe,Gsr),e(pn,Osr),e(pn,Che),e(Che,Xsr),e(pn,zsr),e(pn,Mhe),e(Mhe,Vsr),e(pn,Wsr),e(To,Qsr),e(To,te),e(te,oT),e(oT,Ehe),e(Ehe,Hsr),e(oT,Usr),e(oT,lG),e(lG,Jsr),e(oT,Ysr),e(te,Ksr),e(te,rT),e(rT,yhe),e(yhe,Zsr),e(rT,elr),e(rT,iG),e(iG,olr),e(rT,rlr),e(te,tlr),e(te,tT),e(tT,whe),e(whe,alr),e(tT,nlr),e(tT,dG),e(dG,slr),e(tT,llr),e(te,ilr),e(te,aT),e(aT,Ahe),e(Ahe,dlr),e(aT,clr),e(aT,cG),e(cG,flr),e(aT,mlr),e(te,glr),e(te,nT),e(nT,Lhe),e(Lhe,hlr),e(nT,plr),e(nT,fG),e(fG,_lr),e(nT,ulr),e(te,blr),e(te,sT),e(sT,Bhe),e(Bhe,vlr),e(sT,Tlr),e(sT,mG),e(mG,Flr),e(sT,Clr),e(te,Mlr),e(te,lT),e(lT,khe),e(khe,Elr),e(lT,ylr),e(lT,gG),e(gG,wlr),e(lT,Alr),e(te,Llr),e(te,iT),e(iT,xhe),e(xhe,Blr),e(iT,klr),e(iT,hG),e(hG,xlr),e(iT,Rlr),e(te,Slr),e(te,dT),e(dT,Rhe),e(Rhe,Plr),e(dT,$lr),e(dT,pG),e(pG,Ilr),e(dT,jlr),e(te,Nlr),e(te,cT),e(cT,She),e(She,Dlr),e(cT,qlr),e(cT,_G),e(_G,Glr),e(cT,Olr),e(te,Xlr),e(te,fT),e(fT,Phe),e(Phe,zlr),e(fT,Vlr),e(fT,uG),e(uG,Wlr),e(fT,Qlr),e(te,Hlr),e(te,mT),e(mT,$he),e($he,Ulr),e(mT,Jlr),e(mT,bG),e(bG,Ylr),e(mT,Klr),e(te,Zlr),e(te,gT),e(gT,Ihe),e(Ihe,eir),e(gT,oir),e(gT,vG),e(vG,rir),e(gT,tir),e(te,air),e(te,hT),e(hT,jhe),e(jhe,nir),e(hT,sir),e(hT,TG),e(TG,lir),e(hT,iir),e(te,dir),e(te,pT),e(pT,Nhe),e(Nhe,cir),e(pT,fir),e(pT,FG),e(FG,mir),e(pT,gir),e(te,hir),e(te,_T),e(_T,Dhe),e(Dhe,pir),e(_T,_ir),e(_T,CG),e(CG,uir),e(_T,bir),e(te,vir),e(te,uT),e(uT,qhe),e(qhe,Tir),e(uT,Fir),e(uT,MG),e(MG,Cir),e(uT,Mir),e(To,Eir),e(To,Ghe),e(Ghe,yir),e(To,wir),g(Pw,To,null),b(d,N9e,u),b(d,yc,u),e(yc,bT),e(bT,Ohe),g($w,Ohe,null),e(yc,Air),e(yc,Xhe),e(Xhe,Lir),b(d,D9e,u),b(d,Fr,u),g(Iw,Fr,null),e(Fr,Bir),e(Fr,wc),e(wc,kir),e(wc,zhe),e(zhe,xir),e(wc,Rir),e(wc,Vhe),e(Vhe,Sir),e(wc,Pir),e(Fr,$ir),e(Fr,jw),e(jw,Iir),e(jw,Whe),e(Whe,jir),e(jw,Nir),e(Fr,Dir),e(Fr,ht),g(Nw,ht,null),e(ht,qir),e(ht,Qhe),e(Qhe,Gir),e(ht,Oir),e(ht,Ac),e(Ac,Xir),e(Ac,Hhe),e(Hhe,zir),e(Ac,Vir),e(Ac,Uhe),e(Uhe,Wir),e(Ac,Qir),e(ht,Hir),e(ht,Jhe),e(Jhe,Uir),e(ht,Jir),g(Dw,ht,null),e(Fr,Yir),e(Fr,Fo),g(qw,Fo,null),e(Fo,Kir),e(Fo,Yhe),e(Yhe,Zir),e(Fo,edr),e(Fo,_n),e(_n,odr),e(_n,Khe),e(Khe,rdr),e(_n,tdr),e(_n,Zhe),e(Zhe,adr),e(_n,ndr),e(_n,epe),e(epe,sdr),e(_n,ldr),e(Fo,idr),e(Fo,ope),e(ope,vT),e(vT,rpe),e(rpe,ddr),e(vT,cdr),e(vT,EG),e(EG,fdr),e(vT,mdr),e(Fo,gdr),e(Fo,tpe),e(tpe,hdr),e(Fo,pdr),g(Gw,Fo,null),b(d,q9e,u),b(d,Lc,u),e(Lc,TT),e(TT,ape),g(Ow,ape,null),e(Lc,_dr),e(Lc,npe),e(npe,udr),b(d,G9e,u),b(d,Cr,u),g(Xw,Cr,null),e(Cr,bdr),e(Cr,Bc),e(Bc,vdr),e(Bc,spe),e(spe,Tdr),e(Bc,Fdr),e(Bc,lpe),e(lpe,Cdr),e(Bc,Mdr),e(Cr,Edr),e(Cr,zw),e(zw,ydr),e(zw,ipe),e(ipe,wdr),e(zw,Adr),e(Cr,Ldr),e(Cr,pt),g(Vw,pt,null),e(pt,Bdr),e(pt,dpe),e(dpe,kdr),e(pt,xdr),e(pt,kc),e(kc,Rdr),e(kc,cpe),e(cpe,Sdr),e(kc,Pdr),e(kc,fpe),e(fpe,$dr),e(kc,Idr),e(pt,jdr),e(pt,mpe),e(mpe,Ndr),e(pt,Ddr),g(Ww,pt,null),e(Cr,qdr),e(Cr,Co),g(Qw,Co,null),e(Co,Gdr),e(Co,gpe),e(gpe,Odr),e(Co,Xdr),e(Co,un),e(un,zdr),e(un,hpe),e(hpe,Vdr),e(un,Wdr),e(un,ppe),e(ppe,Qdr),e(un,Hdr),e(un,_pe),e(_pe,Udr),e(un,Jdr),e(Co,Ydr),e(Co,K),e(K,FT),e(FT,upe),e(upe,Kdr),e(FT,Zdr),e(FT,yG),e(yG,ecr),e(FT,ocr),e(K,rcr),e(K,CT),e(CT,bpe),e(bpe,tcr),e(CT,acr),e(CT,wG),e(wG,ncr),e(CT,scr),e(K,lcr),e(K,MT),e(MT,vpe),e(vpe,icr),e(MT,dcr),e(MT,AG),e(AG,ccr),e(MT,fcr),e(K,mcr),e(K,ET),e(ET,Tpe),e(Tpe,gcr),e(ET,hcr),e(ET,LG),e(LG,pcr),e(ET,_cr),e(K,ucr),e(K,yT),e(yT,Fpe),e(Fpe,bcr),e(yT,vcr),e(yT,BG),e(BG,Tcr),e(yT,Fcr),e(K,Ccr),e(K,wT),e(wT,Cpe),e(Cpe,Mcr),e(wT,Ecr),e(wT,kG),e(kG,ycr),e(wT,wcr),e(K,Acr),e(K,AT),e(AT,Mpe),e(Mpe,Lcr),e(AT,Bcr),e(AT,xG),e(xG,kcr),e(AT,xcr),e(K,Rcr),e(K,LT),e(LT,Epe),e(Epe,Scr),e(LT,Pcr),e(LT,RG),e(RG,$cr),e(LT,Icr),e(K,jcr),e(K,BT),e(BT,ype),e(ype,Ncr),e(BT,Dcr),e(BT,SG),e(SG,qcr),e(BT,Gcr),e(K,Ocr),e(K,kT),e(kT,wpe),e(wpe,Xcr),e(kT,zcr),e(kT,PG),e(PG,Vcr),e(kT,Wcr),e(K,Qcr),e(K,xT),e(xT,Ape),e(Ape,Hcr),e(xT,Ucr),e(xT,$G),e($G,Jcr),e(xT,Ycr),e(K,Kcr),e(K,RT),e(RT,Lpe),e(Lpe,Zcr),e(RT,efr),e(RT,IG),e(IG,ofr),e(RT,rfr),e(K,tfr),e(K,ST),e(ST,Bpe),e(Bpe,afr),e(ST,nfr),e(ST,jG),e(jG,sfr),e(ST,lfr),e(K,ifr),e(K,PT),e(PT,kpe),e(kpe,dfr),e(PT,cfr),e(PT,NG),e(NG,ffr),e(PT,mfr),e(K,gfr),e(K,$T),e($T,xpe),e(xpe,hfr),e($T,pfr),e($T,DG),e(DG,_fr),e($T,ufr),e(K,bfr),e(K,IT),e(IT,Rpe),e(Rpe,vfr),e(IT,Tfr),e(IT,qG),e(qG,Ffr),e(IT,Cfr),e(K,Mfr),e(K,jT),e(jT,Spe),e(Spe,Efr),e(jT,yfr),e(jT,GG),e(GG,wfr),e(jT,Afr),e(K,Lfr),e(K,NT),e(NT,Ppe),e(Ppe,Bfr),e(NT,kfr),e(NT,OG),e(OG,xfr),e(NT,Rfr),e(K,Sfr),e(K,DT),e(DT,$pe),e($pe,Pfr),e(DT,$fr),e(DT,XG),e(XG,Ifr),e(DT,jfr),e(K,Nfr),e(K,qT),e(qT,Ipe),e(Ipe,Dfr),e(qT,qfr),e(qT,zG),e(zG,Gfr),e(qT,Ofr),e(Co,Xfr),e(Co,jpe),e(jpe,zfr),e(Co,Vfr),g(Hw,Co,null),b(d,O9e,u),b(d,xc,u),e(xc,GT),e(GT,Npe),g(Uw,Npe,null),e(xc,Wfr),e(xc,Dpe),e(Dpe,Qfr),b(d,X9e,u),b(d,Mr,u),g(Jw,Mr,null),e(Mr,Hfr),e(Mr,Rc),e(Rc,Ufr),e(Rc,qpe),e(qpe,Jfr),e(Rc,Yfr),e(Rc,Gpe),e(Gpe,Kfr),e(Rc,Zfr),e(Mr,emr),e(Mr,Yw),e(Yw,omr),e(Yw,Ope),e(Ope,rmr),e(Yw,tmr),e(Mr,amr),e(Mr,_t),g(Kw,_t,null),e(_t,nmr),e(_t,Xpe),e(Xpe,smr),e(_t,lmr),e(_t,Sc),e(Sc,imr),e(Sc,zpe),e(zpe,dmr),e(Sc,cmr),e(Sc,Vpe),e(Vpe,fmr),e(Sc,mmr),e(_t,gmr),e(_t,Wpe),e(Wpe,hmr),e(_t,pmr),g(Zw,_t,null),e(Mr,_mr),e(Mr,Mo),g(e6,Mo,null),e(Mo,umr),e(Mo,Qpe),e(Qpe,bmr),e(Mo,vmr),e(Mo,bn),e(bn,Tmr),e(bn,Hpe),e(Hpe,Fmr),e(bn,Cmr),e(bn,Upe),e(Upe,Mmr),e(bn,Emr),e(bn,Jpe),e(Jpe,ymr),e(bn,wmr),e(Mo,Amr),e(Mo,Z),e(Z,OT),e(OT,Ype),e(Ype,Lmr),e(OT,Bmr),e(OT,VG),e(VG,kmr),e(OT,xmr),e(Z,Rmr),e(Z,XT),e(XT,Kpe),e(Kpe,Smr),e(XT,Pmr),e(XT,WG),e(WG,$mr),e(XT,Imr),e(Z,jmr),e(Z,zT),e(zT,Zpe),e(Zpe,Nmr),e(zT,Dmr),e(zT,QG),e(QG,qmr),e(zT,Gmr),e(Z,Omr),e(Z,VT),e(VT,e_e),e(e_e,Xmr),e(VT,zmr),e(VT,HG),e(HG,Vmr),e(VT,Wmr),e(Z,Qmr),e(Z,WT),e(WT,o_e),e(o_e,Hmr),e(WT,Umr),e(WT,UG),e(UG,Jmr),e(WT,Ymr),e(Z,Kmr),e(Z,QT),e(QT,r_e),e(r_e,Zmr),e(QT,egr),e(QT,JG),e(JG,ogr),e(QT,rgr),e(Z,tgr),e(Z,HT),e(HT,t_e),e(t_e,agr),e(HT,ngr),e(HT,YG),e(YG,sgr),e(HT,lgr),e(Z,igr),e(Z,UT),e(UT,a_e),e(a_e,dgr),e(UT,cgr),e(UT,KG),e(KG,fgr),e(UT,mgr),e(Z,ggr),e(Z,JT),e(JT,n_e),e(n_e,hgr),e(JT,pgr),e(JT,ZG),e(ZG,_gr),e(JT,ugr),e(Z,bgr),e(Z,YT),e(YT,s_e),e(s_e,vgr),e(YT,Tgr),e(YT,eO),e(eO,Fgr),e(YT,Cgr),e(Z,Mgr),e(Z,KT),e(KT,l_e),e(l_e,Egr),e(KT,ygr),e(KT,oO),e(oO,wgr),e(KT,Agr),e(Z,Lgr),e(Z,ZT),e(ZT,i_e),e(i_e,Bgr),e(ZT,kgr),e(ZT,rO),e(rO,xgr),e(ZT,Rgr),e(Z,Sgr),e(Z,eF),e(eF,d_e),e(d_e,Pgr),e(eF,$gr),e(eF,tO),e(tO,Igr),e(eF,jgr),e(Z,Ngr),e(Z,oF),e(oF,c_e),e(c_e,Dgr),e(oF,qgr),e(oF,aO),e(aO,Ggr),e(oF,Ogr),e(Z,Xgr),e(Z,rF),e(rF,f_e),e(f_e,zgr),e(rF,Vgr),e(rF,nO),e(nO,Wgr),e(rF,Qgr),e(Z,Hgr),e(Z,tF),e(tF,m_e),e(m_e,Ugr),e(tF,Jgr),e(tF,sO),e(sO,Ygr),e(tF,Kgr),e(Z,Zgr),e(Z,aF),e(aF,g_e),e(g_e,ehr),e(aF,ohr),e(aF,lO),e(lO,rhr),e(aF,thr),e(Z,ahr),e(Z,nF),e(nF,h_e),e(h_e,nhr),e(nF,shr),e(nF,iO),e(iO,lhr),e(nF,ihr),e(Z,dhr),e(Z,sF),e(sF,p_e),e(p_e,chr),e(sF,fhr),e(sF,dO),e(dO,mhr),e(sF,ghr),e(Mo,hhr),e(Mo,__e),e(__e,phr),e(Mo,_hr),g(o6,Mo,null),b(d,z9e,u),b(d,Pc,u),e(Pc,lF),e(lF,u_e),g(r6,u_e,null),e(Pc,uhr),e(Pc,b_e),e(b_e,bhr),b(d,V9e,u),b(d,Er,u),g(t6,Er,null),e(Er,vhr),e(Er,$c),e($c,Thr),e($c,v_e),e(v_e,Fhr),e($c,Chr),e($c,T_e),e(T_e,Mhr),e($c,Ehr),e(Er,yhr),e(Er,a6),e(a6,whr),e(a6,F_e),e(F_e,Ahr),e(a6,Lhr),e(Er,Bhr),e(Er,ut),g(n6,ut,null),e(ut,khr),e(ut,C_e),e(C_e,xhr),e(ut,Rhr),e(ut,Ic),e(Ic,Shr),e(Ic,M_e),e(M_e,Phr),e(Ic,$hr),e(Ic,E_e),e(E_e,Ihr),e(Ic,jhr),e(ut,Nhr),e(ut,y_e),e(y_e,Dhr),e(ut,qhr),g(s6,ut,null),e(Er,Ghr),e(Er,Eo),g(l6,Eo,null),e(Eo,Ohr),e(Eo,w_e),e(w_e,Xhr),e(Eo,zhr),e(Eo,vn),e(vn,Vhr),e(vn,A_e),e(A_e,Whr),e(vn,Qhr),e(vn,L_e),e(L_e,Hhr),e(vn,Uhr),e(vn,B_e),e(B_e,Jhr),e(vn,Yhr),e(Eo,Khr),e(Eo,k_e),e(k_e,iF),e(iF,x_e),e(x_e,Zhr),e(iF,epr),e(iF,cO),e(cO,opr),e(iF,rpr),e(Eo,tpr),e(Eo,R_e),e(R_e,apr),e(Eo,npr),g(i6,Eo,null),b(d,W9e,u),b(d,jc,u),e(jc,dF),e(dF,S_e),g(d6,S_e,null),e(jc,spr),e(jc,P_e),e(P_e,lpr),b(d,Q9e,u),b(d,yr,u),g(c6,yr,null),e(yr,ipr),e(yr,Nc),e(Nc,dpr),e(Nc,$_e),e($_e,cpr),e(Nc,fpr),e(Nc,I_e),e(I_e,mpr),e(Nc,gpr),e(yr,hpr),e(yr,f6),e(f6,ppr),e(f6,j_e),e(j_e,_pr),e(f6,upr),e(yr,bpr),e(yr,bt),g(m6,bt,null),e(bt,vpr),e(bt,N_e),e(N_e,Tpr),e(bt,Fpr),e(bt,Dc),e(Dc,Cpr),e(Dc,D_e),e(D_e,Mpr),e(Dc,Epr),e(Dc,q_e),e(q_e,ypr),e(Dc,wpr),e(bt,Apr),e(bt,G_e),e(G_e,Lpr),e(bt,Bpr),g(g6,bt,null),e(yr,kpr),e(yr,yo),g(h6,yo,null),e(yo,xpr),e(yo,O_e),e(O_e,Rpr),e(yo,Spr),e(yo,Tn),e(Tn,Ppr),e(Tn,X_e),e(X_e,$pr),e(Tn,Ipr),e(Tn,z_e),e(z_e,jpr),e(Tn,Npr),e(Tn,V_e),e(V_e,Dpr),e(Tn,qpr),e(yo,Gpr),e(yo,W_e),e(W_e,cF),e(cF,Q_e),e(Q_e,Opr),e(cF,Xpr),e(cF,fO),e(fO,zpr),e(cF,Vpr),e(yo,Wpr),e(yo,H_e),e(H_e,Qpr),e(yo,Hpr),g(p6,yo,null),b(d,H9e,u),b(d,qc,u),e(qc,fF),e(fF,U_e),g(_6,U_e,null),e(qc,Upr),e(qc,J_e),e(J_e,Jpr),b(d,U9e,u),b(d,wr,u),g(u6,wr,null),e(wr,Ypr),e(wr,Gc),e(Gc,Kpr),e(Gc,Y_e),e(Y_e,Zpr),e(Gc,e_r),e(Gc,K_e),e(K_e,o_r),e(Gc,r_r),e(wr,t_r),e(wr,b6),e(b6,a_r),e(b6,Z_e),e(Z_e,n_r),e(b6,s_r),e(wr,l_r),e(wr,vt),g(v6,vt,null),e(vt,i_r),e(vt,eue),e(eue,d_r),e(vt,c_r),e(vt,Oc),e(Oc,f_r),e(Oc,oue),e(oue,m_r),e(Oc,g_r),e(Oc,rue),e(rue,h_r),e(Oc,p_r),e(vt,__r),e(vt,tue),e(tue,u_r),e(vt,b_r),g(T6,vt,null),e(wr,v_r),e(wr,wo),g(F6,wo,null),e(wo,T_r),e(wo,aue),e(aue,F_r),e(wo,C_r),e(wo,Fn),e(Fn,M_r),e(Fn,nue),e(nue,E_r),e(Fn,y_r),e(Fn,sue),e(sue,w_r),e(Fn,A_r),e(Fn,lue),e(lue,L_r),e(Fn,B_r),e(wo,k_r),e(wo,V),e(V,mF),e(mF,iue),e(iue,x_r),e(mF,R_r),e(mF,mO),e(mO,S_r),e(mF,P_r),e(V,$_r),e(V,gF),e(gF,due),e(due,I_r),e(gF,j_r),e(gF,gO),e(gO,N_r),e(gF,D_r),e(V,q_r),e(V,hF),e(hF,cue),e(cue,G_r),e(hF,O_r),e(hF,hO),e(hO,X_r),e(hF,z_r),e(V,V_r),e(V,pF),e(pF,fue),e(fue,W_r),e(pF,Q_r),e(pF,pO),e(pO,H_r),e(pF,U_r),e(V,J_r),e(V,_F),e(_F,mue),e(mue,Y_r),e(_F,K_r),e(_F,_O),e(_O,Z_r),e(_F,eur),e(V,our),e(V,uF),e(uF,gue),e(gue,rur),e(uF,tur),e(uF,uO),e(uO,aur),e(uF,nur),e(V,sur),e(V,bF),e(bF,hue),e(hue,lur),e(bF,iur),e(bF,bO),e(bO,dur),e(bF,cur),e(V,fur),e(V,vF),e(vF,pue),e(pue,mur),e(vF,gur),e(vF,vO),e(vO,hur),e(vF,pur),e(V,_ur),e(V,TF),e(TF,_ue),e(_ue,uur),e(TF,bur),e(TF,TO),e(TO,vur),e(TF,Tur),e(V,Fur),e(V,FF),e(FF,uue),e(uue,Cur),e(FF,Mur),e(FF,FO),e(FO,Eur),e(FF,yur),e(V,wur),e(V,CF),e(CF,bue),e(bue,Aur),e(CF,Lur),e(CF,CO),e(CO,Bur),e(CF,kur),e(V,xur),e(V,MF),e(MF,vue),e(vue,Rur),e(MF,Sur),e(MF,MO),e(MO,Pur),e(MF,$ur),e(V,Iur),e(V,EF),e(EF,Tue),e(Tue,jur),e(EF,Nur),e(EF,EO),e(EO,Dur),e(EF,qur),e(V,Gur),e(V,yF),e(yF,Fue),e(Fue,Our),e(yF,Xur),e(yF,yO),e(yO,zur),e(yF,Vur),e(V,Wur),e(V,wF),e(wF,Cue),e(Cue,Qur),e(wF,Hur),e(wF,wO),e(wO,Uur),e(wF,Jur),e(V,Yur),e(V,AF),e(AF,Mue),e(Mue,Kur),e(AF,Zur),e(AF,AO),e(AO,e1r),e(AF,o1r),e(V,r1r),e(V,LF),e(LF,Eue),e(Eue,t1r),e(LF,a1r),e(LF,LO),e(LO,n1r),e(LF,s1r),e(V,l1r),e(V,BF),e(BF,yue),e(yue,i1r),e(BF,d1r),e(BF,BO),e(BO,c1r),e(BF,f1r),e(V,m1r),e(V,kF),e(kF,wue),e(wue,g1r),e(kF,h1r),e(kF,kO),e(kO,p1r),e(kF,_1r),e(V,u1r),e(V,xF),e(xF,Aue),e(Aue,b1r),e(xF,v1r),e(xF,xO),e(xO,T1r),e(xF,F1r),e(V,C1r),e(V,RF),e(RF,Lue),e(Lue,M1r),e(RF,E1r),e(RF,RO),e(RO,y1r),e(RF,w1r),e(V,A1r),e(V,SF),e(SF,Bue),e(Bue,L1r),e(SF,B1r),e(SF,SO),e(SO,k1r),e(SF,x1r),e(V,R1r),e(V,PF),e(PF,kue),e(kue,S1r),e(PF,P1r),e(PF,PO),e(PO,$1r),e(PF,I1r),e(V,j1r),e(V,$F),e($F,xue),e(xue,N1r),e($F,D1r),e($F,$O),e($O,q1r),e($F,G1r),e(wo,O1r),e(wo,Rue),e(Rue,X1r),e(wo,z1r),g(C6,wo,null),b(d,J9e,u),b(d,Xc,u),e(Xc,IF),e(IF,Sue),g(M6,Sue,null),e(Xc,V1r),e(Xc,Pue),e(Pue,W1r),b(d,Y9e,u),b(d,Ar,u),g(E6,Ar,null),e(Ar,Q1r),e(Ar,zc),e(zc,H1r),e(zc,$ue),e($ue,U1r),e(zc,J1r),e(zc,Iue),e(Iue,Y1r),e(zc,K1r),e(Ar,Z1r),e(Ar,y6),e(y6,e7r),e(y6,jue),e(jue,o7r),e(y6,r7r),e(Ar,t7r),e(Ar,Tt),g(w6,Tt,null),e(Tt,a7r),e(Tt,Nue),e(Nue,n7r),e(Tt,s7r),e(Tt,Vc),e(Vc,l7r),e(Vc,Due),e(Due,i7r),e(Vc,d7r),e(Vc,que),e(que,c7r),e(Vc,f7r),e(Tt,m7r),e(Tt,Gue),e(Gue,g7r),e(Tt,h7r),g(A6,Tt,null),e(Ar,p7r),e(Ar,Ao),g(L6,Ao,null),e(Ao,_7r),e(Ao,Oue),e(Oue,u7r),e(Ao,b7r),e(Ao,Cn),e(Cn,v7r),e(Cn,Xue),e(Xue,T7r),e(Cn,F7r),e(Cn,zue),e(zue,C7r),e(Cn,M7r),e(Cn,Vue),e(Vue,E7r),e(Cn,y7r),e(Ao,w7r),e(Ao,Mn),e(Mn,jF),e(jF,Wue),e(Wue,A7r),e(jF,L7r),e(jF,IO),e(IO,B7r),e(jF,k7r),e(Mn,x7r),e(Mn,NF),e(NF,Que),e(Que,R7r),e(NF,S7r),e(NF,jO),e(jO,P7r),e(NF,$7r),e(Mn,I7r),e(Mn,DF),e(DF,Hue),e(Hue,j7r),e(DF,N7r),e(DF,NO),e(NO,D7r),e(DF,q7r),e(Mn,G7r),e(Mn,qF),e(qF,Uue),e(Uue,O7r),e(qF,X7r),e(qF,DO),e(DO,z7r),e(qF,V7r),e(Ao,W7r),e(Ao,Jue),e(Jue,Q7r),e(Ao,H7r),g(B6,Ao,null),b(d,K9e,u),b(d,Wc,u),e(Wc,GF),e(GF,Yue),g(k6,Yue,null),e(Wc,U7r),e(Wc,Kue),e(Kue,J7r),b(d,Z9e,u),b(d,Lr,u),g(x6,Lr,null),e(Lr,Y7r),e(Lr,Qc),e(Qc,K7r),e(Qc,Zue),e(Zue,Z7r),e(Qc,ebr),e(Qc,e1e),e(e1e,obr),e(Qc,rbr),e(Lr,tbr),e(Lr,R6),e(R6,abr),e(R6,o1e),e(o1e,nbr),e(R6,sbr),e(Lr,lbr),e(Lr,Ft),g(S6,Ft,null),e(Ft,ibr),e(Ft,r1e),e(r1e,dbr),e(Ft,cbr),e(Ft,Hc),e(Hc,fbr),e(Hc,t1e),e(t1e,mbr),e(Hc,gbr),e(Hc,a1e),e(a1e,hbr),e(Hc,pbr),e(Ft,_br),e(Ft,n1e),e(n1e,ubr),e(Ft,bbr),g(P6,Ft,null),e(Lr,vbr),e(Lr,Lo),g($6,Lo,null),e(Lo,Tbr),e(Lo,s1e),e(s1e,Fbr),e(Lo,Cbr),e(Lo,En),e(En,Mbr),e(En,l1e),e(l1e,Ebr),e(En,ybr),e(En,i1e),e(i1e,wbr),e(En,Abr),e(En,d1e),e(d1e,Lbr),e(En,Bbr),e(Lo,kbr),e(Lo,fe),e(fe,OF),e(OF,c1e),e(c1e,xbr),e(OF,Rbr),e(OF,qO),e(qO,Sbr),e(OF,Pbr),e(fe,$br),e(fe,XF),e(XF,f1e),e(f1e,Ibr),e(XF,jbr),e(XF,GO),e(GO,Nbr),e(XF,Dbr),e(fe,qbr),e(fe,zF),e(zF,m1e),e(m1e,Gbr),e(zF,Obr),e(zF,OO),e(OO,Xbr),e(zF,zbr),e(fe,Vbr),e(fe,VF),e(VF,g1e),e(g1e,Wbr),e(VF,Qbr),e(VF,XO),e(XO,Hbr),e(VF,Ubr),e(fe,Jbr),e(fe,WF),e(WF,h1e),e(h1e,Ybr),e(WF,Kbr),e(WF,zO),e(zO,Zbr),e(WF,e5r),e(fe,o5r),e(fe,QF),e(QF,p1e),e(p1e,r5r),e(QF,t5r),e(QF,VO),e(VO,a5r),e(QF,n5r),e(fe,s5r),e(fe,HF),e(HF,_1e),e(_1e,l5r),e(HF,i5r),e(HF,WO),e(WO,d5r),e(HF,c5r),e(fe,f5r),e(fe,UF),e(UF,u1e),e(u1e,m5r),e(UF,g5r),e(UF,QO),e(QO,h5r),e(UF,p5r),e(fe,_5r),e(fe,JF),e(JF,b1e),e(b1e,u5r),e(JF,b5r),e(JF,HO),e(HO,v5r),e(JF,T5r),e(fe,F5r),e(fe,YF),e(YF,v1e),e(v1e,C5r),e(YF,M5r),e(YF,UO),e(UO,E5r),e(YF,y5r),e(fe,w5r),e(fe,KF),e(KF,T1e),e(T1e,A5r),e(KF,L5r),e(KF,JO),e(JO,B5r),e(KF,k5r),e(Lo,x5r),e(Lo,F1e),e(F1e,R5r),e(Lo,S5r),g(I6,Lo,null),b(d,eBe,u),b(d,Uc,u),e(Uc,ZF),e(ZF,C1e),g(j6,C1e,null),e(Uc,P5r),e(Uc,M1e),e(M1e,$5r),b(d,oBe,u),b(d,Br,u),g(N6,Br,null),e(Br,I5r),e(Br,Jc),e(Jc,j5r),e(Jc,E1e),e(E1e,N5r),e(Jc,D5r),e(Jc,y1e),e(y1e,q5r),e(Jc,G5r),e(Br,O5r),e(Br,D6),e(D6,X5r),e(D6,w1e),e(w1e,z5r),e(D6,V5r),e(Br,W5r),e(Br,Ct),g(q6,Ct,null),e(Ct,Q5r),e(Ct,A1e),e(A1e,H5r),e(Ct,U5r),e(Ct,Yc),e(Yc,J5r),e(Yc,L1e),e(L1e,Y5r),e(Yc,K5r),e(Yc,B1e),e(B1e,Z5r),e(Yc,e2r),e(Ct,o2r),e(Ct,k1e),e(k1e,r2r),e(Ct,t2r),g(G6,Ct,null),e(Br,a2r),e(Br,Bo),g(O6,Bo,null),e(Bo,n2r),e(Bo,x1e),e(x1e,s2r),e(Bo,l2r),e(Bo,yn),e(yn,i2r),e(yn,R1e),e(R1e,d2r),e(yn,c2r),e(yn,S1e),e(S1e,f2r),e(yn,m2r),e(yn,P1e),e(P1e,g2r),e(yn,h2r),e(Bo,p2r),e(Bo,ve),e(ve,eC),e(eC,$1e),e($1e,_2r),e(eC,u2r),e(eC,YO),e(YO,b2r),e(eC,v2r),e(ve,T2r),e(ve,oC),e(oC,I1e),e(I1e,F2r),e(oC,C2r),e(oC,KO),e(KO,M2r),e(oC,E2r),e(ve,y2r),e(ve,rC),e(rC,j1e),e(j1e,w2r),e(rC,A2r),e(rC,ZO),e(ZO,L2r),e(rC,B2r),e(ve,k2r),e(ve,tC),e(tC,N1e),e(N1e,x2r),e(tC,R2r),e(tC,eX),e(eX,S2r),e(tC,P2r),e(ve,$2r),e(ve,aC),e(aC,D1e),e(D1e,I2r),e(aC,j2r),e(aC,oX),e(oX,N2r),e(aC,D2r),e(ve,q2r),e(ve,nC),e(nC,q1e),e(q1e,G2r),e(nC,O2r),e(nC,rX),e(rX,X2r),e(nC,z2r),e(ve,V2r),e(ve,sC),e(sC,G1e),e(G1e,W2r),e(sC,Q2r),e(sC,tX),e(tX,H2r),e(sC,U2r),e(ve,J2r),e(ve,lC),e(lC,O1e),e(O1e,Y2r),e(lC,K2r),e(lC,aX),e(aX,Z2r),e(lC,evr),e(ve,ovr),e(ve,iC),e(iC,X1e),e(X1e,rvr),e(iC,tvr),e(iC,nX),e(nX,avr),e(iC,nvr),e(Bo,svr),e(Bo,z1e),e(z1e,lvr),e(Bo,ivr),g(X6,Bo,null),b(d,rBe,u),b(d,Kc,u),e(Kc,dC),e(dC,V1e),g(z6,V1e,null),e(Kc,dvr),e(Kc,W1e),e(W1e,cvr),b(d,tBe,u),b(d,kr,u),g(V6,kr,null),e(kr,fvr),e(kr,Zc),e(Zc,mvr),e(Zc,Q1e),e(Q1e,gvr),e(Zc,hvr),e(Zc,H1e),e(H1e,pvr),e(Zc,_vr),e(kr,uvr),e(kr,W6),e(W6,bvr),e(W6,U1e),e(U1e,vvr),e(W6,Tvr),e(kr,Fvr),e(kr,Mt),g(Q6,Mt,null),e(Mt,Cvr),e(Mt,J1e),e(J1e,Mvr),e(Mt,Evr),e(Mt,ef),e(ef,yvr),e(ef,Y1e),e(Y1e,wvr),e(ef,Avr),e(ef,K1e),e(K1e,Lvr),e(ef,Bvr),e(Mt,kvr),e(Mt,Z1e),e(Z1e,xvr),e(Mt,Rvr),g(H6,Mt,null),e(kr,Svr),e(kr,ko),g(U6,ko,null),e(ko,Pvr),e(ko,e7e),e(e7e,$vr),e(ko,Ivr),e(ko,wn),e(wn,jvr),e(wn,o7e),e(o7e,Nvr),e(wn,Dvr),e(wn,r7e),e(r7e,qvr),e(wn,Gvr),e(wn,t7e),e(t7e,Ovr),e(wn,Xvr),e(ko,zvr),e(ko,Te),e(Te,cC),e(cC,a7e),e(a7e,Vvr),e(cC,Wvr),e(cC,sX),e(sX,Qvr),e(cC,Hvr),e(Te,Uvr),e(Te,fC),e(fC,n7e),e(n7e,Jvr),e(fC,Yvr),e(fC,lX),e(lX,Kvr),e(fC,Zvr),e(Te,e0r),e(Te,mC),e(mC,s7e),e(s7e,o0r),e(mC,r0r),e(mC,iX),e(iX,t0r),e(mC,a0r),e(Te,n0r),e(Te,gC),e(gC,l7e),e(l7e,s0r),e(gC,l0r),e(gC,dX),e(dX,i0r),e(gC,d0r),e(Te,c0r),e(Te,hC),e(hC,i7e),e(i7e,f0r),e(hC,m0r),e(hC,cX),e(cX,g0r),e(hC,h0r),e(Te,p0r),e(Te,pC),e(pC,d7e),e(d7e,_0r),e(pC,u0r),e(pC,fX),e(fX,b0r),e(pC,v0r),e(Te,T0r),e(Te,_C),e(_C,c7e),e(c7e,F0r),e(_C,C0r),e(_C,mX),e(mX,M0r),e(_C,E0r),e(Te,y0r),e(Te,uC),e(uC,f7e),e(f7e,w0r),e(uC,A0r),e(uC,gX),e(gX,L0r),e(uC,B0r),e(Te,k0r),e(Te,bC),e(bC,m7e),e(m7e,x0r),e(bC,R0r),e(bC,hX),e(hX,S0r),e(bC,P0r),e(ko,$0r),e(ko,g7e),e(g7e,I0r),e(ko,j0r),g(J6,ko,null),b(d,aBe,u),b(d,of,u),e(of,vC),e(vC,h7e),g(Y6,h7e,null),e(of,N0r),e(of,p7e),e(p7e,D0r),b(d,nBe,u),b(d,xr,u),g(K6,xr,null),e(xr,q0r),e(xr,rf),e(rf,G0r),e(rf,_7e),e(_7e,O0r),e(rf,X0r),e(rf,u7e),e(u7e,z0r),e(rf,V0r),e(xr,W0r),e(xr,Z6),e(Z6,Q0r),e(Z6,b7e),e(b7e,H0r),e(Z6,U0r),e(xr,J0r),e(xr,Et),g(eA,Et,null),e(Et,Y0r),e(Et,v7e),e(v7e,K0r),e(Et,Z0r),e(Et,tf),e(tf,eTr),e(tf,T7e),e(T7e,oTr),e(tf,rTr),e(tf,F7e),e(F7e,tTr),e(tf,aTr),e(Et,nTr),e(Et,C7e),e(C7e,sTr),e(Et,lTr),g(oA,Et,null),e(xr,iTr),e(xr,xo),g(rA,xo,null),e(xo,dTr),e(xo,M7e),e(M7e,cTr),e(xo,fTr),e(xo,An),e(An,mTr),e(An,E7e),e(E7e,gTr),e(An,hTr),e(An,y7e),e(y7e,pTr),e(An,_Tr),e(An,w7e),e(w7e,uTr),e(An,bTr),e(xo,vTr),e(xo,Fe),e(Fe,TC),e(TC,A7e),e(A7e,TTr),e(TC,FTr),e(TC,pX),e(pX,CTr),e(TC,MTr),e(Fe,ETr),e(Fe,FC),e(FC,L7e),e(L7e,yTr),e(FC,wTr),e(FC,_X),e(_X,ATr),e(FC,LTr),e(Fe,BTr),e(Fe,CC),e(CC,B7e),e(B7e,kTr),e(CC,xTr),e(CC,uX),e(uX,RTr),e(CC,STr),e(Fe,PTr),e(Fe,MC),e(MC,k7e),e(k7e,$Tr),e(MC,ITr),e(MC,bX),e(bX,jTr),e(MC,NTr),e(Fe,DTr),e(Fe,EC),e(EC,x7e),e(x7e,qTr),e(EC,GTr),e(EC,vX),e(vX,OTr),e(EC,XTr),e(Fe,zTr),e(Fe,yC),e(yC,R7e),e(R7e,VTr),e(yC,WTr),e(yC,TX),e(TX,QTr),e(yC,HTr),e(Fe,UTr),e(Fe,wC),e(wC,S7e),e(S7e,JTr),e(wC,YTr),e(wC,FX),e(FX,KTr),e(wC,ZTr),e(Fe,eFr),e(Fe,AC),e(AC,P7e),e(P7e,oFr),e(AC,rFr),e(AC,CX),e(CX,tFr),e(AC,aFr),e(Fe,nFr),e(Fe,LC),e(LC,$7e),e($7e,sFr),e(LC,lFr),e(LC,MX),e(MX,iFr),e(LC,dFr),e(xo,cFr),e(xo,I7e),e(I7e,fFr),e(xo,mFr),g(tA,xo,null),b(d,sBe,u),b(d,af,u),e(af,BC),e(BC,j7e),g(aA,j7e,null),e(af,gFr),e(af,N7e),e(N7e,hFr),b(d,lBe,u),b(d,Rr,u),g(nA,Rr,null),e(Rr,pFr),e(Rr,nf),e(nf,_Fr),e(nf,D7e),e(D7e,uFr),e(nf,bFr),e(nf,q7e),e(q7e,vFr),e(nf,TFr),e(Rr,FFr),e(Rr,sA),e(sA,CFr),e(sA,G7e),e(G7e,MFr),e(sA,EFr),e(Rr,yFr),e(Rr,yt),g(lA,yt,null),e(yt,wFr),e(yt,O7e),e(O7e,AFr),e(yt,LFr),e(yt,sf),e(sf,BFr),e(sf,X7e),e(X7e,kFr),e(sf,xFr),e(sf,z7e),e(z7e,RFr),e(sf,SFr),e(yt,PFr),e(yt,V7e),e(V7e,$Fr),e(yt,IFr),g(iA,yt,null),e(Rr,jFr),e(Rr,Ro),g(dA,Ro,null),e(Ro,NFr),e(Ro,W7e),e(W7e,DFr),e(Ro,qFr),e(Ro,Ln),e(Ln,GFr),e(Ln,Q7e),e(Q7e,OFr),e(Ln,XFr),e(Ln,H7e),e(H7e,zFr),e(Ln,VFr),e(Ln,U7e),e(U7e,WFr),e(Ln,QFr),e(Ro,HFr),e(Ro,Ce),e(Ce,kC),e(kC,J7e),e(J7e,UFr),e(kC,JFr),e(kC,EX),e(EX,YFr),e(kC,KFr),e(Ce,ZFr),e(Ce,xC),e(xC,Y7e),e(Y7e,eCr),e(xC,oCr),e(xC,yX),e(yX,rCr),e(xC,tCr),e(Ce,aCr),e(Ce,RC),e(RC,K7e),e(K7e,nCr),e(RC,sCr),e(RC,wX),e(wX,lCr),e(RC,iCr),e(Ce,dCr),e(Ce,SC),e(SC,Z7e),e(Z7e,cCr),e(SC,fCr),e(SC,AX),e(AX,mCr),e(SC,gCr),e(Ce,hCr),e(Ce,PC),e(PC,ebe),e(ebe,pCr),e(PC,_Cr),e(PC,LX),e(LX,uCr),e(PC,bCr),e(Ce,vCr),e(Ce,$C),e($C,obe),e(obe,TCr),e($C,FCr),e($C,BX),e(BX,CCr),e($C,MCr),e(Ce,ECr),e(Ce,IC),e(IC,rbe),e(rbe,yCr),e(IC,wCr),e(IC,kX),e(kX,ACr),e(IC,LCr),e(Ce,BCr),e(Ce,jC),e(jC,tbe),e(tbe,kCr),e(jC,xCr),e(jC,xX),e(xX,RCr),e(jC,SCr),e(Ce,PCr),e(Ce,NC),e(NC,abe),e(abe,$Cr),e(NC,ICr),e(NC,RX),e(RX,jCr),e(NC,NCr),e(Ro,DCr),e(Ro,nbe),e(nbe,qCr),e(Ro,GCr),g(cA,Ro,null),b(d,iBe,u),b(d,lf,u),e(lf,DC),e(DC,sbe),g(fA,sbe,null),e(lf,OCr),e(lf,lbe),e(lbe,XCr),b(d,dBe,u),b(d,Sr,u),g(mA,Sr,null),e(Sr,zCr),e(Sr,df),e(df,VCr),e(df,ibe),e(ibe,WCr),e(df,QCr),e(df,dbe),e(dbe,HCr),e(df,UCr),e(Sr,JCr),e(Sr,gA),e(gA,YCr),e(gA,cbe),e(cbe,KCr),e(gA,ZCr),e(Sr,e4r),e(Sr,wt),g(hA,wt,null),e(wt,o4r),e(wt,fbe),e(fbe,r4r),e(wt,t4r),e(wt,cf),e(cf,a4r),e(cf,mbe),e(mbe,n4r),e(cf,s4r),e(cf,gbe),e(gbe,l4r),e(cf,i4r),e(wt,d4r),e(wt,hbe),e(hbe,c4r),e(wt,f4r),g(pA,wt,null),e(Sr,m4r),e(Sr,So),g(_A,So,null),e(So,g4r),e(So,pbe),e(pbe,h4r),e(So,p4r),e(So,Bn),e(Bn,_4r),e(Bn,_be),e(_be,u4r),e(Bn,b4r),e(Bn,ube),e(ube,v4r),e(Bn,T4r),e(Bn,bbe),e(bbe,F4r),e(Bn,C4r),e(So,M4r),e(So,so),e(so,qC),e(qC,vbe),e(vbe,E4r),e(qC,y4r),e(qC,SX),e(SX,w4r),e(qC,A4r),e(so,L4r),e(so,GC),e(GC,Tbe),e(Tbe,B4r),e(GC,k4r),e(GC,PX),e(PX,x4r),e(GC,R4r),e(so,S4r),e(so,OC),e(OC,Fbe),e(Fbe,P4r),e(OC,$4r),e(OC,$X),e($X,I4r),e(OC,j4r),e(so,N4r),e(so,XC),e(XC,Cbe),e(Cbe,D4r),e(XC,q4r),e(XC,IX),e(IX,G4r),e(XC,O4r),e(so,X4r),e(so,zC),e(zC,Mbe),e(Mbe,z4r),e(zC,V4r),e(zC,jX),e(jX,W4r),e(zC,Q4r),e(so,H4r),e(so,VC),e(VC,Ebe),e(Ebe,U4r),e(VC,J4r),e(VC,NX),e(NX,Y4r),e(VC,K4r),e(so,Z4r),e(so,WC),e(WC,ybe),e(ybe,eMr),e(WC,oMr),e(WC,DX),e(DX,rMr),e(WC,tMr),e(So,aMr),e(So,wbe),e(wbe,nMr),e(So,sMr),g(uA,So,null),b(d,cBe,u),b(d,ff,u),e(ff,QC),e(QC,Abe),g(bA,Abe,null),e(ff,lMr),e(ff,Lbe),e(Lbe,iMr),b(d,fBe,u),b(d,Pr,u),g(vA,Pr,null),e(Pr,dMr),e(Pr,mf),e(mf,cMr),e(mf,Bbe),e(Bbe,fMr),e(mf,mMr),e(mf,kbe),e(kbe,gMr),e(mf,hMr),e(Pr,pMr),e(Pr,TA),e(TA,_Mr),e(TA,xbe),e(xbe,uMr),e(TA,bMr),e(Pr,vMr),e(Pr,At),g(FA,At,null),e(At,TMr),e(At,Rbe),e(Rbe,FMr),e(At,CMr),e(At,gf),e(gf,MMr),e(gf,Sbe),e(Sbe,EMr),e(gf,yMr),e(gf,Pbe),e(Pbe,wMr),e(gf,AMr),e(At,LMr),e(At,$be),e($be,BMr),e(At,kMr),g(CA,At,null),e(Pr,xMr),e(Pr,Po),g(MA,Po,null),e(Po,RMr),e(Po,Ibe),e(Ibe,SMr),e(Po,PMr),e(Po,kn),e(kn,$Mr),e(kn,jbe),e(jbe,IMr),e(kn,jMr),e(kn,Nbe),e(Nbe,NMr),e(kn,DMr),e(kn,Dbe),e(Dbe,qMr),e(kn,GMr),e(Po,OMr),e(Po,lo),e(lo,HC),e(HC,qbe),e(qbe,XMr),e(HC,zMr),e(HC,qX),e(qX,VMr),e(HC,WMr),e(lo,QMr),e(lo,UC),e(UC,Gbe),e(Gbe,HMr),e(UC,UMr),e(UC,GX),e(GX,JMr),e(UC,YMr),e(lo,KMr),e(lo,JC),e(JC,Obe),e(Obe,ZMr),e(JC,eEr),e(JC,OX),e(OX,oEr),e(JC,rEr),e(lo,tEr),e(lo,YC),e(YC,Xbe),e(Xbe,aEr),e(YC,nEr),e(YC,XX),e(XX,sEr),e(YC,lEr),e(lo,iEr),e(lo,KC),e(KC,zbe),e(zbe,dEr),e(KC,cEr),e(KC,zX),e(zX,fEr),e(KC,mEr),e(lo,gEr),e(lo,ZC),e(ZC,Vbe),e(Vbe,hEr),e(ZC,pEr),e(ZC,VX),e(VX,_Er),e(ZC,uEr),e(lo,bEr),e(lo,e4),e(e4,Wbe),e(Wbe,vEr),e(e4,TEr),e(e4,WX),e(WX,FEr),e(e4,CEr),e(Po,MEr),e(Po,Qbe),e(Qbe,EEr),e(Po,yEr),g(EA,Po,null),b(d,mBe,u),b(d,hf,u),e(hf,o4),e(o4,Hbe),g(yA,Hbe,null),e(hf,wEr),e(hf,Ube),e(Ube,AEr),b(d,gBe,u),b(d,$r,u),g(wA,$r,null),e($r,LEr),e($r,pf),e(pf,BEr),e(pf,Jbe),e(Jbe,kEr),e(pf,xEr),e(pf,Ybe),e(Ybe,REr),e(pf,SEr),e($r,PEr),e($r,AA),e(AA,$Er),e(AA,Kbe),e(Kbe,IEr),e(AA,jEr),e($r,NEr),e($r,Lt),g(LA,Lt,null),e(Lt,DEr),e(Lt,Zbe),e(Zbe,qEr),e(Lt,GEr),e(Lt,_f),e(_f,OEr),e(_f,e5e),e(e5e,XEr),e(_f,zEr),e(_f,o5e),e(o5e,VEr),e(_f,WEr),e(Lt,QEr),e(Lt,r5e),e(r5e,HEr),e(Lt,UEr),g(BA,Lt,null),e($r,JEr),e($r,$o),g(kA,$o,null),e($o,YEr),e($o,t5e),e(t5e,KEr),e($o,ZEr),e($o,xn),e(xn,e3r),e(xn,a5e),e(a5e,o3r),e(xn,r3r),e(xn,n5e),e(n5e,t3r),e(xn,a3r),e(xn,s5e),e(s5e,n3r),e(xn,s3r),e($o,l3r),e($o,l5e),e(l5e,r4),e(r4,i5e),e(i5e,i3r),e(r4,d3r),e(r4,QX),e(QX,c3r),e(r4,f3r),e($o,m3r),e($o,d5e),e(d5e,g3r),e($o,h3r),g(xA,$o,null),b(d,hBe,u),b(d,uf,u),e(uf,t4),e(t4,c5e),g(RA,c5e,null),e(uf,p3r),e(uf,f5e),e(f5e,_3r),b(d,pBe,u),b(d,Ir,u),g(SA,Ir,null),e(Ir,u3r),e(Ir,bf),e(bf,b3r),e(bf,m5e),e(m5e,v3r),e(bf,T3r),e(bf,g5e),e(g5e,F3r),e(bf,C3r),e(Ir,M3r),e(Ir,PA),e(PA,E3r),e(PA,h5e),e(h5e,y3r),e(PA,w3r),e(Ir,A3r),e(Ir,Bt),g($A,Bt,null),e(Bt,L3r),e(Bt,p5e),e(p5e,B3r),e(Bt,k3r),e(Bt,vf),e(vf,x3r),e(vf,_5e),e(_5e,R3r),e(vf,S3r),e(vf,u5e),e(u5e,P3r),e(vf,$3r),e(Bt,I3r),e(Bt,b5e),e(b5e,j3r),e(Bt,N3r),g(IA,Bt,null),e(Ir,D3r),e(Ir,Io),g(jA,Io,null),e(Io,q3r),e(Io,v5e),e(v5e,G3r),e(Io,O3r),e(Io,Rn),e(Rn,X3r),e(Rn,T5e),e(T5e,z3r),e(Rn,V3r),e(Rn,F5e),e(F5e,W3r),e(Rn,Q3r),e(Rn,C5e),e(C5e,H3r),e(Rn,U3r),e(Io,J3r),e(Io,NA),e(NA,a4),e(a4,M5e),e(M5e,Y3r),e(a4,K3r),e(a4,HX),e(HX,Z3r),e(a4,eyr),e(NA,oyr),e(NA,n4),e(n4,E5e),e(E5e,ryr),e(n4,tyr),e(n4,UX),e(UX,ayr),e(n4,nyr),e(Io,syr),e(Io,y5e),e(y5e,lyr),e(Io,iyr),g(DA,Io,null),b(d,_Be,u),b(d,Tf,u),e(Tf,s4),e(s4,w5e),g(qA,w5e,null),e(Tf,dyr),e(Tf,A5e),e(A5e,cyr),b(d,uBe,u),b(d,jr,u),g(GA,jr,null),e(jr,fyr),e(jr,Ff),e(Ff,myr),e(Ff,L5e),e(L5e,gyr),e(Ff,hyr),e(Ff,B5e),e(B5e,pyr),e(Ff,_yr),e(jr,uyr),e(jr,OA),e(OA,byr),e(OA,k5e),e(k5e,vyr),e(OA,Tyr),e(jr,Fyr),e(jr,kt),g(XA,kt,null),e(kt,Cyr),e(kt,x5e),e(x5e,Myr),e(kt,Eyr),e(kt,Cf),e(Cf,yyr),e(Cf,R5e),e(R5e,wyr),e(Cf,Ayr),e(Cf,S5e),e(S5e,Lyr),e(Cf,Byr),e(kt,kyr),e(kt,P5e),e(P5e,xyr),e(kt,Ryr),g(zA,kt,null),e(jr,Syr),e(jr,jo),g(VA,jo,null),e(jo,Pyr),e(jo,$5e),e($5e,$yr),e(jo,Iyr),e(jo,Sn),e(Sn,jyr),e(Sn,I5e),e(I5e,Nyr),e(Sn,Dyr),e(Sn,j5e),e(j5e,qyr),e(Sn,Gyr),e(Sn,N5e),e(N5e,Oyr),e(Sn,Xyr),e(jo,zyr),e(jo,D5e),e(D5e,l4),e(l4,q5e),e(q5e,Vyr),e(l4,Wyr),e(l4,JX),e(JX,Qyr),e(l4,Hyr),e(jo,Uyr),e(jo,G5e),e(G5e,Jyr),e(jo,Yyr),g(WA,jo,null),bBe=!0},p(d,[u]){const QA={};u&2&&(QA.$$scope={dirty:u,ctx:d}),Bf.$set(QA);const O5e={};u&2&&(O5e.$$scope={dirty:u,ctx:d}),ch.$set(O5e);const X5e={};u&2&&(X5e.$$scope={dirty:u,ctx:d}),Fh.$set(X5e)},i(d){bBe||(h(ce.$$.fragment,d),h($a.$$.fragment,d),h(dM.$$.fragment,d),h(cM.$$.fragment,d),h(Bf.$$.fragment,d),h(fM.$$.fragment,d),h(mM.$$.fragment,d),h(pM.$$.fragment,d),h(_M.$$.fragment,d),h(uM.$$.fragment,d),h(bM.$$.fragment,d),h(vM.$$.fragment,d),h(CM.$$.fragment,d),h(MM.$$.fragment,d),h(EM.$$.fragment,d),h(yM.$$.fragment,d),h(wM.$$.fragment,d),h(BM.$$.fragment,d),h(ch.$$.fragment,d),h(kM.$$.fragment,d),h(xM.$$.fragment,d),h(RM.$$.fragment,d),h(SM.$$.fragment,d),h(IM.$$.fragment,d),h(Fh.$$.fragment,d),h(jM.$$.fragment,d),h(NM.$$.fragment,d),h(DM.$$.fragment,d),h(qM.$$.fragment,d),h(OM.$$.fragment,d),h(XM.$$.fragment,d),h(zM.$$.fragment,d),h(VM.$$.fragment,d),h(WM.$$.fragment,d),h(QM.$$.fragment,d),h(UM.$$.fragment,d),h(JM.$$.fragment,d),h(YM.$$.fragment,d),h(KM.$$.fragment,d),h(ZM.$$.fragment,d),h(eE.$$.fragment,d),h(rE.$$.fragment,d),h(tE.$$.fragment,d),h(aE.$$.fragment,d),h(nE.$$.fragment,d),h(sE.$$.fragment,d),h(lE.$$.fragment,d),h(dE.$$.fragment,d),h(cE.$$.fragment,d),h(fE.$$.fragment,d),h(mE.$$.fragment,d),h(gE.$$.fragment,d),h(hE.$$.fragment,d),h(_E.$$.fragment,d),h(uE.$$.fragment,d),h(bE.$$.fragment,d),h(vE.$$.fragment,d),h(TE.$$.fragment,d),h(FE.$$.fragment,d),h(ME.$$.fragment,d),h(EE.$$.fragment,d),h(yE.$$.fragment,d),h(wE.$$.fragment,d),h(AE.$$.fragment,d),h(LE.$$.fragment,d),h(kE.$$.fragment,d),h(xE.$$.fragment,d),h(RE.$$.fragment,d),h(SE.$$.fragment,d),h(PE.$$.fragment,d),h($E.$$.fragment,d),h(jE.$$.fragment,d),h(NE.$$.fragment,d),h(DE.$$.fragment,d),h(qE.$$.fragment,d),h(GE.$$.fragment,d),h(OE.$$.fragment,d),h(zE.$$.fragment,d),h(VE.$$.fragment,d),h(WE.$$.fragment,d),h(QE.$$.fragment,d),h(HE.$$.fragment,d),h(UE.$$.fragment,d),h(YE.$$.fragment,d),h(KE.$$.fragment,d),h(ZE.$$.fragment,d),h(e3.$$.fragment,d),h(o3.$$.fragment,d),h(r3.$$.fragment,d),h(a3.$$.fragment,d),h(n3.$$.fragment,d),h(s3.$$.fragment,d),h(l3.$$.fragment,d),h(i3.$$.fragment,d),h(d3.$$.fragment,d),h(f3.$$.fragment,d),h(m3.$$.fragment,d),h(g3.$$.fragment,d),h(h3.$$.fragment,d),h(p3.$$.fragment,d),h(_3.$$.fragment,d),h(b3.$$.fragment,d),h(v3.$$.fragment,d),h(T3.$$.fragment,d),h(F3.$$.fragment,d),h(C3.$$.fragment,d),h(M3.$$.fragment,d),h(y3.$$.fragment,d),h(w3.$$.fragment,d),h(A3.$$.fragment,d),h(L3.$$.fragment,d),h(B3.$$.fragment,d),h(k3.$$.fragment,d),h(R3.$$.fragment,d),h(S3.$$.fragment,d),h(P3.$$.fragment,d),h($3.$$.fragment,d),h(I3.$$.fragment,d),h(j3.$$.fragment,d),h(D3.$$.fragment,d),h(q3.$$.fragment,d),h(G3.$$.fragment,d),h(O3.$$.fragment,d),h(X3.$$.fragment,d),h(z3.$$.fragment,d),h(W3.$$.fragment,d),h(Q3.$$.fragment,d),h(H3.$$.fragment,d),h(J3.$$.fragment,d),h(Y3.$$.fragment,d),h(K3.$$.fragment,d),h(ey.$$.fragment,d),h(oy.$$.fragment,d),h(ry.$$.fragment,d),h(ty.$$.fragment,d),h(ay.$$.fragment,d),h(ny.$$.fragment,d),h(ly.$$.fragment,d),h(iy.$$.fragment,d),h(dy.$$.fragment,d),h(cy.$$.fragment,d),h(fy.$$.fragment,d),h(my.$$.fragment,d),h(hy.$$.fragment,d),h(py.$$.fragment,d),h(_y.$$.fragment,d),h(uy.$$.fragment,d),h(by.$$.fragment,d),h(vy.$$.fragment,d),h(Fy.$$.fragment,d),h(Cy.$$.fragment,d),h(My.$$.fragment,d),h(Ey.$$.fragment,d),h(yy.$$.fragment,d),h(wy.$$.fragment,d),h(Ly.$$.fragment,d),h(By.$$.fragment,d),h(ky.$$.fragment,d),h(Ry.$$.fragment,d),h(Sy.$$.fragment,d),h(Py.$$.fragment,d),h(Iy.$$.fragment,d),h(jy.$$.fragment,d),h(Ny.$$.fragment,d),h(Dy.$$.fragment,d),h(qy.$$.fragment,d),h(Gy.$$.fragment,d),h(Xy.$$.fragment,d),h(zy.$$.fragment,d),h(Vy.$$.fragment,d),h(Wy.$$.fragment,d),h(Qy.$$.fragment,d),h(Hy.$$.fragment,d),h(Jy.$$.fragment,d),h(Yy.$$.fragment,d),h(Ky.$$.fragment,d),h(Zy.$$.fragment,d),h(ew.$$.fragment,d),h(ow.$$.fragment,d),h(tw.$$.fragment,d),h(aw.$$.fragment,d),h(nw.$$.fragment,d),h(sw.$$.fragment,d),h(lw.$$.fragment,d),h(iw.$$.fragment,d),h(cw.$$.fragment,d),h(fw.$$.fragment,d),h(mw.$$.fragment,d),h(gw.$$.fragment,d),h(hw.$$.fragment,d),h(pw.$$.fragment,d),h(uw.$$.fragment,d),h(bw.$$.fragment,d),h(vw.$$.fragment,d),h(Tw.$$.fragment,d),h(Fw.$$.fragment,d),h(Cw.$$.fragment,d),h(Ew.$$.fragment,d),h(yw.$$.fragment,d),h(ww.$$.fragment,d),h(Aw.$$.fragment,d),h(Lw.$$.fragment,d),h(Bw.$$.fragment,d),h(xw.$$.fragment,d),h(Rw.$$.fragment,d),h(Sw.$$.fragment,d),h(Pw.$$.fragment,d),h($w.$$.fragment,d),h(Iw.$$.fragment,d),h(Nw.$$.fragment,d),h(Dw.$$.fragment,d),h(qw.$$.fragment,d),h(Gw.$$.fragment,d),h(Ow.$$.fragment,d),h(Xw.$$.fragment,d),h(Vw.$$.fragment,d),h(Ww.$$.fragment,d),h(Qw.$$.fragment,d),h(Hw.$$.fragment,d),h(Uw.$$.fragment,d),h(Jw.$$.fragment,d),h(Kw.$$.fragment,d),h(Zw.$$.fragment,d),h(e6.$$.fragment,d),h(o6.$$.fragment,d),h(r6.$$.fragment,d),h(t6.$$.fragment,d),h(n6.$$.fragment,d),h(s6.$$.fragment,d),h(l6.$$.fragment,d),h(i6.$$.fragment,d),h(d6.$$.fragment,d),h(c6.$$.fragment,d),h(m6.$$.fragment,d),h(g6.$$.fragment,d),h(h6.$$.fragment,d),h(p6.$$.fragment,d),h(_6.$$.fragment,d),h(u6.$$.fragment,d),h(v6.$$.fragment,d),h(T6.$$.fragment,d),h(F6.$$.fragment,d),h(C6.$$.fragment,d),h(M6.$$.fragment,d),h(E6.$$.fragment,d),h(w6.$$.fragment,d),h(A6.$$.fragment,d),h(L6.$$.fragment,d),h(B6.$$.fragment,d),h(k6.$$.fragment,d),h(x6.$$.fragment,d),h(S6.$$.fragment,d),h(P6.$$.fragment,d),h($6.$$.fragment,d),h(I6.$$.fragment,d),h(j6.$$.fragment,d),h(N6.$$.fragment,d),h(q6.$$.fragment,d),h(G6.$$.fragment,d),h(O6.$$.fragment,d),h(X6.$$.fragment,d),h(z6.$$.fragment,d),h(V6.$$.fragment,d),h(Q6.$$.fragment,d),h(H6.$$.fragment,d),h(U6.$$.fragment,d),h(J6.$$.fragment,d),h(Y6.$$.fragment,d),h(K6.$$.fragment,d),h(eA.$$.fragment,d),h(oA.$$.fragment,d),h(rA.$$.fragment,d),h(tA.$$.fragment,d),h(aA.$$.fragment,d),h(nA.$$.fragment,d),h(lA.$$.fragment,d),h(iA.$$.fragment,d),h(dA.$$.fragment,d),h(cA.$$.fragment,d),h(fA.$$.fragment,d),h(mA.$$.fragment,d),h(hA.$$.fragment,d),h(pA.$$.fragment,d),h(_A.$$.fragment,d),h(uA.$$.fragment,d),h(bA.$$.fragment,d),h(vA.$$.fragment,d),h(FA.$$.fragment,d),h(CA.$$.fragment,d),h(MA.$$.fragment,d),h(EA.$$.fragment,d),h(yA.$$.fragment,d),h(wA.$$.fragment,d),h(LA.$$.fragment,d),h(BA.$$.fragment,d),h(kA.$$.fragment,d),h(xA.$$.fragment,d),h(RA.$$.fragment,d),h(SA.$$.fragment,d),h($A.$$.fragment,d),h(IA.$$.fragment,d),h(jA.$$.fragment,d),h(DA.$$.fragment,d),h(qA.$$.fragment,d),h(GA.$$.fragment,d),h(XA.$$.fragment,d),h(zA.$$.fragment,d),h(VA.$$.fragment,d),h(WA.$$.fragment,d),bBe=!0)},o(d){p(ce.$$.fragment,d),p($a.$$.fragment,d),p(dM.$$.fragment,d),p(cM.$$.fragment,d),p(Bf.$$.fragment,d),p(fM.$$.fragment,d),p(mM.$$.fragment,d),p(pM.$$.fragment,d),p(_M.$$.fragment,d),p(uM.$$.fragment,d),p(bM.$$.fragment,d),p(vM.$$.fragment,d),p(CM.$$.fragment,d),p(MM.$$.fragment,d),p(EM.$$.fragment,d),p(yM.$$.fragment,d),p(wM.$$.fragment,d),p(BM.$$.fragment,d),p(ch.$$.fragment,d),p(kM.$$.fragment,d),p(xM.$$.fragment,d),p(RM.$$.fragment,d),p(SM.$$.fragment,d),p(IM.$$.fragment,d),p(Fh.$$.fragment,d),p(jM.$$.fragment,d),p(NM.$$.fragment,d),p(DM.$$.fragment,d),p(qM.$$.fragment,d),p(OM.$$.fragment,d),p(XM.$$.fragment,d),p(zM.$$.fragment,d),p(VM.$$.fragment,d),p(WM.$$.fragment,d),p(QM.$$.fragment,d),p(UM.$$.fragment,d),p(JM.$$.fragment,d),p(YM.$$.fragment,d),p(KM.$$.fragment,d),p(ZM.$$.fragment,d),p(eE.$$.fragment,d),p(rE.$$.fragment,d),p(tE.$$.fragment,d),p(aE.$$.fragment,d),p(nE.$$.fragment,d),p(sE.$$.fragment,d),p(lE.$$.fragment,d),p(dE.$$.fragment,d),p(cE.$$.fragment,d),p(fE.$$.fragment,d),p(mE.$$.fragment,d),p(gE.$$.fragment,d),p(hE.$$.fragment,d),p(_E.$$.fragment,d),p(uE.$$.fragment,d),p(bE.$$.fragment,d),p(vE.$$.fragment,d),p(TE.$$.fragment,d),p(FE.$$.fragment,d),p(ME.$$.fragment,d),p(EE.$$.fragment,d),p(yE.$$.fragment,d),p(wE.$$.fragment,d),p(AE.$$.fragment,d),p(LE.$$.fragment,d),p(kE.$$.fragment,d),p(xE.$$.fragment,d),p(RE.$$.fragment,d),p(SE.$$.fragment,d),p(PE.$$.fragment,d),p($E.$$.fragment,d),p(jE.$$.fragment,d),p(NE.$$.fragment,d),p(DE.$$.fragment,d),p(qE.$$.fragment,d),p(GE.$$.fragment,d),p(OE.$$.fragment,d),p(zE.$$.fragment,d),p(VE.$$.fragment,d),p(WE.$$.fragment,d),p(QE.$$.fragment,d),p(HE.$$.fragment,d),p(UE.$$.fragment,d),p(YE.$$.fragment,d),p(KE.$$.fragment,d),p(ZE.$$.fragment,d),p(e3.$$.fragment,d),p(o3.$$.fragment,d),p(r3.$$.fragment,d),p(a3.$$.fragment,d),p(n3.$$.fragment,d),p(s3.$$.fragment,d),p(l3.$$.fragment,d),p(i3.$$.fragment,d),p(d3.$$.fragment,d),p(f3.$$.fragment,d),p(m3.$$.fragment,d),p(g3.$$.fragment,d),p(h3.$$.fragment,d),p(p3.$$.fragment,d),p(_3.$$.fragment,d),p(b3.$$.fragment,d),p(v3.$$.fragment,d),p(T3.$$.fragment,d),p(F3.$$.fragment,d),p(C3.$$.fragment,d),p(M3.$$.fragment,d),p(y3.$$.fragment,d),p(w3.$$.fragment,d),p(A3.$$.fragment,d),p(L3.$$.fragment,d),p(B3.$$.fragment,d),p(k3.$$.fragment,d),p(R3.$$.fragment,d),p(S3.$$.fragment,d),p(P3.$$.fragment,d),p($3.$$.fragment,d),p(I3.$$.fragment,d),p(j3.$$.fragment,d),p(D3.$$.fragment,d),p(q3.$$.fragment,d),p(G3.$$.fragment,d),p(O3.$$.fragment,d),p(X3.$$.fragment,d),p(z3.$$.fragment,d),p(W3.$$.fragment,d),p(Q3.$$.fragment,d),p(H3.$$.fragment,d),p(J3.$$.fragment,d),p(Y3.$$.fragment,d),p(K3.$$.fragment,d),p(ey.$$.fragment,d),p(oy.$$.fragment,d),p(ry.$$.fragment,d),p(ty.$$.fragment,d),p(ay.$$.fragment,d),p(ny.$$.fragment,d),p(ly.$$.fragment,d),p(iy.$$.fragment,d),p(dy.$$.fragment,d),p(cy.$$.fragment,d),p(fy.$$.fragment,d),p(my.$$.fragment,d),p(hy.$$.fragment,d),p(py.$$.fragment,d),p(_y.$$.fragment,d),p(uy.$$.fragment,d),p(by.$$.fragment,d),p(vy.$$.fragment,d),p(Fy.$$.fragment,d),p(Cy.$$.fragment,d),p(My.$$.fragment,d),p(Ey.$$.fragment,d),p(yy.$$.fragment,d),p(wy.$$.fragment,d),p(Ly.$$.fragment,d),p(By.$$.fragment,d),p(ky.$$.fragment,d),p(Ry.$$.fragment,d),p(Sy.$$.fragment,d),p(Py.$$.fragment,d),p(Iy.$$.fragment,d),p(jy.$$.fragment,d),p(Ny.$$.fragment,d),p(Dy.$$.fragment,d),p(qy.$$.fragment,d),p(Gy.$$.fragment,d),p(Xy.$$.fragment,d),p(zy.$$.fragment,d),p(Vy.$$.fragment,d),p(Wy.$$.fragment,d),p(Qy.$$.fragment,d),p(Hy.$$.fragment,d),p(Jy.$$.fragment,d),p(Yy.$$.fragment,d),p(Ky.$$.fragment,d),p(Zy.$$.fragment,d),p(ew.$$.fragment,d),p(ow.$$.fragment,d),p(tw.$$.fragment,d),p(aw.$$.fragment,d),p(nw.$$.fragment,d),p(sw.$$.fragment,d),p(lw.$$.fragment,d),p(iw.$$.fragment,d),p(cw.$$.fragment,d),p(fw.$$.fragment,d),p(mw.$$.fragment,d),p(gw.$$.fragment,d),p(hw.$$.fragment,d),p(pw.$$.fragment,d),p(uw.$$.fragment,d),p(bw.$$.fragment,d),p(vw.$$.fragment,d),p(Tw.$$.fragment,d),p(Fw.$$.fragment,d),p(Cw.$$.fragment,d),p(Ew.$$.fragment,d),p(yw.$$.fragment,d),p(ww.$$.fragment,d),p(Aw.$$.fragment,d),p(Lw.$$.fragment,d),p(Bw.$$.fragment,d),p(xw.$$.fragment,d),p(Rw.$$.fragment,d),p(Sw.$$.fragment,d),p(Pw.$$.fragment,d),p($w.$$.fragment,d),p(Iw.$$.fragment,d),p(Nw.$$.fragment,d),p(Dw.$$.fragment,d),p(qw.$$.fragment,d),p(Gw.$$.fragment,d),p(Ow.$$.fragment,d),p(Xw.$$.fragment,d),p(Vw.$$.fragment,d),p(Ww.$$.fragment,d),p(Qw.$$.fragment,d),p(Hw.$$.fragment,d),p(Uw.$$.fragment,d),p(Jw.$$.fragment,d),p(Kw.$$.fragment,d),p(Zw.$$.fragment,d),p(e6.$$.fragment,d),p(o6.$$.fragment,d),p(r6.$$.fragment,d),p(t6.$$.fragment,d),p(n6.$$.fragment,d),p(s6.$$.fragment,d),p(l6.$$.fragment,d),p(i6.$$.fragment,d),p(d6.$$.fragment,d),p(c6.$$.fragment,d),p(m6.$$.fragment,d),p(g6.$$.fragment,d),p(h6.$$.fragment,d),p(p6.$$.fragment,d),p(_6.$$.fragment,d),p(u6.$$.fragment,d),p(v6.$$.fragment,d),p(T6.$$.fragment,d),p(F6.$$.fragment,d),p(C6.$$.fragment,d),p(M6.$$.fragment,d),p(E6.$$.fragment,d),p(w6.$$.fragment,d),p(A6.$$.fragment,d),p(L6.$$.fragment,d),p(B6.$$.fragment,d),p(k6.$$.fragment,d),p(x6.$$.fragment,d),p(S6.$$.fragment,d),p(P6.$$.fragment,d),p($6.$$.fragment,d),p(I6.$$.fragment,d),p(j6.$$.fragment,d),p(N6.$$.fragment,d),p(q6.$$.fragment,d),p(G6.$$.fragment,d),p(O6.$$.fragment,d),p(X6.$$.fragment,d),p(z6.$$.fragment,d),p(V6.$$.fragment,d),p(Q6.$$.fragment,d),p(H6.$$.fragment,d),p(U6.$$.fragment,d),p(J6.$$.fragment,d),p(Y6.$$.fragment,d),p(K6.$$.fragment,d),p(eA.$$.fragment,d),p(oA.$$.fragment,d),p(rA.$$.fragment,d),p(tA.$$.fragment,d),p(aA.$$.fragment,d),p(nA.$$.fragment,d),p(lA.$$.fragment,d),p(iA.$$.fragment,d),p(dA.$$.fragment,d),p(cA.$$.fragment,d),p(fA.$$.fragment,d),p(mA.$$.fragment,d),p(hA.$$.fragment,d),p(pA.$$.fragment,d),p(_A.$$.fragment,d),p(uA.$$.fragment,d),p(bA.$$.fragment,d),p(vA.$$.fragment,d),p(FA.$$.fragment,d),p(CA.$$.fragment,d),p(MA.$$.fragment,d),p(EA.$$.fragment,d),p(yA.$$.fragment,d),p(wA.$$.fragment,d),p(LA.$$.fragment,d),p(BA.$$.fragment,d),p(kA.$$.fragment,d),p(xA.$$.fragment,d),p(RA.$$.fragment,d),p(SA.$$.fragment,d),p($A.$$.fragment,d),p(IA.$$.fragment,d),p(jA.$$.fragment,d),p(DA.$$.fragment,d),p(qA.$$.fragment,d),p(GA.$$.fragment,d),p(XA.$$.fragment,d),p(zA.$$.fragment,d),p(VA.$$.fragment,d),p(WA.$$.fragment,d),bBe=!1},d(d){t(J),d&&t(Le),d&&t(de),_(ce),d&&t(Ef),d&&t(sa),d&&t(we),d&&t(io),d&&t(wf),_($a,d),d&&t(co),d&&t(ge),d&&t(qo),d&&t(Ia),d&&t(v8e),d&&t(Si),_(dM),d&&t(T8e),d&&t(Nn),d&&t(F8e),_(cM,d),d&&t(C8e),d&&t(HL),d&&t(M8e),_(Bf,d),d&&t(E8e),d&&t(Pi),_(fM),d&&t(y8e),d&&t(Go),_(mM),_(pM),_(_M),_(uM),d&&t(w8e),d&&t(Ii),_(bM),d&&t(A8e),d&&t(Oo),_(vM),_(CM),_(MM),_(EM),d&&t(L8e),d&&t(ji),_(yM),d&&t(B8e),d&&t(Xo),_(wM),_(BM),_(ch),_(kM),_(xM),d&&t(k8e),d&&t(Ni),_(RM),d&&t(x8e),d&&t(zo),_(SM),_(IM),_(Fh),_(jM),_(NM),d&&t(R8e),d&&t(qi),_(DM),d&&t(S8e),d&&t(Vo),_(qM),_(OM),_(XM),_(zM),_(VM),d&&t(P8e),d&&t(Xi),_(WM),d&&t($8e),d&&t(Wo),_(QM),_(UM),_(JM),_(YM),_(KM),d&&t(I8e),d&&t(Wi),_(ZM),d&&t(j8e),d&&t(Qo),_(eE),_(rE),_(tE),_(aE),_(nE),d&&t(N8e),d&&t(Ui),_(sE),d&&t(D8e),d&&t(Ho),_(lE),_(dE),_(cE),_(fE),_(mE),d&&t(q8e),d&&t(Ki),_(gE),d&&t(G8e),d&&t(Uo),_(hE),_(_E),_(uE),_(bE),_(vE),d&&t(O8e),d&&t(od),_(TE),d&&t(X8e),d&&t(Jo),_(FE),_(ME),_(EE),_(yE),_(wE),d&&t(z8e),d&&t(ad),_(AE),d&&t(V8e),d&&t(Yo),_(LE),_(kE),_(xE),_(RE),_(SE),d&&t(W8e),d&&t(ld),_(PE),d&&t(Q8e),d&&t(Ko),_($E),_(jE),_(NE),_(DE),_(qE),d&&t(H8e),d&&t(cd),_(GE),d&&t(U8e),d&&t(Zo),_(OE),_(zE),_(VE),_(WE),_(QE),d&&t(J8e),d&&t(gd),_(HE),d&&t(Y8e),d&&t(er),_(UE),_(YE),_(KE),_(ZE),_(e3),d&&t(K8e),d&&t(_d),_(o3),d&&t(Z8e),d&&t(or),_(r3),_(a3),_(n3),_(s3),_(l3),d&&t(e9e),d&&t(vd),_(i3),d&&t(o9e),d&&t(rr),_(d3),_(f3),_(m3),_(g3),_(h3),d&&t(r9e),d&&t(Cd),_(p3),d&&t(t9e),d&&t(tr),_(_3),_(b3),_(v3),_(T3),_(F3),d&&t(a9e),d&&t(yd),_(C3),d&&t(n9e),d&&t(ar),_(M3),_(y3),_(w3),_(A3),_(L3),d&&t(s9e),d&&t(Ld),_(B3),d&&t(l9e),d&&t(nr),_(k3),_(R3),_(S3),_(P3),_($3),d&&t(i9e),d&&t(Rd),_(I3),d&&t(d9e),d&&t(sr),_(j3),_(D3),_(q3),_(G3),_(O3),d&&t(c9e),d&&t($d),_(X3),d&&t(f9e),d&&t(lr),_(z3),_(W3),_(Q3),_(H3),_(J3),d&&t(m9e),d&&t(Nd),_(Y3),d&&t(g9e),d&&t(ir),_(K3),_(ey),_(oy),_(ry),_(ty),d&&t(h9e),d&&t(Od),_(ay),d&&t(p9e),d&&t(dr),_(ny),_(ly),_(iy),_(dy),_(cy),d&&t(_9e),d&&t(Wd),_(fy),d&&t(u9e),d&&t(cr),_(my),_(hy),_(py),_(_y),_(uy),d&&t(b9e),d&&t(Ud),_(by),d&&t(v9e),d&&t(fr),_(vy),_(Fy),_(Cy),_(My),_(Ey),d&&t(T9e),d&&t(Kd),_(yy),d&&t(F9e),d&&t(mr),_(wy),_(Ly),_(By),_(ky),_(Ry),d&&t(C9e),d&&t(oc),_(Sy),d&&t(M9e),d&&t(gr),_(Py),_(Iy),_(jy),_(Ny),_(Dy),d&&t(E9e),d&&t(ac),_(qy),d&&t(y9e),d&&t(hr),_(Gy),_(Xy),_(zy),_(Vy),_(Wy),d&&t(w9e),d&&t(lc),_(Qy),d&&t(A9e),d&&t(pr),_(Hy),_(Jy),_(Yy),_(Ky),_(Zy),d&&t(L9e),d&&t(cc),_(ew),d&&t(B9e),d&&t(_r),_(ow),_(tw),_(aw),_(nw),_(sw),d&&t(k9e),d&&t(gc),_(lw),d&&t(x9e),d&&t(ur),_(iw),_(cw),_(fw),_(mw),_(gw),d&&t(R9e),d&&t(_c),_(hw),d&&t(S9e),d&&t(br),_(pw),_(uw),_(bw),_(vw),_(Tw),d&&t(P9e),d&&t(vc),_(Fw),d&&t($9e),d&&t(vr),_(Cw),_(Ew),_(yw),_(ww),_(Aw),d&&t(I9e),d&&t(Cc),_(Lw),d&&t(j9e),d&&t(Tr),_(Bw),_(xw),_(Rw),_(Sw),_(Pw),d&&t(N9e),d&&t(yc),_($w),d&&t(D9e),d&&t(Fr),_(Iw),_(Nw),_(Dw),_(qw),_(Gw),d&&t(q9e),d&&t(Lc),_(Ow),d&&t(G9e),d&&t(Cr),_(Xw),_(Vw),_(Ww),_(Qw),_(Hw),d&&t(O9e),d&&t(xc),_(Uw),d&&t(X9e),d&&t(Mr),_(Jw),_(Kw),_(Zw),_(e6),_(o6),d&&t(z9e),d&&t(Pc),_(r6),d&&t(V9e),d&&t(Er),_(t6),_(n6),_(s6),_(l6),_(i6),d&&t(W9e),d&&t(jc),_(d6),d&&t(Q9e),d&&t(yr),_(c6),_(m6),_(g6),_(h6),_(p6),d&&t(H9e),d&&t(qc),_(_6),d&&t(U9e),d&&t(wr),_(u6),_(v6),_(T6),_(F6),_(C6),d&&t(J9e),d&&t(Xc),_(M6),d&&t(Y9e),d&&t(Ar),_(E6),_(w6),_(A6),_(L6),_(B6),d&&t(K9e),d&&t(Wc),_(k6),d&&t(Z9e),d&&t(Lr),_(x6),_(S6),_(P6),_($6),_(I6),d&&t(eBe),d&&t(Uc),_(j6),d&&t(oBe),d&&t(Br),_(N6),_(q6),_(G6),_(O6),_(X6),d&&t(rBe),d&&t(Kc),_(z6),d&&t(tBe),d&&t(kr),_(V6),_(Q6),_(H6),_(U6),_(J6),d&&t(aBe),d&&t(of),_(Y6),d&&t(nBe),d&&t(xr),_(K6),_(eA),_(oA),_(rA),_(tA),d&&t(sBe),d&&t(af),_(aA),d&&t(lBe),d&&t(Rr),_(nA),_(lA),_(iA),_(dA),_(cA),d&&t(iBe),d&&t(lf),_(fA),d&&t(dBe),d&&t(Sr),_(mA),_(hA),_(pA),_(_A),_(uA),d&&t(cBe),d&&t(ff),_(bA),d&&t(fBe),d&&t(Pr),_(vA),_(FA),_(CA),_(MA),_(EA),d&&t(mBe),d&&t(hf),_(yA),d&&t(gBe),d&&t($r),_(wA),_(LA),_(BA),_(kA),_(xA),d&&t(hBe),d&&t(uf),_(RA),d&&t(pBe),d&&t(Ir),_(SA),_($A),_(IA),_(jA),_(DA),d&&t(_Be),d&&t(Tf),_(qA),d&&t(uBe),d&&t(jr),_(GA),_(XA),_(zA),_(VA),_(WA)}}}const put={local:"auto-classes",sections:[{local:"extending-the-auto-classes",title:"Extending the Auto Classes"},{local:"transformers.AutoConfig",title:"AutoConfig"},{local:"transformers.AutoTokenizer",title:"AutoTokenizer"},{local:"transformers.AutoFeatureExtractor",title:"AutoFeatureExtractor"},{local:"transformers.AutoProcessor",title:"AutoProcessor"},{local:"transformers.AutoModel",title:"AutoModel"},{local:"transformers.AutoModelForPreTraining",title:"AutoModelForPreTraining"},{local:"transformers.AutoModelForCausalLM",title:"AutoModelForCausalLM"},{local:"transformers.AutoModelForMaskedLM",title:"AutoModelForMaskedLM"},{local:"transformers.AutoModelForSeq2SeqLM",title:"AutoModelForSeq2SeqLM"},{local:"transformers.AutoModelForSequenceClassification",title:"AutoModelForSequenceClassification"},{local:"transformers.AutoModelForMultipleChoice",title:"AutoModelForMultipleChoice"},{local:"transformers.AutoModelForNextSentencePrediction",title:"AutoModelForNextSentencePrediction"},{local:"transformers.AutoModelForTokenClassification",title:"AutoModelForTokenClassification"},{local:"transformers.AutoModelForQuestionAnswering",title:"AutoModelForQuestionAnswering"},{local:"transformers.AutoModelForTableQuestionAnswering",title:"AutoModelForTableQuestionAnswering"},{local:"transformers.AutoModelForImageClassification",title:"AutoModelForImageClassification"},{local:"transformers.AutoModelForVision2Seq",title:"AutoModelForVision2Seq"},{local:"transformers.AutoModelForAudioClassification",title:"AutoModelForAudioClassification"},{local:"transformers.AutoModelForAudioFrameClassification",title:"AutoModelForAudioFrameClassification"},{local:"transformers.AutoModelForCTC",title:"AutoModelForCTC"},{local:"transformers.AutoModelForSpeechSeq2Seq",title:"AutoModelForSpeechSeq2Seq"},{local:"transformers.AutoModelForAudioXVector",title:"AutoModelForAudioXVector"},{local:"transformers.AutoModelForMaskedImageModeling",title:"AutoModelForMaskedImageModeling"},{local:"transformers.AutoModelForObjectDetection",title:"AutoModelForObjectDetection"},{local:"transformers.AutoModelForImageSegmentation",title:"AutoModelForImageSegmentation"},{local:"transformers.AutoModelForSemanticSegmentation",title:"AutoModelForSemanticSegmentation"},{local:"transformers.TFAutoModel",title:"TFAutoModel"},{local:"transformers.TFAutoModelForPreTraining",title:"TFAutoModelForPreTraining"},{local:"transformers.TFAutoModelForCausalLM",title:"TFAutoModelForCausalLM"},{local:"transformers.TFAutoModelForImageClassification",title:"TFAutoModelForImageClassification"},{local:"transformers.TFAutoModelForMaskedLM",title:"TFAutoModelForMaskedLM"},{local:"transformers.TFAutoModelForSeq2SeqLM",title:"TFAutoModelForSeq2SeqLM"},{local:"transformers.TFAutoModelForSequenceClassification",title:"TFAutoModelForSequenceClassification"},{local:"transformers.TFAutoModelForMultipleChoice",title:"TFAutoModelForMultipleChoice"},{local:"transformers.TFAutoModelForTableQuestionAnswering",title:"TFAutoModelForTableQuestionAnswering"},{local:"transformers.TFAutoModelForTokenClassification",title:"TFAutoModelForTokenClassification"},{local:"transformers.TFAutoModelForQuestionAnswering",title:"TFAutoModelForQuestionAnswering"},{local:"transformers.TFAutoModelForVision2Seq",title:"TFAutoModelForVision2Seq"},{local:"transformers.TFAutoModelForSpeechSeq2Seq",title:"TFAutoModelForSpeechSeq2Seq"},{local:"transformers.FlaxAutoModel",title:"FlaxAutoModel"},{local:"transformers.FlaxAutoModelForCausalLM",title:"FlaxAutoModelForCausalLM"},{local:"transformers.FlaxAutoModelForPreTraining",title:"FlaxAutoModelForPreTraining"},{local:"transformers.FlaxAutoModelForMaskedLM",title:"FlaxAutoModelForMaskedLM"},{local:"transformers.FlaxAutoModelForSeq2SeqLM",title:"FlaxAutoModelForSeq2SeqLM"},{local:"transformers.FlaxAutoModelForSequenceClassification",title:"FlaxAutoModelForSequenceClassification"},{local:"transformers.FlaxAutoModelForQuestionAnswering",title:"FlaxAutoModelForQuestionAnswering"},{local:"transformers.FlaxAutoModelForTokenClassification",title:"FlaxAutoModelForTokenClassification"},{local:"transformers.FlaxAutoModelForMultipleChoice",title:"FlaxAutoModelForMultipleChoice"},{local:"transformers.FlaxAutoModelForNextSentencePrediction",title:"FlaxAutoModelForNextSentencePrediction"},{local:"transformers.FlaxAutoModelForImageClassification",title:"FlaxAutoModelForImageClassification"},{local:"transformers.FlaxAutoModelForVision2Seq",title:"FlaxAutoModelForVision2Seq"}],title:"Auto Classes"};function _ut(yi,J,Le){let{fw:de}=J;return yi.$$set=me=>{"fw"in me&&Le(0,de=me.fw)},[de]}class Mut extends lut{constructor(J){super();iut(this,J,_ut,hut,dut,{fw:0})}}export{Mut as default,put as metadata};
