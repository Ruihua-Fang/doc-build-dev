import{S as by,i as yy,s as $y,e as r,k as l,w as k,t,M as Ey,c as a,d as n,m as d,a as i,x as w,h as o,b as c,F as e,g as h,y as b,q as y,o as $,B as E}from"../../chunks/vendor-4833417e.js";import{T as ze}from"../../chunks/Tip-fffd6df1.js";import{D as X}from"../../chunks/Docstring-7b52c3d4.js";import{C as Oe}from"../../chunks/CodeBlock-6a3d1b46.js";import{I as qe}from"../../chunks/IconCopyLink-4b81c553.js";import"../../chunks/CopyButton-dacfbfaf.js";function My(W){let p,M,m,g,v;return{c(){p=r("p"),M=t("Although the recipe for forward pass needs to be defined within this function, one should call the "),m=r("code"),g=t("Module"),v=t(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(F){p=a(F,"P",{});var _=i(p);M=o(_,"Although the recipe for forward pass needs to be defined within this function, one should call the "),m=a(_,"CODE",{});var z=i(m);g=o(z,"Module"),z.forEach(n),v=o(_,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),_.forEach(n)},m(F,_){h(F,p,_),e(p,M),e(p,m),e(m,g),e(p,v)},d(F){F&&n(p)}}}function zy(W){let p,M,m,g,v;return{c(){p=r("p"),M=t("Although the recipe for forward pass needs to be defined within this function, one should call the "),m=r("code"),g=t("Module"),v=t(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(F){p=a(F,"P",{});var _=i(p);M=o(_,"Although the recipe for forward pass needs to be defined within this function, one should call the "),m=a(_,"CODE",{});var z=i(m);g=o(z,"Module"),z.forEach(n),v=o(_,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),_.forEach(n)},m(F,_){h(F,p,_),e(p,M),e(p,m),e(m,g),e(p,v)},d(F){F&&n(p)}}}function qy(W){let p,M,m,g,v;return{c(){p=r("p"),M=t("Although the recipe for forward pass needs to be defined within this function, one should call the "),m=r("code"),g=t("Module"),v=t(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(F){p=a(F,"P",{});var _=i(p);M=o(_,"Although the recipe for forward pass needs to be defined within this function, one should call the "),m=a(_,"CODE",{});var z=i(m);g=o(z,"Module"),z.forEach(n),v=o(_,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),_.forEach(n)},m(F,_){h(F,p,_),e(p,M),e(p,m),e(m,g),e(p,v)},d(F){F&&n(p)}}}function Py(W){let p,M,m,g,v;return{c(){p=r("p"),M=t("Although the recipe for forward pass needs to be defined within this function, one should call the "),m=r("code"),g=t("Module"),v=t(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(F){p=a(F,"P",{});var _=i(p);M=o(_,"Although the recipe for forward pass needs to be defined within this function, one should call the "),m=a(_,"CODE",{});var z=i(m);g=o(z,"Module"),z.forEach(n),v=o(_,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),_.forEach(n)},m(F,_){h(F,p,_),e(p,M),e(p,m),e(m,g),e(p,v)},d(F){F&&n(p)}}}function Cy(W){let p,M,m,g,v;return{c(){p=r("p"),M=t("Although the recipe for forward pass needs to be defined within this function, one should call the "),m=r("code"),g=t("Module"),v=t(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(F){p=a(F,"P",{});var _=i(p);M=o(_,"Although the recipe for forward pass needs to be defined within this function, one should call the "),m=a(_,"CODE",{});var z=i(m);g=o(z,"Module"),z.forEach(n),v=o(_,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),_.forEach(n)},m(F,_){h(F,p,_),e(p,M),e(p,m),e(m,g),e(p,v)},d(F){F&&n(p)}}}function xy(W){let p,M,m,g,v;return{c(){p=r("p"),M=t("Although the recipe for forward pass needs to be defined within this function, one should call the "),m=r("code"),g=t("Module"),v=t(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(F){p=a(F,"P",{});var _=i(p);M=o(_,"Although the recipe for forward pass needs to be defined within this function, one should call the "),m=a(_,"CODE",{});var z=i(m);g=o(z,"Module"),z.forEach(n),v=o(_,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),_.forEach(n)},m(F,_){h(F,p,_),e(p,M),e(p,m),e(m,g),e(p,v)},d(F){F&&n(p)}}}function jy(W){let p,M,m,g,v;return{c(){p=r("p"),M=t("Although the recipe for forward pass needs to be defined within this function, one should call the "),m=r("code"),g=t("Module"),v=t(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(F){p=a(F,"P",{});var _=i(p);M=o(_,"Although the recipe for forward pass needs to be defined within this function, one should call the "),m=a(_,"CODE",{});var z=i(m);g=o(z,"Module"),z.forEach(n),v=o(_,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),_.forEach(n)},m(F,_){h(F,p,_),e(p,M),e(p,m),e(m,g),e(p,v)},d(F){F&&n(p)}}}function Ly(W){let p,M,m,g,v;return{c(){p=r("p"),M=t("Although the recipe for forward pass needs to be defined within this function, one should call the "),m=r("code"),g=t("Module"),v=t(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(F){p=a(F,"P",{});var _=i(p);M=o(_,"Although the recipe for forward pass needs to be defined within this function, one should call the "),m=a(_,"CODE",{});var z=i(m);g=o(z,"Module"),z.forEach(n),v=o(_,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),_.forEach(n)},m(F,_){h(F,p,_),e(p,M),e(p,m),e(m,g),e(p,v)},d(F){F&&n(p)}}}function Ay(W){let p,M,m,g,v,F,_,z,ce,G,q,J,I,ne,ue,S,pe,ie,U,L,te,Z,P,x,oe,Q,le,se,N,he,de,C,fe,B,ee,ae,R,me,O,A,re,H,ge;return{c(){p=r("p"),M=t("TF 2.0 models accepts two formats as inputs:"),m=l(),g=r("ul"),v=r("li"),F=t("having all inputs as keyword arguments (like PyTorch models), or"),_=l(),z=r("li"),ce=t("having all inputs as a list, tuple or dict in the first positional arguments."),G=l(),q=r("p"),J=t("This second option is useful when using "),I=r("code"),ne=t("tf.keras.Model.fit"),ue=t(` method which currently requires having all the
tensors in the first argument of the model call function: `),S=r("code"),pe=t("model(inputs)"),ie=t("."),U=l(),L=r("p"),te=t(`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),Z=l(),P=r("ul"),x=r("li"),oe=t("a single Tensor with "),Q=r("code"),le=t("input_ids"),se=t(" only and nothing else: "),N=r("code"),he=t("model(inputs_ids)"),de=l(),C=r("li"),fe=t(`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),B=r("code"),ee=t("model([input_ids, attention_mask])"),ae=t(" or "),R=r("code"),me=t("model([input_ids, attention_mask, token_type_ids])"),O=l(),A=r("li"),re=t(`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),H=r("code"),ge=t('model({"input_ids": input_ids, "token_type_ids": token_type_ids})')},l(u){p=a(u,"P",{});var T=i(p);M=o(T,"TF 2.0 models accepts two formats as inputs:"),T.forEach(n),m=d(u),g=a(u,"UL",{});var K=i(g);v=a(K,"LI",{});var Fe=i(v);F=o(Fe,"having all inputs as keyword arguments (like PyTorch models), or"),Fe.forEach(n),_=d(K),z=a(K,"LI",{});var we=i(z);ce=o(we,"having all inputs as a list, tuple or dict in the first positional arguments."),we.forEach(n),K.forEach(n),G=d(u),q=a(u,"P",{});var D=i(q);J=o(D,"This second option is useful when using "),I=a(D,"CODE",{});var ve=i(I);ne=o(ve,"tf.keras.Model.fit"),ve.forEach(n),ue=o(D,` method which currently requires having all the
tensors in the first argument of the model call function: `),S=a(D,"CODE",{});var be=i(S);pe=o(be,"model(inputs)"),be.forEach(n),ie=o(D,"."),D.forEach(n),U=d(u),L=a(u,"P",{});var ye=i(L);te=o(ye,`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),ye.forEach(n),Z=d(u),P=a(u,"UL",{});var j=i(P);x=a(j,"LI",{});var V=i(x);oe=o(V,"a single Tensor with "),Q=a(V,"CODE",{});var $e=i(Q);le=o($e,"input_ids"),$e.forEach(n),se=o(V," only and nothing else: "),N=a(V,"CODE",{});var Te=i(N);he=o(Te,"model(inputs_ids)"),Te.forEach(n),V.forEach(n),de=d(j),C=a(j,"LI",{});var Y=i(C);fe=o(Y,`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),B=a(Y,"CODE",{});var Ee=i(B);ee=o(Ee,"model([input_ids, attention_mask])"),Ee.forEach(n),ae=o(Y," or "),R=a(Y,"CODE",{});var ke=i(R);me=o(ke,"model([input_ids, attention_mask, token_type_ids])"),ke.forEach(n),Y.forEach(n),O=d(j),A=a(j,"LI",{});var _e=i(A);re=o(_e,`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),H=a(_e,"CODE",{});var Me=i(H);ge=o(Me,'model({"input_ids": input_ids, "token_type_ids": token_type_ids})'),Me.forEach(n),_e.forEach(n),j.forEach(n)},m(u,T){h(u,p,T),e(p,M),h(u,m,T),h(u,g,T),e(g,v),e(v,F),e(g,_),e(g,z),e(z,ce),h(u,G,T),h(u,q,T),e(q,J),e(q,I),e(I,ne),e(q,ue),e(q,S),e(S,pe),e(q,ie),h(u,U,T),h(u,L,T),e(L,te),h(u,Z,T),h(u,P,T),e(P,x),e(x,oe),e(x,Q),e(Q,le),e(x,se),e(x,N),e(N,he),e(P,de),e(P,C),e(C,fe),e(C,B),e(B,ee),e(C,ae),e(C,R),e(R,me),e(P,O),e(P,A),e(A,re),e(A,H),e(H,ge)},d(u){u&&n(p),u&&n(m),u&&n(g),u&&n(G),u&&n(q),u&&n(U),u&&n(L),u&&n(Z),u&&n(P)}}}function Dy(W){let p,M,m,g,v;return{c(){p=r("p"),M=t("Although the recipe for forward pass needs to be defined within this function, one should call the "),m=r("code"),g=t("Module"),v=t(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(F){p=a(F,"P",{});var _=i(p);M=o(_,"Although the recipe for forward pass needs to be defined within this function, one should call the "),m=a(_,"CODE",{});var z=i(m);g=o(z,"Module"),z.forEach(n),v=o(_,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),_.forEach(n)},m(F,_){h(F,p,_),e(p,M),e(p,m),e(m,g),e(p,v)},d(F){F&&n(p)}}}function Iy(W){let p,M,m,g,v,F,_,z,ce,G,q,J,I,ne,ue,S,pe,ie,U,L,te,Z,P,x,oe,Q,le,se,N,he,de,C,fe,B,ee,ae,R,me,O,A,re,H,ge;return{c(){p=r("p"),M=t("TF 2.0 models accepts two formats as inputs:"),m=l(),g=r("ul"),v=r("li"),F=t("having all inputs as keyword arguments (like PyTorch models), or"),_=l(),z=r("li"),ce=t("having all inputs as a list, tuple or dict in the first positional arguments."),G=l(),q=r("p"),J=t("This second option is useful when using "),I=r("code"),ne=t("tf.keras.Model.fit"),ue=t(` method which currently requires having all the
tensors in the first argument of the model call function: `),S=r("code"),pe=t("model(inputs)"),ie=t("."),U=l(),L=r("p"),te=t(`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),Z=l(),P=r("ul"),x=r("li"),oe=t("a single Tensor with "),Q=r("code"),le=t("input_ids"),se=t(" only and nothing else: "),N=r("code"),he=t("model(inputs_ids)"),de=l(),C=r("li"),fe=t(`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),B=r("code"),ee=t("model([input_ids, attention_mask])"),ae=t(" or "),R=r("code"),me=t("model([input_ids, attention_mask, token_type_ids])"),O=l(),A=r("li"),re=t(`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),H=r("code"),ge=t('model({"input_ids": input_ids, "token_type_ids": token_type_ids})')},l(u){p=a(u,"P",{});var T=i(p);M=o(T,"TF 2.0 models accepts two formats as inputs:"),T.forEach(n),m=d(u),g=a(u,"UL",{});var K=i(g);v=a(K,"LI",{});var Fe=i(v);F=o(Fe,"having all inputs as keyword arguments (like PyTorch models), or"),Fe.forEach(n),_=d(K),z=a(K,"LI",{});var we=i(z);ce=o(we,"having all inputs as a list, tuple or dict in the first positional arguments."),we.forEach(n),K.forEach(n),G=d(u),q=a(u,"P",{});var D=i(q);J=o(D,"This second option is useful when using "),I=a(D,"CODE",{});var ve=i(I);ne=o(ve,"tf.keras.Model.fit"),ve.forEach(n),ue=o(D,` method which currently requires having all the
tensors in the first argument of the model call function: `),S=a(D,"CODE",{});var be=i(S);pe=o(be,"model(inputs)"),be.forEach(n),ie=o(D,"."),D.forEach(n),U=d(u),L=a(u,"P",{});var ye=i(L);te=o(ye,`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),ye.forEach(n),Z=d(u),P=a(u,"UL",{});var j=i(P);x=a(j,"LI",{});var V=i(x);oe=o(V,"a single Tensor with "),Q=a(V,"CODE",{});var $e=i(Q);le=o($e,"input_ids"),$e.forEach(n),se=o(V," only and nothing else: "),N=a(V,"CODE",{});var Te=i(N);he=o(Te,"model(inputs_ids)"),Te.forEach(n),V.forEach(n),de=d(j),C=a(j,"LI",{});var Y=i(C);fe=o(Y,`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),B=a(Y,"CODE",{});var Ee=i(B);ee=o(Ee,"model([input_ids, attention_mask])"),Ee.forEach(n),ae=o(Y," or "),R=a(Y,"CODE",{});var ke=i(R);me=o(ke,"model([input_ids, attention_mask, token_type_ids])"),ke.forEach(n),Y.forEach(n),O=d(j),A=a(j,"LI",{});var _e=i(A);re=o(_e,`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),H=a(_e,"CODE",{});var Me=i(H);ge=o(Me,'model({"input_ids": input_ids, "token_type_ids": token_type_ids})'),Me.forEach(n),_e.forEach(n),j.forEach(n)},m(u,T){h(u,p,T),e(p,M),h(u,m,T),h(u,g,T),e(g,v),e(v,F),e(g,_),e(g,z),e(z,ce),h(u,G,T),h(u,q,T),e(q,J),e(q,I),e(I,ne),e(q,ue),e(q,S),e(S,pe),e(q,ie),h(u,U,T),h(u,L,T),e(L,te),h(u,Z,T),h(u,P,T),e(P,x),e(x,oe),e(x,Q),e(Q,le),e(x,se),e(x,N),e(N,he),e(P,de),e(P,C),e(C,fe),e(C,B),e(B,ee),e(C,ae),e(C,R),e(R,me),e(P,O),e(P,A),e(A,re),e(A,H),e(H,ge)},d(u){u&&n(p),u&&n(m),u&&n(g),u&&n(G),u&&n(q),u&&n(U),u&&n(L),u&&n(Z),u&&n(P)}}}function Sy(W){let p,M,m,g,v;return{c(){p=r("p"),M=t("Although the recipe for forward pass needs to be defined within this function, one should call the "),m=r("code"),g=t("Module"),v=t(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(F){p=a(F,"P",{});var _=i(p);M=o(_,"Although the recipe for forward pass needs to be defined within this function, one should call the "),m=a(_,"CODE",{});var z=i(m);g=o(z,"Module"),z.forEach(n),v=o(_,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),_.forEach(n)},m(F,_){h(F,p,_),e(p,M),e(p,m),e(m,g),e(p,v)},d(F){F&&n(p)}}}function Ny(W){let p,M,m,g,v,F,_,z,ce,G,q,J,I,ne,ue,S,pe,ie,U,L,te,Z,P,x,oe,Q,le,se,N,he,de,C,fe,B,ee,ae,R,me,O,A,re,H,ge;return{c(){p=r("p"),M=t("TF 2.0 models accepts two formats as inputs:"),m=l(),g=r("ul"),v=r("li"),F=t("having all inputs as keyword arguments (like PyTorch models), or"),_=l(),z=r("li"),ce=t("having all inputs as a list, tuple or dict in the first positional arguments."),G=l(),q=r("p"),J=t("This second option is useful when using "),I=r("code"),ne=t("tf.keras.Model.fit"),ue=t(` method which currently requires having all the
tensors in the first argument of the model call function: `),S=r("code"),pe=t("model(inputs)"),ie=t("."),U=l(),L=r("p"),te=t(`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),Z=l(),P=r("ul"),x=r("li"),oe=t("a single Tensor with "),Q=r("code"),le=t("input_ids"),se=t(" only and nothing else: "),N=r("code"),he=t("model(inputs_ids)"),de=l(),C=r("li"),fe=t(`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),B=r("code"),ee=t("model([input_ids, attention_mask])"),ae=t(" or "),R=r("code"),me=t("model([input_ids, attention_mask, token_type_ids])"),O=l(),A=r("li"),re=t(`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),H=r("code"),ge=t('model({"input_ids": input_ids, "token_type_ids": token_type_ids})')},l(u){p=a(u,"P",{});var T=i(p);M=o(T,"TF 2.0 models accepts two formats as inputs:"),T.forEach(n),m=d(u),g=a(u,"UL",{});var K=i(g);v=a(K,"LI",{});var Fe=i(v);F=o(Fe,"having all inputs as keyword arguments (like PyTorch models), or"),Fe.forEach(n),_=d(K),z=a(K,"LI",{});var we=i(z);ce=o(we,"having all inputs as a list, tuple or dict in the first positional arguments."),we.forEach(n),K.forEach(n),G=d(u),q=a(u,"P",{});var D=i(q);J=o(D,"This second option is useful when using "),I=a(D,"CODE",{});var ve=i(I);ne=o(ve,"tf.keras.Model.fit"),ve.forEach(n),ue=o(D,` method which currently requires having all the
tensors in the first argument of the model call function: `),S=a(D,"CODE",{});var be=i(S);pe=o(be,"model(inputs)"),be.forEach(n),ie=o(D,"."),D.forEach(n),U=d(u),L=a(u,"P",{});var ye=i(L);te=o(ye,`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),ye.forEach(n),Z=d(u),P=a(u,"UL",{});var j=i(P);x=a(j,"LI",{});var V=i(x);oe=o(V,"a single Tensor with "),Q=a(V,"CODE",{});var $e=i(Q);le=o($e,"input_ids"),$e.forEach(n),se=o(V," only and nothing else: "),N=a(V,"CODE",{});var Te=i(N);he=o(Te,"model(inputs_ids)"),Te.forEach(n),V.forEach(n),de=d(j),C=a(j,"LI",{});var Y=i(C);fe=o(Y,`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),B=a(Y,"CODE",{});var Ee=i(B);ee=o(Ee,"model([input_ids, attention_mask])"),Ee.forEach(n),ae=o(Y," or "),R=a(Y,"CODE",{});var ke=i(R);me=o(ke,"model([input_ids, attention_mask, token_type_ids])"),ke.forEach(n),Y.forEach(n),O=d(j),A=a(j,"LI",{});var _e=i(A);re=o(_e,`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),H=a(_e,"CODE",{});var Me=i(H);ge=o(Me,'model({"input_ids": input_ids, "token_type_ids": token_type_ids})'),Me.forEach(n),_e.forEach(n),j.forEach(n)},m(u,T){h(u,p,T),e(p,M),h(u,m,T),h(u,g,T),e(g,v),e(v,F),e(g,_),e(g,z),e(z,ce),h(u,G,T),h(u,q,T),e(q,J),e(q,I),e(I,ne),e(q,ue),e(q,S),e(S,pe),e(q,ie),h(u,U,T),h(u,L,T),e(L,te),h(u,Z,T),h(u,P,T),e(P,x),e(x,oe),e(x,Q),e(Q,le),e(x,se),e(x,N),e(N,he),e(P,de),e(P,C),e(C,fe),e(C,B),e(B,ee),e(C,ae),e(C,R),e(R,me),e(P,O),e(P,A),e(A,re),e(A,H),e(H,ge)},d(u){u&&n(p),u&&n(m),u&&n(g),u&&n(G),u&&n(q),u&&n(U),u&&n(L),u&&n(Z),u&&n(P)}}}function Oy(W){let p,M,m,g,v;return{c(){p=r("p"),M=t("Although the recipe for forward pass needs to be defined within this function, one should call the "),m=r("code"),g=t("Module"),v=t(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(F){p=a(F,"P",{});var _=i(p);M=o(_,"Although the recipe for forward pass needs to be defined within this function, one should call the "),m=a(_,"CODE",{});var z=i(m);g=o(z,"Module"),z.forEach(n),v=o(_,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),_.forEach(n)},m(F,_){h(F,p,_),e(p,M),e(p,m),e(m,g),e(p,v)},d(F){F&&n(p)}}}function By(W){let p,M,m,g,v,F,_,z,ce,G,q,J,I,ne,ue,S,pe,ie,U,L,te,Z,P,x,oe,Q,le,se,N,he,de,C,fe,B,ee,ae,R,me,O,A,re,H,ge;return{c(){p=r("p"),M=t("TF 2.0 models accepts two formats as inputs:"),m=l(),g=r("ul"),v=r("li"),F=t("having all inputs as keyword arguments (like PyTorch models), or"),_=l(),z=r("li"),ce=t("having all inputs as a list, tuple or dict in the first positional arguments."),G=l(),q=r("p"),J=t("This second option is useful when using "),I=r("code"),ne=t("tf.keras.Model.fit"),ue=t(` method which currently requires having all the
tensors in the first argument of the model call function: `),S=r("code"),pe=t("model(inputs)"),ie=t("."),U=l(),L=r("p"),te=t(`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),Z=l(),P=r("ul"),x=r("li"),oe=t("a single Tensor with "),Q=r("code"),le=t("input_ids"),se=t(" only and nothing else: "),N=r("code"),he=t("model(inputs_ids)"),de=l(),C=r("li"),fe=t(`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),B=r("code"),ee=t("model([input_ids, attention_mask])"),ae=t(" or "),R=r("code"),me=t("model([input_ids, attention_mask, token_type_ids])"),O=l(),A=r("li"),re=t(`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),H=r("code"),ge=t('model({"input_ids": input_ids, "token_type_ids": token_type_ids})')},l(u){p=a(u,"P",{});var T=i(p);M=o(T,"TF 2.0 models accepts two formats as inputs:"),T.forEach(n),m=d(u),g=a(u,"UL",{});var K=i(g);v=a(K,"LI",{});var Fe=i(v);F=o(Fe,"having all inputs as keyword arguments (like PyTorch models), or"),Fe.forEach(n),_=d(K),z=a(K,"LI",{});var we=i(z);ce=o(we,"having all inputs as a list, tuple or dict in the first positional arguments."),we.forEach(n),K.forEach(n),G=d(u),q=a(u,"P",{});var D=i(q);J=o(D,"This second option is useful when using "),I=a(D,"CODE",{});var ve=i(I);ne=o(ve,"tf.keras.Model.fit"),ve.forEach(n),ue=o(D,` method which currently requires having all the
tensors in the first argument of the model call function: `),S=a(D,"CODE",{});var be=i(S);pe=o(be,"model(inputs)"),be.forEach(n),ie=o(D,"."),D.forEach(n),U=d(u),L=a(u,"P",{});var ye=i(L);te=o(ye,`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),ye.forEach(n),Z=d(u),P=a(u,"UL",{});var j=i(P);x=a(j,"LI",{});var V=i(x);oe=o(V,"a single Tensor with "),Q=a(V,"CODE",{});var $e=i(Q);le=o($e,"input_ids"),$e.forEach(n),se=o(V," only and nothing else: "),N=a(V,"CODE",{});var Te=i(N);he=o(Te,"model(inputs_ids)"),Te.forEach(n),V.forEach(n),de=d(j),C=a(j,"LI",{});var Y=i(C);fe=o(Y,`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),B=a(Y,"CODE",{});var Ee=i(B);ee=o(Ee,"model([input_ids, attention_mask])"),Ee.forEach(n),ae=o(Y," or "),R=a(Y,"CODE",{});var ke=i(R);me=o(ke,"model([input_ids, attention_mask, token_type_ids])"),ke.forEach(n),Y.forEach(n),O=d(j),A=a(j,"LI",{});var _e=i(A);re=o(_e,`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),H=a(_e,"CODE",{});var Me=i(H);ge=o(Me,'model({"input_ids": input_ids, "token_type_ids": token_type_ids})'),Me.forEach(n),_e.forEach(n),j.forEach(n)},m(u,T){h(u,p,T),e(p,M),h(u,m,T),h(u,g,T),e(g,v),e(v,F),e(g,_),e(g,z),e(z,ce),h(u,G,T),h(u,q,T),e(q,J),e(q,I),e(I,ne),e(q,ue),e(q,S),e(S,pe),e(q,ie),h(u,U,T),h(u,L,T),e(L,te),h(u,Z,T),h(u,P,T),e(P,x),e(x,oe),e(x,Q),e(Q,le),e(x,se),e(x,N),e(N,he),e(P,de),e(P,C),e(C,fe),e(C,B),e(B,ee),e(C,ae),e(C,R),e(R,me),e(P,O),e(P,A),e(A,re),e(A,H),e(H,ge)},d(u){u&&n(p),u&&n(m),u&&n(g),u&&n(G),u&&n(q),u&&n(U),u&&n(L),u&&n(Z),u&&n(P)}}}function Wy(W){let p,M,m,g,v;return{c(){p=r("p"),M=t("Although the recipe for forward pass needs to be defined within this function, one should call the "),m=r("code"),g=t("Module"),v=t(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(F){p=a(F,"P",{});var _=i(p);M=o(_,"Although the recipe for forward pass needs to be defined within this function, one should call the "),m=a(_,"CODE",{});var z=i(m);g=o(z,"Module"),z.forEach(n),v=o(_,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),_.forEach(n)},m(F,_){h(F,p,_),e(p,M),e(p,m),e(m,g),e(p,v)},d(F){F&&n(p)}}}function Qy(W){let p,M,m,g,v,F,_,z,ce,G,q,J,I,ne,ue,S,pe,ie,U,L,te,Z,P,x,oe,Q,le,se,N,he,de,C,fe,B,ee,ae,R,me,O,A,re,H,ge;return{c(){p=r("p"),M=t("TF 2.0 models accepts two formats as inputs:"),m=l(),g=r("ul"),v=r("li"),F=t("having all inputs as keyword arguments (like PyTorch models), or"),_=l(),z=r("li"),ce=t("having all inputs as a list, tuple or dict in the first positional arguments."),G=l(),q=r("p"),J=t("This second option is useful when using "),I=r("code"),ne=t("tf.keras.Model.fit"),ue=t(` method which currently requires having all the
tensors in the first argument of the model call function: `),S=r("code"),pe=t("model(inputs)"),ie=t("."),U=l(),L=r("p"),te=t(`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),Z=l(),P=r("ul"),x=r("li"),oe=t("a single Tensor with "),Q=r("code"),le=t("input_ids"),se=t(" only and nothing else: "),N=r("code"),he=t("model(inputs_ids)"),de=l(),C=r("li"),fe=t(`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),B=r("code"),ee=t("model([input_ids, attention_mask])"),ae=t(" or "),R=r("code"),me=t("model([input_ids, attention_mask, token_type_ids])"),O=l(),A=r("li"),re=t(`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),H=r("code"),ge=t('model({"input_ids": input_ids, "token_type_ids": token_type_ids})')},l(u){p=a(u,"P",{});var T=i(p);M=o(T,"TF 2.0 models accepts two formats as inputs:"),T.forEach(n),m=d(u),g=a(u,"UL",{});var K=i(g);v=a(K,"LI",{});var Fe=i(v);F=o(Fe,"having all inputs as keyword arguments (like PyTorch models), or"),Fe.forEach(n),_=d(K),z=a(K,"LI",{});var we=i(z);ce=o(we,"having all inputs as a list, tuple or dict in the first positional arguments."),we.forEach(n),K.forEach(n),G=d(u),q=a(u,"P",{});var D=i(q);J=o(D,"This second option is useful when using "),I=a(D,"CODE",{});var ve=i(I);ne=o(ve,"tf.keras.Model.fit"),ve.forEach(n),ue=o(D,` method which currently requires having all the
tensors in the first argument of the model call function: `),S=a(D,"CODE",{});var be=i(S);pe=o(be,"model(inputs)"),be.forEach(n),ie=o(D,"."),D.forEach(n),U=d(u),L=a(u,"P",{});var ye=i(L);te=o(ye,`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),ye.forEach(n),Z=d(u),P=a(u,"UL",{});var j=i(P);x=a(j,"LI",{});var V=i(x);oe=o(V,"a single Tensor with "),Q=a(V,"CODE",{});var $e=i(Q);le=o($e,"input_ids"),$e.forEach(n),se=o(V," only and nothing else: "),N=a(V,"CODE",{});var Te=i(N);he=o(Te,"model(inputs_ids)"),Te.forEach(n),V.forEach(n),de=d(j),C=a(j,"LI",{});var Y=i(C);fe=o(Y,`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),B=a(Y,"CODE",{});var Ee=i(B);ee=o(Ee,"model([input_ids, attention_mask])"),Ee.forEach(n),ae=o(Y," or "),R=a(Y,"CODE",{});var ke=i(R);me=o(ke,"model([input_ids, attention_mask, token_type_ids])"),ke.forEach(n),Y.forEach(n),O=d(j),A=a(j,"LI",{});var _e=i(A);re=o(_e,`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),H=a(_e,"CODE",{});var Me=i(H);ge=o(Me,'model({"input_ids": input_ids, "token_type_ids": token_type_ids})'),Me.forEach(n),_e.forEach(n),j.forEach(n)},m(u,T){h(u,p,T),e(p,M),h(u,m,T),h(u,g,T),e(g,v),e(v,F),e(g,_),e(g,z),e(z,ce),h(u,G,T),h(u,q,T),e(q,J),e(q,I),e(I,ne),e(q,ue),e(q,S),e(S,pe),e(q,ie),h(u,U,T),h(u,L,T),e(L,te),h(u,Z,T),h(u,P,T),e(P,x),e(x,oe),e(x,Q),e(Q,le),e(x,se),e(x,N),e(N,he),e(P,de),e(P,C),e(C,fe),e(C,B),e(B,ee),e(C,ae),e(C,R),e(R,me),e(P,O),e(P,A),e(A,re),e(A,H),e(H,ge)},d(u){u&&n(p),u&&n(m),u&&n(g),u&&n(G),u&&n(q),u&&n(U),u&&n(L),u&&n(Z),u&&n(P)}}}function Ry(W){let p,M,m,g,v;return{c(){p=r("p"),M=t("Although the recipe for forward pass needs to be defined within this function, one should call the "),m=r("code"),g=t("Module"),v=t(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(F){p=a(F,"P",{});var _=i(p);M=o(_,"Although the recipe for forward pass needs to be defined within this function, one should call the "),m=a(_,"CODE",{});var z=i(m);g=o(z,"Module"),z.forEach(n),v=o(_,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),_.forEach(n)},m(F,_){h(F,p,_),e(p,M),e(p,m),e(m,g),e(p,v)},d(F){F&&n(p)}}}function Hy(W){let p,M,m,g,v,F,_,z,ce,G,q,J,I,ne,ue,S,pe,ie,U,L,te,Z,P,x,oe,Q,le,se,N,he,de,C,fe,B,ee,ae,R,me,O,A,re,H,ge;return{c(){p=r("p"),M=t("TF 2.0 models accepts two formats as inputs:"),m=l(),g=r("ul"),v=r("li"),F=t("having all inputs as keyword arguments (like PyTorch models), or"),_=l(),z=r("li"),ce=t("having all inputs as a list, tuple or dict in the first positional arguments."),G=l(),q=r("p"),J=t("This second option is useful when using "),I=r("code"),ne=t("tf.keras.Model.fit"),ue=t(` method which currently requires having all the
tensors in the first argument of the model call function: `),S=r("code"),pe=t("model(inputs)"),ie=t("."),U=l(),L=r("p"),te=t(`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),Z=l(),P=r("ul"),x=r("li"),oe=t("a single Tensor with "),Q=r("code"),le=t("input_ids"),se=t(" only and nothing else: "),N=r("code"),he=t("model(inputs_ids)"),de=l(),C=r("li"),fe=t(`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),B=r("code"),ee=t("model([input_ids, attention_mask])"),ae=t(" or "),R=r("code"),me=t("model([input_ids, attention_mask, token_type_ids])"),O=l(),A=r("li"),re=t(`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),H=r("code"),ge=t('model({"input_ids": input_ids, "token_type_ids": token_type_ids})')},l(u){p=a(u,"P",{});var T=i(p);M=o(T,"TF 2.0 models accepts two formats as inputs:"),T.forEach(n),m=d(u),g=a(u,"UL",{});var K=i(g);v=a(K,"LI",{});var Fe=i(v);F=o(Fe,"having all inputs as keyword arguments (like PyTorch models), or"),Fe.forEach(n),_=d(K),z=a(K,"LI",{});var we=i(z);ce=o(we,"having all inputs as a list, tuple or dict in the first positional arguments."),we.forEach(n),K.forEach(n),G=d(u),q=a(u,"P",{});var D=i(q);J=o(D,"This second option is useful when using "),I=a(D,"CODE",{});var ve=i(I);ne=o(ve,"tf.keras.Model.fit"),ve.forEach(n),ue=o(D,` method which currently requires having all the
tensors in the first argument of the model call function: `),S=a(D,"CODE",{});var be=i(S);pe=o(be,"model(inputs)"),be.forEach(n),ie=o(D,"."),D.forEach(n),U=d(u),L=a(u,"P",{});var ye=i(L);te=o(ye,`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),ye.forEach(n),Z=d(u),P=a(u,"UL",{});var j=i(P);x=a(j,"LI",{});var V=i(x);oe=o(V,"a single Tensor with "),Q=a(V,"CODE",{});var $e=i(Q);le=o($e,"input_ids"),$e.forEach(n),se=o(V," only and nothing else: "),N=a(V,"CODE",{});var Te=i(N);he=o(Te,"model(inputs_ids)"),Te.forEach(n),V.forEach(n),de=d(j),C=a(j,"LI",{});var Y=i(C);fe=o(Y,`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),B=a(Y,"CODE",{});var Ee=i(B);ee=o(Ee,"model([input_ids, attention_mask])"),Ee.forEach(n),ae=o(Y," or "),R=a(Y,"CODE",{});var ke=i(R);me=o(ke,"model([input_ids, attention_mask, token_type_ids])"),ke.forEach(n),Y.forEach(n),O=d(j),A=a(j,"LI",{});var _e=i(A);re=o(_e,`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),H=a(_e,"CODE",{});var Me=i(H);ge=o(Me,'model({"input_ids": input_ids, "token_type_ids": token_type_ids})'),Me.forEach(n),_e.forEach(n),j.forEach(n)},m(u,T){h(u,p,T),e(p,M),h(u,m,T),h(u,g,T),e(g,v),e(v,F),e(g,_),e(g,z),e(z,ce),h(u,G,T),h(u,q,T),e(q,J),e(q,I),e(I,ne),e(q,ue),e(q,S),e(S,pe),e(q,ie),h(u,U,T),h(u,L,T),e(L,te),h(u,Z,T),h(u,P,T),e(P,x),e(x,oe),e(x,Q),e(Q,le),e(x,se),e(x,N),e(N,he),e(P,de),e(P,C),e(C,fe),e(C,B),e(B,ee),e(C,ae),e(C,R),e(R,me),e(P,O),e(P,A),e(A,re),e(A,H),e(H,ge)},d(u){u&&n(p),u&&n(m),u&&n(g),u&&n(G),u&&n(q),u&&n(U),u&&n(L),u&&n(Z),u&&n(P)}}}function Vy(W){let p,M,m,g,v;return{c(){p=r("p"),M=t("Although the recipe for forward pass needs to be defined within this function, one should call the "),m=r("code"),g=t("Module"),v=t(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(F){p=a(F,"P",{});var _=i(p);M=o(_,"Although the recipe for forward pass needs to be defined within this function, one should call the "),m=a(_,"CODE",{});var z=i(m);g=o(z,"Module"),z.forEach(n),v=o(_,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),_.forEach(n)},m(F,_){h(F,p,_),e(p,M),e(p,m),e(m,g),e(p,v)},d(F){F&&n(p)}}}function Yy(W){let p,M,m,g,v,F,_,z,ce,G,q,J,I,ne,ue,S,pe,ie,U,L,te,Z,P,x,oe,Q,le,se,N,he,de,C,fe,B,ee,ae,R,me,O,A,re,H,ge;return{c(){p=r("p"),M=t("TF 2.0 models accepts two formats as inputs:"),m=l(),g=r("ul"),v=r("li"),F=t("having all inputs as keyword arguments (like PyTorch models), or"),_=l(),z=r("li"),ce=t("having all inputs as a list, tuple or dict in the first positional arguments."),G=l(),q=r("p"),J=t("This second option is useful when using "),I=r("code"),ne=t("tf.keras.Model.fit"),ue=t(` method which currently requires having all the
tensors in the first argument of the model call function: `),S=r("code"),pe=t("model(inputs)"),ie=t("."),U=l(),L=r("p"),te=t(`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),Z=l(),P=r("ul"),x=r("li"),oe=t("a single Tensor with "),Q=r("code"),le=t("input_ids"),se=t(" only and nothing else: "),N=r("code"),he=t("model(inputs_ids)"),de=l(),C=r("li"),fe=t(`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),B=r("code"),ee=t("model([input_ids, attention_mask])"),ae=t(" or "),R=r("code"),me=t("model([input_ids, attention_mask, token_type_ids])"),O=l(),A=r("li"),re=t(`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),H=r("code"),ge=t('model({"input_ids": input_ids, "token_type_ids": token_type_ids})')},l(u){p=a(u,"P",{});var T=i(p);M=o(T,"TF 2.0 models accepts two formats as inputs:"),T.forEach(n),m=d(u),g=a(u,"UL",{});var K=i(g);v=a(K,"LI",{});var Fe=i(v);F=o(Fe,"having all inputs as keyword arguments (like PyTorch models), or"),Fe.forEach(n),_=d(K),z=a(K,"LI",{});var we=i(z);ce=o(we,"having all inputs as a list, tuple or dict in the first positional arguments."),we.forEach(n),K.forEach(n),G=d(u),q=a(u,"P",{});var D=i(q);J=o(D,"This second option is useful when using "),I=a(D,"CODE",{});var ve=i(I);ne=o(ve,"tf.keras.Model.fit"),ve.forEach(n),ue=o(D,` method which currently requires having all the
tensors in the first argument of the model call function: `),S=a(D,"CODE",{});var be=i(S);pe=o(be,"model(inputs)"),be.forEach(n),ie=o(D,"."),D.forEach(n),U=d(u),L=a(u,"P",{});var ye=i(L);te=o(ye,`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),ye.forEach(n),Z=d(u),P=a(u,"UL",{});var j=i(P);x=a(j,"LI",{});var V=i(x);oe=o(V,"a single Tensor with "),Q=a(V,"CODE",{});var $e=i(Q);le=o($e,"input_ids"),$e.forEach(n),se=o(V," only and nothing else: "),N=a(V,"CODE",{});var Te=i(N);he=o(Te,"model(inputs_ids)"),Te.forEach(n),V.forEach(n),de=d(j),C=a(j,"LI",{});var Y=i(C);fe=o(Y,`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),B=a(Y,"CODE",{});var Ee=i(B);ee=o(Ee,"model([input_ids, attention_mask])"),Ee.forEach(n),ae=o(Y," or "),R=a(Y,"CODE",{});var ke=i(R);me=o(ke,"model([input_ids, attention_mask, token_type_ids])"),ke.forEach(n),Y.forEach(n),O=d(j),A=a(j,"LI",{});var _e=i(A);re=o(_e,`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),H=a(_e,"CODE",{});var Me=i(H);ge=o(Me,'model({"input_ids": input_ids, "token_type_ids": token_type_ids})'),Me.forEach(n),_e.forEach(n),j.forEach(n)},m(u,T){h(u,p,T),e(p,M),h(u,m,T),h(u,g,T),e(g,v),e(v,F),e(g,_),e(g,z),e(z,ce),h(u,G,T),h(u,q,T),e(q,J),e(q,I),e(I,ne),e(q,ue),e(q,S),e(S,pe),e(q,ie),h(u,U,T),h(u,L,T),e(L,te),h(u,Z,T),h(u,P,T),e(P,x),e(x,oe),e(x,Q),e(Q,le),e(x,se),e(x,N),e(N,he),e(P,de),e(P,C),e(C,fe),e(C,B),e(B,ee),e(C,ae),e(C,R),e(R,me),e(P,O),e(P,A),e(A,re),e(A,H),e(H,ge)},d(u){u&&n(p),u&&n(m),u&&n(g),u&&n(G),u&&n(q),u&&n(U),u&&n(L),u&&n(Z),u&&n(P)}}}function Uy(W){let p,M,m,g,v;return{c(){p=r("p"),M=t("Although the recipe for forward pass needs to be defined within this function, one should call the "),m=r("code"),g=t("Module"),v=t(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(F){p=a(F,"P",{});var _=i(p);M=o(_,"Although the recipe for forward pass needs to be defined within this function, one should call the "),m=a(_,"CODE",{});var z=i(m);g=o(z,"Module"),z.forEach(n),v=o(_,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),_.forEach(n)},m(F,_){h(F,p,_),e(p,M),e(p,m),e(m,g),e(p,v)},d(F){F&&n(p)}}}function Gy(W){let p,M,m,g,v,F,_,z,ce,G,q,J,I,ne,ue,S,pe,ie,U,L,te,Z,P,x,oe,Q,le,se,N,he,de,C,fe,B,ee,ae,R,me,O,A,re,H,ge;return{c(){p=r("p"),M=t("TF 2.0 models accepts two formats as inputs:"),m=l(),g=r("ul"),v=r("li"),F=t("having all inputs as keyword arguments (like PyTorch models), or"),_=l(),z=r("li"),ce=t("having all inputs as a list, tuple or dict in the first positional arguments."),G=l(),q=r("p"),J=t("This second option is useful when using "),I=r("code"),ne=t("tf.keras.Model.fit"),ue=t(` method which currently requires having all the
tensors in the first argument of the model call function: `),S=r("code"),pe=t("model(inputs)"),ie=t("."),U=l(),L=r("p"),te=t(`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),Z=l(),P=r("ul"),x=r("li"),oe=t("a single Tensor with "),Q=r("code"),le=t("input_ids"),se=t(" only and nothing else: "),N=r("code"),he=t("model(inputs_ids)"),de=l(),C=r("li"),fe=t(`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),B=r("code"),ee=t("model([input_ids, attention_mask])"),ae=t(" or "),R=r("code"),me=t("model([input_ids, attention_mask, token_type_ids])"),O=l(),A=r("li"),re=t(`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),H=r("code"),ge=t('model({"input_ids": input_ids, "token_type_ids": token_type_ids})')},l(u){p=a(u,"P",{});var T=i(p);M=o(T,"TF 2.0 models accepts two formats as inputs:"),T.forEach(n),m=d(u),g=a(u,"UL",{});var K=i(g);v=a(K,"LI",{});var Fe=i(v);F=o(Fe,"having all inputs as keyword arguments (like PyTorch models), or"),Fe.forEach(n),_=d(K),z=a(K,"LI",{});var we=i(z);ce=o(we,"having all inputs as a list, tuple or dict in the first positional arguments."),we.forEach(n),K.forEach(n),G=d(u),q=a(u,"P",{});var D=i(q);J=o(D,"This second option is useful when using "),I=a(D,"CODE",{});var ve=i(I);ne=o(ve,"tf.keras.Model.fit"),ve.forEach(n),ue=o(D,` method which currently requires having all the
tensors in the first argument of the model call function: `),S=a(D,"CODE",{});var be=i(S);pe=o(be,"model(inputs)"),be.forEach(n),ie=o(D,"."),D.forEach(n),U=d(u),L=a(u,"P",{});var ye=i(L);te=o(ye,`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),ye.forEach(n),Z=d(u),P=a(u,"UL",{});var j=i(P);x=a(j,"LI",{});var V=i(x);oe=o(V,"a single Tensor with "),Q=a(V,"CODE",{});var $e=i(Q);le=o($e,"input_ids"),$e.forEach(n),se=o(V," only and nothing else: "),N=a(V,"CODE",{});var Te=i(N);he=o(Te,"model(inputs_ids)"),Te.forEach(n),V.forEach(n),de=d(j),C=a(j,"LI",{});var Y=i(C);fe=o(Y,`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),B=a(Y,"CODE",{});var Ee=i(B);ee=o(Ee,"model([input_ids, attention_mask])"),Ee.forEach(n),ae=o(Y," or "),R=a(Y,"CODE",{});var ke=i(R);me=o(ke,"model([input_ids, attention_mask, token_type_ids])"),ke.forEach(n),Y.forEach(n),O=d(j),A=a(j,"LI",{});var _e=i(A);re=o(_e,`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),H=a(_e,"CODE",{});var Me=i(H);ge=o(Me,'model({"input_ids": input_ids, "token_type_ids": token_type_ids})'),Me.forEach(n),_e.forEach(n),j.forEach(n)},m(u,T){h(u,p,T),e(p,M),h(u,m,T),h(u,g,T),e(g,v),e(v,F),e(g,_),e(g,z),e(z,ce),h(u,G,T),h(u,q,T),e(q,J),e(q,I),e(I,ne),e(q,ue),e(q,S),e(S,pe),e(q,ie),h(u,U,T),h(u,L,T),e(L,te),h(u,Z,T),h(u,P,T),e(P,x),e(x,oe),e(x,Q),e(Q,le),e(x,se),e(x,N),e(N,he),e(P,de),e(P,C),e(C,fe),e(C,B),e(B,ee),e(C,ae),e(C,R),e(R,me),e(P,O),e(P,A),e(A,re),e(A,H),e(H,ge)},d(u){u&&n(p),u&&n(m),u&&n(g),u&&n(G),u&&n(q),u&&n(U),u&&n(L),u&&n(Z),u&&n(P)}}}function Zy(W){let p,M,m,g,v;return{c(){p=r("p"),M=t("Although the recipe for forward pass needs to be defined within this function, one should call the "),m=r("code"),g=t("Module"),v=t(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(F){p=a(F,"P",{});var _=i(p);M=o(_,"Although the recipe for forward pass needs to be defined within this function, one should call the "),m=a(_,"CODE",{});var z=i(m);g=o(z,"Module"),z.forEach(n),v=o(_,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),_.forEach(n)},m(F,_){h(F,p,_),e(p,M),e(p,m),e(m,g),e(p,v)},d(F){F&&n(p)}}}function Ky(W){let p,M,m,g,v,F,_,z,ce,G,q,J,I,ne,ue,S,pe,ie,U,L,te,Z,P,x,oe,Q,le,se,N,he,de,C,fe,B,ee,ae,R,me,O,A,re,H,ge,u,T,K,Fe,we,D,ve,be,ye,j,V,$e,Te,Y,Ee,ke,_e,Me,Ua,Np,Op,Ec,jn,Bp,So,Wp,Qp,No,Rp,Hp,Mc,Kn,Bt,ul,Oo,Vp,pl,Yp,zc,Cn,Bo,Up,xn,Gp,Ga,Zp,Kp,Za,Xp,Jp,Wo,eh,nh,th,Xn,oh,Ka,sh,rh,Xa,ah,ih,qc,Jn,Wt,hl,Qo,lh,fl,dh,Pc,Pe,Ro,ch,ml,uh,ph,Qt,Ja,hh,fh,ei,mh,gh,_h,Ho,Fh,ni,vh,Th,kh,Ln,Vo,wh,gl,bh,yh,Yo,ti,$h,_l,Eh,Mh,oi,zh,Fl,qh,Ph,Rt,Uo,Ch,Go,xh,vl,jh,Lh,Ah,wn,Zo,Dh,Tl,Ih,Sh,Ko,Nh,et,Oh,kl,Bh,Wh,wl,Qh,Rh,Hh,si,Xo,Cc,nt,Ht,bl,Jo,Vh,yl,Yh,xc,Ze,es,Uh,ns,Gh,$l,Zh,Kh,Xh,Vt,ri,Jh,ef,ai,nf,tf,of,ts,sf,ii,rf,af,lf,bn,os,df,El,cf,uf,ss,pf,tt,hf,Ml,ff,mf,zl,gf,_f,jc,ot,Yt,ql,rs,Ff,Pl,vf,Lc,st,as,Tf,is,kf,li,wf,bf,Ac,rt,ls,yf,ds,$f,di,Ef,Mf,Dc,at,Ut,Cl,cs,zf,xl,qf,Ic,We,us,Pf,jl,Cf,xf,ps,jf,hs,Lf,Af,Df,fs,If,ci,Sf,Nf,Of,ms,Bf,gs,Wf,Qf,Rf,Ke,_s,Hf,it,Vf,ui,Yf,Uf,Ll,Gf,Zf,Kf,Gt,Xf,Al,Jf,em,Fs,Sc,lt,Zt,Dl,vs,nm,Il,tm,Nc,Qe,Ts,om,Sl,sm,rm,ks,am,ws,im,lm,dm,bs,cm,pi,um,pm,hm,ys,fm,$s,mm,gm,_m,Xe,Es,Fm,dt,vm,hi,Tm,km,Nl,wm,bm,ym,Kt,$m,Ol,Em,Mm,Ms,Oc,ct,Xt,Bl,zs,zm,Wl,qm,Bc,ut,qs,Pm,Je,Ps,Cm,pt,xm,fi,jm,Lm,Ql,Am,Dm,Im,Jt,Sm,Rl,Nm,Om,Cs,Wc,ht,eo,Hl,xs,Bm,Vl,Wm,Qc,Re,js,Qm,Ls,Rm,Yl,Hm,Vm,Ym,As,Um,Ds,Gm,Zm,Km,Is,Xm,mi,Jm,eg,ng,Ss,tg,Ns,og,sg,rg,en,Os,ag,ft,ig,gi,lg,dg,Ul,cg,ug,pg,no,hg,Gl,fg,mg,Bs,Rc,mt,to,Zl,Ws,gg,Kl,_g,Hc,He,Qs,Fg,Xl,vg,Tg,Rs,kg,Hs,wg,bg,yg,Vs,$g,_i,Eg,Mg,zg,Ys,qg,Us,Pg,Cg,xg,Be,Gs,jg,gt,Lg,Fi,Ag,Dg,Jl,Ig,Sg,Ng,oo,Og,ed,Bg,Wg,Zs,Qg,nd,Rg,Hg,Ks,Vc,_t,so,td,Xs,Vg,od,Yg,Yc,Ve,Js,Ug,sd,Gg,Zg,er,Kg,nr,Xg,Jg,e_,tr,n_,vi,t_,o_,s_,or,r_,sr,a_,i_,l_,nn,rr,d_,Ft,c_,Ti,u_,p_,rd,h_,f_,m_,ro,g_,ad,__,F_,ar,Uc,vt,ao,id,ir,v_,ld,T_,Gc,Ye,lr,k_,dd,w_,b_,dr,y_,cr,$_,E_,M_,ur,z_,ki,q_,P_,C_,pr,x_,hr,j_,L_,A_,tn,fr,D_,Tt,I_,wi,S_,N_,cd,O_,B_,W_,io,Q_,ud,R_,H_,mr,Zc,kt,lo,pd,gr,V_,hd,Y_,Kc,Ue,_r,U_,wt,G_,fd,Z_,K_,md,X_,J_,eF,Fr,nF,vr,tF,oF,sF,Tr,rF,bi,aF,iF,lF,kr,dF,wr,cF,uF,pF,on,br,hF,bt,fF,yi,mF,gF,gd,_F,FF,vF,co,TF,_d,kF,wF,yr,Xc,yt,uo,Fd,$r,bF,vd,yF,Jc,xe,Er,$F,Td,EF,MF,Mr,zF,zr,qF,PF,CF,qr,xF,$i,jF,LF,AF,Pr,DF,Cr,IF,SF,NF,po,OF,sn,xr,BF,$t,WF,Ei,QF,RF,kd,HF,VF,YF,ho,UF,wd,GF,ZF,jr,eu,Et,fo,bd,Lr,KF,yd,XF,nu,je,Ar,JF,$d,ev,nv,Dr,tv,Ir,ov,sv,rv,Sr,av,Mi,iv,lv,dv,Nr,cv,Or,uv,pv,hv,mo,fv,rn,Br,mv,Mt,gv,zi,_v,Fv,Ed,vv,Tv,kv,go,wv,Md,bv,yv,Wr,tu,zt,_o,zd,Qr,$v,qd,Ev,ou,Le,Rr,Mv,Pd,zv,qv,Hr,Pv,Vr,Cv,xv,jv,Yr,Lv,qi,Av,Dv,Iv,Ur,Sv,Gr,Nv,Ov,Bv,Fo,Wv,an,Zr,Qv,qt,Rv,Pi,Hv,Vv,Cd,Yv,Uv,Gv,vo,Zv,xd,Kv,Xv,Kr,su,Pt,To,jd,Xr,Jv,Ld,eT,ru,Ae,Jr,nT,ea,tT,Ad,oT,sT,rT,na,aT,ta,iT,lT,dT,oa,cT,Ci,uT,pT,hT,sa,fT,ra,mT,gT,_T,ko,FT,ln,aa,vT,Ct,TT,xi,kT,wT,Dd,bT,yT,$T,wo,ET,Id,MT,zT,ia,au,xt,bo,Sd,la,qT,Nd,PT,iu,De,da,CT,Od,xT,jT,ca,LT,ua,AT,DT,IT,pa,ST,ji,NT,OT,BT,ha,WT,fa,QT,RT,HT,yo,VT,dn,ma,YT,jt,UT,Li,GT,ZT,Bd,KT,XT,JT,$o,ek,Wd,nk,tk,ga,lu,Lt,Eo,Qd,_a,ok,Rd,sk,du,Ie,Fa,rk,Hd,ak,ik,va,lk,Ta,dk,ck,uk,ka,pk,Ai,hk,fk,mk,wa,gk,ba,_k,Fk,vk,Mo,Tk,cn,ya,kk,At,wk,Di,bk,yk,Vd,$k,Ek,Mk,zo,zk,Yd,qk,Pk,$a,cu,Dt,qo,Ud,Ea,Ck,Gd,xk,uu,Se,Ma,jk,Zd,Lk,Ak,za,Dk,qa,Ik,Sk,Nk,Pa,Ok,Ii,Bk,Wk,Qk,Ca,Rk,xa,Hk,Vk,Yk,Po,Uk,un,ja,Gk,It,Zk,Si,Kk,Xk,Kd,Jk,e1,n1,Co,t1,Xd,o1,s1,La,pu,St,xo,Jd,Aa,r1,ec,a1,hu,Ne,Da,i1,Nt,l1,nc,d1,c1,tc,u1,p1,h1,Ia,f1,Sa,m1,g1,_1,Na,F1,Ni,v1,T1,k1,Oa,w1,Ba,b1,y1,$1,jo,E1,pn,Wa,M1,Ot,z1,Oi,q1,P1,oc,C1,x1,j1,Lo,L1,sc,A1,D1,Qa,fu;return F=new qe({}),ne=new qe({}),Oo=new qe({}),Bo=new X({props:{name:"class transformers.FunnelConfig",anchor:"transformers.FunnelConfig",parameters:[{name:"vocab_size",val:" = 30522"},{name:"block_sizes",val:" = [4, 4, 4]"},{name:"block_repeats",val:" = None"},{name:"num_decoder_layers",val:" = 2"},{name:"d_model",val:" = 768"},{name:"n_head",val:" = 12"},{name:"d_head",val:" = 64"},{name:"d_inner",val:" = 3072"},{name:"hidden_act",val:" = 'gelu_new'"},{name:"hidden_dropout",val:" = 0.1"},{name:"attention_dropout",val:" = 0.1"},{name:"activation_dropout",val:" = 0.0"},{name:"max_position_embeddings",val:" = 512"},{name:"type_vocab_size",val:" = 3"},{name:"initializer_range",val:" = 0.1"},{name:"initializer_std",val:" = None"},{name:"layer_norm_eps",val:" = 1e-09"},{name:"pooling_type",val:" = 'mean'"},{name:"attention_type",val:" = 'relative_shift'"},{name:"separate_cls",val:" = True"},{name:"truncate_seq",val:" = True"},{name:"pool_q_only",val:" = True"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15907/src/transformers/models/funnel/configuration_funnel.py#L37",parametersDescription:[{anchor:"transformers.FunnelConfig.vocab_size",description:`<strong>vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 30522) &#x2014;
Vocabulary size of the Funnel transformer. Defines the number of different tokens that can be represented
by the <code>inputs_ids</code> passed when calling <a href="/docs/transformers/pr_15907/en/model_doc/funnel#transformers.FunnelModel">FunnelModel</a> or <a href="/docs/transformers/pr_15907/en/model_doc/funnel#transformers.TFFunnelModel">TFFunnelModel</a>.`,name:"vocab_size"},{anchor:"transformers.FunnelConfig.block_sizes",description:`<strong>block_sizes</strong> (<code>List[int]</code>, <em>optional</em>, defaults to <code>[4, 4, 4]</code>) &#x2014;
The sizes of the blocks used in the model.`,name:"block_sizes"},{anchor:"transformers.FunnelConfig.block_repeats",description:`<strong>block_repeats</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
If passed along, each layer of each block is repeated the number of times indicated.`,name:"block_repeats"},{anchor:"transformers.FunnelConfig.num_decoder_layers",description:`<strong>num_decoder_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 2) &#x2014;
The number of layers in the decoder (when not using the base model).`,name:"num_decoder_layers"},{anchor:"transformers.FunnelConfig.d_model",description:`<strong>d_model</strong> (<code>int</code>, <em>optional</em>, defaults to 768) &#x2014;
Dimensionality of the model&#x2019;s hidden states.`,name:"d_model"},{anchor:"transformers.FunnelConfig.n_head",description:`<strong>n_head</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of attention heads for each attention layer in the Transformer encoder.`,name:"n_head"},{anchor:"transformers.FunnelConfig.d_head",description:`<strong>d_head</strong> (<code>int</code>, <em>optional</em>, defaults to 64) &#x2014;
Dimensionality of the model&#x2019;s heads.`,name:"d_head"},{anchor:"transformers.FunnelConfig.d_inner",description:`<strong>d_inner</strong> (<code>int</code>, <em>optional</em>, defaults to 3072) &#x2014;
Inner dimension in the feed-forward blocks.`,name:"d_inner"},{anchor:"transformers.FunnelConfig.hidden_act",description:`<strong>hidden_act</strong> (<code>str</code> or <code>callable</code>, <em>optional</em>, defaults to <code>&quot;gelu_new&quot;</code>) &#x2014;
The non-linear activation function (function or string) in the encoder and pooler. If string, <code>&quot;gelu&quot;</code>,
<code>&quot;relu&quot;</code>, <code>&quot;silu&quot;</code> and <code>&quot;gelu_new&quot;</code> are supported.`,name:"hidden_act"},{anchor:"transformers.FunnelConfig.hidden_dropout",description:`<strong>hidden_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout probability for all fully connected layers in the embeddings, encoder, and pooler.`,name:"hidden_dropout"},{anchor:"transformers.FunnelConfig.attention_dropout",description:`<strong>attention_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout probability for the attention probabilities.`,name:"attention_dropout"},{anchor:"transformers.FunnelConfig.activation_dropout",description:`<strong>activation_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout probability used between the two layers of the feed-forward blocks.`,name:"activation_dropout"},{anchor:"transformers.FunnelConfig.max_position_embeddings",description:`<strong>max_position_embeddings</strong> (<code>int</code>, <em>optional</em>, defaults to 512) &#x2014;
The maximum sequence length that this model might ever be used with. Typically set this to something large
just in case (e.g., 512 or 1024 or 2048).`,name:"max_position_embeddings"},{anchor:"transformers.FunnelConfig.type_vocab_size",description:`<strong>type_vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 3) &#x2014;
The vocabulary size of the <code>token_type_ids</code> passed when calling <a href="/docs/transformers/pr_15907/en/model_doc/funnel#transformers.FunnelModel">FunnelModel</a> or <a href="/docs/transformers/pr_15907/en/model_doc/funnel#transformers.TFFunnelModel">TFFunnelModel</a>.`,name:"type_vocab_size"},{anchor:"transformers.FunnelConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The upper bound of the <em>uniform initializer</em> for initializing all weight matrices in attention layers.`,name:"initializer_range"},{anchor:"transformers.FunnelConfig.initializer_std",description:`<strong>initializer_std</strong> (<code>float</code>, <em>optional</em>) &#x2014;
The standard deviation of the <em>normal initializer</em> for initializing the embedding matrix and the weight of
linear layers. Will default to 1 for the embedding matrix and the value given by Xavier initialization for
linear layers.`,name:"initializer_std"},{anchor:"transformers.FunnelConfig.layer_norm_eps",description:`<strong>layer_norm_eps</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-9) &#x2014;
The epsilon used by the layer normalization layers.`,name:"layer_norm_eps"},{anchor:"transformers.FunnelConfig.pooling_type",description:`<strong>pooling_type</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;mean&quot;</code>) &#x2014;
Possible values are <code>&quot;mean&quot;</code> or <code>&quot;max&quot;</code>. The way pooling is performed at the beginning of each block.`,name:"pooling_type"},{anchor:"transformers.FunnelConfig.attention_type",description:`<strong>attention_type</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;relative_shift&quot;</code>) &#x2014;
Possible values are <code>&quot;relative_shift&quot;</code> or <code>&quot;factorized&quot;</code>. The former is faster on CPU/GPU while the latter
is faster on TPU.`,name:"attention_type"},{anchor:"transformers.FunnelConfig.separate_cls",description:`<strong>separate_cls</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to separate the cls token when applying pooling.`,name:"separate_cls"},{anchor:"transformers.FunnelConfig.truncate_seq",description:`<strong>truncate_seq</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
When using <code>separate_cls</code>, whether or not to truncate the last token when pooling, to avoid getting a
sequence length that is not a multiple of 2.`,name:"truncate_seq"},{anchor:"transformers.FunnelConfig.pool_q_only",description:`<strong>pool_q_only</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to apply the pooling only to the query or to query, key and values for the attention layers.`,name:"pool_q_only"}]}}),Qo=new qe({}),Ro=new X({props:{name:"class transformers.FunnelTokenizer",anchor:"transformers.FunnelTokenizer",parameters:[{name:"vocab_file",val:""},{name:"do_lower_case",val:" = True"},{name:"do_basic_tokenize",val:" = True"},{name:"never_split",val:" = None"},{name:"unk_token",val:" = '<unk>'"},{name:"sep_token",val:" = '<sep>'"},{name:"pad_token",val:" = '<pad>'"},{name:"cls_token",val:" = '<cls>'"},{name:"mask_token",val:" = '<mask>'"},{name:"bos_token",val:" = '<s>'"},{name:"eos_token",val:" = '</s>'"},{name:"tokenize_chinese_chars",val:" = True"},{name:"strip_accents",val:" = None"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15907/src/transformers/models/funnel/tokenization_funnel.py#L58"}}),Vo=new X({props:{name:"build_inputs_with_special_tokens",anchor:"transformers.BertTokenizer.build_inputs_with_special_tokens",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"}],source:"https://github.com/huggingface/transformers/blob/pr_15907/src/transformers/models/bert/tokenization_bert.py#L248",parametersDescription:[{anchor:"transformers.BertTokenizer.build_inputs_with_special_tokens.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs to which the special tokens will be added.`,name:"token_ids_0"},{anchor:"transformers.BertTokenizer.build_inputs_with_special_tokens.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],returnDescription:`
<p>List of <a href="../glossary#input-ids">input IDs</a> with the appropriate special tokens.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),Uo=new X({props:{name:"get_special_tokens_mask",anchor:"transformers.BertTokenizer.get_special_tokens_mask",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"},{name:"already_has_special_tokens",val:": bool = False"}],source:"https://github.com/huggingface/transformers/blob/pr_15907/src/transformers/models/bert/tokenization_bert.py#L273",parametersDescription:[{anchor:"transformers.BertTokenizer.get_special_tokens_mask.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs.`,name:"token_ids_0"},{anchor:"transformers.BertTokenizer.get_special_tokens_mask.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"},{anchor:"transformers.BertTokenizer.get_special_tokens_mask.already_has_special_tokens",description:`<strong>already_has_special_tokens</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not the token list is already formatted with special tokens for the model.`,name:"already_has_special_tokens"}],returnDescription:`
<p>A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),Zo=new X({props:{name:"create_token_type_ids_from_sequences",anchor:"transformers.FunnelTokenizer.create_token_type_ids_from_sequences",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"}],source:"https://github.com/huggingface/transformers/blob/pr_15907/src/transformers/models/funnel/tokenization_funnel.py#L108",parametersDescription:[{anchor:"transformers.FunnelTokenizer.create_token_type_ids_from_sequences.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs.`,name:"token_ids_0"},{anchor:"transformers.FunnelTokenizer.create_token_type_ids_from_sequences.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],returnDescription:`
<p>List of <a href="../glossary#token-type-ids">token type IDs</a> according to the given sequence(s).</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),Ko=new Oe({props:{code:`2 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1
| first sequence    | second sequence |`,highlighted:`2<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 1 </span>1<span class="hljs-number"> 1 </span>1<span class="hljs-number"> 1 </span>1<span class="hljs-number"> 1 </span>1 1
| first sequence    | second sequence |`}}),Xo=new X({props:{name:"save_vocabulary",anchor:"transformers.BertTokenizer.save_vocabulary",parameters:[{name:"save_directory",val:": str"},{name:"filename_prefix",val:": typing.Optional[str] = None"}],source:"https://github.com/huggingface/transformers/blob/pr_15907/src/transformers/models/bert/tokenization_bert.py#L330"}}),Jo=new qe({}),es=new X({props:{name:"class transformers.FunnelTokenizerFast",anchor:"transformers.FunnelTokenizerFast",parameters:[{name:"vocab_file",val:" = None"},{name:"tokenizer_file",val:" = None"},{name:"do_lower_case",val:" = True"},{name:"unk_token",val:" = '<unk>'"},{name:"sep_token",val:" = '<sep>'"},{name:"pad_token",val:" = '<pad>'"},{name:"cls_token",val:" = '<cls>'"},{name:"mask_token",val:" = '<mask>'"},{name:"bos_token",val:" = '<s>'"},{name:"eos_token",val:" = '</s>'"},{name:"clean_text",val:" = True"},{name:"tokenize_chinese_chars",val:" = True"},{name:"strip_accents",val:" = None"},{name:"wordpieces_prefix",val:" = '##'"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15907/src/transformers/models/funnel/tokenization_funnel_fast.py#L71"}}),os=new X({props:{name:"create_token_type_ids_from_sequences",anchor:"transformers.FunnelTokenizerFast.create_token_type_ids_from_sequences",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"}],source:"https://github.com/huggingface/transformers/blob/pr_15907/src/transformers/models/funnel/tokenization_funnel_fast.py#L124",parametersDescription:[{anchor:"transformers.FunnelTokenizerFast.create_token_type_ids_from_sequences.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs.`,name:"token_ids_0"},{anchor:"transformers.FunnelTokenizerFast.create_token_type_ids_from_sequences.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],returnDescription:`
<p>List of <a href="../glossary#token-type-ids">token type IDs</a> according to the given sequence(s).</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),ss=new Oe({props:{code:`2 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1
| first sequence    | second sequence |`,highlighted:`2<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 1 </span>1<span class="hljs-number"> 1 </span>1<span class="hljs-number"> 1 </span>1<span class="hljs-number"> 1 </span>1 1
| first sequence    | second sequence |`}}),rs=new qe({}),as=new X({props:{name:"class transformers.models.funnel.modeling_funnel.FunnelForPreTrainingOutput",anchor:"transformers.models.funnel.modeling_funnel.FunnelForPreTrainingOutput",parameters:[{name:"loss",val:": typing.Optional[torch.FloatTensor] = None"},{name:"logits",val:": FloatTensor = None"},{name:"hidden_states",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"}],source:"https://github.com/huggingface/transformers/blob/pr_15907/src/transformers/models/funnel/modeling_funnel.py#L801",parametersDescription:[{anchor:"transformers.models.funnel.modeling_funnel.FunnelForPreTrainingOutput.loss",description:`<strong>loss</strong> (<em>optional</em>, returned when <code>labels</code> is provided, <code>torch.FloatTensor</code> of shape <code>(1,)</code>) &#x2014;
Total loss of the ELECTRA-style objective.`,name:"loss"},{anchor:"transformers.models.funnel.modeling_funnel.FunnelForPreTrainingOutput.logits",description:`<strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Prediction scores of the head (scores for each token before SoftMax).`,name:"logits"},{anchor:"transformers.models.funnel.modeling_funnel.FunnelForPreTrainingOutput.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.models.funnel.modeling_funnel.FunnelForPreTrainingOutput.attentions",description:`<strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"}]}}),ls=new X({props:{name:"class transformers.models.funnel.modeling_tf_funnel.TFFunnelForPreTrainingOutput",anchor:"transformers.models.funnel.modeling_tf_funnel.TFFunnelForPreTrainingOutput",parameters:[{name:"logits",val:": Tensor = None"},{name:"hidden_states",val:": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"},{name:"attentions",val:": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"}],source:"https://github.com/huggingface/transformers/blob/pr_15907/src/transformers/models/funnel/modeling_tf_funnel.py#L1005",parametersDescription:[{anchor:"transformers.models.funnel.modeling_tf_funnel.TFFunnelForPreTrainingOutput.logits",description:`<strong>logits</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Prediction scores of the head (scores for each token before SoftMax).`,name:"logits"},{anchor:"transformers.models.funnel.modeling_tf_funnel.TFFunnelForPreTrainingOutput.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.models.funnel.modeling_tf_funnel.TFFunnelForPreTrainingOutput.attentions",description:`<strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"}]}}),cs=new qe({}),us=new X({props:{name:"class transformers.FunnelBaseModel",anchor:"transformers.FunnelBaseModel",parameters:[{name:"config",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15907/src/transformers/models/funnel/modeling_funnel.py#L894",parametersDescription:[{anchor:"transformers.FunnelBaseModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15907/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_15907/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),_s=new X({props:{name:"forward",anchor:"transformers.FunnelBaseModel.forward",parameters:[{name:"input_ids",val:" = None"},{name:"attention_mask",val:" = None"},{name:"token_type_ids",val:" = None"},{name:"position_ids",val:" = None"},{name:"head_mask",val:" = None"},{name:"inputs_embeds",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"}],source:"https://github.com/huggingface/transformers/blob/pr_15907/src/transformers/models/funnel/modeling_funnel.py#L910",parametersDescription:[{anchor:"transformers.FunnelBaseModel.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/pr_15907/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a>. See <a href="/docs/transformers/pr_15907/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/pr_15907/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.FunnelBaseModel.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.FunnelBaseModel.forward.token_type_ids",description:`<strong>token_type_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.FunnelBaseModel.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.FunnelBaseModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FunnelBaseModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FunnelBaseModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_15907/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],returnDescription:`
<p>A <a
  href="/docs/transformers/pr_15907/en/main_classes/output#transformers.modeling_outputs.BaseModelOutput"
>transformers.modeling_outputs.BaseModelOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/pr_15907/en/model_doc/funnel#transformers.FunnelConfig"
>FunnelConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) \u2014 Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`}}),Gt=new ze({props:{$$slots:{default:[My]},$$scope:{ctx:W}}}),Fs=new Oe({props:{code:`from transformers import FunnelTokenizer, FunnelBaseModel
import torch

tokenizer = FunnelTokenizer.from_pretrained("funnel-transformer/small-base")
model = FunnelBaseModel.from_pretrained("funnel-transformer/small-base")

inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
outputs = model(**inputs)

last_hidden_states = outputs.last_hidden_state`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FunnelTokenizer, FunnelBaseModel
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = FunnelTokenizer.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FunnelBaseModel.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small-base&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_states = outputs.last_hidden_state`}}),vs=new qe({}),Ts=new X({props:{name:"class transformers.FunnelModel",anchor:"transformers.FunnelModel",parameters:[{name:"config",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15907/src/transformers/models/funnel/modeling_funnel.py#L971",parametersDescription:[{anchor:"transformers.FunnelModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15907/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_15907/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),Es=new X({props:{name:"forward",anchor:"transformers.FunnelModel.forward",parameters:[{name:"input_ids",val:" = None"},{name:"attention_mask",val:" = None"},{name:"token_type_ids",val:" = None"},{name:"inputs_embeds",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"}],source:"https://github.com/huggingface/transformers/blob/pr_15907/src/transformers/models/funnel/modeling_funnel.py#L988",parametersDescription:[{anchor:"transformers.FunnelModel.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/pr_15907/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a>. See <a href="/docs/transformers/pr_15907/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/pr_15907/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.FunnelModel.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.FunnelModel.forward.token_type_ids",description:`<strong>token_type_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.FunnelModel.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.FunnelModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FunnelModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FunnelModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_15907/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],returnDescription:`
<p>A <a
  href="/docs/transformers/pr_15907/en/main_classes/output#transformers.modeling_outputs.BaseModelOutput"
>transformers.modeling_outputs.BaseModelOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/pr_15907/en/model_doc/funnel#transformers.FunnelConfig"
>FunnelConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) \u2014 Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`}}),Kt=new ze({props:{$$slots:{default:[zy]},$$scope:{ctx:W}}}),Ms=new Oe({props:{code:`from transformers import FunnelTokenizer, FunnelModel
import torch

tokenizer = FunnelTokenizer.from_pretrained("funnel-transformer/small")
model = FunnelModel.from_pretrained("funnel-transformer/small")

inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
outputs = model(**inputs)

last_hidden_states = outputs.last_hidden_state`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FunnelTokenizer, FunnelModel
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = FunnelTokenizer.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FunnelModel.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_states = outputs.last_hidden_state`}}),zs=new qe({}),qs=new X({props:{name:"class transformers.FunnelForPreTraining",anchor:"transformers.FunnelForPreTraining",parameters:[{name:"config",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15907/src/transformers/models/funnel/modeling_funnel.py#L1079"}}),Ps=new X({props:{name:"forward",anchor:"transformers.FunnelForPreTraining.forward",parameters:[{name:"input_ids",val:" = None"},{name:"attention_mask",val:" = None"},{name:"token_type_ids",val:" = None"},{name:"inputs_embeds",val:" = None"},{name:"labels",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"}],source:"https://github.com/huggingface/transformers/blob/pr_15907/src/transformers/models/funnel/modeling_funnel.py#L1088",parametersDescription:[{anchor:"transformers.FunnelForPreTraining.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/pr_15907/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a>. See <a href="/docs/transformers/pr_15907/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/pr_15907/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.FunnelForPreTraining.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.FunnelForPreTraining.forward.token_type_ids",description:`<strong>token_type_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.FunnelForPreTraining.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.FunnelForPreTraining.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FunnelForPreTraining.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FunnelForPreTraining.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_15907/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.FunnelForPreTraining.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Labels for computing the ELECTRA-style loss. Input should be a sequence of tokens (see <code>input_ids</code>
docstring) Indices should be in <code>[0, 1]</code>:</p>
<ul>
<li>0 indicates the token is an original token,</li>
<li>1 indicates the token was replaced.</li>
</ul>`,name:"labels"}],returnDescription:`
<p>A <a
  href="/docs/transformers/pr_15907/en/model_doc/funnel#transformers.models.funnel.modeling_funnel.FunnelForPreTrainingOutput"
>transformers.models.funnel.modeling_funnel.FunnelForPreTrainingOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/pr_15907/en/model_doc/funnel#transformers.FunnelConfig"
>FunnelConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<em>optional</em>, returned when <code>labels</code> is provided, <code>torch.FloatTensor</code> of shape <code>(1,)</code>) \u2014 Total loss of the ELECTRA-style objective.</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>) \u2014 Prediction scores of the head (scores for each token before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`}}),Jt=new ze({props:{$$slots:{default:[qy]},$$scope:{ctx:W}}}),Cs=new Oe({props:{code:`from transformers import FunnelTokenizer, FunnelForPreTraining
import torch

tokenizer = FunnelTokenizer.from_pretrained("funnel-transformer/small")
model = FunnelForPreTraining.from_pretrained("funnel-transformer/small")

inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
logits = model(**inputs).logits`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FunnelTokenizer, FunnelForPreTraining
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = FunnelTokenizer.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FunnelForPreTraining.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = model(**inputs).logits`}}),xs=new qe({}),js=new X({props:{name:"class transformers.FunnelForMaskedLM",anchor:"transformers.FunnelForMaskedLM",parameters:[{name:"config",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15907/src/transformers/models/funnel/modeling_funnel.py#L1162",parametersDescription:[{anchor:"transformers.FunnelForMaskedLM.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15907/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_15907/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),Os=new X({props:{name:"forward",anchor:"transformers.FunnelForMaskedLM.forward",parameters:[{name:"input_ids",val:" = None"},{name:"attention_mask",val:" = None"},{name:"token_type_ids",val:" = None"},{name:"inputs_embeds",val:" = None"},{name:"labels",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"}],source:"https://github.com/huggingface/transformers/blob/pr_15907/src/transformers/models/funnel/modeling_funnel.py#L1178",parametersDescription:[{anchor:"transformers.FunnelForMaskedLM.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/pr_15907/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a>. See <a href="/docs/transformers/pr_15907/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/pr_15907/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.FunnelForMaskedLM.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.FunnelForMaskedLM.forward.token_type_ids",description:`<strong>token_type_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.FunnelForMaskedLM.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.FunnelForMaskedLM.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FunnelForMaskedLM.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FunnelForMaskedLM.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_15907/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.FunnelForMaskedLM.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Labels for computing the masked language modeling loss. Indices should be in <code>[-100, 0, ..., config.vocab_size]</code> (see <code>input_ids</code> docstring) Tokens with indices set to <code>-100</code> are ignored (masked), the
loss is only computed for the tokens with labels in <code>[0, ..., config.vocab_size]</code>`,name:"labels"}],returnDescription:`
<p>A <a
  href="/docs/transformers/pr_15907/en/main_classes/output#transformers.modeling_outputs.MaskedLMOutput"
>transformers.modeling_outputs.MaskedLMOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/pr_15907/en/model_doc/funnel#transformers.FunnelConfig"
>FunnelConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) \u2014 Masked language modeling (MLM) loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, config.vocab_size)</code>) \u2014 Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`}}),no=new ze({props:{$$slots:{default:[Py]},$$scope:{ctx:W}}}),Bs=new Oe({props:{code:`from transformers import FunnelTokenizer, FunnelForMaskedLM
import torch

tokenizer = FunnelTokenizer.from_pretrained("funnel-transformer/small")
model = FunnelForMaskedLM.from_pretrained("funnel-transformer/small")

inputs = tokenizer("The capital of France is <mask>.", return_tensors="pt")
labels = tokenizer("The capital of France is Paris.", return_tensors="pt")["input_ids"]

outputs = model(**inputs, labels=labels)
loss = outputs.loss
logits = outputs.logits`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FunnelTokenizer, FunnelForMaskedLM
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = FunnelTokenizer.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FunnelForMaskedLM.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;The capital of France is &lt;mask&gt;.&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>labels = tokenizer(<span class="hljs-string">&quot;The capital of France is Paris.&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)[<span class="hljs-string">&quot;input_ids&quot;</span>]

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs, labels=labels)
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = outputs.loss
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits`}}),Ws=new qe({}),Qs=new X({props:{name:"class transformers.FunnelForSequenceClassification",anchor:"transformers.FunnelForSequenceClassification",parameters:[{name:"config",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15907/src/transformers/models/funnel/modeling_funnel.py#L1242",parametersDescription:[{anchor:"transformers.FunnelForSequenceClassification.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15907/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_15907/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),Gs=new X({props:{name:"forward",anchor:"transformers.FunnelForSequenceClassification.forward",parameters:[{name:"input_ids",val:" = None"},{name:"attention_mask",val:" = None"},{name:"token_type_ids",val:" = None"},{name:"inputs_embeds",val:" = None"},{name:"labels",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"}],source:"https://github.com/huggingface/transformers/blob/pr_15907/src/transformers/models/funnel/modeling_funnel.py#L1253",parametersDescription:[{anchor:"transformers.FunnelForSequenceClassification.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/pr_15907/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a>. See <a href="/docs/transformers/pr_15907/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/pr_15907/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.FunnelForSequenceClassification.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.FunnelForSequenceClassification.forward.token_type_ids",description:`<strong>token_type_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.FunnelForSequenceClassification.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.FunnelForSequenceClassification.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FunnelForSequenceClassification.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FunnelForSequenceClassification.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_15907/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.FunnelForSequenceClassification.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for computing the sequence classification/regression loss. Indices should be in <code>[0, ..., config.num_labels - 1]</code>. If <code>config.num_labels == 1</code> a regression loss is computed (Mean-Square loss), If
<code>config.num_labels &gt; 1</code> a classification loss is computed (Cross-Entropy).`,name:"labels"}],returnDescription:`
<p>A <a
  href="/docs/transformers/pr_15907/en/main_classes/output#transformers.modeling_outputs.SequenceClassifierOutput"
>transformers.modeling_outputs.SequenceClassifierOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/pr_15907/en/model_doc/funnel#transformers.FunnelConfig"
>FunnelConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) \u2014 Classification (or regression if config.num_labels==1) loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, config.num_labels)</code>) \u2014 Classification (or regression if config.num_labels==1) scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`}}),oo=new ze({props:{$$slots:{default:[Cy]},$$scope:{ctx:W}}}),Zs=new Oe({props:{code:`from transformers import FunnelTokenizer, FunnelForSequenceClassification
import torch

tokenizer = FunnelTokenizer.from_pretrained("funnel-transformer/small-base")
model = FunnelForSequenceClassification.from_pretrained("funnel-transformer/small-base")

inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
labels = torch.tensor([1]).unsqueeze(0)  # Batch size 1
outputs = model(**inputs, labels=labels)
loss = outputs.loss
logits = outputs.logits`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FunnelTokenizer, FunnelForSequenceClassification
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = FunnelTokenizer.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FunnelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small-base&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>labels = torch.tensor([<span class="hljs-number">1</span>]).unsqueeze(<span class="hljs-number">0</span>)  <span class="hljs-comment"># Batch size 1</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs, labels=labels)
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = outputs.loss
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits`}}),Ks=new Oe({props:{code:`from transformers import FunnelTokenizer, FunnelForSequenceClassification
import torch

tokenizer = FunnelTokenizer.from_pretrained("funnel-transformer/small-base")
model = FunnelForSequenceClassification.from_pretrained("funnel-transformer/small-base", problem_type="multi_label_classification")

inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
labels = torch.tensor([[1, 1]], dtype=torch.float)  # need dtype=float for BCEWithLogitsLoss
outputs = model(**inputs, labels=labels)
loss = outputs.loss
logits = outputs.logits`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FunnelTokenizer, FunnelForSequenceClassification
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = FunnelTokenizer.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FunnelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small-base&quot;</span>, problem_type=<span class="hljs-string">&quot;multi_label_classification&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>labels = torch.tensor([[<span class="hljs-number">1</span>, <span class="hljs-number">1</span>]], dtype=torch.<span class="hljs-built_in">float</span>)  <span class="hljs-comment"># need dtype=float for BCEWithLogitsLoss</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs, labels=labels)
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = outputs.loss
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits`}}),Xs=new qe({}),Js=new X({props:{name:"class transformers.FunnelForMultipleChoice",anchor:"transformers.FunnelForMultipleChoice",parameters:[{name:"config",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15907/src/transformers/models/funnel/modeling_funnel.py#L1335",parametersDescription:[{anchor:"transformers.FunnelForMultipleChoice.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15907/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_15907/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),rr=new X({props:{name:"forward",anchor:"transformers.FunnelForMultipleChoice.forward",parameters:[{name:"input_ids",val:" = None"},{name:"attention_mask",val:" = None"},{name:"token_type_ids",val:" = None"},{name:"inputs_embeds",val:" = None"},{name:"labels",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"}],source:"https://github.com/huggingface/transformers/blob/pr_15907/src/transformers/models/funnel/modeling_funnel.py#L1344",parametersDescription:[{anchor:"transformers.FunnelForMultipleChoice.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, num_choices, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/pr_15907/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a>. See <a href="/docs/transformers/pr_15907/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/pr_15907/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.FunnelForMultipleChoice.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_choices, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.FunnelForMultipleChoice.forward.token_type_ids",description:`<strong>token_type_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, num_choices, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.FunnelForMultipleChoice.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_choices, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.FunnelForMultipleChoice.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FunnelForMultipleChoice.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FunnelForMultipleChoice.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_15907/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.FunnelForMultipleChoice.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for computing the multiple choice classification loss. Indices should be in <code>[0, ..., num_choices-1]</code> where <code>num_choices</code> is the size of the second dimension of the input tensors. (See
<code>input_ids</code> above)`,name:"labels"}],returnDescription:`
<p>A <a
  href="/docs/transformers/pr_15907/en/main_classes/output#transformers.modeling_outputs.MultipleChoiceModelOutput"
>transformers.modeling_outputs.MultipleChoiceModelOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/pr_15907/en/model_doc/funnel#transformers.FunnelConfig"
>FunnelConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <em>(1,)</em>, <em>optional</em>, returned when <code>labels</code> is provided) \u2014 Classification loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_choices)</code>) \u2014 <em>num_choices</em> is the second dimension of the input tensors. (see <em>input_ids</em> above).</p>
<p>Classification scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`}}),ro=new ze({props:{$$slots:{default:[xy]},$$scope:{ctx:W}}}),ar=new Oe({props:{code:`from transformers import FunnelTokenizer, FunnelForMultipleChoice
import torch

tokenizer = FunnelTokenizer.from_pretrained("funnel-transformer/small-base")
model = FunnelForMultipleChoice.from_pretrained("funnel-transformer/small-base")

prompt = "In Italy, pizza served in formal settings, such as at a restaurant, is presented unsliced."
choice0 = "It is eaten with a fork and a knife."
choice1 = "It is eaten while held in the hand."
labels = torch.tensor(0).unsqueeze(0)  # choice0 is correct (according to Wikipedia ;)), batch size 1

encoding = tokenizer([prompt, prompt], [choice0, choice1], return_tensors="pt", padding=True)
outputs = model(**{k: v.unsqueeze(0) for k, v in encoding.items()}, labels=labels)  # batch size is 1

# the linear classifier still needs to be trained
loss = outputs.loss
logits = outputs.logits`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FunnelTokenizer, FunnelForMultipleChoice
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = FunnelTokenizer.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FunnelForMultipleChoice.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small-base&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>prompt = <span class="hljs-string">&quot;In Italy, pizza served in formal settings, such as at a restaurant, is presented unsliced.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>choice0 = <span class="hljs-string">&quot;It is eaten with a fork and a knife.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>choice1 = <span class="hljs-string">&quot;It is eaten while held in the hand.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>labels = torch.tensor(<span class="hljs-number">0</span>).unsqueeze(<span class="hljs-number">0</span>)  <span class="hljs-comment"># choice0 is correct (according to Wikipedia ;)), batch size 1</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>encoding = tokenizer([prompt, prompt], [choice0, choice1], return_tensors=<span class="hljs-string">&quot;pt&quot;</span>, padding=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**{k: v.unsqueeze(<span class="hljs-number">0</span>) <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> encoding.items()}, labels=labels)  <span class="hljs-comment"># batch size is 1</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># the linear classifier still needs to be trained</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = outputs.loss
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits`}}),ir=new qe({}),lr=new X({props:{name:"class transformers.FunnelForTokenClassification",anchor:"transformers.FunnelForTokenClassification",parameters:[{name:"config",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15907/src/transformers/models/funnel/modeling_funnel.py#L1419",parametersDescription:[{anchor:"transformers.FunnelForTokenClassification.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15907/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_15907/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),fr=new X({props:{name:"forward",anchor:"transformers.FunnelForTokenClassification.forward",parameters:[{name:"input_ids",val:" = None"},{name:"attention_mask",val:" = None"},{name:"token_type_ids",val:" = None"},{name:"inputs_embeds",val:" = None"},{name:"labels",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"}],source:"https://github.com/huggingface/transformers/blob/pr_15907/src/transformers/models/funnel/modeling_funnel.py#L1431",parametersDescription:[{anchor:"transformers.FunnelForTokenClassification.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/pr_15907/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a>. See <a href="/docs/transformers/pr_15907/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/pr_15907/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.FunnelForTokenClassification.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.FunnelForTokenClassification.forward.token_type_ids",description:`<strong>token_type_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.FunnelForTokenClassification.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.FunnelForTokenClassification.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FunnelForTokenClassification.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FunnelForTokenClassification.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_15907/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.FunnelForTokenClassification.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Labels for computing the token classification loss. Indices should be in <code>[0, ..., config.num_labels - 1]</code>.`,name:"labels"}],returnDescription:`
<p>A <a
  href="/docs/transformers/pr_15907/en/main_classes/output#transformers.modeling_outputs.TokenClassifierOutput"
>transformers.modeling_outputs.TokenClassifierOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/pr_15907/en/model_doc/funnel#transformers.FunnelConfig"
>FunnelConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided)  \u2014 Classification loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, config.num_labels)</code>) \u2014 Classification scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`}}),io=new ze({props:{$$slots:{default:[jy]},$$scope:{ctx:W}}}),mr=new Oe({props:{code:`from transformers import FunnelTokenizer, FunnelForTokenClassification
import torch

tokenizer = FunnelTokenizer.from_pretrained("funnel-transformer/small")
model = FunnelForTokenClassification.from_pretrained("funnel-transformer/small")

inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
labels = torch.tensor([1] * inputs["input_ids"].size(1)).unsqueeze(0)  # Batch size 1

outputs = model(**inputs, labels=labels)
loss = outputs.loss
logits = outputs.logits`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FunnelTokenizer, FunnelForTokenClassification
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = FunnelTokenizer.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FunnelForTokenClassification.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>labels = torch.tensor([<span class="hljs-number">1</span>] * inputs[<span class="hljs-string">&quot;input_ids&quot;</span>].size(<span class="hljs-number">1</span>)).unsqueeze(<span class="hljs-number">0</span>)  <span class="hljs-comment"># Batch size 1</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs, labels=labels)
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = outputs.loss
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits`}}),gr=new qe({}),_r=new X({props:{name:"class transformers.FunnelForQuestionAnswering",anchor:"transformers.FunnelForQuestionAnswering",parameters:[{name:"config",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15907/src/transformers/models/funnel/modeling_funnel.py#L1493",parametersDescription:[{anchor:"transformers.FunnelForQuestionAnswering.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15907/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_15907/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),br=new X({props:{name:"forward",anchor:"transformers.FunnelForQuestionAnswering.forward",parameters:[{name:"input_ids",val:" = None"},{name:"attention_mask",val:" = None"},{name:"token_type_ids",val:" = None"},{name:"inputs_embeds",val:" = None"},{name:"start_positions",val:" = None"},{name:"end_positions",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"}],source:"https://github.com/huggingface/transformers/blob/pr_15907/src/transformers/models/funnel/modeling_funnel.py#L1504",parametersDescription:[{anchor:"transformers.FunnelForQuestionAnswering.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/pr_15907/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a>. See <a href="/docs/transformers/pr_15907/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/pr_15907/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.FunnelForQuestionAnswering.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.FunnelForQuestionAnswering.forward.token_type_ids",description:`<strong>token_type_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.FunnelForQuestionAnswering.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.FunnelForQuestionAnswering.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FunnelForQuestionAnswering.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FunnelForQuestionAnswering.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_15907/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.FunnelForQuestionAnswering.forward.start_positions",description:`<strong>start_positions</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for position (index) of the start of the labelled span for computing the token classification loss.
Positions are clamped to the length of the sequence (<code>sequence_length</code>). Position outside of the sequence
are not taken into account for computing the loss.`,name:"start_positions"},{anchor:"transformers.FunnelForQuestionAnswering.forward.end_positions",description:`<strong>end_positions</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for position (index) of the end of the labelled span for computing the token classification loss.
Positions are clamped to the length of the sequence (<code>sequence_length</code>). Position outside of the sequence
are not taken into account for computing the loss.`,name:"end_positions"}],returnDescription:`
<p>A <a
  href="/docs/transformers/pr_15907/en/main_classes/output#transformers.modeling_outputs.QuestionAnsweringModelOutput"
>transformers.modeling_outputs.QuestionAnsweringModelOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/pr_15907/en/model_doc/funnel#transformers.FunnelConfig"
>FunnelConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) \u2014 Total span extraction loss is the sum of a Cross-Entropy for the start and end positions.</p>
</li>
<li>
<p><strong>start_logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>) \u2014 Span-start scores (before SoftMax).</p>
</li>
<li>
<p><strong>end_logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>) \u2014 Span-end scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`}}),co=new ze({props:{$$slots:{default:[Ly]},$$scope:{ctx:W}}}),yr=new Oe({props:{code:`from transformers import FunnelTokenizer, FunnelForQuestionAnswering
import torch

tokenizer = FunnelTokenizer.from_pretrained("funnel-transformer/small")
model = FunnelForQuestionAnswering.from_pretrained("funnel-transformer/small")

question, text = "Who was Jim Henson?", "Jim Henson was a nice puppet"
inputs = tokenizer(question, text, return_tensors="pt")
start_positions = torch.tensor([1])
end_positions = torch.tensor([3])

outputs = model(**inputs, start_positions=start_positions, end_positions=end_positions)
loss = outputs.loss
start_scores = outputs.start_logits
end_scores = outputs.end_logits`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FunnelTokenizer, FunnelForQuestionAnswering
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = FunnelTokenizer.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FunnelForQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>question, text = <span class="hljs-string">&quot;Who was Jim Henson?&quot;</span>, <span class="hljs-string">&quot;Jim Henson was a nice puppet&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(question, text, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>start_positions = torch.tensor([<span class="hljs-number">1</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span>end_positions = torch.tensor([<span class="hljs-number">3</span>])

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs, start_positions=start_positions, end_positions=end_positions)
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = outputs.loss
<span class="hljs-meta">&gt;&gt;&gt; </span>start_scores = outputs.start_logits
<span class="hljs-meta">&gt;&gt;&gt; </span>end_scores = outputs.end_logits`}}),$r=new qe({}),Er=new X({props:{name:"class transformers.TFFunnelBaseModel",anchor:"transformers.TFFunnelBaseModel",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15907/src/transformers/models/funnel/modeling_tf_funnel.py#L1122",parametersDescription:[{anchor:"transformers.TFFunnelBaseModel.config",description:`<strong>config</strong> (<code>XxxConfig</code>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_15907/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),po=new ze({props:{$$slots:{default:[Ay]},$$scope:{ctx:W}}}),xr=new X({props:{name:"call",anchor:"transformers.TFFunnelBaseModel.call",parameters:[{name:"input_ids",val:" = None"},{name:"attention_mask",val:" = None"},{name:"token_type_ids",val:" = None"},{name:"inputs_embeds",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"},{name:"training",val:" = False"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15907/src/transformers/models/funnel/modeling_tf_funnel.py#L1127",parametersDescription:[{anchor:"transformers.TFFunnelBaseModel.call.input_ids",description:`<strong>input_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/pr_15907/en/model_doc/funnel#transformers.FunnelTokenizer">FunnelTokenizer</a>. See <a href="/docs/transformers/pr_15907/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> and
<a href="/docs/transformers/pr_15907/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.TFFunnelBaseModel.call.attention_mask",description:`<strong>attention_mask</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.TFFunnelBaseModel.call.token_type_ids",description:`<strong>token_type_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.TFFunnelBaseModel.call.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.TFFunnelBaseModel.call.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail. This argument can be used only in eager mode, in graph mode the value in the
config will be used instead.`,name:"output_attentions"},{anchor:"transformers.TFFunnelBaseModel.call.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail. This argument can be used only in eager mode, in graph mode the value in the config will be
used instead.`,name:"output_hidden_states"},{anchor:"transformers.TFFunnelBaseModel.call.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_15907/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple. This argument can be used
in eager mode, in graph mode the value will always be set to True.`,name:"return_dict"},{anchor:"transformers.TFFunnelBaseModel.call.training",description:`<strong>training</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to use the model in training mode (some modules like dropout modules have different
behaviors between training and evaluation).`,name:"training"}],returnDescription:`
<p>A <a
  href="/docs/transformers/pr_15907/en/main_classes/output#transformers.modeling_tf_outputs.TFBaseModelOutput"
>transformers.modeling_tf_outputs.TFBaseModelOutput</a> or a tuple of <code>tf.Tensor</code> (if
<code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various elements depending on the
configuration (<a
  href="/docs/transformers/pr_15907/en/model_doc/funnel#transformers.FunnelConfig"
>FunnelConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) \u2014 Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(tf.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`}}),ho=new ze({props:{$$slots:{default:[Dy]},$$scope:{ctx:W}}}),jr=new Oe({props:{code:`from transformers import FunnelTokenizer, TFFunnelBaseModel
import tensorflow as tf

tokenizer = FunnelTokenizer.from_pretrained("funnel-transformer/small-base")
model = TFFunnelBaseModel.from_pretrained("funnel-transformer/small-base")

inputs = tokenizer("Hello, my dog is cute", return_tensors="tf")
outputs = model(inputs)

last_hidden_states = outputs.last_hidden_state`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FunnelTokenizer, TFFunnelBaseModel
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = FunnelTokenizer.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFFunnelBaseModel.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small-base&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_states = outputs.last_hidden_state`}}),Lr=new qe({}),Ar=new X({props:{name:"class transformers.TFFunnelModel",anchor:"transformers.TFFunnelModel",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15907/src/transformers/models/funnel/modeling_tf_funnel.py#L1183",parametersDescription:[{anchor:"transformers.TFFunnelModel.config",description:`<strong>config</strong> (<code>XxxConfig</code>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_15907/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),mo=new ze({props:{$$slots:{default:[Iy]},$$scope:{ctx:W}}}),Br=new X({props:{name:"call",anchor:"transformers.TFFunnelModel.call",parameters:[{name:"input_ids",val:" = None"},{name:"attention_mask",val:" = None"},{name:"token_type_ids",val:" = None"},{name:"inputs_embeds",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"},{name:"training",val:" = False"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15907/src/transformers/models/funnel/modeling_tf_funnel.py#L1188",parametersDescription:[{anchor:"transformers.TFFunnelModel.call.input_ids",description:`<strong>input_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/pr_15907/en/model_doc/funnel#transformers.FunnelTokenizer">FunnelTokenizer</a>. See <a href="/docs/transformers/pr_15907/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> and
<a href="/docs/transformers/pr_15907/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.TFFunnelModel.call.attention_mask",description:`<strong>attention_mask</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.TFFunnelModel.call.token_type_ids",description:`<strong>token_type_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.TFFunnelModel.call.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.TFFunnelModel.call.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail. This argument can be used only in eager mode, in graph mode the value in the
config will be used instead.`,name:"output_attentions"},{anchor:"transformers.TFFunnelModel.call.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail. This argument can be used only in eager mode, in graph mode the value in the config will be
used instead.`,name:"output_hidden_states"},{anchor:"transformers.TFFunnelModel.call.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_15907/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple. This argument can be used
in eager mode, in graph mode the value will always be set to True.`,name:"return_dict"},{anchor:"transformers.TFFunnelModel.call.training",description:`<strong>training</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to use the model in training mode (some modules like dropout modules have different
behaviors between training and evaluation).`,name:"training"}],returnDescription:`
<p>A <a
  href="/docs/transformers/pr_15907/en/main_classes/output#transformers.modeling_tf_outputs.TFBaseModelOutput"
>transformers.modeling_tf_outputs.TFBaseModelOutput</a> or a tuple of <code>tf.Tensor</code> (if
<code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various elements depending on the
configuration (<a
  href="/docs/transformers/pr_15907/en/model_doc/funnel#transformers.FunnelConfig"
>FunnelConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) \u2014 Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(tf.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`}}),go=new ze({props:{$$slots:{default:[Sy]},$$scope:{ctx:W}}}),Wr=new Oe({props:{code:`from transformers import FunnelTokenizer, TFFunnelModel
import tensorflow as tf

tokenizer = FunnelTokenizer.from_pretrained("funnel-transformer/small")
model = TFFunnelModel.from_pretrained("funnel-transformer/small")

inputs = tokenizer("Hello, my dog is cute", return_tensors="tf")
outputs = model(inputs)

last_hidden_states = outputs.last_hidden_state`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FunnelTokenizer, TFFunnelModel
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = FunnelTokenizer.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFFunnelModel.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_states = outputs.last_hidden_state`}}),Qr=new qe({}),Rr=new X({props:{name:"class transformers.TFFunnelForPreTraining",anchor:"transformers.TFFunnelForPreTraining",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15907/src/transformers/models/funnel/modeling_tf_funnel.py#L1246",parametersDescription:[{anchor:"transformers.TFFunnelForPreTraining.config",description:`<strong>config</strong> (<code>XxxConfig</code>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_15907/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),Fo=new ze({props:{$$slots:{default:[Ny]},$$scope:{ctx:W}}}),Zr=new X({props:{name:"call",anchor:"transformers.TFFunnelForPreTraining.call",parameters:[{name:"input_ids",val:" = None"},{name:"attention_mask",val:" = None"},{name:"token_type_ids",val:" = None"},{name:"inputs_embeds",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"},{name:"training",val:" = False"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15907/src/transformers/models/funnel/modeling_tf_funnel.py#L1253",parametersDescription:[{anchor:"transformers.TFFunnelForPreTraining.call.input_ids",description:`<strong>input_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/pr_15907/en/model_doc/funnel#transformers.FunnelTokenizer">FunnelTokenizer</a>. See <a href="/docs/transformers/pr_15907/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> and
<a href="/docs/transformers/pr_15907/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.TFFunnelForPreTraining.call.attention_mask",description:`<strong>attention_mask</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.TFFunnelForPreTraining.call.token_type_ids",description:`<strong>token_type_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.TFFunnelForPreTraining.call.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.TFFunnelForPreTraining.call.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail. This argument can be used only in eager mode, in graph mode the value in the
config will be used instead.`,name:"output_attentions"},{anchor:"transformers.TFFunnelForPreTraining.call.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail. This argument can be used only in eager mode, in graph mode the value in the config will be
used instead.`,name:"output_hidden_states"},{anchor:"transformers.TFFunnelForPreTraining.call.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_15907/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple. This argument can be used
in eager mode, in graph mode the value will always be set to True.`,name:"return_dict"},{anchor:"transformers.TFFunnelForPreTraining.call.training",description:`<strong>training</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to use the model in training mode (some modules like dropout modules have different
behaviors between training and evaluation).`,name:"training"}],returnDescription:`
<p>A <a
  href="/docs/transformers/pr_15907/en/model_doc/funnel#transformers.models.funnel.modeling_tf_funnel.TFFunnelForPreTrainingOutput"
>transformers.models.funnel.modeling_tf_funnel.TFFunnelForPreTrainingOutput</a> or a tuple of <code>tf.Tensor</code> (if
<code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various elements depending on the
configuration (<a
  href="/docs/transformers/pr_15907/en/model_doc/funnel#transformers.FunnelConfig"
>FunnelConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>logits</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) \u2014 Prediction scores of the head (scores for each token before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`}}),vo=new ze({props:{$$slots:{default:[Oy]},$$scope:{ctx:W}}}),Kr=new Oe({props:{code:`from transformers import FunnelTokenizer, TFFunnelForPreTraining
import torch

tokenizer = FunnelTokenizer.from_pretrained("funnel-transformer/small")
model = TFFunnelForPreTraining.from_pretrained("funnel-transformer/small")

inputs = tokenizer("Hello, my dog is cute", return_tensors="tf")
logits = model(inputs).logits`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FunnelTokenizer, TFFunnelForPreTraining
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = FunnelTokenizer.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFFunnelForPreTraining.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = model(inputs).logits`}}),Xr=new qe({}),Jr=new X({props:{name:"class transformers.TFFunnelForMaskedLM",anchor:"transformers.TFFunnelForMaskedLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15907/src/transformers/models/funnel/modeling_tf_funnel.py#L1325",parametersDescription:[{anchor:"transformers.TFFunnelForMaskedLM.config",description:`<strong>config</strong> (<code>XxxConfig</code>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_15907/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),ko=new ze({props:{$$slots:{default:[By]},$$scope:{ctx:W}}}),aa=new X({props:{name:"call",anchor:"transformers.TFFunnelForMaskedLM.call",parameters:[{name:"input_ids",val:" = None"},{name:"attention_mask",val:" = None"},{name:"token_type_ids",val:" = None"},{name:"inputs_embeds",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"},{name:"labels",val:" = None"},{name:"training",val:" = False"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15907/src/transformers/models/funnel/modeling_tf_funnel.py#L1339",parametersDescription:[{anchor:"transformers.TFFunnelForMaskedLM.call.input_ids",description:`<strong>input_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/pr_15907/en/model_doc/funnel#transformers.FunnelTokenizer">FunnelTokenizer</a>. See <a href="/docs/transformers/pr_15907/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> and
<a href="/docs/transformers/pr_15907/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.TFFunnelForMaskedLM.call.attention_mask",description:`<strong>attention_mask</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.TFFunnelForMaskedLM.call.token_type_ids",description:`<strong>token_type_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.TFFunnelForMaskedLM.call.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.TFFunnelForMaskedLM.call.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail. This argument can be used only in eager mode, in graph mode the value in the
config will be used instead.`,name:"output_attentions"},{anchor:"transformers.TFFunnelForMaskedLM.call.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail. This argument can be used only in eager mode, in graph mode the value in the config will be
used instead.`,name:"output_hidden_states"},{anchor:"transformers.TFFunnelForMaskedLM.call.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_15907/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple. This argument can be used
in eager mode, in graph mode the value will always be set to True.`,name:"return_dict"},{anchor:"transformers.TFFunnelForMaskedLM.call.training",description:`<strong>training</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to use the model in training mode (some modules like dropout modules have different
behaviors between training and evaluation).`,name:"training"},{anchor:"transformers.TFFunnelForMaskedLM.call.labels",description:`<strong>labels</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Labels for computing the masked language modeling loss. Indices should be in <code>[-100, 0, ..., config.vocab_size]</code> (see <code>input_ids</code> docstring) Tokens with indices set to <code>-100</code> are ignored (masked), the
loss is only computed for the tokens with labels in <code>[0, ..., config.vocab_size]</code>`,name:"labels"}],returnDescription:`
<p>A <a
  href="/docs/transformers/pr_15907/en/main_classes/output#transformers.modeling_tf_outputs.TFMaskedLMOutput"
>transformers.modeling_tf_outputs.TFMaskedLMOutput</a> or a tuple of <code>tf.Tensor</code> (if
<code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various elements depending on the
configuration (<a
  href="/docs/transformers/pr_15907/en/model_doc/funnel#transformers.FunnelConfig"
>FunnelConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>tf.Tensor</code> of shape <code>(n,)</code>, <em>optional</em>, where n is the number of non-masked labels, returned when <code>labels</code> is provided) \u2014 Masked language modeling (MLM) loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, config.vocab_size)</code>) \u2014 Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`}}),wo=new ze({props:{$$slots:{default:[Wy]},$$scope:{ctx:W}}}),ia=new Oe({props:{code:`from transformers import FunnelTokenizer, TFFunnelForMaskedLM
import tensorflow as tf

tokenizer = FunnelTokenizer.from_pretrained("funnel-transformer/small")
model = TFFunnelForMaskedLM.from_pretrained("funnel-transformer/small")

inputs = tokenizer("The capital of France is [MASK].", return_tensors="tf")
inputs["labels"] = tokenizer("The capital of France is Paris.", return_tensors="tf")["input_ids"]

outputs = model(inputs)
loss = outputs.loss
logits = outputs.logits`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FunnelTokenizer, TFFunnelForMaskedLM
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = FunnelTokenizer.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFFunnelForMaskedLM.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;The capital of France is [MASK].&quot;</span>, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs[<span class="hljs-string">&quot;labels&quot;</span>] = tokenizer(<span class="hljs-string">&quot;The capital of France is Paris.&quot;</span>, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)[<span class="hljs-string">&quot;input_ids&quot;</span>]

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = outputs.loss
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits`}}),la=new qe({}),da=new X({props:{name:"class transformers.TFFunnelForSequenceClassification",anchor:"transformers.TFFunnelForSequenceClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15907/src/transformers/models/funnel/modeling_tf_funnel.py#L1420",parametersDescription:[{anchor:"transformers.TFFunnelForSequenceClassification.config",description:`<strong>config</strong> (<code>XxxConfig</code>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_15907/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),yo=new ze({props:{$$slots:{default:[Qy]},$$scope:{ctx:W}}}),ma=new X({props:{name:"call",anchor:"transformers.TFFunnelForSequenceClassification.call",parameters:[{name:"input_ids",val:" = None"},{name:"attention_mask",val:" = None"},{name:"token_type_ids",val:" = None"},{name:"inputs_embeds",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"},{name:"labels",val:" = None"},{name:"training",val:" = False"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15907/src/transformers/models/funnel/modeling_tf_funnel.py#L1428",parametersDescription:[{anchor:"transformers.TFFunnelForSequenceClassification.call.input_ids",description:`<strong>input_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/pr_15907/en/model_doc/funnel#transformers.FunnelTokenizer">FunnelTokenizer</a>. See <a href="/docs/transformers/pr_15907/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> and
<a href="/docs/transformers/pr_15907/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.TFFunnelForSequenceClassification.call.attention_mask",description:`<strong>attention_mask</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.TFFunnelForSequenceClassification.call.token_type_ids",description:`<strong>token_type_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.TFFunnelForSequenceClassification.call.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.TFFunnelForSequenceClassification.call.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail. This argument can be used only in eager mode, in graph mode the value in the
config will be used instead.`,name:"output_attentions"},{anchor:"transformers.TFFunnelForSequenceClassification.call.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail. This argument can be used only in eager mode, in graph mode the value in the config will be
used instead.`,name:"output_hidden_states"},{anchor:"transformers.TFFunnelForSequenceClassification.call.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_15907/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple. This argument can be used
in eager mode, in graph mode the value will always be set to True.`,name:"return_dict"},{anchor:"transformers.TFFunnelForSequenceClassification.call.training",description:`<strong>training</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to use the model in training mode (some modules like dropout modules have different
behaviors between training and evaluation).`,name:"training"},{anchor:"transformers.TFFunnelForSequenceClassification.call.labels",description:`<strong>labels</strong> (<code>tf.Tensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for computing the sequence classification/regression loss. Indices should be in <code>[0, ..., config.num_labels - 1]</code>. If <code>config.num_labels == 1</code> a regression loss is computed (Mean-Square loss), If
<code>config.num_labels &gt; 1</code> a classification loss is computed (Cross-Entropy).`,name:"labels"}],returnDescription:`
<p>A <a
  href="/docs/transformers/pr_15907/en/main_classes/output#transformers.modeling_tf_outputs.TFSequenceClassifierOutput"
>transformers.modeling_tf_outputs.TFSequenceClassifierOutput</a> or a tuple of <code>tf.Tensor</code> (if
<code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various elements depending on the
configuration (<a
  href="/docs/transformers/pr_15907/en/model_doc/funnel#transformers.FunnelConfig"
>FunnelConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, )</code>, <em>optional</em>, returned when <code>labels</code> is provided) \u2014 Classification (or regression if config.num_labels==1) loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, config.num_labels)</code>) \u2014 Classification (or regression if config.num_labels==1) scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`}}),$o=new ze({props:{$$slots:{default:[Ry]},$$scope:{ctx:W}}}),ga=new Oe({props:{code:`from transformers import FunnelTokenizer, TFFunnelForSequenceClassification
import tensorflow as tf

tokenizer = FunnelTokenizer.from_pretrained("funnel-transformer/small-base")
model = TFFunnelForSequenceClassification.from_pretrained("funnel-transformer/small-base")

inputs = tokenizer("Hello, my dog is cute", return_tensors="tf")
inputs["labels"] = tf.reshape(tf.constant(1), (-1, 1))  # Batch size 1

outputs = model(inputs)
loss = outputs.loss
logits = outputs.logits`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FunnelTokenizer, TFFunnelForSequenceClassification
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = FunnelTokenizer.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFFunnelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small-base&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs[<span class="hljs-string">&quot;labels&quot;</span>] = tf.reshape(tf.constant(<span class="hljs-number">1</span>), (-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>))  <span class="hljs-comment"># Batch size 1</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = outputs.loss
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits`}}),_a=new qe({}),Fa=new X({props:{name:"class transformers.TFFunnelForMultipleChoice",anchor:"transformers.TFFunnelForMultipleChoice",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15907/src/transformers/models/funnel/modeling_tf_funnel.py#L1510",parametersDescription:[{anchor:"transformers.TFFunnelForMultipleChoice.config",description:`<strong>config</strong> (<code>XxxConfig</code>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_15907/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),Mo=new ze({props:{$$slots:{default:[Hy]},$$scope:{ctx:W}}}),ya=new X({props:{name:"call",anchor:"transformers.TFFunnelForMultipleChoice.call",parameters:[{name:"input_ids",val:" = None"},{name:"attention_mask",val:" = None"},{name:"token_type_ids",val:" = None"},{name:"inputs_embeds",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"},{name:"labels",val:" = None"},{name:"training",val:" = False"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15907/src/transformers/models/funnel/modeling_tf_funnel.py#L1527",parametersDescription:[{anchor:"transformers.TFFunnelForMultipleChoice.call.input_ids",description:`<strong>input_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, num_choices, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/pr_15907/en/model_doc/funnel#transformers.FunnelTokenizer">FunnelTokenizer</a>. See <a href="/docs/transformers/pr_15907/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> and
<a href="/docs/transformers/pr_15907/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.TFFunnelForMultipleChoice.call.attention_mask",description:`<strong>attention_mask</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, num_choices, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.TFFunnelForMultipleChoice.call.token_type_ids",description:`<strong>token_type_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, num_choices, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.TFFunnelForMultipleChoice.call.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, num_choices, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.TFFunnelForMultipleChoice.call.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail. This argument can be used only in eager mode, in graph mode the value in the
config will be used instead.`,name:"output_attentions"},{anchor:"transformers.TFFunnelForMultipleChoice.call.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail. This argument can be used only in eager mode, in graph mode the value in the config will be
used instead.`,name:"output_hidden_states"},{anchor:"transformers.TFFunnelForMultipleChoice.call.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_15907/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple. This argument can be used
in eager mode, in graph mode the value will always be set to True.`,name:"return_dict"},{anchor:"transformers.TFFunnelForMultipleChoice.call.training",description:`<strong>training</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to use the model in training mode (some modules like dropout modules have different
behaviors between training and evaluation).`,name:"training"},{anchor:"transformers.TFFunnelForMultipleChoice.call.labels",description:`<strong>labels</strong> (<code>tf.Tensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for computing the multiple choice classification loss. Indices should be in <code>[0, ..., num_choices]</code>
where <code>num_choices</code> is the size of the second dimension of the input tensors. (See <code>input_ids</code> above)`,name:"labels"}],returnDescription:`
<p>A <a
  href="/docs/transformers/pr_15907/en/main_classes/output#transformers.modeling_tf_outputs.TFMultipleChoiceModelOutput"
>transformers.modeling_tf_outputs.TFMultipleChoiceModelOutput</a> or a tuple of <code>tf.Tensor</code> (if
<code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various elements depending on the
configuration (<a
  href="/docs/transformers/pr_15907/en/model_doc/funnel#transformers.FunnelConfig"
>FunnelConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>tf.Tensor</code> of shape <em>(batch_size, )</em>, <em>optional</em>, returned when <code>labels</code> is provided) \u2014 Classification loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, num_choices)</code>) \u2014 <em>num_choices</em> is the second dimension of the input tensors. (see <em>input_ids</em> above).</p>
<p>Classification scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`}}),zo=new ze({props:{$$slots:{default:[Vy]},$$scope:{ctx:W}}}),$a=new Oe({props:{code:`from transformers import FunnelTokenizer, TFFunnelForMultipleChoice
import tensorflow as tf

tokenizer = FunnelTokenizer.from_pretrained("funnel-transformer/small-base")
model = TFFunnelForMultipleChoice.from_pretrained("funnel-transformer/small-base")

prompt = "In Italy, pizza served in formal settings, such as at a restaurant, is presented unsliced."
choice0 = "It is eaten with a fork and a knife."
choice1 = "It is eaten while held in the hand."

encoding = tokenizer([prompt, prompt], [choice0, choice1], return_tensors="tf", padding=True)
inputs = {k: tf.expand_dims(v, 0) for k, v in encoding.items()}
outputs = model(inputs)  # batch size is 1

# the linear classifier still needs to be trained
logits = outputs.logits`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FunnelTokenizer, TFFunnelForMultipleChoice
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = FunnelTokenizer.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFFunnelForMultipleChoice.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small-base&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>prompt = <span class="hljs-string">&quot;In Italy, pizza served in formal settings, such as at a restaurant, is presented unsliced.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>choice0 = <span class="hljs-string">&quot;It is eaten with a fork and a knife.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>choice1 = <span class="hljs-string">&quot;It is eaten while held in the hand.&quot;</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>encoding = tokenizer([prompt, prompt], [choice0, choice1], return_tensors=<span class="hljs-string">&quot;tf&quot;</span>, padding=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = {k: tf.expand_dims(v, <span class="hljs-number">0</span>) <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> encoding.items()}
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(inputs)  <span class="hljs-comment"># batch size is 1</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># the linear classifier still needs to be trained</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits`}}),Ea=new qe({}),Ma=new X({props:{name:"class transformers.TFFunnelForTokenClassification",anchor:"transformers.TFFunnelForTokenClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15907/src/transformers/models/funnel/modeling_tf_funnel.py#L1645",parametersDescription:[{anchor:"transformers.TFFunnelForTokenClassification.config",description:`<strong>config</strong> (<code>XxxConfig</code>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_15907/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),Po=new ze({props:{$$slots:{default:[Yy]},$$scope:{ctx:W}}}),ja=new X({props:{name:"call",anchor:"transformers.TFFunnelForTokenClassification.call",parameters:[{name:"input_ids",val:" = None"},{name:"attention_mask",val:" = None"},{name:"token_type_ids",val:" = None"},{name:"inputs_embeds",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"},{name:"labels",val:" = None"},{name:"training",val:" = False"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15907/src/transformers/models/funnel/modeling_tf_funnel.py#L1656",parametersDescription:[{anchor:"transformers.TFFunnelForTokenClassification.call.input_ids",description:`<strong>input_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/pr_15907/en/model_doc/funnel#transformers.FunnelTokenizer">FunnelTokenizer</a>. See <a href="/docs/transformers/pr_15907/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> and
<a href="/docs/transformers/pr_15907/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.TFFunnelForTokenClassification.call.attention_mask",description:`<strong>attention_mask</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.TFFunnelForTokenClassification.call.token_type_ids",description:`<strong>token_type_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.TFFunnelForTokenClassification.call.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.TFFunnelForTokenClassification.call.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail. This argument can be used only in eager mode, in graph mode the value in the
config will be used instead.`,name:"output_attentions"},{anchor:"transformers.TFFunnelForTokenClassification.call.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail. This argument can be used only in eager mode, in graph mode the value in the config will be
used instead.`,name:"output_hidden_states"},{anchor:"transformers.TFFunnelForTokenClassification.call.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_15907/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple. This argument can be used
in eager mode, in graph mode the value will always be set to True.`,name:"return_dict"},{anchor:"transformers.TFFunnelForTokenClassification.call.training",description:`<strong>training</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to use the model in training mode (some modules like dropout modules have different
behaviors between training and evaluation).`,name:"training"},{anchor:"transformers.TFFunnelForTokenClassification.call.labels",description:`<strong>labels</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Labels for computing the token classification loss. Indices should be in <code>[0, ..., config.num_labels - 1]</code>.`,name:"labels"}],returnDescription:`
<p>A <a
  href="/docs/transformers/pr_15907/en/main_classes/output#transformers.modeling_tf_outputs.TFTokenClassifierOutput"
>transformers.modeling_tf_outputs.TFTokenClassifierOutput</a> or a tuple of <code>tf.Tensor</code> (if
<code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various elements depending on the
configuration (<a
  href="/docs/transformers/pr_15907/en/model_doc/funnel#transformers.FunnelConfig"
>FunnelConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>tf.Tensor</code> of shape <code>(n,)</code>, <em>optional</em>, where n is the number of unmasked labels, returned when <code>labels</code> is provided)  \u2014 Classification loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, config.num_labels)</code>) \u2014 Classification scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`}}),Co=new ze({props:{$$slots:{default:[Uy]},$$scope:{ctx:W}}}),La=new Oe({props:{code:`from transformers import FunnelTokenizer, TFFunnelForTokenClassification
import tensorflow as tf

tokenizer = FunnelTokenizer.from_pretrained("funnel-transformer/small")
model = TFFunnelForTokenClassification.from_pretrained("funnel-transformer/small")

inputs = tokenizer("Hello, my dog is cute", return_tensors="tf")
input_ids = inputs["input_ids"]
inputs["labels"] = tf.reshape(
    tf.constant([1] * tf.size(input_ids).numpy()), (-1, tf.size(input_ids))
)  # Batch size 1

outputs = model(inputs)
loss = outputs.loss
logits = outputs.logits`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FunnelTokenizer, TFFunnelForTokenClassification
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = FunnelTokenizer.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFFunnelForTokenClassification.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = inputs[<span class="hljs-string">&quot;input_ids&quot;</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs[<span class="hljs-string">&quot;labels&quot;</span>] = tf.reshape(
<span class="hljs-meta">... </span>    tf.constant([<span class="hljs-number">1</span>] * tf.size(input_ids).numpy()), (-<span class="hljs-number">1</span>, tf.size(input_ids))
<span class="hljs-meta">&gt;&gt;&gt; </span>)  <span class="hljs-comment"># Batch size 1</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = outputs.loss
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits`}}),Aa=new qe({}),Da=new X({props:{name:"class transformers.TFFunnelForQuestionAnswering",anchor:"transformers.TFFunnelForQuestionAnswering",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15907/src/transformers/models/funnel/modeling_tf_funnel.py#L1737",parametersDescription:[{anchor:"transformers.TFFunnelForQuestionAnswering.config",description:`<strong>config</strong> (<code>XxxConfig</code>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_15907/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),jo=new ze({props:{$$slots:{default:[Gy]},$$scope:{ctx:W}}}),Wa=new X({props:{name:"call",anchor:"transformers.TFFunnelForQuestionAnswering.call",parameters:[{name:"input_ids",val:" = None"},{name:"attention_mask",val:" = None"},{name:"token_type_ids",val:" = None"},{name:"inputs_embeds",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"},{name:"start_positions",val:" = None"},{name:"end_positions",val:" = None"},{name:"training",val:" = False"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15907/src/transformers/models/funnel/modeling_tf_funnel.py#L1747",parametersDescription:[{anchor:"transformers.TFFunnelForQuestionAnswering.call.input_ids",description:`<strong>input_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/pr_15907/en/model_doc/funnel#transformers.FunnelTokenizer">FunnelTokenizer</a>. See <a href="/docs/transformers/pr_15907/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> and
<a href="/docs/transformers/pr_15907/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.TFFunnelForQuestionAnswering.call.attention_mask",description:`<strong>attention_mask</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.TFFunnelForQuestionAnswering.call.token_type_ids",description:`<strong>token_type_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.TFFunnelForQuestionAnswering.call.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.TFFunnelForQuestionAnswering.call.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail. This argument can be used only in eager mode, in graph mode the value in the
config will be used instead.`,name:"output_attentions"},{anchor:"transformers.TFFunnelForQuestionAnswering.call.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail. This argument can be used only in eager mode, in graph mode the value in the config will be
used instead.`,name:"output_hidden_states"},{anchor:"transformers.TFFunnelForQuestionAnswering.call.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_15907/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple. This argument can be used
in eager mode, in graph mode the value will always be set to True.`,name:"return_dict"},{anchor:"transformers.TFFunnelForQuestionAnswering.call.training",description:`<strong>training</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to use the model in training mode (some modules like dropout modules have different
behaviors between training and evaluation).`,name:"training"},{anchor:"transformers.TFFunnelForQuestionAnswering.call.start_positions",description:`<strong>start_positions</strong> (<code>tf.Tensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for position (index) of the start of the labelled span for computing the token classification loss.
Positions are clamped to the length of the sequence (<code>sequence_length</code>). Position outside of the sequence
are not taken into account for computing the loss.`,name:"start_positions"},{anchor:"transformers.TFFunnelForQuestionAnswering.call.end_positions",description:`<strong>end_positions</strong> (<code>tf.Tensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for position (index) of the end of the labelled span for computing the token classification loss.
Positions are clamped to the length of the sequence (<code>sequence_length</code>). Position outside of the sequence
are not taken into account for computing the loss.`,name:"end_positions"}],returnDescription:`
<p>A <a
  href="/docs/transformers/pr_15907/en/main_classes/output#transformers.modeling_tf_outputs.TFQuestionAnsweringModelOutput"
>transformers.modeling_tf_outputs.TFQuestionAnsweringModelOutput</a> or a tuple of <code>tf.Tensor</code> (if
<code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various elements depending on the
configuration (<a
  href="/docs/transformers/pr_15907/en/model_doc/funnel#transformers.FunnelConfig"
>FunnelConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, )</code>, <em>optional</em>, returned when <code>start_positions</code> and <code>end_positions</code> are provided) \u2014 Total span extraction loss is the sum of a Cross-Entropy for the start and end positions.</p>
</li>
<li>
<p><strong>start_logits</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) \u2014 Span-start scores (before SoftMax).</p>
</li>
<li>
<p><strong>end_logits</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) \u2014 Span-end scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`}}),Lo=new ze({props:{$$slots:{default:[Zy]},$$scope:{ctx:W}}}),Qa=new Oe({props:{code:`from transformers import FunnelTokenizer, TFFunnelForQuestionAnswering
import tensorflow as tf

tokenizer = FunnelTokenizer.from_pretrained("funnel-transformer/small")
model = TFFunnelForQuestionAnswering.from_pretrained("funnel-transformer/small")

question, text = "Who was Jim Henson?", "Jim Henson was a nice puppet"
input_dict = tokenizer(question, text, return_tensors="tf")
outputs = model(input_dict)
start_logits = outputs.start_logits
end_logits = outputs.end_logits

all_tokens = tokenizer.convert_ids_to_tokens(input_dict["input_ids"].numpy()[0])
answer = " ".join(all_tokens[tf.math.argmax(start_logits, 1)[0] : tf.math.argmax(end_logits, 1)[0] + 1])`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FunnelTokenizer, TFFunnelForQuestionAnswering
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = FunnelTokenizer.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFFunnelForQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>question, text = <span class="hljs-string">&quot;Who was Jim Henson?&quot;</span>, <span class="hljs-string">&quot;Jim Henson was a nice puppet&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>input_dict = tokenizer(question, text, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(input_dict)
<span class="hljs-meta">&gt;&gt;&gt; </span>start_logits = outputs.start_logits
<span class="hljs-meta">&gt;&gt;&gt; </span>end_logits = outputs.end_logits

<span class="hljs-meta">&gt;&gt;&gt; </span>all_tokens = tokenizer.convert_ids_to_tokens(input_dict[<span class="hljs-string">&quot;input_ids&quot;</span>].numpy()[<span class="hljs-number">0</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span>answer = <span class="hljs-string">&quot; &quot;</span>.join(all_tokens[tf.math.argmax(start_logits, <span class="hljs-number">1</span>)[<span class="hljs-number">0</span>] : tf.math.argmax(end_logits, <span class="hljs-number">1</span>)[<span class="hljs-number">0</span>] + <span class="hljs-number">1</span>])`}}),{c(){p=r("meta"),M=l(),m=r("h1"),g=r("a"),v=r("span"),k(F.$$.fragment),_=l(),z=r("span"),ce=t("Funnel Transformer"),G=l(),q=r("h2"),J=r("a"),I=r("span"),k(ne.$$.fragment),ue=l(),S=r("span"),pe=t("Overview"),ie=l(),U=r("p"),L=t("The Funnel Transformer model was proposed in the paper "),te=r("a"),Z=t(`Funnel-Transformer: Filtering out Sequential Redundancy for
Efficient Language Processing`),P=t(`. It is a bidirectional transformer model, like
BERT, but with a pooling operation after each block of layers, a bit like in traditional convolutional neural networks
(CNN) in computer vision.`),x=l(),oe=r("p"),Q=t("The abstract from the paper is the following:"),le=l(),se=r("p"),N=r("em"),he=t(`With the success of language pretraining, it is highly desirable to develop more efficient architectures of good
scalability that can exploit the abundant unlabeled data at a lower cost. To improve the efficiency, we examine the
much-overlooked redundancy in maintaining a full-length token-level presentation, especially for tasks that only
require a single-vector presentation of the sequence. With this intuition, we propose Funnel-Transformer which
gradually compresses the sequence of hidden states to a shorter one and hence reduces the computation cost. More
importantly, by re-investing the saved FLOPs from length reduction in constructing a deeper or wider model, we further
improve the model capacity. In addition, to perform token-level predictions as required by common pretraining
objectives, Funnel-Transformer is able to recover a deep representation for each token from the reduced hidden sequence
via a decoder. Empirically, with comparable or fewer FLOPs, Funnel-Transformer outperforms the standard Transformer on
a wide variety of sequence-level prediction tasks, including text classification, language understanding, and reading
comprehension.`),de=l(),C=r("p"),fe=t("Tips:"),B=l(),ee=r("ul"),ae=r("li"),R=t(`Since Funnel Transformer uses pooling, the sequence length of the hidden states changes after each block of layers.
The base model therefore has a final sequence length that is a quarter of the original one. This model can be used
directly for tasks that just require a sentence summary (like sequence classification or multiple choice). For other
tasks, the full model is used; this full model has a decoder that upsamples the final hidden states to the same
sequence length as the input.`),me=l(),O=r("li"),A=t(`The Funnel Transformer checkpoints are all available with a full version and a base version. The first ones should be
used for `),re=r("a"),H=t("FunnelModel"),ge=t(", "),u=r("a"),T=t("FunnelForPreTraining"),K=t(`,
`),Fe=r("a"),we=t("FunnelForMaskedLM"),D=t(", "),ve=r("a"),be=t("FunnelForTokenClassification"),ye=t(` and
class:`),j=r("em"),V=t("~transformers.FunnelForQuestionAnswering"),$e=t(`. The second ones should be used for
`),Te=r("a"),Y=t("FunnelBaseModel"),Ee=t(", "),ke=r("a"),_e=t("FunnelForSequenceClassification"),Me=t(` and
`),Ua=r("a"),Np=t("FunnelForMultipleChoice"),Op=t("."),Ec=l(),jn=r("p"),Bp=t("This model was contributed by "),So=r("a"),Wp=t("sgugger"),Qp=t(". The original code can be found "),No=r("a"),Rp=t("here"),Hp=t("."),Mc=l(),Kn=r("h2"),Bt=r("a"),ul=r("span"),k(Oo.$$.fragment),Vp=l(),pl=r("span"),Yp=t("FunnelConfig"),zc=l(),Cn=r("div"),k(Bo.$$.fragment),Up=l(),xn=r("p"),Gp=t("This is the configuration class to store the configuration of a "),Ga=r("a"),Zp=t("FunnelModel"),Kp=t(" or a "),Za=r("a"),Xp=t("TFBertModel"),Jp=t(`. It is used to
instantiate a Funnel Transformer model according to the specified arguments, defining the model architecture.
Instantiating a configuration with the defaults will yield a similar configuration to that of the Funnel
Transformer `),Wo=r("a"),eh=t("funnel-transformer/small"),nh=t(" architecture."),th=l(),Xn=r("p"),oh=t("Configuration objects inherit from "),Ka=r("a"),sh=t("PretrainedConfig"),rh=t(` and can be used to control the model outputs. Read the
documentation from `),Xa=r("a"),ah=t("PretrainedConfig"),ih=t(" for more information."),qc=l(),Jn=r("h2"),Wt=r("a"),hl=r("span"),k(Qo.$$.fragment),lh=l(),fl=r("span"),dh=t("FunnelTokenizer"),Pc=l(),Pe=r("div"),k(Ro.$$.fragment),ch=l(),ml=r("p"),uh=t("Construct a Funnel Transformer tokenizer."),ph=l(),Qt=r("p"),Ja=r("a"),hh=t("FunnelTokenizer"),fh=t(" is identical to "),ei=r("a"),mh=t("BertTokenizer"),gh=t(` and runs end-to-end tokenization: punctuation splitting and
wordpiece.`),_h=l(),Ho=r("p"),Fh=t("Refer to superclass "),ni=r("a"),vh=t("BertTokenizer"),Th=t(" for usage examples and documentation concerning parameters."),kh=l(),Ln=r("div"),k(Vo.$$.fragment),wh=l(),gl=r("p"),bh=t(`Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens. A BERT sequence has the following format:`),yh=l(),Yo=r("ul"),ti=r("li"),$h=t("single sequence: "),_l=r("code"),Eh=t("[CLS] X [SEP]"),Mh=l(),oi=r("li"),zh=t("pair of sequences: "),Fl=r("code"),qh=t("[CLS] A [SEP] B [SEP]"),Ph=l(),Rt=r("div"),k(Uo.$$.fragment),Ch=l(),Go=r("p"),xh=t(`Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding
special tokens using the tokenizer `),vl=r("code"),jh=t("prepare_for_model"),Lh=t(" method."),Ah=l(),wn=r("div"),k(Zo.$$.fragment),Dh=l(),Tl=r("p"),Ih=t(`Create a mask from the two sequences passed to be used in a sequence-pair classification task. A Funnel
Transformer sequence pair mask has the following format:`),Sh=l(),k(Ko.$$.fragment),Nh=l(),et=r("p"),Oh=t("If "),kl=r("code"),Bh=t("token_ids_1"),Wh=t(" is "),wl=r("code"),Qh=t("None"),Rh=t(", this method only returns the first portion of the mask (0s)."),Hh=l(),si=r("div"),k(Xo.$$.fragment),Cc=l(),nt=r("h2"),Ht=r("a"),bl=r("span"),k(Jo.$$.fragment),Vh=l(),yl=r("span"),Yh=t("FunnelTokenizerFast"),xc=l(),Ze=r("div"),k(es.$$.fragment),Uh=l(),ns=r("p"),Gh=t("Construct a \u201Cfast\u201D Funnel Transformer tokenizer (backed by HuggingFace\u2019s "),$l=r("em"),Zh=t("tokenizers"),Kh=t(" library)."),Xh=l(),Vt=r("p"),ri=r("a"),Jh=t("FunnelTokenizerFast"),ef=t(" is identical to "),ai=r("a"),nf=t("BertTokenizerFast"),tf=t(` and runs end-to-end tokenization: punctuation
splitting and wordpiece.`),of=l(),ts=r("p"),sf=t("Refer to superclass "),ii=r("a"),rf=t("BertTokenizerFast"),af=t(" for usage examples and documentation concerning parameters."),lf=l(),bn=r("div"),k(os.$$.fragment),df=l(),El=r("p"),cf=t(`Create a mask from the two sequences passed to be used in a sequence-pair classification task. A Funnel
Transformer sequence pair mask has the following format:`),uf=l(),k(ss.$$.fragment),pf=l(),tt=r("p"),hf=t("If "),Ml=r("code"),ff=t("token_ids_1"),mf=t(" is "),zl=r("code"),gf=t("None"),_f=t(", this method only returns the first portion of the mask (0s)."),jc=l(),ot=r("h2"),Yt=r("a"),ql=r("span"),k(rs.$$.fragment),Ff=l(),Pl=r("span"),vf=t("Funnel specific outputs"),Lc=l(),st=r("div"),k(as.$$.fragment),Tf=l(),is=r("p"),kf=t("Output type of "),li=r("a"),wf=t("FunnelForPreTraining"),bf=t("."),Ac=l(),rt=r("div"),k(ls.$$.fragment),yf=l(),ds=r("p"),$f=t("Output type of "),di=r("a"),Ef=t("FunnelForPreTraining"),Mf=t("."),Dc=l(),at=r("h2"),Ut=r("a"),Cl=r("span"),k(cs.$$.fragment),zf=l(),xl=r("span"),qf=t("FunnelBaseModel"),Ic=l(),We=r("div"),k(us.$$.fragment),Pf=l(),jl=r("p"),Cf=t(`The base Funnel Transformer Model transformer outputting raw hidden-states without upsampling head (also called
decoder) or any task-specific head on top.`),xf=l(),ps=r("p"),jf=t("The Funnel Transformer model was proposed in "),hs=r("a"),Lf=t(`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),Af=t(" by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),Df=l(),fs=r("p"),If=t("This model inherits from "),ci=r("a"),Sf=t("PreTrainedModel"),Nf=t(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Of=l(),ms=r("p"),Bf=t("This model is also a PyTorch "),gs=r("a"),Wf=t("torch.nn.Module"),Qf=t(` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),Rf=l(),Ke=r("div"),k(_s.$$.fragment),Hf=l(),it=r("p"),Vf=t("The "),ui=r("a"),Yf=t("FunnelBaseModel"),Uf=t(" forward method, overrides the "),Ll=r("code"),Gf=t("__call__"),Zf=t(" special method."),Kf=l(),k(Gt.$$.fragment),Xf=l(),Al=r("p"),Jf=t("Example:"),em=l(),k(Fs.$$.fragment),Sc=l(),lt=r("h2"),Zt=r("a"),Dl=r("span"),k(vs.$$.fragment),nm=l(),Il=r("span"),tm=t("FunnelModel"),Nc=l(),Qe=r("div"),k(Ts.$$.fragment),om=l(),Sl=r("p"),sm=t("The bare Funnel Transformer Model transformer outputting raw hidden-states without any specific head on top."),rm=l(),ks=r("p"),am=t("The Funnel Transformer model was proposed in "),ws=r("a"),im=t(`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),lm=t(" by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),dm=l(),bs=r("p"),cm=t("This model inherits from "),pi=r("a"),um=t("PreTrainedModel"),pm=t(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),hm=l(),ys=r("p"),fm=t("This model is also a PyTorch "),$s=r("a"),mm=t("torch.nn.Module"),gm=t(` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),_m=l(),Xe=r("div"),k(Es.$$.fragment),Fm=l(),dt=r("p"),vm=t("The "),hi=r("a"),Tm=t("FunnelModel"),km=t(" forward method, overrides the "),Nl=r("code"),wm=t("__call__"),bm=t(" special method."),ym=l(),k(Kt.$$.fragment),$m=l(),Ol=r("p"),Em=t("Example:"),Mm=l(),k(Ms.$$.fragment),Oc=l(),ct=r("h2"),Xt=r("a"),Bl=r("span"),k(zs.$$.fragment),zm=l(),Wl=r("span"),qm=t("FunnelModelForPreTraining"),Bc=l(),ut=r("div"),k(qs.$$.fragment),Pm=l(),Je=r("div"),k(Ps.$$.fragment),Cm=l(),pt=r("p"),xm=t("The "),fi=r("a"),jm=t("FunnelForPreTraining"),Lm=t(" forward method, overrides the "),Ql=r("code"),Am=t("__call__"),Dm=t(" special method."),Im=l(),k(Jt.$$.fragment),Sm=l(),Rl=r("p"),Nm=t("Examples:"),Om=l(),k(Cs.$$.fragment),Wc=l(),ht=r("h2"),eo=r("a"),Hl=r("span"),k(xs.$$.fragment),Bm=l(),Vl=r("span"),Wm=t("FunnelForMaskedLM"),Qc=l(),Re=r("div"),k(js.$$.fragment),Qm=l(),Ls=r("p"),Rm=t("Funnel Transformer Model with a "),Yl=r("code"),Hm=t("language modeling"),Vm=t(" head on top."),Ym=l(),As=r("p"),Um=t("The Funnel Transformer model was proposed in "),Ds=r("a"),Gm=t(`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),Zm=t(" by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),Km=l(),Is=r("p"),Xm=t("This model inherits from "),mi=r("a"),Jm=t("PreTrainedModel"),eg=t(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),ng=l(),Ss=r("p"),tg=t("This model is also a PyTorch "),Ns=r("a"),og=t("torch.nn.Module"),sg=t(` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),rg=l(),en=r("div"),k(Os.$$.fragment),ag=l(),ft=r("p"),ig=t("The "),gi=r("a"),lg=t("FunnelForMaskedLM"),dg=t(" forward method, overrides the "),Ul=r("code"),cg=t("__call__"),ug=t(" special method."),pg=l(),k(no.$$.fragment),hg=l(),Gl=r("p"),fg=t("Example:"),mg=l(),k(Bs.$$.fragment),Rc=l(),mt=r("h2"),to=r("a"),Zl=r("span"),k(Ws.$$.fragment),gg=l(),Kl=r("span"),_g=t("FunnelForSequenceClassification"),Hc=l(),He=r("div"),k(Qs.$$.fragment),Fg=l(),Xl=r("p"),vg=t(`Funnel Transformer Model with a sequence classification/regression head on top (two linear layer on top of the
first timestep of the last hidden state) e.g. for GLUE tasks.`),Tg=l(),Rs=r("p"),kg=t("The Funnel Transformer model was proposed in "),Hs=r("a"),wg=t(`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),bg=t(" by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),yg=l(),Vs=r("p"),$g=t("This model inherits from "),_i=r("a"),Eg=t("PreTrainedModel"),Mg=t(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),zg=l(),Ys=r("p"),qg=t("This model is also a PyTorch "),Us=r("a"),Pg=t("torch.nn.Module"),Cg=t(` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),xg=l(),Be=r("div"),k(Gs.$$.fragment),jg=l(),gt=r("p"),Lg=t("The "),Fi=r("a"),Ag=t("FunnelForSequenceClassification"),Dg=t(" forward method, overrides the "),Jl=r("code"),Ig=t("__call__"),Sg=t(" special method."),Ng=l(),k(oo.$$.fragment),Og=l(),ed=r("p"),Bg=t("Example of single-label classification:"),Wg=l(),k(Zs.$$.fragment),Qg=l(),nd=r("p"),Rg=t("Example of multi-label classification:"),Hg=l(),k(Ks.$$.fragment),Vc=l(),_t=r("h2"),so=r("a"),td=r("span"),k(Xs.$$.fragment),Vg=l(),od=r("span"),Yg=t("FunnelForMultipleChoice"),Yc=l(),Ve=r("div"),k(Js.$$.fragment),Ug=l(),sd=r("p"),Gg=t(`Funnel Transformer Model with a multiple choice classification head on top (two linear layer on top of the first
timestep of the last hidden state, and a softmax) e.g. for RocStories/SWAG tasks.`),Zg=l(),er=r("p"),Kg=t("The Funnel Transformer model was proposed in "),nr=r("a"),Xg=t(`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),Jg=t(" by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),e_=l(),tr=r("p"),n_=t("This model inherits from "),vi=r("a"),t_=t("PreTrainedModel"),o_=t(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),s_=l(),or=r("p"),r_=t("This model is also a PyTorch "),sr=r("a"),a_=t("torch.nn.Module"),i_=t(` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),l_=l(),nn=r("div"),k(rr.$$.fragment),d_=l(),Ft=r("p"),c_=t("The "),Ti=r("a"),u_=t("FunnelForMultipleChoice"),p_=t(" forward method, overrides the "),rd=r("code"),h_=t("__call__"),f_=t(" special method."),m_=l(),k(ro.$$.fragment),g_=l(),ad=r("p"),__=t("Example:"),F_=l(),k(ar.$$.fragment),Uc=l(),vt=r("h2"),ao=r("a"),id=r("span"),k(ir.$$.fragment),v_=l(),ld=r("span"),T_=t("FunnelForTokenClassification"),Gc=l(),Ye=r("div"),k(lr.$$.fragment),k_=l(),dd=r("p"),w_=t(`Funnel Transformer Model with a token classification head on top (a linear layer on top of the hidden-states
output) e.g. for Named-Entity-Recognition (NER) tasks.`),b_=l(),dr=r("p"),y_=t("The Funnel Transformer model was proposed in "),cr=r("a"),$_=t(`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),E_=t(" by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),M_=l(),ur=r("p"),z_=t("This model inherits from "),ki=r("a"),q_=t("PreTrainedModel"),P_=t(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),C_=l(),pr=r("p"),x_=t("This model is also a PyTorch "),hr=r("a"),j_=t("torch.nn.Module"),L_=t(` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),A_=l(),tn=r("div"),k(fr.$$.fragment),D_=l(),Tt=r("p"),I_=t("The "),wi=r("a"),S_=t("FunnelForTokenClassification"),N_=t(" forward method, overrides the "),cd=r("code"),O_=t("__call__"),B_=t(" special method."),W_=l(),k(io.$$.fragment),Q_=l(),ud=r("p"),R_=t("Example:"),H_=l(),k(mr.$$.fragment),Zc=l(),kt=r("h2"),lo=r("a"),pd=r("span"),k(gr.$$.fragment),V_=l(),hd=r("span"),Y_=t("FunnelForQuestionAnswering"),Kc=l(),Ue=r("div"),k(_r.$$.fragment),U_=l(),wt=r("p"),G_=t(`Funnel Transformer Model with a span classification head on top for extractive question-answering tasks like SQuAD
(a linear layer on top of the hidden-states output to compute `),fd=r("code"),Z_=t("span start logits"),K_=t(" and "),md=r("code"),X_=t("span end logits"),J_=t(")."),eF=l(),Fr=r("p"),nF=t("The Funnel Transformer model was proposed in "),vr=r("a"),tF=t(`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),oF=t(" by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),sF=l(),Tr=r("p"),rF=t("This model inherits from "),bi=r("a"),aF=t("PreTrainedModel"),iF=t(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),lF=l(),kr=r("p"),dF=t("This model is also a PyTorch "),wr=r("a"),cF=t("torch.nn.Module"),uF=t(` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),pF=l(),on=r("div"),k(br.$$.fragment),hF=l(),bt=r("p"),fF=t("The "),yi=r("a"),mF=t("FunnelForQuestionAnswering"),gF=t(" forward method, overrides the "),gd=r("code"),_F=t("__call__"),FF=t(" special method."),vF=l(),k(co.$$.fragment),TF=l(),_d=r("p"),kF=t("Example:"),wF=l(),k(yr.$$.fragment),Xc=l(),yt=r("h2"),uo=r("a"),Fd=r("span"),k($r.$$.fragment),bF=l(),vd=r("span"),yF=t("TFFunnelBaseModel"),Jc=l(),xe=r("div"),k(Er.$$.fragment),$F=l(),Td=r("p"),EF=t(`The base Funnel Transformer Model transformer outputting raw hidden-states without upsampling head (also called
decoder) or any task-specific head on top.`),MF=l(),Mr=r("p"),zF=t("The Funnel Transformer model was proposed in "),zr=r("a"),qF=t(`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),PF=t(" by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),CF=l(),qr=r("p"),xF=t("This model inherits from "),$i=r("a"),jF=t("TFPreTrainedModel"),LF=t(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),AF=l(),Pr=r("p"),DF=t("This model is also a "),Cr=r("a"),IF=t("tf.keras.Model"),SF=t(` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),NF=l(),k(po.$$.fragment),OF=l(),sn=r("div"),k(xr.$$.fragment),BF=l(),$t=r("p"),WF=t("The "),Ei=r("a"),QF=t("TFFunnelBaseModel"),RF=t(" forward method, overrides the "),kd=r("code"),HF=t("__call__"),VF=t(" special method."),YF=l(),k(ho.$$.fragment),UF=l(),wd=r("p"),GF=t("Example:"),ZF=l(),k(jr.$$.fragment),eu=l(),Et=r("h2"),fo=r("a"),bd=r("span"),k(Lr.$$.fragment),KF=l(),yd=r("span"),XF=t("TFFunnelModel"),nu=l(),je=r("div"),k(Ar.$$.fragment),JF=l(),$d=r("p"),ev=t("The bare Funnel Transformer Model transformer outputting raw hidden-states without any specific head on top."),nv=l(),Dr=r("p"),tv=t("The Funnel Transformer model was proposed in "),Ir=r("a"),ov=t(`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),sv=t(" by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),rv=l(),Sr=r("p"),av=t("This model inherits from "),Mi=r("a"),iv=t("TFPreTrainedModel"),lv=t(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),dv=l(),Nr=r("p"),cv=t("This model is also a "),Or=r("a"),uv=t("tf.keras.Model"),pv=t(` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),hv=l(),k(mo.$$.fragment),fv=l(),rn=r("div"),k(Br.$$.fragment),mv=l(),Mt=r("p"),gv=t("The "),zi=r("a"),_v=t("TFFunnelModel"),Fv=t(" forward method, overrides the "),Ed=r("code"),vv=t("__call__"),Tv=t(" special method."),kv=l(),k(go.$$.fragment),wv=l(),Md=r("p"),bv=t("Example:"),yv=l(),k(Wr.$$.fragment),tu=l(),zt=r("h2"),_o=r("a"),zd=r("span"),k(Qr.$$.fragment),$v=l(),qd=r("span"),Ev=t("TFFunnelModelForPreTraining"),ou=l(),Le=r("div"),k(Rr.$$.fragment),Mv=l(),Pd=r("p"),zv=t("Funnel model with a binary classification head on top as used during pretraining for identifying generated tokens."),qv=l(),Hr=r("p"),Pv=t("The Funnel Transformer model was proposed in "),Vr=r("a"),Cv=t(`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),xv=t(" by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),jv=l(),Yr=r("p"),Lv=t("This model inherits from "),qi=r("a"),Av=t("TFPreTrainedModel"),Dv=t(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Iv=l(),Ur=r("p"),Sv=t("This model is also a "),Gr=r("a"),Nv=t("tf.keras.Model"),Ov=t(` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),Bv=l(),k(Fo.$$.fragment),Wv=l(),an=r("div"),k(Zr.$$.fragment),Qv=l(),qt=r("p"),Rv=t("The "),Pi=r("a"),Hv=t("TFFunnelForPreTraining"),Vv=t(" forward method, overrides the "),Cd=r("code"),Yv=t("__call__"),Uv=t(" special method."),Gv=l(),k(vo.$$.fragment),Zv=l(),xd=r("p"),Kv=t("Examples:"),Xv=l(),k(Kr.$$.fragment),su=l(),Pt=r("h2"),To=r("a"),jd=r("span"),k(Xr.$$.fragment),Jv=l(),Ld=r("span"),eT=t("TFFunnelForMaskedLM"),ru=l(),Ae=r("div"),k(Jr.$$.fragment),nT=l(),ea=r("p"),tT=t("Funnel Model with a "),Ad=r("code"),oT=t("language modeling"),sT=t(" head on top."),rT=l(),na=r("p"),aT=t("The Funnel Transformer model was proposed in "),ta=r("a"),iT=t(`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),lT=t(" by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),dT=l(),oa=r("p"),cT=t("This model inherits from "),Ci=r("a"),uT=t("TFPreTrainedModel"),pT=t(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),hT=l(),sa=r("p"),fT=t("This model is also a "),ra=r("a"),mT=t("tf.keras.Model"),gT=t(` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),_T=l(),k(ko.$$.fragment),FT=l(),ln=r("div"),k(aa.$$.fragment),vT=l(),Ct=r("p"),TT=t("The "),xi=r("a"),kT=t("TFFunnelForMaskedLM"),wT=t(" forward method, overrides the "),Dd=r("code"),bT=t("__call__"),yT=t(" special method."),$T=l(),k(wo.$$.fragment),ET=l(),Id=r("p"),MT=t("Example:"),zT=l(),k(ia.$$.fragment),au=l(),xt=r("h2"),bo=r("a"),Sd=r("span"),k(la.$$.fragment),qT=l(),Nd=r("span"),PT=t("TFFunnelForSequenceClassification"),iu=l(),De=r("div"),k(da.$$.fragment),CT=l(),Od=r("p"),xT=t(`Funnel Model transformer with a sequence classification/regression head on top (a linear layer on top of the pooled
output) e.g. for GLUE tasks.`),jT=l(),ca=r("p"),LT=t("The Funnel Transformer model was proposed in "),ua=r("a"),AT=t(`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),DT=t(" by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),IT=l(),pa=r("p"),ST=t("This model inherits from "),ji=r("a"),NT=t("TFPreTrainedModel"),OT=t(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),BT=l(),ha=r("p"),WT=t("This model is also a "),fa=r("a"),QT=t("tf.keras.Model"),RT=t(` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),HT=l(),k(yo.$$.fragment),VT=l(),dn=r("div"),k(ma.$$.fragment),YT=l(),jt=r("p"),UT=t("The "),Li=r("a"),GT=t("TFFunnelForSequenceClassification"),ZT=t(" forward method, overrides the "),Bd=r("code"),KT=t("__call__"),XT=t(" special method."),JT=l(),k($o.$$.fragment),ek=l(),Wd=r("p"),nk=t("Example:"),tk=l(),k(ga.$$.fragment),lu=l(),Lt=r("h2"),Eo=r("a"),Qd=r("span"),k(_a.$$.fragment),ok=l(),Rd=r("span"),sk=t("TFFunnelForMultipleChoice"),du=l(),Ie=r("div"),k(Fa.$$.fragment),rk=l(),Hd=r("p"),ak=t(`Funnel Model with a multiple choice classification head on top (a linear layer on top of the pooled output and a
softmax) e.g. for RocStories/SWAG tasks.`),ik=l(),va=r("p"),lk=t("The Funnel Transformer model was proposed in "),Ta=r("a"),dk=t(`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),ck=t(" by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),uk=l(),ka=r("p"),pk=t("This model inherits from "),Ai=r("a"),hk=t("TFPreTrainedModel"),fk=t(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),mk=l(),wa=r("p"),gk=t("This model is also a "),ba=r("a"),_k=t("tf.keras.Model"),Fk=t(` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),vk=l(),k(Mo.$$.fragment),Tk=l(),cn=r("div"),k(ya.$$.fragment),kk=l(),At=r("p"),wk=t("The "),Di=r("a"),bk=t("TFFunnelForMultipleChoice"),yk=t(" forward method, overrides the "),Vd=r("code"),$k=t("__call__"),Ek=t(" special method."),Mk=l(),k(zo.$$.fragment),zk=l(),Yd=r("p"),qk=t("Example:"),Pk=l(),k($a.$$.fragment),cu=l(),Dt=r("h2"),qo=r("a"),Ud=r("span"),k(Ea.$$.fragment),Ck=l(),Gd=r("span"),xk=t("TFFunnelForTokenClassification"),uu=l(),Se=r("div"),k(Ma.$$.fragment),jk=l(),Zd=r("p"),Lk=t(`Funnel Model with a token classification head on top (a linear layer on top of the hidden-states output) e.g. for
Named-Entity-Recognition (NER) tasks.`),Ak=l(),za=r("p"),Dk=t("The Funnel Transformer model was proposed in "),qa=r("a"),Ik=t(`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),Sk=t(" by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),Nk=l(),Pa=r("p"),Ok=t("This model inherits from "),Ii=r("a"),Bk=t("TFPreTrainedModel"),Wk=t(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Qk=l(),Ca=r("p"),Rk=t("This model is also a "),xa=r("a"),Hk=t("tf.keras.Model"),Vk=t(` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),Yk=l(),k(Po.$$.fragment),Uk=l(),un=r("div"),k(ja.$$.fragment),Gk=l(),It=r("p"),Zk=t("The "),Si=r("a"),Kk=t("TFFunnelForTokenClassification"),Xk=t(" forward method, overrides the "),Kd=r("code"),Jk=t("__call__"),e1=t(" special method."),n1=l(),k(Co.$$.fragment),t1=l(),Xd=r("p"),o1=t("Example:"),s1=l(),k(La.$$.fragment),pu=l(),St=r("h2"),xo=r("a"),Jd=r("span"),k(Aa.$$.fragment),r1=l(),ec=r("span"),a1=t("TFFunnelForQuestionAnswering"),hu=l(),Ne=r("div"),k(Da.$$.fragment),i1=l(),Nt=r("p"),l1=t(`Funnel Model with a span classification head on top for extractive question-answering tasks like SQuAD (a linear
layers on top of the hidden-states output to compute `),nc=r("code"),d1=t("span start logits"),c1=t(" and "),tc=r("code"),u1=t("span end logits"),p1=t(")."),h1=l(),Ia=r("p"),f1=t("The Funnel Transformer model was proposed in "),Sa=r("a"),m1=t(`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),g1=t(" by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),_1=l(),Na=r("p"),F1=t("This model inherits from "),Ni=r("a"),v1=t("TFPreTrainedModel"),T1=t(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),k1=l(),Oa=r("p"),w1=t("This model is also a "),Ba=r("a"),b1=t("tf.keras.Model"),y1=t(` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),$1=l(),k(jo.$$.fragment),E1=l(),pn=r("div"),k(Wa.$$.fragment),M1=l(),Ot=r("p"),z1=t("The "),Oi=r("a"),q1=t("TFFunnelForQuestionAnswering"),P1=t(" forward method, overrides the "),oc=r("code"),C1=t("__call__"),x1=t(" special method."),j1=l(),k(Lo.$$.fragment),L1=l(),sc=r("p"),A1=t("Example:"),D1=l(),k(Qa.$$.fragment),this.h()},l(s){const f=Ey('[data-svelte="svelte-1phssyn"]',document.head);p=a(f,"META",{name:!0,content:!0}),f.forEach(n),M=d(s),m=a(s,"H1",{class:!0});var Ra=i(m);g=a(Ra,"A",{id:!0,class:!0,href:!0});var rc=i(g);v=a(rc,"SPAN",{});var ac=i(v);w(F.$$.fragment,ac),ac.forEach(n),rc.forEach(n),_=d(Ra),z=a(Ra,"SPAN",{});var ic=i(z);ce=o(ic,"Funnel Transformer"),ic.forEach(n),Ra.forEach(n),G=d(s),q=a(s,"H2",{class:!0});var Ha=i(q);J=a(Ha,"A",{id:!0,class:!0,href:!0});var lc=i(J);I=a(lc,"SPAN",{});var dc=i(I);w(ne.$$.fragment,dc),dc.forEach(n),lc.forEach(n),ue=d(Ha),S=a(Ha,"SPAN",{});var cc=i(S);pe=o(cc,"Overview"),cc.forEach(n),Ha.forEach(n),ie=d(s),U=a(s,"P",{});var Va=i(U);L=o(Va,"The Funnel Transformer model was proposed in the paper "),te=a(Va,"A",{href:!0,rel:!0});var uc=i(te);Z=o(uc,`Funnel-Transformer: Filtering out Sequential Redundancy for
Efficient Language Processing`),uc.forEach(n),P=o(Va,`. It is a bidirectional transformer model, like
BERT, but with a pooling operation after each block of layers, a bit like in traditional convolutional neural networks
(CNN) in computer vision.`),Va.forEach(n),x=d(s),oe=a(s,"P",{});var pc=i(oe);Q=o(pc,"The abstract from the paper is the following:"),pc.forEach(n),le=d(s),se=a(s,"P",{});var hc=i(se);N=a(hc,"EM",{});var fc=i(N);he=o(fc,`With the success of language pretraining, it is highly desirable to develop more efficient architectures of good
scalability that can exploit the abundant unlabeled data at a lower cost. To improve the efficiency, we examine the
much-overlooked redundancy in maintaining a full-length token-level presentation, especially for tasks that only
require a single-vector presentation of the sequence. With this intuition, we propose Funnel-Transformer which
gradually compresses the sequence of hidden states to a shorter one and hence reduces the computation cost. More
importantly, by re-investing the saved FLOPs from length reduction in constructing a deeper or wider model, we further
improve the model capacity. In addition, to perform token-level predictions as required by common pretraining
objectives, Funnel-Transformer is able to recover a deep representation for each token from the reduced hidden sequence
via a decoder. Empirically, with comparable or fewer FLOPs, Funnel-Transformer outperforms the standard Transformer on
a wide variety of sequence-level prediction tasks, including text classification, language understanding, and reading
comprehension.`),fc.forEach(n),hc.forEach(n),de=d(s),C=a(s,"P",{});var mc=i(C);fe=o(mc,"Tips:"),mc.forEach(n),B=d(s),ee=a(s,"UL",{});var Ya=i(ee);ae=a(Ya,"LI",{});var gc=i(ae);R=o(gc,`Since Funnel Transformer uses pooling, the sequence length of the hidden states changes after each block of layers.
The base model therefore has a final sequence length that is a quarter of the original one. This model can be used
directly for tasks that just require a sentence summary (like sequence classification or multiple choice). For other
tasks, the full model is used; this full model has a decoder that upsamples the final hidden states to the same
sequence length as the input.`),gc.forEach(n),me=d(Ya),O=a(Ya,"LI",{});var Ce=i(O);A=o(Ce,`The Funnel Transformer checkpoints are all available with a full version and a base version. The first ones should be
used for `),re=a(Ce,"A",{href:!0});var _c=i(re);H=o(_c,"FunnelModel"),_c.forEach(n),ge=o(Ce,", "),u=a(Ce,"A",{href:!0});var Fc=i(u);T=o(Fc,"FunnelForPreTraining"),Fc.forEach(n),K=o(Ce,`,
`),Fe=a(Ce,"A",{href:!0});var vc=i(Fe);we=o(vc,"FunnelForMaskedLM"),vc.forEach(n),D=o(Ce,", "),ve=a(Ce,"A",{href:!0});var Tc=i(ve);be=o(Tc,"FunnelForTokenClassification"),Tc.forEach(n),ye=o(Ce,` and
class:`),j=a(Ce,"EM",{});var kc=i(j);V=o(kc,"~transformers.FunnelForQuestionAnswering"),kc.forEach(n),$e=o(Ce,`. The second ones should be used for
`),Te=a(Ce,"A",{href:!0});var wc=i(Te);Y=o(wc,"FunnelBaseModel"),wc.forEach(n),Ee=o(Ce,", "),ke=a(Ce,"A",{href:!0});var bc=i(ke);_e=o(bc,"FunnelForSequenceClassification"),bc.forEach(n),Me=o(Ce,` and
`),Ua=a(Ce,"A",{href:!0});var N1=i(Ua);Np=o(N1,"FunnelForMultipleChoice"),N1.forEach(n),Op=o(Ce,"."),Ce.forEach(n),Ya.forEach(n),Ec=d(s),jn=a(s,"P",{});var Bi=i(jn);Bp=o(Bi,"This model was contributed by "),So=a(Bi,"A",{href:!0,rel:!0});var O1=i(So);Wp=o(O1,"sgugger"),O1.forEach(n),Qp=o(Bi,". The original code can be found "),No=a(Bi,"A",{href:!0,rel:!0});var B1=i(No);Rp=o(B1,"here"),B1.forEach(n),Hp=o(Bi,"."),Bi.forEach(n),Mc=d(s),Kn=a(s,"H2",{class:!0});var mu=i(Kn);Bt=a(mu,"A",{id:!0,class:!0,href:!0});var W1=i(Bt);ul=a(W1,"SPAN",{});var Q1=i(ul);w(Oo.$$.fragment,Q1),Q1.forEach(n),W1.forEach(n),Vp=d(mu),pl=a(mu,"SPAN",{});var R1=i(pl);Yp=o(R1,"FunnelConfig"),R1.forEach(n),mu.forEach(n),zc=d(s),Cn=a(s,"DIV",{class:!0});var Wi=i(Cn);w(Bo.$$.fragment,Wi),Up=d(Wi),xn=a(Wi,"P",{});var Ao=i(xn);Gp=o(Ao,"This is the configuration class to store the configuration of a "),Ga=a(Ao,"A",{href:!0});var H1=i(Ga);Zp=o(H1,"FunnelModel"),H1.forEach(n),Kp=o(Ao," or a "),Za=a(Ao,"A",{href:!0});var V1=i(Za);Xp=o(V1,"TFBertModel"),V1.forEach(n),Jp=o(Ao,`. It is used to
instantiate a Funnel Transformer model according to the specified arguments, defining the model architecture.
Instantiating a configuration with the defaults will yield a similar configuration to that of the Funnel
Transformer `),Wo=a(Ao,"A",{href:!0,rel:!0});var Y1=i(Wo);eh=o(Y1,"funnel-transformer/small"),Y1.forEach(n),nh=o(Ao," architecture."),Ao.forEach(n),th=d(Wi),Xn=a(Wi,"P",{});var Qi=i(Xn);oh=o(Qi,"Configuration objects inherit from "),Ka=a(Qi,"A",{href:!0});var U1=i(Ka);sh=o(U1,"PretrainedConfig"),U1.forEach(n),rh=o(Qi,` and can be used to control the model outputs. Read the
documentation from `),Xa=a(Qi,"A",{href:!0});var G1=i(Xa);ah=o(G1,"PretrainedConfig"),G1.forEach(n),ih=o(Qi," for more information."),Qi.forEach(n),Wi.forEach(n),qc=d(s),Jn=a(s,"H2",{class:!0});var gu=i(Jn);Wt=a(gu,"A",{id:!0,class:!0,href:!0});var Z1=i(Wt);hl=a(Z1,"SPAN",{});var K1=i(hl);w(Qo.$$.fragment,K1),K1.forEach(n),Z1.forEach(n),lh=d(gu),fl=a(gu,"SPAN",{});var X1=i(fl);dh=o(X1,"FunnelTokenizer"),X1.forEach(n),gu.forEach(n),Pc=d(s),Pe=a(s,"DIV",{class:!0});var Ge=i(Pe);w(Ro.$$.fragment,Ge),ch=d(Ge),ml=a(Ge,"P",{});var J1=i(ml);uh=o(J1,"Construct a Funnel Transformer tokenizer."),J1.forEach(n),ph=d(Ge),Qt=a(Ge,"P",{});var yc=i(Qt);Ja=a(yc,"A",{href:!0});var ew=i(Ja);hh=o(ew,"FunnelTokenizer"),ew.forEach(n),fh=o(yc," is identical to "),ei=a(yc,"A",{href:!0});var nw=i(ei);mh=o(nw,"BertTokenizer"),nw.forEach(n),gh=o(yc,` and runs end-to-end tokenization: punctuation splitting and
wordpiece.`),yc.forEach(n),_h=d(Ge),Ho=a(Ge,"P",{});var _u=i(Ho);Fh=o(_u,"Refer to superclass "),ni=a(_u,"A",{href:!0});var tw=i(ni);vh=o(tw,"BertTokenizer"),tw.forEach(n),Th=o(_u," for usage examples and documentation concerning parameters."),_u.forEach(n),kh=d(Ge),Ln=a(Ge,"DIV",{class:!0});var Ri=i(Ln);w(Vo.$$.fragment,Ri),wh=d(Ri),gl=a(Ri,"P",{});var ow=i(gl);bh=o(ow,`Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens. A BERT sequence has the following format:`),ow.forEach(n),yh=d(Ri),Yo=a(Ri,"UL",{});var Fu=i(Yo);ti=a(Fu,"LI",{});var I1=i(ti);$h=o(I1,"single sequence: "),_l=a(I1,"CODE",{});var sw=i(_l);Eh=o(sw,"[CLS] X [SEP]"),sw.forEach(n),I1.forEach(n),Mh=d(Fu),oi=a(Fu,"LI",{});var S1=i(oi);zh=o(S1,"pair of sequences: "),Fl=a(S1,"CODE",{});var rw=i(Fl);qh=o(rw,"[CLS] A [SEP] B [SEP]"),rw.forEach(n),S1.forEach(n),Fu.forEach(n),Ri.forEach(n),Ph=d(Ge),Rt=a(Ge,"DIV",{class:!0});var vu=i(Rt);w(Uo.$$.fragment,vu),Ch=d(vu),Go=a(vu,"P",{});var Tu=i(Go);xh=o(Tu,`Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding
special tokens using the tokenizer `),vl=a(Tu,"CODE",{});var aw=i(vl);jh=o(aw,"prepare_for_model"),aw.forEach(n),Lh=o(Tu," method."),Tu.forEach(n),vu.forEach(n),Ah=d(Ge),wn=a(Ge,"DIV",{class:!0});var Do=i(wn);w(Zo.$$.fragment,Do),Dh=d(Do),Tl=a(Do,"P",{});var iw=i(Tl);Ih=o(iw,`Create a mask from the two sequences passed to be used in a sequence-pair classification task. A Funnel
Transformer sequence pair mask has the following format:`),iw.forEach(n),Sh=d(Do),w(Ko.$$.fragment,Do),Nh=d(Do),et=a(Do,"P",{});var Hi=i(et);Oh=o(Hi,"If "),kl=a(Hi,"CODE",{});var lw=i(kl);Bh=o(lw,"token_ids_1"),lw.forEach(n),Wh=o(Hi," is "),wl=a(Hi,"CODE",{});var dw=i(wl);Qh=o(dw,"None"),dw.forEach(n),Rh=o(Hi,", this method only returns the first portion of the mask (0s)."),Hi.forEach(n),Do.forEach(n),Hh=d(Ge),si=a(Ge,"DIV",{class:!0});var cw=i(si);w(Xo.$$.fragment,cw),cw.forEach(n),Ge.forEach(n),Cc=d(s),nt=a(s,"H2",{class:!0});var ku=i(nt);Ht=a(ku,"A",{id:!0,class:!0,href:!0});var uw=i(Ht);bl=a(uw,"SPAN",{});var pw=i(bl);w(Jo.$$.fragment,pw),pw.forEach(n),uw.forEach(n),Vh=d(ku),yl=a(ku,"SPAN",{});var hw=i(yl);Yh=o(hw,"FunnelTokenizerFast"),hw.forEach(n),ku.forEach(n),xc=d(s),Ze=a(s,"DIV",{class:!0});var An=i(Ze);w(es.$$.fragment,An),Uh=d(An),ns=a(An,"P",{});var wu=i(ns);Gh=o(wu,"Construct a \u201Cfast\u201D Funnel Transformer tokenizer (backed by HuggingFace\u2019s "),$l=a(wu,"EM",{});var fw=i($l);Zh=o(fw,"tokenizers"),fw.forEach(n),Kh=o(wu," library)."),wu.forEach(n),Xh=d(An),Vt=a(An,"P",{});var $c=i(Vt);ri=a($c,"A",{href:!0});var mw=i(ri);Jh=o(mw,"FunnelTokenizerFast"),mw.forEach(n),ef=o($c," is identical to "),ai=a($c,"A",{href:!0});var gw=i(ai);nf=o(gw,"BertTokenizerFast"),gw.forEach(n),tf=o($c,` and runs end-to-end tokenization: punctuation
splitting and wordpiece.`),$c.forEach(n),of=d(An),ts=a(An,"P",{});var bu=i(ts);sf=o(bu,"Refer to superclass "),ii=a(bu,"A",{href:!0});var _w=i(ii);rf=o(_w,"BertTokenizerFast"),_w.forEach(n),af=o(bu," for usage examples and documentation concerning parameters."),bu.forEach(n),lf=d(An),bn=a(An,"DIV",{class:!0});var Io=i(bn);w(os.$$.fragment,Io),df=d(Io),El=a(Io,"P",{});var Fw=i(El);cf=o(Fw,`Create a mask from the two sequences passed to be used in a sequence-pair classification task. A Funnel
Transformer sequence pair mask has the following format:`),Fw.forEach(n),uf=d(Io),w(ss.$$.fragment,Io),pf=d(Io),tt=a(Io,"P",{});var Vi=i(tt);hf=o(Vi,"If "),Ml=a(Vi,"CODE",{});var vw=i(Ml);ff=o(vw,"token_ids_1"),vw.forEach(n),mf=o(Vi," is "),zl=a(Vi,"CODE",{});var Tw=i(zl);gf=o(Tw,"None"),Tw.forEach(n),_f=o(Vi,", this method only returns the first portion of the mask (0s)."),Vi.forEach(n),Io.forEach(n),An.forEach(n),jc=d(s),ot=a(s,"H2",{class:!0});var yu=i(ot);Yt=a(yu,"A",{id:!0,class:!0,href:!0});var kw=i(Yt);ql=a(kw,"SPAN",{});var ww=i(ql);w(rs.$$.fragment,ww),ww.forEach(n),kw.forEach(n),Ff=d(yu),Pl=a(yu,"SPAN",{});var bw=i(Pl);vf=o(bw,"Funnel specific outputs"),bw.forEach(n),yu.forEach(n),Lc=d(s),st=a(s,"DIV",{class:!0});var $u=i(st);w(as.$$.fragment,$u),Tf=d($u),is=a($u,"P",{});var Eu=i(is);kf=o(Eu,"Output type of "),li=a(Eu,"A",{href:!0});var yw=i(li);wf=o(yw,"FunnelForPreTraining"),yw.forEach(n),bf=o(Eu,"."),Eu.forEach(n),$u.forEach(n),Ac=d(s),rt=a(s,"DIV",{class:!0});var Mu=i(rt);w(ls.$$.fragment,Mu),yf=d(Mu),ds=a(Mu,"P",{});var zu=i(ds);$f=o(zu,"Output type of "),di=a(zu,"A",{href:!0});var $w=i(di);Ef=o($w,"FunnelForPreTraining"),$w.forEach(n),Mf=o(zu,"."),zu.forEach(n),Mu.forEach(n),Dc=d(s),at=a(s,"H2",{class:!0});var qu=i(at);Ut=a(qu,"A",{id:!0,class:!0,href:!0});var Ew=i(Ut);Cl=a(Ew,"SPAN",{});var Mw=i(Cl);w(cs.$$.fragment,Mw),Mw.forEach(n),Ew.forEach(n),zf=d(qu),xl=a(qu,"SPAN",{});var zw=i(xl);qf=o(zw,"FunnelBaseModel"),zw.forEach(n),qu.forEach(n),Ic=d(s),We=a(s,"DIV",{class:!0});var yn=i(We);w(us.$$.fragment,yn),Pf=d(yn),jl=a(yn,"P",{});var qw=i(jl);Cf=o(qw,`The base Funnel Transformer Model transformer outputting raw hidden-states without upsampling head (also called
decoder) or any task-specific head on top.`),qw.forEach(n),xf=d(yn),ps=a(yn,"P",{});var Pu=i(ps);jf=o(Pu,"The Funnel Transformer model was proposed in "),hs=a(Pu,"A",{href:!0,rel:!0});var Pw=i(hs);Lf=o(Pw,`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),Pw.forEach(n),Af=o(Pu," by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),Pu.forEach(n),Df=d(yn),fs=a(yn,"P",{});var Cu=i(fs);If=o(Cu,"This model inherits from "),ci=a(Cu,"A",{href:!0});var Cw=i(ci);Sf=o(Cw,"PreTrainedModel"),Cw.forEach(n),Nf=o(Cu,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Cu.forEach(n),Of=d(yn),ms=a(yn,"P",{});var xu=i(ms);Bf=o(xu,"This model is also a PyTorch "),gs=a(xu,"A",{href:!0,rel:!0});var xw=i(gs);Wf=o(xw,"torch.nn.Module"),xw.forEach(n),Qf=o(xu,` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),xu.forEach(n),Rf=d(yn),Ke=a(yn,"DIV",{class:!0});var Dn=i(Ke);w(_s.$$.fragment,Dn),Hf=d(Dn),it=a(Dn,"P",{});var Yi=i(it);Vf=o(Yi,"The "),ui=a(Yi,"A",{href:!0});var jw=i(ui);Yf=o(jw,"FunnelBaseModel"),jw.forEach(n),Uf=o(Yi," forward method, overrides the "),Ll=a(Yi,"CODE",{});var Lw=i(Ll);Gf=o(Lw,"__call__"),Lw.forEach(n),Zf=o(Yi," special method."),Yi.forEach(n),Kf=d(Dn),w(Gt.$$.fragment,Dn),Xf=d(Dn),Al=a(Dn,"P",{});var Aw=i(Al);Jf=o(Aw,"Example:"),Aw.forEach(n),em=d(Dn),w(Fs.$$.fragment,Dn),Dn.forEach(n),yn.forEach(n),Sc=d(s),lt=a(s,"H2",{class:!0});var ju=i(lt);Zt=a(ju,"A",{id:!0,class:!0,href:!0});var Dw=i(Zt);Dl=a(Dw,"SPAN",{});var Iw=i(Dl);w(vs.$$.fragment,Iw),Iw.forEach(n),Dw.forEach(n),nm=d(ju),Il=a(ju,"SPAN",{});var Sw=i(Il);tm=o(Sw,"FunnelModel"),Sw.forEach(n),ju.forEach(n),Nc=d(s),Qe=a(s,"DIV",{class:!0});var $n=i(Qe);w(Ts.$$.fragment,$n),om=d($n),Sl=a($n,"P",{});var Nw=i(Sl);sm=o(Nw,"The bare Funnel Transformer Model transformer outputting raw hidden-states without any specific head on top."),Nw.forEach(n),rm=d($n),ks=a($n,"P",{});var Lu=i(ks);am=o(Lu,"The Funnel Transformer model was proposed in "),ws=a(Lu,"A",{href:!0,rel:!0});var Ow=i(ws);im=o(Ow,`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),Ow.forEach(n),lm=o(Lu," by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),Lu.forEach(n),dm=d($n),bs=a($n,"P",{});var Au=i(bs);cm=o(Au,"This model inherits from "),pi=a(Au,"A",{href:!0});var Bw=i(pi);um=o(Bw,"PreTrainedModel"),Bw.forEach(n),pm=o(Au,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Au.forEach(n),hm=d($n),ys=a($n,"P",{});var Du=i(ys);fm=o(Du,"This model is also a PyTorch "),$s=a(Du,"A",{href:!0,rel:!0});var Ww=i($s);mm=o(Ww,"torch.nn.Module"),Ww.forEach(n),gm=o(Du,` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),Du.forEach(n),_m=d($n),Xe=a($n,"DIV",{class:!0});var In=i(Xe);w(Es.$$.fragment,In),Fm=d(In),dt=a(In,"P",{});var Ui=i(dt);vm=o(Ui,"The "),hi=a(Ui,"A",{href:!0});var Qw=i(hi);Tm=o(Qw,"FunnelModel"),Qw.forEach(n),km=o(Ui," forward method, overrides the "),Nl=a(Ui,"CODE",{});var Rw=i(Nl);wm=o(Rw,"__call__"),Rw.forEach(n),bm=o(Ui," special method."),Ui.forEach(n),ym=d(In),w(Kt.$$.fragment,In),$m=d(In),Ol=a(In,"P",{});var Hw=i(Ol);Em=o(Hw,"Example:"),Hw.forEach(n),Mm=d(In),w(Ms.$$.fragment,In),In.forEach(n),$n.forEach(n),Oc=d(s),ct=a(s,"H2",{class:!0});var Iu=i(ct);Xt=a(Iu,"A",{id:!0,class:!0,href:!0});var Vw=i(Xt);Bl=a(Vw,"SPAN",{});var Yw=i(Bl);w(zs.$$.fragment,Yw),Yw.forEach(n),Vw.forEach(n),zm=d(Iu),Wl=a(Iu,"SPAN",{});var Uw=i(Wl);qm=o(Uw,"FunnelModelForPreTraining"),Uw.forEach(n),Iu.forEach(n),Bc=d(s),ut=a(s,"DIV",{class:!0});var Su=i(ut);w(qs.$$.fragment,Su),Pm=d(Su),Je=a(Su,"DIV",{class:!0});var Sn=i(Je);w(Ps.$$.fragment,Sn),Cm=d(Sn),pt=a(Sn,"P",{});var Gi=i(pt);xm=o(Gi,"The "),fi=a(Gi,"A",{href:!0});var Gw=i(fi);jm=o(Gw,"FunnelForPreTraining"),Gw.forEach(n),Lm=o(Gi," forward method, overrides the "),Ql=a(Gi,"CODE",{});var Zw=i(Ql);Am=o(Zw,"__call__"),Zw.forEach(n),Dm=o(Gi," special method."),Gi.forEach(n),Im=d(Sn),w(Jt.$$.fragment,Sn),Sm=d(Sn),Rl=a(Sn,"P",{});var Kw=i(Rl);Nm=o(Kw,"Examples:"),Kw.forEach(n),Om=d(Sn),w(Cs.$$.fragment,Sn),Sn.forEach(n),Su.forEach(n),Wc=d(s),ht=a(s,"H2",{class:!0});var Nu=i(ht);eo=a(Nu,"A",{id:!0,class:!0,href:!0});var Xw=i(eo);Hl=a(Xw,"SPAN",{});var Jw=i(Hl);w(xs.$$.fragment,Jw),Jw.forEach(n),Xw.forEach(n),Bm=d(Nu),Vl=a(Nu,"SPAN",{});var eb=i(Vl);Wm=o(eb,"FunnelForMaskedLM"),eb.forEach(n),Nu.forEach(n),Qc=d(s),Re=a(s,"DIV",{class:!0});var En=i(Re);w(js.$$.fragment,En),Qm=d(En),Ls=a(En,"P",{});var Ou=i(Ls);Rm=o(Ou,"Funnel Transformer Model with a "),Yl=a(Ou,"CODE",{});var nb=i(Yl);Hm=o(nb,"language modeling"),nb.forEach(n),Vm=o(Ou," head on top."),Ou.forEach(n),Ym=d(En),As=a(En,"P",{});var Bu=i(As);Um=o(Bu,"The Funnel Transformer model was proposed in "),Ds=a(Bu,"A",{href:!0,rel:!0});var tb=i(Ds);Gm=o(tb,`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),tb.forEach(n),Zm=o(Bu," by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),Bu.forEach(n),Km=d(En),Is=a(En,"P",{});var Wu=i(Is);Xm=o(Wu,"This model inherits from "),mi=a(Wu,"A",{href:!0});var ob=i(mi);Jm=o(ob,"PreTrainedModel"),ob.forEach(n),eg=o(Wu,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Wu.forEach(n),ng=d(En),Ss=a(En,"P",{});var Qu=i(Ss);tg=o(Qu,"This model is also a PyTorch "),Ns=a(Qu,"A",{href:!0,rel:!0});var sb=i(Ns);og=o(sb,"torch.nn.Module"),sb.forEach(n),sg=o(Qu,` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),Qu.forEach(n),rg=d(En),en=a(En,"DIV",{class:!0});var Nn=i(en);w(Os.$$.fragment,Nn),ag=d(Nn),ft=a(Nn,"P",{});var Zi=i(ft);ig=o(Zi,"The "),gi=a(Zi,"A",{href:!0});var rb=i(gi);lg=o(rb,"FunnelForMaskedLM"),rb.forEach(n),dg=o(Zi," forward method, overrides the "),Ul=a(Zi,"CODE",{});var ab=i(Ul);cg=o(ab,"__call__"),ab.forEach(n),ug=o(Zi," special method."),Zi.forEach(n),pg=d(Nn),w(no.$$.fragment,Nn),hg=d(Nn),Gl=a(Nn,"P",{});var ib=i(Gl);fg=o(ib,"Example:"),ib.forEach(n),mg=d(Nn),w(Bs.$$.fragment,Nn),Nn.forEach(n),En.forEach(n),Rc=d(s),mt=a(s,"H2",{class:!0});var Ru=i(mt);to=a(Ru,"A",{id:!0,class:!0,href:!0});var lb=i(to);Zl=a(lb,"SPAN",{});var db=i(Zl);w(Ws.$$.fragment,db),db.forEach(n),lb.forEach(n),gg=d(Ru),Kl=a(Ru,"SPAN",{});var cb=i(Kl);_g=o(cb,"FunnelForSequenceClassification"),cb.forEach(n),Ru.forEach(n),Hc=d(s),He=a(s,"DIV",{class:!0});var Mn=i(He);w(Qs.$$.fragment,Mn),Fg=d(Mn),Xl=a(Mn,"P",{});var ub=i(Xl);vg=o(ub,`Funnel Transformer Model with a sequence classification/regression head on top (two linear layer on top of the
first timestep of the last hidden state) e.g. for GLUE tasks.`),ub.forEach(n),Tg=d(Mn),Rs=a(Mn,"P",{});var Hu=i(Rs);kg=o(Hu,"The Funnel Transformer model was proposed in "),Hs=a(Hu,"A",{href:!0,rel:!0});var pb=i(Hs);wg=o(pb,`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),pb.forEach(n),bg=o(Hu," by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),Hu.forEach(n),yg=d(Mn),Vs=a(Mn,"P",{});var Vu=i(Vs);$g=o(Vu,"This model inherits from "),_i=a(Vu,"A",{href:!0});var hb=i(_i);Eg=o(hb,"PreTrainedModel"),hb.forEach(n),Mg=o(Vu,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Vu.forEach(n),zg=d(Mn),Ys=a(Mn,"P",{});var Yu=i(Ys);qg=o(Yu,"This model is also a PyTorch "),Us=a(Yu,"A",{href:!0,rel:!0});var fb=i(Us);Pg=o(fb,"torch.nn.Module"),fb.forEach(n),Cg=o(Yu,` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),Yu.forEach(n),xg=d(Mn),Be=a(Mn,"DIV",{class:!0});var hn=i(Be);w(Gs.$$.fragment,hn),jg=d(hn),gt=a(hn,"P",{});var Ki=i(gt);Lg=o(Ki,"The "),Fi=a(Ki,"A",{href:!0});var mb=i(Fi);Ag=o(mb,"FunnelForSequenceClassification"),mb.forEach(n),Dg=o(Ki," forward method, overrides the "),Jl=a(Ki,"CODE",{});var gb=i(Jl);Ig=o(gb,"__call__"),gb.forEach(n),Sg=o(Ki," special method."),Ki.forEach(n),Ng=d(hn),w(oo.$$.fragment,hn),Og=d(hn),ed=a(hn,"P",{});var _b=i(ed);Bg=o(_b,"Example of single-label classification:"),_b.forEach(n),Wg=d(hn),w(Zs.$$.fragment,hn),Qg=d(hn),nd=a(hn,"P",{});var Fb=i(nd);Rg=o(Fb,"Example of multi-label classification:"),Fb.forEach(n),Hg=d(hn),w(Ks.$$.fragment,hn),hn.forEach(n),Mn.forEach(n),Vc=d(s),_t=a(s,"H2",{class:!0});var Uu=i(_t);so=a(Uu,"A",{id:!0,class:!0,href:!0});var vb=i(so);td=a(vb,"SPAN",{});var Tb=i(td);w(Xs.$$.fragment,Tb),Tb.forEach(n),vb.forEach(n),Vg=d(Uu),od=a(Uu,"SPAN",{});var kb=i(od);Yg=o(kb,"FunnelForMultipleChoice"),kb.forEach(n),Uu.forEach(n),Yc=d(s),Ve=a(s,"DIV",{class:!0});var zn=i(Ve);w(Js.$$.fragment,zn),Ug=d(zn),sd=a(zn,"P",{});var wb=i(sd);Gg=o(wb,`Funnel Transformer Model with a multiple choice classification head on top (two linear layer on top of the first
timestep of the last hidden state, and a softmax) e.g. for RocStories/SWAG tasks.`),wb.forEach(n),Zg=d(zn),er=a(zn,"P",{});var Gu=i(er);Kg=o(Gu,"The Funnel Transformer model was proposed in "),nr=a(Gu,"A",{href:!0,rel:!0});var bb=i(nr);Xg=o(bb,`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),bb.forEach(n),Jg=o(Gu," by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),Gu.forEach(n),e_=d(zn),tr=a(zn,"P",{});var Zu=i(tr);n_=o(Zu,"This model inherits from "),vi=a(Zu,"A",{href:!0});var yb=i(vi);t_=o(yb,"PreTrainedModel"),yb.forEach(n),o_=o(Zu,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Zu.forEach(n),s_=d(zn),or=a(zn,"P",{});var Ku=i(or);r_=o(Ku,"This model is also a PyTorch "),sr=a(Ku,"A",{href:!0,rel:!0});var $b=i(sr);a_=o($b,"torch.nn.Module"),$b.forEach(n),i_=o(Ku,` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),Ku.forEach(n),l_=d(zn),nn=a(zn,"DIV",{class:!0});var On=i(nn);w(rr.$$.fragment,On),d_=d(On),Ft=a(On,"P",{});var Xi=i(Ft);c_=o(Xi,"The "),Ti=a(Xi,"A",{href:!0});var Eb=i(Ti);u_=o(Eb,"FunnelForMultipleChoice"),Eb.forEach(n),p_=o(Xi," forward method, overrides the "),rd=a(Xi,"CODE",{});var Mb=i(rd);h_=o(Mb,"__call__"),Mb.forEach(n),f_=o(Xi," special method."),Xi.forEach(n),m_=d(On),w(ro.$$.fragment,On),g_=d(On),ad=a(On,"P",{});var zb=i(ad);__=o(zb,"Example:"),zb.forEach(n),F_=d(On),w(ar.$$.fragment,On),On.forEach(n),zn.forEach(n),Uc=d(s),vt=a(s,"H2",{class:!0});var Xu=i(vt);ao=a(Xu,"A",{id:!0,class:!0,href:!0});var qb=i(ao);id=a(qb,"SPAN",{});var Pb=i(id);w(ir.$$.fragment,Pb),Pb.forEach(n),qb.forEach(n),v_=d(Xu),ld=a(Xu,"SPAN",{});var Cb=i(ld);T_=o(Cb,"FunnelForTokenClassification"),Cb.forEach(n),Xu.forEach(n),Gc=d(s),Ye=a(s,"DIV",{class:!0});var qn=i(Ye);w(lr.$$.fragment,qn),k_=d(qn),dd=a(qn,"P",{});var xb=i(dd);w_=o(xb,`Funnel Transformer Model with a token classification head on top (a linear layer on top of the hidden-states
output) e.g. for Named-Entity-Recognition (NER) tasks.`),xb.forEach(n),b_=d(qn),dr=a(qn,"P",{});var Ju=i(dr);y_=o(Ju,"The Funnel Transformer model was proposed in "),cr=a(Ju,"A",{href:!0,rel:!0});var jb=i(cr);$_=o(jb,`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),jb.forEach(n),E_=o(Ju," by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),Ju.forEach(n),M_=d(qn),ur=a(qn,"P",{});var ep=i(ur);z_=o(ep,"This model inherits from "),ki=a(ep,"A",{href:!0});var Lb=i(ki);q_=o(Lb,"PreTrainedModel"),Lb.forEach(n),P_=o(ep,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),ep.forEach(n),C_=d(qn),pr=a(qn,"P",{});var np=i(pr);x_=o(np,"This model is also a PyTorch "),hr=a(np,"A",{href:!0,rel:!0});var Ab=i(hr);j_=o(Ab,"torch.nn.Module"),Ab.forEach(n),L_=o(np,` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),np.forEach(n),A_=d(qn),tn=a(qn,"DIV",{class:!0});var Bn=i(tn);w(fr.$$.fragment,Bn),D_=d(Bn),Tt=a(Bn,"P",{});var Ji=i(Tt);I_=o(Ji,"The "),wi=a(Ji,"A",{href:!0});var Db=i(wi);S_=o(Db,"FunnelForTokenClassification"),Db.forEach(n),N_=o(Ji," forward method, overrides the "),cd=a(Ji,"CODE",{});var Ib=i(cd);O_=o(Ib,"__call__"),Ib.forEach(n),B_=o(Ji," special method."),Ji.forEach(n),W_=d(Bn),w(io.$$.fragment,Bn),Q_=d(Bn),ud=a(Bn,"P",{});var Sb=i(ud);R_=o(Sb,"Example:"),Sb.forEach(n),H_=d(Bn),w(mr.$$.fragment,Bn),Bn.forEach(n),qn.forEach(n),Zc=d(s),kt=a(s,"H2",{class:!0});var tp=i(kt);lo=a(tp,"A",{id:!0,class:!0,href:!0});var Nb=i(lo);pd=a(Nb,"SPAN",{});var Ob=i(pd);w(gr.$$.fragment,Ob),Ob.forEach(n),Nb.forEach(n),V_=d(tp),hd=a(tp,"SPAN",{});var Bb=i(hd);Y_=o(Bb,"FunnelForQuestionAnswering"),Bb.forEach(n),tp.forEach(n),Kc=d(s),Ue=a(s,"DIV",{class:!0});var Pn=i(Ue);w(_r.$$.fragment,Pn),U_=d(Pn),wt=a(Pn,"P",{});var el=i(wt);G_=o(el,`Funnel Transformer Model with a span classification head on top for extractive question-answering tasks like SQuAD
(a linear layer on top of the hidden-states output to compute `),fd=a(el,"CODE",{});var Wb=i(fd);Z_=o(Wb,"span start logits"),Wb.forEach(n),K_=o(el," and "),md=a(el,"CODE",{});var Qb=i(md);X_=o(Qb,"span end logits"),Qb.forEach(n),J_=o(el,")."),el.forEach(n),eF=d(Pn),Fr=a(Pn,"P",{});var op=i(Fr);nF=o(op,"The Funnel Transformer model was proposed in "),vr=a(op,"A",{href:!0,rel:!0});var Rb=i(vr);tF=o(Rb,`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),Rb.forEach(n),oF=o(op," by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),op.forEach(n),sF=d(Pn),Tr=a(Pn,"P",{});var sp=i(Tr);rF=o(sp,"This model inherits from "),bi=a(sp,"A",{href:!0});var Hb=i(bi);aF=o(Hb,"PreTrainedModel"),Hb.forEach(n),iF=o(sp,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),sp.forEach(n),lF=d(Pn),kr=a(Pn,"P",{});var rp=i(kr);dF=o(rp,"This model is also a PyTorch "),wr=a(rp,"A",{href:!0,rel:!0});var Vb=i(wr);cF=o(Vb,"torch.nn.Module"),Vb.forEach(n),uF=o(rp,` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),rp.forEach(n),pF=d(Pn),on=a(Pn,"DIV",{class:!0});var Wn=i(on);w(br.$$.fragment,Wn),hF=d(Wn),bt=a(Wn,"P",{});var nl=i(bt);fF=o(nl,"The "),yi=a(nl,"A",{href:!0});var Yb=i(yi);mF=o(Yb,"FunnelForQuestionAnswering"),Yb.forEach(n),gF=o(nl," forward method, overrides the "),gd=a(nl,"CODE",{});var Ub=i(gd);_F=o(Ub,"__call__"),Ub.forEach(n),FF=o(nl," special method."),nl.forEach(n),vF=d(Wn),w(co.$$.fragment,Wn),TF=d(Wn),_d=a(Wn,"P",{});var Gb=i(_d);kF=o(Gb,"Example:"),Gb.forEach(n),wF=d(Wn),w(yr.$$.fragment,Wn),Wn.forEach(n),Pn.forEach(n),Xc=d(s),yt=a(s,"H2",{class:!0});var ap=i(yt);uo=a(ap,"A",{id:!0,class:!0,href:!0});var Zb=i(uo);Fd=a(Zb,"SPAN",{});var Kb=i(Fd);w($r.$$.fragment,Kb),Kb.forEach(n),Zb.forEach(n),bF=d(ap),vd=a(ap,"SPAN",{});var Xb=i(vd);yF=o(Xb,"TFFunnelBaseModel"),Xb.forEach(n),ap.forEach(n),Jc=d(s),xe=a(s,"DIV",{class:!0});var fn=i(xe);w(Er.$$.fragment,fn),$F=d(fn),Td=a(fn,"P",{});var Jb=i(Td);EF=o(Jb,`The base Funnel Transformer Model transformer outputting raw hidden-states without upsampling head (also called
decoder) or any task-specific head on top.`),Jb.forEach(n),MF=d(fn),Mr=a(fn,"P",{});var ip=i(Mr);zF=o(ip,"The Funnel Transformer model was proposed in "),zr=a(ip,"A",{href:!0,rel:!0});var e0=i(zr);qF=o(e0,`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),e0.forEach(n),PF=o(ip," by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),ip.forEach(n),CF=d(fn),qr=a(fn,"P",{});var lp=i(qr);xF=o(lp,"This model inherits from "),$i=a(lp,"A",{href:!0});var n0=i($i);jF=o(n0,"TFPreTrainedModel"),n0.forEach(n),LF=o(lp,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),lp.forEach(n),AF=d(fn),Pr=a(fn,"P",{});var dp=i(Pr);DF=o(dp,"This model is also a "),Cr=a(dp,"A",{href:!0,rel:!0});var t0=i(Cr);IF=o(t0,"tf.keras.Model"),t0.forEach(n),SF=o(dp,` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),dp.forEach(n),NF=d(fn),w(po.$$.fragment,fn),OF=d(fn),sn=a(fn,"DIV",{class:!0});var Qn=i(sn);w(xr.$$.fragment,Qn),BF=d(Qn),$t=a(Qn,"P",{});var tl=i($t);WF=o(tl,"The "),Ei=a(tl,"A",{href:!0});var o0=i(Ei);QF=o(o0,"TFFunnelBaseModel"),o0.forEach(n),RF=o(tl," forward method, overrides the "),kd=a(tl,"CODE",{});var s0=i(kd);HF=o(s0,"__call__"),s0.forEach(n),VF=o(tl," special method."),tl.forEach(n),YF=d(Qn),w(ho.$$.fragment,Qn),UF=d(Qn),wd=a(Qn,"P",{});var r0=i(wd);GF=o(r0,"Example:"),r0.forEach(n),ZF=d(Qn),w(jr.$$.fragment,Qn),Qn.forEach(n),fn.forEach(n),eu=d(s),Et=a(s,"H2",{class:!0});var cp=i(Et);fo=a(cp,"A",{id:!0,class:!0,href:!0});var a0=i(fo);bd=a(a0,"SPAN",{});var i0=i(bd);w(Lr.$$.fragment,i0),i0.forEach(n),a0.forEach(n),KF=d(cp),yd=a(cp,"SPAN",{});var l0=i(yd);XF=o(l0,"TFFunnelModel"),l0.forEach(n),cp.forEach(n),nu=d(s),je=a(s,"DIV",{class:!0});var mn=i(je);w(Ar.$$.fragment,mn),JF=d(mn),$d=a(mn,"P",{});var d0=i($d);ev=o(d0,"The bare Funnel Transformer Model transformer outputting raw hidden-states without any specific head on top."),d0.forEach(n),nv=d(mn),Dr=a(mn,"P",{});var up=i(Dr);tv=o(up,"The Funnel Transformer model was proposed in "),Ir=a(up,"A",{href:!0,rel:!0});var c0=i(Ir);ov=o(c0,`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),c0.forEach(n),sv=o(up," by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),up.forEach(n),rv=d(mn),Sr=a(mn,"P",{});var pp=i(Sr);av=o(pp,"This model inherits from "),Mi=a(pp,"A",{href:!0});var u0=i(Mi);iv=o(u0,"TFPreTrainedModel"),u0.forEach(n),lv=o(pp,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),pp.forEach(n),dv=d(mn),Nr=a(mn,"P",{});var hp=i(Nr);cv=o(hp,"This model is also a "),Or=a(hp,"A",{href:!0,rel:!0});var p0=i(Or);uv=o(p0,"tf.keras.Model"),p0.forEach(n),pv=o(hp,` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),hp.forEach(n),hv=d(mn),w(mo.$$.fragment,mn),fv=d(mn),rn=a(mn,"DIV",{class:!0});var Rn=i(rn);w(Br.$$.fragment,Rn),mv=d(Rn),Mt=a(Rn,"P",{});var ol=i(Mt);gv=o(ol,"The "),zi=a(ol,"A",{href:!0});var h0=i(zi);_v=o(h0,"TFFunnelModel"),h0.forEach(n),Fv=o(ol," forward method, overrides the "),Ed=a(ol,"CODE",{});var f0=i(Ed);vv=o(f0,"__call__"),f0.forEach(n),Tv=o(ol," special method."),ol.forEach(n),kv=d(Rn),w(go.$$.fragment,Rn),wv=d(Rn),Md=a(Rn,"P",{});var m0=i(Md);bv=o(m0,"Example:"),m0.forEach(n),yv=d(Rn),w(Wr.$$.fragment,Rn),Rn.forEach(n),mn.forEach(n),tu=d(s),zt=a(s,"H2",{class:!0});var fp=i(zt);_o=a(fp,"A",{id:!0,class:!0,href:!0});var g0=i(_o);zd=a(g0,"SPAN",{});var _0=i(zd);w(Qr.$$.fragment,_0),_0.forEach(n),g0.forEach(n),$v=d(fp),qd=a(fp,"SPAN",{});var F0=i(qd);Ev=o(F0,"TFFunnelModelForPreTraining"),F0.forEach(n),fp.forEach(n),ou=d(s),Le=a(s,"DIV",{class:!0});var gn=i(Le);w(Rr.$$.fragment,gn),Mv=d(gn),Pd=a(gn,"P",{});var v0=i(Pd);zv=o(v0,"Funnel model with a binary classification head on top as used during pretraining for identifying generated tokens."),v0.forEach(n),qv=d(gn),Hr=a(gn,"P",{});var mp=i(Hr);Pv=o(mp,"The Funnel Transformer model was proposed in "),Vr=a(mp,"A",{href:!0,rel:!0});var T0=i(Vr);Cv=o(T0,`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),T0.forEach(n),xv=o(mp," by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),mp.forEach(n),jv=d(gn),Yr=a(gn,"P",{});var gp=i(Yr);Lv=o(gp,"This model inherits from "),qi=a(gp,"A",{href:!0});var k0=i(qi);Av=o(k0,"TFPreTrainedModel"),k0.forEach(n),Dv=o(gp,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),gp.forEach(n),Iv=d(gn),Ur=a(gn,"P",{});var _p=i(Ur);Sv=o(_p,"This model is also a "),Gr=a(_p,"A",{href:!0,rel:!0});var w0=i(Gr);Nv=o(w0,"tf.keras.Model"),w0.forEach(n),Ov=o(_p,` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),_p.forEach(n),Bv=d(gn),w(Fo.$$.fragment,gn),Wv=d(gn),an=a(gn,"DIV",{class:!0});var Hn=i(an);w(Zr.$$.fragment,Hn),Qv=d(Hn),qt=a(Hn,"P",{});var sl=i(qt);Rv=o(sl,"The "),Pi=a(sl,"A",{href:!0});var b0=i(Pi);Hv=o(b0,"TFFunnelForPreTraining"),b0.forEach(n),Vv=o(sl," forward method, overrides the "),Cd=a(sl,"CODE",{});var y0=i(Cd);Yv=o(y0,"__call__"),y0.forEach(n),Uv=o(sl," special method."),sl.forEach(n),Gv=d(Hn),w(vo.$$.fragment,Hn),Zv=d(Hn),xd=a(Hn,"P",{});var $0=i(xd);Kv=o($0,"Examples:"),$0.forEach(n),Xv=d(Hn),w(Kr.$$.fragment,Hn),Hn.forEach(n),gn.forEach(n),su=d(s),Pt=a(s,"H2",{class:!0});var Fp=i(Pt);To=a(Fp,"A",{id:!0,class:!0,href:!0});var E0=i(To);jd=a(E0,"SPAN",{});var M0=i(jd);w(Xr.$$.fragment,M0),M0.forEach(n),E0.forEach(n),Jv=d(Fp),Ld=a(Fp,"SPAN",{});var z0=i(Ld);eT=o(z0,"TFFunnelForMaskedLM"),z0.forEach(n),Fp.forEach(n),ru=d(s),Ae=a(s,"DIV",{class:!0});var _n=i(Ae);w(Jr.$$.fragment,_n),nT=d(_n),ea=a(_n,"P",{});var vp=i(ea);tT=o(vp,"Funnel Model with a "),Ad=a(vp,"CODE",{});var q0=i(Ad);oT=o(q0,"language modeling"),q0.forEach(n),sT=o(vp," head on top."),vp.forEach(n),rT=d(_n),na=a(_n,"P",{});var Tp=i(na);aT=o(Tp,"The Funnel Transformer model was proposed in "),ta=a(Tp,"A",{href:!0,rel:!0});var P0=i(ta);iT=o(P0,`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),P0.forEach(n),lT=o(Tp," by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),Tp.forEach(n),dT=d(_n),oa=a(_n,"P",{});var kp=i(oa);cT=o(kp,"This model inherits from "),Ci=a(kp,"A",{href:!0});var C0=i(Ci);uT=o(C0,"TFPreTrainedModel"),C0.forEach(n),pT=o(kp,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),kp.forEach(n),hT=d(_n),sa=a(_n,"P",{});var wp=i(sa);fT=o(wp,"This model is also a "),ra=a(wp,"A",{href:!0,rel:!0});var x0=i(ra);mT=o(x0,"tf.keras.Model"),x0.forEach(n),gT=o(wp,` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),wp.forEach(n),_T=d(_n),w(ko.$$.fragment,_n),FT=d(_n),ln=a(_n,"DIV",{class:!0});var Vn=i(ln);w(aa.$$.fragment,Vn),vT=d(Vn),Ct=a(Vn,"P",{});var rl=i(Ct);TT=o(rl,"The "),xi=a(rl,"A",{href:!0});var j0=i(xi);kT=o(j0,"TFFunnelForMaskedLM"),j0.forEach(n),wT=o(rl," forward method, overrides the "),Dd=a(rl,"CODE",{});var L0=i(Dd);bT=o(L0,"__call__"),L0.forEach(n),yT=o(rl," special method."),rl.forEach(n),$T=d(Vn),w(wo.$$.fragment,Vn),ET=d(Vn),Id=a(Vn,"P",{});var A0=i(Id);MT=o(A0,"Example:"),A0.forEach(n),zT=d(Vn),w(ia.$$.fragment,Vn),Vn.forEach(n),_n.forEach(n),au=d(s),xt=a(s,"H2",{class:!0});var bp=i(xt);bo=a(bp,"A",{id:!0,class:!0,href:!0});var D0=i(bo);Sd=a(D0,"SPAN",{});var I0=i(Sd);w(la.$$.fragment,I0),I0.forEach(n),D0.forEach(n),qT=d(bp),Nd=a(bp,"SPAN",{});var S0=i(Nd);PT=o(S0,"TFFunnelForSequenceClassification"),S0.forEach(n),bp.forEach(n),iu=d(s),De=a(s,"DIV",{class:!0});var Fn=i(De);w(da.$$.fragment,Fn),CT=d(Fn),Od=a(Fn,"P",{});var N0=i(Od);xT=o(N0,`Funnel Model transformer with a sequence classification/regression head on top (a linear layer on top of the pooled
output) e.g. for GLUE tasks.`),N0.forEach(n),jT=d(Fn),ca=a(Fn,"P",{});var yp=i(ca);LT=o(yp,"The Funnel Transformer model was proposed in "),ua=a(yp,"A",{href:!0,rel:!0});var O0=i(ua);AT=o(O0,`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),O0.forEach(n),DT=o(yp," by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),yp.forEach(n),IT=d(Fn),pa=a(Fn,"P",{});var $p=i(pa);ST=o($p,"This model inherits from "),ji=a($p,"A",{href:!0});var B0=i(ji);NT=o(B0,"TFPreTrainedModel"),B0.forEach(n),OT=o($p,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),$p.forEach(n),BT=d(Fn),ha=a(Fn,"P",{});var Ep=i(ha);WT=o(Ep,"This model is also a "),fa=a(Ep,"A",{href:!0,rel:!0});var W0=i(fa);QT=o(W0,"tf.keras.Model"),W0.forEach(n),RT=o(Ep,` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),Ep.forEach(n),HT=d(Fn),w(yo.$$.fragment,Fn),VT=d(Fn),dn=a(Fn,"DIV",{class:!0});var Yn=i(dn);w(ma.$$.fragment,Yn),YT=d(Yn),jt=a(Yn,"P",{});var al=i(jt);UT=o(al,"The "),Li=a(al,"A",{href:!0});var Q0=i(Li);GT=o(Q0,"TFFunnelForSequenceClassification"),Q0.forEach(n),ZT=o(al," forward method, overrides the "),Bd=a(al,"CODE",{});var R0=i(Bd);KT=o(R0,"__call__"),R0.forEach(n),XT=o(al," special method."),al.forEach(n),JT=d(Yn),w($o.$$.fragment,Yn),ek=d(Yn),Wd=a(Yn,"P",{});var H0=i(Wd);nk=o(H0,"Example:"),H0.forEach(n),tk=d(Yn),w(ga.$$.fragment,Yn),Yn.forEach(n),Fn.forEach(n),lu=d(s),Lt=a(s,"H2",{class:!0});var Mp=i(Lt);Eo=a(Mp,"A",{id:!0,class:!0,href:!0});var V0=i(Eo);Qd=a(V0,"SPAN",{});var Y0=i(Qd);w(_a.$$.fragment,Y0),Y0.forEach(n),V0.forEach(n),ok=d(Mp),Rd=a(Mp,"SPAN",{});var U0=i(Rd);sk=o(U0,"TFFunnelForMultipleChoice"),U0.forEach(n),Mp.forEach(n),du=d(s),Ie=a(s,"DIV",{class:!0});var vn=i(Ie);w(Fa.$$.fragment,vn),rk=d(vn),Hd=a(vn,"P",{});var G0=i(Hd);ak=o(G0,`Funnel Model with a multiple choice classification head on top (a linear layer on top of the pooled output and a
softmax) e.g. for RocStories/SWAG tasks.`),G0.forEach(n),ik=d(vn),va=a(vn,"P",{});var zp=i(va);lk=o(zp,"The Funnel Transformer model was proposed in "),Ta=a(zp,"A",{href:!0,rel:!0});var Z0=i(Ta);dk=o(Z0,`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),Z0.forEach(n),ck=o(zp," by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),zp.forEach(n),uk=d(vn),ka=a(vn,"P",{});var qp=i(ka);pk=o(qp,"This model inherits from "),Ai=a(qp,"A",{href:!0});var K0=i(Ai);hk=o(K0,"TFPreTrainedModel"),K0.forEach(n),fk=o(qp,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),qp.forEach(n),mk=d(vn),wa=a(vn,"P",{});var Pp=i(wa);gk=o(Pp,"This model is also a "),ba=a(Pp,"A",{href:!0,rel:!0});var X0=i(ba);_k=o(X0,"tf.keras.Model"),X0.forEach(n),Fk=o(Pp,` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),Pp.forEach(n),vk=d(vn),w(Mo.$$.fragment,vn),Tk=d(vn),cn=a(vn,"DIV",{class:!0});var Un=i(cn);w(ya.$$.fragment,Un),kk=d(Un),At=a(Un,"P",{});var il=i(At);wk=o(il,"The "),Di=a(il,"A",{href:!0});var J0=i(Di);bk=o(J0,"TFFunnelForMultipleChoice"),J0.forEach(n),yk=o(il," forward method, overrides the "),Vd=a(il,"CODE",{});var ey=i(Vd);$k=o(ey,"__call__"),ey.forEach(n),Ek=o(il," special method."),il.forEach(n),Mk=d(Un),w(zo.$$.fragment,Un),zk=d(Un),Yd=a(Un,"P",{});var ny=i(Yd);qk=o(ny,"Example:"),ny.forEach(n),Pk=d(Un),w($a.$$.fragment,Un),Un.forEach(n),vn.forEach(n),cu=d(s),Dt=a(s,"H2",{class:!0});var Cp=i(Dt);qo=a(Cp,"A",{id:!0,class:!0,href:!0});var ty=i(qo);Ud=a(ty,"SPAN",{});var oy=i(Ud);w(Ea.$$.fragment,oy),oy.forEach(n),ty.forEach(n),Ck=d(Cp),Gd=a(Cp,"SPAN",{});var sy=i(Gd);xk=o(sy,"TFFunnelForTokenClassification"),sy.forEach(n),Cp.forEach(n),uu=d(s),Se=a(s,"DIV",{class:!0});var Tn=i(Se);w(Ma.$$.fragment,Tn),jk=d(Tn),Zd=a(Tn,"P",{});var ry=i(Zd);Lk=o(ry,`Funnel Model with a token classification head on top (a linear layer on top of the hidden-states output) e.g. for
Named-Entity-Recognition (NER) tasks.`),ry.forEach(n),Ak=d(Tn),za=a(Tn,"P",{});var xp=i(za);Dk=o(xp,"The Funnel Transformer model was proposed in "),qa=a(xp,"A",{href:!0,rel:!0});var ay=i(qa);Ik=o(ay,`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),ay.forEach(n),Sk=o(xp," by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),xp.forEach(n),Nk=d(Tn),Pa=a(Tn,"P",{});var jp=i(Pa);Ok=o(jp,"This model inherits from "),Ii=a(jp,"A",{href:!0});var iy=i(Ii);Bk=o(iy,"TFPreTrainedModel"),iy.forEach(n),Wk=o(jp,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),jp.forEach(n),Qk=d(Tn),Ca=a(Tn,"P",{});var Lp=i(Ca);Rk=o(Lp,"This model is also a "),xa=a(Lp,"A",{href:!0,rel:!0});var ly=i(xa);Hk=o(ly,"tf.keras.Model"),ly.forEach(n),Vk=o(Lp,` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),Lp.forEach(n),Yk=d(Tn),w(Po.$$.fragment,Tn),Uk=d(Tn),un=a(Tn,"DIV",{class:!0});var Gn=i(un);w(ja.$$.fragment,Gn),Gk=d(Gn),It=a(Gn,"P",{});var ll=i(It);Zk=o(ll,"The "),Si=a(ll,"A",{href:!0});var dy=i(Si);Kk=o(dy,"TFFunnelForTokenClassification"),dy.forEach(n),Xk=o(ll," forward method, overrides the "),Kd=a(ll,"CODE",{});var cy=i(Kd);Jk=o(cy,"__call__"),cy.forEach(n),e1=o(ll," special method."),ll.forEach(n),n1=d(Gn),w(Co.$$.fragment,Gn),t1=d(Gn),Xd=a(Gn,"P",{});var uy=i(Xd);o1=o(uy,"Example:"),uy.forEach(n),s1=d(Gn),w(La.$$.fragment,Gn),Gn.forEach(n),Tn.forEach(n),pu=d(s),St=a(s,"H2",{class:!0});var Ap=i(St);xo=a(Ap,"A",{id:!0,class:!0,href:!0});var py=i(xo);Jd=a(py,"SPAN",{});var hy=i(Jd);w(Aa.$$.fragment,hy),hy.forEach(n),py.forEach(n),r1=d(Ap),ec=a(Ap,"SPAN",{});var fy=i(ec);a1=o(fy,"TFFunnelForQuestionAnswering"),fy.forEach(n),Ap.forEach(n),hu=d(s),Ne=a(s,"DIV",{class:!0});var kn=i(Ne);w(Da.$$.fragment,kn),i1=d(kn),Nt=a(kn,"P",{});var dl=i(Nt);l1=o(dl,`Funnel Model with a span classification head on top for extractive question-answering tasks like SQuAD (a linear
layers on top of the hidden-states output to compute `),nc=a(dl,"CODE",{});var my=i(nc);d1=o(my,"span start logits"),my.forEach(n),c1=o(dl," and "),tc=a(dl,"CODE",{});var gy=i(tc);u1=o(gy,"span end logits"),gy.forEach(n),p1=o(dl,")."),dl.forEach(n),h1=d(kn),Ia=a(kn,"P",{});var Dp=i(Ia);f1=o(Dp,"The Funnel Transformer model was proposed in "),Sa=a(Dp,"A",{href:!0,rel:!0});var _y=i(Sa);m1=o(_y,`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),_y.forEach(n),g1=o(Dp," by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),Dp.forEach(n),_1=d(kn),Na=a(kn,"P",{});var Ip=i(Na);F1=o(Ip,"This model inherits from "),Ni=a(Ip,"A",{href:!0});var Fy=i(Ni);v1=o(Fy,"TFPreTrainedModel"),Fy.forEach(n),T1=o(Ip,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Ip.forEach(n),k1=d(kn),Oa=a(kn,"P",{});var Sp=i(Oa);w1=o(Sp,"This model is also a "),Ba=a(Sp,"A",{href:!0,rel:!0});var vy=i(Ba);b1=o(vy,"tf.keras.Model"),vy.forEach(n),y1=o(Sp,` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),Sp.forEach(n),$1=d(kn),w(jo.$$.fragment,kn),E1=d(kn),pn=a(kn,"DIV",{class:!0});var Zn=i(pn);w(Wa.$$.fragment,Zn),M1=d(Zn),Ot=a(Zn,"P",{});var cl=i(Ot);z1=o(cl,"The "),Oi=a(cl,"A",{href:!0});var Ty=i(Oi);q1=o(Ty,"TFFunnelForQuestionAnswering"),Ty.forEach(n),P1=o(cl," forward method, overrides the "),oc=a(cl,"CODE",{});var ky=i(oc);C1=o(ky,"__call__"),ky.forEach(n),x1=o(cl," special method."),cl.forEach(n),j1=d(Zn),w(Lo.$$.fragment,Zn),L1=d(Zn),sc=a(Zn,"P",{});var wy=i(sc);A1=o(wy,"Example:"),wy.forEach(n),D1=d(Zn),w(Qa.$$.fragment,Zn),Zn.forEach(n),kn.forEach(n),this.h()},h(){c(p,"name","hf:doc:metadata"),c(p,"content",JSON.stringify(Xy)),c(g,"id","funnel-transformer"),c(g,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(g,"href","#funnel-transformer"),c(m,"class","relative group"),c(J,"id","overview"),c(J,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(J,"href","#overview"),c(q,"class","relative group"),c(te,"href","https://arxiv.org/abs/2006.03236"),c(te,"rel","nofollow"),c(re,"href","/docs/transformers/pr_15907/en/model_doc/funnel#transformers.FunnelModel"),c(u,"href","/docs/transformers/pr_15907/en/model_doc/funnel#transformers.FunnelForPreTraining"),c(Fe,"href","/docs/transformers/pr_15907/en/model_doc/funnel#transformers.FunnelForMaskedLM"),c(ve,"href","/docs/transformers/pr_15907/en/model_doc/funnel#transformers.FunnelForTokenClassification"),c(Te,"href","/docs/transformers/pr_15907/en/model_doc/funnel#transformers.FunnelBaseModel"),c(ke,"href","/docs/transformers/pr_15907/en/model_doc/funnel#transformers.FunnelForSequenceClassification"),c(Ua,"href","/docs/transformers/pr_15907/en/model_doc/funnel#transformers.FunnelForMultipleChoice"),c(So,"href","https://huggingface.co/sgugger"),c(So,"rel","nofollow"),c(No,"href","https://github.com/laiguokun/Funnel-Transformer"),c(No,"rel","nofollow"),c(Bt,"id","transformers.FunnelConfig"),c(Bt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Bt,"href","#transformers.FunnelConfig"),c(Kn,"class","relative group"),c(Ga,"href","/docs/transformers/pr_15907/en/model_doc/funnel#transformers.FunnelModel"),c(Za,"href","/docs/transformers/pr_15907/en/model_doc/bert#transformers.TFBertModel"),c(Wo,"href","https://huggingface.co/funnel-transformer/small"),c(Wo,"rel","nofollow"),c(Ka,"href","/docs/transformers/pr_15907/en/main_classes/configuration#transformers.PretrainedConfig"),c(Xa,"href","/docs/transformers/pr_15907/en/main_classes/configuration#transformers.PretrainedConfig"),c(Cn,"class","docstring"),c(Wt,"id","transformers.FunnelTokenizer"),c(Wt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Wt,"href","#transformers.FunnelTokenizer"),c(Jn,"class","relative group"),c(Ja,"href","/docs/transformers/pr_15907/en/model_doc/funnel#transformers.FunnelTokenizer"),c(ei,"href","/docs/transformers/pr_15907/en/model_doc/bert#transformers.BertTokenizer"),c(ni,"href","/docs/transformers/pr_15907/en/model_doc/bert#transformers.BertTokenizer"),c(Ln,"class","docstring"),c(Rt,"class","docstring"),c(wn,"class","docstring"),c(si,"class","docstring"),c(Pe,"class","docstring"),c(Ht,"id","transformers.FunnelTokenizerFast"),c(Ht,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Ht,"href","#transformers.FunnelTokenizerFast"),c(nt,"class","relative group"),c(ri,"href","/docs/transformers/pr_15907/en/model_doc/funnel#transformers.FunnelTokenizerFast"),c(ai,"href","/docs/transformers/pr_15907/en/model_doc/bert#transformers.BertTokenizerFast"),c(ii,"href","/docs/transformers/pr_15907/en/model_doc/bert#transformers.BertTokenizerFast"),c(bn,"class","docstring"),c(Ze,"class","docstring"),c(Yt,"id","transformers.models.funnel.modeling_funnel.FunnelForPreTrainingOutput"),c(Yt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Yt,"href","#transformers.models.funnel.modeling_funnel.FunnelForPreTrainingOutput"),c(ot,"class","relative group"),c(li,"href","/docs/transformers/pr_15907/en/model_doc/funnel#transformers.FunnelForPreTraining"),c(st,"class","docstring"),c(di,"href","/docs/transformers/pr_15907/en/model_doc/funnel#transformers.FunnelForPreTraining"),c(rt,"class","docstring"),c(Ut,"id","transformers.FunnelBaseModel"),c(Ut,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Ut,"href","#transformers.FunnelBaseModel"),c(at,"class","relative group"),c(hs,"href","https://arxiv.org/abs/2006.03236"),c(hs,"rel","nofollow"),c(ci,"href","/docs/transformers/pr_15907/en/main_classes/model#transformers.PreTrainedModel"),c(gs,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),c(gs,"rel","nofollow"),c(ui,"href","/docs/transformers/pr_15907/en/model_doc/funnel#transformers.FunnelBaseModel"),c(Ke,"class","docstring"),c(We,"class","docstring"),c(Zt,"id","transformers.FunnelModel"),c(Zt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Zt,"href","#transformers.FunnelModel"),c(lt,"class","relative group"),c(ws,"href","https://arxiv.org/abs/2006.03236"),c(ws,"rel","nofollow"),c(pi,"href","/docs/transformers/pr_15907/en/main_classes/model#transformers.PreTrainedModel"),c($s,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),c($s,"rel","nofollow"),c(hi,"href","/docs/transformers/pr_15907/en/model_doc/funnel#transformers.FunnelModel"),c(Xe,"class","docstring"),c(Qe,"class","docstring"),c(Xt,"id","transformers.FunnelForPreTraining"),c(Xt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Xt,"href","#transformers.FunnelForPreTraining"),c(ct,"class","relative group"),c(fi,"href","/docs/transformers/pr_15907/en/model_doc/funnel#transformers.FunnelForPreTraining"),c(Je,"class","docstring"),c(ut,"class","docstring"),c(eo,"id","transformers.FunnelForMaskedLM"),c(eo,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(eo,"href","#transformers.FunnelForMaskedLM"),c(ht,"class","relative group"),c(Ds,"href","https://arxiv.org/abs/2006.03236"),c(Ds,"rel","nofollow"),c(mi,"href","/docs/transformers/pr_15907/en/main_classes/model#transformers.PreTrainedModel"),c(Ns,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),c(Ns,"rel","nofollow"),c(gi,"href","/docs/transformers/pr_15907/en/model_doc/funnel#transformers.FunnelForMaskedLM"),c(en,"class","docstring"),c(Re,"class","docstring"),c(to,"id","transformers.FunnelForSequenceClassification"),c(to,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(to,"href","#transformers.FunnelForSequenceClassification"),c(mt,"class","relative group"),c(Hs,"href","https://arxiv.org/abs/2006.03236"),c(Hs,"rel","nofollow"),c(_i,"href","/docs/transformers/pr_15907/en/main_classes/model#transformers.PreTrainedModel"),c(Us,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),c(Us,"rel","nofollow"),c(Fi,"href","/docs/transformers/pr_15907/en/model_doc/funnel#transformers.FunnelForSequenceClassification"),c(Be,"class","docstring"),c(He,"class","docstring"),c(so,"id","transformers.FunnelForMultipleChoice"),c(so,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(so,"href","#transformers.FunnelForMultipleChoice"),c(_t,"class","relative group"),c(nr,"href","https://arxiv.org/abs/2006.03236"),c(nr,"rel","nofollow"),c(vi,"href","/docs/transformers/pr_15907/en/main_classes/model#transformers.PreTrainedModel"),c(sr,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),c(sr,"rel","nofollow"),c(Ti,"href","/docs/transformers/pr_15907/en/model_doc/funnel#transformers.FunnelForMultipleChoice"),c(nn,"class","docstring"),c(Ve,"class","docstring"),c(ao,"id","transformers.FunnelForTokenClassification"),c(ao,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(ao,"href","#transformers.FunnelForTokenClassification"),c(vt,"class","relative group"),c(cr,"href","https://arxiv.org/abs/2006.03236"),c(cr,"rel","nofollow"),c(ki,"href","/docs/transformers/pr_15907/en/main_classes/model#transformers.PreTrainedModel"),c(hr,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),c(hr,"rel","nofollow"),c(wi,"href","/docs/transformers/pr_15907/en/model_doc/funnel#transformers.FunnelForTokenClassification"),c(tn,"class","docstring"),c(Ye,"class","docstring"),c(lo,"id","transformers.FunnelForQuestionAnswering"),c(lo,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(lo,"href","#transformers.FunnelForQuestionAnswering"),c(kt,"class","relative group"),c(vr,"href","https://arxiv.org/abs/2006.03236"),c(vr,"rel","nofollow"),c(bi,"href","/docs/transformers/pr_15907/en/main_classes/model#transformers.PreTrainedModel"),c(wr,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),c(wr,"rel","nofollow"),c(yi,"href","/docs/transformers/pr_15907/en/model_doc/funnel#transformers.FunnelForQuestionAnswering"),c(on,"class","docstring"),c(Ue,"class","docstring"),c(uo,"id","transformers.TFFunnelBaseModel"),c(uo,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(uo,"href","#transformers.TFFunnelBaseModel"),c(yt,"class","relative group"),c(zr,"href","https://arxiv.org/abs/2006.03236"),c(zr,"rel","nofollow"),c($i,"href","/docs/transformers/pr_15907/en/main_classes/model#transformers.TFPreTrainedModel"),c(Cr,"href","https://www.tensorflow.org/api_docs/python/tf/keras/Model"),c(Cr,"rel","nofollow"),c(Ei,"href","/docs/transformers/pr_15907/en/model_doc/funnel#transformers.TFFunnelBaseModel"),c(sn,"class","docstring"),c(xe,"class","docstring"),c(fo,"id","transformers.TFFunnelModel"),c(fo,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(fo,"href","#transformers.TFFunnelModel"),c(Et,"class","relative group"),c(Ir,"href","https://arxiv.org/abs/2006.03236"),c(Ir,"rel","nofollow"),c(Mi,"href","/docs/transformers/pr_15907/en/main_classes/model#transformers.TFPreTrainedModel"),c(Or,"href","https://www.tensorflow.org/api_docs/python/tf/keras/Model"),c(Or,"rel","nofollow"),c(zi,"href","/docs/transformers/pr_15907/en/model_doc/funnel#transformers.TFFunnelModel"),c(rn,"class","docstring"),c(je,"class","docstring"),c(_o,"id","transformers.TFFunnelForPreTraining"),c(_o,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(_o,"href","#transformers.TFFunnelForPreTraining"),c(zt,"class","relative group"),c(Vr,"href","https://arxiv.org/abs/2006.03236"),c(Vr,"rel","nofollow"),c(qi,"href","/docs/transformers/pr_15907/en/main_classes/model#transformers.TFPreTrainedModel"),c(Gr,"href","https://www.tensorflow.org/api_docs/python/tf/keras/Model"),c(Gr,"rel","nofollow"),c(Pi,"href","/docs/transformers/pr_15907/en/model_doc/funnel#transformers.TFFunnelForPreTraining"),c(an,"class","docstring"),c(Le,"class","docstring"),c(To,"id","transformers.TFFunnelForMaskedLM"),c(To,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(To,"href","#transformers.TFFunnelForMaskedLM"),c(Pt,"class","relative group"),c(ta,"href","https://arxiv.org/abs/2006.03236"),c(ta,"rel","nofollow"),c(Ci,"href","/docs/transformers/pr_15907/en/main_classes/model#transformers.TFPreTrainedModel"),c(ra,"href","https://www.tensorflow.org/api_docs/python/tf/keras/Model"),c(ra,"rel","nofollow"),c(xi,"href","/docs/transformers/pr_15907/en/model_doc/funnel#transformers.TFFunnelForMaskedLM"),c(ln,"class","docstring"),c(Ae,"class","docstring"),c(bo,"id","transformers.TFFunnelForSequenceClassification"),c(bo,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(bo,"href","#transformers.TFFunnelForSequenceClassification"),c(xt,"class","relative group"),c(ua,"href","https://arxiv.org/abs/2006.03236"),c(ua,"rel","nofollow"),c(ji,"href","/docs/transformers/pr_15907/en/main_classes/model#transformers.TFPreTrainedModel"),c(fa,"href","https://www.tensorflow.org/api_docs/python/tf/keras/Model"),c(fa,"rel","nofollow"),c(Li,"href","/docs/transformers/pr_15907/en/model_doc/funnel#transformers.TFFunnelForSequenceClassification"),c(dn,"class","docstring"),c(De,"class","docstring"),c(Eo,"id","transformers.TFFunnelForMultipleChoice"),c(Eo,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Eo,"href","#transformers.TFFunnelForMultipleChoice"),c(Lt,"class","relative group"),c(Ta,"href","https://arxiv.org/abs/2006.03236"),c(Ta,"rel","nofollow"),c(Ai,"href","/docs/transformers/pr_15907/en/main_classes/model#transformers.TFPreTrainedModel"),c(ba,"href","https://www.tensorflow.org/api_docs/python/tf/keras/Model"),c(ba,"rel","nofollow"),c(Di,"href","/docs/transformers/pr_15907/en/model_doc/funnel#transformers.TFFunnelForMultipleChoice"),c(cn,"class","docstring"),c(Ie,"class","docstring"),c(qo,"id","transformers.TFFunnelForTokenClassification"),c(qo,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(qo,"href","#transformers.TFFunnelForTokenClassification"),c(Dt,"class","relative group"),c(qa,"href","https://arxiv.org/abs/2006.03236"),c(qa,"rel","nofollow"),c(Ii,"href","/docs/transformers/pr_15907/en/main_classes/model#transformers.TFPreTrainedModel"),c(xa,"href","https://www.tensorflow.org/api_docs/python/tf/keras/Model"),c(xa,"rel","nofollow"),c(Si,"href","/docs/transformers/pr_15907/en/model_doc/funnel#transformers.TFFunnelForTokenClassification"),c(un,"class","docstring"),c(Se,"class","docstring"),c(xo,"id","transformers.TFFunnelForQuestionAnswering"),c(xo,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(xo,"href","#transformers.TFFunnelForQuestionAnswering"),c(St,"class","relative group"),c(Sa,"href","https://arxiv.org/abs/2006.03236"),c(Sa,"rel","nofollow"),c(Ni,"href","/docs/transformers/pr_15907/en/main_classes/model#transformers.TFPreTrainedModel"),c(Ba,"href","https://www.tensorflow.org/api_docs/python/tf/keras/Model"),c(Ba,"rel","nofollow"),c(Oi,"href","/docs/transformers/pr_15907/en/model_doc/funnel#transformers.TFFunnelForQuestionAnswering"),c(pn,"class","docstring"),c(Ne,"class","docstring")},m(s,f){e(document.head,p),h(s,M,f),h(s,m,f),e(m,g),e(g,v),b(F,v,null),e(m,_),e(m,z),e(z,ce),h(s,G,f),h(s,q,f),e(q,J),e(J,I),b(ne,I,null),e(q,ue),e(q,S),e(S,pe),h(s,ie,f),h(s,U,f),e(U,L),e(U,te),e(te,Z),e(U,P),h(s,x,f),h(s,oe,f),e(oe,Q),h(s,le,f),h(s,se,f),e(se,N),e(N,he),h(s,de,f),h(s,C,f),e(C,fe),h(s,B,f),h(s,ee,f),e(ee,ae),e(ae,R),e(ee,me),e(ee,O),e(O,A),e(O,re),e(re,H),e(O,ge),e(O,u),e(u,T),e(O,K),e(O,Fe),e(Fe,we),e(O,D),e(O,ve),e(ve,be),e(O,ye),e(O,j),e(j,V),e(O,$e),e(O,Te),e(Te,Y),e(O,Ee),e(O,ke),e(ke,_e),e(O,Me),e(O,Ua),e(Ua,Np),e(O,Op),h(s,Ec,f),h(s,jn,f),e(jn,Bp),e(jn,So),e(So,Wp),e(jn,Qp),e(jn,No),e(No,Rp),e(jn,Hp),h(s,Mc,f),h(s,Kn,f),e(Kn,Bt),e(Bt,ul),b(Oo,ul,null),e(Kn,Vp),e(Kn,pl),e(pl,Yp),h(s,zc,f),h(s,Cn,f),b(Bo,Cn,null),e(Cn,Up),e(Cn,xn),e(xn,Gp),e(xn,Ga),e(Ga,Zp),e(xn,Kp),e(xn,Za),e(Za,Xp),e(xn,Jp),e(xn,Wo),e(Wo,eh),e(xn,nh),e(Cn,th),e(Cn,Xn),e(Xn,oh),e(Xn,Ka),e(Ka,sh),e(Xn,rh),e(Xn,Xa),e(Xa,ah),e(Xn,ih),h(s,qc,f),h(s,Jn,f),e(Jn,Wt),e(Wt,hl),b(Qo,hl,null),e(Jn,lh),e(Jn,fl),e(fl,dh),h(s,Pc,f),h(s,Pe,f),b(Ro,Pe,null),e(Pe,ch),e(Pe,ml),e(ml,uh),e(Pe,ph),e(Pe,Qt),e(Qt,Ja),e(Ja,hh),e(Qt,fh),e(Qt,ei),e(ei,mh),e(Qt,gh),e(Pe,_h),e(Pe,Ho),e(Ho,Fh),e(Ho,ni),e(ni,vh),e(Ho,Th),e(Pe,kh),e(Pe,Ln),b(Vo,Ln,null),e(Ln,wh),e(Ln,gl),e(gl,bh),e(Ln,yh),e(Ln,Yo),e(Yo,ti),e(ti,$h),e(ti,_l),e(_l,Eh),e(Yo,Mh),e(Yo,oi),e(oi,zh),e(oi,Fl),e(Fl,qh),e(Pe,Ph),e(Pe,Rt),b(Uo,Rt,null),e(Rt,Ch),e(Rt,Go),e(Go,xh),e(Go,vl),e(vl,jh),e(Go,Lh),e(Pe,Ah),e(Pe,wn),b(Zo,wn,null),e(wn,Dh),e(wn,Tl),e(Tl,Ih),e(wn,Sh),b(Ko,wn,null),e(wn,Nh),e(wn,et),e(et,Oh),e(et,kl),e(kl,Bh),e(et,Wh),e(et,wl),e(wl,Qh),e(et,Rh),e(Pe,Hh),e(Pe,si),b(Xo,si,null),h(s,Cc,f),h(s,nt,f),e(nt,Ht),e(Ht,bl),b(Jo,bl,null),e(nt,Vh),e(nt,yl),e(yl,Yh),h(s,xc,f),h(s,Ze,f),b(es,Ze,null),e(Ze,Uh),e(Ze,ns),e(ns,Gh),e(ns,$l),e($l,Zh),e(ns,Kh),e(Ze,Xh),e(Ze,Vt),e(Vt,ri),e(ri,Jh),e(Vt,ef),e(Vt,ai),e(ai,nf),e(Vt,tf),e(Ze,of),e(Ze,ts),e(ts,sf),e(ts,ii),e(ii,rf),e(ts,af),e(Ze,lf),e(Ze,bn),b(os,bn,null),e(bn,df),e(bn,El),e(El,cf),e(bn,uf),b(ss,bn,null),e(bn,pf),e(bn,tt),e(tt,hf),e(tt,Ml),e(Ml,ff),e(tt,mf),e(tt,zl),e(zl,gf),e(tt,_f),h(s,jc,f),h(s,ot,f),e(ot,Yt),e(Yt,ql),b(rs,ql,null),e(ot,Ff),e(ot,Pl),e(Pl,vf),h(s,Lc,f),h(s,st,f),b(as,st,null),e(st,Tf),e(st,is),e(is,kf),e(is,li),e(li,wf),e(is,bf),h(s,Ac,f),h(s,rt,f),b(ls,rt,null),e(rt,yf),e(rt,ds),e(ds,$f),e(ds,di),e(di,Ef),e(ds,Mf),h(s,Dc,f),h(s,at,f),e(at,Ut),e(Ut,Cl),b(cs,Cl,null),e(at,zf),e(at,xl),e(xl,qf),h(s,Ic,f),h(s,We,f),b(us,We,null),e(We,Pf),e(We,jl),e(jl,Cf),e(We,xf),e(We,ps),e(ps,jf),e(ps,hs),e(hs,Lf),e(ps,Af),e(We,Df),e(We,fs),e(fs,If),e(fs,ci),e(ci,Sf),e(fs,Nf),e(We,Of),e(We,ms),e(ms,Bf),e(ms,gs),e(gs,Wf),e(ms,Qf),e(We,Rf),e(We,Ke),b(_s,Ke,null),e(Ke,Hf),e(Ke,it),e(it,Vf),e(it,ui),e(ui,Yf),e(it,Uf),e(it,Ll),e(Ll,Gf),e(it,Zf),e(Ke,Kf),b(Gt,Ke,null),e(Ke,Xf),e(Ke,Al),e(Al,Jf),e(Ke,em),b(Fs,Ke,null),h(s,Sc,f),h(s,lt,f),e(lt,Zt),e(Zt,Dl),b(vs,Dl,null),e(lt,nm),e(lt,Il),e(Il,tm),h(s,Nc,f),h(s,Qe,f),b(Ts,Qe,null),e(Qe,om),e(Qe,Sl),e(Sl,sm),e(Qe,rm),e(Qe,ks),e(ks,am),e(ks,ws),e(ws,im),e(ks,lm),e(Qe,dm),e(Qe,bs),e(bs,cm),e(bs,pi),e(pi,um),e(bs,pm),e(Qe,hm),e(Qe,ys),e(ys,fm),e(ys,$s),e($s,mm),e(ys,gm),e(Qe,_m),e(Qe,Xe),b(Es,Xe,null),e(Xe,Fm),e(Xe,dt),e(dt,vm),e(dt,hi),e(hi,Tm),e(dt,km),e(dt,Nl),e(Nl,wm),e(dt,bm),e(Xe,ym),b(Kt,Xe,null),e(Xe,$m),e(Xe,Ol),e(Ol,Em),e(Xe,Mm),b(Ms,Xe,null),h(s,Oc,f),h(s,ct,f),e(ct,Xt),e(Xt,Bl),b(zs,Bl,null),e(ct,zm),e(ct,Wl),e(Wl,qm),h(s,Bc,f),h(s,ut,f),b(qs,ut,null),e(ut,Pm),e(ut,Je),b(Ps,Je,null),e(Je,Cm),e(Je,pt),e(pt,xm),e(pt,fi),e(fi,jm),e(pt,Lm),e(pt,Ql),e(Ql,Am),e(pt,Dm),e(Je,Im),b(Jt,Je,null),e(Je,Sm),e(Je,Rl),e(Rl,Nm),e(Je,Om),b(Cs,Je,null),h(s,Wc,f),h(s,ht,f),e(ht,eo),e(eo,Hl),b(xs,Hl,null),e(ht,Bm),e(ht,Vl),e(Vl,Wm),h(s,Qc,f),h(s,Re,f),b(js,Re,null),e(Re,Qm),e(Re,Ls),e(Ls,Rm),e(Ls,Yl),e(Yl,Hm),e(Ls,Vm),e(Re,Ym),e(Re,As),e(As,Um),e(As,Ds),e(Ds,Gm),e(As,Zm),e(Re,Km),e(Re,Is),e(Is,Xm),e(Is,mi),e(mi,Jm),e(Is,eg),e(Re,ng),e(Re,Ss),e(Ss,tg),e(Ss,Ns),e(Ns,og),e(Ss,sg),e(Re,rg),e(Re,en),b(Os,en,null),e(en,ag),e(en,ft),e(ft,ig),e(ft,gi),e(gi,lg),e(ft,dg),e(ft,Ul),e(Ul,cg),e(ft,ug),e(en,pg),b(no,en,null),e(en,hg),e(en,Gl),e(Gl,fg),e(en,mg),b(Bs,en,null),h(s,Rc,f),h(s,mt,f),e(mt,to),e(to,Zl),b(Ws,Zl,null),e(mt,gg),e(mt,Kl),e(Kl,_g),h(s,Hc,f),h(s,He,f),b(Qs,He,null),e(He,Fg),e(He,Xl),e(Xl,vg),e(He,Tg),e(He,Rs),e(Rs,kg),e(Rs,Hs),e(Hs,wg),e(Rs,bg),e(He,yg),e(He,Vs),e(Vs,$g),e(Vs,_i),e(_i,Eg),e(Vs,Mg),e(He,zg),e(He,Ys),e(Ys,qg),e(Ys,Us),e(Us,Pg),e(Ys,Cg),e(He,xg),e(He,Be),b(Gs,Be,null),e(Be,jg),e(Be,gt),e(gt,Lg),e(gt,Fi),e(Fi,Ag),e(gt,Dg),e(gt,Jl),e(Jl,Ig),e(gt,Sg),e(Be,Ng),b(oo,Be,null),e(Be,Og),e(Be,ed),e(ed,Bg),e(Be,Wg),b(Zs,Be,null),e(Be,Qg),e(Be,nd),e(nd,Rg),e(Be,Hg),b(Ks,Be,null),h(s,Vc,f),h(s,_t,f),e(_t,so),e(so,td),b(Xs,td,null),e(_t,Vg),e(_t,od),e(od,Yg),h(s,Yc,f),h(s,Ve,f),b(Js,Ve,null),e(Ve,Ug),e(Ve,sd),e(sd,Gg),e(Ve,Zg),e(Ve,er),e(er,Kg),e(er,nr),e(nr,Xg),e(er,Jg),e(Ve,e_),e(Ve,tr),e(tr,n_),e(tr,vi),e(vi,t_),e(tr,o_),e(Ve,s_),e(Ve,or),e(or,r_),e(or,sr),e(sr,a_),e(or,i_),e(Ve,l_),e(Ve,nn),b(rr,nn,null),e(nn,d_),e(nn,Ft),e(Ft,c_),e(Ft,Ti),e(Ti,u_),e(Ft,p_),e(Ft,rd),e(rd,h_),e(Ft,f_),e(nn,m_),b(ro,nn,null),e(nn,g_),e(nn,ad),e(ad,__),e(nn,F_),b(ar,nn,null),h(s,Uc,f),h(s,vt,f),e(vt,ao),e(ao,id),b(ir,id,null),e(vt,v_),e(vt,ld),e(ld,T_),h(s,Gc,f),h(s,Ye,f),b(lr,Ye,null),e(Ye,k_),e(Ye,dd),e(dd,w_),e(Ye,b_),e(Ye,dr),e(dr,y_),e(dr,cr),e(cr,$_),e(dr,E_),e(Ye,M_),e(Ye,ur),e(ur,z_),e(ur,ki),e(ki,q_),e(ur,P_),e(Ye,C_),e(Ye,pr),e(pr,x_),e(pr,hr),e(hr,j_),e(pr,L_),e(Ye,A_),e(Ye,tn),b(fr,tn,null),e(tn,D_),e(tn,Tt),e(Tt,I_),e(Tt,wi),e(wi,S_),e(Tt,N_),e(Tt,cd),e(cd,O_),e(Tt,B_),e(tn,W_),b(io,tn,null),e(tn,Q_),e(tn,ud),e(ud,R_),e(tn,H_),b(mr,tn,null),h(s,Zc,f),h(s,kt,f),e(kt,lo),e(lo,pd),b(gr,pd,null),e(kt,V_),e(kt,hd),e(hd,Y_),h(s,Kc,f),h(s,Ue,f),b(_r,Ue,null),e(Ue,U_),e(Ue,wt),e(wt,G_),e(wt,fd),e(fd,Z_),e(wt,K_),e(wt,md),e(md,X_),e(wt,J_),e(Ue,eF),e(Ue,Fr),e(Fr,nF),e(Fr,vr),e(vr,tF),e(Fr,oF),e(Ue,sF),e(Ue,Tr),e(Tr,rF),e(Tr,bi),e(bi,aF),e(Tr,iF),e(Ue,lF),e(Ue,kr),e(kr,dF),e(kr,wr),e(wr,cF),e(kr,uF),e(Ue,pF),e(Ue,on),b(br,on,null),e(on,hF),e(on,bt),e(bt,fF),e(bt,yi),e(yi,mF),e(bt,gF),e(bt,gd),e(gd,_F),e(bt,FF),e(on,vF),b(co,on,null),e(on,TF),e(on,_d),e(_d,kF),e(on,wF),b(yr,on,null),h(s,Xc,f),h(s,yt,f),e(yt,uo),e(uo,Fd),b($r,Fd,null),e(yt,bF),e(yt,vd),e(vd,yF),h(s,Jc,f),h(s,xe,f),b(Er,xe,null),e(xe,$F),e(xe,Td),e(Td,EF),e(xe,MF),e(xe,Mr),e(Mr,zF),e(Mr,zr),e(zr,qF),e(Mr,PF),e(xe,CF),e(xe,qr),e(qr,xF),e(qr,$i),e($i,jF),e(qr,LF),e(xe,AF),e(xe,Pr),e(Pr,DF),e(Pr,Cr),e(Cr,IF),e(Pr,SF),e(xe,NF),b(po,xe,null),e(xe,OF),e(xe,sn),b(xr,sn,null),e(sn,BF),e(sn,$t),e($t,WF),e($t,Ei),e(Ei,QF),e($t,RF),e($t,kd),e(kd,HF),e($t,VF),e(sn,YF),b(ho,sn,null),e(sn,UF),e(sn,wd),e(wd,GF),e(sn,ZF),b(jr,sn,null),h(s,eu,f),h(s,Et,f),e(Et,fo),e(fo,bd),b(Lr,bd,null),e(Et,KF),e(Et,yd),e(yd,XF),h(s,nu,f),h(s,je,f),b(Ar,je,null),e(je,JF),e(je,$d),e($d,ev),e(je,nv),e(je,Dr),e(Dr,tv),e(Dr,Ir),e(Ir,ov),e(Dr,sv),e(je,rv),e(je,Sr),e(Sr,av),e(Sr,Mi),e(Mi,iv),e(Sr,lv),e(je,dv),e(je,Nr),e(Nr,cv),e(Nr,Or),e(Or,uv),e(Nr,pv),e(je,hv),b(mo,je,null),e(je,fv),e(je,rn),b(Br,rn,null),e(rn,mv),e(rn,Mt),e(Mt,gv),e(Mt,zi),e(zi,_v),e(Mt,Fv),e(Mt,Ed),e(Ed,vv),e(Mt,Tv),e(rn,kv),b(go,rn,null),e(rn,wv),e(rn,Md),e(Md,bv),e(rn,yv),b(Wr,rn,null),h(s,tu,f),h(s,zt,f),e(zt,_o),e(_o,zd),b(Qr,zd,null),e(zt,$v),e(zt,qd),e(qd,Ev),h(s,ou,f),h(s,Le,f),b(Rr,Le,null),e(Le,Mv),e(Le,Pd),e(Pd,zv),e(Le,qv),e(Le,Hr),e(Hr,Pv),e(Hr,Vr),e(Vr,Cv),e(Hr,xv),e(Le,jv),e(Le,Yr),e(Yr,Lv),e(Yr,qi),e(qi,Av),e(Yr,Dv),e(Le,Iv),e(Le,Ur),e(Ur,Sv),e(Ur,Gr),e(Gr,Nv),e(Ur,Ov),e(Le,Bv),b(Fo,Le,null),e(Le,Wv),e(Le,an),b(Zr,an,null),e(an,Qv),e(an,qt),e(qt,Rv),e(qt,Pi),e(Pi,Hv),e(qt,Vv),e(qt,Cd),e(Cd,Yv),e(qt,Uv),e(an,Gv),b(vo,an,null),e(an,Zv),e(an,xd),e(xd,Kv),e(an,Xv),b(Kr,an,null),h(s,su,f),h(s,Pt,f),e(Pt,To),e(To,jd),b(Xr,jd,null),e(Pt,Jv),e(Pt,Ld),e(Ld,eT),h(s,ru,f),h(s,Ae,f),b(Jr,Ae,null),e(Ae,nT),e(Ae,ea),e(ea,tT),e(ea,Ad),e(Ad,oT),e(ea,sT),e(Ae,rT),e(Ae,na),e(na,aT),e(na,ta),e(ta,iT),e(na,lT),e(Ae,dT),e(Ae,oa),e(oa,cT),e(oa,Ci),e(Ci,uT),e(oa,pT),e(Ae,hT),e(Ae,sa),e(sa,fT),e(sa,ra),e(ra,mT),e(sa,gT),e(Ae,_T),b(ko,Ae,null),e(Ae,FT),e(Ae,ln),b(aa,ln,null),e(ln,vT),e(ln,Ct),e(Ct,TT),e(Ct,xi),e(xi,kT),e(Ct,wT),e(Ct,Dd),e(Dd,bT),e(Ct,yT),e(ln,$T),b(wo,ln,null),e(ln,ET),e(ln,Id),e(Id,MT),e(ln,zT),b(ia,ln,null),h(s,au,f),h(s,xt,f),e(xt,bo),e(bo,Sd),b(la,Sd,null),e(xt,qT),e(xt,Nd),e(Nd,PT),h(s,iu,f),h(s,De,f),b(da,De,null),e(De,CT),e(De,Od),e(Od,xT),e(De,jT),e(De,ca),e(ca,LT),e(ca,ua),e(ua,AT),e(ca,DT),e(De,IT),e(De,pa),e(pa,ST),e(pa,ji),e(ji,NT),e(pa,OT),e(De,BT),e(De,ha),e(ha,WT),e(ha,fa),e(fa,QT),e(ha,RT),e(De,HT),b(yo,De,null),e(De,VT),e(De,dn),b(ma,dn,null),e(dn,YT),e(dn,jt),e(jt,UT),e(jt,Li),e(Li,GT),e(jt,ZT),e(jt,Bd),e(Bd,KT),e(jt,XT),e(dn,JT),b($o,dn,null),e(dn,ek),e(dn,Wd),e(Wd,nk),e(dn,tk),b(ga,dn,null),h(s,lu,f),h(s,Lt,f),e(Lt,Eo),e(Eo,Qd),b(_a,Qd,null),e(Lt,ok),e(Lt,Rd),e(Rd,sk),h(s,du,f),h(s,Ie,f),b(Fa,Ie,null),e(Ie,rk),e(Ie,Hd),e(Hd,ak),e(Ie,ik),e(Ie,va),e(va,lk),e(va,Ta),e(Ta,dk),e(va,ck),e(Ie,uk),e(Ie,ka),e(ka,pk),e(ka,Ai),e(Ai,hk),e(ka,fk),e(Ie,mk),e(Ie,wa),e(wa,gk),e(wa,ba),e(ba,_k),e(wa,Fk),e(Ie,vk),b(Mo,Ie,null),e(Ie,Tk),e(Ie,cn),b(ya,cn,null),e(cn,kk),e(cn,At),e(At,wk),e(At,Di),e(Di,bk),e(At,yk),e(At,Vd),e(Vd,$k),e(At,Ek),e(cn,Mk),b(zo,cn,null),e(cn,zk),e(cn,Yd),e(Yd,qk),e(cn,Pk),b($a,cn,null),h(s,cu,f),h(s,Dt,f),e(Dt,qo),e(qo,Ud),b(Ea,Ud,null),e(Dt,Ck),e(Dt,Gd),e(Gd,xk),h(s,uu,f),h(s,Se,f),b(Ma,Se,null),e(Se,jk),e(Se,Zd),e(Zd,Lk),e(Se,Ak),e(Se,za),e(za,Dk),e(za,qa),e(qa,Ik),e(za,Sk),e(Se,Nk),e(Se,Pa),e(Pa,Ok),e(Pa,Ii),e(Ii,Bk),e(Pa,Wk),e(Se,Qk),e(Se,Ca),e(Ca,Rk),e(Ca,xa),e(xa,Hk),e(Ca,Vk),e(Se,Yk),b(Po,Se,null),e(Se,Uk),e(Se,un),b(ja,un,null),e(un,Gk),e(un,It),e(It,Zk),e(It,Si),e(Si,Kk),e(It,Xk),e(It,Kd),e(Kd,Jk),e(It,e1),e(un,n1),b(Co,un,null),e(un,t1),e(un,Xd),e(Xd,o1),e(un,s1),b(La,un,null),h(s,pu,f),h(s,St,f),e(St,xo),e(xo,Jd),b(Aa,Jd,null),e(St,r1),e(St,ec),e(ec,a1),h(s,hu,f),h(s,Ne,f),b(Da,Ne,null),e(Ne,i1),e(Ne,Nt),e(Nt,l1),e(Nt,nc),e(nc,d1),e(Nt,c1),e(Nt,tc),e(tc,u1),e(Nt,p1),e(Ne,h1),e(Ne,Ia),e(Ia,f1),e(Ia,Sa),e(Sa,m1),e(Ia,g1),e(Ne,_1),e(Ne,Na),e(Na,F1),e(Na,Ni),e(Ni,v1),e(Na,T1),e(Ne,k1),e(Ne,Oa),e(Oa,w1),e(Oa,Ba),e(Ba,b1),e(Oa,y1),e(Ne,$1),b(jo,Ne,null),e(Ne,E1),e(Ne,pn),b(Wa,pn,null),e(pn,M1),e(pn,Ot),e(Ot,z1),e(Ot,Oi),e(Oi,q1),e(Ot,P1),e(Ot,oc),e(oc,C1),e(Ot,x1),e(pn,j1),b(Lo,pn,null),e(pn,L1),e(pn,sc),e(sc,A1),e(pn,D1),b(Qa,pn,null),fu=!0},p(s,[f]){const Ra={};f&2&&(Ra.$$scope={dirty:f,ctx:s}),Gt.$set(Ra);const rc={};f&2&&(rc.$$scope={dirty:f,ctx:s}),Kt.$set(rc);const ac={};f&2&&(ac.$$scope={dirty:f,ctx:s}),Jt.$set(ac);const ic={};f&2&&(ic.$$scope={dirty:f,ctx:s}),no.$set(ic);const Ha={};f&2&&(Ha.$$scope={dirty:f,ctx:s}),oo.$set(Ha);const lc={};f&2&&(lc.$$scope={dirty:f,ctx:s}),ro.$set(lc);const dc={};f&2&&(dc.$$scope={dirty:f,ctx:s}),io.$set(dc);const cc={};f&2&&(cc.$$scope={dirty:f,ctx:s}),co.$set(cc);const Va={};f&2&&(Va.$$scope={dirty:f,ctx:s}),po.$set(Va);const uc={};f&2&&(uc.$$scope={dirty:f,ctx:s}),ho.$set(uc);const pc={};f&2&&(pc.$$scope={dirty:f,ctx:s}),mo.$set(pc);const hc={};f&2&&(hc.$$scope={dirty:f,ctx:s}),go.$set(hc);const fc={};f&2&&(fc.$$scope={dirty:f,ctx:s}),Fo.$set(fc);const mc={};f&2&&(mc.$$scope={dirty:f,ctx:s}),vo.$set(mc);const Ya={};f&2&&(Ya.$$scope={dirty:f,ctx:s}),ko.$set(Ya);const gc={};f&2&&(gc.$$scope={dirty:f,ctx:s}),wo.$set(gc);const Ce={};f&2&&(Ce.$$scope={dirty:f,ctx:s}),yo.$set(Ce);const _c={};f&2&&(_c.$$scope={dirty:f,ctx:s}),$o.$set(_c);const Fc={};f&2&&(Fc.$$scope={dirty:f,ctx:s}),Mo.$set(Fc);const vc={};f&2&&(vc.$$scope={dirty:f,ctx:s}),zo.$set(vc);const Tc={};f&2&&(Tc.$$scope={dirty:f,ctx:s}),Po.$set(Tc);const kc={};f&2&&(kc.$$scope={dirty:f,ctx:s}),Co.$set(kc);const wc={};f&2&&(wc.$$scope={dirty:f,ctx:s}),jo.$set(wc);const bc={};f&2&&(bc.$$scope={dirty:f,ctx:s}),Lo.$set(bc)},i(s){fu||(y(F.$$.fragment,s),y(ne.$$.fragment,s),y(Oo.$$.fragment,s),y(Bo.$$.fragment,s),y(Qo.$$.fragment,s),y(Ro.$$.fragment,s),y(Vo.$$.fragment,s),y(Uo.$$.fragment,s),y(Zo.$$.fragment,s),y(Ko.$$.fragment,s),y(Xo.$$.fragment,s),y(Jo.$$.fragment,s),y(es.$$.fragment,s),y(os.$$.fragment,s),y(ss.$$.fragment,s),y(rs.$$.fragment,s),y(as.$$.fragment,s),y(ls.$$.fragment,s),y(cs.$$.fragment,s),y(us.$$.fragment,s),y(_s.$$.fragment,s),y(Gt.$$.fragment,s),y(Fs.$$.fragment,s),y(vs.$$.fragment,s),y(Ts.$$.fragment,s),y(Es.$$.fragment,s),y(Kt.$$.fragment,s),y(Ms.$$.fragment,s),y(zs.$$.fragment,s),y(qs.$$.fragment,s),y(Ps.$$.fragment,s),y(Jt.$$.fragment,s),y(Cs.$$.fragment,s),y(xs.$$.fragment,s),y(js.$$.fragment,s),y(Os.$$.fragment,s),y(no.$$.fragment,s),y(Bs.$$.fragment,s),y(Ws.$$.fragment,s),y(Qs.$$.fragment,s),y(Gs.$$.fragment,s),y(oo.$$.fragment,s),y(Zs.$$.fragment,s),y(Ks.$$.fragment,s),y(Xs.$$.fragment,s),y(Js.$$.fragment,s),y(rr.$$.fragment,s),y(ro.$$.fragment,s),y(ar.$$.fragment,s),y(ir.$$.fragment,s),y(lr.$$.fragment,s),y(fr.$$.fragment,s),y(io.$$.fragment,s),y(mr.$$.fragment,s),y(gr.$$.fragment,s),y(_r.$$.fragment,s),y(br.$$.fragment,s),y(co.$$.fragment,s),y(yr.$$.fragment,s),y($r.$$.fragment,s),y(Er.$$.fragment,s),y(po.$$.fragment,s),y(xr.$$.fragment,s),y(ho.$$.fragment,s),y(jr.$$.fragment,s),y(Lr.$$.fragment,s),y(Ar.$$.fragment,s),y(mo.$$.fragment,s),y(Br.$$.fragment,s),y(go.$$.fragment,s),y(Wr.$$.fragment,s),y(Qr.$$.fragment,s),y(Rr.$$.fragment,s),y(Fo.$$.fragment,s),y(Zr.$$.fragment,s),y(vo.$$.fragment,s),y(Kr.$$.fragment,s),y(Xr.$$.fragment,s),y(Jr.$$.fragment,s),y(ko.$$.fragment,s),y(aa.$$.fragment,s),y(wo.$$.fragment,s),y(ia.$$.fragment,s),y(la.$$.fragment,s),y(da.$$.fragment,s),y(yo.$$.fragment,s),y(ma.$$.fragment,s),y($o.$$.fragment,s),y(ga.$$.fragment,s),y(_a.$$.fragment,s),y(Fa.$$.fragment,s),y(Mo.$$.fragment,s),y(ya.$$.fragment,s),y(zo.$$.fragment,s),y($a.$$.fragment,s),y(Ea.$$.fragment,s),y(Ma.$$.fragment,s),y(Po.$$.fragment,s),y(ja.$$.fragment,s),y(Co.$$.fragment,s),y(La.$$.fragment,s),y(Aa.$$.fragment,s),y(Da.$$.fragment,s),y(jo.$$.fragment,s),y(Wa.$$.fragment,s),y(Lo.$$.fragment,s),y(Qa.$$.fragment,s),fu=!0)},o(s){$(F.$$.fragment,s),$(ne.$$.fragment,s),$(Oo.$$.fragment,s),$(Bo.$$.fragment,s),$(Qo.$$.fragment,s),$(Ro.$$.fragment,s),$(Vo.$$.fragment,s),$(Uo.$$.fragment,s),$(Zo.$$.fragment,s),$(Ko.$$.fragment,s),$(Xo.$$.fragment,s),$(Jo.$$.fragment,s),$(es.$$.fragment,s),$(os.$$.fragment,s),$(ss.$$.fragment,s),$(rs.$$.fragment,s),$(as.$$.fragment,s),$(ls.$$.fragment,s),$(cs.$$.fragment,s),$(us.$$.fragment,s),$(_s.$$.fragment,s),$(Gt.$$.fragment,s),$(Fs.$$.fragment,s),$(vs.$$.fragment,s),$(Ts.$$.fragment,s),$(Es.$$.fragment,s),$(Kt.$$.fragment,s),$(Ms.$$.fragment,s),$(zs.$$.fragment,s),$(qs.$$.fragment,s),$(Ps.$$.fragment,s),$(Jt.$$.fragment,s),$(Cs.$$.fragment,s),$(xs.$$.fragment,s),$(js.$$.fragment,s),$(Os.$$.fragment,s),$(no.$$.fragment,s),$(Bs.$$.fragment,s),$(Ws.$$.fragment,s),$(Qs.$$.fragment,s),$(Gs.$$.fragment,s),$(oo.$$.fragment,s),$(Zs.$$.fragment,s),$(Ks.$$.fragment,s),$(Xs.$$.fragment,s),$(Js.$$.fragment,s),$(rr.$$.fragment,s),$(ro.$$.fragment,s),$(ar.$$.fragment,s),$(ir.$$.fragment,s),$(lr.$$.fragment,s),$(fr.$$.fragment,s),$(io.$$.fragment,s),$(mr.$$.fragment,s),$(gr.$$.fragment,s),$(_r.$$.fragment,s),$(br.$$.fragment,s),$(co.$$.fragment,s),$(yr.$$.fragment,s),$($r.$$.fragment,s),$(Er.$$.fragment,s),$(po.$$.fragment,s),$(xr.$$.fragment,s),$(ho.$$.fragment,s),$(jr.$$.fragment,s),$(Lr.$$.fragment,s),$(Ar.$$.fragment,s),$(mo.$$.fragment,s),$(Br.$$.fragment,s),$(go.$$.fragment,s),$(Wr.$$.fragment,s),$(Qr.$$.fragment,s),$(Rr.$$.fragment,s),$(Fo.$$.fragment,s),$(Zr.$$.fragment,s),$(vo.$$.fragment,s),$(Kr.$$.fragment,s),$(Xr.$$.fragment,s),$(Jr.$$.fragment,s),$(ko.$$.fragment,s),$(aa.$$.fragment,s),$(wo.$$.fragment,s),$(ia.$$.fragment,s),$(la.$$.fragment,s),$(da.$$.fragment,s),$(yo.$$.fragment,s),$(ma.$$.fragment,s),$($o.$$.fragment,s),$(ga.$$.fragment,s),$(_a.$$.fragment,s),$(Fa.$$.fragment,s),$(Mo.$$.fragment,s),$(ya.$$.fragment,s),$(zo.$$.fragment,s),$($a.$$.fragment,s),$(Ea.$$.fragment,s),$(Ma.$$.fragment,s),$(Po.$$.fragment,s),$(ja.$$.fragment,s),$(Co.$$.fragment,s),$(La.$$.fragment,s),$(Aa.$$.fragment,s),$(Da.$$.fragment,s),$(jo.$$.fragment,s),$(Wa.$$.fragment,s),$(Lo.$$.fragment,s),$(Qa.$$.fragment,s),fu=!1},d(s){n(p),s&&n(M),s&&n(m),E(F),s&&n(G),s&&n(q),E(ne),s&&n(ie),s&&n(U),s&&n(x),s&&n(oe),s&&n(le),s&&n(se),s&&n(de),s&&n(C),s&&n(B),s&&n(ee),s&&n(Ec),s&&n(jn),s&&n(Mc),s&&n(Kn),E(Oo),s&&n(zc),s&&n(Cn),E(Bo),s&&n(qc),s&&n(Jn),E(Qo),s&&n(Pc),s&&n(Pe),E(Ro),E(Vo),E(Uo),E(Zo),E(Ko),E(Xo),s&&n(Cc),s&&n(nt),E(Jo),s&&n(xc),s&&n(Ze),E(es),E(os),E(ss),s&&n(jc),s&&n(ot),E(rs),s&&n(Lc),s&&n(st),E(as),s&&n(Ac),s&&n(rt),E(ls),s&&n(Dc),s&&n(at),E(cs),s&&n(Ic),s&&n(We),E(us),E(_s),E(Gt),E(Fs),s&&n(Sc),s&&n(lt),E(vs),s&&n(Nc),s&&n(Qe),E(Ts),E(Es),E(Kt),E(Ms),s&&n(Oc),s&&n(ct),E(zs),s&&n(Bc),s&&n(ut),E(qs),E(Ps),E(Jt),E(Cs),s&&n(Wc),s&&n(ht),E(xs),s&&n(Qc),s&&n(Re),E(js),E(Os),E(no),E(Bs),s&&n(Rc),s&&n(mt),E(Ws),s&&n(Hc),s&&n(He),E(Qs),E(Gs),E(oo),E(Zs),E(Ks),s&&n(Vc),s&&n(_t),E(Xs),s&&n(Yc),s&&n(Ve),E(Js),E(rr),E(ro),E(ar),s&&n(Uc),s&&n(vt),E(ir),s&&n(Gc),s&&n(Ye),E(lr),E(fr),E(io),E(mr),s&&n(Zc),s&&n(kt),E(gr),s&&n(Kc),s&&n(Ue),E(_r),E(br),E(co),E(yr),s&&n(Xc),s&&n(yt),E($r),s&&n(Jc),s&&n(xe),E(Er),E(po),E(xr),E(ho),E(jr),s&&n(eu),s&&n(Et),E(Lr),s&&n(nu),s&&n(je),E(Ar),E(mo),E(Br),E(go),E(Wr),s&&n(tu),s&&n(zt),E(Qr),s&&n(ou),s&&n(Le),E(Rr),E(Fo),E(Zr),E(vo),E(Kr),s&&n(su),s&&n(Pt),E(Xr),s&&n(ru),s&&n(Ae),E(Jr),E(ko),E(aa),E(wo),E(ia),s&&n(au),s&&n(xt),E(la),s&&n(iu),s&&n(De),E(da),E(yo),E(ma),E($o),E(ga),s&&n(lu),s&&n(Lt),E(_a),s&&n(du),s&&n(Ie),E(Fa),E(Mo),E(ya),E(zo),E($a),s&&n(cu),s&&n(Dt),E(Ea),s&&n(uu),s&&n(Se),E(Ma),E(Po),E(ja),E(Co),E(La),s&&n(pu),s&&n(St),E(Aa),s&&n(hu),s&&n(Ne),E(Da),E(jo),E(Wa),E(Lo),E(Qa)}}}const Xy={local:"funnel-transformer",sections:[{local:"overview",title:"Overview"},{local:"transformers.FunnelConfig",title:"FunnelConfig"},{local:"transformers.FunnelTokenizer",title:"FunnelTokenizer"},{local:"transformers.FunnelTokenizerFast",title:"FunnelTokenizerFast"},{local:"transformers.models.funnel.modeling_funnel.FunnelForPreTrainingOutput",title:"Funnel specific outputs"},{local:"transformers.FunnelBaseModel",title:"FunnelBaseModel"},{local:"transformers.FunnelModel",title:"FunnelModel"},{local:"transformers.FunnelForPreTraining",title:"FunnelModelForPreTraining"},{local:"transformers.FunnelForMaskedLM",title:"FunnelForMaskedLM"},{local:"transformers.FunnelForSequenceClassification",title:"FunnelForSequenceClassification"},{local:"transformers.FunnelForMultipleChoice",title:"FunnelForMultipleChoice"},{local:"transformers.FunnelForTokenClassification",title:"FunnelForTokenClassification"},{local:"transformers.FunnelForQuestionAnswering",title:"FunnelForQuestionAnswering"},{local:"transformers.TFFunnelBaseModel",title:"TFFunnelBaseModel"},{local:"transformers.TFFunnelModel",title:"TFFunnelModel"},{local:"transformers.TFFunnelForPreTraining",title:"TFFunnelModelForPreTraining"},{local:"transformers.TFFunnelForMaskedLM",title:"TFFunnelForMaskedLM"},{local:"transformers.TFFunnelForSequenceClassification",title:"TFFunnelForSequenceClassification"},{local:"transformers.TFFunnelForMultipleChoice",title:"TFFunnelForMultipleChoice"},{local:"transformers.TFFunnelForTokenClassification",title:"TFFunnelForTokenClassification"},{local:"transformers.TFFunnelForQuestionAnswering",title:"TFFunnelForQuestionAnswering"}],title:"Funnel Transformer"};function Jy(W,p,M){let{fw:m}=p;return W.$$set=g=>{"fw"in g&&M(0,m=g.fw)},[m]}class a$ extends by{constructor(p){super();yy(this,p,Jy,Ky,$y,{fw:0})}}export{a$ as default,Xy as metadata};
