import{S as u_t,i as b_t,s as v_t,e as a,k as l,w as f,t as o,L as T_t,c as n,d as t,m as i,a as s,x as m,h as r,b as c,J as e,g as b,y as g,q as h,o as p,B as _}from"../../chunks/vendor-9e2b328e.js";import{T as _wr}from"../../chunks/Tip-76f97a76.js";import{D as E}from"../../chunks/Docstring-50fd6873.js";import{C as w}from"../../chunks/CodeBlock-88e23343.js";import{I as z}from"../../chunks/IconCopyLink-fd0e58fd.js";import"../../chunks/CopyButton-4ae140ab.js";function F_t(yi){let J,Ae,ie,me,to,ce,ue,Do,wi,Ef,sa,Ai,Li,oE,yf,ye,io,Bi,Pn,rE,$n,In,tE,ki,jn,aE,xi,wf,$a;return{c(){J=a("p"),Ae=o("If your "),ie=a("code"),me=o("NewModelConfig"),to=o(" is a subclass of "),ce=a("code"),ue=o("PretrainedConfig"),Do=o(`, make sure its
`),wi=a("code"),Ef=o("model_type"),sa=o(" attribute is set to the same key you use when registering the config (here "),Ai=a("code"),Li=o('"new-model"'),oE=o(")."),yf=l(),ye=a("p"),io=o("Likewise, if your "),Bi=a("code"),Pn=o("NewModel"),rE=o(" is a subclass of "),$n=a("a"),In=o("PreTrainedModel"),tE=o(`, make sure its
`),ki=a("code"),jn=o("config_class"),aE=o(` attribute is set to the same class you use when registering the model (here
`),xi=a("code"),wf=o("NewModelConfig"),$a=o(")."),this.h()},l(co){J=n(co,"P",{});var ge=s(J);Ae=r(ge,"If your "),ie=n(ge,"CODE",{});var DL=s(ie);me=r(DL,"NewModelConfig"),DL.forEach(t),to=r(ge," is a subclass of "),ce=n(ge,"CODE",{});var Ri=s(ce);ue=r(Ri,"PretrainedConfig"),Ri.forEach(t),Do=r(ge,`, make sure its
`),wi=n(ge,"CODE",{});var qL=s(wi);Ef=r(qL,"model_type"),qL.forEach(t),sa=r(ge," attribute is set to the same key you use when registering the config (here "),Ai=n(ge,"CODE",{});var GL=s(Ai);Li=r(GL,'"new-model"'),GL.forEach(t),oE=r(ge,")."),ge.forEach(t),yf=i(co),ye=n(co,"P",{});var qo=s(ye);io=r(qo,"Likewise, if your "),Bi=n(qo,"CODE",{});var Ia=s(Bi);Pn=r(Ia,"NewModel"),Ia.forEach(t),rE=r(qo," is a subclass of "),$n=n(qo,"A",{href:!0});var OL=s($n);In=r(OL,"PreTrainedModel"),OL.forEach(t),tE=r(qo,`, make sure its
`),ki=n(qo,"CODE",{});var Af=s(ki);jn=r(Af,"config_class"),Af.forEach(t),aE=r(qo,` attribute is set to the same class you use when registering the model (here
`),xi=n(qo,"CODE",{});var XL=s(xi);wf=r(XL,"NewModelConfig"),XL.forEach(t),$a=r(qo,")."),qo.forEach(t),this.h()},h(){c($n,"href","/docs/transformers/pr_15774/en/main_classes/model#transformers.PreTrainedModel")},m(co,ge){b(co,J,ge),e(J,Ae),e(J,ie),e(ie,me),e(J,to),e(J,ce),e(ce,ue),e(J,Do),e(J,wi),e(wi,Ef),e(J,sa),e(J,Ai),e(Ai,Li),e(J,oE),b(co,yf,ge),b(co,ye,ge),e(ye,io),e(ye,Bi),e(Bi,Pn),e(ye,rE),e(ye,$n),e($n,In),e(ye,tE),e(ye,ki),e(ki,jn),e(ye,aE),e(ye,xi),e(xi,wf),e(ye,$a)},d(co){co&&t(J),co&&t(yf),co&&t(ye)}}}function C_t(yi){let J,Ae,ie,me,to;return{c(){J=a("p"),Ae=o("Passing "),ie=a("code"),me=o("use_auth_token=True"),to=o(" is required when you want to use a private model.")},l(ce){J=n(ce,"P",{});var ue=s(J);Ae=r(ue,"Passing "),ie=n(ue,"CODE",{});var Do=s(ie);me=r(Do,"use_auth_token=True"),Do.forEach(t),to=r(ue," is required when you want to use a private model."),ue.forEach(t)},m(ce,ue){b(ce,J,ue),e(J,Ae),e(J,ie),e(ie,me),e(J,to)},d(ce){ce&&t(J)}}}function M_t(yi){let J,Ae,ie,me,to;return{c(){J=a("p"),Ae=o("Passing "),ie=a("code"),me=o("use_auth_token=True"),to=o(" is required when you want to use a private model.")},l(ce){J=n(ce,"P",{});var ue=s(J);Ae=r(ue,"Passing "),ie=n(ue,"CODE",{});var Do=s(ie);me=r(Do,"use_auth_token=True"),Do.forEach(t),to=r(ue," is required when you want to use a private model."),ue.forEach(t)},m(ce,ue){b(ce,J,ue),e(J,Ae),e(J,ie),e(ie,me),e(J,to)},d(ce){ce&&t(J)}}}function E_t(yi){let J,Ae,ie,me,to,ce,ue,Do,wi,Ef,sa,Ai,Li,oE,yf,ye,io,Bi,Pn,rE,$n,In,tE,ki,jn,aE,xi,wf,$a,co,ge,DL,Ri,qL,GL,qo,Ia,OL,Af,XL,mxe,t8e,Si,Lf,$V,nE,gxe,IV,hxe,a8e,Nn,pxe,jV,_xe,uxe,NV,bxe,vxe,n8e,sE,s8e,zL,Txe,l8e,Bf,i8e,Pi,kf,DV,lE,Fxe,qV,Cxe,d8e,Go,iE,Mxe,dE,Exe,VL,yxe,wxe,Axe,cE,Lxe,GV,Bxe,kxe,xxe,fo,fE,Rxe,OV,Sxe,Pxe,$i,$xe,XV,Ixe,jxe,zV,Nxe,Dxe,qxe,v,xf,VV,Gxe,Oxe,WL,Xxe,zxe,Vxe,Rf,WV,Wxe,Qxe,QL,Hxe,Uxe,Jxe,Sf,QV,Yxe,Kxe,HL,Zxe,eRe,oRe,Pf,HV,rRe,tRe,UL,aRe,nRe,sRe,$f,UV,lRe,iRe,JL,dRe,cRe,fRe,If,JV,mRe,gRe,YL,hRe,pRe,_Re,jf,YV,uRe,bRe,KL,vRe,TRe,FRe,Nf,KV,CRe,MRe,ZL,ERe,yRe,wRe,Df,ZV,ARe,LRe,e8,BRe,kRe,xRe,qf,eW,RRe,SRe,o8,PRe,$Re,IRe,Gf,oW,jRe,NRe,r8,DRe,qRe,GRe,Of,rW,ORe,XRe,t8,zRe,VRe,WRe,Xf,tW,QRe,HRe,a8,URe,JRe,YRe,zf,aW,KRe,ZRe,n8,eSe,oSe,rSe,Vf,nW,tSe,aSe,s8,nSe,sSe,lSe,Wf,sW,iSe,dSe,l8,cSe,fSe,mSe,Qf,lW,gSe,hSe,i8,pSe,_Se,uSe,Hf,iW,bSe,vSe,d8,TSe,FSe,CSe,Uf,dW,MSe,ESe,c8,ySe,wSe,ASe,Jf,cW,LSe,BSe,f8,kSe,xSe,RSe,Yf,fW,SSe,PSe,m8,$Se,ISe,jSe,Kf,mW,NSe,DSe,g8,qSe,GSe,OSe,Zf,gW,XSe,zSe,h8,VSe,WSe,QSe,em,hW,HSe,USe,p8,JSe,YSe,KSe,om,pW,ZSe,ePe,_8,oPe,rPe,tPe,rm,_W,aPe,nPe,u8,sPe,lPe,iPe,tm,uW,dPe,cPe,b8,fPe,mPe,gPe,am,bW,hPe,pPe,v8,_Pe,uPe,bPe,nm,vW,vPe,TPe,T8,FPe,CPe,MPe,sm,TW,EPe,yPe,F8,wPe,APe,LPe,lm,FW,BPe,kPe,C8,xPe,RPe,SPe,im,CW,PPe,$Pe,M8,IPe,jPe,NPe,dm,MW,DPe,qPe,E8,GPe,OPe,XPe,cm,EW,zPe,VPe,y8,WPe,QPe,HPe,fm,yW,UPe,JPe,w8,YPe,KPe,ZPe,mm,wW,e$e,o$e,A8,r$e,t$e,a$e,gm,AW,n$e,s$e,L8,l$e,i$e,d$e,hm,LW,c$e,f$e,B8,m$e,g$e,h$e,pm,BW,p$e,_$e,k8,u$e,b$e,v$e,_m,kW,T$e,F$e,x8,C$e,M$e,E$e,um,xW,y$e,w$e,R8,A$e,L$e,B$e,bm,RW,k$e,x$e,S8,R$e,S$e,P$e,vm,SW,$$e,I$e,P8,j$e,N$e,D$e,Tm,PW,q$e,G$e,$8,O$e,X$e,z$e,Fm,$W,V$e,W$e,I8,Q$e,H$e,U$e,Cm,IW,J$e,Y$e,j8,K$e,Z$e,eIe,Mm,jW,oIe,rIe,N8,tIe,aIe,nIe,Em,NW,sIe,lIe,D8,iIe,dIe,cIe,ym,DW,fIe,mIe,q8,gIe,hIe,pIe,wm,qW,_Ie,uIe,G8,bIe,vIe,TIe,Am,GW,FIe,CIe,O8,MIe,EIe,yIe,Lm,OW,wIe,AIe,X8,LIe,BIe,kIe,Bm,XW,xIe,RIe,z8,SIe,PIe,$Ie,km,zW,IIe,jIe,V8,NIe,DIe,qIe,xm,VW,GIe,OIe,W8,XIe,zIe,VIe,Rm,WW,WIe,QIe,Q8,HIe,UIe,JIe,Sm,QW,YIe,KIe,H8,ZIe,eje,oje,Pm,HW,rje,tje,U8,aje,nje,sje,$m,UW,lje,ije,J8,dje,cje,fje,Im,JW,mje,gje,Y8,hje,pje,_je,jm,YW,uje,bje,K8,vje,Tje,Fje,Nm,KW,Cje,Mje,Z8,Eje,yje,wje,Dm,ZW,Aje,Lje,e9,Bje,kje,xje,qm,eQ,Rje,Sje,o9,Pje,$je,Ije,Gm,oQ,jje,Nje,r9,Dje,qje,Gje,Om,rQ,Oje,Xje,t9,zje,Vje,Wje,Xm,tQ,Qje,Hje,a9,Uje,Jje,Yje,zm,aQ,Kje,Zje,n9,eNe,oNe,rNe,Vm,nQ,tNe,aNe,s9,nNe,sNe,lNe,Wm,sQ,iNe,dNe,l9,cNe,fNe,mNe,Qm,lQ,gNe,hNe,i9,pNe,_Ne,uNe,Hm,iQ,bNe,vNe,d9,TNe,FNe,CNe,Um,dQ,MNe,ENe,c9,yNe,wNe,ANe,Jm,cQ,LNe,BNe,f9,kNe,xNe,RNe,Ym,fQ,SNe,PNe,m9,$Ne,INe,jNe,Km,mQ,NNe,DNe,g9,qNe,GNe,ONe,Zm,gQ,XNe,zNe,h9,VNe,WNe,QNe,eg,hQ,HNe,UNe,p9,JNe,YNe,KNe,og,pQ,ZNe,eDe,_9,oDe,rDe,tDe,rg,_Q,aDe,nDe,u9,sDe,lDe,iDe,tg,uQ,dDe,cDe,b9,fDe,mDe,gDe,ag,bQ,hDe,pDe,v9,_De,uDe,bDe,ng,vQ,vDe,TDe,T9,FDe,CDe,MDe,sg,TQ,EDe,yDe,F9,wDe,ADe,LDe,lg,FQ,BDe,kDe,C9,xDe,RDe,SDe,ig,CQ,PDe,$De,M9,IDe,jDe,NDe,dg,MQ,DDe,qDe,E9,GDe,ODe,XDe,cg,EQ,zDe,VDe,y9,WDe,QDe,HDe,fg,yQ,UDe,JDe,w9,YDe,KDe,ZDe,mg,wQ,eqe,oqe,A9,rqe,tqe,aqe,gg,AQ,nqe,sqe,L9,lqe,iqe,dqe,LQ,cqe,fqe,mE,mqe,hg,gE,gqe,BQ,hqe,c8e,Ii,pg,kQ,hE,pqe,xQ,_qe,f8e,Oo,pE,uqe,_E,bqe,B9,vqe,Tqe,Fqe,uE,Cqe,RQ,Mqe,Eqe,yqe,mo,bE,wqe,SQ,Aqe,Lqe,ja,Bqe,PQ,kqe,xqe,$Q,Rqe,Sqe,IQ,Pqe,$qe,Iqe,M,Dn,jQ,jqe,Nqe,k9,Dqe,qqe,x9,Gqe,Oqe,Xqe,qn,NQ,zqe,Vqe,R9,Wqe,Qqe,S9,Hqe,Uqe,Jqe,Gn,DQ,Yqe,Kqe,P9,Zqe,eGe,$9,oGe,rGe,tGe,_g,qQ,aGe,nGe,I9,sGe,lGe,iGe,On,GQ,dGe,cGe,j9,fGe,mGe,N9,gGe,hGe,pGe,ug,OQ,_Ge,uGe,D9,bGe,vGe,TGe,bg,XQ,FGe,CGe,q9,MGe,EGe,yGe,vg,zQ,wGe,AGe,G9,LGe,BGe,kGe,Xn,VQ,xGe,RGe,O9,SGe,PGe,X9,$Ge,IGe,jGe,zn,WQ,NGe,DGe,z9,qGe,GGe,V9,OGe,XGe,zGe,Vn,QQ,VGe,WGe,W9,QGe,HGe,Q9,UGe,JGe,YGe,Tg,HQ,KGe,ZGe,H9,eOe,oOe,rOe,Fg,UQ,tOe,aOe,U9,nOe,sOe,lOe,Wn,JQ,iOe,dOe,J9,cOe,fOe,Y9,mOe,gOe,hOe,Cg,YQ,pOe,_Oe,K9,uOe,bOe,vOe,Qn,KQ,TOe,FOe,Z9,COe,MOe,eB,EOe,yOe,wOe,Hn,ZQ,AOe,LOe,oB,BOe,kOe,rB,xOe,ROe,SOe,Un,eH,POe,$Oe,tB,IOe,jOe,oH,NOe,DOe,qOe,Mg,rH,GOe,OOe,aB,XOe,zOe,VOe,Jn,tH,WOe,QOe,nB,HOe,UOe,sB,JOe,YOe,KOe,Eg,aH,ZOe,eXe,lB,oXe,rXe,tXe,Yn,nH,aXe,nXe,iB,sXe,lXe,dB,iXe,dXe,cXe,Kn,sH,fXe,mXe,cB,gXe,hXe,fB,pXe,_Xe,uXe,Zn,lH,bXe,vXe,mB,TXe,FXe,gB,CXe,MXe,EXe,yg,iH,yXe,wXe,hB,AXe,LXe,BXe,es,dH,kXe,xXe,pB,RXe,SXe,_B,PXe,$Xe,IXe,wg,cH,jXe,NXe,uB,DXe,qXe,GXe,os,fH,OXe,XXe,bB,zXe,VXe,vB,WXe,QXe,HXe,rs,mH,UXe,JXe,TB,YXe,KXe,FB,ZXe,eze,oze,ts,gH,rze,tze,CB,aze,nze,MB,sze,lze,ize,as,hH,dze,cze,EB,fze,mze,yB,gze,hze,pze,Ag,pH,_ze,uze,wB,bze,vze,Tze,ns,_H,Fze,Cze,AB,Mze,Eze,LB,yze,wze,Aze,ss,uH,Lze,Bze,BB,kze,xze,kB,Rze,Sze,Pze,ls,bH,$ze,Ize,xB,jze,Nze,RB,Dze,qze,Gze,is,vH,Oze,Xze,SB,zze,Vze,PB,Wze,Qze,Hze,ds,TH,Uze,Jze,$B,Yze,Kze,IB,Zze,eVe,oVe,cs,FH,rVe,tVe,jB,aVe,nVe,NB,sVe,lVe,iVe,Lg,CH,dVe,cVe,DB,fVe,mVe,gVe,fs,MH,hVe,pVe,qB,_Ve,uVe,GB,bVe,vVe,TVe,Bg,EH,FVe,CVe,OB,MVe,EVe,yVe,kg,yH,wVe,AVe,XB,LVe,BVe,kVe,ms,wH,xVe,RVe,zB,SVe,PVe,VB,$Ve,IVe,jVe,gs,AH,NVe,DVe,WB,qVe,GVe,QB,OVe,XVe,zVe,xg,LH,VVe,WVe,HB,QVe,HVe,UVe,hs,BH,JVe,YVe,UB,KVe,ZVe,JB,eWe,oWe,rWe,ps,kH,tWe,aWe,YB,nWe,sWe,KB,lWe,iWe,dWe,_s,xH,cWe,fWe,ZB,mWe,gWe,ek,hWe,pWe,_We,us,RH,uWe,bWe,ok,vWe,TWe,rk,FWe,CWe,MWe,bs,SH,EWe,yWe,tk,wWe,AWe,ak,LWe,BWe,kWe,Rg,PH,xWe,RWe,nk,SWe,PWe,$We,Sg,$H,IWe,jWe,sk,NWe,DWe,qWe,Pg,IH,GWe,OWe,lk,XWe,zWe,VWe,$g,jH,WWe,QWe,ik,HWe,UWe,JWe,vs,NH,YWe,KWe,dk,ZWe,eQe,ck,oQe,rQe,tQe,Ig,DH,aQe,nQe,fk,sQe,lQe,iQe,Ts,qH,dQe,cQe,mk,fQe,mQe,gk,gQe,hQe,pQe,Fs,GH,_Qe,uQe,hk,bQe,vQe,pk,TQe,FQe,CQe,Cs,OH,MQe,EQe,_k,yQe,wQe,uk,AQe,LQe,BQe,Ms,XH,kQe,xQe,bk,RQe,SQe,vk,PQe,$Qe,IQe,Es,zH,jQe,NQe,Tk,DQe,qQe,Fk,GQe,OQe,XQe,jg,VH,zQe,VQe,Ck,WQe,QQe,HQe,Ng,WH,UQe,JQe,Mk,YQe,KQe,ZQe,ys,QH,eHe,oHe,Ek,rHe,tHe,yk,aHe,nHe,sHe,ws,HH,lHe,iHe,wk,dHe,cHe,Ak,fHe,mHe,gHe,As,UH,hHe,pHe,Lk,_He,uHe,Bk,bHe,vHe,THe,Dg,JH,FHe,CHe,kk,MHe,EHe,yHe,qg,YH,wHe,AHe,xk,LHe,BHe,kHe,Gg,KH,xHe,RHe,Rk,SHe,PHe,$He,Og,ZH,IHe,jHe,Sk,NHe,DHe,qHe,Ls,eU,GHe,OHe,Pk,XHe,zHe,$k,VHe,WHe,QHe,Xg,oU,HHe,UHe,Ik,JHe,YHe,KHe,zg,rU,ZHe,eUe,jk,oUe,rUe,tUe,Bs,tU,aUe,nUe,Nk,sUe,lUe,Dk,iUe,dUe,cUe,ks,aU,fUe,mUe,qk,gUe,hUe,Gk,pUe,_Ue,uUe,nU,bUe,vUe,vE,TUe,Vg,TE,FUe,sU,CUe,m8e,ji,Wg,lU,FE,MUe,iU,EUe,g8e,Xo,CE,yUe,ME,wUe,Ok,AUe,LUe,BUe,EE,kUe,dU,xUe,RUe,SUe,Le,yE,PUe,cU,$Ue,IUe,Na,jUe,fU,NUe,DUe,mU,qUe,GUe,gU,OUe,XUe,zUe,se,Qg,hU,VUe,WUe,Xk,QUe,HUe,UUe,Hg,pU,JUe,YUe,zk,KUe,ZUe,eJe,Ug,_U,oJe,rJe,Vk,tJe,aJe,nJe,Jg,uU,sJe,lJe,Wk,iJe,dJe,cJe,Yg,bU,fJe,mJe,Qk,gJe,hJe,pJe,Kg,vU,_Je,uJe,Hk,bJe,vJe,TJe,Zg,TU,FJe,CJe,Uk,MJe,EJe,yJe,eh,FU,wJe,AJe,Jk,LJe,BJe,kJe,oh,CU,xJe,RJe,Yk,SJe,PJe,$Je,rh,MU,IJe,jJe,Kk,NJe,DJe,qJe,th,EU,GJe,OJe,Zk,XJe,zJe,VJe,ah,yU,WJe,QJe,ex,HJe,UJe,JJe,nh,wU,YJe,KJe,ox,ZJe,eYe,oYe,sh,AU,rYe,tYe,rx,aYe,nYe,sYe,lh,LU,lYe,iYe,tx,dYe,cYe,fYe,ih,mYe,BU,gYe,hYe,wE,pYe,dh,AE,_Ye,kU,uYe,h8e,Ni,ch,xU,LE,bYe,RU,vYe,p8e,zo,BE,TYe,kE,FYe,ax,CYe,MYe,EYe,xE,yYe,SU,wYe,AYe,LYe,Be,RE,BYe,PU,kYe,xYe,Di,RYe,$U,SYe,PYe,IU,$Ye,IYe,jYe,we,fh,jU,NYe,DYe,nx,qYe,GYe,OYe,mh,NU,XYe,zYe,sx,VYe,WYe,QYe,gh,DU,HYe,UYe,lx,JYe,YYe,KYe,hh,qU,ZYe,eKe,ix,oKe,rKe,tKe,ph,GU,aKe,nKe,dx,sKe,lKe,iKe,_h,OU,dKe,cKe,cx,fKe,mKe,gKe,uh,XU,hKe,pKe,fx,_Ke,uKe,bKe,bh,zU,vKe,TKe,mx,FKe,CKe,MKe,vh,EKe,VU,yKe,wKe,SE,AKe,Th,PE,LKe,WU,BKe,_8e,qi,Fh,QU,$E,kKe,HU,xKe,u8e,Vo,IE,RKe,Gi,SKe,UU,PKe,$Ke,JU,IKe,jKe,NKe,jE,DKe,YU,qKe,GKe,OKe,Nr,NE,XKe,KU,zKe,VKe,Oi,WKe,ZU,QKe,HKe,eJ,UKe,JKe,YKe,oJ,KKe,ZKe,DE,eZe,ke,qE,oZe,rJ,rZe,tZe,Da,aZe,tJ,nZe,sZe,aJ,lZe,iZe,nJ,dZe,cZe,fZe,F,Ch,sJ,mZe,gZe,gx,hZe,pZe,_Ze,Mh,lJ,uZe,bZe,hx,vZe,TZe,FZe,Eh,iJ,CZe,MZe,px,EZe,yZe,wZe,yh,dJ,AZe,LZe,_x,BZe,kZe,xZe,wh,cJ,RZe,SZe,ux,PZe,$Ze,IZe,Ah,fJ,jZe,NZe,bx,DZe,qZe,GZe,Lh,mJ,OZe,XZe,vx,zZe,VZe,WZe,Bh,gJ,QZe,HZe,Tx,UZe,JZe,YZe,kh,hJ,KZe,ZZe,Fx,eeo,oeo,reo,xh,pJ,teo,aeo,Cx,neo,seo,leo,Rh,_J,ieo,deo,Mx,ceo,feo,meo,Sh,uJ,geo,heo,Ex,peo,_eo,ueo,Ph,bJ,beo,veo,yx,Teo,Feo,Ceo,$h,vJ,Meo,Eeo,wx,yeo,weo,Aeo,Ih,TJ,Leo,Beo,Ax,keo,xeo,Reo,jh,FJ,Seo,Peo,Lx,$eo,Ieo,jeo,Nh,CJ,Neo,Deo,Bx,qeo,Geo,Oeo,Dh,MJ,Xeo,zeo,kx,Veo,Weo,Qeo,qh,EJ,Heo,Ueo,xx,Jeo,Yeo,Keo,Gh,yJ,Zeo,eoo,Rx,ooo,roo,too,Oh,wJ,aoo,noo,Sx,soo,loo,ioo,Xh,AJ,doo,coo,Px,foo,moo,goo,zh,LJ,hoo,poo,$x,_oo,uoo,boo,Vh,BJ,voo,Too,Ix,Foo,Coo,Moo,Wh,kJ,Eoo,yoo,jx,woo,Aoo,Loo,xs,xJ,Boo,koo,Nx,xoo,Roo,Dx,Soo,Poo,$oo,Qh,RJ,Ioo,joo,qx,Noo,Doo,qoo,Hh,SJ,Goo,Ooo,Gx,Xoo,zoo,Voo,Uh,PJ,Woo,Qoo,Ox,Hoo,Uoo,Joo,Jh,$J,Yoo,Koo,Xx,Zoo,ero,oro,Yh,IJ,rro,tro,zx,aro,nro,sro,Kh,jJ,lro,iro,Vx,dro,cro,fro,Zh,NJ,mro,gro,Wx,hro,pro,_ro,ep,DJ,uro,bro,Qx,vro,Tro,Fro,op,qJ,Cro,Mro,Hx,Ero,yro,wro,rp,GJ,Aro,Lro,Ux,Bro,kro,xro,tp,OJ,Rro,Sro,Jx,Pro,$ro,Iro,ap,XJ,jro,Nro,Yx,Dro,qro,Gro,np,zJ,Oro,Xro,Kx,zro,Vro,Wro,sp,VJ,Qro,Hro,Zx,Uro,Jro,Yro,lp,WJ,Kro,Zro,eR,eto,oto,rto,ip,QJ,tto,ato,oR,nto,sto,lto,dp,HJ,ito,dto,rR,cto,fto,mto,cp,UJ,gto,hto,tR,pto,_to,uto,fp,JJ,bto,vto,aR,Tto,Fto,Cto,mp,YJ,Mto,Eto,nR,yto,wto,Ato,gp,KJ,Lto,Bto,sR,kto,xto,Rto,hp,ZJ,Sto,Pto,lR,$to,Ito,jto,pp,eY,Nto,Dto,iR,qto,Gto,Oto,_p,oY,Xto,zto,dR,Vto,Wto,Qto,up,rY,Hto,Uto,cR,Jto,Yto,Kto,bp,tY,Zto,eao,fR,oao,rao,tao,vp,aY,aao,nao,mR,sao,lao,iao,Tp,nY,dao,cao,gR,fao,mao,gao,Fp,sY,hao,pao,hR,_ao,uao,bao,Cp,lY,vao,Tao,pR,Fao,Cao,Mao,Mp,iY,Eao,yao,_R,wao,Aao,Lao,Ep,dY,Bao,kao,uR,xao,Rao,Sao,yp,cY,Pao,$ao,bR,Iao,jao,Nao,wp,fY,Dao,qao,vR,Gao,Oao,Xao,Ap,mY,zao,Vao,TR,Wao,Qao,Hao,Lp,gY,Uao,Jao,FR,Yao,Kao,Zao,Bp,hY,eno,ono,CR,rno,tno,ano,kp,pY,nno,sno,MR,lno,ino,dno,xp,_Y,cno,fno,ER,mno,gno,hno,Rp,uY,pno,_no,yR,uno,bno,vno,Sp,bY,Tno,Fno,wR,Cno,Mno,Eno,Pp,vY,yno,wno,AR,Ano,Lno,Bno,$p,TY,kno,xno,LR,Rno,Sno,Pno,Ip,FY,$no,Ino,BR,jno,Nno,Dno,jp,CY,qno,Gno,kR,Ono,Xno,zno,Np,MY,Vno,Wno,xR,Qno,Hno,Uno,Dp,EY,Jno,Yno,RR,Kno,Zno,eso,qp,yY,oso,rso,SR,tso,aso,nso,Gp,wY,sso,lso,PR,iso,dso,cso,Op,AY,fso,mso,$R,gso,hso,pso,Xp,LY,_so,uso,IR,bso,vso,Tso,zp,BY,Fso,Cso,jR,Mso,Eso,yso,Vp,kY,wso,Aso,NR,Lso,Bso,kso,Wp,xY,xso,Rso,DR,Sso,Pso,$so,Qp,RY,Iso,jso,qR,Nso,Dso,qso,Hp,SY,Gso,Oso,GR,Xso,zso,Vso,Up,PY,Wso,Qso,OR,Hso,Uso,Jso,Jp,$Y,Yso,Kso,XR,Zso,elo,olo,Yp,rlo,IY,tlo,alo,jY,nlo,slo,NY,llo,ilo,GE,b8e,Xi,Kp,DY,OE,dlo,qY,clo,v8e,Wo,XE,flo,zi,mlo,GY,glo,hlo,OY,plo,_lo,ulo,zE,blo,XY,vlo,Tlo,Flo,Dr,VE,Clo,zY,Mlo,Elo,Vi,ylo,VY,wlo,Alo,WY,Llo,Blo,klo,QY,xlo,Rlo,WE,Slo,xe,QE,Plo,HY,$lo,Ilo,qa,jlo,UY,Nlo,Dlo,JY,qlo,Glo,YY,Olo,Xlo,zlo,x,Zp,KY,Vlo,Wlo,zR,Qlo,Hlo,Ulo,e_,ZY,Jlo,Ylo,VR,Klo,Zlo,eio,o_,eK,oio,rio,WR,tio,aio,nio,r_,oK,sio,lio,QR,iio,dio,cio,t_,rK,fio,mio,HR,gio,hio,pio,a_,tK,_io,uio,UR,bio,vio,Tio,n_,aK,Fio,Cio,JR,Mio,Eio,yio,s_,nK,wio,Aio,YR,Lio,Bio,kio,l_,sK,xio,Rio,KR,Sio,Pio,$io,i_,lK,Iio,jio,ZR,Nio,Dio,qio,d_,iK,Gio,Oio,eS,Xio,zio,Vio,c_,dK,Wio,Qio,oS,Hio,Uio,Jio,f_,cK,Yio,Kio,rS,Zio,edo,odo,m_,fK,rdo,tdo,tS,ado,ndo,sdo,g_,mK,ldo,ido,aS,ddo,cdo,fdo,h_,gK,mdo,gdo,nS,hdo,pdo,_do,p_,hK,udo,bdo,sS,vdo,Tdo,Fdo,__,pK,Cdo,Mdo,lS,Edo,ydo,wdo,u_,_K,Ado,Ldo,iS,Bdo,kdo,xdo,b_,uK,Rdo,Sdo,dS,Pdo,$do,Ido,v_,bK,jdo,Ndo,cS,Ddo,qdo,Gdo,T_,vK,Odo,Xdo,fS,zdo,Vdo,Wdo,F_,TK,Qdo,Hdo,mS,Udo,Jdo,Ydo,C_,FK,Kdo,Zdo,gS,eco,oco,rco,M_,CK,tco,aco,hS,nco,sco,lco,E_,MK,ico,dco,pS,cco,fco,mco,y_,EK,gco,hco,_S,pco,_co,uco,w_,yK,bco,vco,uS,Tco,Fco,Cco,A_,wK,Mco,Eco,bS,yco,wco,Aco,L_,AK,Lco,Bco,vS,kco,xco,Rco,B_,LK,Sco,Pco,TS,$co,Ico,jco,k_,BK,Nco,Dco,FS,qco,Gco,Oco,x_,kK,Xco,zco,CS,Vco,Wco,Qco,R_,xK,Hco,Uco,MS,Jco,Yco,Kco,S_,RK,Zco,efo,ES,ofo,rfo,tfo,P_,SK,afo,nfo,yS,sfo,lfo,ifo,$_,PK,dfo,cfo,wS,ffo,mfo,gfo,I_,$K,hfo,pfo,AS,_fo,ufo,bfo,j_,vfo,IK,Tfo,Ffo,jK,Cfo,Mfo,NK,Efo,yfo,HE,T8e,Wi,N_,DK,UE,wfo,qK,Afo,F8e,Qo,JE,Lfo,Qi,Bfo,GK,kfo,xfo,OK,Rfo,Sfo,Pfo,YE,$fo,XK,Ifo,jfo,Nfo,qr,KE,Dfo,zK,qfo,Gfo,Hi,Ofo,VK,Xfo,zfo,WK,Vfo,Wfo,Qfo,QK,Hfo,Ufo,ZE,Jfo,Re,e3,Yfo,HK,Kfo,Zfo,Ga,emo,UK,omo,rmo,JK,tmo,amo,YK,nmo,smo,lmo,$,D_,KK,imo,dmo,LS,cmo,fmo,mmo,q_,ZK,gmo,hmo,BS,pmo,_mo,umo,G_,eZ,bmo,vmo,kS,Tmo,Fmo,Cmo,O_,oZ,Mmo,Emo,xS,ymo,wmo,Amo,X_,rZ,Lmo,Bmo,RS,kmo,xmo,Rmo,z_,tZ,Smo,Pmo,SS,$mo,Imo,jmo,V_,aZ,Nmo,Dmo,PS,qmo,Gmo,Omo,W_,nZ,Xmo,zmo,$S,Vmo,Wmo,Qmo,Q_,sZ,Hmo,Umo,IS,Jmo,Ymo,Kmo,H_,lZ,Zmo,ego,jS,ogo,rgo,tgo,U_,iZ,ago,ngo,NS,sgo,lgo,igo,J_,dZ,dgo,cgo,DS,fgo,mgo,ggo,Y_,cZ,hgo,pgo,qS,_go,ugo,bgo,K_,fZ,vgo,Tgo,GS,Fgo,Cgo,Mgo,Z_,mZ,Ego,ygo,OS,wgo,Ago,Lgo,eu,gZ,Bgo,kgo,XS,xgo,Rgo,Sgo,ou,hZ,Pgo,$go,zS,Igo,jgo,Ngo,ru,pZ,Dgo,qgo,VS,Ggo,Ogo,Xgo,tu,_Z,zgo,Vgo,WS,Wgo,Qgo,Hgo,au,uZ,Ugo,Jgo,QS,Ygo,Kgo,Zgo,nu,bZ,eho,oho,HS,rho,tho,aho,su,vZ,nho,sho,US,lho,iho,dho,lu,TZ,cho,fho,JS,mho,gho,hho,iu,FZ,pho,_ho,YS,uho,bho,vho,du,CZ,Tho,Fho,KS,Cho,Mho,Eho,cu,MZ,yho,who,ZS,Aho,Lho,Bho,fu,EZ,kho,xho,eP,Rho,Sho,Pho,mu,yZ,$ho,Iho,oP,jho,Nho,Dho,gu,wZ,qho,Gho,rP,Oho,Xho,zho,hu,AZ,Vho,Who,tP,Qho,Hho,Uho,pu,LZ,Jho,Yho,aP,Kho,Zho,epo,_u,BZ,opo,rpo,nP,tpo,apo,npo,uu,kZ,spo,lpo,sP,ipo,dpo,cpo,bu,xZ,fpo,mpo,lP,gpo,hpo,ppo,vu,_po,RZ,upo,bpo,SZ,vpo,Tpo,PZ,Fpo,Cpo,o3,C8e,Ui,Tu,$Z,r3,Mpo,IZ,Epo,M8e,Ho,t3,ypo,Ji,wpo,jZ,Apo,Lpo,NZ,Bpo,kpo,xpo,a3,Rpo,DZ,Spo,Ppo,$po,Gr,n3,Ipo,qZ,jpo,Npo,Yi,Dpo,GZ,qpo,Gpo,OZ,Opo,Xpo,zpo,XZ,Vpo,Wpo,s3,Qpo,Se,l3,Hpo,zZ,Upo,Jpo,Oa,Ypo,VZ,Kpo,Zpo,WZ,e_o,o_o,QZ,r_o,t_o,a_o,I,Fu,HZ,n_o,s_o,iP,l_o,i_o,d_o,Cu,UZ,c_o,f_o,dP,m_o,g_o,h_o,Mu,JZ,p_o,__o,cP,u_o,b_o,v_o,Eu,YZ,T_o,F_o,fP,C_o,M_o,E_o,yu,KZ,y_o,w_o,mP,A_o,L_o,B_o,wu,ZZ,k_o,x_o,gP,R_o,S_o,P_o,Au,eee,$_o,I_o,hP,j_o,N_o,D_o,Lu,oee,q_o,G_o,pP,O_o,X_o,z_o,Bu,ree,V_o,W_o,_P,Q_o,H_o,U_o,ku,tee,J_o,Y_o,uP,K_o,Z_o,euo,xu,aee,ouo,ruo,bP,tuo,auo,nuo,Ru,nee,suo,luo,vP,iuo,duo,cuo,Su,see,fuo,muo,TP,guo,huo,puo,Pu,lee,_uo,uuo,FP,buo,vuo,Tuo,$u,iee,Fuo,Cuo,CP,Muo,Euo,yuo,Iu,dee,wuo,Auo,MP,Luo,Buo,kuo,ju,cee,xuo,Ruo,EP,Suo,Puo,$uo,Nu,fee,Iuo,juo,yP,Nuo,Duo,quo,Du,mee,Guo,Ouo,wP,Xuo,zuo,Vuo,qu,gee,Wuo,Quo,AP,Huo,Uuo,Juo,Gu,hee,Yuo,Kuo,LP,Zuo,e1o,o1o,Ou,pee,r1o,t1o,BP,a1o,n1o,s1o,Xu,_ee,l1o,i1o,kP,d1o,c1o,f1o,zu,uee,m1o,g1o,xP,h1o,p1o,_1o,Vu,bee,u1o,b1o,RP,v1o,T1o,F1o,Wu,vee,C1o,M1o,SP,E1o,y1o,w1o,Qu,Tee,A1o,L1o,PP,B1o,k1o,x1o,Hu,Fee,R1o,S1o,$P,P1o,$1o,I1o,Uu,Cee,j1o,N1o,IP,D1o,q1o,G1o,Ju,Mee,O1o,X1o,Eee,z1o,V1o,W1o,Yu,yee,Q1o,H1o,jP,U1o,J1o,Y1o,Ku,wee,K1o,Z1o,NP,e7o,o7o,r7o,Zu,Aee,t7o,a7o,DP,n7o,s7o,l7o,e1,Lee,i7o,d7o,qP,c7o,f7o,m7o,o1,g7o,Bee,h7o,p7o,kee,_7o,u7o,xee,b7o,v7o,i3,E8e,Ki,r1,Ree,d3,T7o,See,F7o,y8e,Uo,c3,C7o,Zi,M7o,Pee,E7o,y7o,$ee,w7o,A7o,L7o,f3,B7o,Iee,k7o,x7o,R7o,Or,m3,S7o,jee,P7o,$7o,ed,I7o,Nee,j7o,N7o,Dee,D7o,q7o,G7o,qee,O7o,X7o,g3,z7o,Pe,h3,V7o,Gee,W7o,Q7o,Xa,H7o,Oee,U7o,J7o,Xee,Y7o,K7o,zee,Z7o,e4o,o4o,ae,t1,Vee,r4o,t4o,GP,a4o,n4o,s4o,a1,Wee,l4o,i4o,OP,d4o,c4o,f4o,n1,Qee,m4o,g4o,XP,h4o,p4o,_4o,s1,Hee,u4o,b4o,zP,v4o,T4o,F4o,l1,Uee,C4o,M4o,VP,E4o,y4o,w4o,i1,Jee,A4o,L4o,WP,B4o,k4o,x4o,d1,Yee,R4o,S4o,QP,P4o,$4o,I4o,c1,Kee,j4o,N4o,HP,D4o,q4o,G4o,f1,Zee,O4o,X4o,UP,z4o,V4o,W4o,m1,eoe,Q4o,H4o,JP,U4o,J4o,Y4o,g1,ooe,K4o,Z4o,YP,ebo,obo,rbo,h1,roe,tbo,abo,KP,nbo,sbo,lbo,p1,toe,ibo,dbo,ZP,cbo,fbo,mbo,_1,aoe,gbo,hbo,e$,pbo,_bo,ubo,u1,noe,bbo,vbo,o$,Tbo,Fbo,Cbo,b1,soe,Mbo,Ebo,r$,ybo,wbo,Abo,v1,Lbo,loe,Bbo,kbo,ioe,xbo,Rbo,doe,Sbo,Pbo,p3,w8e,od,T1,coe,_3,$bo,foe,Ibo,A8e,Jo,u3,jbo,rd,Nbo,moe,Dbo,qbo,goe,Gbo,Obo,Xbo,b3,zbo,hoe,Vbo,Wbo,Qbo,Xr,v3,Hbo,poe,Ubo,Jbo,td,Ybo,_oe,Kbo,Zbo,uoe,e5o,o5o,r5o,boe,t5o,a5o,T3,n5o,$e,F3,s5o,voe,l5o,i5o,za,d5o,Toe,c5o,f5o,Foe,m5o,g5o,Coe,h5o,p5o,_5o,A,F1,Moe,u5o,b5o,t$,v5o,T5o,F5o,C1,Eoe,C5o,M5o,a$,E5o,y5o,w5o,M1,yoe,A5o,L5o,n$,B5o,k5o,x5o,E1,woe,R5o,S5o,s$,P5o,$5o,I5o,y1,Aoe,j5o,N5o,l$,D5o,q5o,G5o,w1,Loe,O5o,X5o,i$,z5o,V5o,W5o,A1,Boe,Q5o,H5o,d$,U5o,J5o,Y5o,L1,koe,K5o,Z5o,c$,e2o,o2o,r2o,B1,xoe,t2o,a2o,f$,n2o,s2o,l2o,k1,Roe,i2o,d2o,m$,c2o,f2o,m2o,x1,Soe,g2o,h2o,g$,p2o,_2o,u2o,R1,Poe,b2o,v2o,h$,T2o,F2o,C2o,S1,$oe,M2o,E2o,p$,y2o,w2o,A2o,P1,Ioe,L2o,B2o,_$,k2o,x2o,R2o,$1,joe,S2o,P2o,u$,$2o,I2o,j2o,I1,Noe,N2o,D2o,b$,q2o,G2o,O2o,j1,Doe,X2o,z2o,v$,V2o,W2o,Q2o,N1,qoe,H2o,U2o,T$,J2o,Y2o,K2o,D1,Goe,Z2o,evo,F$,ovo,rvo,tvo,q1,Ooe,avo,nvo,C$,svo,lvo,ivo,G1,Xoe,dvo,cvo,M$,fvo,mvo,gvo,O1,zoe,hvo,pvo,E$,_vo,uvo,bvo,X1,Voe,vvo,Tvo,y$,Fvo,Cvo,Mvo,z1,Woe,Evo,yvo,w$,wvo,Avo,Lvo,V1,Qoe,Bvo,kvo,A$,xvo,Rvo,Svo,W1,Hoe,Pvo,$vo,L$,Ivo,jvo,Nvo,Q1,Uoe,Dvo,qvo,B$,Gvo,Ovo,Xvo,H1,Joe,zvo,Vvo,k$,Wvo,Qvo,Hvo,U1,Yoe,Uvo,Jvo,x$,Yvo,Kvo,Zvo,J1,Koe,eTo,oTo,R$,rTo,tTo,aTo,Y1,Zoe,nTo,sTo,S$,lTo,iTo,dTo,K1,ere,cTo,fTo,P$,mTo,gTo,hTo,Z1,ore,pTo,_To,$$,uTo,bTo,vTo,e7,rre,TTo,FTo,I$,CTo,MTo,ETo,o7,tre,yTo,wTo,j$,ATo,LTo,BTo,r7,are,kTo,xTo,N$,RTo,STo,PTo,t7,nre,$To,ITo,D$,jTo,NTo,DTo,a7,sre,qTo,GTo,q$,OTo,XTo,zTo,n7,lre,VTo,WTo,G$,QTo,HTo,UTo,s7,ire,JTo,YTo,O$,KTo,ZTo,eFo,l7,dre,oFo,rFo,X$,tFo,aFo,nFo,i7,cre,sFo,lFo,z$,iFo,dFo,cFo,d7,fre,fFo,mFo,V$,gFo,hFo,pFo,c7,mre,_Fo,uFo,W$,bFo,vFo,TFo,f7,gre,FFo,CFo,Q$,MFo,EFo,yFo,m7,wFo,hre,AFo,LFo,pre,BFo,kFo,_re,xFo,RFo,C3,L8e,ad,g7,ure,M3,SFo,bre,PFo,B8e,Yo,E3,$Fo,nd,IFo,vre,jFo,NFo,Tre,DFo,qFo,GFo,y3,OFo,Fre,XFo,zFo,VFo,zr,w3,WFo,Cre,QFo,HFo,sd,UFo,Mre,JFo,YFo,Ere,KFo,ZFo,eCo,yre,oCo,rCo,A3,tCo,Ie,L3,aCo,wre,nCo,sCo,Va,lCo,Are,iCo,dCo,Lre,cCo,fCo,Bre,mCo,gCo,hCo,G,h7,kre,pCo,_Co,H$,uCo,bCo,vCo,p7,xre,TCo,FCo,U$,CCo,MCo,ECo,_7,Rre,yCo,wCo,J$,ACo,LCo,BCo,u7,Sre,kCo,xCo,Y$,RCo,SCo,PCo,b7,Pre,$Co,ICo,K$,jCo,NCo,DCo,v7,$re,qCo,GCo,Z$,OCo,XCo,zCo,T7,Ire,VCo,WCo,eI,QCo,HCo,UCo,F7,jre,JCo,YCo,oI,KCo,ZCo,eMo,C7,Nre,oMo,rMo,rI,tMo,aMo,nMo,M7,Dre,sMo,lMo,tI,iMo,dMo,cMo,E7,qre,fMo,mMo,aI,gMo,hMo,pMo,y7,Gre,_Mo,uMo,nI,bMo,vMo,TMo,w7,Ore,FMo,CMo,sI,MMo,EMo,yMo,A7,Xre,wMo,AMo,lI,LMo,BMo,kMo,L7,zre,xMo,RMo,iI,SMo,PMo,$Mo,B7,Vre,IMo,jMo,dI,NMo,DMo,qMo,k7,Wre,GMo,OMo,cI,XMo,zMo,VMo,x7,Qre,WMo,QMo,fI,HMo,UMo,JMo,R7,Hre,YMo,KMo,mI,ZMo,eEo,oEo,S7,Ure,rEo,tEo,gI,aEo,nEo,sEo,P7,Jre,lEo,iEo,hI,dEo,cEo,fEo,$7,Yre,mEo,gEo,pI,hEo,pEo,_Eo,I7,Kre,uEo,bEo,_I,vEo,TEo,FEo,j7,Zre,CEo,MEo,uI,EEo,yEo,wEo,N7,ete,AEo,LEo,bI,BEo,kEo,xEo,D7,ote,REo,SEo,vI,PEo,$Eo,IEo,q7,rte,jEo,NEo,TI,DEo,qEo,GEo,G7,OEo,tte,XEo,zEo,ate,VEo,WEo,nte,QEo,HEo,B3,k8e,ld,O7,ste,k3,UEo,lte,JEo,x8e,Ko,x3,YEo,id,KEo,ite,ZEo,e3o,dte,o3o,r3o,t3o,R3,a3o,cte,n3o,s3o,l3o,Vr,S3,i3o,fte,d3o,c3o,dd,f3o,mte,m3o,g3o,gte,h3o,p3o,_3o,hte,u3o,b3o,P3,v3o,je,$3,T3o,pte,F3o,C3o,Wa,M3o,_te,E3o,y3o,ute,w3o,A3o,bte,L3o,B3o,k3o,na,X7,vte,x3o,R3o,FI,S3o,P3o,$3o,z7,Tte,I3o,j3o,CI,N3o,D3o,q3o,V7,Fte,G3o,O3o,MI,X3o,z3o,V3o,W7,Cte,W3o,Q3o,EI,H3o,U3o,J3o,Q7,Mte,Y3o,K3o,yI,Z3o,eyo,oyo,H7,ryo,Ete,tyo,ayo,yte,nyo,syo,wte,lyo,iyo,I3,R8e,cd,U7,Ate,j3,dyo,Lte,cyo,S8e,Zo,N3,fyo,fd,myo,Bte,gyo,hyo,kte,pyo,_yo,uyo,D3,byo,xte,vyo,Tyo,Fyo,Wr,q3,Cyo,Rte,Myo,Eyo,md,yyo,Ste,wyo,Ayo,Pte,Lyo,Byo,kyo,$te,xyo,Ryo,G3,Syo,Ne,O3,Pyo,Ite,$yo,Iyo,Qa,jyo,jte,Nyo,Dyo,Nte,qyo,Gyo,Dte,Oyo,Xyo,zyo,D,J7,qte,Vyo,Wyo,wI,Qyo,Hyo,Uyo,Y7,Gte,Jyo,Yyo,AI,Kyo,Zyo,ewo,K7,Ote,owo,rwo,LI,two,awo,nwo,Z7,Xte,swo,lwo,BI,iwo,dwo,cwo,e4,zte,fwo,mwo,kI,gwo,hwo,pwo,o4,Vte,_wo,uwo,xI,bwo,vwo,Two,r4,Wte,Fwo,Cwo,RI,Mwo,Ewo,ywo,t4,Qte,wwo,Awo,SI,Lwo,Bwo,kwo,a4,Hte,xwo,Rwo,PI,Swo,Pwo,$wo,n4,Ute,Iwo,jwo,$I,Nwo,Dwo,qwo,s4,Jte,Gwo,Owo,II,Xwo,zwo,Vwo,l4,Yte,Wwo,Qwo,jI,Hwo,Uwo,Jwo,i4,Kte,Ywo,Kwo,NI,Zwo,eAo,oAo,d4,Zte,rAo,tAo,DI,aAo,nAo,sAo,c4,eae,lAo,iAo,qI,dAo,cAo,fAo,f4,oae,mAo,gAo,GI,hAo,pAo,_Ao,m4,rae,uAo,bAo,OI,vAo,TAo,FAo,g4,tae,CAo,MAo,XI,EAo,yAo,wAo,h4,aae,AAo,LAo,zI,BAo,kAo,xAo,p4,nae,RAo,SAo,VI,PAo,$Ao,IAo,_4,sae,jAo,NAo,WI,DAo,qAo,GAo,u4,lae,OAo,XAo,QI,zAo,VAo,WAo,b4,iae,QAo,HAo,HI,UAo,JAo,YAo,v4,dae,KAo,ZAo,UI,e6o,o6o,r6o,T4,cae,t6o,a6o,JI,n6o,s6o,l6o,F4,fae,i6o,d6o,YI,c6o,f6o,m6o,C4,mae,g6o,h6o,KI,p6o,_6o,u6o,M4,gae,b6o,v6o,ZI,T6o,F6o,C6o,E4,hae,M6o,E6o,ej,y6o,w6o,A6o,y4,pae,L6o,B6o,oj,k6o,x6o,R6o,w4,_ae,S6o,P6o,rj,$6o,I6o,j6o,A4,uae,N6o,D6o,tj,q6o,G6o,O6o,L4,X6o,bae,z6o,V6o,vae,W6o,Q6o,Tae,H6o,U6o,X3,P8e,gd,B4,Fae,z3,J6o,Cae,Y6o,$8e,er,V3,K6o,hd,Z6o,Mae,e0o,o0o,Eae,r0o,t0o,a0o,W3,n0o,yae,s0o,l0o,i0o,Qr,Q3,d0o,wae,c0o,f0o,pd,m0o,Aae,g0o,h0o,Lae,p0o,_0o,u0o,Bae,b0o,v0o,H3,T0o,De,U3,F0o,kae,C0o,M0o,Ha,E0o,xae,y0o,w0o,Rae,A0o,L0o,Sae,B0o,k0o,x0o,R,k4,Pae,R0o,S0o,aj,P0o,$0o,I0o,x4,$ae,j0o,N0o,nj,D0o,q0o,G0o,R4,Iae,O0o,X0o,sj,z0o,V0o,W0o,S4,jae,Q0o,H0o,lj,U0o,J0o,Y0o,P4,Nae,K0o,Z0o,ij,eLo,oLo,rLo,$4,Dae,tLo,aLo,dj,nLo,sLo,lLo,I4,qae,iLo,dLo,cj,cLo,fLo,mLo,j4,Gae,gLo,hLo,fj,pLo,_Lo,uLo,N4,Oae,bLo,vLo,mj,TLo,FLo,CLo,D4,Xae,MLo,ELo,gj,yLo,wLo,ALo,q4,zae,LLo,BLo,hj,kLo,xLo,RLo,G4,Vae,SLo,PLo,pj,$Lo,ILo,jLo,O4,Wae,NLo,DLo,_j,qLo,GLo,OLo,X4,Qae,XLo,zLo,uj,VLo,WLo,QLo,z4,Hae,HLo,ULo,bj,JLo,YLo,KLo,V4,Uae,ZLo,e8o,vj,o8o,r8o,t8o,W4,Jae,a8o,n8o,Tj,s8o,l8o,i8o,Q4,Yae,d8o,c8o,Fj,f8o,m8o,g8o,H4,Kae,h8o,p8o,Cj,_8o,u8o,b8o,U4,Zae,v8o,T8o,Mj,F8o,C8o,M8o,J4,ene,E8o,y8o,Ej,w8o,A8o,L8o,Y4,one,B8o,k8o,yj,x8o,R8o,S8o,K4,rne,P8o,$8o,wj,I8o,j8o,N8o,Z4,tne,D8o,q8o,Aj,G8o,O8o,X8o,eb,ane,z8o,V8o,Lj,W8o,Q8o,H8o,ob,nne,U8o,J8o,Bj,Y8o,K8o,Z8o,rb,sne,e9o,o9o,kj,r9o,t9o,a9o,tb,lne,n9o,s9o,xj,l9o,i9o,d9o,ab,ine,c9o,f9o,Rj,m9o,g9o,h9o,nb,dne,p9o,_9o,Sj,u9o,b9o,v9o,sb,cne,T9o,F9o,Pj,C9o,M9o,E9o,lb,fne,y9o,w9o,$j,A9o,L9o,B9o,ib,mne,k9o,x9o,Ij,R9o,S9o,P9o,db,gne,$9o,I9o,jj,j9o,N9o,D9o,cb,hne,q9o,G9o,Nj,O9o,X9o,z9o,fb,pne,V9o,W9o,Dj,Q9o,H9o,U9o,mb,_ne,J9o,Y9o,qj,K9o,Z9o,eBo,gb,une,oBo,rBo,Gj,tBo,aBo,nBo,hb,sBo,bne,lBo,iBo,vne,dBo,cBo,Tne,fBo,mBo,J3,I8e,_d,pb,Fne,Y3,gBo,Cne,hBo,j8e,or,K3,pBo,ud,_Bo,Mne,uBo,bBo,Ene,vBo,TBo,FBo,Z3,CBo,yne,MBo,EBo,yBo,Hr,ey,wBo,wne,ABo,LBo,bd,BBo,Ane,kBo,xBo,Lne,RBo,SBo,PBo,Bne,$Bo,IBo,oy,jBo,qe,ry,NBo,kne,DBo,qBo,Ua,GBo,xne,OBo,XBo,Rne,zBo,VBo,Sne,WBo,QBo,HBo,Pne,_b,$ne,UBo,JBo,Oj,YBo,KBo,ZBo,ub,eko,Ine,oko,rko,jne,tko,ako,Nne,nko,sko,ty,N8e,vd,bb,Dne,ay,lko,qne,iko,D8e,rr,ny,dko,Td,cko,Gne,fko,mko,One,gko,hko,pko,sy,_ko,Xne,uko,bko,vko,Ur,ly,Tko,zne,Fko,Cko,Fd,Mko,Vne,Eko,yko,Wne,wko,Ako,Lko,Qne,Bko,kko,iy,xko,Ge,dy,Rko,Hne,Sko,Pko,Ja,$ko,Une,Iko,jko,Jne,Nko,Dko,Yne,qko,Gko,Oko,be,vb,Kne,Xko,zko,Xj,Vko,Wko,Qko,Tb,Zne,Hko,Uko,zj,Jko,Yko,Kko,Rs,ese,Zko,exo,Vj,oxo,rxo,Wj,txo,axo,nxo,Fb,ose,sxo,lxo,Qj,ixo,dxo,cxo,la,rse,fxo,mxo,Hj,gxo,hxo,Uj,pxo,_xo,Jj,uxo,bxo,vxo,Cb,tse,Txo,Fxo,Yj,Cxo,Mxo,Exo,Mb,ase,yxo,wxo,Kj,Axo,Lxo,Bxo,Eb,nse,kxo,xxo,Zj,Rxo,Sxo,Pxo,yb,sse,$xo,Ixo,eN,jxo,Nxo,Dxo,wb,qxo,lse,Gxo,Oxo,ise,Xxo,zxo,dse,Vxo,Wxo,cy,q8e,Cd,Ab,cse,fy,Qxo,fse,Hxo,G8e,tr,my,Uxo,Md,Jxo,mse,Yxo,Kxo,gse,Zxo,eRo,oRo,gy,rRo,hse,tRo,aRo,nRo,Jr,hy,sRo,pse,lRo,iRo,Ed,dRo,_se,cRo,fRo,use,mRo,gRo,hRo,bse,pRo,_Ro,py,uRo,Oe,_y,bRo,vse,vRo,TRo,Ya,FRo,Tse,CRo,MRo,Fse,ERo,yRo,Cse,wRo,ARo,LRo,Mse,Lb,Ese,BRo,kRo,oN,xRo,RRo,SRo,Bb,PRo,yse,$Ro,IRo,wse,jRo,NRo,Ase,DRo,qRo,uy,O8e,yd,kb,Lse,by,GRo,Bse,ORo,X8e,ar,vy,XRo,wd,zRo,kse,VRo,WRo,xse,QRo,HRo,URo,Ty,JRo,Rse,YRo,KRo,ZRo,Yr,Fy,eSo,Sse,oSo,rSo,Ad,tSo,Pse,aSo,nSo,$se,sSo,lSo,iSo,Ise,dSo,cSo,Cy,fSo,Xe,My,mSo,jse,gSo,hSo,Ka,pSo,Nse,_So,uSo,Dse,bSo,vSo,qse,TSo,FSo,CSo,ao,xb,Gse,MSo,ESo,rN,ySo,wSo,ASo,Rb,Ose,LSo,BSo,tN,kSo,xSo,RSo,Sb,Xse,SSo,PSo,aN,$So,ISo,jSo,Pb,zse,NSo,DSo,nN,qSo,GSo,OSo,$b,Vse,XSo,zSo,sN,VSo,WSo,QSo,Ib,Wse,HSo,USo,lN,JSo,YSo,KSo,jb,Qse,ZSo,ePo,iN,oPo,rPo,tPo,Nb,aPo,Hse,nPo,sPo,Use,lPo,iPo,Jse,dPo,cPo,Ey,z8e,Ld,Db,Yse,yy,fPo,Kse,mPo,V8e,nr,wy,gPo,Bd,hPo,Zse,pPo,_Po,ele,uPo,bPo,vPo,Ay,TPo,ole,FPo,CPo,MPo,Kr,Ly,EPo,rle,yPo,wPo,kd,APo,tle,LPo,BPo,ale,kPo,xPo,RPo,nle,SPo,PPo,By,$Po,ze,ky,IPo,sle,jPo,NPo,Za,DPo,lle,qPo,GPo,ile,OPo,XPo,dle,zPo,VPo,WPo,xd,qb,cle,QPo,HPo,dN,UPo,JPo,YPo,Gb,fle,KPo,ZPo,cN,e$o,o$o,r$o,Ob,mle,t$o,a$o,fN,n$o,s$o,l$o,Xb,i$o,gle,d$o,c$o,hle,f$o,m$o,ple,g$o,h$o,xy,W8e,Rd,zb,_le,Ry,p$o,ule,_$o,Q8e,sr,Sy,u$o,Sd,b$o,ble,v$o,T$o,vle,F$o,C$o,M$o,Py,E$o,Tle,y$o,w$o,A$o,Zr,$y,L$o,Fle,B$o,k$o,Pd,x$o,Cle,R$o,S$o,Mle,P$o,$$o,I$o,Ele,j$o,N$o,Iy,D$o,Ve,jy,q$o,yle,G$o,O$o,en,X$o,wle,z$o,V$o,Ale,W$o,Q$o,Lle,H$o,U$o,J$o,no,Vb,Ble,Y$o,K$o,mN,Z$o,eIo,oIo,Wb,kle,rIo,tIo,gN,aIo,nIo,sIo,Qb,xle,lIo,iIo,hN,dIo,cIo,fIo,Hb,Rle,mIo,gIo,pN,hIo,pIo,_Io,Ub,Sle,uIo,bIo,_N,vIo,TIo,FIo,Jb,Ple,CIo,MIo,uN,EIo,yIo,wIo,Yb,$le,AIo,LIo,bN,BIo,kIo,xIo,Kb,RIo,Ile,SIo,PIo,jle,$Io,IIo,Nle,jIo,NIo,Ny,H8e,$d,Zb,Dle,Dy,DIo,qle,qIo,U8e,lr,qy,GIo,Id,OIo,Gle,XIo,zIo,Ole,VIo,WIo,QIo,Gy,HIo,Xle,UIo,JIo,YIo,et,Oy,KIo,zle,ZIo,ejo,jd,ojo,Vle,rjo,tjo,Wle,ajo,njo,sjo,Qle,ljo,ijo,Xy,djo,We,zy,cjo,Hle,fjo,mjo,on,gjo,Ule,hjo,pjo,Jle,_jo,ujo,Yle,bjo,vjo,Tjo,Vy,e5,Kle,Fjo,Cjo,vN,Mjo,Ejo,yjo,o5,Zle,wjo,Ajo,TN,Ljo,Bjo,kjo,r5,xjo,eie,Rjo,Sjo,oie,Pjo,$jo,rie,Ijo,jjo,Wy,J8e,Nd,t5,tie,Qy,Njo,aie,Djo,Y8e,ir,Hy,qjo,Dd,Gjo,nie,Ojo,Xjo,sie,zjo,Vjo,Wjo,Uy,Qjo,lie,Hjo,Ujo,Jjo,ot,Jy,Yjo,iie,Kjo,Zjo,qd,eNo,die,oNo,rNo,cie,tNo,aNo,nNo,fie,sNo,lNo,Yy,iNo,Qe,Ky,dNo,mie,cNo,fNo,rn,mNo,gie,gNo,hNo,hie,pNo,_No,pie,uNo,bNo,vNo,Gd,a5,_ie,TNo,FNo,FN,CNo,MNo,ENo,n5,uie,yNo,wNo,CN,ANo,LNo,BNo,s5,bie,kNo,xNo,MN,RNo,SNo,PNo,l5,$No,vie,INo,jNo,Tie,NNo,DNo,Fie,qNo,GNo,Zy,K8e,Od,i5,Cie,ew,ONo,Mie,XNo,Z8e,dr,ow,zNo,Xd,VNo,Eie,WNo,QNo,yie,HNo,UNo,JNo,rw,YNo,wie,KNo,ZNo,eDo,rt,tw,oDo,Aie,rDo,tDo,zd,aDo,Lie,nDo,sDo,Bie,lDo,iDo,dDo,kie,cDo,fDo,aw,mDo,He,nw,gDo,xie,hDo,pDo,tn,_Do,Rie,uDo,bDo,Sie,vDo,TDo,Pie,FDo,CDo,MDo,Vd,d5,$ie,EDo,yDo,EN,wDo,ADo,LDo,c5,Iie,BDo,kDo,yN,xDo,RDo,SDo,f5,jie,PDo,$Do,wN,IDo,jDo,NDo,m5,DDo,Nie,qDo,GDo,Die,ODo,XDo,qie,zDo,VDo,sw,e9e,Wd,g5,Gie,lw,WDo,Oie,QDo,o9e,cr,iw,HDo,Qd,UDo,Xie,JDo,YDo,zie,KDo,ZDo,eqo,dw,oqo,Vie,rqo,tqo,aqo,tt,cw,nqo,Wie,sqo,lqo,Hd,iqo,Qie,dqo,cqo,Hie,fqo,mqo,gqo,Uie,hqo,pqo,fw,_qo,Ue,mw,uqo,Jie,bqo,vqo,an,Tqo,Yie,Fqo,Cqo,Kie,Mqo,Eqo,Zie,yqo,wqo,Aqo,ede,h5,ode,Lqo,Bqo,AN,kqo,xqo,Rqo,p5,Sqo,rde,Pqo,$qo,tde,Iqo,jqo,ade,Nqo,Dqo,gw,r9e,Ud,_5,nde,hw,qqo,sde,Gqo,t9e,fr,pw,Oqo,Jd,Xqo,lde,zqo,Vqo,ide,Wqo,Qqo,Hqo,_w,Uqo,dde,Jqo,Yqo,Kqo,at,uw,Zqo,cde,eGo,oGo,Yd,rGo,fde,tGo,aGo,mde,nGo,sGo,lGo,gde,iGo,dGo,bw,cGo,Je,vw,fGo,hde,mGo,gGo,nn,hGo,pde,pGo,_Go,_de,uGo,bGo,ude,vGo,TGo,FGo,bde,u5,vde,CGo,MGo,LN,EGo,yGo,wGo,b5,AGo,Tde,LGo,BGo,Fde,kGo,xGo,Cde,RGo,SGo,Tw,a9e,Kd,v5,Mde,Fw,PGo,Ede,$Go,n9e,mr,Cw,IGo,Zd,jGo,yde,NGo,DGo,wde,qGo,GGo,OGo,Mw,XGo,Ade,zGo,VGo,WGo,nt,Ew,QGo,Lde,HGo,UGo,ec,JGo,Bde,YGo,KGo,kde,ZGo,eOo,oOo,xde,rOo,tOo,yw,aOo,Ye,ww,nOo,Rde,sOo,lOo,sn,iOo,Sde,dOo,cOo,Pde,fOo,mOo,$de,gOo,hOo,pOo,Aw,T5,Ide,_Oo,uOo,BN,bOo,vOo,TOo,F5,jde,FOo,COo,kN,MOo,EOo,yOo,C5,wOo,Nde,AOo,LOo,Dde,BOo,kOo,qde,xOo,ROo,Lw,s9e,oc,M5,Gde,Bw,SOo,Ode,POo,l9e,gr,kw,$Oo,rc,IOo,Xde,jOo,NOo,zde,DOo,qOo,GOo,xw,OOo,Vde,XOo,zOo,VOo,st,Rw,WOo,Wde,QOo,HOo,tc,UOo,Qde,JOo,YOo,Hde,KOo,ZOo,eXo,Ude,oXo,rXo,Sw,tXo,go,Pw,aXo,Jde,nXo,sXo,ln,lXo,Yde,iXo,dXo,Kde,cXo,fXo,Zde,mXo,gXo,hXo,B,E5,ece,pXo,_Xo,xN,uXo,bXo,vXo,y5,oce,TXo,FXo,RN,CXo,MXo,EXo,w5,rce,yXo,wXo,SN,AXo,LXo,BXo,A5,tce,kXo,xXo,PN,RXo,SXo,PXo,L5,ace,$Xo,IXo,$N,jXo,NXo,DXo,B5,nce,qXo,GXo,IN,OXo,XXo,zXo,k5,sce,VXo,WXo,jN,QXo,HXo,UXo,x5,lce,JXo,YXo,NN,KXo,ZXo,ezo,R5,ice,ozo,rzo,DN,tzo,azo,nzo,S5,dce,szo,lzo,qN,izo,dzo,czo,P5,cce,fzo,mzo,GN,gzo,hzo,pzo,$5,fce,_zo,uzo,ON,bzo,vzo,Tzo,I5,mce,Fzo,Czo,XN,Mzo,Ezo,yzo,j5,gce,wzo,Azo,zN,Lzo,Bzo,kzo,N5,hce,xzo,Rzo,VN,Szo,Pzo,$zo,Ss,pce,Izo,jzo,WN,Nzo,Dzo,QN,qzo,Gzo,Ozo,D5,_ce,Xzo,zzo,HN,Vzo,Wzo,Qzo,q5,uce,Hzo,Uzo,UN,Jzo,Yzo,Kzo,G5,bce,Zzo,eVo,JN,oVo,rVo,tVo,O5,vce,aVo,nVo,YN,sVo,lVo,iVo,X5,Tce,dVo,cVo,KN,fVo,mVo,gVo,z5,Fce,hVo,pVo,ZN,_Vo,uVo,bVo,V5,Cce,vVo,TVo,eD,FVo,CVo,MVo,W5,Mce,EVo,yVo,oD,wVo,AVo,LVo,Q5,Ece,BVo,kVo,rD,xVo,RVo,SVo,H5,yce,PVo,$Vo,tD,IVo,jVo,NVo,U5,wce,DVo,qVo,aD,GVo,OVo,XVo,J5,Ace,zVo,VVo,nD,WVo,QVo,HVo,Y5,Lce,UVo,JVo,sD,YVo,KVo,ZVo,K5,Bce,eWo,oWo,lD,rWo,tWo,aWo,Z5,kce,nWo,sWo,iD,lWo,iWo,dWo,e2,xce,cWo,fWo,dD,mWo,gWo,hWo,o2,Rce,pWo,_Wo,cD,uWo,bWo,vWo,r2,Sce,TWo,FWo,fD,CWo,MWo,EWo,t2,Pce,yWo,wWo,mD,AWo,LWo,BWo,a2,$ce,kWo,xWo,gD,RWo,SWo,PWo,n2,Ice,$Wo,IWo,hD,jWo,NWo,DWo,s2,jce,qWo,GWo,pD,OWo,XWo,zWo,l2,Nce,VWo,WWo,_D,QWo,HWo,UWo,i2,Dce,JWo,YWo,uD,KWo,ZWo,eQo,d2,qce,oQo,rQo,bD,tQo,aQo,nQo,Gce,sQo,lQo,$w,i9e,ac,c2,Oce,Iw,iQo,Xce,dQo,d9e,hr,jw,cQo,nc,fQo,zce,mQo,gQo,Vce,hQo,pQo,_Qo,Nw,uQo,Wce,bQo,vQo,TQo,lt,Dw,FQo,Qce,CQo,MQo,sc,EQo,Hce,yQo,wQo,Uce,AQo,LQo,BQo,Jce,kQo,xQo,qw,RQo,ho,Gw,SQo,Yce,PQo,$Qo,dn,IQo,Kce,jQo,NQo,Zce,DQo,qQo,efe,GQo,OQo,XQo,H,f2,ofe,zQo,VQo,vD,WQo,QQo,HQo,m2,rfe,UQo,JQo,TD,YQo,KQo,ZQo,g2,tfe,eHo,oHo,FD,rHo,tHo,aHo,h2,afe,nHo,sHo,CD,lHo,iHo,dHo,p2,nfe,cHo,fHo,MD,mHo,gHo,hHo,_2,sfe,pHo,_Ho,ED,uHo,bHo,vHo,u2,lfe,THo,FHo,yD,CHo,MHo,EHo,b2,ife,yHo,wHo,wD,AHo,LHo,BHo,v2,dfe,kHo,xHo,AD,RHo,SHo,PHo,T2,cfe,$Ho,IHo,LD,jHo,NHo,DHo,F2,ffe,qHo,GHo,BD,OHo,XHo,zHo,C2,mfe,VHo,WHo,kD,QHo,HHo,UHo,M2,gfe,JHo,YHo,xD,KHo,ZHo,eUo,E2,hfe,oUo,rUo,RD,tUo,aUo,nUo,y2,pfe,sUo,lUo,SD,iUo,dUo,cUo,w2,_fe,fUo,mUo,PD,gUo,hUo,pUo,A2,ufe,_Uo,uUo,$D,bUo,vUo,TUo,L2,bfe,FUo,CUo,ID,MUo,EUo,yUo,B2,vfe,wUo,AUo,jD,LUo,BUo,kUo,k2,Tfe,xUo,RUo,ND,SUo,PUo,$Uo,x2,Ffe,IUo,jUo,DD,NUo,DUo,qUo,R2,Cfe,GUo,OUo,qD,XUo,zUo,VUo,Mfe,WUo,QUo,Ow,c9e,lc,S2,Efe,Xw,HUo,yfe,UUo,f9e,pr,zw,JUo,ic,YUo,wfe,KUo,ZUo,Afe,eJo,oJo,rJo,Vw,tJo,Lfe,aJo,nJo,sJo,it,Ww,lJo,Bfe,iJo,dJo,dc,cJo,kfe,fJo,mJo,xfe,gJo,hJo,pJo,Rfe,_Jo,uJo,Qw,bJo,po,Hw,vJo,Sfe,TJo,FJo,cn,CJo,Pfe,MJo,EJo,$fe,yJo,wJo,Ife,AJo,LJo,BJo,he,P2,jfe,kJo,xJo,GD,RJo,SJo,PJo,$2,Nfe,$Jo,IJo,OD,jJo,NJo,DJo,I2,Dfe,qJo,GJo,XD,OJo,XJo,zJo,j2,qfe,VJo,WJo,zD,QJo,HJo,UJo,N2,Gfe,JJo,YJo,VD,KJo,ZJo,eYo,D2,Ofe,oYo,rYo,WD,tYo,aYo,nYo,q2,Xfe,sYo,lYo,QD,iYo,dYo,cYo,G2,zfe,fYo,mYo,HD,gYo,hYo,pYo,O2,Vfe,_Yo,uYo,UD,bYo,vYo,TYo,X2,Wfe,FYo,CYo,JD,MYo,EYo,yYo,Qfe,wYo,AYo,Uw,m9e,cc,z2,Hfe,Jw,LYo,Ufe,BYo,g9e,_r,Yw,kYo,fc,xYo,Jfe,RYo,SYo,Yfe,PYo,$Yo,IYo,Kw,jYo,Kfe,NYo,DYo,qYo,dt,Zw,GYo,Zfe,OYo,XYo,mc,zYo,eme,VYo,WYo,ome,QYo,HYo,UYo,rme,JYo,YYo,eA,KYo,_o,oA,ZYo,tme,eKo,oKo,fn,rKo,ame,tKo,aKo,nme,nKo,sKo,sme,lKo,iKo,dKo,lme,V2,ime,cKo,fKo,YD,mKo,gKo,hKo,dme,pKo,_Ko,rA,h9e,gc,W2,cme,tA,uKo,fme,bKo,p9e,ur,aA,vKo,hc,TKo,mme,FKo,CKo,gme,MKo,EKo,yKo,nA,wKo,hme,AKo,LKo,BKo,ct,sA,kKo,pme,xKo,RKo,pc,SKo,_me,PKo,$Ko,ume,IKo,jKo,NKo,bme,DKo,qKo,lA,GKo,uo,iA,OKo,vme,XKo,zKo,mn,VKo,Tme,WKo,QKo,Fme,HKo,UKo,Cme,JKo,YKo,KKo,Y,Q2,Mme,ZKo,eZo,KD,oZo,rZo,tZo,H2,Eme,aZo,nZo,ZD,sZo,lZo,iZo,U2,yme,dZo,cZo,eq,fZo,mZo,gZo,J2,wme,hZo,pZo,oq,_Zo,uZo,bZo,Y2,Ame,vZo,TZo,rq,FZo,CZo,MZo,K2,Lme,EZo,yZo,tq,wZo,AZo,LZo,Z2,Bme,BZo,kZo,aq,xZo,RZo,SZo,ev,kme,PZo,$Zo,nq,IZo,jZo,NZo,ov,xme,DZo,qZo,sq,GZo,OZo,XZo,rv,Rme,zZo,VZo,lq,WZo,QZo,HZo,tv,Sme,UZo,JZo,iq,YZo,KZo,ZZo,av,Pme,eer,oer,dq,rer,ter,aer,nv,$me,ner,ser,cq,ler,ier,der,sv,Ime,cer,fer,fq,mer,ger,her,lv,jme,per,_er,mq,uer,ber,ver,iv,Nme,Ter,Fer,gq,Cer,Mer,Eer,dv,Dme,yer,wer,hq,Aer,Ler,Ber,cv,qme,ker,xer,pq,Rer,Ser,Per,fv,Gme,$er,Ier,_q,jer,Ner,Der,mv,Ome,qer,Ger,uq,Oer,Xer,zer,Xme,Ver,Wer,dA,_9e,_c,gv,zme,cA,Qer,Vme,Her,u9e,br,fA,Uer,uc,Jer,Wme,Yer,Ker,Qme,Zer,eor,oor,mA,ror,Hme,tor,aor,nor,ft,gA,sor,Ume,lor,ior,bc,dor,Jme,cor,mor,Yme,gor,hor,por,Kme,_or,uor,hA,bor,bo,pA,vor,Zme,Tor,For,gn,Cor,ege,Mor,Eor,oge,yor,wor,rge,Aor,Lor,Bor,pe,hv,tge,kor,xor,bq,Ror,Sor,Por,pv,age,$or,Ior,vq,jor,Nor,Dor,_v,nge,qor,Gor,Tq,Oor,Xor,zor,uv,sge,Vor,Wor,Fq,Qor,Hor,Uor,bv,lge,Jor,Yor,Cq,Kor,Zor,err,vv,ige,orr,rrr,Mq,trr,arr,nrr,Tv,dge,srr,lrr,Eq,irr,drr,crr,Fv,cge,frr,mrr,yq,grr,hrr,prr,Cv,fge,_rr,urr,wq,brr,vrr,Trr,Mv,mge,Frr,Crr,Aq,Mrr,Err,yrr,gge,wrr,Arr,_A,b9e,vc,Ev,hge,uA,Lrr,pge,Brr,v9e,vr,bA,krr,Tc,xrr,_ge,Rrr,Srr,uge,Prr,$rr,Irr,vA,jrr,bge,Nrr,Drr,qrr,mt,TA,Grr,vge,Orr,Xrr,Fc,zrr,Tge,Vrr,Wrr,Fge,Qrr,Hrr,Urr,Cge,Jrr,Yrr,FA,Krr,vo,CA,Zrr,Mge,etr,otr,hn,rtr,Ege,ttr,atr,yge,ntr,str,wge,ltr,itr,dtr,X,yv,Age,ctr,ftr,Lq,mtr,gtr,htr,wv,Lge,ptr,_tr,Bq,utr,btr,vtr,Av,Bge,Ttr,Ftr,kq,Ctr,Mtr,Etr,Lv,kge,ytr,wtr,xq,Atr,Ltr,Btr,Bv,xge,ktr,xtr,Rq,Rtr,Str,Ptr,kv,Rge,$tr,Itr,Sq,jtr,Ntr,Dtr,xv,Sge,qtr,Gtr,Pq,Otr,Xtr,ztr,Rv,Pge,Vtr,Wtr,$q,Qtr,Htr,Utr,Sv,$ge,Jtr,Ytr,Iq,Ktr,Ztr,ear,Pv,Ige,oar,rar,jq,tar,aar,nar,$v,jge,sar,lar,Nq,iar,dar,car,Iv,Nge,far,mar,Dq,gar,har,par,jv,Dge,_ar,uar,qq,bar,Tar,Far,Nv,qge,Car,Mar,Gq,Ear,yar,war,Dv,Gge,Aar,Lar,Oq,Bar,kar,xar,qv,Oge,Rar,Sar,Xq,Par,$ar,Iar,Gv,Xge,jar,Nar,zq,Dar,qar,Gar,Ov,zge,Oar,Xar,Vq,zar,Var,War,Xv,Vge,Qar,Har,Wq,Uar,Jar,Yar,zv,Wge,Kar,Zar,Qq,enr,onr,rnr,Vv,Qge,tnr,anr,Hq,nnr,snr,lnr,Wv,Hge,inr,dnr,Uq,cnr,fnr,mnr,Qv,Uge,gnr,hnr,Jq,pnr,_nr,unr,Hv,Jge,bnr,vnr,Yq,Tnr,Fnr,Cnr,Uv,Yge,Mnr,Enr,Kq,ynr,wnr,Anr,Kge,Lnr,Bnr,MA,T9e,Cc,Jv,Zge,EA,knr,ehe,xnr,F9e,Tr,yA,Rnr,Mc,Snr,ohe,Pnr,$nr,rhe,Inr,jnr,Nnr,wA,Dnr,the,qnr,Gnr,Onr,gt,AA,Xnr,ahe,znr,Vnr,Ec,Wnr,nhe,Qnr,Hnr,she,Unr,Jnr,Ynr,lhe,Knr,Znr,LA,esr,To,BA,osr,ihe,rsr,tsr,pn,asr,dhe,nsr,ssr,che,lsr,isr,fhe,dsr,csr,fsr,te,Yv,mhe,msr,gsr,Zq,hsr,psr,_sr,Kv,ghe,usr,bsr,eG,vsr,Tsr,Fsr,Zv,hhe,Csr,Msr,oG,Esr,ysr,wsr,eT,phe,Asr,Lsr,rG,Bsr,ksr,xsr,oT,_he,Rsr,Ssr,tG,Psr,$sr,Isr,rT,uhe,jsr,Nsr,aG,Dsr,qsr,Gsr,tT,bhe,Osr,Xsr,nG,zsr,Vsr,Wsr,aT,vhe,Qsr,Hsr,sG,Usr,Jsr,Ysr,nT,The,Ksr,Zsr,lG,elr,olr,rlr,sT,Fhe,tlr,alr,iG,nlr,slr,llr,lT,Che,ilr,dlr,dG,clr,flr,mlr,iT,Mhe,glr,hlr,cG,plr,_lr,ulr,dT,Ehe,blr,vlr,fG,Tlr,Flr,Clr,cT,yhe,Mlr,Elr,mG,ylr,wlr,Alr,fT,whe,Llr,Blr,gG,klr,xlr,Rlr,mT,Ahe,Slr,Plr,hG,$lr,Ilr,jlr,gT,Lhe,Nlr,Dlr,pG,qlr,Glr,Olr,Bhe,Xlr,zlr,kA,C9e,yc,hT,khe,xA,Vlr,xhe,Wlr,M9e,Fr,RA,Qlr,wc,Hlr,Rhe,Ulr,Jlr,She,Ylr,Klr,Zlr,SA,eir,Phe,oir,rir,tir,ht,PA,air,$he,nir,sir,Ac,lir,Ihe,iir,dir,jhe,cir,fir,mir,Nhe,gir,hir,$A,pir,Fo,IA,_ir,Dhe,uir,bir,_n,vir,qhe,Tir,Fir,Ghe,Cir,Mir,Ohe,Eir,yir,wir,Xhe,pT,zhe,Air,Lir,_G,Bir,kir,xir,Vhe,Rir,Sir,jA,E9e,Lc,_T,Whe,NA,Pir,Qhe,$ir,y9e,Cr,DA,Iir,Bc,jir,Hhe,Nir,Dir,Uhe,qir,Gir,Oir,qA,Xir,Jhe,zir,Vir,Wir,pt,GA,Qir,Yhe,Hir,Uir,kc,Jir,Khe,Yir,Kir,Zhe,Zir,edr,odr,epe,rdr,tdr,OA,adr,Co,XA,ndr,ope,sdr,ldr,un,idr,rpe,ddr,cdr,tpe,fdr,mdr,ape,gdr,hdr,pdr,K,uT,npe,_dr,udr,uG,bdr,vdr,Tdr,bT,spe,Fdr,Cdr,bG,Mdr,Edr,ydr,vT,lpe,wdr,Adr,vG,Ldr,Bdr,kdr,TT,ipe,xdr,Rdr,TG,Sdr,Pdr,$dr,FT,dpe,Idr,jdr,FG,Ndr,Ddr,qdr,CT,cpe,Gdr,Odr,CG,Xdr,zdr,Vdr,MT,fpe,Wdr,Qdr,MG,Hdr,Udr,Jdr,ET,mpe,Ydr,Kdr,EG,Zdr,ecr,ocr,yT,gpe,rcr,tcr,yG,acr,ncr,scr,wT,hpe,lcr,icr,wG,dcr,ccr,fcr,AT,ppe,mcr,gcr,AG,hcr,pcr,_cr,LT,_pe,ucr,bcr,LG,vcr,Tcr,Fcr,BT,upe,Ccr,Mcr,BG,Ecr,ycr,wcr,kT,bpe,Acr,Lcr,kG,Bcr,kcr,xcr,xT,vpe,Rcr,Scr,xG,Pcr,$cr,Icr,RT,Tpe,jcr,Ncr,RG,Dcr,qcr,Gcr,ST,Fpe,Ocr,Xcr,SG,zcr,Vcr,Wcr,PT,Cpe,Qcr,Hcr,PG,Ucr,Jcr,Ycr,$T,Mpe,Kcr,Zcr,$G,efr,ofr,rfr,IT,Epe,tfr,afr,IG,nfr,sfr,lfr,ype,ifr,dfr,zA,w9e,xc,jT,wpe,VA,cfr,Ape,ffr,A9e,Mr,WA,mfr,Rc,gfr,Lpe,hfr,pfr,Bpe,_fr,ufr,bfr,QA,vfr,kpe,Tfr,Ffr,Cfr,_t,HA,Mfr,xpe,Efr,yfr,Sc,wfr,Rpe,Afr,Lfr,Spe,Bfr,kfr,xfr,Ppe,Rfr,Sfr,UA,Pfr,Mo,JA,$fr,$pe,Ifr,jfr,bn,Nfr,Ipe,Dfr,qfr,jpe,Gfr,Ofr,Npe,Xfr,zfr,Vfr,Z,NT,Dpe,Wfr,Qfr,jG,Hfr,Ufr,Jfr,DT,qpe,Yfr,Kfr,NG,Zfr,emr,omr,qT,Gpe,rmr,tmr,DG,amr,nmr,smr,GT,Ope,lmr,imr,qG,dmr,cmr,fmr,OT,Xpe,mmr,gmr,GG,hmr,pmr,_mr,XT,zpe,umr,bmr,OG,vmr,Tmr,Fmr,zT,Vpe,Cmr,Mmr,XG,Emr,ymr,wmr,VT,Wpe,Amr,Lmr,zG,Bmr,kmr,xmr,WT,Qpe,Rmr,Smr,VG,Pmr,$mr,Imr,QT,Hpe,jmr,Nmr,WG,Dmr,qmr,Gmr,HT,Upe,Omr,Xmr,QG,zmr,Vmr,Wmr,UT,Jpe,Qmr,Hmr,HG,Umr,Jmr,Ymr,JT,Ype,Kmr,Zmr,UG,egr,ogr,rgr,YT,Kpe,tgr,agr,JG,ngr,sgr,lgr,KT,Zpe,igr,dgr,YG,cgr,fgr,mgr,ZT,e_e,ggr,hgr,KG,pgr,_gr,ugr,eF,o_e,bgr,vgr,ZG,Tgr,Fgr,Cgr,oF,r_e,Mgr,Egr,eO,ygr,wgr,Agr,rF,t_e,Lgr,Bgr,oO,kgr,xgr,Rgr,a_e,Sgr,Pgr,YA,L9e,Pc,tF,n_e,KA,$gr,s_e,Igr,B9e,Er,ZA,jgr,$c,Ngr,l_e,Dgr,qgr,i_e,Ggr,Ogr,Xgr,e6,zgr,d_e,Vgr,Wgr,Qgr,ut,o6,Hgr,c_e,Ugr,Jgr,Ic,Ygr,f_e,Kgr,Zgr,m_e,ehr,ohr,rhr,g_e,thr,ahr,r6,nhr,Eo,t6,shr,h_e,lhr,ihr,vn,dhr,p_e,chr,fhr,__e,mhr,ghr,u_e,hhr,phr,_hr,b_e,aF,v_e,uhr,bhr,rO,vhr,Thr,Fhr,T_e,Chr,Mhr,a6,k9e,jc,nF,F_e,n6,Ehr,C_e,yhr,x9e,yr,s6,whr,Nc,Ahr,M_e,Lhr,Bhr,E_e,khr,xhr,Rhr,l6,Shr,y_e,Phr,$hr,Ihr,bt,i6,jhr,w_e,Nhr,Dhr,Dc,qhr,A_e,Ghr,Ohr,L_e,Xhr,zhr,Vhr,B_e,Whr,Qhr,d6,Hhr,yo,c6,Uhr,k_e,Jhr,Yhr,Tn,Khr,x_e,Zhr,epr,R_e,opr,rpr,S_e,tpr,apr,npr,P_e,sF,$_e,spr,lpr,tO,ipr,dpr,cpr,I_e,fpr,mpr,f6,R9e,qc,lF,j_e,m6,gpr,N_e,hpr,S9e,wr,g6,ppr,Gc,_pr,D_e,upr,bpr,q_e,vpr,Tpr,Fpr,h6,Cpr,G_e,Mpr,Epr,ypr,vt,p6,wpr,O_e,Apr,Lpr,Oc,Bpr,X_e,kpr,xpr,z_e,Rpr,Spr,Ppr,V_e,$pr,Ipr,_6,jpr,wo,u6,Npr,W_e,Dpr,qpr,Fn,Gpr,Q_e,Opr,Xpr,H_e,zpr,Vpr,U_e,Wpr,Qpr,Hpr,V,iF,J_e,Upr,Jpr,aO,Ypr,Kpr,Zpr,dF,Y_e,e_r,o_r,nO,r_r,t_r,a_r,cF,K_e,n_r,s_r,sO,l_r,i_r,d_r,fF,Z_e,c_r,f_r,lO,m_r,g_r,h_r,mF,eue,p_r,__r,iO,u_r,b_r,v_r,gF,oue,T_r,F_r,dO,C_r,M_r,E_r,hF,rue,y_r,w_r,cO,A_r,L_r,B_r,pF,tue,k_r,x_r,fO,R_r,S_r,P_r,_F,aue,$_r,I_r,mO,j_r,N_r,D_r,uF,nue,q_r,G_r,gO,O_r,X_r,z_r,bF,sue,V_r,W_r,hO,Q_r,H_r,U_r,vF,lue,J_r,Y_r,pO,K_r,Z_r,eur,TF,iue,our,rur,_O,tur,aur,nur,FF,due,sur,lur,uO,iur,dur,cur,CF,cue,fur,mur,bO,gur,hur,pur,MF,fue,_ur,uur,vO,bur,vur,Tur,EF,mue,Fur,Cur,TO,Mur,Eur,yur,yF,gue,wur,Aur,FO,Lur,Bur,kur,wF,hue,xur,Rur,CO,Sur,Pur,$ur,AF,pue,Iur,jur,MO,Nur,Dur,qur,LF,_ue,Gur,Our,EO,Xur,zur,Vur,BF,uue,Wur,Qur,yO,Hur,Uur,Jur,kF,bue,Yur,Kur,wO,Zur,e1r,o1r,xF,vue,r1r,t1r,AO,a1r,n1r,s1r,Tue,l1r,i1r,b6,P9e,Xc,RF,Fue,v6,d1r,Cue,c1r,$9e,Ar,T6,f1r,zc,m1r,Mue,g1r,h1r,Eue,p1r,_1r,u1r,F6,b1r,yue,v1r,T1r,F1r,Tt,C6,C1r,wue,M1r,E1r,Vc,y1r,Aue,w1r,A1r,Lue,L1r,B1r,k1r,Bue,x1r,R1r,M6,S1r,Ao,E6,P1r,kue,$1r,I1r,Cn,j1r,xue,N1r,D1r,Rue,q1r,G1r,Sue,O1r,X1r,z1r,Mn,SF,Pue,V1r,W1r,LO,Q1r,H1r,U1r,PF,$ue,J1r,Y1r,BO,K1r,Z1r,e7r,$F,Iue,o7r,r7r,kO,t7r,a7r,n7r,IF,jue,s7r,l7r,xO,i7r,d7r,c7r,Nue,f7r,m7r,y6,I9e,Wc,jF,Due,w6,g7r,que,h7r,j9e,Lr,A6,p7r,Qc,_7r,Gue,u7r,b7r,Oue,v7r,T7r,F7r,L6,C7r,Xue,M7r,E7r,y7r,Ft,B6,w7r,zue,A7r,L7r,Hc,B7r,Vue,k7r,x7r,Wue,R7r,S7r,P7r,Que,$7r,I7r,k6,j7r,Lo,x6,N7r,Hue,D7r,q7r,En,G7r,Uue,O7r,X7r,Jue,z7r,V7r,Yue,W7r,Q7r,H7r,fe,NF,Kue,U7r,J7r,RO,Y7r,K7r,Z7r,DF,Zue,e4r,o4r,SO,r4r,t4r,a4r,qF,e1e,n4r,s4r,PO,l4r,i4r,d4r,GF,o1e,c4r,f4r,$O,m4r,g4r,h4r,OF,r1e,p4r,_4r,IO,u4r,b4r,v4r,XF,t1e,T4r,F4r,jO,C4r,M4r,E4r,zF,a1e,y4r,w4r,NO,A4r,L4r,B4r,VF,n1e,k4r,x4r,DO,R4r,S4r,P4r,WF,s1e,$4r,I4r,qO,j4r,N4r,D4r,QF,l1e,q4r,G4r,GO,O4r,X4r,z4r,HF,i1e,V4r,W4r,OO,Q4r,H4r,U4r,d1e,J4r,Y4r,R6,N9e,Uc,UF,c1e,S6,K4r,f1e,Z4r,D9e,Br,P6,ebr,Jc,obr,m1e,rbr,tbr,g1e,abr,nbr,sbr,$6,lbr,h1e,ibr,dbr,cbr,Ct,I6,fbr,p1e,mbr,gbr,Yc,hbr,_1e,pbr,_br,u1e,ubr,bbr,vbr,b1e,Tbr,Fbr,j6,Cbr,Bo,N6,Mbr,v1e,Ebr,ybr,yn,wbr,T1e,Abr,Lbr,F1e,Bbr,kbr,C1e,xbr,Rbr,Sbr,ve,JF,M1e,Pbr,$br,XO,Ibr,jbr,Nbr,YF,E1e,Dbr,qbr,zO,Gbr,Obr,Xbr,KF,y1e,zbr,Vbr,VO,Wbr,Qbr,Hbr,ZF,w1e,Ubr,Jbr,WO,Ybr,Kbr,Zbr,eC,A1e,e5r,o5r,QO,r5r,t5r,a5r,oC,L1e,n5r,s5r,HO,l5r,i5r,d5r,rC,B1e,c5r,f5r,UO,m5r,g5r,h5r,tC,k1e,p5r,_5r,JO,u5r,b5r,v5r,aC,x1e,T5r,F5r,YO,C5r,M5r,E5r,R1e,y5r,w5r,D6,q9e,Kc,nC,S1e,q6,A5r,P1e,L5r,G9e,kr,G6,B5r,Zc,k5r,$1e,x5r,R5r,I1e,S5r,P5r,$5r,O6,I5r,j1e,j5r,N5r,D5r,Mt,X6,q5r,N1e,G5r,O5r,ef,X5r,D1e,z5r,V5r,q1e,W5r,Q5r,H5r,G1e,U5r,J5r,z6,Y5r,ko,V6,K5r,O1e,Z5r,e2r,wn,o2r,X1e,r2r,t2r,z1e,a2r,n2r,V1e,s2r,l2r,i2r,Te,sC,W1e,d2r,c2r,KO,f2r,m2r,g2r,lC,Q1e,h2r,p2r,ZO,_2r,u2r,b2r,iC,H1e,v2r,T2r,eX,F2r,C2r,M2r,dC,U1e,E2r,y2r,oX,w2r,A2r,L2r,cC,J1e,B2r,k2r,rX,x2r,R2r,S2r,fC,Y1e,P2r,$2r,tX,I2r,j2r,N2r,mC,K1e,D2r,q2r,aX,G2r,O2r,X2r,gC,Z1e,z2r,V2r,nX,W2r,Q2r,H2r,hC,e7e,U2r,J2r,sX,Y2r,K2r,Z2r,o7e,evr,ovr,W6,O9e,of,pC,r7e,Q6,rvr,t7e,tvr,X9e,xr,H6,avr,rf,nvr,a7e,svr,lvr,n7e,ivr,dvr,cvr,U6,fvr,s7e,mvr,gvr,hvr,Et,J6,pvr,l7e,_vr,uvr,tf,bvr,i7e,vvr,Tvr,d7e,Fvr,Cvr,Mvr,c7e,Evr,yvr,Y6,wvr,xo,K6,Avr,f7e,Lvr,Bvr,An,kvr,m7e,xvr,Rvr,g7e,Svr,Pvr,h7e,$vr,Ivr,jvr,Fe,_C,p7e,Nvr,Dvr,lX,qvr,Gvr,Ovr,uC,_7e,Xvr,zvr,iX,Vvr,Wvr,Qvr,bC,u7e,Hvr,Uvr,dX,Jvr,Yvr,Kvr,vC,b7e,Zvr,eTr,cX,oTr,rTr,tTr,TC,v7e,aTr,nTr,fX,sTr,lTr,iTr,FC,T7e,dTr,cTr,mX,fTr,mTr,gTr,CC,F7e,hTr,pTr,gX,_Tr,uTr,bTr,MC,C7e,vTr,TTr,hX,FTr,CTr,MTr,EC,M7e,ETr,yTr,pX,wTr,ATr,LTr,E7e,BTr,kTr,Z6,z9e,af,yC,y7e,e0,xTr,w7e,RTr,V9e,Rr,o0,STr,nf,PTr,A7e,$Tr,ITr,L7e,jTr,NTr,DTr,r0,qTr,B7e,GTr,OTr,XTr,yt,t0,zTr,k7e,VTr,WTr,sf,QTr,x7e,HTr,UTr,R7e,JTr,YTr,KTr,S7e,ZTr,eFr,a0,oFr,Ro,n0,rFr,P7e,tFr,aFr,Ln,nFr,$7e,sFr,lFr,I7e,iFr,dFr,j7e,cFr,fFr,mFr,Ce,wC,N7e,gFr,hFr,_X,pFr,_Fr,uFr,AC,D7e,bFr,vFr,uX,TFr,FFr,CFr,LC,q7e,MFr,EFr,bX,yFr,wFr,AFr,BC,G7e,LFr,BFr,vX,kFr,xFr,RFr,kC,O7e,SFr,PFr,TX,$Fr,IFr,jFr,xC,X7e,NFr,DFr,FX,qFr,GFr,OFr,RC,z7e,XFr,zFr,CX,VFr,WFr,QFr,SC,V7e,HFr,UFr,MX,JFr,YFr,KFr,PC,W7e,ZFr,eCr,EX,oCr,rCr,tCr,Q7e,aCr,nCr,s0,W9e,lf,$C,H7e,l0,sCr,U7e,lCr,Q9e,Sr,i0,iCr,df,dCr,J7e,cCr,fCr,Y7e,mCr,gCr,hCr,d0,pCr,K7e,_Cr,uCr,bCr,wt,c0,vCr,Z7e,TCr,FCr,cf,CCr,e4e,MCr,ECr,o4e,yCr,wCr,ACr,r4e,LCr,BCr,f0,kCr,So,m0,xCr,t4e,RCr,SCr,Bn,PCr,a4e,$Cr,ICr,n4e,jCr,NCr,s4e,DCr,qCr,GCr,so,IC,l4e,OCr,XCr,yX,zCr,VCr,WCr,jC,i4e,QCr,HCr,wX,UCr,JCr,YCr,NC,d4e,KCr,ZCr,AX,eMr,oMr,rMr,DC,c4e,tMr,aMr,LX,nMr,sMr,lMr,qC,f4e,iMr,dMr,BX,cMr,fMr,mMr,GC,m4e,gMr,hMr,kX,pMr,_Mr,uMr,OC,g4e,bMr,vMr,xX,TMr,FMr,CMr,h4e,MMr,EMr,g0,H9e,ff,XC,p4e,h0,yMr,_4e,wMr,U9e,Pr,p0,AMr,mf,LMr,u4e,BMr,kMr,b4e,xMr,RMr,SMr,_0,PMr,v4e,$Mr,IMr,jMr,At,u0,NMr,T4e,DMr,qMr,gf,GMr,F4e,OMr,XMr,C4e,zMr,VMr,WMr,M4e,QMr,HMr,b0,UMr,Po,v0,JMr,E4e,YMr,KMr,kn,ZMr,y4e,eEr,oEr,w4e,rEr,tEr,A4e,aEr,nEr,sEr,lo,zC,L4e,lEr,iEr,RX,dEr,cEr,fEr,VC,B4e,mEr,gEr,SX,hEr,pEr,_Er,WC,k4e,uEr,bEr,PX,vEr,TEr,FEr,QC,x4e,CEr,MEr,$X,EEr,yEr,wEr,HC,R4e,AEr,LEr,IX,BEr,kEr,xEr,UC,S4e,REr,SEr,jX,PEr,$Er,IEr,JC,P4e,jEr,NEr,NX,DEr,qEr,GEr,$4e,OEr,XEr,T0,J9e,hf,YC,I4e,F0,zEr,j4e,VEr,Y9e,$r,C0,WEr,pf,QEr,N4e,HEr,UEr,D4e,JEr,YEr,KEr,M0,ZEr,q4e,e3r,o3r,r3r,Lt,E0,t3r,G4e,a3r,n3r,_f,s3r,O4e,l3r,i3r,X4e,d3r,c3r,f3r,z4e,m3r,g3r,y0,h3r,$o,w0,p3r,V4e,_3r,u3r,xn,b3r,W4e,v3r,T3r,Q4e,F3r,C3r,H4e,M3r,E3r,y3r,U4e,KC,J4e,w3r,A3r,DX,L3r,B3r,k3r,Y4e,x3r,R3r,A0,K9e,uf,ZC,K4e,L0,S3r,Z4e,P3r,Z9e,Ir,B0,$3r,bf,I3r,ebe,j3r,N3r,obe,D3r,q3r,G3r,k0,O3r,rbe,X3r,z3r,V3r,Bt,x0,W3r,tbe,Q3r,H3r,vf,U3r,abe,J3r,Y3r,nbe,K3r,Z3r,eyr,sbe,oyr,ryr,R0,tyr,Io,S0,ayr,lbe,nyr,syr,Rn,lyr,ibe,iyr,dyr,dbe,cyr,fyr,cbe,myr,gyr,hyr,P0,eM,fbe,pyr,_yr,qX,uyr,byr,vyr,oM,mbe,Tyr,Fyr,GX,Cyr,Myr,Eyr,gbe,yyr,wyr,$0,eBe,Tf,rM,hbe,I0,Ayr,pbe,Lyr,oBe,jr,j0,Byr,Ff,kyr,_be,xyr,Ryr,ube,Syr,Pyr,$yr,N0,Iyr,bbe,jyr,Nyr,Dyr,kt,D0,qyr,vbe,Gyr,Oyr,Cf,Xyr,Tbe,zyr,Vyr,Fbe,Wyr,Qyr,Hyr,Cbe,Uyr,Jyr,q0,Yyr,jo,G0,Kyr,Mbe,Zyr,ewr,Sn,owr,Ebe,rwr,twr,ybe,awr,nwr,wbe,swr,lwr,iwr,Abe,tM,Lbe,dwr,cwr,OX,fwr,mwr,gwr,Bbe,hwr,pwr,O0,rBe;return ce=new z({}),$a=new w({props:{code:'model = AutoModel.from_pretrained("bert-base-cased"),',highlighted:'model = AutoModel.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)'}}),nE=new z({}),sE=new w({props:{code:`from transformers import AutoConfig, AutoModel

AutoConfig.register("new-model", NewModelConfig)
AutoModel.register(NewModelConfig, NewModel),`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModel

AutoConfig.register(<span class="hljs-string">&quot;new-model&quot;</span>, NewModelConfig)
AutoModel.register(NewModelConfig, NewModel)`}}),Bf=new _wr({props:{warning:"&lcub;true}",$$slots:{default:[F_t]},$$scope:{ctx:yi}}}),lE=new z({}),iE=new E({props:{name:"class transformers.AutoConfig",anchor:"transformers.AutoConfig",parameters:[],source:"https://github.com/huggingface/transformers/blob/pr_15774/src/transformers/models/auto/configuration_auto.py#L515"}}),fE=new E({props:{name:"from_pretrained",anchor:"transformers.AutoConfig.from_pretrained",parameters:[{name:"pretrained_model_name_or_path",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15774/src/transformers/models/auto/configuration_auto.py#L538",parametersDescription:[{anchor:"transformers.AutoConfig.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model configuration hosted inside a model repo on
huggingface.co. Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or
namespaced under a user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing a configuration file saved using the
<a href="/docs/transformers/pr_15774/en/main_classes/configuration#transformers.PretrainedConfig.save_pretrained">save_pretrained()</a> method, or the <a href="/docs/transformers/pr_15774/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> method,
e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a saved configuration JSON <em>file</em>, e.g.,
<code>./my_model_directory/configuration.json</code>.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.AutoConfig.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.AutoConfig.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download the model weights and configuration files and override the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.AutoConfig.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.AutoConfig.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.AutoConfig.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.AutoConfig.from_pretrained.return_unused_kwargs",description:`<strong>return_unused_kwargs</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
If <code>False</code>, then this function returns just the final configuration object.</p>
<p>If <code>True</code>, then this functions returns a <code>Tuple(config, unused_kwargs)</code> where <em>unused_kwargs</em> is a
dictionary consisting of the key/value pairs whose keys are not configuration attributes: i.e., the
part of <code>kwargs</code> which has not been used to update <code>config</code> and is otherwise ignored.`,name:"return_unused_kwargs"},{anchor:"transformers.AutoConfig.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.AutoConfig.from_pretrained.kwargs(additional",description:`<strong>kwargs(additional</strong> keyword arguments, <em>optional</em>) &#x2014;
The values in kwargs of any keys which are configuration attributes will be used to override the loaded
values. Behavior concerning key/value pairs whose keys are <em>not</em> configuration attributes is controlled
by the <code>return_unused_kwargs</code> keyword parameter.`,name:"kwargs(additional"}]}}),mE=new w({props:{code:`from transformers import AutoConfig

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-uncased")

# Download configuration from huggingface.co (user-uploaded) and cache.
config = AutoConfig.from_pretrained("dbmdz/bert-base-german-cased")

# If configuration file is in a directory (e.g., was saved using *save_pretrained('./test/saved_model/')*).
config = AutoConfig.from_pretrained("./test/bert_saved_model/")

# Load a specific configuration file.
config = AutoConfig.from_pretrained("./test/bert_saved_model/my_configuration.json")

# Change some config attributes when loading a pretrained config.
config = AutoConfig.from_pretrained("bert-base-uncased", output_attentions=True, foo=False)
config.output_attentions

config, unused_kwargs = AutoConfig.from_pretrained(
    "bert-base-uncased", output_attentions=True, foo=False, return_unused_kwargs=True
)
config.output_attentions

config.unused_kwargs,`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-uncased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co (user-uploaded) and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;dbmdz/bert-base-german-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># If configuration file is in a directory (e.g., was saved using *save_pretrained(&#x27;./test/saved_model/&#x27;)*).</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./test/bert_saved_model/&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Load a specific configuration file.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./test/bert_saved_model/my_configuration.json&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Change some config attributes when loading a pretrained config.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-uncased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>, foo=<span class="hljs-literal">False</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>config, unused_kwargs = AutoConfig.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;bert-base-uncased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>, foo=<span class="hljs-literal">False</span>, return_unused_kwargs=<span class="hljs-literal">True</span>
<span class="hljs-meta">... </span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>config.unused_kwargs
{<span class="hljs-string">&#x27;foo&#x27;</span>: <span class="hljs-literal">False</span>}`}}),gE=new E({props:{name:"register",anchor:"transformers.AutoConfig.register",parameters:[{name:"model_type",val:""},{name:"config",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15774/src/transformers/models/auto/configuration_auto.py#L660",parametersDescription:[{anchor:"transformers.AutoConfig.register.model_type",description:"<strong>model_type</strong> (<code>str</code>) &#x2014; The model type like &#x201C;bert&#x201D; or &#x201C;gpt&#x201D;.",name:"model_type"},{anchor:"transformers.AutoConfig.register.config",description:'<strong>config</strong> (<a href="/docs/transformers/pr_15774/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014; The config to register.',name:"config"}]}}),hE=new z({}),pE=new E({props:{name:"class transformers.AutoTokenizer",anchor:"transformers.AutoTokenizer",parameters:[],source:"https://github.com/huggingface/transformers/blob/pr_15774/src/transformers/models/auto/tokenization_auto.py#L351"}}),bE=new E({props:{name:"from_pretrained",anchor:"transformers.AutoTokenizer.from_pretrained",parameters:[{name:"pretrained_model_name_or_path",val:""},{name:"*inputs",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15774/src/transformers/models/auto/tokenization_auto.py#L365",parametersDescription:[{anchor:"transformers.AutoTokenizer.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a predefined tokenizer hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing vocabulary files required by the tokenizer, for instance saved
using the <a href="/docs/transformers/pr_15774/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.save_pretrained">save_pretrained()</a> method, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a single saved vocabulary file if and only if the tokenizer only requires a
single vocabulary file (like Bert or XLNet), e.g.: <code>./my_model_directory/vocab.txt</code>. (Not
applicable to all derived classes)</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.AutoTokenizer.from_pretrained.inputs",description:`<strong>inputs</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the Tokenizer <code>__init__()</code> method.`,name:"inputs"},{anchor:"transformers.AutoTokenizer.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15774/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
The configuration object used to dertermine the tokenizer class to instantiate.`,name:"config"},{anchor:"transformers.AutoTokenizer.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.AutoTokenizer.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download the model weights and configuration files and override the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.AutoTokenizer.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.AutoTokenizer.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.AutoTokenizer.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.AutoTokenizer.from_pretrained.subfolder",description:`<strong>subfolder</strong> (<code>str</code>, <em>optional</em>) &#x2014;
In case the relevant files are located inside a subfolder of the model repo on huggingface.co (e.g. for
facebook/rag-token-base), specify it here.`,name:"subfolder"},{anchor:"transformers.AutoTokenizer.from_pretrained.use_fast",description:`<strong>use_fast</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to try to load the fast version of the tokenizer.`,name:"use_fast"},{anchor:"transformers.AutoTokenizer.from_pretrained.tokenizer_type",description:`<strong>tokenizer_type</strong> (<code>str</code>, <em>optional</em>) &#x2014;
Tokenizer type to be loaded.`,name:"tokenizer_type"},{anchor:"transformers.AutoTokenizer.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.AutoTokenizer.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Will be passed to the Tokenizer <code>__init__()</code> method. Can be used to set special tokens like
<code>bos_token</code>, <code>eos_token</code>, <code>unk_token</code>, <code>sep_token</code>, <code>pad_token</code>, <code>cls_token</code>, <code>mask_token</code>,
<code>additional_special_tokens</code>. See parameters in the <code>__init__()</code> for more details.`,name:"kwargs"}]}}),vE=new w({props:{code:`from transformers import AutoTokenizer

# Download vocabulary from huggingface.co and cache.
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

# Download vocabulary from huggingface.co (user-uploaded) and cache.
tokenizer = AutoTokenizer.from_pretrained("dbmdz/bert-base-german-cased")

# If vocabulary files are in a directory (e.g. tokenizer was saved using *save_pretrained('./test/saved_model/')*)
tokenizer = AutoTokenizer.from_pretrained("./test/bert_saved_model/"),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download vocabulary from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;bert-base-uncased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download vocabulary from huggingface.co (user-uploaded) and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;dbmdz/bert-base-german-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># If vocabulary files are in a directory (e.g. tokenizer was saved using *save_pretrained(&#x27;./test/saved_model/&#x27;)*)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;./test/bert_saved_model/&quot;</span>)`}}),TE=new E({props:{name:"register",anchor:"transformers.AutoTokenizer.register",parameters:[{name:"config_class",val:""},{name:"slow_tokenizer_class",val:" = None"},{name:"fast_tokenizer_class",val:" = None"}],source:"https://github.com/huggingface/transformers/blob/pr_15774/src/transformers/models/auto/tokenization_auto.py#L561",parametersDescription:[{anchor:"transformers.AutoTokenizer.register.config_class",description:`<strong>config_class</strong> (<a href="/docs/transformers/pr_15774/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The configuration corresponding to the model to register.`,name:"config_class"},{anchor:"transformers.AutoTokenizer.register.slow_tokenizer_class",description:`<strong>slow_tokenizer_class</strong> (<code>PretrainedTokenizer</code>, <em>optional</em>) &#x2014;
The slow tokenizer to register.`,name:"slow_tokenizer_class"},{anchor:"transformers.AutoTokenizer.register.slow_tokenizer_class",description:`<strong>slow_tokenizer_class</strong> (<code>PretrainedTokenizerFast</code>, <em>optional</em>) &#x2014;
The fast tokenizer to register.`,name:"slow_tokenizer_class"}]}}),FE=new z({}),CE=new E({props:{name:"class transformers.AutoFeatureExtractor",anchor:"transformers.AutoFeatureExtractor",parameters:[],source:"https://github.com/huggingface/transformers/blob/pr_15774/src/transformers/models/auto/feature_extraction_auto.py#L169"}}),yE=new E({props:{name:"from_pretrained",anchor:"transformers.AutoFeatureExtractor.from_pretrained",parameters:[{name:"pretrained_model_name_or_path",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15774/src/transformers/models/auto/feature_extraction_auto.py#L183",parametersDescription:[{anchor:"transformers.AutoFeatureExtractor.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
This can be either:</p>
<ul>
<li>a string, the <em>model id</em> of a pretrained feature_extractor hosted inside a model repo on
huggingface.co. Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or
namespaced under a user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>a path to a <em>directory</em> containing a feature extractor file saved using the
<a href="/docs/transformers/pr_15774/en/main_classes/feature_extractor#transformers.FeatureExtractionMixin.save_pretrained">save_pretrained()</a> method, e.g.,
<code>./my_model_directory/</code>.</li>
<li>a path or url to a saved feature extractor JSON <em>file</em>, e.g.,
<code>./my_model_directory/preprocessor_config.json</code>.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.AutoFeatureExtractor.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model feature extractor should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.AutoFeatureExtractor.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force to (re-)download the feature extractor files and override the cached versions
if they exist.`,name:"force_download"},{anchor:"transformers.AutoFeatureExtractor.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received file. Attempts to resume the download if such a file
exists.`,name:"resume_download"},{anchor:"transformers.AutoFeatureExtractor.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}.</code> The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.AutoFeatureExtractor.from_pretrained.use_auth_token",description:`<strong>use_auth_token</strong> (<code>str</code> or <em>bool</em>, <em>optional</em>) &#x2014;
The token to use as HTTP bearer authorization for remote files. If <code>True</code>, will use the token generated
when running <code>transformers-cli login</code> (stored in <code>~/.huggingface</code>).`,name:"use_auth_token"},{anchor:"transformers.AutoFeatureExtractor.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.AutoFeatureExtractor.from_pretrained.return_unused_kwargs",description:`<strong>return_unused_kwargs</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
If <code>False</code>, then this function returns just the final feature extractor object. If <code>True</code>, then this
functions returns a <code>Tuple(feature_extractor, unused_kwargs)</code> where <em>unused_kwargs</em> is a dictionary
consisting of the key/value pairs whose keys are not feature extractor attributes: i.e., the part of
<code>kwargs</code> which has not been used to update <code>feature_extractor</code> and is otherwise ignored.`,name:"return_unused_kwargs"},{anchor:"transformers.AutoFeatureExtractor.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.AutoFeatureExtractor.from_pretrained.kwargs",description:`<strong>kwargs</strong> (<code>Dict[str, Any]</code>, <em>optional</em>) &#x2014;
The values in kwargs of any keys which are feature extractor attributes will be used to override the
loaded values. Behavior concerning key/value pairs whose keys are <em>not</em> feature extractor attributes is
controlled by the <code>return_unused_kwargs</code> keyword parameter.`,name:"kwargs"}]}}),ih=new _wr({props:{$$slots:{default:[C_t]},$$scope:{ctx:yi}}}),wE=new w({props:{code:`from transformers import AutoFeatureExtractor

# Download feature extractor from huggingface.co and cache.
feature_extractor = AutoFeatureExtractor.from_pretrained("facebook/wav2vec2-base-960h")

# If feature extractor files are in a directory (e.g. feature extractor was saved using *save_pretrained('./test/saved_model/')*)
feature_extractor = AutoFeatureExtractor.from_pretrained("./test/saved_model/"),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoFeatureExtractor

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download feature extractor from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>feature_extractor = AutoFeatureExtractor.from_pretrained(<span class="hljs-string">&quot;facebook/wav2vec2-base-960h&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># If feature extractor files are in a directory (e.g. feature extractor was saved using *save_pretrained(&#x27;./test/saved_model/&#x27;)*)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>feature_extractor = AutoFeatureExtractor.from_pretrained(<span class="hljs-string">&quot;./test/saved_model/&quot;</span>)`}}),AE=new E({props:{name:"register",anchor:"transformers.AutoFeatureExtractor.register",parameters:[{name:"config_class",val:""},{name:"feature_extractor_class",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15774/src/transformers/models/auto/feature_extraction_auto.py#L310",parametersDescription:[{anchor:"transformers.AutoFeatureExtractor.register.config_class",description:`<strong>config_class</strong> (<a href="/docs/transformers/pr_15774/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The configuration corresponding to the model to register.`,name:"config_class"},{anchor:"transformers.AutoFeatureExtractor.register.feature_extractor_class",description:"<strong>feature_extractor_class</strong> (<code>FeatureExtractorMixin</code>) &#x2014; The feature extractor to register.",name:"feature_extractor_class"}]}}),LE=new z({}),BE=new E({props:{name:"class transformers.AutoProcessor",anchor:"transformers.AutoProcessor",parameters:[],source:"https://github.com/huggingface/transformers/blob/pr_15774/src/transformers/models/auto/processing_auto.py#L71"}}),RE=new E({props:{name:"from_pretrained",anchor:"transformers.AutoProcessor.from_pretrained",parameters:[{name:"pretrained_model_name_or_path",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15774/src/transformers/models/auto/processing_auto.py#L85",parametersDescription:[{anchor:"transformers.AutoProcessor.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
This can be either:</p>
<ul>
<li>a string, the <em>model id</em> of a pretrained feature_extractor hosted inside a model repo on
huggingface.co. Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or
namespaced under a user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>a path to a <em>directory</em> containing a processor files saved using the <code>save_pretrained()</code> method,
e.g., <code>./my_model_directory/</code>.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.AutoProcessor.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model feature extractor should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.AutoProcessor.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force to (re-)download the feature extractor files and override the cached versions
if they exist.`,name:"force_download"},{anchor:"transformers.AutoProcessor.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received file. Attempts to resume the download if such a file
exists.`,name:"resume_download"},{anchor:"transformers.AutoProcessor.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}.</code> The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.AutoProcessor.from_pretrained.use_auth_token",description:`<strong>use_auth_token</strong> (<code>str</code> or <em>bool</em>, <em>optional</em>) &#x2014;
The token to use as HTTP bearer authorization for remote files. If <code>True</code>, will use the token generated
when running <code>transformers-cli login</code> (stored in <code>~/.huggingface</code>).`,name:"use_auth_token"},{anchor:"transformers.AutoProcessor.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.AutoProcessor.from_pretrained.return_unused_kwargs",description:`<strong>return_unused_kwargs</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
If <code>False</code>, then this function returns just the final feature extractor object. If <code>True</code>, then this
functions returns a <code>Tuple(feature_extractor, unused_kwargs)</code> where <em>unused_kwargs</em> is a dictionary
consisting of the key/value pairs whose keys are not feature extractor attributes: i.e., the part of
<code>kwargs</code> which has not been used to update <code>feature_extractor</code> and is otherwise ignored.`,name:"return_unused_kwargs"},{anchor:"transformers.AutoProcessor.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.AutoProcessor.from_pretrained.kwargs",description:`<strong>kwargs</strong> (<code>Dict[str, Any]</code>, <em>optional</em>) &#x2014;
The values in kwargs of any keys which are feature extractor attributes will be used to override the
loaded values. Behavior concerning key/value pairs whose keys are <em>not</em> feature extractor attributes is
controlled by the <code>return_unused_kwargs</code> keyword parameter.`,name:"kwargs"}]}}),vh=new _wr({props:{$$slots:{default:[M_t]},$$scope:{ctx:yi}}}),SE=new w({props:{code:`from transformers import AutoProcessor

# Download processor from huggingface.co and cache.
processor = AutoProcessor.from_pretrained("facebook/wav2vec2-base-960h")

# If processor files are in a directory (e.g. processor was saved using *save_pretrained('./test/saved_model/')*)
processor = AutoProcessor.from_pretrained("./test/saved_model/"),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoProcessor

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download processor from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>processor = AutoProcessor.from_pretrained(<span class="hljs-string">&quot;facebook/wav2vec2-base-960h&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># If processor files are in a directory (e.g. processor was saved using *save_pretrained(&#x27;./test/saved_model/&#x27;)*)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>processor = AutoProcessor.from_pretrained(<span class="hljs-string">&quot;./test/saved_model/&quot;</span>)`}}),PE=new E({props:{name:"register",anchor:"transformers.AutoProcessor.register",parameters:[{name:"config_class",val:""},{name:"processor_class",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15774/src/transformers/models/auto/processing_auto.py#L238",parametersDescription:[{anchor:"transformers.AutoProcessor.register.config_class",description:`<strong>config_class</strong> (<a href="/docs/transformers/pr_15774/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The configuration corresponding to the model to register.`,name:"config_class"},{anchor:"transformers.AutoProcessor.register.processor_class",description:"<strong>processor_class</strong> (<code>FeatureExtractorMixin</code>) &#x2014; The processor to register.",name:"processor_class"}]}}),$E=new z({}),IE=new E({props:{name:"class transformers.AutoModel",anchor:"transformers.AutoModel",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15774/src/transformers/models/auto/modeling_auto.py#L672"}}),NE=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15774/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15774/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15774/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/albert#transformers.AlbertModel">AlbertModel</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/bart#transformers.BartModel">BartModel</a> (BART model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/beit#transformers.BeitConfig">BeitConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/beit#transformers.BeitModel">BeitModel</a> (BEiT model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/bert#transformers.BertModel">BertModel</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/bert-generation#transformers.BertGenerationConfig">BertGenerationConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/bert-generation#transformers.BertGenerationEncoder">BertGenerationEncoder</a> (Bert Generation model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/big_bird#transformers.BigBirdModel">BigBirdModel</a> (BigBird model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusConfig">BigBirdPegasusConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusModel">BigBirdPegasusModel</a> (BigBirdPegasus model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/blenderbot#transformers.BlenderbotConfig">BlenderbotConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/blenderbot#transformers.BlenderbotModel">BlenderbotModel</a> (Blenderbot model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/blenderbot-small#transformers.BlenderbotSmallConfig">BlenderbotSmallConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/blenderbot-small#transformers.BlenderbotSmallModel">BlenderbotSmallModel</a> (BlenderbotSmall model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/clip#transformers.CLIPConfig">CLIPConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/clip#transformers.CLIPModel">CLIPModel</a> (CLIP model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/ctrl#transformers.CTRLConfig">CTRLConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/ctrl#transformers.CTRLModel">CTRLModel</a> (CTRL model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/camembert#transformers.CamembertModel">CamembertModel</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/canine#transformers.CanineConfig">CanineConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/canine#transformers.CanineModel">CanineModel</a> (Canine model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/convbert#transformers.ConvBertModel">ConvBertModel</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/convnext#transformers.ConvNextConfig">ConvNextConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/convnext#transformers.ConvNextModel">ConvNextModel</a> (ConvNext model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/dpr#transformers.DPRConfig">DPRConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/dpr#transformers.DPRQuestionEncoder">DPRQuestionEncoder</a> (DPR model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/deberta#transformers.DebertaModel">DebertaModel</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/deberta-v2#transformers.DebertaV2Model">DebertaV2Model</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/deit#transformers.DeiTConfig">DeiTConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/deit#transformers.DeiTModel">DeiTModel</a> (DeiT model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/detr#transformers.DetrConfig">DetrConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/detr#transformers.DetrModel">DetrModel</a> (DETR model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/distilbert#transformers.DistilBertModel">DistilBertModel</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/electra#transformers.ElectraModel">ElectraModel</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/fnet#transformers.FNetConfig">FNetConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/fnet#transformers.FNetModel">FNetModel</a> (FNet model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/fsmt#transformers.FSMTConfig">FSMTConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/fsmt#transformers.FSMTModel">FSMTModel</a> (FairSeq Machine-Translation model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/flaubert#transformers.FlaubertModel">FlaubertModel</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/funnel#transformers.FunnelModel">FunnelModel</a> or <a href="/docs/transformers/pr_15774/en/model_doc/funnel#transformers.FunnelBaseModel">FunnelBaseModel</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/gpt2#transformers.GPT2Model">GPT2Model</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/gptj#transformers.GPTJConfig">GPTJConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/gptj#transformers.GPTJModel">GPTJModel</a> (GPT-J model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/gpt_neo#transformers.GPTNeoConfig">GPTNeoConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/gpt_neo#transformers.GPTNeoModel">GPTNeoModel</a> (GPT Neo model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/hubert#transformers.HubertConfig">HubertConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/hubert#transformers.HubertModel">HubertModel</a> (Hubert model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/ibert#transformers.IBertConfig">IBertConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/ibert#transformers.IBertModel">IBertModel</a> (I-BERT model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/imagegpt#transformers.ImageGPTConfig">ImageGPTConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/imagegpt#transformers.ImageGPTModel">ImageGPTModel</a> (ImageGPT model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/led#transformers.LEDConfig">LEDConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/led#transformers.LEDModel">LEDModel</a> (LED model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/layoutlm#transformers.LayoutLMModel">LayoutLMModel</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/layoutlmv2#transformers.LayoutLMv2Config">LayoutLMv2Config</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/layoutlmv2#transformers.LayoutLMv2Model">LayoutLMv2Model</a> (LayoutLMv2 model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/longformer#transformers.LongformerModel">LongformerModel</a> (Longformer model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/luke#transformers.LukeConfig">LukeConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/luke#transformers.LukeModel">LukeModel</a> (LUKE model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/lxmert#transformers.LxmertConfig">LxmertConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/lxmert#transformers.LxmertModel">LxmertModel</a> (LXMERT model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/m2m_100#transformers.M2M100Config">M2M100Config</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/m2m_100#transformers.M2M100Model">M2M100Model</a> (M2M100 model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/mbart#transformers.MBartModel">MBartModel</a> (mBART model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/mpnet#transformers.MPNetModel">MPNetModel</a> (MPNet model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/mt5#transformers.MT5Config">MT5Config</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/mt5#transformers.MT5Model">MT5Model</a> (mT5 model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/marian#transformers.MarianConfig">MarianConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/marian#transformers.MarianModel">MarianModel</a> (Marian model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/megatron-bert#transformers.MegatronBertConfig">MegatronBertConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/megatron-bert#transformers.MegatronBertModel">MegatronBertModel</a> (MegatronBert model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/mobilebert#transformers.MobileBertModel">MobileBertModel</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/nystromformer#transformers.NystromformerConfig">NystromformerConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/nystromformer#transformers.NystromformerModel">NystromformerModel</a> (Nystromformer model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig">OpenAIGPTConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/openai-gpt#transformers.OpenAIGPTModel">OpenAIGPTModel</a> (OpenAI GPT model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/plbart#transformers.PLBartConfig">PLBartConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/plbart#transformers.PLBartModel">PLBartModel</a> (PLBart model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/pegasus#transformers.PegasusConfig">PegasusConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/pegasus#transformers.PegasusModel">PegasusModel</a> (Pegasus model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/perceiver#transformers.PerceiverConfig">PerceiverConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/perceiver#transformers.PerceiverModel">PerceiverModel</a> (Perceiver model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/poolformer#transformers.PoolFormerConfig">PoolFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/poolformer#transformers.PoolFormerModel">PoolFormerModel</a> (PoolFormer model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/prophetnet#transformers.ProphetNetConfig">ProphetNetConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/prophetnet#transformers.ProphetNetModel">ProphetNetModel</a> (ProphetNet model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/qdqbert#transformers.QDQBertConfig">QDQBertConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/qdqbert#transformers.QDQBertModel">QDQBertModel</a> (QDQBert model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/reformer#transformers.ReformerConfig">ReformerConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/reformer#transformers.ReformerModel">ReformerModel</a> (Reformer model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/rembert#transformers.RemBertModel">RemBertModel</a> (RemBERT model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/retribert#transformers.RetriBertConfig">RetriBertConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/retribert#transformers.RetriBertModel">RetriBertModel</a> (RetriBERT model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/roformer#transformers.RoFormerModel">RoFormerModel</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/roberta#transformers.RobertaModel">RobertaModel</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/sew#transformers.SEWConfig">SEWConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/sew#transformers.SEWModel">SEWModel</a> (SEW model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/sew-d#transformers.SEWDConfig">SEWDConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/sew-d#transformers.SEWDModel">SEWDModel</a> (SEW-D model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/segformer#transformers.SegformerConfig">SegformerConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/segformer#transformers.SegformerModel">SegformerModel</a> (SegFormer model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/speech_to_text#transformers.Speech2TextConfig">Speech2TextConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/speech_to_text#transformers.Speech2TextModel">Speech2TextModel</a> (Speech2Text model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/splinter#transformers.SplinterConfig">SplinterConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/splinter#transformers.SplinterModel">SplinterModel</a> (Splinter model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/squeezebert#transformers.SqueezeBertConfig">SqueezeBertConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/squeezebert#transformers.SqueezeBertModel">SqueezeBertModel</a> (SqueezeBERT model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/swin#transformers.SwinConfig">SwinConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/swin#transformers.SwinModel">SwinModel</a> (Swin model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/t5#transformers.T5Config">T5Config</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/t5#transformers.T5Model">T5Model</a> (T5 model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/tapas#transformers.TapasModel">TapasModel</a> (TAPAS model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/transfo-xl#transformers.TransfoXLConfig">TransfoXLConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/transfo-xl#transformers.TransfoXLModel">TransfoXLModel</a> (Transformer-XL model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/unispeech#transformers.UniSpeechConfig">UniSpeechConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/unispeech#transformers.UniSpeechModel">UniSpeechModel</a> (UniSpeech model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/unispeech-sat#transformers.UniSpeechSatConfig">UniSpeechSatConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/unispeech-sat#transformers.UniSpeechSatModel">UniSpeechSatModel</a> (UniSpeechSat model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/vit#transformers.ViTConfig">ViTConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/vit#transformers.ViTModel">ViTModel</a> (ViT model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/vit_mae#transformers.ViTMAEConfig">ViTMAEConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/vit_mae#transformers.ViTMAEModel">ViTMAEModel</a> (ViTMAE model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/vilt#transformers.ViltConfig">ViltConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/vilt#transformers.ViltModel">ViltModel</a> (ViLT model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/vision-text-dual-encoder#transformers.VisionTextDualEncoderConfig">VisionTextDualEncoderConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/vision-text-dual-encoder#transformers.VisionTextDualEncoderModel">VisionTextDualEncoderModel</a> (VisionTextDualEncoder model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/visual_bert#transformers.VisualBertConfig">VisualBertConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/visual_bert#transformers.VisualBertModel">VisualBertModel</a> (VisualBert model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/wav2vec2#transformers.Wav2Vec2Model">Wav2Vec2Model</a> (Wav2Vec2 model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/wavlm#transformers.WavLMConfig">WavLMConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/wavlm#transformers.WavLMModel">WavLMModel</a> (WavLM model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/xglm#transformers.XGLMConfig">XGLMConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/xglm#transformers.XGLMModel">XGLMModel</a> (XGLM model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/xlm#transformers.XLMModel">XLMModel</a> (XLM model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetConfig">XLMProphetNetConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetModel">XLMProphetNetModel</a> (XLMProphetNet model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/xlm-roberta#transformers.XLMRobertaModel">XLMRobertaModel</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLConfig">XLMRobertaXLConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLModel">XLMRobertaXLModel</a> (XLM-RoBERTa-XL model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/xlnet#transformers.XLNetModel">XLNetModel</a> (XLNet model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/yoso#transformers.YosoConfig">YosoConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/yoso#transformers.YosoModel">YosoModel</a> (YOSO model)</li>
</ul>`,name:"config"}]}}),DE=new w({props:{code:`from transformers import AutoConfig, AutoModel

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModel.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModel.from_config(config)`}}),qE=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15774/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15774/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15774/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15774/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15774/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15774/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15774/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),GE=new w({props:{code:`from transformers import AutoConfig, AutoModel

# Download model and configuration from huggingface.co and cache.
model = AutoModel.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModel.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModel.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModel.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModel.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModel.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),OE=new z({}),XE=new E({props:{name:"class transformers.AutoModelForPreTraining",anchor:"transformers.AutoModelForPreTraining",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15774/src/transformers/models/auto/modeling_auto.py#L679"}}),VE=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15774/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15774/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15774/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/albert#transformers.AlbertForPreTraining">AlbertForPreTraining</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/bart#transformers.BartForConditionalGeneration">BartForConditionalGeneration</a> (BART model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/bert#transformers.BertForPreTraining">BertForPreTraining</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/big_bird#transformers.BigBirdForPreTraining">BigBirdForPreTraining</a> (BigBird model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/ctrl#transformers.CTRLConfig">CTRLConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/ctrl#transformers.CTRLLMHeadModel">CTRLLMHeadModel</a> (CTRL model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/camembert#transformers.CamembertForMaskedLM">CamembertForMaskedLM</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/deberta#transformers.DebertaForMaskedLM">DebertaForMaskedLM</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/deberta-v2#transformers.DebertaV2ForMaskedLM">DebertaV2ForMaskedLM</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/distilbert#transformers.DistilBertForMaskedLM">DistilBertForMaskedLM</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/electra#transformers.ElectraForPreTraining">ElectraForPreTraining</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/fnet#transformers.FNetConfig">FNetConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/fnet#transformers.FNetForPreTraining">FNetForPreTraining</a> (FNet model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/fsmt#transformers.FSMTConfig">FSMTConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/fsmt#transformers.FSMTForConditionalGeneration">FSMTForConditionalGeneration</a> (FairSeq Machine-Translation model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/flaubert#transformers.FlaubertWithLMHeadModel">FlaubertWithLMHeadModel</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/funnel#transformers.FunnelForPreTraining">FunnelForPreTraining</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/gpt2#transformers.GPT2LMHeadModel">GPT2LMHeadModel</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/ibert#transformers.IBertConfig">IBertConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/ibert#transformers.IBertForMaskedLM">IBertForMaskedLM</a> (I-BERT model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/layoutlm#transformers.LayoutLMForMaskedLM">LayoutLMForMaskedLM</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/longformer#transformers.LongformerForMaskedLM">LongformerForMaskedLM</a> (Longformer model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/lxmert#transformers.LxmertConfig">LxmertConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/lxmert#transformers.LxmertForPreTraining">LxmertForPreTraining</a> (LXMERT model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/mpnet#transformers.MPNetForMaskedLM">MPNetForMaskedLM</a> (MPNet model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/megatron-bert#transformers.MegatronBertConfig">MegatronBertConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/megatron-bert#transformers.MegatronBertForPreTraining">MegatronBertForPreTraining</a> (MegatronBert model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/mobilebert#transformers.MobileBertForPreTraining">MobileBertForPreTraining</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig">OpenAIGPTConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/openai-gpt#transformers.OpenAIGPTLMHeadModel">OpenAIGPTLMHeadModel</a> (OpenAI GPT model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/retribert#transformers.RetriBertConfig">RetriBertConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/retribert#transformers.RetriBertModel">RetriBertModel</a> (RetriBERT model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/roberta#transformers.RobertaForMaskedLM">RobertaForMaskedLM</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/squeezebert#transformers.SqueezeBertConfig">SqueezeBertConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/squeezebert#transformers.SqueezeBertForMaskedLM">SqueezeBertForMaskedLM</a> (SqueezeBERT model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/t5#transformers.T5Config">T5Config</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/t5#transformers.T5ForConditionalGeneration">T5ForConditionalGeneration</a> (T5 model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/tapas#transformers.TapasForMaskedLM">TapasForMaskedLM</a> (TAPAS model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/transfo-xl#transformers.TransfoXLConfig">TransfoXLConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/transfo-xl#transformers.TransfoXLLMHeadModel">TransfoXLLMHeadModel</a> (Transformer-XL model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/unispeech#transformers.UniSpeechConfig">UniSpeechConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/unispeech#transformers.UniSpeechForPreTraining">UniSpeechForPreTraining</a> (UniSpeech model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/unispeech-sat#transformers.UniSpeechSatConfig">UniSpeechSatConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/unispeech-sat#transformers.UniSpeechSatForPreTraining">UniSpeechSatForPreTraining</a> (UniSpeechSat model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/vit_mae#transformers.ViTMAEConfig">ViTMAEConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/vit_mae#transformers.ViTMAEForPreTraining">ViTMAEForPreTraining</a> (ViTMAE model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/visual_bert#transformers.VisualBertConfig">VisualBertConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/visual_bert#transformers.VisualBertForPreTraining">VisualBertForPreTraining</a> (VisualBert model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/wav2vec2#transformers.Wav2Vec2ForPreTraining">Wav2Vec2ForPreTraining</a> (Wav2Vec2 model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/xlm#transformers.XLMWithLMHeadModel">XLMWithLMHeadModel</a> (XLM model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/xlm-roberta#transformers.XLMRobertaForMaskedLM">XLMRobertaForMaskedLM</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLConfig">XLMRobertaXLConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForMaskedLM">XLMRobertaXLForMaskedLM</a> (XLM-RoBERTa-XL model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/xlnet#transformers.XLNetLMHeadModel">XLNetLMHeadModel</a> (XLNet model)</li>
</ul>`,name:"config"}]}}),WE=new w({props:{code:`from transformers import AutoConfig, AutoModelForPreTraining

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForPreTraining.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForPreTraining

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForPreTraining.from_config(config)`}}),QE=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15774/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15774/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15774/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15774/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15774/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15774/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15774/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),HE=new w({props:{code:`from transformers import AutoConfig, AutoModelForPreTraining

# Download model and configuration from huggingface.co and cache.
model = AutoModelForPreTraining.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForPreTraining.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForPreTraining.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForPreTraining

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForPreTraining.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForPreTraining.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForPreTraining.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),UE=new z({}),JE=new E({props:{name:"class transformers.AutoModelForCausalLM",anchor:"transformers.AutoModelForCausalLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15774/src/transformers/models/auto/modeling_auto.py#L694"}}),KE=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15774/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15774/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15774/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/bart#transformers.BartForCausalLM">BartForCausalLM</a> (BART model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/bert#transformers.BertLMHeadModel">BertLMHeadModel</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/bert-generation#transformers.BertGenerationConfig">BertGenerationConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/bert-generation#transformers.BertGenerationDecoder">BertGenerationDecoder</a> (Bert Generation model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/big_bird#transformers.BigBirdForCausalLM">BigBirdForCausalLM</a> (BigBird model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusConfig">BigBirdPegasusConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusForCausalLM">BigBirdPegasusForCausalLM</a> (BigBirdPegasus model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/blenderbot#transformers.BlenderbotConfig">BlenderbotConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/blenderbot#transformers.BlenderbotForCausalLM">BlenderbotForCausalLM</a> (Blenderbot model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/blenderbot-small#transformers.BlenderbotSmallConfig">BlenderbotSmallConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/blenderbot-small#transformers.BlenderbotSmallForCausalLM">BlenderbotSmallForCausalLM</a> (BlenderbotSmall model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/ctrl#transformers.CTRLConfig">CTRLConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/ctrl#transformers.CTRLLMHeadModel">CTRLLMHeadModel</a> (CTRL model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/camembert#transformers.CamembertForCausalLM">CamembertForCausalLM</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/electra#transformers.ElectraForCausalLM">ElectraForCausalLM</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/gpt2#transformers.GPT2LMHeadModel">GPT2LMHeadModel</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/gptj#transformers.GPTJConfig">GPTJConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/gptj#transformers.GPTJForCausalLM">GPTJForCausalLM</a> (GPT-J model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/gpt_neo#transformers.GPTNeoConfig">GPTNeoConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/gpt_neo#transformers.GPTNeoForCausalLM">GPTNeoForCausalLM</a> (GPT Neo model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/mbart#transformers.MBartForCausalLM">MBartForCausalLM</a> (mBART model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/marian#transformers.MarianConfig">MarianConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/marian#transformers.MarianForCausalLM">MarianForCausalLM</a> (Marian model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/megatron-bert#transformers.MegatronBertConfig">MegatronBertConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/megatron-bert#transformers.MegatronBertForCausalLM">MegatronBertForCausalLM</a> (MegatronBert model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig">OpenAIGPTConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/openai-gpt#transformers.OpenAIGPTLMHeadModel">OpenAIGPTLMHeadModel</a> (OpenAI GPT model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/plbart#transformers.PLBartConfig">PLBartConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/plbart#transformers.PLBartForCausalLM">PLBartForCausalLM</a> (PLBart model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/pegasus#transformers.PegasusConfig">PegasusConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/pegasus#transformers.PegasusForCausalLM">PegasusForCausalLM</a> (Pegasus model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/prophetnet#transformers.ProphetNetConfig">ProphetNetConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/prophetnet#transformers.ProphetNetForCausalLM">ProphetNetForCausalLM</a> (ProphetNet model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/qdqbert#transformers.QDQBertConfig">QDQBertConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/qdqbert#transformers.QDQBertLMHeadModel">QDQBertLMHeadModel</a> (QDQBert model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/reformer#transformers.ReformerConfig">ReformerConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/reformer#transformers.ReformerModelWithLMHead">ReformerModelWithLMHead</a> (Reformer model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/rembert#transformers.RemBertForCausalLM">RemBertForCausalLM</a> (RemBERT model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/roformer#transformers.RoFormerForCausalLM">RoFormerForCausalLM</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/roberta#transformers.RobertaForCausalLM">RobertaForCausalLM</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/speech_to_text_2#transformers.Speech2Text2Config">Speech2Text2Config</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/speech_to_text_2#transformers.Speech2Text2ForCausalLM">Speech2Text2ForCausalLM</a> (Speech2Text2 model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/trocr#transformers.TrOCRConfig">TrOCRConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/trocr#transformers.TrOCRForCausalLM">TrOCRForCausalLM</a> (TrOCR model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/transfo-xl#transformers.TransfoXLConfig">TransfoXLConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/transfo-xl#transformers.TransfoXLLMHeadModel">TransfoXLLMHeadModel</a> (Transformer-XL model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/xglm#transformers.XGLMConfig">XGLMConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/xglm#transformers.XGLMForCausalLM">XGLMForCausalLM</a> (XGLM model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/xlm#transformers.XLMWithLMHeadModel">XLMWithLMHeadModel</a> (XLM model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetConfig">XLMProphetNetConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetForCausalLM">XLMProphetNetForCausalLM</a> (XLMProphetNet model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/xlm-roberta#transformers.XLMRobertaForCausalLM">XLMRobertaForCausalLM</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLConfig">XLMRobertaXLConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForCausalLM">XLMRobertaXLForCausalLM</a> (XLM-RoBERTa-XL model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/xlnet#transformers.XLNetLMHeadModel">XLNetLMHeadModel</a> (XLNet model)</li>
</ul>`,name:"config"}]}}),ZE=new w({props:{code:`from transformers import AutoConfig, AutoModelForCausalLM

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForCausalLM.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForCausalLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCausalLM.from_config(config)`}}),e3=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15774/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15774/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15774/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15774/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15774/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15774/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15774/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),o3=new w({props:{code:`from transformers import AutoConfig, AutoModelForCausalLM

# Download model and configuration from huggingface.co and cache.
model = AutoModelForCausalLM.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForCausalLM.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForCausalLM.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForCausalLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCausalLM.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),r3=new z({}),t3=new E({props:{name:"class transformers.AutoModelForMaskedLM",anchor:"transformers.AutoModelForMaskedLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15774/src/transformers/models/auto/modeling_auto.py#L701"}}),n3=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15774/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15774/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15774/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/albert#transformers.AlbertForMaskedLM">AlbertForMaskedLM</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/bart#transformers.BartForConditionalGeneration">BartForConditionalGeneration</a> (BART model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/bert#transformers.BertForMaskedLM">BertForMaskedLM</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/big_bird#transformers.BigBirdForMaskedLM">BigBirdForMaskedLM</a> (BigBird model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/camembert#transformers.CamembertForMaskedLM">CamembertForMaskedLM</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/convbert#transformers.ConvBertForMaskedLM">ConvBertForMaskedLM</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/deberta#transformers.DebertaForMaskedLM">DebertaForMaskedLM</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/deberta-v2#transformers.DebertaV2ForMaskedLM">DebertaV2ForMaskedLM</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/distilbert#transformers.DistilBertForMaskedLM">DistilBertForMaskedLM</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/electra#transformers.ElectraForMaskedLM">ElectraForMaskedLM</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/fnet#transformers.FNetConfig">FNetConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/fnet#transformers.FNetForMaskedLM">FNetForMaskedLM</a> (FNet model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/flaubert#transformers.FlaubertWithLMHeadModel">FlaubertWithLMHeadModel</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/funnel#transformers.FunnelForMaskedLM">FunnelForMaskedLM</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/ibert#transformers.IBertConfig">IBertConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/ibert#transformers.IBertForMaskedLM">IBertForMaskedLM</a> (I-BERT model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/layoutlm#transformers.LayoutLMForMaskedLM">LayoutLMForMaskedLM</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/longformer#transformers.LongformerForMaskedLM">LongformerForMaskedLM</a> (Longformer model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/mbart#transformers.MBartForConditionalGeneration">MBartForConditionalGeneration</a> (mBART model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/mpnet#transformers.MPNetForMaskedLM">MPNetForMaskedLM</a> (MPNet model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/megatron-bert#transformers.MegatronBertConfig">MegatronBertConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/megatron-bert#transformers.MegatronBertForMaskedLM">MegatronBertForMaskedLM</a> (MegatronBert model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/mobilebert#transformers.MobileBertForMaskedLM">MobileBertForMaskedLM</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/nystromformer#transformers.NystromformerConfig">NystromformerConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/nystromformer#transformers.NystromformerForMaskedLM">NystromformerForMaskedLM</a> (Nystromformer model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/perceiver#transformers.PerceiverConfig">PerceiverConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/perceiver#transformers.PerceiverForMaskedLM">PerceiverForMaskedLM</a> (Perceiver model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/qdqbert#transformers.QDQBertConfig">QDQBertConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/qdqbert#transformers.QDQBertForMaskedLM">QDQBertForMaskedLM</a> (QDQBert model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/reformer#transformers.ReformerConfig">ReformerConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/reformer#transformers.ReformerForMaskedLM">ReformerForMaskedLM</a> (Reformer model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/rembert#transformers.RemBertForMaskedLM">RemBertForMaskedLM</a> (RemBERT model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/roformer#transformers.RoFormerForMaskedLM">RoFormerForMaskedLM</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/roberta#transformers.RobertaForMaskedLM">RobertaForMaskedLM</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/squeezebert#transformers.SqueezeBertConfig">SqueezeBertConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/squeezebert#transformers.SqueezeBertForMaskedLM">SqueezeBertForMaskedLM</a> (SqueezeBERT model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/tapas#transformers.TapasForMaskedLM">TapasForMaskedLM</a> (TAPAS model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <code>Wav2Vec2ForMaskedLM</code>(Wav2Vec2 model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/xlm#transformers.XLMWithLMHeadModel">XLMWithLMHeadModel</a> (XLM model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/xlm-roberta#transformers.XLMRobertaForMaskedLM">XLMRobertaForMaskedLM</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLConfig">XLMRobertaXLConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForMaskedLM">XLMRobertaXLForMaskedLM</a> (XLM-RoBERTa-XL model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/yoso#transformers.YosoConfig">YosoConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/yoso#transformers.YosoForMaskedLM">YosoForMaskedLM</a> (YOSO model)</li>
</ul>`,name:"config"}]}}),s3=new w({props:{code:`from transformers import AutoConfig, AutoModelForMaskedLM

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForMaskedLM.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForMaskedLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMaskedLM.from_config(config)`}}),l3=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15774/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15774/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15774/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15774/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15774/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15774/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15774/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),i3=new w({props:{code:`from transformers import AutoConfig, AutoModelForMaskedLM

# Download model and configuration from huggingface.co and cache.
model = AutoModelForMaskedLM.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForMaskedLM.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForMaskedLM.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForMaskedLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMaskedLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMaskedLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMaskedLM.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),d3=new z({}),c3=new E({props:{name:"class transformers.AutoModelForSeq2SeqLM",anchor:"transformers.AutoModelForSeq2SeqLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15774/src/transformers/models/auto/modeling_auto.py#L708"}}),m3=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15774/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15774/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15774/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/bart#transformers.BartForConditionalGeneration">BartForConditionalGeneration</a> (BART model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusConfig">BigBirdPegasusConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusForConditionalGeneration">BigBirdPegasusForConditionalGeneration</a> (BigBirdPegasus model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/blenderbot#transformers.BlenderbotConfig">BlenderbotConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/blenderbot#transformers.BlenderbotForConditionalGeneration">BlenderbotForConditionalGeneration</a> (Blenderbot model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/blenderbot-small#transformers.BlenderbotSmallConfig">BlenderbotSmallConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/blenderbot-small#transformers.BlenderbotSmallForConditionalGeneration">BlenderbotSmallForConditionalGeneration</a> (BlenderbotSmall model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/encoder-decoder#transformers.EncoderDecoderConfig">EncoderDecoderConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/encoder-decoder#transformers.EncoderDecoderModel">EncoderDecoderModel</a> (Encoder decoder model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/fsmt#transformers.FSMTConfig">FSMTConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/fsmt#transformers.FSMTForConditionalGeneration">FSMTForConditionalGeneration</a> (FairSeq Machine-Translation model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/led#transformers.LEDConfig">LEDConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/led#transformers.LEDForConditionalGeneration">LEDForConditionalGeneration</a> (LED model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/m2m_100#transformers.M2M100Config">M2M100Config</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/m2m_100#transformers.M2M100ForConditionalGeneration">M2M100ForConditionalGeneration</a> (M2M100 model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/mbart#transformers.MBartForConditionalGeneration">MBartForConditionalGeneration</a> (mBART model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/mt5#transformers.MT5Config">MT5Config</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/mt5#transformers.MT5ForConditionalGeneration">MT5ForConditionalGeneration</a> (mT5 model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/marian#transformers.MarianConfig">MarianConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/marian#transformers.MarianMTModel">MarianMTModel</a> (Marian model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/plbart#transformers.PLBartConfig">PLBartConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/plbart#transformers.PLBartForConditionalGeneration">PLBartForConditionalGeneration</a> (PLBart model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/pegasus#transformers.PegasusConfig">PegasusConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/pegasus#transformers.PegasusForConditionalGeneration">PegasusForConditionalGeneration</a> (Pegasus model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/prophetnet#transformers.ProphetNetConfig">ProphetNetConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/prophetnet#transformers.ProphetNetForConditionalGeneration">ProphetNetForConditionalGeneration</a> (ProphetNet model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/t5#transformers.T5Config">T5Config</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/t5#transformers.T5ForConditionalGeneration">T5ForConditionalGeneration</a> (T5 model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetConfig">XLMProphetNetConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetForConditionalGeneration">XLMProphetNetForConditionalGeneration</a> (XLMProphetNet model)</li>
</ul>`,name:"config"}]}}),g3=new w({props:{code:`from transformers import AutoConfig, AutoModelForSeq2SeqLM

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("t5-base")
model = AutoModelForSeq2SeqLM.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForSeq2SeqLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;t5-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSeq2SeqLM.from_config(config)`}}),h3=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15774/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15774/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15774/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15774/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15774/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15774/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15774/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),p3=new w({props:{code:`from transformers import AutoConfig, AutoModelForSeq2SeqLM

# Download model and configuration from huggingface.co and cache.
model = AutoModelForSeq2SeqLM.from_pretrained("t5-base")

# Update configuration during loading
model = AutoModelForSeq2SeqLM.from_pretrained("t5-base", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/t5_tf_model_config.json")
model = AutoModelForSeq2SeqLM.from_pretrained(
    "./tf_model/t5_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForSeq2SeqLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSeq2SeqLM.from_pretrained(<span class="hljs-string">&quot;t5-base&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSeq2SeqLM.from_pretrained(<span class="hljs-string">&quot;t5-base&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/t5_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSeq2SeqLM.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/t5_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),_3=new z({}),u3=new E({props:{name:"class transformers.AutoModelForSequenceClassification",anchor:"transformers.AutoModelForSequenceClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15774/src/transformers/models/auto/modeling_auto.py#L717"}}),v3=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15774/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15774/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15774/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/albert#transformers.AlbertForSequenceClassification">AlbertForSequenceClassification</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/bart#transformers.BartForSequenceClassification">BartForSequenceClassification</a> (BART model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/bert#transformers.BertForSequenceClassification">BertForSequenceClassification</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/big_bird#transformers.BigBirdForSequenceClassification">BigBirdForSequenceClassification</a> (BigBird model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusConfig">BigBirdPegasusConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusForSequenceClassification">BigBirdPegasusForSequenceClassification</a> (BigBirdPegasus model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/ctrl#transformers.CTRLConfig">CTRLConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/ctrl#transformers.CTRLForSequenceClassification">CTRLForSequenceClassification</a> (CTRL model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/camembert#transformers.CamembertForSequenceClassification">CamembertForSequenceClassification</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/canine#transformers.CanineConfig">CanineConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/canine#transformers.CanineForSequenceClassification">CanineForSequenceClassification</a> (Canine model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/convbert#transformers.ConvBertForSequenceClassification">ConvBertForSequenceClassification</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/deberta#transformers.DebertaForSequenceClassification">DebertaForSequenceClassification</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/deberta-v2#transformers.DebertaV2ForSequenceClassification">DebertaV2ForSequenceClassification</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/distilbert#transformers.DistilBertForSequenceClassification">DistilBertForSequenceClassification</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/electra#transformers.ElectraForSequenceClassification">ElectraForSequenceClassification</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/fnet#transformers.FNetConfig">FNetConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/fnet#transformers.FNetForSequenceClassification">FNetForSequenceClassification</a> (FNet model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/flaubert#transformers.FlaubertForSequenceClassification">FlaubertForSequenceClassification</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/funnel#transformers.FunnelForSequenceClassification">FunnelForSequenceClassification</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/gpt2#transformers.GPT2ForSequenceClassification">GPT2ForSequenceClassification</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/gptj#transformers.GPTJConfig">GPTJConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/gptj#transformers.GPTJForSequenceClassification">GPTJForSequenceClassification</a> (GPT-J model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/gpt_neo#transformers.GPTNeoConfig">GPTNeoConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/gpt_neo#transformers.GPTNeoForSequenceClassification">GPTNeoForSequenceClassification</a> (GPT Neo model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/ibert#transformers.IBertConfig">IBertConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/ibert#transformers.IBertForSequenceClassification">IBertForSequenceClassification</a> (I-BERT model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/led#transformers.LEDConfig">LEDConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/led#transformers.LEDForSequenceClassification">LEDForSequenceClassification</a> (LED model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/layoutlm#transformers.LayoutLMForSequenceClassification">LayoutLMForSequenceClassification</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/layoutlmv2#transformers.LayoutLMv2Config">LayoutLMv2Config</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/layoutlmv2#transformers.LayoutLMv2ForSequenceClassification">LayoutLMv2ForSequenceClassification</a> (LayoutLMv2 model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/longformer#transformers.LongformerForSequenceClassification">LongformerForSequenceClassification</a> (Longformer model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/mbart#transformers.MBartForSequenceClassification">MBartForSequenceClassification</a> (mBART model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/mpnet#transformers.MPNetForSequenceClassification">MPNetForSequenceClassification</a> (MPNet model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/megatron-bert#transformers.MegatronBertConfig">MegatronBertConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/megatron-bert#transformers.MegatronBertForSequenceClassification">MegatronBertForSequenceClassification</a> (MegatronBert model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/mobilebert#transformers.MobileBertForSequenceClassification">MobileBertForSequenceClassification</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/nystromformer#transformers.NystromformerConfig">NystromformerConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/nystromformer#transformers.NystromformerForSequenceClassification">NystromformerForSequenceClassification</a> (Nystromformer model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig">OpenAIGPTConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/openai-gpt#transformers.OpenAIGPTForSequenceClassification">OpenAIGPTForSequenceClassification</a> (OpenAI GPT model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/plbart#transformers.PLBartConfig">PLBartConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/plbart#transformers.PLBartForSequenceClassification">PLBartForSequenceClassification</a> (PLBart model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/perceiver#transformers.PerceiverConfig">PerceiverConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/perceiver#transformers.PerceiverForSequenceClassification">PerceiverForSequenceClassification</a> (Perceiver model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/qdqbert#transformers.QDQBertConfig">QDQBertConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/qdqbert#transformers.QDQBertForSequenceClassification">QDQBertForSequenceClassification</a> (QDQBert model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/reformer#transformers.ReformerConfig">ReformerConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/reformer#transformers.ReformerForSequenceClassification">ReformerForSequenceClassification</a> (Reformer model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/rembert#transformers.RemBertForSequenceClassification">RemBertForSequenceClassification</a> (RemBERT model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/roformer#transformers.RoFormerForSequenceClassification">RoFormerForSequenceClassification</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/roberta#transformers.RobertaForSequenceClassification">RobertaForSequenceClassification</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/squeezebert#transformers.SqueezeBertConfig">SqueezeBertConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/squeezebert#transformers.SqueezeBertForSequenceClassification">SqueezeBertForSequenceClassification</a> (SqueezeBERT model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/tapas#transformers.TapasForSequenceClassification">TapasForSequenceClassification</a> (TAPAS model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/transfo-xl#transformers.TransfoXLConfig">TransfoXLConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/transfo-xl#transformers.TransfoXLForSequenceClassification">TransfoXLForSequenceClassification</a> (Transformer-XL model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/xlm#transformers.XLMForSequenceClassification">XLMForSequenceClassification</a> (XLM model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/xlm-roberta#transformers.XLMRobertaForSequenceClassification">XLMRobertaForSequenceClassification</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLConfig">XLMRobertaXLConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForSequenceClassification">XLMRobertaXLForSequenceClassification</a> (XLM-RoBERTa-XL model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/xlnet#transformers.XLNetForSequenceClassification">XLNetForSequenceClassification</a> (XLNet model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/yoso#transformers.YosoConfig">YosoConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/yoso#transformers.YosoForSequenceClassification">YosoForSequenceClassification</a> (YOSO model)</li>
</ul>`,name:"config"}]}}),T3=new w({props:{code:`from transformers import AutoConfig, AutoModelForSequenceClassification

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForSequenceClassification.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSequenceClassification.from_config(config)`}}),F3=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15774/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15774/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15774/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15774/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15774/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15774/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15774/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),C3=new w({props:{code:`from transformers import AutoConfig, AutoModelForSequenceClassification

# Download model and configuration from huggingface.co and cache.
model = AutoModelForSequenceClassification.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForSequenceClassification.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForSequenceClassification.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSequenceClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),M3=new z({}),E3=new E({props:{name:"class transformers.AutoModelForMultipleChoice",anchor:"transformers.AutoModelForMultipleChoice",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15774/src/transformers/models/auto/modeling_auto.py#L751"}}),w3=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15774/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15774/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15774/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/albert#transformers.AlbertForMultipleChoice">AlbertForMultipleChoice</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/bert#transformers.BertForMultipleChoice">BertForMultipleChoice</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/big_bird#transformers.BigBirdForMultipleChoice">BigBirdForMultipleChoice</a> (BigBird model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/camembert#transformers.CamembertForMultipleChoice">CamembertForMultipleChoice</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/canine#transformers.CanineConfig">CanineConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/canine#transformers.CanineForMultipleChoice">CanineForMultipleChoice</a> (Canine model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/convbert#transformers.ConvBertForMultipleChoice">ConvBertForMultipleChoice</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/distilbert#transformers.DistilBertForMultipleChoice">DistilBertForMultipleChoice</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/electra#transformers.ElectraForMultipleChoice">ElectraForMultipleChoice</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/fnet#transformers.FNetConfig">FNetConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/fnet#transformers.FNetForMultipleChoice">FNetForMultipleChoice</a> (FNet model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/flaubert#transformers.FlaubertForMultipleChoice">FlaubertForMultipleChoice</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/funnel#transformers.FunnelForMultipleChoice">FunnelForMultipleChoice</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/ibert#transformers.IBertConfig">IBertConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/ibert#transformers.IBertForMultipleChoice">IBertForMultipleChoice</a> (I-BERT model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/longformer#transformers.LongformerForMultipleChoice">LongformerForMultipleChoice</a> (Longformer model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/mpnet#transformers.MPNetForMultipleChoice">MPNetForMultipleChoice</a> (MPNet model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/megatron-bert#transformers.MegatronBertConfig">MegatronBertConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/megatron-bert#transformers.MegatronBertForMultipleChoice">MegatronBertForMultipleChoice</a> (MegatronBert model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/mobilebert#transformers.MobileBertForMultipleChoice">MobileBertForMultipleChoice</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/nystromformer#transformers.NystromformerConfig">NystromformerConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/nystromformer#transformers.NystromformerForMultipleChoice">NystromformerForMultipleChoice</a> (Nystromformer model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/qdqbert#transformers.QDQBertConfig">QDQBertConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/qdqbert#transformers.QDQBertForMultipleChoice">QDQBertForMultipleChoice</a> (QDQBert model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/rembert#transformers.RemBertForMultipleChoice">RemBertForMultipleChoice</a> (RemBERT model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/roformer#transformers.RoFormerForMultipleChoice">RoFormerForMultipleChoice</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/roberta#transformers.RobertaForMultipleChoice">RobertaForMultipleChoice</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/squeezebert#transformers.SqueezeBertConfig">SqueezeBertConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/squeezebert#transformers.SqueezeBertForMultipleChoice">SqueezeBertForMultipleChoice</a> (SqueezeBERT model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/xlm#transformers.XLMForMultipleChoice">XLMForMultipleChoice</a> (XLM model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/xlm-roberta#transformers.XLMRobertaForMultipleChoice">XLMRobertaForMultipleChoice</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLConfig">XLMRobertaXLConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForMultipleChoice">XLMRobertaXLForMultipleChoice</a> (XLM-RoBERTa-XL model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/xlnet#transformers.XLNetForMultipleChoice">XLNetForMultipleChoice</a> (XLNet model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/yoso#transformers.YosoConfig">YosoConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/yoso#transformers.YosoForMultipleChoice">YosoForMultipleChoice</a> (YOSO model)</li>
</ul>`,name:"config"}]}}),A3=new w({props:{code:`from transformers import AutoConfig, AutoModelForMultipleChoice

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForMultipleChoice.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForMultipleChoice

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMultipleChoice.from_config(config)`}}),L3=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15774/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15774/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15774/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15774/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15774/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15774/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15774/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),B3=new w({props:{code:`from transformers import AutoConfig, AutoModelForMultipleChoice

# Download model and configuration from huggingface.co and cache.
model = AutoModelForMultipleChoice.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForMultipleChoice.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForMultipleChoice.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForMultipleChoice

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMultipleChoice.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMultipleChoice.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMultipleChoice.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),k3=new z({}),x3=new E({props:{name:"class transformers.AutoModelForNextSentencePrediction",anchor:"transformers.AutoModelForNextSentencePrediction",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15774/src/transformers/models/auto/modeling_auto.py#L758"}}),S3=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15774/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15774/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15774/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/bert#transformers.BertForNextSentencePrediction">BertForNextSentencePrediction</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/fnet#transformers.FNetConfig">FNetConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/fnet#transformers.FNetForNextSentencePrediction">FNetForNextSentencePrediction</a> (FNet model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/megatron-bert#transformers.MegatronBertConfig">MegatronBertConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/megatron-bert#transformers.MegatronBertForNextSentencePrediction">MegatronBertForNextSentencePrediction</a> (MegatronBert model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/mobilebert#transformers.MobileBertForNextSentencePrediction">MobileBertForNextSentencePrediction</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/qdqbert#transformers.QDQBertConfig">QDQBertConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/qdqbert#transformers.QDQBertForNextSentencePrediction">QDQBertForNextSentencePrediction</a> (QDQBert model)</li>
</ul>`,name:"config"}]}}),P3=new w({props:{code:`from transformers import AutoConfig, AutoModelForNextSentencePrediction

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForNextSentencePrediction.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForNextSentencePrediction

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForNextSentencePrediction.from_config(config)`}}),$3=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15774/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15774/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15774/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15774/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15774/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15774/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15774/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),I3=new w({props:{code:`from transformers import AutoConfig, AutoModelForNextSentencePrediction

# Download model and configuration from huggingface.co and cache.
model = AutoModelForNextSentencePrediction.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForNextSentencePrediction.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForNextSentencePrediction.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForNextSentencePrediction

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForNextSentencePrediction.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForNextSentencePrediction.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForNextSentencePrediction.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),j3=new z({}),N3=new E({props:{name:"class transformers.AutoModelForTokenClassification",anchor:"transformers.AutoModelForTokenClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15774/src/transformers/models/auto/modeling_auto.py#L744"}}),q3=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15774/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15774/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15774/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/albert#transformers.AlbertForTokenClassification">AlbertForTokenClassification</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/bert#transformers.BertForTokenClassification">BertForTokenClassification</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/big_bird#transformers.BigBirdForTokenClassification">BigBirdForTokenClassification</a> (BigBird model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/camembert#transformers.CamembertForTokenClassification">CamembertForTokenClassification</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/canine#transformers.CanineConfig">CanineConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/canine#transformers.CanineForTokenClassification">CanineForTokenClassification</a> (Canine model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/convbert#transformers.ConvBertForTokenClassification">ConvBertForTokenClassification</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/deberta#transformers.DebertaForTokenClassification">DebertaForTokenClassification</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/deberta-v2#transformers.DebertaV2ForTokenClassification">DebertaV2ForTokenClassification</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/distilbert#transformers.DistilBertForTokenClassification">DistilBertForTokenClassification</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/electra#transformers.ElectraForTokenClassification">ElectraForTokenClassification</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/fnet#transformers.FNetConfig">FNetConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/fnet#transformers.FNetForTokenClassification">FNetForTokenClassification</a> (FNet model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/flaubert#transformers.FlaubertForTokenClassification">FlaubertForTokenClassification</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/funnel#transformers.FunnelForTokenClassification">FunnelForTokenClassification</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/gpt2#transformers.GPT2ForTokenClassification">GPT2ForTokenClassification</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/ibert#transformers.IBertConfig">IBertConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/ibert#transformers.IBertForTokenClassification">IBertForTokenClassification</a> (I-BERT model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/layoutlm#transformers.LayoutLMForTokenClassification">LayoutLMForTokenClassification</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/layoutlmv2#transformers.LayoutLMv2Config">LayoutLMv2Config</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/layoutlmv2#transformers.LayoutLMv2ForTokenClassification">LayoutLMv2ForTokenClassification</a> (LayoutLMv2 model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/longformer#transformers.LongformerForTokenClassification">LongformerForTokenClassification</a> (Longformer model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/mpnet#transformers.MPNetForTokenClassification">MPNetForTokenClassification</a> (MPNet model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/megatron-bert#transformers.MegatronBertConfig">MegatronBertConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/megatron-bert#transformers.MegatronBertForTokenClassification">MegatronBertForTokenClassification</a> (MegatronBert model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/mobilebert#transformers.MobileBertForTokenClassification">MobileBertForTokenClassification</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/nystromformer#transformers.NystromformerConfig">NystromformerConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/nystromformer#transformers.NystromformerForTokenClassification">NystromformerForTokenClassification</a> (Nystromformer model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/qdqbert#transformers.QDQBertConfig">QDQBertConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/qdqbert#transformers.QDQBertForTokenClassification">QDQBertForTokenClassification</a> (QDQBert model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/rembert#transformers.RemBertForTokenClassification">RemBertForTokenClassification</a> (RemBERT model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/roformer#transformers.RoFormerForTokenClassification">RoFormerForTokenClassification</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/roberta#transformers.RobertaForTokenClassification">RobertaForTokenClassification</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/squeezebert#transformers.SqueezeBertConfig">SqueezeBertConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/squeezebert#transformers.SqueezeBertForTokenClassification">SqueezeBertForTokenClassification</a> (SqueezeBERT model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/xlm#transformers.XLMForTokenClassification">XLMForTokenClassification</a> (XLM model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/xlm-roberta#transformers.XLMRobertaForTokenClassification">XLMRobertaForTokenClassification</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLConfig">XLMRobertaXLConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForTokenClassification">XLMRobertaXLForTokenClassification</a> (XLM-RoBERTa-XL model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/xlnet#transformers.XLNetForTokenClassification">XLNetForTokenClassification</a> (XLNet model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/yoso#transformers.YosoConfig">YosoConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/yoso#transformers.YosoForTokenClassification">YosoForTokenClassification</a> (YOSO model)</li>
</ul>`,name:"config"}]}}),G3=new w({props:{code:`from transformers import AutoConfig, AutoModelForTokenClassification

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForTokenClassification.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForTokenClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForTokenClassification.from_config(config)`}}),O3=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15774/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15774/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15774/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15774/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15774/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15774/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15774/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),X3=new w({props:{code:`from transformers import AutoConfig, AutoModelForTokenClassification

# Download model and configuration from huggingface.co and cache.
model = AutoModelForTokenClassification.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForTokenClassification.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForTokenClassification.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForTokenClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForTokenClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForTokenClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForTokenClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),z3=new z({}),V3=new E({props:{name:"class transformers.AutoModelForQuestionAnswering",anchor:"transformers.AutoModelForQuestionAnswering",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15774/src/transformers/models/auto/modeling_auto.py#L726"}}),Q3=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15774/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15774/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15774/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/albert#transformers.AlbertForQuestionAnswering">AlbertForQuestionAnswering</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/bart#transformers.BartForQuestionAnswering">BartForQuestionAnswering</a> (BART model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/bert#transformers.BertForQuestionAnswering">BertForQuestionAnswering</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/big_bird#transformers.BigBirdForQuestionAnswering">BigBirdForQuestionAnswering</a> (BigBird model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusConfig">BigBirdPegasusConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusForQuestionAnswering">BigBirdPegasusForQuestionAnswering</a> (BigBirdPegasus model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/camembert#transformers.CamembertForQuestionAnswering">CamembertForQuestionAnswering</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/canine#transformers.CanineConfig">CanineConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/canine#transformers.CanineForQuestionAnswering">CanineForQuestionAnswering</a> (Canine model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/convbert#transformers.ConvBertForQuestionAnswering">ConvBertForQuestionAnswering</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/deberta#transformers.DebertaForQuestionAnswering">DebertaForQuestionAnswering</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/deberta-v2#transformers.DebertaV2ForQuestionAnswering">DebertaV2ForQuestionAnswering</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/distilbert#transformers.DistilBertForQuestionAnswering">DistilBertForQuestionAnswering</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/electra#transformers.ElectraForQuestionAnswering">ElectraForQuestionAnswering</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/fnet#transformers.FNetConfig">FNetConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/fnet#transformers.FNetForQuestionAnswering">FNetForQuestionAnswering</a> (FNet model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/flaubert#transformers.FlaubertForQuestionAnsweringSimple">FlaubertForQuestionAnsweringSimple</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/funnel#transformers.FunnelForQuestionAnswering">FunnelForQuestionAnswering</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/gptj#transformers.GPTJConfig">GPTJConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/gptj#transformers.GPTJForQuestionAnswering">GPTJForQuestionAnswering</a> (GPT-J model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/ibert#transformers.IBertConfig">IBertConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/ibert#transformers.IBertForQuestionAnswering">IBertForQuestionAnswering</a> (I-BERT model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/led#transformers.LEDConfig">LEDConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/led#transformers.LEDForQuestionAnswering">LEDForQuestionAnswering</a> (LED model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/layoutlmv2#transformers.LayoutLMv2Config">LayoutLMv2Config</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/layoutlmv2#transformers.LayoutLMv2ForQuestionAnswering">LayoutLMv2ForQuestionAnswering</a> (LayoutLMv2 model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/longformer#transformers.LongformerForQuestionAnswering">LongformerForQuestionAnswering</a> (Longformer model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/lxmert#transformers.LxmertConfig">LxmertConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/lxmert#transformers.LxmertForQuestionAnswering">LxmertForQuestionAnswering</a> (LXMERT model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/mbart#transformers.MBartForQuestionAnswering">MBartForQuestionAnswering</a> (mBART model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/mpnet#transformers.MPNetForQuestionAnswering">MPNetForQuestionAnswering</a> (MPNet model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/megatron-bert#transformers.MegatronBertConfig">MegatronBertConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/megatron-bert#transformers.MegatronBertForQuestionAnswering">MegatronBertForQuestionAnswering</a> (MegatronBert model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/mobilebert#transformers.MobileBertForQuestionAnswering">MobileBertForQuestionAnswering</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/nystromformer#transformers.NystromformerConfig">NystromformerConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/nystromformer#transformers.NystromformerForQuestionAnswering">NystromformerForQuestionAnswering</a> (Nystromformer model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/qdqbert#transformers.QDQBertConfig">QDQBertConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/qdqbert#transformers.QDQBertForQuestionAnswering">QDQBertForQuestionAnswering</a> (QDQBert model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/reformer#transformers.ReformerConfig">ReformerConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/reformer#transformers.ReformerForQuestionAnswering">ReformerForQuestionAnswering</a> (Reformer model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/rembert#transformers.RemBertForQuestionAnswering">RemBertForQuestionAnswering</a> (RemBERT model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/roformer#transformers.RoFormerForQuestionAnswering">RoFormerForQuestionAnswering</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/roberta#transformers.RobertaForQuestionAnswering">RobertaForQuestionAnswering</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/splinter#transformers.SplinterConfig">SplinterConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/splinter#transformers.SplinterForQuestionAnswering">SplinterForQuestionAnswering</a> (Splinter model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/squeezebert#transformers.SqueezeBertConfig">SqueezeBertConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/squeezebert#transformers.SqueezeBertForQuestionAnswering">SqueezeBertForQuestionAnswering</a> (SqueezeBERT model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/xlm#transformers.XLMForQuestionAnsweringSimple">XLMForQuestionAnsweringSimple</a> (XLM model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/xlm-roberta#transformers.XLMRobertaForQuestionAnswering">XLMRobertaForQuestionAnswering</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLConfig">XLMRobertaXLConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForQuestionAnswering">XLMRobertaXLForQuestionAnswering</a> (XLM-RoBERTa-XL model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/xlnet#transformers.XLNetForQuestionAnsweringSimple">XLNetForQuestionAnsweringSimple</a> (XLNet model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/yoso#transformers.YosoConfig">YosoConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/yoso#transformers.YosoForQuestionAnswering">YosoForQuestionAnswering</a> (YOSO model)</li>
</ul>`,name:"config"}]}}),H3=new w({props:{code:`from transformers import AutoConfig, AutoModelForQuestionAnswering

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForQuestionAnswering.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForQuestionAnswering.from_config(config)`}}),U3=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15774/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15774/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15774/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15774/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15774/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15774/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15774/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),J3=new w({props:{code:`from transformers import AutoConfig, AutoModelForQuestionAnswering

# Download model and configuration from huggingface.co and cache.
model = AutoModelForQuestionAnswering.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForQuestionAnswering.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForQuestionAnswering.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForQuestionAnswering.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),Y3=new z({}),K3=new E({props:{name:"class transformers.AutoModelForTableQuestionAnswering",anchor:"transformers.AutoModelForTableQuestionAnswering",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15774/src/transformers/models/auto/modeling_auto.py#L733"}}),ey=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15774/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15774/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15774/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/tapas#transformers.TapasForQuestionAnswering">TapasForQuestionAnswering</a> (TAPAS model)</li>
</ul>`,name:"config"}]}}),oy=new w({props:{code:`from transformers import AutoConfig, AutoModelForTableQuestionAnswering

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("google/tapas-base-finetuned-wtq")
model = AutoModelForTableQuestionAnswering.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForTableQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;google/tapas-base-finetuned-wtq&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForTableQuestionAnswering.from_config(config)`}}),ry=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15774/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15774/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15774/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15774/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15774/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15774/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15774/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),ty=new w({props:{code:`from transformers import AutoConfig, AutoModelForTableQuestionAnswering

# Download model and configuration from huggingface.co and cache.
model = AutoModelForTableQuestionAnswering.from_pretrained("google/tapas-base-finetuned-wtq")

# Update configuration during loading
model = AutoModelForTableQuestionAnswering.from_pretrained("google/tapas-base-finetuned-wtq", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/tapas_tf_model_config.json")
model = AutoModelForTableQuestionAnswering.from_pretrained(
    "./tf_model/tapas_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForTableQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForTableQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;google/tapas-base-finetuned-wtq&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForTableQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;google/tapas-base-finetuned-wtq&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/tapas_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForTableQuestionAnswering.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/tapas_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),ay=new z({}),ny=new E({props:{name:"class transformers.AutoModelForImageClassification",anchor:"transformers.AutoModelForImageClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15774/src/transformers/models/auto/modeling_auto.py#L767"}}),ly=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15774/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15774/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15774/en/model_doc/beit#transformers.BeitConfig">BeitConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/beit#transformers.BeitForImageClassification">BeitForImageClassification</a> (BEiT model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/convnext#transformers.ConvNextConfig">ConvNextConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/convnext#transformers.ConvNextForImageClassification">ConvNextForImageClassification</a> (ConvNext model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/deit#transformers.DeiTConfig">DeiTConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/deit#transformers.DeiTForImageClassification">DeiTForImageClassification</a> or <a href="/docs/transformers/pr_15774/en/model_doc/deit#transformers.DeiTForImageClassificationWithTeacher">DeiTForImageClassificationWithTeacher</a> (DeiT model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/imagegpt#transformers.ImageGPTConfig">ImageGPTConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/imagegpt#transformers.ImageGPTForImageClassification">ImageGPTForImageClassification</a> (ImageGPT model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/perceiver#transformers.PerceiverConfig">PerceiverConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/perceiver#transformers.PerceiverForImageClassificationLearned">PerceiverForImageClassificationLearned</a> or <a href="/docs/transformers/pr_15774/en/model_doc/perceiver#transformers.PerceiverForImageClassificationFourier">PerceiverForImageClassificationFourier</a> or <a href="/docs/transformers/pr_15774/en/model_doc/perceiver#transformers.PerceiverForImageClassificationConvProcessing">PerceiverForImageClassificationConvProcessing</a> (Perceiver model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/poolformer#transformers.PoolFormerConfig">PoolFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/poolformer#transformers.PoolFormerForImageClassification">PoolFormerForImageClassification</a> (PoolFormer model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/segformer#transformers.SegformerConfig">SegformerConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/segformer#transformers.SegformerForImageClassification">SegformerForImageClassification</a> (SegFormer model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/swin#transformers.SwinConfig">SwinConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/swin#transformers.SwinForImageClassification">SwinForImageClassification</a> (Swin model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/vit#transformers.ViTConfig">ViTConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/vit#transformers.ViTForImageClassification">ViTForImageClassification</a> (ViT model)</li>
</ul>`,name:"config"}]}}),iy=new w({props:{code:`from transformers import AutoConfig, AutoModelForImageClassification

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForImageClassification.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForImageClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForImageClassification.from_config(config)`}}),dy=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15774/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15774/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15774/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15774/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15774/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15774/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15774/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),cy=new w({props:{code:`from transformers import AutoConfig, AutoModelForImageClassification

# Download model and configuration from huggingface.co and cache.
model = AutoModelForImageClassification.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForImageClassification.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForImageClassification.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForImageClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForImageClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForImageClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForImageClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),fy=new z({}),my=new E({props:{name:"class transformers.AutoModelForVision2Seq",anchor:"transformers.AutoModelForVision2Seq",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15774/src/transformers/models/auto/modeling_auto.py#L797"}}),hy=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15774/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15774/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15774/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderConfig">VisionEncoderDecoderConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderModel">VisionEncoderDecoderModel</a> (Vision Encoder decoder model)</li>
</ul>`,name:"config"}]}}),py=new w({props:{code:`from transformers import AutoConfig, AutoModelForVision2Seq

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForVision2Seq.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForVision2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForVision2Seq.from_config(config)`}}),_y=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15774/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15774/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15774/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15774/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15774/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15774/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15774/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),uy=new w({props:{code:`from transformers import AutoConfig, AutoModelForVision2Seq

# Download model and configuration from huggingface.co and cache.
model = AutoModelForVision2Seq.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForVision2Seq.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForVision2Seq.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForVision2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForVision2Seq.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForVision2Seq.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForVision2Seq.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),by=new z({}),vy=new E({props:{name:"class transformers.AutoModelForAudioClassification",anchor:"transformers.AutoModelForAudioClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15774/src/transformers/models/auto/modeling_auto.py#L804"}}),Fy=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15774/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15774/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15774/en/model_doc/hubert#transformers.HubertConfig">HubertConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/hubert#transformers.HubertForSequenceClassification">HubertForSequenceClassification</a> (Hubert model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/sew#transformers.SEWConfig">SEWConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/sew#transformers.SEWForSequenceClassification">SEWForSequenceClassification</a> (SEW model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/sew-d#transformers.SEWDConfig">SEWDConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/sew-d#transformers.SEWDForSequenceClassification">SEWDForSequenceClassification</a> (SEW-D model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/unispeech#transformers.UniSpeechConfig">UniSpeechConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/unispeech#transformers.UniSpeechForSequenceClassification">UniSpeechForSequenceClassification</a> (UniSpeech model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/unispeech-sat#transformers.UniSpeechSatConfig">UniSpeechSatConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/unispeech-sat#transformers.UniSpeechSatForSequenceClassification">UniSpeechSatForSequenceClassification</a> (UniSpeechSat model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/wav2vec2#transformers.Wav2Vec2ForSequenceClassification">Wav2Vec2ForSequenceClassification</a> (Wav2Vec2 model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/wavlm#transformers.WavLMConfig">WavLMConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/wavlm#transformers.WavLMForSequenceClassification">WavLMForSequenceClassification</a> (WavLM model)</li>
</ul>`,name:"config"}]}}),Cy=new w({props:{code:`from transformers import AutoConfig, AutoModelForAudioClassification

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForAudioClassification.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForAudioClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioClassification.from_config(config)`}}),My=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15774/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15774/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15774/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15774/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15774/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15774/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15774/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),Ey=new w({props:{code:`from transformers import AutoConfig, AutoModelForAudioClassification

# Download model and configuration from huggingface.co and cache.
model = AutoModelForAudioClassification.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForAudioClassification.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForAudioClassification.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForAudioClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),yy=new z({}),wy=new E({props:{name:"class transformers.AutoModelForAudioFrameClassification",anchor:"transformers.AutoModelForAudioFrameClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15774/src/transformers/models/auto/modeling_auto.py#L827"}}),Ly=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15774/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15774/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15774/en/model_doc/unispeech-sat#transformers.UniSpeechSatConfig">UniSpeechSatConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/unispeech-sat#transformers.UniSpeechSatForAudioFrameClassification">UniSpeechSatForAudioFrameClassification</a> (UniSpeechSat model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/wav2vec2#transformers.Wav2Vec2ForAudioFrameClassification">Wav2Vec2ForAudioFrameClassification</a> (Wav2Vec2 model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/wavlm#transformers.WavLMConfig">WavLMConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/wavlm#transformers.WavLMForAudioFrameClassification">WavLMForAudioFrameClassification</a> (WavLM model)</li>
</ul>`,name:"config"}]}}),By=new w({props:{code:`from transformers import AutoConfig, AutoModelForAudioFrameClassification

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForAudioFrameClassification.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForAudioFrameClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioFrameClassification.from_config(config)`}}),ky=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15774/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15774/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15774/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15774/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15774/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15774/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15774/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),xy=new w({props:{code:`from transformers import AutoConfig, AutoModelForAudioFrameClassification

# Download model and configuration from huggingface.co and cache.
model = AutoModelForAudioFrameClassification.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForAudioFrameClassification.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForAudioFrameClassification.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForAudioFrameClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioFrameClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioFrameClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioFrameClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),Ry=new z({}),Sy=new E({props:{name:"class transformers.AutoModelForCTC",anchor:"transformers.AutoModelForCTC",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15774/src/transformers/models/auto/modeling_auto.py#L811"}}),$y=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15774/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15774/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15774/en/model_doc/hubert#transformers.HubertConfig">HubertConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/hubert#transformers.HubertForCTC">HubertForCTC</a> (Hubert model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/sew#transformers.SEWConfig">SEWConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/sew#transformers.SEWForCTC">SEWForCTC</a> (SEW model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/sew-d#transformers.SEWDConfig">SEWDConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/sew-d#transformers.SEWDForCTC">SEWDForCTC</a> (SEW-D model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/unispeech#transformers.UniSpeechConfig">UniSpeechConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/unispeech#transformers.UniSpeechForCTC">UniSpeechForCTC</a> (UniSpeech model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/unispeech-sat#transformers.UniSpeechSatConfig">UniSpeechSatConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/unispeech-sat#transformers.UniSpeechSatForCTC">UniSpeechSatForCTC</a> (UniSpeechSat model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/wav2vec2#transformers.Wav2Vec2ForCTC">Wav2Vec2ForCTC</a> (Wav2Vec2 model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/wavlm#transformers.WavLMConfig">WavLMConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/wavlm#transformers.WavLMForCTC">WavLMForCTC</a> (WavLM model)</li>
</ul>`,name:"config"}]}}),Iy=new w({props:{code:`from transformers import AutoConfig, AutoModelForCTC

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForCTC.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForCTC

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCTC.from_config(config)`}}),jy=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15774/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15774/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15774/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15774/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15774/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15774/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15774/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),Ny=new w({props:{code:`from transformers import AutoConfig, AutoModelForCTC

# Download model and configuration from huggingface.co and cache.
model = AutoModelForCTC.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForCTC.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForCTC.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForCTC

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCTC.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCTC.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCTC.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),Dy=new z({}),qy=new E({props:{name:"class transformers.AutoModelForSpeechSeq2Seq",anchor:"transformers.AutoModelForSpeechSeq2Seq",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15774/src/transformers/models/auto/modeling_auto.py#L818"}}),Oy=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15774/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15774/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15774/en/model_doc/speech_to_text#transformers.Speech2TextConfig">Speech2TextConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/speech_to_text#transformers.Speech2TextForConditionalGeneration">Speech2TextForConditionalGeneration</a> (Speech2Text model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/speech-encoder-decoder#transformers.SpeechEncoderDecoderConfig">SpeechEncoderDecoderConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/speech-encoder-decoder#transformers.SpeechEncoderDecoderModel">SpeechEncoderDecoderModel</a> (Speech Encoder decoder model)</li>
</ul>`,name:"config"}]}}),Xy=new w({props:{code:`from transformers import AutoConfig, AutoModelForSpeechSeq2Seq

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForSpeechSeq2Seq.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForSpeechSeq2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSpeechSeq2Seq.from_config(config)`}}),zy=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15774/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15774/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15774/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15774/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15774/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15774/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15774/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),Wy=new w({props:{code:`from transformers import AutoConfig, AutoModelForSpeechSeq2Seq

# Download model and configuration from huggingface.co and cache.
model = AutoModelForSpeechSeq2Seq.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForSpeechSeq2Seq.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForSpeechSeq2Seq.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForSpeechSeq2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSpeechSeq2Seq.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSpeechSeq2Seq.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSpeechSeq2Seq.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),Qy=new z({}),Hy=new E({props:{name:"class transformers.AutoModelForAudioXVector",anchor:"transformers.AutoModelForAudioXVector",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15774/src/transformers/models/auto/modeling_auto.py#L836"}}),Jy=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15774/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15774/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15774/en/model_doc/unispeech-sat#transformers.UniSpeechSatConfig">UniSpeechSatConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/unispeech-sat#transformers.UniSpeechSatForXVector">UniSpeechSatForXVector</a> (UniSpeechSat model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/wav2vec2#transformers.Wav2Vec2ForXVector">Wav2Vec2ForXVector</a> (Wav2Vec2 model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/wavlm#transformers.WavLMConfig">WavLMConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/wavlm#transformers.WavLMForXVector">WavLMForXVector</a> (WavLM model)</li>
</ul>`,name:"config"}]}}),Yy=new w({props:{code:`from transformers import AutoConfig, AutoModelForAudioXVector

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForAudioXVector.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForAudioXVector

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioXVector.from_config(config)`}}),Ky=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15774/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15774/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15774/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15774/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15774/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15774/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15774/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),Zy=new w({props:{code:`from transformers import AutoConfig, AutoModelForAudioXVector

# Download model and configuration from huggingface.co and cache.
model = AutoModelForAudioXVector.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForAudioXVector.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForAudioXVector.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForAudioXVector

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioXVector.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioXVector.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioXVector.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),ew=new z({}),ow=new E({props:{name:"class transformers.AutoModelForMaskedImageModeling",anchor:"transformers.AutoModelForMaskedImageModeling",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15774/src/transformers/models/auto/modeling_auto.py#L843"}}),tw=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15774/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15774/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15774/en/model_doc/deit#transformers.DeiTConfig">DeiTConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/deit#transformers.DeiTForMaskedImageModeling">DeiTForMaskedImageModeling</a> (DeiT model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/swin#transformers.SwinConfig">SwinConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/swin#transformers.SwinForMaskedImageModeling">SwinForMaskedImageModeling</a> (Swin model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/vit#transformers.ViTConfig">ViTConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/vit#transformers.ViTForMaskedImageModeling">ViTForMaskedImageModeling</a> (ViT model)</li>
</ul>`,name:"config"}]}}),aw=new w({props:{code:`from transformers import AutoConfig, AutoModelForMaskedImageModeling

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForMaskedImageModeling.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForMaskedImageModeling

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMaskedImageModeling.from_config(config)`}}),nw=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15774/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15774/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15774/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15774/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15774/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15774/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15774/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),sw=new w({props:{code:`from transformers import AutoConfig, AutoModelForMaskedImageModeling

# Download model and configuration from huggingface.co and cache.
model = AutoModelForMaskedImageModeling.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForMaskedImageModeling.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForMaskedImageModeling.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForMaskedImageModeling

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMaskedImageModeling.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMaskedImageModeling.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMaskedImageModeling.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),lw=new z({}),iw=new E({props:{name:"class transformers.AutoModelForObjectDetection",anchor:"transformers.AutoModelForObjectDetection",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15774/src/transformers/models/auto/modeling_auto.py#L790"}}),cw=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15774/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15774/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15774/en/model_doc/detr#transformers.DetrConfig">DetrConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/detr#transformers.DetrForObjectDetection">DetrForObjectDetection</a> (DETR model)</li>
</ul>`,name:"config"}]}}),fw=new w({props:{code:`from transformers import AutoConfig, AutoModelForObjectDetection

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForObjectDetection.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForObjectDetection

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForObjectDetection.from_config(config)`}}),mw=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15774/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15774/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15774/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15774/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15774/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15774/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15774/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),gw=new w({props:{code:`from transformers import AutoConfig, AutoModelForObjectDetection

# Download model and configuration from huggingface.co and cache.
model = AutoModelForObjectDetection.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForObjectDetection.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForObjectDetection.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForObjectDetection

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForObjectDetection.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForObjectDetection.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForObjectDetection.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),hw=new z({}),pw=new E({props:{name:"class transformers.AutoModelForImageSegmentation",anchor:"transformers.AutoModelForImageSegmentation",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15774/src/transformers/models/auto/modeling_auto.py#L774"}}),uw=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15774/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15774/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15774/en/model_doc/detr#transformers.DetrConfig">DetrConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/detr#transformers.DetrForSegmentation">DetrForSegmentation</a> (DETR model)</li>
</ul>`,name:"config"}]}}),bw=new w({props:{code:`from transformers import AutoConfig, AutoModelForImageSegmentation

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForImageSegmentation.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForImageSegmentation

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForImageSegmentation.from_config(config)`}}),vw=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15774/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15774/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15774/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15774/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15774/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15774/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15774/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),Tw=new w({props:{code:`from transformers import AutoConfig, AutoModelForImageSegmentation

# Download model and configuration from huggingface.co and cache.
model = AutoModelForImageSegmentation.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForImageSegmentation.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForImageSegmentation.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForImageSegmentation

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForImageSegmentation.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForImageSegmentation.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForImageSegmentation.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),Fw=new z({}),Cw=new E({props:{name:"class transformers.AutoModelForSemanticSegmentation",anchor:"transformers.AutoModelForSemanticSegmentation",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15774/src/transformers/models/auto/modeling_auto.py#L781"}}),Ew=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15774/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15774/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15774/en/model_doc/beit#transformers.BeitConfig">BeitConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/beit#transformers.BeitForSemanticSegmentation">BeitForSemanticSegmentation</a> (BEiT model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/segformer#transformers.SegformerConfig">SegformerConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/segformer#transformers.SegformerForSemanticSegmentation">SegformerForSemanticSegmentation</a> (SegFormer model)</li>
</ul>`,name:"config"}]}}),yw=new w({props:{code:`from transformers import AutoConfig, AutoModelForSemanticSegmentation

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForSemanticSegmentation.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForSemanticSegmentation

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSemanticSegmentation.from_config(config)`}}),ww=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15774/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15774/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15774/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15774/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15774/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15774/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15774/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),Lw=new w({props:{code:`from transformers import AutoConfig, AutoModelForSemanticSegmentation

# Download model and configuration from huggingface.co and cache.
model = AutoModelForSemanticSegmentation.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForSemanticSegmentation.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForSemanticSegmentation.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForSemanticSegmentation

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSemanticSegmentation.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSemanticSegmentation.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSemanticSegmentation.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),Bw=new z({}),kw=new E({props:{name:"class transformers.TFAutoModel",anchor:"transformers.TFAutoModel",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15774/src/transformers/models/auto/modeling_tf_auto.py#L371"}}),Rw=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15774/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15774/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15774/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/albert#transformers.TFAlbertModel">TFAlbertModel</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/bart#transformers.TFBartModel">TFBartModel</a> (BART model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/bert#transformers.TFBertModel">TFBertModel</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/blenderbot#transformers.BlenderbotConfig">BlenderbotConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/blenderbot#transformers.TFBlenderbotModel">TFBlenderbotModel</a> (Blenderbot model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/blenderbot-small#transformers.BlenderbotSmallConfig">BlenderbotSmallConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/blenderbot-small#transformers.TFBlenderbotSmallModel">TFBlenderbotSmallModel</a> (BlenderbotSmall model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/clip#transformers.CLIPConfig">CLIPConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/clip#transformers.TFCLIPModel">TFCLIPModel</a> (CLIP model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/ctrl#transformers.CTRLConfig">CTRLConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/ctrl#transformers.TFCTRLModel">TFCTRLModel</a> (CTRL model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/camembert#transformers.TFCamembertModel">TFCamembertModel</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/convbert#transformers.TFConvBertModel">TFConvBertModel</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/dpr#transformers.DPRConfig">DPRConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/dpr#transformers.TFDPRQuestionEncoder">TFDPRQuestionEncoder</a> (DPR model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/deberta#transformers.TFDebertaModel">TFDebertaModel</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/deberta-v2#transformers.TFDebertaV2Model">TFDebertaV2Model</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/distilbert#transformers.TFDistilBertModel">TFDistilBertModel</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/electra#transformers.TFElectraModel">TFElectraModel</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/flaubert#transformers.TFFlaubertModel">TFFlaubertModel</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/funnel#transformers.TFFunnelModel">TFFunnelModel</a> or <a href="/docs/transformers/pr_15774/en/model_doc/funnel#transformers.TFFunnelBaseModel">TFFunnelBaseModel</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/gpt2#transformers.TFGPT2Model">TFGPT2Model</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/hubert#transformers.HubertConfig">HubertConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/hubert#transformers.TFHubertModel">TFHubertModel</a> (Hubert model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/led#transformers.LEDConfig">LEDConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/led#transformers.TFLEDModel">TFLEDModel</a> (LED model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/layoutlm#transformers.TFLayoutLMModel">TFLayoutLMModel</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/longformer#transformers.TFLongformerModel">TFLongformerModel</a> (Longformer model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/lxmert#transformers.LxmertConfig">LxmertConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/lxmert#transformers.TFLxmertModel">TFLxmertModel</a> (LXMERT model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/mbart#transformers.TFMBartModel">TFMBartModel</a> (mBART model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/mpnet#transformers.TFMPNetModel">TFMPNetModel</a> (MPNet model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/mt5#transformers.MT5Config">MT5Config</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/mt5#transformers.TFMT5Model">TFMT5Model</a> (mT5 model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/marian#transformers.MarianConfig">MarianConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/marian#transformers.TFMarianModel">TFMarianModel</a> (Marian model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/mobilebert#transformers.TFMobileBertModel">TFMobileBertModel</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig">OpenAIGPTConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/openai-gpt#transformers.TFOpenAIGPTModel">TFOpenAIGPTModel</a> (OpenAI GPT model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/pegasus#transformers.PegasusConfig">PegasusConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/pegasus#transformers.TFPegasusModel">TFPegasusModel</a> (Pegasus model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/rembert#transformers.TFRemBertModel">TFRemBertModel</a> (RemBERT model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/roformer#transformers.TFRoFormerModel">TFRoFormerModel</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/roberta#transformers.TFRobertaModel">TFRobertaModel</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/speech_to_text#transformers.Speech2TextConfig">Speech2TextConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/speech_to_text#transformers.TFSpeech2TextModel">TFSpeech2TextModel</a> (Speech2Text model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/t5#transformers.T5Config">T5Config</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/t5#transformers.TFT5Model">TFT5Model</a> (T5 model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/tapas#transformers.TFTapasModel">TFTapasModel</a> (TAPAS model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/transfo-xl#transformers.TransfoXLConfig">TransfoXLConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/transfo-xl#transformers.TFTransfoXLModel">TFTransfoXLModel</a> (Transformer-XL model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/vit#transformers.ViTConfig">ViTConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/vit#transformers.TFViTModel">TFViTModel</a> (ViT model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/wav2vec2#transformers.TFWav2Vec2Model">TFWav2Vec2Model</a> (Wav2Vec2 model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/xlm#transformers.TFXLMModel">TFXLMModel</a> (XLM model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/xlm-roberta#transformers.TFXLMRobertaModel">TFXLMRobertaModel</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/xlnet#transformers.TFXLNetModel">TFXLNetModel</a> (XLNet model)</li>
</ul>`,name:"config"}]}}),Sw=new w({props:{code:`from transformers import AutoConfig, TFAutoModel

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = TFAutoModel.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModel.from_config(config)`}}),Pw=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15774/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15774/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15774/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15774/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15774/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),$w=new w({props:{code:`from transformers import AutoConfig, TFAutoModel

# Download model and configuration from huggingface.co and cache.
model = TFAutoModel.from_pretrained("bert-base-cased")

# Update configuration during loading
model = TFAutoModel.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = TFAutoModel.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModel.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModel.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModel.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),Iw=new z({}),jw=new E({props:{name:"class transformers.TFAutoModelForPreTraining",anchor:"transformers.TFAutoModelForPreTraining",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15774/src/transformers/models/auto/modeling_tf_auto.py#L378"}}),Dw=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15774/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15774/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15774/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/albert#transformers.TFAlbertForPreTraining">TFAlbertForPreTraining</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/bart#transformers.TFBartForConditionalGeneration">TFBartForConditionalGeneration</a> (BART model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/bert#transformers.TFBertForPreTraining">TFBertForPreTraining</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/ctrl#transformers.CTRLConfig">CTRLConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/ctrl#transformers.TFCTRLLMHeadModel">TFCTRLLMHeadModel</a> (CTRL model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/camembert#transformers.TFCamembertForMaskedLM">TFCamembertForMaskedLM</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/distilbert#transformers.TFDistilBertForMaskedLM">TFDistilBertForMaskedLM</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/electra#transformers.TFElectraForPreTraining">TFElectraForPreTraining</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/flaubert#transformers.TFFlaubertWithLMHeadModel">TFFlaubertWithLMHeadModel</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/funnel#transformers.TFFunnelForPreTraining">TFFunnelForPreTraining</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/gpt2#transformers.TFGPT2LMHeadModel">TFGPT2LMHeadModel</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/layoutlm#transformers.TFLayoutLMForMaskedLM">TFLayoutLMForMaskedLM</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/lxmert#transformers.LxmertConfig">LxmertConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/lxmert#transformers.TFLxmertForPreTraining">TFLxmertForPreTraining</a> (LXMERT model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/mpnet#transformers.TFMPNetForMaskedLM">TFMPNetForMaskedLM</a> (MPNet model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/mobilebert#transformers.TFMobileBertForPreTraining">TFMobileBertForPreTraining</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig">OpenAIGPTConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/openai-gpt#transformers.TFOpenAIGPTLMHeadModel">TFOpenAIGPTLMHeadModel</a> (OpenAI GPT model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/roberta#transformers.TFRobertaForMaskedLM">TFRobertaForMaskedLM</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/t5#transformers.T5Config">T5Config</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/t5#transformers.TFT5ForConditionalGeneration">TFT5ForConditionalGeneration</a> (T5 model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/tapas#transformers.TFTapasForMaskedLM">TFTapasForMaskedLM</a> (TAPAS model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/transfo-xl#transformers.TransfoXLConfig">TransfoXLConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/transfo-xl#transformers.TFTransfoXLLMHeadModel">TFTransfoXLLMHeadModel</a> (Transformer-XL model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/xlm#transformers.TFXLMWithLMHeadModel">TFXLMWithLMHeadModel</a> (XLM model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForMaskedLM">TFXLMRobertaForMaskedLM</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/xlnet#transformers.TFXLNetLMHeadModel">TFXLNetLMHeadModel</a> (XLNet model)</li>
</ul>`,name:"config"}]}}),qw=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForPreTraining

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = TFAutoModelForPreTraining.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForPreTraining

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForPreTraining.from_config(config)`}}),Gw=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15774/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15774/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15774/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15774/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15774/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),Ow=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForPreTraining

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForPreTraining.from_pretrained("bert-base-cased")

# Update configuration during loading
model = TFAutoModelForPreTraining.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = TFAutoModelForPreTraining.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForPreTraining

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForPreTraining.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForPreTraining.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForPreTraining.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),Xw=new z({}),zw=new E({props:{name:"class transformers.TFAutoModelForCausalLM",anchor:"transformers.TFAutoModelForCausalLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15774/src/transformers/models/auto/modeling_tf_auto.py#L393"}}),Ww=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15774/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15774/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15774/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/bert#transformers.TFBertLMHeadModel">TFBertLMHeadModel</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/ctrl#transformers.CTRLConfig">CTRLConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/ctrl#transformers.TFCTRLLMHeadModel">TFCTRLLMHeadModel</a> (CTRL model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/gpt2#transformers.TFGPT2LMHeadModel">TFGPT2LMHeadModel</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig">OpenAIGPTConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/openai-gpt#transformers.TFOpenAIGPTLMHeadModel">TFOpenAIGPTLMHeadModel</a> (OpenAI GPT model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/rembert#transformers.TFRemBertForCausalLM">TFRemBertForCausalLM</a> (RemBERT model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/roformer#transformers.TFRoFormerForCausalLM">TFRoFormerForCausalLM</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/roberta#transformers.TFRobertaForCausalLM">TFRobertaForCausalLM</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/transfo-xl#transformers.TransfoXLConfig">TransfoXLConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/transfo-xl#transformers.TFTransfoXLLMHeadModel">TFTransfoXLLMHeadModel</a> (Transformer-XL model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/xlm#transformers.TFXLMWithLMHeadModel">TFXLMWithLMHeadModel</a> (XLM model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/xlnet#transformers.TFXLNetLMHeadModel">TFXLNetLMHeadModel</a> (XLNet model)</li>
</ul>`,name:"config"}]}}),Qw=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForCausalLM

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = TFAutoModelForCausalLM.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForCausalLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForCausalLM.from_config(config)`}}),Hw=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15774/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15774/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15774/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15774/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15774/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),Uw=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForCausalLM

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForCausalLM.from_pretrained("bert-base-cased")

# Update configuration during loading
model = TFAutoModelForCausalLM.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = TFAutoModelForCausalLM.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForCausalLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForCausalLM.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),Jw=new z({}),Yw=new E({props:{name:"class transformers.TFAutoModelForImageClassification",anchor:"transformers.TFAutoModelForImageClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15774/src/transformers/models/auto/modeling_tf_auto.py#L400"}}),Zw=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15774/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15774/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15774/en/model_doc/vit#transformers.ViTConfig">ViTConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/vit#transformers.TFViTForImageClassification">TFViTForImageClassification</a> (ViT model)</li>
</ul>`,name:"config"}]}}),eA=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForImageClassification

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = TFAutoModelForImageClassification.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForImageClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForImageClassification.from_config(config)`}}),oA=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15774/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15774/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15774/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15774/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15774/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),rA=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForImageClassification

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForImageClassification.from_pretrained("bert-base-cased")

# Update configuration during loading
model = TFAutoModelForImageClassification.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = TFAutoModelForImageClassification.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForImageClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForImageClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForImageClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForImageClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),tA=new z({}),aA=new E({props:{name:"class transformers.TFAutoModelForMaskedLM",anchor:"transformers.TFAutoModelForMaskedLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15774/src/transformers/models/auto/modeling_tf_auto.py#L414"}}),sA=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15774/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15774/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15774/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/albert#transformers.TFAlbertForMaskedLM">TFAlbertForMaskedLM</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/bert#transformers.TFBertForMaskedLM">TFBertForMaskedLM</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/camembert#transformers.TFCamembertForMaskedLM">TFCamembertForMaskedLM</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/convbert#transformers.TFConvBertForMaskedLM">TFConvBertForMaskedLM</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/deberta#transformers.TFDebertaForMaskedLM">TFDebertaForMaskedLM</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/deberta-v2#transformers.TFDebertaV2ForMaskedLM">TFDebertaV2ForMaskedLM</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/distilbert#transformers.TFDistilBertForMaskedLM">TFDistilBertForMaskedLM</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/electra#transformers.TFElectraForMaskedLM">TFElectraForMaskedLM</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/flaubert#transformers.TFFlaubertWithLMHeadModel">TFFlaubertWithLMHeadModel</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/funnel#transformers.TFFunnelForMaskedLM">TFFunnelForMaskedLM</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/layoutlm#transformers.TFLayoutLMForMaskedLM">TFLayoutLMForMaskedLM</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/longformer#transformers.TFLongformerForMaskedLM">TFLongformerForMaskedLM</a> (Longformer model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/mpnet#transformers.TFMPNetForMaskedLM">TFMPNetForMaskedLM</a> (MPNet model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/mobilebert#transformers.TFMobileBertForMaskedLM">TFMobileBertForMaskedLM</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/rembert#transformers.TFRemBertForMaskedLM">TFRemBertForMaskedLM</a> (RemBERT model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/roformer#transformers.TFRoFormerForMaskedLM">TFRoFormerForMaskedLM</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/roberta#transformers.TFRobertaForMaskedLM">TFRobertaForMaskedLM</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/tapas#transformers.TFTapasForMaskedLM">TFTapasForMaskedLM</a> (TAPAS model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/xlm#transformers.TFXLMWithLMHeadModel">TFXLMWithLMHeadModel</a> (XLM model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForMaskedLM">TFXLMRobertaForMaskedLM</a> (XLM-RoBERTa model)</li>
</ul>`,name:"config"}]}}),lA=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForMaskedLM

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = TFAutoModelForMaskedLM.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForMaskedLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForMaskedLM.from_config(config)`}}),iA=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15774/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15774/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15774/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15774/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15774/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),dA=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForMaskedLM

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForMaskedLM.from_pretrained("bert-base-cased")

# Update configuration during loading
model = TFAutoModelForMaskedLM.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = TFAutoModelForMaskedLM.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForMaskedLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForMaskedLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForMaskedLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForMaskedLM.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),cA=new z({}),fA=new E({props:{name:"class transformers.TFAutoModelForSeq2SeqLM",anchor:"transformers.TFAutoModelForSeq2SeqLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15774/src/transformers/models/auto/modeling_tf_auto.py#L421"}}),gA=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15774/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15774/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15774/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/bart#transformers.TFBartForConditionalGeneration">TFBartForConditionalGeneration</a> (BART model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/blenderbot#transformers.BlenderbotConfig">BlenderbotConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/blenderbot#transformers.TFBlenderbotForConditionalGeneration">TFBlenderbotForConditionalGeneration</a> (Blenderbot model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/blenderbot-small#transformers.BlenderbotSmallConfig">BlenderbotSmallConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/blenderbot-small#transformers.TFBlenderbotSmallForConditionalGeneration">TFBlenderbotSmallForConditionalGeneration</a> (BlenderbotSmall model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/encoder-decoder#transformers.EncoderDecoderConfig">EncoderDecoderConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/encoder-decoder#transformers.TFEncoderDecoderModel">TFEncoderDecoderModel</a> (Encoder decoder model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/led#transformers.LEDConfig">LEDConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/led#transformers.TFLEDForConditionalGeneration">TFLEDForConditionalGeneration</a> (LED model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/mbart#transformers.TFMBartForConditionalGeneration">TFMBartForConditionalGeneration</a> (mBART model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/mt5#transformers.MT5Config">MT5Config</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/mt5#transformers.TFMT5ForConditionalGeneration">TFMT5ForConditionalGeneration</a> (mT5 model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/marian#transformers.MarianConfig">MarianConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/marian#transformers.TFMarianMTModel">TFMarianMTModel</a> (Marian model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/pegasus#transformers.PegasusConfig">PegasusConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/pegasus#transformers.TFPegasusForConditionalGeneration">TFPegasusForConditionalGeneration</a> (Pegasus model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/t5#transformers.T5Config">T5Config</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/t5#transformers.TFT5ForConditionalGeneration">TFT5ForConditionalGeneration</a> (T5 model)</li>
</ul>`,name:"config"}]}}),hA=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForSeq2SeqLM

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("t5-base")
model = TFAutoModelForSeq2SeqLM.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForSeq2SeqLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;t5-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSeq2SeqLM.from_config(config)`}}),pA=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15774/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15774/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15774/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15774/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15774/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),_A=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForSeq2SeqLM

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForSeq2SeqLM.from_pretrained("t5-base")

# Update configuration during loading
model = TFAutoModelForSeq2SeqLM.from_pretrained("t5-base", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/t5_pt_model_config.json")
model = TFAutoModelForSeq2SeqLM.from_pretrained(
    "./pt_model/t5_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForSeq2SeqLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSeq2SeqLM.from_pretrained(<span class="hljs-string">&quot;t5-base&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSeq2SeqLM.from_pretrained(<span class="hljs-string">&quot;t5-base&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/t5_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSeq2SeqLM.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/t5_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),uA=new z({}),bA=new E({props:{name:"class transformers.TFAutoModelForSequenceClassification",anchor:"transformers.TFAutoModelForSequenceClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15774/src/transformers/models/auto/modeling_tf_auto.py#L430"}}),TA=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15774/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15774/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15774/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/albert#transformers.TFAlbertForSequenceClassification">TFAlbertForSequenceClassification</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/bert#transformers.TFBertForSequenceClassification">TFBertForSequenceClassification</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/ctrl#transformers.CTRLConfig">CTRLConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/ctrl#transformers.TFCTRLForSequenceClassification">TFCTRLForSequenceClassification</a> (CTRL model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/camembert#transformers.TFCamembertForSequenceClassification">TFCamembertForSequenceClassification</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/convbert#transformers.TFConvBertForSequenceClassification">TFConvBertForSequenceClassification</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/deberta#transformers.TFDebertaForSequenceClassification">TFDebertaForSequenceClassification</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/deberta-v2#transformers.TFDebertaV2ForSequenceClassification">TFDebertaV2ForSequenceClassification</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/distilbert#transformers.TFDistilBertForSequenceClassification">TFDistilBertForSequenceClassification</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/electra#transformers.TFElectraForSequenceClassification">TFElectraForSequenceClassification</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/flaubert#transformers.TFFlaubertForSequenceClassification">TFFlaubertForSequenceClassification</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/funnel#transformers.TFFunnelForSequenceClassification">TFFunnelForSequenceClassification</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/gpt2#transformers.TFGPT2ForSequenceClassification">TFGPT2ForSequenceClassification</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/layoutlm#transformers.TFLayoutLMForSequenceClassification">TFLayoutLMForSequenceClassification</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/longformer#transformers.TFLongformerForSequenceClassification">TFLongformerForSequenceClassification</a> (Longformer model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/mpnet#transformers.TFMPNetForSequenceClassification">TFMPNetForSequenceClassification</a> (MPNet model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/mobilebert#transformers.TFMobileBertForSequenceClassification">TFMobileBertForSequenceClassification</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig">OpenAIGPTConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/openai-gpt#transformers.TFOpenAIGPTForSequenceClassification">TFOpenAIGPTForSequenceClassification</a> (OpenAI GPT model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/rembert#transformers.TFRemBertForSequenceClassification">TFRemBertForSequenceClassification</a> (RemBERT model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/roformer#transformers.TFRoFormerForSequenceClassification">TFRoFormerForSequenceClassification</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/roberta#transformers.TFRobertaForSequenceClassification">TFRobertaForSequenceClassification</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/tapas#transformers.TFTapasForSequenceClassification">TFTapasForSequenceClassification</a> (TAPAS model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/transfo-xl#transformers.TransfoXLConfig">TransfoXLConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/transfo-xl#transformers.TFTransfoXLForSequenceClassification">TFTransfoXLForSequenceClassification</a> (Transformer-XL model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/xlm#transformers.TFXLMForSequenceClassification">TFXLMForSequenceClassification</a> (XLM model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForSequenceClassification">TFXLMRobertaForSequenceClassification</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/xlnet#transformers.TFXLNetForSequenceClassification">TFXLNetForSequenceClassification</a> (XLNet model)</li>
</ul>`,name:"config"}]}}),FA=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForSequenceClassification

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = TFAutoModelForSequenceClassification.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSequenceClassification.from_config(config)`}}),CA=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15774/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15774/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15774/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15774/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15774/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),MA=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForSequenceClassification

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForSequenceClassification.from_pretrained("bert-base-cased")

# Update configuration during loading
model = TFAutoModelForSequenceClassification.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = TFAutoModelForSequenceClassification.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSequenceClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),EA=new z({}),yA=new E({props:{name:"class transformers.TFAutoModelForMultipleChoice",anchor:"transformers.TFAutoModelForMultipleChoice",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15774/src/transformers/models/auto/modeling_tf_auto.py#L466"}}),AA=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15774/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15774/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15774/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/albert#transformers.TFAlbertForMultipleChoice">TFAlbertForMultipleChoice</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/bert#transformers.TFBertForMultipleChoice">TFBertForMultipleChoice</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/camembert#transformers.TFCamembertForMultipleChoice">TFCamembertForMultipleChoice</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/convbert#transformers.TFConvBertForMultipleChoice">TFConvBertForMultipleChoice</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/distilbert#transformers.TFDistilBertForMultipleChoice">TFDistilBertForMultipleChoice</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/electra#transformers.TFElectraForMultipleChoice">TFElectraForMultipleChoice</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/flaubert#transformers.TFFlaubertForMultipleChoice">TFFlaubertForMultipleChoice</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/funnel#transformers.TFFunnelForMultipleChoice">TFFunnelForMultipleChoice</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/longformer#transformers.TFLongformerForMultipleChoice">TFLongformerForMultipleChoice</a> (Longformer model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/mpnet#transformers.TFMPNetForMultipleChoice">TFMPNetForMultipleChoice</a> (MPNet model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/mobilebert#transformers.TFMobileBertForMultipleChoice">TFMobileBertForMultipleChoice</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/rembert#transformers.TFRemBertForMultipleChoice">TFRemBertForMultipleChoice</a> (RemBERT model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/roformer#transformers.TFRoFormerForMultipleChoice">TFRoFormerForMultipleChoice</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/roberta#transformers.TFRobertaForMultipleChoice">TFRobertaForMultipleChoice</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/xlm#transformers.TFXLMForMultipleChoice">TFXLMForMultipleChoice</a> (XLM model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForMultipleChoice">TFXLMRobertaForMultipleChoice</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/xlnet#transformers.TFXLNetForMultipleChoice">TFXLNetForMultipleChoice</a> (XLNet model)</li>
</ul>`,name:"config"}]}}),LA=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForMultipleChoice

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = TFAutoModelForMultipleChoice.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForMultipleChoice

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForMultipleChoice.from_config(config)`}}),BA=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15774/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15774/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15774/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15774/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15774/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),kA=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForMultipleChoice

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForMultipleChoice.from_pretrained("bert-base-cased")

# Update configuration during loading
model = TFAutoModelForMultipleChoice.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = TFAutoModelForMultipleChoice.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForMultipleChoice

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForMultipleChoice.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForMultipleChoice.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForMultipleChoice.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),xA=new z({}),RA=new E({props:{name:"class transformers.TFAutoModelForTableQuestionAnswering",anchor:"transformers.TFAutoModelForTableQuestionAnswering",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15774/src/transformers/models/auto/modeling_tf_auto.py#L446"}}),PA=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15774/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15774/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15774/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/tapas#transformers.TFTapasForQuestionAnswering">TFTapasForQuestionAnswering</a> (TAPAS model)</li>
</ul>`,name:"config"}]}}),$A=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForTableQuestionAnswering

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("google/tapas-base-finetuned-wtq")
model = TFAutoModelForTableQuestionAnswering.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForTableQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;google/tapas-base-finetuned-wtq&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForTableQuestionAnswering.from_config(config)`}}),IA=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15774/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15774/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15774/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15774/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15774/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),jA=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForTableQuestionAnswering

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForTableQuestionAnswering.from_pretrained("google/tapas-base-finetuned-wtq")

# Update configuration during loading
model = TFAutoModelForTableQuestionAnswering.from_pretrained("google/tapas-base-finetuned-wtq", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/tapas_pt_model_config.json")
model = TFAutoModelForTableQuestionAnswering.from_pretrained(
    "./pt_model/tapas_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForTableQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForTableQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;google/tapas-base-finetuned-wtq&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForTableQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;google/tapas-base-finetuned-wtq&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/tapas_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForTableQuestionAnswering.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/tapas_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),NA=new z({}),DA=new E({props:{name:"class transformers.TFAutoModelForTokenClassification",anchor:"transformers.TFAutoModelForTokenClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15774/src/transformers/models/auto/modeling_tf_auto.py#L457"}}),GA=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15774/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15774/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15774/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/albert#transformers.TFAlbertForTokenClassification">TFAlbertForTokenClassification</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/bert#transformers.TFBertForTokenClassification">TFBertForTokenClassification</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/camembert#transformers.TFCamembertForTokenClassification">TFCamembertForTokenClassification</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/convbert#transformers.TFConvBertForTokenClassification">TFConvBertForTokenClassification</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/deberta#transformers.TFDebertaForTokenClassification">TFDebertaForTokenClassification</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/deberta-v2#transformers.TFDebertaV2ForTokenClassification">TFDebertaV2ForTokenClassification</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/distilbert#transformers.TFDistilBertForTokenClassification">TFDistilBertForTokenClassification</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/electra#transformers.TFElectraForTokenClassification">TFElectraForTokenClassification</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/flaubert#transformers.TFFlaubertForTokenClassification">TFFlaubertForTokenClassification</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/funnel#transformers.TFFunnelForTokenClassification">TFFunnelForTokenClassification</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/layoutlm#transformers.TFLayoutLMForTokenClassification">TFLayoutLMForTokenClassification</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/longformer#transformers.TFLongformerForTokenClassification">TFLongformerForTokenClassification</a> (Longformer model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/mpnet#transformers.TFMPNetForTokenClassification">TFMPNetForTokenClassification</a> (MPNet model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/mobilebert#transformers.TFMobileBertForTokenClassification">TFMobileBertForTokenClassification</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/rembert#transformers.TFRemBertForTokenClassification">TFRemBertForTokenClassification</a> (RemBERT model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/roformer#transformers.TFRoFormerForTokenClassification">TFRoFormerForTokenClassification</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/roberta#transformers.TFRobertaForTokenClassification">TFRobertaForTokenClassification</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/xlm#transformers.TFXLMForTokenClassification">TFXLMForTokenClassification</a> (XLM model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForTokenClassification">TFXLMRobertaForTokenClassification</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/xlnet#transformers.TFXLNetForTokenClassification">TFXLNetForTokenClassification</a> (XLNet model)</li>
</ul>`,name:"config"}]}}),OA=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForTokenClassification

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = TFAutoModelForTokenClassification.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForTokenClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForTokenClassification.from_config(config)`}}),XA=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15774/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15774/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15774/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15774/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15774/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),zA=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForTokenClassification

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForTokenClassification.from_pretrained("bert-base-cased")

# Update configuration during loading
model = TFAutoModelForTokenClassification.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = TFAutoModelForTokenClassification.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForTokenClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForTokenClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForTokenClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForTokenClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),VA=new z({}),WA=new E({props:{name:"class transformers.TFAutoModelForQuestionAnswering",anchor:"transformers.TFAutoModelForQuestionAnswering",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15774/src/transformers/models/auto/modeling_tf_auto.py#L439"}}),HA=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15774/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15774/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15774/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/albert#transformers.TFAlbertForQuestionAnswering">TFAlbertForQuestionAnswering</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/bert#transformers.TFBertForQuestionAnswering">TFBertForQuestionAnswering</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/camembert#transformers.TFCamembertForQuestionAnswering">TFCamembertForQuestionAnswering</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/convbert#transformers.TFConvBertForQuestionAnswering">TFConvBertForQuestionAnswering</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/deberta#transformers.TFDebertaForQuestionAnswering">TFDebertaForQuestionAnswering</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/deberta-v2#transformers.TFDebertaV2ForQuestionAnswering">TFDebertaV2ForQuestionAnswering</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/distilbert#transformers.TFDistilBertForQuestionAnswering">TFDistilBertForQuestionAnswering</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/electra#transformers.TFElectraForQuestionAnswering">TFElectraForQuestionAnswering</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/flaubert#transformers.TFFlaubertForQuestionAnsweringSimple">TFFlaubertForQuestionAnsweringSimple</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/funnel#transformers.TFFunnelForQuestionAnswering">TFFunnelForQuestionAnswering</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/longformer#transformers.TFLongformerForQuestionAnswering">TFLongformerForQuestionAnswering</a> (Longformer model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/mpnet#transformers.TFMPNetForQuestionAnswering">TFMPNetForQuestionAnswering</a> (MPNet model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/mobilebert#transformers.TFMobileBertForQuestionAnswering">TFMobileBertForQuestionAnswering</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/rembert#transformers.TFRemBertForQuestionAnswering">TFRemBertForQuestionAnswering</a> (RemBERT model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/roformer#transformers.TFRoFormerForQuestionAnswering">TFRoFormerForQuestionAnswering</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/roberta#transformers.TFRobertaForQuestionAnswering">TFRobertaForQuestionAnswering</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/xlm#transformers.TFXLMForQuestionAnsweringSimple">TFXLMForQuestionAnsweringSimple</a> (XLM model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForQuestionAnswering">TFXLMRobertaForQuestionAnswering</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/xlnet#transformers.TFXLNetForQuestionAnsweringSimple">TFXLNetForQuestionAnsweringSimple</a> (XLNet model)</li>
</ul>`,name:"config"}]}}),UA=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForQuestionAnswering

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = TFAutoModelForQuestionAnswering.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForQuestionAnswering.from_config(config)`}}),JA=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15774/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15774/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15774/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15774/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15774/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),YA=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForQuestionAnswering

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForQuestionAnswering.from_pretrained("bert-base-cased")

# Update configuration during loading
model = TFAutoModelForQuestionAnswering.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = TFAutoModelForQuestionAnswering.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForQuestionAnswering.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),KA=new z({}),ZA=new E({props:{name:"class transformers.TFAutoModelForVision2Seq",anchor:"transformers.TFAutoModelForVision2Seq",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15774/src/transformers/models/auto/modeling_tf_auto.py#L407"}}),o6=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15774/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15774/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15774/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderConfig">VisionEncoderDecoderConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/vision-encoder-decoder#transformers.TFVisionEncoderDecoderModel">TFVisionEncoderDecoderModel</a> (Vision Encoder decoder model)</li>
</ul>`,name:"config"}]}}),r6=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForVision2Seq

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = TFAutoModelForVision2Seq.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForVision2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForVision2Seq.from_config(config)`}}),t6=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15774/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15774/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15774/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15774/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15774/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),a6=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForVision2Seq

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForVision2Seq.from_pretrained("bert-base-cased")

# Update configuration during loading
model = TFAutoModelForVision2Seq.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = TFAutoModelForVision2Seq.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForVision2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForVision2Seq.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForVision2Seq.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForVision2Seq.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),n6=new z({}),s6=new E({props:{name:"class transformers.TFAutoModelForSpeechSeq2Seq",anchor:"transformers.TFAutoModelForSpeechSeq2Seq",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15774/src/transformers/models/auto/modeling_tf_auto.py#L482"}}),i6=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15774/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15774/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15774/en/model_doc/speech_to_text#transformers.Speech2TextConfig">Speech2TextConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/speech_to_text#transformers.TFSpeech2TextForConditionalGeneration">TFSpeech2TextForConditionalGeneration</a> (Speech2Text model)</li>
</ul>`,name:"config"}]}}),d6=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForSpeechSeq2Seq

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = TFAutoModelForSpeechSeq2Seq.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForSpeechSeq2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSpeechSeq2Seq.from_config(config)`}}),c6=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15774/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15774/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15774/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15774/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15774/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),f6=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForSpeechSeq2Seq

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForSpeechSeq2Seq.from_pretrained("bert-base-cased")

# Update configuration during loading
model = TFAutoModelForSpeechSeq2Seq.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = TFAutoModelForSpeechSeq2Seq.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForSpeechSeq2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSpeechSeq2Seq.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSpeechSeq2Seq.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSpeechSeq2Seq.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),m6=new z({}),g6=new E({props:{name:"class transformers.FlaxAutoModel",anchor:"transformers.FlaxAutoModel",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15774/src/transformers/models/auto/modeling_flax_auto.py#L220"}}),p6=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15774/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15774/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15774/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/albert#transformers.FlaxAlbertModel">FlaxAlbertModel</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/bart#transformers.FlaxBartModel">FlaxBartModel</a> (BART model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/beit#transformers.BeitConfig">BeitConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/beit#transformers.FlaxBeitModel">FlaxBeitModel</a> (BEiT model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/bert#transformers.FlaxBertModel">FlaxBertModel</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/big_bird#transformers.FlaxBigBirdModel">FlaxBigBirdModel</a> (BigBird model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/blenderbot#transformers.BlenderbotConfig">BlenderbotConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/blenderbot#transformers.FlaxBlenderbotModel">FlaxBlenderbotModel</a> (Blenderbot model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/blenderbot-small#transformers.BlenderbotSmallConfig">BlenderbotSmallConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/blenderbot-small#transformers.FlaxBlenderbotSmallModel">FlaxBlenderbotSmallModel</a> (BlenderbotSmall model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/clip#transformers.CLIPConfig">CLIPConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/clip#transformers.FlaxCLIPModel">FlaxCLIPModel</a> (CLIP model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/distilbert#transformers.FlaxDistilBertModel">FlaxDistilBertModel</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/electra#transformers.FlaxElectraModel">FlaxElectraModel</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/gpt2#transformers.FlaxGPT2Model">FlaxGPT2Model</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/gptj#transformers.GPTJConfig">GPTJConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/gptj#transformers.FlaxGPTJModel">FlaxGPTJModel</a> (GPT-J model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/gpt_neo#transformers.GPTNeoConfig">GPTNeoConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/gpt_neo#transformers.FlaxGPTNeoModel">FlaxGPTNeoModel</a> (GPT Neo model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/mbart#transformers.FlaxMBartModel">FlaxMBartModel</a> (mBART model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/mt5#transformers.MT5Config">MT5Config</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/mt5#transformers.FlaxMT5Model">FlaxMT5Model</a> (mT5 model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/marian#transformers.MarianConfig">MarianConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/marian#transformers.FlaxMarianModel">FlaxMarianModel</a> (Marian model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/pegasus#transformers.PegasusConfig">PegasusConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/pegasus#transformers.FlaxPegasusModel">FlaxPegasusModel</a> (Pegasus model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/roformer#transformers.FlaxRoFormerModel">FlaxRoFormerModel</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/roberta#transformers.FlaxRobertaModel">FlaxRobertaModel</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/t5#transformers.T5Config">T5Config</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/t5#transformers.FlaxT5Model">FlaxT5Model</a> (T5 model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/vit#transformers.ViTConfig">ViTConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/vit#transformers.FlaxViTModel">FlaxViTModel</a> (ViT model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/vision-text-dual-encoder#transformers.VisionTextDualEncoderConfig">VisionTextDualEncoderConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/vision-text-dual-encoder#transformers.FlaxVisionTextDualEncoderModel">FlaxVisionTextDualEncoderModel</a> (VisionTextDualEncoder model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/wav2vec2#transformers.FlaxWav2Vec2Model">FlaxWav2Vec2Model</a> (Wav2Vec2 model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/xglm#transformers.XGLMConfig">XGLMConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/xglm#transformers.FlaxXGLMModel">FlaxXGLMModel</a> (XGLM model)</li>
</ul>`,name:"config"}]}}),_6=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModel

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = FlaxAutoModel.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModel.from_config(config)`}}),u6=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15774/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15774/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15774/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15774/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15774/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),b6=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModel

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModel.from_pretrained("bert-base-cased")

# Update configuration during loading
model = FlaxAutoModel.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = FlaxAutoModel.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModel.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModel.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModel.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),v6=new z({}),T6=new E({props:{name:"class transformers.FlaxAutoModelForCausalLM",anchor:"transformers.FlaxAutoModelForCausalLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15774/src/transformers/models/auto/modeling_flax_auto.py#L234"}}),C6=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15774/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15774/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15774/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/gpt2#transformers.FlaxGPT2LMHeadModel">FlaxGPT2LMHeadModel</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/gptj#transformers.GPTJConfig">GPTJConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/gptj#transformers.FlaxGPTJForCausalLM">FlaxGPTJForCausalLM</a> (GPT-J model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/gpt_neo#transformers.GPTNeoConfig">GPTNeoConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/gpt_neo#transformers.FlaxGPTNeoForCausalLM">FlaxGPTNeoForCausalLM</a> (GPT Neo model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/xglm#transformers.XGLMConfig">XGLMConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/xglm#transformers.FlaxXGLMForCausalLM">FlaxXGLMForCausalLM</a> (XGLM model)</li>
</ul>`,name:"config"}]}}),M6=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForCausalLM

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = FlaxAutoModelForCausalLM.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForCausalLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForCausalLM.from_config(config)`}}),E6=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15774/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15774/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15774/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15774/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15774/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),y6=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForCausalLM

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForCausalLM.from_pretrained("bert-base-cased")

# Update configuration during loading
model = FlaxAutoModelForCausalLM.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = FlaxAutoModelForCausalLM.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForCausalLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForCausalLM.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),w6=new z({}),A6=new E({props:{name:"class transformers.FlaxAutoModelForPreTraining",anchor:"transformers.FlaxAutoModelForPreTraining",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15774/src/transformers/models/auto/modeling_flax_auto.py#L227"}}),B6=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15774/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15774/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15774/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/albert#transformers.FlaxAlbertForPreTraining">FlaxAlbertForPreTraining</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/bart#transformers.FlaxBartForConditionalGeneration">FlaxBartForConditionalGeneration</a> (BART model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/bert#transformers.FlaxBertForPreTraining">FlaxBertForPreTraining</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/big_bird#transformers.FlaxBigBirdForPreTraining">FlaxBigBirdForPreTraining</a> (BigBird model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/electra#transformers.FlaxElectraForPreTraining">FlaxElectraForPreTraining</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/mbart#transformers.FlaxMBartForConditionalGeneration">FlaxMBartForConditionalGeneration</a> (mBART model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/mt5#transformers.MT5Config">MT5Config</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/mt5#transformers.FlaxMT5ForConditionalGeneration">FlaxMT5ForConditionalGeneration</a> (mT5 model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/roformer#transformers.FlaxRoFormerForMaskedLM">FlaxRoFormerForMaskedLM</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/roberta#transformers.FlaxRobertaForMaskedLM">FlaxRobertaForMaskedLM</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/t5#transformers.T5Config">T5Config</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/t5#transformers.FlaxT5ForConditionalGeneration">FlaxT5ForConditionalGeneration</a> (T5 model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/wav2vec2#transformers.FlaxWav2Vec2ForPreTraining">FlaxWav2Vec2ForPreTraining</a> (Wav2Vec2 model)</li>
</ul>`,name:"config"}]}}),k6=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForPreTraining

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = FlaxAutoModelForPreTraining.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForPreTraining

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForPreTraining.from_config(config)`}}),x6=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15774/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15774/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15774/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15774/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15774/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),R6=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForPreTraining

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForPreTraining.from_pretrained("bert-base-cased")

# Update configuration during loading
model = FlaxAutoModelForPreTraining.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = FlaxAutoModelForPreTraining.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForPreTraining

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForPreTraining.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForPreTraining.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForPreTraining.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),S6=new z({}),P6=new E({props:{name:"class transformers.FlaxAutoModelForMaskedLM",anchor:"transformers.FlaxAutoModelForMaskedLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15774/src/transformers/models/auto/modeling_flax_auto.py#L241"}}),I6=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15774/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15774/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15774/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/albert#transformers.FlaxAlbertForMaskedLM">FlaxAlbertForMaskedLM</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/bart#transformers.FlaxBartForConditionalGeneration">FlaxBartForConditionalGeneration</a> (BART model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/bert#transformers.FlaxBertForMaskedLM">FlaxBertForMaskedLM</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/big_bird#transformers.FlaxBigBirdForMaskedLM">FlaxBigBirdForMaskedLM</a> (BigBird model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/distilbert#transformers.FlaxDistilBertForMaskedLM">FlaxDistilBertForMaskedLM</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/electra#transformers.FlaxElectraForMaskedLM">FlaxElectraForMaskedLM</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/mbart#transformers.FlaxMBartForConditionalGeneration">FlaxMBartForConditionalGeneration</a> (mBART model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/roformer#transformers.FlaxRoFormerForMaskedLM">FlaxRoFormerForMaskedLM</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/roberta#transformers.FlaxRobertaForMaskedLM">FlaxRobertaForMaskedLM</a> (RoBERTa model)</li>
</ul>`,name:"config"}]}}),j6=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForMaskedLM

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = FlaxAutoModelForMaskedLM.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForMaskedLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForMaskedLM.from_config(config)`}}),N6=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15774/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15774/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15774/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15774/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15774/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),D6=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForMaskedLM

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForMaskedLM.from_pretrained("bert-base-cased")

# Update configuration during loading
model = FlaxAutoModelForMaskedLM.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = FlaxAutoModelForMaskedLM.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForMaskedLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForMaskedLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForMaskedLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForMaskedLM.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),q6=new z({}),G6=new E({props:{name:"class transformers.FlaxAutoModelForSeq2SeqLM",anchor:"transformers.FlaxAutoModelForSeq2SeqLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15774/src/transformers/models/auto/modeling_flax_auto.py#L248"}}),X6=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15774/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15774/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15774/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/bart#transformers.FlaxBartForConditionalGeneration">FlaxBartForConditionalGeneration</a> (BART model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/blenderbot#transformers.BlenderbotConfig">BlenderbotConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/blenderbot#transformers.FlaxBlenderbotForConditionalGeneration">FlaxBlenderbotForConditionalGeneration</a> (Blenderbot model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/blenderbot-small#transformers.BlenderbotSmallConfig">BlenderbotSmallConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/blenderbot-small#transformers.FlaxBlenderbotSmallForConditionalGeneration">FlaxBlenderbotSmallForConditionalGeneration</a> (BlenderbotSmall model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/encoder-decoder#transformers.EncoderDecoderConfig">EncoderDecoderConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/encoder-decoder#transformers.FlaxEncoderDecoderModel">FlaxEncoderDecoderModel</a> (Encoder decoder model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/mbart#transformers.FlaxMBartForConditionalGeneration">FlaxMBartForConditionalGeneration</a> (mBART model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/mt5#transformers.MT5Config">MT5Config</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/mt5#transformers.FlaxMT5ForConditionalGeneration">FlaxMT5ForConditionalGeneration</a> (mT5 model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/marian#transformers.MarianConfig">MarianConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/marian#transformers.FlaxMarianMTModel">FlaxMarianMTModel</a> (Marian model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/pegasus#transformers.PegasusConfig">PegasusConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/pegasus#transformers.FlaxPegasusForConditionalGeneration">FlaxPegasusForConditionalGeneration</a> (Pegasus model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/t5#transformers.T5Config">T5Config</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/t5#transformers.FlaxT5ForConditionalGeneration">FlaxT5ForConditionalGeneration</a> (T5 model)</li>
</ul>`,name:"config"}]}}),z6=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForSeq2SeqLM

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("t5-base")
model = FlaxAutoModelForSeq2SeqLM.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForSeq2SeqLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;t5-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForSeq2SeqLM.from_config(config)`}}),V6=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15774/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15774/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15774/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15774/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15774/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),W6=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForSeq2SeqLM

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForSeq2SeqLM.from_pretrained("t5-base")

# Update configuration during loading
model = FlaxAutoModelForSeq2SeqLM.from_pretrained("t5-base", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/t5_pt_model_config.json")
model = FlaxAutoModelForSeq2SeqLM.from_pretrained(
    "./pt_model/t5_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForSeq2SeqLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForSeq2SeqLM.from_pretrained(<span class="hljs-string">&quot;t5-base&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForSeq2SeqLM.from_pretrained(<span class="hljs-string">&quot;t5-base&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/t5_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForSeq2SeqLM.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/t5_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),Q6=new z({}),H6=new E({props:{name:"class transformers.FlaxAutoModelForSequenceClassification",anchor:"transformers.FlaxAutoModelForSequenceClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15774/src/transformers/models/auto/modeling_flax_auto.py#L257"}}),J6=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15774/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15774/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15774/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/albert#transformers.FlaxAlbertForSequenceClassification">FlaxAlbertForSequenceClassification</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/bart#transformers.FlaxBartForSequenceClassification">FlaxBartForSequenceClassification</a> (BART model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/bert#transformers.FlaxBertForSequenceClassification">FlaxBertForSequenceClassification</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/big_bird#transformers.FlaxBigBirdForSequenceClassification">FlaxBigBirdForSequenceClassification</a> (BigBird model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/distilbert#transformers.FlaxDistilBertForSequenceClassification">FlaxDistilBertForSequenceClassification</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/electra#transformers.FlaxElectraForSequenceClassification">FlaxElectraForSequenceClassification</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/mbart#transformers.FlaxMBartForSequenceClassification">FlaxMBartForSequenceClassification</a> (mBART model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/roformer#transformers.FlaxRoFormerForSequenceClassification">FlaxRoFormerForSequenceClassification</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/roberta#transformers.FlaxRobertaForSequenceClassification">FlaxRobertaForSequenceClassification</a> (RoBERTa model)</li>
</ul>`,name:"config"}]}}),Y6=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForSequenceClassification

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = FlaxAutoModelForSequenceClassification.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForSequenceClassification.from_config(config)`}}),K6=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15774/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15774/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15774/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15774/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15774/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),Z6=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForSequenceClassification

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForSequenceClassification.from_pretrained("bert-base-cased")

# Update configuration during loading
model = FlaxAutoModelForSequenceClassification.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = FlaxAutoModelForSequenceClassification.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForSequenceClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),e0=new z({}),o0=new E({props:{name:"class transformers.FlaxAutoModelForQuestionAnswering",anchor:"transformers.FlaxAutoModelForQuestionAnswering",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15774/src/transformers/models/auto/modeling_flax_auto.py#L266"}}),t0=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15774/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15774/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15774/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/albert#transformers.FlaxAlbertForQuestionAnswering">FlaxAlbertForQuestionAnswering</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/bart#transformers.FlaxBartForQuestionAnswering">FlaxBartForQuestionAnswering</a> (BART model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/bert#transformers.FlaxBertForQuestionAnswering">FlaxBertForQuestionAnswering</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/big_bird#transformers.FlaxBigBirdForQuestionAnswering">FlaxBigBirdForQuestionAnswering</a> (BigBird model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/distilbert#transformers.FlaxDistilBertForQuestionAnswering">FlaxDistilBertForQuestionAnswering</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/electra#transformers.FlaxElectraForQuestionAnswering">FlaxElectraForQuestionAnswering</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/mbart#transformers.FlaxMBartForQuestionAnswering">FlaxMBartForQuestionAnswering</a> (mBART model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/roformer#transformers.FlaxRoFormerForQuestionAnswering">FlaxRoFormerForQuestionAnswering</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/roberta#transformers.FlaxRobertaForQuestionAnswering">FlaxRobertaForQuestionAnswering</a> (RoBERTa model)</li>
</ul>`,name:"config"}]}}),a0=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForQuestionAnswering

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = FlaxAutoModelForQuestionAnswering.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForQuestionAnswering.from_config(config)`}}),n0=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15774/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15774/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15774/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15774/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15774/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),s0=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForQuestionAnswering

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForQuestionAnswering.from_pretrained("bert-base-cased")

# Update configuration during loading
model = FlaxAutoModelForQuestionAnswering.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = FlaxAutoModelForQuestionAnswering.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForQuestionAnswering.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),l0=new z({}),i0=new E({props:{name:"class transformers.FlaxAutoModelForTokenClassification",anchor:"transformers.FlaxAutoModelForTokenClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15774/src/transformers/models/auto/modeling_flax_auto.py#L273"}}),c0=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15774/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15774/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15774/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/albert#transformers.FlaxAlbertForTokenClassification">FlaxAlbertForTokenClassification</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/bert#transformers.FlaxBertForTokenClassification">FlaxBertForTokenClassification</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/big_bird#transformers.FlaxBigBirdForTokenClassification">FlaxBigBirdForTokenClassification</a> (BigBird model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/distilbert#transformers.FlaxDistilBertForTokenClassification">FlaxDistilBertForTokenClassification</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/electra#transformers.FlaxElectraForTokenClassification">FlaxElectraForTokenClassification</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/roformer#transformers.FlaxRoFormerForTokenClassification">FlaxRoFormerForTokenClassification</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/roberta#transformers.FlaxRobertaForTokenClassification">FlaxRobertaForTokenClassification</a> (RoBERTa model)</li>
</ul>`,name:"config"}]}}),f0=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForTokenClassification

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = FlaxAutoModelForTokenClassification.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForTokenClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForTokenClassification.from_config(config)`}}),m0=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15774/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15774/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15774/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15774/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15774/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),g0=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForTokenClassification

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForTokenClassification.from_pretrained("bert-base-cased")

# Update configuration during loading
model = FlaxAutoModelForTokenClassification.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = FlaxAutoModelForTokenClassification.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForTokenClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForTokenClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForTokenClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForTokenClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),h0=new z({}),p0=new E({props:{name:"class transformers.FlaxAutoModelForMultipleChoice",anchor:"transformers.FlaxAutoModelForMultipleChoice",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15774/src/transformers/models/auto/modeling_flax_auto.py#L282"}}),u0=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15774/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15774/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15774/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/albert#transformers.FlaxAlbertForMultipleChoice">FlaxAlbertForMultipleChoice</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/bert#transformers.FlaxBertForMultipleChoice">FlaxBertForMultipleChoice</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/big_bird#transformers.FlaxBigBirdForMultipleChoice">FlaxBigBirdForMultipleChoice</a> (BigBird model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/distilbert#transformers.FlaxDistilBertForMultipleChoice">FlaxDistilBertForMultipleChoice</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/electra#transformers.FlaxElectraForMultipleChoice">FlaxElectraForMultipleChoice</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/roformer#transformers.FlaxRoFormerForMultipleChoice">FlaxRoFormerForMultipleChoice</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/roberta#transformers.FlaxRobertaForMultipleChoice">FlaxRobertaForMultipleChoice</a> (RoBERTa model)</li>
</ul>`,name:"config"}]}}),b0=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForMultipleChoice

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = FlaxAutoModelForMultipleChoice.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForMultipleChoice

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForMultipleChoice.from_config(config)`}}),v0=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15774/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15774/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15774/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15774/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15774/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),T0=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForMultipleChoice

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForMultipleChoice.from_pretrained("bert-base-cased")

# Update configuration during loading
model = FlaxAutoModelForMultipleChoice.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = FlaxAutoModelForMultipleChoice.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForMultipleChoice

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForMultipleChoice.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForMultipleChoice.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForMultipleChoice.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),F0=new z({}),C0=new E({props:{name:"class transformers.FlaxAutoModelForNextSentencePrediction",anchor:"transformers.FlaxAutoModelForNextSentencePrediction",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15774/src/transformers/models/auto/modeling_flax_auto.py#L289"}}),E0=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15774/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15774/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15774/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/bert#transformers.FlaxBertForNextSentencePrediction">FlaxBertForNextSentencePrediction</a> (BERT model)</li>
</ul>`,name:"config"}]}}),y0=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForNextSentencePrediction

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = FlaxAutoModelForNextSentencePrediction.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForNextSentencePrediction

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForNextSentencePrediction.from_config(config)`}}),w0=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15774/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15774/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15774/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15774/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15774/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),A0=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForNextSentencePrediction

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForNextSentencePrediction.from_pretrained("bert-base-cased")

# Update configuration during loading
model = FlaxAutoModelForNextSentencePrediction.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = FlaxAutoModelForNextSentencePrediction.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForNextSentencePrediction

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForNextSentencePrediction.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForNextSentencePrediction.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForNextSentencePrediction.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),L0=new z({}),B0=new E({props:{name:"class transformers.FlaxAutoModelForImageClassification",anchor:"transformers.FlaxAutoModelForImageClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15774/src/transformers/models/auto/modeling_flax_auto.py#L298"}}),x0=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15774/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15774/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15774/en/model_doc/beit#transformers.BeitConfig">BeitConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/beit#transformers.FlaxBeitForImageClassification">FlaxBeitForImageClassification</a> (BEiT model)</li>
<li><a href="/docs/transformers/pr_15774/en/model_doc/vit#transformers.ViTConfig">ViTConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/vit#transformers.FlaxViTForImageClassification">FlaxViTForImageClassification</a> (ViT model)</li>
</ul>`,name:"config"}]}}),R0=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForImageClassification

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = FlaxAutoModelForImageClassification.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForImageClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForImageClassification.from_config(config)`}}),S0=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15774/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15774/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15774/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15774/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15774/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),$0=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForImageClassification

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForImageClassification.from_pretrained("bert-base-cased")

# Update configuration during loading
model = FlaxAutoModelForImageClassification.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = FlaxAutoModelForImageClassification.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForImageClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForImageClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForImageClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForImageClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),I0=new z({}),j0=new E({props:{name:"class transformers.FlaxAutoModelForVision2Seq",anchor:"transformers.FlaxAutoModelForVision2Seq",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15774/src/transformers/models/auto/modeling_flax_auto.py#L307"}}),D0=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15774/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15774/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15774/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderConfig">VisionEncoderDecoderConfig</a> configuration class: <a href="/docs/transformers/pr_15774/en/model_doc/vision-encoder-decoder#transformers.FlaxVisionEncoderDecoderModel">FlaxVisionEncoderDecoderModel</a> (Vision Encoder decoder model)</li>
</ul>`,name:"config"}]}}),q0=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForVision2Seq

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = FlaxAutoModelForVision2Seq.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForVision2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForVision2Seq.from_config(config)`}}),G0=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15774/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15774/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15774/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15774/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15774/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),O0=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForVision2Seq

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForVision2Seq.from_pretrained("bert-base-cased")

# Update configuration during loading
model = FlaxAutoModelForVision2Seq.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = FlaxAutoModelForVision2Seq.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForVision2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForVision2Seq.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForVision2Seq.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForVision2Seq.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),{c(){J=a("meta"),Ae=l(),ie=a("h1"),me=a("a"),to=a("span"),f(ce.$$.fragment),ue=l(),Do=a("span"),wi=o("Auto Classes"),Ef=l(),sa=a("p"),Ai=o(`In many cases, the architecture you want to use can be guessed from the name or the path of the pretrained model you
are supplying to the `),Li=a("code"),oE=o("from_pretrained()"),yf=o(` method. AutoClasses are here to do this job for you so that you
automatically retrieve the relevant model given the name/path to the pretrained weights/config/vocabulary.`),ye=l(),io=a("p"),Bi=o("Instantiating one of "),Pn=a("a"),rE=o("AutoConfig"),$n=o(", "),In=a("a"),tE=o("AutoModel"),ki=o(`, and
`),jn=a("a"),aE=o("AutoTokenizer"),xi=o(" will directly create a class of the relevant architecture. For instance"),wf=l(),f($a.$$.fragment),co=l(),ge=a("p"),DL=o("will create a model that is an instance of "),Ri=a("a"),qL=o("BertModel"),GL=o("."),qo=l(),Ia=a("p"),OL=o("There is one class of "),Af=a("code"),XL=o("AutoModel"),mxe=o(" for each task, and for each backend (PyTorch, TensorFlow, or Flax)."),t8e=l(),Si=a("h2"),Lf=a("a"),$V=a("span"),f(nE.$$.fragment),gxe=l(),IV=a("span"),hxe=o("Extending the Auto Classes"),a8e=l(),Nn=a("p"),pxe=o(`Each of the auto classes has a method to be extended with your custom classes. For instance, if you have defined a
custom class of model `),jV=a("code"),_xe=o("NewModel"),uxe=o(", make sure you have a "),NV=a("code"),bxe=o("NewModelConfig"),vxe=o(` then you can add those to the auto
classes like this:`),n8e=l(),f(sE.$$.fragment),s8e=l(),zL=a("p"),Txe=o("You will then be able to use the auto classes like you would usually do!"),l8e=l(),f(Bf.$$.fragment),i8e=l(),Pi=a("h2"),kf=a("a"),DV=a("span"),f(lE.$$.fragment),Fxe=l(),qV=a("span"),Cxe=o("AutoConfig"),d8e=l(),Go=a("div"),f(iE.$$.fragment),Mxe=l(),dE=a("p"),Exe=o(`This is a generic configuration class that will be instantiated as one of the configuration classes of the library
when created with the `),VL=a("a"),yxe=o("from_pretrained()"),wxe=o(" class method."),Axe=l(),cE=a("p"),Lxe=o("This class cannot be instantiated directly using "),GV=a("code"),Bxe=o("__init__()"),kxe=o(" (throws an error)."),xxe=l(),fo=a("div"),f(fE.$$.fragment),Rxe=l(),OV=a("p"),Sxe=o("Instantiate one of the configuration classes of the library from a pretrained model configuration."),Pxe=l(),$i=a("p"),$xe=o("The configuration class to instantiate is selected based on the "),XV=a("code"),Ixe=o("model_type"),jxe=o(` property of the config object that
is loaded, or when it\u2019s missing, by falling back to using pattern matching on `),zV=a("code"),Nxe=o("pretrained_model_name_or_path"),Dxe=o(":"),qxe=l(),v=a("ul"),xf=a("li"),VV=a("strong"),Gxe=o("albert"),Oxe=o(" \u2014 "),WL=a("a"),Xxe=o("AlbertConfig"),zxe=o(" (ALBERT model)"),Vxe=l(),Rf=a("li"),WV=a("strong"),Wxe=o("bart"),Qxe=o(" \u2014 "),QL=a("a"),Hxe=o("BartConfig"),Uxe=o(" (BART model)"),Jxe=l(),Sf=a("li"),QV=a("strong"),Yxe=o("beit"),Kxe=o(" \u2014 "),HL=a("a"),Zxe=o("BeitConfig"),eRe=o(" (BEiT model)"),oRe=l(),Pf=a("li"),HV=a("strong"),rRe=o("bert"),tRe=o(" \u2014 "),UL=a("a"),aRe=o("BertConfig"),nRe=o(" (BERT model)"),sRe=l(),$f=a("li"),UV=a("strong"),lRe=o("bert-generation"),iRe=o(" \u2014 "),JL=a("a"),dRe=o("BertGenerationConfig"),cRe=o(" (Bert Generation model)"),fRe=l(),If=a("li"),JV=a("strong"),mRe=o("big_bird"),gRe=o(" \u2014 "),YL=a("a"),hRe=o("BigBirdConfig"),pRe=o(" (BigBird model)"),_Re=l(),jf=a("li"),YV=a("strong"),uRe=o("bigbird_pegasus"),bRe=o(" \u2014 "),KL=a("a"),vRe=o("BigBirdPegasusConfig"),TRe=o(" (BigBirdPegasus model)"),FRe=l(),Nf=a("li"),KV=a("strong"),CRe=o("blenderbot"),MRe=o(" \u2014 "),ZL=a("a"),ERe=o("BlenderbotConfig"),yRe=o(" (Blenderbot model)"),wRe=l(),Df=a("li"),ZV=a("strong"),ARe=o("blenderbot-small"),LRe=o(" \u2014 "),e8=a("a"),BRe=o("BlenderbotSmallConfig"),kRe=o(" (BlenderbotSmall model)"),xRe=l(),qf=a("li"),eW=a("strong"),RRe=o("camembert"),SRe=o(" \u2014 "),o8=a("a"),PRe=o("CamembertConfig"),$Re=o(" (CamemBERT model)"),IRe=l(),Gf=a("li"),oW=a("strong"),jRe=o("canine"),NRe=o(" \u2014 "),r8=a("a"),DRe=o("CanineConfig"),qRe=o(" (Canine model)"),GRe=l(),Of=a("li"),rW=a("strong"),ORe=o("clip"),XRe=o(" \u2014 "),t8=a("a"),zRe=o("CLIPConfig"),VRe=o(" (CLIP model)"),WRe=l(),Xf=a("li"),tW=a("strong"),QRe=o("convbert"),HRe=o(" \u2014 "),a8=a("a"),URe=o("ConvBertConfig"),JRe=o(" (ConvBERT model)"),YRe=l(),zf=a("li"),aW=a("strong"),KRe=o("convnext"),ZRe=o(" \u2014 "),n8=a("a"),eSe=o("ConvNextConfig"),oSe=o(" (ConvNext model)"),rSe=l(),Vf=a("li"),nW=a("strong"),tSe=o("ctrl"),aSe=o(" \u2014 "),s8=a("a"),nSe=o("CTRLConfig"),sSe=o(" (CTRL model)"),lSe=l(),Wf=a("li"),sW=a("strong"),iSe=o("deberta"),dSe=o(" \u2014 "),l8=a("a"),cSe=o("DebertaConfig"),fSe=o(" (DeBERTa model)"),mSe=l(),Qf=a("li"),lW=a("strong"),gSe=o("deberta-v2"),hSe=o(" \u2014 "),i8=a("a"),pSe=o("DebertaV2Config"),_Se=o(" (DeBERTa-v2 model)"),uSe=l(),Hf=a("li"),iW=a("strong"),bSe=o("deit"),vSe=o(" \u2014 "),d8=a("a"),TSe=o("DeiTConfig"),FSe=o(" (DeiT model)"),CSe=l(),Uf=a("li"),dW=a("strong"),MSe=o("detr"),ESe=o(" \u2014 "),c8=a("a"),ySe=o("DetrConfig"),wSe=o(" (DETR model)"),ASe=l(),Jf=a("li"),cW=a("strong"),LSe=o("distilbert"),BSe=o(" \u2014 "),f8=a("a"),kSe=o("DistilBertConfig"),xSe=o(" (DistilBERT model)"),RSe=l(),Yf=a("li"),fW=a("strong"),SSe=o("dpr"),PSe=o(" \u2014 "),m8=a("a"),$Se=o("DPRConfig"),ISe=o(" (DPR model)"),jSe=l(),Kf=a("li"),mW=a("strong"),NSe=o("electra"),DSe=o(" \u2014 "),g8=a("a"),qSe=o("ElectraConfig"),GSe=o(" (ELECTRA model)"),OSe=l(),Zf=a("li"),gW=a("strong"),XSe=o("encoder-decoder"),zSe=o(" \u2014 "),h8=a("a"),VSe=o("EncoderDecoderConfig"),WSe=o(" (Encoder decoder model)"),QSe=l(),em=a("li"),hW=a("strong"),HSe=o("flaubert"),USe=o(" \u2014 "),p8=a("a"),JSe=o("FlaubertConfig"),YSe=o(" (FlauBERT model)"),KSe=l(),om=a("li"),pW=a("strong"),ZSe=o("fnet"),ePe=o(" \u2014 "),_8=a("a"),oPe=o("FNetConfig"),rPe=o(" (FNet model)"),tPe=l(),rm=a("li"),_W=a("strong"),aPe=o("fsmt"),nPe=o(" \u2014 "),u8=a("a"),sPe=o("FSMTConfig"),lPe=o(" (FairSeq Machine-Translation model)"),iPe=l(),tm=a("li"),uW=a("strong"),dPe=o("funnel"),cPe=o(" \u2014 "),b8=a("a"),fPe=o("FunnelConfig"),mPe=o(" (Funnel Transformer model)"),gPe=l(),am=a("li"),bW=a("strong"),hPe=o("gpt2"),pPe=o(" \u2014 "),v8=a("a"),_Pe=o("GPT2Config"),uPe=o(" (OpenAI GPT-2 model)"),bPe=l(),nm=a("li"),vW=a("strong"),vPe=o("gpt_neo"),TPe=o(" \u2014 "),T8=a("a"),FPe=o("GPTNeoConfig"),CPe=o(" (GPT Neo model)"),MPe=l(),sm=a("li"),TW=a("strong"),EPe=o("gptj"),yPe=o(" \u2014 "),F8=a("a"),wPe=o("GPTJConfig"),APe=o(" (GPT-J model)"),LPe=l(),lm=a("li"),FW=a("strong"),BPe=o("hubert"),kPe=o(" \u2014 "),C8=a("a"),xPe=o("HubertConfig"),RPe=o(" (Hubert model)"),SPe=l(),im=a("li"),CW=a("strong"),PPe=o("ibert"),$Pe=o(" \u2014 "),M8=a("a"),IPe=o("IBertConfig"),jPe=o(" (I-BERT model)"),NPe=l(),dm=a("li"),MW=a("strong"),DPe=o("imagegpt"),qPe=o(" \u2014 "),E8=a("a"),GPe=o("ImageGPTConfig"),OPe=o(" (ImageGPT model)"),XPe=l(),cm=a("li"),EW=a("strong"),zPe=o("layoutlm"),VPe=o(" \u2014 "),y8=a("a"),WPe=o("LayoutLMConfig"),QPe=o(" (LayoutLM model)"),HPe=l(),fm=a("li"),yW=a("strong"),UPe=o("layoutlmv2"),JPe=o(" \u2014 "),w8=a("a"),YPe=o("LayoutLMv2Config"),KPe=o(" (LayoutLMv2 model)"),ZPe=l(),mm=a("li"),wW=a("strong"),e$e=o("led"),o$e=o(" \u2014 "),A8=a("a"),r$e=o("LEDConfig"),t$e=o(" (LED model)"),a$e=l(),gm=a("li"),AW=a("strong"),n$e=o("longformer"),s$e=o(" \u2014 "),L8=a("a"),l$e=o("LongformerConfig"),i$e=o(" (Longformer model)"),d$e=l(),hm=a("li"),LW=a("strong"),c$e=o("luke"),f$e=o(" \u2014 "),B8=a("a"),m$e=o("LukeConfig"),g$e=o(" (LUKE model)"),h$e=l(),pm=a("li"),BW=a("strong"),p$e=o("lxmert"),_$e=o(" \u2014 "),k8=a("a"),u$e=o("LxmertConfig"),b$e=o(" (LXMERT model)"),v$e=l(),_m=a("li"),kW=a("strong"),T$e=o("m2m_100"),F$e=o(" \u2014 "),x8=a("a"),C$e=o("M2M100Config"),M$e=o(" (M2M100 model)"),E$e=l(),um=a("li"),xW=a("strong"),y$e=o("marian"),w$e=o(" \u2014 "),R8=a("a"),A$e=o("MarianConfig"),L$e=o(" (Marian model)"),B$e=l(),bm=a("li"),RW=a("strong"),k$e=o("mbart"),x$e=o(" \u2014 "),S8=a("a"),R$e=o("MBartConfig"),S$e=o(" (mBART model)"),P$e=l(),vm=a("li"),SW=a("strong"),$$e=o("megatron-bert"),I$e=o(" \u2014 "),P8=a("a"),j$e=o("MegatronBertConfig"),N$e=o(" (MegatronBert model)"),D$e=l(),Tm=a("li"),PW=a("strong"),q$e=o("mobilebert"),G$e=o(" \u2014 "),$8=a("a"),O$e=o("MobileBertConfig"),X$e=o(" (MobileBERT model)"),z$e=l(),Fm=a("li"),$W=a("strong"),V$e=o("mpnet"),W$e=o(" \u2014 "),I8=a("a"),Q$e=o("MPNetConfig"),H$e=o(" (MPNet model)"),U$e=l(),Cm=a("li"),IW=a("strong"),J$e=o("mt5"),Y$e=o(" \u2014 "),j8=a("a"),K$e=o("MT5Config"),Z$e=o(" (mT5 model)"),eIe=l(),Mm=a("li"),jW=a("strong"),oIe=o("nystromformer"),rIe=o(" \u2014 "),N8=a("a"),tIe=o("NystromformerConfig"),aIe=o(" (Nystromformer model)"),nIe=l(),Em=a("li"),NW=a("strong"),sIe=o("openai-gpt"),lIe=o(" \u2014 "),D8=a("a"),iIe=o("OpenAIGPTConfig"),dIe=o(" (OpenAI GPT model)"),cIe=l(),ym=a("li"),DW=a("strong"),fIe=o("pegasus"),mIe=o(" \u2014 "),q8=a("a"),gIe=o("PegasusConfig"),hIe=o(" (Pegasus model)"),pIe=l(),wm=a("li"),qW=a("strong"),_Ie=o("perceiver"),uIe=o(" \u2014 "),G8=a("a"),bIe=o("PerceiverConfig"),vIe=o(" (Perceiver model)"),TIe=l(),Am=a("li"),GW=a("strong"),FIe=o("plbart"),CIe=o(" \u2014 "),O8=a("a"),MIe=o("PLBartConfig"),EIe=o(" (PLBart model)"),yIe=l(),Lm=a("li"),OW=a("strong"),wIe=o("poolformer"),AIe=o(" \u2014 "),X8=a("a"),LIe=o("PoolFormerConfig"),BIe=o(" (PoolFormer model)"),kIe=l(),Bm=a("li"),XW=a("strong"),xIe=o("prophetnet"),RIe=o(" \u2014 "),z8=a("a"),SIe=o("ProphetNetConfig"),PIe=o(" (ProphetNet model)"),$Ie=l(),km=a("li"),zW=a("strong"),IIe=o("qdqbert"),jIe=o(" \u2014 "),V8=a("a"),NIe=o("QDQBertConfig"),DIe=o(" (QDQBert model)"),qIe=l(),xm=a("li"),VW=a("strong"),GIe=o("rag"),OIe=o(" \u2014 "),W8=a("a"),XIe=o("RagConfig"),zIe=o(" (RAG model)"),VIe=l(),Rm=a("li"),WW=a("strong"),WIe=o("realm"),QIe=o(" \u2014 "),Q8=a("a"),HIe=o("RealmConfig"),UIe=o(" (Realm model)"),JIe=l(),Sm=a("li"),QW=a("strong"),YIe=o("reformer"),KIe=o(" \u2014 "),H8=a("a"),ZIe=o("ReformerConfig"),eje=o(" (Reformer model)"),oje=l(),Pm=a("li"),HW=a("strong"),rje=o("rembert"),tje=o(" \u2014 "),U8=a("a"),aje=o("RemBertConfig"),nje=o(" (RemBERT model)"),sje=l(),$m=a("li"),UW=a("strong"),lje=o("retribert"),ije=o(" \u2014 "),J8=a("a"),dje=o("RetriBertConfig"),cje=o(" (RetriBERT model)"),fje=l(),Im=a("li"),JW=a("strong"),mje=o("roberta"),gje=o(" \u2014 "),Y8=a("a"),hje=o("RobertaConfig"),pje=o(" (RoBERTa model)"),_je=l(),jm=a("li"),YW=a("strong"),uje=o("roformer"),bje=o(" \u2014 "),K8=a("a"),vje=o("RoFormerConfig"),Tje=o(" (RoFormer model)"),Fje=l(),Nm=a("li"),KW=a("strong"),Cje=o("segformer"),Mje=o(" \u2014 "),Z8=a("a"),Eje=o("SegformerConfig"),yje=o(" (SegFormer model)"),wje=l(),Dm=a("li"),ZW=a("strong"),Aje=o("sew"),Lje=o(" \u2014 "),e9=a("a"),Bje=o("SEWConfig"),kje=o(" (SEW model)"),xje=l(),qm=a("li"),eQ=a("strong"),Rje=o("sew-d"),Sje=o(" \u2014 "),o9=a("a"),Pje=o("SEWDConfig"),$je=o(" (SEW-D model)"),Ije=l(),Gm=a("li"),oQ=a("strong"),jje=o("speech-encoder-decoder"),Nje=o(" \u2014 "),r9=a("a"),Dje=o("SpeechEncoderDecoderConfig"),qje=o(" (Speech Encoder decoder model)"),Gje=l(),Om=a("li"),rQ=a("strong"),Oje=o("speech_to_text"),Xje=o(" \u2014 "),t9=a("a"),zje=o("Speech2TextConfig"),Vje=o(" (Speech2Text model)"),Wje=l(),Xm=a("li"),tQ=a("strong"),Qje=o("speech_to_text_2"),Hje=o(" \u2014 "),a9=a("a"),Uje=o("Speech2Text2Config"),Jje=o(" (Speech2Text2 model)"),Yje=l(),zm=a("li"),aQ=a("strong"),Kje=o("splinter"),Zje=o(" \u2014 "),n9=a("a"),eNe=o("SplinterConfig"),oNe=o(" (Splinter model)"),rNe=l(),Vm=a("li"),nQ=a("strong"),tNe=o("squeezebert"),aNe=o(" \u2014 "),s9=a("a"),nNe=o("SqueezeBertConfig"),sNe=o(" (SqueezeBERT model)"),lNe=l(),Wm=a("li"),sQ=a("strong"),iNe=o("swin"),dNe=o(" \u2014 "),l9=a("a"),cNe=o("SwinConfig"),fNe=o(" (Swin model)"),mNe=l(),Qm=a("li"),lQ=a("strong"),gNe=o("t5"),hNe=o(" \u2014 "),i9=a("a"),pNe=o("T5Config"),_Ne=o(" (T5 model)"),uNe=l(),Hm=a("li"),iQ=a("strong"),bNe=o("tapas"),vNe=o(" \u2014 "),d9=a("a"),TNe=o("TapasConfig"),FNe=o(" (TAPAS model)"),CNe=l(),Um=a("li"),dQ=a("strong"),MNe=o("transfo-xl"),ENe=o(" \u2014 "),c9=a("a"),yNe=o("TransfoXLConfig"),wNe=o(" (Transformer-XL model)"),ANe=l(),Jm=a("li"),cQ=a("strong"),LNe=o("trocr"),BNe=o(" \u2014 "),f9=a("a"),kNe=o("TrOCRConfig"),xNe=o(" (TrOCR model)"),RNe=l(),Ym=a("li"),fQ=a("strong"),SNe=o("unispeech"),PNe=o(" \u2014 "),m9=a("a"),$Ne=o("UniSpeechConfig"),INe=o(" (UniSpeech model)"),jNe=l(),Km=a("li"),mQ=a("strong"),NNe=o("unispeech-sat"),DNe=o(" \u2014 "),g9=a("a"),qNe=o("UniSpeechSatConfig"),GNe=o(" (UniSpeechSat model)"),ONe=l(),Zm=a("li"),gQ=a("strong"),XNe=o("vilt"),zNe=o(" \u2014 "),h9=a("a"),VNe=o("ViltConfig"),WNe=o(" (ViLT model)"),QNe=l(),eg=a("li"),hQ=a("strong"),HNe=o("vision-encoder-decoder"),UNe=o(" \u2014 "),p9=a("a"),JNe=o("VisionEncoderDecoderConfig"),YNe=o(" (Vision Encoder decoder model)"),KNe=l(),og=a("li"),pQ=a("strong"),ZNe=o("vision-text-dual-encoder"),eDe=o(" \u2014 "),_9=a("a"),oDe=o("VisionTextDualEncoderConfig"),rDe=o(" (VisionTextDualEncoder model)"),tDe=l(),rg=a("li"),_Q=a("strong"),aDe=o("visual_bert"),nDe=o(" \u2014 "),u9=a("a"),sDe=o("VisualBertConfig"),lDe=o(" (VisualBert model)"),iDe=l(),tg=a("li"),uQ=a("strong"),dDe=o("vit"),cDe=o(" \u2014 "),b9=a("a"),fDe=o("ViTConfig"),mDe=o(" (ViT model)"),gDe=l(),ag=a("li"),bQ=a("strong"),hDe=o("vit_mae"),pDe=o(" \u2014 "),v9=a("a"),_De=o("ViTMAEConfig"),uDe=o(" (ViTMAE model)"),bDe=l(),ng=a("li"),vQ=a("strong"),vDe=o("wav2vec2"),TDe=o(" \u2014 "),T9=a("a"),FDe=o("Wav2Vec2Config"),CDe=o(" (Wav2Vec2 model)"),MDe=l(),sg=a("li"),TQ=a("strong"),EDe=o("wavlm"),yDe=o(" \u2014 "),F9=a("a"),wDe=o("WavLMConfig"),ADe=o(" (WavLM model)"),LDe=l(),lg=a("li"),FQ=a("strong"),BDe=o("xglm"),kDe=o(" \u2014 "),C9=a("a"),xDe=o("XGLMConfig"),RDe=o(" (XGLM model)"),SDe=l(),ig=a("li"),CQ=a("strong"),PDe=o("xlm"),$De=o(" \u2014 "),M9=a("a"),IDe=o("XLMConfig"),jDe=o(" (XLM model)"),NDe=l(),dg=a("li"),MQ=a("strong"),DDe=o("xlm-prophetnet"),qDe=o(" \u2014 "),E9=a("a"),GDe=o("XLMProphetNetConfig"),ODe=o(" (XLMProphetNet model)"),XDe=l(),cg=a("li"),EQ=a("strong"),zDe=o("xlm-roberta"),VDe=o(" \u2014 "),y9=a("a"),WDe=o("XLMRobertaConfig"),QDe=o(" (XLM-RoBERTa model)"),HDe=l(),fg=a("li"),yQ=a("strong"),UDe=o("xlm-roberta-xl"),JDe=o(" \u2014 "),w9=a("a"),YDe=o("XLMRobertaXLConfig"),KDe=o(" (XLM-RoBERTa-XL model)"),ZDe=l(),mg=a("li"),wQ=a("strong"),eqe=o("xlnet"),oqe=o(" \u2014 "),A9=a("a"),rqe=o("XLNetConfig"),tqe=o(" (XLNet model)"),aqe=l(),gg=a("li"),AQ=a("strong"),nqe=o("yoso"),sqe=o(" \u2014 "),L9=a("a"),lqe=o("YosoConfig"),iqe=o(" (YOSO model)"),dqe=l(),LQ=a("p"),cqe=o("Examples:"),fqe=l(),f(mE.$$.fragment),mqe=l(),hg=a("div"),f(gE.$$.fragment),gqe=l(),BQ=a("p"),hqe=o("Register a new configuration for this class."),c8e=l(),Ii=a("h2"),pg=a("a"),kQ=a("span"),f(hE.$$.fragment),pqe=l(),xQ=a("span"),_qe=o("AutoTokenizer"),f8e=l(),Oo=a("div"),f(pE.$$.fragment),uqe=l(),_E=a("p"),bqe=o(`This is a generic tokenizer class that will be instantiated as one of the tokenizer classes of the library when
created with the `),B9=a("a"),vqe=o("AutoTokenizer.from_pretrained()"),Tqe=o(" class method."),Fqe=l(),uE=a("p"),Cqe=o("This class cannot be instantiated directly using "),RQ=a("code"),Mqe=o("__init__()"),Eqe=o(" (throws an error)."),yqe=l(),mo=a("div"),f(bE.$$.fragment),wqe=l(),SQ=a("p"),Aqe=o("Instantiate one of the tokenizer classes of the library from a pretrained model vocabulary."),Lqe=l(),ja=a("p"),Bqe=o("The tokenizer class to instantiate is selected based on the "),PQ=a("code"),kqe=o("model_type"),xqe=o(` property of the config object (either
passed as an argument or loaded from `),$Q=a("code"),Rqe=o("pretrained_model_name_or_path"),Sqe=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),IQ=a("code"),Pqe=o("pretrained_model_name_or_path"),$qe=o(":"),Iqe=l(),M=a("ul"),Dn=a("li"),jQ=a("strong"),jqe=o("albert"),Nqe=o(" \u2014 "),k9=a("a"),Dqe=o("AlbertTokenizer"),qqe=o(" or "),x9=a("a"),Gqe=o("AlbertTokenizerFast"),Oqe=o(" (ALBERT model)"),Xqe=l(),qn=a("li"),NQ=a("strong"),zqe=o("bart"),Vqe=o(" \u2014 "),R9=a("a"),Wqe=o("BartTokenizer"),Qqe=o(" or "),S9=a("a"),Hqe=o("BartTokenizerFast"),Uqe=o(" (BART model)"),Jqe=l(),Gn=a("li"),DQ=a("strong"),Yqe=o("barthez"),Kqe=o(" \u2014 "),P9=a("a"),Zqe=o("BarthezTokenizer"),eGe=o(" or "),$9=a("a"),oGe=o("BarthezTokenizerFast"),rGe=o(" (BARThez model)"),tGe=l(),_g=a("li"),qQ=a("strong"),aGe=o("bartpho"),nGe=o(" \u2014 "),I9=a("a"),sGe=o("BartphoTokenizer"),lGe=o(" (BARTpho model)"),iGe=l(),On=a("li"),GQ=a("strong"),dGe=o("bert"),cGe=o(" \u2014 "),j9=a("a"),fGe=o("BertTokenizer"),mGe=o(" or "),N9=a("a"),gGe=o("BertTokenizerFast"),hGe=o(" (BERT model)"),pGe=l(),ug=a("li"),OQ=a("strong"),_Ge=o("bert-generation"),uGe=o(" \u2014 "),D9=a("a"),bGe=o("BertGenerationTokenizer"),vGe=o(" (Bert Generation model)"),TGe=l(),bg=a("li"),XQ=a("strong"),FGe=o("bert-japanese"),CGe=o(" \u2014 "),q9=a("a"),MGe=o("BertJapaneseTokenizer"),EGe=o(" (BertJapanese model)"),yGe=l(),vg=a("li"),zQ=a("strong"),wGe=o("bertweet"),AGe=o(" \u2014 "),G9=a("a"),LGe=o("BertweetTokenizer"),BGe=o(" (Bertweet model)"),kGe=l(),Xn=a("li"),VQ=a("strong"),xGe=o("big_bird"),RGe=o(" \u2014 "),O9=a("a"),SGe=o("BigBirdTokenizer"),PGe=o(" or "),X9=a("a"),$Ge=o("BigBirdTokenizerFast"),IGe=o(" (BigBird model)"),jGe=l(),zn=a("li"),WQ=a("strong"),NGe=o("bigbird_pegasus"),DGe=o(" \u2014 "),z9=a("a"),qGe=o("PegasusTokenizer"),GGe=o(" or "),V9=a("a"),OGe=o("PegasusTokenizerFast"),XGe=o(" (BigBirdPegasus model)"),zGe=l(),Vn=a("li"),QQ=a("strong"),VGe=o("blenderbot"),WGe=o(" \u2014 "),W9=a("a"),QGe=o("BlenderbotTokenizer"),HGe=o(" or "),Q9=a("a"),UGe=o("BlenderbotTokenizerFast"),JGe=o(" (Blenderbot model)"),YGe=l(),Tg=a("li"),HQ=a("strong"),KGe=o("blenderbot-small"),ZGe=o(" \u2014 "),H9=a("a"),eOe=o("BlenderbotSmallTokenizer"),oOe=o(" (BlenderbotSmall model)"),rOe=l(),Fg=a("li"),UQ=a("strong"),tOe=o("byt5"),aOe=o(" \u2014 "),U9=a("a"),nOe=o("ByT5Tokenizer"),sOe=o(" (ByT5 model)"),lOe=l(),Wn=a("li"),JQ=a("strong"),iOe=o("camembert"),dOe=o(" \u2014 "),J9=a("a"),cOe=o("CamembertTokenizer"),fOe=o(" or "),Y9=a("a"),mOe=o("CamembertTokenizerFast"),gOe=o(" (CamemBERT model)"),hOe=l(),Cg=a("li"),YQ=a("strong"),pOe=o("canine"),_Oe=o(" \u2014 "),K9=a("a"),uOe=o("CanineTokenizer"),bOe=o(" (Canine model)"),vOe=l(),Qn=a("li"),KQ=a("strong"),TOe=o("clip"),FOe=o(" \u2014 "),Z9=a("a"),COe=o("CLIPTokenizer"),MOe=o(" or "),eB=a("a"),EOe=o("CLIPTokenizerFast"),yOe=o(" (CLIP model)"),wOe=l(),Hn=a("li"),ZQ=a("strong"),AOe=o("convbert"),LOe=o(" \u2014 "),oB=a("a"),BOe=o("ConvBertTokenizer"),kOe=o(" or "),rB=a("a"),xOe=o("ConvBertTokenizerFast"),ROe=o(" (ConvBERT model)"),SOe=l(),Un=a("li"),eH=a("strong"),POe=o("cpm"),$Oe=o(" \u2014 "),tB=a("a"),IOe=o("CpmTokenizer"),jOe=o(" or "),oH=a("code"),NOe=o("CpmTokenizerFast"),DOe=o(" (CPM model)"),qOe=l(),Mg=a("li"),rH=a("strong"),GOe=o("ctrl"),OOe=o(" \u2014 "),aB=a("a"),XOe=o("CTRLTokenizer"),zOe=o(" (CTRL model)"),VOe=l(),Jn=a("li"),tH=a("strong"),WOe=o("deberta"),QOe=o(" \u2014 "),nB=a("a"),HOe=o("DebertaTokenizer"),UOe=o(" or "),sB=a("a"),JOe=o("DebertaTokenizerFast"),YOe=o(" (DeBERTa model)"),KOe=l(),Eg=a("li"),aH=a("strong"),ZOe=o("deberta-v2"),eXe=o(" \u2014 "),lB=a("a"),oXe=o("DebertaV2Tokenizer"),rXe=o(" (DeBERTa-v2 model)"),tXe=l(),Yn=a("li"),nH=a("strong"),aXe=o("distilbert"),nXe=o(" \u2014 "),iB=a("a"),sXe=o("DistilBertTokenizer"),lXe=o(" or "),dB=a("a"),iXe=o("DistilBertTokenizerFast"),dXe=o(" (DistilBERT model)"),cXe=l(),Kn=a("li"),sH=a("strong"),fXe=o("dpr"),mXe=o(" \u2014 "),cB=a("a"),gXe=o("DPRQuestionEncoderTokenizer"),hXe=o(" or "),fB=a("a"),pXe=o("DPRQuestionEncoderTokenizerFast"),_Xe=o(" (DPR model)"),uXe=l(),Zn=a("li"),lH=a("strong"),bXe=o("electra"),vXe=o(" \u2014 "),mB=a("a"),TXe=o("ElectraTokenizer"),FXe=o(" or "),gB=a("a"),CXe=o("ElectraTokenizerFast"),MXe=o(" (ELECTRA model)"),EXe=l(),yg=a("li"),iH=a("strong"),yXe=o("flaubert"),wXe=o(" \u2014 "),hB=a("a"),AXe=o("FlaubertTokenizer"),LXe=o(" (FlauBERT model)"),BXe=l(),es=a("li"),dH=a("strong"),kXe=o("fnet"),xXe=o(" \u2014 "),pB=a("a"),RXe=o("FNetTokenizer"),SXe=o(" or "),_B=a("a"),PXe=o("FNetTokenizerFast"),$Xe=o(" (FNet model)"),IXe=l(),wg=a("li"),cH=a("strong"),jXe=o("fsmt"),NXe=o(" \u2014 "),uB=a("a"),DXe=o("FSMTTokenizer"),qXe=o(" (FairSeq Machine-Translation model)"),GXe=l(),os=a("li"),fH=a("strong"),OXe=o("funnel"),XXe=o(" \u2014 "),bB=a("a"),zXe=o("FunnelTokenizer"),VXe=o(" or "),vB=a("a"),WXe=o("FunnelTokenizerFast"),QXe=o(" (Funnel Transformer model)"),HXe=l(),rs=a("li"),mH=a("strong"),UXe=o("gpt2"),JXe=o(" \u2014 "),TB=a("a"),YXe=o("GPT2Tokenizer"),KXe=o(" or "),FB=a("a"),ZXe=o("GPT2TokenizerFast"),eze=o(" (OpenAI GPT-2 model)"),oze=l(),ts=a("li"),gH=a("strong"),rze=o("gpt_neo"),tze=o(" \u2014 "),CB=a("a"),aze=o("GPT2Tokenizer"),nze=o(" or "),MB=a("a"),sze=o("GPT2TokenizerFast"),lze=o(" (GPT Neo model)"),ize=l(),as=a("li"),hH=a("strong"),dze=o("herbert"),cze=o(" \u2014 "),EB=a("a"),fze=o("HerbertTokenizer"),mze=o(" or "),yB=a("a"),gze=o("HerbertTokenizerFast"),hze=o(" (HerBERT model)"),pze=l(),Ag=a("li"),pH=a("strong"),_ze=o("hubert"),uze=o(" \u2014 "),wB=a("a"),bze=o("Wav2Vec2CTCTokenizer"),vze=o(" (Hubert model)"),Tze=l(),ns=a("li"),_H=a("strong"),Fze=o("ibert"),Cze=o(" \u2014 "),AB=a("a"),Mze=o("RobertaTokenizer"),Eze=o(" or "),LB=a("a"),yze=o("RobertaTokenizerFast"),wze=o(" (I-BERT model)"),Aze=l(),ss=a("li"),uH=a("strong"),Lze=o("layoutlm"),Bze=o(" \u2014 "),BB=a("a"),kze=o("LayoutLMTokenizer"),xze=o(" or "),kB=a("a"),Rze=o("LayoutLMTokenizerFast"),Sze=o(" (LayoutLM model)"),Pze=l(),ls=a("li"),bH=a("strong"),$ze=o("layoutlmv2"),Ize=o(" \u2014 "),xB=a("a"),jze=o("LayoutLMv2Tokenizer"),Nze=o(" or "),RB=a("a"),Dze=o("LayoutLMv2TokenizerFast"),qze=o(" (LayoutLMv2 model)"),Gze=l(),is=a("li"),vH=a("strong"),Oze=o("layoutxlm"),Xze=o(" \u2014 "),SB=a("a"),zze=o("LayoutXLMTokenizer"),Vze=o(" or "),PB=a("a"),Wze=o("LayoutXLMTokenizerFast"),Qze=o(" (LayoutXLM model)"),Hze=l(),ds=a("li"),TH=a("strong"),Uze=o("led"),Jze=o(" \u2014 "),$B=a("a"),Yze=o("LEDTokenizer"),Kze=o(" or "),IB=a("a"),Zze=o("LEDTokenizerFast"),eVe=o(" (LED model)"),oVe=l(),cs=a("li"),FH=a("strong"),rVe=o("longformer"),tVe=o(" \u2014 "),jB=a("a"),aVe=o("LongformerTokenizer"),nVe=o(" or "),NB=a("a"),sVe=o("LongformerTokenizerFast"),lVe=o(" (Longformer model)"),iVe=l(),Lg=a("li"),CH=a("strong"),dVe=o("luke"),cVe=o(" \u2014 "),DB=a("a"),fVe=o("LukeTokenizer"),mVe=o(" (LUKE model)"),gVe=l(),fs=a("li"),MH=a("strong"),hVe=o("lxmert"),pVe=o(" \u2014 "),qB=a("a"),_Ve=o("LxmertTokenizer"),uVe=o(" or "),GB=a("a"),bVe=o("LxmertTokenizerFast"),vVe=o(" (LXMERT model)"),TVe=l(),Bg=a("li"),EH=a("strong"),FVe=o("m2m_100"),CVe=o(" \u2014 "),OB=a("a"),MVe=o("M2M100Tokenizer"),EVe=o(" (M2M100 model)"),yVe=l(),kg=a("li"),yH=a("strong"),wVe=o("marian"),AVe=o(" \u2014 "),XB=a("a"),LVe=o("MarianTokenizer"),BVe=o(" (Marian model)"),kVe=l(),ms=a("li"),wH=a("strong"),xVe=o("mbart"),RVe=o(" \u2014 "),zB=a("a"),SVe=o("MBartTokenizer"),PVe=o(" or "),VB=a("a"),$Ve=o("MBartTokenizerFast"),IVe=o(" (mBART model)"),jVe=l(),gs=a("li"),AH=a("strong"),NVe=o("mbart50"),DVe=o(" \u2014 "),WB=a("a"),qVe=o("MBart50Tokenizer"),GVe=o(" or "),QB=a("a"),OVe=o("MBart50TokenizerFast"),XVe=o(" (mBART-50 model)"),zVe=l(),xg=a("li"),LH=a("strong"),VVe=o("mluke"),WVe=o(" \u2014 "),HB=a("a"),QVe=o("MLukeTokenizer"),HVe=o(" (mLUKE model)"),UVe=l(),hs=a("li"),BH=a("strong"),JVe=o("mobilebert"),YVe=o(" \u2014 "),UB=a("a"),KVe=o("MobileBertTokenizer"),ZVe=o(" or "),JB=a("a"),eWe=o("MobileBertTokenizerFast"),oWe=o(" (MobileBERT model)"),rWe=l(),ps=a("li"),kH=a("strong"),tWe=o("mpnet"),aWe=o(" \u2014 "),YB=a("a"),nWe=o("MPNetTokenizer"),sWe=o(" or "),KB=a("a"),lWe=o("MPNetTokenizerFast"),iWe=o(" (MPNet model)"),dWe=l(),_s=a("li"),xH=a("strong"),cWe=o("mt5"),fWe=o(" \u2014 "),ZB=a("a"),mWe=o("MT5Tokenizer"),gWe=o(" or "),ek=a("a"),hWe=o("MT5TokenizerFast"),pWe=o(" (mT5 model)"),_We=l(),us=a("li"),RH=a("strong"),uWe=o("openai-gpt"),bWe=o(" \u2014 "),ok=a("a"),vWe=o("OpenAIGPTTokenizer"),TWe=o(" or "),rk=a("a"),FWe=o("OpenAIGPTTokenizerFast"),CWe=o(" (OpenAI GPT model)"),MWe=l(),bs=a("li"),SH=a("strong"),EWe=o("pegasus"),yWe=o(" \u2014 "),tk=a("a"),wWe=o("PegasusTokenizer"),AWe=o(" or "),ak=a("a"),LWe=o("PegasusTokenizerFast"),BWe=o(" (Pegasus model)"),kWe=l(),Rg=a("li"),PH=a("strong"),xWe=o("perceiver"),RWe=o(" \u2014 "),nk=a("a"),SWe=o("PerceiverTokenizer"),PWe=o(" (Perceiver model)"),$We=l(),Sg=a("li"),$H=a("strong"),IWe=o("phobert"),jWe=o(" \u2014 "),sk=a("a"),NWe=o("PhobertTokenizer"),DWe=o(" (PhoBERT model)"),qWe=l(),Pg=a("li"),IH=a("strong"),GWe=o("plbart"),OWe=o(" \u2014 "),lk=a("a"),XWe=o("PLBartTokenizer"),zWe=o(" (PLBart model)"),VWe=l(),$g=a("li"),jH=a("strong"),WWe=o("prophetnet"),QWe=o(" \u2014 "),ik=a("a"),HWe=o("ProphetNetTokenizer"),UWe=o(" (ProphetNet model)"),JWe=l(),vs=a("li"),NH=a("strong"),YWe=o("qdqbert"),KWe=o(" \u2014 "),dk=a("a"),ZWe=o("BertTokenizer"),eQe=o(" or "),ck=a("a"),oQe=o("BertTokenizerFast"),rQe=o(" (QDQBert model)"),tQe=l(),Ig=a("li"),DH=a("strong"),aQe=o("rag"),nQe=o(" \u2014 "),fk=a("a"),sQe=o("RagTokenizer"),lQe=o(" (RAG model)"),iQe=l(),Ts=a("li"),qH=a("strong"),dQe=o("reformer"),cQe=o(" \u2014 "),mk=a("a"),fQe=o("ReformerTokenizer"),mQe=o(" or "),gk=a("a"),gQe=o("ReformerTokenizerFast"),hQe=o(" (Reformer model)"),pQe=l(),Fs=a("li"),GH=a("strong"),_Qe=o("rembert"),uQe=o(" \u2014 "),hk=a("a"),bQe=o("RemBertTokenizer"),vQe=o(" or "),pk=a("a"),TQe=o("RemBertTokenizerFast"),FQe=o(" (RemBERT model)"),CQe=l(),Cs=a("li"),OH=a("strong"),MQe=o("retribert"),EQe=o(" \u2014 "),_k=a("a"),yQe=o("RetriBertTokenizer"),wQe=o(" or "),uk=a("a"),AQe=o("RetriBertTokenizerFast"),LQe=o(" (RetriBERT model)"),BQe=l(),Ms=a("li"),XH=a("strong"),kQe=o("roberta"),xQe=o(" \u2014 "),bk=a("a"),RQe=o("RobertaTokenizer"),SQe=o(" or "),vk=a("a"),PQe=o("RobertaTokenizerFast"),$Qe=o(" (RoBERTa model)"),IQe=l(),Es=a("li"),zH=a("strong"),jQe=o("roformer"),NQe=o(" \u2014 "),Tk=a("a"),DQe=o("RoFormerTokenizer"),qQe=o(" or "),Fk=a("a"),GQe=o("RoFormerTokenizerFast"),OQe=o(" (RoFormer model)"),XQe=l(),jg=a("li"),VH=a("strong"),zQe=o("speech_to_text"),VQe=o(" \u2014 "),Ck=a("a"),WQe=o("Speech2TextTokenizer"),QQe=o(" (Speech2Text model)"),HQe=l(),Ng=a("li"),WH=a("strong"),UQe=o("speech_to_text_2"),JQe=o(" \u2014 "),Mk=a("a"),YQe=o("Speech2Text2Tokenizer"),KQe=o(" (Speech2Text2 model)"),ZQe=l(),ys=a("li"),QH=a("strong"),eHe=o("splinter"),oHe=o(" \u2014 "),Ek=a("a"),rHe=o("SplinterTokenizer"),tHe=o(" or "),yk=a("a"),aHe=o("SplinterTokenizerFast"),nHe=o(" (Splinter model)"),sHe=l(),ws=a("li"),HH=a("strong"),lHe=o("squeezebert"),iHe=o(" \u2014 "),wk=a("a"),dHe=o("SqueezeBertTokenizer"),cHe=o(" or "),Ak=a("a"),fHe=o("SqueezeBertTokenizerFast"),mHe=o(" (SqueezeBERT model)"),gHe=l(),As=a("li"),UH=a("strong"),hHe=o("t5"),pHe=o(" \u2014 "),Lk=a("a"),_He=o("T5Tokenizer"),uHe=o(" or "),Bk=a("a"),bHe=o("T5TokenizerFast"),vHe=o(" (T5 model)"),THe=l(),Dg=a("li"),JH=a("strong"),FHe=o("tapas"),CHe=o(" \u2014 "),kk=a("a"),MHe=o("TapasTokenizer"),EHe=o(" (TAPAS model)"),yHe=l(),qg=a("li"),YH=a("strong"),wHe=o("transfo-xl"),AHe=o(" \u2014 "),xk=a("a"),LHe=o("TransfoXLTokenizer"),BHe=o(" (Transformer-XL model)"),kHe=l(),Gg=a("li"),KH=a("strong"),xHe=o("wav2vec2"),RHe=o(" \u2014 "),Rk=a("a"),SHe=o("Wav2Vec2CTCTokenizer"),PHe=o(" (Wav2Vec2 model)"),$He=l(),Og=a("li"),ZH=a("strong"),IHe=o("wav2vec2_phoneme"),jHe=o(" \u2014 "),Sk=a("a"),NHe=o("Wav2Vec2PhonemeCTCTokenizer"),DHe=o(" (Wav2Vec2Phoneme model)"),qHe=l(),Ls=a("li"),eU=a("strong"),GHe=o("xglm"),OHe=o(" \u2014 "),Pk=a("a"),XHe=o("XGLMTokenizer"),zHe=o(" or "),$k=a("a"),VHe=o("XGLMTokenizerFast"),WHe=o(" (XGLM model)"),QHe=l(),Xg=a("li"),oU=a("strong"),HHe=o("xlm"),UHe=o(" \u2014 "),Ik=a("a"),JHe=o("XLMTokenizer"),YHe=o(" (XLM model)"),KHe=l(),zg=a("li"),rU=a("strong"),ZHe=o("xlm-prophetnet"),eUe=o(" \u2014 "),jk=a("a"),oUe=o("XLMProphetNetTokenizer"),rUe=o(" (XLMProphetNet model)"),tUe=l(),Bs=a("li"),tU=a("strong"),aUe=o("xlm-roberta"),nUe=o(" \u2014 "),Nk=a("a"),sUe=o("XLMRobertaTokenizer"),lUe=o(" or "),Dk=a("a"),iUe=o("XLMRobertaTokenizerFast"),dUe=o(" (XLM-RoBERTa model)"),cUe=l(),ks=a("li"),aU=a("strong"),fUe=o("xlnet"),mUe=o(" \u2014 "),qk=a("a"),gUe=o("XLNetTokenizer"),hUe=o(" or "),Gk=a("a"),pUe=o("XLNetTokenizerFast"),_Ue=o(" (XLNet model)"),uUe=l(),nU=a("p"),bUe=o("Examples:"),vUe=l(),f(vE.$$.fragment),TUe=l(),Vg=a("div"),f(TE.$$.fragment),FUe=l(),sU=a("p"),CUe=o("Register a new tokenizer in this mapping."),m8e=l(),ji=a("h2"),Wg=a("a"),lU=a("span"),f(FE.$$.fragment),MUe=l(),iU=a("span"),EUe=o("AutoFeatureExtractor"),g8e=l(),Xo=a("div"),f(CE.$$.fragment),yUe=l(),ME=a("p"),wUe=o(`This is a generic feature extractor class that will be instantiated as one of the feature extractor classes of the
library when created with the `),Ok=a("a"),AUe=o("AutoFeatureExtractor.from_pretrained()"),LUe=o(" class method."),BUe=l(),EE=a("p"),kUe=o("This class cannot be instantiated directly using "),dU=a("code"),xUe=o("__init__()"),RUe=o(" (throws an error)."),SUe=l(),Le=a("div"),f(yE.$$.fragment),PUe=l(),cU=a("p"),$Ue=o("Instantiate one of the feature extractor classes of the library from a pretrained model vocabulary."),IUe=l(),Na=a("p"),jUe=o("The feature extractor class to instantiate is selected based on the "),fU=a("code"),NUe=o("model_type"),DUe=o(` property of the config object
(either passed as an argument or loaded from `),mU=a("code"),qUe=o("pretrained_model_name_or_path"),GUe=o(` if possible), or when it\u2019s
missing, by falling back to using pattern matching on `),gU=a("code"),OUe=o("pretrained_model_name_or_path"),XUe=o(":"),zUe=l(),se=a("ul"),Qg=a("li"),hU=a("strong"),VUe=o("beit"),WUe=o(" \u2014 "),Xk=a("a"),QUe=o("BeitFeatureExtractor"),HUe=o(" (BEiT model)"),UUe=l(),Hg=a("li"),pU=a("strong"),JUe=o("clip"),YUe=o(" \u2014 "),zk=a("a"),KUe=o("CLIPFeatureExtractor"),ZUe=o(" (CLIP model)"),eJe=l(),Ug=a("li"),_U=a("strong"),oJe=o("convnext"),rJe=o(" \u2014 "),Vk=a("a"),tJe=o("ConvNextFeatureExtractor"),aJe=o(" (ConvNext model)"),nJe=l(),Jg=a("li"),uU=a("strong"),sJe=o("deit"),lJe=o(" \u2014 "),Wk=a("a"),iJe=o("DeiTFeatureExtractor"),dJe=o(" (DeiT model)"),cJe=l(),Yg=a("li"),bU=a("strong"),fJe=o("detr"),mJe=o(" \u2014 "),Qk=a("a"),gJe=o("DetrFeatureExtractor"),hJe=o(" (DETR model)"),pJe=l(),Kg=a("li"),vU=a("strong"),_Je=o("hubert"),uJe=o(" \u2014 "),Hk=a("a"),bJe=o("Wav2Vec2FeatureExtractor"),vJe=o(" (Hubert model)"),TJe=l(),Zg=a("li"),TU=a("strong"),FJe=o("layoutlmv2"),CJe=o(" \u2014 "),Uk=a("a"),MJe=o("LayoutLMv2FeatureExtractor"),EJe=o(" (LayoutLMv2 model)"),yJe=l(),eh=a("li"),FU=a("strong"),wJe=o("perceiver"),AJe=o(" \u2014 "),Jk=a("a"),LJe=o("PerceiverFeatureExtractor"),BJe=o(" (Perceiver model)"),kJe=l(),oh=a("li"),CU=a("strong"),xJe=o("poolformer"),RJe=o(" \u2014 "),Yk=a("a"),SJe=o("PoolFormerFeatureExtractor"),PJe=o(" (PoolFormer model)"),$Je=l(),rh=a("li"),MU=a("strong"),IJe=o("segformer"),jJe=o(" \u2014 "),Kk=a("a"),NJe=o("SegformerFeatureExtractor"),DJe=o(" (SegFormer model)"),qJe=l(),th=a("li"),EU=a("strong"),GJe=o("speech_to_text"),OJe=o(" \u2014 "),Zk=a("a"),XJe=o("Speech2TextFeatureExtractor"),zJe=o(" (Speech2Text model)"),VJe=l(),ah=a("li"),yU=a("strong"),WJe=o("swin"),QJe=o(" \u2014 "),ex=a("a"),HJe=o("ViTFeatureExtractor"),UJe=o(" (Swin model)"),JJe=l(),nh=a("li"),wU=a("strong"),YJe=o("vit"),KJe=o(" \u2014 "),ox=a("a"),ZJe=o("ViTFeatureExtractor"),eYe=o(" (ViT model)"),oYe=l(),sh=a("li"),AU=a("strong"),rYe=o("vit_mae"),tYe=o(" \u2014 "),rx=a("a"),aYe=o("ViTFeatureExtractor"),nYe=o(" (ViTMAE model)"),sYe=l(),lh=a("li"),LU=a("strong"),lYe=o("wav2vec2"),iYe=o(" \u2014 "),tx=a("a"),dYe=o("Wav2Vec2FeatureExtractor"),cYe=o(" (Wav2Vec2 model)"),fYe=l(),f(ih.$$.fragment),mYe=l(),BU=a("p"),gYe=o("Examples:"),hYe=l(),f(wE.$$.fragment),pYe=l(),dh=a("div"),f(AE.$$.fragment),_Ye=l(),kU=a("p"),uYe=o("Register a new feature extractor for this class."),h8e=l(),Ni=a("h2"),ch=a("a"),xU=a("span"),f(LE.$$.fragment),bYe=l(),RU=a("span"),vYe=o("AutoProcessor"),p8e=l(),zo=a("div"),f(BE.$$.fragment),TYe=l(),kE=a("p"),FYe=o(`This is a generic processor class that will be instantiated as one of the processor classes of the library when
created with the `),ax=a("a"),CYe=o("AutoProcessor.from_pretrained()"),MYe=o(" class method."),EYe=l(),xE=a("p"),yYe=o("This class cannot be instantiated directly using "),SU=a("code"),wYe=o("__init__()"),AYe=o(" (throws an error)."),LYe=l(),Be=a("div"),f(RE.$$.fragment),BYe=l(),PU=a("p"),kYe=o("Instantiate one of the processor classes of the library from a pretrained model vocabulary."),xYe=l(),Di=a("p"),RYe=o("The processor class to instantiate is selected based on the "),$U=a("code"),SYe=o("model_type"),PYe=o(` property of the config object (either
passed as an argument or loaded from `),IU=a("code"),$Ye=o("pretrained_model_name_or_path"),IYe=o(" if possible):"),jYe=l(),we=a("ul"),fh=a("li"),jU=a("strong"),NYe=o("clip"),DYe=o(" \u2014 "),nx=a("a"),qYe=o("CLIPProcessor"),GYe=o(" (CLIP model)"),OYe=l(),mh=a("li"),NU=a("strong"),XYe=o("layoutlmv2"),zYe=o(" \u2014 "),sx=a("a"),VYe=o("LayoutLMv2Processor"),WYe=o(" (LayoutLMv2 model)"),QYe=l(),gh=a("li"),DU=a("strong"),HYe=o("layoutxlm"),UYe=o(" \u2014 "),lx=a("a"),JYe=o("LayoutXLMProcessor"),YYe=o(" (LayoutXLM model)"),KYe=l(),hh=a("li"),qU=a("strong"),ZYe=o("speech_to_text"),eKe=o(" \u2014 "),ix=a("a"),oKe=o("Speech2TextProcessor"),rKe=o(" (Speech2Text model)"),tKe=l(),ph=a("li"),GU=a("strong"),aKe=o("speech_to_text_2"),nKe=o(" \u2014 "),dx=a("a"),sKe=o("Speech2Text2Processor"),lKe=o(" (Speech2Text2 model)"),iKe=l(),_h=a("li"),OU=a("strong"),dKe=o("trocr"),cKe=o(" \u2014 "),cx=a("a"),fKe=o("TrOCRProcessor"),mKe=o(" (TrOCR model)"),gKe=l(),uh=a("li"),XU=a("strong"),hKe=o("vision-text-dual-encoder"),pKe=o(" \u2014 "),fx=a("a"),_Ke=o("VisionTextDualEncoderProcessor"),uKe=o(" (VisionTextDualEncoder model)"),bKe=l(),bh=a("li"),zU=a("strong"),vKe=o("wav2vec2"),TKe=o(" \u2014 "),mx=a("a"),FKe=o("Wav2Vec2Processor"),CKe=o(" (Wav2Vec2 model)"),MKe=l(),f(vh.$$.fragment),EKe=l(),VU=a("p"),yKe=o("Examples:"),wKe=l(),f(SE.$$.fragment),AKe=l(),Th=a("div"),f(PE.$$.fragment),LKe=l(),WU=a("p"),BKe=o("Register a new processor for this class."),_8e=l(),qi=a("h2"),Fh=a("a"),QU=a("span"),f($E.$$.fragment),kKe=l(),HU=a("span"),xKe=o("AutoModel"),u8e=l(),Vo=a("div"),f(IE.$$.fragment),RKe=l(),Gi=a("p"),SKe=o(`This is a generic model class that will be instantiated as one of the base model classes of the library when created
with the `),UU=a("code"),PKe=o("from_pretrained()"),$Ke=o("class method or the "),JU=a("code"),IKe=o("from_config()"),jKe=o(`class
method.`),NKe=l(),jE=a("p"),DKe=o("This class cannot be instantiated directly using "),YU=a("code"),qKe=o("__init__()"),GKe=o(" (throws an error)."),OKe=l(),Nr=a("div"),f(NE.$$.fragment),XKe=l(),KU=a("p"),zKe=o("Instantiates one of the base model classes of the library from a configuration."),VKe=l(),Oi=a("p"),WKe=o(`Note:
Loading a model from its configuration file does `),ZU=a("strong"),QKe=o("not"),HKe=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),eJ=a("code"),UKe=o("from_pretrained()"),JKe=o("to load the model weights."),YKe=l(),oJ=a("p"),KKe=o("Examples:"),ZKe=l(),f(DE.$$.fragment),eZe=l(),ke=a("div"),f(qE.$$.fragment),oZe=l(),rJ=a("p"),rZe=o("Instantiate one of the base model classes of the library from a pretrained model."),tZe=l(),Da=a("p"),aZe=o("The model class to instantiate is selected based on the "),tJ=a("code"),nZe=o("model_type"),sZe=o(` property of the config object (either
passed as an argument or loaded from `),aJ=a("code"),lZe=o("pretrained_model_name_or_path"),iZe=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),nJ=a("code"),dZe=o("pretrained_model_name_or_path"),cZe=o(":"),fZe=l(),F=a("ul"),Ch=a("li"),sJ=a("strong"),mZe=o("albert"),gZe=o(" \u2014 "),gx=a("a"),hZe=o("AlbertModel"),pZe=o(" (ALBERT model)"),_Ze=l(),Mh=a("li"),lJ=a("strong"),uZe=o("bart"),bZe=o(" \u2014 "),hx=a("a"),vZe=o("BartModel"),TZe=o(" (BART model)"),FZe=l(),Eh=a("li"),iJ=a("strong"),CZe=o("beit"),MZe=o(" \u2014 "),px=a("a"),EZe=o("BeitModel"),yZe=o(" (BEiT model)"),wZe=l(),yh=a("li"),dJ=a("strong"),AZe=o("bert"),LZe=o(" \u2014 "),_x=a("a"),BZe=o("BertModel"),kZe=o(" (BERT model)"),xZe=l(),wh=a("li"),cJ=a("strong"),RZe=o("bert-generation"),SZe=o(" \u2014 "),ux=a("a"),PZe=o("BertGenerationEncoder"),$Ze=o(" (Bert Generation model)"),IZe=l(),Ah=a("li"),fJ=a("strong"),jZe=o("big_bird"),NZe=o(" \u2014 "),bx=a("a"),DZe=o("BigBirdModel"),qZe=o(" (BigBird model)"),GZe=l(),Lh=a("li"),mJ=a("strong"),OZe=o("bigbird_pegasus"),XZe=o(" \u2014 "),vx=a("a"),zZe=o("BigBirdPegasusModel"),VZe=o(" (BigBirdPegasus model)"),WZe=l(),Bh=a("li"),gJ=a("strong"),QZe=o("blenderbot"),HZe=o(" \u2014 "),Tx=a("a"),UZe=o("BlenderbotModel"),JZe=o(" (Blenderbot model)"),YZe=l(),kh=a("li"),hJ=a("strong"),KZe=o("blenderbot-small"),ZZe=o(" \u2014 "),Fx=a("a"),eeo=o("BlenderbotSmallModel"),oeo=o(" (BlenderbotSmall model)"),reo=l(),xh=a("li"),pJ=a("strong"),teo=o("camembert"),aeo=o(" \u2014 "),Cx=a("a"),neo=o("CamembertModel"),seo=o(" (CamemBERT model)"),leo=l(),Rh=a("li"),_J=a("strong"),ieo=o("canine"),deo=o(" \u2014 "),Mx=a("a"),ceo=o("CanineModel"),feo=o(" (Canine model)"),meo=l(),Sh=a("li"),uJ=a("strong"),geo=o("clip"),heo=o(" \u2014 "),Ex=a("a"),peo=o("CLIPModel"),_eo=o(" (CLIP model)"),ueo=l(),Ph=a("li"),bJ=a("strong"),beo=o("convbert"),veo=o(" \u2014 "),yx=a("a"),Teo=o("ConvBertModel"),Feo=o(" (ConvBERT model)"),Ceo=l(),$h=a("li"),vJ=a("strong"),Meo=o("convnext"),Eeo=o(" \u2014 "),wx=a("a"),yeo=o("ConvNextModel"),weo=o(" (ConvNext model)"),Aeo=l(),Ih=a("li"),TJ=a("strong"),Leo=o("ctrl"),Beo=o(" \u2014 "),Ax=a("a"),keo=o("CTRLModel"),xeo=o(" (CTRL model)"),Reo=l(),jh=a("li"),FJ=a("strong"),Seo=o("deberta"),Peo=o(" \u2014 "),Lx=a("a"),$eo=o("DebertaModel"),Ieo=o(" (DeBERTa model)"),jeo=l(),Nh=a("li"),CJ=a("strong"),Neo=o("deberta-v2"),Deo=o(" \u2014 "),Bx=a("a"),qeo=o("DebertaV2Model"),Geo=o(" (DeBERTa-v2 model)"),Oeo=l(),Dh=a("li"),MJ=a("strong"),Xeo=o("deit"),zeo=o(" \u2014 "),kx=a("a"),Veo=o("DeiTModel"),Weo=o(" (DeiT model)"),Qeo=l(),qh=a("li"),EJ=a("strong"),Heo=o("detr"),Ueo=o(" \u2014 "),xx=a("a"),Jeo=o("DetrModel"),Yeo=o(" (DETR model)"),Keo=l(),Gh=a("li"),yJ=a("strong"),Zeo=o("distilbert"),eoo=o(" \u2014 "),Rx=a("a"),ooo=o("DistilBertModel"),roo=o(" (DistilBERT model)"),too=l(),Oh=a("li"),wJ=a("strong"),aoo=o("dpr"),noo=o(" \u2014 "),Sx=a("a"),soo=o("DPRQuestionEncoder"),loo=o(" (DPR model)"),ioo=l(),Xh=a("li"),AJ=a("strong"),doo=o("electra"),coo=o(" \u2014 "),Px=a("a"),foo=o("ElectraModel"),moo=o(" (ELECTRA model)"),goo=l(),zh=a("li"),LJ=a("strong"),hoo=o("flaubert"),poo=o(" \u2014 "),$x=a("a"),_oo=o("FlaubertModel"),uoo=o(" (FlauBERT model)"),boo=l(),Vh=a("li"),BJ=a("strong"),voo=o("fnet"),Too=o(" \u2014 "),Ix=a("a"),Foo=o("FNetModel"),Coo=o(" (FNet model)"),Moo=l(),Wh=a("li"),kJ=a("strong"),Eoo=o("fsmt"),yoo=o(" \u2014 "),jx=a("a"),woo=o("FSMTModel"),Aoo=o(" (FairSeq Machine-Translation model)"),Loo=l(),xs=a("li"),xJ=a("strong"),Boo=o("funnel"),koo=o(" \u2014 "),Nx=a("a"),xoo=o("FunnelModel"),Roo=o(" or "),Dx=a("a"),Soo=o("FunnelBaseModel"),Poo=o(" (Funnel Transformer model)"),$oo=l(),Qh=a("li"),RJ=a("strong"),Ioo=o("gpt2"),joo=o(" \u2014 "),qx=a("a"),Noo=o("GPT2Model"),Doo=o(" (OpenAI GPT-2 model)"),qoo=l(),Hh=a("li"),SJ=a("strong"),Goo=o("gpt_neo"),Ooo=o(" \u2014 "),Gx=a("a"),Xoo=o("GPTNeoModel"),zoo=o(" (GPT Neo model)"),Voo=l(),Uh=a("li"),PJ=a("strong"),Woo=o("gptj"),Qoo=o(" \u2014 "),Ox=a("a"),Hoo=o("GPTJModel"),Uoo=o(" (GPT-J model)"),Joo=l(),Jh=a("li"),$J=a("strong"),Yoo=o("hubert"),Koo=o(" \u2014 "),Xx=a("a"),Zoo=o("HubertModel"),ero=o(" (Hubert model)"),oro=l(),Yh=a("li"),IJ=a("strong"),rro=o("ibert"),tro=o(" \u2014 "),zx=a("a"),aro=o("IBertModel"),nro=o(" (I-BERT model)"),sro=l(),Kh=a("li"),jJ=a("strong"),lro=o("imagegpt"),iro=o(" \u2014 "),Vx=a("a"),dro=o("ImageGPTModel"),cro=o(" (ImageGPT model)"),fro=l(),Zh=a("li"),NJ=a("strong"),mro=o("layoutlm"),gro=o(" \u2014 "),Wx=a("a"),hro=o("LayoutLMModel"),pro=o(" (LayoutLM model)"),_ro=l(),ep=a("li"),DJ=a("strong"),uro=o("layoutlmv2"),bro=o(" \u2014 "),Qx=a("a"),vro=o("LayoutLMv2Model"),Tro=o(" (LayoutLMv2 model)"),Fro=l(),op=a("li"),qJ=a("strong"),Cro=o("led"),Mro=o(" \u2014 "),Hx=a("a"),Ero=o("LEDModel"),yro=o(" (LED model)"),wro=l(),rp=a("li"),GJ=a("strong"),Aro=o("longformer"),Lro=o(" \u2014 "),Ux=a("a"),Bro=o("LongformerModel"),kro=o(" (Longformer model)"),xro=l(),tp=a("li"),OJ=a("strong"),Rro=o("luke"),Sro=o(" \u2014 "),Jx=a("a"),Pro=o("LukeModel"),$ro=o(" (LUKE model)"),Iro=l(),ap=a("li"),XJ=a("strong"),jro=o("lxmert"),Nro=o(" \u2014 "),Yx=a("a"),Dro=o("LxmertModel"),qro=o(" (LXMERT model)"),Gro=l(),np=a("li"),zJ=a("strong"),Oro=o("m2m_100"),Xro=o(" \u2014 "),Kx=a("a"),zro=o("M2M100Model"),Vro=o(" (M2M100 model)"),Wro=l(),sp=a("li"),VJ=a("strong"),Qro=o("marian"),Hro=o(" \u2014 "),Zx=a("a"),Uro=o("MarianModel"),Jro=o(" (Marian model)"),Yro=l(),lp=a("li"),WJ=a("strong"),Kro=o("mbart"),Zro=o(" \u2014 "),eR=a("a"),eto=o("MBartModel"),oto=o(" (mBART model)"),rto=l(),ip=a("li"),QJ=a("strong"),tto=o("megatron-bert"),ato=o(" \u2014 "),oR=a("a"),nto=o("MegatronBertModel"),sto=o(" (MegatronBert model)"),lto=l(),dp=a("li"),HJ=a("strong"),ito=o("mobilebert"),dto=o(" \u2014 "),rR=a("a"),cto=o("MobileBertModel"),fto=o(" (MobileBERT model)"),mto=l(),cp=a("li"),UJ=a("strong"),gto=o("mpnet"),hto=o(" \u2014 "),tR=a("a"),pto=o("MPNetModel"),_to=o(" (MPNet model)"),uto=l(),fp=a("li"),JJ=a("strong"),bto=o("mt5"),vto=o(" \u2014 "),aR=a("a"),Tto=o("MT5Model"),Fto=o(" (mT5 model)"),Cto=l(),mp=a("li"),YJ=a("strong"),Mto=o("nystromformer"),Eto=o(" \u2014 "),nR=a("a"),yto=o("NystromformerModel"),wto=o(" (Nystromformer model)"),Ato=l(),gp=a("li"),KJ=a("strong"),Lto=o("openai-gpt"),Bto=o(" \u2014 "),sR=a("a"),kto=o("OpenAIGPTModel"),xto=o(" (OpenAI GPT model)"),Rto=l(),hp=a("li"),ZJ=a("strong"),Sto=o("pegasus"),Pto=o(" \u2014 "),lR=a("a"),$to=o("PegasusModel"),Ito=o(" (Pegasus model)"),jto=l(),pp=a("li"),eY=a("strong"),Nto=o("perceiver"),Dto=o(" \u2014 "),iR=a("a"),qto=o("PerceiverModel"),Gto=o(" (Perceiver model)"),Oto=l(),_p=a("li"),oY=a("strong"),Xto=o("plbart"),zto=o(" \u2014 "),dR=a("a"),Vto=o("PLBartModel"),Wto=o(" (PLBart model)"),Qto=l(),up=a("li"),rY=a("strong"),Hto=o("poolformer"),Uto=o(" \u2014 "),cR=a("a"),Jto=o("PoolFormerModel"),Yto=o(" (PoolFormer model)"),Kto=l(),bp=a("li"),tY=a("strong"),Zto=o("prophetnet"),eao=o(" \u2014 "),fR=a("a"),oao=o("ProphetNetModel"),rao=o(" (ProphetNet model)"),tao=l(),vp=a("li"),aY=a("strong"),aao=o("qdqbert"),nao=o(" \u2014 "),mR=a("a"),sao=o("QDQBertModel"),lao=o(" (QDQBert model)"),iao=l(),Tp=a("li"),nY=a("strong"),dao=o("reformer"),cao=o(" \u2014 "),gR=a("a"),fao=o("ReformerModel"),mao=o(" (Reformer model)"),gao=l(),Fp=a("li"),sY=a("strong"),hao=o("rembert"),pao=o(" \u2014 "),hR=a("a"),_ao=o("RemBertModel"),uao=o(" (RemBERT model)"),bao=l(),Cp=a("li"),lY=a("strong"),vao=o("retribert"),Tao=o(" \u2014 "),pR=a("a"),Fao=o("RetriBertModel"),Cao=o(" (RetriBERT model)"),Mao=l(),Mp=a("li"),iY=a("strong"),Eao=o("roberta"),yao=o(" \u2014 "),_R=a("a"),wao=o("RobertaModel"),Aao=o(" (RoBERTa model)"),Lao=l(),Ep=a("li"),dY=a("strong"),Bao=o("roformer"),kao=o(" \u2014 "),uR=a("a"),xao=o("RoFormerModel"),Rao=o(" (RoFormer model)"),Sao=l(),yp=a("li"),cY=a("strong"),Pao=o("segformer"),$ao=o(" \u2014 "),bR=a("a"),Iao=o("SegformerModel"),jao=o(" (SegFormer model)"),Nao=l(),wp=a("li"),fY=a("strong"),Dao=o("sew"),qao=o(" \u2014 "),vR=a("a"),Gao=o("SEWModel"),Oao=o(" (SEW model)"),Xao=l(),Ap=a("li"),mY=a("strong"),zao=o("sew-d"),Vao=o(" \u2014 "),TR=a("a"),Wao=o("SEWDModel"),Qao=o(" (SEW-D model)"),Hao=l(),Lp=a("li"),gY=a("strong"),Uao=o("speech_to_text"),Jao=o(" \u2014 "),FR=a("a"),Yao=o("Speech2TextModel"),Kao=o(" (Speech2Text model)"),Zao=l(),Bp=a("li"),hY=a("strong"),eno=o("splinter"),ono=o(" \u2014 "),CR=a("a"),rno=o("SplinterModel"),tno=o(" (Splinter model)"),ano=l(),kp=a("li"),pY=a("strong"),nno=o("squeezebert"),sno=o(" \u2014 "),MR=a("a"),lno=o("SqueezeBertModel"),ino=o(" (SqueezeBERT model)"),dno=l(),xp=a("li"),_Y=a("strong"),cno=o("swin"),fno=o(" \u2014 "),ER=a("a"),mno=o("SwinModel"),gno=o(" (Swin model)"),hno=l(),Rp=a("li"),uY=a("strong"),pno=o("t5"),_no=o(" \u2014 "),yR=a("a"),uno=o("T5Model"),bno=o(" (T5 model)"),vno=l(),Sp=a("li"),bY=a("strong"),Tno=o("tapas"),Fno=o(" \u2014 "),wR=a("a"),Cno=o("TapasModel"),Mno=o(" (TAPAS model)"),Eno=l(),Pp=a("li"),vY=a("strong"),yno=o("transfo-xl"),wno=o(" \u2014 "),AR=a("a"),Ano=o("TransfoXLModel"),Lno=o(" (Transformer-XL model)"),Bno=l(),$p=a("li"),TY=a("strong"),kno=o("unispeech"),xno=o(" \u2014 "),LR=a("a"),Rno=o("UniSpeechModel"),Sno=o(" (UniSpeech model)"),Pno=l(),Ip=a("li"),FY=a("strong"),$no=o("unispeech-sat"),Ino=o(" \u2014 "),BR=a("a"),jno=o("UniSpeechSatModel"),Nno=o(" (UniSpeechSat model)"),Dno=l(),jp=a("li"),CY=a("strong"),qno=o("vilt"),Gno=o(" \u2014 "),kR=a("a"),Ono=o("ViltModel"),Xno=o(" (ViLT model)"),zno=l(),Np=a("li"),MY=a("strong"),Vno=o("vision-text-dual-encoder"),Wno=o(" \u2014 "),xR=a("a"),Qno=o("VisionTextDualEncoderModel"),Hno=o(" (VisionTextDualEncoder model)"),Uno=l(),Dp=a("li"),EY=a("strong"),Jno=o("visual_bert"),Yno=o(" \u2014 "),RR=a("a"),Kno=o("VisualBertModel"),Zno=o(" (VisualBert model)"),eso=l(),qp=a("li"),yY=a("strong"),oso=o("vit"),rso=o(" \u2014 "),SR=a("a"),tso=o("ViTModel"),aso=o(" (ViT model)"),nso=l(),Gp=a("li"),wY=a("strong"),sso=o("vit_mae"),lso=o(" \u2014 "),PR=a("a"),iso=o("ViTMAEModel"),dso=o(" (ViTMAE model)"),cso=l(),Op=a("li"),AY=a("strong"),fso=o("wav2vec2"),mso=o(" \u2014 "),$R=a("a"),gso=o("Wav2Vec2Model"),hso=o(" (Wav2Vec2 model)"),pso=l(),Xp=a("li"),LY=a("strong"),_so=o("wavlm"),uso=o(" \u2014 "),IR=a("a"),bso=o("WavLMModel"),vso=o(" (WavLM model)"),Tso=l(),zp=a("li"),BY=a("strong"),Fso=o("xglm"),Cso=o(" \u2014 "),jR=a("a"),Mso=o("XGLMModel"),Eso=o(" (XGLM model)"),yso=l(),Vp=a("li"),kY=a("strong"),wso=o("xlm"),Aso=o(" \u2014 "),NR=a("a"),Lso=o("XLMModel"),Bso=o(" (XLM model)"),kso=l(),Wp=a("li"),xY=a("strong"),xso=o("xlm-prophetnet"),Rso=o(" \u2014 "),DR=a("a"),Sso=o("XLMProphetNetModel"),Pso=o(" (XLMProphetNet model)"),$so=l(),Qp=a("li"),RY=a("strong"),Iso=o("xlm-roberta"),jso=o(" \u2014 "),qR=a("a"),Nso=o("XLMRobertaModel"),Dso=o(" (XLM-RoBERTa model)"),qso=l(),Hp=a("li"),SY=a("strong"),Gso=o("xlm-roberta-xl"),Oso=o(" \u2014 "),GR=a("a"),Xso=o("XLMRobertaXLModel"),zso=o(" (XLM-RoBERTa-XL model)"),Vso=l(),Up=a("li"),PY=a("strong"),Wso=o("xlnet"),Qso=o(" \u2014 "),OR=a("a"),Hso=o("XLNetModel"),Uso=o(" (XLNet model)"),Jso=l(),Jp=a("li"),$Y=a("strong"),Yso=o("yoso"),Kso=o(" \u2014 "),XR=a("a"),Zso=o("YosoModel"),elo=o(" (YOSO model)"),olo=l(),Yp=a("p"),rlo=o("The model is set in evaluation mode by default using "),IY=a("code"),tlo=o("model.eval()"),alo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),jY=a("code"),nlo=o("model.train()"),slo=l(),NY=a("p"),llo=o("Examples:"),ilo=l(),f(GE.$$.fragment),b8e=l(),Xi=a("h2"),Kp=a("a"),DY=a("span"),f(OE.$$.fragment),dlo=l(),qY=a("span"),clo=o("AutoModelForPreTraining"),v8e=l(),Wo=a("div"),f(XE.$$.fragment),flo=l(),zi=a("p"),mlo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a pretraining head) when created
with the `),GY=a("code"),glo=o("from_pretrained()"),hlo=o("class method or the "),OY=a("code"),plo=o("from_config()"),_lo=o(`class
method.`),ulo=l(),zE=a("p"),blo=o("This class cannot be instantiated directly using "),XY=a("code"),vlo=o("__init__()"),Tlo=o(" (throws an error)."),Flo=l(),Dr=a("div"),f(VE.$$.fragment),Clo=l(),zY=a("p"),Mlo=o("Instantiates one of the model classes of the library (with a pretraining head) from a configuration."),Elo=l(),Vi=a("p"),ylo=o(`Note:
Loading a model from its configuration file does `),VY=a("strong"),wlo=o("not"),Alo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),WY=a("code"),Llo=o("from_pretrained()"),Blo=o("to load the model weights."),klo=l(),QY=a("p"),xlo=o("Examples:"),Rlo=l(),f(WE.$$.fragment),Slo=l(),xe=a("div"),f(QE.$$.fragment),Plo=l(),HY=a("p"),$lo=o("Instantiate one of the model classes of the library (with a pretraining head) from a pretrained model."),Ilo=l(),qa=a("p"),jlo=o("The model class to instantiate is selected based on the "),UY=a("code"),Nlo=o("model_type"),Dlo=o(` property of the config object (either
passed as an argument or loaded from `),JY=a("code"),qlo=o("pretrained_model_name_or_path"),Glo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),YY=a("code"),Olo=o("pretrained_model_name_or_path"),Xlo=o(":"),zlo=l(),x=a("ul"),Zp=a("li"),KY=a("strong"),Vlo=o("albert"),Wlo=o(" \u2014 "),zR=a("a"),Qlo=o("AlbertForPreTraining"),Hlo=o(" (ALBERT model)"),Ulo=l(),e_=a("li"),ZY=a("strong"),Jlo=o("bart"),Ylo=o(" \u2014 "),VR=a("a"),Klo=o("BartForConditionalGeneration"),Zlo=o(" (BART model)"),eio=l(),o_=a("li"),eK=a("strong"),oio=o("bert"),rio=o(" \u2014 "),WR=a("a"),tio=o("BertForPreTraining"),aio=o(" (BERT model)"),nio=l(),r_=a("li"),oK=a("strong"),sio=o("big_bird"),lio=o(" \u2014 "),QR=a("a"),iio=o("BigBirdForPreTraining"),dio=o(" (BigBird model)"),cio=l(),t_=a("li"),rK=a("strong"),fio=o("camembert"),mio=o(" \u2014 "),HR=a("a"),gio=o("CamembertForMaskedLM"),hio=o(" (CamemBERT model)"),pio=l(),a_=a("li"),tK=a("strong"),_io=o("ctrl"),uio=o(" \u2014 "),UR=a("a"),bio=o("CTRLLMHeadModel"),vio=o(" (CTRL model)"),Tio=l(),n_=a("li"),aK=a("strong"),Fio=o("deberta"),Cio=o(" \u2014 "),JR=a("a"),Mio=o("DebertaForMaskedLM"),Eio=o(" (DeBERTa model)"),yio=l(),s_=a("li"),nK=a("strong"),wio=o("deberta-v2"),Aio=o(" \u2014 "),YR=a("a"),Lio=o("DebertaV2ForMaskedLM"),Bio=o(" (DeBERTa-v2 model)"),kio=l(),l_=a("li"),sK=a("strong"),xio=o("distilbert"),Rio=o(" \u2014 "),KR=a("a"),Sio=o("DistilBertForMaskedLM"),Pio=o(" (DistilBERT model)"),$io=l(),i_=a("li"),lK=a("strong"),Iio=o("electra"),jio=o(" \u2014 "),ZR=a("a"),Nio=o("ElectraForPreTraining"),Dio=o(" (ELECTRA model)"),qio=l(),d_=a("li"),iK=a("strong"),Gio=o("flaubert"),Oio=o(" \u2014 "),eS=a("a"),Xio=o("FlaubertWithLMHeadModel"),zio=o(" (FlauBERT model)"),Vio=l(),c_=a("li"),dK=a("strong"),Wio=o("fnet"),Qio=o(" \u2014 "),oS=a("a"),Hio=o("FNetForPreTraining"),Uio=o(" (FNet model)"),Jio=l(),f_=a("li"),cK=a("strong"),Yio=o("fsmt"),Kio=o(" \u2014 "),rS=a("a"),Zio=o("FSMTForConditionalGeneration"),edo=o(" (FairSeq Machine-Translation model)"),odo=l(),m_=a("li"),fK=a("strong"),rdo=o("funnel"),tdo=o(" \u2014 "),tS=a("a"),ado=o("FunnelForPreTraining"),ndo=o(" (Funnel Transformer model)"),sdo=l(),g_=a("li"),mK=a("strong"),ldo=o("gpt2"),ido=o(" \u2014 "),aS=a("a"),ddo=o("GPT2LMHeadModel"),cdo=o(" (OpenAI GPT-2 model)"),fdo=l(),h_=a("li"),gK=a("strong"),mdo=o("ibert"),gdo=o(" \u2014 "),nS=a("a"),hdo=o("IBertForMaskedLM"),pdo=o(" (I-BERT model)"),_do=l(),p_=a("li"),hK=a("strong"),udo=o("layoutlm"),bdo=o(" \u2014 "),sS=a("a"),vdo=o("LayoutLMForMaskedLM"),Tdo=o(" (LayoutLM model)"),Fdo=l(),__=a("li"),pK=a("strong"),Cdo=o("longformer"),Mdo=o(" \u2014 "),lS=a("a"),Edo=o("LongformerForMaskedLM"),ydo=o(" (Longformer model)"),wdo=l(),u_=a("li"),_K=a("strong"),Ado=o("lxmert"),Ldo=o(" \u2014 "),iS=a("a"),Bdo=o("LxmertForPreTraining"),kdo=o(" (LXMERT model)"),xdo=l(),b_=a("li"),uK=a("strong"),Rdo=o("megatron-bert"),Sdo=o(" \u2014 "),dS=a("a"),Pdo=o("MegatronBertForPreTraining"),$do=o(" (MegatronBert model)"),Ido=l(),v_=a("li"),bK=a("strong"),jdo=o("mobilebert"),Ndo=o(" \u2014 "),cS=a("a"),Ddo=o("MobileBertForPreTraining"),qdo=o(" (MobileBERT model)"),Gdo=l(),T_=a("li"),vK=a("strong"),Odo=o("mpnet"),Xdo=o(" \u2014 "),fS=a("a"),zdo=o("MPNetForMaskedLM"),Vdo=o(" (MPNet model)"),Wdo=l(),F_=a("li"),TK=a("strong"),Qdo=o("openai-gpt"),Hdo=o(" \u2014 "),mS=a("a"),Udo=o("OpenAIGPTLMHeadModel"),Jdo=o(" (OpenAI GPT model)"),Ydo=l(),C_=a("li"),FK=a("strong"),Kdo=o("retribert"),Zdo=o(" \u2014 "),gS=a("a"),eco=o("RetriBertModel"),oco=o(" (RetriBERT model)"),rco=l(),M_=a("li"),CK=a("strong"),tco=o("roberta"),aco=o(" \u2014 "),hS=a("a"),nco=o("RobertaForMaskedLM"),sco=o(" (RoBERTa model)"),lco=l(),E_=a("li"),MK=a("strong"),ico=o("squeezebert"),dco=o(" \u2014 "),pS=a("a"),cco=o("SqueezeBertForMaskedLM"),fco=o(" (SqueezeBERT model)"),mco=l(),y_=a("li"),EK=a("strong"),gco=o("t5"),hco=o(" \u2014 "),_S=a("a"),pco=o("T5ForConditionalGeneration"),_co=o(" (T5 model)"),uco=l(),w_=a("li"),yK=a("strong"),bco=o("tapas"),vco=o(" \u2014 "),uS=a("a"),Tco=o("TapasForMaskedLM"),Fco=o(" (TAPAS model)"),Cco=l(),A_=a("li"),wK=a("strong"),Mco=o("transfo-xl"),Eco=o(" \u2014 "),bS=a("a"),yco=o("TransfoXLLMHeadModel"),wco=o(" (Transformer-XL model)"),Aco=l(),L_=a("li"),AK=a("strong"),Lco=o("unispeech"),Bco=o(" \u2014 "),vS=a("a"),kco=o("UniSpeechForPreTraining"),xco=o(" (UniSpeech model)"),Rco=l(),B_=a("li"),LK=a("strong"),Sco=o("unispeech-sat"),Pco=o(" \u2014 "),TS=a("a"),$co=o("UniSpeechSatForPreTraining"),Ico=o(" (UniSpeechSat model)"),jco=l(),k_=a("li"),BK=a("strong"),Nco=o("visual_bert"),Dco=o(" \u2014 "),FS=a("a"),qco=o("VisualBertForPreTraining"),Gco=o(" (VisualBert model)"),Oco=l(),x_=a("li"),kK=a("strong"),Xco=o("vit_mae"),zco=o(" \u2014 "),CS=a("a"),Vco=o("ViTMAEForPreTraining"),Wco=o(" (ViTMAE model)"),Qco=l(),R_=a("li"),xK=a("strong"),Hco=o("wav2vec2"),Uco=o(" \u2014 "),MS=a("a"),Jco=o("Wav2Vec2ForPreTraining"),Yco=o(" (Wav2Vec2 model)"),Kco=l(),S_=a("li"),RK=a("strong"),Zco=o("xlm"),efo=o(" \u2014 "),ES=a("a"),ofo=o("XLMWithLMHeadModel"),rfo=o(" (XLM model)"),tfo=l(),P_=a("li"),SK=a("strong"),afo=o("xlm-roberta"),nfo=o(" \u2014 "),yS=a("a"),sfo=o("XLMRobertaForMaskedLM"),lfo=o(" (XLM-RoBERTa model)"),ifo=l(),$_=a("li"),PK=a("strong"),dfo=o("xlm-roberta-xl"),cfo=o(" \u2014 "),wS=a("a"),ffo=o("XLMRobertaXLForMaskedLM"),mfo=o(" (XLM-RoBERTa-XL model)"),gfo=l(),I_=a("li"),$K=a("strong"),hfo=o("xlnet"),pfo=o(" \u2014 "),AS=a("a"),_fo=o("XLNetLMHeadModel"),ufo=o(" (XLNet model)"),bfo=l(),j_=a("p"),vfo=o("The model is set in evaluation mode by default using "),IK=a("code"),Tfo=o("model.eval()"),Ffo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),jK=a("code"),Cfo=o("model.train()"),Mfo=l(),NK=a("p"),Efo=o("Examples:"),yfo=l(),f(HE.$$.fragment),T8e=l(),Wi=a("h2"),N_=a("a"),DK=a("span"),f(UE.$$.fragment),wfo=l(),qK=a("span"),Afo=o("AutoModelForCausalLM"),F8e=l(),Qo=a("div"),f(JE.$$.fragment),Lfo=l(),Qi=a("p"),Bfo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a causal language modeling head) when created
with the `),GK=a("code"),kfo=o("from_pretrained()"),xfo=o("class method or the "),OK=a("code"),Rfo=o("from_config()"),Sfo=o(`class
method.`),Pfo=l(),YE=a("p"),$fo=o("This class cannot be instantiated directly using "),XK=a("code"),Ifo=o("__init__()"),jfo=o(" (throws an error)."),Nfo=l(),qr=a("div"),f(KE.$$.fragment),Dfo=l(),zK=a("p"),qfo=o("Instantiates one of the model classes of the library (with a causal language modeling head) from a configuration."),Gfo=l(),Hi=a("p"),Ofo=o(`Note:
Loading a model from its configuration file does `),VK=a("strong"),Xfo=o("not"),zfo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),WK=a("code"),Vfo=o("from_pretrained()"),Wfo=o("to load the model weights."),Qfo=l(),QK=a("p"),Hfo=o("Examples:"),Ufo=l(),f(ZE.$$.fragment),Jfo=l(),Re=a("div"),f(e3.$$.fragment),Yfo=l(),HK=a("p"),Kfo=o("Instantiate one of the model classes of the library (with a causal language modeling head) from a pretrained model."),Zfo=l(),Ga=a("p"),emo=o("The model class to instantiate is selected based on the "),UK=a("code"),omo=o("model_type"),rmo=o(` property of the config object (either
passed as an argument or loaded from `),JK=a("code"),tmo=o("pretrained_model_name_or_path"),amo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),YK=a("code"),nmo=o("pretrained_model_name_or_path"),smo=o(":"),lmo=l(),$=a("ul"),D_=a("li"),KK=a("strong"),imo=o("bart"),dmo=o(" \u2014 "),LS=a("a"),cmo=o("BartForCausalLM"),fmo=o(" (BART model)"),mmo=l(),q_=a("li"),ZK=a("strong"),gmo=o("bert"),hmo=o(" \u2014 "),BS=a("a"),pmo=o("BertLMHeadModel"),_mo=o(" (BERT model)"),umo=l(),G_=a("li"),eZ=a("strong"),bmo=o("bert-generation"),vmo=o(" \u2014 "),kS=a("a"),Tmo=o("BertGenerationDecoder"),Fmo=o(" (Bert Generation model)"),Cmo=l(),O_=a("li"),oZ=a("strong"),Mmo=o("big_bird"),Emo=o(" \u2014 "),xS=a("a"),ymo=o("BigBirdForCausalLM"),wmo=o(" (BigBird model)"),Amo=l(),X_=a("li"),rZ=a("strong"),Lmo=o("bigbird_pegasus"),Bmo=o(" \u2014 "),RS=a("a"),kmo=o("BigBirdPegasusForCausalLM"),xmo=o(" (BigBirdPegasus model)"),Rmo=l(),z_=a("li"),tZ=a("strong"),Smo=o("blenderbot"),Pmo=o(" \u2014 "),SS=a("a"),$mo=o("BlenderbotForCausalLM"),Imo=o(" (Blenderbot model)"),jmo=l(),V_=a("li"),aZ=a("strong"),Nmo=o("blenderbot-small"),Dmo=o(" \u2014 "),PS=a("a"),qmo=o("BlenderbotSmallForCausalLM"),Gmo=o(" (BlenderbotSmall model)"),Omo=l(),W_=a("li"),nZ=a("strong"),Xmo=o("camembert"),zmo=o(" \u2014 "),$S=a("a"),Vmo=o("CamembertForCausalLM"),Wmo=o(" (CamemBERT model)"),Qmo=l(),Q_=a("li"),sZ=a("strong"),Hmo=o("ctrl"),Umo=o(" \u2014 "),IS=a("a"),Jmo=o("CTRLLMHeadModel"),Ymo=o(" (CTRL model)"),Kmo=l(),H_=a("li"),lZ=a("strong"),Zmo=o("electra"),ego=o(" \u2014 "),jS=a("a"),ogo=o("ElectraForCausalLM"),rgo=o(" (ELECTRA model)"),tgo=l(),U_=a("li"),iZ=a("strong"),ago=o("gpt2"),ngo=o(" \u2014 "),NS=a("a"),sgo=o("GPT2LMHeadModel"),lgo=o(" (OpenAI GPT-2 model)"),igo=l(),J_=a("li"),dZ=a("strong"),dgo=o("gpt_neo"),cgo=o(" \u2014 "),DS=a("a"),fgo=o("GPTNeoForCausalLM"),mgo=o(" (GPT Neo model)"),ggo=l(),Y_=a("li"),cZ=a("strong"),hgo=o("gptj"),pgo=o(" \u2014 "),qS=a("a"),_go=o("GPTJForCausalLM"),ugo=o(" (GPT-J model)"),bgo=l(),K_=a("li"),fZ=a("strong"),vgo=o("marian"),Tgo=o(" \u2014 "),GS=a("a"),Fgo=o("MarianForCausalLM"),Cgo=o(" (Marian model)"),Mgo=l(),Z_=a("li"),mZ=a("strong"),Ego=o("mbart"),ygo=o(" \u2014 "),OS=a("a"),wgo=o("MBartForCausalLM"),Ago=o(" (mBART model)"),Lgo=l(),eu=a("li"),gZ=a("strong"),Bgo=o("megatron-bert"),kgo=o(" \u2014 "),XS=a("a"),xgo=o("MegatronBertForCausalLM"),Rgo=o(" (MegatronBert model)"),Sgo=l(),ou=a("li"),hZ=a("strong"),Pgo=o("openai-gpt"),$go=o(" \u2014 "),zS=a("a"),Igo=o("OpenAIGPTLMHeadModel"),jgo=o(" (OpenAI GPT model)"),Ngo=l(),ru=a("li"),pZ=a("strong"),Dgo=o("pegasus"),qgo=o(" \u2014 "),VS=a("a"),Ggo=o("PegasusForCausalLM"),Ogo=o(" (Pegasus model)"),Xgo=l(),tu=a("li"),_Z=a("strong"),zgo=o("plbart"),Vgo=o(" \u2014 "),WS=a("a"),Wgo=o("PLBartForCausalLM"),Qgo=o(" (PLBart model)"),Hgo=l(),au=a("li"),uZ=a("strong"),Ugo=o("prophetnet"),Jgo=o(" \u2014 "),QS=a("a"),Ygo=o("ProphetNetForCausalLM"),Kgo=o(" (ProphetNet model)"),Zgo=l(),nu=a("li"),bZ=a("strong"),eho=o("qdqbert"),oho=o(" \u2014 "),HS=a("a"),rho=o("QDQBertLMHeadModel"),tho=o(" (QDQBert model)"),aho=l(),su=a("li"),vZ=a("strong"),nho=o("reformer"),sho=o(" \u2014 "),US=a("a"),lho=o("ReformerModelWithLMHead"),iho=o(" (Reformer model)"),dho=l(),lu=a("li"),TZ=a("strong"),cho=o("rembert"),fho=o(" \u2014 "),JS=a("a"),mho=o("RemBertForCausalLM"),gho=o(" (RemBERT model)"),hho=l(),iu=a("li"),FZ=a("strong"),pho=o("roberta"),_ho=o(" \u2014 "),YS=a("a"),uho=o("RobertaForCausalLM"),bho=o(" (RoBERTa model)"),vho=l(),du=a("li"),CZ=a("strong"),Tho=o("roformer"),Fho=o(" \u2014 "),KS=a("a"),Cho=o("RoFormerForCausalLM"),Mho=o(" (RoFormer model)"),Eho=l(),cu=a("li"),MZ=a("strong"),yho=o("speech_to_text_2"),who=o(" \u2014 "),ZS=a("a"),Aho=o("Speech2Text2ForCausalLM"),Lho=o(" (Speech2Text2 model)"),Bho=l(),fu=a("li"),EZ=a("strong"),kho=o("transfo-xl"),xho=o(" \u2014 "),eP=a("a"),Rho=o("TransfoXLLMHeadModel"),Sho=o(" (Transformer-XL model)"),Pho=l(),mu=a("li"),yZ=a("strong"),$ho=o("trocr"),Iho=o(" \u2014 "),oP=a("a"),jho=o("TrOCRForCausalLM"),Nho=o(" (TrOCR model)"),Dho=l(),gu=a("li"),wZ=a("strong"),qho=o("xglm"),Gho=o(" \u2014 "),rP=a("a"),Oho=o("XGLMForCausalLM"),Xho=o(" (XGLM model)"),zho=l(),hu=a("li"),AZ=a("strong"),Vho=o("xlm"),Who=o(" \u2014 "),tP=a("a"),Qho=o("XLMWithLMHeadModel"),Hho=o(" (XLM model)"),Uho=l(),pu=a("li"),LZ=a("strong"),Jho=o("xlm-prophetnet"),Yho=o(" \u2014 "),aP=a("a"),Kho=o("XLMProphetNetForCausalLM"),Zho=o(" (XLMProphetNet model)"),epo=l(),_u=a("li"),BZ=a("strong"),opo=o("xlm-roberta"),rpo=o(" \u2014 "),nP=a("a"),tpo=o("XLMRobertaForCausalLM"),apo=o(" (XLM-RoBERTa model)"),npo=l(),uu=a("li"),kZ=a("strong"),spo=o("xlm-roberta-xl"),lpo=o(" \u2014 "),sP=a("a"),ipo=o("XLMRobertaXLForCausalLM"),dpo=o(" (XLM-RoBERTa-XL model)"),cpo=l(),bu=a("li"),xZ=a("strong"),fpo=o("xlnet"),mpo=o(" \u2014 "),lP=a("a"),gpo=o("XLNetLMHeadModel"),hpo=o(" (XLNet model)"),ppo=l(),vu=a("p"),_po=o("The model is set in evaluation mode by default using "),RZ=a("code"),upo=o("model.eval()"),bpo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),SZ=a("code"),vpo=o("model.train()"),Tpo=l(),PZ=a("p"),Fpo=o("Examples:"),Cpo=l(),f(o3.$$.fragment),C8e=l(),Ui=a("h2"),Tu=a("a"),$Z=a("span"),f(r3.$$.fragment),Mpo=l(),IZ=a("span"),Epo=o("AutoModelForMaskedLM"),M8e=l(),Ho=a("div"),f(t3.$$.fragment),ypo=l(),Ji=a("p"),wpo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a masked language modeling head) when created
with the `),jZ=a("code"),Apo=o("from_pretrained()"),Lpo=o("class method or the "),NZ=a("code"),Bpo=o("from_config()"),kpo=o(`class
method.`),xpo=l(),a3=a("p"),Rpo=o("This class cannot be instantiated directly using "),DZ=a("code"),Spo=o("__init__()"),Ppo=o(" (throws an error)."),$po=l(),Gr=a("div"),f(n3.$$.fragment),Ipo=l(),qZ=a("p"),jpo=o("Instantiates one of the model classes of the library (with a masked language modeling head) from a configuration."),Npo=l(),Yi=a("p"),Dpo=o(`Note:
Loading a model from its configuration file does `),GZ=a("strong"),qpo=o("not"),Gpo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),OZ=a("code"),Opo=o("from_pretrained()"),Xpo=o("to load the model weights."),zpo=l(),XZ=a("p"),Vpo=o("Examples:"),Wpo=l(),f(s3.$$.fragment),Qpo=l(),Se=a("div"),f(l3.$$.fragment),Hpo=l(),zZ=a("p"),Upo=o("Instantiate one of the model classes of the library (with a masked language modeling head) from a pretrained model."),Jpo=l(),Oa=a("p"),Ypo=o("The model class to instantiate is selected based on the "),VZ=a("code"),Kpo=o("model_type"),Zpo=o(` property of the config object (either
passed as an argument or loaded from `),WZ=a("code"),e_o=o("pretrained_model_name_or_path"),o_o=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),QZ=a("code"),r_o=o("pretrained_model_name_or_path"),t_o=o(":"),a_o=l(),I=a("ul"),Fu=a("li"),HZ=a("strong"),n_o=o("albert"),s_o=o(" \u2014 "),iP=a("a"),l_o=o("AlbertForMaskedLM"),i_o=o(" (ALBERT model)"),d_o=l(),Cu=a("li"),UZ=a("strong"),c_o=o("bart"),f_o=o(" \u2014 "),dP=a("a"),m_o=o("BartForConditionalGeneration"),g_o=o(" (BART model)"),h_o=l(),Mu=a("li"),JZ=a("strong"),p_o=o("bert"),__o=o(" \u2014 "),cP=a("a"),u_o=o("BertForMaskedLM"),b_o=o(" (BERT model)"),v_o=l(),Eu=a("li"),YZ=a("strong"),T_o=o("big_bird"),F_o=o(" \u2014 "),fP=a("a"),C_o=o("BigBirdForMaskedLM"),M_o=o(" (BigBird model)"),E_o=l(),yu=a("li"),KZ=a("strong"),y_o=o("camembert"),w_o=o(" \u2014 "),mP=a("a"),A_o=o("CamembertForMaskedLM"),L_o=o(" (CamemBERT model)"),B_o=l(),wu=a("li"),ZZ=a("strong"),k_o=o("convbert"),x_o=o(" \u2014 "),gP=a("a"),R_o=o("ConvBertForMaskedLM"),S_o=o(" (ConvBERT model)"),P_o=l(),Au=a("li"),eee=a("strong"),$_o=o("deberta"),I_o=o(" \u2014 "),hP=a("a"),j_o=o("DebertaForMaskedLM"),N_o=o(" (DeBERTa model)"),D_o=l(),Lu=a("li"),oee=a("strong"),q_o=o("deberta-v2"),G_o=o(" \u2014 "),pP=a("a"),O_o=o("DebertaV2ForMaskedLM"),X_o=o(" (DeBERTa-v2 model)"),z_o=l(),Bu=a("li"),ree=a("strong"),V_o=o("distilbert"),W_o=o(" \u2014 "),_P=a("a"),Q_o=o("DistilBertForMaskedLM"),H_o=o(" (DistilBERT model)"),U_o=l(),ku=a("li"),tee=a("strong"),J_o=o("electra"),Y_o=o(" \u2014 "),uP=a("a"),K_o=o("ElectraForMaskedLM"),Z_o=o(" (ELECTRA model)"),euo=l(),xu=a("li"),aee=a("strong"),ouo=o("flaubert"),ruo=o(" \u2014 "),bP=a("a"),tuo=o("FlaubertWithLMHeadModel"),auo=o(" (FlauBERT model)"),nuo=l(),Ru=a("li"),nee=a("strong"),suo=o("fnet"),luo=o(" \u2014 "),vP=a("a"),iuo=o("FNetForMaskedLM"),duo=o(" (FNet model)"),cuo=l(),Su=a("li"),see=a("strong"),fuo=o("funnel"),muo=o(" \u2014 "),TP=a("a"),guo=o("FunnelForMaskedLM"),huo=o(" (Funnel Transformer model)"),puo=l(),Pu=a("li"),lee=a("strong"),_uo=o("ibert"),uuo=o(" \u2014 "),FP=a("a"),buo=o("IBertForMaskedLM"),vuo=o(" (I-BERT model)"),Tuo=l(),$u=a("li"),iee=a("strong"),Fuo=o("layoutlm"),Cuo=o(" \u2014 "),CP=a("a"),Muo=o("LayoutLMForMaskedLM"),Euo=o(" (LayoutLM model)"),yuo=l(),Iu=a("li"),dee=a("strong"),wuo=o("longformer"),Auo=o(" \u2014 "),MP=a("a"),Luo=o("LongformerForMaskedLM"),Buo=o(" (Longformer model)"),kuo=l(),ju=a("li"),cee=a("strong"),xuo=o("mbart"),Ruo=o(" \u2014 "),EP=a("a"),Suo=o("MBartForConditionalGeneration"),Puo=o(" (mBART model)"),$uo=l(),Nu=a("li"),fee=a("strong"),Iuo=o("megatron-bert"),juo=o(" \u2014 "),yP=a("a"),Nuo=o("MegatronBertForMaskedLM"),Duo=o(" (MegatronBert model)"),quo=l(),Du=a("li"),mee=a("strong"),Guo=o("mobilebert"),Ouo=o(" \u2014 "),wP=a("a"),Xuo=o("MobileBertForMaskedLM"),zuo=o(" (MobileBERT model)"),Vuo=l(),qu=a("li"),gee=a("strong"),Wuo=o("mpnet"),Quo=o(" \u2014 "),AP=a("a"),Huo=o("MPNetForMaskedLM"),Uuo=o(" (MPNet model)"),Juo=l(),Gu=a("li"),hee=a("strong"),Yuo=o("nystromformer"),Kuo=o(" \u2014 "),LP=a("a"),Zuo=o("NystromformerForMaskedLM"),e1o=o(" (Nystromformer model)"),o1o=l(),Ou=a("li"),pee=a("strong"),r1o=o("perceiver"),t1o=o(" \u2014 "),BP=a("a"),a1o=o("PerceiverForMaskedLM"),n1o=o(" (Perceiver model)"),s1o=l(),Xu=a("li"),_ee=a("strong"),l1o=o("qdqbert"),i1o=o(" \u2014 "),kP=a("a"),d1o=o("QDQBertForMaskedLM"),c1o=o(" (QDQBert model)"),f1o=l(),zu=a("li"),uee=a("strong"),m1o=o("reformer"),g1o=o(" \u2014 "),xP=a("a"),h1o=o("ReformerForMaskedLM"),p1o=o(" (Reformer model)"),_1o=l(),Vu=a("li"),bee=a("strong"),u1o=o("rembert"),b1o=o(" \u2014 "),RP=a("a"),v1o=o("RemBertForMaskedLM"),T1o=o(" (RemBERT model)"),F1o=l(),Wu=a("li"),vee=a("strong"),C1o=o("roberta"),M1o=o(" \u2014 "),SP=a("a"),E1o=o("RobertaForMaskedLM"),y1o=o(" (RoBERTa model)"),w1o=l(),Qu=a("li"),Tee=a("strong"),A1o=o("roformer"),L1o=o(" \u2014 "),PP=a("a"),B1o=o("RoFormerForMaskedLM"),k1o=o(" (RoFormer model)"),x1o=l(),Hu=a("li"),Fee=a("strong"),R1o=o("squeezebert"),S1o=o(" \u2014 "),$P=a("a"),P1o=o("SqueezeBertForMaskedLM"),$1o=o(" (SqueezeBERT model)"),I1o=l(),Uu=a("li"),Cee=a("strong"),j1o=o("tapas"),N1o=o(" \u2014 "),IP=a("a"),D1o=o("TapasForMaskedLM"),q1o=o(" (TAPAS model)"),G1o=l(),Ju=a("li"),Mee=a("strong"),O1o=o("wav2vec2"),X1o=o(" \u2014 "),Eee=a("code"),z1o=o("Wav2Vec2ForMaskedLM"),V1o=o("(Wav2Vec2 model)"),W1o=l(),Yu=a("li"),yee=a("strong"),Q1o=o("xlm"),H1o=o(" \u2014 "),jP=a("a"),U1o=o("XLMWithLMHeadModel"),J1o=o(" (XLM model)"),Y1o=l(),Ku=a("li"),wee=a("strong"),K1o=o("xlm-roberta"),Z1o=o(" \u2014 "),NP=a("a"),e7o=o("XLMRobertaForMaskedLM"),o7o=o(" (XLM-RoBERTa model)"),r7o=l(),Zu=a("li"),Aee=a("strong"),t7o=o("xlm-roberta-xl"),a7o=o(" \u2014 "),DP=a("a"),n7o=o("XLMRobertaXLForMaskedLM"),s7o=o(" (XLM-RoBERTa-XL model)"),l7o=l(),e1=a("li"),Lee=a("strong"),i7o=o("yoso"),d7o=o(" \u2014 "),qP=a("a"),c7o=o("YosoForMaskedLM"),f7o=o(" (YOSO model)"),m7o=l(),o1=a("p"),g7o=o("The model is set in evaluation mode by default using "),Bee=a("code"),h7o=o("model.eval()"),p7o=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),kee=a("code"),_7o=o("model.train()"),u7o=l(),xee=a("p"),b7o=o("Examples:"),v7o=l(),f(i3.$$.fragment),E8e=l(),Ki=a("h2"),r1=a("a"),Ree=a("span"),f(d3.$$.fragment),T7o=l(),See=a("span"),F7o=o("AutoModelForSeq2SeqLM"),y8e=l(),Uo=a("div"),f(c3.$$.fragment),C7o=l(),Zi=a("p"),M7o=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence language modeling head) when created
with the `),Pee=a("code"),E7o=o("from_pretrained()"),y7o=o("class method or the "),$ee=a("code"),w7o=o("from_config()"),A7o=o(`class
method.`),L7o=l(),f3=a("p"),B7o=o("This class cannot be instantiated directly using "),Iee=a("code"),k7o=o("__init__()"),x7o=o(" (throws an error)."),R7o=l(),Or=a("div"),f(m3.$$.fragment),S7o=l(),jee=a("p"),P7o=o("Instantiates one of the model classes of the library (with a sequence-to-sequence language modeling head) from a configuration."),$7o=l(),ed=a("p"),I7o=o(`Note:
Loading a model from its configuration file does `),Nee=a("strong"),j7o=o("not"),N7o=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Dee=a("code"),D7o=o("from_pretrained()"),q7o=o("to load the model weights."),G7o=l(),qee=a("p"),O7o=o("Examples:"),X7o=l(),f(g3.$$.fragment),z7o=l(),Pe=a("div"),f(h3.$$.fragment),V7o=l(),Gee=a("p"),W7o=o("Instantiate one of the model classes of the library (with a sequence-to-sequence language modeling head) from a pretrained model."),Q7o=l(),Xa=a("p"),H7o=o("The model class to instantiate is selected based on the "),Oee=a("code"),U7o=o("model_type"),J7o=o(` property of the config object (either
passed as an argument or loaded from `),Xee=a("code"),Y7o=o("pretrained_model_name_or_path"),K7o=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),zee=a("code"),Z7o=o("pretrained_model_name_or_path"),e4o=o(":"),o4o=l(),ae=a("ul"),t1=a("li"),Vee=a("strong"),r4o=o("bart"),t4o=o(" \u2014 "),GP=a("a"),a4o=o("BartForConditionalGeneration"),n4o=o(" (BART model)"),s4o=l(),a1=a("li"),Wee=a("strong"),l4o=o("bigbird_pegasus"),i4o=o(" \u2014 "),OP=a("a"),d4o=o("BigBirdPegasusForConditionalGeneration"),c4o=o(" (BigBirdPegasus model)"),f4o=l(),n1=a("li"),Qee=a("strong"),m4o=o("blenderbot"),g4o=o(" \u2014 "),XP=a("a"),h4o=o("BlenderbotForConditionalGeneration"),p4o=o(" (Blenderbot model)"),_4o=l(),s1=a("li"),Hee=a("strong"),u4o=o("blenderbot-small"),b4o=o(" \u2014 "),zP=a("a"),v4o=o("BlenderbotSmallForConditionalGeneration"),T4o=o(" (BlenderbotSmall model)"),F4o=l(),l1=a("li"),Uee=a("strong"),C4o=o("encoder-decoder"),M4o=o(" \u2014 "),VP=a("a"),E4o=o("EncoderDecoderModel"),y4o=o(" (Encoder decoder model)"),w4o=l(),i1=a("li"),Jee=a("strong"),A4o=o("fsmt"),L4o=o(" \u2014 "),WP=a("a"),B4o=o("FSMTForConditionalGeneration"),k4o=o(" (FairSeq Machine-Translation model)"),x4o=l(),d1=a("li"),Yee=a("strong"),R4o=o("led"),S4o=o(" \u2014 "),QP=a("a"),P4o=o("LEDForConditionalGeneration"),$4o=o(" (LED model)"),I4o=l(),c1=a("li"),Kee=a("strong"),j4o=o("m2m_100"),N4o=o(" \u2014 "),HP=a("a"),D4o=o("M2M100ForConditionalGeneration"),q4o=o(" (M2M100 model)"),G4o=l(),f1=a("li"),Zee=a("strong"),O4o=o("marian"),X4o=o(" \u2014 "),UP=a("a"),z4o=o("MarianMTModel"),V4o=o(" (Marian model)"),W4o=l(),m1=a("li"),eoe=a("strong"),Q4o=o("mbart"),H4o=o(" \u2014 "),JP=a("a"),U4o=o("MBartForConditionalGeneration"),J4o=o(" (mBART model)"),Y4o=l(),g1=a("li"),ooe=a("strong"),K4o=o("mt5"),Z4o=o(" \u2014 "),YP=a("a"),ebo=o("MT5ForConditionalGeneration"),obo=o(" (mT5 model)"),rbo=l(),h1=a("li"),roe=a("strong"),tbo=o("pegasus"),abo=o(" \u2014 "),KP=a("a"),nbo=o("PegasusForConditionalGeneration"),sbo=o(" (Pegasus model)"),lbo=l(),p1=a("li"),toe=a("strong"),ibo=o("plbart"),dbo=o(" \u2014 "),ZP=a("a"),cbo=o("PLBartForConditionalGeneration"),fbo=o(" (PLBart model)"),mbo=l(),_1=a("li"),aoe=a("strong"),gbo=o("prophetnet"),hbo=o(" \u2014 "),e$=a("a"),pbo=o("ProphetNetForConditionalGeneration"),_bo=o(" (ProphetNet model)"),ubo=l(),u1=a("li"),noe=a("strong"),bbo=o("t5"),vbo=o(" \u2014 "),o$=a("a"),Tbo=o("T5ForConditionalGeneration"),Fbo=o(" (T5 model)"),Cbo=l(),b1=a("li"),soe=a("strong"),Mbo=o("xlm-prophetnet"),Ebo=o(" \u2014 "),r$=a("a"),ybo=o("XLMProphetNetForConditionalGeneration"),wbo=o(" (XLMProphetNet model)"),Abo=l(),v1=a("p"),Lbo=o("The model is set in evaluation mode by default using "),loe=a("code"),Bbo=o("model.eval()"),kbo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),ioe=a("code"),xbo=o("model.train()"),Rbo=l(),doe=a("p"),Sbo=o("Examples:"),Pbo=l(),f(p3.$$.fragment),w8e=l(),od=a("h2"),T1=a("a"),coe=a("span"),f(_3.$$.fragment),$bo=l(),foe=a("span"),Ibo=o("AutoModelForSequenceClassification"),A8e=l(),Jo=a("div"),f(u3.$$.fragment),jbo=l(),rd=a("p"),Nbo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence classification head) when created
with the `),moe=a("code"),Dbo=o("from_pretrained()"),qbo=o("class method or the "),goe=a("code"),Gbo=o("from_config()"),Obo=o(`class
method.`),Xbo=l(),b3=a("p"),zbo=o("This class cannot be instantiated directly using "),hoe=a("code"),Vbo=o("__init__()"),Wbo=o(" (throws an error)."),Qbo=l(),Xr=a("div"),f(v3.$$.fragment),Hbo=l(),poe=a("p"),Ubo=o("Instantiates one of the model classes of the library (with a sequence classification head) from a configuration."),Jbo=l(),td=a("p"),Ybo=o(`Note:
Loading a model from its configuration file does `),_oe=a("strong"),Kbo=o("not"),Zbo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),uoe=a("code"),e5o=o("from_pretrained()"),o5o=o("to load the model weights."),r5o=l(),boe=a("p"),t5o=o("Examples:"),a5o=l(),f(T3.$$.fragment),n5o=l(),$e=a("div"),f(F3.$$.fragment),s5o=l(),voe=a("p"),l5o=o("Instantiate one of the model classes of the library (with a sequence classification head) from a pretrained model."),i5o=l(),za=a("p"),d5o=o("The model class to instantiate is selected based on the "),Toe=a("code"),c5o=o("model_type"),f5o=o(` property of the config object (either
passed as an argument or loaded from `),Foe=a("code"),m5o=o("pretrained_model_name_or_path"),g5o=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Coe=a("code"),h5o=o("pretrained_model_name_or_path"),p5o=o(":"),_5o=l(),A=a("ul"),F1=a("li"),Moe=a("strong"),u5o=o("albert"),b5o=o(" \u2014 "),t$=a("a"),v5o=o("AlbertForSequenceClassification"),T5o=o(" (ALBERT model)"),F5o=l(),C1=a("li"),Eoe=a("strong"),C5o=o("bart"),M5o=o(" \u2014 "),a$=a("a"),E5o=o("BartForSequenceClassification"),y5o=o(" (BART model)"),w5o=l(),M1=a("li"),yoe=a("strong"),A5o=o("bert"),L5o=o(" \u2014 "),n$=a("a"),B5o=o("BertForSequenceClassification"),k5o=o(" (BERT model)"),x5o=l(),E1=a("li"),woe=a("strong"),R5o=o("big_bird"),S5o=o(" \u2014 "),s$=a("a"),P5o=o("BigBirdForSequenceClassification"),$5o=o(" (BigBird model)"),I5o=l(),y1=a("li"),Aoe=a("strong"),j5o=o("bigbird_pegasus"),N5o=o(" \u2014 "),l$=a("a"),D5o=o("BigBirdPegasusForSequenceClassification"),q5o=o(" (BigBirdPegasus model)"),G5o=l(),w1=a("li"),Loe=a("strong"),O5o=o("camembert"),X5o=o(" \u2014 "),i$=a("a"),z5o=o("CamembertForSequenceClassification"),V5o=o(" (CamemBERT model)"),W5o=l(),A1=a("li"),Boe=a("strong"),Q5o=o("canine"),H5o=o(" \u2014 "),d$=a("a"),U5o=o("CanineForSequenceClassification"),J5o=o(" (Canine model)"),Y5o=l(),L1=a("li"),koe=a("strong"),K5o=o("convbert"),Z5o=o(" \u2014 "),c$=a("a"),e2o=o("ConvBertForSequenceClassification"),o2o=o(" (ConvBERT model)"),r2o=l(),B1=a("li"),xoe=a("strong"),t2o=o("ctrl"),a2o=o(" \u2014 "),f$=a("a"),n2o=o("CTRLForSequenceClassification"),s2o=o(" (CTRL model)"),l2o=l(),k1=a("li"),Roe=a("strong"),i2o=o("deberta"),d2o=o(" \u2014 "),m$=a("a"),c2o=o("DebertaForSequenceClassification"),f2o=o(" (DeBERTa model)"),m2o=l(),x1=a("li"),Soe=a("strong"),g2o=o("deberta-v2"),h2o=o(" \u2014 "),g$=a("a"),p2o=o("DebertaV2ForSequenceClassification"),_2o=o(" (DeBERTa-v2 model)"),u2o=l(),R1=a("li"),Poe=a("strong"),b2o=o("distilbert"),v2o=o(" \u2014 "),h$=a("a"),T2o=o("DistilBertForSequenceClassification"),F2o=o(" (DistilBERT model)"),C2o=l(),S1=a("li"),$oe=a("strong"),M2o=o("electra"),E2o=o(" \u2014 "),p$=a("a"),y2o=o("ElectraForSequenceClassification"),w2o=o(" (ELECTRA model)"),A2o=l(),P1=a("li"),Ioe=a("strong"),L2o=o("flaubert"),B2o=o(" \u2014 "),_$=a("a"),k2o=o("FlaubertForSequenceClassification"),x2o=o(" (FlauBERT model)"),R2o=l(),$1=a("li"),joe=a("strong"),S2o=o("fnet"),P2o=o(" \u2014 "),u$=a("a"),$2o=o("FNetForSequenceClassification"),I2o=o(" (FNet model)"),j2o=l(),I1=a("li"),Noe=a("strong"),N2o=o("funnel"),D2o=o(" \u2014 "),b$=a("a"),q2o=o("FunnelForSequenceClassification"),G2o=o(" (Funnel Transformer model)"),O2o=l(),j1=a("li"),Doe=a("strong"),X2o=o("gpt2"),z2o=o(" \u2014 "),v$=a("a"),V2o=o("GPT2ForSequenceClassification"),W2o=o(" (OpenAI GPT-2 model)"),Q2o=l(),N1=a("li"),qoe=a("strong"),H2o=o("gpt_neo"),U2o=o(" \u2014 "),T$=a("a"),J2o=o("GPTNeoForSequenceClassification"),Y2o=o(" (GPT Neo model)"),K2o=l(),D1=a("li"),Goe=a("strong"),Z2o=o("gptj"),evo=o(" \u2014 "),F$=a("a"),ovo=o("GPTJForSequenceClassification"),rvo=o(" (GPT-J model)"),tvo=l(),q1=a("li"),Ooe=a("strong"),avo=o("ibert"),nvo=o(" \u2014 "),C$=a("a"),svo=o("IBertForSequenceClassification"),lvo=o(" (I-BERT model)"),ivo=l(),G1=a("li"),Xoe=a("strong"),dvo=o("layoutlm"),cvo=o(" \u2014 "),M$=a("a"),fvo=o("LayoutLMForSequenceClassification"),mvo=o(" (LayoutLM model)"),gvo=l(),O1=a("li"),zoe=a("strong"),hvo=o("layoutlmv2"),pvo=o(" \u2014 "),E$=a("a"),_vo=o("LayoutLMv2ForSequenceClassification"),uvo=o(" (LayoutLMv2 model)"),bvo=l(),X1=a("li"),Voe=a("strong"),vvo=o("led"),Tvo=o(" \u2014 "),y$=a("a"),Fvo=o("LEDForSequenceClassification"),Cvo=o(" (LED model)"),Mvo=l(),z1=a("li"),Woe=a("strong"),Evo=o("longformer"),yvo=o(" \u2014 "),w$=a("a"),wvo=o("LongformerForSequenceClassification"),Avo=o(" (Longformer model)"),Lvo=l(),V1=a("li"),Qoe=a("strong"),Bvo=o("mbart"),kvo=o(" \u2014 "),A$=a("a"),xvo=o("MBartForSequenceClassification"),Rvo=o(" (mBART model)"),Svo=l(),W1=a("li"),Hoe=a("strong"),Pvo=o("megatron-bert"),$vo=o(" \u2014 "),L$=a("a"),Ivo=o("MegatronBertForSequenceClassification"),jvo=o(" (MegatronBert model)"),Nvo=l(),Q1=a("li"),Uoe=a("strong"),Dvo=o("mobilebert"),qvo=o(" \u2014 "),B$=a("a"),Gvo=o("MobileBertForSequenceClassification"),Ovo=o(" (MobileBERT model)"),Xvo=l(),H1=a("li"),Joe=a("strong"),zvo=o("mpnet"),Vvo=o(" \u2014 "),k$=a("a"),Wvo=o("MPNetForSequenceClassification"),Qvo=o(" (MPNet model)"),Hvo=l(),U1=a("li"),Yoe=a("strong"),Uvo=o("nystromformer"),Jvo=o(" \u2014 "),x$=a("a"),Yvo=o("NystromformerForSequenceClassification"),Kvo=o(" (Nystromformer model)"),Zvo=l(),J1=a("li"),Koe=a("strong"),eTo=o("openai-gpt"),oTo=o(" \u2014 "),R$=a("a"),rTo=o("OpenAIGPTForSequenceClassification"),tTo=o(" (OpenAI GPT model)"),aTo=l(),Y1=a("li"),Zoe=a("strong"),nTo=o("perceiver"),sTo=o(" \u2014 "),S$=a("a"),lTo=o("PerceiverForSequenceClassification"),iTo=o(" (Perceiver model)"),dTo=l(),K1=a("li"),ere=a("strong"),cTo=o("plbart"),fTo=o(" \u2014 "),P$=a("a"),mTo=o("PLBartForSequenceClassification"),gTo=o(" (PLBart model)"),hTo=l(),Z1=a("li"),ore=a("strong"),pTo=o("qdqbert"),_To=o(" \u2014 "),$$=a("a"),uTo=o("QDQBertForSequenceClassification"),bTo=o(" (QDQBert model)"),vTo=l(),e7=a("li"),rre=a("strong"),TTo=o("reformer"),FTo=o(" \u2014 "),I$=a("a"),CTo=o("ReformerForSequenceClassification"),MTo=o(" (Reformer model)"),ETo=l(),o7=a("li"),tre=a("strong"),yTo=o("rembert"),wTo=o(" \u2014 "),j$=a("a"),ATo=o("RemBertForSequenceClassification"),LTo=o(" (RemBERT model)"),BTo=l(),r7=a("li"),are=a("strong"),kTo=o("roberta"),xTo=o(" \u2014 "),N$=a("a"),RTo=o("RobertaForSequenceClassification"),STo=o(" (RoBERTa model)"),PTo=l(),t7=a("li"),nre=a("strong"),$To=o("roformer"),ITo=o(" \u2014 "),D$=a("a"),jTo=o("RoFormerForSequenceClassification"),NTo=o(" (RoFormer model)"),DTo=l(),a7=a("li"),sre=a("strong"),qTo=o("squeezebert"),GTo=o(" \u2014 "),q$=a("a"),OTo=o("SqueezeBertForSequenceClassification"),XTo=o(" (SqueezeBERT model)"),zTo=l(),n7=a("li"),lre=a("strong"),VTo=o("tapas"),WTo=o(" \u2014 "),G$=a("a"),QTo=o("TapasForSequenceClassification"),HTo=o(" (TAPAS model)"),UTo=l(),s7=a("li"),ire=a("strong"),JTo=o("transfo-xl"),YTo=o(" \u2014 "),O$=a("a"),KTo=o("TransfoXLForSequenceClassification"),ZTo=o(" (Transformer-XL model)"),eFo=l(),l7=a("li"),dre=a("strong"),oFo=o("xlm"),rFo=o(" \u2014 "),X$=a("a"),tFo=o("XLMForSequenceClassification"),aFo=o(" (XLM model)"),nFo=l(),i7=a("li"),cre=a("strong"),sFo=o("xlm-roberta"),lFo=o(" \u2014 "),z$=a("a"),iFo=o("XLMRobertaForSequenceClassification"),dFo=o(" (XLM-RoBERTa model)"),cFo=l(),d7=a("li"),fre=a("strong"),fFo=o("xlm-roberta-xl"),mFo=o(" \u2014 "),V$=a("a"),gFo=o("XLMRobertaXLForSequenceClassification"),hFo=o(" (XLM-RoBERTa-XL model)"),pFo=l(),c7=a("li"),mre=a("strong"),_Fo=o("xlnet"),uFo=o(" \u2014 "),W$=a("a"),bFo=o("XLNetForSequenceClassification"),vFo=o(" (XLNet model)"),TFo=l(),f7=a("li"),gre=a("strong"),FFo=o("yoso"),CFo=o(" \u2014 "),Q$=a("a"),MFo=o("YosoForSequenceClassification"),EFo=o(" (YOSO model)"),yFo=l(),m7=a("p"),wFo=o("The model is set in evaluation mode by default using "),hre=a("code"),AFo=o("model.eval()"),LFo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),pre=a("code"),BFo=o("model.train()"),kFo=l(),_re=a("p"),xFo=o("Examples:"),RFo=l(),f(C3.$$.fragment),L8e=l(),ad=a("h2"),g7=a("a"),ure=a("span"),f(M3.$$.fragment),SFo=l(),bre=a("span"),PFo=o("AutoModelForMultipleChoice"),B8e=l(),Yo=a("div"),f(E3.$$.fragment),$Fo=l(),nd=a("p"),IFo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a multiple choice head) when created
with the `),vre=a("code"),jFo=o("from_pretrained()"),NFo=o("class method or the "),Tre=a("code"),DFo=o("from_config()"),qFo=o(`class
method.`),GFo=l(),y3=a("p"),OFo=o("This class cannot be instantiated directly using "),Fre=a("code"),XFo=o("__init__()"),zFo=o(" (throws an error)."),VFo=l(),zr=a("div"),f(w3.$$.fragment),WFo=l(),Cre=a("p"),QFo=o("Instantiates one of the model classes of the library (with a multiple choice head) from a configuration."),HFo=l(),sd=a("p"),UFo=o(`Note:
Loading a model from its configuration file does `),Mre=a("strong"),JFo=o("not"),YFo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Ere=a("code"),KFo=o("from_pretrained()"),ZFo=o("to load the model weights."),eCo=l(),yre=a("p"),oCo=o("Examples:"),rCo=l(),f(A3.$$.fragment),tCo=l(),Ie=a("div"),f(L3.$$.fragment),aCo=l(),wre=a("p"),nCo=o("Instantiate one of the model classes of the library (with a multiple choice head) from a pretrained model."),sCo=l(),Va=a("p"),lCo=o("The model class to instantiate is selected based on the "),Are=a("code"),iCo=o("model_type"),dCo=o(` property of the config object (either
passed as an argument or loaded from `),Lre=a("code"),cCo=o("pretrained_model_name_or_path"),fCo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Bre=a("code"),mCo=o("pretrained_model_name_or_path"),gCo=o(":"),hCo=l(),G=a("ul"),h7=a("li"),kre=a("strong"),pCo=o("albert"),_Co=o(" \u2014 "),H$=a("a"),uCo=o("AlbertForMultipleChoice"),bCo=o(" (ALBERT model)"),vCo=l(),p7=a("li"),xre=a("strong"),TCo=o("bert"),FCo=o(" \u2014 "),U$=a("a"),CCo=o("BertForMultipleChoice"),MCo=o(" (BERT model)"),ECo=l(),_7=a("li"),Rre=a("strong"),yCo=o("big_bird"),wCo=o(" \u2014 "),J$=a("a"),ACo=o("BigBirdForMultipleChoice"),LCo=o(" (BigBird model)"),BCo=l(),u7=a("li"),Sre=a("strong"),kCo=o("camembert"),xCo=o(" \u2014 "),Y$=a("a"),RCo=o("CamembertForMultipleChoice"),SCo=o(" (CamemBERT model)"),PCo=l(),b7=a("li"),Pre=a("strong"),$Co=o("canine"),ICo=o(" \u2014 "),K$=a("a"),jCo=o("CanineForMultipleChoice"),NCo=o(" (Canine model)"),DCo=l(),v7=a("li"),$re=a("strong"),qCo=o("convbert"),GCo=o(" \u2014 "),Z$=a("a"),OCo=o("ConvBertForMultipleChoice"),XCo=o(" (ConvBERT model)"),zCo=l(),T7=a("li"),Ire=a("strong"),VCo=o("distilbert"),WCo=o(" \u2014 "),eI=a("a"),QCo=o("DistilBertForMultipleChoice"),HCo=o(" (DistilBERT model)"),UCo=l(),F7=a("li"),jre=a("strong"),JCo=o("electra"),YCo=o(" \u2014 "),oI=a("a"),KCo=o("ElectraForMultipleChoice"),ZCo=o(" (ELECTRA model)"),eMo=l(),C7=a("li"),Nre=a("strong"),oMo=o("flaubert"),rMo=o(" \u2014 "),rI=a("a"),tMo=o("FlaubertForMultipleChoice"),aMo=o(" (FlauBERT model)"),nMo=l(),M7=a("li"),Dre=a("strong"),sMo=o("fnet"),lMo=o(" \u2014 "),tI=a("a"),iMo=o("FNetForMultipleChoice"),dMo=o(" (FNet model)"),cMo=l(),E7=a("li"),qre=a("strong"),fMo=o("funnel"),mMo=o(" \u2014 "),aI=a("a"),gMo=o("FunnelForMultipleChoice"),hMo=o(" (Funnel Transformer model)"),pMo=l(),y7=a("li"),Gre=a("strong"),_Mo=o("ibert"),uMo=o(" \u2014 "),nI=a("a"),bMo=o("IBertForMultipleChoice"),vMo=o(" (I-BERT model)"),TMo=l(),w7=a("li"),Ore=a("strong"),FMo=o("longformer"),CMo=o(" \u2014 "),sI=a("a"),MMo=o("LongformerForMultipleChoice"),EMo=o(" (Longformer model)"),yMo=l(),A7=a("li"),Xre=a("strong"),wMo=o("megatron-bert"),AMo=o(" \u2014 "),lI=a("a"),LMo=o("MegatronBertForMultipleChoice"),BMo=o(" (MegatronBert model)"),kMo=l(),L7=a("li"),zre=a("strong"),xMo=o("mobilebert"),RMo=o(" \u2014 "),iI=a("a"),SMo=o("MobileBertForMultipleChoice"),PMo=o(" (MobileBERT model)"),$Mo=l(),B7=a("li"),Vre=a("strong"),IMo=o("mpnet"),jMo=o(" \u2014 "),dI=a("a"),NMo=o("MPNetForMultipleChoice"),DMo=o(" (MPNet model)"),qMo=l(),k7=a("li"),Wre=a("strong"),GMo=o("nystromformer"),OMo=o(" \u2014 "),cI=a("a"),XMo=o("NystromformerForMultipleChoice"),zMo=o(" (Nystromformer model)"),VMo=l(),x7=a("li"),Qre=a("strong"),WMo=o("qdqbert"),QMo=o(" \u2014 "),fI=a("a"),HMo=o("QDQBertForMultipleChoice"),UMo=o(" (QDQBert model)"),JMo=l(),R7=a("li"),Hre=a("strong"),YMo=o("rembert"),KMo=o(" \u2014 "),mI=a("a"),ZMo=o("RemBertForMultipleChoice"),eEo=o(" (RemBERT model)"),oEo=l(),S7=a("li"),Ure=a("strong"),rEo=o("roberta"),tEo=o(" \u2014 "),gI=a("a"),aEo=o("RobertaForMultipleChoice"),nEo=o(" (RoBERTa model)"),sEo=l(),P7=a("li"),Jre=a("strong"),lEo=o("roformer"),iEo=o(" \u2014 "),hI=a("a"),dEo=o("RoFormerForMultipleChoice"),cEo=o(" (RoFormer model)"),fEo=l(),$7=a("li"),Yre=a("strong"),mEo=o("squeezebert"),gEo=o(" \u2014 "),pI=a("a"),hEo=o("SqueezeBertForMultipleChoice"),pEo=o(" (SqueezeBERT model)"),_Eo=l(),I7=a("li"),Kre=a("strong"),uEo=o("xlm"),bEo=o(" \u2014 "),_I=a("a"),vEo=o("XLMForMultipleChoice"),TEo=o(" (XLM model)"),FEo=l(),j7=a("li"),Zre=a("strong"),CEo=o("xlm-roberta"),MEo=o(" \u2014 "),uI=a("a"),EEo=o("XLMRobertaForMultipleChoice"),yEo=o(" (XLM-RoBERTa model)"),wEo=l(),N7=a("li"),ete=a("strong"),AEo=o("xlm-roberta-xl"),LEo=o(" \u2014 "),bI=a("a"),BEo=o("XLMRobertaXLForMultipleChoice"),kEo=o(" (XLM-RoBERTa-XL model)"),xEo=l(),D7=a("li"),ote=a("strong"),REo=o("xlnet"),SEo=o(" \u2014 "),vI=a("a"),PEo=o("XLNetForMultipleChoice"),$Eo=o(" (XLNet model)"),IEo=l(),q7=a("li"),rte=a("strong"),jEo=o("yoso"),NEo=o(" \u2014 "),TI=a("a"),DEo=o("YosoForMultipleChoice"),qEo=o(" (YOSO model)"),GEo=l(),G7=a("p"),OEo=o("The model is set in evaluation mode by default using "),tte=a("code"),XEo=o("model.eval()"),zEo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),ate=a("code"),VEo=o("model.train()"),WEo=l(),nte=a("p"),QEo=o("Examples:"),HEo=l(),f(B3.$$.fragment),k8e=l(),ld=a("h2"),O7=a("a"),ste=a("span"),f(k3.$$.fragment),UEo=l(),lte=a("span"),JEo=o("AutoModelForNextSentencePrediction"),x8e=l(),Ko=a("div"),f(x3.$$.fragment),YEo=l(),id=a("p"),KEo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a next sentence prediction head) when created
with the `),ite=a("code"),ZEo=o("from_pretrained()"),e3o=o("class method or the "),dte=a("code"),o3o=o("from_config()"),r3o=o(`class
method.`),t3o=l(),R3=a("p"),a3o=o("This class cannot be instantiated directly using "),cte=a("code"),n3o=o("__init__()"),s3o=o(" (throws an error)."),l3o=l(),Vr=a("div"),f(S3.$$.fragment),i3o=l(),fte=a("p"),d3o=o("Instantiates one of the model classes of the library (with a next sentence prediction head) from a configuration."),c3o=l(),dd=a("p"),f3o=o(`Note:
Loading a model from its configuration file does `),mte=a("strong"),m3o=o("not"),g3o=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),gte=a("code"),h3o=o("from_pretrained()"),p3o=o("to load the model weights."),_3o=l(),hte=a("p"),u3o=o("Examples:"),b3o=l(),f(P3.$$.fragment),v3o=l(),je=a("div"),f($3.$$.fragment),T3o=l(),pte=a("p"),F3o=o("Instantiate one of the model classes of the library (with a next sentence prediction head) from a pretrained model."),C3o=l(),Wa=a("p"),M3o=o("The model class to instantiate is selected based on the "),_te=a("code"),E3o=o("model_type"),y3o=o(` property of the config object (either
passed as an argument or loaded from `),ute=a("code"),w3o=o("pretrained_model_name_or_path"),A3o=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),bte=a("code"),L3o=o("pretrained_model_name_or_path"),B3o=o(":"),k3o=l(),na=a("ul"),X7=a("li"),vte=a("strong"),x3o=o("bert"),R3o=o(" \u2014 "),FI=a("a"),S3o=o("BertForNextSentencePrediction"),P3o=o(" (BERT model)"),$3o=l(),z7=a("li"),Tte=a("strong"),I3o=o("fnet"),j3o=o(" \u2014 "),CI=a("a"),N3o=o("FNetForNextSentencePrediction"),D3o=o(" (FNet model)"),q3o=l(),V7=a("li"),Fte=a("strong"),G3o=o("megatron-bert"),O3o=o(" \u2014 "),MI=a("a"),X3o=o("MegatronBertForNextSentencePrediction"),z3o=o(" (MegatronBert model)"),V3o=l(),W7=a("li"),Cte=a("strong"),W3o=o("mobilebert"),Q3o=o(" \u2014 "),EI=a("a"),H3o=o("MobileBertForNextSentencePrediction"),U3o=o(" (MobileBERT model)"),J3o=l(),Q7=a("li"),Mte=a("strong"),Y3o=o("qdqbert"),K3o=o(" \u2014 "),yI=a("a"),Z3o=o("QDQBertForNextSentencePrediction"),eyo=o(" (QDQBert model)"),oyo=l(),H7=a("p"),ryo=o("The model is set in evaluation mode by default using "),Ete=a("code"),tyo=o("model.eval()"),ayo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),yte=a("code"),nyo=o("model.train()"),syo=l(),wte=a("p"),lyo=o("Examples:"),iyo=l(),f(I3.$$.fragment),R8e=l(),cd=a("h2"),U7=a("a"),Ate=a("span"),f(j3.$$.fragment),dyo=l(),Lte=a("span"),cyo=o("AutoModelForTokenClassification"),S8e=l(),Zo=a("div"),f(N3.$$.fragment),fyo=l(),fd=a("p"),myo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a token classification head) when created
with the `),Bte=a("code"),gyo=o("from_pretrained()"),hyo=o("class method or the "),kte=a("code"),pyo=o("from_config()"),_yo=o(`class
method.`),uyo=l(),D3=a("p"),byo=o("This class cannot be instantiated directly using "),xte=a("code"),vyo=o("__init__()"),Tyo=o(" (throws an error)."),Fyo=l(),Wr=a("div"),f(q3.$$.fragment),Cyo=l(),Rte=a("p"),Myo=o("Instantiates one of the model classes of the library (with a token classification head) from a configuration."),Eyo=l(),md=a("p"),yyo=o(`Note:
Loading a model from its configuration file does `),Ste=a("strong"),wyo=o("not"),Ayo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Pte=a("code"),Lyo=o("from_pretrained()"),Byo=o("to load the model weights."),kyo=l(),$te=a("p"),xyo=o("Examples:"),Ryo=l(),f(G3.$$.fragment),Syo=l(),Ne=a("div"),f(O3.$$.fragment),Pyo=l(),Ite=a("p"),$yo=o("Instantiate one of the model classes of the library (with a token classification head) from a pretrained model."),Iyo=l(),Qa=a("p"),jyo=o("The model class to instantiate is selected based on the "),jte=a("code"),Nyo=o("model_type"),Dyo=o(` property of the config object (either
passed as an argument or loaded from `),Nte=a("code"),qyo=o("pretrained_model_name_or_path"),Gyo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Dte=a("code"),Oyo=o("pretrained_model_name_or_path"),Xyo=o(":"),zyo=l(),D=a("ul"),J7=a("li"),qte=a("strong"),Vyo=o("albert"),Wyo=o(" \u2014 "),wI=a("a"),Qyo=o("AlbertForTokenClassification"),Hyo=o(" (ALBERT model)"),Uyo=l(),Y7=a("li"),Gte=a("strong"),Jyo=o("bert"),Yyo=o(" \u2014 "),AI=a("a"),Kyo=o("BertForTokenClassification"),Zyo=o(" (BERT model)"),ewo=l(),K7=a("li"),Ote=a("strong"),owo=o("big_bird"),rwo=o(" \u2014 "),LI=a("a"),two=o("BigBirdForTokenClassification"),awo=o(" (BigBird model)"),nwo=l(),Z7=a("li"),Xte=a("strong"),swo=o("camembert"),lwo=o(" \u2014 "),BI=a("a"),iwo=o("CamembertForTokenClassification"),dwo=o(" (CamemBERT model)"),cwo=l(),e4=a("li"),zte=a("strong"),fwo=o("canine"),mwo=o(" \u2014 "),kI=a("a"),gwo=o("CanineForTokenClassification"),hwo=o(" (Canine model)"),pwo=l(),o4=a("li"),Vte=a("strong"),_wo=o("convbert"),uwo=o(" \u2014 "),xI=a("a"),bwo=o("ConvBertForTokenClassification"),vwo=o(" (ConvBERT model)"),Two=l(),r4=a("li"),Wte=a("strong"),Fwo=o("deberta"),Cwo=o(" \u2014 "),RI=a("a"),Mwo=o("DebertaForTokenClassification"),Ewo=o(" (DeBERTa model)"),ywo=l(),t4=a("li"),Qte=a("strong"),wwo=o("deberta-v2"),Awo=o(" \u2014 "),SI=a("a"),Lwo=o("DebertaV2ForTokenClassification"),Bwo=o(" (DeBERTa-v2 model)"),kwo=l(),a4=a("li"),Hte=a("strong"),xwo=o("distilbert"),Rwo=o(" \u2014 "),PI=a("a"),Swo=o("DistilBertForTokenClassification"),Pwo=o(" (DistilBERT model)"),$wo=l(),n4=a("li"),Ute=a("strong"),Iwo=o("electra"),jwo=o(" \u2014 "),$I=a("a"),Nwo=o("ElectraForTokenClassification"),Dwo=o(" (ELECTRA model)"),qwo=l(),s4=a("li"),Jte=a("strong"),Gwo=o("flaubert"),Owo=o(" \u2014 "),II=a("a"),Xwo=o("FlaubertForTokenClassification"),zwo=o(" (FlauBERT model)"),Vwo=l(),l4=a("li"),Yte=a("strong"),Wwo=o("fnet"),Qwo=o(" \u2014 "),jI=a("a"),Hwo=o("FNetForTokenClassification"),Uwo=o(" (FNet model)"),Jwo=l(),i4=a("li"),Kte=a("strong"),Ywo=o("funnel"),Kwo=o(" \u2014 "),NI=a("a"),Zwo=o("FunnelForTokenClassification"),eAo=o(" (Funnel Transformer model)"),oAo=l(),d4=a("li"),Zte=a("strong"),rAo=o("gpt2"),tAo=o(" \u2014 "),DI=a("a"),aAo=o("GPT2ForTokenClassification"),nAo=o(" (OpenAI GPT-2 model)"),sAo=l(),c4=a("li"),eae=a("strong"),lAo=o("ibert"),iAo=o(" \u2014 "),qI=a("a"),dAo=o("IBertForTokenClassification"),cAo=o(" (I-BERT model)"),fAo=l(),f4=a("li"),oae=a("strong"),mAo=o("layoutlm"),gAo=o(" \u2014 "),GI=a("a"),hAo=o("LayoutLMForTokenClassification"),pAo=o(" (LayoutLM model)"),_Ao=l(),m4=a("li"),rae=a("strong"),uAo=o("layoutlmv2"),bAo=o(" \u2014 "),OI=a("a"),vAo=o("LayoutLMv2ForTokenClassification"),TAo=o(" (LayoutLMv2 model)"),FAo=l(),g4=a("li"),tae=a("strong"),CAo=o("longformer"),MAo=o(" \u2014 "),XI=a("a"),EAo=o("LongformerForTokenClassification"),yAo=o(" (Longformer model)"),wAo=l(),h4=a("li"),aae=a("strong"),AAo=o("megatron-bert"),LAo=o(" \u2014 "),zI=a("a"),BAo=o("MegatronBertForTokenClassification"),kAo=o(" (MegatronBert model)"),xAo=l(),p4=a("li"),nae=a("strong"),RAo=o("mobilebert"),SAo=o(" \u2014 "),VI=a("a"),PAo=o("MobileBertForTokenClassification"),$Ao=o(" (MobileBERT model)"),IAo=l(),_4=a("li"),sae=a("strong"),jAo=o("mpnet"),NAo=o(" \u2014 "),WI=a("a"),DAo=o("MPNetForTokenClassification"),qAo=o(" (MPNet model)"),GAo=l(),u4=a("li"),lae=a("strong"),OAo=o("nystromformer"),XAo=o(" \u2014 "),QI=a("a"),zAo=o("NystromformerForTokenClassification"),VAo=o(" (Nystromformer model)"),WAo=l(),b4=a("li"),iae=a("strong"),QAo=o("qdqbert"),HAo=o(" \u2014 "),HI=a("a"),UAo=o("QDQBertForTokenClassification"),JAo=o(" (QDQBert model)"),YAo=l(),v4=a("li"),dae=a("strong"),KAo=o("rembert"),ZAo=o(" \u2014 "),UI=a("a"),e6o=o("RemBertForTokenClassification"),o6o=o(" (RemBERT model)"),r6o=l(),T4=a("li"),cae=a("strong"),t6o=o("roberta"),a6o=o(" \u2014 "),JI=a("a"),n6o=o("RobertaForTokenClassification"),s6o=o(" (RoBERTa model)"),l6o=l(),F4=a("li"),fae=a("strong"),i6o=o("roformer"),d6o=o(" \u2014 "),YI=a("a"),c6o=o("RoFormerForTokenClassification"),f6o=o(" (RoFormer model)"),m6o=l(),C4=a("li"),mae=a("strong"),g6o=o("squeezebert"),h6o=o(" \u2014 "),KI=a("a"),p6o=o("SqueezeBertForTokenClassification"),_6o=o(" (SqueezeBERT model)"),u6o=l(),M4=a("li"),gae=a("strong"),b6o=o("xlm"),v6o=o(" \u2014 "),ZI=a("a"),T6o=o("XLMForTokenClassification"),F6o=o(" (XLM model)"),C6o=l(),E4=a("li"),hae=a("strong"),M6o=o("xlm-roberta"),E6o=o(" \u2014 "),ej=a("a"),y6o=o("XLMRobertaForTokenClassification"),w6o=o(" (XLM-RoBERTa model)"),A6o=l(),y4=a("li"),pae=a("strong"),L6o=o("xlm-roberta-xl"),B6o=o(" \u2014 "),oj=a("a"),k6o=o("XLMRobertaXLForTokenClassification"),x6o=o(" (XLM-RoBERTa-XL model)"),R6o=l(),w4=a("li"),_ae=a("strong"),S6o=o("xlnet"),P6o=o(" \u2014 "),rj=a("a"),$6o=o("XLNetForTokenClassification"),I6o=o(" (XLNet model)"),j6o=l(),A4=a("li"),uae=a("strong"),N6o=o("yoso"),D6o=o(" \u2014 "),tj=a("a"),q6o=o("YosoForTokenClassification"),G6o=o(" (YOSO model)"),O6o=l(),L4=a("p"),X6o=o("The model is set in evaluation mode by default using "),bae=a("code"),z6o=o("model.eval()"),V6o=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),vae=a("code"),W6o=o("model.train()"),Q6o=l(),Tae=a("p"),H6o=o("Examples:"),U6o=l(),f(X3.$$.fragment),P8e=l(),gd=a("h2"),B4=a("a"),Fae=a("span"),f(z3.$$.fragment),J6o=l(),Cae=a("span"),Y6o=o("AutoModelForQuestionAnswering"),$8e=l(),er=a("div"),f(V3.$$.fragment),K6o=l(),hd=a("p"),Z6o=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a question answering head) when created
with the `),Mae=a("code"),e0o=o("from_pretrained()"),o0o=o("class method or the "),Eae=a("code"),r0o=o("from_config()"),t0o=o(`class
method.`),a0o=l(),W3=a("p"),n0o=o("This class cannot be instantiated directly using "),yae=a("code"),s0o=o("__init__()"),l0o=o(" (throws an error)."),i0o=l(),Qr=a("div"),f(Q3.$$.fragment),d0o=l(),wae=a("p"),c0o=o("Instantiates one of the model classes of the library (with a question answering head) from a configuration."),f0o=l(),pd=a("p"),m0o=o(`Note:
Loading a model from its configuration file does `),Aae=a("strong"),g0o=o("not"),h0o=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Lae=a("code"),p0o=o("from_pretrained()"),_0o=o("to load the model weights."),u0o=l(),Bae=a("p"),b0o=o("Examples:"),v0o=l(),f(H3.$$.fragment),T0o=l(),De=a("div"),f(U3.$$.fragment),F0o=l(),kae=a("p"),C0o=o("Instantiate one of the model classes of the library (with a question answering head) from a pretrained model."),M0o=l(),Ha=a("p"),E0o=o("The model class to instantiate is selected based on the "),xae=a("code"),y0o=o("model_type"),w0o=o(` property of the config object (either
passed as an argument or loaded from `),Rae=a("code"),A0o=o("pretrained_model_name_or_path"),L0o=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Sae=a("code"),B0o=o("pretrained_model_name_or_path"),k0o=o(":"),x0o=l(),R=a("ul"),k4=a("li"),Pae=a("strong"),R0o=o("albert"),S0o=o(" \u2014 "),aj=a("a"),P0o=o("AlbertForQuestionAnswering"),$0o=o(" (ALBERT model)"),I0o=l(),x4=a("li"),$ae=a("strong"),j0o=o("bart"),N0o=o(" \u2014 "),nj=a("a"),D0o=o("BartForQuestionAnswering"),q0o=o(" (BART model)"),G0o=l(),R4=a("li"),Iae=a("strong"),O0o=o("bert"),X0o=o(" \u2014 "),sj=a("a"),z0o=o("BertForQuestionAnswering"),V0o=o(" (BERT model)"),W0o=l(),S4=a("li"),jae=a("strong"),Q0o=o("big_bird"),H0o=o(" \u2014 "),lj=a("a"),U0o=o("BigBirdForQuestionAnswering"),J0o=o(" (BigBird model)"),Y0o=l(),P4=a("li"),Nae=a("strong"),K0o=o("bigbird_pegasus"),Z0o=o(" \u2014 "),ij=a("a"),eLo=o("BigBirdPegasusForQuestionAnswering"),oLo=o(" (BigBirdPegasus model)"),rLo=l(),$4=a("li"),Dae=a("strong"),tLo=o("camembert"),aLo=o(" \u2014 "),dj=a("a"),nLo=o("CamembertForQuestionAnswering"),sLo=o(" (CamemBERT model)"),lLo=l(),I4=a("li"),qae=a("strong"),iLo=o("canine"),dLo=o(" \u2014 "),cj=a("a"),cLo=o("CanineForQuestionAnswering"),fLo=o(" (Canine model)"),mLo=l(),j4=a("li"),Gae=a("strong"),gLo=o("convbert"),hLo=o(" \u2014 "),fj=a("a"),pLo=o("ConvBertForQuestionAnswering"),_Lo=o(" (ConvBERT model)"),uLo=l(),N4=a("li"),Oae=a("strong"),bLo=o("deberta"),vLo=o(" \u2014 "),mj=a("a"),TLo=o("DebertaForQuestionAnswering"),FLo=o(" (DeBERTa model)"),CLo=l(),D4=a("li"),Xae=a("strong"),MLo=o("deberta-v2"),ELo=o(" \u2014 "),gj=a("a"),yLo=o("DebertaV2ForQuestionAnswering"),wLo=o(" (DeBERTa-v2 model)"),ALo=l(),q4=a("li"),zae=a("strong"),LLo=o("distilbert"),BLo=o(" \u2014 "),hj=a("a"),kLo=o("DistilBertForQuestionAnswering"),xLo=o(" (DistilBERT model)"),RLo=l(),G4=a("li"),Vae=a("strong"),SLo=o("electra"),PLo=o(" \u2014 "),pj=a("a"),$Lo=o("ElectraForQuestionAnswering"),ILo=o(" (ELECTRA model)"),jLo=l(),O4=a("li"),Wae=a("strong"),NLo=o("flaubert"),DLo=o(" \u2014 "),_j=a("a"),qLo=o("FlaubertForQuestionAnsweringSimple"),GLo=o(" (FlauBERT model)"),OLo=l(),X4=a("li"),Qae=a("strong"),XLo=o("fnet"),zLo=o(" \u2014 "),uj=a("a"),VLo=o("FNetForQuestionAnswering"),WLo=o(" (FNet model)"),QLo=l(),z4=a("li"),Hae=a("strong"),HLo=o("funnel"),ULo=o(" \u2014 "),bj=a("a"),JLo=o("FunnelForQuestionAnswering"),YLo=o(" (Funnel Transformer model)"),KLo=l(),V4=a("li"),Uae=a("strong"),ZLo=o("gptj"),e8o=o(" \u2014 "),vj=a("a"),o8o=o("GPTJForQuestionAnswering"),r8o=o(" (GPT-J model)"),t8o=l(),W4=a("li"),Jae=a("strong"),a8o=o("ibert"),n8o=o(" \u2014 "),Tj=a("a"),s8o=o("IBertForQuestionAnswering"),l8o=o(" (I-BERT model)"),i8o=l(),Q4=a("li"),Yae=a("strong"),d8o=o("layoutlmv2"),c8o=o(" \u2014 "),Fj=a("a"),f8o=o("LayoutLMv2ForQuestionAnswering"),m8o=o(" (LayoutLMv2 model)"),g8o=l(),H4=a("li"),Kae=a("strong"),h8o=o("led"),p8o=o(" \u2014 "),Cj=a("a"),_8o=o("LEDForQuestionAnswering"),u8o=o(" (LED model)"),b8o=l(),U4=a("li"),Zae=a("strong"),v8o=o("longformer"),T8o=o(" \u2014 "),Mj=a("a"),F8o=o("LongformerForQuestionAnswering"),C8o=o(" (Longformer model)"),M8o=l(),J4=a("li"),ene=a("strong"),E8o=o("lxmert"),y8o=o(" \u2014 "),Ej=a("a"),w8o=o("LxmertForQuestionAnswering"),A8o=o(" (LXMERT model)"),L8o=l(),Y4=a("li"),one=a("strong"),B8o=o("mbart"),k8o=o(" \u2014 "),yj=a("a"),x8o=o("MBartForQuestionAnswering"),R8o=o(" (mBART model)"),S8o=l(),K4=a("li"),rne=a("strong"),P8o=o("megatron-bert"),$8o=o(" \u2014 "),wj=a("a"),I8o=o("MegatronBertForQuestionAnswering"),j8o=o(" (MegatronBert model)"),N8o=l(),Z4=a("li"),tne=a("strong"),D8o=o("mobilebert"),q8o=o(" \u2014 "),Aj=a("a"),G8o=o("MobileBertForQuestionAnswering"),O8o=o(" (MobileBERT model)"),X8o=l(),eb=a("li"),ane=a("strong"),z8o=o("mpnet"),V8o=o(" \u2014 "),Lj=a("a"),W8o=o("MPNetForQuestionAnswering"),Q8o=o(" (MPNet model)"),H8o=l(),ob=a("li"),nne=a("strong"),U8o=o("nystromformer"),J8o=o(" \u2014 "),Bj=a("a"),Y8o=o("NystromformerForQuestionAnswering"),K8o=o(" (Nystromformer model)"),Z8o=l(),rb=a("li"),sne=a("strong"),e9o=o("qdqbert"),o9o=o(" \u2014 "),kj=a("a"),r9o=o("QDQBertForQuestionAnswering"),t9o=o(" (QDQBert model)"),a9o=l(),tb=a("li"),lne=a("strong"),n9o=o("reformer"),s9o=o(" \u2014 "),xj=a("a"),l9o=o("ReformerForQuestionAnswering"),i9o=o(" (Reformer model)"),d9o=l(),ab=a("li"),ine=a("strong"),c9o=o("rembert"),f9o=o(" \u2014 "),Rj=a("a"),m9o=o("RemBertForQuestionAnswering"),g9o=o(" (RemBERT model)"),h9o=l(),nb=a("li"),dne=a("strong"),p9o=o("roberta"),_9o=o(" \u2014 "),Sj=a("a"),u9o=o("RobertaForQuestionAnswering"),b9o=o(" (RoBERTa model)"),v9o=l(),sb=a("li"),cne=a("strong"),T9o=o("roformer"),F9o=o(" \u2014 "),Pj=a("a"),C9o=o("RoFormerForQuestionAnswering"),M9o=o(" (RoFormer model)"),E9o=l(),lb=a("li"),fne=a("strong"),y9o=o("splinter"),w9o=o(" \u2014 "),$j=a("a"),A9o=o("SplinterForQuestionAnswering"),L9o=o(" (Splinter model)"),B9o=l(),ib=a("li"),mne=a("strong"),k9o=o("squeezebert"),x9o=o(" \u2014 "),Ij=a("a"),R9o=o("SqueezeBertForQuestionAnswering"),S9o=o(" (SqueezeBERT model)"),P9o=l(),db=a("li"),gne=a("strong"),$9o=o("xlm"),I9o=o(" \u2014 "),jj=a("a"),j9o=o("XLMForQuestionAnsweringSimple"),N9o=o(" (XLM model)"),D9o=l(),cb=a("li"),hne=a("strong"),q9o=o("xlm-roberta"),G9o=o(" \u2014 "),Nj=a("a"),O9o=o("XLMRobertaForQuestionAnswering"),X9o=o(" (XLM-RoBERTa model)"),z9o=l(),fb=a("li"),pne=a("strong"),V9o=o("xlm-roberta-xl"),W9o=o(" \u2014 "),Dj=a("a"),Q9o=o("XLMRobertaXLForQuestionAnswering"),H9o=o(" (XLM-RoBERTa-XL model)"),U9o=l(),mb=a("li"),_ne=a("strong"),J9o=o("xlnet"),Y9o=o(" \u2014 "),qj=a("a"),K9o=o("XLNetForQuestionAnsweringSimple"),Z9o=o(" (XLNet model)"),eBo=l(),gb=a("li"),une=a("strong"),oBo=o("yoso"),rBo=o(" \u2014 "),Gj=a("a"),tBo=o("YosoForQuestionAnswering"),aBo=o(" (YOSO model)"),nBo=l(),hb=a("p"),sBo=o("The model is set in evaluation mode by default using "),bne=a("code"),lBo=o("model.eval()"),iBo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),vne=a("code"),dBo=o("model.train()"),cBo=l(),Tne=a("p"),fBo=o("Examples:"),mBo=l(),f(J3.$$.fragment),I8e=l(),_d=a("h2"),pb=a("a"),Fne=a("span"),f(Y3.$$.fragment),gBo=l(),Cne=a("span"),hBo=o("AutoModelForTableQuestionAnswering"),j8e=l(),or=a("div"),f(K3.$$.fragment),pBo=l(),ud=a("p"),_Bo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a table question answering head) when created
with the `),Mne=a("code"),uBo=o("from_pretrained()"),bBo=o("class method or the "),Ene=a("code"),vBo=o("from_config()"),TBo=o(`class
method.`),FBo=l(),Z3=a("p"),CBo=o("This class cannot be instantiated directly using "),yne=a("code"),MBo=o("__init__()"),EBo=o(" (throws an error)."),yBo=l(),Hr=a("div"),f(ey.$$.fragment),wBo=l(),wne=a("p"),ABo=o("Instantiates one of the model classes of the library (with a table question answering head) from a configuration."),LBo=l(),bd=a("p"),BBo=o(`Note:
Loading a model from its configuration file does `),Ane=a("strong"),kBo=o("not"),xBo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Lne=a("code"),RBo=o("from_pretrained()"),SBo=o("to load the model weights."),PBo=l(),Bne=a("p"),$Bo=o("Examples:"),IBo=l(),f(oy.$$.fragment),jBo=l(),qe=a("div"),f(ry.$$.fragment),NBo=l(),kne=a("p"),DBo=o("Instantiate one of the model classes of the library (with a table question answering head) from a pretrained model."),qBo=l(),Ua=a("p"),GBo=o("The model class to instantiate is selected based on the "),xne=a("code"),OBo=o("model_type"),XBo=o(` property of the config object (either
passed as an argument or loaded from `),Rne=a("code"),zBo=o("pretrained_model_name_or_path"),VBo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Sne=a("code"),WBo=o("pretrained_model_name_or_path"),QBo=o(":"),HBo=l(),Pne=a("ul"),_b=a("li"),$ne=a("strong"),UBo=o("tapas"),JBo=o(" \u2014 "),Oj=a("a"),YBo=o("TapasForQuestionAnswering"),KBo=o(" (TAPAS model)"),ZBo=l(),ub=a("p"),eko=o("The model is set in evaluation mode by default using "),Ine=a("code"),oko=o("model.eval()"),rko=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),jne=a("code"),tko=o("model.train()"),ako=l(),Nne=a("p"),nko=o("Examples:"),sko=l(),f(ty.$$.fragment),N8e=l(),vd=a("h2"),bb=a("a"),Dne=a("span"),f(ay.$$.fragment),lko=l(),qne=a("span"),iko=o("AutoModelForImageClassification"),D8e=l(),rr=a("div"),f(ny.$$.fragment),dko=l(),Td=a("p"),cko=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a image classification head) when created
with the `),Gne=a("code"),fko=o("from_pretrained()"),mko=o("class method or the "),One=a("code"),gko=o("from_config()"),hko=o(`class
method.`),pko=l(),sy=a("p"),_ko=o("This class cannot be instantiated directly using "),Xne=a("code"),uko=o("__init__()"),bko=o(" (throws an error)."),vko=l(),Ur=a("div"),f(ly.$$.fragment),Tko=l(),zne=a("p"),Fko=o("Instantiates one of the model classes of the library (with a image classification head) from a configuration."),Cko=l(),Fd=a("p"),Mko=o(`Note:
Loading a model from its configuration file does `),Vne=a("strong"),Eko=o("not"),yko=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Wne=a("code"),wko=o("from_pretrained()"),Ako=o("to load the model weights."),Lko=l(),Qne=a("p"),Bko=o("Examples:"),kko=l(),f(iy.$$.fragment),xko=l(),Ge=a("div"),f(dy.$$.fragment),Rko=l(),Hne=a("p"),Sko=o("Instantiate one of the model classes of the library (with a image classification head) from a pretrained model."),Pko=l(),Ja=a("p"),$ko=o("The model class to instantiate is selected based on the "),Une=a("code"),Iko=o("model_type"),jko=o(` property of the config object (either
passed as an argument or loaded from `),Jne=a("code"),Nko=o("pretrained_model_name_or_path"),Dko=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Yne=a("code"),qko=o("pretrained_model_name_or_path"),Gko=o(":"),Oko=l(),be=a("ul"),vb=a("li"),Kne=a("strong"),Xko=o("beit"),zko=o(" \u2014 "),Xj=a("a"),Vko=o("BeitForImageClassification"),Wko=o(" (BEiT model)"),Qko=l(),Tb=a("li"),Zne=a("strong"),Hko=o("convnext"),Uko=o(" \u2014 "),zj=a("a"),Jko=o("ConvNextForImageClassification"),Yko=o(" (ConvNext model)"),Kko=l(),Rs=a("li"),ese=a("strong"),Zko=o("deit"),exo=o(" \u2014 "),Vj=a("a"),oxo=o("DeiTForImageClassification"),rxo=o(" or "),Wj=a("a"),txo=o("DeiTForImageClassificationWithTeacher"),axo=o(" (DeiT model)"),nxo=l(),Fb=a("li"),ose=a("strong"),sxo=o("imagegpt"),lxo=o(" \u2014 "),Qj=a("a"),ixo=o("ImageGPTForImageClassification"),dxo=o(" (ImageGPT model)"),cxo=l(),la=a("li"),rse=a("strong"),fxo=o("perceiver"),mxo=o(" \u2014 "),Hj=a("a"),gxo=o("PerceiverForImageClassificationLearned"),hxo=o(" or "),Uj=a("a"),pxo=o("PerceiverForImageClassificationFourier"),_xo=o(" or "),Jj=a("a"),uxo=o("PerceiverForImageClassificationConvProcessing"),bxo=o(" (Perceiver model)"),vxo=l(),Cb=a("li"),tse=a("strong"),Txo=o("poolformer"),Fxo=o(" \u2014 "),Yj=a("a"),Cxo=o("PoolFormerForImageClassification"),Mxo=o(" (PoolFormer model)"),Exo=l(),Mb=a("li"),ase=a("strong"),yxo=o("segformer"),wxo=o(" \u2014 "),Kj=a("a"),Axo=o("SegformerForImageClassification"),Lxo=o(" (SegFormer model)"),Bxo=l(),Eb=a("li"),nse=a("strong"),kxo=o("swin"),xxo=o(" \u2014 "),Zj=a("a"),Rxo=o("SwinForImageClassification"),Sxo=o(" (Swin model)"),Pxo=l(),yb=a("li"),sse=a("strong"),$xo=o("vit"),Ixo=o(" \u2014 "),eN=a("a"),jxo=o("ViTForImageClassification"),Nxo=o(" (ViT model)"),Dxo=l(),wb=a("p"),qxo=o("The model is set in evaluation mode by default using "),lse=a("code"),Gxo=o("model.eval()"),Oxo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),ise=a("code"),Xxo=o("model.train()"),zxo=l(),dse=a("p"),Vxo=o("Examples:"),Wxo=l(),f(cy.$$.fragment),q8e=l(),Cd=a("h2"),Ab=a("a"),cse=a("span"),f(fy.$$.fragment),Qxo=l(),fse=a("span"),Hxo=o("AutoModelForVision2Seq"),G8e=l(),tr=a("div"),f(my.$$.fragment),Uxo=l(),Md=a("p"),Jxo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a vision-to-text modeling head) when created
with the `),mse=a("code"),Yxo=o("from_pretrained()"),Kxo=o("class method or the "),gse=a("code"),Zxo=o("from_config()"),eRo=o(`class
method.`),oRo=l(),gy=a("p"),rRo=o("This class cannot be instantiated directly using "),hse=a("code"),tRo=o("__init__()"),aRo=o(" (throws an error)."),nRo=l(),Jr=a("div"),f(hy.$$.fragment),sRo=l(),pse=a("p"),lRo=o("Instantiates one of the model classes of the library (with a vision-to-text modeling head) from a configuration."),iRo=l(),Ed=a("p"),dRo=o(`Note:
Loading a model from its configuration file does `),_se=a("strong"),cRo=o("not"),fRo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),use=a("code"),mRo=o("from_pretrained()"),gRo=o("to load the model weights."),hRo=l(),bse=a("p"),pRo=o("Examples:"),_Ro=l(),f(py.$$.fragment),uRo=l(),Oe=a("div"),f(_y.$$.fragment),bRo=l(),vse=a("p"),vRo=o("Instantiate one of the model classes of the library (with a vision-to-text modeling head) from a pretrained model."),TRo=l(),Ya=a("p"),FRo=o("The model class to instantiate is selected based on the "),Tse=a("code"),CRo=o("model_type"),MRo=o(` property of the config object (either
passed as an argument or loaded from `),Fse=a("code"),ERo=o("pretrained_model_name_or_path"),yRo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Cse=a("code"),wRo=o("pretrained_model_name_or_path"),ARo=o(":"),LRo=l(),Mse=a("ul"),Lb=a("li"),Ese=a("strong"),BRo=o("vision-encoder-decoder"),kRo=o(" \u2014 "),oN=a("a"),xRo=o("VisionEncoderDecoderModel"),RRo=o(" (Vision Encoder decoder model)"),SRo=l(),Bb=a("p"),PRo=o("The model is set in evaluation mode by default using "),yse=a("code"),$Ro=o("model.eval()"),IRo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),wse=a("code"),jRo=o("model.train()"),NRo=l(),Ase=a("p"),DRo=o("Examples:"),qRo=l(),f(uy.$$.fragment),O8e=l(),yd=a("h2"),kb=a("a"),Lse=a("span"),f(by.$$.fragment),GRo=l(),Bse=a("span"),ORo=o("AutoModelForAudioClassification"),X8e=l(),ar=a("div"),f(vy.$$.fragment),XRo=l(),wd=a("p"),zRo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a audio classification head) when created
with the `),kse=a("code"),VRo=o("from_pretrained()"),WRo=o("class method or the "),xse=a("code"),QRo=o("from_config()"),HRo=o(`class
method.`),URo=l(),Ty=a("p"),JRo=o("This class cannot be instantiated directly using "),Rse=a("code"),YRo=o("__init__()"),KRo=o(" (throws an error)."),ZRo=l(),Yr=a("div"),f(Fy.$$.fragment),eSo=l(),Sse=a("p"),oSo=o("Instantiates one of the model classes of the library (with a audio classification head) from a configuration."),rSo=l(),Ad=a("p"),tSo=o(`Note:
Loading a model from its configuration file does `),Pse=a("strong"),aSo=o("not"),nSo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),$se=a("code"),sSo=o("from_pretrained()"),lSo=o("to load the model weights."),iSo=l(),Ise=a("p"),dSo=o("Examples:"),cSo=l(),f(Cy.$$.fragment),fSo=l(),Xe=a("div"),f(My.$$.fragment),mSo=l(),jse=a("p"),gSo=o("Instantiate one of the model classes of the library (with a audio classification head) from a pretrained model."),hSo=l(),Ka=a("p"),pSo=o("The model class to instantiate is selected based on the "),Nse=a("code"),_So=o("model_type"),uSo=o(` property of the config object (either
passed as an argument or loaded from `),Dse=a("code"),bSo=o("pretrained_model_name_or_path"),vSo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),qse=a("code"),TSo=o("pretrained_model_name_or_path"),FSo=o(":"),CSo=l(),ao=a("ul"),xb=a("li"),Gse=a("strong"),MSo=o("hubert"),ESo=o(" \u2014 "),rN=a("a"),ySo=o("HubertForSequenceClassification"),wSo=o(" (Hubert model)"),ASo=l(),Rb=a("li"),Ose=a("strong"),LSo=o("sew"),BSo=o(" \u2014 "),tN=a("a"),kSo=o("SEWForSequenceClassification"),xSo=o(" (SEW model)"),RSo=l(),Sb=a("li"),Xse=a("strong"),SSo=o("sew-d"),PSo=o(" \u2014 "),aN=a("a"),$So=o("SEWDForSequenceClassification"),ISo=o(" (SEW-D model)"),jSo=l(),Pb=a("li"),zse=a("strong"),NSo=o("unispeech"),DSo=o(" \u2014 "),nN=a("a"),qSo=o("UniSpeechForSequenceClassification"),GSo=o(" (UniSpeech model)"),OSo=l(),$b=a("li"),Vse=a("strong"),XSo=o("unispeech-sat"),zSo=o(" \u2014 "),sN=a("a"),VSo=o("UniSpeechSatForSequenceClassification"),WSo=o(" (UniSpeechSat model)"),QSo=l(),Ib=a("li"),Wse=a("strong"),HSo=o("wav2vec2"),USo=o(" \u2014 "),lN=a("a"),JSo=o("Wav2Vec2ForSequenceClassification"),YSo=o(" (Wav2Vec2 model)"),KSo=l(),jb=a("li"),Qse=a("strong"),ZSo=o("wavlm"),ePo=o(" \u2014 "),iN=a("a"),oPo=o("WavLMForSequenceClassification"),rPo=o(" (WavLM model)"),tPo=l(),Nb=a("p"),aPo=o("The model is set in evaluation mode by default using "),Hse=a("code"),nPo=o("model.eval()"),sPo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Use=a("code"),lPo=o("model.train()"),iPo=l(),Jse=a("p"),dPo=o("Examples:"),cPo=l(),f(Ey.$$.fragment),z8e=l(),Ld=a("h2"),Db=a("a"),Yse=a("span"),f(yy.$$.fragment),fPo=l(),Kse=a("span"),mPo=o("AutoModelForAudioFrameClassification"),V8e=l(),nr=a("div"),f(wy.$$.fragment),gPo=l(),Bd=a("p"),hPo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a audio frame (token) classification head) when created
with the `),Zse=a("code"),pPo=o("from_pretrained()"),_Po=o("class method or the "),ele=a("code"),uPo=o("from_config()"),bPo=o(`class
method.`),vPo=l(),Ay=a("p"),TPo=o("This class cannot be instantiated directly using "),ole=a("code"),FPo=o("__init__()"),CPo=o(" (throws an error)."),MPo=l(),Kr=a("div"),f(Ly.$$.fragment),EPo=l(),rle=a("p"),yPo=o("Instantiates one of the model classes of the library (with a audio frame (token) classification head) from a configuration."),wPo=l(),kd=a("p"),APo=o(`Note:
Loading a model from its configuration file does `),tle=a("strong"),LPo=o("not"),BPo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),ale=a("code"),kPo=o("from_pretrained()"),xPo=o("to load the model weights."),RPo=l(),nle=a("p"),SPo=o("Examples:"),PPo=l(),f(By.$$.fragment),$Po=l(),ze=a("div"),f(ky.$$.fragment),IPo=l(),sle=a("p"),jPo=o("Instantiate one of the model classes of the library (with a audio frame (token) classification head) from a pretrained model."),NPo=l(),Za=a("p"),DPo=o("The model class to instantiate is selected based on the "),lle=a("code"),qPo=o("model_type"),GPo=o(` property of the config object (either
passed as an argument or loaded from `),ile=a("code"),OPo=o("pretrained_model_name_or_path"),XPo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),dle=a("code"),zPo=o("pretrained_model_name_or_path"),VPo=o(":"),WPo=l(),xd=a("ul"),qb=a("li"),cle=a("strong"),QPo=o("unispeech-sat"),HPo=o(" \u2014 "),dN=a("a"),UPo=o("UniSpeechSatForAudioFrameClassification"),JPo=o(" (UniSpeechSat model)"),YPo=l(),Gb=a("li"),fle=a("strong"),KPo=o("wav2vec2"),ZPo=o(" \u2014 "),cN=a("a"),e$o=o("Wav2Vec2ForAudioFrameClassification"),o$o=o(" (Wav2Vec2 model)"),r$o=l(),Ob=a("li"),mle=a("strong"),t$o=o("wavlm"),a$o=o(" \u2014 "),fN=a("a"),n$o=o("WavLMForAudioFrameClassification"),s$o=o(" (WavLM model)"),l$o=l(),Xb=a("p"),i$o=o("The model is set in evaluation mode by default using "),gle=a("code"),d$o=o("model.eval()"),c$o=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),hle=a("code"),f$o=o("model.train()"),m$o=l(),ple=a("p"),g$o=o("Examples:"),h$o=l(),f(xy.$$.fragment),W8e=l(),Rd=a("h2"),zb=a("a"),_le=a("span"),f(Ry.$$.fragment),p$o=l(),ule=a("span"),_$o=o("AutoModelForCTC"),Q8e=l(),sr=a("div"),f(Sy.$$.fragment),u$o=l(),Sd=a("p"),b$o=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a connectionist temporal classification head) when created
with the `),ble=a("code"),v$o=o("from_pretrained()"),T$o=o("class method or the "),vle=a("code"),F$o=o("from_config()"),C$o=o(`class
method.`),M$o=l(),Py=a("p"),E$o=o("This class cannot be instantiated directly using "),Tle=a("code"),y$o=o("__init__()"),w$o=o(" (throws an error)."),A$o=l(),Zr=a("div"),f($y.$$.fragment),L$o=l(),Fle=a("p"),B$o=o("Instantiates one of the model classes of the library (with a connectionist temporal classification head) from a configuration."),k$o=l(),Pd=a("p"),x$o=o(`Note:
Loading a model from its configuration file does `),Cle=a("strong"),R$o=o("not"),S$o=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Mle=a("code"),P$o=o("from_pretrained()"),$$o=o("to load the model weights."),I$o=l(),Ele=a("p"),j$o=o("Examples:"),N$o=l(),f(Iy.$$.fragment),D$o=l(),Ve=a("div"),f(jy.$$.fragment),q$o=l(),yle=a("p"),G$o=o("Instantiate one of the model classes of the library (with a connectionist temporal classification head) from a pretrained model."),O$o=l(),en=a("p"),X$o=o("The model class to instantiate is selected based on the "),wle=a("code"),z$o=o("model_type"),V$o=o(` property of the config object (either
passed as an argument or loaded from `),Ale=a("code"),W$o=o("pretrained_model_name_or_path"),Q$o=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Lle=a("code"),H$o=o("pretrained_model_name_or_path"),U$o=o(":"),J$o=l(),no=a("ul"),Vb=a("li"),Ble=a("strong"),Y$o=o("hubert"),K$o=o(" \u2014 "),mN=a("a"),Z$o=o("HubertForCTC"),eIo=o(" (Hubert model)"),oIo=l(),Wb=a("li"),kle=a("strong"),rIo=o("sew"),tIo=o(" \u2014 "),gN=a("a"),aIo=o("SEWForCTC"),nIo=o(" (SEW model)"),sIo=l(),Qb=a("li"),xle=a("strong"),lIo=o("sew-d"),iIo=o(" \u2014 "),hN=a("a"),dIo=o("SEWDForCTC"),cIo=o(" (SEW-D model)"),fIo=l(),Hb=a("li"),Rle=a("strong"),mIo=o("unispeech"),gIo=o(" \u2014 "),pN=a("a"),hIo=o("UniSpeechForCTC"),pIo=o(" (UniSpeech model)"),_Io=l(),Ub=a("li"),Sle=a("strong"),uIo=o("unispeech-sat"),bIo=o(" \u2014 "),_N=a("a"),vIo=o("UniSpeechSatForCTC"),TIo=o(" (UniSpeechSat model)"),FIo=l(),Jb=a("li"),Ple=a("strong"),CIo=o("wav2vec2"),MIo=o(" \u2014 "),uN=a("a"),EIo=o("Wav2Vec2ForCTC"),yIo=o(" (Wav2Vec2 model)"),wIo=l(),Yb=a("li"),$le=a("strong"),AIo=o("wavlm"),LIo=o(" \u2014 "),bN=a("a"),BIo=o("WavLMForCTC"),kIo=o(" (WavLM model)"),xIo=l(),Kb=a("p"),RIo=o("The model is set in evaluation mode by default using "),Ile=a("code"),SIo=o("model.eval()"),PIo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),jle=a("code"),$Io=o("model.train()"),IIo=l(),Nle=a("p"),jIo=o("Examples:"),NIo=l(),f(Ny.$$.fragment),H8e=l(),$d=a("h2"),Zb=a("a"),Dle=a("span"),f(Dy.$$.fragment),DIo=l(),qle=a("span"),qIo=o("AutoModelForSpeechSeq2Seq"),U8e=l(),lr=a("div"),f(qy.$$.fragment),GIo=l(),Id=a("p"),OIo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) when created
with the `),Gle=a("code"),XIo=o("from_pretrained()"),zIo=o("class method or the "),Ole=a("code"),VIo=o("from_config()"),WIo=o(`class
method.`),QIo=l(),Gy=a("p"),HIo=o("This class cannot be instantiated directly using "),Xle=a("code"),UIo=o("__init__()"),JIo=o(" (throws an error)."),YIo=l(),et=a("div"),f(Oy.$$.fragment),KIo=l(),zle=a("p"),ZIo=o("Instantiates one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) from a configuration."),ejo=l(),jd=a("p"),ojo=o(`Note:
Loading a model from its configuration file does `),Vle=a("strong"),rjo=o("not"),tjo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Wle=a("code"),ajo=o("from_pretrained()"),njo=o("to load the model weights."),sjo=l(),Qle=a("p"),ljo=o("Examples:"),ijo=l(),f(Xy.$$.fragment),djo=l(),We=a("div"),f(zy.$$.fragment),cjo=l(),Hle=a("p"),fjo=o("Instantiate one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) from a pretrained model."),mjo=l(),on=a("p"),gjo=o("The model class to instantiate is selected based on the "),Ule=a("code"),hjo=o("model_type"),pjo=o(` property of the config object (either
passed as an argument or loaded from `),Jle=a("code"),_jo=o("pretrained_model_name_or_path"),ujo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Yle=a("code"),bjo=o("pretrained_model_name_or_path"),vjo=o(":"),Tjo=l(),Vy=a("ul"),e5=a("li"),Kle=a("strong"),Fjo=o("speech-encoder-decoder"),Cjo=o(" \u2014 "),vN=a("a"),Mjo=o("SpeechEncoderDecoderModel"),Ejo=o(" (Speech Encoder decoder model)"),yjo=l(),o5=a("li"),Zle=a("strong"),wjo=o("speech_to_text"),Ajo=o(" \u2014 "),TN=a("a"),Ljo=o("Speech2TextForConditionalGeneration"),Bjo=o(" (Speech2Text model)"),kjo=l(),r5=a("p"),xjo=o("The model is set in evaluation mode by default using "),eie=a("code"),Rjo=o("model.eval()"),Sjo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),oie=a("code"),Pjo=o("model.train()"),$jo=l(),rie=a("p"),Ijo=o("Examples:"),jjo=l(),f(Wy.$$.fragment),J8e=l(),Nd=a("h2"),t5=a("a"),tie=a("span"),f(Qy.$$.fragment),Njo=l(),aie=a("span"),Djo=o("AutoModelForAudioXVector"),Y8e=l(),ir=a("div"),f(Hy.$$.fragment),qjo=l(),Dd=a("p"),Gjo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a audio retrieval via x-vector head) when created
with the `),nie=a("code"),Ojo=o("from_pretrained()"),Xjo=o("class method or the "),sie=a("code"),zjo=o("from_config()"),Vjo=o(`class
method.`),Wjo=l(),Uy=a("p"),Qjo=o("This class cannot be instantiated directly using "),lie=a("code"),Hjo=o("__init__()"),Ujo=o(" (throws an error)."),Jjo=l(),ot=a("div"),f(Jy.$$.fragment),Yjo=l(),iie=a("p"),Kjo=o("Instantiates one of the model classes of the library (with a audio retrieval via x-vector head) from a configuration."),Zjo=l(),qd=a("p"),eNo=o(`Note:
Loading a model from its configuration file does `),die=a("strong"),oNo=o("not"),rNo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),cie=a("code"),tNo=o("from_pretrained()"),aNo=o("to load the model weights."),nNo=l(),fie=a("p"),sNo=o("Examples:"),lNo=l(),f(Yy.$$.fragment),iNo=l(),Qe=a("div"),f(Ky.$$.fragment),dNo=l(),mie=a("p"),cNo=o("Instantiate one of the model classes of the library (with a audio retrieval via x-vector head) from a pretrained model."),fNo=l(),rn=a("p"),mNo=o("The model class to instantiate is selected based on the "),gie=a("code"),gNo=o("model_type"),hNo=o(` property of the config object (either
passed as an argument or loaded from `),hie=a("code"),pNo=o("pretrained_model_name_or_path"),_No=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),pie=a("code"),uNo=o("pretrained_model_name_or_path"),bNo=o(":"),vNo=l(),Gd=a("ul"),a5=a("li"),_ie=a("strong"),TNo=o("unispeech-sat"),FNo=o(" \u2014 "),FN=a("a"),CNo=o("UniSpeechSatForXVector"),MNo=o(" (UniSpeechSat model)"),ENo=l(),n5=a("li"),uie=a("strong"),yNo=o("wav2vec2"),wNo=o(" \u2014 "),CN=a("a"),ANo=o("Wav2Vec2ForXVector"),LNo=o(" (Wav2Vec2 model)"),BNo=l(),s5=a("li"),bie=a("strong"),kNo=o("wavlm"),xNo=o(" \u2014 "),MN=a("a"),RNo=o("WavLMForXVector"),SNo=o(" (WavLM model)"),PNo=l(),l5=a("p"),$No=o("The model is set in evaluation mode by default using "),vie=a("code"),INo=o("model.eval()"),jNo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Tie=a("code"),NNo=o("model.train()"),DNo=l(),Fie=a("p"),qNo=o("Examples:"),GNo=l(),f(Zy.$$.fragment),K8e=l(),Od=a("h2"),i5=a("a"),Cie=a("span"),f(ew.$$.fragment),ONo=l(),Mie=a("span"),XNo=o("AutoModelForMaskedImageModeling"),Z8e=l(),dr=a("div"),f(ow.$$.fragment),zNo=l(),Xd=a("p"),VNo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a masked image modeling head) when created
with the `),Eie=a("code"),WNo=o("from_pretrained()"),QNo=o("class method or the "),yie=a("code"),HNo=o("from_config()"),UNo=o(`class
method.`),JNo=l(),rw=a("p"),YNo=o("This class cannot be instantiated directly using "),wie=a("code"),KNo=o("__init__()"),ZNo=o(" (throws an error)."),eDo=l(),rt=a("div"),f(tw.$$.fragment),oDo=l(),Aie=a("p"),rDo=o("Instantiates one of the model classes of the library (with a masked image modeling head) from a configuration."),tDo=l(),zd=a("p"),aDo=o(`Note:
Loading a model from its configuration file does `),Lie=a("strong"),nDo=o("not"),sDo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Bie=a("code"),lDo=o("from_pretrained()"),iDo=o("to load the model weights."),dDo=l(),kie=a("p"),cDo=o("Examples:"),fDo=l(),f(aw.$$.fragment),mDo=l(),He=a("div"),f(nw.$$.fragment),gDo=l(),xie=a("p"),hDo=o("Instantiate one of the model classes of the library (with a masked image modeling head) from a pretrained model."),pDo=l(),tn=a("p"),_Do=o("The model class to instantiate is selected based on the "),Rie=a("code"),uDo=o("model_type"),bDo=o(` property of the config object (either
passed as an argument or loaded from `),Sie=a("code"),vDo=o("pretrained_model_name_or_path"),TDo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Pie=a("code"),FDo=o("pretrained_model_name_or_path"),CDo=o(":"),MDo=l(),Vd=a("ul"),d5=a("li"),$ie=a("strong"),EDo=o("deit"),yDo=o(" \u2014 "),EN=a("a"),wDo=o("DeiTForMaskedImageModeling"),ADo=o(" (DeiT model)"),LDo=l(),c5=a("li"),Iie=a("strong"),BDo=o("swin"),kDo=o(" \u2014 "),yN=a("a"),xDo=o("SwinForMaskedImageModeling"),RDo=o(" (Swin model)"),SDo=l(),f5=a("li"),jie=a("strong"),PDo=o("vit"),$Do=o(" \u2014 "),wN=a("a"),IDo=o("ViTForMaskedImageModeling"),jDo=o(" (ViT model)"),NDo=l(),m5=a("p"),DDo=o("The model is set in evaluation mode by default using "),Nie=a("code"),qDo=o("model.eval()"),GDo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Die=a("code"),ODo=o("model.train()"),XDo=l(),qie=a("p"),zDo=o("Examples:"),VDo=l(),f(sw.$$.fragment),e9e=l(),Wd=a("h2"),g5=a("a"),Gie=a("span"),f(lw.$$.fragment),WDo=l(),Oie=a("span"),QDo=o("AutoModelForObjectDetection"),o9e=l(),cr=a("div"),f(iw.$$.fragment),HDo=l(),Qd=a("p"),UDo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a object detection head) when created
with the `),Xie=a("code"),JDo=o("from_pretrained()"),YDo=o("class method or the "),zie=a("code"),KDo=o("from_config()"),ZDo=o(`class
method.`),eqo=l(),dw=a("p"),oqo=o("This class cannot be instantiated directly using "),Vie=a("code"),rqo=o("__init__()"),tqo=o(" (throws an error)."),aqo=l(),tt=a("div"),f(cw.$$.fragment),nqo=l(),Wie=a("p"),sqo=o("Instantiates one of the model classes of the library (with a object detection head) from a configuration."),lqo=l(),Hd=a("p"),iqo=o(`Note:
Loading a model from its configuration file does `),Qie=a("strong"),dqo=o("not"),cqo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Hie=a("code"),fqo=o("from_pretrained()"),mqo=o("to load the model weights."),gqo=l(),Uie=a("p"),hqo=o("Examples:"),pqo=l(),f(fw.$$.fragment),_qo=l(),Ue=a("div"),f(mw.$$.fragment),uqo=l(),Jie=a("p"),bqo=o("Instantiate one of the model classes of the library (with a object detection head) from a pretrained model."),vqo=l(),an=a("p"),Tqo=o("The model class to instantiate is selected based on the "),Yie=a("code"),Fqo=o("model_type"),Cqo=o(` property of the config object (either
passed as an argument or loaded from `),Kie=a("code"),Mqo=o("pretrained_model_name_or_path"),Eqo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Zie=a("code"),yqo=o("pretrained_model_name_or_path"),wqo=o(":"),Aqo=l(),ede=a("ul"),h5=a("li"),ode=a("strong"),Lqo=o("detr"),Bqo=o(" \u2014 "),AN=a("a"),kqo=o("DetrForObjectDetection"),xqo=o(" (DETR model)"),Rqo=l(),p5=a("p"),Sqo=o("The model is set in evaluation mode by default using "),rde=a("code"),Pqo=o("model.eval()"),$qo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),tde=a("code"),Iqo=o("model.train()"),jqo=l(),ade=a("p"),Nqo=o("Examples:"),Dqo=l(),f(gw.$$.fragment),r9e=l(),Ud=a("h2"),_5=a("a"),nde=a("span"),f(hw.$$.fragment),qqo=l(),sde=a("span"),Gqo=o("AutoModelForImageSegmentation"),t9e=l(),fr=a("div"),f(pw.$$.fragment),Oqo=l(),Jd=a("p"),Xqo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a image segmentation head) when created
with the `),lde=a("code"),zqo=o("from_pretrained()"),Vqo=o("class method or the "),ide=a("code"),Wqo=o("from_config()"),Qqo=o(`class
method.`),Hqo=l(),_w=a("p"),Uqo=o("This class cannot be instantiated directly using "),dde=a("code"),Jqo=o("__init__()"),Yqo=o(" (throws an error)."),Kqo=l(),at=a("div"),f(uw.$$.fragment),Zqo=l(),cde=a("p"),eGo=o("Instantiates one of the model classes of the library (with a image segmentation head) from a configuration."),oGo=l(),Yd=a("p"),rGo=o(`Note:
Loading a model from its configuration file does `),fde=a("strong"),tGo=o("not"),aGo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),mde=a("code"),nGo=o("from_pretrained()"),sGo=o("to load the model weights."),lGo=l(),gde=a("p"),iGo=o("Examples:"),dGo=l(),f(bw.$$.fragment),cGo=l(),Je=a("div"),f(vw.$$.fragment),fGo=l(),hde=a("p"),mGo=o("Instantiate one of the model classes of the library (with a image segmentation head) from a pretrained model."),gGo=l(),nn=a("p"),hGo=o("The model class to instantiate is selected based on the "),pde=a("code"),pGo=o("model_type"),_Go=o(` property of the config object (either
passed as an argument or loaded from `),_de=a("code"),uGo=o("pretrained_model_name_or_path"),bGo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),ude=a("code"),vGo=o("pretrained_model_name_or_path"),TGo=o(":"),FGo=l(),bde=a("ul"),u5=a("li"),vde=a("strong"),CGo=o("detr"),MGo=o(" \u2014 "),LN=a("a"),EGo=o("DetrForSegmentation"),yGo=o(" (DETR model)"),wGo=l(),b5=a("p"),AGo=o("The model is set in evaluation mode by default using "),Tde=a("code"),LGo=o("model.eval()"),BGo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Fde=a("code"),kGo=o("model.train()"),xGo=l(),Cde=a("p"),RGo=o("Examples:"),SGo=l(),f(Tw.$$.fragment),a9e=l(),Kd=a("h2"),v5=a("a"),Mde=a("span"),f(Fw.$$.fragment),PGo=l(),Ede=a("span"),$Go=o("AutoModelForSemanticSegmentation"),n9e=l(),mr=a("div"),f(Cw.$$.fragment),IGo=l(),Zd=a("p"),jGo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a semantic segmentation head) when created
with the `),yde=a("code"),NGo=o("from_pretrained()"),DGo=o("class method or the "),wde=a("code"),qGo=o("from_config()"),GGo=o(`class
method.`),OGo=l(),Mw=a("p"),XGo=o("This class cannot be instantiated directly using "),Ade=a("code"),zGo=o("__init__()"),VGo=o(" (throws an error)."),WGo=l(),nt=a("div"),f(Ew.$$.fragment),QGo=l(),Lde=a("p"),HGo=o("Instantiates one of the model classes of the library (with a semantic segmentation head) from a configuration."),UGo=l(),ec=a("p"),JGo=o(`Note:
Loading a model from its configuration file does `),Bde=a("strong"),YGo=o("not"),KGo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),kde=a("code"),ZGo=o("from_pretrained()"),eOo=o("to load the model weights."),oOo=l(),xde=a("p"),rOo=o("Examples:"),tOo=l(),f(yw.$$.fragment),aOo=l(),Ye=a("div"),f(ww.$$.fragment),nOo=l(),Rde=a("p"),sOo=o("Instantiate one of the model classes of the library (with a semantic segmentation head) from a pretrained model."),lOo=l(),sn=a("p"),iOo=o("The model class to instantiate is selected based on the "),Sde=a("code"),dOo=o("model_type"),cOo=o(` property of the config object (either
passed as an argument or loaded from `),Pde=a("code"),fOo=o("pretrained_model_name_or_path"),mOo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),$de=a("code"),gOo=o("pretrained_model_name_or_path"),hOo=o(":"),pOo=l(),Aw=a("ul"),T5=a("li"),Ide=a("strong"),_Oo=o("beit"),uOo=o(" \u2014 "),BN=a("a"),bOo=o("BeitForSemanticSegmentation"),vOo=o(" (BEiT model)"),TOo=l(),F5=a("li"),jde=a("strong"),FOo=o("segformer"),COo=o(" \u2014 "),kN=a("a"),MOo=o("SegformerForSemanticSegmentation"),EOo=o(" (SegFormer model)"),yOo=l(),C5=a("p"),wOo=o("The model is set in evaluation mode by default using "),Nde=a("code"),AOo=o("model.eval()"),LOo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Dde=a("code"),BOo=o("model.train()"),kOo=l(),qde=a("p"),xOo=o("Examples:"),ROo=l(),f(Lw.$$.fragment),s9e=l(),oc=a("h2"),M5=a("a"),Gde=a("span"),f(Bw.$$.fragment),SOo=l(),Ode=a("span"),POo=o("TFAutoModel"),l9e=l(),gr=a("div"),f(kw.$$.fragment),$Oo=l(),rc=a("p"),IOo=o(`This is a generic model class that will be instantiated as one of the base model classes of the library when created
with the `),Xde=a("code"),jOo=o("from_pretrained()"),NOo=o("class method or the "),zde=a("code"),DOo=o("from_config()"),qOo=o(`class
method.`),GOo=l(),xw=a("p"),OOo=o("This class cannot be instantiated directly using "),Vde=a("code"),XOo=o("__init__()"),zOo=o(" (throws an error)."),VOo=l(),st=a("div"),f(Rw.$$.fragment),WOo=l(),Wde=a("p"),QOo=o("Instantiates one of the base model classes of the library from a configuration."),HOo=l(),tc=a("p"),UOo=o(`Note:
Loading a model from its configuration file does `),Qde=a("strong"),JOo=o("not"),YOo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Hde=a("code"),KOo=o("from_pretrained()"),ZOo=o("to load the model weights."),eXo=l(),Ude=a("p"),oXo=o("Examples:"),rXo=l(),f(Sw.$$.fragment),tXo=l(),go=a("div"),f(Pw.$$.fragment),aXo=l(),Jde=a("p"),nXo=o("Instantiate one of the base model classes of the library from a pretrained model."),sXo=l(),ln=a("p"),lXo=o("The model class to instantiate is selected based on the "),Yde=a("code"),iXo=o("model_type"),dXo=o(` property of the config object (either
passed as an argument or loaded from `),Kde=a("code"),cXo=o("pretrained_model_name_or_path"),fXo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Zde=a("code"),mXo=o("pretrained_model_name_or_path"),gXo=o(":"),hXo=l(),B=a("ul"),E5=a("li"),ece=a("strong"),pXo=o("albert"),_Xo=o(" \u2014 "),xN=a("a"),uXo=o("TFAlbertModel"),bXo=o(" (ALBERT model)"),vXo=l(),y5=a("li"),oce=a("strong"),TXo=o("bart"),FXo=o(" \u2014 "),RN=a("a"),CXo=o("TFBartModel"),MXo=o(" (BART model)"),EXo=l(),w5=a("li"),rce=a("strong"),yXo=o("bert"),wXo=o(" \u2014 "),SN=a("a"),AXo=o("TFBertModel"),LXo=o(" (BERT model)"),BXo=l(),A5=a("li"),tce=a("strong"),kXo=o("blenderbot"),xXo=o(" \u2014 "),PN=a("a"),RXo=o("TFBlenderbotModel"),SXo=o(" (Blenderbot model)"),PXo=l(),L5=a("li"),ace=a("strong"),$Xo=o("blenderbot-small"),IXo=o(" \u2014 "),$N=a("a"),jXo=o("TFBlenderbotSmallModel"),NXo=o(" (BlenderbotSmall model)"),DXo=l(),B5=a("li"),nce=a("strong"),qXo=o("camembert"),GXo=o(" \u2014 "),IN=a("a"),OXo=o("TFCamembertModel"),XXo=o(" (CamemBERT model)"),zXo=l(),k5=a("li"),sce=a("strong"),VXo=o("clip"),WXo=o(" \u2014 "),jN=a("a"),QXo=o("TFCLIPModel"),HXo=o(" (CLIP model)"),UXo=l(),x5=a("li"),lce=a("strong"),JXo=o("convbert"),YXo=o(" \u2014 "),NN=a("a"),KXo=o("TFConvBertModel"),ZXo=o(" (ConvBERT model)"),ezo=l(),R5=a("li"),ice=a("strong"),ozo=o("ctrl"),rzo=o(" \u2014 "),DN=a("a"),tzo=o("TFCTRLModel"),azo=o(" (CTRL model)"),nzo=l(),S5=a("li"),dce=a("strong"),szo=o("deberta"),lzo=o(" \u2014 "),qN=a("a"),izo=o("TFDebertaModel"),dzo=o(" (DeBERTa model)"),czo=l(),P5=a("li"),cce=a("strong"),fzo=o("deberta-v2"),mzo=o(" \u2014 "),GN=a("a"),gzo=o("TFDebertaV2Model"),hzo=o(" (DeBERTa-v2 model)"),pzo=l(),$5=a("li"),fce=a("strong"),_zo=o("distilbert"),uzo=o(" \u2014 "),ON=a("a"),bzo=o("TFDistilBertModel"),vzo=o(" (DistilBERT model)"),Tzo=l(),I5=a("li"),mce=a("strong"),Fzo=o("dpr"),Czo=o(" \u2014 "),XN=a("a"),Mzo=o("TFDPRQuestionEncoder"),Ezo=o(" (DPR model)"),yzo=l(),j5=a("li"),gce=a("strong"),wzo=o("electra"),Azo=o(" \u2014 "),zN=a("a"),Lzo=o("TFElectraModel"),Bzo=o(" (ELECTRA model)"),kzo=l(),N5=a("li"),hce=a("strong"),xzo=o("flaubert"),Rzo=o(" \u2014 "),VN=a("a"),Szo=o("TFFlaubertModel"),Pzo=o(" (FlauBERT model)"),$zo=l(),Ss=a("li"),pce=a("strong"),Izo=o("funnel"),jzo=o(" \u2014 "),WN=a("a"),Nzo=o("TFFunnelModel"),Dzo=o(" or "),QN=a("a"),qzo=o("TFFunnelBaseModel"),Gzo=o(" (Funnel Transformer model)"),Ozo=l(),D5=a("li"),_ce=a("strong"),Xzo=o("gpt2"),zzo=o(" \u2014 "),HN=a("a"),Vzo=o("TFGPT2Model"),Wzo=o(" (OpenAI GPT-2 model)"),Qzo=l(),q5=a("li"),uce=a("strong"),Hzo=o("hubert"),Uzo=o(" \u2014 "),UN=a("a"),Jzo=o("TFHubertModel"),Yzo=o(" (Hubert model)"),Kzo=l(),G5=a("li"),bce=a("strong"),Zzo=o("layoutlm"),eVo=o(" \u2014 "),JN=a("a"),oVo=o("TFLayoutLMModel"),rVo=o(" (LayoutLM model)"),tVo=l(),O5=a("li"),vce=a("strong"),aVo=o("led"),nVo=o(" \u2014 "),YN=a("a"),sVo=o("TFLEDModel"),lVo=o(" (LED model)"),iVo=l(),X5=a("li"),Tce=a("strong"),dVo=o("longformer"),cVo=o(" \u2014 "),KN=a("a"),fVo=o("TFLongformerModel"),mVo=o(" (Longformer model)"),gVo=l(),z5=a("li"),Fce=a("strong"),hVo=o("lxmert"),pVo=o(" \u2014 "),ZN=a("a"),_Vo=o("TFLxmertModel"),uVo=o(" (LXMERT model)"),bVo=l(),V5=a("li"),Cce=a("strong"),vVo=o("marian"),TVo=o(" \u2014 "),eD=a("a"),FVo=o("TFMarianModel"),CVo=o(" (Marian model)"),MVo=l(),W5=a("li"),Mce=a("strong"),EVo=o("mbart"),yVo=o(" \u2014 "),oD=a("a"),wVo=o("TFMBartModel"),AVo=o(" (mBART model)"),LVo=l(),Q5=a("li"),Ece=a("strong"),BVo=o("mobilebert"),kVo=o(" \u2014 "),rD=a("a"),xVo=o("TFMobileBertModel"),RVo=o(" (MobileBERT model)"),SVo=l(),H5=a("li"),yce=a("strong"),PVo=o("mpnet"),$Vo=o(" \u2014 "),tD=a("a"),IVo=o("TFMPNetModel"),jVo=o(" (MPNet model)"),NVo=l(),U5=a("li"),wce=a("strong"),DVo=o("mt5"),qVo=o(" \u2014 "),aD=a("a"),GVo=o("TFMT5Model"),OVo=o(" (mT5 model)"),XVo=l(),J5=a("li"),Ace=a("strong"),zVo=o("openai-gpt"),VVo=o(" \u2014 "),nD=a("a"),WVo=o("TFOpenAIGPTModel"),QVo=o(" (OpenAI GPT model)"),HVo=l(),Y5=a("li"),Lce=a("strong"),UVo=o("pegasus"),JVo=o(" \u2014 "),sD=a("a"),YVo=o("TFPegasusModel"),KVo=o(" (Pegasus model)"),ZVo=l(),K5=a("li"),Bce=a("strong"),eWo=o("rembert"),oWo=o(" \u2014 "),lD=a("a"),rWo=o("TFRemBertModel"),tWo=o(" (RemBERT model)"),aWo=l(),Z5=a("li"),kce=a("strong"),nWo=o("roberta"),sWo=o(" \u2014 "),iD=a("a"),lWo=o("TFRobertaModel"),iWo=o(" (RoBERTa model)"),dWo=l(),e2=a("li"),xce=a("strong"),cWo=o("roformer"),fWo=o(" \u2014 "),dD=a("a"),mWo=o("TFRoFormerModel"),gWo=o(" (RoFormer model)"),hWo=l(),o2=a("li"),Rce=a("strong"),pWo=o("speech_to_text"),_Wo=o(" \u2014 "),cD=a("a"),uWo=o("TFSpeech2TextModel"),bWo=o(" (Speech2Text model)"),vWo=l(),r2=a("li"),Sce=a("strong"),TWo=o("t5"),FWo=o(" \u2014 "),fD=a("a"),CWo=o("TFT5Model"),MWo=o(" (T5 model)"),EWo=l(),t2=a("li"),Pce=a("strong"),yWo=o("tapas"),wWo=o(" \u2014 "),mD=a("a"),AWo=o("TFTapasModel"),LWo=o(" (TAPAS model)"),BWo=l(),a2=a("li"),$ce=a("strong"),kWo=o("transfo-xl"),xWo=o(" \u2014 "),gD=a("a"),RWo=o("TFTransfoXLModel"),SWo=o(" (Transformer-XL model)"),PWo=l(),n2=a("li"),Ice=a("strong"),$Wo=o("vit"),IWo=o(" \u2014 "),hD=a("a"),jWo=o("TFViTModel"),NWo=o(" (ViT model)"),DWo=l(),s2=a("li"),jce=a("strong"),qWo=o("wav2vec2"),GWo=o(" \u2014 "),pD=a("a"),OWo=o("TFWav2Vec2Model"),XWo=o(" (Wav2Vec2 model)"),zWo=l(),l2=a("li"),Nce=a("strong"),VWo=o("xlm"),WWo=o(" \u2014 "),_D=a("a"),QWo=o("TFXLMModel"),HWo=o(" (XLM model)"),UWo=l(),i2=a("li"),Dce=a("strong"),JWo=o("xlm-roberta"),YWo=o(" \u2014 "),uD=a("a"),KWo=o("TFXLMRobertaModel"),ZWo=o(" (XLM-RoBERTa model)"),eQo=l(),d2=a("li"),qce=a("strong"),oQo=o("xlnet"),rQo=o(" \u2014 "),bD=a("a"),tQo=o("TFXLNetModel"),aQo=o(" (XLNet model)"),nQo=l(),Gce=a("p"),sQo=o("Examples:"),lQo=l(),f($w.$$.fragment),i9e=l(),ac=a("h2"),c2=a("a"),Oce=a("span"),f(Iw.$$.fragment),iQo=l(),Xce=a("span"),dQo=o("TFAutoModelForPreTraining"),d9e=l(),hr=a("div"),f(jw.$$.fragment),cQo=l(),nc=a("p"),fQo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a pretraining head) when created
with the `),zce=a("code"),mQo=o("from_pretrained()"),gQo=o("class method or the "),Vce=a("code"),hQo=o("from_config()"),pQo=o(`class
method.`),_Qo=l(),Nw=a("p"),uQo=o("This class cannot be instantiated directly using "),Wce=a("code"),bQo=o("__init__()"),vQo=o(" (throws an error)."),TQo=l(),lt=a("div"),f(Dw.$$.fragment),FQo=l(),Qce=a("p"),CQo=o("Instantiates one of the model classes of the library (with a pretraining head) from a configuration."),MQo=l(),sc=a("p"),EQo=o(`Note:
Loading a model from its configuration file does `),Hce=a("strong"),yQo=o("not"),wQo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Uce=a("code"),AQo=o("from_pretrained()"),LQo=o("to load the model weights."),BQo=l(),Jce=a("p"),kQo=o("Examples:"),xQo=l(),f(qw.$$.fragment),RQo=l(),ho=a("div"),f(Gw.$$.fragment),SQo=l(),Yce=a("p"),PQo=o("Instantiate one of the model classes of the library (with a pretraining head) from a pretrained model."),$Qo=l(),dn=a("p"),IQo=o("The model class to instantiate is selected based on the "),Kce=a("code"),jQo=o("model_type"),NQo=o(` property of the config object (either
passed as an argument or loaded from `),Zce=a("code"),DQo=o("pretrained_model_name_or_path"),qQo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),efe=a("code"),GQo=o("pretrained_model_name_or_path"),OQo=o(":"),XQo=l(),H=a("ul"),f2=a("li"),ofe=a("strong"),zQo=o("albert"),VQo=o(" \u2014 "),vD=a("a"),WQo=o("TFAlbertForPreTraining"),QQo=o(" (ALBERT model)"),HQo=l(),m2=a("li"),rfe=a("strong"),UQo=o("bart"),JQo=o(" \u2014 "),TD=a("a"),YQo=o("TFBartForConditionalGeneration"),KQo=o(" (BART model)"),ZQo=l(),g2=a("li"),tfe=a("strong"),eHo=o("bert"),oHo=o(" \u2014 "),FD=a("a"),rHo=o("TFBertForPreTraining"),tHo=o(" (BERT model)"),aHo=l(),h2=a("li"),afe=a("strong"),nHo=o("camembert"),sHo=o(" \u2014 "),CD=a("a"),lHo=o("TFCamembertForMaskedLM"),iHo=o(" (CamemBERT model)"),dHo=l(),p2=a("li"),nfe=a("strong"),cHo=o("ctrl"),fHo=o(" \u2014 "),MD=a("a"),mHo=o("TFCTRLLMHeadModel"),gHo=o(" (CTRL model)"),hHo=l(),_2=a("li"),sfe=a("strong"),pHo=o("distilbert"),_Ho=o(" \u2014 "),ED=a("a"),uHo=o("TFDistilBertForMaskedLM"),bHo=o(" (DistilBERT model)"),vHo=l(),u2=a("li"),lfe=a("strong"),THo=o("electra"),FHo=o(" \u2014 "),yD=a("a"),CHo=o("TFElectraForPreTraining"),MHo=o(" (ELECTRA model)"),EHo=l(),b2=a("li"),ife=a("strong"),yHo=o("flaubert"),wHo=o(" \u2014 "),wD=a("a"),AHo=o("TFFlaubertWithLMHeadModel"),LHo=o(" (FlauBERT model)"),BHo=l(),v2=a("li"),dfe=a("strong"),kHo=o("funnel"),xHo=o(" \u2014 "),AD=a("a"),RHo=o("TFFunnelForPreTraining"),SHo=o(" (Funnel Transformer model)"),PHo=l(),T2=a("li"),cfe=a("strong"),$Ho=o("gpt2"),IHo=o(" \u2014 "),LD=a("a"),jHo=o("TFGPT2LMHeadModel"),NHo=o(" (OpenAI GPT-2 model)"),DHo=l(),F2=a("li"),ffe=a("strong"),qHo=o("layoutlm"),GHo=o(" \u2014 "),BD=a("a"),OHo=o("TFLayoutLMForMaskedLM"),XHo=o(" (LayoutLM model)"),zHo=l(),C2=a("li"),mfe=a("strong"),VHo=o("lxmert"),WHo=o(" \u2014 "),kD=a("a"),QHo=o("TFLxmertForPreTraining"),HHo=o(" (LXMERT model)"),UHo=l(),M2=a("li"),gfe=a("strong"),JHo=o("mobilebert"),YHo=o(" \u2014 "),xD=a("a"),KHo=o("TFMobileBertForPreTraining"),ZHo=o(" (MobileBERT model)"),eUo=l(),E2=a("li"),hfe=a("strong"),oUo=o("mpnet"),rUo=o(" \u2014 "),RD=a("a"),tUo=o("TFMPNetForMaskedLM"),aUo=o(" (MPNet model)"),nUo=l(),y2=a("li"),pfe=a("strong"),sUo=o("openai-gpt"),lUo=o(" \u2014 "),SD=a("a"),iUo=o("TFOpenAIGPTLMHeadModel"),dUo=o(" (OpenAI GPT model)"),cUo=l(),w2=a("li"),_fe=a("strong"),fUo=o("roberta"),mUo=o(" \u2014 "),PD=a("a"),gUo=o("TFRobertaForMaskedLM"),hUo=o(" (RoBERTa model)"),pUo=l(),A2=a("li"),ufe=a("strong"),_Uo=o("t5"),uUo=o(" \u2014 "),$D=a("a"),bUo=o("TFT5ForConditionalGeneration"),vUo=o(" (T5 model)"),TUo=l(),L2=a("li"),bfe=a("strong"),FUo=o("tapas"),CUo=o(" \u2014 "),ID=a("a"),MUo=o("TFTapasForMaskedLM"),EUo=o(" (TAPAS model)"),yUo=l(),B2=a("li"),vfe=a("strong"),wUo=o("transfo-xl"),AUo=o(" \u2014 "),jD=a("a"),LUo=o("TFTransfoXLLMHeadModel"),BUo=o(" (Transformer-XL model)"),kUo=l(),k2=a("li"),Tfe=a("strong"),xUo=o("xlm"),RUo=o(" \u2014 "),ND=a("a"),SUo=o("TFXLMWithLMHeadModel"),PUo=o(" (XLM model)"),$Uo=l(),x2=a("li"),Ffe=a("strong"),IUo=o("xlm-roberta"),jUo=o(" \u2014 "),DD=a("a"),NUo=o("TFXLMRobertaForMaskedLM"),DUo=o(" (XLM-RoBERTa model)"),qUo=l(),R2=a("li"),Cfe=a("strong"),GUo=o("xlnet"),OUo=o(" \u2014 "),qD=a("a"),XUo=o("TFXLNetLMHeadModel"),zUo=o(" (XLNet model)"),VUo=l(),Mfe=a("p"),WUo=o("Examples:"),QUo=l(),f(Ow.$$.fragment),c9e=l(),lc=a("h2"),S2=a("a"),Efe=a("span"),f(Xw.$$.fragment),HUo=l(),yfe=a("span"),UUo=o("TFAutoModelForCausalLM"),f9e=l(),pr=a("div"),f(zw.$$.fragment),JUo=l(),ic=a("p"),YUo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a causal language modeling head) when created
with the `),wfe=a("code"),KUo=o("from_pretrained()"),ZUo=o("class method or the "),Afe=a("code"),eJo=o("from_config()"),oJo=o(`class
method.`),rJo=l(),Vw=a("p"),tJo=o("This class cannot be instantiated directly using "),Lfe=a("code"),aJo=o("__init__()"),nJo=o(" (throws an error)."),sJo=l(),it=a("div"),f(Ww.$$.fragment),lJo=l(),Bfe=a("p"),iJo=o("Instantiates one of the model classes of the library (with a causal language modeling head) from a configuration."),dJo=l(),dc=a("p"),cJo=o(`Note:
Loading a model from its configuration file does `),kfe=a("strong"),fJo=o("not"),mJo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),xfe=a("code"),gJo=o("from_pretrained()"),hJo=o("to load the model weights."),pJo=l(),Rfe=a("p"),_Jo=o("Examples:"),uJo=l(),f(Qw.$$.fragment),bJo=l(),po=a("div"),f(Hw.$$.fragment),vJo=l(),Sfe=a("p"),TJo=o("Instantiate one of the model classes of the library (with a causal language modeling head) from a pretrained model."),FJo=l(),cn=a("p"),CJo=o("The model class to instantiate is selected based on the "),Pfe=a("code"),MJo=o("model_type"),EJo=o(` property of the config object (either
passed as an argument or loaded from `),$fe=a("code"),yJo=o("pretrained_model_name_or_path"),wJo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Ife=a("code"),AJo=o("pretrained_model_name_or_path"),LJo=o(":"),BJo=l(),he=a("ul"),P2=a("li"),jfe=a("strong"),kJo=o("bert"),xJo=o(" \u2014 "),GD=a("a"),RJo=o("TFBertLMHeadModel"),SJo=o(" (BERT model)"),PJo=l(),$2=a("li"),Nfe=a("strong"),$Jo=o("ctrl"),IJo=o(" \u2014 "),OD=a("a"),jJo=o("TFCTRLLMHeadModel"),NJo=o(" (CTRL model)"),DJo=l(),I2=a("li"),Dfe=a("strong"),qJo=o("gpt2"),GJo=o(" \u2014 "),XD=a("a"),OJo=o("TFGPT2LMHeadModel"),XJo=o(" (OpenAI GPT-2 model)"),zJo=l(),j2=a("li"),qfe=a("strong"),VJo=o("openai-gpt"),WJo=o(" \u2014 "),zD=a("a"),QJo=o("TFOpenAIGPTLMHeadModel"),HJo=o(" (OpenAI GPT model)"),UJo=l(),N2=a("li"),Gfe=a("strong"),JJo=o("rembert"),YJo=o(" \u2014 "),VD=a("a"),KJo=o("TFRemBertForCausalLM"),ZJo=o(" (RemBERT model)"),eYo=l(),D2=a("li"),Ofe=a("strong"),oYo=o("roberta"),rYo=o(" \u2014 "),WD=a("a"),tYo=o("TFRobertaForCausalLM"),aYo=o(" (RoBERTa model)"),nYo=l(),q2=a("li"),Xfe=a("strong"),sYo=o("roformer"),lYo=o(" \u2014 "),QD=a("a"),iYo=o("TFRoFormerForCausalLM"),dYo=o(" (RoFormer model)"),cYo=l(),G2=a("li"),zfe=a("strong"),fYo=o("transfo-xl"),mYo=o(" \u2014 "),HD=a("a"),gYo=o("TFTransfoXLLMHeadModel"),hYo=o(" (Transformer-XL model)"),pYo=l(),O2=a("li"),Vfe=a("strong"),_Yo=o("xlm"),uYo=o(" \u2014 "),UD=a("a"),bYo=o("TFXLMWithLMHeadModel"),vYo=o(" (XLM model)"),TYo=l(),X2=a("li"),Wfe=a("strong"),FYo=o("xlnet"),CYo=o(" \u2014 "),JD=a("a"),MYo=o("TFXLNetLMHeadModel"),EYo=o(" (XLNet model)"),yYo=l(),Qfe=a("p"),wYo=o("Examples:"),AYo=l(),f(Uw.$$.fragment),m9e=l(),cc=a("h2"),z2=a("a"),Hfe=a("span"),f(Jw.$$.fragment),LYo=l(),Ufe=a("span"),BYo=o("TFAutoModelForImageClassification"),g9e=l(),_r=a("div"),f(Yw.$$.fragment),kYo=l(),fc=a("p"),xYo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a image classification head) when created
with the `),Jfe=a("code"),RYo=o("from_pretrained()"),SYo=o("class method or the "),Yfe=a("code"),PYo=o("from_config()"),$Yo=o(`class
method.`),IYo=l(),Kw=a("p"),jYo=o("This class cannot be instantiated directly using "),Kfe=a("code"),NYo=o("__init__()"),DYo=o(" (throws an error)."),qYo=l(),dt=a("div"),f(Zw.$$.fragment),GYo=l(),Zfe=a("p"),OYo=o("Instantiates one of the model classes of the library (with a image classification head) from a configuration."),XYo=l(),mc=a("p"),zYo=o(`Note:
Loading a model from its configuration file does `),eme=a("strong"),VYo=o("not"),WYo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),ome=a("code"),QYo=o("from_pretrained()"),HYo=o("to load the model weights."),UYo=l(),rme=a("p"),JYo=o("Examples:"),YYo=l(),f(eA.$$.fragment),KYo=l(),_o=a("div"),f(oA.$$.fragment),ZYo=l(),tme=a("p"),eKo=o("Instantiate one of the model classes of the library (with a image classification head) from a pretrained model."),oKo=l(),fn=a("p"),rKo=o("The model class to instantiate is selected based on the "),ame=a("code"),tKo=o("model_type"),aKo=o(` property of the config object (either
passed as an argument or loaded from `),nme=a("code"),nKo=o("pretrained_model_name_or_path"),sKo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),sme=a("code"),lKo=o("pretrained_model_name_or_path"),iKo=o(":"),dKo=l(),lme=a("ul"),V2=a("li"),ime=a("strong"),cKo=o("vit"),fKo=o(" \u2014 "),YD=a("a"),mKo=o("TFViTForImageClassification"),gKo=o(" (ViT model)"),hKo=l(),dme=a("p"),pKo=o("Examples:"),_Ko=l(),f(rA.$$.fragment),h9e=l(),gc=a("h2"),W2=a("a"),cme=a("span"),f(tA.$$.fragment),uKo=l(),fme=a("span"),bKo=o("TFAutoModelForMaskedLM"),p9e=l(),ur=a("div"),f(aA.$$.fragment),vKo=l(),hc=a("p"),TKo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a masked language modeling head) when created
with the `),mme=a("code"),FKo=o("from_pretrained()"),CKo=o("class method or the "),gme=a("code"),MKo=o("from_config()"),EKo=o(`class
method.`),yKo=l(),nA=a("p"),wKo=o("This class cannot be instantiated directly using "),hme=a("code"),AKo=o("__init__()"),LKo=o(" (throws an error)."),BKo=l(),ct=a("div"),f(sA.$$.fragment),kKo=l(),pme=a("p"),xKo=o("Instantiates one of the model classes of the library (with a masked language modeling head) from a configuration."),RKo=l(),pc=a("p"),SKo=o(`Note:
Loading a model from its configuration file does `),_me=a("strong"),PKo=o("not"),$Ko=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),ume=a("code"),IKo=o("from_pretrained()"),jKo=o("to load the model weights."),NKo=l(),bme=a("p"),DKo=o("Examples:"),qKo=l(),f(lA.$$.fragment),GKo=l(),uo=a("div"),f(iA.$$.fragment),OKo=l(),vme=a("p"),XKo=o("Instantiate one of the model classes of the library (with a masked language modeling head) from a pretrained model."),zKo=l(),mn=a("p"),VKo=o("The model class to instantiate is selected based on the "),Tme=a("code"),WKo=o("model_type"),QKo=o(` property of the config object (either
passed as an argument or loaded from `),Fme=a("code"),HKo=o("pretrained_model_name_or_path"),UKo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Cme=a("code"),JKo=o("pretrained_model_name_or_path"),YKo=o(":"),KKo=l(),Y=a("ul"),Q2=a("li"),Mme=a("strong"),ZKo=o("albert"),eZo=o(" \u2014 "),KD=a("a"),oZo=o("TFAlbertForMaskedLM"),rZo=o(" (ALBERT model)"),tZo=l(),H2=a("li"),Eme=a("strong"),aZo=o("bert"),nZo=o(" \u2014 "),ZD=a("a"),sZo=o("TFBertForMaskedLM"),lZo=o(" (BERT model)"),iZo=l(),U2=a("li"),yme=a("strong"),dZo=o("camembert"),cZo=o(" \u2014 "),eq=a("a"),fZo=o("TFCamembertForMaskedLM"),mZo=o(" (CamemBERT model)"),gZo=l(),J2=a("li"),wme=a("strong"),hZo=o("convbert"),pZo=o(" \u2014 "),oq=a("a"),_Zo=o("TFConvBertForMaskedLM"),uZo=o(" (ConvBERT model)"),bZo=l(),Y2=a("li"),Ame=a("strong"),vZo=o("deberta"),TZo=o(" \u2014 "),rq=a("a"),FZo=o("TFDebertaForMaskedLM"),CZo=o(" (DeBERTa model)"),MZo=l(),K2=a("li"),Lme=a("strong"),EZo=o("deberta-v2"),yZo=o(" \u2014 "),tq=a("a"),wZo=o("TFDebertaV2ForMaskedLM"),AZo=o(" (DeBERTa-v2 model)"),LZo=l(),Z2=a("li"),Bme=a("strong"),BZo=o("distilbert"),kZo=o(" \u2014 "),aq=a("a"),xZo=o("TFDistilBertForMaskedLM"),RZo=o(" (DistilBERT model)"),SZo=l(),ev=a("li"),kme=a("strong"),PZo=o("electra"),$Zo=o(" \u2014 "),nq=a("a"),IZo=o("TFElectraForMaskedLM"),jZo=o(" (ELECTRA model)"),NZo=l(),ov=a("li"),xme=a("strong"),DZo=o("flaubert"),qZo=o(" \u2014 "),sq=a("a"),GZo=o("TFFlaubertWithLMHeadModel"),OZo=o(" (FlauBERT model)"),XZo=l(),rv=a("li"),Rme=a("strong"),zZo=o("funnel"),VZo=o(" \u2014 "),lq=a("a"),WZo=o("TFFunnelForMaskedLM"),QZo=o(" (Funnel Transformer model)"),HZo=l(),tv=a("li"),Sme=a("strong"),UZo=o("layoutlm"),JZo=o(" \u2014 "),iq=a("a"),YZo=o("TFLayoutLMForMaskedLM"),KZo=o(" (LayoutLM model)"),ZZo=l(),av=a("li"),Pme=a("strong"),eer=o("longformer"),oer=o(" \u2014 "),dq=a("a"),rer=o("TFLongformerForMaskedLM"),ter=o(" (Longformer model)"),aer=l(),nv=a("li"),$me=a("strong"),ner=o("mobilebert"),ser=o(" \u2014 "),cq=a("a"),ler=o("TFMobileBertForMaskedLM"),ier=o(" (MobileBERT model)"),der=l(),sv=a("li"),Ime=a("strong"),cer=o("mpnet"),fer=o(" \u2014 "),fq=a("a"),mer=o("TFMPNetForMaskedLM"),ger=o(" (MPNet model)"),her=l(),lv=a("li"),jme=a("strong"),per=o("rembert"),_er=o(" \u2014 "),mq=a("a"),uer=o("TFRemBertForMaskedLM"),ber=o(" (RemBERT model)"),ver=l(),iv=a("li"),Nme=a("strong"),Ter=o("roberta"),Fer=o(" \u2014 "),gq=a("a"),Cer=o("TFRobertaForMaskedLM"),Mer=o(" (RoBERTa model)"),Eer=l(),dv=a("li"),Dme=a("strong"),yer=o("roformer"),wer=o(" \u2014 "),hq=a("a"),Aer=o("TFRoFormerForMaskedLM"),Ler=o(" (RoFormer model)"),Ber=l(),cv=a("li"),qme=a("strong"),ker=o("tapas"),xer=o(" \u2014 "),pq=a("a"),Rer=o("TFTapasForMaskedLM"),Ser=o(" (TAPAS model)"),Per=l(),fv=a("li"),Gme=a("strong"),$er=o("xlm"),Ier=o(" \u2014 "),_q=a("a"),jer=o("TFXLMWithLMHeadModel"),Ner=o(" (XLM model)"),Der=l(),mv=a("li"),Ome=a("strong"),qer=o("xlm-roberta"),Ger=o(" \u2014 "),uq=a("a"),Oer=o("TFXLMRobertaForMaskedLM"),Xer=o(" (XLM-RoBERTa model)"),zer=l(),Xme=a("p"),Ver=o("Examples:"),Wer=l(),f(dA.$$.fragment),_9e=l(),_c=a("h2"),gv=a("a"),zme=a("span"),f(cA.$$.fragment),Qer=l(),Vme=a("span"),Her=o("TFAutoModelForSeq2SeqLM"),u9e=l(),br=a("div"),f(fA.$$.fragment),Uer=l(),uc=a("p"),Jer=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence language modeling head) when created
with the `),Wme=a("code"),Yer=o("from_pretrained()"),Ker=o("class method or the "),Qme=a("code"),Zer=o("from_config()"),eor=o(`class
method.`),oor=l(),mA=a("p"),ror=o("This class cannot be instantiated directly using "),Hme=a("code"),tor=o("__init__()"),aor=o(" (throws an error)."),nor=l(),ft=a("div"),f(gA.$$.fragment),sor=l(),Ume=a("p"),lor=o("Instantiates one of the model classes of the library (with a sequence-to-sequence language modeling head) from a configuration."),ior=l(),bc=a("p"),dor=o(`Note:
Loading a model from its configuration file does `),Jme=a("strong"),cor=o("not"),mor=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Yme=a("code"),gor=o("from_pretrained()"),hor=o("to load the model weights."),por=l(),Kme=a("p"),_or=o("Examples:"),uor=l(),f(hA.$$.fragment),bor=l(),bo=a("div"),f(pA.$$.fragment),vor=l(),Zme=a("p"),Tor=o("Instantiate one of the model classes of the library (with a sequence-to-sequence language modeling head) from a pretrained model."),For=l(),gn=a("p"),Cor=o("The model class to instantiate is selected based on the "),ege=a("code"),Mor=o("model_type"),Eor=o(` property of the config object (either
passed as an argument or loaded from `),oge=a("code"),yor=o("pretrained_model_name_or_path"),wor=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),rge=a("code"),Aor=o("pretrained_model_name_or_path"),Lor=o(":"),Bor=l(),pe=a("ul"),hv=a("li"),tge=a("strong"),kor=o("bart"),xor=o(" \u2014 "),bq=a("a"),Ror=o("TFBartForConditionalGeneration"),Sor=o(" (BART model)"),Por=l(),pv=a("li"),age=a("strong"),$or=o("blenderbot"),Ior=o(" \u2014 "),vq=a("a"),jor=o("TFBlenderbotForConditionalGeneration"),Nor=o(" (Blenderbot model)"),Dor=l(),_v=a("li"),nge=a("strong"),qor=o("blenderbot-small"),Gor=o(" \u2014 "),Tq=a("a"),Oor=o("TFBlenderbotSmallForConditionalGeneration"),Xor=o(" (BlenderbotSmall model)"),zor=l(),uv=a("li"),sge=a("strong"),Vor=o("encoder-decoder"),Wor=o(" \u2014 "),Fq=a("a"),Qor=o("TFEncoderDecoderModel"),Hor=o(" (Encoder decoder model)"),Uor=l(),bv=a("li"),lge=a("strong"),Jor=o("led"),Yor=o(" \u2014 "),Cq=a("a"),Kor=o("TFLEDForConditionalGeneration"),Zor=o(" (LED model)"),err=l(),vv=a("li"),ige=a("strong"),orr=o("marian"),rrr=o(" \u2014 "),Mq=a("a"),trr=o("TFMarianMTModel"),arr=o(" (Marian model)"),nrr=l(),Tv=a("li"),dge=a("strong"),srr=o("mbart"),lrr=o(" \u2014 "),Eq=a("a"),irr=o("TFMBartForConditionalGeneration"),drr=o(" (mBART model)"),crr=l(),Fv=a("li"),cge=a("strong"),frr=o("mt5"),mrr=o(" \u2014 "),yq=a("a"),grr=o("TFMT5ForConditionalGeneration"),hrr=o(" (mT5 model)"),prr=l(),Cv=a("li"),fge=a("strong"),_rr=o("pegasus"),urr=o(" \u2014 "),wq=a("a"),brr=o("TFPegasusForConditionalGeneration"),vrr=o(" (Pegasus model)"),Trr=l(),Mv=a("li"),mge=a("strong"),Frr=o("t5"),Crr=o(" \u2014 "),Aq=a("a"),Mrr=o("TFT5ForConditionalGeneration"),Err=o(" (T5 model)"),yrr=l(),gge=a("p"),wrr=o("Examples:"),Arr=l(),f(_A.$$.fragment),b9e=l(),vc=a("h2"),Ev=a("a"),hge=a("span"),f(uA.$$.fragment),Lrr=l(),pge=a("span"),Brr=o("TFAutoModelForSequenceClassification"),v9e=l(),vr=a("div"),f(bA.$$.fragment),krr=l(),Tc=a("p"),xrr=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence classification head) when created
with the `),_ge=a("code"),Rrr=o("from_pretrained()"),Srr=o("class method or the "),uge=a("code"),Prr=o("from_config()"),$rr=o(`class
method.`),Irr=l(),vA=a("p"),jrr=o("This class cannot be instantiated directly using "),bge=a("code"),Nrr=o("__init__()"),Drr=o(" (throws an error)."),qrr=l(),mt=a("div"),f(TA.$$.fragment),Grr=l(),vge=a("p"),Orr=o("Instantiates one of the model classes of the library (with a sequence classification head) from a configuration."),Xrr=l(),Fc=a("p"),zrr=o(`Note:
Loading a model from its configuration file does `),Tge=a("strong"),Vrr=o("not"),Wrr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Fge=a("code"),Qrr=o("from_pretrained()"),Hrr=o("to load the model weights."),Urr=l(),Cge=a("p"),Jrr=o("Examples:"),Yrr=l(),f(FA.$$.fragment),Krr=l(),vo=a("div"),f(CA.$$.fragment),Zrr=l(),Mge=a("p"),etr=o("Instantiate one of the model classes of the library (with a sequence classification head) from a pretrained model."),otr=l(),hn=a("p"),rtr=o("The model class to instantiate is selected based on the "),Ege=a("code"),ttr=o("model_type"),atr=o(` property of the config object (either
passed as an argument or loaded from `),yge=a("code"),ntr=o("pretrained_model_name_or_path"),str=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),wge=a("code"),ltr=o("pretrained_model_name_or_path"),itr=o(":"),dtr=l(),X=a("ul"),yv=a("li"),Age=a("strong"),ctr=o("albert"),ftr=o(" \u2014 "),Lq=a("a"),mtr=o("TFAlbertForSequenceClassification"),gtr=o(" (ALBERT model)"),htr=l(),wv=a("li"),Lge=a("strong"),ptr=o("bert"),_tr=o(" \u2014 "),Bq=a("a"),utr=o("TFBertForSequenceClassification"),btr=o(" (BERT model)"),vtr=l(),Av=a("li"),Bge=a("strong"),Ttr=o("camembert"),Ftr=o(" \u2014 "),kq=a("a"),Ctr=o("TFCamembertForSequenceClassification"),Mtr=o(" (CamemBERT model)"),Etr=l(),Lv=a("li"),kge=a("strong"),ytr=o("convbert"),wtr=o(" \u2014 "),xq=a("a"),Atr=o("TFConvBertForSequenceClassification"),Ltr=o(" (ConvBERT model)"),Btr=l(),Bv=a("li"),xge=a("strong"),ktr=o("ctrl"),xtr=o(" \u2014 "),Rq=a("a"),Rtr=o("TFCTRLForSequenceClassification"),Str=o(" (CTRL model)"),Ptr=l(),kv=a("li"),Rge=a("strong"),$tr=o("deberta"),Itr=o(" \u2014 "),Sq=a("a"),jtr=o("TFDebertaForSequenceClassification"),Ntr=o(" (DeBERTa model)"),Dtr=l(),xv=a("li"),Sge=a("strong"),qtr=o("deberta-v2"),Gtr=o(" \u2014 "),Pq=a("a"),Otr=o("TFDebertaV2ForSequenceClassification"),Xtr=o(" (DeBERTa-v2 model)"),ztr=l(),Rv=a("li"),Pge=a("strong"),Vtr=o("distilbert"),Wtr=o(" \u2014 "),$q=a("a"),Qtr=o("TFDistilBertForSequenceClassification"),Htr=o(" (DistilBERT model)"),Utr=l(),Sv=a("li"),$ge=a("strong"),Jtr=o("electra"),Ytr=o(" \u2014 "),Iq=a("a"),Ktr=o("TFElectraForSequenceClassification"),Ztr=o(" (ELECTRA model)"),ear=l(),Pv=a("li"),Ige=a("strong"),oar=o("flaubert"),rar=o(" \u2014 "),jq=a("a"),tar=o("TFFlaubertForSequenceClassification"),aar=o(" (FlauBERT model)"),nar=l(),$v=a("li"),jge=a("strong"),sar=o("funnel"),lar=o(" \u2014 "),Nq=a("a"),iar=o("TFFunnelForSequenceClassification"),dar=o(" (Funnel Transformer model)"),car=l(),Iv=a("li"),Nge=a("strong"),far=o("gpt2"),mar=o(" \u2014 "),Dq=a("a"),gar=o("TFGPT2ForSequenceClassification"),har=o(" (OpenAI GPT-2 model)"),par=l(),jv=a("li"),Dge=a("strong"),_ar=o("layoutlm"),uar=o(" \u2014 "),qq=a("a"),bar=o("TFLayoutLMForSequenceClassification"),Tar=o(" (LayoutLM model)"),Far=l(),Nv=a("li"),qge=a("strong"),Car=o("longformer"),Mar=o(" \u2014 "),Gq=a("a"),Ear=o("TFLongformerForSequenceClassification"),yar=o(" (Longformer model)"),war=l(),Dv=a("li"),Gge=a("strong"),Aar=o("mobilebert"),Lar=o(" \u2014 "),Oq=a("a"),Bar=o("TFMobileBertForSequenceClassification"),kar=o(" (MobileBERT model)"),xar=l(),qv=a("li"),Oge=a("strong"),Rar=o("mpnet"),Sar=o(" \u2014 "),Xq=a("a"),Par=o("TFMPNetForSequenceClassification"),$ar=o(" (MPNet model)"),Iar=l(),Gv=a("li"),Xge=a("strong"),jar=o("openai-gpt"),Nar=o(" \u2014 "),zq=a("a"),Dar=o("TFOpenAIGPTForSequenceClassification"),qar=o(" (OpenAI GPT model)"),Gar=l(),Ov=a("li"),zge=a("strong"),Oar=o("rembert"),Xar=o(" \u2014 "),Vq=a("a"),zar=o("TFRemBertForSequenceClassification"),Var=o(" (RemBERT model)"),War=l(),Xv=a("li"),Vge=a("strong"),Qar=o("roberta"),Har=o(" \u2014 "),Wq=a("a"),Uar=o("TFRobertaForSequenceClassification"),Jar=o(" (RoBERTa model)"),Yar=l(),zv=a("li"),Wge=a("strong"),Kar=o("roformer"),Zar=o(" \u2014 "),Qq=a("a"),enr=o("TFRoFormerForSequenceClassification"),onr=o(" (RoFormer model)"),rnr=l(),Vv=a("li"),Qge=a("strong"),tnr=o("tapas"),anr=o(" \u2014 "),Hq=a("a"),nnr=o("TFTapasForSequenceClassification"),snr=o(" (TAPAS model)"),lnr=l(),Wv=a("li"),Hge=a("strong"),inr=o("transfo-xl"),dnr=o(" \u2014 "),Uq=a("a"),cnr=o("TFTransfoXLForSequenceClassification"),fnr=o(" (Transformer-XL model)"),mnr=l(),Qv=a("li"),Uge=a("strong"),gnr=o("xlm"),hnr=o(" \u2014 "),Jq=a("a"),pnr=o("TFXLMForSequenceClassification"),_nr=o(" (XLM model)"),unr=l(),Hv=a("li"),Jge=a("strong"),bnr=o("xlm-roberta"),vnr=o(" \u2014 "),Yq=a("a"),Tnr=o("TFXLMRobertaForSequenceClassification"),Fnr=o(" (XLM-RoBERTa model)"),Cnr=l(),Uv=a("li"),Yge=a("strong"),Mnr=o("xlnet"),Enr=o(" \u2014 "),Kq=a("a"),ynr=o("TFXLNetForSequenceClassification"),wnr=o(" (XLNet model)"),Anr=l(),Kge=a("p"),Lnr=o("Examples:"),Bnr=l(),f(MA.$$.fragment),T9e=l(),Cc=a("h2"),Jv=a("a"),Zge=a("span"),f(EA.$$.fragment),knr=l(),ehe=a("span"),xnr=o("TFAutoModelForMultipleChoice"),F9e=l(),Tr=a("div"),f(yA.$$.fragment),Rnr=l(),Mc=a("p"),Snr=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a multiple choice head) when created
with the `),ohe=a("code"),Pnr=o("from_pretrained()"),$nr=o("class method or the "),rhe=a("code"),Inr=o("from_config()"),jnr=o(`class
method.`),Nnr=l(),wA=a("p"),Dnr=o("This class cannot be instantiated directly using "),the=a("code"),qnr=o("__init__()"),Gnr=o(" (throws an error)."),Onr=l(),gt=a("div"),f(AA.$$.fragment),Xnr=l(),ahe=a("p"),znr=o("Instantiates one of the model classes of the library (with a multiple choice head) from a configuration."),Vnr=l(),Ec=a("p"),Wnr=o(`Note:
Loading a model from its configuration file does `),nhe=a("strong"),Qnr=o("not"),Hnr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),she=a("code"),Unr=o("from_pretrained()"),Jnr=o("to load the model weights."),Ynr=l(),lhe=a("p"),Knr=o("Examples:"),Znr=l(),f(LA.$$.fragment),esr=l(),To=a("div"),f(BA.$$.fragment),osr=l(),ihe=a("p"),rsr=o("Instantiate one of the model classes of the library (with a multiple choice head) from a pretrained model."),tsr=l(),pn=a("p"),asr=o("The model class to instantiate is selected based on the "),dhe=a("code"),nsr=o("model_type"),ssr=o(` property of the config object (either
passed as an argument or loaded from `),che=a("code"),lsr=o("pretrained_model_name_or_path"),isr=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),fhe=a("code"),dsr=o("pretrained_model_name_or_path"),csr=o(":"),fsr=l(),te=a("ul"),Yv=a("li"),mhe=a("strong"),msr=o("albert"),gsr=o(" \u2014 "),Zq=a("a"),hsr=o("TFAlbertForMultipleChoice"),psr=o(" (ALBERT model)"),_sr=l(),Kv=a("li"),ghe=a("strong"),usr=o("bert"),bsr=o(" \u2014 "),eG=a("a"),vsr=o("TFBertForMultipleChoice"),Tsr=o(" (BERT model)"),Fsr=l(),Zv=a("li"),hhe=a("strong"),Csr=o("camembert"),Msr=o(" \u2014 "),oG=a("a"),Esr=o("TFCamembertForMultipleChoice"),ysr=o(" (CamemBERT model)"),wsr=l(),eT=a("li"),phe=a("strong"),Asr=o("convbert"),Lsr=o(" \u2014 "),rG=a("a"),Bsr=o("TFConvBertForMultipleChoice"),ksr=o(" (ConvBERT model)"),xsr=l(),oT=a("li"),_he=a("strong"),Rsr=o("distilbert"),Ssr=o(" \u2014 "),tG=a("a"),Psr=o("TFDistilBertForMultipleChoice"),$sr=o(" (DistilBERT model)"),Isr=l(),rT=a("li"),uhe=a("strong"),jsr=o("electra"),Nsr=o(" \u2014 "),aG=a("a"),Dsr=o("TFElectraForMultipleChoice"),qsr=o(" (ELECTRA model)"),Gsr=l(),tT=a("li"),bhe=a("strong"),Osr=o("flaubert"),Xsr=o(" \u2014 "),nG=a("a"),zsr=o("TFFlaubertForMultipleChoice"),Vsr=o(" (FlauBERT model)"),Wsr=l(),aT=a("li"),vhe=a("strong"),Qsr=o("funnel"),Hsr=o(" \u2014 "),sG=a("a"),Usr=o("TFFunnelForMultipleChoice"),Jsr=o(" (Funnel Transformer model)"),Ysr=l(),nT=a("li"),The=a("strong"),Ksr=o("longformer"),Zsr=o(" \u2014 "),lG=a("a"),elr=o("TFLongformerForMultipleChoice"),olr=o(" (Longformer model)"),rlr=l(),sT=a("li"),Fhe=a("strong"),tlr=o("mobilebert"),alr=o(" \u2014 "),iG=a("a"),nlr=o("TFMobileBertForMultipleChoice"),slr=o(" (MobileBERT model)"),llr=l(),lT=a("li"),Che=a("strong"),ilr=o("mpnet"),dlr=o(" \u2014 "),dG=a("a"),clr=o("TFMPNetForMultipleChoice"),flr=o(" (MPNet model)"),mlr=l(),iT=a("li"),Mhe=a("strong"),glr=o("rembert"),hlr=o(" \u2014 "),cG=a("a"),plr=o("TFRemBertForMultipleChoice"),_lr=o(" (RemBERT model)"),ulr=l(),dT=a("li"),Ehe=a("strong"),blr=o("roberta"),vlr=o(" \u2014 "),fG=a("a"),Tlr=o("TFRobertaForMultipleChoice"),Flr=o(" (RoBERTa model)"),Clr=l(),cT=a("li"),yhe=a("strong"),Mlr=o("roformer"),Elr=o(" \u2014 "),mG=a("a"),ylr=o("TFRoFormerForMultipleChoice"),wlr=o(" (RoFormer model)"),Alr=l(),fT=a("li"),whe=a("strong"),Llr=o("xlm"),Blr=o(" \u2014 "),gG=a("a"),klr=o("TFXLMForMultipleChoice"),xlr=o(" (XLM model)"),Rlr=l(),mT=a("li"),Ahe=a("strong"),Slr=o("xlm-roberta"),Plr=o(" \u2014 "),hG=a("a"),$lr=o("TFXLMRobertaForMultipleChoice"),Ilr=o(" (XLM-RoBERTa model)"),jlr=l(),gT=a("li"),Lhe=a("strong"),Nlr=o("xlnet"),Dlr=o(" \u2014 "),pG=a("a"),qlr=o("TFXLNetForMultipleChoice"),Glr=o(" (XLNet model)"),Olr=l(),Bhe=a("p"),Xlr=o("Examples:"),zlr=l(),f(kA.$$.fragment),C9e=l(),yc=a("h2"),hT=a("a"),khe=a("span"),f(xA.$$.fragment),Vlr=l(),xhe=a("span"),Wlr=o("TFAutoModelForTableQuestionAnswering"),M9e=l(),Fr=a("div"),f(RA.$$.fragment),Qlr=l(),wc=a("p"),Hlr=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a table question answering head) when created
with the `),Rhe=a("code"),Ulr=o("from_pretrained()"),Jlr=o("class method or the "),She=a("code"),Ylr=o("from_config()"),Klr=o(`class
method.`),Zlr=l(),SA=a("p"),eir=o("This class cannot be instantiated directly using "),Phe=a("code"),oir=o("__init__()"),rir=o(" (throws an error)."),tir=l(),ht=a("div"),f(PA.$$.fragment),air=l(),$he=a("p"),nir=o("Instantiates one of the model classes of the library (with a table question answering head) from a configuration."),sir=l(),Ac=a("p"),lir=o(`Note:
Loading a model from its configuration file does `),Ihe=a("strong"),iir=o("not"),dir=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),jhe=a("code"),cir=o("from_pretrained()"),fir=o("to load the model weights."),mir=l(),Nhe=a("p"),gir=o("Examples:"),hir=l(),f($A.$$.fragment),pir=l(),Fo=a("div"),f(IA.$$.fragment),_ir=l(),Dhe=a("p"),uir=o("Instantiate one of the model classes of the library (with a table question answering head) from a pretrained model."),bir=l(),_n=a("p"),vir=o("The model class to instantiate is selected based on the "),qhe=a("code"),Tir=o("model_type"),Fir=o(` property of the config object (either
passed as an argument or loaded from `),Ghe=a("code"),Cir=o("pretrained_model_name_or_path"),Mir=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Ohe=a("code"),Eir=o("pretrained_model_name_or_path"),yir=o(":"),wir=l(),Xhe=a("ul"),pT=a("li"),zhe=a("strong"),Air=o("tapas"),Lir=o(" \u2014 "),_G=a("a"),Bir=o("TFTapasForQuestionAnswering"),kir=o(" (TAPAS model)"),xir=l(),Vhe=a("p"),Rir=o("Examples:"),Sir=l(),f(jA.$$.fragment),E9e=l(),Lc=a("h2"),_T=a("a"),Whe=a("span"),f(NA.$$.fragment),Pir=l(),Qhe=a("span"),$ir=o("TFAutoModelForTokenClassification"),y9e=l(),Cr=a("div"),f(DA.$$.fragment),Iir=l(),Bc=a("p"),jir=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a token classification head) when created
with the `),Hhe=a("code"),Nir=o("from_pretrained()"),Dir=o("class method or the "),Uhe=a("code"),qir=o("from_config()"),Gir=o(`class
method.`),Oir=l(),qA=a("p"),Xir=o("This class cannot be instantiated directly using "),Jhe=a("code"),zir=o("__init__()"),Vir=o(" (throws an error)."),Wir=l(),pt=a("div"),f(GA.$$.fragment),Qir=l(),Yhe=a("p"),Hir=o("Instantiates one of the model classes of the library (with a token classification head) from a configuration."),Uir=l(),kc=a("p"),Jir=o(`Note:
Loading a model from its configuration file does `),Khe=a("strong"),Yir=o("not"),Kir=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Zhe=a("code"),Zir=o("from_pretrained()"),edr=o("to load the model weights."),odr=l(),epe=a("p"),rdr=o("Examples:"),tdr=l(),f(OA.$$.fragment),adr=l(),Co=a("div"),f(XA.$$.fragment),ndr=l(),ope=a("p"),sdr=o("Instantiate one of the model classes of the library (with a token classification head) from a pretrained model."),ldr=l(),un=a("p"),idr=o("The model class to instantiate is selected based on the "),rpe=a("code"),ddr=o("model_type"),cdr=o(` property of the config object (either
passed as an argument or loaded from `),tpe=a("code"),fdr=o("pretrained_model_name_or_path"),mdr=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),ape=a("code"),gdr=o("pretrained_model_name_or_path"),hdr=o(":"),pdr=l(),K=a("ul"),uT=a("li"),npe=a("strong"),_dr=o("albert"),udr=o(" \u2014 "),uG=a("a"),bdr=o("TFAlbertForTokenClassification"),vdr=o(" (ALBERT model)"),Tdr=l(),bT=a("li"),spe=a("strong"),Fdr=o("bert"),Cdr=o(" \u2014 "),bG=a("a"),Mdr=o("TFBertForTokenClassification"),Edr=o(" (BERT model)"),ydr=l(),vT=a("li"),lpe=a("strong"),wdr=o("camembert"),Adr=o(" \u2014 "),vG=a("a"),Ldr=o("TFCamembertForTokenClassification"),Bdr=o(" (CamemBERT model)"),kdr=l(),TT=a("li"),ipe=a("strong"),xdr=o("convbert"),Rdr=o(" \u2014 "),TG=a("a"),Sdr=o("TFConvBertForTokenClassification"),Pdr=o(" (ConvBERT model)"),$dr=l(),FT=a("li"),dpe=a("strong"),Idr=o("deberta"),jdr=o(" \u2014 "),FG=a("a"),Ndr=o("TFDebertaForTokenClassification"),Ddr=o(" (DeBERTa model)"),qdr=l(),CT=a("li"),cpe=a("strong"),Gdr=o("deberta-v2"),Odr=o(" \u2014 "),CG=a("a"),Xdr=o("TFDebertaV2ForTokenClassification"),zdr=o(" (DeBERTa-v2 model)"),Vdr=l(),MT=a("li"),fpe=a("strong"),Wdr=o("distilbert"),Qdr=o(" \u2014 "),MG=a("a"),Hdr=o("TFDistilBertForTokenClassification"),Udr=o(" (DistilBERT model)"),Jdr=l(),ET=a("li"),mpe=a("strong"),Ydr=o("electra"),Kdr=o(" \u2014 "),EG=a("a"),Zdr=o("TFElectraForTokenClassification"),ecr=o(" (ELECTRA model)"),ocr=l(),yT=a("li"),gpe=a("strong"),rcr=o("flaubert"),tcr=o(" \u2014 "),yG=a("a"),acr=o("TFFlaubertForTokenClassification"),ncr=o(" (FlauBERT model)"),scr=l(),wT=a("li"),hpe=a("strong"),lcr=o("funnel"),icr=o(" \u2014 "),wG=a("a"),dcr=o("TFFunnelForTokenClassification"),ccr=o(" (Funnel Transformer model)"),fcr=l(),AT=a("li"),ppe=a("strong"),mcr=o("layoutlm"),gcr=o(" \u2014 "),AG=a("a"),hcr=o("TFLayoutLMForTokenClassification"),pcr=o(" (LayoutLM model)"),_cr=l(),LT=a("li"),_pe=a("strong"),ucr=o("longformer"),bcr=o(" \u2014 "),LG=a("a"),vcr=o("TFLongformerForTokenClassification"),Tcr=o(" (Longformer model)"),Fcr=l(),BT=a("li"),upe=a("strong"),Ccr=o("mobilebert"),Mcr=o(" \u2014 "),BG=a("a"),Ecr=o("TFMobileBertForTokenClassification"),ycr=o(" (MobileBERT model)"),wcr=l(),kT=a("li"),bpe=a("strong"),Acr=o("mpnet"),Lcr=o(" \u2014 "),kG=a("a"),Bcr=o("TFMPNetForTokenClassification"),kcr=o(" (MPNet model)"),xcr=l(),xT=a("li"),vpe=a("strong"),Rcr=o("rembert"),Scr=o(" \u2014 "),xG=a("a"),Pcr=o("TFRemBertForTokenClassification"),$cr=o(" (RemBERT model)"),Icr=l(),RT=a("li"),Tpe=a("strong"),jcr=o("roberta"),Ncr=o(" \u2014 "),RG=a("a"),Dcr=o("TFRobertaForTokenClassification"),qcr=o(" (RoBERTa model)"),Gcr=l(),ST=a("li"),Fpe=a("strong"),Ocr=o("roformer"),Xcr=o(" \u2014 "),SG=a("a"),zcr=o("TFRoFormerForTokenClassification"),Vcr=o(" (RoFormer model)"),Wcr=l(),PT=a("li"),Cpe=a("strong"),Qcr=o("xlm"),Hcr=o(" \u2014 "),PG=a("a"),Ucr=o("TFXLMForTokenClassification"),Jcr=o(" (XLM model)"),Ycr=l(),$T=a("li"),Mpe=a("strong"),Kcr=o("xlm-roberta"),Zcr=o(" \u2014 "),$G=a("a"),efr=o("TFXLMRobertaForTokenClassification"),ofr=o(" (XLM-RoBERTa model)"),rfr=l(),IT=a("li"),Epe=a("strong"),tfr=o("xlnet"),afr=o(" \u2014 "),IG=a("a"),nfr=o("TFXLNetForTokenClassification"),sfr=o(" (XLNet model)"),lfr=l(),ype=a("p"),ifr=o("Examples:"),dfr=l(),f(zA.$$.fragment),w9e=l(),xc=a("h2"),jT=a("a"),wpe=a("span"),f(VA.$$.fragment),cfr=l(),Ape=a("span"),ffr=o("TFAutoModelForQuestionAnswering"),A9e=l(),Mr=a("div"),f(WA.$$.fragment),mfr=l(),Rc=a("p"),gfr=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a question answering head) when created
with the `),Lpe=a("code"),hfr=o("from_pretrained()"),pfr=o("class method or the "),Bpe=a("code"),_fr=o("from_config()"),ufr=o(`class
method.`),bfr=l(),QA=a("p"),vfr=o("This class cannot be instantiated directly using "),kpe=a("code"),Tfr=o("__init__()"),Ffr=o(" (throws an error)."),Cfr=l(),_t=a("div"),f(HA.$$.fragment),Mfr=l(),xpe=a("p"),Efr=o("Instantiates one of the model classes of the library (with a question answering head) from a configuration."),yfr=l(),Sc=a("p"),wfr=o(`Note:
Loading a model from its configuration file does `),Rpe=a("strong"),Afr=o("not"),Lfr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Spe=a("code"),Bfr=o("from_pretrained()"),kfr=o("to load the model weights."),xfr=l(),Ppe=a("p"),Rfr=o("Examples:"),Sfr=l(),f(UA.$$.fragment),Pfr=l(),Mo=a("div"),f(JA.$$.fragment),$fr=l(),$pe=a("p"),Ifr=o("Instantiate one of the model classes of the library (with a question answering head) from a pretrained model."),jfr=l(),bn=a("p"),Nfr=o("The model class to instantiate is selected based on the "),Ipe=a("code"),Dfr=o("model_type"),qfr=o(` property of the config object (either
passed as an argument or loaded from `),jpe=a("code"),Gfr=o("pretrained_model_name_or_path"),Ofr=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Npe=a("code"),Xfr=o("pretrained_model_name_or_path"),zfr=o(":"),Vfr=l(),Z=a("ul"),NT=a("li"),Dpe=a("strong"),Wfr=o("albert"),Qfr=o(" \u2014 "),jG=a("a"),Hfr=o("TFAlbertForQuestionAnswering"),Ufr=o(" (ALBERT model)"),Jfr=l(),DT=a("li"),qpe=a("strong"),Yfr=o("bert"),Kfr=o(" \u2014 "),NG=a("a"),Zfr=o("TFBertForQuestionAnswering"),emr=o(" (BERT model)"),omr=l(),qT=a("li"),Gpe=a("strong"),rmr=o("camembert"),tmr=o(" \u2014 "),DG=a("a"),amr=o("TFCamembertForQuestionAnswering"),nmr=o(" (CamemBERT model)"),smr=l(),GT=a("li"),Ope=a("strong"),lmr=o("convbert"),imr=o(" \u2014 "),qG=a("a"),dmr=o("TFConvBertForQuestionAnswering"),cmr=o(" (ConvBERT model)"),fmr=l(),OT=a("li"),Xpe=a("strong"),mmr=o("deberta"),gmr=o(" \u2014 "),GG=a("a"),hmr=o("TFDebertaForQuestionAnswering"),pmr=o(" (DeBERTa model)"),_mr=l(),XT=a("li"),zpe=a("strong"),umr=o("deberta-v2"),bmr=o(" \u2014 "),OG=a("a"),vmr=o("TFDebertaV2ForQuestionAnswering"),Tmr=o(" (DeBERTa-v2 model)"),Fmr=l(),zT=a("li"),Vpe=a("strong"),Cmr=o("distilbert"),Mmr=o(" \u2014 "),XG=a("a"),Emr=o("TFDistilBertForQuestionAnswering"),ymr=o(" (DistilBERT model)"),wmr=l(),VT=a("li"),Wpe=a("strong"),Amr=o("electra"),Lmr=o(" \u2014 "),zG=a("a"),Bmr=o("TFElectraForQuestionAnswering"),kmr=o(" (ELECTRA model)"),xmr=l(),WT=a("li"),Qpe=a("strong"),Rmr=o("flaubert"),Smr=o(" \u2014 "),VG=a("a"),Pmr=o("TFFlaubertForQuestionAnsweringSimple"),$mr=o(" (FlauBERT model)"),Imr=l(),QT=a("li"),Hpe=a("strong"),jmr=o("funnel"),Nmr=o(" \u2014 "),WG=a("a"),Dmr=o("TFFunnelForQuestionAnswering"),qmr=o(" (Funnel Transformer model)"),Gmr=l(),HT=a("li"),Upe=a("strong"),Omr=o("longformer"),Xmr=o(" \u2014 "),QG=a("a"),zmr=o("TFLongformerForQuestionAnswering"),Vmr=o(" (Longformer model)"),Wmr=l(),UT=a("li"),Jpe=a("strong"),Qmr=o("mobilebert"),Hmr=o(" \u2014 "),HG=a("a"),Umr=o("TFMobileBertForQuestionAnswering"),Jmr=o(" (MobileBERT model)"),Ymr=l(),JT=a("li"),Ype=a("strong"),Kmr=o("mpnet"),Zmr=o(" \u2014 "),UG=a("a"),egr=o("TFMPNetForQuestionAnswering"),ogr=o(" (MPNet model)"),rgr=l(),YT=a("li"),Kpe=a("strong"),tgr=o("rembert"),agr=o(" \u2014 "),JG=a("a"),ngr=o("TFRemBertForQuestionAnswering"),sgr=o(" (RemBERT model)"),lgr=l(),KT=a("li"),Zpe=a("strong"),igr=o("roberta"),dgr=o(" \u2014 "),YG=a("a"),cgr=o("TFRobertaForQuestionAnswering"),fgr=o(" (RoBERTa model)"),mgr=l(),ZT=a("li"),e_e=a("strong"),ggr=o("roformer"),hgr=o(" \u2014 "),KG=a("a"),pgr=o("TFRoFormerForQuestionAnswering"),_gr=o(" (RoFormer model)"),ugr=l(),eF=a("li"),o_e=a("strong"),bgr=o("xlm"),vgr=o(" \u2014 "),ZG=a("a"),Tgr=o("TFXLMForQuestionAnsweringSimple"),Fgr=o(" (XLM model)"),Cgr=l(),oF=a("li"),r_e=a("strong"),Mgr=o("xlm-roberta"),Egr=o(" \u2014 "),eO=a("a"),ygr=o("TFXLMRobertaForQuestionAnswering"),wgr=o(" (XLM-RoBERTa model)"),Agr=l(),rF=a("li"),t_e=a("strong"),Lgr=o("xlnet"),Bgr=o(" \u2014 "),oO=a("a"),kgr=o("TFXLNetForQuestionAnsweringSimple"),xgr=o(" (XLNet model)"),Rgr=l(),a_e=a("p"),Sgr=o("Examples:"),Pgr=l(),f(YA.$$.fragment),L9e=l(),Pc=a("h2"),tF=a("a"),n_e=a("span"),f(KA.$$.fragment),$gr=l(),s_e=a("span"),Igr=o("TFAutoModelForVision2Seq"),B9e=l(),Er=a("div"),f(ZA.$$.fragment),jgr=l(),$c=a("p"),Ngr=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a vision-to-text modeling head) when created
with the `),l_e=a("code"),Dgr=o("from_pretrained()"),qgr=o("class method or the "),i_e=a("code"),Ggr=o("from_config()"),Ogr=o(`class
method.`),Xgr=l(),e6=a("p"),zgr=o("This class cannot be instantiated directly using "),d_e=a("code"),Vgr=o("__init__()"),Wgr=o(" (throws an error)."),Qgr=l(),ut=a("div"),f(o6.$$.fragment),Hgr=l(),c_e=a("p"),Ugr=o("Instantiates one of the model classes of the library (with a vision-to-text modeling head) from a configuration."),Jgr=l(),Ic=a("p"),Ygr=o(`Note:
Loading a model from its configuration file does `),f_e=a("strong"),Kgr=o("not"),Zgr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),m_e=a("code"),ehr=o("from_pretrained()"),ohr=o("to load the model weights."),rhr=l(),g_e=a("p"),thr=o("Examples:"),ahr=l(),f(r6.$$.fragment),nhr=l(),Eo=a("div"),f(t6.$$.fragment),shr=l(),h_e=a("p"),lhr=o("Instantiate one of the model classes of the library (with a vision-to-text modeling head) from a pretrained model."),ihr=l(),vn=a("p"),dhr=o("The model class to instantiate is selected based on the "),p_e=a("code"),chr=o("model_type"),fhr=o(` property of the config object (either
passed as an argument or loaded from `),__e=a("code"),mhr=o("pretrained_model_name_or_path"),ghr=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),u_e=a("code"),hhr=o("pretrained_model_name_or_path"),phr=o(":"),_hr=l(),b_e=a("ul"),aF=a("li"),v_e=a("strong"),uhr=o("vision-encoder-decoder"),bhr=o(" \u2014 "),rO=a("a"),vhr=o("TFVisionEncoderDecoderModel"),Thr=o(" (Vision Encoder decoder model)"),Fhr=l(),T_e=a("p"),Chr=o("Examples:"),Mhr=l(),f(a6.$$.fragment),k9e=l(),jc=a("h2"),nF=a("a"),F_e=a("span"),f(n6.$$.fragment),Ehr=l(),C_e=a("span"),yhr=o("TFAutoModelForSpeechSeq2Seq"),x9e=l(),yr=a("div"),f(s6.$$.fragment),whr=l(),Nc=a("p"),Ahr=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) when created
with the `),M_e=a("code"),Lhr=o("from_pretrained()"),Bhr=o("class method or the "),E_e=a("code"),khr=o("from_config()"),xhr=o(`class
method.`),Rhr=l(),l6=a("p"),Shr=o("This class cannot be instantiated directly using "),y_e=a("code"),Phr=o("__init__()"),$hr=o(" (throws an error)."),Ihr=l(),bt=a("div"),f(i6.$$.fragment),jhr=l(),w_e=a("p"),Nhr=o("Instantiates one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) from a configuration."),Dhr=l(),Dc=a("p"),qhr=o(`Note:
Loading a model from its configuration file does `),A_e=a("strong"),Ghr=o("not"),Ohr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),L_e=a("code"),Xhr=o("from_pretrained()"),zhr=o("to load the model weights."),Vhr=l(),B_e=a("p"),Whr=o("Examples:"),Qhr=l(),f(d6.$$.fragment),Hhr=l(),yo=a("div"),f(c6.$$.fragment),Uhr=l(),k_e=a("p"),Jhr=o("Instantiate one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) from a pretrained model."),Yhr=l(),Tn=a("p"),Khr=o("The model class to instantiate is selected based on the "),x_e=a("code"),Zhr=o("model_type"),epr=o(` property of the config object (either
passed as an argument or loaded from `),R_e=a("code"),opr=o("pretrained_model_name_or_path"),rpr=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),S_e=a("code"),tpr=o("pretrained_model_name_or_path"),apr=o(":"),npr=l(),P_e=a("ul"),sF=a("li"),$_e=a("strong"),spr=o("speech_to_text"),lpr=o(" \u2014 "),tO=a("a"),ipr=o("TFSpeech2TextForConditionalGeneration"),dpr=o(" (Speech2Text model)"),cpr=l(),I_e=a("p"),fpr=o("Examples:"),mpr=l(),f(f6.$$.fragment),R9e=l(),qc=a("h2"),lF=a("a"),j_e=a("span"),f(m6.$$.fragment),gpr=l(),N_e=a("span"),hpr=o("FlaxAutoModel"),S9e=l(),wr=a("div"),f(g6.$$.fragment),ppr=l(),Gc=a("p"),_pr=o(`This is a generic model class that will be instantiated as one of the base model classes of the library when created
with the `),D_e=a("code"),upr=o("from_pretrained()"),bpr=o("class method or the "),q_e=a("code"),vpr=o("from_config()"),Tpr=o(`class
method.`),Fpr=l(),h6=a("p"),Cpr=o("This class cannot be instantiated directly using "),G_e=a("code"),Mpr=o("__init__()"),Epr=o(" (throws an error)."),ypr=l(),vt=a("div"),f(p6.$$.fragment),wpr=l(),O_e=a("p"),Apr=o("Instantiates one of the base model classes of the library from a configuration."),Lpr=l(),Oc=a("p"),Bpr=o(`Note:
Loading a model from its configuration file does `),X_e=a("strong"),kpr=o("not"),xpr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),z_e=a("code"),Rpr=o("from_pretrained()"),Spr=o("to load the model weights."),Ppr=l(),V_e=a("p"),$pr=o("Examples:"),Ipr=l(),f(_6.$$.fragment),jpr=l(),wo=a("div"),f(u6.$$.fragment),Npr=l(),W_e=a("p"),Dpr=o("Instantiate one of the base model classes of the library from a pretrained model."),qpr=l(),Fn=a("p"),Gpr=o("The model class to instantiate is selected based on the "),Q_e=a("code"),Opr=o("model_type"),Xpr=o(` property of the config object (either
passed as an argument or loaded from `),H_e=a("code"),zpr=o("pretrained_model_name_or_path"),Vpr=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),U_e=a("code"),Wpr=o("pretrained_model_name_or_path"),Qpr=o(":"),Hpr=l(),V=a("ul"),iF=a("li"),J_e=a("strong"),Upr=o("albert"),Jpr=o(" \u2014 "),aO=a("a"),Ypr=o("FlaxAlbertModel"),Kpr=o(" (ALBERT model)"),Zpr=l(),dF=a("li"),Y_e=a("strong"),e_r=o("bart"),o_r=o(" \u2014 "),nO=a("a"),r_r=o("FlaxBartModel"),t_r=o(" (BART model)"),a_r=l(),cF=a("li"),K_e=a("strong"),n_r=o("beit"),s_r=o(" \u2014 "),sO=a("a"),l_r=o("FlaxBeitModel"),i_r=o(" (BEiT model)"),d_r=l(),fF=a("li"),Z_e=a("strong"),c_r=o("bert"),f_r=o(" \u2014 "),lO=a("a"),m_r=o("FlaxBertModel"),g_r=o(" (BERT model)"),h_r=l(),mF=a("li"),eue=a("strong"),p_r=o("big_bird"),__r=o(" \u2014 "),iO=a("a"),u_r=o("FlaxBigBirdModel"),b_r=o(" (BigBird model)"),v_r=l(),gF=a("li"),oue=a("strong"),T_r=o("blenderbot"),F_r=o(" \u2014 "),dO=a("a"),C_r=o("FlaxBlenderbotModel"),M_r=o(" (Blenderbot model)"),E_r=l(),hF=a("li"),rue=a("strong"),y_r=o("blenderbot-small"),w_r=o(" \u2014 "),cO=a("a"),A_r=o("FlaxBlenderbotSmallModel"),L_r=o(" (BlenderbotSmall model)"),B_r=l(),pF=a("li"),tue=a("strong"),k_r=o("clip"),x_r=o(" \u2014 "),fO=a("a"),R_r=o("FlaxCLIPModel"),S_r=o(" (CLIP model)"),P_r=l(),_F=a("li"),aue=a("strong"),$_r=o("distilbert"),I_r=o(" \u2014 "),mO=a("a"),j_r=o("FlaxDistilBertModel"),N_r=o(" (DistilBERT model)"),D_r=l(),uF=a("li"),nue=a("strong"),q_r=o("electra"),G_r=o(" \u2014 "),gO=a("a"),O_r=o("FlaxElectraModel"),X_r=o(" (ELECTRA model)"),z_r=l(),bF=a("li"),sue=a("strong"),V_r=o("gpt2"),W_r=o(" \u2014 "),hO=a("a"),Q_r=o("FlaxGPT2Model"),H_r=o(" (OpenAI GPT-2 model)"),U_r=l(),vF=a("li"),lue=a("strong"),J_r=o("gpt_neo"),Y_r=o(" \u2014 "),pO=a("a"),K_r=o("FlaxGPTNeoModel"),Z_r=o(" (GPT Neo model)"),eur=l(),TF=a("li"),iue=a("strong"),our=o("gptj"),rur=o(" \u2014 "),_O=a("a"),tur=o("FlaxGPTJModel"),aur=o(" (GPT-J model)"),nur=l(),FF=a("li"),due=a("strong"),sur=o("marian"),lur=o(" \u2014 "),uO=a("a"),iur=o("FlaxMarianModel"),dur=o(" (Marian model)"),cur=l(),CF=a("li"),cue=a("strong"),fur=o("mbart"),mur=o(" \u2014 "),bO=a("a"),gur=o("FlaxMBartModel"),hur=o(" (mBART model)"),pur=l(),MF=a("li"),fue=a("strong"),_ur=o("mt5"),uur=o(" \u2014 "),vO=a("a"),bur=o("FlaxMT5Model"),vur=o(" (mT5 model)"),Tur=l(),EF=a("li"),mue=a("strong"),Fur=o("pegasus"),Cur=o(" \u2014 "),TO=a("a"),Mur=o("FlaxPegasusModel"),Eur=o(" (Pegasus model)"),yur=l(),yF=a("li"),gue=a("strong"),wur=o("roberta"),Aur=o(" \u2014 "),FO=a("a"),Lur=o("FlaxRobertaModel"),Bur=o(" (RoBERTa model)"),kur=l(),wF=a("li"),hue=a("strong"),xur=o("roformer"),Rur=o(" \u2014 "),CO=a("a"),Sur=o("FlaxRoFormerModel"),Pur=o(" (RoFormer model)"),$ur=l(),AF=a("li"),pue=a("strong"),Iur=o("t5"),jur=o(" \u2014 "),MO=a("a"),Nur=o("FlaxT5Model"),Dur=o(" (T5 model)"),qur=l(),LF=a("li"),_ue=a("strong"),Gur=o("vision-text-dual-encoder"),Our=o(" \u2014 "),EO=a("a"),Xur=o("FlaxVisionTextDualEncoderModel"),zur=o(" (VisionTextDualEncoder model)"),Vur=l(),BF=a("li"),uue=a("strong"),Wur=o("vit"),Qur=o(" \u2014 "),yO=a("a"),Hur=o("FlaxViTModel"),Uur=o(" (ViT model)"),Jur=l(),kF=a("li"),bue=a("strong"),Yur=o("wav2vec2"),Kur=o(" \u2014 "),wO=a("a"),Zur=o("FlaxWav2Vec2Model"),e1r=o(" (Wav2Vec2 model)"),o1r=l(),xF=a("li"),vue=a("strong"),r1r=o("xglm"),t1r=o(" \u2014 "),AO=a("a"),a1r=o("FlaxXGLMModel"),n1r=o(" (XGLM model)"),s1r=l(),Tue=a("p"),l1r=o("Examples:"),i1r=l(),f(b6.$$.fragment),P9e=l(),Xc=a("h2"),RF=a("a"),Fue=a("span"),f(v6.$$.fragment),d1r=l(),Cue=a("span"),c1r=o("FlaxAutoModelForCausalLM"),$9e=l(),Ar=a("div"),f(T6.$$.fragment),f1r=l(),zc=a("p"),m1r=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a causal language modeling head) when created
with the `),Mue=a("code"),g1r=o("from_pretrained()"),h1r=o("class method or the "),Eue=a("code"),p1r=o("from_config()"),_1r=o(`class
method.`),u1r=l(),F6=a("p"),b1r=o("This class cannot be instantiated directly using "),yue=a("code"),v1r=o("__init__()"),T1r=o(" (throws an error)."),F1r=l(),Tt=a("div"),f(C6.$$.fragment),C1r=l(),wue=a("p"),M1r=o("Instantiates one of the model classes of the library (with a causal language modeling head) from a configuration."),E1r=l(),Vc=a("p"),y1r=o(`Note:
Loading a model from its configuration file does `),Aue=a("strong"),w1r=o("not"),A1r=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Lue=a("code"),L1r=o("from_pretrained()"),B1r=o("to load the model weights."),k1r=l(),Bue=a("p"),x1r=o("Examples:"),R1r=l(),f(M6.$$.fragment),S1r=l(),Ao=a("div"),f(E6.$$.fragment),P1r=l(),kue=a("p"),$1r=o("Instantiate one of the model classes of the library (with a causal language modeling head) from a pretrained model."),I1r=l(),Cn=a("p"),j1r=o("The model class to instantiate is selected based on the "),xue=a("code"),N1r=o("model_type"),D1r=o(` property of the config object (either
passed as an argument or loaded from `),Rue=a("code"),q1r=o("pretrained_model_name_or_path"),G1r=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Sue=a("code"),O1r=o("pretrained_model_name_or_path"),X1r=o(":"),z1r=l(),Mn=a("ul"),SF=a("li"),Pue=a("strong"),V1r=o("gpt2"),W1r=o(" \u2014 "),LO=a("a"),Q1r=o("FlaxGPT2LMHeadModel"),H1r=o(" (OpenAI GPT-2 model)"),U1r=l(),PF=a("li"),$ue=a("strong"),J1r=o("gpt_neo"),Y1r=o(" \u2014 "),BO=a("a"),K1r=o("FlaxGPTNeoForCausalLM"),Z1r=o(" (GPT Neo model)"),e7r=l(),$F=a("li"),Iue=a("strong"),o7r=o("gptj"),r7r=o(" \u2014 "),kO=a("a"),t7r=o("FlaxGPTJForCausalLM"),a7r=o(" (GPT-J model)"),n7r=l(),IF=a("li"),jue=a("strong"),s7r=o("xglm"),l7r=o(" \u2014 "),xO=a("a"),i7r=o("FlaxXGLMForCausalLM"),d7r=o(" (XGLM model)"),c7r=l(),Nue=a("p"),f7r=o("Examples:"),m7r=l(),f(y6.$$.fragment),I9e=l(),Wc=a("h2"),jF=a("a"),Due=a("span"),f(w6.$$.fragment),g7r=l(),que=a("span"),h7r=o("FlaxAutoModelForPreTraining"),j9e=l(),Lr=a("div"),f(A6.$$.fragment),p7r=l(),Qc=a("p"),_7r=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a pretraining head) when created
with the `),Gue=a("code"),u7r=o("from_pretrained()"),b7r=o("class method or the "),Oue=a("code"),v7r=o("from_config()"),T7r=o(`class
method.`),F7r=l(),L6=a("p"),C7r=o("This class cannot be instantiated directly using "),Xue=a("code"),M7r=o("__init__()"),E7r=o(" (throws an error)."),y7r=l(),Ft=a("div"),f(B6.$$.fragment),w7r=l(),zue=a("p"),A7r=o("Instantiates one of the model classes of the library (with a pretraining head) from a configuration."),L7r=l(),Hc=a("p"),B7r=o(`Note:
Loading a model from its configuration file does `),Vue=a("strong"),k7r=o("not"),x7r=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Wue=a("code"),R7r=o("from_pretrained()"),S7r=o("to load the model weights."),P7r=l(),Que=a("p"),$7r=o("Examples:"),I7r=l(),f(k6.$$.fragment),j7r=l(),Lo=a("div"),f(x6.$$.fragment),N7r=l(),Hue=a("p"),D7r=o("Instantiate one of the model classes of the library (with a pretraining head) from a pretrained model."),q7r=l(),En=a("p"),G7r=o("The model class to instantiate is selected based on the "),Uue=a("code"),O7r=o("model_type"),X7r=o(` property of the config object (either
passed as an argument or loaded from `),Jue=a("code"),z7r=o("pretrained_model_name_or_path"),V7r=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Yue=a("code"),W7r=o("pretrained_model_name_or_path"),Q7r=o(":"),H7r=l(),fe=a("ul"),NF=a("li"),Kue=a("strong"),U7r=o("albert"),J7r=o(" \u2014 "),RO=a("a"),Y7r=o("FlaxAlbertForPreTraining"),K7r=o(" (ALBERT model)"),Z7r=l(),DF=a("li"),Zue=a("strong"),e4r=o("bart"),o4r=o(" \u2014 "),SO=a("a"),r4r=o("FlaxBartForConditionalGeneration"),t4r=o(" (BART model)"),a4r=l(),qF=a("li"),e1e=a("strong"),n4r=o("bert"),s4r=o(" \u2014 "),PO=a("a"),l4r=o("FlaxBertForPreTraining"),i4r=o(" (BERT model)"),d4r=l(),GF=a("li"),o1e=a("strong"),c4r=o("big_bird"),f4r=o(" \u2014 "),$O=a("a"),m4r=o("FlaxBigBirdForPreTraining"),g4r=o(" (BigBird model)"),h4r=l(),OF=a("li"),r1e=a("strong"),p4r=o("electra"),_4r=o(" \u2014 "),IO=a("a"),u4r=o("FlaxElectraForPreTraining"),b4r=o(" (ELECTRA model)"),v4r=l(),XF=a("li"),t1e=a("strong"),T4r=o("mbart"),F4r=o(" \u2014 "),jO=a("a"),C4r=o("FlaxMBartForConditionalGeneration"),M4r=o(" (mBART model)"),E4r=l(),zF=a("li"),a1e=a("strong"),y4r=o("mt5"),w4r=o(" \u2014 "),NO=a("a"),A4r=o("FlaxMT5ForConditionalGeneration"),L4r=o(" (mT5 model)"),B4r=l(),VF=a("li"),n1e=a("strong"),k4r=o("roberta"),x4r=o(" \u2014 "),DO=a("a"),R4r=o("FlaxRobertaForMaskedLM"),S4r=o(" (RoBERTa model)"),P4r=l(),WF=a("li"),s1e=a("strong"),$4r=o("roformer"),I4r=o(" \u2014 "),qO=a("a"),j4r=o("FlaxRoFormerForMaskedLM"),N4r=o(" (RoFormer model)"),D4r=l(),QF=a("li"),l1e=a("strong"),q4r=o("t5"),G4r=o(" \u2014 "),GO=a("a"),O4r=o("FlaxT5ForConditionalGeneration"),X4r=o(" (T5 model)"),z4r=l(),HF=a("li"),i1e=a("strong"),V4r=o("wav2vec2"),W4r=o(" \u2014 "),OO=a("a"),Q4r=o("FlaxWav2Vec2ForPreTraining"),H4r=o(" (Wav2Vec2 model)"),U4r=l(),d1e=a("p"),J4r=o("Examples:"),Y4r=l(),f(R6.$$.fragment),N9e=l(),Uc=a("h2"),UF=a("a"),c1e=a("span"),f(S6.$$.fragment),K4r=l(),f1e=a("span"),Z4r=o("FlaxAutoModelForMaskedLM"),D9e=l(),Br=a("div"),f(P6.$$.fragment),ebr=l(),Jc=a("p"),obr=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a masked language modeling head) when created
with the `),m1e=a("code"),rbr=o("from_pretrained()"),tbr=o("class method or the "),g1e=a("code"),abr=o("from_config()"),nbr=o(`class
method.`),sbr=l(),$6=a("p"),lbr=o("This class cannot be instantiated directly using "),h1e=a("code"),ibr=o("__init__()"),dbr=o(" (throws an error)."),cbr=l(),Ct=a("div"),f(I6.$$.fragment),fbr=l(),p1e=a("p"),mbr=o("Instantiates one of the model classes of the library (with a masked language modeling head) from a configuration."),gbr=l(),Yc=a("p"),hbr=o(`Note:
Loading a model from its configuration file does `),_1e=a("strong"),pbr=o("not"),_br=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),u1e=a("code"),ubr=o("from_pretrained()"),bbr=o("to load the model weights."),vbr=l(),b1e=a("p"),Tbr=o("Examples:"),Fbr=l(),f(j6.$$.fragment),Cbr=l(),Bo=a("div"),f(N6.$$.fragment),Mbr=l(),v1e=a("p"),Ebr=o("Instantiate one of the model classes of the library (with a masked language modeling head) from a pretrained model."),ybr=l(),yn=a("p"),wbr=o("The model class to instantiate is selected based on the "),T1e=a("code"),Abr=o("model_type"),Lbr=o(` property of the config object (either
passed as an argument or loaded from `),F1e=a("code"),Bbr=o("pretrained_model_name_or_path"),kbr=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),C1e=a("code"),xbr=o("pretrained_model_name_or_path"),Rbr=o(":"),Sbr=l(),ve=a("ul"),JF=a("li"),M1e=a("strong"),Pbr=o("albert"),$br=o(" \u2014 "),XO=a("a"),Ibr=o("FlaxAlbertForMaskedLM"),jbr=o(" (ALBERT model)"),Nbr=l(),YF=a("li"),E1e=a("strong"),Dbr=o("bart"),qbr=o(" \u2014 "),zO=a("a"),Gbr=o("FlaxBartForConditionalGeneration"),Obr=o(" (BART model)"),Xbr=l(),KF=a("li"),y1e=a("strong"),zbr=o("bert"),Vbr=o(" \u2014 "),VO=a("a"),Wbr=o("FlaxBertForMaskedLM"),Qbr=o(" (BERT model)"),Hbr=l(),ZF=a("li"),w1e=a("strong"),Ubr=o("big_bird"),Jbr=o(" \u2014 "),WO=a("a"),Ybr=o("FlaxBigBirdForMaskedLM"),Kbr=o(" (BigBird model)"),Zbr=l(),eC=a("li"),A1e=a("strong"),e5r=o("distilbert"),o5r=o(" \u2014 "),QO=a("a"),r5r=o("FlaxDistilBertForMaskedLM"),t5r=o(" (DistilBERT model)"),a5r=l(),oC=a("li"),L1e=a("strong"),n5r=o("electra"),s5r=o(" \u2014 "),HO=a("a"),l5r=o("FlaxElectraForMaskedLM"),i5r=o(" (ELECTRA model)"),d5r=l(),rC=a("li"),B1e=a("strong"),c5r=o("mbart"),f5r=o(" \u2014 "),UO=a("a"),m5r=o("FlaxMBartForConditionalGeneration"),g5r=o(" (mBART model)"),h5r=l(),tC=a("li"),k1e=a("strong"),p5r=o("roberta"),_5r=o(" \u2014 "),JO=a("a"),u5r=o("FlaxRobertaForMaskedLM"),b5r=o(" (RoBERTa model)"),v5r=l(),aC=a("li"),x1e=a("strong"),T5r=o("roformer"),F5r=o(" \u2014 "),YO=a("a"),C5r=o("FlaxRoFormerForMaskedLM"),M5r=o(" (RoFormer model)"),E5r=l(),R1e=a("p"),y5r=o("Examples:"),w5r=l(),f(D6.$$.fragment),q9e=l(),Kc=a("h2"),nC=a("a"),S1e=a("span"),f(q6.$$.fragment),A5r=l(),P1e=a("span"),L5r=o("FlaxAutoModelForSeq2SeqLM"),G9e=l(),kr=a("div"),f(G6.$$.fragment),B5r=l(),Zc=a("p"),k5r=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence language modeling head) when created
with the `),$1e=a("code"),x5r=o("from_pretrained()"),R5r=o("class method or the "),I1e=a("code"),S5r=o("from_config()"),P5r=o(`class
method.`),$5r=l(),O6=a("p"),I5r=o("This class cannot be instantiated directly using "),j1e=a("code"),j5r=o("__init__()"),N5r=o(" (throws an error)."),D5r=l(),Mt=a("div"),f(X6.$$.fragment),q5r=l(),N1e=a("p"),G5r=o("Instantiates one of the model classes of the library (with a sequence-to-sequence language modeling head) from a configuration."),O5r=l(),ef=a("p"),X5r=o(`Note:
Loading a model from its configuration file does `),D1e=a("strong"),z5r=o("not"),V5r=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),q1e=a("code"),W5r=o("from_pretrained()"),Q5r=o("to load the model weights."),H5r=l(),G1e=a("p"),U5r=o("Examples:"),J5r=l(),f(z6.$$.fragment),Y5r=l(),ko=a("div"),f(V6.$$.fragment),K5r=l(),O1e=a("p"),Z5r=o("Instantiate one of the model classes of the library (with a sequence-to-sequence language modeling head) from a pretrained model."),e2r=l(),wn=a("p"),o2r=o("The model class to instantiate is selected based on the "),X1e=a("code"),r2r=o("model_type"),t2r=o(` property of the config object (either
passed as an argument or loaded from `),z1e=a("code"),a2r=o("pretrained_model_name_or_path"),n2r=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),V1e=a("code"),s2r=o("pretrained_model_name_or_path"),l2r=o(":"),i2r=l(),Te=a("ul"),sC=a("li"),W1e=a("strong"),d2r=o("bart"),c2r=o(" \u2014 "),KO=a("a"),f2r=o("FlaxBartForConditionalGeneration"),m2r=o(" (BART model)"),g2r=l(),lC=a("li"),Q1e=a("strong"),h2r=o("blenderbot"),p2r=o(" \u2014 "),ZO=a("a"),_2r=o("FlaxBlenderbotForConditionalGeneration"),u2r=o(" (Blenderbot model)"),b2r=l(),iC=a("li"),H1e=a("strong"),v2r=o("blenderbot-small"),T2r=o(" \u2014 "),eX=a("a"),F2r=o("FlaxBlenderbotSmallForConditionalGeneration"),C2r=o(" (BlenderbotSmall model)"),M2r=l(),dC=a("li"),U1e=a("strong"),E2r=o("encoder-decoder"),y2r=o(" \u2014 "),oX=a("a"),w2r=o("FlaxEncoderDecoderModel"),A2r=o(" (Encoder decoder model)"),L2r=l(),cC=a("li"),J1e=a("strong"),B2r=o("marian"),k2r=o(" \u2014 "),rX=a("a"),x2r=o("FlaxMarianMTModel"),R2r=o(" (Marian model)"),S2r=l(),fC=a("li"),Y1e=a("strong"),P2r=o("mbart"),$2r=o(" \u2014 "),tX=a("a"),I2r=o("FlaxMBartForConditionalGeneration"),j2r=o(" (mBART model)"),N2r=l(),mC=a("li"),K1e=a("strong"),D2r=o("mt5"),q2r=o(" \u2014 "),aX=a("a"),G2r=o("FlaxMT5ForConditionalGeneration"),O2r=o(" (mT5 model)"),X2r=l(),gC=a("li"),Z1e=a("strong"),z2r=o("pegasus"),V2r=o(" \u2014 "),nX=a("a"),W2r=o("FlaxPegasusForConditionalGeneration"),Q2r=o(" (Pegasus model)"),H2r=l(),hC=a("li"),e7e=a("strong"),U2r=o("t5"),J2r=o(" \u2014 "),sX=a("a"),Y2r=o("FlaxT5ForConditionalGeneration"),K2r=o(" (T5 model)"),Z2r=l(),o7e=a("p"),evr=o("Examples:"),ovr=l(),f(W6.$$.fragment),O9e=l(),of=a("h2"),pC=a("a"),r7e=a("span"),f(Q6.$$.fragment),rvr=l(),t7e=a("span"),tvr=o("FlaxAutoModelForSequenceClassification"),X9e=l(),xr=a("div"),f(H6.$$.fragment),avr=l(),rf=a("p"),nvr=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence classification head) when created
with the `),a7e=a("code"),svr=o("from_pretrained()"),lvr=o("class method or the "),n7e=a("code"),ivr=o("from_config()"),dvr=o(`class
method.`),cvr=l(),U6=a("p"),fvr=o("This class cannot be instantiated directly using "),s7e=a("code"),mvr=o("__init__()"),gvr=o(" (throws an error)."),hvr=l(),Et=a("div"),f(J6.$$.fragment),pvr=l(),l7e=a("p"),_vr=o("Instantiates one of the model classes of the library (with a sequence classification head) from a configuration."),uvr=l(),tf=a("p"),bvr=o(`Note:
Loading a model from its configuration file does `),i7e=a("strong"),vvr=o("not"),Tvr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),d7e=a("code"),Fvr=o("from_pretrained()"),Cvr=o("to load the model weights."),Mvr=l(),c7e=a("p"),Evr=o("Examples:"),yvr=l(),f(Y6.$$.fragment),wvr=l(),xo=a("div"),f(K6.$$.fragment),Avr=l(),f7e=a("p"),Lvr=o("Instantiate one of the model classes of the library (with a sequence classification head) from a pretrained model."),Bvr=l(),An=a("p"),kvr=o("The model class to instantiate is selected based on the "),m7e=a("code"),xvr=o("model_type"),Rvr=o(` property of the config object (either
passed as an argument or loaded from `),g7e=a("code"),Svr=o("pretrained_model_name_or_path"),Pvr=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),h7e=a("code"),$vr=o("pretrained_model_name_or_path"),Ivr=o(":"),jvr=l(),Fe=a("ul"),_C=a("li"),p7e=a("strong"),Nvr=o("albert"),Dvr=o(" \u2014 "),lX=a("a"),qvr=o("FlaxAlbertForSequenceClassification"),Gvr=o(" (ALBERT model)"),Ovr=l(),uC=a("li"),_7e=a("strong"),Xvr=o("bart"),zvr=o(" \u2014 "),iX=a("a"),Vvr=o("FlaxBartForSequenceClassification"),Wvr=o(" (BART model)"),Qvr=l(),bC=a("li"),u7e=a("strong"),Hvr=o("bert"),Uvr=o(" \u2014 "),dX=a("a"),Jvr=o("FlaxBertForSequenceClassification"),Yvr=o(" (BERT model)"),Kvr=l(),vC=a("li"),b7e=a("strong"),Zvr=o("big_bird"),eTr=o(" \u2014 "),cX=a("a"),oTr=o("FlaxBigBirdForSequenceClassification"),rTr=o(" (BigBird model)"),tTr=l(),TC=a("li"),v7e=a("strong"),aTr=o("distilbert"),nTr=o(" \u2014 "),fX=a("a"),sTr=o("FlaxDistilBertForSequenceClassification"),lTr=o(" (DistilBERT model)"),iTr=l(),FC=a("li"),T7e=a("strong"),dTr=o("electra"),cTr=o(" \u2014 "),mX=a("a"),fTr=o("FlaxElectraForSequenceClassification"),mTr=o(" (ELECTRA model)"),gTr=l(),CC=a("li"),F7e=a("strong"),hTr=o("mbart"),pTr=o(" \u2014 "),gX=a("a"),_Tr=o("FlaxMBartForSequenceClassification"),uTr=o(" (mBART model)"),bTr=l(),MC=a("li"),C7e=a("strong"),vTr=o("roberta"),TTr=o(" \u2014 "),hX=a("a"),FTr=o("FlaxRobertaForSequenceClassification"),CTr=o(" (RoBERTa model)"),MTr=l(),EC=a("li"),M7e=a("strong"),ETr=o("roformer"),yTr=o(" \u2014 "),pX=a("a"),wTr=o("FlaxRoFormerForSequenceClassification"),ATr=o(" (RoFormer model)"),LTr=l(),E7e=a("p"),BTr=o("Examples:"),kTr=l(),f(Z6.$$.fragment),z9e=l(),af=a("h2"),yC=a("a"),y7e=a("span"),f(e0.$$.fragment),xTr=l(),w7e=a("span"),RTr=o("FlaxAutoModelForQuestionAnswering"),V9e=l(),Rr=a("div"),f(o0.$$.fragment),STr=l(),nf=a("p"),PTr=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a question answering head) when created
with the `),A7e=a("code"),$Tr=o("from_pretrained()"),ITr=o("class method or the "),L7e=a("code"),jTr=o("from_config()"),NTr=o(`class
method.`),DTr=l(),r0=a("p"),qTr=o("This class cannot be instantiated directly using "),B7e=a("code"),GTr=o("__init__()"),OTr=o(" (throws an error)."),XTr=l(),yt=a("div"),f(t0.$$.fragment),zTr=l(),k7e=a("p"),VTr=o("Instantiates one of the model classes of the library (with a question answering head) from a configuration."),WTr=l(),sf=a("p"),QTr=o(`Note:
Loading a model from its configuration file does `),x7e=a("strong"),HTr=o("not"),UTr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),R7e=a("code"),JTr=o("from_pretrained()"),YTr=o("to load the model weights."),KTr=l(),S7e=a("p"),ZTr=o("Examples:"),eFr=l(),f(a0.$$.fragment),oFr=l(),Ro=a("div"),f(n0.$$.fragment),rFr=l(),P7e=a("p"),tFr=o("Instantiate one of the model classes of the library (with a question answering head) from a pretrained model."),aFr=l(),Ln=a("p"),nFr=o("The model class to instantiate is selected based on the "),$7e=a("code"),sFr=o("model_type"),lFr=o(` property of the config object (either
passed as an argument or loaded from `),I7e=a("code"),iFr=o("pretrained_model_name_or_path"),dFr=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),j7e=a("code"),cFr=o("pretrained_model_name_or_path"),fFr=o(":"),mFr=l(),Ce=a("ul"),wC=a("li"),N7e=a("strong"),gFr=o("albert"),hFr=o(" \u2014 "),_X=a("a"),pFr=o("FlaxAlbertForQuestionAnswering"),_Fr=o(" (ALBERT model)"),uFr=l(),AC=a("li"),D7e=a("strong"),bFr=o("bart"),vFr=o(" \u2014 "),uX=a("a"),TFr=o("FlaxBartForQuestionAnswering"),FFr=o(" (BART model)"),CFr=l(),LC=a("li"),q7e=a("strong"),MFr=o("bert"),EFr=o(" \u2014 "),bX=a("a"),yFr=o("FlaxBertForQuestionAnswering"),wFr=o(" (BERT model)"),AFr=l(),BC=a("li"),G7e=a("strong"),LFr=o("big_bird"),BFr=o(" \u2014 "),vX=a("a"),kFr=o("FlaxBigBirdForQuestionAnswering"),xFr=o(" (BigBird model)"),RFr=l(),kC=a("li"),O7e=a("strong"),SFr=o("distilbert"),PFr=o(" \u2014 "),TX=a("a"),$Fr=o("FlaxDistilBertForQuestionAnswering"),IFr=o(" (DistilBERT model)"),jFr=l(),xC=a("li"),X7e=a("strong"),NFr=o("electra"),DFr=o(" \u2014 "),FX=a("a"),qFr=o("FlaxElectraForQuestionAnswering"),GFr=o(" (ELECTRA model)"),OFr=l(),RC=a("li"),z7e=a("strong"),XFr=o("mbart"),zFr=o(" \u2014 "),CX=a("a"),VFr=o("FlaxMBartForQuestionAnswering"),WFr=o(" (mBART model)"),QFr=l(),SC=a("li"),V7e=a("strong"),HFr=o("roberta"),UFr=o(" \u2014 "),MX=a("a"),JFr=o("FlaxRobertaForQuestionAnswering"),YFr=o(" (RoBERTa model)"),KFr=l(),PC=a("li"),W7e=a("strong"),ZFr=o("roformer"),eCr=o(" \u2014 "),EX=a("a"),oCr=o("FlaxRoFormerForQuestionAnswering"),rCr=o(" (RoFormer model)"),tCr=l(),Q7e=a("p"),aCr=o("Examples:"),nCr=l(),f(s0.$$.fragment),W9e=l(),lf=a("h2"),$C=a("a"),H7e=a("span"),f(l0.$$.fragment),sCr=l(),U7e=a("span"),lCr=o("FlaxAutoModelForTokenClassification"),Q9e=l(),Sr=a("div"),f(i0.$$.fragment),iCr=l(),df=a("p"),dCr=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a token classification head) when created
with the `),J7e=a("code"),cCr=o("from_pretrained()"),fCr=o("class method or the "),Y7e=a("code"),mCr=o("from_config()"),gCr=o(`class
method.`),hCr=l(),d0=a("p"),pCr=o("This class cannot be instantiated directly using "),K7e=a("code"),_Cr=o("__init__()"),uCr=o(" (throws an error)."),bCr=l(),wt=a("div"),f(c0.$$.fragment),vCr=l(),Z7e=a("p"),TCr=o("Instantiates one of the model classes of the library (with a token classification head) from a configuration."),FCr=l(),cf=a("p"),CCr=o(`Note:
Loading a model from its configuration file does `),e4e=a("strong"),MCr=o("not"),ECr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),o4e=a("code"),yCr=o("from_pretrained()"),wCr=o("to load the model weights."),ACr=l(),r4e=a("p"),LCr=o("Examples:"),BCr=l(),f(f0.$$.fragment),kCr=l(),So=a("div"),f(m0.$$.fragment),xCr=l(),t4e=a("p"),RCr=o("Instantiate one of the model classes of the library (with a token classification head) from a pretrained model."),SCr=l(),Bn=a("p"),PCr=o("The model class to instantiate is selected based on the "),a4e=a("code"),$Cr=o("model_type"),ICr=o(` property of the config object (either
passed as an argument or loaded from `),n4e=a("code"),jCr=o("pretrained_model_name_or_path"),NCr=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),s4e=a("code"),DCr=o("pretrained_model_name_or_path"),qCr=o(":"),GCr=l(),so=a("ul"),IC=a("li"),l4e=a("strong"),OCr=o("albert"),XCr=o(" \u2014 "),yX=a("a"),zCr=o("FlaxAlbertForTokenClassification"),VCr=o(" (ALBERT model)"),WCr=l(),jC=a("li"),i4e=a("strong"),QCr=o("bert"),HCr=o(" \u2014 "),wX=a("a"),UCr=o("FlaxBertForTokenClassification"),JCr=o(" (BERT model)"),YCr=l(),NC=a("li"),d4e=a("strong"),KCr=o("big_bird"),ZCr=o(" \u2014 "),AX=a("a"),eMr=o("FlaxBigBirdForTokenClassification"),oMr=o(" (BigBird model)"),rMr=l(),DC=a("li"),c4e=a("strong"),tMr=o("distilbert"),aMr=o(" \u2014 "),LX=a("a"),nMr=o("FlaxDistilBertForTokenClassification"),sMr=o(" (DistilBERT model)"),lMr=l(),qC=a("li"),f4e=a("strong"),iMr=o("electra"),dMr=o(" \u2014 "),BX=a("a"),cMr=o("FlaxElectraForTokenClassification"),fMr=o(" (ELECTRA model)"),mMr=l(),GC=a("li"),m4e=a("strong"),gMr=o("roberta"),hMr=o(" \u2014 "),kX=a("a"),pMr=o("FlaxRobertaForTokenClassification"),_Mr=o(" (RoBERTa model)"),uMr=l(),OC=a("li"),g4e=a("strong"),bMr=o("roformer"),vMr=o(" \u2014 "),xX=a("a"),TMr=o("FlaxRoFormerForTokenClassification"),FMr=o(" (RoFormer model)"),CMr=l(),h4e=a("p"),MMr=o("Examples:"),EMr=l(),f(g0.$$.fragment),H9e=l(),ff=a("h2"),XC=a("a"),p4e=a("span"),f(h0.$$.fragment),yMr=l(),_4e=a("span"),wMr=o("FlaxAutoModelForMultipleChoice"),U9e=l(),Pr=a("div"),f(p0.$$.fragment),AMr=l(),mf=a("p"),LMr=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a multiple choice head) when created
with the `),u4e=a("code"),BMr=o("from_pretrained()"),kMr=o("class method or the "),b4e=a("code"),xMr=o("from_config()"),RMr=o(`class
method.`),SMr=l(),_0=a("p"),PMr=o("This class cannot be instantiated directly using "),v4e=a("code"),$Mr=o("__init__()"),IMr=o(" (throws an error)."),jMr=l(),At=a("div"),f(u0.$$.fragment),NMr=l(),T4e=a("p"),DMr=o("Instantiates one of the model classes of the library (with a multiple choice head) from a configuration."),qMr=l(),gf=a("p"),GMr=o(`Note:
Loading a model from its configuration file does `),F4e=a("strong"),OMr=o("not"),XMr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),C4e=a("code"),zMr=o("from_pretrained()"),VMr=o("to load the model weights."),WMr=l(),M4e=a("p"),QMr=o("Examples:"),HMr=l(),f(b0.$$.fragment),UMr=l(),Po=a("div"),f(v0.$$.fragment),JMr=l(),E4e=a("p"),YMr=o("Instantiate one of the model classes of the library (with a multiple choice head) from a pretrained model."),KMr=l(),kn=a("p"),ZMr=o("The model class to instantiate is selected based on the "),y4e=a("code"),eEr=o("model_type"),oEr=o(` property of the config object (either
passed as an argument or loaded from `),w4e=a("code"),rEr=o("pretrained_model_name_or_path"),tEr=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),A4e=a("code"),aEr=o("pretrained_model_name_or_path"),nEr=o(":"),sEr=l(),lo=a("ul"),zC=a("li"),L4e=a("strong"),lEr=o("albert"),iEr=o(" \u2014 "),RX=a("a"),dEr=o("FlaxAlbertForMultipleChoice"),cEr=o(" (ALBERT model)"),fEr=l(),VC=a("li"),B4e=a("strong"),mEr=o("bert"),gEr=o(" \u2014 "),SX=a("a"),hEr=o("FlaxBertForMultipleChoice"),pEr=o(" (BERT model)"),_Er=l(),WC=a("li"),k4e=a("strong"),uEr=o("big_bird"),bEr=o(" \u2014 "),PX=a("a"),vEr=o("FlaxBigBirdForMultipleChoice"),TEr=o(" (BigBird model)"),FEr=l(),QC=a("li"),x4e=a("strong"),CEr=o("distilbert"),MEr=o(" \u2014 "),$X=a("a"),EEr=o("FlaxDistilBertForMultipleChoice"),yEr=o(" (DistilBERT model)"),wEr=l(),HC=a("li"),R4e=a("strong"),AEr=o("electra"),LEr=o(" \u2014 "),IX=a("a"),BEr=o("FlaxElectraForMultipleChoice"),kEr=o(" (ELECTRA model)"),xEr=l(),UC=a("li"),S4e=a("strong"),REr=o("roberta"),SEr=o(" \u2014 "),jX=a("a"),PEr=o("FlaxRobertaForMultipleChoice"),$Er=o(" (RoBERTa model)"),IEr=l(),JC=a("li"),P4e=a("strong"),jEr=o("roformer"),NEr=o(" \u2014 "),NX=a("a"),DEr=o("FlaxRoFormerForMultipleChoice"),qEr=o(" (RoFormer model)"),GEr=l(),$4e=a("p"),OEr=o("Examples:"),XEr=l(),f(T0.$$.fragment),J9e=l(),hf=a("h2"),YC=a("a"),I4e=a("span"),f(F0.$$.fragment),zEr=l(),j4e=a("span"),VEr=o("FlaxAutoModelForNextSentencePrediction"),Y9e=l(),$r=a("div"),f(C0.$$.fragment),WEr=l(),pf=a("p"),QEr=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a next sentence prediction head) when created
with the `),N4e=a("code"),HEr=o("from_pretrained()"),UEr=o("class method or the "),D4e=a("code"),JEr=o("from_config()"),YEr=o(`class
method.`),KEr=l(),M0=a("p"),ZEr=o("This class cannot be instantiated directly using "),q4e=a("code"),e3r=o("__init__()"),o3r=o(" (throws an error)."),r3r=l(),Lt=a("div"),f(E0.$$.fragment),t3r=l(),G4e=a("p"),a3r=o("Instantiates one of the model classes of the library (with a next sentence prediction head) from a configuration."),n3r=l(),_f=a("p"),s3r=o(`Note:
Loading a model from its configuration file does `),O4e=a("strong"),l3r=o("not"),i3r=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),X4e=a("code"),d3r=o("from_pretrained()"),c3r=o("to load the model weights."),f3r=l(),z4e=a("p"),m3r=o("Examples:"),g3r=l(),f(y0.$$.fragment),h3r=l(),$o=a("div"),f(w0.$$.fragment),p3r=l(),V4e=a("p"),_3r=o("Instantiate one of the model classes of the library (with a next sentence prediction head) from a pretrained model."),u3r=l(),xn=a("p"),b3r=o("The model class to instantiate is selected based on the "),W4e=a("code"),v3r=o("model_type"),T3r=o(` property of the config object (either
passed as an argument or loaded from `),Q4e=a("code"),F3r=o("pretrained_model_name_or_path"),C3r=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),H4e=a("code"),M3r=o("pretrained_model_name_or_path"),E3r=o(":"),y3r=l(),U4e=a("ul"),KC=a("li"),J4e=a("strong"),w3r=o("bert"),A3r=o(" \u2014 "),DX=a("a"),L3r=o("FlaxBertForNextSentencePrediction"),B3r=o(" (BERT model)"),k3r=l(),Y4e=a("p"),x3r=o("Examples:"),R3r=l(),f(A0.$$.fragment),K9e=l(),uf=a("h2"),ZC=a("a"),K4e=a("span"),f(L0.$$.fragment),S3r=l(),Z4e=a("span"),P3r=o("FlaxAutoModelForImageClassification"),Z9e=l(),Ir=a("div"),f(B0.$$.fragment),$3r=l(),bf=a("p"),I3r=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a image classification head) when created
with the `),ebe=a("code"),j3r=o("from_pretrained()"),N3r=o("class method or the "),obe=a("code"),D3r=o("from_config()"),q3r=o(`class
method.`),G3r=l(),k0=a("p"),O3r=o("This class cannot be instantiated directly using "),rbe=a("code"),X3r=o("__init__()"),z3r=o(" (throws an error)."),V3r=l(),Bt=a("div"),f(x0.$$.fragment),W3r=l(),tbe=a("p"),Q3r=o("Instantiates one of the model classes of the library (with a image classification head) from a configuration."),H3r=l(),vf=a("p"),U3r=o(`Note:
Loading a model from its configuration file does `),abe=a("strong"),J3r=o("not"),Y3r=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),nbe=a("code"),K3r=o("from_pretrained()"),Z3r=o("to load the model weights."),eyr=l(),sbe=a("p"),oyr=o("Examples:"),ryr=l(),f(R0.$$.fragment),tyr=l(),Io=a("div"),f(S0.$$.fragment),ayr=l(),lbe=a("p"),nyr=o("Instantiate one of the model classes of the library (with a image classification head) from a pretrained model."),syr=l(),Rn=a("p"),lyr=o("The model class to instantiate is selected based on the "),ibe=a("code"),iyr=o("model_type"),dyr=o(` property of the config object (either
passed as an argument or loaded from `),dbe=a("code"),cyr=o("pretrained_model_name_or_path"),fyr=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),cbe=a("code"),myr=o("pretrained_model_name_or_path"),gyr=o(":"),hyr=l(),P0=a("ul"),eM=a("li"),fbe=a("strong"),pyr=o("beit"),_yr=o(" \u2014 "),qX=a("a"),uyr=o("FlaxBeitForImageClassification"),byr=o(" (BEiT model)"),vyr=l(),oM=a("li"),mbe=a("strong"),Tyr=o("vit"),Fyr=o(" \u2014 "),GX=a("a"),Cyr=o("FlaxViTForImageClassification"),Myr=o(" (ViT model)"),Eyr=l(),gbe=a("p"),yyr=o("Examples:"),wyr=l(),f($0.$$.fragment),eBe=l(),Tf=a("h2"),rM=a("a"),hbe=a("span"),f(I0.$$.fragment),Ayr=l(),pbe=a("span"),Lyr=o("FlaxAutoModelForVision2Seq"),oBe=l(),jr=a("div"),f(j0.$$.fragment),Byr=l(),Ff=a("p"),kyr=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a vision-to-text modeling head) when created
with the `),_be=a("code"),xyr=o("from_pretrained()"),Ryr=o("class method or the "),ube=a("code"),Syr=o("from_config()"),Pyr=o(`class
method.`),$yr=l(),N0=a("p"),Iyr=o("This class cannot be instantiated directly using "),bbe=a("code"),jyr=o("__init__()"),Nyr=o(" (throws an error)."),Dyr=l(),kt=a("div"),f(D0.$$.fragment),qyr=l(),vbe=a("p"),Gyr=o("Instantiates one of the model classes of the library (with a vision-to-text modeling head) from a configuration."),Oyr=l(),Cf=a("p"),Xyr=o(`Note:
Loading a model from its configuration file does `),Tbe=a("strong"),zyr=o("not"),Vyr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Fbe=a("code"),Wyr=o("from_pretrained()"),Qyr=o("to load the model weights."),Hyr=l(),Cbe=a("p"),Uyr=o("Examples:"),Jyr=l(),f(q0.$$.fragment),Yyr=l(),jo=a("div"),f(G0.$$.fragment),Kyr=l(),Mbe=a("p"),Zyr=o("Instantiate one of the model classes of the library (with a vision-to-text modeling head) from a pretrained model."),ewr=l(),Sn=a("p"),owr=o("The model class to instantiate is selected based on the "),Ebe=a("code"),rwr=o("model_type"),twr=o(` property of the config object (either
passed as an argument or loaded from `),ybe=a("code"),awr=o("pretrained_model_name_or_path"),nwr=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),wbe=a("code"),swr=o("pretrained_model_name_or_path"),lwr=o(":"),iwr=l(),Abe=a("ul"),tM=a("li"),Lbe=a("strong"),dwr=o("vision-encoder-decoder"),cwr=o(" \u2014 "),OX=a("a"),fwr=o("FlaxVisionEncoderDecoderModel"),mwr=o(" (Vision Encoder decoder model)"),gwr=l(),Bbe=a("p"),hwr=o("Examples:"),pwr=l(),f(O0.$$.fragment),this.h()},l(d){const u=T_t('[data-svelte="svelte-1phssyn"]',document.head);J=n(u,"META",{name:!0,content:!0}),u.forEach(t),Ae=i(d),ie=n(d,"H1",{class:!0});var X0=s(ie);me=n(X0,"A",{id:!0,class:!0,href:!0});var kbe=s(me);to=n(kbe,"SPAN",{});var xbe=s(to);m(ce.$$.fragment,xbe),xbe.forEach(t),kbe.forEach(t),ue=i(X0),Do=n(X0,"SPAN",{});var uwr=s(Do);wi=r(uwr,"Auto Classes"),uwr.forEach(t),X0.forEach(t),Ef=i(d),sa=n(d,"P",{});var tBe=s(sa);Ai=r(tBe,`In many cases, the architecture you want to use can be guessed from the name or the path of the pretrained model you
are supplying to the `),Li=n(tBe,"CODE",{});var bwr=s(Li);oE=r(bwr,"from_pretrained()"),bwr.forEach(t),yf=r(tBe,` method. AutoClasses are here to do this job for you so that you
automatically retrieve the relevant model given the name/path to the pretrained weights/config/vocabulary.`),tBe.forEach(t),ye=i(d),io=n(d,"P",{});var aM=s(io);Bi=r(aM,"Instantiating one of "),Pn=n(aM,"A",{href:!0});var vwr=s(Pn);rE=r(vwr,"AutoConfig"),vwr.forEach(t),$n=r(aM,", "),In=n(aM,"A",{href:!0});var Twr=s(In);tE=r(Twr,"AutoModel"),Twr.forEach(t),ki=r(aM,`, and
`),jn=n(aM,"A",{href:!0});var Fwr=s(jn);aE=r(Fwr,"AutoTokenizer"),Fwr.forEach(t),xi=r(aM," will directly create a class of the relevant architecture. For instance"),aM.forEach(t),wf=i(d),m($a.$$.fragment,d),co=i(d),ge=n(d,"P",{});var aBe=s(ge);DL=r(aBe,"will create a model that is an instance of "),Ri=n(aBe,"A",{href:!0});var Cwr=s(Ri);qL=r(Cwr,"BertModel"),Cwr.forEach(t),GL=r(aBe,"."),aBe.forEach(t),qo=i(d),Ia=n(d,"P",{});var nBe=s(Ia);OL=r(nBe,"There is one class of "),Af=n(nBe,"CODE",{});var Mwr=s(Af);XL=r(Mwr,"AutoModel"),Mwr.forEach(t),mxe=r(nBe," for each task, and for each backend (PyTorch, TensorFlow, or Flax)."),nBe.forEach(t),t8e=i(d),Si=n(d,"H2",{class:!0});var sBe=s(Si);Lf=n(sBe,"A",{id:!0,class:!0,href:!0});var Ewr=s(Lf);$V=n(Ewr,"SPAN",{});var ywr=s($V);m(nE.$$.fragment,ywr),ywr.forEach(t),Ewr.forEach(t),gxe=i(sBe),IV=n(sBe,"SPAN",{});var wwr=s(IV);hxe=r(wwr,"Extending the Auto Classes"),wwr.forEach(t),sBe.forEach(t),a8e=i(d),Nn=n(d,"P",{});var XX=s(Nn);pxe=r(XX,`Each of the auto classes has a method to be extended with your custom classes. For instance, if you have defined a
custom class of model `),jV=n(XX,"CODE",{});var Awr=s(jV);_xe=r(Awr,"NewModel"),Awr.forEach(t),uxe=r(XX,", make sure you have a "),NV=n(XX,"CODE",{});var Lwr=s(NV);bxe=r(Lwr,"NewModelConfig"),Lwr.forEach(t),vxe=r(XX,` then you can add those to the auto
classes like this:`),XX.forEach(t),n8e=i(d),m(sE.$$.fragment,d),s8e=i(d),zL=n(d,"P",{});var Bwr=s(zL);Txe=r(Bwr,"You will then be able to use the auto classes like you would usually do!"),Bwr.forEach(t),l8e=i(d),m(Bf.$$.fragment,d),i8e=i(d),Pi=n(d,"H2",{class:!0});var lBe=s(Pi);kf=n(lBe,"A",{id:!0,class:!0,href:!0});var kwr=s(kf);DV=n(kwr,"SPAN",{});var xwr=s(DV);m(lE.$$.fragment,xwr),xwr.forEach(t),kwr.forEach(t),Fxe=i(lBe),qV=n(lBe,"SPAN",{});var Rwr=s(qV);Cxe=r(Rwr,"AutoConfig"),Rwr.forEach(t),lBe.forEach(t),d8e=i(d),Go=n(d,"DIV",{class:!0});var Ps=s(Go);m(iE.$$.fragment,Ps),Mxe=i(Ps),dE=n(Ps,"P",{});var iBe=s(dE);Exe=r(iBe,`This is a generic configuration class that will be instantiated as one of the configuration classes of the library
when created with the `),VL=n(iBe,"A",{href:!0});var Swr=s(VL);yxe=r(Swr,"from_pretrained()"),Swr.forEach(t),wxe=r(iBe," class method."),iBe.forEach(t),Axe=i(Ps),cE=n(Ps,"P",{});var dBe=s(cE);Lxe=r(dBe,"This class cannot be instantiated directly using "),GV=n(dBe,"CODE",{});var Pwr=s(GV);Bxe=r(Pwr,"__init__()"),Pwr.forEach(t),kxe=r(dBe," (throws an error)."),dBe.forEach(t),xxe=i(Ps),fo=n(Ps,"DIV",{class:!0});var ia=s(fo);m(fE.$$.fragment,ia),Rxe=i(ia),OV=n(ia,"P",{});var $wr=s(OV);Sxe=r($wr,"Instantiate one of the configuration classes of the library from a pretrained model configuration."),$wr.forEach(t),Pxe=i(ia),$i=n(ia,"P",{});var zX=s($i);$xe=r(zX,"The configuration class to instantiate is selected based on the "),XV=n(zX,"CODE",{});var Iwr=s(XV);Ixe=r(Iwr,"model_type"),Iwr.forEach(t),jxe=r(zX,` property of the config object that
is loaded, or when it\u2019s missing, by falling back to using pattern matching on `),zV=n(zX,"CODE",{});var jwr=s(zV);Nxe=r(jwr,"pretrained_model_name_or_path"),jwr.forEach(t),Dxe=r(zX,":"),zX.forEach(t),qxe=i(ia),v=n(ia,"UL",{});var T=s(v);xf=n(T,"LI",{});var Rbe=s(xf);VV=n(Rbe,"STRONG",{});var Nwr=s(VV);Gxe=r(Nwr,"albert"),Nwr.forEach(t),Oxe=r(Rbe," \u2014 "),WL=n(Rbe,"A",{href:!0});var Dwr=s(WL);Xxe=r(Dwr,"AlbertConfig"),Dwr.forEach(t),zxe=r(Rbe," (ALBERT model)"),Rbe.forEach(t),Vxe=i(T),Rf=n(T,"LI",{});var Sbe=s(Rf);WV=n(Sbe,"STRONG",{});var qwr=s(WV);Wxe=r(qwr,"bart"),qwr.forEach(t),Qxe=r(Sbe," \u2014 "),QL=n(Sbe,"A",{href:!0});var Gwr=s(QL);Hxe=r(Gwr,"BartConfig"),Gwr.forEach(t),Uxe=r(Sbe," (BART model)"),Sbe.forEach(t),Jxe=i(T),Sf=n(T,"LI",{});var Pbe=s(Sf);QV=n(Pbe,"STRONG",{});var Owr=s(QV);Yxe=r(Owr,"beit"),Owr.forEach(t),Kxe=r(Pbe," \u2014 "),HL=n(Pbe,"A",{href:!0});var Xwr=s(HL);Zxe=r(Xwr,"BeitConfig"),Xwr.forEach(t),eRe=r(Pbe," (BEiT model)"),Pbe.forEach(t),oRe=i(T),Pf=n(T,"LI",{});var $be=s(Pf);HV=n($be,"STRONG",{});var zwr=s(HV);rRe=r(zwr,"bert"),zwr.forEach(t),tRe=r($be," \u2014 "),UL=n($be,"A",{href:!0});var Vwr=s(UL);aRe=r(Vwr,"BertConfig"),Vwr.forEach(t),nRe=r($be," (BERT model)"),$be.forEach(t),sRe=i(T),$f=n(T,"LI",{});var Ibe=s($f);UV=n(Ibe,"STRONG",{});var Wwr=s(UV);lRe=r(Wwr,"bert-generation"),Wwr.forEach(t),iRe=r(Ibe," \u2014 "),JL=n(Ibe,"A",{href:!0});var Qwr=s(JL);dRe=r(Qwr,"BertGenerationConfig"),Qwr.forEach(t),cRe=r(Ibe," (Bert Generation model)"),Ibe.forEach(t),fRe=i(T),If=n(T,"LI",{});var jbe=s(If);JV=n(jbe,"STRONG",{});var Hwr=s(JV);mRe=r(Hwr,"big_bird"),Hwr.forEach(t),gRe=r(jbe," \u2014 "),YL=n(jbe,"A",{href:!0});var Uwr=s(YL);hRe=r(Uwr,"BigBirdConfig"),Uwr.forEach(t),pRe=r(jbe," (BigBird model)"),jbe.forEach(t),_Re=i(T),jf=n(T,"LI",{});var Nbe=s(jf);YV=n(Nbe,"STRONG",{});var Jwr=s(YV);uRe=r(Jwr,"bigbird_pegasus"),Jwr.forEach(t),bRe=r(Nbe," \u2014 "),KL=n(Nbe,"A",{href:!0});var Ywr=s(KL);vRe=r(Ywr,"BigBirdPegasusConfig"),Ywr.forEach(t),TRe=r(Nbe," (BigBirdPegasus model)"),Nbe.forEach(t),FRe=i(T),Nf=n(T,"LI",{});var Dbe=s(Nf);KV=n(Dbe,"STRONG",{});var Kwr=s(KV);CRe=r(Kwr,"blenderbot"),Kwr.forEach(t),MRe=r(Dbe," \u2014 "),ZL=n(Dbe,"A",{href:!0});var Zwr=s(ZL);ERe=r(Zwr,"BlenderbotConfig"),Zwr.forEach(t),yRe=r(Dbe," (Blenderbot model)"),Dbe.forEach(t),wRe=i(T),Df=n(T,"LI",{});var qbe=s(Df);ZV=n(qbe,"STRONG",{});var eAr=s(ZV);ARe=r(eAr,"blenderbot-small"),eAr.forEach(t),LRe=r(qbe," \u2014 "),e8=n(qbe,"A",{href:!0});var oAr=s(e8);BRe=r(oAr,"BlenderbotSmallConfig"),oAr.forEach(t),kRe=r(qbe," (BlenderbotSmall model)"),qbe.forEach(t),xRe=i(T),qf=n(T,"LI",{});var Gbe=s(qf);eW=n(Gbe,"STRONG",{});var rAr=s(eW);RRe=r(rAr,"camembert"),rAr.forEach(t),SRe=r(Gbe," \u2014 "),o8=n(Gbe,"A",{href:!0});var tAr=s(o8);PRe=r(tAr,"CamembertConfig"),tAr.forEach(t),$Re=r(Gbe," (CamemBERT model)"),Gbe.forEach(t),IRe=i(T),Gf=n(T,"LI",{});var Obe=s(Gf);oW=n(Obe,"STRONG",{});var aAr=s(oW);jRe=r(aAr,"canine"),aAr.forEach(t),NRe=r(Obe," \u2014 "),r8=n(Obe,"A",{href:!0});var nAr=s(r8);DRe=r(nAr,"CanineConfig"),nAr.forEach(t),qRe=r(Obe," (Canine model)"),Obe.forEach(t),GRe=i(T),Of=n(T,"LI",{});var Xbe=s(Of);rW=n(Xbe,"STRONG",{});var sAr=s(rW);ORe=r(sAr,"clip"),sAr.forEach(t),XRe=r(Xbe," \u2014 "),t8=n(Xbe,"A",{href:!0});var lAr=s(t8);zRe=r(lAr,"CLIPConfig"),lAr.forEach(t),VRe=r(Xbe," (CLIP model)"),Xbe.forEach(t),WRe=i(T),Xf=n(T,"LI",{});var zbe=s(Xf);tW=n(zbe,"STRONG",{});var iAr=s(tW);QRe=r(iAr,"convbert"),iAr.forEach(t),HRe=r(zbe," \u2014 "),a8=n(zbe,"A",{href:!0});var dAr=s(a8);URe=r(dAr,"ConvBertConfig"),dAr.forEach(t),JRe=r(zbe," (ConvBERT model)"),zbe.forEach(t),YRe=i(T),zf=n(T,"LI",{});var Vbe=s(zf);aW=n(Vbe,"STRONG",{});var cAr=s(aW);KRe=r(cAr,"convnext"),cAr.forEach(t),ZRe=r(Vbe," \u2014 "),n8=n(Vbe,"A",{href:!0});var fAr=s(n8);eSe=r(fAr,"ConvNextConfig"),fAr.forEach(t),oSe=r(Vbe," (ConvNext model)"),Vbe.forEach(t),rSe=i(T),Vf=n(T,"LI",{});var Wbe=s(Vf);nW=n(Wbe,"STRONG",{});var mAr=s(nW);tSe=r(mAr,"ctrl"),mAr.forEach(t),aSe=r(Wbe," \u2014 "),s8=n(Wbe,"A",{href:!0});var gAr=s(s8);nSe=r(gAr,"CTRLConfig"),gAr.forEach(t),sSe=r(Wbe," (CTRL model)"),Wbe.forEach(t),lSe=i(T),Wf=n(T,"LI",{});var Qbe=s(Wf);sW=n(Qbe,"STRONG",{});var hAr=s(sW);iSe=r(hAr,"deberta"),hAr.forEach(t),dSe=r(Qbe," \u2014 "),l8=n(Qbe,"A",{href:!0});var pAr=s(l8);cSe=r(pAr,"DebertaConfig"),pAr.forEach(t),fSe=r(Qbe," (DeBERTa model)"),Qbe.forEach(t),mSe=i(T),Qf=n(T,"LI",{});var Hbe=s(Qf);lW=n(Hbe,"STRONG",{});var _Ar=s(lW);gSe=r(_Ar,"deberta-v2"),_Ar.forEach(t),hSe=r(Hbe," \u2014 "),i8=n(Hbe,"A",{href:!0});var uAr=s(i8);pSe=r(uAr,"DebertaV2Config"),uAr.forEach(t),_Se=r(Hbe," (DeBERTa-v2 model)"),Hbe.forEach(t),uSe=i(T),Hf=n(T,"LI",{});var Ube=s(Hf);iW=n(Ube,"STRONG",{});var bAr=s(iW);bSe=r(bAr,"deit"),bAr.forEach(t),vSe=r(Ube," \u2014 "),d8=n(Ube,"A",{href:!0});var vAr=s(d8);TSe=r(vAr,"DeiTConfig"),vAr.forEach(t),FSe=r(Ube," (DeiT model)"),Ube.forEach(t),CSe=i(T),Uf=n(T,"LI",{});var Jbe=s(Uf);dW=n(Jbe,"STRONG",{});var TAr=s(dW);MSe=r(TAr,"detr"),TAr.forEach(t),ESe=r(Jbe," \u2014 "),c8=n(Jbe,"A",{href:!0});var FAr=s(c8);ySe=r(FAr,"DetrConfig"),FAr.forEach(t),wSe=r(Jbe," (DETR model)"),Jbe.forEach(t),ASe=i(T),Jf=n(T,"LI",{});var Ybe=s(Jf);cW=n(Ybe,"STRONG",{});var CAr=s(cW);LSe=r(CAr,"distilbert"),CAr.forEach(t),BSe=r(Ybe," \u2014 "),f8=n(Ybe,"A",{href:!0});var MAr=s(f8);kSe=r(MAr,"DistilBertConfig"),MAr.forEach(t),xSe=r(Ybe," (DistilBERT model)"),Ybe.forEach(t),RSe=i(T),Yf=n(T,"LI",{});var Kbe=s(Yf);fW=n(Kbe,"STRONG",{});var EAr=s(fW);SSe=r(EAr,"dpr"),EAr.forEach(t),PSe=r(Kbe," \u2014 "),m8=n(Kbe,"A",{href:!0});var yAr=s(m8);$Se=r(yAr,"DPRConfig"),yAr.forEach(t),ISe=r(Kbe," (DPR model)"),Kbe.forEach(t),jSe=i(T),Kf=n(T,"LI",{});var Zbe=s(Kf);mW=n(Zbe,"STRONG",{});var wAr=s(mW);NSe=r(wAr,"electra"),wAr.forEach(t),DSe=r(Zbe," \u2014 "),g8=n(Zbe,"A",{href:!0});var AAr=s(g8);qSe=r(AAr,"ElectraConfig"),AAr.forEach(t),GSe=r(Zbe," (ELECTRA model)"),Zbe.forEach(t),OSe=i(T),Zf=n(T,"LI",{});var e5e=s(Zf);gW=n(e5e,"STRONG",{});var LAr=s(gW);XSe=r(LAr,"encoder-decoder"),LAr.forEach(t),zSe=r(e5e," \u2014 "),h8=n(e5e,"A",{href:!0});var BAr=s(h8);VSe=r(BAr,"EncoderDecoderConfig"),BAr.forEach(t),WSe=r(e5e," (Encoder decoder model)"),e5e.forEach(t),QSe=i(T),em=n(T,"LI",{});var o5e=s(em);hW=n(o5e,"STRONG",{});var kAr=s(hW);HSe=r(kAr,"flaubert"),kAr.forEach(t),USe=r(o5e," \u2014 "),p8=n(o5e,"A",{href:!0});var xAr=s(p8);JSe=r(xAr,"FlaubertConfig"),xAr.forEach(t),YSe=r(o5e," (FlauBERT model)"),o5e.forEach(t),KSe=i(T),om=n(T,"LI",{});var r5e=s(om);pW=n(r5e,"STRONG",{});var RAr=s(pW);ZSe=r(RAr,"fnet"),RAr.forEach(t),ePe=r(r5e," \u2014 "),_8=n(r5e,"A",{href:!0});var SAr=s(_8);oPe=r(SAr,"FNetConfig"),SAr.forEach(t),rPe=r(r5e," (FNet model)"),r5e.forEach(t),tPe=i(T),rm=n(T,"LI",{});var t5e=s(rm);_W=n(t5e,"STRONG",{});var PAr=s(_W);aPe=r(PAr,"fsmt"),PAr.forEach(t),nPe=r(t5e," \u2014 "),u8=n(t5e,"A",{href:!0});var $Ar=s(u8);sPe=r($Ar,"FSMTConfig"),$Ar.forEach(t),lPe=r(t5e," (FairSeq Machine-Translation model)"),t5e.forEach(t),iPe=i(T),tm=n(T,"LI",{});var a5e=s(tm);uW=n(a5e,"STRONG",{});var IAr=s(uW);dPe=r(IAr,"funnel"),IAr.forEach(t),cPe=r(a5e," \u2014 "),b8=n(a5e,"A",{href:!0});var jAr=s(b8);fPe=r(jAr,"FunnelConfig"),jAr.forEach(t),mPe=r(a5e," (Funnel Transformer model)"),a5e.forEach(t),gPe=i(T),am=n(T,"LI",{});var n5e=s(am);bW=n(n5e,"STRONG",{});var NAr=s(bW);hPe=r(NAr,"gpt2"),NAr.forEach(t),pPe=r(n5e," \u2014 "),v8=n(n5e,"A",{href:!0});var DAr=s(v8);_Pe=r(DAr,"GPT2Config"),DAr.forEach(t),uPe=r(n5e," (OpenAI GPT-2 model)"),n5e.forEach(t),bPe=i(T),nm=n(T,"LI",{});var s5e=s(nm);vW=n(s5e,"STRONG",{});var qAr=s(vW);vPe=r(qAr,"gpt_neo"),qAr.forEach(t),TPe=r(s5e," \u2014 "),T8=n(s5e,"A",{href:!0});var GAr=s(T8);FPe=r(GAr,"GPTNeoConfig"),GAr.forEach(t),CPe=r(s5e," (GPT Neo model)"),s5e.forEach(t),MPe=i(T),sm=n(T,"LI",{});var l5e=s(sm);TW=n(l5e,"STRONG",{});var OAr=s(TW);EPe=r(OAr,"gptj"),OAr.forEach(t),yPe=r(l5e," \u2014 "),F8=n(l5e,"A",{href:!0});var XAr=s(F8);wPe=r(XAr,"GPTJConfig"),XAr.forEach(t),APe=r(l5e," (GPT-J model)"),l5e.forEach(t),LPe=i(T),lm=n(T,"LI",{});var i5e=s(lm);FW=n(i5e,"STRONG",{});var zAr=s(FW);BPe=r(zAr,"hubert"),zAr.forEach(t),kPe=r(i5e," \u2014 "),C8=n(i5e,"A",{href:!0});var VAr=s(C8);xPe=r(VAr,"HubertConfig"),VAr.forEach(t),RPe=r(i5e," (Hubert model)"),i5e.forEach(t),SPe=i(T),im=n(T,"LI",{});var d5e=s(im);CW=n(d5e,"STRONG",{});var WAr=s(CW);PPe=r(WAr,"ibert"),WAr.forEach(t),$Pe=r(d5e," \u2014 "),M8=n(d5e,"A",{href:!0});var QAr=s(M8);IPe=r(QAr,"IBertConfig"),QAr.forEach(t),jPe=r(d5e," (I-BERT model)"),d5e.forEach(t),NPe=i(T),dm=n(T,"LI",{});var c5e=s(dm);MW=n(c5e,"STRONG",{});var HAr=s(MW);DPe=r(HAr,"imagegpt"),HAr.forEach(t),qPe=r(c5e," \u2014 "),E8=n(c5e,"A",{href:!0});var UAr=s(E8);GPe=r(UAr,"ImageGPTConfig"),UAr.forEach(t),OPe=r(c5e," (ImageGPT model)"),c5e.forEach(t),XPe=i(T),cm=n(T,"LI",{});var f5e=s(cm);EW=n(f5e,"STRONG",{});var JAr=s(EW);zPe=r(JAr,"layoutlm"),JAr.forEach(t),VPe=r(f5e," \u2014 "),y8=n(f5e,"A",{href:!0});var YAr=s(y8);WPe=r(YAr,"LayoutLMConfig"),YAr.forEach(t),QPe=r(f5e," (LayoutLM model)"),f5e.forEach(t),HPe=i(T),fm=n(T,"LI",{});var m5e=s(fm);yW=n(m5e,"STRONG",{});var KAr=s(yW);UPe=r(KAr,"layoutlmv2"),KAr.forEach(t),JPe=r(m5e," \u2014 "),w8=n(m5e,"A",{href:!0});var ZAr=s(w8);YPe=r(ZAr,"LayoutLMv2Config"),ZAr.forEach(t),KPe=r(m5e," (LayoutLMv2 model)"),m5e.forEach(t),ZPe=i(T),mm=n(T,"LI",{});var g5e=s(mm);wW=n(g5e,"STRONG",{});var e6r=s(wW);e$e=r(e6r,"led"),e6r.forEach(t),o$e=r(g5e," \u2014 "),A8=n(g5e,"A",{href:!0});var o6r=s(A8);r$e=r(o6r,"LEDConfig"),o6r.forEach(t),t$e=r(g5e," (LED model)"),g5e.forEach(t),a$e=i(T),gm=n(T,"LI",{});var h5e=s(gm);AW=n(h5e,"STRONG",{});var r6r=s(AW);n$e=r(r6r,"longformer"),r6r.forEach(t),s$e=r(h5e," \u2014 "),L8=n(h5e,"A",{href:!0});var t6r=s(L8);l$e=r(t6r,"LongformerConfig"),t6r.forEach(t),i$e=r(h5e," (Longformer model)"),h5e.forEach(t),d$e=i(T),hm=n(T,"LI",{});var p5e=s(hm);LW=n(p5e,"STRONG",{});var a6r=s(LW);c$e=r(a6r,"luke"),a6r.forEach(t),f$e=r(p5e," \u2014 "),B8=n(p5e,"A",{href:!0});var n6r=s(B8);m$e=r(n6r,"LukeConfig"),n6r.forEach(t),g$e=r(p5e," (LUKE model)"),p5e.forEach(t),h$e=i(T),pm=n(T,"LI",{});var _5e=s(pm);BW=n(_5e,"STRONG",{});var s6r=s(BW);p$e=r(s6r,"lxmert"),s6r.forEach(t),_$e=r(_5e," \u2014 "),k8=n(_5e,"A",{href:!0});var l6r=s(k8);u$e=r(l6r,"LxmertConfig"),l6r.forEach(t),b$e=r(_5e," (LXMERT model)"),_5e.forEach(t),v$e=i(T),_m=n(T,"LI",{});var u5e=s(_m);kW=n(u5e,"STRONG",{});var i6r=s(kW);T$e=r(i6r,"m2m_100"),i6r.forEach(t),F$e=r(u5e," \u2014 "),x8=n(u5e,"A",{href:!0});var d6r=s(x8);C$e=r(d6r,"M2M100Config"),d6r.forEach(t),M$e=r(u5e," (M2M100 model)"),u5e.forEach(t),E$e=i(T),um=n(T,"LI",{});var b5e=s(um);xW=n(b5e,"STRONG",{});var c6r=s(xW);y$e=r(c6r,"marian"),c6r.forEach(t),w$e=r(b5e," \u2014 "),R8=n(b5e,"A",{href:!0});var f6r=s(R8);A$e=r(f6r,"MarianConfig"),f6r.forEach(t),L$e=r(b5e," (Marian model)"),b5e.forEach(t),B$e=i(T),bm=n(T,"LI",{});var v5e=s(bm);RW=n(v5e,"STRONG",{});var m6r=s(RW);k$e=r(m6r,"mbart"),m6r.forEach(t),x$e=r(v5e," \u2014 "),S8=n(v5e,"A",{href:!0});var g6r=s(S8);R$e=r(g6r,"MBartConfig"),g6r.forEach(t),S$e=r(v5e," (mBART model)"),v5e.forEach(t),P$e=i(T),vm=n(T,"LI",{});var T5e=s(vm);SW=n(T5e,"STRONG",{});var h6r=s(SW);$$e=r(h6r,"megatron-bert"),h6r.forEach(t),I$e=r(T5e," \u2014 "),P8=n(T5e,"A",{href:!0});var p6r=s(P8);j$e=r(p6r,"MegatronBertConfig"),p6r.forEach(t),N$e=r(T5e," (MegatronBert model)"),T5e.forEach(t),D$e=i(T),Tm=n(T,"LI",{});var F5e=s(Tm);PW=n(F5e,"STRONG",{});var _6r=s(PW);q$e=r(_6r,"mobilebert"),_6r.forEach(t),G$e=r(F5e," \u2014 "),$8=n(F5e,"A",{href:!0});var u6r=s($8);O$e=r(u6r,"MobileBertConfig"),u6r.forEach(t),X$e=r(F5e," (MobileBERT model)"),F5e.forEach(t),z$e=i(T),Fm=n(T,"LI",{});var C5e=s(Fm);$W=n(C5e,"STRONG",{});var b6r=s($W);V$e=r(b6r,"mpnet"),b6r.forEach(t),W$e=r(C5e," \u2014 "),I8=n(C5e,"A",{href:!0});var v6r=s(I8);Q$e=r(v6r,"MPNetConfig"),v6r.forEach(t),H$e=r(C5e," (MPNet model)"),C5e.forEach(t),U$e=i(T),Cm=n(T,"LI",{});var M5e=s(Cm);IW=n(M5e,"STRONG",{});var T6r=s(IW);J$e=r(T6r,"mt5"),T6r.forEach(t),Y$e=r(M5e," \u2014 "),j8=n(M5e,"A",{href:!0});var F6r=s(j8);K$e=r(F6r,"MT5Config"),F6r.forEach(t),Z$e=r(M5e," (mT5 model)"),M5e.forEach(t),eIe=i(T),Mm=n(T,"LI",{});var E5e=s(Mm);jW=n(E5e,"STRONG",{});var C6r=s(jW);oIe=r(C6r,"nystromformer"),C6r.forEach(t),rIe=r(E5e," \u2014 "),N8=n(E5e,"A",{href:!0});var M6r=s(N8);tIe=r(M6r,"NystromformerConfig"),M6r.forEach(t),aIe=r(E5e," (Nystromformer model)"),E5e.forEach(t),nIe=i(T),Em=n(T,"LI",{});var y5e=s(Em);NW=n(y5e,"STRONG",{});var E6r=s(NW);sIe=r(E6r,"openai-gpt"),E6r.forEach(t),lIe=r(y5e," \u2014 "),D8=n(y5e,"A",{href:!0});var y6r=s(D8);iIe=r(y6r,"OpenAIGPTConfig"),y6r.forEach(t),dIe=r(y5e," (OpenAI GPT model)"),y5e.forEach(t),cIe=i(T),ym=n(T,"LI",{});var w5e=s(ym);DW=n(w5e,"STRONG",{});var w6r=s(DW);fIe=r(w6r,"pegasus"),w6r.forEach(t),mIe=r(w5e," \u2014 "),q8=n(w5e,"A",{href:!0});var A6r=s(q8);gIe=r(A6r,"PegasusConfig"),A6r.forEach(t),hIe=r(w5e," (Pegasus model)"),w5e.forEach(t),pIe=i(T),wm=n(T,"LI",{});var A5e=s(wm);qW=n(A5e,"STRONG",{});var L6r=s(qW);_Ie=r(L6r,"perceiver"),L6r.forEach(t),uIe=r(A5e," \u2014 "),G8=n(A5e,"A",{href:!0});var B6r=s(G8);bIe=r(B6r,"PerceiverConfig"),B6r.forEach(t),vIe=r(A5e," (Perceiver model)"),A5e.forEach(t),TIe=i(T),Am=n(T,"LI",{});var L5e=s(Am);GW=n(L5e,"STRONG",{});var k6r=s(GW);FIe=r(k6r,"plbart"),k6r.forEach(t),CIe=r(L5e," \u2014 "),O8=n(L5e,"A",{href:!0});var x6r=s(O8);MIe=r(x6r,"PLBartConfig"),x6r.forEach(t),EIe=r(L5e," (PLBart model)"),L5e.forEach(t),yIe=i(T),Lm=n(T,"LI",{});var B5e=s(Lm);OW=n(B5e,"STRONG",{});var R6r=s(OW);wIe=r(R6r,"poolformer"),R6r.forEach(t),AIe=r(B5e," \u2014 "),X8=n(B5e,"A",{href:!0});var S6r=s(X8);LIe=r(S6r,"PoolFormerConfig"),S6r.forEach(t),BIe=r(B5e," (PoolFormer model)"),B5e.forEach(t),kIe=i(T),Bm=n(T,"LI",{});var k5e=s(Bm);XW=n(k5e,"STRONG",{});var P6r=s(XW);xIe=r(P6r,"prophetnet"),P6r.forEach(t),RIe=r(k5e," \u2014 "),z8=n(k5e,"A",{href:!0});var $6r=s(z8);SIe=r($6r,"ProphetNetConfig"),$6r.forEach(t),PIe=r(k5e," (ProphetNet model)"),k5e.forEach(t),$Ie=i(T),km=n(T,"LI",{});var x5e=s(km);zW=n(x5e,"STRONG",{});var I6r=s(zW);IIe=r(I6r,"qdqbert"),I6r.forEach(t),jIe=r(x5e," \u2014 "),V8=n(x5e,"A",{href:!0});var j6r=s(V8);NIe=r(j6r,"QDQBertConfig"),j6r.forEach(t),DIe=r(x5e," (QDQBert model)"),x5e.forEach(t),qIe=i(T),xm=n(T,"LI",{});var R5e=s(xm);VW=n(R5e,"STRONG",{});var N6r=s(VW);GIe=r(N6r,"rag"),N6r.forEach(t),OIe=r(R5e," \u2014 "),W8=n(R5e,"A",{href:!0});var D6r=s(W8);XIe=r(D6r,"RagConfig"),D6r.forEach(t),zIe=r(R5e," (RAG model)"),R5e.forEach(t),VIe=i(T),Rm=n(T,"LI",{});var S5e=s(Rm);WW=n(S5e,"STRONG",{});var q6r=s(WW);WIe=r(q6r,"realm"),q6r.forEach(t),QIe=r(S5e," \u2014 "),Q8=n(S5e,"A",{href:!0});var G6r=s(Q8);HIe=r(G6r,"RealmConfig"),G6r.forEach(t),UIe=r(S5e," (Realm model)"),S5e.forEach(t),JIe=i(T),Sm=n(T,"LI",{});var P5e=s(Sm);QW=n(P5e,"STRONG",{});var O6r=s(QW);YIe=r(O6r,"reformer"),O6r.forEach(t),KIe=r(P5e," \u2014 "),H8=n(P5e,"A",{href:!0});var X6r=s(H8);ZIe=r(X6r,"ReformerConfig"),X6r.forEach(t),eje=r(P5e," (Reformer model)"),P5e.forEach(t),oje=i(T),Pm=n(T,"LI",{});var $5e=s(Pm);HW=n($5e,"STRONG",{});var z6r=s(HW);rje=r(z6r,"rembert"),z6r.forEach(t),tje=r($5e," \u2014 "),U8=n($5e,"A",{href:!0});var V6r=s(U8);aje=r(V6r,"RemBertConfig"),V6r.forEach(t),nje=r($5e," (RemBERT model)"),$5e.forEach(t),sje=i(T),$m=n(T,"LI",{});var I5e=s($m);UW=n(I5e,"STRONG",{});var W6r=s(UW);lje=r(W6r,"retribert"),W6r.forEach(t),ije=r(I5e," \u2014 "),J8=n(I5e,"A",{href:!0});var Q6r=s(J8);dje=r(Q6r,"RetriBertConfig"),Q6r.forEach(t),cje=r(I5e," (RetriBERT model)"),I5e.forEach(t),fje=i(T),Im=n(T,"LI",{});var j5e=s(Im);JW=n(j5e,"STRONG",{});var H6r=s(JW);mje=r(H6r,"roberta"),H6r.forEach(t),gje=r(j5e," \u2014 "),Y8=n(j5e,"A",{href:!0});var U6r=s(Y8);hje=r(U6r,"RobertaConfig"),U6r.forEach(t),pje=r(j5e," (RoBERTa model)"),j5e.forEach(t),_je=i(T),jm=n(T,"LI",{});var N5e=s(jm);YW=n(N5e,"STRONG",{});var J6r=s(YW);uje=r(J6r,"roformer"),J6r.forEach(t),bje=r(N5e," \u2014 "),K8=n(N5e,"A",{href:!0});var Y6r=s(K8);vje=r(Y6r,"RoFormerConfig"),Y6r.forEach(t),Tje=r(N5e," (RoFormer model)"),N5e.forEach(t),Fje=i(T),Nm=n(T,"LI",{});var D5e=s(Nm);KW=n(D5e,"STRONG",{});var K6r=s(KW);Cje=r(K6r,"segformer"),K6r.forEach(t),Mje=r(D5e," \u2014 "),Z8=n(D5e,"A",{href:!0});var Z6r=s(Z8);Eje=r(Z6r,"SegformerConfig"),Z6r.forEach(t),yje=r(D5e," (SegFormer model)"),D5e.forEach(t),wje=i(T),Dm=n(T,"LI",{});var q5e=s(Dm);ZW=n(q5e,"STRONG",{});var e0r=s(ZW);Aje=r(e0r,"sew"),e0r.forEach(t),Lje=r(q5e," \u2014 "),e9=n(q5e,"A",{href:!0});var o0r=s(e9);Bje=r(o0r,"SEWConfig"),o0r.forEach(t),kje=r(q5e," (SEW model)"),q5e.forEach(t),xje=i(T),qm=n(T,"LI",{});var G5e=s(qm);eQ=n(G5e,"STRONG",{});var r0r=s(eQ);Rje=r(r0r,"sew-d"),r0r.forEach(t),Sje=r(G5e," \u2014 "),o9=n(G5e,"A",{href:!0});var t0r=s(o9);Pje=r(t0r,"SEWDConfig"),t0r.forEach(t),$je=r(G5e," (SEW-D model)"),G5e.forEach(t),Ije=i(T),Gm=n(T,"LI",{});var O5e=s(Gm);oQ=n(O5e,"STRONG",{});var a0r=s(oQ);jje=r(a0r,"speech-encoder-decoder"),a0r.forEach(t),Nje=r(O5e," \u2014 "),r9=n(O5e,"A",{href:!0});var n0r=s(r9);Dje=r(n0r,"SpeechEncoderDecoderConfig"),n0r.forEach(t),qje=r(O5e," (Speech Encoder decoder model)"),O5e.forEach(t),Gje=i(T),Om=n(T,"LI",{});var X5e=s(Om);rQ=n(X5e,"STRONG",{});var s0r=s(rQ);Oje=r(s0r,"speech_to_text"),s0r.forEach(t),Xje=r(X5e," \u2014 "),t9=n(X5e,"A",{href:!0});var l0r=s(t9);zje=r(l0r,"Speech2TextConfig"),l0r.forEach(t),Vje=r(X5e," (Speech2Text model)"),X5e.forEach(t),Wje=i(T),Xm=n(T,"LI",{});var z5e=s(Xm);tQ=n(z5e,"STRONG",{});var i0r=s(tQ);Qje=r(i0r,"speech_to_text_2"),i0r.forEach(t),Hje=r(z5e," \u2014 "),a9=n(z5e,"A",{href:!0});var d0r=s(a9);Uje=r(d0r,"Speech2Text2Config"),d0r.forEach(t),Jje=r(z5e," (Speech2Text2 model)"),z5e.forEach(t),Yje=i(T),zm=n(T,"LI",{});var V5e=s(zm);aQ=n(V5e,"STRONG",{});var c0r=s(aQ);Kje=r(c0r,"splinter"),c0r.forEach(t),Zje=r(V5e," \u2014 "),n9=n(V5e,"A",{href:!0});var f0r=s(n9);eNe=r(f0r,"SplinterConfig"),f0r.forEach(t),oNe=r(V5e," (Splinter model)"),V5e.forEach(t),rNe=i(T),Vm=n(T,"LI",{});var W5e=s(Vm);nQ=n(W5e,"STRONG",{});var m0r=s(nQ);tNe=r(m0r,"squeezebert"),m0r.forEach(t),aNe=r(W5e," \u2014 "),s9=n(W5e,"A",{href:!0});var g0r=s(s9);nNe=r(g0r,"SqueezeBertConfig"),g0r.forEach(t),sNe=r(W5e," (SqueezeBERT model)"),W5e.forEach(t),lNe=i(T),Wm=n(T,"LI",{});var Q5e=s(Wm);sQ=n(Q5e,"STRONG",{});var h0r=s(sQ);iNe=r(h0r,"swin"),h0r.forEach(t),dNe=r(Q5e," \u2014 "),l9=n(Q5e,"A",{href:!0});var p0r=s(l9);cNe=r(p0r,"SwinConfig"),p0r.forEach(t),fNe=r(Q5e," (Swin model)"),Q5e.forEach(t),mNe=i(T),Qm=n(T,"LI",{});var H5e=s(Qm);lQ=n(H5e,"STRONG",{});var _0r=s(lQ);gNe=r(_0r,"t5"),_0r.forEach(t),hNe=r(H5e," \u2014 "),i9=n(H5e,"A",{href:!0});var u0r=s(i9);pNe=r(u0r,"T5Config"),u0r.forEach(t),_Ne=r(H5e," (T5 model)"),H5e.forEach(t),uNe=i(T),Hm=n(T,"LI",{});var U5e=s(Hm);iQ=n(U5e,"STRONG",{});var b0r=s(iQ);bNe=r(b0r,"tapas"),b0r.forEach(t),vNe=r(U5e," \u2014 "),d9=n(U5e,"A",{href:!0});var v0r=s(d9);TNe=r(v0r,"TapasConfig"),v0r.forEach(t),FNe=r(U5e," (TAPAS model)"),U5e.forEach(t),CNe=i(T),Um=n(T,"LI",{});var J5e=s(Um);dQ=n(J5e,"STRONG",{});var T0r=s(dQ);MNe=r(T0r,"transfo-xl"),T0r.forEach(t),ENe=r(J5e," \u2014 "),c9=n(J5e,"A",{href:!0});var F0r=s(c9);yNe=r(F0r,"TransfoXLConfig"),F0r.forEach(t),wNe=r(J5e," (Transformer-XL model)"),J5e.forEach(t),ANe=i(T),Jm=n(T,"LI",{});var Y5e=s(Jm);cQ=n(Y5e,"STRONG",{});var C0r=s(cQ);LNe=r(C0r,"trocr"),C0r.forEach(t),BNe=r(Y5e," \u2014 "),f9=n(Y5e,"A",{href:!0});var M0r=s(f9);kNe=r(M0r,"TrOCRConfig"),M0r.forEach(t),xNe=r(Y5e," (TrOCR model)"),Y5e.forEach(t),RNe=i(T),Ym=n(T,"LI",{});var K5e=s(Ym);fQ=n(K5e,"STRONG",{});var E0r=s(fQ);SNe=r(E0r,"unispeech"),E0r.forEach(t),PNe=r(K5e," \u2014 "),m9=n(K5e,"A",{href:!0});var y0r=s(m9);$Ne=r(y0r,"UniSpeechConfig"),y0r.forEach(t),INe=r(K5e," (UniSpeech model)"),K5e.forEach(t),jNe=i(T),Km=n(T,"LI",{});var Z5e=s(Km);mQ=n(Z5e,"STRONG",{});var w0r=s(mQ);NNe=r(w0r,"unispeech-sat"),w0r.forEach(t),DNe=r(Z5e," \u2014 "),g9=n(Z5e,"A",{href:!0});var A0r=s(g9);qNe=r(A0r,"UniSpeechSatConfig"),A0r.forEach(t),GNe=r(Z5e," (UniSpeechSat model)"),Z5e.forEach(t),ONe=i(T),Zm=n(T,"LI",{});var e2e=s(Zm);gQ=n(e2e,"STRONG",{});var L0r=s(gQ);XNe=r(L0r,"vilt"),L0r.forEach(t),zNe=r(e2e," \u2014 "),h9=n(e2e,"A",{href:!0});var B0r=s(h9);VNe=r(B0r,"ViltConfig"),B0r.forEach(t),WNe=r(e2e," (ViLT model)"),e2e.forEach(t),QNe=i(T),eg=n(T,"LI",{});var o2e=s(eg);hQ=n(o2e,"STRONG",{});var k0r=s(hQ);HNe=r(k0r,"vision-encoder-decoder"),k0r.forEach(t),UNe=r(o2e," \u2014 "),p9=n(o2e,"A",{href:!0});var x0r=s(p9);JNe=r(x0r,"VisionEncoderDecoderConfig"),x0r.forEach(t),YNe=r(o2e," (Vision Encoder decoder model)"),o2e.forEach(t),KNe=i(T),og=n(T,"LI",{});var r2e=s(og);pQ=n(r2e,"STRONG",{});var R0r=s(pQ);ZNe=r(R0r,"vision-text-dual-encoder"),R0r.forEach(t),eDe=r(r2e," \u2014 "),_9=n(r2e,"A",{href:!0});var S0r=s(_9);oDe=r(S0r,"VisionTextDualEncoderConfig"),S0r.forEach(t),rDe=r(r2e," (VisionTextDualEncoder model)"),r2e.forEach(t),tDe=i(T),rg=n(T,"LI",{});var t2e=s(rg);_Q=n(t2e,"STRONG",{});var P0r=s(_Q);aDe=r(P0r,"visual_bert"),P0r.forEach(t),nDe=r(t2e," \u2014 "),u9=n(t2e,"A",{href:!0});var $0r=s(u9);sDe=r($0r,"VisualBertConfig"),$0r.forEach(t),lDe=r(t2e," (VisualBert model)"),t2e.forEach(t),iDe=i(T),tg=n(T,"LI",{});var a2e=s(tg);uQ=n(a2e,"STRONG",{});var I0r=s(uQ);dDe=r(I0r,"vit"),I0r.forEach(t),cDe=r(a2e," \u2014 "),b9=n(a2e,"A",{href:!0});var j0r=s(b9);fDe=r(j0r,"ViTConfig"),j0r.forEach(t),mDe=r(a2e," (ViT model)"),a2e.forEach(t),gDe=i(T),ag=n(T,"LI",{});var n2e=s(ag);bQ=n(n2e,"STRONG",{});var N0r=s(bQ);hDe=r(N0r,"vit_mae"),N0r.forEach(t),pDe=r(n2e," \u2014 "),v9=n(n2e,"A",{href:!0});var D0r=s(v9);_De=r(D0r,"ViTMAEConfig"),D0r.forEach(t),uDe=r(n2e," (ViTMAE model)"),n2e.forEach(t),bDe=i(T),ng=n(T,"LI",{});var s2e=s(ng);vQ=n(s2e,"STRONG",{});var q0r=s(vQ);vDe=r(q0r,"wav2vec2"),q0r.forEach(t),TDe=r(s2e," \u2014 "),T9=n(s2e,"A",{href:!0});var G0r=s(T9);FDe=r(G0r,"Wav2Vec2Config"),G0r.forEach(t),CDe=r(s2e," (Wav2Vec2 model)"),s2e.forEach(t),MDe=i(T),sg=n(T,"LI",{});var l2e=s(sg);TQ=n(l2e,"STRONG",{});var O0r=s(TQ);EDe=r(O0r,"wavlm"),O0r.forEach(t),yDe=r(l2e," \u2014 "),F9=n(l2e,"A",{href:!0});var X0r=s(F9);wDe=r(X0r,"WavLMConfig"),X0r.forEach(t),ADe=r(l2e," (WavLM model)"),l2e.forEach(t),LDe=i(T),lg=n(T,"LI",{});var i2e=s(lg);FQ=n(i2e,"STRONG",{});var z0r=s(FQ);BDe=r(z0r,"xglm"),z0r.forEach(t),kDe=r(i2e," \u2014 "),C9=n(i2e,"A",{href:!0});var V0r=s(C9);xDe=r(V0r,"XGLMConfig"),V0r.forEach(t),RDe=r(i2e," (XGLM model)"),i2e.forEach(t),SDe=i(T),ig=n(T,"LI",{});var d2e=s(ig);CQ=n(d2e,"STRONG",{});var W0r=s(CQ);PDe=r(W0r,"xlm"),W0r.forEach(t),$De=r(d2e," \u2014 "),M9=n(d2e,"A",{href:!0});var Q0r=s(M9);IDe=r(Q0r,"XLMConfig"),Q0r.forEach(t),jDe=r(d2e," (XLM model)"),d2e.forEach(t),NDe=i(T),dg=n(T,"LI",{});var c2e=s(dg);MQ=n(c2e,"STRONG",{});var H0r=s(MQ);DDe=r(H0r,"xlm-prophetnet"),H0r.forEach(t),qDe=r(c2e," \u2014 "),E9=n(c2e,"A",{href:!0});var U0r=s(E9);GDe=r(U0r,"XLMProphetNetConfig"),U0r.forEach(t),ODe=r(c2e," (XLMProphetNet model)"),c2e.forEach(t),XDe=i(T),cg=n(T,"LI",{});var f2e=s(cg);EQ=n(f2e,"STRONG",{});var J0r=s(EQ);zDe=r(J0r,"xlm-roberta"),J0r.forEach(t),VDe=r(f2e," \u2014 "),y9=n(f2e,"A",{href:!0});var Y0r=s(y9);WDe=r(Y0r,"XLMRobertaConfig"),Y0r.forEach(t),QDe=r(f2e," (XLM-RoBERTa model)"),f2e.forEach(t),HDe=i(T),fg=n(T,"LI",{});var m2e=s(fg);yQ=n(m2e,"STRONG",{});var K0r=s(yQ);UDe=r(K0r,"xlm-roberta-xl"),K0r.forEach(t),JDe=r(m2e," \u2014 "),w9=n(m2e,"A",{href:!0});var Z0r=s(w9);YDe=r(Z0r,"XLMRobertaXLConfig"),Z0r.forEach(t),KDe=r(m2e," (XLM-RoBERTa-XL model)"),m2e.forEach(t),ZDe=i(T),mg=n(T,"LI",{});var g2e=s(mg);wQ=n(g2e,"STRONG",{});var eLr=s(wQ);eqe=r(eLr,"xlnet"),eLr.forEach(t),oqe=r(g2e," \u2014 "),A9=n(g2e,"A",{href:!0});var oLr=s(A9);rqe=r(oLr,"XLNetConfig"),oLr.forEach(t),tqe=r(g2e," (XLNet model)"),g2e.forEach(t),aqe=i(T),gg=n(T,"LI",{});var h2e=s(gg);AQ=n(h2e,"STRONG",{});var rLr=s(AQ);nqe=r(rLr,"yoso"),rLr.forEach(t),sqe=r(h2e," \u2014 "),L9=n(h2e,"A",{href:!0});var tLr=s(L9);lqe=r(tLr,"YosoConfig"),tLr.forEach(t),iqe=r(h2e," (YOSO model)"),h2e.forEach(t),T.forEach(t),dqe=i(ia),LQ=n(ia,"P",{});var aLr=s(LQ);cqe=r(aLr,"Examples:"),aLr.forEach(t),fqe=i(ia),m(mE.$$.fragment,ia),ia.forEach(t),mqe=i(Ps),hg=n(Ps,"DIV",{class:!0});var cBe=s(hg);m(gE.$$.fragment,cBe),gqe=i(cBe),BQ=n(cBe,"P",{});var nLr=s(BQ);hqe=r(nLr,"Register a new configuration for this class."),nLr.forEach(t),cBe.forEach(t),Ps.forEach(t),c8e=i(d),Ii=n(d,"H2",{class:!0});var fBe=s(Ii);pg=n(fBe,"A",{id:!0,class:!0,href:!0});var sLr=s(pg);kQ=n(sLr,"SPAN",{});var lLr=s(kQ);m(hE.$$.fragment,lLr),lLr.forEach(t),sLr.forEach(t),pqe=i(fBe),xQ=n(fBe,"SPAN",{});var iLr=s(xQ);_qe=r(iLr,"AutoTokenizer"),iLr.forEach(t),fBe.forEach(t),f8e=i(d),Oo=n(d,"DIV",{class:!0});var $s=s(Oo);m(pE.$$.fragment,$s),uqe=i($s),_E=n($s,"P",{});var mBe=s(_E);bqe=r(mBe,`This is a generic tokenizer class that will be instantiated as one of the tokenizer classes of the library when
created with the `),B9=n(mBe,"A",{href:!0});var dLr=s(B9);vqe=r(dLr,"AutoTokenizer.from_pretrained()"),dLr.forEach(t),Tqe=r(mBe," class method."),mBe.forEach(t),Fqe=i($s),uE=n($s,"P",{});var gBe=s(uE);Cqe=r(gBe,"This class cannot be instantiated directly using "),RQ=n(gBe,"CODE",{});var cLr=s(RQ);Mqe=r(cLr,"__init__()"),cLr.forEach(t),Eqe=r(gBe," (throws an error)."),gBe.forEach(t),yqe=i($s),mo=n($s,"DIV",{class:!0});var da=s(mo);m(bE.$$.fragment,da),wqe=i(da),SQ=n(da,"P",{});var fLr=s(SQ);Aqe=r(fLr,"Instantiate one of the tokenizer classes of the library from a pretrained model vocabulary."),fLr.forEach(t),Lqe=i(da),ja=n(da,"P",{});var nM=s(ja);Bqe=r(nM,"The tokenizer class to instantiate is selected based on the "),PQ=n(nM,"CODE",{});var mLr=s(PQ);kqe=r(mLr,"model_type"),mLr.forEach(t),xqe=r(nM,` property of the config object (either
passed as an argument or loaded from `),$Q=n(nM,"CODE",{});var gLr=s($Q);Rqe=r(gLr,"pretrained_model_name_or_path"),gLr.forEach(t),Sqe=r(nM,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),IQ=n(nM,"CODE",{});var hLr=s(IQ);Pqe=r(hLr,"pretrained_model_name_or_path"),hLr.forEach(t),$qe=r(nM,":"),nM.forEach(t),Iqe=i(da),M=n(da,"UL",{});var y=s(M);Dn=n(y,"LI",{});var z0=s(Dn);jQ=n(z0,"STRONG",{});var pLr=s(jQ);jqe=r(pLr,"albert"),pLr.forEach(t),Nqe=r(z0," \u2014 "),k9=n(z0,"A",{href:!0});var _Lr=s(k9);Dqe=r(_Lr,"AlbertTokenizer"),_Lr.forEach(t),qqe=r(z0," or "),x9=n(z0,"A",{href:!0});var uLr=s(x9);Gqe=r(uLr,"AlbertTokenizerFast"),uLr.forEach(t),Oqe=r(z0," (ALBERT model)"),z0.forEach(t),Xqe=i(y),qn=n(y,"LI",{});var V0=s(qn);NQ=n(V0,"STRONG",{});var bLr=s(NQ);zqe=r(bLr,"bart"),bLr.forEach(t),Vqe=r(V0," \u2014 "),R9=n(V0,"A",{href:!0});var vLr=s(R9);Wqe=r(vLr,"BartTokenizer"),vLr.forEach(t),Qqe=r(V0," or "),S9=n(V0,"A",{href:!0});var TLr=s(S9);Hqe=r(TLr,"BartTokenizerFast"),TLr.forEach(t),Uqe=r(V0," (BART model)"),V0.forEach(t),Jqe=i(y),Gn=n(y,"LI",{});var W0=s(Gn);DQ=n(W0,"STRONG",{});var FLr=s(DQ);Yqe=r(FLr,"barthez"),FLr.forEach(t),Kqe=r(W0," \u2014 "),P9=n(W0,"A",{href:!0});var CLr=s(P9);Zqe=r(CLr,"BarthezTokenizer"),CLr.forEach(t),eGe=r(W0," or "),$9=n(W0,"A",{href:!0});var MLr=s($9);oGe=r(MLr,"BarthezTokenizerFast"),MLr.forEach(t),rGe=r(W0," (BARThez model)"),W0.forEach(t),tGe=i(y),_g=n(y,"LI",{});var p2e=s(_g);qQ=n(p2e,"STRONG",{});var ELr=s(qQ);aGe=r(ELr,"bartpho"),ELr.forEach(t),nGe=r(p2e," \u2014 "),I9=n(p2e,"A",{href:!0});var yLr=s(I9);sGe=r(yLr,"BartphoTokenizer"),yLr.forEach(t),lGe=r(p2e," (BARTpho model)"),p2e.forEach(t),iGe=i(y),On=n(y,"LI",{});var Q0=s(On);GQ=n(Q0,"STRONG",{});var wLr=s(GQ);dGe=r(wLr,"bert"),wLr.forEach(t),cGe=r(Q0," \u2014 "),j9=n(Q0,"A",{href:!0});var ALr=s(j9);fGe=r(ALr,"BertTokenizer"),ALr.forEach(t),mGe=r(Q0," or "),N9=n(Q0,"A",{href:!0});var LLr=s(N9);gGe=r(LLr,"BertTokenizerFast"),LLr.forEach(t),hGe=r(Q0," (BERT model)"),Q0.forEach(t),pGe=i(y),ug=n(y,"LI",{});var _2e=s(ug);OQ=n(_2e,"STRONG",{});var BLr=s(OQ);_Ge=r(BLr,"bert-generation"),BLr.forEach(t),uGe=r(_2e," \u2014 "),D9=n(_2e,"A",{href:!0});var kLr=s(D9);bGe=r(kLr,"BertGenerationTokenizer"),kLr.forEach(t),vGe=r(_2e," (Bert Generation model)"),_2e.forEach(t),TGe=i(y),bg=n(y,"LI",{});var u2e=s(bg);XQ=n(u2e,"STRONG",{});var xLr=s(XQ);FGe=r(xLr,"bert-japanese"),xLr.forEach(t),CGe=r(u2e," \u2014 "),q9=n(u2e,"A",{href:!0});var RLr=s(q9);MGe=r(RLr,"BertJapaneseTokenizer"),RLr.forEach(t),EGe=r(u2e," (BertJapanese model)"),u2e.forEach(t),yGe=i(y),vg=n(y,"LI",{});var b2e=s(vg);zQ=n(b2e,"STRONG",{});var SLr=s(zQ);wGe=r(SLr,"bertweet"),SLr.forEach(t),AGe=r(b2e," \u2014 "),G9=n(b2e,"A",{href:!0});var PLr=s(G9);LGe=r(PLr,"BertweetTokenizer"),PLr.forEach(t),BGe=r(b2e," (Bertweet model)"),b2e.forEach(t),kGe=i(y),Xn=n(y,"LI",{});var H0=s(Xn);VQ=n(H0,"STRONG",{});var $Lr=s(VQ);xGe=r($Lr,"big_bird"),$Lr.forEach(t),RGe=r(H0," \u2014 "),O9=n(H0,"A",{href:!0});var ILr=s(O9);SGe=r(ILr,"BigBirdTokenizer"),ILr.forEach(t),PGe=r(H0," or "),X9=n(H0,"A",{href:!0});var jLr=s(X9);$Ge=r(jLr,"BigBirdTokenizerFast"),jLr.forEach(t),IGe=r(H0," (BigBird model)"),H0.forEach(t),jGe=i(y),zn=n(y,"LI",{});var U0=s(zn);WQ=n(U0,"STRONG",{});var NLr=s(WQ);NGe=r(NLr,"bigbird_pegasus"),NLr.forEach(t),DGe=r(U0," \u2014 "),z9=n(U0,"A",{href:!0});var DLr=s(z9);qGe=r(DLr,"PegasusTokenizer"),DLr.forEach(t),GGe=r(U0," or "),V9=n(U0,"A",{href:!0});var qLr=s(V9);OGe=r(qLr,"PegasusTokenizerFast"),qLr.forEach(t),XGe=r(U0," (BigBirdPegasus model)"),U0.forEach(t),zGe=i(y),Vn=n(y,"LI",{});var J0=s(Vn);QQ=n(J0,"STRONG",{});var GLr=s(QQ);VGe=r(GLr,"blenderbot"),GLr.forEach(t),WGe=r(J0," \u2014 "),W9=n(J0,"A",{href:!0});var OLr=s(W9);QGe=r(OLr,"BlenderbotTokenizer"),OLr.forEach(t),HGe=r(J0," or "),Q9=n(J0,"A",{href:!0});var XLr=s(Q9);UGe=r(XLr,"BlenderbotTokenizerFast"),XLr.forEach(t),JGe=r(J0," (Blenderbot model)"),J0.forEach(t),YGe=i(y),Tg=n(y,"LI",{});var v2e=s(Tg);HQ=n(v2e,"STRONG",{});var zLr=s(HQ);KGe=r(zLr,"blenderbot-small"),zLr.forEach(t),ZGe=r(v2e," \u2014 "),H9=n(v2e,"A",{href:!0});var VLr=s(H9);eOe=r(VLr,"BlenderbotSmallTokenizer"),VLr.forEach(t),oOe=r(v2e," (BlenderbotSmall model)"),v2e.forEach(t),rOe=i(y),Fg=n(y,"LI",{});var T2e=s(Fg);UQ=n(T2e,"STRONG",{});var WLr=s(UQ);tOe=r(WLr,"byt5"),WLr.forEach(t),aOe=r(T2e," \u2014 "),U9=n(T2e,"A",{href:!0});var QLr=s(U9);nOe=r(QLr,"ByT5Tokenizer"),QLr.forEach(t),sOe=r(T2e," (ByT5 model)"),T2e.forEach(t),lOe=i(y),Wn=n(y,"LI",{});var Y0=s(Wn);JQ=n(Y0,"STRONG",{});var HLr=s(JQ);iOe=r(HLr,"camembert"),HLr.forEach(t),dOe=r(Y0," \u2014 "),J9=n(Y0,"A",{href:!0});var ULr=s(J9);cOe=r(ULr,"CamembertTokenizer"),ULr.forEach(t),fOe=r(Y0," or "),Y9=n(Y0,"A",{href:!0});var JLr=s(Y9);mOe=r(JLr,"CamembertTokenizerFast"),JLr.forEach(t),gOe=r(Y0," (CamemBERT model)"),Y0.forEach(t),hOe=i(y),Cg=n(y,"LI",{});var F2e=s(Cg);YQ=n(F2e,"STRONG",{});var YLr=s(YQ);pOe=r(YLr,"canine"),YLr.forEach(t),_Oe=r(F2e," \u2014 "),K9=n(F2e,"A",{href:!0});var KLr=s(K9);uOe=r(KLr,"CanineTokenizer"),KLr.forEach(t),bOe=r(F2e," (Canine model)"),F2e.forEach(t),vOe=i(y),Qn=n(y,"LI",{});var K0=s(Qn);KQ=n(K0,"STRONG",{});var ZLr=s(KQ);TOe=r(ZLr,"clip"),ZLr.forEach(t),FOe=r(K0," \u2014 "),Z9=n(K0,"A",{href:!0});var e8r=s(Z9);COe=r(e8r,"CLIPTokenizer"),e8r.forEach(t),MOe=r(K0," or "),eB=n(K0,"A",{href:!0});var o8r=s(eB);EOe=r(o8r,"CLIPTokenizerFast"),o8r.forEach(t),yOe=r(K0," (CLIP model)"),K0.forEach(t),wOe=i(y),Hn=n(y,"LI",{});var Z0=s(Hn);ZQ=n(Z0,"STRONG",{});var r8r=s(ZQ);AOe=r(r8r,"convbert"),r8r.forEach(t),LOe=r(Z0," \u2014 "),oB=n(Z0,"A",{href:!0});var t8r=s(oB);BOe=r(t8r,"ConvBertTokenizer"),t8r.forEach(t),kOe=r(Z0," or "),rB=n(Z0,"A",{href:!0});var a8r=s(rB);xOe=r(a8r,"ConvBertTokenizerFast"),a8r.forEach(t),ROe=r(Z0," (ConvBERT model)"),Z0.forEach(t),SOe=i(y),Un=n(y,"LI",{});var eL=s(Un);eH=n(eL,"STRONG",{});var n8r=s(eH);POe=r(n8r,"cpm"),n8r.forEach(t),$Oe=r(eL," \u2014 "),tB=n(eL,"A",{href:!0});var s8r=s(tB);IOe=r(s8r,"CpmTokenizer"),s8r.forEach(t),jOe=r(eL," or "),oH=n(eL,"CODE",{});var l8r=s(oH);NOe=r(l8r,"CpmTokenizerFast"),l8r.forEach(t),DOe=r(eL," (CPM model)"),eL.forEach(t),qOe=i(y),Mg=n(y,"LI",{});var C2e=s(Mg);rH=n(C2e,"STRONG",{});var i8r=s(rH);GOe=r(i8r,"ctrl"),i8r.forEach(t),OOe=r(C2e," \u2014 "),aB=n(C2e,"A",{href:!0});var d8r=s(aB);XOe=r(d8r,"CTRLTokenizer"),d8r.forEach(t),zOe=r(C2e," (CTRL model)"),C2e.forEach(t),VOe=i(y),Jn=n(y,"LI",{});var oL=s(Jn);tH=n(oL,"STRONG",{});var c8r=s(tH);WOe=r(c8r,"deberta"),c8r.forEach(t),QOe=r(oL," \u2014 "),nB=n(oL,"A",{href:!0});var f8r=s(nB);HOe=r(f8r,"DebertaTokenizer"),f8r.forEach(t),UOe=r(oL," or "),sB=n(oL,"A",{href:!0});var m8r=s(sB);JOe=r(m8r,"DebertaTokenizerFast"),m8r.forEach(t),YOe=r(oL," (DeBERTa model)"),oL.forEach(t),KOe=i(y),Eg=n(y,"LI",{});var M2e=s(Eg);aH=n(M2e,"STRONG",{});var g8r=s(aH);ZOe=r(g8r,"deberta-v2"),g8r.forEach(t),eXe=r(M2e," \u2014 "),lB=n(M2e,"A",{href:!0});var h8r=s(lB);oXe=r(h8r,"DebertaV2Tokenizer"),h8r.forEach(t),rXe=r(M2e," (DeBERTa-v2 model)"),M2e.forEach(t),tXe=i(y),Yn=n(y,"LI",{});var rL=s(Yn);nH=n(rL,"STRONG",{});var p8r=s(nH);aXe=r(p8r,"distilbert"),p8r.forEach(t),nXe=r(rL," \u2014 "),iB=n(rL,"A",{href:!0});var _8r=s(iB);sXe=r(_8r,"DistilBertTokenizer"),_8r.forEach(t),lXe=r(rL," or "),dB=n(rL,"A",{href:!0});var u8r=s(dB);iXe=r(u8r,"DistilBertTokenizerFast"),u8r.forEach(t),dXe=r(rL," (DistilBERT model)"),rL.forEach(t),cXe=i(y),Kn=n(y,"LI",{});var tL=s(Kn);sH=n(tL,"STRONG",{});var b8r=s(sH);fXe=r(b8r,"dpr"),b8r.forEach(t),mXe=r(tL," \u2014 "),cB=n(tL,"A",{href:!0});var v8r=s(cB);gXe=r(v8r,"DPRQuestionEncoderTokenizer"),v8r.forEach(t),hXe=r(tL," or "),fB=n(tL,"A",{href:!0});var T8r=s(fB);pXe=r(T8r,"DPRQuestionEncoderTokenizerFast"),T8r.forEach(t),_Xe=r(tL," (DPR model)"),tL.forEach(t),uXe=i(y),Zn=n(y,"LI",{});var aL=s(Zn);lH=n(aL,"STRONG",{});var F8r=s(lH);bXe=r(F8r,"electra"),F8r.forEach(t),vXe=r(aL," \u2014 "),mB=n(aL,"A",{href:!0});var C8r=s(mB);TXe=r(C8r,"ElectraTokenizer"),C8r.forEach(t),FXe=r(aL," or "),gB=n(aL,"A",{href:!0});var M8r=s(gB);CXe=r(M8r,"ElectraTokenizerFast"),M8r.forEach(t),MXe=r(aL," (ELECTRA model)"),aL.forEach(t),EXe=i(y),yg=n(y,"LI",{});var E2e=s(yg);iH=n(E2e,"STRONG",{});var E8r=s(iH);yXe=r(E8r,"flaubert"),E8r.forEach(t),wXe=r(E2e," \u2014 "),hB=n(E2e,"A",{href:!0});var y8r=s(hB);AXe=r(y8r,"FlaubertTokenizer"),y8r.forEach(t),LXe=r(E2e," (FlauBERT model)"),E2e.forEach(t),BXe=i(y),es=n(y,"LI",{});var nL=s(es);dH=n(nL,"STRONG",{});var w8r=s(dH);kXe=r(w8r,"fnet"),w8r.forEach(t),xXe=r(nL," \u2014 "),pB=n(nL,"A",{href:!0});var A8r=s(pB);RXe=r(A8r,"FNetTokenizer"),A8r.forEach(t),SXe=r(nL," or "),_B=n(nL,"A",{href:!0});var L8r=s(_B);PXe=r(L8r,"FNetTokenizerFast"),L8r.forEach(t),$Xe=r(nL," (FNet model)"),nL.forEach(t),IXe=i(y),wg=n(y,"LI",{});var y2e=s(wg);cH=n(y2e,"STRONG",{});var B8r=s(cH);jXe=r(B8r,"fsmt"),B8r.forEach(t),NXe=r(y2e," \u2014 "),uB=n(y2e,"A",{href:!0});var k8r=s(uB);DXe=r(k8r,"FSMTTokenizer"),k8r.forEach(t),qXe=r(y2e," (FairSeq Machine-Translation model)"),y2e.forEach(t),GXe=i(y),os=n(y,"LI",{});var sL=s(os);fH=n(sL,"STRONG",{});var x8r=s(fH);OXe=r(x8r,"funnel"),x8r.forEach(t),XXe=r(sL," \u2014 "),bB=n(sL,"A",{href:!0});var R8r=s(bB);zXe=r(R8r,"FunnelTokenizer"),R8r.forEach(t),VXe=r(sL," or "),vB=n(sL,"A",{href:!0});var S8r=s(vB);WXe=r(S8r,"FunnelTokenizerFast"),S8r.forEach(t),QXe=r(sL," (Funnel Transformer model)"),sL.forEach(t),HXe=i(y),rs=n(y,"LI",{});var lL=s(rs);mH=n(lL,"STRONG",{});var P8r=s(mH);UXe=r(P8r,"gpt2"),P8r.forEach(t),JXe=r(lL," \u2014 "),TB=n(lL,"A",{href:!0});var $8r=s(TB);YXe=r($8r,"GPT2Tokenizer"),$8r.forEach(t),KXe=r(lL," or "),FB=n(lL,"A",{href:!0});var I8r=s(FB);ZXe=r(I8r,"GPT2TokenizerFast"),I8r.forEach(t),eze=r(lL," (OpenAI GPT-2 model)"),lL.forEach(t),oze=i(y),ts=n(y,"LI",{});var iL=s(ts);gH=n(iL,"STRONG",{});var j8r=s(gH);rze=r(j8r,"gpt_neo"),j8r.forEach(t),tze=r(iL," \u2014 "),CB=n(iL,"A",{href:!0});var N8r=s(CB);aze=r(N8r,"GPT2Tokenizer"),N8r.forEach(t),nze=r(iL," or "),MB=n(iL,"A",{href:!0});var D8r=s(MB);sze=r(D8r,"GPT2TokenizerFast"),D8r.forEach(t),lze=r(iL," (GPT Neo model)"),iL.forEach(t),ize=i(y),as=n(y,"LI",{});var dL=s(as);hH=n(dL,"STRONG",{});var q8r=s(hH);dze=r(q8r,"herbert"),q8r.forEach(t),cze=r(dL," \u2014 "),EB=n(dL,"A",{href:!0});var G8r=s(EB);fze=r(G8r,"HerbertTokenizer"),G8r.forEach(t),mze=r(dL," or "),yB=n(dL,"A",{href:!0});var O8r=s(yB);gze=r(O8r,"HerbertTokenizerFast"),O8r.forEach(t),hze=r(dL," (HerBERT model)"),dL.forEach(t),pze=i(y),Ag=n(y,"LI",{});var w2e=s(Ag);pH=n(w2e,"STRONG",{});var X8r=s(pH);_ze=r(X8r,"hubert"),X8r.forEach(t),uze=r(w2e," \u2014 "),wB=n(w2e,"A",{href:!0});var z8r=s(wB);bze=r(z8r,"Wav2Vec2CTCTokenizer"),z8r.forEach(t),vze=r(w2e," (Hubert model)"),w2e.forEach(t),Tze=i(y),ns=n(y,"LI",{});var cL=s(ns);_H=n(cL,"STRONG",{});var V8r=s(_H);Fze=r(V8r,"ibert"),V8r.forEach(t),Cze=r(cL," \u2014 "),AB=n(cL,"A",{href:!0});var W8r=s(AB);Mze=r(W8r,"RobertaTokenizer"),W8r.forEach(t),Eze=r(cL," or "),LB=n(cL,"A",{href:!0});var Q8r=s(LB);yze=r(Q8r,"RobertaTokenizerFast"),Q8r.forEach(t),wze=r(cL," (I-BERT model)"),cL.forEach(t),Aze=i(y),ss=n(y,"LI",{});var fL=s(ss);uH=n(fL,"STRONG",{});var H8r=s(uH);Lze=r(H8r,"layoutlm"),H8r.forEach(t),Bze=r(fL," \u2014 "),BB=n(fL,"A",{href:!0});var U8r=s(BB);kze=r(U8r,"LayoutLMTokenizer"),U8r.forEach(t),xze=r(fL," or "),kB=n(fL,"A",{href:!0});var J8r=s(kB);Rze=r(J8r,"LayoutLMTokenizerFast"),J8r.forEach(t),Sze=r(fL," (LayoutLM model)"),fL.forEach(t),Pze=i(y),ls=n(y,"LI",{});var mL=s(ls);bH=n(mL,"STRONG",{});var Y8r=s(bH);$ze=r(Y8r,"layoutlmv2"),Y8r.forEach(t),Ize=r(mL," \u2014 "),xB=n(mL,"A",{href:!0});var K8r=s(xB);jze=r(K8r,"LayoutLMv2Tokenizer"),K8r.forEach(t),Nze=r(mL," or "),RB=n(mL,"A",{href:!0});var Z8r=s(RB);Dze=r(Z8r,"LayoutLMv2TokenizerFast"),Z8r.forEach(t),qze=r(mL," (LayoutLMv2 model)"),mL.forEach(t),Gze=i(y),is=n(y,"LI",{});var gL=s(is);vH=n(gL,"STRONG",{});var e9r=s(vH);Oze=r(e9r,"layoutxlm"),e9r.forEach(t),Xze=r(gL," \u2014 "),SB=n(gL,"A",{href:!0});var o9r=s(SB);zze=r(o9r,"LayoutXLMTokenizer"),o9r.forEach(t),Vze=r(gL," or "),PB=n(gL,"A",{href:!0});var r9r=s(PB);Wze=r(r9r,"LayoutXLMTokenizerFast"),r9r.forEach(t),Qze=r(gL," (LayoutXLM model)"),gL.forEach(t),Hze=i(y),ds=n(y,"LI",{});var hL=s(ds);TH=n(hL,"STRONG",{});var t9r=s(TH);Uze=r(t9r,"led"),t9r.forEach(t),Jze=r(hL," \u2014 "),$B=n(hL,"A",{href:!0});var a9r=s($B);Yze=r(a9r,"LEDTokenizer"),a9r.forEach(t),Kze=r(hL," or "),IB=n(hL,"A",{href:!0});var n9r=s(IB);Zze=r(n9r,"LEDTokenizerFast"),n9r.forEach(t),eVe=r(hL," (LED model)"),hL.forEach(t),oVe=i(y),cs=n(y,"LI",{});var pL=s(cs);FH=n(pL,"STRONG",{});var s9r=s(FH);rVe=r(s9r,"longformer"),s9r.forEach(t),tVe=r(pL," \u2014 "),jB=n(pL,"A",{href:!0});var l9r=s(jB);aVe=r(l9r,"LongformerTokenizer"),l9r.forEach(t),nVe=r(pL," or "),NB=n(pL,"A",{href:!0});var i9r=s(NB);sVe=r(i9r,"LongformerTokenizerFast"),i9r.forEach(t),lVe=r(pL," (Longformer model)"),pL.forEach(t),iVe=i(y),Lg=n(y,"LI",{});var A2e=s(Lg);CH=n(A2e,"STRONG",{});var d9r=s(CH);dVe=r(d9r,"luke"),d9r.forEach(t),cVe=r(A2e," \u2014 "),DB=n(A2e,"A",{href:!0});var c9r=s(DB);fVe=r(c9r,"LukeTokenizer"),c9r.forEach(t),mVe=r(A2e," (LUKE model)"),A2e.forEach(t),gVe=i(y),fs=n(y,"LI",{});var _L=s(fs);MH=n(_L,"STRONG",{});var f9r=s(MH);hVe=r(f9r,"lxmert"),f9r.forEach(t),pVe=r(_L," \u2014 "),qB=n(_L,"A",{href:!0});var m9r=s(qB);_Ve=r(m9r,"LxmertTokenizer"),m9r.forEach(t),uVe=r(_L," or "),GB=n(_L,"A",{href:!0});var g9r=s(GB);bVe=r(g9r,"LxmertTokenizerFast"),g9r.forEach(t),vVe=r(_L," (LXMERT model)"),_L.forEach(t),TVe=i(y),Bg=n(y,"LI",{});var L2e=s(Bg);EH=n(L2e,"STRONG",{});var h9r=s(EH);FVe=r(h9r,"m2m_100"),h9r.forEach(t),CVe=r(L2e," \u2014 "),OB=n(L2e,"A",{href:!0});var p9r=s(OB);MVe=r(p9r,"M2M100Tokenizer"),p9r.forEach(t),EVe=r(L2e," (M2M100 model)"),L2e.forEach(t),yVe=i(y),kg=n(y,"LI",{});var B2e=s(kg);yH=n(B2e,"STRONG",{});var _9r=s(yH);wVe=r(_9r,"marian"),_9r.forEach(t),AVe=r(B2e," \u2014 "),XB=n(B2e,"A",{href:!0});var u9r=s(XB);LVe=r(u9r,"MarianTokenizer"),u9r.forEach(t),BVe=r(B2e," (Marian model)"),B2e.forEach(t),kVe=i(y),ms=n(y,"LI",{});var uL=s(ms);wH=n(uL,"STRONG",{});var b9r=s(wH);xVe=r(b9r,"mbart"),b9r.forEach(t),RVe=r(uL," \u2014 "),zB=n(uL,"A",{href:!0});var v9r=s(zB);SVe=r(v9r,"MBartTokenizer"),v9r.forEach(t),PVe=r(uL," or "),VB=n(uL,"A",{href:!0});var T9r=s(VB);$Ve=r(T9r,"MBartTokenizerFast"),T9r.forEach(t),IVe=r(uL," (mBART model)"),uL.forEach(t),jVe=i(y),gs=n(y,"LI",{});var bL=s(gs);AH=n(bL,"STRONG",{});var F9r=s(AH);NVe=r(F9r,"mbart50"),F9r.forEach(t),DVe=r(bL," \u2014 "),WB=n(bL,"A",{href:!0});var C9r=s(WB);qVe=r(C9r,"MBart50Tokenizer"),C9r.forEach(t),GVe=r(bL," or "),QB=n(bL,"A",{href:!0});var M9r=s(QB);OVe=r(M9r,"MBart50TokenizerFast"),M9r.forEach(t),XVe=r(bL," (mBART-50 model)"),bL.forEach(t),zVe=i(y),xg=n(y,"LI",{});var k2e=s(xg);LH=n(k2e,"STRONG",{});var E9r=s(LH);VVe=r(E9r,"mluke"),E9r.forEach(t),WVe=r(k2e," \u2014 "),HB=n(k2e,"A",{href:!0});var y9r=s(HB);QVe=r(y9r,"MLukeTokenizer"),y9r.forEach(t),HVe=r(k2e," (mLUKE model)"),k2e.forEach(t),UVe=i(y),hs=n(y,"LI",{});var vL=s(hs);BH=n(vL,"STRONG",{});var w9r=s(BH);JVe=r(w9r,"mobilebert"),w9r.forEach(t),YVe=r(vL," \u2014 "),UB=n(vL,"A",{href:!0});var A9r=s(UB);KVe=r(A9r,"MobileBertTokenizer"),A9r.forEach(t),ZVe=r(vL," or "),JB=n(vL,"A",{href:!0});var L9r=s(JB);eWe=r(L9r,"MobileBertTokenizerFast"),L9r.forEach(t),oWe=r(vL," (MobileBERT model)"),vL.forEach(t),rWe=i(y),ps=n(y,"LI",{});var TL=s(ps);kH=n(TL,"STRONG",{});var B9r=s(kH);tWe=r(B9r,"mpnet"),B9r.forEach(t),aWe=r(TL," \u2014 "),YB=n(TL,"A",{href:!0});var k9r=s(YB);nWe=r(k9r,"MPNetTokenizer"),k9r.forEach(t),sWe=r(TL," or "),KB=n(TL,"A",{href:!0});var x9r=s(KB);lWe=r(x9r,"MPNetTokenizerFast"),x9r.forEach(t),iWe=r(TL," (MPNet model)"),TL.forEach(t),dWe=i(y),_s=n(y,"LI",{});var FL=s(_s);xH=n(FL,"STRONG",{});var R9r=s(xH);cWe=r(R9r,"mt5"),R9r.forEach(t),fWe=r(FL," \u2014 "),ZB=n(FL,"A",{href:!0});var S9r=s(ZB);mWe=r(S9r,"MT5Tokenizer"),S9r.forEach(t),gWe=r(FL," or "),ek=n(FL,"A",{href:!0});var P9r=s(ek);hWe=r(P9r,"MT5TokenizerFast"),P9r.forEach(t),pWe=r(FL," (mT5 model)"),FL.forEach(t),_We=i(y),us=n(y,"LI",{});var CL=s(us);RH=n(CL,"STRONG",{});var $9r=s(RH);uWe=r($9r,"openai-gpt"),$9r.forEach(t),bWe=r(CL," \u2014 "),ok=n(CL,"A",{href:!0});var I9r=s(ok);vWe=r(I9r,"OpenAIGPTTokenizer"),I9r.forEach(t),TWe=r(CL," or "),rk=n(CL,"A",{href:!0});var j9r=s(rk);FWe=r(j9r,"OpenAIGPTTokenizerFast"),j9r.forEach(t),CWe=r(CL," (OpenAI GPT model)"),CL.forEach(t),MWe=i(y),bs=n(y,"LI",{});var ML=s(bs);SH=n(ML,"STRONG",{});var N9r=s(SH);EWe=r(N9r,"pegasus"),N9r.forEach(t),yWe=r(ML," \u2014 "),tk=n(ML,"A",{href:!0});var D9r=s(tk);wWe=r(D9r,"PegasusTokenizer"),D9r.forEach(t),AWe=r(ML," or "),ak=n(ML,"A",{href:!0});var q9r=s(ak);LWe=r(q9r,"PegasusTokenizerFast"),q9r.forEach(t),BWe=r(ML," (Pegasus model)"),ML.forEach(t),kWe=i(y),Rg=n(y,"LI",{});var x2e=s(Rg);PH=n(x2e,"STRONG",{});var G9r=s(PH);xWe=r(G9r,"perceiver"),G9r.forEach(t),RWe=r(x2e," \u2014 "),nk=n(x2e,"A",{href:!0});var O9r=s(nk);SWe=r(O9r,"PerceiverTokenizer"),O9r.forEach(t),PWe=r(x2e," (Perceiver model)"),x2e.forEach(t),$We=i(y),Sg=n(y,"LI",{});var R2e=s(Sg);$H=n(R2e,"STRONG",{});var X9r=s($H);IWe=r(X9r,"phobert"),X9r.forEach(t),jWe=r(R2e," \u2014 "),sk=n(R2e,"A",{href:!0});var z9r=s(sk);NWe=r(z9r,"PhobertTokenizer"),z9r.forEach(t),DWe=r(R2e," (PhoBERT model)"),R2e.forEach(t),qWe=i(y),Pg=n(y,"LI",{});var S2e=s(Pg);IH=n(S2e,"STRONG",{});var V9r=s(IH);GWe=r(V9r,"plbart"),V9r.forEach(t),OWe=r(S2e," \u2014 "),lk=n(S2e,"A",{href:!0});var W9r=s(lk);XWe=r(W9r,"PLBartTokenizer"),W9r.forEach(t),zWe=r(S2e," (PLBart model)"),S2e.forEach(t),VWe=i(y),$g=n(y,"LI",{});var P2e=s($g);jH=n(P2e,"STRONG",{});var Q9r=s(jH);WWe=r(Q9r,"prophetnet"),Q9r.forEach(t),QWe=r(P2e," \u2014 "),ik=n(P2e,"A",{href:!0});var H9r=s(ik);HWe=r(H9r,"ProphetNetTokenizer"),H9r.forEach(t),UWe=r(P2e," (ProphetNet model)"),P2e.forEach(t),JWe=i(y),vs=n(y,"LI",{});var EL=s(vs);NH=n(EL,"STRONG",{});var U9r=s(NH);YWe=r(U9r,"qdqbert"),U9r.forEach(t),KWe=r(EL," \u2014 "),dk=n(EL,"A",{href:!0});var J9r=s(dk);ZWe=r(J9r,"BertTokenizer"),J9r.forEach(t),eQe=r(EL," or "),ck=n(EL,"A",{href:!0});var Y9r=s(ck);oQe=r(Y9r,"BertTokenizerFast"),Y9r.forEach(t),rQe=r(EL," (QDQBert model)"),EL.forEach(t),tQe=i(y),Ig=n(y,"LI",{});var $2e=s(Ig);DH=n($2e,"STRONG",{});var K9r=s(DH);aQe=r(K9r,"rag"),K9r.forEach(t),nQe=r($2e," \u2014 "),fk=n($2e,"A",{href:!0});var Z9r=s(fk);sQe=r(Z9r,"RagTokenizer"),Z9r.forEach(t),lQe=r($2e," (RAG model)"),$2e.forEach(t),iQe=i(y),Ts=n(y,"LI",{});var yL=s(Ts);qH=n(yL,"STRONG",{});var eBr=s(qH);dQe=r(eBr,"reformer"),eBr.forEach(t),cQe=r(yL," \u2014 "),mk=n(yL,"A",{href:!0});var oBr=s(mk);fQe=r(oBr,"ReformerTokenizer"),oBr.forEach(t),mQe=r(yL," or "),gk=n(yL,"A",{href:!0});var rBr=s(gk);gQe=r(rBr,"ReformerTokenizerFast"),rBr.forEach(t),hQe=r(yL," (Reformer model)"),yL.forEach(t),pQe=i(y),Fs=n(y,"LI",{});var wL=s(Fs);GH=n(wL,"STRONG",{});var tBr=s(GH);_Qe=r(tBr,"rembert"),tBr.forEach(t),uQe=r(wL," \u2014 "),hk=n(wL,"A",{href:!0});var aBr=s(hk);bQe=r(aBr,"RemBertTokenizer"),aBr.forEach(t),vQe=r(wL," or "),pk=n(wL,"A",{href:!0});var nBr=s(pk);TQe=r(nBr,"RemBertTokenizerFast"),nBr.forEach(t),FQe=r(wL," (RemBERT model)"),wL.forEach(t),CQe=i(y),Cs=n(y,"LI",{});var AL=s(Cs);OH=n(AL,"STRONG",{});var sBr=s(OH);MQe=r(sBr,"retribert"),sBr.forEach(t),EQe=r(AL," \u2014 "),_k=n(AL,"A",{href:!0});var lBr=s(_k);yQe=r(lBr,"RetriBertTokenizer"),lBr.forEach(t),wQe=r(AL," or "),uk=n(AL,"A",{href:!0});var iBr=s(uk);AQe=r(iBr,"RetriBertTokenizerFast"),iBr.forEach(t),LQe=r(AL," (RetriBERT model)"),AL.forEach(t),BQe=i(y),Ms=n(y,"LI",{});var LL=s(Ms);XH=n(LL,"STRONG",{});var dBr=s(XH);kQe=r(dBr,"roberta"),dBr.forEach(t),xQe=r(LL," \u2014 "),bk=n(LL,"A",{href:!0});var cBr=s(bk);RQe=r(cBr,"RobertaTokenizer"),cBr.forEach(t),SQe=r(LL," or "),vk=n(LL,"A",{href:!0});var fBr=s(vk);PQe=r(fBr,"RobertaTokenizerFast"),fBr.forEach(t),$Qe=r(LL," (RoBERTa model)"),LL.forEach(t),IQe=i(y),Es=n(y,"LI",{});var BL=s(Es);zH=n(BL,"STRONG",{});var mBr=s(zH);jQe=r(mBr,"roformer"),mBr.forEach(t),NQe=r(BL," \u2014 "),Tk=n(BL,"A",{href:!0});var gBr=s(Tk);DQe=r(gBr,"RoFormerTokenizer"),gBr.forEach(t),qQe=r(BL," or "),Fk=n(BL,"A",{href:!0});var hBr=s(Fk);GQe=r(hBr,"RoFormerTokenizerFast"),hBr.forEach(t),OQe=r(BL," (RoFormer model)"),BL.forEach(t),XQe=i(y),jg=n(y,"LI",{});var I2e=s(jg);VH=n(I2e,"STRONG",{});var pBr=s(VH);zQe=r(pBr,"speech_to_text"),pBr.forEach(t),VQe=r(I2e," \u2014 "),Ck=n(I2e,"A",{href:!0});var _Br=s(Ck);WQe=r(_Br,"Speech2TextTokenizer"),_Br.forEach(t),QQe=r(I2e," (Speech2Text model)"),I2e.forEach(t),HQe=i(y),Ng=n(y,"LI",{});var j2e=s(Ng);WH=n(j2e,"STRONG",{});var uBr=s(WH);UQe=r(uBr,"speech_to_text_2"),uBr.forEach(t),JQe=r(j2e," \u2014 "),Mk=n(j2e,"A",{href:!0});var bBr=s(Mk);YQe=r(bBr,"Speech2Text2Tokenizer"),bBr.forEach(t),KQe=r(j2e," (Speech2Text2 model)"),j2e.forEach(t),ZQe=i(y),ys=n(y,"LI",{});var kL=s(ys);QH=n(kL,"STRONG",{});var vBr=s(QH);eHe=r(vBr,"splinter"),vBr.forEach(t),oHe=r(kL," \u2014 "),Ek=n(kL,"A",{href:!0});var TBr=s(Ek);rHe=r(TBr,"SplinterTokenizer"),TBr.forEach(t),tHe=r(kL," or "),yk=n(kL,"A",{href:!0});var FBr=s(yk);aHe=r(FBr,"SplinterTokenizerFast"),FBr.forEach(t),nHe=r(kL," (Splinter model)"),kL.forEach(t),sHe=i(y),ws=n(y,"LI",{});var xL=s(ws);HH=n(xL,"STRONG",{});var CBr=s(HH);lHe=r(CBr,"squeezebert"),CBr.forEach(t),iHe=r(xL," \u2014 "),wk=n(xL,"A",{href:!0});var MBr=s(wk);dHe=r(MBr,"SqueezeBertTokenizer"),MBr.forEach(t),cHe=r(xL," or "),Ak=n(xL,"A",{href:!0});var EBr=s(Ak);fHe=r(EBr,"SqueezeBertTokenizerFast"),EBr.forEach(t),mHe=r(xL," (SqueezeBERT model)"),xL.forEach(t),gHe=i(y),As=n(y,"LI",{});var RL=s(As);UH=n(RL,"STRONG",{});var yBr=s(UH);hHe=r(yBr,"t5"),yBr.forEach(t),pHe=r(RL," \u2014 "),Lk=n(RL,"A",{href:!0});var wBr=s(Lk);_He=r(wBr,"T5Tokenizer"),wBr.forEach(t),uHe=r(RL," or "),Bk=n(RL,"A",{href:!0});var ABr=s(Bk);bHe=r(ABr,"T5TokenizerFast"),ABr.forEach(t),vHe=r(RL," (T5 model)"),RL.forEach(t),THe=i(y),Dg=n(y,"LI",{});var N2e=s(Dg);JH=n(N2e,"STRONG",{});var LBr=s(JH);FHe=r(LBr,"tapas"),LBr.forEach(t),CHe=r(N2e," \u2014 "),kk=n(N2e,"A",{href:!0});var BBr=s(kk);MHe=r(BBr,"TapasTokenizer"),BBr.forEach(t),EHe=r(N2e," (TAPAS model)"),N2e.forEach(t),yHe=i(y),qg=n(y,"LI",{});var D2e=s(qg);YH=n(D2e,"STRONG",{});var kBr=s(YH);wHe=r(kBr,"transfo-xl"),kBr.forEach(t),AHe=r(D2e," \u2014 "),xk=n(D2e,"A",{href:!0});var xBr=s(xk);LHe=r(xBr,"TransfoXLTokenizer"),xBr.forEach(t),BHe=r(D2e," (Transformer-XL model)"),D2e.forEach(t),kHe=i(y),Gg=n(y,"LI",{});var q2e=s(Gg);KH=n(q2e,"STRONG",{});var RBr=s(KH);xHe=r(RBr,"wav2vec2"),RBr.forEach(t),RHe=r(q2e," \u2014 "),Rk=n(q2e,"A",{href:!0});var SBr=s(Rk);SHe=r(SBr,"Wav2Vec2CTCTokenizer"),SBr.forEach(t),PHe=r(q2e," (Wav2Vec2 model)"),q2e.forEach(t),$He=i(y),Og=n(y,"LI",{});var G2e=s(Og);ZH=n(G2e,"STRONG",{});var PBr=s(ZH);IHe=r(PBr,"wav2vec2_phoneme"),PBr.forEach(t),jHe=r(G2e," \u2014 "),Sk=n(G2e,"A",{href:!0});var $Br=s(Sk);NHe=r($Br,"Wav2Vec2PhonemeCTCTokenizer"),$Br.forEach(t),DHe=r(G2e," (Wav2Vec2Phoneme model)"),G2e.forEach(t),qHe=i(y),Ls=n(y,"LI",{});var SL=s(Ls);eU=n(SL,"STRONG",{});var IBr=s(eU);GHe=r(IBr,"xglm"),IBr.forEach(t),OHe=r(SL," \u2014 "),Pk=n(SL,"A",{href:!0});var jBr=s(Pk);XHe=r(jBr,"XGLMTokenizer"),jBr.forEach(t),zHe=r(SL," or "),$k=n(SL,"A",{href:!0});var NBr=s($k);VHe=r(NBr,"XGLMTokenizerFast"),NBr.forEach(t),WHe=r(SL," (XGLM model)"),SL.forEach(t),QHe=i(y),Xg=n(y,"LI",{});var O2e=s(Xg);oU=n(O2e,"STRONG",{});var DBr=s(oU);HHe=r(DBr,"xlm"),DBr.forEach(t),UHe=r(O2e," \u2014 "),Ik=n(O2e,"A",{href:!0});var qBr=s(Ik);JHe=r(qBr,"XLMTokenizer"),qBr.forEach(t),YHe=r(O2e," (XLM model)"),O2e.forEach(t),KHe=i(y),zg=n(y,"LI",{});var X2e=s(zg);rU=n(X2e,"STRONG",{});var GBr=s(rU);ZHe=r(GBr,"xlm-prophetnet"),GBr.forEach(t),eUe=r(X2e," \u2014 "),jk=n(X2e,"A",{href:!0});var OBr=s(jk);oUe=r(OBr,"XLMProphetNetTokenizer"),OBr.forEach(t),rUe=r(X2e," (XLMProphetNet model)"),X2e.forEach(t),tUe=i(y),Bs=n(y,"LI",{});var PL=s(Bs);tU=n(PL,"STRONG",{});var XBr=s(tU);aUe=r(XBr,"xlm-roberta"),XBr.forEach(t),nUe=r(PL," \u2014 "),Nk=n(PL,"A",{href:!0});var zBr=s(Nk);sUe=r(zBr,"XLMRobertaTokenizer"),zBr.forEach(t),lUe=r(PL," or "),Dk=n(PL,"A",{href:!0});var VBr=s(Dk);iUe=r(VBr,"XLMRobertaTokenizerFast"),VBr.forEach(t),dUe=r(PL," (XLM-RoBERTa model)"),PL.forEach(t),cUe=i(y),ks=n(y,"LI",{});var $L=s(ks);aU=n($L,"STRONG",{});var WBr=s(aU);fUe=r(WBr,"xlnet"),WBr.forEach(t),mUe=r($L," \u2014 "),qk=n($L,"A",{href:!0});var QBr=s(qk);gUe=r(QBr,"XLNetTokenizer"),QBr.forEach(t),hUe=r($L," or "),Gk=n($L,"A",{href:!0});var HBr=s(Gk);pUe=r(HBr,"XLNetTokenizerFast"),HBr.forEach(t),_Ue=r($L," (XLNet model)"),$L.forEach(t),y.forEach(t),uUe=i(da),nU=n(da,"P",{});var UBr=s(nU);bUe=r(UBr,"Examples:"),UBr.forEach(t),vUe=i(da),m(vE.$$.fragment,da),da.forEach(t),TUe=i($s),Vg=n($s,"DIV",{class:!0});var hBe=s(Vg);m(TE.$$.fragment,hBe),FUe=i(hBe),sU=n(hBe,"P",{});var JBr=s(sU);CUe=r(JBr,"Register a new tokenizer in this mapping."),JBr.forEach(t),hBe.forEach(t),$s.forEach(t),m8e=i(d),ji=n(d,"H2",{class:!0});var pBe=s(ji);Wg=n(pBe,"A",{id:!0,class:!0,href:!0});var YBr=s(Wg);lU=n(YBr,"SPAN",{});var KBr=s(lU);m(FE.$$.fragment,KBr),KBr.forEach(t),YBr.forEach(t),MUe=i(pBe),iU=n(pBe,"SPAN",{});var ZBr=s(iU);EUe=r(ZBr,"AutoFeatureExtractor"),ZBr.forEach(t),pBe.forEach(t),g8e=i(d),Xo=n(d,"DIV",{class:!0});var Is=s(Xo);m(CE.$$.fragment,Is),yUe=i(Is),ME=n(Is,"P",{});var _Be=s(ME);wUe=r(_Be,`This is a generic feature extractor class that will be instantiated as one of the feature extractor classes of the
library when created with the `),Ok=n(_Be,"A",{href:!0});var ekr=s(Ok);AUe=r(ekr,"AutoFeatureExtractor.from_pretrained()"),ekr.forEach(t),LUe=r(_Be," class method."),_Be.forEach(t),BUe=i(Is),EE=n(Is,"P",{});var uBe=s(EE);kUe=r(uBe,"This class cannot be instantiated directly using "),dU=n(uBe,"CODE",{});var okr=s(dU);xUe=r(okr,"__init__()"),okr.forEach(t),RUe=r(uBe," (throws an error)."),uBe.forEach(t),SUe=i(Is),Le=n(Is,"DIV",{class:!0});var xt=s(Le);m(yE.$$.fragment,xt),PUe=i(xt),cU=n(xt,"P",{});var rkr=s(cU);$Ue=r(rkr,"Instantiate one of the feature extractor classes of the library from a pretrained model vocabulary."),rkr.forEach(t),IUe=i(xt),Na=n(xt,"P",{});var sM=s(Na);jUe=r(sM,"The feature extractor class to instantiate is selected based on the "),fU=n(sM,"CODE",{});var tkr=s(fU);NUe=r(tkr,"model_type"),tkr.forEach(t),DUe=r(sM,` property of the config object
(either passed as an argument or loaded from `),mU=n(sM,"CODE",{});var akr=s(mU);qUe=r(akr,"pretrained_model_name_or_path"),akr.forEach(t),GUe=r(sM,` if possible), or when it\u2019s
missing, by falling back to using pattern matching on `),gU=n(sM,"CODE",{});var nkr=s(gU);OUe=r(nkr,"pretrained_model_name_or_path"),nkr.forEach(t),XUe=r(sM,":"),sM.forEach(t),zUe=i(xt),se=n(xt,"UL",{});var de=s(se);Qg=n(de,"LI",{});var z2e=s(Qg);hU=n(z2e,"STRONG",{});var skr=s(hU);VUe=r(skr,"beit"),skr.forEach(t),WUe=r(z2e," \u2014 "),Xk=n(z2e,"A",{href:!0});var lkr=s(Xk);QUe=r(lkr,"BeitFeatureExtractor"),lkr.forEach(t),HUe=r(z2e," (BEiT model)"),z2e.forEach(t),UUe=i(de),Hg=n(de,"LI",{});var V2e=s(Hg);pU=n(V2e,"STRONG",{});var ikr=s(pU);JUe=r(ikr,"clip"),ikr.forEach(t),YUe=r(V2e," \u2014 "),zk=n(V2e,"A",{href:!0});var dkr=s(zk);KUe=r(dkr,"CLIPFeatureExtractor"),dkr.forEach(t),ZUe=r(V2e," (CLIP model)"),V2e.forEach(t),eJe=i(de),Ug=n(de,"LI",{});var W2e=s(Ug);_U=n(W2e,"STRONG",{});var ckr=s(_U);oJe=r(ckr,"convnext"),ckr.forEach(t),rJe=r(W2e," \u2014 "),Vk=n(W2e,"A",{href:!0});var fkr=s(Vk);tJe=r(fkr,"ConvNextFeatureExtractor"),fkr.forEach(t),aJe=r(W2e," (ConvNext model)"),W2e.forEach(t),nJe=i(de),Jg=n(de,"LI",{});var Q2e=s(Jg);uU=n(Q2e,"STRONG",{});var mkr=s(uU);sJe=r(mkr,"deit"),mkr.forEach(t),lJe=r(Q2e," \u2014 "),Wk=n(Q2e,"A",{href:!0});var gkr=s(Wk);iJe=r(gkr,"DeiTFeatureExtractor"),gkr.forEach(t),dJe=r(Q2e," (DeiT model)"),Q2e.forEach(t),cJe=i(de),Yg=n(de,"LI",{});var H2e=s(Yg);bU=n(H2e,"STRONG",{});var hkr=s(bU);fJe=r(hkr,"detr"),hkr.forEach(t),mJe=r(H2e," \u2014 "),Qk=n(H2e,"A",{href:!0});var pkr=s(Qk);gJe=r(pkr,"DetrFeatureExtractor"),pkr.forEach(t),hJe=r(H2e," (DETR model)"),H2e.forEach(t),pJe=i(de),Kg=n(de,"LI",{});var U2e=s(Kg);vU=n(U2e,"STRONG",{});var _kr=s(vU);_Je=r(_kr,"hubert"),_kr.forEach(t),uJe=r(U2e," \u2014 "),Hk=n(U2e,"A",{href:!0});var ukr=s(Hk);bJe=r(ukr,"Wav2Vec2FeatureExtractor"),ukr.forEach(t),vJe=r(U2e," (Hubert model)"),U2e.forEach(t),TJe=i(de),Zg=n(de,"LI",{});var J2e=s(Zg);TU=n(J2e,"STRONG",{});var bkr=s(TU);FJe=r(bkr,"layoutlmv2"),bkr.forEach(t),CJe=r(J2e," \u2014 "),Uk=n(J2e,"A",{href:!0});var vkr=s(Uk);MJe=r(vkr,"LayoutLMv2FeatureExtractor"),vkr.forEach(t),EJe=r(J2e," (LayoutLMv2 model)"),J2e.forEach(t),yJe=i(de),eh=n(de,"LI",{});var Y2e=s(eh);FU=n(Y2e,"STRONG",{});var Tkr=s(FU);wJe=r(Tkr,"perceiver"),Tkr.forEach(t),AJe=r(Y2e," \u2014 "),Jk=n(Y2e,"A",{href:!0});var Fkr=s(Jk);LJe=r(Fkr,"PerceiverFeatureExtractor"),Fkr.forEach(t),BJe=r(Y2e," (Perceiver model)"),Y2e.forEach(t),kJe=i(de),oh=n(de,"LI",{});var K2e=s(oh);CU=n(K2e,"STRONG",{});var Ckr=s(CU);xJe=r(Ckr,"poolformer"),Ckr.forEach(t),RJe=r(K2e," \u2014 "),Yk=n(K2e,"A",{href:!0});var Mkr=s(Yk);SJe=r(Mkr,"PoolFormerFeatureExtractor"),Mkr.forEach(t),PJe=r(K2e," (PoolFormer model)"),K2e.forEach(t),$Je=i(de),rh=n(de,"LI",{});var Z2e=s(rh);MU=n(Z2e,"STRONG",{});var Ekr=s(MU);IJe=r(Ekr,"segformer"),Ekr.forEach(t),jJe=r(Z2e," \u2014 "),Kk=n(Z2e,"A",{href:!0});var ykr=s(Kk);NJe=r(ykr,"SegformerFeatureExtractor"),ykr.forEach(t),DJe=r(Z2e," (SegFormer model)"),Z2e.forEach(t),qJe=i(de),th=n(de,"LI",{});var eve=s(th);EU=n(eve,"STRONG",{});var wkr=s(EU);GJe=r(wkr,"speech_to_text"),wkr.forEach(t),OJe=r(eve," \u2014 "),Zk=n(eve,"A",{href:!0});var Akr=s(Zk);XJe=r(Akr,"Speech2TextFeatureExtractor"),Akr.forEach(t),zJe=r(eve," (Speech2Text model)"),eve.forEach(t),VJe=i(de),ah=n(de,"LI",{});var ove=s(ah);yU=n(ove,"STRONG",{});var Lkr=s(yU);WJe=r(Lkr,"swin"),Lkr.forEach(t),QJe=r(ove," \u2014 "),ex=n(ove,"A",{href:!0});var Bkr=s(ex);HJe=r(Bkr,"ViTFeatureExtractor"),Bkr.forEach(t),UJe=r(ove," (Swin model)"),ove.forEach(t),JJe=i(de),nh=n(de,"LI",{});var rve=s(nh);wU=n(rve,"STRONG",{});var kkr=s(wU);YJe=r(kkr,"vit"),kkr.forEach(t),KJe=r(rve," \u2014 "),ox=n(rve,"A",{href:!0});var xkr=s(ox);ZJe=r(xkr,"ViTFeatureExtractor"),xkr.forEach(t),eYe=r(rve," (ViT model)"),rve.forEach(t),oYe=i(de),sh=n(de,"LI",{});var tve=s(sh);AU=n(tve,"STRONG",{});var Rkr=s(AU);rYe=r(Rkr,"vit_mae"),Rkr.forEach(t),tYe=r(tve," \u2014 "),rx=n(tve,"A",{href:!0});var Skr=s(rx);aYe=r(Skr,"ViTFeatureExtractor"),Skr.forEach(t),nYe=r(tve," (ViTMAE model)"),tve.forEach(t),sYe=i(de),lh=n(de,"LI",{});var ave=s(lh);LU=n(ave,"STRONG",{});var Pkr=s(LU);lYe=r(Pkr,"wav2vec2"),Pkr.forEach(t),iYe=r(ave," \u2014 "),tx=n(ave,"A",{href:!0});var $kr=s(tx);dYe=r($kr,"Wav2Vec2FeatureExtractor"),$kr.forEach(t),cYe=r(ave," (Wav2Vec2 model)"),ave.forEach(t),de.forEach(t),fYe=i(xt),m(ih.$$.fragment,xt),mYe=i(xt),BU=n(xt,"P",{});var Ikr=s(BU);gYe=r(Ikr,"Examples:"),Ikr.forEach(t),hYe=i(xt),m(wE.$$.fragment,xt),xt.forEach(t),pYe=i(Is),dh=n(Is,"DIV",{class:!0});var bBe=s(dh);m(AE.$$.fragment,bBe),_Ye=i(bBe),kU=n(bBe,"P",{});var jkr=s(kU);uYe=r(jkr,"Register a new feature extractor for this class."),jkr.forEach(t),bBe.forEach(t),Is.forEach(t),h8e=i(d),Ni=n(d,"H2",{class:!0});var vBe=s(Ni);ch=n(vBe,"A",{id:!0,class:!0,href:!0});var Nkr=s(ch);xU=n(Nkr,"SPAN",{});var Dkr=s(xU);m(LE.$$.fragment,Dkr),Dkr.forEach(t),Nkr.forEach(t),bYe=i(vBe),RU=n(vBe,"SPAN",{});var qkr=s(RU);vYe=r(qkr,"AutoProcessor"),qkr.forEach(t),vBe.forEach(t),p8e=i(d),zo=n(d,"DIV",{class:!0});var js=s(zo);m(BE.$$.fragment,js),TYe=i(js),kE=n(js,"P",{});var TBe=s(kE);FYe=r(TBe,`This is a generic processor class that will be instantiated as one of the processor classes of the library when
created with the `),ax=n(TBe,"A",{href:!0});var Gkr=s(ax);CYe=r(Gkr,"AutoProcessor.from_pretrained()"),Gkr.forEach(t),MYe=r(TBe," class method."),TBe.forEach(t),EYe=i(js),xE=n(js,"P",{});var FBe=s(xE);yYe=r(FBe,"This class cannot be instantiated directly using "),SU=n(FBe,"CODE",{});var Okr=s(SU);wYe=r(Okr,"__init__()"),Okr.forEach(t),AYe=r(FBe," (throws an error)."),FBe.forEach(t),LYe=i(js),Be=n(js,"DIV",{class:!0});var Rt=s(Be);m(RE.$$.fragment,Rt),BYe=i(Rt),PU=n(Rt,"P",{});var Xkr=s(PU);kYe=r(Xkr,"Instantiate one of the processor classes of the library from a pretrained model vocabulary."),Xkr.forEach(t),xYe=i(Rt),Di=n(Rt,"P",{});var VX=s(Di);RYe=r(VX,"The processor class to instantiate is selected based on the "),$U=n(VX,"CODE",{});var zkr=s($U);SYe=r(zkr,"model_type"),zkr.forEach(t),PYe=r(VX,` property of the config object (either
passed as an argument or loaded from `),IU=n(VX,"CODE",{});var Vkr=s(IU);$Ye=r(Vkr,"pretrained_model_name_or_path"),Vkr.forEach(t),IYe=r(VX," if possible):"),VX.forEach(t),jYe=i(Rt),we=n(Rt,"UL",{});var No=s(we);fh=n(No,"LI",{});var nve=s(fh);jU=n(nve,"STRONG",{});var Wkr=s(jU);NYe=r(Wkr,"clip"),Wkr.forEach(t),DYe=r(nve," \u2014 "),nx=n(nve,"A",{href:!0});var Qkr=s(nx);qYe=r(Qkr,"CLIPProcessor"),Qkr.forEach(t),GYe=r(nve," (CLIP model)"),nve.forEach(t),OYe=i(No),mh=n(No,"LI",{});var sve=s(mh);NU=n(sve,"STRONG",{});var Hkr=s(NU);XYe=r(Hkr,"layoutlmv2"),Hkr.forEach(t),zYe=r(sve," \u2014 "),sx=n(sve,"A",{href:!0});var Ukr=s(sx);VYe=r(Ukr,"LayoutLMv2Processor"),Ukr.forEach(t),WYe=r(sve," (LayoutLMv2 model)"),sve.forEach(t),QYe=i(No),gh=n(No,"LI",{});var lve=s(gh);DU=n(lve,"STRONG",{});var Jkr=s(DU);HYe=r(Jkr,"layoutxlm"),Jkr.forEach(t),UYe=r(lve," \u2014 "),lx=n(lve,"A",{href:!0});var Ykr=s(lx);JYe=r(Ykr,"LayoutXLMProcessor"),Ykr.forEach(t),YYe=r(lve," (LayoutXLM model)"),lve.forEach(t),KYe=i(No),hh=n(No,"LI",{});var ive=s(hh);qU=n(ive,"STRONG",{});var Kkr=s(qU);ZYe=r(Kkr,"speech_to_text"),Kkr.forEach(t),eKe=r(ive," \u2014 "),ix=n(ive,"A",{href:!0});var Zkr=s(ix);oKe=r(Zkr,"Speech2TextProcessor"),Zkr.forEach(t),rKe=r(ive," (Speech2Text model)"),ive.forEach(t),tKe=i(No),ph=n(No,"LI",{});var dve=s(ph);GU=n(dve,"STRONG",{});var exr=s(GU);aKe=r(exr,"speech_to_text_2"),exr.forEach(t),nKe=r(dve," \u2014 "),dx=n(dve,"A",{href:!0});var oxr=s(dx);sKe=r(oxr,"Speech2Text2Processor"),oxr.forEach(t),lKe=r(dve," (Speech2Text2 model)"),dve.forEach(t),iKe=i(No),_h=n(No,"LI",{});var cve=s(_h);OU=n(cve,"STRONG",{});var rxr=s(OU);dKe=r(rxr,"trocr"),rxr.forEach(t),cKe=r(cve," \u2014 "),cx=n(cve,"A",{href:!0});var txr=s(cx);fKe=r(txr,"TrOCRProcessor"),txr.forEach(t),mKe=r(cve," (TrOCR model)"),cve.forEach(t),gKe=i(No),uh=n(No,"LI",{});var fve=s(uh);XU=n(fve,"STRONG",{});var axr=s(XU);hKe=r(axr,"vision-text-dual-encoder"),axr.forEach(t),pKe=r(fve," \u2014 "),fx=n(fve,"A",{href:!0});var nxr=s(fx);_Ke=r(nxr,"VisionTextDualEncoderProcessor"),nxr.forEach(t),uKe=r(fve," (VisionTextDualEncoder model)"),fve.forEach(t),bKe=i(No),bh=n(No,"LI",{});var mve=s(bh);zU=n(mve,"STRONG",{});var sxr=s(zU);vKe=r(sxr,"wav2vec2"),sxr.forEach(t),TKe=r(mve," \u2014 "),mx=n(mve,"A",{href:!0});var lxr=s(mx);FKe=r(lxr,"Wav2Vec2Processor"),lxr.forEach(t),CKe=r(mve," (Wav2Vec2 model)"),mve.forEach(t),No.forEach(t),MKe=i(Rt),m(vh.$$.fragment,Rt),EKe=i(Rt),VU=n(Rt,"P",{});var ixr=s(VU);yKe=r(ixr,"Examples:"),ixr.forEach(t),wKe=i(Rt),m(SE.$$.fragment,Rt),Rt.forEach(t),AKe=i(js),Th=n(js,"DIV",{class:!0});var CBe=s(Th);m(PE.$$.fragment,CBe),LKe=i(CBe),WU=n(CBe,"P",{});var dxr=s(WU);BKe=r(dxr,"Register a new processor for this class."),dxr.forEach(t),CBe.forEach(t),js.forEach(t),_8e=i(d),qi=n(d,"H2",{class:!0});var MBe=s(qi);Fh=n(MBe,"A",{id:!0,class:!0,href:!0});var cxr=s(Fh);QU=n(cxr,"SPAN",{});var fxr=s(QU);m($E.$$.fragment,fxr),fxr.forEach(t),cxr.forEach(t),kKe=i(MBe),HU=n(MBe,"SPAN",{});var mxr=s(HU);xKe=r(mxr,"AutoModel"),mxr.forEach(t),MBe.forEach(t),u8e=i(d),Vo=n(d,"DIV",{class:!0});var Ns=s(Vo);m(IE.$$.fragment,Ns),RKe=i(Ns),Gi=n(Ns,"P",{});var WX=s(Gi);SKe=r(WX,`This is a generic model class that will be instantiated as one of the base model classes of the library when created
with the `),UU=n(WX,"CODE",{});var gxr=s(UU);PKe=r(gxr,"from_pretrained()"),gxr.forEach(t),$Ke=r(WX,"class method or the "),JU=n(WX,"CODE",{});var hxr=s(JU);IKe=r(hxr,"from_config()"),hxr.forEach(t),jKe=r(WX,`class
method.`),WX.forEach(t),NKe=i(Ns),jE=n(Ns,"P",{});var EBe=s(jE);DKe=r(EBe,"This class cannot be instantiated directly using "),YU=n(EBe,"CODE",{});var pxr=s(YU);qKe=r(pxr,"__init__()"),pxr.forEach(t),GKe=r(EBe," (throws an error)."),EBe.forEach(t),OKe=i(Ns),Nr=n(Ns,"DIV",{class:!0});var Ds=s(Nr);m(NE.$$.fragment,Ds),XKe=i(Ds),KU=n(Ds,"P",{});var _xr=s(KU);zKe=r(_xr,"Instantiates one of the base model classes of the library from a configuration."),_xr.forEach(t),VKe=i(Ds),Oi=n(Ds,"P",{});var QX=s(Oi);WKe=r(QX,`Note:
Loading a model from its configuration file does `),ZU=n(QX,"STRONG",{});var uxr=s(ZU);QKe=r(uxr,"not"),uxr.forEach(t),HKe=r(QX,` load the model weights. It only affects the
model\u2019s configuration. Use `),eJ=n(QX,"CODE",{});var bxr=s(eJ);UKe=r(bxr,"from_pretrained()"),bxr.forEach(t),JKe=r(QX,"to load the model weights."),QX.forEach(t),YKe=i(Ds),oJ=n(Ds,"P",{});var vxr=s(oJ);KKe=r(vxr,"Examples:"),vxr.forEach(t),ZKe=i(Ds),m(DE.$$.fragment,Ds),Ds.forEach(t),eZe=i(Ns),ke=n(Ns,"DIV",{class:!0});var St=s(ke);m(qE.$$.fragment,St),oZe=i(St),rJ=n(St,"P",{});var Txr=s(rJ);rZe=r(Txr,"Instantiate one of the base model classes of the library from a pretrained model."),Txr.forEach(t),tZe=i(St),Da=n(St,"P",{});var lM=s(Da);aZe=r(lM,"The model class to instantiate is selected based on the "),tJ=n(lM,"CODE",{});var Fxr=s(tJ);nZe=r(Fxr,"model_type"),Fxr.forEach(t),sZe=r(lM,` property of the config object (either
passed as an argument or loaded from `),aJ=n(lM,"CODE",{});var Cxr=s(aJ);lZe=r(Cxr,"pretrained_model_name_or_path"),Cxr.forEach(t),iZe=r(lM,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),nJ=n(lM,"CODE",{});var Mxr=s(nJ);dZe=r(Mxr,"pretrained_model_name_or_path"),Mxr.forEach(t),cZe=r(lM,":"),lM.forEach(t),fZe=i(St),F=n(St,"UL",{});var C=s(F);Ch=n(C,"LI",{});var gve=s(Ch);sJ=n(gve,"STRONG",{});var Exr=s(sJ);mZe=r(Exr,"albert"),Exr.forEach(t),gZe=r(gve," \u2014 "),gx=n(gve,"A",{href:!0});var yxr=s(gx);hZe=r(yxr,"AlbertModel"),yxr.forEach(t),pZe=r(gve," (ALBERT model)"),gve.forEach(t),_Ze=i(C),Mh=n(C,"LI",{});var hve=s(Mh);lJ=n(hve,"STRONG",{});var wxr=s(lJ);uZe=r(wxr,"bart"),wxr.forEach(t),bZe=r(hve," \u2014 "),hx=n(hve,"A",{href:!0});var Axr=s(hx);vZe=r(Axr,"BartModel"),Axr.forEach(t),TZe=r(hve," (BART model)"),hve.forEach(t),FZe=i(C),Eh=n(C,"LI",{});var pve=s(Eh);iJ=n(pve,"STRONG",{});var Lxr=s(iJ);CZe=r(Lxr,"beit"),Lxr.forEach(t),MZe=r(pve," \u2014 "),px=n(pve,"A",{href:!0});var Bxr=s(px);EZe=r(Bxr,"BeitModel"),Bxr.forEach(t),yZe=r(pve," (BEiT model)"),pve.forEach(t),wZe=i(C),yh=n(C,"LI",{});var _ve=s(yh);dJ=n(_ve,"STRONG",{});var kxr=s(dJ);AZe=r(kxr,"bert"),kxr.forEach(t),LZe=r(_ve," \u2014 "),_x=n(_ve,"A",{href:!0});var xxr=s(_x);BZe=r(xxr,"BertModel"),xxr.forEach(t),kZe=r(_ve," (BERT model)"),_ve.forEach(t),xZe=i(C),wh=n(C,"LI",{});var uve=s(wh);cJ=n(uve,"STRONG",{});var Rxr=s(cJ);RZe=r(Rxr,"bert-generation"),Rxr.forEach(t),SZe=r(uve," \u2014 "),ux=n(uve,"A",{href:!0});var Sxr=s(ux);PZe=r(Sxr,"BertGenerationEncoder"),Sxr.forEach(t),$Ze=r(uve," (Bert Generation model)"),uve.forEach(t),IZe=i(C),Ah=n(C,"LI",{});var bve=s(Ah);fJ=n(bve,"STRONG",{});var Pxr=s(fJ);jZe=r(Pxr,"big_bird"),Pxr.forEach(t),NZe=r(bve," \u2014 "),bx=n(bve,"A",{href:!0});var $xr=s(bx);DZe=r($xr,"BigBirdModel"),$xr.forEach(t),qZe=r(bve," (BigBird model)"),bve.forEach(t),GZe=i(C),Lh=n(C,"LI",{});var vve=s(Lh);mJ=n(vve,"STRONG",{});var Ixr=s(mJ);OZe=r(Ixr,"bigbird_pegasus"),Ixr.forEach(t),XZe=r(vve," \u2014 "),vx=n(vve,"A",{href:!0});var jxr=s(vx);zZe=r(jxr,"BigBirdPegasusModel"),jxr.forEach(t),VZe=r(vve," (BigBirdPegasus model)"),vve.forEach(t),WZe=i(C),Bh=n(C,"LI",{});var Tve=s(Bh);gJ=n(Tve,"STRONG",{});var Nxr=s(gJ);QZe=r(Nxr,"blenderbot"),Nxr.forEach(t),HZe=r(Tve," \u2014 "),Tx=n(Tve,"A",{href:!0});var Dxr=s(Tx);UZe=r(Dxr,"BlenderbotModel"),Dxr.forEach(t),JZe=r(Tve," (Blenderbot model)"),Tve.forEach(t),YZe=i(C),kh=n(C,"LI",{});var Fve=s(kh);hJ=n(Fve,"STRONG",{});var qxr=s(hJ);KZe=r(qxr,"blenderbot-small"),qxr.forEach(t),ZZe=r(Fve," \u2014 "),Fx=n(Fve,"A",{href:!0});var Gxr=s(Fx);eeo=r(Gxr,"BlenderbotSmallModel"),Gxr.forEach(t),oeo=r(Fve," (BlenderbotSmall model)"),Fve.forEach(t),reo=i(C),xh=n(C,"LI",{});var Cve=s(xh);pJ=n(Cve,"STRONG",{});var Oxr=s(pJ);teo=r(Oxr,"camembert"),Oxr.forEach(t),aeo=r(Cve," \u2014 "),Cx=n(Cve,"A",{href:!0});var Xxr=s(Cx);neo=r(Xxr,"CamembertModel"),Xxr.forEach(t),seo=r(Cve," (CamemBERT model)"),Cve.forEach(t),leo=i(C),Rh=n(C,"LI",{});var Mve=s(Rh);_J=n(Mve,"STRONG",{});var zxr=s(_J);ieo=r(zxr,"canine"),zxr.forEach(t),deo=r(Mve," \u2014 "),Mx=n(Mve,"A",{href:!0});var Vxr=s(Mx);ceo=r(Vxr,"CanineModel"),Vxr.forEach(t),feo=r(Mve," (Canine model)"),Mve.forEach(t),meo=i(C),Sh=n(C,"LI",{});var Eve=s(Sh);uJ=n(Eve,"STRONG",{});var Wxr=s(uJ);geo=r(Wxr,"clip"),Wxr.forEach(t),heo=r(Eve," \u2014 "),Ex=n(Eve,"A",{href:!0});var Qxr=s(Ex);peo=r(Qxr,"CLIPModel"),Qxr.forEach(t),_eo=r(Eve," (CLIP model)"),Eve.forEach(t),ueo=i(C),Ph=n(C,"LI",{});var yve=s(Ph);bJ=n(yve,"STRONG",{});var Hxr=s(bJ);beo=r(Hxr,"convbert"),Hxr.forEach(t),veo=r(yve," \u2014 "),yx=n(yve,"A",{href:!0});var Uxr=s(yx);Teo=r(Uxr,"ConvBertModel"),Uxr.forEach(t),Feo=r(yve," (ConvBERT model)"),yve.forEach(t),Ceo=i(C),$h=n(C,"LI",{});var wve=s($h);vJ=n(wve,"STRONG",{});var Jxr=s(vJ);Meo=r(Jxr,"convnext"),Jxr.forEach(t),Eeo=r(wve," \u2014 "),wx=n(wve,"A",{href:!0});var Yxr=s(wx);yeo=r(Yxr,"ConvNextModel"),Yxr.forEach(t),weo=r(wve," (ConvNext model)"),wve.forEach(t),Aeo=i(C),Ih=n(C,"LI",{});var Ave=s(Ih);TJ=n(Ave,"STRONG",{});var Kxr=s(TJ);Leo=r(Kxr,"ctrl"),Kxr.forEach(t),Beo=r(Ave," \u2014 "),Ax=n(Ave,"A",{href:!0});var Zxr=s(Ax);keo=r(Zxr,"CTRLModel"),Zxr.forEach(t),xeo=r(Ave," (CTRL model)"),Ave.forEach(t),Reo=i(C),jh=n(C,"LI",{});var Lve=s(jh);FJ=n(Lve,"STRONG",{});var eRr=s(FJ);Seo=r(eRr,"deberta"),eRr.forEach(t),Peo=r(Lve," \u2014 "),Lx=n(Lve,"A",{href:!0});var oRr=s(Lx);$eo=r(oRr,"DebertaModel"),oRr.forEach(t),Ieo=r(Lve," (DeBERTa model)"),Lve.forEach(t),jeo=i(C),Nh=n(C,"LI",{});var Bve=s(Nh);CJ=n(Bve,"STRONG",{});var rRr=s(CJ);Neo=r(rRr,"deberta-v2"),rRr.forEach(t),Deo=r(Bve," \u2014 "),Bx=n(Bve,"A",{href:!0});var tRr=s(Bx);qeo=r(tRr,"DebertaV2Model"),tRr.forEach(t),Geo=r(Bve," (DeBERTa-v2 model)"),Bve.forEach(t),Oeo=i(C),Dh=n(C,"LI",{});var kve=s(Dh);MJ=n(kve,"STRONG",{});var aRr=s(MJ);Xeo=r(aRr,"deit"),aRr.forEach(t),zeo=r(kve," \u2014 "),kx=n(kve,"A",{href:!0});var nRr=s(kx);Veo=r(nRr,"DeiTModel"),nRr.forEach(t),Weo=r(kve," (DeiT model)"),kve.forEach(t),Qeo=i(C),qh=n(C,"LI",{});var xve=s(qh);EJ=n(xve,"STRONG",{});var sRr=s(EJ);Heo=r(sRr,"detr"),sRr.forEach(t),Ueo=r(xve," \u2014 "),xx=n(xve,"A",{href:!0});var lRr=s(xx);Jeo=r(lRr,"DetrModel"),lRr.forEach(t),Yeo=r(xve," (DETR model)"),xve.forEach(t),Keo=i(C),Gh=n(C,"LI",{});var Rve=s(Gh);yJ=n(Rve,"STRONG",{});var iRr=s(yJ);Zeo=r(iRr,"distilbert"),iRr.forEach(t),eoo=r(Rve," \u2014 "),Rx=n(Rve,"A",{href:!0});var dRr=s(Rx);ooo=r(dRr,"DistilBertModel"),dRr.forEach(t),roo=r(Rve," (DistilBERT model)"),Rve.forEach(t),too=i(C),Oh=n(C,"LI",{});var Sve=s(Oh);wJ=n(Sve,"STRONG",{});var cRr=s(wJ);aoo=r(cRr,"dpr"),cRr.forEach(t),noo=r(Sve," \u2014 "),Sx=n(Sve,"A",{href:!0});var fRr=s(Sx);soo=r(fRr,"DPRQuestionEncoder"),fRr.forEach(t),loo=r(Sve," (DPR model)"),Sve.forEach(t),ioo=i(C),Xh=n(C,"LI",{});var Pve=s(Xh);AJ=n(Pve,"STRONG",{});var mRr=s(AJ);doo=r(mRr,"electra"),mRr.forEach(t),coo=r(Pve," \u2014 "),Px=n(Pve,"A",{href:!0});var gRr=s(Px);foo=r(gRr,"ElectraModel"),gRr.forEach(t),moo=r(Pve," (ELECTRA model)"),Pve.forEach(t),goo=i(C),zh=n(C,"LI",{});var $ve=s(zh);LJ=n($ve,"STRONG",{});var hRr=s(LJ);hoo=r(hRr,"flaubert"),hRr.forEach(t),poo=r($ve," \u2014 "),$x=n($ve,"A",{href:!0});var pRr=s($x);_oo=r(pRr,"FlaubertModel"),pRr.forEach(t),uoo=r($ve," (FlauBERT model)"),$ve.forEach(t),boo=i(C),Vh=n(C,"LI",{});var Ive=s(Vh);BJ=n(Ive,"STRONG",{});var _Rr=s(BJ);voo=r(_Rr,"fnet"),_Rr.forEach(t),Too=r(Ive," \u2014 "),Ix=n(Ive,"A",{href:!0});var uRr=s(Ix);Foo=r(uRr,"FNetModel"),uRr.forEach(t),Coo=r(Ive," (FNet model)"),Ive.forEach(t),Moo=i(C),Wh=n(C,"LI",{});var jve=s(Wh);kJ=n(jve,"STRONG",{});var bRr=s(kJ);Eoo=r(bRr,"fsmt"),bRr.forEach(t),yoo=r(jve," \u2014 "),jx=n(jve,"A",{href:!0});var vRr=s(jx);woo=r(vRr,"FSMTModel"),vRr.forEach(t),Aoo=r(jve," (FairSeq Machine-Translation model)"),jve.forEach(t),Loo=i(C),xs=n(C,"LI",{});var IL=s(xs);xJ=n(IL,"STRONG",{});var TRr=s(xJ);Boo=r(TRr,"funnel"),TRr.forEach(t),koo=r(IL," \u2014 "),Nx=n(IL,"A",{href:!0});var FRr=s(Nx);xoo=r(FRr,"FunnelModel"),FRr.forEach(t),Roo=r(IL," or "),Dx=n(IL,"A",{href:!0});var CRr=s(Dx);Soo=r(CRr,"FunnelBaseModel"),CRr.forEach(t),Poo=r(IL," (Funnel Transformer model)"),IL.forEach(t),$oo=i(C),Qh=n(C,"LI",{});var Nve=s(Qh);RJ=n(Nve,"STRONG",{});var MRr=s(RJ);Ioo=r(MRr,"gpt2"),MRr.forEach(t),joo=r(Nve," \u2014 "),qx=n(Nve,"A",{href:!0});var ERr=s(qx);Noo=r(ERr,"GPT2Model"),ERr.forEach(t),Doo=r(Nve," (OpenAI GPT-2 model)"),Nve.forEach(t),qoo=i(C),Hh=n(C,"LI",{});var Dve=s(Hh);SJ=n(Dve,"STRONG",{});var yRr=s(SJ);Goo=r(yRr,"gpt_neo"),yRr.forEach(t),Ooo=r(Dve," \u2014 "),Gx=n(Dve,"A",{href:!0});var wRr=s(Gx);Xoo=r(wRr,"GPTNeoModel"),wRr.forEach(t),zoo=r(Dve," (GPT Neo model)"),Dve.forEach(t),Voo=i(C),Uh=n(C,"LI",{});var qve=s(Uh);PJ=n(qve,"STRONG",{});var ARr=s(PJ);Woo=r(ARr,"gptj"),ARr.forEach(t),Qoo=r(qve," \u2014 "),Ox=n(qve,"A",{href:!0});var LRr=s(Ox);Hoo=r(LRr,"GPTJModel"),LRr.forEach(t),Uoo=r(qve," (GPT-J model)"),qve.forEach(t),Joo=i(C),Jh=n(C,"LI",{});var Gve=s(Jh);$J=n(Gve,"STRONG",{});var BRr=s($J);Yoo=r(BRr,"hubert"),BRr.forEach(t),Koo=r(Gve," \u2014 "),Xx=n(Gve,"A",{href:!0});var kRr=s(Xx);Zoo=r(kRr,"HubertModel"),kRr.forEach(t),ero=r(Gve," (Hubert model)"),Gve.forEach(t),oro=i(C),Yh=n(C,"LI",{});var Ove=s(Yh);IJ=n(Ove,"STRONG",{});var xRr=s(IJ);rro=r(xRr,"ibert"),xRr.forEach(t),tro=r(Ove," \u2014 "),zx=n(Ove,"A",{href:!0});var RRr=s(zx);aro=r(RRr,"IBertModel"),RRr.forEach(t),nro=r(Ove," (I-BERT model)"),Ove.forEach(t),sro=i(C),Kh=n(C,"LI",{});var Xve=s(Kh);jJ=n(Xve,"STRONG",{});var SRr=s(jJ);lro=r(SRr,"imagegpt"),SRr.forEach(t),iro=r(Xve," \u2014 "),Vx=n(Xve,"A",{href:!0});var PRr=s(Vx);dro=r(PRr,"ImageGPTModel"),PRr.forEach(t),cro=r(Xve," (ImageGPT model)"),Xve.forEach(t),fro=i(C),Zh=n(C,"LI",{});var zve=s(Zh);NJ=n(zve,"STRONG",{});var $Rr=s(NJ);mro=r($Rr,"layoutlm"),$Rr.forEach(t),gro=r(zve," \u2014 "),Wx=n(zve,"A",{href:!0});var IRr=s(Wx);hro=r(IRr,"LayoutLMModel"),IRr.forEach(t),pro=r(zve," (LayoutLM model)"),zve.forEach(t),_ro=i(C),ep=n(C,"LI",{});var Vve=s(ep);DJ=n(Vve,"STRONG",{});var jRr=s(DJ);uro=r(jRr,"layoutlmv2"),jRr.forEach(t),bro=r(Vve," \u2014 "),Qx=n(Vve,"A",{href:!0});var NRr=s(Qx);vro=r(NRr,"LayoutLMv2Model"),NRr.forEach(t),Tro=r(Vve," (LayoutLMv2 model)"),Vve.forEach(t),Fro=i(C),op=n(C,"LI",{});var Wve=s(op);qJ=n(Wve,"STRONG",{});var DRr=s(qJ);Cro=r(DRr,"led"),DRr.forEach(t),Mro=r(Wve," \u2014 "),Hx=n(Wve,"A",{href:!0});var qRr=s(Hx);Ero=r(qRr,"LEDModel"),qRr.forEach(t),yro=r(Wve," (LED model)"),Wve.forEach(t),wro=i(C),rp=n(C,"LI",{});var Qve=s(rp);GJ=n(Qve,"STRONG",{});var GRr=s(GJ);Aro=r(GRr,"longformer"),GRr.forEach(t),Lro=r(Qve," \u2014 "),Ux=n(Qve,"A",{href:!0});var ORr=s(Ux);Bro=r(ORr,"LongformerModel"),ORr.forEach(t),kro=r(Qve," (Longformer model)"),Qve.forEach(t),xro=i(C),tp=n(C,"LI",{});var Hve=s(tp);OJ=n(Hve,"STRONG",{});var XRr=s(OJ);Rro=r(XRr,"luke"),XRr.forEach(t),Sro=r(Hve," \u2014 "),Jx=n(Hve,"A",{href:!0});var zRr=s(Jx);Pro=r(zRr,"LukeModel"),zRr.forEach(t),$ro=r(Hve," (LUKE model)"),Hve.forEach(t),Iro=i(C),ap=n(C,"LI",{});var Uve=s(ap);XJ=n(Uve,"STRONG",{});var VRr=s(XJ);jro=r(VRr,"lxmert"),VRr.forEach(t),Nro=r(Uve," \u2014 "),Yx=n(Uve,"A",{href:!0});var WRr=s(Yx);Dro=r(WRr,"LxmertModel"),WRr.forEach(t),qro=r(Uve," (LXMERT model)"),Uve.forEach(t),Gro=i(C),np=n(C,"LI",{});var Jve=s(np);zJ=n(Jve,"STRONG",{});var QRr=s(zJ);Oro=r(QRr,"m2m_100"),QRr.forEach(t),Xro=r(Jve," \u2014 "),Kx=n(Jve,"A",{href:!0});var HRr=s(Kx);zro=r(HRr,"M2M100Model"),HRr.forEach(t),Vro=r(Jve," (M2M100 model)"),Jve.forEach(t),Wro=i(C),sp=n(C,"LI",{});var Yve=s(sp);VJ=n(Yve,"STRONG",{});var URr=s(VJ);Qro=r(URr,"marian"),URr.forEach(t),Hro=r(Yve," \u2014 "),Zx=n(Yve,"A",{href:!0});var JRr=s(Zx);Uro=r(JRr,"MarianModel"),JRr.forEach(t),Jro=r(Yve," (Marian model)"),Yve.forEach(t),Yro=i(C),lp=n(C,"LI",{});var Kve=s(lp);WJ=n(Kve,"STRONG",{});var YRr=s(WJ);Kro=r(YRr,"mbart"),YRr.forEach(t),Zro=r(Kve," \u2014 "),eR=n(Kve,"A",{href:!0});var KRr=s(eR);eto=r(KRr,"MBartModel"),KRr.forEach(t),oto=r(Kve," (mBART model)"),Kve.forEach(t),rto=i(C),ip=n(C,"LI",{});var Zve=s(ip);QJ=n(Zve,"STRONG",{});var ZRr=s(QJ);tto=r(ZRr,"megatron-bert"),ZRr.forEach(t),ato=r(Zve," \u2014 "),oR=n(Zve,"A",{href:!0});var eSr=s(oR);nto=r(eSr,"MegatronBertModel"),eSr.forEach(t),sto=r(Zve," (MegatronBert model)"),Zve.forEach(t),lto=i(C),dp=n(C,"LI",{});var eTe=s(dp);HJ=n(eTe,"STRONG",{});var oSr=s(HJ);ito=r(oSr,"mobilebert"),oSr.forEach(t),dto=r(eTe," \u2014 "),rR=n(eTe,"A",{href:!0});var rSr=s(rR);cto=r(rSr,"MobileBertModel"),rSr.forEach(t),fto=r(eTe," (MobileBERT model)"),eTe.forEach(t),mto=i(C),cp=n(C,"LI",{});var oTe=s(cp);UJ=n(oTe,"STRONG",{});var tSr=s(UJ);gto=r(tSr,"mpnet"),tSr.forEach(t),hto=r(oTe," \u2014 "),tR=n(oTe,"A",{href:!0});var aSr=s(tR);pto=r(aSr,"MPNetModel"),aSr.forEach(t),_to=r(oTe," (MPNet model)"),oTe.forEach(t),uto=i(C),fp=n(C,"LI",{});var rTe=s(fp);JJ=n(rTe,"STRONG",{});var nSr=s(JJ);bto=r(nSr,"mt5"),nSr.forEach(t),vto=r(rTe," \u2014 "),aR=n(rTe,"A",{href:!0});var sSr=s(aR);Tto=r(sSr,"MT5Model"),sSr.forEach(t),Fto=r(rTe," (mT5 model)"),rTe.forEach(t),Cto=i(C),mp=n(C,"LI",{});var tTe=s(mp);YJ=n(tTe,"STRONG",{});var lSr=s(YJ);Mto=r(lSr,"nystromformer"),lSr.forEach(t),Eto=r(tTe," \u2014 "),nR=n(tTe,"A",{href:!0});var iSr=s(nR);yto=r(iSr,"NystromformerModel"),iSr.forEach(t),wto=r(tTe," (Nystromformer model)"),tTe.forEach(t),Ato=i(C),gp=n(C,"LI",{});var aTe=s(gp);KJ=n(aTe,"STRONG",{});var dSr=s(KJ);Lto=r(dSr,"openai-gpt"),dSr.forEach(t),Bto=r(aTe," \u2014 "),sR=n(aTe,"A",{href:!0});var cSr=s(sR);kto=r(cSr,"OpenAIGPTModel"),cSr.forEach(t),xto=r(aTe," (OpenAI GPT model)"),aTe.forEach(t),Rto=i(C),hp=n(C,"LI",{});var nTe=s(hp);ZJ=n(nTe,"STRONG",{});var fSr=s(ZJ);Sto=r(fSr,"pegasus"),fSr.forEach(t),Pto=r(nTe," \u2014 "),lR=n(nTe,"A",{href:!0});var mSr=s(lR);$to=r(mSr,"PegasusModel"),mSr.forEach(t),Ito=r(nTe," (Pegasus model)"),nTe.forEach(t),jto=i(C),pp=n(C,"LI",{});var sTe=s(pp);eY=n(sTe,"STRONG",{});var gSr=s(eY);Nto=r(gSr,"perceiver"),gSr.forEach(t),Dto=r(sTe," \u2014 "),iR=n(sTe,"A",{href:!0});var hSr=s(iR);qto=r(hSr,"PerceiverModel"),hSr.forEach(t),Gto=r(sTe," (Perceiver model)"),sTe.forEach(t),Oto=i(C),_p=n(C,"LI",{});var lTe=s(_p);oY=n(lTe,"STRONG",{});var pSr=s(oY);Xto=r(pSr,"plbart"),pSr.forEach(t),zto=r(lTe," \u2014 "),dR=n(lTe,"A",{href:!0});var _Sr=s(dR);Vto=r(_Sr,"PLBartModel"),_Sr.forEach(t),Wto=r(lTe," (PLBart model)"),lTe.forEach(t),Qto=i(C),up=n(C,"LI",{});var iTe=s(up);rY=n(iTe,"STRONG",{});var uSr=s(rY);Hto=r(uSr,"poolformer"),uSr.forEach(t),Uto=r(iTe," \u2014 "),cR=n(iTe,"A",{href:!0});var bSr=s(cR);Jto=r(bSr,"PoolFormerModel"),bSr.forEach(t),Yto=r(iTe," (PoolFormer model)"),iTe.forEach(t),Kto=i(C),bp=n(C,"LI",{});var dTe=s(bp);tY=n(dTe,"STRONG",{});var vSr=s(tY);Zto=r(vSr,"prophetnet"),vSr.forEach(t),eao=r(dTe," \u2014 "),fR=n(dTe,"A",{href:!0});var TSr=s(fR);oao=r(TSr,"ProphetNetModel"),TSr.forEach(t),rao=r(dTe," (ProphetNet model)"),dTe.forEach(t),tao=i(C),vp=n(C,"LI",{});var cTe=s(vp);aY=n(cTe,"STRONG",{});var FSr=s(aY);aao=r(FSr,"qdqbert"),FSr.forEach(t),nao=r(cTe," \u2014 "),mR=n(cTe,"A",{href:!0});var CSr=s(mR);sao=r(CSr,"QDQBertModel"),CSr.forEach(t),lao=r(cTe," (QDQBert model)"),cTe.forEach(t),iao=i(C),Tp=n(C,"LI",{});var fTe=s(Tp);nY=n(fTe,"STRONG",{});var MSr=s(nY);dao=r(MSr,"reformer"),MSr.forEach(t),cao=r(fTe," \u2014 "),gR=n(fTe,"A",{href:!0});var ESr=s(gR);fao=r(ESr,"ReformerModel"),ESr.forEach(t),mao=r(fTe," (Reformer model)"),fTe.forEach(t),gao=i(C),Fp=n(C,"LI",{});var mTe=s(Fp);sY=n(mTe,"STRONG",{});var ySr=s(sY);hao=r(ySr,"rembert"),ySr.forEach(t),pao=r(mTe," \u2014 "),hR=n(mTe,"A",{href:!0});var wSr=s(hR);_ao=r(wSr,"RemBertModel"),wSr.forEach(t),uao=r(mTe," (RemBERT model)"),mTe.forEach(t),bao=i(C),Cp=n(C,"LI",{});var gTe=s(Cp);lY=n(gTe,"STRONG",{});var ASr=s(lY);vao=r(ASr,"retribert"),ASr.forEach(t),Tao=r(gTe," \u2014 "),pR=n(gTe,"A",{href:!0});var LSr=s(pR);Fao=r(LSr,"RetriBertModel"),LSr.forEach(t),Cao=r(gTe," (RetriBERT model)"),gTe.forEach(t),Mao=i(C),Mp=n(C,"LI",{});var hTe=s(Mp);iY=n(hTe,"STRONG",{});var BSr=s(iY);Eao=r(BSr,"roberta"),BSr.forEach(t),yao=r(hTe," \u2014 "),_R=n(hTe,"A",{href:!0});var kSr=s(_R);wao=r(kSr,"RobertaModel"),kSr.forEach(t),Aao=r(hTe," (RoBERTa model)"),hTe.forEach(t),Lao=i(C),Ep=n(C,"LI",{});var pTe=s(Ep);dY=n(pTe,"STRONG",{});var xSr=s(dY);Bao=r(xSr,"roformer"),xSr.forEach(t),kao=r(pTe," \u2014 "),uR=n(pTe,"A",{href:!0});var RSr=s(uR);xao=r(RSr,"RoFormerModel"),RSr.forEach(t),Rao=r(pTe," (RoFormer model)"),pTe.forEach(t),Sao=i(C),yp=n(C,"LI",{});var _Te=s(yp);cY=n(_Te,"STRONG",{});var SSr=s(cY);Pao=r(SSr,"segformer"),SSr.forEach(t),$ao=r(_Te," \u2014 "),bR=n(_Te,"A",{href:!0});var PSr=s(bR);Iao=r(PSr,"SegformerModel"),PSr.forEach(t),jao=r(_Te," (SegFormer model)"),_Te.forEach(t),Nao=i(C),wp=n(C,"LI",{});var uTe=s(wp);fY=n(uTe,"STRONG",{});var $Sr=s(fY);Dao=r($Sr,"sew"),$Sr.forEach(t),qao=r(uTe," \u2014 "),vR=n(uTe,"A",{href:!0});var ISr=s(vR);Gao=r(ISr,"SEWModel"),ISr.forEach(t),Oao=r(uTe," (SEW model)"),uTe.forEach(t),Xao=i(C),Ap=n(C,"LI",{});var bTe=s(Ap);mY=n(bTe,"STRONG",{});var jSr=s(mY);zao=r(jSr,"sew-d"),jSr.forEach(t),Vao=r(bTe," \u2014 "),TR=n(bTe,"A",{href:!0});var NSr=s(TR);Wao=r(NSr,"SEWDModel"),NSr.forEach(t),Qao=r(bTe," (SEW-D model)"),bTe.forEach(t),Hao=i(C),Lp=n(C,"LI",{});var vTe=s(Lp);gY=n(vTe,"STRONG",{});var DSr=s(gY);Uao=r(DSr,"speech_to_text"),DSr.forEach(t),Jao=r(vTe," \u2014 "),FR=n(vTe,"A",{href:!0});var qSr=s(FR);Yao=r(qSr,"Speech2TextModel"),qSr.forEach(t),Kao=r(vTe," (Speech2Text model)"),vTe.forEach(t),Zao=i(C),Bp=n(C,"LI",{});var TTe=s(Bp);hY=n(TTe,"STRONG",{});var GSr=s(hY);eno=r(GSr,"splinter"),GSr.forEach(t),ono=r(TTe," \u2014 "),CR=n(TTe,"A",{href:!0});var OSr=s(CR);rno=r(OSr,"SplinterModel"),OSr.forEach(t),tno=r(TTe," (Splinter model)"),TTe.forEach(t),ano=i(C),kp=n(C,"LI",{});var FTe=s(kp);pY=n(FTe,"STRONG",{});var XSr=s(pY);nno=r(XSr,"squeezebert"),XSr.forEach(t),sno=r(FTe," \u2014 "),MR=n(FTe,"A",{href:!0});var zSr=s(MR);lno=r(zSr,"SqueezeBertModel"),zSr.forEach(t),ino=r(FTe," (SqueezeBERT model)"),FTe.forEach(t),dno=i(C),xp=n(C,"LI",{});var CTe=s(xp);_Y=n(CTe,"STRONG",{});var VSr=s(_Y);cno=r(VSr,"swin"),VSr.forEach(t),fno=r(CTe," \u2014 "),ER=n(CTe,"A",{href:!0});var WSr=s(ER);mno=r(WSr,"SwinModel"),WSr.forEach(t),gno=r(CTe," (Swin model)"),CTe.forEach(t),hno=i(C),Rp=n(C,"LI",{});var MTe=s(Rp);uY=n(MTe,"STRONG",{});var QSr=s(uY);pno=r(QSr,"t5"),QSr.forEach(t),_no=r(MTe," \u2014 "),yR=n(MTe,"A",{href:!0});var HSr=s(yR);uno=r(HSr,"T5Model"),HSr.forEach(t),bno=r(MTe," (T5 model)"),MTe.forEach(t),vno=i(C),Sp=n(C,"LI",{});var ETe=s(Sp);bY=n(ETe,"STRONG",{});var USr=s(bY);Tno=r(USr,"tapas"),USr.forEach(t),Fno=r(ETe," \u2014 "),wR=n(ETe,"A",{href:!0});var JSr=s(wR);Cno=r(JSr,"TapasModel"),JSr.forEach(t),Mno=r(ETe," (TAPAS model)"),ETe.forEach(t),Eno=i(C),Pp=n(C,"LI",{});var yTe=s(Pp);vY=n(yTe,"STRONG",{});var YSr=s(vY);yno=r(YSr,"transfo-xl"),YSr.forEach(t),wno=r(yTe," \u2014 "),AR=n(yTe,"A",{href:!0});var KSr=s(AR);Ano=r(KSr,"TransfoXLModel"),KSr.forEach(t),Lno=r(yTe," (Transformer-XL model)"),yTe.forEach(t),Bno=i(C),$p=n(C,"LI",{});var wTe=s($p);TY=n(wTe,"STRONG",{});var ZSr=s(TY);kno=r(ZSr,"unispeech"),ZSr.forEach(t),xno=r(wTe," \u2014 "),LR=n(wTe,"A",{href:!0});var ePr=s(LR);Rno=r(ePr,"UniSpeechModel"),ePr.forEach(t),Sno=r(wTe," (UniSpeech model)"),wTe.forEach(t),Pno=i(C),Ip=n(C,"LI",{});var ATe=s(Ip);FY=n(ATe,"STRONG",{});var oPr=s(FY);$no=r(oPr,"unispeech-sat"),oPr.forEach(t),Ino=r(ATe," \u2014 "),BR=n(ATe,"A",{href:!0});var rPr=s(BR);jno=r(rPr,"UniSpeechSatModel"),rPr.forEach(t),Nno=r(ATe," (UniSpeechSat model)"),ATe.forEach(t),Dno=i(C),jp=n(C,"LI",{});var LTe=s(jp);CY=n(LTe,"STRONG",{});var tPr=s(CY);qno=r(tPr,"vilt"),tPr.forEach(t),Gno=r(LTe," \u2014 "),kR=n(LTe,"A",{href:!0});var aPr=s(kR);Ono=r(aPr,"ViltModel"),aPr.forEach(t),Xno=r(LTe," (ViLT model)"),LTe.forEach(t),zno=i(C),Np=n(C,"LI",{});var BTe=s(Np);MY=n(BTe,"STRONG",{});var nPr=s(MY);Vno=r(nPr,"vision-text-dual-encoder"),nPr.forEach(t),Wno=r(BTe," \u2014 "),xR=n(BTe,"A",{href:!0});var sPr=s(xR);Qno=r(sPr,"VisionTextDualEncoderModel"),sPr.forEach(t),Hno=r(BTe," (VisionTextDualEncoder model)"),BTe.forEach(t),Uno=i(C),Dp=n(C,"LI",{});var kTe=s(Dp);EY=n(kTe,"STRONG",{});var lPr=s(EY);Jno=r(lPr,"visual_bert"),lPr.forEach(t),Yno=r(kTe," \u2014 "),RR=n(kTe,"A",{href:!0});var iPr=s(RR);Kno=r(iPr,"VisualBertModel"),iPr.forEach(t),Zno=r(kTe," (VisualBert model)"),kTe.forEach(t),eso=i(C),qp=n(C,"LI",{});var xTe=s(qp);yY=n(xTe,"STRONG",{});var dPr=s(yY);oso=r(dPr,"vit"),dPr.forEach(t),rso=r(xTe," \u2014 "),SR=n(xTe,"A",{href:!0});var cPr=s(SR);tso=r(cPr,"ViTModel"),cPr.forEach(t),aso=r(xTe," (ViT model)"),xTe.forEach(t),nso=i(C),Gp=n(C,"LI",{});var RTe=s(Gp);wY=n(RTe,"STRONG",{});var fPr=s(wY);sso=r(fPr,"vit_mae"),fPr.forEach(t),lso=r(RTe," \u2014 "),PR=n(RTe,"A",{href:!0});var mPr=s(PR);iso=r(mPr,"ViTMAEModel"),mPr.forEach(t),dso=r(RTe," (ViTMAE model)"),RTe.forEach(t),cso=i(C),Op=n(C,"LI",{});var STe=s(Op);AY=n(STe,"STRONG",{});var gPr=s(AY);fso=r(gPr,"wav2vec2"),gPr.forEach(t),mso=r(STe," \u2014 "),$R=n(STe,"A",{href:!0});var hPr=s($R);gso=r(hPr,"Wav2Vec2Model"),hPr.forEach(t),hso=r(STe," (Wav2Vec2 model)"),STe.forEach(t),pso=i(C),Xp=n(C,"LI",{});var PTe=s(Xp);LY=n(PTe,"STRONG",{});var pPr=s(LY);_so=r(pPr,"wavlm"),pPr.forEach(t),uso=r(PTe," \u2014 "),IR=n(PTe,"A",{href:!0});var _Pr=s(IR);bso=r(_Pr,"WavLMModel"),_Pr.forEach(t),vso=r(PTe," (WavLM model)"),PTe.forEach(t),Tso=i(C),zp=n(C,"LI",{});var $Te=s(zp);BY=n($Te,"STRONG",{});var uPr=s(BY);Fso=r(uPr,"xglm"),uPr.forEach(t),Cso=r($Te," \u2014 "),jR=n($Te,"A",{href:!0});var bPr=s(jR);Mso=r(bPr,"XGLMModel"),bPr.forEach(t),Eso=r($Te," (XGLM model)"),$Te.forEach(t),yso=i(C),Vp=n(C,"LI",{});var ITe=s(Vp);kY=n(ITe,"STRONG",{});var vPr=s(kY);wso=r(vPr,"xlm"),vPr.forEach(t),Aso=r(ITe," \u2014 "),NR=n(ITe,"A",{href:!0});var TPr=s(NR);Lso=r(TPr,"XLMModel"),TPr.forEach(t),Bso=r(ITe," (XLM model)"),ITe.forEach(t),kso=i(C),Wp=n(C,"LI",{});var jTe=s(Wp);xY=n(jTe,"STRONG",{});var FPr=s(xY);xso=r(FPr,"xlm-prophetnet"),FPr.forEach(t),Rso=r(jTe," \u2014 "),DR=n(jTe,"A",{href:!0});var CPr=s(DR);Sso=r(CPr,"XLMProphetNetModel"),CPr.forEach(t),Pso=r(jTe," (XLMProphetNet model)"),jTe.forEach(t),$so=i(C),Qp=n(C,"LI",{});var NTe=s(Qp);RY=n(NTe,"STRONG",{});var MPr=s(RY);Iso=r(MPr,"xlm-roberta"),MPr.forEach(t),jso=r(NTe," \u2014 "),qR=n(NTe,"A",{href:!0});var EPr=s(qR);Nso=r(EPr,"XLMRobertaModel"),EPr.forEach(t),Dso=r(NTe," (XLM-RoBERTa model)"),NTe.forEach(t),qso=i(C),Hp=n(C,"LI",{});var DTe=s(Hp);SY=n(DTe,"STRONG",{});var yPr=s(SY);Gso=r(yPr,"xlm-roberta-xl"),yPr.forEach(t),Oso=r(DTe," \u2014 "),GR=n(DTe,"A",{href:!0});var wPr=s(GR);Xso=r(wPr,"XLMRobertaXLModel"),wPr.forEach(t),zso=r(DTe," (XLM-RoBERTa-XL model)"),DTe.forEach(t),Vso=i(C),Up=n(C,"LI",{});var qTe=s(Up);PY=n(qTe,"STRONG",{});var APr=s(PY);Wso=r(APr,"xlnet"),APr.forEach(t),Qso=r(qTe," \u2014 "),OR=n(qTe,"A",{href:!0});var LPr=s(OR);Hso=r(LPr,"XLNetModel"),LPr.forEach(t),Uso=r(qTe," (XLNet model)"),qTe.forEach(t),Jso=i(C),Jp=n(C,"LI",{});var GTe=s(Jp);$Y=n(GTe,"STRONG",{});var BPr=s($Y);Yso=r(BPr,"yoso"),BPr.forEach(t),Kso=r(GTe," \u2014 "),XR=n(GTe,"A",{href:!0});var kPr=s(XR);Zso=r(kPr,"YosoModel"),kPr.forEach(t),elo=r(GTe," (YOSO model)"),GTe.forEach(t),C.forEach(t),olo=i(St),Yp=n(St,"P",{});var OTe=s(Yp);rlo=r(OTe,"The model is set in evaluation mode by default using "),IY=n(OTe,"CODE",{});var xPr=s(IY);tlo=r(xPr,"model.eval()"),xPr.forEach(t),alo=r(OTe,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),jY=n(OTe,"CODE",{});var RPr=s(jY);nlo=r(RPr,"model.train()"),RPr.forEach(t),OTe.forEach(t),slo=i(St),NY=n(St,"P",{});var SPr=s(NY);llo=r(SPr,"Examples:"),SPr.forEach(t),ilo=i(St),m(GE.$$.fragment,St),St.forEach(t),Ns.forEach(t),b8e=i(d),Xi=n(d,"H2",{class:!0});var yBe=s(Xi);Kp=n(yBe,"A",{id:!0,class:!0,href:!0});var PPr=s(Kp);DY=n(PPr,"SPAN",{});var $Pr=s(DY);m(OE.$$.fragment,$Pr),$Pr.forEach(t),PPr.forEach(t),dlo=i(yBe),qY=n(yBe,"SPAN",{});var IPr=s(qY);clo=r(IPr,"AutoModelForPreTraining"),IPr.forEach(t),yBe.forEach(t),v8e=i(d),Wo=n(d,"DIV",{class:!0});var qs=s(Wo);m(XE.$$.fragment,qs),flo=i(qs),zi=n(qs,"P",{});var HX=s(zi);mlo=r(HX,`This is a generic model class that will be instantiated as one of the model classes of the library (with a pretraining head) when created
with the `),GY=n(HX,"CODE",{});var jPr=s(GY);glo=r(jPr,"from_pretrained()"),jPr.forEach(t),hlo=r(HX,"class method or the "),OY=n(HX,"CODE",{});var NPr=s(OY);plo=r(NPr,"from_config()"),NPr.forEach(t),_lo=r(HX,`class
method.`),HX.forEach(t),ulo=i(qs),zE=n(qs,"P",{});var wBe=s(zE);blo=r(wBe,"This class cannot be instantiated directly using "),XY=n(wBe,"CODE",{});var DPr=s(XY);vlo=r(DPr,"__init__()"),DPr.forEach(t),Tlo=r(wBe," (throws an error)."),wBe.forEach(t),Flo=i(qs),Dr=n(qs,"DIV",{class:!0});var Gs=s(Dr);m(VE.$$.fragment,Gs),Clo=i(Gs),zY=n(Gs,"P",{});var qPr=s(zY);Mlo=r(qPr,"Instantiates one of the model classes of the library (with a pretraining head) from a configuration."),qPr.forEach(t),Elo=i(Gs),Vi=n(Gs,"P",{});var UX=s(Vi);ylo=r(UX,`Note:
Loading a model from its configuration file does `),VY=n(UX,"STRONG",{});var GPr=s(VY);wlo=r(GPr,"not"),GPr.forEach(t),Alo=r(UX,` load the model weights. It only affects the
model\u2019s configuration. Use `),WY=n(UX,"CODE",{});var OPr=s(WY);Llo=r(OPr,"from_pretrained()"),OPr.forEach(t),Blo=r(UX,"to load the model weights."),UX.forEach(t),klo=i(Gs),QY=n(Gs,"P",{});var XPr=s(QY);xlo=r(XPr,"Examples:"),XPr.forEach(t),Rlo=i(Gs),m(WE.$$.fragment,Gs),Gs.forEach(t),Slo=i(qs),xe=n(qs,"DIV",{class:!0});var Pt=s(xe);m(QE.$$.fragment,Pt),Plo=i(Pt),HY=n(Pt,"P",{});var zPr=s(HY);$lo=r(zPr,"Instantiate one of the model classes of the library (with a pretraining head) from a pretrained model."),zPr.forEach(t),Ilo=i(Pt),qa=n(Pt,"P",{});var iM=s(qa);jlo=r(iM,"The model class to instantiate is selected based on the "),UY=n(iM,"CODE",{});var VPr=s(UY);Nlo=r(VPr,"model_type"),VPr.forEach(t),Dlo=r(iM,` property of the config object (either
passed as an argument or loaded from `),JY=n(iM,"CODE",{});var WPr=s(JY);qlo=r(WPr,"pretrained_model_name_or_path"),WPr.forEach(t),Glo=r(iM,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),YY=n(iM,"CODE",{});var QPr=s(YY);Olo=r(QPr,"pretrained_model_name_or_path"),QPr.forEach(t),Xlo=r(iM,":"),iM.forEach(t),zlo=i(Pt),x=n(Pt,"UL",{});var S=s(x);Zp=n(S,"LI",{});var XTe=s(Zp);KY=n(XTe,"STRONG",{});var HPr=s(KY);Vlo=r(HPr,"albert"),HPr.forEach(t),Wlo=r(XTe," \u2014 "),zR=n(XTe,"A",{href:!0});var UPr=s(zR);Qlo=r(UPr,"AlbertForPreTraining"),UPr.forEach(t),Hlo=r(XTe," (ALBERT model)"),XTe.forEach(t),Ulo=i(S),e_=n(S,"LI",{});var zTe=s(e_);ZY=n(zTe,"STRONG",{});var JPr=s(ZY);Jlo=r(JPr,"bart"),JPr.forEach(t),Ylo=r(zTe," \u2014 "),VR=n(zTe,"A",{href:!0});var YPr=s(VR);Klo=r(YPr,"BartForConditionalGeneration"),YPr.forEach(t),Zlo=r(zTe," (BART model)"),zTe.forEach(t),eio=i(S),o_=n(S,"LI",{});var VTe=s(o_);eK=n(VTe,"STRONG",{});var KPr=s(eK);oio=r(KPr,"bert"),KPr.forEach(t),rio=r(VTe," \u2014 "),WR=n(VTe,"A",{href:!0});var ZPr=s(WR);tio=r(ZPr,"BertForPreTraining"),ZPr.forEach(t),aio=r(VTe," (BERT model)"),VTe.forEach(t),nio=i(S),r_=n(S,"LI",{});var WTe=s(r_);oK=n(WTe,"STRONG",{});var e$r=s(oK);sio=r(e$r,"big_bird"),e$r.forEach(t),lio=r(WTe," \u2014 "),QR=n(WTe,"A",{href:!0});var o$r=s(QR);iio=r(o$r,"BigBirdForPreTraining"),o$r.forEach(t),dio=r(WTe," (BigBird model)"),WTe.forEach(t),cio=i(S),t_=n(S,"LI",{});var QTe=s(t_);rK=n(QTe,"STRONG",{});var r$r=s(rK);fio=r(r$r,"camembert"),r$r.forEach(t),mio=r(QTe," \u2014 "),HR=n(QTe,"A",{href:!0});var t$r=s(HR);gio=r(t$r,"CamembertForMaskedLM"),t$r.forEach(t),hio=r(QTe," (CamemBERT model)"),QTe.forEach(t),pio=i(S),a_=n(S,"LI",{});var HTe=s(a_);tK=n(HTe,"STRONG",{});var a$r=s(tK);_io=r(a$r,"ctrl"),a$r.forEach(t),uio=r(HTe," \u2014 "),UR=n(HTe,"A",{href:!0});var n$r=s(UR);bio=r(n$r,"CTRLLMHeadModel"),n$r.forEach(t),vio=r(HTe," (CTRL model)"),HTe.forEach(t),Tio=i(S),n_=n(S,"LI",{});var UTe=s(n_);aK=n(UTe,"STRONG",{});var s$r=s(aK);Fio=r(s$r,"deberta"),s$r.forEach(t),Cio=r(UTe," \u2014 "),JR=n(UTe,"A",{href:!0});var l$r=s(JR);Mio=r(l$r,"DebertaForMaskedLM"),l$r.forEach(t),Eio=r(UTe," (DeBERTa model)"),UTe.forEach(t),yio=i(S),s_=n(S,"LI",{});var JTe=s(s_);nK=n(JTe,"STRONG",{});var i$r=s(nK);wio=r(i$r,"deberta-v2"),i$r.forEach(t),Aio=r(JTe," \u2014 "),YR=n(JTe,"A",{href:!0});var d$r=s(YR);Lio=r(d$r,"DebertaV2ForMaskedLM"),d$r.forEach(t),Bio=r(JTe," (DeBERTa-v2 model)"),JTe.forEach(t),kio=i(S),l_=n(S,"LI",{});var YTe=s(l_);sK=n(YTe,"STRONG",{});var c$r=s(sK);xio=r(c$r,"distilbert"),c$r.forEach(t),Rio=r(YTe," \u2014 "),KR=n(YTe,"A",{href:!0});var f$r=s(KR);Sio=r(f$r,"DistilBertForMaskedLM"),f$r.forEach(t),Pio=r(YTe," (DistilBERT model)"),YTe.forEach(t),$io=i(S),i_=n(S,"LI",{});var KTe=s(i_);lK=n(KTe,"STRONG",{});var m$r=s(lK);Iio=r(m$r,"electra"),m$r.forEach(t),jio=r(KTe," \u2014 "),ZR=n(KTe,"A",{href:!0});var g$r=s(ZR);Nio=r(g$r,"ElectraForPreTraining"),g$r.forEach(t),Dio=r(KTe," (ELECTRA model)"),KTe.forEach(t),qio=i(S),d_=n(S,"LI",{});var ZTe=s(d_);iK=n(ZTe,"STRONG",{});var h$r=s(iK);Gio=r(h$r,"flaubert"),h$r.forEach(t),Oio=r(ZTe," \u2014 "),eS=n(ZTe,"A",{href:!0});var p$r=s(eS);Xio=r(p$r,"FlaubertWithLMHeadModel"),p$r.forEach(t),zio=r(ZTe," (FlauBERT model)"),ZTe.forEach(t),Vio=i(S),c_=n(S,"LI",{});var eFe=s(c_);dK=n(eFe,"STRONG",{});var _$r=s(dK);Wio=r(_$r,"fnet"),_$r.forEach(t),Qio=r(eFe," \u2014 "),oS=n(eFe,"A",{href:!0});var u$r=s(oS);Hio=r(u$r,"FNetForPreTraining"),u$r.forEach(t),Uio=r(eFe," (FNet model)"),eFe.forEach(t),Jio=i(S),f_=n(S,"LI",{});var oFe=s(f_);cK=n(oFe,"STRONG",{});var b$r=s(cK);Yio=r(b$r,"fsmt"),b$r.forEach(t),Kio=r(oFe," \u2014 "),rS=n(oFe,"A",{href:!0});var v$r=s(rS);Zio=r(v$r,"FSMTForConditionalGeneration"),v$r.forEach(t),edo=r(oFe," (FairSeq Machine-Translation model)"),oFe.forEach(t),odo=i(S),m_=n(S,"LI",{});var rFe=s(m_);fK=n(rFe,"STRONG",{});var T$r=s(fK);rdo=r(T$r,"funnel"),T$r.forEach(t),tdo=r(rFe," \u2014 "),tS=n(rFe,"A",{href:!0});var F$r=s(tS);ado=r(F$r,"FunnelForPreTraining"),F$r.forEach(t),ndo=r(rFe," (Funnel Transformer model)"),rFe.forEach(t),sdo=i(S),g_=n(S,"LI",{});var tFe=s(g_);mK=n(tFe,"STRONG",{});var C$r=s(mK);ldo=r(C$r,"gpt2"),C$r.forEach(t),ido=r(tFe," \u2014 "),aS=n(tFe,"A",{href:!0});var M$r=s(aS);ddo=r(M$r,"GPT2LMHeadModel"),M$r.forEach(t),cdo=r(tFe," (OpenAI GPT-2 model)"),tFe.forEach(t),fdo=i(S),h_=n(S,"LI",{});var aFe=s(h_);gK=n(aFe,"STRONG",{});var E$r=s(gK);mdo=r(E$r,"ibert"),E$r.forEach(t),gdo=r(aFe," \u2014 "),nS=n(aFe,"A",{href:!0});var y$r=s(nS);hdo=r(y$r,"IBertForMaskedLM"),y$r.forEach(t),pdo=r(aFe," (I-BERT model)"),aFe.forEach(t),_do=i(S),p_=n(S,"LI",{});var nFe=s(p_);hK=n(nFe,"STRONG",{});var w$r=s(hK);udo=r(w$r,"layoutlm"),w$r.forEach(t),bdo=r(nFe," \u2014 "),sS=n(nFe,"A",{href:!0});var A$r=s(sS);vdo=r(A$r,"LayoutLMForMaskedLM"),A$r.forEach(t),Tdo=r(nFe," (LayoutLM model)"),nFe.forEach(t),Fdo=i(S),__=n(S,"LI",{});var sFe=s(__);pK=n(sFe,"STRONG",{});var L$r=s(pK);Cdo=r(L$r,"longformer"),L$r.forEach(t),Mdo=r(sFe," \u2014 "),lS=n(sFe,"A",{href:!0});var B$r=s(lS);Edo=r(B$r,"LongformerForMaskedLM"),B$r.forEach(t),ydo=r(sFe," (Longformer model)"),sFe.forEach(t),wdo=i(S),u_=n(S,"LI",{});var lFe=s(u_);_K=n(lFe,"STRONG",{});var k$r=s(_K);Ado=r(k$r,"lxmert"),k$r.forEach(t),Ldo=r(lFe," \u2014 "),iS=n(lFe,"A",{href:!0});var x$r=s(iS);Bdo=r(x$r,"LxmertForPreTraining"),x$r.forEach(t),kdo=r(lFe," (LXMERT model)"),lFe.forEach(t),xdo=i(S),b_=n(S,"LI",{});var iFe=s(b_);uK=n(iFe,"STRONG",{});var R$r=s(uK);Rdo=r(R$r,"megatron-bert"),R$r.forEach(t),Sdo=r(iFe," \u2014 "),dS=n(iFe,"A",{href:!0});var S$r=s(dS);Pdo=r(S$r,"MegatronBertForPreTraining"),S$r.forEach(t),$do=r(iFe," (MegatronBert model)"),iFe.forEach(t),Ido=i(S),v_=n(S,"LI",{});var dFe=s(v_);bK=n(dFe,"STRONG",{});var P$r=s(bK);jdo=r(P$r,"mobilebert"),P$r.forEach(t),Ndo=r(dFe," \u2014 "),cS=n(dFe,"A",{href:!0});var $$r=s(cS);Ddo=r($$r,"MobileBertForPreTraining"),$$r.forEach(t),qdo=r(dFe," (MobileBERT model)"),dFe.forEach(t),Gdo=i(S),T_=n(S,"LI",{});var cFe=s(T_);vK=n(cFe,"STRONG",{});var I$r=s(vK);Odo=r(I$r,"mpnet"),I$r.forEach(t),Xdo=r(cFe," \u2014 "),fS=n(cFe,"A",{href:!0});var j$r=s(fS);zdo=r(j$r,"MPNetForMaskedLM"),j$r.forEach(t),Vdo=r(cFe," (MPNet model)"),cFe.forEach(t),Wdo=i(S),F_=n(S,"LI",{});var fFe=s(F_);TK=n(fFe,"STRONG",{});var N$r=s(TK);Qdo=r(N$r,"openai-gpt"),N$r.forEach(t),Hdo=r(fFe," \u2014 "),mS=n(fFe,"A",{href:!0});var D$r=s(mS);Udo=r(D$r,"OpenAIGPTLMHeadModel"),D$r.forEach(t),Jdo=r(fFe," (OpenAI GPT model)"),fFe.forEach(t),Ydo=i(S),C_=n(S,"LI",{});var mFe=s(C_);FK=n(mFe,"STRONG",{});var q$r=s(FK);Kdo=r(q$r,"retribert"),q$r.forEach(t),Zdo=r(mFe," \u2014 "),gS=n(mFe,"A",{href:!0});var G$r=s(gS);eco=r(G$r,"RetriBertModel"),G$r.forEach(t),oco=r(mFe," (RetriBERT model)"),mFe.forEach(t),rco=i(S),M_=n(S,"LI",{});var gFe=s(M_);CK=n(gFe,"STRONG",{});var O$r=s(CK);tco=r(O$r,"roberta"),O$r.forEach(t),aco=r(gFe," \u2014 "),hS=n(gFe,"A",{href:!0});var X$r=s(hS);nco=r(X$r,"RobertaForMaskedLM"),X$r.forEach(t),sco=r(gFe," (RoBERTa model)"),gFe.forEach(t),lco=i(S),E_=n(S,"LI",{});var hFe=s(E_);MK=n(hFe,"STRONG",{});var z$r=s(MK);ico=r(z$r,"squeezebert"),z$r.forEach(t),dco=r(hFe," \u2014 "),pS=n(hFe,"A",{href:!0});var V$r=s(pS);cco=r(V$r,"SqueezeBertForMaskedLM"),V$r.forEach(t),fco=r(hFe," (SqueezeBERT model)"),hFe.forEach(t),mco=i(S),y_=n(S,"LI",{});var pFe=s(y_);EK=n(pFe,"STRONG",{});var W$r=s(EK);gco=r(W$r,"t5"),W$r.forEach(t),hco=r(pFe," \u2014 "),_S=n(pFe,"A",{href:!0});var Q$r=s(_S);pco=r(Q$r,"T5ForConditionalGeneration"),Q$r.forEach(t),_co=r(pFe," (T5 model)"),pFe.forEach(t),uco=i(S),w_=n(S,"LI",{});var _Fe=s(w_);yK=n(_Fe,"STRONG",{});var H$r=s(yK);bco=r(H$r,"tapas"),H$r.forEach(t),vco=r(_Fe," \u2014 "),uS=n(_Fe,"A",{href:!0});var U$r=s(uS);Tco=r(U$r,"TapasForMaskedLM"),U$r.forEach(t),Fco=r(_Fe," (TAPAS model)"),_Fe.forEach(t),Cco=i(S),A_=n(S,"LI",{});var uFe=s(A_);wK=n(uFe,"STRONG",{});var J$r=s(wK);Mco=r(J$r,"transfo-xl"),J$r.forEach(t),Eco=r(uFe," \u2014 "),bS=n(uFe,"A",{href:!0});var Y$r=s(bS);yco=r(Y$r,"TransfoXLLMHeadModel"),Y$r.forEach(t),wco=r(uFe," (Transformer-XL model)"),uFe.forEach(t),Aco=i(S),L_=n(S,"LI",{});var bFe=s(L_);AK=n(bFe,"STRONG",{});var K$r=s(AK);Lco=r(K$r,"unispeech"),K$r.forEach(t),Bco=r(bFe," \u2014 "),vS=n(bFe,"A",{href:!0});var Z$r=s(vS);kco=r(Z$r,"UniSpeechForPreTraining"),Z$r.forEach(t),xco=r(bFe," (UniSpeech model)"),bFe.forEach(t),Rco=i(S),B_=n(S,"LI",{});var vFe=s(B_);LK=n(vFe,"STRONG",{});var eIr=s(LK);Sco=r(eIr,"unispeech-sat"),eIr.forEach(t),Pco=r(vFe," \u2014 "),TS=n(vFe,"A",{href:!0});var oIr=s(TS);$co=r(oIr,"UniSpeechSatForPreTraining"),oIr.forEach(t),Ico=r(vFe," (UniSpeechSat model)"),vFe.forEach(t),jco=i(S),k_=n(S,"LI",{});var TFe=s(k_);BK=n(TFe,"STRONG",{});var rIr=s(BK);Nco=r(rIr,"visual_bert"),rIr.forEach(t),Dco=r(TFe," \u2014 "),FS=n(TFe,"A",{href:!0});var tIr=s(FS);qco=r(tIr,"VisualBertForPreTraining"),tIr.forEach(t),Gco=r(TFe," (VisualBert model)"),TFe.forEach(t),Oco=i(S),x_=n(S,"LI",{});var FFe=s(x_);kK=n(FFe,"STRONG",{});var aIr=s(kK);Xco=r(aIr,"vit_mae"),aIr.forEach(t),zco=r(FFe," \u2014 "),CS=n(FFe,"A",{href:!0});var nIr=s(CS);Vco=r(nIr,"ViTMAEForPreTraining"),nIr.forEach(t),Wco=r(FFe," (ViTMAE model)"),FFe.forEach(t),Qco=i(S),R_=n(S,"LI",{});var CFe=s(R_);xK=n(CFe,"STRONG",{});var sIr=s(xK);Hco=r(sIr,"wav2vec2"),sIr.forEach(t),Uco=r(CFe," \u2014 "),MS=n(CFe,"A",{href:!0});var lIr=s(MS);Jco=r(lIr,"Wav2Vec2ForPreTraining"),lIr.forEach(t),Yco=r(CFe," (Wav2Vec2 model)"),CFe.forEach(t),Kco=i(S),S_=n(S,"LI",{});var MFe=s(S_);RK=n(MFe,"STRONG",{});var iIr=s(RK);Zco=r(iIr,"xlm"),iIr.forEach(t),efo=r(MFe," \u2014 "),ES=n(MFe,"A",{href:!0});var dIr=s(ES);ofo=r(dIr,"XLMWithLMHeadModel"),dIr.forEach(t),rfo=r(MFe," (XLM model)"),MFe.forEach(t),tfo=i(S),P_=n(S,"LI",{});var EFe=s(P_);SK=n(EFe,"STRONG",{});var cIr=s(SK);afo=r(cIr,"xlm-roberta"),cIr.forEach(t),nfo=r(EFe," \u2014 "),yS=n(EFe,"A",{href:!0});var fIr=s(yS);sfo=r(fIr,"XLMRobertaForMaskedLM"),fIr.forEach(t),lfo=r(EFe," (XLM-RoBERTa model)"),EFe.forEach(t),ifo=i(S),$_=n(S,"LI",{});var yFe=s($_);PK=n(yFe,"STRONG",{});var mIr=s(PK);dfo=r(mIr,"xlm-roberta-xl"),mIr.forEach(t),cfo=r(yFe," \u2014 "),wS=n(yFe,"A",{href:!0});var gIr=s(wS);ffo=r(gIr,"XLMRobertaXLForMaskedLM"),gIr.forEach(t),mfo=r(yFe," (XLM-RoBERTa-XL model)"),yFe.forEach(t),gfo=i(S),I_=n(S,"LI",{});var wFe=s(I_);$K=n(wFe,"STRONG",{});var hIr=s($K);hfo=r(hIr,"xlnet"),hIr.forEach(t),pfo=r(wFe," \u2014 "),AS=n(wFe,"A",{href:!0});var pIr=s(AS);_fo=r(pIr,"XLNetLMHeadModel"),pIr.forEach(t),ufo=r(wFe," (XLNet model)"),wFe.forEach(t),S.forEach(t),bfo=i(Pt),j_=n(Pt,"P",{});var AFe=s(j_);vfo=r(AFe,"The model is set in evaluation mode by default using "),IK=n(AFe,"CODE",{});var _Ir=s(IK);Tfo=r(_Ir,"model.eval()"),_Ir.forEach(t),Ffo=r(AFe,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),jK=n(AFe,"CODE",{});var uIr=s(jK);Cfo=r(uIr,"model.train()"),uIr.forEach(t),AFe.forEach(t),Mfo=i(Pt),NK=n(Pt,"P",{});var bIr=s(NK);Efo=r(bIr,"Examples:"),bIr.forEach(t),yfo=i(Pt),m(HE.$$.fragment,Pt),Pt.forEach(t),qs.forEach(t),T8e=i(d),Wi=n(d,"H2",{class:!0});var ABe=s(Wi);N_=n(ABe,"A",{id:!0,class:!0,href:!0});var vIr=s(N_);DK=n(vIr,"SPAN",{});var TIr=s(DK);m(UE.$$.fragment,TIr),TIr.forEach(t),vIr.forEach(t),wfo=i(ABe),qK=n(ABe,"SPAN",{});var FIr=s(qK);Afo=r(FIr,"AutoModelForCausalLM"),FIr.forEach(t),ABe.forEach(t),F8e=i(d),Qo=n(d,"DIV",{class:!0});var Os=s(Qo);m(JE.$$.fragment,Os),Lfo=i(Os),Qi=n(Os,"P",{});var JX=s(Qi);Bfo=r(JX,`This is a generic model class that will be instantiated as one of the model classes of the library (with a causal language modeling head) when created
with the `),GK=n(JX,"CODE",{});var CIr=s(GK);kfo=r(CIr,"from_pretrained()"),CIr.forEach(t),xfo=r(JX,"class method or the "),OK=n(JX,"CODE",{});var MIr=s(OK);Rfo=r(MIr,"from_config()"),MIr.forEach(t),Sfo=r(JX,`class
method.`),JX.forEach(t),Pfo=i(Os),YE=n(Os,"P",{});var LBe=s(YE);$fo=r(LBe,"This class cannot be instantiated directly using "),XK=n(LBe,"CODE",{});var EIr=s(XK);Ifo=r(EIr,"__init__()"),EIr.forEach(t),jfo=r(LBe," (throws an error)."),LBe.forEach(t),Nfo=i(Os),qr=n(Os,"DIV",{class:!0});var Xs=s(qr);m(KE.$$.fragment,Xs),Dfo=i(Xs),zK=n(Xs,"P",{});var yIr=s(zK);qfo=r(yIr,"Instantiates one of the model classes of the library (with a causal language modeling head) from a configuration."),yIr.forEach(t),Gfo=i(Xs),Hi=n(Xs,"P",{});var YX=s(Hi);Ofo=r(YX,`Note:
Loading a model from its configuration file does `),VK=n(YX,"STRONG",{});var wIr=s(VK);Xfo=r(wIr,"not"),wIr.forEach(t),zfo=r(YX,` load the model weights. It only affects the
model\u2019s configuration. Use `),WK=n(YX,"CODE",{});var AIr=s(WK);Vfo=r(AIr,"from_pretrained()"),AIr.forEach(t),Wfo=r(YX,"to load the model weights."),YX.forEach(t),Qfo=i(Xs),QK=n(Xs,"P",{});var LIr=s(QK);Hfo=r(LIr,"Examples:"),LIr.forEach(t),Ufo=i(Xs),m(ZE.$$.fragment,Xs),Xs.forEach(t),Jfo=i(Os),Re=n(Os,"DIV",{class:!0});var $t=s(Re);m(e3.$$.fragment,$t),Yfo=i($t),HK=n($t,"P",{});var BIr=s(HK);Kfo=r(BIr,"Instantiate one of the model classes of the library (with a causal language modeling head) from a pretrained model."),BIr.forEach(t),Zfo=i($t),Ga=n($t,"P",{});var dM=s(Ga);emo=r(dM,"The model class to instantiate is selected based on the "),UK=n(dM,"CODE",{});var kIr=s(UK);omo=r(kIr,"model_type"),kIr.forEach(t),rmo=r(dM,` property of the config object (either
passed as an argument or loaded from `),JK=n(dM,"CODE",{});var xIr=s(JK);tmo=r(xIr,"pretrained_model_name_or_path"),xIr.forEach(t),amo=r(dM,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),YK=n(dM,"CODE",{});var RIr=s(YK);nmo=r(RIr,"pretrained_model_name_or_path"),RIr.forEach(t),smo=r(dM,":"),dM.forEach(t),lmo=i($t),$=n($t,"UL",{});var j=s($);D_=n(j,"LI",{});var LFe=s(D_);KK=n(LFe,"STRONG",{});var SIr=s(KK);imo=r(SIr,"bart"),SIr.forEach(t),dmo=r(LFe," \u2014 "),LS=n(LFe,"A",{href:!0});var PIr=s(LS);cmo=r(PIr,"BartForCausalLM"),PIr.forEach(t),fmo=r(LFe," (BART model)"),LFe.forEach(t),mmo=i(j),q_=n(j,"LI",{});var BFe=s(q_);ZK=n(BFe,"STRONG",{});var $Ir=s(ZK);gmo=r($Ir,"bert"),$Ir.forEach(t),hmo=r(BFe," \u2014 "),BS=n(BFe,"A",{href:!0});var IIr=s(BS);pmo=r(IIr,"BertLMHeadModel"),IIr.forEach(t),_mo=r(BFe," (BERT model)"),BFe.forEach(t),umo=i(j),G_=n(j,"LI",{});var kFe=s(G_);eZ=n(kFe,"STRONG",{});var jIr=s(eZ);bmo=r(jIr,"bert-generation"),jIr.forEach(t),vmo=r(kFe," \u2014 "),kS=n(kFe,"A",{href:!0});var NIr=s(kS);Tmo=r(NIr,"BertGenerationDecoder"),NIr.forEach(t),Fmo=r(kFe," (Bert Generation model)"),kFe.forEach(t),Cmo=i(j),O_=n(j,"LI",{});var xFe=s(O_);oZ=n(xFe,"STRONG",{});var DIr=s(oZ);Mmo=r(DIr,"big_bird"),DIr.forEach(t),Emo=r(xFe," \u2014 "),xS=n(xFe,"A",{href:!0});var qIr=s(xS);ymo=r(qIr,"BigBirdForCausalLM"),qIr.forEach(t),wmo=r(xFe," (BigBird model)"),xFe.forEach(t),Amo=i(j),X_=n(j,"LI",{});var RFe=s(X_);rZ=n(RFe,"STRONG",{});var GIr=s(rZ);Lmo=r(GIr,"bigbird_pegasus"),GIr.forEach(t),Bmo=r(RFe," \u2014 "),RS=n(RFe,"A",{href:!0});var OIr=s(RS);kmo=r(OIr,"BigBirdPegasusForCausalLM"),OIr.forEach(t),xmo=r(RFe," (BigBirdPegasus model)"),RFe.forEach(t),Rmo=i(j),z_=n(j,"LI",{});var SFe=s(z_);tZ=n(SFe,"STRONG",{});var XIr=s(tZ);Smo=r(XIr,"blenderbot"),XIr.forEach(t),Pmo=r(SFe," \u2014 "),SS=n(SFe,"A",{href:!0});var zIr=s(SS);$mo=r(zIr,"BlenderbotForCausalLM"),zIr.forEach(t),Imo=r(SFe," (Blenderbot model)"),SFe.forEach(t),jmo=i(j),V_=n(j,"LI",{});var PFe=s(V_);aZ=n(PFe,"STRONG",{});var VIr=s(aZ);Nmo=r(VIr,"blenderbot-small"),VIr.forEach(t),Dmo=r(PFe," \u2014 "),PS=n(PFe,"A",{href:!0});var WIr=s(PS);qmo=r(WIr,"BlenderbotSmallForCausalLM"),WIr.forEach(t),Gmo=r(PFe," (BlenderbotSmall model)"),PFe.forEach(t),Omo=i(j),W_=n(j,"LI",{});var $Fe=s(W_);nZ=n($Fe,"STRONG",{});var QIr=s(nZ);Xmo=r(QIr,"camembert"),QIr.forEach(t),zmo=r($Fe," \u2014 "),$S=n($Fe,"A",{href:!0});var HIr=s($S);Vmo=r(HIr,"CamembertForCausalLM"),HIr.forEach(t),Wmo=r($Fe," (CamemBERT model)"),$Fe.forEach(t),Qmo=i(j),Q_=n(j,"LI",{});var IFe=s(Q_);sZ=n(IFe,"STRONG",{});var UIr=s(sZ);Hmo=r(UIr,"ctrl"),UIr.forEach(t),Umo=r(IFe," \u2014 "),IS=n(IFe,"A",{href:!0});var JIr=s(IS);Jmo=r(JIr,"CTRLLMHeadModel"),JIr.forEach(t),Ymo=r(IFe," (CTRL model)"),IFe.forEach(t),Kmo=i(j),H_=n(j,"LI",{});var jFe=s(H_);lZ=n(jFe,"STRONG",{});var YIr=s(lZ);Zmo=r(YIr,"electra"),YIr.forEach(t),ego=r(jFe," \u2014 "),jS=n(jFe,"A",{href:!0});var KIr=s(jS);ogo=r(KIr,"ElectraForCausalLM"),KIr.forEach(t),rgo=r(jFe," (ELECTRA model)"),jFe.forEach(t),tgo=i(j),U_=n(j,"LI",{});var NFe=s(U_);iZ=n(NFe,"STRONG",{});var ZIr=s(iZ);ago=r(ZIr,"gpt2"),ZIr.forEach(t),ngo=r(NFe," \u2014 "),NS=n(NFe,"A",{href:!0});var ejr=s(NS);sgo=r(ejr,"GPT2LMHeadModel"),ejr.forEach(t),lgo=r(NFe," (OpenAI GPT-2 model)"),NFe.forEach(t),igo=i(j),J_=n(j,"LI",{});var DFe=s(J_);dZ=n(DFe,"STRONG",{});var ojr=s(dZ);dgo=r(ojr,"gpt_neo"),ojr.forEach(t),cgo=r(DFe," \u2014 "),DS=n(DFe,"A",{href:!0});var rjr=s(DS);fgo=r(rjr,"GPTNeoForCausalLM"),rjr.forEach(t),mgo=r(DFe," (GPT Neo model)"),DFe.forEach(t),ggo=i(j),Y_=n(j,"LI",{});var qFe=s(Y_);cZ=n(qFe,"STRONG",{});var tjr=s(cZ);hgo=r(tjr,"gptj"),tjr.forEach(t),pgo=r(qFe," \u2014 "),qS=n(qFe,"A",{href:!0});var ajr=s(qS);_go=r(ajr,"GPTJForCausalLM"),ajr.forEach(t),ugo=r(qFe," (GPT-J model)"),qFe.forEach(t),bgo=i(j),K_=n(j,"LI",{});var GFe=s(K_);fZ=n(GFe,"STRONG",{});var njr=s(fZ);vgo=r(njr,"marian"),njr.forEach(t),Tgo=r(GFe," \u2014 "),GS=n(GFe,"A",{href:!0});var sjr=s(GS);Fgo=r(sjr,"MarianForCausalLM"),sjr.forEach(t),Cgo=r(GFe," (Marian model)"),GFe.forEach(t),Mgo=i(j),Z_=n(j,"LI",{});var OFe=s(Z_);mZ=n(OFe,"STRONG",{});var ljr=s(mZ);Ego=r(ljr,"mbart"),ljr.forEach(t),ygo=r(OFe," \u2014 "),OS=n(OFe,"A",{href:!0});var ijr=s(OS);wgo=r(ijr,"MBartForCausalLM"),ijr.forEach(t),Ago=r(OFe," (mBART model)"),OFe.forEach(t),Lgo=i(j),eu=n(j,"LI",{});var XFe=s(eu);gZ=n(XFe,"STRONG",{});var djr=s(gZ);Bgo=r(djr,"megatron-bert"),djr.forEach(t),kgo=r(XFe," \u2014 "),XS=n(XFe,"A",{href:!0});var cjr=s(XS);xgo=r(cjr,"MegatronBertForCausalLM"),cjr.forEach(t),Rgo=r(XFe," (MegatronBert model)"),XFe.forEach(t),Sgo=i(j),ou=n(j,"LI",{});var zFe=s(ou);hZ=n(zFe,"STRONG",{});var fjr=s(hZ);Pgo=r(fjr,"openai-gpt"),fjr.forEach(t),$go=r(zFe," \u2014 "),zS=n(zFe,"A",{href:!0});var mjr=s(zS);Igo=r(mjr,"OpenAIGPTLMHeadModel"),mjr.forEach(t),jgo=r(zFe," (OpenAI GPT model)"),zFe.forEach(t),Ngo=i(j),ru=n(j,"LI",{});var VFe=s(ru);pZ=n(VFe,"STRONG",{});var gjr=s(pZ);Dgo=r(gjr,"pegasus"),gjr.forEach(t),qgo=r(VFe," \u2014 "),VS=n(VFe,"A",{href:!0});var hjr=s(VS);Ggo=r(hjr,"PegasusForCausalLM"),hjr.forEach(t),Ogo=r(VFe," (Pegasus model)"),VFe.forEach(t),Xgo=i(j),tu=n(j,"LI",{});var WFe=s(tu);_Z=n(WFe,"STRONG",{});var pjr=s(_Z);zgo=r(pjr,"plbart"),pjr.forEach(t),Vgo=r(WFe," \u2014 "),WS=n(WFe,"A",{href:!0});var _jr=s(WS);Wgo=r(_jr,"PLBartForCausalLM"),_jr.forEach(t),Qgo=r(WFe," (PLBart model)"),WFe.forEach(t),Hgo=i(j),au=n(j,"LI",{});var QFe=s(au);uZ=n(QFe,"STRONG",{});var ujr=s(uZ);Ugo=r(ujr,"prophetnet"),ujr.forEach(t),Jgo=r(QFe," \u2014 "),QS=n(QFe,"A",{href:!0});var bjr=s(QS);Ygo=r(bjr,"ProphetNetForCausalLM"),bjr.forEach(t),Kgo=r(QFe," (ProphetNet model)"),QFe.forEach(t),Zgo=i(j),nu=n(j,"LI",{});var HFe=s(nu);bZ=n(HFe,"STRONG",{});var vjr=s(bZ);eho=r(vjr,"qdqbert"),vjr.forEach(t),oho=r(HFe," \u2014 "),HS=n(HFe,"A",{href:!0});var Tjr=s(HS);rho=r(Tjr,"QDQBertLMHeadModel"),Tjr.forEach(t),tho=r(HFe," (QDQBert model)"),HFe.forEach(t),aho=i(j),su=n(j,"LI",{});var UFe=s(su);vZ=n(UFe,"STRONG",{});var Fjr=s(vZ);nho=r(Fjr,"reformer"),Fjr.forEach(t),sho=r(UFe," \u2014 "),US=n(UFe,"A",{href:!0});var Cjr=s(US);lho=r(Cjr,"ReformerModelWithLMHead"),Cjr.forEach(t),iho=r(UFe," (Reformer model)"),UFe.forEach(t),dho=i(j),lu=n(j,"LI",{});var JFe=s(lu);TZ=n(JFe,"STRONG",{});var Mjr=s(TZ);cho=r(Mjr,"rembert"),Mjr.forEach(t),fho=r(JFe," \u2014 "),JS=n(JFe,"A",{href:!0});var Ejr=s(JS);mho=r(Ejr,"RemBertForCausalLM"),Ejr.forEach(t),gho=r(JFe," (RemBERT model)"),JFe.forEach(t),hho=i(j),iu=n(j,"LI",{});var YFe=s(iu);FZ=n(YFe,"STRONG",{});var yjr=s(FZ);pho=r(yjr,"roberta"),yjr.forEach(t),_ho=r(YFe," \u2014 "),YS=n(YFe,"A",{href:!0});var wjr=s(YS);uho=r(wjr,"RobertaForCausalLM"),wjr.forEach(t),bho=r(YFe," (RoBERTa model)"),YFe.forEach(t),vho=i(j),du=n(j,"LI",{});var KFe=s(du);CZ=n(KFe,"STRONG",{});var Ajr=s(CZ);Tho=r(Ajr,"roformer"),Ajr.forEach(t),Fho=r(KFe," \u2014 "),KS=n(KFe,"A",{href:!0});var Ljr=s(KS);Cho=r(Ljr,"RoFormerForCausalLM"),Ljr.forEach(t),Mho=r(KFe," (RoFormer model)"),KFe.forEach(t),Eho=i(j),cu=n(j,"LI",{});var ZFe=s(cu);MZ=n(ZFe,"STRONG",{});var Bjr=s(MZ);yho=r(Bjr,"speech_to_text_2"),Bjr.forEach(t),who=r(ZFe," \u2014 "),ZS=n(ZFe,"A",{href:!0});var kjr=s(ZS);Aho=r(kjr,"Speech2Text2ForCausalLM"),kjr.forEach(t),Lho=r(ZFe," (Speech2Text2 model)"),ZFe.forEach(t),Bho=i(j),fu=n(j,"LI",{});var eCe=s(fu);EZ=n(eCe,"STRONG",{});var xjr=s(EZ);kho=r(xjr,"transfo-xl"),xjr.forEach(t),xho=r(eCe," \u2014 "),eP=n(eCe,"A",{href:!0});var Rjr=s(eP);Rho=r(Rjr,"TransfoXLLMHeadModel"),Rjr.forEach(t),Sho=r(eCe," (Transformer-XL model)"),eCe.forEach(t),Pho=i(j),mu=n(j,"LI",{});var oCe=s(mu);yZ=n(oCe,"STRONG",{});var Sjr=s(yZ);$ho=r(Sjr,"trocr"),Sjr.forEach(t),Iho=r(oCe," \u2014 "),oP=n(oCe,"A",{href:!0});var Pjr=s(oP);jho=r(Pjr,"TrOCRForCausalLM"),Pjr.forEach(t),Nho=r(oCe," (TrOCR model)"),oCe.forEach(t),Dho=i(j),gu=n(j,"LI",{});var rCe=s(gu);wZ=n(rCe,"STRONG",{});var $jr=s(wZ);qho=r($jr,"xglm"),$jr.forEach(t),Gho=r(rCe," \u2014 "),rP=n(rCe,"A",{href:!0});var Ijr=s(rP);Oho=r(Ijr,"XGLMForCausalLM"),Ijr.forEach(t),Xho=r(rCe," (XGLM model)"),rCe.forEach(t),zho=i(j),hu=n(j,"LI",{});var tCe=s(hu);AZ=n(tCe,"STRONG",{});var jjr=s(AZ);Vho=r(jjr,"xlm"),jjr.forEach(t),Who=r(tCe," \u2014 "),tP=n(tCe,"A",{href:!0});var Njr=s(tP);Qho=r(Njr,"XLMWithLMHeadModel"),Njr.forEach(t),Hho=r(tCe," (XLM model)"),tCe.forEach(t),Uho=i(j),pu=n(j,"LI",{});var aCe=s(pu);LZ=n(aCe,"STRONG",{});var Djr=s(LZ);Jho=r(Djr,"xlm-prophetnet"),Djr.forEach(t),Yho=r(aCe," \u2014 "),aP=n(aCe,"A",{href:!0});var qjr=s(aP);Kho=r(qjr,"XLMProphetNetForCausalLM"),qjr.forEach(t),Zho=r(aCe," (XLMProphetNet model)"),aCe.forEach(t),epo=i(j),_u=n(j,"LI",{});var nCe=s(_u);BZ=n(nCe,"STRONG",{});var Gjr=s(BZ);opo=r(Gjr,"xlm-roberta"),Gjr.forEach(t),rpo=r(nCe," \u2014 "),nP=n(nCe,"A",{href:!0});var Ojr=s(nP);tpo=r(Ojr,"XLMRobertaForCausalLM"),Ojr.forEach(t),apo=r(nCe," (XLM-RoBERTa model)"),nCe.forEach(t),npo=i(j),uu=n(j,"LI",{});var sCe=s(uu);kZ=n(sCe,"STRONG",{});var Xjr=s(kZ);spo=r(Xjr,"xlm-roberta-xl"),Xjr.forEach(t),lpo=r(sCe," \u2014 "),sP=n(sCe,"A",{href:!0});var zjr=s(sP);ipo=r(zjr,"XLMRobertaXLForCausalLM"),zjr.forEach(t),dpo=r(sCe," (XLM-RoBERTa-XL model)"),sCe.forEach(t),cpo=i(j),bu=n(j,"LI",{});var lCe=s(bu);xZ=n(lCe,"STRONG",{});var Vjr=s(xZ);fpo=r(Vjr,"xlnet"),Vjr.forEach(t),mpo=r(lCe," \u2014 "),lP=n(lCe,"A",{href:!0});var Wjr=s(lP);gpo=r(Wjr,"XLNetLMHeadModel"),Wjr.forEach(t),hpo=r(lCe," (XLNet model)"),lCe.forEach(t),j.forEach(t),ppo=i($t),vu=n($t,"P",{});var iCe=s(vu);_po=r(iCe,"The model is set in evaluation mode by default using "),RZ=n(iCe,"CODE",{});var Qjr=s(RZ);upo=r(Qjr,"model.eval()"),Qjr.forEach(t),bpo=r(iCe,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),SZ=n(iCe,"CODE",{});var Hjr=s(SZ);vpo=r(Hjr,"model.train()"),Hjr.forEach(t),iCe.forEach(t),Tpo=i($t),PZ=n($t,"P",{});var Ujr=s(PZ);Fpo=r(Ujr,"Examples:"),Ujr.forEach(t),Cpo=i($t),m(o3.$$.fragment,$t),$t.forEach(t),Os.forEach(t),C8e=i(d),Ui=n(d,"H2",{class:!0});var BBe=s(Ui);Tu=n(BBe,"A",{id:!0,class:!0,href:!0});var Jjr=s(Tu);$Z=n(Jjr,"SPAN",{});var Yjr=s($Z);m(r3.$$.fragment,Yjr),Yjr.forEach(t),Jjr.forEach(t),Mpo=i(BBe),IZ=n(BBe,"SPAN",{});var Kjr=s(IZ);Epo=r(Kjr,"AutoModelForMaskedLM"),Kjr.forEach(t),BBe.forEach(t),M8e=i(d),Ho=n(d,"DIV",{class:!0});var zs=s(Ho);m(t3.$$.fragment,zs),ypo=i(zs),Ji=n(zs,"P",{});var KX=s(Ji);wpo=r(KX,`This is a generic model class that will be instantiated as one of the model classes of the library (with a masked language modeling head) when created
with the `),jZ=n(KX,"CODE",{});var Zjr=s(jZ);Apo=r(Zjr,"from_pretrained()"),Zjr.forEach(t),Lpo=r(KX,"class method or the "),NZ=n(KX,"CODE",{});var eNr=s(NZ);Bpo=r(eNr,"from_config()"),eNr.forEach(t),kpo=r(KX,`class
method.`),KX.forEach(t),xpo=i(zs),a3=n(zs,"P",{});var kBe=s(a3);Rpo=r(kBe,"This class cannot be instantiated directly using "),DZ=n(kBe,"CODE",{});var oNr=s(DZ);Spo=r(oNr,"__init__()"),oNr.forEach(t),Ppo=r(kBe," (throws an error)."),kBe.forEach(t),$po=i(zs),Gr=n(zs,"DIV",{class:!0});var Vs=s(Gr);m(n3.$$.fragment,Vs),Ipo=i(Vs),qZ=n(Vs,"P",{});var rNr=s(qZ);jpo=r(rNr,"Instantiates one of the model classes of the library (with a masked language modeling head) from a configuration."),rNr.forEach(t),Npo=i(Vs),Yi=n(Vs,"P",{});var ZX=s(Yi);Dpo=r(ZX,`Note:
Loading a model from its configuration file does `),GZ=n(ZX,"STRONG",{});var tNr=s(GZ);qpo=r(tNr,"not"),tNr.forEach(t),Gpo=r(ZX,` load the model weights. It only affects the
model\u2019s configuration. Use `),OZ=n(ZX,"CODE",{});var aNr=s(OZ);Opo=r(aNr,"from_pretrained()"),aNr.forEach(t),Xpo=r(ZX,"to load the model weights."),ZX.forEach(t),zpo=i(Vs),XZ=n(Vs,"P",{});var nNr=s(XZ);Vpo=r(nNr,"Examples:"),nNr.forEach(t),Wpo=i(Vs),m(s3.$$.fragment,Vs),Vs.forEach(t),Qpo=i(zs),Se=n(zs,"DIV",{class:!0});var It=s(Se);m(l3.$$.fragment,It),Hpo=i(It),zZ=n(It,"P",{});var sNr=s(zZ);Upo=r(sNr,"Instantiate one of the model classes of the library (with a masked language modeling head) from a pretrained model."),sNr.forEach(t),Jpo=i(It),Oa=n(It,"P",{});var cM=s(Oa);Ypo=r(cM,"The model class to instantiate is selected based on the "),VZ=n(cM,"CODE",{});var lNr=s(VZ);Kpo=r(lNr,"model_type"),lNr.forEach(t),Zpo=r(cM,` property of the config object (either
passed as an argument or loaded from `),WZ=n(cM,"CODE",{});var iNr=s(WZ);e_o=r(iNr,"pretrained_model_name_or_path"),iNr.forEach(t),o_o=r(cM,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),QZ=n(cM,"CODE",{});var dNr=s(QZ);r_o=r(dNr,"pretrained_model_name_or_path"),dNr.forEach(t),t_o=r(cM,":"),cM.forEach(t),a_o=i(It),I=n(It,"UL",{});var N=s(I);Fu=n(N,"LI",{});var dCe=s(Fu);HZ=n(dCe,"STRONG",{});var cNr=s(HZ);n_o=r(cNr,"albert"),cNr.forEach(t),s_o=r(dCe," \u2014 "),iP=n(dCe,"A",{href:!0});var fNr=s(iP);l_o=r(fNr,"AlbertForMaskedLM"),fNr.forEach(t),i_o=r(dCe," (ALBERT model)"),dCe.forEach(t),d_o=i(N),Cu=n(N,"LI",{});var cCe=s(Cu);UZ=n(cCe,"STRONG",{});var mNr=s(UZ);c_o=r(mNr,"bart"),mNr.forEach(t),f_o=r(cCe," \u2014 "),dP=n(cCe,"A",{href:!0});var gNr=s(dP);m_o=r(gNr,"BartForConditionalGeneration"),gNr.forEach(t),g_o=r(cCe," (BART model)"),cCe.forEach(t),h_o=i(N),Mu=n(N,"LI",{});var fCe=s(Mu);JZ=n(fCe,"STRONG",{});var hNr=s(JZ);p_o=r(hNr,"bert"),hNr.forEach(t),__o=r(fCe," \u2014 "),cP=n(fCe,"A",{href:!0});var pNr=s(cP);u_o=r(pNr,"BertForMaskedLM"),pNr.forEach(t),b_o=r(fCe," (BERT model)"),fCe.forEach(t),v_o=i(N),Eu=n(N,"LI",{});var mCe=s(Eu);YZ=n(mCe,"STRONG",{});var _Nr=s(YZ);T_o=r(_Nr,"big_bird"),_Nr.forEach(t),F_o=r(mCe," \u2014 "),fP=n(mCe,"A",{href:!0});var uNr=s(fP);C_o=r(uNr,"BigBirdForMaskedLM"),uNr.forEach(t),M_o=r(mCe," (BigBird model)"),mCe.forEach(t),E_o=i(N),yu=n(N,"LI",{});var gCe=s(yu);KZ=n(gCe,"STRONG",{});var bNr=s(KZ);y_o=r(bNr,"camembert"),bNr.forEach(t),w_o=r(gCe," \u2014 "),mP=n(gCe,"A",{href:!0});var vNr=s(mP);A_o=r(vNr,"CamembertForMaskedLM"),vNr.forEach(t),L_o=r(gCe," (CamemBERT model)"),gCe.forEach(t),B_o=i(N),wu=n(N,"LI",{});var hCe=s(wu);ZZ=n(hCe,"STRONG",{});var TNr=s(ZZ);k_o=r(TNr,"convbert"),TNr.forEach(t),x_o=r(hCe," \u2014 "),gP=n(hCe,"A",{href:!0});var FNr=s(gP);R_o=r(FNr,"ConvBertForMaskedLM"),FNr.forEach(t),S_o=r(hCe," (ConvBERT model)"),hCe.forEach(t),P_o=i(N),Au=n(N,"LI",{});var pCe=s(Au);eee=n(pCe,"STRONG",{});var CNr=s(eee);$_o=r(CNr,"deberta"),CNr.forEach(t),I_o=r(pCe," \u2014 "),hP=n(pCe,"A",{href:!0});var MNr=s(hP);j_o=r(MNr,"DebertaForMaskedLM"),MNr.forEach(t),N_o=r(pCe," (DeBERTa model)"),pCe.forEach(t),D_o=i(N),Lu=n(N,"LI",{});var _Ce=s(Lu);oee=n(_Ce,"STRONG",{});var ENr=s(oee);q_o=r(ENr,"deberta-v2"),ENr.forEach(t),G_o=r(_Ce," \u2014 "),pP=n(_Ce,"A",{href:!0});var yNr=s(pP);O_o=r(yNr,"DebertaV2ForMaskedLM"),yNr.forEach(t),X_o=r(_Ce," (DeBERTa-v2 model)"),_Ce.forEach(t),z_o=i(N),Bu=n(N,"LI",{});var uCe=s(Bu);ree=n(uCe,"STRONG",{});var wNr=s(ree);V_o=r(wNr,"distilbert"),wNr.forEach(t),W_o=r(uCe," \u2014 "),_P=n(uCe,"A",{href:!0});var ANr=s(_P);Q_o=r(ANr,"DistilBertForMaskedLM"),ANr.forEach(t),H_o=r(uCe," (DistilBERT model)"),uCe.forEach(t),U_o=i(N),ku=n(N,"LI",{});var bCe=s(ku);tee=n(bCe,"STRONG",{});var LNr=s(tee);J_o=r(LNr,"electra"),LNr.forEach(t),Y_o=r(bCe," \u2014 "),uP=n(bCe,"A",{href:!0});var BNr=s(uP);K_o=r(BNr,"ElectraForMaskedLM"),BNr.forEach(t),Z_o=r(bCe," (ELECTRA model)"),bCe.forEach(t),euo=i(N),xu=n(N,"LI",{});var vCe=s(xu);aee=n(vCe,"STRONG",{});var kNr=s(aee);ouo=r(kNr,"flaubert"),kNr.forEach(t),ruo=r(vCe," \u2014 "),bP=n(vCe,"A",{href:!0});var xNr=s(bP);tuo=r(xNr,"FlaubertWithLMHeadModel"),xNr.forEach(t),auo=r(vCe," (FlauBERT model)"),vCe.forEach(t),nuo=i(N),Ru=n(N,"LI",{});var TCe=s(Ru);nee=n(TCe,"STRONG",{});var RNr=s(nee);suo=r(RNr,"fnet"),RNr.forEach(t),luo=r(TCe," \u2014 "),vP=n(TCe,"A",{href:!0});var SNr=s(vP);iuo=r(SNr,"FNetForMaskedLM"),SNr.forEach(t),duo=r(TCe," (FNet model)"),TCe.forEach(t),cuo=i(N),Su=n(N,"LI",{});var FCe=s(Su);see=n(FCe,"STRONG",{});var PNr=s(see);fuo=r(PNr,"funnel"),PNr.forEach(t),muo=r(FCe," \u2014 "),TP=n(FCe,"A",{href:!0});var $Nr=s(TP);guo=r($Nr,"FunnelForMaskedLM"),$Nr.forEach(t),huo=r(FCe," (Funnel Transformer model)"),FCe.forEach(t),puo=i(N),Pu=n(N,"LI",{});var CCe=s(Pu);lee=n(CCe,"STRONG",{});var INr=s(lee);_uo=r(INr,"ibert"),INr.forEach(t),uuo=r(CCe," \u2014 "),FP=n(CCe,"A",{href:!0});var jNr=s(FP);buo=r(jNr,"IBertForMaskedLM"),jNr.forEach(t),vuo=r(CCe," (I-BERT model)"),CCe.forEach(t),Tuo=i(N),$u=n(N,"LI",{});var MCe=s($u);iee=n(MCe,"STRONG",{});var NNr=s(iee);Fuo=r(NNr,"layoutlm"),NNr.forEach(t),Cuo=r(MCe," \u2014 "),CP=n(MCe,"A",{href:!0});var DNr=s(CP);Muo=r(DNr,"LayoutLMForMaskedLM"),DNr.forEach(t),Euo=r(MCe," (LayoutLM model)"),MCe.forEach(t),yuo=i(N),Iu=n(N,"LI",{});var ECe=s(Iu);dee=n(ECe,"STRONG",{});var qNr=s(dee);wuo=r(qNr,"longformer"),qNr.forEach(t),Auo=r(ECe," \u2014 "),MP=n(ECe,"A",{href:!0});var GNr=s(MP);Luo=r(GNr,"LongformerForMaskedLM"),GNr.forEach(t),Buo=r(ECe," (Longformer model)"),ECe.forEach(t),kuo=i(N),ju=n(N,"LI",{});var yCe=s(ju);cee=n(yCe,"STRONG",{});var ONr=s(cee);xuo=r(ONr,"mbart"),ONr.forEach(t),Ruo=r(yCe," \u2014 "),EP=n(yCe,"A",{href:!0});var XNr=s(EP);Suo=r(XNr,"MBartForConditionalGeneration"),XNr.forEach(t),Puo=r(yCe," (mBART model)"),yCe.forEach(t),$uo=i(N),Nu=n(N,"LI",{});var wCe=s(Nu);fee=n(wCe,"STRONG",{});var zNr=s(fee);Iuo=r(zNr,"megatron-bert"),zNr.forEach(t),juo=r(wCe," \u2014 "),yP=n(wCe,"A",{href:!0});var VNr=s(yP);Nuo=r(VNr,"MegatronBertForMaskedLM"),VNr.forEach(t),Duo=r(wCe," (MegatronBert model)"),wCe.forEach(t),quo=i(N),Du=n(N,"LI",{});var ACe=s(Du);mee=n(ACe,"STRONG",{});var WNr=s(mee);Guo=r(WNr,"mobilebert"),WNr.forEach(t),Ouo=r(ACe," \u2014 "),wP=n(ACe,"A",{href:!0});var QNr=s(wP);Xuo=r(QNr,"MobileBertForMaskedLM"),QNr.forEach(t),zuo=r(ACe," (MobileBERT model)"),ACe.forEach(t),Vuo=i(N),qu=n(N,"LI",{});var LCe=s(qu);gee=n(LCe,"STRONG",{});var HNr=s(gee);Wuo=r(HNr,"mpnet"),HNr.forEach(t),Quo=r(LCe," \u2014 "),AP=n(LCe,"A",{href:!0});var UNr=s(AP);Huo=r(UNr,"MPNetForMaskedLM"),UNr.forEach(t),Uuo=r(LCe," (MPNet model)"),LCe.forEach(t),Juo=i(N),Gu=n(N,"LI",{});var BCe=s(Gu);hee=n(BCe,"STRONG",{});var JNr=s(hee);Yuo=r(JNr,"nystromformer"),JNr.forEach(t),Kuo=r(BCe," \u2014 "),LP=n(BCe,"A",{href:!0});var YNr=s(LP);Zuo=r(YNr,"NystromformerForMaskedLM"),YNr.forEach(t),e1o=r(BCe," (Nystromformer model)"),BCe.forEach(t),o1o=i(N),Ou=n(N,"LI",{});var kCe=s(Ou);pee=n(kCe,"STRONG",{});var KNr=s(pee);r1o=r(KNr,"perceiver"),KNr.forEach(t),t1o=r(kCe," \u2014 "),BP=n(kCe,"A",{href:!0});var ZNr=s(BP);a1o=r(ZNr,"PerceiverForMaskedLM"),ZNr.forEach(t),n1o=r(kCe," (Perceiver model)"),kCe.forEach(t),s1o=i(N),Xu=n(N,"LI",{});var xCe=s(Xu);_ee=n(xCe,"STRONG",{});var eDr=s(_ee);l1o=r(eDr,"qdqbert"),eDr.forEach(t),i1o=r(xCe," \u2014 "),kP=n(xCe,"A",{href:!0});var oDr=s(kP);d1o=r(oDr,"QDQBertForMaskedLM"),oDr.forEach(t),c1o=r(xCe," (QDQBert model)"),xCe.forEach(t),f1o=i(N),zu=n(N,"LI",{});var RCe=s(zu);uee=n(RCe,"STRONG",{});var rDr=s(uee);m1o=r(rDr,"reformer"),rDr.forEach(t),g1o=r(RCe," \u2014 "),xP=n(RCe,"A",{href:!0});var tDr=s(xP);h1o=r(tDr,"ReformerForMaskedLM"),tDr.forEach(t),p1o=r(RCe," (Reformer model)"),RCe.forEach(t),_1o=i(N),Vu=n(N,"LI",{});var SCe=s(Vu);bee=n(SCe,"STRONG",{});var aDr=s(bee);u1o=r(aDr,"rembert"),aDr.forEach(t),b1o=r(SCe," \u2014 "),RP=n(SCe,"A",{href:!0});var nDr=s(RP);v1o=r(nDr,"RemBertForMaskedLM"),nDr.forEach(t),T1o=r(SCe," (RemBERT model)"),SCe.forEach(t),F1o=i(N),Wu=n(N,"LI",{});var PCe=s(Wu);vee=n(PCe,"STRONG",{});var sDr=s(vee);C1o=r(sDr,"roberta"),sDr.forEach(t),M1o=r(PCe," \u2014 "),SP=n(PCe,"A",{href:!0});var lDr=s(SP);E1o=r(lDr,"RobertaForMaskedLM"),lDr.forEach(t),y1o=r(PCe," (RoBERTa model)"),PCe.forEach(t),w1o=i(N),Qu=n(N,"LI",{});var $Ce=s(Qu);Tee=n($Ce,"STRONG",{});var iDr=s(Tee);A1o=r(iDr,"roformer"),iDr.forEach(t),L1o=r($Ce," \u2014 "),PP=n($Ce,"A",{href:!0});var dDr=s(PP);B1o=r(dDr,"RoFormerForMaskedLM"),dDr.forEach(t),k1o=r($Ce," (RoFormer model)"),$Ce.forEach(t),x1o=i(N),Hu=n(N,"LI",{});var ICe=s(Hu);Fee=n(ICe,"STRONG",{});var cDr=s(Fee);R1o=r(cDr,"squeezebert"),cDr.forEach(t),S1o=r(ICe," \u2014 "),$P=n(ICe,"A",{href:!0});var fDr=s($P);P1o=r(fDr,"SqueezeBertForMaskedLM"),fDr.forEach(t),$1o=r(ICe," (SqueezeBERT model)"),ICe.forEach(t),I1o=i(N),Uu=n(N,"LI",{});var jCe=s(Uu);Cee=n(jCe,"STRONG",{});var mDr=s(Cee);j1o=r(mDr,"tapas"),mDr.forEach(t),N1o=r(jCe," \u2014 "),IP=n(jCe,"A",{href:!0});var gDr=s(IP);D1o=r(gDr,"TapasForMaskedLM"),gDr.forEach(t),q1o=r(jCe," (TAPAS model)"),jCe.forEach(t),G1o=i(N),Ju=n(N,"LI",{});var NCe=s(Ju);Mee=n(NCe,"STRONG",{});var hDr=s(Mee);O1o=r(hDr,"wav2vec2"),hDr.forEach(t),X1o=r(NCe," \u2014 "),Eee=n(NCe,"CODE",{});var pDr=s(Eee);z1o=r(pDr,"Wav2Vec2ForMaskedLM"),pDr.forEach(t),V1o=r(NCe,"(Wav2Vec2 model)"),NCe.forEach(t),W1o=i(N),Yu=n(N,"LI",{});var DCe=s(Yu);yee=n(DCe,"STRONG",{});var _Dr=s(yee);Q1o=r(_Dr,"xlm"),_Dr.forEach(t),H1o=r(DCe," \u2014 "),jP=n(DCe,"A",{href:!0});var uDr=s(jP);U1o=r(uDr,"XLMWithLMHeadModel"),uDr.forEach(t),J1o=r(DCe," (XLM model)"),DCe.forEach(t),Y1o=i(N),Ku=n(N,"LI",{});var qCe=s(Ku);wee=n(qCe,"STRONG",{});var bDr=s(wee);K1o=r(bDr,"xlm-roberta"),bDr.forEach(t),Z1o=r(qCe," \u2014 "),NP=n(qCe,"A",{href:!0});var vDr=s(NP);e7o=r(vDr,"XLMRobertaForMaskedLM"),vDr.forEach(t),o7o=r(qCe," (XLM-RoBERTa model)"),qCe.forEach(t),r7o=i(N),Zu=n(N,"LI",{});var GCe=s(Zu);Aee=n(GCe,"STRONG",{});var TDr=s(Aee);t7o=r(TDr,"xlm-roberta-xl"),TDr.forEach(t),a7o=r(GCe," \u2014 "),DP=n(GCe,"A",{href:!0});var FDr=s(DP);n7o=r(FDr,"XLMRobertaXLForMaskedLM"),FDr.forEach(t),s7o=r(GCe," (XLM-RoBERTa-XL model)"),GCe.forEach(t),l7o=i(N),e1=n(N,"LI",{});var OCe=s(e1);Lee=n(OCe,"STRONG",{});var CDr=s(Lee);i7o=r(CDr,"yoso"),CDr.forEach(t),d7o=r(OCe," \u2014 "),qP=n(OCe,"A",{href:!0});var MDr=s(qP);c7o=r(MDr,"YosoForMaskedLM"),MDr.forEach(t),f7o=r(OCe," (YOSO model)"),OCe.forEach(t),N.forEach(t),m7o=i(It),o1=n(It,"P",{});var XCe=s(o1);g7o=r(XCe,"The model is set in evaluation mode by default using "),Bee=n(XCe,"CODE",{});var EDr=s(Bee);h7o=r(EDr,"model.eval()"),EDr.forEach(t),p7o=r(XCe,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),kee=n(XCe,"CODE",{});var yDr=s(kee);_7o=r(yDr,"model.train()"),yDr.forEach(t),XCe.forEach(t),u7o=i(It),xee=n(It,"P",{});var wDr=s(xee);b7o=r(wDr,"Examples:"),wDr.forEach(t),v7o=i(It),m(i3.$$.fragment,It),It.forEach(t),zs.forEach(t),E8e=i(d),Ki=n(d,"H2",{class:!0});var xBe=s(Ki);r1=n(xBe,"A",{id:!0,class:!0,href:!0});var ADr=s(r1);Ree=n(ADr,"SPAN",{});var LDr=s(Ree);m(d3.$$.fragment,LDr),LDr.forEach(t),ADr.forEach(t),T7o=i(xBe),See=n(xBe,"SPAN",{});var BDr=s(See);F7o=r(BDr,"AutoModelForSeq2SeqLM"),BDr.forEach(t),xBe.forEach(t),y8e=i(d),Uo=n(d,"DIV",{class:!0});var Ws=s(Uo);m(c3.$$.fragment,Ws),C7o=i(Ws),Zi=n(Ws,"P",{});var ez=s(Zi);M7o=r(ez,`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence language modeling head) when created
with the `),Pee=n(ez,"CODE",{});var kDr=s(Pee);E7o=r(kDr,"from_pretrained()"),kDr.forEach(t),y7o=r(ez,"class method or the "),$ee=n(ez,"CODE",{});var xDr=s($ee);w7o=r(xDr,"from_config()"),xDr.forEach(t),A7o=r(ez,`class
method.`),ez.forEach(t),L7o=i(Ws),f3=n(Ws,"P",{});var RBe=s(f3);B7o=r(RBe,"This class cannot be instantiated directly using "),Iee=n(RBe,"CODE",{});var RDr=s(Iee);k7o=r(RDr,"__init__()"),RDr.forEach(t),x7o=r(RBe," (throws an error)."),RBe.forEach(t),R7o=i(Ws),Or=n(Ws,"DIV",{class:!0});var Qs=s(Or);m(m3.$$.fragment,Qs),S7o=i(Qs),jee=n(Qs,"P",{});var SDr=s(jee);P7o=r(SDr,"Instantiates one of the model classes of the library (with a sequence-to-sequence language modeling head) from a configuration."),SDr.forEach(t),$7o=i(Qs),ed=n(Qs,"P",{});var oz=s(ed);I7o=r(oz,`Note:
Loading a model from its configuration file does `),Nee=n(oz,"STRONG",{});var PDr=s(Nee);j7o=r(PDr,"not"),PDr.forEach(t),N7o=r(oz,` load the model weights. It only affects the
model\u2019s configuration. Use `),Dee=n(oz,"CODE",{});var $Dr=s(Dee);D7o=r($Dr,"from_pretrained()"),$Dr.forEach(t),q7o=r(oz,"to load the model weights."),oz.forEach(t),G7o=i(Qs),qee=n(Qs,"P",{});var IDr=s(qee);O7o=r(IDr,"Examples:"),IDr.forEach(t),X7o=i(Qs),m(g3.$$.fragment,Qs),Qs.forEach(t),z7o=i(Ws),Pe=n(Ws,"DIV",{class:!0});var jt=s(Pe);m(h3.$$.fragment,jt),V7o=i(jt),Gee=n(jt,"P",{});var jDr=s(Gee);W7o=r(jDr,"Instantiate one of the model classes of the library (with a sequence-to-sequence language modeling head) from a pretrained model."),jDr.forEach(t),Q7o=i(jt),Xa=n(jt,"P",{});var fM=s(Xa);H7o=r(fM,"The model class to instantiate is selected based on the "),Oee=n(fM,"CODE",{});var NDr=s(Oee);U7o=r(NDr,"model_type"),NDr.forEach(t),J7o=r(fM,` property of the config object (either
passed as an argument or loaded from `),Xee=n(fM,"CODE",{});var DDr=s(Xee);Y7o=r(DDr,"pretrained_model_name_or_path"),DDr.forEach(t),K7o=r(fM,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),zee=n(fM,"CODE",{});var qDr=s(zee);Z7o=r(qDr,"pretrained_model_name_or_path"),qDr.forEach(t),e4o=r(fM,":"),fM.forEach(t),o4o=i(jt),ae=n(jt,"UL",{});var le=s(ae);t1=n(le,"LI",{});var zCe=s(t1);Vee=n(zCe,"STRONG",{});var GDr=s(Vee);r4o=r(GDr,"bart"),GDr.forEach(t),t4o=r(zCe," \u2014 "),GP=n(zCe,"A",{href:!0});var ODr=s(GP);a4o=r(ODr,"BartForConditionalGeneration"),ODr.forEach(t),n4o=r(zCe," (BART model)"),zCe.forEach(t),s4o=i(le),a1=n(le,"LI",{});var VCe=s(a1);Wee=n(VCe,"STRONG",{});var XDr=s(Wee);l4o=r(XDr,"bigbird_pegasus"),XDr.forEach(t),i4o=r(VCe," \u2014 "),OP=n(VCe,"A",{href:!0});var zDr=s(OP);d4o=r(zDr,"BigBirdPegasusForConditionalGeneration"),zDr.forEach(t),c4o=r(VCe," (BigBirdPegasus model)"),VCe.forEach(t),f4o=i(le),n1=n(le,"LI",{});var WCe=s(n1);Qee=n(WCe,"STRONG",{});var VDr=s(Qee);m4o=r(VDr,"blenderbot"),VDr.forEach(t),g4o=r(WCe," \u2014 "),XP=n(WCe,"A",{href:!0});var WDr=s(XP);h4o=r(WDr,"BlenderbotForConditionalGeneration"),WDr.forEach(t),p4o=r(WCe," (Blenderbot model)"),WCe.forEach(t),_4o=i(le),s1=n(le,"LI",{});var QCe=s(s1);Hee=n(QCe,"STRONG",{});var QDr=s(Hee);u4o=r(QDr,"blenderbot-small"),QDr.forEach(t),b4o=r(QCe," \u2014 "),zP=n(QCe,"A",{href:!0});var HDr=s(zP);v4o=r(HDr,"BlenderbotSmallForConditionalGeneration"),HDr.forEach(t),T4o=r(QCe," (BlenderbotSmall model)"),QCe.forEach(t),F4o=i(le),l1=n(le,"LI",{});var HCe=s(l1);Uee=n(HCe,"STRONG",{});var UDr=s(Uee);C4o=r(UDr,"encoder-decoder"),UDr.forEach(t),M4o=r(HCe," \u2014 "),VP=n(HCe,"A",{href:!0});var JDr=s(VP);E4o=r(JDr,"EncoderDecoderModel"),JDr.forEach(t),y4o=r(HCe," (Encoder decoder model)"),HCe.forEach(t),w4o=i(le),i1=n(le,"LI",{});var UCe=s(i1);Jee=n(UCe,"STRONG",{});var YDr=s(Jee);A4o=r(YDr,"fsmt"),YDr.forEach(t),L4o=r(UCe," \u2014 "),WP=n(UCe,"A",{href:!0});var KDr=s(WP);B4o=r(KDr,"FSMTForConditionalGeneration"),KDr.forEach(t),k4o=r(UCe," (FairSeq Machine-Translation model)"),UCe.forEach(t),x4o=i(le),d1=n(le,"LI",{});var JCe=s(d1);Yee=n(JCe,"STRONG",{});var ZDr=s(Yee);R4o=r(ZDr,"led"),ZDr.forEach(t),S4o=r(JCe," \u2014 "),QP=n(JCe,"A",{href:!0});var eqr=s(QP);P4o=r(eqr,"LEDForConditionalGeneration"),eqr.forEach(t),$4o=r(JCe," (LED model)"),JCe.forEach(t),I4o=i(le),c1=n(le,"LI",{});var YCe=s(c1);Kee=n(YCe,"STRONG",{});var oqr=s(Kee);j4o=r(oqr,"m2m_100"),oqr.forEach(t),N4o=r(YCe," \u2014 "),HP=n(YCe,"A",{href:!0});var rqr=s(HP);D4o=r(rqr,"M2M100ForConditionalGeneration"),rqr.forEach(t),q4o=r(YCe," (M2M100 model)"),YCe.forEach(t),G4o=i(le),f1=n(le,"LI",{});var KCe=s(f1);Zee=n(KCe,"STRONG",{});var tqr=s(Zee);O4o=r(tqr,"marian"),tqr.forEach(t),X4o=r(KCe," \u2014 "),UP=n(KCe,"A",{href:!0});var aqr=s(UP);z4o=r(aqr,"MarianMTModel"),aqr.forEach(t),V4o=r(KCe," (Marian model)"),KCe.forEach(t),W4o=i(le),m1=n(le,"LI",{});var ZCe=s(m1);eoe=n(ZCe,"STRONG",{});var nqr=s(eoe);Q4o=r(nqr,"mbart"),nqr.forEach(t),H4o=r(ZCe," \u2014 "),JP=n(ZCe,"A",{href:!0});var sqr=s(JP);U4o=r(sqr,"MBartForConditionalGeneration"),sqr.forEach(t),J4o=r(ZCe," (mBART model)"),ZCe.forEach(t),Y4o=i(le),g1=n(le,"LI",{});var eMe=s(g1);ooe=n(eMe,"STRONG",{});var lqr=s(ooe);K4o=r(lqr,"mt5"),lqr.forEach(t),Z4o=r(eMe," \u2014 "),YP=n(eMe,"A",{href:!0});var iqr=s(YP);ebo=r(iqr,"MT5ForConditionalGeneration"),iqr.forEach(t),obo=r(eMe," (mT5 model)"),eMe.forEach(t),rbo=i(le),h1=n(le,"LI",{});var oMe=s(h1);roe=n(oMe,"STRONG",{});var dqr=s(roe);tbo=r(dqr,"pegasus"),dqr.forEach(t),abo=r(oMe," \u2014 "),KP=n(oMe,"A",{href:!0});var cqr=s(KP);nbo=r(cqr,"PegasusForConditionalGeneration"),cqr.forEach(t),sbo=r(oMe," (Pegasus model)"),oMe.forEach(t),lbo=i(le),p1=n(le,"LI",{});var rMe=s(p1);toe=n(rMe,"STRONG",{});var fqr=s(toe);ibo=r(fqr,"plbart"),fqr.forEach(t),dbo=r(rMe," \u2014 "),ZP=n(rMe,"A",{href:!0});var mqr=s(ZP);cbo=r(mqr,"PLBartForConditionalGeneration"),mqr.forEach(t),fbo=r(rMe," (PLBart model)"),rMe.forEach(t),mbo=i(le),_1=n(le,"LI",{});var tMe=s(_1);aoe=n(tMe,"STRONG",{});var gqr=s(aoe);gbo=r(gqr,"prophetnet"),gqr.forEach(t),hbo=r(tMe," \u2014 "),e$=n(tMe,"A",{href:!0});var hqr=s(e$);pbo=r(hqr,"ProphetNetForConditionalGeneration"),hqr.forEach(t),_bo=r(tMe," (ProphetNet model)"),tMe.forEach(t),ubo=i(le),u1=n(le,"LI",{});var aMe=s(u1);noe=n(aMe,"STRONG",{});var pqr=s(noe);bbo=r(pqr,"t5"),pqr.forEach(t),vbo=r(aMe," \u2014 "),o$=n(aMe,"A",{href:!0});var _qr=s(o$);Tbo=r(_qr,"T5ForConditionalGeneration"),_qr.forEach(t),Fbo=r(aMe," (T5 model)"),aMe.forEach(t),Cbo=i(le),b1=n(le,"LI",{});var nMe=s(b1);soe=n(nMe,"STRONG",{});var uqr=s(soe);Mbo=r(uqr,"xlm-prophetnet"),uqr.forEach(t),Ebo=r(nMe," \u2014 "),r$=n(nMe,"A",{href:!0});var bqr=s(r$);ybo=r(bqr,"XLMProphetNetForConditionalGeneration"),bqr.forEach(t),wbo=r(nMe," (XLMProphetNet model)"),nMe.forEach(t),le.forEach(t),Abo=i(jt),v1=n(jt,"P",{});var sMe=s(v1);Lbo=r(sMe,"The model is set in evaluation mode by default using "),loe=n(sMe,"CODE",{});var vqr=s(loe);Bbo=r(vqr,"model.eval()"),vqr.forEach(t),kbo=r(sMe,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),ioe=n(sMe,"CODE",{});var Tqr=s(ioe);xbo=r(Tqr,"model.train()"),Tqr.forEach(t),sMe.forEach(t),Rbo=i(jt),doe=n(jt,"P",{});var Fqr=s(doe);Sbo=r(Fqr,"Examples:"),Fqr.forEach(t),Pbo=i(jt),m(p3.$$.fragment,jt),jt.forEach(t),Ws.forEach(t),w8e=i(d),od=n(d,"H2",{class:!0});var SBe=s(od);T1=n(SBe,"A",{id:!0,class:!0,href:!0});var Cqr=s(T1);coe=n(Cqr,"SPAN",{});var Mqr=s(coe);m(_3.$$.fragment,Mqr),Mqr.forEach(t),Cqr.forEach(t),$bo=i(SBe),foe=n(SBe,"SPAN",{});var Eqr=s(foe);Ibo=r(Eqr,"AutoModelForSequenceClassification"),Eqr.forEach(t),SBe.forEach(t),A8e=i(d),Jo=n(d,"DIV",{class:!0});var Hs=s(Jo);m(u3.$$.fragment,Hs),jbo=i(Hs),rd=n(Hs,"P",{});var rz=s(rd);Nbo=r(rz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence classification head) when created
with the `),moe=n(rz,"CODE",{});var yqr=s(moe);Dbo=r(yqr,"from_pretrained()"),yqr.forEach(t),qbo=r(rz,"class method or the "),goe=n(rz,"CODE",{});var wqr=s(goe);Gbo=r(wqr,"from_config()"),wqr.forEach(t),Obo=r(rz,`class
method.`),rz.forEach(t),Xbo=i(Hs),b3=n(Hs,"P",{});var PBe=s(b3);zbo=r(PBe,"This class cannot be instantiated directly using "),hoe=n(PBe,"CODE",{});var Aqr=s(hoe);Vbo=r(Aqr,"__init__()"),Aqr.forEach(t),Wbo=r(PBe," (throws an error)."),PBe.forEach(t),Qbo=i(Hs),Xr=n(Hs,"DIV",{class:!0});var Us=s(Xr);m(v3.$$.fragment,Us),Hbo=i(Us),poe=n(Us,"P",{});var Lqr=s(poe);Ubo=r(Lqr,"Instantiates one of the model classes of the library (with a sequence classification head) from a configuration."),Lqr.forEach(t),Jbo=i(Us),td=n(Us,"P",{});var tz=s(td);Ybo=r(tz,`Note:
Loading a model from its configuration file does `),_oe=n(tz,"STRONG",{});var Bqr=s(_oe);Kbo=r(Bqr,"not"),Bqr.forEach(t),Zbo=r(tz,` load the model weights. It only affects the
model\u2019s configuration. Use `),uoe=n(tz,"CODE",{});var kqr=s(uoe);e5o=r(kqr,"from_pretrained()"),kqr.forEach(t),o5o=r(tz,"to load the model weights."),tz.forEach(t),r5o=i(Us),boe=n(Us,"P",{});var xqr=s(boe);t5o=r(xqr,"Examples:"),xqr.forEach(t),a5o=i(Us),m(T3.$$.fragment,Us),Us.forEach(t),n5o=i(Hs),$e=n(Hs,"DIV",{class:!0});var Nt=s($e);m(F3.$$.fragment,Nt),s5o=i(Nt),voe=n(Nt,"P",{});var Rqr=s(voe);l5o=r(Rqr,"Instantiate one of the model classes of the library (with a sequence classification head) from a pretrained model."),Rqr.forEach(t),i5o=i(Nt),za=n(Nt,"P",{});var mM=s(za);d5o=r(mM,"The model class to instantiate is selected based on the "),Toe=n(mM,"CODE",{});var Sqr=s(Toe);c5o=r(Sqr,"model_type"),Sqr.forEach(t),f5o=r(mM,` property of the config object (either
passed as an argument or loaded from `),Foe=n(mM,"CODE",{});var Pqr=s(Foe);m5o=r(Pqr,"pretrained_model_name_or_path"),Pqr.forEach(t),g5o=r(mM,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Coe=n(mM,"CODE",{});var $qr=s(Coe);h5o=r($qr,"pretrained_model_name_or_path"),$qr.forEach(t),p5o=r(mM,":"),mM.forEach(t),_5o=i(Nt),A=n(Nt,"UL",{});var L=s(A);F1=n(L,"LI",{});var lMe=s(F1);Moe=n(lMe,"STRONG",{});var Iqr=s(Moe);u5o=r(Iqr,"albert"),Iqr.forEach(t),b5o=r(lMe," \u2014 "),t$=n(lMe,"A",{href:!0});var jqr=s(t$);v5o=r(jqr,"AlbertForSequenceClassification"),jqr.forEach(t),T5o=r(lMe," (ALBERT model)"),lMe.forEach(t),F5o=i(L),C1=n(L,"LI",{});var iMe=s(C1);Eoe=n(iMe,"STRONG",{});var Nqr=s(Eoe);C5o=r(Nqr,"bart"),Nqr.forEach(t),M5o=r(iMe," \u2014 "),a$=n(iMe,"A",{href:!0});var Dqr=s(a$);E5o=r(Dqr,"BartForSequenceClassification"),Dqr.forEach(t),y5o=r(iMe," (BART model)"),iMe.forEach(t),w5o=i(L),M1=n(L,"LI",{});var dMe=s(M1);yoe=n(dMe,"STRONG",{});var qqr=s(yoe);A5o=r(qqr,"bert"),qqr.forEach(t),L5o=r(dMe," \u2014 "),n$=n(dMe,"A",{href:!0});var Gqr=s(n$);B5o=r(Gqr,"BertForSequenceClassification"),Gqr.forEach(t),k5o=r(dMe," (BERT model)"),dMe.forEach(t),x5o=i(L),E1=n(L,"LI",{});var cMe=s(E1);woe=n(cMe,"STRONG",{});var Oqr=s(woe);R5o=r(Oqr,"big_bird"),Oqr.forEach(t),S5o=r(cMe," \u2014 "),s$=n(cMe,"A",{href:!0});var Xqr=s(s$);P5o=r(Xqr,"BigBirdForSequenceClassification"),Xqr.forEach(t),$5o=r(cMe," (BigBird model)"),cMe.forEach(t),I5o=i(L),y1=n(L,"LI",{});var fMe=s(y1);Aoe=n(fMe,"STRONG",{});var zqr=s(Aoe);j5o=r(zqr,"bigbird_pegasus"),zqr.forEach(t),N5o=r(fMe," \u2014 "),l$=n(fMe,"A",{href:!0});var Vqr=s(l$);D5o=r(Vqr,"BigBirdPegasusForSequenceClassification"),Vqr.forEach(t),q5o=r(fMe," (BigBirdPegasus model)"),fMe.forEach(t),G5o=i(L),w1=n(L,"LI",{});var mMe=s(w1);Loe=n(mMe,"STRONG",{});var Wqr=s(Loe);O5o=r(Wqr,"camembert"),Wqr.forEach(t),X5o=r(mMe," \u2014 "),i$=n(mMe,"A",{href:!0});var Qqr=s(i$);z5o=r(Qqr,"CamembertForSequenceClassification"),Qqr.forEach(t),V5o=r(mMe," (CamemBERT model)"),mMe.forEach(t),W5o=i(L),A1=n(L,"LI",{});var gMe=s(A1);Boe=n(gMe,"STRONG",{});var Hqr=s(Boe);Q5o=r(Hqr,"canine"),Hqr.forEach(t),H5o=r(gMe," \u2014 "),d$=n(gMe,"A",{href:!0});var Uqr=s(d$);U5o=r(Uqr,"CanineForSequenceClassification"),Uqr.forEach(t),J5o=r(gMe," (Canine model)"),gMe.forEach(t),Y5o=i(L),L1=n(L,"LI",{});var hMe=s(L1);koe=n(hMe,"STRONG",{});var Jqr=s(koe);K5o=r(Jqr,"convbert"),Jqr.forEach(t),Z5o=r(hMe," \u2014 "),c$=n(hMe,"A",{href:!0});var Yqr=s(c$);e2o=r(Yqr,"ConvBertForSequenceClassification"),Yqr.forEach(t),o2o=r(hMe," (ConvBERT model)"),hMe.forEach(t),r2o=i(L),B1=n(L,"LI",{});var pMe=s(B1);xoe=n(pMe,"STRONG",{});var Kqr=s(xoe);t2o=r(Kqr,"ctrl"),Kqr.forEach(t),a2o=r(pMe," \u2014 "),f$=n(pMe,"A",{href:!0});var Zqr=s(f$);n2o=r(Zqr,"CTRLForSequenceClassification"),Zqr.forEach(t),s2o=r(pMe," (CTRL model)"),pMe.forEach(t),l2o=i(L),k1=n(L,"LI",{});var _Me=s(k1);Roe=n(_Me,"STRONG",{});var eGr=s(Roe);i2o=r(eGr,"deberta"),eGr.forEach(t),d2o=r(_Me," \u2014 "),m$=n(_Me,"A",{href:!0});var oGr=s(m$);c2o=r(oGr,"DebertaForSequenceClassification"),oGr.forEach(t),f2o=r(_Me," (DeBERTa model)"),_Me.forEach(t),m2o=i(L),x1=n(L,"LI",{});var uMe=s(x1);Soe=n(uMe,"STRONG",{});var rGr=s(Soe);g2o=r(rGr,"deberta-v2"),rGr.forEach(t),h2o=r(uMe," \u2014 "),g$=n(uMe,"A",{href:!0});var tGr=s(g$);p2o=r(tGr,"DebertaV2ForSequenceClassification"),tGr.forEach(t),_2o=r(uMe," (DeBERTa-v2 model)"),uMe.forEach(t),u2o=i(L),R1=n(L,"LI",{});var bMe=s(R1);Poe=n(bMe,"STRONG",{});var aGr=s(Poe);b2o=r(aGr,"distilbert"),aGr.forEach(t),v2o=r(bMe," \u2014 "),h$=n(bMe,"A",{href:!0});var nGr=s(h$);T2o=r(nGr,"DistilBertForSequenceClassification"),nGr.forEach(t),F2o=r(bMe," (DistilBERT model)"),bMe.forEach(t),C2o=i(L),S1=n(L,"LI",{});var vMe=s(S1);$oe=n(vMe,"STRONG",{});var sGr=s($oe);M2o=r(sGr,"electra"),sGr.forEach(t),E2o=r(vMe," \u2014 "),p$=n(vMe,"A",{href:!0});var lGr=s(p$);y2o=r(lGr,"ElectraForSequenceClassification"),lGr.forEach(t),w2o=r(vMe," (ELECTRA model)"),vMe.forEach(t),A2o=i(L),P1=n(L,"LI",{});var TMe=s(P1);Ioe=n(TMe,"STRONG",{});var iGr=s(Ioe);L2o=r(iGr,"flaubert"),iGr.forEach(t),B2o=r(TMe," \u2014 "),_$=n(TMe,"A",{href:!0});var dGr=s(_$);k2o=r(dGr,"FlaubertForSequenceClassification"),dGr.forEach(t),x2o=r(TMe," (FlauBERT model)"),TMe.forEach(t),R2o=i(L),$1=n(L,"LI",{});var FMe=s($1);joe=n(FMe,"STRONG",{});var cGr=s(joe);S2o=r(cGr,"fnet"),cGr.forEach(t),P2o=r(FMe," \u2014 "),u$=n(FMe,"A",{href:!0});var fGr=s(u$);$2o=r(fGr,"FNetForSequenceClassification"),fGr.forEach(t),I2o=r(FMe," (FNet model)"),FMe.forEach(t),j2o=i(L),I1=n(L,"LI",{});var CMe=s(I1);Noe=n(CMe,"STRONG",{});var mGr=s(Noe);N2o=r(mGr,"funnel"),mGr.forEach(t),D2o=r(CMe," \u2014 "),b$=n(CMe,"A",{href:!0});var gGr=s(b$);q2o=r(gGr,"FunnelForSequenceClassification"),gGr.forEach(t),G2o=r(CMe," (Funnel Transformer model)"),CMe.forEach(t),O2o=i(L),j1=n(L,"LI",{});var MMe=s(j1);Doe=n(MMe,"STRONG",{});var hGr=s(Doe);X2o=r(hGr,"gpt2"),hGr.forEach(t),z2o=r(MMe," \u2014 "),v$=n(MMe,"A",{href:!0});var pGr=s(v$);V2o=r(pGr,"GPT2ForSequenceClassification"),pGr.forEach(t),W2o=r(MMe," (OpenAI GPT-2 model)"),MMe.forEach(t),Q2o=i(L),N1=n(L,"LI",{});var EMe=s(N1);qoe=n(EMe,"STRONG",{});var _Gr=s(qoe);H2o=r(_Gr,"gpt_neo"),_Gr.forEach(t),U2o=r(EMe," \u2014 "),T$=n(EMe,"A",{href:!0});var uGr=s(T$);J2o=r(uGr,"GPTNeoForSequenceClassification"),uGr.forEach(t),Y2o=r(EMe," (GPT Neo model)"),EMe.forEach(t),K2o=i(L),D1=n(L,"LI",{});var yMe=s(D1);Goe=n(yMe,"STRONG",{});var bGr=s(Goe);Z2o=r(bGr,"gptj"),bGr.forEach(t),evo=r(yMe," \u2014 "),F$=n(yMe,"A",{href:!0});var vGr=s(F$);ovo=r(vGr,"GPTJForSequenceClassification"),vGr.forEach(t),rvo=r(yMe," (GPT-J model)"),yMe.forEach(t),tvo=i(L),q1=n(L,"LI",{});var wMe=s(q1);Ooe=n(wMe,"STRONG",{});var TGr=s(Ooe);avo=r(TGr,"ibert"),TGr.forEach(t),nvo=r(wMe," \u2014 "),C$=n(wMe,"A",{href:!0});var FGr=s(C$);svo=r(FGr,"IBertForSequenceClassification"),FGr.forEach(t),lvo=r(wMe," (I-BERT model)"),wMe.forEach(t),ivo=i(L),G1=n(L,"LI",{});var AMe=s(G1);Xoe=n(AMe,"STRONG",{});var CGr=s(Xoe);dvo=r(CGr,"layoutlm"),CGr.forEach(t),cvo=r(AMe," \u2014 "),M$=n(AMe,"A",{href:!0});var MGr=s(M$);fvo=r(MGr,"LayoutLMForSequenceClassification"),MGr.forEach(t),mvo=r(AMe," (LayoutLM model)"),AMe.forEach(t),gvo=i(L),O1=n(L,"LI",{});var LMe=s(O1);zoe=n(LMe,"STRONG",{});var EGr=s(zoe);hvo=r(EGr,"layoutlmv2"),EGr.forEach(t),pvo=r(LMe," \u2014 "),E$=n(LMe,"A",{href:!0});var yGr=s(E$);_vo=r(yGr,"LayoutLMv2ForSequenceClassification"),yGr.forEach(t),uvo=r(LMe," (LayoutLMv2 model)"),LMe.forEach(t),bvo=i(L),X1=n(L,"LI",{});var BMe=s(X1);Voe=n(BMe,"STRONG",{});var wGr=s(Voe);vvo=r(wGr,"led"),wGr.forEach(t),Tvo=r(BMe," \u2014 "),y$=n(BMe,"A",{href:!0});var AGr=s(y$);Fvo=r(AGr,"LEDForSequenceClassification"),AGr.forEach(t),Cvo=r(BMe," (LED model)"),BMe.forEach(t),Mvo=i(L),z1=n(L,"LI",{});var kMe=s(z1);Woe=n(kMe,"STRONG",{});var LGr=s(Woe);Evo=r(LGr,"longformer"),LGr.forEach(t),yvo=r(kMe," \u2014 "),w$=n(kMe,"A",{href:!0});var BGr=s(w$);wvo=r(BGr,"LongformerForSequenceClassification"),BGr.forEach(t),Avo=r(kMe," (Longformer model)"),kMe.forEach(t),Lvo=i(L),V1=n(L,"LI",{});var xMe=s(V1);Qoe=n(xMe,"STRONG",{});var kGr=s(Qoe);Bvo=r(kGr,"mbart"),kGr.forEach(t),kvo=r(xMe," \u2014 "),A$=n(xMe,"A",{href:!0});var xGr=s(A$);xvo=r(xGr,"MBartForSequenceClassification"),xGr.forEach(t),Rvo=r(xMe," (mBART model)"),xMe.forEach(t),Svo=i(L),W1=n(L,"LI",{});var RMe=s(W1);Hoe=n(RMe,"STRONG",{});var RGr=s(Hoe);Pvo=r(RGr,"megatron-bert"),RGr.forEach(t),$vo=r(RMe," \u2014 "),L$=n(RMe,"A",{href:!0});var SGr=s(L$);Ivo=r(SGr,"MegatronBertForSequenceClassification"),SGr.forEach(t),jvo=r(RMe," (MegatronBert model)"),RMe.forEach(t),Nvo=i(L),Q1=n(L,"LI",{});var SMe=s(Q1);Uoe=n(SMe,"STRONG",{});var PGr=s(Uoe);Dvo=r(PGr,"mobilebert"),PGr.forEach(t),qvo=r(SMe," \u2014 "),B$=n(SMe,"A",{href:!0});var $Gr=s(B$);Gvo=r($Gr,"MobileBertForSequenceClassification"),$Gr.forEach(t),Ovo=r(SMe," (MobileBERT model)"),SMe.forEach(t),Xvo=i(L),H1=n(L,"LI",{});var PMe=s(H1);Joe=n(PMe,"STRONG",{});var IGr=s(Joe);zvo=r(IGr,"mpnet"),IGr.forEach(t),Vvo=r(PMe," \u2014 "),k$=n(PMe,"A",{href:!0});var jGr=s(k$);Wvo=r(jGr,"MPNetForSequenceClassification"),jGr.forEach(t),Qvo=r(PMe," (MPNet model)"),PMe.forEach(t),Hvo=i(L),U1=n(L,"LI",{});var $Me=s(U1);Yoe=n($Me,"STRONG",{});var NGr=s(Yoe);Uvo=r(NGr,"nystromformer"),NGr.forEach(t),Jvo=r($Me," \u2014 "),x$=n($Me,"A",{href:!0});var DGr=s(x$);Yvo=r(DGr,"NystromformerForSequenceClassification"),DGr.forEach(t),Kvo=r($Me," (Nystromformer model)"),$Me.forEach(t),Zvo=i(L),J1=n(L,"LI",{});var IMe=s(J1);Koe=n(IMe,"STRONG",{});var qGr=s(Koe);eTo=r(qGr,"openai-gpt"),qGr.forEach(t),oTo=r(IMe," \u2014 "),R$=n(IMe,"A",{href:!0});var GGr=s(R$);rTo=r(GGr,"OpenAIGPTForSequenceClassification"),GGr.forEach(t),tTo=r(IMe," (OpenAI GPT model)"),IMe.forEach(t),aTo=i(L),Y1=n(L,"LI",{});var jMe=s(Y1);Zoe=n(jMe,"STRONG",{});var OGr=s(Zoe);nTo=r(OGr,"perceiver"),OGr.forEach(t),sTo=r(jMe," \u2014 "),S$=n(jMe,"A",{href:!0});var XGr=s(S$);lTo=r(XGr,"PerceiverForSequenceClassification"),XGr.forEach(t),iTo=r(jMe," (Perceiver model)"),jMe.forEach(t),dTo=i(L),K1=n(L,"LI",{});var NMe=s(K1);ere=n(NMe,"STRONG",{});var zGr=s(ere);cTo=r(zGr,"plbart"),zGr.forEach(t),fTo=r(NMe," \u2014 "),P$=n(NMe,"A",{href:!0});var VGr=s(P$);mTo=r(VGr,"PLBartForSequenceClassification"),VGr.forEach(t),gTo=r(NMe," (PLBart model)"),NMe.forEach(t),hTo=i(L),Z1=n(L,"LI",{});var DMe=s(Z1);ore=n(DMe,"STRONG",{});var WGr=s(ore);pTo=r(WGr,"qdqbert"),WGr.forEach(t),_To=r(DMe," \u2014 "),$$=n(DMe,"A",{href:!0});var QGr=s($$);uTo=r(QGr,"QDQBertForSequenceClassification"),QGr.forEach(t),bTo=r(DMe," (QDQBert model)"),DMe.forEach(t),vTo=i(L),e7=n(L,"LI",{});var qMe=s(e7);rre=n(qMe,"STRONG",{});var HGr=s(rre);TTo=r(HGr,"reformer"),HGr.forEach(t),FTo=r(qMe," \u2014 "),I$=n(qMe,"A",{href:!0});var UGr=s(I$);CTo=r(UGr,"ReformerForSequenceClassification"),UGr.forEach(t),MTo=r(qMe," (Reformer model)"),qMe.forEach(t),ETo=i(L),o7=n(L,"LI",{});var GMe=s(o7);tre=n(GMe,"STRONG",{});var JGr=s(tre);yTo=r(JGr,"rembert"),JGr.forEach(t),wTo=r(GMe," \u2014 "),j$=n(GMe,"A",{href:!0});var YGr=s(j$);ATo=r(YGr,"RemBertForSequenceClassification"),YGr.forEach(t),LTo=r(GMe," (RemBERT model)"),GMe.forEach(t),BTo=i(L),r7=n(L,"LI",{});var OMe=s(r7);are=n(OMe,"STRONG",{});var KGr=s(are);kTo=r(KGr,"roberta"),KGr.forEach(t),xTo=r(OMe," \u2014 "),N$=n(OMe,"A",{href:!0});var ZGr=s(N$);RTo=r(ZGr,"RobertaForSequenceClassification"),ZGr.forEach(t),STo=r(OMe," (RoBERTa model)"),OMe.forEach(t),PTo=i(L),t7=n(L,"LI",{});var XMe=s(t7);nre=n(XMe,"STRONG",{});var eOr=s(nre);$To=r(eOr,"roformer"),eOr.forEach(t),ITo=r(XMe," \u2014 "),D$=n(XMe,"A",{href:!0});var oOr=s(D$);jTo=r(oOr,"RoFormerForSequenceClassification"),oOr.forEach(t),NTo=r(XMe," (RoFormer model)"),XMe.forEach(t),DTo=i(L),a7=n(L,"LI",{});var zMe=s(a7);sre=n(zMe,"STRONG",{});var rOr=s(sre);qTo=r(rOr,"squeezebert"),rOr.forEach(t),GTo=r(zMe," \u2014 "),q$=n(zMe,"A",{href:!0});var tOr=s(q$);OTo=r(tOr,"SqueezeBertForSequenceClassification"),tOr.forEach(t),XTo=r(zMe," (SqueezeBERT model)"),zMe.forEach(t),zTo=i(L),n7=n(L,"LI",{});var VMe=s(n7);lre=n(VMe,"STRONG",{});var aOr=s(lre);VTo=r(aOr,"tapas"),aOr.forEach(t),WTo=r(VMe," \u2014 "),G$=n(VMe,"A",{href:!0});var nOr=s(G$);QTo=r(nOr,"TapasForSequenceClassification"),nOr.forEach(t),HTo=r(VMe," (TAPAS model)"),VMe.forEach(t),UTo=i(L),s7=n(L,"LI",{});var WMe=s(s7);ire=n(WMe,"STRONG",{});var sOr=s(ire);JTo=r(sOr,"transfo-xl"),sOr.forEach(t),YTo=r(WMe," \u2014 "),O$=n(WMe,"A",{href:!0});var lOr=s(O$);KTo=r(lOr,"TransfoXLForSequenceClassification"),lOr.forEach(t),ZTo=r(WMe," (Transformer-XL model)"),WMe.forEach(t),eFo=i(L),l7=n(L,"LI",{});var QMe=s(l7);dre=n(QMe,"STRONG",{});var iOr=s(dre);oFo=r(iOr,"xlm"),iOr.forEach(t),rFo=r(QMe," \u2014 "),X$=n(QMe,"A",{href:!0});var dOr=s(X$);tFo=r(dOr,"XLMForSequenceClassification"),dOr.forEach(t),aFo=r(QMe," (XLM model)"),QMe.forEach(t),nFo=i(L),i7=n(L,"LI",{});var HMe=s(i7);cre=n(HMe,"STRONG",{});var cOr=s(cre);sFo=r(cOr,"xlm-roberta"),cOr.forEach(t),lFo=r(HMe," \u2014 "),z$=n(HMe,"A",{href:!0});var fOr=s(z$);iFo=r(fOr,"XLMRobertaForSequenceClassification"),fOr.forEach(t),dFo=r(HMe," (XLM-RoBERTa model)"),HMe.forEach(t),cFo=i(L),d7=n(L,"LI",{});var UMe=s(d7);fre=n(UMe,"STRONG",{});var mOr=s(fre);fFo=r(mOr,"xlm-roberta-xl"),mOr.forEach(t),mFo=r(UMe," \u2014 "),V$=n(UMe,"A",{href:!0});var gOr=s(V$);gFo=r(gOr,"XLMRobertaXLForSequenceClassification"),gOr.forEach(t),hFo=r(UMe," (XLM-RoBERTa-XL model)"),UMe.forEach(t),pFo=i(L),c7=n(L,"LI",{});var JMe=s(c7);mre=n(JMe,"STRONG",{});var hOr=s(mre);_Fo=r(hOr,"xlnet"),hOr.forEach(t),uFo=r(JMe," \u2014 "),W$=n(JMe,"A",{href:!0});var pOr=s(W$);bFo=r(pOr,"XLNetForSequenceClassification"),pOr.forEach(t),vFo=r(JMe," (XLNet model)"),JMe.forEach(t),TFo=i(L),f7=n(L,"LI",{});var YMe=s(f7);gre=n(YMe,"STRONG",{});var _Or=s(gre);FFo=r(_Or,"yoso"),_Or.forEach(t),CFo=r(YMe," \u2014 "),Q$=n(YMe,"A",{href:!0});var uOr=s(Q$);MFo=r(uOr,"YosoForSequenceClassification"),uOr.forEach(t),EFo=r(YMe," (YOSO model)"),YMe.forEach(t),L.forEach(t),yFo=i(Nt),m7=n(Nt,"P",{});var KMe=s(m7);wFo=r(KMe,"The model is set in evaluation mode by default using "),hre=n(KMe,"CODE",{});var bOr=s(hre);AFo=r(bOr,"model.eval()"),bOr.forEach(t),LFo=r(KMe,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),pre=n(KMe,"CODE",{});var vOr=s(pre);BFo=r(vOr,"model.train()"),vOr.forEach(t),KMe.forEach(t),kFo=i(Nt),_re=n(Nt,"P",{});var TOr=s(_re);xFo=r(TOr,"Examples:"),TOr.forEach(t),RFo=i(Nt),m(C3.$$.fragment,Nt),Nt.forEach(t),Hs.forEach(t),L8e=i(d),ad=n(d,"H2",{class:!0});var $Be=s(ad);g7=n($Be,"A",{id:!0,class:!0,href:!0});var FOr=s(g7);ure=n(FOr,"SPAN",{});var COr=s(ure);m(M3.$$.fragment,COr),COr.forEach(t),FOr.forEach(t),SFo=i($Be),bre=n($Be,"SPAN",{});var MOr=s(bre);PFo=r(MOr,"AutoModelForMultipleChoice"),MOr.forEach(t),$Be.forEach(t),B8e=i(d),Yo=n(d,"DIV",{class:!0});var Js=s(Yo);m(E3.$$.fragment,Js),$Fo=i(Js),nd=n(Js,"P",{});var az=s(nd);IFo=r(az,`This is a generic model class that will be instantiated as one of the model classes of the library (with a multiple choice head) when created
with the `),vre=n(az,"CODE",{});var EOr=s(vre);jFo=r(EOr,"from_pretrained()"),EOr.forEach(t),NFo=r(az,"class method or the "),Tre=n(az,"CODE",{});var yOr=s(Tre);DFo=r(yOr,"from_config()"),yOr.forEach(t),qFo=r(az,`class
method.`),az.forEach(t),GFo=i(Js),y3=n(Js,"P",{});var IBe=s(y3);OFo=r(IBe,"This class cannot be instantiated directly using "),Fre=n(IBe,"CODE",{});var wOr=s(Fre);XFo=r(wOr,"__init__()"),wOr.forEach(t),zFo=r(IBe," (throws an error)."),IBe.forEach(t),VFo=i(Js),zr=n(Js,"DIV",{class:!0});var Ys=s(zr);m(w3.$$.fragment,Ys),WFo=i(Ys),Cre=n(Ys,"P",{});var AOr=s(Cre);QFo=r(AOr,"Instantiates one of the model classes of the library (with a multiple choice head) from a configuration."),AOr.forEach(t),HFo=i(Ys),sd=n(Ys,"P",{});var nz=s(sd);UFo=r(nz,`Note:
Loading a model from its configuration file does `),Mre=n(nz,"STRONG",{});var LOr=s(Mre);JFo=r(LOr,"not"),LOr.forEach(t),YFo=r(nz,` load the model weights. It only affects the
model\u2019s configuration. Use `),Ere=n(nz,"CODE",{});var BOr=s(Ere);KFo=r(BOr,"from_pretrained()"),BOr.forEach(t),ZFo=r(nz,"to load the model weights."),nz.forEach(t),eCo=i(Ys),yre=n(Ys,"P",{});var kOr=s(yre);oCo=r(kOr,"Examples:"),kOr.forEach(t),rCo=i(Ys),m(A3.$$.fragment,Ys),Ys.forEach(t),tCo=i(Js),Ie=n(Js,"DIV",{class:!0});var Dt=s(Ie);m(L3.$$.fragment,Dt),aCo=i(Dt),wre=n(Dt,"P",{});var xOr=s(wre);nCo=r(xOr,"Instantiate one of the model classes of the library (with a multiple choice head) from a pretrained model."),xOr.forEach(t),sCo=i(Dt),Va=n(Dt,"P",{});var gM=s(Va);lCo=r(gM,"The model class to instantiate is selected based on the "),Are=n(gM,"CODE",{});var ROr=s(Are);iCo=r(ROr,"model_type"),ROr.forEach(t),dCo=r(gM,` property of the config object (either
passed as an argument or loaded from `),Lre=n(gM,"CODE",{});var SOr=s(Lre);cCo=r(SOr,"pretrained_model_name_or_path"),SOr.forEach(t),fCo=r(gM,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Bre=n(gM,"CODE",{});var POr=s(Bre);mCo=r(POr,"pretrained_model_name_or_path"),POr.forEach(t),gCo=r(gM,":"),gM.forEach(t),hCo=i(Dt),G=n(Dt,"UL",{});var O=s(G);h7=n(O,"LI",{});var ZMe=s(h7);kre=n(ZMe,"STRONG",{});var $Or=s(kre);pCo=r($Or,"albert"),$Or.forEach(t),_Co=r(ZMe," \u2014 "),H$=n(ZMe,"A",{href:!0});var IOr=s(H$);uCo=r(IOr,"AlbertForMultipleChoice"),IOr.forEach(t),bCo=r(ZMe," (ALBERT model)"),ZMe.forEach(t),vCo=i(O),p7=n(O,"LI",{});var eEe=s(p7);xre=n(eEe,"STRONG",{});var jOr=s(xre);TCo=r(jOr,"bert"),jOr.forEach(t),FCo=r(eEe," \u2014 "),U$=n(eEe,"A",{href:!0});var NOr=s(U$);CCo=r(NOr,"BertForMultipleChoice"),NOr.forEach(t),MCo=r(eEe," (BERT model)"),eEe.forEach(t),ECo=i(O),_7=n(O,"LI",{});var oEe=s(_7);Rre=n(oEe,"STRONG",{});var DOr=s(Rre);yCo=r(DOr,"big_bird"),DOr.forEach(t),wCo=r(oEe," \u2014 "),J$=n(oEe,"A",{href:!0});var qOr=s(J$);ACo=r(qOr,"BigBirdForMultipleChoice"),qOr.forEach(t),LCo=r(oEe," (BigBird model)"),oEe.forEach(t),BCo=i(O),u7=n(O,"LI",{});var rEe=s(u7);Sre=n(rEe,"STRONG",{});var GOr=s(Sre);kCo=r(GOr,"camembert"),GOr.forEach(t),xCo=r(rEe," \u2014 "),Y$=n(rEe,"A",{href:!0});var OOr=s(Y$);RCo=r(OOr,"CamembertForMultipleChoice"),OOr.forEach(t),SCo=r(rEe," (CamemBERT model)"),rEe.forEach(t),PCo=i(O),b7=n(O,"LI",{});var tEe=s(b7);Pre=n(tEe,"STRONG",{});var XOr=s(Pre);$Co=r(XOr,"canine"),XOr.forEach(t),ICo=r(tEe," \u2014 "),K$=n(tEe,"A",{href:!0});var zOr=s(K$);jCo=r(zOr,"CanineForMultipleChoice"),zOr.forEach(t),NCo=r(tEe," (Canine model)"),tEe.forEach(t),DCo=i(O),v7=n(O,"LI",{});var aEe=s(v7);$re=n(aEe,"STRONG",{});var VOr=s($re);qCo=r(VOr,"convbert"),VOr.forEach(t),GCo=r(aEe," \u2014 "),Z$=n(aEe,"A",{href:!0});var WOr=s(Z$);OCo=r(WOr,"ConvBertForMultipleChoice"),WOr.forEach(t),XCo=r(aEe," (ConvBERT model)"),aEe.forEach(t),zCo=i(O),T7=n(O,"LI",{});var nEe=s(T7);Ire=n(nEe,"STRONG",{});var QOr=s(Ire);VCo=r(QOr,"distilbert"),QOr.forEach(t),WCo=r(nEe," \u2014 "),eI=n(nEe,"A",{href:!0});var HOr=s(eI);QCo=r(HOr,"DistilBertForMultipleChoice"),HOr.forEach(t),HCo=r(nEe," (DistilBERT model)"),nEe.forEach(t),UCo=i(O),F7=n(O,"LI",{});var sEe=s(F7);jre=n(sEe,"STRONG",{});var UOr=s(jre);JCo=r(UOr,"electra"),UOr.forEach(t),YCo=r(sEe," \u2014 "),oI=n(sEe,"A",{href:!0});var JOr=s(oI);KCo=r(JOr,"ElectraForMultipleChoice"),JOr.forEach(t),ZCo=r(sEe," (ELECTRA model)"),sEe.forEach(t),eMo=i(O),C7=n(O,"LI",{});var lEe=s(C7);Nre=n(lEe,"STRONG",{});var YOr=s(Nre);oMo=r(YOr,"flaubert"),YOr.forEach(t),rMo=r(lEe," \u2014 "),rI=n(lEe,"A",{href:!0});var KOr=s(rI);tMo=r(KOr,"FlaubertForMultipleChoice"),KOr.forEach(t),aMo=r(lEe," (FlauBERT model)"),lEe.forEach(t),nMo=i(O),M7=n(O,"LI",{});var iEe=s(M7);Dre=n(iEe,"STRONG",{});var ZOr=s(Dre);sMo=r(ZOr,"fnet"),ZOr.forEach(t),lMo=r(iEe," \u2014 "),tI=n(iEe,"A",{href:!0});var eXr=s(tI);iMo=r(eXr,"FNetForMultipleChoice"),eXr.forEach(t),dMo=r(iEe," (FNet model)"),iEe.forEach(t),cMo=i(O),E7=n(O,"LI",{});var dEe=s(E7);qre=n(dEe,"STRONG",{});var oXr=s(qre);fMo=r(oXr,"funnel"),oXr.forEach(t),mMo=r(dEe," \u2014 "),aI=n(dEe,"A",{href:!0});var rXr=s(aI);gMo=r(rXr,"FunnelForMultipleChoice"),rXr.forEach(t),hMo=r(dEe," (Funnel Transformer model)"),dEe.forEach(t),pMo=i(O),y7=n(O,"LI",{});var cEe=s(y7);Gre=n(cEe,"STRONG",{});var tXr=s(Gre);_Mo=r(tXr,"ibert"),tXr.forEach(t),uMo=r(cEe," \u2014 "),nI=n(cEe,"A",{href:!0});var aXr=s(nI);bMo=r(aXr,"IBertForMultipleChoice"),aXr.forEach(t),vMo=r(cEe," (I-BERT model)"),cEe.forEach(t),TMo=i(O),w7=n(O,"LI",{});var fEe=s(w7);Ore=n(fEe,"STRONG",{});var nXr=s(Ore);FMo=r(nXr,"longformer"),nXr.forEach(t),CMo=r(fEe," \u2014 "),sI=n(fEe,"A",{href:!0});var sXr=s(sI);MMo=r(sXr,"LongformerForMultipleChoice"),sXr.forEach(t),EMo=r(fEe," (Longformer model)"),fEe.forEach(t),yMo=i(O),A7=n(O,"LI",{});var mEe=s(A7);Xre=n(mEe,"STRONG",{});var lXr=s(Xre);wMo=r(lXr,"megatron-bert"),lXr.forEach(t),AMo=r(mEe," \u2014 "),lI=n(mEe,"A",{href:!0});var iXr=s(lI);LMo=r(iXr,"MegatronBertForMultipleChoice"),iXr.forEach(t),BMo=r(mEe," (MegatronBert model)"),mEe.forEach(t),kMo=i(O),L7=n(O,"LI",{});var gEe=s(L7);zre=n(gEe,"STRONG",{});var dXr=s(zre);xMo=r(dXr,"mobilebert"),dXr.forEach(t),RMo=r(gEe," \u2014 "),iI=n(gEe,"A",{href:!0});var cXr=s(iI);SMo=r(cXr,"MobileBertForMultipleChoice"),cXr.forEach(t),PMo=r(gEe," (MobileBERT model)"),gEe.forEach(t),$Mo=i(O),B7=n(O,"LI",{});var hEe=s(B7);Vre=n(hEe,"STRONG",{});var fXr=s(Vre);IMo=r(fXr,"mpnet"),fXr.forEach(t),jMo=r(hEe," \u2014 "),dI=n(hEe,"A",{href:!0});var mXr=s(dI);NMo=r(mXr,"MPNetForMultipleChoice"),mXr.forEach(t),DMo=r(hEe," (MPNet model)"),hEe.forEach(t),qMo=i(O),k7=n(O,"LI",{});var pEe=s(k7);Wre=n(pEe,"STRONG",{});var gXr=s(Wre);GMo=r(gXr,"nystromformer"),gXr.forEach(t),OMo=r(pEe," \u2014 "),cI=n(pEe,"A",{href:!0});var hXr=s(cI);XMo=r(hXr,"NystromformerForMultipleChoice"),hXr.forEach(t),zMo=r(pEe," (Nystromformer model)"),pEe.forEach(t),VMo=i(O),x7=n(O,"LI",{});var _Ee=s(x7);Qre=n(_Ee,"STRONG",{});var pXr=s(Qre);WMo=r(pXr,"qdqbert"),pXr.forEach(t),QMo=r(_Ee," \u2014 "),fI=n(_Ee,"A",{href:!0});var _Xr=s(fI);HMo=r(_Xr,"QDQBertForMultipleChoice"),_Xr.forEach(t),UMo=r(_Ee," (QDQBert model)"),_Ee.forEach(t),JMo=i(O),R7=n(O,"LI",{});var uEe=s(R7);Hre=n(uEe,"STRONG",{});var uXr=s(Hre);YMo=r(uXr,"rembert"),uXr.forEach(t),KMo=r(uEe," \u2014 "),mI=n(uEe,"A",{href:!0});var bXr=s(mI);ZMo=r(bXr,"RemBertForMultipleChoice"),bXr.forEach(t),eEo=r(uEe," (RemBERT model)"),uEe.forEach(t),oEo=i(O),S7=n(O,"LI",{});var bEe=s(S7);Ure=n(bEe,"STRONG",{});var vXr=s(Ure);rEo=r(vXr,"roberta"),vXr.forEach(t),tEo=r(bEe," \u2014 "),gI=n(bEe,"A",{href:!0});var TXr=s(gI);aEo=r(TXr,"RobertaForMultipleChoice"),TXr.forEach(t),nEo=r(bEe," (RoBERTa model)"),bEe.forEach(t),sEo=i(O),P7=n(O,"LI",{});var vEe=s(P7);Jre=n(vEe,"STRONG",{});var FXr=s(Jre);lEo=r(FXr,"roformer"),FXr.forEach(t),iEo=r(vEe," \u2014 "),hI=n(vEe,"A",{href:!0});var CXr=s(hI);dEo=r(CXr,"RoFormerForMultipleChoice"),CXr.forEach(t),cEo=r(vEe," (RoFormer model)"),vEe.forEach(t),fEo=i(O),$7=n(O,"LI",{});var TEe=s($7);Yre=n(TEe,"STRONG",{});var MXr=s(Yre);mEo=r(MXr,"squeezebert"),MXr.forEach(t),gEo=r(TEe," \u2014 "),pI=n(TEe,"A",{href:!0});var EXr=s(pI);hEo=r(EXr,"SqueezeBertForMultipleChoice"),EXr.forEach(t),pEo=r(TEe," (SqueezeBERT model)"),TEe.forEach(t),_Eo=i(O),I7=n(O,"LI",{});var FEe=s(I7);Kre=n(FEe,"STRONG",{});var yXr=s(Kre);uEo=r(yXr,"xlm"),yXr.forEach(t),bEo=r(FEe," \u2014 "),_I=n(FEe,"A",{href:!0});var wXr=s(_I);vEo=r(wXr,"XLMForMultipleChoice"),wXr.forEach(t),TEo=r(FEe," (XLM model)"),FEe.forEach(t),FEo=i(O),j7=n(O,"LI",{});var CEe=s(j7);Zre=n(CEe,"STRONG",{});var AXr=s(Zre);CEo=r(AXr,"xlm-roberta"),AXr.forEach(t),MEo=r(CEe," \u2014 "),uI=n(CEe,"A",{href:!0});var LXr=s(uI);EEo=r(LXr,"XLMRobertaForMultipleChoice"),LXr.forEach(t),yEo=r(CEe," (XLM-RoBERTa model)"),CEe.forEach(t),wEo=i(O),N7=n(O,"LI",{});var MEe=s(N7);ete=n(MEe,"STRONG",{});var BXr=s(ete);AEo=r(BXr,"xlm-roberta-xl"),BXr.forEach(t),LEo=r(MEe," \u2014 "),bI=n(MEe,"A",{href:!0});var kXr=s(bI);BEo=r(kXr,"XLMRobertaXLForMultipleChoice"),kXr.forEach(t),kEo=r(MEe," (XLM-RoBERTa-XL model)"),MEe.forEach(t),xEo=i(O),D7=n(O,"LI",{});var EEe=s(D7);ote=n(EEe,"STRONG",{});var xXr=s(ote);REo=r(xXr,"xlnet"),xXr.forEach(t),SEo=r(EEe," \u2014 "),vI=n(EEe,"A",{href:!0});var RXr=s(vI);PEo=r(RXr,"XLNetForMultipleChoice"),RXr.forEach(t),$Eo=r(EEe," (XLNet model)"),EEe.forEach(t),IEo=i(O),q7=n(O,"LI",{});var yEe=s(q7);rte=n(yEe,"STRONG",{});var SXr=s(rte);jEo=r(SXr,"yoso"),SXr.forEach(t),NEo=r(yEe," \u2014 "),TI=n(yEe,"A",{href:!0});var PXr=s(TI);DEo=r(PXr,"YosoForMultipleChoice"),PXr.forEach(t),qEo=r(yEe," (YOSO model)"),yEe.forEach(t),O.forEach(t),GEo=i(Dt),G7=n(Dt,"P",{});var wEe=s(G7);OEo=r(wEe,"The model is set in evaluation mode by default using "),tte=n(wEe,"CODE",{});var $Xr=s(tte);XEo=r($Xr,"model.eval()"),$Xr.forEach(t),zEo=r(wEe,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),ate=n(wEe,"CODE",{});var IXr=s(ate);VEo=r(IXr,"model.train()"),IXr.forEach(t),wEe.forEach(t),WEo=i(Dt),nte=n(Dt,"P",{});var jXr=s(nte);QEo=r(jXr,"Examples:"),jXr.forEach(t),HEo=i(Dt),m(B3.$$.fragment,Dt),Dt.forEach(t),Js.forEach(t),k8e=i(d),ld=n(d,"H2",{class:!0});var jBe=s(ld);O7=n(jBe,"A",{id:!0,class:!0,href:!0});var NXr=s(O7);ste=n(NXr,"SPAN",{});var DXr=s(ste);m(k3.$$.fragment,DXr),DXr.forEach(t),NXr.forEach(t),UEo=i(jBe),lte=n(jBe,"SPAN",{});var qXr=s(lte);JEo=r(qXr,"AutoModelForNextSentencePrediction"),qXr.forEach(t),jBe.forEach(t),x8e=i(d),Ko=n(d,"DIV",{class:!0});var Ks=s(Ko);m(x3.$$.fragment,Ks),YEo=i(Ks),id=n(Ks,"P",{});var sz=s(id);KEo=r(sz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a next sentence prediction head) when created
with the `),ite=n(sz,"CODE",{});var GXr=s(ite);ZEo=r(GXr,"from_pretrained()"),GXr.forEach(t),e3o=r(sz,"class method or the "),dte=n(sz,"CODE",{});var OXr=s(dte);o3o=r(OXr,"from_config()"),OXr.forEach(t),r3o=r(sz,`class
method.`),sz.forEach(t),t3o=i(Ks),R3=n(Ks,"P",{});var NBe=s(R3);a3o=r(NBe,"This class cannot be instantiated directly using "),cte=n(NBe,"CODE",{});var XXr=s(cte);n3o=r(XXr,"__init__()"),XXr.forEach(t),s3o=r(NBe," (throws an error)."),NBe.forEach(t),l3o=i(Ks),Vr=n(Ks,"DIV",{class:!0});var Zs=s(Vr);m(S3.$$.fragment,Zs),i3o=i(Zs),fte=n(Zs,"P",{});var zXr=s(fte);d3o=r(zXr,"Instantiates one of the model classes of the library (with a next sentence prediction head) from a configuration."),zXr.forEach(t),c3o=i(Zs),dd=n(Zs,"P",{});var lz=s(dd);f3o=r(lz,`Note:
Loading a model from its configuration file does `),mte=n(lz,"STRONG",{});var VXr=s(mte);m3o=r(VXr,"not"),VXr.forEach(t),g3o=r(lz,` load the model weights. It only affects the
model\u2019s configuration. Use `),gte=n(lz,"CODE",{});var WXr=s(gte);h3o=r(WXr,"from_pretrained()"),WXr.forEach(t),p3o=r(lz,"to load the model weights."),lz.forEach(t),_3o=i(Zs),hte=n(Zs,"P",{});var QXr=s(hte);u3o=r(QXr,"Examples:"),QXr.forEach(t),b3o=i(Zs),m(P3.$$.fragment,Zs),Zs.forEach(t),v3o=i(Ks),je=n(Ks,"DIV",{class:!0});var qt=s(je);m($3.$$.fragment,qt),T3o=i(qt),pte=n(qt,"P",{});var HXr=s(pte);F3o=r(HXr,"Instantiate one of the model classes of the library (with a next sentence prediction head) from a pretrained model."),HXr.forEach(t),C3o=i(qt),Wa=n(qt,"P",{});var hM=s(Wa);M3o=r(hM,"The model class to instantiate is selected based on the "),_te=n(hM,"CODE",{});var UXr=s(_te);E3o=r(UXr,"model_type"),UXr.forEach(t),y3o=r(hM,` property of the config object (either
passed as an argument or loaded from `),ute=n(hM,"CODE",{});var JXr=s(ute);w3o=r(JXr,"pretrained_model_name_or_path"),JXr.forEach(t),A3o=r(hM,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),bte=n(hM,"CODE",{});var YXr=s(bte);L3o=r(YXr,"pretrained_model_name_or_path"),YXr.forEach(t),B3o=r(hM,":"),hM.forEach(t),k3o=i(qt),na=n(qt,"UL",{});var el=s(na);X7=n(el,"LI",{});var AEe=s(X7);vte=n(AEe,"STRONG",{});var KXr=s(vte);x3o=r(KXr,"bert"),KXr.forEach(t),R3o=r(AEe," \u2014 "),FI=n(AEe,"A",{href:!0});var ZXr=s(FI);S3o=r(ZXr,"BertForNextSentencePrediction"),ZXr.forEach(t),P3o=r(AEe," (BERT model)"),AEe.forEach(t),$3o=i(el),z7=n(el,"LI",{});var LEe=s(z7);Tte=n(LEe,"STRONG",{});var ezr=s(Tte);I3o=r(ezr,"fnet"),ezr.forEach(t),j3o=r(LEe," \u2014 "),CI=n(LEe,"A",{href:!0});var ozr=s(CI);N3o=r(ozr,"FNetForNextSentencePrediction"),ozr.forEach(t),D3o=r(LEe," (FNet model)"),LEe.forEach(t),q3o=i(el),V7=n(el,"LI",{});var BEe=s(V7);Fte=n(BEe,"STRONG",{});var rzr=s(Fte);G3o=r(rzr,"megatron-bert"),rzr.forEach(t),O3o=r(BEe," \u2014 "),MI=n(BEe,"A",{href:!0});var tzr=s(MI);X3o=r(tzr,"MegatronBertForNextSentencePrediction"),tzr.forEach(t),z3o=r(BEe," (MegatronBert model)"),BEe.forEach(t),V3o=i(el),W7=n(el,"LI",{});var kEe=s(W7);Cte=n(kEe,"STRONG",{});var azr=s(Cte);W3o=r(azr,"mobilebert"),azr.forEach(t),Q3o=r(kEe," \u2014 "),EI=n(kEe,"A",{href:!0});var nzr=s(EI);H3o=r(nzr,"MobileBertForNextSentencePrediction"),nzr.forEach(t),U3o=r(kEe," (MobileBERT model)"),kEe.forEach(t),J3o=i(el),Q7=n(el,"LI",{});var xEe=s(Q7);Mte=n(xEe,"STRONG",{});var szr=s(Mte);Y3o=r(szr,"qdqbert"),szr.forEach(t),K3o=r(xEe," \u2014 "),yI=n(xEe,"A",{href:!0});var lzr=s(yI);Z3o=r(lzr,"QDQBertForNextSentencePrediction"),lzr.forEach(t),eyo=r(xEe," (QDQBert model)"),xEe.forEach(t),el.forEach(t),oyo=i(qt),H7=n(qt,"P",{});var REe=s(H7);ryo=r(REe,"The model is set in evaluation mode by default using "),Ete=n(REe,"CODE",{});var izr=s(Ete);tyo=r(izr,"model.eval()"),izr.forEach(t),ayo=r(REe,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),yte=n(REe,"CODE",{});var dzr=s(yte);nyo=r(dzr,"model.train()"),dzr.forEach(t),REe.forEach(t),syo=i(qt),wte=n(qt,"P",{});var czr=s(wte);lyo=r(czr,"Examples:"),czr.forEach(t),iyo=i(qt),m(I3.$$.fragment,qt),qt.forEach(t),Ks.forEach(t),R8e=i(d),cd=n(d,"H2",{class:!0});var DBe=s(cd);U7=n(DBe,"A",{id:!0,class:!0,href:!0});var fzr=s(U7);Ate=n(fzr,"SPAN",{});var mzr=s(Ate);m(j3.$$.fragment,mzr),mzr.forEach(t),fzr.forEach(t),dyo=i(DBe),Lte=n(DBe,"SPAN",{});var gzr=s(Lte);cyo=r(gzr,"AutoModelForTokenClassification"),gzr.forEach(t),DBe.forEach(t),S8e=i(d),Zo=n(d,"DIV",{class:!0});var ol=s(Zo);m(N3.$$.fragment,ol),fyo=i(ol),fd=n(ol,"P",{});var iz=s(fd);myo=r(iz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a token classification head) when created
with the `),Bte=n(iz,"CODE",{});var hzr=s(Bte);gyo=r(hzr,"from_pretrained()"),hzr.forEach(t),hyo=r(iz,"class method or the "),kte=n(iz,"CODE",{});var pzr=s(kte);pyo=r(pzr,"from_config()"),pzr.forEach(t),_yo=r(iz,`class
method.`),iz.forEach(t),uyo=i(ol),D3=n(ol,"P",{});var qBe=s(D3);byo=r(qBe,"This class cannot be instantiated directly using "),xte=n(qBe,"CODE",{});var _zr=s(xte);vyo=r(_zr,"__init__()"),_zr.forEach(t),Tyo=r(qBe," (throws an error)."),qBe.forEach(t),Fyo=i(ol),Wr=n(ol,"DIV",{class:!0});var rl=s(Wr);m(q3.$$.fragment,rl),Cyo=i(rl),Rte=n(rl,"P",{});var uzr=s(Rte);Myo=r(uzr,"Instantiates one of the model classes of the library (with a token classification head) from a configuration."),uzr.forEach(t),Eyo=i(rl),md=n(rl,"P",{});var dz=s(md);yyo=r(dz,`Note:
Loading a model from its configuration file does `),Ste=n(dz,"STRONG",{});var bzr=s(Ste);wyo=r(bzr,"not"),bzr.forEach(t),Ayo=r(dz,` load the model weights. It only affects the
model\u2019s configuration. Use `),Pte=n(dz,"CODE",{});var vzr=s(Pte);Lyo=r(vzr,"from_pretrained()"),vzr.forEach(t),Byo=r(dz,"to load the model weights."),dz.forEach(t),kyo=i(rl),$te=n(rl,"P",{});var Tzr=s($te);xyo=r(Tzr,"Examples:"),Tzr.forEach(t),Ryo=i(rl),m(G3.$$.fragment,rl),rl.forEach(t),Syo=i(ol),Ne=n(ol,"DIV",{class:!0});var Gt=s(Ne);m(O3.$$.fragment,Gt),Pyo=i(Gt),Ite=n(Gt,"P",{});var Fzr=s(Ite);$yo=r(Fzr,"Instantiate one of the model classes of the library (with a token classification head) from a pretrained model."),Fzr.forEach(t),Iyo=i(Gt),Qa=n(Gt,"P",{});var pM=s(Qa);jyo=r(pM,"The model class to instantiate is selected based on the "),jte=n(pM,"CODE",{});var Czr=s(jte);Nyo=r(Czr,"model_type"),Czr.forEach(t),Dyo=r(pM,` property of the config object (either
passed as an argument or loaded from `),Nte=n(pM,"CODE",{});var Mzr=s(Nte);qyo=r(Mzr,"pretrained_model_name_or_path"),Mzr.forEach(t),Gyo=r(pM,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Dte=n(pM,"CODE",{});var Ezr=s(Dte);Oyo=r(Ezr,"pretrained_model_name_or_path"),Ezr.forEach(t),Xyo=r(pM,":"),pM.forEach(t),zyo=i(Gt),D=n(Gt,"UL",{});var q=s(D);J7=n(q,"LI",{});var SEe=s(J7);qte=n(SEe,"STRONG",{});var yzr=s(qte);Vyo=r(yzr,"albert"),yzr.forEach(t),Wyo=r(SEe," \u2014 "),wI=n(SEe,"A",{href:!0});var wzr=s(wI);Qyo=r(wzr,"AlbertForTokenClassification"),wzr.forEach(t),Hyo=r(SEe," (ALBERT model)"),SEe.forEach(t),Uyo=i(q),Y7=n(q,"LI",{});var PEe=s(Y7);Gte=n(PEe,"STRONG",{});var Azr=s(Gte);Jyo=r(Azr,"bert"),Azr.forEach(t),Yyo=r(PEe," \u2014 "),AI=n(PEe,"A",{href:!0});var Lzr=s(AI);Kyo=r(Lzr,"BertForTokenClassification"),Lzr.forEach(t),Zyo=r(PEe," (BERT model)"),PEe.forEach(t),ewo=i(q),K7=n(q,"LI",{});var $Ee=s(K7);Ote=n($Ee,"STRONG",{});var Bzr=s(Ote);owo=r(Bzr,"big_bird"),Bzr.forEach(t),rwo=r($Ee," \u2014 "),LI=n($Ee,"A",{href:!0});var kzr=s(LI);two=r(kzr,"BigBirdForTokenClassification"),kzr.forEach(t),awo=r($Ee," (BigBird model)"),$Ee.forEach(t),nwo=i(q),Z7=n(q,"LI",{});var IEe=s(Z7);Xte=n(IEe,"STRONG",{});var xzr=s(Xte);swo=r(xzr,"camembert"),xzr.forEach(t),lwo=r(IEe," \u2014 "),BI=n(IEe,"A",{href:!0});var Rzr=s(BI);iwo=r(Rzr,"CamembertForTokenClassification"),Rzr.forEach(t),dwo=r(IEe," (CamemBERT model)"),IEe.forEach(t),cwo=i(q),e4=n(q,"LI",{});var jEe=s(e4);zte=n(jEe,"STRONG",{});var Szr=s(zte);fwo=r(Szr,"canine"),Szr.forEach(t),mwo=r(jEe," \u2014 "),kI=n(jEe,"A",{href:!0});var Pzr=s(kI);gwo=r(Pzr,"CanineForTokenClassification"),Pzr.forEach(t),hwo=r(jEe," (Canine model)"),jEe.forEach(t),pwo=i(q),o4=n(q,"LI",{});var NEe=s(o4);Vte=n(NEe,"STRONG",{});var $zr=s(Vte);_wo=r($zr,"convbert"),$zr.forEach(t),uwo=r(NEe," \u2014 "),xI=n(NEe,"A",{href:!0});var Izr=s(xI);bwo=r(Izr,"ConvBertForTokenClassification"),Izr.forEach(t),vwo=r(NEe," (ConvBERT model)"),NEe.forEach(t),Two=i(q),r4=n(q,"LI",{});var DEe=s(r4);Wte=n(DEe,"STRONG",{});var jzr=s(Wte);Fwo=r(jzr,"deberta"),jzr.forEach(t),Cwo=r(DEe," \u2014 "),RI=n(DEe,"A",{href:!0});var Nzr=s(RI);Mwo=r(Nzr,"DebertaForTokenClassification"),Nzr.forEach(t),Ewo=r(DEe," (DeBERTa model)"),DEe.forEach(t),ywo=i(q),t4=n(q,"LI",{});var qEe=s(t4);Qte=n(qEe,"STRONG",{});var Dzr=s(Qte);wwo=r(Dzr,"deberta-v2"),Dzr.forEach(t),Awo=r(qEe," \u2014 "),SI=n(qEe,"A",{href:!0});var qzr=s(SI);Lwo=r(qzr,"DebertaV2ForTokenClassification"),qzr.forEach(t),Bwo=r(qEe," (DeBERTa-v2 model)"),qEe.forEach(t),kwo=i(q),a4=n(q,"LI",{});var GEe=s(a4);Hte=n(GEe,"STRONG",{});var Gzr=s(Hte);xwo=r(Gzr,"distilbert"),Gzr.forEach(t),Rwo=r(GEe," \u2014 "),PI=n(GEe,"A",{href:!0});var Ozr=s(PI);Swo=r(Ozr,"DistilBertForTokenClassification"),Ozr.forEach(t),Pwo=r(GEe," (DistilBERT model)"),GEe.forEach(t),$wo=i(q),n4=n(q,"LI",{});var OEe=s(n4);Ute=n(OEe,"STRONG",{});var Xzr=s(Ute);Iwo=r(Xzr,"electra"),Xzr.forEach(t),jwo=r(OEe," \u2014 "),$I=n(OEe,"A",{href:!0});var zzr=s($I);Nwo=r(zzr,"ElectraForTokenClassification"),zzr.forEach(t),Dwo=r(OEe," (ELECTRA model)"),OEe.forEach(t),qwo=i(q),s4=n(q,"LI",{});var XEe=s(s4);Jte=n(XEe,"STRONG",{});var Vzr=s(Jte);Gwo=r(Vzr,"flaubert"),Vzr.forEach(t),Owo=r(XEe," \u2014 "),II=n(XEe,"A",{href:!0});var Wzr=s(II);Xwo=r(Wzr,"FlaubertForTokenClassification"),Wzr.forEach(t),zwo=r(XEe," (FlauBERT model)"),XEe.forEach(t),Vwo=i(q),l4=n(q,"LI",{});var zEe=s(l4);Yte=n(zEe,"STRONG",{});var Qzr=s(Yte);Wwo=r(Qzr,"fnet"),Qzr.forEach(t),Qwo=r(zEe," \u2014 "),jI=n(zEe,"A",{href:!0});var Hzr=s(jI);Hwo=r(Hzr,"FNetForTokenClassification"),Hzr.forEach(t),Uwo=r(zEe," (FNet model)"),zEe.forEach(t),Jwo=i(q),i4=n(q,"LI",{});var VEe=s(i4);Kte=n(VEe,"STRONG",{});var Uzr=s(Kte);Ywo=r(Uzr,"funnel"),Uzr.forEach(t),Kwo=r(VEe," \u2014 "),NI=n(VEe,"A",{href:!0});var Jzr=s(NI);Zwo=r(Jzr,"FunnelForTokenClassification"),Jzr.forEach(t),eAo=r(VEe," (Funnel Transformer model)"),VEe.forEach(t),oAo=i(q),d4=n(q,"LI",{});var WEe=s(d4);Zte=n(WEe,"STRONG",{});var Yzr=s(Zte);rAo=r(Yzr,"gpt2"),Yzr.forEach(t),tAo=r(WEe," \u2014 "),DI=n(WEe,"A",{href:!0});var Kzr=s(DI);aAo=r(Kzr,"GPT2ForTokenClassification"),Kzr.forEach(t),nAo=r(WEe," (OpenAI GPT-2 model)"),WEe.forEach(t),sAo=i(q),c4=n(q,"LI",{});var QEe=s(c4);eae=n(QEe,"STRONG",{});var Zzr=s(eae);lAo=r(Zzr,"ibert"),Zzr.forEach(t),iAo=r(QEe," \u2014 "),qI=n(QEe,"A",{href:!0});var eVr=s(qI);dAo=r(eVr,"IBertForTokenClassification"),eVr.forEach(t),cAo=r(QEe," (I-BERT model)"),QEe.forEach(t),fAo=i(q),f4=n(q,"LI",{});var HEe=s(f4);oae=n(HEe,"STRONG",{});var oVr=s(oae);mAo=r(oVr,"layoutlm"),oVr.forEach(t),gAo=r(HEe," \u2014 "),GI=n(HEe,"A",{href:!0});var rVr=s(GI);hAo=r(rVr,"LayoutLMForTokenClassification"),rVr.forEach(t),pAo=r(HEe," (LayoutLM model)"),HEe.forEach(t),_Ao=i(q),m4=n(q,"LI",{});var UEe=s(m4);rae=n(UEe,"STRONG",{});var tVr=s(rae);uAo=r(tVr,"layoutlmv2"),tVr.forEach(t),bAo=r(UEe," \u2014 "),OI=n(UEe,"A",{href:!0});var aVr=s(OI);vAo=r(aVr,"LayoutLMv2ForTokenClassification"),aVr.forEach(t),TAo=r(UEe," (LayoutLMv2 model)"),UEe.forEach(t),FAo=i(q),g4=n(q,"LI",{});var JEe=s(g4);tae=n(JEe,"STRONG",{});var nVr=s(tae);CAo=r(nVr,"longformer"),nVr.forEach(t),MAo=r(JEe," \u2014 "),XI=n(JEe,"A",{href:!0});var sVr=s(XI);EAo=r(sVr,"LongformerForTokenClassification"),sVr.forEach(t),yAo=r(JEe," (Longformer model)"),JEe.forEach(t),wAo=i(q),h4=n(q,"LI",{});var YEe=s(h4);aae=n(YEe,"STRONG",{});var lVr=s(aae);AAo=r(lVr,"megatron-bert"),lVr.forEach(t),LAo=r(YEe," \u2014 "),zI=n(YEe,"A",{href:!0});var iVr=s(zI);BAo=r(iVr,"MegatronBertForTokenClassification"),iVr.forEach(t),kAo=r(YEe," (MegatronBert model)"),YEe.forEach(t),xAo=i(q),p4=n(q,"LI",{});var KEe=s(p4);nae=n(KEe,"STRONG",{});var dVr=s(nae);RAo=r(dVr,"mobilebert"),dVr.forEach(t),SAo=r(KEe," \u2014 "),VI=n(KEe,"A",{href:!0});var cVr=s(VI);PAo=r(cVr,"MobileBertForTokenClassification"),cVr.forEach(t),$Ao=r(KEe," (MobileBERT model)"),KEe.forEach(t),IAo=i(q),_4=n(q,"LI",{});var ZEe=s(_4);sae=n(ZEe,"STRONG",{});var fVr=s(sae);jAo=r(fVr,"mpnet"),fVr.forEach(t),NAo=r(ZEe," \u2014 "),WI=n(ZEe,"A",{href:!0});var mVr=s(WI);DAo=r(mVr,"MPNetForTokenClassification"),mVr.forEach(t),qAo=r(ZEe," (MPNet model)"),ZEe.forEach(t),GAo=i(q),u4=n(q,"LI",{});var e3e=s(u4);lae=n(e3e,"STRONG",{});var gVr=s(lae);OAo=r(gVr,"nystromformer"),gVr.forEach(t),XAo=r(e3e," \u2014 "),QI=n(e3e,"A",{href:!0});var hVr=s(QI);zAo=r(hVr,"NystromformerForTokenClassification"),hVr.forEach(t),VAo=r(e3e," (Nystromformer model)"),e3e.forEach(t),WAo=i(q),b4=n(q,"LI",{});var o3e=s(b4);iae=n(o3e,"STRONG",{});var pVr=s(iae);QAo=r(pVr,"qdqbert"),pVr.forEach(t),HAo=r(o3e," \u2014 "),HI=n(o3e,"A",{href:!0});var _Vr=s(HI);UAo=r(_Vr,"QDQBertForTokenClassification"),_Vr.forEach(t),JAo=r(o3e," (QDQBert model)"),o3e.forEach(t),YAo=i(q),v4=n(q,"LI",{});var r3e=s(v4);dae=n(r3e,"STRONG",{});var uVr=s(dae);KAo=r(uVr,"rembert"),uVr.forEach(t),ZAo=r(r3e," \u2014 "),UI=n(r3e,"A",{href:!0});var bVr=s(UI);e6o=r(bVr,"RemBertForTokenClassification"),bVr.forEach(t),o6o=r(r3e," (RemBERT model)"),r3e.forEach(t),r6o=i(q),T4=n(q,"LI",{});var t3e=s(T4);cae=n(t3e,"STRONG",{});var vVr=s(cae);t6o=r(vVr,"roberta"),vVr.forEach(t),a6o=r(t3e," \u2014 "),JI=n(t3e,"A",{href:!0});var TVr=s(JI);n6o=r(TVr,"RobertaForTokenClassification"),TVr.forEach(t),s6o=r(t3e," (RoBERTa model)"),t3e.forEach(t),l6o=i(q),F4=n(q,"LI",{});var a3e=s(F4);fae=n(a3e,"STRONG",{});var FVr=s(fae);i6o=r(FVr,"roformer"),FVr.forEach(t),d6o=r(a3e," \u2014 "),YI=n(a3e,"A",{href:!0});var CVr=s(YI);c6o=r(CVr,"RoFormerForTokenClassification"),CVr.forEach(t),f6o=r(a3e," (RoFormer model)"),a3e.forEach(t),m6o=i(q),C4=n(q,"LI",{});var n3e=s(C4);mae=n(n3e,"STRONG",{});var MVr=s(mae);g6o=r(MVr,"squeezebert"),MVr.forEach(t),h6o=r(n3e," \u2014 "),KI=n(n3e,"A",{href:!0});var EVr=s(KI);p6o=r(EVr,"SqueezeBertForTokenClassification"),EVr.forEach(t),_6o=r(n3e," (SqueezeBERT model)"),n3e.forEach(t),u6o=i(q),M4=n(q,"LI",{});var s3e=s(M4);gae=n(s3e,"STRONG",{});var yVr=s(gae);b6o=r(yVr,"xlm"),yVr.forEach(t),v6o=r(s3e," \u2014 "),ZI=n(s3e,"A",{href:!0});var wVr=s(ZI);T6o=r(wVr,"XLMForTokenClassification"),wVr.forEach(t),F6o=r(s3e," (XLM model)"),s3e.forEach(t),C6o=i(q),E4=n(q,"LI",{});var l3e=s(E4);hae=n(l3e,"STRONG",{});var AVr=s(hae);M6o=r(AVr,"xlm-roberta"),AVr.forEach(t),E6o=r(l3e," \u2014 "),ej=n(l3e,"A",{href:!0});var LVr=s(ej);y6o=r(LVr,"XLMRobertaForTokenClassification"),LVr.forEach(t),w6o=r(l3e," (XLM-RoBERTa model)"),l3e.forEach(t),A6o=i(q),y4=n(q,"LI",{});var i3e=s(y4);pae=n(i3e,"STRONG",{});var BVr=s(pae);L6o=r(BVr,"xlm-roberta-xl"),BVr.forEach(t),B6o=r(i3e," \u2014 "),oj=n(i3e,"A",{href:!0});var kVr=s(oj);k6o=r(kVr,"XLMRobertaXLForTokenClassification"),kVr.forEach(t),x6o=r(i3e," (XLM-RoBERTa-XL model)"),i3e.forEach(t),R6o=i(q),w4=n(q,"LI",{});var d3e=s(w4);_ae=n(d3e,"STRONG",{});var xVr=s(_ae);S6o=r(xVr,"xlnet"),xVr.forEach(t),P6o=r(d3e," \u2014 "),rj=n(d3e,"A",{href:!0});var RVr=s(rj);$6o=r(RVr,"XLNetForTokenClassification"),RVr.forEach(t),I6o=r(d3e," (XLNet model)"),d3e.forEach(t),j6o=i(q),A4=n(q,"LI",{});var c3e=s(A4);uae=n(c3e,"STRONG",{});var SVr=s(uae);N6o=r(SVr,"yoso"),SVr.forEach(t),D6o=r(c3e," \u2014 "),tj=n(c3e,"A",{href:!0});var PVr=s(tj);q6o=r(PVr,"YosoForTokenClassification"),PVr.forEach(t),G6o=r(c3e," (YOSO model)"),c3e.forEach(t),q.forEach(t),O6o=i(Gt),L4=n(Gt,"P",{});var f3e=s(L4);X6o=r(f3e,"The model is set in evaluation mode by default using "),bae=n(f3e,"CODE",{});var $Vr=s(bae);z6o=r($Vr,"model.eval()"),$Vr.forEach(t),V6o=r(f3e,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),vae=n(f3e,"CODE",{});var IVr=s(vae);W6o=r(IVr,"model.train()"),IVr.forEach(t),f3e.forEach(t),Q6o=i(Gt),Tae=n(Gt,"P",{});var jVr=s(Tae);H6o=r(jVr,"Examples:"),jVr.forEach(t),U6o=i(Gt),m(X3.$$.fragment,Gt),Gt.forEach(t),ol.forEach(t),P8e=i(d),gd=n(d,"H2",{class:!0});var GBe=s(gd);B4=n(GBe,"A",{id:!0,class:!0,href:!0});var NVr=s(B4);Fae=n(NVr,"SPAN",{});var DVr=s(Fae);m(z3.$$.fragment,DVr),DVr.forEach(t),NVr.forEach(t),J6o=i(GBe),Cae=n(GBe,"SPAN",{});var qVr=s(Cae);Y6o=r(qVr,"AutoModelForQuestionAnswering"),qVr.forEach(t),GBe.forEach(t),$8e=i(d),er=n(d,"DIV",{class:!0});var tl=s(er);m(V3.$$.fragment,tl),K6o=i(tl),hd=n(tl,"P",{});var cz=s(hd);Z6o=r(cz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a question answering head) when created
with the `),Mae=n(cz,"CODE",{});var GVr=s(Mae);e0o=r(GVr,"from_pretrained()"),GVr.forEach(t),o0o=r(cz,"class method or the "),Eae=n(cz,"CODE",{});var OVr=s(Eae);r0o=r(OVr,"from_config()"),OVr.forEach(t),t0o=r(cz,`class
method.`),cz.forEach(t),a0o=i(tl),W3=n(tl,"P",{});var OBe=s(W3);n0o=r(OBe,"This class cannot be instantiated directly using "),yae=n(OBe,"CODE",{});var XVr=s(yae);s0o=r(XVr,"__init__()"),XVr.forEach(t),l0o=r(OBe," (throws an error)."),OBe.forEach(t),i0o=i(tl),Qr=n(tl,"DIV",{class:!0});var al=s(Qr);m(Q3.$$.fragment,al),d0o=i(al),wae=n(al,"P",{});var zVr=s(wae);c0o=r(zVr,"Instantiates one of the model classes of the library (with a question answering head) from a configuration."),zVr.forEach(t),f0o=i(al),pd=n(al,"P",{});var fz=s(pd);m0o=r(fz,`Note:
Loading a model from its configuration file does `),Aae=n(fz,"STRONG",{});var VVr=s(Aae);g0o=r(VVr,"not"),VVr.forEach(t),h0o=r(fz,` load the model weights. It only affects the
model\u2019s configuration. Use `),Lae=n(fz,"CODE",{});var WVr=s(Lae);p0o=r(WVr,"from_pretrained()"),WVr.forEach(t),_0o=r(fz,"to load the model weights."),fz.forEach(t),u0o=i(al),Bae=n(al,"P",{});var QVr=s(Bae);b0o=r(QVr,"Examples:"),QVr.forEach(t),v0o=i(al),m(H3.$$.fragment,al),al.forEach(t),T0o=i(tl),De=n(tl,"DIV",{class:!0});var Ot=s(De);m(U3.$$.fragment,Ot),F0o=i(Ot),kae=n(Ot,"P",{});var HVr=s(kae);C0o=r(HVr,"Instantiate one of the model classes of the library (with a question answering head) from a pretrained model."),HVr.forEach(t),M0o=i(Ot),Ha=n(Ot,"P",{});var _M=s(Ha);E0o=r(_M,"The model class to instantiate is selected based on the "),xae=n(_M,"CODE",{});var UVr=s(xae);y0o=r(UVr,"model_type"),UVr.forEach(t),w0o=r(_M,` property of the config object (either
passed as an argument or loaded from `),Rae=n(_M,"CODE",{});var JVr=s(Rae);A0o=r(JVr,"pretrained_model_name_or_path"),JVr.forEach(t),L0o=r(_M,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Sae=n(_M,"CODE",{});var YVr=s(Sae);B0o=r(YVr,"pretrained_model_name_or_path"),YVr.forEach(t),k0o=r(_M,":"),_M.forEach(t),x0o=i(Ot),R=n(Ot,"UL",{});var P=s(R);k4=n(P,"LI",{});var m3e=s(k4);Pae=n(m3e,"STRONG",{});var KVr=s(Pae);R0o=r(KVr,"albert"),KVr.forEach(t),S0o=r(m3e," \u2014 "),aj=n(m3e,"A",{href:!0});var ZVr=s(aj);P0o=r(ZVr,"AlbertForQuestionAnswering"),ZVr.forEach(t),$0o=r(m3e," (ALBERT model)"),m3e.forEach(t),I0o=i(P),x4=n(P,"LI",{});var g3e=s(x4);$ae=n(g3e,"STRONG",{});var eWr=s($ae);j0o=r(eWr,"bart"),eWr.forEach(t),N0o=r(g3e," \u2014 "),nj=n(g3e,"A",{href:!0});var oWr=s(nj);D0o=r(oWr,"BartForQuestionAnswering"),oWr.forEach(t),q0o=r(g3e," (BART model)"),g3e.forEach(t),G0o=i(P),R4=n(P,"LI",{});var h3e=s(R4);Iae=n(h3e,"STRONG",{});var rWr=s(Iae);O0o=r(rWr,"bert"),rWr.forEach(t),X0o=r(h3e," \u2014 "),sj=n(h3e,"A",{href:!0});var tWr=s(sj);z0o=r(tWr,"BertForQuestionAnswering"),tWr.forEach(t),V0o=r(h3e," (BERT model)"),h3e.forEach(t),W0o=i(P),S4=n(P,"LI",{});var p3e=s(S4);jae=n(p3e,"STRONG",{});var aWr=s(jae);Q0o=r(aWr,"big_bird"),aWr.forEach(t),H0o=r(p3e," \u2014 "),lj=n(p3e,"A",{href:!0});var nWr=s(lj);U0o=r(nWr,"BigBirdForQuestionAnswering"),nWr.forEach(t),J0o=r(p3e," (BigBird model)"),p3e.forEach(t),Y0o=i(P),P4=n(P,"LI",{});var _3e=s(P4);Nae=n(_3e,"STRONG",{});var sWr=s(Nae);K0o=r(sWr,"bigbird_pegasus"),sWr.forEach(t),Z0o=r(_3e," \u2014 "),ij=n(_3e,"A",{href:!0});var lWr=s(ij);eLo=r(lWr,"BigBirdPegasusForQuestionAnswering"),lWr.forEach(t),oLo=r(_3e," (BigBirdPegasus model)"),_3e.forEach(t),rLo=i(P),$4=n(P,"LI",{});var u3e=s($4);Dae=n(u3e,"STRONG",{});var iWr=s(Dae);tLo=r(iWr,"camembert"),iWr.forEach(t),aLo=r(u3e," \u2014 "),dj=n(u3e,"A",{href:!0});var dWr=s(dj);nLo=r(dWr,"CamembertForQuestionAnswering"),dWr.forEach(t),sLo=r(u3e," (CamemBERT model)"),u3e.forEach(t),lLo=i(P),I4=n(P,"LI",{});var b3e=s(I4);qae=n(b3e,"STRONG",{});var cWr=s(qae);iLo=r(cWr,"canine"),cWr.forEach(t),dLo=r(b3e," \u2014 "),cj=n(b3e,"A",{href:!0});var fWr=s(cj);cLo=r(fWr,"CanineForQuestionAnswering"),fWr.forEach(t),fLo=r(b3e," (Canine model)"),b3e.forEach(t),mLo=i(P),j4=n(P,"LI",{});var v3e=s(j4);Gae=n(v3e,"STRONG",{});var mWr=s(Gae);gLo=r(mWr,"convbert"),mWr.forEach(t),hLo=r(v3e," \u2014 "),fj=n(v3e,"A",{href:!0});var gWr=s(fj);pLo=r(gWr,"ConvBertForQuestionAnswering"),gWr.forEach(t),_Lo=r(v3e," (ConvBERT model)"),v3e.forEach(t),uLo=i(P),N4=n(P,"LI",{});var T3e=s(N4);Oae=n(T3e,"STRONG",{});var hWr=s(Oae);bLo=r(hWr,"deberta"),hWr.forEach(t),vLo=r(T3e," \u2014 "),mj=n(T3e,"A",{href:!0});var pWr=s(mj);TLo=r(pWr,"DebertaForQuestionAnswering"),pWr.forEach(t),FLo=r(T3e," (DeBERTa model)"),T3e.forEach(t),CLo=i(P),D4=n(P,"LI",{});var F3e=s(D4);Xae=n(F3e,"STRONG",{});var _Wr=s(Xae);MLo=r(_Wr,"deberta-v2"),_Wr.forEach(t),ELo=r(F3e," \u2014 "),gj=n(F3e,"A",{href:!0});var uWr=s(gj);yLo=r(uWr,"DebertaV2ForQuestionAnswering"),uWr.forEach(t),wLo=r(F3e," (DeBERTa-v2 model)"),F3e.forEach(t),ALo=i(P),q4=n(P,"LI",{});var C3e=s(q4);zae=n(C3e,"STRONG",{});var bWr=s(zae);LLo=r(bWr,"distilbert"),bWr.forEach(t),BLo=r(C3e," \u2014 "),hj=n(C3e,"A",{href:!0});var vWr=s(hj);kLo=r(vWr,"DistilBertForQuestionAnswering"),vWr.forEach(t),xLo=r(C3e," (DistilBERT model)"),C3e.forEach(t),RLo=i(P),G4=n(P,"LI",{});var M3e=s(G4);Vae=n(M3e,"STRONG",{});var TWr=s(Vae);SLo=r(TWr,"electra"),TWr.forEach(t),PLo=r(M3e," \u2014 "),pj=n(M3e,"A",{href:!0});var FWr=s(pj);$Lo=r(FWr,"ElectraForQuestionAnswering"),FWr.forEach(t),ILo=r(M3e," (ELECTRA model)"),M3e.forEach(t),jLo=i(P),O4=n(P,"LI",{});var E3e=s(O4);Wae=n(E3e,"STRONG",{});var CWr=s(Wae);NLo=r(CWr,"flaubert"),CWr.forEach(t),DLo=r(E3e," \u2014 "),_j=n(E3e,"A",{href:!0});var MWr=s(_j);qLo=r(MWr,"FlaubertForQuestionAnsweringSimple"),MWr.forEach(t),GLo=r(E3e," (FlauBERT model)"),E3e.forEach(t),OLo=i(P),X4=n(P,"LI",{});var y3e=s(X4);Qae=n(y3e,"STRONG",{});var EWr=s(Qae);XLo=r(EWr,"fnet"),EWr.forEach(t),zLo=r(y3e," \u2014 "),uj=n(y3e,"A",{href:!0});var yWr=s(uj);VLo=r(yWr,"FNetForQuestionAnswering"),yWr.forEach(t),WLo=r(y3e," (FNet model)"),y3e.forEach(t),QLo=i(P),z4=n(P,"LI",{});var w3e=s(z4);Hae=n(w3e,"STRONG",{});var wWr=s(Hae);HLo=r(wWr,"funnel"),wWr.forEach(t),ULo=r(w3e," \u2014 "),bj=n(w3e,"A",{href:!0});var AWr=s(bj);JLo=r(AWr,"FunnelForQuestionAnswering"),AWr.forEach(t),YLo=r(w3e," (Funnel Transformer model)"),w3e.forEach(t),KLo=i(P),V4=n(P,"LI",{});var A3e=s(V4);Uae=n(A3e,"STRONG",{});var LWr=s(Uae);ZLo=r(LWr,"gptj"),LWr.forEach(t),e8o=r(A3e," \u2014 "),vj=n(A3e,"A",{href:!0});var BWr=s(vj);o8o=r(BWr,"GPTJForQuestionAnswering"),BWr.forEach(t),r8o=r(A3e," (GPT-J model)"),A3e.forEach(t),t8o=i(P),W4=n(P,"LI",{});var L3e=s(W4);Jae=n(L3e,"STRONG",{});var kWr=s(Jae);a8o=r(kWr,"ibert"),kWr.forEach(t),n8o=r(L3e," \u2014 "),Tj=n(L3e,"A",{href:!0});var xWr=s(Tj);s8o=r(xWr,"IBertForQuestionAnswering"),xWr.forEach(t),l8o=r(L3e," (I-BERT model)"),L3e.forEach(t),i8o=i(P),Q4=n(P,"LI",{});var B3e=s(Q4);Yae=n(B3e,"STRONG",{});var RWr=s(Yae);d8o=r(RWr,"layoutlmv2"),RWr.forEach(t),c8o=r(B3e," \u2014 "),Fj=n(B3e,"A",{href:!0});var SWr=s(Fj);f8o=r(SWr,"LayoutLMv2ForQuestionAnswering"),SWr.forEach(t),m8o=r(B3e," (LayoutLMv2 model)"),B3e.forEach(t),g8o=i(P),H4=n(P,"LI",{});var k3e=s(H4);Kae=n(k3e,"STRONG",{});var PWr=s(Kae);h8o=r(PWr,"led"),PWr.forEach(t),p8o=r(k3e," \u2014 "),Cj=n(k3e,"A",{href:!0});var $Wr=s(Cj);_8o=r($Wr,"LEDForQuestionAnswering"),$Wr.forEach(t),u8o=r(k3e," (LED model)"),k3e.forEach(t),b8o=i(P),U4=n(P,"LI",{});var x3e=s(U4);Zae=n(x3e,"STRONG",{});var IWr=s(Zae);v8o=r(IWr,"longformer"),IWr.forEach(t),T8o=r(x3e," \u2014 "),Mj=n(x3e,"A",{href:!0});var jWr=s(Mj);F8o=r(jWr,"LongformerForQuestionAnswering"),jWr.forEach(t),C8o=r(x3e," (Longformer model)"),x3e.forEach(t),M8o=i(P),J4=n(P,"LI",{});var R3e=s(J4);ene=n(R3e,"STRONG",{});var NWr=s(ene);E8o=r(NWr,"lxmert"),NWr.forEach(t),y8o=r(R3e," \u2014 "),Ej=n(R3e,"A",{href:!0});var DWr=s(Ej);w8o=r(DWr,"LxmertForQuestionAnswering"),DWr.forEach(t),A8o=r(R3e," (LXMERT model)"),R3e.forEach(t),L8o=i(P),Y4=n(P,"LI",{});var S3e=s(Y4);one=n(S3e,"STRONG",{});var qWr=s(one);B8o=r(qWr,"mbart"),qWr.forEach(t),k8o=r(S3e," \u2014 "),yj=n(S3e,"A",{href:!0});var GWr=s(yj);x8o=r(GWr,"MBartForQuestionAnswering"),GWr.forEach(t),R8o=r(S3e," (mBART model)"),S3e.forEach(t),S8o=i(P),K4=n(P,"LI",{});var P3e=s(K4);rne=n(P3e,"STRONG",{});var OWr=s(rne);P8o=r(OWr,"megatron-bert"),OWr.forEach(t),$8o=r(P3e," \u2014 "),wj=n(P3e,"A",{href:!0});var XWr=s(wj);I8o=r(XWr,"MegatronBertForQuestionAnswering"),XWr.forEach(t),j8o=r(P3e," (MegatronBert model)"),P3e.forEach(t),N8o=i(P),Z4=n(P,"LI",{});var $3e=s(Z4);tne=n($3e,"STRONG",{});var zWr=s(tne);D8o=r(zWr,"mobilebert"),zWr.forEach(t),q8o=r($3e," \u2014 "),Aj=n($3e,"A",{href:!0});var VWr=s(Aj);G8o=r(VWr,"MobileBertForQuestionAnswering"),VWr.forEach(t),O8o=r($3e," (MobileBERT model)"),$3e.forEach(t),X8o=i(P),eb=n(P,"LI",{});var I3e=s(eb);ane=n(I3e,"STRONG",{});var WWr=s(ane);z8o=r(WWr,"mpnet"),WWr.forEach(t),V8o=r(I3e," \u2014 "),Lj=n(I3e,"A",{href:!0});var QWr=s(Lj);W8o=r(QWr,"MPNetForQuestionAnswering"),QWr.forEach(t),Q8o=r(I3e," (MPNet model)"),I3e.forEach(t),H8o=i(P),ob=n(P,"LI",{});var j3e=s(ob);nne=n(j3e,"STRONG",{});var HWr=s(nne);U8o=r(HWr,"nystromformer"),HWr.forEach(t),J8o=r(j3e," \u2014 "),Bj=n(j3e,"A",{href:!0});var UWr=s(Bj);Y8o=r(UWr,"NystromformerForQuestionAnswering"),UWr.forEach(t),K8o=r(j3e," (Nystromformer model)"),j3e.forEach(t),Z8o=i(P),rb=n(P,"LI",{});var N3e=s(rb);sne=n(N3e,"STRONG",{});var JWr=s(sne);e9o=r(JWr,"qdqbert"),JWr.forEach(t),o9o=r(N3e," \u2014 "),kj=n(N3e,"A",{href:!0});var YWr=s(kj);r9o=r(YWr,"QDQBertForQuestionAnswering"),YWr.forEach(t),t9o=r(N3e," (QDQBert model)"),N3e.forEach(t),a9o=i(P),tb=n(P,"LI",{});var D3e=s(tb);lne=n(D3e,"STRONG",{});var KWr=s(lne);n9o=r(KWr,"reformer"),KWr.forEach(t),s9o=r(D3e," \u2014 "),xj=n(D3e,"A",{href:!0});var ZWr=s(xj);l9o=r(ZWr,"ReformerForQuestionAnswering"),ZWr.forEach(t),i9o=r(D3e," (Reformer model)"),D3e.forEach(t),d9o=i(P),ab=n(P,"LI",{});var q3e=s(ab);ine=n(q3e,"STRONG",{});var eQr=s(ine);c9o=r(eQr,"rembert"),eQr.forEach(t),f9o=r(q3e," \u2014 "),Rj=n(q3e,"A",{href:!0});var oQr=s(Rj);m9o=r(oQr,"RemBertForQuestionAnswering"),oQr.forEach(t),g9o=r(q3e," (RemBERT model)"),q3e.forEach(t),h9o=i(P),nb=n(P,"LI",{});var G3e=s(nb);dne=n(G3e,"STRONG",{});var rQr=s(dne);p9o=r(rQr,"roberta"),rQr.forEach(t),_9o=r(G3e," \u2014 "),Sj=n(G3e,"A",{href:!0});var tQr=s(Sj);u9o=r(tQr,"RobertaForQuestionAnswering"),tQr.forEach(t),b9o=r(G3e," (RoBERTa model)"),G3e.forEach(t),v9o=i(P),sb=n(P,"LI",{});var O3e=s(sb);cne=n(O3e,"STRONG",{});var aQr=s(cne);T9o=r(aQr,"roformer"),aQr.forEach(t),F9o=r(O3e," \u2014 "),Pj=n(O3e,"A",{href:!0});var nQr=s(Pj);C9o=r(nQr,"RoFormerForQuestionAnswering"),nQr.forEach(t),M9o=r(O3e," (RoFormer model)"),O3e.forEach(t),E9o=i(P),lb=n(P,"LI",{});var X3e=s(lb);fne=n(X3e,"STRONG",{});var sQr=s(fne);y9o=r(sQr,"splinter"),sQr.forEach(t),w9o=r(X3e," \u2014 "),$j=n(X3e,"A",{href:!0});var lQr=s($j);A9o=r(lQr,"SplinterForQuestionAnswering"),lQr.forEach(t),L9o=r(X3e," (Splinter model)"),X3e.forEach(t),B9o=i(P),ib=n(P,"LI",{});var z3e=s(ib);mne=n(z3e,"STRONG",{});var iQr=s(mne);k9o=r(iQr,"squeezebert"),iQr.forEach(t),x9o=r(z3e," \u2014 "),Ij=n(z3e,"A",{href:!0});var dQr=s(Ij);R9o=r(dQr,"SqueezeBertForQuestionAnswering"),dQr.forEach(t),S9o=r(z3e," (SqueezeBERT model)"),z3e.forEach(t),P9o=i(P),db=n(P,"LI",{});var V3e=s(db);gne=n(V3e,"STRONG",{});var cQr=s(gne);$9o=r(cQr,"xlm"),cQr.forEach(t),I9o=r(V3e," \u2014 "),jj=n(V3e,"A",{href:!0});var fQr=s(jj);j9o=r(fQr,"XLMForQuestionAnsweringSimple"),fQr.forEach(t),N9o=r(V3e," (XLM model)"),V3e.forEach(t),D9o=i(P),cb=n(P,"LI",{});var W3e=s(cb);hne=n(W3e,"STRONG",{});var mQr=s(hne);q9o=r(mQr,"xlm-roberta"),mQr.forEach(t),G9o=r(W3e," \u2014 "),Nj=n(W3e,"A",{href:!0});var gQr=s(Nj);O9o=r(gQr,"XLMRobertaForQuestionAnswering"),gQr.forEach(t),X9o=r(W3e," (XLM-RoBERTa model)"),W3e.forEach(t),z9o=i(P),fb=n(P,"LI",{});var Q3e=s(fb);pne=n(Q3e,"STRONG",{});var hQr=s(pne);V9o=r(hQr,"xlm-roberta-xl"),hQr.forEach(t),W9o=r(Q3e," \u2014 "),Dj=n(Q3e,"A",{href:!0});var pQr=s(Dj);Q9o=r(pQr,"XLMRobertaXLForQuestionAnswering"),pQr.forEach(t),H9o=r(Q3e," (XLM-RoBERTa-XL model)"),Q3e.forEach(t),U9o=i(P),mb=n(P,"LI",{});var H3e=s(mb);_ne=n(H3e,"STRONG",{});var _Qr=s(_ne);J9o=r(_Qr,"xlnet"),_Qr.forEach(t),Y9o=r(H3e," \u2014 "),qj=n(H3e,"A",{href:!0});var uQr=s(qj);K9o=r(uQr,"XLNetForQuestionAnsweringSimple"),uQr.forEach(t),Z9o=r(H3e," (XLNet model)"),H3e.forEach(t),eBo=i(P),gb=n(P,"LI",{});var U3e=s(gb);une=n(U3e,"STRONG",{});var bQr=s(une);oBo=r(bQr,"yoso"),bQr.forEach(t),rBo=r(U3e," \u2014 "),Gj=n(U3e,"A",{href:!0});var vQr=s(Gj);tBo=r(vQr,"YosoForQuestionAnswering"),vQr.forEach(t),aBo=r(U3e," (YOSO model)"),U3e.forEach(t),P.forEach(t),nBo=i(Ot),hb=n(Ot,"P",{});var J3e=s(hb);sBo=r(J3e,"The model is set in evaluation mode by default using "),bne=n(J3e,"CODE",{});var TQr=s(bne);lBo=r(TQr,"model.eval()"),TQr.forEach(t),iBo=r(J3e,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),vne=n(J3e,"CODE",{});var FQr=s(vne);dBo=r(FQr,"model.train()"),FQr.forEach(t),J3e.forEach(t),cBo=i(Ot),Tne=n(Ot,"P",{});var CQr=s(Tne);fBo=r(CQr,"Examples:"),CQr.forEach(t),mBo=i(Ot),m(J3.$$.fragment,Ot),Ot.forEach(t),tl.forEach(t),I8e=i(d),_d=n(d,"H2",{class:!0});var XBe=s(_d);pb=n(XBe,"A",{id:!0,class:!0,href:!0});var MQr=s(pb);Fne=n(MQr,"SPAN",{});var EQr=s(Fne);m(Y3.$$.fragment,EQr),EQr.forEach(t),MQr.forEach(t),gBo=i(XBe),Cne=n(XBe,"SPAN",{});var yQr=s(Cne);hBo=r(yQr,"AutoModelForTableQuestionAnswering"),yQr.forEach(t),XBe.forEach(t),j8e=i(d),or=n(d,"DIV",{class:!0});var nl=s(or);m(K3.$$.fragment,nl),pBo=i(nl),ud=n(nl,"P",{});var mz=s(ud);_Bo=r(mz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a table question answering head) when created
with the `),Mne=n(mz,"CODE",{});var wQr=s(Mne);uBo=r(wQr,"from_pretrained()"),wQr.forEach(t),bBo=r(mz,"class method or the "),Ene=n(mz,"CODE",{});var AQr=s(Ene);vBo=r(AQr,"from_config()"),AQr.forEach(t),TBo=r(mz,`class
method.`),mz.forEach(t),FBo=i(nl),Z3=n(nl,"P",{});var zBe=s(Z3);CBo=r(zBe,"This class cannot be instantiated directly using "),yne=n(zBe,"CODE",{});var LQr=s(yne);MBo=r(LQr,"__init__()"),LQr.forEach(t),EBo=r(zBe," (throws an error)."),zBe.forEach(t),yBo=i(nl),Hr=n(nl,"DIV",{class:!0});var sl=s(Hr);m(ey.$$.fragment,sl),wBo=i(sl),wne=n(sl,"P",{});var BQr=s(wne);ABo=r(BQr,"Instantiates one of the model classes of the library (with a table question answering head) from a configuration."),BQr.forEach(t),LBo=i(sl),bd=n(sl,"P",{});var gz=s(bd);BBo=r(gz,`Note:
Loading a model from its configuration file does `),Ane=n(gz,"STRONG",{});var kQr=s(Ane);kBo=r(kQr,"not"),kQr.forEach(t),xBo=r(gz,` load the model weights. It only affects the
model\u2019s configuration. Use `),Lne=n(gz,"CODE",{});var xQr=s(Lne);RBo=r(xQr,"from_pretrained()"),xQr.forEach(t),SBo=r(gz,"to load the model weights."),gz.forEach(t),PBo=i(sl),Bne=n(sl,"P",{});var RQr=s(Bne);$Bo=r(RQr,"Examples:"),RQr.forEach(t),IBo=i(sl),m(oy.$$.fragment,sl),sl.forEach(t),jBo=i(nl),qe=n(nl,"DIV",{class:!0});var Xt=s(qe);m(ry.$$.fragment,Xt),NBo=i(Xt),kne=n(Xt,"P",{});var SQr=s(kne);DBo=r(SQr,"Instantiate one of the model classes of the library (with a table question answering head) from a pretrained model."),SQr.forEach(t),qBo=i(Xt),Ua=n(Xt,"P",{});var uM=s(Ua);GBo=r(uM,"The model class to instantiate is selected based on the "),xne=n(uM,"CODE",{});var PQr=s(xne);OBo=r(PQr,"model_type"),PQr.forEach(t),XBo=r(uM,` property of the config object (either
passed as an argument or loaded from `),Rne=n(uM,"CODE",{});var $Qr=s(Rne);zBo=r($Qr,"pretrained_model_name_or_path"),$Qr.forEach(t),VBo=r(uM,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Sne=n(uM,"CODE",{});var IQr=s(Sne);WBo=r(IQr,"pretrained_model_name_or_path"),IQr.forEach(t),QBo=r(uM,":"),uM.forEach(t),HBo=i(Xt),Pne=n(Xt,"UL",{});var jQr=s(Pne);_b=n(jQr,"LI",{});var Y3e=s(_b);$ne=n(Y3e,"STRONG",{});var NQr=s($ne);UBo=r(NQr,"tapas"),NQr.forEach(t),JBo=r(Y3e," \u2014 "),Oj=n(Y3e,"A",{href:!0});var DQr=s(Oj);YBo=r(DQr,"TapasForQuestionAnswering"),DQr.forEach(t),KBo=r(Y3e," (TAPAS model)"),Y3e.forEach(t),jQr.forEach(t),ZBo=i(Xt),ub=n(Xt,"P",{});var K3e=s(ub);eko=r(K3e,"The model is set in evaluation mode by default using "),Ine=n(K3e,"CODE",{});var qQr=s(Ine);oko=r(qQr,"model.eval()"),qQr.forEach(t),rko=r(K3e,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),jne=n(K3e,"CODE",{});var GQr=s(jne);tko=r(GQr,"model.train()"),GQr.forEach(t),K3e.forEach(t),ako=i(Xt),Nne=n(Xt,"P",{});var OQr=s(Nne);nko=r(OQr,"Examples:"),OQr.forEach(t),sko=i(Xt),m(ty.$$.fragment,Xt),Xt.forEach(t),nl.forEach(t),N8e=i(d),vd=n(d,"H2",{class:!0});var VBe=s(vd);bb=n(VBe,"A",{id:!0,class:!0,href:!0});var XQr=s(bb);Dne=n(XQr,"SPAN",{});var zQr=s(Dne);m(ay.$$.fragment,zQr),zQr.forEach(t),XQr.forEach(t),lko=i(VBe),qne=n(VBe,"SPAN",{});var VQr=s(qne);iko=r(VQr,"AutoModelForImageClassification"),VQr.forEach(t),VBe.forEach(t),D8e=i(d),rr=n(d,"DIV",{class:!0});var ll=s(rr);m(ny.$$.fragment,ll),dko=i(ll),Td=n(ll,"P",{});var hz=s(Td);cko=r(hz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a image classification head) when created
with the `),Gne=n(hz,"CODE",{});var WQr=s(Gne);fko=r(WQr,"from_pretrained()"),WQr.forEach(t),mko=r(hz,"class method or the "),One=n(hz,"CODE",{});var QQr=s(One);gko=r(QQr,"from_config()"),QQr.forEach(t),hko=r(hz,`class
method.`),hz.forEach(t),pko=i(ll),sy=n(ll,"P",{});var WBe=s(sy);_ko=r(WBe,"This class cannot be instantiated directly using "),Xne=n(WBe,"CODE",{});var HQr=s(Xne);uko=r(HQr,"__init__()"),HQr.forEach(t),bko=r(WBe," (throws an error)."),WBe.forEach(t),vko=i(ll),Ur=n(ll,"DIV",{class:!0});var il=s(Ur);m(ly.$$.fragment,il),Tko=i(il),zne=n(il,"P",{});var UQr=s(zne);Fko=r(UQr,"Instantiates one of the model classes of the library (with a image classification head) from a configuration."),UQr.forEach(t),Cko=i(il),Fd=n(il,"P",{});var pz=s(Fd);Mko=r(pz,`Note:
Loading a model from its configuration file does `),Vne=n(pz,"STRONG",{});var JQr=s(Vne);Eko=r(JQr,"not"),JQr.forEach(t),yko=r(pz,` load the model weights. It only affects the
model\u2019s configuration. Use `),Wne=n(pz,"CODE",{});var YQr=s(Wne);wko=r(YQr,"from_pretrained()"),YQr.forEach(t),Ako=r(pz,"to load the model weights."),pz.forEach(t),Lko=i(il),Qne=n(il,"P",{});var KQr=s(Qne);Bko=r(KQr,"Examples:"),KQr.forEach(t),kko=i(il),m(iy.$$.fragment,il),il.forEach(t),xko=i(ll),Ge=n(ll,"DIV",{class:!0});var zt=s(Ge);m(dy.$$.fragment,zt),Rko=i(zt),Hne=n(zt,"P",{});var ZQr=s(Hne);Sko=r(ZQr,"Instantiate one of the model classes of the library (with a image classification head) from a pretrained model."),ZQr.forEach(t),Pko=i(zt),Ja=n(zt,"P",{});var bM=s(Ja);$ko=r(bM,"The model class to instantiate is selected based on the "),Une=n(bM,"CODE",{});var eHr=s(Une);Iko=r(eHr,"model_type"),eHr.forEach(t),jko=r(bM,` property of the config object (either
passed as an argument or loaded from `),Jne=n(bM,"CODE",{});var oHr=s(Jne);Nko=r(oHr,"pretrained_model_name_or_path"),oHr.forEach(t),Dko=r(bM,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Yne=n(bM,"CODE",{});var rHr=s(Yne);qko=r(rHr,"pretrained_model_name_or_path"),rHr.forEach(t),Gko=r(bM,":"),bM.forEach(t),Oko=i(zt),be=n(zt,"UL",{});var Ke=s(be);vb=n(Ke,"LI",{});var Z3e=s(vb);Kne=n(Z3e,"STRONG",{});var tHr=s(Kne);Xko=r(tHr,"beit"),tHr.forEach(t),zko=r(Z3e," \u2014 "),Xj=n(Z3e,"A",{href:!0});var aHr=s(Xj);Vko=r(aHr,"BeitForImageClassification"),aHr.forEach(t),Wko=r(Z3e," (BEiT model)"),Z3e.forEach(t),Qko=i(Ke),Tb=n(Ke,"LI",{});var eye=s(Tb);Zne=n(eye,"STRONG",{});var nHr=s(Zne);Hko=r(nHr,"convnext"),nHr.forEach(t),Uko=r(eye," \u2014 "),zj=n(eye,"A",{href:!0});var sHr=s(zj);Jko=r(sHr,"ConvNextForImageClassification"),sHr.forEach(t),Yko=r(eye," (ConvNext model)"),eye.forEach(t),Kko=i(Ke),Rs=n(Ke,"LI",{});var jL=s(Rs);ese=n(jL,"STRONG",{});var lHr=s(ese);Zko=r(lHr,"deit"),lHr.forEach(t),exo=r(jL," \u2014 "),Vj=n(jL,"A",{href:!0});var iHr=s(Vj);oxo=r(iHr,"DeiTForImageClassification"),iHr.forEach(t),rxo=r(jL," or "),Wj=n(jL,"A",{href:!0});var dHr=s(Wj);txo=r(dHr,"DeiTForImageClassificationWithTeacher"),dHr.forEach(t),axo=r(jL," (DeiT model)"),jL.forEach(t),nxo=i(Ke),Fb=n(Ke,"LI",{});var oye=s(Fb);ose=n(oye,"STRONG",{});var cHr=s(ose);sxo=r(cHr,"imagegpt"),cHr.forEach(t),lxo=r(oye," \u2014 "),Qj=n(oye,"A",{href:!0});var fHr=s(Qj);ixo=r(fHr,"ImageGPTForImageClassification"),fHr.forEach(t),dxo=r(oye," (ImageGPT model)"),oye.forEach(t),cxo=i(Ke),la=n(Ke,"LI",{});var Mf=s(la);rse=n(Mf,"STRONG",{});var mHr=s(rse);fxo=r(mHr,"perceiver"),mHr.forEach(t),mxo=r(Mf," \u2014 "),Hj=n(Mf,"A",{href:!0});var gHr=s(Hj);gxo=r(gHr,"PerceiverForImageClassificationLearned"),gHr.forEach(t),hxo=r(Mf," or "),Uj=n(Mf,"A",{href:!0});var hHr=s(Uj);pxo=r(hHr,"PerceiverForImageClassificationFourier"),hHr.forEach(t),_xo=r(Mf," or "),Jj=n(Mf,"A",{href:!0});var pHr=s(Jj);uxo=r(pHr,"PerceiverForImageClassificationConvProcessing"),pHr.forEach(t),bxo=r(Mf," (Perceiver model)"),Mf.forEach(t),vxo=i(Ke),Cb=n(Ke,"LI",{});var rye=s(Cb);tse=n(rye,"STRONG",{});var _Hr=s(tse);Txo=r(_Hr,"poolformer"),_Hr.forEach(t),Fxo=r(rye," \u2014 "),Yj=n(rye,"A",{href:!0});var uHr=s(Yj);Cxo=r(uHr,"PoolFormerForImageClassification"),uHr.forEach(t),Mxo=r(rye," (PoolFormer model)"),rye.forEach(t),Exo=i(Ke),Mb=n(Ke,"LI",{});var tye=s(Mb);ase=n(tye,"STRONG",{});var bHr=s(ase);yxo=r(bHr,"segformer"),bHr.forEach(t),wxo=r(tye," \u2014 "),Kj=n(tye,"A",{href:!0});var vHr=s(Kj);Axo=r(vHr,"SegformerForImageClassification"),vHr.forEach(t),Lxo=r(tye," (SegFormer model)"),tye.forEach(t),Bxo=i(Ke),Eb=n(Ke,"LI",{});var aye=s(Eb);nse=n(aye,"STRONG",{});var THr=s(nse);kxo=r(THr,"swin"),THr.forEach(t),xxo=r(aye," \u2014 "),Zj=n(aye,"A",{href:!0});var FHr=s(Zj);Rxo=r(FHr,"SwinForImageClassification"),FHr.forEach(t),Sxo=r(aye," (Swin model)"),aye.forEach(t),Pxo=i(Ke),yb=n(Ke,"LI",{});var nye=s(yb);sse=n(nye,"STRONG",{});var CHr=s(sse);$xo=r(CHr,"vit"),CHr.forEach(t),Ixo=r(nye," \u2014 "),eN=n(nye,"A",{href:!0});var MHr=s(eN);jxo=r(MHr,"ViTForImageClassification"),MHr.forEach(t),Nxo=r(nye," (ViT model)"),nye.forEach(t),Ke.forEach(t),Dxo=i(zt),wb=n(zt,"P",{});var sye=s(wb);qxo=r(sye,"The model is set in evaluation mode by default using "),lse=n(sye,"CODE",{});var EHr=s(lse);Gxo=r(EHr,"model.eval()"),EHr.forEach(t),Oxo=r(sye,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),ise=n(sye,"CODE",{});var yHr=s(ise);Xxo=r(yHr,"model.train()"),yHr.forEach(t),sye.forEach(t),zxo=i(zt),dse=n(zt,"P",{});var wHr=s(dse);Vxo=r(wHr,"Examples:"),wHr.forEach(t),Wxo=i(zt),m(cy.$$.fragment,zt),zt.forEach(t),ll.forEach(t),q8e=i(d),Cd=n(d,"H2",{class:!0});var QBe=s(Cd);Ab=n(QBe,"A",{id:!0,class:!0,href:!0});var AHr=s(Ab);cse=n(AHr,"SPAN",{});var LHr=s(cse);m(fy.$$.fragment,LHr),LHr.forEach(t),AHr.forEach(t),Qxo=i(QBe),fse=n(QBe,"SPAN",{});var BHr=s(fse);Hxo=r(BHr,"AutoModelForVision2Seq"),BHr.forEach(t),QBe.forEach(t),G8e=i(d),tr=n(d,"DIV",{class:!0});var dl=s(tr);m(my.$$.fragment,dl),Uxo=i(dl),Md=n(dl,"P",{});var _z=s(Md);Jxo=r(_z,`This is a generic model class that will be instantiated as one of the model classes of the library (with a vision-to-text modeling head) when created
with the `),mse=n(_z,"CODE",{});var kHr=s(mse);Yxo=r(kHr,"from_pretrained()"),kHr.forEach(t),Kxo=r(_z,"class method or the "),gse=n(_z,"CODE",{});var xHr=s(gse);Zxo=r(xHr,"from_config()"),xHr.forEach(t),eRo=r(_z,`class
method.`),_z.forEach(t),oRo=i(dl),gy=n(dl,"P",{});var HBe=s(gy);rRo=r(HBe,"This class cannot be instantiated directly using "),hse=n(HBe,"CODE",{});var RHr=s(hse);tRo=r(RHr,"__init__()"),RHr.forEach(t),aRo=r(HBe," (throws an error)."),HBe.forEach(t),nRo=i(dl),Jr=n(dl,"DIV",{class:!0});var cl=s(Jr);m(hy.$$.fragment,cl),sRo=i(cl),pse=n(cl,"P",{});var SHr=s(pse);lRo=r(SHr,"Instantiates one of the model classes of the library (with a vision-to-text modeling head) from a configuration."),SHr.forEach(t),iRo=i(cl),Ed=n(cl,"P",{});var uz=s(Ed);dRo=r(uz,`Note:
Loading a model from its configuration file does `),_se=n(uz,"STRONG",{});var PHr=s(_se);cRo=r(PHr,"not"),PHr.forEach(t),fRo=r(uz,` load the model weights. It only affects the
model\u2019s configuration. Use `),use=n(uz,"CODE",{});var $Hr=s(use);mRo=r($Hr,"from_pretrained()"),$Hr.forEach(t),gRo=r(uz,"to load the model weights."),uz.forEach(t),hRo=i(cl),bse=n(cl,"P",{});var IHr=s(bse);pRo=r(IHr,"Examples:"),IHr.forEach(t),_Ro=i(cl),m(py.$$.fragment,cl),cl.forEach(t),uRo=i(dl),Oe=n(dl,"DIV",{class:!0});var Vt=s(Oe);m(_y.$$.fragment,Vt),bRo=i(Vt),vse=n(Vt,"P",{});var jHr=s(vse);vRo=r(jHr,"Instantiate one of the model classes of the library (with a vision-to-text modeling head) from a pretrained model."),jHr.forEach(t),TRo=i(Vt),Ya=n(Vt,"P",{});var vM=s(Ya);FRo=r(vM,"The model class to instantiate is selected based on the "),Tse=n(vM,"CODE",{});var NHr=s(Tse);CRo=r(NHr,"model_type"),NHr.forEach(t),MRo=r(vM,` property of the config object (either
passed as an argument or loaded from `),Fse=n(vM,"CODE",{});var DHr=s(Fse);ERo=r(DHr,"pretrained_model_name_or_path"),DHr.forEach(t),yRo=r(vM,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Cse=n(vM,"CODE",{});var qHr=s(Cse);wRo=r(qHr,"pretrained_model_name_or_path"),qHr.forEach(t),ARo=r(vM,":"),vM.forEach(t),LRo=i(Vt),Mse=n(Vt,"UL",{});var GHr=s(Mse);Lb=n(GHr,"LI",{});var lye=s(Lb);Ese=n(lye,"STRONG",{});var OHr=s(Ese);BRo=r(OHr,"vision-encoder-decoder"),OHr.forEach(t),kRo=r(lye," \u2014 "),oN=n(lye,"A",{href:!0});var XHr=s(oN);xRo=r(XHr,"VisionEncoderDecoderModel"),XHr.forEach(t),RRo=r(lye," (Vision Encoder decoder model)"),lye.forEach(t),GHr.forEach(t),SRo=i(Vt),Bb=n(Vt,"P",{});var iye=s(Bb);PRo=r(iye,"The model is set in evaluation mode by default using "),yse=n(iye,"CODE",{});var zHr=s(yse);$Ro=r(zHr,"model.eval()"),zHr.forEach(t),IRo=r(iye,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),wse=n(iye,"CODE",{});var VHr=s(wse);jRo=r(VHr,"model.train()"),VHr.forEach(t),iye.forEach(t),NRo=i(Vt),Ase=n(Vt,"P",{});var WHr=s(Ase);DRo=r(WHr,"Examples:"),WHr.forEach(t),qRo=i(Vt),m(uy.$$.fragment,Vt),Vt.forEach(t),dl.forEach(t),O8e=i(d),yd=n(d,"H2",{class:!0});var UBe=s(yd);kb=n(UBe,"A",{id:!0,class:!0,href:!0});var QHr=s(kb);Lse=n(QHr,"SPAN",{});var HHr=s(Lse);m(by.$$.fragment,HHr),HHr.forEach(t),QHr.forEach(t),GRo=i(UBe),Bse=n(UBe,"SPAN",{});var UHr=s(Bse);ORo=r(UHr,"AutoModelForAudioClassification"),UHr.forEach(t),UBe.forEach(t),X8e=i(d),ar=n(d,"DIV",{class:!0});var fl=s(ar);m(vy.$$.fragment,fl),XRo=i(fl),wd=n(fl,"P",{});var bz=s(wd);zRo=r(bz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a audio classification head) when created
with the `),kse=n(bz,"CODE",{});var JHr=s(kse);VRo=r(JHr,"from_pretrained()"),JHr.forEach(t),WRo=r(bz,"class method or the "),xse=n(bz,"CODE",{});var YHr=s(xse);QRo=r(YHr,"from_config()"),YHr.forEach(t),HRo=r(bz,`class
method.`),bz.forEach(t),URo=i(fl),Ty=n(fl,"P",{});var JBe=s(Ty);JRo=r(JBe,"This class cannot be instantiated directly using "),Rse=n(JBe,"CODE",{});var KHr=s(Rse);YRo=r(KHr,"__init__()"),KHr.forEach(t),KRo=r(JBe," (throws an error)."),JBe.forEach(t),ZRo=i(fl),Yr=n(fl,"DIV",{class:!0});var ml=s(Yr);m(Fy.$$.fragment,ml),eSo=i(ml),Sse=n(ml,"P",{});var ZHr=s(Sse);oSo=r(ZHr,"Instantiates one of the model classes of the library (with a audio classification head) from a configuration."),ZHr.forEach(t),rSo=i(ml),Ad=n(ml,"P",{});var vz=s(Ad);tSo=r(vz,`Note:
Loading a model from its configuration file does `),Pse=n(vz,"STRONG",{});var eUr=s(Pse);aSo=r(eUr,"not"),eUr.forEach(t),nSo=r(vz,` load the model weights. It only affects the
model\u2019s configuration. Use `),$se=n(vz,"CODE",{});var oUr=s($se);sSo=r(oUr,"from_pretrained()"),oUr.forEach(t),lSo=r(vz,"to load the model weights."),vz.forEach(t),iSo=i(ml),Ise=n(ml,"P",{});var rUr=s(Ise);dSo=r(rUr,"Examples:"),rUr.forEach(t),cSo=i(ml),m(Cy.$$.fragment,ml),ml.forEach(t),fSo=i(fl),Xe=n(fl,"DIV",{class:!0});var Wt=s(Xe);m(My.$$.fragment,Wt),mSo=i(Wt),jse=n(Wt,"P",{});var tUr=s(jse);gSo=r(tUr,"Instantiate one of the model classes of the library (with a audio classification head) from a pretrained model."),tUr.forEach(t),hSo=i(Wt),Ka=n(Wt,"P",{});var TM=s(Ka);pSo=r(TM,"The model class to instantiate is selected based on the "),Nse=n(TM,"CODE",{});var aUr=s(Nse);_So=r(aUr,"model_type"),aUr.forEach(t),uSo=r(TM,` property of the config object (either
passed as an argument or loaded from `),Dse=n(TM,"CODE",{});var nUr=s(Dse);bSo=r(nUr,"pretrained_model_name_or_path"),nUr.forEach(t),vSo=r(TM,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),qse=n(TM,"CODE",{});var sUr=s(qse);TSo=r(sUr,"pretrained_model_name_or_path"),sUr.forEach(t),FSo=r(TM,":"),TM.forEach(t),CSo=i(Wt),ao=n(Wt,"UL",{});var Qt=s(ao);xb=n(Qt,"LI",{});var dye=s(xb);Gse=n(dye,"STRONG",{});var lUr=s(Gse);MSo=r(lUr,"hubert"),lUr.forEach(t),ESo=r(dye," \u2014 "),rN=n(dye,"A",{href:!0});var iUr=s(rN);ySo=r(iUr,"HubertForSequenceClassification"),iUr.forEach(t),wSo=r(dye," (Hubert model)"),dye.forEach(t),ASo=i(Qt),Rb=n(Qt,"LI",{});var cye=s(Rb);Ose=n(cye,"STRONG",{});var dUr=s(Ose);LSo=r(dUr,"sew"),dUr.forEach(t),BSo=r(cye," \u2014 "),tN=n(cye,"A",{href:!0});var cUr=s(tN);kSo=r(cUr,"SEWForSequenceClassification"),cUr.forEach(t),xSo=r(cye," (SEW model)"),cye.forEach(t),RSo=i(Qt),Sb=n(Qt,"LI",{});var fye=s(Sb);Xse=n(fye,"STRONG",{});var fUr=s(Xse);SSo=r(fUr,"sew-d"),fUr.forEach(t),PSo=r(fye," \u2014 "),aN=n(fye,"A",{href:!0});var mUr=s(aN);$So=r(mUr,"SEWDForSequenceClassification"),mUr.forEach(t),ISo=r(fye," (SEW-D model)"),fye.forEach(t),jSo=i(Qt),Pb=n(Qt,"LI",{});var mye=s(Pb);zse=n(mye,"STRONG",{});var gUr=s(zse);NSo=r(gUr,"unispeech"),gUr.forEach(t),DSo=r(mye," \u2014 "),nN=n(mye,"A",{href:!0});var hUr=s(nN);qSo=r(hUr,"UniSpeechForSequenceClassification"),hUr.forEach(t),GSo=r(mye," (UniSpeech model)"),mye.forEach(t),OSo=i(Qt),$b=n(Qt,"LI",{});var gye=s($b);Vse=n(gye,"STRONG",{});var pUr=s(Vse);XSo=r(pUr,"unispeech-sat"),pUr.forEach(t),zSo=r(gye," \u2014 "),sN=n(gye,"A",{href:!0});var _Ur=s(sN);VSo=r(_Ur,"UniSpeechSatForSequenceClassification"),_Ur.forEach(t),WSo=r(gye," (UniSpeechSat model)"),gye.forEach(t),QSo=i(Qt),Ib=n(Qt,"LI",{});var hye=s(Ib);Wse=n(hye,"STRONG",{});var uUr=s(Wse);HSo=r(uUr,"wav2vec2"),uUr.forEach(t),USo=r(hye," \u2014 "),lN=n(hye,"A",{href:!0});var bUr=s(lN);JSo=r(bUr,"Wav2Vec2ForSequenceClassification"),bUr.forEach(t),YSo=r(hye," (Wav2Vec2 model)"),hye.forEach(t),KSo=i(Qt),jb=n(Qt,"LI",{});var pye=s(jb);Qse=n(pye,"STRONG",{});var vUr=s(Qse);ZSo=r(vUr,"wavlm"),vUr.forEach(t),ePo=r(pye," \u2014 "),iN=n(pye,"A",{href:!0});var TUr=s(iN);oPo=r(TUr,"WavLMForSequenceClassification"),TUr.forEach(t),rPo=r(pye," (WavLM model)"),pye.forEach(t),Qt.forEach(t),tPo=i(Wt),Nb=n(Wt,"P",{});var _ye=s(Nb);aPo=r(_ye,"The model is set in evaluation mode by default using "),Hse=n(_ye,"CODE",{});var FUr=s(Hse);nPo=r(FUr,"model.eval()"),FUr.forEach(t),sPo=r(_ye,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Use=n(_ye,"CODE",{});var CUr=s(Use);lPo=r(CUr,"model.train()"),CUr.forEach(t),_ye.forEach(t),iPo=i(Wt),Jse=n(Wt,"P",{});var MUr=s(Jse);dPo=r(MUr,"Examples:"),MUr.forEach(t),cPo=i(Wt),m(Ey.$$.fragment,Wt),Wt.forEach(t),fl.forEach(t),z8e=i(d),Ld=n(d,"H2",{class:!0});var YBe=s(Ld);Db=n(YBe,"A",{id:!0,class:!0,href:!0});var EUr=s(Db);Yse=n(EUr,"SPAN",{});var yUr=s(Yse);m(yy.$$.fragment,yUr),yUr.forEach(t),EUr.forEach(t),fPo=i(YBe),Kse=n(YBe,"SPAN",{});var wUr=s(Kse);mPo=r(wUr,"AutoModelForAudioFrameClassification"),wUr.forEach(t),YBe.forEach(t),V8e=i(d),nr=n(d,"DIV",{class:!0});var gl=s(nr);m(wy.$$.fragment,gl),gPo=i(gl),Bd=n(gl,"P",{});var Tz=s(Bd);hPo=r(Tz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a audio frame (token) classification head) when created
with the `),Zse=n(Tz,"CODE",{});var AUr=s(Zse);pPo=r(AUr,"from_pretrained()"),AUr.forEach(t),_Po=r(Tz,"class method or the "),ele=n(Tz,"CODE",{});var LUr=s(ele);uPo=r(LUr,"from_config()"),LUr.forEach(t),bPo=r(Tz,`class
method.`),Tz.forEach(t),vPo=i(gl),Ay=n(gl,"P",{});var KBe=s(Ay);TPo=r(KBe,"This class cannot be instantiated directly using "),ole=n(KBe,"CODE",{});var BUr=s(ole);FPo=r(BUr,"__init__()"),BUr.forEach(t),CPo=r(KBe," (throws an error)."),KBe.forEach(t),MPo=i(gl),Kr=n(gl,"DIV",{class:!0});var hl=s(Kr);m(Ly.$$.fragment,hl),EPo=i(hl),rle=n(hl,"P",{});var kUr=s(rle);yPo=r(kUr,"Instantiates one of the model classes of the library (with a audio frame (token) classification head) from a configuration."),kUr.forEach(t),wPo=i(hl),kd=n(hl,"P",{});var Fz=s(kd);APo=r(Fz,`Note:
Loading a model from its configuration file does `),tle=n(Fz,"STRONG",{});var xUr=s(tle);LPo=r(xUr,"not"),xUr.forEach(t),BPo=r(Fz,` load the model weights. It only affects the
model\u2019s configuration. Use `),ale=n(Fz,"CODE",{});var RUr=s(ale);kPo=r(RUr,"from_pretrained()"),RUr.forEach(t),xPo=r(Fz,"to load the model weights."),Fz.forEach(t),RPo=i(hl),nle=n(hl,"P",{});var SUr=s(nle);SPo=r(SUr,"Examples:"),SUr.forEach(t),PPo=i(hl),m(By.$$.fragment,hl),hl.forEach(t),$Po=i(gl),ze=n(gl,"DIV",{class:!0});var Ht=s(ze);m(ky.$$.fragment,Ht),IPo=i(Ht),sle=n(Ht,"P",{});var PUr=s(sle);jPo=r(PUr,"Instantiate one of the model classes of the library (with a audio frame (token) classification head) from a pretrained model."),PUr.forEach(t),NPo=i(Ht),Za=n(Ht,"P",{});var FM=s(Za);DPo=r(FM,"The model class to instantiate is selected based on the "),lle=n(FM,"CODE",{});var $Ur=s(lle);qPo=r($Ur,"model_type"),$Ur.forEach(t),GPo=r(FM,` property of the config object (either
passed as an argument or loaded from `),ile=n(FM,"CODE",{});var IUr=s(ile);OPo=r(IUr,"pretrained_model_name_or_path"),IUr.forEach(t),XPo=r(FM,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),dle=n(FM,"CODE",{});var jUr=s(dle);zPo=r(jUr,"pretrained_model_name_or_path"),jUr.forEach(t),VPo=r(FM,":"),FM.forEach(t),WPo=i(Ht),xd=n(Ht,"UL",{});var Cz=s(xd);qb=n(Cz,"LI",{});var uye=s(qb);cle=n(uye,"STRONG",{});var NUr=s(cle);QPo=r(NUr,"unispeech-sat"),NUr.forEach(t),HPo=r(uye," \u2014 "),dN=n(uye,"A",{href:!0});var DUr=s(dN);UPo=r(DUr,"UniSpeechSatForAudioFrameClassification"),DUr.forEach(t),JPo=r(uye," (UniSpeechSat model)"),uye.forEach(t),YPo=i(Cz),Gb=n(Cz,"LI",{});var bye=s(Gb);fle=n(bye,"STRONG",{});var qUr=s(fle);KPo=r(qUr,"wav2vec2"),qUr.forEach(t),ZPo=r(bye," \u2014 "),cN=n(bye,"A",{href:!0});var GUr=s(cN);e$o=r(GUr,"Wav2Vec2ForAudioFrameClassification"),GUr.forEach(t),o$o=r(bye," (Wav2Vec2 model)"),bye.forEach(t),r$o=i(Cz),Ob=n(Cz,"LI",{});var vye=s(Ob);mle=n(vye,"STRONG",{});var OUr=s(mle);t$o=r(OUr,"wavlm"),OUr.forEach(t),a$o=r(vye," \u2014 "),fN=n(vye,"A",{href:!0});var XUr=s(fN);n$o=r(XUr,"WavLMForAudioFrameClassification"),XUr.forEach(t),s$o=r(vye," (WavLM model)"),vye.forEach(t),Cz.forEach(t),l$o=i(Ht),Xb=n(Ht,"P",{});var Tye=s(Xb);i$o=r(Tye,"The model is set in evaluation mode by default using "),gle=n(Tye,"CODE",{});var zUr=s(gle);d$o=r(zUr,"model.eval()"),zUr.forEach(t),c$o=r(Tye,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),hle=n(Tye,"CODE",{});var VUr=s(hle);f$o=r(VUr,"model.train()"),VUr.forEach(t),Tye.forEach(t),m$o=i(Ht),ple=n(Ht,"P",{});var WUr=s(ple);g$o=r(WUr,"Examples:"),WUr.forEach(t),h$o=i(Ht),m(xy.$$.fragment,Ht),Ht.forEach(t),gl.forEach(t),W8e=i(d),Rd=n(d,"H2",{class:!0});var ZBe=s(Rd);zb=n(ZBe,"A",{id:!0,class:!0,href:!0});var QUr=s(zb);_le=n(QUr,"SPAN",{});var HUr=s(_le);m(Ry.$$.fragment,HUr),HUr.forEach(t),QUr.forEach(t),p$o=i(ZBe),ule=n(ZBe,"SPAN",{});var UUr=s(ule);_$o=r(UUr,"AutoModelForCTC"),UUr.forEach(t),ZBe.forEach(t),Q8e=i(d),sr=n(d,"DIV",{class:!0});var pl=s(sr);m(Sy.$$.fragment,pl),u$o=i(pl),Sd=n(pl,"P",{});var Mz=s(Sd);b$o=r(Mz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a connectionist temporal classification head) when created
with the `),ble=n(Mz,"CODE",{});var JUr=s(ble);v$o=r(JUr,"from_pretrained()"),JUr.forEach(t),T$o=r(Mz,"class method or the "),vle=n(Mz,"CODE",{});var YUr=s(vle);F$o=r(YUr,"from_config()"),YUr.forEach(t),C$o=r(Mz,`class
method.`),Mz.forEach(t),M$o=i(pl),Py=n(pl,"P",{});var eke=s(Py);E$o=r(eke,"This class cannot be instantiated directly using "),Tle=n(eke,"CODE",{});var KUr=s(Tle);y$o=r(KUr,"__init__()"),KUr.forEach(t),w$o=r(eke," (throws an error)."),eke.forEach(t),A$o=i(pl),Zr=n(pl,"DIV",{class:!0});var _l=s(Zr);m($y.$$.fragment,_l),L$o=i(_l),Fle=n(_l,"P",{});var ZUr=s(Fle);B$o=r(ZUr,"Instantiates one of the model classes of the library (with a connectionist temporal classification head) from a configuration."),ZUr.forEach(t),k$o=i(_l),Pd=n(_l,"P",{});var Ez=s(Pd);x$o=r(Ez,`Note:
Loading a model from its configuration file does `),Cle=n(Ez,"STRONG",{});var eJr=s(Cle);R$o=r(eJr,"not"),eJr.forEach(t),S$o=r(Ez,` load the model weights. It only affects the
model\u2019s configuration. Use `),Mle=n(Ez,"CODE",{});var oJr=s(Mle);P$o=r(oJr,"from_pretrained()"),oJr.forEach(t),$$o=r(Ez,"to load the model weights."),Ez.forEach(t),I$o=i(_l),Ele=n(_l,"P",{});var rJr=s(Ele);j$o=r(rJr,"Examples:"),rJr.forEach(t),N$o=i(_l),m(Iy.$$.fragment,_l),_l.forEach(t),D$o=i(pl),Ve=n(pl,"DIV",{class:!0});var Ut=s(Ve);m(jy.$$.fragment,Ut),q$o=i(Ut),yle=n(Ut,"P",{});var tJr=s(yle);G$o=r(tJr,"Instantiate one of the model classes of the library (with a connectionist temporal classification head) from a pretrained model."),tJr.forEach(t),O$o=i(Ut),en=n(Ut,"P",{});var CM=s(en);X$o=r(CM,"The model class to instantiate is selected based on the "),wle=n(CM,"CODE",{});var aJr=s(wle);z$o=r(aJr,"model_type"),aJr.forEach(t),V$o=r(CM,` property of the config object (either
passed as an argument or loaded from `),Ale=n(CM,"CODE",{});var nJr=s(Ale);W$o=r(nJr,"pretrained_model_name_or_path"),nJr.forEach(t),Q$o=r(CM,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Lle=n(CM,"CODE",{});var sJr=s(Lle);H$o=r(sJr,"pretrained_model_name_or_path"),sJr.forEach(t),U$o=r(CM,":"),CM.forEach(t),J$o=i(Ut),no=n(Ut,"UL",{});var Jt=s(no);Vb=n(Jt,"LI",{});var Fye=s(Vb);Ble=n(Fye,"STRONG",{});var lJr=s(Ble);Y$o=r(lJr,"hubert"),lJr.forEach(t),K$o=r(Fye," \u2014 "),mN=n(Fye,"A",{href:!0});var iJr=s(mN);Z$o=r(iJr,"HubertForCTC"),iJr.forEach(t),eIo=r(Fye," (Hubert model)"),Fye.forEach(t),oIo=i(Jt),Wb=n(Jt,"LI",{});var Cye=s(Wb);kle=n(Cye,"STRONG",{});var dJr=s(kle);rIo=r(dJr,"sew"),dJr.forEach(t),tIo=r(Cye," \u2014 "),gN=n(Cye,"A",{href:!0});var cJr=s(gN);aIo=r(cJr,"SEWForCTC"),cJr.forEach(t),nIo=r(Cye," (SEW model)"),Cye.forEach(t),sIo=i(Jt),Qb=n(Jt,"LI",{});var Mye=s(Qb);xle=n(Mye,"STRONG",{});var fJr=s(xle);lIo=r(fJr,"sew-d"),fJr.forEach(t),iIo=r(Mye," \u2014 "),hN=n(Mye,"A",{href:!0});var mJr=s(hN);dIo=r(mJr,"SEWDForCTC"),mJr.forEach(t),cIo=r(Mye," (SEW-D model)"),Mye.forEach(t),fIo=i(Jt),Hb=n(Jt,"LI",{});var Eye=s(Hb);Rle=n(Eye,"STRONG",{});var gJr=s(Rle);mIo=r(gJr,"unispeech"),gJr.forEach(t),gIo=r(Eye," \u2014 "),pN=n(Eye,"A",{href:!0});var hJr=s(pN);hIo=r(hJr,"UniSpeechForCTC"),hJr.forEach(t),pIo=r(Eye," (UniSpeech model)"),Eye.forEach(t),_Io=i(Jt),Ub=n(Jt,"LI",{});var yye=s(Ub);Sle=n(yye,"STRONG",{});var pJr=s(Sle);uIo=r(pJr,"unispeech-sat"),pJr.forEach(t),bIo=r(yye," \u2014 "),_N=n(yye,"A",{href:!0});var _Jr=s(_N);vIo=r(_Jr,"UniSpeechSatForCTC"),_Jr.forEach(t),TIo=r(yye," (UniSpeechSat model)"),yye.forEach(t),FIo=i(Jt),Jb=n(Jt,"LI",{});var wye=s(Jb);Ple=n(wye,"STRONG",{});var uJr=s(Ple);CIo=r(uJr,"wav2vec2"),uJr.forEach(t),MIo=r(wye," \u2014 "),uN=n(wye,"A",{href:!0});var bJr=s(uN);EIo=r(bJr,"Wav2Vec2ForCTC"),bJr.forEach(t),yIo=r(wye," (Wav2Vec2 model)"),wye.forEach(t),wIo=i(Jt),Yb=n(Jt,"LI",{});var Aye=s(Yb);$le=n(Aye,"STRONG",{});var vJr=s($le);AIo=r(vJr,"wavlm"),vJr.forEach(t),LIo=r(Aye," \u2014 "),bN=n(Aye,"A",{href:!0});var TJr=s(bN);BIo=r(TJr,"WavLMForCTC"),TJr.forEach(t),kIo=r(Aye," (WavLM model)"),Aye.forEach(t),Jt.forEach(t),xIo=i(Ut),Kb=n(Ut,"P",{});var Lye=s(Kb);RIo=r(Lye,"The model is set in evaluation mode by default using "),Ile=n(Lye,"CODE",{});var FJr=s(Ile);SIo=r(FJr,"model.eval()"),FJr.forEach(t),PIo=r(Lye,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),jle=n(Lye,"CODE",{});var CJr=s(jle);$Io=r(CJr,"model.train()"),CJr.forEach(t),Lye.forEach(t),IIo=i(Ut),Nle=n(Ut,"P",{});var MJr=s(Nle);jIo=r(MJr,"Examples:"),MJr.forEach(t),NIo=i(Ut),m(Ny.$$.fragment,Ut),Ut.forEach(t),pl.forEach(t),H8e=i(d),$d=n(d,"H2",{class:!0});var oke=s($d);Zb=n(oke,"A",{id:!0,class:!0,href:!0});var EJr=s(Zb);Dle=n(EJr,"SPAN",{});var yJr=s(Dle);m(Dy.$$.fragment,yJr),yJr.forEach(t),EJr.forEach(t),DIo=i(oke),qle=n(oke,"SPAN",{});var wJr=s(qle);qIo=r(wJr,"AutoModelForSpeechSeq2Seq"),wJr.forEach(t),oke.forEach(t),U8e=i(d),lr=n(d,"DIV",{class:!0});var ul=s(lr);m(qy.$$.fragment,ul),GIo=i(ul),Id=n(ul,"P",{});var yz=s(Id);OIo=r(yz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) when created
with the `),Gle=n(yz,"CODE",{});var AJr=s(Gle);XIo=r(AJr,"from_pretrained()"),AJr.forEach(t),zIo=r(yz,"class method or the "),Ole=n(yz,"CODE",{});var LJr=s(Ole);VIo=r(LJr,"from_config()"),LJr.forEach(t),WIo=r(yz,`class
method.`),yz.forEach(t),QIo=i(ul),Gy=n(ul,"P",{});var rke=s(Gy);HIo=r(rke,"This class cannot be instantiated directly using "),Xle=n(rke,"CODE",{});var BJr=s(Xle);UIo=r(BJr,"__init__()"),BJr.forEach(t),JIo=r(rke," (throws an error)."),rke.forEach(t),YIo=i(ul),et=n(ul,"DIV",{class:!0});var bl=s(et);m(Oy.$$.fragment,bl),KIo=i(bl),zle=n(bl,"P",{});var kJr=s(zle);ZIo=r(kJr,"Instantiates one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) from a configuration."),kJr.forEach(t),ejo=i(bl),jd=n(bl,"P",{});var wz=s(jd);ojo=r(wz,`Note:
Loading a model from its configuration file does `),Vle=n(wz,"STRONG",{});var xJr=s(Vle);rjo=r(xJr,"not"),xJr.forEach(t),tjo=r(wz,` load the model weights. It only affects the
model\u2019s configuration. Use `),Wle=n(wz,"CODE",{});var RJr=s(Wle);ajo=r(RJr,"from_pretrained()"),RJr.forEach(t),njo=r(wz,"to load the model weights."),wz.forEach(t),sjo=i(bl),Qle=n(bl,"P",{});var SJr=s(Qle);ljo=r(SJr,"Examples:"),SJr.forEach(t),ijo=i(bl),m(Xy.$$.fragment,bl),bl.forEach(t),djo=i(ul),We=n(ul,"DIV",{class:!0});var Yt=s(We);m(zy.$$.fragment,Yt),cjo=i(Yt),Hle=n(Yt,"P",{});var PJr=s(Hle);fjo=r(PJr,"Instantiate one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) from a pretrained model."),PJr.forEach(t),mjo=i(Yt),on=n(Yt,"P",{});var MM=s(on);gjo=r(MM,"The model class to instantiate is selected based on the "),Ule=n(MM,"CODE",{});var $Jr=s(Ule);hjo=r($Jr,"model_type"),$Jr.forEach(t),pjo=r(MM,` property of the config object (either
passed as an argument or loaded from `),Jle=n(MM,"CODE",{});var IJr=s(Jle);_jo=r(IJr,"pretrained_model_name_or_path"),IJr.forEach(t),ujo=r(MM,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Yle=n(MM,"CODE",{});var jJr=s(Yle);bjo=r(jJr,"pretrained_model_name_or_path"),jJr.forEach(t),vjo=r(MM,":"),MM.forEach(t),Tjo=i(Yt),Vy=n(Yt,"UL",{});var tke=s(Vy);e5=n(tke,"LI",{});var Bye=s(e5);Kle=n(Bye,"STRONG",{});var NJr=s(Kle);Fjo=r(NJr,"speech-encoder-decoder"),NJr.forEach(t),Cjo=r(Bye," \u2014 "),vN=n(Bye,"A",{href:!0});var DJr=s(vN);Mjo=r(DJr,"SpeechEncoderDecoderModel"),DJr.forEach(t),Ejo=r(Bye," (Speech Encoder decoder model)"),Bye.forEach(t),yjo=i(tke),o5=n(tke,"LI",{});var kye=s(o5);Zle=n(kye,"STRONG",{});var qJr=s(Zle);wjo=r(qJr,"speech_to_text"),qJr.forEach(t),Ajo=r(kye," \u2014 "),TN=n(kye,"A",{href:!0});var GJr=s(TN);Ljo=r(GJr,"Speech2TextForConditionalGeneration"),GJr.forEach(t),Bjo=r(kye," (Speech2Text model)"),kye.forEach(t),tke.forEach(t),kjo=i(Yt),r5=n(Yt,"P",{});var xye=s(r5);xjo=r(xye,"The model is set in evaluation mode by default using "),eie=n(xye,"CODE",{});var OJr=s(eie);Rjo=r(OJr,"model.eval()"),OJr.forEach(t),Sjo=r(xye,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),oie=n(xye,"CODE",{});var XJr=s(oie);Pjo=r(XJr,"model.train()"),XJr.forEach(t),xye.forEach(t),$jo=i(Yt),rie=n(Yt,"P",{});var zJr=s(rie);Ijo=r(zJr,"Examples:"),zJr.forEach(t),jjo=i(Yt),m(Wy.$$.fragment,Yt),Yt.forEach(t),ul.forEach(t),J8e=i(d),Nd=n(d,"H2",{class:!0});var ake=s(Nd);t5=n(ake,"A",{id:!0,class:!0,href:!0});var VJr=s(t5);tie=n(VJr,"SPAN",{});var WJr=s(tie);m(Qy.$$.fragment,WJr),WJr.forEach(t),VJr.forEach(t),Njo=i(ake),aie=n(ake,"SPAN",{});var QJr=s(aie);Djo=r(QJr,"AutoModelForAudioXVector"),QJr.forEach(t),ake.forEach(t),Y8e=i(d),ir=n(d,"DIV",{class:!0});var vl=s(ir);m(Hy.$$.fragment,vl),qjo=i(vl),Dd=n(vl,"P",{});var Az=s(Dd);Gjo=r(Az,`This is a generic model class that will be instantiated as one of the model classes of the library (with a audio retrieval via x-vector head) when created
with the `),nie=n(Az,"CODE",{});var HJr=s(nie);Ojo=r(HJr,"from_pretrained()"),HJr.forEach(t),Xjo=r(Az,"class method or the "),sie=n(Az,"CODE",{});var UJr=s(sie);zjo=r(UJr,"from_config()"),UJr.forEach(t),Vjo=r(Az,`class
method.`),Az.forEach(t),Wjo=i(vl),Uy=n(vl,"P",{});var nke=s(Uy);Qjo=r(nke,"This class cannot be instantiated directly using "),lie=n(nke,"CODE",{});var JJr=s(lie);Hjo=r(JJr,"__init__()"),JJr.forEach(t),Ujo=r(nke," (throws an error)."),nke.forEach(t),Jjo=i(vl),ot=n(vl,"DIV",{class:!0});var Tl=s(ot);m(Jy.$$.fragment,Tl),Yjo=i(Tl),iie=n(Tl,"P",{});var YJr=s(iie);Kjo=r(YJr,"Instantiates one of the model classes of the library (with a audio retrieval via x-vector head) from a configuration."),YJr.forEach(t),Zjo=i(Tl),qd=n(Tl,"P",{});var Lz=s(qd);eNo=r(Lz,`Note:
Loading a model from its configuration file does `),die=n(Lz,"STRONG",{});var KJr=s(die);oNo=r(KJr,"not"),KJr.forEach(t),rNo=r(Lz,` load the model weights. It only affects the
model\u2019s configuration. Use `),cie=n(Lz,"CODE",{});var ZJr=s(cie);tNo=r(ZJr,"from_pretrained()"),ZJr.forEach(t),aNo=r(Lz,"to load the model weights."),Lz.forEach(t),nNo=i(Tl),fie=n(Tl,"P",{});var eYr=s(fie);sNo=r(eYr,"Examples:"),eYr.forEach(t),lNo=i(Tl),m(Yy.$$.fragment,Tl),Tl.forEach(t),iNo=i(vl),Qe=n(vl,"DIV",{class:!0});var Kt=s(Qe);m(Ky.$$.fragment,Kt),dNo=i(Kt),mie=n(Kt,"P",{});var oYr=s(mie);cNo=r(oYr,"Instantiate one of the model classes of the library (with a audio retrieval via x-vector head) from a pretrained model."),oYr.forEach(t),fNo=i(Kt),rn=n(Kt,"P",{});var EM=s(rn);mNo=r(EM,"The model class to instantiate is selected based on the "),gie=n(EM,"CODE",{});var rYr=s(gie);gNo=r(rYr,"model_type"),rYr.forEach(t),hNo=r(EM,` property of the config object (either
passed as an argument or loaded from `),hie=n(EM,"CODE",{});var tYr=s(hie);pNo=r(tYr,"pretrained_model_name_or_path"),tYr.forEach(t),_No=r(EM,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),pie=n(EM,"CODE",{});var aYr=s(pie);uNo=r(aYr,"pretrained_model_name_or_path"),aYr.forEach(t),bNo=r(EM,":"),EM.forEach(t),vNo=i(Kt),Gd=n(Kt,"UL",{});var Bz=s(Gd);a5=n(Bz,"LI",{});var Rye=s(a5);_ie=n(Rye,"STRONG",{});var nYr=s(_ie);TNo=r(nYr,"unispeech-sat"),nYr.forEach(t),FNo=r(Rye," \u2014 "),FN=n(Rye,"A",{href:!0});var sYr=s(FN);CNo=r(sYr,"UniSpeechSatForXVector"),sYr.forEach(t),MNo=r(Rye," (UniSpeechSat model)"),Rye.forEach(t),ENo=i(Bz),n5=n(Bz,"LI",{});var Sye=s(n5);uie=n(Sye,"STRONG",{});var lYr=s(uie);yNo=r(lYr,"wav2vec2"),lYr.forEach(t),wNo=r(Sye," \u2014 "),CN=n(Sye,"A",{href:!0});var iYr=s(CN);ANo=r(iYr,"Wav2Vec2ForXVector"),iYr.forEach(t),LNo=r(Sye," (Wav2Vec2 model)"),Sye.forEach(t),BNo=i(Bz),s5=n(Bz,"LI",{});var Pye=s(s5);bie=n(Pye,"STRONG",{});var dYr=s(bie);kNo=r(dYr,"wavlm"),dYr.forEach(t),xNo=r(Pye," \u2014 "),MN=n(Pye,"A",{href:!0});var cYr=s(MN);RNo=r(cYr,"WavLMForXVector"),cYr.forEach(t),SNo=r(Pye," (WavLM model)"),Pye.forEach(t),Bz.forEach(t),PNo=i(Kt),l5=n(Kt,"P",{});var $ye=s(l5);$No=r($ye,"The model is set in evaluation mode by default using "),vie=n($ye,"CODE",{});var fYr=s(vie);INo=r(fYr,"model.eval()"),fYr.forEach(t),jNo=r($ye,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Tie=n($ye,"CODE",{});var mYr=s(Tie);NNo=r(mYr,"model.train()"),mYr.forEach(t),$ye.forEach(t),DNo=i(Kt),Fie=n(Kt,"P",{});var gYr=s(Fie);qNo=r(gYr,"Examples:"),gYr.forEach(t),GNo=i(Kt),m(Zy.$$.fragment,Kt),Kt.forEach(t),vl.forEach(t),K8e=i(d),Od=n(d,"H2",{class:!0});var ske=s(Od);i5=n(ske,"A",{id:!0,class:!0,href:!0});var hYr=s(i5);Cie=n(hYr,"SPAN",{});var pYr=s(Cie);m(ew.$$.fragment,pYr),pYr.forEach(t),hYr.forEach(t),ONo=i(ske),Mie=n(ske,"SPAN",{});var _Yr=s(Mie);XNo=r(_Yr,"AutoModelForMaskedImageModeling"),_Yr.forEach(t),ske.forEach(t),Z8e=i(d),dr=n(d,"DIV",{class:!0});var Fl=s(dr);m(ow.$$.fragment,Fl),zNo=i(Fl),Xd=n(Fl,"P",{});var kz=s(Xd);VNo=r(kz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a masked image modeling head) when created
with the `),Eie=n(kz,"CODE",{});var uYr=s(Eie);WNo=r(uYr,"from_pretrained()"),uYr.forEach(t),QNo=r(kz,"class method or the "),yie=n(kz,"CODE",{});var bYr=s(yie);HNo=r(bYr,"from_config()"),bYr.forEach(t),UNo=r(kz,`class
method.`),kz.forEach(t),JNo=i(Fl),rw=n(Fl,"P",{});var lke=s(rw);YNo=r(lke,"This class cannot be instantiated directly using "),wie=n(lke,"CODE",{});var vYr=s(wie);KNo=r(vYr,"__init__()"),vYr.forEach(t),ZNo=r(lke," (throws an error)."),lke.forEach(t),eDo=i(Fl),rt=n(Fl,"DIV",{class:!0});var Cl=s(rt);m(tw.$$.fragment,Cl),oDo=i(Cl),Aie=n(Cl,"P",{});var TYr=s(Aie);rDo=r(TYr,"Instantiates one of the model classes of the library (with a masked image modeling head) from a configuration."),TYr.forEach(t),tDo=i(Cl),zd=n(Cl,"P",{});var xz=s(zd);aDo=r(xz,`Note:
Loading a model from its configuration file does `),Lie=n(xz,"STRONG",{});var FYr=s(Lie);nDo=r(FYr,"not"),FYr.forEach(t),sDo=r(xz,` load the model weights. It only affects the
model\u2019s configuration. Use `),Bie=n(xz,"CODE",{});var CYr=s(Bie);lDo=r(CYr,"from_pretrained()"),CYr.forEach(t),iDo=r(xz,"to load the model weights."),xz.forEach(t),dDo=i(Cl),kie=n(Cl,"P",{});var MYr=s(kie);cDo=r(MYr,"Examples:"),MYr.forEach(t),fDo=i(Cl),m(aw.$$.fragment,Cl),Cl.forEach(t),mDo=i(Fl),He=n(Fl,"DIV",{class:!0});var Zt=s(He);m(nw.$$.fragment,Zt),gDo=i(Zt),xie=n(Zt,"P",{});var EYr=s(xie);hDo=r(EYr,"Instantiate one of the model classes of the library (with a masked image modeling head) from a pretrained model."),EYr.forEach(t),pDo=i(Zt),tn=n(Zt,"P",{});var yM=s(tn);_Do=r(yM,"The model class to instantiate is selected based on the "),Rie=n(yM,"CODE",{});var yYr=s(Rie);uDo=r(yYr,"model_type"),yYr.forEach(t),bDo=r(yM,` property of the config object (either
passed as an argument or loaded from `),Sie=n(yM,"CODE",{});var wYr=s(Sie);vDo=r(wYr,"pretrained_model_name_or_path"),wYr.forEach(t),TDo=r(yM,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Pie=n(yM,"CODE",{});var AYr=s(Pie);FDo=r(AYr,"pretrained_model_name_or_path"),AYr.forEach(t),CDo=r(yM,":"),yM.forEach(t),MDo=i(Zt),Vd=n(Zt,"UL",{});var Rz=s(Vd);d5=n(Rz,"LI",{});var Iye=s(d5);$ie=n(Iye,"STRONG",{});var LYr=s($ie);EDo=r(LYr,"deit"),LYr.forEach(t),yDo=r(Iye," \u2014 "),EN=n(Iye,"A",{href:!0});var BYr=s(EN);wDo=r(BYr,"DeiTForMaskedImageModeling"),BYr.forEach(t),ADo=r(Iye," (DeiT model)"),Iye.forEach(t),LDo=i(Rz),c5=n(Rz,"LI",{});var jye=s(c5);Iie=n(jye,"STRONG",{});var kYr=s(Iie);BDo=r(kYr,"swin"),kYr.forEach(t),kDo=r(jye," \u2014 "),yN=n(jye,"A",{href:!0});var xYr=s(yN);xDo=r(xYr,"SwinForMaskedImageModeling"),xYr.forEach(t),RDo=r(jye," (Swin model)"),jye.forEach(t),SDo=i(Rz),f5=n(Rz,"LI",{});var Nye=s(f5);jie=n(Nye,"STRONG",{});var RYr=s(jie);PDo=r(RYr,"vit"),RYr.forEach(t),$Do=r(Nye," \u2014 "),wN=n(Nye,"A",{href:!0});var SYr=s(wN);IDo=r(SYr,"ViTForMaskedImageModeling"),SYr.forEach(t),jDo=r(Nye," (ViT model)"),Nye.forEach(t),Rz.forEach(t),NDo=i(Zt),m5=n(Zt,"P",{});var Dye=s(m5);DDo=r(Dye,"The model is set in evaluation mode by default using "),Nie=n(Dye,"CODE",{});var PYr=s(Nie);qDo=r(PYr,"model.eval()"),PYr.forEach(t),GDo=r(Dye,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Die=n(Dye,"CODE",{});var $Yr=s(Die);ODo=r($Yr,"model.train()"),$Yr.forEach(t),Dye.forEach(t),XDo=i(Zt),qie=n(Zt,"P",{});var IYr=s(qie);zDo=r(IYr,"Examples:"),IYr.forEach(t),VDo=i(Zt),m(sw.$$.fragment,Zt),Zt.forEach(t),Fl.forEach(t),e9e=i(d),Wd=n(d,"H2",{class:!0});var ike=s(Wd);g5=n(ike,"A",{id:!0,class:!0,href:!0});var jYr=s(g5);Gie=n(jYr,"SPAN",{});var NYr=s(Gie);m(lw.$$.fragment,NYr),NYr.forEach(t),jYr.forEach(t),WDo=i(ike),Oie=n(ike,"SPAN",{});var DYr=s(Oie);QDo=r(DYr,"AutoModelForObjectDetection"),DYr.forEach(t),ike.forEach(t),o9e=i(d),cr=n(d,"DIV",{class:!0});var Ml=s(cr);m(iw.$$.fragment,Ml),HDo=i(Ml),Qd=n(Ml,"P",{});var Sz=s(Qd);UDo=r(Sz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a object detection head) when created
with the `),Xie=n(Sz,"CODE",{});var qYr=s(Xie);JDo=r(qYr,"from_pretrained()"),qYr.forEach(t),YDo=r(Sz,"class method or the "),zie=n(Sz,"CODE",{});var GYr=s(zie);KDo=r(GYr,"from_config()"),GYr.forEach(t),ZDo=r(Sz,`class
method.`),Sz.forEach(t),eqo=i(Ml),dw=n(Ml,"P",{});var dke=s(dw);oqo=r(dke,"This class cannot be instantiated directly using "),Vie=n(dke,"CODE",{});var OYr=s(Vie);rqo=r(OYr,"__init__()"),OYr.forEach(t),tqo=r(dke," (throws an error)."),dke.forEach(t),aqo=i(Ml),tt=n(Ml,"DIV",{class:!0});var El=s(tt);m(cw.$$.fragment,El),nqo=i(El),Wie=n(El,"P",{});var XYr=s(Wie);sqo=r(XYr,"Instantiates one of the model classes of the library (with a object detection head) from a configuration."),XYr.forEach(t),lqo=i(El),Hd=n(El,"P",{});var Pz=s(Hd);iqo=r(Pz,`Note:
Loading a model from its configuration file does `),Qie=n(Pz,"STRONG",{});var zYr=s(Qie);dqo=r(zYr,"not"),zYr.forEach(t),cqo=r(Pz,` load the model weights. It only affects the
model\u2019s configuration. Use `),Hie=n(Pz,"CODE",{});var VYr=s(Hie);fqo=r(VYr,"from_pretrained()"),VYr.forEach(t),mqo=r(Pz,"to load the model weights."),Pz.forEach(t),gqo=i(El),Uie=n(El,"P",{});var WYr=s(Uie);hqo=r(WYr,"Examples:"),WYr.forEach(t),pqo=i(El),m(fw.$$.fragment,El),El.forEach(t),_qo=i(Ml),Ue=n(Ml,"DIV",{class:!0});var ea=s(Ue);m(mw.$$.fragment,ea),uqo=i(ea),Jie=n(ea,"P",{});var QYr=s(Jie);bqo=r(QYr,"Instantiate one of the model classes of the library (with a object detection head) from a pretrained model."),QYr.forEach(t),vqo=i(ea),an=n(ea,"P",{});var wM=s(an);Tqo=r(wM,"The model class to instantiate is selected based on the "),Yie=n(wM,"CODE",{});var HYr=s(Yie);Fqo=r(HYr,"model_type"),HYr.forEach(t),Cqo=r(wM,` property of the config object (either
passed as an argument or loaded from `),Kie=n(wM,"CODE",{});var UYr=s(Kie);Mqo=r(UYr,"pretrained_model_name_or_path"),UYr.forEach(t),Eqo=r(wM,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Zie=n(wM,"CODE",{});var JYr=s(Zie);yqo=r(JYr,"pretrained_model_name_or_path"),JYr.forEach(t),wqo=r(wM,":"),wM.forEach(t),Aqo=i(ea),ede=n(ea,"UL",{});var YYr=s(ede);h5=n(YYr,"LI",{});var qye=s(h5);ode=n(qye,"STRONG",{});var KYr=s(ode);Lqo=r(KYr,"detr"),KYr.forEach(t),Bqo=r(qye," \u2014 "),AN=n(qye,"A",{href:!0});var ZYr=s(AN);kqo=r(ZYr,"DetrForObjectDetection"),ZYr.forEach(t),xqo=r(qye," (DETR model)"),qye.forEach(t),YYr.forEach(t),Rqo=i(ea),p5=n(ea,"P",{});var Gye=s(p5);Sqo=r(Gye,"The model is set in evaluation mode by default using "),rde=n(Gye,"CODE",{});var eKr=s(rde);Pqo=r(eKr,"model.eval()"),eKr.forEach(t),$qo=r(Gye,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),tde=n(Gye,"CODE",{});var oKr=s(tde);Iqo=r(oKr,"model.train()"),oKr.forEach(t),Gye.forEach(t),jqo=i(ea),ade=n(ea,"P",{});var rKr=s(ade);Nqo=r(rKr,"Examples:"),rKr.forEach(t),Dqo=i(ea),m(gw.$$.fragment,ea),ea.forEach(t),Ml.forEach(t),r9e=i(d),Ud=n(d,"H2",{class:!0});var cke=s(Ud);_5=n(cke,"A",{id:!0,class:!0,href:!0});var tKr=s(_5);nde=n(tKr,"SPAN",{});var aKr=s(nde);m(hw.$$.fragment,aKr),aKr.forEach(t),tKr.forEach(t),qqo=i(cke),sde=n(cke,"SPAN",{});var nKr=s(sde);Gqo=r(nKr,"AutoModelForImageSegmentation"),nKr.forEach(t),cke.forEach(t),t9e=i(d),fr=n(d,"DIV",{class:!0});var yl=s(fr);m(pw.$$.fragment,yl),Oqo=i(yl),Jd=n(yl,"P",{});var $z=s(Jd);Xqo=r($z,`This is a generic model class that will be instantiated as one of the model classes of the library (with a image segmentation head) when created
with the `),lde=n($z,"CODE",{});var sKr=s(lde);zqo=r(sKr,"from_pretrained()"),sKr.forEach(t),Vqo=r($z,"class method or the "),ide=n($z,"CODE",{});var lKr=s(ide);Wqo=r(lKr,"from_config()"),lKr.forEach(t),Qqo=r($z,`class
method.`),$z.forEach(t),Hqo=i(yl),_w=n(yl,"P",{});var fke=s(_w);Uqo=r(fke,"This class cannot be instantiated directly using "),dde=n(fke,"CODE",{});var iKr=s(dde);Jqo=r(iKr,"__init__()"),iKr.forEach(t),Yqo=r(fke," (throws an error)."),fke.forEach(t),Kqo=i(yl),at=n(yl,"DIV",{class:!0});var wl=s(at);m(uw.$$.fragment,wl),Zqo=i(wl),cde=n(wl,"P",{});var dKr=s(cde);eGo=r(dKr,"Instantiates one of the model classes of the library (with a image segmentation head) from a configuration."),dKr.forEach(t),oGo=i(wl),Yd=n(wl,"P",{});var Iz=s(Yd);rGo=r(Iz,`Note:
Loading a model from its configuration file does `),fde=n(Iz,"STRONG",{});var cKr=s(fde);tGo=r(cKr,"not"),cKr.forEach(t),aGo=r(Iz,` load the model weights. It only affects the
model\u2019s configuration. Use `),mde=n(Iz,"CODE",{});var fKr=s(mde);nGo=r(fKr,"from_pretrained()"),fKr.forEach(t),sGo=r(Iz,"to load the model weights."),Iz.forEach(t),lGo=i(wl),gde=n(wl,"P",{});var mKr=s(gde);iGo=r(mKr,"Examples:"),mKr.forEach(t),dGo=i(wl),m(bw.$$.fragment,wl),wl.forEach(t),cGo=i(yl),Je=n(yl,"DIV",{class:!0});var oa=s(Je);m(vw.$$.fragment,oa),fGo=i(oa),hde=n(oa,"P",{});var gKr=s(hde);mGo=r(gKr,"Instantiate one of the model classes of the library (with a image segmentation head) from a pretrained model."),gKr.forEach(t),gGo=i(oa),nn=n(oa,"P",{});var AM=s(nn);hGo=r(AM,"The model class to instantiate is selected based on the "),pde=n(AM,"CODE",{});var hKr=s(pde);pGo=r(hKr,"model_type"),hKr.forEach(t),_Go=r(AM,` property of the config object (either
passed as an argument or loaded from `),_de=n(AM,"CODE",{});var pKr=s(_de);uGo=r(pKr,"pretrained_model_name_or_path"),pKr.forEach(t),bGo=r(AM,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),ude=n(AM,"CODE",{});var _Kr=s(ude);vGo=r(_Kr,"pretrained_model_name_or_path"),_Kr.forEach(t),TGo=r(AM,":"),AM.forEach(t),FGo=i(oa),bde=n(oa,"UL",{});var uKr=s(bde);u5=n(uKr,"LI",{});var Oye=s(u5);vde=n(Oye,"STRONG",{});var bKr=s(vde);CGo=r(bKr,"detr"),bKr.forEach(t),MGo=r(Oye," \u2014 "),LN=n(Oye,"A",{href:!0});var vKr=s(LN);EGo=r(vKr,"DetrForSegmentation"),vKr.forEach(t),yGo=r(Oye," (DETR model)"),Oye.forEach(t),uKr.forEach(t),wGo=i(oa),b5=n(oa,"P",{});var Xye=s(b5);AGo=r(Xye,"The model is set in evaluation mode by default using "),Tde=n(Xye,"CODE",{});var TKr=s(Tde);LGo=r(TKr,"model.eval()"),TKr.forEach(t),BGo=r(Xye,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Fde=n(Xye,"CODE",{});var FKr=s(Fde);kGo=r(FKr,"model.train()"),FKr.forEach(t),Xye.forEach(t),xGo=i(oa),Cde=n(oa,"P",{});var CKr=s(Cde);RGo=r(CKr,"Examples:"),CKr.forEach(t),SGo=i(oa),m(Tw.$$.fragment,oa),oa.forEach(t),yl.forEach(t),a9e=i(d),Kd=n(d,"H2",{class:!0});var mke=s(Kd);v5=n(mke,"A",{id:!0,class:!0,href:!0});var MKr=s(v5);Mde=n(MKr,"SPAN",{});var EKr=s(Mde);m(Fw.$$.fragment,EKr),EKr.forEach(t),MKr.forEach(t),PGo=i(mke),Ede=n(mke,"SPAN",{});var yKr=s(Ede);$Go=r(yKr,"AutoModelForSemanticSegmentation"),yKr.forEach(t),mke.forEach(t),n9e=i(d),mr=n(d,"DIV",{class:!0});var Al=s(mr);m(Cw.$$.fragment,Al),IGo=i(Al),Zd=n(Al,"P",{});var jz=s(Zd);jGo=r(jz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a semantic segmentation head) when created
with the `),yde=n(jz,"CODE",{});var wKr=s(yde);NGo=r(wKr,"from_pretrained()"),wKr.forEach(t),DGo=r(jz,"class method or the "),wde=n(jz,"CODE",{});var AKr=s(wde);qGo=r(AKr,"from_config()"),AKr.forEach(t),GGo=r(jz,`class
method.`),jz.forEach(t),OGo=i(Al),Mw=n(Al,"P",{});var gke=s(Mw);XGo=r(gke,"This class cannot be instantiated directly using "),Ade=n(gke,"CODE",{});var LKr=s(Ade);zGo=r(LKr,"__init__()"),LKr.forEach(t),VGo=r(gke," (throws an error)."),gke.forEach(t),WGo=i(Al),nt=n(Al,"DIV",{class:!0});var Ll=s(nt);m(Ew.$$.fragment,Ll),QGo=i(Ll),Lde=n(Ll,"P",{});var BKr=s(Lde);HGo=r(BKr,"Instantiates one of the model classes of the library (with a semantic segmentation head) from a configuration."),BKr.forEach(t),UGo=i(Ll),ec=n(Ll,"P",{});var Nz=s(ec);JGo=r(Nz,`Note:
Loading a model from its configuration file does `),Bde=n(Nz,"STRONG",{});var kKr=s(Bde);YGo=r(kKr,"not"),kKr.forEach(t),KGo=r(Nz,` load the model weights. It only affects the
model\u2019s configuration. Use `),kde=n(Nz,"CODE",{});var xKr=s(kde);ZGo=r(xKr,"from_pretrained()"),xKr.forEach(t),eOo=r(Nz,"to load the model weights."),Nz.forEach(t),oOo=i(Ll),xde=n(Ll,"P",{});var RKr=s(xde);rOo=r(RKr,"Examples:"),RKr.forEach(t),tOo=i(Ll),m(yw.$$.fragment,Ll),Ll.forEach(t),aOo=i(Al),Ye=n(Al,"DIV",{class:!0});var ra=s(Ye);m(ww.$$.fragment,ra),nOo=i(ra),Rde=n(ra,"P",{});var SKr=s(Rde);sOo=r(SKr,"Instantiate one of the model classes of the library (with a semantic segmentation head) from a pretrained model."),SKr.forEach(t),lOo=i(ra),sn=n(ra,"P",{});var LM=s(sn);iOo=r(LM,"The model class to instantiate is selected based on the "),Sde=n(LM,"CODE",{});var PKr=s(Sde);dOo=r(PKr,"model_type"),PKr.forEach(t),cOo=r(LM,` property of the config object (either
passed as an argument or loaded from `),Pde=n(LM,"CODE",{});var $Kr=s(Pde);fOo=r($Kr,"pretrained_model_name_or_path"),$Kr.forEach(t),mOo=r(LM,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),$de=n(LM,"CODE",{});var IKr=s($de);gOo=r(IKr,"pretrained_model_name_or_path"),IKr.forEach(t),hOo=r(LM,":"),LM.forEach(t),pOo=i(ra),Aw=n(ra,"UL",{});var hke=s(Aw);T5=n(hke,"LI",{});var zye=s(T5);Ide=n(zye,"STRONG",{});var jKr=s(Ide);_Oo=r(jKr,"beit"),jKr.forEach(t),uOo=r(zye," \u2014 "),BN=n(zye,"A",{href:!0});var NKr=s(BN);bOo=r(NKr,"BeitForSemanticSegmentation"),NKr.forEach(t),vOo=r(zye," (BEiT model)"),zye.forEach(t),TOo=i(hke),F5=n(hke,"LI",{});var Vye=s(F5);jde=n(Vye,"STRONG",{});var DKr=s(jde);FOo=r(DKr,"segformer"),DKr.forEach(t),COo=r(Vye," \u2014 "),kN=n(Vye,"A",{href:!0});var qKr=s(kN);MOo=r(qKr,"SegformerForSemanticSegmentation"),qKr.forEach(t),EOo=r(Vye," (SegFormer model)"),Vye.forEach(t),hke.forEach(t),yOo=i(ra),C5=n(ra,"P",{});var Wye=s(C5);wOo=r(Wye,"The model is set in evaluation mode by default using "),Nde=n(Wye,"CODE",{});var GKr=s(Nde);AOo=r(GKr,"model.eval()"),GKr.forEach(t),LOo=r(Wye,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Dde=n(Wye,"CODE",{});var OKr=s(Dde);BOo=r(OKr,"model.train()"),OKr.forEach(t),Wye.forEach(t),kOo=i(ra),qde=n(ra,"P",{});var XKr=s(qde);xOo=r(XKr,"Examples:"),XKr.forEach(t),ROo=i(ra),m(Lw.$$.fragment,ra),ra.forEach(t),Al.forEach(t),s9e=i(d),oc=n(d,"H2",{class:!0});var pke=s(oc);M5=n(pke,"A",{id:!0,class:!0,href:!0});var zKr=s(M5);Gde=n(zKr,"SPAN",{});var VKr=s(Gde);m(Bw.$$.fragment,VKr),VKr.forEach(t),zKr.forEach(t),SOo=i(pke),Ode=n(pke,"SPAN",{});var WKr=s(Ode);POo=r(WKr,"TFAutoModel"),WKr.forEach(t),pke.forEach(t),l9e=i(d),gr=n(d,"DIV",{class:!0});var Bl=s(gr);m(kw.$$.fragment,Bl),$Oo=i(Bl),rc=n(Bl,"P",{});var Dz=s(rc);IOo=r(Dz,`This is a generic model class that will be instantiated as one of the base model classes of the library when created
with the `),Xde=n(Dz,"CODE",{});var QKr=s(Xde);jOo=r(QKr,"from_pretrained()"),QKr.forEach(t),NOo=r(Dz,"class method or the "),zde=n(Dz,"CODE",{});var HKr=s(zde);DOo=r(HKr,"from_config()"),HKr.forEach(t),qOo=r(Dz,`class
method.`),Dz.forEach(t),GOo=i(Bl),xw=n(Bl,"P",{});var _ke=s(xw);OOo=r(_ke,"This class cannot be instantiated directly using "),Vde=n(_ke,"CODE",{});var UKr=s(Vde);XOo=r(UKr,"__init__()"),UKr.forEach(t),zOo=r(_ke," (throws an error)."),_ke.forEach(t),VOo=i(Bl),st=n(Bl,"DIV",{class:!0});var kl=s(st);m(Rw.$$.fragment,kl),WOo=i(kl),Wde=n(kl,"P",{});var JKr=s(Wde);QOo=r(JKr,"Instantiates one of the base model classes of the library from a configuration."),JKr.forEach(t),HOo=i(kl),tc=n(kl,"P",{});var qz=s(tc);UOo=r(qz,`Note:
Loading a model from its configuration file does `),Qde=n(qz,"STRONG",{});var YKr=s(Qde);JOo=r(YKr,"not"),YKr.forEach(t),YOo=r(qz,` load the model weights. It only affects the
model\u2019s configuration. Use `),Hde=n(qz,"CODE",{});var KKr=s(Hde);KOo=r(KKr,"from_pretrained()"),KKr.forEach(t),ZOo=r(qz,"to load the model weights."),qz.forEach(t),eXo=i(kl),Ude=n(kl,"P",{});var ZKr=s(Ude);oXo=r(ZKr,"Examples:"),ZKr.forEach(t),rXo=i(kl),m(Sw.$$.fragment,kl),kl.forEach(t),tXo=i(Bl),go=n(Bl,"DIV",{class:!0});var ca=s(go);m(Pw.$$.fragment,ca),aXo=i(ca),Jde=n(ca,"P",{});var eZr=s(Jde);nXo=r(eZr,"Instantiate one of the base model classes of the library from a pretrained model."),eZr.forEach(t),sXo=i(ca),ln=n(ca,"P",{});var BM=s(ln);lXo=r(BM,"The model class to instantiate is selected based on the "),Yde=n(BM,"CODE",{});var oZr=s(Yde);iXo=r(oZr,"model_type"),oZr.forEach(t),dXo=r(BM,` property of the config object (either
passed as an argument or loaded from `),Kde=n(BM,"CODE",{});var rZr=s(Kde);cXo=r(rZr,"pretrained_model_name_or_path"),rZr.forEach(t),fXo=r(BM,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Zde=n(BM,"CODE",{});var tZr=s(Zde);mXo=r(tZr,"pretrained_model_name_or_path"),tZr.forEach(t),gXo=r(BM,":"),BM.forEach(t),hXo=i(ca),B=n(ca,"UL",{});var k=s(B);E5=n(k,"LI",{});var Qye=s(E5);ece=n(Qye,"STRONG",{});var aZr=s(ece);pXo=r(aZr,"albert"),aZr.forEach(t),_Xo=r(Qye," \u2014 "),xN=n(Qye,"A",{href:!0});var nZr=s(xN);uXo=r(nZr,"TFAlbertModel"),nZr.forEach(t),bXo=r(Qye," (ALBERT model)"),Qye.forEach(t),vXo=i(k),y5=n(k,"LI",{});var Hye=s(y5);oce=n(Hye,"STRONG",{});var sZr=s(oce);TXo=r(sZr,"bart"),sZr.forEach(t),FXo=r(Hye," \u2014 "),RN=n(Hye,"A",{href:!0});var lZr=s(RN);CXo=r(lZr,"TFBartModel"),lZr.forEach(t),MXo=r(Hye," (BART model)"),Hye.forEach(t),EXo=i(k),w5=n(k,"LI",{});var Uye=s(w5);rce=n(Uye,"STRONG",{});var iZr=s(rce);yXo=r(iZr,"bert"),iZr.forEach(t),wXo=r(Uye," \u2014 "),SN=n(Uye,"A",{href:!0});var dZr=s(SN);AXo=r(dZr,"TFBertModel"),dZr.forEach(t),LXo=r(Uye," (BERT model)"),Uye.forEach(t),BXo=i(k),A5=n(k,"LI",{});var Jye=s(A5);tce=n(Jye,"STRONG",{});var cZr=s(tce);kXo=r(cZr,"blenderbot"),cZr.forEach(t),xXo=r(Jye," \u2014 "),PN=n(Jye,"A",{href:!0});var fZr=s(PN);RXo=r(fZr,"TFBlenderbotModel"),fZr.forEach(t),SXo=r(Jye," (Blenderbot model)"),Jye.forEach(t),PXo=i(k),L5=n(k,"LI",{});var Yye=s(L5);ace=n(Yye,"STRONG",{});var mZr=s(ace);$Xo=r(mZr,"blenderbot-small"),mZr.forEach(t),IXo=r(Yye," \u2014 "),$N=n(Yye,"A",{href:!0});var gZr=s($N);jXo=r(gZr,"TFBlenderbotSmallModel"),gZr.forEach(t),NXo=r(Yye," (BlenderbotSmall model)"),Yye.forEach(t),DXo=i(k),B5=n(k,"LI",{});var Kye=s(B5);nce=n(Kye,"STRONG",{});var hZr=s(nce);qXo=r(hZr,"camembert"),hZr.forEach(t),GXo=r(Kye," \u2014 "),IN=n(Kye,"A",{href:!0});var pZr=s(IN);OXo=r(pZr,"TFCamembertModel"),pZr.forEach(t),XXo=r(Kye," (CamemBERT model)"),Kye.forEach(t),zXo=i(k),k5=n(k,"LI",{});var Zye=s(k5);sce=n(Zye,"STRONG",{});var _Zr=s(sce);VXo=r(_Zr,"clip"),_Zr.forEach(t),WXo=r(Zye," \u2014 "),jN=n(Zye,"A",{href:!0});var uZr=s(jN);QXo=r(uZr,"TFCLIPModel"),uZr.forEach(t),HXo=r(Zye," (CLIP model)"),Zye.forEach(t),UXo=i(k),x5=n(k,"LI",{});var ewe=s(x5);lce=n(ewe,"STRONG",{});var bZr=s(lce);JXo=r(bZr,"convbert"),bZr.forEach(t),YXo=r(ewe," \u2014 "),NN=n(ewe,"A",{href:!0});var vZr=s(NN);KXo=r(vZr,"TFConvBertModel"),vZr.forEach(t),ZXo=r(ewe," (ConvBERT model)"),ewe.forEach(t),ezo=i(k),R5=n(k,"LI",{});var owe=s(R5);ice=n(owe,"STRONG",{});var TZr=s(ice);ozo=r(TZr,"ctrl"),TZr.forEach(t),rzo=r(owe," \u2014 "),DN=n(owe,"A",{href:!0});var FZr=s(DN);tzo=r(FZr,"TFCTRLModel"),FZr.forEach(t),azo=r(owe," (CTRL model)"),owe.forEach(t),nzo=i(k),S5=n(k,"LI",{});var rwe=s(S5);dce=n(rwe,"STRONG",{});var CZr=s(dce);szo=r(CZr,"deberta"),CZr.forEach(t),lzo=r(rwe," \u2014 "),qN=n(rwe,"A",{href:!0});var MZr=s(qN);izo=r(MZr,"TFDebertaModel"),MZr.forEach(t),dzo=r(rwe," (DeBERTa model)"),rwe.forEach(t),czo=i(k),P5=n(k,"LI",{});var twe=s(P5);cce=n(twe,"STRONG",{});var EZr=s(cce);fzo=r(EZr,"deberta-v2"),EZr.forEach(t),mzo=r(twe," \u2014 "),GN=n(twe,"A",{href:!0});var yZr=s(GN);gzo=r(yZr,"TFDebertaV2Model"),yZr.forEach(t),hzo=r(twe," (DeBERTa-v2 model)"),twe.forEach(t),pzo=i(k),$5=n(k,"LI",{});var awe=s($5);fce=n(awe,"STRONG",{});var wZr=s(fce);_zo=r(wZr,"distilbert"),wZr.forEach(t),uzo=r(awe," \u2014 "),ON=n(awe,"A",{href:!0});var AZr=s(ON);bzo=r(AZr,"TFDistilBertModel"),AZr.forEach(t),vzo=r(awe," (DistilBERT model)"),awe.forEach(t),Tzo=i(k),I5=n(k,"LI",{});var nwe=s(I5);mce=n(nwe,"STRONG",{});var LZr=s(mce);Fzo=r(LZr,"dpr"),LZr.forEach(t),Czo=r(nwe," \u2014 "),XN=n(nwe,"A",{href:!0});var BZr=s(XN);Mzo=r(BZr,"TFDPRQuestionEncoder"),BZr.forEach(t),Ezo=r(nwe," (DPR model)"),nwe.forEach(t),yzo=i(k),j5=n(k,"LI",{});var swe=s(j5);gce=n(swe,"STRONG",{});var kZr=s(gce);wzo=r(kZr,"electra"),kZr.forEach(t),Azo=r(swe," \u2014 "),zN=n(swe,"A",{href:!0});var xZr=s(zN);Lzo=r(xZr,"TFElectraModel"),xZr.forEach(t),Bzo=r(swe," (ELECTRA model)"),swe.forEach(t),kzo=i(k),N5=n(k,"LI",{});var lwe=s(N5);hce=n(lwe,"STRONG",{});var RZr=s(hce);xzo=r(RZr,"flaubert"),RZr.forEach(t),Rzo=r(lwe," \u2014 "),VN=n(lwe,"A",{href:!0});var SZr=s(VN);Szo=r(SZr,"TFFlaubertModel"),SZr.forEach(t),Pzo=r(lwe," (FlauBERT model)"),lwe.forEach(t),$zo=i(k),Ss=n(k,"LI",{});var NL=s(Ss);pce=n(NL,"STRONG",{});var PZr=s(pce);Izo=r(PZr,"funnel"),PZr.forEach(t),jzo=r(NL," \u2014 "),WN=n(NL,"A",{href:!0});var $Zr=s(WN);Nzo=r($Zr,"TFFunnelModel"),$Zr.forEach(t),Dzo=r(NL," or "),QN=n(NL,"A",{href:!0});var IZr=s(QN);qzo=r(IZr,"TFFunnelBaseModel"),IZr.forEach(t),Gzo=r(NL," (Funnel Transformer model)"),NL.forEach(t),Ozo=i(k),D5=n(k,"LI",{});var iwe=s(D5);_ce=n(iwe,"STRONG",{});var jZr=s(_ce);Xzo=r(jZr,"gpt2"),jZr.forEach(t),zzo=r(iwe," \u2014 "),HN=n(iwe,"A",{href:!0});var NZr=s(HN);Vzo=r(NZr,"TFGPT2Model"),NZr.forEach(t),Wzo=r(iwe," (OpenAI GPT-2 model)"),iwe.forEach(t),Qzo=i(k),q5=n(k,"LI",{});var dwe=s(q5);uce=n(dwe,"STRONG",{});var DZr=s(uce);Hzo=r(DZr,"hubert"),DZr.forEach(t),Uzo=r(dwe," \u2014 "),UN=n(dwe,"A",{href:!0});var qZr=s(UN);Jzo=r(qZr,"TFHubertModel"),qZr.forEach(t),Yzo=r(dwe," (Hubert model)"),dwe.forEach(t),Kzo=i(k),G5=n(k,"LI",{});var cwe=s(G5);bce=n(cwe,"STRONG",{});var GZr=s(bce);Zzo=r(GZr,"layoutlm"),GZr.forEach(t),eVo=r(cwe," \u2014 "),JN=n(cwe,"A",{href:!0});var OZr=s(JN);oVo=r(OZr,"TFLayoutLMModel"),OZr.forEach(t),rVo=r(cwe," (LayoutLM model)"),cwe.forEach(t),tVo=i(k),O5=n(k,"LI",{});var fwe=s(O5);vce=n(fwe,"STRONG",{});var XZr=s(vce);aVo=r(XZr,"led"),XZr.forEach(t),nVo=r(fwe," \u2014 "),YN=n(fwe,"A",{href:!0});var zZr=s(YN);sVo=r(zZr,"TFLEDModel"),zZr.forEach(t),lVo=r(fwe," (LED model)"),fwe.forEach(t),iVo=i(k),X5=n(k,"LI",{});var mwe=s(X5);Tce=n(mwe,"STRONG",{});var VZr=s(Tce);dVo=r(VZr,"longformer"),VZr.forEach(t),cVo=r(mwe," \u2014 "),KN=n(mwe,"A",{href:!0});var WZr=s(KN);fVo=r(WZr,"TFLongformerModel"),WZr.forEach(t),mVo=r(mwe," (Longformer model)"),mwe.forEach(t),gVo=i(k),z5=n(k,"LI",{});var gwe=s(z5);Fce=n(gwe,"STRONG",{});var QZr=s(Fce);hVo=r(QZr,"lxmert"),QZr.forEach(t),pVo=r(gwe," \u2014 "),ZN=n(gwe,"A",{href:!0});var HZr=s(ZN);_Vo=r(HZr,"TFLxmertModel"),HZr.forEach(t),uVo=r(gwe," (LXMERT model)"),gwe.forEach(t),bVo=i(k),V5=n(k,"LI",{});var hwe=s(V5);Cce=n(hwe,"STRONG",{});var UZr=s(Cce);vVo=r(UZr,"marian"),UZr.forEach(t),TVo=r(hwe," \u2014 "),eD=n(hwe,"A",{href:!0});var JZr=s(eD);FVo=r(JZr,"TFMarianModel"),JZr.forEach(t),CVo=r(hwe," (Marian model)"),hwe.forEach(t),MVo=i(k),W5=n(k,"LI",{});var pwe=s(W5);Mce=n(pwe,"STRONG",{});var YZr=s(Mce);EVo=r(YZr,"mbart"),YZr.forEach(t),yVo=r(pwe," \u2014 "),oD=n(pwe,"A",{href:!0});var KZr=s(oD);wVo=r(KZr,"TFMBartModel"),KZr.forEach(t),AVo=r(pwe," (mBART model)"),pwe.forEach(t),LVo=i(k),Q5=n(k,"LI",{});var _we=s(Q5);Ece=n(_we,"STRONG",{});var ZZr=s(Ece);BVo=r(ZZr,"mobilebert"),ZZr.forEach(t),kVo=r(_we," \u2014 "),rD=n(_we,"A",{href:!0});var eet=s(rD);xVo=r(eet,"TFMobileBertModel"),eet.forEach(t),RVo=r(_we," (MobileBERT model)"),_we.forEach(t),SVo=i(k),H5=n(k,"LI",{});var uwe=s(H5);yce=n(uwe,"STRONG",{});var oet=s(yce);PVo=r(oet,"mpnet"),oet.forEach(t),$Vo=r(uwe," \u2014 "),tD=n(uwe,"A",{href:!0});var ret=s(tD);IVo=r(ret,"TFMPNetModel"),ret.forEach(t),jVo=r(uwe," (MPNet model)"),uwe.forEach(t),NVo=i(k),U5=n(k,"LI",{});var bwe=s(U5);wce=n(bwe,"STRONG",{});var tet=s(wce);DVo=r(tet,"mt5"),tet.forEach(t),qVo=r(bwe," \u2014 "),aD=n(bwe,"A",{href:!0});var aet=s(aD);GVo=r(aet,"TFMT5Model"),aet.forEach(t),OVo=r(bwe," (mT5 model)"),bwe.forEach(t),XVo=i(k),J5=n(k,"LI",{});var vwe=s(J5);Ace=n(vwe,"STRONG",{});var net=s(Ace);zVo=r(net,"openai-gpt"),net.forEach(t),VVo=r(vwe," \u2014 "),nD=n(vwe,"A",{href:!0});var set=s(nD);WVo=r(set,"TFOpenAIGPTModel"),set.forEach(t),QVo=r(vwe," (OpenAI GPT model)"),vwe.forEach(t),HVo=i(k),Y5=n(k,"LI",{});var Twe=s(Y5);Lce=n(Twe,"STRONG",{});var iet=s(Lce);UVo=r(iet,"pegasus"),iet.forEach(t),JVo=r(Twe," \u2014 "),sD=n(Twe,"A",{href:!0});var det=s(sD);YVo=r(det,"TFPegasusModel"),det.forEach(t),KVo=r(Twe," (Pegasus model)"),Twe.forEach(t),ZVo=i(k),K5=n(k,"LI",{});var Fwe=s(K5);Bce=n(Fwe,"STRONG",{});var cet=s(Bce);eWo=r(cet,"rembert"),cet.forEach(t),oWo=r(Fwe," \u2014 "),lD=n(Fwe,"A",{href:!0});var fet=s(lD);rWo=r(fet,"TFRemBertModel"),fet.forEach(t),tWo=r(Fwe," (RemBERT model)"),Fwe.forEach(t),aWo=i(k),Z5=n(k,"LI",{});var Cwe=s(Z5);kce=n(Cwe,"STRONG",{});var met=s(kce);nWo=r(met,"roberta"),met.forEach(t),sWo=r(Cwe," \u2014 "),iD=n(Cwe,"A",{href:!0});var get=s(iD);lWo=r(get,"TFRobertaModel"),get.forEach(t),iWo=r(Cwe," (RoBERTa model)"),Cwe.forEach(t),dWo=i(k),e2=n(k,"LI",{});var Mwe=s(e2);xce=n(Mwe,"STRONG",{});var het=s(xce);cWo=r(het,"roformer"),het.forEach(t),fWo=r(Mwe," \u2014 "),dD=n(Mwe,"A",{href:!0});var pet=s(dD);mWo=r(pet,"TFRoFormerModel"),pet.forEach(t),gWo=r(Mwe," (RoFormer model)"),Mwe.forEach(t),hWo=i(k),o2=n(k,"LI",{});var Ewe=s(o2);Rce=n(Ewe,"STRONG",{});var _et=s(Rce);pWo=r(_et,"speech_to_text"),_et.forEach(t),_Wo=r(Ewe," \u2014 "),cD=n(Ewe,"A",{href:!0});var uet=s(cD);uWo=r(uet,"TFSpeech2TextModel"),uet.forEach(t),bWo=r(Ewe," (Speech2Text model)"),Ewe.forEach(t),vWo=i(k),r2=n(k,"LI",{});var ywe=s(r2);Sce=n(ywe,"STRONG",{});var bet=s(Sce);TWo=r(bet,"t5"),bet.forEach(t),FWo=r(ywe," \u2014 "),fD=n(ywe,"A",{href:!0});var vet=s(fD);CWo=r(vet,"TFT5Model"),vet.forEach(t),MWo=r(ywe," (T5 model)"),ywe.forEach(t),EWo=i(k),t2=n(k,"LI",{});var wwe=s(t2);Pce=n(wwe,"STRONG",{});var Tet=s(Pce);yWo=r(Tet,"tapas"),Tet.forEach(t),wWo=r(wwe," \u2014 "),mD=n(wwe,"A",{href:!0});var Fet=s(mD);AWo=r(Fet,"TFTapasModel"),Fet.forEach(t),LWo=r(wwe," (TAPAS model)"),wwe.forEach(t),BWo=i(k),a2=n(k,"LI",{});var Awe=s(a2);$ce=n(Awe,"STRONG",{});var Cet=s($ce);kWo=r(Cet,"transfo-xl"),Cet.forEach(t),xWo=r(Awe," \u2014 "),gD=n(Awe,"A",{href:!0});var Met=s(gD);RWo=r(Met,"TFTransfoXLModel"),Met.forEach(t),SWo=r(Awe," (Transformer-XL model)"),Awe.forEach(t),PWo=i(k),n2=n(k,"LI",{});var Lwe=s(n2);Ice=n(Lwe,"STRONG",{});var Eet=s(Ice);$Wo=r(Eet,"vit"),Eet.forEach(t),IWo=r(Lwe," \u2014 "),hD=n(Lwe,"A",{href:!0});var yet=s(hD);jWo=r(yet,"TFViTModel"),yet.forEach(t),NWo=r(Lwe," (ViT model)"),Lwe.forEach(t),DWo=i(k),s2=n(k,"LI",{});var Bwe=s(s2);jce=n(Bwe,"STRONG",{});var wet=s(jce);qWo=r(wet,"wav2vec2"),wet.forEach(t),GWo=r(Bwe," \u2014 "),pD=n(Bwe,"A",{href:!0});var Aet=s(pD);OWo=r(Aet,"TFWav2Vec2Model"),Aet.forEach(t),XWo=r(Bwe," (Wav2Vec2 model)"),Bwe.forEach(t),zWo=i(k),l2=n(k,"LI",{});var kwe=s(l2);Nce=n(kwe,"STRONG",{});var Let=s(Nce);VWo=r(Let,"xlm"),Let.forEach(t),WWo=r(kwe," \u2014 "),_D=n(kwe,"A",{href:!0});var Bet=s(_D);QWo=r(Bet,"TFXLMModel"),Bet.forEach(t),HWo=r(kwe," (XLM model)"),kwe.forEach(t),UWo=i(k),i2=n(k,"LI",{});var xwe=s(i2);Dce=n(xwe,"STRONG",{});var ket=s(Dce);JWo=r(ket,"xlm-roberta"),ket.forEach(t),YWo=r(xwe," \u2014 "),uD=n(xwe,"A",{href:!0});var xet=s(uD);KWo=r(xet,"TFXLMRobertaModel"),xet.forEach(t),ZWo=r(xwe," (XLM-RoBERTa model)"),xwe.forEach(t),eQo=i(k),d2=n(k,"LI",{});var Rwe=s(d2);qce=n(Rwe,"STRONG",{});var Ret=s(qce);oQo=r(Ret,"xlnet"),Ret.forEach(t),rQo=r(Rwe," \u2014 "),bD=n(Rwe,"A",{href:!0});var Set=s(bD);tQo=r(Set,"TFXLNetModel"),Set.forEach(t),aQo=r(Rwe," (XLNet model)"),Rwe.forEach(t),k.forEach(t),nQo=i(ca),Gce=n(ca,"P",{});var Pet=s(Gce);sQo=r(Pet,"Examples:"),Pet.forEach(t),lQo=i(ca),m($w.$$.fragment,ca),ca.forEach(t),Bl.forEach(t),i9e=i(d),ac=n(d,"H2",{class:!0});var uke=s(ac);c2=n(uke,"A",{id:!0,class:!0,href:!0});var $et=s(c2);Oce=n($et,"SPAN",{});var Iet=s(Oce);m(Iw.$$.fragment,Iet),Iet.forEach(t),$et.forEach(t),iQo=i(uke),Xce=n(uke,"SPAN",{});var jet=s(Xce);dQo=r(jet,"TFAutoModelForPreTraining"),jet.forEach(t),uke.forEach(t),d9e=i(d),hr=n(d,"DIV",{class:!0});var xl=s(hr);m(jw.$$.fragment,xl),cQo=i(xl),nc=n(xl,"P",{});var Gz=s(nc);fQo=r(Gz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a pretraining head) when created
with the `),zce=n(Gz,"CODE",{});var Net=s(zce);mQo=r(Net,"from_pretrained()"),Net.forEach(t),gQo=r(Gz,"class method or the "),Vce=n(Gz,"CODE",{});var Det=s(Vce);hQo=r(Det,"from_config()"),Det.forEach(t),pQo=r(Gz,`class
method.`),Gz.forEach(t),_Qo=i(xl),Nw=n(xl,"P",{});var bke=s(Nw);uQo=r(bke,"This class cannot be instantiated directly using "),Wce=n(bke,"CODE",{});var qet=s(Wce);bQo=r(qet,"__init__()"),qet.forEach(t),vQo=r(bke," (throws an error)."),bke.forEach(t),TQo=i(xl),lt=n(xl,"DIV",{class:!0});var Rl=s(lt);m(Dw.$$.fragment,Rl),FQo=i(Rl),Qce=n(Rl,"P",{});var Get=s(Qce);CQo=r(Get,"Instantiates one of the model classes of the library (with a pretraining head) from a configuration."),Get.forEach(t),MQo=i(Rl),sc=n(Rl,"P",{});var Oz=s(sc);EQo=r(Oz,`Note:
Loading a model from its configuration file does `),Hce=n(Oz,"STRONG",{});var Oet=s(Hce);yQo=r(Oet,"not"),Oet.forEach(t),wQo=r(Oz,` load the model weights. It only affects the
model\u2019s configuration. Use `),Uce=n(Oz,"CODE",{});var Xet=s(Uce);AQo=r(Xet,"from_pretrained()"),Xet.forEach(t),LQo=r(Oz,"to load the model weights."),Oz.forEach(t),BQo=i(Rl),Jce=n(Rl,"P",{});var zet=s(Jce);kQo=r(zet,"Examples:"),zet.forEach(t),xQo=i(Rl),m(qw.$$.fragment,Rl),Rl.forEach(t),RQo=i(xl),ho=n(xl,"DIV",{class:!0});var fa=s(ho);m(Gw.$$.fragment,fa),SQo=i(fa),Yce=n(fa,"P",{});var Vet=s(Yce);PQo=r(Vet,"Instantiate one of the model classes of the library (with a pretraining head) from a pretrained model."),Vet.forEach(t),$Qo=i(fa),dn=n(fa,"P",{});var kM=s(dn);IQo=r(kM,"The model class to instantiate is selected based on the "),Kce=n(kM,"CODE",{});var Wet=s(Kce);jQo=r(Wet,"model_type"),Wet.forEach(t),NQo=r(kM,` property of the config object (either
passed as an argument or loaded from `),Zce=n(kM,"CODE",{});var Qet=s(Zce);DQo=r(Qet,"pretrained_model_name_or_path"),Qet.forEach(t),qQo=r(kM,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),efe=n(kM,"CODE",{});var Het=s(efe);GQo=r(Het,"pretrained_model_name_or_path"),Het.forEach(t),OQo=r(kM,":"),kM.forEach(t),XQo=i(fa),H=n(fa,"UL",{});var U=s(H);f2=n(U,"LI",{});var Swe=s(f2);ofe=n(Swe,"STRONG",{});var Uet=s(ofe);zQo=r(Uet,"albert"),Uet.forEach(t),VQo=r(Swe," \u2014 "),vD=n(Swe,"A",{href:!0});var Jet=s(vD);WQo=r(Jet,"TFAlbertForPreTraining"),Jet.forEach(t),QQo=r(Swe," (ALBERT model)"),Swe.forEach(t),HQo=i(U),m2=n(U,"LI",{});var Pwe=s(m2);rfe=n(Pwe,"STRONG",{});var Yet=s(rfe);UQo=r(Yet,"bart"),Yet.forEach(t),JQo=r(Pwe," \u2014 "),TD=n(Pwe,"A",{href:!0});var Ket=s(TD);YQo=r(Ket,"TFBartForConditionalGeneration"),Ket.forEach(t),KQo=r(Pwe," (BART model)"),Pwe.forEach(t),ZQo=i(U),g2=n(U,"LI",{});var $we=s(g2);tfe=n($we,"STRONG",{});var Zet=s(tfe);eHo=r(Zet,"bert"),Zet.forEach(t),oHo=r($we," \u2014 "),FD=n($we,"A",{href:!0});var eot=s(FD);rHo=r(eot,"TFBertForPreTraining"),eot.forEach(t),tHo=r($we," (BERT model)"),$we.forEach(t),aHo=i(U),h2=n(U,"LI",{});var Iwe=s(h2);afe=n(Iwe,"STRONG",{});var oot=s(afe);nHo=r(oot,"camembert"),oot.forEach(t),sHo=r(Iwe," \u2014 "),CD=n(Iwe,"A",{href:!0});var rot=s(CD);lHo=r(rot,"TFCamembertForMaskedLM"),rot.forEach(t),iHo=r(Iwe," (CamemBERT model)"),Iwe.forEach(t),dHo=i(U),p2=n(U,"LI",{});var jwe=s(p2);nfe=n(jwe,"STRONG",{});var tot=s(nfe);cHo=r(tot,"ctrl"),tot.forEach(t),fHo=r(jwe," \u2014 "),MD=n(jwe,"A",{href:!0});var aot=s(MD);mHo=r(aot,"TFCTRLLMHeadModel"),aot.forEach(t),gHo=r(jwe," (CTRL model)"),jwe.forEach(t),hHo=i(U),_2=n(U,"LI",{});var Nwe=s(_2);sfe=n(Nwe,"STRONG",{});var not=s(sfe);pHo=r(not,"distilbert"),not.forEach(t),_Ho=r(Nwe," \u2014 "),ED=n(Nwe,"A",{href:!0});var sot=s(ED);uHo=r(sot,"TFDistilBertForMaskedLM"),sot.forEach(t),bHo=r(Nwe," (DistilBERT model)"),Nwe.forEach(t),vHo=i(U),u2=n(U,"LI",{});var Dwe=s(u2);lfe=n(Dwe,"STRONG",{});var lot=s(lfe);THo=r(lot,"electra"),lot.forEach(t),FHo=r(Dwe," \u2014 "),yD=n(Dwe,"A",{href:!0});var iot=s(yD);CHo=r(iot,"TFElectraForPreTraining"),iot.forEach(t),MHo=r(Dwe," (ELECTRA model)"),Dwe.forEach(t),EHo=i(U),b2=n(U,"LI",{});var qwe=s(b2);ife=n(qwe,"STRONG",{});var dot=s(ife);yHo=r(dot,"flaubert"),dot.forEach(t),wHo=r(qwe," \u2014 "),wD=n(qwe,"A",{href:!0});var cot=s(wD);AHo=r(cot,"TFFlaubertWithLMHeadModel"),cot.forEach(t),LHo=r(qwe," (FlauBERT model)"),qwe.forEach(t),BHo=i(U),v2=n(U,"LI",{});var Gwe=s(v2);dfe=n(Gwe,"STRONG",{});var fot=s(dfe);kHo=r(fot,"funnel"),fot.forEach(t),xHo=r(Gwe," \u2014 "),AD=n(Gwe,"A",{href:!0});var mot=s(AD);RHo=r(mot,"TFFunnelForPreTraining"),mot.forEach(t),SHo=r(Gwe," (Funnel Transformer model)"),Gwe.forEach(t),PHo=i(U),T2=n(U,"LI",{});var Owe=s(T2);cfe=n(Owe,"STRONG",{});var got=s(cfe);$Ho=r(got,"gpt2"),got.forEach(t),IHo=r(Owe," \u2014 "),LD=n(Owe,"A",{href:!0});var hot=s(LD);jHo=r(hot,"TFGPT2LMHeadModel"),hot.forEach(t),NHo=r(Owe," (OpenAI GPT-2 model)"),Owe.forEach(t),DHo=i(U),F2=n(U,"LI",{});var Xwe=s(F2);ffe=n(Xwe,"STRONG",{});var pot=s(ffe);qHo=r(pot,"layoutlm"),pot.forEach(t),GHo=r(Xwe," \u2014 "),BD=n(Xwe,"A",{href:!0});var _ot=s(BD);OHo=r(_ot,"TFLayoutLMForMaskedLM"),_ot.forEach(t),XHo=r(Xwe," (LayoutLM model)"),Xwe.forEach(t),zHo=i(U),C2=n(U,"LI",{});var zwe=s(C2);mfe=n(zwe,"STRONG",{});var uot=s(mfe);VHo=r(uot,"lxmert"),uot.forEach(t),WHo=r(zwe," \u2014 "),kD=n(zwe,"A",{href:!0});var bot=s(kD);QHo=r(bot,"TFLxmertForPreTraining"),bot.forEach(t),HHo=r(zwe," (LXMERT model)"),zwe.forEach(t),UHo=i(U),M2=n(U,"LI",{});var Vwe=s(M2);gfe=n(Vwe,"STRONG",{});var vot=s(gfe);JHo=r(vot,"mobilebert"),vot.forEach(t),YHo=r(Vwe," \u2014 "),xD=n(Vwe,"A",{href:!0});var Tot=s(xD);KHo=r(Tot,"TFMobileBertForPreTraining"),Tot.forEach(t),ZHo=r(Vwe," (MobileBERT model)"),Vwe.forEach(t),eUo=i(U),E2=n(U,"LI",{});var Wwe=s(E2);hfe=n(Wwe,"STRONG",{});var Fot=s(hfe);oUo=r(Fot,"mpnet"),Fot.forEach(t),rUo=r(Wwe," \u2014 "),RD=n(Wwe,"A",{href:!0});var Cot=s(RD);tUo=r(Cot,"TFMPNetForMaskedLM"),Cot.forEach(t),aUo=r(Wwe," (MPNet model)"),Wwe.forEach(t),nUo=i(U),y2=n(U,"LI",{});var Qwe=s(y2);pfe=n(Qwe,"STRONG",{});var Mot=s(pfe);sUo=r(Mot,"openai-gpt"),Mot.forEach(t),lUo=r(Qwe," \u2014 "),SD=n(Qwe,"A",{href:!0});var Eot=s(SD);iUo=r(Eot,"TFOpenAIGPTLMHeadModel"),Eot.forEach(t),dUo=r(Qwe," (OpenAI GPT model)"),Qwe.forEach(t),cUo=i(U),w2=n(U,"LI",{});var Hwe=s(w2);_fe=n(Hwe,"STRONG",{});var yot=s(_fe);fUo=r(yot,"roberta"),yot.forEach(t),mUo=r(Hwe," \u2014 "),PD=n(Hwe,"A",{href:!0});var wot=s(PD);gUo=r(wot,"TFRobertaForMaskedLM"),wot.forEach(t),hUo=r(Hwe," (RoBERTa model)"),Hwe.forEach(t),pUo=i(U),A2=n(U,"LI",{});var Uwe=s(A2);ufe=n(Uwe,"STRONG",{});var Aot=s(ufe);_Uo=r(Aot,"t5"),Aot.forEach(t),uUo=r(Uwe," \u2014 "),$D=n(Uwe,"A",{href:!0});var Lot=s($D);bUo=r(Lot,"TFT5ForConditionalGeneration"),Lot.forEach(t),vUo=r(Uwe," (T5 model)"),Uwe.forEach(t),TUo=i(U),L2=n(U,"LI",{});var Jwe=s(L2);bfe=n(Jwe,"STRONG",{});var Bot=s(bfe);FUo=r(Bot,"tapas"),Bot.forEach(t),CUo=r(Jwe," \u2014 "),ID=n(Jwe,"A",{href:!0});var kot=s(ID);MUo=r(kot,"TFTapasForMaskedLM"),kot.forEach(t),EUo=r(Jwe," (TAPAS model)"),Jwe.forEach(t),yUo=i(U),B2=n(U,"LI",{});var Ywe=s(B2);vfe=n(Ywe,"STRONG",{});var xot=s(vfe);wUo=r(xot,"transfo-xl"),xot.forEach(t),AUo=r(Ywe," \u2014 "),jD=n(Ywe,"A",{href:!0});var Rot=s(jD);LUo=r(Rot,"TFTransfoXLLMHeadModel"),Rot.forEach(t),BUo=r(Ywe," (Transformer-XL model)"),Ywe.forEach(t),kUo=i(U),k2=n(U,"LI",{});var Kwe=s(k2);Tfe=n(Kwe,"STRONG",{});var Sot=s(Tfe);xUo=r(Sot,"xlm"),Sot.forEach(t),RUo=r(Kwe," \u2014 "),ND=n(Kwe,"A",{href:!0});var Pot=s(ND);SUo=r(Pot,"TFXLMWithLMHeadModel"),Pot.forEach(t),PUo=r(Kwe," (XLM model)"),Kwe.forEach(t),$Uo=i(U),x2=n(U,"LI",{});var Zwe=s(x2);Ffe=n(Zwe,"STRONG",{});var $ot=s(Ffe);IUo=r($ot,"xlm-roberta"),$ot.forEach(t),jUo=r(Zwe," \u2014 "),DD=n(Zwe,"A",{href:!0});var Iot=s(DD);NUo=r(Iot,"TFXLMRobertaForMaskedLM"),Iot.forEach(t),DUo=r(Zwe," (XLM-RoBERTa model)"),Zwe.forEach(t),qUo=i(U),R2=n(U,"LI",{});var eAe=s(R2);Cfe=n(eAe,"STRONG",{});var jot=s(Cfe);GUo=r(jot,"xlnet"),jot.forEach(t),OUo=r(eAe," \u2014 "),qD=n(eAe,"A",{href:!0});var Not=s(qD);XUo=r(Not,"TFXLNetLMHeadModel"),Not.forEach(t),zUo=r(eAe," (XLNet model)"),eAe.forEach(t),U.forEach(t),VUo=i(fa),Mfe=n(fa,"P",{});var Dot=s(Mfe);WUo=r(Dot,"Examples:"),Dot.forEach(t),QUo=i(fa),m(Ow.$$.fragment,fa),fa.forEach(t),xl.forEach(t),c9e=i(d),lc=n(d,"H2",{class:!0});var vke=s(lc);S2=n(vke,"A",{id:!0,class:!0,href:!0});var qot=s(S2);Efe=n(qot,"SPAN",{});var Got=s(Efe);m(Xw.$$.fragment,Got),Got.forEach(t),qot.forEach(t),HUo=i(vke),yfe=n(vke,"SPAN",{});var Oot=s(yfe);UUo=r(Oot,"TFAutoModelForCausalLM"),Oot.forEach(t),vke.forEach(t),f9e=i(d),pr=n(d,"DIV",{class:!0});var Sl=s(pr);m(zw.$$.fragment,Sl),JUo=i(Sl),ic=n(Sl,"P",{});var Xz=s(ic);YUo=r(Xz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a causal language modeling head) when created
with the `),wfe=n(Xz,"CODE",{});var Xot=s(wfe);KUo=r(Xot,"from_pretrained()"),Xot.forEach(t),ZUo=r(Xz,"class method or the "),Afe=n(Xz,"CODE",{});var zot=s(Afe);eJo=r(zot,"from_config()"),zot.forEach(t),oJo=r(Xz,`class
method.`),Xz.forEach(t),rJo=i(Sl),Vw=n(Sl,"P",{});var Tke=s(Vw);tJo=r(Tke,"This class cannot be instantiated directly using "),Lfe=n(Tke,"CODE",{});var Vot=s(Lfe);aJo=r(Vot,"__init__()"),Vot.forEach(t),nJo=r(Tke," (throws an error)."),Tke.forEach(t),sJo=i(Sl),it=n(Sl,"DIV",{class:!0});var Pl=s(it);m(Ww.$$.fragment,Pl),lJo=i(Pl),Bfe=n(Pl,"P",{});var Wot=s(Bfe);iJo=r(Wot,"Instantiates one of the model classes of the library (with a causal language modeling head) from a configuration."),Wot.forEach(t),dJo=i(Pl),dc=n(Pl,"P",{});var zz=s(dc);cJo=r(zz,`Note:
Loading a model from its configuration file does `),kfe=n(zz,"STRONG",{});var Qot=s(kfe);fJo=r(Qot,"not"),Qot.forEach(t),mJo=r(zz,` load the model weights. It only affects the
model\u2019s configuration. Use `),xfe=n(zz,"CODE",{});var Hot=s(xfe);gJo=r(Hot,"from_pretrained()"),Hot.forEach(t),hJo=r(zz,"to load the model weights."),zz.forEach(t),pJo=i(Pl),Rfe=n(Pl,"P",{});var Uot=s(Rfe);_Jo=r(Uot,"Examples:"),Uot.forEach(t),uJo=i(Pl),m(Qw.$$.fragment,Pl),Pl.forEach(t),bJo=i(Sl),po=n(Sl,"DIV",{class:!0});var ma=s(po);m(Hw.$$.fragment,ma),vJo=i(ma),Sfe=n(ma,"P",{});var Jot=s(Sfe);TJo=r(Jot,"Instantiate one of the model classes of the library (with a causal language modeling head) from a pretrained model."),Jot.forEach(t),FJo=i(ma),cn=n(ma,"P",{});var xM=s(cn);CJo=r(xM,"The model class to instantiate is selected based on the "),Pfe=n(xM,"CODE",{});var Yot=s(Pfe);MJo=r(Yot,"model_type"),Yot.forEach(t),EJo=r(xM,` property of the config object (either
passed as an argument or loaded from `),$fe=n(xM,"CODE",{});var Kot=s($fe);yJo=r(Kot,"pretrained_model_name_or_path"),Kot.forEach(t),wJo=r(xM,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Ife=n(xM,"CODE",{});var Zot=s(Ife);AJo=r(Zot,"pretrained_model_name_or_path"),Zot.forEach(t),LJo=r(xM,":"),xM.forEach(t),BJo=i(ma),he=n(ma,"UL",{});var Me=s(he);P2=n(Me,"LI",{});var oAe=s(P2);jfe=n(oAe,"STRONG",{});var ert=s(jfe);kJo=r(ert,"bert"),ert.forEach(t),xJo=r(oAe," \u2014 "),GD=n(oAe,"A",{href:!0});var ort=s(GD);RJo=r(ort,"TFBertLMHeadModel"),ort.forEach(t),SJo=r(oAe," (BERT model)"),oAe.forEach(t),PJo=i(Me),$2=n(Me,"LI",{});var rAe=s($2);Nfe=n(rAe,"STRONG",{});var rrt=s(Nfe);$Jo=r(rrt,"ctrl"),rrt.forEach(t),IJo=r(rAe," \u2014 "),OD=n(rAe,"A",{href:!0});var trt=s(OD);jJo=r(trt,"TFCTRLLMHeadModel"),trt.forEach(t),NJo=r(rAe," (CTRL model)"),rAe.forEach(t),DJo=i(Me),I2=n(Me,"LI",{});var tAe=s(I2);Dfe=n(tAe,"STRONG",{});var art=s(Dfe);qJo=r(art,"gpt2"),art.forEach(t),GJo=r(tAe," \u2014 "),XD=n(tAe,"A",{href:!0});var nrt=s(XD);OJo=r(nrt,"TFGPT2LMHeadModel"),nrt.forEach(t),XJo=r(tAe," (OpenAI GPT-2 model)"),tAe.forEach(t),zJo=i(Me),j2=n(Me,"LI",{});var aAe=s(j2);qfe=n(aAe,"STRONG",{});var srt=s(qfe);VJo=r(srt,"openai-gpt"),srt.forEach(t),WJo=r(aAe," \u2014 "),zD=n(aAe,"A",{href:!0});var lrt=s(zD);QJo=r(lrt,"TFOpenAIGPTLMHeadModel"),lrt.forEach(t),HJo=r(aAe," (OpenAI GPT model)"),aAe.forEach(t),UJo=i(Me),N2=n(Me,"LI",{});var nAe=s(N2);Gfe=n(nAe,"STRONG",{});var irt=s(Gfe);JJo=r(irt,"rembert"),irt.forEach(t),YJo=r(nAe," \u2014 "),VD=n(nAe,"A",{href:!0});var drt=s(VD);KJo=r(drt,"TFRemBertForCausalLM"),drt.forEach(t),ZJo=r(nAe," (RemBERT model)"),nAe.forEach(t),eYo=i(Me),D2=n(Me,"LI",{});var sAe=s(D2);Ofe=n(sAe,"STRONG",{});var crt=s(Ofe);oYo=r(crt,"roberta"),crt.forEach(t),rYo=r(sAe," \u2014 "),WD=n(sAe,"A",{href:!0});var frt=s(WD);tYo=r(frt,"TFRobertaForCausalLM"),frt.forEach(t),aYo=r(sAe," (RoBERTa model)"),sAe.forEach(t),nYo=i(Me),q2=n(Me,"LI",{});var lAe=s(q2);Xfe=n(lAe,"STRONG",{});var mrt=s(Xfe);sYo=r(mrt,"roformer"),mrt.forEach(t),lYo=r(lAe," \u2014 "),QD=n(lAe,"A",{href:!0});var grt=s(QD);iYo=r(grt,"TFRoFormerForCausalLM"),grt.forEach(t),dYo=r(lAe," (RoFormer model)"),lAe.forEach(t),cYo=i(Me),G2=n(Me,"LI",{});var iAe=s(G2);zfe=n(iAe,"STRONG",{});var hrt=s(zfe);fYo=r(hrt,"transfo-xl"),hrt.forEach(t),mYo=r(iAe," \u2014 "),HD=n(iAe,"A",{href:!0});var prt=s(HD);gYo=r(prt,"TFTransfoXLLMHeadModel"),prt.forEach(t),hYo=r(iAe," (Transformer-XL model)"),iAe.forEach(t),pYo=i(Me),O2=n(Me,"LI",{});var dAe=s(O2);Vfe=n(dAe,"STRONG",{});var _rt=s(Vfe);_Yo=r(_rt,"xlm"),_rt.forEach(t),uYo=r(dAe," \u2014 "),UD=n(dAe,"A",{href:!0});var urt=s(UD);bYo=r(urt,"TFXLMWithLMHeadModel"),urt.forEach(t),vYo=r(dAe," (XLM model)"),dAe.forEach(t),TYo=i(Me),X2=n(Me,"LI",{});var cAe=s(X2);Wfe=n(cAe,"STRONG",{});var brt=s(Wfe);FYo=r(brt,"xlnet"),brt.forEach(t),CYo=r(cAe," \u2014 "),JD=n(cAe,"A",{href:!0});var vrt=s(JD);MYo=r(vrt,"TFXLNetLMHeadModel"),vrt.forEach(t),EYo=r(cAe," (XLNet model)"),cAe.forEach(t),Me.forEach(t),yYo=i(ma),Qfe=n(ma,"P",{});var Trt=s(Qfe);wYo=r(Trt,"Examples:"),Trt.forEach(t),AYo=i(ma),m(Uw.$$.fragment,ma),ma.forEach(t),Sl.forEach(t),m9e=i(d),cc=n(d,"H2",{class:!0});var Fke=s(cc);z2=n(Fke,"A",{id:!0,class:!0,href:!0});var Frt=s(z2);Hfe=n(Frt,"SPAN",{});var Crt=s(Hfe);m(Jw.$$.fragment,Crt),Crt.forEach(t),Frt.forEach(t),LYo=i(Fke),Ufe=n(Fke,"SPAN",{});var Mrt=s(Ufe);BYo=r(Mrt,"TFAutoModelForImageClassification"),Mrt.forEach(t),Fke.forEach(t),g9e=i(d),_r=n(d,"DIV",{class:!0});var $l=s(_r);m(Yw.$$.fragment,$l),kYo=i($l),fc=n($l,"P",{});var Vz=s(fc);xYo=r(Vz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a image classification head) when created
with the `),Jfe=n(Vz,"CODE",{});var Ert=s(Jfe);RYo=r(Ert,"from_pretrained()"),Ert.forEach(t),SYo=r(Vz,"class method or the "),Yfe=n(Vz,"CODE",{});var yrt=s(Yfe);PYo=r(yrt,"from_config()"),yrt.forEach(t),$Yo=r(Vz,`class
method.`),Vz.forEach(t),IYo=i($l),Kw=n($l,"P",{});var Cke=s(Kw);jYo=r(Cke,"This class cannot be instantiated directly using "),Kfe=n(Cke,"CODE",{});var wrt=s(Kfe);NYo=r(wrt,"__init__()"),wrt.forEach(t),DYo=r(Cke," (throws an error)."),Cke.forEach(t),qYo=i($l),dt=n($l,"DIV",{class:!0});var Il=s(dt);m(Zw.$$.fragment,Il),GYo=i(Il),Zfe=n(Il,"P",{});var Art=s(Zfe);OYo=r(Art,"Instantiates one of the model classes of the library (with a image classification head) from a configuration."),Art.forEach(t),XYo=i(Il),mc=n(Il,"P",{});var Wz=s(mc);zYo=r(Wz,`Note:
Loading a model from its configuration file does `),eme=n(Wz,"STRONG",{});var Lrt=s(eme);VYo=r(Lrt,"not"),Lrt.forEach(t),WYo=r(Wz,` load the model weights. It only affects the
model\u2019s configuration. Use `),ome=n(Wz,"CODE",{});var Brt=s(ome);QYo=r(Brt,"from_pretrained()"),Brt.forEach(t),HYo=r(Wz,"to load the model weights."),Wz.forEach(t),UYo=i(Il),rme=n(Il,"P",{});var krt=s(rme);JYo=r(krt,"Examples:"),krt.forEach(t),YYo=i(Il),m(eA.$$.fragment,Il),Il.forEach(t),KYo=i($l),_o=n($l,"DIV",{class:!0});var ga=s(_o);m(oA.$$.fragment,ga),ZYo=i(ga),tme=n(ga,"P",{});var xrt=s(tme);eKo=r(xrt,"Instantiate one of the model classes of the library (with a image classification head) from a pretrained model."),xrt.forEach(t),oKo=i(ga),fn=n(ga,"P",{});var RM=s(fn);rKo=r(RM,"The model class to instantiate is selected based on the "),ame=n(RM,"CODE",{});var Rrt=s(ame);tKo=r(Rrt,"model_type"),Rrt.forEach(t),aKo=r(RM,` property of the config object (either
passed as an argument or loaded from `),nme=n(RM,"CODE",{});var Srt=s(nme);nKo=r(Srt,"pretrained_model_name_or_path"),Srt.forEach(t),sKo=r(RM,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),sme=n(RM,"CODE",{});var Prt=s(sme);lKo=r(Prt,"pretrained_model_name_or_path"),Prt.forEach(t),iKo=r(RM,":"),RM.forEach(t),dKo=i(ga),lme=n(ga,"UL",{});var $rt=s(lme);V2=n($rt,"LI",{});var fAe=s(V2);ime=n(fAe,"STRONG",{});var Irt=s(ime);cKo=r(Irt,"vit"),Irt.forEach(t),fKo=r(fAe," \u2014 "),YD=n(fAe,"A",{href:!0});var jrt=s(YD);mKo=r(jrt,"TFViTForImageClassification"),jrt.forEach(t),gKo=r(fAe," (ViT model)"),fAe.forEach(t),$rt.forEach(t),hKo=i(ga),dme=n(ga,"P",{});var Nrt=s(dme);pKo=r(Nrt,"Examples:"),Nrt.forEach(t),_Ko=i(ga),m(rA.$$.fragment,ga),ga.forEach(t),$l.forEach(t),h9e=i(d),gc=n(d,"H2",{class:!0});var Mke=s(gc);W2=n(Mke,"A",{id:!0,class:!0,href:!0});var Drt=s(W2);cme=n(Drt,"SPAN",{});var qrt=s(cme);m(tA.$$.fragment,qrt),qrt.forEach(t),Drt.forEach(t),uKo=i(Mke),fme=n(Mke,"SPAN",{});var Grt=s(fme);bKo=r(Grt,"TFAutoModelForMaskedLM"),Grt.forEach(t),Mke.forEach(t),p9e=i(d),ur=n(d,"DIV",{class:!0});var jl=s(ur);m(aA.$$.fragment,jl),vKo=i(jl),hc=n(jl,"P",{});var Qz=s(hc);TKo=r(Qz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a masked language modeling head) when created
with the `),mme=n(Qz,"CODE",{});var Ort=s(mme);FKo=r(Ort,"from_pretrained()"),Ort.forEach(t),CKo=r(Qz,"class method or the "),gme=n(Qz,"CODE",{});var Xrt=s(gme);MKo=r(Xrt,"from_config()"),Xrt.forEach(t),EKo=r(Qz,`class
method.`),Qz.forEach(t),yKo=i(jl),nA=n(jl,"P",{});var Eke=s(nA);wKo=r(Eke,"This class cannot be instantiated directly using "),hme=n(Eke,"CODE",{});var zrt=s(hme);AKo=r(zrt,"__init__()"),zrt.forEach(t),LKo=r(Eke," (throws an error)."),Eke.forEach(t),BKo=i(jl),ct=n(jl,"DIV",{class:!0});var Nl=s(ct);m(sA.$$.fragment,Nl),kKo=i(Nl),pme=n(Nl,"P",{});var Vrt=s(pme);xKo=r(Vrt,"Instantiates one of the model classes of the library (with a masked language modeling head) from a configuration."),Vrt.forEach(t),RKo=i(Nl),pc=n(Nl,"P",{});var Hz=s(pc);SKo=r(Hz,`Note:
Loading a model from its configuration file does `),_me=n(Hz,"STRONG",{});var Wrt=s(_me);PKo=r(Wrt,"not"),Wrt.forEach(t),$Ko=r(Hz,` load the model weights. It only affects the
model\u2019s configuration. Use `),ume=n(Hz,"CODE",{});var Qrt=s(ume);IKo=r(Qrt,"from_pretrained()"),Qrt.forEach(t),jKo=r(Hz,"to load the model weights."),Hz.forEach(t),NKo=i(Nl),bme=n(Nl,"P",{});var Hrt=s(bme);DKo=r(Hrt,"Examples:"),Hrt.forEach(t),qKo=i(Nl),m(lA.$$.fragment,Nl),Nl.forEach(t),GKo=i(jl),uo=n(jl,"DIV",{class:!0});var ha=s(uo);m(iA.$$.fragment,ha),OKo=i(ha),vme=n(ha,"P",{});var Urt=s(vme);XKo=r(Urt,"Instantiate one of the model classes of the library (with a masked language modeling head) from a pretrained model."),Urt.forEach(t),zKo=i(ha),mn=n(ha,"P",{});var SM=s(mn);VKo=r(SM,"The model class to instantiate is selected based on the "),Tme=n(SM,"CODE",{});var Jrt=s(Tme);WKo=r(Jrt,"model_type"),Jrt.forEach(t),QKo=r(SM,` property of the config object (either
passed as an argument or loaded from `),Fme=n(SM,"CODE",{});var Yrt=s(Fme);HKo=r(Yrt,"pretrained_model_name_or_path"),Yrt.forEach(t),UKo=r(SM,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Cme=n(SM,"CODE",{});var Krt=s(Cme);JKo=r(Krt,"pretrained_model_name_or_path"),Krt.forEach(t),YKo=r(SM,":"),SM.forEach(t),KKo=i(ha),Y=n(ha,"UL",{});var ee=s(Y);Q2=n(ee,"LI",{});var mAe=s(Q2);Mme=n(mAe,"STRONG",{});var Zrt=s(Mme);ZKo=r(Zrt,"albert"),Zrt.forEach(t),eZo=r(mAe," \u2014 "),KD=n(mAe,"A",{href:!0});var ett=s(KD);oZo=r(ett,"TFAlbertForMaskedLM"),ett.forEach(t),rZo=r(mAe," (ALBERT model)"),mAe.forEach(t),tZo=i(ee),H2=n(ee,"LI",{});var gAe=s(H2);Eme=n(gAe,"STRONG",{});var ott=s(Eme);aZo=r(ott,"bert"),ott.forEach(t),nZo=r(gAe," \u2014 "),ZD=n(gAe,"A",{href:!0});var rtt=s(ZD);sZo=r(rtt,"TFBertForMaskedLM"),rtt.forEach(t),lZo=r(gAe," (BERT model)"),gAe.forEach(t),iZo=i(ee),U2=n(ee,"LI",{});var hAe=s(U2);yme=n(hAe,"STRONG",{});var ttt=s(yme);dZo=r(ttt,"camembert"),ttt.forEach(t),cZo=r(hAe," \u2014 "),eq=n(hAe,"A",{href:!0});var att=s(eq);fZo=r(att,"TFCamembertForMaskedLM"),att.forEach(t),mZo=r(hAe," (CamemBERT model)"),hAe.forEach(t),gZo=i(ee),J2=n(ee,"LI",{});var pAe=s(J2);wme=n(pAe,"STRONG",{});var ntt=s(wme);hZo=r(ntt,"convbert"),ntt.forEach(t),pZo=r(pAe," \u2014 "),oq=n(pAe,"A",{href:!0});var stt=s(oq);_Zo=r(stt,"TFConvBertForMaskedLM"),stt.forEach(t),uZo=r(pAe," (ConvBERT model)"),pAe.forEach(t),bZo=i(ee),Y2=n(ee,"LI",{});var _Ae=s(Y2);Ame=n(_Ae,"STRONG",{});var ltt=s(Ame);vZo=r(ltt,"deberta"),ltt.forEach(t),TZo=r(_Ae," \u2014 "),rq=n(_Ae,"A",{href:!0});var itt=s(rq);FZo=r(itt,"TFDebertaForMaskedLM"),itt.forEach(t),CZo=r(_Ae," (DeBERTa model)"),_Ae.forEach(t),MZo=i(ee),K2=n(ee,"LI",{});var uAe=s(K2);Lme=n(uAe,"STRONG",{});var dtt=s(Lme);EZo=r(dtt,"deberta-v2"),dtt.forEach(t),yZo=r(uAe," \u2014 "),tq=n(uAe,"A",{href:!0});var ctt=s(tq);wZo=r(ctt,"TFDebertaV2ForMaskedLM"),ctt.forEach(t),AZo=r(uAe," (DeBERTa-v2 model)"),uAe.forEach(t),LZo=i(ee),Z2=n(ee,"LI",{});var bAe=s(Z2);Bme=n(bAe,"STRONG",{});var ftt=s(Bme);BZo=r(ftt,"distilbert"),ftt.forEach(t),kZo=r(bAe," \u2014 "),aq=n(bAe,"A",{href:!0});var mtt=s(aq);xZo=r(mtt,"TFDistilBertForMaskedLM"),mtt.forEach(t),RZo=r(bAe," (DistilBERT model)"),bAe.forEach(t),SZo=i(ee),ev=n(ee,"LI",{});var vAe=s(ev);kme=n(vAe,"STRONG",{});var gtt=s(kme);PZo=r(gtt,"electra"),gtt.forEach(t),$Zo=r(vAe," \u2014 "),nq=n(vAe,"A",{href:!0});var htt=s(nq);IZo=r(htt,"TFElectraForMaskedLM"),htt.forEach(t),jZo=r(vAe," (ELECTRA model)"),vAe.forEach(t),NZo=i(ee),ov=n(ee,"LI",{});var TAe=s(ov);xme=n(TAe,"STRONG",{});var ptt=s(xme);DZo=r(ptt,"flaubert"),ptt.forEach(t),qZo=r(TAe," \u2014 "),sq=n(TAe,"A",{href:!0});var _tt=s(sq);GZo=r(_tt,"TFFlaubertWithLMHeadModel"),_tt.forEach(t),OZo=r(TAe," (FlauBERT model)"),TAe.forEach(t),XZo=i(ee),rv=n(ee,"LI",{});var FAe=s(rv);Rme=n(FAe,"STRONG",{});var utt=s(Rme);zZo=r(utt,"funnel"),utt.forEach(t),VZo=r(FAe," \u2014 "),lq=n(FAe,"A",{href:!0});var btt=s(lq);WZo=r(btt,"TFFunnelForMaskedLM"),btt.forEach(t),QZo=r(FAe," (Funnel Transformer model)"),FAe.forEach(t),HZo=i(ee),tv=n(ee,"LI",{});var CAe=s(tv);Sme=n(CAe,"STRONG",{});var vtt=s(Sme);UZo=r(vtt,"layoutlm"),vtt.forEach(t),JZo=r(CAe," \u2014 "),iq=n(CAe,"A",{href:!0});var Ttt=s(iq);YZo=r(Ttt,"TFLayoutLMForMaskedLM"),Ttt.forEach(t),KZo=r(CAe," (LayoutLM model)"),CAe.forEach(t),ZZo=i(ee),av=n(ee,"LI",{});var MAe=s(av);Pme=n(MAe,"STRONG",{});var Ftt=s(Pme);eer=r(Ftt,"longformer"),Ftt.forEach(t),oer=r(MAe," \u2014 "),dq=n(MAe,"A",{href:!0});var Ctt=s(dq);rer=r(Ctt,"TFLongformerForMaskedLM"),Ctt.forEach(t),ter=r(MAe," (Longformer model)"),MAe.forEach(t),aer=i(ee),nv=n(ee,"LI",{});var EAe=s(nv);$me=n(EAe,"STRONG",{});var Mtt=s($me);ner=r(Mtt,"mobilebert"),Mtt.forEach(t),ser=r(EAe," \u2014 "),cq=n(EAe,"A",{href:!0});var Ett=s(cq);ler=r(Ett,"TFMobileBertForMaskedLM"),Ett.forEach(t),ier=r(EAe," (MobileBERT model)"),EAe.forEach(t),der=i(ee),sv=n(ee,"LI",{});var yAe=s(sv);Ime=n(yAe,"STRONG",{});var ytt=s(Ime);cer=r(ytt,"mpnet"),ytt.forEach(t),fer=r(yAe," \u2014 "),fq=n(yAe,"A",{href:!0});var wtt=s(fq);mer=r(wtt,"TFMPNetForMaskedLM"),wtt.forEach(t),ger=r(yAe," (MPNet model)"),yAe.forEach(t),her=i(ee),lv=n(ee,"LI",{});var wAe=s(lv);jme=n(wAe,"STRONG",{});var Att=s(jme);per=r(Att,"rembert"),Att.forEach(t),_er=r(wAe," \u2014 "),mq=n(wAe,"A",{href:!0});var Ltt=s(mq);uer=r(Ltt,"TFRemBertForMaskedLM"),Ltt.forEach(t),ber=r(wAe," (RemBERT model)"),wAe.forEach(t),ver=i(ee),iv=n(ee,"LI",{});var AAe=s(iv);Nme=n(AAe,"STRONG",{});var Btt=s(Nme);Ter=r(Btt,"roberta"),Btt.forEach(t),Fer=r(AAe," \u2014 "),gq=n(AAe,"A",{href:!0});var ktt=s(gq);Cer=r(ktt,"TFRobertaForMaskedLM"),ktt.forEach(t),Mer=r(AAe," (RoBERTa model)"),AAe.forEach(t),Eer=i(ee),dv=n(ee,"LI",{});var LAe=s(dv);Dme=n(LAe,"STRONG",{});var xtt=s(Dme);yer=r(xtt,"roformer"),xtt.forEach(t),wer=r(LAe," \u2014 "),hq=n(LAe,"A",{href:!0});var Rtt=s(hq);Aer=r(Rtt,"TFRoFormerForMaskedLM"),Rtt.forEach(t),Ler=r(LAe," (RoFormer model)"),LAe.forEach(t),Ber=i(ee),cv=n(ee,"LI",{});var BAe=s(cv);qme=n(BAe,"STRONG",{});var Stt=s(qme);ker=r(Stt,"tapas"),Stt.forEach(t),xer=r(BAe," \u2014 "),pq=n(BAe,"A",{href:!0});var Ptt=s(pq);Rer=r(Ptt,"TFTapasForMaskedLM"),Ptt.forEach(t),Ser=r(BAe," (TAPAS model)"),BAe.forEach(t),Per=i(ee),fv=n(ee,"LI",{});var kAe=s(fv);Gme=n(kAe,"STRONG",{});var $tt=s(Gme);$er=r($tt,"xlm"),$tt.forEach(t),Ier=r(kAe," \u2014 "),_q=n(kAe,"A",{href:!0});var Itt=s(_q);jer=r(Itt,"TFXLMWithLMHeadModel"),Itt.forEach(t),Ner=r(kAe," (XLM model)"),kAe.forEach(t),Der=i(ee),mv=n(ee,"LI",{});var xAe=s(mv);Ome=n(xAe,"STRONG",{});var jtt=s(Ome);qer=r(jtt,"xlm-roberta"),jtt.forEach(t),Ger=r(xAe," \u2014 "),uq=n(xAe,"A",{href:!0});var Ntt=s(uq);Oer=r(Ntt,"TFXLMRobertaForMaskedLM"),Ntt.forEach(t),Xer=r(xAe," (XLM-RoBERTa model)"),xAe.forEach(t),ee.forEach(t),zer=i(ha),Xme=n(ha,"P",{});var Dtt=s(Xme);Ver=r(Dtt,"Examples:"),Dtt.forEach(t),Wer=i(ha),m(dA.$$.fragment,ha),ha.forEach(t),jl.forEach(t),_9e=i(d),_c=n(d,"H2",{class:!0});var yke=s(_c);gv=n(yke,"A",{id:!0,class:!0,href:!0});var qtt=s(gv);zme=n(qtt,"SPAN",{});var Gtt=s(zme);m(cA.$$.fragment,Gtt),Gtt.forEach(t),qtt.forEach(t),Qer=i(yke),Vme=n(yke,"SPAN",{});var Ott=s(Vme);Her=r(Ott,"TFAutoModelForSeq2SeqLM"),Ott.forEach(t),yke.forEach(t),u9e=i(d),br=n(d,"DIV",{class:!0});var Dl=s(br);m(fA.$$.fragment,Dl),Uer=i(Dl),uc=n(Dl,"P",{});var Uz=s(uc);Jer=r(Uz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence language modeling head) when created
with the `),Wme=n(Uz,"CODE",{});var Xtt=s(Wme);Yer=r(Xtt,"from_pretrained()"),Xtt.forEach(t),Ker=r(Uz,"class method or the "),Qme=n(Uz,"CODE",{});var ztt=s(Qme);Zer=r(ztt,"from_config()"),ztt.forEach(t),eor=r(Uz,`class
method.`),Uz.forEach(t),oor=i(Dl),mA=n(Dl,"P",{});var wke=s(mA);ror=r(wke,"This class cannot be instantiated directly using "),Hme=n(wke,"CODE",{});var Vtt=s(Hme);tor=r(Vtt,"__init__()"),Vtt.forEach(t),aor=r(wke," (throws an error)."),wke.forEach(t),nor=i(Dl),ft=n(Dl,"DIV",{class:!0});var ql=s(ft);m(gA.$$.fragment,ql),sor=i(ql),Ume=n(ql,"P",{});var Wtt=s(Ume);lor=r(Wtt,"Instantiates one of the model classes of the library (with a sequence-to-sequence language modeling head) from a configuration."),Wtt.forEach(t),ior=i(ql),bc=n(ql,"P",{});var Jz=s(bc);dor=r(Jz,`Note:
Loading a model from its configuration file does `),Jme=n(Jz,"STRONG",{});var Qtt=s(Jme);cor=r(Qtt,"not"),Qtt.forEach(t),mor=r(Jz,` load the model weights. It only affects the
model\u2019s configuration. Use `),Yme=n(Jz,"CODE",{});var Htt=s(Yme);gor=r(Htt,"from_pretrained()"),Htt.forEach(t),hor=r(Jz,"to load the model weights."),Jz.forEach(t),por=i(ql),Kme=n(ql,"P",{});var Utt=s(Kme);_or=r(Utt,"Examples:"),Utt.forEach(t),uor=i(ql),m(hA.$$.fragment,ql),ql.forEach(t),bor=i(Dl),bo=n(Dl,"DIV",{class:!0});var pa=s(bo);m(pA.$$.fragment,pa),vor=i(pa),Zme=n(pa,"P",{});var Jtt=s(Zme);Tor=r(Jtt,"Instantiate one of the model classes of the library (with a sequence-to-sequence language modeling head) from a pretrained model."),Jtt.forEach(t),For=i(pa),gn=n(pa,"P",{});var PM=s(gn);Cor=r(PM,"The model class to instantiate is selected based on the "),ege=n(PM,"CODE",{});var Ytt=s(ege);Mor=r(Ytt,"model_type"),Ytt.forEach(t),Eor=r(PM,` property of the config object (either
passed as an argument or loaded from `),oge=n(PM,"CODE",{});var Ktt=s(oge);yor=r(Ktt,"pretrained_model_name_or_path"),Ktt.forEach(t),wor=r(PM,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),rge=n(PM,"CODE",{});var Ztt=s(rge);Aor=r(Ztt,"pretrained_model_name_or_path"),Ztt.forEach(t),Lor=r(PM,":"),PM.forEach(t),Bor=i(pa),pe=n(pa,"UL",{});var Ee=s(pe);hv=n(Ee,"LI",{});var RAe=s(hv);tge=n(RAe,"STRONG",{});var eat=s(tge);kor=r(eat,"bart"),eat.forEach(t),xor=r(RAe," \u2014 "),bq=n(RAe,"A",{href:!0});var oat=s(bq);Ror=r(oat,"TFBartForConditionalGeneration"),oat.forEach(t),Sor=r(RAe," (BART model)"),RAe.forEach(t),Por=i(Ee),pv=n(Ee,"LI",{});var SAe=s(pv);age=n(SAe,"STRONG",{});var rat=s(age);$or=r(rat,"blenderbot"),rat.forEach(t),Ior=r(SAe," \u2014 "),vq=n(SAe,"A",{href:!0});var tat=s(vq);jor=r(tat,"TFBlenderbotForConditionalGeneration"),tat.forEach(t),Nor=r(SAe," (Blenderbot model)"),SAe.forEach(t),Dor=i(Ee),_v=n(Ee,"LI",{});var PAe=s(_v);nge=n(PAe,"STRONG",{});var aat=s(nge);qor=r(aat,"blenderbot-small"),aat.forEach(t),Gor=r(PAe," \u2014 "),Tq=n(PAe,"A",{href:!0});var nat=s(Tq);Oor=r(nat,"TFBlenderbotSmallForConditionalGeneration"),nat.forEach(t),Xor=r(PAe," (BlenderbotSmall model)"),PAe.forEach(t),zor=i(Ee),uv=n(Ee,"LI",{});var $Ae=s(uv);sge=n($Ae,"STRONG",{});var sat=s(sge);Vor=r(sat,"encoder-decoder"),sat.forEach(t),Wor=r($Ae," \u2014 "),Fq=n($Ae,"A",{href:!0});var lat=s(Fq);Qor=r(lat,"TFEncoderDecoderModel"),lat.forEach(t),Hor=r($Ae," (Encoder decoder model)"),$Ae.forEach(t),Uor=i(Ee),bv=n(Ee,"LI",{});var IAe=s(bv);lge=n(IAe,"STRONG",{});var iat=s(lge);Jor=r(iat,"led"),iat.forEach(t),Yor=r(IAe," \u2014 "),Cq=n(IAe,"A",{href:!0});var dat=s(Cq);Kor=r(dat,"TFLEDForConditionalGeneration"),dat.forEach(t),Zor=r(IAe," (LED model)"),IAe.forEach(t),err=i(Ee),vv=n(Ee,"LI",{});var jAe=s(vv);ige=n(jAe,"STRONG",{});var cat=s(ige);orr=r(cat,"marian"),cat.forEach(t),rrr=r(jAe," \u2014 "),Mq=n(jAe,"A",{href:!0});var fat=s(Mq);trr=r(fat,"TFMarianMTModel"),fat.forEach(t),arr=r(jAe," (Marian model)"),jAe.forEach(t),nrr=i(Ee),Tv=n(Ee,"LI",{});var NAe=s(Tv);dge=n(NAe,"STRONG",{});var mat=s(dge);srr=r(mat,"mbart"),mat.forEach(t),lrr=r(NAe," \u2014 "),Eq=n(NAe,"A",{href:!0});var gat=s(Eq);irr=r(gat,"TFMBartForConditionalGeneration"),gat.forEach(t),drr=r(NAe," (mBART model)"),NAe.forEach(t),crr=i(Ee),Fv=n(Ee,"LI",{});var DAe=s(Fv);cge=n(DAe,"STRONG",{});var hat=s(cge);frr=r(hat,"mt5"),hat.forEach(t),mrr=r(DAe," \u2014 "),yq=n(DAe,"A",{href:!0});var pat=s(yq);grr=r(pat,"TFMT5ForConditionalGeneration"),pat.forEach(t),hrr=r(DAe," (mT5 model)"),DAe.forEach(t),prr=i(Ee),Cv=n(Ee,"LI",{});var qAe=s(Cv);fge=n(qAe,"STRONG",{});var _at=s(fge);_rr=r(_at,"pegasus"),_at.forEach(t),urr=r(qAe," \u2014 "),wq=n(qAe,"A",{href:!0});var uat=s(wq);brr=r(uat,"TFPegasusForConditionalGeneration"),uat.forEach(t),vrr=r(qAe," (Pegasus model)"),qAe.forEach(t),Trr=i(Ee),Mv=n(Ee,"LI",{});var GAe=s(Mv);mge=n(GAe,"STRONG",{});var bat=s(mge);Frr=r(bat,"t5"),bat.forEach(t),Crr=r(GAe," \u2014 "),Aq=n(GAe,"A",{href:!0});var vat=s(Aq);Mrr=r(vat,"TFT5ForConditionalGeneration"),vat.forEach(t),Err=r(GAe," (T5 model)"),GAe.forEach(t),Ee.forEach(t),yrr=i(pa),gge=n(pa,"P",{});var Tat=s(gge);wrr=r(Tat,"Examples:"),Tat.forEach(t),Arr=i(pa),m(_A.$$.fragment,pa),pa.forEach(t),Dl.forEach(t),b9e=i(d),vc=n(d,"H2",{class:!0});var Ake=s(vc);Ev=n(Ake,"A",{id:!0,class:!0,href:!0});var Fat=s(Ev);hge=n(Fat,"SPAN",{});var Cat=s(hge);m(uA.$$.fragment,Cat),Cat.forEach(t),Fat.forEach(t),Lrr=i(Ake),pge=n(Ake,"SPAN",{});var Mat=s(pge);Brr=r(Mat,"TFAutoModelForSequenceClassification"),Mat.forEach(t),Ake.forEach(t),v9e=i(d),vr=n(d,"DIV",{class:!0});var Gl=s(vr);m(bA.$$.fragment,Gl),krr=i(Gl),Tc=n(Gl,"P",{});var Yz=s(Tc);xrr=r(Yz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence classification head) when created
with the `),_ge=n(Yz,"CODE",{});var Eat=s(_ge);Rrr=r(Eat,"from_pretrained()"),Eat.forEach(t),Srr=r(Yz,"class method or the "),uge=n(Yz,"CODE",{});var yat=s(uge);Prr=r(yat,"from_config()"),yat.forEach(t),$rr=r(Yz,`class
method.`),Yz.forEach(t),Irr=i(Gl),vA=n(Gl,"P",{});var Lke=s(vA);jrr=r(Lke,"This class cannot be instantiated directly using "),bge=n(Lke,"CODE",{});var wat=s(bge);Nrr=r(wat,"__init__()"),wat.forEach(t),Drr=r(Lke," (throws an error)."),Lke.forEach(t),qrr=i(Gl),mt=n(Gl,"DIV",{class:!0});var Ol=s(mt);m(TA.$$.fragment,Ol),Grr=i(Ol),vge=n(Ol,"P",{});var Aat=s(vge);Orr=r(Aat,"Instantiates one of the model classes of the library (with a sequence classification head) from a configuration."),Aat.forEach(t),Xrr=i(Ol),Fc=n(Ol,"P",{});var Kz=s(Fc);zrr=r(Kz,`Note:
Loading a model from its configuration file does `),Tge=n(Kz,"STRONG",{});var Lat=s(Tge);Vrr=r(Lat,"not"),Lat.forEach(t),Wrr=r(Kz,` load the model weights. It only affects the
model\u2019s configuration. Use `),Fge=n(Kz,"CODE",{});var Bat=s(Fge);Qrr=r(Bat,"from_pretrained()"),Bat.forEach(t),Hrr=r(Kz,"to load the model weights."),Kz.forEach(t),Urr=i(Ol),Cge=n(Ol,"P",{});var kat=s(Cge);Jrr=r(kat,"Examples:"),kat.forEach(t),Yrr=i(Ol),m(FA.$$.fragment,Ol),Ol.forEach(t),Krr=i(Gl),vo=n(Gl,"DIV",{class:!0});var _a=s(vo);m(CA.$$.fragment,_a),Zrr=i(_a),Mge=n(_a,"P",{});var xat=s(Mge);etr=r(xat,"Instantiate one of the model classes of the library (with a sequence classification head) from a pretrained model."),xat.forEach(t),otr=i(_a),hn=n(_a,"P",{});var $M=s(hn);rtr=r($M,"The model class to instantiate is selected based on the "),Ege=n($M,"CODE",{});var Rat=s(Ege);ttr=r(Rat,"model_type"),Rat.forEach(t),atr=r($M,` property of the config object (either
passed as an argument or loaded from `),yge=n($M,"CODE",{});var Sat=s(yge);ntr=r(Sat,"pretrained_model_name_or_path"),Sat.forEach(t),str=r($M,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),wge=n($M,"CODE",{});var Pat=s(wge);ltr=r(Pat,"pretrained_model_name_or_path"),Pat.forEach(t),itr=r($M,":"),$M.forEach(t),dtr=i(_a),X=n(_a,"UL",{});var W=s(X);yv=n(W,"LI",{});var OAe=s(yv);Age=n(OAe,"STRONG",{});var $at=s(Age);ctr=r($at,"albert"),$at.forEach(t),ftr=r(OAe," \u2014 "),Lq=n(OAe,"A",{href:!0});var Iat=s(Lq);mtr=r(Iat,"TFAlbertForSequenceClassification"),Iat.forEach(t),gtr=r(OAe," (ALBERT model)"),OAe.forEach(t),htr=i(W),wv=n(W,"LI",{});var XAe=s(wv);Lge=n(XAe,"STRONG",{});var jat=s(Lge);ptr=r(jat,"bert"),jat.forEach(t),_tr=r(XAe," \u2014 "),Bq=n(XAe,"A",{href:!0});var Nat=s(Bq);utr=r(Nat,"TFBertForSequenceClassification"),Nat.forEach(t),btr=r(XAe," (BERT model)"),XAe.forEach(t),vtr=i(W),Av=n(W,"LI",{});var zAe=s(Av);Bge=n(zAe,"STRONG",{});var Dat=s(Bge);Ttr=r(Dat,"camembert"),Dat.forEach(t),Ftr=r(zAe," \u2014 "),kq=n(zAe,"A",{href:!0});var qat=s(kq);Ctr=r(qat,"TFCamembertForSequenceClassification"),qat.forEach(t),Mtr=r(zAe," (CamemBERT model)"),zAe.forEach(t),Etr=i(W),Lv=n(W,"LI",{});var VAe=s(Lv);kge=n(VAe,"STRONG",{});var Gat=s(kge);ytr=r(Gat,"convbert"),Gat.forEach(t),wtr=r(VAe," \u2014 "),xq=n(VAe,"A",{href:!0});var Oat=s(xq);Atr=r(Oat,"TFConvBertForSequenceClassification"),Oat.forEach(t),Ltr=r(VAe," (ConvBERT model)"),VAe.forEach(t),Btr=i(W),Bv=n(W,"LI",{});var WAe=s(Bv);xge=n(WAe,"STRONG",{});var Xat=s(xge);ktr=r(Xat,"ctrl"),Xat.forEach(t),xtr=r(WAe," \u2014 "),Rq=n(WAe,"A",{href:!0});var zat=s(Rq);Rtr=r(zat,"TFCTRLForSequenceClassification"),zat.forEach(t),Str=r(WAe," (CTRL model)"),WAe.forEach(t),Ptr=i(W),kv=n(W,"LI",{});var QAe=s(kv);Rge=n(QAe,"STRONG",{});var Vat=s(Rge);$tr=r(Vat,"deberta"),Vat.forEach(t),Itr=r(QAe," \u2014 "),Sq=n(QAe,"A",{href:!0});var Wat=s(Sq);jtr=r(Wat,"TFDebertaForSequenceClassification"),Wat.forEach(t),Ntr=r(QAe," (DeBERTa model)"),QAe.forEach(t),Dtr=i(W),xv=n(W,"LI",{});var HAe=s(xv);Sge=n(HAe,"STRONG",{});var Qat=s(Sge);qtr=r(Qat,"deberta-v2"),Qat.forEach(t),Gtr=r(HAe," \u2014 "),Pq=n(HAe,"A",{href:!0});var Hat=s(Pq);Otr=r(Hat,"TFDebertaV2ForSequenceClassification"),Hat.forEach(t),Xtr=r(HAe," (DeBERTa-v2 model)"),HAe.forEach(t),ztr=i(W),Rv=n(W,"LI",{});var UAe=s(Rv);Pge=n(UAe,"STRONG",{});var Uat=s(Pge);Vtr=r(Uat,"distilbert"),Uat.forEach(t),Wtr=r(UAe," \u2014 "),$q=n(UAe,"A",{href:!0});var Jat=s($q);Qtr=r(Jat,"TFDistilBertForSequenceClassification"),Jat.forEach(t),Htr=r(UAe," (DistilBERT model)"),UAe.forEach(t),Utr=i(W),Sv=n(W,"LI",{});var JAe=s(Sv);$ge=n(JAe,"STRONG",{});var Yat=s($ge);Jtr=r(Yat,"electra"),Yat.forEach(t),Ytr=r(JAe," \u2014 "),Iq=n(JAe,"A",{href:!0});var Kat=s(Iq);Ktr=r(Kat,"TFElectraForSequenceClassification"),Kat.forEach(t),Ztr=r(JAe," (ELECTRA model)"),JAe.forEach(t),ear=i(W),Pv=n(W,"LI",{});var YAe=s(Pv);Ige=n(YAe,"STRONG",{});var Zat=s(Ige);oar=r(Zat,"flaubert"),Zat.forEach(t),rar=r(YAe," \u2014 "),jq=n(YAe,"A",{href:!0});var ent=s(jq);tar=r(ent,"TFFlaubertForSequenceClassification"),ent.forEach(t),aar=r(YAe," (FlauBERT model)"),YAe.forEach(t),nar=i(W),$v=n(W,"LI",{});var KAe=s($v);jge=n(KAe,"STRONG",{});var ont=s(jge);sar=r(ont,"funnel"),ont.forEach(t),lar=r(KAe," \u2014 "),Nq=n(KAe,"A",{href:!0});var rnt=s(Nq);iar=r(rnt,"TFFunnelForSequenceClassification"),rnt.forEach(t),dar=r(KAe," (Funnel Transformer model)"),KAe.forEach(t),car=i(W),Iv=n(W,"LI",{});var ZAe=s(Iv);Nge=n(ZAe,"STRONG",{});var tnt=s(Nge);far=r(tnt,"gpt2"),tnt.forEach(t),mar=r(ZAe," \u2014 "),Dq=n(ZAe,"A",{href:!0});var ant=s(Dq);gar=r(ant,"TFGPT2ForSequenceClassification"),ant.forEach(t),har=r(ZAe," (OpenAI GPT-2 model)"),ZAe.forEach(t),par=i(W),jv=n(W,"LI",{});var e6e=s(jv);Dge=n(e6e,"STRONG",{});var nnt=s(Dge);_ar=r(nnt,"layoutlm"),nnt.forEach(t),uar=r(e6e," \u2014 "),qq=n(e6e,"A",{href:!0});var snt=s(qq);bar=r(snt,"TFLayoutLMForSequenceClassification"),snt.forEach(t),Tar=r(e6e," (LayoutLM model)"),e6e.forEach(t),Far=i(W),Nv=n(W,"LI",{});var o6e=s(Nv);qge=n(o6e,"STRONG",{});var lnt=s(qge);Car=r(lnt,"longformer"),lnt.forEach(t),Mar=r(o6e," \u2014 "),Gq=n(o6e,"A",{href:!0});var int=s(Gq);Ear=r(int,"TFLongformerForSequenceClassification"),int.forEach(t),yar=r(o6e," (Longformer model)"),o6e.forEach(t),war=i(W),Dv=n(W,"LI",{});var r6e=s(Dv);Gge=n(r6e,"STRONG",{});var dnt=s(Gge);Aar=r(dnt,"mobilebert"),dnt.forEach(t),Lar=r(r6e," \u2014 "),Oq=n(r6e,"A",{href:!0});var cnt=s(Oq);Bar=r(cnt,"TFMobileBertForSequenceClassification"),cnt.forEach(t),kar=r(r6e," (MobileBERT model)"),r6e.forEach(t),xar=i(W),qv=n(W,"LI",{});var t6e=s(qv);Oge=n(t6e,"STRONG",{});var fnt=s(Oge);Rar=r(fnt,"mpnet"),fnt.forEach(t),Sar=r(t6e," \u2014 "),Xq=n(t6e,"A",{href:!0});var mnt=s(Xq);Par=r(mnt,"TFMPNetForSequenceClassification"),mnt.forEach(t),$ar=r(t6e," (MPNet model)"),t6e.forEach(t),Iar=i(W),Gv=n(W,"LI",{});var a6e=s(Gv);Xge=n(a6e,"STRONG",{});var gnt=s(Xge);jar=r(gnt,"openai-gpt"),gnt.forEach(t),Nar=r(a6e," \u2014 "),zq=n(a6e,"A",{href:!0});var hnt=s(zq);Dar=r(hnt,"TFOpenAIGPTForSequenceClassification"),hnt.forEach(t),qar=r(a6e," (OpenAI GPT model)"),a6e.forEach(t),Gar=i(W),Ov=n(W,"LI",{});var n6e=s(Ov);zge=n(n6e,"STRONG",{});var pnt=s(zge);Oar=r(pnt,"rembert"),pnt.forEach(t),Xar=r(n6e," \u2014 "),Vq=n(n6e,"A",{href:!0});var _nt=s(Vq);zar=r(_nt,"TFRemBertForSequenceClassification"),_nt.forEach(t),Var=r(n6e," (RemBERT model)"),n6e.forEach(t),War=i(W),Xv=n(W,"LI",{});var s6e=s(Xv);Vge=n(s6e,"STRONG",{});var unt=s(Vge);Qar=r(unt,"roberta"),unt.forEach(t),Har=r(s6e," \u2014 "),Wq=n(s6e,"A",{href:!0});var bnt=s(Wq);Uar=r(bnt,"TFRobertaForSequenceClassification"),bnt.forEach(t),Jar=r(s6e," (RoBERTa model)"),s6e.forEach(t),Yar=i(W),zv=n(W,"LI",{});var l6e=s(zv);Wge=n(l6e,"STRONG",{});var vnt=s(Wge);Kar=r(vnt,"roformer"),vnt.forEach(t),Zar=r(l6e," \u2014 "),Qq=n(l6e,"A",{href:!0});var Tnt=s(Qq);enr=r(Tnt,"TFRoFormerForSequenceClassification"),Tnt.forEach(t),onr=r(l6e," (RoFormer model)"),l6e.forEach(t),rnr=i(W),Vv=n(W,"LI",{});var i6e=s(Vv);Qge=n(i6e,"STRONG",{});var Fnt=s(Qge);tnr=r(Fnt,"tapas"),Fnt.forEach(t),anr=r(i6e," \u2014 "),Hq=n(i6e,"A",{href:!0});var Cnt=s(Hq);nnr=r(Cnt,"TFTapasForSequenceClassification"),Cnt.forEach(t),snr=r(i6e," (TAPAS model)"),i6e.forEach(t),lnr=i(W),Wv=n(W,"LI",{});var d6e=s(Wv);Hge=n(d6e,"STRONG",{});var Mnt=s(Hge);inr=r(Mnt,"transfo-xl"),Mnt.forEach(t),dnr=r(d6e," \u2014 "),Uq=n(d6e,"A",{href:!0});var Ent=s(Uq);cnr=r(Ent,"TFTransfoXLForSequenceClassification"),Ent.forEach(t),fnr=r(d6e," (Transformer-XL model)"),d6e.forEach(t),mnr=i(W),Qv=n(W,"LI",{});var c6e=s(Qv);Uge=n(c6e,"STRONG",{});var ynt=s(Uge);gnr=r(ynt,"xlm"),ynt.forEach(t),hnr=r(c6e," \u2014 "),Jq=n(c6e,"A",{href:!0});var wnt=s(Jq);pnr=r(wnt,"TFXLMForSequenceClassification"),wnt.forEach(t),_nr=r(c6e," (XLM model)"),c6e.forEach(t),unr=i(W),Hv=n(W,"LI",{});var f6e=s(Hv);Jge=n(f6e,"STRONG",{});var Ant=s(Jge);bnr=r(Ant,"xlm-roberta"),Ant.forEach(t),vnr=r(f6e," \u2014 "),Yq=n(f6e,"A",{href:!0});var Lnt=s(Yq);Tnr=r(Lnt,"TFXLMRobertaForSequenceClassification"),Lnt.forEach(t),Fnr=r(f6e," (XLM-RoBERTa model)"),f6e.forEach(t),Cnr=i(W),Uv=n(W,"LI",{});var m6e=s(Uv);Yge=n(m6e,"STRONG",{});var Bnt=s(Yge);Mnr=r(Bnt,"xlnet"),Bnt.forEach(t),Enr=r(m6e," \u2014 "),Kq=n(m6e,"A",{href:!0});var knt=s(Kq);ynr=r(knt,"TFXLNetForSequenceClassification"),knt.forEach(t),wnr=r(m6e," (XLNet model)"),m6e.forEach(t),W.forEach(t),Anr=i(_a),Kge=n(_a,"P",{});var xnt=s(Kge);Lnr=r(xnt,"Examples:"),xnt.forEach(t),Bnr=i(_a),m(MA.$$.fragment,_a),_a.forEach(t),Gl.forEach(t),T9e=i(d),Cc=n(d,"H2",{class:!0});var Bke=s(Cc);Jv=n(Bke,"A",{id:!0,class:!0,href:!0});var Rnt=s(Jv);Zge=n(Rnt,"SPAN",{});var Snt=s(Zge);m(EA.$$.fragment,Snt),Snt.forEach(t),Rnt.forEach(t),knr=i(Bke),ehe=n(Bke,"SPAN",{});var Pnt=s(ehe);xnr=r(Pnt,"TFAutoModelForMultipleChoice"),Pnt.forEach(t),Bke.forEach(t),F9e=i(d),Tr=n(d,"DIV",{class:!0});var Xl=s(Tr);m(yA.$$.fragment,Xl),Rnr=i(Xl),Mc=n(Xl,"P",{});var Zz=s(Mc);Snr=r(Zz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a multiple choice head) when created
with the `),ohe=n(Zz,"CODE",{});var $nt=s(ohe);Pnr=r($nt,"from_pretrained()"),$nt.forEach(t),$nr=r(Zz,"class method or the "),rhe=n(Zz,"CODE",{});var Int=s(rhe);Inr=r(Int,"from_config()"),Int.forEach(t),jnr=r(Zz,`class
method.`),Zz.forEach(t),Nnr=i(Xl),wA=n(Xl,"P",{});var kke=s(wA);Dnr=r(kke,"This class cannot be instantiated directly using "),the=n(kke,"CODE",{});var jnt=s(the);qnr=r(jnt,"__init__()"),jnt.forEach(t),Gnr=r(kke," (throws an error)."),kke.forEach(t),Onr=i(Xl),gt=n(Xl,"DIV",{class:!0});var zl=s(gt);m(AA.$$.fragment,zl),Xnr=i(zl),ahe=n(zl,"P",{});var Nnt=s(ahe);znr=r(Nnt,"Instantiates one of the model classes of the library (with a multiple choice head) from a configuration."),Nnt.forEach(t),Vnr=i(zl),Ec=n(zl,"P",{});var eV=s(Ec);Wnr=r(eV,`Note:
Loading a model from its configuration file does `),nhe=n(eV,"STRONG",{});var Dnt=s(nhe);Qnr=r(Dnt,"not"),Dnt.forEach(t),Hnr=r(eV,` load the model weights. It only affects the
model\u2019s configuration. Use `),she=n(eV,"CODE",{});var qnt=s(she);Unr=r(qnt,"from_pretrained()"),qnt.forEach(t),Jnr=r(eV,"to load the model weights."),eV.forEach(t),Ynr=i(zl),lhe=n(zl,"P",{});var Gnt=s(lhe);Knr=r(Gnt,"Examples:"),Gnt.forEach(t),Znr=i(zl),m(LA.$$.fragment,zl),zl.forEach(t),esr=i(Xl),To=n(Xl,"DIV",{class:!0});var ua=s(To);m(BA.$$.fragment,ua),osr=i(ua),ihe=n(ua,"P",{});var Ont=s(ihe);rsr=r(Ont,"Instantiate one of the model classes of the library (with a multiple choice head) from a pretrained model."),Ont.forEach(t),tsr=i(ua),pn=n(ua,"P",{});var IM=s(pn);asr=r(IM,"The model class to instantiate is selected based on the "),dhe=n(IM,"CODE",{});var Xnt=s(dhe);nsr=r(Xnt,"model_type"),Xnt.forEach(t),ssr=r(IM,` property of the config object (either
passed as an argument or loaded from `),che=n(IM,"CODE",{});var znt=s(che);lsr=r(znt,"pretrained_model_name_or_path"),znt.forEach(t),isr=r(IM,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),fhe=n(IM,"CODE",{});var Vnt=s(fhe);dsr=r(Vnt,"pretrained_model_name_or_path"),Vnt.forEach(t),csr=r(IM,":"),IM.forEach(t),fsr=i(ua),te=n(ua,"UL",{});var ne=s(te);Yv=n(ne,"LI",{});var g6e=s(Yv);mhe=n(g6e,"STRONG",{});var Wnt=s(mhe);msr=r(Wnt,"albert"),Wnt.forEach(t),gsr=r(g6e," \u2014 "),Zq=n(g6e,"A",{href:!0});var Qnt=s(Zq);hsr=r(Qnt,"TFAlbertForMultipleChoice"),Qnt.forEach(t),psr=r(g6e," (ALBERT model)"),g6e.forEach(t),_sr=i(ne),Kv=n(ne,"LI",{});var h6e=s(Kv);ghe=n(h6e,"STRONG",{});var Hnt=s(ghe);usr=r(Hnt,"bert"),Hnt.forEach(t),bsr=r(h6e," \u2014 "),eG=n(h6e,"A",{href:!0});var Unt=s(eG);vsr=r(Unt,"TFBertForMultipleChoice"),Unt.forEach(t),Tsr=r(h6e," (BERT model)"),h6e.forEach(t),Fsr=i(ne),Zv=n(ne,"LI",{});var p6e=s(Zv);hhe=n(p6e,"STRONG",{});var Jnt=s(hhe);Csr=r(Jnt,"camembert"),Jnt.forEach(t),Msr=r(p6e," \u2014 "),oG=n(p6e,"A",{href:!0});var Ynt=s(oG);Esr=r(Ynt,"TFCamembertForMultipleChoice"),Ynt.forEach(t),ysr=r(p6e," (CamemBERT model)"),p6e.forEach(t),wsr=i(ne),eT=n(ne,"LI",{});var _6e=s(eT);phe=n(_6e,"STRONG",{});var Knt=s(phe);Asr=r(Knt,"convbert"),Knt.forEach(t),Lsr=r(_6e," \u2014 "),rG=n(_6e,"A",{href:!0});var Znt=s(rG);Bsr=r(Znt,"TFConvBertForMultipleChoice"),Znt.forEach(t),ksr=r(_6e," (ConvBERT model)"),_6e.forEach(t),xsr=i(ne),oT=n(ne,"LI",{});var u6e=s(oT);_he=n(u6e,"STRONG",{});var est=s(_he);Rsr=r(est,"distilbert"),est.forEach(t),Ssr=r(u6e," \u2014 "),tG=n(u6e,"A",{href:!0});var ost=s(tG);Psr=r(ost,"TFDistilBertForMultipleChoice"),ost.forEach(t),$sr=r(u6e," (DistilBERT model)"),u6e.forEach(t),Isr=i(ne),rT=n(ne,"LI",{});var b6e=s(rT);uhe=n(b6e,"STRONG",{});var rst=s(uhe);jsr=r(rst,"electra"),rst.forEach(t),Nsr=r(b6e," \u2014 "),aG=n(b6e,"A",{href:!0});var tst=s(aG);Dsr=r(tst,"TFElectraForMultipleChoice"),tst.forEach(t),qsr=r(b6e," (ELECTRA model)"),b6e.forEach(t),Gsr=i(ne),tT=n(ne,"LI",{});var v6e=s(tT);bhe=n(v6e,"STRONG",{});var ast=s(bhe);Osr=r(ast,"flaubert"),ast.forEach(t),Xsr=r(v6e," \u2014 "),nG=n(v6e,"A",{href:!0});var nst=s(nG);zsr=r(nst,"TFFlaubertForMultipleChoice"),nst.forEach(t),Vsr=r(v6e," (FlauBERT model)"),v6e.forEach(t),Wsr=i(ne),aT=n(ne,"LI",{});var T6e=s(aT);vhe=n(T6e,"STRONG",{});var sst=s(vhe);Qsr=r(sst,"funnel"),sst.forEach(t),Hsr=r(T6e," \u2014 "),sG=n(T6e,"A",{href:!0});var lst=s(sG);Usr=r(lst,"TFFunnelForMultipleChoice"),lst.forEach(t),Jsr=r(T6e," (Funnel Transformer model)"),T6e.forEach(t),Ysr=i(ne),nT=n(ne,"LI",{});var F6e=s(nT);The=n(F6e,"STRONG",{});var ist=s(The);Ksr=r(ist,"longformer"),ist.forEach(t),Zsr=r(F6e," \u2014 "),lG=n(F6e,"A",{href:!0});var dst=s(lG);elr=r(dst,"TFLongformerForMultipleChoice"),dst.forEach(t),olr=r(F6e," (Longformer model)"),F6e.forEach(t),rlr=i(ne),sT=n(ne,"LI",{});var C6e=s(sT);Fhe=n(C6e,"STRONG",{});var cst=s(Fhe);tlr=r(cst,"mobilebert"),cst.forEach(t),alr=r(C6e," \u2014 "),iG=n(C6e,"A",{href:!0});var fst=s(iG);nlr=r(fst,"TFMobileBertForMultipleChoice"),fst.forEach(t),slr=r(C6e," (MobileBERT model)"),C6e.forEach(t),llr=i(ne),lT=n(ne,"LI",{});var M6e=s(lT);Che=n(M6e,"STRONG",{});var mst=s(Che);ilr=r(mst,"mpnet"),mst.forEach(t),dlr=r(M6e," \u2014 "),dG=n(M6e,"A",{href:!0});var gst=s(dG);clr=r(gst,"TFMPNetForMultipleChoice"),gst.forEach(t),flr=r(M6e," (MPNet model)"),M6e.forEach(t),mlr=i(ne),iT=n(ne,"LI",{});var E6e=s(iT);Mhe=n(E6e,"STRONG",{});var hst=s(Mhe);glr=r(hst,"rembert"),hst.forEach(t),hlr=r(E6e," \u2014 "),cG=n(E6e,"A",{href:!0});var pst=s(cG);plr=r(pst,"TFRemBertForMultipleChoice"),pst.forEach(t),_lr=r(E6e," (RemBERT model)"),E6e.forEach(t),ulr=i(ne),dT=n(ne,"LI",{});var y6e=s(dT);Ehe=n(y6e,"STRONG",{});var _st=s(Ehe);blr=r(_st,"roberta"),_st.forEach(t),vlr=r(y6e," \u2014 "),fG=n(y6e,"A",{href:!0});var ust=s(fG);Tlr=r(ust,"TFRobertaForMultipleChoice"),ust.forEach(t),Flr=r(y6e," (RoBERTa model)"),y6e.forEach(t),Clr=i(ne),cT=n(ne,"LI",{});var w6e=s(cT);yhe=n(w6e,"STRONG",{});var bst=s(yhe);Mlr=r(bst,"roformer"),bst.forEach(t),Elr=r(w6e," \u2014 "),mG=n(w6e,"A",{href:!0});var vst=s(mG);ylr=r(vst,"TFRoFormerForMultipleChoice"),vst.forEach(t),wlr=r(w6e," (RoFormer model)"),w6e.forEach(t),Alr=i(ne),fT=n(ne,"LI",{});var A6e=s(fT);whe=n(A6e,"STRONG",{});var Tst=s(whe);Llr=r(Tst,"xlm"),Tst.forEach(t),Blr=r(A6e," \u2014 "),gG=n(A6e,"A",{href:!0});var Fst=s(gG);klr=r(Fst,"TFXLMForMultipleChoice"),Fst.forEach(t),xlr=r(A6e," (XLM model)"),A6e.forEach(t),Rlr=i(ne),mT=n(ne,"LI",{});var L6e=s(mT);Ahe=n(L6e,"STRONG",{});var Cst=s(Ahe);Slr=r(Cst,"xlm-roberta"),Cst.forEach(t),Plr=r(L6e," \u2014 "),hG=n(L6e,"A",{href:!0});var Mst=s(hG);$lr=r(Mst,"TFXLMRobertaForMultipleChoice"),Mst.forEach(t),Ilr=r(L6e," (XLM-RoBERTa model)"),L6e.forEach(t),jlr=i(ne),gT=n(ne,"LI",{});var B6e=s(gT);Lhe=n(B6e,"STRONG",{});var Est=s(Lhe);Nlr=r(Est,"xlnet"),Est.forEach(t),Dlr=r(B6e," \u2014 "),pG=n(B6e,"A",{href:!0});var yst=s(pG);qlr=r(yst,"TFXLNetForMultipleChoice"),yst.forEach(t),Glr=r(B6e," (XLNet model)"),B6e.forEach(t),ne.forEach(t),Olr=i(ua),Bhe=n(ua,"P",{});var wst=s(Bhe);Xlr=r(wst,"Examples:"),wst.forEach(t),zlr=i(ua),m(kA.$$.fragment,ua),ua.forEach(t),Xl.forEach(t),C9e=i(d),yc=n(d,"H2",{class:!0});var xke=s(yc);hT=n(xke,"A",{id:!0,class:!0,href:!0});var Ast=s(hT);khe=n(Ast,"SPAN",{});var Lst=s(khe);m(xA.$$.fragment,Lst),Lst.forEach(t),Ast.forEach(t),Vlr=i(xke),xhe=n(xke,"SPAN",{});var Bst=s(xhe);Wlr=r(Bst,"TFAutoModelForTableQuestionAnswering"),Bst.forEach(t),xke.forEach(t),M9e=i(d),Fr=n(d,"DIV",{class:!0});var Vl=s(Fr);m(RA.$$.fragment,Vl),Qlr=i(Vl),wc=n(Vl,"P",{});var oV=s(wc);Hlr=r(oV,`This is a generic model class that will be instantiated as one of the model classes of the library (with a table question answering head) when created
with the `),Rhe=n(oV,"CODE",{});var kst=s(Rhe);Ulr=r(kst,"from_pretrained()"),kst.forEach(t),Jlr=r(oV,"class method or the "),She=n(oV,"CODE",{});var xst=s(She);Ylr=r(xst,"from_config()"),xst.forEach(t),Klr=r(oV,`class
method.`),oV.forEach(t),Zlr=i(Vl),SA=n(Vl,"P",{});var Rke=s(SA);eir=r(Rke,"This class cannot be instantiated directly using "),Phe=n(Rke,"CODE",{});var Rst=s(Phe);oir=r(Rst,"__init__()"),Rst.forEach(t),rir=r(Rke," (throws an error)."),Rke.forEach(t),tir=i(Vl),ht=n(Vl,"DIV",{class:!0});var Wl=s(ht);m(PA.$$.fragment,Wl),air=i(Wl),$he=n(Wl,"P",{});var Sst=s($he);nir=r(Sst,"Instantiates one of the model classes of the library (with a table question answering head) from a configuration."),Sst.forEach(t),sir=i(Wl),Ac=n(Wl,"P",{});var rV=s(Ac);lir=r(rV,`Note:
Loading a model from its configuration file does `),Ihe=n(rV,"STRONG",{});var Pst=s(Ihe);iir=r(Pst,"not"),Pst.forEach(t),dir=r(rV,` load the model weights. It only affects the
model\u2019s configuration. Use `),jhe=n(rV,"CODE",{});var $st=s(jhe);cir=r($st,"from_pretrained()"),$st.forEach(t),fir=r(rV,"to load the model weights."),rV.forEach(t),mir=i(Wl),Nhe=n(Wl,"P",{});var Ist=s(Nhe);gir=r(Ist,"Examples:"),Ist.forEach(t),hir=i(Wl),m($A.$$.fragment,Wl),Wl.forEach(t),pir=i(Vl),Fo=n(Vl,"DIV",{class:!0});var ba=s(Fo);m(IA.$$.fragment,ba),_ir=i(ba),Dhe=n(ba,"P",{});var jst=s(Dhe);uir=r(jst,"Instantiate one of the model classes of the library (with a table question answering head) from a pretrained model."),jst.forEach(t),bir=i(ba),_n=n(ba,"P",{});var jM=s(_n);vir=r(jM,"The model class to instantiate is selected based on the "),qhe=n(jM,"CODE",{});var Nst=s(qhe);Tir=r(Nst,"model_type"),Nst.forEach(t),Fir=r(jM,` property of the config object (either
passed as an argument or loaded from `),Ghe=n(jM,"CODE",{});var Dst=s(Ghe);Cir=r(Dst,"pretrained_model_name_or_path"),Dst.forEach(t),Mir=r(jM,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Ohe=n(jM,"CODE",{});var qst=s(Ohe);Eir=r(qst,"pretrained_model_name_or_path"),qst.forEach(t),yir=r(jM,":"),jM.forEach(t),wir=i(ba),Xhe=n(ba,"UL",{});var Gst=s(Xhe);pT=n(Gst,"LI",{});var k6e=s(pT);zhe=n(k6e,"STRONG",{});var Ost=s(zhe);Air=r(Ost,"tapas"),Ost.forEach(t),Lir=r(k6e," \u2014 "),_G=n(k6e,"A",{href:!0});var Xst=s(_G);Bir=r(Xst,"TFTapasForQuestionAnswering"),Xst.forEach(t),kir=r(k6e," (TAPAS model)"),k6e.forEach(t),Gst.forEach(t),xir=i(ba),Vhe=n(ba,"P",{});var zst=s(Vhe);Rir=r(zst,"Examples:"),zst.forEach(t),Sir=i(ba),m(jA.$$.fragment,ba),ba.forEach(t),Vl.forEach(t),E9e=i(d),Lc=n(d,"H2",{class:!0});var Ske=s(Lc);_T=n(Ske,"A",{id:!0,class:!0,href:!0});var Vst=s(_T);Whe=n(Vst,"SPAN",{});var Wst=s(Whe);m(NA.$$.fragment,Wst),Wst.forEach(t),Vst.forEach(t),Pir=i(Ske),Qhe=n(Ske,"SPAN",{});var Qst=s(Qhe);$ir=r(Qst,"TFAutoModelForTokenClassification"),Qst.forEach(t),Ske.forEach(t),y9e=i(d),Cr=n(d,"DIV",{class:!0});var Ql=s(Cr);m(DA.$$.fragment,Ql),Iir=i(Ql),Bc=n(Ql,"P",{});var tV=s(Bc);jir=r(tV,`This is a generic model class that will be instantiated as one of the model classes of the library (with a token classification head) when created
with the `),Hhe=n(tV,"CODE",{});var Hst=s(Hhe);Nir=r(Hst,"from_pretrained()"),Hst.forEach(t),Dir=r(tV,"class method or the "),Uhe=n(tV,"CODE",{});var Ust=s(Uhe);qir=r(Ust,"from_config()"),Ust.forEach(t),Gir=r(tV,`class
method.`),tV.forEach(t),Oir=i(Ql),qA=n(Ql,"P",{});var Pke=s(qA);Xir=r(Pke,"This class cannot be instantiated directly using "),Jhe=n(Pke,"CODE",{});var Jst=s(Jhe);zir=r(Jst,"__init__()"),Jst.forEach(t),Vir=r(Pke," (throws an error)."),Pke.forEach(t),Wir=i(Ql),pt=n(Ql,"DIV",{class:!0});var Hl=s(pt);m(GA.$$.fragment,Hl),Qir=i(Hl),Yhe=n(Hl,"P",{});var Yst=s(Yhe);Hir=r(Yst,"Instantiates one of the model classes of the library (with a token classification head) from a configuration."),Yst.forEach(t),Uir=i(Hl),kc=n(Hl,"P",{});var aV=s(kc);Jir=r(aV,`Note:
Loading a model from its configuration file does `),Khe=n(aV,"STRONG",{});var Kst=s(Khe);Yir=r(Kst,"not"),Kst.forEach(t),Kir=r(aV,` load the model weights. It only affects the
model\u2019s configuration. Use `),Zhe=n(aV,"CODE",{});var Zst=s(Zhe);Zir=r(Zst,"from_pretrained()"),Zst.forEach(t),edr=r(aV,"to load the model weights."),aV.forEach(t),odr=i(Hl),epe=n(Hl,"P",{});var elt=s(epe);rdr=r(elt,"Examples:"),elt.forEach(t),tdr=i(Hl),m(OA.$$.fragment,Hl),Hl.forEach(t),adr=i(Ql),Co=n(Ql,"DIV",{class:!0});var va=s(Co);m(XA.$$.fragment,va),ndr=i(va),ope=n(va,"P",{});var olt=s(ope);sdr=r(olt,"Instantiate one of the model classes of the library (with a token classification head) from a pretrained model."),olt.forEach(t),ldr=i(va),un=n(va,"P",{});var NM=s(un);idr=r(NM,"The model class to instantiate is selected based on the "),rpe=n(NM,"CODE",{});var rlt=s(rpe);ddr=r(rlt,"model_type"),rlt.forEach(t),cdr=r(NM,` property of the config object (either
passed as an argument or loaded from `),tpe=n(NM,"CODE",{});var tlt=s(tpe);fdr=r(tlt,"pretrained_model_name_or_path"),tlt.forEach(t),mdr=r(NM,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),ape=n(NM,"CODE",{});var alt=s(ape);gdr=r(alt,"pretrained_model_name_or_path"),alt.forEach(t),hdr=r(NM,":"),NM.forEach(t),pdr=i(va),K=n(va,"UL",{});var oe=s(K);uT=n(oe,"LI",{});var x6e=s(uT);npe=n(x6e,"STRONG",{});var nlt=s(npe);_dr=r(nlt,"albert"),nlt.forEach(t),udr=r(x6e," \u2014 "),uG=n(x6e,"A",{href:!0});var slt=s(uG);bdr=r(slt,"TFAlbertForTokenClassification"),slt.forEach(t),vdr=r(x6e," (ALBERT model)"),x6e.forEach(t),Tdr=i(oe),bT=n(oe,"LI",{});var R6e=s(bT);spe=n(R6e,"STRONG",{});var llt=s(spe);Fdr=r(llt,"bert"),llt.forEach(t),Cdr=r(R6e," \u2014 "),bG=n(R6e,"A",{href:!0});var ilt=s(bG);Mdr=r(ilt,"TFBertForTokenClassification"),ilt.forEach(t),Edr=r(R6e," (BERT model)"),R6e.forEach(t),ydr=i(oe),vT=n(oe,"LI",{});var S6e=s(vT);lpe=n(S6e,"STRONG",{});var dlt=s(lpe);wdr=r(dlt,"camembert"),dlt.forEach(t),Adr=r(S6e," \u2014 "),vG=n(S6e,"A",{href:!0});var clt=s(vG);Ldr=r(clt,"TFCamembertForTokenClassification"),clt.forEach(t),Bdr=r(S6e," (CamemBERT model)"),S6e.forEach(t),kdr=i(oe),TT=n(oe,"LI",{});var P6e=s(TT);ipe=n(P6e,"STRONG",{});var flt=s(ipe);xdr=r(flt,"convbert"),flt.forEach(t),Rdr=r(P6e," \u2014 "),TG=n(P6e,"A",{href:!0});var mlt=s(TG);Sdr=r(mlt,"TFConvBertForTokenClassification"),mlt.forEach(t),Pdr=r(P6e," (ConvBERT model)"),P6e.forEach(t),$dr=i(oe),FT=n(oe,"LI",{});var $6e=s(FT);dpe=n($6e,"STRONG",{});var glt=s(dpe);Idr=r(glt,"deberta"),glt.forEach(t),jdr=r($6e," \u2014 "),FG=n($6e,"A",{href:!0});var hlt=s(FG);Ndr=r(hlt,"TFDebertaForTokenClassification"),hlt.forEach(t),Ddr=r($6e," (DeBERTa model)"),$6e.forEach(t),qdr=i(oe),CT=n(oe,"LI",{});var I6e=s(CT);cpe=n(I6e,"STRONG",{});var plt=s(cpe);Gdr=r(plt,"deberta-v2"),plt.forEach(t),Odr=r(I6e," \u2014 "),CG=n(I6e,"A",{href:!0});var _lt=s(CG);Xdr=r(_lt,"TFDebertaV2ForTokenClassification"),_lt.forEach(t),zdr=r(I6e," (DeBERTa-v2 model)"),I6e.forEach(t),Vdr=i(oe),MT=n(oe,"LI",{});var j6e=s(MT);fpe=n(j6e,"STRONG",{});var ult=s(fpe);Wdr=r(ult,"distilbert"),ult.forEach(t),Qdr=r(j6e," \u2014 "),MG=n(j6e,"A",{href:!0});var blt=s(MG);Hdr=r(blt,"TFDistilBertForTokenClassification"),blt.forEach(t),Udr=r(j6e," (DistilBERT model)"),j6e.forEach(t),Jdr=i(oe),ET=n(oe,"LI",{});var N6e=s(ET);mpe=n(N6e,"STRONG",{});var vlt=s(mpe);Ydr=r(vlt,"electra"),vlt.forEach(t),Kdr=r(N6e," \u2014 "),EG=n(N6e,"A",{href:!0});var Tlt=s(EG);Zdr=r(Tlt,"TFElectraForTokenClassification"),Tlt.forEach(t),ecr=r(N6e," (ELECTRA model)"),N6e.forEach(t),ocr=i(oe),yT=n(oe,"LI",{});var D6e=s(yT);gpe=n(D6e,"STRONG",{});var Flt=s(gpe);rcr=r(Flt,"flaubert"),Flt.forEach(t),tcr=r(D6e," \u2014 "),yG=n(D6e,"A",{href:!0});var Clt=s(yG);acr=r(Clt,"TFFlaubertForTokenClassification"),Clt.forEach(t),ncr=r(D6e," (FlauBERT model)"),D6e.forEach(t),scr=i(oe),wT=n(oe,"LI",{});var q6e=s(wT);hpe=n(q6e,"STRONG",{});var Mlt=s(hpe);lcr=r(Mlt,"funnel"),Mlt.forEach(t),icr=r(q6e," \u2014 "),wG=n(q6e,"A",{href:!0});var Elt=s(wG);dcr=r(Elt,"TFFunnelForTokenClassification"),Elt.forEach(t),ccr=r(q6e," (Funnel Transformer model)"),q6e.forEach(t),fcr=i(oe),AT=n(oe,"LI",{});var G6e=s(AT);ppe=n(G6e,"STRONG",{});var ylt=s(ppe);mcr=r(ylt,"layoutlm"),ylt.forEach(t),gcr=r(G6e," \u2014 "),AG=n(G6e,"A",{href:!0});var wlt=s(AG);hcr=r(wlt,"TFLayoutLMForTokenClassification"),wlt.forEach(t),pcr=r(G6e," (LayoutLM model)"),G6e.forEach(t),_cr=i(oe),LT=n(oe,"LI",{});var O6e=s(LT);_pe=n(O6e,"STRONG",{});var Alt=s(_pe);ucr=r(Alt,"longformer"),Alt.forEach(t),bcr=r(O6e," \u2014 "),LG=n(O6e,"A",{href:!0});var Llt=s(LG);vcr=r(Llt,"TFLongformerForTokenClassification"),Llt.forEach(t),Tcr=r(O6e," (Longformer model)"),O6e.forEach(t),Fcr=i(oe),BT=n(oe,"LI",{});var X6e=s(BT);upe=n(X6e,"STRONG",{});var Blt=s(upe);Ccr=r(Blt,"mobilebert"),Blt.forEach(t),Mcr=r(X6e," \u2014 "),BG=n(X6e,"A",{href:!0});var klt=s(BG);Ecr=r(klt,"TFMobileBertForTokenClassification"),klt.forEach(t),ycr=r(X6e," (MobileBERT model)"),X6e.forEach(t),wcr=i(oe),kT=n(oe,"LI",{});var z6e=s(kT);bpe=n(z6e,"STRONG",{});var xlt=s(bpe);Acr=r(xlt,"mpnet"),xlt.forEach(t),Lcr=r(z6e," \u2014 "),kG=n(z6e,"A",{href:!0});var Rlt=s(kG);Bcr=r(Rlt,"TFMPNetForTokenClassification"),Rlt.forEach(t),kcr=r(z6e," (MPNet model)"),z6e.forEach(t),xcr=i(oe),xT=n(oe,"LI",{});var V6e=s(xT);vpe=n(V6e,"STRONG",{});var Slt=s(vpe);Rcr=r(Slt,"rembert"),Slt.forEach(t),Scr=r(V6e," \u2014 "),xG=n(V6e,"A",{href:!0});var Plt=s(xG);Pcr=r(Plt,"TFRemBertForTokenClassification"),Plt.forEach(t),$cr=r(V6e," (RemBERT model)"),V6e.forEach(t),Icr=i(oe),RT=n(oe,"LI",{});var W6e=s(RT);Tpe=n(W6e,"STRONG",{});var $lt=s(Tpe);jcr=r($lt,"roberta"),$lt.forEach(t),Ncr=r(W6e," \u2014 "),RG=n(W6e,"A",{href:!0});var Ilt=s(RG);Dcr=r(Ilt,"TFRobertaForTokenClassification"),Ilt.forEach(t),qcr=r(W6e," (RoBERTa model)"),W6e.forEach(t),Gcr=i(oe),ST=n(oe,"LI",{});var Q6e=s(ST);Fpe=n(Q6e,"STRONG",{});var jlt=s(Fpe);Ocr=r(jlt,"roformer"),jlt.forEach(t),Xcr=r(Q6e," \u2014 "),SG=n(Q6e,"A",{href:!0});var Nlt=s(SG);zcr=r(Nlt,"TFRoFormerForTokenClassification"),Nlt.forEach(t),Vcr=r(Q6e," (RoFormer model)"),Q6e.forEach(t),Wcr=i(oe),PT=n(oe,"LI",{});var H6e=s(PT);Cpe=n(H6e,"STRONG",{});var Dlt=s(Cpe);Qcr=r(Dlt,"xlm"),Dlt.forEach(t),Hcr=r(H6e," \u2014 "),PG=n(H6e,"A",{href:!0});var qlt=s(PG);Ucr=r(qlt,"TFXLMForTokenClassification"),qlt.forEach(t),Jcr=r(H6e," (XLM model)"),H6e.forEach(t),Ycr=i(oe),$T=n(oe,"LI",{});var U6e=s($T);Mpe=n(U6e,"STRONG",{});var Glt=s(Mpe);Kcr=r(Glt,"xlm-roberta"),Glt.forEach(t),Zcr=r(U6e," \u2014 "),$G=n(U6e,"A",{href:!0});var Olt=s($G);efr=r(Olt,"TFXLMRobertaForTokenClassification"),Olt.forEach(t),ofr=r(U6e," (XLM-RoBERTa model)"),U6e.forEach(t),rfr=i(oe),IT=n(oe,"LI",{});var J6e=s(IT);Epe=n(J6e,"STRONG",{});var Xlt=s(Epe);tfr=r(Xlt,"xlnet"),Xlt.forEach(t),afr=r(J6e," \u2014 "),IG=n(J6e,"A",{href:!0});var zlt=s(IG);nfr=r(zlt,"TFXLNetForTokenClassification"),zlt.forEach(t),sfr=r(J6e," (XLNet model)"),J6e.forEach(t),oe.forEach(t),lfr=i(va),ype=n(va,"P",{});var Vlt=s(ype);ifr=r(Vlt,"Examples:"),Vlt.forEach(t),dfr=i(va),m(zA.$$.fragment,va),va.forEach(t),Ql.forEach(t),w9e=i(d),xc=n(d,"H2",{class:!0});var $ke=s(xc);jT=n($ke,"A",{id:!0,class:!0,href:!0});var Wlt=s(jT);wpe=n(Wlt,"SPAN",{});var Qlt=s(wpe);m(VA.$$.fragment,Qlt),Qlt.forEach(t),Wlt.forEach(t),cfr=i($ke),Ape=n($ke,"SPAN",{});var Hlt=s(Ape);ffr=r(Hlt,"TFAutoModelForQuestionAnswering"),Hlt.forEach(t),$ke.forEach(t),A9e=i(d),Mr=n(d,"DIV",{class:!0});var Ul=s(Mr);m(WA.$$.fragment,Ul),mfr=i(Ul),Rc=n(Ul,"P",{});var nV=s(Rc);gfr=r(nV,`This is a generic model class that will be instantiated as one of the model classes of the library (with a question answering head) when created
with the `),Lpe=n(nV,"CODE",{});var Ult=s(Lpe);hfr=r(Ult,"from_pretrained()"),Ult.forEach(t),pfr=r(nV,"class method or the "),Bpe=n(nV,"CODE",{});var Jlt=s(Bpe);_fr=r(Jlt,"from_config()"),Jlt.forEach(t),ufr=r(nV,`class
method.`),nV.forEach(t),bfr=i(Ul),QA=n(Ul,"P",{});var Ike=s(QA);vfr=r(Ike,"This class cannot be instantiated directly using "),kpe=n(Ike,"CODE",{});var Ylt=s(kpe);Tfr=r(Ylt,"__init__()"),Ylt.forEach(t),Ffr=r(Ike," (throws an error)."),Ike.forEach(t),Cfr=i(Ul),_t=n(Ul,"DIV",{class:!0});var Jl=s(_t);m(HA.$$.fragment,Jl),Mfr=i(Jl),xpe=n(Jl,"P",{});var Klt=s(xpe);Efr=r(Klt,"Instantiates one of the model classes of the library (with a question answering head) from a configuration."),Klt.forEach(t),yfr=i(Jl),Sc=n(Jl,"P",{});var sV=s(Sc);wfr=r(sV,`Note:
Loading a model from its configuration file does `),Rpe=n(sV,"STRONG",{});var Zlt=s(Rpe);Afr=r(Zlt,"not"),Zlt.forEach(t),Lfr=r(sV,` load the model weights. It only affects the
model\u2019s configuration. Use `),Spe=n(sV,"CODE",{});var eit=s(Spe);Bfr=r(eit,"from_pretrained()"),eit.forEach(t),kfr=r(sV,"to load the model weights."),sV.forEach(t),xfr=i(Jl),Ppe=n(Jl,"P",{});var oit=s(Ppe);Rfr=r(oit,"Examples:"),oit.forEach(t),Sfr=i(Jl),m(UA.$$.fragment,Jl),Jl.forEach(t),Pfr=i(Ul),Mo=n(Ul,"DIV",{class:!0});var Ta=s(Mo);m(JA.$$.fragment,Ta),$fr=i(Ta),$pe=n(Ta,"P",{});var rit=s($pe);Ifr=r(rit,"Instantiate one of the model classes of the library (with a question answering head) from a pretrained model."),rit.forEach(t),jfr=i(Ta),bn=n(Ta,"P",{});var DM=s(bn);Nfr=r(DM,"The model class to instantiate is selected based on the "),Ipe=n(DM,"CODE",{});var tit=s(Ipe);Dfr=r(tit,"model_type"),tit.forEach(t),qfr=r(DM,` property of the config object (either
passed as an argument or loaded from `),jpe=n(DM,"CODE",{});var ait=s(jpe);Gfr=r(ait,"pretrained_model_name_or_path"),ait.forEach(t),Ofr=r(DM,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Npe=n(DM,"CODE",{});var nit=s(Npe);Xfr=r(nit,"pretrained_model_name_or_path"),nit.forEach(t),zfr=r(DM,":"),DM.forEach(t),Vfr=i(Ta),Z=n(Ta,"UL",{});var re=s(Z);NT=n(re,"LI",{});var Y6e=s(NT);Dpe=n(Y6e,"STRONG",{});var sit=s(Dpe);Wfr=r(sit,"albert"),sit.forEach(t),Qfr=r(Y6e," \u2014 "),jG=n(Y6e,"A",{href:!0});var lit=s(jG);Hfr=r(lit,"TFAlbertForQuestionAnswering"),lit.forEach(t),Ufr=r(Y6e," (ALBERT model)"),Y6e.forEach(t),Jfr=i(re),DT=n(re,"LI",{});var K6e=s(DT);qpe=n(K6e,"STRONG",{});var iit=s(qpe);Yfr=r(iit,"bert"),iit.forEach(t),Kfr=r(K6e," \u2014 "),NG=n(K6e,"A",{href:!0});var dit=s(NG);Zfr=r(dit,"TFBertForQuestionAnswering"),dit.forEach(t),emr=r(K6e," (BERT model)"),K6e.forEach(t),omr=i(re),qT=n(re,"LI",{});var Z6e=s(qT);Gpe=n(Z6e,"STRONG",{});var cit=s(Gpe);rmr=r(cit,"camembert"),cit.forEach(t),tmr=r(Z6e," \u2014 "),DG=n(Z6e,"A",{href:!0});var fit=s(DG);amr=r(fit,"TFCamembertForQuestionAnswering"),fit.forEach(t),nmr=r(Z6e," (CamemBERT model)"),Z6e.forEach(t),smr=i(re),GT=n(re,"LI",{});var e0e=s(GT);Ope=n(e0e,"STRONG",{});var mit=s(Ope);lmr=r(mit,"convbert"),mit.forEach(t),imr=r(e0e," \u2014 "),qG=n(e0e,"A",{href:!0});var git=s(qG);dmr=r(git,"TFConvBertForQuestionAnswering"),git.forEach(t),cmr=r(e0e," (ConvBERT model)"),e0e.forEach(t),fmr=i(re),OT=n(re,"LI",{});var o0e=s(OT);Xpe=n(o0e,"STRONG",{});var hit=s(Xpe);mmr=r(hit,"deberta"),hit.forEach(t),gmr=r(o0e," \u2014 "),GG=n(o0e,"A",{href:!0});var pit=s(GG);hmr=r(pit,"TFDebertaForQuestionAnswering"),pit.forEach(t),pmr=r(o0e," (DeBERTa model)"),o0e.forEach(t),_mr=i(re),XT=n(re,"LI",{});var r0e=s(XT);zpe=n(r0e,"STRONG",{});var _it=s(zpe);umr=r(_it,"deberta-v2"),_it.forEach(t),bmr=r(r0e," \u2014 "),OG=n(r0e,"A",{href:!0});var uit=s(OG);vmr=r(uit,"TFDebertaV2ForQuestionAnswering"),uit.forEach(t),Tmr=r(r0e," (DeBERTa-v2 model)"),r0e.forEach(t),Fmr=i(re),zT=n(re,"LI",{});var t0e=s(zT);Vpe=n(t0e,"STRONG",{});var bit=s(Vpe);Cmr=r(bit,"distilbert"),bit.forEach(t),Mmr=r(t0e," \u2014 "),XG=n(t0e,"A",{href:!0});var vit=s(XG);Emr=r(vit,"TFDistilBertForQuestionAnswering"),vit.forEach(t),ymr=r(t0e," (DistilBERT model)"),t0e.forEach(t),wmr=i(re),VT=n(re,"LI",{});var a0e=s(VT);Wpe=n(a0e,"STRONG",{});var Tit=s(Wpe);Amr=r(Tit,"electra"),Tit.forEach(t),Lmr=r(a0e," \u2014 "),zG=n(a0e,"A",{href:!0});var Fit=s(zG);Bmr=r(Fit,"TFElectraForQuestionAnswering"),Fit.forEach(t),kmr=r(a0e," (ELECTRA model)"),a0e.forEach(t),xmr=i(re),WT=n(re,"LI",{});var n0e=s(WT);Qpe=n(n0e,"STRONG",{});var Cit=s(Qpe);Rmr=r(Cit,"flaubert"),Cit.forEach(t),Smr=r(n0e," \u2014 "),VG=n(n0e,"A",{href:!0});var Mit=s(VG);Pmr=r(Mit,"TFFlaubertForQuestionAnsweringSimple"),Mit.forEach(t),$mr=r(n0e," (FlauBERT model)"),n0e.forEach(t),Imr=i(re),QT=n(re,"LI",{});var s0e=s(QT);Hpe=n(s0e,"STRONG",{});var Eit=s(Hpe);jmr=r(Eit,"funnel"),Eit.forEach(t),Nmr=r(s0e," \u2014 "),WG=n(s0e,"A",{href:!0});var yit=s(WG);Dmr=r(yit,"TFFunnelForQuestionAnswering"),yit.forEach(t),qmr=r(s0e," (Funnel Transformer model)"),s0e.forEach(t),Gmr=i(re),HT=n(re,"LI",{});var l0e=s(HT);Upe=n(l0e,"STRONG",{});var wit=s(Upe);Omr=r(wit,"longformer"),wit.forEach(t),Xmr=r(l0e," \u2014 "),QG=n(l0e,"A",{href:!0});var Ait=s(QG);zmr=r(Ait,"TFLongformerForQuestionAnswering"),Ait.forEach(t),Vmr=r(l0e," (Longformer model)"),l0e.forEach(t),Wmr=i(re),UT=n(re,"LI",{});var i0e=s(UT);Jpe=n(i0e,"STRONG",{});var Lit=s(Jpe);Qmr=r(Lit,"mobilebert"),Lit.forEach(t),Hmr=r(i0e," \u2014 "),HG=n(i0e,"A",{href:!0});var Bit=s(HG);Umr=r(Bit,"TFMobileBertForQuestionAnswering"),Bit.forEach(t),Jmr=r(i0e," (MobileBERT model)"),i0e.forEach(t),Ymr=i(re),JT=n(re,"LI",{});var d0e=s(JT);Ype=n(d0e,"STRONG",{});var kit=s(Ype);Kmr=r(kit,"mpnet"),kit.forEach(t),Zmr=r(d0e," \u2014 "),UG=n(d0e,"A",{href:!0});var xit=s(UG);egr=r(xit,"TFMPNetForQuestionAnswering"),xit.forEach(t),ogr=r(d0e," (MPNet model)"),d0e.forEach(t),rgr=i(re),YT=n(re,"LI",{});var c0e=s(YT);Kpe=n(c0e,"STRONG",{});var Rit=s(Kpe);tgr=r(Rit,"rembert"),Rit.forEach(t),agr=r(c0e," \u2014 "),JG=n(c0e,"A",{href:!0});var Sit=s(JG);ngr=r(Sit,"TFRemBertForQuestionAnswering"),Sit.forEach(t),sgr=r(c0e," (RemBERT model)"),c0e.forEach(t),lgr=i(re),KT=n(re,"LI",{});var f0e=s(KT);Zpe=n(f0e,"STRONG",{});var Pit=s(Zpe);igr=r(Pit,"roberta"),Pit.forEach(t),dgr=r(f0e," \u2014 "),YG=n(f0e,"A",{href:!0});var $it=s(YG);cgr=r($it,"TFRobertaForQuestionAnswering"),$it.forEach(t),fgr=r(f0e," (RoBERTa model)"),f0e.forEach(t),mgr=i(re),ZT=n(re,"LI",{});var m0e=s(ZT);e_e=n(m0e,"STRONG",{});var Iit=s(e_e);ggr=r(Iit,"roformer"),Iit.forEach(t),hgr=r(m0e," \u2014 "),KG=n(m0e,"A",{href:!0});var jit=s(KG);pgr=r(jit,"TFRoFormerForQuestionAnswering"),jit.forEach(t),_gr=r(m0e," (RoFormer model)"),m0e.forEach(t),ugr=i(re),eF=n(re,"LI",{});var g0e=s(eF);o_e=n(g0e,"STRONG",{});var Nit=s(o_e);bgr=r(Nit,"xlm"),Nit.forEach(t),vgr=r(g0e," \u2014 "),ZG=n(g0e,"A",{href:!0});var Dit=s(ZG);Tgr=r(Dit,"TFXLMForQuestionAnsweringSimple"),Dit.forEach(t),Fgr=r(g0e," (XLM model)"),g0e.forEach(t),Cgr=i(re),oF=n(re,"LI",{});var h0e=s(oF);r_e=n(h0e,"STRONG",{});var qit=s(r_e);Mgr=r(qit,"xlm-roberta"),qit.forEach(t),Egr=r(h0e," \u2014 "),eO=n(h0e,"A",{href:!0});var Git=s(eO);ygr=r(Git,"TFXLMRobertaForQuestionAnswering"),Git.forEach(t),wgr=r(h0e," (XLM-RoBERTa model)"),h0e.forEach(t),Agr=i(re),rF=n(re,"LI",{});var p0e=s(rF);t_e=n(p0e,"STRONG",{});var Oit=s(t_e);Lgr=r(Oit,"xlnet"),Oit.forEach(t),Bgr=r(p0e," \u2014 "),oO=n(p0e,"A",{href:!0});var Xit=s(oO);kgr=r(Xit,"TFXLNetForQuestionAnsweringSimple"),Xit.forEach(t),xgr=r(p0e," (XLNet model)"),p0e.forEach(t),re.forEach(t),Rgr=i(Ta),a_e=n(Ta,"P",{});var zit=s(a_e);Sgr=r(zit,"Examples:"),zit.forEach(t),Pgr=i(Ta),m(YA.$$.fragment,Ta),Ta.forEach(t),Ul.forEach(t),L9e=i(d),Pc=n(d,"H2",{class:!0});var jke=s(Pc);tF=n(jke,"A",{id:!0,class:!0,href:!0});var Vit=s(tF);n_e=n(Vit,"SPAN",{});var Wit=s(n_e);m(KA.$$.fragment,Wit),Wit.forEach(t),Vit.forEach(t),$gr=i(jke),s_e=n(jke,"SPAN",{});var Qit=s(s_e);Igr=r(Qit,"TFAutoModelForVision2Seq"),Qit.forEach(t),jke.forEach(t),B9e=i(d),Er=n(d,"DIV",{class:!0});var Yl=s(Er);m(ZA.$$.fragment,Yl),jgr=i(Yl),$c=n(Yl,"P",{});var lV=s($c);Ngr=r(lV,`This is a generic model class that will be instantiated as one of the model classes of the library (with a vision-to-text modeling head) when created
with the `),l_e=n(lV,"CODE",{});var Hit=s(l_e);Dgr=r(Hit,"from_pretrained()"),Hit.forEach(t),qgr=r(lV,"class method or the "),i_e=n(lV,"CODE",{});var Uit=s(i_e);Ggr=r(Uit,"from_config()"),Uit.forEach(t),Ogr=r(lV,`class
method.`),lV.forEach(t),Xgr=i(Yl),e6=n(Yl,"P",{});var Nke=s(e6);zgr=r(Nke,"This class cannot be instantiated directly using "),d_e=n(Nke,"CODE",{});var Jit=s(d_e);Vgr=r(Jit,"__init__()"),Jit.forEach(t),Wgr=r(Nke," (throws an error)."),Nke.forEach(t),Qgr=i(Yl),ut=n(Yl,"DIV",{class:!0});var Kl=s(ut);m(o6.$$.fragment,Kl),Hgr=i(Kl),c_e=n(Kl,"P",{});var Yit=s(c_e);Ugr=r(Yit,"Instantiates one of the model classes of the library (with a vision-to-text modeling head) from a configuration."),Yit.forEach(t),Jgr=i(Kl),Ic=n(Kl,"P",{});var iV=s(Ic);Ygr=r(iV,`Note:
Loading a model from its configuration file does `),f_e=n(iV,"STRONG",{});var Kit=s(f_e);Kgr=r(Kit,"not"),Kit.forEach(t),Zgr=r(iV,` load the model weights. It only affects the
model\u2019s configuration. Use `),m_e=n(iV,"CODE",{});var Zit=s(m_e);ehr=r(Zit,"from_pretrained()"),Zit.forEach(t),ohr=r(iV,"to load the model weights."),iV.forEach(t),rhr=i(Kl),g_e=n(Kl,"P",{});var edt=s(g_e);thr=r(edt,"Examples:"),edt.forEach(t),ahr=i(Kl),m(r6.$$.fragment,Kl),Kl.forEach(t),nhr=i(Yl),Eo=n(Yl,"DIV",{class:!0});var Fa=s(Eo);m(t6.$$.fragment,Fa),shr=i(Fa),h_e=n(Fa,"P",{});var odt=s(h_e);lhr=r(odt,"Instantiate one of the model classes of the library (with a vision-to-text modeling head) from a pretrained model."),odt.forEach(t),ihr=i(Fa),vn=n(Fa,"P",{});var qM=s(vn);dhr=r(qM,"The model class to instantiate is selected based on the "),p_e=n(qM,"CODE",{});var rdt=s(p_e);chr=r(rdt,"model_type"),rdt.forEach(t),fhr=r(qM,` property of the config object (either
passed as an argument or loaded from `),__e=n(qM,"CODE",{});var tdt=s(__e);mhr=r(tdt,"pretrained_model_name_or_path"),tdt.forEach(t),ghr=r(qM,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),u_e=n(qM,"CODE",{});var adt=s(u_e);hhr=r(adt,"pretrained_model_name_or_path"),adt.forEach(t),phr=r(qM,":"),qM.forEach(t),_hr=i(Fa),b_e=n(Fa,"UL",{});var ndt=s(b_e);aF=n(ndt,"LI",{});var _0e=s(aF);v_e=n(_0e,"STRONG",{});var sdt=s(v_e);uhr=r(sdt,"vision-encoder-decoder"),sdt.forEach(t),bhr=r(_0e," \u2014 "),rO=n(_0e,"A",{href:!0});var ldt=s(rO);vhr=r(ldt,"TFVisionEncoderDecoderModel"),ldt.forEach(t),Thr=r(_0e," (Vision Encoder decoder model)"),_0e.forEach(t),ndt.forEach(t),Fhr=i(Fa),T_e=n(Fa,"P",{});var idt=s(T_e);Chr=r(idt,"Examples:"),idt.forEach(t),Mhr=i(Fa),m(a6.$$.fragment,Fa),Fa.forEach(t),Yl.forEach(t),k9e=i(d),jc=n(d,"H2",{class:!0});var Dke=s(jc);nF=n(Dke,"A",{id:!0,class:!0,href:!0});var ddt=s(nF);F_e=n(ddt,"SPAN",{});var cdt=s(F_e);m(n6.$$.fragment,cdt),cdt.forEach(t),ddt.forEach(t),Ehr=i(Dke),C_e=n(Dke,"SPAN",{});var fdt=s(C_e);yhr=r(fdt,"TFAutoModelForSpeechSeq2Seq"),fdt.forEach(t),Dke.forEach(t),x9e=i(d),yr=n(d,"DIV",{class:!0});var Zl=s(yr);m(s6.$$.fragment,Zl),whr=i(Zl),Nc=n(Zl,"P",{});var dV=s(Nc);Ahr=r(dV,`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) when created
with the `),M_e=n(dV,"CODE",{});var mdt=s(M_e);Lhr=r(mdt,"from_pretrained()"),mdt.forEach(t),Bhr=r(dV,"class method or the "),E_e=n(dV,"CODE",{});var gdt=s(E_e);khr=r(gdt,"from_config()"),gdt.forEach(t),xhr=r(dV,`class
method.`),dV.forEach(t),Rhr=i(Zl),l6=n(Zl,"P",{});var qke=s(l6);Shr=r(qke,"This class cannot be instantiated directly using "),y_e=n(qke,"CODE",{});var hdt=s(y_e);Phr=r(hdt,"__init__()"),hdt.forEach(t),$hr=r(qke," (throws an error)."),qke.forEach(t),Ihr=i(Zl),bt=n(Zl,"DIV",{class:!0});var ei=s(bt);m(i6.$$.fragment,ei),jhr=i(ei),w_e=n(ei,"P",{});var pdt=s(w_e);Nhr=r(pdt,"Instantiates one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) from a configuration."),pdt.forEach(t),Dhr=i(ei),Dc=n(ei,"P",{});var cV=s(Dc);qhr=r(cV,`Note:
Loading a model from its configuration file does `),A_e=n(cV,"STRONG",{});var _dt=s(A_e);Ghr=r(_dt,"not"),_dt.forEach(t),Ohr=r(cV,` load the model weights. It only affects the
model\u2019s configuration. Use `),L_e=n(cV,"CODE",{});var udt=s(L_e);Xhr=r(udt,"from_pretrained()"),udt.forEach(t),zhr=r(cV,"to load the model weights."),cV.forEach(t),Vhr=i(ei),B_e=n(ei,"P",{});var bdt=s(B_e);Whr=r(bdt,"Examples:"),bdt.forEach(t),Qhr=i(ei),m(d6.$$.fragment,ei),ei.forEach(t),Hhr=i(Zl),yo=n(Zl,"DIV",{class:!0});var Ca=s(yo);m(c6.$$.fragment,Ca),Uhr=i(Ca),k_e=n(Ca,"P",{});var vdt=s(k_e);Jhr=r(vdt,"Instantiate one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) from a pretrained model."),vdt.forEach(t),Yhr=i(Ca),Tn=n(Ca,"P",{});var GM=s(Tn);Khr=r(GM,"The model class to instantiate is selected based on the "),x_e=n(GM,"CODE",{});var Tdt=s(x_e);Zhr=r(Tdt,"model_type"),Tdt.forEach(t),epr=r(GM,` property of the config object (either
passed as an argument or loaded from `),R_e=n(GM,"CODE",{});var Fdt=s(R_e);opr=r(Fdt,"pretrained_model_name_or_path"),Fdt.forEach(t),rpr=r(GM,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),S_e=n(GM,"CODE",{});var Cdt=s(S_e);tpr=r(Cdt,"pretrained_model_name_or_path"),Cdt.forEach(t),apr=r(GM,":"),GM.forEach(t),npr=i(Ca),P_e=n(Ca,"UL",{});var Mdt=s(P_e);sF=n(Mdt,"LI",{});var u0e=s(sF);$_e=n(u0e,"STRONG",{});var Edt=s($_e);spr=r(Edt,"speech_to_text"),Edt.forEach(t),lpr=r(u0e," \u2014 "),tO=n(u0e,"A",{href:!0});var ydt=s(tO);ipr=r(ydt,"TFSpeech2TextForConditionalGeneration"),ydt.forEach(t),dpr=r(u0e," (Speech2Text model)"),u0e.forEach(t),Mdt.forEach(t),cpr=i(Ca),I_e=n(Ca,"P",{});var wdt=s(I_e);fpr=r(wdt,"Examples:"),wdt.forEach(t),mpr=i(Ca),m(f6.$$.fragment,Ca),Ca.forEach(t),Zl.forEach(t),R9e=i(d),qc=n(d,"H2",{class:!0});var Gke=s(qc);lF=n(Gke,"A",{id:!0,class:!0,href:!0});var Adt=s(lF);j_e=n(Adt,"SPAN",{});var Ldt=s(j_e);m(m6.$$.fragment,Ldt),Ldt.forEach(t),Adt.forEach(t),gpr=i(Gke),N_e=n(Gke,"SPAN",{});var Bdt=s(N_e);hpr=r(Bdt,"FlaxAutoModel"),Bdt.forEach(t),Gke.forEach(t),S9e=i(d),wr=n(d,"DIV",{class:!0});var oi=s(wr);m(g6.$$.fragment,oi),ppr=i(oi),Gc=n(oi,"P",{});var fV=s(Gc);_pr=r(fV,`This is a generic model class that will be instantiated as one of the base model classes of the library when created
with the `),D_e=n(fV,"CODE",{});var kdt=s(D_e);upr=r(kdt,"from_pretrained()"),kdt.forEach(t),bpr=r(fV,"class method or the "),q_e=n(fV,"CODE",{});var xdt=s(q_e);vpr=r(xdt,"from_config()"),xdt.forEach(t),Tpr=r(fV,`class
method.`),fV.forEach(t),Fpr=i(oi),h6=n(oi,"P",{});var Oke=s(h6);Cpr=r(Oke,"This class cannot be instantiated directly using "),G_e=n(Oke,"CODE",{});var Rdt=s(G_e);Mpr=r(Rdt,"__init__()"),Rdt.forEach(t),Epr=r(Oke," (throws an error)."),Oke.forEach(t),ypr=i(oi),vt=n(oi,"DIV",{class:!0});var ri=s(vt);m(p6.$$.fragment,ri),wpr=i(ri),O_e=n(ri,"P",{});var Sdt=s(O_e);Apr=r(Sdt,"Instantiates one of the base model classes of the library from a configuration."),Sdt.forEach(t),Lpr=i(ri),Oc=n(ri,"P",{});var mV=s(Oc);Bpr=r(mV,`Note:
Loading a model from its configuration file does `),X_e=n(mV,"STRONG",{});var Pdt=s(X_e);kpr=r(Pdt,"not"),Pdt.forEach(t),xpr=r(mV,` load the model weights. It only affects the
model\u2019s configuration. Use `),z_e=n(mV,"CODE",{});var $dt=s(z_e);Rpr=r($dt,"from_pretrained()"),$dt.forEach(t),Spr=r(mV,"to load the model weights."),mV.forEach(t),Ppr=i(ri),V_e=n(ri,"P",{});var Idt=s(V_e);$pr=r(Idt,"Examples:"),Idt.forEach(t),Ipr=i(ri),m(_6.$$.fragment,ri),ri.forEach(t),jpr=i(oi),wo=n(oi,"DIV",{class:!0});var Ma=s(wo);m(u6.$$.fragment,Ma),Npr=i(Ma),W_e=n(Ma,"P",{});var jdt=s(W_e);Dpr=r(jdt,"Instantiate one of the base model classes of the library from a pretrained model."),jdt.forEach(t),qpr=i(Ma),Fn=n(Ma,"P",{});var OM=s(Fn);Gpr=r(OM,"The model class to instantiate is selected based on the "),Q_e=n(OM,"CODE",{});var Ndt=s(Q_e);Opr=r(Ndt,"model_type"),Ndt.forEach(t),Xpr=r(OM,` property of the config object (either
passed as an argument or loaded from `),H_e=n(OM,"CODE",{});var Ddt=s(H_e);zpr=r(Ddt,"pretrained_model_name_or_path"),Ddt.forEach(t),Vpr=r(OM,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),U_e=n(OM,"CODE",{});var qdt=s(U_e);Wpr=r(qdt,"pretrained_model_name_or_path"),qdt.forEach(t),Qpr=r(OM,":"),OM.forEach(t),Hpr=i(Ma),V=n(Ma,"UL",{});var Q=s(V);iF=n(Q,"LI",{});var b0e=s(iF);J_e=n(b0e,"STRONG",{});var Gdt=s(J_e);Upr=r(Gdt,"albert"),Gdt.forEach(t),Jpr=r(b0e," \u2014 "),aO=n(b0e,"A",{href:!0});var Odt=s(aO);Ypr=r(Odt,"FlaxAlbertModel"),Odt.forEach(t),Kpr=r(b0e," (ALBERT model)"),b0e.forEach(t),Zpr=i(Q),dF=n(Q,"LI",{});var v0e=s(dF);Y_e=n(v0e,"STRONG",{});var Xdt=s(Y_e);e_r=r(Xdt,"bart"),Xdt.forEach(t),o_r=r(v0e," \u2014 "),nO=n(v0e,"A",{href:!0});var zdt=s(nO);r_r=r(zdt,"FlaxBartModel"),zdt.forEach(t),t_r=r(v0e," (BART model)"),v0e.forEach(t),a_r=i(Q),cF=n(Q,"LI",{});var T0e=s(cF);K_e=n(T0e,"STRONG",{});var Vdt=s(K_e);n_r=r(Vdt,"beit"),Vdt.forEach(t),s_r=r(T0e," \u2014 "),sO=n(T0e,"A",{href:!0});var Wdt=s(sO);l_r=r(Wdt,"FlaxBeitModel"),Wdt.forEach(t),i_r=r(T0e," (BEiT model)"),T0e.forEach(t),d_r=i(Q),fF=n(Q,"LI",{});var F0e=s(fF);Z_e=n(F0e,"STRONG",{});var Qdt=s(Z_e);c_r=r(Qdt,"bert"),Qdt.forEach(t),f_r=r(F0e," \u2014 "),lO=n(F0e,"A",{href:!0});var Hdt=s(lO);m_r=r(Hdt,"FlaxBertModel"),Hdt.forEach(t),g_r=r(F0e," (BERT model)"),F0e.forEach(t),h_r=i(Q),mF=n(Q,"LI",{});var C0e=s(mF);eue=n(C0e,"STRONG",{});var Udt=s(eue);p_r=r(Udt,"big_bird"),Udt.forEach(t),__r=r(C0e," \u2014 "),iO=n(C0e,"A",{href:!0});var Jdt=s(iO);u_r=r(Jdt,"FlaxBigBirdModel"),Jdt.forEach(t),b_r=r(C0e," (BigBird model)"),C0e.forEach(t),v_r=i(Q),gF=n(Q,"LI",{});var M0e=s(gF);oue=n(M0e,"STRONG",{});var Ydt=s(oue);T_r=r(Ydt,"blenderbot"),Ydt.forEach(t),F_r=r(M0e," \u2014 "),dO=n(M0e,"A",{href:!0});var Kdt=s(dO);C_r=r(Kdt,"FlaxBlenderbotModel"),Kdt.forEach(t),M_r=r(M0e," (Blenderbot model)"),M0e.forEach(t),E_r=i(Q),hF=n(Q,"LI",{});var E0e=s(hF);rue=n(E0e,"STRONG",{});var Zdt=s(rue);y_r=r(Zdt,"blenderbot-small"),Zdt.forEach(t),w_r=r(E0e," \u2014 "),cO=n(E0e,"A",{href:!0});var ect=s(cO);A_r=r(ect,"FlaxBlenderbotSmallModel"),ect.forEach(t),L_r=r(E0e," (BlenderbotSmall model)"),E0e.forEach(t),B_r=i(Q),pF=n(Q,"LI",{});var y0e=s(pF);tue=n(y0e,"STRONG",{});var oct=s(tue);k_r=r(oct,"clip"),oct.forEach(t),x_r=r(y0e," \u2014 "),fO=n(y0e,"A",{href:!0});var rct=s(fO);R_r=r(rct,"FlaxCLIPModel"),rct.forEach(t),S_r=r(y0e," (CLIP model)"),y0e.forEach(t),P_r=i(Q),_F=n(Q,"LI",{});var w0e=s(_F);aue=n(w0e,"STRONG",{});var tct=s(aue);$_r=r(tct,"distilbert"),tct.forEach(t),I_r=r(w0e," \u2014 "),mO=n(w0e,"A",{href:!0});var act=s(mO);j_r=r(act,"FlaxDistilBertModel"),act.forEach(t),N_r=r(w0e," (DistilBERT model)"),w0e.forEach(t),D_r=i(Q),uF=n(Q,"LI",{});var A0e=s(uF);nue=n(A0e,"STRONG",{});var nct=s(nue);q_r=r(nct,"electra"),nct.forEach(t),G_r=r(A0e," \u2014 "),gO=n(A0e,"A",{href:!0});var sct=s(gO);O_r=r(sct,"FlaxElectraModel"),sct.forEach(t),X_r=r(A0e," (ELECTRA model)"),A0e.forEach(t),z_r=i(Q),bF=n(Q,"LI",{});var L0e=s(bF);sue=n(L0e,"STRONG",{});var lct=s(sue);V_r=r(lct,"gpt2"),lct.forEach(t),W_r=r(L0e," \u2014 "),hO=n(L0e,"A",{href:!0});var ict=s(hO);Q_r=r(ict,"FlaxGPT2Model"),ict.forEach(t),H_r=r(L0e," (OpenAI GPT-2 model)"),L0e.forEach(t),U_r=i(Q),vF=n(Q,"LI",{});var B0e=s(vF);lue=n(B0e,"STRONG",{});var dct=s(lue);J_r=r(dct,"gpt_neo"),dct.forEach(t),Y_r=r(B0e," \u2014 "),pO=n(B0e,"A",{href:!0});var cct=s(pO);K_r=r(cct,"FlaxGPTNeoModel"),cct.forEach(t),Z_r=r(B0e," (GPT Neo model)"),B0e.forEach(t),eur=i(Q),TF=n(Q,"LI",{});var k0e=s(TF);iue=n(k0e,"STRONG",{});var fct=s(iue);our=r(fct,"gptj"),fct.forEach(t),rur=r(k0e," \u2014 "),_O=n(k0e,"A",{href:!0});var mct=s(_O);tur=r(mct,"FlaxGPTJModel"),mct.forEach(t),aur=r(k0e," (GPT-J model)"),k0e.forEach(t),nur=i(Q),FF=n(Q,"LI",{});var x0e=s(FF);due=n(x0e,"STRONG",{});var gct=s(due);sur=r(gct,"marian"),gct.forEach(t),lur=r(x0e," \u2014 "),uO=n(x0e,"A",{href:!0});var hct=s(uO);iur=r(hct,"FlaxMarianModel"),hct.forEach(t),dur=r(x0e," (Marian model)"),x0e.forEach(t),cur=i(Q),CF=n(Q,"LI",{});var R0e=s(CF);cue=n(R0e,"STRONG",{});var pct=s(cue);fur=r(pct,"mbart"),pct.forEach(t),mur=r(R0e," \u2014 "),bO=n(R0e,"A",{href:!0});var _ct=s(bO);gur=r(_ct,"FlaxMBartModel"),_ct.forEach(t),hur=r(R0e," (mBART model)"),R0e.forEach(t),pur=i(Q),MF=n(Q,"LI",{});var S0e=s(MF);fue=n(S0e,"STRONG",{});var uct=s(fue);_ur=r(uct,"mt5"),uct.forEach(t),uur=r(S0e," \u2014 "),vO=n(S0e,"A",{href:!0});var bct=s(vO);bur=r(bct,"FlaxMT5Model"),bct.forEach(t),vur=r(S0e," (mT5 model)"),S0e.forEach(t),Tur=i(Q),EF=n(Q,"LI",{});var P0e=s(EF);mue=n(P0e,"STRONG",{});var vct=s(mue);Fur=r(vct,"pegasus"),vct.forEach(t),Cur=r(P0e," \u2014 "),TO=n(P0e,"A",{href:!0});var Tct=s(TO);Mur=r(Tct,"FlaxPegasusModel"),Tct.forEach(t),Eur=r(P0e," (Pegasus model)"),P0e.forEach(t),yur=i(Q),yF=n(Q,"LI",{});var $0e=s(yF);gue=n($0e,"STRONG",{});var Fct=s(gue);wur=r(Fct,"roberta"),Fct.forEach(t),Aur=r($0e," \u2014 "),FO=n($0e,"A",{href:!0});var Cct=s(FO);Lur=r(Cct,"FlaxRobertaModel"),Cct.forEach(t),Bur=r($0e," (RoBERTa model)"),$0e.forEach(t),kur=i(Q),wF=n(Q,"LI",{});var I0e=s(wF);hue=n(I0e,"STRONG",{});var Mct=s(hue);xur=r(Mct,"roformer"),Mct.forEach(t),Rur=r(I0e," \u2014 "),CO=n(I0e,"A",{href:!0});var Ect=s(CO);Sur=r(Ect,"FlaxRoFormerModel"),Ect.forEach(t),Pur=r(I0e," (RoFormer model)"),I0e.forEach(t),$ur=i(Q),AF=n(Q,"LI",{});var j0e=s(AF);pue=n(j0e,"STRONG",{});var yct=s(pue);Iur=r(yct,"t5"),yct.forEach(t),jur=r(j0e," \u2014 "),MO=n(j0e,"A",{href:!0});var wct=s(MO);Nur=r(wct,"FlaxT5Model"),wct.forEach(t),Dur=r(j0e," (T5 model)"),j0e.forEach(t),qur=i(Q),LF=n(Q,"LI",{});var N0e=s(LF);_ue=n(N0e,"STRONG",{});var Act=s(_ue);Gur=r(Act,"vision-text-dual-encoder"),Act.forEach(t),Our=r(N0e," \u2014 "),EO=n(N0e,"A",{href:!0});var Lct=s(EO);Xur=r(Lct,"FlaxVisionTextDualEncoderModel"),Lct.forEach(t),zur=r(N0e," (VisionTextDualEncoder model)"),N0e.forEach(t),Vur=i(Q),BF=n(Q,"LI",{});var D0e=s(BF);uue=n(D0e,"STRONG",{});var Bct=s(uue);Wur=r(Bct,"vit"),Bct.forEach(t),Qur=r(D0e," \u2014 "),yO=n(D0e,"A",{href:!0});var kct=s(yO);Hur=r(kct,"FlaxViTModel"),kct.forEach(t),Uur=r(D0e," (ViT model)"),D0e.forEach(t),Jur=i(Q),kF=n(Q,"LI",{});var q0e=s(kF);bue=n(q0e,"STRONG",{});var xct=s(bue);Yur=r(xct,"wav2vec2"),xct.forEach(t),Kur=r(q0e," \u2014 "),wO=n(q0e,"A",{href:!0});var Rct=s(wO);Zur=r(Rct,"FlaxWav2Vec2Model"),Rct.forEach(t),e1r=r(q0e," (Wav2Vec2 model)"),q0e.forEach(t),o1r=i(Q),xF=n(Q,"LI",{});var G0e=s(xF);vue=n(G0e,"STRONG",{});var Sct=s(vue);r1r=r(Sct,"xglm"),Sct.forEach(t),t1r=r(G0e," \u2014 "),AO=n(G0e,"A",{href:!0});var Pct=s(AO);a1r=r(Pct,"FlaxXGLMModel"),Pct.forEach(t),n1r=r(G0e," (XGLM model)"),G0e.forEach(t),Q.forEach(t),s1r=i(Ma),Tue=n(Ma,"P",{});var $ct=s(Tue);l1r=r($ct,"Examples:"),$ct.forEach(t),i1r=i(Ma),m(b6.$$.fragment,Ma),Ma.forEach(t),oi.forEach(t),P9e=i(d),Xc=n(d,"H2",{class:!0});var Xke=s(Xc);RF=n(Xke,"A",{id:!0,class:!0,href:!0});var Ict=s(RF);Fue=n(Ict,"SPAN",{});var jct=s(Fue);m(v6.$$.fragment,jct),jct.forEach(t),Ict.forEach(t),d1r=i(Xke),Cue=n(Xke,"SPAN",{});var Nct=s(Cue);c1r=r(Nct,"FlaxAutoModelForCausalLM"),Nct.forEach(t),Xke.forEach(t),$9e=i(d),Ar=n(d,"DIV",{class:!0});var ti=s(Ar);m(T6.$$.fragment,ti),f1r=i(ti),zc=n(ti,"P",{});var gV=s(zc);m1r=r(gV,`This is a generic model class that will be instantiated as one of the model classes of the library (with a causal language modeling head) when created
with the `),Mue=n(gV,"CODE",{});var Dct=s(Mue);g1r=r(Dct,"from_pretrained()"),Dct.forEach(t),h1r=r(gV,"class method or the "),Eue=n(gV,"CODE",{});var qct=s(Eue);p1r=r(qct,"from_config()"),qct.forEach(t),_1r=r(gV,`class
method.`),gV.forEach(t),u1r=i(ti),F6=n(ti,"P",{});var zke=s(F6);b1r=r(zke,"This class cannot be instantiated directly using "),yue=n(zke,"CODE",{});var Gct=s(yue);v1r=r(Gct,"__init__()"),Gct.forEach(t),T1r=r(zke," (throws an error)."),zke.forEach(t),F1r=i(ti),Tt=n(ti,"DIV",{class:!0});var ai=s(Tt);m(C6.$$.fragment,ai),C1r=i(ai),wue=n(ai,"P",{});var Oct=s(wue);M1r=r(Oct,"Instantiates one of the model classes of the library (with a causal language modeling head) from a configuration."),Oct.forEach(t),E1r=i(ai),Vc=n(ai,"P",{});var hV=s(Vc);y1r=r(hV,`Note:
Loading a model from its configuration file does `),Aue=n(hV,"STRONG",{});var Xct=s(Aue);w1r=r(Xct,"not"),Xct.forEach(t),A1r=r(hV,` load the model weights. It only affects the
model\u2019s configuration. Use `),Lue=n(hV,"CODE",{});var zct=s(Lue);L1r=r(zct,"from_pretrained()"),zct.forEach(t),B1r=r(hV,"to load the model weights."),hV.forEach(t),k1r=i(ai),Bue=n(ai,"P",{});var Vct=s(Bue);x1r=r(Vct,"Examples:"),Vct.forEach(t),R1r=i(ai),m(M6.$$.fragment,ai),ai.forEach(t),S1r=i(ti),Ao=n(ti,"DIV",{class:!0});var Ea=s(Ao);m(E6.$$.fragment,Ea),P1r=i(Ea),kue=n(Ea,"P",{});var Wct=s(kue);$1r=r(Wct,"Instantiate one of the model classes of the library (with a causal language modeling head) from a pretrained model."),Wct.forEach(t),I1r=i(Ea),Cn=n(Ea,"P",{});var XM=s(Cn);j1r=r(XM,"The model class to instantiate is selected based on the "),xue=n(XM,"CODE",{});var Qct=s(xue);N1r=r(Qct,"model_type"),Qct.forEach(t),D1r=r(XM,` property of the config object (either
passed as an argument or loaded from `),Rue=n(XM,"CODE",{});var Hct=s(Rue);q1r=r(Hct,"pretrained_model_name_or_path"),Hct.forEach(t),G1r=r(XM,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Sue=n(XM,"CODE",{});var Uct=s(Sue);O1r=r(Uct,"pretrained_model_name_or_path"),Uct.forEach(t),X1r=r(XM,":"),XM.forEach(t),z1r=i(Ea),Mn=n(Ea,"UL",{});var zM=s(Mn);SF=n(zM,"LI",{});var O0e=s(SF);Pue=n(O0e,"STRONG",{});var Jct=s(Pue);V1r=r(Jct,"gpt2"),Jct.forEach(t),W1r=r(O0e," \u2014 "),LO=n(O0e,"A",{href:!0});var Yct=s(LO);Q1r=r(Yct,"FlaxGPT2LMHeadModel"),Yct.forEach(t),H1r=r(O0e," (OpenAI GPT-2 model)"),O0e.forEach(t),U1r=i(zM),PF=n(zM,"LI",{});var X0e=s(PF);$ue=n(X0e,"STRONG",{});var Kct=s($ue);J1r=r(Kct,"gpt_neo"),Kct.forEach(t),Y1r=r(X0e," \u2014 "),BO=n(X0e,"A",{href:!0});var Zct=s(BO);K1r=r(Zct,"FlaxGPTNeoForCausalLM"),Zct.forEach(t),Z1r=r(X0e," (GPT Neo model)"),X0e.forEach(t),e7r=i(zM),$F=n(zM,"LI",{});var z0e=s($F);Iue=n(z0e,"STRONG",{});var eft=s(Iue);o7r=r(eft,"gptj"),eft.forEach(t),r7r=r(z0e," \u2014 "),kO=n(z0e,"A",{href:!0});var oft=s(kO);t7r=r(oft,"FlaxGPTJForCausalLM"),oft.forEach(t),a7r=r(z0e," (GPT-J model)"),z0e.forEach(t),n7r=i(zM),IF=n(zM,"LI",{});var V0e=s(IF);jue=n(V0e,"STRONG",{});var rft=s(jue);s7r=r(rft,"xglm"),rft.forEach(t),l7r=r(V0e," \u2014 "),xO=n(V0e,"A",{href:!0});var tft=s(xO);i7r=r(tft,"FlaxXGLMForCausalLM"),tft.forEach(t),d7r=r(V0e," (XGLM model)"),V0e.forEach(t),zM.forEach(t),c7r=i(Ea),Nue=n(Ea,"P",{});var aft=s(Nue);f7r=r(aft,"Examples:"),aft.forEach(t),m7r=i(Ea),m(y6.$$.fragment,Ea),Ea.forEach(t),ti.forEach(t),I9e=i(d),Wc=n(d,"H2",{class:!0});var Vke=s(Wc);jF=n(Vke,"A",{id:!0,class:!0,href:!0});var nft=s(jF);Due=n(nft,"SPAN",{});var sft=s(Due);m(w6.$$.fragment,sft),sft.forEach(t),nft.forEach(t),g7r=i(Vke),que=n(Vke,"SPAN",{});var lft=s(que);h7r=r(lft,"FlaxAutoModelForPreTraining"),lft.forEach(t),Vke.forEach(t),j9e=i(d),Lr=n(d,"DIV",{class:!0});var ni=s(Lr);m(A6.$$.fragment,ni),p7r=i(ni),Qc=n(ni,"P",{});var pV=s(Qc);_7r=r(pV,`This is a generic model class that will be instantiated as one of the model classes of the library (with a pretraining head) when created
with the `),Gue=n(pV,"CODE",{});var ift=s(Gue);u7r=r(ift,"from_pretrained()"),ift.forEach(t),b7r=r(pV,"class method or the "),Oue=n(pV,"CODE",{});var dft=s(Oue);v7r=r(dft,"from_config()"),dft.forEach(t),T7r=r(pV,`class
method.`),pV.forEach(t),F7r=i(ni),L6=n(ni,"P",{});var Wke=s(L6);C7r=r(Wke,"This class cannot be instantiated directly using "),Xue=n(Wke,"CODE",{});var cft=s(Xue);M7r=r(cft,"__init__()"),cft.forEach(t),E7r=r(Wke," (throws an error)."),Wke.forEach(t),y7r=i(ni),Ft=n(ni,"DIV",{class:!0});var si=s(Ft);m(B6.$$.fragment,si),w7r=i(si),zue=n(si,"P",{});var fft=s(zue);A7r=r(fft,"Instantiates one of the model classes of the library (with a pretraining head) from a configuration."),fft.forEach(t),L7r=i(si),Hc=n(si,"P",{});var _V=s(Hc);B7r=r(_V,`Note:
Loading a model from its configuration file does `),Vue=n(_V,"STRONG",{});var mft=s(Vue);k7r=r(mft,"not"),mft.forEach(t),x7r=r(_V,` load the model weights. It only affects the
model\u2019s configuration. Use `),Wue=n(_V,"CODE",{});var gft=s(Wue);R7r=r(gft,"from_pretrained()"),gft.forEach(t),S7r=r(_V,"to load the model weights."),_V.forEach(t),P7r=i(si),Que=n(si,"P",{});var hft=s(Que);$7r=r(hft,"Examples:"),hft.forEach(t),I7r=i(si),m(k6.$$.fragment,si),si.forEach(t),j7r=i(ni),Lo=n(ni,"DIV",{class:!0});var ya=s(Lo);m(x6.$$.fragment,ya),N7r=i(ya),Hue=n(ya,"P",{});var pft=s(Hue);D7r=r(pft,"Instantiate one of the model classes of the library (with a pretraining head) from a pretrained model."),pft.forEach(t),q7r=i(ya),En=n(ya,"P",{});var VM=s(En);G7r=r(VM,"The model class to instantiate is selected based on the "),Uue=n(VM,"CODE",{});var _ft=s(Uue);O7r=r(_ft,"model_type"),_ft.forEach(t),X7r=r(VM,` property of the config object (either
passed as an argument or loaded from `),Jue=n(VM,"CODE",{});var uft=s(Jue);z7r=r(uft,"pretrained_model_name_or_path"),uft.forEach(t),V7r=r(VM,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Yue=n(VM,"CODE",{});var bft=s(Yue);W7r=r(bft,"pretrained_model_name_or_path"),bft.forEach(t),Q7r=r(VM,":"),VM.forEach(t),H7r=i(ya),fe=n(ya,"UL",{});var _e=s(fe);NF=n(_e,"LI",{});var W0e=s(NF);Kue=n(W0e,"STRONG",{});var vft=s(Kue);U7r=r(vft,"albert"),vft.forEach(t),J7r=r(W0e," \u2014 "),RO=n(W0e,"A",{href:!0});var Tft=s(RO);Y7r=r(Tft,"FlaxAlbertForPreTraining"),Tft.forEach(t),K7r=r(W0e," (ALBERT model)"),W0e.forEach(t),Z7r=i(_e),DF=n(_e,"LI",{});var Q0e=s(DF);Zue=n(Q0e,"STRONG",{});var Fft=s(Zue);e4r=r(Fft,"bart"),Fft.forEach(t),o4r=r(Q0e," \u2014 "),SO=n(Q0e,"A",{href:!0});var Cft=s(SO);r4r=r(Cft,"FlaxBartForConditionalGeneration"),Cft.forEach(t),t4r=r(Q0e," (BART model)"),Q0e.forEach(t),a4r=i(_e),qF=n(_e,"LI",{});var H0e=s(qF);e1e=n(H0e,"STRONG",{});var Mft=s(e1e);n4r=r(Mft,"bert"),Mft.forEach(t),s4r=r(H0e," \u2014 "),PO=n(H0e,"A",{href:!0});var Eft=s(PO);l4r=r(Eft,"FlaxBertForPreTraining"),Eft.forEach(t),i4r=r(H0e," (BERT model)"),H0e.forEach(t),d4r=i(_e),GF=n(_e,"LI",{});var U0e=s(GF);o1e=n(U0e,"STRONG",{});var yft=s(o1e);c4r=r(yft,"big_bird"),yft.forEach(t),f4r=r(U0e," \u2014 "),$O=n(U0e,"A",{href:!0});var wft=s($O);m4r=r(wft,"FlaxBigBirdForPreTraining"),wft.forEach(t),g4r=r(U0e," (BigBird model)"),U0e.forEach(t),h4r=i(_e),OF=n(_e,"LI",{});var J0e=s(OF);r1e=n(J0e,"STRONG",{});var Aft=s(r1e);p4r=r(Aft,"electra"),Aft.forEach(t),_4r=r(J0e," \u2014 "),IO=n(J0e,"A",{href:!0});var Lft=s(IO);u4r=r(Lft,"FlaxElectraForPreTraining"),Lft.forEach(t),b4r=r(J0e," (ELECTRA model)"),J0e.forEach(t),v4r=i(_e),XF=n(_e,"LI",{});var Y0e=s(XF);t1e=n(Y0e,"STRONG",{});var Bft=s(t1e);T4r=r(Bft,"mbart"),Bft.forEach(t),F4r=r(Y0e," \u2014 "),jO=n(Y0e,"A",{href:!0});var kft=s(jO);C4r=r(kft,"FlaxMBartForConditionalGeneration"),kft.forEach(t),M4r=r(Y0e," (mBART model)"),Y0e.forEach(t),E4r=i(_e),zF=n(_e,"LI",{});var K0e=s(zF);a1e=n(K0e,"STRONG",{});var xft=s(a1e);y4r=r(xft,"mt5"),xft.forEach(t),w4r=r(K0e," \u2014 "),NO=n(K0e,"A",{href:!0});var Rft=s(NO);A4r=r(Rft,"FlaxMT5ForConditionalGeneration"),Rft.forEach(t),L4r=r(K0e," (mT5 model)"),K0e.forEach(t),B4r=i(_e),VF=n(_e,"LI",{});var Z0e=s(VF);n1e=n(Z0e,"STRONG",{});var Sft=s(n1e);k4r=r(Sft,"roberta"),Sft.forEach(t),x4r=r(Z0e," \u2014 "),DO=n(Z0e,"A",{href:!0});var Pft=s(DO);R4r=r(Pft,"FlaxRobertaForMaskedLM"),Pft.forEach(t),S4r=r(Z0e," (RoBERTa model)"),Z0e.forEach(t),P4r=i(_e),WF=n(_e,"LI",{});var eLe=s(WF);s1e=n(eLe,"STRONG",{});var $ft=s(s1e);$4r=r($ft,"roformer"),$ft.forEach(t),I4r=r(eLe," \u2014 "),qO=n(eLe,"A",{href:!0});var Ift=s(qO);j4r=r(Ift,"FlaxRoFormerForMaskedLM"),Ift.forEach(t),N4r=r(eLe," (RoFormer model)"),eLe.forEach(t),D4r=i(_e),QF=n(_e,"LI",{});var oLe=s(QF);l1e=n(oLe,"STRONG",{});var jft=s(l1e);q4r=r(jft,"t5"),jft.forEach(t),G4r=r(oLe," \u2014 "),GO=n(oLe,"A",{href:!0});var Nft=s(GO);O4r=r(Nft,"FlaxT5ForConditionalGeneration"),Nft.forEach(t),X4r=r(oLe," (T5 model)"),oLe.forEach(t),z4r=i(_e),HF=n(_e,"LI",{});var rLe=s(HF);i1e=n(rLe,"STRONG",{});var Dft=s(i1e);V4r=r(Dft,"wav2vec2"),Dft.forEach(t),W4r=r(rLe," \u2014 "),OO=n(rLe,"A",{href:!0});var qft=s(OO);Q4r=r(qft,"FlaxWav2Vec2ForPreTraining"),qft.forEach(t),H4r=r(rLe," (Wav2Vec2 model)"),rLe.forEach(t),_e.forEach(t),U4r=i(ya),d1e=n(ya,"P",{});var Gft=s(d1e);J4r=r(Gft,"Examples:"),Gft.forEach(t),Y4r=i(ya),m(R6.$$.fragment,ya),ya.forEach(t),ni.forEach(t),N9e=i(d),Uc=n(d,"H2",{class:!0});var Qke=s(Uc);UF=n(Qke,"A",{id:!0,class:!0,href:!0});var Oft=s(UF);c1e=n(Oft,"SPAN",{});var Xft=s(c1e);m(S6.$$.fragment,Xft),Xft.forEach(t),Oft.forEach(t),K4r=i(Qke),f1e=n(Qke,"SPAN",{});var zft=s(f1e);Z4r=r(zft,"FlaxAutoModelForMaskedLM"),zft.forEach(t),Qke.forEach(t),D9e=i(d),Br=n(d,"DIV",{class:!0});var li=s(Br);m(P6.$$.fragment,li),ebr=i(li),Jc=n(li,"P",{});var uV=s(Jc);obr=r(uV,`This is a generic model class that will be instantiated as one of the model classes of the library (with a masked language modeling head) when created
with the `),m1e=n(uV,"CODE",{});var Vft=s(m1e);rbr=r(Vft,"from_pretrained()"),Vft.forEach(t),tbr=r(uV,"class method or the "),g1e=n(uV,"CODE",{});var Wft=s(g1e);abr=r(Wft,"from_config()"),Wft.forEach(t),nbr=r(uV,`class
method.`),uV.forEach(t),sbr=i(li),$6=n(li,"P",{});var Hke=s($6);lbr=r(Hke,"This class cannot be instantiated directly using "),h1e=n(Hke,"CODE",{});var Qft=s(h1e);ibr=r(Qft,"__init__()"),Qft.forEach(t),dbr=r(Hke," (throws an error)."),Hke.forEach(t),cbr=i(li),Ct=n(li,"DIV",{class:!0});var ii=s(Ct);m(I6.$$.fragment,ii),fbr=i(ii),p1e=n(ii,"P",{});var Hft=s(p1e);mbr=r(Hft,"Instantiates one of the model classes of the library (with a masked language modeling head) from a configuration."),Hft.forEach(t),gbr=i(ii),Yc=n(ii,"P",{});var bV=s(Yc);hbr=r(bV,`Note:
Loading a model from its configuration file does `),_1e=n(bV,"STRONG",{});var Uft=s(_1e);pbr=r(Uft,"not"),Uft.forEach(t),_br=r(bV,` load the model weights. It only affects the
model\u2019s configuration. Use `),u1e=n(bV,"CODE",{});var Jft=s(u1e);ubr=r(Jft,"from_pretrained()"),Jft.forEach(t),bbr=r(bV,"to load the model weights."),bV.forEach(t),vbr=i(ii),b1e=n(ii,"P",{});var Yft=s(b1e);Tbr=r(Yft,"Examples:"),Yft.forEach(t),Fbr=i(ii),m(j6.$$.fragment,ii),ii.forEach(t),Cbr=i(li),Bo=n(li,"DIV",{class:!0});var wa=s(Bo);m(N6.$$.fragment,wa),Mbr=i(wa),v1e=n(wa,"P",{});var Kft=s(v1e);Ebr=r(Kft,"Instantiate one of the model classes of the library (with a masked language modeling head) from a pretrained model."),Kft.forEach(t),ybr=i(wa),yn=n(wa,"P",{});var WM=s(yn);wbr=r(WM,"The model class to instantiate is selected based on the "),T1e=n(WM,"CODE",{});var Zft=s(T1e);Abr=r(Zft,"model_type"),Zft.forEach(t),Lbr=r(WM,` property of the config object (either
passed as an argument or loaded from `),F1e=n(WM,"CODE",{});var emt=s(F1e);Bbr=r(emt,"pretrained_model_name_or_path"),emt.forEach(t),kbr=r(WM,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),C1e=n(WM,"CODE",{});var omt=s(C1e);xbr=r(omt,"pretrained_model_name_or_path"),omt.forEach(t),Rbr=r(WM,":"),WM.forEach(t),Sbr=i(wa),ve=n(wa,"UL",{});var Ze=s(ve);JF=n(Ze,"LI",{});var tLe=s(JF);M1e=n(tLe,"STRONG",{});var rmt=s(M1e);Pbr=r(rmt,"albert"),rmt.forEach(t),$br=r(tLe," \u2014 "),XO=n(tLe,"A",{href:!0});var tmt=s(XO);Ibr=r(tmt,"FlaxAlbertForMaskedLM"),tmt.forEach(t),jbr=r(tLe," (ALBERT model)"),tLe.forEach(t),Nbr=i(Ze),YF=n(Ze,"LI",{});var aLe=s(YF);E1e=n(aLe,"STRONG",{});var amt=s(E1e);Dbr=r(amt,"bart"),amt.forEach(t),qbr=r(aLe," \u2014 "),zO=n(aLe,"A",{href:!0});var nmt=s(zO);Gbr=r(nmt,"FlaxBartForConditionalGeneration"),nmt.forEach(t),Obr=r(aLe," (BART model)"),aLe.forEach(t),Xbr=i(Ze),KF=n(Ze,"LI",{});var nLe=s(KF);y1e=n(nLe,"STRONG",{});var smt=s(y1e);zbr=r(smt,"bert"),smt.forEach(t),Vbr=r(nLe," \u2014 "),VO=n(nLe,"A",{href:!0});var lmt=s(VO);Wbr=r(lmt,"FlaxBertForMaskedLM"),lmt.forEach(t),Qbr=r(nLe," (BERT model)"),nLe.forEach(t),Hbr=i(Ze),ZF=n(Ze,"LI",{});var sLe=s(ZF);w1e=n(sLe,"STRONG",{});var imt=s(w1e);Ubr=r(imt,"big_bird"),imt.forEach(t),Jbr=r(sLe," \u2014 "),WO=n(sLe,"A",{href:!0});var dmt=s(WO);Ybr=r(dmt,"FlaxBigBirdForMaskedLM"),dmt.forEach(t),Kbr=r(sLe," (BigBird model)"),sLe.forEach(t),Zbr=i(Ze),eC=n(Ze,"LI",{});var lLe=s(eC);A1e=n(lLe,"STRONG",{});var cmt=s(A1e);e5r=r(cmt,"distilbert"),cmt.forEach(t),o5r=r(lLe," \u2014 "),QO=n(lLe,"A",{href:!0});var fmt=s(QO);r5r=r(fmt,"FlaxDistilBertForMaskedLM"),fmt.forEach(t),t5r=r(lLe," (DistilBERT model)"),lLe.forEach(t),a5r=i(Ze),oC=n(Ze,"LI",{});var iLe=s(oC);L1e=n(iLe,"STRONG",{});var mmt=s(L1e);n5r=r(mmt,"electra"),mmt.forEach(t),s5r=r(iLe," \u2014 "),HO=n(iLe,"A",{href:!0});var gmt=s(HO);l5r=r(gmt,"FlaxElectraForMaskedLM"),gmt.forEach(t),i5r=r(iLe," (ELECTRA model)"),iLe.forEach(t),d5r=i(Ze),rC=n(Ze,"LI",{});var dLe=s(rC);B1e=n(dLe,"STRONG",{});var hmt=s(B1e);c5r=r(hmt,"mbart"),hmt.forEach(t),f5r=r(dLe," \u2014 "),UO=n(dLe,"A",{href:!0});var pmt=s(UO);m5r=r(pmt,"FlaxMBartForConditionalGeneration"),pmt.forEach(t),g5r=r(dLe," (mBART model)"),dLe.forEach(t),h5r=i(Ze),tC=n(Ze,"LI",{});var cLe=s(tC);k1e=n(cLe,"STRONG",{});var _mt=s(k1e);p5r=r(_mt,"roberta"),_mt.forEach(t),_5r=r(cLe," \u2014 "),JO=n(cLe,"A",{href:!0});var umt=s(JO);u5r=r(umt,"FlaxRobertaForMaskedLM"),umt.forEach(t),b5r=r(cLe," (RoBERTa model)"),cLe.forEach(t),v5r=i(Ze),aC=n(Ze,"LI",{});var fLe=s(aC);x1e=n(fLe,"STRONG",{});var bmt=s(x1e);T5r=r(bmt,"roformer"),bmt.forEach(t),F5r=r(fLe," \u2014 "),YO=n(fLe,"A",{href:!0});var vmt=s(YO);C5r=r(vmt,"FlaxRoFormerForMaskedLM"),vmt.forEach(t),M5r=r(fLe," (RoFormer model)"),fLe.forEach(t),Ze.forEach(t),E5r=i(wa),R1e=n(wa,"P",{});var Tmt=s(R1e);y5r=r(Tmt,"Examples:"),Tmt.forEach(t),w5r=i(wa),m(D6.$$.fragment,wa),wa.forEach(t),li.forEach(t),q9e=i(d),Kc=n(d,"H2",{class:!0});var Uke=s(Kc);nC=n(Uke,"A",{id:!0,class:!0,href:!0});var Fmt=s(nC);S1e=n(Fmt,"SPAN",{});var Cmt=s(S1e);m(q6.$$.fragment,Cmt),Cmt.forEach(t),Fmt.forEach(t),A5r=i(Uke),P1e=n(Uke,"SPAN",{});var Mmt=s(P1e);L5r=r(Mmt,"FlaxAutoModelForSeq2SeqLM"),Mmt.forEach(t),Uke.forEach(t),G9e=i(d),kr=n(d,"DIV",{class:!0});var di=s(kr);m(G6.$$.fragment,di),B5r=i(di),Zc=n(di,"P",{});var vV=s(Zc);k5r=r(vV,`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence language modeling head) when created
with the `),$1e=n(vV,"CODE",{});var Emt=s($1e);x5r=r(Emt,"from_pretrained()"),Emt.forEach(t),R5r=r(vV,"class method or the "),I1e=n(vV,"CODE",{});var ymt=s(I1e);S5r=r(ymt,"from_config()"),ymt.forEach(t),P5r=r(vV,`class
method.`),vV.forEach(t),$5r=i(di),O6=n(di,"P",{});var Jke=s(O6);I5r=r(Jke,"This class cannot be instantiated directly using "),j1e=n(Jke,"CODE",{});var wmt=s(j1e);j5r=r(wmt,"__init__()"),wmt.forEach(t),N5r=r(Jke," (throws an error)."),Jke.forEach(t),D5r=i(di),Mt=n(di,"DIV",{class:!0});var ci=s(Mt);m(X6.$$.fragment,ci),q5r=i(ci),N1e=n(ci,"P",{});var Amt=s(N1e);G5r=r(Amt,"Instantiates one of the model classes of the library (with a sequence-to-sequence language modeling head) from a configuration."),Amt.forEach(t),O5r=i(ci),ef=n(ci,"P",{});var TV=s(ef);X5r=r(TV,`Note:
Loading a model from its configuration file does `),D1e=n(TV,"STRONG",{});var Lmt=s(D1e);z5r=r(Lmt,"not"),Lmt.forEach(t),V5r=r(TV,` load the model weights. It only affects the
model\u2019s configuration. Use `),q1e=n(TV,"CODE",{});var Bmt=s(q1e);W5r=r(Bmt,"from_pretrained()"),Bmt.forEach(t),Q5r=r(TV,"to load the model weights."),TV.forEach(t),H5r=i(ci),G1e=n(ci,"P",{});var kmt=s(G1e);U5r=r(kmt,"Examples:"),kmt.forEach(t),J5r=i(ci),m(z6.$$.fragment,ci),ci.forEach(t),Y5r=i(di),ko=n(di,"DIV",{class:!0});var Aa=s(ko);m(V6.$$.fragment,Aa),K5r=i(Aa),O1e=n(Aa,"P",{});var xmt=s(O1e);Z5r=r(xmt,"Instantiate one of the model classes of the library (with a sequence-to-sequence language modeling head) from a pretrained model."),xmt.forEach(t),e2r=i(Aa),wn=n(Aa,"P",{});var QM=s(wn);o2r=r(QM,"The model class to instantiate is selected based on the "),X1e=n(QM,"CODE",{});var Rmt=s(X1e);r2r=r(Rmt,"model_type"),Rmt.forEach(t),t2r=r(QM,` property of the config object (either
passed as an argument or loaded from `),z1e=n(QM,"CODE",{});var Smt=s(z1e);a2r=r(Smt,"pretrained_model_name_or_path"),Smt.forEach(t),n2r=r(QM,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),V1e=n(QM,"CODE",{});var Pmt=s(V1e);s2r=r(Pmt,"pretrained_model_name_or_path"),Pmt.forEach(t),l2r=r(QM,":"),QM.forEach(t),i2r=i(Aa),Te=n(Aa,"UL",{});var eo=s(Te);sC=n(eo,"LI",{});var mLe=s(sC);W1e=n(mLe,"STRONG",{});var $mt=s(W1e);d2r=r($mt,"bart"),$mt.forEach(t),c2r=r(mLe," \u2014 "),KO=n(mLe,"A",{href:!0});var Imt=s(KO);f2r=r(Imt,"FlaxBartForConditionalGeneration"),Imt.forEach(t),m2r=r(mLe," (BART model)"),mLe.forEach(t),g2r=i(eo),lC=n(eo,"LI",{});var gLe=s(lC);Q1e=n(gLe,"STRONG",{});var jmt=s(Q1e);h2r=r(jmt,"blenderbot"),jmt.forEach(t),p2r=r(gLe," \u2014 "),ZO=n(gLe,"A",{href:!0});var Nmt=s(ZO);_2r=r(Nmt,"FlaxBlenderbotForConditionalGeneration"),Nmt.forEach(t),u2r=r(gLe," (Blenderbot model)"),gLe.forEach(t),b2r=i(eo),iC=n(eo,"LI",{});var hLe=s(iC);H1e=n(hLe,"STRONG",{});var Dmt=s(H1e);v2r=r(Dmt,"blenderbot-small"),Dmt.forEach(t),T2r=r(hLe," \u2014 "),eX=n(hLe,"A",{href:!0});var qmt=s(eX);F2r=r(qmt,"FlaxBlenderbotSmallForConditionalGeneration"),qmt.forEach(t),C2r=r(hLe," (BlenderbotSmall model)"),hLe.forEach(t),M2r=i(eo),dC=n(eo,"LI",{});var pLe=s(dC);U1e=n(pLe,"STRONG",{});var Gmt=s(U1e);E2r=r(Gmt,"encoder-decoder"),Gmt.forEach(t),y2r=r(pLe," \u2014 "),oX=n(pLe,"A",{href:!0});var Omt=s(oX);w2r=r(Omt,"FlaxEncoderDecoderModel"),Omt.forEach(t),A2r=r(pLe," (Encoder decoder model)"),pLe.forEach(t),L2r=i(eo),cC=n(eo,"LI",{});var _Le=s(cC);J1e=n(_Le,"STRONG",{});var Xmt=s(J1e);B2r=r(Xmt,"marian"),Xmt.forEach(t),k2r=r(_Le," \u2014 "),rX=n(_Le,"A",{href:!0});var zmt=s(rX);x2r=r(zmt,"FlaxMarianMTModel"),zmt.forEach(t),R2r=r(_Le," (Marian model)"),_Le.forEach(t),S2r=i(eo),fC=n(eo,"LI",{});var uLe=s(fC);Y1e=n(uLe,"STRONG",{});var Vmt=s(Y1e);P2r=r(Vmt,"mbart"),Vmt.forEach(t),$2r=r(uLe," \u2014 "),tX=n(uLe,"A",{href:!0});var Wmt=s(tX);I2r=r(Wmt,"FlaxMBartForConditionalGeneration"),Wmt.forEach(t),j2r=r(uLe," (mBART model)"),uLe.forEach(t),N2r=i(eo),mC=n(eo,"LI",{});var bLe=s(mC);K1e=n(bLe,"STRONG",{});var Qmt=s(K1e);D2r=r(Qmt,"mt5"),Qmt.forEach(t),q2r=r(bLe," \u2014 "),aX=n(bLe,"A",{href:!0});var Hmt=s(aX);G2r=r(Hmt,"FlaxMT5ForConditionalGeneration"),Hmt.forEach(t),O2r=r(bLe," (mT5 model)"),bLe.forEach(t),X2r=i(eo),gC=n(eo,"LI",{});var vLe=s(gC);Z1e=n(vLe,"STRONG",{});var Umt=s(Z1e);z2r=r(Umt,"pegasus"),Umt.forEach(t),V2r=r(vLe," \u2014 "),nX=n(vLe,"A",{href:!0});var Jmt=s(nX);W2r=r(Jmt,"FlaxPegasusForConditionalGeneration"),Jmt.forEach(t),Q2r=r(vLe," (Pegasus model)"),vLe.forEach(t),H2r=i(eo),hC=n(eo,"LI",{});var TLe=s(hC);e7e=n(TLe,"STRONG",{});var Ymt=s(e7e);U2r=r(Ymt,"t5"),Ymt.forEach(t),J2r=r(TLe," \u2014 "),sX=n(TLe,"A",{href:!0});var Kmt=s(sX);Y2r=r(Kmt,"FlaxT5ForConditionalGeneration"),Kmt.forEach(t),K2r=r(TLe," (T5 model)"),TLe.forEach(t),eo.forEach(t),Z2r=i(Aa),o7e=n(Aa,"P",{});var Zmt=s(o7e);evr=r(Zmt,"Examples:"),Zmt.forEach(t),ovr=i(Aa),m(W6.$$.fragment,Aa),Aa.forEach(t),di.forEach(t),O9e=i(d),of=n(d,"H2",{class:!0});var Yke=s(of);pC=n(Yke,"A",{id:!0,class:!0,href:!0});var egt=s(pC);r7e=n(egt,"SPAN",{});var ogt=s(r7e);m(Q6.$$.fragment,ogt),ogt.forEach(t),egt.forEach(t),rvr=i(Yke),t7e=n(Yke,"SPAN",{});var rgt=s(t7e);tvr=r(rgt,"FlaxAutoModelForSequenceClassification"),rgt.forEach(t),Yke.forEach(t),X9e=i(d),xr=n(d,"DIV",{class:!0});var fi=s(xr);m(H6.$$.fragment,fi),avr=i(fi),rf=n(fi,"P",{});var FV=s(rf);nvr=r(FV,`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence classification head) when created
with the `),a7e=n(FV,"CODE",{});var tgt=s(a7e);svr=r(tgt,"from_pretrained()"),tgt.forEach(t),lvr=r(FV,"class method or the "),n7e=n(FV,"CODE",{});var agt=s(n7e);ivr=r(agt,"from_config()"),agt.forEach(t),dvr=r(FV,`class
method.`),FV.forEach(t),cvr=i(fi),U6=n(fi,"P",{});var Kke=s(U6);fvr=r(Kke,"This class cannot be instantiated directly using "),s7e=n(Kke,"CODE",{});var ngt=s(s7e);mvr=r(ngt,"__init__()"),ngt.forEach(t),gvr=r(Kke," (throws an error)."),Kke.forEach(t),hvr=i(fi),Et=n(fi,"DIV",{class:!0});var mi=s(Et);m(J6.$$.fragment,mi),pvr=i(mi),l7e=n(mi,"P",{});var sgt=s(l7e);_vr=r(sgt,"Instantiates one of the model classes of the library (with a sequence classification head) from a configuration."),sgt.forEach(t),uvr=i(mi),tf=n(mi,"P",{});var CV=s(tf);bvr=r(CV,`Note:
Loading a model from its configuration file does `),i7e=n(CV,"STRONG",{});var lgt=s(i7e);vvr=r(lgt,"not"),lgt.forEach(t),Tvr=r(CV,` load the model weights. It only affects the
model\u2019s configuration. Use `),d7e=n(CV,"CODE",{});var igt=s(d7e);Fvr=r(igt,"from_pretrained()"),igt.forEach(t),Cvr=r(CV,"to load the model weights."),CV.forEach(t),Mvr=i(mi),c7e=n(mi,"P",{});var dgt=s(c7e);Evr=r(dgt,"Examples:"),dgt.forEach(t),yvr=i(mi),m(Y6.$$.fragment,mi),mi.forEach(t),wvr=i(fi),xo=n(fi,"DIV",{class:!0});var La=s(xo);m(K6.$$.fragment,La),Avr=i(La),f7e=n(La,"P",{});var cgt=s(f7e);Lvr=r(cgt,"Instantiate one of the model classes of the library (with a sequence classification head) from a pretrained model."),cgt.forEach(t),Bvr=i(La),An=n(La,"P",{});var HM=s(An);kvr=r(HM,"The model class to instantiate is selected based on the "),m7e=n(HM,"CODE",{});var fgt=s(m7e);xvr=r(fgt,"model_type"),fgt.forEach(t),Rvr=r(HM,` property of the config object (either
passed as an argument or loaded from `),g7e=n(HM,"CODE",{});var mgt=s(g7e);Svr=r(mgt,"pretrained_model_name_or_path"),mgt.forEach(t),Pvr=r(HM,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),h7e=n(HM,"CODE",{});var ggt=s(h7e);$vr=r(ggt,"pretrained_model_name_or_path"),ggt.forEach(t),Ivr=r(HM,":"),HM.forEach(t),jvr=i(La),Fe=n(La,"UL",{});var oo=s(Fe);_C=n(oo,"LI",{});var FLe=s(_C);p7e=n(FLe,"STRONG",{});var hgt=s(p7e);Nvr=r(hgt,"albert"),hgt.forEach(t),Dvr=r(FLe," \u2014 "),lX=n(FLe,"A",{href:!0});var pgt=s(lX);qvr=r(pgt,"FlaxAlbertForSequenceClassification"),pgt.forEach(t),Gvr=r(FLe," (ALBERT model)"),FLe.forEach(t),Ovr=i(oo),uC=n(oo,"LI",{});var CLe=s(uC);_7e=n(CLe,"STRONG",{});var _gt=s(_7e);Xvr=r(_gt,"bart"),_gt.forEach(t),zvr=r(CLe," \u2014 "),iX=n(CLe,"A",{href:!0});var ugt=s(iX);Vvr=r(ugt,"FlaxBartForSequenceClassification"),ugt.forEach(t),Wvr=r(CLe," (BART model)"),CLe.forEach(t),Qvr=i(oo),bC=n(oo,"LI",{});var MLe=s(bC);u7e=n(MLe,"STRONG",{});var bgt=s(u7e);Hvr=r(bgt,"bert"),bgt.forEach(t),Uvr=r(MLe," \u2014 "),dX=n(MLe,"A",{href:!0});var vgt=s(dX);Jvr=r(vgt,"FlaxBertForSequenceClassification"),vgt.forEach(t),Yvr=r(MLe," (BERT model)"),MLe.forEach(t),Kvr=i(oo),vC=n(oo,"LI",{});var ELe=s(vC);b7e=n(ELe,"STRONG",{});var Tgt=s(b7e);Zvr=r(Tgt,"big_bird"),Tgt.forEach(t),eTr=r(ELe," \u2014 "),cX=n(ELe,"A",{href:!0});var Fgt=s(cX);oTr=r(Fgt,"FlaxBigBirdForSequenceClassification"),Fgt.forEach(t),rTr=r(ELe," (BigBird model)"),ELe.forEach(t),tTr=i(oo),TC=n(oo,"LI",{});var yLe=s(TC);v7e=n(yLe,"STRONG",{});var Cgt=s(v7e);aTr=r(Cgt,"distilbert"),Cgt.forEach(t),nTr=r(yLe," \u2014 "),fX=n(yLe,"A",{href:!0});var Mgt=s(fX);sTr=r(Mgt,"FlaxDistilBertForSequenceClassification"),Mgt.forEach(t),lTr=r(yLe," (DistilBERT model)"),yLe.forEach(t),iTr=i(oo),FC=n(oo,"LI",{});var wLe=s(FC);T7e=n(wLe,"STRONG",{});var Egt=s(T7e);dTr=r(Egt,"electra"),Egt.forEach(t),cTr=r(wLe," \u2014 "),mX=n(wLe,"A",{href:!0});var ygt=s(mX);fTr=r(ygt,"FlaxElectraForSequenceClassification"),ygt.forEach(t),mTr=r(wLe," (ELECTRA model)"),wLe.forEach(t),gTr=i(oo),CC=n(oo,"LI",{});var ALe=s(CC);F7e=n(ALe,"STRONG",{});var wgt=s(F7e);hTr=r(wgt,"mbart"),wgt.forEach(t),pTr=r(ALe," \u2014 "),gX=n(ALe,"A",{href:!0});var Agt=s(gX);_Tr=r(Agt,"FlaxMBartForSequenceClassification"),Agt.forEach(t),uTr=r(ALe," (mBART model)"),ALe.forEach(t),bTr=i(oo),MC=n(oo,"LI",{});var LLe=s(MC);C7e=n(LLe,"STRONG",{});var Lgt=s(C7e);vTr=r(Lgt,"roberta"),Lgt.forEach(t),TTr=r(LLe," \u2014 "),hX=n(LLe,"A",{href:!0});var Bgt=s(hX);FTr=r(Bgt,"FlaxRobertaForSequenceClassification"),Bgt.forEach(t),CTr=r(LLe," (RoBERTa model)"),LLe.forEach(t),MTr=i(oo),EC=n(oo,"LI",{});var BLe=s(EC);M7e=n(BLe,"STRONG",{});var kgt=s(M7e);ETr=r(kgt,"roformer"),kgt.forEach(t),yTr=r(BLe," \u2014 "),pX=n(BLe,"A",{href:!0});var xgt=s(pX);wTr=r(xgt,"FlaxRoFormerForSequenceClassification"),xgt.forEach(t),ATr=r(BLe," (RoFormer model)"),BLe.forEach(t),oo.forEach(t),LTr=i(La),E7e=n(La,"P",{});var Rgt=s(E7e);BTr=r(Rgt,"Examples:"),Rgt.forEach(t),kTr=i(La),m(Z6.$$.fragment,La),La.forEach(t),fi.forEach(t),z9e=i(d),af=n(d,"H2",{class:!0});var Zke=s(af);yC=n(Zke,"A",{id:!0,class:!0,href:!0});var Sgt=s(yC);y7e=n(Sgt,"SPAN",{});var Pgt=s(y7e);m(e0.$$.fragment,Pgt),Pgt.forEach(t),Sgt.forEach(t),xTr=i(Zke),w7e=n(Zke,"SPAN",{});var $gt=s(w7e);RTr=r($gt,"FlaxAutoModelForQuestionAnswering"),$gt.forEach(t),Zke.forEach(t),V9e=i(d),Rr=n(d,"DIV",{class:!0});var gi=s(Rr);m(o0.$$.fragment,gi),STr=i(gi),nf=n(gi,"P",{});var MV=s(nf);PTr=r(MV,`This is a generic model class that will be instantiated as one of the model classes of the library (with a question answering head) when created
with the `),A7e=n(MV,"CODE",{});var Igt=s(A7e);$Tr=r(Igt,"from_pretrained()"),Igt.forEach(t),ITr=r(MV,"class method or the "),L7e=n(MV,"CODE",{});var jgt=s(L7e);jTr=r(jgt,"from_config()"),jgt.forEach(t),NTr=r(MV,`class
method.`),MV.forEach(t),DTr=i(gi),r0=n(gi,"P",{});var exe=s(r0);qTr=r(exe,"This class cannot be instantiated directly using "),B7e=n(exe,"CODE",{});var Ngt=s(B7e);GTr=r(Ngt,"__init__()"),Ngt.forEach(t),OTr=r(exe," (throws an error)."),exe.forEach(t),XTr=i(gi),yt=n(gi,"DIV",{class:!0});var hi=s(yt);m(t0.$$.fragment,hi),zTr=i(hi),k7e=n(hi,"P",{});var Dgt=s(k7e);VTr=r(Dgt,"Instantiates one of the model classes of the library (with a question answering head) from a configuration."),Dgt.forEach(t),WTr=i(hi),sf=n(hi,"P",{});var EV=s(sf);QTr=r(EV,`Note:
Loading a model from its configuration file does `),x7e=n(EV,"STRONG",{});var qgt=s(x7e);HTr=r(qgt,"not"),qgt.forEach(t),UTr=r(EV,` load the model weights. It only affects the
model\u2019s configuration. Use `),R7e=n(EV,"CODE",{});var Ggt=s(R7e);JTr=r(Ggt,"from_pretrained()"),Ggt.forEach(t),YTr=r(EV,"to load the model weights."),EV.forEach(t),KTr=i(hi),S7e=n(hi,"P",{});var Ogt=s(S7e);ZTr=r(Ogt,"Examples:"),Ogt.forEach(t),eFr=i(hi),m(a0.$$.fragment,hi),hi.forEach(t),oFr=i(gi),Ro=n(gi,"DIV",{class:!0});var Ba=s(Ro);m(n0.$$.fragment,Ba),rFr=i(Ba),P7e=n(Ba,"P",{});var Xgt=s(P7e);tFr=r(Xgt,"Instantiate one of the model classes of the library (with a question answering head) from a pretrained model."),Xgt.forEach(t),aFr=i(Ba),Ln=n(Ba,"P",{});var UM=s(Ln);nFr=r(UM,"The model class to instantiate is selected based on the "),$7e=n(UM,"CODE",{});var zgt=s($7e);sFr=r(zgt,"model_type"),zgt.forEach(t),lFr=r(UM,` property of the config object (either
passed as an argument or loaded from `),I7e=n(UM,"CODE",{});var Vgt=s(I7e);iFr=r(Vgt,"pretrained_model_name_or_path"),Vgt.forEach(t),dFr=r(UM,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),j7e=n(UM,"CODE",{});var Wgt=s(j7e);cFr=r(Wgt,"pretrained_model_name_or_path"),Wgt.forEach(t),fFr=r(UM,":"),UM.forEach(t),mFr=i(Ba),Ce=n(Ba,"UL",{});var ro=s(Ce);wC=n(ro,"LI",{});var kLe=s(wC);N7e=n(kLe,"STRONG",{});var Qgt=s(N7e);gFr=r(Qgt,"albert"),Qgt.forEach(t),hFr=r(kLe," \u2014 "),_X=n(kLe,"A",{href:!0});var Hgt=s(_X);pFr=r(Hgt,"FlaxAlbertForQuestionAnswering"),Hgt.forEach(t),_Fr=r(kLe," (ALBERT model)"),kLe.forEach(t),uFr=i(ro),AC=n(ro,"LI",{});var xLe=s(AC);D7e=n(xLe,"STRONG",{});var Ugt=s(D7e);bFr=r(Ugt,"bart"),Ugt.forEach(t),vFr=r(xLe," \u2014 "),uX=n(xLe,"A",{href:!0});var Jgt=s(uX);TFr=r(Jgt,"FlaxBartForQuestionAnswering"),Jgt.forEach(t),FFr=r(xLe," (BART model)"),xLe.forEach(t),CFr=i(ro),LC=n(ro,"LI",{});var RLe=s(LC);q7e=n(RLe,"STRONG",{});var Ygt=s(q7e);MFr=r(Ygt,"bert"),Ygt.forEach(t),EFr=r(RLe," \u2014 "),bX=n(RLe,"A",{href:!0});var Kgt=s(bX);yFr=r(Kgt,"FlaxBertForQuestionAnswering"),Kgt.forEach(t),wFr=r(RLe," (BERT model)"),RLe.forEach(t),AFr=i(ro),BC=n(ro,"LI",{});var SLe=s(BC);G7e=n(SLe,"STRONG",{});var Zgt=s(G7e);LFr=r(Zgt,"big_bird"),Zgt.forEach(t),BFr=r(SLe," \u2014 "),vX=n(SLe,"A",{href:!0});var eht=s(vX);kFr=r(eht,"FlaxBigBirdForQuestionAnswering"),eht.forEach(t),xFr=r(SLe," (BigBird model)"),SLe.forEach(t),RFr=i(ro),kC=n(ro,"LI",{});var PLe=s(kC);O7e=n(PLe,"STRONG",{});var oht=s(O7e);SFr=r(oht,"distilbert"),oht.forEach(t),PFr=r(PLe," \u2014 "),TX=n(PLe,"A",{href:!0});var rht=s(TX);$Fr=r(rht,"FlaxDistilBertForQuestionAnswering"),rht.forEach(t),IFr=r(PLe," (DistilBERT model)"),PLe.forEach(t),jFr=i(ro),xC=n(ro,"LI",{});var $Le=s(xC);X7e=n($Le,"STRONG",{});var tht=s(X7e);NFr=r(tht,"electra"),tht.forEach(t),DFr=r($Le," \u2014 "),FX=n($Le,"A",{href:!0});var aht=s(FX);qFr=r(aht,"FlaxElectraForQuestionAnswering"),aht.forEach(t),GFr=r($Le," (ELECTRA model)"),$Le.forEach(t),OFr=i(ro),RC=n(ro,"LI",{});var ILe=s(RC);z7e=n(ILe,"STRONG",{});var nht=s(z7e);XFr=r(nht,"mbart"),nht.forEach(t),zFr=r(ILe," \u2014 "),CX=n(ILe,"A",{href:!0});var sht=s(CX);VFr=r(sht,"FlaxMBartForQuestionAnswering"),sht.forEach(t),WFr=r(ILe," (mBART model)"),ILe.forEach(t),QFr=i(ro),SC=n(ro,"LI",{});var jLe=s(SC);V7e=n(jLe,"STRONG",{});var lht=s(V7e);HFr=r(lht,"roberta"),lht.forEach(t),UFr=r(jLe," \u2014 "),MX=n(jLe,"A",{href:!0});var iht=s(MX);JFr=r(iht,"FlaxRobertaForQuestionAnswering"),iht.forEach(t),YFr=r(jLe," (RoBERTa model)"),jLe.forEach(t),KFr=i(ro),PC=n(ro,"LI",{});var NLe=s(PC);W7e=n(NLe,"STRONG",{});var dht=s(W7e);ZFr=r(dht,"roformer"),dht.forEach(t),eCr=r(NLe," \u2014 "),EX=n(NLe,"A",{href:!0});var cht=s(EX);oCr=r(cht,"FlaxRoFormerForQuestionAnswering"),cht.forEach(t),rCr=r(NLe," (RoFormer model)"),NLe.forEach(t),ro.forEach(t),tCr=i(Ba),Q7e=n(Ba,"P",{});var fht=s(Q7e);aCr=r(fht,"Examples:"),fht.forEach(t),nCr=i(Ba),m(s0.$$.fragment,Ba),Ba.forEach(t),gi.forEach(t),W9e=i(d),lf=n(d,"H2",{class:!0});var oxe=s(lf);$C=n(oxe,"A",{id:!0,class:!0,href:!0});var mht=s($C);H7e=n(mht,"SPAN",{});var ght=s(H7e);m(l0.$$.fragment,ght),ght.forEach(t),mht.forEach(t),sCr=i(oxe),U7e=n(oxe,"SPAN",{});var hht=s(U7e);lCr=r(hht,"FlaxAutoModelForTokenClassification"),hht.forEach(t),oxe.forEach(t),Q9e=i(d),Sr=n(d,"DIV",{class:!0});var pi=s(Sr);m(i0.$$.fragment,pi),iCr=i(pi),df=n(pi,"P",{});var yV=s(df);dCr=r(yV,`This is a generic model class that will be instantiated as one of the model classes of the library (with a token classification head) when created
with the `),J7e=n(yV,"CODE",{});var pht=s(J7e);cCr=r(pht,"from_pretrained()"),pht.forEach(t),fCr=r(yV,"class method or the "),Y7e=n(yV,"CODE",{});var _ht=s(Y7e);mCr=r(_ht,"from_config()"),_ht.forEach(t),gCr=r(yV,`class
method.`),yV.forEach(t),hCr=i(pi),d0=n(pi,"P",{});var rxe=s(d0);pCr=r(rxe,"This class cannot be instantiated directly using "),K7e=n(rxe,"CODE",{});var uht=s(K7e);_Cr=r(uht,"__init__()"),uht.forEach(t),uCr=r(rxe," (throws an error)."),rxe.forEach(t),bCr=i(pi),wt=n(pi,"DIV",{class:!0});var _i=s(wt);m(c0.$$.fragment,_i),vCr=i(_i),Z7e=n(_i,"P",{});var bht=s(Z7e);TCr=r(bht,"Instantiates one of the model classes of the library (with a token classification head) from a configuration."),bht.forEach(t),FCr=i(_i),cf=n(_i,"P",{});var wV=s(cf);CCr=r(wV,`Note:
Loading a model from its configuration file does `),e4e=n(wV,"STRONG",{});var vht=s(e4e);MCr=r(vht,"not"),vht.forEach(t),ECr=r(wV,` load the model weights. It only affects the
model\u2019s configuration. Use `),o4e=n(wV,"CODE",{});var Tht=s(o4e);yCr=r(Tht,"from_pretrained()"),Tht.forEach(t),wCr=r(wV,"to load the model weights."),wV.forEach(t),ACr=i(_i),r4e=n(_i,"P",{});var Fht=s(r4e);LCr=r(Fht,"Examples:"),Fht.forEach(t),BCr=i(_i),m(f0.$$.fragment,_i),_i.forEach(t),kCr=i(pi),So=n(pi,"DIV",{class:!0});var ka=s(So);m(m0.$$.fragment,ka),xCr=i(ka),t4e=n(ka,"P",{});var Cht=s(t4e);RCr=r(Cht,"Instantiate one of the model classes of the library (with a token classification head) from a pretrained model."),Cht.forEach(t),SCr=i(ka),Bn=n(ka,"P",{});var JM=s(Bn);PCr=r(JM,"The model class to instantiate is selected based on the "),a4e=n(JM,"CODE",{});var Mht=s(a4e);$Cr=r(Mht,"model_type"),Mht.forEach(t),ICr=r(JM,` property of the config object (either
passed as an argument or loaded from `),n4e=n(JM,"CODE",{});var Eht=s(n4e);jCr=r(Eht,"pretrained_model_name_or_path"),Eht.forEach(t),NCr=r(JM,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),s4e=n(JM,"CODE",{});var yht=s(s4e);DCr=r(yht,"pretrained_model_name_or_path"),yht.forEach(t),qCr=r(JM,":"),JM.forEach(t),GCr=i(ka),so=n(ka,"UL",{});var ta=s(so);IC=n(ta,"LI",{});var DLe=s(IC);l4e=n(DLe,"STRONG",{});var wht=s(l4e);OCr=r(wht,"albert"),wht.forEach(t),XCr=r(DLe," \u2014 "),yX=n(DLe,"A",{href:!0});var Aht=s(yX);zCr=r(Aht,"FlaxAlbertForTokenClassification"),Aht.forEach(t),VCr=r(DLe," (ALBERT model)"),DLe.forEach(t),WCr=i(ta),jC=n(ta,"LI",{});var qLe=s(jC);i4e=n(qLe,"STRONG",{});var Lht=s(i4e);QCr=r(Lht,"bert"),Lht.forEach(t),HCr=r(qLe," \u2014 "),wX=n(qLe,"A",{href:!0});var Bht=s(wX);UCr=r(Bht,"FlaxBertForTokenClassification"),Bht.forEach(t),JCr=r(qLe," (BERT model)"),qLe.forEach(t),YCr=i(ta),NC=n(ta,"LI",{});var GLe=s(NC);d4e=n(GLe,"STRONG",{});var kht=s(d4e);KCr=r(kht,"big_bird"),kht.forEach(t),ZCr=r(GLe," \u2014 "),AX=n(GLe,"A",{href:!0});var xht=s(AX);eMr=r(xht,"FlaxBigBirdForTokenClassification"),xht.forEach(t),oMr=r(GLe," (BigBird model)"),GLe.forEach(t),rMr=i(ta),DC=n(ta,"LI",{});var OLe=s(DC);c4e=n(OLe,"STRONG",{});var Rht=s(c4e);tMr=r(Rht,"distilbert"),Rht.forEach(t),aMr=r(OLe," \u2014 "),LX=n(OLe,"A",{href:!0});var Sht=s(LX);nMr=r(Sht,"FlaxDistilBertForTokenClassification"),Sht.forEach(t),sMr=r(OLe," (DistilBERT model)"),OLe.forEach(t),lMr=i(ta),qC=n(ta,"LI",{});var XLe=s(qC);f4e=n(XLe,"STRONG",{});var Pht=s(f4e);iMr=r(Pht,"electra"),Pht.forEach(t),dMr=r(XLe," \u2014 "),BX=n(XLe,"A",{href:!0});var $ht=s(BX);cMr=r($ht,"FlaxElectraForTokenClassification"),$ht.forEach(t),fMr=r(XLe," (ELECTRA model)"),XLe.forEach(t),mMr=i(ta),GC=n(ta,"LI",{});var zLe=s(GC);m4e=n(zLe,"STRONG",{});var Iht=s(m4e);gMr=r(Iht,"roberta"),Iht.forEach(t),hMr=r(zLe," \u2014 "),kX=n(zLe,"A",{href:!0});var jht=s(kX);pMr=r(jht,"FlaxRobertaForTokenClassification"),jht.forEach(t),_Mr=r(zLe," (RoBERTa model)"),zLe.forEach(t),uMr=i(ta),OC=n(ta,"LI",{});var VLe=s(OC);g4e=n(VLe,"STRONG",{});var Nht=s(g4e);bMr=r(Nht,"roformer"),Nht.forEach(t),vMr=r(VLe," \u2014 "),xX=n(VLe,"A",{href:!0});var Dht=s(xX);TMr=r(Dht,"FlaxRoFormerForTokenClassification"),Dht.forEach(t),FMr=r(VLe," (RoFormer model)"),VLe.forEach(t),ta.forEach(t),CMr=i(ka),h4e=n(ka,"P",{});var qht=s(h4e);MMr=r(qht,"Examples:"),qht.forEach(t),EMr=i(ka),m(g0.$$.fragment,ka),ka.forEach(t),pi.forEach(t),H9e=i(d),ff=n(d,"H2",{class:!0});var txe=s(ff);XC=n(txe,"A",{id:!0,class:!0,href:!0});var Ght=s(XC);p4e=n(Ght,"SPAN",{});var Oht=s(p4e);m(h0.$$.fragment,Oht),Oht.forEach(t),Ght.forEach(t),yMr=i(txe),_4e=n(txe,"SPAN",{});var Xht=s(_4e);wMr=r(Xht,"FlaxAutoModelForMultipleChoice"),Xht.forEach(t),txe.forEach(t),U9e=i(d),Pr=n(d,"DIV",{class:!0});var ui=s(Pr);m(p0.$$.fragment,ui),AMr=i(ui),mf=n(ui,"P",{});var AV=s(mf);LMr=r(AV,`This is a generic model class that will be instantiated as one of the model classes of the library (with a multiple choice head) when created
with the `),u4e=n(AV,"CODE",{});var zht=s(u4e);BMr=r(zht,"from_pretrained()"),zht.forEach(t),kMr=r(AV,"class method or the "),b4e=n(AV,"CODE",{});var Vht=s(b4e);xMr=r(Vht,"from_config()"),Vht.forEach(t),RMr=r(AV,`class
method.`),AV.forEach(t),SMr=i(ui),_0=n(ui,"P",{});var axe=s(_0);PMr=r(axe,"This class cannot be instantiated directly using "),v4e=n(axe,"CODE",{});var Wht=s(v4e);$Mr=r(Wht,"__init__()"),Wht.forEach(t),IMr=r(axe," (throws an error)."),axe.forEach(t),jMr=i(ui),At=n(ui,"DIV",{class:!0});var bi=s(At);m(u0.$$.fragment,bi),NMr=i(bi),T4e=n(bi,"P",{});var Qht=s(T4e);DMr=r(Qht,"Instantiates one of the model classes of the library (with a multiple choice head) from a configuration."),Qht.forEach(t),qMr=i(bi),gf=n(bi,"P",{});var LV=s(gf);GMr=r(LV,`Note:
Loading a model from its configuration file does `),F4e=n(LV,"STRONG",{});var Hht=s(F4e);OMr=r(Hht,"not"),Hht.forEach(t),XMr=r(LV,` load the model weights. It only affects the
model\u2019s configuration. Use `),C4e=n(LV,"CODE",{});var Uht=s(C4e);zMr=r(Uht,"from_pretrained()"),Uht.forEach(t),VMr=r(LV,"to load the model weights."),LV.forEach(t),WMr=i(bi),M4e=n(bi,"P",{});var Jht=s(M4e);QMr=r(Jht,"Examples:"),Jht.forEach(t),HMr=i(bi),m(b0.$$.fragment,bi),bi.forEach(t),UMr=i(ui),Po=n(ui,"DIV",{class:!0});var xa=s(Po);m(v0.$$.fragment,xa),JMr=i(xa),E4e=n(xa,"P",{});var Yht=s(E4e);YMr=r(Yht,"Instantiate one of the model classes of the library (with a multiple choice head) from a pretrained model."),Yht.forEach(t),KMr=i(xa),kn=n(xa,"P",{});var YM=s(kn);ZMr=r(YM,"The model class to instantiate is selected based on the "),y4e=n(YM,"CODE",{});var Kht=s(y4e);eEr=r(Kht,"model_type"),Kht.forEach(t),oEr=r(YM,` property of the config object (either
passed as an argument or loaded from `),w4e=n(YM,"CODE",{});var Zht=s(w4e);rEr=r(Zht,"pretrained_model_name_or_path"),Zht.forEach(t),tEr=r(YM,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),A4e=n(YM,"CODE",{});var ept=s(A4e);aEr=r(ept,"pretrained_model_name_or_path"),ept.forEach(t),nEr=r(YM,":"),YM.forEach(t),sEr=i(xa),lo=n(xa,"UL",{});var aa=s(lo);zC=n(aa,"LI",{});var WLe=s(zC);L4e=n(WLe,"STRONG",{});var opt=s(L4e);lEr=r(opt,"albert"),opt.forEach(t),iEr=r(WLe," \u2014 "),RX=n(WLe,"A",{href:!0});var rpt=s(RX);dEr=r(rpt,"FlaxAlbertForMultipleChoice"),rpt.forEach(t),cEr=r(WLe," (ALBERT model)"),WLe.forEach(t),fEr=i(aa),VC=n(aa,"LI",{});var QLe=s(VC);B4e=n(QLe,"STRONG",{});var tpt=s(B4e);mEr=r(tpt,"bert"),tpt.forEach(t),gEr=r(QLe," \u2014 "),SX=n(QLe,"A",{href:!0});var apt=s(SX);hEr=r(apt,"FlaxBertForMultipleChoice"),apt.forEach(t),pEr=r(QLe," (BERT model)"),QLe.forEach(t),_Er=i(aa),WC=n(aa,"LI",{});var HLe=s(WC);k4e=n(HLe,"STRONG",{});var npt=s(k4e);uEr=r(npt,"big_bird"),npt.forEach(t),bEr=r(HLe," \u2014 "),PX=n(HLe,"A",{href:!0});var spt=s(PX);vEr=r(spt,"FlaxBigBirdForMultipleChoice"),spt.forEach(t),TEr=r(HLe," (BigBird model)"),HLe.forEach(t),FEr=i(aa),QC=n(aa,"LI",{});var ULe=s(QC);x4e=n(ULe,"STRONG",{});var lpt=s(x4e);CEr=r(lpt,"distilbert"),lpt.forEach(t),MEr=r(ULe," \u2014 "),$X=n(ULe,"A",{href:!0});var ipt=s($X);EEr=r(ipt,"FlaxDistilBertForMultipleChoice"),ipt.forEach(t),yEr=r(ULe," (DistilBERT model)"),ULe.forEach(t),wEr=i(aa),HC=n(aa,"LI",{});var JLe=s(HC);R4e=n(JLe,"STRONG",{});var dpt=s(R4e);AEr=r(dpt,"electra"),dpt.forEach(t),LEr=r(JLe," \u2014 "),IX=n(JLe,"A",{href:!0});var cpt=s(IX);BEr=r(cpt,"FlaxElectraForMultipleChoice"),cpt.forEach(t),kEr=r(JLe," (ELECTRA model)"),JLe.forEach(t),xEr=i(aa),UC=n(aa,"LI",{});var YLe=s(UC);S4e=n(YLe,"STRONG",{});var fpt=s(S4e);REr=r(fpt,"roberta"),fpt.forEach(t),SEr=r(YLe," \u2014 "),jX=n(YLe,"A",{href:!0});var mpt=s(jX);PEr=r(mpt,"FlaxRobertaForMultipleChoice"),mpt.forEach(t),$Er=r(YLe," (RoBERTa model)"),YLe.forEach(t),IEr=i(aa),JC=n(aa,"LI",{});var KLe=s(JC);P4e=n(KLe,"STRONG",{});var gpt=s(P4e);jEr=r(gpt,"roformer"),gpt.forEach(t),NEr=r(KLe," \u2014 "),NX=n(KLe,"A",{href:!0});var hpt=s(NX);DEr=r(hpt,"FlaxRoFormerForMultipleChoice"),hpt.forEach(t),qEr=r(KLe," (RoFormer model)"),KLe.forEach(t),aa.forEach(t),GEr=i(xa),$4e=n(xa,"P",{});var ppt=s($4e);OEr=r(ppt,"Examples:"),ppt.forEach(t),XEr=i(xa),m(T0.$$.fragment,xa),xa.forEach(t),ui.forEach(t),J9e=i(d),hf=n(d,"H2",{class:!0});var nxe=s(hf);YC=n(nxe,"A",{id:!0,class:!0,href:!0});var _pt=s(YC);I4e=n(_pt,"SPAN",{});var upt=s(I4e);m(F0.$$.fragment,upt),upt.forEach(t),_pt.forEach(t),zEr=i(nxe),j4e=n(nxe,"SPAN",{});var bpt=s(j4e);VEr=r(bpt,"FlaxAutoModelForNextSentencePrediction"),bpt.forEach(t),nxe.forEach(t),Y9e=i(d),$r=n(d,"DIV",{class:!0});var vi=s($r);m(C0.$$.fragment,vi),WEr=i(vi),pf=n(vi,"P",{});var BV=s(pf);QEr=r(BV,`This is a generic model class that will be instantiated as one of the model classes of the library (with a next sentence prediction head) when created
with the `),N4e=n(BV,"CODE",{});var vpt=s(N4e);HEr=r(vpt,"from_pretrained()"),vpt.forEach(t),UEr=r(BV,"class method or the "),D4e=n(BV,"CODE",{});var Tpt=s(D4e);JEr=r(Tpt,"from_config()"),Tpt.forEach(t),YEr=r(BV,`class
method.`),BV.forEach(t),KEr=i(vi),M0=n(vi,"P",{});var sxe=s(M0);ZEr=r(sxe,"This class cannot be instantiated directly using "),q4e=n(sxe,"CODE",{});var Fpt=s(q4e);e3r=r(Fpt,"__init__()"),Fpt.forEach(t),o3r=r(sxe," (throws an error)."),sxe.forEach(t),r3r=i(vi),Lt=n(vi,"DIV",{class:!0});var Ti=s(Lt);m(E0.$$.fragment,Ti),t3r=i(Ti),G4e=n(Ti,"P",{});var Cpt=s(G4e);a3r=r(Cpt,"Instantiates one of the model classes of the library (with a next sentence prediction head) from a configuration."),Cpt.forEach(t),n3r=i(Ti),_f=n(Ti,"P",{});var kV=s(_f);s3r=r(kV,`Note:
Loading a model from its configuration file does `),O4e=n(kV,"STRONG",{});var Mpt=s(O4e);l3r=r(Mpt,"not"),Mpt.forEach(t),i3r=r(kV,` load the model weights. It only affects the
model\u2019s configuration. Use `),X4e=n(kV,"CODE",{});var Ept=s(X4e);d3r=r(Ept,"from_pretrained()"),Ept.forEach(t),c3r=r(kV,"to load the model weights."),kV.forEach(t),f3r=i(Ti),z4e=n(Ti,"P",{});var ypt=s(z4e);m3r=r(ypt,"Examples:"),ypt.forEach(t),g3r=i(Ti),m(y0.$$.fragment,Ti),Ti.forEach(t),h3r=i(vi),$o=n(vi,"DIV",{class:!0});var Ra=s($o);m(w0.$$.fragment,Ra),p3r=i(Ra),V4e=n(Ra,"P",{});var wpt=s(V4e);_3r=r(wpt,"Instantiate one of the model classes of the library (with a next sentence prediction head) from a pretrained model."),wpt.forEach(t),u3r=i(Ra),xn=n(Ra,"P",{});var KM=s(xn);b3r=r(KM,"The model class to instantiate is selected based on the "),W4e=n(KM,"CODE",{});var Apt=s(W4e);v3r=r(Apt,"model_type"),Apt.forEach(t),T3r=r(KM,` property of the config object (either
passed as an argument or loaded from `),Q4e=n(KM,"CODE",{});var Lpt=s(Q4e);F3r=r(Lpt,"pretrained_model_name_or_path"),Lpt.forEach(t),C3r=r(KM,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),H4e=n(KM,"CODE",{});var Bpt=s(H4e);M3r=r(Bpt,"pretrained_model_name_or_path"),Bpt.forEach(t),E3r=r(KM,":"),KM.forEach(t),y3r=i(Ra),U4e=n(Ra,"UL",{});var kpt=s(U4e);KC=n(kpt,"LI",{});var ZLe=s(KC);J4e=n(ZLe,"STRONG",{});var xpt=s(J4e);w3r=r(xpt,"bert"),xpt.forEach(t),A3r=r(ZLe," \u2014 "),DX=n(ZLe,"A",{href:!0});var Rpt=s(DX);L3r=r(Rpt,"FlaxBertForNextSentencePrediction"),Rpt.forEach(t),B3r=r(ZLe," (BERT model)"),ZLe.forEach(t),kpt.forEach(t),k3r=i(Ra),Y4e=n(Ra,"P",{});var Spt=s(Y4e);x3r=r(Spt,"Examples:"),Spt.forEach(t),R3r=i(Ra),m(A0.$$.fragment,Ra),Ra.forEach(t),vi.forEach(t),K9e=i(d),uf=n(d,"H2",{class:!0});var lxe=s(uf);ZC=n(lxe,"A",{id:!0,class:!0,href:!0});var Ppt=s(ZC);K4e=n(Ppt,"SPAN",{});var $pt=s(K4e);m(L0.$$.fragment,$pt),$pt.forEach(t),Ppt.forEach(t),S3r=i(lxe),Z4e=n(lxe,"SPAN",{});var Ipt=s(Z4e);P3r=r(Ipt,"FlaxAutoModelForImageClassification"),Ipt.forEach(t),lxe.forEach(t),Z9e=i(d),Ir=n(d,"DIV",{class:!0});var Fi=s(Ir);m(B0.$$.fragment,Fi),$3r=i(Fi),bf=n(Fi,"P",{});var xV=s(bf);I3r=r(xV,`This is a generic model class that will be instantiated as one of the model classes of the library (with a image classification head) when created
with the `),ebe=n(xV,"CODE",{});var jpt=s(ebe);j3r=r(jpt,"from_pretrained()"),jpt.forEach(t),N3r=r(xV,"class method or the "),obe=n(xV,"CODE",{});var Npt=s(obe);D3r=r(Npt,"from_config()"),Npt.forEach(t),q3r=r(xV,`class
method.`),xV.forEach(t),G3r=i(Fi),k0=n(Fi,"P",{});var ixe=s(k0);O3r=r(ixe,"This class cannot be instantiated directly using "),rbe=n(ixe,"CODE",{});var Dpt=s(rbe);X3r=r(Dpt,"__init__()"),Dpt.forEach(t),z3r=r(ixe," (throws an error)."),ixe.forEach(t),V3r=i(Fi),Bt=n(Fi,"DIV",{class:!0});var Ci=s(Bt);m(x0.$$.fragment,Ci),W3r=i(Ci),tbe=n(Ci,"P",{});var qpt=s(tbe);Q3r=r(qpt,"Instantiates one of the model classes of the library (with a image classification head) from a configuration."),qpt.forEach(t),H3r=i(Ci),vf=n(Ci,"P",{});var RV=s(vf);U3r=r(RV,`Note:
Loading a model from its configuration file does `),abe=n(RV,"STRONG",{});var Gpt=s(abe);J3r=r(Gpt,"not"),Gpt.forEach(t),Y3r=r(RV,` load the model weights. It only affects the
model\u2019s configuration. Use `),nbe=n(RV,"CODE",{});var Opt=s(nbe);K3r=r(Opt,"from_pretrained()"),Opt.forEach(t),Z3r=r(RV,"to load the model weights."),RV.forEach(t),eyr=i(Ci),sbe=n(Ci,"P",{});var Xpt=s(sbe);oyr=r(Xpt,"Examples:"),Xpt.forEach(t),ryr=i(Ci),m(R0.$$.fragment,Ci),Ci.forEach(t),tyr=i(Fi),Io=n(Fi,"DIV",{class:!0});var Sa=s(Io);m(S0.$$.fragment,Sa),ayr=i(Sa),lbe=n(Sa,"P",{});var zpt=s(lbe);nyr=r(zpt,"Instantiate one of the model classes of the library (with a image classification head) from a pretrained model."),zpt.forEach(t),syr=i(Sa),Rn=n(Sa,"P",{});var ZM=s(Rn);lyr=r(ZM,"The model class to instantiate is selected based on the "),ibe=n(ZM,"CODE",{});var Vpt=s(ibe);iyr=r(Vpt,"model_type"),Vpt.forEach(t),dyr=r(ZM,` property of the config object (either
passed as an argument or loaded from `),dbe=n(ZM,"CODE",{});var Wpt=s(dbe);cyr=r(Wpt,"pretrained_model_name_or_path"),Wpt.forEach(t),fyr=r(ZM,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),cbe=n(ZM,"CODE",{});var Qpt=s(cbe);myr=r(Qpt,"pretrained_model_name_or_path"),Qpt.forEach(t),gyr=r(ZM,":"),ZM.forEach(t),hyr=i(Sa),P0=n(Sa,"UL",{});var dxe=s(P0);eM=n(dxe,"LI",{});var e8e=s(eM);fbe=n(e8e,"STRONG",{});var Hpt=s(fbe);pyr=r(Hpt,"beit"),Hpt.forEach(t),_yr=r(e8e," \u2014 "),qX=n(e8e,"A",{href:!0});var Upt=s(qX);uyr=r(Upt,"FlaxBeitForImageClassification"),Upt.forEach(t),byr=r(e8e," (BEiT model)"),e8e.forEach(t),vyr=i(dxe),oM=n(dxe,"LI",{});var o8e=s(oM);mbe=n(o8e,"STRONG",{});var Jpt=s(mbe);Tyr=r(Jpt,"vit"),Jpt.forEach(t),Fyr=r(o8e," \u2014 "),GX=n(o8e,"A",{href:!0});var Ypt=s(GX);Cyr=r(Ypt,"FlaxViTForImageClassification"),Ypt.forEach(t),Myr=r(o8e," (ViT model)"),o8e.forEach(t),dxe.forEach(t),Eyr=i(Sa),gbe=n(Sa,"P",{});var Kpt=s(gbe);yyr=r(Kpt,"Examples:"),Kpt.forEach(t),wyr=i(Sa),m($0.$$.fragment,Sa),Sa.forEach(t),Fi.forEach(t),eBe=i(d),Tf=n(d,"H2",{class:!0});var cxe=s(Tf);rM=n(cxe,"A",{id:!0,class:!0,href:!0});var Zpt=s(rM);hbe=n(Zpt,"SPAN",{});var e_t=s(hbe);m(I0.$$.fragment,e_t),e_t.forEach(t),Zpt.forEach(t),Ayr=i(cxe),pbe=n(cxe,"SPAN",{});var o_t=s(pbe);Lyr=r(o_t,"FlaxAutoModelForVision2Seq"),o_t.forEach(t),cxe.forEach(t),oBe=i(d),jr=n(d,"DIV",{class:!0});var Mi=s(jr);m(j0.$$.fragment,Mi),Byr=i(Mi),Ff=n(Mi,"P",{});var SV=s(Ff);kyr=r(SV,`This is a generic model class that will be instantiated as one of the model classes of the library (with a vision-to-text modeling head) when created
with the `),_be=n(SV,"CODE",{});var r_t=s(_be);xyr=r(r_t,"from_pretrained()"),r_t.forEach(t),Ryr=r(SV,"class method or the "),ube=n(SV,"CODE",{});var t_t=s(ube);Syr=r(t_t,"from_config()"),t_t.forEach(t),Pyr=r(SV,`class
method.`),SV.forEach(t),$yr=i(Mi),N0=n(Mi,"P",{});var fxe=s(N0);Iyr=r(fxe,"This class cannot be instantiated directly using "),bbe=n(fxe,"CODE",{});var a_t=s(bbe);jyr=r(a_t,"__init__()"),a_t.forEach(t),Nyr=r(fxe," (throws an error)."),fxe.forEach(t),Dyr=i(Mi),kt=n(Mi,"DIV",{class:!0});var Ei=s(kt);m(D0.$$.fragment,Ei),qyr=i(Ei),vbe=n(Ei,"P",{});var n_t=s(vbe);Gyr=r(n_t,"Instantiates one of the model classes of the library (with a vision-to-text modeling head) from a configuration."),n_t.forEach(t),Oyr=i(Ei),Cf=n(Ei,"P",{});var PV=s(Cf);Xyr=r(PV,`Note:
Loading a model from its configuration file does `),Tbe=n(PV,"STRONG",{});var s_t=s(Tbe);zyr=r(s_t,"not"),s_t.forEach(t),Vyr=r(PV,` load the model weights. It only affects the
model\u2019s configuration. Use `),Fbe=n(PV,"CODE",{});var l_t=s(Fbe);Wyr=r(l_t,"from_pretrained()"),l_t.forEach(t),Qyr=r(PV,"to load the model weights."),PV.forEach(t),Hyr=i(Ei),Cbe=n(Ei,"P",{});var i_t=s(Cbe);Uyr=r(i_t,"Examples:"),i_t.forEach(t),Jyr=i(Ei),m(q0.$$.fragment,Ei),Ei.forEach(t),Yyr=i(Mi),jo=n(Mi,"DIV",{class:!0});var Pa=s(jo);m(G0.$$.fragment,Pa),Kyr=i(Pa),Mbe=n(Pa,"P",{});var d_t=s(Mbe);Zyr=r(d_t,"Instantiate one of the model classes of the library (with a vision-to-text modeling head) from a pretrained model."),d_t.forEach(t),ewr=i(Pa),Sn=n(Pa,"P",{});var eE=s(Sn);owr=r(eE,"The model class to instantiate is selected based on the "),Ebe=n(eE,"CODE",{});var c_t=s(Ebe);rwr=r(c_t,"model_type"),c_t.forEach(t),twr=r(eE,` property of the config object (either
passed as an argument or loaded from `),ybe=n(eE,"CODE",{});var f_t=s(ybe);awr=r(f_t,"pretrained_model_name_or_path"),f_t.forEach(t),nwr=r(eE,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),wbe=n(eE,"CODE",{});var m_t=s(wbe);swr=r(m_t,"pretrained_model_name_or_path"),m_t.forEach(t),lwr=r(eE,":"),eE.forEach(t),iwr=i(Pa),Abe=n(Pa,"UL",{});var g_t=s(Abe);tM=n(g_t,"LI",{});var r8e=s(tM);Lbe=n(r8e,"STRONG",{});var h_t=s(Lbe);dwr=r(h_t,"vision-encoder-decoder"),h_t.forEach(t),cwr=r(r8e," \u2014 "),OX=n(r8e,"A",{href:!0});var p_t=s(OX);fwr=r(p_t,"FlaxVisionEncoderDecoderModel"),p_t.forEach(t),mwr=r(r8e," (Vision Encoder decoder model)"),r8e.forEach(t),g_t.forEach(t),gwr=i(Pa),Bbe=n(Pa,"P",{});var __t=s(Bbe);hwr=r(__t,"Examples:"),__t.forEach(t),pwr=i(Pa),m(O0.$$.fragment,Pa),Pa.forEach(t),Mi.forEach(t),this.h()},h(){c(J,"name","hf:doc:metadata"),c(J,"content",JSON.stringify(y_t)),c(me,"id","auto-classes"),c(me,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(me,"href","#auto-classes"),c(ie,"class","relative group"),c(Pn,"href","/docs/transformers/pr_15774/en/model_doc/auto#transformers.AutoConfig"),c(In,"href","/docs/transformers/pr_15774/en/model_doc/auto#transformers.AutoModel"),c(jn,"href","/docs/transformers/pr_15774/en/model_doc/auto#transformers.AutoTokenizer"),c(Ri,"href","/docs/transformers/pr_15774/en/model_doc/bert#transformers.BertModel"),c(Lf,"id","extending-the-auto-classes"),c(Lf,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Lf,"href","#extending-the-auto-classes"),c(Si,"class","relative group"),c(kf,"id","transformers.AutoConfig"),c(kf,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(kf,"href","#transformers.AutoConfig"),c(Pi,"class","relative group"),c(VL,"href","/docs/transformers/pr_15774/en/model_doc/auto#transformers.AutoConfig.from_pretrained"),c(WL,"href","/docs/transformers/pr_15774/en/model_doc/albert#transformers.AlbertConfig"),c(QL,"href","/docs/transformers/pr_15774/en/model_doc/bart#transformers.BartConfig"),c(HL,"href","/docs/transformers/pr_15774/en/model_doc/beit#transformers.BeitConfig"),c(UL,"href","/docs/transformers/pr_15774/en/model_doc/bert#transformers.BertConfig"),c(JL,"href","/docs/transformers/pr_15774/en/model_doc/bert-generation#transformers.BertGenerationConfig"),c(YL,"href","/docs/transformers/pr_15774/en/model_doc/big_bird#transformers.BigBirdConfig"),c(KL,"href","/docs/transformers/pr_15774/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusConfig"),c(ZL,"href","/docs/transformers/pr_15774/en/model_doc/blenderbot#transformers.BlenderbotConfig"),c(e8,"href","/docs/transformers/pr_15774/en/model_doc/blenderbot-small#transformers.BlenderbotSmallConfig"),c(o8,"href","/docs/transformers/pr_15774/en/model_doc/camembert#transformers.CamembertConfig"),c(r8,"href","/docs/transformers/pr_15774/en/model_doc/canine#transformers.CanineConfig"),c(t8,"href","/docs/transformers/pr_15774/en/model_doc/clip#transformers.CLIPConfig"),c(a8,"href","/docs/transformers/pr_15774/en/model_doc/convbert#transformers.ConvBertConfig"),c(n8,"href","/docs/transformers/pr_15774/en/model_doc/convnext#transformers.ConvNextConfig"),c(s8,"href","/docs/transformers/pr_15774/en/model_doc/ctrl#transformers.CTRLConfig"),c(l8,"href","/docs/transformers/pr_15774/en/model_doc/deberta#transformers.DebertaConfig"),c(i8,"href","/docs/transformers/pr_15774/en/model_doc/deberta-v2#transformers.DebertaV2Config"),c(d8,"href","/docs/transformers/pr_15774/en/model_doc/deit#transformers.DeiTConfig"),c(c8,"href","/docs/transformers/pr_15774/en/model_doc/detr#transformers.DetrConfig"),c(f8,"href","/docs/transformers/pr_15774/en/model_doc/distilbert#transformers.DistilBertConfig"),c(m8,"href","/docs/transformers/pr_15774/en/model_doc/dpr#transformers.DPRConfig"),c(g8,"href","/docs/transformers/pr_15774/en/model_doc/electra#transformers.ElectraConfig"),c(h8,"href","/docs/transformers/pr_15774/en/model_doc/encoder-decoder#transformers.EncoderDecoderConfig"),c(p8,"href","/docs/transformers/pr_15774/en/model_doc/flaubert#transformers.FlaubertConfig"),c(_8,"href","/docs/transformers/pr_15774/en/model_doc/fnet#transformers.FNetConfig"),c(u8,"href","/docs/transformers/pr_15774/en/model_doc/fsmt#transformers.FSMTConfig"),c(b8,"href","/docs/transformers/pr_15774/en/model_doc/funnel#transformers.FunnelConfig"),c(v8,"href","/docs/transformers/pr_15774/en/model_doc/gpt2#transformers.GPT2Config"),c(T8,"href","/docs/transformers/pr_15774/en/model_doc/gpt_neo#transformers.GPTNeoConfig"),c(F8,"href","/docs/transformers/pr_15774/en/model_doc/gptj#transformers.GPTJConfig"),c(C8,"href","/docs/transformers/pr_15774/en/model_doc/hubert#transformers.HubertConfig"),c(M8,"href","/docs/transformers/pr_15774/en/model_doc/ibert#transformers.IBertConfig"),c(E8,"href","/docs/transformers/pr_15774/en/model_doc/imagegpt#transformers.ImageGPTConfig"),c(y8,"href","/docs/transformers/pr_15774/en/model_doc/layoutlm#transformers.LayoutLMConfig"),c(w8,"href","/docs/transformers/pr_15774/en/model_doc/layoutlmv2#transformers.LayoutLMv2Config"),c(A8,"href","/docs/transformers/pr_15774/en/model_doc/led#transformers.LEDConfig"),c(L8,"href","/docs/transformers/pr_15774/en/model_doc/longformer#transformers.LongformerConfig"),c(B8,"href","/docs/transformers/pr_15774/en/model_doc/luke#transformers.LukeConfig"),c(k8,"href","/docs/transformers/pr_15774/en/model_doc/lxmert#transformers.LxmertConfig"),c(x8,"href","/docs/transformers/pr_15774/en/model_doc/m2m_100#transformers.M2M100Config"),c(R8,"href","/docs/transformers/pr_15774/en/model_doc/marian#transformers.MarianConfig"),c(S8,"href","/docs/transformers/pr_15774/en/model_doc/mbart#transformers.MBartConfig"),c(P8,"href","/docs/transformers/pr_15774/en/model_doc/megatron-bert#transformers.MegatronBertConfig"),c($8,"href","/docs/transformers/pr_15774/en/model_doc/mobilebert#transformers.MobileBertConfig"),c(I8,"href","/docs/transformers/pr_15774/en/model_doc/mpnet#transformers.MPNetConfig"),c(j8,"href","/docs/transformers/pr_15774/en/model_doc/mt5#transformers.MT5Config"),c(N8,"href","/docs/transformers/pr_15774/en/model_doc/nystromformer#transformers.NystromformerConfig"),c(D8,"href","/docs/transformers/pr_15774/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig"),c(q8,"href","/docs/transformers/pr_15774/en/model_doc/pegasus#transformers.PegasusConfig"),c(G8,"href","/docs/transformers/pr_15774/en/model_doc/perceiver#transformers.PerceiverConfig"),c(O8,"href","/docs/transformers/pr_15774/en/model_doc/plbart#transformers.PLBartConfig"),c(X8,"href","/docs/transformers/pr_15774/en/model_doc/poolformer#transformers.PoolFormerConfig"),c(z8,"href","/docs/transformers/pr_15774/en/model_doc/prophetnet#transformers.ProphetNetConfig"),c(V8,"href","/docs/transformers/pr_15774/en/model_doc/qdqbert#transformers.QDQBertConfig"),c(W8,"href","/docs/transformers/pr_15774/en/model_doc/rag#transformers.RagConfig"),c(Q8,"href","/docs/transformers/pr_15774/en/model_doc/realm#transformers.RealmConfig"),c(H8,"href","/docs/transformers/pr_15774/en/model_doc/reformer#transformers.ReformerConfig"),c(U8,"href","/docs/transformers/pr_15774/en/model_doc/rembert#transformers.RemBertConfig"),c(J8,"href","/docs/transformers/pr_15774/en/model_doc/retribert#transformers.RetriBertConfig"),c(Y8,"href","/docs/transformers/pr_15774/en/model_doc/roberta#transformers.RobertaConfig"),c(K8,"href","/docs/transformers/pr_15774/en/model_doc/roformer#transformers.RoFormerConfig"),c(Z8,"href","/docs/transformers/pr_15774/en/model_doc/segformer#transformers.SegformerConfig"),c(e9,"href","/docs/transformers/pr_15774/en/model_doc/sew#transformers.SEWConfig"),c(o9,"href","/docs/transformers/pr_15774/en/model_doc/sew-d#transformers.SEWDConfig"),c(r9,"href","/docs/transformers/pr_15774/en/model_doc/speech-encoder-decoder#transformers.SpeechEncoderDecoderConfig"),c(t9,"href","/docs/transformers/pr_15774/en/model_doc/speech_to_text#transformers.Speech2TextConfig"),c(a9,"href","/docs/transformers/pr_15774/en/model_doc/speech_to_text_2#transformers.Speech2Text2Config"),c(n9,"href","/docs/transformers/pr_15774/en/model_doc/splinter#transformers.SplinterConfig"),c(s9,"href","/docs/transformers/pr_15774/en/model_doc/squeezebert#transformers.SqueezeBertConfig"),c(l9,"href","/docs/transformers/pr_15774/en/model_doc/swin#transformers.SwinConfig"),c(i9,"href","/docs/transformers/pr_15774/en/model_doc/t5#transformers.T5Config"),c(d9,"href","/docs/transformers/pr_15774/en/model_doc/tapas#transformers.TapasConfig"),c(c9,"href","/docs/transformers/pr_15774/en/model_doc/transfo-xl#transformers.TransfoXLConfig"),c(f9,"href","/docs/transformers/pr_15774/en/model_doc/trocr#transformers.TrOCRConfig"),c(m9,"href","/docs/transformers/pr_15774/en/model_doc/unispeech#transformers.UniSpeechConfig"),c(g9,"href","/docs/transformers/pr_15774/en/model_doc/unispeech-sat#transformers.UniSpeechSatConfig"),c(h9,"href","/docs/transformers/pr_15774/en/model_doc/vilt#transformers.ViltConfig"),c(p9,"href","/docs/transformers/pr_15774/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderConfig"),c(_9,"href","/docs/transformers/pr_15774/en/model_doc/vision-text-dual-encoder#transformers.VisionTextDualEncoderConfig"),c(u9,"href","/docs/transformers/pr_15774/en/model_doc/visual_bert#transformers.VisualBertConfig"),c(b9,"href","/docs/transformers/pr_15774/en/model_doc/vit#transformers.ViTConfig"),c(v9,"href","/docs/transformers/pr_15774/en/model_doc/vit_mae#transformers.ViTMAEConfig"),c(T9,"href","/docs/transformers/pr_15774/en/model_doc/wav2vec2#transformers.Wav2Vec2Config"),c(F9,"href","/docs/transformers/pr_15774/en/model_doc/wavlm#transformers.WavLMConfig"),c(C9,"href","/docs/transformers/pr_15774/en/model_doc/xglm#transformers.XGLMConfig"),c(M9,"href","/docs/transformers/pr_15774/en/model_doc/xlm#transformers.XLMConfig"),c(E9,"href","/docs/transformers/pr_15774/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetConfig"),c(y9,"href","/docs/transformers/pr_15774/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig"),c(w9,"href","/docs/transformers/pr_15774/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLConfig"),c(A9,"href","/docs/transformers/pr_15774/en/model_doc/xlnet#transformers.XLNetConfig"),c(L9,"href","/docs/transformers/pr_15774/en/model_doc/yoso#transformers.YosoConfig"),c(fo,"class","docstring"),c(hg,"class","docstring"),c(Go,"class","docstring"),c(pg,"id","transformers.AutoTokenizer"),c(pg,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(pg,"href","#transformers.AutoTokenizer"),c(Ii,"class","relative group"),c(B9,"href","/docs/transformers/pr_15774/en/model_doc/auto#transformers.AutoTokenizer.from_pretrained"),c(k9,"href","/docs/transformers/pr_15774/en/model_doc/albert#transformers.AlbertTokenizer"),c(x9,"href","/docs/transformers/pr_15774/en/model_doc/albert#transformers.AlbertTokenizerFast"),c(R9,"href","/docs/transformers/pr_15774/en/model_doc/bart#transformers.BartTokenizer"),c(S9,"href","/docs/transformers/pr_15774/en/model_doc/bart#transformers.BartTokenizerFast"),c(P9,"href","/docs/transformers/pr_15774/en/model_doc/barthez#transformers.BarthezTokenizer"),c($9,"href","/docs/transformers/pr_15774/en/model_doc/barthez#transformers.BarthezTokenizerFast"),c(I9,"href","/docs/transformers/pr_15774/en/model_doc/bartpho#transformers.BartphoTokenizer"),c(j9,"href","/docs/transformers/pr_15774/en/model_doc/bert#transformers.BertTokenizer"),c(N9,"href","/docs/transformers/pr_15774/en/model_doc/bert#transformers.BertTokenizerFast"),c(D9,"href","/docs/transformers/pr_15774/en/model_doc/bert-generation#transformers.BertGenerationTokenizer"),c(q9,"href","/docs/transformers/pr_15774/en/model_doc/bert-japanese#transformers.BertJapaneseTokenizer"),c(G9,"href","/docs/transformers/pr_15774/en/model_doc/bertweet#transformers.BertweetTokenizer"),c(O9,"href","/docs/transformers/pr_15774/en/model_doc/big_bird#transformers.BigBirdTokenizer"),c(X9,"href","/docs/transformers/pr_15774/en/model_doc/big_bird#transformers.BigBirdTokenizerFast"),c(z9,"href","/docs/transformers/pr_15774/en/model_doc/pegasus#transformers.PegasusTokenizer"),c(V9,"href","/docs/transformers/pr_15774/en/model_doc/pegasus#transformers.PegasusTokenizerFast"),c(W9,"href","/docs/transformers/pr_15774/en/model_doc/blenderbot#transformers.BlenderbotTokenizer"),c(Q9,"href","/docs/transformers/pr_15774/en/model_doc/blenderbot#transformers.BlenderbotTokenizerFast"),c(H9,"href","/docs/transformers/pr_15774/en/model_doc/blenderbot-small#transformers.BlenderbotSmallTokenizer"),c(U9,"href","/docs/transformers/pr_15774/en/model_doc/byt5#transformers.ByT5Tokenizer"),c(J9,"href","/docs/transformers/pr_15774/en/model_doc/camembert#transformers.CamembertTokenizer"),c(Y9,"href","/docs/transformers/pr_15774/en/model_doc/camembert#transformers.CamembertTokenizerFast"),c(K9,"href","/docs/transformers/pr_15774/en/model_doc/canine#transformers.CanineTokenizer"),c(Z9,"href","/docs/transformers/pr_15774/en/model_doc/clip#transformers.CLIPTokenizer"),c(eB,"href","/docs/transformers/pr_15774/en/model_doc/clip#transformers.CLIPTokenizerFast"),c(oB,"href","/docs/transformers/pr_15774/en/model_doc/convbert#transformers.ConvBertTokenizer"),c(rB,"href","/docs/transformers/pr_15774/en/model_doc/convbert#transformers.ConvBertTokenizerFast"),c(tB,"href","/docs/transformers/pr_15774/en/model_doc/cpm#transformers.CpmTokenizer"),c(aB,"href","/docs/transformers/pr_15774/en/model_doc/ctrl#transformers.CTRLTokenizer"),c(nB,"href","/docs/transformers/pr_15774/en/model_doc/deberta#transformers.DebertaTokenizer"),c(sB,"href","/docs/transformers/pr_15774/en/model_doc/deberta#transformers.DebertaTokenizerFast"),c(lB,"href","/docs/transformers/pr_15774/en/model_doc/deberta-v2#transformers.DebertaV2Tokenizer"),c(iB,"href","/docs/transformers/pr_15774/en/model_doc/distilbert#transformers.DistilBertTokenizer"),c(dB,"href","/docs/transformers/pr_15774/en/model_doc/distilbert#transformers.DistilBertTokenizerFast"),c(cB,"href","/docs/transformers/pr_15774/en/model_doc/dpr#transformers.DPRQuestionEncoderTokenizer"),c(fB,"href","/docs/transformers/pr_15774/en/model_doc/dpr#transformers.DPRQuestionEncoderTokenizerFast"),c(mB,"href","/docs/transformers/pr_15774/en/model_doc/electra#transformers.ElectraTokenizer"),c(gB,"href","/docs/transformers/pr_15774/en/model_doc/electra#transformers.ElectraTokenizerFast"),c(hB,"href","/docs/transformers/pr_15774/en/model_doc/flaubert#transformers.FlaubertTokenizer"),c(pB,"href","/docs/transformers/pr_15774/en/model_doc/fnet#transformers.FNetTokenizer"),c(_B,"href","/docs/transformers/pr_15774/en/model_doc/fnet#transformers.FNetTokenizerFast"),c(uB,"href","/docs/transformers/pr_15774/en/model_doc/fsmt#transformers.FSMTTokenizer"),c(bB,"href","/docs/transformers/pr_15774/en/model_doc/funnel#transformers.FunnelTokenizer"),c(vB,"href","/docs/transformers/pr_15774/en/model_doc/funnel#transformers.FunnelTokenizerFast"),c(TB,"href","/docs/transformers/pr_15774/en/model_doc/gpt2#transformers.GPT2Tokenizer"),c(FB,"href","/docs/transformers/pr_15774/en/model_doc/gpt2#transformers.GPT2TokenizerFast"),c(CB,"href","/docs/transformers/pr_15774/en/model_doc/gpt2#transformers.GPT2Tokenizer"),c(MB,"href","/docs/transformers/pr_15774/en/model_doc/gpt2#transformers.GPT2TokenizerFast"),c(EB,"href","/docs/transformers/pr_15774/en/model_doc/herbert#transformers.HerbertTokenizer"),c(yB,"href","/docs/transformers/pr_15774/en/model_doc/herbert#transformers.HerbertTokenizerFast"),c(wB,"href","/docs/transformers/pr_15774/en/model_doc/wav2vec2#transformers.Wav2Vec2CTCTokenizer"),c(AB,"href","/docs/transformers/pr_15774/en/model_doc/roberta#transformers.RobertaTokenizer"),c(LB,"href","/docs/transformers/pr_15774/en/model_doc/roberta#transformers.RobertaTokenizerFast"),c(BB,"href","/docs/transformers/pr_15774/en/model_doc/layoutlm#transformers.LayoutLMTokenizer"),c(kB,"href","/docs/transformers/pr_15774/en/model_doc/layoutlm#transformers.LayoutLMTokenizerFast"),c(xB,"href","/docs/transformers/pr_15774/en/model_doc/layoutlmv2#transformers.LayoutLMv2Tokenizer"),c(RB,"href","/docs/transformers/pr_15774/en/model_doc/layoutlmv2#transformers.LayoutLMv2TokenizerFast"),c(SB,"href","/docs/transformers/pr_15774/en/model_doc/layoutxlm#transformers.LayoutXLMTokenizer"),c(PB,"href","/docs/transformers/pr_15774/en/model_doc/layoutxlm#transformers.LayoutXLMTokenizerFast"),c($B,"href","/docs/transformers/pr_15774/en/model_doc/led#transformers.LEDTokenizer"),c(IB,"href","/docs/transformers/pr_15774/en/model_doc/led#transformers.LEDTokenizerFast"),c(jB,"href","/docs/transformers/pr_15774/en/model_doc/longformer#transformers.LongformerTokenizer"),c(NB,"href","/docs/transformers/pr_15774/en/model_doc/longformer#transformers.LongformerTokenizerFast"),c(DB,"href","/docs/transformers/pr_15774/en/model_doc/luke#transformers.LukeTokenizer"),c(qB,"href","/docs/transformers/pr_15774/en/model_doc/lxmert#transformers.LxmertTokenizer"),c(GB,"href","/docs/transformers/pr_15774/en/model_doc/lxmert#transformers.LxmertTokenizerFast"),c(OB,"href","/docs/transformers/pr_15774/en/model_doc/m2m_100#transformers.M2M100Tokenizer"),c(XB,"href","/docs/transformers/pr_15774/en/model_doc/marian#transformers.MarianTokenizer"),c(zB,"href","/docs/transformers/pr_15774/en/model_doc/mbart#transformers.MBartTokenizer"),c(VB,"href","/docs/transformers/pr_15774/en/model_doc/mbart#transformers.MBartTokenizerFast"),c(WB,"href","/docs/transformers/pr_15774/en/model_doc/mbart#transformers.MBart50Tokenizer"),c(QB,"href","/docs/transformers/pr_15774/en/model_doc/mbart#transformers.MBart50TokenizerFast"),c(HB,"href","/docs/transformers/pr_15774/en/model_doc/mluke#transformers.MLukeTokenizer"),c(UB,"href","/docs/transformers/pr_15774/en/model_doc/mobilebert#transformers.MobileBertTokenizer"),c(JB,"href","/docs/transformers/pr_15774/en/model_doc/mobilebert#transformers.MobileBertTokenizerFast"),c(YB,"href","/docs/transformers/pr_15774/en/model_doc/mpnet#transformers.MPNetTokenizer"),c(KB,"href","/docs/transformers/pr_15774/en/model_doc/mpnet#transformers.MPNetTokenizerFast"),c(ZB,"href","/docs/transformers/pr_15774/en/model_doc/mt5#transformers.T5Tokenizer"),c(ek,"href","/docs/transformers/pr_15774/en/model_doc/mt5#transformers.T5TokenizerFast"),c(ok,"href","/docs/transformers/pr_15774/en/model_doc/openai-gpt#transformers.OpenAIGPTTokenizer"),c(rk,"href","/docs/transformers/pr_15774/en/model_doc/openai-gpt#transformers.OpenAIGPTTokenizerFast"),c(tk,"href","/docs/transformers/pr_15774/en/model_doc/pegasus#transformers.PegasusTokenizer"),c(ak,"href","/docs/transformers/pr_15774/en/model_doc/pegasus#transformers.PegasusTokenizerFast"),c(nk,"href","/docs/transformers/pr_15774/en/model_doc/perceiver#transformers.PerceiverTokenizer"),c(sk,"href","/docs/transformers/pr_15774/en/model_doc/phobert#transformers.PhobertTokenizer"),c(lk,"href","/docs/transformers/pr_15774/en/model_doc/plbart#transformers.PLBartTokenizer"),c(ik,"href","/docs/transformers/pr_15774/en/model_doc/prophetnet#transformers.ProphetNetTokenizer"),c(dk,"href","/docs/transformers/pr_15774/en/model_doc/bert#transformers.BertTokenizer"),c(ck,"href","/docs/transformers/pr_15774/en/model_doc/bert#transformers.BertTokenizerFast"),c(fk,"href","/docs/transformers/pr_15774/en/model_doc/rag#transformers.RagTokenizer"),c(mk,"href","/docs/transformers/pr_15774/en/model_doc/reformer#transformers.ReformerTokenizer"),c(gk,"href","/docs/transformers/pr_15774/en/model_doc/reformer#transformers.ReformerTokenizerFast"),c(hk,"href","/docs/transformers/pr_15774/en/model_doc/rembert#transformers.RemBertTokenizer"),c(pk,"href","/docs/transformers/pr_15774/en/model_doc/rembert#transformers.RemBertTokenizerFast"),c(_k,"href","/docs/transformers/pr_15774/en/model_doc/retribert#transformers.RetriBertTokenizer"),c(uk,"href","/docs/transformers/pr_15774/en/model_doc/retribert#transformers.RetriBertTokenizerFast"),c(bk,"href","/docs/transformers/pr_15774/en/model_doc/roberta#transformers.RobertaTokenizer"),c(vk,"href","/docs/transformers/pr_15774/en/model_doc/roberta#transformers.RobertaTokenizerFast"),c(Tk,"href","/docs/transformers/pr_15774/en/model_doc/roformer#transformers.RoFormerTokenizer"),c(Fk,"href","/docs/transformers/pr_15774/en/model_doc/roformer#transformers.RoFormerTokenizerFast"),c(Ck,"href","/docs/transformers/pr_15774/en/model_doc/speech_to_text#transformers.Speech2TextTokenizer"),c(Mk,"href","/docs/transformers/pr_15774/en/model_doc/speech_to_text_2#transformers.Speech2Text2Tokenizer"),c(Ek,"href","/docs/transformers/pr_15774/en/model_doc/splinter#transformers.SplinterTokenizer"),c(yk,"href","/docs/transformers/pr_15774/en/model_doc/splinter#transformers.SplinterTokenizerFast"),c(wk,"href","/docs/transformers/pr_15774/en/model_doc/squeezebert#transformers.SqueezeBertTokenizer"),c(Ak,"href","/docs/transformers/pr_15774/en/model_doc/squeezebert#transformers.SqueezeBertTokenizerFast"),c(Lk,"href","/docs/transformers/pr_15774/en/model_doc/mt5#transformers.T5Tokenizer"),c(Bk,"href","/docs/transformers/pr_15774/en/model_doc/mt5#transformers.T5TokenizerFast"),c(kk,"href","/docs/transformers/pr_15774/en/model_doc/tapas#transformers.TapasTokenizer"),c(xk,"href","/docs/transformers/pr_15774/en/model_doc/transfo-xl#transformers.TransfoXLTokenizer"),c(Rk,"href","/docs/transformers/pr_15774/en/model_doc/wav2vec2#transformers.Wav2Vec2CTCTokenizer"),c(Sk,"href","/docs/transformers/pr_15774/en/model_doc/wav2vec2_phoneme#transformers.Wav2Vec2PhonemeCTCTokenizer"),c(Pk,"href","/docs/transformers/pr_15774/en/model_doc/xglm#transformers.XGLMTokenizer"),c($k,"href","/docs/transformers/pr_15774/en/model_doc/xglm#transformers.XGLMTokenizerFast"),c(Ik,"href","/docs/transformers/pr_15774/en/model_doc/xlm#transformers.XLMTokenizer"),c(jk,"href","/docs/transformers/pr_15774/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetTokenizer"),c(Nk,"href","/docs/transformers/pr_15774/en/model_doc/xlm-roberta#transformers.XLMRobertaTokenizer"),c(Dk,"href","/docs/transformers/pr_15774/en/model_doc/xlm-roberta#transformers.XLMRobertaTokenizerFast"),c(qk,"href","/docs/transformers/pr_15774/en/model_doc/xlnet#transformers.XLNetTokenizer"),c(Gk,"href","/docs/transformers/pr_15774/en/model_doc/xlnet#transformers.XLNetTokenizerFast"),c(mo,"class","docstring"),c(Vg,"class","docstring"),c(Oo,"class","docstring"),c(Wg,"id","transformers.AutoFeatureExtractor"),c(Wg,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Wg,"href","#transformers.AutoFeatureExtractor"),c(ji,"class","relative group"),c(Ok,"href","/docs/transformers/pr_15774/en/model_doc/auto#transformers.AutoFeatureExtractor.from_pretrained"),c(Xk,"href","/docs/transformers/pr_15774/en/model_doc/beit#transformers.BeitFeatureExtractor"),c(zk,"href","/docs/transformers/pr_15774/en/model_doc/clip#transformers.CLIPFeatureExtractor"),c(Vk,"href","/docs/transformers/pr_15774/en/model_doc/convnext#transformers.ConvNextFeatureExtractor"),c(Wk,"href","/docs/transformers/pr_15774/en/model_doc/deit#transformers.DeiTFeatureExtractor"),c(Qk,"href","/docs/transformers/pr_15774/en/model_doc/detr#transformers.DetrFeatureExtractor"),c(Hk,"href","/docs/transformers/pr_15774/en/model_doc/wav2vec2#transformers.Wav2Vec2FeatureExtractor"),c(Uk,"href","/docs/transformers/pr_15774/en/model_doc/layoutlmv2#transformers.LayoutLMv2FeatureExtractor"),c(Jk,"href","/docs/transformers/pr_15774/en/model_doc/perceiver#transformers.PerceiverFeatureExtractor"),c(Yk,"href","/docs/transformers/pr_15774/en/model_doc/poolformer#transformers.PoolFormerFeatureExtractor"),c(Kk,"href","/docs/transformers/pr_15774/en/model_doc/segformer#transformers.SegformerFeatureExtractor"),c(Zk,"href","/docs/transformers/pr_15774/en/model_doc/speech_to_text#transformers.Speech2TextFeatureExtractor"),c(ex,"href","/docs/transformers/pr_15774/en/model_doc/vit#transformers.ViTFeatureExtractor"),c(ox,"href","/docs/transformers/pr_15774/en/model_doc/vit#transformers.ViTFeatureExtractor"),c(rx,"href","/docs/transformers/pr_15774/en/model_doc/vit#transformers.ViTFeatureExtractor"),c(tx,"href","/docs/transformers/pr_15774/en/model_doc/wav2vec2#transformers.Wav2Vec2FeatureExtractor"),c(Le,"class","docstring"),c(dh,"class","docstring"),c(Xo,"class","docstring"),c(ch,"id","transformers.AutoProcessor"),c(ch,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(ch,"href","#transformers.AutoProcessor"),c(Ni,"class","relative group"),c(ax,"href","/docs/transformers/pr_15774/en/model_doc/auto#transformers.AutoProcessor.from_pretrained"),c(nx,"href","/docs/transformers/pr_15774/en/model_doc/clip#transformers.CLIPProcessor"),c(sx,"href","/docs/transformers/pr_15774/en/model_doc/layoutlmv2#transformers.LayoutLMv2Processor"),c(lx,"href","/docs/transformers/pr_15774/en/model_doc/layoutxlm#transformers.LayoutXLMProcessor"),c(ix,"href","/docs/transformers/pr_15774/en/model_doc/speech_to_text#transformers.Speech2TextProcessor"),c(dx,"href","/docs/transformers/pr_15774/en/model_doc/speech_to_text_2#transformers.Speech2Text2Processor"),c(cx,"href","/docs/transformers/pr_15774/en/model_doc/trocr#transformers.TrOCRProcessor"),c(fx,"href","/docs/transformers/pr_15774/en/model_doc/vision-text-dual-encoder#transformers.VisionTextDualEncoderProcessor"),c(mx,"href","/docs/transformers/pr_15774/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor"),c(Be,"class","docstring"),c(Th,"class","docstring"),c(zo,"class","docstring"),c(Fh,"id","transformers.AutoModel"),c(Fh,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Fh,"href","#transformers.AutoModel"),c(qi,"class","relative group"),c(Nr,"class","docstring"),c(gx,"href","/docs/transformers/pr_15774/en/model_doc/albert#transformers.AlbertModel"),c(hx,"href","/docs/transformers/pr_15774/en/model_doc/bart#transformers.BartModel"),c(px,"href","/docs/transformers/pr_15774/en/model_doc/beit#transformers.BeitModel"),c(_x,"href","/docs/transformers/pr_15774/en/model_doc/bert#transformers.BertModel"),c(ux,"href","/docs/transformers/pr_15774/en/model_doc/bert-generation#transformers.BertGenerationEncoder"),c(bx,"href","/docs/transformers/pr_15774/en/model_doc/big_bird#transformers.BigBirdModel"),c(vx,"href","/docs/transformers/pr_15774/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusModel"),c(Tx,"href","/docs/transformers/pr_15774/en/model_doc/blenderbot#transformers.BlenderbotModel"),c(Fx,"href","/docs/transformers/pr_15774/en/model_doc/blenderbot-small#transformers.BlenderbotSmallModel"),c(Cx,"href","/docs/transformers/pr_15774/en/model_doc/camembert#transformers.CamembertModel"),c(Mx,"href","/docs/transformers/pr_15774/en/model_doc/canine#transformers.CanineModel"),c(Ex,"href","/docs/transformers/pr_15774/en/model_doc/clip#transformers.CLIPModel"),c(yx,"href","/docs/transformers/pr_15774/en/model_doc/convbert#transformers.ConvBertModel"),c(wx,"href","/docs/transformers/pr_15774/en/model_doc/convnext#transformers.ConvNextModel"),c(Ax,"href","/docs/transformers/pr_15774/en/model_doc/ctrl#transformers.CTRLModel"),c(Lx,"href","/docs/transformers/pr_15774/en/model_doc/deberta#transformers.DebertaModel"),c(Bx,"href","/docs/transformers/pr_15774/en/model_doc/deberta-v2#transformers.DebertaV2Model"),c(kx,"href","/docs/transformers/pr_15774/en/model_doc/deit#transformers.DeiTModel"),c(xx,"href","/docs/transformers/pr_15774/en/model_doc/detr#transformers.DetrModel"),c(Rx,"href","/docs/transformers/pr_15774/en/model_doc/distilbert#transformers.DistilBertModel"),c(Sx,"href","/docs/transformers/pr_15774/en/model_doc/dpr#transformers.DPRQuestionEncoder"),c(Px,"href","/docs/transformers/pr_15774/en/model_doc/electra#transformers.ElectraModel"),c($x,"href","/docs/transformers/pr_15774/en/model_doc/flaubert#transformers.FlaubertModel"),c(Ix,"href","/docs/transformers/pr_15774/en/model_doc/fnet#transformers.FNetModel"),c(jx,"href","/docs/transformers/pr_15774/en/model_doc/fsmt#transformers.FSMTModel"),c(Nx,"href","/docs/transformers/pr_15774/en/model_doc/funnel#transformers.FunnelModel"),c(Dx,"href","/docs/transformers/pr_15774/en/model_doc/funnel#transformers.FunnelBaseModel"),c(qx,"href","/docs/transformers/pr_15774/en/model_doc/gpt2#transformers.GPT2Model"),c(Gx,"href","/docs/transformers/pr_15774/en/model_doc/gpt_neo#transformers.GPTNeoModel"),c(Ox,"href","/docs/transformers/pr_15774/en/model_doc/gptj#transformers.GPTJModel"),c(Xx,"href","/docs/transformers/pr_15774/en/model_doc/hubert#transformers.HubertModel"),c(zx,"href","/docs/transformers/pr_15774/en/model_doc/ibert#transformers.IBertModel"),c(Vx,"href","/docs/transformers/pr_15774/en/model_doc/imagegpt#transformers.ImageGPTModel"),c(Wx,"href","/docs/transformers/pr_15774/en/model_doc/layoutlm#transformers.LayoutLMModel"),c(Qx,"href","/docs/transformers/pr_15774/en/model_doc/layoutlmv2#transformers.LayoutLMv2Model"),c(Hx,"href","/docs/transformers/pr_15774/en/model_doc/led#transformers.LEDModel"),c(Ux,"href","/docs/transformers/pr_15774/en/model_doc/longformer#transformers.LongformerModel"),c(Jx,"href","/docs/transformers/pr_15774/en/model_doc/luke#transformers.LukeModel"),c(Yx,"href","/docs/transformers/pr_15774/en/model_doc/lxmert#transformers.LxmertModel"),c(Kx,"href","/docs/transformers/pr_15774/en/model_doc/m2m_100#transformers.M2M100Model"),c(Zx,"href","/docs/transformers/pr_15774/en/model_doc/marian#transformers.MarianModel"),c(eR,"href","/docs/transformers/pr_15774/en/model_doc/mbart#transformers.MBartModel"),c(oR,"href","/docs/transformers/pr_15774/en/model_doc/megatron-bert#transformers.MegatronBertModel"),c(rR,"href","/docs/transformers/pr_15774/en/model_doc/mobilebert#transformers.MobileBertModel"),c(tR,"href","/docs/transformers/pr_15774/en/model_doc/mpnet#transformers.MPNetModel"),c(aR,"href","/docs/transformers/pr_15774/en/model_doc/mt5#transformers.MT5Model"),c(nR,"href","/docs/transformers/pr_15774/en/model_doc/nystromformer#transformers.NystromformerModel"),c(sR,"href","/docs/transformers/pr_15774/en/model_doc/openai-gpt#transformers.OpenAIGPTModel"),c(lR,"href","/docs/transformers/pr_15774/en/model_doc/pegasus#transformers.PegasusModel"),c(iR,"href","/docs/transformers/pr_15774/en/model_doc/perceiver#transformers.PerceiverModel"),c(dR,"href","/docs/transformers/pr_15774/en/model_doc/plbart#transformers.PLBartModel"),c(cR,"href","/docs/transformers/pr_15774/en/model_doc/poolformer#transformers.PoolFormerModel"),c(fR,"href","/docs/transformers/pr_15774/en/model_doc/prophetnet#transformers.ProphetNetModel"),c(mR,"href","/docs/transformers/pr_15774/en/model_doc/qdqbert#transformers.QDQBertModel"),c(gR,"href","/docs/transformers/pr_15774/en/model_doc/reformer#transformers.ReformerModel"),c(hR,"href","/docs/transformers/pr_15774/en/model_doc/rembert#transformers.RemBertModel"),c(pR,"href","/docs/transformers/pr_15774/en/model_doc/retribert#transformers.RetriBertModel"),c(_R,"href","/docs/transformers/pr_15774/en/model_doc/roberta#transformers.RobertaModel"),c(uR,"href","/docs/transformers/pr_15774/en/model_doc/roformer#transformers.RoFormerModel"),c(bR,"href","/docs/transformers/pr_15774/en/model_doc/segformer#transformers.SegformerModel"),c(vR,"href","/docs/transformers/pr_15774/en/model_doc/sew#transformers.SEWModel"),c(TR,"href","/docs/transformers/pr_15774/en/model_doc/sew-d#transformers.SEWDModel"),c(FR,"href","/docs/transformers/pr_15774/en/model_doc/speech_to_text#transformers.Speech2TextModel"),c(CR,"href","/docs/transformers/pr_15774/en/model_doc/splinter#transformers.SplinterModel"),c(MR,"href","/docs/transformers/pr_15774/en/model_doc/squeezebert#transformers.SqueezeBertModel"),c(ER,"href","/docs/transformers/pr_15774/en/model_doc/swin#transformers.SwinModel"),c(yR,"href","/docs/transformers/pr_15774/en/model_doc/t5#transformers.T5Model"),c(wR,"href","/docs/transformers/pr_15774/en/model_doc/tapas#transformers.TapasModel"),c(AR,"href","/docs/transformers/pr_15774/en/model_doc/transfo-xl#transformers.TransfoXLModel"),c(LR,"href","/docs/transformers/pr_15774/en/model_doc/unispeech#transformers.UniSpeechModel"),c(BR,"href","/docs/transformers/pr_15774/en/model_doc/unispeech-sat#transformers.UniSpeechSatModel"),c(kR,"href","/docs/transformers/pr_15774/en/model_doc/vilt#transformers.ViltModel"),c(xR,"href","/docs/transformers/pr_15774/en/model_doc/vision-text-dual-encoder#transformers.VisionTextDualEncoderModel"),c(RR,"href","/docs/transformers/pr_15774/en/model_doc/visual_bert#transformers.VisualBertModel"),c(SR,"href","/docs/transformers/pr_15774/en/model_doc/vit#transformers.ViTModel"),c(PR,"href","/docs/transformers/pr_15774/en/model_doc/vit_mae#transformers.ViTMAEModel"),c($R,"href","/docs/transformers/pr_15774/en/model_doc/wav2vec2#transformers.Wav2Vec2Model"),c(IR,"href","/docs/transformers/pr_15774/en/model_doc/wavlm#transformers.WavLMModel"),c(jR,"href","/docs/transformers/pr_15774/en/model_doc/xglm#transformers.XGLMModel"),c(NR,"href","/docs/transformers/pr_15774/en/model_doc/xlm#transformers.XLMModel"),c(DR,"href","/docs/transformers/pr_15774/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetModel"),c(qR,"href","/docs/transformers/pr_15774/en/model_doc/xlm-roberta#transformers.XLMRobertaModel"),c(GR,"href","/docs/transformers/pr_15774/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLModel"),c(OR,"href","/docs/transformers/pr_15774/en/model_doc/xlnet#transformers.XLNetModel"),c(XR,"href","/docs/transformers/pr_15774/en/model_doc/yoso#transformers.YosoModel"),c(ke,"class","docstring"),c(Vo,"class","docstring"),c(Kp,"id","transformers.AutoModelForPreTraining"),c(Kp,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Kp,"href","#transformers.AutoModelForPreTraining"),c(Xi,"class","relative group"),c(Dr,"class","docstring"),c(zR,"href","/docs/transformers/pr_15774/en/model_doc/albert#transformers.AlbertForPreTraining"),c(VR,"href","/docs/transformers/pr_15774/en/model_doc/bart#transformers.BartForConditionalGeneration"),c(WR,"href","/docs/transformers/pr_15774/en/model_doc/bert#transformers.BertForPreTraining"),c(QR,"href","/docs/transformers/pr_15774/en/model_doc/big_bird#transformers.BigBirdForPreTraining"),c(HR,"href","/docs/transformers/pr_15774/en/model_doc/camembert#transformers.CamembertForMaskedLM"),c(UR,"href","/docs/transformers/pr_15774/en/model_doc/ctrl#transformers.CTRLLMHeadModel"),c(JR,"href","/docs/transformers/pr_15774/en/model_doc/deberta#transformers.DebertaForMaskedLM"),c(YR,"href","/docs/transformers/pr_15774/en/model_doc/deberta-v2#transformers.DebertaV2ForMaskedLM"),c(KR,"href","/docs/transformers/pr_15774/en/model_doc/distilbert#transformers.DistilBertForMaskedLM"),c(ZR,"href","/docs/transformers/pr_15774/en/model_doc/electra#transformers.ElectraForPreTraining"),c(eS,"href","/docs/transformers/pr_15774/en/model_doc/flaubert#transformers.FlaubertWithLMHeadModel"),c(oS,"href","/docs/transformers/pr_15774/en/model_doc/fnet#transformers.FNetForPreTraining"),c(rS,"href","/docs/transformers/pr_15774/en/model_doc/fsmt#transformers.FSMTForConditionalGeneration"),c(tS,"href","/docs/transformers/pr_15774/en/model_doc/funnel#transformers.FunnelForPreTraining"),c(aS,"href","/docs/transformers/pr_15774/en/model_doc/gpt2#transformers.GPT2LMHeadModel"),c(nS,"href","/docs/transformers/pr_15774/en/model_doc/ibert#transformers.IBertForMaskedLM"),c(sS,"href","/docs/transformers/pr_15774/en/model_doc/layoutlm#transformers.LayoutLMForMaskedLM"),c(lS,"href","/docs/transformers/pr_15774/en/model_doc/longformer#transformers.LongformerForMaskedLM"),c(iS,"href","/docs/transformers/pr_15774/en/model_doc/lxmert#transformers.LxmertForPreTraining"),c(dS,"href","/docs/transformers/pr_15774/en/model_doc/megatron-bert#transformers.MegatronBertForPreTraining"),c(cS,"href","/docs/transformers/pr_15774/en/model_doc/mobilebert#transformers.MobileBertForPreTraining"),c(fS,"href","/docs/transformers/pr_15774/en/model_doc/mpnet#transformers.MPNetForMaskedLM"),c(mS,"href","/docs/transformers/pr_15774/en/model_doc/openai-gpt#transformers.OpenAIGPTLMHeadModel"),c(gS,"href","/docs/transformers/pr_15774/en/model_doc/retribert#transformers.RetriBertModel"),c(hS,"href","/docs/transformers/pr_15774/en/model_doc/roberta#transformers.RobertaForMaskedLM"),c(pS,"href","/docs/transformers/pr_15774/en/model_doc/squeezebert#transformers.SqueezeBertForMaskedLM"),c(_S,"href","/docs/transformers/pr_15774/en/model_doc/t5#transformers.T5ForConditionalGeneration"),c(uS,"href","/docs/transformers/pr_15774/en/model_doc/tapas#transformers.TapasForMaskedLM"),c(bS,"href","/docs/transformers/pr_15774/en/model_doc/transfo-xl#transformers.TransfoXLLMHeadModel"),c(vS,"href","/docs/transformers/pr_15774/en/model_doc/unispeech#transformers.UniSpeechForPreTraining"),c(TS,"href","/docs/transformers/pr_15774/en/model_doc/unispeech-sat#transformers.UniSpeechSatForPreTraining"),c(FS,"href","/docs/transformers/pr_15774/en/model_doc/visual_bert#transformers.VisualBertForPreTraining"),c(CS,"href","/docs/transformers/pr_15774/en/model_doc/vit_mae#transformers.ViTMAEForPreTraining"),c(MS,"href","/docs/transformers/pr_15774/en/model_doc/wav2vec2#transformers.Wav2Vec2ForPreTraining"),c(ES,"href","/docs/transformers/pr_15774/en/model_doc/xlm#transformers.XLMWithLMHeadModel"),c(yS,"href","/docs/transformers/pr_15774/en/model_doc/xlm-roberta#transformers.XLMRobertaForMaskedLM"),c(wS,"href","/docs/transformers/pr_15774/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForMaskedLM"),c(AS,"href","/docs/transformers/pr_15774/en/model_doc/xlnet#transformers.XLNetLMHeadModel"),c(xe,"class","docstring"),c(Wo,"class","docstring"),c(N_,"id","transformers.AutoModelForCausalLM"),c(N_,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(N_,"href","#transformers.AutoModelForCausalLM"),c(Wi,"class","relative group"),c(qr,"class","docstring"),c(LS,"href","/docs/transformers/pr_15774/en/model_doc/bart#transformers.BartForCausalLM"),c(BS,"href","/docs/transformers/pr_15774/en/model_doc/bert#transformers.BertLMHeadModel"),c(kS,"href","/docs/transformers/pr_15774/en/model_doc/bert-generation#transformers.BertGenerationDecoder"),c(xS,"href","/docs/transformers/pr_15774/en/model_doc/big_bird#transformers.BigBirdForCausalLM"),c(RS,"href","/docs/transformers/pr_15774/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusForCausalLM"),c(SS,"href","/docs/transformers/pr_15774/en/model_doc/blenderbot#transformers.BlenderbotForCausalLM"),c(PS,"href","/docs/transformers/pr_15774/en/model_doc/blenderbot-small#transformers.BlenderbotSmallForCausalLM"),c($S,"href","/docs/transformers/pr_15774/en/model_doc/camembert#transformers.CamembertForCausalLM"),c(IS,"href","/docs/transformers/pr_15774/en/model_doc/ctrl#transformers.CTRLLMHeadModel"),c(jS,"href","/docs/transformers/pr_15774/en/model_doc/electra#transformers.ElectraForCausalLM"),c(NS,"href","/docs/transformers/pr_15774/en/model_doc/gpt2#transformers.GPT2LMHeadModel"),c(DS,"href","/docs/transformers/pr_15774/en/model_doc/gpt_neo#transformers.GPTNeoForCausalLM"),c(qS,"href","/docs/transformers/pr_15774/en/model_doc/gptj#transformers.GPTJForCausalLM"),c(GS,"href","/docs/transformers/pr_15774/en/model_doc/marian#transformers.MarianForCausalLM"),c(OS,"href","/docs/transformers/pr_15774/en/model_doc/mbart#transformers.MBartForCausalLM"),c(XS,"href","/docs/transformers/pr_15774/en/model_doc/megatron-bert#transformers.MegatronBertForCausalLM"),c(zS,"href","/docs/transformers/pr_15774/en/model_doc/openai-gpt#transformers.OpenAIGPTLMHeadModel"),c(VS,"href","/docs/transformers/pr_15774/en/model_doc/pegasus#transformers.PegasusForCausalLM"),c(WS,"href","/docs/transformers/pr_15774/en/model_doc/plbart#transformers.PLBartForCausalLM"),c(QS,"href","/docs/transformers/pr_15774/en/model_doc/prophetnet#transformers.ProphetNetForCausalLM"),c(HS,"href","/docs/transformers/pr_15774/en/model_doc/qdqbert#transformers.QDQBertLMHeadModel"),c(US,"href","/docs/transformers/pr_15774/en/model_doc/reformer#transformers.ReformerModelWithLMHead"),c(JS,"href","/docs/transformers/pr_15774/en/model_doc/rembert#transformers.RemBertForCausalLM"),c(YS,"href","/docs/transformers/pr_15774/en/model_doc/roberta#transformers.RobertaForCausalLM"),c(KS,"href","/docs/transformers/pr_15774/en/model_doc/roformer#transformers.RoFormerForCausalLM"),c(ZS,"href","/docs/transformers/pr_15774/en/model_doc/speech_to_text_2#transformers.Speech2Text2ForCausalLM"),c(eP,"href","/docs/transformers/pr_15774/en/model_doc/transfo-xl#transformers.TransfoXLLMHeadModel"),c(oP,"href","/docs/transformers/pr_15774/en/model_doc/trocr#transformers.TrOCRForCausalLM"),c(rP,"href","/docs/transformers/pr_15774/en/model_doc/xglm#transformers.XGLMForCausalLM"),c(tP,"href","/docs/transformers/pr_15774/en/model_doc/xlm#transformers.XLMWithLMHeadModel"),c(aP,"href","/docs/transformers/pr_15774/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetForCausalLM"),c(nP,"href","/docs/transformers/pr_15774/en/model_doc/xlm-roberta#transformers.XLMRobertaForCausalLM"),c(sP,"href","/docs/transformers/pr_15774/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForCausalLM"),c(lP,"href","/docs/transformers/pr_15774/en/model_doc/xlnet#transformers.XLNetLMHeadModel"),c(Re,"class","docstring"),c(Qo,"class","docstring"),c(Tu,"id","transformers.AutoModelForMaskedLM"),c(Tu,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Tu,"href","#transformers.AutoModelForMaskedLM"),c(Ui,"class","relative group"),c(Gr,"class","docstring"),c(iP,"href","/docs/transformers/pr_15774/en/model_doc/albert#transformers.AlbertForMaskedLM"),c(dP,"href","/docs/transformers/pr_15774/en/model_doc/bart#transformers.BartForConditionalGeneration"),c(cP,"href","/docs/transformers/pr_15774/en/model_doc/bert#transformers.BertForMaskedLM"),c(fP,"href","/docs/transformers/pr_15774/en/model_doc/big_bird#transformers.BigBirdForMaskedLM"),c(mP,"href","/docs/transformers/pr_15774/en/model_doc/camembert#transformers.CamembertForMaskedLM"),c(gP,"href","/docs/transformers/pr_15774/en/model_doc/convbert#transformers.ConvBertForMaskedLM"),c(hP,"href","/docs/transformers/pr_15774/en/model_doc/deberta#transformers.DebertaForMaskedLM"),c(pP,"href","/docs/transformers/pr_15774/en/model_doc/deberta-v2#transformers.DebertaV2ForMaskedLM"),c(_P,"href","/docs/transformers/pr_15774/en/model_doc/distilbert#transformers.DistilBertForMaskedLM"),c(uP,"href","/docs/transformers/pr_15774/en/model_doc/electra#transformers.ElectraForMaskedLM"),c(bP,"href","/docs/transformers/pr_15774/en/model_doc/flaubert#transformers.FlaubertWithLMHeadModel"),c(vP,"href","/docs/transformers/pr_15774/en/model_doc/fnet#transformers.FNetForMaskedLM"),c(TP,"href","/docs/transformers/pr_15774/en/model_doc/funnel#transformers.FunnelForMaskedLM"),c(FP,"href","/docs/transformers/pr_15774/en/model_doc/ibert#transformers.IBertForMaskedLM"),c(CP,"href","/docs/transformers/pr_15774/en/model_doc/layoutlm#transformers.LayoutLMForMaskedLM"),c(MP,"href","/docs/transformers/pr_15774/en/model_doc/longformer#transformers.LongformerForMaskedLM"),c(EP,"href","/docs/transformers/pr_15774/en/model_doc/mbart#transformers.MBartForConditionalGeneration"),c(yP,"href","/docs/transformers/pr_15774/en/model_doc/megatron-bert#transformers.MegatronBertForMaskedLM"),c(wP,"href","/docs/transformers/pr_15774/en/model_doc/mobilebert#transformers.MobileBertForMaskedLM"),c(AP,"href","/docs/transformers/pr_15774/en/model_doc/mpnet#transformers.MPNetForMaskedLM"),c(LP,"href","/docs/transformers/pr_15774/en/model_doc/nystromformer#transformers.NystromformerForMaskedLM"),c(BP,"href","/docs/transformers/pr_15774/en/model_doc/perceiver#transformers.PerceiverForMaskedLM"),c(kP,"href","/docs/transformers/pr_15774/en/model_doc/qdqbert#transformers.QDQBertForMaskedLM"),c(xP,"href","/docs/transformers/pr_15774/en/model_doc/reformer#transformers.ReformerForMaskedLM"),c(RP,"href","/docs/transformers/pr_15774/en/model_doc/rembert#transformers.RemBertForMaskedLM"),c(SP,"href","/docs/transformers/pr_15774/en/model_doc/roberta#transformers.RobertaForMaskedLM"),c(PP,"href","/docs/transformers/pr_15774/en/model_doc/roformer#transformers.RoFormerForMaskedLM"),c($P,"href","/docs/transformers/pr_15774/en/model_doc/squeezebert#transformers.SqueezeBertForMaskedLM"),c(IP,"href","/docs/transformers/pr_15774/en/model_doc/tapas#transformers.TapasForMaskedLM"),c(jP,"href","/docs/transformers/pr_15774/en/model_doc/xlm#transformers.XLMWithLMHeadModel"),c(NP,"href","/docs/transformers/pr_15774/en/model_doc/xlm-roberta#transformers.XLMRobertaForMaskedLM"),c(DP,"href","/docs/transformers/pr_15774/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForMaskedLM"),c(qP,"href","/docs/transformers/pr_15774/en/model_doc/yoso#transformers.YosoForMaskedLM"),c(Se,"class","docstring"),c(Ho,"class","docstring"),c(r1,"id","transformers.AutoModelForSeq2SeqLM"),c(r1,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(r1,"href","#transformers.AutoModelForSeq2SeqLM"),c(Ki,"class","relative group"),c(Or,"class","docstring"),c(GP,"href","/docs/transformers/pr_15774/en/model_doc/bart#transformers.BartForConditionalGeneration"),c(OP,"href","/docs/transformers/pr_15774/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusForConditionalGeneration"),c(XP,"href","/docs/transformers/pr_15774/en/model_doc/blenderbot#transformers.BlenderbotForConditionalGeneration"),c(zP,"href","/docs/transformers/pr_15774/en/model_doc/blenderbot-small#transformers.BlenderbotSmallForConditionalGeneration"),c(VP,"href","/docs/transformers/pr_15774/en/model_doc/encoder-decoder#transformers.EncoderDecoderModel"),c(WP,"href","/docs/transformers/pr_15774/en/model_doc/fsmt#transformers.FSMTForConditionalGeneration"),c(QP,"href","/docs/transformers/pr_15774/en/model_doc/led#transformers.LEDForConditionalGeneration"),c(HP,"href","/docs/transformers/pr_15774/en/model_doc/m2m_100#transformers.M2M100ForConditionalGeneration"),c(UP,"href","/docs/transformers/pr_15774/en/model_doc/marian#transformers.MarianMTModel"),c(JP,"href","/docs/transformers/pr_15774/en/model_doc/mbart#transformers.MBartForConditionalGeneration"),c(YP,"href","/docs/transformers/pr_15774/en/model_doc/mt5#transformers.MT5ForConditionalGeneration"),c(KP,"href","/docs/transformers/pr_15774/en/model_doc/pegasus#transformers.PegasusForConditionalGeneration"),c(ZP,"href","/docs/transformers/pr_15774/en/model_doc/plbart#transformers.PLBartForConditionalGeneration"),c(e$,"href","/docs/transformers/pr_15774/en/model_doc/prophetnet#transformers.ProphetNetForConditionalGeneration"),c(o$,"href","/docs/transformers/pr_15774/en/model_doc/t5#transformers.T5ForConditionalGeneration"),c(r$,"href","/docs/transformers/pr_15774/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetForConditionalGeneration"),c(Pe,"class","docstring"),c(Uo,"class","docstring"),c(T1,"id","transformers.AutoModelForSequenceClassification"),c(T1,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(T1,"href","#transformers.AutoModelForSequenceClassification"),c(od,"class","relative group"),c(Xr,"class","docstring"),c(t$,"href","/docs/transformers/pr_15774/en/model_doc/albert#transformers.AlbertForSequenceClassification"),c(a$,"href","/docs/transformers/pr_15774/en/model_doc/bart#transformers.BartForSequenceClassification"),c(n$,"href","/docs/transformers/pr_15774/en/model_doc/bert#transformers.BertForSequenceClassification"),c(s$,"href","/docs/transformers/pr_15774/en/model_doc/big_bird#transformers.BigBirdForSequenceClassification"),c(l$,"href","/docs/transformers/pr_15774/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusForSequenceClassification"),c(i$,"href","/docs/transformers/pr_15774/en/model_doc/camembert#transformers.CamembertForSequenceClassification"),c(d$,"href","/docs/transformers/pr_15774/en/model_doc/canine#transformers.CanineForSequenceClassification"),c(c$,"href","/docs/transformers/pr_15774/en/model_doc/convbert#transformers.ConvBertForSequenceClassification"),c(f$,"href","/docs/transformers/pr_15774/en/model_doc/ctrl#transformers.CTRLForSequenceClassification"),c(m$,"href","/docs/transformers/pr_15774/en/model_doc/deberta#transformers.DebertaForSequenceClassification"),c(g$,"href","/docs/transformers/pr_15774/en/model_doc/deberta-v2#transformers.DebertaV2ForSequenceClassification"),c(h$,"href","/docs/transformers/pr_15774/en/model_doc/distilbert#transformers.DistilBertForSequenceClassification"),c(p$,"href","/docs/transformers/pr_15774/en/model_doc/electra#transformers.ElectraForSequenceClassification"),c(_$,"href","/docs/transformers/pr_15774/en/model_doc/flaubert#transformers.FlaubertForSequenceClassification"),c(u$,"href","/docs/transformers/pr_15774/en/model_doc/fnet#transformers.FNetForSequenceClassification"),c(b$,"href","/docs/transformers/pr_15774/en/model_doc/funnel#transformers.FunnelForSequenceClassification"),c(v$,"href","/docs/transformers/pr_15774/en/model_doc/gpt2#transformers.GPT2ForSequenceClassification"),c(T$,"href","/docs/transformers/pr_15774/en/model_doc/gpt_neo#transformers.GPTNeoForSequenceClassification"),c(F$,"href","/docs/transformers/pr_15774/en/model_doc/gptj#transformers.GPTJForSequenceClassification"),c(C$,"href","/docs/transformers/pr_15774/en/model_doc/ibert#transformers.IBertForSequenceClassification"),c(M$,"href","/docs/transformers/pr_15774/en/model_doc/layoutlm#transformers.LayoutLMForSequenceClassification"),c(E$,"href","/docs/transformers/pr_15774/en/model_doc/layoutlmv2#transformers.LayoutLMv2ForSequenceClassification"),c(y$,"href","/docs/transformers/pr_15774/en/model_doc/led#transformers.LEDForSequenceClassification"),c(w$,"href","/docs/transformers/pr_15774/en/model_doc/longformer#transformers.LongformerForSequenceClassification"),c(A$,"href","/docs/transformers/pr_15774/en/model_doc/mbart#transformers.MBartForSequenceClassification"),c(L$,"href","/docs/transformers/pr_15774/en/model_doc/megatron-bert#transformers.MegatronBertForSequenceClassification"),c(B$,"href","/docs/transformers/pr_15774/en/model_doc/mobilebert#transformers.MobileBertForSequenceClassification"),c(k$,"href","/docs/transformers/pr_15774/en/model_doc/mpnet#transformers.MPNetForSequenceClassification"),c(x$,"href","/docs/transformers/pr_15774/en/model_doc/nystromformer#transformers.NystromformerForSequenceClassification"),c(R$,"href","/docs/transformers/pr_15774/en/model_doc/openai-gpt#transformers.OpenAIGPTForSequenceClassification"),c(S$,"href","/docs/transformers/pr_15774/en/model_doc/perceiver#transformers.PerceiverForSequenceClassification"),c(P$,"href","/docs/transformers/pr_15774/en/model_doc/plbart#transformers.PLBartForSequenceClassification"),c($$,"href","/docs/transformers/pr_15774/en/model_doc/qdqbert#transformers.QDQBertForSequenceClassification"),c(I$,"href","/docs/transformers/pr_15774/en/model_doc/reformer#transformers.ReformerForSequenceClassification"),c(j$,"href","/docs/transformers/pr_15774/en/model_doc/rembert#transformers.RemBertForSequenceClassification"),c(N$,"href","/docs/transformers/pr_15774/en/model_doc/roberta#transformers.RobertaForSequenceClassification"),c(D$,"href","/docs/transformers/pr_15774/en/model_doc/roformer#transformers.RoFormerForSequenceClassification"),c(q$,"href","/docs/transformers/pr_15774/en/model_doc/squeezebert#transformers.SqueezeBertForSequenceClassification"),c(G$,"href","/docs/transformers/pr_15774/en/model_doc/tapas#transformers.TapasForSequenceClassification"),c(O$,"href","/docs/transformers/pr_15774/en/model_doc/transfo-xl#transformers.TransfoXLForSequenceClassification"),c(X$,"href","/docs/transformers/pr_15774/en/model_doc/xlm#transformers.XLMForSequenceClassification"),c(z$,"href","/docs/transformers/pr_15774/en/model_doc/xlm-roberta#transformers.XLMRobertaForSequenceClassification"),c(V$,"href","/docs/transformers/pr_15774/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForSequenceClassification"),c(W$,"href","/docs/transformers/pr_15774/en/model_doc/xlnet#transformers.XLNetForSequenceClassification"),c(Q$,"href","/docs/transformers/pr_15774/en/model_doc/yoso#transformers.YosoForSequenceClassification"),c($e,"class","docstring"),c(Jo,"class","docstring"),c(g7,"id","transformers.AutoModelForMultipleChoice"),c(g7,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(g7,"href","#transformers.AutoModelForMultipleChoice"),c(ad,"class","relative group"),c(zr,"class","docstring"),c(H$,"href","/docs/transformers/pr_15774/en/model_doc/albert#transformers.AlbertForMultipleChoice"),c(U$,"href","/docs/transformers/pr_15774/en/model_doc/bert#transformers.BertForMultipleChoice"),c(J$,"href","/docs/transformers/pr_15774/en/model_doc/big_bird#transformers.BigBirdForMultipleChoice"),c(Y$,"href","/docs/transformers/pr_15774/en/model_doc/camembert#transformers.CamembertForMultipleChoice"),c(K$,"href","/docs/transformers/pr_15774/en/model_doc/canine#transformers.CanineForMultipleChoice"),c(Z$,"href","/docs/transformers/pr_15774/en/model_doc/convbert#transformers.ConvBertForMultipleChoice"),c(eI,"href","/docs/transformers/pr_15774/en/model_doc/distilbert#transformers.DistilBertForMultipleChoice"),c(oI,"href","/docs/transformers/pr_15774/en/model_doc/electra#transformers.ElectraForMultipleChoice"),c(rI,"href","/docs/transformers/pr_15774/en/model_doc/flaubert#transformers.FlaubertForMultipleChoice"),c(tI,"href","/docs/transformers/pr_15774/en/model_doc/fnet#transformers.FNetForMultipleChoice"),c(aI,"href","/docs/transformers/pr_15774/en/model_doc/funnel#transformers.FunnelForMultipleChoice"),c(nI,"href","/docs/transformers/pr_15774/en/model_doc/ibert#transformers.IBertForMultipleChoice"),c(sI,"href","/docs/transformers/pr_15774/en/model_doc/longformer#transformers.LongformerForMultipleChoice"),c(lI,"href","/docs/transformers/pr_15774/en/model_doc/megatron-bert#transformers.MegatronBertForMultipleChoice"),c(iI,"href","/docs/transformers/pr_15774/en/model_doc/mobilebert#transformers.MobileBertForMultipleChoice"),c(dI,"href","/docs/transformers/pr_15774/en/model_doc/mpnet#transformers.MPNetForMultipleChoice"),c(cI,"href","/docs/transformers/pr_15774/en/model_doc/nystromformer#transformers.NystromformerForMultipleChoice"),c(fI,"href","/docs/transformers/pr_15774/en/model_doc/qdqbert#transformers.QDQBertForMultipleChoice"),c(mI,"href","/docs/transformers/pr_15774/en/model_doc/rembert#transformers.RemBertForMultipleChoice"),c(gI,"href","/docs/transformers/pr_15774/en/model_doc/roberta#transformers.RobertaForMultipleChoice"),c(hI,"href","/docs/transformers/pr_15774/en/model_doc/roformer#transformers.RoFormerForMultipleChoice"),c(pI,"href","/docs/transformers/pr_15774/en/model_doc/squeezebert#transformers.SqueezeBertForMultipleChoice"),c(_I,"href","/docs/transformers/pr_15774/en/model_doc/xlm#transformers.XLMForMultipleChoice"),c(uI,"href","/docs/transformers/pr_15774/en/model_doc/xlm-roberta#transformers.XLMRobertaForMultipleChoice"),c(bI,"href","/docs/transformers/pr_15774/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForMultipleChoice"),c(vI,"href","/docs/transformers/pr_15774/en/model_doc/xlnet#transformers.XLNetForMultipleChoice"),c(TI,"href","/docs/transformers/pr_15774/en/model_doc/yoso#transformers.YosoForMultipleChoice"),c(Ie,"class","docstring"),c(Yo,"class","docstring"),c(O7,"id","transformers.AutoModelForNextSentencePrediction"),c(O7,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(O7,"href","#transformers.AutoModelForNextSentencePrediction"),c(ld,"class","relative group"),c(Vr,"class","docstring"),c(FI,"href","/docs/transformers/pr_15774/en/model_doc/bert#transformers.BertForNextSentencePrediction"),c(CI,"href","/docs/transformers/pr_15774/en/model_doc/fnet#transformers.FNetForNextSentencePrediction"),c(MI,"href","/docs/transformers/pr_15774/en/model_doc/megatron-bert#transformers.MegatronBertForNextSentencePrediction"),c(EI,"href","/docs/transformers/pr_15774/en/model_doc/mobilebert#transformers.MobileBertForNextSentencePrediction"),c(yI,"href","/docs/transformers/pr_15774/en/model_doc/qdqbert#transformers.QDQBertForNextSentencePrediction"),c(je,"class","docstring"),c(Ko,"class","docstring"),c(U7,"id","transformers.AutoModelForTokenClassification"),c(U7,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(U7,"href","#transformers.AutoModelForTokenClassification"),c(cd,"class","relative group"),c(Wr,"class","docstring"),c(wI,"href","/docs/transformers/pr_15774/en/model_doc/albert#transformers.AlbertForTokenClassification"),c(AI,"href","/docs/transformers/pr_15774/en/model_doc/bert#transformers.BertForTokenClassification"),c(LI,"href","/docs/transformers/pr_15774/en/model_doc/big_bird#transformers.BigBirdForTokenClassification"),c(BI,"href","/docs/transformers/pr_15774/en/model_doc/camembert#transformers.CamembertForTokenClassification"),c(kI,"href","/docs/transformers/pr_15774/en/model_doc/canine#transformers.CanineForTokenClassification"),c(xI,"href","/docs/transformers/pr_15774/en/model_doc/convbert#transformers.ConvBertForTokenClassification"),c(RI,"href","/docs/transformers/pr_15774/en/model_doc/deberta#transformers.DebertaForTokenClassification"),c(SI,"href","/docs/transformers/pr_15774/en/model_doc/deberta-v2#transformers.DebertaV2ForTokenClassification"),c(PI,"href","/docs/transformers/pr_15774/en/model_doc/distilbert#transformers.DistilBertForTokenClassification"),c($I,"href","/docs/transformers/pr_15774/en/model_doc/electra#transformers.ElectraForTokenClassification"),c(II,"href","/docs/transformers/pr_15774/en/model_doc/flaubert#transformers.FlaubertForTokenClassification"),c(jI,"href","/docs/transformers/pr_15774/en/model_doc/fnet#transformers.FNetForTokenClassification"),c(NI,"href","/docs/transformers/pr_15774/en/model_doc/funnel#transformers.FunnelForTokenClassification"),c(DI,"href","/docs/transformers/pr_15774/en/model_doc/gpt2#transformers.GPT2ForTokenClassification"),c(qI,"href","/docs/transformers/pr_15774/en/model_doc/ibert#transformers.IBertForTokenClassification"),c(GI,"href","/docs/transformers/pr_15774/en/model_doc/layoutlm#transformers.LayoutLMForTokenClassification"),c(OI,"href","/docs/transformers/pr_15774/en/model_doc/layoutlmv2#transformers.LayoutLMv2ForTokenClassification"),c(XI,"href","/docs/transformers/pr_15774/en/model_doc/longformer#transformers.LongformerForTokenClassification"),c(zI,"href","/docs/transformers/pr_15774/en/model_doc/megatron-bert#transformers.MegatronBertForTokenClassification"),c(VI,"href","/docs/transformers/pr_15774/en/model_doc/mobilebert#transformers.MobileBertForTokenClassification"),c(WI,"href","/docs/transformers/pr_15774/en/model_doc/mpnet#transformers.MPNetForTokenClassification"),c(QI,"href","/docs/transformers/pr_15774/en/model_doc/nystromformer#transformers.NystromformerForTokenClassification"),c(HI,"href","/docs/transformers/pr_15774/en/model_doc/qdqbert#transformers.QDQBertForTokenClassification"),c(UI,"href","/docs/transformers/pr_15774/en/model_doc/rembert#transformers.RemBertForTokenClassification"),c(JI,"href","/docs/transformers/pr_15774/en/model_doc/roberta#transformers.RobertaForTokenClassification"),c(YI,"href","/docs/transformers/pr_15774/en/model_doc/roformer#transformers.RoFormerForTokenClassification"),c(KI,"href","/docs/transformers/pr_15774/en/model_doc/squeezebert#transformers.SqueezeBertForTokenClassification"),c(ZI,"href","/docs/transformers/pr_15774/en/model_doc/xlm#transformers.XLMForTokenClassification"),c(ej,"href","/docs/transformers/pr_15774/en/model_doc/xlm-roberta#transformers.XLMRobertaForTokenClassification"),c(oj,"href","/docs/transformers/pr_15774/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForTokenClassification"),c(rj,"href","/docs/transformers/pr_15774/en/model_doc/xlnet#transformers.XLNetForTokenClassification"),c(tj,"href","/docs/transformers/pr_15774/en/model_doc/yoso#transformers.YosoForTokenClassification"),c(Ne,"class","docstring"),c(Zo,"class","docstring"),c(B4,"id","transformers.AutoModelForQuestionAnswering"),c(B4,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(B4,"href","#transformers.AutoModelForQuestionAnswering"),c(gd,"class","relative group"),c(Qr,"class","docstring"),c(aj,"href","/docs/transformers/pr_15774/en/model_doc/albert#transformers.AlbertForQuestionAnswering"),c(nj,"href","/docs/transformers/pr_15774/en/model_doc/bart#transformers.BartForQuestionAnswering"),c(sj,"href","/docs/transformers/pr_15774/en/model_doc/bert#transformers.BertForQuestionAnswering"),c(lj,"href","/docs/transformers/pr_15774/en/model_doc/big_bird#transformers.BigBirdForQuestionAnswering"),c(ij,"href","/docs/transformers/pr_15774/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusForQuestionAnswering"),c(dj,"href","/docs/transformers/pr_15774/en/model_doc/camembert#transformers.CamembertForQuestionAnswering"),c(cj,"href","/docs/transformers/pr_15774/en/model_doc/canine#transformers.CanineForQuestionAnswering"),c(fj,"href","/docs/transformers/pr_15774/en/model_doc/convbert#transformers.ConvBertForQuestionAnswering"),c(mj,"href","/docs/transformers/pr_15774/en/model_doc/deberta#transformers.DebertaForQuestionAnswering"),c(gj,"href","/docs/transformers/pr_15774/en/model_doc/deberta-v2#transformers.DebertaV2ForQuestionAnswering"),c(hj,"href","/docs/transformers/pr_15774/en/model_doc/distilbert#transformers.DistilBertForQuestionAnswering"),c(pj,"href","/docs/transformers/pr_15774/en/model_doc/electra#transformers.ElectraForQuestionAnswering"),c(_j,"href","/docs/transformers/pr_15774/en/model_doc/flaubert#transformers.FlaubertForQuestionAnsweringSimple"),c(uj,"href","/docs/transformers/pr_15774/en/model_doc/fnet#transformers.FNetForQuestionAnswering"),c(bj,"href","/docs/transformers/pr_15774/en/model_doc/funnel#transformers.FunnelForQuestionAnswering"),c(vj,"href","/docs/transformers/pr_15774/en/model_doc/gptj#transformers.GPTJForQuestionAnswering"),c(Tj,"href","/docs/transformers/pr_15774/en/model_doc/ibert#transformers.IBertForQuestionAnswering"),c(Fj,"href","/docs/transformers/pr_15774/en/model_doc/layoutlmv2#transformers.LayoutLMv2ForQuestionAnswering"),c(Cj,"href","/docs/transformers/pr_15774/en/model_doc/led#transformers.LEDForQuestionAnswering"),c(Mj,"href","/docs/transformers/pr_15774/en/model_doc/longformer#transformers.LongformerForQuestionAnswering"),c(Ej,"href","/docs/transformers/pr_15774/en/model_doc/lxmert#transformers.LxmertForQuestionAnswering"),c(yj,"href","/docs/transformers/pr_15774/en/model_doc/mbart#transformers.MBartForQuestionAnswering"),c(wj,"href","/docs/transformers/pr_15774/en/model_doc/megatron-bert#transformers.MegatronBertForQuestionAnswering"),c(Aj,"href","/docs/transformers/pr_15774/en/model_doc/mobilebert#transformers.MobileBertForQuestionAnswering"),c(Lj,"href","/docs/transformers/pr_15774/en/model_doc/mpnet#transformers.MPNetForQuestionAnswering"),c(Bj,"href","/docs/transformers/pr_15774/en/model_doc/nystromformer#transformers.NystromformerForQuestionAnswering"),c(kj,"href","/docs/transformers/pr_15774/en/model_doc/qdqbert#transformers.QDQBertForQuestionAnswering"),c(xj,"href","/docs/transformers/pr_15774/en/model_doc/reformer#transformers.ReformerForQuestionAnswering"),c(Rj,"href","/docs/transformers/pr_15774/en/model_doc/rembert#transformers.RemBertForQuestionAnswering"),c(Sj,"href","/docs/transformers/pr_15774/en/model_doc/roberta#transformers.RobertaForQuestionAnswering"),c(Pj,"href","/docs/transformers/pr_15774/en/model_doc/roformer#transformers.RoFormerForQuestionAnswering"),c($j,"href","/docs/transformers/pr_15774/en/model_doc/splinter#transformers.SplinterForQuestionAnswering"),c(Ij,"href","/docs/transformers/pr_15774/en/model_doc/squeezebert#transformers.SqueezeBertForQuestionAnswering"),c(jj,"href","/docs/transformers/pr_15774/en/model_doc/xlm#transformers.XLMForQuestionAnsweringSimple"),c(Nj,"href","/docs/transformers/pr_15774/en/model_doc/xlm-roberta#transformers.XLMRobertaForQuestionAnswering"),c(Dj,"href","/docs/transformers/pr_15774/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForQuestionAnswering"),c(qj,"href","/docs/transformers/pr_15774/en/model_doc/xlnet#transformers.XLNetForQuestionAnsweringSimple"),c(Gj,"href","/docs/transformers/pr_15774/en/model_doc/yoso#transformers.YosoForQuestionAnswering"),c(De,"class","docstring"),c(er,"class","docstring"),c(pb,"id","transformers.AutoModelForTableQuestionAnswering"),c(pb,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(pb,"href","#transformers.AutoModelForTableQuestionAnswering"),c(_d,"class","relative group"),c(Hr,"class","docstring"),c(Oj,"href","/docs/transformers/pr_15774/en/model_doc/tapas#transformers.TapasForQuestionAnswering"),c(qe,"class","docstring"),c(or,"class","docstring"),c(bb,"id","transformers.AutoModelForImageClassification"),c(bb,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(bb,"href","#transformers.AutoModelForImageClassification"),c(vd,"class","relative group"),c(Ur,"class","docstring"),c(Xj,"href","/docs/transformers/pr_15774/en/model_doc/beit#transformers.BeitForImageClassification"),c(zj,"href","/docs/transformers/pr_15774/en/model_doc/convnext#transformers.ConvNextForImageClassification"),c(Vj,"href","/docs/transformers/pr_15774/en/model_doc/deit#transformers.DeiTForImageClassification"),c(Wj,"href","/docs/transformers/pr_15774/en/model_doc/deit#transformers.DeiTForImageClassificationWithTeacher"),c(Qj,"href","/docs/transformers/pr_15774/en/model_doc/imagegpt#transformers.ImageGPTForImageClassification"),c(Hj,"href","/docs/transformers/pr_15774/en/model_doc/perceiver#transformers.PerceiverForImageClassificationLearned"),c(Uj,"href","/docs/transformers/pr_15774/en/model_doc/perceiver#transformers.PerceiverForImageClassificationFourier"),c(Jj,"href","/docs/transformers/pr_15774/en/model_doc/perceiver#transformers.PerceiverForImageClassificationConvProcessing"),c(Yj,"href","/docs/transformers/pr_15774/en/model_doc/poolformer#transformers.PoolFormerForImageClassification"),c(Kj,"href","/docs/transformers/pr_15774/en/model_doc/segformer#transformers.SegformerForImageClassification"),c(Zj,"href","/docs/transformers/pr_15774/en/model_doc/swin#transformers.SwinForImageClassification"),c(eN,"href","/docs/transformers/pr_15774/en/model_doc/vit#transformers.ViTForImageClassification"),c(Ge,"class","docstring"),c(rr,"class","docstring"),c(Ab,"id","transformers.AutoModelForVision2Seq"),c(Ab,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Ab,"href","#transformers.AutoModelForVision2Seq"),c(Cd,"class","relative group"),c(Jr,"class","docstring"),c(oN,"href","/docs/transformers/pr_15774/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderModel"),c(Oe,"class","docstring"),c(tr,"class","docstring"),c(kb,"id","transformers.AutoModelForAudioClassification"),c(kb,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(kb,"href","#transformers.AutoModelForAudioClassification"),c(yd,"class","relative group"),c(Yr,"class","docstring"),c(rN,"href","/docs/transformers/pr_15774/en/model_doc/hubert#transformers.HubertForSequenceClassification"),c(tN,"href","/docs/transformers/pr_15774/en/model_doc/sew#transformers.SEWForSequenceClassification"),c(aN,"href","/docs/transformers/pr_15774/en/model_doc/sew-d#transformers.SEWDForSequenceClassification"),c(nN,"href","/docs/transformers/pr_15774/en/model_doc/unispeech#transformers.UniSpeechForSequenceClassification"),c(sN,"href","/docs/transformers/pr_15774/en/model_doc/unispeech-sat#transformers.UniSpeechSatForSequenceClassification"),c(lN,"href","/docs/transformers/pr_15774/en/model_doc/wav2vec2#transformers.Wav2Vec2ForSequenceClassification"),c(iN,"href","/docs/transformers/pr_15774/en/model_doc/wavlm#transformers.WavLMForSequenceClassification"),c(Xe,"class","docstring"),c(ar,"class","docstring"),c(Db,"id","transformers.AutoModelForAudioFrameClassification"),c(Db,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Db,"href","#transformers.AutoModelForAudioFrameClassification"),c(Ld,"class","relative group"),c(Kr,"class","docstring"),c(dN,"href","/docs/transformers/pr_15774/en/model_doc/unispeech-sat#transformers.UniSpeechSatForAudioFrameClassification"),c(cN,"href","/docs/transformers/pr_15774/en/model_doc/wav2vec2#transformers.Wav2Vec2ForAudioFrameClassification"),c(fN,"href","/docs/transformers/pr_15774/en/model_doc/wavlm#transformers.WavLMForAudioFrameClassification"),c(ze,"class","docstring"),c(nr,"class","docstring"),c(zb,"id","transformers.AutoModelForCTC"),c(zb,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(zb,"href","#transformers.AutoModelForCTC"),c(Rd,"class","relative group"),c(Zr,"class","docstring"),c(mN,"href","/docs/transformers/pr_15774/en/model_doc/hubert#transformers.HubertForCTC"),c(gN,"href","/docs/transformers/pr_15774/en/model_doc/sew#transformers.SEWForCTC"),c(hN,"href","/docs/transformers/pr_15774/en/model_doc/sew-d#transformers.SEWDForCTC"),c(pN,"href","/docs/transformers/pr_15774/en/model_doc/unispeech#transformers.UniSpeechForCTC"),c(_N,"href","/docs/transformers/pr_15774/en/model_doc/unispeech-sat#transformers.UniSpeechSatForCTC"),c(uN,"href","/docs/transformers/pr_15774/en/model_doc/wav2vec2#transformers.Wav2Vec2ForCTC"),c(bN,"href","/docs/transformers/pr_15774/en/model_doc/wavlm#transformers.WavLMForCTC"),c(Ve,"class","docstring"),c(sr,"class","docstring"),c(Zb,"id","transformers.AutoModelForSpeechSeq2Seq"),c(Zb,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Zb,"href","#transformers.AutoModelForSpeechSeq2Seq"),c($d,"class","relative group"),c(et,"class","docstring"),c(vN,"href","/docs/transformers/pr_15774/en/model_doc/speech-encoder-decoder#transformers.SpeechEncoderDecoderModel"),c(TN,"href","/docs/transformers/pr_15774/en/model_doc/speech_to_text#transformers.Speech2TextForConditionalGeneration"),c(We,"class","docstring"),c(lr,"class","docstring"),c(t5,"id","transformers.AutoModelForAudioXVector"),c(t5,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(t5,"href","#transformers.AutoModelForAudioXVector"),c(Nd,"class","relative group"),c(ot,"class","docstring"),c(FN,"href","/docs/transformers/pr_15774/en/model_doc/unispeech-sat#transformers.UniSpeechSatForXVector"),c(CN,"href","/docs/transformers/pr_15774/en/model_doc/wav2vec2#transformers.Wav2Vec2ForXVector"),c(MN,"href","/docs/transformers/pr_15774/en/model_doc/wavlm#transformers.WavLMForXVector"),c(Qe,"class","docstring"),c(ir,"class","docstring"),c(i5,"id","transformers.AutoModelForMaskedImageModeling"),c(i5,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(i5,"href","#transformers.AutoModelForMaskedImageModeling"),c(Od,"class","relative group"),c(rt,"class","docstring"),c(EN,"href","/docs/transformers/pr_15774/en/model_doc/deit#transformers.DeiTForMaskedImageModeling"),c(yN,"href","/docs/transformers/pr_15774/en/model_doc/swin#transformers.SwinForMaskedImageModeling"),c(wN,"href","/docs/transformers/pr_15774/en/model_doc/vit#transformers.ViTForMaskedImageModeling"),c(He,"class","docstring"),c(dr,"class","docstring"),c(g5,"id","transformers.AutoModelForObjectDetection"),c(g5,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(g5,"href","#transformers.AutoModelForObjectDetection"),c(Wd,"class","relative group"),c(tt,"class","docstring"),c(AN,"href","/docs/transformers/pr_15774/en/model_doc/detr#transformers.DetrForObjectDetection"),c(Ue,"class","docstring"),c(cr,"class","docstring"),c(_5,"id","transformers.AutoModelForImageSegmentation"),c(_5,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(_5,"href","#transformers.AutoModelForImageSegmentation"),c(Ud,"class","relative group"),c(at,"class","docstring"),c(LN,"href","/docs/transformers/pr_15774/en/model_doc/detr#transformers.DetrForSegmentation"),c(Je,"class","docstring"),c(fr,"class","docstring"),c(v5,"id","transformers.AutoModelForSemanticSegmentation"),c(v5,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(v5,"href","#transformers.AutoModelForSemanticSegmentation"),c(Kd,"class","relative group"),c(nt,"class","docstring"),c(BN,"href","/docs/transformers/pr_15774/en/model_doc/beit#transformers.BeitForSemanticSegmentation"),c(kN,"href","/docs/transformers/pr_15774/en/model_doc/segformer#transformers.SegformerForSemanticSegmentation"),c(Ye,"class","docstring"),c(mr,"class","docstring"),c(M5,"id","transformers.TFAutoModel"),c(M5,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(M5,"href","#transformers.TFAutoModel"),c(oc,"class","relative group"),c(st,"class","docstring"),c(xN,"href","/docs/transformers/pr_15774/en/model_doc/albert#transformers.TFAlbertModel"),c(RN,"href","/docs/transformers/pr_15774/en/model_doc/bart#transformers.TFBartModel"),c(SN,"href","/docs/transformers/pr_15774/en/model_doc/bert#transformers.TFBertModel"),c(PN,"href","/docs/transformers/pr_15774/en/model_doc/blenderbot#transformers.TFBlenderbotModel"),c($N,"href","/docs/transformers/pr_15774/en/model_doc/blenderbot-small#transformers.TFBlenderbotSmallModel"),c(IN,"href","/docs/transformers/pr_15774/en/model_doc/camembert#transformers.TFCamembertModel"),c(jN,"href","/docs/transformers/pr_15774/en/model_doc/clip#transformers.TFCLIPModel"),c(NN,"href","/docs/transformers/pr_15774/en/model_doc/convbert#transformers.TFConvBertModel"),c(DN,"href","/docs/transformers/pr_15774/en/model_doc/ctrl#transformers.TFCTRLModel"),c(qN,"href","/docs/transformers/pr_15774/en/model_doc/deberta#transformers.TFDebertaModel"),c(GN,"href","/docs/transformers/pr_15774/en/model_doc/deberta-v2#transformers.TFDebertaV2Model"),c(ON,"href","/docs/transformers/pr_15774/en/model_doc/distilbert#transformers.TFDistilBertModel"),c(XN,"href","/docs/transformers/pr_15774/en/model_doc/dpr#transformers.TFDPRQuestionEncoder"),c(zN,"href","/docs/transformers/pr_15774/en/model_doc/electra#transformers.TFElectraModel"),c(VN,"href","/docs/transformers/pr_15774/en/model_doc/flaubert#transformers.TFFlaubertModel"),c(WN,"href","/docs/transformers/pr_15774/en/model_doc/funnel#transformers.TFFunnelModel"),c(QN,"href","/docs/transformers/pr_15774/en/model_doc/funnel#transformers.TFFunnelBaseModel"),c(HN,"href","/docs/transformers/pr_15774/en/model_doc/gpt2#transformers.TFGPT2Model"),c(UN,"href","/docs/transformers/pr_15774/en/model_doc/hubert#transformers.TFHubertModel"),c(JN,"href","/docs/transformers/pr_15774/en/model_doc/layoutlm#transformers.TFLayoutLMModel"),c(YN,"href","/docs/transformers/pr_15774/en/model_doc/led#transformers.TFLEDModel"),c(KN,"href","/docs/transformers/pr_15774/en/model_doc/longformer#transformers.TFLongformerModel"),c(ZN,"href","/docs/transformers/pr_15774/en/model_doc/lxmert#transformers.TFLxmertModel"),c(eD,"href","/docs/transformers/pr_15774/en/model_doc/marian#transformers.TFMarianModel"),c(oD,"href","/docs/transformers/pr_15774/en/model_doc/mbart#transformers.TFMBartModel"),c(rD,"href","/docs/transformers/pr_15774/en/model_doc/mobilebert#transformers.TFMobileBertModel"),c(tD,"href","/docs/transformers/pr_15774/en/model_doc/mpnet#transformers.TFMPNetModel"),c(aD,"href","/docs/transformers/pr_15774/en/model_doc/mt5#transformers.TFMT5Model"),c(nD,"href","/docs/transformers/pr_15774/en/model_doc/openai-gpt#transformers.TFOpenAIGPTModel"),c(sD,"href","/docs/transformers/pr_15774/en/model_doc/pegasus#transformers.TFPegasusModel"),c(lD,"href","/docs/transformers/pr_15774/en/model_doc/rembert#transformers.TFRemBertModel"),c(iD,"href","/docs/transformers/pr_15774/en/model_doc/roberta#transformers.TFRobertaModel"),c(dD,"href","/docs/transformers/pr_15774/en/model_doc/roformer#transformers.TFRoFormerModel"),c(cD,"href","/docs/transformers/pr_15774/en/model_doc/speech_to_text#transformers.TFSpeech2TextModel"),c(fD,"href","/docs/transformers/pr_15774/en/model_doc/t5#transformers.TFT5Model"),c(mD,"href","/docs/transformers/pr_15774/en/model_doc/tapas#transformers.TFTapasModel"),c(gD,"href","/docs/transformers/pr_15774/en/model_doc/transfo-xl#transformers.TFTransfoXLModel"),c(hD,"href","/docs/transformers/pr_15774/en/model_doc/vit#transformers.TFViTModel"),c(pD,"href","/docs/transformers/pr_15774/en/model_doc/wav2vec2#transformers.TFWav2Vec2Model"),c(_D,"href","/docs/transformers/pr_15774/en/model_doc/xlm#transformers.TFXLMModel"),c(uD,"href","/docs/transformers/pr_15774/en/model_doc/xlm-roberta#transformers.TFXLMRobertaModel"),c(bD,"href","/docs/transformers/pr_15774/en/model_doc/xlnet#transformers.TFXLNetModel"),c(go,"class","docstring"),c(gr,"class","docstring"),c(c2,"id","transformers.TFAutoModelForPreTraining"),c(c2,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(c2,"href","#transformers.TFAutoModelForPreTraining"),c(ac,"class","relative group"),c(lt,"class","docstring"),c(vD,"href","/docs/transformers/pr_15774/en/model_doc/albert#transformers.TFAlbertForPreTraining"),c(TD,"href","/docs/transformers/pr_15774/en/model_doc/bart#transformers.TFBartForConditionalGeneration"),c(FD,"href","/docs/transformers/pr_15774/en/model_doc/bert#transformers.TFBertForPreTraining"),c(CD,"href","/docs/transformers/pr_15774/en/model_doc/camembert#transformers.TFCamembertForMaskedLM"),c(MD,"href","/docs/transformers/pr_15774/en/model_doc/ctrl#transformers.TFCTRLLMHeadModel"),c(ED,"href","/docs/transformers/pr_15774/en/model_doc/distilbert#transformers.TFDistilBertForMaskedLM"),c(yD,"href","/docs/transformers/pr_15774/en/model_doc/electra#transformers.TFElectraForPreTraining"),c(wD,"href","/docs/transformers/pr_15774/en/model_doc/flaubert#transformers.TFFlaubertWithLMHeadModel"),c(AD,"href","/docs/transformers/pr_15774/en/model_doc/funnel#transformers.TFFunnelForPreTraining"),c(LD,"href","/docs/transformers/pr_15774/en/model_doc/gpt2#transformers.TFGPT2LMHeadModel"),c(BD,"href","/docs/transformers/pr_15774/en/model_doc/layoutlm#transformers.TFLayoutLMForMaskedLM"),c(kD,"href","/docs/transformers/pr_15774/en/model_doc/lxmert#transformers.TFLxmertForPreTraining"),c(xD,"href","/docs/transformers/pr_15774/en/model_doc/mobilebert#transformers.TFMobileBertForPreTraining"),c(RD,"href","/docs/transformers/pr_15774/en/model_doc/mpnet#transformers.TFMPNetForMaskedLM"),c(SD,"href","/docs/transformers/pr_15774/en/model_doc/openai-gpt#transformers.TFOpenAIGPTLMHeadModel"),c(PD,"href","/docs/transformers/pr_15774/en/model_doc/roberta#transformers.TFRobertaForMaskedLM"),c($D,"href","/docs/transformers/pr_15774/en/model_doc/t5#transformers.TFT5ForConditionalGeneration"),c(ID,"href","/docs/transformers/pr_15774/en/model_doc/tapas#transformers.TFTapasForMaskedLM"),c(jD,"href","/docs/transformers/pr_15774/en/model_doc/transfo-xl#transformers.TFTransfoXLLMHeadModel"),c(ND,"href","/docs/transformers/pr_15774/en/model_doc/xlm#transformers.TFXLMWithLMHeadModel"),c(DD,"href","/docs/transformers/pr_15774/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForMaskedLM"),c(qD,"href","/docs/transformers/pr_15774/en/model_doc/xlnet#transformers.TFXLNetLMHeadModel"),c(ho,"class","docstring"),c(hr,"class","docstring"),c(S2,"id","transformers.TFAutoModelForCausalLM"),c(S2,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(S2,"href","#transformers.TFAutoModelForCausalLM"),c(lc,"class","relative group"),c(it,"class","docstring"),c(GD,"href","/docs/transformers/pr_15774/en/model_doc/bert#transformers.TFBertLMHeadModel"),c(OD,"href","/docs/transformers/pr_15774/en/model_doc/ctrl#transformers.TFCTRLLMHeadModel"),c(XD,"href","/docs/transformers/pr_15774/en/model_doc/gpt2#transformers.TFGPT2LMHeadModel"),c(zD,"href","/docs/transformers/pr_15774/en/model_doc/openai-gpt#transformers.TFOpenAIGPTLMHeadModel"),c(VD,"href","/docs/transformers/pr_15774/en/model_doc/rembert#transformers.TFRemBertForCausalLM"),c(WD,"href","/docs/transformers/pr_15774/en/model_doc/roberta#transformers.TFRobertaForCausalLM"),c(QD,"href","/docs/transformers/pr_15774/en/model_doc/roformer#transformers.TFRoFormerForCausalLM"),c(HD,"href","/docs/transformers/pr_15774/en/model_doc/transfo-xl#transformers.TFTransfoXLLMHeadModel"),c(UD,"href","/docs/transformers/pr_15774/en/model_doc/xlm#transformers.TFXLMWithLMHeadModel"),c(JD,"href","/docs/transformers/pr_15774/en/model_doc/xlnet#transformers.TFXLNetLMHeadModel"),c(po,"class","docstring"),c(pr,"class","docstring"),c(z2,"id","transformers.TFAutoModelForImageClassification"),c(z2,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(z2,"href","#transformers.TFAutoModelForImageClassification"),c(cc,"class","relative group"),c(dt,"class","docstring"),c(YD,"href","/docs/transformers/pr_15774/en/model_doc/vit#transformers.TFViTForImageClassification"),c(_o,"class","docstring"),c(_r,"class","docstring"),c(W2,"id","transformers.TFAutoModelForMaskedLM"),c(W2,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(W2,"href","#transformers.TFAutoModelForMaskedLM"),c(gc,"class","relative group"),c(ct,"class","docstring"),c(KD,"href","/docs/transformers/pr_15774/en/model_doc/albert#transformers.TFAlbertForMaskedLM"),c(ZD,"href","/docs/transformers/pr_15774/en/model_doc/bert#transformers.TFBertForMaskedLM"),c(eq,"href","/docs/transformers/pr_15774/en/model_doc/camembert#transformers.TFCamembertForMaskedLM"),c(oq,"href","/docs/transformers/pr_15774/en/model_doc/convbert#transformers.TFConvBertForMaskedLM"),c(rq,"href","/docs/transformers/pr_15774/en/model_doc/deberta#transformers.TFDebertaForMaskedLM"),c(tq,"href","/docs/transformers/pr_15774/en/model_doc/deberta-v2#transformers.TFDebertaV2ForMaskedLM"),c(aq,"href","/docs/transformers/pr_15774/en/model_doc/distilbert#transformers.TFDistilBertForMaskedLM"),c(nq,"href","/docs/transformers/pr_15774/en/model_doc/electra#transformers.TFElectraForMaskedLM"),c(sq,"href","/docs/transformers/pr_15774/en/model_doc/flaubert#transformers.TFFlaubertWithLMHeadModel"),c(lq,"href","/docs/transformers/pr_15774/en/model_doc/funnel#transformers.TFFunnelForMaskedLM"),c(iq,"href","/docs/transformers/pr_15774/en/model_doc/layoutlm#transformers.TFLayoutLMForMaskedLM"),c(dq,"href","/docs/transformers/pr_15774/en/model_doc/longformer#transformers.TFLongformerForMaskedLM"),c(cq,"href","/docs/transformers/pr_15774/en/model_doc/mobilebert#transformers.TFMobileBertForMaskedLM"),c(fq,"href","/docs/transformers/pr_15774/en/model_doc/mpnet#transformers.TFMPNetForMaskedLM"),c(mq,"href","/docs/transformers/pr_15774/en/model_doc/rembert#transformers.TFRemBertForMaskedLM"),c(gq,"href","/docs/transformers/pr_15774/en/model_doc/roberta#transformers.TFRobertaForMaskedLM"),c(hq,"href","/docs/transformers/pr_15774/en/model_doc/roformer#transformers.TFRoFormerForMaskedLM"),c(pq,"href","/docs/transformers/pr_15774/en/model_doc/tapas#transformers.TFTapasForMaskedLM"),c(_q,"href","/docs/transformers/pr_15774/en/model_doc/xlm#transformers.TFXLMWithLMHeadModel"),c(uq,"href","/docs/transformers/pr_15774/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForMaskedLM"),c(uo,"class","docstring"),c(ur,"class","docstring"),c(gv,"id","transformers.TFAutoModelForSeq2SeqLM"),c(gv,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(gv,"href","#transformers.TFAutoModelForSeq2SeqLM"),c(_c,"class","relative group"),c(ft,"class","docstring"),c(bq,"href","/docs/transformers/pr_15774/en/model_doc/bart#transformers.TFBartForConditionalGeneration"),c(vq,"href","/docs/transformers/pr_15774/en/model_doc/blenderbot#transformers.TFBlenderbotForConditionalGeneration"),c(Tq,"href","/docs/transformers/pr_15774/en/model_doc/blenderbot-small#transformers.TFBlenderbotSmallForConditionalGeneration"),c(Fq,"href","/docs/transformers/pr_15774/en/model_doc/encoder-decoder#transformers.TFEncoderDecoderModel"),c(Cq,"href","/docs/transformers/pr_15774/en/model_doc/led#transformers.TFLEDForConditionalGeneration"),c(Mq,"href","/docs/transformers/pr_15774/en/model_doc/marian#transformers.TFMarianMTModel"),c(Eq,"href","/docs/transformers/pr_15774/en/model_doc/mbart#transformers.TFMBartForConditionalGeneration"),c(yq,"href","/docs/transformers/pr_15774/en/model_doc/mt5#transformers.TFMT5ForConditionalGeneration"),c(wq,"href","/docs/transformers/pr_15774/en/model_doc/pegasus#transformers.TFPegasusForConditionalGeneration"),c(Aq,"href","/docs/transformers/pr_15774/en/model_doc/t5#transformers.TFT5ForConditionalGeneration"),c(bo,"class","docstring"),c(br,"class","docstring"),c(Ev,"id","transformers.TFAutoModelForSequenceClassification"),c(Ev,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Ev,"href","#transformers.TFAutoModelForSequenceClassification"),c(vc,"class","relative group"),c(mt,"class","docstring"),c(Lq,"href","/docs/transformers/pr_15774/en/model_doc/albert#transformers.TFAlbertForSequenceClassification"),c(Bq,"href","/docs/transformers/pr_15774/en/model_doc/bert#transformers.TFBertForSequenceClassification"),c(kq,"href","/docs/transformers/pr_15774/en/model_doc/camembert#transformers.TFCamembertForSequenceClassification"),c(xq,"href","/docs/transformers/pr_15774/en/model_doc/convbert#transformers.TFConvBertForSequenceClassification"),c(Rq,"href","/docs/transformers/pr_15774/en/model_doc/ctrl#transformers.TFCTRLForSequenceClassification"),c(Sq,"href","/docs/transformers/pr_15774/en/model_doc/deberta#transformers.TFDebertaForSequenceClassification"),c(Pq,"href","/docs/transformers/pr_15774/en/model_doc/deberta-v2#transformers.TFDebertaV2ForSequenceClassification"),c($q,"href","/docs/transformers/pr_15774/en/model_doc/distilbert#transformers.TFDistilBertForSequenceClassification"),c(Iq,"href","/docs/transformers/pr_15774/en/model_doc/electra#transformers.TFElectraForSequenceClassification"),c(jq,"href","/docs/transformers/pr_15774/en/model_doc/flaubert#transformers.TFFlaubertForSequenceClassification"),c(Nq,"href","/docs/transformers/pr_15774/en/model_doc/funnel#transformers.TFFunnelForSequenceClassification"),c(Dq,"href","/docs/transformers/pr_15774/en/model_doc/gpt2#transformers.TFGPT2ForSequenceClassification"),c(qq,"href","/docs/transformers/pr_15774/en/model_doc/layoutlm#transformers.TFLayoutLMForSequenceClassification"),c(Gq,"href","/docs/transformers/pr_15774/en/model_doc/longformer#transformers.TFLongformerForSequenceClassification"),c(Oq,"href","/docs/transformers/pr_15774/en/model_doc/mobilebert#transformers.TFMobileBertForSequenceClassification"),c(Xq,"href","/docs/transformers/pr_15774/en/model_doc/mpnet#transformers.TFMPNetForSequenceClassification"),c(zq,"href","/docs/transformers/pr_15774/en/model_doc/openai-gpt#transformers.TFOpenAIGPTForSequenceClassification"),c(Vq,"href","/docs/transformers/pr_15774/en/model_doc/rembert#transformers.TFRemBertForSequenceClassification"),c(Wq,"href","/docs/transformers/pr_15774/en/model_doc/roberta#transformers.TFRobertaForSequenceClassification"),c(Qq,"href","/docs/transformers/pr_15774/en/model_doc/roformer#transformers.TFRoFormerForSequenceClassification"),c(Hq,"href","/docs/transformers/pr_15774/en/model_doc/tapas#transformers.TFTapasForSequenceClassification"),c(Uq,"href","/docs/transformers/pr_15774/en/model_doc/transfo-xl#transformers.TFTransfoXLForSequenceClassification"),c(Jq,"href","/docs/transformers/pr_15774/en/model_doc/xlm#transformers.TFXLMForSequenceClassification"),c(Yq,"href","/docs/transformers/pr_15774/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForSequenceClassification"),c(Kq,"href","/docs/transformers/pr_15774/en/model_doc/xlnet#transformers.TFXLNetForSequenceClassification"),c(vo,"class","docstring"),c(vr,"class","docstring"),c(Jv,"id","transformers.TFAutoModelForMultipleChoice"),c(Jv,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Jv,"href","#transformers.TFAutoModelForMultipleChoice"),c(Cc,"class","relative group"),c(gt,"class","docstring"),c(Zq,"href","/docs/transformers/pr_15774/en/model_doc/albert#transformers.TFAlbertForMultipleChoice"),c(eG,"href","/docs/transformers/pr_15774/en/model_doc/bert#transformers.TFBertForMultipleChoice"),c(oG,"href","/docs/transformers/pr_15774/en/model_doc/camembert#transformers.TFCamembertForMultipleChoice"),c(rG,"href","/docs/transformers/pr_15774/en/model_doc/convbert#transformers.TFConvBertForMultipleChoice"),c(tG,"href","/docs/transformers/pr_15774/en/model_doc/distilbert#transformers.TFDistilBertForMultipleChoice"),c(aG,"href","/docs/transformers/pr_15774/en/model_doc/electra#transformers.TFElectraForMultipleChoice"),c(nG,"href","/docs/transformers/pr_15774/en/model_doc/flaubert#transformers.TFFlaubertForMultipleChoice"),c(sG,"href","/docs/transformers/pr_15774/en/model_doc/funnel#transformers.TFFunnelForMultipleChoice"),c(lG,"href","/docs/transformers/pr_15774/en/model_doc/longformer#transformers.TFLongformerForMultipleChoice"),c(iG,"href","/docs/transformers/pr_15774/en/model_doc/mobilebert#transformers.TFMobileBertForMultipleChoice"),c(dG,"href","/docs/transformers/pr_15774/en/model_doc/mpnet#transformers.TFMPNetForMultipleChoice"),c(cG,"href","/docs/transformers/pr_15774/en/model_doc/rembert#transformers.TFRemBertForMultipleChoice"),c(fG,"href","/docs/transformers/pr_15774/en/model_doc/roberta#transformers.TFRobertaForMultipleChoice"),c(mG,"href","/docs/transformers/pr_15774/en/model_doc/roformer#transformers.TFRoFormerForMultipleChoice"),c(gG,"href","/docs/transformers/pr_15774/en/model_doc/xlm#transformers.TFXLMForMultipleChoice"),c(hG,"href","/docs/transformers/pr_15774/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForMultipleChoice"),c(pG,"href","/docs/transformers/pr_15774/en/model_doc/xlnet#transformers.TFXLNetForMultipleChoice"),c(To,"class","docstring"),c(Tr,"class","docstring"),c(hT,"id","transformers.TFAutoModelForTableQuestionAnswering"),c(hT,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(hT,"href","#transformers.TFAutoModelForTableQuestionAnswering"),c(yc,"class","relative group"),c(ht,"class","docstring"),c(_G,"href","/docs/transformers/pr_15774/en/model_doc/tapas#transformers.TFTapasForQuestionAnswering"),c(Fo,"class","docstring"),c(Fr,"class","docstring"),c(_T,"id","transformers.TFAutoModelForTokenClassification"),c(_T,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(_T,"href","#transformers.TFAutoModelForTokenClassification"),c(Lc,"class","relative group"),c(pt,"class","docstring"),c(uG,"href","/docs/transformers/pr_15774/en/model_doc/albert#transformers.TFAlbertForTokenClassification"),c(bG,"href","/docs/transformers/pr_15774/en/model_doc/bert#transformers.TFBertForTokenClassification"),c(vG,"href","/docs/transformers/pr_15774/en/model_doc/camembert#transformers.TFCamembertForTokenClassification"),c(TG,"href","/docs/transformers/pr_15774/en/model_doc/convbert#transformers.TFConvBertForTokenClassification"),c(FG,"href","/docs/transformers/pr_15774/en/model_doc/deberta#transformers.TFDebertaForTokenClassification"),c(CG,"href","/docs/transformers/pr_15774/en/model_doc/deberta-v2#transformers.TFDebertaV2ForTokenClassification"),c(MG,"href","/docs/transformers/pr_15774/en/model_doc/distilbert#transformers.TFDistilBertForTokenClassification"),c(EG,"href","/docs/transformers/pr_15774/en/model_doc/electra#transformers.TFElectraForTokenClassification"),c(yG,"href","/docs/transformers/pr_15774/en/model_doc/flaubert#transformers.TFFlaubertForTokenClassification"),c(wG,"href","/docs/transformers/pr_15774/en/model_doc/funnel#transformers.TFFunnelForTokenClassification"),c(AG,"href","/docs/transformers/pr_15774/en/model_doc/layoutlm#transformers.TFLayoutLMForTokenClassification"),c(LG,"href","/docs/transformers/pr_15774/en/model_doc/longformer#transformers.TFLongformerForTokenClassification"),c(BG,"href","/docs/transformers/pr_15774/en/model_doc/mobilebert#transformers.TFMobileBertForTokenClassification"),c(kG,"href","/docs/transformers/pr_15774/en/model_doc/mpnet#transformers.TFMPNetForTokenClassification"),c(xG,"href","/docs/transformers/pr_15774/en/model_doc/rembert#transformers.TFRemBertForTokenClassification"),c(RG,"href","/docs/transformers/pr_15774/en/model_doc/roberta#transformers.TFRobertaForTokenClassification"),c(SG,"href","/docs/transformers/pr_15774/en/model_doc/roformer#transformers.TFRoFormerForTokenClassification"),c(PG,"href","/docs/transformers/pr_15774/en/model_doc/xlm#transformers.TFXLMForTokenClassification"),c($G,"href","/docs/transformers/pr_15774/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForTokenClassification"),c(IG,"href","/docs/transformers/pr_15774/en/model_doc/xlnet#transformers.TFXLNetForTokenClassification"),c(Co,"class","docstring"),c(Cr,"class","docstring"),c(jT,"id","transformers.TFAutoModelForQuestionAnswering"),c(jT,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(jT,"href","#transformers.TFAutoModelForQuestionAnswering"),c(xc,"class","relative group"),c(_t,"class","docstring"),c(jG,"href","/docs/transformers/pr_15774/en/model_doc/albert#transformers.TFAlbertForQuestionAnswering"),c(NG,"href","/docs/transformers/pr_15774/en/model_doc/bert#transformers.TFBertForQuestionAnswering"),c(DG,"href","/docs/transformers/pr_15774/en/model_doc/camembert#transformers.TFCamembertForQuestionAnswering"),c(qG,"href","/docs/transformers/pr_15774/en/model_doc/convbert#transformers.TFConvBertForQuestionAnswering"),c(GG,"href","/docs/transformers/pr_15774/en/model_doc/deberta#transformers.TFDebertaForQuestionAnswering"),c(OG,"href","/docs/transformers/pr_15774/en/model_doc/deberta-v2#transformers.TFDebertaV2ForQuestionAnswering"),c(XG,"href","/docs/transformers/pr_15774/en/model_doc/distilbert#transformers.TFDistilBertForQuestionAnswering"),c(zG,"href","/docs/transformers/pr_15774/en/model_doc/electra#transformers.TFElectraForQuestionAnswering"),c(VG,"href","/docs/transformers/pr_15774/en/model_doc/flaubert#transformers.TFFlaubertForQuestionAnsweringSimple"),c(WG,"href","/docs/transformers/pr_15774/en/model_doc/funnel#transformers.TFFunnelForQuestionAnswering"),c(QG,"href","/docs/transformers/pr_15774/en/model_doc/longformer#transformers.TFLongformerForQuestionAnswering"),c(HG,"href","/docs/transformers/pr_15774/en/model_doc/mobilebert#transformers.TFMobileBertForQuestionAnswering"),c(UG,"href","/docs/transformers/pr_15774/en/model_doc/mpnet#transformers.TFMPNetForQuestionAnswering"),c(JG,"href","/docs/transformers/pr_15774/en/model_doc/rembert#transformers.TFRemBertForQuestionAnswering"),c(YG,"href","/docs/transformers/pr_15774/en/model_doc/roberta#transformers.TFRobertaForQuestionAnswering"),c(KG,"href","/docs/transformers/pr_15774/en/model_doc/roformer#transformers.TFRoFormerForQuestionAnswering"),c(ZG,"href","/docs/transformers/pr_15774/en/model_doc/xlm#transformers.TFXLMForQuestionAnsweringSimple"),c(eO,"href","/docs/transformers/pr_15774/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForQuestionAnswering"),c(oO,"href","/docs/transformers/pr_15774/en/model_doc/xlnet#transformers.TFXLNetForQuestionAnsweringSimple"),c(Mo,"class","docstring"),c(Mr,"class","docstring"),c(tF,"id","transformers.TFAutoModelForVision2Seq"),c(tF,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(tF,"href","#transformers.TFAutoModelForVision2Seq"),c(Pc,"class","relative group"),c(ut,"class","docstring"),c(rO,"href","/docs/transformers/pr_15774/en/model_doc/vision-encoder-decoder#transformers.TFVisionEncoderDecoderModel"),c(Eo,"class","docstring"),c(Er,"class","docstring"),c(nF,"id","transformers.TFAutoModelForSpeechSeq2Seq"),c(nF,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(nF,"href","#transformers.TFAutoModelForSpeechSeq2Seq"),c(jc,"class","relative group"),c(bt,"class","docstring"),c(tO,"href","/docs/transformers/pr_15774/en/model_doc/speech_to_text#transformers.TFSpeech2TextForConditionalGeneration"),c(yo,"class","docstring"),c(yr,"class","docstring"),c(lF,"id","transformers.FlaxAutoModel"),c(lF,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(lF,"href","#transformers.FlaxAutoModel"),c(qc,"class","relative group"),c(vt,"class","docstring"),c(aO,"href","/docs/transformers/pr_15774/en/model_doc/albert#transformers.FlaxAlbertModel"),c(nO,"href","/docs/transformers/pr_15774/en/model_doc/bart#transformers.FlaxBartModel"),c(sO,"href","/docs/transformers/pr_15774/en/model_doc/beit#transformers.FlaxBeitModel"),c(lO,"href","/docs/transformers/pr_15774/en/model_doc/bert#transformers.FlaxBertModel"),c(iO,"href","/docs/transformers/pr_15774/en/model_doc/big_bird#transformers.FlaxBigBirdModel"),c(dO,"href","/docs/transformers/pr_15774/en/model_doc/blenderbot#transformers.FlaxBlenderbotModel"),c(cO,"href","/docs/transformers/pr_15774/en/model_doc/blenderbot-small#transformers.FlaxBlenderbotSmallModel"),c(fO,"href","/docs/transformers/pr_15774/en/model_doc/clip#transformers.FlaxCLIPModel"),c(mO,"href","/docs/transformers/pr_15774/en/model_doc/distilbert#transformers.FlaxDistilBertModel"),c(gO,"href","/docs/transformers/pr_15774/en/model_doc/electra#transformers.FlaxElectraModel"),c(hO,"href","/docs/transformers/pr_15774/en/model_doc/gpt2#transformers.FlaxGPT2Model"),c(pO,"href","/docs/transformers/pr_15774/en/model_doc/gpt_neo#transformers.FlaxGPTNeoModel"),c(_O,"href","/docs/transformers/pr_15774/en/model_doc/gptj#transformers.FlaxGPTJModel"),c(uO,"href","/docs/transformers/pr_15774/en/model_doc/marian#transformers.FlaxMarianModel"),c(bO,"href","/docs/transformers/pr_15774/en/model_doc/mbart#transformers.FlaxMBartModel"),c(vO,"href","/docs/transformers/pr_15774/en/model_doc/mt5#transformers.FlaxMT5Model"),c(TO,"href","/docs/transformers/pr_15774/en/model_doc/pegasus#transformers.FlaxPegasusModel"),c(FO,"href","/docs/transformers/pr_15774/en/model_doc/roberta#transformers.FlaxRobertaModel"),c(CO,"href","/docs/transformers/pr_15774/en/model_doc/roformer#transformers.FlaxRoFormerModel"),c(MO,"href","/docs/transformers/pr_15774/en/model_doc/t5#transformers.FlaxT5Model"),c(EO,"href","/docs/transformers/pr_15774/en/model_doc/vision-text-dual-encoder#transformers.FlaxVisionTextDualEncoderModel"),c(yO,"href","/docs/transformers/pr_15774/en/model_doc/vit#transformers.FlaxViTModel"),c(wO,"href","/docs/transformers/pr_15774/en/model_doc/wav2vec2#transformers.FlaxWav2Vec2Model"),c(AO,"href","/docs/transformers/pr_15774/en/model_doc/xglm#transformers.FlaxXGLMModel"),c(wo,"class","docstring"),c(wr,"class","docstring"),c(RF,"id","transformers.FlaxAutoModelForCausalLM"),c(RF,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(RF,"href","#transformers.FlaxAutoModelForCausalLM"),c(Xc,"class","relative group"),c(Tt,"class","docstring"),c(LO,"href","/docs/transformers/pr_15774/en/model_doc/gpt2#transformers.FlaxGPT2LMHeadModel"),c(BO,"href","/docs/transformers/pr_15774/en/model_doc/gpt_neo#transformers.FlaxGPTNeoForCausalLM"),c(kO,"href","/docs/transformers/pr_15774/en/model_doc/gptj#transformers.FlaxGPTJForCausalLM"),c(xO,"href","/docs/transformers/pr_15774/en/model_doc/xglm#transformers.FlaxXGLMForCausalLM"),c(Ao,"class","docstring"),c(Ar,"class","docstring"),c(jF,"id","transformers.FlaxAutoModelForPreTraining"),c(jF,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(jF,"href","#transformers.FlaxAutoModelForPreTraining"),c(Wc,"class","relative group"),c(Ft,"class","docstring"),c(RO,"href","/docs/transformers/pr_15774/en/model_doc/albert#transformers.FlaxAlbertForPreTraining"),c(SO,"href","/docs/transformers/pr_15774/en/model_doc/bart#transformers.FlaxBartForConditionalGeneration"),c(PO,"href","/docs/transformers/pr_15774/en/model_doc/bert#transformers.FlaxBertForPreTraining"),c($O,"href","/docs/transformers/pr_15774/en/model_doc/big_bird#transformers.FlaxBigBirdForPreTraining"),c(IO,"href","/docs/transformers/pr_15774/en/model_doc/electra#transformers.FlaxElectraForPreTraining"),c(jO,"href","/docs/transformers/pr_15774/en/model_doc/mbart#transformers.FlaxMBartForConditionalGeneration"),c(NO,"href","/docs/transformers/pr_15774/en/model_doc/mt5#transformers.FlaxMT5ForConditionalGeneration"),c(DO,"href","/docs/transformers/pr_15774/en/model_doc/roberta#transformers.FlaxRobertaForMaskedLM"),c(qO,"href","/docs/transformers/pr_15774/en/model_doc/roformer#transformers.FlaxRoFormerForMaskedLM"),c(GO,"href","/docs/transformers/pr_15774/en/model_doc/t5#transformers.FlaxT5ForConditionalGeneration"),c(OO,"href","/docs/transformers/pr_15774/en/model_doc/wav2vec2#transformers.FlaxWav2Vec2ForPreTraining"),c(Lo,"class","docstring"),c(Lr,"class","docstring"),c(UF,"id","transformers.FlaxAutoModelForMaskedLM"),c(UF,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(UF,"href","#transformers.FlaxAutoModelForMaskedLM"),c(Uc,"class","relative group"),c(Ct,"class","docstring"),c(XO,"href","/docs/transformers/pr_15774/en/model_doc/albert#transformers.FlaxAlbertForMaskedLM"),c(zO,"href","/docs/transformers/pr_15774/en/model_doc/bart#transformers.FlaxBartForConditionalGeneration"),c(VO,"href","/docs/transformers/pr_15774/en/model_doc/bert#transformers.FlaxBertForMaskedLM"),c(WO,"href","/docs/transformers/pr_15774/en/model_doc/big_bird#transformers.FlaxBigBirdForMaskedLM"),c(QO,"href","/docs/transformers/pr_15774/en/model_doc/distilbert#transformers.FlaxDistilBertForMaskedLM"),c(HO,"href","/docs/transformers/pr_15774/en/model_doc/electra#transformers.FlaxElectraForMaskedLM"),c(UO,"href","/docs/transformers/pr_15774/en/model_doc/mbart#transformers.FlaxMBartForConditionalGeneration"),c(JO,"href","/docs/transformers/pr_15774/en/model_doc/roberta#transformers.FlaxRobertaForMaskedLM"),c(YO,"href","/docs/transformers/pr_15774/en/model_doc/roformer#transformers.FlaxRoFormerForMaskedLM"),c(Bo,"class","docstring"),c(Br,"class","docstring"),c(nC,"id","transformers.FlaxAutoModelForSeq2SeqLM"),c(nC,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(nC,"href","#transformers.FlaxAutoModelForSeq2SeqLM"),c(Kc,"class","relative group"),c(Mt,"class","docstring"),c(KO,"href","/docs/transformers/pr_15774/en/model_doc/bart#transformers.FlaxBartForConditionalGeneration"),c(ZO,"href","/docs/transformers/pr_15774/en/model_doc/blenderbot#transformers.FlaxBlenderbotForConditionalGeneration"),c(eX,"href","/docs/transformers/pr_15774/en/model_doc/blenderbot-small#transformers.FlaxBlenderbotSmallForConditionalGeneration"),c(oX,"href","/docs/transformers/pr_15774/en/model_doc/encoder-decoder#transformers.FlaxEncoderDecoderModel"),c(rX,"href","/docs/transformers/pr_15774/en/model_doc/marian#transformers.FlaxMarianMTModel"),c(tX,"href","/docs/transformers/pr_15774/en/model_doc/mbart#transformers.FlaxMBartForConditionalGeneration"),c(aX,"href","/docs/transformers/pr_15774/en/model_doc/mt5#transformers.FlaxMT5ForConditionalGeneration"),c(nX,"href","/docs/transformers/pr_15774/en/model_doc/pegasus#transformers.FlaxPegasusForConditionalGeneration"),c(sX,"href","/docs/transformers/pr_15774/en/model_doc/t5#transformers.FlaxT5ForConditionalGeneration"),c(ko,"class","docstring"),c(kr,"class","docstring"),c(pC,"id","transformers.FlaxAutoModelForSequenceClassification"),c(pC,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(pC,"href","#transformers.FlaxAutoModelForSequenceClassification"),c(of,"class","relative group"),c(Et,"class","docstring"),c(lX,"href","/docs/transformers/pr_15774/en/model_doc/albert#transformers.FlaxAlbertForSequenceClassification"),c(iX,"href","/docs/transformers/pr_15774/en/model_doc/bart#transformers.FlaxBartForSequenceClassification"),c(dX,"href","/docs/transformers/pr_15774/en/model_doc/bert#transformers.FlaxBertForSequenceClassification"),c(cX,"href","/docs/transformers/pr_15774/en/model_doc/big_bird#transformers.FlaxBigBirdForSequenceClassification"),c(fX,"href","/docs/transformers/pr_15774/en/model_doc/distilbert#transformers.FlaxDistilBertForSequenceClassification"),c(mX,"href","/docs/transformers/pr_15774/en/model_doc/electra#transformers.FlaxElectraForSequenceClassification"),c(gX,"href","/docs/transformers/pr_15774/en/model_doc/mbart#transformers.FlaxMBartForSequenceClassification"),c(hX,"href","/docs/transformers/pr_15774/en/model_doc/roberta#transformers.FlaxRobertaForSequenceClassification"),c(pX,"href","/docs/transformers/pr_15774/en/model_doc/roformer#transformers.FlaxRoFormerForSequenceClassification"),c(xo,"class","docstring"),c(xr,"class","docstring"),c(yC,"id","transformers.FlaxAutoModelForQuestionAnswering"),c(yC,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(yC,"href","#transformers.FlaxAutoModelForQuestionAnswering"),c(af,"class","relative group"),c(yt,"class","docstring"),c(_X,"href","/docs/transformers/pr_15774/en/model_doc/albert#transformers.FlaxAlbertForQuestionAnswering"),c(uX,"href","/docs/transformers/pr_15774/en/model_doc/bart#transformers.FlaxBartForQuestionAnswering"),c(bX,"href","/docs/transformers/pr_15774/en/model_doc/bert#transformers.FlaxBertForQuestionAnswering"),c(vX,"href","/docs/transformers/pr_15774/en/model_doc/big_bird#transformers.FlaxBigBirdForQuestionAnswering"),c(TX,"href","/docs/transformers/pr_15774/en/model_doc/distilbert#transformers.FlaxDistilBertForQuestionAnswering"),c(FX,"href","/docs/transformers/pr_15774/en/model_doc/electra#transformers.FlaxElectraForQuestionAnswering"),c(CX,"href","/docs/transformers/pr_15774/en/model_doc/mbart#transformers.FlaxMBartForQuestionAnswering"),c(MX,"href","/docs/transformers/pr_15774/en/model_doc/roberta#transformers.FlaxRobertaForQuestionAnswering"),c(EX,"href","/docs/transformers/pr_15774/en/model_doc/roformer#transformers.FlaxRoFormerForQuestionAnswering"),c(Ro,"class","docstring"),c(Rr,"class","docstring"),c($C,"id","transformers.FlaxAutoModelForTokenClassification"),c($C,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c($C,"href","#transformers.FlaxAutoModelForTokenClassification"),c(lf,"class","relative group"),c(wt,"class","docstring"),c(yX,"href","/docs/transformers/pr_15774/en/model_doc/albert#transformers.FlaxAlbertForTokenClassification"),c(wX,"href","/docs/transformers/pr_15774/en/model_doc/bert#transformers.FlaxBertForTokenClassification"),c(AX,"href","/docs/transformers/pr_15774/en/model_doc/big_bird#transformers.FlaxBigBirdForTokenClassification"),c(LX,"href","/docs/transformers/pr_15774/en/model_doc/distilbert#transformers.FlaxDistilBertForTokenClassification"),c(BX,"href","/docs/transformers/pr_15774/en/model_doc/electra#transformers.FlaxElectraForTokenClassification"),c(kX,"href","/docs/transformers/pr_15774/en/model_doc/roberta#transformers.FlaxRobertaForTokenClassification"),c(xX,"href","/docs/transformers/pr_15774/en/model_doc/roformer#transformers.FlaxRoFormerForTokenClassification"),c(So,"class","docstring"),c(Sr,"class","docstring"),c(XC,"id","transformers.FlaxAutoModelForMultipleChoice"),c(XC,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(XC,"href","#transformers.FlaxAutoModelForMultipleChoice"),c(ff,"class","relative group"),c(At,"class","docstring"),c(RX,"href","/docs/transformers/pr_15774/en/model_doc/albert#transformers.FlaxAlbertForMultipleChoice"),c(SX,"href","/docs/transformers/pr_15774/en/model_doc/bert#transformers.FlaxBertForMultipleChoice"),c(PX,"href","/docs/transformers/pr_15774/en/model_doc/big_bird#transformers.FlaxBigBirdForMultipleChoice"),c($X,"href","/docs/transformers/pr_15774/en/model_doc/distilbert#transformers.FlaxDistilBertForMultipleChoice"),c(IX,"href","/docs/transformers/pr_15774/en/model_doc/electra#transformers.FlaxElectraForMultipleChoice"),c(jX,"href","/docs/transformers/pr_15774/en/model_doc/roberta#transformers.FlaxRobertaForMultipleChoice"),c(NX,"href","/docs/transformers/pr_15774/en/model_doc/roformer#transformers.FlaxRoFormerForMultipleChoice"),c(Po,"class","docstring"),c(Pr,"class","docstring"),c(YC,"id","transformers.FlaxAutoModelForNextSentencePrediction"),c(YC,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(YC,"href","#transformers.FlaxAutoModelForNextSentencePrediction"),c(hf,"class","relative group"),c(Lt,"class","docstring"),c(DX,"href","/docs/transformers/pr_15774/en/model_doc/bert#transformers.FlaxBertForNextSentencePrediction"),c($o,"class","docstring"),c($r,"class","docstring"),c(ZC,"id","transformers.FlaxAutoModelForImageClassification"),c(ZC,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(ZC,"href","#transformers.FlaxAutoModelForImageClassification"),c(uf,"class","relative group"),c(Bt,"class","docstring"),c(qX,"href","/docs/transformers/pr_15774/en/model_doc/beit#transformers.FlaxBeitForImageClassification"),c(GX,"href","/docs/transformers/pr_15774/en/model_doc/vit#transformers.FlaxViTForImageClassification"),c(Io,"class","docstring"),c(Ir,"class","docstring"),c(rM,"id","transformers.FlaxAutoModelForVision2Seq"),c(rM,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(rM,"href","#transformers.FlaxAutoModelForVision2Seq"),c(Tf,"class","relative group"),c(kt,"class","docstring"),c(OX,"href","/docs/transformers/pr_15774/en/model_doc/vision-encoder-decoder#transformers.FlaxVisionEncoderDecoderModel"),c(jo,"class","docstring"),c(jr,"class","docstring")},m(d,u){e(document.head,J),b(d,Ae,u),b(d,ie,u),e(ie,me),e(me,to),g(ce,to,null),e(ie,ue),e(ie,Do),e(Do,wi),b(d,Ef,u),b(d,sa,u),e(sa,Ai),e(sa,Li),e(Li,oE),e(sa,yf),b(d,ye,u),b(d,io,u),e(io,Bi),e(io,Pn),e(Pn,rE),e(io,$n),e(io,In),e(In,tE),e(io,ki),e(io,jn),e(jn,aE),e(io,xi),b(d,wf,u),g($a,d,u),b(d,co,u),b(d,ge,u),e(ge,DL),e(ge,Ri),e(Ri,qL),e(ge,GL),b(d,qo,u),b(d,Ia,u),e(Ia,OL),e(Ia,Af),e(Af,XL),e(Ia,mxe),b(d,t8e,u),b(d,Si,u),e(Si,Lf),e(Lf,$V),g(nE,$V,null),e(Si,gxe),e(Si,IV),e(IV,hxe),b(d,a8e,u),b(d,Nn,u),e(Nn,pxe),e(Nn,jV),e(jV,_xe),e(Nn,uxe),e(Nn,NV),e(NV,bxe),e(Nn,vxe),b(d,n8e,u),g(sE,d,u),b(d,s8e,u),b(d,zL,u),e(zL,Txe),b(d,l8e,u),g(Bf,d,u),b(d,i8e,u),b(d,Pi,u),e(Pi,kf),e(kf,DV),g(lE,DV,null),e(Pi,Fxe),e(Pi,qV),e(qV,Cxe),b(d,d8e,u),b(d,Go,u),g(iE,Go,null),e(Go,Mxe),e(Go,dE),e(dE,Exe),e(dE,VL),e(VL,yxe),e(dE,wxe),e(Go,Axe),e(Go,cE),e(cE,Lxe),e(cE,GV),e(GV,Bxe),e(cE,kxe),e(Go,xxe),e(Go,fo),g(fE,fo,null),e(fo,Rxe),e(fo,OV),e(OV,Sxe),e(fo,Pxe),e(fo,$i),e($i,$xe),e($i,XV),e(XV,Ixe),e($i,jxe),e($i,zV),e(zV,Nxe),e($i,Dxe),e(fo,qxe),e(fo,v),e(v,xf),e(xf,VV),e(VV,Gxe),e(xf,Oxe),e(xf,WL),e(WL,Xxe),e(xf,zxe),e(v,Vxe),e(v,Rf),e(Rf,WV),e(WV,Wxe),e(Rf,Qxe),e(Rf,QL),e(QL,Hxe),e(Rf,Uxe),e(v,Jxe),e(v,Sf),e(Sf,QV),e(QV,Yxe),e(Sf,Kxe),e(Sf,HL),e(HL,Zxe),e(Sf,eRe),e(v,oRe),e(v,Pf),e(Pf,HV),e(HV,rRe),e(Pf,tRe),e(Pf,UL),e(UL,aRe),e(Pf,nRe),e(v,sRe),e(v,$f),e($f,UV),e(UV,lRe),e($f,iRe),e($f,JL),e(JL,dRe),e($f,cRe),e(v,fRe),e(v,If),e(If,JV),e(JV,mRe),e(If,gRe),e(If,YL),e(YL,hRe),e(If,pRe),e(v,_Re),e(v,jf),e(jf,YV),e(YV,uRe),e(jf,bRe),e(jf,KL),e(KL,vRe),e(jf,TRe),e(v,FRe),e(v,Nf),e(Nf,KV),e(KV,CRe),e(Nf,MRe),e(Nf,ZL),e(ZL,ERe),e(Nf,yRe),e(v,wRe),e(v,Df),e(Df,ZV),e(ZV,ARe),e(Df,LRe),e(Df,e8),e(e8,BRe),e(Df,kRe),e(v,xRe),e(v,qf),e(qf,eW),e(eW,RRe),e(qf,SRe),e(qf,o8),e(o8,PRe),e(qf,$Re),e(v,IRe),e(v,Gf),e(Gf,oW),e(oW,jRe),e(Gf,NRe),e(Gf,r8),e(r8,DRe),e(Gf,qRe),e(v,GRe),e(v,Of),e(Of,rW),e(rW,ORe),e(Of,XRe),e(Of,t8),e(t8,zRe),e(Of,VRe),e(v,WRe),e(v,Xf),e(Xf,tW),e(tW,QRe),e(Xf,HRe),e(Xf,a8),e(a8,URe),e(Xf,JRe),e(v,YRe),e(v,zf),e(zf,aW),e(aW,KRe),e(zf,ZRe),e(zf,n8),e(n8,eSe),e(zf,oSe),e(v,rSe),e(v,Vf),e(Vf,nW),e(nW,tSe),e(Vf,aSe),e(Vf,s8),e(s8,nSe),e(Vf,sSe),e(v,lSe),e(v,Wf),e(Wf,sW),e(sW,iSe),e(Wf,dSe),e(Wf,l8),e(l8,cSe),e(Wf,fSe),e(v,mSe),e(v,Qf),e(Qf,lW),e(lW,gSe),e(Qf,hSe),e(Qf,i8),e(i8,pSe),e(Qf,_Se),e(v,uSe),e(v,Hf),e(Hf,iW),e(iW,bSe),e(Hf,vSe),e(Hf,d8),e(d8,TSe),e(Hf,FSe),e(v,CSe),e(v,Uf),e(Uf,dW),e(dW,MSe),e(Uf,ESe),e(Uf,c8),e(c8,ySe),e(Uf,wSe),e(v,ASe),e(v,Jf),e(Jf,cW),e(cW,LSe),e(Jf,BSe),e(Jf,f8),e(f8,kSe),e(Jf,xSe),e(v,RSe),e(v,Yf),e(Yf,fW),e(fW,SSe),e(Yf,PSe),e(Yf,m8),e(m8,$Se),e(Yf,ISe),e(v,jSe),e(v,Kf),e(Kf,mW),e(mW,NSe),e(Kf,DSe),e(Kf,g8),e(g8,qSe),e(Kf,GSe),e(v,OSe),e(v,Zf),e(Zf,gW),e(gW,XSe),e(Zf,zSe),e(Zf,h8),e(h8,VSe),e(Zf,WSe),e(v,QSe),e(v,em),e(em,hW),e(hW,HSe),e(em,USe),e(em,p8),e(p8,JSe),e(em,YSe),e(v,KSe),e(v,om),e(om,pW),e(pW,ZSe),e(om,ePe),e(om,_8),e(_8,oPe),e(om,rPe),e(v,tPe),e(v,rm),e(rm,_W),e(_W,aPe),e(rm,nPe),e(rm,u8),e(u8,sPe),e(rm,lPe),e(v,iPe),e(v,tm),e(tm,uW),e(uW,dPe),e(tm,cPe),e(tm,b8),e(b8,fPe),e(tm,mPe),e(v,gPe),e(v,am),e(am,bW),e(bW,hPe),e(am,pPe),e(am,v8),e(v8,_Pe),e(am,uPe),e(v,bPe),e(v,nm),e(nm,vW),e(vW,vPe),e(nm,TPe),e(nm,T8),e(T8,FPe),e(nm,CPe),e(v,MPe),e(v,sm),e(sm,TW),e(TW,EPe),e(sm,yPe),e(sm,F8),e(F8,wPe),e(sm,APe),e(v,LPe),e(v,lm),e(lm,FW),e(FW,BPe),e(lm,kPe),e(lm,C8),e(C8,xPe),e(lm,RPe),e(v,SPe),e(v,im),e(im,CW),e(CW,PPe),e(im,$Pe),e(im,M8),e(M8,IPe),e(im,jPe),e(v,NPe),e(v,dm),e(dm,MW),e(MW,DPe),e(dm,qPe),e(dm,E8),e(E8,GPe),e(dm,OPe),e(v,XPe),e(v,cm),e(cm,EW),e(EW,zPe),e(cm,VPe),e(cm,y8),e(y8,WPe),e(cm,QPe),e(v,HPe),e(v,fm),e(fm,yW),e(yW,UPe),e(fm,JPe),e(fm,w8),e(w8,YPe),e(fm,KPe),e(v,ZPe),e(v,mm),e(mm,wW),e(wW,e$e),e(mm,o$e),e(mm,A8),e(A8,r$e),e(mm,t$e),e(v,a$e),e(v,gm),e(gm,AW),e(AW,n$e),e(gm,s$e),e(gm,L8),e(L8,l$e),e(gm,i$e),e(v,d$e),e(v,hm),e(hm,LW),e(LW,c$e),e(hm,f$e),e(hm,B8),e(B8,m$e),e(hm,g$e),e(v,h$e),e(v,pm),e(pm,BW),e(BW,p$e),e(pm,_$e),e(pm,k8),e(k8,u$e),e(pm,b$e),e(v,v$e),e(v,_m),e(_m,kW),e(kW,T$e),e(_m,F$e),e(_m,x8),e(x8,C$e),e(_m,M$e),e(v,E$e),e(v,um),e(um,xW),e(xW,y$e),e(um,w$e),e(um,R8),e(R8,A$e),e(um,L$e),e(v,B$e),e(v,bm),e(bm,RW),e(RW,k$e),e(bm,x$e),e(bm,S8),e(S8,R$e),e(bm,S$e),e(v,P$e),e(v,vm),e(vm,SW),e(SW,$$e),e(vm,I$e),e(vm,P8),e(P8,j$e),e(vm,N$e),e(v,D$e),e(v,Tm),e(Tm,PW),e(PW,q$e),e(Tm,G$e),e(Tm,$8),e($8,O$e),e(Tm,X$e),e(v,z$e),e(v,Fm),e(Fm,$W),e($W,V$e),e(Fm,W$e),e(Fm,I8),e(I8,Q$e),e(Fm,H$e),e(v,U$e),e(v,Cm),e(Cm,IW),e(IW,J$e),e(Cm,Y$e),e(Cm,j8),e(j8,K$e),e(Cm,Z$e),e(v,eIe),e(v,Mm),e(Mm,jW),e(jW,oIe),e(Mm,rIe),e(Mm,N8),e(N8,tIe),e(Mm,aIe),e(v,nIe),e(v,Em),e(Em,NW),e(NW,sIe),e(Em,lIe),e(Em,D8),e(D8,iIe),e(Em,dIe),e(v,cIe),e(v,ym),e(ym,DW),e(DW,fIe),e(ym,mIe),e(ym,q8),e(q8,gIe),e(ym,hIe),e(v,pIe),e(v,wm),e(wm,qW),e(qW,_Ie),e(wm,uIe),e(wm,G8),e(G8,bIe),e(wm,vIe),e(v,TIe),e(v,Am),e(Am,GW),e(GW,FIe),e(Am,CIe),e(Am,O8),e(O8,MIe),e(Am,EIe),e(v,yIe),e(v,Lm),e(Lm,OW),e(OW,wIe),e(Lm,AIe),e(Lm,X8),e(X8,LIe),e(Lm,BIe),e(v,kIe),e(v,Bm),e(Bm,XW),e(XW,xIe),e(Bm,RIe),e(Bm,z8),e(z8,SIe),e(Bm,PIe),e(v,$Ie),e(v,km),e(km,zW),e(zW,IIe),e(km,jIe),e(km,V8),e(V8,NIe),e(km,DIe),e(v,qIe),e(v,xm),e(xm,VW),e(VW,GIe),e(xm,OIe),e(xm,W8),e(W8,XIe),e(xm,zIe),e(v,VIe),e(v,Rm),e(Rm,WW),e(WW,WIe),e(Rm,QIe),e(Rm,Q8),e(Q8,HIe),e(Rm,UIe),e(v,JIe),e(v,Sm),e(Sm,QW),e(QW,YIe),e(Sm,KIe),e(Sm,H8),e(H8,ZIe),e(Sm,eje),e(v,oje),e(v,Pm),e(Pm,HW),e(HW,rje),e(Pm,tje),e(Pm,U8),e(U8,aje),e(Pm,nje),e(v,sje),e(v,$m),e($m,UW),e(UW,lje),e($m,ije),e($m,J8),e(J8,dje),e($m,cje),e(v,fje),e(v,Im),e(Im,JW),e(JW,mje),e(Im,gje),e(Im,Y8),e(Y8,hje),e(Im,pje),e(v,_je),e(v,jm),e(jm,YW),e(YW,uje),e(jm,bje),e(jm,K8),e(K8,vje),e(jm,Tje),e(v,Fje),e(v,Nm),e(Nm,KW),e(KW,Cje),e(Nm,Mje),e(Nm,Z8),e(Z8,Eje),e(Nm,yje),e(v,wje),e(v,Dm),e(Dm,ZW),e(ZW,Aje),e(Dm,Lje),e(Dm,e9),e(e9,Bje),e(Dm,kje),e(v,xje),e(v,qm),e(qm,eQ),e(eQ,Rje),e(qm,Sje),e(qm,o9),e(o9,Pje),e(qm,$je),e(v,Ije),e(v,Gm),e(Gm,oQ),e(oQ,jje),e(Gm,Nje),e(Gm,r9),e(r9,Dje),e(Gm,qje),e(v,Gje),e(v,Om),e(Om,rQ),e(rQ,Oje),e(Om,Xje),e(Om,t9),e(t9,zje),e(Om,Vje),e(v,Wje),e(v,Xm),e(Xm,tQ),e(tQ,Qje),e(Xm,Hje),e(Xm,a9),e(a9,Uje),e(Xm,Jje),e(v,Yje),e(v,zm),e(zm,aQ),e(aQ,Kje),e(zm,Zje),e(zm,n9),e(n9,eNe),e(zm,oNe),e(v,rNe),e(v,Vm),e(Vm,nQ),e(nQ,tNe),e(Vm,aNe),e(Vm,s9),e(s9,nNe),e(Vm,sNe),e(v,lNe),e(v,Wm),e(Wm,sQ),e(sQ,iNe),e(Wm,dNe),e(Wm,l9),e(l9,cNe),e(Wm,fNe),e(v,mNe),e(v,Qm),e(Qm,lQ),e(lQ,gNe),e(Qm,hNe),e(Qm,i9),e(i9,pNe),e(Qm,_Ne),e(v,uNe),e(v,Hm),e(Hm,iQ),e(iQ,bNe),e(Hm,vNe),e(Hm,d9),e(d9,TNe),e(Hm,FNe),e(v,CNe),e(v,Um),e(Um,dQ),e(dQ,MNe),e(Um,ENe),e(Um,c9),e(c9,yNe),e(Um,wNe),e(v,ANe),e(v,Jm),e(Jm,cQ),e(cQ,LNe),e(Jm,BNe),e(Jm,f9),e(f9,kNe),e(Jm,xNe),e(v,RNe),e(v,Ym),e(Ym,fQ),e(fQ,SNe),e(Ym,PNe),e(Ym,m9),e(m9,$Ne),e(Ym,INe),e(v,jNe),e(v,Km),e(Km,mQ),e(mQ,NNe),e(Km,DNe),e(Km,g9),e(g9,qNe),e(Km,GNe),e(v,ONe),e(v,Zm),e(Zm,gQ),e(gQ,XNe),e(Zm,zNe),e(Zm,h9),e(h9,VNe),e(Zm,WNe),e(v,QNe),e(v,eg),e(eg,hQ),e(hQ,HNe),e(eg,UNe),e(eg,p9),e(p9,JNe),e(eg,YNe),e(v,KNe),e(v,og),e(og,pQ),e(pQ,ZNe),e(og,eDe),e(og,_9),e(_9,oDe),e(og,rDe),e(v,tDe),e(v,rg),e(rg,_Q),e(_Q,aDe),e(rg,nDe),e(rg,u9),e(u9,sDe),e(rg,lDe),e(v,iDe),e(v,tg),e(tg,uQ),e(uQ,dDe),e(tg,cDe),e(tg,b9),e(b9,fDe),e(tg,mDe),e(v,gDe),e(v,ag),e(ag,bQ),e(bQ,hDe),e(ag,pDe),e(ag,v9),e(v9,_De),e(ag,uDe),e(v,bDe),e(v,ng),e(ng,vQ),e(vQ,vDe),e(ng,TDe),e(ng,T9),e(T9,FDe),e(ng,CDe),e(v,MDe),e(v,sg),e(sg,TQ),e(TQ,EDe),e(sg,yDe),e(sg,F9),e(F9,wDe),e(sg,ADe),e(v,LDe),e(v,lg),e(lg,FQ),e(FQ,BDe),e(lg,kDe),e(lg,C9),e(C9,xDe),e(lg,RDe),e(v,SDe),e(v,ig),e(ig,CQ),e(CQ,PDe),e(ig,$De),e(ig,M9),e(M9,IDe),e(ig,jDe),e(v,NDe),e(v,dg),e(dg,MQ),e(MQ,DDe),e(dg,qDe),e(dg,E9),e(E9,GDe),e(dg,ODe),e(v,XDe),e(v,cg),e(cg,EQ),e(EQ,zDe),e(cg,VDe),e(cg,y9),e(y9,WDe),e(cg,QDe),e(v,HDe),e(v,fg),e(fg,yQ),e(yQ,UDe),e(fg,JDe),e(fg,w9),e(w9,YDe),e(fg,KDe),e(v,ZDe),e(v,mg),e(mg,wQ),e(wQ,eqe),e(mg,oqe),e(mg,A9),e(A9,rqe),e(mg,tqe),e(v,aqe),e(v,gg),e(gg,AQ),e(AQ,nqe),e(gg,sqe),e(gg,L9),e(L9,lqe),e(gg,iqe),e(fo,dqe),e(fo,LQ),e(LQ,cqe),e(fo,fqe),g(mE,fo,null),e(Go,mqe),e(Go,hg),g(gE,hg,null),e(hg,gqe),e(hg,BQ),e(BQ,hqe),b(d,c8e,u),b(d,Ii,u),e(Ii,pg),e(pg,kQ),g(hE,kQ,null),e(Ii,pqe),e(Ii,xQ),e(xQ,_qe),b(d,f8e,u),b(d,Oo,u),g(pE,Oo,null),e(Oo,uqe),e(Oo,_E),e(_E,bqe),e(_E,B9),e(B9,vqe),e(_E,Tqe),e(Oo,Fqe),e(Oo,uE),e(uE,Cqe),e(uE,RQ),e(RQ,Mqe),e(uE,Eqe),e(Oo,yqe),e(Oo,mo),g(bE,mo,null),e(mo,wqe),e(mo,SQ),e(SQ,Aqe),e(mo,Lqe),e(mo,ja),e(ja,Bqe),e(ja,PQ),e(PQ,kqe),e(ja,xqe),e(ja,$Q),e($Q,Rqe),e(ja,Sqe),e(ja,IQ),e(IQ,Pqe),e(ja,$qe),e(mo,Iqe),e(mo,M),e(M,Dn),e(Dn,jQ),e(jQ,jqe),e(Dn,Nqe),e(Dn,k9),e(k9,Dqe),e(Dn,qqe),e(Dn,x9),e(x9,Gqe),e(Dn,Oqe),e(M,Xqe),e(M,qn),e(qn,NQ),e(NQ,zqe),e(qn,Vqe),e(qn,R9),e(R9,Wqe),e(qn,Qqe),e(qn,S9),e(S9,Hqe),e(qn,Uqe),e(M,Jqe),e(M,Gn),e(Gn,DQ),e(DQ,Yqe),e(Gn,Kqe),e(Gn,P9),e(P9,Zqe),e(Gn,eGe),e(Gn,$9),e($9,oGe),e(Gn,rGe),e(M,tGe),e(M,_g),e(_g,qQ),e(qQ,aGe),e(_g,nGe),e(_g,I9),e(I9,sGe),e(_g,lGe),e(M,iGe),e(M,On),e(On,GQ),e(GQ,dGe),e(On,cGe),e(On,j9),e(j9,fGe),e(On,mGe),e(On,N9),e(N9,gGe),e(On,hGe),e(M,pGe),e(M,ug),e(ug,OQ),e(OQ,_Ge),e(ug,uGe),e(ug,D9),e(D9,bGe),e(ug,vGe),e(M,TGe),e(M,bg),e(bg,XQ),e(XQ,FGe),e(bg,CGe),e(bg,q9),e(q9,MGe),e(bg,EGe),e(M,yGe),e(M,vg),e(vg,zQ),e(zQ,wGe),e(vg,AGe),e(vg,G9),e(G9,LGe),e(vg,BGe),e(M,kGe),e(M,Xn),e(Xn,VQ),e(VQ,xGe),e(Xn,RGe),e(Xn,O9),e(O9,SGe),e(Xn,PGe),e(Xn,X9),e(X9,$Ge),e(Xn,IGe),e(M,jGe),e(M,zn),e(zn,WQ),e(WQ,NGe),e(zn,DGe),e(zn,z9),e(z9,qGe),e(zn,GGe),e(zn,V9),e(V9,OGe),e(zn,XGe),e(M,zGe),e(M,Vn),e(Vn,QQ),e(QQ,VGe),e(Vn,WGe),e(Vn,W9),e(W9,QGe),e(Vn,HGe),e(Vn,Q9),e(Q9,UGe),e(Vn,JGe),e(M,YGe),e(M,Tg),e(Tg,HQ),e(HQ,KGe),e(Tg,ZGe),e(Tg,H9),e(H9,eOe),e(Tg,oOe),e(M,rOe),e(M,Fg),e(Fg,UQ),e(UQ,tOe),e(Fg,aOe),e(Fg,U9),e(U9,nOe),e(Fg,sOe),e(M,lOe),e(M,Wn),e(Wn,JQ),e(JQ,iOe),e(Wn,dOe),e(Wn,J9),e(J9,cOe),e(Wn,fOe),e(Wn,Y9),e(Y9,mOe),e(Wn,gOe),e(M,hOe),e(M,Cg),e(Cg,YQ),e(YQ,pOe),e(Cg,_Oe),e(Cg,K9),e(K9,uOe),e(Cg,bOe),e(M,vOe),e(M,Qn),e(Qn,KQ),e(KQ,TOe),e(Qn,FOe),e(Qn,Z9),e(Z9,COe),e(Qn,MOe),e(Qn,eB),e(eB,EOe),e(Qn,yOe),e(M,wOe),e(M,Hn),e(Hn,ZQ),e(ZQ,AOe),e(Hn,LOe),e(Hn,oB),e(oB,BOe),e(Hn,kOe),e(Hn,rB),e(rB,xOe),e(Hn,ROe),e(M,SOe),e(M,Un),e(Un,eH),e(eH,POe),e(Un,$Oe),e(Un,tB),e(tB,IOe),e(Un,jOe),e(Un,oH),e(oH,NOe),e(Un,DOe),e(M,qOe),e(M,Mg),e(Mg,rH),e(rH,GOe),e(Mg,OOe),e(Mg,aB),e(aB,XOe),e(Mg,zOe),e(M,VOe),e(M,Jn),e(Jn,tH),e(tH,WOe),e(Jn,QOe),e(Jn,nB),e(nB,HOe),e(Jn,UOe),e(Jn,sB),e(sB,JOe),e(Jn,YOe),e(M,KOe),e(M,Eg),e(Eg,aH),e(aH,ZOe),e(Eg,eXe),e(Eg,lB),e(lB,oXe),e(Eg,rXe),e(M,tXe),e(M,Yn),e(Yn,nH),e(nH,aXe),e(Yn,nXe),e(Yn,iB),e(iB,sXe),e(Yn,lXe),e(Yn,dB),e(dB,iXe),e(Yn,dXe),e(M,cXe),e(M,Kn),e(Kn,sH),e(sH,fXe),e(Kn,mXe),e(Kn,cB),e(cB,gXe),e(Kn,hXe),e(Kn,fB),e(fB,pXe),e(Kn,_Xe),e(M,uXe),e(M,Zn),e(Zn,lH),e(lH,bXe),e(Zn,vXe),e(Zn,mB),e(mB,TXe),e(Zn,FXe),e(Zn,gB),e(gB,CXe),e(Zn,MXe),e(M,EXe),e(M,yg),e(yg,iH),e(iH,yXe),e(yg,wXe),e(yg,hB),e(hB,AXe),e(yg,LXe),e(M,BXe),e(M,es),e(es,dH),e(dH,kXe),e(es,xXe),e(es,pB),e(pB,RXe),e(es,SXe),e(es,_B),e(_B,PXe),e(es,$Xe),e(M,IXe),e(M,wg),e(wg,cH),e(cH,jXe),e(wg,NXe),e(wg,uB),e(uB,DXe),e(wg,qXe),e(M,GXe),e(M,os),e(os,fH),e(fH,OXe),e(os,XXe),e(os,bB),e(bB,zXe),e(os,VXe),e(os,vB),e(vB,WXe),e(os,QXe),e(M,HXe),e(M,rs),e(rs,mH),e(mH,UXe),e(rs,JXe),e(rs,TB),e(TB,YXe),e(rs,KXe),e(rs,FB),e(FB,ZXe),e(rs,eze),e(M,oze),e(M,ts),e(ts,gH),e(gH,rze),e(ts,tze),e(ts,CB),e(CB,aze),e(ts,nze),e(ts,MB),e(MB,sze),e(ts,lze),e(M,ize),e(M,as),e(as,hH),e(hH,dze),e(as,cze),e(as,EB),e(EB,fze),e(as,mze),e(as,yB),e(yB,gze),e(as,hze),e(M,pze),e(M,Ag),e(Ag,pH),e(pH,_ze),e(Ag,uze),e(Ag,wB),e(wB,bze),e(Ag,vze),e(M,Tze),e(M,ns),e(ns,_H),e(_H,Fze),e(ns,Cze),e(ns,AB),e(AB,Mze),e(ns,Eze),e(ns,LB),e(LB,yze),e(ns,wze),e(M,Aze),e(M,ss),e(ss,uH),e(uH,Lze),e(ss,Bze),e(ss,BB),e(BB,kze),e(ss,xze),e(ss,kB),e(kB,Rze),e(ss,Sze),e(M,Pze),e(M,ls),e(ls,bH),e(bH,$ze),e(ls,Ize),e(ls,xB),e(xB,jze),e(ls,Nze),e(ls,RB),e(RB,Dze),e(ls,qze),e(M,Gze),e(M,is),e(is,vH),e(vH,Oze),e(is,Xze),e(is,SB),e(SB,zze),e(is,Vze),e(is,PB),e(PB,Wze),e(is,Qze),e(M,Hze),e(M,ds),e(ds,TH),e(TH,Uze),e(ds,Jze),e(ds,$B),e($B,Yze),e(ds,Kze),e(ds,IB),e(IB,Zze),e(ds,eVe),e(M,oVe),e(M,cs),e(cs,FH),e(FH,rVe),e(cs,tVe),e(cs,jB),e(jB,aVe),e(cs,nVe),e(cs,NB),e(NB,sVe),e(cs,lVe),e(M,iVe),e(M,Lg),e(Lg,CH),e(CH,dVe),e(Lg,cVe),e(Lg,DB),e(DB,fVe),e(Lg,mVe),e(M,gVe),e(M,fs),e(fs,MH),e(MH,hVe),e(fs,pVe),e(fs,qB),e(qB,_Ve),e(fs,uVe),e(fs,GB),e(GB,bVe),e(fs,vVe),e(M,TVe),e(M,Bg),e(Bg,EH),e(EH,FVe),e(Bg,CVe),e(Bg,OB),e(OB,MVe),e(Bg,EVe),e(M,yVe),e(M,kg),e(kg,yH),e(yH,wVe),e(kg,AVe),e(kg,XB),e(XB,LVe),e(kg,BVe),e(M,kVe),e(M,ms),e(ms,wH),e(wH,xVe),e(ms,RVe),e(ms,zB),e(zB,SVe),e(ms,PVe),e(ms,VB),e(VB,$Ve),e(ms,IVe),e(M,jVe),e(M,gs),e(gs,AH),e(AH,NVe),e(gs,DVe),e(gs,WB),e(WB,qVe),e(gs,GVe),e(gs,QB),e(QB,OVe),e(gs,XVe),e(M,zVe),e(M,xg),e(xg,LH),e(LH,VVe),e(xg,WVe),e(xg,HB),e(HB,QVe),e(xg,HVe),e(M,UVe),e(M,hs),e(hs,BH),e(BH,JVe),e(hs,YVe),e(hs,UB),e(UB,KVe),e(hs,ZVe),e(hs,JB),e(JB,eWe),e(hs,oWe),e(M,rWe),e(M,ps),e(ps,kH),e(kH,tWe),e(ps,aWe),e(ps,YB),e(YB,nWe),e(ps,sWe),e(ps,KB),e(KB,lWe),e(ps,iWe),e(M,dWe),e(M,_s),e(_s,xH),e(xH,cWe),e(_s,fWe),e(_s,ZB),e(ZB,mWe),e(_s,gWe),e(_s,ek),e(ek,hWe),e(_s,pWe),e(M,_We),e(M,us),e(us,RH),e(RH,uWe),e(us,bWe),e(us,ok),e(ok,vWe),e(us,TWe),e(us,rk),e(rk,FWe),e(us,CWe),e(M,MWe),e(M,bs),e(bs,SH),e(SH,EWe),e(bs,yWe),e(bs,tk),e(tk,wWe),e(bs,AWe),e(bs,ak),e(ak,LWe),e(bs,BWe),e(M,kWe),e(M,Rg),e(Rg,PH),e(PH,xWe),e(Rg,RWe),e(Rg,nk),e(nk,SWe),e(Rg,PWe),e(M,$We),e(M,Sg),e(Sg,$H),e($H,IWe),e(Sg,jWe),e(Sg,sk),e(sk,NWe),e(Sg,DWe),e(M,qWe),e(M,Pg),e(Pg,IH),e(IH,GWe),e(Pg,OWe),e(Pg,lk),e(lk,XWe),e(Pg,zWe),e(M,VWe),e(M,$g),e($g,jH),e(jH,WWe),e($g,QWe),e($g,ik),e(ik,HWe),e($g,UWe),e(M,JWe),e(M,vs),e(vs,NH),e(NH,YWe),e(vs,KWe),e(vs,dk),e(dk,ZWe),e(vs,eQe),e(vs,ck),e(ck,oQe),e(vs,rQe),e(M,tQe),e(M,Ig),e(Ig,DH),e(DH,aQe),e(Ig,nQe),e(Ig,fk),e(fk,sQe),e(Ig,lQe),e(M,iQe),e(M,Ts),e(Ts,qH),e(qH,dQe),e(Ts,cQe),e(Ts,mk),e(mk,fQe),e(Ts,mQe),e(Ts,gk),e(gk,gQe),e(Ts,hQe),e(M,pQe),e(M,Fs),e(Fs,GH),e(GH,_Qe),e(Fs,uQe),e(Fs,hk),e(hk,bQe),e(Fs,vQe),e(Fs,pk),e(pk,TQe),e(Fs,FQe),e(M,CQe),e(M,Cs),e(Cs,OH),e(OH,MQe),e(Cs,EQe),e(Cs,_k),e(_k,yQe),e(Cs,wQe),e(Cs,uk),e(uk,AQe),e(Cs,LQe),e(M,BQe),e(M,Ms),e(Ms,XH),e(XH,kQe),e(Ms,xQe),e(Ms,bk),e(bk,RQe),e(Ms,SQe),e(Ms,vk),e(vk,PQe),e(Ms,$Qe),e(M,IQe),e(M,Es),e(Es,zH),e(zH,jQe),e(Es,NQe),e(Es,Tk),e(Tk,DQe),e(Es,qQe),e(Es,Fk),e(Fk,GQe),e(Es,OQe),e(M,XQe),e(M,jg),e(jg,VH),e(VH,zQe),e(jg,VQe),e(jg,Ck),e(Ck,WQe),e(jg,QQe),e(M,HQe),e(M,Ng),e(Ng,WH),e(WH,UQe),e(Ng,JQe),e(Ng,Mk),e(Mk,YQe),e(Ng,KQe),e(M,ZQe),e(M,ys),e(ys,QH),e(QH,eHe),e(ys,oHe),e(ys,Ek),e(Ek,rHe),e(ys,tHe),e(ys,yk),e(yk,aHe),e(ys,nHe),e(M,sHe),e(M,ws),e(ws,HH),e(HH,lHe),e(ws,iHe),e(ws,wk),e(wk,dHe),e(ws,cHe),e(ws,Ak),e(Ak,fHe),e(ws,mHe),e(M,gHe),e(M,As),e(As,UH),e(UH,hHe),e(As,pHe),e(As,Lk),e(Lk,_He),e(As,uHe),e(As,Bk),e(Bk,bHe),e(As,vHe),e(M,THe),e(M,Dg),e(Dg,JH),e(JH,FHe),e(Dg,CHe),e(Dg,kk),e(kk,MHe),e(Dg,EHe),e(M,yHe),e(M,qg),e(qg,YH),e(YH,wHe),e(qg,AHe),e(qg,xk),e(xk,LHe),e(qg,BHe),e(M,kHe),e(M,Gg),e(Gg,KH),e(KH,xHe),e(Gg,RHe),e(Gg,Rk),e(Rk,SHe),e(Gg,PHe),e(M,$He),e(M,Og),e(Og,ZH),e(ZH,IHe),e(Og,jHe),e(Og,Sk),e(Sk,NHe),e(Og,DHe),e(M,qHe),e(M,Ls),e(Ls,eU),e(eU,GHe),e(Ls,OHe),e(Ls,Pk),e(Pk,XHe),e(Ls,zHe),e(Ls,$k),e($k,VHe),e(Ls,WHe),e(M,QHe),e(M,Xg),e(Xg,oU),e(oU,HHe),e(Xg,UHe),e(Xg,Ik),e(Ik,JHe),e(Xg,YHe),e(M,KHe),e(M,zg),e(zg,rU),e(rU,ZHe),e(zg,eUe),e(zg,jk),e(jk,oUe),e(zg,rUe),e(M,tUe),e(M,Bs),e(Bs,tU),e(tU,aUe),e(Bs,nUe),e(Bs,Nk),e(Nk,sUe),e(Bs,lUe),e(Bs,Dk),e(Dk,iUe),e(Bs,dUe),e(M,cUe),e(M,ks),e(ks,aU),e(aU,fUe),e(ks,mUe),e(ks,qk),e(qk,gUe),e(ks,hUe),e(ks,Gk),e(Gk,pUe),e(ks,_Ue),e(mo,uUe),e(mo,nU),e(nU,bUe),e(mo,vUe),g(vE,mo,null),e(Oo,TUe),e(Oo,Vg),g(TE,Vg,null),e(Vg,FUe),e(Vg,sU),e(sU,CUe),b(d,m8e,u),b(d,ji,u),e(ji,Wg),e(Wg,lU),g(FE,lU,null),e(ji,MUe),e(ji,iU),e(iU,EUe),b(d,g8e,u),b(d,Xo,u),g(CE,Xo,null),e(Xo,yUe),e(Xo,ME),e(ME,wUe),e(ME,Ok),e(Ok,AUe),e(ME,LUe),e(Xo,BUe),e(Xo,EE),e(EE,kUe),e(EE,dU),e(dU,xUe),e(EE,RUe),e(Xo,SUe),e(Xo,Le),g(yE,Le,null),e(Le,PUe),e(Le,cU),e(cU,$Ue),e(Le,IUe),e(Le,Na),e(Na,jUe),e(Na,fU),e(fU,NUe),e(Na,DUe),e(Na,mU),e(mU,qUe),e(Na,GUe),e(Na,gU),e(gU,OUe),e(Na,XUe),e(Le,zUe),e(Le,se),e(se,Qg),e(Qg,hU),e(hU,VUe),e(Qg,WUe),e(Qg,Xk),e(Xk,QUe),e(Qg,HUe),e(se,UUe),e(se,Hg),e(Hg,pU),e(pU,JUe),e(Hg,YUe),e(Hg,zk),e(zk,KUe),e(Hg,ZUe),e(se,eJe),e(se,Ug),e(Ug,_U),e(_U,oJe),e(Ug,rJe),e(Ug,Vk),e(Vk,tJe),e(Ug,aJe),e(se,nJe),e(se,Jg),e(Jg,uU),e(uU,sJe),e(Jg,lJe),e(Jg,Wk),e(Wk,iJe),e(Jg,dJe),e(se,cJe),e(se,Yg),e(Yg,bU),e(bU,fJe),e(Yg,mJe),e(Yg,Qk),e(Qk,gJe),e(Yg,hJe),e(se,pJe),e(se,Kg),e(Kg,vU),e(vU,_Je),e(Kg,uJe),e(Kg,Hk),e(Hk,bJe),e(Kg,vJe),e(se,TJe),e(se,Zg),e(Zg,TU),e(TU,FJe),e(Zg,CJe),e(Zg,Uk),e(Uk,MJe),e(Zg,EJe),e(se,yJe),e(se,eh),e(eh,FU),e(FU,wJe),e(eh,AJe),e(eh,Jk),e(Jk,LJe),e(eh,BJe),e(se,kJe),e(se,oh),e(oh,CU),e(CU,xJe),e(oh,RJe),e(oh,Yk),e(Yk,SJe),e(oh,PJe),e(se,$Je),e(se,rh),e(rh,MU),e(MU,IJe),e(rh,jJe),e(rh,Kk),e(Kk,NJe),e(rh,DJe),e(se,qJe),e(se,th),e(th,EU),e(EU,GJe),e(th,OJe),e(th,Zk),e(Zk,XJe),e(th,zJe),e(se,VJe),e(se,ah),e(ah,yU),e(yU,WJe),e(ah,QJe),e(ah,ex),e(ex,HJe),e(ah,UJe),e(se,JJe),e(se,nh),e(nh,wU),e(wU,YJe),e(nh,KJe),e(nh,ox),e(ox,ZJe),e(nh,eYe),e(se,oYe),e(se,sh),e(sh,AU),e(AU,rYe),e(sh,tYe),e(sh,rx),e(rx,aYe),e(sh,nYe),e(se,sYe),e(se,lh),e(lh,LU),e(LU,lYe),e(lh,iYe),e(lh,tx),e(tx,dYe),e(lh,cYe),e(Le,fYe),g(ih,Le,null),e(Le,mYe),e(Le,BU),e(BU,gYe),e(Le,hYe),g(wE,Le,null),e(Xo,pYe),e(Xo,dh),g(AE,dh,null),e(dh,_Ye),e(dh,kU),e(kU,uYe),b(d,h8e,u),b(d,Ni,u),e(Ni,ch),e(ch,xU),g(LE,xU,null),e(Ni,bYe),e(Ni,RU),e(RU,vYe),b(d,p8e,u),b(d,zo,u),g(BE,zo,null),e(zo,TYe),e(zo,kE),e(kE,FYe),e(kE,ax),e(ax,CYe),e(kE,MYe),e(zo,EYe),e(zo,xE),e(xE,yYe),e(xE,SU),e(SU,wYe),e(xE,AYe),e(zo,LYe),e(zo,Be),g(RE,Be,null),e(Be,BYe),e(Be,PU),e(PU,kYe),e(Be,xYe),e(Be,Di),e(Di,RYe),e(Di,$U),e($U,SYe),e(Di,PYe),e(Di,IU),e(IU,$Ye),e(Di,IYe),e(Be,jYe),e(Be,we),e(we,fh),e(fh,jU),e(jU,NYe),e(fh,DYe),e(fh,nx),e(nx,qYe),e(fh,GYe),e(we,OYe),e(we,mh),e(mh,NU),e(NU,XYe),e(mh,zYe),e(mh,sx),e(sx,VYe),e(mh,WYe),e(we,QYe),e(we,gh),e(gh,DU),e(DU,HYe),e(gh,UYe),e(gh,lx),e(lx,JYe),e(gh,YYe),e(we,KYe),e(we,hh),e(hh,qU),e(qU,ZYe),e(hh,eKe),e(hh,ix),e(ix,oKe),e(hh,rKe),e(we,tKe),e(we,ph),e(ph,GU),e(GU,aKe),e(ph,nKe),e(ph,dx),e(dx,sKe),e(ph,lKe),e(we,iKe),e(we,_h),e(_h,OU),e(OU,dKe),e(_h,cKe),e(_h,cx),e(cx,fKe),e(_h,mKe),e(we,gKe),e(we,uh),e(uh,XU),e(XU,hKe),e(uh,pKe),e(uh,fx),e(fx,_Ke),e(uh,uKe),e(we,bKe),e(we,bh),e(bh,zU),e(zU,vKe),e(bh,TKe),e(bh,mx),e(mx,FKe),e(bh,CKe),e(Be,MKe),g(vh,Be,null),e(Be,EKe),e(Be,VU),e(VU,yKe),e(Be,wKe),g(SE,Be,null),e(zo,AKe),e(zo,Th),g(PE,Th,null),e(Th,LKe),e(Th,WU),e(WU,BKe),b(d,_8e,u),b(d,qi,u),e(qi,Fh),e(Fh,QU),g($E,QU,null),e(qi,kKe),e(qi,HU),e(HU,xKe),b(d,u8e,u),b(d,Vo,u),g(IE,Vo,null),e(Vo,RKe),e(Vo,Gi),e(Gi,SKe),e(Gi,UU),e(UU,PKe),e(Gi,$Ke),e(Gi,JU),e(JU,IKe),e(Gi,jKe),e(Vo,NKe),e(Vo,jE),e(jE,DKe),e(jE,YU),e(YU,qKe),e(jE,GKe),e(Vo,OKe),e(Vo,Nr),g(NE,Nr,null),e(Nr,XKe),e(Nr,KU),e(KU,zKe),e(Nr,VKe),e(Nr,Oi),e(Oi,WKe),e(Oi,ZU),e(ZU,QKe),e(Oi,HKe),e(Oi,eJ),e(eJ,UKe),e(Oi,JKe),e(Nr,YKe),e(Nr,oJ),e(oJ,KKe),e(Nr,ZKe),g(DE,Nr,null),e(Vo,eZe),e(Vo,ke),g(qE,ke,null),e(ke,oZe),e(ke,rJ),e(rJ,rZe),e(ke,tZe),e(ke,Da),e(Da,aZe),e(Da,tJ),e(tJ,nZe),e(Da,sZe),e(Da,aJ),e(aJ,lZe),e(Da,iZe),e(Da,nJ),e(nJ,dZe),e(Da,cZe),e(ke,fZe),e(ke,F),e(F,Ch),e(Ch,sJ),e(sJ,mZe),e(Ch,gZe),e(Ch,gx),e(gx,hZe),e(Ch,pZe),e(F,_Ze),e(F,Mh),e(Mh,lJ),e(lJ,uZe),e(Mh,bZe),e(Mh,hx),e(hx,vZe),e(Mh,TZe),e(F,FZe),e(F,Eh),e(Eh,iJ),e(iJ,CZe),e(Eh,MZe),e(Eh,px),e(px,EZe),e(Eh,yZe),e(F,wZe),e(F,yh),e(yh,dJ),e(dJ,AZe),e(yh,LZe),e(yh,_x),e(_x,BZe),e(yh,kZe),e(F,xZe),e(F,wh),e(wh,cJ),e(cJ,RZe),e(wh,SZe),e(wh,ux),e(ux,PZe),e(wh,$Ze),e(F,IZe),e(F,Ah),e(Ah,fJ),e(fJ,jZe),e(Ah,NZe),e(Ah,bx),e(bx,DZe),e(Ah,qZe),e(F,GZe),e(F,Lh),e(Lh,mJ),e(mJ,OZe),e(Lh,XZe),e(Lh,vx),e(vx,zZe),e(Lh,VZe),e(F,WZe),e(F,Bh),e(Bh,gJ),e(gJ,QZe),e(Bh,HZe),e(Bh,Tx),e(Tx,UZe),e(Bh,JZe),e(F,YZe),e(F,kh),e(kh,hJ),e(hJ,KZe),e(kh,ZZe),e(kh,Fx),e(Fx,eeo),e(kh,oeo),e(F,reo),e(F,xh),e(xh,pJ),e(pJ,teo),e(xh,aeo),e(xh,Cx),e(Cx,neo),e(xh,seo),e(F,leo),e(F,Rh),e(Rh,_J),e(_J,ieo),e(Rh,deo),e(Rh,Mx),e(Mx,ceo),e(Rh,feo),e(F,meo),e(F,Sh),e(Sh,uJ),e(uJ,geo),e(Sh,heo),e(Sh,Ex),e(Ex,peo),e(Sh,_eo),e(F,ueo),e(F,Ph),e(Ph,bJ),e(bJ,beo),e(Ph,veo),e(Ph,yx),e(yx,Teo),e(Ph,Feo),e(F,Ceo),e(F,$h),e($h,vJ),e(vJ,Meo),e($h,Eeo),e($h,wx),e(wx,yeo),e($h,weo),e(F,Aeo),e(F,Ih),e(Ih,TJ),e(TJ,Leo),e(Ih,Beo),e(Ih,Ax),e(Ax,keo),e(Ih,xeo),e(F,Reo),e(F,jh),e(jh,FJ),e(FJ,Seo),e(jh,Peo),e(jh,Lx),e(Lx,$eo),e(jh,Ieo),e(F,jeo),e(F,Nh),e(Nh,CJ),e(CJ,Neo),e(Nh,Deo),e(Nh,Bx),e(Bx,qeo),e(Nh,Geo),e(F,Oeo),e(F,Dh),e(Dh,MJ),e(MJ,Xeo),e(Dh,zeo),e(Dh,kx),e(kx,Veo),e(Dh,Weo),e(F,Qeo),e(F,qh),e(qh,EJ),e(EJ,Heo),e(qh,Ueo),e(qh,xx),e(xx,Jeo),e(qh,Yeo),e(F,Keo),e(F,Gh),e(Gh,yJ),e(yJ,Zeo),e(Gh,eoo),e(Gh,Rx),e(Rx,ooo),e(Gh,roo),e(F,too),e(F,Oh),e(Oh,wJ),e(wJ,aoo),e(Oh,noo),e(Oh,Sx),e(Sx,soo),e(Oh,loo),e(F,ioo),e(F,Xh),e(Xh,AJ),e(AJ,doo),e(Xh,coo),e(Xh,Px),e(Px,foo),e(Xh,moo),e(F,goo),e(F,zh),e(zh,LJ),e(LJ,hoo),e(zh,poo),e(zh,$x),e($x,_oo),e(zh,uoo),e(F,boo),e(F,Vh),e(Vh,BJ),e(BJ,voo),e(Vh,Too),e(Vh,Ix),e(Ix,Foo),e(Vh,Coo),e(F,Moo),e(F,Wh),e(Wh,kJ),e(kJ,Eoo),e(Wh,yoo),e(Wh,jx),e(jx,woo),e(Wh,Aoo),e(F,Loo),e(F,xs),e(xs,xJ),e(xJ,Boo),e(xs,koo),e(xs,Nx),e(Nx,xoo),e(xs,Roo),e(xs,Dx),e(Dx,Soo),e(xs,Poo),e(F,$oo),e(F,Qh),e(Qh,RJ),e(RJ,Ioo),e(Qh,joo),e(Qh,qx),e(qx,Noo),e(Qh,Doo),e(F,qoo),e(F,Hh),e(Hh,SJ),e(SJ,Goo),e(Hh,Ooo),e(Hh,Gx),e(Gx,Xoo),e(Hh,zoo),e(F,Voo),e(F,Uh),e(Uh,PJ),e(PJ,Woo),e(Uh,Qoo),e(Uh,Ox),e(Ox,Hoo),e(Uh,Uoo),e(F,Joo),e(F,Jh),e(Jh,$J),e($J,Yoo),e(Jh,Koo),e(Jh,Xx),e(Xx,Zoo),e(Jh,ero),e(F,oro),e(F,Yh),e(Yh,IJ),e(IJ,rro),e(Yh,tro),e(Yh,zx),e(zx,aro),e(Yh,nro),e(F,sro),e(F,Kh),e(Kh,jJ),e(jJ,lro),e(Kh,iro),e(Kh,Vx),e(Vx,dro),e(Kh,cro),e(F,fro),e(F,Zh),e(Zh,NJ),e(NJ,mro),e(Zh,gro),e(Zh,Wx),e(Wx,hro),e(Zh,pro),e(F,_ro),e(F,ep),e(ep,DJ),e(DJ,uro),e(ep,bro),e(ep,Qx),e(Qx,vro),e(ep,Tro),e(F,Fro),e(F,op),e(op,qJ),e(qJ,Cro),e(op,Mro),e(op,Hx),e(Hx,Ero),e(op,yro),e(F,wro),e(F,rp),e(rp,GJ),e(GJ,Aro),e(rp,Lro),e(rp,Ux),e(Ux,Bro),e(rp,kro),e(F,xro),e(F,tp),e(tp,OJ),e(OJ,Rro),e(tp,Sro),e(tp,Jx),e(Jx,Pro),e(tp,$ro),e(F,Iro),e(F,ap),e(ap,XJ),e(XJ,jro),e(ap,Nro),e(ap,Yx),e(Yx,Dro),e(ap,qro),e(F,Gro),e(F,np),e(np,zJ),e(zJ,Oro),e(np,Xro),e(np,Kx),e(Kx,zro),e(np,Vro),e(F,Wro),e(F,sp),e(sp,VJ),e(VJ,Qro),e(sp,Hro),e(sp,Zx),e(Zx,Uro),e(sp,Jro),e(F,Yro),e(F,lp),e(lp,WJ),e(WJ,Kro),e(lp,Zro),e(lp,eR),e(eR,eto),e(lp,oto),e(F,rto),e(F,ip),e(ip,QJ),e(QJ,tto),e(ip,ato),e(ip,oR),e(oR,nto),e(ip,sto),e(F,lto),e(F,dp),e(dp,HJ),e(HJ,ito),e(dp,dto),e(dp,rR),e(rR,cto),e(dp,fto),e(F,mto),e(F,cp),e(cp,UJ),e(UJ,gto),e(cp,hto),e(cp,tR),e(tR,pto),e(cp,_to),e(F,uto),e(F,fp),e(fp,JJ),e(JJ,bto),e(fp,vto),e(fp,aR),e(aR,Tto),e(fp,Fto),e(F,Cto),e(F,mp),e(mp,YJ),e(YJ,Mto),e(mp,Eto),e(mp,nR),e(nR,yto),e(mp,wto),e(F,Ato),e(F,gp),e(gp,KJ),e(KJ,Lto),e(gp,Bto),e(gp,sR),e(sR,kto),e(gp,xto),e(F,Rto),e(F,hp),e(hp,ZJ),e(ZJ,Sto),e(hp,Pto),e(hp,lR),e(lR,$to),e(hp,Ito),e(F,jto),e(F,pp),e(pp,eY),e(eY,Nto),e(pp,Dto),e(pp,iR),e(iR,qto),e(pp,Gto),e(F,Oto),e(F,_p),e(_p,oY),e(oY,Xto),e(_p,zto),e(_p,dR),e(dR,Vto),e(_p,Wto),e(F,Qto),e(F,up),e(up,rY),e(rY,Hto),e(up,Uto),e(up,cR),e(cR,Jto),e(up,Yto),e(F,Kto),e(F,bp),e(bp,tY),e(tY,Zto),e(bp,eao),e(bp,fR),e(fR,oao),e(bp,rao),e(F,tao),e(F,vp),e(vp,aY),e(aY,aao),e(vp,nao),e(vp,mR),e(mR,sao),e(vp,lao),e(F,iao),e(F,Tp),e(Tp,nY),e(nY,dao),e(Tp,cao),e(Tp,gR),e(gR,fao),e(Tp,mao),e(F,gao),e(F,Fp),e(Fp,sY),e(sY,hao),e(Fp,pao),e(Fp,hR),e(hR,_ao),e(Fp,uao),e(F,bao),e(F,Cp),e(Cp,lY),e(lY,vao),e(Cp,Tao),e(Cp,pR),e(pR,Fao),e(Cp,Cao),e(F,Mao),e(F,Mp),e(Mp,iY),e(iY,Eao),e(Mp,yao),e(Mp,_R),e(_R,wao),e(Mp,Aao),e(F,Lao),e(F,Ep),e(Ep,dY),e(dY,Bao),e(Ep,kao),e(Ep,uR),e(uR,xao),e(Ep,Rao),e(F,Sao),e(F,yp),e(yp,cY),e(cY,Pao),e(yp,$ao),e(yp,bR),e(bR,Iao),e(yp,jao),e(F,Nao),e(F,wp),e(wp,fY),e(fY,Dao),e(wp,qao),e(wp,vR),e(vR,Gao),e(wp,Oao),e(F,Xao),e(F,Ap),e(Ap,mY),e(mY,zao),e(Ap,Vao),e(Ap,TR),e(TR,Wao),e(Ap,Qao),e(F,Hao),e(F,Lp),e(Lp,gY),e(gY,Uao),e(Lp,Jao),e(Lp,FR),e(FR,Yao),e(Lp,Kao),e(F,Zao),e(F,Bp),e(Bp,hY),e(hY,eno),e(Bp,ono),e(Bp,CR),e(CR,rno),e(Bp,tno),e(F,ano),e(F,kp),e(kp,pY),e(pY,nno),e(kp,sno),e(kp,MR),e(MR,lno),e(kp,ino),e(F,dno),e(F,xp),e(xp,_Y),e(_Y,cno),e(xp,fno),e(xp,ER),e(ER,mno),e(xp,gno),e(F,hno),e(F,Rp),e(Rp,uY),e(uY,pno),e(Rp,_no),e(Rp,yR),e(yR,uno),e(Rp,bno),e(F,vno),e(F,Sp),e(Sp,bY),e(bY,Tno),e(Sp,Fno),e(Sp,wR),e(wR,Cno),e(Sp,Mno),e(F,Eno),e(F,Pp),e(Pp,vY),e(vY,yno),e(Pp,wno),e(Pp,AR),e(AR,Ano),e(Pp,Lno),e(F,Bno),e(F,$p),e($p,TY),e(TY,kno),e($p,xno),e($p,LR),e(LR,Rno),e($p,Sno),e(F,Pno),e(F,Ip),e(Ip,FY),e(FY,$no),e(Ip,Ino),e(Ip,BR),e(BR,jno),e(Ip,Nno),e(F,Dno),e(F,jp),e(jp,CY),e(CY,qno),e(jp,Gno),e(jp,kR),e(kR,Ono),e(jp,Xno),e(F,zno),e(F,Np),e(Np,MY),e(MY,Vno),e(Np,Wno),e(Np,xR),e(xR,Qno),e(Np,Hno),e(F,Uno),e(F,Dp),e(Dp,EY),e(EY,Jno),e(Dp,Yno),e(Dp,RR),e(RR,Kno),e(Dp,Zno),e(F,eso),e(F,qp),e(qp,yY),e(yY,oso),e(qp,rso),e(qp,SR),e(SR,tso),e(qp,aso),e(F,nso),e(F,Gp),e(Gp,wY),e(wY,sso),e(Gp,lso),e(Gp,PR),e(PR,iso),e(Gp,dso),e(F,cso),e(F,Op),e(Op,AY),e(AY,fso),e(Op,mso),e(Op,$R),e($R,gso),e(Op,hso),e(F,pso),e(F,Xp),e(Xp,LY),e(LY,_so),e(Xp,uso),e(Xp,IR),e(IR,bso),e(Xp,vso),e(F,Tso),e(F,zp),e(zp,BY),e(BY,Fso),e(zp,Cso),e(zp,jR),e(jR,Mso),e(zp,Eso),e(F,yso),e(F,Vp),e(Vp,kY),e(kY,wso),e(Vp,Aso),e(Vp,NR),e(NR,Lso),e(Vp,Bso),e(F,kso),e(F,Wp),e(Wp,xY),e(xY,xso),e(Wp,Rso),e(Wp,DR),e(DR,Sso),e(Wp,Pso),e(F,$so),e(F,Qp),e(Qp,RY),e(RY,Iso),e(Qp,jso),e(Qp,qR),e(qR,Nso),e(Qp,Dso),e(F,qso),e(F,Hp),e(Hp,SY),e(SY,Gso),e(Hp,Oso),e(Hp,GR),e(GR,Xso),e(Hp,zso),e(F,Vso),e(F,Up),e(Up,PY),e(PY,Wso),e(Up,Qso),e(Up,OR),e(OR,Hso),e(Up,Uso),e(F,Jso),e(F,Jp),e(Jp,$Y),e($Y,Yso),e(Jp,Kso),e(Jp,XR),e(XR,Zso),e(Jp,elo),e(ke,olo),e(ke,Yp),e(Yp,rlo),e(Yp,IY),e(IY,tlo),e(Yp,alo),e(Yp,jY),e(jY,nlo),e(ke,slo),e(ke,NY),e(NY,llo),e(ke,ilo),g(GE,ke,null),b(d,b8e,u),b(d,Xi,u),e(Xi,Kp),e(Kp,DY),g(OE,DY,null),e(Xi,dlo),e(Xi,qY),e(qY,clo),b(d,v8e,u),b(d,Wo,u),g(XE,Wo,null),e(Wo,flo),e(Wo,zi),e(zi,mlo),e(zi,GY),e(GY,glo),e(zi,hlo),e(zi,OY),e(OY,plo),e(zi,_lo),e(Wo,ulo),e(Wo,zE),e(zE,blo),e(zE,XY),e(XY,vlo),e(zE,Tlo),e(Wo,Flo),e(Wo,Dr),g(VE,Dr,null),e(Dr,Clo),e(Dr,zY),e(zY,Mlo),e(Dr,Elo),e(Dr,Vi),e(Vi,ylo),e(Vi,VY),e(VY,wlo),e(Vi,Alo),e(Vi,WY),e(WY,Llo),e(Vi,Blo),e(Dr,klo),e(Dr,QY),e(QY,xlo),e(Dr,Rlo),g(WE,Dr,null),e(Wo,Slo),e(Wo,xe),g(QE,xe,null),e(xe,Plo),e(xe,HY),e(HY,$lo),e(xe,Ilo),e(xe,qa),e(qa,jlo),e(qa,UY),e(UY,Nlo),e(qa,Dlo),e(qa,JY),e(JY,qlo),e(qa,Glo),e(qa,YY),e(YY,Olo),e(qa,Xlo),e(xe,zlo),e(xe,x),e(x,Zp),e(Zp,KY),e(KY,Vlo),e(Zp,Wlo),e(Zp,zR),e(zR,Qlo),e(Zp,Hlo),e(x,Ulo),e(x,e_),e(e_,ZY),e(ZY,Jlo),e(e_,Ylo),e(e_,VR),e(VR,Klo),e(e_,Zlo),e(x,eio),e(x,o_),e(o_,eK),e(eK,oio),e(o_,rio),e(o_,WR),e(WR,tio),e(o_,aio),e(x,nio),e(x,r_),e(r_,oK),e(oK,sio),e(r_,lio),e(r_,QR),e(QR,iio),e(r_,dio),e(x,cio),e(x,t_),e(t_,rK),e(rK,fio),e(t_,mio),e(t_,HR),e(HR,gio),e(t_,hio),e(x,pio),e(x,a_),e(a_,tK),e(tK,_io),e(a_,uio),e(a_,UR),e(UR,bio),e(a_,vio),e(x,Tio),e(x,n_),e(n_,aK),e(aK,Fio),e(n_,Cio),e(n_,JR),e(JR,Mio),e(n_,Eio),e(x,yio),e(x,s_),e(s_,nK),e(nK,wio),e(s_,Aio),e(s_,YR),e(YR,Lio),e(s_,Bio),e(x,kio),e(x,l_),e(l_,sK),e(sK,xio),e(l_,Rio),e(l_,KR),e(KR,Sio),e(l_,Pio),e(x,$io),e(x,i_),e(i_,lK),e(lK,Iio),e(i_,jio),e(i_,ZR),e(ZR,Nio),e(i_,Dio),e(x,qio),e(x,d_),e(d_,iK),e(iK,Gio),e(d_,Oio),e(d_,eS),e(eS,Xio),e(d_,zio),e(x,Vio),e(x,c_),e(c_,dK),e(dK,Wio),e(c_,Qio),e(c_,oS),e(oS,Hio),e(c_,Uio),e(x,Jio),e(x,f_),e(f_,cK),e(cK,Yio),e(f_,Kio),e(f_,rS),e(rS,Zio),e(f_,edo),e(x,odo),e(x,m_),e(m_,fK),e(fK,rdo),e(m_,tdo),e(m_,tS),e(tS,ado),e(m_,ndo),e(x,sdo),e(x,g_),e(g_,mK),e(mK,ldo),e(g_,ido),e(g_,aS),e(aS,ddo),e(g_,cdo),e(x,fdo),e(x,h_),e(h_,gK),e(gK,mdo),e(h_,gdo),e(h_,nS),e(nS,hdo),e(h_,pdo),e(x,_do),e(x,p_),e(p_,hK),e(hK,udo),e(p_,bdo),e(p_,sS),e(sS,vdo),e(p_,Tdo),e(x,Fdo),e(x,__),e(__,pK),e(pK,Cdo),e(__,Mdo),e(__,lS),e(lS,Edo),e(__,ydo),e(x,wdo),e(x,u_),e(u_,_K),e(_K,Ado),e(u_,Ldo),e(u_,iS),e(iS,Bdo),e(u_,kdo),e(x,xdo),e(x,b_),e(b_,uK),e(uK,Rdo),e(b_,Sdo),e(b_,dS),e(dS,Pdo),e(b_,$do),e(x,Ido),e(x,v_),e(v_,bK),e(bK,jdo),e(v_,Ndo),e(v_,cS),e(cS,Ddo),e(v_,qdo),e(x,Gdo),e(x,T_),e(T_,vK),e(vK,Odo),e(T_,Xdo),e(T_,fS),e(fS,zdo),e(T_,Vdo),e(x,Wdo),e(x,F_),e(F_,TK),e(TK,Qdo),e(F_,Hdo),e(F_,mS),e(mS,Udo),e(F_,Jdo),e(x,Ydo),e(x,C_),e(C_,FK),e(FK,Kdo),e(C_,Zdo),e(C_,gS),e(gS,eco),e(C_,oco),e(x,rco),e(x,M_),e(M_,CK),e(CK,tco),e(M_,aco),e(M_,hS),e(hS,nco),e(M_,sco),e(x,lco),e(x,E_),e(E_,MK),e(MK,ico),e(E_,dco),e(E_,pS),e(pS,cco),e(E_,fco),e(x,mco),e(x,y_),e(y_,EK),e(EK,gco),e(y_,hco),e(y_,_S),e(_S,pco),e(y_,_co),e(x,uco),e(x,w_),e(w_,yK),e(yK,bco),e(w_,vco),e(w_,uS),e(uS,Tco),e(w_,Fco),e(x,Cco),e(x,A_),e(A_,wK),e(wK,Mco),e(A_,Eco),e(A_,bS),e(bS,yco),e(A_,wco),e(x,Aco),e(x,L_),e(L_,AK),e(AK,Lco),e(L_,Bco),e(L_,vS),e(vS,kco),e(L_,xco),e(x,Rco),e(x,B_),e(B_,LK),e(LK,Sco),e(B_,Pco),e(B_,TS),e(TS,$co),e(B_,Ico),e(x,jco),e(x,k_),e(k_,BK),e(BK,Nco),e(k_,Dco),e(k_,FS),e(FS,qco),e(k_,Gco),e(x,Oco),e(x,x_),e(x_,kK),e(kK,Xco),e(x_,zco),e(x_,CS),e(CS,Vco),e(x_,Wco),e(x,Qco),e(x,R_),e(R_,xK),e(xK,Hco),e(R_,Uco),e(R_,MS),e(MS,Jco),e(R_,Yco),e(x,Kco),e(x,S_),e(S_,RK),e(RK,Zco),e(S_,efo),e(S_,ES),e(ES,ofo),e(S_,rfo),e(x,tfo),e(x,P_),e(P_,SK),e(SK,afo),e(P_,nfo),e(P_,yS),e(yS,sfo),e(P_,lfo),e(x,ifo),e(x,$_),e($_,PK),e(PK,dfo),e($_,cfo),e($_,wS),e(wS,ffo),e($_,mfo),e(x,gfo),e(x,I_),e(I_,$K),e($K,hfo),e(I_,pfo),e(I_,AS),e(AS,_fo),e(I_,ufo),e(xe,bfo),e(xe,j_),e(j_,vfo),e(j_,IK),e(IK,Tfo),e(j_,Ffo),e(j_,jK),e(jK,Cfo),e(xe,Mfo),e(xe,NK),e(NK,Efo),e(xe,yfo),g(HE,xe,null),b(d,T8e,u),b(d,Wi,u),e(Wi,N_),e(N_,DK),g(UE,DK,null),e(Wi,wfo),e(Wi,qK),e(qK,Afo),b(d,F8e,u),b(d,Qo,u),g(JE,Qo,null),e(Qo,Lfo),e(Qo,Qi),e(Qi,Bfo),e(Qi,GK),e(GK,kfo),e(Qi,xfo),e(Qi,OK),e(OK,Rfo),e(Qi,Sfo),e(Qo,Pfo),e(Qo,YE),e(YE,$fo),e(YE,XK),e(XK,Ifo),e(YE,jfo),e(Qo,Nfo),e(Qo,qr),g(KE,qr,null),e(qr,Dfo),e(qr,zK),e(zK,qfo),e(qr,Gfo),e(qr,Hi),e(Hi,Ofo),e(Hi,VK),e(VK,Xfo),e(Hi,zfo),e(Hi,WK),e(WK,Vfo),e(Hi,Wfo),e(qr,Qfo),e(qr,QK),e(QK,Hfo),e(qr,Ufo),g(ZE,qr,null),e(Qo,Jfo),e(Qo,Re),g(e3,Re,null),e(Re,Yfo),e(Re,HK),e(HK,Kfo),e(Re,Zfo),e(Re,Ga),e(Ga,emo),e(Ga,UK),e(UK,omo),e(Ga,rmo),e(Ga,JK),e(JK,tmo),e(Ga,amo),e(Ga,YK),e(YK,nmo),e(Ga,smo),e(Re,lmo),e(Re,$),e($,D_),e(D_,KK),e(KK,imo),e(D_,dmo),e(D_,LS),e(LS,cmo),e(D_,fmo),e($,mmo),e($,q_),e(q_,ZK),e(ZK,gmo),e(q_,hmo),e(q_,BS),e(BS,pmo),e(q_,_mo),e($,umo),e($,G_),e(G_,eZ),e(eZ,bmo),e(G_,vmo),e(G_,kS),e(kS,Tmo),e(G_,Fmo),e($,Cmo),e($,O_),e(O_,oZ),e(oZ,Mmo),e(O_,Emo),e(O_,xS),e(xS,ymo),e(O_,wmo),e($,Amo),e($,X_),e(X_,rZ),e(rZ,Lmo),e(X_,Bmo),e(X_,RS),e(RS,kmo),e(X_,xmo),e($,Rmo),e($,z_),e(z_,tZ),e(tZ,Smo),e(z_,Pmo),e(z_,SS),e(SS,$mo),e(z_,Imo),e($,jmo),e($,V_),e(V_,aZ),e(aZ,Nmo),e(V_,Dmo),e(V_,PS),e(PS,qmo),e(V_,Gmo),e($,Omo),e($,W_),e(W_,nZ),e(nZ,Xmo),e(W_,zmo),e(W_,$S),e($S,Vmo),e(W_,Wmo),e($,Qmo),e($,Q_),e(Q_,sZ),e(sZ,Hmo),e(Q_,Umo),e(Q_,IS),e(IS,Jmo),e(Q_,Ymo),e($,Kmo),e($,H_),e(H_,lZ),e(lZ,Zmo),e(H_,ego),e(H_,jS),e(jS,ogo),e(H_,rgo),e($,tgo),e($,U_),e(U_,iZ),e(iZ,ago),e(U_,ngo),e(U_,NS),e(NS,sgo),e(U_,lgo),e($,igo),e($,J_),e(J_,dZ),e(dZ,dgo),e(J_,cgo),e(J_,DS),e(DS,fgo),e(J_,mgo),e($,ggo),e($,Y_),e(Y_,cZ),e(cZ,hgo),e(Y_,pgo),e(Y_,qS),e(qS,_go),e(Y_,ugo),e($,bgo),e($,K_),e(K_,fZ),e(fZ,vgo),e(K_,Tgo),e(K_,GS),e(GS,Fgo),e(K_,Cgo),e($,Mgo),e($,Z_),e(Z_,mZ),e(mZ,Ego),e(Z_,ygo),e(Z_,OS),e(OS,wgo),e(Z_,Ago),e($,Lgo),e($,eu),e(eu,gZ),e(gZ,Bgo),e(eu,kgo),e(eu,XS),e(XS,xgo),e(eu,Rgo),e($,Sgo),e($,ou),e(ou,hZ),e(hZ,Pgo),e(ou,$go),e(ou,zS),e(zS,Igo),e(ou,jgo),e($,Ngo),e($,ru),e(ru,pZ),e(pZ,Dgo),e(ru,qgo),e(ru,VS),e(VS,Ggo),e(ru,Ogo),e($,Xgo),e($,tu),e(tu,_Z),e(_Z,zgo),e(tu,Vgo),e(tu,WS),e(WS,Wgo),e(tu,Qgo),e($,Hgo),e($,au),e(au,uZ),e(uZ,Ugo),e(au,Jgo),e(au,QS),e(QS,Ygo),e(au,Kgo),e($,Zgo),e($,nu),e(nu,bZ),e(bZ,eho),e(nu,oho),e(nu,HS),e(HS,rho),e(nu,tho),e($,aho),e($,su),e(su,vZ),e(vZ,nho),e(su,sho),e(su,US),e(US,lho),e(su,iho),e($,dho),e($,lu),e(lu,TZ),e(TZ,cho),e(lu,fho),e(lu,JS),e(JS,mho),e(lu,gho),e($,hho),e($,iu),e(iu,FZ),e(FZ,pho),e(iu,_ho),e(iu,YS),e(YS,uho),e(iu,bho),e($,vho),e($,du),e(du,CZ),e(CZ,Tho),e(du,Fho),e(du,KS),e(KS,Cho),e(du,Mho),e($,Eho),e($,cu),e(cu,MZ),e(MZ,yho),e(cu,who),e(cu,ZS),e(ZS,Aho),e(cu,Lho),e($,Bho),e($,fu),e(fu,EZ),e(EZ,kho),e(fu,xho),e(fu,eP),e(eP,Rho),e(fu,Sho),e($,Pho),e($,mu),e(mu,yZ),e(yZ,$ho),e(mu,Iho),e(mu,oP),e(oP,jho),e(mu,Nho),e($,Dho),e($,gu),e(gu,wZ),e(wZ,qho),e(gu,Gho),e(gu,rP),e(rP,Oho),e(gu,Xho),e($,zho),e($,hu),e(hu,AZ),e(AZ,Vho),e(hu,Who),e(hu,tP),e(tP,Qho),e(hu,Hho),e($,Uho),e($,pu),e(pu,LZ),e(LZ,Jho),e(pu,Yho),e(pu,aP),e(aP,Kho),e(pu,Zho),e($,epo),e($,_u),e(_u,BZ),e(BZ,opo),e(_u,rpo),e(_u,nP),e(nP,tpo),e(_u,apo),e($,npo),e($,uu),e(uu,kZ),e(kZ,spo),e(uu,lpo),e(uu,sP),e(sP,ipo),e(uu,dpo),e($,cpo),e($,bu),e(bu,xZ),e(xZ,fpo),e(bu,mpo),e(bu,lP),e(lP,gpo),e(bu,hpo),e(Re,ppo),e(Re,vu),e(vu,_po),e(vu,RZ),e(RZ,upo),e(vu,bpo),e(vu,SZ),e(SZ,vpo),e(Re,Tpo),e(Re,PZ),e(PZ,Fpo),e(Re,Cpo),g(o3,Re,null),b(d,C8e,u),b(d,Ui,u),e(Ui,Tu),e(Tu,$Z),g(r3,$Z,null),e(Ui,Mpo),e(Ui,IZ),e(IZ,Epo),b(d,M8e,u),b(d,Ho,u),g(t3,Ho,null),e(Ho,ypo),e(Ho,Ji),e(Ji,wpo),e(Ji,jZ),e(jZ,Apo),e(Ji,Lpo),e(Ji,NZ),e(NZ,Bpo),e(Ji,kpo),e(Ho,xpo),e(Ho,a3),e(a3,Rpo),e(a3,DZ),e(DZ,Spo),e(a3,Ppo),e(Ho,$po),e(Ho,Gr),g(n3,Gr,null),e(Gr,Ipo),e(Gr,qZ),e(qZ,jpo),e(Gr,Npo),e(Gr,Yi),e(Yi,Dpo),e(Yi,GZ),e(GZ,qpo),e(Yi,Gpo),e(Yi,OZ),e(OZ,Opo),e(Yi,Xpo),e(Gr,zpo),e(Gr,XZ),e(XZ,Vpo),e(Gr,Wpo),g(s3,Gr,null),e(Ho,Qpo),e(Ho,Se),g(l3,Se,null),e(Se,Hpo),e(Se,zZ),e(zZ,Upo),e(Se,Jpo),e(Se,Oa),e(Oa,Ypo),e(Oa,VZ),e(VZ,Kpo),e(Oa,Zpo),e(Oa,WZ),e(WZ,e_o),e(Oa,o_o),e(Oa,QZ),e(QZ,r_o),e(Oa,t_o),e(Se,a_o),e(Se,I),e(I,Fu),e(Fu,HZ),e(HZ,n_o),e(Fu,s_o),e(Fu,iP),e(iP,l_o),e(Fu,i_o),e(I,d_o),e(I,Cu),e(Cu,UZ),e(UZ,c_o),e(Cu,f_o),e(Cu,dP),e(dP,m_o),e(Cu,g_o),e(I,h_o),e(I,Mu),e(Mu,JZ),e(JZ,p_o),e(Mu,__o),e(Mu,cP),e(cP,u_o),e(Mu,b_o),e(I,v_o),e(I,Eu),e(Eu,YZ),e(YZ,T_o),e(Eu,F_o),e(Eu,fP),e(fP,C_o),e(Eu,M_o),e(I,E_o),e(I,yu),e(yu,KZ),e(KZ,y_o),e(yu,w_o),e(yu,mP),e(mP,A_o),e(yu,L_o),e(I,B_o),e(I,wu),e(wu,ZZ),e(ZZ,k_o),e(wu,x_o),e(wu,gP),e(gP,R_o),e(wu,S_o),e(I,P_o),e(I,Au),e(Au,eee),e(eee,$_o),e(Au,I_o),e(Au,hP),e(hP,j_o),e(Au,N_o),e(I,D_o),e(I,Lu),e(Lu,oee),e(oee,q_o),e(Lu,G_o),e(Lu,pP),e(pP,O_o),e(Lu,X_o),e(I,z_o),e(I,Bu),e(Bu,ree),e(ree,V_o),e(Bu,W_o),e(Bu,_P),e(_P,Q_o),e(Bu,H_o),e(I,U_o),e(I,ku),e(ku,tee),e(tee,J_o),e(ku,Y_o),e(ku,uP),e(uP,K_o),e(ku,Z_o),e(I,euo),e(I,xu),e(xu,aee),e(aee,ouo),e(xu,ruo),e(xu,bP),e(bP,tuo),e(xu,auo),e(I,nuo),e(I,Ru),e(Ru,nee),e(nee,suo),e(Ru,luo),e(Ru,vP),e(vP,iuo),e(Ru,duo),e(I,cuo),e(I,Su),e(Su,see),e(see,fuo),e(Su,muo),e(Su,TP),e(TP,guo),e(Su,huo),e(I,puo),e(I,Pu),e(Pu,lee),e(lee,_uo),e(Pu,uuo),e(Pu,FP),e(FP,buo),e(Pu,vuo),e(I,Tuo),e(I,$u),e($u,iee),e(iee,Fuo),e($u,Cuo),e($u,CP),e(CP,Muo),e($u,Euo),e(I,yuo),e(I,Iu),e(Iu,dee),e(dee,wuo),e(Iu,Auo),e(Iu,MP),e(MP,Luo),e(Iu,Buo),e(I,kuo),e(I,ju),e(ju,cee),e(cee,xuo),e(ju,Ruo),e(ju,EP),e(EP,Suo),e(ju,Puo),e(I,$uo),e(I,Nu),e(Nu,fee),e(fee,Iuo),e(Nu,juo),e(Nu,yP),e(yP,Nuo),e(Nu,Duo),e(I,quo),e(I,Du),e(Du,mee),e(mee,Guo),e(Du,Ouo),e(Du,wP),e(wP,Xuo),e(Du,zuo),e(I,Vuo),e(I,qu),e(qu,gee),e(gee,Wuo),e(qu,Quo),e(qu,AP),e(AP,Huo),e(qu,Uuo),e(I,Juo),e(I,Gu),e(Gu,hee),e(hee,Yuo),e(Gu,Kuo),e(Gu,LP),e(LP,Zuo),e(Gu,e1o),e(I,o1o),e(I,Ou),e(Ou,pee),e(pee,r1o),e(Ou,t1o),e(Ou,BP),e(BP,a1o),e(Ou,n1o),e(I,s1o),e(I,Xu),e(Xu,_ee),e(_ee,l1o),e(Xu,i1o),e(Xu,kP),e(kP,d1o),e(Xu,c1o),e(I,f1o),e(I,zu),e(zu,uee),e(uee,m1o),e(zu,g1o),e(zu,xP),e(xP,h1o),e(zu,p1o),e(I,_1o),e(I,Vu),e(Vu,bee),e(bee,u1o),e(Vu,b1o),e(Vu,RP),e(RP,v1o),e(Vu,T1o),e(I,F1o),e(I,Wu),e(Wu,vee),e(vee,C1o),e(Wu,M1o),e(Wu,SP),e(SP,E1o),e(Wu,y1o),e(I,w1o),e(I,Qu),e(Qu,Tee),e(Tee,A1o),e(Qu,L1o),e(Qu,PP),e(PP,B1o),e(Qu,k1o),e(I,x1o),e(I,Hu),e(Hu,Fee),e(Fee,R1o),e(Hu,S1o),e(Hu,$P),e($P,P1o),e(Hu,$1o),e(I,I1o),e(I,Uu),e(Uu,Cee),e(Cee,j1o),e(Uu,N1o),e(Uu,IP),e(IP,D1o),e(Uu,q1o),e(I,G1o),e(I,Ju),e(Ju,Mee),e(Mee,O1o),e(Ju,X1o),e(Ju,Eee),e(Eee,z1o),e(Ju,V1o),e(I,W1o),e(I,Yu),e(Yu,yee),e(yee,Q1o),e(Yu,H1o),e(Yu,jP),e(jP,U1o),e(Yu,J1o),e(I,Y1o),e(I,Ku),e(Ku,wee),e(wee,K1o),e(Ku,Z1o),e(Ku,NP),e(NP,e7o),e(Ku,o7o),e(I,r7o),e(I,Zu),e(Zu,Aee),e(Aee,t7o),e(Zu,a7o),e(Zu,DP),e(DP,n7o),e(Zu,s7o),e(I,l7o),e(I,e1),e(e1,Lee),e(Lee,i7o),e(e1,d7o),e(e1,qP),e(qP,c7o),e(e1,f7o),e(Se,m7o),e(Se,o1),e(o1,g7o),e(o1,Bee),e(Bee,h7o),e(o1,p7o),e(o1,kee),e(kee,_7o),e(Se,u7o),e(Se,xee),e(xee,b7o),e(Se,v7o),g(i3,Se,null),b(d,E8e,u),b(d,Ki,u),e(Ki,r1),e(r1,Ree),g(d3,Ree,null),e(Ki,T7o),e(Ki,See),e(See,F7o),b(d,y8e,u),b(d,Uo,u),g(c3,Uo,null),e(Uo,C7o),e(Uo,Zi),e(Zi,M7o),e(Zi,Pee),e(Pee,E7o),e(Zi,y7o),e(Zi,$ee),e($ee,w7o),e(Zi,A7o),e(Uo,L7o),e(Uo,f3),e(f3,B7o),e(f3,Iee),e(Iee,k7o),e(f3,x7o),e(Uo,R7o),e(Uo,Or),g(m3,Or,null),e(Or,S7o),e(Or,jee),e(jee,P7o),e(Or,$7o),e(Or,ed),e(ed,I7o),e(ed,Nee),e(Nee,j7o),e(ed,N7o),e(ed,Dee),e(Dee,D7o),e(ed,q7o),e(Or,G7o),e(Or,qee),e(qee,O7o),e(Or,X7o),g(g3,Or,null),e(Uo,z7o),e(Uo,Pe),g(h3,Pe,null),e(Pe,V7o),e(Pe,Gee),e(Gee,W7o),e(Pe,Q7o),e(Pe,Xa),e(Xa,H7o),e(Xa,Oee),e(Oee,U7o),e(Xa,J7o),e(Xa,Xee),e(Xee,Y7o),e(Xa,K7o),e(Xa,zee),e(zee,Z7o),e(Xa,e4o),e(Pe,o4o),e(Pe,ae),e(ae,t1),e(t1,Vee),e(Vee,r4o),e(t1,t4o),e(t1,GP),e(GP,a4o),e(t1,n4o),e(ae,s4o),e(ae,a1),e(a1,Wee),e(Wee,l4o),e(a1,i4o),e(a1,OP),e(OP,d4o),e(a1,c4o),e(ae,f4o),e(ae,n1),e(n1,Qee),e(Qee,m4o),e(n1,g4o),e(n1,XP),e(XP,h4o),e(n1,p4o),e(ae,_4o),e(ae,s1),e(s1,Hee),e(Hee,u4o),e(s1,b4o),e(s1,zP),e(zP,v4o),e(s1,T4o),e(ae,F4o),e(ae,l1),e(l1,Uee),e(Uee,C4o),e(l1,M4o),e(l1,VP),e(VP,E4o),e(l1,y4o),e(ae,w4o),e(ae,i1),e(i1,Jee),e(Jee,A4o),e(i1,L4o),e(i1,WP),e(WP,B4o),e(i1,k4o),e(ae,x4o),e(ae,d1),e(d1,Yee),e(Yee,R4o),e(d1,S4o),e(d1,QP),e(QP,P4o),e(d1,$4o),e(ae,I4o),e(ae,c1),e(c1,Kee),e(Kee,j4o),e(c1,N4o),e(c1,HP),e(HP,D4o),e(c1,q4o),e(ae,G4o),e(ae,f1),e(f1,Zee),e(Zee,O4o),e(f1,X4o),e(f1,UP),e(UP,z4o),e(f1,V4o),e(ae,W4o),e(ae,m1),e(m1,eoe),e(eoe,Q4o),e(m1,H4o),e(m1,JP),e(JP,U4o),e(m1,J4o),e(ae,Y4o),e(ae,g1),e(g1,ooe),e(ooe,K4o),e(g1,Z4o),e(g1,YP),e(YP,ebo),e(g1,obo),e(ae,rbo),e(ae,h1),e(h1,roe),e(roe,tbo),e(h1,abo),e(h1,KP),e(KP,nbo),e(h1,sbo),e(ae,lbo),e(ae,p1),e(p1,toe),e(toe,ibo),e(p1,dbo),e(p1,ZP),e(ZP,cbo),e(p1,fbo),e(ae,mbo),e(ae,_1),e(_1,aoe),e(aoe,gbo),e(_1,hbo),e(_1,e$),e(e$,pbo),e(_1,_bo),e(ae,ubo),e(ae,u1),e(u1,noe),e(noe,bbo),e(u1,vbo),e(u1,o$),e(o$,Tbo),e(u1,Fbo),e(ae,Cbo),e(ae,b1),e(b1,soe),e(soe,Mbo),e(b1,Ebo),e(b1,r$),e(r$,ybo),e(b1,wbo),e(Pe,Abo),e(Pe,v1),e(v1,Lbo),e(v1,loe),e(loe,Bbo),e(v1,kbo),e(v1,ioe),e(ioe,xbo),e(Pe,Rbo),e(Pe,doe),e(doe,Sbo),e(Pe,Pbo),g(p3,Pe,null),b(d,w8e,u),b(d,od,u),e(od,T1),e(T1,coe),g(_3,coe,null),e(od,$bo),e(od,foe),e(foe,Ibo),b(d,A8e,u),b(d,Jo,u),g(u3,Jo,null),e(Jo,jbo),e(Jo,rd),e(rd,Nbo),e(rd,moe),e(moe,Dbo),e(rd,qbo),e(rd,goe),e(goe,Gbo),e(rd,Obo),e(Jo,Xbo),e(Jo,b3),e(b3,zbo),e(b3,hoe),e(hoe,Vbo),e(b3,Wbo),e(Jo,Qbo),e(Jo,Xr),g(v3,Xr,null),e(Xr,Hbo),e(Xr,poe),e(poe,Ubo),e(Xr,Jbo),e(Xr,td),e(td,Ybo),e(td,_oe),e(_oe,Kbo),e(td,Zbo),e(td,uoe),e(uoe,e5o),e(td,o5o),e(Xr,r5o),e(Xr,boe),e(boe,t5o),e(Xr,a5o),g(T3,Xr,null),e(Jo,n5o),e(Jo,$e),g(F3,$e,null),e($e,s5o),e($e,voe),e(voe,l5o),e($e,i5o),e($e,za),e(za,d5o),e(za,Toe),e(Toe,c5o),e(za,f5o),e(za,Foe),e(Foe,m5o),e(za,g5o),e(za,Coe),e(Coe,h5o),e(za,p5o),e($e,_5o),e($e,A),e(A,F1),e(F1,Moe),e(Moe,u5o),e(F1,b5o),e(F1,t$),e(t$,v5o),e(F1,T5o),e(A,F5o),e(A,C1),e(C1,Eoe),e(Eoe,C5o),e(C1,M5o),e(C1,a$),e(a$,E5o),e(C1,y5o),e(A,w5o),e(A,M1),e(M1,yoe),e(yoe,A5o),e(M1,L5o),e(M1,n$),e(n$,B5o),e(M1,k5o),e(A,x5o),e(A,E1),e(E1,woe),e(woe,R5o),e(E1,S5o),e(E1,s$),e(s$,P5o),e(E1,$5o),e(A,I5o),e(A,y1),e(y1,Aoe),e(Aoe,j5o),e(y1,N5o),e(y1,l$),e(l$,D5o),e(y1,q5o),e(A,G5o),e(A,w1),e(w1,Loe),e(Loe,O5o),e(w1,X5o),e(w1,i$),e(i$,z5o),e(w1,V5o),e(A,W5o),e(A,A1),e(A1,Boe),e(Boe,Q5o),e(A1,H5o),e(A1,d$),e(d$,U5o),e(A1,J5o),e(A,Y5o),e(A,L1),e(L1,koe),e(koe,K5o),e(L1,Z5o),e(L1,c$),e(c$,e2o),e(L1,o2o),e(A,r2o),e(A,B1),e(B1,xoe),e(xoe,t2o),e(B1,a2o),e(B1,f$),e(f$,n2o),e(B1,s2o),e(A,l2o),e(A,k1),e(k1,Roe),e(Roe,i2o),e(k1,d2o),e(k1,m$),e(m$,c2o),e(k1,f2o),e(A,m2o),e(A,x1),e(x1,Soe),e(Soe,g2o),e(x1,h2o),e(x1,g$),e(g$,p2o),e(x1,_2o),e(A,u2o),e(A,R1),e(R1,Poe),e(Poe,b2o),e(R1,v2o),e(R1,h$),e(h$,T2o),e(R1,F2o),e(A,C2o),e(A,S1),e(S1,$oe),e($oe,M2o),e(S1,E2o),e(S1,p$),e(p$,y2o),e(S1,w2o),e(A,A2o),e(A,P1),e(P1,Ioe),e(Ioe,L2o),e(P1,B2o),e(P1,_$),e(_$,k2o),e(P1,x2o),e(A,R2o),e(A,$1),e($1,joe),e(joe,S2o),e($1,P2o),e($1,u$),e(u$,$2o),e($1,I2o),e(A,j2o),e(A,I1),e(I1,Noe),e(Noe,N2o),e(I1,D2o),e(I1,b$),e(b$,q2o),e(I1,G2o),e(A,O2o),e(A,j1),e(j1,Doe),e(Doe,X2o),e(j1,z2o),e(j1,v$),e(v$,V2o),e(j1,W2o),e(A,Q2o),e(A,N1),e(N1,qoe),e(qoe,H2o),e(N1,U2o),e(N1,T$),e(T$,J2o),e(N1,Y2o),e(A,K2o),e(A,D1),e(D1,Goe),e(Goe,Z2o),e(D1,evo),e(D1,F$),e(F$,ovo),e(D1,rvo),e(A,tvo),e(A,q1),e(q1,Ooe),e(Ooe,avo),e(q1,nvo),e(q1,C$),e(C$,svo),e(q1,lvo),e(A,ivo),e(A,G1),e(G1,Xoe),e(Xoe,dvo),e(G1,cvo),e(G1,M$),e(M$,fvo),e(G1,mvo),e(A,gvo),e(A,O1),e(O1,zoe),e(zoe,hvo),e(O1,pvo),e(O1,E$),e(E$,_vo),e(O1,uvo),e(A,bvo),e(A,X1),e(X1,Voe),e(Voe,vvo),e(X1,Tvo),e(X1,y$),e(y$,Fvo),e(X1,Cvo),e(A,Mvo),e(A,z1),e(z1,Woe),e(Woe,Evo),e(z1,yvo),e(z1,w$),e(w$,wvo),e(z1,Avo),e(A,Lvo),e(A,V1),e(V1,Qoe),e(Qoe,Bvo),e(V1,kvo),e(V1,A$),e(A$,xvo),e(V1,Rvo),e(A,Svo),e(A,W1),e(W1,Hoe),e(Hoe,Pvo),e(W1,$vo),e(W1,L$),e(L$,Ivo),e(W1,jvo),e(A,Nvo),e(A,Q1),e(Q1,Uoe),e(Uoe,Dvo),e(Q1,qvo),e(Q1,B$),e(B$,Gvo),e(Q1,Ovo),e(A,Xvo),e(A,H1),e(H1,Joe),e(Joe,zvo),e(H1,Vvo),e(H1,k$),e(k$,Wvo),e(H1,Qvo),e(A,Hvo),e(A,U1),e(U1,Yoe),e(Yoe,Uvo),e(U1,Jvo),e(U1,x$),e(x$,Yvo),e(U1,Kvo),e(A,Zvo),e(A,J1),e(J1,Koe),e(Koe,eTo),e(J1,oTo),e(J1,R$),e(R$,rTo),e(J1,tTo),e(A,aTo),e(A,Y1),e(Y1,Zoe),e(Zoe,nTo),e(Y1,sTo),e(Y1,S$),e(S$,lTo),e(Y1,iTo),e(A,dTo),e(A,K1),e(K1,ere),e(ere,cTo),e(K1,fTo),e(K1,P$),e(P$,mTo),e(K1,gTo),e(A,hTo),e(A,Z1),e(Z1,ore),e(ore,pTo),e(Z1,_To),e(Z1,$$),e($$,uTo),e(Z1,bTo),e(A,vTo),e(A,e7),e(e7,rre),e(rre,TTo),e(e7,FTo),e(e7,I$),e(I$,CTo),e(e7,MTo),e(A,ETo),e(A,o7),e(o7,tre),e(tre,yTo),e(o7,wTo),e(o7,j$),e(j$,ATo),e(o7,LTo),e(A,BTo),e(A,r7),e(r7,are),e(are,kTo),e(r7,xTo),e(r7,N$),e(N$,RTo),e(r7,STo),e(A,PTo),e(A,t7),e(t7,nre),e(nre,$To),e(t7,ITo),e(t7,D$),e(D$,jTo),e(t7,NTo),e(A,DTo),e(A,a7),e(a7,sre),e(sre,qTo),e(a7,GTo),e(a7,q$),e(q$,OTo),e(a7,XTo),e(A,zTo),e(A,n7),e(n7,lre),e(lre,VTo),e(n7,WTo),e(n7,G$),e(G$,QTo),e(n7,HTo),e(A,UTo),e(A,s7),e(s7,ire),e(ire,JTo),e(s7,YTo),e(s7,O$),e(O$,KTo),e(s7,ZTo),e(A,eFo),e(A,l7),e(l7,dre),e(dre,oFo),e(l7,rFo),e(l7,X$),e(X$,tFo),e(l7,aFo),e(A,nFo),e(A,i7),e(i7,cre),e(cre,sFo),e(i7,lFo),e(i7,z$),e(z$,iFo),e(i7,dFo),e(A,cFo),e(A,d7),e(d7,fre),e(fre,fFo),e(d7,mFo),e(d7,V$),e(V$,gFo),e(d7,hFo),e(A,pFo),e(A,c7),e(c7,mre),e(mre,_Fo),e(c7,uFo),e(c7,W$),e(W$,bFo),e(c7,vFo),e(A,TFo),e(A,f7),e(f7,gre),e(gre,FFo),e(f7,CFo),e(f7,Q$),e(Q$,MFo),e(f7,EFo),e($e,yFo),e($e,m7),e(m7,wFo),e(m7,hre),e(hre,AFo),e(m7,LFo),e(m7,pre),e(pre,BFo),e($e,kFo),e($e,_re),e(_re,xFo),e($e,RFo),g(C3,$e,null),b(d,L8e,u),b(d,ad,u),e(ad,g7),e(g7,ure),g(M3,ure,null),e(ad,SFo),e(ad,bre),e(bre,PFo),b(d,B8e,u),b(d,Yo,u),g(E3,Yo,null),e(Yo,$Fo),e(Yo,nd),e(nd,IFo),e(nd,vre),e(vre,jFo),e(nd,NFo),e(nd,Tre),e(Tre,DFo),e(nd,qFo),e(Yo,GFo),e(Yo,y3),e(y3,OFo),e(y3,Fre),e(Fre,XFo),e(y3,zFo),e(Yo,VFo),e(Yo,zr),g(w3,zr,null),e(zr,WFo),e(zr,Cre),e(Cre,QFo),e(zr,HFo),e(zr,sd),e(sd,UFo),e(sd,Mre),e(Mre,JFo),e(sd,YFo),e(sd,Ere),e(Ere,KFo),e(sd,ZFo),e(zr,eCo),e(zr,yre),e(yre,oCo),e(zr,rCo),g(A3,zr,null),e(Yo,tCo),e(Yo,Ie),g(L3,Ie,null),e(Ie,aCo),e(Ie,wre),e(wre,nCo),e(Ie,sCo),e(Ie,Va),e(Va,lCo),e(Va,Are),e(Are,iCo),e(Va,dCo),e(Va,Lre),e(Lre,cCo),e(Va,fCo),e(Va,Bre),e(Bre,mCo),e(Va,gCo),e(Ie,hCo),e(Ie,G),e(G,h7),e(h7,kre),e(kre,pCo),e(h7,_Co),e(h7,H$),e(H$,uCo),e(h7,bCo),e(G,vCo),e(G,p7),e(p7,xre),e(xre,TCo),e(p7,FCo),e(p7,U$),e(U$,CCo),e(p7,MCo),e(G,ECo),e(G,_7),e(_7,Rre),e(Rre,yCo),e(_7,wCo),e(_7,J$),e(J$,ACo),e(_7,LCo),e(G,BCo),e(G,u7),e(u7,Sre),e(Sre,kCo),e(u7,xCo),e(u7,Y$),e(Y$,RCo),e(u7,SCo),e(G,PCo),e(G,b7),e(b7,Pre),e(Pre,$Co),e(b7,ICo),e(b7,K$),e(K$,jCo),e(b7,NCo),e(G,DCo),e(G,v7),e(v7,$re),e($re,qCo),e(v7,GCo),e(v7,Z$),e(Z$,OCo),e(v7,XCo),e(G,zCo),e(G,T7),e(T7,Ire),e(Ire,VCo),e(T7,WCo),e(T7,eI),e(eI,QCo),e(T7,HCo),e(G,UCo),e(G,F7),e(F7,jre),e(jre,JCo),e(F7,YCo),e(F7,oI),e(oI,KCo),e(F7,ZCo),e(G,eMo),e(G,C7),e(C7,Nre),e(Nre,oMo),e(C7,rMo),e(C7,rI),e(rI,tMo),e(C7,aMo),e(G,nMo),e(G,M7),e(M7,Dre),e(Dre,sMo),e(M7,lMo),e(M7,tI),e(tI,iMo),e(M7,dMo),e(G,cMo),e(G,E7),e(E7,qre),e(qre,fMo),e(E7,mMo),e(E7,aI),e(aI,gMo),e(E7,hMo),e(G,pMo),e(G,y7),e(y7,Gre),e(Gre,_Mo),e(y7,uMo),e(y7,nI),e(nI,bMo),e(y7,vMo),e(G,TMo),e(G,w7),e(w7,Ore),e(Ore,FMo),e(w7,CMo),e(w7,sI),e(sI,MMo),e(w7,EMo),e(G,yMo),e(G,A7),e(A7,Xre),e(Xre,wMo),e(A7,AMo),e(A7,lI),e(lI,LMo),e(A7,BMo),e(G,kMo),e(G,L7),e(L7,zre),e(zre,xMo),e(L7,RMo),e(L7,iI),e(iI,SMo),e(L7,PMo),e(G,$Mo),e(G,B7),e(B7,Vre),e(Vre,IMo),e(B7,jMo),e(B7,dI),e(dI,NMo),e(B7,DMo),e(G,qMo),e(G,k7),e(k7,Wre),e(Wre,GMo),e(k7,OMo),e(k7,cI),e(cI,XMo),e(k7,zMo),e(G,VMo),e(G,x7),e(x7,Qre),e(Qre,WMo),e(x7,QMo),e(x7,fI),e(fI,HMo),e(x7,UMo),e(G,JMo),e(G,R7),e(R7,Hre),e(Hre,YMo),e(R7,KMo),e(R7,mI),e(mI,ZMo),e(R7,eEo),e(G,oEo),e(G,S7),e(S7,Ure),e(Ure,rEo),e(S7,tEo),e(S7,gI),e(gI,aEo),e(S7,nEo),e(G,sEo),e(G,P7),e(P7,Jre),e(Jre,lEo),e(P7,iEo),e(P7,hI),e(hI,dEo),e(P7,cEo),e(G,fEo),e(G,$7),e($7,Yre),e(Yre,mEo),e($7,gEo),e($7,pI),e(pI,hEo),e($7,pEo),e(G,_Eo),e(G,I7),e(I7,Kre),e(Kre,uEo),e(I7,bEo),e(I7,_I),e(_I,vEo),e(I7,TEo),e(G,FEo),e(G,j7),e(j7,Zre),e(Zre,CEo),e(j7,MEo),e(j7,uI),e(uI,EEo),e(j7,yEo),e(G,wEo),e(G,N7),e(N7,ete),e(ete,AEo),e(N7,LEo),e(N7,bI),e(bI,BEo),e(N7,kEo),e(G,xEo),e(G,D7),e(D7,ote),e(ote,REo),e(D7,SEo),e(D7,vI),e(vI,PEo),e(D7,$Eo),e(G,IEo),e(G,q7),e(q7,rte),e(rte,jEo),e(q7,NEo),e(q7,TI),e(TI,DEo),e(q7,qEo),e(Ie,GEo),e(Ie,G7),e(G7,OEo),e(G7,tte),e(tte,XEo),e(G7,zEo),e(G7,ate),e(ate,VEo),e(Ie,WEo),e(Ie,nte),e(nte,QEo),e(Ie,HEo),g(B3,Ie,null),b(d,k8e,u),b(d,ld,u),e(ld,O7),e(O7,ste),g(k3,ste,null),e(ld,UEo),e(ld,lte),e(lte,JEo),b(d,x8e,u),b(d,Ko,u),g(x3,Ko,null),e(Ko,YEo),e(Ko,id),e(id,KEo),e(id,ite),e(ite,ZEo),e(id,e3o),e(id,dte),e(dte,o3o),e(id,r3o),e(Ko,t3o),e(Ko,R3),e(R3,a3o),e(R3,cte),e(cte,n3o),e(R3,s3o),e(Ko,l3o),e(Ko,Vr),g(S3,Vr,null),e(Vr,i3o),e(Vr,fte),e(fte,d3o),e(Vr,c3o),e(Vr,dd),e(dd,f3o),e(dd,mte),e(mte,m3o),e(dd,g3o),e(dd,gte),e(gte,h3o),e(dd,p3o),e(Vr,_3o),e(Vr,hte),e(hte,u3o),e(Vr,b3o),g(P3,Vr,null),e(Ko,v3o),e(Ko,je),g($3,je,null),e(je,T3o),e(je,pte),e(pte,F3o),e(je,C3o),e(je,Wa),e(Wa,M3o),e(Wa,_te),e(_te,E3o),e(Wa,y3o),e(Wa,ute),e(ute,w3o),e(Wa,A3o),e(Wa,bte),e(bte,L3o),e(Wa,B3o),e(je,k3o),e(je,na),e(na,X7),e(X7,vte),e(vte,x3o),e(X7,R3o),e(X7,FI),e(FI,S3o),e(X7,P3o),e(na,$3o),e(na,z7),e(z7,Tte),e(Tte,I3o),e(z7,j3o),e(z7,CI),e(CI,N3o),e(z7,D3o),e(na,q3o),e(na,V7),e(V7,Fte),e(Fte,G3o),e(V7,O3o),e(V7,MI),e(MI,X3o),e(V7,z3o),e(na,V3o),e(na,W7),e(W7,Cte),e(Cte,W3o),e(W7,Q3o),e(W7,EI),e(EI,H3o),e(W7,U3o),e(na,J3o),e(na,Q7),e(Q7,Mte),e(Mte,Y3o),e(Q7,K3o),e(Q7,yI),e(yI,Z3o),e(Q7,eyo),e(je,oyo),e(je,H7),e(H7,ryo),e(H7,Ete),e(Ete,tyo),e(H7,ayo),e(H7,yte),e(yte,nyo),e(je,syo),e(je,wte),e(wte,lyo),e(je,iyo),g(I3,je,null),b(d,R8e,u),b(d,cd,u),e(cd,U7),e(U7,Ate),g(j3,Ate,null),e(cd,dyo),e(cd,Lte),e(Lte,cyo),b(d,S8e,u),b(d,Zo,u),g(N3,Zo,null),e(Zo,fyo),e(Zo,fd),e(fd,myo),e(fd,Bte),e(Bte,gyo),e(fd,hyo),e(fd,kte),e(kte,pyo),e(fd,_yo),e(Zo,uyo),e(Zo,D3),e(D3,byo),e(D3,xte),e(xte,vyo),e(D3,Tyo),e(Zo,Fyo),e(Zo,Wr),g(q3,Wr,null),e(Wr,Cyo),e(Wr,Rte),e(Rte,Myo),e(Wr,Eyo),e(Wr,md),e(md,yyo),e(md,Ste),e(Ste,wyo),e(md,Ayo),e(md,Pte),e(Pte,Lyo),e(md,Byo),e(Wr,kyo),e(Wr,$te),e($te,xyo),e(Wr,Ryo),g(G3,Wr,null),e(Zo,Syo),e(Zo,Ne),g(O3,Ne,null),e(Ne,Pyo),e(Ne,Ite),e(Ite,$yo),e(Ne,Iyo),e(Ne,Qa),e(Qa,jyo),e(Qa,jte),e(jte,Nyo),e(Qa,Dyo),e(Qa,Nte),e(Nte,qyo),e(Qa,Gyo),e(Qa,Dte),e(Dte,Oyo),e(Qa,Xyo),e(Ne,zyo),e(Ne,D),e(D,J7),e(J7,qte),e(qte,Vyo),e(J7,Wyo),e(J7,wI),e(wI,Qyo),e(J7,Hyo),e(D,Uyo),e(D,Y7),e(Y7,Gte),e(Gte,Jyo),e(Y7,Yyo),e(Y7,AI),e(AI,Kyo),e(Y7,Zyo),e(D,ewo),e(D,K7),e(K7,Ote),e(Ote,owo),e(K7,rwo),e(K7,LI),e(LI,two),e(K7,awo),e(D,nwo),e(D,Z7),e(Z7,Xte),e(Xte,swo),e(Z7,lwo),e(Z7,BI),e(BI,iwo),e(Z7,dwo),e(D,cwo),e(D,e4),e(e4,zte),e(zte,fwo),e(e4,mwo),e(e4,kI),e(kI,gwo),e(e4,hwo),e(D,pwo),e(D,o4),e(o4,Vte),e(Vte,_wo),e(o4,uwo),e(o4,xI),e(xI,bwo),e(o4,vwo),e(D,Two),e(D,r4),e(r4,Wte),e(Wte,Fwo),e(r4,Cwo),e(r4,RI),e(RI,Mwo),e(r4,Ewo),e(D,ywo),e(D,t4),e(t4,Qte),e(Qte,wwo),e(t4,Awo),e(t4,SI),e(SI,Lwo),e(t4,Bwo),e(D,kwo),e(D,a4),e(a4,Hte),e(Hte,xwo),e(a4,Rwo),e(a4,PI),e(PI,Swo),e(a4,Pwo),e(D,$wo),e(D,n4),e(n4,Ute),e(Ute,Iwo),e(n4,jwo),e(n4,$I),e($I,Nwo),e(n4,Dwo),e(D,qwo),e(D,s4),e(s4,Jte),e(Jte,Gwo),e(s4,Owo),e(s4,II),e(II,Xwo),e(s4,zwo),e(D,Vwo),e(D,l4),e(l4,Yte),e(Yte,Wwo),e(l4,Qwo),e(l4,jI),e(jI,Hwo),e(l4,Uwo),e(D,Jwo),e(D,i4),e(i4,Kte),e(Kte,Ywo),e(i4,Kwo),e(i4,NI),e(NI,Zwo),e(i4,eAo),e(D,oAo),e(D,d4),e(d4,Zte),e(Zte,rAo),e(d4,tAo),e(d4,DI),e(DI,aAo),e(d4,nAo),e(D,sAo),e(D,c4),e(c4,eae),e(eae,lAo),e(c4,iAo),e(c4,qI),e(qI,dAo),e(c4,cAo),e(D,fAo),e(D,f4),e(f4,oae),e(oae,mAo),e(f4,gAo),e(f4,GI),e(GI,hAo),e(f4,pAo),e(D,_Ao),e(D,m4),e(m4,rae),e(rae,uAo),e(m4,bAo),e(m4,OI),e(OI,vAo),e(m4,TAo),e(D,FAo),e(D,g4),e(g4,tae),e(tae,CAo),e(g4,MAo),e(g4,XI),e(XI,EAo),e(g4,yAo),e(D,wAo),e(D,h4),e(h4,aae),e(aae,AAo),e(h4,LAo),e(h4,zI),e(zI,BAo),e(h4,kAo),e(D,xAo),e(D,p4),e(p4,nae),e(nae,RAo),e(p4,SAo),e(p4,VI),e(VI,PAo),e(p4,$Ao),e(D,IAo),e(D,_4),e(_4,sae),e(sae,jAo),e(_4,NAo),e(_4,WI),e(WI,DAo),e(_4,qAo),e(D,GAo),e(D,u4),e(u4,lae),e(lae,OAo),e(u4,XAo),e(u4,QI),e(QI,zAo),e(u4,VAo),e(D,WAo),e(D,b4),e(b4,iae),e(iae,QAo),e(b4,HAo),e(b4,HI),e(HI,UAo),e(b4,JAo),e(D,YAo),e(D,v4),e(v4,dae),e(dae,KAo),e(v4,ZAo),e(v4,UI),e(UI,e6o),e(v4,o6o),e(D,r6o),e(D,T4),e(T4,cae),e(cae,t6o),e(T4,a6o),e(T4,JI),e(JI,n6o),e(T4,s6o),e(D,l6o),e(D,F4),e(F4,fae),e(fae,i6o),e(F4,d6o),e(F4,YI),e(YI,c6o),e(F4,f6o),e(D,m6o),e(D,C4),e(C4,mae),e(mae,g6o),e(C4,h6o),e(C4,KI),e(KI,p6o),e(C4,_6o),e(D,u6o),e(D,M4),e(M4,gae),e(gae,b6o),e(M4,v6o),e(M4,ZI),e(ZI,T6o),e(M4,F6o),e(D,C6o),e(D,E4),e(E4,hae),e(hae,M6o),e(E4,E6o),e(E4,ej),e(ej,y6o),e(E4,w6o),e(D,A6o),e(D,y4),e(y4,pae),e(pae,L6o),e(y4,B6o),e(y4,oj),e(oj,k6o),e(y4,x6o),e(D,R6o),e(D,w4),e(w4,_ae),e(_ae,S6o),e(w4,P6o),e(w4,rj),e(rj,$6o),e(w4,I6o),e(D,j6o),e(D,A4),e(A4,uae),e(uae,N6o),e(A4,D6o),e(A4,tj),e(tj,q6o),e(A4,G6o),e(Ne,O6o),e(Ne,L4),e(L4,X6o),e(L4,bae),e(bae,z6o),e(L4,V6o),e(L4,vae),e(vae,W6o),e(Ne,Q6o),e(Ne,Tae),e(Tae,H6o),e(Ne,U6o),g(X3,Ne,null),b(d,P8e,u),b(d,gd,u),e(gd,B4),e(B4,Fae),g(z3,Fae,null),e(gd,J6o),e(gd,Cae),e(Cae,Y6o),b(d,$8e,u),b(d,er,u),g(V3,er,null),e(er,K6o),e(er,hd),e(hd,Z6o),e(hd,Mae),e(Mae,e0o),e(hd,o0o),e(hd,Eae),e(Eae,r0o),e(hd,t0o),e(er,a0o),e(er,W3),e(W3,n0o),e(W3,yae),e(yae,s0o),e(W3,l0o),e(er,i0o),e(er,Qr),g(Q3,Qr,null),e(Qr,d0o),e(Qr,wae),e(wae,c0o),e(Qr,f0o),e(Qr,pd),e(pd,m0o),e(pd,Aae),e(Aae,g0o),e(pd,h0o),e(pd,Lae),e(Lae,p0o),e(pd,_0o),e(Qr,u0o),e(Qr,Bae),e(Bae,b0o),e(Qr,v0o),g(H3,Qr,null),e(er,T0o),e(er,De),g(U3,De,null),e(De,F0o),e(De,kae),e(kae,C0o),e(De,M0o),e(De,Ha),e(Ha,E0o),e(Ha,xae),e(xae,y0o),e(Ha,w0o),e(Ha,Rae),e(Rae,A0o),e(Ha,L0o),e(Ha,Sae),e(Sae,B0o),e(Ha,k0o),e(De,x0o),e(De,R),e(R,k4),e(k4,Pae),e(Pae,R0o),e(k4,S0o),e(k4,aj),e(aj,P0o),e(k4,$0o),e(R,I0o),e(R,x4),e(x4,$ae),e($ae,j0o),e(x4,N0o),e(x4,nj),e(nj,D0o),e(x4,q0o),e(R,G0o),e(R,R4),e(R4,Iae),e(Iae,O0o),e(R4,X0o),e(R4,sj),e(sj,z0o),e(R4,V0o),e(R,W0o),e(R,S4),e(S4,jae),e(jae,Q0o),e(S4,H0o),e(S4,lj),e(lj,U0o),e(S4,J0o),e(R,Y0o),e(R,P4),e(P4,Nae),e(Nae,K0o),e(P4,Z0o),e(P4,ij),e(ij,eLo),e(P4,oLo),e(R,rLo),e(R,$4),e($4,Dae),e(Dae,tLo),e($4,aLo),e($4,dj),e(dj,nLo),e($4,sLo),e(R,lLo),e(R,I4),e(I4,qae),e(qae,iLo),e(I4,dLo),e(I4,cj),e(cj,cLo),e(I4,fLo),e(R,mLo),e(R,j4),e(j4,Gae),e(Gae,gLo),e(j4,hLo),e(j4,fj),e(fj,pLo),e(j4,_Lo),e(R,uLo),e(R,N4),e(N4,Oae),e(Oae,bLo),e(N4,vLo),e(N4,mj),e(mj,TLo),e(N4,FLo),e(R,CLo),e(R,D4),e(D4,Xae),e(Xae,MLo),e(D4,ELo),e(D4,gj),e(gj,yLo),e(D4,wLo),e(R,ALo),e(R,q4),e(q4,zae),e(zae,LLo),e(q4,BLo),e(q4,hj),e(hj,kLo),e(q4,xLo),e(R,RLo),e(R,G4),e(G4,Vae),e(Vae,SLo),e(G4,PLo),e(G4,pj),e(pj,$Lo),e(G4,ILo),e(R,jLo),e(R,O4),e(O4,Wae),e(Wae,NLo),e(O4,DLo),e(O4,_j),e(_j,qLo),e(O4,GLo),e(R,OLo),e(R,X4),e(X4,Qae),e(Qae,XLo),e(X4,zLo),e(X4,uj),e(uj,VLo),e(X4,WLo),e(R,QLo),e(R,z4),e(z4,Hae),e(Hae,HLo),e(z4,ULo),e(z4,bj),e(bj,JLo),e(z4,YLo),e(R,KLo),e(R,V4),e(V4,Uae),e(Uae,ZLo),e(V4,e8o),e(V4,vj),e(vj,o8o),e(V4,r8o),e(R,t8o),e(R,W4),e(W4,Jae),e(Jae,a8o),e(W4,n8o),e(W4,Tj),e(Tj,s8o),e(W4,l8o),e(R,i8o),e(R,Q4),e(Q4,Yae),e(Yae,d8o),e(Q4,c8o),e(Q4,Fj),e(Fj,f8o),e(Q4,m8o),e(R,g8o),e(R,H4),e(H4,Kae),e(Kae,h8o),e(H4,p8o),e(H4,Cj),e(Cj,_8o),e(H4,u8o),e(R,b8o),e(R,U4),e(U4,Zae),e(Zae,v8o),e(U4,T8o),e(U4,Mj),e(Mj,F8o),e(U4,C8o),e(R,M8o),e(R,J4),e(J4,ene),e(ene,E8o),e(J4,y8o),e(J4,Ej),e(Ej,w8o),e(J4,A8o),e(R,L8o),e(R,Y4),e(Y4,one),e(one,B8o),e(Y4,k8o),e(Y4,yj),e(yj,x8o),e(Y4,R8o),e(R,S8o),e(R,K4),e(K4,rne),e(rne,P8o),e(K4,$8o),e(K4,wj),e(wj,I8o),e(K4,j8o),e(R,N8o),e(R,Z4),e(Z4,tne),e(tne,D8o),e(Z4,q8o),e(Z4,Aj),e(Aj,G8o),e(Z4,O8o),e(R,X8o),e(R,eb),e(eb,ane),e(ane,z8o),e(eb,V8o),e(eb,Lj),e(Lj,W8o),e(eb,Q8o),e(R,H8o),e(R,ob),e(ob,nne),e(nne,U8o),e(ob,J8o),e(ob,Bj),e(Bj,Y8o),e(ob,K8o),e(R,Z8o),e(R,rb),e(rb,sne),e(sne,e9o),e(rb,o9o),e(rb,kj),e(kj,r9o),e(rb,t9o),e(R,a9o),e(R,tb),e(tb,lne),e(lne,n9o),e(tb,s9o),e(tb,xj),e(xj,l9o),e(tb,i9o),e(R,d9o),e(R,ab),e(ab,ine),e(ine,c9o),e(ab,f9o),e(ab,Rj),e(Rj,m9o),e(ab,g9o),e(R,h9o),e(R,nb),e(nb,dne),e(dne,p9o),e(nb,_9o),e(nb,Sj),e(Sj,u9o),e(nb,b9o),e(R,v9o),e(R,sb),e(sb,cne),e(cne,T9o),e(sb,F9o),e(sb,Pj),e(Pj,C9o),e(sb,M9o),e(R,E9o),e(R,lb),e(lb,fne),e(fne,y9o),e(lb,w9o),e(lb,$j),e($j,A9o),e(lb,L9o),e(R,B9o),e(R,ib),e(ib,mne),e(mne,k9o),e(ib,x9o),e(ib,Ij),e(Ij,R9o),e(ib,S9o),e(R,P9o),e(R,db),e(db,gne),e(gne,$9o),e(db,I9o),e(db,jj),e(jj,j9o),e(db,N9o),e(R,D9o),e(R,cb),e(cb,hne),e(hne,q9o),e(cb,G9o),e(cb,Nj),e(Nj,O9o),e(cb,X9o),e(R,z9o),e(R,fb),e(fb,pne),e(pne,V9o),e(fb,W9o),e(fb,Dj),e(Dj,Q9o),e(fb,H9o),e(R,U9o),e(R,mb),e(mb,_ne),e(_ne,J9o),e(mb,Y9o),e(mb,qj),e(qj,K9o),e(mb,Z9o),e(R,eBo),e(R,gb),e(gb,une),e(une,oBo),e(gb,rBo),e(gb,Gj),e(Gj,tBo),e(gb,aBo),e(De,nBo),e(De,hb),e(hb,sBo),e(hb,bne),e(bne,lBo),e(hb,iBo),e(hb,vne),e(vne,dBo),e(De,cBo),e(De,Tne),e(Tne,fBo),e(De,mBo),g(J3,De,null),b(d,I8e,u),b(d,_d,u),e(_d,pb),e(pb,Fne),g(Y3,Fne,null),e(_d,gBo),e(_d,Cne),e(Cne,hBo),b(d,j8e,u),b(d,or,u),g(K3,or,null),e(or,pBo),e(or,ud),e(ud,_Bo),e(ud,Mne),e(Mne,uBo),e(ud,bBo),e(ud,Ene),e(Ene,vBo),e(ud,TBo),e(or,FBo),e(or,Z3),e(Z3,CBo),e(Z3,yne),e(yne,MBo),e(Z3,EBo),e(or,yBo),e(or,Hr),g(ey,Hr,null),e(Hr,wBo),e(Hr,wne),e(wne,ABo),e(Hr,LBo),e(Hr,bd),e(bd,BBo),e(bd,Ane),e(Ane,kBo),e(bd,xBo),e(bd,Lne),e(Lne,RBo),e(bd,SBo),e(Hr,PBo),e(Hr,Bne),e(Bne,$Bo),e(Hr,IBo),g(oy,Hr,null),e(or,jBo),e(or,qe),g(ry,qe,null),e(qe,NBo),e(qe,kne),e(kne,DBo),e(qe,qBo),e(qe,Ua),e(Ua,GBo),e(Ua,xne),e(xne,OBo),e(Ua,XBo),e(Ua,Rne),e(Rne,zBo),e(Ua,VBo),e(Ua,Sne),e(Sne,WBo),e(Ua,QBo),e(qe,HBo),e(qe,Pne),e(Pne,_b),e(_b,$ne),e($ne,UBo),e(_b,JBo),e(_b,Oj),e(Oj,YBo),e(_b,KBo),e(qe,ZBo),e(qe,ub),e(ub,eko),e(ub,Ine),e(Ine,oko),e(ub,rko),e(ub,jne),e(jne,tko),e(qe,ako),e(qe,Nne),e(Nne,nko),e(qe,sko),g(ty,qe,null),b(d,N8e,u),b(d,vd,u),e(vd,bb),e(bb,Dne),g(ay,Dne,null),e(vd,lko),e(vd,qne),e(qne,iko),b(d,D8e,u),b(d,rr,u),g(ny,rr,null),e(rr,dko),e(rr,Td),e(Td,cko),e(Td,Gne),e(Gne,fko),e(Td,mko),e(Td,One),e(One,gko),e(Td,hko),e(rr,pko),e(rr,sy),e(sy,_ko),e(sy,Xne),e(Xne,uko),e(sy,bko),e(rr,vko),e(rr,Ur),g(ly,Ur,null),e(Ur,Tko),e(Ur,zne),e(zne,Fko),e(Ur,Cko),e(Ur,Fd),e(Fd,Mko),e(Fd,Vne),e(Vne,Eko),e(Fd,yko),e(Fd,Wne),e(Wne,wko),e(Fd,Ako),e(Ur,Lko),e(Ur,Qne),e(Qne,Bko),e(Ur,kko),g(iy,Ur,null),e(rr,xko),e(rr,Ge),g(dy,Ge,null),e(Ge,Rko),e(Ge,Hne),e(Hne,Sko),e(Ge,Pko),e(Ge,Ja),e(Ja,$ko),e(Ja,Une),e(Une,Iko),e(Ja,jko),e(Ja,Jne),e(Jne,Nko),e(Ja,Dko),e(Ja,Yne),e(Yne,qko),e(Ja,Gko),e(Ge,Oko),e(Ge,be),e(be,vb),e(vb,Kne),e(Kne,Xko),e(vb,zko),e(vb,Xj),e(Xj,Vko),e(vb,Wko),e(be,Qko),e(be,Tb),e(Tb,Zne),e(Zne,Hko),e(Tb,Uko),e(Tb,zj),e(zj,Jko),e(Tb,Yko),e(be,Kko),e(be,Rs),e(Rs,ese),e(ese,Zko),e(Rs,exo),e(Rs,Vj),e(Vj,oxo),e(Rs,rxo),e(Rs,Wj),e(Wj,txo),e(Rs,axo),e(be,nxo),e(be,Fb),e(Fb,ose),e(ose,sxo),e(Fb,lxo),e(Fb,Qj),e(Qj,ixo),e(Fb,dxo),e(be,cxo),e(be,la),e(la,rse),e(rse,fxo),e(la,mxo),e(la,Hj),e(Hj,gxo),e(la,hxo),e(la,Uj),e(Uj,pxo),e(la,_xo),e(la,Jj),e(Jj,uxo),e(la,bxo),e(be,vxo),e(be,Cb),e(Cb,tse),e(tse,Txo),e(Cb,Fxo),e(Cb,Yj),e(Yj,Cxo),e(Cb,Mxo),e(be,Exo),e(be,Mb),e(Mb,ase),e(ase,yxo),e(Mb,wxo),e(Mb,Kj),e(Kj,Axo),e(Mb,Lxo),e(be,Bxo),e(be,Eb),e(Eb,nse),e(nse,kxo),e(Eb,xxo),e(Eb,Zj),e(Zj,Rxo),e(Eb,Sxo),e(be,Pxo),e(be,yb),e(yb,sse),e(sse,$xo),e(yb,Ixo),e(yb,eN),e(eN,jxo),e(yb,Nxo),e(Ge,Dxo),e(Ge,wb),e(wb,qxo),e(wb,lse),e(lse,Gxo),e(wb,Oxo),e(wb,ise),e(ise,Xxo),e(Ge,zxo),e(Ge,dse),e(dse,Vxo),e(Ge,Wxo),g(cy,Ge,null),b(d,q8e,u),b(d,Cd,u),e(Cd,Ab),e(Ab,cse),g(fy,cse,null),e(Cd,Qxo),e(Cd,fse),e(fse,Hxo),b(d,G8e,u),b(d,tr,u),g(my,tr,null),e(tr,Uxo),e(tr,Md),e(Md,Jxo),e(Md,mse),e(mse,Yxo),e(Md,Kxo),e(Md,gse),e(gse,Zxo),e(Md,eRo),e(tr,oRo),e(tr,gy),e(gy,rRo),e(gy,hse),e(hse,tRo),e(gy,aRo),e(tr,nRo),e(tr,Jr),g(hy,Jr,null),e(Jr,sRo),e(Jr,pse),e(pse,lRo),e(Jr,iRo),e(Jr,Ed),e(Ed,dRo),e(Ed,_se),e(_se,cRo),e(Ed,fRo),e(Ed,use),e(use,mRo),e(Ed,gRo),e(Jr,hRo),e(Jr,bse),e(bse,pRo),e(Jr,_Ro),g(py,Jr,null),e(tr,uRo),e(tr,Oe),g(_y,Oe,null),e(Oe,bRo),e(Oe,vse),e(vse,vRo),e(Oe,TRo),e(Oe,Ya),e(Ya,FRo),e(Ya,Tse),e(Tse,CRo),e(Ya,MRo),e(Ya,Fse),e(Fse,ERo),e(Ya,yRo),e(Ya,Cse),e(Cse,wRo),e(Ya,ARo),e(Oe,LRo),e(Oe,Mse),e(Mse,Lb),e(Lb,Ese),e(Ese,BRo),e(Lb,kRo),e(Lb,oN),e(oN,xRo),e(Lb,RRo),e(Oe,SRo),e(Oe,Bb),e(Bb,PRo),e(Bb,yse),e(yse,$Ro),e(Bb,IRo),e(Bb,wse),e(wse,jRo),e(Oe,NRo),e(Oe,Ase),e(Ase,DRo),e(Oe,qRo),g(uy,Oe,null),b(d,O8e,u),b(d,yd,u),e(yd,kb),e(kb,Lse),g(by,Lse,null),e(yd,GRo),e(yd,Bse),e(Bse,ORo),b(d,X8e,u),b(d,ar,u),g(vy,ar,null),e(ar,XRo),e(ar,wd),e(wd,zRo),e(wd,kse),e(kse,VRo),e(wd,WRo),e(wd,xse),e(xse,QRo),e(wd,HRo),e(ar,URo),e(ar,Ty),e(Ty,JRo),e(Ty,Rse),e(Rse,YRo),e(Ty,KRo),e(ar,ZRo),e(ar,Yr),g(Fy,Yr,null),e(Yr,eSo),e(Yr,Sse),e(Sse,oSo),e(Yr,rSo),e(Yr,Ad),e(Ad,tSo),e(Ad,Pse),e(Pse,aSo),e(Ad,nSo),e(Ad,$se),e($se,sSo),e(Ad,lSo),e(Yr,iSo),e(Yr,Ise),e(Ise,dSo),e(Yr,cSo),g(Cy,Yr,null),e(ar,fSo),e(ar,Xe),g(My,Xe,null),e(Xe,mSo),e(Xe,jse),e(jse,gSo),e(Xe,hSo),e(Xe,Ka),e(Ka,pSo),e(Ka,Nse),e(Nse,_So),e(Ka,uSo),e(Ka,Dse),e(Dse,bSo),e(Ka,vSo),e(Ka,qse),e(qse,TSo),e(Ka,FSo),e(Xe,CSo),e(Xe,ao),e(ao,xb),e(xb,Gse),e(Gse,MSo),e(xb,ESo),e(xb,rN),e(rN,ySo),e(xb,wSo),e(ao,ASo),e(ao,Rb),e(Rb,Ose),e(Ose,LSo),e(Rb,BSo),e(Rb,tN),e(tN,kSo),e(Rb,xSo),e(ao,RSo),e(ao,Sb),e(Sb,Xse),e(Xse,SSo),e(Sb,PSo),e(Sb,aN),e(aN,$So),e(Sb,ISo),e(ao,jSo),e(ao,Pb),e(Pb,zse),e(zse,NSo),e(Pb,DSo),e(Pb,nN),e(nN,qSo),e(Pb,GSo),e(ao,OSo),e(ao,$b),e($b,Vse),e(Vse,XSo),e($b,zSo),e($b,sN),e(sN,VSo),e($b,WSo),e(ao,QSo),e(ao,Ib),e(Ib,Wse),e(Wse,HSo),e(Ib,USo),e(Ib,lN),e(lN,JSo),e(Ib,YSo),e(ao,KSo),e(ao,jb),e(jb,Qse),e(Qse,ZSo),e(jb,ePo),e(jb,iN),e(iN,oPo),e(jb,rPo),e(Xe,tPo),e(Xe,Nb),e(Nb,aPo),e(Nb,Hse),e(Hse,nPo),e(Nb,sPo),e(Nb,Use),e(Use,lPo),e(Xe,iPo),e(Xe,Jse),e(Jse,dPo),e(Xe,cPo),g(Ey,Xe,null),b(d,z8e,u),b(d,Ld,u),e(Ld,Db),e(Db,Yse),g(yy,Yse,null),e(Ld,fPo),e(Ld,Kse),e(Kse,mPo),b(d,V8e,u),b(d,nr,u),g(wy,nr,null),e(nr,gPo),e(nr,Bd),e(Bd,hPo),e(Bd,Zse),e(Zse,pPo),e(Bd,_Po),e(Bd,ele),e(ele,uPo),e(Bd,bPo),e(nr,vPo),e(nr,Ay),e(Ay,TPo),e(Ay,ole),e(ole,FPo),e(Ay,CPo),e(nr,MPo),e(nr,Kr),g(Ly,Kr,null),e(Kr,EPo),e(Kr,rle),e(rle,yPo),e(Kr,wPo),e(Kr,kd),e(kd,APo),e(kd,tle),e(tle,LPo),e(kd,BPo),e(kd,ale),e(ale,kPo),e(kd,xPo),e(Kr,RPo),e(Kr,nle),e(nle,SPo),e(Kr,PPo),g(By,Kr,null),e(nr,$Po),e(nr,ze),g(ky,ze,null),e(ze,IPo),e(ze,sle),e(sle,jPo),e(ze,NPo),e(ze,Za),e(Za,DPo),e(Za,lle),e(lle,qPo),e(Za,GPo),e(Za,ile),e(ile,OPo),e(Za,XPo),e(Za,dle),e(dle,zPo),e(Za,VPo),e(ze,WPo),e(ze,xd),e(xd,qb),e(qb,cle),e(cle,QPo),e(qb,HPo),e(qb,dN),e(dN,UPo),e(qb,JPo),e(xd,YPo),e(xd,Gb),e(Gb,fle),e(fle,KPo),e(Gb,ZPo),e(Gb,cN),e(cN,e$o),e(Gb,o$o),e(xd,r$o),e(xd,Ob),e(Ob,mle),e(mle,t$o),e(Ob,a$o),e(Ob,fN),e(fN,n$o),e(Ob,s$o),e(ze,l$o),e(ze,Xb),e(Xb,i$o),e(Xb,gle),e(gle,d$o),e(Xb,c$o),e(Xb,hle),e(hle,f$o),e(ze,m$o),e(ze,ple),e(ple,g$o),e(ze,h$o),g(xy,ze,null),b(d,W8e,u),b(d,Rd,u),e(Rd,zb),e(zb,_le),g(Ry,_le,null),e(Rd,p$o),e(Rd,ule),e(ule,_$o),b(d,Q8e,u),b(d,sr,u),g(Sy,sr,null),e(sr,u$o),e(sr,Sd),e(Sd,b$o),e(Sd,ble),e(ble,v$o),e(Sd,T$o),e(Sd,vle),e(vle,F$o),e(Sd,C$o),e(sr,M$o),e(sr,Py),e(Py,E$o),e(Py,Tle),e(Tle,y$o),e(Py,w$o),e(sr,A$o),e(sr,Zr),g($y,Zr,null),e(Zr,L$o),e(Zr,Fle),e(Fle,B$o),e(Zr,k$o),e(Zr,Pd),e(Pd,x$o),e(Pd,Cle),e(Cle,R$o),e(Pd,S$o),e(Pd,Mle),e(Mle,P$o),e(Pd,$$o),e(Zr,I$o),e(Zr,Ele),e(Ele,j$o),e(Zr,N$o),g(Iy,Zr,null),e(sr,D$o),e(sr,Ve),g(jy,Ve,null),e(Ve,q$o),e(Ve,yle),e(yle,G$o),e(Ve,O$o),e(Ve,en),e(en,X$o),e(en,wle),e(wle,z$o),e(en,V$o),e(en,Ale),e(Ale,W$o),e(en,Q$o),e(en,Lle),e(Lle,H$o),e(en,U$o),e(Ve,J$o),e(Ve,no),e(no,Vb),e(Vb,Ble),e(Ble,Y$o),e(Vb,K$o),e(Vb,mN),e(mN,Z$o),e(Vb,eIo),e(no,oIo),e(no,Wb),e(Wb,kle),e(kle,rIo),e(Wb,tIo),e(Wb,gN),e(gN,aIo),e(Wb,nIo),e(no,sIo),e(no,Qb),e(Qb,xle),e(xle,lIo),e(Qb,iIo),e(Qb,hN),e(hN,dIo),e(Qb,cIo),e(no,fIo),e(no,Hb),e(Hb,Rle),e(Rle,mIo),e(Hb,gIo),e(Hb,pN),e(pN,hIo),e(Hb,pIo),e(no,_Io),e(no,Ub),e(Ub,Sle),e(Sle,uIo),e(Ub,bIo),e(Ub,_N),e(_N,vIo),e(Ub,TIo),e(no,FIo),e(no,Jb),e(Jb,Ple),e(Ple,CIo),e(Jb,MIo),e(Jb,uN),e(uN,EIo),e(Jb,yIo),e(no,wIo),e(no,Yb),e(Yb,$le),e($le,AIo),e(Yb,LIo),e(Yb,bN),e(bN,BIo),e(Yb,kIo),e(Ve,xIo),e(Ve,Kb),e(Kb,RIo),e(Kb,Ile),e(Ile,SIo),e(Kb,PIo),e(Kb,jle),e(jle,$Io),e(Ve,IIo),e(Ve,Nle),e(Nle,jIo),e(Ve,NIo),g(Ny,Ve,null),b(d,H8e,u),b(d,$d,u),e($d,Zb),e(Zb,Dle),g(Dy,Dle,null),e($d,DIo),e($d,qle),e(qle,qIo),b(d,U8e,u),b(d,lr,u),g(qy,lr,null),e(lr,GIo),e(lr,Id),e(Id,OIo),e(Id,Gle),e(Gle,XIo),e(Id,zIo),e(Id,Ole),e(Ole,VIo),e(Id,WIo),e(lr,QIo),e(lr,Gy),e(Gy,HIo),e(Gy,Xle),e(Xle,UIo),e(Gy,JIo),e(lr,YIo),e(lr,et),g(Oy,et,null),e(et,KIo),e(et,zle),e(zle,ZIo),e(et,ejo),e(et,jd),e(jd,ojo),e(jd,Vle),e(Vle,rjo),e(jd,tjo),e(jd,Wle),e(Wle,ajo),e(jd,njo),e(et,sjo),e(et,Qle),e(Qle,ljo),e(et,ijo),g(Xy,et,null),e(lr,djo),e(lr,We),g(zy,We,null),e(We,cjo),e(We,Hle),e(Hle,fjo),e(We,mjo),e(We,on),e(on,gjo),e(on,Ule),e(Ule,hjo),e(on,pjo),e(on,Jle),e(Jle,_jo),e(on,ujo),e(on,Yle),e(Yle,bjo),e(on,vjo),e(We,Tjo),e(We,Vy),e(Vy,e5),e(e5,Kle),e(Kle,Fjo),e(e5,Cjo),e(e5,vN),e(vN,Mjo),e(e5,Ejo),e(Vy,yjo),e(Vy,o5),e(o5,Zle),e(Zle,wjo),e(o5,Ajo),e(o5,TN),e(TN,Ljo),e(o5,Bjo),e(We,kjo),e(We,r5),e(r5,xjo),e(r5,eie),e(eie,Rjo),e(r5,Sjo),e(r5,oie),e(oie,Pjo),e(We,$jo),e(We,rie),e(rie,Ijo),e(We,jjo),g(Wy,We,null),b(d,J8e,u),b(d,Nd,u),e(Nd,t5),e(t5,tie),g(Qy,tie,null),e(Nd,Njo),e(Nd,aie),e(aie,Djo),b(d,Y8e,u),b(d,ir,u),g(Hy,ir,null),e(ir,qjo),e(ir,Dd),e(Dd,Gjo),e(Dd,nie),e(nie,Ojo),e(Dd,Xjo),e(Dd,sie),e(sie,zjo),e(Dd,Vjo),e(ir,Wjo),e(ir,Uy),e(Uy,Qjo),e(Uy,lie),e(lie,Hjo),e(Uy,Ujo),e(ir,Jjo),e(ir,ot),g(Jy,ot,null),e(ot,Yjo),e(ot,iie),e(iie,Kjo),e(ot,Zjo),e(ot,qd),e(qd,eNo),e(qd,die),e(die,oNo),e(qd,rNo),e(qd,cie),e(cie,tNo),e(qd,aNo),e(ot,nNo),e(ot,fie),e(fie,sNo),e(ot,lNo),g(Yy,ot,null),e(ir,iNo),e(ir,Qe),g(Ky,Qe,null),e(Qe,dNo),e(Qe,mie),e(mie,cNo),e(Qe,fNo),e(Qe,rn),e(rn,mNo),e(rn,gie),e(gie,gNo),e(rn,hNo),e(rn,hie),e(hie,pNo),e(rn,_No),e(rn,pie),e(pie,uNo),e(rn,bNo),e(Qe,vNo),e(Qe,Gd),e(Gd,a5),e(a5,_ie),e(_ie,TNo),e(a5,FNo),e(a5,FN),e(FN,CNo),e(a5,MNo),e(Gd,ENo),e(Gd,n5),e(n5,uie),e(uie,yNo),e(n5,wNo),e(n5,CN),e(CN,ANo),e(n5,LNo),e(Gd,BNo),e(Gd,s5),e(s5,bie),e(bie,kNo),e(s5,xNo),e(s5,MN),e(MN,RNo),e(s5,SNo),e(Qe,PNo),e(Qe,l5),e(l5,$No),e(l5,vie),e(vie,INo),e(l5,jNo),e(l5,Tie),e(Tie,NNo),e(Qe,DNo),e(Qe,Fie),e(Fie,qNo),e(Qe,GNo),g(Zy,Qe,null),b(d,K8e,u),b(d,Od,u),e(Od,i5),e(i5,Cie),g(ew,Cie,null),e(Od,ONo),e(Od,Mie),e(Mie,XNo),b(d,Z8e,u),b(d,dr,u),g(ow,dr,null),e(dr,zNo),e(dr,Xd),e(Xd,VNo),e(Xd,Eie),e(Eie,WNo),e(Xd,QNo),e(Xd,yie),e(yie,HNo),e(Xd,UNo),e(dr,JNo),e(dr,rw),e(rw,YNo),e(rw,wie),e(wie,KNo),e(rw,ZNo),e(dr,eDo),e(dr,rt),g(tw,rt,null),e(rt,oDo),e(rt,Aie),e(Aie,rDo),e(rt,tDo),e(rt,zd),e(zd,aDo),e(zd,Lie),e(Lie,nDo),e(zd,sDo),e(zd,Bie),e(Bie,lDo),e(zd,iDo),e(rt,dDo),e(rt,kie),e(kie,cDo),e(rt,fDo),g(aw,rt,null),e(dr,mDo),e(dr,He),g(nw,He,null),e(He,gDo),e(He,xie),e(xie,hDo),e(He,pDo),e(He,tn),e(tn,_Do),e(tn,Rie),e(Rie,uDo),e(tn,bDo),e(tn,Sie),e(Sie,vDo),e(tn,TDo),e(tn,Pie),e(Pie,FDo),e(tn,CDo),e(He,MDo),e(He,Vd),e(Vd,d5),e(d5,$ie),e($ie,EDo),e(d5,yDo),e(d5,EN),e(EN,wDo),e(d5,ADo),e(Vd,LDo),e(Vd,c5),e(c5,Iie),e(Iie,BDo),e(c5,kDo),e(c5,yN),e(yN,xDo),e(c5,RDo),e(Vd,SDo),e(Vd,f5),e(f5,jie),e(jie,PDo),e(f5,$Do),e(f5,wN),e(wN,IDo),e(f5,jDo),e(He,NDo),e(He,m5),e(m5,DDo),e(m5,Nie),e(Nie,qDo),e(m5,GDo),e(m5,Die),e(Die,ODo),e(He,XDo),e(He,qie),e(qie,zDo),e(He,VDo),g(sw,He,null),b(d,e9e,u),b(d,Wd,u),e(Wd,g5),e(g5,Gie),g(lw,Gie,null),e(Wd,WDo),e(Wd,Oie),e(Oie,QDo),b(d,o9e,u),b(d,cr,u),g(iw,cr,null),e(cr,HDo),e(cr,Qd),e(Qd,UDo),e(Qd,Xie),e(Xie,JDo),e(Qd,YDo),e(Qd,zie),e(zie,KDo),e(Qd,ZDo),e(cr,eqo),e(cr,dw),e(dw,oqo),e(dw,Vie),e(Vie,rqo),e(dw,tqo),e(cr,aqo),e(cr,tt),g(cw,tt,null),e(tt,nqo),e(tt,Wie),e(Wie,sqo),e(tt,lqo),e(tt,Hd),e(Hd,iqo),e(Hd,Qie),e(Qie,dqo),e(Hd,cqo),e(Hd,Hie),e(Hie,fqo),e(Hd,mqo),e(tt,gqo),e(tt,Uie),e(Uie,hqo),e(tt,pqo),g(fw,tt,null),e(cr,_qo),e(cr,Ue),g(mw,Ue,null),e(Ue,uqo),e(Ue,Jie),e(Jie,bqo),e(Ue,vqo),e(Ue,an),e(an,Tqo),e(an,Yie),e(Yie,Fqo),e(an,Cqo),e(an,Kie),e(Kie,Mqo),e(an,Eqo),e(an,Zie),e(Zie,yqo),e(an,wqo),e(Ue,Aqo),e(Ue,ede),e(ede,h5),e(h5,ode),e(ode,Lqo),e(h5,Bqo),e(h5,AN),e(AN,kqo),e(h5,xqo),e(Ue,Rqo),e(Ue,p5),e(p5,Sqo),e(p5,rde),e(rde,Pqo),e(p5,$qo),e(p5,tde),e(tde,Iqo),e(Ue,jqo),e(Ue,ade),e(ade,Nqo),e(Ue,Dqo),g(gw,Ue,null),b(d,r9e,u),b(d,Ud,u),e(Ud,_5),e(_5,nde),g(hw,nde,null),e(Ud,qqo),e(Ud,sde),e(sde,Gqo),b(d,t9e,u),b(d,fr,u),g(pw,fr,null),e(fr,Oqo),e(fr,Jd),e(Jd,Xqo),e(Jd,lde),e(lde,zqo),e(Jd,Vqo),e(Jd,ide),e(ide,Wqo),e(Jd,Qqo),e(fr,Hqo),e(fr,_w),e(_w,Uqo),e(_w,dde),e(dde,Jqo),e(_w,Yqo),e(fr,Kqo),e(fr,at),g(uw,at,null),e(at,Zqo),e(at,cde),e(cde,eGo),e(at,oGo),e(at,Yd),e(Yd,rGo),e(Yd,fde),e(fde,tGo),e(Yd,aGo),e(Yd,mde),e(mde,nGo),e(Yd,sGo),e(at,lGo),e(at,gde),e(gde,iGo),e(at,dGo),g(bw,at,null),e(fr,cGo),e(fr,Je),g(vw,Je,null),e(Je,fGo),e(Je,hde),e(hde,mGo),e(Je,gGo),e(Je,nn),e(nn,hGo),e(nn,pde),e(pde,pGo),e(nn,_Go),e(nn,_de),e(_de,uGo),e(nn,bGo),e(nn,ude),e(ude,vGo),e(nn,TGo),e(Je,FGo),e(Je,bde),e(bde,u5),e(u5,vde),e(vde,CGo),e(u5,MGo),e(u5,LN),e(LN,EGo),e(u5,yGo),e(Je,wGo),e(Je,b5),e(b5,AGo),e(b5,Tde),e(Tde,LGo),e(b5,BGo),e(b5,Fde),e(Fde,kGo),e(Je,xGo),e(Je,Cde),e(Cde,RGo),e(Je,SGo),g(Tw,Je,null),b(d,a9e,u),b(d,Kd,u),e(Kd,v5),e(v5,Mde),g(Fw,Mde,null),e(Kd,PGo),e(Kd,Ede),e(Ede,$Go),b(d,n9e,u),b(d,mr,u),g(Cw,mr,null),e(mr,IGo),e(mr,Zd),e(Zd,jGo),e(Zd,yde),e(yde,NGo),e(Zd,DGo),e(Zd,wde),e(wde,qGo),e(Zd,GGo),e(mr,OGo),e(mr,Mw),e(Mw,XGo),e(Mw,Ade),e(Ade,zGo),e(Mw,VGo),e(mr,WGo),e(mr,nt),g(Ew,nt,null),e(nt,QGo),e(nt,Lde),e(Lde,HGo),e(nt,UGo),e(nt,ec),e(ec,JGo),e(ec,Bde),e(Bde,YGo),e(ec,KGo),e(ec,kde),e(kde,ZGo),e(ec,eOo),e(nt,oOo),e(nt,xde),e(xde,rOo),e(nt,tOo),g(yw,nt,null),e(mr,aOo),e(mr,Ye),g(ww,Ye,null),e(Ye,nOo),e(Ye,Rde),e(Rde,sOo),e(Ye,lOo),e(Ye,sn),e(sn,iOo),e(sn,Sde),e(Sde,dOo),e(sn,cOo),e(sn,Pde),e(Pde,fOo),e(sn,mOo),e(sn,$de),e($de,gOo),e(sn,hOo),e(Ye,pOo),e(Ye,Aw),e(Aw,T5),e(T5,Ide),e(Ide,_Oo),e(T5,uOo),e(T5,BN),e(BN,bOo),e(T5,vOo),e(Aw,TOo),e(Aw,F5),e(F5,jde),e(jde,FOo),e(F5,COo),e(F5,kN),e(kN,MOo),e(F5,EOo),e(Ye,yOo),e(Ye,C5),e(C5,wOo),e(C5,Nde),e(Nde,AOo),e(C5,LOo),e(C5,Dde),e(Dde,BOo),e(Ye,kOo),e(Ye,qde),e(qde,xOo),e(Ye,ROo),g(Lw,Ye,null),b(d,s9e,u),b(d,oc,u),e(oc,M5),e(M5,Gde),g(Bw,Gde,null),e(oc,SOo),e(oc,Ode),e(Ode,POo),b(d,l9e,u),b(d,gr,u),g(kw,gr,null),e(gr,$Oo),e(gr,rc),e(rc,IOo),e(rc,Xde),e(Xde,jOo),e(rc,NOo),e(rc,zde),e(zde,DOo),e(rc,qOo),e(gr,GOo),e(gr,xw),e(xw,OOo),e(xw,Vde),e(Vde,XOo),e(xw,zOo),e(gr,VOo),e(gr,st),g(Rw,st,null),e(st,WOo),e(st,Wde),e(Wde,QOo),e(st,HOo),e(st,tc),e(tc,UOo),e(tc,Qde),e(Qde,JOo),e(tc,YOo),e(tc,Hde),e(Hde,KOo),e(tc,ZOo),e(st,eXo),e(st,Ude),e(Ude,oXo),e(st,rXo),g(Sw,st,null),e(gr,tXo),e(gr,go),g(Pw,go,null),e(go,aXo),e(go,Jde),e(Jde,nXo),e(go,sXo),e(go,ln),e(ln,lXo),e(ln,Yde),e(Yde,iXo),e(ln,dXo),e(ln,Kde),e(Kde,cXo),e(ln,fXo),e(ln,Zde),e(Zde,mXo),e(ln,gXo),e(go,hXo),e(go,B),e(B,E5),e(E5,ece),e(ece,pXo),e(E5,_Xo),e(E5,xN),e(xN,uXo),e(E5,bXo),e(B,vXo),e(B,y5),e(y5,oce),e(oce,TXo),e(y5,FXo),e(y5,RN),e(RN,CXo),e(y5,MXo),e(B,EXo),e(B,w5),e(w5,rce),e(rce,yXo),e(w5,wXo),e(w5,SN),e(SN,AXo),e(w5,LXo),e(B,BXo),e(B,A5),e(A5,tce),e(tce,kXo),e(A5,xXo),e(A5,PN),e(PN,RXo),e(A5,SXo),e(B,PXo),e(B,L5),e(L5,ace),e(ace,$Xo),e(L5,IXo),e(L5,$N),e($N,jXo),e(L5,NXo),e(B,DXo),e(B,B5),e(B5,nce),e(nce,qXo),e(B5,GXo),e(B5,IN),e(IN,OXo),e(B5,XXo),e(B,zXo),e(B,k5),e(k5,sce),e(sce,VXo),e(k5,WXo),e(k5,jN),e(jN,QXo),e(k5,HXo),e(B,UXo),e(B,x5),e(x5,lce),e(lce,JXo),e(x5,YXo),e(x5,NN),e(NN,KXo),e(x5,ZXo),e(B,ezo),e(B,R5),e(R5,ice),e(ice,ozo),e(R5,rzo),e(R5,DN),e(DN,tzo),e(R5,azo),e(B,nzo),e(B,S5),e(S5,dce),e(dce,szo),e(S5,lzo),e(S5,qN),e(qN,izo),e(S5,dzo),e(B,czo),e(B,P5),e(P5,cce),e(cce,fzo),e(P5,mzo),e(P5,GN),e(GN,gzo),e(P5,hzo),e(B,pzo),e(B,$5),e($5,fce),e(fce,_zo),e($5,uzo),e($5,ON),e(ON,bzo),e($5,vzo),e(B,Tzo),e(B,I5),e(I5,mce),e(mce,Fzo),e(I5,Czo),e(I5,XN),e(XN,Mzo),e(I5,Ezo),e(B,yzo),e(B,j5),e(j5,gce),e(gce,wzo),e(j5,Azo),e(j5,zN),e(zN,Lzo),e(j5,Bzo),e(B,kzo),e(B,N5),e(N5,hce),e(hce,xzo),e(N5,Rzo),e(N5,VN),e(VN,Szo),e(N5,Pzo),e(B,$zo),e(B,Ss),e(Ss,pce),e(pce,Izo),e(Ss,jzo),e(Ss,WN),e(WN,Nzo),e(Ss,Dzo),e(Ss,QN),e(QN,qzo),e(Ss,Gzo),e(B,Ozo),e(B,D5),e(D5,_ce),e(_ce,Xzo),e(D5,zzo),e(D5,HN),e(HN,Vzo),e(D5,Wzo),e(B,Qzo),e(B,q5),e(q5,uce),e(uce,Hzo),e(q5,Uzo),e(q5,UN),e(UN,Jzo),e(q5,Yzo),e(B,Kzo),e(B,G5),e(G5,bce),e(bce,Zzo),e(G5,eVo),e(G5,JN),e(JN,oVo),e(G5,rVo),e(B,tVo),e(B,O5),e(O5,vce),e(vce,aVo),e(O5,nVo),e(O5,YN),e(YN,sVo),e(O5,lVo),e(B,iVo),e(B,X5),e(X5,Tce),e(Tce,dVo),e(X5,cVo),e(X5,KN),e(KN,fVo),e(X5,mVo),e(B,gVo),e(B,z5),e(z5,Fce),e(Fce,hVo),e(z5,pVo),e(z5,ZN),e(ZN,_Vo),e(z5,uVo),e(B,bVo),e(B,V5),e(V5,Cce),e(Cce,vVo),e(V5,TVo),e(V5,eD),e(eD,FVo),e(V5,CVo),e(B,MVo),e(B,W5),e(W5,Mce),e(Mce,EVo),e(W5,yVo),e(W5,oD),e(oD,wVo),e(W5,AVo),e(B,LVo),e(B,Q5),e(Q5,Ece),e(Ece,BVo),e(Q5,kVo),e(Q5,rD),e(rD,xVo),e(Q5,RVo),e(B,SVo),e(B,H5),e(H5,yce),e(yce,PVo),e(H5,$Vo),e(H5,tD),e(tD,IVo),e(H5,jVo),e(B,NVo),e(B,U5),e(U5,wce),e(wce,DVo),e(U5,qVo),e(U5,aD),e(aD,GVo),e(U5,OVo),e(B,XVo),e(B,J5),e(J5,Ace),e(Ace,zVo),e(J5,VVo),e(J5,nD),e(nD,WVo),e(J5,QVo),e(B,HVo),e(B,Y5),e(Y5,Lce),e(Lce,UVo),e(Y5,JVo),e(Y5,sD),e(sD,YVo),e(Y5,KVo),e(B,ZVo),e(B,K5),e(K5,Bce),e(Bce,eWo),e(K5,oWo),e(K5,lD),e(lD,rWo),e(K5,tWo),e(B,aWo),e(B,Z5),e(Z5,kce),e(kce,nWo),e(Z5,sWo),e(Z5,iD),e(iD,lWo),e(Z5,iWo),e(B,dWo),e(B,e2),e(e2,xce),e(xce,cWo),e(e2,fWo),e(e2,dD),e(dD,mWo),e(e2,gWo),e(B,hWo),e(B,o2),e(o2,Rce),e(Rce,pWo),e(o2,_Wo),e(o2,cD),e(cD,uWo),e(o2,bWo),e(B,vWo),e(B,r2),e(r2,Sce),e(Sce,TWo),e(r2,FWo),e(r2,fD),e(fD,CWo),e(r2,MWo),e(B,EWo),e(B,t2),e(t2,Pce),e(Pce,yWo),e(t2,wWo),e(t2,mD),e(mD,AWo),e(t2,LWo),e(B,BWo),e(B,a2),e(a2,$ce),e($ce,kWo),e(a2,xWo),e(a2,gD),e(gD,RWo),e(a2,SWo),e(B,PWo),e(B,n2),e(n2,Ice),e(Ice,$Wo),e(n2,IWo),e(n2,hD),e(hD,jWo),e(n2,NWo),e(B,DWo),e(B,s2),e(s2,jce),e(jce,qWo),e(s2,GWo),e(s2,pD),e(pD,OWo),e(s2,XWo),e(B,zWo),e(B,l2),e(l2,Nce),e(Nce,VWo),e(l2,WWo),e(l2,_D),e(_D,QWo),e(l2,HWo),e(B,UWo),e(B,i2),e(i2,Dce),e(Dce,JWo),e(i2,YWo),e(i2,uD),e(uD,KWo),e(i2,ZWo),e(B,eQo),e(B,d2),e(d2,qce),e(qce,oQo),e(d2,rQo),e(d2,bD),e(bD,tQo),e(d2,aQo),e(go,nQo),e(go,Gce),e(Gce,sQo),e(go,lQo),g($w,go,null),b(d,i9e,u),b(d,ac,u),e(ac,c2),e(c2,Oce),g(Iw,Oce,null),e(ac,iQo),e(ac,Xce),e(Xce,dQo),b(d,d9e,u),b(d,hr,u),g(jw,hr,null),e(hr,cQo),e(hr,nc),e(nc,fQo),e(nc,zce),e(zce,mQo),e(nc,gQo),e(nc,Vce),e(Vce,hQo),e(nc,pQo),e(hr,_Qo),e(hr,Nw),e(Nw,uQo),e(Nw,Wce),e(Wce,bQo),e(Nw,vQo),e(hr,TQo),e(hr,lt),g(Dw,lt,null),e(lt,FQo),e(lt,Qce),e(Qce,CQo),e(lt,MQo),e(lt,sc),e(sc,EQo),e(sc,Hce),e(Hce,yQo),e(sc,wQo),e(sc,Uce),e(Uce,AQo),e(sc,LQo),e(lt,BQo),e(lt,Jce),e(Jce,kQo),e(lt,xQo),g(qw,lt,null),e(hr,RQo),e(hr,ho),g(Gw,ho,null),e(ho,SQo),e(ho,Yce),e(Yce,PQo),e(ho,$Qo),e(ho,dn),e(dn,IQo),e(dn,Kce),e(Kce,jQo),e(dn,NQo),e(dn,Zce),e(Zce,DQo),e(dn,qQo),e(dn,efe),e(efe,GQo),e(dn,OQo),e(ho,XQo),e(ho,H),e(H,f2),e(f2,ofe),e(ofe,zQo),e(f2,VQo),e(f2,vD),e(vD,WQo),e(f2,QQo),e(H,HQo),e(H,m2),e(m2,rfe),e(rfe,UQo),e(m2,JQo),e(m2,TD),e(TD,YQo),e(m2,KQo),e(H,ZQo),e(H,g2),e(g2,tfe),e(tfe,eHo),e(g2,oHo),e(g2,FD),e(FD,rHo),e(g2,tHo),e(H,aHo),e(H,h2),e(h2,afe),e(afe,nHo),e(h2,sHo),e(h2,CD),e(CD,lHo),e(h2,iHo),e(H,dHo),e(H,p2),e(p2,nfe),e(nfe,cHo),e(p2,fHo),e(p2,MD),e(MD,mHo),e(p2,gHo),e(H,hHo),e(H,_2),e(_2,sfe),e(sfe,pHo),e(_2,_Ho),e(_2,ED),e(ED,uHo),e(_2,bHo),e(H,vHo),e(H,u2),e(u2,lfe),e(lfe,THo),e(u2,FHo),e(u2,yD),e(yD,CHo),e(u2,MHo),e(H,EHo),e(H,b2),e(b2,ife),e(ife,yHo),e(b2,wHo),e(b2,wD),e(wD,AHo),e(b2,LHo),e(H,BHo),e(H,v2),e(v2,dfe),e(dfe,kHo),e(v2,xHo),e(v2,AD),e(AD,RHo),e(v2,SHo),e(H,PHo),e(H,T2),e(T2,cfe),e(cfe,$Ho),e(T2,IHo),e(T2,LD),e(LD,jHo),e(T2,NHo),e(H,DHo),e(H,F2),e(F2,ffe),e(ffe,qHo),e(F2,GHo),e(F2,BD),e(BD,OHo),e(F2,XHo),e(H,zHo),e(H,C2),e(C2,mfe),e(mfe,VHo),e(C2,WHo),e(C2,kD),e(kD,QHo),e(C2,HHo),e(H,UHo),e(H,M2),e(M2,gfe),e(gfe,JHo),e(M2,YHo),e(M2,xD),e(xD,KHo),e(M2,ZHo),e(H,eUo),e(H,E2),e(E2,hfe),e(hfe,oUo),e(E2,rUo),e(E2,RD),e(RD,tUo),e(E2,aUo),e(H,nUo),e(H,y2),e(y2,pfe),e(pfe,sUo),e(y2,lUo),e(y2,SD),e(SD,iUo),e(y2,dUo),e(H,cUo),e(H,w2),e(w2,_fe),e(_fe,fUo),e(w2,mUo),e(w2,PD),e(PD,gUo),e(w2,hUo),e(H,pUo),e(H,A2),e(A2,ufe),e(ufe,_Uo),e(A2,uUo),e(A2,$D),e($D,bUo),e(A2,vUo),e(H,TUo),e(H,L2),e(L2,bfe),e(bfe,FUo),e(L2,CUo),e(L2,ID),e(ID,MUo),e(L2,EUo),e(H,yUo),e(H,B2),e(B2,vfe),e(vfe,wUo),e(B2,AUo),e(B2,jD),e(jD,LUo),e(B2,BUo),e(H,kUo),e(H,k2),e(k2,Tfe),e(Tfe,xUo),e(k2,RUo),e(k2,ND),e(ND,SUo),e(k2,PUo),e(H,$Uo),e(H,x2),e(x2,Ffe),e(Ffe,IUo),e(x2,jUo),e(x2,DD),e(DD,NUo),e(x2,DUo),e(H,qUo),e(H,R2),e(R2,Cfe),e(Cfe,GUo),e(R2,OUo),e(R2,qD),e(qD,XUo),e(R2,zUo),e(ho,VUo),e(ho,Mfe),e(Mfe,WUo),e(ho,QUo),g(Ow,ho,null),b(d,c9e,u),b(d,lc,u),e(lc,S2),e(S2,Efe),g(Xw,Efe,null),e(lc,HUo),e(lc,yfe),e(yfe,UUo),b(d,f9e,u),b(d,pr,u),g(zw,pr,null),e(pr,JUo),e(pr,ic),e(ic,YUo),e(ic,wfe),e(wfe,KUo),e(ic,ZUo),e(ic,Afe),e(Afe,eJo),e(ic,oJo),e(pr,rJo),e(pr,Vw),e(Vw,tJo),e(Vw,Lfe),e(Lfe,aJo),e(Vw,nJo),e(pr,sJo),e(pr,it),g(Ww,it,null),e(it,lJo),e(it,Bfe),e(Bfe,iJo),e(it,dJo),e(it,dc),e(dc,cJo),e(dc,kfe),e(kfe,fJo),e(dc,mJo),e(dc,xfe),e(xfe,gJo),e(dc,hJo),e(it,pJo),e(it,Rfe),e(Rfe,_Jo),e(it,uJo),g(Qw,it,null),e(pr,bJo),e(pr,po),g(Hw,po,null),e(po,vJo),e(po,Sfe),e(Sfe,TJo),e(po,FJo),e(po,cn),e(cn,CJo),e(cn,Pfe),e(Pfe,MJo),e(cn,EJo),e(cn,$fe),e($fe,yJo),e(cn,wJo),e(cn,Ife),e(Ife,AJo),e(cn,LJo),e(po,BJo),e(po,he),e(he,P2),e(P2,jfe),e(jfe,kJo),e(P2,xJo),e(P2,GD),e(GD,RJo),e(P2,SJo),e(he,PJo),e(he,$2),e($2,Nfe),e(Nfe,$Jo),e($2,IJo),e($2,OD),e(OD,jJo),e($2,NJo),e(he,DJo),e(he,I2),e(I2,Dfe),e(Dfe,qJo),e(I2,GJo),e(I2,XD),e(XD,OJo),e(I2,XJo),e(he,zJo),e(he,j2),e(j2,qfe),e(qfe,VJo),e(j2,WJo),e(j2,zD),e(zD,QJo),e(j2,HJo),e(he,UJo),e(he,N2),e(N2,Gfe),e(Gfe,JJo),e(N2,YJo),e(N2,VD),e(VD,KJo),e(N2,ZJo),e(he,eYo),e(he,D2),e(D2,Ofe),e(Ofe,oYo),e(D2,rYo),e(D2,WD),e(WD,tYo),e(D2,aYo),e(he,nYo),e(he,q2),e(q2,Xfe),e(Xfe,sYo),e(q2,lYo),e(q2,QD),e(QD,iYo),e(q2,dYo),e(he,cYo),e(he,G2),e(G2,zfe),e(zfe,fYo),e(G2,mYo),e(G2,HD),e(HD,gYo),e(G2,hYo),e(he,pYo),e(he,O2),e(O2,Vfe),e(Vfe,_Yo),e(O2,uYo),e(O2,UD),e(UD,bYo),e(O2,vYo),e(he,TYo),e(he,X2),e(X2,Wfe),e(Wfe,FYo),e(X2,CYo),e(X2,JD),e(JD,MYo),e(X2,EYo),e(po,yYo),e(po,Qfe),e(Qfe,wYo),e(po,AYo),g(Uw,po,null),b(d,m9e,u),b(d,cc,u),e(cc,z2),e(z2,Hfe),g(Jw,Hfe,null),e(cc,LYo),e(cc,Ufe),e(Ufe,BYo),b(d,g9e,u),b(d,_r,u),g(Yw,_r,null),e(_r,kYo),e(_r,fc),e(fc,xYo),e(fc,Jfe),e(Jfe,RYo),e(fc,SYo),e(fc,Yfe),e(Yfe,PYo),e(fc,$Yo),e(_r,IYo),e(_r,Kw),e(Kw,jYo),e(Kw,Kfe),e(Kfe,NYo),e(Kw,DYo),e(_r,qYo),e(_r,dt),g(Zw,dt,null),e(dt,GYo),e(dt,Zfe),e(Zfe,OYo),e(dt,XYo),e(dt,mc),e(mc,zYo),e(mc,eme),e(eme,VYo),e(mc,WYo),e(mc,ome),e(ome,QYo),e(mc,HYo),e(dt,UYo),e(dt,rme),e(rme,JYo),e(dt,YYo),g(eA,dt,null),e(_r,KYo),e(_r,_o),g(oA,_o,null),e(_o,ZYo),e(_o,tme),e(tme,eKo),e(_o,oKo),e(_o,fn),e(fn,rKo),e(fn,ame),e(ame,tKo),e(fn,aKo),e(fn,nme),e(nme,nKo),e(fn,sKo),e(fn,sme),e(sme,lKo),e(fn,iKo),e(_o,dKo),e(_o,lme),e(lme,V2),e(V2,ime),e(ime,cKo),e(V2,fKo),e(V2,YD),e(YD,mKo),e(V2,gKo),e(_o,hKo),e(_o,dme),e(dme,pKo),e(_o,_Ko),g(rA,_o,null),b(d,h9e,u),b(d,gc,u),e(gc,W2),e(W2,cme),g(tA,cme,null),e(gc,uKo),e(gc,fme),e(fme,bKo),b(d,p9e,u),b(d,ur,u),g(aA,ur,null),e(ur,vKo),e(ur,hc),e(hc,TKo),e(hc,mme),e(mme,FKo),e(hc,CKo),e(hc,gme),e(gme,MKo),e(hc,EKo),e(ur,yKo),e(ur,nA),e(nA,wKo),e(nA,hme),e(hme,AKo),e(nA,LKo),e(ur,BKo),e(ur,ct),g(sA,ct,null),e(ct,kKo),e(ct,pme),e(pme,xKo),e(ct,RKo),e(ct,pc),e(pc,SKo),e(pc,_me),e(_me,PKo),e(pc,$Ko),e(pc,ume),e(ume,IKo),e(pc,jKo),e(ct,NKo),e(ct,bme),e(bme,DKo),e(ct,qKo),g(lA,ct,null),e(ur,GKo),e(ur,uo),g(iA,uo,null),e(uo,OKo),e(uo,vme),e(vme,XKo),e(uo,zKo),e(uo,mn),e(mn,VKo),e(mn,Tme),e(Tme,WKo),e(mn,QKo),e(mn,Fme),e(Fme,HKo),e(mn,UKo),e(mn,Cme),e(Cme,JKo),e(mn,YKo),e(uo,KKo),e(uo,Y),e(Y,Q2),e(Q2,Mme),e(Mme,ZKo),e(Q2,eZo),e(Q2,KD),e(KD,oZo),e(Q2,rZo),e(Y,tZo),e(Y,H2),e(H2,Eme),e(Eme,aZo),e(H2,nZo),e(H2,ZD),e(ZD,sZo),e(H2,lZo),e(Y,iZo),e(Y,U2),e(U2,yme),e(yme,dZo),e(U2,cZo),e(U2,eq),e(eq,fZo),e(U2,mZo),e(Y,gZo),e(Y,J2),e(J2,wme),e(wme,hZo),e(J2,pZo),e(J2,oq),e(oq,_Zo),e(J2,uZo),e(Y,bZo),e(Y,Y2),e(Y2,Ame),e(Ame,vZo),e(Y2,TZo),e(Y2,rq),e(rq,FZo),e(Y2,CZo),e(Y,MZo),e(Y,K2),e(K2,Lme),e(Lme,EZo),e(K2,yZo),e(K2,tq),e(tq,wZo),e(K2,AZo),e(Y,LZo),e(Y,Z2),e(Z2,Bme),e(Bme,BZo),e(Z2,kZo),e(Z2,aq),e(aq,xZo),e(Z2,RZo),e(Y,SZo),e(Y,ev),e(ev,kme),e(kme,PZo),e(ev,$Zo),e(ev,nq),e(nq,IZo),e(ev,jZo),e(Y,NZo),e(Y,ov),e(ov,xme),e(xme,DZo),e(ov,qZo),e(ov,sq),e(sq,GZo),e(ov,OZo),e(Y,XZo),e(Y,rv),e(rv,Rme),e(Rme,zZo),e(rv,VZo),e(rv,lq),e(lq,WZo),e(rv,QZo),e(Y,HZo),e(Y,tv),e(tv,Sme),e(Sme,UZo),e(tv,JZo),e(tv,iq),e(iq,YZo),e(tv,KZo),e(Y,ZZo),e(Y,av),e(av,Pme),e(Pme,eer),e(av,oer),e(av,dq),e(dq,rer),e(av,ter),e(Y,aer),e(Y,nv),e(nv,$me),e($me,ner),e(nv,ser),e(nv,cq),e(cq,ler),e(nv,ier),e(Y,der),e(Y,sv),e(sv,Ime),e(Ime,cer),e(sv,fer),e(sv,fq),e(fq,mer),e(sv,ger),e(Y,her),e(Y,lv),e(lv,jme),e(jme,per),e(lv,_er),e(lv,mq),e(mq,uer),e(lv,ber),e(Y,ver),e(Y,iv),e(iv,Nme),e(Nme,Ter),e(iv,Fer),e(iv,gq),e(gq,Cer),e(iv,Mer),e(Y,Eer),e(Y,dv),e(dv,Dme),e(Dme,yer),e(dv,wer),e(dv,hq),e(hq,Aer),e(dv,Ler),e(Y,Ber),e(Y,cv),e(cv,qme),e(qme,ker),e(cv,xer),e(cv,pq),e(pq,Rer),e(cv,Ser),e(Y,Per),e(Y,fv),e(fv,Gme),e(Gme,$er),e(fv,Ier),e(fv,_q),e(_q,jer),e(fv,Ner),e(Y,Der),e(Y,mv),e(mv,Ome),e(Ome,qer),e(mv,Ger),e(mv,uq),e(uq,Oer),e(mv,Xer),e(uo,zer),e(uo,Xme),e(Xme,Ver),e(uo,Wer),g(dA,uo,null),b(d,_9e,u),b(d,_c,u),e(_c,gv),e(gv,zme),g(cA,zme,null),e(_c,Qer),e(_c,Vme),e(Vme,Her),b(d,u9e,u),b(d,br,u),g(fA,br,null),e(br,Uer),e(br,uc),e(uc,Jer),e(uc,Wme),e(Wme,Yer),e(uc,Ker),e(uc,Qme),e(Qme,Zer),e(uc,eor),e(br,oor),e(br,mA),e(mA,ror),e(mA,Hme),e(Hme,tor),e(mA,aor),e(br,nor),e(br,ft),g(gA,ft,null),e(ft,sor),e(ft,Ume),e(Ume,lor),e(ft,ior),e(ft,bc),e(bc,dor),e(bc,Jme),e(Jme,cor),e(bc,mor),e(bc,Yme),e(Yme,gor),e(bc,hor),e(ft,por),e(ft,Kme),e(Kme,_or),e(ft,uor),g(hA,ft,null),e(br,bor),e(br,bo),g(pA,bo,null),e(bo,vor),e(bo,Zme),e(Zme,Tor),e(bo,For),e(bo,gn),e(gn,Cor),e(gn,ege),e(ege,Mor),e(gn,Eor),e(gn,oge),e(oge,yor),e(gn,wor),e(gn,rge),e(rge,Aor),e(gn,Lor),e(bo,Bor),e(bo,pe),e(pe,hv),e(hv,tge),e(tge,kor),e(hv,xor),e(hv,bq),e(bq,Ror),e(hv,Sor),e(pe,Por),e(pe,pv),e(pv,age),e(age,$or),e(pv,Ior),e(pv,vq),e(vq,jor),e(pv,Nor),e(pe,Dor),e(pe,_v),e(_v,nge),e(nge,qor),e(_v,Gor),e(_v,Tq),e(Tq,Oor),e(_v,Xor),e(pe,zor),e(pe,uv),e(uv,sge),e(sge,Vor),e(uv,Wor),e(uv,Fq),e(Fq,Qor),e(uv,Hor),e(pe,Uor),e(pe,bv),e(bv,lge),e(lge,Jor),e(bv,Yor),e(bv,Cq),e(Cq,Kor),e(bv,Zor),e(pe,err),e(pe,vv),e(vv,ige),e(ige,orr),e(vv,rrr),e(vv,Mq),e(Mq,trr),e(vv,arr),e(pe,nrr),e(pe,Tv),e(Tv,dge),e(dge,srr),e(Tv,lrr),e(Tv,Eq),e(Eq,irr),e(Tv,drr),e(pe,crr),e(pe,Fv),e(Fv,cge),e(cge,frr),e(Fv,mrr),e(Fv,yq),e(yq,grr),e(Fv,hrr),e(pe,prr),e(pe,Cv),e(Cv,fge),e(fge,_rr),e(Cv,urr),e(Cv,wq),e(wq,brr),e(Cv,vrr),e(pe,Trr),e(pe,Mv),e(Mv,mge),e(mge,Frr),e(Mv,Crr),e(Mv,Aq),e(Aq,Mrr),e(Mv,Err),e(bo,yrr),e(bo,gge),e(gge,wrr),e(bo,Arr),g(_A,bo,null),b(d,b9e,u),b(d,vc,u),e(vc,Ev),e(Ev,hge),g(uA,hge,null),e(vc,Lrr),e(vc,pge),e(pge,Brr),b(d,v9e,u),b(d,vr,u),g(bA,vr,null),e(vr,krr),e(vr,Tc),e(Tc,xrr),e(Tc,_ge),e(_ge,Rrr),e(Tc,Srr),e(Tc,uge),e(uge,Prr),e(Tc,$rr),e(vr,Irr),e(vr,vA),e(vA,jrr),e(vA,bge),e(bge,Nrr),e(vA,Drr),e(vr,qrr),e(vr,mt),g(TA,mt,null),e(mt,Grr),e(mt,vge),e(vge,Orr),e(mt,Xrr),e(mt,Fc),e(Fc,zrr),e(Fc,Tge),e(Tge,Vrr),e(Fc,Wrr),e(Fc,Fge),e(Fge,Qrr),e(Fc,Hrr),e(mt,Urr),e(mt,Cge),e(Cge,Jrr),e(mt,Yrr),g(FA,mt,null),e(vr,Krr),e(vr,vo),g(CA,vo,null),e(vo,Zrr),e(vo,Mge),e(Mge,etr),e(vo,otr),e(vo,hn),e(hn,rtr),e(hn,Ege),e(Ege,ttr),e(hn,atr),e(hn,yge),e(yge,ntr),e(hn,str),e(hn,wge),e(wge,ltr),e(hn,itr),e(vo,dtr),e(vo,X),e(X,yv),e(yv,Age),e(Age,ctr),e(yv,ftr),e(yv,Lq),e(Lq,mtr),e(yv,gtr),e(X,htr),e(X,wv),e(wv,Lge),e(Lge,ptr),e(wv,_tr),e(wv,Bq),e(Bq,utr),e(wv,btr),e(X,vtr),e(X,Av),e(Av,Bge),e(Bge,Ttr),e(Av,Ftr),e(Av,kq),e(kq,Ctr),e(Av,Mtr),e(X,Etr),e(X,Lv),e(Lv,kge),e(kge,ytr),e(Lv,wtr),e(Lv,xq),e(xq,Atr),e(Lv,Ltr),e(X,Btr),e(X,Bv),e(Bv,xge),e(xge,ktr),e(Bv,xtr),e(Bv,Rq),e(Rq,Rtr),e(Bv,Str),e(X,Ptr),e(X,kv),e(kv,Rge),e(Rge,$tr),e(kv,Itr),e(kv,Sq),e(Sq,jtr),e(kv,Ntr),e(X,Dtr),e(X,xv),e(xv,Sge),e(Sge,qtr),e(xv,Gtr),e(xv,Pq),e(Pq,Otr),e(xv,Xtr),e(X,ztr),e(X,Rv),e(Rv,Pge),e(Pge,Vtr),e(Rv,Wtr),e(Rv,$q),e($q,Qtr),e(Rv,Htr),e(X,Utr),e(X,Sv),e(Sv,$ge),e($ge,Jtr),e(Sv,Ytr),e(Sv,Iq),e(Iq,Ktr),e(Sv,Ztr),e(X,ear),e(X,Pv),e(Pv,Ige),e(Ige,oar),e(Pv,rar),e(Pv,jq),e(jq,tar),e(Pv,aar),e(X,nar),e(X,$v),e($v,jge),e(jge,sar),e($v,lar),e($v,Nq),e(Nq,iar),e($v,dar),e(X,car),e(X,Iv),e(Iv,Nge),e(Nge,far),e(Iv,mar),e(Iv,Dq),e(Dq,gar),e(Iv,har),e(X,par),e(X,jv),e(jv,Dge),e(Dge,_ar),e(jv,uar),e(jv,qq),e(qq,bar),e(jv,Tar),e(X,Far),e(X,Nv),e(Nv,qge),e(qge,Car),e(Nv,Mar),e(Nv,Gq),e(Gq,Ear),e(Nv,yar),e(X,war),e(X,Dv),e(Dv,Gge),e(Gge,Aar),e(Dv,Lar),e(Dv,Oq),e(Oq,Bar),e(Dv,kar),e(X,xar),e(X,qv),e(qv,Oge),e(Oge,Rar),e(qv,Sar),e(qv,Xq),e(Xq,Par),e(qv,$ar),e(X,Iar),e(X,Gv),e(Gv,Xge),e(Xge,jar),e(Gv,Nar),e(Gv,zq),e(zq,Dar),e(Gv,qar),e(X,Gar),e(X,Ov),e(Ov,zge),e(zge,Oar),e(Ov,Xar),e(Ov,Vq),e(Vq,zar),e(Ov,Var),e(X,War),e(X,Xv),e(Xv,Vge),e(Vge,Qar),e(Xv,Har),e(Xv,Wq),e(Wq,Uar),e(Xv,Jar),e(X,Yar),e(X,zv),e(zv,Wge),e(Wge,Kar),e(zv,Zar),e(zv,Qq),e(Qq,enr),e(zv,onr),e(X,rnr),e(X,Vv),e(Vv,Qge),e(Qge,tnr),e(Vv,anr),e(Vv,Hq),e(Hq,nnr),e(Vv,snr),e(X,lnr),e(X,Wv),e(Wv,Hge),e(Hge,inr),e(Wv,dnr),e(Wv,Uq),e(Uq,cnr),e(Wv,fnr),e(X,mnr),e(X,Qv),e(Qv,Uge),e(Uge,gnr),e(Qv,hnr),e(Qv,Jq),e(Jq,pnr),e(Qv,_nr),e(X,unr),e(X,Hv),e(Hv,Jge),e(Jge,bnr),e(Hv,vnr),e(Hv,Yq),e(Yq,Tnr),e(Hv,Fnr),e(X,Cnr),e(X,Uv),e(Uv,Yge),e(Yge,Mnr),e(Uv,Enr),e(Uv,Kq),e(Kq,ynr),e(Uv,wnr),e(vo,Anr),e(vo,Kge),e(Kge,Lnr),e(vo,Bnr),g(MA,vo,null),b(d,T9e,u),b(d,Cc,u),e(Cc,Jv),e(Jv,Zge),g(EA,Zge,null),e(Cc,knr),e(Cc,ehe),e(ehe,xnr),b(d,F9e,u),b(d,Tr,u),g(yA,Tr,null),e(Tr,Rnr),e(Tr,Mc),e(Mc,Snr),e(Mc,ohe),e(ohe,Pnr),e(Mc,$nr),e(Mc,rhe),e(rhe,Inr),e(Mc,jnr),e(Tr,Nnr),e(Tr,wA),e(wA,Dnr),e(wA,the),e(the,qnr),e(wA,Gnr),e(Tr,Onr),e(Tr,gt),g(AA,gt,null),e(gt,Xnr),e(gt,ahe),e(ahe,znr),e(gt,Vnr),e(gt,Ec),e(Ec,Wnr),e(Ec,nhe),e(nhe,Qnr),e(Ec,Hnr),e(Ec,she),e(she,Unr),e(Ec,Jnr),e(gt,Ynr),e(gt,lhe),e(lhe,Knr),e(gt,Znr),g(LA,gt,null),e(Tr,esr),e(Tr,To),g(BA,To,null),e(To,osr),e(To,ihe),e(ihe,rsr),e(To,tsr),e(To,pn),e(pn,asr),e(pn,dhe),e(dhe,nsr),e(pn,ssr),e(pn,che),e(che,lsr),e(pn,isr),e(pn,fhe),e(fhe,dsr),e(pn,csr),e(To,fsr),e(To,te),e(te,Yv),e(Yv,mhe),e(mhe,msr),e(Yv,gsr),e(Yv,Zq),e(Zq,hsr),e(Yv,psr),e(te,_sr),e(te,Kv),e(Kv,ghe),e(ghe,usr),e(Kv,bsr),e(Kv,eG),e(eG,vsr),e(Kv,Tsr),e(te,Fsr),e(te,Zv),e(Zv,hhe),e(hhe,Csr),e(Zv,Msr),e(Zv,oG),e(oG,Esr),e(Zv,ysr),e(te,wsr),e(te,eT),e(eT,phe),e(phe,Asr),e(eT,Lsr),e(eT,rG),e(rG,Bsr),e(eT,ksr),e(te,xsr),e(te,oT),e(oT,_he),e(_he,Rsr),e(oT,Ssr),e(oT,tG),e(tG,Psr),e(oT,$sr),e(te,Isr),e(te,rT),e(rT,uhe),e(uhe,jsr),e(rT,Nsr),e(rT,aG),e(aG,Dsr),e(rT,qsr),e(te,Gsr),e(te,tT),e(tT,bhe),e(bhe,Osr),e(tT,Xsr),e(tT,nG),e(nG,zsr),e(tT,Vsr),e(te,Wsr),e(te,aT),e(aT,vhe),e(vhe,Qsr),e(aT,Hsr),e(aT,sG),e(sG,Usr),e(aT,Jsr),e(te,Ysr),e(te,nT),e(nT,The),e(The,Ksr),e(nT,Zsr),e(nT,lG),e(lG,elr),e(nT,olr),e(te,rlr),e(te,sT),e(sT,Fhe),e(Fhe,tlr),e(sT,alr),e(sT,iG),e(iG,nlr),e(sT,slr),e(te,llr),e(te,lT),e(lT,Che),e(Che,ilr),e(lT,dlr),e(lT,dG),e(dG,clr),e(lT,flr),e(te,mlr),e(te,iT),e(iT,Mhe),e(Mhe,glr),e(iT,hlr),e(iT,cG),e(cG,plr),e(iT,_lr),e(te,ulr),e(te,dT),e(dT,Ehe),e(Ehe,blr),e(dT,vlr),e(dT,fG),e(fG,Tlr),e(dT,Flr),e(te,Clr),e(te,cT),e(cT,yhe),e(yhe,Mlr),e(cT,Elr),e(cT,mG),e(mG,ylr),e(cT,wlr),e(te,Alr),e(te,fT),e(fT,whe),e(whe,Llr),e(fT,Blr),e(fT,gG),e(gG,klr),e(fT,xlr),e(te,Rlr),e(te,mT),e(mT,Ahe),e(Ahe,Slr),e(mT,Plr),e(mT,hG),e(hG,$lr),e(mT,Ilr),e(te,jlr),e(te,gT),e(gT,Lhe),e(Lhe,Nlr),e(gT,Dlr),e(gT,pG),e(pG,qlr),e(gT,Glr),e(To,Olr),e(To,Bhe),e(Bhe,Xlr),e(To,zlr),g(kA,To,null),b(d,C9e,u),b(d,yc,u),e(yc,hT),e(hT,khe),g(xA,khe,null),e(yc,Vlr),e(yc,xhe),e(xhe,Wlr),b(d,M9e,u),b(d,Fr,u),g(RA,Fr,null),e(Fr,Qlr),e(Fr,wc),e(wc,Hlr),e(wc,Rhe),e(Rhe,Ulr),e(wc,Jlr),e(wc,She),e(She,Ylr),e(wc,Klr),e(Fr,Zlr),e(Fr,SA),e(SA,eir),e(SA,Phe),e(Phe,oir),e(SA,rir),e(Fr,tir),e(Fr,ht),g(PA,ht,null),e(ht,air),e(ht,$he),e($he,nir),e(ht,sir),e(ht,Ac),e(Ac,lir),e(Ac,Ihe),e(Ihe,iir),e(Ac,dir),e(Ac,jhe),e(jhe,cir),e(Ac,fir),e(ht,mir),e(ht,Nhe),e(Nhe,gir),e(ht,hir),g($A,ht,null),e(Fr,pir),e(Fr,Fo),g(IA,Fo,null),e(Fo,_ir),e(Fo,Dhe),e(Dhe,uir),e(Fo,bir),e(Fo,_n),e(_n,vir),e(_n,qhe),e(qhe,Tir),e(_n,Fir),e(_n,Ghe),e(Ghe,Cir),e(_n,Mir),e(_n,Ohe),e(Ohe,Eir),e(_n,yir),e(Fo,wir),e(Fo,Xhe),e(Xhe,pT),e(pT,zhe),e(zhe,Air),e(pT,Lir),e(pT,_G),e(_G,Bir),e(pT,kir),e(Fo,xir),e(Fo,Vhe),e(Vhe,Rir),e(Fo,Sir),g(jA,Fo,null),b(d,E9e,u),b(d,Lc,u),e(Lc,_T),e(_T,Whe),g(NA,Whe,null),e(Lc,Pir),e(Lc,Qhe),e(Qhe,$ir),b(d,y9e,u),b(d,Cr,u),g(DA,Cr,null),e(Cr,Iir),e(Cr,Bc),e(Bc,jir),e(Bc,Hhe),e(Hhe,Nir),e(Bc,Dir),e(Bc,Uhe),e(Uhe,qir),e(Bc,Gir),e(Cr,Oir),e(Cr,qA),e(qA,Xir),e(qA,Jhe),e(Jhe,zir),e(qA,Vir),e(Cr,Wir),e(Cr,pt),g(GA,pt,null),e(pt,Qir),e(pt,Yhe),e(Yhe,Hir),e(pt,Uir),e(pt,kc),e(kc,Jir),e(kc,Khe),e(Khe,Yir),e(kc,Kir),e(kc,Zhe),e(Zhe,Zir),e(kc,edr),e(pt,odr),e(pt,epe),e(epe,rdr),e(pt,tdr),g(OA,pt,null),e(Cr,adr),e(Cr,Co),g(XA,Co,null),e(Co,ndr),e(Co,ope),e(ope,sdr),e(Co,ldr),e(Co,un),e(un,idr),e(un,rpe),e(rpe,ddr),e(un,cdr),e(un,tpe),e(tpe,fdr),e(un,mdr),e(un,ape),e(ape,gdr),e(un,hdr),e(Co,pdr),e(Co,K),e(K,uT),e(uT,npe),e(npe,_dr),e(uT,udr),e(uT,uG),e(uG,bdr),e(uT,vdr),e(K,Tdr),e(K,bT),e(bT,spe),e(spe,Fdr),e(bT,Cdr),e(bT,bG),e(bG,Mdr),e(bT,Edr),e(K,ydr),e(K,vT),e(vT,lpe),e(lpe,wdr),e(vT,Adr),e(vT,vG),e(vG,Ldr),e(vT,Bdr),e(K,kdr),e(K,TT),e(TT,ipe),e(ipe,xdr),e(TT,Rdr),e(TT,TG),e(TG,Sdr),e(TT,Pdr),e(K,$dr),e(K,FT),e(FT,dpe),e(dpe,Idr),e(FT,jdr),e(FT,FG),e(FG,Ndr),e(FT,Ddr),e(K,qdr),e(K,CT),e(CT,cpe),e(cpe,Gdr),e(CT,Odr),e(CT,CG),e(CG,Xdr),e(CT,zdr),e(K,Vdr),e(K,MT),e(MT,fpe),e(fpe,Wdr),e(MT,Qdr),e(MT,MG),e(MG,Hdr),e(MT,Udr),e(K,Jdr),e(K,ET),e(ET,mpe),e(mpe,Ydr),e(ET,Kdr),e(ET,EG),e(EG,Zdr),e(ET,ecr),e(K,ocr),e(K,yT),e(yT,gpe),e(gpe,rcr),e(yT,tcr),e(yT,yG),e(yG,acr),e(yT,ncr),e(K,scr),e(K,wT),e(wT,hpe),e(hpe,lcr),e(wT,icr),e(wT,wG),e(wG,dcr),e(wT,ccr),e(K,fcr),e(K,AT),e(AT,ppe),e(ppe,mcr),e(AT,gcr),e(AT,AG),e(AG,hcr),e(AT,pcr),e(K,_cr),e(K,LT),e(LT,_pe),e(_pe,ucr),e(LT,bcr),e(LT,LG),e(LG,vcr),e(LT,Tcr),e(K,Fcr),e(K,BT),e(BT,upe),e(upe,Ccr),e(BT,Mcr),e(BT,BG),e(BG,Ecr),e(BT,ycr),e(K,wcr),e(K,kT),e(kT,bpe),e(bpe,Acr),e(kT,Lcr),e(kT,kG),e(kG,Bcr),e(kT,kcr),e(K,xcr),e(K,xT),e(xT,vpe),e(vpe,Rcr),e(xT,Scr),e(xT,xG),e(xG,Pcr),e(xT,$cr),e(K,Icr),e(K,RT),e(RT,Tpe),e(Tpe,jcr),e(RT,Ncr),e(RT,RG),e(RG,Dcr),e(RT,qcr),e(K,Gcr),e(K,ST),e(ST,Fpe),e(Fpe,Ocr),e(ST,Xcr),e(ST,SG),e(SG,zcr),e(ST,Vcr),e(K,Wcr),e(K,PT),e(PT,Cpe),e(Cpe,Qcr),e(PT,Hcr),e(PT,PG),e(PG,Ucr),e(PT,Jcr),e(K,Ycr),e(K,$T),e($T,Mpe),e(Mpe,Kcr),e($T,Zcr),e($T,$G),e($G,efr),e($T,ofr),e(K,rfr),e(K,IT),e(IT,Epe),e(Epe,tfr),e(IT,afr),e(IT,IG),e(IG,nfr),e(IT,sfr),e(Co,lfr),e(Co,ype),e(ype,ifr),e(Co,dfr),g(zA,Co,null),b(d,w9e,u),b(d,xc,u),e(xc,jT),e(jT,wpe),g(VA,wpe,null),e(xc,cfr),e(xc,Ape),e(Ape,ffr),b(d,A9e,u),b(d,Mr,u),g(WA,Mr,null),e(Mr,mfr),e(Mr,Rc),e(Rc,gfr),e(Rc,Lpe),e(Lpe,hfr),e(Rc,pfr),e(Rc,Bpe),e(Bpe,_fr),e(Rc,ufr),e(Mr,bfr),e(Mr,QA),e(QA,vfr),e(QA,kpe),e(kpe,Tfr),e(QA,Ffr),e(Mr,Cfr),e(Mr,_t),g(HA,_t,null),e(_t,Mfr),e(_t,xpe),e(xpe,Efr),e(_t,yfr),e(_t,Sc),e(Sc,wfr),e(Sc,Rpe),e(Rpe,Afr),e(Sc,Lfr),e(Sc,Spe),e(Spe,Bfr),e(Sc,kfr),e(_t,xfr),e(_t,Ppe),e(Ppe,Rfr),e(_t,Sfr),g(UA,_t,null),e(Mr,Pfr),e(Mr,Mo),g(JA,Mo,null),e(Mo,$fr),e(Mo,$pe),e($pe,Ifr),e(Mo,jfr),e(Mo,bn),e(bn,Nfr),e(bn,Ipe),e(Ipe,Dfr),e(bn,qfr),e(bn,jpe),e(jpe,Gfr),e(bn,Ofr),e(bn,Npe),e(Npe,Xfr),e(bn,zfr),e(Mo,Vfr),e(Mo,Z),e(Z,NT),e(NT,Dpe),e(Dpe,Wfr),e(NT,Qfr),e(NT,jG),e(jG,Hfr),e(NT,Ufr),e(Z,Jfr),e(Z,DT),e(DT,qpe),e(qpe,Yfr),e(DT,Kfr),e(DT,NG),e(NG,Zfr),e(DT,emr),e(Z,omr),e(Z,qT),e(qT,Gpe),e(Gpe,rmr),e(qT,tmr),e(qT,DG),e(DG,amr),e(qT,nmr),e(Z,smr),e(Z,GT),e(GT,Ope),e(Ope,lmr),e(GT,imr),e(GT,qG),e(qG,dmr),e(GT,cmr),e(Z,fmr),e(Z,OT),e(OT,Xpe),e(Xpe,mmr),e(OT,gmr),e(OT,GG),e(GG,hmr),e(OT,pmr),e(Z,_mr),e(Z,XT),e(XT,zpe),e(zpe,umr),e(XT,bmr),e(XT,OG),e(OG,vmr),e(XT,Tmr),e(Z,Fmr),e(Z,zT),e(zT,Vpe),e(Vpe,Cmr),e(zT,Mmr),e(zT,XG),e(XG,Emr),e(zT,ymr),e(Z,wmr),e(Z,VT),e(VT,Wpe),e(Wpe,Amr),e(VT,Lmr),e(VT,zG),e(zG,Bmr),e(VT,kmr),e(Z,xmr),e(Z,WT),e(WT,Qpe),e(Qpe,Rmr),e(WT,Smr),e(WT,VG),e(VG,Pmr),e(WT,$mr),e(Z,Imr),e(Z,QT),e(QT,Hpe),e(Hpe,jmr),e(QT,Nmr),e(QT,WG),e(WG,Dmr),e(QT,qmr),e(Z,Gmr),e(Z,HT),e(HT,Upe),e(Upe,Omr),e(HT,Xmr),e(HT,QG),e(QG,zmr),e(HT,Vmr),e(Z,Wmr),e(Z,UT),e(UT,Jpe),e(Jpe,Qmr),e(UT,Hmr),e(UT,HG),e(HG,Umr),e(UT,Jmr),e(Z,Ymr),e(Z,JT),e(JT,Ype),e(Ype,Kmr),e(JT,Zmr),e(JT,UG),e(UG,egr),e(JT,ogr),e(Z,rgr),e(Z,YT),e(YT,Kpe),e(Kpe,tgr),e(YT,agr),e(YT,JG),e(JG,ngr),e(YT,sgr),e(Z,lgr),e(Z,KT),e(KT,Zpe),e(Zpe,igr),e(KT,dgr),e(KT,YG),e(YG,cgr),e(KT,fgr),e(Z,mgr),e(Z,ZT),e(ZT,e_e),e(e_e,ggr),e(ZT,hgr),e(ZT,KG),e(KG,pgr),e(ZT,_gr),e(Z,ugr),e(Z,eF),e(eF,o_e),e(o_e,bgr),e(eF,vgr),e(eF,ZG),e(ZG,Tgr),e(eF,Fgr),e(Z,Cgr),e(Z,oF),e(oF,r_e),e(r_e,Mgr),e(oF,Egr),e(oF,eO),e(eO,ygr),e(oF,wgr),e(Z,Agr),e(Z,rF),e(rF,t_e),e(t_e,Lgr),e(rF,Bgr),e(rF,oO),e(oO,kgr),e(rF,xgr),e(Mo,Rgr),e(Mo,a_e),e(a_e,Sgr),e(Mo,Pgr),g(YA,Mo,null),b(d,L9e,u),b(d,Pc,u),e(Pc,tF),e(tF,n_e),g(KA,n_e,null),e(Pc,$gr),e(Pc,s_e),e(s_e,Igr),b(d,B9e,u),b(d,Er,u),g(ZA,Er,null),e(Er,jgr),e(Er,$c),e($c,Ngr),e($c,l_e),e(l_e,Dgr),e($c,qgr),e($c,i_e),e(i_e,Ggr),e($c,Ogr),e(Er,Xgr),e(Er,e6),e(e6,zgr),e(e6,d_e),e(d_e,Vgr),e(e6,Wgr),e(Er,Qgr),e(Er,ut),g(o6,ut,null),e(ut,Hgr),e(ut,c_e),e(c_e,Ugr),e(ut,Jgr),e(ut,Ic),e(Ic,Ygr),e(Ic,f_e),e(f_e,Kgr),e(Ic,Zgr),e(Ic,m_e),e(m_e,ehr),e(Ic,ohr),e(ut,rhr),e(ut,g_e),e(g_e,thr),e(ut,ahr),g(r6,ut,null),e(Er,nhr),e(Er,Eo),g(t6,Eo,null),e(Eo,shr),e(Eo,h_e),e(h_e,lhr),e(Eo,ihr),e(Eo,vn),e(vn,dhr),e(vn,p_e),e(p_e,chr),e(vn,fhr),e(vn,__e),e(__e,mhr),e(vn,ghr),e(vn,u_e),e(u_e,hhr),e(vn,phr),e(Eo,_hr),e(Eo,b_e),e(b_e,aF),e(aF,v_e),e(v_e,uhr),e(aF,bhr),e(aF,rO),e(rO,vhr),e(aF,Thr),e(Eo,Fhr),e(Eo,T_e),e(T_e,Chr),e(Eo,Mhr),g(a6,Eo,null),b(d,k9e,u),b(d,jc,u),e(jc,nF),e(nF,F_e),g(n6,F_e,null),e(jc,Ehr),e(jc,C_e),e(C_e,yhr),b(d,x9e,u),b(d,yr,u),g(s6,yr,null),e(yr,whr),e(yr,Nc),e(Nc,Ahr),e(Nc,M_e),e(M_e,Lhr),e(Nc,Bhr),e(Nc,E_e),e(E_e,khr),e(Nc,xhr),e(yr,Rhr),e(yr,l6),e(l6,Shr),e(l6,y_e),e(y_e,Phr),e(l6,$hr),e(yr,Ihr),e(yr,bt),g(i6,bt,null),e(bt,jhr),e(bt,w_e),e(w_e,Nhr),e(bt,Dhr),e(bt,Dc),e(Dc,qhr),e(Dc,A_e),e(A_e,Ghr),e(Dc,Ohr),e(Dc,L_e),e(L_e,Xhr),e(Dc,zhr),e(bt,Vhr),e(bt,B_e),e(B_e,Whr),e(bt,Qhr),g(d6,bt,null),e(yr,Hhr),e(yr,yo),g(c6,yo,null),e(yo,Uhr),e(yo,k_e),e(k_e,Jhr),e(yo,Yhr),e(yo,Tn),e(Tn,Khr),e(Tn,x_e),e(x_e,Zhr),e(Tn,epr),e(Tn,R_e),e(R_e,opr),e(Tn,rpr),e(Tn,S_e),e(S_e,tpr),e(Tn,apr),e(yo,npr),e(yo,P_e),e(P_e,sF),e(sF,$_e),e($_e,spr),e(sF,lpr),e(sF,tO),e(tO,ipr),e(sF,dpr),e(yo,cpr),e(yo,I_e),e(I_e,fpr),e(yo,mpr),g(f6,yo,null),b(d,R9e,u),b(d,qc,u),e(qc,lF),e(lF,j_e),g(m6,j_e,null),e(qc,gpr),e(qc,N_e),e(N_e,hpr),b(d,S9e,u),b(d,wr,u),g(g6,wr,null),e(wr,ppr),e(wr,Gc),e(Gc,_pr),e(Gc,D_e),e(D_e,upr),e(Gc,bpr),e(Gc,q_e),e(q_e,vpr),e(Gc,Tpr),e(wr,Fpr),e(wr,h6),e(h6,Cpr),e(h6,G_e),e(G_e,Mpr),e(h6,Epr),e(wr,ypr),e(wr,vt),g(p6,vt,null),e(vt,wpr),e(vt,O_e),e(O_e,Apr),e(vt,Lpr),e(vt,Oc),e(Oc,Bpr),e(Oc,X_e),e(X_e,kpr),e(Oc,xpr),e(Oc,z_e),e(z_e,Rpr),e(Oc,Spr),e(vt,Ppr),e(vt,V_e),e(V_e,$pr),e(vt,Ipr),g(_6,vt,null),e(wr,jpr),e(wr,wo),g(u6,wo,null),e(wo,Npr),e(wo,W_e),e(W_e,Dpr),e(wo,qpr),e(wo,Fn),e(Fn,Gpr),e(Fn,Q_e),e(Q_e,Opr),e(Fn,Xpr),e(Fn,H_e),e(H_e,zpr),e(Fn,Vpr),e(Fn,U_e),e(U_e,Wpr),e(Fn,Qpr),e(wo,Hpr),e(wo,V),e(V,iF),e(iF,J_e),e(J_e,Upr),e(iF,Jpr),e(iF,aO),e(aO,Ypr),e(iF,Kpr),e(V,Zpr),e(V,dF),e(dF,Y_e),e(Y_e,e_r),e(dF,o_r),e(dF,nO),e(nO,r_r),e(dF,t_r),e(V,a_r),e(V,cF),e(cF,K_e),e(K_e,n_r),e(cF,s_r),e(cF,sO),e(sO,l_r),e(cF,i_r),e(V,d_r),e(V,fF),e(fF,Z_e),e(Z_e,c_r),e(fF,f_r),e(fF,lO),e(lO,m_r),e(fF,g_r),e(V,h_r),e(V,mF),e(mF,eue),e(eue,p_r),e(mF,__r),e(mF,iO),e(iO,u_r),e(mF,b_r),e(V,v_r),e(V,gF),e(gF,oue),e(oue,T_r),e(gF,F_r),e(gF,dO),e(dO,C_r),e(gF,M_r),e(V,E_r),e(V,hF),e(hF,rue),e(rue,y_r),e(hF,w_r),e(hF,cO),e(cO,A_r),e(hF,L_r),e(V,B_r),e(V,pF),e(pF,tue),e(tue,k_r),e(pF,x_r),e(pF,fO),e(fO,R_r),e(pF,S_r),e(V,P_r),e(V,_F),e(_F,aue),e(aue,$_r),e(_F,I_r),e(_F,mO),e(mO,j_r),e(_F,N_r),e(V,D_r),e(V,uF),e(uF,nue),e(nue,q_r),e(uF,G_r),e(uF,gO),e(gO,O_r),e(uF,X_r),e(V,z_r),e(V,bF),e(bF,sue),e(sue,V_r),e(bF,W_r),e(bF,hO),e(hO,Q_r),e(bF,H_r),e(V,U_r),e(V,vF),e(vF,lue),e(lue,J_r),e(vF,Y_r),e(vF,pO),e(pO,K_r),e(vF,Z_r),e(V,eur),e(V,TF),e(TF,iue),e(iue,our),e(TF,rur),e(TF,_O),e(_O,tur),e(TF,aur),e(V,nur),e(V,FF),e(FF,due),e(due,sur),e(FF,lur),e(FF,uO),e(uO,iur),e(FF,dur),e(V,cur),e(V,CF),e(CF,cue),e(cue,fur),e(CF,mur),e(CF,bO),e(bO,gur),e(CF,hur),e(V,pur),e(V,MF),e(MF,fue),e(fue,_ur),e(MF,uur),e(MF,vO),e(vO,bur),e(MF,vur),e(V,Tur),e(V,EF),e(EF,mue),e(mue,Fur),e(EF,Cur),e(EF,TO),e(TO,Mur),e(EF,Eur),e(V,yur),e(V,yF),e(yF,gue),e(gue,wur),e(yF,Aur),e(yF,FO),e(FO,Lur),e(yF,Bur),e(V,kur),e(V,wF),e(wF,hue),e(hue,xur),e(wF,Rur),e(wF,CO),e(CO,Sur),e(wF,Pur),e(V,$ur),e(V,AF),e(AF,pue),e(pue,Iur),e(AF,jur),e(AF,MO),e(MO,Nur),e(AF,Dur),e(V,qur),e(V,LF),e(LF,_ue),e(_ue,Gur),e(LF,Our),e(LF,EO),e(EO,Xur),e(LF,zur),e(V,Vur),e(V,BF),e(BF,uue),e(uue,Wur),e(BF,Qur),e(BF,yO),e(yO,Hur),e(BF,Uur),e(V,Jur),e(V,kF),e(kF,bue),e(bue,Yur),e(kF,Kur),e(kF,wO),e(wO,Zur),e(kF,e1r),e(V,o1r),e(V,xF),e(xF,vue),e(vue,r1r),e(xF,t1r),e(xF,AO),e(AO,a1r),e(xF,n1r),e(wo,s1r),e(wo,Tue),e(Tue,l1r),e(wo,i1r),g(b6,wo,null),b(d,P9e,u),b(d,Xc,u),e(Xc,RF),e(RF,Fue),g(v6,Fue,null),e(Xc,d1r),e(Xc,Cue),e(Cue,c1r),b(d,$9e,u),b(d,Ar,u),g(T6,Ar,null),e(Ar,f1r),e(Ar,zc),e(zc,m1r),e(zc,Mue),e(Mue,g1r),e(zc,h1r),e(zc,Eue),e(Eue,p1r),e(zc,_1r),e(Ar,u1r),e(Ar,F6),e(F6,b1r),e(F6,yue),e(yue,v1r),e(F6,T1r),e(Ar,F1r),e(Ar,Tt),g(C6,Tt,null),e(Tt,C1r),e(Tt,wue),e(wue,M1r),e(Tt,E1r),e(Tt,Vc),e(Vc,y1r),e(Vc,Aue),e(Aue,w1r),e(Vc,A1r),e(Vc,Lue),e(Lue,L1r),e(Vc,B1r),e(Tt,k1r),e(Tt,Bue),e(Bue,x1r),e(Tt,R1r),g(M6,Tt,null),e(Ar,S1r),e(Ar,Ao),g(E6,Ao,null),e(Ao,P1r),e(Ao,kue),e(kue,$1r),e(Ao,I1r),e(Ao,Cn),e(Cn,j1r),e(Cn,xue),e(xue,N1r),e(Cn,D1r),e(Cn,Rue),e(Rue,q1r),e(Cn,G1r),e(Cn,Sue),e(Sue,O1r),e(Cn,X1r),e(Ao,z1r),e(Ao,Mn),e(Mn,SF),e(SF,Pue),e(Pue,V1r),e(SF,W1r),e(SF,LO),e(LO,Q1r),e(SF,H1r),e(Mn,U1r),e(Mn,PF),e(PF,$ue),e($ue,J1r),e(PF,Y1r),e(PF,BO),e(BO,K1r),e(PF,Z1r),e(Mn,e7r),e(Mn,$F),e($F,Iue),e(Iue,o7r),e($F,r7r),e($F,kO),e(kO,t7r),e($F,a7r),e(Mn,n7r),e(Mn,IF),e(IF,jue),e(jue,s7r),e(IF,l7r),e(IF,xO),e(xO,i7r),e(IF,d7r),e(Ao,c7r),e(Ao,Nue),e(Nue,f7r),e(Ao,m7r),g(y6,Ao,null),b(d,I9e,u),b(d,Wc,u),e(Wc,jF),e(jF,Due),g(w6,Due,null),e(Wc,g7r),e(Wc,que),e(que,h7r),b(d,j9e,u),b(d,Lr,u),g(A6,Lr,null),e(Lr,p7r),e(Lr,Qc),e(Qc,_7r),e(Qc,Gue),e(Gue,u7r),e(Qc,b7r),e(Qc,Oue),e(Oue,v7r),e(Qc,T7r),e(Lr,F7r),e(Lr,L6),e(L6,C7r),e(L6,Xue),e(Xue,M7r),e(L6,E7r),e(Lr,y7r),e(Lr,Ft),g(B6,Ft,null),e(Ft,w7r),e(Ft,zue),e(zue,A7r),e(Ft,L7r),e(Ft,Hc),e(Hc,B7r),e(Hc,Vue),e(Vue,k7r),e(Hc,x7r),e(Hc,Wue),e(Wue,R7r),e(Hc,S7r),e(Ft,P7r),e(Ft,Que),e(Que,$7r),e(Ft,I7r),g(k6,Ft,null),e(Lr,j7r),e(Lr,Lo),g(x6,Lo,null),e(Lo,N7r),e(Lo,Hue),e(Hue,D7r),e(Lo,q7r),e(Lo,En),e(En,G7r),e(En,Uue),e(Uue,O7r),e(En,X7r),e(En,Jue),e(Jue,z7r),e(En,V7r),e(En,Yue),e(Yue,W7r),e(En,Q7r),e(Lo,H7r),e(Lo,fe),e(fe,NF),e(NF,Kue),e(Kue,U7r),e(NF,J7r),e(NF,RO),e(RO,Y7r),e(NF,K7r),e(fe,Z7r),e(fe,DF),e(DF,Zue),e(Zue,e4r),e(DF,o4r),e(DF,SO),e(SO,r4r),e(DF,t4r),e(fe,a4r),e(fe,qF),e(qF,e1e),e(e1e,n4r),e(qF,s4r),e(qF,PO),e(PO,l4r),e(qF,i4r),e(fe,d4r),e(fe,GF),e(GF,o1e),e(o1e,c4r),e(GF,f4r),e(GF,$O),e($O,m4r),e(GF,g4r),e(fe,h4r),e(fe,OF),e(OF,r1e),e(r1e,p4r),e(OF,_4r),e(OF,IO),e(IO,u4r),e(OF,b4r),e(fe,v4r),e(fe,XF),e(XF,t1e),e(t1e,T4r),e(XF,F4r),e(XF,jO),e(jO,C4r),e(XF,M4r),e(fe,E4r),e(fe,zF),e(zF,a1e),e(a1e,y4r),e(zF,w4r),e(zF,NO),e(NO,A4r),e(zF,L4r),e(fe,B4r),e(fe,VF),e(VF,n1e),e(n1e,k4r),e(VF,x4r),e(VF,DO),e(DO,R4r),e(VF,S4r),e(fe,P4r),e(fe,WF),e(WF,s1e),e(s1e,$4r),e(WF,I4r),e(WF,qO),e(qO,j4r),e(WF,N4r),e(fe,D4r),e(fe,QF),e(QF,l1e),e(l1e,q4r),e(QF,G4r),e(QF,GO),e(GO,O4r),e(QF,X4r),e(fe,z4r),e(fe,HF),e(HF,i1e),e(i1e,V4r),e(HF,W4r),e(HF,OO),e(OO,Q4r),e(HF,H4r),e(Lo,U4r),e(Lo,d1e),e(d1e,J4r),e(Lo,Y4r),g(R6,Lo,null),b(d,N9e,u),b(d,Uc,u),e(Uc,UF),e(UF,c1e),g(S6,c1e,null),e(Uc,K4r),e(Uc,f1e),e(f1e,Z4r),b(d,D9e,u),b(d,Br,u),g(P6,Br,null),e(Br,ebr),e(Br,Jc),e(Jc,obr),e(Jc,m1e),e(m1e,rbr),e(Jc,tbr),e(Jc,g1e),e(g1e,abr),e(Jc,nbr),e(Br,sbr),e(Br,$6),e($6,lbr),e($6,h1e),e(h1e,ibr),e($6,dbr),e(Br,cbr),e(Br,Ct),g(I6,Ct,null),e(Ct,fbr),e(Ct,p1e),e(p1e,mbr),e(Ct,gbr),e(Ct,Yc),e(Yc,hbr),e(Yc,_1e),e(_1e,pbr),e(Yc,_br),e(Yc,u1e),e(u1e,ubr),e(Yc,bbr),e(Ct,vbr),e(Ct,b1e),e(b1e,Tbr),e(Ct,Fbr),g(j6,Ct,null),e(Br,Cbr),e(Br,Bo),g(N6,Bo,null),e(Bo,Mbr),e(Bo,v1e),e(v1e,Ebr),e(Bo,ybr),e(Bo,yn),e(yn,wbr),e(yn,T1e),e(T1e,Abr),e(yn,Lbr),e(yn,F1e),e(F1e,Bbr),e(yn,kbr),e(yn,C1e),e(C1e,xbr),e(yn,Rbr),e(Bo,Sbr),e(Bo,ve),e(ve,JF),e(JF,M1e),e(M1e,Pbr),e(JF,$br),e(JF,XO),e(XO,Ibr),e(JF,jbr),e(ve,Nbr),e(ve,YF),e(YF,E1e),e(E1e,Dbr),e(YF,qbr),e(YF,zO),e(zO,Gbr),e(YF,Obr),e(ve,Xbr),e(ve,KF),e(KF,y1e),e(y1e,zbr),e(KF,Vbr),e(KF,VO),e(VO,Wbr),e(KF,Qbr),e(ve,Hbr),e(ve,ZF),e(ZF,w1e),e(w1e,Ubr),e(ZF,Jbr),e(ZF,WO),e(WO,Ybr),e(ZF,Kbr),e(ve,Zbr),e(ve,eC),e(eC,A1e),e(A1e,e5r),e(eC,o5r),e(eC,QO),e(QO,r5r),e(eC,t5r),e(ve,a5r),e(ve,oC),e(oC,L1e),e(L1e,n5r),e(oC,s5r),e(oC,HO),e(HO,l5r),e(oC,i5r),e(ve,d5r),e(ve,rC),e(rC,B1e),e(B1e,c5r),e(rC,f5r),e(rC,UO),e(UO,m5r),e(rC,g5r),e(ve,h5r),e(ve,tC),e(tC,k1e),e(k1e,p5r),e(tC,_5r),e(tC,JO),e(JO,u5r),e(tC,b5r),e(ve,v5r),e(ve,aC),e(aC,x1e),e(x1e,T5r),e(aC,F5r),e(aC,YO),e(YO,C5r),e(aC,M5r),e(Bo,E5r),e(Bo,R1e),e(R1e,y5r),e(Bo,w5r),g(D6,Bo,null),b(d,q9e,u),b(d,Kc,u),e(Kc,nC),e(nC,S1e),g(q6,S1e,null),e(Kc,A5r),e(Kc,P1e),e(P1e,L5r),b(d,G9e,u),b(d,kr,u),g(G6,kr,null),e(kr,B5r),e(kr,Zc),e(Zc,k5r),e(Zc,$1e),e($1e,x5r),e(Zc,R5r),e(Zc,I1e),e(I1e,S5r),e(Zc,P5r),e(kr,$5r),e(kr,O6),e(O6,I5r),e(O6,j1e),e(j1e,j5r),e(O6,N5r),e(kr,D5r),e(kr,Mt),g(X6,Mt,null),e(Mt,q5r),e(Mt,N1e),e(N1e,G5r),e(Mt,O5r),e(Mt,ef),e(ef,X5r),e(ef,D1e),e(D1e,z5r),e(ef,V5r),e(ef,q1e),e(q1e,W5r),e(ef,Q5r),e(Mt,H5r),e(Mt,G1e),e(G1e,U5r),e(Mt,J5r),g(z6,Mt,null),e(kr,Y5r),e(kr,ko),g(V6,ko,null),e(ko,K5r),e(ko,O1e),e(O1e,Z5r),e(ko,e2r),e(ko,wn),e(wn,o2r),e(wn,X1e),e(X1e,r2r),e(wn,t2r),e(wn,z1e),e(z1e,a2r),e(wn,n2r),e(wn,V1e),e(V1e,s2r),e(wn,l2r),e(ko,i2r),e(ko,Te),e(Te,sC),e(sC,W1e),e(W1e,d2r),e(sC,c2r),e(sC,KO),e(KO,f2r),e(sC,m2r),e(Te,g2r),e(Te,lC),e(lC,Q1e),e(Q1e,h2r),e(lC,p2r),e(lC,ZO),e(ZO,_2r),e(lC,u2r),e(Te,b2r),e(Te,iC),e(iC,H1e),e(H1e,v2r),e(iC,T2r),e(iC,eX),e(eX,F2r),e(iC,C2r),e(Te,M2r),e(Te,dC),e(dC,U1e),e(U1e,E2r),e(dC,y2r),e(dC,oX),e(oX,w2r),e(dC,A2r),e(Te,L2r),e(Te,cC),e(cC,J1e),e(J1e,B2r),e(cC,k2r),e(cC,rX),e(rX,x2r),e(cC,R2r),e(Te,S2r),e(Te,fC),e(fC,Y1e),e(Y1e,P2r),e(fC,$2r),e(fC,tX),e(tX,I2r),e(fC,j2r),e(Te,N2r),e(Te,mC),e(mC,K1e),e(K1e,D2r),e(mC,q2r),e(mC,aX),e(aX,G2r),e(mC,O2r),e(Te,X2r),e(Te,gC),e(gC,Z1e),e(Z1e,z2r),e(gC,V2r),e(gC,nX),e(nX,W2r),e(gC,Q2r),e(Te,H2r),e(Te,hC),e(hC,e7e),e(e7e,U2r),e(hC,J2r),e(hC,sX),e(sX,Y2r),e(hC,K2r),e(ko,Z2r),e(ko,o7e),e(o7e,evr),e(ko,ovr),g(W6,ko,null),b(d,O9e,u),b(d,of,u),e(of,pC),e(pC,r7e),g(Q6,r7e,null),e(of,rvr),e(of,t7e),e(t7e,tvr),b(d,X9e,u),b(d,xr,u),g(H6,xr,null),e(xr,avr),e(xr,rf),e(rf,nvr),e(rf,a7e),e(a7e,svr),e(rf,lvr),e(rf,n7e),e(n7e,ivr),e(rf,dvr),e(xr,cvr),e(xr,U6),e(U6,fvr),e(U6,s7e),e(s7e,mvr),e(U6,gvr),e(xr,hvr),e(xr,Et),g(J6,Et,null),e(Et,pvr),e(Et,l7e),e(l7e,_vr),e(Et,uvr),e(Et,tf),e(tf,bvr),e(tf,i7e),e(i7e,vvr),e(tf,Tvr),e(tf,d7e),e(d7e,Fvr),e(tf,Cvr),e(Et,Mvr),e(Et,c7e),e(c7e,Evr),e(Et,yvr),g(Y6,Et,null),e(xr,wvr),e(xr,xo),g(K6,xo,null),e(xo,Avr),e(xo,f7e),e(f7e,Lvr),e(xo,Bvr),e(xo,An),e(An,kvr),e(An,m7e),e(m7e,xvr),e(An,Rvr),e(An,g7e),e(g7e,Svr),e(An,Pvr),e(An,h7e),e(h7e,$vr),e(An,Ivr),e(xo,jvr),e(xo,Fe),e(Fe,_C),e(_C,p7e),e(p7e,Nvr),e(_C,Dvr),e(_C,lX),e(lX,qvr),e(_C,Gvr),e(Fe,Ovr),e(Fe,uC),e(uC,_7e),e(_7e,Xvr),e(uC,zvr),e(uC,iX),e(iX,Vvr),e(uC,Wvr),e(Fe,Qvr),e(Fe,bC),e(bC,u7e),e(u7e,Hvr),e(bC,Uvr),e(bC,dX),e(dX,Jvr),e(bC,Yvr),e(Fe,Kvr),e(Fe,vC),e(vC,b7e),e(b7e,Zvr),e(vC,eTr),e(vC,cX),e(cX,oTr),e(vC,rTr),e(Fe,tTr),e(Fe,TC),e(TC,v7e),e(v7e,aTr),e(TC,nTr),e(TC,fX),e(fX,sTr),e(TC,lTr),e(Fe,iTr),e(Fe,FC),e(FC,T7e),e(T7e,dTr),e(FC,cTr),e(FC,mX),e(mX,fTr),e(FC,mTr),e(Fe,gTr),e(Fe,CC),e(CC,F7e),e(F7e,hTr),e(CC,pTr),e(CC,gX),e(gX,_Tr),e(CC,uTr),e(Fe,bTr),e(Fe,MC),e(MC,C7e),e(C7e,vTr),e(MC,TTr),e(MC,hX),e(hX,FTr),e(MC,CTr),e(Fe,MTr),e(Fe,EC),e(EC,M7e),e(M7e,ETr),e(EC,yTr),e(EC,pX),e(pX,wTr),e(EC,ATr),e(xo,LTr),e(xo,E7e),e(E7e,BTr),e(xo,kTr),g(Z6,xo,null),b(d,z9e,u),b(d,af,u),e(af,yC),e(yC,y7e),g(e0,y7e,null),e(af,xTr),e(af,w7e),e(w7e,RTr),b(d,V9e,u),b(d,Rr,u),g(o0,Rr,null),e(Rr,STr),e(Rr,nf),e(nf,PTr),e(nf,A7e),e(A7e,$Tr),e(nf,ITr),e(nf,L7e),e(L7e,jTr),e(nf,NTr),e(Rr,DTr),e(Rr,r0),e(r0,qTr),e(r0,B7e),e(B7e,GTr),e(r0,OTr),e(Rr,XTr),e(Rr,yt),g(t0,yt,null),e(yt,zTr),e(yt,k7e),e(k7e,VTr),e(yt,WTr),e(yt,sf),e(sf,QTr),e(sf,x7e),e(x7e,HTr),e(sf,UTr),e(sf,R7e),e(R7e,JTr),e(sf,YTr),e(yt,KTr),e(yt,S7e),e(S7e,ZTr),e(yt,eFr),g(a0,yt,null),e(Rr,oFr),e(Rr,Ro),g(n0,Ro,null),e(Ro,rFr),e(Ro,P7e),e(P7e,tFr),e(Ro,aFr),e(Ro,Ln),e(Ln,nFr),e(Ln,$7e),e($7e,sFr),e(Ln,lFr),e(Ln,I7e),e(I7e,iFr),e(Ln,dFr),e(Ln,j7e),e(j7e,cFr),e(Ln,fFr),e(Ro,mFr),e(Ro,Ce),e(Ce,wC),e(wC,N7e),e(N7e,gFr),e(wC,hFr),e(wC,_X),e(_X,pFr),e(wC,_Fr),e(Ce,uFr),e(Ce,AC),e(AC,D7e),e(D7e,bFr),e(AC,vFr),e(AC,uX),e(uX,TFr),e(AC,FFr),e(Ce,CFr),e(Ce,LC),e(LC,q7e),e(q7e,MFr),e(LC,EFr),e(LC,bX),e(bX,yFr),e(LC,wFr),e(Ce,AFr),e(Ce,BC),e(BC,G7e),e(G7e,LFr),e(BC,BFr),e(BC,vX),e(vX,kFr),e(BC,xFr),e(Ce,RFr),e(Ce,kC),e(kC,O7e),e(O7e,SFr),e(kC,PFr),e(kC,TX),e(TX,$Fr),e(kC,IFr),e(Ce,jFr),e(Ce,xC),e(xC,X7e),e(X7e,NFr),e(xC,DFr),e(xC,FX),e(FX,qFr),e(xC,GFr),e(Ce,OFr),e(Ce,RC),e(RC,z7e),e(z7e,XFr),e(RC,zFr),e(RC,CX),e(CX,VFr),e(RC,WFr),e(Ce,QFr),e(Ce,SC),e(SC,V7e),e(V7e,HFr),e(SC,UFr),e(SC,MX),e(MX,JFr),e(SC,YFr),e(Ce,KFr),e(Ce,PC),e(PC,W7e),e(W7e,ZFr),e(PC,eCr),e(PC,EX),e(EX,oCr),e(PC,rCr),e(Ro,tCr),e(Ro,Q7e),e(Q7e,aCr),e(Ro,nCr),g(s0,Ro,null),b(d,W9e,u),b(d,lf,u),e(lf,$C),e($C,H7e),g(l0,H7e,null),e(lf,sCr),e(lf,U7e),e(U7e,lCr),b(d,Q9e,u),b(d,Sr,u),g(i0,Sr,null),e(Sr,iCr),e(Sr,df),e(df,dCr),e(df,J7e),e(J7e,cCr),e(df,fCr),e(df,Y7e),e(Y7e,mCr),e(df,gCr),e(Sr,hCr),e(Sr,d0),e(d0,pCr),e(d0,K7e),e(K7e,_Cr),e(d0,uCr),e(Sr,bCr),e(Sr,wt),g(c0,wt,null),e(wt,vCr),e(wt,Z7e),e(Z7e,TCr),e(wt,FCr),e(wt,cf),e(cf,CCr),e(cf,e4e),e(e4e,MCr),e(cf,ECr),e(cf,o4e),e(o4e,yCr),e(cf,wCr),e(wt,ACr),e(wt,r4e),e(r4e,LCr),e(wt,BCr),g(f0,wt,null),e(Sr,kCr),e(Sr,So),g(m0,So,null),e(So,xCr),e(So,t4e),e(t4e,RCr),e(So,SCr),e(So,Bn),e(Bn,PCr),e(Bn,a4e),e(a4e,$Cr),e(Bn,ICr),e(Bn,n4e),e(n4e,jCr),e(Bn,NCr),e(Bn,s4e),e(s4e,DCr),e(Bn,qCr),e(So,GCr),e(So,so),e(so,IC),e(IC,l4e),e(l4e,OCr),e(IC,XCr),e(IC,yX),e(yX,zCr),e(IC,VCr),e(so,WCr),e(so,jC),e(jC,i4e),e(i4e,QCr),e(jC,HCr),e(jC,wX),e(wX,UCr),e(jC,JCr),e(so,YCr),e(so,NC),e(NC,d4e),e(d4e,KCr),e(NC,ZCr),e(NC,AX),e(AX,eMr),e(NC,oMr),e(so,rMr),e(so,DC),e(DC,c4e),e(c4e,tMr),e(DC,aMr),e(DC,LX),e(LX,nMr),e(DC,sMr),e(so,lMr),e(so,qC),e(qC,f4e),e(f4e,iMr),e(qC,dMr),e(qC,BX),e(BX,cMr),e(qC,fMr),e(so,mMr),e(so,GC),e(GC,m4e),e(m4e,gMr),e(GC,hMr),e(GC,kX),e(kX,pMr),e(GC,_Mr),e(so,uMr),e(so,OC),e(OC,g4e),e(g4e,bMr),e(OC,vMr),e(OC,xX),e(xX,TMr),e(OC,FMr),e(So,CMr),e(So,h4e),e(h4e,MMr),e(So,EMr),g(g0,So,null),b(d,H9e,u),b(d,ff,u),e(ff,XC),e(XC,p4e),g(h0,p4e,null),e(ff,yMr),e(ff,_4e),e(_4e,wMr),b(d,U9e,u),b(d,Pr,u),g(p0,Pr,null),e(Pr,AMr),e(Pr,mf),e(mf,LMr),e(mf,u4e),e(u4e,BMr),e(mf,kMr),e(mf,b4e),e(b4e,xMr),e(mf,RMr),e(Pr,SMr),e(Pr,_0),e(_0,PMr),e(_0,v4e),e(v4e,$Mr),e(_0,IMr),e(Pr,jMr),e(Pr,At),g(u0,At,null),e(At,NMr),e(At,T4e),e(T4e,DMr),e(At,qMr),e(At,gf),e(gf,GMr),e(gf,F4e),e(F4e,OMr),e(gf,XMr),e(gf,C4e),e(C4e,zMr),e(gf,VMr),e(At,WMr),e(At,M4e),e(M4e,QMr),e(At,HMr),g(b0,At,null),e(Pr,UMr),e(Pr,Po),g(v0,Po,null),e(Po,JMr),e(Po,E4e),e(E4e,YMr),e(Po,KMr),e(Po,kn),e(kn,ZMr),e(kn,y4e),e(y4e,eEr),e(kn,oEr),e(kn,w4e),e(w4e,rEr),e(kn,tEr),e(kn,A4e),e(A4e,aEr),e(kn,nEr),e(Po,sEr),e(Po,lo),e(lo,zC),e(zC,L4e),e(L4e,lEr),e(zC,iEr),e(zC,RX),e(RX,dEr),e(zC,cEr),e(lo,fEr),e(lo,VC),e(VC,B4e),e(B4e,mEr),e(VC,gEr),e(VC,SX),e(SX,hEr),e(VC,pEr),e(lo,_Er),e(lo,WC),e(WC,k4e),e(k4e,uEr),e(WC,bEr),e(WC,PX),e(PX,vEr),e(WC,TEr),e(lo,FEr),e(lo,QC),e(QC,x4e),e(x4e,CEr),e(QC,MEr),e(QC,$X),e($X,EEr),e(QC,yEr),e(lo,wEr),e(lo,HC),e(HC,R4e),e(R4e,AEr),e(HC,LEr),e(HC,IX),e(IX,BEr),e(HC,kEr),e(lo,xEr),e(lo,UC),e(UC,S4e),e(S4e,REr),e(UC,SEr),e(UC,jX),e(jX,PEr),e(UC,$Er),e(lo,IEr),e(lo,JC),e(JC,P4e),e(P4e,jEr),e(JC,NEr),e(JC,NX),e(NX,DEr),e(JC,qEr),e(Po,GEr),e(Po,$4e),e($4e,OEr),e(Po,XEr),g(T0,Po,null),b(d,J9e,u),b(d,hf,u),e(hf,YC),e(YC,I4e),g(F0,I4e,null),e(hf,zEr),e(hf,j4e),e(j4e,VEr),b(d,Y9e,u),b(d,$r,u),g(C0,$r,null),e($r,WEr),e($r,pf),e(pf,QEr),e(pf,N4e),e(N4e,HEr),e(pf,UEr),e(pf,D4e),e(D4e,JEr),e(pf,YEr),e($r,KEr),e($r,M0),e(M0,ZEr),e(M0,q4e),e(q4e,e3r),e(M0,o3r),e($r,r3r),e($r,Lt),g(E0,Lt,null),e(Lt,t3r),e(Lt,G4e),e(G4e,a3r),e(Lt,n3r),e(Lt,_f),e(_f,s3r),e(_f,O4e),e(O4e,l3r),e(_f,i3r),e(_f,X4e),e(X4e,d3r),e(_f,c3r),e(Lt,f3r),e(Lt,z4e),e(z4e,m3r),e(Lt,g3r),g(y0,Lt,null),e($r,h3r),e($r,$o),g(w0,$o,null),e($o,p3r),e($o,V4e),e(V4e,_3r),e($o,u3r),e($o,xn),e(xn,b3r),e(xn,W4e),e(W4e,v3r),e(xn,T3r),e(xn,Q4e),e(Q4e,F3r),e(xn,C3r),e(xn,H4e),e(H4e,M3r),e(xn,E3r),e($o,y3r),e($o,U4e),e(U4e,KC),e(KC,J4e),e(J4e,w3r),e(KC,A3r),e(KC,DX),e(DX,L3r),e(KC,B3r),e($o,k3r),e($o,Y4e),e(Y4e,x3r),e($o,R3r),g(A0,$o,null),b(d,K9e,u),b(d,uf,u),e(uf,ZC),e(ZC,K4e),g(L0,K4e,null),e(uf,S3r),e(uf,Z4e),e(Z4e,P3r),b(d,Z9e,u),b(d,Ir,u),g(B0,Ir,null),e(Ir,$3r),e(Ir,bf),e(bf,I3r),e(bf,ebe),e(ebe,j3r),e(bf,N3r),e(bf,obe),e(obe,D3r),e(bf,q3r),e(Ir,G3r),e(Ir,k0),e(k0,O3r),e(k0,rbe),e(rbe,X3r),e(k0,z3r),e(Ir,V3r),e(Ir,Bt),g(x0,Bt,null),e(Bt,W3r),e(Bt,tbe),e(tbe,Q3r),e(Bt,H3r),e(Bt,vf),e(vf,U3r),e(vf,abe),e(abe,J3r),e(vf,Y3r),e(vf,nbe),e(nbe,K3r),e(vf,Z3r),e(Bt,eyr),e(Bt,sbe),e(sbe,oyr),e(Bt,ryr),g(R0,Bt,null),e(Ir,tyr),e(Ir,Io),g(S0,Io,null),e(Io,ayr),e(Io,lbe),e(lbe,nyr),e(Io,syr),e(Io,Rn),e(Rn,lyr),e(Rn,ibe),e(ibe,iyr),e(Rn,dyr),e(Rn,dbe),e(dbe,cyr),e(Rn,fyr),e(Rn,cbe),e(cbe,myr),e(Rn,gyr),e(Io,hyr),e(Io,P0),e(P0,eM),e(eM,fbe),e(fbe,pyr),e(eM,_yr),e(eM,qX),e(qX,uyr),e(eM,byr),e(P0,vyr),e(P0,oM),e(oM,mbe),e(mbe,Tyr),e(oM,Fyr),e(oM,GX),e(GX,Cyr),e(oM,Myr),e(Io,Eyr),e(Io,gbe),e(gbe,yyr),e(Io,wyr),g($0,Io,null),b(d,eBe,u),b(d,Tf,u),e(Tf,rM),e(rM,hbe),g(I0,hbe,null),e(Tf,Ayr),e(Tf,pbe),e(pbe,Lyr),b(d,oBe,u),b(d,jr,u),g(j0,jr,null),e(jr,Byr),e(jr,Ff),e(Ff,kyr),e(Ff,_be),e(_be,xyr),e(Ff,Ryr),e(Ff,ube),e(ube,Syr),e(Ff,Pyr),e(jr,$yr),e(jr,N0),e(N0,Iyr),e(N0,bbe),e(bbe,jyr),e(N0,Nyr),e(jr,Dyr),e(jr,kt),g(D0,kt,null),e(kt,qyr),e(kt,vbe),e(vbe,Gyr),e(kt,Oyr),e(kt,Cf),e(Cf,Xyr),e(Cf,Tbe),e(Tbe,zyr),e(Cf,Vyr),e(Cf,Fbe),e(Fbe,Wyr),e(Cf,Qyr),e(kt,Hyr),e(kt,Cbe),e(Cbe,Uyr),e(kt,Jyr),g(q0,kt,null),e(jr,Yyr),e(jr,jo),g(G0,jo,null),e(jo,Kyr),e(jo,Mbe),e(Mbe,Zyr),e(jo,ewr),e(jo,Sn),e(Sn,owr),e(Sn,Ebe),e(Ebe,rwr),e(Sn,twr),e(Sn,ybe),e(ybe,awr),e(Sn,nwr),e(Sn,wbe),e(wbe,swr),e(Sn,lwr),e(jo,iwr),e(jo,Abe),e(Abe,tM),e(tM,Lbe),e(Lbe,dwr),e(tM,cwr),e(tM,OX),e(OX,fwr),e(tM,mwr),e(jo,gwr),e(jo,Bbe),e(Bbe,hwr),e(jo,pwr),g(O0,jo,null),rBe=!0},p(d,[u]){const X0={};u&2&&(X0.$$scope={dirty:u,ctx:d}),Bf.$set(X0);const kbe={};u&2&&(kbe.$$scope={dirty:u,ctx:d}),ih.$set(kbe);const xbe={};u&2&&(xbe.$$scope={dirty:u,ctx:d}),vh.$set(xbe)},i(d){rBe||(h(ce.$$.fragment,d),h($a.$$.fragment,d),h(nE.$$.fragment,d),h(sE.$$.fragment,d),h(Bf.$$.fragment,d),h(lE.$$.fragment,d),h(iE.$$.fragment,d),h(fE.$$.fragment,d),h(mE.$$.fragment,d),h(gE.$$.fragment,d),h(hE.$$.fragment,d),h(pE.$$.fragment,d),h(bE.$$.fragment,d),h(vE.$$.fragment,d),h(TE.$$.fragment,d),h(FE.$$.fragment,d),h(CE.$$.fragment,d),h(yE.$$.fragment,d),h(ih.$$.fragment,d),h(wE.$$.fragment,d),h(AE.$$.fragment,d),h(LE.$$.fragment,d),h(BE.$$.fragment,d),h(RE.$$.fragment,d),h(vh.$$.fragment,d),h(SE.$$.fragment,d),h(PE.$$.fragment,d),h($E.$$.fragment,d),h(IE.$$.fragment,d),h(NE.$$.fragment,d),h(DE.$$.fragment,d),h(qE.$$.fragment,d),h(GE.$$.fragment,d),h(OE.$$.fragment,d),h(XE.$$.fragment,d),h(VE.$$.fragment,d),h(WE.$$.fragment,d),h(QE.$$.fragment,d),h(HE.$$.fragment,d),h(UE.$$.fragment,d),h(JE.$$.fragment,d),h(KE.$$.fragment,d),h(ZE.$$.fragment,d),h(e3.$$.fragment,d),h(o3.$$.fragment,d),h(r3.$$.fragment,d),h(t3.$$.fragment,d),h(n3.$$.fragment,d),h(s3.$$.fragment,d),h(l3.$$.fragment,d),h(i3.$$.fragment,d),h(d3.$$.fragment,d),h(c3.$$.fragment,d),h(m3.$$.fragment,d),h(g3.$$.fragment,d),h(h3.$$.fragment,d),h(p3.$$.fragment,d),h(_3.$$.fragment,d),h(u3.$$.fragment,d),h(v3.$$.fragment,d),h(T3.$$.fragment,d),h(F3.$$.fragment,d),h(C3.$$.fragment,d),h(M3.$$.fragment,d),h(E3.$$.fragment,d),h(w3.$$.fragment,d),h(A3.$$.fragment,d),h(L3.$$.fragment,d),h(B3.$$.fragment,d),h(k3.$$.fragment,d),h(x3.$$.fragment,d),h(S3.$$.fragment,d),h(P3.$$.fragment,d),h($3.$$.fragment,d),h(I3.$$.fragment,d),h(j3.$$.fragment,d),h(N3.$$.fragment,d),h(q3.$$.fragment,d),h(G3.$$.fragment,d),h(O3.$$.fragment,d),h(X3.$$.fragment,d),h(z3.$$.fragment,d),h(V3.$$.fragment,d),h(Q3.$$.fragment,d),h(H3.$$.fragment,d),h(U3.$$.fragment,d),h(J3.$$.fragment,d),h(Y3.$$.fragment,d),h(K3.$$.fragment,d),h(ey.$$.fragment,d),h(oy.$$.fragment,d),h(ry.$$.fragment,d),h(ty.$$.fragment,d),h(ay.$$.fragment,d),h(ny.$$.fragment,d),h(ly.$$.fragment,d),h(iy.$$.fragment,d),h(dy.$$.fragment,d),h(cy.$$.fragment,d),h(fy.$$.fragment,d),h(my.$$.fragment,d),h(hy.$$.fragment,d),h(py.$$.fragment,d),h(_y.$$.fragment,d),h(uy.$$.fragment,d),h(by.$$.fragment,d),h(vy.$$.fragment,d),h(Fy.$$.fragment,d),h(Cy.$$.fragment,d),h(My.$$.fragment,d),h(Ey.$$.fragment,d),h(yy.$$.fragment,d),h(wy.$$.fragment,d),h(Ly.$$.fragment,d),h(By.$$.fragment,d),h(ky.$$.fragment,d),h(xy.$$.fragment,d),h(Ry.$$.fragment,d),h(Sy.$$.fragment,d),h($y.$$.fragment,d),h(Iy.$$.fragment,d),h(jy.$$.fragment,d),h(Ny.$$.fragment,d),h(Dy.$$.fragment,d),h(qy.$$.fragment,d),h(Oy.$$.fragment,d),h(Xy.$$.fragment,d),h(zy.$$.fragment,d),h(Wy.$$.fragment,d),h(Qy.$$.fragment,d),h(Hy.$$.fragment,d),h(Jy.$$.fragment,d),h(Yy.$$.fragment,d),h(Ky.$$.fragment,d),h(Zy.$$.fragment,d),h(ew.$$.fragment,d),h(ow.$$.fragment,d),h(tw.$$.fragment,d),h(aw.$$.fragment,d),h(nw.$$.fragment,d),h(sw.$$.fragment,d),h(lw.$$.fragment,d),h(iw.$$.fragment,d),h(cw.$$.fragment,d),h(fw.$$.fragment,d),h(mw.$$.fragment,d),h(gw.$$.fragment,d),h(hw.$$.fragment,d),h(pw.$$.fragment,d),h(uw.$$.fragment,d),h(bw.$$.fragment,d),h(vw.$$.fragment,d),h(Tw.$$.fragment,d),h(Fw.$$.fragment,d),h(Cw.$$.fragment,d),h(Ew.$$.fragment,d),h(yw.$$.fragment,d),h(ww.$$.fragment,d),h(Lw.$$.fragment,d),h(Bw.$$.fragment,d),h(kw.$$.fragment,d),h(Rw.$$.fragment,d),h(Sw.$$.fragment,d),h(Pw.$$.fragment,d),h($w.$$.fragment,d),h(Iw.$$.fragment,d),h(jw.$$.fragment,d),h(Dw.$$.fragment,d),h(qw.$$.fragment,d),h(Gw.$$.fragment,d),h(Ow.$$.fragment,d),h(Xw.$$.fragment,d),h(zw.$$.fragment,d),h(Ww.$$.fragment,d),h(Qw.$$.fragment,d),h(Hw.$$.fragment,d),h(Uw.$$.fragment,d),h(Jw.$$.fragment,d),h(Yw.$$.fragment,d),h(Zw.$$.fragment,d),h(eA.$$.fragment,d),h(oA.$$.fragment,d),h(rA.$$.fragment,d),h(tA.$$.fragment,d),h(aA.$$.fragment,d),h(sA.$$.fragment,d),h(lA.$$.fragment,d),h(iA.$$.fragment,d),h(dA.$$.fragment,d),h(cA.$$.fragment,d),h(fA.$$.fragment,d),h(gA.$$.fragment,d),h(hA.$$.fragment,d),h(pA.$$.fragment,d),h(_A.$$.fragment,d),h(uA.$$.fragment,d),h(bA.$$.fragment,d),h(TA.$$.fragment,d),h(FA.$$.fragment,d),h(CA.$$.fragment,d),h(MA.$$.fragment,d),h(EA.$$.fragment,d),h(yA.$$.fragment,d),h(AA.$$.fragment,d),h(LA.$$.fragment,d),h(BA.$$.fragment,d),h(kA.$$.fragment,d),h(xA.$$.fragment,d),h(RA.$$.fragment,d),h(PA.$$.fragment,d),h($A.$$.fragment,d),h(IA.$$.fragment,d),h(jA.$$.fragment,d),h(NA.$$.fragment,d),h(DA.$$.fragment,d),h(GA.$$.fragment,d),h(OA.$$.fragment,d),h(XA.$$.fragment,d),h(zA.$$.fragment,d),h(VA.$$.fragment,d),h(WA.$$.fragment,d),h(HA.$$.fragment,d),h(UA.$$.fragment,d),h(JA.$$.fragment,d),h(YA.$$.fragment,d),h(KA.$$.fragment,d),h(ZA.$$.fragment,d),h(o6.$$.fragment,d),h(r6.$$.fragment,d),h(t6.$$.fragment,d),h(a6.$$.fragment,d),h(n6.$$.fragment,d),h(s6.$$.fragment,d),h(i6.$$.fragment,d),h(d6.$$.fragment,d),h(c6.$$.fragment,d),h(f6.$$.fragment,d),h(m6.$$.fragment,d),h(g6.$$.fragment,d),h(p6.$$.fragment,d),h(_6.$$.fragment,d),h(u6.$$.fragment,d),h(b6.$$.fragment,d),h(v6.$$.fragment,d),h(T6.$$.fragment,d),h(C6.$$.fragment,d),h(M6.$$.fragment,d),h(E6.$$.fragment,d),h(y6.$$.fragment,d),h(w6.$$.fragment,d),h(A6.$$.fragment,d),h(B6.$$.fragment,d),h(k6.$$.fragment,d),h(x6.$$.fragment,d),h(R6.$$.fragment,d),h(S6.$$.fragment,d),h(P6.$$.fragment,d),h(I6.$$.fragment,d),h(j6.$$.fragment,d),h(N6.$$.fragment,d),h(D6.$$.fragment,d),h(q6.$$.fragment,d),h(G6.$$.fragment,d),h(X6.$$.fragment,d),h(z6.$$.fragment,d),h(V6.$$.fragment,d),h(W6.$$.fragment,d),h(Q6.$$.fragment,d),h(H6.$$.fragment,d),h(J6.$$.fragment,d),h(Y6.$$.fragment,d),h(K6.$$.fragment,d),h(Z6.$$.fragment,d),h(e0.$$.fragment,d),h(o0.$$.fragment,d),h(t0.$$.fragment,d),h(a0.$$.fragment,d),h(n0.$$.fragment,d),h(s0.$$.fragment,d),h(l0.$$.fragment,d),h(i0.$$.fragment,d),h(c0.$$.fragment,d),h(f0.$$.fragment,d),h(m0.$$.fragment,d),h(g0.$$.fragment,d),h(h0.$$.fragment,d),h(p0.$$.fragment,d),h(u0.$$.fragment,d),h(b0.$$.fragment,d),h(v0.$$.fragment,d),h(T0.$$.fragment,d),h(F0.$$.fragment,d),h(C0.$$.fragment,d),h(E0.$$.fragment,d),h(y0.$$.fragment,d),h(w0.$$.fragment,d),h(A0.$$.fragment,d),h(L0.$$.fragment,d),h(B0.$$.fragment,d),h(x0.$$.fragment,d),h(R0.$$.fragment,d),h(S0.$$.fragment,d),h($0.$$.fragment,d),h(I0.$$.fragment,d),h(j0.$$.fragment,d),h(D0.$$.fragment,d),h(q0.$$.fragment,d),h(G0.$$.fragment,d),h(O0.$$.fragment,d),rBe=!0)},o(d){p(ce.$$.fragment,d),p($a.$$.fragment,d),p(nE.$$.fragment,d),p(sE.$$.fragment,d),p(Bf.$$.fragment,d),p(lE.$$.fragment,d),p(iE.$$.fragment,d),p(fE.$$.fragment,d),p(mE.$$.fragment,d),p(gE.$$.fragment,d),p(hE.$$.fragment,d),p(pE.$$.fragment,d),p(bE.$$.fragment,d),p(vE.$$.fragment,d),p(TE.$$.fragment,d),p(FE.$$.fragment,d),p(CE.$$.fragment,d),p(yE.$$.fragment,d),p(ih.$$.fragment,d),p(wE.$$.fragment,d),p(AE.$$.fragment,d),p(LE.$$.fragment,d),p(BE.$$.fragment,d),p(RE.$$.fragment,d),p(vh.$$.fragment,d),p(SE.$$.fragment,d),p(PE.$$.fragment,d),p($E.$$.fragment,d),p(IE.$$.fragment,d),p(NE.$$.fragment,d),p(DE.$$.fragment,d),p(qE.$$.fragment,d),p(GE.$$.fragment,d),p(OE.$$.fragment,d),p(XE.$$.fragment,d),p(VE.$$.fragment,d),p(WE.$$.fragment,d),p(QE.$$.fragment,d),p(HE.$$.fragment,d),p(UE.$$.fragment,d),p(JE.$$.fragment,d),p(KE.$$.fragment,d),p(ZE.$$.fragment,d),p(e3.$$.fragment,d),p(o3.$$.fragment,d),p(r3.$$.fragment,d),p(t3.$$.fragment,d),p(n3.$$.fragment,d),p(s3.$$.fragment,d),p(l3.$$.fragment,d),p(i3.$$.fragment,d),p(d3.$$.fragment,d),p(c3.$$.fragment,d),p(m3.$$.fragment,d),p(g3.$$.fragment,d),p(h3.$$.fragment,d),p(p3.$$.fragment,d),p(_3.$$.fragment,d),p(u3.$$.fragment,d),p(v3.$$.fragment,d),p(T3.$$.fragment,d),p(F3.$$.fragment,d),p(C3.$$.fragment,d),p(M3.$$.fragment,d),p(E3.$$.fragment,d),p(w3.$$.fragment,d),p(A3.$$.fragment,d),p(L3.$$.fragment,d),p(B3.$$.fragment,d),p(k3.$$.fragment,d),p(x3.$$.fragment,d),p(S3.$$.fragment,d),p(P3.$$.fragment,d),p($3.$$.fragment,d),p(I3.$$.fragment,d),p(j3.$$.fragment,d),p(N3.$$.fragment,d),p(q3.$$.fragment,d),p(G3.$$.fragment,d),p(O3.$$.fragment,d),p(X3.$$.fragment,d),p(z3.$$.fragment,d),p(V3.$$.fragment,d),p(Q3.$$.fragment,d),p(H3.$$.fragment,d),p(U3.$$.fragment,d),p(J3.$$.fragment,d),p(Y3.$$.fragment,d),p(K3.$$.fragment,d),p(ey.$$.fragment,d),p(oy.$$.fragment,d),p(ry.$$.fragment,d),p(ty.$$.fragment,d),p(ay.$$.fragment,d),p(ny.$$.fragment,d),p(ly.$$.fragment,d),p(iy.$$.fragment,d),p(dy.$$.fragment,d),p(cy.$$.fragment,d),p(fy.$$.fragment,d),p(my.$$.fragment,d),p(hy.$$.fragment,d),p(py.$$.fragment,d),p(_y.$$.fragment,d),p(uy.$$.fragment,d),p(by.$$.fragment,d),p(vy.$$.fragment,d),p(Fy.$$.fragment,d),p(Cy.$$.fragment,d),p(My.$$.fragment,d),p(Ey.$$.fragment,d),p(yy.$$.fragment,d),p(wy.$$.fragment,d),p(Ly.$$.fragment,d),p(By.$$.fragment,d),p(ky.$$.fragment,d),p(xy.$$.fragment,d),p(Ry.$$.fragment,d),p(Sy.$$.fragment,d),p($y.$$.fragment,d),p(Iy.$$.fragment,d),p(jy.$$.fragment,d),p(Ny.$$.fragment,d),p(Dy.$$.fragment,d),p(qy.$$.fragment,d),p(Oy.$$.fragment,d),p(Xy.$$.fragment,d),p(zy.$$.fragment,d),p(Wy.$$.fragment,d),p(Qy.$$.fragment,d),p(Hy.$$.fragment,d),p(Jy.$$.fragment,d),p(Yy.$$.fragment,d),p(Ky.$$.fragment,d),p(Zy.$$.fragment,d),p(ew.$$.fragment,d),p(ow.$$.fragment,d),p(tw.$$.fragment,d),p(aw.$$.fragment,d),p(nw.$$.fragment,d),p(sw.$$.fragment,d),p(lw.$$.fragment,d),p(iw.$$.fragment,d),p(cw.$$.fragment,d),p(fw.$$.fragment,d),p(mw.$$.fragment,d),p(gw.$$.fragment,d),p(hw.$$.fragment,d),p(pw.$$.fragment,d),p(uw.$$.fragment,d),p(bw.$$.fragment,d),p(vw.$$.fragment,d),p(Tw.$$.fragment,d),p(Fw.$$.fragment,d),p(Cw.$$.fragment,d),p(Ew.$$.fragment,d),p(yw.$$.fragment,d),p(ww.$$.fragment,d),p(Lw.$$.fragment,d),p(Bw.$$.fragment,d),p(kw.$$.fragment,d),p(Rw.$$.fragment,d),p(Sw.$$.fragment,d),p(Pw.$$.fragment,d),p($w.$$.fragment,d),p(Iw.$$.fragment,d),p(jw.$$.fragment,d),p(Dw.$$.fragment,d),p(qw.$$.fragment,d),p(Gw.$$.fragment,d),p(Ow.$$.fragment,d),p(Xw.$$.fragment,d),p(zw.$$.fragment,d),p(Ww.$$.fragment,d),p(Qw.$$.fragment,d),p(Hw.$$.fragment,d),p(Uw.$$.fragment,d),p(Jw.$$.fragment,d),p(Yw.$$.fragment,d),p(Zw.$$.fragment,d),p(eA.$$.fragment,d),p(oA.$$.fragment,d),p(rA.$$.fragment,d),p(tA.$$.fragment,d),p(aA.$$.fragment,d),p(sA.$$.fragment,d),p(lA.$$.fragment,d),p(iA.$$.fragment,d),p(dA.$$.fragment,d),p(cA.$$.fragment,d),p(fA.$$.fragment,d),p(gA.$$.fragment,d),p(hA.$$.fragment,d),p(pA.$$.fragment,d),p(_A.$$.fragment,d),p(uA.$$.fragment,d),p(bA.$$.fragment,d),p(TA.$$.fragment,d),p(FA.$$.fragment,d),p(CA.$$.fragment,d),p(MA.$$.fragment,d),p(EA.$$.fragment,d),p(yA.$$.fragment,d),p(AA.$$.fragment,d),p(LA.$$.fragment,d),p(BA.$$.fragment,d),p(kA.$$.fragment,d),p(xA.$$.fragment,d),p(RA.$$.fragment,d),p(PA.$$.fragment,d),p($A.$$.fragment,d),p(IA.$$.fragment,d),p(jA.$$.fragment,d),p(NA.$$.fragment,d),p(DA.$$.fragment,d),p(GA.$$.fragment,d),p(OA.$$.fragment,d),p(XA.$$.fragment,d),p(zA.$$.fragment,d),p(VA.$$.fragment,d),p(WA.$$.fragment,d),p(HA.$$.fragment,d),p(UA.$$.fragment,d),p(JA.$$.fragment,d),p(YA.$$.fragment,d),p(KA.$$.fragment,d),p(ZA.$$.fragment,d),p(o6.$$.fragment,d),p(r6.$$.fragment,d),p(t6.$$.fragment,d),p(a6.$$.fragment,d),p(n6.$$.fragment,d),p(s6.$$.fragment,d),p(i6.$$.fragment,d),p(d6.$$.fragment,d),p(c6.$$.fragment,d),p(f6.$$.fragment,d),p(m6.$$.fragment,d),p(g6.$$.fragment,d),p(p6.$$.fragment,d),p(_6.$$.fragment,d),p(u6.$$.fragment,d),p(b6.$$.fragment,d),p(v6.$$.fragment,d),p(T6.$$.fragment,d),p(C6.$$.fragment,d),p(M6.$$.fragment,d),p(E6.$$.fragment,d),p(y6.$$.fragment,d),p(w6.$$.fragment,d),p(A6.$$.fragment,d),p(B6.$$.fragment,d),p(k6.$$.fragment,d),p(x6.$$.fragment,d),p(R6.$$.fragment,d),p(S6.$$.fragment,d),p(P6.$$.fragment,d),p(I6.$$.fragment,d),p(j6.$$.fragment,d),p(N6.$$.fragment,d),p(D6.$$.fragment,d),p(q6.$$.fragment,d),p(G6.$$.fragment,d),p(X6.$$.fragment,d),p(z6.$$.fragment,d),p(V6.$$.fragment,d),p(W6.$$.fragment,d),p(Q6.$$.fragment,d),p(H6.$$.fragment,d),p(J6.$$.fragment,d),p(Y6.$$.fragment,d),p(K6.$$.fragment,d),p(Z6.$$.fragment,d),p(e0.$$.fragment,d),p(o0.$$.fragment,d),p(t0.$$.fragment,d),p(a0.$$.fragment,d),p(n0.$$.fragment,d),p(s0.$$.fragment,d),p(l0.$$.fragment,d),p(i0.$$.fragment,d),p(c0.$$.fragment,d),p(f0.$$.fragment,d),p(m0.$$.fragment,d),p(g0.$$.fragment,d),p(h0.$$.fragment,d),p(p0.$$.fragment,d),p(u0.$$.fragment,d),p(b0.$$.fragment,d),p(v0.$$.fragment,d),p(T0.$$.fragment,d),p(F0.$$.fragment,d),p(C0.$$.fragment,d),p(E0.$$.fragment,d),p(y0.$$.fragment,d),p(w0.$$.fragment,d),p(A0.$$.fragment,d),p(L0.$$.fragment,d),p(B0.$$.fragment,d),p(x0.$$.fragment,d),p(R0.$$.fragment,d),p(S0.$$.fragment,d),p($0.$$.fragment,d),p(I0.$$.fragment,d),p(j0.$$.fragment,d),p(D0.$$.fragment,d),p(q0.$$.fragment,d),p(G0.$$.fragment,d),p(O0.$$.fragment,d),rBe=!1},d(d){t(J),d&&t(Ae),d&&t(ie),_(ce),d&&t(Ef),d&&t(sa),d&&t(ye),d&&t(io),d&&t(wf),_($a,d),d&&t(co),d&&t(ge),d&&t(qo),d&&t(Ia),d&&t(t8e),d&&t(Si),_(nE),d&&t(a8e),d&&t(Nn),d&&t(n8e),_(sE,d),d&&t(s8e),d&&t(zL),d&&t(l8e),_(Bf,d),d&&t(i8e),d&&t(Pi),_(lE),d&&t(d8e),d&&t(Go),_(iE),_(fE),_(mE),_(gE),d&&t(c8e),d&&t(Ii),_(hE),d&&t(f8e),d&&t(Oo),_(pE),_(bE),_(vE),_(TE),d&&t(m8e),d&&t(ji),_(FE),d&&t(g8e),d&&t(Xo),_(CE),_(yE),_(ih),_(wE),_(AE),d&&t(h8e),d&&t(Ni),_(LE),d&&t(p8e),d&&t(zo),_(BE),_(RE),_(vh),_(SE),_(PE),d&&t(_8e),d&&t(qi),_($E),d&&t(u8e),d&&t(Vo),_(IE),_(NE),_(DE),_(qE),_(GE),d&&t(b8e),d&&t(Xi),_(OE),d&&t(v8e),d&&t(Wo),_(XE),_(VE),_(WE),_(QE),_(HE),d&&t(T8e),d&&t(Wi),_(UE),d&&t(F8e),d&&t(Qo),_(JE),_(KE),_(ZE),_(e3),_(o3),d&&t(C8e),d&&t(Ui),_(r3),d&&t(M8e),d&&t(Ho),_(t3),_(n3),_(s3),_(l3),_(i3),d&&t(E8e),d&&t(Ki),_(d3),d&&t(y8e),d&&t(Uo),_(c3),_(m3),_(g3),_(h3),_(p3),d&&t(w8e),d&&t(od),_(_3),d&&t(A8e),d&&t(Jo),_(u3),_(v3),_(T3),_(F3),_(C3),d&&t(L8e),d&&t(ad),_(M3),d&&t(B8e),d&&t(Yo),_(E3),_(w3),_(A3),_(L3),_(B3),d&&t(k8e),d&&t(ld),_(k3),d&&t(x8e),d&&t(Ko),_(x3),_(S3),_(P3),_($3),_(I3),d&&t(R8e),d&&t(cd),_(j3),d&&t(S8e),d&&t(Zo),_(N3),_(q3),_(G3),_(O3),_(X3),d&&t(P8e),d&&t(gd),_(z3),d&&t($8e),d&&t(er),_(V3),_(Q3),_(H3),_(U3),_(J3),d&&t(I8e),d&&t(_d),_(Y3),d&&t(j8e),d&&t(or),_(K3),_(ey),_(oy),_(ry),_(ty),d&&t(N8e),d&&t(vd),_(ay),d&&t(D8e),d&&t(rr),_(ny),_(ly),_(iy),_(dy),_(cy),d&&t(q8e),d&&t(Cd),_(fy),d&&t(G8e),d&&t(tr),_(my),_(hy),_(py),_(_y),_(uy),d&&t(O8e),d&&t(yd),_(by),d&&t(X8e),d&&t(ar),_(vy),_(Fy),_(Cy),_(My),_(Ey),d&&t(z8e),d&&t(Ld),_(yy),d&&t(V8e),d&&t(nr),_(wy),_(Ly),_(By),_(ky),_(xy),d&&t(W8e),d&&t(Rd),_(Ry),d&&t(Q8e),d&&t(sr),_(Sy),_($y),_(Iy),_(jy),_(Ny),d&&t(H8e),d&&t($d),_(Dy),d&&t(U8e),d&&t(lr),_(qy),_(Oy),_(Xy),_(zy),_(Wy),d&&t(J8e),d&&t(Nd),_(Qy),d&&t(Y8e),d&&t(ir),_(Hy),_(Jy),_(Yy),_(Ky),_(Zy),d&&t(K8e),d&&t(Od),_(ew),d&&t(Z8e),d&&t(dr),_(ow),_(tw),_(aw),_(nw),_(sw),d&&t(e9e),d&&t(Wd),_(lw),d&&t(o9e),d&&t(cr),_(iw),_(cw),_(fw),_(mw),_(gw),d&&t(r9e),d&&t(Ud),_(hw),d&&t(t9e),d&&t(fr),_(pw),_(uw),_(bw),_(vw),_(Tw),d&&t(a9e),d&&t(Kd),_(Fw),d&&t(n9e),d&&t(mr),_(Cw),_(Ew),_(yw),_(ww),_(Lw),d&&t(s9e),d&&t(oc),_(Bw),d&&t(l9e),d&&t(gr),_(kw),_(Rw),_(Sw),_(Pw),_($w),d&&t(i9e),d&&t(ac),_(Iw),d&&t(d9e),d&&t(hr),_(jw),_(Dw),_(qw),_(Gw),_(Ow),d&&t(c9e),d&&t(lc),_(Xw),d&&t(f9e),d&&t(pr),_(zw),_(Ww),_(Qw),_(Hw),_(Uw),d&&t(m9e),d&&t(cc),_(Jw),d&&t(g9e),d&&t(_r),_(Yw),_(Zw),_(eA),_(oA),_(rA),d&&t(h9e),d&&t(gc),_(tA),d&&t(p9e),d&&t(ur),_(aA),_(sA),_(lA),_(iA),_(dA),d&&t(_9e),d&&t(_c),_(cA),d&&t(u9e),d&&t(br),_(fA),_(gA),_(hA),_(pA),_(_A),d&&t(b9e),d&&t(vc),_(uA),d&&t(v9e),d&&t(vr),_(bA),_(TA),_(FA),_(CA),_(MA),d&&t(T9e),d&&t(Cc),_(EA),d&&t(F9e),d&&t(Tr),_(yA),_(AA),_(LA),_(BA),_(kA),d&&t(C9e),d&&t(yc),_(xA),d&&t(M9e),d&&t(Fr),_(RA),_(PA),_($A),_(IA),_(jA),d&&t(E9e),d&&t(Lc),_(NA),d&&t(y9e),d&&t(Cr),_(DA),_(GA),_(OA),_(XA),_(zA),d&&t(w9e),d&&t(xc),_(VA),d&&t(A9e),d&&t(Mr),_(WA),_(HA),_(UA),_(JA),_(YA),d&&t(L9e),d&&t(Pc),_(KA),d&&t(B9e),d&&t(Er),_(ZA),_(o6),_(r6),_(t6),_(a6),d&&t(k9e),d&&t(jc),_(n6),d&&t(x9e),d&&t(yr),_(s6),_(i6),_(d6),_(c6),_(f6),d&&t(R9e),d&&t(qc),_(m6),d&&t(S9e),d&&t(wr),_(g6),_(p6),_(_6),_(u6),_(b6),d&&t(P9e),d&&t(Xc),_(v6),d&&t($9e),d&&t(Ar),_(T6),_(C6),_(M6),_(E6),_(y6),d&&t(I9e),d&&t(Wc),_(w6),d&&t(j9e),d&&t(Lr),_(A6),_(B6),_(k6),_(x6),_(R6),d&&t(N9e),d&&t(Uc),_(S6),d&&t(D9e),d&&t(Br),_(P6),_(I6),_(j6),_(N6),_(D6),d&&t(q9e),d&&t(Kc),_(q6),d&&t(G9e),d&&t(kr),_(G6),_(X6),_(z6),_(V6),_(W6),d&&t(O9e),d&&t(of),_(Q6),d&&t(X9e),d&&t(xr),_(H6),_(J6),_(Y6),_(K6),_(Z6),d&&t(z9e),d&&t(af),_(e0),d&&t(V9e),d&&t(Rr),_(o0),_(t0),_(a0),_(n0),_(s0),d&&t(W9e),d&&t(lf),_(l0),d&&t(Q9e),d&&t(Sr),_(i0),_(c0),_(f0),_(m0),_(g0),d&&t(H9e),d&&t(ff),_(h0),d&&t(U9e),d&&t(Pr),_(p0),_(u0),_(b0),_(v0),_(T0),d&&t(J9e),d&&t(hf),_(F0),d&&t(Y9e),d&&t($r),_(C0),_(E0),_(y0),_(w0),_(A0),d&&t(K9e),d&&t(uf),_(L0),d&&t(Z9e),d&&t(Ir),_(B0),_(x0),_(R0),_(S0),_($0),d&&t(eBe),d&&t(Tf),_(I0),d&&t(oBe),d&&t(jr),_(j0),_(D0),_(q0),_(G0),_(O0)}}}const y_t={local:"auto-classes",sections:[{local:"extending-the-auto-classes",title:"Extending the Auto Classes"},{local:"transformers.AutoConfig",title:"AutoConfig"},{local:"transformers.AutoTokenizer",title:"AutoTokenizer"},{local:"transformers.AutoFeatureExtractor",title:"AutoFeatureExtractor"},{local:"transformers.AutoProcessor",title:"AutoProcessor"},{local:"transformers.AutoModel",title:"AutoModel"},{local:"transformers.AutoModelForPreTraining",title:"AutoModelForPreTraining"},{local:"transformers.AutoModelForCausalLM",title:"AutoModelForCausalLM"},{local:"transformers.AutoModelForMaskedLM",title:"AutoModelForMaskedLM"},{local:"transformers.AutoModelForSeq2SeqLM",title:"AutoModelForSeq2SeqLM"},{local:"transformers.AutoModelForSequenceClassification",title:"AutoModelForSequenceClassification"},{local:"transformers.AutoModelForMultipleChoice",title:"AutoModelForMultipleChoice"},{local:"transformers.AutoModelForNextSentencePrediction",title:"AutoModelForNextSentencePrediction"},{local:"transformers.AutoModelForTokenClassification",title:"AutoModelForTokenClassification"},{local:"transformers.AutoModelForQuestionAnswering",title:"AutoModelForQuestionAnswering"},{local:"transformers.AutoModelForTableQuestionAnswering",title:"AutoModelForTableQuestionAnswering"},{local:"transformers.AutoModelForImageClassification",title:"AutoModelForImageClassification"},{local:"transformers.AutoModelForVision2Seq",title:"AutoModelForVision2Seq"},{local:"transformers.AutoModelForAudioClassification",title:"AutoModelForAudioClassification"},{local:"transformers.AutoModelForAudioFrameClassification",title:"AutoModelForAudioFrameClassification"},{local:"transformers.AutoModelForCTC",title:"AutoModelForCTC"},{local:"transformers.AutoModelForSpeechSeq2Seq",title:"AutoModelForSpeechSeq2Seq"},{local:"transformers.AutoModelForAudioXVector",title:"AutoModelForAudioXVector"},{local:"transformers.AutoModelForMaskedImageModeling",title:"AutoModelForMaskedImageModeling"},{local:"transformers.AutoModelForObjectDetection",title:"AutoModelForObjectDetection"},{local:"transformers.AutoModelForImageSegmentation",title:"AutoModelForImageSegmentation"},{local:"transformers.AutoModelForSemanticSegmentation",title:"AutoModelForSemanticSegmentation"},{local:"transformers.TFAutoModel",title:"TFAutoModel"},{local:"transformers.TFAutoModelForPreTraining",title:"TFAutoModelForPreTraining"},{local:"transformers.TFAutoModelForCausalLM",title:"TFAutoModelForCausalLM"},{local:"transformers.TFAutoModelForImageClassification",title:"TFAutoModelForImageClassification"},{local:"transformers.TFAutoModelForMaskedLM",title:"TFAutoModelForMaskedLM"},{local:"transformers.TFAutoModelForSeq2SeqLM",title:"TFAutoModelForSeq2SeqLM"},{local:"transformers.TFAutoModelForSequenceClassification",title:"TFAutoModelForSequenceClassification"},{local:"transformers.TFAutoModelForMultipleChoice",title:"TFAutoModelForMultipleChoice"},{local:"transformers.TFAutoModelForTableQuestionAnswering",title:"TFAutoModelForTableQuestionAnswering"},{local:"transformers.TFAutoModelForTokenClassification",title:"TFAutoModelForTokenClassification"},{local:"transformers.TFAutoModelForQuestionAnswering",title:"TFAutoModelForQuestionAnswering"},{local:"transformers.TFAutoModelForVision2Seq",title:"TFAutoModelForVision2Seq"},{local:"transformers.TFAutoModelForSpeechSeq2Seq",title:"TFAutoModelForSpeechSeq2Seq"},{local:"transformers.FlaxAutoModel",title:"FlaxAutoModel"},{local:"transformers.FlaxAutoModelForCausalLM",title:"FlaxAutoModelForCausalLM"},{local:"transformers.FlaxAutoModelForPreTraining",title:"FlaxAutoModelForPreTraining"},{local:"transformers.FlaxAutoModelForMaskedLM",title:"FlaxAutoModelForMaskedLM"},{local:"transformers.FlaxAutoModelForSeq2SeqLM",title:"FlaxAutoModelForSeq2SeqLM"},{local:"transformers.FlaxAutoModelForSequenceClassification",title:"FlaxAutoModelForSequenceClassification"},{local:"transformers.FlaxAutoModelForQuestionAnswering",title:"FlaxAutoModelForQuestionAnswering"},{local:"transformers.FlaxAutoModelForTokenClassification",title:"FlaxAutoModelForTokenClassification"},{local:"transformers.FlaxAutoModelForMultipleChoice",title:"FlaxAutoModelForMultipleChoice"},{local:"transformers.FlaxAutoModelForNextSentencePrediction",title:"FlaxAutoModelForNextSentencePrediction"},{local:"transformers.FlaxAutoModelForImageClassification",title:"FlaxAutoModelForImageClassification"},{local:"transformers.FlaxAutoModelForVision2Seq",title:"FlaxAutoModelForVision2Seq"}],title:"Auto Classes"};function w_t(yi,J,Ae){let{fw:ie}=J;return yi.$$set=me=>{"fw"in me&&Ae(0,ie=me.fw)},[ie]}class S_t extends u_t{constructor(J){super();b_t(this,J,w_t,E_t,v_t,{fw:0})}}export{S_t as default,y_t as metadata};
