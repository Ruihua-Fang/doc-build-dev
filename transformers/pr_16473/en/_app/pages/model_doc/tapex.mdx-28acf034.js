import{S as Qp,i as Rp,s as Wp,e as n,k as i,w as F,t,M as Mp,c as r,d as a,m as c,a as s,x as N,h as o,b as d,F as e,g as u,y as B,L as Up,q as X,o as Q,B as R,v as Gp}from"../../chunks/vendor-6b77c823.js";import{D as Vc}from"../../chunks/Docstring-abef54e3.js";import{C as Hc}from"../../chunks/CodeBlock-3a8b25a8.js";import{I as Tn}from"../../chunks/IconCopyLink-7a11ce68.js";function Vp(Jc){let W,Va,M,ee,ht,ye,kn,pt,yn,Ha,U,te,ut,qe,qn,ft,xn,Ja,oe,Cn,xe,Pn,An,Za,Qe,Dn,Ka,L,Re,Ce,zn,On,Ln,We,Pe,$n,In,jn,Me,Ae,Sn,Fn,Nn,Ue,De,Bn,Xn,Ya,Ge,Qn,en,Ve,mt,Rn,tn,He,Wn,on,$,gt,Mn,Un,vt,Gn,Vn,G,Hn,_t,Jn,Zn,bt,Kn,Yn,er,V,tr,Et,or,ar,wt,nr,rr,an,H,ae,Tt,ze,sr,kt,lr,nn,Je,ir,rn,Oe,sn,ne,cr,Ze,dr,hr,ln,Le,cn,re,pr,Ke,ur,fr,dn,$e,hn,J,se,yt,Ie,mr,qt,gr,pn,v,je,vr,xt,_r,br,Ct,Er,wr,Pt,Tr,kr,At,yr,qr,Dt,xr,Cr,Se,Pr,Ye,Ar,Dr,zr,_,Fe,Or,b,Lr,zt,$r,Ir,Ot,jr,Sr,Lt,Fr,Nr,$t,Br,Xr,It,Qr,Rr,et,Wr,Mr,jt,Ur,Gr,St,Vr,Hr,Jr,P,Ft,le,Nt,Zr,Kr,Bt,Yr,es,ts,Xt,ie,Qt,os,as,Rt,ns,rs,ss,Wt,k,Mt,ls,is,Ut,cs,ds,Gt,hs,ps,Vt,us,fs,tt,ms,gs,Ht,vs,_s,Jt,bs,Es,ws,Zt,j,Kt,Ts,ks,Yt,ys,qs,eo,xs,Cs,Ps,to,ce,oo,As,Ds,ao,zs,Os,Ls,no,de,ro,$s,Is,so,js,Ss,Fs,lo,I,io,Ns,Bs,co,Xs,Qs,ho,Rs,Ws,po,Ms,Us,Gs,p,Vs,uo,Hs,Js,fo,Zs,Ks,mo,Ys,el,go,tl,ol,vo,al,nl,_o,rl,sl,bo,ll,il,Eo,cl,dl,wo,hl,pl,To,ul,fl,ko,ml,gl,yo,vl,_l,ot,bl,El,qo,wl,Tl,kl,Z,he,xo,yl,ql,Co,xl,Cl,Pl,pe,Po,Al,Dl,Ao,zl,Ol,Ll,ue,Do,$l,Il,zo,jl,Sl,Fl,E,Nl,Oo,Bl,Xl,Lo,Ql,Rl,$o,Wl,Ml,Io,Ul,Gl,jo,Vl,Hl,at,Jl,Zl,So,Kl,Yl,Fo,ei,ti,oi,K,fe,No,ai,ni,Bo,ri,si,li,me,Xo,ii,ci,Qo,di,hi,pi,w,Ro,ui,fi,Wo,mi,gi,Mo,vi,_i,Uo,bi,Ei,Go,wi,Ti,nt,ki,yi,Vo,qi,xi,Ho,Ci,Pi,Ai,Jo,Di,zi,T,Zo,ge,Ko,Oi,Li,Yo,$i,Ii,ji,ea,S,ta,Si,Fi,oa,Ni,Bi,aa,Xi,Qi,Ri,na,ve,ra,Wi,Mi,sa,Ui,Gi,Vi,la,_e,ia,Hi,Ji,ca,Zi,Ki,Yi,da,f,ha,ec,tc,pa,oc,ac,ua,nc,rc,fa,sc,lc,ma,ic,cc,ga,dc,hc,va,pc,uc,_a,fc,mc,ba,gc,vc,Ea,_c,bc,wa,Ec,wc,Ta,Tc,kc,rt,yc,qc,ka,xc,Cc,Pc,ya,be,qa,Ac,Dc,xa,zc,Oc,Lc,Ca,Ee,Pa,$c,Ic,Aa,jc,Sc,Fc,Da,we,za,Nc,Bc,Oa,Xc,Qc,Rc,st,Ne,un;return ye=new Tn({}),qe=new Tn({}),ze=new Tn({}),Oe=new Hc({props:{code:`from transformers import TapexTokenizer, BartForConditionalGeneration
import pandas as pd

tokenizer = TapexTokenizer.from_pretrained("microsoft/tapex-large-finetuned-wtq")
model = BartForConditionalGeneration.from_pretrained("microsoft/tapex-large-finetuned-wtq")

# prepare table + question
data = {"Actors": ["Brad Pitt", "Leonardo Di Caprio", "George Clooney"], "Number of movies": ["87", "53", "69"]}
table = pd.DataFrame.from_dict(data)
question = "how many movies does Leonardo Di Caprio have?"

encoding = tokenizer(table, question, return_tensors="pt")

# let the model generate an answer autoregressively
outputs = model.generate(**encoding)

# decode back to text
tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]
# should print '53'`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TapexTokenizer, BartForConditionalGeneration
<span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd

tokenizer = TapexTokenizer.from_pretrained(<span class="hljs-string">&quot;microsoft/tapex-large-finetuned-wtq&quot;</span>)
model = BartForConditionalGeneration.from_pretrained(<span class="hljs-string">&quot;microsoft/tapex-large-finetuned-wtq&quot;</span>)

<span class="hljs-comment"># prepare table + question</span>
data = {<span class="hljs-string">&quot;Actors&quot;</span>: [<span class="hljs-string">&quot;Brad Pitt&quot;</span>, <span class="hljs-string">&quot;Leonardo Di Caprio&quot;</span>, <span class="hljs-string">&quot;George Clooney&quot;</span>], <span class="hljs-string">&quot;Number of movies&quot;</span>: [<span class="hljs-string">&quot;87&quot;</span>, <span class="hljs-string">&quot;53&quot;</span>, <span class="hljs-string">&quot;69&quot;</span>]}
table = pd.DataFrame.from_dict(data)
question = <span class="hljs-string">&quot;how many movies does Leonardo Di Caprio have?&quot;</span>

encoding = tokenizer(table, question, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-comment"># let the model generate an answer autoregressively</span>
outputs = model.generate(**encoding)

<span class="hljs-comment"># decode back to text</span>
tokenizer.batch_decode(outputs, skip_special_tokens=<span class="hljs-literal">True</span>)[<span class="hljs-number">0</span>]
<span class="hljs-comment"># should print &#x27;53&#x27;</span>`}}),Le=new Hc({props:{code:`# prepare table + question
data = {"Actors": ["Brad Pitt", "Leonardo Di Caprio", "George Clooney"], "Number of movies": ["87", "53", "69"]}
table = pd.DataFrame.from_dict(data)
questions = [
    "how many movies does Leonardo Di Caprio have?",
    "which actor has 69 movies?",
    "what's the first name of the actor who has 87 movies?",
]
encoding = tokenizer(table, questions, padding=True, return_tensors="pt")

# let the model generate an answer autoregressively
outputs = model.generate(**encoding)

# decode back to text
tokenizer.batch_decode(outputs, skip_special_tokens=True)
# should print '['53', 'george clooney', 'brad pitt']'`,highlighted:`<span class="hljs-comment"># prepare table + question</span>
data = {<span class="hljs-string">&quot;Actors&quot;</span>: [<span class="hljs-string">&quot;Brad Pitt&quot;</span>, <span class="hljs-string">&quot;Leonardo Di Caprio&quot;</span>, <span class="hljs-string">&quot;George Clooney&quot;</span>], <span class="hljs-string">&quot;Number of movies&quot;</span>: [<span class="hljs-string">&quot;87&quot;</span>, <span class="hljs-string">&quot;53&quot;</span>, <span class="hljs-string">&quot;69&quot;</span>]}
table = pd.DataFrame.from_dict(data)
questions = [
    <span class="hljs-string">&quot;how many movies does Leonardo Di Caprio have?&quot;</span>,
    <span class="hljs-string">&quot;which actor has 69 movies?&quot;</span>,
    <span class="hljs-string">&quot;what&#x27;s the first name of the actor who has 87 movies?&quot;</span>,
]
encoding = tokenizer(table, questions, padding=<span class="hljs-literal">True</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-comment"># let the model generate an answer autoregressively</span>
outputs = model.generate(**encoding)

<span class="hljs-comment"># decode back to text</span>
tokenizer.batch_decode(outputs, skip_special_tokens=<span class="hljs-literal">True</span>)
<span class="hljs-comment"># should print &#x27;[&#x27;53&#x27;, &#x27;george clooney&#x27;, &#x27;brad pitt&#x27;]&#x27;</span>`}}),$e=new Hc({props:{code:`from transformers import TapexTokenizer, BartForSequenceClassification

tokenizer = TapexTokenizer.from_pretrained("microsoft/tapex-large-finetuned-tabfact")
model = BartForSequenceClassification.from_pretrained("microsoft/tapex-large-finetuned-tabfact")

# prepare table + sentence
data = {"Actors": ["Brad Pitt", "Leonardo Di Caprio", "George Clooney"], "Number of movies": ["87", "53", "69"]}
table = pd.DataFrame.from_dict(data)
sentence = "George Clooney has 30 movies"

encoding = tokenizer(table, sentence, return_tensors="pt")

# forward pass
outputs = model(**encoding)

# print prediction
print(outputs.logits.argmax(-1).item())
# should print '0' (which means, refuted)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TapexTokenizer, BartForSequenceClassification

tokenizer = TapexTokenizer.from_pretrained(<span class="hljs-string">&quot;microsoft/tapex-large-finetuned-tabfact&quot;</span>)
model = BartForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;microsoft/tapex-large-finetuned-tabfact&quot;</span>)

<span class="hljs-comment"># prepare table + sentence</span>
data = {<span class="hljs-string">&quot;Actors&quot;</span>: [<span class="hljs-string">&quot;Brad Pitt&quot;</span>, <span class="hljs-string">&quot;Leonardo Di Caprio&quot;</span>, <span class="hljs-string">&quot;George Clooney&quot;</span>], <span class="hljs-string">&quot;Number of movies&quot;</span>: [<span class="hljs-string">&quot;87&quot;</span>, <span class="hljs-string">&quot;53&quot;</span>, <span class="hljs-string">&quot;69&quot;</span>]}
table = pd.DataFrame.from_dict(data)
sentence = <span class="hljs-string">&quot;George Clooney has 30 movies&quot;</span>

encoding = tokenizer(table, sentence, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-comment"># forward pass</span>
outputs = model(**encoding)

<span class="hljs-comment"># print prediction</span>
<span class="hljs-built_in">print</span>(outputs.logits.argmax(-<span class="hljs-number">1</span>).item())
<span class="hljs-comment"># should print &#x27;0&#x27; (which means, refuted)</span>`}}),Ie=new Tn({}),je=new Vc({props:{name:"class transformers.TapexTokenizer",anchor:"transformers.TapexTokenizer",parameters:[{name:"vocab_file",val:""},{name:"merges_file",val:""},{name:"do_lower_case",val:" = True"},{name:"errors",val:" = 'replace'"},{name:"bos_token",val:" = '<s>'"},{name:"eos_token",val:" = '</s>'"},{name:"sep_token",val:" = '</s>'"},{name:"cls_token",val:" = '<s>'"},{name:"unk_token",val:" = '<unk>'"},{name:"pad_token",val:" = '<pad>'"},{name:"mask_token",val:" = '<mask>'"},{name:"add_prefix_space",val:" = False"},{name:"max_cell_length",val:" = 15"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_16473/src/transformers/models/tapex/tokenization_tapex.py#L228",parametersDescription:[{anchor:"transformers.TapexTokenizer.vocab_file",description:`<strong>vocab_file</strong> (<code>str</code>) &#x2014;
Path to the vocabulary file.`,name:"vocab_file"},{anchor:"transformers.TapexTokenizer.merges_file",description:`<strong>merges_file</strong> (<code>str</code>) &#x2014;
Path to the merges file.`,name:"merges_file"},{anchor:"transformers.TapexTokenizer.do_lower_case",description:`<strong>do_lower_case</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to lowercase the input when tokenizing.`,name:"do_lower_case"},{anchor:"transformers.TapexTokenizer.errors",description:`<strong>errors</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;replace&quot;</code>) &#x2014;
Paradigm to follow when decoding bytes to UTF-8. See
<a href="https://docs.python.org/3/library/stdtypes.html#bytes.decode" rel="nofollow">bytes.decode</a> for more information.`,name:"errors"},{anchor:"transformers.TapexTokenizer.bos_token",description:`<strong>bos_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;s&gt;&quot;</code>) &#x2014;
The beginning of sequence token that was used during pretraining. Can be used a sequence classifier token.</p>
<div class="course-tip  bg-gradient-to-br dark:bg-gradient-to-r before:border-green-500 dark:before:border-green-800 from-green-50 dark:from-gray-900 to-white dark:to-gray-950 border border-green-50 text-green-700 dark:text-gray-400">
						
<p>When building a sequence using special tokens, this is not the token that is used for the beginning of
sequence. The token used is the <code>cls_token</code>.</p>

					</div>`,name:"bos_token"},{anchor:"transformers.TapexTokenizer.eos_token",description:`<strong>eos_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;/s&gt;&quot;</code>) &#x2014;
The end of sequence token.</p>
<div class="course-tip  bg-gradient-to-br dark:bg-gradient-to-r before:border-green-500 dark:before:border-green-800 from-green-50 dark:from-gray-900 to-white dark:to-gray-950 border border-green-50 text-green-700 dark:text-gray-400">
						
<p>When building a sequence using special tokens, this is not the token that is used for the end of sequence.
The token used is the <code>sep_token</code>.</p>

					</div>`,name:"eos_token"},{anchor:"transformers.TapexTokenizer.sep_token",description:`<strong>sep_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;/s&gt;&quot;</code>) &#x2014;
The separator token, which is used when building a sequence from multiple sequences, e.g. two sequences for
sequence classification or for a text and a question for question answering. It is also used as the last
token of a sequence built with special tokens.`,name:"sep_token"},{anchor:"transformers.TapexTokenizer.cls_token",description:`<strong>cls_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;s&gt;&quot;</code>) &#x2014;
The classifier token which is used when doing sequence classification (classification of the whole sequence
instead of per-token classification). It is the first token of the sequence when built with special tokens.`,name:"cls_token"},{anchor:"transformers.TapexTokenizer.unk_token",description:`<strong>unk_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;unk&gt;&quot;</code>) &#x2014;
The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this
token instead.`,name:"unk_token"},{anchor:"transformers.TapexTokenizer.pad_token",description:`<strong>pad_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;pad&gt;&quot;</code>) &#x2014;
The token used for padding, for example when batching sequences of different lengths.`,name:"pad_token"},{anchor:"transformers.TapexTokenizer.mask_token",description:`<strong>mask_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;mask&gt;&quot;</code>) &#x2014;
The token used for masking values. This is the token used when training this model with masked language
modeling. This is the token which the model will try to predict.`,name:"mask_token"},{anchor:"transformers.TapexTokenizer.add_prefix_space",description:`<strong>add_prefix_space</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to add an initial space to the input. This allows to treat the leading word just as any
other word. (BART tokenizer detect beginning of words by the preceding space).`,name:"add_prefix_space"},{anchor:"transformers.TapexTokenizer.max_cell_length",description:`<strong>max_cell_length</strong> (<code>int</code>, <em>optional</em>, defaults to 15) &#x2014;
Maximum number of characters per cell when linearizing a table. If this number is exceeded, truncation
takes place.`,name:"max_cell_length"}]}}),Fe=new Vc({props:{name:"__call__",anchor:"transformers.TapexTokenizer.__call__",parameters:[{name:"table",val:": typing.Union[ForwardRef('pd.DataFrame'), typing.List[ForwardRef('pd.DataFrame')]] = None"},{name:"query",val:": typing.Union[str, typing.List[str], NoneType] = None"},{name:"answer",val:": typing.Union[str, typing.List[str]] = None"},{name:"add_special_tokens",val:": bool = True"},{name:"padding",val:": typing.Union[bool, str, transformers.utils.generic.PaddingStrategy] = False"},{name:"truncation",val:": typing.Union[bool, str, transformers.tokenization_utils_base.TruncationStrategy] = False"},{name:"max_length",val:": typing.Optional[int] = None"},{name:"stride",val:": int = 0"},{name:"pad_to_multiple_of",val:": typing.Optional[int] = None"},{name:"return_tensors",val:": typing.Union[str, transformers.utils.generic.TensorType, NoneType] = None"},{name:"return_token_type_ids",val:": typing.Optional[bool] = None"},{name:"return_attention_mask",val:": typing.Optional[bool] = None"},{name:"return_overflowing_tokens",val:": bool = False"},{name:"return_special_tokens_mask",val:": bool = False"},{name:"return_offsets_mapping",val:": bool = False"},{name:"return_length",val:": bool = False"},{name:"verbose",val:": bool = True"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_16473/src/transformers/models/tapex/tokenization_tapex.py#L550"}}),Ne=new Vc({props:{name:"save_vocabulary",anchor:"transformers.TapexTokenizer.save_vocabulary",parameters:[{name:"save_directory",val:": str"},{name:"filename_prefix",val:": typing.Optional[str] = None"}],source:"https://github.com/huggingface/transformers/blob/pr_16473/src/transformers/models/tapex/tokenization_tapex.py#L521"}}),{c(){W=n("meta"),Va=i(),M=n("h1"),ee=n("a"),ht=n("span"),F(ye.$$.fragment),kn=i(),pt=n("span"),yn=t("TAPEX"),Ha=i(),U=n("h2"),te=n("a"),ut=n("span"),F(qe.$$.fragment),qn=i(),ft=n("span"),xn=t("Overview"),Ja=i(),oe=n("p"),Cn=t("The TAPEX model was proposed in "),xe=n("a"),Pn=t("TAPEX: Table Pre-training via Learning a Neural SQL Executor"),An=t(` by Qian Liu,
Bei Chen, Jiaqi Guo, Morteza Ziyadi, Zeqi Lin, Weizhu Chen, Jian-Guang Lou. TAPEX pre-trains a BART model to solve synthetic SQL queries, after
which it can be fine-tuned to answer natural language questions related to tabular data, as well as performing table fact checking.`),Za=i(),Qe=n("p"),Dn=t("TAPEX has been fine-tuned on several datasets:"),Ka=i(),L=n("ul"),Re=n("li"),Ce=n("a"),zn=t("SQA"),On=t(" (Sequential Question Answering by Microsoft)"),Ln=i(),We=n("li"),Pe=n("a"),$n=t("WTQ"),In=t(" (Wiki Table Questions by Stanford University)"),jn=i(),Me=n("li"),Ae=n("a"),Sn=t("WikiSQL"),Fn=t(" (by Salesforce)"),Nn=i(),Ue=n("li"),De=n("a"),Bn=t("TabFact"),Xn=t(" (by USCB NLP Lab)."),Ya=i(),Ge=n("p"),Qn=t("The abstract from the paper is the following:"),en=i(),Ve=n("p"),mt=n("em"),Rn=t(`Recent progress in language model pre-training has achieved a great success via leveraging large-scale unstructured textual data. However, it is
still a challenge to apply pre-training on structured tabular data due to the absence of large-scale high-quality tabular data. In this paper, we
propose TAPEX to show that table pre-training can be achieved by learning a neural SQL executor over a synthetic corpus, which is obtained by automatically
synthesizing executable SQL queries and their execution outputs. TAPEX addresses the data scarcity challenge via guiding the language model to mimic a SQL
executor on the diverse, large-scale and high-quality synthetic corpus. We evaluate TAPEX on four benchmark datasets. Experimental results demonstrate that
TAPEX outperforms previous table pre-training approaches by a large margin and achieves new state-of-the-art results on all of them. This includes improvements
on the weakly-supervised WikiSQL denotation accuracy to 89.5% (+2.3%), the WikiTableQuestions denotation accuracy to 57.5% (+4.8%), the SQA denotation accuracy
to 74.5% (+3.5%), and the TabFact accuracy to 84.2% (+3.2%). To our knowledge, this is the first work to exploit table pre-training via synthetic executable programs
and to achieve new state-of-the-art results on various downstream tasks.`),tn=i(),He=n("p"),Wn=t("Tips:"),on=i(),$=n("ul"),gt=n("li"),Mn=t("TAPEX is a generative (seq2seq) model. One can directly plug in the weights of TAPEX into a BART model."),Un=i(),vt=n("li"),Gn=t("TAPEX has checkpoints on the hub that are either pre-trained only, or fine-tuned on WTQ, SQA, WikiSQL and TabFact."),Vn=i(),G=n("li"),Hn=t("Sentences + tables are presented to the model as "),_t=n("code"),Jn=t('sentence + " " + linearized table'),Zn=t(`. The linearized table has the following format:
`),bt=n("code"),Kn=t("col: col1 | col2 | col 3 row 1 : val1 | val2 | val3 row 2 : ..."),Yn=t("."),er=i(),V=n("li"),tr=t(`TAPEX has its own tokenizer, that allows to prepare all data for the model easily. One can pass Pandas DataFrames and strings to the tokenizer,
and it will automatically create the `),Et=n("code"),or=t("input_ids"),ar=t(" and "),wt=n("code"),nr=t("attention_mask"),rr=t(" (as shown in the usage examples below)."),an=i(),H=n("h2"),ae=n("a"),Tt=n("span"),F(ze.$$.fragment),sr=i(),kt=n("span"),lr=t("Usage: inference"),nn=i(),Je=n("p"),ir=t("Below, we illustrate how to use TAPEX for TableQA. As one can see, one can directly plug in the weights of TAPEX into a BART model."),rn=i(),F(Oe.$$.fragment),sn=i(),ne=n("p"),cr=t("Note that "),Ze=n("a"),dr=t("TapexTokenizer"),hr=t(` also supports batched inference. Hence, one can provide a batch of different tables/questions, or a batch of a single table
and multiple questions, or a batch of a single query and multiple tables. Let\u2019s illustrate this:`),ln=i(),F(Le.$$.fragment),cn=i(),re=n("p"),pr=t(`In case one wants to do table verification (i.e. the task of determining whether a given sentence is supported or refuted by the contents
of a table), one can instantiate a `),Ke=n("a"),ur=t("BartForSequenceClassification"),fr=t(` model. TAPEX has checkpoints on the hub fine-tuned on TabFact, an important
benchmark for table fact checking (it achieves 84% accuracy).`),dn=i(),F($e.$$.fragment),hn=i(),J=n("h2"),se=n("a"),yt=n("span"),F(Ie.$$.fragment),mr=i(),qt=n("span"),gr=t("TapexTokenizer"),pn=i(),v=n("div"),F(je.$$.fragment),vr=i(),xt=n("p"),_r=t("Construct a TAPEX tokenizer. Based on byte-level Byte-Pair-Encoding (BPE)."),br=i(),Ct=n("p"),Er=t(`This tokenizer can be used to flatten one or more table(s) and concatenate them with one or more related sentences
to be used by TAPEX models. The format that the TAPEX tokenizer creates is the following:`),wr=i(),Pt=n("p"),Tr=t("sentence col: col1 | col2 | col 3 row 1 : val1 | val2 | val3 row 2 : \u2026"),kr=i(),At=n("p"),yr=t(`The tokenizer supports a single table + single query, a single table and multiple queries (in which case the table
will be duplicated for every query), a single query and multiple tables (in which case the query will be duplicated
for every table), and multiple tables and queries. In other words, you can provide a batch of tables + questions to
the tokenizer for instance to prepare them for the model.`),qr=i(),Dt=n("p"),xr=t("Tokenization itself is based on the BPE algorithm. It is identical to the one used by BART, RoBERTa and GPT-2."),Cr=i(),Se=n("p"),Pr=t("This tokenizer inherits from "),Ye=n("a"),Ar=t("PreTrainedTokenizer"),Dr=t(` which contains most of the main methods. Users should refer to
this superclass for more information regarding those methods.`),zr=i(),_=n("div"),F(Fe.$$.fragment),Or=i(),b=n("p"),Lr=t("add_special_tokens ("),zt=n("code"),$r=t("bool"),Ir=t(", "),Ot=n("em"),jr=t("optional"),Sr=t(", defaults to "),Lt=n("code"),Fr=t("True"),Nr=t(`):
Whether or not to encode the sequences with the special tokens relative to their model.
padding (`),$t=n("code"),Br=t("bool"),Xr=t(", "),It=n("code"),Qr=t("str"),Rr=t(" or "),et=n("a"),Wr=t("PaddingStrategy"),Mr=t(", "),jt=n("em"),Ur=t("optional"),Gr=t(", defaults to "),St=n("code"),Vr=t("False"),Hr=t(`):
Activates and controls padding. Accepts the following values:`),Jr=i(),P=n("ul"),Ft=n("li"),le=n("p"),Nt=n("code"),Zr=t("True"),Kr=t(" or "),Bt=n("code"),Yr=t("'longest'"),es=t(`: Pad to the longest sequence in the batch (or no padding if only a single
sequence if provided).`),ts=i(),Xt=n("li"),ie=n("p"),Qt=n("code"),os=t("'max_length'"),as=t(": Pad to a maximum length specified with the argument "),Rt=n("code"),ns=t("max_length"),rs=t(` or to the maximum
acceptable input length for the model if that argument is not provided.`),ss=i(),Wt=n("li"),k=n("p"),Mt=n("code"),ls=t("False"),is=t(" or "),Ut=n("code"),cs=t("'do_not_pad'"),ds=t(` (default): No padding (i.e., can output a batch with sequences of different
lengths).
truncation (`),Gt=n("code"),hs=t("bool"),ps=t(", "),Vt=n("code"),us=t("str"),fs=t(" or "),tt=n("a"),ms=t("TruncationStrategy"),gs=t(", "),Ht=n("em"),vs=t("optional"),_s=t(", defaults to "),Jt=n("code"),bs=t("False"),Es=t(`):
Activates and controls truncation. Accepts the following values:`),ws=i(),Zt=n("li"),j=n("p"),Kt=n("code"),Ts=t("True"),ks=t(" or "),Yt=n("code"),ys=t("'longest_first'"),qs=t(": Truncate to a maximum length specified with the argument "),eo=n("code"),xs=t("max_length"),Cs=t(` or
to the maximum acceptable input length for the model if that argument is not provided. This will
truncate token by token, removing a token from the longest sequence in the pair if a pair of
sequences (or a batch of pairs) is provided.`),Ps=i(),to=n("li"),ce=n("p"),oo=n("code"),As=t("'only_first'"),Ds=t(": Truncate to a maximum length specified with the argument "),ao=n("code"),zs=t("max_length"),Os=t(` or to the
maximum acceptable input length for the model if that argument is not provided. This will only
truncate the first sequence of a pair if a pair of sequences (or a batch of pairs) is provided.`),Ls=i(),no=n("li"),de=n("p"),ro=n("code"),$s=t("'only_second'"),Is=t(": Truncate to a maximum length specified with the argument "),so=n("code"),js=t("max_length"),Ss=t(` or to the
maximum acceptable input length for the model if that argument is not provided. This will only
truncate the second sequence of a pair if a pair of sequences (or a batch of pairs) is provided.`),Fs=i(),lo=n("li"),I=n("p"),io=n("code"),Ns=t("False"),Bs=t(" or "),co=n("code"),Xs=t("'do_not_truncate'"),Qs=t(` (default): No truncation (i.e., can output batch with sequence lengths
greater than the model maximum admissible input size).
max_length (`),ho=n("code"),Rs=t("int"),Ws=t(", "),po=n("em"),Ms=t("optional"),Us=t(`):
Controls the maximum length to use by one of the truncation/padding parameters.`),Gs=i(),p=n("p"),Vs=t("If left unset or set to "),uo=n("code"),Hs=t("None"),Js=t(`, this will use the predefined model maximum length if a maximum length
is required by one of the truncation/padding parameters. If the model has no specific maximum input
length (like XLNet) truncation/padding to a maximum length will be deactivated.
stride (`),fo=n("code"),Zs=t("int"),Ks=t(", "),mo=n("em"),Ys=t("optional"),el=t(`, defaults to 0):
If set to a number along with `),go=n("code"),tl=t("max_length"),ol=t(`, the overflowing tokens returned when
`),vo=n("code"),al=t("return_overflowing_tokens=True"),nl=t(` will contain some tokens from the end of the truncated sequence
returned to provide some overlap between truncated and overflowing sequences. The value of this
argument defines the number of overlapping tokens.
is_split_into_words (`),_o=n("code"),rl=t("bool"),sl=t(", "),bo=n("em"),ll=t("optional"),il=t(", defaults to "),Eo=n("code"),cl=t("False"),dl=t(`):
Whether or not the input is already pre-tokenized (e.g., split into words). If set to `),wo=n("code"),hl=t("True"),pl=t(`, the
tokenizer assumes the input is already split into words (for instance, by splitting it on whitespace)
which it will tokenize. This is useful for NER or token classification.
pad_to_multiple_of (`),To=n("code"),ul=t("int"),fl=t(", "),ko=n("em"),ml=t("optional"),gl=t(`):
If set will pad the sequence to a multiple of the provided value. This is especially useful to enable
the use of Tensor Cores on NVIDIA hardware with compute capability >= 7.5 (Volta).
return_tensors (`),yo=n("code"),vl=t("str"),_l=t(" or "),ot=n("a"),bl=t("TensorType"),El=t(", "),qo=n("em"),wl=t("optional"),Tl=t(`):
If set, will return tensors instead of list of python integers. Acceptable values are:`),kl=i(),Z=n("ul"),he=n("li"),xo=n("code"),yl=t("'tf'"),ql=t(": Return TensorFlow "),Co=n("code"),xl=t("tf.constant"),Cl=t(" objects."),Pl=i(),pe=n("li"),Po=n("code"),Al=t("'pt'"),Dl=t(": Return PyTorch "),Ao=n("code"),zl=t("torch.Tensor"),Ol=t(" objects."),Ll=i(),ue=n("li"),Do=n("code"),$l=t("'np'"),Il=t(": Return Numpy "),zo=n("code"),jl=t("np.ndarray"),Sl=t(" objects."),Fl=i(),E=n("p"),Nl=t("add_special_tokens ("),Oo=n("code"),Bl=t("bool"),Xl=t(", "),Lo=n("em"),Ql=t("optional"),Rl=t(", defaults to "),$o=n("code"),Wl=t("True"),Ml=t(`):
Whether or not to encode the sequences with the special tokens relative to their model.
padding (`),Io=n("code"),Ul=t("bool"),Gl=t(", "),jo=n("code"),Vl=t("str"),Hl=t(" or "),at=n("a"),Jl=t("PaddingStrategy"),Zl=t(", "),So=n("em"),Kl=t("optional"),Yl=t(", defaults to "),Fo=n("code"),ei=t("False"),ti=t(`):
Activates and controls padding. Accepts the following values:`),oi=i(),K=n("ul"),fe=n("li"),No=n("code"),ai=t("True"),ni=t(" or "),Bo=n("code"),ri=t("'longest'"),si=t(`: Pad to the longest sequence in the batch (or no padding if only a single
sequence if provided).`),li=i(),me=n("li"),Xo=n("code"),ii=t("'max_length'"),ci=t(": Pad to a maximum length specified with the argument "),Qo=n("code"),di=t("max_length"),hi=t(` or to the maximum
acceptable input length for the model if that argument is not provided.`),pi=i(),w=n("li"),Ro=n("code"),ui=t("False"),fi=t(" or "),Wo=n("code"),mi=t("'do_not_pad'"),gi=t(` (default): No padding (i.e., can output a batch with sequences of different
lengths).
truncation (`),Mo=n("code"),vi=t("bool"),_i=t(", "),Uo=n("code"),bi=t("str"),Ei=t(", "),Go=n("code"),wi=t("TapexTruncationStrategy"),Ti=t(" or "),nt=n("a"),ki=t("TruncationStrategy"),yi=t(`,
`),Vo=n("em"),qi=t("optional"),xi=t(", defaults to "),Ho=n("code"),Ci=t("False"),Pi=t("):"),Ai=i(),Jo=n("p"),Di=t("Activates and controls truncation. Accepts the following values:"),zi=i(),T=n("ul"),Zo=n("li"),ge=n("p"),Ko=n("code"),Oi=t("'drop_rows_to_fit'"),Li=t(": Truncate to a maximum length specified with the argument "),Yo=n("code"),$i=t("max_length"),Ii=t(` or to the
maximum acceptable input length for the model if that argument is not provided. This will truncate
row by row, removing rows from the table.`),ji=i(),ea=n("li"),S=n("p"),ta=n("code"),Si=t("True"),Fi=t(" or "),oa=n("code"),Ni=t("'longest_first'"),Bi=t(": Truncate to a maximum length specified with the argument "),aa=n("code"),Xi=t("max_length"),Qi=t(` or
to the maximum acceptable input length for the model if that argument is not provided. This will
truncate token by token, removing a token from the longest sequence in the pair if a pair of
sequences (or a batch of pairs) is provided.`),Ri=i(),na=n("li"),ve=n("p"),ra=n("code"),Wi=t("'only_first'"),Mi=t(": Truncate to a maximum length specified with the argument "),sa=n("code"),Ui=t("max_length"),Gi=t(` or to the
maximum acceptable input length for the model if that argument is not provided. This will only
truncate the first sequence of a pair if a pair of sequences (or a batch of pairs) is provided.`),Vi=i(),la=n("li"),_e=n("p"),ia=n("code"),Hi=t("'only_second'"),Ji=t(": Truncate to a maximum length specified with the argument "),ca=n("code"),Zi=t("max_length"),Ki=t(` or to the
maximum acceptable input length for the model if that argument is not provided. This will only
truncate the second sequence of a pair if a pair of sequences (or a batch of pairs) is provided.`),Yi=i(),da=n("li"),f=n("p"),ha=n("code"),ec=t("False"),tc=t(" or "),pa=n("code"),oc=t("'do_not_truncate'"),ac=t(` (default): No truncation (i.e., can output batch with sequence lengths
greater than the model maximum admissible input size).
max_length (`),ua=n("code"),nc=t("int"),rc=t(", "),fa=n("em"),sc=t("optional"),lc=t(`):
Controls the maximum length to use by one of the truncation/padding parameters. If left unset or set to
`),ma=n("code"),ic=t("None"),cc=t(`, this will use the predefined model maximum length if a maximum length is required by one of the
truncation/padding parameters. If the model has no specific maximum input length (like XLNet)
truncation/padding to a maximum length will be deactivated.
stride (`),ga=n("code"),dc=t("int"),hc=t(", "),va=n("em"),pc=t("optional"),uc=t(`, defaults to 0):
If set to a number along with `),_a=n("code"),fc=t("max_length"),mc=t(`, the overflowing tokens returned when
`),ba=n("code"),gc=t("return_overflowing_tokens=True"),vc=t(` will contain some tokens from the end of the truncated sequence
returned to provide some overlap between truncated and overflowing sequences. The value of this
argument defines the number of overlapping tokens.
pad_to_multiple_of (`),Ea=n("code"),_c=t("int"),bc=t(", "),wa=n("em"),Ec=t("optional"),wc=t(`):
If set will pad the sequence to a multiple of the provided value. This is especially useful to enable
the use of Tensor Cores on NVIDIA hardware with compute capability >= 7.5 (Volta).
return_tensors (`),Ta=n("code"),Tc=t("str"),kc=t(" or "),rt=n("a"),yc=t("TensorType"),qc=t(", "),ka=n("em"),xc=t("optional"),Cc=t(`):
If set, will return tensors instead of list of python integers. Acceptable values are:`),Pc=i(),ya=n("li"),be=n("p"),qa=n("code"),Ac=t("'tf'"),Dc=t(": Return TensorFlow "),xa=n("code"),zc=t("tf.constant"),Oc=t(" objects."),Lc=i(),Ca=n("li"),Ee=n("p"),Pa=n("code"),$c=t("'pt'"),Ic=t(": Return PyTorch "),Aa=n("code"),jc=t("torch.Tensor"),Sc=t(" objects."),Fc=i(),Da=n("li"),we=n("p"),za=n("code"),Nc=t("'np'"),Bc=t(": Return Numpy "),Oa=n("code"),Xc=t("np.ndarray"),Qc=t(" objects."),Rc=i(),st=n("div"),F(Ne.$$.fragment),this.h()},l(l){const h=Mp('[data-svelte="svelte-1phssyn"]',document.head);W=r(h,"META",{name:!0,content:!0}),h.forEach(a),Va=c(l),M=r(l,"H1",{class:!0});var fn=s(M);ee=r(fn,"A",{id:!0,class:!0,href:!0});var Zc=s(ee);ht=r(Zc,"SPAN",{});var Kc=s(ht);N(ye.$$.fragment,Kc),Kc.forEach(a),Zc.forEach(a),kn=c(fn),pt=r(fn,"SPAN",{});var Yc=s(pt);yn=o(Yc,"TAPEX"),Yc.forEach(a),fn.forEach(a),Ha=c(l),U=r(l,"H2",{class:!0});var mn=s(U);te=r(mn,"A",{id:!0,class:!0,href:!0});var ed=s(te);ut=r(ed,"SPAN",{});var td=s(ut);N(qe.$$.fragment,td),td.forEach(a),ed.forEach(a),qn=c(mn),ft=r(mn,"SPAN",{});var od=s(ft);xn=o(od,"Overview"),od.forEach(a),mn.forEach(a),Ja=c(l),oe=r(l,"P",{});var gn=s(oe);Cn=o(gn,"The TAPEX model was proposed in "),xe=r(gn,"A",{href:!0,rel:!0});var ad=s(xe);Pn=o(ad,"TAPEX: Table Pre-training via Learning a Neural SQL Executor"),ad.forEach(a),An=o(gn,` by Qian Liu,
Bei Chen, Jiaqi Guo, Morteza Ziyadi, Zeqi Lin, Weizhu Chen, Jian-Guang Lou. TAPEX pre-trains a BART model to solve synthetic SQL queries, after
which it can be fine-tuned to answer natural language questions related to tabular data, as well as performing table fact checking.`),gn.forEach(a),Za=c(l),Qe=r(l,"P",{});var nd=s(Qe);Dn=o(nd,"TAPEX has been fine-tuned on several datasets:"),nd.forEach(a),Ka=c(l),L=r(l,"UL",{});var Te=s(L);Re=r(Te,"LI",{});var Wc=s(Re);Ce=r(Wc,"A",{href:!0,rel:!0});var rd=s(Ce);zn=o(rd,"SQA"),rd.forEach(a),On=o(Wc," (Sequential Question Answering by Microsoft)"),Wc.forEach(a),Ln=c(Te),We=r(Te,"LI",{});var Mc=s(We);Pe=r(Mc,"A",{href:!0,rel:!0});var sd=s(Pe);$n=o(sd,"WTQ"),sd.forEach(a),In=o(Mc," (Wiki Table Questions by Stanford University)"),Mc.forEach(a),jn=c(Te),Me=r(Te,"LI",{});var Uc=s(Me);Ae=r(Uc,"A",{href:!0,rel:!0});var ld=s(Ae);Sn=o(ld,"WikiSQL"),ld.forEach(a),Fn=o(Uc," (by Salesforce)"),Uc.forEach(a),Nn=c(Te),Ue=r(Te,"LI",{});var Gc=s(Ue);De=r(Gc,"A",{href:!0,rel:!0});var id=s(De);Bn=o(id,"TabFact"),id.forEach(a),Xn=o(Gc," (by USCB NLP Lab)."),Gc.forEach(a),Te.forEach(a),Ya=c(l),Ge=r(l,"P",{});var cd=s(Ge);Qn=o(cd,"The abstract from the paper is the following:"),cd.forEach(a),en=c(l),Ve=r(l,"P",{});var dd=s(Ve);mt=r(dd,"EM",{});var hd=s(mt);Rn=o(hd,`Recent progress in language model pre-training has achieved a great success via leveraging large-scale unstructured textual data. However, it is
still a challenge to apply pre-training on structured tabular data due to the absence of large-scale high-quality tabular data. In this paper, we
propose TAPEX to show that table pre-training can be achieved by learning a neural SQL executor over a synthetic corpus, which is obtained by automatically
synthesizing executable SQL queries and their execution outputs. TAPEX addresses the data scarcity challenge via guiding the language model to mimic a SQL
executor on the diverse, large-scale and high-quality synthetic corpus. We evaluate TAPEX on four benchmark datasets. Experimental results demonstrate that
TAPEX outperforms previous table pre-training approaches by a large margin and achieves new state-of-the-art results on all of them. This includes improvements
on the weakly-supervised WikiSQL denotation accuracy to 89.5% (+2.3%), the WikiTableQuestions denotation accuracy to 57.5% (+4.8%), the SQA denotation accuracy
to 74.5% (+3.5%), and the TabFact accuracy to 84.2% (+3.2%). To our knowledge, this is the first work to exploit table pre-training via synthetic executable programs
and to achieve new state-of-the-art results on various downstream tasks.`),hd.forEach(a),dd.forEach(a),tn=c(l),He=r(l,"P",{});var pd=s(He);Wn=o(pd,"Tips:"),pd.forEach(a),on=c(l),$=r(l,"UL",{});var ke=s($);gt=r(ke,"LI",{});var ud=s(gt);Mn=o(ud,"TAPEX is a generative (seq2seq) model. One can directly plug in the weights of TAPEX into a BART model."),ud.forEach(a),Un=c(ke),vt=r(ke,"LI",{});var fd=s(vt);Gn=o(fd,"TAPEX has checkpoints on the hub that are either pre-trained only, or fine-tuned on WTQ, SQA, WikiSQL and TabFact."),fd.forEach(a),Vn=c(ke),G=r(ke,"LI",{});var lt=s(G);Hn=o(lt,"Sentences + tables are presented to the model as "),_t=r(lt,"CODE",{});var md=s(_t);Jn=o(md,'sentence + " " + linearized table'),md.forEach(a),Zn=o(lt,`. The linearized table has the following format:
`),bt=r(lt,"CODE",{});var gd=s(bt);Kn=o(gd,"col: col1 | col2 | col 3 row 1 : val1 | val2 | val3 row 2 : ..."),gd.forEach(a),Yn=o(lt,"."),lt.forEach(a),er=c(ke),V=r(ke,"LI",{});var it=s(V);tr=o(it,`TAPEX has its own tokenizer, that allows to prepare all data for the model easily. One can pass Pandas DataFrames and strings to the tokenizer,
and it will automatically create the `),Et=r(it,"CODE",{});var vd=s(Et);or=o(vd,"input_ids"),vd.forEach(a),ar=o(it," and "),wt=r(it,"CODE",{});var _d=s(wt);nr=o(_d,"attention_mask"),_d.forEach(a),rr=o(it," (as shown in the usage examples below)."),it.forEach(a),ke.forEach(a),an=c(l),H=r(l,"H2",{class:!0});var vn=s(H);ae=r(vn,"A",{id:!0,class:!0,href:!0});var bd=s(ae);Tt=r(bd,"SPAN",{});var Ed=s(Tt);N(ze.$$.fragment,Ed),Ed.forEach(a),bd.forEach(a),sr=c(vn),kt=r(vn,"SPAN",{});var wd=s(kt);lr=o(wd,"Usage: inference"),wd.forEach(a),vn.forEach(a),nn=c(l),Je=r(l,"P",{});var Td=s(Je);ir=o(Td,"Below, we illustrate how to use TAPEX for TableQA. As one can see, one can directly plug in the weights of TAPEX into a BART model."),Td.forEach(a),rn=c(l),N(Oe.$$.fragment,l),sn=c(l),ne=r(l,"P",{});var _n=s(ne);cr=o(_n,"Note that "),Ze=r(_n,"A",{href:!0});var kd=s(Ze);dr=o(kd,"TapexTokenizer"),kd.forEach(a),hr=o(_n,` also supports batched inference. Hence, one can provide a batch of different tables/questions, or a batch of a single table
and multiple questions, or a batch of a single query and multiple tables. Let\u2019s illustrate this:`),_n.forEach(a),ln=c(l),N(Le.$$.fragment,l),cn=c(l),re=r(l,"P",{});var bn=s(re);pr=o(bn,`In case one wants to do table verification (i.e. the task of determining whether a given sentence is supported or refuted by the contents
of a table), one can instantiate a `),Ke=r(bn,"A",{href:!0});var yd=s(Ke);ur=o(yd,"BartForSequenceClassification"),yd.forEach(a),fr=o(bn,` model. TAPEX has checkpoints on the hub fine-tuned on TabFact, an important
benchmark for table fact checking (it achieves 84% accuracy).`),bn.forEach(a),dn=c(l),N($e.$$.fragment,l),hn=c(l),J=r(l,"H2",{class:!0});var En=s(J);se=r(En,"A",{id:!0,class:!0,href:!0});var qd=s(se);yt=r(qd,"SPAN",{});var xd=s(yt);N(Ie.$$.fragment,xd),xd.forEach(a),qd.forEach(a),mr=c(En),qt=r(En,"SPAN",{});var Cd=s(qt);gr=o(Cd,"TapexTokenizer"),Cd.forEach(a),En.forEach(a),pn=c(l),v=r(l,"DIV",{class:!0});var y=s(v);N(je.$$.fragment,y),vr=c(y),xt=r(y,"P",{});var Pd=s(xt);_r=o(Pd,"Construct a TAPEX tokenizer. Based on byte-level Byte-Pair-Encoding (BPE)."),Pd.forEach(a),br=c(y),Ct=r(y,"P",{});var Ad=s(Ct);Er=o(Ad,`This tokenizer can be used to flatten one or more table(s) and concatenate them with one or more related sentences
to be used by TAPEX models. The format that the TAPEX tokenizer creates is the following:`),Ad.forEach(a),wr=c(y),Pt=r(y,"P",{});var Dd=s(Pt);Tr=o(Dd,"sentence col: col1 | col2 | col 3 row 1 : val1 | val2 | val3 row 2 : \u2026"),Dd.forEach(a),kr=c(y),At=r(y,"P",{});var zd=s(At);yr=o(zd,`The tokenizer supports a single table + single query, a single table and multiple queries (in which case the table
will be duplicated for every query), a single query and multiple tables (in which case the query will be duplicated
for every table), and multiple tables and queries. In other words, you can provide a batch of tables + questions to
the tokenizer for instance to prepare them for the model.`),zd.forEach(a),qr=c(y),Dt=r(y,"P",{});var Od=s(Dt);xr=o(Od,"Tokenization itself is based on the BPE algorithm. It is identical to the one used by BART, RoBERTa and GPT-2."),Od.forEach(a),Cr=c(y),Se=r(y,"P",{});var wn=s(Se);Pr=o(wn,"This tokenizer inherits from "),Ye=r(wn,"A",{href:!0});var Ld=s(Ye);Ar=o(Ld,"PreTrainedTokenizer"),Ld.forEach(a),Dr=o(wn,` which contains most of the main methods. Users should refer to
this superclass for more information regarding those methods.`),wn.forEach(a),zr=c(y),_=r(y,"DIV",{class:!0});var q=s(_);N(Fe.$$.fragment,q),Or=c(q),b=r(q,"P",{});var x=s(b);Lr=o(x,"add_special_tokens ("),zt=r(x,"CODE",{});var $d=s(zt);$r=o($d,"bool"),$d.forEach(a),Ir=o(x,", "),Ot=r(x,"EM",{});var Id=s(Ot);jr=o(Id,"optional"),Id.forEach(a),Sr=o(x,", defaults to "),Lt=r(x,"CODE",{});var jd=s(Lt);Fr=o(jd,"True"),jd.forEach(a),Nr=o(x,`):
Whether or not to encode the sequences with the special tokens relative to their model.
padding (`),$t=r(x,"CODE",{});var Sd=s($t);Br=o(Sd,"bool"),Sd.forEach(a),Xr=o(x,", "),It=r(x,"CODE",{});var Fd=s(It);Qr=o(Fd,"str"),Fd.forEach(a),Rr=o(x," or "),et=r(x,"A",{href:!0});var Nd=s(et);Wr=o(Nd,"PaddingStrategy"),Nd.forEach(a),Mr=o(x,", "),jt=r(x,"EM",{});var Bd=s(jt);Ur=o(Bd,"optional"),Bd.forEach(a),Gr=o(x,", defaults to "),St=r(x,"CODE",{});var Xd=s(St);Vr=o(Xd,"False"),Xd.forEach(a),Hr=o(x,`):
Activates and controls padding. Accepts the following values:`),x.forEach(a),Jr=c(q),P=r(q,"UL",{});var O=s(P);Ft=r(O,"LI",{});var Qd=s(Ft);le=r(Qd,"P",{});var La=s(le);Nt=r(La,"CODE",{});var Rd=s(Nt);Zr=o(Rd,"True"),Rd.forEach(a),Kr=o(La," or "),Bt=r(La,"CODE",{});var Wd=s(Bt);Yr=o(Wd,"'longest'"),Wd.forEach(a),es=o(La,`: Pad to the longest sequence in the batch (or no padding if only a single
sequence if provided).`),La.forEach(a),Qd.forEach(a),ts=c(O),Xt=r(O,"LI",{});var Md=s(Xt);ie=r(Md,"P",{});var $a=s(ie);Qt=r($a,"CODE",{});var Ud=s(Qt);os=o(Ud,"'max_length'"),Ud.forEach(a),as=o($a,": Pad to a maximum length specified with the argument "),Rt=r($a,"CODE",{});var Gd=s(Rt);ns=o(Gd,"max_length"),Gd.forEach(a),rs=o($a,` or to the maximum
acceptable input length for the model if that argument is not provided.`),$a.forEach(a),Md.forEach(a),ss=c(O),Wt=r(O,"LI",{});var Vd=s(Wt);k=r(Vd,"P",{});var z=s(k);Mt=r(z,"CODE",{});var Hd=s(Mt);ls=o(Hd,"False"),Hd.forEach(a),is=o(z," or "),Ut=r(z,"CODE",{});var Jd=s(Ut);cs=o(Jd,"'do_not_pad'"),Jd.forEach(a),ds=o(z,` (default): No padding (i.e., can output a batch with sequences of different
lengths).
truncation (`),Gt=r(z,"CODE",{});var Zd=s(Gt);hs=o(Zd,"bool"),Zd.forEach(a),ps=o(z,", "),Vt=r(z,"CODE",{});var Kd=s(Vt);us=o(Kd,"str"),Kd.forEach(a),fs=o(z," or "),tt=r(z,"A",{href:!0});var Yd=s(tt);ms=o(Yd,"TruncationStrategy"),Yd.forEach(a),gs=o(z,", "),Ht=r(z,"EM",{});var eh=s(Ht);vs=o(eh,"optional"),eh.forEach(a),_s=o(z,", defaults to "),Jt=r(z,"CODE",{});var th=s(Jt);bs=o(th,"False"),th.forEach(a),Es=o(z,`):
Activates and controls truncation. Accepts the following values:`),z.forEach(a),Vd.forEach(a),ws=c(O),Zt=r(O,"LI",{});var oh=s(Zt);j=r(oh,"P",{});var Be=s(j);Kt=r(Be,"CODE",{});var ah=s(Kt);Ts=o(ah,"True"),ah.forEach(a),ks=o(Be," or "),Yt=r(Be,"CODE",{});var nh=s(Yt);ys=o(nh,"'longest_first'"),nh.forEach(a),qs=o(Be,": Truncate to a maximum length specified with the argument "),eo=r(Be,"CODE",{});var rh=s(eo);xs=o(rh,"max_length"),rh.forEach(a),Cs=o(Be,` or
to the maximum acceptable input length for the model if that argument is not provided. This will
truncate token by token, removing a token from the longest sequence in the pair if a pair of
sequences (or a batch of pairs) is provided.`),Be.forEach(a),oh.forEach(a),Ps=c(O),to=r(O,"LI",{});var sh=s(to);ce=r(sh,"P",{});var Ia=s(ce);oo=r(Ia,"CODE",{});var lh=s(oo);As=o(lh,"'only_first'"),lh.forEach(a),Ds=o(Ia,": Truncate to a maximum length specified with the argument "),ao=r(Ia,"CODE",{});var ih=s(ao);zs=o(ih,"max_length"),ih.forEach(a),Os=o(Ia,` or to the
maximum acceptable input length for the model if that argument is not provided. This will only
truncate the first sequence of a pair if a pair of sequences (or a batch of pairs) is provided.`),Ia.forEach(a),sh.forEach(a),Ls=c(O),no=r(O,"LI",{});var ch=s(no);de=r(ch,"P",{});var ja=s(de);ro=r(ja,"CODE",{});var dh=s(ro);$s=o(dh,"'only_second'"),dh.forEach(a),Is=o(ja,": Truncate to a maximum length specified with the argument "),so=r(ja,"CODE",{});var hh=s(so);js=o(hh,"max_length"),hh.forEach(a),Ss=o(ja,` or to the
maximum acceptable input length for the model if that argument is not provided. This will only
truncate the second sequence of a pair if a pair of sequences (or a batch of pairs) is provided.`),ja.forEach(a),ch.forEach(a),Fs=c(O),lo=r(O,"LI",{});var ph=s(lo);I=r(ph,"P",{});var Y=s(I);io=r(Y,"CODE",{});var uh=s(io);Ns=o(uh,"False"),uh.forEach(a),Bs=o(Y," or "),co=r(Y,"CODE",{});var fh=s(co);Xs=o(fh,"'do_not_truncate'"),fh.forEach(a),Qs=o(Y,` (default): No truncation (i.e., can output batch with sequence lengths
greater than the model maximum admissible input size).
max_length (`),ho=r(Y,"CODE",{});var mh=s(ho);Rs=o(mh,"int"),mh.forEach(a),Ws=o(Y,", "),po=r(Y,"EM",{});var gh=s(po);Ms=o(gh,"optional"),gh.forEach(a),Us=o(Y,`):
Controls the maximum length to use by one of the truncation/padding parameters.`),Y.forEach(a),ph.forEach(a),O.forEach(a),Gs=c(q),p=r(q,"P",{});var m=s(p);Vs=o(m,"If left unset or set to "),uo=r(m,"CODE",{});var vh=s(uo);Hs=o(vh,"None"),vh.forEach(a),Js=o(m,`, this will use the predefined model maximum length if a maximum length
is required by one of the truncation/padding parameters. If the model has no specific maximum input
length (like XLNet) truncation/padding to a maximum length will be deactivated.
stride (`),fo=r(m,"CODE",{});var _h=s(fo);Zs=o(_h,"int"),_h.forEach(a),Ks=o(m,", "),mo=r(m,"EM",{});var bh=s(mo);Ys=o(bh,"optional"),bh.forEach(a),el=o(m,`, defaults to 0):
If set to a number along with `),go=r(m,"CODE",{});var Eh=s(go);tl=o(Eh,"max_length"),Eh.forEach(a),ol=o(m,`, the overflowing tokens returned when
`),vo=r(m,"CODE",{});var wh=s(vo);al=o(wh,"return_overflowing_tokens=True"),wh.forEach(a),nl=o(m,` will contain some tokens from the end of the truncated sequence
returned to provide some overlap between truncated and overflowing sequences. The value of this
argument defines the number of overlapping tokens.
is_split_into_words (`),_o=r(m,"CODE",{});var Th=s(_o);rl=o(Th,"bool"),Th.forEach(a),sl=o(m,", "),bo=r(m,"EM",{});var kh=s(bo);ll=o(kh,"optional"),kh.forEach(a),il=o(m,", defaults to "),Eo=r(m,"CODE",{});var yh=s(Eo);cl=o(yh,"False"),yh.forEach(a),dl=o(m,`):
Whether or not the input is already pre-tokenized (e.g., split into words). If set to `),wo=r(m,"CODE",{});var qh=s(wo);hl=o(qh,"True"),qh.forEach(a),pl=o(m,`, the
tokenizer assumes the input is already split into words (for instance, by splitting it on whitespace)
which it will tokenize. This is useful for NER or token classification.
pad_to_multiple_of (`),To=r(m,"CODE",{});var xh=s(To);ul=o(xh,"int"),xh.forEach(a),fl=o(m,", "),ko=r(m,"EM",{});var Ch=s(ko);ml=o(Ch,"optional"),Ch.forEach(a),gl=o(m,`):
If set will pad the sequence to a multiple of the provided value. This is especially useful to enable
the use of Tensor Cores on NVIDIA hardware with compute capability >= 7.5 (Volta).
return_tensors (`),yo=r(m,"CODE",{});var Ph=s(yo);vl=o(Ph,"str"),Ph.forEach(a),_l=o(m," or "),ot=r(m,"A",{href:!0});var Ah=s(ot);bl=o(Ah,"TensorType"),Ah.forEach(a),El=o(m,", "),qo=r(m,"EM",{});var Dh=s(qo);wl=o(Dh,"optional"),Dh.forEach(a),Tl=o(m,`):
If set, will return tensors instead of list of python integers. Acceptable values are:`),m.forEach(a),kl=c(q),Z=r(q,"UL",{});var ct=s(Z);he=r(ct,"LI",{});var Sa=s(he);xo=r(Sa,"CODE",{});var zh=s(xo);yl=o(zh,"'tf'"),zh.forEach(a),ql=o(Sa,": Return TensorFlow "),Co=r(Sa,"CODE",{});var Oh=s(Co);xl=o(Oh,"tf.constant"),Oh.forEach(a),Cl=o(Sa," objects."),Sa.forEach(a),Pl=c(ct),pe=r(ct,"LI",{});var Fa=s(pe);Po=r(Fa,"CODE",{});var Lh=s(Po);Al=o(Lh,"'pt'"),Lh.forEach(a),Dl=o(Fa,": Return PyTorch "),Ao=r(Fa,"CODE",{});var $h=s(Ao);zl=o($h,"torch.Tensor"),$h.forEach(a),Ol=o(Fa," objects."),Fa.forEach(a),Ll=c(ct),ue=r(ct,"LI",{});var Na=s(ue);Do=r(Na,"CODE",{});var Ih=s(Do);$l=o(Ih,"'np'"),Ih.forEach(a),Il=o(Na,": Return Numpy "),zo=r(Na,"CODE",{});var jh=s(zo);jl=o(jh,"np.ndarray"),jh.forEach(a),Sl=o(Na," objects."),Na.forEach(a),ct.forEach(a),Fl=c(q),E=r(q,"P",{});var C=s(E);Nl=o(C,"add_special_tokens ("),Oo=r(C,"CODE",{});var Sh=s(Oo);Bl=o(Sh,"bool"),Sh.forEach(a),Xl=o(C,", "),Lo=r(C,"EM",{});var Fh=s(Lo);Ql=o(Fh,"optional"),Fh.forEach(a),Rl=o(C,", defaults to "),$o=r(C,"CODE",{});var Nh=s($o);Wl=o(Nh,"True"),Nh.forEach(a),Ml=o(C,`):
Whether or not to encode the sequences with the special tokens relative to their model.
padding (`),Io=r(C,"CODE",{});var Bh=s(Io);Ul=o(Bh,"bool"),Bh.forEach(a),Gl=o(C,", "),jo=r(C,"CODE",{});var Xh=s(jo);Vl=o(Xh,"str"),Xh.forEach(a),Hl=o(C," or "),at=r(C,"A",{href:!0});var Qh=s(at);Jl=o(Qh,"PaddingStrategy"),Qh.forEach(a),Zl=o(C,", "),So=r(C,"EM",{});var Rh=s(So);Kl=o(Rh,"optional"),Rh.forEach(a),Yl=o(C,", defaults to "),Fo=r(C,"CODE",{});var Wh=s(Fo);ei=o(Wh,"False"),Wh.forEach(a),ti=o(C,`):
Activates and controls padding. Accepts the following values:`),C.forEach(a),oi=c(q),K=r(q,"UL",{});var dt=s(K);fe=r(dt,"LI",{});var Ba=s(fe);No=r(Ba,"CODE",{});var Mh=s(No);ai=o(Mh,"True"),Mh.forEach(a),ni=o(Ba," or "),Bo=r(Ba,"CODE",{});var Uh=s(Bo);ri=o(Uh,"'longest'"),Uh.forEach(a),si=o(Ba,`: Pad to the longest sequence in the batch (or no padding if only a single
sequence if provided).`),Ba.forEach(a),li=c(dt),me=r(dt,"LI",{});var Xa=s(me);Xo=r(Xa,"CODE",{});var Gh=s(Xo);ii=o(Gh,"'max_length'"),Gh.forEach(a),ci=o(Xa,": Pad to a maximum length specified with the argument "),Qo=r(Xa,"CODE",{});var Vh=s(Qo);di=o(Vh,"max_length"),Vh.forEach(a),hi=o(Xa,` or to the maximum
acceptable input length for the model if that argument is not provided.`),Xa.forEach(a),pi=c(dt),w=r(dt,"LI",{});var A=s(w);Ro=r(A,"CODE",{});var Hh=s(Ro);ui=o(Hh,"False"),Hh.forEach(a),fi=o(A," or "),Wo=r(A,"CODE",{});var Jh=s(Wo);mi=o(Jh,"'do_not_pad'"),Jh.forEach(a),gi=o(A,` (default): No padding (i.e., can output a batch with sequences of different
lengths).
truncation (`),Mo=r(A,"CODE",{});var Zh=s(Mo);vi=o(Zh,"bool"),Zh.forEach(a),_i=o(A,", "),Uo=r(A,"CODE",{});var Kh=s(Uo);bi=o(Kh,"str"),Kh.forEach(a),Ei=o(A,", "),Go=r(A,"CODE",{});var Yh=s(Go);wi=o(Yh,"TapexTruncationStrategy"),Yh.forEach(a),Ti=o(A," or "),nt=r(A,"A",{href:!0});var ep=s(nt);ki=o(ep,"TruncationStrategy"),ep.forEach(a),yi=o(A,`,
`),Vo=r(A,"EM",{});var tp=s(Vo);qi=o(tp,"optional"),tp.forEach(a),xi=o(A,", defaults to "),Ho=r(A,"CODE",{});var op=s(Ho);Ci=o(op,"False"),op.forEach(a),Pi=o(A,"):"),A.forEach(a),dt.forEach(a),Ai=c(q),Jo=r(q,"P",{});var ap=s(Jo);Di=o(ap,"Activates and controls truncation. Accepts the following values:"),ap.forEach(a),zi=c(q),T=r(q,"UL",{});var D=s(T);Zo=r(D,"LI",{});var np=s(Zo);ge=r(np,"P",{});var Qa=s(ge);Ko=r(Qa,"CODE",{});var rp=s(Ko);Oi=o(rp,"'drop_rows_to_fit'"),rp.forEach(a),Li=o(Qa,": Truncate to a maximum length specified with the argument "),Yo=r(Qa,"CODE",{});var sp=s(Yo);$i=o(sp,"max_length"),sp.forEach(a),Ii=o(Qa,` or to the
maximum acceptable input length for the model if that argument is not provided. This will truncate
row by row, removing rows from the table.`),Qa.forEach(a),np.forEach(a),ji=c(D),ea=r(D,"LI",{});var lp=s(ea);S=r(lp,"P",{});var Xe=s(S);ta=r(Xe,"CODE",{});var ip=s(ta);Si=o(ip,"True"),ip.forEach(a),Fi=o(Xe," or "),oa=r(Xe,"CODE",{});var cp=s(oa);Ni=o(cp,"'longest_first'"),cp.forEach(a),Bi=o(Xe,": Truncate to a maximum length specified with the argument "),aa=r(Xe,"CODE",{});var dp=s(aa);Xi=o(dp,"max_length"),dp.forEach(a),Qi=o(Xe,` or
to the maximum acceptable input length for the model if that argument is not provided. This will
truncate token by token, removing a token from the longest sequence in the pair if a pair of
sequences (or a batch of pairs) is provided.`),Xe.forEach(a),lp.forEach(a),Ri=c(D),na=r(D,"LI",{});var hp=s(na);ve=r(hp,"P",{});var Ra=s(ve);ra=r(Ra,"CODE",{});var pp=s(ra);Wi=o(pp,"'only_first'"),pp.forEach(a),Mi=o(Ra,": Truncate to a maximum length specified with the argument "),sa=r(Ra,"CODE",{});var up=s(sa);Ui=o(up,"max_length"),up.forEach(a),Gi=o(Ra,` or to the
maximum acceptable input length for the model if that argument is not provided. This will only
truncate the first sequence of a pair if a pair of sequences (or a batch of pairs) is provided.`),Ra.forEach(a),hp.forEach(a),Vi=c(D),la=r(D,"LI",{});var fp=s(la);_e=r(fp,"P",{});var Wa=s(_e);ia=r(Wa,"CODE",{});var mp=s(ia);Hi=o(mp,"'only_second'"),mp.forEach(a),Ji=o(Wa,": Truncate to a maximum length specified with the argument "),ca=r(Wa,"CODE",{});var gp=s(ca);Zi=o(gp,"max_length"),gp.forEach(a),Ki=o(Wa,` or to the
maximum acceptable input length for the model if that argument is not provided. This will only
truncate the second sequence of a pair if a pair of sequences (or a batch of pairs) is provided.`),Wa.forEach(a),fp.forEach(a),Yi=c(D),da=r(D,"LI",{});var vp=s(da);f=r(vp,"P",{});var g=s(f);ha=r(g,"CODE",{});var _p=s(ha);ec=o(_p,"False"),_p.forEach(a),tc=o(g," or "),pa=r(g,"CODE",{});var bp=s(pa);oc=o(bp,"'do_not_truncate'"),bp.forEach(a),ac=o(g,` (default): No truncation (i.e., can output batch with sequence lengths
greater than the model maximum admissible input size).
max_length (`),ua=r(g,"CODE",{});var Ep=s(ua);nc=o(Ep,"int"),Ep.forEach(a),rc=o(g,", "),fa=r(g,"EM",{});var wp=s(fa);sc=o(wp,"optional"),wp.forEach(a),lc=o(g,`):
Controls the maximum length to use by one of the truncation/padding parameters. If left unset or set to
`),ma=r(g,"CODE",{});var Tp=s(ma);ic=o(Tp,"None"),Tp.forEach(a),cc=o(g,`, this will use the predefined model maximum length if a maximum length is required by one of the
truncation/padding parameters. If the model has no specific maximum input length (like XLNet)
truncation/padding to a maximum length will be deactivated.
stride (`),ga=r(g,"CODE",{});var kp=s(ga);dc=o(kp,"int"),kp.forEach(a),hc=o(g,", "),va=r(g,"EM",{});var yp=s(va);pc=o(yp,"optional"),yp.forEach(a),uc=o(g,`, defaults to 0):
If set to a number along with `),_a=r(g,"CODE",{});var qp=s(_a);fc=o(qp,"max_length"),qp.forEach(a),mc=o(g,`, the overflowing tokens returned when
`),ba=r(g,"CODE",{});var xp=s(ba);gc=o(xp,"return_overflowing_tokens=True"),xp.forEach(a),vc=o(g,` will contain some tokens from the end of the truncated sequence
returned to provide some overlap between truncated and overflowing sequences. The value of this
argument defines the number of overlapping tokens.
pad_to_multiple_of (`),Ea=r(g,"CODE",{});var Cp=s(Ea);_c=o(Cp,"int"),Cp.forEach(a),bc=o(g,", "),wa=r(g,"EM",{});var Pp=s(wa);Ec=o(Pp,"optional"),Pp.forEach(a),wc=o(g,`):
If set will pad the sequence to a multiple of the provided value. This is especially useful to enable
the use of Tensor Cores on NVIDIA hardware with compute capability >= 7.5 (Volta).
return_tensors (`),Ta=r(g,"CODE",{});var Ap=s(Ta);Tc=o(Ap,"str"),Ap.forEach(a),kc=o(g," or "),rt=r(g,"A",{href:!0});var Dp=s(rt);yc=o(Dp,"TensorType"),Dp.forEach(a),qc=o(g,", "),ka=r(g,"EM",{});var zp=s(ka);xc=o(zp,"optional"),zp.forEach(a),Cc=o(g,`):
If set, will return tensors instead of list of python integers. Acceptable values are:`),g.forEach(a),vp.forEach(a),Pc=c(D),ya=r(D,"LI",{});var Op=s(ya);be=r(Op,"P",{});var Ma=s(be);qa=r(Ma,"CODE",{});var Lp=s(qa);Ac=o(Lp,"'tf'"),Lp.forEach(a),Dc=o(Ma,": Return TensorFlow "),xa=r(Ma,"CODE",{});var $p=s(xa);zc=o($p,"tf.constant"),$p.forEach(a),Oc=o(Ma," objects."),Ma.forEach(a),Op.forEach(a),Lc=c(D),Ca=r(D,"LI",{});var Ip=s(Ca);Ee=r(Ip,"P",{});var Ua=s(Ee);Pa=r(Ua,"CODE",{});var jp=s(Pa);$c=o(jp,"'pt'"),jp.forEach(a),Ic=o(Ua,": Return PyTorch "),Aa=r(Ua,"CODE",{});var Sp=s(Aa);jc=o(Sp,"torch.Tensor"),Sp.forEach(a),Sc=o(Ua," objects."),Ua.forEach(a),Ip.forEach(a),Fc=c(D),Da=r(D,"LI",{});var Fp=s(Da);we=r(Fp,"P",{});var Ga=s(we);za=r(Ga,"CODE",{});var Np=s(za);Nc=o(Np,"'np'"),Np.forEach(a),Bc=o(Ga,": Return Numpy "),Oa=r(Ga,"CODE",{});var Bp=s(Oa);Xc=o(Bp,"np.ndarray"),Bp.forEach(a),Qc=o(Ga," objects."),Ga.forEach(a),Fp.forEach(a),D.forEach(a),q.forEach(a),Rc=c(y),st=r(y,"DIV",{class:!0});var Xp=s(st);N(Ne.$$.fragment,Xp),Xp.forEach(a),y.forEach(a),this.h()},h(){d(W,"name","hf:doc:metadata"),d(W,"content",JSON.stringify(Hp)),d(ee,"id","tapex"),d(ee,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(ee,"href","#tapex"),d(M,"class","relative group"),d(te,"id","overview"),d(te,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(te,"href","#overview"),d(U,"class","relative group"),d(xe,"href","https://arxiv.org/abs/2107.07653"),d(xe,"rel","nofollow"),d(Ce,"href","https://www.microsoft.com/en-us/download/details.aspx?id=54253"),d(Ce,"rel","nofollow"),d(Pe,"href","https://github.com/ppasupat/WikiTableQuestions"),d(Pe,"rel","nofollow"),d(Ae,"href","https://github.com/salesforce/WikiSQL"),d(Ae,"rel","nofollow"),d(De,"href","https://tabfact.github.io/"),d(De,"rel","nofollow"),d(ae,"id","usage-inference"),d(ae,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(ae,"href","#usage-inference"),d(H,"class","relative group"),d(Ze,"href","/docs/transformers/pr_16473/en/model_doc/tapex#transformers.TapexTokenizer"),d(Ke,"href","/docs/transformers/pr_16473/en/model_doc/bart#transformers.BartForSequenceClassification"),d(se,"id","transformers.TapexTokenizer"),d(se,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(se,"href","#transformers.TapexTokenizer"),d(J,"class","relative group"),d(Ye,"href","/docs/transformers/pr_16473/en/main_classes/tokenizer#transformers.PreTrainedTokenizer"),d(et,"href","/docs/transformers/pr_16473/en/internal/file_utils#transformers.utils.PaddingStrategy"),d(tt,"href","/docs/transformers/pr_16473/en/internal/tokenization_utils#transformers.tokenization_utils_base.TruncationStrategy"),d(ot,"href","/docs/transformers/pr_16473/en/internal/file_utils#transformers.TensorType"),d(at,"href","/docs/transformers/pr_16473/en/internal/file_utils#transformers.utils.PaddingStrategy"),d(nt,"href","/docs/transformers/pr_16473/en/internal/tokenization_utils#transformers.tokenization_utils_base.TruncationStrategy"),d(rt,"href","/docs/transformers/pr_16473/en/internal/file_utils#transformers.TensorType"),d(_,"class","docstring"),d(st,"class","docstring"),d(v,"class","docstring")},m(l,h){e(document.head,W),u(l,Va,h),u(l,M,h),e(M,ee),e(ee,ht),B(ye,ht,null),e(M,kn),e(M,pt),e(pt,yn),u(l,Ha,h),u(l,U,h),e(U,te),e(te,ut),B(qe,ut,null),e(U,qn),e(U,ft),e(ft,xn),u(l,Ja,h),u(l,oe,h),e(oe,Cn),e(oe,xe),e(xe,Pn),e(oe,An),u(l,Za,h),u(l,Qe,h),e(Qe,Dn),u(l,Ka,h),u(l,L,h),e(L,Re),e(Re,Ce),e(Ce,zn),e(Re,On),e(L,Ln),e(L,We),e(We,Pe),e(Pe,$n),e(We,In),e(L,jn),e(L,Me),e(Me,Ae),e(Ae,Sn),e(Me,Fn),e(L,Nn),e(L,Ue),e(Ue,De),e(De,Bn),e(Ue,Xn),u(l,Ya,h),u(l,Ge,h),e(Ge,Qn),u(l,en,h),u(l,Ve,h),e(Ve,mt),e(mt,Rn),u(l,tn,h),u(l,He,h),e(He,Wn),u(l,on,h),u(l,$,h),e($,gt),e(gt,Mn),e($,Un),e($,vt),e(vt,Gn),e($,Vn),e($,G),e(G,Hn),e(G,_t),e(_t,Jn),e(G,Zn),e(G,bt),e(bt,Kn),e(G,Yn),e($,er),e($,V),e(V,tr),e(V,Et),e(Et,or),e(V,ar),e(V,wt),e(wt,nr),e(V,rr),u(l,an,h),u(l,H,h),e(H,ae),e(ae,Tt),B(ze,Tt,null),e(H,sr),e(H,kt),e(kt,lr),u(l,nn,h),u(l,Je,h),e(Je,ir),u(l,rn,h),B(Oe,l,h),u(l,sn,h),u(l,ne,h),e(ne,cr),e(ne,Ze),e(Ze,dr),e(ne,hr),u(l,ln,h),B(Le,l,h),u(l,cn,h),u(l,re,h),e(re,pr),e(re,Ke),e(Ke,ur),e(re,fr),u(l,dn,h),B($e,l,h),u(l,hn,h),u(l,J,h),e(J,se),e(se,yt),B(Ie,yt,null),e(J,mr),e(J,qt),e(qt,gr),u(l,pn,h),u(l,v,h),B(je,v,null),e(v,vr),e(v,xt),e(xt,_r),e(v,br),e(v,Ct),e(Ct,Er),e(v,wr),e(v,Pt),e(Pt,Tr),e(v,kr),e(v,At),e(At,yr),e(v,qr),e(v,Dt),e(Dt,xr),e(v,Cr),e(v,Se),e(Se,Pr),e(Se,Ye),e(Ye,Ar),e(Se,Dr),e(v,zr),e(v,_),B(Fe,_,null),e(_,Or),e(_,b),e(b,Lr),e(b,zt),e(zt,$r),e(b,Ir),e(b,Ot),e(Ot,jr),e(b,Sr),e(b,Lt),e(Lt,Fr),e(b,Nr),e(b,$t),e($t,Br),e(b,Xr),e(b,It),e(It,Qr),e(b,Rr),e(b,et),e(et,Wr),e(b,Mr),e(b,jt),e(jt,Ur),e(b,Gr),e(b,St),e(St,Vr),e(b,Hr),e(_,Jr),e(_,P),e(P,Ft),e(Ft,le),e(le,Nt),e(Nt,Zr),e(le,Kr),e(le,Bt),e(Bt,Yr),e(le,es),e(P,ts),e(P,Xt),e(Xt,ie),e(ie,Qt),e(Qt,os),e(ie,as),e(ie,Rt),e(Rt,ns),e(ie,rs),e(P,ss),e(P,Wt),e(Wt,k),e(k,Mt),e(Mt,ls),e(k,is),e(k,Ut),e(Ut,cs),e(k,ds),e(k,Gt),e(Gt,hs),e(k,ps),e(k,Vt),e(Vt,us),e(k,fs),e(k,tt),e(tt,ms),e(k,gs),e(k,Ht),e(Ht,vs),e(k,_s),e(k,Jt),e(Jt,bs),e(k,Es),e(P,ws),e(P,Zt),e(Zt,j),e(j,Kt),e(Kt,Ts),e(j,ks),e(j,Yt),e(Yt,ys),e(j,qs),e(j,eo),e(eo,xs),e(j,Cs),e(P,Ps),e(P,to),e(to,ce),e(ce,oo),e(oo,As),e(ce,Ds),e(ce,ao),e(ao,zs),e(ce,Os),e(P,Ls),e(P,no),e(no,de),e(de,ro),e(ro,$s),e(de,Is),e(de,so),e(so,js),e(de,Ss),e(P,Fs),e(P,lo),e(lo,I),e(I,io),e(io,Ns),e(I,Bs),e(I,co),e(co,Xs),e(I,Qs),e(I,ho),e(ho,Rs),e(I,Ws),e(I,po),e(po,Ms),e(I,Us),e(_,Gs),e(_,p),e(p,Vs),e(p,uo),e(uo,Hs),e(p,Js),e(p,fo),e(fo,Zs),e(p,Ks),e(p,mo),e(mo,Ys),e(p,el),e(p,go),e(go,tl),e(p,ol),e(p,vo),e(vo,al),e(p,nl),e(p,_o),e(_o,rl),e(p,sl),e(p,bo),e(bo,ll),e(p,il),e(p,Eo),e(Eo,cl),e(p,dl),e(p,wo),e(wo,hl),e(p,pl),e(p,To),e(To,ul),e(p,fl),e(p,ko),e(ko,ml),e(p,gl),e(p,yo),e(yo,vl),e(p,_l),e(p,ot),e(ot,bl),e(p,El),e(p,qo),e(qo,wl),e(p,Tl),e(_,kl),e(_,Z),e(Z,he),e(he,xo),e(xo,yl),e(he,ql),e(he,Co),e(Co,xl),e(he,Cl),e(Z,Pl),e(Z,pe),e(pe,Po),e(Po,Al),e(pe,Dl),e(pe,Ao),e(Ao,zl),e(pe,Ol),e(Z,Ll),e(Z,ue),e(ue,Do),e(Do,$l),e(ue,Il),e(ue,zo),e(zo,jl),e(ue,Sl),e(_,Fl),e(_,E),e(E,Nl),e(E,Oo),e(Oo,Bl),e(E,Xl),e(E,Lo),e(Lo,Ql),e(E,Rl),e(E,$o),e($o,Wl),e(E,Ml),e(E,Io),e(Io,Ul),e(E,Gl),e(E,jo),e(jo,Vl),e(E,Hl),e(E,at),e(at,Jl),e(E,Zl),e(E,So),e(So,Kl),e(E,Yl),e(E,Fo),e(Fo,ei),e(E,ti),e(_,oi),e(_,K),e(K,fe),e(fe,No),e(No,ai),e(fe,ni),e(fe,Bo),e(Bo,ri),e(fe,si),e(K,li),e(K,me),e(me,Xo),e(Xo,ii),e(me,ci),e(me,Qo),e(Qo,di),e(me,hi),e(K,pi),e(K,w),e(w,Ro),e(Ro,ui),e(w,fi),e(w,Wo),e(Wo,mi),e(w,gi),e(w,Mo),e(Mo,vi),e(w,_i),e(w,Uo),e(Uo,bi),e(w,Ei),e(w,Go),e(Go,wi),e(w,Ti),e(w,nt),e(nt,ki),e(w,yi),e(w,Vo),e(Vo,qi),e(w,xi),e(w,Ho),e(Ho,Ci),e(w,Pi),e(_,Ai),e(_,Jo),e(Jo,Di),e(_,zi),e(_,T),e(T,Zo),e(Zo,ge),e(ge,Ko),e(Ko,Oi),e(ge,Li),e(ge,Yo),e(Yo,$i),e(ge,Ii),e(T,ji),e(T,ea),e(ea,S),e(S,ta),e(ta,Si),e(S,Fi),e(S,oa),e(oa,Ni),e(S,Bi),e(S,aa),e(aa,Xi),e(S,Qi),e(T,Ri),e(T,na),e(na,ve),e(ve,ra),e(ra,Wi),e(ve,Mi),e(ve,sa),e(sa,Ui),e(ve,Gi),e(T,Vi),e(T,la),e(la,_e),e(_e,ia),e(ia,Hi),e(_e,Ji),e(_e,ca),e(ca,Zi),e(_e,Ki),e(T,Yi),e(T,da),e(da,f),e(f,ha),e(ha,ec),e(f,tc),e(f,pa),e(pa,oc),e(f,ac),e(f,ua),e(ua,nc),e(f,rc),e(f,fa),e(fa,sc),e(f,lc),e(f,ma),e(ma,ic),e(f,cc),e(f,ga),e(ga,dc),e(f,hc),e(f,va),e(va,pc),e(f,uc),e(f,_a),e(_a,fc),e(f,mc),e(f,ba),e(ba,gc),e(f,vc),e(f,Ea),e(Ea,_c),e(f,bc),e(f,wa),e(wa,Ec),e(f,wc),e(f,Ta),e(Ta,Tc),e(f,kc),e(f,rt),e(rt,yc),e(f,qc),e(f,ka),e(ka,xc),e(f,Cc),e(T,Pc),e(T,ya),e(ya,be),e(be,qa),e(qa,Ac),e(be,Dc),e(be,xa),e(xa,zc),e(be,Oc),e(T,Lc),e(T,Ca),e(Ca,Ee),e(Ee,Pa),e(Pa,$c),e(Ee,Ic),e(Ee,Aa),e(Aa,jc),e(Ee,Sc),e(T,Fc),e(T,Da),e(Da,we),e(we,za),e(za,Nc),e(we,Bc),e(we,Oa),e(Oa,Xc),e(we,Qc),e(v,Rc),e(v,st),B(Ne,st,null),un=!0},p:Up,i(l){un||(X(ye.$$.fragment,l),X(qe.$$.fragment,l),X(ze.$$.fragment,l),X(Oe.$$.fragment,l),X(Le.$$.fragment,l),X($e.$$.fragment,l),X(Ie.$$.fragment,l),X(je.$$.fragment,l),X(Fe.$$.fragment,l),X(Ne.$$.fragment,l),un=!0)},o(l){Q(ye.$$.fragment,l),Q(qe.$$.fragment,l),Q(ze.$$.fragment,l),Q(Oe.$$.fragment,l),Q(Le.$$.fragment,l),Q($e.$$.fragment,l),Q(Ie.$$.fragment,l),Q(je.$$.fragment,l),Q(Fe.$$.fragment,l),Q(Ne.$$.fragment,l),un=!1},d(l){a(W),l&&a(Va),l&&a(M),R(ye),l&&a(Ha),l&&a(U),R(qe),l&&a(Ja),l&&a(oe),l&&a(Za),l&&a(Qe),l&&a(Ka),l&&a(L),l&&a(Ya),l&&a(Ge),l&&a(en),l&&a(Ve),l&&a(tn),l&&a(He),l&&a(on),l&&a($),l&&a(an),l&&a(H),R(ze),l&&a(nn),l&&a(Je),l&&a(rn),R(Oe,l),l&&a(sn),l&&a(ne),l&&a(ln),R(Le,l),l&&a(cn),l&&a(re),l&&a(dn),R($e,l),l&&a(hn),l&&a(J),R(Ie),l&&a(pn),l&&a(v),R(je),R(Fe),R(Ne)}}}const Hp={local:"tapex",sections:[{local:"overview",title:"Overview"},{local:"usage-inference",title:"Usage: inference"},{local:"transformers.TapexTokenizer",title:"TapexTokenizer"}],title:"TAPEX"};function Jp(Jc){return Gp(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class tu extends Qp{constructor(W){super();Rp(this,W,Jp,Vp,Wp,{})}}export{tu as default,Hp as metadata};
