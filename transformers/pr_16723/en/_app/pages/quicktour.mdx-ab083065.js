import{S as Pp,i as Fp,s as Mp,e as l,k as h,w as y,t as o,M as Sp,c as i,d as s,m as d,a as p,x as b,h as n,b as $,F as t,g as u,y as E,q as A,o as T,B as j,v as Ip,L as ze}from"../chunks/vendor-6b77c823.js";import{T as Is}from"../chunks/Tip-39098574.js";import{Y as zp}from"../chunks/Youtube-5c6e11e6.js";import{I as ct}from"../chunks/IconCopyLink-7a11ce68.js";import{C as W}from"../chunks/CodeBlock-3a8b25a8.js";import{D as Cp}from"../chunks/DocNotebookDropdown-f2b55cd8.js";import{F as Ss,M as _e}from"../chunks/Markdown-4489c441.js";function Op(P){let a,m;return{c(){a=l("p"),m=o(`All code examples presented in the documentation have a toggle on the top left for PyTorch and TensorFlow. If
not, the code is expected to work for both backends without any change.`)},l(r){a=i(r,"P",{});var c=p(a);m=n(c,`All code examples presented in the documentation have a toggle on the top left for PyTorch and TensorFlow. If
not, the code is expected to work for both backends without any change.`),c.forEach(s)},m(r,c){u(r,a,c),t(a,m)},d(r){r&&s(a)}}}function Np(P){let a,m,r,c,_,v,z,I;return{c(){a=l("p"),m=o("For more details about the "),r=l("a"),c=o("pipeline()"),_=o(" and associated tasks, refer to the documentation "),v=l("a"),z=o("here"),I=o("."),this.h()},l(w){a=i(w,"P",{});var M=p(a);m=n(M,"For more details about the "),r=i(M,"A",{href:!0});var C=p(r);c=n(C,"pipeline()"),C.forEach(s),_=n(M," and associated tasks, refer to the documentation "),v=i(M,"A",{href:!0});var O=p(v);z=n(O,"here"),O.forEach(s),I=n(M,"."),M.forEach(s),this.h()},h(){$(r,"href","/docs/transformers/pr_16723/en/main_classes/pipelines#transformers.pipeline"),$(v,"href","./main_classes/pipelines")},m(w,M){u(w,a,M),t(a,m),t(a,r),t(r,c),t(a,_),t(a,v),t(v,z),t(a,I)},d(w){w&&s(a)}}}function Dp(P){let a,m;return a=new W({props:{code:"pip install torch",highlighted:"pip install torch"}}),{c(){y(a.$$.fragment)},l(r){b(a.$$.fragment,r)},m(r,c){E(a,r,c),m=!0},p:ze,i(r){m||(A(a.$$.fragment,r),m=!0)},o(r){T(a.$$.fragment,r),m=!1},d(r){j(a,r)}}}function Hp(P){let a,m;return a=new _e({props:{$$slots:{default:[Dp]},$$scope:{ctx:P}}}),{c(){y(a.$$.fragment)},l(r){b(a.$$.fragment,r)},m(r,c){E(a,r,c),m=!0},p(r,c){const _={};c&2&&(_.$$scope={dirty:c,ctx:r}),a.$set(_)},i(r){m||(A(a.$$.fragment,r),m=!0)},o(r){T(a.$$.fragment,r),m=!1},d(r){j(a,r)}}}function Lp(P){let a,m;return a=new W({props:{code:"pip install tensorflow",highlighted:"pip install tensorflow"}}),{c(){y(a.$$.fragment)},l(r){b(a.$$.fragment,r)},m(r,c){E(a,r,c),m=!0},p:ze,i(r){m||(A(a.$$.fragment,r),m=!0)},o(r){T(a.$$.fragment,r),m=!1},d(r){j(a,r)}}}function Wp(P){let a,m;return a=new _e({props:{$$slots:{default:[Lp]},$$scope:{ctx:P}}}),{c(){y(a.$$.fragment)},l(r){b(a.$$.fragment,r)},m(r,c){E(a,r,c),m=!0},p(r,c){const _={};c&2&&(_.$$scope={dirty:c,ctx:r}),a.$set(_)},i(r){m||(A(a.$$.fragment,r),m=!0)},o(r){T(a.$$.fragment,r),m=!1},d(r){j(a,r)}}}function Up(P){let a,m,r,c,_,v,z,I,w,M,C,O,D,L;return D=new W({props:{code:`from transformers import AutoTokenizer, AutoModelForSequenceClassification

model = AutoModelForSequenceClassification.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, AutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSequenceClassification.from_pretrained(model_name)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(model_name)`}}),{c(){a=l("p"),m=o("Use the "),r=l("a"),c=o("AutoModelForSequenceClassification"),_=o(" and "),v=l("a"),z=o("AutoTokenizer"),I=o(" to load the pretrained model and it\u2019s associated tokenizer (more on an "),w=l("code"),M=o("AutoClass"),C=o(" below):"),O=h(),y(D.$$.fragment),this.h()},l(x){a=i(x,"P",{});var S=p(a);m=n(S,"Use the "),r=i(S,"A",{href:!0});var g=p(r);c=n(g,"AutoModelForSequenceClassification"),g.forEach(s),_=n(S," and "),v=i(S,"A",{href:!0});var F=p(v);z=n(F,"AutoTokenizer"),F.forEach(s),I=n(S," to load the pretrained model and it\u2019s associated tokenizer (more on an "),w=i(S,"CODE",{});var R=p(w);M=n(R,"AutoClass"),R.forEach(s),C=n(S," below):"),S.forEach(s),O=d(x),b(D.$$.fragment,x),this.h()},h(){$(r,"href","/docs/transformers/pr_16723/en/model_doc/auto#transformers.AutoModelForSequenceClassification"),$(v,"href","/docs/transformers/pr_16723/en/model_doc/auto#transformers.AutoTokenizer")},m(x,S){u(x,a,S),t(a,m),t(a,r),t(r,c),t(a,_),t(a,v),t(v,z),t(a,I),t(a,w),t(w,M),t(a,C),u(x,O,S),E(D,x,S),L=!0},p:ze,i(x){L||(A(D.$$.fragment,x),L=!0)},o(x){T(D.$$.fragment,x),L=!1},d(x){x&&s(a),x&&s(O),j(D,x)}}}function Rp(P){let a,m;return a=new _e({props:{$$slots:{default:[Up]},$$scope:{ctx:P}}}),{c(){y(a.$$.fragment)},l(r){b(a.$$.fragment,r)},m(r,c){E(a,r,c),m=!0},p(r,c){const _={};c&2&&(_.$$scope={dirty:c,ctx:r}),a.$set(_)},i(r){m||(A(a.$$.fragment,r),m=!0)},o(r){T(a.$$.fragment,r),m=!1},d(r){j(a,r)}}}function Gp(P){let a,m,r,c,_,v,z,I,w,M,C,O,D,L;return D=new W({props:{code:`from transformers import AutoTokenizer, TFAutoModelForSequenceClassification

model = TFAutoModelForSequenceClassification.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, TFAutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSequenceClassification.from_pretrained(model_name)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(model_name)`}}),{c(){a=l("p"),m=o("Use the "),r=l("a"),c=o("TFAutoModelForSequenceClassification"),_=o(" and "),v=l("a"),z=o("AutoTokenizer"),I=o(" to load the pretrained model and it\u2019s associated tokenizer (more on an "),w=l("code"),M=o("TFAutoClass"),C=o(" below):"),O=h(),y(D.$$.fragment),this.h()},l(x){a=i(x,"P",{});var S=p(a);m=n(S,"Use the "),r=i(S,"A",{href:!0});var g=p(r);c=n(g,"TFAutoModelForSequenceClassification"),g.forEach(s),_=n(S," and "),v=i(S,"A",{href:!0});var F=p(v);z=n(F,"AutoTokenizer"),F.forEach(s),I=n(S," to load the pretrained model and it\u2019s associated tokenizer (more on an "),w=i(S,"CODE",{});var R=p(w);M=n(R,"TFAutoClass"),R.forEach(s),C=n(S," below):"),S.forEach(s),O=d(x),b(D.$$.fragment,x),this.h()},h(){$(r,"href","/docs/transformers/pr_16723/en/model_doc/auto#transformers.TFAutoModelForSequenceClassification"),$(v,"href","/docs/transformers/pr_16723/en/model_doc/auto#transformers.AutoTokenizer")},m(x,S){u(x,a,S),t(a,m),t(a,r),t(r,c),t(a,_),t(a,v),t(v,z),t(a,I),t(a,w),t(w,M),t(a,C),u(x,O,S),E(D,x,S),L=!0},p:ze,i(x){L||(A(D.$$.fragment,x),L=!0)},o(x){T(D.$$.fragment,x),L=!1},d(x){x&&s(a),x&&s(O),j(D,x)}}}function Yp(P){let a,m;return a=new _e({props:{$$slots:{default:[Gp]},$$scope:{ctx:P}}}),{c(){y(a.$$.fragment)},l(r){b(a.$$.fragment,r)},m(r,c){E(a,r,c),m=!0},p(r,c){const _={};c&2&&(_.$$scope={dirty:c,ctx:r}),a.$set(_)},i(r){m||(A(a.$$.fragment,r),m=!0)},o(r){T(a.$$.fragment,r),m=!1},d(r){j(a,r)}}}function Jp(P){let a,m;return a=new W({props:{code:`pt_batch = tokenizer(
    ["We are very happy to show you the \u{1F917} Transformers library.", "We hope you don't hate it."],
    padding=True,
    truncation=True,
    max_length=512,
    return_tensors="pt",
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>pt_batch = tokenizer(
<span class="hljs-meta">... </span>    [<span class="hljs-string">&quot;We are very happy to show you the \u{1F917} Transformers library.&quot;</span>, <span class="hljs-string">&quot;We hope you don&#x27;t hate it.&quot;</span>],
<span class="hljs-meta">... </span>    padding=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>    truncation=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>    max_length=<span class="hljs-number">512</span>,
<span class="hljs-meta">... </span>    return_tensors=<span class="hljs-string">&quot;pt&quot;</span>,
<span class="hljs-meta">... </span>)`}}),{c(){y(a.$$.fragment)},l(r){b(a.$$.fragment,r)},m(r,c){E(a,r,c),m=!0},p:ze,i(r){m||(A(a.$$.fragment,r),m=!0)},o(r){T(a.$$.fragment,r),m=!1},d(r){j(a,r)}}}function Qp(P){let a,m;return a=new _e({props:{$$slots:{default:[Jp]},$$scope:{ctx:P}}}),{c(){y(a.$$.fragment)},l(r){b(a.$$.fragment,r)},m(r,c){E(a,r,c),m=!0},p(r,c){const _={};c&2&&(_.$$scope={dirty:c,ctx:r}),a.$set(_)},i(r){m||(A(a.$$.fragment,r),m=!0)},o(r){T(a.$$.fragment,r),m=!1},d(r){j(a,r)}}}function Bp(P){let a,m;return a=new W({props:{code:`tf_batch = tokenizer(
    ["We are very happy to show you the \u{1F917} Transformers library.", "We hope you don't hate it."],
    padding=True,
    truncation=True,
    max_length=512,
    return_tensors="tf",
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>tf_batch = tokenizer(
<span class="hljs-meta">... </span>    [<span class="hljs-string">&quot;We are very happy to show you the \u{1F917} Transformers library.&quot;</span>, <span class="hljs-string">&quot;We hope you don&#x27;t hate it.&quot;</span>],
<span class="hljs-meta">... </span>    padding=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>    truncation=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>    max_length=<span class="hljs-number">512</span>,
<span class="hljs-meta">... </span>    return_tensors=<span class="hljs-string">&quot;tf&quot;</span>,
<span class="hljs-meta">... </span>)`}}),{c(){y(a.$$.fragment)},l(r){b(a.$$.fragment,r)},m(r,c){E(a,r,c),m=!0},p:ze,i(r){m||(A(a.$$.fragment,r),m=!0)},o(r){T(a.$$.fragment,r),m=!1},d(r){j(a,r)}}}function Vp(P){let a,m;return a=new _e({props:{$$slots:{default:[Bp]},$$scope:{ctx:P}}}),{c(){y(a.$$.fragment)},l(r){b(a.$$.fragment,r)},m(r,c){E(a,r,c),m=!0},p(r,c){const _={};c&2&&(_.$$scope={dirty:c,ctx:r}),a.$set(_)},i(r){m||(A(a.$$.fragment,r),m=!0)},o(r){T(a.$$.fragment,r),m=!1},d(r){j(a,r)}}}function Kp(P){let a,m,r,c,_,v,z,I;return{c(){a=l("p"),m=o("See the "),r=l("a"),c=o("task summary"),_=o(" for which "),v=l("a"),z=o("AutoModel"),I=o(" class to use for which task."),this.h()},l(w){a=i(w,"P",{});var M=p(a);m=n(M,"See the "),r=i(M,"A",{href:!0});var C=p(r);c=n(C,"task summary"),C.forEach(s),_=n(M," for which "),v=i(M,"A",{href:!0});var O=p(v);z=n(O,"AutoModel"),O.forEach(s),I=n(M," class to use for which task."),M.forEach(s),this.h()},h(){$(r,"href","./task_summary"),$(v,"href","/docs/transformers/pr_16723/en/model_doc/auto#transformers.AutoModel")},m(w,M){u(w,a,M),t(a,m),t(a,r),t(r,c),t(a,_),t(a,v),t(v,z),t(a,I)},d(w){w&&s(a)}}}function Zp(P){let a,m,r,c,_,v,z,I,w,M,C,O,D,L,x,S,g,F,R,U,Q,J,se,B,G,ee,V,K,ce,re,de,oe,te,ne,$e,q,N,le;return S=new W({props:{code:`from transformers import AutoModelForSequenceClassification

model_name = "nlptown/bert-base-multilingual-uncased-sentiment"
pt_model = AutoModelForSequenceClassification.from_pretrained(model_name)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>model_name = <span class="hljs-string">&quot;nlptown/bert-base-multilingual-uncased-sentiment&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>pt_model = AutoModelForSequenceClassification.from_pretrained(model_name)`}}),F=new Is({props:{$$slots:{default:[Kp]},$$scope:{ctx:P}}}),ee=new W({props:{code:"pt_outputs = pt_model(**pt_batch)",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>pt_outputs = pt_model(**pt_batch)'}}),N=new W({props:{code:`from torch import nn

pt_predictions = nn.functional.softmax(pt_outputs.logits, dim=-1)
print(pt_predictions)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn

<span class="hljs-meta">&gt;&gt;&gt; </span>pt_predictions = nn.functional.softmax(pt_outputs.logits, dim=-<span class="hljs-number">1</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(pt_predictions)
tensor([[<span class="hljs-number">0.0021</span>, <span class="hljs-number">0.0018</span>, <span class="hljs-number">0.0115</span>, <span class="hljs-number">0.2121</span>, <span class="hljs-number">0.7725</span>],
        [<span class="hljs-number">0.2084</span>, <span class="hljs-number">0.1826</span>, <span class="hljs-number">0.1969</span>, <span class="hljs-number">0.1755</span>, <span class="hljs-number">0.2365</span>]], grad_fn=&lt;SoftmaxBackward0&gt;)`}}),{c(){a=l("p"),m=o("\u{1F917} Transformers provides a simple and unified way to load pretrained instances. This means you can load an "),r=l("a"),c=o("AutoModel"),_=o(" like you would load an "),v=l("a"),z=o("AutoTokenizer"),I=o(". The only difference is selecting the correct "),w=l("a"),M=o("AutoModel"),C=o(" for the task. Since you are doing text - or sequence - classification, load "),O=l("a"),D=o("AutoModelForSequenceClassification"),L=o(":"),x=h(),y(S.$$.fragment),g=h(),y(F.$$.fragment),R=h(),U=l("p"),Q=o("Now you can pass your preprocessed batch of inputs directly to the model. You just have to unpack the dictionary by adding "),J=l("code"),se=o("**"),B=o(":"),G=h(),y(ee.$$.fragment),V=h(),K=l("p"),ce=o("The model outputs the final activations in the "),re=l("code"),de=o("logits"),oe=o(" attribute. Apply the softmax function to the "),te=l("code"),ne=o("logits"),$e=o(" to retrieve the probabilities:"),q=h(),y(N.$$.fragment),this.h()},l(k){a=i(k,"P",{});var H=p(a);m=n(H,"\u{1F917} Transformers provides a simple and unified way to load pretrained instances. This means you can load an "),r=i(H,"A",{href:!0});var ie=p(r);c=n(ie,"AutoModel"),ie.forEach(s),_=n(H," like you would load an "),v=i(H,"A",{href:!0});var Pe=p(v);z=n(Pe,"AutoTokenizer"),Pe.forEach(s),I=n(H,". The only difference is selecting the correct "),w=i(H,"A",{href:!0});var he=p(w);M=n(he,"AutoModel"),he.forEach(s),C=n(H," for the task. Since you are doing text - or sequence - classification, load "),O=i(H,"A",{href:!0});var ge=p(O);D=n(ge,"AutoModelForSequenceClassification"),ge.forEach(s),L=n(H,":"),H.forEach(s),x=d(k),b(S.$$.fragment,k),g=d(k),b(F.$$.fragment,k),R=d(k),U=i(k,"P",{});var pe=p(U);Q=n(pe,"Now you can pass your preprocessed batch of inputs directly to the model. You just have to unpack the dictionary by adding "),J=i(pe,"CODE",{});var De=p(J);se=n(De,"**"),De.forEach(s),B=n(pe,":"),pe.forEach(s),G=d(k),b(ee.$$.fragment,k),V=d(k),K=i(k,"P",{});var ve=p(K);ce=n(ve,"The model outputs the final activations in the "),re=i(ve,"CODE",{});var Jt=p(re);de=n(Jt,"logits"),Jt.forEach(s),oe=n(ve," attribute. Apply the softmax function to the "),te=i(ve,"CODE",{});var ht=p(te);ne=n(ht,"logits"),ht.forEach(s),$e=n(ve," to retrieve the probabilities:"),ve.forEach(s),q=d(k),b(N.$$.fragment,k),this.h()},h(){$(r,"href","/docs/transformers/pr_16723/en/model_doc/auto#transformers.AutoModel"),$(v,"href","/docs/transformers/pr_16723/en/model_doc/auto#transformers.AutoTokenizer"),$(w,"href","/docs/transformers/pr_16723/en/model_doc/auto#transformers.AutoModel"),$(O,"href","/docs/transformers/pr_16723/en/model_doc/auto#transformers.AutoModelForSequenceClassification")},m(k,H){u(k,a,H),t(a,m),t(a,r),t(r,c),t(a,_),t(a,v),t(v,z),t(a,I),t(a,w),t(w,M),t(a,C),t(a,O),t(O,D),t(a,L),u(k,x,H),E(S,k,H),u(k,g,H),E(F,k,H),u(k,R,H),u(k,U,H),t(U,Q),t(U,J),t(J,se),t(U,B),u(k,G,H),E(ee,k,H),u(k,V,H),u(k,K,H),t(K,ce),t(K,re),t(re,de),t(K,oe),t(K,te),t(te,ne),t(K,$e),u(k,q,H),E(N,k,H),le=!0},p(k,H){const ie={};H&2&&(ie.$$scope={dirty:H,ctx:k}),F.$set(ie)},i(k){le||(A(S.$$.fragment,k),A(F.$$.fragment,k),A(ee.$$.fragment,k),A(N.$$.fragment,k),le=!0)},o(k){T(S.$$.fragment,k),T(F.$$.fragment,k),T(ee.$$.fragment,k),T(N.$$.fragment,k),le=!1},d(k){k&&s(a),k&&s(x),j(S,k),k&&s(g),j(F,k),k&&s(R),k&&s(U),k&&s(G),j(ee,k),k&&s(V),k&&s(K),k&&s(q),j(N,k)}}}function Xp(P){let a,m;return a=new _e({props:{$$slots:{default:[Zp]},$$scope:{ctx:P}}}),{c(){y(a.$$.fragment)},l(r){b(a.$$.fragment,r)},m(r,c){E(a,r,c),m=!0},p(r,c){const _={};c&2&&(_.$$scope={dirty:c,ctx:r}),a.$set(_)},i(r){m||(A(a.$$.fragment,r),m=!0)},o(r){T(a.$$.fragment,r),m=!1},d(r){j(a,r)}}}function ef(P){let a,m,r,c,_,v,z,I;return{c(){a=l("p"),m=o("See the "),r=l("a"),c=o("task summary"),_=o(" for which "),v=l("a"),z=o("AutoModel"),I=o(" class to use for which task."),this.h()},l(w){a=i(w,"P",{});var M=p(a);m=n(M,"See the "),r=i(M,"A",{href:!0});var C=p(r);c=n(C,"task summary"),C.forEach(s),_=n(M," for which "),v=i(M,"A",{href:!0});var O=p(v);z=n(O,"AutoModel"),O.forEach(s),I=n(M," class to use for which task."),M.forEach(s),this.h()},h(){$(r,"href","./task_summary"),$(v,"href","/docs/transformers/pr_16723/en/model_doc/auto#transformers.AutoModel")},m(w,M){u(w,a,M),t(a,m),t(a,r),t(r,c),t(a,_),t(a,v),t(v,z),t(a,I)},d(w){w&&s(a)}}}function tf(P){let a,m,r,c,_,v,z,I,w,M,C,O,D,L,x,S,g,F,R,U,Q,J,se,B,G,ee,V,K,ce,re,de,oe,te,ne,$e;return S=new W({props:{code:`from transformers import TFAutoModelForSequenceClassification

model_name = "nlptown/bert-base-multilingual-uncased-sentiment"
tf_model = TFAutoModelForSequenceClassification.from_pretrained(model_name)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFAutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>model_name = <span class="hljs-string">&quot;nlptown/bert-base-multilingual-uncased-sentiment&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tf_model = TFAutoModelForSequenceClassification.from_pretrained(model_name)`}}),F=new Is({props:{$$slots:{default:[ef]},$$scope:{ctx:P}}}),se=new W({props:{code:"tf_outputs = tf_model(tf_batch)",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>tf_outputs = tf_model(tf_batch)'}}),ne=new W({props:{code:`import tensorflow as tf

tf_predictions = tf.nn.softmax(tf_outputs.logits, axis=-1)
tf_predictions`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

<span class="hljs-meta">&gt;&gt;&gt; </span>tf_predictions = tf.nn.softmax(tf_outputs.logits, axis=-<span class="hljs-number">1</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tf_predictions`}}),{c(){a=l("p"),m=o("\u{1F917} Transformers provides a simple and unified way to load pretrained instances. This means you can load an "),r=l("a"),c=o("TFAutoModel"),_=o(" like you would load an "),v=l("a"),z=o("AutoTokenizer"),I=o(". The only difference is selecting the correct "),w=l("a"),M=o("TFAutoModel"),C=o(" for the task. Since you are doing text - or sequence - classification, load "),O=l("a"),D=o("TFAutoModelForSequenceClassification"),L=o(":"),x=h(),y(S.$$.fragment),g=h(),y(F.$$.fragment),R=h(),U=l("p"),Q=o("Now you can pass your preprocessed batch of inputs directly to the model by passing the dictionary keys directly to the tensors:"),J=h(),y(se.$$.fragment),B=h(),G=l("p"),ee=o("The model outputs the final activations in the "),V=l("code"),K=o("logits"),ce=o(" attribute. Apply the softmax function to the "),re=l("code"),de=o("logits"),oe=o(" to retrieve the probabilities:"),te=h(),y(ne.$$.fragment),this.h()},l(q){a=i(q,"P",{});var N=p(a);m=n(N,"\u{1F917} Transformers provides a simple and unified way to load pretrained instances. This means you can load an "),r=i(N,"A",{href:!0});var le=p(r);c=n(le,"TFAutoModel"),le.forEach(s),_=n(N," like you would load an "),v=i(N,"A",{href:!0});var k=p(v);z=n(k,"AutoTokenizer"),k.forEach(s),I=n(N,". The only difference is selecting the correct "),w=i(N,"A",{href:!0});var H=p(w);M=n(H,"TFAutoModel"),H.forEach(s),C=n(N," for the task. Since you are doing text - or sequence - classification, load "),O=i(N,"A",{href:!0});var ie=p(O);D=n(ie,"TFAutoModelForSequenceClassification"),ie.forEach(s),L=n(N,":"),N.forEach(s),x=d(q),b(S.$$.fragment,q),g=d(q),b(F.$$.fragment,q),R=d(q),U=i(q,"P",{});var Pe=p(U);Q=n(Pe,"Now you can pass your preprocessed batch of inputs directly to the model by passing the dictionary keys directly to the tensors:"),Pe.forEach(s),J=d(q),b(se.$$.fragment,q),B=d(q),G=i(q,"P",{});var he=p(G);ee=n(he,"The model outputs the final activations in the "),V=i(he,"CODE",{});var ge=p(V);K=n(ge,"logits"),ge.forEach(s),ce=n(he," attribute. Apply the softmax function to the "),re=i(he,"CODE",{});var pe=p(re);de=n(pe,"logits"),pe.forEach(s),oe=n(he," to retrieve the probabilities:"),he.forEach(s),te=d(q),b(ne.$$.fragment,q),this.h()},h(){$(r,"href","/docs/transformers/pr_16723/en/model_doc/auto#transformers.TFAutoModel"),$(v,"href","/docs/transformers/pr_16723/en/model_doc/auto#transformers.AutoTokenizer"),$(w,"href","/docs/transformers/pr_16723/en/model_doc/auto#transformers.TFAutoModel"),$(O,"href","/docs/transformers/pr_16723/en/model_doc/auto#transformers.TFAutoModelForSequenceClassification")},m(q,N){u(q,a,N),t(a,m),t(a,r),t(r,c),t(a,_),t(a,v),t(v,z),t(a,I),t(a,w),t(w,M),t(a,C),t(a,O),t(O,D),t(a,L),u(q,x,N),E(S,q,N),u(q,g,N),E(F,q,N),u(q,R,N),u(q,U,N),t(U,Q),u(q,J,N),E(se,q,N),u(q,B,N),u(q,G,N),t(G,ee),t(G,V),t(V,K),t(G,ce),t(G,re),t(re,de),t(G,oe),u(q,te,N),E(ne,q,N),$e=!0},p(q,N){const le={};N&2&&(le.$$scope={dirty:N,ctx:q}),F.$set(le)},i(q){$e||(A(S.$$.fragment,q),A(F.$$.fragment,q),A(se.$$.fragment,q),A(ne.$$.fragment,q),$e=!0)},o(q){T(S.$$.fragment,q),T(F.$$.fragment,q),T(se.$$.fragment,q),T(ne.$$.fragment,q),$e=!1},d(q){q&&s(a),q&&s(x),j(S,q),q&&s(g),j(F,q),q&&s(R),q&&s(U),q&&s(J),j(se,q),q&&s(B),q&&s(G),q&&s(te),j(ne,q)}}}function sf(P){let a,m;return a=new _e({props:{$$slots:{default:[tf]},$$scope:{ctx:P}}}),{c(){y(a.$$.fragment)},l(r){b(a.$$.fragment,r)},m(r,c){E(a,r,c),m=!0},p(r,c){const _={};c&2&&(_.$$scope={dirty:c,ctx:r}),a.$set(_)},i(r){m||(A(a.$$.fragment,r),m=!0)},o(r){T(a.$$.fragment,r),m=!1},d(r){j(a,r)}}}function af(P){let a,m,r,c,_;return{c(){a=l("p"),m=o("All \u{1F917} Transformers models (PyTorch or TensorFlow) outputs the tensors "),r=l("em"),c=o("before"),_=o(` the final activation
function (like softmax) because the final activation function is often fused with the loss.`)},l(v){a=i(v,"P",{});var z=p(a);m=n(z,"All \u{1F917} Transformers models (PyTorch or TensorFlow) outputs the tensors "),r=i(z,"EM",{});var I=p(r);c=n(I,"before"),I.forEach(s),_=n(z,` the final activation
function (like softmax) because the final activation function is often fused with the loss.`),z.forEach(s)},m(v,z){u(v,a,z),t(a,m),t(a,r),t(r,c),t(a,_)},d(v){v&&s(a)}}}function rf(P){let a,m,r,c,_;return{c(){a=l("p"),m=o(`\u{1F917} Transformers model outputs are special dataclasses so their attributes are autocompleted in an IDE.
The model outputs also behave like a tuple or a dictionary (e.g., you can index with an integer, a slice or a string) in which case the attributes that are `),r=l("code"),c=o("None"),_=o(" are ignored.")},l(v){a=i(v,"P",{});var z=p(a);m=n(z,`\u{1F917} Transformers model outputs are special dataclasses so their attributes are autocompleted in an IDE.
The model outputs also behave like a tuple or a dictionary (e.g., you can index with an integer, a slice or a string) in which case the attributes that are `),r=i(z,"CODE",{});var I=p(r);c=n(I,"None"),I.forEach(s),_=n(z," are ignored."),z.forEach(s)},m(v,z){u(v,a,z),t(a,m),t(a,r),t(r,c),t(a,_)},d(v){v&&s(a)}}}function of(P){let a,m,r,c,_,v,z,I,w,M,C,O,D,L,x,S;return z=new W({props:{code:`pt_save_directory = "./pt_save_pretrained"
tokenizer.save_pretrained(pt_save_directory)
pt_model.save_pretrained(pt_save_directory)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>pt_save_directory = <span class="hljs-string">&quot;./pt_save_pretrained&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.save_pretrained(pt_save_directory)
<span class="hljs-meta">&gt;&gt;&gt; </span>pt_model.save_pretrained(pt_save_directory)`}}),x=new W({props:{code:'pt_model = AutoModelForSequenceClassification.from_pretrained("./pt_save_pretrained")',highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>pt_model = AutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;./pt_save_pretrained&quot;</span>)'}}),{c(){a=l("p"),m=o("Once your model is fine-tuned, you can save it with its tokenizer using "),r=l("a"),c=o("PreTrainedModel.save_pretrained()"),_=o(":"),v=h(),y(z.$$.fragment),I=h(),w=l("p"),M=o("When you are ready to use the model again, reload it with "),C=l("a"),O=o("PreTrainedModel.from_pretrained()"),D=o(":"),L=h(),y(x.$$.fragment),this.h()},l(g){a=i(g,"P",{});var F=p(a);m=n(F,"Once your model is fine-tuned, you can save it with its tokenizer using "),r=i(F,"A",{href:!0});var R=p(r);c=n(R,"PreTrainedModel.save_pretrained()"),R.forEach(s),_=n(F,":"),F.forEach(s),v=d(g),b(z.$$.fragment,g),I=d(g),w=i(g,"P",{});var U=p(w);M=n(U,"When you are ready to use the model again, reload it with "),C=i(U,"A",{href:!0});var Q=p(C);O=n(Q,"PreTrainedModel.from_pretrained()"),Q.forEach(s),D=n(U,":"),U.forEach(s),L=d(g),b(x.$$.fragment,g),this.h()},h(){$(r,"href","/docs/transformers/pr_16723/en/main_classes/model#transformers.PreTrainedModel.save_pretrained"),$(C,"href","/docs/transformers/pr_16723/en/main_classes/model#transformers.PreTrainedModel.from_pretrained")},m(g,F){u(g,a,F),t(a,m),t(a,r),t(r,c),t(a,_),u(g,v,F),E(z,g,F),u(g,I,F),u(g,w,F),t(w,M),t(w,C),t(C,O),t(w,D),u(g,L,F),E(x,g,F),S=!0},p:ze,i(g){S||(A(z.$$.fragment,g),A(x.$$.fragment,g),S=!0)},o(g){T(z.$$.fragment,g),T(x.$$.fragment,g),S=!1},d(g){g&&s(a),g&&s(v),j(z,g),g&&s(I),g&&s(w),g&&s(L),j(x,g)}}}function nf(P){let a,m;return a=new _e({props:{$$slots:{default:[of]},$$scope:{ctx:P}}}),{c(){y(a.$$.fragment)},l(r){b(a.$$.fragment,r)},m(r,c){E(a,r,c),m=!0},p(r,c){const _={};c&2&&(_.$$scope={dirty:c,ctx:r}),a.$set(_)},i(r){m||(A(a.$$.fragment,r),m=!0)},o(r){T(a.$$.fragment,r),m=!1},d(r){j(a,r)}}}function lf(P){let a,m,r,c,_,v,z,I,w,M,C,O,D,L,x,S;return z=new W({props:{code:`tf_save_directory = "./tf_save_pretrained"
tokenizer.save_pretrained(tf_save_directory)
tf_model.save_pretrained(tf_save_directory)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>tf_save_directory = <span class="hljs-string">&quot;./tf_save_pretrained&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.save_pretrained(tf_save_directory)
<span class="hljs-meta">&gt;&gt;&gt; </span>tf_model.save_pretrained(tf_save_directory)`}}),x=new W({props:{code:'tf_model = TFAutoModelForSequenceClassification.from_pretrained("./tf_save_pretrained")',highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>tf_model = TFAutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;./tf_save_pretrained&quot;</span>)'}}),{c(){a=l("p"),m=o("Once your model is fine-tuned, you can save it with its tokenizer using "),r=l("a"),c=o("TFPreTrainedModel.save_pretrained()"),_=o(":"),v=h(),y(z.$$.fragment),I=h(),w=l("p"),M=o("When you are ready to use the model again, reload it with "),C=l("a"),O=o("TFPreTrainedModel.from_pretrained()"),D=o(":"),L=h(),y(x.$$.fragment),this.h()},l(g){a=i(g,"P",{});var F=p(a);m=n(F,"Once your model is fine-tuned, you can save it with its tokenizer using "),r=i(F,"A",{href:!0});var R=p(r);c=n(R,"TFPreTrainedModel.save_pretrained()"),R.forEach(s),_=n(F,":"),F.forEach(s),v=d(g),b(z.$$.fragment,g),I=d(g),w=i(g,"P",{});var U=p(w);M=n(U,"When you are ready to use the model again, reload it with "),C=i(U,"A",{href:!0});var Q=p(C);O=n(Q,"TFPreTrainedModel.from_pretrained()"),Q.forEach(s),D=n(U,":"),U.forEach(s),L=d(g),b(x.$$.fragment,g),this.h()},h(){$(r,"href","/docs/transformers/pr_16723/en/main_classes/model#transformers.TFPreTrainedModel.save_pretrained"),$(C,"href","/docs/transformers/pr_16723/en/main_classes/model#transformers.TFPreTrainedModel.from_pretrained")},m(g,F){u(g,a,F),t(a,m),t(a,r),t(r,c),t(a,_),u(g,v,F),E(z,g,F),u(g,I,F),u(g,w,F),t(w,M),t(w,C),t(C,O),t(w,D),u(g,L,F),E(x,g,F),S=!0},p:ze,i(g){S||(A(z.$$.fragment,g),A(x.$$.fragment,g),S=!0)},o(g){T(z.$$.fragment,g),T(x.$$.fragment,g),S=!1},d(g){g&&s(a),g&&s(v),j(z,g),g&&s(I),g&&s(w),g&&s(L),j(x,g)}}}function pf(P){let a,m;return a=new _e({props:{$$slots:{default:[lf]},$$scope:{ctx:P}}}),{c(){y(a.$$.fragment)},l(r){b(a.$$.fragment,r)},m(r,c){E(a,r,c),m=!0},p(r,c){const _={};c&2&&(_.$$scope={dirty:c,ctx:r}),a.$set(_)},i(r){m||(A(a.$$.fragment,r),m=!0)},o(r){T(a.$$.fragment,r),m=!1},d(r){j(a,r)}}}function ff(P){let a,m;return a=new W({props:{code:`from transformers import AutoModel

tokenizer = AutoTokenizer.from_pretrained(tf_save_directory)
pt_model = AutoModelForSequenceClassification.from_pretrained(tf_save_directory, from_tf=True)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModel

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(tf_save_directory)
<span class="hljs-meta">&gt;&gt;&gt; </span>pt_model = AutoModelForSequenceClassification.from_pretrained(tf_save_directory, from_tf=<span class="hljs-literal">True</span>)`}}),{c(){y(a.$$.fragment)},l(r){b(a.$$.fragment,r)},m(r,c){E(a,r,c),m=!0},p:ze,i(r){m||(A(a.$$.fragment,r),m=!0)},o(r){T(a.$$.fragment,r),m=!1},d(r){j(a,r)}}}function uf(P){let a,m;return a=new _e({props:{$$slots:{default:[ff]},$$scope:{ctx:P}}}),{c(){y(a.$$.fragment)},l(r){b(a.$$.fragment,r)},m(r,c){E(a,r,c),m=!0},p(r,c){const _={};c&2&&(_.$$scope={dirty:c,ctx:r}),a.$set(_)},i(r){m||(A(a.$$.fragment,r),m=!0)},o(r){T(a.$$.fragment,r),m=!1},d(r){j(a,r)}}}function mf(P){let a,m;return a=new W({props:{code:`from transformers import TFAutoModel

tokenizer = AutoTokenizer.from_pretrained(pt_save_directory)
tf_model = TFAutoModelForSequenceClassification.from_pretrained(pt_save_directory, from_pt=True)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFAutoModel

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(pt_save_directory)
<span class="hljs-meta">&gt;&gt;&gt; </span>tf_model = TFAutoModelForSequenceClassification.from_pretrained(pt_save_directory, from_pt=<span class="hljs-literal">True</span>)`}}),{c(){y(a.$$.fragment)},l(r){b(a.$$.fragment,r)},m(r,c){E(a,r,c),m=!0},p:ze,i(r){m||(A(a.$$.fragment,r),m=!0)},o(r){T(a.$$.fragment,r),m=!1},d(r){j(a,r)}}}function cf(P){let a,m;return a=new _e({props:{$$slots:{default:[mf]},$$scope:{ctx:P}}}),{c(){y(a.$$.fragment)},l(r){b(a.$$.fragment,r)},m(r,c){E(a,r,c),m=!0},p(r,c){const _={};c&2&&(_.$$scope={dirty:c,ctx:r}),a.$set(_)},i(r){m||(A(a.$$.fragment,r),m=!0)},o(r){T(a.$$.fragment,r),m=!1},d(r){j(a,r)}}}function hf(P){let a,m,r,c,_,v,z,I,w,M,C,O,D,L,x,S,g,F,R,U,Q,J,se,B,G,ee,V,K,ce,re,de,oe,te,ne,$e,q,N,le,k,H,ie,Pe,he,ge,pe,De,ve,Jt,ht,Y,Cs,Zr,Xr,Os,eo,to,Ns,so,ao,Ds,ro,oo,Hs,no,lo,Ls,io,po,Ws,fo,uo,Us,mo,xa,dt,Rs,co,ho,qa,we,Gs,$o,_o,Ys,go,vo,Js,wo,za,$t,Qs,ko,yo,Pa,He,Bs,bo,Eo,Vs,Ao,Fa,Le,Ma,Fe,We,Ks,_t,To,Zs,jo,Sa,Ue,xo,Qt,qo,zo,Ia,Bt,Po,Ca,Re,Oa,Ge,Fo,Vt,Mo,So,Na,gt,Da,ke,Io,vt,Co,Oo,Xs,No,Do,Ha,wt,La,Ye,Ho,Kt,Lo,Wo,Wa,kt,Ua,ye,Uo,Zt,Ro,Go,yt,Yo,Jo,Ra,bt,Ga,Je,Qo,Xt,Bo,Vo,Ya,Et,Ja,be,Ko,At,Zo,Xo,Tt,en,tn,Qa,jt,Ba,Qe,sn,ea,an,rn,Va,xt,Ka,es,on,Za,qt,Xa,Be,nn,ts,ln,pn,er,Me,Ve,ta,zt,fn,sa,un,tr,fe,mn,ss,cn,hn,Pt,dn,$n,as,_n,gn,Ft,vn,wn,sr,Mt,ar,Ke,rr,Ee,kn,rs,yn,bn,aa,En,An,or,St,nr,Ae,Tn,os,jn,xn,ns,qn,zn,lr,Se,Ze,ra,It,Pn,oa,Fn,ir,Ct,pr,Z,Mn,ls,Sn,In,is,Cn,On,ps,Nn,Dn,fs,Hn,Ln,na,Wn,Un,us,Rn,Gn,fr,Te,Yn,la,Jn,Qn,ms,Bn,Vn,ur,Ie,Xe,ia,Ot,Kn,pa,Zn,mr,je,Xn,fa,el,tl,cs,sl,al,cr,et,rl,hs,ol,nl,hr,Nt,dr,tt,ll,ua,il,pl,$r,ds,fl,_r,Dt,gr,$s,ul,vr,st,_s,gs,ml,cl,hl,vs,ws,dl,$l,wr,at,_l,ks,gl,vl,kr,rt,yr,ot,wl,ys,kl,yl,br,Ce,nt,ma,Ht,bl,ca,El,Er,lt,Ar,it,Tr,X,Al,Lt,ha,Tl,jl,Wt,da,xl,ql,bs,zl,Pl,$a,Fl,Ml,Ut,Sl,Il,Es,Cl,Ol,jr,pt,xr,Oe,ft,_a,Rt,Nl,ga,Dl,qr,ut,zr,xe,Hl,va,Ll,Wl,wa,Ul,Rl,Pr,mt,Fr;return v=new ct({}),C=new Cp({props:{classNames:"absolute z-10 right-0 top-0",options:[{label:"Mixed",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/main/transformers_doc/en/quicktour.ipynb"},{label:"PyTorch",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/main/transformers_doc/en/pytorch/quicktour.ipynb"},{label:"TensorFlow",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/main/transformers_doc/en/tensorflow/quicktour.ipynb"},{label:"Mixed",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/transformers_doc/en/quicktour.ipynb"},{label:"PyTorch",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/transformers_doc/en/pytorch/quicktour.ipynb"},{label:"TensorFlow",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/transformers_doc/en/tensorflow/quicktour.ipynb"}]}}),J=new Is({props:{$$slots:{default:[Op]},$$scope:{ctx:P}}}),V=new ct({}),N=new zp({props:{id:"tiZFewofSLM"}}),Le=new Is({props:{$$slots:{default:[Np]},$$scope:{ctx:P}}}),_t=new ct({}),Re=new Ss({props:{pytorch:!0,tensorflow:!0,jax:!1,$$slots:{tensorflow:[Wp],pytorch:[Hp]},$$scope:{ctx:P}}}),gt=new W({props:{code:`from transformers import pipeline

classifier = pipeline("sentiment-analysis")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> pipeline

<span class="hljs-meta">&gt;&gt;&gt; </span>classifier = pipeline(<span class="hljs-string">&quot;sentiment-analysis&quot;</span>)`}}),wt=new W({props:{code:'classifier("We are very happy to show you the \u{1F917} Transformers library.")',highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>classifier(<span class="hljs-string">&quot;We are very happy to show you the \u{1F917} Transformers library.&quot;</span>)
[{<span class="hljs-string">&#x27;label&#x27;</span>: <span class="hljs-string">&#x27;POSITIVE&#x27;</span>, <span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.9998</span>}]`}}),kt=new W({props:{code:`results = classifier(["We are very happy to show you the \u{1F917} Transformers library.", "We hope you don't hate it."])
for result in results:
    print(f"label: {result['label']}, with score: {round(result['score'], 4)}")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>results = classifier([<span class="hljs-string">&quot;We are very happy to show you the \u{1F917} Transformers library.&quot;</span>, <span class="hljs-string">&quot;We hope you don&#x27;t hate it.&quot;</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">for</span> result <span class="hljs-keyword">in</span> results:
<span class="hljs-meta">... </span>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;label: <span class="hljs-subst">{result[<span class="hljs-string">&#x27;label&#x27;</span>]}</span>, with score: <span class="hljs-subst">{<span class="hljs-built_in">round</span>(result[<span class="hljs-string">&#x27;score&#x27;</span>], <span class="hljs-number">4</span>)}</span>&quot;</span>)
label: POSITIVE, <span class="hljs-keyword">with</span> score: <span class="hljs-number">0.9998</span>
label: NEGATIVE, <span class="hljs-keyword">with</span> score: <span class="hljs-number">0.5309</span>`}}),bt=new W({props:{code:"pip install datasets ",highlighted:"pip install datasets "}}),Et=new W({props:{code:`import torch
from transformers import pipeline

speech_recognizer = pipeline("automatic-speech-recognition", model="facebook/wav2vec2-base-960h")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> pipeline

<span class="hljs-meta">&gt;&gt;&gt; </span>speech_recognizer = pipeline(<span class="hljs-string">&quot;automatic-speech-recognition&quot;</span>, model=<span class="hljs-string">&quot;facebook/wav2vec2-base-960h&quot;</span>)`}}),jt=new W({props:{code:`from datasets import load_dataset, Audio

dataset = load_dataset("PolyAI/minds14", name="en-US", split="train")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset, Audio

<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;PolyAI/minds14&quot;</span>, name=<span class="hljs-string">&quot;en-US&quot;</span>, split=<span class="hljs-string">&quot;train&quot;</span>)`}}),xt=new W({props:{code:'dataset = dataset.cast_column("audio", Audio(sampling_rate=speech_recognizer.feature_extractor.sampling_rate))',highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = dataset.cast_column(<span class="hljs-string">&quot;audio&quot;</span>, Audio(sampling_rate=speech_recognizer.feature_extractor.sampling_rate))'}}),qt=new W({props:{code:`raw_audio_waveforms = [d["array"] for d in dataset[:4]["audio"]]
speech_recognizer(raw_audio_waveforms)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>raw_audio_waveforms = [d[<span class="hljs-string">&quot;array&quot;</span>] <span class="hljs-keyword">for</span> d <span class="hljs-keyword">in</span> dataset[:<span class="hljs-number">4</span>][<span class="hljs-string">&quot;audio&quot;</span>]]
<span class="hljs-meta">&gt;&gt;&gt; </span>speech_recognizer(raw_audio_waveforms)
[{<span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;I WOULD LIKE TO SET UP A JOINT ACCOUNT WITH MY PARTNER HOW DO I PROCEED WITH DOING THAT&#x27;</span>},
 {<span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&quot;FONDERING HOW I&#x27;D SET UP A JOIN TO HET WITH MY WIFE AND WHERE THE AP MIGHT BE&quot;</span>}, 
 {<span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&quot;I I&#x27;D LIKE TOY SET UP A JOINT ACCOUNT WITH MY PARTNER I&#x27;M NOT SEEING THE OPTION TO DO IT ON THE APSO I CALLED IN TO GET SOME HELP CAN I JUST DO IT OVER THE PHONE WITH YOU AND GIVE YOU THE INFORMATION OR SHOULD I DO IT IN THE AP AND I&#x27;M MISSING SOMETHING UQUETTE HAD PREFERRED TO JUST DO IT OVER THE PHONE OF POSSIBLE THINGS&quot;</span>},
 {<span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;HOW DO I TURN A JOIN A COUNT&#x27;</span>}]`}}),zt=new ct({}),Mt=new W({props:{code:'model_name = "nlptown/bert-base-multilingual-uncased-sentiment"',highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>model_name = <span class="hljs-string">&quot;nlptown/bert-base-multilingual-uncased-sentiment&quot;</span>'}}),Ke=new Ss({props:{pytorch:!0,tensorflow:!0,jax:!1,$$slots:{tensorflow:[Yp],pytorch:[Rp]},$$scope:{ctx:P}}}),St=new W({props:{code:`classifier = pipeline("sentiment-analysis", model=model, tokenizer=tokenizer)
classifier("Nous sommes tr\xE8s heureux de vous pr\xE9senter la biblioth\xE8que \u{1F917} Transformers.")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>classifier = pipeline(<span class="hljs-string">&quot;sentiment-analysis&quot;</span>, model=model, tokenizer=tokenizer)
<span class="hljs-meta">&gt;&gt;&gt; </span>classifier(<span class="hljs-string">&quot;Nous sommes tr\xE8s heureux de vous pr\xE9senter la biblioth\xE8que \u{1F917} Transformers.&quot;</span>)
[{<span class="hljs-string">&#x27;label&#x27;</span>: <span class="hljs-string">&#x27;5 stars&#x27;</span>, <span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.7273</span>}]`}}),It=new ct({}),Ct=new zp({props:{id:"AhChOFRegn4"}}),Ot=new ct({}),Nt=new W({props:{code:`from transformers import AutoTokenizer

model_name = "nlptown/bert-base-multilingual-uncased-sentiment"
tokenizer = AutoTokenizer.from_pretrained(model_name)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>model_name = <span class="hljs-string">&quot;nlptown/bert-base-multilingual-uncased-sentiment&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(model_name)`}}),Dt=new W({props:{code:`encoding = tokenizer("We are very happy to show you the \u{1F917} Transformers library.")
print(encoding)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>encoding = tokenizer(<span class="hljs-string">&quot;We are very happy to show you the \u{1F917} Transformers library.&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(encoding)
{<span class="hljs-string">&#x27;input_ids&#x27;</span>: [<span class="hljs-number">101</span>, <span class="hljs-number">11312</span>, <span class="hljs-number">10320</span>, <span class="hljs-number">12495</span>, <span class="hljs-number">19308</span>, <span class="hljs-number">10114</span>, <span class="hljs-number">11391</span>, <span class="hljs-number">10855</span>, <span class="hljs-number">10103</span>, <span class="hljs-number">100</span>, <span class="hljs-number">58263</span>, <span class="hljs-number">13299</span>, <span class="hljs-number">119</span>, <span class="hljs-number">102</span>],
 <span class="hljs-string">&#x27;token_type_ids&#x27;</span>: [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
 <span class="hljs-string">&#x27;attention_mask&#x27;</span>: [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]}`}}),rt=new Ss({props:{pytorch:!0,tensorflow:!0,jax:!1,$$slots:{tensorflow:[Vp],pytorch:[Qp]},$$scope:{ctx:P}}}),Ht=new ct({}),lt=new Ss({props:{pytorch:!0,tensorflow:!0,jax:!1,$$slots:{tensorflow:[sf],pytorch:[Xp]},$$scope:{ctx:P}}}),it=new Is({props:{$$slots:{default:[af]},$$scope:{ctx:P}}}),pt=new Is({props:{$$slots:{default:[rf]},$$scope:{ctx:P}}}),Rt=new ct({}),ut=new Ss({props:{pytorch:!0,tensorflow:!0,jax:!1,$$slots:{tensorflow:[pf],pytorch:[nf]},$$scope:{ctx:P}}}),mt=new Ss({props:{pytorch:!0,tensorflow:!0,jax:!1,$$slots:{tensorflow:[cf],pytorch:[uf]},$$scope:{ctx:P}}}),{c(){a=l("meta"),m=h(),r=l("h1"),c=l("a"),_=l("span"),y(v.$$.fragment),z=h(),I=l("span"),w=o("Quick tour"),M=h(),y(C.$$.fragment),O=h(),D=l("p"),L=o("Get up and running with \u{1F917} Transformers! Start using the "),x=l("a"),S=o("pipeline()"),g=o(" for rapid inference, and quickly load a pretrained model and tokenizer with an "),F=l("a"),R=o("AutoClass"),U=o(" to solve your text, vision or audio task."),Q=h(),y(J.$$.fragment),se=h(),B=l("h2"),G=l("a"),ee=l("span"),y(V.$$.fragment),K=h(),ce=l("span"),re=o("Pipeline"),de=h(),oe=l("p"),te=l("a"),ne=o("pipeline()"),$e=o(" is the easiest way to use a pretrained model for a given task."),q=h(),y(N.$$.fragment),le=h(),k=l("p"),H=o("The "),ie=l("a"),Pe=o("pipeline()"),he=o(" supports many common tasks out-of-the-box:"),ge=h(),pe=l("p"),De=l("strong"),ve=o("Text"),Jt=o(":"),ht=h(),Y=l("ul"),Cs=l("li"),Zr=o("Sentiment analysis: classify the polarity of a given text."),Xr=h(),Os=l("li"),eo=o("Text generation (in English): generate text from a given input."),to=h(),Ns=l("li"),so=o("Name entity recognition (NER): label each word with the entity it represents (person, date, location, etc.)."),ao=h(),Ds=l("li"),ro=o("Question answering: extract the answer from the context, given some context and a question."),oo=h(),Hs=l("li"),no=o("Fill-mask: fill in the blank given a text with masked words."),lo=h(),Ls=l("li"),io=o("Summarization: generate a summary of a long sequence of text or document."),po=h(),Ws=l("li"),fo=o("Translation: translate text into another language."),uo=h(),Us=l("li"),mo=o("Feature extraction: create a tensor representation of the text."),xa=h(),dt=l("p"),Rs=l("strong"),co=o("Image"),ho=o(":"),qa=h(),we=l("ul"),Gs=l("li"),$o=o("Image classification: classify an image."),_o=h(),Ys=l("li"),go=o("Image segmentation: classify every pixel in an image."),vo=h(),Js=l("li"),wo=o("Object detection: detect objects within an image."),za=h(),$t=l("p"),Qs=l("strong"),ko=o("Audio"),yo=o(":"),Pa=h(),He=l("ul"),Bs=l("li"),bo=o("Audio classification: assign a label to a given segment of audio."),Eo=h(),Vs=l("li"),Ao=o("Automatic speech recognition (ASR): transcribe audio data into text."),Fa=h(),y(Le.$$.fragment),Ma=h(),Fe=l("h3"),We=l("a"),Ks=l("span"),y(_t.$$.fragment),To=h(),Zs=l("span"),jo=o("Pipeline usage"),Sa=h(),Ue=l("p"),xo=o("In the following example, you will use the "),Qt=l("a"),qo=o("pipeline()"),zo=o(" for sentiment analysis."),Ia=h(),Bt=l("p"),Po=o("Install the following dependencies if you haven\u2019t already:"),Ca=h(),y(Re.$$.fragment),Oa=h(),Ge=l("p"),Fo=o("Import "),Vt=l("a"),Mo=o("pipeline()"),So=o(" and specify the task you want to complete:"),Na=h(),y(gt.$$.fragment),Da=h(),ke=l("p"),Io=o("The pipeline downloads and caches a default "),vt=l("a"),Co=o("pretrained model"),Oo=o(" and tokenizer for sentiment analysis. Now you can use the "),Xs=l("code"),No=o("classifier"),Do=o(" on your target text:"),Ha=h(),y(wt.$$.fragment),La=h(),Ye=l("p"),Ho=o("For more than one sentence, pass a list of sentences to the "),Kt=l("a"),Lo=o("pipeline()"),Wo=o(" which returns a list of dictionaries:"),Wa=h(),y(kt.$$.fragment),Ua=h(),ye=l("p"),Uo=o("The "),Zt=l("a"),Ro=o("pipeline()"),Go=o(" can also iterate over an entire dataset. Start by installing the "),yt=l("a"),Yo=o("\u{1F917} Datasets"),Jo=o(" library:"),Ra=h(),y(bt.$$.fragment),Ga=h(),Je=l("p"),Qo=o("Create a "),Xt=l("a"),Bo=o("pipeline()"),Vo=o(" with the task you want to solve for and the model you want to use."),Ya=h(),y(Et.$$.fragment),Ja=h(),be=l("p"),Ko=o("Next, load a dataset (see the \u{1F917} Datasets "),At=l("a"),Zo=o("Quick Start"),Xo=o(" for more details) you\u2019d like to iterate over. For example, let\u2019s load the "),Tt=l("a"),en=o("MInDS-14"),tn=o(" dataset:"),Qa=h(),y(jt.$$.fragment),Ba=h(),Qe=l("p"),sn=o(`We need to make sure that the sampling rate of the dataset matches the sampling
rate `),ea=l("code"),an=o("facebook/wav2vec2-base-960h"),rn=o(" was trained on."),Va=h(),y(xt.$$.fragment),Ka=h(),es=l("p"),on=o("You can pass a whole dataset pipeline:"),Za=h(),y(qt.$$.fragment),Xa=h(),Be=l("p"),nn=o("For a larger dataset where the inputs are big (like in speech or vision), you will want to pass along a generator instead of a list that loads all the inputs in memory. See the "),ts=l("a"),ln=o("pipeline documentation"),pn=o(" for more information."),er=h(),Me=l("h3"),Ve=l("a"),ta=l("span"),y(zt.$$.fragment),fn=h(),sa=l("span"),un=o("Use another model and tokenizer in the pipeline"),tr=h(),fe=l("p"),mn=o("The "),ss=l("a"),cn=o("pipeline()"),hn=o(" can accommodate any model from the "),Pt=l("a"),dn=o("Model Hub"),$n=o(", making it easy to adapt the "),as=l("a"),_n=o("pipeline()"),gn=o(" for other use-cases. For example, if you\u2019d like a model capable of handling French text, use the tags on the Model Hub to filter for an appropriate model. The top filtered result returns a multilingual "),Ft=l("a"),vn=o("BERT model"),wn=o(" fine-tuned for sentiment analysis. Great, let\u2019s use this model!"),sr=h(),y(Mt.$$.fragment),ar=h(),y(Ke.$$.fragment),rr=h(),Ee=l("p"),kn=o("Then you can specify the model and tokenizer in the "),rs=l("a"),yn=o("pipeline()"),bn=o(", and apply the "),aa=l("code"),En=o("classifier"),An=o(" on your target text:"),or=h(),y(St.$$.fragment),nr=h(),Ae=l("p"),Tn=o("If you can\u2019t find a model for your use-case, you will need to fine-tune a pretrained model on your data. Take a look at our "),os=l("a"),jn=o("fine-tuning tutorial"),xn=o(" to learn how. Finally, after you\u2019ve fine-tuned your pretrained model, please consider sharing it (see tutorial "),ns=l("a"),qn=o("here"),zn=o(") with the community on the Model Hub to democratize NLP for everyone! \u{1F917}"),lr=h(),Se=l("h2"),Ze=l("a"),ra=l("span"),y(It.$$.fragment),Pn=h(),oa=l("span"),Fn=o("AutoClass"),ir=h(),y(Ct.$$.fragment),pr=h(),Z=l("p"),Mn=o("Under the hood, the "),ls=l("a"),Sn=o("AutoModelForSequenceClassification"),In=o(" and "),is=l("a"),Cn=o("AutoTokenizer"),On=o(" classes work together to power the "),ps=l("a"),Nn=o("pipeline()"),Dn=o(". An "),fs=l("a"),Hn=o("AutoClass"),Ln=o(" is a shortcut that automatically retrieves the architecture of a pretrained model from it\u2019s name or path. You only need to select the appropriate "),na=l("code"),Wn=o("AutoClass"),Un=o(" for your task and it\u2019s associated tokenizer with "),us=l("a"),Rn=o("AutoTokenizer"),Gn=o("."),fr=h(),Te=l("p"),Yn=o("Let\u2019s return to our example and see how you can use the "),la=l("code"),Jn=o("AutoClass"),Qn=o(" to replicate the results of the "),ms=l("a"),Bn=o("pipeline()"),Vn=o("."),ur=h(),Ie=l("h3"),Xe=l("a"),ia=l("span"),y(Ot.$$.fragment),Kn=h(),pa=l("span"),Zn=o("AutoTokenizer"),mr=h(),je=l("p"),Xn=o("A tokenizer is responsible for preprocessing text into a format that is understandable to the model. First, the tokenizer will split the text into words called "),fa=l("em"),el=o("tokens"),tl=o(". There are multiple rules that govern the tokenization process, including how to split a word and at what level (learn more about tokenization "),cs=l("a"),sl=o("here"),al=o("). The most important thing to remember though is you need to instantiate the tokenizer with the same model name to ensure you\u2019re using the same tokenization rules a model was pretrained with."),cr=h(),et=l("p"),rl=o("Load a tokenizer with "),hs=l("a"),ol=o("AutoTokenizer"),nl=o(":"),hr=h(),y(Nt.$$.fragment),dr=h(),tt=l("p"),ll=o("Next, the tokenizer converts the tokens into numbers in order to construct a tensor as input to the model. This is known as the model\u2019s "),ua=l("em"),il=o("vocabulary"),pl=o("."),$r=h(),ds=l("p"),fl=o("Pass your text to the tokenizer:"),_r=h(),y(Dt.$$.fragment),gr=h(),$s=l("p"),ul=o("The tokenizer will return a dictionary containing:"),vr=h(),st=l("ul"),_s=l("li"),gs=l("a"),ml=o("input_ids"),cl=o(": numerical representions of your tokens."),hl=h(),vs=l("li"),ws=l("a"),dl=o("atttention_mask"),$l=o(": indicates which tokens should be attended to."),wr=h(),at=l("p"),_l=o("Just like the "),ks=l("a"),gl=o("pipeline()"),vl=o(", the tokenizer will accept a list of inputs. In addition, the tokenizer can also pad and truncate the text to return a batch with uniform length:"),kr=h(),y(rt.$$.fragment),yr=h(),ot=l("p"),wl=o("Read the "),ys=l("a"),kl=o("preprocessing"),yl=o(" tutorial for more details about tokenization."),br=h(),Ce=l("h3"),nt=l("a"),ma=l("span"),y(Ht.$$.fragment),bl=h(),ca=l("span"),El=o("AutoModel"),Er=h(),y(lt.$$.fragment),Ar=h(),y(it.$$.fragment),Tr=h(),X=l("p"),Al=o("Models are a standard "),Lt=l("a"),ha=l("code"),Tl=o("torch.nn.Module"),jl=o(" or a "),Wt=l("a"),da=l("code"),xl=o("tf.keras.Model"),ql=o(" so you can use them in your usual training loop. However, to make things easier, \u{1F917} Transformers provides a "),bs=l("a"),zl=o("Trainer"),Pl=o(" class for PyTorch that adds functionality for distributed training, mixed precision, and more. For TensorFlow, you can use the "),$a=l("code"),Fl=o("fit"),Ml=o(" method from "),Ut=l("a"),Sl=o("Keras"),Il=o(". Refer to the "),Es=l("a"),Cl=o("training tutorial"),Ol=o(" for more details."),jr=h(),y(pt.$$.fragment),xr=h(),Oe=l("h3"),ft=l("a"),_a=l("span"),y(Rt.$$.fragment),Nl=h(),ga=l("span"),Dl=o("Save a model"),qr=h(),y(ut.$$.fragment),zr=h(),xe=l("p"),Hl=o("One particularly cool \u{1F917} Transformers feature is the ability to save a model and reload it as either a PyTorch or TensorFlow model. The "),va=l("code"),Ll=o("from_pt"),Wl=o(" or "),wa=l("code"),Ul=o("from_tf"),Rl=o(" parameter can convert the model from one framework to the other:"),Pr=h(),y(mt.$$.fragment),this.h()},l(e){const f=Sp('[data-svelte="svelte-1phssyn"]',document.head);a=i(f,"META",{name:!0,content:!0}),f.forEach(s),m=d(e),r=i(e,"H1",{class:!0});var Gt=p(r);c=i(Gt,"A",{id:!0,class:!0,href:!0});var ka=p(c);_=i(ka,"SPAN",{});var ya=p(_);b(v.$$.fragment,ya),ya.forEach(s),ka.forEach(s),z=d(Gt),I=i(Gt,"SPAN",{});var ba=p(I);w=n(ba,"Quick tour"),ba.forEach(s),Gt.forEach(s),M=d(e),b(C.$$.fragment,e),O=d(e),D=i(e,"P",{});var Ne=p(D);L=n(Ne,"Get up and running with \u{1F917} Transformers! Start using the "),x=i(Ne,"A",{href:!0});var Ea=p(x);S=n(Ea,"pipeline()"),Ea.forEach(s),g=n(Ne," for rapid inference, and quickly load a pretrained model and tokenizer with an "),F=i(Ne,"A",{href:!0});var Aa=p(F);R=n(Aa,"AutoClass"),Aa.forEach(s),U=n(Ne," to solve your text, vision or audio task."),Ne.forEach(s),Q=d(e),b(J.$$.fragment,e),se=d(e),B=i(e,"H2",{class:!0});var Yt=p(B);G=i(Yt,"A",{id:!0,class:!0,href:!0});var Ta=p(G);ee=i(Ta,"SPAN",{});var ja=p(ee);b(V.$$.fragment,ja),ja.forEach(s),Ta.forEach(s),K=d(Yt),ce=i(Yt,"SPAN",{});var Kl=p(ce);re=n(Kl,"Pipeline"),Kl.forEach(s),Yt.forEach(s),de=d(e),oe=i(e,"P",{});var Gl=p(oe);te=i(Gl,"A",{href:!0});var Zl=p(te);ne=n(Zl,"pipeline()"),Zl.forEach(s),$e=n(Gl," is the easiest way to use a pretrained model for a given task."),Gl.forEach(s),q=d(e),b(N.$$.fragment,e),le=d(e),k=i(e,"P",{});var Mr=p(k);H=n(Mr,"The "),ie=i(Mr,"A",{href:!0});var Xl=p(ie);Pe=n(Xl,"pipeline()"),Xl.forEach(s),he=n(Mr," supports many common tasks out-of-the-box:"),Mr.forEach(s),ge=d(e),pe=i(e,"P",{});var Yl=p(pe);De=i(Yl,"STRONG",{});var ei=p(De);ve=n(ei,"Text"),ei.forEach(s),Jt=n(Yl,":"),Yl.forEach(s),ht=d(e),Y=i(e,"UL",{});var ae=p(Y);Cs=i(ae,"LI",{});var ti=p(Cs);Zr=n(ti,"Sentiment analysis: classify the polarity of a given text."),ti.forEach(s),Xr=d(ae),Os=i(ae,"LI",{});var si=p(Os);eo=n(si,"Text generation (in English): generate text from a given input."),si.forEach(s),to=d(ae),Ns=i(ae,"LI",{});var ai=p(Ns);so=n(ai,"Name entity recognition (NER): label each word with the entity it represents (person, date, location, etc.)."),ai.forEach(s),ao=d(ae),Ds=i(ae,"LI",{});var ri=p(Ds);ro=n(ri,"Question answering: extract the answer from the context, given some context and a question."),ri.forEach(s),oo=d(ae),Hs=i(ae,"LI",{});var oi=p(Hs);no=n(oi,"Fill-mask: fill in the blank given a text with masked words."),oi.forEach(s),lo=d(ae),Ls=i(ae,"LI",{});var ni=p(Ls);io=n(ni,"Summarization: generate a summary of a long sequence of text or document."),ni.forEach(s),po=d(ae),Ws=i(ae,"LI",{});var li=p(Ws);fo=n(li,"Translation: translate text into another language."),li.forEach(s),uo=d(ae),Us=i(ae,"LI",{});var ii=p(Us);mo=n(ii,"Feature extraction: create a tensor representation of the text."),ii.forEach(s),ae.forEach(s),xa=d(e),dt=i(e,"P",{});var Jl=p(dt);Rs=i(Jl,"STRONG",{});var pi=p(Rs);co=n(pi,"Image"),pi.forEach(s),ho=n(Jl,":"),Jl.forEach(s),qa=d(e),we=i(e,"UL",{});var As=p(we);Gs=i(As,"LI",{});var fi=p(Gs);$o=n(fi,"Image classification: classify an image."),fi.forEach(s),_o=d(As),Ys=i(As,"LI",{});var ui=p(Ys);go=n(ui,"Image segmentation: classify every pixel in an image."),ui.forEach(s),vo=d(As),Js=i(As,"LI",{});var mi=p(Js);wo=n(mi,"Object detection: detect objects within an image."),mi.forEach(s),As.forEach(s),za=d(e),$t=i(e,"P",{});var Ql=p($t);Qs=i(Ql,"STRONG",{});var ci=p(Qs);ko=n(ci,"Audio"),ci.forEach(s),yo=n(Ql,":"),Ql.forEach(s),Pa=d(e),He=i(e,"UL",{});var Sr=p(He);Bs=i(Sr,"LI",{});var hi=p(Bs);bo=n(hi,"Audio classification: assign a label to a given segment of audio."),hi.forEach(s),Eo=d(Sr),Vs=i(Sr,"LI",{});var di=p(Vs);Ao=n(di,"Automatic speech recognition (ASR): transcribe audio data into text."),di.forEach(s),Sr.forEach(s),Fa=d(e),b(Le.$$.fragment,e),Ma=d(e),Fe=i(e,"H3",{class:!0});var Ir=p(Fe);We=i(Ir,"A",{id:!0,class:!0,href:!0});var $i=p(We);Ks=i($i,"SPAN",{});var _i=p(Ks);b(_t.$$.fragment,_i),_i.forEach(s),$i.forEach(s),To=d(Ir),Zs=i(Ir,"SPAN",{});var gi=p(Zs);jo=n(gi,"Pipeline usage"),gi.forEach(s),Ir.forEach(s),Sa=d(e),Ue=i(e,"P",{});var Cr=p(Ue);xo=n(Cr,"In the following example, you will use the "),Qt=i(Cr,"A",{href:!0});var vi=p(Qt);qo=n(vi,"pipeline()"),vi.forEach(s),zo=n(Cr," for sentiment analysis."),Cr.forEach(s),Ia=d(e),Bt=i(e,"P",{});var wi=p(Bt);Po=n(wi,"Install the following dependencies if you haven\u2019t already:"),wi.forEach(s),Ca=d(e),b(Re.$$.fragment,e),Oa=d(e),Ge=i(e,"P",{});var Or=p(Ge);Fo=n(Or,"Import "),Vt=i(Or,"A",{href:!0});var ki=p(Vt);Mo=n(ki,"pipeline()"),ki.forEach(s),So=n(Or," and specify the task you want to complete:"),Or.forEach(s),Na=d(e),b(gt.$$.fragment,e),Da=d(e),ke=i(e,"P",{});var Ts=p(ke);Io=n(Ts,"The pipeline downloads and caches a default "),vt=i(Ts,"A",{href:!0,rel:!0});var yi=p(vt);Co=n(yi,"pretrained model"),yi.forEach(s),Oo=n(Ts," and tokenizer for sentiment analysis. Now you can use the "),Xs=i(Ts,"CODE",{});var bi=p(Xs);No=n(bi,"classifier"),bi.forEach(s),Do=n(Ts," on your target text:"),Ts.forEach(s),Ha=d(e),b(wt.$$.fragment,e),La=d(e),Ye=i(e,"P",{});var Nr=p(Ye);Ho=n(Nr,"For more than one sentence, pass a list of sentences to the "),Kt=i(Nr,"A",{href:!0});var Ei=p(Kt);Lo=n(Ei,"pipeline()"),Ei.forEach(s),Wo=n(Nr," which returns a list of dictionaries:"),Nr.forEach(s),Wa=d(e),b(kt.$$.fragment,e),Ua=d(e),ye=i(e,"P",{});var js=p(ye);Uo=n(js,"The "),Zt=i(js,"A",{href:!0});var Ai=p(Zt);Ro=n(Ai,"pipeline()"),Ai.forEach(s),Go=n(js," can also iterate over an entire dataset. Start by installing the "),yt=i(js,"A",{href:!0,rel:!0});var Ti=p(yt);Yo=n(Ti,"\u{1F917} Datasets"),Ti.forEach(s),Jo=n(js," library:"),js.forEach(s),Ra=d(e),b(bt.$$.fragment,e),Ga=d(e),Je=i(e,"P",{});var Dr=p(Je);Qo=n(Dr,"Create a "),Xt=i(Dr,"A",{href:!0});var ji=p(Xt);Bo=n(ji,"pipeline()"),ji.forEach(s),Vo=n(Dr," with the task you want to solve for and the model you want to use."),Dr.forEach(s),Ya=d(e),b(Et.$$.fragment,e),Ja=d(e),be=i(e,"P",{});var xs=p(be);Ko=n(xs,"Next, load a dataset (see the \u{1F917} Datasets "),At=i(xs,"A",{href:!0,rel:!0});var xi=p(At);Zo=n(xi,"Quick Start"),xi.forEach(s),Xo=n(xs," for more details) you\u2019d like to iterate over. For example, let\u2019s load the "),Tt=i(xs,"A",{href:!0,rel:!0});var qi=p(Tt);en=n(qi,"MInDS-14"),qi.forEach(s),tn=n(xs," dataset:"),xs.forEach(s),Qa=d(e),b(jt.$$.fragment,e),Ba=d(e),Qe=i(e,"P",{});var Hr=p(Qe);sn=n(Hr,`We need to make sure that the sampling rate of the dataset matches the sampling
rate `),ea=i(Hr,"CODE",{});var zi=p(ea);an=n(zi,"facebook/wav2vec2-base-960h"),zi.forEach(s),rn=n(Hr," was trained on."),Hr.forEach(s),Va=d(e),b(xt.$$.fragment,e),Ka=d(e),es=i(e,"P",{});var Pi=p(es);on=n(Pi,"You can pass a whole dataset pipeline:"),Pi.forEach(s),Za=d(e),b(qt.$$.fragment,e),Xa=d(e),Be=i(e,"P",{});var Lr=p(Be);nn=n(Lr,"For a larger dataset where the inputs are big (like in speech or vision), you will want to pass along a generator instead of a list that loads all the inputs in memory. See the "),ts=i(Lr,"A",{href:!0});var Fi=p(ts);ln=n(Fi,"pipeline documentation"),Fi.forEach(s),pn=n(Lr," for more information."),Lr.forEach(s),er=d(e),Me=i(e,"H3",{class:!0});var Wr=p(Me);Ve=i(Wr,"A",{id:!0,class:!0,href:!0});var Mi=p(Ve);ta=i(Mi,"SPAN",{});var Si=p(ta);b(zt.$$.fragment,Si),Si.forEach(s),Mi.forEach(s),fn=d(Wr),sa=i(Wr,"SPAN",{});var Ii=p(sa);un=n(Ii,"Use another model and tokenizer in the pipeline"),Ii.forEach(s),Wr.forEach(s),tr=d(e),fe=i(e,"P",{});var qe=p(fe);mn=n(qe,"The "),ss=i(qe,"A",{href:!0});var Ci=p(ss);cn=n(Ci,"pipeline()"),Ci.forEach(s),hn=n(qe," can accommodate any model from the "),Pt=i(qe,"A",{href:!0,rel:!0});var Oi=p(Pt);dn=n(Oi,"Model Hub"),Oi.forEach(s),$n=n(qe,", making it easy to adapt the "),as=i(qe,"A",{href:!0});var Ni=p(as);_n=n(Ni,"pipeline()"),Ni.forEach(s),gn=n(qe," for other use-cases. For example, if you\u2019d like a model capable of handling French text, use the tags on the Model Hub to filter for an appropriate model. The top filtered result returns a multilingual "),Ft=i(qe,"A",{href:!0,rel:!0});var Di=p(Ft);vn=n(Di,"BERT model"),Di.forEach(s),wn=n(qe," fine-tuned for sentiment analysis. Great, let\u2019s use this model!"),qe.forEach(s),sr=d(e),b(Mt.$$.fragment,e),ar=d(e),b(Ke.$$.fragment,e),rr=d(e),Ee=i(e,"P",{});var qs=p(Ee);kn=n(qs,"Then you can specify the model and tokenizer in the "),rs=i(qs,"A",{href:!0});var Hi=p(rs);yn=n(Hi,"pipeline()"),Hi.forEach(s),bn=n(qs,", and apply the "),aa=i(qs,"CODE",{});var Li=p(aa);En=n(Li,"classifier"),Li.forEach(s),An=n(qs," on your target text:"),qs.forEach(s),or=d(e),b(St.$$.fragment,e),nr=d(e),Ae=i(e,"P",{});var zs=p(Ae);Tn=n(zs,"If you can\u2019t find a model for your use-case, you will need to fine-tune a pretrained model on your data. Take a look at our "),os=i(zs,"A",{href:!0});var Wi=p(os);jn=n(Wi,"fine-tuning tutorial"),Wi.forEach(s),xn=n(zs," to learn how. Finally, after you\u2019ve fine-tuned your pretrained model, please consider sharing it (see tutorial "),ns=i(zs,"A",{href:!0});var Ui=p(ns);qn=n(Ui,"here"),Ui.forEach(s),zn=n(zs,") with the community on the Model Hub to democratize NLP for everyone! \u{1F917}"),zs.forEach(s),lr=d(e),Se=i(e,"H2",{class:!0});var Ur=p(Se);Ze=i(Ur,"A",{id:!0,class:!0,href:!0});var Ri=p(Ze);ra=i(Ri,"SPAN",{});var Gi=p(ra);b(It.$$.fragment,Gi),Gi.forEach(s),Ri.forEach(s),Pn=d(Ur),oa=i(Ur,"SPAN",{});var Yi=p(oa);Fn=n(Yi,"AutoClass"),Yi.forEach(s),Ur.forEach(s),ir=d(e),b(Ct.$$.fragment,e),pr=d(e),Z=i(e,"P",{});var ue=p(Z);Mn=n(ue,"Under the hood, the "),ls=i(ue,"A",{href:!0});var Ji=p(ls);Sn=n(Ji,"AutoModelForSequenceClassification"),Ji.forEach(s),In=n(ue," and "),is=i(ue,"A",{href:!0});var Qi=p(is);Cn=n(Qi,"AutoTokenizer"),Qi.forEach(s),On=n(ue," classes work together to power the "),ps=i(ue,"A",{href:!0});var Bi=p(ps);Nn=n(Bi,"pipeline()"),Bi.forEach(s),Dn=n(ue,". An "),fs=i(ue,"A",{href:!0});var Vi=p(fs);Hn=n(Vi,"AutoClass"),Vi.forEach(s),Ln=n(ue," is a shortcut that automatically retrieves the architecture of a pretrained model from it\u2019s name or path. You only need to select the appropriate "),na=i(ue,"CODE",{});var Ki=p(na);Wn=n(Ki,"AutoClass"),Ki.forEach(s),Un=n(ue," for your task and it\u2019s associated tokenizer with "),us=i(ue,"A",{href:!0});var Zi=p(us);Rn=n(Zi,"AutoTokenizer"),Zi.forEach(s),Gn=n(ue,"."),ue.forEach(s),fr=d(e),Te=i(e,"P",{});var Ps=p(Te);Yn=n(Ps,"Let\u2019s return to our example and see how you can use the "),la=i(Ps,"CODE",{});var Xi=p(la);Jn=n(Xi,"AutoClass"),Xi.forEach(s),Qn=n(Ps," to replicate the results of the "),ms=i(Ps,"A",{href:!0});var ep=p(ms);Bn=n(ep,"pipeline()"),ep.forEach(s),Vn=n(Ps,"."),Ps.forEach(s),ur=d(e),Ie=i(e,"H3",{class:!0});var Rr=p(Ie);Xe=i(Rr,"A",{id:!0,class:!0,href:!0});var tp=p(Xe);ia=i(tp,"SPAN",{});var sp=p(ia);b(Ot.$$.fragment,sp),sp.forEach(s),tp.forEach(s),Kn=d(Rr),pa=i(Rr,"SPAN",{});var ap=p(pa);Zn=n(ap,"AutoTokenizer"),ap.forEach(s),Rr.forEach(s),mr=d(e),je=i(e,"P",{});var Fs=p(je);Xn=n(Fs,"A tokenizer is responsible for preprocessing text into a format that is understandable to the model. First, the tokenizer will split the text into words called "),fa=i(Fs,"EM",{});var rp=p(fa);el=n(rp,"tokens"),rp.forEach(s),tl=n(Fs,". There are multiple rules that govern the tokenization process, including how to split a word and at what level (learn more about tokenization "),cs=i(Fs,"A",{href:!0});var op=p(cs);sl=n(op,"here"),op.forEach(s),al=n(Fs,"). The most important thing to remember though is you need to instantiate the tokenizer with the same model name to ensure you\u2019re using the same tokenization rules a model was pretrained with."),Fs.forEach(s),cr=d(e),et=i(e,"P",{});var Gr=p(et);rl=n(Gr,"Load a tokenizer with "),hs=i(Gr,"A",{href:!0});var np=p(hs);ol=n(np,"AutoTokenizer"),np.forEach(s),nl=n(Gr,":"),Gr.forEach(s),hr=d(e),b(Nt.$$.fragment,e),dr=d(e),tt=i(e,"P",{});var Yr=p(tt);ll=n(Yr,"Next, the tokenizer converts the tokens into numbers in order to construct a tensor as input to the model. This is known as the model\u2019s "),ua=i(Yr,"EM",{});var lp=p(ua);il=n(lp,"vocabulary"),lp.forEach(s),pl=n(Yr,"."),Yr.forEach(s),$r=d(e),ds=i(e,"P",{});var ip=p(ds);fl=n(ip,"Pass your text to the tokenizer:"),ip.forEach(s),_r=d(e),b(Dt.$$.fragment,e),gr=d(e),$s=i(e,"P",{});var pp=p($s);ul=n(pp,"The tokenizer will return a dictionary containing:"),pp.forEach(s),vr=d(e),st=i(e,"UL",{});var Jr=p(st);_s=i(Jr,"LI",{});var Bl=p(_s);gs=i(Bl,"A",{href:!0});var fp=p(gs);ml=n(fp,"input_ids"),fp.forEach(s),cl=n(Bl,": numerical representions of your tokens."),Bl.forEach(s),hl=d(Jr),vs=i(Jr,"LI",{});var Vl=p(vs);ws=i(Vl,"A",{href:!0});var up=p(ws);dl=n(up,"atttention_mask"),up.forEach(s),$l=n(Vl,": indicates which tokens should be attended to."),Vl.forEach(s),Jr.forEach(s),wr=d(e),at=i(e,"P",{});var Qr=p(at);_l=n(Qr,"Just like the "),ks=i(Qr,"A",{href:!0});var mp=p(ks);gl=n(mp,"pipeline()"),mp.forEach(s),vl=n(Qr,", the tokenizer will accept a list of inputs. In addition, the tokenizer can also pad and truncate the text to return a batch with uniform length:"),Qr.forEach(s),kr=d(e),b(rt.$$.fragment,e),yr=d(e),ot=i(e,"P",{});var Br=p(ot);wl=n(Br,"Read the "),ys=i(Br,"A",{href:!0});var cp=p(ys);kl=n(cp,"preprocessing"),cp.forEach(s),yl=n(Br," tutorial for more details about tokenization."),Br.forEach(s),br=d(e),Ce=i(e,"H3",{class:!0});var Vr=p(Ce);nt=i(Vr,"A",{id:!0,class:!0,href:!0});var hp=p(nt);ma=i(hp,"SPAN",{});var dp=p(ma);b(Ht.$$.fragment,dp),dp.forEach(s),hp.forEach(s),bl=d(Vr),ca=i(Vr,"SPAN",{});var $p=p(ca);El=n($p,"AutoModel"),$p.forEach(s),Vr.forEach(s),Er=d(e),b(lt.$$.fragment,e),Ar=d(e),b(it.$$.fragment,e),Tr=d(e),X=i(e,"P",{});var me=p(X);Al=n(me,"Models are a standard "),Lt=i(me,"A",{href:!0,rel:!0});var _p=p(Lt);ha=i(_p,"CODE",{});var gp=p(ha);Tl=n(gp,"torch.nn.Module"),gp.forEach(s),_p.forEach(s),jl=n(me," or a "),Wt=i(me,"A",{href:!0,rel:!0});var vp=p(Wt);da=i(vp,"CODE",{});var wp=p(da);xl=n(wp,"tf.keras.Model"),wp.forEach(s),vp.forEach(s),ql=n(me," so you can use them in your usual training loop. However, to make things easier, \u{1F917} Transformers provides a "),bs=i(me,"A",{href:!0});var kp=p(bs);zl=n(kp,"Trainer"),kp.forEach(s),Pl=n(me," class for PyTorch that adds functionality for distributed training, mixed precision, and more. For TensorFlow, you can use the "),$a=i(me,"CODE",{});var yp=p($a);Fl=n(yp,"fit"),yp.forEach(s),Ml=n(me," method from "),Ut=i(me,"A",{href:!0,rel:!0});var bp=p(Ut);Sl=n(bp,"Keras"),bp.forEach(s),Il=n(me,". Refer to the "),Es=i(me,"A",{href:!0});var Ep=p(Es);Cl=n(Ep,"training tutorial"),Ep.forEach(s),Ol=n(me," for more details."),me.forEach(s),jr=d(e),b(pt.$$.fragment,e),xr=d(e),Oe=i(e,"H3",{class:!0});var Kr=p(Oe);ft=i(Kr,"A",{id:!0,class:!0,href:!0});var Ap=p(ft);_a=i(Ap,"SPAN",{});var Tp=p(_a);b(Rt.$$.fragment,Tp),Tp.forEach(s),Ap.forEach(s),Nl=d(Kr),ga=i(Kr,"SPAN",{});var jp=p(ga);Dl=n(jp,"Save a model"),jp.forEach(s),Kr.forEach(s),qr=d(e),b(ut.$$.fragment,e),zr=d(e),xe=i(e,"P",{});var Ms=p(xe);Hl=n(Ms,"One particularly cool \u{1F917} Transformers feature is the ability to save a model and reload it as either a PyTorch or TensorFlow model. The "),va=i(Ms,"CODE",{});var xp=p(va);Ll=n(xp,"from_pt"),xp.forEach(s),Wl=n(Ms," or "),wa=i(Ms,"CODE",{});var qp=p(wa);Ul=n(qp,"from_tf"),qp.forEach(s),Rl=n(Ms," parameter can convert the model from one framework to the other:"),Ms.forEach(s),Pr=d(e),b(mt.$$.fragment,e),this.h()},h(){$(a,"name","hf:doc:metadata"),$(a,"content",JSON.stringify(df)),$(c,"id","quick-tour"),$(c,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),$(c,"href","#quick-tour"),$(r,"class","relative group"),$(x,"href","/docs/transformers/pr_16723/en/main_classes/pipelines#transformers.pipeline"),$(F,"href","./model_doc/auto"),$(G,"id","pipeline"),$(G,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),$(G,"href","#pipeline"),$(B,"class","relative group"),$(te,"href","/docs/transformers/pr_16723/en/main_classes/pipelines#transformers.pipeline"),$(ie,"href","/docs/transformers/pr_16723/en/main_classes/pipelines#transformers.pipeline"),$(We,"id","pipeline-usage"),$(We,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),$(We,"href","#pipeline-usage"),$(Fe,"class","relative group"),$(Qt,"href","/docs/transformers/pr_16723/en/main_classes/pipelines#transformers.pipeline"),$(Vt,"href","/docs/transformers/pr_16723/en/main_classes/pipelines#transformers.pipeline"),$(vt,"href","https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english"),$(vt,"rel","nofollow"),$(Kt,"href","/docs/transformers/pr_16723/en/main_classes/pipelines#transformers.pipeline"),$(Zt,"href","/docs/transformers/pr_16723/en/main_classes/pipelines#transformers.pipeline"),$(yt,"href","https://huggingface.co/docs/datasets/"),$(yt,"rel","nofollow"),$(Xt,"href","/docs/transformers/pr_16723/en/main_classes/pipelines#transformers.pipeline"),$(At,"href","https://huggingface.co/docs/datasets/quickstart.html"),$(At,"rel","nofollow"),$(Tt,"href","https://huggingface.co/datasets/PolyAI/minds14"),$(Tt,"rel","nofollow"),$(ts,"href","./main_classes/pipelines"),$(Ve,"id","use-another-model-and-tokenizer-in-the-pipeline"),$(Ve,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),$(Ve,"href","#use-another-model-and-tokenizer-in-the-pipeline"),$(Me,"class","relative group"),$(ss,"href","/docs/transformers/pr_16723/en/main_classes/pipelines#transformers.pipeline"),$(Pt,"href","https://huggingface.co/models"),$(Pt,"rel","nofollow"),$(as,"href","/docs/transformers/pr_16723/en/main_classes/pipelines#transformers.pipeline"),$(Ft,"href","https://huggingface.co/nlptown/bert-base-multilingual-uncased-sentiment"),$(Ft,"rel","nofollow"),$(rs,"href","/docs/transformers/pr_16723/en/main_classes/pipelines#transformers.pipeline"),$(os,"href","./training"),$(ns,"href","./model_sharing"),$(Ze,"id","autoclass"),$(Ze,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),$(Ze,"href","#autoclass"),$(Se,"class","relative group"),$(ls,"href","/docs/transformers/pr_16723/en/model_doc/auto#transformers.AutoModelForSequenceClassification"),$(is,"href","/docs/transformers/pr_16723/en/model_doc/auto#transformers.AutoTokenizer"),$(ps,"href","/docs/transformers/pr_16723/en/main_classes/pipelines#transformers.pipeline"),$(fs,"href","./model_doc/auto"),$(us,"href","/docs/transformers/pr_16723/en/model_doc/auto#transformers.AutoTokenizer"),$(ms,"href","/docs/transformers/pr_16723/en/main_classes/pipelines#transformers.pipeline"),$(Xe,"id","autotokenizer"),$(Xe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),$(Xe,"href","#autotokenizer"),$(Ie,"class","relative group"),$(cs,"href","./tokenizer_summary"),$(hs,"href","/docs/transformers/pr_16723/en/model_doc/auto#transformers.AutoTokenizer"),$(gs,"href","./glossary#input-ids"),$(ws,"href",".glossary#attention-mask"),$(ks,"href","/docs/transformers/pr_16723/en/main_classes/pipelines#transformers.pipeline"),$(ys,"href","./preprocessing"),$(nt,"id","automodel"),$(nt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),$(nt,"href","#automodel"),$(Ce,"class","relative group"),$(Lt,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),$(Lt,"rel","nofollow"),$(Wt,"href","https://www.tensorflow.org/api_docs/python/tf/keras/Model"),$(Wt,"rel","nofollow"),$(bs,"href","/docs/transformers/pr_16723/en/main_classes/trainer#transformers.Trainer"),$(Ut,"href","https://keras.io/"),$(Ut,"rel","nofollow"),$(Es,"href","./training"),$(ft,"id","save-a-model"),$(ft,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),$(ft,"href","#save-a-model"),$(Oe,"class","relative group")},m(e,f){t(document.head,a),u(e,m,f),u(e,r,f),t(r,c),t(c,_),E(v,_,null),t(r,z),t(r,I),t(I,w),u(e,M,f),E(C,e,f),u(e,O,f),u(e,D,f),t(D,L),t(D,x),t(x,S),t(D,g),t(D,F),t(F,R),t(D,U),u(e,Q,f),E(J,e,f),u(e,se,f),u(e,B,f),t(B,G),t(G,ee),E(V,ee,null),t(B,K),t(B,ce),t(ce,re),u(e,de,f),u(e,oe,f),t(oe,te),t(te,ne),t(oe,$e),u(e,q,f),E(N,e,f),u(e,le,f),u(e,k,f),t(k,H),t(k,ie),t(ie,Pe),t(k,he),u(e,ge,f),u(e,pe,f),t(pe,De),t(De,ve),t(pe,Jt),u(e,ht,f),u(e,Y,f),t(Y,Cs),t(Cs,Zr),t(Y,Xr),t(Y,Os),t(Os,eo),t(Y,to),t(Y,Ns),t(Ns,so),t(Y,ao),t(Y,Ds),t(Ds,ro),t(Y,oo),t(Y,Hs),t(Hs,no),t(Y,lo),t(Y,Ls),t(Ls,io),t(Y,po),t(Y,Ws),t(Ws,fo),t(Y,uo),t(Y,Us),t(Us,mo),u(e,xa,f),u(e,dt,f),t(dt,Rs),t(Rs,co),t(dt,ho),u(e,qa,f),u(e,we,f),t(we,Gs),t(Gs,$o),t(we,_o),t(we,Ys),t(Ys,go),t(we,vo),t(we,Js),t(Js,wo),u(e,za,f),u(e,$t,f),t($t,Qs),t(Qs,ko),t($t,yo),u(e,Pa,f),u(e,He,f),t(He,Bs),t(Bs,bo),t(He,Eo),t(He,Vs),t(Vs,Ao),u(e,Fa,f),E(Le,e,f),u(e,Ma,f),u(e,Fe,f),t(Fe,We),t(We,Ks),E(_t,Ks,null),t(Fe,To),t(Fe,Zs),t(Zs,jo),u(e,Sa,f),u(e,Ue,f),t(Ue,xo),t(Ue,Qt),t(Qt,qo),t(Ue,zo),u(e,Ia,f),u(e,Bt,f),t(Bt,Po),u(e,Ca,f),E(Re,e,f),u(e,Oa,f),u(e,Ge,f),t(Ge,Fo),t(Ge,Vt),t(Vt,Mo),t(Ge,So),u(e,Na,f),E(gt,e,f),u(e,Da,f),u(e,ke,f),t(ke,Io),t(ke,vt),t(vt,Co),t(ke,Oo),t(ke,Xs),t(Xs,No),t(ke,Do),u(e,Ha,f),E(wt,e,f),u(e,La,f),u(e,Ye,f),t(Ye,Ho),t(Ye,Kt),t(Kt,Lo),t(Ye,Wo),u(e,Wa,f),E(kt,e,f),u(e,Ua,f),u(e,ye,f),t(ye,Uo),t(ye,Zt),t(Zt,Ro),t(ye,Go),t(ye,yt),t(yt,Yo),t(ye,Jo),u(e,Ra,f),E(bt,e,f),u(e,Ga,f),u(e,Je,f),t(Je,Qo),t(Je,Xt),t(Xt,Bo),t(Je,Vo),u(e,Ya,f),E(Et,e,f),u(e,Ja,f),u(e,be,f),t(be,Ko),t(be,At),t(At,Zo),t(be,Xo),t(be,Tt),t(Tt,en),t(be,tn),u(e,Qa,f),E(jt,e,f),u(e,Ba,f),u(e,Qe,f),t(Qe,sn),t(Qe,ea),t(ea,an),t(Qe,rn),u(e,Va,f),E(xt,e,f),u(e,Ka,f),u(e,es,f),t(es,on),u(e,Za,f),E(qt,e,f),u(e,Xa,f),u(e,Be,f),t(Be,nn),t(Be,ts),t(ts,ln),t(Be,pn),u(e,er,f),u(e,Me,f),t(Me,Ve),t(Ve,ta),E(zt,ta,null),t(Me,fn),t(Me,sa),t(sa,un),u(e,tr,f),u(e,fe,f),t(fe,mn),t(fe,ss),t(ss,cn),t(fe,hn),t(fe,Pt),t(Pt,dn),t(fe,$n),t(fe,as),t(as,_n),t(fe,gn),t(fe,Ft),t(Ft,vn),t(fe,wn),u(e,sr,f),E(Mt,e,f),u(e,ar,f),E(Ke,e,f),u(e,rr,f),u(e,Ee,f),t(Ee,kn),t(Ee,rs),t(rs,yn),t(Ee,bn),t(Ee,aa),t(aa,En),t(Ee,An),u(e,or,f),E(St,e,f),u(e,nr,f),u(e,Ae,f),t(Ae,Tn),t(Ae,os),t(os,jn),t(Ae,xn),t(Ae,ns),t(ns,qn),t(Ae,zn),u(e,lr,f),u(e,Se,f),t(Se,Ze),t(Ze,ra),E(It,ra,null),t(Se,Pn),t(Se,oa),t(oa,Fn),u(e,ir,f),E(Ct,e,f),u(e,pr,f),u(e,Z,f),t(Z,Mn),t(Z,ls),t(ls,Sn),t(Z,In),t(Z,is),t(is,Cn),t(Z,On),t(Z,ps),t(ps,Nn),t(Z,Dn),t(Z,fs),t(fs,Hn),t(Z,Ln),t(Z,na),t(na,Wn),t(Z,Un),t(Z,us),t(us,Rn),t(Z,Gn),u(e,fr,f),u(e,Te,f),t(Te,Yn),t(Te,la),t(la,Jn),t(Te,Qn),t(Te,ms),t(ms,Bn),t(Te,Vn),u(e,ur,f),u(e,Ie,f),t(Ie,Xe),t(Xe,ia),E(Ot,ia,null),t(Ie,Kn),t(Ie,pa),t(pa,Zn),u(e,mr,f),u(e,je,f),t(je,Xn),t(je,fa),t(fa,el),t(je,tl),t(je,cs),t(cs,sl),t(je,al),u(e,cr,f),u(e,et,f),t(et,rl),t(et,hs),t(hs,ol),t(et,nl),u(e,hr,f),E(Nt,e,f),u(e,dr,f),u(e,tt,f),t(tt,ll),t(tt,ua),t(ua,il),t(tt,pl),u(e,$r,f),u(e,ds,f),t(ds,fl),u(e,_r,f),E(Dt,e,f),u(e,gr,f),u(e,$s,f),t($s,ul),u(e,vr,f),u(e,st,f),t(st,_s),t(_s,gs),t(gs,ml),t(_s,cl),t(st,hl),t(st,vs),t(vs,ws),t(ws,dl),t(vs,$l),u(e,wr,f),u(e,at,f),t(at,_l),t(at,ks),t(ks,gl),t(at,vl),u(e,kr,f),E(rt,e,f),u(e,yr,f),u(e,ot,f),t(ot,wl),t(ot,ys),t(ys,kl),t(ot,yl),u(e,br,f),u(e,Ce,f),t(Ce,nt),t(nt,ma),E(Ht,ma,null),t(Ce,bl),t(Ce,ca),t(ca,El),u(e,Er,f),E(lt,e,f),u(e,Ar,f),E(it,e,f),u(e,Tr,f),u(e,X,f),t(X,Al),t(X,Lt),t(Lt,ha),t(ha,Tl),t(X,jl),t(X,Wt),t(Wt,da),t(da,xl),t(X,ql),t(X,bs),t(bs,zl),t(X,Pl),t(X,$a),t($a,Fl),t(X,Ml),t(X,Ut),t(Ut,Sl),t(X,Il),t(X,Es),t(Es,Cl),t(X,Ol),u(e,jr,f),E(pt,e,f),u(e,xr,f),u(e,Oe,f),t(Oe,ft),t(ft,_a),E(Rt,_a,null),t(Oe,Nl),t(Oe,ga),t(ga,Dl),u(e,qr,f),E(ut,e,f),u(e,zr,f),u(e,xe,f),t(xe,Hl),t(xe,va),t(va,Ll),t(xe,Wl),t(xe,wa),t(wa,Ul),t(xe,Rl),u(e,Pr,f),E(mt,e,f),Fr=!0},p(e,[f]){const Gt={};f&2&&(Gt.$$scope={dirty:f,ctx:e}),J.$set(Gt);const ka={};f&2&&(ka.$$scope={dirty:f,ctx:e}),Le.$set(ka);const ya={};f&2&&(ya.$$scope={dirty:f,ctx:e}),Re.$set(ya);const ba={};f&2&&(ba.$$scope={dirty:f,ctx:e}),Ke.$set(ba);const Ne={};f&2&&(Ne.$$scope={dirty:f,ctx:e}),rt.$set(Ne);const Ea={};f&2&&(Ea.$$scope={dirty:f,ctx:e}),lt.$set(Ea);const Aa={};f&2&&(Aa.$$scope={dirty:f,ctx:e}),it.$set(Aa);const Yt={};f&2&&(Yt.$$scope={dirty:f,ctx:e}),pt.$set(Yt);const Ta={};f&2&&(Ta.$$scope={dirty:f,ctx:e}),ut.$set(Ta);const ja={};f&2&&(ja.$$scope={dirty:f,ctx:e}),mt.$set(ja)},i(e){Fr||(A(v.$$.fragment,e),A(C.$$.fragment,e),A(J.$$.fragment,e),A(V.$$.fragment,e),A(N.$$.fragment,e),A(Le.$$.fragment,e),A(_t.$$.fragment,e),A(Re.$$.fragment,e),A(gt.$$.fragment,e),A(wt.$$.fragment,e),A(kt.$$.fragment,e),A(bt.$$.fragment,e),A(Et.$$.fragment,e),A(jt.$$.fragment,e),A(xt.$$.fragment,e),A(qt.$$.fragment,e),A(zt.$$.fragment,e),A(Mt.$$.fragment,e),A(Ke.$$.fragment,e),A(St.$$.fragment,e),A(It.$$.fragment,e),A(Ct.$$.fragment,e),A(Ot.$$.fragment,e),A(Nt.$$.fragment,e),A(Dt.$$.fragment,e),A(rt.$$.fragment,e),A(Ht.$$.fragment,e),A(lt.$$.fragment,e),A(it.$$.fragment,e),A(pt.$$.fragment,e),A(Rt.$$.fragment,e),A(ut.$$.fragment,e),A(mt.$$.fragment,e),Fr=!0)},o(e){T(v.$$.fragment,e),T(C.$$.fragment,e),T(J.$$.fragment,e),T(V.$$.fragment,e),T(N.$$.fragment,e),T(Le.$$.fragment,e),T(_t.$$.fragment,e),T(Re.$$.fragment,e),T(gt.$$.fragment,e),T(wt.$$.fragment,e),T(kt.$$.fragment,e),T(bt.$$.fragment,e),T(Et.$$.fragment,e),T(jt.$$.fragment,e),T(xt.$$.fragment,e),T(qt.$$.fragment,e),T(zt.$$.fragment,e),T(Mt.$$.fragment,e),T(Ke.$$.fragment,e),T(St.$$.fragment,e),T(It.$$.fragment,e),T(Ct.$$.fragment,e),T(Ot.$$.fragment,e),T(Nt.$$.fragment,e),T(Dt.$$.fragment,e),T(rt.$$.fragment,e),T(Ht.$$.fragment,e),T(lt.$$.fragment,e),T(it.$$.fragment,e),T(pt.$$.fragment,e),T(Rt.$$.fragment,e),T(ut.$$.fragment,e),T(mt.$$.fragment,e),Fr=!1},d(e){s(a),e&&s(m),e&&s(r),j(v),e&&s(M),j(C,e),e&&s(O),e&&s(D),e&&s(Q),j(J,e),e&&s(se),e&&s(B),j(V),e&&s(de),e&&s(oe),e&&s(q),j(N,e),e&&s(le),e&&s(k),e&&s(ge),e&&s(pe),e&&s(ht),e&&s(Y),e&&s(xa),e&&s(dt),e&&s(qa),e&&s(we),e&&s(za),e&&s($t),e&&s(Pa),e&&s(He),e&&s(Fa),j(Le,e),e&&s(Ma),e&&s(Fe),j(_t),e&&s(Sa),e&&s(Ue),e&&s(Ia),e&&s(Bt),e&&s(Ca),j(Re,e),e&&s(Oa),e&&s(Ge),e&&s(Na),j(gt,e),e&&s(Da),e&&s(ke),e&&s(Ha),j(wt,e),e&&s(La),e&&s(Ye),e&&s(Wa),j(kt,e),e&&s(Ua),e&&s(ye),e&&s(Ra),j(bt,e),e&&s(Ga),e&&s(Je),e&&s(Ya),j(Et,e),e&&s(Ja),e&&s(be),e&&s(Qa),j(jt,e),e&&s(Ba),e&&s(Qe),e&&s(Va),j(xt,e),e&&s(Ka),e&&s(es),e&&s(Za),j(qt,e),e&&s(Xa),e&&s(Be),e&&s(er),e&&s(Me),j(zt),e&&s(tr),e&&s(fe),e&&s(sr),j(Mt,e),e&&s(ar),j(Ke,e),e&&s(rr),e&&s(Ee),e&&s(or),j(St,e),e&&s(nr),e&&s(Ae),e&&s(lr),e&&s(Se),j(It),e&&s(ir),j(Ct,e),e&&s(pr),e&&s(Z),e&&s(fr),e&&s(Te),e&&s(ur),e&&s(Ie),j(Ot),e&&s(mr),e&&s(je),e&&s(cr),e&&s(et),e&&s(hr),j(Nt,e),e&&s(dr),e&&s(tt),e&&s($r),e&&s(ds),e&&s(_r),j(Dt,e),e&&s(gr),e&&s($s),e&&s(vr),e&&s(st),e&&s(wr),e&&s(at),e&&s(kr),j(rt,e),e&&s(yr),e&&s(ot),e&&s(br),e&&s(Ce),j(Ht),e&&s(Er),j(lt,e),e&&s(Ar),j(it,e),e&&s(Tr),e&&s(X),e&&s(jr),j(pt,e),e&&s(xr),e&&s(Oe),j(Rt),e&&s(qr),j(ut,e),e&&s(zr),e&&s(xe),e&&s(Pr),j(mt,e)}}}const df={local:"quick-tour",sections:[{local:"pipeline",sections:[{local:"pipeline-usage",title:"Pipeline usage"},{local:"use-another-model-and-tokenizer-in-the-pipeline",title:"Use another model and tokenizer in the pipeline"}],title:"Pipeline"},{local:"autoclass",sections:[{local:"autotokenizer",title:"AutoTokenizer"},{local:"automodel",title:"AutoModel"},{local:"save-a-model",title:"Save a model"}],title:"AutoClass"}],title:"Quick tour"};function $f(P){return Ip(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Ef extends Pp{constructor(a){super();Fp(this,a,$f,hf,Mp,{})}}export{Ef as default,df as metadata};
