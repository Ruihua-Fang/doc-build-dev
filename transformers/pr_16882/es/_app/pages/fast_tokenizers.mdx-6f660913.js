import{S as Ta,i as Ea,s as Pa,e as t,k as p,w as C,t as i,M as wa,c as r,d as a,m as c,a as n,x as F,h as l,b as m,F as s,g as d,y as L,L as ba,q as O,o as B,B as x,v as ya}from"../chunks/vendor-c570b7f7.js";import{I as aa,C as Pe}from"../chunks/CodeBlock-8a2530c2.js";function Sa(sa){let k,te,h,_,H,P,we,I,be,re,z,R,ye,Se,w,qe,Ae,ne,J,Ne,ie,b,le,U,Ce,de,g,j,G,y,Fe,Q,Le,pe,f,Oe,X,Be,xe,Y,Je,Ue,ce,S,me,$,De,D,Ke,Me,fe,v,T,Z,q,Ve,ee,We,ue,K,He,ke,A,he,u,Ie,ae,Re,Ge,se,Qe,Xe,ze,N,ge,E,Ye,M,Ze,ea,ve;return P=new aa({}),b=new Pe({props:{code:`from tokenizers import Tokenizer
from tokenizers.models import BPE
from tokenizers.trainers import BpeTrainer
from tokenizers.pre_tokenizers import Whitespace

tokenizer = Tokenizer(BPE(unk_token="[UNK]"))
trainer = BpeTrainer(special_tokens=["[UNK]", "[CLS]", "[SEP]", "[PAD]", "[MASK]"])

tokenizer.pre_tokenizer = Whitespace()
files = [...]
tokenizer.train(files, trainer)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> tokenizers <span class="hljs-keyword">import</span> Tokenizer
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> tokenizers.models <span class="hljs-keyword">import</span> BPE
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> tokenizers.trainers <span class="hljs-keyword">import</span> BpeTrainer
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> tokenizers.pre_tokenizers <span class="hljs-keyword">import</span> Whitespace

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = Tokenizer(BPE(unk_token=<span class="hljs-string">&quot;[UNK]&quot;</span>))
<span class="hljs-meta">&gt;&gt;&gt; </span>trainer = BpeTrainer(special_tokens=[<span class="hljs-string">&quot;[UNK]&quot;</span>, <span class="hljs-string">&quot;[CLS]&quot;</span>, <span class="hljs-string">&quot;[SEP]&quot;</span>, <span class="hljs-string">&quot;[PAD]&quot;</span>, <span class="hljs-string">&quot;[MASK]&quot;</span>])

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.pre_tokenizer = Whitespace()
<span class="hljs-meta">&gt;&gt;&gt; </span>files = [...]
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.train(files, trainer)`}}),y=new aa({}),S=new Pe({props:{code:`from transformers import PreTrainedTokenizerFast

fast_tokenizer = PreTrainedTokenizerFast(tokenizer_object=tokenizer)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> PreTrainedTokenizerFast

<span class="hljs-meta">&gt;&gt;&gt; </span>fast_tokenizer = PreTrainedTokenizerFast(tokenizer_object=tokenizer)`}}),q=new aa({}),A=new Pe({props:{code:'tokenizer.save("tokenizer.json")',highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.save(<span class="hljs-string">&quot;tokenizer.json&quot;</span>)'}}),N=new Pe({props:{code:`from transformers import PreTrainedTokenizerFast

fast_tokenizer = PreTrainedTokenizerFast(tokenizer_file="tokenizer.json")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> PreTrainedTokenizerFast

<span class="hljs-meta">&gt;&gt;&gt; </span>fast_tokenizer = PreTrainedTokenizerFast(tokenizer_file=<span class="hljs-string">&quot;tokenizer.json&quot;</span>)`}}),{c(){k=t("meta"),te=p(),h=t("h1"),_=t("a"),H=t("span"),C(P.$$.fragment),we=p(),I=t("span"),be=i("Usa los tokenizadores de \u{1F917} Tokenizers"),re=p(),z=t("p"),R=t("code"),ye=i("PreTrainedTokenizerFast"),Se=i(" depende de la librer\xEDa "),w=t("a"),qe=i("\u{1F917} Tokenizers"),Ae=i(`. Los tokenizadores obtenidos desde la librer\xEDa \u{1F917} Tokenizers pueden ser
cargados muy simplemente a \u{1F917} Transformers.`),ne=p(),J=t("p"),Ne=i("Antes de entrar en los detalles, podemos crear un tokenizador dummy en unas cuantas l\xEDneas:"),ie=p(),C(b.$$.fragment),le=p(),U=t("p"),Ce=i(`Ahora tenemos un tokenizador entrenado con los archivos (files en el c\xF3digo) que fueron definidos. Lo podemos seguir utilizando en ese entorno de ejecuci\xF3n (runtime en ingl\xE9s), o se lo puede guardar
en un archivo JSON para reutilizarlo en un futuro.`),de=p(),g=t("h2"),j=t("a"),G=t("span"),C(y.$$.fragment),Fe=p(),Q=t("span"),Le=i("Cargando directamente desde el objeto tokenizador"),pe=p(),f=t("p"),Oe=i(`Veamos como utilizar este objeto tokenizador en la librer\xEDa \u{1F917} Transformers. La clase
`),X=t("code"),Be=i("PreTrainedTokenizerFast"),xe=i(` permite una instanciaci\xF3n simple, al aceptar el objeto
`),Y=t("em"),Je=i("tokenizer"),Ue=i(" instanciado como argumento:"),ce=p(),C(S.$$.fragment),me=p(),$=t("p"),De=i("Este objeto ya puede ser utilizado con todos los m\xE9todos compartidos por los tokenizadores de \u{1F917} Transformers! Visita la "),D=t("a"),Ke=i("p\xE1gina sobre tokenizadores"),Me=i(" para m\xE1s informaci\xF3n."),fe=p(),v=t("h2"),T=t("a"),Z=t("span"),C(q.$$.fragment),Ve=p(),ee=t("span"),We=i("Cargando desde un archivo JSON"),ue=p(),K=t("p"),He=i("Para cargar un tokenizador desde un archivo JSON, comencemos por guardar nuestro tokenizador:"),ke=p(),C(A.$$.fragment),he=p(),u=t("p"),Ie=i("La localizaci\xF3n (path en ingl\xE9s) donde este archivo es guardado puede ser incluida en el m\xE9todo de inicializaci\xF3n de "),ae=t("code"),Re=i("PreTrainedTokenizerFast"),Ge=i(`
utilizando el par\xE1metro `),se=t("code"),Qe=i("tokenizer_file"),Xe=i(":"),ze=p(),C(N.$$.fragment),ge=p(),E=t("p"),Ye=i("Este objeto ya puede ser utilizado con todos los m\xE9todos compartidos por los tokenizadores de \u{1F917} Transformers! Visita la "),M=t("a"),Ze=i("p\xE1gina sobre tokenizadores"),ea=i(" para m\xE1s informaci\xF3n."),this.h()},l(e){const o=wa('[data-svelte="svelte-1phssyn"]',document.head);k=r(o,"META",{name:!0,content:!0}),o.forEach(a),te=c(e),h=r(e,"H1",{class:!0});var _e=n(h);_=r(_e,"A",{id:!0,class:!0,href:!0});var oa=n(_);H=r(oa,"SPAN",{});var ta=n(H);F(P.$$.fragment,ta),ta.forEach(a),oa.forEach(a),we=c(_e),I=r(_e,"SPAN",{});var ra=n(I);be=l(ra,"Usa los tokenizadores de \u{1F917} Tokenizers"),ra.forEach(a),_e.forEach(a),re=c(e),z=r(e,"P",{});var oe=n(z);R=r(oe,"CODE",{});var na=n(R);ye=l(na,"PreTrainedTokenizerFast"),na.forEach(a),Se=l(oe," depende de la librer\xEDa "),w=r(oe,"A",{href:!0,rel:!0});var ia=n(w);qe=l(ia,"\u{1F917} Tokenizers"),ia.forEach(a),Ae=l(oe,`. Los tokenizadores obtenidos desde la librer\xEDa \u{1F917} Tokenizers pueden ser
cargados muy simplemente a \u{1F917} Transformers.`),oe.forEach(a),ne=c(e),J=r(e,"P",{});var la=n(J);Ne=l(la,"Antes de entrar en los detalles, podemos crear un tokenizador dummy en unas cuantas l\xEDneas:"),la.forEach(a),ie=c(e),F(b.$$.fragment,e),le=c(e),U=r(e,"P",{});var da=n(U);Ce=l(da,`Ahora tenemos un tokenizador entrenado con los archivos (files en el c\xF3digo) que fueron definidos. Lo podemos seguir utilizando en ese entorno de ejecuci\xF3n (runtime en ingl\xE9s), o se lo puede guardar
en un archivo JSON para reutilizarlo en un futuro.`),da.forEach(a),de=c(e),g=r(e,"H2",{class:!0});var je=n(g);j=r(je,"A",{id:!0,class:!0,href:!0});var pa=n(j);G=r(pa,"SPAN",{});var ca=n(G);F(y.$$.fragment,ca),ca.forEach(a),pa.forEach(a),Fe=c(je),Q=r(je,"SPAN",{});var ma=n(Q);Le=l(ma,"Cargando directamente desde el objeto tokenizador"),ma.forEach(a),je.forEach(a),pe=c(e),f=r(e,"P",{});var V=n(f);Oe=l(V,`Veamos como utilizar este objeto tokenizador en la librer\xEDa \u{1F917} Transformers. La clase
`),X=r(V,"CODE",{});var fa=n(X);Be=l(fa,"PreTrainedTokenizerFast"),fa.forEach(a),xe=l(V,` permite una instanciaci\xF3n simple, al aceptar el objeto
`),Y=r(V,"EM",{});var ua=n(Y);Je=l(ua,"tokenizer"),ua.forEach(a),Ue=l(V," instanciado como argumento:"),V.forEach(a),ce=c(e),F(S.$$.fragment,e),me=c(e),$=r(e,"P",{});var $e=n($);De=l($e,"Este objeto ya puede ser utilizado con todos los m\xE9todos compartidos por los tokenizadores de \u{1F917} Transformers! Visita la "),D=r($e,"A",{href:!0});var ka=n(D);Ke=l(ka,"p\xE1gina sobre tokenizadores"),ka.forEach(a),Me=l($e," para m\xE1s informaci\xF3n."),$e.forEach(a),fe=c(e),v=r(e,"H2",{class:!0});var Te=n(v);T=r(Te,"A",{id:!0,class:!0,href:!0});var ha=n(T);Z=r(ha,"SPAN",{});var za=n(Z);F(q.$$.fragment,za),za.forEach(a),ha.forEach(a),Ve=c(Te),ee=r(Te,"SPAN",{});var ga=n(ee);We=l(ga,"Cargando desde un archivo JSON"),ga.forEach(a),Te.forEach(a),ue=c(e),K=r(e,"P",{});var va=n(K);He=l(va,"Para cargar un tokenizador desde un archivo JSON, comencemos por guardar nuestro tokenizador:"),va.forEach(a),ke=c(e),F(A.$$.fragment,e),he=c(e),u=r(e,"P",{});var W=n(u);Ie=l(W,"La localizaci\xF3n (path en ingl\xE9s) donde este archivo es guardado puede ser incluida en el m\xE9todo de inicializaci\xF3n de "),ae=r(W,"CODE",{});var _a=n(ae);Re=l(_a,"PreTrainedTokenizerFast"),_a.forEach(a),Ge=l(W,`
utilizando el par\xE1metro `),se=r(W,"CODE",{});var ja=n(se);Qe=l(ja,"tokenizer_file"),ja.forEach(a),Xe=l(W,":"),W.forEach(a),ze=c(e),F(N.$$.fragment,e),ge=c(e),E=r(e,"P",{});var Ee=n(E);Ye=l(Ee,"Este objeto ya puede ser utilizado con todos los m\xE9todos compartidos por los tokenizadores de \u{1F917} Transformers! Visita la "),M=r(Ee,"A",{href:!0});var $a=n(M);Ze=l($a,"p\xE1gina sobre tokenizadores"),$a.forEach(a),ea=l(Ee," para m\xE1s informaci\xF3n."),Ee.forEach(a),this.h()},h(){m(k,"name","hf:doc:metadata"),m(k,"content",JSON.stringify(qa)),m(_,"id","usa-los-tokenizadores-de-tokenizers"),m(_,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(_,"href","#usa-los-tokenizadores-de-tokenizers"),m(h,"class","relative group"),m(w,"href","https://huggingface.co/docs/tokenizers"),m(w,"rel","nofollow"),m(j,"id","cargando-directamente-desde-el-objeto-tokenizador"),m(j,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(j,"href","#cargando-directamente-desde-el-objeto-tokenizador"),m(g,"class","relative group"),m(D,"href","main_classes/tokenizer"),m(T,"id","cargando-desde-un-archivo-json"),m(T,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(T,"href","#cargando-desde-un-archivo-json"),m(v,"class","relative group"),m(M,"href","main_classes/tokenizer")},m(e,o){s(document.head,k),d(e,te,o),d(e,h,o),s(h,_),s(_,H),L(P,H,null),s(h,we),s(h,I),s(I,be),d(e,re,o),d(e,z,o),s(z,R),s(R,ye),s(z,Se),s(z,w),s(w,qe),s(z,Ae),d(e,ne,o),d(e,J,o),s(J,Ne),d(e,ie,o),L(b,e,o),d(e,le,o),d(e,U,o),s(U,Ce),d(e,de,o),d(e,g,o),s(g,j),s(j,G),L(y,G,null),s(g,Fe),s(g,Q),s(Q,Le),d(e,pe,o),d(e,f,o),s(f,Oe),s(f,X),s(X,Be),s(f,xe),s(f,Y),s(Y,Je),s(f,Ue),d(e,ce,o),L(S,e,o),d(e,me,o),d(e,$,o),s($,De),s($,D),s(D,Ke),s($,Me),d(e,fe,o),d(e,v,o),s(v,T),s(T,Z),L(q,Z,null),s(v,Ve),s(v,ee),s(ee,We),d(e,ue,o),d(e,K,o),s(K,He),d(e,ke,o),L(A,e,o),d(e,he,o),d(e,u,o),s(u,Ie),s(u,ae),s(ae,Re),s(u,Ge),s(u,se),s(se,Qe),s(u,Xe),d(e,ze,o),L(N,e,o),d(e,ge,o),d(e,E,o),s(E,Ye),s(E,M),s(M,Ze),s(E,ea),ve=!0},p:ba,i(e){ve||(O(P.$$.fragment,e),O(b.$$.fragment,e),O(y.$$.fragment,e),O(S.$$.fragment,e),O(q.$$.fragment,e),O(A.$$.fragment,e),O(N.$$.fragment,e),ve=!0)},o(e){B(P.$$.fragment,e),B(b.$$.fragment,e),B(y.$$.fragment,e),B(S.$$.fragment,e),B(q.$$.fragment,e),B(A.$$.fragment,e),B(N.$$.fragment,e),ve=!1},d(e){a(k),e&&a(te),e&&a(h),x(P),e&&a(re),e&&a(z),e&&a(ne),e&&a(J),e&&a(ie),x(b,e),e&&a(le),e&&a(U),e&&a(de),e&&a(g),x(y),e&&a(pe),e&&a(f),e&&a(ce),x(S,e),e&&a(me),e&&a($),e&&a(fe),e&&a(v),x(q),e&&a(ue),e&&a(K),e&&a(ke),x(A,e),e&&a(he),e&&a(u),e&&a(ze),x(N,e),e&&a(ge),e&&a(E)}}}const qa={local:"usa-los-tokenizadores-de-tokenizers",sections:[{local:"cargando-directamente-desde-el-objeto-tokenizador",title:"Cargando directamente desde el objeto tokenizador "},{local:"cargando-desde-un-archivo-json",title:"Cargando desde un archivo JSON"}],title:"Usa los tokenizadores de \u{1F917} Tokenizers"};function Aa(sa){return ya(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Fa extends Ta{constructor(k){super();Ea(this,k,Aa,Sa,Pa,{})}}export{Fa as default,qa as metadata};
