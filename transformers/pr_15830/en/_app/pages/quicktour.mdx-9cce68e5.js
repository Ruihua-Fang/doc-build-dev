import{S as eh,i as th,s as sh,e as n,k as f,w as d,t as a,M as ah,c as l,d as s,m as h,a as i,x as _,h as o,b as m,F as t,g as p,y as g,q as v,o as $,B as y}from"../chunks/vendor-4833417e.js";import{T as ja}from"../chunks/Tip-fffd6df1.js";import{Y as Xf}from"../chunks/Youtube-27813aed.js";import{I as Oe}from"../chunks/IconCopyLink-4b81c553.js";import{C as D}from"../chunks/CodeBlock-90ffda97.js";import{C as fe}from"../chunks/CodeBlockFw-03f30a28.js";import{D as oh}from"../chunks/DocNotebookDropdown-ecff2a90.js";import"../chunks/CopyButton-04a16537.js";function rh(N){let u,k;return{c(){u=n("p"),k=a(`All code examples presented in the documentation have a toggle on the top left for PyTorch and TensorFlow. If
not, the code is expected to work for both backends without any change.`)},l(c){u=l(c,"P",{});var w=i(u);k=o(w,`All code examples presented in the documentation have a toggle on the top left for PyTorch and TensorFlow. If
not, the code is expected to work for both backends without any change.`),w.forEach(s)},m(c,w){p(c,u,w),t(u,k)},d(c){c&&s(u)}}}function nh(N){let u,k,c,w,A,b,j,x;return{c(){u=n("p"),k=a("For more details about the "),c=n("a"),w=a("pipeline()"),A=a(" and associated tasks, refer to the documentation "),b=n("a"),j=a("here"),x=a("."),this.h()},l(P){u=l(P,"P",{});var E=i(u);k=o(E,"For more details about the "),c=l(E,"A",{href:!0});var O=i(c);w=o(O,"pipeline()"),O.forEach(s),A=o(E," and associated tasks, refer to the documentation "),b=l(E,"A",{href:!0});var U=i(b);j=o(U,"here"),U.forEach(s),x=o(E,"."),E.forEach(s),this.h()},h(){m(c,"href","/docs/transformers/pr_15830/en/main_classes/pipelines#transformers.pipeline"),m(b,"href","./main_classes/pipelines")},m(P,E){p(P,u,E),t(u,k),t(u,c),t(c,w),t(u,A),t(u,b),t(b,j),t(u,x)},d(P){P&&s(u)}}}function lh(N){let u,k,c,w,A,b,j,x;return{c(){u=n("p"),k=a("See the "),c=n("a"),w=a("task summary"),A=a(" for which "),b=n("a"),j=a("AutoModel"),x=a(" class to use for which task."),this.h()},l(P){u=l(P,"P",{});var E=i(u);k=o(E,"See the "),c=l(E,"A",{href:!0});var O=i(c);w=o(O,"task summary"),O.forEach(s),A=o(E," for which "),b=l(E,"A",{href:!0});var U=i(b);j=o(U,"AutoModel"),U.forEach(s),x=o(E," class to use for which task."),E.forEach(s),this.h()},h(){m(c,"href","./task_summary"),m(b,"href","/docs/transformers/pr_15830/en/model_doc/auto#transformers.AutoModel")},m(P,E){p(P,u,E),t(u,k),t(u,c),t(c,w),t(u,A),t(u,b),t(b,j),t(u,x)},d(P){P&&s(u)}}}function ih(N){let u,k,c,w,A;return{c(){u=n("p"),k=a("All \u{1F917} Transformers models (PyTorch or TensorFlow) outputs the tensors "),c=n("em"),w=a("before"),A=a(` the final activation
function (like softmax) because the final activation function is often fused with the loss.`)},l(b){u=l(b,"P",{});var j=i(u);k=o(j,"All \u{1F917} Transformers models (PyTorch or TensorFlow) outputs the tensors "),c=l(j,"EM",{});var x=i(c);w=o(x,"before"),x.forEach(s),A=o(j,` the final activation
function (like softmax) because the final activation function is often fused with the loss.`),j.forEach(s)},m(b,j){p(b,u,j),t(u,k),t(u,c),t(c,w),t(u,A)},d(b){b&&s(u)}}}function ph(N){let u,k,c,w,A;return{c(){u=n("p"),k=a(`\u{1F917} Transformers model outputs are special dataclasses so their attributes are autocompleted in an IDE.
The model outputs also behave like a tuple or a dictionary (e.g., you can index with an integer, a slice or a string) in which case the attributes that are `),c=n("code"),w=a("None"),A=a(" are ignored.")},l(b){u=l(b,"P",{});var j=i(u);k=o(j,`\u{1F917} Transformers model outputs are special dataclasses so their attributes are autocompleted in an IDE.
The model outputs also behave like a tuple or a dictionary (e.g., you can index with an integer, a slice or a string) in which case the attributes that are `),c=l(j,"CODE",{});var x=i(c);w=o(x,"None"),x.forEach(s),A=o(j," are ignored."),j.forEach(s)},m(b,j){p(b,u,j),t(u,k),t(u,c),t(c,w),t(u,A)},d(b){b&&s(u)}}}function fh(N){let u,k,c,w,A,b,j,x,P,E,O,U,H,ur,xt,cr,dr,qt,_r,gr,Ea,he,Aa,se,me,xs,Le,vr,qs,$r,Ta,Re,zt,yr,br,xa,De,qa,ue,wr,St,kr,jr,za,Ue,zs,Er,Ar,Sa,T,Ss,Tr,xr,Fs,qr,zr,Ps,Sr,Fr,Ms,Pr,Mr,Cs,Cr,Ir,Is,Nr,Or,Ns,Lr,Rr,Os,Dr,Fa,He,Ls,Ur,Hr,Pa,W,Rs,Wr,Gr,Ds,Br,Yr,Us,Qr,Ma,We,Hs,Jr,Kr,Ca,ce,Ws,Vr,Zr,Gs,Xr,Ia,de,Na,ae,_e,Bs,Ge,en,Ys,tn,Oa,ge,sn,Ft,an,on,La,Pt,rn,Ra,Be,Da,ve,nn,Mt,ln,pn,Ua,Ye,Ha,G,fn,Qe,hn,mn,Qs,un,cn,Wa,Je,Ga,$e,dn,Ct,_n,gn,Ba,Ke,Ya,B,vn,It,$n,yn,Ve,bn,wn,Qa,Ze,Ja,L,kn,Nt,jn,En,Js,An,Tn,Ks,xn,qn,Ka,Xe,Va,Y,zn,et,Sn,Fn,tt,Pn,Mn,Za,st,Xa,Ot,Cn,eo,at,to,ye,In,Lt,Nn,On,so,oe,be,Vs,ot,Ln,Zs,Rn,ao,M,Dn,Rt,Un,Hn,rt,Wn,Gn,Dt,Bn,Yn,nt,Qn,Jn,oo,lt,ro,Q,Kn,Ut,Vn,Zn,Xs,Xn,el,no,it,lo,J,tl,Ht,sl,al,ea,ol,rl,io,pt,po,K,nl,Wt,ll,il,Gt,pl,fl,fo,re,we,ta,ft,hl,sa,ml,ho,ht,mo,q,ul,Bt,cl,dl,Yt,_l,gl,Qt,vl,$l,Jt,yl,bl,aa,wl,kl,Kt,jl,El,uo,V,Al,oa,Tl,xl,Vt,ql,zl,co,ne,ke,ra,mt,Sl,na,Fl,_o,Z,Pl,la,Ml,Cl,Zt,Il,Nl,go,je,Ol,Xt,Ll,Rl,vo,ut,$o,Ee,Dl,ia,Ul,Hl,yo,es,Wl,bo,ct,wo,ts,Gl,ko,Ae,ss,as,Bl,Yl,Ql,os,rs,Jl,Kl,jo,Te,Vl,ns,Zl,Xl,Eo,dt,Ao,xe,ei,ls,ti,si,To,le,qe,pa,_t,ai,fa,oi,xo,S,ri,is,ni,li,ps,ii,pi,fs,fi,hi,hs,mi,ui,ms,ci,di,qo,gt,zo,ze,So,Se,_i,ha,gi,vi,Fo,vt,Po,X,$i,ma,yi,bi,ua,wi,ki,Mo,$t,Co,Fe,Io,z,ji,yt,ca,Ei,Ai,bt,da,Ti,xi,us,qi,zi,_a,Si,Fi,wt,Pi,Mi,cs,Ci,Ii,No,Pe,Oo,ie,Me,ga,kt,Ni,va,Oi,Lo,Ce,Li,ds,Ri,Di,Ro,jt,Do,Ie,Ui,_s,Hi,Wi,Uo,Et,Ho,ee,Gi,$a,Bi,Yi,ya,Qi,Ji,Wo,At,Go;return b=new Oe({}),O=new oh({props:{classNames:"absolute z-10 right-0 top-0",options:[{label:"Mixed",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/master/transformers_doc/quicktour.ipynb"},{label:"PyTorch",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/master/transformers_doc/pytorch/quicktour.ipynb"},{label:"TensorFlow",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/master/transformers_doc/tensorflow/quicktour.ipynb"},{label:"Mixed",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/transformers_doc/quicktour.ipynb"},{label:"PyTorch",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/transformers_doc/pytorch/quicktour.ipynb"},{label:"TensorFlow",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/transformers_doc/tensorflow/quicktour.ipynb"}]}}),he=new ja({props:{$$slots:{default:[rh]},$$scope:{ctx:N}}}),Le=new Oe({}),De=new Xf({props:{id:"tiZFewofSLM"}}),de=new ja({props:{$$slots:{default:[nh]},$$scope:{ctx:N}}}),Ge=new Oe({}),Be=new fe({props:{pt:{code:"pip install torch",highlighted:"pip install torch"},tf:{code:"pip install tensorflow",highlighted:"pip install tensorflow"}}}),Ye=new D({props:{code:`from transformers import pipeline

classifier = pipeline("sentiment-analysis"),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> pipeline

<span class="hljs-meta">&gt;&gt;&gt; </span>classifier = pipeline(<span class="hljs-string">&quot;sentiment-analysis&quot;</span>)`}}),Je=new D({props:{code:'classifier("We are very happy to show you the \u{1F917} Transformers library."),',highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>classifier(<span class="hljs-string">&quot;We are very happy to show you the \u{1F917} Transformers library.&quot;</span>)
[{<span class="hljs-string">&#x27;label&#x27;</span>: <span class="hljs-string">&#x27;POSITIVE&#x27;</span>, <span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.9998</span>}]`}}),Ke=new D({props:{code:`results = classifier(["We are very happy to show you the \u{1F917} Transformers library.", "We hope you don't hate it."])
for result in results:
    print(f"label: {result['label']}, with score: {round(result['score'], 4)}"),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>results = classifier([<span class="hljs-string">&quot;We are very happy to show you the \u{1F917} Transformers library.&quot;</span>, <span class="hljs-string">&quot;We hope you don&#x27;t hate it.&quot;</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">for</span> result <span class="hljs-keyword">in</span> results:
<span class="hljs-meta">... </span>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;label: <span class="hljs-subst">{result[<span class="hljs-string">&#x27;label&#x27;</span>]}</span>, with score: <span class="hljs-subst">{<span class="hljs-built_in">round</span>(result[<span class="hljs-string">&#x27;score&#x27;</span>], <span class="hljs-number">4</span>)}</span>&quot;</span>)
label: POSITIVE, <span class="hljs-keyword">with</span> score: <span class="hljs-number">0.9998</span>
label: NEGATIVE, <span class="hljs-keyword">with</span> score: <span class="hljs-number">0.5309</span>`}}),Ze=new D({props:{code:"pip install datasets ,",highlighted:"pip install datasets "}}),Xe=new D({props:{code:`from transformers import pipeline

speech_recognizer = pipeline("automatic-speech-recognition", model="facebook/wav2vec2-base-960h", device=0),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> pipeline

<span class="hljs-meta">&gt;&gt;&gt; </span>speech_recognizer = pipeline(<span class="hljs-string">&quot;automatic-speech-recognition&quot;</span>, model=<span class="hljs-string">&quot;facebook/wav2vec2-base-960h&quot;</span>, device=<span class="hljs-number">0</span>)`}}),st=new D({props:{code:`import datasets

dataset = datasets.load_dataset("superb", name="asr", split="test")  # doctest: +IGNORE_RESULT,`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> datasets

<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = datasets.load_dataset(<span class="hljs-string">&quot;superb&quot;</span>, name=<span class="hljs-string">&quot;asr&quot;</span>, split=<span class="hljs-string">&quot;test&quot;</span>)  <span class="hljs-comment"># doctest: +IGNORE_RESULT</span>`}}),at=new D({props:{code:`files = dataset["file"]
speech_recognizer(files[:4]),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>files = dataset[<span class="hljs-string">&quot;file&quot;</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>speech_recognizer(files[:<span class="hljs-number">4</span>])
[{<span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;HE HOPED THERE WOULD BE STEW FOR DINNER TURNIPS AND CARROTS AND BRUISED POTATOES AND FAT MUTTON PIECES TO BE LADLED OUT IN THICK PEPPERED FLOWER FAT AND SAUCE&#x27;</span>},
 {<span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;STUFFERED INTO YOU HIS BELLY COUNSELLED HIM&#x27;</span>},
 {<span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;AFTER EARLY NIGHTFALL THE YELLOW LAMPS WOULD LIGHT UP HERE AND THERE THE SQUALID QUARTER OF THE BROTHELS&#x27;</span>},
 {<span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;HO BERTIE ANY GOOD IN YOUR MIND&#x27;</span>}]`}}),ot=new Oe({}),lt=new D({props:{code:'model_name = "nlptown/bert-base-multilingual-uncased-sentiment",',highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>model_name = <span class="hljs-string">&quot;nlptown/bert-base-multilingual-uncased-sentiment&quot;</span>'}}),it=new fe({props:{pt:{code:`from transformers import AutoTokenizer, AutoModelForSequenceClassification

model = AutoModelForSequenceClassification.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)
#`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, AutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSequenceClassification.from_pretrained(model_name)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(model_name)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment">#</span>`},tf:{code:`from transformers import AutoTokenizer, TFAutoModelForSequenceClassification

model = TFAutoModelForSequenceClassification.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, TFAutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSequenceClassification.from_pretrained(model_name)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(model_name)`}}}),pt=new D({props:{code:`classifier = pipeline("sentiment-analysis", model=model, tokenizer=tokenizer)
classifier("Nous sommes tr\xE8s heureux de vous pr\xE9senter la biblioth\xE8que \u{1F917} Transformers."),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>classifier = pipeline(<span class="hljs-string">&quot;sentiment-analysis&quot;</span>, model=model, tokenizer=tokenizer)
<span class="hljs-meta">&gt;&gt;&gt; </span>classifier(<span class="hljs-string">&quot;Nous sommes tr\xE8s heureux de vous pr\xE9senter la biblioth\xE8que \u{1F917} Transformers.&quot;</span>)
[{<span class="hljs-string">&#x27;label&#x27;</span>: <span class="hljs-string">&#x27;5 stars&#x27;</span>, <span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.7273</span>}]`}}),ft=new Oe({}),ht=new Xf({props:{id:"AhChOFRegn4"}}),mt=new Oe({}),ut=new D({props:{code:`from transformers import AutoTokenizer

model_name = "nlptown/bert-base-multilingual-uncased-sentiment"
tokenizer = AutoTokenizer.from_pretrained(model_name),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>model_name = <span class="hljs-string">&quot;nlptown/bert-base-multilingual-uncased-sentiment&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(model_name)`}}),ct=new D({props:{code:`encoding = tokenizer("We are very happy to show you the \u{1F917} Transformers library.")
print(encoding),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>encoding = tokenizer(<span class="hljs-string">&quot;We are very happy to show you the \u{1F917} Transformers library.&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(encoding)
{<span class="hljs-string">&#x27;input_ids&#x27;</span>: [<span class="hljs-number">101</span>, <span class="hljs-number">11312</span>, <span class="hljs-number">10320</span>, <span class="hljs-number">12495</span>, <span class="hljs-number">19308</span>, <span class="hljs-number">10114</span>, <span class="hljs-number">11391</span>, <span class="hljs-number">10855</span>, <span class="hljs-number">10103</span>, <span class="hljs-number">100</span>, <span class="hljs-number">58263</span>, <span class="hljs-number">13299</span>, <span class="hljs-number">119</span>, <span class="hljs-number">102</span>],
 <span class="hljs-string">&#x27;token_type_ids&#x27;</span>: [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
 <span class="hljs-string">&#x27;attention_mask&#x27;</span>: [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]}`}}),dt=new fe({props:{pt:{code:`pt_batch = tokenizer(
    ["We are very happy to show you the \u{1F917} Transformers library.", "We hope you don't hate it."],
    padding=True,
    truncation=True,
    max_length=512,
    return_tensors="pt",
)
#`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>pt_batch = tokenizer(
<span class="hljs-meta">... </span>    [<span class="hljs-string">&quot;We are very happy to show you the \u{1F917} Transformers library.&quot;</span>, <span class="hljs-string">&quot;We hope you don&#x27;t hate it.&quot;</span>],
<span class="hljs-meta">... </span>    padding=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>    truncation=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>    max_length=<span class="hljs-number">512</span>,
<span class="hljs-meta">... </span>    return_tensors=<span class="hljs-string">&quot;pt&quot;</span>,
<span class="hljs-meta">... </span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment">#</span>`},tf:{code:`tf_batch = tokenizer(
    ["We are very happy to show you the \u{1F917} Transformers library.", "We hope you don't hate it."],
    padding=True,
    truncation=True,
    max_length=512,
    return_tensors="tf",
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>tf_batch = tokenizer(
<span class="hljs-meta">... </span>    [<span class="hljs-string">&quot;We are very happy to show you the \u{1F917} Transformers library.&quot;</span>, <span class="hljs-string">&quot;We hope you don&#x27;t hate it.&quot;</span>],
<span class="hljs-meta">... </span>    padding=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>    truncation=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>    max_length=<span class="hljs-number">512</span>,
<span class="hljs-meta">... </span>    return_tensors=<span class="hljs-string">&quot;tf&quot;</span>,
<span class="hljs-meta">... </span>)`}}}),_t=new Oe({}),gt=new fe({props:{pt:{code:`from transformers import AutoModelForSequenceClassification

model_name = "nlptown/bert-base-multilingual-uncased-sentiment"
pt_model = AutoModelForSequenceClassification.from_pretrained(model_name)
#`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>model_name = <span class="hljs-string">&quot;nlptown/bert-base-multilingual-uncased-sentiment&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>pt_model = AutoModelForSequenceClassification.from_pretrained(model_name)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment">#</span>`},tf:{code:`from transformers import TFAutoModelForSequenceClassification

model_name = "nlptown/bert-base-multilingual-uncased-sentiment"
tf_model = TFAutoModelForSequenceClassification.from_pretrained(model_name)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFAutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>model_name = <span class="hljs-string">&quot;nlptown/bert-base-multilingual-uncased-sentiment&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tf_model = TFAutoModelForSequenceClassification.from_pretrained(model_name)`}}}),ze=new ja({props:{$$slots:{default:[lh]},$$scope:{ctx:N}}}),vt=new fe({props:{pt:{code:`pt_outputs = pt_model(**pt_batch)
#`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>pt_outputs = pt_model(**pt_batch)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment">#</span>`},tf:{code:"tf_outputs = tf_model(tf_batch)",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>tf_outputs = tf_model(tf_batch)'}}}),$t=new fe({props:{pt:{code:`from torch import nn

pt_predictions = nn.functional.softmax(pt_outputs.logits, dim=-1)
print(pt_predictions)

#`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn

<span class="hljs-meta">&gt;&gt;&gt; </span>pt_predictions = nn.functional.softmax(pt_outputs.logits, dim=-<span class="hljs-number">1</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(pt_predictions)
tensor([[<span class="hljs-number">0.0021</span>, <span class="hljs-number">0.0018</span>, <span class="hljs-number">0.0115</span>, <span class="hljs-number">0.2121</span>, <span class="hljs-number">0.7725</span>],
        [<span class="hljs-number">0.2084</span>, <span class="hljs-number">0.1826</span>, <span class="hljs-number">0.1969</span>, <span class="hljs-number">0.1755</span>, <span class="hljs-number">0.2365</span>]], grad_fn=&lt;SoftmaxBackward0&gt;)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment">#</span>`},tf:{code:`import tensorflow as tf

tf_predictions = tf.nn.softmax(tf_outputs.logits, axis=-1)
print(tf_predictions)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

<span class="hljs-meta">&gt;&gt;&gt; </span>tf_predictions = tf.nn.softmax(tf_outputs.logits, axis=-<span class="hljs-number">1</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(tf_predictions)
tf.Tensor(
[[<span class="hljs-number">0.00206</span> <span class="hljs-number">0.00177</span> <span class="hljs-number">0.01155</span> <span class="hljs-number">0.21209</span> <span class="hljs-number">0.77253</span>]
 [<span class="hljs-number">0.20842</span> <span class="hljs-number">0.18262</span> <span class="hljs-number">0.19693</span> <span class="hljs-number">0.1755</span>  <span class="hljs-number">0.23652</span>]], shape=(<span class="hljs-number">2</span>, <span class="hljs-number">5</span>), dtype=float32)`}}}),Fe=new ja({props:{$$slots:{default:[ih]},$$scope:{ctx:N}}}),Pe=new ja({props:{$$slots:{default:[ph]},$$scope:{ctx:N}}}),kt=new Oe({}),jt=new fe({props:{pt:{code:`pt_save_directory = "./pt_save_pretrained"
tokenizer.save_pretrained(pt_save_directory)  # doctest: +IGNORE_RESULT
pt_model.save_pretrained(pt_save_directory)
#`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>pt_save_directory = <span class="hljs-string">&quot;./pt_save_pretrained&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.save_pretrained(pt_save_directory)  <span class="hljs-comment"># doctest: +IGNORE_RESULT</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>pt_model.save_pretrained(pt_save_directory)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment">#</span>`},tf:{code:`tf_save_directory = "./tf_save_pretrained"
tokenizer.save_pretrained(tf_save_directory)  # doctest: +IGNORE_RESULT
tf_model.save_pretrained(tf_save_directory)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>tf_save_directory = <span class="hljs-string">&quot;./tf_save_pretrained&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.save_pretrained(tf_save_directory)  <span class="hljs-comment"># doctest: +IGNORE_RESULT</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tf_model.save_pretrained(tf_save_directory)`}}}),Et=new fe({props:{pt:{code:`pt_model = AutoModelForSequenceClassification.from_pretrained("./pt_save_pretrained")
#`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>pt_model = AutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;./pt_save_pretrained&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment">#</span>`},tf:{code:'tf_model = TFAutoModelForSequenceClassification.from_pretrained("./tf_save_pretrained")',highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>tf_model = TFAutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;./tf_save_pretrained&quot;</span>)'}}}),At=new fe({props:{pt:{code:`from transformers import AutoModel

tokenizer = AutoTokenizer.from_pretrained(tf_save_directory)
pt_model = AutoModelForSequenceClassification.from_pretrained(tf_save_directory, from_tf=True)
#`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModel

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(tf_save_directory)
<span class="hljs-meta">&gt;&gt;&gt; </span>pt_model = AutoModelForSequenceClassification.from_pretrained(tf_save_directory, from_tf=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment">#</span>`},tf:{code:`from transformers import TFAutoModel

tokenizer = AutoTokenizer.from_pretrained(pt_save_directory)
tf_model = TFAutoModelForSequenceClassification.from_pretrained(pt_save_directory, from_pt=True)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFAutoModel

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(pt_save_directory)
<span class="hljs-meta">&gt;&gt;&gt; </span>tf_model = TFAutoModelForSequenceClassification.from_pretrained(pt_save_directory, from_pt=<span class="hljs-literal">True</span>)`}}}),{c(){u=n("meta"),k=f(),c=n("h1"),w=n("a"),A=n("span"),d(b.$$.fragment),j=f(),x=n("span"),P=a("Quick tour"),E=f(),d(O.$$.fragment),U=f(),H=n("p"),ur=a("Get up and running with \u{1F917} Transformers! Start using the "),xt=n("a"),cr=a("pipeline()"),dr=a(" for rapid inference, and quickly load a pretrained model and tokenizer with an "),qt=n("a"),_r=a("AutoClass"),gr=a(" to solve your text, vision or audio task."),Ea=f(),d(he.$$.fragment),Aa=f(),se=n("h2"),me=n("a"),xs=n("span"),d(Le.$$.fragment),vr=f(),qs=n("span"),$r=a("Pipeline"),Ta=f(),Re=n("p"),zt=n("a"),yr=a("pipeline()"),br=a(" is the easiest way to use a pretrained model for a given task."),xa=f(),d(De.$$.fragment),qa=f(),ue=n("p"),wr=a("The "),St=n("a"),kr=a("pipeline()"),jr=a(" supports many common tasks out-of-the-box:"),za=f(),Ue=n("p"),zs=n("strong"),Er=a("Text"),Ar=a(":"),Sa=f(),T=n("ul"),Ss=n("li"),Tr=a("Sentiment analysis: classify the polarity of a given text."),xr=f(),Fs=n("li"),qr=a("Text generation (in English): generate text from a given input."),zr=f(),Ps=n("li"),Sr=a("Name entity recognition (NER): label each word with the entity it represents (person, date, location, etc.)."),Fr=f(),Ms=n("li"),Pr=a("Question answering: extract the answer from the context, given some context and a question."),Mr=f(),Cs=n("li"),Cr=a("Fill-mask: fill in the blank given a text with masked words."),Ir=f(),Is=n("li"),Nr=a("Summarization: generate a summary of a long sequence of text or document."),Or=f(),Ns=n("li"),Lr=a("Translation: translate text into another language."),Rr=f(),Os=n("li"),Dr=a("Feature extraction: create a tensor representation of the text."),Fa=f(),He=n("p"),Ls=n("strong"),Ur=a("Image"),Hr=a(":"),Pa=f(),W=n("ul"),Rs=n("li"),Wr=a("Image classification: classify an image."),Gr=f(),Ds=n("li"),Br=a("Image segmentation: classify every pixel in an image."),Yr=f(),Us=n("li"),Qr=a("Object detection: detect objects within an image."),Ma=f(),We=n("p"),Hs=n("strong"),Jr=a("Audio"),Kr=a(":"),Ca=f(),ce=n("ul"),Ws=n("li"),Vr=a("Audio classification: assign a label to a given segment of audio."),Zr=f(),Gs=n("li"),Xr=a("Automatic speech recognition (ASR): transcribe audio data into text."),Ia=f(),d(de.$$.fragment),Na=f(),ae=n("h3"),_e=n("a"),Bs=n("span"),d(Ge.$$.fragment),en=f(),Ys=n("span"),tn=a("Pipeline usage"),Oa=f(),ge=n("p"),sn=a("In the following example, you will use the "),Ft=n("a"),an=a("pipeline()"),on=a(" for sentiment analysis."),La=f(),Pt=n("p"),rn=a("Install the following dependencies if you haven\u2019t already:"),Ra=f(),d(Be.$$.fragment),Da=f(),ve=n("p"),nn=a("Import "),Mt=n("a"),ln=a("pipeline()"),pn=a(" and specify the task you want to complete:"),Ua=f(),d(Ye.$$.fragment),Ha=f(),G=n("p"),fn=a("The pipeline downloads and caches a default "),Qe=n("a"),hn=a("pretrained model"),mn=a(" and tokenizer for sentiment analysis. Now you can use the "),Qs=n("code"),un=a("classifier"),cn=a(" on your target text:"),Wa=f(),d(Je.$$.fragment),Ga=f(),$e=n("p"),dn=a("For more than one sentence, pass a list of sentences to the "),Ct=n("a"),_n=a("pipeline()"),gn=a(" which returns a list of dictionaries:"),Ba=f(),d(Ke.$$.fragment),Ya=f(),B=n("p"),vn=a("The "),It=n("a"),$n=a("pipeline()"),yn=a(" can also iterate over an entire dataset. Start by installing the "),Ve=n("a"),bn=a("\u{1F917} Datasets"),wn=a(" library:"),Qa=f(),d(Ze.$$.fragment),Ja=f(),L=n("p"),kn=a("Create a "),Nt=n("a"),jn=a("pipeline()"),En=a(" with the task you want to solve for and the model you want to use. Set the "),Js=n("code"),An=a("device"),Tn=a(" parameter to "),Ks=n("code"),xn=a("0"),qn=a(" to place the tensors on a CUDA device:"),Ka=f(),d(Xe.$$.fragment),Va=f(),Y=n("p"),zn=a("Next, load a dataset (see the \u{1F917} Datasets "),et=n("a"),Sn=a("Quick Start"),Fn=a(" for more details) you\u2019d like to iterate over. For example, let\u2019s load the "),tt=n("a"),Pn=a("SUPERB"),Mn=a(" dataset:"),Za=f(),d(st.$$.fragment),Xa=f(),Ot=n("p"),Cn=a("You can pass a whole dataset pipeline:"),eo=f(),d(at.$$.fragment),to=f(),ye=n("p"),In=a("For a larger dataset where the inputs are big (like in speech or vision), you will want to pass along a generator instead of a list that loads all the inputs in memory. See the "),Lt=n("a"),Nn=a("pipeline documentation"),On=a(" for more information."),so=f(),oe=n("h3"),be=n("a"),Vs=n("span"),d(ot.$$.fragment),Ln=f(),Zs=n("span"),Rn=a("Use another model and tokenizer in the pipeline"),ao=f(),M=n("p"),Dn=a("The "),Rt=n("a"),Un=a("pipeline()"),Hn=a(" can accommodate any model from the "),rt=n("a"),Wn=a("Model Hub"),Gn=a(", making it easy to adapt the "),Dt=n("a"),Bn=a("pipeline()"),Yn=a(" for other use-cases. For example, if you\u2019d like a model capable of handling French text, use the tags on the Model Hub to filter for an appropriate model. The top filtered result returns a multilingual "),nt=n("a"),Qn=a("BERT model"),Jn=a(" fine-tuned for sentiment analysis. Great, let\u2019s use this model!"),oo=f(),d(lt.$$.fragment),ro=f(),Q=n("p"),Kn=a("Use the "),Ut=n("a"),Vn=a("AutoModelForSequenceClassification"),Zn=a(" and [\u2018AutoTokenizer\u2019] to load the pretrained model and it\u2019s associated tokenizer (more on an "),Xs=n("code"),Xn=a("AutoClass"),el=a(" below):"),no=f(),d(it.$$.fragment),lo=f(),J=n("p"),tl=a("Then you can specify the model and tokenizer in the "),Ht=n("a"),sl=a("pipeline()"),al=a(", and apply the "),ea=n("code"),ol=a("classifier"),rl=a(" on your target text:"),io=f(),d(pt.$$.fragment),po=f(),K=n("p"),nl=a("If you can\u2019t find a model for your use-case, you will need to fine-tune a pretrained model on your data. Take a look at our "),Wt=n("a"),ll=a("fine-tuning tutorial"),il=a(" to learn how. Finally, after you\u2019ve fine-tuned your pretrained model, please consider sharing it (see tutorial "),Gt=n("a"),pl=a("here"),fl=a(") with the community on the Model Hub to democratize NLP for everyone! \u{1F917}"),fo=f(),re=n("h2"),we=n("a"),ta=n("span"),d(ft.$$.fragment),hl=f(),sa=n("span"),ml=a("AutoClass"),ho=f(),d(ht.$$.fragment),mo=f(),q=n("p"),ul=a("Under the hood, the "),Bt=n("a"),cl=a("AutoModelForSequenceClassification"),dl=a(" and "),Yt=n("a"),_l=a("AutoTokenizer"),gl=a(" classes work together to power the "),Qt=n("a"),vl=a("pipeline()"),$l=a(". An "),Jt=n("a"),yl=a("AutoClass"),bl=a(" is a shortcut that automatically retrieves the architecture of a pretrained model from it\u2019s name or path. You only need to select the appropriate "),aa=n("code"),wl=a("AutoClass"),kl=a(" for your task and it\u2019s associated tokenizer with "),Kt=n("a"),jl=a("AutoTokenizer"),El=a("."),uo=f(),V=n("p"),Al=a("Let\u2019s return to our example and see how you can use the "),oa=n("code"),Tl=a("AutoClass"),xl=a(" to replicate the results of the "),Vt=n("a"),ql=a("pipeline()"),zl=a("."),co=f(),ne=n("h3"),ke=n("a"),ra=n("span"),d(mt.$$.fragment),Sl=f(),na=n("span"),Fl=a("AutoTokenizer"),_o=f(),Z=n("p"),Pl=a("A tokenizer is responsible for preprocessing text into a format that is understandable to the model. First, the tokenizer will split the text into words called "),la=n("em"),Ml=a("tokens"),Cl=a(". There are multiple rules that govern the tokenization process, including how to split a word and at what level (learn more about tokenization "),Zt=n("a"),Il=a("here"),Nl=a("). The most important thing to remember though is you need to instantiate the tokenizer with the same model name to ensure you\u2019re using the same tokenization rules a model was pretrained with."),go=f(),je=n("p"),Ol=a("Load a tokenizer with "),Xt=n("a"),Ll=a("AutoTokenizer"),Rl=a(":"),vo=f(),d(ut.$$.fragment),$o=f(),Ee=n("p"),Dl=a("Next, the tokenizer converts the tokens into numbers in order to construct a tensor as input to the model. This is known as the model\u2019s "),ia=n("em"),Ul=a("vocabulary"),Hl=a("."),yo=f(),es=n("p"),Wl=a("Pass your text to the tokenizer:"),bo=f(),d(ct.$$.fragment),wo=f(),ts=n("p"),Gl=a("The tokenizer will return a dictionary containing:"),ko=f(),Ae=n("ul"),ss=n("li"),as=n("a"),Bl=a("input_ids"),Yl=a(": numerical representions of your tokens."),Ql=f(),os=n("li"),rs=n("a"),Jl=a("atttention_mask"),Kl=a(": indicates which tokens should be attended to."),jo=f(),Te=n("p"),Vl=a("Just like the "),ns=n("a"),Zl=a("pipeline()"),Xl=a(", the tokenizer will accept a list of inputs. In addition, the tokenizer can also pad and truncate the text to return a batch with uniform length:"),Eo=f(),d(dt.$$.fragment),Ao=f(),xe=n("p"),ei=a("Read the "),ls=n("a"),ti=a("preprocessing"),si=a(" tutorial for more details about tokenization."),To=f(),le=n("h3"),qe=n("a"),pa=n("span"),d(_t.$$.fragment),ai=f(),fa=n("span"),oi=a("AutoModel"),xo=f(),S=n("p"),ri=a("\u{1F917} Transformers provides a simple and unified way to load pretrained instances. This means you can load an "),is=n("a"),ni=a("AutoModel"),li=a(" like you would load an "),ps=n("a"),ii=a("AutoTokenizer"),pi=a(". The only difference is selecting the correct "),fs=n("a"),fi=a("AutoModel"),hi=a(" for the task. Since you are doing text - or sequence - classification, load "),hs=n("a"),mi=a("AutoModelForSequenceClassification"),ui=a(". The TensorFlow equivalent is simply "),ms=n("a"),ci=a("TFAutoModelForSequenceClassification"),di=a(":"),qo=f(),d(gt.$$.fragment),zo=f(),d(ze.$$.fragment),So=f(),Se=n("p"),_i=a("Now you can pass your preprocessed batch of inputs directly to the model. If you are using a PyTorch model, unpack the dictionary by adding "),ha=n("code"),gi=a("**"),vi=a(". For TensorFlow models, pass the dictionary keys directly to the tensors:"),Fo=f(),d(vt.$$.fragment),Po=f(),X=n("p"),$i=a("The model outputs the final activations in the "),ma=n("code"),yi=a("logits"),bi=a(" attribute. Apply the softmax function to the "),ua=n("code"),wi=a("logits"),ki=a(" to retrieve the probabilities:"),Mo=f(),d($t.$$.fragment),Co=f(),d(Fe.$$.fragment),Io=f(),z=n("p"),ji=a("Models are a standard "),yt=n("a"),ca=n("code"),Ei=a("torch.nn.Module"),Ai=a(" or a "),bt=n("a"),da=n("code"),Ti=a("tf.keras.Model"),xi=a(" so you can use them in your usual training loop. However, to make things easier, \u{1F917} Transformers provides a "),us=n("a"),qi=a("Trainer"),zi=a(" class for PyTorch that adds functionality for distributed training, mixed precision, and more. For TensorFlow, you can use the "),_a=n("code"),Si=a("fit"),Fi=a(" method from "),wt=n("a"),Pi=a("Keras"),Mi=a(". Refer to the "),cs=n("a"),Ci=a("training tutorial"),Ii=a(" for more details."),No=f(),d(Pe.$$.fragment),Oo=f(),ie=n("h3"),Me=n("a"),ga=n("span"),d(kt.$$.fragment),Ni=f(),va=n("span"),Oi=a("Save a model"),Lo=f(),Ce=n("p"),Li=a("Once your model is fine-tuned, you can save it with its tokenizer using "),ds=n("a"),Ri=a("PreTrainedModel.save_pretrained()"),Di=a(":"),Ro=f(),d(jt.$$.fragment),Do=f(),Ie=n("p"),Ui=a("When you are ready to use the model again, reload it with "),_s=n("a"),Hi=a("PreTrainedModel.from_pretrained()"),Wi=a(":"),Uo=f(),d(Et.$$.fragment),Ho=f(),ee=n("p"),Gi=a("One particularly cool \u{1F917} Transformers feature is the ability to save a model and reload it as either a PyTorch or TensorFlow model. The "),$a=n("code"),Bi=a("from_pt"),Yi=a(" or "),ya=n("code"),Qi=a("from_tf"),Ji=a(" parameter can convert the model from one framework to the other:"),Wo=f(),d(At.$$.fragment),this.h()},l(e){const r=ah('[data-svelte="svelte-1phssyn"]',document.head);u=l(r,"META",{name:!0,content:!0}),r.forEach(s),k=h(e),c=l(e,"H1",{class:!0});var Tt=i(c);w=l(Tt,"A",{id:!0,class:!0,href:!0});var ba=i(w);A=l(ba,"SPAN",{});var wa=i(A);_(b.$$.fragment,wa),wa.forEach(s),ba.forEach(s),j=h(Tt),x=l(Tt,"SPAN",{});var ka=i(x);P=o(ka,"Quick tour"),ka.forEach(s),Tt.forEach(s),E=h(e),_(O.$$.fragment,e),U=h(e),H=l(e,"P",{});var pe=i(H);ur=o(pe,"Get up and running with \u{1F917} Transformers! Start using the "),xt=l(pe,"A",{href:!0});var sp=i(xt);cr=o(sp,"pipeline()"),sp.forEach(s),dr=o(pe," for rapid inference, and quickly load a pretrained model and tokenizer with an "),qt=l(pe,"A",{href:!0});var ap=i(qt);_r=o(ap,"AutoClass"),ap.forEach(s),gr=o(pe," to solve your text, vision or audio task."),pe.forEach(s),Ea=h(e),_(he.$$.fragment,e),Aa=h(e),se=l(e,"H2",{class:!0});var Bo=i(se);me=l(Bo,"A",{id:!0,class:!0,href:!0});var op=i(me);xs=l(op,"SPAN",{});var rp=i(xs);_(Le.$$.fragment,rp),rp.forEach(s),op.forEach(s),vr=h(Bo),qs=l(Bo,"SPAN",{});var np=i(qs);$r=o(np,"Pipeline"),np.forEach(s),Bo.forEach(s),Ta=h(e),Re=l(e,"P",{});var Ki=i(Re);zt=l(Ki,"A",{href:!0});var lp=i(zt);yr=o(lp,"pipeline()"),lp.forEach(s),br=o(Ki," is the easiest way to use a pretrained model for a given task."),Ki.forEach(s),xa=h(e),_(De.$$.fragment,e),qa=h(e),ue=l(e,"P",{});var Yo=i(ue);wr=o(Yo,"The "),St=l(Yo,"A",{href:!0});var ip=i(St);kr=o(ip,"pipeline()"),ip.forEach(s),jr=o(Yo," supports many common tasks out-of-the-box:"),Yo.forEach(s),za=h(e),Ue=l(e,"P",{});var Vi=i(Ue);zs=l(Vi,"STRONG",{});var pp=i(zs);Er=o(pp,"Text"),pp.forEach(s),Ar=o(Vi,":"),Vi.forEach(s),Sa=h(e),T=l(e,"UL",{});var F=i(T);Ss=l(F,"LI",{});var fp=i(Ss);Tr=o(fp,"Sentiment analysis: classify the polarity of a given text."),fp.forEach(s),xr=h(F),Fs=l(F,"LI",{});var hp=i(Fs);qr=o(hp,"Text generation (in English): generate text from a given input."),hp.forEach(s),zr=h(F),Ps=l(F,"LI",{});var mp=i(Ps);Sr=o(mp,"Name entity recognition (NER): label each word with the entity it represents (person, date, location, etc.)."),mp.forEach(s),Fr=h(F),Ms=l(F,"LI",{});var up=i(Ms);Pr=o(up,"Question answering: extract the answer from the context, given some context and a question."),up.forEach(s),Mr=h(F),Cs=l(F,"LI",{});var cp=i(Cs);Cr=o(cp,"Fill-mask: fill in the blank given a text with masked words."),cp.forEach(s),Ir=h(F),Is=l(F,"LI",{});var dp=i(Is);Nr=o(dp,"Summarization: generate a summary of a long sequence of text or document."),dp.forEach(s),Or=h(F),Ns=l(F,"LI",{});var _p=i(Ns);Lr=o(_p,"Translation: translate text into another language."),_p.forEach(s),Rr=h(F),Os=l(F,"LI",{});var gp=i(Os);Dr=o(gp,"Feature extraction: create a tensor representation of the text."),gp.forEach(s),F.forEach(s),Fa=h(e),He=l(e,"P",{});var Zi=i(He);Ls=l(Zi,"STRONG",{});var vp=i(Ls);Ur=o(vp,"Image"),vp.forEach(s),Hr=o(Zi,":"),Zi.forEach(s),Pa=h(e),W=l(e,"UL",{});var gs=i(W);Rs=l(gs,"LI",{});var $p=i(Rs);Wr=o($p,"Image classification: classify an image."),$p.forEach(s),Gr=h(gs),Ds=l(gs,"LI",{});var yp=i(Ds);Br=o(yp,"Image segmentation: classify every pixel in an image."),yp.forEach(s),Yr=h(gs),Us=l(gs,"LI",{});var bp=i(Us);Qr=o(bp,"Object detection: detect objects within an image."),bp.forEach(s),gs.forEach(s),Ma=h(e),We=l(e,"P",{});var Xi=i(We);Hs=l(Xi,"STRONG",{});var wp=i(Hs);Jr=o(wp,"Audio"),wp.forEach(s),Kr=o(Xi,":"),Xi.forEach(s),Ca=h(e),ce=l(e,"UL",{});var Qo=i(ce);Ws=l(Qo,"LI",{});var kp=i(Ws);Vr=o(kp,"Audio classification: assign a label to a given segment of audio."),kp.forEach(s),Zr=h(Qo),Gs=l(Qo,"LI",{});var jp=i(Gs);Xr=o(jp,"Automatic speech recognition (ASR): transcribe audio data into text."),jp.forEach(s),Qo.forEach(s),Ia=h(e),_(de.$$.fragment,e),Na=h(e),ae=l(e,"H3",{class:!0});var Jo=i(ae);_e=l(Jo,"A",{id:!0,class:!0,href:!0});var Ep=i(_e);Bs=l(Ep,"SPAN",{});var Ap=i(Bs);_(Ge.$$.fragment,Ap),Ap.forEach(s),Ep.forEach(s),en=h(Jo),Ys=l(Jo,"SPAN",{});var Tp=i(Ys);tn=o(Tp,"Pipeline usage"),Tp.forEach(s),Jo.forEach(s),Oa=h(e),ge=l(e,"P",{});var Ko=i(ge);sn=o(Ko,"In the following example, you will use the "),Ft=l(Ko,"A",{href:!0});var xp=i(Ft);an=o(xp,"pipeline()"),xp.forEach(s),on=o(Ko," for sentiment analysis."),Ko.forEach(s),La=h(e),Pt=l(e,"P",{});var qp=i(Pt);rn=o(qp,"Install the following dependencies if you haven\u2019t already:"),qp.forEach(s),Ra=h(e),_(Be.$$.fragment,e),Da=h(e),ve=l(e,"P",{});var Vo=i(ve);nn=o(Vo,"Import "),Mt=l(Vo,"A",{href:!0});var zp=i(Mt);ln=o(zp,"pipeline()"),zp.forEach(s),pn=o(Vo," and specify the task you want to complete:"),Vo.forEach(s),Ua=h(e),_(Ye.$$.fragment,e),Ha=h(e),G=l(e,"P",{});var vs=i(G);fn=o(vs,"The pipeline downloads and caches a default "),Qe=l(vs,"A",{href:!0,rel:!0});var Sp=i(Qe);hn=o(Sp,"pretrained model"),Sp.forEach(s),mn=o(vs," and tokenizer for sentiment analysis. Now you can use the "),Qs=l(vs,"CODE",{});var Fp=i(Qs);un=o(Fp,"classifier"),Fp.forEach(s),cn=o(vs," on your target text:"),vs.forEach(s),Wa=h(e),_(Je.$$.fragment,e),Ga=h(e),$e=l(e,"P",{});var Zo=i($e);dn=o(Zo,"For more than one sentence, pass a list of sentences to the "),Ct=l(Zo,"A",{href:!0});var Pp=i(Ct);_n=o(Pp,"pipeline()"),Pp.forEach(s),gn=o(Zo," which returns a list of dictionaries:"),Zo.forEach(s),Ba=h(e),_(Ke.$$.fragment,e),Ya=h(e),B=l(e,"P",{});var $s=i(B);vn=o($s,"The "),It=l($s,"A",{href:!0});var Mp=i(It);$n=o(Mp,"pipeline()"),Mp.forEach(s),yn=o($s," can also iterate over an entire dataset. Start by installing the "),Ve=l($s,"A",{href:!0,rel:!0});var Cp=i(Ve);bn=o(Cp,"\u{1F917} Datasets"),Cp.forEach(s),wn=o($s," library:"),$s.forEach(s),Qa=h(e),_(Ze.$$.fragment,e),Ja=h(e),L=l(e,"P",{});var Ne=i(L);kn=o(Ne,"Create a "),Nt=l(Ne,"A",{href:!0});var Ip=i(Nt);jn=o(Ip,"pipeline()"),Ip.forEach(s),En=o(Ne," with the task you want to solve for and the model you want to use. Set the "),Js=l(Ne,"CODE",{});var Np=i(Js);An=o(Np,"device"),Np.forEach(s),Tn=o(Ne," parameter to "),Ks=l(Ne,"CODE",{});var Op=i(Ks);xn=o(Op,"0"),Op.forEach(s),qn=o(Ne," to place the tensors on a CUDA device:"),Ne.forEach(s),Ka=h(e),_(Xe.$$.fragment,e),Va=h(e),Y=l(e,"P",{});var ys=i(Y);zn=o(ys,"Next, load a dataset (see the \u{1F917} Datasets "),et=l(ys,"A",{href:!0,rel:!0});var Lp=i(et);Sn=o(Lp,"Quick Start"),Lp.forEach(s),Fn=o(ys," for more details) you\u2019d like to iterate over. For example, let\u2019s load the "),tt=l(ys,"A",{href:!0,rel:!0});var Rp=i(tt);Pn=o(Rp,"SUPERB"),Rp.forEach(s),Mn=o(ys," dataset:"),ys.forEach(s),Za=h(e),_(st.$$.fragment,e),Xa=h(e),Ot=l(e,"P",{});var Dp=i(Ot);Cn=o(Dp,"You can pass a whole dataset pipeline:"),Dp.forEach(s),eo=h(e),_(at.$$.fragment,e),to=h(e),ye=l(e,"P",{});var Xo=i(ye);In=o(Xo,"For a larger dataset where the inputs are big (like in speech or vision), you will want to pass along a generator instead of a list that loads all the inputs in memory. See the "),Lt=l(Xo,"A",{href:!0});var Up=i(Lt);Nn=o(Up,"pipeline documentation"),Up.forEach(s),On=o(Xo," for more information."),Xo.forEach(s),so=h(e),oe=l(e,"H3",{class:!0});var er=i(oe);be=l(er,"A",{id:!0,class:!0,href:!0});var Hp=i(be);Vs=l(Hp,"SPAN",{});var Wp=i(Vs);_(ot.$$.fragment,Wp),Wp.forEach(s),Hp.forEach(s),Ln=h(er),Zs=l(er,"SPAN",{});var Gp=i(Zs);Rn=o(Gp,"Use another model and tokenizer in the pipeline"),Gp.forEach(s),er.forEach(s),ao=h(e),M=l(e,"P",{});var te=i(M);Dn=o(te,"The "),Rt=l(te,"A",{href:!0});var Bp=i(Rt);Un=o(Bp,"pipeline()"),Bp.forEach(s),Hn=o(te," can accommodate any model from the "),rt=l(te,"A",{href:!0,rel:!0});var Yp=i(rt);Wn=o(Yp,"Model Hub"),Yp.forEach(s),Gn=o(te,", making it easy to adapt the "),Dt=l(te,"A",{href:!0});var Qp=i(Dt);Bn=o(Qp,"pipeline()"),Qp.forEach(s),Yn=o(te," for other use-cases. For example, if you\u2019d like a model capable of handling French text, use the tags on the Model Hub to filter for an appropriate model. The top filtered result returns a multilingual "),nt=l(te,"A",{href:!0,rel:!0});var Jp=i(nt);Qn=o(Jp,"BERT model"),Jp.forEach(s),Jn=o(te," fine-tuned for sentiment analysis. Great, let\u2019s use this model!"),te.forEach(s),oo=h(e),_(lt.$$.fragment,e),ro=h(e),Q=l(e,"P",{});var bs=i(Q);Kn=o(bs,"Use the "),Ut=l(bs,"A",{href:!0});var Kp=i(Ut);Vn=o(Kp,"AutoModelForSequenceClassification"),Kp.forEach(s),Zn=o(bs," and [\u2018AutoTokenizer\u2019] to load the pretrained model and it\u2019s associated tokenizer (more on an "),Xs=l(bs,"CODE",{});var Vp=i(Xs);Xn=o(Vp,"AutoClass"),Vp.forEach(s),el=o(bs," below):"),bs.forEach(s),no=h(e),_(it.$$.fragment,e),lo=h(e),J=l(e,"P",{});var ws=i(J);tl=o(ws,"Then you can specify the model and tokenizer in the "),Ht=l(ws,"A",{href:!0});var Zp=i(Ht);sl=o(Zp,"pipeline()"),Zp.forEach(s),al=o(ws,", and apply the "),ea=l(ws,"CODE",{});var Xp=i(ea);ol=o(Xp,"classifier"),Xp.forEach(s),rl=o(ws," on your target text:"),ws.forEach(s),io=h(e),_(pt.$$.fragment,e),po=h(e),K=l(e,"P",{});var ks=i(K);nl=o(ks,"If you can\u2019t find a model for your use-case, you will need to fine-tune a pretrained model on your data. Take a look at our "),Wt=l(ks,"A",{href:!0});var ef=i(Wt);ll=o(ef,"fine-tuning tutorial"),ef.forEach(s),il=o(ks," to learn how. Finally, after you\u2019ve fine-tuned your pretrained model, please consider sharing it (see tutorial "),Gt=l(ks,"A",{href:!0});var tf=i(Gt);pl=o(tf,"here"),tf.forEach(s),fl=o(ks,") with the community on the Model Hub to democratize NLP for everyone! \u{1F917}"),ks.forEach(s),fo=h(e),re=l(e,"H2",{class:!0});var tr=i(re);we=l(tr,"A",{id:!0,class:!0,href:!0});var sf=i(we);ta=l(sf,"SPAN",{});var af=i(ta);_(ft.$$.fragment,af),af.forEach(s),sf.forEach(s),hl=h(tr),sa=l(tr,"SPAN",{});var of=i(sa);ml=o(of,"AutoClass"),of.forEach(s),tr.forEach(s),ho=h(e),_(ht.$$.fragment,e),mo=h(e),q=l(e,"P",{});var C=i(q);ul=o(C,"Under the hood, the "),Bt=l(C,"A",{href:!0});var rf=i(Bt);cl=o(rf,"AutoModelForSequenceClassification"),rf.forEach(s),dl=o(C," and "),Yt=l(C,"A",{href:!0});var nf=i(Yt);_l=o(nf,"AutoTokenizer"),nf.forEach(s),gl=o(C," classes work together to power the "),Qt=l(C,"A",{href:!0});var lf=i(Qt);vl=o(lf,"pipeline()"),lf.forEach(s),$l=o(C,". An "),Jt=l(C,"A",{href:!0});var pf=i(Jt);yl=o(pf,"AutoClass"),pf.forEach(s),bl=o(C," is a shortcut that automatically retrieves the architecture of a pretrained model from it\u2019s name or path. You only need to select the appropriate "),aa=l(C,"CODE",{});var ff=i(aa);wl=o(ff,"AutoClass"),ff.forEach(s),kl=o(C," for your task and it\u2019s associated tokenizer with "),Kt=l(C,"A",{href:!0});var hf=i(Kt);jl=o(hf,"AutoTokenizer"),hf.forEach(s),El=o(C,"."),C.forEach(s),uo=h(e),V=l(e,"P",{});var js=i(V);Al=o(js,"Let\u2019s return to our example and see how you can use the "),oa=l(js,"CODE",{});var mf=i(oa);Tl=o(mf,"AutoClass"),mf.forEach(s),xl=o(js," to replicate the results of the "),Vt=l(js,"A",{href:!0});var uf=i(Vt);ql=o(uf,"pipeline()"),uf.forEach(s),zl=o(js,"."),js.forEach(s),co=h(e),ne=l(e,"H3",{class:!0});var sr=i(ne);ke=l(sr,"A",{id:!0,class:!0,href:!0});var cf=i(ke);ra=l(cf,"SPAN",{});var df=i(ra);_(mt.$$.fragment,df),df.forEach(s),cf.forEach(s),Sl=h(sr),na=l(sr,"SPAN",{});var _f=i(na);Fl=o(_f,"AutoTokenizer"),_f.forEach(s),sr.forEach(s),_o=h(e),Z=l(e,"P",{});var Es=i(Z);Pl=o(Es,"A tokenizer is responsible for preprocessing text into a format that is understandable to the model. First, the tokenizer will split the text into words called "),la=l(Es,"EM",{});var gf=i(la);Ml=o(gf,"tokens"),gf.forEach(s),Cl=o(Es,". There are multiple rules that govern the tokenization process, including how to split a word and at what level (learn more about tokenization "),Zt=l(Es,"A",{href:!0});var vf=i(Zt);Il=o(vf,"here"),vf.forEach(s),Nl=o(Es,"). The most important thing to remember though is you need to instantiate the tokenizer with the same model name to ensure you\u2019re using the same tokenization rules a model was pretrained with."),Es.forEach(s),go=h(e),je=l(e,"P",{});var ar=i(je);Ol=o(ar,"Load a tokenizer with "),Xt=l(ar,"A",{href:!0});var $f=i(Xt);Ll=o($f,"AutoTokenizer"),$f.forEach(s),Rl=o(ar,":"),ar.forEach(s),vo=h(e),_(ut.$$.fragment,e),$o=h(e),Ee=l(e,"P",{});var or=i(Ee);Dl=o(or,"Next, the tokenizer converts the tokens into numbers in order to construct a tensor as input to the model. This is known as the model\u2019s "),ia=l(or,"EM",{});var yf=i(ia);Ul=o(yf,"vocabulary"),yf.forEach(s),Hl=o(or,"."),or.forEach(s),yo=h(e),es=l(e,"P",{});var bf=i(es);Wl=o(bf,"Pass your text to the tokenizer:"),bf.forEach(s),bo=h(e),_(ct.$$.fragment,e),wo=h(e),ts=l(e,"P",{});var wf=i(ts);Gl=o(wf,"The tokenizer will return a dictionary containing:"),wf.forEach(s),ko=h(e),Ae=l(e,"UL",{});var rr=i(Ae);ss=l(rr,"LI",{});var ep=i(ss);as=l(ep,"A",{href:!0});var kf=i(as);Bl=o(kf,"input_ids"),kf.forEach(s),Yl=o(ep,": numerical representions of your tokens."),ep.forEach(s),Ql=h(rr),os=l(rr,"LI",{});var tp=i(os);rs=l(tp,"A",{href:!0});var jf=i(rs);Jl=o(jf,"atttention_mask"),jf.forEach(s),Kl=o(tp,": indicates which tokens should be attended to."),tp.forEach(s),rr.forEach(s),jo=h(e),Te=l(e,"P",{});var nr=i(Te);Vl=o(nr,"Just like the "),ns=l(nr,"A",{href:!0});var Ef=i(ns);Zl=o(Ef,"pipeline()"),Ef.forEach(s),Xl=o(nr,", the tokenizer will accept a list of inputs. In addition, the tokenizer can also pad and truncate the text to return a batch with uniform length:"),nr.forEach(s),Eo=h(e),_(dt.$$.fragment,e),Ao=h(e),xe=l(e,"P",{});var lr=i(xe);ei=o(lr,"Read the "),ls=l(lr,"A",{href:!0});var Af=i(ls);ti=o(Af,"preprocessing"),Af.forEach(s),si=o(lr," tutorial for more details about tokenization."),lr.forEach(s),To=h(e),le=l(e,"H3",{class:!0});var ir=i(le);qe=l(ir,"A",{id:!0,class:!0,href:!0});var Tf=i(qe);pa=l(Tf,"SPAN",{});var xf=i(pa);_(_t.$$.fragment,xf),xf.forEach(s),Tf.forEach(s),ai=h(ir),fa=l(ir,"SPAN",{});var qf=i(fa);oi=o(qf,"AutoModel"),qf.forEach(s),ir.forEach(s),xo=h(e),S=l(e,"P",{});var R=i(S);ri=o(R,"\u{1F917} Transformers provides a simple and unified way to load pretrained instances. This means you can load an "),is=l(R,"A",{href:!0});var zf=i(is);ni=o(zf,"AutoModel"),zf.forEach(s),li=o(R," like you would load an "),ps=l(R,"A",{href:!0});var Sf=i(ps);ii=o(Sf,"AutoTokenizer"),Sf.forEach(s),pi=o(R,". The only difference is selecting the correct "),fs=l(R,"A",{href:!0});var Ff=i(fs);fi=o(Ff,"AutoModel"),Ff.forEach(s),hi=o(R," for the task. Since you are doing text - or sequence - classification, load "),hs=l(R,"A",{href:!0});var Pf=i(hs);mi=o(Pf,"AutoModelForSequenceClassification"),Pf.forEach(s),ui=o(R,". The TensorFlow equivalent is simply "),ms=l(R,"A",{href:!0});var Mf=i(ms);ci=o(Mf,"TFAutoModelForSequenceClassification"),Mf.forEach(s),di=o(R,":"),R.forEach(s),qo=h(e),_(gt.$$.fragment,e),zo=h(e),_(ze.$$.fragment,e),So=h(e),Se=l(e,"P",{});var pr=i(Se);_i=o(pr,"Now you can pass your preprocessed batch of inputs directly to the model. If you are using a PyTorch model, unpack the dictionary by adding "),ha=l(pr,"CODE",{});var Cf=i(ha);gi=o(Cf,"**"),Cf.forEach(s),vi=o(pr,". For TensorFlow models, pass the dictionary keys directly to the tensors:"),pr.forEach(s),Fo=h(e),_(vt.$$.fragment,e),Po=h(e),X=l(e,"P",{});var As=i(X);$i=o(As,"The model outputs the final activations in the "),ma=l(As,"CODE",{});var If=i(ma);yi=o(If,"logits"),If.forEach(s),bi=o(As," attribute. Apply the softmax function to the "),ua=l(As,"CODE",{});var Nf=i(ua);wi=o(Nf,"logits"),Nf.forEach(s),ki=o(As," to retrieve the probabilities:"),As.forEach(s),Mo=h(e),_($t.$$.fragment,e),Co=h(e),_(Fe.$$.fragment,e),Io=h(e),z=l(e,"P",{});var I=i(z);ji=o(I,"Models are a standard "),yt=l(I,"A",{href:!0,rel:!0});var Of=i(yt);ca=l(Of,"CODE",{});var Lf=i(ca);Ei=o(Lf,"torch.nn.Module"),Lf.forEach(s),Of.forEach(s),Ai=o(I," or a "),bt=l(I,"A",{href:!0,rel:!0});var Rf=i(bt);da=l(Rf,"CODE",{});var Df=i(da);Ti=o(Df,"tf.keras.Model"),Df.forEach(s),Rf.forEach(s),xi=o(I," so you can use them in your usual training loop. However, to make things easier, \u{1F917} Transformers provides a "),us=l(I,"A",{href:!0});var Uf=i(us);qi=o(Uf,"Trainer"),Uf.forEach(s),zi=o(I," class for PyTorch that adds functionality for distributed training, mixed precision, and more. For TensorFlow, you can use the "),_a=l(I,"CODE",{});var Hf=i(_a);Si=o(Hf,"fit"),Hf.forEach(s),Fi=o(I," method from "),wt=l(I,"A",{href:!0,rel:!0});var Wf=i(wt);Pi=o(Wf,"Keras"),Wf.forEach(s),Mi=o(I,". Refer to the "),cs=l(I,"A",{href:!0});var Gf=i(cs);Ci=o(Gf,"training tutorial"),Gf.forEach(s),Ii=o(I," for more details."),I.forEach(s),No=h(e),_(Pe.$$.fragment,e),Oo=h(e),ie=l(e,"H3",{class:!0});var fr=i(ie);Me=l(fr,"A",{id:!0,class:!0,href:!0});var Bf=i(Me);ga=l(Bf,"SPAN",{});var Yf=i(ga);_(kt.$$.fragment,Yf),Yf.forEach(s),Bf.forEach(s),Ni=h(fr),va=l(fr,"SPAN",{});var Qf=i(va);Oi=o(Qf,"Save a model"),Qf.forEach(s),fr.forEach(s),Lo=h(e),Ce=l(e,"P",{});var hr=i(Ce);Li=o(hr,"Once your model is fine-tuned, you can save it with its tokenizer using "),ds=l(hr,"A",{href:!0});var Jf=i(ds);Ri=o(Jf,"PreTrainedModel.save_pretrained()"),Jf.forEach(s),Di=o(hr,":"),hr.forEach(s),Ro=h(e),_(jt.$$.fragment,e),Do=h(e),Ie=l(e,"P",{});var mr=i(Ie);Ui=o(mr,"When you are ready to use the model again, reload it with "),_s=l(mr,"A",{href:!0});var Kf=i(_s);Hi=o(Kf,"PreTrainedModel.from_pretrained()"),Kf.forEach(s),Wi=o(mr,":"),mr.forEach(s),Uo=h(e),_(Et.$$.fragment,e),Ho=h(e),ee=l(e,"P",{});var Ts=i(ee);Gi=o(Ts,"One particularly cool \u{1F917} Transformers feature is the ability to save a model and reload it as either a PyTorch or TensorFlow model. The "),$a=l(Ts,"CODE",{});var Vf=i($a);Bi=o(Vf,"from_pt"),Vf.forEach(s),Yi=o(Ts," or "),ya=l(Ts,"CODE",{});var Zf=i(ya);Qi=o(Zf,"from_tf"),Zf.forEach(s),Ji=o(Ts," parameter can convert the model from one framework to the other:"),Ts.forEach(s),Wo=h(e),_(At.$$.fragment,e),this.h()},h(){m(u,"name","hf:doc:metadata"),m(u,"content",JSON.stringify(hh)),m(w,"id","quick-tour"),m(w,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(w,"href","#quick-tour"),m(c,"class","relative group"),m(xt,"href","/docs/transformers/pr_15830/en/main_classes/pipelines#transformers.pipeline"),m(qt,"href","./model_doc/auto"),m(me,"id","pipeline"),m(me,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(me,"href","#pipeline"),m(se,"class","relative group"),m(zt,"href","/docs/transformers/pr_15830/en/main_classes/pipelines#transformers.pipeline"),m(St,"href","/docs/transformers/pr_15830/en/main_classes/pipelines#transformers.pipeline"),m(_e,"id","pipeline-usage"),m(_e,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(_e,"href","#pipeline-usage"),m(ae,"class","relative group"),m(Ft,"href","/docs/transformers/pr_15830/en/main_classes/pipelines#transformers.pipeline"),m(Mt,"href","/docs/transformers/pr_15830/en/main_classes/pipelines#transformers.pipeline"),m(Qe,"href","https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english"),m(Qe,"rel","nofollow"),m(Ct,"href","/docs/transformers/pr_15830/en/main_classes/pipelines#transformers.pipeline"),m(It,"href","/docs/transformers/pr_15830/en/main_classes/pipelines#transformers.pipeline"),m(Ve,"href","https://huggingface.co/docs/datasets/"),m(Ve,"rel","nofollow"),m(Nt,"href","/docs/transformers/pr_15830/en/main_classes/pipelines#transformers.pipeline"),m(et,"href","https://huggingface.co/docs/datasets/quickstart.html"),m(et,"rel","nofollow"),m(tt,"href","https://huggingface.co/datasets/superb"),m(tt,"rel","nofollow"),m(Lt,"href","main_classes/pipeline"),m(be,"id","use-another-model-and-tokenizer-in-the-pipeline"),m(be,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(be,"href","#use-another-model-and-tokenizer-in-the-pipeline"),m(oe,"class","relative group"),m(Rt,"href","/docs/transformers/pr_15830/en/main_classes/pipelines#transformers.pipeline"),m(rt,"href","https://huggingface.co/models"),m(rt,"rel","nofollow"),m(Dt,"href","/docs/transformers/pr_15830/en/main_classes/pipelines#transformers.pipeline"),m(nt,"href","https://huggingface.co/nlptown/bert-base-multilingual-uncased-sentiment"),m(nt,"rel","nofollow"),m(Ut,"href","/docs/transformers/pr_15830/en/model_doc/auto#transformers.AutoModelForSequenceClassification"),m(Ht,"href","/docs/transformers/pr_15830/en/main_classes/pipelines#transformers.pipeline"),m(Wt,"href","./training"),m(Gt,"href","./model_sharing"),m(we,"id","autoclass"),m(we,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(we,"href","#autoclass"),m(re,"class","relative group"),m(Bt,"href","/docs/transformers/pr_15830/en/model_doc/auto#transformers.AutoModelForSequenceClassification"),m(Yt,"href","/docs/transformers/pr_15830/en/model_doc/auto#transformers.AutoTokenizer"),m(Qt,"href","/docs/transformers/pr_15830/en/main_classes/pipelines#transformers.pipeline"),m(Jt,"href","./model_doc/auto"),m(Kt,"href","/docs/transformers/pr_15830/en/model_doc/auto#transformers.AutoTokenizer"),m(Vt,"href","/docs/transformers/pr_15830/en/main_classes/pipelines#transformers.pipeline"),m(ke,"id","autotokenizer"),m(ke,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(ke,"href","#autotokenizer"),m(ne,"class","relative group"),m(Zt,"href","./tokenizer_summary"),m(Xt,"href","/docs/transformers/pr_15830/en/model_doc/auto#transformers.AutoTokenizer"),m(as,"href","./glossary#input-ids"),m(rs,"href",".glossary#attention-mask"),m(ns,"href","/docs/transformers/pr_15830/en/main_classes/pipelines#transformers.pipeline"),m(ls,"href","./preprocessing"),m(qe,"id","automodel"),m(qe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(qe,"href","#automodel"),m(le,"class","relative group"),m(is,"href","/docs/transformers/pr_15830/en/model_doc/auto#transformers.AutoModel"),m(ps,"href","/docs/transformers/pr_15830/en/model_doc/auto#transformers.AutoTokenizer"),m(fs,"href","/docs/transformers/pr_15830/en/model_doc/auto#transformers.AutoModel"),m(hs,"href","/docs/transformers/pr_15830/en/model_doc/auto#transformers.AutoModelForSequenceClassification"),m(ms,"href","/docs/transformers/pr_15830/en/model_doc/auto#transformers.TFAutoModelForSequenceClassification"),m(yt,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),m(yt,"rel","nofollow"),m(bt,"href","https://www.tensorflow.org/api_docs/python/tf/keras/Model"),m(bt,"rel","nofollow"),m(us,"href","/docs/transformers/pr_15830/en/main_classes/trainer#transformers.Trainer"),m(wt,"href","https://keras.io/"),m(wt,"rel","nofollow"),m(cs,"href","./training"),m(Me,"id","save-a-model"),m(Me,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(Me,"href","#save-a-model"),m(ie,"class","relative group"),m(ds,"href","/docs/transformers/pr_15830/en/main_classes/model#transformers.PreTrainedModel.save_pretrained"),m(_s,"href","/docs/transformers/pr_15830/en/main_classes/model#transformers.PreTrainedModel.from_pretrained")},m(e,r){t(document.head,u),p(e,k,r),p(e,c,r),t(c,w),t(w,A),g(b,A,null),t(c,j),t(c,x),t(x,P),p(e,E,r),g(O,e,r),p(e,U,r),p(e,H,r),t(H,ur),t(H,xt),t(xt,cr),t(H,dr),t(H,qt),t(qt,_r),t(H,gr),p(e,Ea,r),g(he,e,r),p(e,Aa,r),p(e,se,r),t(se,me),t(me,xs),g(Le,xs,null),t(se,vr),t(se,qs),t(qs,$r),p(e,Ta,r),p(e,Re,r),t(Re,zt),t(zt,yr),t(Re,br),p(e,xa,r),g(De,e,r),p(e,qa,r),p(e,ue,r),t(ue,wr),t(ue,St),t(St,kr),t(ue,jr),p(e,za,r),p(e,Ue,r),t(Ue,zs),t(zs,Er),t(Ue,Ar),p(e,Sa,r),p(e,T,r),t(T,Ss),t(Ss,Tr),t(T,xr),t(T,Fs),t(Fs,qr),t(T,zr),t(T,Ps),t(Ps,Sr),t(T,Fr),t(T,Ms),t(Ms,Pr),t(T,Mr),t(T,Cs),t(Cs,Cr),t(T,Ir),t(T,Is),t(Is,Nr),t(T,Or),t(T,Ns),t(Ns,Lr),t(T,Rr),t(T,Os),t(Os,Dr),p(e,Fa,r),p(e,He,r),t(He,Ls),t(Ls,Ur),t(He,Hr),p(e,Pa,r),p(e,W,r),t(W,Rs),t(Rs,Wr),t(W,Gr),t(W,Ds),t(Ds,Br),t(W,Yr),t(W,Us),t(Us,Qr),p(e,Ma,r),p(e,We,r),t(We,Hs),t(Hs,Jr),t(We,Kr),p(e,Ca,r),p(e,ce,r),t(ce,Ws),t(Ws,Vr),t(ce,Zr),t(ce,Gs),t(Gs,Xr),p(e,Ia,r),g(de,e,r),p(e,Na,r),p(e,ae,r),t(ae,_e),t(_e,Bs),g(Ge,Bs,null),t(ae,en),t(ae,Ys),t(Ys,tn),p(e,Oa,r),p(e,ge,r),t(ge,sn),t(ge,Ft),t(Ft,an),t(ge,on),p(e,La,r),p(e,Pt,r),t(Pt,rn),p(e,Ra,r),g(Be,e,r),p(e,Da,r),p(e,ve,r),t(ve,nn),t(ve,Mt),t(Mt,ln),t(ve,pn),p(e,Ua,r),g(Ye,e,r),p(e,Ha,r),p(e,G,r),t(G,fn),t(G,Qe),t(Qe,hn),t(G,mn),t(G,Qs),t(Qs,un),t(G,cn),p(e,Wa,r),g(Je,e,r),p(e,Ga,r),p(e,$e,r),t($e,dn),t($e,Ct),t(Ct,_n),t($e,gn),p(e,Ba,r),g(Ke,e,r),p(e,Ya,r),p(e,B,r),t(B,vn),t(B,It),t(It,$n),t(B,yn),t(B,Ve),t(Ve,bn),t(B,wn),p(e,Qa,r),g(Ze,e,r),p(e,Ja,r),p(e,L,r),t(L,kn),t(L,Nt),t(Nt,jn),t(L,En),t(L,Js),t(Js,An),t(L,Tn),t(L,Ks),t(Ks,xn),t(L,qn),p(e,Ka,r),g(Xe,e,r),p(e,Va,r),p(e,Y,r),t(Y,zn),t(Y,et),t(et,Sn),t(Y,Fn),t(Y,tt),t(tt,Pn),t(Y,Mn),p(e,Za,r),g(st,e,r),p(e,Xa,r),p(e,Ot,r),t(Ot,Cn),p(e,eo,r),g(at,e,r),p(e,to,r),p(e,ye,r),t(ye,In),t(ye,Lt),t(Lt,Nn),t(ye,On),p(e,so,r),p(e,oe,r),t(oe,be),t(be,Vs),g(ot,Vs,null),t(oe,Ln),t(oe,Zs),t(Zs,Rn),p(e,ao,r),p(e,M,r),t(M,Dn),t(M,Rt),t(Rt,Un),t(M,Hn),t(M,rt),t(rt,Wn),t(M,Gn),t(M,Dt),t(Dt,Bn),t(M,Yn),t(M,nt),t(nt,Qn),t(M,Jn),p(e,oo,r),g(lt,e,r),p(e,ro,r),p(e,Q,r),t(Q,Kn),t(Q,Ut),t(Ut,Vn),t(Q,Zn),t(Q,Xs),t(Xs,Xn),t(Q,el),p(e,no,r),g(it,e,r),p(e,lo,r),p(e,J,r),t(J,tl),t(J,Ht),t(Ht,sl),t(J,al),t(J,ea),t(ea,ol),t(J,rl),p(e,io,r),g(pt,e,r),p(e,po,r),p(e,K,r),t(K,nl),t(K,Wt),t(Wt,ll),t(K,il),t(K,Gt),t(Gt,pl),t(K,fl),p(e,fo,r),p(e,re,r),t(re,we),t(we,ta),g(ft,ta,null),t(re,hl),t(re,sa),t(sa,ml),p(e,ho,r),g(ht,e,r),p(e,mo,r),p(e,q,r),t(q,ul),t(q,Bt),t(Bt,cl),t(q,dl),t(q,Yt),t(Yt,_l),t(q,gl),t(q,Qt),t(Qt,vl),t(q,$l),t(q,Jt),t(Jt,yl),t(q,bl),t(q,aa),t(aa,wl),t(q,kl),t(q,Kt),t(Kt,jl),t(q,El),p(e,uo,r),p(e,V,r),t(V,Al),t(V,oa),t(oa,Tl),t(V,xl),t(V,Vt),t(Vt,ql),t(V,zl),p(e,co,r),p(e,ne,r),t(ne,ke),t(ke,ra),g(mt,ra,null),t(ne,Sl),t(ne,na),t(na,Fl),p(e,_o,r),p(e,Z,r),t(Z,Pl),t(Z,la),t(la,Ml),t(Z,Cl),t(Z,Zt),t(Zt,Il),t(Z,Nl),p(e,go,r),p(e,je,r),t(je,Ol),t(je,Xt),t(Xt,Ll),t(je,Rl),p(e,vo,r),g(ut,e,r),p(e,$o,r),p(e,Ee,r),t(Ee,Dl),t(Ee,ia),t(ia,Ul),t(Ee,Hl),p(e,yo,r),p(e,es,r),t(es,Wl),p(e,bo,r),g(ct,e,r),p(e,wo,r),p(e,ts,r),t(ts,Gl),p(e,ko,r),p(e,Ae,r),t(Ae,ss),t(ss,as),t(as,Bl),t(ss,Yl),t(Ae,Ql),t(Ae,os),t(os,rs),t(rs,Jl),t(os,Kl),p(e,jo,r),p(e,Te,r),t(Te,Vl),t(Te,ns),t(ns,Zl),t(Te,Xl),p(e,Eo,r),g(dt,e,r),p(e,Ao,r),p(e,xe,r),t(xe,ei),t(xe,ls),t(ls,ti),t(xe,si),p(e,To,r),p(e,le,r),t(le,qe),t(qe,pa),g(_t,pa,null),t(le,ai),t(le,fa),t(fa,oi),p(e,xo,r),p(e,S,r),t(S,ri),t(S,is),t(is,ni),t(S,li),t(S,ps),t(ps,ii),t(S,pi),t(S,fs),t(fs,fi),t(S,hi),t(S,hs),t(hs,mi),t(S,ui),t(S,ms),t(ms,ci),t(S,di),p(e,qo,r),g(gt,e,r),p(e,zo,r),g(ze,e,r),p(e,So,r),p(e,Se,r),t(Se,_i),t(Se,ha),t(ha,gi),t(Se,vi),p(e,Fo,r),g(vt,e,r),p(e,Po,r),p(e,X,r),t(X,$i),t(X,ma),t(ma,yi),t(X,bi),t(X,ua),t(ua,wi),t(X,ki),p(e,Mo,r),g($t,e,r),p(e,Co,r),g(Fe,e,r),p(e,Io,r),p(e,z,r),t(z,ji),t(z,yt),t(yt,ca),t(ca,Ei),t(z,Ai),t(z,bt),t(bt,da),t(da,Ti),t(z,xi),t(z,us),t(us,qi),t(z,zi),t(z,_a),t(_a,Si),t(z,Fi),t(z,wt),t(wt,Pi),t(z,Mi),t(z,cs),t(cs,Ci),t(z,Ii),p(e,No,r),g(Pe,e,r),p(e,Oo,r),p(e,ie,r),t(ie,Me),t(Me,ga),g(kt,ga,null),t(ie,Ni),t(ie,va),t(va,Oi),p(e,Lo,r),p(e,Ce,r),t(Ce,Li),t(Ce,ds),t(ds,Ri),t(Ce,Di),p(e,Ro,r),g(jt,e,r),p(e,Do,r),p(e,Ie,r),t(Ie,Ui),t(Ie,_s),t(_s,Hi),t(Ie,Wi),p(e,Uo,r),g(Et,e,r),p(e,Ho,r),p(e,ee,r),t(ee,Gi),t(ee,$a),t($a,Bi),t(ee,Yi),t(ee,ya),t(ya,Qi),t(ee,Ji),p(e,Wo,r),g(At,e,r),Go=!0},p(e,[r]){const Tt={};r&2&&(Tt.$$scope={dirty:r,ctx:e}),he.$set(Tt);const ba={};r&2&&(ba.$$scope={dirty:r,ctx:e}),de.$set(ba);const wa={};r&2&&(wa.$$scope={dirty:r,ctx:e}),ze.$set(wa);const ka={};r&2&&(ka.$$scope={dirty:r,ctx:e}),Fe.$set(ka);const pe={};r&2&&(pe.$$scope={dirty:r,ctx:e}),Pe.$set(pe)},i(e){Go||(v(b.$$.fragment,e),v(O.$$.fragment,e),v(he.$$.fragment,e),v(Le.$$.fragment,e),v(De.$$.fragment,e),v(de.$$.fragment,e),v(Ge.$$.fragment,e),v(Be.$$.fragment,e),v(Ye.$$.fragment,e),v(Je.$$.fragment,e),v(Ke.$$.fragment,e),v(Ze.$$.fragment,e),v(Xe.$$.fragment,e),v(st.$$.fragment,e),v(at.$$.fragment,e),v(ot.$$.fragment,e),v(lt.$$.fragment,e),v(it.$$.fragment,e),v(pt.$$.fragment,e),v(ft.$$.fragment,e),v(ht.$$.fragment,e),v(mt.$$.fragment,e),v(ut.$$.fragment,e),v(ct.$$.fragment,e),v(dt.$$.fragment,e),v(_t.$$.fragment,e),v(gt.$$.fragment,e),v(ze.$$.fragment,e),v(vt.$$.fragment,e),v($t.$$.fragment,e),v(Fe.$$.fragment,e),v(Pe.$$.fragment,e),v(kt.$$.fragment,e),v(jt.$$.fragment,e),v(Et.$$.fragment,e),v(At.$$.fragment,e),Go=!0)},o(e){$(b.$$.fragment,e),$(O.$$.fragment,e),$(he.$$.fragment,e),$(Le.$$.fragment,e),$(De.$$.fragment,e),$(de.$$.fragment,e),$(Ge.$$.fragment,e),$(Be.$$.fragment,e),$(Ye.$$.fragment,e),$(Je.$$.fragment,e),$(Ke.$$.fragment,e),$(Ze.$$.fragment,e),$(Xe.$$.fragment,e),$(st.$$.fragment,e),$(at.$$.fragment,e),$(ot.$$.fragment,e),$(lt.$$.fragment,e),$(it.$$.fragment,e),$(pt.$$.fragment,e),$(ft.$$.fragment,e),$(ht.$$.fragment,e),$(mt.$$.fragment,e),$(ut.$$.fragment,e),$(ct.$$.fragment,e),$(dt.$$.fragment,e),$(_t.$$.fragment,e),$(gt.$$.fragment,e),$(ze.$$.fragment,e),$(vt.$$.fragment,e),$($t.$$.fragment,e),$(Fe.$$.fragment,e),$(Pe.$$.fragment,e),$(kt.$$.fragment,e),$(jt.$$.fragment,e),$(Et.$$.fragment,e),$(At.$$.fragment,e),Go=!1},d(e){s(u),e&&s(k),e&&s(c),y(b),e&&s(E),y(O,e),e&&s(U),e&&s(H),e&&s(Ea),y(he,e),e&&s(Aa),e&&s(se),y(Le),e&&s(Ta),e&&s(Re),e&&s(xa),y(De,e),e&&s(qa),e&&s(ue),e&&s(za),e&&s(Ue),e&&s(Sa),e&&s(T),e&&s(Fa),e&&s(He),e&&s(Pa),e&&s(W),e&&s(Ma),e&&s(We),e&&s(Ca),e&&s(ce),e&&s(Ia),y(de,e),e&&s(Na),e&&s(ae),y(Ge),e&&s(Oa),e&&s(ge),e&&s(La),e&&s(Pt),e&&s(Ra),y(Be,e),e&&s(Da),e&&s(ve),e&&s(Ua),y(Ye,e),e&&s(Ha),e&&s(G),e&&s(Wa),y(Je,e),e&&s(Ga),e&&s($e),e&&s(Ba),y(Ke,e),e&&s(Ya),e&&s(B),e&&s(Qa),y(Ze,e),e&&s(Ja),e&&s(L),e&&s(Ka),y(Xe,e),e&&s(Va),e&&s(Y),e&&s(Za),y(st,e),e&&s(Xa),e&&s(Ot),e&&s(eo),y(at,e),e&&s(to),e&&s(ye),e&&s(so),e&&s(oe),y(ot),e&&s(ao),e&&s(M),e&&s(oo),y(lt,e),e&&s(ro),e&&s(Q),e&&s(no),y(it,e),e&&s(lo),e&&s(J),e&&s(io),y(pt,e),e&&s(po),e&&s(K),e&&s(fo),e&&s(re),y(ft),e&&s(ho),y(ht,e),e&&s(mo),e&&s(q),e&&s(uo),e&&s(V),e&&s(co),e&&s(ne),y(mt),e&&s(_o),e&&s(Z),e&&s(go),e&&s(je),e&&s(vo),y(ut,e),e&&s($o),e&&s(Ee),e&&s(yo),e&&s(es),e&&s(bo),y(ct,e),e&&s(wo),e&&s(ts),e&&s(ko),e&&s(Ae),e&&s(jo),e&&s(Te),e&&s(Eo),y(dt,e),e&&s(Ao),e&&s(xe),e&&s(To),e&&s(le),y(_t),e&&s(xo),e&&s(S),e&&s(qo),y(gt,e),e&&s(zo),y(ze,e),e&&s(So),e&&s(Se),e&&s(Fo),y(vt,e),e&&s(Po),e&&s(X),e&&s(Mo),y($t,e),e&&s(Co),y(Fe,e),e&&s(Io),e&&s(z),e&&s(No),y(Pe,e),e&&s(Oo),e&&s(ie),y(kt),e&&s(Lo),e&&s(Ce),e&&s(Ro),y(jt,e),e&&s(Do),e&&s(Ie),e&&s(Uo),y(Et,e),e&&s(Ho),e&&s(ee),e&&s(Wo),y(At,e)}}}const hh={local:"quick-tour",sections:[{local:"pipeline",sections:[{local:"pipeline-usage",title:"Pipeline usage"},{local:"use-another-model-and-tokenizer-in-the-pipeline",title:"Use another model and tokenizer in the pipeline"}],title:"Pipeline"},{local:"autoclass",sections:[{local:"autotokenizer",title:"AutoTokenizer"},{local:"automodel",title:"AutoModel"},{local:"save-a-model",title:"Save a model"}],title:"AutoClass"}],title:"Quick tour"};function mh(N,u,k){let{fw:c}=u;return N.$$set=w=>{"fw"in w&&k(0,c=w.fw)},[c]}class bh extends eh{constructor(u){super();th(this,u,mh,fh,sh,{fw:0})}}export{bh as default,hh as metadata};
